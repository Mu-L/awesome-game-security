Project Path: arc_facebookresearch_CUTracer_y6r2fyq_

Source Tree:

```txt
arc_facebookresearch_CUTracer_y6r2fyq_
â”œâ”€â”€ CODE_OF_CONDUCT.md
â”œâ”€â”€ CONTRIBUTING.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ LICENSE-BSD
â”œâ”€â”€ Makefile
â”œâ”€â”€ format.sh
â”œâ”€â”€ include
â”‚   â”œâ”€â”€ analysis.h
â”‚   â”œâ”€â”€ common.h
â”‚   â”œâ”€â”€ delay_inject_config.h
â”‚   â”œâ”€â”€ env_config.h
â”‚   â”œâ”€â”€ implicit_regs.h
â”‚   â”œâ”€â”€ instr_category.h
â”‚   â”œâ”€â”€ instrument.h
â”‚   â”œâ”€â”€ log.h
â”‚   â””â”€â”€ trace_writer.h
â”œâ”€â”€ install_third_party.sh
â”œâ”€â”€ logo.svg
â”œâ”€â”€ pr.md
â”œâ”€â”€ python
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ cutracer
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ __main__.py
â”‚   â”‚   â”œâ”€â”€ analyze
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ cli.py
â”‚   â”‚   â”œâ”€â”€ cli.py
â”‚   â”‚   â”œâ”€â”€ kernel_config.py
â”‚   â”‚   â”œâ”€â”€ query
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ cli.py
â”‚   â”‚   â”‚   â”œâ”€â”€ clp.py
â”‚   â”‚   â”‚   â”œâ”€â”€ formatters.py
â”‚   â”‚   â”‚   â”œâ”€â”€ grouper.py
â”‚   â”‚   â”‚   â”œâ”€â”€ reader.py
â”‚   â”‚   â”‚   â””â”€â”€ warp_summary.py
â”‚   â”‚   â”œâ”€â”€ reduce
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ cli.py
â”‚   â”‚   â”‚   â”œâ”€â”€ config_mutator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ reduce.py
â”‚   â”‚   â”‚   â””â”€â”€ report.py
â”‚   â”‚   â”œâ”€â”€ shared_vars.py
â”‚   â”‚   â””â”€â”€ validation
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â”œâ”€â”€ cli.py
â”‚   â”‚       â”œâ”€â”€ compression.py
â”‚   â”‚       â”œâ”€â”€ consistency.py
â”‚   â”‚       â”œâ”€â”€ json_validator.py
â”‚   â”‚       â”œâ”€â”€ schema_loader.py
â”‚   â”‚       â”œâ”€â”€ schemas
â”‚   â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚       â”‚   â”œâ”€â”€ delay_config.schema.json
â”‚   â”‚       â”‚   â”œâ”€â”€ kernel_metadata.schema.json
â”‚   â”‚       â”‚   â”œâ”€â”€ mem_trace.schema.json
â”‚   â”‚       â”‚   â”œâ”€â”€ opcode_only.schema.json
â”‚   â”‚       â”‚   â””â”€â”€ reg_trace.schema.json
â”‚   â”‚       â””â”€â”€ text_validator.py
â”‚   â”œâ”€â”€ pyproject.toml
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ tests
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ example_inputs
â”‚       â”‚   â”œâ”€â”€ invalid_schema.ndjson
â”‚       â”‚   â”œâ”€â”€ invalid_syntax.ndjson
â”‚       â”‚   â”œâ”€â”€ reg_trace_sample.log
â”‚       â”‚   â”œâ”€â”€ reg_trace_sample.ndjson
â”‚       â”‚   â””â”€â”€ reg_trace_sample.ndjson.zst
â”‚       â”œâ”€â”€ test_analyze_cli.py
â”‚       â”œâ”€â”€ test_base.py
â”‚       â”œâ”€â”€ test_cli.py
â”‚       â”œâ”€â”€ test_compression.py
â”‚       â”œâ”€â”€ test_consistency.py
â”‚       â”œâ”€â”€ test_formatters.py
â”‚       â”œâ”€â”€ test_grouper.py
â”‚       â”œâ”€â”€ test_json_validator.py
â”‚       â”œâ”€â”€ test_query_cli.py
â”‚       â”œâ”€â”€ test_query_cli_clp.py
â”‚       â”œâ”€â”€ test_reader.py
â”‚       â”œâ”€â”€ test_reduce.py
â”‚       â”œâ”€â”€ test_reduce_cli.py
â”‚       â”œâ”€â”€ test_schemas.py
â”‚       â”œâ”€â”€ test_text_validator.py
â”‚       â””â”€â”€ test_warp_summary.py
â”œâ”€â”€ readme.md
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ install_cuda.sh
â”‚   â”œâ”€â”€ killgpu.sh
â”‚   â””â”€â”€ parse_instr_hist_trace.py
â”œâ”€â”€ src
â”‚   â”œâ”€â”€ analysis.cu
â”‚   â”œâ”€â”€ cutracer.cu
â”‚   â”œâ”€â”€ delay_inject_config.cu
â”‚   â”œâ”€â”€ env_config.cu
â”‚   â”œâ”€â”€ inject_funcs.cu
â”‚   â”œâ”€â”€ instrument.cu
â”‚   â”œâ”€â”€ log.cu
â”‚   â””â”€â”€ trace_writer.cpp
â””â”€â”€ tests
    â”œâ”€â”€ datarace_test
    â”‚   â”œâ”€â”€ blackwell-fa-ws_data_race_test.py
    â”‚   â”œâ”€â”€ cutracer_reduce_test.sh
    â”‚   â””â”€â”€ hopper-gemm-ws_data_race_test.py
    â”œâ”€â”€ hang_test
    â”‚   â””â”€â”€ test_hang.py
    â”œâ”€â”€ proton_tests
    â”‚   â””â”€â”€ vector-add-instrumented.py
    â”œâ”€â”€ py_add
    â”‚   â””â”€â”€ test_add.py
    â”œâ”€â”€ unit
    â”‚   â”œâ”€â”€ Makefile
    â”‚   â””â”€â”€ test_instr_category.cpp
    â”œâ”€â”€ vectoradd
    â”‚   â”œâ”€â”€ Makefile
    â”‚   â””â”€â”€ vectoradd.cu
    â””â”€â”€ vectoradd_smem
        â”œâ”€â”€ Makefile
        â””â”€â”€ vectoradd_smem.cu

```

`CODE_OF_CONDUCT.md`:

```md
# Code of Conduct

## Our Pledge

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to make participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, sex characteristics, gender identity and expression,
level of experience, education, socio-economic status, nationality, personal
appearance, race, religion, or sexual identity and orientation.

## Our Standards

Examples of behavior that contributes to creating a positive environment
include:

* Using welcoming and inclusive language
* Being respectful of differing viewpoints and experiences
* Gracefully accepting constructive criticism
* Focusing on what is best for the community
* Showing empathy towards other community members

Examples of unacceptable behavior by participants include:

* The use of sexualized language or imagery and unwelcome sexual attention or
advances
* Trolling, insulting/derogatory comments, and personal or political attacks
* Public or private harassment
* Publishing others' private information, such as a physical or electronic
address, without explicit permission
* Other conduct which could reasonably be considered inappropriate in a
professional setting

## Our Responsibilities

Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.

Project maintainers have the right and responsibility to remove, edit, or
reject comments, commits, code, wiki edits, issues, and other contributions
that are not aligned to this Code of Conduct, or to ban temporarily or
permanently any contributor for other behaviors that they deem inappropriate,
threatening, offensive, or harmful.

## Scope

This Code of Conduct applies within all project spaces, and it also applies when
an individual is representing the project or its community in public spaces.
Examples of representing a project or community include using an official
project e-mail address, posting via an official social media account, or acting
as an appointed representative at an online or offline event. Representation of
a project may be further defined and clarified by project maintainers.

This Code of Conduct also applies outside the project spaces when there is a
reasonable belief that an individual's behavior may have a negative impact on
the project or its community.

## Enforcement

Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by contacting the project team at <opensource-conduct@meta.com>. All
complaints will be reviewed and investigated and will result in a response that
is deemed necessary and appropriate to the circumstances. The project team is
obligated to maintain confidentiality with regard to the reporter of an incident.
Further details of specific enforcement policies may be posted separately.

Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project's leadership.

## Attribution

This Code of Conduct is adapted from the [Contributor Covenant][homepage], version 1.4,
available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html

[homepage]: https://www.contributor-covenant.org

For answers to common questions about this code of conduct, see
https://www.contributor-covenant.org/faq

```

`CONTRIBUTING.md`:

```md
# Contributing to Meta Open Source Projects

We want to make contributing to this project as easy and transparent as
possible.

## Pull Requests
We actively welcome your pull requests.

Note: pull requests are not imported into the GitHub directory in the usual way. There is an internal Meta repository that is the "source of truth" for the project. The GitHub repository is generated *from* the internal Meta repository. So we don't merge GitHub PRs directly to the GitHub repository -- they must first be imported into internal Meta repository. When Meta employees look at the GitHub PR, there is a special button visible only to them that executes that import. The changes are then automatically reflected from the internal Meta repository back to GitHub. This is why you won't see your PR having being directly merged, but you still see your changes in the repository once it reflects the imported changes.

1. Fork the repo and create your branch from `main`.
2. If you've added code that should be tested, add tests.
3. If you've changed APIs, update the documentation.
4. Ensure the test suite passes.
5. Make sure your code lints.
6. If you haven't already, complete the Contributor License Agreement ("CLA").

## Contributor License Agreement ("CLA")
In order to accept your pull request, we need you to submit a CLA. You only need
to do this once to work on any of Meta's open source projects.

Complete your CLA here: <https://code.facebook.com/cla>

## Issues
We use GitHub issues to track public bugs. Please ensure your description is
clear and has sufficient instructions to be able to reproduce the issue.

Meta has a [bounty program](https://www.facebook.com/whitehat/) for the safe
disclosure of security bugs. In those cases, please go through the process
outlined on that page and do not file a public issue.

## License
By contributing to this project, you agree that your contributions will be licensed
under the LICENSE file in the root directory of this source tree.

```

`LICENSE`:

```
MIT License

Copyright (c) Meta Platforms, Inc. and affiliates.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

`LICENSE-BSD`:

```
/*
 * SPDX-FileCopyrightText: Copyright (c) 2019 NVIDIA CORPORATION & AFFILIATES.
 * All rights reserved.
 * SPDX-License-Identifier: BSD-3-Clause
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice, this
 * list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 * this list of conditions and the following disclaimer in the documentation
 * and/or other materials provided with the distribution.
 *
 * 3. Neither the name of the copyright holder nor the names of its
 * contributors may be used to endorse or promote products derived from
 * this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
 * DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
 * SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
 * CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
 * OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */
```

`Makefile`:

```
# MIT License
# Copyright (c) Meta Platforms, Inc. and affiliates.
# See LICENSE file for details.
# Project name
PROJECT := cutracer

# Compiler settings
CXX ?=
NVCC=nvcc -ccbin=$(CXX) -D_FORCE_INLINES
PTXAS=ptxas

# Version checks
NVCC_VER_REQ=10.1
NVCC_VER=$(shell $(NVCC) --version | grep release | cut -f2 -d, | cut -f3 -d' ')
NVCC_VER_CHECK=$(shell echo "${NVCC_VER} >= $(NVCC_VER_REQ)" | bc)

ifeq ($(NVCC_VER_CHECK),0)
$(error ERROR: nvcc version >= $(NVCC_VER_REQ) required to compile an nvbit tool! Instrumented applications can still use lower versions of nvcc.)
endif

PTXAS_VER_ADD_FLAG=12.3
PTXAS_VER=$(shell $(PTXAS) --version | grep release | cut -f2 -d, | cut -f3 -d' ')
PTXAS_VER_CHECK=$(shell echo "${PTXAS_VER} >= $(PTXAS_VER_ADD_FLAG)" | bc)

ifeq ($(PTXAS_VER_CHECK), 0)
MAXRREGCOUNT_FLAG=-maxrregcount=24
else
MAXRREGCOUNT_FLAG=
endif

# Debug settings
ifeq ($(DEBUG),1)
DEBUG_FLAGS := -g -O0
else
DEBUG_FLAGS := -O3 -g
endif

# Directory structure
SRC_DIR := src
OBJ_DIR := obj
LIB_DIR := lib
INCLUDE_DIR := include

# NVBIT settings
NVBIT_PATH=./third_party/nvbit/core
INCLUDES=-I$(NVBIT_PATH) -I./$(INCLUDE_DIR) -I./third_party

# Libraries
# zstd linking strategy:
#   - RHEL/CentOS/Fedora: static linking (their libzstd.a is PIC-compatible)
#   - Ubuntu/Debian/others: dynamic linking (their libzstd.a lacks -fPIC)
#   - Override with STATIC_ZSTD=1 or DYNAMIC_ZSTD=1
# Note: -lpthread is required because zstd uses POSIX threads internally

# Detect OS type from /etc/os-release
OS_ID := $(shell . /etc/os-release 2>/dev/null && echo $$ID)
OS_ID_LIKE := $(shell . /etc/os-release 2>/dev/null && echo $$ID_LIKE)
IS_RHEL_LIKE := $(if $(or $(findstring rhel,$(OS_ID) $(OS_ID_LIKE)),\
                          $(findstring centos,$(OS_ID)),\
                          $(findstring fedora,$(OS_ID)),\
                          $(findstring rocky,$(OS_ID)),\
                          $(findstring almalinux,$(OS_ID))),1,)

# Helper function to find static zstd library
define find_static_zstd
$(or $(wildcard $(shell pkg-config --variable=libdir libzstd 2>/dev/null)/libzstd.a),\
     $(if $(filter-out libzstd.a,$(shell $(CC) -print-file-name=libzstd.a 2>/dev/null)),\
          $(shell $(CC) -print-file-name=libzstd.a 2>/dev/null),))
endef

ifdef DYNAMIC_ZSTD
    # User explicitly requested dynamic linking
    ZSTD_LIB := -lzstd
else ifdef STATIC_ZSTD
    # User explicitly requested static linking
    ZSTD_LIB := $(call find_static_zstd)
    ifeq ($(ZSTD_LIB),)
        $(error ERROR: libzstd.a not found. Install with: dnf install libzstd-static (RHEL/Fedora) or apt install libzstd-dev (Ubuntu/Debian))
    endif
else ifdef IS_RHEL_LIKE
    # RHEL-like OS: default to static linking (their static lib is PIC-compatible)
    ZSTD_LIB := $(call find_static_zstd)
    ifeq ($(ZSTD_LIB),)
        $(error ERROR: libzstd.a not found. Install with: dnf install libzstd-static)
    endif
else
    # Other OS (Ubuntu, Debian, etc.): default to dynamic linking
    # Their libzstd.a is not compiled with -fPIC, so it can't be linked into a .so
    ZSTD_LIB := -lzstd
endif

LIBS=-L$(NVBIT_PATH) -lnvbit $(ZSTD_LIB) -lpthread
NVCC_PATH=-L $(subst bin/nvcc,lib64,$(shell which nvcc | tr -s /))

# Identify inject_funcs.cu specifically
INJECT_FUNCS_SRC := $(SRC_DIR)/inject_funcs.cu
INJECT_FUNCS_OBJ := $(OBJ_DIR)/inject_funcs.o

# Source files (excluding inject_funcs.cu)
CU_SRCS := $(filter-out $(INJECT_FUNCS_SRC),$(wildcard $(SRC_DIR)/*.cu))
CPP_SRCS := $(wildcard $(SRC_DIR)/*.cpp)

# Internal fb/ source files (only compiled if fb/ directory exists)
FB_SRC_DIR := $(SRC_DIR)/fb
FB_CU_SRCS := $(wildcard $(FB_SRC_DIR)/*.cu)
# Separate inject_funcs_fb.cu from other fb/ files (needs special flags)
FB_INJECT_FUNCS_SRC := $(FB_SRC_DIR)/inject_funcs_fb.cu
FB_INJECT_FUNCS_OBJ := $(if $(wildcard $(FB_INJECT_FUNCS_SRC)),$(OBJ_DIR)/fb_inject_funcs_fb.o,)
FB_REGULAR_CU_SRCS := $(filter-out $(FB_INJECT_FUNCS_SRC),$(FB_CU_SRCS))
FB_REGULAR_OBJS := $(patsubst $(FB_SRC_DIR)/%.cu,$(OBJ_DIR)/fb_%.o,$(FB_REGULAR_CU_SRCS))
FB_OBJS := $(FB_REGULAR_OBJS) $(FB_INJECT_FUNCS_OBJ)

# Object files
REGULAR_OBJS := $(patsubst $(SRC_DIR)/%.cu,$(OBJ_DIR)/%.o,$(CU_SRCS))
CPP_OBJS := $(patsubst $(SRC_DIR)/%.cpp,$(OBJ_DIR)/%.o,$(CPP_SRCS))

# All objects (regular + inject_funcs + cpp + fb)
OBJS := $(REGULAR_OBJS) $(INJECT_FUNCS_OBJ) $(CPP_OBJS) $(FB_OBJS)

# Architecture
ARCH?=all

# Output file
NVBIT_TOOL=$(LIB_DIR)/$(PROJECT).so

# Main targets
all: dirs $(NVBIT_TOOL)
	@echo ""
	@echo "âœ… Build successful! Output: $(NVBIT_TOOL)"
	@echo ""

dirs: $(OBJ_DIR) $(LIB_DIR)

$(OBJ_DIR):
	mkdir -p $@

$(LIB_DIR):
	mkdir -p $@

# Linking rule
$(NVBIT_TOOL): $(OBJS) $(NVBIT_PATH)/libnvbit.a
	$(NVCC) -arch=$(ARCH) $(DEBUG_FLAGS) $(OBJS) $(LIBS) $(NVCC_PATH) -Wno-deprecated-gpu-targets -lcuda -lcudart_static -shared -o $@

# Compilation rule for regular CUDA files (excluding inject_funcs.cu)
$(REGULAR_OBJS): $(OBJ_DIR)/%.o: $(SRC_DIR)/%.cu
	$(NVCC) -dc -c -std=c++17 $(INCLUDES) -Xptxas -cloning=no -Wno-deprecated-gpu-targets -Xcompiler -Wall -arch=$(ARCH) $(DEBUG_FLAGS) -Xcompiler -fPIC $< -o $@

# Special rule for inject_funcs.cu
$(INJECT_FUNCS_OBJ): $(INJECT_FUNCS_SRC)
	$(NVCC) $(INCLUDES) $(MAXRREGCOUNT_FLAG) -Wno-deprecated-gpu-targets -Xptxas -astoolspatch --keep-device-functions -arch=$(ARCH) -Xcompiler -Wall -Xcompiler -fPIC -c $< -o $@

# Compilation rule for C++ files
$(OBJ_DIR)/%.o: $(SRC_DIR)/%.cpp
	$(CXX) -std=c++17 $(INCLUDES) -Wall $(DEBUG_FLAGS) -fPIC -c $< -o $@

# Compilation rule for internal fb/ CUDA files (only if fb/ directory exists)
$(OBJ_DIR)/fb_%.o: $(FB_SRC_DIR)/%.cu
	$(NVCC) -dc -c -std=c++17 $(INCLUDES) -Xptxas -cloning=no -Wno-deprecated-gpu-targets -Xcompiler -Wall -arch=$(ARCH) $(DEBUG_FLAGS) -Xcompiler -fPIC $< -o $@

# Special rule for inject_funcs_fb.cu (same flags as inject_funcs.cu for NVBit device functions)
$(OBJ_DIR)/fb_inject_funcs_fb.o: $(FB_INJECT_FUNCS_SRC)
	$(NVCC) $(INCLUDES) $(MAXRREGCOUNT_FLAG) -Wno-deprecated-gpu-targets -Xptxas -astoolspatch --keep-device-functions -arch=$(ARCH) -Xcompiler -Wall -Xcompiler -fPIC -c $< -o $@

clean:
	rm -rf $(OBJ_DIR) $(LIB_DIR)

.PHONY: all clean dirs

```

`format.sh`:

```sh
#!/bin/bash
# MIT License
# Copyright (c) Meta Platforms, Inc. and affiliates.
# See LICENSE file for details.
# A script to check or apply code formatting using clang-format.
# It automatically finds all relevant source files in the project.

# --- Go to script's directory ---
# This ensures that the script can be called from any location and that all
# subsequent paths are relative to the project root.
SCRIPT_DIR=$(cd -- "$(dirname -- "${BASH_SOURCE[0]}")" &>/dev/null && pwd)
cd -- "$SCRIPT_DIR" || exit

# --- Check for dependencies ---
CLANG_FORMAT_VERSION="21.1.2"

if ! command -v clang-format &>/dev/null; then
  echo "âŒ Error: clang-format is not installed or not in your PATH." >&2
  echo "Please install clang-format v${CLANG_FORMAT_VERSION} (must match CI version):" >&2
  echo "  - Recommended: pip install clang-format==${CLANG_FORMAT_VERSION}" >&2
  echo "  - On Debian/Ubuntu: sudo apt install clang-format (âš ï¸ version may differ)" >&2
  echo "  - On Fedora/CentOS: sudo dnf install clang-tools-extra (âš ï¸ version may differ)" >&2
  echo "  - On macOS (Homebrew): brew install clang-format (âš ï¸ version may differ)" >&2
  exit 1
fi

# --- Configuration ---
# Define the directories to be processed.
# Add or remove directories as needed for your project.
DIRECTORIES_TO_SCAN=(src include tests tutorial .ci)

# Define the file extensions to be processed.
# Add or remove extensions as needed for your project.
FILE_PATTERNS=(-name "*.h" -o -name "*.hpp" -o -name "*.cpp" -o -name "*.cu" -o -name "*.cuh")

# (Optional) Python formatting configuration can be supplied via pyproject.toml

# --- Helper Functions ---

# Function to process a single file.
# It checks if a file needs formatting, formats it if necessary,
# and prints the filename to stdout if it was changed.
format_and_report_changes() {
  file="$1"
  # Use diff to compare the original file with clang-format's output.
  # If they differ, format the file in-place and print its name.
  if ! diff -q "${file}" <(clang-format "${file}") >/dev/null; then
    clang-format -i "${file}"
    echo "${file}"
  fi
}
# Export the function so it's available to the subshells created by xargs.
export -f format_and_report_changes

# Python checks/format: ufmt (sorter + formatter from pyproject.toml) -> ruff linting
python_check() {
  local failed=0

  if command -v ufmt &>/dev/null; then
    ufmt check .
    [ $? -eq 0 ] || failed=1
  else
    echo "âŒ ufmt not found (required for Python formatting)." >&2
    failed=1
  fi

  if command -v ruff &>/dev/null; then
    ruff check . --diff
    [ $? -eq 0 ] || failed=1
  else
    echo "âŒ ruff not found (required for Python linting)." >&2
    failed=1
  fi

  return $failed
}

python_format() {
  if command -v ufmt &>/dev/null; then
    echo "ğŸ¨  Formatting Python code with ufmt..."
    ufmt format .
  else
    echo "âš ï¸  ufmt not found; skipping formatting." >&2
  fi

  if command -v ruff &>/dev/null; then
    echo "ğŸ”§  Fixing Python linting issues with ruff..."
    ruff check . --fix
  else
    echo "âš ï¸  ruff not found; skipping lint fixes." >&2
  fi
}

# Function to print usage instructions
usage() {
  echo "Usage: $0 {check|format}"
  echo "  check      Check for formatting issues without modifying files."
  echo "             Exits with an error code if issues are found."
  echo "  format     Apply formatting to files in-place."
  exit 1
}

# --- Main Script Logic ---

# Check if an argument was provided
if [ "$#" -ne 1 ]; then
  echo "Error: No mode specified."
  usage
fi

MODE=$1

# Filter for directories that actually exist to prevent 'find' errors.
EXISTING_DIRS=()
for dir in "${DIRECTORIES_TO_SCAN[@]}"; do
  if [ -d "$dir" ]; then
    EXISTING_DIRS+=("$dir")
  fi
done

if [ ${#EXISTING_DIRS[@]} -eq 0 ]; then
  echo "â©  No source directories found to process. Searched for: ${DIRECTORIES_TO_SCAN[*]}. Exiting."
  exit 0
fi

# We pipe the output of find directly to xargs.
# This avoids issues with storing null-delimited strings in shell variables.
# The -r flag for xargs prevents it from running the command if find returns no files.

case "$MODE" in
check)
  echo "ğŸ”  Checking code formatting in: ${EXISTING_DIRS[*]}..."

  # Step 1: C/C++ check with --Werror to get a reliable exit code
  find "${EXISTING_DIRS[@]}" -type f \( "${FILE_PATTERNS[@]}" \) -print0 | xargs -0 -r clang-format --dry-run --Werror
  CXX_STATUS=$?

  # Step 2: Python check using ufmt or black/usort
  python_check
  PY_STATUS=$?

  if [ $CXX_STATUS -ne 0 ] || [ $PY_STATUS -ne 0 ]; then
    echo "----------------------------------------------------"
    echo "Please run './format.sh format' to fix them."
    exit 1
  fi

  echo "âœ…  All files are correctly formatted (or no files were found to check)."
  exit 0
  ;;

format)
  echo "ğŸ¨  Applying code formatting to: ${EXISTING_DIRS[*]}..."

  # Use xargs to run the formatting function in parallel.
  # -P 0 tells xargs to use as many processes as available CPU cores.
  # The output will be a list of files that were actually changed.
  CHANGED_FILES=$(find "${EXISTING_DIRS[@]}" -type f \( "${FILE_PATTERNS[@]}" \) -print0 | \
    xargs -0 -P 0 -I {} bash -c 'format_and_report_changes "{}"')

  if [ -n "$CHANGED_FILES" ]; then
    echo "âœ¨ Changed files:"
    # Use printf to format the list of changed files neatly.
    printf "  - %s\n" $CHANGED_FILES
  else
    echo "No files needed formatting."
  fi

  # Python formatting
  python_format

  echo "âœ…  Formatting complete."
  ;;

*)
  echo "Error: Invalid mode '$MODE'."
  usage
  ;;
esac

exit 0

```

`include/analysis.h`:

```h
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 *
 * See LICENSE file in the root directory for Meta's license terms.
 */

#ifndef ANALYSIS_H
#define ANALYSIS_H
#include <cstdint>
#include <ctime>
#include <deque>
#include <map>
#include <mutex>
#include <nlohmann/json.hpp>
#include <set>
#include <shared_mutex>
#include <sstream>
#include <string>
#include <unordered_map>
#include <unordered_set>
#include <vector>

#include "common.h"
#include "instr_category.h"
#include "nvbit.h"
/* for channel */
#include "utils/channel.hpp"

// Forward declaration for TraceWriter (defined in trace_writer.h)
class TraceWriter;

/* Channel buffer size - increased for mem_value_trace support */
#define CHANNEL_SIZE (1l << 22)  // 4MB

/* Thread state enum */
enum class RecvThreadState {
  WORKING,
  STOP,
  FINISHED,
};

/* ===== Data Structures ===== */

/* Structure to uniquely identify a warp */
struct WarpKey {
  int cta_id_x;
  int cta_id_y;
  int cta_id_z;
  // global warp id
  int warp_id;

  // Operator for map comparison
  bool operator<(const WarpKey& other) const {
    if (cta_id_x != other.cta_id_x) return cta_id_x < other.cta_id_x;
    if (cta_id_y != other.cta_id_y) return cta_id_y < other.cta_id_y;
    if (cta_id_z != other.cta_id_z) return cta_id_z < other.cta_id_z;
    return warp_id < other.warp_id;
  }

  // Hash function for unordered_map
  struct Hash {
    size_t operator()(const WarpKey& k) const {
      return (size_t)k.cta_id_x ^ ((size_t)k.cta_id_y << 10) ^ ((size_t)k.cta_id_z << 20) ^ ((size_t)k.warp_id << 30);
    }
  };

  // Equality operator for unordered_map
  bool operator==(const WarpKey& other) const {
    return cta_id_x == other.cta_id_x && cta_id_y == other.cta_id_y && cta_id_z == other.cta_id_z &&
           warp_id == other.warp_id;
  }
};

// Merged trace record containing mandatory reg trace and optional mem trace
struct TraceRecordMerged {
  reg_info_t reg;
  bool has_mem = false;
  uint64_t mem_addrs[32] = {0};
};

// Structure to track the loop state of a warp
struct WarpLoopState {
  // Circular buffer of recent merged trace records
  std::vector<TraceRecordMerged> history;
  uint8_t head;    // Next write position in circular buffer
  uint8_t filled;  // Number of valid entries written, capped at buffer size
  uint64_t last_sig;
  uint8_t last_period;
  uint32_t repeat_cnt;
  bool loop_flag;
  time_t first_loop_time;

  // Structure to hold complete loop information
  struct LoopInfo {
    std::vector<TraceRecordMerged> instructions;  // Copy of one canonical period
    uint8_t period;
  };

  LoopInfo current_loop;

  WarpLoopState()
      : head(0), filled(0), last_sig(0), last_period(0), repeat_cnt(0), loop_flag(false), first_loop_time(0) {
  }
};

/**
 * @brief Represents the state of a single warp during instruction histogram
 * analysis.
 *
 * This structure tracks whether a warp is currently in a region of interest
 * for collection and stores the histogram data for that region.
 */
struct WarpState {
  /**
   * @brief A flag indicating whether instruction collection is active for this
   * warp.
   *
   * This acts as a switch, turned on by a "start" clock instruction and off
   * by an "end" clock instruction.
   */
  bool is_collecting = false;
  /**
   * @brief A counter for the number of regions analyzed for this warp.
   *
   * This helps in uniquely identifying each region within a warp's execution.
   */
  int region_counter = 0;
  /**
   * @brief The histogram of instructions collected for the current region.
   *
   * Maps an instruction name (string) to its execution count (int).
   */
  std::map<std::string, int> histogram;
};

/**
 * @brief Stores the completed instruction histogram for a specific region of a
 * warp.
 */
struct RegionHistogram {
  /**
   * @brief The ID of the warp.
   */
  int warp_id;
  /**
   * @brief The ID of the region within the warp.
   */
  int region_id;
  /**
   * @brief The completed histogram for this region.
   */
  std::map<std::string, int> histogram;
};

/**
 * @brief Grid and block dimensions for a kernel launch.
 */
struct KernelDimensions {
  unsigned int gridDimX;
  unsigned int gridDimY;
  unsigned int gridDimZ;
  unsigned int blockDimX;
  unsigned int blockDimY;
  unsigned int blockDimZ;
};

/**
 * @brief Per-function static metadata collected once during instrumentation.
 *
 * Aggregates all per-function attributes that do not change across launches,
 * eliminating the need to re-query CUDA driver APIs on every launch.
 */
struct KernelFuncMetadata {
  std::string mangled_name;
  std::string unmangled_name;
  std::string kernel_checksum;  // FNV-1a hash hex string
  std::string cubin_path;       // Only set when dump_cubin is enabled
  uint64_t func_addr = 0;       // nvbit_get_func_addr()
  int nregs = 0;                // CU_FUNC_ATTRIBUTE_NUM_REGS
  int shmem_static_nbytes = 0;  // CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES

  /// Serialize per-function static attributes to JSON.
  nlohmann::json to_json() const {
    nlohmann::json j;
    j["mangled_name"] = mangled_name;
    j["unmangled_name"] = unmangled_name;
    j["kernel_checksum"] = kernel_checksum;
    std::ostringstream oss;
    oss << "0x" << std::hex << func_addr;
    j["func_addr"] = oss.str();
    j["nregs"] = nregs;
    j["shmem_static"] = shmem_static_nbytes;
    if (!cubin_path.empty()) {
      j["cubin_path"] = cubin_path;
    }
    return j;
  }
};

/**
 * @brief Tracks warp statistics for a single kernel launch.
 *
 * This structure maintains complete information about all warps in a kernel:
 * - Total number of warps (calculated from grid/block dimensions)
 * - All warps ever seen executing
 * - Warps that have finished execution
 * - Currently active warps (maintained elsewhere in CTXstate)
 */
struct KernelWarpStats {
  // Total number of warps in this kernel launch
  uint32_t total_warps;

  // Grid and block dimensions
  KernelDimensions dimensions;

  // All warps that have ever been observed executing
  std::unordered_set<WarpKey, WarpKey::Hash> all_seen_warps;

  // Warps that were once active but have finished
  std::unordered_set<WarpKey, WarpKey::Hash> finished_warps;

  KernelWarpStats() : total_warps(0) {
  }
};

/**
 * @brief Stores the completed instruction histogram for a specific region of a
 * warp.
 */
struct CTXstate {
  /* context id */
  int id;

  /* Channel used to communicate from GPU to CPU receiving thread */
  ChannelDev* channel_dev;
  ChannelHost channel_host;

  // After initialization, set it to WORKING to make recv thread get data,
  // parent thread sets it to STOP to make recv thread stop working.
  // recv thread sets it to FINISHED when it cleans up.
  // parent thread should wait until the state becomes FINISHED to clean up.
  volatile RecvThreadState recv_thread_done = RecvThreadState::STOP;

  // Per-function SASS mappings for instruction histogram feature
  std::unordered_map<CUfunction, std::map<int, std::string>> id_to_sass_map;
  // Per-function register indices mapping (static data, collected at instrumentation time)
  std::unordered_map<CUfunction, std::map<int, RegIndices>> id_to_reg_indices_map;
  std::unordered_map<CUfunction, std::unordered_set<int>> clock_opcode_ids;
  // Per-function EXIT opcode ids (statically identified at instrumentation time)
  std::unordered_map<CUfunction, std::unordered_set<int>> exit_opcode_ids;

  /* State for Deadlock/Hang Detection */
  std::map<WarpKey, WarpLoopState> loop_states;
  std::set<WarpKey> active_warps;
  time_t last_hang_check_time;

  // Pending mem traces per warp for out-of-order arrival (mem before reg)
  std::unordered_map<WarpKey, std::deque<mem_addr_access_t>, WarpKey::Hash> pending_mem_by_warp;

  // Per-warp activity timestamps for inactive cleanup
  std::unordered_map<WarpKey, time_t, WarpKey::Hash> last_seen_time_by_warp;
  std::unordered_map<WarpKey, time_t, WarpKey::Hash> exit_candidate_since_by_warp;

  // Per-warp last observed state: whether last instruction was BAR.SYNC.DEFER_BLOCKING
  std::unordered_map<WarpKey, bool, WarpKey::Hash> last_is_defer_blocking_by_warp;

  // Deadlock handling
  int deadlock_consecutive_hits = 0;
  bool deadlock_termination_initiated = false;

  // Warp statistics tracking per kernel launch
  std::unordered_map<uint64_t, KernelWarpStats> kernel_warp_tracking;

  // Per-launch TraceWriter map for JSON output (mode 1/2)
  // Maps kernel_launch_id -> TraceWriter*
  // Each kernel launch gets its own trace file
  std::map<uint64_t, TraceWriter*> trace_writers;
  mutable std::shared_mutex writers_mutex;

  // Per-kernel trace index counter (monotonically increasing)
  // Maps kernel_launch_id -> current trace_index
  std::unordered_map<uint64_t, uint64_t> trace_index_by_kernel;
};

// =================================================================================
// Function Declarations
// =================================================================================

/**
 * @brief The main thread function for receiving and processing data from the
 * GPU.
 *
 * This function runs in a separate CPU thread, continuously receiving data
 * packets (like `reg_info_t`, `mem_access_t`, `opcode_only_t`) from the GPU
 * channel and dispatching them for analysis.
 *
 * @param args A pointer to the `CUcontext` for which this thread is launched.
 * @return void*
 */
void* recv_thread_fun(void* args);

/**
 * @brief Writes a set of histograms to a formatted CSV file.
 *
 * @param ctx The CUDA context.
 * @param func The kernel function.
 * @param iteration The iteration number of the kernel launch.
 * @param histograms The histogram data to be written.
 */
void dump_histograms_to_csv(CUcontext ctx, CUfunction func, uint32_t iteration,
                            const std::vector<RegionHistogram>& histograms);

#endif /* ANALYSIS_H */

```

`include/common.h`:

```h
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-FileCopyrightText: Copyright (c) 2019 NVIDIA CORPORATION & AFFILIATES.
 * SPDX-License-Identifier: MIT AND BSD-3-Clause
 *
 * This source code contains modifications by Meta Platforms, Inc. licensed under MIT,
 * based on original NVIDIA nvbit sample code licensed under BSD-3-Clause.
 * See LICENSE file in the root directory for Meta's license terms.
 * See LICENSE-BSD file in the root directory for NVIDIA's license terms.
 */

#ifndef COMMON_H
#define COMMON_H

#include <stdint.h>

/* Maximum number of register operands tracked per instruction.
 * This is a configurable buffer limit, NOT a hardware constraint.
 * Increase if instructions with more operands need to be traced. */
#define MAX_REG_OPERANDS 16
#define MAX_UREG_OPERANDS 16

/* Message type enum to identify different message types */
typedef enum {
  MSG_TYPE_REG_INFO = 0,
  MSG_TYPE_MEM_ADDR_ACCESS = 1,
  MSG_TYPE_OPCODE_ONLY = 2,
  MSG_TYPE_MEM_VALUE_ACCESS = 3,  // Memory access with value tracing
  MSG_TYPE_TMA_ACCESS = 4         // TMA (Tensor Memory Accelerator) access
} message_type_t;

/* Common header for all message types */
typedef struct {
  message_type_t type;  // Type of the message
} message_header_t;

/* Based on NVIDIA record_reg_vals example with Meta modifications for message type support and adds CUTracer specific
 * extensions */
typedef struct {
  message_header_t header;  // Common header with type=MSG_TYPE_REG_INFO
  int32_t cta_id_x;
  int32_t cta_id_y;
  int32_t cta_id_z;
  int32_t warp_id;
  int32_t opcode_id;
  int32_t num_regs;
  /* 32 lanes, each thread can store up to MAX_REG_OPERANDS register values */
  uint32_t reg_vals[32][MAX_REG_OPERANDS];

  // CUTracer extensions
  uint64_t kernel_launch_id;              // Global kernel launch id
  uint64_t pc;                            // Program counter for the instruction
  int32_t num_uregs;                      // Number of unified registers
  uint32_t ureg_vals[MAX_UREG_OPERANDS];  // Unified registers shared by all threads in the same warp
} reg_info_t;

/* Based on NVIDIA mem_trace example with Meta modifications for message type support */
typedef struct {
  message_header_t header;  // Common header with type=MSG_TYPE_MEM_ADDR_ACCESS
  uint64_t kernel_launch_id;
  int cta_id_x;
  int cta_id_y;
  int cta_id_z;
  uint64_t pc;
  int warp_id;
  int opcode_id;
  uint64_t addrs[32];
} mem_addr_access_t;

/**
 * @brief Memory access with value tracing structure.
 *
 * This structure captures both memory addresses AND values for detailed
 * data flow analysis. It is larger than mem_addr_access_t (~820 bytes vs ~300 bytes)
 * due to the values array.
 *
 * Used when CUTRACER_INSTRUMENT=mem_value_trace is enabled.
 * Always captured at IPOINT_AFTER for consistent timing semantics.
 */
typedef struct {
  message_header_t header;  // type=MSG_TYPE_MEM_VALUE_ACCESS
  uint64_t kernel_launch_id;
  int cta_id_x;
  int cta_id_y;
  int cta_id_z;
  uint64_t pc;
  int warp_id;
  int opcode_id;
  int mem_space;           // Memory space: GLOBAL=1, SHARED=4, LOCAL=5 (matches InstrType::MemorySpace)
  int is_load;             // 1=load, 0=store
  int access_size;         // Access size in bytes (1, 2, 4, 8, 16)
  uint64_t addrs[32];      // Memory addresses for each lane
  uint32_t values[32][4];  // Values: [lane][reg_idx], max 128-bit (4x32-bit) per lane
} mem_value_access_t;

/**
 * @brief A lightweight data packet for instruction histogram analysis.
 *
 * This structure is sent from the GPU to the CPU when `OPCODE_ONLY`
 * instrumentation is enabled. It contains the minimal information required
 * to identify an instruction and its execution context without the overhead
 * of register or memory data.
 */
typedef struct {
  message_header_t header;  // Common header with type=MSG_TYPE_OPCODE_ONLY
  uint64_t kernel_launch_id;
  int cta_id_x;
  int cta_id_y;
  int cta_id_z;
  uint64_t pc;
  int warp_id;
  int opcode_id;
} opcode_only_t;

/* Host-only C++ structures */
#ifdef __cplusplus
#include <cstdint>
#include <vector>

/**
 * @brief Register indices for CPU-side static mapping.
 *
 * Maps reg_vals/ureg_vals array positions to actual register numbers.
 * Collected at instrumentation time, not transmitted over GPU channel.
 * This avoids runtime overhead and buffer size increase for static data.
 */
struct RegIndices {
  std::vector<uint8_t> reg_indices;   // R register numbers: 0-254 (R0-R254)
  std::vector<uint8_t> ureg_indices;  // UR register numbers: 0-62 (UR0-UR62)
};

#endif /* __cplusplus */

/**
 * @brief TMA (Tensor Memory Accelerator) access tracing structure.
 *
 * This structure captures TMA descriptor information for UTMALDG.2D (load)
 * and UTMASTG.2D (store) instructions. It reads the 128-byte TMA descriptor
 * from the address pointed to by the uniform register operand.
 *
 * The TMA descriptor contains tensor metadata for bulk async memory transfers
 * between global and shared memory on Hopper/Blackwell GPUs.
 *
 * Enabled via CUTRACER_INSTR_CATEGORIES=TMA.
 *
 */
typedef struct {
  message_header_t header;  // type=MSG_TYPE_TMA_ACCESS
  uint64_t kernel_launch_id;
  int cta_id_x;
  int cta_id_y;
  int cta_id_z;
  uint64_t pc;
  int warp_id;
  int opcode_id;

  // TMA descriptor address
  uint64_t desc_addr;

  // Raw TMA descriptor
  uint64_t desc_raw[16];
} tma_access_t;

#endif /* COMMON_H */

```

`include/delay_inject_config.h`:

```h
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 * See LICENSE file in the root directory for Meta's license terms.
 */

#ifndef DELAY_INJECT_CONFIG_H
#define DELAY_INJECT_CONFIG_H

#include <stdint.h>

#include <map>
#include <string>

#include "nvbit.h"

/**
 * @brief Represents a single instrumentation point for delay injection.
 *
 * Each instrumentation point captures:
 * - pc_offset: Program counter offset for the instruction (used as key)
 * - sass: The SASS assembly instruction string
 * - delay_ns: The delay value in nanoseconds
 * - enabled: Whether this point is enabled (randomly set, 50% probability)
 */
struct DelayInstrumentationPoint {
  uint64_t pc_offset;
  std::string sass;
  uint32_t delay_ns;
  bool enabled;
};

/**
 * @brief Configuration for delay instrumentation points in a single kernel.
 */
struct KernelDelayInjectConfig {
  std::string kernel_name;
  // FNV-1a hash of kernel name + all SASS instructions (hex string).
  // This provides robust kernel identification across recompilations:
  // same kernel name with different SASS produces different checksum.
  std::string kernel_checksum;
  std::string timestamp;  // ISO 8601 timestamp when kernel was instrumented
  std::map<uint64_t, DelayInstrumentationPoint> instrumentation_points;  // Keyed by pc_offset
};

/**
 * @brief Master configuration containing all kernel delay configs.
 */
struct DelayInjectConfig {
  std::string version = "1.0";
  uint32_t delay_ns;
  std::map<std::string, KernelDelayInjectConfig> kernels;  // Indexed by kernel name

  bool save_to_file(const std::string& filepath) const;
};

// Global delay injection configuration
extern DelayInjectConfig g_delay_inject_config;

/**
 * @brief Initialize delay JSON configuration for export.
 *
 * Sets up the global delay config with the delay value from env.
 * This is used for exporting instrumentation points to JSON for replay.
 */
void init_delay_json_config();

/**
 * @brief Create a new kernel delay config for a given kernel name.
 *
 * Should be called once per kernel before the instruction iteration loop.
 *
 * @param kernel_name The kernel name
 * @param kernel_checksum FNV-1a hash of kernel name + SASS instructions (hex string) for robust identification
 * @return Pointer to the newly created kernel config
 */
KernelDelayInjectConfig* create_kernel_delay_config(const std::string& kernel_name, const std::string& kernel_checksum);

/**
 * @brief Register an instrumentation point for a kernel.
 *
 * Creates and registers a DelayInstrumentationPoint from the instruction.
 *
 * @param kdc Pointer to the kernel delay config (from create_kernel_delay_config)
 * @param instr The NVBit instruction to register
 * @param delay_ns The delay value in nanoseconds
 * @param enabled Whether this point is enabled
 */
void register_delay_instrumentation_point(KernelDelayInjectConfig* kdc, Instr* instr, uint32_t delay_ns, bool enabled);

/**
 * @brief Finalize and save delay config at context termination.
 */
void finalize_delay_config();

/**
 * @brief Load delay configuration from a JSON file for replay mode.
 *
 * @param filepath Path to the JSON config file
 * @return true if loading succeeded, false otherwise
 */
bool load_delay_config(const std::string& filepath);

/**
 * @brief Check if delay replay mode is active.
 *
 * Delay replay mode uses a previously saved delay config file to deterministically
 * reproduce the same instrumentation pattern.
 *
 * @return true if delay replay mode is active, false otherwise
 */
bool is_delay_replay_mode();

/**
 * @brief Look up an instrumentation point configuration for replay.
 *
 * @param replay_points Pointer to the instrumentation points map (from get_replay_instrumentation_points)
 * @param pc_offset The program counter offset of the instruction
 * @param[out] enabled Output: whether the point is enabled
 * @param[out] delay_ns Output: the delay value in nanoseconds
 * @return true if found, false if not found
 */
bool lookup_replay_config(const std::map<uint64_t, DelayInstrumentationPoint>* replay_points, uint64_t pc_offset,
                          bool& enabled, uint32_t& delay_ns);

/**
 * @brief Get the instrumentation points map for a kernel in replay mode.
 *
 * Should be called once per kernel before the instruction iteration loop.
 * Matches kernels by kernel_checksum (computed from kernel name + SASS) for robust identification.
 *
 * @param kernel_name The kernel name to look up
 * @param kernel_checksum The kernel checksum to match (FNV-1a hash of kernel name + SASS)
 * @return Pointer to the instrumentation points map, or nullptr if not found
 */
const std::map<uint64_t, DelayInstrumentationPoint>* get_replay_instrumentation_points(
    const std::string& kernel_name, const std::string& kernel_checksum);

#endif /* DELAY_INJECT_CONFIG_H */

```

`include/env_config.h`:

```h
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 *
 * See LICENSE file in the root directory for Meta's license terms.
 */

#pragma once

#include <stdint.h>

#include <string>
#include <unordered_set>
#include <vector>
// Forward declaration to avoid circular dependency
enum class InstrumentType;
enum class InstrCategory;

/**
 * @brief Defines the type of analysis to be performed on the collected trace
 * data.
 */
enum class AnalysisType {
  /**
   * @brief No analysis is performed.
   */
  ANALYSIS_NONE = 0,
  /**
   * @brief Enables instruction histogram analysis.
   *
   * This corresponds to the `proton_instr_histogram` setting and requires
   * `OPCODE_ONLY` instrumentation.
   */
  PROTON_INSTR_HISTOGRAM = 1,

  /**
   * @brief Enables deadlock detection analysis.
   */
  DEADLOCK_DETECTION = 2,

  /**
   * @brief Enables random delay injection for data race detection.
   */
  RANDOM_DELAY = 3,
};

// Configuration variables
extern uint32_t instr_begin_interval;
extern uint32_t instr_end_interval;
extern int verbose;
extern bool dump_cubin;

// Kernel name filters
extern std::vector<std::string> kernel_filters;
// Instrumentation configuration
extern std::unordered_set<InstrumentType> enabled_instrument_types;

// Analysis configuration
extern std::unordered_set<AnalysisType> enabled_analysis_types;

// Initialize configuration from environment variables
void init_config_from_env();

// Check if a specific instrumentation type is enabled
bool is_instrument_type_enabled(InstrumentType type);

// Check if any instrumentation type is enabled
bool has_any_instrumentation_enabled();

// Check if a specific analysis type is enabled
bool is_analysis_type_enabled(AnalysisType type);

// Initialize instrumentation configuration
void init_instrumentation(const std::string& instrument_str);

// Initialize analysis configuration
void init_analysis(const std::string& analysis_str);

// Trace format configuration
// 0 = text format (default)
// 1 = NDJSON+Zstd (compressed JSON)
// 2 = NDJSON only (uncompressed JSON, good for debugging)
extern int trace_format_ndjson;

// Zstd compression level (1-22, higher = better compression but slower)
// Default: 9 (good compression with reasonable speed)
extern int zstd_compression_level;

// Delay value in nanoseconds for random delay instrumentation
extern uint32_t g_delay_ns;

// Delay dump output path (optional)
// If set, instrumentation points will be written to this JSON file for later replay
extern std::string delay_dump_path;

// Delay load path (optional)
// If set, instrumentation points will be read from this JSON file for replay mode
extern std::string delay_load_path;

// Trace output directory for dumping trace files (optional)
// When set, trace files will be written to this directory instead of the current directory
// Set via CUTRACER_TRACE_OUTPUT_DIR environment variable
extern std::string trace_output_dir;

// Instruction category filtering for conditional instrumentation
// If empty, all instructions are instrumented
// If set, only instructions in the specified categories are instrumented
extern std::unordered_set<InstrCategory> enabled_instr_categories;

// Initialize instruction category filtering from environment variable
void init_instr_categories(const std::string& categories_str);

// Check if a specific instruction category should be instrumented
bool should_instrument_category(InstrCategory category);

// Check if category-based filtering is enabled
bool has_category_filter_enabled();

```

`include/implicit_regs.h`:

```h
/*
 * MIT License
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * Implicit Registers Collection for SASS Instructions
 *
 * NVBit's operand API only exposes registers explicitly written in assembly.
 * Some instructions implicitly use additional sequential registers based on
 * their semantics (e.g., UTMALDG uses URb+1 for barrier address).
 *
 * This module augments the operand list with these implicit registers
 * for more accurate tracing.
 */

#pragma once

#include <vector>

#include "common.h"

/**
 * @brief Context capturing operand information for implicit register collection.
 *
 * Stores operand data by type, preserving order of appearance.
 * Each vector index corresponds to the nth operand of that type.
 */
struct OperandContext {
  std::vector<int> mref_urs;              // UR numbers from MREF operands (in order)
  std::vector<int> mref_ras;              // RA numbers from MREF operands (in order)
  std::vector<int> desc_urs;              // UR numbers from MEM_DESC operands (in order)
  std::vector<int> generic_urs;           // UR numbers from GENERIC operands (in order)
  std::vector<std::string> generic_strs;  // Raw GENERIC operand strings (e.g., "gdesc[UR44]", "tmem[UR53]")
};

// Try to load internal implementation first
#if __has_include("fb/implicit_regs_fb.h")
#include "fb/implicit_regs_fb.h"
#define IMPLICIT_REGS_IMPL_DEFINED
#endif

// Fallback: no-op for OSS builds
#ifndef IMPLICIT_REGS_IMPL_DEFINED
/**
 * @brief Collect implicit registers for SASS instructions (OSS stub).
 *
 * @param sass      SASS instruction string (used to identify instruction type)
 * @param ctx       Operand context with all explicit operand info
 * @param operands  OperandLists to augment (modified in-place)
 *
 * In OSS builds, this is a no-op. Internal builds provide instruction-specific
 * logic.
 */
inline void collect_implicit_regs(const char* /*sass*/, const OperandContext& /*ctx*/, OperandLists& /*operands*/) {
  // No-op in OSS builds
}
#endif

```

`include/instr_category.h`:

```h
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 * See LICENSE file in the root directory for Meta's license terms.
 */

#ifndef INSTR_CATEGORY_H
#define INSTR_CATEGORY_H

#include <string>
#include <unordered_map>
#include <unordered_set>
#include <vector>

/**
 * @brief Instruction categories for classification and tracing.
 *
 * This enum defines logical categories of SASS instructions that can be
 * detected and traced. Each category groups related instructions together
 * for easier analysis and extension.
 *
 * To add a new category:
 * 1. Add a new enum value here
 * 2. Add pattern(s) to INSTR_CATEGORY_PATTERNS below
 * 3. The detection and logging will happen automatically
 */
enum class InstrCategory {
  NONE = 0,  // No category / unknown instruction

  // Matrix Multiply-Accumulate instructions
  MMA,

  // Tensor Memory Access instructions
  TMA,

  // Synchronization instructions
  SYNC,

  // Add new categories here as needed
  // Example: LDST, ALU, CONTROL, etc.
};

/**
 * @brief Get a human-readable name for an instruction category.
 */
inline const char* get_instr_category_name(InstrCategory cat) {
  switch (cat) {
    case InstrCategory::NONE:
      return "NONE";
    case InstrCategory::MMA:
      return "MMA";
    case InstrCategory::TMA:
      return "TMA";
    case InstrCategory::SYNC:
      return "SYNC";
    default:
      return "UNKNOWN";
  }
}

/**
 * @brief Pattern definition for instruction category matching.
 */
struct InstrCategoryPattern {
  const char* pattern;      // SASS substring to match
  InstrCategory category;   // Category this pattern belongs to
  const char* description;  // Human-readable description
};

/**
 * @brief Instruction category patterns.
 *
 * Each entry maps a SASS instruction pattern to a category.
 * Patterns are matched using substring search (strstr).
 *
 * To add support for new instructions:
 * - Add a new entry with the SASS pattern, category, and description
 * - The pattern should be specific enough to avoid false matches
 */
static const std::vector<InstrCategoryPattern> INSTR_CATEGORY_PATTERNS = {
    // MMA (Matrix Multiply-Accumulate) instructions
    // Hopper GMMA
    {"HGMMA", InstrCategory::MMA, "Hopper GMMA (sm_90)"},
    // Blackwell UTC*MMA variants (sm_100+)
    {"UTCHMMA", InstrCategory::MMA, "Blackwell UTCHMMA"},
    {"UTCIMMA", InstrCategory::MMA, "Blackwell UTCIMMA"},
    {"UTCQMMA", InstrCategory::MMA, "Blackwell UTCQMMA"},
    {"UTCOMMA", InstrCategory::MMA, "Blackwell UTCOMMA"},

    // TMA (Tensor Memory Access) instructions
    {"UTMALDG", InstrCategory::TMA, "Unified TMA Load Global"},
    {"UTMASTG", InstrCategory::TMA, "Unified TMA Store Global"},

    // SYNC (Synchronization) instructions
    {"WARPGROUP.DEPBAR", InstrCategory::SYNC, "Warpgroup dependency barrier"},
    // Add more SYNC patterns here as needed
};

/**
 * @brief Detect the category of an instruction from its SASS string.
 *
 * @param sass The SASS instruction string
 * @return The detected category, or InstrCategory::NONE if no match
 */
inline InstrCategory detect_instr_category(const char* sass) {
  if (sass == nullptr) {
    return InstrCategory::NONE;
  }

  for (const auto& entry : INSTR_CATEGORY_PATTERNS) {
    if (strstr(sass, entry.pattern) != nullptr) {
      return entry.category;
    }
  }

  return InstrCategory::NONE;
}

/**
 * @brief Get the pattern description for a matched instruction.
 *
 * @param sass The SASS instruction string
 * @return The description of the matched pattern, or nullptr if no match
 */
inline const char* get_instr_pattern_description(const char* sass) {
  if (sass == nullptr) {
    return nullptr;
  }

  for (const auto& entry : INSTR_CATEGORY_PATTERNS) {
    if (strstr(sass, entry.pattern) != nullptr) {
      return entry.description;
    }
  }

  return nullptr;
}

/**
 * @brief Check if an instruction belongs to a specific category.
 *
 * @param sass The SASS instruction string
 * @param category The category to check for
 * @return true if the instruction belongs to the category
 */
inline bool is_instr_category(const char* sass, InstrCategory category) {
  return detect_instr_category(sass) == category;
}

/**
 * @brief Get all patterns for a specific category.
 *
 * @param category The category to get patterns for
 * @return Vector of pattern strings for the category
 */
inline std::vector<const char*> get_patterns_for_category(InstrCategory category) {
  std::vector<const char*> patterns;
  for (const auto& entry : INSTR_CATEGORY_PATTERNS) {
    if (entry.category == category) {
      patterns.push_back(entry.pattern);
    }
  }
  return patterns;
}

#endif /* INSTR_CATEGORY_H */

```

`include/instrument.h`:

```h
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 * See LICENSE file in the root directory for Meta's license terms.
 */

#ifndef INSTRUMENT_H
#define INSTRUMENT_H

#include <vector>

#include "analysis.h"

/**
 * @brief Instrumentation types for different data collection modes
 */
enum class InstrumentType {
  OPCODE_ONLY,      // Lightweight: only collect opcode information
  REG_TRACE,        // Medium: collect register values
  MEM_ADDR_TRACE,   // Heavy: collect memory access information (address only)
  MEM_VALUE_TRACE,  // Heavy: collect memory access with values
  TMA_TRACE,        // TMA descriptor tracing for Tensor Memory Accelerator instructions
  RANDOM_DELAY,     // Inject random delays on synchronization instructions
};

/**
 * @brief Structure to hold operand information for instrumentation
 *
 * This structure groups operand data needed for instrumentation,
 * making it easy to extend support for additional operand types without
 * changing function signatures.
 */
struct OperandLists {
  std::vector<int> reg_nums;   // Regular register numbers
  std::vector<int> ureg_nums;  // Uniform register numbers
  // Future: add support for other types like pred_nums, generic_vals, etc.
};

/**
 * @brief Insert lightweight opcode-only instrumentation for instruction histogram analysis
 *
 * This is optimized for Proton instruction statistic analysis where only opcode
 * information is needed for histogram generation.
 *
 * @param instr The instruction to instrument
 * @param opcode_id The opcode identifier for this instruction
 * @param ctx_state The context state containing channel information
 */
void instrument_opcode_only(Instr* instr, int opcode_id, CTXstate* ctx_state);

/**
 * @brief Insert register tracing instrumentation
 *
 * Collects register values for detailed register flow analysis.
 *
 * @param instr The instruction to instrument
 * @param opcode_id The opcode identifier for this instruction
 * @param ctx_state The context state containing channel information
 * @param operands Structure containing all operand information (reg, ureg, etc.)
 */
void instrument_register_trace(Instr* instr, int opcode_id, CTXstate* ctx_state, const OperandLists& operands);

/**
 * @brief Insert memory access tracing instrumentation
 *
 * Collects memory access information for memory pattern analysis.
 *
 * @param instr The instruction to instrument
 * @param opcode_id The opcode identifier for this instruction
 * @param ctx_state The context state containing channel information
 * @param mref_idx Memory reference index
 */
void instrument_memory_addr_trace(Instr* instr, int opcode_id, CTXstate* ctx_state, int mref_idx);

/**
 * @brief Insert memory access tracing with value capture instrumentation
 *
 * Collects memory addresses AND values for data flow analysis.
 * Always uses IPOINT_AFTER for consistent timing semantics.
 *
 * @param instr The instruction to instrument
 * @param opcode_id The opcode identifier for this instruction
 * @param ctx_state The context state containing channel information
 * @param mref_idx Memory reference index
 * @param mem_space Memory space type (obtained via instr->getMemorySpace() in cutracer.cu)
 */
void instrument_memory_value_trace(Instr* instr, int opcode_id, CTXstate* ctx_state, int mref_idx, int mem_space);

/**
 * @brief Instruments an instruction to inject a fixed delay.
 *
 * Inserts a call to the `instrument_delay` device function before the
 * instruction. The delay value is a fixed value determined by CUTRACER_DELAY_NS.
 *
 * @param instr The instruction to instrument
 * @param delay_ns Fixed delay in nanoseconds
 */
void instrument_delay_injection(Instr* instr, uint32_t delay_ns);

/**
 * @brief SASS instruction patterns for delay injection.
 */
static const std::vector<const char*> DELAY_INJECTION_PATTERNS = {
    "SYNCS.PHASECHK.TRANS64.TRYWAIT",  // mbarrier try_wait
    "SYNCS.ARRIVE.TRANS64.RED.A1T0",   // mbarrier arrive
    "UTMALDG.2D",                      // TMA load
    "WARPGROUP.DEPBAR.LE",             // MMA wait
};

/**
 * @brief Check if an instruction should have delay injected
 *
 * @param instr The instruction to check
 * @param patterns Vector of SASS substrings to match against
 * @return true if the instruction matches any delay injection pattern
 */
bool shouldInjectDelay(Instr* instr, const std::vector<const char*>& patterns);

// Include internal TMA instrumentation if available, otherwise provide stubs
#if __has_include("fb/instrument_fb.h")
#include "fb/instrument_fb.h"
#else

/**
 * @brief Instrument a TMA instruction for tracing.
 *
 * OSS stub - does nothing.
 *
 * @param instr The instruction to instrument
 * @param opcode_id The opcode identifier
 * @param ctx_state The context state
 * @param operands The operand lists with ureg_nums populated
 */
inline void instrument_tma_trace(Instr* instr, int opcode_id, CTXstate* ctx_state, const OperandLists& operands) {
  (void)instr;
  (void)opcode_id;
  (void)ctx_state;
  (void)operands;
}

#endif  // __has_include

#endif /* INSTRUMENT_H */

```

`include/log.h`:

```h
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 *
 * See LICENSE file in the root directory for Meta's license terms.
 */

#ifndef LOG_HANDLE_H
#define LOG_HANDLE_H

#include <stdint.h>
#include <stdio.h>
#include <time.h>

#include <string>

#include "cuda.h"

// Forward declarations - use void* to avoid conflicts with CUDA types
typedef void* CUcontext_ptr;
typedef void* CUfunction_ptr;

/* ===== Logging Functions ===== */

/**
 * lprintf - print to the currently active log file only (log print)
 */
void lprintf(const char* format, ...);

/* ===== Verbose-conditional Logging Macros ===== */

// Forward declaration for verbose variable
extern int verbose;

/**
 * loprintf_v - verbose-conditional loprintf
 * Only prints when verbose is non-zero.
 */
#define loprintf_v(fmt, ...)                   \
  do {                                         \
    if (verbose) loprintf(fmt, ##__VA_ARGS__); \
  } while (0)

/**
 * loprintf_vl - verbose level-conditional loprintf
 * Only prints when verbose >= level.
 */
#define loprintf_vl(level, fmt, ...)                      \
  do {                                                    \
    if (verbose >= (level)) loprintf(fmt, ##__VA_ARGS__); \
  } while (0)

/**
 * lprintf_v - verbose-conditional lprintf
 * Only prints to log file when verbose is non-zero.
 */
#define lprintf_v(fmt, ...)                   \
  do {                                        \
    if (verbose) lprintf(fmt, ##__VA_ARGS__); \
  } while (0)

/**
 * oprintf_v - verbose-conditional oprintf
 * Only prints to stdout when verbose is non-zero.
 */
#define oprintf_v(fmt, ...)                   \
  do {                                        \
    if (verbose) oprintf(fmt, ##__VA_ARGS__); \
  } while (0)

/**
 * oprintf - print to stdout only (output print)
 */
void oprintf(const char* format, ...);

/**
 * loprintf - print to the currently active log file and stdout (log and output print)
 */
void loprintf(const char* format, ...);

/**
 * trace_lprintf - print to the kernel trace log file only
 */
void trace_lprintf(const char* format, ...);

/* ===== File Management Functions ===== */

/**
 * Opens a new log file for a specific kernel invocation.
 * This should be called on kernel entry.
 * @param ctx CUDA context
 * @param func CUfunction representing the kernel
 * @param iteration Current iteration of the kernel execution
 * @param kernel_checksum FNV-1a hash of kernel name + SASS (hex string)
 */
void log_open_kernel_file(CUcontext_ptr ctx, CUfunction_ptr func, uint32_t iteration,
                          const std::string& kernel_checksum);

/**
 * Closes the kernel-specific log file.
 * This should be called on kernel exit.
 */
void log_close_kernel_file();

/**
 * Initializes the log handle system. Creates the main process log file.
 */
void init_log_handle();

/**
 * Cleans up the log handle system. Closes the main process log file.
 */
void cleanup_log_handle();

/**
 * Builds a deterministic base filename for a kernel's trace log.
 *
 * Format:
 *   "kernel_<kernel_checksum>_iter<iteration>_<truncated_mangled_name>"
 *
 * Details:
 * - Uses the kernel_checksum (FNV-1a hash of kernel name + SASS instructions)
 *   for robust kernel identification across recompilations.
 * - Appends the kernel iteration number to distinguish repeated launches.
 * - Includes a truncated (up to 150 chars) copy of the mangled name to aid
 *   human readability while keeping the filename manageable.
 *
 * Args:
 *   ctx: CUDA context associated with the kernel function.
 *   func: The CUfunction handle of the kernel.
 *   iteration: Per-kernel iteration counter maintained by the caller.
 *   kernel_checksum: FNV-1a hash of kernel name + SASS (hex string).
 *
 * Returns:
 *   The base filename (without extension) for the kernel-specific log file.
 */
std::string generate_kernel_log_basename(CUcontext ctx, CUfunction func, uint32_t iteration,
                                         const std::string& kernel_checksum);

#endif /* LOG_HANDLE_H */

```

`include/trace_writer.h`:

```h
// (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.

/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 */

#pragma once

#include <zstd.h>

#include <cstdint>
#include <cstdio>
#include <nlohmann/json.hpp>
#include <string>
#include <vector>

#include "common.h"
#include "nvbit.h"

enum class TraceMode : std::uint8_t {
  TEXT = 0,
  COMPRESSED_NDJSON = 1,
  UNCOMPRESSED_NDJSON = 2,
  CLP = 3,
};

/**
 * @brief Rich trace record combining GPU trace data with metadata.
 *
 * This structure acts as an intermediate layer between raw GPU communication
 * structs (reg_info_t, mem_addr_access_t, opcode_only_t) and the JSON output format.
 *
 * Benefits:
 * - GPU communication protocol remains unchanged
 * - Easy to add metadata fields without modifying GPU structs
 * - One record = one JSON line (atomic write operation)
 */
struct TraceRecord {
  // ========== Metadata (not in original GPU structs) ==========

  /**
   * @brief CUDA context pointer (for debugging/correlation).
   *
   * Source: recv_thread_fun() args
   * JSON field: "ctx" (hex string)
   */
  CUcontext context;

  /**
   * @brief SASS instruction string for human readability.
   *
   * Source: ctx_state->id_to_sass_map[func][opcode_id]
   * JSON field: "sass"
   */
  std::string sass_instruction;

  /**
   * @brief Per-kernel trace sequence number (monotonically increasing).
   *
   * Used to track trace ordering within a kernel for analysis.
   * Source: Maintained by analysis.cu per kernel_launch_id
   * JSON field: "trace_index"
   */
  uint64_t trace_index;

  /**
   * @brief Host-side timestamp when trace was received (nanoseconds).
   *
   * Source: Current time when trace is processed
   * JSON field: "timestamp"
   */
  uint64_t timestamp;

  // ========== Original trace data ==========

  /**
   * @brief Type of the trace record.
   */
  message_type_t type;

  /**
   * @brief Union containing pointer to original GPU data.
   *
   * Lifetime: Caller ensures the pointed-to struct remains valid
   * until write_trace() completes (typically local variable in analysis.cu).
   */
  union {
    const reg_info_t* reg_info;
    const mem_addr_access_t* mem_access;
    const mem_value_access_t* mem_value_access;
    const opcode_only_t* opcode_only;
    const tma_access_t* tma_access;
  } data;

  /**
   * @brief Register indices from CPU-side static mapping (optional).
   *
   * Source: ctx_state->id_to_reg_indices_map[func][opcode_id]
   * Lifetime: Caller ensures pointed-to RegIndices outlives write_trace() call.
   * nullptr when not available (e.g., lookup failure).
   */
  const RegIndices* reg_indices = nullptr;

  // ========== Constructors for convenience ==========

  /**
   * @brief Create a TraceRecord for reg_info_t.
   */
  static TraceRecord create_reg_trace(CUcontext ctx, const std::string& sass, uint64_t trace_idx, uint64_t ts,
                                      const reg_info_t* reg, const RegIndices* indices = nullptr) {
    TraceRecord record;
    record.context = ctx;
    record.sass_instruction = sass;
    record.trace_index = trace_idx;
    record.timestamp = ts;
    record.type = MSG_TYPE_REG_INFO;
    record.data.reg_info = reg;
    record.reg_indices = indices;
    return record;
  }

  /**
   * @brief Create a TraceRecord for mem_addr_access_t.
   */
  static TraceRecord create_mem_trace(CUcontext ctx, const std::string& sass, uint64_t trace_idx, uint64_t ts,
                                      const mem_addr_access_t* mem) {
    TraceRecord record;
    record.context = ctx;
    record.sass_instruction = sass;
    record.trace_index = trace_idx;
    record.timestamp = ts;
    record.type = MSG_TYPE_MEM_ADDR_ACCESS;
    record.data.mem_access = mem;
    return record;
  }

  /**
   * @brief Create a TraceRecord for mem_value_access_t.
   */
  static TraceRecord create_mem_value_trace(CUcontext ctx, const std::string& sass, uint64_t trace_idx, uint64_t ts,
                                            const mem_value_access_t* mem_value) {
    TraceRecord record;
    record.context = ctx;
    record.sass_instruction = sass;
    record.trace_index = trace_idx;
    record.timestamp = ts;
    record.type = MSG_TYPE_MEM_VALUE_ACCESS;
    record.data.mem_value_access = mem_value;
    return record;
  }

  /**
   * @brief Create a TraceRecord for opcode_only_t.
   */
  static TraceRecord create_opcode_trace(CUcontext ctx, const std::string& sass, uint64_t trace_idx, uint64_t ts,
                                         const opcode_only_t* opcode) {
    TraceRecord record;
    record.context = ctx;
    record.sass_instruction = sass;
    record.trace_index = trace_idx;
    record.timestamp = ts;
    record.type = MSG_TYPE_OPCODE_ONLY;
    record.data.opcode_only = opcode;
    return record;
  }

  /**
   * @brief Create a TraceRecord for tma_access_t.
   */
  static TraceRecord create_tma_trace(CUcontext ctx, const std::string& sass, uint64_t trace_idx, uint64_t ts,
                                      const tma_access_t* tma) {
    TraceRecord record;
    record.context = ctx;
    record.sass_instruction = sass;
    record.trace_index = trace_idx;
    record.timestamp = ts;
    record.type = MSG_TYPE_TMA_ACCESS;
    record.data.tma_access = tma;
    return record;
  }
};

/**
 * @brief TraceWriter for trace output in multiple formats.
 *
 * Unified writer supporting three output modes:
 * - Mode 0: Text format (legacy .log files)
 * - Mode 1: NDJSON + Zstd compression (.ndjson.zst)
 * - Mode 2: NDJSON uncompressed (.ndjson)
 *
 * Key features:
 * - Single unified API: write_trace(TraceRecord)
 * - Automatic format dispatch based on trace_mode
 * - Buffering for I/O efficiency
 * - Metadata (ctx, sass, trace_index, timestamp) automatically included
 */
class TraceWriter {
 private:
  std::string filename_;
  FILE* file_handle_;
  int fd_;  // File descriptor for POSIX write() (Mode 1/2 only)
  std::string json_buffer_;
  size_t buffer_threshold_;
  TraceMode trace_mode_;
  bool enabled_;

  // ========== Mode 1 (Zstd compression) support ==========
  ZSTD_CCtx* zstd_ctx_;                  // Zstd compression context
  std::vector<char> compressed_buffer_;  // Pre-allocated compression output buffer
  int compression_level_;                // Zstd compression level (1-22, default 22)

 public:
  /**
   * @brief Construct TraceWriter.
   *
   * @param filename Base filename (extension added automatically based on mode)
   * @param trace_mode 0 for text, 1 for compressed JSON, 2 for uncompressed JSON
   * @param buffer_threshold Buffer flush threshold (default 1MB)
   */
  TraceWriter(const std::string& filename, int trace_mode, size_t buffer_threshold = 1024 * 1024);

  /**
   * @brief Destructor - flushes remaining data and closes file.
   */
  ~TraceWriter();

  // Disable copy
  TraceWriter(const TraceWriter&) = delete;
  TraceWriter& operator=(const TraceWriter&) = delete;

  /**
   * @brief Write a trace record to output.
   *
   * Mode 0: Formats as text and writes to .log file
   * Mode 1/2: Serializes to JSON and writes to .ndjson[.zst] file
   *
   * @param record Complete trace record with all information
   * @return true if successful, false on error
   */
  bool write_trace(const TraceRecord& record);

  /**
   * @brief Write a metadata JSON object as the first line(s) of the trace file.
   *
   * Only effective in NDJSON modes (mode 1/2/3). In text mode (mode 0),
   * this is a no-op since text mode does not use json_buffer_.
   *
   * The metadata is appended to json_buffer_ without triggering a flush,
   * so it will be written together with subsequent trace data.
   *
   * @param metadata A JSON object to write (e.g., kernel_metadata event)
   */
  void write_metadata(const nlohmann::json& metadata);

  /**
   * @brief Flush buffered data to disk.
   */
  void flush();

  /**
   * @brief Check if writer is functional.
   */
  bool is_enabled() const {
    return enabled_;
  }

 private:
  // ========== Output methods ==========

  /**
   * @brief Write record in text format (mode 0).
   */
  void write_text_format(const TraceRecord& record);

  /**
   * @brief Write record in JSON format (mode 1/2).
   */
  void write_json_format(const TraceRecord& record);

  /**
   * @brief Write CLP archive (mode 3).
   */
  void write_clp_archive();

  /**
   * @brief Write uncompressed buffer to file (mode 2).
   */
  void write_uncompressed();

  /**
   * @brief Compress and write buffer to file (mode 1).
   *
   * Compresses json_buffer_ using Zstd and writes to .ndjson.zst file.
   * Each flush creates an independent Zstd frame for incremental writing.
   */
  void write_compressed();

  /**
   * @brief Reliably write data to file with retry logic.
   *
   * Handles partial writes, EINTR interrupts, and detects zero-write deadlock.
   * On fatal error, sets enabled_ = false.
   *
   * @param data Pointer to data buffer
   * @param size Number of bytes to write
   * @param data_type Description for error messages (e.g., "bytes", "compressed bytes")
   * @return true if all data written successfully, false on error
   */
  bool write_data(const char* data, size_t size, const char* data_type);

  // ========== JSON serialization ==========

  /**
   * @brief Serialize reg_info_t fields to JSON object.
   */
  void serialize_reg_info(nlohmann::json& j, const reg_info_t* reg, const RegIndices* indices);

  /**
   * @brief Serialize mem_addr_access_t fields to JSON object.
   */
  void serialize_mem_access(nlohmann::json& j, const mem_addr_access_t* mem);

  /**
   * @brief Serialize opcode_only_t fields to JSON object.
   */
  void serialize_opcode_only(nlohmann::json& j, const opcode_only_t* opcode);

  /**
   * @brief Serialize mem_value_access_t fields to JSON object.
   */
  void serialize_mem_value_access(nlohmann::json& j, const mem_value_access_t* mem);

  /**
   * @brief Serialize tma_access_t fields to JSON object.
   */
  void serialize_tma_access(nlohmann::json& j, const tma_access_t* tma);
};

```

`install_third_party.sh`:

```sh
#!/bin/bash
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Script to download and install third-party dependencies for CUTracer
#
# Environment Variables:
#   NVBIT_VERSION  - NVBit version (or "latest" for latest release)
#   JSON_VERSION   - nlohmann/json version
#
# Usage:
#   ./install_third_party.sh                        # Use defaults
#   NVBIT_VERSION=latest ./install_third_party.sh   # Use latest NVBit
#   NVBIT_VERSION=1.7.5 JSON_VERSION=3.10.0 ./install_third_party.sh

# ============================================================
# Configuration: Set default values if not provided
# ============================================================
NVBIT_VERSION="${NVBIT_VERSION:-1.7.7.1}"
JSON_VERSION="${JSON_VERSION:-3.11.3}"

# ============================================================
# Install NVBit
# ============================================================

# Create third_party directory (if it doesn't exist)
mkdir -p third_party

# Detect host CPU architecture
HOST_ARCH=$(uname -m)
case "$HOST_ARCH" in
  x86_64)
    NVBIT_ARCH="x86_64"
    ;;
  aarch64|arm64)
    NVBIT_ARCH="aarch64"
    ;;
  *)
    echo "Error: Unsupported host architecture: $HOST_ARCH"
    echo "NVBit supports x86_64 and aarch64 only."
    exit 1
    ;;
esac
echo "Detected host architecture: $HOST_ARCH (NVBit arch: $NVBIT_ARCH)"

# Handle version: fetch latest or use specified version
if [ "$NVBIT_VERSION" = "latest" ]; then
  echo "Getting latest NVBit version information..."
  RELEASE_INFO=$(curl -s https://api.github.com/repos/NVlabs/NVBit/releases/latest)
  NVBIT_VERSION=$(echo "$RELEASE_INFO" | grep -o '"tag_name": "[^"]*' | cut -d'"' -f4)
  echo "Latest version: $NVBIT_VERSION"
else
  # Strip 'v' prefix if present for consistency
  [[ "$NVBIT_VERSION" =~ ^v ]] && NVBIT_VERSION="${NVBIT_VERSION#v}"
  echo "Using NVBit version: $NVBIT_VERSION"
  RELEASE_INFO=$(curl -s "https://api.github.com/repos/NVlabs/NVBit/releases/tags/v${NVBIT_VERSION}")
fi

# Check if API call was successful
if [ $? -ne 0 ]; then
  echo "Error: Unable to get NVBit release information. Please check your network connection or GitHub API access."
  exit 1
fi

# Verify version exists
TAG_CHECK=$(echo "$RELEASE_INFO" | grep -o '"tag_name": "[^"]*' | cut -d'"' -f4)
if [ -z "$TAG_CHECK" ]; then
  echo "Error: Specified version $NVBIT_VERSION not found."
  exit 1
fi

# Find the download link for the detected architecture
DOWNLOAD_URL=$(echo "$RELEASE_INFO" | grep -o '"browser_download_url": "[^"]*'"$NVBIT_ARCH"'[^"]*\.tar\.bz2"' | cut -d'"' -f4)

# Check if download link was found
if [ -z "$DOWNLOAD_URL" ]; then
  echo "Error: Unable to find download link for $NVBIT_ARCH version."
  echo "Please check if NVBit $NVBIT_VERSION supports $NVBIT_ARCH."
  exit 1
fi

echo "Download link: $DOWNLOAD_URL"

# Download NVBit package
echo "Downloading NVBit..."
TEMP_FILE=$(mktemp)
curl -L -o "$TEMP_FILE" "$DOWNLOAD_URL"

# Check if download was successful
if [ $? -ne 0 ]; then
  echo "Error: Download failed."
  rm -f "$TEMP_FILE"
  exit 1
fi

# Clean up old version (if exists)
echo "Cleaning up old version..."
rm -rf third_party/nvbit

# Extract to temporary directory
echo "Extracting NVBit..."
TEMP_DIR=$(mktemp -d)
tar -xjf "$TEMP_FILE" -C "$TEMP_DIR"

# Check if extraction was successful
if [ $? -ne 0 ]; then
  echo "Error: Extraction failed."
  rm -f "$TEMP_FILE"
  rm -rf "$TEMP_DIR"
  exit 1
fi

# Find the extracted directory
EXTRACTED_DIR=$(find "$TEMP_DIR" -maxdepth 1 -name "nvbit*" -type d | head -1)
if [ -z "$EXTRACTED_DIR" ]; then
  echo "Error: Unable to find extracted NVBit directory."
  rm -f "$TEMP_FILE"
  rm -rf "$TEMP_DIR"
  exit 1
fi

# Move the extracted directory to third_party/nvbit
echo "Installing NVBit to third_party/nvbit..."
mv "$EXTRACTED_DIR" third_party/nvbit

# Clean up temporary files and directories
rm -f "$TEMP_FILE"
rm -rf "$TEMP_DIR"

echo "NVBit $NVBIT_VERSION has been successfully installed to third_party/nvbit directory."

# ============================================================
# Install nlohmann/json
# ============================================================
echo ""
echo "Downloading nlohmann/json..."
JSON_URL="https://github.com/nlohmann/json/releases/download/v${JSON_VERSION}/json.hpp"

mkdir -p third_party/nlohmann
curl -L -o third_party/nlohmann/json.hpp "$JSON_URL"

if [ $? -eq 0 ]; then
    echo "nlohmann/json ${JSON_VERSION} has been successfully installed."
else
    echo "Error: Failed to download nlohmann/json."
    exit 1
fi

echo ""
echo "All third-party dependencies have been successfully installed."

```

`logo.svg`:

```svg
<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 280 280" role="img" aria-label="CUTracer icon â€” 9 centered outer pins (fixed)">
  <style>
    :root{
      --ink:#0b1221;
      --accent:#76B900;   /* NVIDIA green */
      --chip:#f7f9fb;     /* chip background */
      --pin:#76B900;      /* pin green */
      --mem:#9aa3ad;      /* memory grid gray */
      --reg:#9aa3ad;      /* register gray */
    }
    @media (prefers-color-scheme: dark){
      :root{
        --ink:#e5e7eb; --chip:#1e293b; --mem:#7a828c; --reg:#7a828c; --pin:#76B900;
      }
    }
    .ns{vector-effect:non-scaling-stroke}
  </style>

  <!-- chip body (internal content limited to 40â€“240) -->
  <rect x="40" y="40" width="200" height="200" rx="20"
        fill="var(--chip)" stroke="var(--ink)" stroke-width="2" class="ns"/>

  <!-- outer pins: 9 per side, 10Ã—10, 13 spacing, start at 43, strictly centered -->
  <g fill="var(--pin)">
    <!-- top -->
    <g transform="translate(43,28)">
      <rect x="0"   y="0" width="10" height="10" rx="2"/>
      <rect x="23"  y="0" width="10" height="10" rx="2"/>
      <rect x="46"  y="0" width="10" height="10" rx="2"/>
      <rect x="69"  y="0" width="10" height="10" rx="2"/>
      <rect x="92"  y="0" width="10" height="10" rx="2"/>
      <rect x="115" y="0" width="10" height="10" rx="2"/>
      <rect x="138" y="0" width="10" height="10" rx="2"/>
      <rect x="161" y="0" width="10" height="10" rx="2"/>
      <rect x="184" y="0" width="10" height="10" rx="2"/>
    </g>
    <!-- bottom -->
    <g transform="translate(43,242)">
      <rect x="0"   y="0" width="10" height="10" rx="2"/>
      <rect x="23"  y="0" width="10" height="10" rx="2"/>
      <rect x="46"  y="0" width="10" height="10" rx="2"/>
      <rect x="69"  y="0" width="10" height="10" rx="2"/>
      <rect x="92"  y="0" width="10" height="10" rx="2"/>
      <rect x="115" y="0" width="10" height="10" rx="2"/>
      <rect x="138" y="0" width="10" height="10" rx="2"/>
      <rect x="161" y="0" width="10" height="10" rx="2"/>
      <rect x="184" y="0" width="10" height="10" rx="2"/>
    </g>
    <!-- left -->
    <g transform="translate(28,43)">
      <rect x="0" y="0"   width="10" height="10" rx="2"/>
      <rect x="0" y="23"  width="10" height="10" rx="2"/>
      <rect x="0" y="46"  width="10" height="10" rx="2"/>
      <rect x="0" y="69"  width="10" height="10" rx="2"/>
      <rect x="0" y="92"  width="10" height="10" rx="2"/>
      <rect x="0" y="115" width="10" height="10" rx="2"/>
      <rect x="0" y="138" width="10" height="10" rx="2"/>
      <rect x="0" y="161" width="10" height="10" rx="2"/>
      <rect x="0" y="184" width="10" height="10" rx="2"/>
    </g>
    <!-- right -->
    <g transform="translate(242,43)">
      <rect x="0" y="0"   width="10" height="10" rx="2"/>
      <rect x="0" y="23"  width="10" height="10" rx="2"/>
      <rect x="0" y="46"  width="10" height="10" rx="2"/>
      <rect x="0" y="69"  width="10" height="10" rx="2"/>
      <rect x="0" y="92"  width="10" height="10" rx="2"/>
      <rect x="0" y="115" width="10" height="10" rx="2"/>
      <rect x="0" y="138" width="10" height="10" rx="2"/>
      <rect x="0" y="161" width="10" height="10" rx="2"/>
      <rect x="0" y="184" width="10" height="10" rx="2"/>
    </g>
  </g>

  <!-- internal elements -->
  <!-- memory grid -->
  <g transform="translate(56,56)" fill="var(--mem)">
    <rect x="0"  y="0"  width="24" height="24" rx="4"/>
    <rect x="30" y="0"  width="24" height="24" rx="4"/>
    <rect x="60" y="0"  width="24" height="24" rx="4"/>
    <rect x="0"  y="30" width="24" height="24" rx="4"/>
    <rect x="30" y="30" width="24" height="24" rx="4"/>
    <rect x="60" y="30" width="24" height="24" rx="4"/>
    <rect x="0"  y="60" width="24" height="24" rx="4"/>
    <rect x="30" y="60" width="24" height="24" rx="4"/>
    <rect x="60" y="60" width="24" height="24" rx="4"/>
  </g>

  <!-- register lanes -->
  <g transform="translate(164,56)" fill="var(--reg)">
    <rect x="0" y="0"  width="60" height="6" rx="3"/>
    <rect x="0" y="10" width="60" height="6" rx="3"/>
    <rect x="0" y="20" width="60" height="6" rx="3"/>
    <rect x="0" y="30" width="60" height="6" rx="3"/>
    <rect x="0" y="40" width="60" height="6" rx="3"/>
    <rect x="0" y="50" width="60" height="6" rx="3"/>
    <rect x="0" y="60" width="60" height="6" rx="3"/>
    <rect x="0" y="70" width="60" height="6" rx="3"/>
  </g>

  <!-- SASS trace (avoid overlapping with magnifier) -->
  <polyline points="60,230 100,204 130,214 160,190 190,202 220,192"
            fill="none" stroke="var(--accent)" stroke-width="8" stroke-linecap="round" stroke-linejoin="round" class="ns"/>
  <g fill="var(--accent)">
    <circle cx="60" cy="230" r="6"/>
    <circle cx="130" cy="214" r="6"/>
    <circle cx="160" cy="190" r="6"/>
    <circle cx="220" cy="192" r="6"/>
  </g>

  <!-- magnifier -->
  <g transform="translate(180,140)">
    <circle r="22" fill="none" stroke="var(--accent)" stroke-width="4" class="ns"/>
    <line x1="14" y1="14" x2="30" y2="30" stroke="var(--accent)" stroke-width="6" stroke-linecap="round" class="ns"/>
    <circle cx="8" cy="4" r="5" fill="var(--accent)"/>
  </g>

  <!-- deadlock cue -->
  <g transform="translate(220,120)">
    <path d="M0,-12 A12,12 0 1 1 -0.01,-12" fill="none" stroke="var(--accent)" stroke-width="3.5"
          stroke-dasharray="20 12" stroke-linecap="round" class="ns"/>
  </g>

  <!-- IPC semicircle -->
  <g transform="translate(110,130)">
    <path d="M0,0 A26,26 0 1 1 0.01,0" fill="none" stroke="rgba(118,185,0,.25)" stroke-width="7" class="ns"/>
    <path d="M0,0 A26,26 0 0 1 20,-12" fill="none" stroke="var(--accent)" stroke-width="7" stroke-linecap="round" class="ns"/>
  </g>
</svg>

```

`pr.md`:

```md
## Summary

Add `CUTRACER_ZSTD_LEVEL` environment variable to configure zstd compression level (1-22). This allows users to trade off compression speed vs compression ratio based on their use case.

## Changes

- `include/env_config.h`: Add extern declaration for `zstd_compression_level`
- `src/env_config.cu`: Add environment variable reading logic with validation (range 1-22)
- `src/trace_writer.cpp`: Use configurable compression level instead of hardcoded value 22
- `readme.md`: Document new `CUTRACER_ZSTD_LEVEL` environment variable

## Configuration

`CUTRACER_ZSTD_LEVEL`: Zstd compression level (1-22, default 22)
- Lower values (1-3): Faster compression, slightly larger output
- Higher values (19-22): Maximum compression, slower but smallest output
- Default of 22 provides maximum compression for smallest output

## Motivation

The default compression level of 22 (maximum) prioritizes compression ratio over speed. For use cases where compression speed is more important, users can set a lower level (e.g., 3) to get nearly the same compression ratio with significantly faster compression. This change allows users to choose the trade-off that best suits their workflow.

## Example Usage

```bash
# Fast compression (level 1)
CUTRACER_ZSTD_LEVEL=1 CUDA_INJECTION64_PATH=~/CUTracer/lib/cutracer.so ./app

# Maximum compression (level 22, default)
CUDA_INJECTION64_PATH=~/CUTracer/lib/cutracer.so ./app

# Balanced compression (level 9)
CUTRACER_ZSTD_LEVEL=9 CUDA_INJECTION64_PATH=~/CUTracer/lib/cutracer.so ./app
```

## Test Plan

1. Build CUTracer: `make -j$(nproc)`
2. Verify default level works: Run with `TRACE_FORMAT_NDJSON=1` and check output
3. Verify custom level: Set `CUTRACER_ZSTD_LEVEL=1` and verify faster compression
4. Verify validation: Set invalid value (e.g., 25) and verify warning + fallback to default

```

`python/README.md`:

```md
# CUTracer Python Module

Python tools for CUTracer trace validation, parsing, and analysis.

## Overview

The `cutracer` Python package provides a comprehensive framework for working with CUTracer trace files. This module is designed to be:
- **Reusable**: Import and use in your own Python scripts
- **Testable**: Full unittest suite with real trace data
- **Type-safe**: Type hints and mypy compatibility
- **Extensible**: Plugin architecture for future enhancements

## Installation

### For Development

```bash
cd /path/to/CUTracer/python
pip install -e ".[dev]"
```

### For Production Use

```bash
cd /path/to/CUTracer/python
pip install .
```

## Features

### Trace Validation (Current)

- **JSON Validation**: Validate NDJSON trace files (mode 2) for syntax and schema compliance
- **Text Validation**: Validate text-format trace files (mode 0) for format compliance
- **Cross-Format Consistency**: Compare different trace formats for data consistency

### Planned Features

- **Trace Parsing**: Parse trace files into structured Python objects
- **Analysis Tools**: Instruction histograms, performance metrics, trace comparison
- **Format Conversion**: Convert between different trace formats
- **Compression Support**: Handle zstd-compressed traces (mode 1)

## Usage

### Python API

```python
from cutracer.validation import (
    validate_json_trace,
    validate_text_trace,
    compare_trace_formats,
)

# Validate JSON trace
result = validate_json_trace("kernel_trace.ndjson")
if result["valid"]:
    print(f"âœ“ Valid JSON trace with {result['record_count']} records")
else:
    print(f"âœ— Validation failed: {result['errors']}")

# Validate text trace
result = validate_text_trace("kernel_trace.log")
if result["valid"]:
    print(f"âœ“ Valid text trace")
else:
    print(f"âœ— Validation failed: {result['errors']}")

# Compare two formats
result = compare_trace_formats("kernel_trace.log", "kernel_trace.ndjson")
if result["consistent"]:
    print("âœ“ Formats are consistent")
else:
    print(f"âœ— Inconsistencies found: {result['differences']}")
```

## Module Structure

```
python/
â”œâ”€â”€ cutracer/                        # Main package
â”‚   â”œâ”€â”€ __init__.py                  # Package entry point with version
â”‚   â””â”€â”€ validation/                  # Validation framework
â”‚       â”œâ”€â”€ __init__.py              # Validation API exports
â”‚       â”œâ”€â”€ schema_loader.py         # JSON Schema loader
â”‚       â”œâ”€â”€ json_validator.py        # JSON syntax & schema validation
â”‚       â”œâ”€â”€ text_validator.py        # Text format validation
â”‚       â”œâ”€â”€ consistency.py           # Cross-format consistency checks
â”‚       â””â”€â”€ schemas/                 # JSON Schema definitions
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ reg_trace.schema.json
â”‚           â”œâ”€â”€ mem_trace.schema.json
â”‚           â”œâ”€â”€ opcode_only.schema.json
â”‚           â””â”€â”€ delay_config.schema.json
â”œâ”€â”€ tests/                           # Unit tests
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_base.py                 # Base test class and utilities
â”‚   â”œâ”€â”€ test_schemas.py              # Schema loading tests
â”‚   â”œâ”€â”€ test_json_validator.py       # JSON validation tests
â”‚   â”œâ”€â”€ test_text_validator.py       # Text validation tests
â”‚   â”œâ”€â”€ test_consistency.py          # Consistency check tests
â”‚   â””â”€â”€ example_inputs/              # Real trace data for tests
â”‚       â”œâ”€â”€ reg_trace_sample.ndjson
â”‚       â”œâ”€â”€ reg_trace_sample.log
â”‚       â”œâ”€â”€ invalid_syntax.ndjson
â”‚       â””â”€â”€ invalid_schema.ndjson
â”œâ”€â”€ pyproject.toml                   # Modern Python project config
â””â”€â”€ README.md                        # This file
```

## Development

### Running Tests

```bash
cd python/

# Run all tests
python -m unittest discover -s tests -v

# Run specific test file
python -m unittest tests.test_json_validator -v
```

### Type Checking

```bash
cd python/
mypy cutracer/
```

### Code Formatting

```bash
# From project root directory
./format.sh format

# Or manually with ufmt
ufmt format python/
usort format python/
```

### Running All Checks

```bash
# Format code
./format.sh format

# Type check
mypy cutracer/

# Run tests
python -m unittest discover -s tests -v
```

## Validation Details

### JSON Trace Validation

The JSON validator checks:

- **Syntax**: Valid JSON format on each line (NDJSON)
- **Schema**: Correct field types and structure per JSON Schema
- **Required Fields**: `message_type`, `ctx`, `kernel_launch_id`, `trace_index`, `timestamp`, `sass`, etc.
- **Register Values**: Arrays of integers with proper format
- **CTA/Warp IDs**: Valid integer ranges

### Text Trace Validation

The text validator checks:

- **Format Patterns**: Correct CTX/CTA/warp header patterns
- **Register Output**: Proper hex format (e.g., `Reg0_T00: 0x...`)
- **Memory Access**: Valid memory address patterns

### Consistency Validation

The consistency validator compares:

- **Record Counts**: Same number of records in both formats
- **Content Matching**: Same kernel IDs, trace indices, SASS strings
- **Timestamp Order**: Consistent ordering between formats

## Trace Format Reference

### JSON Format (NDJSON - Mode 2)

Each line is a JSON object with the following structure:

```json
{
  "message_type": "reg_trace",
  "ctx": "0x58a0c0",
  "kernel_launch_id": 0,
  "trace_index": 0,
  "timestamp": 1762026820167834792,
  "sass": "LDC R1, c[0x0][0x28] ;",
  "pc": 0,
  "opcode_id": 0,
  "warp": 0,
  "cta": [0, 0, 0],
  "regs": [[0, 0, 0, ...]]
}
```

### Text Format (Mode 0)

Human-readable format with CTX headers and register values:

```
CTX 0x58a0c0 - CTA 0,0,0 - warp 0 - LDC R1, c[0x0][0x28] ;:
    * Reg0_T00: 0x0000000000000000  Reg0_T01: 0x0000000000000000 ...
```

## Contributing

1. Install development dependencies: `pip install -e ".[dev]"`
2. Make your changes
3. Run tests: `python -m unittest discover -s tests -v`
4. Run type checker: `mypy cutracer/`
5. Format code: `./format.sh format`
6. Submit a pull request

## License

MIT License - See LICENSE file for details.

## Support

For issues and questions, please open an issue on the CUTracer GitHub repository.

```

`python/cutracer/__init__.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
CUTracer Python SDK

This package provides tools for validating, parsing, and analyzing
CUDA trace data generated by CUTracer.
"""

from importlib.metadata import PackageNotFoundError, version

try:
    __version__ = version("cutracer")
except PackageNotFoundError:
    __version__ = "0+unknown"  # Development mode or not installed

from cutracer.validation import (
    compare_trace_formats,
    validate_json_trace,
    validate_text_trace,
)

__all__ = [
    "__version__",
    "validate_json_trace",
    "validate_text_trace",
    "compare_trace_formats",
]

```

`python/cutracer/__main__.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Allow running cutracer as a module: python -m cutracer
"""

from cutracer.cli import main

if __name__ == "__main__":
    main()

```

`python/cutracer/analyze/__init__.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
CUTracer analyze module.

Provides analysis algorithms that produce derived insights:
- warp-summary: Warp execution status analysis (completed, in-progress, missing)
- (future: hot-instructions, data-race, memory-pattern, divergence, ...)
"""

from cutracer.query.warp_summary import (
    compute_warp_summary,
    format_ranges,
    format_warp_summary_text,
    is_exit_instruction,
    merge_to_ranges,
    warp_summary_to_dict,
    WarpSummary,
)

__all__ = [
    # Warp summary
    "WarpSummary",
    "is_exit_instruction",
    "merge_to_ranges",
    "format_ranges",
    "compute_warp_summary",
    "format_warp_summary_text",
    "warp_summary_to_dict",
]

```

`python/cutracer/analyze/cli.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
CLI implementation for the analyze command group.

Provides command-line interface for trace analysis commands.
"""

import json
from pathlib import Path
from typing import Optional

import click
from cutracer.query.grouper import StreamingGrouper
from cutracer.query.reader import TraceReader
from cutracer.query.warp_summary import (
    compute_warp_summary,
    format_warp_summary_text,
    warp_summary_to_dict,
)
from cutracer.shared_vars import is_fbcode


@click.group(name="analyze")
def analyze_command() -> None:
    """
    Analyze trace data for patterns and insights.

    Available subcommands:
      warp-summary      Analyze warp execution status
    """
    pass


@click.command(name="warp-summary")
@click.argument("file", type=click.Path(exists=True, path_type=Path))
@click.option(
    "--format",
    "-f",
    "output_format",
    type=click.Choice(["text", "json"]),
    default="text",
    show_default=True,
    help="Output format.",
)
@click.option(
    "--output",
    "-o",
    "output_file",
    type=click.Path(path_type=Path),
    default=None,
    help="Output file path (default: stdout).",
)
def warp_summary_command(
    file: Path,
    output_format: str,
    output_file: Optional[Path],
) -> None:
    """
    Analyze warp execution status from a trace file.

    Identifies completed, in-progress, and missing warps by analyzing
    whether each warp executed an EXIT instruction.

    \b
    Examples:
      cutracer analyze warp-summary trace.ndjson
      cutracer analyze warp-summary trace.ndjson --format json
      cutracer analyze warp-summary trace.ndjson -o summary.json -f json
    """
    try:
        reader = TraceReader(file)
    except FileNotFoundError as e:
        raise click.ClickException(str(e))

    # Group records by warp and get all records per group
    grouper = StreamingGrouper(reader.iter_records(), "warp")
    groups = grouper.all_per_group()

    if not groups:
        raise click.ClickException("No records found in trace file.")

    summary = compute_warp_summary(groups)
    if summary is None:
        raise click.ClickException(
            "Could not compute warp summary. "
            "Make sure records have integer 'warp' field."
        )

    if output_format == "json":
        output = json.dumps(warp_summary_to_dict(summary), indent=2)
    else:
        output = format_warp_summary_text(summary)

    if output_file:
        output_file.parent.mkdir(parents=True, exist_ok=True)
        output_file.write_text(output + "\n")
        click.echo(f"Output written to {output_file}", err=True)
    else:
        click.echo(output)


# Register subcommands
analyze_command.add_command(warp_summary_command)

# Conditionally register internal commands
if is_fbcode():
    from cutracer.analyze.fb.data_race.cli import data_race_command

    analyze_command.add_command(data_race_command)

```

`python/cutracer/cli.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
CUTracer CLI entry point.

Provides command-line interface for trace validation, query, and analysis.
"""

import sys
from importlib.metadata import PackageNotFoundError, version

import click
from cutracer.analyze.cli import analyze_command
from cutracer.query.cli import query_command
from cutracer.reduce.cli import reduce_command
from cutracer.validation.cli import compare_command, validate_command


def _get_package_version() -> str:
    """Get package version from metadata."""
    try:
        return version("cutracer")
    except PackageNotFoundError:
        return "0+unknown"


EXAMPLES = """
Examples:
  cutraceross validate kernel_trace.ndjson
  cutraceross validate kernel_trace.ndjson.zst --verbose
  cutraceross validate trace.log --format text
  cutraceross query trace.ndjson --filter "warp=24"
  cutraceross query trace.ndjson -f "pc=0x43d0;warp=24"
  cutraceross query trace.ndjson --group-by warp --count
  cutraceross analyze warp-summary trace.ndjson
"""


@click.group(epilog=EXAMPLES)
@click.version_option(version=_get_package_version(), prog_name="cutraceross")
def main() -> None:
    """CUTracer: CUDA trace validation, query, and analysis tools."""
    pass


# Register subcommands
main.add_command(analyze_command)
main.add_command(compare_command)
main.add_command(query_command)
main.add_command(reduce_command)
main.add_command(validate_command)


if __name__ == "__main__":
    sys.exit(main())

```

`python/cutracer/kernel_config.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Kernel Configuration Abstraction for Trace Metadata

"""
Kernel-level configuration extracted from trace metadata.

Provides KernelConfig dataclass that captures kernel launch parameters
from the kernel_metadata event in CUTracer trace files.
"""

from dataclasses import dataclass
from typing import Any

THREADS_PER_WARP = 32
WARPS_PER_WARPGROUP = 4  # Hopper (SM90) warpgroup size


@dataclass(frozen=True)
class KernelConfig:
    """
    Kernel-level configuration extracted from trace metadata.

    Combines static metadata (from kernel_metadata event) with
    derived properties computed from launch parameters.

    Attributes:
        kernel_name: Unmangled kernel function name
        kernel_checksum: Binary fingerprint for kernel identification
        block_dims: Threads per CTA as (x, y, z) tuple
        grid_dims: CTAs per grid as (x, y, z) tuple
        shmem_dynamic_bytes: Dynamic shared memory allocation size
        shmem_static_bytes: Static shared memory allocation size
        nregs: Register usage per thread
        cubin_path: Relative path to dumped cubin file (only when dump_cubin enabled)
    """

    kernel_name: str
    kernel_checksum: str
    block_dims: tuple[int, int, int]
    grid_dims: tuple[int, int, int]
    shmem_dynamic_bytes: int
    shmem_static_bytes: int = 0
    nregs: int = 0
    cubin_path: str = ""  # Only set when dump_cubin is enabled

    @property
    def threads_per_cta(self) -> int:
        """Total threads per CTA (block_dims product)."""
        return self.block_dims[0] * self.block_dims[1] * self.block_dims[2]

    @property
    def warps_per_cta(self) -> int:
        """Number of warps per CTA."""
        return (self.threads_per_cta + THREADS_PER_WARP - 1) // THREADS_PER_WARP

    @property
    def warpgroups_per_cta(self) -> int:
        """Number of warpgroups per CTA."""
        return (self.warps_per_cta + WARPS_PER_WARPGROUP - 1) // WARPS_PER_WARPGROUP

    @property
    def total_shmem_bytes(self) -> int:
        """Total shared memory (dynamic + static)."""
        return self.shmem_dynamic_bytes + self.shmem_static_bytes

    @property
    def total_ctas(self) -> int:
        """Total CTAs in grid (grid_dims product)."""
        return self.grid_dims[0] * self.grid_dims[1] * self.grid_dims[2]


def parse_kernel_metadata(record: dict[str, Any]) -> KernelConfig | None:
    """
    Parse kernel_metadata event into KernelConfig.

    The kernel_metadata event is the first event in new-format CUTracer
    trace files, containing kernel launch parameters captured via CUDA
    driver API.

    Args:
        record: First event from trace file

    Returns:
        KernelConfig if record is kernel_metadata type, None otherwise

    Example:
        >>> record = {"type": "kernel_metadata", "block": [384, 1, 1], ...}
        >>> config = parse_kernel_metadata(record)
        >>> config.warps_per_cta
        12
    """
    if record.get("type") != "kernel_metadata":
        return None

    block = record.get("block", [0, 0, 0])
    grid = record.get("grid", [0, 0, 0])

    return KernelConfig(
        kernel_name=record.get("unmangled_name", record.get("mangled_name", "")),
        kernel_checksum=record.get("kernel_checksum", ""),
        block_dims=(block[0], block[1], block[2]),
        grid_dims=(grid[0], grid[1], grid[2]),
        shmem_dynamic_bytes=record.get("shmem_dynamic", 0),
        shmem_static_bytes=record.get("shmem_static", 0),
        nregs=record.get("nregs", 0),
        cubin_path=record.get("cubin_path", ""),
    )

```

`python/cutracer/query/__init__.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
CUTracer query module.

Provides data query utilities for trace files:
- TraceReader: Read and iterate over trace records
- parse_filter_expr: Parse filter expressions for record filtering
- select_records: Memory-efficient record selection
- StreamingGrouper: Memory-efficient grouped queries
- Formatters: Output formatting for table/json/csv
- WarpSummary: Warp execution status summary
"""

from .formatters import (
    DEFAULT_FIELDS,
    format_records_csv,
    format_records_json,
    format_records_table,
    format_value,
    get_display_fields,
)
from .grouper import StreamingGrouper
from .reader import (
    build_filter_predicate,
    parse_filter_expr,
    select_records,
    TraceReader,
)
from .warp_summary import (
    compute_warp_summary,
    format_ranges,
    format_warp_summary_text,
    is_exit_instruction,
    merge_to_ranges,
    warp_summary_to_dict,
    WarpSummary,
)

__all__ = [
    "TraceReader",
    "parse_filter_expr",
    "build_filter_predicate",
    "select_records",
    # Grouper
    "StreamingGrouper",
    # Formatters
    "DEFAULT_FIELDS",
    "format_value",
    "get_display_fields",
    "format_records_table",
    "format_records_json",
    "format_records_csv",
    # Warp Summary
    "WarpSummary",
    "is_exit_instruction",
    "merge_to_ranges",
    "format_ranges",
    "compute_warp_summary",
    "format_warp_summary_text",
    "warp_summary_to_dict",
]

```

`python/cutracer/query/cli.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
CLI implementation for the query subcommand.

Provides command-line interface for querying and viewing CUTracer trace files.
"""

import csv
import io
import json
from pathlib import Path
from typing import Any, Optional

import click
from cutracer.query.formatters import (
    format_records_csv,
    format_records_json,
    format_records_ndjson,
    format_records_table,
    get_display_fields,
)
from cutracer.query.grouper import StreamingGrouper
from cutracer.query.reader import build_filter_predicate, select_records, TraceReader
from cutracer.query.warp_summary import (
    compute_warp_summary,
    format_warp_summary_text,
    warp_summary_to_dict,
)
from tabulate import tabulate


def _write_output(
    output: str,
    output_file: Optional[Path],
    compress: bool = False,
    record_count: Optional[int] = None,
) -> None:
    """Write output to file or stdout."""
    if output_file:
        output_file.parent.mkdir(parents=True, exist_ok=True)
        if compress:
            import zstandard as zstd

            cctx = zstd.ZstdCompressor()
            output_file.write_bytes(cctx.compress((output + "\n").encode("utf-8")))
        else:
            output_file.write_text(output + "\n")
        if record_count is not None:
            click.echo(f"{record_count} records written to {output_file}", err=True)
        else:
            click.echo(f"Output written to {output_file}", err=True)
    else:
        click.echo(output)


def _format_counts(
    counts: dict[Any, int],
    group_field: str,
    top_n: Optional[int],
    output_format: str,
    show_header: bool,
) -> str:
    """
    Format record counts per group.

    Args:
        counts: Dict mapping group key to count
        group_field: Name of the field used for grouping
        top_n: Only show top N groups by count (None for all)
        output_format: Output format (table, json, csv, ndjson)
        show_header: Whether to show headers

    Returns:
        Formatted output string
    """
    if not counts:
        return "No records found."

    # Sort by count (descending), then by key (ascending) for stability
    sorted_counts = sorted(counts.items(), key=lambda x: (-x[1], str(x[0])))

    # Apply top N filter if specified
    if top_n is not None and top_n > 0:
        sorted_counts = sorted_counts[:top_n]

    if output_format == "json":
        output_data = {str(k): v for k, v in sorted_counts}
        return json.dumps(output_data, indent=2)
    elif output_format == "csv":
        output = io.StringIO(newline="")
        writer = csv.writer(output)
        if show_header:
            writer.writerow([group_field, "count"])
        for key, cnt in sorted_counts:
            writer.writerow([key, cnt])
        return output.getvalue().rstrip("\n").replace("\r", "")
    elif output_format == "ndjson":
        lines = [json.dumps({group_field: k, "count": v}) for k, v in sorted_counts]
        return "\n".join(lines)
    else:  # table
        table_data = [[key, cnt] for key, cnt in sorted_counts]
        headers = [group_field.upper(), "COUNT"] if show_header else []
        return tabulate(table_data, headers=headers, tablefmt="plain")


def _format_groups(
    groups: dict[Any, list[dict]],
    group_by: str,
    output_format: str,
    fields: Optional[str],
    no_header: bool,
) -> str:
    """Format grouped records with group headers."""
    if not groups:
        return "No records found."

    # Compute warp summary if grouping by warp
    warp_summary = None
    if group_by == "warp":
        warp_summary = compute_warp_summary(groups)

    output_parts = []

    if output_format == "json":
        # JSON output with optional warp_summary
        groups_data = {}
        for group_key, records in sorted(groups.items(), key=lambda x: str(x[0])):
            if not records:
                continue
            display_fields = get_display_fields(records, fields)
            filtered_records = [
                {f: r.get(f) for f in display_fields if f in r} for r in records
            ]
            groups_data[str(group_key)] = filtered_records

        if warp_summary:
            output_data = {
                "groups": groups_data,
                "warp_summary": warp_summary_to_dict(warp_summary),
            }
        else:
            output_data = groups_data

        return json.dumps(output_data, indent=2)

    elif output_format == "ndjson":
        # NDJSON output: each record on a line, with group field added
        is_all_fields = fields and fields.strip() in ("*", "all")
        for group_key, records in sorted(groups.items(), key=lambda x: str(x[0])):
            if not records:
                continue
            for record in records:
                if is_all_fields:
                    # All fields: output each record as-is, preserving all fields
                    out_record = dict(record)
                else:
                    display_fields = get_display_fields(records, fields)
                    out_record = {
                        f: record.get(f) for f in display_fields if f in record
                    }
                out_record["_group"] = group_key
                output_parts.append(json.dumps(out_record))
        return "\n".join(output_parts)

    else:
        # Table or CSV output
        for group_key, records in sorted(groups.items(), key=lambda x: str(x[0])):
            if not records:
                continue
            display_fields = get_display_fields(records, fields)
            output_parts.append(
                f"\n=== Group: {group_key} ({len(records)} records) ==="
            )
            if output_format == "csv":
                output_parts.append(
                    format_records_csv(
                        records, display_fields, show_header=not no_header
                    )
                )
            else:  # table
                output_parts.append(
                    format_records_table(
                        records, display_fields, show_header=not no_header
                    )
                )

        # Print warp summary for table format only (not CSV)
        if output_format == "table" and warp_summary:
            output_parts.append(format_warp_summary_text(warp_summary))

        return "\n".join(output_parts)


@click.command(name="query")
@click.argument("file", type=click.Path(exists=True, path_type=Path))
@click.option(
    "--head",
    "-n",
    type=int,
    default=10,
    show_default=True,
    help="Number of records to show from the beginning.",
)
@click.option(
    "--tail",
    "-t",
    type=int,
    default=None,
    help="Number of records to show from the end (overrides --head).",
)
@click.option(
    "--all-lines",
    "-a",
    "all_records",
    is_flag=True,
    help="Show all records (overrides --head and --tail).",
)
@click.option(
    "--filter",
    "-f",
    "filter_exprs",
    type=str,
    default=None,
    multiple=True,
    help="Filter expression (e.g., 'warp=24', 'pc=0x43d0', 'pc=0x43d0;warp=24'). Can be specified multiple times for AND logic.",
)
@click.option(
    "--format",
    "output_format",
    type=click.Choice(["table", "json", "csv", "ndjson"]),
    default="table",
    show_default=True,
    help="Output format.",
)
@click.option(
    "--fields",
    type=str,
    default=None,
    help="Comma-separated list of fields to display (e.g., 'warp,pc,sass'), or '*'/'all' for all fields.",
)
@click.option(
    "--no-header",
    is_flag=True,
    default=False,
    help="Hide the table/CSV header row.",
)
@click.option(
    "--group-by",
    "-g",
    "group_by",
    type=str,
    default=None,
    help="Group records by field (e.g., 'warp').",
)
@click.option(
    "--count",
    is_flag=True,
    help="Show record count per group (requires --group-by).",
)
@click.option(
    "--top",
    type=int,
    default=None,
    help="Show only top N groups by count (requires --group-by --count).",
)
@click.option(
    "--output",
    "-o",
    "output_file",
    type=click.Path(path_type=Path),
    default=None,
    help="Output file path (default: stdout).",
)
@click.option(
    "--compress",
    is_flag=True,
    default=False,
    help="Compress output with Zstd (requires --output).",
)
def query_command(
    file: Path,
    head: int,
    tail: Optional[int],
    all_records: bool,
    filter_exprs: tuple[str, ...],
    output_format: str,
    fields: Optional[str],
    no_header: bool,
    group_by: Optional[str],
    count: bool,
    top: Optional[int],
    output_file: Optional[Path],
    compress: bool,
) -> None:
    """
    Query and view trace data from FILE.

    Reads NDJSON trace files (plain or Zstd-compressed) and displays
    records in a formatted table.

    \b
    Examples:
      cutracer query trace.ndjson
      cutracer query trace.ndjson.zst --head 20
      cutracer query trace.ndjson --tail 5
      cutracer query trace.ndjson --all-lines --format ndjson
      cutracer query trace.ndjson --filter "warp=24"
      cutracer query trace.ndjson --filter "pc=0x43d0;warp=24"
      cutracer query trace.ndjson -f "pc=0x43d0" -f "warp=24"
      cutracer query trace.ndjson --filter "pc=0x43d0" --all-lines --output filtered.ndjson
      cutracer query trace.ndjson --filter "pc=0x43d0" --all-lines --format ndjson -o out.zst --compress
      cutracer query trace.ndjson --group-by warp
      cutracer query trace.ndjson --group-by warp --count
      cutracer query trace.ndjson --group-by sass --count --top 20
    """
    # Validate option combinations
    if count and not group_by:
        raise click.ClickException("--count requires --group-by")
    if top is not None and not count:
        raise click.ClickException("--top requires --count")
    if compress and not output_file:
        raise click.ClickException("--compress requires --output")

    USE_CLP = True if file.name.endswith(".clp") else False
    if USE_CLP:
        from cutracer.query.clp import TraceReaderCLP

        reader_cls = TraceReaderCLP
    else:
        reader_cls = TraceReader

    # Create reader
    try:
        reader = reader_cls(file)
    except FileNotFoundError as e:
        raise click.ClickException(str(e))

    # Get records iterator
    if USE_CLP:
        records = reader.iter_records(filter_exprs)
    else:
        records = reader.iter_records()

        # Apply filter if specified
        if filter_exprs:
            try:
                predicate = build_filter_predicate(filter_exprs)
            except ValueError as e:
                raise click.ClickException(str(e))

            records = (r for r in records if predicate(r))

    # Handle grouped output
    if group_by:
        grouper = StreamingGrouper(records, group_by)

        if count:
            # Count mode - show record count per group
            counts = grouper.count_per_group()
            output = _format_counts(counts, group_by, top, output_format, not no_header)
        elif all_records:
            groups = grouper.all_per_group()
            output = _format_groups(groups, group_by, output_format, fields, no_header)
        elif tail is not None:
            groups = grouper.tail_per_group(tail)
            output = _format_groups(groups, group_by, output_format, fields, no_header)
        else:
            groups = grouper.head_per_group(head)
            output = _format_groups(groups, group_by, output_format, fields, no_header)

        _write_output(output, output_file, compress)
        return

    # Apply head/tail/all selection
    if all_records:
        selected = list(records)
    else:
        selected = select_records(records, head=head, tail=tail)

    # Check if user requested all fields
    is_all_fields = fields and fields.strip() in ("*", "all")

    # Format output based on format option
    if output_format == "ndjson" and is_all_fields:
        # NDJSON + all fields: output each record as-is without field filtering.
        # This preserves all fields including those that only appear in some records
        # (e.g., 'uregs' in UREG instructions, 'addrs'/'values' in mem_value_trace).
        output = format_records_ndjson(selected, fields=None)
    else:
        # Determine fields to display
        display_fields = get_display_fields(selected, fields)

        if output_format == "json":
            output = format_records_json(selected, display_fields)
        elif output_format == "csv":
            output = format_records_csv(
                selected, display_fields, show_header=not no_header
            )
        elif output_format == "ndjson":
            output = format_records_ndjson(selected, display_fields)
        else:  # table
            output = format_records_table(
                selected, display_fields, show_header=not no_header
            )

    _write_output(output, output_file, compress, record_count=len(selected))

```

`python/cutracer/query/clp.py`:

```py
from typing import Generator, Optional

import yscope_clp_core  # type: ignore
from cutracer.query.reader import TraceReaderBase
from yscope_clp_core import KqlQuery


class TraceReaderCLP(TraceReaderBase):
    """
    Reader for CUTracer trace files.

    Supports CLP format.
    Provides efficient iteration over trace records.

    Example:
        >>> reader = TraceReaderCLP("trace.clp")
        >>> for record in reader.iter_records():
        ...     print(record["sass"])
    """

    def __init__(self, file):
        assert file.exists(), f"Non-exist clp archive file: {file.absolute()}"
        self._archive = file

    def _filter_expr_to_clp_query(self, filter_exprs: Optional[tuple[str, ...]]) -> str:
        if not filter_exprs:
            return "*"
        result = []
        for filter_expr in filter_exprs:
            filter_array = filter_expr.split(";")
            for filter_clause in filter_array:
                if "=" not in filter_clause:
                    raise ValueError(
                        f"Invalid filter expression: '{filter_clause}'. Expected format: 'field=value'"
                    )
                lhs, _sym, rhs = filter_clause.partition("=")
                result.append((lhs, rhs))
        result_str = " AND ".join(f"{lhs}: {rhs}" for lhs, rhs in result)
        if result_str:
            return result_str
        return "*"

    def iter_records(self, filter_exprs: Optional[tuple[str, ...]] = None) -> Generator:
        clp_query = KqlQuery(self._filter_expr_to_clp_query(filter_exprs))
        with yscope_clp_core.search_archive(self._archive, clp_query) as record_iter:
            for next_record in record_iter:
                yield next_record.get_kv_pairs()

```

`python/cutracer/query/formatters.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Record formatting utilities for different output formats.

Provides functions to format trace records as table, JSON, or CSV.
All functions are pure (no side effects) and return strings.
"""

import csv
import io
import json
from typing import Optional

# Default fields to display when --fields is not specified
DEFAULT_FIELDS = ["warp", "pc", "sass"]


def format_value(value) -> str:
    """
    Format a value for display.

    Handles special cases:
    - None -> empty string
    - bool -> lowercase "true"/"false"
    - list -> comma-separated values in brackets
    - dict -> JSON string
    - other -> str()

    Args:
        value: Any value to format

    Returns:
        String representation suitable for display
    """
    if value is None:
        return ""
    if isinstance(value, bool):
        return "true" if value else "false"
    if isinstance(value, list):
        return "[" + ",".join(str(v) for v in value) + "]"
    if isinstance(value, dict):
        return json.dumps(value)
    return str(value)


def get_display_fields(
    records: list[dict], requested_fields: Optional[str] = None
) -> list[str]:
    """
    Determine which fields to display.

    Args:
        records: List of trace records
        requested_fields: Comma-separated field names, "*" or "all" for all fields, or None

    Returns:
        List of field names to display
    """
    if requested_fields:
        # Handle "*" or "all" to show all available fields
        if requested_fields.strip() in ("*", "all"):
            if records:
                # Collect union of all fields across all records.
                # Use dict.fromkeys to preserve insertion order (first record's order first).
                # This ensures fields like 'uregs' that only appear in some records
                # are included in the output.
                seen = dict.fromkeys(records[0].keys())
                for record in records[1:]:
                    for key in record:
                        if key not in seen:
                            seen[key] = None
                return list(seen)
            return DEFAULT_FIELDS
        return [f.strip() for f in requested_fields.split(",")]

    # Default: return all fields (union of all records' keys)
    if records:
        seen = dict.fromkeys(records[0].keys())
        for record in records[1:]:
            for key in record:
                if key not in seen:
                    seen[key] = None
        return list(seen)

    return DEFAULT_FIELDS


def format_records_table(
    records: list[dict],
    fields: list[str],
    show_header: bool = True,
) -> str:
    """
    Format records as a plain text table with aligned columns.

    Args:
        records: List of trace records
        fields: List of field names to display
        show_header: Whether to show the table header

    Returns:
        Formatted table string
    """
    if not records:
        return "No records found."

    # Calculate column widths
    col_widths = {}
    for field in fields:
        # Start with header width if showing header
        max_width = len(field) if show_header else 0
        for record in records:
            val_str = format_value(record.get(field, ""))
            max_width = max(max_width, len(val_str))
        col_widths[field] = max_width

    lines = []

    # Header row
    if show_header:
        header_parts = [field.upper().ljust(col_widths[field]) for field in fields]
        lines.append("  ".join(header_parts))

    # Data rows
    for record in records:
        row_parts = []
        for field in fields:
            val_str = format_value(record.get(field, ""))
            row_parts.append(val_str.ljust(col_widths[field]))
        lines.append("  ".join(row_parts))

    return "\n".join(lines)


def format_records_json(records: list[dict], fields: list[str]) -> str:
    """
    Format records as JSON.

    Args:
        records: List of trace records
        fields: List of field names to include

    Returns:
        JSON formatted string (pretty-printed array)
    """
    if not records:
        return "[]"

    # Filter to only requested fields
    filtered_records = []
    for record in records:
        filtered_record = {f: record.get(f) for f in fields if f in record}
        filtered_records.append(filtered_record)

    return json.dumps(filtered_records, indent=2)


def format_records_csv(
    records: list[dict],
    fields: list[str],
    show_header: bool = True,
) -> str:
    """
    Format records as CSV.

    Args:
        records: List of trace records
        fields: List of field names to include
        show_header: Whether to include the header row

    Returns:
        CSV formatted string
    """
    if not records:
        return ""

    output = io.StringIO(newline="")
    writer = csv.writer(output)

    if show_header:
        writer.writerow(fields)

    for record in records:
        row = [format_value(record.get(f, "")) for f in fields]
        writer.writerow(row)

    return output.getvalue().rstrip("\r\n").replace("\r\n", "\n").replace("\r", "")


def format_records_ndjson(
    records: list[dict], fields: Optional[list[str]] = None
) -> str:
    """
    Format records as NDJSON (Newline-Delimited JSON).

    Each record is output as a single JSON object on its own line.
    This format is ideal for streaming and line-by-line processing.

    Args:
        records: List of trace records
        fields: Optional list of field names to include. If None, all fields are included.

    Returns:
        NDJSON formatted string (one JSON object per line)
    """
    if not records:
        return ""

    lines = []
    for record in records:
        if fields:
            filtered_record = {f: record.get(f) for f in fields if f in record}
            lines.append(json.dumps(filtered_record))
        else:
            lines.append(json.dumps(record))

    return "\n".join(lines)

```

`python/cutracer/query/grouper.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Streaming grouper for trace record aggregation.

This module provides memory-efficient grouping of trace records,
using single-pass streaming with bounded memory per group.
"""

from collections import Counter, defaultdict, deque
from typing import Any, Iterator


class StreamingGrouper:
    """
    Stream-based grouper for trace records.

    Processes records in a single pass, maintaining bounded memory
    per group using deque for tail operations.

    Design principles:
    - Single-pass: records iterator is consumed only once
    - Bounded memory: uses deque(maxlen=N) for tail operations
    - Memory complexity: O(groups Ã— N) for head/tail, O(groups) for count

    Example:
        >>> records = iter([{"warp": 0, "pc": 16}, {"warp": 1, "pc": 32}, ...])
        >>> grouper = StreamingGrouper(records, "warp")
        >>> groups = grouper.tail_per_group(10)
        >>> for warp, records in groups.items():
        ...     print(f"Warp {warp}: {len(records)} records")
    """

    def __init__(self, records: Iterator[dict], group_field: str) -> None:
        """
        Initialize the streaming grouper.

        Args:
            records: Iterator of trace records (consumed once!)
            group_field: Field name to group by (e.g., "warp", "cta", "sass")
        """
        self._records = records
        self._group_field = group_field
        self._consumed = False

    def _ensure_not_consumed(self) -> None:
        """Raise error if records have already been consumed."""
        if self._consumed:
            raise RuntimeError(
                "StreamingGrouper records have already been consumed. "
                "Create a new grouper to process again."
            )
        self._consumed = True

    def head_per_group(self, n: int) -> dict[Any, list[dict]]:
        """
        Get first N records per group.

        Memory complexity: O(groups Ã— N)

        Args:
            n: Maximum records per group

        Returns:
            Dict mapping group key to list of first N records
        """
        self._ensure_not_consumed()

        if n <= 0:
            return {}

        groups: dict[Any, list[dict]] = defaultdict(list)
        group_counts: Counter = Counter()

        for record in self._records:
            key = record.get(self._group_field)
            if group_counts[key] < n:
                groups[key].append(record)
                group_counts[key] += 1

        return dict(groups)

    def tail_per_group(self, n: int) -> dict[Any, list[dict]]:
        """
        Get last N records per group.

        Uses deque(maxlen=N) for each group to bound memory usage.
        Memory complexity: O(groups Ã— N)

        Args:
            n: Maximum records per group

        Returns:
            Dict mapping group key to list of last N records
        """
        self._ensure_not_consumed()

        if n <= 0:
            return {}

        groups: dict[Any, deque] = defaultdict(lambda: deque(maxlen=n))

        for record in self._records:
            key = record.get(self._group_field)
            groups[key].append(record)

        return {k: list(v) for k, v in groups.items()}

    def count_per_group(self) -> dict[Any, int]:
        """
        Count records per group.

        Memory complexity: O(groups) - only stores counts, not records

        Returns:
            Dict mapping group key to record count
        """
        self._ensure_not_consumed()

        counts: Counter = Counter()

        for record in self._records:
            key = record.get(self._group_field)
            counts[key] += 1

        return dict(counts)

    def all_per_group(self) -> dict[Any, list[dict]]:
        """
        Get all records per group.

        Warning: Memory usage is unbounded - O(total records).
        Use only when you need all records and memory is sufficient.

        Returns:
            Dict mapping group key to list of all records in that group
        """
        self._ensure_not_consumed()

        groups: dict[Any, list[dict]] = defaultdict(list)

        for record in self._records:
            key = record.get(self._group_field)
            groups[key].append(record)

        return dict(groups)

```

`python/cutracer/query/reader.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Trace reader for CUTracer analysis.

This module provides the TraceReader class for reading and iterating
over trace records from NDJSON files (plain or Zstd-compressed).
"""

import json
from abc import ABC, abstractmethod
from collections import deque
from itertools import islice
from pathlib import Path
from typing import Any, Callable, Iterator, Optional, Union

from cutracer.validation.compression import detect_compression, open_trace_file


def _parse_single_filter(expr: str) -> Callable[[dict], bool]:
    """
    Parse a single 'field=value' filter expression and return a predicate.

    Values are automatically converted to int if possible.

    Args:
        expr: Single filter expression (e.g., "warp=24", "type=mem_trace")

    Returns:
        Predicate function that takes a record and returns bool

    Raises:
        ValueError: If the filter expression is invalid
    """
    if "=" not in expr:
        raise ValueError(
            f"Invalid filter expression: '{expr}'. Expected format: 'field=value'"
        )

    field, value = expr.split("=", 1)
    field = field.strip()
    value = value.strip()

    if not field:
        raise ValueError("Filter field name cannot be empty")

    # Try to convert value to int (supports hex with 0x, octal with 0o, binary with 0b)
    try:
        int_value: Any = int(value, 0)
        # Match both string and int representations for backward compatibility
        return (
            lambda record: record.get(field) == value or record.get(field) == int_value
        )
    except ValueError:
        pass

    # Try JSON parsing for complex types (list/dict), e.g. cta=[0,0,0]
    try:
        json_value = json.loads(value)
        if isinstance(json_value, (list, dict)):
            return lambda record: record.get(field) == json_value
    except (json.JSONDecodeError, ValueError):
        pass

    return lambda record: record.get(field) == value


def parse_filter_expr(filter_expr: str) -> Callable[[dict], bool]:
    """
    Parse a filter expression and return a predicate function.

    Supports simple equality filters like "field=value" and
    semicolon-separated multiple conditions combined with AND logic.

    Semicolons are used as the separator (not commas) because field values
    may contain commas (e.g., "cta=[5, 0, 0]").

    Args:
        filter_expr: Filter expression. Can be a single condition
            (e.g., "warp=24") or semicolon-separated multiple conditions
            (e.g., "pc=0x1030;warp=64").

    Returns:
        Predicate function that takes a record and returns bool

    Raises:
        ValueError: If any filter expression is invalid

    Examples:
        >>> pred = parse_filter_expr("warp=24")
        >>> pred({"warp": 24})
        True
        >>> pred = parse_filter_expr("pc=0x100;warp=24")
        >>> pred({"pc": 256, "warp": 24})
        True
        >>> pred({"pc": 256, "warp": 25})
        False
    """
    parts = [p.strip() for p in filter_expr.split(";") if p.strip()]

    if not parts:
        raise ValueError("Filter expression cannot be empty")

    if len(parts) == 1:
        return _parse_single_filter(parts[0])

    predicates = [_parse_single_filter(p) for p in parts]
    return lambda record: all(p(record) for p in predicates)


def build_filter_predicate(
    filter_exprs: tuple[str, ...],
) -> Callable[[dict], bool]:
    """
    Build a combined AND predicate from multiple filter expressions.

    Each expression is parsed via parse_filter_expr (which itself supports
    semicolon-separated conditions). Multiple expressions are combined
    with AND logic.

    Args:
        filter_exprs: One or more filter expressions (e.g., from multiple -f flags)

    Returns:
        A single predicate function that returns True only when all conditions match

    Raises:
        ValueError: If any filter expression is invalid
    """
    predicates = [parse_filter_expr(expr) for expr in filter_exprs]

    if len(predicates) == 1:
        return predicates[0]

    def combined(record: dict) -> bool:
        return all(p(record) for p in predicates)

    return combined


def select_records(
    records: Iterator[dict],
    head: Optional[int] = None,
    tail: Optional[int] = None,
) -> list[dict]:
    """
    Memory-efficient record selection using streaming.

    This function processes records in a streaming fashion:
    - For head: Uses itertools.islice to stop early after N records
    - For tail: Uses collections.deque(maxlen=N) to keep only last N records

    Memory complexity:
    - head N: O(N) - only stores N records
    - tail N: O(N) - deque automatically discards older records

    Note: The records iterator is consumed after calling this function.

    Args:
        records: Iterator of trace records (consumed once!)
        head: Number of records from the beginning (default: 10)
        tail: Number of records from the end (overrides head)

    Returns:
        Selected subset of records as a list
    """
    if tail is not None:
        if tail <= 0:
            return []
        # deque with maxlen automatically discards oldest items
        return list(deque(records, maxlen=tail))

    # Default head to 10 if not specified
    head = head if head is not None else 10
    if head <= 0:
        return []
    # islice stops iteration after head items
    return list(islice(records, head))


class TraceReaderBase(ABC):
    """
    Reader for CUTracer trace files.
    Example:
        >>> reader = TraceReader("trace.ndjson.zst")
        >>> for record in reader.iter_records():
        ...     print(record["sass"])
    """

    @abstractmethod
    def __init__(self, file_path: Union[str, Path]) -> None:
        pass

    @abstractmethod
    def iter_records(
        self, filter_exprs: Optional[tuple[str, ...]] = None
    ) -> Iterator[dict]:
        pass


class TraceReader(TraceReaderBase):
    """
    Reader for CUTracer trace files.

    Supports NDJSON format with optional Zstd compression.
    Provides efficient iteration over trace records.

    Example:
        >>> reader = TraceReader("trace.ndjson.zst")
        >>> for record in reader.iter_records():
        ...     print(record["sass"])
    """

    def __init__(self, file_path: Union[str, Path]) -> None:
        """
        Initialize the TraceReader.

        Args:
            file_path: Path to the trace file (.ndjson or .ndjson.zst)

        Raises:
            FileNotFoundError: If the file does not exist
        """
        self.file_path = Path(file_path)

        if not self.file_path.exists():
            raise FileNotFoundError(f"File not found: {self.file_path}")

        self.compression = detect_compression(self.file_path)

    def iter_records(
        self, filter_exprs: Optional[tuple[str, ...]] = None
    ) -> Iterator[dict]:
        """
        Iterate over all trace records in the file.

        Yields:
            dict: Each trace record as a dictionary

        Raises:
            json.JSONDecodeError: If a line contains invalid JSON

        Example:
            >>> reader = TraceReader("trace.ndjson")
            >>> for record in reader.iter_records():
            ...     process(record)
        """
        with open_trace_file(self.file_path) as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                yield json.loads(line)

```

`python/cutracer/query/warp_summary.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Warp execution status summary for GPU hang analysis.

This module provides utilities for analyzing warp execution status
from trace records grouped by warp ID. It identifies:
- Completed warps: executed EXIT instruction (normal termination)
- In-progress warps: did not execute EXIT (may be hung or interrupted)
- Missing warps: never appeared in trace (scheduling issues)
"""

from dataclasses import dataclass, field
from typing import Any, Optional


@dataclass
class WarpSummary:
    """Summary statistics for warp grouping."""

    total_observed: int
    min_warp_id: int
    max_warp_id: int
    completed_warp_ids: list[int] = field(default_factory=list)
    inprogress_warp_ids: list[int] = field(default_factory=list)
    missing_warp_ids: list[int] = field(default_factory=list)


def is_exit_instruction(record: dict) -> bool:
    """
    Check if a record's SASS instruction is an EXIT instruction.

    EXIT instructions can be:
    - "EXIT;"
    - "@P0 EXIT;"  (predicated)
    - "EXIT.KEEPREFCOUNT;"  (with modifier)

    Args:
        record: A trace record dictionary

    Returns:
        True if the instruction is EXIT
    """
    sass = record.get("sass", "")
    if not sass:
        return False
    return "EXIT" in sass.upper() and sass.strip().endswith(";")


def merge_to_ranges(ids: list[int]) -> list[tuple[int, int]]:
    """
    Merge consecutive IDs into ranges.

    Args:
        ids: List of integer IDs (will be sorted)

    Returns:
        List of (start, end) tuples representing ranges

    Example:
        [0, 1, 2, 3, 6, 7, 8, 9] -> [(0, 3), (6, 9)]
    """
    if not ids:
        return []

    sorted_ids = sorted(ids)
    ranges = []
    start = end = sorted_ids[0]

    for i in sorted_ids[1:]:
        if i == end + 1:
            end = i
        else:
            ranges.append((start, end))
            start = end = i

    ranges.append((start, end))
    return ranges


def format_ranges(ranges: list[tuple[int, int]]) -> str:
    """
    Format ranges as a human-readable string.

    Args:
        ranges: List of (start, end) tuples

    Returns:
        Formatted string like "0-3, 6-9, 16-127"

    Example:
        [(0, 3), (6, 9)] -> "0-3, 6-9"
        [(5, 5)] -> "5"
    """
    if not ranges:
        return "(none)"

    parts = []
    for start, end in ranges:
        if start == end:
            parts.append(str(start))
        else:
            parts.append(f"{start}-{end}")

    return ", ".join(parts)


def compute_warp_summary(groups: dict[Any, list[dict]]) -> Optional[WarpSummary]:
    """
    Compute warp summary statistics from grouped records.

    Args:
        groups: Dict mapping warp ID to list of records

    Returns:
        WarpSummary object, or None if groups is empty or warp IDs are not integers
    """
    if not groups:
        return None

    try:
        warp_ids = [int(k) for k in groups.keys()]
    except (ValueError, TypeError):
        return None

    min_warp = min(warp_ids)
    max_warp = max(warp_ids)

    completed_ids = []
    inprogress_ids = []

    for warp_id, records in groups.items():
        warp_int = int(warp_id)
        if records:
            last_record = records[-1]
            if is_exit_instruction(last_record):
                completed_ids.append(warp_int)
            else:
                inprogress_ids.append(warp_int)

    observed_set = set(warp_ids)
    all_expected = set(range(0, max_warp + 1))
    missing_ids = sorted(all_expected - observed_set)

    return WarpSummary(
        total_observed=len(groups),
        min_warp_id=min_warp,
        max_warp_id=max_warp,
        completed_warp_ids=sorted(completed_ids),
        inprogress_warp_ids=sorted(inprogress_ids),
        missing_warp_ids=missing_ids,
    )


def format_warp_summary_text(summary: WarpSummary) -> str:
    """
    Format warp summary as human-readable text.

    Args:
        summary: WarpSummary object

    Returns:
        Formatted text string suitable for terminal output
    """
    total = summary.total_observed
    completed = len(summary.completed_warp_ids)
    inprogress = len(summary.inprogress_warp_ids)
    missing = len(summary.missing_warp_ids)

    completed_pct = (completed / total * 100) if total > 0 else 0
    inprogress_pct = (inprogress / total * 100) if total > 0 else 0
    expected_count = summary.max_warp_id + 1
    missing_pct = (missing / expected_count * 100) if expected_count > 0 else 0

    completed_ranges = merge_to_ranges(summary.completed_warp_ids)
    inprogress_ranges = merge_to_ranges(summary.inprogress_warp_ids)
    missing_ranges = merge_to_ranges(summary.missing_warp_ids)

    lines = [
        "",
        "â”€" * 50,
        "Warp Summary",
        "â”€" * 50,
        f"  Total warps observed:   {total}",
        f"  Warp ID range:          {summary.min_warp_id} - {summary.max_warp_id}",
        "",
        f"  Completed (EXIT):       {completed:>6}  ({completed_pct:.1f}%)",
        f"    IDs: {format_ranges(completed_ranges)}",
        "",
        f"  In-progress:            {inprogress:>6}  ({inprogress_pct:.1f}%)",
        f"    IDs: {format_ranges(inprogress_ranges)}",
        "",
        f"  Missing (never seen):   {missing:>6}  ({missing_pct:.1f}%)",
        f"    IDs: {format_ranges(missing_ranges)}",
    ]
    return "\n".join(lines)


def warp_summary_to_dict(summary: WarpSummary) -> dict:
    """
    Convert WarpSummary to a dictionary for JSON output.

    Args:
        summary: WarpSummary object

    Returns:
        Dictionary representation suitable for JSON serialization
    """
    total = summary.total_observed
    completed = len(summary.completed_warp_ids)
    inprogress = len(summary.inprogress_warp_ids)
    missing = len(summary.missing_warp_ids)
    expected_count = summary.max_warp_id + 1

    return {
        "total_observed": total,
        "warp_id_range": [summary.min_warp_id, summary.max_warp_id],
        "completed": {
            "count": completed,
            "percentage": round(completed / total * 100, 1) if total > 0 else 0,
            "ranges": merge_to_ranges(summary.completed_warp_ids),
        },
        "in_progress": {
            "count": inprogress,
            "percentage": round(inprogress / total * 100, 1) if total > 0 else 0,
            "ranges": merge_to_ranges(summary.inprogress_warp_ids),
        },
        "missing": {
            "count": missing,
            "percentage": (
                round(missing / expected_count * 100, 1) if expected_count > 0 else 0
            ),
            "ranges": merge_to_ranges(summary.missing_warp_ids),
        },
    }

```

`python/cutracer/reduce/__init__.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
CUTracer Reduce Module.

Provides tools for reducing delay injection configurations to find minimal
sets that trigger data races.
"""

from cutracer.reduce.config_mutator import DelayConfigMutator
from cutracer.reduce.reduce import reduce_delay_points

__all__ = ["reduce_delay_points", "DelayConfigMutator"]

```

`python/cutracer/reduce/cli.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
CLI for CUTracer reduce command.

Provides command-line interface for reducing delay injection configurations.
"""

import sys
from typing import NoReturn

import click
from cutracer.reduce.config_mutator import DelayPoint
from cutracer.reduce.reduce import reduce_delay_points, ReduceConfig, ReduceResult
from cutracer.reduce.report import generate_report, save_report


def progress_callback(
    current: int, total: int, point: DelayPoint, is_essential: bool
) -> None:
    """Print progress during reduction."""
    status = "âœ“ ESSENTIAL" if is_essential else "  not essential"
    click.echo(f"[{current}/{total}] {point.sass[:50]:<50} {status}")


def _exit_with_error(
    message: str, verbose: bool = False, exception: Exception | None = None
) -> NoReturn:
    """Print error message and exit."""
    click.echo(f"\nâŒ Error: {message}", err=True)
    if verbose and exception:
        import traceback

        traceback.print_exc()
    sys.exit(1)


def _run_reduction(reduce_config: ReduceConfig, verbose: bool) -> ReduceResult:
    """Run the reduction algorithm and handle errors."""
    try:
        return reduce_delay_points(
            config=reduce_config,
            progress_callback=progress_callback if not verbose else None,
        )
    except ValueError as e:
        _exit_with_error(str(e))
    except Exception as e:
        _exit_with_error(f"Unexpected error: {e}", verbose=verbose, exception=e)


@click.command(name="reduce")
@click.option(
    "--config",
    "-c",
    required=True,
    type=click.Path(exists=True),
    help="Path to delay config JSON file (repro config).",
)
@click.option(
    "--test",
    "-t",
    required=True,
    type=str,
    help="Test script. Returns 0 if race occurs, non-zero if no race.",
)
@click.option(
    "--output",
    "-o",
    type=click.Path(),
    default="reduce_report.json",
    help="Output path for the report JSON.",
)
@click.option(
    "--minimal-config",
    "-m",
    type=click.Path(),
    default="minimal_config.json",
    help="Output path for the minimal config.",
)
@click.option(
    "--verbose",
    "-v",
    is_flag=True,
    help="Enable verbose output.",
)
@click.option(
    "--no-validate-schema",
    is_flag=True,
    help="Skip JSON schema validation of the config file.",
)
def reduce_command(
    config: str,
    test: str,
    output: str,
    minimal_config: str,
    verbose: bool,
    no_validate_schema: bool,
) -> None:
    """
    Reduce delay injection config to find minimal race trigger.

    Uses delta debugging to find the minimal set of delay points
    that trigger a data race.

    Test script convention (same as llvm-reduce):

    \b
      - Exit 0: Interesting (race occurred)
      - Exit 1+: Not interesting (no race)

    Example:

    \b
      cutraceross reduce --config repro.json --test ./test_race.sh
    """
    click.echo("=" * 60)
    click.echo(" CUTRACER REDUCE")
    click.echo("=" * 60)
    click.echo(f"Config: {config}")
    click.echo(f"Test script: {test}")
    click.echo("")

    # Create reduce config
    reduce_config = ReduceConfig(
        config_path=config,
        test_script=test,
        output_path=minimal_config,
        verbose=verbose,
        validate_schema=not no_validate_schema,
    )

    # Run reduction
    click.echo("\nğŸ” Starting reduction...\n")
    result = _run_reduction(reduce_config, verbose)

    # Generate report
    click.echo("\n" + "=" * 60)
    click.echo(" REDUCTION COMPLETE")
    click.echo("=" * 60)

    if result.essential_points:
        click.echo(f"\nâœ… Found {len(result.essential_points)} essential point(s):\n")
        for i, point in enumerate(result.essential_points, 1):
            click.echo(f"  {i}. {point.sass}")
            click.echo(f"     PC: {point.pc_offset}")
            click.echo(f"     Kernel: {point.kernel_name}")
            click.echo("")
    else:
        click.echo("\nâš ï¸  No essential points found.")
        click.echo("    This may indicate the race is not deterministically triggered")
        click.echo("    by the delay injection points in this config.")

    # Generate and save report
    report = generate_report(
        result=result,
        config_path=config,
        test_script=test,
    )
    save_report(report, output)
    click.echo(f"\nğŸ“Š Report saved to: {output}")

    if result.minimal_config_path:
        click.echo(f"ğŸ“‹ Minimal config saved to: {result.minimal_config_path}")

    # Print summary stats
    click.echo("\nğŸ“ˆ Stats:")
    click.echo(f"   Total points: {result.total_points}")
    click.echo(f"   Essential: {len(result.essential_points)}")
    click.echo(f"   Iterations: {result.iterations}")

    click.echo("")

```

`python/cutracer/reduce/config_mutator.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Delay config mutator for reduction.

Provides utilities to load, modify, and save delay injection configurations.
Uses the JSON schema from cutracer.validation for config validation.
"""

import copy
import json
import tempfile
from dataclasses import dataclass
from pathlib import Path
from typing import Optional

import jsonschema
from cutracer.validation import DELAY_CONFIG_SCHEMA


@dataclass
class DelayPoint:
    """Represents a single delay injection point."""

    kernel_key: str
    kernel_name: str
    pc_offset: str
    sass: str
    delay_ns: int
    enabled: bool

    def __repr__(self) -> str:
        status = "ON" if self.enabled else "OFF"
        return (
            f"[{status}] {self.sass} (PC: {self.pc_offset}, kernel: {self.kernel_name})"
        )


class DelayConfigMutator:
    """
    Mutator for delay injection configurations.

    Provides methods to load, modify, and save delay configs for reduction.
    Validates configs against the DELAY_CONFIG_SCHEMA on load.
    """

    def __init__(self, config_path: str, validate: bool = True):
        """
        Load a delay config from file.

        Args:
            config_path: Path to the delay config JSON file.
            validate: Whether to validate config against JSON schema (default: True).

        Raises:
            ValueError: If config fails schema validation.
            FileNotFoundError: If config file does not exist.
            json.JSONDecodeError: If config file contains invalid JSON.
        """
        self.config_path = Path(config_path)
        with open(self.config_path) as f:
            self.config = json.load(f)

        # Validate against schema
        if validate:
            try:
                jsonschema.validate(self.config, DELAY_CONFIG_SCHEMA)
            except jsonschema.ValidationError as e:
                raise ValueError(
                    f"Invalid delay config '{config_path}': {e.message}"
                ) from e

        self._delay_points: list[DelayPoint] = []
        self._parse_delay_points()

    def _parse_delay_points(self) -> None:
        """Parse delay points from the config."""
        self._delay_points = []
        for kernel_key, kernel_config in self.config.get("kernels", {}).items():
            kernel_name = kernel_config.get("kernel_name", kernel_key)
            for pc_offset, point in kernel_config.get(
                "instrumentation_points", {}
            ).items():
                self._delay_points.append(
                    DelayPoint(
                        kernel_key=kernel_key,
                        kernel_name=kernel_name,
                        pc_offset=pc_offset,
                        sass=point.get("sass", ""),
                        delay_ns=point.get("delay", 0),
                        enabled=point.get("on", False),
                    )
                )

    @property
    def delay_points(self) -> list[DelayPoint]:
        """Get all delay points."""
        return self._delay_points

    @property
    def enabled_points(self) -> list[DelayPoint]:
        """Get only enabled delay points."""
        return [p for p in self._delay_points if p.enabled]

    def set_point_enabled(self, point: DelayPoint, enabled: bool) -> None:
        """
        Enable or disable a delay point.

        Args:
            point: The delay point to modify.
            enabled: Whether to enable or disable the point.
        """
        point.enabled = enabled
        self.config["kernels"][point.kernel_key]["instrumentation_points"][
            point.pc_offset
        ]["on"] = enabled

    def set_all_enabled(self, enabled: bool) -> None:
        """Enable or disable all delay points."""
        for point in self._delay_points:
            self.set_point_enabled(point, enabled)

    def save(self, path: Optional[str] = None) -> str:
        """
        Save the config to a file.

        Args:
            path: Optional path to save to. If None, creates a temp file.

        Returns:
            Path to the saved file.
        """
        if path is None:
            fd, path = tempfile.mkstemp(suffix=".json", prefix="cutracer_bisect_")
            with open(fd, "w") as f:
                json.dump(self.config, f, indent=2)
        else:
            with open(path, "w") as f:
                json.dump(self.config, f, indent=2)
        return path

    def clone(self) -> "DelayConfigMutator":
        """Create a deep copy of this mutator."""
        new_mutator = DelayConfigMutator.__new__(DelayConfigMutator)
        new_mutator.config_path = self.config_path
        new_mutator.config = copy.deepcopy(self.config)
        new_mutator._delay_points = []
        new_mutator._parse_delay_points()
        return new_mutator

    def __len__(self) -> int:
        return len(self._delay_points)

    def __repr__(self) -> str:
        enabled = len(self.enabled_points)
        total = len(self._delay_points)
        return f"DelayConfigMutator({enabled}/{total} points enabled)"

```

`python/cutracer/reduce/reduce.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Core reduction algorithm for finding minimal race-triggering delay configurations.

Implements delta debugging to find the minimal set of delay injection points
that trigger a data race.
"""

import os
import subprocess
from dataclasses import dataclass
from typing import Callable, Optional

from cutracer.reduce.config_mutator import DelayConfigMutator, DelayPoint


@dataclass
class ReduceResult:
    """Result of a reduction run."""

    total_points: int
    essential_points: list[DelayPoint]
    iterations: int
    minimal_config_path: Optional[str] = None

    @property
    def success(self) -> bool:
        """Whether reduction found essential points."""
        return len(self.essential_points) > 0

    def summary(self) -> str:
        """Generate a summary string."""
        lines = [
            "Reduction complete!",
            f"  Total points tested: {self.total_points}",
            f"  Essential points found: {len(self.essential_points)}",
            f"  Iterations: {self.iterations}",
        ]
        if self.essential_points:
            lines.append("\nEssential delay points:")
            for i, point in enumerate(self.essential_points, 1):
                lines.append(f"  {i}. {point.sass}")
                lines.append(f"     PC: {point.pc_offset}, Kernel: {point.kernel_name}")
        return "\n".join(lines)


@dataclass
class ReduceConfig:
    """Configuration for reduction."""

    config_path: str
    test_script: str
    output_path: Optional[str] = None
    verbose: bool = False
    validate_schema: bool = True


def run_test(
    test_script: str,
    config_path: str,
    verbose: bool = False,
) -> bool:
    """
    Run the test script with a given delay config.

    Args:
        test_script: Path to the test script.
        config_path: Path to the delay config JSON file.
        verbose: Whether to print test output.

    Returns:
        True if data race occurred (exit code 0), False otherwise.
    """
    env = os.environ.copy()
    env["CUTRACER_DELAY_LOAD_PATH"] = config_path

    try:
        result = subprocess.run(
            test_script,
            shell=True,
            env=env,
            capture_output=not verbose,
            timeout=300,  # 5 minute timeout
        )
        return result.returncode == 0  # 0 = bad (race), non-zero = good
    except subprocess.TimeoutExpired:
        if verbose:
            print("  Test timed out (treating as no race)")
        return False
    except Exception as e:
        if verbose:
            print(f"  Test error: {e}")
        return False


def _cleanup_temp_file(path: str) -> None:
    """Safely remove a temporary file."""
    try:
        os.unlink(path)
    except OSError:
        pass


def _find_point_in_mutator(
    mutator: DelayConfigMutator, kernel_key: str, pc_offset: str
) -> Optional[DelayPoint]:
    """Find a matching point in a mutator by kernel_key and pc_offset."""
    for p in mutator.enabled_points:
        if p.kernel_key == kernel_key and p.pc_offset == pc_offset:
            return p
    return None


def _test_point_essentiality(
    point: DelayPoint,
    mutator: DelayConfigMutator,
    test_script: str,
    verbose: bool,
) -> tuple[bool, str]:
    """
    Test if a point is essential by disabling it and running the test.

    Returns:
        Tuple of (is_essential, temp_config_path).
    """
    test_mutator = mutator.clone()
    cloned_point = _find_point_in_mutator(
        test_mutator, point.kernel_key, point.pc_offset
    )
    if cloned_point:
        test_mutator.set_point_enabled(cloned_point, False)

    test_config_path = test_mutator.save()
    race_occurs = run_test(test_script, test_config_path, verbose)
    is_essential = not race_occurs

    return is_essential, test_config_path


def _validate_initial_config(
    mutator: DelayConfigMutator,
    config: ReduceConfig,
) -> str:
    """
    Validate that the initial config triggers a race.

    Returns:
        Path to the saved initial config.

    Raises:
        ValueError: If no enabled points or initial config doesn't trigger race.
    """
    enabled_points = mutator.enabled_points

    if not enabled_points:
        raise ValueError("No enabled delay points in config")

    if config.verbose:
        print(f"Loaded {len(enabled_points)} enabled delay points")

    initial_config_path = mutator.save()
    if not run_test(config.test_script, initial_config_path, config.verbose):
        raise ValueError(
            "Initial config does not trigger the race! "
            "Test script must return 0 (race) with the original config."
        )

    if config.verbose:
        print("âœ“ Initial config triggers the race")

    return initial_config_path


def reduce_delay_points(
    config: ReduceConfig,
    progress_callback: Optional[Callable[[int, int, DelayPoint, bool], None]] = None,
) -> ReduceResult:
    """
    Find minimal set of delay points that trigger a data race.

    Uses delta debugging algorithm:
    1. Start with all enabled points
    2. For each point, disable it and test
    3. If race still occurs, the point is not essential
    4. If race disappears, the point is essential

    Args:
        config: Reduction configuration.
        progress_callback: Optional callback for progress updates.
            Called with (current_iteration, total, point, is_essential).

    Returns:
        ReduceResult with essential points and statistics.
    """
    mutator = DelayConfigMutator(config.config_path, validate=config.validate_schema)
    initial_config_path = _validate_initial_config(mutator, config)

    points_to_test = list(mutator.enabled_points)
    essential_points: list[DelayPoint] = []
    iteration = 0

    for point in points_to_test:
        iteration += 1

        if config.verbose:
            print(f"\n[{iteration}/{len(points_to_test)}] Testing: {point.sass}")

        is_essential, test_config_path = _test_point_essentiality(
            point, mutator, config.test_script, config.verbose
        )

        if is_essential:
            if config.verbose:
                print("   â†’ Race disappears (point IS essential)")
            essential_points.append(point)
        else:
            if config.verbose:
                print("   â†’ Race still occurs (point NOT essential)")
            # Update mutator to keep this point disabled
            original_point = _find_point_in_mutator(
                mutator, point.kernel_key, point.pc_offset
            )
            if original_point:
                mutator.set_point_enabled(original_point, False)

        if progress_callback:
            progress_callback(iteration, len(points_to_test), point, is_essential)

        _cleanup_temp_file(test_config_path)

    # Save minimal config if output path specified
    minimal_config_path = None
    if config.output_path:
        minimal_config_path = mutator.save(config.output_path)
        if config.verbose:
            print(f"\nMinimal config saved to: {minimal_config_path}")

    _cleanup_temp_file(initial_config_path)

    return ReduceResult(
        total_points=len(points_to_test),
        essential_points=essential_points,
        iterations=iteration,
        minimal_config_path=minimal_config_path,
    )

```

`python/cutracer/reduce/report.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Report generator for reduction results.

Generates human-readable and JSON reports from reduction results.
"""

import json
from datetime import datetime
from typing import Any, Optional

from cutracer.reduce.reduce import ReduceResult


def generate_report(
    result: ReduceResult,
    config_path: str,
    test_script: str,
    source_path: Optional[str] = None,
) -> dict[str, Any]:
    """
    Generate a detailed report from reduction results.

    Args:
        result: The reduction result.
        config_path: Path to the original config file.
        test_script: Path to the test script used.
        source_path: Optional path to source code.

    Returns:
        Dictionary containing the full report.
    """
    report = {
        "metadata": {
            "timestamp": datetime.now().isoformat(),
            "config_file": str(config_path),
            "test_script": str(test_script),
            "source_file": source_path,
        },
        "summary": {
            "total_points": result.total_points,
            "essential_points": len(result.essential_points),
            "iterations": result.iterations,
        },
        "essential_delay_points": [
            {
                "kernel_key": p.kernel_key,
                "kernel_name": p.kernel_name,
                "pc_offset": p.pc_offset,
                "sass": p.sass,
                "delay_ns": p.delay_ns,
            }
            for p in result.essential_points
        ],
        "minimal_config_path": result.minimal_config_path,
    }

    return report


def save_report(report: dict[str, Any], output_path: str) -> None:
    """
    Save report to a JSON file.

    Args:
        report: The report dictionary.
        output_path: Path to save the report.
    """
    with open(output_path, "w") as f:
        json.dump(report, f, indent=2)


def format_report_text(report: dict[str, Any]) -> str:
    """
    Format report as human-readable text.

    Args:
        report: The report dictionary.

    Returns:
        Formatted text string.
    """
    lines = []

    # Header
    lines.append("=" * 60)
    lines.append(" CUTRACER REDUCE REPORT")
    lines.append("=" * 60)
    lines.append("")

    # Metadata
    lines.append(f"Timestamp: {report['metadata']['timestamp']}")
    lines.append(f"Config: {report['metadata']['config_file']}")
    lines.append(f"Test script: {report['metadata']['test_script']}")
    if report["metadata"].get("source_file"):
        lines.append(f"Source: {report['metadata']['source_file']}")
    lines.append("")

    # Summary
    summary = report["summary"]
    lines.append("SUMMARY")
    lines.append("-" * 40)
    lines.append(f"  Total points tested: {summary['total_points']}")
    lines.append(f"  Essential points found: {summary['essential_points']}")
    lines.append(f"  Iterations: {summary['iterations']}")
    lines.append("")

    # Essential points
    essential = report.get("essential_delay_points", [])
    if essential:
        lines.append("ESSENTIAL DELAY POINTS")
        lines.append("-" * 40)
        for i, point in enumerate(essential, 1):
            lines.append(f"  {i}. {point['sass']}")
            lines.append(f"     PC: {point['pc_offset']}")
            lines.append(f"     Kernel: {point['kernel_name']}")
            lines.append("")

    # Minimal config
    if report.get("minimal_config_path"):
        lines.append(f"Minimal config saved to: {report['minimal_config_path']}")

    lines.append("")
    lines.append("=" * 60)

    return "\n".join(lines)

```

`python/cutracer/shared_vars.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""Shared variables and utility functions for CUTracer."""

import importlib.util


def is_fbcode():
    """Check if running in fbcode environment."""
    return importlib.util.find_spec("cutracer.analyze.fb") is not None

```

`python/cutracer/validation/__init__.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
CUTracer validation module.

This module provides validation functions for trace files in different formats:
- JSON validation (syntax and schema)
- Text format validation
- Cross-format consistency checking
- Compression handling (Zstd)
"""

from .compression import (
    detect_compression,
    get_file_size,
    get_trace_format,
    iter_lines,
    open_trace_file,
)
from .consistency import (
    compare_record_counts,
    compare_trace_content,
    compare_trace_formats,
    get_trace_statistics,
)
from .json_validator import (
    JsonValidationError,
    validate_json_schema,
    validate_json_syntax,
    validate_json_trace,
)
from .schema_loader import (
    DELAY_CONFIG_SCHEMA,
    MEM_ACCESS_SCHEMA,
    OPCODE_ONLY_SCHEMA,
    REG_INFO_SCHEMA,
    SCHEMAS_BY_TYPE,
)
from .text_validator import (
    parse_text_trace_record,
    TextValidationError,
    validate_text_format,
    validate_text_trace,
)

__all__ = [
    # Compression utilities
    "detect_compression",
    "get_trace_format",
    "open_trace_file",
    "iter_lines",
    "get_file_size",
    # JSON validation
    "validate_json_syntax",
    "validate_json_schema",
    "validate_json_trace",
    "JsonValidationError",
    # Text validation
    "validate_text_format",
    "validate_text_trace",
    "parse_text_trace_record",
    "TextValidationError",
    # Cross-format consistency
    "compare_record_counts",
    "compare_trace_content",
    "compare_trace_formats",
    "get_trace_statistics",
    # Schemas
    "REG_INFO_SCHEMA",
    "MEM_ACCESS_SCHEMA",
    "OPCODE_ONLY_SCHEMA",
    "DELAY_CONFIG_SCHEMA",
    "SCHEMAS_BY_TYPE",
]

```

`python/cutracer/validation/cli.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
CLI implementation for the validate and compare subcommands.

This module provides command-line interface for validating CUTracer trace files
and comparing trace formats for cross-format consistency.
"""

import json
import sys
from pathlib import Path
from typing import Any

import click

from .consistency import compare_trace_formats
from .json_validator import validate_json_trace
from .text_validator import validate_text_trace


def _detect_format(file_path: Path) -> str:
    """Auto-detect file format from extension."""
    suffixes = "".join(file_path.suffixes).lower()
    if ".ndjson" in suffixes:
        return "json"
    elif file_path.suffix == ".log":
        return "text"
    else:
        return "unknown"


def _format_size(size_bytes: int) -> str:
    """Format file size for display."""
    if size_bytes < 1024:
        return f"{size_bytes} B"
    elif size_bytes < 1024 * 1024:
        return f"{size_bytes / 1024:.1f} KB"
    else:
        return f"{size_bytes / (1024 * 1024):.1f} MB"


def _format_trace_format(result: dict[str, Any]) -> str:
    """Format trace format for display based on result."""
    compression = result.get("compression", "none")
    message_type = result.get("message_type")

    if compression == "zstd":
        return "NDJSON + Zstd"
    elif message_type:
        return "NDJSON"
    else:
        return "Text"


def _print_validation_result(result: dict[str, Any], verbose: bool = False) -> None:
    """Print validation result in human-readable format."""
    if result["valid"]:
        click.echo("\u2705 Valid trace file")
        click.echo(f"   Format:       {_format_trace_format(result)}")
        click.echo(f"   Records:      {result['record_count']}")
        if result.get("message_type"):
            click.echo(f"   Message type: {result['message_type']}")
        if result.get("file_size"):
            click.echo(f"   File size:    {_format_size(result['file_size'])}")
        if verbose and result.get("compression") == "zstd":
            click.echo("   Compression:  zstd")
    else:
        click.echo("\u274c Validation failed")
        for error in result.get("errors", []):
            click.echo(f"   {error}")


@click.command(name="validate")
@click.argument("file", type=click.Path(exists=True, path_type=Path))
@click.option(
    "--format",
    "-f",
    "file_format",
    type=click.Choice(["json", "text", "auto"]),
    default="auto",
    help="File format. Default: auto-detect from extension.",
)
@click.option(
    "--quiet",
    "-q",
    is_flag=True,
    help="Quiet mode. Only return exit code.",
)
@click.option(
    "--json",
    "json_output",
    is_flag=True,
    help="Output results in JSON format.",
)
@click.option(
    "--verbose",
    "-v",
    is_flag=True,
    help="Verbose output with additional details.",
)
def validate_command(
    file: Path,
    file_format: str,
    quiet: bool,
    json_output: bool,
    verbose: bool,
) -> None:
    """Validate a CUTracer trace file.

    Checks syntax and schema compliance for NDJSON, Zstd-compressed,
    and text format trace files.

    FILE is the path to the trace file to validate.
    """
    file_path = file

    # Detect format
    if file_format == "auto":
        file_format = _detect_format(file_path)
        if file_format == "unknown":
            if not quiet:
                click.echo(
                    f"Error: Cannot auto-detect format for {file_path}. "
                    "Use --format to specify.",
                    err=True,
                )
            sys.exit(2)

    # Run validation
    if file_format == "json":
        result = validate_json_trace(file_path)
    else:
        result = validate_text_trace(file_path)

    # Handle quiet mode
    if quiet:
        sys.exit(0 if result["valid"] else 1)

    # Handle JSON output
    if json_output:
        # Convert Path objects to strings for JSON serialization
        output = {k: str(v) if isinstance(v, Path) else v for k, v in result.items()}
        click.echo(json.dumps(output, indent=2))
        sys.exit(0 if result["valid"] else 1)

    # Human-readable output
    _print_validation_result(result, verbose)

    sys.exit(0 if result["valid"] else 1)


@click.command(name="compare")
@click.argument("text_file", type=click.Path(exists=True, path_type=Path))
@click.argument("json_file", type=click.Path(exists=True, path_type=Path))
@click.option(
    "--quiet",
    "-q",
    is_flag=True,
    help="Quiet mode. Only return exit code.",
)
@click.option(
    "--json",
    "json_output",
    is_flag=True,
    help="Output results in JSON format.",
)
def compare_command(
    text_file: Path,
    json_file: Path,
    quiet: bool,
    json_output: bool,
) -> None:
    """Compare text and JSON trace formats for cross-format consistency.

    Validates both files and compares record counts and statistical content
    (unique CTAs, warps, SASS instructions).

    TEXT_FILE is the path to the text trace file (.log).
    JSON_FILE is the path to the JSON trace file (.ndjson or .ndjson.zst).
    """
    result = compare_trace_formats(text_file, json_file)

    if quiet:
        sys.exit(0 if result["consistent"] else 1)

    if json_output:
        output = {k: str(v) if isinstance(v, Path) else v for k, v in result.items()}
        click.echo(json.dumps(output, indent=2))
        sys.exit(0 if result["consistent"] else 1)

    # Human-readable output
    click.echo(f"Text: {text_file.name}")
    click.echo(f"JSON: {json_file.name}")
    click.echo()

    click.echo(f"Text records: {result['text_records']}")
    click.echo(f"JSON records: {result['json_records']}")
    click.echo(f"Unique CTAs:  {result.get('unique_ctas_count', 'N/A')}")
    click.echo(f"Unique warps: {result.get('unique_warps_count', 'N/A')}")
    click.echo(f"Unique SASS:  {result.get('unique_sass_count', 'N/A')}")
    click.echo()

    if result["consistent"]:
        click.echo("\u2705 Formats are consistent")
    else:
        click.echo("\u274c Inconsistencies found:")
        for diff in result.get("differences", []):
            click.echo(f"   {diff}")

    sys.exit(0 if result["consistent"] else 1)

```

`python/cutracer/validation/compression.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Compression utilities for CUTracer trace files.

This module provides transparent handling of compressed trace files,
supporting Zstd compression (Mode 1) used by CUTracer.

CUTracer's Zstd format:
- Multiple independent Zstd frames appended together
- Each frame contains ~1MB of uncompressed NDJSON
- Compression level: 22 (maximum)
"""

import io
from contextlib import contextmanager
from pathlib import Path
from typing import Iterator, TextIO, Union

import zstandard as zstd

# Zstd magic number (little-endian): 0xFD2FB528
ZSTD_MAGIC = b"\x28\xb5\x2f\xfd"


def detect_compression(filepath: Union[str, Path]) -> str:
    """
    Detect compression format of a file.

    Uses magic number detection for reliability (works regardless of extension).

    Args:
        filepath: Path to the file to check

    Returns:
        Compression type: "zstd" or "none"

    Raises:
        FileNotFoundError: If file does not exist
    """
    filepath = Path(filepath)

    if not filepath.exists():
        raise FileNotFoundError(f"File not found: {filepath}")

    # Check magic number (works for all files regardless of extension)
    with open(filepath, "rb") as f:
        magic = f.read(4)
        if magic == ZSTD_MAGIC:
            return "zstd"

    return "none"


def get_trace_format(filepath: Union[str, Path]) -> tuple[str, str]:
    """
    Determine the base format and compression of a trace file.

    Args:
        filepath: Path to the trace file

    Returns:
        Tuple of (base_format, compression) where:
            - base_format: "ndjson", "text", or "unknown"
            - compression: "zstd" or "none"

    Examples:
        >>> get_trace_format("trace.ndjson")
        ('ndjson', 'none')
        >>> get_trace_format("trace.ndjson.zst")
        ('ndjson', 'zstd')
        >>> get_trace_format("trace.log")
        ('text', 'none')
    """
    filepath = Path(filepath)
    compression = detect_compression(filepath)

    # Determine base format from extension
    suffixes = "".join(filepath.suffixes).lower()

    if ".ndjson" in suffixes:
        return ("ndjson", compression)
    elif suffixes.endswith(".log"):
        return ("text", compression)
    else:
        return ("unknown", compression)


@contextmanager
def open_trace_file(filepath: Union[str, Path]) -> Iterator[TextIO]:
    """
    Open a trace file, automatically handling compression.

    This is a context manager that returns a text stream for reading.
    Compression is detected automatically and handled transparently.

    Args:
        filepath: Path to the trace file

    Yields:
        Text stream for reading the file contents

    Raises:
        FileNotFoundError: If file does not exist

    Example:
        >>> with open_trace_file("trace.ndjson.zst") as f:
        ...     for line in f:
        ...         process(line)
    """
    filepath = Path(filepath)

    if not filepath.exists():
        raise FileNotFoundError(f"File not found: {filepath}")

    compression = detect_compression(filepath)

    if compression == "zstd":
        # Use zstandard's stream_reader which handles multiple frames
        dctx = zstd.ZstdDecompressor()
        with open(filepath, "rb") as binary_file:
            with dctx.stream_reader(binary_file) as reader:
                with io.TextIOWrapper(reader, encoding="utf-8") as text_stream:
                    yield text_stream
    else:
        # Plain text file
        with open(filepath, "r", encoding="utf-8") as f:
            yield f


def iter_lines(filepath: Union[str, Path]) -> Iterator[str]:
    """
    Iterate over lines in a trace file, handling compression transparently.

    This is a memory-efficient way to process large trace files line by line.

    Args:
        filepath: Path to the trace file

    Yields:
        Lines from the file (stripped of trailing newlines)

    Raises:
        FileNotFoundError: If file does not exist

    Example:
        >>> for line in iter_lines("trace.ndjson.zst"):
        ...     record = json.loads(line)
    """
    with open_trace_file(filepath) as f:
        for line in f:
            yield line.rstrip("\n\r")


def get_file_size(filepath: Union[str, Path], compressed: bool = True) -> int:
    """
    Get the size of a trace file.

    Args:
        filepath: Path to the trace file
        compressed: If True, return compressed size; if False, return
                   uncompressed size (requires reading the entire file
                   for compressed files)

    Returns:
        File size in bytes

    Raises:
        FileNotFoundError: If file does not exist
    """
    filepath = Path(filepath)

    if not filepath.exists():
        raise FileNotFoundError(f"File not found: {filepath}")

    if compressed:
        return filepath.stat().st_size

    # For uncompressed size of compressed files, we need to read through
    compression = detect_compression(filepath)
    if compression == "none":
        return filepath.stat().st_size

    # Read raw bytes directly for accurate size calculation
    total_size = 0
    dctx = zstd.ZstdDecompressor()
    with open(filepath, "rb") as f:
        with dctx.stream_reader(f) as reader:
            while True:
                chunk = reader.read(65536)  # 64KB chunks
                if not chunk:
                    break
                total_size += len(chunk)

    return total_size

```

`python/cutracer/validation/consistency.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Cross-format consistency checker for CUTracer traces.

This module provides functions to compare trace files in different formats
(text vs NDJSON) for data consistency. Supports Zstd-compressed files.

Because Mode 0 (text) and Mode 2 (NDJSON) are produced by separate GPU
executions, per-record ordering is non-deterministic due to GPU warp
scheduling. Instead of per-record comparison, this module uses statistical
aggregation (record counts, unique CTA/warp/SASS sets) to verify that
both code paths serialize the same logical data correctly.
"""

import json
import re
from pathlib import Path
from typing import Any, Dict, Set, Tuple, Union

from .compression import detect_compression, open_trace_file
from .text_validator import MEM_ACCESS_HEADER_PATTERN, REG_INFO_HEADER_PATTERN


def compare_record_counts(
    text_metadata: Dict[str, Any], json_metadata: Dict[str, Any], tolerance: float = 0.0
) -> bool:
    """
    Compare record counts between text and JSON formats.

    Args:
        text_metadata: Metadata from validate_text_trace()
        json_metadata: Metadata from validate_json_trace()
        tolerance: Allowed difference as fraction (default 0%)

    Returns:
        True if counts are within tolerance

    Raises:
        ValueError: If metadata is invalid or missing required fields
    """
    if "record_count" not in text_metadata:
        raise ValueError("text_metadata missing 'record_count' field")
    if "record_count" not in json_metadata:
        raise ValueError("json_metadata missing 'record_count' field")

    text_count = text_metadata["record_count"]
    json_count = json_metadata["record_count"]

    if text_count == 0 and json_count == 0:
        return True

    if text_count == 0 or json_count == 0:
        return False

    # Calculate relative difference
    max_count = max(text_count, json_count)
    diff = abs(text_count - json_count)
    relative_diff = diff / max_count

    return relative_diff <= tolerance


def _extract_json_stats(
    json_file: Path,
) -> Dict[str, Any]:
    """
    Extract statistical summary from an NDJSON trace file.

    Supports Zstd-compressed files via open_trace_file().

    Returns:
        Dictionary with type_counts, unique_ctas, unique_warps, unique_sass.
    """
    type_counts: Dict[str, int] = {}
    unique_ctas: Set[Tuple[int, ...]] = set()
    unique_warps: Set[int] = set()
    unique_sass: Set[str] = set()

    with open_trace_file(json_file) as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            record = json.loads(line)

            msg_type = record.get("type", "unknown")
            if msg_type == "kernel_metadata":
                continue

            type_counts[msg_type] = type_counts.get(msg_type, 0) + 1
            if "cta" in record:
                unique_ctas.add(tuple(record["cta"]))
            if "warp" in record:
                unique_warps.add(record["warp"])
            if "sass" in record:
                unique_sass.add(record["sass"])

    return {
        "type_counts": type_counts,
        "unique_ctas": unique_ctas,
        "unique_warps": unique_warps,
        "unique_sass": unique_sass,
    }


# Regex with capture groups for extracting fields from reg_info header lines
_REG_HEADER_FIELDS = re.compile(
    r"^CTX\s+0x[0-9a-fA-F]+\s+-\s+CTA\s+(\d+),(\d+),(\d+)\s+-\s+"
    r"warp\s+(\d+)\s+-\s+(.+):$"
)

# Regex with capture groups for extracting fields from mem_access header lines
_MEM_HEADER_FIELDS = re.compile(
    r"^CTX\s+0x[0-9a-fA-F]+\s+-\s+kernel_launch_id\s+\d+\s+-\s+"
    r"CTA\s+(\d+),(\d+),(\d+)\s+-\s+warp\s+(\d+)\s+-\s+PC\s+\d+\s+-\s+(.+):$"
)


def _extract_text_stats(
    text_file: Path,
) -> Dict[str, Any]:
    """
    Extract statistical summary from a text-format trace file.

    Returns:
        Dictionary with type_counts, unique_ctas, unique_warps, unique_sass.
    """
    type_counts: Dict[str, int] = {}
    unique_ctas: Set[Tuple[int, ...]] = set()
    unique_warps: Set[int] = set()
    unique_sass: Set[str] = set()

    with open(text_file, "r", encoding="utf-8") as f:
        for line in f:
            m = _REG_HEADER_FIELDS.match(line)
            if m:
                cx, cy, cz, warp, sass = m.groups()
                type_counts["reg_trace"] = type_counts.get("reg_trace", 0) + 1
                unique_ctas.add((int(cx), int(cy), int(cz)))
                unique_warps.add(int(warp))
                unique_sass.add(sass.strip())
                continue

            m = _MEM_HEADER_FIELDS.match(line)
            if m:
                cx, cy, cz, warp, sass = m.groups()
                type_counts["mem_trace"] = type_counts.get("mem_trace", 0) + 1
                unique_ctas.add((int(cx), int(cy), int(cz)))
                unique_warps.add(int(warp))
                unique_sass.add(sass.strip())

    return {
        "type_counts": type_counts,
        "unique_ctas": unique_ctas,
        "unique_warps": unique_warps,
        "unique_sass": unique_sass,
    }


def compare_trace_content(
    text_file: Union[str, Path],
    json_file: Union[str, Path],
) -> Dict[str, Any]:
    """
    Compare trace content using statistical aggregation.

    Because Mode 0 (text) and Mode 2 (NDJSON) come from separate GPU
    executions, per-record ordering is non-deterministic. This function
    compares aggregate statistics that must be identical across runs of
    the same kernel binary:
      - Record counts by type
      - Set of unique CTA coordinates
      - Set of unique warp IDs
      - Set of unique SASS instructions

    Supports Zstd-compressed JSON files.

    Args:
        text_file: Path to text trace file
        json_file: Path to NDJSON trace file (supports .ndjson.zst)

    Returns:
        Dictionary containing:
            - consistent: bool - Whether statistics are consistent
            - differences: List[str] - Differences found
            - text_stats: Dict - Statistics extracted from text file
            - json_stats: Dict - Statistics extracted from JSON file

    Raises:
        FileNotFoundError: If either file does not exist
    """
    text_file = Path(text_file)
    json_file = Path(json_file)

    if not text_file.exists():
        raise FileNotFoundError(f"Text file not found: {text_file}")
    if not json_file.exists():
        raise FileNotFoundError(f"JSON file not found: {json_file}")

    result: Dict[str, Any] = {
        "consistent": True,
        "differences": [],
        "text_stats": {},
        "json_stats": {},
    }

    try:
        text_stats = _extract_text_stats(text_file)
        json_stats = _extract_json_stats(json_file)
        result["text_stats"] = text_stats
        result["json_stats"] = json_stats

        # 1. Compare record counts by type
        if text_stats["type_counts"] != json_stats["type_counts"]:
            result["differences"].append(
                f"Type counts mismatch: text={text_stats['type_counts']}, "
                f"json={json_stats['type_counts']}"
            )
            result["consistent"] = False

        # 2. Compare unique CTA set
        if text_stats["unique_ctas"] != json_stats["unique_ctas"]:
            text_only = text_stats["unique_ctas"] - json_stats["unique_ctas"]
            json_only = json_stats["unique_ctas"] - text_stats["unique_ctas"]
            result["differences"].append(
                f"CTA set mismatch: text_only={text_only}, json_only={json_only}"
            )
            result["consistent"] = False

        # 3. Compare unique warp set
        if text_stats["unique_warps"] != json_stats["unique_warps"]:
            text_only = text_stats["unique_warps"] - json_stats["unique_warps"]
            json_only = json_stats["unique_warps"] - text_stats["unique_warps"]
            result["differences"].append(
                f"Warp set mismatch: text_only={text_only}, json_only={json_only}"
            )
            result["consistent"] = False

        # 4. Compare unique SASS instruction set
        if text_stats["unique_sass"] != json_stats["unique_sass"]:
            text_only = text_stats["unique_sass"] - json_stats["unique_sass"]
            json_only = json_stats["unique_sass"] - text_stats["unique_sass"]
            result["differences"].append(
                f"SASS set mismatch: text_only={text_only}, json_only={json_only}"
            )
            result["consistent"] = False

    except Exception as e:
        result["differences"].append(f"Error during comparison: {str(e)}")
        result["consistent"] = False

    return result


def compare_trace_formats(
    text_file: Path, json_file: Path, tolerance: float = 0.0
) -> Dict[str, Any]:
    """
    Comprehensive comparison of two trace formats.

    Performs record count comparison and statistical content comparison.

    Args:
        text_file: Path to text trace file
        json_file: Path to NDJSON trace file
        tolerance: Allowed difference for record count (default: 0%)

    Returns:
        Dictionary containing:
            - consistent: bool - Overall consistency
            - record_count_match: bool - Whether counts match
            - content_match: bool - Whether statistical content matches
            - text_records: int - Number of text records
            - json_records: int - Number of JSON records
            - unique_ctas_count: int - Number of unique CTAs
            - unique_warps_count: int - Number of unique warps
            - unique_sass_count: int - Number of unique SASS instructions
            - differences: List[str] - All differences found

    Raises:
        FileNotFoundError: If either file does not exist
    """
    from .json_validator import validate_json_trace
    from .text_validator import validate_text_trace

    if not text_file.exists():
        raise FileNotFoundError(f"Text file not found: {text_file}")
    if not json_file.exists():
        raise FileNotFoundError(f"JSON file not found: {json_file}")

    result: Dict[str, Any] = {
        "consistent": False,
        "record_count_match": False,
        "content_match": False,
        "text_records": 0,
        "json_records": 0,
        "unique_ctas_count": 0,
        "unique_warps_count": 0,
        "unique_sass_count": 0,
        "differences": [],
    }

    try:
        # Validate both files
        text_metadata = validate_text_trace(text_file)
        json_metadata = validate_json_trace(json_file)

        result["text_records"] = text_metadata["record_count"]
        result["json_records"] = json_metadata["record_count"]

        # Check if validation passed
        if not text_metadata["valid"]:
            result["differences"].append(
                f"Text file validation failed: {text_metadata['errors']}"
            )
            return result

        if not json_metadata["valid"]:
            result["differences"].append(
                f"JSON file validation failed: {json_metadata['errors']}"
            )
            return result

        # Compare record counts
        count_match = compare_record_counts(
            text_metadata, json_metadata, tolerance=tolerance
        )
        result["record_count_match"] = count_match

        if not count_match:
            diff = abs(result["text_records"] - result["json_records"])
            result["differences"].append(
                f"Record count mismatch: text={result['text_records']}, "
                f"json={result['json_records']}, diff={diff}"
            )

        # Compare content via statistical aggregation
        content_result = compare_trace_content(text_file, json_file)
        result["content_match"] = content_result["consistent"]

        # Populate stats counts from content result
        json_stats = content_result.get("json_stats", {})
        result["unique_ctas_count"] = len(json_stats.get("unique_ctas", set()))
        result["unique_warps_count"] = len(json_stats.get("unique_warps", set()))
        result["unique_sass_count"] = len(json_stats.get("unique_sass", set()))

        if not content_result["consistent"]:
            result["differences"].extend(content_result["differences"])

        # Overall consistency
        result["consistent"] = count_match and content_result["consistent"]

    except Exception as e:
        result["differences"].append(f"Comparison error: {str(e)}")

    return result


def get_trace_statistics(filepath: Union[str, Path]) -> Dict[str, Any]:
    """
    Extract statistics from a trace file.

    Works with both text and JSON formats. Supports Zstd-compressed JSON files.

    Args:
        filepath: Path to trace file

    Returns:
        Dictionary containing:
            - format: str - "text" or "json"
            - compression: str - "zstd" or "none"
            - record_count: int
            - file_size: int
            - message_types: Dict[str, int] - Count by message type
            - unique_ctxs: int - Number of unique contexts
            - unique_warps: int - Number of unique warps

    Raises:
        FileNotFoundError: If file does not exist
        ValueError: If file format is not recognized
    """
    filepath = Path(filepath)

    if not filepath.exists():
        raise FileNotFoundError(f"File not found: {filepath}")

    compression = detect_compression(filepath)

    stats: Dict[str, Any] = {
        "format": None,
        "compression": compression,
        "record_count": 0,
        "file_size": filepath.stat().st_size,
        "message_types": {},
        "unique_ctxs": set(),
        "unique_warps": set(),
    }

    # Determine format by extension
    suffixes = "".join(filepath.suffixes).lower()

    if ".ndjson" in suffixes:
        stats["format"] = "json"
        with open_trace_file(filepath) as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    record = json.loads(line)
                    stats["record_count"] += 1

                    msg_type = record.get("type", "unknown")
                    stats["message_types"][msg_type] = (
                        stats["message_types"].get(msg_type, 0) + 1
                    )

                    if "ctx" in record:
                        stats["unique_ctxs"].add(record["ctx"])
                    if "warp" in record:
                        stats["unique_warps"].add(record["warp"])

                except json.JSONDecodeError:
                    pass

    elif filepath.suffix == ".log":
        stats["format"] = "text"
        with open(filepath, "r", encoding="utf-8") as f:
            for line in f:
                if REG_INFO_HEADER_PATTERN.match(line):
                    stats["record_count"] += 1
                    stats["message_types"]["reg_info"] = (
                        stats["message_types"].get("reg_info", 0) + 1
                    )

                    # Extract context
                    ctx_match = re.search(r"CTX\s+(0x[0-9a-fA-F]+)", line)
                    if ctx_match:
                        stats["unique_ctxs"].add(ctx_match.group(1))

                    # Extract warp
                    warp_match = re.search(r"warp\s+(\d+)", line)
                    if warp_match:
                        stats["unique_warps"].add(int(warp_match.group(1)))

                elif MEM_ACCESS_HEADER_PATTERN.match(line):
                    stats["record_count"] += 1
                    stats["message_types"]["mem_access"] = (
                        stats["message_types"].get("mem_access", 0) + 1
                    )

                    ctx_match = re.search(r"CTX\s+(0x[0-9a-fA-F]+)", line)
                    if ctx_match:
                        stats["unique_ctxs"].add(ctx_match.group(1))

                    warp_match = re.search(r"warp\s+(\d+)", line)
                    if warp_match:
                        stats["unique_warps"].add(int(warp_match.group(1)))
    else:
        raise ValueError(f"Unknown file format: {filepath.suffix}")

    # Convert sets to counts
    stats["unique_ctxs"] = len(stats["unique_ctxs"])
    stats["unique_warps"] = len(stats["unique_warps"])

    return stats

```

`python/cutracer/validation/json_validator.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
JSON validator for CUTracer NDJSON trace files.

This module provides functions to validate NDJSON trace files produced by
CUTracer for syntax correctness and schema compliance. Supports both
uncompressed (.ndjson) and Zstd-compressed (.ndjson.zst) files.
"""

import json
from pathlib import Path
from typing import Any, Dict, List, Tuple, Union

import jsonschema

from .compression import detect_compression, open_trace_file
from .schema_loader import SCHEMAS_BY_TYPE


class JsonValidationError(Exception):
    """Exception raised for JSON validation errors."""

    pass


def validate_json_syntax(
    filepath: Union[str, Path],
) -> Tuple[int, List[str]]:
    """
    Validate JSON syntax line-by-line for NDJSON file.

    Supports both uncompressed (.ndjson) and Zstd-compressed (.ndjson.zst) files.

    Args:
        filepath: Path to NDJSON trace file

    Returns:
        Tuple of (valid_count, errors) where:
            - valid_count: Number of valid JSON lines parsed
            - errors: List of error messages with line numbers

    Raises:
        FileNotFoundError: If file does not exist
    """
    filepath = Path(filepath)
    if not filepath.exists():
        raise FileNotFoundError(f"File not found: {filepath}")

    valid_count = 0
    errors: List[str] = []

    try:
        with open_trace_file(filepath) as f:
            for line_num, line in enumerate(f, start=1):
                line = line.strip()
                if not line:
                    continue

                try:
                    json.loads(line)
                    valid_count += 1
                except json.JSONDecodeError as e:
                    errors.append(f"Line {line_num}: JSON decode error - {e.msg}")

    except Exception as e:
        errors.append(f"File reading error: {str(e)}")

    return valid_count, errors


def validate_json_schema(
    filepath: Union[str, Path],
    message_type: str = "reg_trace",
    max_errors: int = 10,
    allow_mixed_types: bool = False,
) -> bool:
    """
    Validate JSON schema against TraceRecord definition.

    Supports both uncompressed (.ndjson) and Zstd-compressed (.ndjson.zst) files.

    Args:
        filepath: Path to NDJSON trace file
        message_type: Expected message type (default: "reg_trace")
        max_errors: Maximum number of schema errors to collect (default: 10)
        allow_mixed_types: If True, validate each record against its own schema
                          based on the 'type' field. If False, all records must
                          match the specified message_type. (default: False)

    Returns:
        True if all records pass schema validation

    Raises:
        JsonValidationError: If schema validation fails (includes detailed errors)
        FileNotFoundError: If file does not exist
        ValueError: If message_type is not recognized
    """
    filepath = Path(filepath)
    if not filepath.exists():
        raise FileNotFoundError(f"File not found: {filepath}")

    if message_type not in SCHEMAS_BY_TYPE:
        valid_types = ", ".join(SCHEMAS_BY_TYPE.keys())
        raise ValueError(
            f"Unknown message type: {message_type}. Valid types: {valid_types}"
        )

    # Pre-create validators for all types if allowing mixed types
    validators = {
        msg_type: jsonschema.Draft7Validator(schema)
        for msg_type, schema in SCHEMAS_BY_TYPE.items()
    }
    default_validator = validators[message_type]

    errors: List[str] = []
    line_num = 0

    try:
        with open_trace_file(filepath) as f:
            for line_num, line in enumerate(f, start=1):
                line = line.strip()
                if not line:
                    continue

                try:
                    record = json.loads(line)
                except json.JSONDecodeError as e:
                    errors.append(f"Line {line_num}: JSON syntax error - {e.msg}")
                    if len(errors) >= max_errors:
                        break
                    continue

                # Get the record's type
                record_type = record.get("type", "")

                if allow_mixed_types:
                    # Validate against the appropriate schema for this record's type
                    if record_type not in SCHEMAS_BY_TYPE:
                        errors.append(
                            f"Line {line_num}: Unknown message type '{record_type}'"
                        )
                        if len(errors) >= max_errors:
                            break
                        continue
                    validator = validators[record_type]
                else:
                    # Check if type field matches expected type
                    if record_type != message_type:
                        errors.append(
                            f"Line {line_num}: Expected type '{message_type}', "
                            f"got '{record_type}'"
                        )
                        if len(errors) >= max_errors:
                            break
                        continue
                    validator = default_validator

                # Validate against schema
                validation_errors = list(validator.iter_errors(record))
                if validation_errors:
                    for error in validation_errors[
                        :3
                    ]:  # Show first 3 errors per record
                        field_path = ".".join(str(p) for p in error.path)
                        errors.append(
                            f"Line {line_num}: Schema error at '{field_path}': "
                            f"{error.message}"
                        )
                    if len(errors) >= max_errors:
                        break

    except Exception as e:
        errors.append(f"File reading error: {str(e)}")

    if errors:
        error_summary = "\n".join(errors)
        if len(errors) >= max_errors:
            error_summary += f"\n... (showing first {max_errors} errors)"
        raise JsonValidationError(f"Schema validation failed:\n{error_summary}")

    return True


def validate_json_trace(filepath: Union[str, Path]) -> Dict[str, Any]:
    """
    Complete validation of NDJSON trace file.

    Performs both syntax and schema validation, with auto-detection of
    message type from the first record. Supports both uncompressed (.ndjson)
    and Zstd-compressed (.ndjson.zst) files.

    Args:
        filepath: Path to NDJSON trace file

    Returns:
        Dictionary containing:
            - valid: bool - Whether validation passed
            - record_count: int - Number of valid records
            - file_size: int - File size in bytes (compressed size for .zst files)
            - compression: str - Compression type ("zstd" or "none")
            - message_type: str - Detected message type
            - errors: List[str] - Error messages (empty if valid)

    Raises:
        FileNotFoundError: If file does not exist
    """
    filepath = Path(filepath)
    if not filepath.exists():
        raise FileNotFoundError(f"File not found: {filepath}")

    compression = detect_compression(filepath)

    result: Dict[str, Any] = {
        "valid": False,
        "record_count": 0,
        "file_size": filepath.stat().st_size,
        "compression": compression,
        "message_type": None,
        "errors": [],
    }

    # Step 1: Validate syntax
    try:
        valid_count, syntax_errors = validate_json_syntax(filepath)
        result["record_count"] = valid_count

        if syntax_errors:
            result["errors"].extend(syntax_errors)
            return result

        if valid_count == 0:
            result["errors"].append("No valid JSON records found in file")
            return result

    except Exception as e:
        result["errors"].append(f"Syntax validation error: {str(e)}")
        return result

    # Step 2: Auto-detect message type from first non-metadata record
    # kernel_metadata lines are header records (not trace data), so we skip
    # them when detecting the dominant message type and exclude them from
    # record_count to keep parity with text mode (which has no metadata line).
    try:
        metadata_count = 0
        with open_trace_file(filepath) as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                record = json.loads(line)
                message_type = record.get("type")
                if message_type == "kernel_metadata":
                    metadata_count += 1
                    continue
                if message_type not in SCHEMAS_BY_TYPE:
                    result["errors"].append(
                        f"Unknown message type in file: {message_type}"
                    )
                    return result
                result["message_type"] = message_type
                break
        result["record_count"] -= metadata_count
    except Exception as e:
        result["errors"].append(f"Failed to detect message type: {str(e)}")
        return result

    # Step 3: Validate schema (allow mixed types since trace files often contain multiple record types)
    try:
        validate_json_schema(
            filepath, message_type=result["message_type"], allow_mixed_types=True
        )
        result["valid"] = True
    except JsonValidationError as e:
        result["errors"].append(str(e))
    except Exception as e:
        result["errors"].append(f"Schema validation error: {str(e)}")

    return result

```

`python/cutracer/validation/schema_loader.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
JSON Schema definitions for CUTracer trace formats.

This module loads JSON Schema definitions from external .json files for validating
NDJSON trace files produced by CUTracer. Each schema corresponds to a specific
message type.

Schema files are located in the 'schemas/' subdirectory:
- reg_trace.schema.json: Schema for register trace records
- mem_trace.schema.json: Schema for memory access trace records
- opcode_only.schema.json: Schema for opcode-only trace records
"""

import json
import sys
from pathlib import Path
from typing import Any, Dict

if sys.version_info >= (3, 11):
    import importlib.resources as resources
else:
    import importlib_resources as resources


def _load_schema(schema_name: str) -> Dict[str, Any]:
    """
    Load a JSON schema from the schemas directory.

    Args:
        schema_name: Name of the schema file (without .schema.json extension)

    Returns:
        Parsed JSON schema as a dictionary

    Raises:
        FileNotFoundError: If schema file does not exist
        json.JSONDecodeError: If schema file contains invalid JSON
    """
    # Try using importlib.resources first (for installed packages)
    try:
        schema_files = resources.files("cutracer.validation.schemas")
        schema_path = schema_files.joinpath(f"{schema_name}.schema.json")
        schema_text = schema_path.read_text(encoding="utf-8")
        return json.loads(schema_text)
    except (ModuleNotFoundError, FileNotFoundError, TypeError):
        # Fall back to file system loading (for development)
        pass

    # Fallback: load from file system relative to this module
    schema_dir = Path(__file__).parent / "schemas"
    schema_file = schema_dir / f"{schema_name}.schema.json"

    if not schema_file.exists():
        raise FileNotFoundError(f"Schema file not found: {schema_file}")

    with open(schema_file, "r", encoding="utf-8") as f:
        return json.load(f)


# Load schemas from JSON files
REG_INFO_SCHEMA: Dict[str, Any] = _load_schema("reg_trace")
MEM_ACCESS_SCHEMA: Dict[str, Any] = _load_schema("mem_trace")
OPCODE_ONLY_SCHEMA: Dict[str, Any] = _load_schema("opcode_only")
DELAY_CONFIG_SCHEMA: Dict[str, Any] = _load_schema("delay_config")
KERNEL_METADATA_SCHEMA: Dict[str, Any] = _load_schema("kernel_metadata")

# Mapping from type field to schema (for trace records with "type" field)
SCHEMAS_BY_TYPE: Dict[str, Dict[str, Any]] = {
    "reg_trace": REG_INFO_SCHEMA,
    "mem_trace": MEM_ACCESS_SCHEMA,
    "opcode_only": OPCODE_ONLY_SCHEMA,
    "kernel_metadata": KERNEL_METADATA_SCHEMA,
}

```

`python/cutracer/validation/schemas/__init__.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
CUTracer JSON Schema files.

This package contains JSON Schema definition files for validating
CUTracer trace records.
"""

```

`python/cutracer/validation/schemas/delay_config.schema.json`:

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://github.com/facebookresearch/CUTracer/schemas/delay_config.schema.json",
  "title": "CUTracer Delay Injection Configuration",
  "description": "Schema for delay injection configuration files used for deterministic replay of random delay instrumentation",
  "type": "object",
  "required": ["version", "delay_ns", "kernels"],
  "properties": {
    "version": {
      "type": "string",
      "description": "Configuration file format version",
      "pattern": "^[0-9]+\\.[0-9]+$",
      "examples": ["1.0"]
    },
    "delay_ns": {
      "type": "integer",
      "minimum": 0,
      "description": "Default delay value in nanoseconds used during instrumentation"
    },
    "kernels": {
      "type": "object",
      "description": "Map of kernel configurations, keyed by kernel_name + timestamp",
      "additionalProperties": {
        "$ref": "#/definitions/KernelDelayConfig"
      }
    }
  },
  "additionalProperties": false,
  "definitions": {
    "KernelDelayConfig": {
      "type": "object",
      "description": "Delay configuration for a single kernel",
      "required": ["kernel_name", "kernel_checksum", "timestamp", "instrumentation_points"],
      "properties": {
        "kernel_name": {
          "type": "string",
          "minLength": 1,
          "description": "Name of the CUDA kernel"
        },
        "kernel_checksum": {
          "type": "string",
          "description": "FNV-1a hash of kernel name + all SASS instructions (hex string). Provides robust kernel identification across recompilations.",
          "pattern": "^[0-9a-f]+$",
          "examples": ["a1b2c3d4e5f67890"]
        },
        "timestamp": {
          "type": "string",
          "description": "ISO 8601 timestamp when the kernel was instrumented",
          "pattern": "^[0-9]{4}-[0-9]{2}-[0-9]{2}T[0-9]{2}:[0-9]{2}:[0-9]{2}\\.[0-9]{3}$",
          "examples": ["2026-02-03T21:15:21.567"]
        },
        "instrumentation_points": {
          "type": "object",
          "description": "Map of instrumentation points, keyed by pc_offset as string",
          "additionalProperties": {
            "$ref": "#/definitions/InstrumentationPoint"
          }
        }
      },
      "additionalProperties": false
    },
    "InstrumentationPoint": {
      "type": "object",
      "description": "A single delay instrumentation point",
      "required": ["pc", "sass", "delay", "on"],
      "properties": {
        "pc": {
          "type": "integer",
          "minimum": 0,
          "description": "Program counter offset for the instruction"
        },
        "sass": {
          "type": "string",
          "description": "SASS assembly instruction string"
        },
        "delay": {
          "type": "integer",
          "minimum": 0,
          "description": "Delay value in nanoseconds for this instrumentation point"
        },
        "on": {
          "type": "boolean",
          "description": "Whether delay injection is enabled for this point"
        }
      },
      "additionalProperties": false
    }
  }
}

```

`python/cutracer/validation/schemas/kernel_metadata.schema.json`:

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://github.com/facebookresearch/CUTracer/schemas/kernel_metadata.schema.json",
  "title": "CUTracer Kernel Metadata Record",
  "description": "Schema for kernel_metadata - per-launch metadata written as the first line in NDJSON trace files",
  "type": "object",
  "required": [
    "type",
    "mangled_name",
    "unmangled_name",
    "kernel_checksum",
    "func_addr",
    "nregs",
    "shmem_static",
    "grid",
    "block",
    "shmem_dynamic"
  ],
  "properties": {
    "type": {
      "type": "string",
      "enum": ["kernel_metadata"],
      "description": "Message type identifier"
    },
    "mangled_name": {
      "type": "string",
      "minLength": 1,
      "description": "Mangled kernel function name"
    },
    "unmangled_name": {
      "type": "string",
      "minLength": 1,
      "description": "Demangled kernel function name"
    },
    "kernel_checksum": {
      "type": "string",
      "minLength": 1,
      "description": "FNV-1a hash hex string identifying the kernel"
    },
    "func_addr": {
      "type": "string",
      "pattern": "^0x[0-9a-fA-F]+$",
      "description": "Kernel function address in hex format"
    },
    "nregs": {
      "type": "integer",
      "minimum": 0,
      "description": "Number of registers used by the kernel (CU_FUNC_ATTRIBUTE_NUM_REGS)"
    },
    "shmem_static": {
      "type": "integer",
      "minimum": 0,
      "description": "Static shared memory size in bytes (CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES)"
    },
    "cubin_path": {
      "type": "string",
      "description": "Path to dumped cubin file (only present when dump_cubin is enabled)"
    },
    "grid": {
      "type": "array",
      "items": {
        "type": "integer",
        "minimum": 0
      },
      "minItems": 3,
      "maxItems": 3,
      "description": "Grid dimensions [x, y, z]"
    },
    "block": {
      "type": "array",
      "items": {
        "type": "integer",
        "minimum": 1
      },
      "minItems": 3,
      "maxItems": 3,
      "description": "Block dimensions [x, y, z]"
    },
    "shmem_dynamic": {
      "type": "integer",
      "minimum": 0,
      "description": "Dynamic shared memory size in bytes"
    }
  },
  "additionalProperties": false
}

```

`python/cutracer/validation/schemas/mem_trace.schema.json`:

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://github.com/facebookresearch/CUTracer/schemas/mem_trace.schema.json",
  "title": "CUTracer Memory Access Trace Record",
  "description": "Schema for MSG_TYPE_MEM_ACCESS (type='mem_trace') - records memory addresses accessed by each thread",
  "type": "object",
  "required": [
    "type",
    "ctx",
    "sass",
    "trace_index",
    "timestamp",
    "grid_launch_id",
    "cta",
    "warp",
    "opcode_id",
    "pc",
    "addrs"
  ],
  "properties": {
    "type": {
      "type": "string",
      "enum": ["mem_trace"],
      "description": "Message type identifier"
    },
    "ctx": {
      "type": "string",
      "pattern": "^0x[0-9a-fA-F]+$",
      "description": "CUDA context pointer in hex format"
    },
    "sass": {
      "type": "string",
      "minLength": 1,
      "description": "SASS instruction string"
    },
    "trace_index": {
      "type": "integer",
      "minimum": 0,
      "description": "Sequential index of this trace record"
    },
    "timestamp": {
      "type": "integer",
      "minimum": 0,
      "description": "Timestamp when trace was captured"
    },
    "grid_launch_id": {
      "type": "integer",
      "minimum": 0,
      "description": "Kernel launch identifier"
    },
    "cta": {
      "type": "array",
      "items": {
        "type": "integer",
        "minimum": 0
      },
      "minItems": 3,
      "maxItems": 3,
      "description": "CTA (Cooperative Thread Array) coordinates [x, y, z]"
    },
    "warp": {
      "type": "integer",
      "minimum": 0,
      "description": "Warp ID within the CTA"
    },
    "opcode_id": {
      "type": "integer",
      "minimum": 0,
      "description": "Opcode identifier"
    },
    "pc": {
      "type": "string",
      "pattern": "^0x[0-9a-fA-F]+$",
      "description": "Program counter value in hex format"
    },
    "addrs": {
      "type": "array",
      "items": {
        "type": "integer"
      },
      "minItems": 32,
      "maxItems": 32,
      "description": "Memory addresses accessed by each of the 32 threads in the warp"
    }
  },
  "additionalProperties": false
}

```

`python/cutracer/validation/schemas/opcode_only.schema.json`:

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://github.com/facebookresearch/CUTracer/schemas/opcode_only.schema.json",
  "title": "CUTracer Opcode Only Trace Record",
  "description": "Schema for MSG_TYPE_OPCODE_ONLY (type='opcode_only') - records only instruction execution without data",
  "type": "object",
  "required": [
    "type",
    "ctx",
    "sass",
    "trace_index",
    "timestamp",
    "grid_launch_id",
    "cta",
    "warp",
    "opcode_id",
    "pc"
  ],
  "properties": {
    "type": {
      "type": "string",
      "enum": ["opcode_only"],
      "description": "Message type identifier"
    },
    "ctx": {
      "type": "string",
      "pattern": "^0x[0-9a-fA-F]+$",
      "description": "CUDA context pointer in hex format"
    },
    "sass": {
      "type": "string",
      "minLength": 1,
      "description": "SASS instruction string"
    },
    "trace_index": {
      "type": "integer",
      "minimum": 0,
      "description": "Sequential index of this trace record"
    },
    "timestamp": {
      "type": "integer",
      "minimum": 0,
      "description": "Timestamp when trace was captured"
    },
    "grid_launch_id": {
      "type": "integer",
      "minimum": 0,
      "description": "Kernel launch identifier"
    },
    "cta": {
      "type": "array",
      "items": {
        "type": "integer",
        "minimum": 0
      },
      "minItems": 3,
      "maxItems": 3,
      "description": "CTA (Cooperative Thread Array) coordinates [x, y, z]"
    },
    "warp": {
      "type": "integer",
      "minimum": 0,
      "description": "Warp ID within the CTA"
    },
    "opcode_id": {
      "type": "integer",
      "minimum": 0,
      "description": "Opcode identifier"
    },
    "pc": {
      "type": "string",
      "pattern": "^0x[0-9a-fA-F]+$",
      "description": "Program counter value in hex format"
    }
  },
  "additionalProperties": false
}

```

`python/cutracer/validation/schemas/reg_trace.schema.json`:

```json
{
  "$schema": "http://json-schema.org/draft-07/schema#",
  "$id": "https://github.com/facebookresearch/CUTracer/schemas/reg_trace.schema.json",
  "title": "CUTracer Register Trace Record",
  "description": "Schema for MSG_TYPE_REG_INFO (type='reg_trace') - records register values for each warp",
  "type": "object",
  "required": [
    "type",
    "ctx",
    "sass",
    "trace_index",
    "timestamp",
    "grid_launch_id",
    "cta",
    "warp",
    "opcode_id",
    "pc",
    "regs"
  ],
  "properties": {
    "type": {
      "type": "string",
      "enum": ["reg_trace"],
      "description": "Message type identifier"
    },
    "ctx": {
      "type": "string",
      "pattern": "^0x[0-9a-fA-F]+$",
      "description": "CUDA context pointer in hex format"
    },
    "sass": {
      "type": "string",
      "minLength": 1,
      "description": "SASS instruction string"
    },
    "trace_index": {
      "type": "integer",
      "minimum": 0,
      "description": "Sequential index of this trace record"
    },
    "timestamp": {
      "type": "integer",
      "minimum": 0,
      "description": "Timestamp when trace was captured"
    },
    "grid_launch_id": {
      "type": "integer",
      "minimum": 0,
      "description": "Kernel launch identifier"
    },
    "cta": {
      "type": "array",
      "items": {
        "type": "integer",
        "minimum": 0
      },
      "minItems": 3,
      "maxItems": 3,
      "description": "CTA (Cooperative Thread Array) coordinates [x, y, z]"
    },
    "warp": {
      "type": "integer",
      "minimum": 0,
      "description": "Warp ID within the CTA"
    },
    "opcode_id": {
      "type": "integer",
      "minimum": 0,
      "description": "Opcode identifier"
    },
    "pc": {
      "type": "string",
      "pattern": "^0x[0-9a-fA-F]+$",
      "description": "Program counter value in hex format"
    },
    "regs": {
      "type": "array",
      "items": {
        "type": "array",
        "items": {
          "type": "integer"
        },
        "minItems": 32,
        "maxItems": 32
      },
      "description": "2D array of register values [reg_index][thread_index]"
    },
    "regs_indices": {
      "type": "array",
      "items": {
        "type": "integer",
        "minimum": 0,
        "maximum": 255
      },
      "description": "Register indices corresponding to regs values (e.g., [5, 6, 7] means R5, R6, R7)"
    },
    "uregs": {
      "type": "array",
      "items": {
        "type": "integer"
      },
      "description": "Optional uniform register values"
    },
    "uregs_indices": {
      "type": "array",
      "items": {
        "type": "integer",
        "minimum": 0,
        "maximum": 62
      },
      "description": "Uniform register indices corresponding to uregs values (e.g., [8, 9, 12, 13] means UR8, UR9, UR12, UR13)"
    }
  },
  "additionalProperties": false
}

```

`python/cutracer/validation/text_validator.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Text format validator for CUTracer text trace files.

This module provides functions to validate text-format trace files (mode 0)
produced by CUTracer for format compliance.
"""

import re
from pathlib import Path
from typing import Any, Dict, List


class TextValidationError(Exception):
    """Exception raised for text validation errors."""

    pass


# Regex patterns for validating text trace format
# Pattern for header line (MSG_TYPE_REG_INFO)
REG_INFO_HEADER_PATTERN = re.compile(
    r"^CTX\s+0x[0-9a-fA-F]+\s+-\s+CTA\s+\d+,\d+,\d+\s+-\s+warp\s+\d+\s+-\s+.+:$"
)

# Pattern for register value line
REGISTER_VALUE_PATTERN = re.compile(r"^\s+\*\s+(Reg\d+_T\d+:\s+0x[0-9a-fA-F]+\s*)+$")

# Pattern for memory access header (MSG_TYPE_MEM_ACCESS)
MEM_ACCESS_HEADER_PATTERN = re.compile(
    r"^CTX\s+0x[0-9a-fA-F]+\s+-\s+kernel_launch_id\s+\d+\s+-\s+CTA\s+\d+,\d+,\d+\s+-\s+"
    r"warp\s+\d+\s+-\s+PC\s+\d+\s+-\s+.+:$"
)

# Pattern for memory addresses
MEMORY_ADDRESS_PATTERN = re.compile(r"T\d+:\s+0x[0-9a-fA-F]{16}")


def validate_text_format(filepath: Path) -> bool:
    """
    Validate text format of trace file.

    Checks for proper structure:
    - Header lines with CTX, CTA, warp, and SASS instruction
    - Register value lines or memory address lines
    - Proper hex formatting

    Args:
        filepath: Path to text trace file

    Returns:
        True if format is valid

    Raises:
        TextValidationError: If format validation fails
        FileNotFoundError: If file does not exist
    """
    if not filepath.exists():
        raise FileNotFoundError(f"File not found: {filepath}")

    errors: List[str] = []
    line_num = 0
    last_was_header = False

    try:
        with open(filepath, "r", encoding="utf-8") as f:
            for line_num, line in enumerate(f, start=1):
                # Skip empty lines
                if not line.strip():
                    last_was_header = False
                    continue

                # Check if this is a header line
                is_reg_header = REG_INFO_HEADER_PATTERN.match(line)
                is_mem_header = MEM_ACCESS_HEADER_PATTERN.match(line)

                if is_reg_header or is_mem_header:
                    last_was_header = True
                    continue

                # Check if this is a register value line
                if REGISTER_VALUE_PATTERN.match(line):
                    if not last_was_header:
                        errors.append(
                            f"Line {line_num}: Register values without header"
                        )
                    continue

                # Check if this line contains memory addresses
                if "Memory Addresses:" in line:
                    continue

                if MEMORY_ADDRESS_PATTERN.search(line):
                    continue

                # If we get here, the line doesn't match any expected pattern
                # Only report if it's not whitespace-only or a separator
                if line.strip() and not line.strip().startswith("*"):
                    errors.append(f"Line {line_num}: Unrecognized format: {line[:50]}")

    except Exception as e:
        errors.append(f"File reading error: {str(e)}")

    if errors:
        max_errors = 10
        error_summary = "\n".join(errors[:max_errors])
        if len(errors) > max_errors:
            error_summary += (
                f"\n... (showing first {max_errors} of {len(errors)} errors)"
            )
        raise TextValidationError(f"Text format validation failed:\n{error_summary}")

    return True


def validate_text_trace(filepath: Path) -> Dict[str, Any]:
    """
    Complete validation of text trace file.

    Performs format validation and collects file statistics.

    Args:
        filepath: Path to text trace file

    Returns:
        Dictionary containing:
            - valid: bool - Whether validation passed
            - record_count: int - Number of trace records (header lines)
            - file_size: int - File size in bytes
            - errors: List[str] - Error messages (empty if valid)

    Raises:
        FileNotFoundError: If file does not exist
    """
    if not filepath.exists():
        raise FileNotFoundError(f"File not found: {filepath}")

    result: Dict[str, Any] = {
        "valid": False,
        "record_count": 0,
        "file_size": filepath.stat().st_size,
        "errors": [],
    }

    # Count records (header lines)
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            for line in f:
                if REG_INFO_HEADER_PATTERN.match(
                    line
                ) or MEM_ACCESS_HEADER_PATTERN.match(line):
                    result["record_count"] += 1

        if result["record_count"] == 0:
            result["errors"].append("No trace records found in file")
            return result

    except Exception as e:
        result["errors"].append(f"Error counting records: {str(e)}")
        return result

    # Validate format
    try:
        validate_text_format(filepath)
        result["valid"] = True
    except TextValidationError as e:
        result["errors"].append(str(e))
    except Exception as e:
        result["errors"].append(f"Format validation error: {str(e)}")

    return result


def parse_text_trace_record(lines: List[str]) -> Dict[str, Any]:
    """
    Parse a single trace record from text format.

    Args:
        lines: List of lines comprising a single trace record
              (header line + data lines)

    Returns:
        Dictionary containing parsed fields:
            - ctx: str - Context pointer
            - cta: List[int] - CTA coordinates [x, y, z]
            - warp: int - Warp ID
            - sass: str - SASS instruction
            - record_type: str - "reg_info" or "mem_access"
            - data: Dict[str, Any] - Type-specific data

    Raises:
        ValueError: If record format is invalid
    """
    if not lines:
        raise ValueError("Empty record")

    header = lines[0]

    # Try to match reg_info header
    reg_match = re.match(
        r"^CTX\s+(0x[0-9a-fA-F]+)\s+-\s+CTA\s+(\d+),(\d+),(\d+)\s+-\s+"
        r"warp\s+(\d+)\s+-\s+(.+):$",
        header,
    )

    if reg_match:
        ctx, cta_x, cta_y, cta_z, warp, sass = reg_match.groups()
        return {
            "ctx": ctx,
            "cta": [int(cta_x), int(cta_y), int(cta_z)],
            "warp": int(warp),
            "sass": sass,
            "record_type": "reg_info",
            "data": {},
        }

    # Try to match mem_access header
    mem_match = re.match(
        r"^CTX\s+(0x[0-9a-fA-F]+)\s+-\s+kernel_launch_id\s+(\d+)\s+-\s+"
        r"CTA\s+(\d+),(\d+),(\d+)\s+-\s+warp\s+(\d+)\s+-\s+PC\s+(\d+)\s+-\s+(.+):$",
        header,
    )

    if mem_match:
        ctx, kid, cta_x, cta_y, cta_z, warp, pc, sass = mem_match.groups()
        return {
            "ctx": ctx,
            "kernel_launch_id": int(kid),
            "cta": [int(cta_x), int(cta_y), int(cta_z)],
            "warp": int(warp),
            "pc": int(pc),
            "sass": sass,
            "record_type": "mem_access",
            "data": {},
        }

    raise ValueError(f"Unrecognized header format: {header}")

```

`python/pyproject.toml`:

```toml
# Copyright (c) Meta Platforms, Inc. and affiliates.

[build-system]
requires = ["setuptools>=64", "wheel", "setuptools-scm>=8.0.0"]
build-backend = "setuptools.build_meta"

[project]
name = "cutracer"
dynamic = ["version"]
description = "Python tools for CUTracer trace validation and analysis"
requires-python = ">=3.10"
authors = [
    {name = "Yueming Hao", email = "yhao@meta.com"}
]
readme = "README.md"
license = "MIT"

dependencies = [
    "click>=8.0.0",
    "jsonschema>=4.0.0",
    "zstandard>=0.20.0",
    "tabulate>=0.9.0",
    "importlib_resources>=5.0.0; python_version < '3.11'",
    "tritonparse>=0.4.0",
    "yscope_clp_core>=0.7.1b2"
]

[project.optional-dependencies]
dev = [
    "mypy>=1.0.0",
    "ufmt==2.9.0",
    "usort==1.1.0",
    "ruff-api==0.2.0",
    "ruff>=0.4.0",
]

[project.urls]
"Homepage" = "https://github.com/facebookresearch/CUTracer"
"Bug Tracker" = "https://github.com/facebookresearch/CUTracer/issues"

[project.scripts]
cutraceross = "cutracer.cli:main"

[tool.setuptools.packages.find]
include = ["cutracer*"]

[tool.setuptools.package-data]
"cutracer.validation.schemas" = ["*.json"]

[tool.ufmt]
formatter = "ruff-api"
sorter = "usort"

[tool.ruff]
line-length = 88
target-version = "py310"

[tool.usort]
first_party_detection = false

[tool.setuptools_scm]
root = ".."
version_scheme = "guess-next-dev"
local_scheme = "no-local-version"

```

`python/requirements.txt`:

```txt
# Copyright (c) Meta Platforms, Inc. and affiliates.
#
# Requirements file for CUTracer Python package.
#
# NOTE: This file is synced with pyproject.toml dependencies.
# When updating dependencies, please keep both files in sync.
#
# To install: pip install -r requirements.txt

# Core dependencies (synced from pyproject.toml [project.dependencies])
click>=8.0.0
jsonschema>=4.0.0
zstandard>=0.20.0
tabulate>=0.9.0
importlib_resources>=5.0.0; python_version < '3.11'
tritonparse>=0.4.0
yscope_clp_core>=0.7.1b2

# Development dependencies (synced from pyproject.toml [project.optional-dependencies.dev])
mypy>=1.0.0
ufmt==2.9.0
usort==1.1.0
ruff-api==0.2.0
ruff>=0.4.0

```

`python/tests/__init__.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Test package for CUTracer validation module.
"""

```

`python/tests/example_inputs/invalid_schema.ndjson`:

```ndjson
JSONL Schema (1 sample line):
Fields: ctx, type
Sample: {"type": "reg_trace", "ctx": "0x5a1100"}
... [1 more lines omitted]

```

`python/tests/example_inputs/invalid_syntax.ndjson`:

```ndjson
JSONL Schema (1 sample line):
Fields: cta, ctx, grid_launch_id, opcode_id, pc, regs, sass, timestamp, trace_index, type, warp
Sample: {"type": "reg_trace", "ctx": "0x5a1100", "sass": "LDC R1, c[0x0][0x28] ;", "trace_index": 0, "timestamp": 1763069214784647489, "grid_launch_id": 0, "cta": [0, 0, 0], "warp": 0, "opcode_id": 0, "pc": "0x0", "regs": [[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]]}
... [2 more lines omitted]

```

`python/tests/example_inputs/reg_trace_sample.log`:

```log
CTX 0x5a46d0 - CTA 0,0,0 - warp 0 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 1,0,0 - warp 6 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 0,0,0 - warp 1 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 1,0,0 - warp 5 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 0,0,0 - warp 2 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 1,0,0 - warp 4 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 2,0,0 - warp 9 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 0,0,0 - warp 3 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 3,0,0 - warp 12 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 1,0,0 - warp 7 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 4,0,0 - warp 16 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 5,0,0 - warp 20 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 6,0,0 - warp 25 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 2,0,0 - warp 8 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 7,0,0 - warp 28 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 3,0,0 - warp 13 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 6,0,0 - warp 24 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 4,0,0 - warp 17 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 7,0,0 - warp 29 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 5,0,0 - warp 21 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 6,0,0 - warp 27 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 2,0,0 - warp 11 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 7,0,0 - warp 30 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 3,0,0 - warp 14 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 6,0,0 - warp 26 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 4,0,0 - warp 18 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 7,0,0 - warp 31 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 5,0,0 - warp 22 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 2,0,0 - warp 10 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 3,0,0 - warp 15 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 4,0,0 - warp 19 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 5,0,0 - warp 23 - LDC R1, c[0x0][0x28] ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 1,0,0 - warp 5 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 0,0,0 - warp 2 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 3,0,0 - warp 15 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 7,0,0 - warp 28 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 1,0,0 - warp 6 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 2,0,0 - warp 9 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 2,0,0 - warp 11 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 1,0,0 - warp 4 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 6,0,0 - warp 24 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 5,0,0 - warp 20 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 7,0,0 - warp 31 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 6,0,0 - warp 25 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 0,0,0 - warp 3 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 2,0,0 - warp 10 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 2,0,0 - warp 8 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 5,0,0 - warp 22 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 1,0,0 - warp 7 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 5,0,0 - warp 23 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 3,0,0 - warp 13 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 3,0,0 - warp 14 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 6,0,0 - warp 27 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 7,0,0 - warp 29 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 7,0,0 - warp 30 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 4,0,0 - warp 19 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 0,0,0 - warp 0 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 4,0,0 - warp 17 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 3,0,0 - warp 12 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 4,0,0 - warp 18 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 6,0,0 - warp 26 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 5,0,0 - warp 21 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 4,0,0 - warp 16 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 0,0,0 - warp 1 - S2R R0, SR_TID.X ;:
  * Reg0_T00: 0x00000000 Reg0_T01: 0x00000000 Reg0_T02: 0x00000000 Reg0_T03: 0x00000000 Reg0_T04: 0x00000000 Reg0_T05: 0x00000000 Reg0_T06: 0x00000000 Reg0_T07: 0x00000000 Reg0_T08: 0x00000000 Reg0_T09: 0x00000000 Reg0_T10: 0x00000000 Reg0_T11: 0x00000000 Reg0_T12: 0x00000000 Reg0_T13: 0x00000000 Reg0_T14: 0x00000000 Reg0_T15: 0x00000000 Reg0_T16: 0x00000000 Reg0_T17: 0x00000000 Reg0_T18: 0x00000000 Reg0_T19: 0x00000000 Reg0_T20: 0x00000000 Reg0_T21: 0x00000000 Reg0_T22: 0x00000000 Reg0_T23: 0x00000000 Reg0_T24: 0x00000000 Reg0_T25: 0x00000000 Reg0_T26: 0x00000000 Reg0_T27: 0x00000000 Reg0_T28: 0x00000000 Reg0_T29: 0x00000000 Reg0_T30: 0x00000000 Reg0_T31: 0x00000000 

CTX 0x5a46d0 - CTA 1,0,0 - warp 5 - S2UR UR4, SR_CTAID.X ;:
  * UR: UR0: 0x00000000 

CTX 0x5a46d0 - CTA 1,0,0 - warp 6 - S2UR UR4, SR_CTAID.X ;:
  * UR: UR0: 0x00000000 

CTX 0x5a46d0 - CTA 3,0,0 - warp 15 - S2UR UR4, SR_CTAID.X ;:
  * UR: UR0: 0x00000000 

```

`python/tests/example_inputs/reg_trace_sample.ndjson`:

```ndjson
JSONL Schema (1 sample line):
Fields: cta, ctx, grid_launch_id, opcode_id, pc, regs, sass, timestamp, trace_index, type, warp
Sample: {"cta":[0,0,0],"ctx":"0x5a1100","grid_launch_id":0,"opcode_id":0,"pc":"0x0","regs":[[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]],"sass":"LDC R1, c[0x0][0x28] ;","timestamp":1763069214784647489,"trace_index":0,"type":"reg_trace","warp":0}
... [99 more lines omitted]

```

`python/tests/test_analyze_cli.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""Tests for the analyze command."""

import json
import tempfile
import unittest
from pathlib import Path

from click.testing import CliRunner
from cutracer.cli import main


class AnalyzeWarpSummaryTest(unittest.TestCase):
    """Tests for analyze warp-summary command."""

    def setUp(self):
        self.runner = CliRunner()
        self.temp_dir = tempfile.mkdtemp()

        # Create a test trace file with warp data
        self.trace_file = Path(self.temp_dir) / "trace.ndjson"
        with open(self.trace_file, "w") as f:
            records = [
                {"warp": 0, "pc": 100, "sass": "MOV R1, R2;"},
                {"warp": 0, "pc": 200, "sass": "ADD R1, R2, R3;"},
                {"warp": 0, "pc": 300, "sass": "EXIT;"},
                {"warp": 1, "pc": 100, "sass": "MOV R1, R2;"},
                {"warp": 1, "pc": 200, "sass": "ADD R1, R2, R3;"},
                {"warp": 2, "pc": 100, "sass": "MOV R1, R2;"},
                {"warp": 2, "pc": 200, "sass": "EXIT;"},
            ]
            for record in records:
                f.write(json.dumps(record) + "\n")

    def test_warp_summary_basic(self):
        """Test basic warp-summary command."""
        result = self.runner.invoke(
            main,
            ["analyze", "warp-summary", str(self.trace_file)],
        )

        self.assertEqual(result.exit_code, 0)
        self.assertIn("Warp Summary", result.output)
        self.assertIn("Total warps observed", result.output)
        self.assertIn("Completed (EXIT)", result.output)
        self.assertIn("In-progress", result.output)

    def test_warp_summary_json_format(self):
        """Test warp-summary with JSON output."""
        result = self.runner.invoke(
            main,
            ["analyze", "warp-summary", str(self.trace_file), "--format", "json"],
        )

        self.assertEqual(result.exit_code, 0)

        # Parse JSON output
        output = json.loads(result.output)
        self.assertIn("total_observed", output)
        self.assertIn("completed", output)
        self.assertIn("in_progress", output)
        self.assertIn("missing", output)

        # Verify counts
        self.assertEqual(output["total_observed"], 3)
        self.assertEqual(output["completed"]["count"], 2)  # warp 0 and 2
        self.assertEqual(output["in_progress"]["count"], 1)  # warp 1

    def test_warp_summary_output_file(self):
        """Test warp-summary with output file."""
        output_file = Path(self.temp_dir) / "summary.txt"

        result = self.runner.invoke(
            main,
            [
                "analyze",
                "warp-summary",
                str(self.trace_file),
                "-o",
                str(output_file),
            ],
        )

        self.assertEqual(result.exit_code, 0)
        self.assertIn("Output written to", result.output)
        self.assertTrue(output_file.exists())

        content = output_file.read_text()
        self.assertIn("Warp Summary", content)

    def test_warp_summary_json_output_file(self):
        """Test warp-summary with JSON output to file."""
        output_file = Path(self.temp_dir) / "summary.json"

        result = self.runner.invoke(
            main,
            [
                "analyze",
                "warp-summary",
                str(self.trace_file),
                "--format",
                "json",
                "-o",
                str(output_file),
            ],
        )

        self.assertEqual(result.exit_code, 0)
        self.assertTrue(output_file.exists())

        with open(output_file) as f:
            output = json.load(f)
        self.assertIn("total_observed", output)

    def test_warp_summary_nonexistent_file(self):
        """Test warp-summary with nonexistent file."""
        result = self.runner.invoke(
            main,
            ["analyze", "warp-summary", "/nonexistent/file.ndjson"],
        )

        self.assertNotEqual(result.exit_code, 0)

    def test_warp_summary_help(self):
        """Test warp-summary --help."""
        result = self.runner.invoke(
            main,
            ["analyze", "warp-summary", "--help"],
        )

        self.assertEqual(result.exit_code, 0)
        self.assertIn("Analyze warp execution status", result.output)
        self.assertIn("--format", result.output)
        self.assertIn("--output", result.output)

    def test_analyze_help(self):
        """Test analyze --help shows subcommands."""
        result = self.runner.invoke(
            main,
            ["analyze", "--help"],
        )

        self.assertEqual(result.exit_code, 0)
        self.assertIn("Analyze trace data", result.output)
        self.assertIn("warp-summary", result.output)

    def test_analyze_no_subcommand(self):
        """Test analyze without subcommand shows help."""
        result = self.runner.invoke(
            main,
            ["analyze"],
        )

        # Click 8+ returns 0 and shows help; Click 7 returns 2
        self.assertIn(result.exit_code, (0, 2))
        self.assertIn("warp-summary", result.output)


class AnalyzeIntegrationTest(unittest.TestCase):
    """Integration tests for analyze command with compressed files."""

    def setUp(self):
        self.runner = CliRunner()
        self.temp_dir = tempfile.mkdtemp()

    def test_warp_summary_from_zst_file(self):
        """Test warp-summary from Zstd compressed file."""
        import zstandard as zstd

        # Create compressed test file
        trace_file = Path(self.temp_dir) / "trace.ndjson.zst"
        records = [
            {"warp": 0, "pc": 100, "sass": "EXIT;"},
            {"warp": 1, "pc": 100, "sass": "MOV R1, R2;"},
        ]

        cctx = zstd.ZstdCompressor()
        with open(trace_file, "wb") as f:
            with cctx.stream_writer(f) as writer:
                for record in records:
                    writer.write((json.dumps(record) + "\n").encode("utf-8"))

        result = self.runner.invoke(
            main,
            ["analyze", "warp-summary", str(trace_file)],
        )

        self.assertEqual(result.exit_code, 0)
        self.assertIn("Warp Summary", result.output)


if __name__ == "__main__":
    unittest.main()

```

`python/tests/test_base.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Base test class and shared test data for CUTracer validation tests.
"""

import shutil
import tempfile
import unittest
from pathlib import Path


# Example inputs directory
EXAMPLE_INPUTS_DIR = Path(__file__).parent / "example_inputs"

# Real data file paths
REG_TRACE_NDJSON = EXAMPLE_INPUTS_DIR / "reg_trace_sample.ndjson"
REG_TRACE_NDJSON_ZST = EXAMPLE_INPUTS_DIR / "reg_trace_sample.ndjson.zst"
REG_TRACE_LOG = EXAMPLE_INPUTS_DIR / "reg_trace_sample.log"
INVALID_SYNTAX_NDJSON = EXAMPLE_INPUTS_DIR / "invalid_syntax.ndjson"
INVALID_SCHEMA_NDJSON = EXAMPLE_INPUTS_DIR / "invalid_schema.ndjson"

# Expected record counts for sample files
REG_TRACE_NDJSON_RECORD_COUNT = 100
REG_TRACE_NDJSON_ZST_RECORD_COUNT = 100
REG_TRACE_LOG_RECORD_COUNT = 67


class BaseValidationTest(unittest.TestCase):
    """Base class for validation tests with shared setup/teardown."""

    @classmethod
    def setUpClass(cls):
        """Verify example input files exist."""
        required_files = [
            REG_TRACE_NDJSON,
            REG_TRACE_LOG,
            INVALID_SYNTAX_NDJSON,
            INVALID_SCHEMA_NDJSON,
        ]
        for path in required_files:
            if not path.exists():
                raise FileNotFoundError(f"Example input file not found: {path}")

    def setUp(self):
        """Create a temporary directory for test files that need to be generated."""
        self.temp_dir = Path(tempfile.mkdtemp())

    def tearDown(self):
        """Remove the temporary directory."""
        shutil.rmtree(self.temp_dir)

    def create_temp_file(self, filename: str, content: str) -> Path:
        """Create a temporary file with given content."""
        filepath = self.temp_dir / filename
        filepath.write_text(content)
        return filepath

```

`python/tests/test_cli.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""Tests for cutracer CLI."""

import subprocess
import sys
import unittest
from pathlib import Path

from click.testing import CliRunner
from cutracer.cli import main
from cutracer.validation.cli import _detect_format, _format_size


class DetectFormatTest(unittest.TestCase):
    """Tests for _detect_format function."""

    def test_detect_ndjson(self):
        self.assertEqual(_detect_format(Path("trace.ndjson")), "json")

    def test_detect_ndjson_zst(self):
        self.assertEqual(_detect_format(Path("trace.ndjson.zst")), "json")

    def test_detect_log(self):
        self.assertEqual(_detect_format(Path("trace.log")), "text")

    def test_detect_unknown(self):
        self.assertEqual(_detect_format(Path("trace.txt")), "unknown")
        self.assertEqual(_detect_format(Path("trace.bin")), "unknown")


class FormatSizeTest(unittest.TestCase):
    """Tests for _format_size function."""

    def test_format_bytes(self):
        self.assertEqual(_format_size(500), "500 B")

    def test_format_kilobytes(self):
        self.assertEqual(_format_size(2048), "2.0 KB")

    def test_format_megabytes(self):
        self.assertEqual(_format_size(1048576), "1.0 MB")
        self.assertEqual(_format_size(2621440), "2.5 MB")


class ValidateCommandTest(unittest.TestCase):
    """Tests for validate subcommand."""

    def setUp(self):
        self.test_dir = Path(__file__).parent / "example_inputs"
        self.runner = CliRunner()

    def test_validate_valid_json(self):
        """Test validating a valid NDJSON file."""
        result = self.runner.invoke(
            main, ["validate", str(self.test_dir / "reg_trace_sample.ndjson")]
        )
        self.assertEqual(result.exit_code, 0)

    def test_validate_valid_json_zst(self):
        """Test validating a valid Zstd-compressed NDJSON file."""
        result = self.runner.invoke(
            main, ["validate", str(self.test_dir / "reg_trace_sample.ndjson.zst")]
        )
        self.assertEqual(result.exit_code, 0)

    def test_validate_valid_text(self):
        """Test validating a valid text log file."""
        result = self.runner.invoke(
            main, ["validate", str(self.test_dir / "reg_trace_sample.log")]
        )
        self.assertEqual(result.exit_code, 0)

    def test_validate_invalid_syntax(self):
        """Test validating a file with invalid JSON syntax."""
        result = self.runner.invoke(
            main, ["validate", str(self.test_dir / "invalid_syntax.ndjson")]
        )
        self.assertEqual(result.exit_code, 1)

    def test_validate_invalid_schema(self):
        """Test validating a file with schema errors."""
        result = self.runner.invoke(
            main, ["validate", str(self.test_dir / "invalid_schema.ndjson")]
        )
        self.assertEqual(result.exit_code, 1)

    def test_validate_quiet_mode(self):
        """Test quiet mode returns only exit code."""
        result = self.runner.invoke(
            main,
            ["validate", "--quiet", str(self.test_dir / "reg_trace_sample.ndjson")],
        )
        self.assertEqual(result.exit_code, 0)
        # Quiet mode should produce no output
        self.assertEqual(result.output.strip(), "")

    def test_validate_json_output(self):
        """Test JSON output format."""
        result = self.runner.invoke(
            main,
            ["validate", "--json", str(self.test_dir / "reg_trace_sample.ndjson")],
        )
        self.assertEqual(result.exit_code, 0)
        # Should contain JSON output
        self.assertIn('"valid"', result.output)

    def test_validate_file_not_found(self):
        """Test error handling for non-existent file."""
        result = self.runner.invoke(main, ["validate", "/nonexistent/file.ndjson"])
        self.assertEqual(result.exit_code, 2)

    def test_validate_unknown_format(self):
        """Test error handling for unknown format."""
        # Create a temporary file with unknown extension
        unknown_file = self.test_dir / "reg_trace_sample.unknown"
        # Copy content from existing file for the test
        if not unknown_file.exists():
            import shutil

            shutil.copy(self.test_dir / "reg_trace_sample.ndjson", unknown_file)

        try:
            result = self.runner.invoke(main, ["validate", str(unknown_file)])
            self.assertEqual(result.exit_code, 2)
            self.assertIn("Cannot auto-detect format", result.output)
        finally:
            # Cleanup
            if unknown_file.exists():
                unknown_file.unlink()

    def test_validate_explicit_format_json(self):
        """Test explicit --format json option."""
        result = self.runner.invoke(
            main,
            [
                "validate",
                "--format",
                "json",
                str(self.test_dir / "reg_trace_sample.ndjson"),
            ],
        )
        self.assertEqual(result.exit_code, 0)

    def test_validate_explicit_format_text(self):
        """Test explicit --format text option."""
        result = self.runner.invoke(
            main,
            [
                "validate",
                "--format",
                "text",
                str(self.test_dir / "reg_trace_sample.log"),
            ],
        )
        self.assertEqual(result.exit_code, 0)


class MainEntryPointTest(unittest.TestCase):
    """Tests for main entry point."""

    def setUp(self):
        self.runner = CliRunner()

    def test_version_flag(self):
        """Test --version flag."""
        result = self.runner.invoke(main, ["--version"])
        self.assertEqual(result.exit_code, 0)
        self.assertIn("cutraceross", result.output)

    def test_help_flag(self):
        """Test --help flag."""
        result = self.runner.invoke(main, ["--help"])
        self.assertEqual(result.exit_code, 0)
        self.assertIn("validate", result.output)

    def test_no_command(self):
        """Test error when no command is provided."""
        result = self.runner.invoke(main, [])
        # Click group with required subcommand returns exit code 2 when no command provided
        self.assertEqual(result.exit_code, 2)


class ModuleEntryPointTest(unittest.TestCase):
    """Tests for python -m cutracer entry point."""

    def test_module_help(self):
        """Test python -m cutracer --help works."""
        result = subprocess.run(
            [sys.executable, "-m", "cutracer", "--help"],
            capture_output=True,
            text=True,
            cwd=Path(__file__).parent.parent,
        )
        self.assertEqual(result.returncode, 0)
        self.assertIn("cutraceross", result.stdout)
        self.assertIn("validate", result.stdout)


if __name__ == "__main__":
    unittest.main()

```

`python/tests/test_compression.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""Tests for compression module."""

import unittest

from cutracer.validation.compression import (
    detect_compression,
    get_trace_format,
    iter_lines,
    open_trace_file,
)
from tests.test_base import (
    EXAMPLE_INPUTS_DIR,
    REG_TRACE_LOG,
    REG_TRACE_NDJSON,
    REG_TRACE_NDJSON_ZST,
)


class CompressionDetectionTest(unittest.TestCase):
    """Tests for compression detection functions."""

    def test_detect_compression_zstd(self) -> None:
        """Test detection of Zstd compressed file."""
        if not REG_TRACE_NDJSON_ZST.exists():
            self.skipTest("Zstd test file not available")

        result = detect_compression(REG_TRACE_NDJSON_ZST)
        self.assertEqual(result, "zstd")

    def test_detect_compression_none(self) -> None:
        """Test detection of uncompressed file."""
        result = detect_compression(REG_TRACE_NDJSON)
        self.assertEqual(result, "none")

    def test_detect_compression_nonexistent(self) -> None:
        """Test detection raises error for non-existent file."""
        with self.assertRaises(FileNotFoundError):
            detect_compression(EXAMPLE_INPUTS_DIR / "nonexistent.ndjson")


class TraceFormatDetectionTest(unittest.TestCase):
    """Tests for trace format detection."""

    def test_get_trace_format_ndjson(self) -> None:
        """Test format detection for uncompressed NDJSON."""
        base_format, compression = get_trace_format(REG_TRACE_NDJSON)
        self.assertEqual(base_format, "ndjson")
        self.assertEqual(compression, "none")

    def test_get_trace_format_ndjson_zst(self) -> None:
        """Test format detection for compressed NDJSON."""
        if not REG_TRACE_NDJSON_ZST.exists():
            self.skipTest("Zstd test file not available")

        base_format, compression = get_trace_format(REG_TRACE_NDJSON_ZST)
        self.assertEqual(base_format, "ndjson")
        self.assertEqual(compression, "zstd")

    def test_get_trace_format_text(self) -> None:
        """Test format detection for text log file."""
        base_format, compression = get_trace_format(REG_TRACE_LOG)
        self.assertEqual(base_format, "text")
        self.assertEqual(compression, "none")


class OpenTraceFileTest(unittest.TestCase):
    """Tests for open_trace_file function."""

    def test_open_uncompressed(self) -> None:
        """Test opening uncompressed file."""
        with open_trace_file(REG_TRACE_NDJSON) as f:
            first_line = next(iter(f))
            self.assertTrue(first_line.strip().startswith("{"))

    def test_open_compressed(self) -> None:
        """Test opening Zstd compressed file."""
        if not REG_TRACE_NDJSON_ZST.exists():
            self.skipTest("Zstd test file not available")

        with open_trace_file(REG_TRACE_NDJSON_ZST) as f:
            first_line = next(iter(f))
            self.assertTrue(first_line.strip().startswith("{"))

    def test_open_nonexistent(self) -> None:
        """Test opening non-existent file raises error."""
        with self.assertRaises(FileNotFoundError):
            with open_trace_file(EXAMPLE_INPUTS_DIR / "nonexistent.ndjson"):
                pass


class IterLinesTest(unittest.TestCase):
    """Tests for iter_lines function."""

    def test_iter_lines_uncompressed(self) -> None:
        """Test iterating lines from uncompressed file."""
        lines = list(iter_lines(REG_TRACE_NDJSON))
        self.assertGreater(len(lines), 0)
        # Each line should be valid JSON (starts with '{')
        for line in lines[:10]:
            self.assertTrue(line.startswith("{"), f"Invalid line: {line[:50]}")

    def test_iter_lines_compressed(self) -> None:
        """Test iterating lines from compressed file."""
        if not REG_TRACE_NDJSON_ZST.exists():
            self.skipTest("Zstd test file not available")

        lines = list(iter_lines(REG_TRACE_NDJSON_ZST))
        self.assertGreater(len(lines), 0)
        # Each line should be valid JSON (starts with '{')
        for line in lines[:10]:
            self.assertTrue(line.startswith("{"), f"Invalid line: {line[:50]}")


if __name__ == "__main__":
    unittest.main()

```

`python/tests/test_consistency.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Unit tests for consistency checker module.
"""

import unittest

from cutracer.validation.consistency import (
    compare_record_counts,
    compare_trace_formats,
    get_trace_statistics,
)
from tests.test_base import (
    BaseValidationTest,
    REG_TRACE_LOG,
    REG_TRACE_LOG_RECORD_COUNT,
    REG_TRACE_NDJSON,
    REG_TRACE_NDJSON_RECORD_COUNT,
    REG_TRACE_NDJSON_ZST,
    REG_TRACE_NDJSON_ZST_RECORD_COUNT,
)


class ConsistencyTest(BaseValidationTest):
    """Core tests for consistency checker functions."""

    def test_compare_record_counts_exact_match(self):
        """Test that exact count match returns True."""
        text_metadata = {"record_count": 100}
        json_metadata = {"record_count": 100}

        result = compare_record_counts(text_metadata, json_metadata)

        self.assertTrue(result)

    def test_compare_record_counts_outside_tolerance(self):
        """Test that counts outside tolerance return False."""
        text_metadata = {"record_count": 100}
        json_metadata = {"record_count": 150}

        result = compare_record_counts(text_metadata, json_metadata, tolerance=0.1)

        self.assertFalse(result)

    def test_compare_record_counts_missing_field(self):
        """Test that missing record_count field raises ValueError."""
        with self.assertRaises(ValueError):
            compare_record_counts({}, {"record_count": 100})

        with self.assertRaises(ValueError):
            compare_record_counts({"record_count": 100}, {})

    def test_get_trace_statistics_real_ndjson(self):
        """Test statistics extraction from real NDJSON file."""
        stats = get_trace_statistics(REG_TRACE_NDJSON)

        self.assertEqual(stats["format"], "json")
        self.assertEqual(stats["record_count"], REG_TRACE_NDJSON_RECORD_COUNT)
        self.assertGreater(stats["file_size"], 0)
        self.assertIn("reg_trace", stats["message_types"])

    def test_get_trace_statistics_real_text(self):
        """Test statistics extraction from real text file."""
        stats = get_trace_statistics(REG_TRACE_LOG)

        self.assertEqual(stats["format"], "text")
        self.assertEqual(stats["record_count"], REG_TRACE_LOG_RECORD_COUNT)
        self.assertGreater(stats["file_size"], 0)

    def test_get_trace_statistics_file_not_found(self):
        """Test handling of non-existent file."""
        non_existent = self.temp_dir / "missing.ndjson"

        with self.assertRaises(FileNotFoundError):
            get_trace_statistics(non_existent)

    def test_compare_trace_formats_real_files(self):
        """Test comparison of real text and JSON trace files."""
        result = compare_trace_formats(REG_TRACE_LOG, REG_TRACE_NDJSON)

        self.assertIn("consistent", result)
        self.assertIn("record_count_match", result)
        self.assertIn("text_records", result)
        self.assertIn("json_records", result)
        self.assertEqual(result["text_records"], REG_TRACE_LOG_RECORD_COUNT)
        self.assertEqual(result["json_records"], REG_TRACE_NDJSON_RECORD_COUNT)


class ConsistencyCompressedTest(BaseValidationTest):
    """Tests for consistency checker with compressed files."""

    def test_get_trace_statistics_compressed(self):
        """Test statistics extraction from compressed NDJSON file."""
        if not REG_TRACE_NDJSON_ZST.exists():
            self.skipTest("Zstd test file not available")

        stats = get_trace_statistics(REG_TRACE_NDJSON_ZST)

        self.assertEqual(stats["format"], "json")
        self.assertEqual(stats["compression"], "zstd")
        self.assertEqual(stats["record_count"], REG_TRACE_NDJSON_ZST_RECORD_COUNT)
        self.assertGreater(stats["file_size"], 0)
        self.assertIn("reg_trace", stats["message_types"])

    def test_get_trace_statistics_compression_field_uncompressed(self):
        """Test that uncompressed files have compression='none'."""
        stats = get_trace_statistics(REG_TRACE_NDJSON)

        self.assertEqual(stats["compression"], "none")

    def test_compare_trace_formats_with_compressed_json(self):
        """Test comparison of text and compressed JSON trace files."""
        if not REG_TRACE_NDJSON_ZST.exists():
            self.skipTest("Zstd test file not available")

        result = compare_trace_formats(REG_TRACE_LOG, REG_TRACE_NDJSON_ZST)

        self.assertIn("consistent", result)
        self.assertIn("record_count_match", result)
        self.assertEqual(result["text_records"], REG_TRACE_LOG_RECORD_COUNT)
        self.assertEqual(result["json_records"], REG_TRACE_NDJSON_ZST_RECORD_COUNT)


if __name__ == "__main__":
    unittest.main()

```

`python/tests/test_formatters.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Unit tests for formatters module.
"""

import json
import unittest

from cutracer.query.formatters import (
    DEFAULT_FIELDS,
    format_records_csv,
    format_records_json,
    format_records_table,
    format_value,
    get_display_fields,
)


class TestFormatValue(unittest.TestCase):
    """Tests for format_value function."""

    def test_format_value_none(self):
        """Test None returns empty string."""
        self.assertEqual(format_value(None), "")

    def test_format_value_bool_true(self):
        """Test True returns lowercase 'true'."""
        self.assertEqual(format_value(True), "true")

    def test_format_value_bool_false(self):
        """Test False returns lowercase 'false'."""
        self.assertEqual(format_value(False), "false")

    def test_format_value_int(self):
        """Test int returns string representation."""
        self.assertEqual(format_value(42), "42")

    def test_format_value_str(self):
        """Test string returns as-is."""
        self.assertEqual(format_value("hello"), "hello")

    def test_format_value_list(self):
        """Test list returns bracketed comma-separated values."""
        self.assertEqual(format_value([1, 2, 3]), "[1,2,3]")

    def test_format_value_empty_list(self):
        """Test empty list returns empty brackets."""
        self.assertEqual(format_value([]), "[]")

    def test_format_value_dict(self):
        """Test dict returns JSON string."""
        result = format_value({"a": 1})
        self.assertEqual(result, '{"a": 1}')


class TestGetDisplayFields(unittest.TestCase):
    """Tests for get_display_fields function."""

    def test_get_display_fields_default(self):
        """Test default returns all fields when none requested."""
        records = [{"warp": 0, "pc": "0x100", "sass": "NOP ;", "extra": "data"}]
        fields = get_display_fields(records)
        # Default behavior now returns all fields from the record
        self.assertEqual(fields, ["warp", "pc", "sass", "extra"])

    def test_get_display_fields_requested(self):
        """Test user-specified fields are used."""
        records = [{"warp": 0, "pc": "0x100", "sass": "NOP ;"}]
        fields = get_display_fields(records, "warp,sass")
        self.assertEqual(fields, ["warp", "sass"])

    def test_get_display_fields_with_spaces(self):
        """Test fields with spaces are trimmed."""
        records = [{"warp": 0, "pc": "0x100"}]
        fields = get_display_fields(records, " warp , pc ")
        self.assertEqual(fields, ["warp", "pc"])

    def test_get_display_fields_empty_records(self):
        """Test empty records returns DEFAULT_FIELDS."""
        fields = get_display_fields([])
        self.assertEqual(fields, DEFAULT_FIELDS)

    def test_get_display_fields_missing_default(self):
        """Test default returns all record fields."""
        records = [{"warp": 0, "custom": "value"}]
        fields = get_display_fields(records)
        # Default behavior now returns all fields from the record
        self.assertEqual(fields, ["warp", "custom"])

    def test_get_display_fields_all_union(self):
        """Test --fields all returns union of fields from all records.

        This tests the fix for the bug where fields like 'uregs' that only
        appear in some records were missing from the output.
        """
        records = [
            {"warp": 0, "pc": "0x0", "regs": []},
            {"warp": 0, "pc": "0x20", "regs": [], "uregs": [1, 2]},
            {"warp": 0, "pc": "0x30", "addrs": [100], "values": [200]},
        ]
        fields = get_display_fields(records, "all")

        # Should include fields from all records
        self.assertIn("uregs", fields)
        self.assertIn("addrs", fields)
        self.assertIn("values", fields)

        # First record's fields should come first (preserving order)
        self.assertEqual(fields[:3], ["warp", "pc", "regs"])

    def test_get_display_fields_star_union(self):
        """Test --fields '*' also returns union of fields."""
        records = [
            {"warp": 0, "pc": "0x0"},
            {"warp": 0, "pc": "0x20", "uregs": [1, 2]},
        ]
        fields = get_display_fields(records, "*")

        self.assertIn("uregs", fields)
        self.assertEqual(fields[:2], ["warp", "pc"])

    def test_get_display_fields_all_with_spaces(self):
        """Test --fields ' all ' with whitespace is handled."""
        records = [
            {"warp": 0, "pc": "0x0"},
            {"warp": 0, "pc": "0x20", "uregs": [1, 2]},
        ]
        fields = get_display_fields(records, "  all  ")

        self.assertIn("uregs", fields)

    def test_get_display_fields_all_single_record(self):
        """Test --fields all with single record returns that record's fields."""
        records = [{"warp": 0, "pc": "0x0", "sass": "NOP ;"}]
        fields = get_display_fields(records, "all")

        self.assertEqual(fields, ["warp", "pc", "sass"])


class TestFormatRecordsTable(unittest.TestCase):
    """Tests for format_records_table function."""

    def test_format_table_empty_records(self):
        """Test empty records returns message."""
        result = format_records_table([], ["warp", "pc"])
        self.assertEqual(result, "No records found.")

    def test_format_table_with_header(self):
        """Test table with header row."""
        records = [{"warp": 0, "pc": "0x100"}]
        result = format_records_table(records, ["warp", "pc"], show_header=True)
        lines = result.split("\n")
        self.assertEqual(len(lines), 2)
        self.assertIn("WARP", lines[0])
        self.assertIn("PC", lines[0])
        self.assertIn("0", lines[1])
        self.assertIn("0x100", lines[1])

    def test_format_table_without_header(self):
        """Test table without header row."""
        records = [{"warp": 0, "pc": "0x100"}]
        result = format_records_table(records, ["warp", "pc"], show_header=False)
        lines = result.split("\n")
        self.assertEqual(len(lines), 1)
        self.assertNotIn("WARP", result)

    def test_format_table_column_alignment(self):
        """Test columns are aligned by width."""
        records = [
            {"warp": 0, "sass": "NOP ;"},
            {"warp": 123, "sass": "EXIT ;"},
        ]
        result = format_records_table(records, ["warp", "sass"], show_header=True)
        lines = result.split("\n")
        # All lines should have consistent column positions
        self.assertEqual(len(lines), 3)  # header + 2 data rows

    def test_format_table_missing_field(self):
        """Test missing field shows empty string."""
        records = [{"warp": 0}]  # no 'pc' field
        result = format_records_table(records, ["warp", "pc"], show_header=False)
        self.assertIn("0", result)


class TestFormatRecordsJson(unittest.TestCase):
    """Tests for format_records_json function."""

    def test_format_json_empty_records(self):
        """Test empty records returns empty array."""
        result = format_records_json([], ["warp"])
        self.assertEqual(result, "[]")

    def test_format_json_single_record(self):
        """Test single record formatting."""
        records = [{"warp": 0, "pc": "0x100", "extra": "ignored"}]
        result = format_records_json(records, ["warp", "pc"])
        parsed = json.loads(result)
        self.assertEqual(len(parsed), 1)
        self.assertEqual(parsed[0]["warp"], 0)
        self.assertEqual(parsed[0]["pc"], "0x100")
        self.assertNotIn("extra", parsed[0])

    def test_format_json_multiple_records(self):
        """Test multiple records formatting."""
        records = [{"warp": 0}, {"warp": 1}]
        result = format_records_json(records, ["warp"])
        parsed = json.loads(result)
        self.assertEqual(len(parsed), 2)

    def test_format_json_filters_fields(self):
        """Test only requested fields are included."""
        records = [{"warp": 0, "pc": "0x100", "sass": "NOP ;"}]
        result = format_records_json(records, ["warp"])
        parsed = json.loads(result)
        self.assertEqual(list(parsed[0].keys()), ["warp"])


class TestFormatRecordsCsv(unittest.TestCase):
    """Tests for format_records_csv function."""

    def test_format_csv_empty_records(self):
        """Test empty records returns empty string."""
        result = format_records_csv([], ["warp"])
        self.assertEqual(result, "")

    def test_format_csv_with_header(self):
        """Test CSV with header row."""
        records = [{"warp": 0, "pc": "0x100"}]
        result = format_records_csv(records, ["warp", "pc"], show_header=True)
        lines = result.split("\n")
        self.assertEqual(len(lines), 2)
        self.assertEqual(lines[0], "warp,pc")
        self.assertEqual(lines[1], "0,0x100")

    def test_format_csv_without_header(self):
        """Test CSV without header row."""
        records = [{"warp": 0, "pc": "0x100"}]
        result = format_records_csv(records, ["warp", "pc"], show_header=False)
        lines = result.split("\n")
        self.assertEqual(len(lines), 1)
        self.assertEqual(lines[0], "0,0x100")

    def test_format_csv_escapes_comma(self):
        """Test CSV properly escapes values with commas."""
        records = [{"sass": "MOV R0, R1 ;"}]
        result = format_records_csv(records, ["sass"], show_header=False)
        # CSV should quote the value containing comma
        self.assertIn('"', result)

    def test_format_csv_multiple_records(self):
        """Test CSV with multiple records."""
        records = [{"warp": 0}, {"warp": 1}, {"warp": 2}]
        result = format_records_csv(records, ["warp"], show_header=True)
        lines = result.split("\n")
        self.assertEqual(len(lines), 4)  # 1 header + 3 data


if __name__ == "__main__":
    unittest.main()

```

`python/tests/test_grouper.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Unit tests for StreamingGrouper class.
"""

import unittest

from cutracer.query.grouper import StreamingGrouper


class TestStreamingGrouperInit(unittest.TestCase):
    """Tests for StreamingGrouper initialization."""

    def test_init_stores_group_field(self):
        """Test that group field is stored correctly."""
        records = iter([])
        grouper = StreamingGrouper(records, "warp")
        self.assertEqual(grouper._group_field, "warp")

    def test_init_not_consumed(self):
        """Test that grouper is not consumed initially."""
        records = iter([])
        grouper = StreamingGrouper(records, "warp")
        self.assertFalse(grouper._consumed)


class TestHeadPerGroup(unittest.TestCase):
    """Tests for head_per_group method."""

    def test_head_per_group_basic(self):
        """Test basic head per group functionality."""
        records = iter(
            [
                {"warp": 0, "pc": 1},
                {"warp": 0, "pc": 2},
                {"warp": 1, "pc": 3},
                {"warp": 0, "pc": 4},
                {"warp": 1, "pc": 5},
            ]
        )
        grouper = StreamingGrouper(records, "warp")
        groups = grouper.head_per_group(2)

        self.assertEqual(len(groups), 2)
        self.assertEqual(len(groups[0]), 2)
        self.assertEqual(len(groups[1]), 2)
        self.assertEqual(groups[0][0]["pc"], 1)
        self.assertEqual(groups[0][1]["pc"], 2)

    def test_head_per_group_limits_count(self):
        """Test that head limits records per group."""
        records = iter([{"warp": 0, "pc": i} for i in range(10)])
        grouper = StreamingGrouper(records, "warp")
        groups = grouper.head_per_group(3)

        self.assertEqual(len(groups[0]), 3)

    def test_head_per_group_zero(self):
        """Test head=0 returns empty dict."""
        records = iter([{"warp": 0}])
        grouper = StreamingGrouper(records, "warp")
        groups = grouper.head_per_group(0)

        self.assertEqual(groups, {})

    def test_head_per_group_empty_records(self):
        """Test with empty records iterator."""
        records = iter([])
        grouper = StreamingGrouper(records, "warp")
        groups = grouper.head_per_group(10)

        self.assertEqual(groups, {})


class TestTailPerGroup(unittest.TestCase):
    """Tests for tail_per_group method."""

    def test_tail_per_group_basic(self):
        """Test basic tail per group functionality."""
        records = iter(
            [
                {"warp": 0, "pc": 1},
                {"warp": 0, "pc": 2},
                {"warp": 0, "pc": 3},
                {"warp": 1, "pc": 10},
                {"warp": 1, "pc": 20},
            ]
        )
        grouper = StreamingGrouper(records, "warp")
        groups = grouper.tail_per_group(2)

        self.assertEqual(len(groups), 2)
        self.assertEqual(len(groups[0]), 2)
        # Should have last 2 records for warp 0
        self.assertEqual(groups[0][0]["pc"], 2)
        self.assertEqual(groups[0][1]["pc"], 3)

    def test_tail_per_group_discards_old(self):
        """Test that tail discards older records."""
        records = iter([{"warp": 0, "pc": i} for i in range(10)])
        grouper = StreamingGrouper(records, "warp")
        groups = grouper.tail_per_group(3)

        self.assertEqual(len(groups[0]), 3)
        # Should have pc values 7, 8, 9
        self.assertEqual(groups[0][0]["pc"], 7)
        self.assertEqual(groups[0][1]["pc"], 8)
        self.assertEqual(groups[0][2]["pc"], 9)

    def test_tail_per_group_zero(self):
        """Test tail=0 returns empty dict."""
        records = iter([{"warp": 0}])
        grouper = StreamingGrouper(records, "warp")
        groups = grouper.tail_per_group(0)

        self.assertEqual(groups, {})


class TestCountPerGroup(unittest.TestCase):
    """Tests for count_per_group method."""

    def test_count_per_group_basic(self):
        """Test basic count per group functionality."""
        records = iter(
            [
                {"warp": 0},
                {"warp": 0},
                {"warp": 0},
                {"warp": 1},
                {"warp": 1},
                {"warp": 2},
            ]
        )
        grouper = StreamingGrouper(records, "warp")
        counts = grouper.count_per_group()

        self.assertEqual(counts[0], 3)
        self.assertEqual(counts[1], 2)
        self.assertEqual(counts[2], 1)

    def test_count_per_group_empty(self):
        """Test count with empty records."""
        records = iter([])
        grouper = StreamingGrouper(records, "warp")
        counts = grouper.count_per_group()

        self.assertEqual(counts, {})


class TestConsumedBehavior(unittest.TestCase):
    """Tests for consumed state handling."""

    def test_cannot_use_after_consumed(self):
        """Test that grouper raises error after consumption."""
        records = iter([{"warp": 0}])
        grouper = StreamingGrouper(records, "warp")

        # First call consumes the iterator
        grouper.head_per_group(10)

        # Second call should raise
        with self.assertRaises(RuntimeError) as ctx:
            grouper.head_per_group(10)

        self.assertIn("already been consumed", str(ctx.exception))

    def test_different_methods_consume(self):
        """Test that any method marks as consumed."""
        records = iter([{"warp": 0}])
        grouper = StreamingGrouper(records, "warp")

        grouper.count_per_group()

        with self.assertRaises(RuntimeError):
            grouper.tail_per_group(10)


if __name__ == "__main__":
    unittest.main()

```

`python/tests/test_json_validator.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Unit tests for JSON validator module.
"""

import unittest

from cutracer.validation.json_validator import (
    JsonValidationError,
    validate_json_schema,
    validate_json_syntax,
    validate_json_trace,
)
from tests.test_base import (
    BaseValidationTest,
    INVALID_SCHEMA_NDJSON,
    INVALID_SYNTAX_NDJSON,
    REG_TRACE_NDJSON,
    REG_TRACE_NDJSON_RECORD_COUNT,
    REG_TRACE_NDJSON_ZST,
    REG_TRACE_NDJSON_ZST_RECORD_COUNT,
)


class JsonValidatorTest(BaseValidationTest):
    """Core tests for JSON validator functions."""

    def test_validate_json_syntax_valid_file(self):
        """Test validation of real NDJSON trace file."""
        valid_count, errors = validate_json_syntax(REG_TRACE_NDJSON)

        self.assertEqual(valid_count, REG_TRACE_NDJSON_RECORD_COUNT)
        self.assertEqual(len(errors), 0)

    def test_validate_json_syntax_invalid_json(self):
        """Test detection of invalid JSON syntax."""
        valid_count, errors = validate_json_syntax(INVALID_SYNTAX_NDJSON)

        self.assertEqual(valid_count, 2)
        self.assertEqual(len(errors), 1)
        self.assertIn("Line 2", errors[0])

    def test_validate_json_syntax_file_not_found(self):
        """Test handling of non-existent file."""
        non_existent = self.temp_dir / "missing.ndjson"

        with self.assertRaises(FileNotFoundError):
            validate_json_syntax(non_existent)

    def test_validate_json_schema_valid(self):
        """Test schema validation with real trace data."""
        result = validate_json_schema(REG_TRACE_NDJSON, message_type="reg_trace")

        self.assertTrue(result)

    def test_validate_json_schema_invalid(self):
        """Test schema validation with invalid records."""
        with self.assertRaises(JsonValidationError) as ctx:
            validate_json_schema(INVALID_SCHEMA_NDJSON, message_type="reg_trace")

        self.assertIn("Schema validation failed", str(ctx.exception))

    def test_validate_json_trace_complete(self):
        """Test complete validation of real trace file."""
        result = validate_json_trace(REG_TRACE_NDJSON)

        self.assertTrue(result["valid"])
        self.assertEqual(result["record_count"], REG_TRACE_NDJSON_RECORD_COUNT)
        self.assertEqual(result["message_type"], "reg_trace")
        self.assertEqual(len(result["errors"]), 0)

    def test_validate_json_trace_empty_file(self):
        """Test validation of empty file."""
        empty_file = self.create_temp_file("empty.ndjson", "")

        result = validate_json_trace(empty_file)

        self.assertFalse(result["valid"])
        self.assertEqual(result["record_count"], 0)


class JsonValidatorCompressedTest(BaseValidationTest):
    """Tests for JSON validator with compressed files."""

    def test_validate_json_syntax_compressed(self):
        """Test validation of Zstd compressed NDJSON trace file."""
        if not REG_TRACE_NDJSON_ZST.exists():
            self.skipTest("Zstd test file not available")

        valid_count, errors = validate_json_syntax(REG_TRACE_NDJSON_ZST)

        self.assertEqual(valid_count, REG_TRACE_NDJSON_ZST_RECORD_COUNT)
        self.assertEqual(len(errors), 0)

    def test_validate_json_schema_compressed(self):
        """Test schema validation with compressed trace data."""
        if not REG_TRACE_NDJSON_ZST.exists():
            self.skipTest("Zstd test file not available")

        result = validate_json_schema(REG_TRACE_NDJSON_ZST, message_type="reg_trace")

        self.assertTrue(result)

    def test_validate_json_trace_compressed(self):
        """Test complete validation of compressed trace file."""
        if not REG_TRACE_NDJSON_ZST.exists():
            self.skipTest("Zstd test file not available")

        result = validate_json_trace(REG_TRACE_NDJSON_ZST)

        self.assertTrue(result["valid"])
        self.assertEqual(result["record_count"], REG_TRACE_NDJSON_ZST_RECORD_COUNT)
        self.assertEqual(result["message_type"], "reg_trace")
        self.assertEqual(result["compression"], "zstd")
        self.assertEqual(len(result["errors"]), 0)

    def test_validate_json_trace_compression_field_uncompressed(self):
        """Test that uncompressed files have compression='none'."""
        result = validate_json_trace(REG_TRACE_NDJSON)

        self.assertEqual(result["compression"], "none")


if __name__ == "__main__":
    unittest.main()

```

`python/tests/test_query_cli.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Unit tests for query CLI command.
"""

import unittest

from click.testing import CliRunner
from cutracer.cli import main
from tests.test_base import BaseValidationTest, REG_TRACE_NDJSON, REG_TRACE_NDJSON_ZST


class TestQueryCommand(BaseValidationTest):
    """Tests for query CLI command."""

    def setUp(self):
        super().setUp()
        self.runner = CliRunner()

    def test_query_help(self):
        """Test query --help shows usage information."""
        result = self.runner.invoke(main, ["query", "--help"])
        self.assertEqual(result.exit_code, 0)
        self.assertIn("Query and view trace data", result.output)
        self.assertIn("--head", result.output)
        self.assertIn("--tail", result.output)
        self.assertIn("--filter", result.output)

    def test_analyze_default_head(self):
        """Test analyze with default head (10 records)."""
        result = self.runner.invoke(main, ["query", str(REG_TRACE_NDJSON)])
        self.assertEqual(result.exit_code, 0)
        # Should have header + 10 data rows = 11 lines
        lines = [line for line in result.output.strip().split("\n") if line]
        self.assertEqual(len(lines), 11)

    def test_analyze_custom_head(self):
        """Test analyze with custom head value."""
        result = self.runner.invoke(
            main, ["query", str(REG_TRACE_NDJSON), "--head", "5"]
        )
        self.assertEqual(result.exit_code, 0)
        lines = [line for line in result.output.strip().split("\n") if line]
        self.assertEqual(len(lines), 6)  # header + 5 data rows

    def test_analyze_tail(self):
        """Test analyze with tail option."""
        result = self.runner.invoke(
            main, ["query", str(REG_TRACE_NDJSON), "--tail", "3"]
        )
        self.assertEqual(result.exit_code, 0)
        lines = [line for line in result.output.strip().split("\n") if line]
        self.assertEqual(len(lines), 4)  # header + 3 data rows

    def test_analyze_zst_file(self):
        """Test analyze with Zstd-compressed file."""
        result = self.runner.invoke(
            main, ["query", str(REG_TRACE_NDJSON_ZST), "--head", "5"]
        )
        self.assertEqual(result.exit_code, 0)
        lines = [line for line in result.output.strip().split("\n") if line]
        self.assertEqual(len(lines), 6)

    def test_analyze_filter(self):
        """Test analyze with filter expression."""
        result = self.runner.invoke(
            main,
            ["query", str(REG_TRACE_NDJSON), "--filter", "warp=0", "--head", "5"],
        )
        self.assertEqual(result.exit_code, 0)
        # All displayed records should have warp=0
        # Check that output contains "0" in the warp column
        self.assertIn("0", result.output)

    def test_analyze_filter_invalid(self):
        """Test analyze with invalid filter expression."""
        result = self.runner.invoke(
            main, ["query", str(REG_TRACE_NDJSON), "--filter", "invalid"]
        )
        self.assertNotEqual(result.exit_code, 0)
        self.assertIn("Invalid filter expression", result.output)

    def test_analyze_nonexistent_file(self):
        """Test analyze with non-existent file."""
        result = self.runner.invoke(main, ["query", "/nonexistent/file.ndjson"])
        self.assertNotEqual(result.exit_code, 0)

    def test_analyze_empty_file(self):
        """Test analyze with empty file."""
        empty_file = self.create_temp_file("empty.ndjson", "")
        result = self.runner.invoke(main, ["query", str(empty_file)])
        self.assertEqual(result.exit_code, 0)
        self.assertIn("No records found", result.output)

    def test_analyze_short_options(self):
        """Test analyze with short option names."""
        result = self.runner.invoke(
            main, ["query", str(REG_TRACE_NDJSON), "-n", "3", "-f", "warp=0"]
        )
        self.assertEqual(result.exit_code, 0)

    def test_analyze_format_json(self):
        """Test analyze with JSON output format."""
        result = self.runner.invoke(
            main, ["query", str(REG_TRACE_NDJSON), "--format", "json", "--head", "3"]
        )
        self.assertEqual(result.exit_code, 0)
        # JSON output should be parseable
        import json

        data = json.loads(result.output)
        self.assertEqual(len(data), 3)

    def test_analyze_format_csv(self):
        """Test analyze with CSV output format."""
        result = self.runner.invoke(
            main, ["query", str(REG_TRACE_NDJSON), "--format", "csv", "--head", "3"]
        )
        self.assertEqual(result.exit_code, 0)
        lines = result.output.strip().split("\n")
        self.assertEqual(len(lines), 4)  # header + 3 data rows
        # CSV header should contain field names
        self.assertIn("warp", lines[0])

    def test_analyze_format_table_explicit(self):
        """Test analyze with explicit table format."""
        result = self.runner.invoke(
            main,
            ["query", str(REG_TRACE_NDJSON), "--format", "table", "--head", "3"],
        )
        self.assertEqual(result.exit_code, 0)
        lines = [line for line in result.output.strip().split("\n") if line]
        self.assertEqual(len(lines), 4)  # header + 3 data rows

    def test_analyze_fields_custom(self):
        """Test analyze with custom fields."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(REG_TRACE_NDJSON),
                "--fields",
                "warp,sass",
                "--head",
                "3",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # Output should contain WARP and SASS headers
        self.assertIn("WARP", result.output)
        self.assertIn("SASS", result.output)
        # But not PC (since we only asked for warp,sass)
        self.assertNotIn("PC", result.output.split("\n")[0])

    def test_analyze_no_header(self):
        """Test analyze with --no-header flag."""
        result = self.runner.invoke(
            main, ["query", str(REG_TRACE_NDJSON), "--no-header", "--head", "3"]
        )
        self.assertEqual(result.exit_code, 0)
        lines = [line for line in result.output.strip().split("\n") if line]
        self.assertEqual(len(lines), 3)  # 3 data rows, no header

    def test_analyze_format_csv_no_header(self):
        """Test analyze with CSV format and no header."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(REG_TRACE_NDJSON),
                "--format",
                "csv",
                "--no-header",
                "--head",
                "3",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        lines = result.output.strip().split("\n")
        self.assertEqual(len(lines), 3)  # 3 data rows, no header

    def test_analyze_group_by_basic(self):
        """Test analyze with --group-by option."""
        result = self.runner.invoke(
            main,
            ["query", str(REG_TRACE_NDJSON), "--group-by", "warp", "--head", "3"],
        )
        self.assertEqual(result.exit_code, 0)
        # Should have group headers
        self.assertIn("=== Group:", result.output)
        self.assertIn("records) ===", result.output)

    def test_analyze_group_by_with_tail(self):
        """Test analyze with --group-by and --tail."""
        result = self.runner.invoke(
            main,
            ["query", str(REG_TRACE_NDJSON), "--group-by", "warp", "--tail", "2"],
        )
        self.assertEqual(result.exit_code, 0)
        self.assertIn("=== Group:", result.output)

    def test_analyze_group_by_with_filter(self):
        """Test analyze with --group-by and --filter."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(REG_TRACE_NDJSON),
                "--group-by",
                "warp",
                "--filter",
                "warp=0",
                "--head",
                "5",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # Should only have warp=0 group
        self.assertIn("=== Group: 0", result.output)

    def test_analyze_group_by_json_format(self):
        """Test analyze with --group-by and JSON format."""
        import json

        result = self.runner.invoke(
            main,
            [
                "query",
                str(REG_TRACE_NDJSON),
                "--group-by",
                "warp",
                "--format",
                "json",
                "--head",
                "2",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # JSON output should be valid and parseable
        data = json.loads(result.output)
        # Since --group-by warp is used, output should have groups and warp_summary
        self.assertIn("groups", data)
        self.assertIsInstance(data["groups"], dict)
        # Each group should be a list of records
        for records in data["groups"].values():
            self.assertIsInstance(records, list)

    def test_analyze_group_by_csv_format(self):
        """Test analyze with --group-by and CSV format."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(REG_TRACE_NDJSON),
                "--group-by",
                "warp",
                "--format",
                "csv",
                "--head",
                "2",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        self.assertIn("=== Group:", result.output)

    def test_analyze_group_by_short_option(self):
        """Test analyze with -g short option for --group-by."""
        result = self.runner.invoke(
            main,
            ["query", str(REG_TRACE_NDJSON), "-g", "warp", "-n", "2"],
        )
        self.assertEqual(result.exit_code, 0)
        self.assertIn("=== Group:", result.output)

    def test_analyze_count_requires_group_by(self):
        """Test that --count requires --group-by."""
        result = self.runner.invoke(main, ["query", str(REG_TRACE_NDJSON), "--count"])
        self.assertNotEqual(result.exit_code, 0)
        self.assertIn("--count requires --group-by", result.output)

    def test_analyze_top_requires_count(self):
        """Test that --top requires --count."""
        result = self.runner.invoke(
            main,
            ["query", str(REG_TRACE_NDJSON), "--group-by", "warp", "--top", "5"],
        )
        self.assertNotEqual(result.exit_code, 0)
        self.assertIn("--top requires --count", result.output)

    def test_analyze_count_basic(self):
        """Test analyze with --group-by and --count."""
        result = self.runner.invoke(
            main,
            ["query", str(REG_TRACE_NDJSON), "--group-by", "warp", "--count"],
        )
        self.assertEqual(result.exit_code, 0)
        self.assertIn("WARP", result.output)
        self.assertIn("COUNT", result.output)

    def test_analyze_count_top(self):
        """Test analyze with --count --top."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(REG_TRACE_NDJSON),
                "--group-by",
                "warp",
                "--count",
                "--top",
                "3",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        lines = [line for line in result.output.strip().split("\n") if line]
        self.assertEqual(len(lines), 4)  # header + 3 data rows

    def test_analyze_count_json_format(self):
        """Test analyze --count with JSON format."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(REG_TRACE_NDJSON),
                "--group-by",
                "warp",
                "--count",
                "--format",
                "json",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        import json

        data = json.loads(result.output)
        self.assertIsInstance(data, dict)

    def test_analyze_count_csv_format(self):
        """Test analyze --count with CSV format."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(REG_TRACE_NDJSON),
                "--group-by",
                "warp",
                "--count",
                "--format",
                "csv",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        self.assertIn("warp,count", result.output)

    def test_analyze_count_no_header(self):
        """Test analyze --count with --no-header."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(REG_TRACE_NDJSON),
                "--group-by",
                "warp",
                "--count",
                "--no-header",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # Should not contain header row
        self.assertNotIn("WARP", result.output)
        self.assertNotIn("COUNT", result.output)

    def test_analyze_group_by_warp_shows_summary(self):
        """Test analyze --group-by warp shows warp summary."""
        result = self.runner.invoke(
            main,
            ["query", str(REG_TRACE_NDJSON), "--group-by", "warp", "--tail", "3"],
        )
        self.assertEqual(result.exit_code, 0)
        # Should have warp summary section
        self.assertIn("Warp Summary", result.output)
        self.assertIn("Total warps observed:", result.output)
        self.assertIn("Completed (EXIT):", result.output)
        self.assertIn("In-progress:", result.output)
        self.assertIn("Missing (never seen):", result.output)

    def test_analyze_group_by_warp_json_has_summary(self):
        """Test analyze --group-by warp with JSON format includes warp_summary."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(REG_TRACE_NDJSON),
                "--group-by",
                "warp",
                "--format",
                "json",
                "--head",
                "2",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        import json

        data = json.loads(result.output)
        # JSON should have groups and warp_summary keys
        self.assertIn("groups", data)
        self.assertIn("warp_summary", data)
        self.assertIn("total_observed", data["warp_summary"])
        self.assertIn("completed", data["warp_summary"])
        self.assertIn("in_progress", data["warp_summary"])
        self.assertIn("missing", data["warp_summary"])

    def test_analyze_group_by_non_warp_no_summary(self):
        """Test analyze --group-by with non-warp field does not show warp summary."""
        result = self.runner.invoke(
            main,
            ["query", str(REG_TRACE_NDJSON), "--group-by", "sass", "--head", "2"],
        )
        self.assertEqual(result.exit_code, 0)
        # Should NOT have warp summary section
        self.assertNotIn("Warp Summary", result.output)

    def test_analyze_group_by_warp_csv_no_summary(self):
        """Test analyze --group-by warp with CSV format does not include summary."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(REG_TRACE_NDJSON),
                "--group-by",
                "warp",
                "--format",
                "csv",
                "--head",
                "2",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # CSV format should NOT have warp summary section
        self.assertNotIn("Warp Summary", result.output)

    def test_analyze_all_records(self):
        """Test analyze with --all-lines flag to show all records."""
        # First count total records
        result_all = self.runner.invoke(
            main, ["query", str(REG_TRACE_NDJSON), "--all-lines", "--no-header"]
        )
        self.assertEqual(result_all.exit_code, 0)
        all_lines = [line for line in result_all.output.strip().split("\n") if line]

        # Compare with head 10
        result_head = self.runner.invoke(
            main, ["query", str(REG_TRACE_NDJSON), "--head", "10", "--no-header"]
        )
        head_lines = [line for line in result_head.output.strip().split("\n") if line]

        # All should have >= 10 records (or same if file has exactly 10)
        self.assertGreaterEqual(len(all_lines), len(head_lines))

    def test_analyze_all_short_option(self):
        """Test analyze with -a short option for --all-lines."""
        result = self.runner.invoke(
            main, ["query", str(REG_TRACE_NDJSON), "-a", "--no-header"]
        )
        self.assertEqual(result.exit_code, 0)

    def test_analyze_format_ndjson(self):
        """Test analyze with NDJSON output format."""
        result = self.runner.invoke(
            main, ["query", str(REG_TRACE_NDJSON), "--format", "ndjson", "--head", "3"]
        )
        self.assertEqual(result.exit_code, 0)
        lines = result.output.strip().split("\n")
        self.assertEqual(len(lines), 3)
        # Each line should be valid JSON
        import json

        for line in lines:
            data = json.loads(line)
            self.assertIsInstance(data, dict)

    def test_analyze_filter_hex_pc(self):
        """Test analyze with hex filter for PC value."""
        # First get the first record's PC
        result = self.runner.invoke(
            main,
            ["query", str(REG_TRACE_NDJSON), "--format", "json", "--head", "1"],
        )
        self.assertEqual(result.exit_code, 0)
        import json

        records = json.loads(result.output)
        if records and "pc" in records[0]:
            pc_value = records[0]["pc"]
            # pc is now a hex string like "0x0" or "0x10"
            # Filter using the hex string directly
            result_hex = self.runner.invoke(
                main,
                [
                    "query",
                    str(REG_TRACE_NDJSON),
                    "--filter",
                    f"pc={pc_value}",
                    "--head",
                    "5",
                ],
            )
            self.assertEqual(result_hex.exit_code, 0)
            # All filtered records should have this PC
            self.assertIn(pc_value, result_hex.output)

    def test_analyze_fields_all(self):
        """Test analyze with --fields '*' to show all fields."""
        result = self.runner.invoke(
            main,
            ["query", str(REG_TRACE_NDJSON), "--fields", "*", "--head", "1"],
        )
        self.assertEqual(result.exit_code, 0)
        # Should show more than just default fields (warp, pc, sass)
        header = result.output.strip().split("\n")[0]
        # Check that non-default fields are included (like type, time, etc.)
        # At minimum should have WARP, PC, SASS
        self.assertIn("WARP", header)
        self.assertIn("PC", header)

    def test_analyze_fields_all_keyword(self):
        """Test analyze with --fields 'all' to show all fields."""
        result = self.runner.invoke(
            main,
            ["query", str(REG_TRACE_NDJSON), "--fields", "all", "--head", "1"],
        )
        self.assertEqual(result.exit_code, 0)
        header = result.output.strip().split("\n")[0]
        self.assertIn("WARP", header)
        self.assertIn("PC", header)

    def test_analyze_output_file(self):
        """Test analyze with --output to write to file."""
        import os
        import tempfile

        with tempfile.TemporaryDirectory() as tmpdir:
            output_path = os.path.join(tmpdir, "output.txt")
            result = self.runner.invoke(
                main,
                [
                    "query",
                    str(REG_TRACE_NDJSON),
                    "--head",
                    "3",
                    "--output",
                    output_path,
                ],
            )
            self.assertEqual(result.exit_code, 0)
            # Check file was created
            self.assertTrue(os.path.exists(output_path))
            # Check file content
            with open(output_path) as f:
                content = f.read()
            lines = [line for line in content.strip().split("\n") if line]
            self.assertEqual(len(lines), 4)  # header + 3 rows

    def test_analyze_output_ndjson_file(self):
        """Test analyze with --output and --format ndjson."""
        import json
        import os
        import tempfile

        with tempfile.TemporaryDirectory() as tmpdir:
            output_path = os.path.join(tmpdir, "output.ndjson")
            result = self.runner.invoke(
                main,
                [
                    "query",
                    str(REG_TRACE_NDJSON),
                    "--format",
                    "ndjson",
                    "--head",
                    "3",
                    "--output",
                    output_path,
                ],
            )
            self.assertEqual(result.exit_code, 0)
            self.assertTrue(os.path.exists(output_path))
            # Check file content is valid NDJSON
            with open(output_path) as f:
                lines = f.read().strip().split("\n")
            self.assertEqual(len(lines), 3)
            for line in lines:
                data = json.loads(line)
                self.assertIsInstance(data, dict)

    def test_analyze_group_by_all_records(self):
        """Test analyze with --group-by and --all-lines."""
        result = self.runner.invoke(
            main,
            ["query", str(REG_TRACE_NDJSON), "--group-by", "warp", "--all-lines"],
        )
        self.assertEqual(result.exit_code, 0)
        self.assertIn("=== Group:", result.output)

    def test_analyze_count_ndjson_format(self):
        """Test analyze --count with NDJSON format."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(REG_TRACE_NDJSON),
                "--group-by",
                "warp",
                "--count",
                "--format",
                "ndjson",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        import json

        lines = result.output.strip().split("\n")
        for line in lines:
            data = json.loads(line)
            self.assertIn("warp", data)
            self.assertIn("count", data)

    def test_query_output_compress(self):
        """Test query with --output and --compress produces valid Zstd file."""
        import json
        import os
        import tempfile

        import zstandard as zstd

        with tempfile.TemporaryDirectory() as tmpdir:
            output_path = os.path.join(tmpdir, "output.ndjson.zst")
            result = self.runner.invoke(
                main,
                [
                    "query",
                    str(REG_TRACE_NDJSON),
                    "--format",
                    "ndjson",
                    "--head",
                    "3",
                    "--output",
                    output_path,
                    "--compress",
                ],
            )
            self.assertEqual(result.exit_code, 0)
            self.assertTrue(os.path.exists(output_path))

            # Verify it's valid zstd compressed data
            dctx = zstd.ZstdDecompressor()
            with open(output_path, "rb") as f:
                content = dctx.decompress(f.read()).decode("utf-8")
            lines = [line for line in content.strip().split("\n") if line]
            self.assertEqual(len(lines), 3)
            for line in lines:
                data = json.loads(line)
                self.assertIsInstance(data, dict)

    def test_query_compress_requires_output(self):
        """Test that --compress without --output raises an error."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(REG_TRACE_NDJSON),
                "--head",
                "3",
                "--compress",
            ],
        )
        self.assertNotEqual(result.exit_code, 0)
        self.assertIn("--compress requires --output", result.output)

    def test_query_ndjson_all_fields_heterogeneous(self):
        """Test --fields all --format ndjson preserves all fields from heterogeneous records.

        This tests the fix for the bug where fields like 'uregs' that only
        appear in some records were missing from the NDJSON output.
        """
        import json
        import os
        import tempfile

        # Create test file with heterogeneous records (different field sets)
        test_records = [
            {"warp": 0, "pc": "0x0", "sass": "NOP ;", "regs": [1, 2]},
            {"warp": 0, "pc": "0x20", "sass": "ULDC ;", "regs": [], "uregs": [10, 20]},
            {"warp": 0, "pc": "0x30", "sass": "LDG ;", "addrs": [100], "values": [200]},
        ]

        with tempfile.TemporaryDirectory() as tmpdir:
            input_path = os.path.join(tmpdir, "hetero.ndjson")
            output_path = os.path.join(tmpdir, "output.ndjson")

            # Write test input
            with open(input_path, "w") as f:
                for record in test_records:
                    f.write(json.dumps(record) + "\n")

            # Run query with --fields all --format ndjson
            result = self.runner.invoke(
                main,
                [
                    "query",
                    input_path,
                    "--fields",
                    "all",
                    "--format",
                    "ndjson",
                    "--all-lines",
                    "--output",
                    output_path,
                ],
            )
            self.assertEqual(result.exit_code, 0)

            # Read back and verify all records preserved
            with open(output_path) as f:
                output_lines = f.read().strip().split("\n")

            self.assertEqual(len(output_lines), 3)

            # Parse each line
            out_records = [json.loads(line) for line in output_lines]

            # First record should NOT have 'uregs' (it's not in the original)
            self.assertNotIn("uregs", out_records[0])
            self.assertIn("regs", out_records[0])

            # Second record MUST have 'uregs' (this was the bug)
            self.assertIn("uregs", out_records[1])
            self.assertEqual(out_records[1]["uregs"], [10, 20])

            # Third record should have 'addrs' and 'values'
            self.assertIn("addrs", out_records[2])
            self.assertIn("values", out_records[2])

    def test_query_ndjson_all_fields_preserves_original_fields(self):
        """Test --fields all with NDJSON preserves each record's original fields exactly.

        Records should only contain the fields they originally had - no extra fields
        should be added even if other records have them.
        """
        import json
        import os
        import tempfile

        test_records = [
            {"warp": 0, "pc": "0x0", "regs": [1]},
            {"warp": 1, "pc": "0x10", "uregs": [2]},
        ]

        with tempfile.TemporaryDirectory() as tmpdir:
            input_path = os.path.join(tmpdir, "input.ndjson")
            output_path = os.path.join(tmpdir, "output.ndjson")

            with open(input_path, "w") as f:
                for record in test_records:
                    f.write(json.dumps(record) + "\n")

            result = self.runner.invoke(
                main,
                [
                    "query",
                    input_path,
                    "--fields",
                    "all",
                    "--format",
                    "ndjson",
                    "--all-lines",
                    "--output",
                    output_path,
                ],
            )
            self.assertEqual(result.exit_code, 0)

            with open(output_path) as f:
                output_lines = f.read().strip().split("\n")

            out_records = [json.loads(line) for line in output_lines]

            # First record should NOT have 'uregs' added
            self.assertIn("regs", out_records[0])
            self.assertNotIn("uregs", out_records[0])

            # Second record should NOT have 'regs' added
            self.assertIn("uregs", out_records[1])
            self.assertNotIn("regs", out_records[1])


if __name__ == "__main__":
    unittest.main()

```

`python/tests/test_query_cli_clp.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Unit tests for query CLI command.
"""

import unittest

import yscope_clp_core
from click.testing import CliRunner
from cutracer.cli import main
from tests.test_base import BaseValidationTest, REG_TRACE_NDJSON


class TestQueryCommand(BaseValidationTest):
    """Tests for query CLI command."""

    def setUp(self):
        super().setUp()
        self.runner = CliRunner()
        # create clp archive from ndjson
        self.clp_archive_path = self.temp_dir.joinpath("example_input.clp")
        with yscope_clp_core.open_archive(self.clp_archive_path, "w") as clp_archive:
            clp_archive.add(REG_TRACE_NDJSON)

    def test_analyze_default_head(self):
        """Test analyze with default head (10 records)."""
        result = self.runner.invoke(
            main, ["query", str(str(self.clp_archive_path.absolute()))]
        )
        self.assertEqual(result.exit_code, 0)
        # Should have header + 10 data rows = 11 lines
        lines = [line for line in result.stdout.strip().split("\n") if line]
        self.assertEqual(len(lines), 11)

    def test_analyze_custom_head(self):
        """Test analyze with custom head value."""
        result = self.runner.invoke(
            main, ["query", str(str(self.clp_archive_path.absolute())), "--head", "5"]
        )
        self.assertEqual(result.exit_code, 0)
        lines = [line for line in result.stdout.strip().split("\n") if line]
        self.assertEqual(len(lines), 6)  # header + 5 data rows

    def test_analyze_tail(self):
        """Test analyze with tail option."""
        result = self.runner.invoke(
            main, ["query", str(str(self.clp_archive_path.absolute())), "--tail", "3"]
        )
        self.assertEqual(result.exit_code, 0)
        lines = [line for line in result.stdout.strip().split("\n") if line]
        self.assertEqual(len(lines), 4)  # header + 3 data rows

    def test_analyze_filter(self):
        """Test analyze with filter expression."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--filter",
                "warp=0",
                "--head",
                "5",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # All displayed records should have warp=0
        # Check that output contains "0" in the warp column
        self.assertIn("0", result.stdout)

    def test_analyze_filter_invalid(self):
        """Test analyze with invalid filter expression."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--filter",
                "invalid",
            ],
        )
        self.assertNotEqual(result.exit_code, 0)
        self.assertIn("Invalid filter expression", str(result.exception))

    def test_analyze_short_options(self):
        """Test analyze with short option names."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "-n",
                "3",
                "-f",
                "warp=0",
            ],
        )
        self.assertEqual(result.exit_code, 0)

    def test_analyze_format_json(self):
        """Test analyze with JSON output format."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--format",
                "json",
                "--head",
                "3",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # JSON output should be parseable
        import json

        data = json.loads(result.stdout)
        self.assertEqual(len(data), 3)

    def test_analyze_format_csv(self):
        """Test analyze with CSV output format."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--format",
                "csv",
                "--head",
                "3",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        lines = result.stdout.strip().split("\n")
        self.assertEqual(len(lines), 4)  # header + 3 data rows
        # CSV header should contain field names
        self.assertIn("warp", lines[0])

    def test_analyze_format_table_explicit(self):
        """Test analyze with explicit table format."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--format",
                "table",
                "--head",
                "3",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        lines = [line for line in result.stdout.strip().split("\n") if line]
        self.assertEqual(len(lines), 4)  # header + 3 data rows

    def test_analyze_fields_custom(self):
        """Test analyze with custom fields."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--fields",
                "warp,sass",
                "--head",
                "3",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # Output should contain WARP and SASS headers
        self.assertIn("WARP", result.stdout)
        self.assertIn("SASS", result.stdout)
        # But not PC (since we only asked for warp,sass)
        self.assertNotIn("PC", result.stdout.split("\n")[0])

    def test_analyze_no_header(self):
        """Test analyze with --no-header flag."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--no-header",
                "--head",
                "3",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        lines = [line for line in result.stdout.strip().split("\n") if line]
        self.assertEqual(len(lines), 3)  # 3 data rows, no header

    def test_analyze_format_csv_no_header(self):
        """Test analyze with CSV format and no header."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--format",
                "csv",
                "--no-header",
                "--head",
                "3",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        lines = result.stdout.strip().split("\n")
        self.assertEqual(len(lines), 3)  # 3 data rows, no header

    def test_analyze_group_by_basic(self):
        """Test analyze with --group-by option."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--head",
                "3",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # Should have group headers
        self.assertIn("=== Group:", result.stdout)
        self.assertIn("records) ===", result.stdout)

    def test_analyze_group_by_with_tail(self):
        """Test analyze with --group-by and --tail."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--tail",
                "2",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        self.assertIn("=== Group:", result.stdout)

    def test_analyze_group_by_with_filter(self):
        """Test analyze with --group-by and --filter."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--filter",
                "warp=0",
                "--head",
                "5",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # Should only have warp=0 group
        self.assertIn("=== Group: 0", result.stdout)

    def test_analyze_group_by_json_format(self):
        """Test analyze with --group-by and JSON format."""
        import json

        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--format",
                "json",
                "--head",
                "2",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # JSON output should be valid and parseable
        data = json.loads(result.stdout)
        # Since --group-by warp is used, output should have groups and warp_summary
        self.assertIn("groups", data)
        self.assertIsInstance(data["groups"], dict)
        # Each group should be a list of records
        for records in data["groups"].values():
            self.assertIsInstance(records, list)

    def test_analyze_group_by_csv_format(self):
        """Test analyze with --group-by and CSV format."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--format",
                "csv",
                "--head",
                "2",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        self.assertIn("=== Group:", result.stdout)

    def test_analyze_group_by_short_option(self):
        """Test analyze with -g short option for --group-by."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "-g",
                "warp",
                "-n",
                "2",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        self.assertIn("=== Group:", result.stdout)

    def test_analyze_count_requires_group_by(self):
        """Test that --count requires --group-by."""
        result = self.runner.invoke(
            main, ["query", str(str(self.clp_archive_path.absolute())), "--count"]
        )
        self.assertNotEqual(result.exit_code, 0)
        self.assertIn("--count requires --group-by", str(result.stderr))

    def test_analyze_top_requires_count(self):
        """Test that --top requires --count."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--top",
                "5",
            ],
        )
        self.assertNotEqual(result.exit_code, 0)
        self.assertIn("--top requires --count", result.output)

    def test_analyze_count_basic(self):
        """Test analyze with --group-by and --count."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--count",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        self.assertIn("WARP", result.stdout)
        self.assertIn("COUNT", result.stdout)

    def test_analyze_count_top(self):
        """Test analyze with --count --top."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--count",
                "--top",
                "3",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        lines = [line for line in result.stdout.strip().split("\n") if line]
        self.assertEqual(len(lines), 4)  # header + 3 data rows

    def test_analyze_count_json_format(self):
        """Test analyze --count with JSON format."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--count",
                "--format",
                "json",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        import json

        data = json.loads(result.stdout)
        self.assertIsInstance(data, dict)

    def test_analyze_count_csv_format(self):
        """Test analyze --count with CSV format."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--count",
                "--format",
                "csv",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        self.assertIn("warp,count", result.stdout)

    def test_analyze_count_no_header(self):
        """Test analyze --count with --no-header."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--count",
                "--no-header",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # Should not contain header row
        self.assertNotIn("WARP", result.stdout)
        self.assertNotIn("COUNT", result.stdout)

    def test_analyze_group_by_warp_shows_summary(self):
        """Test analyze --group-by warp shows warp summary."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--tail",
                "3",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # Should have warp summary section
        self.assertIn("Warp Summary", result.stdout)
        self.assertIn("Total warps observed:", result.stdout)
        self.assertIn("Completed (EXIT):", result.stdout)
        self.assertIn("In-progress:", result.stdout)
        self.assertIn("Missing (never seen):", result.stdout)

    def test_analyze_group_by_warp_json_has_summary(self):
        """Test analyze --group-by warp with JSON format includes warp_summary."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--format",
                "json",
                "--head",
                "2",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        import json

        data = json.loads(result.stdout)
        # JSON should have groups and warp_summary keys
        self.assertIn("groups", data)
        self.assertIn("warp_summary", data)
        self.assertIn("total_observed", data["warp_summary"])
        self.assertIn("completed", data["warp_summary"])
        self.assertIn("in_progress", data["warp_summary"])
        self.assertIn("missing", data["warp_summary"])

    def test_analyze_group_by_non_warp_no_summary(self):
        """Test analyze --group-by with non-warp field does not show warp summary."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "sass",
                "--head",
                "2",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # Should NOT have warp summary section
        self.assertNotIn("Warp Summary", result.stdout)

    def test_analyze_group_by_warp_csv_no_summary(self):
        """Test analyze --group-by warp with CSV format does not include summary."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--format",
                "csv",
                "--head",
                "2",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # CSV format should NOT have warp summary section
        self.assertNotIn("Warp Summary", result.stdout)

    def test_analyze_all_records(self):
        """Test analyze with --all-lines flag to show all records."""
        # First count total records
        result_all = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--all-lines",
                "--no-header",
            ],
        )
        self.assertEqual(result_all.exit_code, 0)
        all_lines = [line for line in result_all.output.strip().split("\n") if line]

        # Compare with head 10
        result_head = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--head",
                "10",
                "--no-header",
            ],
        )
        head_lines = [line for line in result_head.output.strip().split("\n") if line]

        # All should have >= 10 records (or same if file has exactly 10)
        self.assertGreaterEqual(len(all_lines), len(head_lines))

    def test_analyze_all_short_option(self):
        """Test analyze with -a short option for --all-lines."""
        result = self.runner.invoke(
            main,
            ["query", str(str(self.clp_archive_path.absolute())), "-a", "--no-header"],
        )
        self.assertEqual(result.exit_code, 0)

    def test_analyze_format_ndjson(self):
        """Test analyze with NDJSON output format."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--format",
                "ndjson",
                "--head",
                "3",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        lines = result.stdout.strip().split("\n")
        self.assertEqual(len(lines), 3)
        # Each line should be valid JSON
        import json

        for line in lines:
            data = json.loads(line)
            self.assertIsInstance(data, dict)

    def test_analyze_filter_hex_pc(self):
        """Test analyze with hex filter for PC value."""
        # First get the first record's PC
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--format",
                "json",
                "--head",
                "1",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        import json

        records = json.loads(result.stdout)
        if records and "pc" in records[0]:
            pc_value = records[0]["pc"]
            # pc is now a hex string like "0x0" or "0x10"
            # Filter using the hex string directly
            result_hex = self.runner.invoke(
                main,
                [
                    "query",
                    str(str(self.clp_archive_path.absolute())),
                    "--filter",
                    f"pc={pc_value}",
                    "--head",
                    "5",
                ],
            )
            self.assertEqual(result_hex.exit_code, 0)
            # All filtered records should have this PC
            self.assertIn(pc_value, result_hex.output)

    def test_analyze_fields_all(self):
        """Test analyze with --fields '*' to show all fields."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--fields",
                "*",
                "--head",
                "1",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        # Should show more than just default fields (warp, pc, sass)
        header = result.stdout.strip().split("\n")[0]
        # Check that non-default fields are included (like type, time, etc.)
        # At minimum should have WARP, PC, SASS
        self.assertIn("WARP", header)
        self.assertIn("PC", header)

    def test_analyze_fields_all_keyword(self):
        """Test analyze with --fields 'all' to show all fields."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--fields",
                "all",
                "--head",
                "1",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        header = result.stdout.strip().split("\n")[0]
        self.assertIn("WARP", header)
        self.assertIn("PC", header)

    def test_analyze_output_file(self):
        """Test analyze with --output to write to file."""
        import os
        import tempfile

        with tempfile.TemporaryDirectory() as tmpdir:
            output_path = os.path.join(tmpdir, "output.txt")
            result = self.runner.invoke(
                main,
                [
                    "query",
                    str(str(self.clp_archive_path.absolute())),
                    "--head",
                    "3",
                    "--output",
                    output_path,
                ],
            )
            self.assertEqual(result.exit_code, 0)
            # Check file was created
            self.assertTrue(os.path.exists(output_path))
            # Check file content
            with open(output_path) as f:
                content = f.read()
            lines = [line for line in content.strip().split("\n") if line]
            self.assertEqual(len(lines), 4)  # header + 3 rows

    def test_analyze_output_ndjson_file(self):
        """Test analyze with --output and --format ndjson."""
        import json
        import os
        import tempfile

        with tempfile.TemporaryDirectory() as tmpdir:
            output_path = os.path.join(tmpdir, "output.ndjson")
            result = self.runner.invoke(
                main,
                [
                    "query",
                    str(str(self.clp_archive_path.absolute())),
                    "--format",
                    "ndjson",
                    "--head",
                    "3",
                    "--output",
                    output_path,
                ],
            )
            self.assertEqual(result.exit_code, 0)
            self.assertTrue(os.path.exists(output_path))
            # Check file content is valid NDJSON
            with open(output_path) as f:
                lines = f.read().strip().split("\n")
            self.assertEqual(len(lines), 3)
            for line in lines:
                data = json.loads(line)
                self.assertIsInstance(data, dict)

    def test_analyze_group_by_all_records(self):
        """Test analyze with --group-by and --all-lines."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--all-lines",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        self.assertIn("=== Group:", result.stdout)

    def test_analyze_count_ndjson_format(self):
        """Test analyze --count with NDJSON format."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(str(self.clp_archive_path.absolute())),
                "--group-by",
                "warp",
                "--count",
                "--format",
                "ndjson",
            ],
        )
        self.assertEqual(result.exit_code, 0)
        import json

        lines = result.stdout.strip().split("\n")
        for line in lines:
            data = json.loads(line)
            self.assertIn("warp", data)
            self.assertIn("count", data)

    def test_query_output_compress(self):
        """Test query with --output and --compress produces valid Zstd file."""
        import json
        import os
        import tempfile

        import zstandard as zstd

        with tempfile.TemporaryDirectory() as tmpdir:
            output_path = os.path.join(tmpdir, "output.ndjson.zst")
            result = self.runner.invoke(
                main,
                [
                    "query",
                    str(str(self.clp_archive_path.absolute())),
                    "--format",
                    "ndjson",
                    "--head",
                    "3",
                    "--output",
                    output_path,
                    "--compress",
                ],
            )
            self.assertEqual(result.exit_code, 0)
            self.assertTrue(os.path.exists(output_path))

            # Verify it's valid zstd compressed data
            dctx = zstd.ZstdDecompressor()
            with open(output_path, "rb") as f:
                content = dctx.decompress(f.read()).decode("utf-8")
            lines = [line for line in content.strip().split("\n") if line]
            self.assertEqual(len(lines), 3)
            for line in lines:
                data = json.loads(line)
                self.assertIsInstance(data, dict)

    def test_query_compress_requires_output(self):
        """Test that --compress without --output raises an error."""
        result = self.runner.invoke(
            main,
            [
                "query",
                str(self.clp_archive_path.absolute()),
                "--head",
                "3",
                "--compress",
            ],
        )
        self.assertNotEqual(result.exit_code, 0)
        self.assertIn("--compress requires --output", result.output)


if __name__ == "__main__":
    unittest.main()

```

`python/tests/test_reader.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Unit tests for TraceReader class and related functions.
"""

import json
import types
import unittest

from cutracer.query import parse_filter_expr, select_records, TraceReader
from tests.test_base import (
    BaseValidationTest,
    REG_TRACE_NDJSON,
    REG_TRACE_NDJSON_RECORD_COUNT,
    REG_TRACE_NDJSON_ZST,
    REG_TRACE_NDJSON_ZST_RECORD_COUNT,
)


class TestParseFilterExpr(unittest.TestCase):
    """Tests for parse_filter_expr function."""

    def test_parse_filter_int_value(self):
        """Test parsing filter with integer value."""
        pred = parse_filter_expr("warp=24")
        self.assertTrue(pred({"warp": 24}))
        self.assertFalse(pred({"warp": 25}))
        # String "24" matches int 24 for backward compatibility
        self.assertTrue(pred({"warp": "24"}))

    def test_parse_filter_string_value(self):
        """Test parsing filter with string value."""
        pred = parse_filter_expr("type=mem_trace")
        self.assertTrue(pred({"type": "mem_trace"}))
        self.assertFalse(pred({"type": "reg_trace"}))

    def test_parse_filter_missing_field(self):
        """Test filter returns False for missing field."""
        pred = parse_filter_expr("warp=0")
        self.assertFalse(pred({"type": "reg_trace"}))

    def test_parse_filter_with_spaces(self):
        """Test filter with spaces around = is handled."""
        pred = parse_filter_expr(" warp = 24 ")
        self.assertTrue(pred({"warp": 24}))

    def test_parse_filter_invalid_no_equals(self):
        """Test that filter without = raises ValueError."""
        with self.assertRaises(ValueError) as ctx:
            parse_filter_expr("warp")
        self.assertIn("Invalid filter expression", str(ctx.exception))

    def test_parse_filter_invalid_empty_field(self):
        """Test that filter with empty field raises ValueError."""
        with self.assertRaises(ValueError) as ctx:
            parse_filter_expr("=24")
        self.assertIn("empty", str(ctx.exception))

    def test_parse_filter_semicolon_separated_and(self):
        """Test semicolon-separated multi-condition filter (AND logic)."""
        pred = parse_filter_expr("warp=0;pc=100")
        self.assertTrue(pred({"warp": 0, "pc": 100}))
        self.assertFalse(pred({"warp": 0, "pc": 200}))
        self.assertFalse(pred({"warp": 1, "pc": 100}))

    def test_parse_filter_semicolon_separated_hex(self):
        """Test semicolon-separated filter with hex values."""
        pred = parse_filter_expr("pc=0x64;warp=1")
        self.assertTrue(pred({"pc": 100, "warp": 1}))
        self.assertFalse(pred({"pc": 100, "warp": 2}))
        self.assertFalse(pred({"pc": 200, "warp": 1}))

    def test_parse_filter_semicolon_separated_three_conditions(self):
        """Test three semicolon-separated conditions."""
        pred = parse_filter_expr("warp=0;pc=100;sass=MOV")
        self.assertTrue(pred({"warp": 0, "pc": 100, "sass": "MOV"}))
        self.assertFalse(pred({"warp": 0, "pc": 100, "sass": "ADD"}))

    def test_parse_filter_semicolon_separated_with_spaces(self):
        """Test semicolon-separated filter with spaces around semicolons."""
        pred = parse_filter_expr(" warp=0 ; pc=100 ")
        self.assertTrue(pred({"warp": 0, "pc": 100}))
        self.assertFalse(pred({"warp": 1, "pc": 100}))

    def test_parse_filter_semicolon_separated_invalid_part(self):
        """Test semicolon-separated filter with an invalid part raises ValueError."""
        with self.assertRaises(ValueError):
            parse_filter_expr("warp=0;invalid")

    def test_parse_filter_empty_expression(self):
        """Test empty filter expression raises ValueError."""
        with self.assertRaises(ValueError):
            parse_filter_expr("")

    def test_parse_filter_list_value(self):
        """Test parsing filter with JSON list value (e.g., cta=[0,0,0])."""
        pred = parse_filter_expr("cta=[0,0,0]")
        self.assertTrue(pred({"cta": [0, 0, 0]}))
        self.assertFalse(pred({"cta": [1, 0, 0]}))
        self.assertFalse(pred({"cta": [0, 0]}))

    def test_parse_filter_list_value_with_spaces(self):
        """Test parsing filter with JSON list value containing spaces."""
        pred = parse_filter_expr("cta=[0, 0, 0]")
        self.assertTrue(pred({"cta": [0, 0, 0]}))

    def test_parse_filter_list_missing_field(self):
        """Test list filter returns False for missing field."""
        pred = parse_filter_expr("cta=[0,0,0]")
        self.assertFalse(pred({"warp": 0}))

    def test_parse_filter_semicolon_with_list(self):
        """Test semicolon-separated filter combining list and int values."""
        pred = parse_filter_expr("cta=[0,0,0];warp=1")
        self.assertTrue(pred({"cta": [0, 0, 0], "warp": 1}))
        self.assertFalse(pred({"cta": [0, 0, 0], "warp": 2}))
        self.assertFalse(pred({"cta": [1, 0, 0], "warp": 1}))

    def test_parse_filter_single_still_works(self):
        """Test that single condition still works after refactor."""
        pred = parse_filter_expr("warp=24")
        self.assertTrue(pred({"warp": 24}))
        self.assertFalse(pred({"warp": 25}))


class TestSelectRecords(unittest.TestCase):
    """Tests for select_records function."""

    def test_select_head_default(self):
        """Test default head selection (10 records)."""
        records = iter([{"i": i} for i in range(20)])
        result = select_records(records)
        self.assertEqual(len(result), 10)
        self.assertEqual(result[0]["i"], 0)
        self.assertEqual(result[9]["i"], 9)

    def test_select_head_custom(self):
        """Test custom head selection."""
        records = iter([{"i": i} for i in range(20)])
        result = select_records(records, head=5)
        self.assertEqual(len(result), 5)
        self.assertEqual(result[4]["i"], 4)

    def test_select_tail(self):
        """Test tail selection."""
        records = iter([{"i": i} for i in range(20)])
        result = select_records(records, tail=5)
        self.assertEqual(len(result), 5)
        self.assertEqual(result[0]["i"], 15)
        self.assertEqual(result[4]["i"], 19)

    def test_select_tail_overrides_head(self):
        """Test that tail overrides head when both specified."""
        records = iter([{"i": i} for i in range(20)])
        result = select_records(records, head=3, tail=5)
        self.assertEqual(len(result), 5)
        self.assertEqual(result[0]["i"], 15)

    def test_select_head_zero(self):
        """Test head=0 returns empty list."""
        records = iter([{"i": i} for i in range(10)])
        result = select_records(records, head=0)
        self.assertEqual(result, [])

    def test_select_tail_zero(self):
        """Test tail=0 returns empty list."""
        records = iter([{"i": i} for i in range(10)])
        result = select_records(records, tail=0)
        self.assertEqual(result, [])

    def test_select_head_more_than_available(self):
        """Test head larger than available records."""
        records = iter([{"i": i} for i in range(5)])
        result = select_records(records, head=20)
        self.assertEqual(len(result), 5)

    def test_select_tail_more_than_available(self):
        """Test tail larger than available records."""
        records = iter([{"i": i} for i in range(5)])
        result = select_records(records, tail=20)
        self.assertEqual(len(result), 5)

    def test_select_empty_iterator(self):
        """Test selection from empty iterator."""
        records = iter([])
        result = select_records(records, head=10)
        self.assertEqual(result, [])


class TestTraceReaderInit(BaseValidationTest):
    """Tests for TraceReader initialization."""

    def test_init_with_ndjson_file(self):
        """Test initialization with a plain NDJSON file."""
        reader = TraceReader(REG_TRACE_NDJSON)
        self.assertEqual(reader.file_path, REG_TRACE_NDJSON)
        self.assertEqual(reader.compression, "none")

    def test_init_with_zst_file(self):
        """Test initialization with a Zstd-compressed NDJSON file."""
        reader = TraceReader(REG_TRACE_NDJSON_ZST)
        self.assertEqual(reader.file_path, REG_TRACE_NDJSON_ZST)
        self.assertEqual(reader.compression, "zstd")

    def test_init_with_string_path(self):
        """Test initialization with a string path."""
        reader = TraceReader(str(REG_TRACE_NDJSON))
        self.assertEqual(reader.file_path, REG_TRACE_NDJSON)

    def test_init_with_nonexistent_file(self):
        """Test that FileNotFoundError is raised for non-existent file."""
        with self.assertRaises(FileNotFoundError):
            TraceReader("/nonexistent/path/file.ndjson")


class TestTraceReaderIterRecords(BaseValidationTest):
    """Tests for TraceReader.iter_records() method."""

    def test_iter_records_ndjson(self):
        """Test iterating over records in a plain NDJSON file."""
        reader = TraceReader(REG_TRACE_NDJSON)
        records = list(reader.iter_records())

        self.assertEqual(len(records), REG_TRACE_NDJSON_RECORD_COUNT)

        # Verify first record structure
        first_record = records[0]
        self.assertIn("type", first_record)
        self.assertIn("warp", first_record)
        self.assertIn("sass", first_record)
        self.assertEqual(first_record["type"], "reg_trace")

    def test_iter_records_zst(self):
        """Test iterating over records in a Zstd-compressed NDJSON file."""
        reader = TraceReader(REG_TRACE_NDJSON_ZST)
        records = list(reader.iter_records())

        self.assertEqual(len(records), REG_TRACE_NDJSON_ZST_RECORD_COUNT)

    def test_iter_records_zst_has_valid_structure(self):
        """Test that Zstd compressed file records have valid structure."""
        reader = TraceReader(REG_TRACE_NDJSON_ZST)
        records = list(reader.iter_records())

        self.assertGreater(len(records), 0)

        # Verify record structure
        first_record = records[0]
        self.assertIn("type", first_record)
        self.assertIn("warp", first_record)
        self.assertIn("sass", first_record)

    def test_iter_records_is_generator(self):
        """Test that iter_records returns a generator (lazy evaluation)."""
        reader = TraceReader(REG_TRACE_NDJSON)
        result = reader.iter_records()

        # Should be a generator, not a list
        self.assertIsInstance(result, types.GeneratorType)

    def test_iter_records_can_iterate_multiple_times(self):
        """Test that iter_records can be called multiple times."""
        reader = TraceReader(REG_TRACE_NDJSON)

        # First iteration
        records1 = list(reader.iter_records())
        # Second iteration
        records2 = list(reader.iter_records())

        self.assertEqual(len(records1), len(records2))
        self.assertEqual(records1[0], records2[0])


class TestTraceReaderEdgeCases(BaseValidationTest):
    """Edge case tests for TraceReader."""

    def test_empty_file(self):
        """Test reading an empty file."""
        filepath = self.create_temp_file("empty.ndjson", "")

        reader = TraceReader(filepath)
        records = list(reader.iter_records())

        self.assertEqual(len(records), 0)

    def test_single_record_file(self):
        """Test reading a file with a single record."""
        content = '{"type": "reg_trace", "warp": 0, "sass": "NOP ;"}\n'
        filepath = self.create_temp_file("single.ndjson", content)

        reader = TraceReader(filepath)
        records = list(reader.iter_records())

        self.assertEqual(len(records), 1)
        self.assertEqual(records[0]["type"], "reg_trace")

    def test_empty_lines_skipped(self):
        """Test that empty lines in the file are skipped."""
        content = (
            '{"type": "reg_trace", "warp": 0}\n\n{"type": "reg_trace", "warp": 1}\n'
        )
        filepath = self.create_temp_file("test.ndjson", content)

        reader = TraceReader(filepath)
        records = list(reader.iter_records())

        self.assertEqual(len(records), 2)
        self.assertEqual(records[0]["warp"], 0)
        self.assertEqual(records[1]["warp"], 1)

    def test_invalid_json_raises_error(self):
        """Test that invalid JSON raises JSONDecodeError."""
        content = '{"type": "reg_trace"}\n{invalid json}\n'
        filepath = self.create_temp_file("invalid.ndjson", content)

        reader = TraceReader(filepath)
        iterator = reader.iter_records()

        # First record should work
        first = next(iterator)
        self.assertEqual(first["type"], "reg_trace")

        # Second record should raise
        with self.assertRaises(json.JSONDecodeError):
            next(iterator)


class TestIntegration(BaseValidationTest):
    """Integration tests combining TraceReader with filter and select."""

    def test_filter_and_select(self):
        """Test combining TraceReader with filter and select_records."""
        reader = TraceReader(REG_TRACE_NDJSON)
        pred = parse_filter_expr("warp=0")

        # Filter and select first 5
        filtered = (r for r in reader.iter_records() if pred(r))
        result = select_records(filtered, head=5)

        self.assertLessEqual(len(result), 5)
        for record in result:
            self.assertEqual(record["warp"], 0)


if __name__ == "__main__":
    unittest.main()

```

`python/tests/test_reduce.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Tests for CUTracer reduce module.

Tests cover:
- DelayPoint and DelayConfigMutator (config_mutator.py)
- Reduction algorithm (reduce.py)
- Report generation (report.py)
"""

import json
import os
import shutil
import tempfile
import unittest
from pathlib import Path
from unittest.mock import patch

from cutracer.reduce.config_mutator import DelayConfigMutator, DelayPoint
from cutracer.reduce.reduce import reduce_delay_points, ReduceConfig, ReduceResult
from cutracer.reduce.report import format_report_text, generate_report


# Sample delay config for testing (matches DELAY_CONFIG_SCHEMA)
SAMPLE_DELAY_CONFIG = {
    "version": "1.0",
    "delay_ns": 100,
    "kernels": {
        "kernel_0_a1b2c3d4e5f60001": {
            "kernel_name": "matmul_kernel",
            "kernel_checksum": "a1b2c3d4e5f60001",
            "timestamp": "2026-02-03T21:15:21.567",
            "instrumentation_points": {
                "0x100": {
                    "pc": 256,
                    "sass": "LDG.E.64 R8, [R2.64]",
                    "delay": 100,
                    "on": True,
                },
                "0x108": {
                    "pc": 264,
                    "sass": "STG.E.64 [R4.64], R8",
                    "delay": 100,
                    "on": True,
                },
                "0x110": {
                    "pc": 272,
                    "sass": "BAR.SYNC 0",
                    "delay": 100,
                    "on": False,
                },
            },
        },
        "kernel_1_a1b2c3d4e5f60002": {
            "kernel_name": "softmax_kernel",
            "kernel_checksum": "a1b2c3d4e5f60002",
            "timestamp": "2026-02-03T21:15:22.123",
            "instrumentation_points": {
                "0x200": {
                    "pc": 512,
                    "sass": "FADD R0, R1, R2",
                    "delay": 50,
                    "on": True,
                },
            },
        },
    },
}


class DelayConfigMutatorTest(unittest.TestCase):
    """Tests for DelayPoint and DelayConfigMutator classes."""

    def setUp(self):
        """Create a temporary config file for testing."""
        self.temp_dir = Path(tempfile.mkdtemp())
        self.config_path = self.temp_dir / "test_config.json"
        with open(self.config_path, "w") as f:
            json.dump(SAMPLE_DELAY_CONFIG, f)

    def tearDown(self):
        """Remove the temporary directory."""
        shutil.rmtree(self.temp_dir)

    def test_delay_point_creation_and_repr(self):
        """Test creating DelayPoints and string representation."""
        point_on = DelayPoint(
            kernel_key="kernel_0",
            kernel_name="matmul_kernel",
            pc_offset="0x100",
            sass="LDG.E.64 R8, [R2.64]",
            delay_ns=100,
            enabled=True,
        )
        point_off = DelayPoint(
            kernel_key="k0",
            kernel_name="test",
            pc_offset="0x00",
            sass="STG [R1], R0",
            delay_ns=100,
            enabled=False,
        )

        self.assertEqual(point_on.kernel_key, "kernel_0")
        self.assertEqual(point_on.kernel_name, "matmul_kernel")
        self.assertTrue(point_on.enabled)
        self.assertFalse(point_off.enabled)
        self.assertIn("[ON]", repr(point_on))
        self.assertIn("[OFF]", repr(point_off))

    def test_load_config_and_enabled_points(self):
        """Test loading a delay config and getting enabled points."""
        mutator = DelayConfigMutator(str(self.config_path))

        # Should have 4 delay points total (3 from kernel_0, 1 from kernel_1)
        self.assertEqual(len(mutator), 4)
        self.assertEqual(len(mutator.delay_points), 4)

        # 3 points are enabled (2 from kernel_0 + 1 from kernel_1)
        enabled = mutator.enabled_points
        self.assertEqual(len(enabled), 3)
        for point in enabled:
            self.assertTrue(point.enabled)

        # Test repr
        self.assertIn("DelayConfigMutator", repr(mutator))
        self.assertIn("3/4", repr(mutator))

    def test_set_point_enabled_and_all_enabled(self):
        """Test enabling/disabling delay points individually and all at once."""
        mutator = DelayConfigMutator(str(self.config_path))
        point = mutator.enabled_points[0]
        self.assertTrue(point.enabled)

        # Disable single point
        mutator.set_point_enabled(point, False)
        self.assertFalse(point.enabled)
        self.assertFalse(
            mutator.config["kernels"][point.kernel_key]["instrumentation_points"][
                point.pc_offset
            ]["on"]
        )

        # Disable all
        mutator.set_all_enabled(False)
        self.assertEqual(len(mutator.enabled_points), 0)

        # Enable all
        mutator.set_all_enabled(True)
        self.assertEqual(len(mutator.enabled_points), len(mutator.delay_points))

    def test_save_config_and_clone(self):
        """Test saving config and cloning mutator."""
        mutator = DelayConfigMutator(str(self.config_path))

        # Modify and save to new file
        mutator.set_all_enabled(False)
        output_path = self.temp_dir / "output_config.json"
        saved_path = mutator.save(str(output_path))

        # Verify saved file
        self.assertEqual(saved_path, str(output_path))
        with open(saved_path) as f:
            saved_config = json.load(f)
        for kernel_config in saved_config["kernels"].values():
            for point in kernel_config["instrumentation_points"].values():
                self.assertFalse(point["on"])

        # Test save to temp file
        mutator2 = DelayConfigMutator(str(self.config_path))
        temp_saved = mutator2.save()
        self.assertTrue(os.path.exists(temp_saved))
        os.unlink(temp_saved)

        # Test clone isolation
        mutator3 = DelayConfigMutator(str(self.config_path))
        cloned = mutator3.clone()
        mutator3.set_all_enabled(False)
        self.assertEqual(len(cloned.enabled_points), 3)  # Clone unaffected

    def test_schema_validation(self):
        """Test schema validation for config files."""
        # Valid config passes validation
        mutator = DelayConfigMutator(str(self.config_path), validate=True)
        self.assertEqual(len(mutator.delay_points), 4)

        # Invalid config fails validation
        invalid_config = {"delay_ns": 100, "kernels": {}}  # Missing 'version'
        invalid_path = self.temp_dir / "invalid_config.json"
        with open(invalid_path, "w") as f:
            json.dump(invalid_config, f)

        with self.assertRaises(ValueError) as context:
            DelayConfigMutator(str(invalid_path), validate=True)
        self.assertIn("Invalid delay config", str(context.exception))

        # Validation can be skipped
        simple_config = {
            "kernels": {
                "kernel_0": {
                    "kernel_name": "test",
                    "instrumentation_points": {
                        "0x100": {"sass": "NOP", "delay": 100, "on": True},
                    },
                }
            }
        }
        simple_path = self.temp_dir / "simple_config.json"
        with open(simple_path, "w") as f:
            json.dump(simple_config, f)

        mutator = DelayConfigMutator(str(simple_path), validate=False)
        self.assertEqual(len(mutator.delay_points), 1)


class ReduceDelayPointsTest(unittest.TestCase):
    """Tests for reduce_delay_points function and ReduceResult."""

    def setUp(self):
        """Create temporary config for testing."""
        self.temp_dir = Path(tempfile.mkdtemp())
        self.config_path = self.temp_dir / "test_config.json"

        config = {
            "kernels": {
                "kernel_0": {
                    "kernel_name": "test_kernel",
                    "instrumentation_points": {
                        "0x100": {"sass": "LDG R0, [R1]", "delay": 100, "on": True},
                        "0x108": {"sass": "STG [R2], R3", "delay": 100, "on": True},
                        "0x110": {"sass": "BAR.SYNC 0", "delay": 100, "on": True},
                    },
                }
            }
        }
        with open(self.config_path, "w") as f:
            json.dump(config, f)

    def tearDown(self):
        """Remove the temporary directory."""
        shutil.rmtree(self.temp_dir)

    def test_reduce_result_properties(self):
        """Test ReduceResult success property and summary."""
        point = DelayPoint("k0", "matmul", "0x100", "BAR.SYNC 0", 100, True)

        result_with_points = ReduceResult(
            total_points=10, essential_points=[point], iterations=10
        )
        self.assertTrue(result_with_points.success)
        summary = result_with_points.summary()
        self.assertIn("Reduction complete", summary)
        self.assertIn("Total points tested: 10", summary)
        self.assertIn("Essential points found: 1", summary)

        result_no_points = ReduceResult(
            total_points=5, essential_points=[], iterations=5
        )
        self.assertFalse(result_no_points.success)

    @patch("cutracer.reduce.reduce.run_test")
    def test_reduce_finds_essential_points(self, mock_run_test):
        """Test reduction finds essential points."""

        def test_behavior(script, config_path, verbose=False):
            with open(config_path) as f:
                cfg = json.load(f)
            points = cfg["kernels"]["kernel_0"]["instrumentation_points"]
            return points["0x108"]["on"]  # Race only when STG is enabled

        mock_run_test.side_effect = test_behavior

        reduce_config = ReduceConfig(
            config_path=str(self.config_path),
            test_script="./test.sh",
            validate_schema=False,
        )
        result = reduce_delay_points(reduce_config)

        self.assertTrue(result.success)
        self.assertEqual(len(result.essential_points), 1)
        self.assertEqual(result.essential_points[0].pc_offset, "0x108")

    @patch("cutracer.reduce.reduce.run_test")
    def test_reduce_edge_cases(self, mock_run_test):
        """Test reduction edge cases: no essential points, initial no race."""
        # Test: No essential points (race always occurs)
        mock_run_test.return_value = True
        reduce_config = ReduceConfig(
            config_path=str(self.config_path),
            test_script="./test.sh",
            validate_schema=False,
        )
        result = reduce_delay_points(reduce_config)
        self.assertFalse(result.success)
        self.assertEqual(len(result.essential_points), 0)

        # Test: Initial config doesn't trigger race
        mock_run_test.return_value = False
        with self.assertRaises(ValueError) as context:
            reduce_delay_points(reduce_config)
        self.assertIn(
            "Initial config does not trigger the race", str(context.exception)
        )

        # Test: Empty config (no enabled points)
        empty_config = {
            "kernels": {
                "kernel_0": {
                    "kernel_name": "test",
                    "instrumentation_points": {
                        "0x100": {"sass": "LDG R0, [R1]", "delay": 100, "on": False},
                    },
                }
            }
        }
        empty_config_path = self.temp_dir / "empty_config.json"
        with open(empty_config_path, "w") as f:
            json.dump(empty_config, f)

        empty_reduce_config = ReduceConfig(
            config_path=str(empty_config_path),
            test_script="./test.sh",
            validate_schema=False,
        )
        with self.assertRaises(ValueError) as context:
            reduce_delay_points(empty_reduce_config)
        self.assertIn("No enabled delay points", str(context.exception))

    @patch("cutracer.reduce.reduce.run_test")
    def test_reduce_with_callback_and_output(self, mock_run_test):
        """Test reduction with progress callback and output file."""
        mock_run_test.return_value = True
        callback_calls = []

        def progress_callback(iteration, total, point, is_essential):
            callback_calls.append((iteration, total, point.pc_offset, is_essential))

        output_path = str(self.temp_dir / "minimal.json")
        reduce_config = ReduceConfig(
            config_path=str(self.config_path),
            test_script="./test.sh",
            output_path=output_path,
            validate_schema=False,
        )
        result = reduce_delay_points(reduce_config, progress_callback=progress_callback)

        # Verify callbacks
        self.assertEqual(len(callback_calls), 3)
        for iteration, total, pc, _is_essential in callback_calls:
            self.assertGreater(iteration, 0)
            self.assertEqual(total, 3)
            self.assertTrue(pc.startswith("0x"))

        # Verify output saved
        self.assertEqual(result.minimal_config_path, output_path)
        self.assertTrue(os.path.exists(output_path))


class ReportTest(unittest.TestCase):
    """Tests for report generation."""

    def test_generate_and_format_report(self):
        """Test generating and formatting a report."""
        point = DelayPoint("k0", "matmul", "0x100", "BAR.SYNC 0", 100, True)
        result = ReduceResult(
            total_points=10,
            essential_points=[point],
            iterations=10,
            minimal_config_path="/tmp/minimal.json",
        )

        report = generate_report(
            result=result,
            config_path="/tmp/config.json",
            test_script="./test.sh",
            source_path="/tmp/source.py",
        )

        # Verify report structure
        self.assertIn("metadata", report)
        self.assertIn("summary", report)
        self.assertIn("essential_delay_points", report)
        self.assertEqual(report["summary"]["total_points"], 10)
        self.assertEqual(report["summary"]["essential_points"], 1)
        self.assertEqual(len(report["essential_delay_points"]), 1)
        self.assertEqual(report["essential_delay_points"][0]["sass"], "BAR.SYNC 0")

        # Verify text format
        text = format_report_text(report)
        self.assertIn("CUTRACER REDUCE REPORT", text)
        self.assertIn("SUMMARY", text)
        self.assertIn("ESSENTIAL DELAY POINTS", text)
        self.assertIn("BAR.SYNC 0", text)
        self.assertIn("Total points tested: 10", text)


if __name__ == "__main__":
    unittest.main()

```

`python/tests/test_reduce_cli.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""Tests for the reduce CLI command."""

import json
import shutil
import tempfile
import unittest
from pathlib import Path

from click.testing import CliRunner
from cutracer.cli import main


class ReduceCliTest(unittest.TestCase):
    """Tests for reduce command CLI interface."""

    def setUp(self):
        self.runner = CliRunner()
        self.temp_dir = tempfile.mkdtemp()

        # Create a test delay config file matching DELAY_CONFIG_SCHEMA
        self.config_file = Path(self.temp_dir) / "delay_config.json"
        self.config_data = {
            "version": "1.0",
            "delay_ns": 10000,
            "kernels": {
                "matmul_kernel_1234567890abcdef": {
                    "kernel_name": "matmul_kernel",
                    "kernel_checksum": "1234567890abcdef",
                    "timestamp": "2026-01-01T00:00:00.000",
                    "instrumentation_points": {
                        "256": {
                            "pc": 256,
                            "sass": "STG.E [R2], R4;",
                            "delay": 10000,
                            "on": True,
                        },
                        "512": {
                            "pc": 512,
                            "sass": "LDG.E R5, [R6];",
                            "delay": 10000,
                            "on": True,
                        },
                        "768": {
                            "pc": 768,
                            "sass": "BAR.SYNC 0x0;",
                            "delay": 10000,
                            "on": True,
                        },
                    },
                },
                "softmax_kernel_fedcba0987654321": {
                    "kernel_name": "softmax_kernel",
                    "kernel_checksum": "fedcba0987654321",
                    "timestamp": "2026-01-01T00:00:01.000",
                    "instrumentation_points": {
                        "1024": {
                            "pc": 1024,
                            "sass": "STG.E [R10], R12;",
                            "delay": 10000,
                            "on": True,
                        },
                    },
                },
            },
        }
        with open(self.config_file, "w") as f:
            json.dump(self.config_data, f, indent=2)

        # Create a test script that always returns "race detected" (exit 0)
        self.test_script_race = Path(self.temp_dir) / "test_race.sh"
        self.test_script_race.write_text("#!/bin/bash\nexit 0\n")
        self.test_script_race.chmod(0o755)

        # Create a test script that always returns "no race" (exit 1)
        self.test_script_no_race = Path(self.temp_dir) / "test_no_race.sh"
        self.test_script_no_race.write_text("#!/bin/bash\nexit 1\n")
        self.test_script_no_race.chmod(0o755)

    def tearDown(self):
        shutil.rmtree(self.temp_dir)

    def test_reduce_help(self):
        """Test reduce --help shows usage information."""
        result = self.runner.invoke(
            main,
            ["reduce", "--help"],
        )

        self.assertEqual(result.exit_code, 0)
        self.assertIn("Reduce delay injection config", result.output)
        self.assertIn("--config", result.output)
        self.assertIn("--test", result.output)

    def test_reduce_missing_config(self):
        """Test reduce fails with missing config file."""
        result = self.runner.invoke(
            main,
            [
                "reduce",
                "--config",
                "/nonexistent/config.json",
                "--test",
                str(self.test_script_race),
            ],
        )

        self.assertNotEqual(result.exit_code, 0)

    def test_reduce_missing_test_script(self):
        """Test reduce fails with missing test script."""
        result = self.runner.invoke(
            main,
            [
                "reduce",
                "--config",
                str(self.config_file),
                "--test",
                "/nonexistent/test.sh",
            ],
        )

        # Should fail when trying to run the nonexistent test script
        self.assertNotEqual(result.exit_code, 0)

    def test_reduce_basic_with_race(self):
        """Test basic reduce command with race always detected."""
        output_file = Path(self.temp_dir) / "report.json"
        minimal_config = Path(self.temp_dir) / "minimal.json"

        result = self.runner.invoke(
            main,
            [
                "reduce",
                "--config",
                str(self.config_file),
                "--test",
                str(self.test_script_race),
                "--output",
                str(output_file),
                "--minimal-config",
                str(minimal_config),
                "-v",
            ],
        )

        self.assertEqual(result.exit_code, 0, f"Failed with output: {result.output}")
        self.assertIn("CUTRACER REDUCE", result.output)
        self.assertIn("REDUCTION COMPLETE", result.output)
        self.assertTrue(output_file.exists())
        self.assertTrue(minimal_config.exists())

    def test_reduce_initial_no_race_error(self):
        """Test reduce fails if initial config doesn't trigger race."""
        result = self.runner.invoke(
            main,
            [
                "reduce",
                "--config",
                str(self.config_file),
                "--test",
                str(self.test_script_no_race),
            ],
        )

        self.assertNotEqual(result.exit_code, 0)
        self.assertIn("Error", result.output)

    def test_reduce_verbose_output(self):
        """Test reduce with verbose output."""
        result = self.runner.invoke(
            main,
            [
                "reduce",
                "--config",
                str(self.config_file),
                "--test",
                str(self.test_script_race),
                "-v",
            ],
        )

        self.assertEqual(result.exit_code, 0, f"Failed with output: {result.output}")

    def test_reduce_report_json_format(self):
        """Test that reduce report is valid JSON."""
        output_file = Path(self.temp_dir) / "report.json"

        result = self.runner.invoke(
            main,
            [
                "reduce",
                "--config",
                str(self.config_file),
                "--test",
                str(self.test_script_race),
                "--output",
                str(output_file),
            ],
        )

        self.assertEqual(result.exit_code, 0, f"Failed with output: {result.output}")
        self.assertTrue(output_file.exists())

        # Verify it's valid JSON
        with open(output_file) as f:
            report = json.load(f)
        self.assertIn("metadata", report)

    def test_reduce_stats_output(self):
        """Test reduce shows statistics at the end."""
        result = self.runner.invoke(
            main,
            [
                "reduce",
                "--config",
                str(self.config_file),
                "--test",
                str(self.test_script_race),
            ],
        )

        self.assertEqual(result.exit_code, 0, f"Failed with output: {result.output}")
        self.assertIn("Stats:", result.output)
        self.assertIn("Total points:", result.output)
        self.assertIn("Essential:", result.output)
        self.assertIn("Iterations:", result.output)


class ReduceCliEmptyConfigTest(unittest.TestCase):
    """Tests for reduce command with edge case configs."""

    def setUp(self):
        self.runner = CliRunner()
        self.temp_dir = tempfile.mkdtemp()

        # Create test script
        self.test_script = Path(self.temp_dir) / "test.sh"
        self.test_script.write_text("#!/bin/bash\nexit 0\n")
        self.test_script.chmod(0o755)

    def tearDown(self):
        shutil.rmtree(self.temp_dir)

    def test_reduce_empty_config(self):
        """Test reduce with config that has no delay points."""
        config_file = Path(self.temp_dir) / "empty_config.json"
        config_data = {
            "version": "1.0",
            "delay_ns": 10000,
            "kernels": {},
        }
        with open(config_file, "w") as f:
            json.dump(config_data, f)

        result = self.runner.invoke(
            main,
            [
                "reduce",
                "--config",
                str(config_file),
                "--test",
                str(self.test_script),
            ],
        )

        self.assertNotEqual(result.exit_code, 0)
        self.assertIn("Error", result.output)

    def test_reduce_all_disabled_points(self):
        """Test reduce with config where all points are disabled."""
        config_file = Path(self.temp_dir) / "disabled_config.json"
        config_data = {
            "version": "1.0",
            "delay_ns": 10000,
            "kernels": {
                "test_kernel_a1b2c3d4e5f67890": {
                    "kernel_name": "test_kernel",
                    "kernel_checksum": "a1b2c3d4e5f67890",
                    "timestamp": "2026-01-01T00:00:00.000",
                    "instrumentation_points": {
                        "256": {
                            "pc": 256,
                            "sass": "STG.E [R2], R4;",
                            "delay": 10000,
                            "on": False,
                        },
                    },
                },
            },
        }
        with open(config_file, "w") as f:
            json.dump(config_data, f)

        result = self.runner.invoke(
            main,
            [
                "reduce",
                "--config",
                str(config_file),
                "--test",
                str(self.test_script),
            ],
        )

        self.assertNotEqual(result.exit_code, 0)
        self.assertIn("Error", result.output)


if __name__ == "__main__":
    unittest.main()

```

`python/tests/test_schemas.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Unit tests for JSON schema definitions.
"""

import json
import unittest

import jsonschema
from cutracer.validation.schema_loader import (
    DELAY_CONFIG_SCHEMA,
    MEM_ACCESS_SCHEMA,
    OPCODE_ONLY_SCHEMA,
    REG_INFO_SCHEMA,
    SCHEMAS_BY_TYPE,
)
from tests.test_base import REG_TRACE_NDJSON


class SchemaTest(unittest.TestCase):
    """Core tests for JSON schema definitions."""

    def test_schemas_contain_all_types(self):
        """Test that SCHEMAS_BY_TYPE contains all expected message types."""
        expected_types = ["reg_trace", "mem_trace", "opcode_only"]
        for msg_type in expected_types:
            self.assertIn(msg_type, SCHEMAS_BY_TYPE)

    def test_schemas_are_valid_json_schema(self):
        """Test that all schemas are valid JSON Schema documents."""
        for schema in SCHEMAS_BY_TYPE.values():
            jsonschema.Draft7Validator.check_schema(schema)

    def test_delay_config_schema_is_valid(self):
        """Test that delay config schema is a valid JSON Schema document."""
        jsonschema.Draft7Validator.check_schema(DELAY_CONFIG_SCHEMA)

    def test_real_reg_trace_record_passes(self):
        """Test that real reg_trace records from sample file pass schema validation."""
        with open(REG_TRACE_NDJSON, "r") as f:
            for i, line in enumerate(f):
                if i >= 10:  # Test first 10 records
                    break
                record = json.loads(line.strip())
                # Should not raise any exception
                jsonschema.validate(record, REG_INFO_SCHEMA)

    def test_mem_trace_schema_structure(self):
        """Test that mem_trace schema has required structure."""
        self.assertIn("properties", MEM_ACCESS_SCHEMA)
        self.assertIn("type", MEM_ACCESS_SCHEMA["properties"])
        self.assertIn("addrs", MEM_ACCESS_SCHEMA["properties"])

    def test_opcode_only_schema_structure(self):
        """Test that opcode_only schema has required structure."""
        self.assertIn("properties", OPCODE_ONLY_SCHEMA)
        self.assertIn("type", OPCODE_ONLY_SCHEMA["properties"])
        # opcode_only should NOT have regs or addrs
        self.assertNotIn("regs", OPCODE_ONLY_SCHEMA.get("required", []))
        self.assertNotIn("addrs", OPCODE_ONLY_SCHEMA.get("required", []))


class DelayConfigSchemaTest(unittest.TestCase):
    """Tests for delay injection configuration schema."""

    def test_valid_delay_config_passes(self):
        """Test that a valid delay config passes schema validation."""
        valid_config = {
            "version": "1.0",
            "delay_ns": 10000,
            "kernels": {
                "my_kernel_a1b2c3d4e5f67890": {
                    "kernel_name": "my_kernel",
                    "kernel_checksum": "a1b2c3d4e5f67890",
                    "timestamp": "2026-02-03T21:15:21.567",
                    "instrumentation_points": {
                        "10192": {
                            "pc": 10192,
                            "sass": "SYNCS.PHASECHK.TRANS64.TRYWAIT P0, [UR15+0x38110], R4 ;",
                            "delay": 10000,
                            "on": True,
                        },
                    },
                }
            },
        }
        jsonschema.validate(valid_config, DELAY_CONFIG_SCHEMA)

    def test_missing_required_field_fails(self):
        """Test that missing required fields fail validation."""
        # Missing version
        with self.assertRaises(jsonschema.ValidationError):
            jsonschema.validate({"delay_ns": 10000, "kernels": {}}, DELAY_CONFIG_SCHEMA)

    def test_invalid_instrumentation_point_fails(self):
        """Test that incomplete instrumentation point fails validation."""
        invalid_config = {
            "version": "1.0",
            "delay_ns": 10000,
            "kernels": {
                "my_kernel_2026-02-03T21:15:21.567": {
                    "kernel_name": "my_kernel",
                    "timestamp": "2026-02-03T21:15:21.567",
                    "instrumentation_points": {
                        "100": {"pc": 100, "sass": "NOP;"},  # Missing "delay" and "on"
                    },
                }
            },
        }
        with self.assertRaises(jsonschema.ValidationError):
            jsonschema.validate(invalid_config, DELAY_CONFIG_SCHEMA)


if __name__ == "__main__":
    unittest.main()

```

`python/tests/test_text_validator.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Unit tests for text validator module.
"""

import unittest

from cutracer.validation.text_validator import (
    parse_text_trace_record,
    REG_INFO_HEADER_PATTERN,
    validate_text_format,
    validate_text_trace,
)
from tests.test_base import (
    BaseValidationTest,
    REG_TRACE_LOG,
    REG_TRACE_LOG_RECORD_COUNT,
)


class TextValidatorTest(BaseValidationTest):
    """Core tests for text validator functions."""

    def test_reg_info_header_pattern_matches_real_data(self):
        """Test that REG_INFO_HEADER_PATTERN matches headers in real trace file."""
        with open(REG_TRACE_LOG, "r") as f:
            match_count = 0
            for line in f:
                if REG_INFO_HEADER_PATTERN.match(line):
                    match_count += 1
                    if match_count >= 10:
                        break
        self.assertEqual(match_count, 10)

    def test_validate_text_format_real_file(self):
        """Test validation of real text trace file."""
        result = validate_text_format(REG_TRACE_LOG)

        self.assertTrue(result)

    def test_validate_text_format_file_not_found(self):
        """Test handling of non-existent file."""
        non_existent = self.temp_dir / "missing.log"

        with self.assertRaises(FileNotFoundError):
            validate_text_format(non_existent)

    def test_validate_text_trace_real_file(self):
        """Test complete validation of real text trace file."""
        result = validate_text_trace(REG_TRACE_LOG)

        self.assertTrue(result["valid"])
        self.assertEqual(result["record_count"], REG_TRACE_LOG_RECORD_COUNT)
        self.assertGreater(result["file_size"], 0)
        self.assertEqual(len(result["errors"]), 0)

    def test_validate_text_trace_empty_file(self):
        """Test validation of empty file."""
        empty_file = self.create_temp_file("empty.log", "")

        result = validate_text_trace(empty_file)

        self.assertFalse(result["valid"])
        self.assertEqual(result["record_count"], 0)
        self.assertIn("No trace records", str(result["errors"]))

    def test_parse_text_trace_record_from_real_file(self):
        """Test parsing of real trace record header."""
        with open(REG_TRACE_LOG, "r") as f:
            first_line = f.readline().strip()

        result = parse_text_trace_record([first_line])

        self.assertIn("ctx", result)
        self.assertIn("cta", result)
        self.assertIn("warp", result)
        self.assertIn("sass", result)
        self.assertEqual(result["record_type"], "reg_info")
        self.assertEqual(len(result["cta"]), 3)

    def test_parse_text_trace_record_invalid(self):
        """Test that invalid header raises ValueError."""
        with self.assertRaises(ValueError):
            parse_text_trace_record(["This is not a valid header"])

        with self.assertRaises(ValueError):
            parse_text_trace_record([])


if __name__ == "__main__":
    unittest.main()

```

`python/tests/test_warp_summary.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.

"""
Unit tests for warp_summary module.
"""

import unittest

from cutracer.query.warp_summary import (
    compute_warp_summary,
    format_ranges,
    format_warp_summary_text,
    is_exit_instruction,
    merge_to_ranges,
    warp_summary_to_dict,
    WarpSummary,
)


class TestIsExitInstruction(unittest.TestCase):
    """Tests for is_exit_instruction function."""

    def test_simple_exit(self):
        """Test simple EXIT instruction."""
        record = {"sass": "EXIT;"}
        self.assertTrue(is_exit_instruction(record))

    def test_predicated_exit(self):
        """Test predicated EXIT instruction."""
        record = {"sass": "@P0 EXIT;"}
        self.assertTrue(is_exit_instruction(record))

    def test_exit_with_modifier(self):
        """Test EXIT with modifier."""
        record = {"sass": "EXIT.KEEPREFCOUNT;"}
        self.assertTrue(is_exit_instruction(record))

    def test_lowercase_exit(self):
        """Test lowercase exit."""
        record = {"sass": "exit;"}
        self.assertTrue(is_exit_instruction(record))

    def test_non_exit_instruction(self):
        """Test non-EXIT instruction."""
        record = {"sass": "MOV R1, R0;"}
        self.assertFalse(is_exit_instruction(record))

    def test_empty_sass(self):
        """Test empty sass field."""
        record = {"sass": ""}
        self.assertFalse(is_exit_instruction(record))

    def test_no_sass_field(self):
        """Test record without sass field."""
        record = {"warp": 0, "pc": 16}
        self.assertFalse(is_exit_instruction(record))

    def test_exit_without_semicolon(self):
        """Test EXIT without semicolon should return False."""
        record = {"sass": "EXIT"}
        self.assertFalse(is_exit_instruction(record))


class TestMergeToRanges(unittest.TestCase):
    """Tests for merge_to_ranges function."""

    def test_empty_list(self):
        """Test with empty list."""
        result = merge_to_ranges([])
        self.assertEqual(result, [])

    def test_single_id(self):
        """Test with single ID."""
        result = merge_to_ranges([5])
        self.assertEqual(result, [(5, 5)])

    def test_consecutive_ids(self):
        """Test consecutive IDs."""
        result = merge_to_ranges([0, 1, 2, 3])
        self.assertEqual(result, [(0, 3)])

    def test_non_consecutive_ids(self):
        """Test non-consecutive IDs."""
        result = merge_to_ranges([0, 1, 2, 5, 6, 7])
        self.assertEqual(result, [(0, 2), (5, 7)])

    def test_unsorted_ids(self):
        """Test unsorted IDs are sorted."""
        result = merge_to_ranges([5, 2, 3, 0, 1])
        self.assertEqual(result, [(0, 3), (5, 5)])

    def test_multiple_gaps(self):
        """Test multiple gaps."""
        result = merge_to_ranges([0, 2, 4, 6])
        self.assertEqual(result, [(0, 0), (2, 2), (4, 4), (6, 6)])


class TestFormatRanges(unittest.TestCase):
    """Tests for format_ranges function."""

    def test_empty_ranges(self):
        """Test empty ranges."""
        result = format_ranges([])
        self.assertEqual(result, "(none)")

    def test_single_value_range(self):
        """Test single value range."""
        result = format_ranges([(5, 5)])
        self.assertEqual(result, "5")

    def test_actual_range(self):
        """Test actual range."""
        result = format_ranges([(0, 3)])
        self.assertEqual(result, "0-3")

    def test_multiple_ranges(self):
        """Test multiple ranges."""
        result = format_ranges([(0, 3), (6, 9)])
        self.assertEqual(result, "0-3, 6-9")

    def test_mixed_ranges(self):
        """Test mixed single values and ranges."""
        result = format_ranges([(0, 3), (5, 5), (8, 10)])
        self.assertEqual(result, "0-3, 5, 8-10")


class TestComputeWarpSummary(unittest.TestCase):
    """Tests for compute_warp_summary function."""

    def test_empty_groups(self):
        """Test with empty groups."""
        result = compute_warp_summary({})
        self.assertIsNone(result)

    def test_non_integer_keys(self):
        """Test with non-integer keys."""
        groups = {"a": [{"sass": "MOV R1, R0;"}], "b": [{"sass": "EXIT;"}]}
        result = compute_warp_summary(groups)
        self.assertIsNone(result)

    def test_all_completed(self):
        """Test all warps completed."""
        groups = {
            0: [{"sass": "MOV R1, R0;"}, {"sass": "EXIT;"}],
            1: [{"sass": "ADD R1, R2;"}, {"sass": "EXIT;"}],
        }
        result = compute_warp_summary(groups)
        self.assertIsNotNone(result)
        self.assertEqual(result.total_observed, 2)
        self.assertEqual(result.completed_warp_ids, [0, 1])
        self.assertEqual(result.inprogress_warp_ids, [])

    def test_all_inprogress(self):
        """Test all warps in progress."""
        groups = {
            0: [{"sass": "MOV R1, R0;"}],
            1: [{"sass": "ADD R1, R2;"}],
        }
        result = compute_warp_summary(groups)
        self.assertIsNotNone(result)
        self.assertEqual(result.completed_warp_ids, [])
        self.assertEqual(result.inprogress_warp_ids, [0, 1])

    def test_mixed_status(self):
        """Test mixed completed and in-progress."""
        groups = {
            0: [{"sass": "EXIT;"}],
            1: [{"sass": "MOV R1, R0;"}],
            2: [{"sass": "EXIT;"}],
        }
        result = compute_warp_summary(groups)
        self.assertIsNotNone(result)
        self.assertEqual(result.completed_warp_ids, [0, 2])
        self.assertEqual(result.inprogress_warp_ids, [1])

    def test_missing_warps(self):
        """Test missing warps detection."""
        groups = {
            0: [{"sass": "EXIT;"}],
            3: [{"sass": "EXIT;"}],
        }
        result = compute_warp_summary(groups)
        self.assertIsNotNone(result)
        self.assertEqual(result.missing_warp_ids, [1, 2])

    def test_warp_id_range(self):
        """Test warp ID range calculation."""
        groups = {
            5: [{"sass": "EXIT;"}],
            10: [{"sass": "EXIT;"}],
        }
        result = compute_warp_summary(groups)
        self.assertIsNotNone(result)
        self.assertEqual(result.min_warp_id, 5)
        self.assertEqual(result.max_warp_id, 10)


class TestFormatWarpSummaryText(unittest.TestCase):
    """Tests for format_warp_summary_text function."""

    def test_format_basic(self):
        """Test basic formatting."""
        summary = WarpSummary(
            total_observed=3,
            min_warp_id=0,
            max_warp_id=2,
            completed_warp_ids=[0, 2],
            inprogress_warp_ids=[1],
            missing_warp_ids=[],
        )
        result = format_warp_summary_text(summary)
        self.assertIn("Warp Summary", result)
        self.assertIn("Total warps observed:   3", result)
        self.assertIn("Completed (EXIT):", result)
        self.assertIn("In-progress:", result)
        self.assertIn("Missing (never seen):", result)

    def test_format_percentages(self):
        """Test percentage calculation."""
        summary = WarpSummary(
            total_observed=4,
            min_warp_id=0,
            max_warp_id=3,
            completed_warp_ids=[0, 1],
            inprogress_warp_ids=[2, 3],
            missing_warp_ids=[],
        )
        result = format_warp_summary_text(summary)
        self.assertIn("50.0%", result)


class TestWarpSummaryToDict(unittest.TestCase):
    """Tests for warp_summary_to_dict function."""

    def test_to_dict_basic(self):
        """Test basic dict conversion."""
        summary = WarpSummary(
            total_observed=2,
            min_warp_id=0,
            max_warp_id=1,
            completed_warp_ids=[0],
            inprogress_warp_ids=[1],
            missing_warp_ids=[],
        )
        result = warp_summary_to_dict(summary)
        self.assertEqual(result["total_observed"], 2)
        self.assertEqual(result["warp_id_range"], [0, 1])
        self.assertEqual(result["completed"]["count"], 1)
        self.assertEqual(result["in_progress"]["count"], 1)
        self.assertEqual(result["missing"]["count"], 0)

    def test_to_dict_percentages(self):
        """Test percentage calculation in dict."""
        summary = WarpSummary(
            total_observed=4,
            min_warp_id=0,
            max_warp_id=3,
            completed_warp_ids=[0, 1, 2, 3],
            inprogress_warp_ids=[],
            missing_warp_ids=[],
        )
        result = warp_summary_to_dict(summary)
        self.assertEqual(result["completed"]["percentage"], 100.0)
        self.assertEqual(result["in_progress"]["percentage"], 0.0)

    def test_to_dict_ranges(self):
        """Test ranges in dict."""
        summary = WarpSummary(
            total_observed=2,
            min_warp_id=0,
            max_warp_id=3,
            completed_warp_ids=[0, 1],
            inprogress_warp_ids=[],
            missing_warp_ids=[2, 3],
        )
        result = warp_summary_to_dict(summary)
        self.assertEqual(result["completed"]["ranges"], [(0, 1)])
        self.assertEqual(result["missing"]["ranges"], [(2, 3)])


if __name__ == "__main__":
    unittest.main()

```

`readme.md`:

```md
# CUTracer

CUTracer is an [NVBit-based](https://github.com/NVlabs/NVBit) CUDA binary instrumentation tool. It cleanly separates lightweight data collection (instrumentation) from host-side processing (analysis). Typical workflows include per-warp instruction histograms (delimited by GPU clock reads) and kernel hang detection.

## Features

-   NVBit-powered, runtime attach via `CUDA_INJECTION64_PATH` (no app rebuild needed)
-   Multiple instrumentation modes: opcode-only, register trace, memory trace, random delay
-   Built-in analyses:
    -   Instruction Histogram (for Proton/Triton workflows)
    -   Deadlock/Hang Detection
    -   Data Race Detection
-   CUDA Graph and stream-capture aware flows
-   Deterministic kernel log file naming and CSV outputs

## Requirements

All requirements are aligned with NVBit.

Unique requirements:
- **libzstd**: Required for trace compression

## Installation

1. Clone the repository:

```bash
git clone git@github.com:facebookresearch/CUTracer.git
cd CUTracer
```

2. Install system dependencies (libzstd static library for self-contained builds):

```bash
# Ubuntu/Debian
# On most Ubuntu/Debian systems, libzstd-dev provides both shared and static libs (libzstd.a).
# You can verify this with: dpkg -L libzstd-dev | grep 'libzstd.a'
# If your distribution does not ship the static library in libzstd-dev, you may need to
# build zstd from source or install a distro-specific static libzstd package.
sudo apt-get install libzstd-dev

# CentOS/RHEL/Fedora (static library for portable builds)
sudo dnf install libzstd-static

# If static library is not available, the build will fall back to dynamic linking
# and display a warning. The resulting binary will not be self-contained.
```

3. Download third-party dependencies:

```bash
./install_third_party.sh
```

This will download:
- NVBit (NVIDIA Binary Instrumentation Tool)
- nlohmann/json (JSON library for C++)

4. Build the tool:

```bash
make -j$(nproc)
```

## Quickstart

Run your CUDA app with CUTracer (example: No instrumentation):

```bash
CUDA_INJECTION64_PATH=~/CUTracer/lib/cutracer.so \
./your_app
```

## Configuration (env vars)

-   `CUTRACER_INSTRUMENT`: comma-separated modes: `opcode_only`, `reg_trace`, `mem_trace`, `random_delay`
-   `CUTRACER_ANALYSIS`: comma-separated analyses: `proton_instr_histogram`, `deadlock_detection`, `random_delay`
    -   Enabling `proton_instr_histogram` auto-enables `opcode_only`
    -   Enabling `deadlock_detection` auto-enables `reg_trace`
    -   Enabling `random_delay` requires `CUTRACER_DELAY_NS` to be set
-   `KERNEL_FILTERS`: comma-separated substrings matching unmangled or mangled kernel names
-   `INSTR_BEGIN`, `INSTR_END`: static instruction index gate during instrumentation
-   `TOOL_VERBOSE`: 0/1/2
-   `TRACE_FORMAT_NDJSON`: trace output format
    -   **1** (default): NDJSON+Zstd compressed (`.ndjson.zst`, ~12x compression, 92% space savings)
    -   0: Plain text (`.log`, legacy format, verbose)
    -   2: NDJSON uncompressed (`.ndjson`, for debugging)
-   `CUTRACER_ZSTD_LEVEL`: Zstd compression level (1-22, default 22)
    -   Lower values (1-3): Faster compression, slightly larger output
    -   Higher values (19-22): Maximum compression, slower but smallest output
    -   Default of 22 provides maximum compression for smallest output
- `CUTRACER_DELAY_NS`: Fixed delay value in nanoseconds for race detection (required for `random_delay` analysis)
- `CUTRACER_DELAY_DUMP_PATH`: Output path for delay config JSON file (for recording instrumentation patterns)
- `CUTRACER_DELAY_LOAD_PATH`: Input path for delay config JSON file (for replay mode - deterministic reproduction)

Note: The tool sets `CUDA_MANAGED_FORCE_DEVICE_ALLOC=1` to simplify channel memory handling.

## Analyses

### Instruction Histogram (proton_instr_histogram)

-   Counts SASS instruction mnemonics per warp within regions delimited by clock reads (start/stop model; nested regions not supported)
-   Output: one CSV per kernel launch with columns `warp_id,region_id,instruction,count`

Example (Triton/Proton + IPC):

```bash
cd ~/CUTracer/tests/proton_tests

# 1) Collect histogram with CUTracer
CUDA_INJECTION64_PATH=~/CUTracer/lib/cutracer.so \
CUTRACER_ANALYSIS=proton_instr_histogram \
KERNEL_FILTERS=add_kernel \
python ./vector-add-instrumented.py

# 2) Run without CUTracer to generate a clean Chrome trace
python ./vector-add-instrumented.py

# 3) Merge and compute IPC
python ~/CUTracer/scripts/parse_instr_hist_trace.py \
  --chrome-trace ./vector.chrome_trace \
  --cutracer-trace ./kernel_*_add_kernel_hist.csv \
  --cutracer-log ./cutracer_main_*.log \
  --output vectoradd_ipc.csv
```

### Deadlock / Hang Detection (deadlock_detection)

-   Detects sustained hangs by identifying warps stuck in stable PC loops; logs and issues SIGTERMâ†’SIGKILL if sustained
-   Requires `reg_trace` (auto-enabled)

Example (intentional loop):

```bash
cd ~/CUTracer/tests/hang_test
CUDA_INJECTION64_PATH=~/CUTracer/lib/cutracer.so \
CUTRACER_ANALYSIS=deadlock_detection \
python ./test_hang.py
```

### Data Race Detection (random_delay)

-   Data races depend on timing and often pass by luck. This analysis uses random delay injection to detect races by injecting delays before synchronization instructions, disrupting the timing and forcing hidden races to show up
-   Each instrumentation point is randomly enabled/disabled (50% probability) with a fixed delay value
-   Requires `CUTRACER_DELAY_NS` to be set

Example:

```bash
CUTRACER_DELAY_NS=10000 \
CUTRACER_ANALYSIS=random_delay \
CUDA_INJECTION64_PATH=~/CUTracer/lib/cutracer.so \
python3 your_kernel.py
```

#### Delay Dump and Replay

CUTracer supports dumping delay configurations to JSON for deterministic reproduction of data races:

- **Dump mode**: Set `CUTRACER_DELAY_DUMP_PATH` to save the random instrumentation pattern to a JSON file
- **Replay mode**: Set `CUTRACER_DELAY_LOAD_PATH` to load a saved config and reproduce the exact same delay pattern

**Note**: You cannot use both at the same time.

**Workflow**:
1. Run with `CUTRACER_DELAY_DUMP_PATH=/tmp/config.json` to record the delay pattern
2. When a failure occurs, save the config file
3. Replay with `CUTRACER_DELAY_LOAD_PATH=/tmp/config.json` to reproduce deterministically

## Troubleshooting

-   No CSV/log: check `CUDA_INJECTION64_PATH`, `KERNEL_FILTERS`, and write permissions
-   Empty histogram: ensure kernels emit clock instructions (e.g., Triton `pl.scope`)
-   High overhead: prefer opcode-only; narrow filters; use `INSTR_BEGIN/INSTR_END`
-   CUDA Graph/stream capture: data is flushed at `cuGraphLaunch` exit; ensure stream sync
-   IPC merge issues: resolve warp mismatches and kernel hash ambiguity with parser flags

## License

This repository contains code under the MIT license (Meta) and the BSD-3-Clause license (NVIDIA). See [LICENSE](LICENSE) and [LICENSE-BSD](LICENSE-BSD) for details.

## More Documentation

The full documentation lives in the [Wiki](https://github.com/facebookresearch/CUTracer/wiki). Key topics include Quickstart, Analyses, Post-processing, Configuration, Outputs, API & Data Structures, Developer Guide, and Troubleshooting.

```

`scripts/install_cuda.sh`:

```sh
#!/bin/bash
# Usage: CUDA_INSTALL_PREFIX=/home/yhao/opt ./install_cuda.sh 12.8
# Notice: Part of this script is synced with https://github.com/pytorch/pytorch/blob/main/.ci/docker/common/install_cuda.sh
set -ex

# Debug settings - log all execution steps
# Use user-specific directory to avoid permission issues in multi-user environments
DEBUG_LOG_DIR="/tmp/${USER}"
mkdir -p "${DEBUG_LOG_DIR}"
exec 5>"${DEBUG_LOG_DIR}/cuda_install_debug.txt"
BASH_XTRACEFD="5"
PS4='${LINENO}: '

# Record script start time for duration calculation
SCRIPT_START_TIME=$(date +%s)

echo "ğŸš€ ===== CUDA Installation Script Started ====="

# Basic settings
CUDA_INSTALL_PREFIX=${CUDA_INSTALL_PREFIX:-$HOME/opt}
CUDA_INSTALL_PREFIX=${CUDA_INSTALL_PREFIX%/}
CUDA_VERSION=${CUDA_VERSION:-12.8}
NVSHMEM_VERSION=${NVSHMEM_VERSION:-3.4.5}
INSTALL_NCCL=${INSTALL_NCCL:-1}
NCCL_VERSION=${NCCL_VERSION:-v2.28.9-1}

# Minimum required disk space in GB (can be overridden)
REQUIRED_DISK_SPACE_GB=${REQUIRED_DISK_SPACE_GB:-15}

echo "CUDA_INSTALL_PREFIX=${CUDA_INSTALL_PREFIX}"
echo "CUDA_VERSION=${CUDA_VERSION}"
echo "INSTALL_NCCL=${INSTALL_NCCL}"
echo "NCCL_VERSION=${NCCL_VERSION}"

# Version configuration using associative arrays
declare -A CUDA_FULL_VERSION=(
  ["12.6"]="12.6.3"
  ["12.8"]="12.8.1"
  ["12.9"]="12.9.1"
  ["13.0"]="13.0.2"
  ["13.1"]="13.1.1"
)

declare -A CUDA_RUNFILE=(
  ["12.6"]="cuda_12.6.3_560.35.05_linux"
  ["12.8"]="cuda_12.8.1_570.124.06_linux"
  ["12.9"]="cuda_12.9.1_575.57.08_linux"
  ["13.0"]="cuda_13.0.2_580.95.05_linux"
  ["13.1"]="cuda_13.1.1_590.48.01_linux"
)

declare -A CUDNN_VERSIONS=(
  ["12.6"]="9.10.2.21"
  ["12.8"]="9.17.1.4"
  ["12.9"]="9.17.1.4"
  ["13.0"]="9.17.1.4"
  ["13.1"]="9.17.1.4"
)

declare -A CUDA_MAJOR=(
  ["12.6"]="12"
  ["12.8"]="12"
  ["12.9"]="12"
  ["13.0"]="13"
  ["13.1"]="13"
)

# Create temporary directory
USER_TMPDIR="/tmp/${USER}/cuda_install"
mkdir -p "${USER_TMPDIR}"
export TMPDIR="${USER_TMPDIR}"

# Error handling function
function error_exit {
  echo "âŒ ERROR: $1" >&2
  # Save error message to log file
  echo "$(date): ERROR - $1" >>"${USER_TMPDIR}/error.log"
  # Leave error marker file for debugging
  echo "$1" >"${USER_TMPDIR}/error_message"
  touch "${USER_TMPDIR}/error_occurred"
  exit 1
}

# Cleanup function for temporary directories
# Uses absolute paths based on USER_TMPDIR to ensure cleanup works regardless of current directory
function cleanup_temp_dirs {
  echo "Cleaning up temporary directories..."
  local base_dir
  base_dir=$(pwd)

  # Clean up directories in the current working directory (where script was started)
  [ -d "${base_dir}/tmp_cusparselt" ] && rm -rf "${base_dir}/tmp_cusparselt"
  [ -d "${base_dir}/tmp_cudnn" ] && rm -rf "${base_dir}/tmp_cudnn"
  [ -d "${base_dir}/nccl" ] && rm -rf "${base_dir}/nccl"
  [ -d "${base_dir}/tmp_nvshmem" ] && rm -rf "${base_dir}/tmp_nvshmem"

  # Also clean up in USER_TMPDIR if different from base_dir
  if [ "${USER_TMPDIR}" != "${base_dir}" ]; then
    [ -d "${USER_TMPDIR}/tmp_cusparselt" ] && rm -rf "${USER_TMPDIR}/tmp_cusparselt"
    [ -d "${USER_TMPDIR}/tmp_cudnn" ] && rm -rf "${USER_TMPDIR}/tmp_cudnn"
    [ -d "${USER_TMPDIR}/nccl" ] && rm -rf "${USER_TMPDIR}/nccl"
    [ -d "${USER_TMPDIR}/tmp_nvshmem" ] && rm -rf "${USER_TMPDIR}/tmp_nvshmem"
  fi

  touch "${USER_TMPDIR}/cleanup_completed"
}

# Cleanup function called on script exit (trap handler)
function cleanup_on_exit {
  local exit_code=$?
  echo "Script exiting with code: ${exit_code}"
  cleanup_temp_dirs
  if [ ${exit_code} -ne 0 ]; then
    echo "âš ï¸  Script exited with errors. Check ${USER_TMPDIR}/error.log for details."
  fi
}

# Set trap after functions are defined to avoid calling undefined functions
trap cleanup_on_exit EXIT
trap 'error_exit "Script interrupted"' INT TERM

# Detect architecture
export TARGETARCH=${TARGETARCH:-$(uname -m)}
if [ "${TARGETARCH}" = 'aarch64' ] || [ "${TARGETARCH}" = 'arm64' ]; then
  ARCH_PATH='sbsa'
else
  ARCH_PATH='x86_64'
fi

echo "ğŸ—ï¸  Architecture: ${ARCH_PATH}"

# Check if command exists
function command_exists {
  command -v "$1" &>/dev/null
}

# Check dependencies
function check_dependencies {
  echo "Checking dependencies..."
  local missing_deps=()

  for cmd in wget git make; do
    if ! command_exists "$cmd"; then
      missing_deps+=("$cmd")
    fi
  done

  if [ ${#missing_deps[@]} -gt 0 ]; then
    error_exit "Missing required dependencies: ${missing_deps[*]}"
  fi

  echo "âœ… All dependency checks passed"
}

# Check disk space (CUDA toolkit requires approximately 10-15GB)
function check_disk_space {
  local required_gb=${1:-15}
  local target_dir="${CUDA_INSTALL_PREFIX}"

  echo "Checking disk space (need ${required_gb}GB)..."

  local available_kb
  available_kb=$(df "${target_dir}" | awk 'NR==2 {print $4}')
  local available_gb=$((available_kb / 1024 / 1024))

  if [ "${available_gb}" -lt "${required_gb}" ]; then
    error_exit "Insufficient disk space: need ${required_gb}GB, only ${available_gb}GB available at ${target_dir}"
  fi

  echo "âœ… Disk space check passed (${available_gb}GB available)"
}

# Check network connectivity to NVIDIA download server
function check_network {
  echo "Checking network connectivity to NVIDIA servers..."

  local test_url="https://developer.nvidia.com/cuda/toolkit"
  local http_code

  http_code=$(curl -s -o /dev/null -w "%{http_code}" --connect-timeout 10 "${test_url}" 2>/dev/null || echo "000")

  if [[ "${http_code}" == "000" ]]; then
    error_exit "Cannot connect to NVIDIA download server (${test_url}). Please check your network connection."
  elif [[ "${http_code}" =~ ^(2|3)[0-9][0-9]$ ]]; then
    echo "âœ… Network connectivity check passed (HTTP ${http_code})"
  else
    echo "âš ï¸  Warning: NVIDIA server returned HTTP ${http_code}, continuing anyway..."
  fi
}

# Get disk space information for a given path
# Returns: total_gb used_gb available_gb use_percent
function get_disk_info {
  local target_path="${1:-/}"
  df -BG "${target_path}" | awk 'NR==2 {
    gsub(/G/, "", $2); gsub(/G/, "", $3); gsub(/G/, "", $4); gsub(/%/, "", $5);
    print $2, $3, $4, $5
  }'
}

# Print disk space summary
function print_disk_summary {
  local label="$1"
  local target_path="${2:-/}"

  local disk_info
  disk_info=$(get_disk_info "${target_path}")
  local total_gb
  local used_gb
  local available_gb
  local use_percent
  total_gb=$(echo "${disk_info}" | awk '{print $1}')
  used_gb=$(echo "${disk_info}" | awk '{print $2}')
  available_gb=$(echo "${disk_info}" | awk '{print $3}')
  use_percent=$(echo "${disk_info}" | awk '{print $4}')

  echo "ğŸ’¾ ${label}:"
  echo "   Total: ${total_gb}GB | Used: ${used_gb}GB | Available: ${available_gb}GB | Usage: ${use_percent}%"
}

# Check if file or directory exists
function check_exists {
  if [ ! -e "$1" ]; then
    error_exit "Path does not exist: $1"
  fi
}

# Check directory
check_exists "${CUDA_INSTALL_PREFIX}"

# Check dependencies
check_dependencies

# Check disk space
check_disk_space "${REQUIRED_DISK_SPACE_GB}"

# Check network connectivity
check_network

# Real CUDA installation function
function install_cuda {
  local version=$1
  local runfile=$2
  local major_minor=${version%.*}

  echo "Installing CUDA ${version}..."
  rm -rf "${CUDA_INSTALL_PREFIX}/cuda-${major_minor}" "${CUDA_INSTALL_PREFIX}/cuda"

  if [[ ${ARCH_PATH} == 'sbsa' ]]; then
    runfile="${runfile}_sbsa"
  fi
  runfile="${runfile}.run"

  echo "Downloading CUDA installation file: ${runfile}"
  # Use -c for resume support, -t 3 for retry, -q for quiet mode
  if ! wget -c -t 3 -q "https://developer.download.nvidia.com/compute/cuda/${version}/local_installers/${runfile}" -O "${runfile}"; then
    error_exit "CUDA installation file download failed: ${runfile}"
  fi

  echo "CUDA installation file download complete, preparing to install..."
  chmod +x "${runfile}"

  echo "Executing CUDA installation script..."
  if ! ./"${runfile}" --toolkit --silent --toolkitpath="${CUDA_INSTALL_PREFIX}/cuda-${major_minor}"; then
    echo "CUDA installation failed, keeping installation file for troubleshooting"
    error_exit "CUDA ${version} installation failed"
  fi

  # Create installation complete marker
  touch "${USER_TMPDIR}/cuda_${version}_install_complete"

  echo "CUDA installation complete, creating symbolic link..."
  rm -f "${CUDA_INSTALL_PREFIX}/cuda" && ln -s "${CUDA_INSTALL_PREFIX}/cuda-${major_minor}" "${CUDA_INSTALL_PREFIX}/cuda"

  echo "Removing installation file..."
  rm -f "${runfile}"

  echo "CUDA ${version} installation completed"
  return 0
}

# Real cuDNN installation function
function install_cudnn {
  local cuda_major_version=$1
  local cudnn_version=$2

  echo "Installing cuDNN ${cudnn_version} for CUDA ${cuda_major_version}..."

  # Create temporary directory and enter it using pushd for consistent style
  mkdir -p tmp_cudnn
  pushd tmp_cudnn || error_exit "Failed to enter cuDNN temporary directory"

  # cuDNN license: https://developer.nvidia.com/cudnn/license_agreement
  local filepath="cudnn-linux-${ARCH_PATH}-${cudnn_version}_cuda${cuda_major_version}-archive"
  echo "Downloading cuDNN: ${filepath}.tar.xz"
  # Use -c for resume support, -t 3 for retry
  if ! wget -c -t 3 -q "https://developer.download.nvidia.com/compute/cudnn/redist/cudnn/linux-${ARCH_PATH}/${filepath}.tar.xz"; then
    popd
    rm -rf tmp_cudnn
    error_exit "cuDNN download failed: ${filepath}.tar.xz"
  fi

  echo "cuDNN download complete, preparing to extract..."
  if ! tar xf "${filepath}.tar.xz"; then
    popd
    rm -rf tmp_cudnn
    error_exit "cuDNN extraction failed: ${filepath}.tar.xz"
  fi

  echo "cuDNN extraction complete, installing to CUDA directory..."
  cp -a "${filepath}/include/"* "${CUDA_INSTALL_PREFIX}/cuda/include/"
  cp -a "${filepath}/lib/"* "${CUDA_INSTALL_PREFIX}/cuda/lib64/"

  echo "cuDNN installation complete, cleaning up temporary files..."
  popd
  rm -rf tmp_cudnn

  # Create cuDNN installation complete marker
  touch "${USER_TMPDIR}/cudnn_${cudnn_version}_installed"

  echo "cuDNN ${cudnn_version} installation completed"
  return 0
}

# Real NCCL installation function
function install_nccl {
  if [ "$INSTALL_NCCL" -eq 0 ]; then
    echo "NCCL installation skipped as per INSTALL_NCCL=${INSTALL_NCCL}"
    return 0
  fi

  echo "Installing NCCL for CUDA ${CUDA_VERSION}..."
  if [[ -z "${NCCL_VERSION}" ]]; then
    error_exit "NCCL_VERSION is empty"
  fi

  echo "Using NCCL version: ${NCCL_VERSION}"
  echo "${NCCL_VERSION}" >"${USER_TMPDIR}/nccl_version.txt"

  # NCCL license: https://docs.nvidia.com/deeplearning/nccl/#licenses
  # Follow build: https://github.com/NVIDIA/nccl/tree/master?tab=readme-ov-file#build
  echo "Cloning NCCL repository..."
  if ! git clone -b "${NCCL_VERSION}" --depth 1 https://github.com/NVIDIA/nccl.git; then
    error_exit "NCCL repository clone failed"
  fi

  echo "Starting NCCL compilation..."
  pushd nccl || error_exit "Failed to enter NCCL directory"

  if ! make -j src.build CUDA_HOME="${CUDA_INSTALL_PREFIX}/cuda"; then
    popd
    rm -rf nccl
    error_exit "NCCL compilation failed"
  fi

  echo "NCCL compilation complete, installing to CUDA directory..."
  cp -a build/include/* "${CUDA_INSTALL_PREFIX}/cuda/include/"
  cp -a build/lib/* "${CUDA_INSTALL_PREFIX}/cuda/lib64/"

  popd
  rm -rf nccl

  # Note: ldconfig is called once at the end of install_cuda_version
  # No need to call it here

  # Create NCCL installation complete marker
  touch "${USER_TMPDIR}/nccl_installed"

  echo "NCCL installation completed"
  return 0
}

# Real cuSparseLt installation function
function install_cusparselt {
  echo "Installing cuSparseLt for CUDA ${CUDA_VERSION}..."
  mkdir -p tmp_cusparselt
  pushd tmp_cusparselt || error_exit "Failed to create cuSparseLt temporary directory"

  local cusparselt_version
  local arch_path=${ARCH_PATH}

  local CUSPARSELT_NAME
  if [[ ${CUDA_VERSION:0:2} == "13" ]]; then
    cusparselt_version="0.8.0.4"
    CUSPARSELT_NAME="libcusparse_lt-linux-${arch_path}-${cusparselt_version}_cuda13-archive"
  elif [[ ${CUDA_VERSION} =~ ^12\.([5-9]|[1-9][0-9]+)$ ]]; then
    cusparselt_version="0.7.1.0"
    CUSPARSELT_NAME="libcusparse_lt-linux-${arch_path}-${cusparselt_version}-archive"
  else
    popd
    rm -rf tmp_cusparselt
    error_exit "Unsupported CUDA version: ${CUDA_VERSION}"
  fi

  echo "${cusparselt_version}" >"${USER_TMPDIR}/cusparselt_version.txt"

  echo "Downloading cuSparseLt: ${CUSPARSELT_NAME}.tar.xz"
  # Use -c for resume support, -t 3 for retry
  if ! wget -c -t 3 -q "https://developer.download.nvidia.com/compute/cusparselt/redist/libcusparse_lt/linux-${arch_path}/${CUSPARSELT_NAME}.tar.xz"; then
    popd
    rm -rf tmp_cusparselt
    error_exit "cuSparseLt download failed"
  fi

  echo "cuSparseLt download complete, preparing to extract..."
  if ! tar xf "${CUSPARSELT_NAME}.tar.xz"; then
    popd
    rm -rf tmp_cusparselt
    error_exit "cuSparseLt extraction failed"
  fi

  echo "cuSparseLt extraction complete, installing to CUDA directory..."
  cp -a "${CUSPARSELT_NAME}/include/"* "${CUDA_INSTALL_PREFIX}/cuda/include/"
  cp -a "${CUSPARSELT_NAME}/lib/"* "${CUDA_INSTALL_PREFIX}/cuda/lib64/"

  popd
  rm -rf tmp_cusparselt

  touch "${USER_TMPDIR}/cusparselt_installed"

  echo "cuSparseLt installation completed"
  return 0
}

# nvSHMEM installation function
function install_nvshmem {
  local cuda_major_version=$1 # e.g. "12"
  local nvshmem_version=${NVSHMEM_VERSION}
  local arch_path=${ARCH_PATH}

  local tmpdir="tmp_nvshmem"
  mkdir -p "${tmpdir}"
  pushd "${tmpdir}" || error_exit "Failed to enter nvSHMEM temporary directory"

  # nvSHMEM license: https://docs.nvidia.com/nvshmem/api/sla.html
  local filename="libnvshmem-linux-${arch_path}-${nvshmem_version}_cuda${cuda_major_version}-archive"
  local suffix=".tar.xz"
  local url="https://developer.download.nvidia.com/compute/nvshmem/redist/libnvshmem/linux-${arch_path}/${filename}${suffix}"

  echo "Downloading nvSHMEM: ${filename}${suffix}"
  # Use -c for resume support, -t 3 for retry
  if ! wget -c -t 3 -q "${url}"; then
    popd
    rm -rf "${tmpdir}"
    error_exit "nvSHMEM download failed: ${filename}${suffix}"
  fi

  echo "Extracting nvSHMEM..."
  if ! tar xf "${filename}${suffix}"; then
    popd
    rm -rf "${tmpdir}"
    error_exit "nvSHMEM extraction failed: ${filename}${suffix}"
  fi

  echo "Installing nvSHMEM to CUDA directory..."
  cp -a "${filename}/include/"* "${CUDA_INSTALL_PREFIX}/cuda/include/"
  cp -a "${filename}/lib/"* "${CUDA_INSTALL_PREFIX}/cuda/lib64/"

  popd
  rm -rf "${tmpdir}"

  # Create nvshmem installation complete marker
  touch "${USER_TMPDIR}/nvshmem_${nvshmem_version}_installed"

  echo "nvSHMEM ${nvshmem_version} for CUDA ${cuda_major_version} (${arch_path}) installed."
  return 0
}

# Generic CUDA version installation function
# Uses version configuration from associative arrays defined at the top
function install_cuda_version {
  local version=$1

  # Get configuration for this version
  local cuda_full=${CUDA_FULL_VERSION[$version]}
  local runfile=${CUDA_RUNFILE[$version]}
  local cudnn_ver=${CUDNN_VERSIONS[$version]}
  local cuda_major=${CUDA_MAJOR[$version]}

  if [[ -z "$cuda_full" ]]; then
    error_exit "No configuration found for CUDA version: $version"
  fi

  # Export CUDNN_VERSION for summary display
  export INSTALLED_CUDNN_VERSION="${cudnn_ver}"

  echo "Starting installation for CUDA ${version}..."
  echo "  Full version: ${cuda_full}"
  echo "  Runfile: ${runfile}"
  echo "  cuDNN version: ${cudnn_ver}"
  echo "  CUDA major: ${cuda_major}"

  # All installation functions use consistent error handling:
  # - Functions return 0 on success, non-zero on failure
  # - Functions call error_exit internally for detailed error messages
  # - The || error_exit pattern provides a fallback if the function returns non-zero without calling error_exit

  echo "ğŸ“¦ STEP 1: Installing CUDA toolkit..."
  if ! install_cuda "${cuda_full}" "${runfile}"; then
    error_exit "CUDA ${cuda_full} toolkit installation failed"
  fi

  echo "ğŸ§  STEP 2: Installing cuDNN..."
  if ! install_cudnn "${cuda_major}" "${cudnn_ver}"; then
    error_exit "cuDNN installation failed"
  fi

  echo "ğŸ”— STEP 3: Installing NCCL..."
  if ! install_nccl; then
    error_exit "NCCL installation failed"
  fi

  echo "âš¡ STEP 4: Installing cuSparseLt..."
  if ! install_cusparselt; then
    error_exit "cuSparseLt installation failed"
  fi

  echo "ğŸ’¾ STEP 5: Installing nvSHMEM..."
  if ! install_nvshmem "${cuda_major}"; then
    error_exit "nvSHMEM installation failed"
  fi

  if [ "$(id -u)" -eq 0 ]; then
    ldconfig
  else
    echo "âš ï¸  Note: Running as non-root user, ldconfig was not executed."
    echo "   You may need to set LD_LIBRARY_PATH to use the installed libraries."
  fi

  echo "âœ… CUDA ${version} installation completed"
  return 0
}



# Main execution logic
echo "ğŸ”§ ===== Parsing command line arguments ====="
# Generate VALID_VERSIONS from the configuration arrays to keep them in sync
VALID_VERSIONS=("${!CUDA_FULL_VERSION[@]}")
# Sort the versions for consistent ordering
mapfile -t VALID_VERSIONS < <(printf '%s\n' "${VALID_VERSIONS[@]}" | sort -V)

# Parse command line arguments
while test $# -gt 0; do
  echo "Processing argument: $1"
  if [[ " ${VALID_VERSIONS[*]} " =~ " $1 " ]]; then
    CUDA_VERSION=$1
    echo "Setting CUDA version to: $CUDA_VERSION"
  else
    error_exit "Invalid argument: $1, CUDA version must be one of: ${VALID_VERSIONS[*]}"
  fi
  shift
done

# Note: Version validation is already done in the while loop above
# No need for redundant validation here
echo "Using CUDA version: $CUDA_VERSION"

# Perform cleanup before installation
cleanup_temp_dirs

# Perform installation
echo "âš™ï¸  ===== Starting installation of CUDA ${CUDA_VERSION} ====="
echo "Calling install_cuda_version function with version: ${CUDA_VERSION}"

# Use set +e to temporarily disable error exit, for capturing errors and logging
set +e
install_cuda_version "${CUDA_VERSION}"
INSTALL_RESULT=$?
set -e

if [ $INSTALL_RESULT -ne 0 ]; then
  error_exit "Installation failed, exit code: $INSTALL_RESULT"
fi



# Final cleanup
cleanup_temp_dirs

# Version verification
echo "ğŸ” ===== Verifying CUDA installation ====="
if [ -f "${CUDA_INSTALL_PREFIX}/cuda/bin/nvcc" ]; then
  echo "âœ… nvcc found - checking version:"
  echo "ğŸ“‹ $( "${CUDA_INSTALL_PREFIX}/cuda/bin/nvcc" --version | head -1 )"
  echo ""
else
  echo "âš ï¸  Warning: nvcc not found at ${CUDA_INSTALL_PREFIX}/cuda/bin/nvcc"
fi

# Check nvidia-smi from system PATH (it's part of the driver, not toolkit)
if command -v nvidia-smi &>/dev/null; then
  echo "âœ… nvidia-smi found in system PATH - checking GPU status:"
  echo "ğŸ“‹ $(nvidia-smi --query-gpu=driver_version,name --format=csv,noheader | head -1)"
  echo ""
else
  echo "â„¹ï¸  Note: nvidia-smi not found in PATH (GPU driver may not be installed)"
fi

echo "ğŸ‰ ===== Script execution completed ====="
touch "${USER_TMPDIR}/script_completed_successfully"
# Calculate and display installation duration
SCRIPT_END_TIME=$(date +%s)
INSTALL_DURATION=$((SCRIPT_END_TIME - SCRIPT_START_TIME))
INSTALL_MINUTES=$((INSTALL_DURATION / 60))
INSTALL_SECONDS=$((INSTALL_DURATION % 60))

echo "âœ… CUDA ${CUDA_VERSION} has been successfully installed to ${CUDA_INSTALL_PREFIX}"
echo "â±ï¸  Total installation time: ${INSTALL_MINUTES}m ${INSTALL_SECONDS}s"
echo "ğŸ“Š ========================================="
echo " ğŸ“‹ CUDA & Related Libraries Installation Summary"
echo "ğŸ“Š ========================================="
echo "  CUDA        : ${CUDA_VERSION}"
echo "  cuDNN       : ${INSTALLED_CUDNN_VERSION:-(not available)}"
if [ "$INSTALL_NCCL" -eq 1 ]; then
  if [ -f "${USER_TMPDIR}/nccl_version.txt" ]; then
    NCCL_VERSION_PRINT=$(cat "${USER_TMPDIR}/nccl_version.txt")
    echo "  NCCL        : ${NCCL_VERSION_PRINT}"
  else
    echo "  NCCL        : (not found)"
  fi
else
  echo "  NCCL        : (skipped)"
fi
if [ -f "${USER_TMPDIR}/cusparselt_version.txt" ]; then
  CUSPARSELT_VERSION_PRINT=$(cat "${USER_TMPDIR}/cusparselt_version.txt")
  echo "  cuSparseLt  : ${CUSPARSELT_VERSION_PRINT}"
else
  echo "  cuSparseLt  : (not found)"
fi
echo "  nvSHMEM     : ${NVSHMEM_VERSION}"
echo "ğŸ”— -----------------------------------------"
echo "ğŸ“  Install Path : ${CUDA_INSTALL_PREFIX}/cuda"
echo "ğŸ’¡  Usage:"
echo "    export PATH=${CUDA_INSTALL_PREFIX}/cuda/bin:\$PATH"
echo "    export LD_LIBRARY_PATH=${CUDA_INSTALL_PREFIX}/cuda/lib64:\$LD_LIBRARY_PATH"
echo "ğŸ“Š ========================================="

```

`scripts/killgpu.sh`:

```sh
#!/bin/bash

# Script to kill all GPU processes owned by the current user
#
# Quick Setup (run once):
#   mkdir -p ~/.local/bin
#   ln -s $(realpath ./killgpu.sh) ~/.local/bin/killgpu
#   export PATH=~/.local/bin:$PATH  # Add this to ~/.bashrc for persistence
#
# Then you can simply run: killgpu

# Check if nvidia-smi is available
if ! command -v nvidia-smi &>/dev/null; then
  echo "nvidia-smi command not found. Are NVIDIA drivers installed?"
  exit 1
fi

# Get current username
CURRENT_USER=$(whoami)
echo "Current user: $CURRENT_USER"

# Get all process IDs using GPUs
echo "Fetching GPU processes..."
GPU_PIDS=$(nvidia-smi --query-compute-apps=pid --format=csv,noheader,nounits)

if [ -z "$GPU_PIDS" ]; then
  echo "No GPU processes found."
  exit 0
fi

# Check if any processes belong to current user
HAS_USER_PROCESSES=false
for PID in $GPU_PIDS; do
  PROCESS_USER=$(ps -o user= -p $PID 2>/dev/null)
  if [ "$PROCESS_USER" = "$CURRENT_USER" ]; then
    HAS_USER_PROCESSES=true
    break
  fi
done

if [ "$HAS_USER_PROCESSES" = false ]; then
  echo "No GPU processes found belonging to $CURRENT_USER."
  exit 0
fi

# Count processes
PROCESS_COUNT=$(echo "$GPU_PIDS" | wc -l)
echo "Found $PROCESS_COUNT GPU processes. Checking ownership..."

# Kill each process that belongs to current user
for PID in $GPU_PIDS; do
  PROCESS_USER=$(ps -o user= -p $PID 2>/dev/null)
  if [ "$PROCESS_USER" = "$CURRENT_USER" ]; then
    PROCESS_NAME=$(ps -p $PID -o comm= 2>/dev/null)
    echo "Killing process $PID ($PROCESS_NAME) owned by $PROCESS_USER..."
    # let's not use -9 to avoid killing the process forcefully
    kill $PID
    if [ $? -eq 0 ]; then
      echo "Process $PID terminated successfully."
    else
      echo "Failed to terminate process $PID."
    fi
  else
    echo "Skipping process $PID (owned by $PROCESS_USER)..."
  fi
done

echo "All user's GPU processes have been terminated."
echo "Sleeping for 2 seconds to verify..."
sleep 2

# Verify all user's processes are gone
REMAINING=$(nvidia-smi --query-compute-apps=pid --format=csv,noheader,nounits)
if [ -z "$REMAINING" ]; then
  echo "Verification complete: No GPU processes remaining."
else
  echo "Remaining GPU processes:"
  CURRENT_USER_REMAINING=false
  for PID in $REMAINING; do
    PROCESS_USER=$(ps -o user= -p $PID 2>/dev/null)
    PROCESS_NAME=$(ps -p $PID -o comm= 2>/dev/null)
    echo "PID: $PID, User: $PROCESS_USER, Process: $PROCESS_NAME"
    if [ "$PROCESS_USER" = "$CURRENT_USER" ]; then
      CURRENT_USER_REMAINING=true
    fi
  done

  if [ "$CURRENT_USER_REMAINING" = true ]; then
    echo "WARNING: There are still GPU processes owned by $CURRENT_USER running!"
    echo "You might need to use 'kill -9' to force terminate these processes."
  fi
fi

```

`scripts/parse_instr_hist_trace.py`:

```py
#!/usr/bin/env python3
#  Copyright (c) Meta Platforms, Inc. and affiliates.

import json
import os
import re
import sys

import click
import pandas as pd


def get_chrome_trace_df(input_file_path):
    """
    Parses a Chrome trace file and returns a pandas DataFrame.

    Args:
        input_file_path (str): The path to the input Chrome trace JSON file.

    Returns:
        pandas.DataFrame: A DataFrame containing the parsed event data.
    """
    try:
        with open(input_file_path, "r") as f:
            trace_data = json.load(f)
    except FileNotFoundError:
        print(f"Error: Input file not found at {input_file_path}")
        return None
    except json.JSONDecodeError:
        print(f"Error: Could not decode JSON from {input_file_path}")
        return None

    trace_events = trace_data.get("traceEvents", [])
    if not trace_events:
        print("No trace events found in the file.")
        return None

    pid_pattern = re.compile(r"Core(\d+)\s+CTA(\d+)")
    tid_pattern = re.compile(r"warp (\d+)")

    parsed_data = []
    for event in trace_events:
        pid = event.get("pid", "")
        tid = event.get("tid", "")

        pid_match = pid_pattern.search(pid)
        tid_match = tid_pattern.search(tid)

        if pid_match:
            core_id = int(pid_match.group(1))
            cta_id = int(pid_match.group(2))
            # This is the local warp ID within a CTA
            local_warp_id = int(tid_match.group(1)) if tid_match else 0

            dur = event.get("dur")
            parsed_data.append(
                {
                    "name": event.get("name"),
                    "category": event.get("cat"),
                    "cycles": dur * 1000 if dur is not None else None,
                    "timestamp_ns": (
                        event.get("ts") * 1000 if event.get("ts") is not None else None
                    ),
                    "core": core_id,
                    "cta": cta_id,
                    "local_warp_id": local_warp_id,
                }
            )

    return pd.DataFrame(parsed_data)


def get_cutracer_hist_df(input_file_path):
    """
    Parses a CUTRICER trace histogram file and returns an aggregated DataFrame.

    Args:
        input_file_path (str): The path to the input CUTRICER trace CSV file.

    Returns:
        pandas.DataFrame: A DataFrame with aggregated instruction counts per warp and region.
    """
    try:
        df = pd.read_csv(input_file_path)
    except FileNotFoundError:
        print(f"Error: Input file not found at {input_file_path}")
        return None
    except Exception as e:
        print(f"An error occurred while reading the CSV file: {e}")
        return None

    required_columns = {"warp_id", "region_id", "count"}
    if not required_columns.issubset(df.columns):
        print(f"Error: Input CSV must contain the columns: {list(required_columns)}")
        return None

    summary = df.groupby(["warp_id", "region_id"])["count"].sum().reset_index()
    summary = summary.rename(
        columns={
            "count": "total_instruction_count",
            "warp_id": "global_warp_id",  # Clarify that this is the global warp ID
        }
    )
    return summary


def parse_cutracer_log(log_file_path, kernel_hash_hex):
    """
    Parses a CUTRICER log file to find the launch parameters for a *specific* kernel hash.
    If multiple launches with the same hash are found, it uses the first one and prints a warning.

    Args:
        log_file_path (str): Path to the CUTRICER log file.
        kernel_hash_hex (str): The mandatory target kernel hash (e.g., "0x7fa21c3").

    Returns:
        dict: { 'grid_size': (gx, gy, gz), 'block_size': (bx, by, bz) } or None if not found.
    """
    grid_pattern = re.compile(r"grid size\s+([0-9]+),([0-9]+),([0-9]+)")
    block_pattern = re.compile(r"block size\s+([0-9]+),([0-9]+),([0-9]+)")
    hash_pattern = re.compile(r"kernel hash\s+0x([0-9a-fA-F]+)")

    expected_hash = kernel_hash_hex.lower().lstrip("0x")
    matching_launches = []

    try:
        with open(log_file_path, "r") as f:
            for line in f:
                if "LAUNCH" not in line:
                    continue

                hash_match = hash_pattern.search(line)
                if not hash_match or hash_match.group(1).lower() != expected_hash:
                    continue

                # Found a matching launch line
                grid_match = grid_pattern.search(line)
                block_match = block_pattern.search(line)

                if grid_match and block_match:
                    launch_info = {
                        "grid_size": tuple(map(int, grid_match.groups())),
                        "block_size": tuple(map(int, block_match.groups())),
                    }
                    matching_launches.append(launch_info)

    except FileNotFoundError:
        print(f"Error: Log file not found at {log_file_path}")
        return None

    if not matching_launches:
        return None

    if len(matching_launches) > 1:
        print(
            f"Warning: Found {len(matching_launches)} launches for kernel hash '{kernel_hash_hex}'. "
            "Defaulting to the first one.",
            file=sys.stderr,
        )

    return matching_launches[0]


def calculate_ipc(cycles, instruction_count):
    """
    Calculates Instructions Per Cycle (IPC).

    Args:
        cycles (float): Number of cycles.
        instruction_count (float): Number of instructions.

    Returns:
        float: IPC value, or None if calculation not possible.
    """
    if (
        pd.isna(cycles)
        or pd.isna(instruction_count)
        or cycles <= 0
        or instruction_count <= 0
    ):
        return None

    # Calculate IPC
    ipc = instruction_count / cycles
    return ipc


def validate_warp_coverage(chrome_df, hist_df):
    """
    Validates that Chrome trace and CUTRICER histogram have matching warp IDs.

    Args:
        chrome_df (pandas.DataFrame): Chrome trace data with global_warp_id
        hist_df (pandas.DataFrame): CUTRICER histogram data with global_warp_id

    Returns:
        bool: True if all warp IDs match, False otherwise
    """
    chrome_warp_ids = set(chrome_df["global_warp_id"].unique())
    hist_warp_ids = set(hist_df["global_warp_id"].unique())

    chrome_only = chrome_warp_ids - hist_warp_ids
    hist_only = hist_warp_ids - chrome_warp_ids

    if chrome_only:
        print(
            f"ERROR: Chrome trace has {len(chrome_only)} warps not found in histogram:"
        )
        sorted_chrome_only = sorted(list(chrome_only))
        if len(sorted_chrome_only) <= 10:
            print(f"  Missing warp IDs: {sorted_chrome_only}")
        else:
            print(f"  First 10 missing warp IDs: {sorted_chrome_only[:10]}")
            print(f"  ... and {len(sorted_chrome_only) - 10} more")

    if hist_only:
        print(f"ERROR: Histogram has {len(hist_only)} warps not found in Chrome trace:")
        sorted_hist_only = sorted(list(hist_only))
        if len(sorted_hist_only) <= 10:
            print(f"  Extra warp IDs: {sorted_hist_only}")
        else:
            print(f"  First 10 extra warp IDs: {sorted_hist_only[:10]}")
            print(f"  ... and {len(sorted_hist_only) - 10} more")

    if len(chrome_only) == 0 and len(hist_only) == 0:
        print("âœ“ Warp ID validation passed: All warps match between data sources")
        return True
    else:
        print(
            f"âœ— Warp ID validation failed: {len(chrome_only)} Chrome-only + {len(hist_only)} histogram-only warps"
        )
        return False


def merge_traces(
    chrome_trace_path,
    cutracer_hist_path,
    cutracer_log_path,
    output_path,
    kernel_hash_hex=None,
):
    """
    Merges data from Chrome trace, CUTRICER histogram, and CUTRICER log.
    """
    # If kernel_hash_hex is not provided, try to extract it from the histogram file name
    if not kernel_hash_hex:
        hist_filename = os.path.basename(cutracer_hist_path)
        hash_match = re.search(r"kernel_([0-9a-fA-F]+)_", hist_filename)
        if hash_match:
            kernel_hash_hex = hash_match.group(1)
            print(f"Successfully extracted kernel hash: {kernel_hash_hex}")
        else:
            print(
                f"ERROR: Could not extract kernel hash from filename: {hist_filename}"
            )
            print(
                "       Please provide a kernel hash using the --kernel-hash argument or"
            )
            print(
                "       ensure the histogram file name is in the format 'kernel_<hash>_...'."
            )
            return

    print("Parsing log file to get metadata...")
    launch_info = parse_cutracer_log(cutracer_log_path, kernel_hash_hex)
    if not launch_info:
        print(
            f"Could not find launch info for kernel hash {kernel_hash_hex} in log file. Aborting merge."
        )
        return

    grid_size = launch_info["grid_size"]  # (x, y, z)
    block_size = launch_info["block_size"]  # (x, y, z)

    # Calculate total threads per block and warps per block
    threads_per_block = block_size[0] * block_size[1] * block_size[2]
    warp_size = 32  # Standard for NVIDIA GPUs
    warps_per_block = (threads_per_block + warp_size - 1) // warp_size

    if kernel_hash_hex:
        print(
            f"Launch info parsed for kernel hash {kernel_hash_hex}: Grid size = {grid_size}, Block size = {block_size}, Warps per block = {warps_per_block}"
        )
    else:
        print(
            f"Launch info parsed: Grid size = {grid_size}, Block size = {block_size}, Warps per block = {warps_per_block}"
        )

    print("Parsing Chrome trace file...")
    chrome_df = get_chrome_trace_df(chrome_trace_path)
    if chrome_df is None:
        print("Failed to parse Chrome trace. Aborting merge.")
        return

    print("Parsing CUTRICER histogram file...")
    hist_df = get_cutracer_hist_df(cutracer_hist_path)
    if hist_df is None:
        print("Failed to parse CUTRICER histogram. Aborting merge.")
        return

    # Calculate global_warp_id in the chrome trace data
    # For 3D grids, we need to check if cta is already linearized or if we need to linearize it
    # Based on your grid (1, 528, 1), it seems cta is already a linear block ID
    # global_warp_id = block_linear_id * warps_per_block + local_warp_id

    print("Analyzing CTA ID distribution...")
    print(f"CTA ID range: {chrome_df['cta'].min()} to {chrome_df['cta'].max()}")
    total_blocks = grid_size[0] * grid_size[1] * grid_size[2]
    print(f"Expected total blocks from grid size: {total_blocks}")

    # If cta IDs are 1-based and linearized, we need to convert to 0-based
    # If cta IDs start from a different base, we need to adjust accordingly
    chrome_df["global_warp_id"] = (
        chrome_df["cta"] * warps_per_block + chrome_df["local_warp_id"]
    )

    print(
        f"Global warp ID range: {chrome_df['global_warp_id'].min()} to {chrome_df['global_warp_id'].max()}"
    )

    print("Validating warp coverage...")
    is_valid = validate_warp_coverage(chrome_df, hist_df)
    if not is_valid:
        print("WARNING: Proceeding with merge despite warp ID mismatches.")
        print("         This may result in missing data in the output.")

    print("Merging the dataframes...")
    # Merge all regions - create cross join between chrome trace events and histogram regions
    merged_df = pd.merge(chrome_df, hist_df, on="global_warp_id", how="left")

    # Reorder and select columns for clarity
    output_columns = [
        "core",
        "cta",
        "local_warp_id",
        "global_warp_id",
        "region_id",
        "name",
        "category",
        "cycles",
        "timestamp_ns",
        "total_instruction_count",
    ]

    # Add IPC calculation
    print("Calculating IPC (Instructions Per Cycle)...")
    merged_df["ipc"] = merged_df.apply(
        lambda row: calculate_ipc(row["cycles"], row["total_instruction_count"]), axis=1
    )
    output_columns.append("ipc")

    # Filter for columns that actually exist after the merge
    final_columns = [col for col in output_columns if col in merged_df.columns]
    final_df = merged_df[final_columns]

    # Sort by global_warp_id first, then by region_id for better organization
    print("Sorting output by global_warp_id, then region_id...")
    final_df = final_df.sort_values(
        ["global_warp_id", "region_id"], ascending=True
    ).reset_index(drop=True)

    final_df.to_csv(output_path, index=False)
    print(f"Successfully merged data and saved to {output_path}")


@click.command()
@click.option(
    "--chrome-trace",
    "chrome_trace_input",
    type=click.Path(exists=True),
    help="Path to the Chrome trace JSON file.",
)
@click.option(
    "--cutracer-trace",
    "cutracer_trace_input",
    type=click.Path(exists=True),
    help="Path to the CUTRICER histogram CSV file.",
)
@click.option(
    "--cutracer-log",
    "cutracer_log_input",
    type=click.Path(exists=True),
    help="Path to the CUTRICER log file to enable merge mode.",
)
@click.option(
    "--kernel-hash",
    "kernel_hash_hex",
    help="Optional kernel hash (e.g., 0x7fa21c3) to select a specific launch from the log.",
)
@click.option(
    "--output",
    required=True,
    type=click.Path(),
    help="Path for the output CSV file.",
)
def main(
    chrome_trace_input,
    cutracer_trace_input,
    cutracer_log_input,
    kernel_hash_hex,
    output,
):
    """Parse and merge trace files from Chrome's tracer and CUTRICER.

    Supports three modes:

    \b
    1. Merge mode (requires --cutracer-log, --chrome-trace, and --cutracer-trace):
       Merges Chrome trace, CUTRICER histogram, and log data.

    \b
    2. Chrome trace only (--chrome-trace):
       Parses a Chrome trace JSON file to CSV.

    \b
    3. CUTRICER histogram only (--cutracer-trace):
       Parses a CUTRICER histogram CSV file.
    """
    if cutracer_log_input:
        # Merge mode
        if not all([chrome_trace_input, cutracer_trace_input]):
            raise click.UsageError(
                "--cutracer-log requires --chrome-trace and --cutracer-trace."
            )

        merge_traces(
            chrome_trace_input,
            cutracer_trace_input,
            cutracer_log_input,
            output,
            kernel_hash_hex,
        )
    elif chrome_trace_input:
        # Standalone Chrome trace parsing
        df = get_chrome_trace_df(chrome_trace_input)
        if df is not None:
            df.to_csv(output, index=False)
            print(f"Successfully parsed Chrome trace and saved to {output}")
    elif cutracer_trace_input:
        # Standalone CUTRICER hist parsing
        df = get_cutracer_hist_df(cutracer_trace_input)
        if df is not None:
            df.to_csv(output, index=False)
            print(f"Successfully parsed CUTRICER histogram and saved to {output}")
    else:
        raise click.UsageError(
            "At least one of --chrome-trace, --cutracer-trace, or --cutracer-log is required."
        )


if __name__ == "__main__":
    main()

```

`src/analysis.cu`:

```cu
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-FileCopyrightText: Copyright (c) 2019 NVIDIA CORPORATION & AFFILIATES.
 * SPDX-License-Identifier: MIT AND BSD-3-Clause
 *
 * This source code contains modifications by Meta Platforms, Inc. licensed under MIT,
 * based on original NVIDIA nvbit sample code licensed under BSD-3-Clause.
 * See LICENSE file in the root directory for Meta's license terms.
 * See LICENSE-BSD file in the root directory for NVIDIA's license terms.
 */

#include <assert.h>
#include <pthread.h>
#include <signal.h>
#include <stdio.h>
#include <stdlib.h>

#include <chrono>
#include <map>
#include <string>
#include <unordered_map>
#include <unordered_set>

#include "analysis.h"
#include "common.h"
#include "cuda.h"
#include "env_config.h"
#include "log.h"
#include "trace_writer.h"
#include "utils/channel.hpp"

#define PC_HISTORY_LEN 32
#define LOOP_REPEAT_THRESH 3
// Throttle interval for hang checks (seconds)
#define HANG_CHECK_THROTTLE_SECS 1

extern pthread_mutex_t mutex;
extern std::unordered_map<CUcontext, CTXstate*> ctx_state_map;
extern std::map<uint64_t, std::pair<CUcontext, CUfunction>> kernel_launch_to_func_map;
extern std::map<uint64_t, uint32_t> kernel_launch_to_iter_map;
extern std::map<uint64_t, KernelDimensions> kernel_launch_to_dimensions_map;
extern std::unordered_map<CUfunction, KernelFuncMetadata> kernel_metadata_by_func;

// Forward declaration for helper defined below in this file
std::string extract_instruction_name(const std::string& sass_line);

/**
 * @brief Get current timestamp in nanoseconds.
 *
 * This helper function provides high-resolution timestamps for trace records,
 * allowing precise timing analysis of GPU execution.
 *
 * @return Current time since epoch in nanoseconds
 */
static inline uint64_t get_timestamp_ns() {
  auto now = std::chrono::high_resolution_clock::now();
  auto duration = now.time_since_epoch();
  return std::chrono::duration_cast<std::chrono::nanoseconds>(duration).count();
}

/**
 * Print one instruction line in the exact same format as loop body entries.
 *
 * Purpose:
 * - Provide a single-line renderer that matches the loop body formatting so
 *   different states (looping/barrier/progressing) can share the same
 *   presentation logic for an instruction row.
 *
 * Output format (aligned columns):
 *   Example: [idx] PC <dec>; Offset <dec> [0x<hex>];  <SASS> (has_mem)\n
 * - The trailing "(has_mem)" is printed only when the merged record has
 *   associated memory info.
 * - The index field width is currently fixed to 1 for consistency with our
 *   existing loop body output. Adjust if you later support wider indices.
 *
 * Parameters:
 * - index: The row index to print inside square brackets. For barrier/
 *   progressing single-line outputs, callers typically pass 0.
 * - instr: The merged trace record containing the reg info (and optional mem).
 * - sass_map_for_func: Per-function opcode->SASS map to resolve the mnemonic.
 *   If null or missing entry, prints "UNKNOWN" as fallback.
 * - pc_dec_width: Precomputed decimal width for PC/Offset alignment.
 * - hex_nibbles_max: Precomputed hex width (in nibbles), rounded to multiples
 *   of 4 and clamped to [4,16], used for the 0x... field.
 *
 * Notes:
 * - This function does not perform width computation. Callers should compute
 *   widths across the intended set of rows (entire loop body, or just the last
 *   single record) and pass them here to ensure consistent alignment.
 */
static inline void print_instruction_line(size_t index, const TraceRecordMerged& instr,
                                          const std::map<int, std::string>* sass_map_for_func, int pc_dec_width,
                                          int hex_nibbles_max) {
  uint64_t pc_val = instr.reg.pc;
  const char* sass_cstr = "UNKNOWN";
  if (sass_map_for_func && sass_map_for_func->count(instr.reg.opcode_id)) {
    sass_cstr = sass_map_for_func->at(instr.reg.opcode_id).c_str();
  }
  loprintf("        [%*zu] PC %*lu; Offset %*lu /*0x%0*lx*/;  %s", 1, index, pc_dec_width, (unsigned long)pc_val,
           pc_dec_width, (unsigned long)pc_val, hex_nibbles_max, (unsigned long)pc_val, sass_cstr);
  if (instr.has_mem) {
    loprintf(" (has_mem)");
  }
  loprintf("\n");
}

/**
 * Print the most recent instruction for a warp as a single line (loop-body style).
 *
 * Purpose:
 * - Provide a one-liner for BARRIER/PROGRESSING states that mirrors the loop
 *   body row format for visual consistency with LOOPING output.
 *
 * Behavior:
 * - Fetches the latest entry from the per-warp ring buffer
 *   ((head + filled - 1) % PC_HISTORY_LEN).
 * - Computes widths (decimal/hex) based solely on this last record and prints
 *   a single formatted line prefixed by "last: ".
 * - If there is no available entry (null state or empty), prints a line with
 *   UNKNOWN mnemonic and zero PC/Offset.
 *
 * Parameters:
 * - loop_state: Pointer to the warp's loop state holding the ring buffer.
 * - sass_map_for_func: Per-function opcode->SASS map to resolve the mnemonic.
 *
 * Notes:
 * - This function intentionally recomputes widths for the single last record.
 *   For multi-row loop bodies, prefer computing widths once across all rows and
 *   calling print_instruction_line_like_loop per row.
 */
static inline void print_last_instruction_line(const WarpLoopState* loop_state,
                                               const std::map<int, std::string>* sass_map_for_func) {
  uint64_t last_pc_val = 0;
  bool last_has_mem = false;
  const char* last_sass_cstr = "UNKNOWN";

  if (loop_state && loop_state->filled > 0 && sass_map_for_func) {
    int idx_last = (loop_state->head + (int)loop_state->filled + PC_HISTORY_LEN - 1) % PC_HISTORY_LEN;
    int opcode_last = loop_state->history[idx_last].reg.opcode_id;
    last_pc_val = loop_state->history[idx_last].reg.pc;
    last_has_mem = loop_state->history[idx_last].has_mem;
    if (sass_map_for_func->count(opcode_last)) {
      last_sass_cstr = sass_map_for_func->at(opcode_last).c_str();
    }
  }

  int pc_dec_width = 1;
  int hex_nibbles_max = 4;
  {
    uint64_t td = last_pc_val;
    int dec_w = 1;
    while (td >= 10) {
      td /= 10;
      dec_w++;
    }
    if (dec_w > pc_dec_width) pc_dec_width = dec_w;
    int nibbles = 1;
    if (last_pc_val != 0) {
      nibbles = 0;
      uint64_t th = last_pc_val;
      while (th) {
        th >>= 4;
        nibbles++;
      }
    }
    if (nibbles > hex_nibbles_max) hex_nibbles_max = nibbles;
    hex_nibbles_max = ((hex_nibbles_max + 3) / 4) * 4;
    if (hex_nibbles_max < 4) hex_nibbles_max = 4;
    if (hex_nibbles_max > 16) hex_nibbles_max = 16;
  }
  loprintf("      Last: [%*d] PC %*lu; Offset %*lu /*0x%0*lx*/;  %s", 1, 0, pc_dec_width, (unsigned long)last_pc_val,
           pc_dec_width, (unsigned long)last_pc_val, hex_nibbles_max, (unsigned long)last_pc_val, last_sass_cstr);
  if (last_has_mem) {
    loprintf(" (has_mem)");
  }
}

static inline bool matches_barrier_defer_blocking(const std::string& mnemonic) {
  if (mnemonic == "BAR.SYNC.DEFER_BLOCKING") return true;
  // Conservative fallback: prefix BAR.SYNC and contains .DEFER_BLOCKING
  if (mnemonic.rfind("BAR.SYNC", 0) == 0 && mnemonic.find(".DEFER_BLOCKING") != std::string::npos) return true;
  return false;
}

static bool is_barrier_defer_blocking_for_opcode(CTXstate* ctx_state, CUfunction func, int opcode_id) {
  if (!ctx_state) return false;
  if (!ctx_state->id_to_sass_map.count(func)) return false;
  const std::map<int, std::string>& sass_map = ctx_state->id_to_sass_map[func];
  std::map<int, std::string>::const_iterator it = sass_map.find(opcode_id);
  if (it == sass_map.end()) return false;
  const std::string& sass_line = it->second;
  std::string mnemonic = extract_instruction_name(sass_line);
  return matches_barrier_defer_blocking(mnemonic);
}

/**
 * @brief Extracts the full instruction mnemonic from a SASS line.
 *
 * This function parses a SASS instruction string to extract the mnemonic,
 * which includes the base instruction and any dot-separated modifiers
 * (e.g., "IMAD.MOV.U32"). It correctly handles and skips optional
 * predicates (e.g., "@!P0").
 *
 * @param sass_line The full SASS instruction line.
 * @return The extracted instruction mnemonic as a string.
 */
std::string extract_instruction_name(const std::string& sass_line) {
  // SASS format examples:
  // CS2R.32 R7, SR_CLOCKLO ;
  // @!P0 IMAD.MOV.U32 R6, RZ, RZ, 0x800000 ;

  size_t start_pos = 0;

  // Skip whitespace
  while (start_pos < sass_line.length() && isspace(sass_line[start_pos])) {
    start_pos++;
  }

  // Skip predicate if present (starts with @)
  if (start_pos < sass_line.length() && sass_line[start_pos] == '@') {
    // Find the end of predicate part (next space)
    while (start_pos < sass_line.length() && !isspace(sass_line[start_pos])) {
      start_pos++;
    }
    // Skip whitespace after predicate
    while (start_pos < sass_line.length() && isspace(sass_line[start_pos])) {
      start_pos++;
    }
  }

  // Extract instruction name (until first space)
  size_t end_pos = start_pos;
  while (end_pos < sass_line.length() && !isspace(sass_line[end_pos])) {
    end_pos++;
  }

  if (start_pos >= sass_line.length() || end_pos <= start_pos) {
    return "UNKNOWN";
  }

  return sass_line.substr(start_pos, end_pos - start_pos);
}

/**
 * @brief Processes a single instruction packet for histogram analysis.
 *
 * This function is the core of the instruction histogram feature. It uses
 * special "clock" instructions (generated by `pl.scope`) as markers to define
 * regions of interest.
 *
 * The logic operates in a start/stop fashion:
 * - The first clock instruction encountered by a warp starts the collection.
 * - The second clock instruction stops the collection and saves the histogram for
 *   the completed region.
 * - The third starts a new region, the fourth stops it, and so on.
 *
 * @warning This start/stop model does not support nested `pl.scope` blocks.
 * A nested scope will be flattened into a single sequence of start/stop
 * markers, which may lead to unintended region definitions.
 *
 * @param ri Pointer to the received opcode data packet (`opcode_only_t`).
 * @param ctx_state Pointer to the state for the current CUDA context.
 * @param warp_states A map tracking the collection state of each warp.
 * @param completed_histograms A vector where histograms of completed regions are
 * stored.
 */
void process_instruction_histogram(const opcode_only_t* ri, CTXstate* ctx_state,
                                   std::unordered_map<int, WarpState>& warp_states,
                                   std::vector<RegionHistogram>& completed_histograms) {
  // Get current function from kernel launch ID to find the correct SASS maps.
  std::map<uint64_t, std::pair<CUcontext, CUfunction>>::iterator func_iter =
      kernel_launch_to_func_map.find(ri->kernel_launch_id);
  if (func_iter == kernel_launch_to_func_map.end()) {
    return;  // Unknown kernel, skip histogram processing
  }

  CUfunction current_func = func_iter->second.second;

  // Get clock opcode IDs for this function, which mark region boundaries.
  const std::unordered_set<int>* clock_opcode_ids = nullptr;
  if (ctx_state->clock_opcode_ids.count(current_func)) {
    clock_opcode_ids = &ctx_state->clock_opcode_ids.at(current_func);
  }

  // Get SASS mapping for this function
  const std::map<int, std::string>* sass_map_for_func = nullptr;
  if (ctx_state->id_to_sass_map.count(current_func)) {
    sass_map_for_func = &ctx_state->id_to_sass_map.at(current_func);
  }

  if (!clock_opcode_ids || !sass_map_for_func) {
    return;  // No SASS or clock instruction mapping available for this function.
  }

  int warp_id = ri->warp_id;
  WarpState& current_state = warp_states[warp_id];
  bool is_clock_instruction = clock_opcode_ids->count(ri->opcode_id) > 0;

  // This block implements the start/stop logic for regions.
  if (is_clock_instruction) {
    if (current_state.is_collecting) {
      // This is an "end" clock: the region is complete.
      if (!current_state.histogram.empty()) {
        // Save the completed histogram.
        completed_histograms.push_back({warp_id, current_state.region_counter, current_state.histogram});
        current_state.histogram.clear();
        current_state.region_counter++;
      }
      // Stop collecting until the next "start" clock is found.
      current_state.is_collecting = false;
    } else {
      // This is a "start" clock: begin collecting instructions.
      current_state.is_collecting = true;
    }
  }

  // If collection is active, record the current instruction.
  if (current_state.is_collecting && sass_map_for_func->count(ri->opcode_id)) {
    // Extract the base instruction name from the full SASS string.
    const std::string& sass_line = sass_map_for_func->at(ri->opcode_id);
    std::string instruction_name = extract_instruction_name(sass_line);
    current_state.histogram[instruction_name]++;
  }
}

/**
 * @brief Dumps the collected histograms for a completed kernel launch to a file.
 *
 * This function is called when a kernel boundary is detected (i.e., when a new
 * kernel_launch_id is seen). It collates all histograms from the *previous*
 * kernel launch and triggers the process to write them to a CSV file.
 *
 * @param kernel_launch_id The ID of the kernel launch that has just finished.
 * @param histograms A vector containing all the completed region histograms for
 * that kernel.
 */
void dump_previous_kernel_data(uint64_t kernel_launch_id, const std::vector<RegionHistogram>& histograms) {
  if (histograms.empty()) {
    return;  // Nothing to dump.
  }

  // Find kernel info from global mapping
  if (kernel_launch_to_func_map.find(kernel_launch_id) != kernel_launch_to_func_map.end()) {
    auto [ctx, func] = kernel_launch_to_func_map[kernel_launch_id];
    uint32_t iteration = kernel_launch_to_iter_map[kernel_launch_id];

    // Use existing CSV generation logic.
    dump_histograms_to_csv(ctx, func, iteration, histograms);

    // Clean up mapping tables to free memory for subsequent kernels.
    kernel_launch_to_func_map.erase(kernel_launch_id);
    kernel_launch_to_iter_map.erase(kernel_launch_id);
    kernel_launch_to_dimensions_map.erase(kernel_launch_id);
  }
}

/**
 * @brief Writes a set of histograms to a formatted CSV file.
 *
 * This function handles the file I/O for persisting the analysis results. It
 * creates a uniquely named CSV file for a given kernel launch and writes the
 * histogram data in a structured format.
 *
 * @param ctx The CUDA context of the kernel.
 * @param func The kernel function.
 * @param iteration The iteration number of the kernel launch.
 * @param histograms The histogram data to be written to the file.
 */
void dump_histograms_to_csv(CUcontext ctx, CUfunction func, uint32_t iteration,
                            const std::vector<RegionHistogram>& histograms) {
  if (histograms.empty()) {
    return;  // Nothing to dump.
  }

  std::string checksum;
  auto meta_it = kernel_metadata_by_func.find(func);
  if (meta_it != kernel_metadata_by_func.end()) {
    checksum = meta_it->second.kernel_checksum;
  }

  std::string basename = generate_kernel_log_basename(ctx, func, iteration, checksum);
  std::string csv_filename = basename + "_hist.csv";

  FILE* fp = fopen(csv_filename.c_str(), "w");
  if (!fp) {
    oprintf("ERROR: Could not open histogram file %s\n", csv_filename.c_str());
    return;
  }

  // Header for the CSV file.
  fprintf(fp, "warp_id,region_id,instruction,count\n");
  // Iterate through each completed region and write its histogram data.
  for (const RegionHistogram& region_result : histograms) {
    for (const std::pair<const std::string, int>& pair : region_result.histogram) {
      const std::string& instruction_name = pair.first;
      int count = pair.second;
      fprintf(fp, "%d,%d,\"%s\",%d\n", region_result.warp_id, region_result.region_id, instruction_name.c_str(), count);
    }
  }
  fclose(fp);
  loprintf("Histogram data dumped to %s\n", csv_filename.c_str());
}

/**
 * @brief Extract kernel launch ID from different message types
 *
 * This helper function provides a unified interface to retrieve the kernel_launch_id
 * field from various message structures. It's used for kernel boundary detection
 * to determine when processing transitions from one CUDA kernel to another.
 *
 * @param header Pointer to the message header containing the message type
 * @return The kernel launch ID for the message, or 0 if the message type is unknown
 */
static uint64_t get_kernel_launch_id(const message_header_t* header) {
  switch (header->type) {
    case MSG_TYPE_REG_INFO:
      return ((const reg_info_t*)header)->kernel_launch_id;
    case MSG_TYPE_OPCODE_ONLY:
      return ((const opcode_only_t*)header)->kernel_launch_id;
    case MSG_TYPE_MEM_ADDR_ACCESS:
      return ((const mem_addr_access_t*)header)->kernel_launch_id;
    case MSG_TYPE_MEM_VALUE_ACCESS:
      return ((const mem_value_access_t*)header)->kernel_launch_id;
    case MSG_TYPE_TMA_ACCESS:
      return ((const tma_access_t*)header)->kernel_launch_id;
    default:
      return 0;  // Invalid/unknown message type - no kernel ID available
  }
}

/**
 * @brief Computes a canonical signature for a sequence of PCs to detect loops.
 *
 * This function analyzes the recent history of Program Counters (PCs) for a warp
 * to identify repeating patterns, which indicate a loop. The process involves:
 * 1.  **Period Detection**: It finds the shortest repeating sequence of PCs in
 *     the history buffer. If no repeating pattern is found, it returns 0.
 * 2.  **Canonicalization**: To ensure that the same loop produces the same
 *     signature regardless of the entry point, it finds the lexicographically
 *     smallest rotation of the detected period. For example, `[3,1,2]` becomes
 *     `[1,2,3]`.
 * 3.  **Hashing**: It computes an FNV-1a hash of the canonical sequence, seeded
 *     with the period length, to produce the final signature.
 *
 * @param history A constant reference to the ring buffer of merged trace records.
 * @param ring_size The total size of the ring buffer (must be `PC_HISTORY_LEN`).
 * @param head The index of the oldest element in the ring buffer.
 * @param out_period A reference to a `uint8_t` that will be set to the detected
 *                   period length. If no period is found, it's set to 0.
 * @return A 64-bit canonical signature of the loop, or 0 if no loop is detected.
 */
static uint64_t compute_canonical_signature(const std::vector<TraceRecordMerged>& history, int ring_size, uint8_t head,
                                            uint8_t& out_period) {
  // Reconstruct linear PC sequence in chronological order (oldest -> newest).
  uint64_t pcs[PC_HISTORY_LEN];
  for (int i = 0; i < ring_size; ++i) {
    int idx = (head + i) % ring_size;  // head points to oldest element position
    pcs[i] = history[idx].reg.pc;
  }

  // Detect the shortest repeating period p (1..N-1) such that pcs[i] == pcs[i-p]
  uint8_t period = 0;
  for (uint8_t p = 1; p < ring_size; ++p) {
    bool match = true;
    for (uint8_t i = p; i < ring_size; ++i) {
      if (pcs[i] != pcs[i - p]) {
        match = false;
        break;
      }
    }
    if (match) {
      period = p;
      break;
    }
  }
  if (period == 0) {
    out_period = 0;
    return 0;
  }

  // Find minimal rotation of the period segment to get canonical representation
  // Build candidate of length period from the first period entries
  uint64_t seg[PC_HISTORY_LEN];
  for (uint8_t i = 0; i < period; ++i) seg[i] = pcs[i];
  uint8_t min_rot = 0;
  for (uint8_t r = 1; r < period; ++r) {
    // Compare rotation r with current min_rot lexicographically
    bool smaller = false;
    for (uint8_t i = 0; i < period; ++i) {
      uint64_t a = seg[(i + r) % period];
      uint64_t b = seg[(i + min_rot) % period];
      if (a == b) continue;
      if (a < b) smaller = true;
      break;
    }
    if (smaller) min_rot = r;
  }

  // FNV-1a style hash seeded by period for better distribution
  const uint64_t FNV_OFFSET = 14695981039346656037ULL;
  const uint64_t FNV_PRIME = 1099511628211ULL;
  uint64_t h = FNV_OFFSET ^ period;
  for (uint8_t i = 0; i < period; ++i) {
    uint64_t pcv = seg[(i + min_rot) % period];
    h = (h ^ pcv) * FNV_PRIME;
  }
  out_period = period;
  return h;
}

/**
 * @brief Updates the loop detection state for a given warp.
 *
 * This function is the core of the host-side loop detection logic. It maintains
 * a history of instructions for each warp and uses it to detect when a warp
 * enters a stable loop.
 *
 * The process for each new instruction is as follows:
 * 1.  **History Update**: The new instruction record (`reg_info_t`) is added to
 *     the warp's ring buffer. The function also tries to match it with any
 *     pending memory access records for the same instruction.
 * 2.  **Signature Calculation**: Once the history buffer is full, it calls
 *     `compute_canonical_signature` to get a signature of the current PC sequence.
 * 3.  **Loop State Tracking**:
 *     - If the new signature and period match the previous one, a `repeat_cnt`
 *       is incremented.
 *     - If they don't match, the counter is reset.
 *     - When `repeat_cnt` exceeds `LOOP_REPEAT_THRESH`, the warp is officially
 *       considered to be in a loop (`loop_flag` is set to true), and the loop
 *       body (one full period) is captured and stored.
 *
 * @param ctx_state Pointer to the state for the current CUDA context.
 * @param key The `WarpKey` identifying the warp to be updated.
 * @param ri Pointer to the `reg_info_t` packet for the current instruction.
 */
static void update_loop_state(CTXstate* ctx_state, const WarpKey& key, const reg_info_t* ri) {
  WarpLoopState& state = ctx_state->loop_states[key];
  // One-time buffer allocation per warp
  if (state.history.size() != PC_HISTORY_LEN) {
    state.history.assign(PC_HISTORY_LEN, TraceRecordMerged());
    state.head = 0;
    state.filled = 0;
  }

  // Write the incoming reg record into ring buffer
  TraceRecordMerged& slot = state.history[state.head];
  slot.reg = *ri;
  slot.has_mem = false;  // will be flipped if we find a matching pending mem
  memset(slot.mem_addrs, 0, sizeof(slot.mem_addrs));

  // Try to match any pending mem for this warp (mem may arrive before reg)
  auto& pending = ctx_state->pending_mem_by_warp[key];
  if (!pending.empty()) {
    // Find first matching mem by pc/opcode
    for (auto it = pending.begin(); it != pending.end(); ++it) {
      if (it->pc == ri->pc && it->opcode_id == ri->opcode_id) {
        slot.has_mem = true;
        memcpy(slot.mem_addrs, it->addrs, sizeof(slot.mem_addrs));
        pending.erase(it);
        break;
      }
    }
  }

  // Advance ring pointers
  state.head = (uint8_t)((state.head + 1) % PC_HISTORY_LEN);
  if (state.filled < PC_HISTORY_LEN) state.filled++;

  // Only check for loops once the history buffer is full
  if (state.filled < PC_HISTORY_LEN) {
    return;
  }

  // Compute canonical signature and period from the ring buffer
  uint8_t period = 0;
  uint64_t current_sig = compute_canonical_signature(state.history, PC_HISTORY_LEN, state.head, period);

  if (current_sig != 0 && current_sig == state.last_sig && period == state.last_period) {
    state.repeat_cnt++;
  } else {
    state.repeat_cnt = 1;  // current observed once
    state.loop_flag = false;
  }

  if (state.repeat_cnt > LOOP_REPEAT_THRESH) {
    if (!state.loop_flag) {
      state.loop_flag = true;
      state.first_loop_time = time(nullptr);
      // Capture the loop body records (one period) from the ring buffer in chronological order
      state.current_loop.period = period;
      state.current_loop.instructions.clear();
      state.current_loop.instructions.reserve(period);
      // head points to oldest, so sequence starts from head index
      for (uint8_t i = 0; i < period; ++i) {
        int idx = (state.head + i) % PC_HISTORY_LEN;
        state.current_loop.instructions.push_back(state.history[idx]);
      }
    }
  }
  state.last_sig = current_sig;
  state.last_period = period;
}

/**
 * @brief Clears all state related to deadlock and hang detection.
 *
 * This function is called at the boundary of a new kernel launch to ensure that
 * the state from the previous kernel does not interfere with the analysis of the
 * new one. It clears all maps and sets that track warp activity, loop states,
 * and pending memory operations.
 *
 * @param ctx_state Pointer to the state for the current CUDA context.
 * @param kernel_launch_id The kernel launch ID to clear tracking data for (0 to skip).
 */
static void clear_deadlock_state(CTXstate* ctx_state, uint64_t kernel_launch_id = 0) {
  ctx_state->loop_states.clear();
  ctx_state->active_warps.clear();
  ctx_state->pending_mem_by_warp.clear();
  ctx_state->last_seen_time_by_warp.clear();
  ctx_state->exit_candidate_since_by_warp.clear();
  ctx_state->last_is_defer_blocking_by_warp.clear();

  // Clear kernel warp tracking for the specified kernel
  if (kernel_launch_id != 0) {
    ctx_state->kernel_warp_tracking.erase(kernel_launch_id);
  }
}

/**
 * @brief Prints detailed status information for all active warps including loop states.
 *
 * This function provides a comprehensive view of each warp's current state including:
 * - Basic warp identification (CTA coordinates, warp ID)
 * - Loop detection status (whether in loop, loop period, repeat count)
 * - Activity timestamps and exit candidate status
 * - Loop body instruction details if available
 *
 * @param ctx_state Pointer to the state for the current CUDA context
 * @param current_kernel_launch_id The current kernel launch ID for context
 */
static void print_warp_status_summary(CTXstate* ctx_state, uint64_t current_kernel_launch_id) {
  time_t now = time(nullptr);

  // Print warp statistics if available
  if (ctx_state->kernel_warp_tracking.count(current_kernel_launch_id)) {
    const KernelWarpStats& stats = ctx_state->kernel_warp_tracking[current_kernel_launch_id];
    size_t total_warps = stats.total_warps;
    size_t seen_warps = stats.all_seen_warps.size();
    size_t finished_warps = stats.finished_warps.size();
    size_t active_warps = ctx_state->active_warps.size();

    // Collect warp IDs in different categories
    std::set<int> finished_warp_ids;
    std::set<int> active_warp_ids;
    std::set<int> never_executed_warp_ids;

    for (const WarpKey& key : stats.finished_warps) {
      finished_warp_ids.insert(key.warp_id);
    }
    for (const WarpKey& key : ctx_state->active_warps) {
      active_warp_ids.insert(key.warp_id);
    }

    // Calculate never executed warps (all possible warp IDs minus those we've seen)
    std::set<int> all_seen_warp_ids;
    for (const WarpKey& key : stats.all_seen_warps) {
      all_seen_warp_ids.insert(key.warp_id);
    }

    // Determine max warp ID based on total warps
    for (uint32_t wid = 0; wid < total_warps; wid++) {
      if (all_seen_warp_ids.count(wid) == 0) {
        never_executed_warp_ids.insert(wid);
      }
    }

    size_t never_executed = never_executed_warp_ids.size();

    loprintf("==> WARP STATISTICS for kernel_launch_id=%lu:\n", current_kernel_launch_id);
    loprintf("    Grid: <%u,%u,%u>, Block: <%u,%u,%u>\n", stats.dimensions.gridDimX, stats.dimensions.gridDimY,
             stats.dimensions.gridDimZ, stats.dimensions.blockDimX, stats.dimensions.blockDimY,
             stats.dimensions.blockDimZ);
    loprintf("\n");
    loprintf("    Summary:\n");
    loprintf("      Total warps:           %5zu (100.0%%)\n", total_warps);
    loprintf("      Finished warps:        %5zu (%5.1f%%)\n", finished_warps,
             total_warps > 0 ? 100.0 * finished_warps / total_warps : 0.0);
    loprintf("      Active warps:          %5zu (%5.1f%%)\n", active_warps,
             total_warps > 0 ? 100.0 * active_warps / total_warps : 0.0);
    loprintf("      Never executed warps:  %5zu (%5.1f%%)\n", never_executed,
             total_warps > 0 ? 100.0 * never_executed / total_warps : 0.0);

    // Helper lambda to format ranges from a set of IDs
    auto format_ranges = [](const std::set<int>& ids) -> std::string {
      if (ids.empty()) return "none";

      std::string result;
      auto it = ids.begin();
      int range_start = *it;
      int range_end = *it;

      for (++it; it != ids.end(); ++it) {
        if (*it == range_end + 1) {
          // Continue current range
          range_end = *it;
        } else {
          // End current range and start new one
          if (!result.empty()) result += ", ";
          if (range_start == range_end) {
            result += std::to_string(range_start);
          } else {
            result += std::to_string(range_start) + "-" + std::to_string(range_end);
          }
          range_start = range_end = *it;
        }
      }

      // Add final range
      if (!result.empty()) result += ", ";
      if (range_start == range_end) {
        result += std::to_string(range_start);
      } else {
        result += std::to_string(range_start) + "-" + std::to_string(range_end);
      }

      return result;
    };

    loprintf("\n");
    loprintf("    Warp ID Ranges:\n");
    loprintf("      Finished:       %s\n", format_ranges(finished_warp_ids).c_str());
    loprintf("      Active:         %s\n", format_ranges(active_warp_ids).c_str());
    loprintf("      Never executed: %s\n", format_ranges(never_executed_warp_ids).c_str());

    loprintf("    -----------------------------------------------------------------------\n");
  }

  if (ctx_state->active_warps.empty()) {
    loprintf("==> WARP STATUS: No active warps for kernel_launch_id=%lu\n", current_kernel_launch_id);
    return;
  }

  loprintf("==> WARP STATUS SUMMARY for kernel_launch_id=%lu (%zu active warps):\n", current_kernel_launch_id,
           ctx_state->active_warps.size());
  loprintf("    Format: WarpID[CTA_x,y,z] - LoopStatus - Activity\n");
  loprintf("    -----------------------------------------------------------------------\n");

  // Resolve SASS map for the current function, if available
  const std::map<int, std::string>* sass_map_for_func = nullptr;
  {
    std::map<uint64_t, std::pair<CUcontext, CUfunction>>::iterator func_iter =
        kernel_launch_to_func_map.find(current_kernel_launch_id);
    if (func_iter != kernel_launch_to_func_map.end()) {
      CUfunction f_func = func_iter->second.second;
      if (ctx_state->id_to_sass_map.count(f_func)) {
        sass_map_for_func = &ctx_state->id_to_sass_map[f_func];
      }
    }
  }

  for (const auto& warp_key : ctx_state->active_warps) {
    // Basic warp info
    loprintf("    Warp%d[%d,%d,%d]: ", warp_key.warp_id, warp_key.cta_id_x, warp_key.cta_id_y, warp_key.cta_id_z);

    // Loop state info
    auto loop_iter = ctx_state->loop_states.find(warp_key);
    bool is_looping = false;
    if (loop_iter != ctx_state->loop_states.end()) {
      const WarpLoopState& loop_state = loop_iter->second;
      if (loop_state.loop_flag) {
        is_looping = true;
        time_t last_seen_secs = 0;
        auto seen_it2 = ctx_state->last_seen_time_by_warp.find(warp_key);
        if (seen_it2 != ctx_state->last_seen_time_by_warp.end()) {
          last_seen_secs = now - seen_it2->second;
        }
        loprintf("LOOPING(period=%d, repeat=%d) last_seen=%lds ", loop_state.last_period, loop_state.repeat_cnt,
                 last_seen_secs);
      }
    }

    // Activity info
    time_t inactive_duration = 0;
    auto seen_iter = ctx_state->last_seen_time_by_warp.find(warp_key);
    if (seen_iter != ctx_state->last_seen_time_by_warp.end()) {
      inactive_duration = now - seen_iter->second;
    }

    if (!is_looping) {
      // If not looping, distinguish barrier vs progressing by last_is_defer_blocking_by_warp
      bool is_barrier = false;
      auto itBar = ctx_state->last_is_defer_blocking_by_warp.find(warp_key);
      if (itBar != ctx_state->last_is_defer_blocking_by_warp.end()) {
        is_barrier = itBar->second;
      }

      // Use unified single-line printer for last instruction; no temporary variables needed here

      if (is_barrier) {
        // Barrier category (last observed instruction is BAR.SYNC.DEFER_BLOCKING)
        // Include inactivity seconds for quick assessment
        int period_val = 0;
        int repeat_val = 0;
        if (loop_iter != ctx_state->loop_states.end()) {
          period_val = loop_iter->second.last_period;
          repeat_val = loop_iter->second.repeat_cnt;
        }
        loprintf("BARRIER(inactive=%lds) no_loop(period=%d, repeat=%d)\n", inactive_duration, period_val, repeat_val);
        print_last_instruction_line(loop_iter != ctx_state->loop_states.end() ? &loop_iter->second : nullptr,
                                    sass_map_for_func);
      } else {
        // Progressing category
        int period_val = 0;
        int repeat_val = 0;
        if (loop_iter != ctx_state->loop_states.end()) {
          period_val = loop_iter->second.last_period;
          repeat_val = loop_iter->second.repeat_cnt;
        }
        loprintf("PROGRESSING no_loop(period=%d, repeat=%d)\n", period_val, repeat_val);
        print_last_instruction_line(loop_iter != ctx_state->loop_states.end() ? &loop_iter->second : nullptr,
                                    sass_map_for_func);
      }
    }

    // Exit candidate status
    auto exit_iter = ctx_state->exit_candidate_since_by_warp.find(warp_key);
    if (exit_iter != ctx_state->exit_candidate_since_by_warp.end()) {
      time_t exit_duration = now - exit_iter->second;
      loprintf("- EXIT_CANDIDATE(%lds)", exit_duration);
    }

    loprintf("\n");

    // Print loop body details if warp is in a confirmed loop
    if (loop_iter != ctx_state->loop_states.end() && loop_iter->second.loop_flag) {
      const WarpLoopState& loop_state = loop_iter->second;
      if (!loop_state.current_loop.instructions.empty()) {
        loprintf("      Loop Body (%d instructions):\n", loop_state.current_loop.period);
        // Pre-compute alignment for PC/Offset columns across the loop body
        int pc_dec_width = 1;
        int hex_nibbles_max = 4;
        {
          size_t upper = std::min(static_cast<size_t>(loop_state.current_loop.period),
                                  loop_state.current_loop.instructions.size());
          for (size_t j = 0; j < upper; ++j) {
            uint64_t pc = loop_state.current_loop.instructions[j].reg.pc;
            int dec_w = 1;
            uint64_t td = pc;
            while (td >= 10) {
              td /= 10;
              dec_w++;
            }
            if (dec_w > pc_dec_width) pc_dec_width = dec_w;
            int nibbles = 1;
            if (pc != 0) {
              nibbles = 0;
              uint64_t th = pc;
              while (th) {
                th >>= 4;
                nibbles++;
              }
            }
            if (nibbles > hex_nibbles_max) hex_nibbles_max = nibbles;
          }
          hex_nibbles_max = ((hex_nibbles_max + 3) / 4) * 4;
          if (hex_nibbles_max < 4) hex_nibbles_max = 4;
          if (hex_nibbles_max > 16) hex_nibbles_max = 16;
        }
        for (size_t i = 0;
             i < static_cast<size_t>(loop_state.current_loop.period) && i < loop_state.current_loop.instructions.size();
             ++i) {
          const auto& instr = loop_state.current_loop.instructions[i];
          print_instruction_line(i, instr, sass_map_for_func, pc_dec_width, hex_nibbles_max);
        }
      }
    }
  }
  loprintf("    -----------------------------------------------------------------------\n");
}

// Checks for potential kernel hangs by determining if all active warps are stuck in loops.
// The check is throttled to run at most once every HANG_CHECK_THROTTLE_SECS.
// If all warps are looping and the condition persists for several checks, it terminates the process.
// Before checking, it prunes warps that are candidates for exiting and have been inactive.
static void check_kernel_hang(CTXstate* ctx_state, uint64_t current_kernel_launch_id) {
  time_t now = time(nullptr);
  if (now - ctx_state->last_hang_check_time < HANG_CHECK_THROTTLE_SECS) {  // Throttle to configured interval
    return;
  }
  ctx_state->last_hang_check_time = now;

  if (ctx_state->active_warps.empty()) {
    return;
  }

  // Cleanup exit-candidate warps before evaluating loop state
  std::vector<WarpKey> to_remove;
  to_remove.reserve(ctx_state->active_warps.size());
  for (const WarpKey& key : ctx_state->active_warps) {
    auto itExit = ctx_state->exit_candidate_since_by_warp.find(key);
    if (itExit == ctx_state->exit_candidate_since_by_warp.end()) continue;
    time_t exit_since = itExit->second;
    time_t last_seen = 0;
    auto itSeen = ctx_state->last_seen_time_by_warp.find(key);
    if (itSeen != ctx_state->last_seen_time_by_warp.end()) last_seen = itSeen->second;
    // Remove only if no activity since EXIT and at least one throttle interval has passed
    if (last_seen <= exit_since && (now - exit_since) >= HANG_CHECK_THROTTLE_SECS) {
      to_remove.push_back(key);
    }
  }
  if (!to_remove.empty()) {
    for (const WarpKey& key : to_remove) {
      // Track this warp as finished before removing it from active_warps
      if (ctx_state->kernel_warp_tracking.count(current_kernel_launch_id)) {
        ctx_state->kernel_warp_tracking[current_kernel_launch_id].finished_warps.insert(key);
      }

      ctx_state->active_warps.erase(key);
      ctx_state->loop_states.erase(key);
      ctx_state->pending_mem_by_warp.erase(key);
      ctx_state->last_seen_time_by_warp.erase(key);
      ctx_state->exit_candidate_since_by_warp.erase(key);
    }
  }

  size_t looping_cnt = 0;
  size_t barrier_cnt = 0;
  size_t progressing_cnt = 0;
  bool candidate_hang = false;
  for (const WarpKey& warp_key : ctx_state->active_warps) {
    bool is_looping = false;
    {
      std::map<WarpKey, WarpLoopState>::const_iterator it = ctx_state->loop_states.find(warp_key);
      if (it != ctx_state->loop_states.end() && it->second.loop_flag) is_looping = true;
    }

    bool is_barrier = false;
    {
      std::unordered_map<WarpKey, bool, WarpKey::Hash>::const_iterator itBar =
          ctx_state->last_is_defer_blocking_by_warp.find(warp_key);
      if (itBar != ctx_state->last_is_defer_blocking_by_warp.end()) is_barrier = itBar->second;
    }

    if (is_barrier)
      barrier_cnt++;
    else if (is_looping)
      looping_cnt++;
    else
      progressing_cnt++;
  }

  // Hang trigger: no warp is progressing â†’ kernel is effectively stalled.
  // We partition active warps into three mutually exclusive categories:
  //  - barrier: the last observed instruction is BAR.SYNC.DEFER_BLOCKING
  //  - looping: not barrier and loop_flag == true (stable PC cycle detected)
  //  - progressing: not barrier and loop_flag == false (still making forward progress)
  // If progressing_cnt == 0 (and there are active warps), the system is stalled.
  // This single condition covers the three intended hang scenarios:
  //  (1) All barrier: barrier_cnt == active_warps.size(), looping_cnt == 0, progressing_cnt == 0
  //  (2) Barrier + the rest looping: barrier_cnt > 0, looping_cnt > 0, progressing_cnt == 0
  //  (3) All looping: looping_cnt == active_warps.size(), barrier_cnt == 0, progressing_cnt == 0
  if (!ctx_state->active_warps.empty() && progressing_cnt == 0) {
    candidate_hang = true;
  }

  if (candidate_hang) {
    time_t hang_time = 0;
    if (!ctx_state->loop_states.empty()) {
      hang_time = now - ctx_state->loop_states.begin()->second.first_loop_time;
    }
    loprintf(
        "Possible kernel hang: launch_id=%lu â€” state(looping=%zu, barrier=%zu, progressing=%zu) for %ld seconds.\n",
        current_kernel_launch_id, looping_cnt, barrier_cnt, progressing_cnt, hang_time);
    print_warp_status_summary(ctx_state, current_kernel_launch_id);
    if (!ctx_state->deadlock_termination_initiated) {
      ctx_state->deadlock_consecutive_hits++;
      if (ctx_state->deadlock_consecutive_hits >= 3) {
        ctx_state->deadlock_termination_initiated = true;
        loprintf("Deadlock sustained for %d checks; sending SIGTERM.\n", ctx_state->deadlock_consecutive_hits);
        fflush(stdout);
        fflush(stderr);
        raise(SIGTERM);
        sleep(2);
        loprintf("Process still alive after SIGTERM; sending SIGKILL.\n");
        raise(SIGKILL);
      }
    }
  } else {
    ctx_state->deadlock_consecutive_hits = 0;
  }
}

/**
 * @brief The main thread function for receiving and processing data from the
 * GPU.
 *
 * This function is based on the `recv_thread_fun` from NVIDIA's `mem_trace`
 * example. It runs in a separate CPU thread for each CUDA context, continuously
 * receiving data packets from the GPU channel and processing them.
 *
 * Meta's enhancements transform this from a simple single-purpose function to a
 * versatile multi-analysis pipeline:
 *  - **Generic Message-Passing System**: The original function only handled one
 *    data type (`mem_addr_access_t`). This version uses a `message_header_t` to
 *    identify different packet types (`reg_info_t`, `opcode_only_t`, etc.) and
 *    dispatch them to the appropriate analysis logic.
 *  - **Instruction Histogram Analysis**: It contains the complete host-side logic
 *    for the `PROTON_INSTR_HISTOGRAM` feature, including state management for
 *    each warp (`warp_states`) and tracking completed regions.
 *  - **Kernel Boundary Detection**: It introduces robust state management across
 *    kernel launches by tracking `kernel_launch_id`. This allows it to detect
 *    when a kernel has finished, ensuring that all pending data for that kernel
 *    is finalized and dumped before processing the next one.
 *  - **SASS String Enrichment**: For richer logging, it looks up the SASS string
 *    for a given `opcode_id` to provide more context in the trace output.
 *
 * @param args A void pointer to the `CUcontext` for which this thread is
 * launched.
 * @return void* Always returns NULL.
 */
void* recv_thread_fun(void* args) {
  CUcontext ctx = (CUcontext)args;

  pthread_mutex_lock(&mutex);
  /* get context state from map */
  assert(ctx_state_map.find(ctx) != ctx_state_map.end());
  CTXstate* ctx_state = ctx_state_map[ctx];

  ChannelHost* ch_host = &ctx_state->channel_host;
  pthread_mutex_unlock(&mutex);
  char* recv_buffer = (char*)malloc(CHANNEL_SIZE);

  // Per-thread, per-context state for histogram analysis.
  std::unordered_map<int, WarpState> warp_states;
  std::vector<RegionHistogram> local_completed_histograms;

  // Used to detect when a new kernel begins.
  uint64_t last_seen_kernel_launch_id = UINT64_MAX;  // Initial invalid value

  while (ctx_state->recv_thread_done == RecvThreadState::WORKING) {
    uint32_t num_recv_bytes = ch_host->recv(recv_buffer, CHANNEL_SIZE);

    if (num_recv_bytes > 0) {
      // Process data packets in this chunk

      uint32_t num_processed_bytes = 0;
      while (num_processed_bytes < num_recv_bytes) {
        // First read the message header to determine the message type
        message_header_t* header = (message_header_t*)&recv_buffer[num_processed_bytes];

        uint64_t current_launch_id = get_kernel_launch_id(header);
        bool is_new_kernel = false;
        if (current_launch_id != 0 && current_launch_id != last_seen_kernel_launch_id) {
          is_new_kernel = true;
          if (last_seen_kernel_launch_id != UINT64_MAX) {
            // Cleanup for the previous kernel
            if (is_analysis_type_enabled(AnalysisType::PROTON_INSTR_HISTOGRAM)) {
              // Dump any remaining histograms for warps that were collecting
              for (auto& pair : warp_states) {
                if (pair.second.is_collecting && !pair.second.histogram.empty()) {
                  local_completed_histograms.push_back({pair.first, pair.second.region_counter, pair.second.histogram});
                }
              }
              dump_previous_kernel_data(last_seen_kernel_launch_id, local_completed_histograms);
              local_completed_histograms.clear();
              warp_states.clear();
            }
            if (is_analysis_type_enabled(AnalysisType::DEADLOCK_DETECTION)) {
              clear_deadlock_state(ctx_state, last_seen_kernel_launch_id);
            }

            // Close the previous kernel's TraceWriter (per-launch file)
            // Since kernels are serialized and channel is FIFO, all data for
            // last_seen_kernel_launch_id has been processed when we see a new launch_id
            {
              std::unique_lock<std::shared_mutex> lock(ctx_state->writers_mutex);
              auto it = ctx_state->trace_writers.find(last_seen_kernel_launch_id);
              if (it != ctx_state->trace_writers.end() && it->second) {
                it->second->flush();
                delete it->second;
                ctx_state->trace_writers.erase(it);
                loprintf_v("Closed TraceWriter for launch_id %lu\n", last_seen_kernel_launch_id);
              }
            }
          }
          last_seen_kernel_launch_id = current_launch_id;

          // Initialize kernel warp tracking for the new kernel
          if (is_analysis_type_enabled(AnalysisType::DEADLOCK_DETECTION)) {
            if (kernel_launch_to_dimensions_map.count(current_launch_id)) {
              KernelDimensions& dims = kernel_launch_to_dimensions_map[current_launch_id];
              KernelWarpStats& stats = ctx_state->kernel_warp_tracking[current_launch_id];
              stats.dimensions = dims;

              // Calculate total warps: grid_size * warps_per_block
              // Each block has (blockDim.x * blockDim.y * blockDim.z + 31) / 32 warps
              uint32_t threads_per_block = dims.blockDimX * dims.blockDimY * dims.blockDimZ;
              uint32_t warps_per_block = (threads_per_block + 31) / 32;
              uint32_t total_blocks = dims.gridDimX * dims.gridDimY * dims.gridDimZ;
              stats.total_warps = total_blocks * warps_per_block;
            }
          }
        }

        if (header->type == MSG_TYPE_REG_INFO) {
          reg_info_t* ri = (reg_info_t*)&recv_buffer[num_processed_bytes];

          if (is_analysis_type_enabled(AnalysisType::DEADLOCK_DETECTION)) {
            WarpKey key = {ri->cta_id_x, ri->cta_id_y, ri->cta_id_z, ri->warp_id};
            if (is_new_kernel) {
              ctx_state->last_hang_check_time = time(nullptr);
            }
            ctx_state->active_warps.insert(key);
            // Update last seen time for this warp
            ctx_state->last_seen_time_by_warp[key] = time(nullptr);

            // Track this warp in the kernel's all_seen_warps set
            if (ctx_state->kernel_warp_tracking.count(ri->kernel_launch_id)) {
              ctx_state->kernel_warp_tracking[ri->kernel_launch_id].all_seen_warps.insert(key);
            }

            update_loop_state(ctx_state, key, ri);

            // Determine if current instruction is BAR.SYNC.DEFER_BLOCKING
            bool is_barrier_defer = false;
            CUfunction f_func2 = nullptr;
            auto func_iter2 = kernel_launch_to_func_map.find(ri->kernel_launch_id);
            if (func_iter2 != kernel_launch_to_func_map.end()) {
              f_func2 = func_iter2->second.second;
              is_barrier_defer = is_barrier_defer_blocking_for_opcode(ctx_state, f_func2, ri->opcode_id);
            }
            ctx_state->last_is_defer_blocking_by_warp[key] = is_barrier_defer;

            // Mark EXIT candidate if this opcode_id is an EXIT for the current function
            if (func_iter2 != kernel_launch_to_func_map.end()) {
              if (ctx_state->exit_opcode_ids.count(f_func2) &&
                  ctx_state->exit_opcode_ids[f_func2].count(ri->opcode_id)) {
                if (!ctx_state->exit_candidate_since_by_warp.count(key)) {
                  ctx_state->exit_candidate_since_by_warp[key] = time(nullptr);
                }
              }
            }
          }
          // Get SASS string and register indices for trace output
          std::string sass_str_cpp;
          const RegIndices* reg_indices_ptr = nullptr;
          auto func_iter = kernel_launch_to_func_map.find(ri->kernel_launch_id);
          if (func_iter != kernel_launch_to_func_map.end()) {
            auto [f_ctx, f_func] = func_iter->second;
            if (ctx_state->id_to_sass_map.count(f_func) && ctx_state->id_to_sass_map[f_func].count(ri->opcode_id)) {
              sass_str_cpp = ctx_state->id_to_sass_map[f_func][ri->opcode_id];
            }
            // Lookup register indices (static data from instrumentation time)
            if (ctx_state->id_to_reg_indices_map.count(f_func) &&
                ctx_state->id_to_reg_indices_map[f_func].count(ri->opcode_id)) {
              reg_indices_ptr = &ctx_state->id_to_reg_indices_map[f_func][ri->opcode_id];
            }
          }

          // Unified trace output through TraceWriter for all modes (per-launch files)
          {
            std::shared_lock<std::shared_mutex> lock(ctx_state->writers_mutex);
            auto it = ctx_state->trace_writers.find(ri->kernel_launch_id);
            if (it != ctx_state->trace_writers.end() && it->second) {
              // Get trace_index (monotonically increasing per kernel)
              uint64_t trace_idx = ctx_state->trace_index_by_kernel[ri->kernel_launch_id]++;

              // Get timestamp
              uint64_t timestamp = get_timestamp_ns();

              // Create TraceRecord and write (mode 0/1/2 handled by TraceWriter)
              auto record = TraceRecord::create_reg_trace(ctx, sass_str_cpp, trace_idx, timestamp, ri, reg_indices_ptr);
              it->second->write_trace(record);
            }
          }
          num_processed_bytes += sizeof(reg_info_t);

        } else if (header->type == MSG_TYPE_OPCODE_ONLY) {
          if (is_analysis_type_enabled(AnalysisType::PROTON_INSTR_HISTOGRAM)) {
            opcode_only_t* oi = (opcode_only_t*)&recv_buffer[num_processed_bytes];

            process_instruction_histogram(oi, ctx_state, warp_states, local_completed_histograms);
          }
          num_processed_bytes += sizeof(opcode_only_t);

        } else if (header->type == MSG_TYPE_MEM_ADDR_ACCESS) {
          mem_addr_access_t* mem = (mem_addr_access_t*)&recv_buffer[num_processed_bytes];

          // Get SASS string for trace output
          std::string sass_str_cpp;
          std::map<uint64_t, std::pair<CUcontext, CUfunction>>::iterator func_iter =
              kernel_launch_to_func_map.find(mem->kernel_launch_id);
          if (func_iter != kernel_launch_to_func_map.end()) {
            std::pair<CUcontext, CUfunction> kernel_info = func_iter->second;
            CUfunction f_func = kernel_info.second;
            if (ctx_state->id_to_sass_map.count(f_func) && ctx_state->id_to_sass_map[f_func].count(mem->opcode_id)) {
              sass_str_cpp = ctx_state->id_to_sass_map[f_func][mem->opcode_id];
            }
          }

          // Unified TraceWriter output (per-launch files)
          {
            std::shared_lock<std::shared_mutex> lock(ctx_state->writers_mutex);
            auto it = ctx_state->trace_writers.find(mem->kernel_launch_id);
            if (it != ctx_state->trace_writers.end() && it->second) {
              uint64_t trace_idx = ctx_state->trace_index_by_kernel[mem->kernel_launch_id]++;
              uint64_t timestamp = get_timestamp_ns();
              auto record = TraceRecord::create_mem_trace(ctx, sass_str_cpp, trace_idx, timestamp, mem);
              it->second->write_trace(record);
            }
          }

          num_processed_bytes += sizeof(mem_addr_access_t);

        } else if (header->type == MSG_TYPE_MEM_VALUE_ACCESS) {
          mem_value_access_t* mem_value = (mem_value_access_t*)&recv_buffer[num_processed_bytes];

          // Get SASS string for trace output
          std::string sass_str_cpp;
          std::map<uint64_t, std::pair<CUcontext, CUfunction>>::iterator func_iter =
              kernel_launch_to_func_map.find(mem_value->kernel_launch_id);
          if (func_iter != kernel_launch_to_func_map.end()) {
            std::pair<CUcontext, CUfunction> kernel_info = func_iter->second;
            CUfunction f_func = kernel_info.second;
            if (ctx_state->id_to_sass_map.count(f_func) &&
                ctx_state->id_to_sass_map[f_func].count(mem_value->opcode_id)) {
              sass_str_cpp = ctx_state->id_to_sass_map[f_func][mem_value->opcode_id];
            }
          }

          // Unified TraceWriter output (per-launch files)
          {
            std::shared_lock<std::shared_mutex> lock(ctx_state->writers_mutex);
            auto it = ctx_state->trace_writers.find(mem_value->kernel_launch_id);
            if (it != ctx_state->trace_writers.end() && it->second) {
              uint64_t trace_idx = ctx_state->trace_index_by_kernel[mem_value->kernel_launch_id]++;
              uint64_t timestamp = get_timestamp_ns();
              auto record = TraceRecord::create_mem_value_trace(ctx, sass_str_cpp, trace_idx, timestamp, mem_value);
              it->second->write_trace(record);
            }
          }

          num_processed_bytes += sizeof(mem_value_access_t);
        } else if (header->type == MSG_TYPE_TMA_ACCESS) {
          tma_access_t* tma = (tma_access_t*)&recv_buffer[num_processed_bytes];

          // Get SASS string for trace output
          std::string sass_str_cpp;
          std::map<uint64_t, std::pair<CUcontext, CUfunction>>::iterator func_iter =
              kernel_launch_to_func_map.find(tma->kernel_launch_id);
          if (func_iter != kernel_launch_to_func_map.end()) {
            std::pair<CUcontext, CUfunction> kernel_info = func_iter->second;
            CUfunction f_func = kernel_info.second;
            if (ctx_state->id_to_sass_map.count(f_func) && ctx_state->id_to_sass_map[f_func].count(tma->opcode_id)) {
              sass_str_cpp = ctx_state->id_to_sass_map[f_func][tma->opcode_id];
            }
          }

          loprintf_v("TMA: cta=[%d,%d,%d] warp=%d pc=0x%lx %s desc_addr=0x%lx\n", tma->cta_id_x, tma->cta_id_y,
                     tma->cta_id_z, tma->warp_id, tma->pc,
                     (sass_str_cpp.find("UTMALDG") != std::string::npos) ? "LOAD" : "STORE", tma->desc_addr);

          // Unified TraceWriter output (per-launch files)
          {
            std::shared_lock<std::shared_mutex> lock(ctx_state->writers_mutex);
            auto it = ctx_state->trace_writers.find(tma->kernel_launch_id);
            if (it != ctx_state->trace_writers.end() && it->second) {
              uint64_t trace_idx = ctx_state->trace_index_by_kernel[tma->kernel_launch_id]++;
              uint64_t timestamp = get_timestamp_ns();
              auto record = TraceRecord::create_tma_trace(ctx, sass_str_cpp, trace_idx, timestamp, tma);
              it->second->write_trace(record);
            } else {
              loprintf("WARNING: No TraceWriter for TMA kernel_launch_id=%lu\n", tma->kernel_launch_id);
            }
          }

          num_processed_bytes += sizeof(tma_access_t);
        } else {
          // Unknown message type, print error and break loop
          // TODO: handle error message in our current log mechanism
          fprintf(stderr,
                  "ERROR: Unknown message type %d received in recv_thread_fun. "
                  "Stopping processing of this chunk.\n",
                  header->type);
          continue;
        }
      }
    }

    if (is_analysis_type_enabled(AnalysisType::DEADLOCK_DETECTION) && last_seen_kernel_launch_id != UINT64_MAX) {
      check_kernel_hang(ctx_state, last_seen_kernel_launch_id);
    }
  }

  // Dump data for the very last kernel if it exists.
  if (last_seen_kernel_launch_id != UINT64_MAX) {
    // Dump any remaining histograms for warps that were still collecting.
    for (const std::pair<const int, WarpState>& pair : warp_states) {
      if (pair.second.is_collecting && !pair.second.histogram.empty()) {
        local_completed_histograms.push_back({pair.first, pair.second.region_counter, pair.second.histogram});
      }
    }
    if (!local_completed_histograms.empty()) {
      dump_previous_kernel_data(last_seen_kernel_launch_id, local_completed_histograms);
    }

    // Close the last kernel's TraceWriter
    {
      std::unique_lock<std::shared_mutex> lock(ctx_state->writers_mutex);
      auto it = ctx_state->trace_writers.find(last_seen_kernel_launch_id);
      if (it != ctx_state->trace_writers.end() && it->second) {
        it->second->flush();
        delete it->second;
        ctx_state->trace_writers.erase(it);
        loprintf_v("Closed final TraceWriter for launch_id %lu\n", last_seen_kernel_launch_id);
      }
    }
  }

  free(recv_buffer);
  ctx_state->recv_thread_done = RecvThreadState::FINISHED;
  return NULL;
}

```

`src/cutracer.cu`:

```cu
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-FileCopyrightText: Copyright (c) 2019 NVIDIA CORPORATION & AFFILIATES.
 * SPDX-License-Identifier: MIT AND BSD-3-Clause
 *
 * This source code contains modifications by Meta Platforms, Inc. licensed
 * under MIT, based on original NVIDIA NVBit sample code licensed under
 * BSD-3-Clause. See LICENSE file in the root directory for Meta's license
 * terms. See LICENSE-BSD file in the root directory for NVIDIA's license terms.
 */

#include <assert.h>
#include <pthread.h>
#include <stdio.h>
#include <unistd.h>

#include <map>
#include <random>
#include <regex>
#include <sstream>
#include <string>
#include <unordered_map>
#include <unordered_set>

/* every tool needs to include this once */
#include "nvbit_tool.h"

/* nvbit interface file */
#include "nvbit.h"

/* contains definition of the reg_info_t and mem_access_t structure */
#include "common.h"

/* analysis functionality */
#include "analysis.h"

/* instrumentation functionality */
#include "instrument.h"

/* env config */
#include "env_config.h"
#include "trace_writer.h"

/* logging functionality */
#include "log.h"

/* delay configuration */
#include "delay_inject_config.h"

/* implicit registers collection */
#include "implicit_regs.h"

#define CUDA_CHECK_LAST_ERROR()                                                                       \
  do {                                                                                                \
    cudaError_t err = cudaGetLastError();                                                             \
    if (err != cudaSuccess) {                                                                         \
      loprintf("FATAL: CUDA Last Error: %s at %s:%d\n", cudaGetErrorString(err), __FILE__, __LINE__); \
      fflush(stderr);                                                                                 \
      assert(err == cudaSuccess);                                                                     \
    }                                                                                                 \
  } while (0)

/* lock */
pthread_mutex_t mutex;
pthread_mutex_t cuda_event_mutex;

/* map to store context state */
std::unordered_map<CUcontext, CTXstate*> ctx_state_map;

/* skip flag used to avoid re-entry on the nvbit_callback when issuing
 * flush_channel kernel call */
bool skip_callback_flag = false;

/* The kernel launch is identified by a global id */
uint64_t global_kernel_launch_id = 0;

/* Random number generator for delay injection on/off control.
 * Uses std::random_device to seed std::mt19937 for true randomness
 * across different runs/processes. */
static std::mt19937 g_delay_rng;
static bool g_delay_rng_initialized = false;

/* Generate random enabled state (50% probability) */
static bool generate_random_delay_enabled() {
  if (!g_delay_rng_initialized) {
    // Seed with std::random_device for true randomness each run
    std::random_device rd;
    g_delay_rng.seed(rd());
    g_delay_rng_initialized = true;
  }
  std::uniform_int_distribution<int> dist(0, 1);
  return dist(g_delay_rng) == 1;
}

// Global mapping tables for kernel launch tracking
std::map<uint64_t, std::pair<CUcontext, CUfunction>> kernel_launch_to_func_map;
std::map<uint64_t, uint32_t> kernel_launch_to_iter_map;
std::map<uint64_t, KernelDimensions> kernel_launch_to_dimensions_map;

// map to store the iteration count for each kernel
static std::map<CUfunction, uint32_t> kernel_iter_map;

// Per-function static metadata (populated during instrumentation)
std::unordered_map<CUfunction, KernelFuncMetadata> kernel_metadata_by_func;

/* Set used to avoid re-instrumenting the same functions multiple times */
std::unordered_set<CUfunction> already_instrumented;

/**
 * @brief Compute a kernel checksum for robust kernel identification.
 *
 * Uses FNV-1a algorithm to hash the kernel name AND all SASS instruction strings.
 * This provides robust identification across recompilations - same kernel name
 * with different SASS (due to different compilation options, optimizations, etc.)
 * will produce different checksums.
 *
 * NOTE: This uses the ORIGINAL SASS from nvbit_get_instrs() before any
 * instrumentation is applied. The delay injection happens later and does
 * not affect the checksum.
 *
 * @param kernel_name The kernel's mangled name
 * @param instrs Vector of instructions from nvbit_get_instrs()
 * @return Hex string of the FNV-1a hash
 */
static std::string compute_kernel_checksum(const std::string& kernel_name, const std::vector<Instr*>& instrs) {
  // FNV-1a constants
  const uint64_t FNV_OFFSET = 14695981039346656037ULL;
  const uint64_t FNV_PRIME = 1099511628211ULL;

  uint64_t hash = FNV_OFFSET;

  // First, hash the kernel name
  for (char c : kernel_name) {
    hash ^= static_cast<uint64_t>(c);
    hash *= FNV_PRIME;
  }

  // Then hash all SASS instructions
  for (const auto& instr : instrs) {
    const char* sass = instr->getSass();
    if (sass) {
      // Hash each character of the SASS string
      for (const char* p = sass; *p; ++p) {
        hash ^= static_cast<uint64_t>(*p);
        hash *= FNV_PRIME;
      }
    }
    // Also include offset for additional uniqueness
    uint64_t offset = instr->getOffset();
    hash ^= offset;
    hash *= FNV_PRIME;
  }

  // Convert to hex string
  std::ostringstream oss;
  oss << std::hex << std::nouppercase << hash;
  return oss.str();
}

/**
 * @brief Check if an instruction should be instrumented based on category filtering.
 *
 * When category filtering is enabled (CUTRACER_INSTR_CATEGORIES is set),
 * only instructions belonging to enabled categories will be instrumented.
 * When category filtering is disabled, all instructions are instrumented.
 *
 * @param instr The instruction to check
 * @return true if the instruction should be instrumented
 */
static bool should_instrument_instr_by_category(Instr* instr) {
  if (!has_category_filter_enabled()) {
    return true;
  }

  const char* sass_str = instr->getSass();
  InstrCategory category = detect_instr_category(sass_str);

  if (category == InstrCategory::NONE) {
    // Not a categorized instruction - skip when category filtering is enabled
    return false;
  }

  // Check if this category is enabled
  bool should_instrument = should_instrument_category(category);
  if (should_instrument) {
    loprintf_v("Category filter: instrumenting %s instruction at pc=0x%lx: %s\n", get_instr_category_name(category),
               instr->getOffset(), sass_str);
  }
  return should_instrument;
}

/* ===== Main Functionality ===== */

/**
 * @brief Get or create per-function metadata entry.
 *
 * If the function is seen for the first time, inserts a new entry and
 * populates lightweight fields (name, func_addr, nregs, shmem) via
 * CUDA driver API.  Subsequent calls return the existing entry directly.
 */
static KernelFuncMetadata& get_or_create_kernel_func_metadata(CUfunction func, CUcontext ctx) {
  auto [it, inserted] = kernel_metadata_by_func.emplace(func, KernelFuncMetadata{});
  if (inserted) {
    it->second.unmangled_name = nvbit_get_func_name(ctx, func, false);
    it->second.func_addr = nvbit_get_func_addr(ctx, func);
    CUDA_SAFECALL(cuFuncGetAttribute(&it->second.nregs, CU_FUNC_ATTRIBUTE_NUM_REGS, func));
    CUDA_SAFECALL(cuFuncGetAttribute(&it->second.shmem_static_nbytes, CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES, func));
  }
  return it->second;
}

/**
 * @brief Conditionally instruments a CUDA function by delegating to specialized
 * instrumentation functions.
 *
 * This function is based on the `instrument_kernel` function from NVIDIA's
 * `mem_trace` example. It inspects a function and, if it matches the filters,
 * iterates through its SASS instructions, delegating the actual instrumentation
 * to functions in `instrument.cu`.
 *
 * Meta's enhancements include:
 *  - Combining memory (MREF) and register (REG) tracing.
 *  - Adding support for Unified Registers (UREG).
 *  - Kernel Filtering: A system (`kernel_filters`) to selectively
 *    instrument kernels based on environment variables, avoiding unnecessary
 *    overhead.
 *  - Modular Instrumentation API: The core instrumentation logic is refactored
 *    into `instrument.cu`, providing a clear API for different tracing types.
 *
 * @param ctx The CUDA context of the function.
 * @param func The `CUfunction` to inspect and potentially instrument.
 * @return `true` if instrumentation should be enabled, `false` otherwise.
 */
bool instrument_function_if_needed(CUcontext ctx, CUfunction func) {
  if (!has_any_instrumentation_enabled()) {
    return false;
  }
  assert(ctx_state_map.find(ctx) != ctx_state_map.end());
  CTXstate* ctx_state = ctx_state_map[ctx];

  // Static regex pattern for extracting UR register numbers from GENERIC operands
  static std::regex ureg_pattern(R"(UR(\d+))");

  /* Get related functions of the kernel (device function that can be
   * called by the kernel) */
  std::vector<CUfunction> related_functions = nvbit_get_related_functions(ctx, func);

  /* add kernel itself to the related function vector */
  related_functions.push_back(func);

  // No filters specified implies we want to instrument all functions
  bool should_instrument = kernel_filters.empty();
  /* iterate on function */
  for (auto f : related_functions) {
    const char* unmangled_name = nvbit_get_func_name(ctx, f, false);
    const char* mangled_name = nvbit_get_func_name(ctx, f, true);

    // Check if the current function matches any kernel filter
    bool kernel_filter_matched = kernel_filters.empty();
    for (const auto& filter : kernel_filters) {
      if ((unmangled_name && strstr(unmangled_name, filter.c_str()) != NULL) ||
          (mangled_name && strstr(mangled_name, filter.c_str()) != NULL)) {
        kernel_filter_matched = true;
        // Update return value (determines if instrumented code should run)
        should_instrument = true;
        loprintf_v("Found matching kernel for filter '%s': %s (mangled: %s)\n", filter.c_str(),
                   unmangled_name ? unmangled_name : "unknown", mangled_name ? mangled_name : "unknown");
        break;
      }
    }

    // Skip if filter not matched or already instrumented
    if (!kernel_filter_matched || !already_instrumented.insert(f).second) {
      continue;
    }

    const std::vector<Instr*>& instrs = nvbit_get_instrs(ctx, f);
    loprintf_v("Inspecting kernel %s at address 0x%lx\n", nvbit_get_func_name(ctx, f), nvbit_get_func_addr(ctx, f));

    // Get or create per-function metadata (basic fields filled by helper)
    auto& meta = get_or_create_kernel_func_metadata(f, ctx);
    meta.mangled_name = mangled_name;
    meta.kernel_checksum = compute_kernel_checksum(mangled_name, instrs);
    loprintf_v("Computed kernel checksum for %s: %s\n", mangled_name, meta.kernel_checksum.c_str());

    if (dump_cubin) {
      // Use the same "kernel_{checksum}_{name}" prefix as trace files
      // so Python analyze can derive cubin path from trace filename
      // (trace: kernel_{checksum}_iter{N}_{name}.ndjson â†’ cubin: kernel_{checksum}_{name}.cubin)
      std::string truncated_name = std::string(mangled_name).substr(0, 150);
      meta.cubin_path = "kernel_" + meta.kernel_checksum + "_" + truncated_name + ".cubin";
      nvbit_dump_cubin(ctx, f, meta.cubin_path.c_str());
    }

    // Create kernel delay config if dump path is set (for exporting to JSON)
    // Note: dump mode and replay mode are mutually exclusive
    KernelDelayInjectConfig* kernel_delay_config = nullptr;
    if (!delay_dump_path.empty() && is_instrument_type_enabled(InstrumentType::RANDOM_DELAY)) {
      assert(!is_delay_replay_mode() && "Dump and replay modes are mutually exclusive");
      // Reuse the kernel_checksum already computed above for trace writer
      kernel_delay_config = create_kernel_delay_config(mangled_name, meta.kernel_checksum);
    }

    // Get replay instrumentation points once for replay mode
    const std::map<uint64_t, DelayInstrumentationPoint>* replay_points = nullptr;
    if (is_delay_replay_mode() && is_instrument_type_enabled(InstrumentType::RANDOM_DELAY)) {
      assert(delay_dump_path.empty() && "Dump and replay modes are mutually exclusive");
      // Reuse the kernel_checksum already computed above for trace writer
      replay_points = get_replay_instrumentation_points(mangled_name, meta.kernel_checksum);
    }

    uint32_t cnt = 0;
    /* iterate on all the static instructions in the function */
    for (auto instr : instrs) {
      if (cnt < instr_begin_interval || cnt >= instr_end_interval) {
        cnt++;
        continue;
      }
      if (verbose) {
        instr->printDecoded();
      }

      OperandLists operands;
      OperandContext op_ctx;  // Context for implicit register collection
      int mref_idx = 0;
      int opcode_id = instr->getIdx();
      ctx_state->id_to_sass_map[f][opcode_id] = std::string(instr->getSass());
      /* iterate on the operands */
      for (int i = 0; i < instr->getNumOperands(); i++) {
        /* get the operand "i" */
        const InstrType::operand_t* op = instr->getOperand(i);
        if (op->type == InstrType::OperandType::REG) {
          for (int reg_idx = 0; reg_idx < instr->getSize() / 4; reg_idx++) {
            operands.reg_nums.push_back(op->u.reg.num + reg_idx);
          }
        } else if (op->type == InstrType::OperandType::UREG) {
          for (int reg_idx = 0; reg_idx < instr->getSize() / 4; reg_idx++) {
            operands.ureg_nums.push_back(op->u.reg.num + reg_idx);
          }
        } else if (op->type == InstrType::OperandType::GENERIC) {
          loprintf_v("  GENERIC operand[%d]: '%s'\n", i, op->u.generic.array);

          // Extract UR register numbers from GENERIC operand using regex
          try {
            std::string generic_str(op->u.generic.array);
            std::sregex_iterator begin(generic_str.begin(), generic_str.end(), ureg_pattern);
            std::sregex_iterator end;

            int match_count = 0;
            for (auto it = begin; it != end; ++it) {
              std::smatch match = *it;
              // match[0] is the full match "URxx"
              // match[1] is the captured group (the number part)
              int ureg_num = std::stoi(match[1].str());

              operands.ureg_nums.push_back(ureg_num);
              op_ctx.generic_urs.push_back(ureg_num);
              op_ctx.generic_strs.push_back(generic_str);  // Store raw string for prefix analysis
              match_count++;

              loprintf_v("    Extracted UREG: UR%d\n", ureg_num);
            }

            if (match_count == 0) {
              loprintf_v("    No UREG found in GENERIC operand\n");
            }
          } catch (const std::exception& e) {
            loprintf("ERROR: Failed to parse GENERIC operand: %s\n", e.what());
          }
        } else if (op->type == InstrType::OperandType::MEM_DESC) {
          loprintf_v("  MEM_DESC operand[%d]: ureg_num=%d\n", i, op->u.mem_desc.ureg_num);
          op_ctx.desc_urs.push_back(op->u.mem_desc.ureg_num);
        } else if (op->type == InstrType::OperandType::MREF) {
          if (op->u.mref.has_ra) {
            operands.reg_nums.push_back(op->u.mref.ra_num);
            op_ctx.mref_ras.push_back(op->u.mref.ra_num);
          }
          if (op->u.mref.has_ur) {
            operands.ureg_nums.push_back(op->u.mref.ur_num);
            op_ctx.mref_urs.push_back(op->u.mref.ur_num);
          }
          if (op->u.mref.has_desc) {
            operands.ureg_nums.push_back(op->u.mref.desc_ureg_num);
            operands.ureg_nums.push_back(op->u.mref.desc_ureg_num + 1);
          }

          // Use new instrumentation interface for memory tracing
          if (is_instrument_type_enabled(InstrumentType::MEM_ADDR_TRACE)) {
            instrument_memory_addr_trace(instr, opcode_id, ctx_state, mref_idx);
          }
          // Memory value tracing (captures addresses + values at IPOINT_AFTER)
          if (is_instrument_type_enabled(InstrumentType::MEM_VALUE_TRACE)) {
            int mem_space = (int)instr->getMemorySpace();
            instrument_memory_value_trace(instr, opcode_id, ctx_state, mref_idx, mem_space);
          }
          mref_idx++;
        }
      }

      // Collect implicit registers based on instruction semantics
      // (e.g., UTMALDG uses URb+1 for barrier address, not exposed by NVBit)
      collect_implicit_regs(instr->getSass(), op_ctx, operands);

      // Store register indices in CPU-side static map (no GPU overhead)
      // Must be AFTER collect_implicit_regs() since implicit regs modify operands
      {
        RegIndices reg_idx;
        reg_idx.reg_indices.reserve(operands.reg_nums.size());
        for (int num : operands.reg_nums) {
          reg_idx.reg_indices.push_back(static_cast<uint8_t>(num));
        }
        reg_idx.ureg_indices.reserve(operands.ureg_nums.size());
        for (int num : operands.ureg_nums) {
          reg_idx.ureg_indices.push_back(static_cast<uint8_t>(num));
        }
        ctx_state->id_to_reg_indices_map[f][opcode_id] = std::move(reg_idx);
      }

      // Choose instrumentation based on enabled types (only if this instruction passes category filter)
      if (should_instrument_instr_by_category(instr)) {
        if (is_instrument_type_enabled(InstrumentType::OPCODE_ONLY)) {
          // Lightweight instrumentation for instruction histogram analysis.
          // This sends minimal data (opcode_id, warp_id) to the CPU, reducing
          // overhead.
          instrument_opcode_only(instr, opcode_id, ctx_state);
        } else if (is_instrument_type_enabled(InstrumentType::REG_TRACE)) {
          // Full register tracing.
          instrument_register_trace(instr, opcode_id, ctx_state, operands);
        }

        // TMA_TRACE: tensor memory descriptor tracing (independent of REG_TRACE/OPCODE_ONLY)
        if (is_instrument_type_enabled(InstrumentType::TMA_TRACE)) {
          if (is_instr_category(instr->getSass(), InstrCategory::TMA)) {
            instrument_tma_trace(instr, opcode_id, ctx_state, operands);
          }
        }
      }

      // Delay instrumentation for synchronization instructions
      if (is_instrument_type_enabled(InstrumentType::RANDOM_DELAY) &&
          shouldInjectDelay(instr, DELAY_INJECTION_PATTERNS)) {
        bool enabled;
        uint32_t delay_ns;
        bool found = true;

        if (is_delay_replay_mode()) {
          // Replay mode: always use config values, no random
          uint64_t pc_offset = instr->getOffset();
          found = lookup_replay_config(replay_points, pc_offset, enabled, delay_ns);
          if (!found) {
            // Point not found in config - skip instrumentation for this point
            loprintf("Replay mode: kernel=%s pc=0x%lx NOT FOUND in config, skipping\n", unmangled_name, pc_offset);
          }
        } else {
          // Random mode: generate random on/off with 50% probability
          enabled = generate_random_delay_enabled();
          delay_ns = g_delay_ns;

          // Register the point for config output if kernel_delay_config was created
          if (kernel_delay_config) {
            register_delay_instrumentation_point(kernel_delay_config, instr, delay_ns, enabled);
          }
        }

        if (found && enabled) {
          instrument_delay_injection(instr, delay_ns);
        }
        if (found) {
          loprintf_v("Delay injection: kernel=%s pc=0x%lx sass='%s' enabled=%s delay=%u%s\n", unmangled_name,
                     instr->getOffset(), instr->getSass(), enabled ? "true" : "false", enabled ? delay_ns : 0,
                     is_delay_replay_mode() ? " [REPLAY]" : "");
        }
      }
    }

    // Statically identify special instructions for this function:
    // - "clock" (CS2R SR_CLOCKLO) used for histogram region boundaries
    // - "EXIT" used as a candidate signal for warp completion on host side
    // - Categorized instructions (MMA, TMA, SYNC, etc.) for tracing
    for (std::map<int, std::string>::const_iterator it_sass = ctx_state->id_to_sass_map[f].begin();
         it_sass != ctx_state->id_to_sass_map[f].end(); ++it_sass) {
      const char* sass_cstr = it_sass->second.c_str();
      if (strstr(sass_cstr, "CS2R") && strstr(sass_cstr, "SR_CLOCKLO")) {
        ctx_state->clock_opcode_ids[f].insert(it_sass->first);
      }
      // Simple substring detection for EXIT mnemonic (predication handled by extract on host side)
      if (strstr(sass_cstr, "EXIT")) {
        ctx_state->exit_opcode_ids[f].insert(it_sass->first);
      }
      // Detect instruction category and log if matched
      InstrCategory category = detect_instr_category(sass_cstr);
      if (category != InstrCategory::NONE) {
        const char* desc = get_instr_pattern_description(sass_cstr);
        loprintf("%s detected: kernel=%s opcode_id=%d sass='%s' (%s)\n", get_instr_category_name(category),
                 unmangled_name, it_sass->first, sass_cstr, desc ? desc : "");
      }
    }

    /* ============================================ */
  }

  return should_instrument;
}

// Reference code from NVIDIA nvbit mem_trace tool
/* flush channel */
__global__ void flush_channel(ChannelDev* ch_dev) {
  ch_dev->flush();
}

// Reference code from NVIDIA nvbit mem_trace tool
void init_context_state(CUcontext ctx) {
  assert(ctx_state_map.find(ctx) != ctx_state_map.end());
  CTXstate* ctx_state = ctx_state_map[ctx];
  ctx_state->recv_thread_done = RecvThreadState::WORKING;
  cudaMallocManaged(&ctx_state->channel_dev, sizeof(ChannelDev));
  ctx_state->channel_host.init((int)ctx_state_map.size() - 1, CHANNEL_SIZE, ctx_state->channel_dev, recv_thread_fun,
                               ctx);
  nvbit_set_tool_pthread(ctx_state->channel_host.get_thread());
}

/**
 * @brief Build a kernel_metadata JSON object for the trace file header.
 *
 * Combines per-function static attributes (from KernelFuncMetadata) with
 * per-launch dynamic attributes (grid/block dims, dynamic shared memory).
 */
static nlohmann::json build_kernel_metadata_json(const KernelFuncMetadata& meta, const KernelDimensions& dims,
                                                 unsigned int dynamic_shmem) {
  auto md = meta.to_json();
  md["type"] = "kernel_metadata";
  md["grid"] = {dims.gridDimX, dims.gridDimY, dims.gridDimZ};
  md["block"] = {dims.blockDimX, dims.blockDimY, dims.blockDimZ};
  md["shmem_dynamic"] = dynamic_shmem;
  return md;
}

// Reference code from NVIDIA nvbit mem_trace tool
/**
 * @brief Prepares for a kernel launch, conditionally instrumenting and logging.
 *
 * Based on `enter_kernel_launch` from NVIDIA's `mem_trace` example, this
 * function is called just before a kernel launch. It determines if a kernel
 * should be instrumented based on active filters.
 *
 * Key modifications by Meta include making instrumentation and log file creation
 * conditional on the filtering result. This prevents empty log files for
 * skipped kernels.
 *
 * @param ctx The current CUDA context.
 * @param func The kernel function being launched.
 * @param kernel_launch_id A reference to the global kernel launch counter.
 * @param cbid The NVBit callback ID for the CUDA API call.
 * @param params A pointer to the parameters of the CUDA launch call.
 * @param stream_capture True if the launch is part of a CUDA stream capture.
 * @param build_graph True if the launch is part of a manual graph build.
 * @return `true` if the kernel was instrumented, `false` otherwise.
 */
static bool enter_kernel_launch(CUcontext ctx, CUfunction func, uint64_t& kernel_launch_id, nvbit_api_cuda_t cbid,
                                void* params, bool stream_capture = false, bool build_graph = false) {
  CTXstate* ctx_state = ctx_state_map[ctx];
  // no need to sync during stream capture or manual graph build, since no
  // kernel is actually launched.
  if (!stream_capture && !build_graph) {
    /* Make sure GPU is idle */
    cudaDeviceSynchronize();
    CUDA_CHECK_LAST_ERROR();
  }

  bool should_instrument = instrument_function_if_needed(ctx, func);

  // Get or create per-function metadata (basic fields auto-populated on first access)
  const auto& meta = get_or_create_kernel_func_metadata(func, ctx);
  const char* func_name = meta.unmangled_name.c_str();
  uint64_t pc = meta.func_addr;

  // Per-launch dynamic attributes, populated in the normal launch path below.
  // Declared here so they are accessible when writing kernel_metadata after TraceWriter creation.
  KernelDimensions dims{};
  unsigned int dynamic_shmem = 0;

  // during stream capture or manual graph build, no kernel is launched, so
  // do not set launch argument, do not print kernel info, do not increase
  // grid_launch_id. All these should be done at graph node launch time.
  if (!stream_capture && !build_graph) {
    /* set kernel launch id at launch time */
    nvbit_set_at_launch(ctx, func, (uint64_t)kernel_launch_id);

    if (cbid == API_CUDA_cuLaunchKernelEx_ptsz || cbid == API_CUDA_cuLaunchKernelEx) {
      cuLaunchKernelEx_params* p = (cuLaunchKernelEx_params*)params;
      loprintf(
          "CUTracer: CTX 0x%016lx - LAUNCH - Kernel pc 0x%016lx - "
          "Kernel name %s - kernel hash 0x%s - kernel launch id %ld - grid size %d,%d,%d "
          "- block size %d,%d,%d - nregs %d - shmem %d - cuda stream "
          "id %ld\n",
          (uint64_t)ctx, pc, func_name, meta.kernel_checksum.c_str(), kernel_launch_id, p->config->gridDimX,
          p->config->gridDimY, p->config->gridDimZ, p->config->blockDimX, p->config->blockDimY, p->config->blockDimZ,
          meta.nregs, meta.shmem_static_nbytes + p->config->sharedMemBytes, (uint64_t)p->config->hStream);
    } else {
      cuLaunchKernel_params* p = (cuLaunchKernel_params*)params;
      loprintf(
          "CUTracer: CTX 0x%016lx - LAUNCH - Kernel pc 0x%016lx - "
          "Kernel name %s - kernel hash 0x%s - kernel launch id %ld - grid size %d,%d,%d "
          "- block size %d,%d,%d - nregs %d - shmem %d - cuda stream "
          "id %ld\n",
          (uint64_t)ctx, pc, func_name, meta.kernel_checksum.c_str(), kernel_launch_id, p->gridDimX, p->gridDimY,
          p->gridDimZ, p->blockDimX, p->blockDimY, p->blockDimZ, meta.nregs,
          meta.shmem_static_nbytes + p->sharedMemBytes, (uint64_t)p->hStream);
    }

    // For histogram analysis, we need to map the kernel launch ID back to its
    // function and context to access the correct SASS and clock opcode maps.
    // We also track the iteration count for unique file naming.
    uint32_t current_iter = kernel_iter_map[func];
    kernel_launch_to_func_map[kernel_launch_id] = {ctx, func};
    kernel_launch_to_iter_map[kernel_launch_id] = current_iter;

    // Store kernel dimensions and dynamic shared memory for warp statistics tracking
    // (dims and dynamic_shmem declared at function scope for metadata writing)
    if (cbid == API_CUDA_cuLaunchKernelEx_ptsz || cbid == API_CUDA_cuLaunchKernelEx) {
      cuLaunchKernelEx_params* p = (cuLaunchKernelEx_params*)params;
      dims = {p->config->gridDimX,  p->config->gridDimY,  p->config->gridDimZ,
              p->config->blockDimX, p->config->blockDimY, p->config->blockDimZ};
      dynamic_shmem = p->config->sharedMemBytes;
    } else {
      cuLaunchKernel_params* p = (cuLaunchKernel_params*)params;
      dims = {p->gridDimX, p->gridDimY, p->gridDimZ, p->blockDimX, p->blockDimY, p->blockDimZ};
      dynamic_shmem = p->sharedMemBytes;
    }
    kernel_launch_to_dimensions_map[kernel_launch_id] = dims;

    // increment kernel launch id for next launch
    // kernel id can be changed here, since nvbit_set_at_launch() has copied
    // its value above.
    kernel_launch_id++;
  }

  if (should_instrument) {
    // Create TraceWriter for each kernel launch (per-launch trace files)
    std::string base_filename = generate_kernel_log_basename(ctx, func, kernel_iter_map[func]++, meta.kernel_checksum);
    uint64_t current_launch_id = kernel_launch_id - 1;  // kernel_launch_id was already incremented

    {
      std::unique_lock<std::shared_mutex> lock(ctx_state->writers_mutex);
      ctx_state->trace_writers[current_launch_id] = new TraceWriter(base_filename, trace_format_ndjson);
    }

    // Initialize trace_index for this kernel
    if (!stream_capture && !build_graph) {
      ctx_state->trace_index_by_kernel[current_launch_id] = 0;
    }

    // Write kernel_metadata as the first JSON line in the trace file.
    // Only for normal launches (stream_capture/build_graph get metadata at graph node launch time).
    if (!stream_capture && !build_graph) {
      auto metadata = build_kernel_metadata_json(meta, dims, dynamic_shmem);
      ctx_state->trace_writers[current_launch_id]->write_metadata(metadata);
    }

    loprintf_v("Created TraceWriter for launch_id %lu, mode %d, file: %s\n", current_launch_id, trace_format_ndjson,
               base_filename.c_str());
  }

  nvbit_enable_instrumented(ctx, func, should_instrument);
  return should_instrument;
}

// the function is only called for non cuda graph launch cases.
static void leave_kernel_launch(CTXstate* ctx_state, uint64_t& grid_launch_id) {
  // make sure user kernel finishes to avoid deadlock
  cudaDeviceSynchronize();
  /* push a flush channel kernel */
  flush_channel<<<1, 1>>>(ctx_state->channel_dev);

  /* Make sure GPU is idle */
  cudaDeviceSynchronize();
  CUDA_CHECK_LAST_ERROR();

  // Flush current launch's TraceWriter buffer (but don't close it yet)
  // The recv_thread will close writers when it detects launch_id changes
  uint64_t current_launch_id = grid_launch_id - 1;
  {
    std::shared_lock<std::shared_mutex> lock(ctx_state->writers_mutex);
    auto it = ctx_state->trace_writers.find(current_launch_id);
    if (it != ctx_state->trace_writers.end() && it->second) {
      it->second->flush();
    }
  }
}

// Reference code from NVIDIA nvbit mem_trace tool
void nvbit_at_cuda_event(CUcontext ctx, int is_exit, nvbit_api_cuda_t cbid, const char* name, void* params,
                         CUresult* pStatus) {
  pthread_mutex_lock(&cuda_event_mutex);

  /* we prevent re-entry on this callback when issuing CUDA functions inside
   * this function */
  if (skip_callback_flag) {
    pthread_mutex_unlock(&cuda_event_mutex);
    return;
  }
  skip_callback_flag = true;

  CTXstate* ctx_state = ctx_state_map[ctx];

  switch (cbid) {
    // Identify all the possible CUDA launch events without stream
    // parameters, they will not get involved with cuda graph
    case API_CUDA_cuLaunch:
    case API_CUDA_cuLaunchGrid: {
      cuLaunch_params* p = (cuLaunch_params*)params;
      CUfunction func = p->f;
      if (!is_exit) {
        enter_kernel_launch(ctx, func, global_kernel_launch_id, cbid, params);
      } else {
        leave_kernel_launch(ctx_state, global_kernel_launch_id);
      }
    } break;
    // To support kernel launched by cuda graph (in addition to existing kernel
    // launche method), we need to do:
    //
    // 1. instrument kernels at cudaGraphAddKernelNode event. This is for cases
    // that kernels are manually added to a cuda graph.
    // 2. distinguish captured kernels when kernels are recorded to a graph
    // using stream capture. cudaStreamIsCapturing() tells us whether a stream
    // is capturiong.
    // 3. per-kernel instruction counters, since cuda graph can launch multiple
    // kernels at the same time.
    //
    // Three cases:
    //
    // 1. original kernel launch:
    //     1a. for any kernel launch without using a stream, we instrument it
    //     before it is launched, call cudaDeviceSynchronize after it is
    //     launched and read the instruction counter of the kernel.
    //     1b. for any kernel launch using a stream, but the stream is not
    //     capturing, we do the same thing as 1a.
    //
    //  2. cuda graph using stream capturing: if a kernel is launched in a
    //  stream and the stream is capturing. We instrument the kernel before it
    //  is launched and do nothing after it is launched, because the kernel is
    //  not running until cudaGraphLaunch. Instead, we issue a
    //  cudaStreamSynchronize after cudaGraphLaunch is done and reset the
    //  instruction counters, since a cloned graph might be launched afterwards.
    //
    //  3. cuda graph manual: we instrument the kernel added by
    //  cudaGraphAddKernelNode and do the same thing for cudaGraphLaunch as 2.
    //
    // The above method should handle most of cuda graph launch cases.
    // kernel launches with stream parameter, they can be used for cuda graph
    case API_CUDA_cuLaunchKernel_ptsz:
    case API_CUDA_cuLaunchKernel:
    case API_CUDA_cuLaunchCooperativeKernel:
    case API_CUDA_cuLaunchCooperativeKernel_ptsz:
    case API_CUDA_cuLaunchKernelEx:
    case API_CUDA_cuLaunchKernelEx_ptsz:
    case API_CUDA_cuLaunchGridAsync: {
      CUfunction func;
      CUstream hStream;

      if (cbid == API_CUDA_cuLaunchKernelEx_ptsz || cbid == API_CUDA_cuLaunchKernelEx) {
        cuLaunchKernelEx_params* p = (cuLaunchKernelEx_params*)params;
        func = p->f;
        hStream = p->config->hStream;
      } else if (cbid == API_CUDA_cuLaunchKernel_ptsz || cbid == API_CUDA_cuLaunchKernel ||
                 cbid == API_CUDA_cuLaunchCooperativeKernel_ptsz || cbid == API_CUDA_cuLaunchCooperativeKernel) {
        cuLaunchKernel_params* p = (cuLaunchKernel_params*)params;
        func = p->f;
        hStream = p->hStream;
      } else {
        cuLaunchGridAsync_params* p = (cuLaunchGridAsync_params*)params;
        func = p->f;
        hStream = p->hStream;
      }

      cudaStreamCaptureStatus streamStatus;
      /* check if the stream is capturing, if yes, do not sync */
      CUDA_SAFECALL(cudaStreamIsCapturing(hStream, &streamStatus));
      if (!is_exit) {
        bool stream_capture = (streamStatus == cudaStreamCaptureStatusActive);
        enter_kernel_launch(ctx, func, global_kernel_launch_id, cbid, params, stream_capture);
      } else {
        if (streamStatus != cudaStreamCaptureStatusActive) {
          loprintf_vl(1, "kernel %s not captured by cuda graph\n", nvbit_get_func_name(ctx, func));
          leave_kernel_launch(ctx_state, global_kernel_launch_id);
        } else {
          loprintf_vl(1, "kernel %s captured by cuda graph\n", nvbit_get_func_name(ctx, func));
        }
      }
    } break;
    case API_CUDA_cuGraphAddKernelNode: {
      cuGraphAddKernelNode_params* p = (cuGraphAddKernelNode_params*)params;
      CUfunction func = p->nodeParams->func;

      if (!is_exit) {
        // cuGraphAddKernelNode_params->nodeParams is the same as
        // cuLaunchKernel_params up to sharedMemBytes
        enter_kernel_launch(ctx, func, global_kernel_launch_id, cbid, (void*)p->nodeParams, false, true);
      }
    } break;
    case API_CUDA_cuGraphLaunch: {
      // if we are exiting a cuda graph launch:
      // Wait until the graph is completed using
      // cudaStreamSynchronize()
      if (is_exit) {
        cuGraphLaunch_params* p = (cuGraphLaunch_params*)params;

        CUDA_SAFECALL(cudaStreamSynchronize(p->hStream));
        CUDA_CHECK_LAST_ERROR();
        /* push a flush channel kernel */
        flush_channel<<<1, 1, 0, p->hStream>>>(ctx_state->channel_dev);
        CUDA_SAFECALL(cudaStreamSynchronize(p->hStream));
        CUDA_CHECK_LAST_ERROR();
      }

    } break;
    default:
      break;
  };

  skip_callback_flag = false;
  pthread_mutex_unlock(&cuda_event_mutex);
}

// Reference NVIDIA record_reg_vals example
void nvbit_tool_init(CUcontext ctx) {
  pthread_mutex_lock(&mutex);
  assert(ctx_state_map.find(ctx) != ctx_state_map.end());
  init_context_state(ctx);
  pthread_mutex_unlock(&mutex);
}

// Reference code from NVIDIA nvbit mem_trace tool
void nvbit_at_ctx_init(CUcontext ctx) {
  pthread_mutex_lock(&mutex);
  if (verbose) {
    printf("MEMTRACE: STARTING CONTEXT %p\n", ctx);
  }
  assert(ctx_state_map.find(ctx) == ctx_state_map.end());
  CTXstate* ctx_state = new CTXstate;
  ctx_state_map[ctx] = ctx_state;
  pthread_mutex_unlock(&mutex);
}

// Reference code from NVIDIA nvbit mem_trace tool
void nvbit_at_ctx_term(CUcontext ctx) {
  pthread_mutex_lock(&mutex);
  skip_callback_flag = true;
  loprintf_v("CUTracer: TERMINATING CONTEXT %p\n", ctx);
  /* get context state from map */
  assert(ctx_state_map.find(ctx) != ctx_state_map.end());
  CTXstate* ctx_state = ctx_state_map[ctx];

  /* Notify receiver thread and wait for receiver thread to
   * notify back */
  ctx_state->recv_thread_done = RecvThreadState::STOP;
  while (ctx_state->recv_thread_done != RecvThreadState::FINISHED);

  // Cleanup all TraceWriters (fallback - most should be closed by recv_thread)
  {
    std::unique_lock<std::shared_mutex> lock(ctx_state->writers_mutex);
    for (auto& [launch_id, writer] : ctx_state->trace_writers) {
      if (writer) {
        loprintf_v("Cleaning up remaining writer for launch_id %lu\n", launch_id);
        writer->flush();
        delete writer;
      }
    }
    ctx_state->trace_writers.clear();
  }

  // Clean up any remaining kernel mapping entries
  // (in case there were kernels launched but no data received)
  kernel_launch_to_func_map.clear();
  kernel_launch_to_iter_map.clear();

  ctx_state->channel_host.destroy(false);
  cudaFree(ctx_state->channel_dev);
  skip_callback_flag = false;
  delete ctx_state;
  pthread_mutex_unlock(&mutex);
  // Cleanup log handle system

  cleanup_log_handle();

  // Finalize delay configuration (saves to JSON if path is set)
  finalize_delay_config();
}

// Reference code from NVIDIA nvbit mem_trace tool
void nvbit_at_graph_node_launch(CUcontext ctx, CUfunction func, CUstream stream, uint64_t launch_handle) {
  func_config_t config = {0};
  const char* func_name = nvbit_get_func_name(ctx, func);
  uint64_t pc = nvbit_get_func_addr(ctx, func);

  pthread_mutex_lock(&mutex);

  assert(ctx_state_map.find(ctx) != ctx_state_map.end());
  CTXstate* ctx_state = ctx_state_map[ctx];

  nvbit_set_at_launch(ctx, func, (uint64_t)global_kernel_launch_id, stream, launch_handle);
  nvbit_get_func_config(ctx, func, &config);

  loprintf(
      "MEMTRACE: CTX 0x%016lx - LAUNCH - Kernel pc 0x%016lx - "
      "Kernel name %s - grid launch id %ld - grid size %d,%d,%d "
      "- block size %d,%d,%d - nregs %d - shmem %d - cuda stream "
      "id %ld\n",
      (uint64_t)ctx, pc, func_name, global_kernel_launch_id, config.gridDimX, config.gridDimY, config.gridDimZ,
      config.blockDimX, config.blockDimY, config.blockDimZ, config.num_registers,
      config.shmem_static_nbytes + config.shmem_dynamic_nbytes, (uint64_t)stream);

  // Store kernel launch mapping for graph node launches
  kernel_launch_to_func_map[global_kernel_launch_id] = {ctx, func};
  kernel_launch_to_iter_map[global_kernel_launch_id] = kernel_iter_map[func];

  // Store kernel dimensions
  KernelDimensions dims = {config.gridDimX,  config.gridDimY,  config.gridDimZ,
                           config.blockDimX, config.blockDimY, config.blockDimZ};
  kernel_launch_to_dimensions_map[global_kernel_launch_id] = dims;

  // Create TraceWriter for this graph node launch only if instrumentation is enabled
  // Otherwise, trace files would be empty (no data to write)
  if (has_any_instrumentation_enabled()) {
    const auto& meta = get_or_create_kernel_func_metadata(func, ctx);
    std::string base_filename = generate_kernel_log_basename(ctx, func, kernel_iter_map[func]++, meta.kernel_checksum);
    {
      std::unique_lock<std::shared_mutex> lock(ctx_state->writers_mutex);
      ctx_state->trace_writers[global_kernel_launch_id] = new TraceWriter(base_filename, trace_format_ndjson);
    }
    ctx_state->trace_index_by_kernel[global_kernel_launch_id] = 0;

    // Write kernel_metadata for graph node launch trace files
    auto metadata = build_kernel_metadata_json(meta, dims, config.shmem_dynamic_nbytes);
    ctx_state->trace_writers[global_kernel_launch_id]->write_metadata(metadata);

    loprintf_v("Created TraceWriter for graph node launch_id %lu, file: %s\n", global_kernel_launch_id,
               base_filename.c_str());
  }

  // grid id can be changed here, since nvbit_set_at_launch() has copied its
  // value above.
  global_kernel_launch_id++;
  pthread_mutex_unlock(&mutex);
}

// Reference code from NVIDIA nvbit mem_trace tool with Meta modifications for
// env config
void nvbit_at_init() {
  // Initialize configuration from environment variables
  init_config_from_env();
  // Initialize delay JSON config for export (also loads replay config if specified)
  init_delay_json_config();
  /* set mutex as recursive */
  pthread_mutexattr_t attr;
  pthread_mutexattr_init(&attr);
  pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_RECURSIVE);
  pthread_mutex_init(&mutex, &attr);

  pthread_mutex_init(&cuda_event_mutex, &attr);
}

```

`src/delay_inject_config.cu`:

```cu
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 * See LICENSE file in the root directory for Meta's license terms.
 */

#include <chrono>
#include <ctime>
#include <fstream>
#include <iomanip>
#include <sstream>

#include "delay_inject_config.h"
#include "env_config.h"
#include "log.h"
#include "nlohmann/json.hpp"

using json = nlohmann::json;

// Global delay injection configuration
DelayInjectConfig g_delay_inject_config;

// Replay mode configuration (loaded from file)
static DelayInjectConfig g_replay_config;
static bool g_delay_replay_mode_active = false;

bool load_delay_config(const std::string& filepath);

// Helper function to get current timestamp in ISO 8601 format
static std::string get_current_timestamp() {
  auto now = std::chrono::system_clock::now();
  auto time_t_now = std::chrono::system_clock::to_time_t(now);
  auto ms = std::chrono::duration_cast<std::chrono::milliseconds>(now.time_since_epoch()) % 1000;

  std::tm tm_now;
  localtime_r(&time_t_now, &tm_now);

  std::ostringstream oss;
  oss << std::put_time(&tm_now, "%Y-%m-%dT%H:%M:%S");
  oss << '.' << std::setfill('0') << std::setw(3) << ms.count();
  return oss.str();
}

bool DelayInjectConfig::save_to_file(const std::string& filepath) const {
  std::ofstream file(filepath);
  if (!file.is_open()) {
    return false;
  }

  json config_json;
  config_json["version"] = version;
  config_json["delay_ns"] = delay_ns;

  json kernels_json = json::object();
  for (const auto& [kernel_name, kdc] : kernels) {
    json kernel_json;
    kernel_json["kernel_name"] = kdc.kernel_name;
    kernel_json["kernel_checksum"] = kdc.kernel_checksum;
    kernel_json["timestamp"] = kdc.timestamp;

    nlohmann::ordered_json points_json = nlohmann::ordered_json::object();
    for (const auto& [pc_offset, ip] : kdc.instrumentation_points) {
      nlohmann::ordered_json point_json;
      point_json["pc"] = ip.pc_offset;
      point_json["sass"] = ip.sass;
      point_json["delay"] = ip.delay_ns;
      point_json["on"] = ip.enabled;
      points_json[std::to_string(pc_offset)] = point_json;
    }
    kernel_json["instrumentation_points"] = points_json;
    kernels_json[kernel_name] = kernel_json;
  }
  config_json["kernels"] = kernels_json;

  file << config_json.dump(2);
  return true;
}

void init_delay_json_config() {
  g_delay_inject_config.version = "1.0";
  g_delay_inject_config.delay_ns = g_delay_ns;
  g_delay_inject_config.kernels.clear();

  // Load replay config if specified
  if (!delay_load_path.empty()) {
    if (!load_delay_config(delay_load_path)) {
      fprintf(stderr, "FATAL: Failed to load replay config from %s\n", delay_load_path.c_str());
      exit(1);
    }
  }
}

KernelDelayInjectConfig* create_kernel_delay_config(const std::string& kernel_name,
                                                    const std::string& kernel_checksum) {
  // Create new entry with kernel_checksum-based key for robust identification
  std::string timestamp = get_current_timestamp();
  // Use kernel_checksum as key for robust kernel identification across recompilations
  // Same kernel with different SASS will get different entries
  std::string key = kernel_name + "_" + kernel_checksum;

  KernelDelayInjectConfig kdc;
  kdc.kernel_name = kernel_name;
  kdc.kernel_checksum = kernel_checksum;
  kdc.timestamp = timestamp;
  g_delay_inject_config.kernels[key] = kdc;

  loprintf_v("Created delay config for kernel: %s (checksum: %s, key: %s)\n", kernel_name.c_str(),
             kernel_checksum.c_str(), key.c_str());
  return &g_delay_inject_config.kernels[key];
}

void register_delay_instrumentation_point(KernelDelayInjectConfig* kdc, Instr* instr, uint32_t delay_ns, bool enabled) {
  if (!kdc || !instr) {
    return;
  }

  uint64_t pc_offset = instr->getOffset();
  auto ret = kdc->instrumentation_points.emplace(pc_offset, DelayInstrumentationPoint());
  if (!ret.second) {
    return;  // Already registered
  }

  auto& ip = ret.first->second;
  ip.pc_offset = pc_offset;
  ip.sass = std::string(instr->getSass());
  ip.delay_ns = delay_ns;
  ip.enabled = enabled;
}

void finalize_delay_config() {
  if (delay_dump_path.empty()) {
    return;
  }

  loprintf_v("Saving delay config to %s (%zu kernels)\n", delay_dump_path.c_str(),
             g_delay_inject_config.kernels.size());

  if (!g_delay_inject_config.save_to_file(delay_dump_path)) {
    fprintf(stderr, "ERROR: Failed to save delay config to %s\n", delay_dump_path.c_str());
  }
}

bool load_delay_config(const std::string& filepath) {
  std::ifstream file(filepath);
  if (!file.is_open()) {
    fprintf(stderr, "ERROR: Failed to open delay config file: %s\n", filepath.c_str());
    return false;
  }

  try {
    json config_json;
    file >> config_json;

    g_replay_config.version = config_json.value("version", "1.0");
    g_replay_config.delay_ns = config_json.value("delay_ns", 0u);
    g_replay_config.kernels.clear();

    if (config_json.contains("kernels")) {
      for (const auto& [key, kernel_json] : config_json["kernels"].items()) {
        KernelDelayInjectConfig kdc;
        kdc.kernel_name = kernel_json.value("kernel_name", "");
        kdc.kernel_checksum = kernel_json.value("kernel_checksum", "");
        kdc.timestamp = kernel_json.value("timestamp", "");

        if (kernel_json.contains("instrumentation_points")) {
          for (const auto& [pc_str, point_json] : kernel_json["instrumentation_points"].items()) {
            DelayInstrumentationPoint ip;
            ip.pc_offset = std::stoull(pc_str);
            ip.sass = point_json.value("sass", "");
            ip.delay_ns = point_json.value("delay", 0u);
            ip.enabled = point_json.value("on", false);
            kdc.instrumentation_points[ip.pc_offset] = ip;
          }
        }

        g_replay_config.kernels[key] = kdc;
        loprintf_v("Loaded kernel config: %s with %zu instrumentation points\n", kdc.kernel_name.c_str(),
                   kdc.instrumentation_points.size());
      }
    }

    g_delay_replay_mode_active = true;
    loprintf_v("Loaded delay config from %s (replay mode active)\n", filepath.c_str());
    loprintf_v("  Version: %s, Delay: %u ns, Kernels: %zu\n", g_replay_config.version.c_str(), g_replay_config.delay_ns,
               g_replay_config.kernels.size());

    return true;

  } catch (const json::exception& e) {
    fprintf(stderr, "ERROR: Failed to parse delay config JSON: %s\n", e.what());
    return false;
  }
}

bool is_delay_replay_mode() {
  return g_delay_replay_mode_active;
}

const std::map<uint64_t, DelayInstrumentationPoint>* get_replay_instrumentation_points(
    const std::string& kernel_name, const std::string& kernel_checksum) {
  if (!g_delay_replay_mode_active) {
    return nullptr;
  }

  // Search through all kernel configs to find a matching kernel by kernel_checksum
  // Since kernel_name is encoded in the checksum, matching checksum implies matching name
  for (const auto& [key, kdc] : g_replay_config.kernels) {
    if (kdc.kernel_checksum == kernel_checksum) {
      // Sanity check: if checksum matches, name must also match (since name is part of checksum)
      assert(kdc.kernel_name == kernel_name && "Checksum matched but kernel name differs - hash collision?");
      loprintf_v("Replay: Found config for kernel %s (checksum: %s) with %zu instrumentation points\n",
                 kernel_name.c_str(), kernel_checksum.c_str(), kdc.instrumentation_points.size());
      return &kdc.instrumentation_points;
    }
  }

  loprintf_v("Replay: No config found for kernel %s (checksum: %s)\n", kernel_name.c_str(), kernel_checksum.c_str());
  return nullptr;
}

bool lookup_replay_config(const std::map<uint64_t, DelayInstrumentationPoint>* replay_points, uint64_t pc_offset,
                          bool& enabled, uint32_t& delay_ns) {
  if (!replay_points) {
    return false;
  }

  auto it = replay_points->find(pc_offset);
  if (it != replay_points->end()) {
    enabled = it->second.enabled;
    delay_ns = it->second.delay_ns;
    return true;
  }

  return false;
}

```

`src/env_config.cu`:

```cu
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 *
 * See LICENSE file in the root directory for Meta's license terms.
 */

#include <stdio.h>
#include <stdlib.h>

#include <filesystem>

#include "env_config.h"
#include "instr_category.h"
#include "instrument.h"
#include "log.h"

namespace fs = std::filesystem;

// Define configuration variables
// EVERY VARIABLE MUST BE INITIALIZED IN init_config_from_env()
uint32_t instr_begin_interval;
uint32_t instr_end_interval;
int verbose;
bool dump_cubin;
// kernel name filters
std::vector<std::string> kernel_filters;
// enabled instrumentation types
std::unordered_set<InstrumentType> enabled_instrument_types;
// enabled analysis types
std::unordered_set<AnalysisType> enabled_analysis_types;
// enabled instruction categories for conditional instrumentation
std::unordered_set<InstrCategory> enabled_instr_categories;

// Trace format configuration variable
int trace_format_ndjson;

// Zstd compression level
int zstd_compression_level;

// Delay value in nanoseconds for synchronization instrumentation
uint32_t g_delay_ns;

// Delay config dump output path (optional)
std::string delay_dump_path;

// Delay config load path (optional)
std::string delay_load_path;

// Trace output directory for dumping trace files (optional)
std::string trace_output_dir;

/**
 * @brief Parses a comma-separated string of kernel name filters for substring matching.
 *
 * This function takes a string from an environment variable, splits it by commas,
 * and populates the global `kernel_filters` vector with the individual filters.
 * These filters are then used to determine which CUDA kernels to instrument by
 * checking if a filter string appears as a **substring** of the kernel's name
 * (either mangled or unmangled).
 * Empty tokens resulting from ",," or trailing/leading commas are ignored.
 *
 * @param filters_env A C-style string containing comma-separated kernel name filters.
 *                     If NULL, the function does nothing.
 *
 * @example
 * If the environment variable (e.g., `KERNEL_FILTERS`) is set as:
 * `export KERNEL_FILTERS="add,_Z2_gemm,reduce"`
 *
 * A kernel named "add_kernel" would be matched by the "add" filter. A kernel
 * named "my_reduce_kernel" would be matched by "reduce".
 *
 * After calling this function with the example string, the `kernel_filters`
 * vector will contain: `{"add", "_Z2_gemm", "reduce"}`
 */
static void parse_kernel_filters(const std::string& filters_env) {
  if (filters_env.empty()) return;

  std::string filters_str = filters_env;
  size_t pos = 0;
  std::string token;

  // Split by commas
  while ((pos = filters_str.find(',')) != std::string::npos) {
    token = filters_str.substr(0, pos);
    if (!token.empty()) {
      kernel_filters.push_back(token);
    }
    filters_str.erase(0, pos + 1);
  }

  // Add the last token (if it exists)
  if (!filters_str.empty()) {
    kernel_filters.push_back(filters_str);
  }

  printf("Kernel name filters to instrument:\n");
  for (const auto& filter : kernel_filters) {
    printf("  - %s\n", filter.c_str());
  }
}

// Helper function for reading environment variables
static void get_var_int(int& var, const char* env_name, int default_val, const char* description) {
  const char* env_val = getenv(env_name);
  if (env_val) {
    var = atoi(env_val);
  } else {
    var = default_val;
  }
  loprintf("%s = %d (%s)\n", env_name, var, description);
}

static void get_var_uint32(uint32_t& var, const char* env_name, uint32_t default_val, const char* description) {
  const char* env_val = getenv(env_name);
  if (env_val) {
    var = (uint32_t)atoll(env_val);
  } else {
    var = default_val;
  }
  loprintf("%s = %u (%s)\n", env_name, var, description);
}

static void get_var_uint64(uint64_t& var, const char* env_name, uint64_t default_val, const char* description) {
  const char* env_val = getenv(env_name);
  if (env_val) {
    var = (uint64_t)strtoull(env_val, nullptr, 10);
  } else {
    var = default_val;
  }
  loprintf("%s = %lu (%s)\n", env_name, var, description);
}

static void get_var_str(std::string& var, const char* env_name, const std::string& default_val,
                        const char* description) {
  const char* env_val = getenv(env_name);
  if (env_val) {
    var = std::string(env_val);
  } else {
    var = default_val;
  }
  loprintf("%s = %s (%s)\n", env_name, var.c_str(), description);
}

/**
 * @brief Initialize instrumentation system based on environment variables
 *
 * Parses CUTRACER_INSTRUMENT environment variable and sets up enabled types.
 * This function is called within init_config_from_env().
 */
void init_instrumentation(const std::string& instrument_str) {
  if (instrument_str.empty()) {
    return;
  }
  loprintf("Using instrumentation types: %s\n", instrument_str.c_str());

  if (instrument_str.find("reg_trace") != std::string::npos) {
    enabled_instrument_types.insert(InstrumentType::REG_TRACE);
    loprintf("  - Enabled: reg_trace (register value tracing)\n");
  }
  if (instrument_str.find("mem_addr_trace") != std::string::npos) {
    enabled_instrument_types.insert(InstrumentType::MEM_ADDR_TRACE);
    loprintf("  - Enabled: mem_addr_trace (memory access address tracing)\n");
  }
  if (instrument_str.find("mem_value_trace") != std::string::npos) {
    enabled_instrument_types.insert(InstrumentType::MEM_VALUE_TRACE);
    loprintf("  - Enabled: mem_value_trace (memory access with value tracing)\n");
  }
  if (instrument_str.find("random_delay") != std::string::npos) {
    enabled_instrument_types.insert(InstrumentType::RANDOM_DELAY);
    loprintf("  - Enabled: random_delay (random delay injection)\n");
  }
  if (instrument_str.find("tma_trace") != std::string::npos) {
    enabled_instrument_types.insert(InstrumentType::TMA_TRACE);
    loprintf("  - Enabled: tma_trace (TMA descriptor tracing)\n");
  }

  // Warn if both mem_addr_trace and mem_value_trace are enabled
  if (enabled_instrument_types.count(InstrumentType::MEM_ADDR_TRACE) &&
      enabled_instrument_types.count(InstrumentType::MEM_VALUE_TRACE)) {
    loprintf("WARNING: Both 'mem_addr_trace' and 'mem_value_trace' are enabled.\n");
    loprintf("- mem_addr_trace: records addresses at IPOINT_BEFORE\n");
    loprintf("- mem_value_trace: records addresses+values at IPOINT_AFTER\n");
    loprintf("Note: mem_value_trace already includes address information.\n");
    loprintf("If you only need value tracing, consider using mem_value_trace alone.\n");
  }
}

void parse_delay_config() {
  uint64_t delay_val = 0;
  get_var_uint64(delay_val, "CUTRACER_DELAY_NS", 0, "Delay in nanoseconds for synchronization instructions");

  // If random_delay analysis is enabled but no valid delay value, error out.
  if (delay_val == 0 && is_analysis_type_enabled(AnalysisType::RANDOM_DELAY)) {
    fprintf(stderr,
            "FATAL: CUTRACER_ANALYSIS includes 'random_delay' but no delay value is set.\n"
            "Please set CUTRACER_DELAY_NS to a positive value (in nanoseconds).\n"
            "Example: export CUTRACER_DELAY_NS=1000000  (1ms delay)\n");
    exit(1);
  }

  // Validate range: nanosleep uses uint32_t, so delay must fit in 32 bits.
  if (delay_val > UINT32_MAX) {
    fprintf(stderr, "FATAL: Delay value %lu exceeds maximum value of %u.\n", delay_val, UINT32_MAX);
    exit(1);
  }

  g_delay_ns = (uint32_t)delay_val;

  // Get delay config dump output path
  get_var_str(delay_dump_path, "CUTRACER_DELAY_DUMP_PATH", "", "Output path to dump delay config JSON for replay");

  // Get delay load path (for replay mode)
  get_var_str(delay_load_path, "CUTRACER_DELAY_LOAD_PATH", "",
              "Load delay config JSON for replay mode (uses saved delay values instead of random)");

  // Validate that load and dump paths are not both set
  if (!delay_dump_path.empty() && !delay_load_path.empty()) {
    fprintf(stderr,
            "FATAL: Both CUTRACER_DELAY_DUMP_PATH and CUTRACER_DELAY_LOAD_PATH are set.\n"
            "Please use only one: DUMP for recording, LOAD for replay.\n");
    exit(1);
  }
}

void init_analysis(const std::string& analysis_str) {
  enabled_analysis_types.clear();

  if (analysis_str.empty()) {
    loprintf("No analysis types specified.\n");
    return;
  }
  loprintf("Using analysis types: %s\n", analysis_str.c_str());

  // Parse comma-separated values
  if (analysis_str.find("proton_instr_histogram") != std::string::npos) {
    enabled_analysis_types.insert(AnalysisType::PROTON_INSTR_HISTOGRAM);
    loprintf("  - Enabled: proton_instr_histogram\n");

    // If proton_instr_histogram is enabled, force opcode_only instrumentation
    if (!is_instrument_type_enabled(InstrumentType::OPCODE_ONLY)) {
      enabled_instrument_types.insert(InstrumentType::OPCODE_ONLY);
      loprintf(
          "`proton_instr_histogram` analysis is enabled, forcing `opcode_only` "
          "instrumentation.\n");
    }
  }

  // deadlock_detection: enable analysis type and ensure REG_TRACE is on
  if (analysis_str.find("deadlock_detection") != std::string::npos) {
    enabled_analysis_types.insert(AnalysisType::DEADLOCK_DETECTION);
    loprintf("  - Enabled: deadlock_detection\n");
    if (!is_instrument_type_enabled(InstrumentType::REG_TRACE)) {
      enabled_instrument_types.insert(InstrumentType::REG_TRACE);
      loprintf("  - deadlock_detection: forcing reg_trace instrumentation\n");
    }
  }

  // random_delay: enable analysis type and ensure RANDOM_DELAY instrumentation is on
  // Note: CUTRACER_DELAY_NS is validated later in init_config_from_env()
  if (analysis_str.find("random_delay") != std::string::npos) {
    enabled_analysis_types.insert(AnalysisType::RANDOM_DELAY);
    loprintf("  - Enabled: random_delay\n");
    if (!is_instrument_type_enabled(InstrumentType::RANDOM_DELAY)) {
      enabled_instrument_types.insert(InstrumentType::RANDOM_DELAY);
      loprintf("  - random_delay: forcing random_delay instrumentation\n");
    }
  }
}

/**
 * @brief Check if a specific instrumentation type is enabled
 *
 * @param type The instrumentation type to check
 * @return true if the instrumentation type is enabled
 */
bool is_instrument_type_enabled(InstrumentType type) {
  return enabled_instrument_types.count(type);
}

/**
 * @brief Check if any instrumentation type is enabled
 *
 * This function checks if CUTRACER_INSTRUMENT was set to a non-empty value,
 * meaning at least one instrumentation type (reg_trace, mem_addr_trace, etc.)
 * is enabled. This is used to decide whether to create TraceWriter instances -
 * if no instrumentation is enabled, there's no point creating trace files
 * since they will be empty.
 *
 * @return true if at least one instrumentation type is enabled
 */
bool has_any_instrumentation_enabled() {
  return !enabled_instrument_types.empty();
}

bool is_analysis_type_enabled(AnalysisType type) {
  return enabled_analysis_types.count(type);
}

/**
 * @brief Initialize instruction category filtering from environment variable
 *
 * Parses CUTRACER_INSTR_CATEGORIES environment variable and sets up enabled categories.
 * If empty, no category filtering is applied (all instructions are instrumented).
 * If set, only instructions matching the specified categories are instrumented.
 *
 * @param categories_str Comma-separated category names (e.g., "mma,tma,sync")
 */
void init_instr_categories(const std::string& categories_str) {
  enabled_instr_categories.clear();

  if (categories_str.empty()) {
    // No category filtering - all instructions will be instrumented
    return;
  }

  loprintf("Using instruction category filters: %s\n", categories_str.c_str());

  // Parse comma-separated values (case-insensitive)
  std::string str = categories_str;

  // Convert to lowercase for case-insensitive matching
  for (char& c : str) {
    c = std::tolower(c);
  }

  if (str.find("mma") != std::string::npos) {
    enabled_instr_categories.insert(InstrCategory::MMA);
    loprintf("  - Enabled category: MMA (UTCMMA, HMMA, etc.)\n");
  }
  if (str.find("tma") != std::string::npos) {
    enabled_instr_categories.insert(InstrCategory::TMA);
    loprintf("  - Enabled category: TMA (UTMALDG, UTMASTG, etc.)\n");
  }
  if (str.find("sync") != std::string::npos) {
    enabled_instr_categories.insert(InstrCategory::SYNC);
    loprintf("  - Enabled category: SYNC (WARPGROUP.DEPBAR, etc.)\n");
  }

  if (enabled_instr_categories.empty()) {
    loprintf("WARNING: CUTRACER_INSTR_CATEGORIES set but no valid categories found.\n");
    loprintf("         Valid categories: mma, tma, sync\n");
  }
}

/**
 * @brief Check if a specific instruction category should be instrumented
 *
 * @param category The instruction category to check
 * @return true if the category should be instrumented (either no filter or category is enabled)
 */
bool should_instrument_category(InstrCategory category) {
  // If no category filter is set, instrument all categories
  if (enabled_instr_categories.empty()) {
    return true;
  }
  // Otherwise, only instrument if the category is explicitly enabled
  return enabled_instr_categories.count(category) > 0;
}

/**
 * @brief Check if category-based filtering is enabled
 *
 * @return true if CUTRACER_INSTR_CATEGORIES was set to a non-empty value
 */
bool has_category_filter_enabled() {
  return !enabled_instr_categories.empty();
}

/**
 * @brief Initialize trace output directory from environment variable
 *
 * Reads CUTRACER_TRACE_OUTPUT_DIR and validates:
 * 1. The path exists
 * 2. It is a directory
 * 3. The directory has write permission
 * If not set, trace files will be written to the current directory.
 */
void init_trace_output_dir() {
  get_var_str(trace_output_dir, "CUTRACER_TRACE_OUTPUT_DIR", "",
              "Output directory for trace files (default: current directory)");

  if (!trace_output_dir.empty()) {
    fs::path dir_path(trace_output_dir);

    if (!fs::exists(dir_path)) {
      fprintf(stderr,
              "FATAL: CUTRACER_TRACE_OUTPUT_DIR '%s' does not exist.\n"
              "Please create the directory first or specify a valid directory.\n",
              trace_output_dir.c_str());
      exit(1);
    }
    if (!fs::is_directory(dir_path)) {
      fprintf(stderr,
              "FATAL: CUTRACER_TRACE_OUTPUT_DIR '%s' is not a directory.\n"
              "Please specify a valid directory.\n",
              trace_output_dir.c_str());
      exit(1);
    }
    auto perms = fs::status(dir_path).permissions();
    if ((perms & fs::perms::owner_write) == fs::perms::none) {
      fprintf(stderr,
              "FATAL: CUTRACER_TRACE_OUTPUT_DIR '%s' is not writable.\n"
              "Please check directory permissions (chmod) or choose a different directory.\n",
              trace_output_dir.c_str());
      exit(1);
    }
  }
}

// Initialize all configuration variables
void init_config_from_env() {
  // Enable device memory allocation
  setenv("CUDA_MANAGED_FORCE_DEVICE_ALLOC", "1", 1);
  // Initialize log handle
  init_log_handle();
  // Get other configuration variables
  get_var_int(verbose, "TOOL_VERBOSE", 0, "Enable verbosity inside the tool");
  int dump_cubin_int = 0;
  get_var_int(dump_cubin_int, "CUTRACER_DUMP_CUBIN", 0, "Dump cubin files for instrumented kernels");
  dump_cubin = (dump_cubin_int != 0);
  // If INSTRS is not set, fall back to the old INSTR_BEGIN/INSTR_END behavior
  get_var_uint32(instr_begin_interval, "INSTR_BEGIN", 0,
                 "Beginning of the instruction interval where to apply instrumentation");
  get_var_uint32(instr_end_interval, "INSTR_END", UINT32_MAX,
                 "End of the instruction interval where to apply instrumentation");
  std::string instrument_str;
  get_var_str(instrument_str, "CUTRACER_INSTRUMENT", "",
              "Instrumentation types to enable (opcode_only,reg_trace,mem_addr_trace)");
  std::string kernel_filters_env;
  get_var_str(kernel_filters_env, "KERNEL_FILTERS", "", "Kernel name filters");
  std::string analysis_str;
  get_var_str(analysis_str, "CUTRACER_ANALYSIS", "",
              "Analysis types to enable (proton_instr_histogram, deadlock_detection, random_delay)");

  //===== Initializations ==========
  // Get kernel name filters
  parse_kernel_filters(kernel_filters_env);

  // Clear enabled types at the beginning
  enabled_instrument_types.clear();

  // Initialize analysis first, as it may enable instrumentation types
  init_analysis(analysis_str);
  // Initialize instrumentation from user settings
  init_instrumentation(instrument_str);

  // Trace format configuration
  get_var_int(trace_format_ndjson, "TRACE_FORMAT_NDJSON", 1,
              "Trace format: 0=text, 1=NDJSON+Zstd, 2=NDJSON only, 3=CLP Archive");

  // Validate trace format range
  if (trace_format_ndjson < 0 || trace_format_ndjson > 3) {
    printf("WARNING: Invalid TRACE_FORMAT_NDJSON=%d. Using default=1 (NDJSON+Zstd).\n", trace_format_ndjson);
    trace_format_ndjson = 1;
  }

  // Zstd compression level (only used when trace_format_ndjson == 1)
  get_var_int(zstd_compression_level, "CUTRACER_ZSTD_LEVEL", 22, "Zstd compression level (1-22, default 22)");

  // Validate compression level range
  if (zstd_compression_level < 1 || zstd_compression_level > 22) {
    printf("WARNING: Invalid CUTRACER_ZSTD_LEVEL=%d. Using default=22.\n", zstd_compression_level);
    zstd_compression_level = 22;
  }

  // Parse and validate delay configuration (includes config paths)
  parse_delay_config();

  // Initialize trace output directory
  init_trace_output_dir();

  // Parse instruction category filters (optional)
  std::string instr_categories_str;
  get_var_str(instr_categories_str, "CUTRACER_INSTR_CATEGORIES", "",
              "Instruction categories to instrument (mma,tma,sync). Empty = all instructions");
  init_instr_categories(instr_categories_str);

  std::string pad(100, '-');
  loprintf("%s\n", pad.c_str());
}

```

`src/inject_funcs.cu`:

```cu
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-FileCopyrightText: Copyright (c) 2019 NVIDIA CORPORATION & AFFILIATES.
 * SPDX-License-Identifier: MIT AND BSD-3-Clause
 *
 * This source code contains modifications by Meta Platforms, Inc. licensed under MIT,
 * based on original NVIDIA nvbit sample code licensed under BSD-3-Clause.
 * See LICENSE file in the root directory for Meta's license terms.
 * See LICENSE-BSD file in the root directory for NVIDIA's license terms.
 */

#include <cuda_runtime.h>
#include <stdarg.h>
#include <stdint.h>

#include "utils/utils.h"

/* for channel */
#include "common.h"
#include "utils/channel.hpp"

/* Based on NVIDIA NVBit reg_trace example with Meta modifications for
message type, unified register, and kernel launch id support*/
extern "C" __device__ __noinline__ void instrument_reg_val(int pred, int opcode_id, uint64_t pchannel_dev,
                                                           uint64_t kernel_launch_id, uint64_t pc, int32_t num_regs,
                                                           int32_t num_uregs, ...) {
  if (!pred) {
    return;
  }

  int active_mask = __ballot_sync(__activemask(), 1);
  const int laneid = get_laneid();
  const int first_laneid = __ffs(active_mask) - 1;

  reg_info_t ri;

  ri.header.type = MSG_TYPE_REG_INFO;

  int4 cta = get_ctaid();
  ri.cta_id_x = cta.x;
  ri.cta_id_y = cta.y;
  ri.cta_id_z = cta.z;
  ri.warp_id = get_global_warp_id();
  ri.opcode_id = opcode_id;
  ri.num_regs = num_regs;
  ri.num_uregs = num_uregs;
  ri.kernel_launch_id = kernel_launch_id;
  ri.pc = pc;

  if (num_regs || num_uregs) {
    // Initialize variable argument list
    va_list vl;
    va_start(vl, num_uregs);

    // First batch: collect register values
    for (int i = 0; i < num_regs; i++) {
      uint32_t val = va_arg(vl, uint32_t);

      /* collect register values from other threads */
      for (int tid = 0; tid < 32; tid++) {
        ri.reg_vals[tid][i] = __shfl_sync(active_mask, val, tid);
      }
    }
    // Only the first thread in the warp needs to process unified registers
    if (first_laneid == laneid) {
      for (int i = 0; i < num_uregs; i++) {
        ri.ureg_vals[i] = va_arg(vl, uint32_t);
      }
    }

    va_end(vl);
  }

  if (first_laneid == laneid) {
    ChannelDev* channel_dev = (ChannelDev*)pchannel_dev;
    channel_dev->push(&ri, sizeof(reg_info_t));
  }
}

/* Based on NVIDIA NVBit mem_trace example with Meta modifications for message type */
extern "C" __device__ __noinline__ void instrument_mem(int pred, int opcode_id, uint64_t addr,
                                                       uint64_t kernel_launch_id, uint64_t pc, uint64_t pchannel_dev) {
  /* if thread is predicated off, return */
  if (!pred) {
    return;
  }

  int active_mask = __ballot_sync(__activemask(), 1);
  const int laneid = get_laneid();
  const int first_laneid = __ffs(active_mask) - 1;

  mem_addr_access_t ma;

  ma.header.type = MSG_TYPE_MEM_ADDR_ACCESS;

  /* collect memory address information from other threads */
  for (int i = 0; i < 32; i++) {
    ma.addrs[i] = __shfl_sync(active_mask, addr, i);
  }
  ma.kernel_launch_id = kernel_launch_id;
  int4 cta = get_ctaid();
  ma.cta_id_x = cta.x;
  ma.cta_id_y = cta.y;
  ma.cta_id_z = cta.z;
  ma.pc = pc;
  ma.warp_id = get_global_warp_id();
  ma.opcode_id = opcode_id;

  /* first active lane pushes information on the channel */
  if (first_laneid == laneid) {
    ChannelDev* channel_dev = (ChannelDev*)pchannel_dev;
    channel_dev->push(&ma, sizeof(mem_addr_access_t));
  }
}

extern "C" __device__ __noinline__ void instrument_opcode(int pred, int opcode_id, uint64_t pchannel_dev,
                                                          uint64_t kernel_launch_id, uint64_t pc) {
  if (!pred) {
    return;
  }

  int active_mask = __ballot_sync(__activemask(), 1);
  const int laneid = get_laneid();
  const int first_laneid = __ffs(active_mask) - 1;

  opcode_only_t oi;
  oi.header.type = MSG_TYPE_OPCODE_ONLY;

  int4 cta = get_ctaid();
  oi.cta_id_x = cta.x;
  oi.cta_id_y = cta.y;
  oi.cta_id_z = cta.z;
  oi.warp_id = get_global_warp_id();
  oi.opcode_id = opcode_id;
  oi.kernel_launch_id = kernel_launch_id;
  oi.pc = pc;

  if (first_laneid == laneid) {
    ChannelDev* channel_dev = (ChannelDev*)pchannel_dev;
    channel_dev->push(&oi, sizeof(opcode_only_t));
  }
}

/**
 * @brief Helper function to read a 32-bit value from shared memory using byte-level access.
 *
 * Uses __cvta_shared_to_generic to convert shared memory segment address
 * to a generic pointer, then reads bytes individually to handle arbitrary alignment.
 * This avoids "misaligned address" errors that occur when reading uint32_t from
 * non-4-byte-aligned addresses.
 *
 * @param smemAddr The shared memory segment address (not a generic pointer)
 * @return The 32-bit value at the given shared memory address (little-endian)
 */
__device__ __forceinline__ uint32_t loadSmemValue32(uint64_t smemAddr) {
  const auto ptr = static_cast<unsigned char*>(__cvta_shared_to_generic(static_cast<unsigned>(smemAddr)));
  // Read bytes individually and assemble into uint32_t (little-endian)
  uint32_t value = 0;
  value |= static_cast<uint32_t>(ptr[0]);
  value |= static_cast<uint32_t>(ptr[1]) << 8;
  value |= static_cast<uint32_t>(ptr[2]) << 16;
  value |= static_cast<uint32_t>(ptr[3]) << 24;
  return value;
}

/* Memory space constants matching InstrType::MemorySpace */
#define MEM_SPACE_NONE 0
#define MEM_SPACE_GLOBAL 1
#define MEM_SPACE_SHARED 4
#define MEM_SPACE_LOCAL 5

/**
 * @brief Device function to trace memory access with values.
 *
 * This function collects both memory addresses AND values for detailed data flow analysis.
 * It uses IPOINT_AFTER timing to capture values after the memory operation completes.
 *
 * For Global/Local memory: values are read from registers (passed as variadic args)
 * For Shared memory: values are read directly from memory using address space conversion
 *
 * @param pred Guard predicate value
 * @param opcode_id The opcode identifier for this instruction
 * @param addr Memory address accessed by this thread
 * @param access_size Access size in bytes (1, 2, 4, 8, 16)
 * @param mem_space Memory space type (GLOBAL=1, SHARED=4, LOCAL=5)
 * @param is_load 1 for load operations, 0 for store operations
 * @param num_regs Number of register values following (variadic)
 * @param pchannel_dev Pointer to the channel device
 * @param kernel_launch_id Global kernel launch identifier
 * @param pc Program counter (instruction offset)
 * @param ... Variadic register values (num_regs uint32_t values)
 */
extern "C" __device__ __noinline__ void instrument_mem_value(int pred, int opcode_id, uint64_t addr, int access_size,
                                                             int mem_space, int is_load, int num_regs,
                                                             uint64_t pchannel_dev, uint64_t kernel_launch_id,
                                                             uint64_t pc, ...) {
  if (!pred) {
    return;
  }

  int active_mask = __ballot_sync(__activemask(), 1);
  const int laneid = get_laneid();
  const int first_laneid = __ffs(active_mask) - 1;

  mem_value_access_t mv;
  mv.header.type = MSG_TYPE_MEM_VALUE_ACCESS;

  // Fill basic fields
  mv.kernel_launch_id = kernel_launch_id;
  int4 cta = get_ctaid();
  mv.cta_id_x = cta.x;
  mv.cta_id_y = cta.y;
  mv.cta_id_z = cta.z;
  mv.pc = pc;
  mv.warp_id = get_global_warp_id();
  mv.opcode_id = opcode_id;
  mv.mem_space = mem_space;
  mv.is_load = is_load;
  mv.access_size = access_size;

  // Collect addresses from all lanes
  for (int i = 0; i < 32; i++) {
    mv.addrs[i] = __shfl_sync(active_mask, addr, i);
  }

  // Initialize values array to zero
  for (int i = 0; i < 32; i++) {
    for (int j = 0; j < 4; j++) {
      mv.values[i][j] = 0;
    }
  }

  // Collect values based on memory space type
  if (mem_space == MEM_SPACE_SHARED) {
    // Shared memory: use address space conversion to read values directly
    // Read up to 4 32-bit words based on access_size
    int words_to_read = (access_size + 3) / 4;
    if (words_to_read > 4) words_to_read = 4;

    uint32_t my_values[4] = {0, 0, 0, 0};
    if (addr != 0) {
      for (int w = 0; w < words_to_read; w++) {
        my_values[w] = loadSmemValue32(addr + w * 4);
      }
    }

    // Broadcast values from each lane
    for (int i = 0; i < 32; i++) {
      for (int w = 0; w < words_to_read; w++) {
        mv.values[i][w] = __shfl_sync(active_mask, my_values[w], i);
      }
    }
  } else {
    // Global/Local memory: read values from variadic register arguments
    if (num_regs > 0) {
      va_list vl;
      va_start(vl, pc);

      int regs_to_process = num_regs;
      if (regs_to_process > 4) regs_to_process = 4;

      for (int r = 0; r < regs_to_process; r++) {
        uint32_t val = va_arg(vl, uint32_t);
        // Broadcast this register value from all lanes
        for (int tid = 0; tid < 32; tid++) {
          mv.values[tid][r] = __shfl_sync(active_mask, val, tid);
        }
      }

      va_end(vl);
    }
  }

  // First active lane pushes the data to the channel
  if (first_laneid == laneid) {
    ChannelDev* channel_dev = (ChannelDev*)pchannel_dev;
    channel_dev->push(&mv, sizeof(mem_value_access_t));
  }
}

/**
 * @brief Device function to inject delay before any instrumented instruction.
 *
 * Injects a nanosleep delay before an instruction to expose potential race
 * conditions. The delay value is computed on the host during instrumentation,
 * so each instruction receives a unique, fixed delay value.
 *
 * Uses __nanosleep() intrinsic on SM 7.0+ (Volta and later), with a fallback
 * to busy-wait using clock64() on older architectures.
 *
 * @param pred Guard predicate value (from nvbit_add_call_arg_guard_pred_val)
 * @param delay_ns Delay in nanoseconds (passed from host, 0 = no delay)
 */
extern "C" __device__ __noinline__ void instrument_delay(int pred, uint32_t delay_ns) {
  if (!pred) {
    return;
  }

#if defined(__CUDA_ARCH__) && (__CUDA_ARCH__ >= 700)
  __nanosleep(delay_ns);
#else
  // Fallback: busy-wait using clock64 (approximate 2 cycles per ns)
  uint64_t delay_cycles = delay_ns * 2;
  uint64_t start = clock64();
  while ((clock64() - start) < delay_cycles) {
  }
#endif
}

```

`src/instrument.cu`:

```cu
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-FileCopyrightText: Copyright (c) 2019 NVIDIA CORPORATION & AFFILIATES.
 * SPDX-License-Identifier: MIT AND BSD-3-Clause
 *
 * This source code contains modifications by Meta Platforms, Inc. licensed
 * under MIT, based on original NVIDIA NVBit sample code licensed under
 * BSD-3-Clause. See LICENSE file in the root directory for Meta's license
 * terms. See LICENSE-BSD file in the root directory for NVIDIA's license terms.
 */

#include <cstdlib>

#include "analysis.h"
#include "instrument.h"
#include "log.h"
#include "nvbit.h"

/**
 * @brief Instruments an instruction to record its opcode for lightweight analysis.
 *
 * This function was developed by Meta to support analyses like instruction
 * histograms, where only the instruction's identity is needed, not its operand
 * values.
 *
 * It injects a call to the `instrument_opcode` device function, passing the
 * opcode ID, program counter (PC), and global kernel launch ID.
 */
void instrument_opcode_only(Instr* instr, int opcode_id, CTXstate* ctx_state) {
  /* insert call to the instrumentation function with its arguments */
  nvbit_insert_call(instr, "instrument_opcode", IPOINT_BEFORE);
  /* guard predicate value */
  nvbit_add_call_arg_guard_pred_val(instr);
  /* opcode id */
  nvbit_add_call_arg_const_val32(instr, opcode_id);
  /* pass the pointer to the channel on the device */
  nvbit_add_call_arg_const_val64(instr, (uint64_t)ctx_state->channel_dev);
  /* add "space" for kernel function pointer that will be set
   * at launch time (64 bit value at offset 0 of the dynamic
   * arguments). it is used to pass global kernel launch id*/
  nvbit_add_call_arg_launch_val64(instr, 0);
  /* add instruction offset */
  nvbit_add_call_arg_const_val64(instr, instr->getOffset());
}

/**
 * @brief Instruments an instruction to trace the values of its register operands.
 *
 * This function is based on the instrumentation logic from NVIDIA's
 * `record_reg_vals` example (`third_party/nvbit/tools/record_reg_vals/record_reg_vals.cu`).
 * It injects a call to the `instrument_reg_val` device function to capture operand values.
 *
 * Key enhancements by Meta include:
 *  - **Unified Register (UREG) Support**: Added tracing for UREGs, which was
 *    not present in the original example.
 *  - **Enhanced Context**: Passes the global kernel launch ID and the
 *    instruction's program counter (PC) to correlate data more effectively
 *    during analysis.
 *  - **Refactoring**: Encapsulated the logic into this dedicated function,
 *    separating it from the main instruction iteration loop in `cutracer.cu`.
 */
void instrument_register_trace(Instr* instr, int opcode_id, CTXstate* ctx_state, const OperandLists& operands) {
  /* insert call to the instrumentation function with its arguments */
  nvbit_insert_call(instr, "instrument_reg_val", IPOINT_BEFORE);
  /* guard predicate value */
  nvbit_add_call_arg_guard_pred_val(instr);
  /* opcode id */
  nvbit_add_call_arg_const_val32(instr, opcode_id);
  /* add pointer to channel_dev*/
  nvbit_add_call_arg_const_val64(instr, (uint64_t)ctx_state->channel_dev);
  /* add "space" for kernel function pointer that will be set
   * at launch time (64 bit value at offset 0 of the dynamic
   * arguments). it is used to pass global kernel launch id*/
  nvbit_add_call_arg_launch_val64(instr, 0);
  /* add instruction offset */
  nvbit_add_call_arg_const_val64(instr, instr->getOffset());
  /* how many register values are passed next */
  nvbit_add_call_arg_const_val32(instr, operands.reg_nums.size());
  nvbit_add_call_arg_const_val32(instr, operands.ureg_nums.size());

  // Pass register values (variadic)
  for (int num : operands.reg_nums) {
    /* last parameter tells it is a variadic parameter passed to
     * the instrument function record_reg_val() */
    nvbit_add_call_arg_reg_val(instr, num, true);
  }
  for (int num : operands.ureg_nums) {
    nvbit_add_call_arg_ureg_val(instr, num, true);
  }
}

/**
 * @brief Instruments a memory instruction to trace memory access details.
 *
 * This function is based on the instrumentation logic from NVIDIA's `mem_trace`
 * example. It injects a call to the `instrument_mem` device function.
 *
 * Meta's enhancements include:
 *  - **Refactoring**: Moving the instrumentation logic from the main loop in
 *    `cutracer.cu` into this modular function.
 *  - **Contextual Information**: Passing the global kernel launch ID and the
 *    instruction's program counter (PC) alongside the memory address. This
 *    allows for more detailed analysis by linking each memory access to a
 *    specific kernel launch and instruction.
 *
 * Note: This function implements `mem_addr_trace` mode which only captures
 * memory addresses, not values. For value tracing, use `mem_value_trace` mode.
 */
void instrument_memory_addr_trace(Instr* instr, int opcode_id, CTXstate* ctx_state, int mref_idx) {
  /* insert call to the instrumentation function with its
   * arguments */
  nvbit_insert_call(instr, "instrument_mem", IPOINT_BEFORE);
  /* predicate value */
  nvbit_add_call_arg_guard_pred_val(instr);
  /* opcode id */
  nvbit_add_call_arg_const_val32(instr, opcode_id);
  /* memory reference 64 bit address */
  nvbit_add_call_arg_mref_addr64(instr, mref_idx);
  /* add "space" for kernel function pointer that will be set
   * at launch time (64 bit value at offset 0 of the dynamic
   * arguments). it is used to pass global kernel launch id*/
  nvbit_add_call_arg_launch_val64(instr, 0);
  /* add instruction offset */
  nvbit_add_call_arg_const_val64(instr, instr->getOffset());
  /* add pointer to channel_dev*/
  nvbit_add_call_arg_const_val64(instr, (uint64_t)ctx_state->channel_dev);
}

/**
 * @brief Check if an instruction should have delay injected.
 *
 * @param instr The instruction to check
 * @param patterns Vector of SASS substrings to match against
 * @return true if the instruction's SASS matches any pattern
 */
bool shouldInjectDelay(Instr* instr, const std::vector<const char*>& patterns) {
  const char* sass = instr->getSass();
  if (sass == nullptr) {
    return false;
  }

  for (const char* pattern : patterns) {
    if (strstr(sass, pattern) != nullptr) {
      return true;
    }
  }

  return false;
}

/**
 * @brief Instruments an instruction to inject a fixed delay.
 *
 * Inserts a call to the `instrument_delay` device function before the
 * instruction. The delay value is a fixed value determined by CUTRACER_DELAY_NS.
 *
 * @param instr The instruction to instrument
 * @param delay_ns Fixed delay in nanoseconds
 */
void instrument_delay_injection(Instr* instr, uint32_t delay_ns) {
  /* insert call to the instrumentation function with its arguments */
  nvbit_insert_call(instr, "instrument_delay", IPOINT_BEFORE);
  /* guard predicate value */
  nvbit_add_call_arg_guard_pred_val(instr);
  /* delay in nanoseconds */
  nvbit_add_call_arg_const_val32(instr, delay_ns);
}

/**
 * @brief Instruments a memory instruction to trace memory access with values.
 *
 * This function instruments memory instructions to capture both addresses AND
 * values for detailed data flow analysis. Unlike instrument_memory_trace() which
 * uses IPOINT_BEFORE, this function uses IPOINT_AFTER to capture values after
 * the memory operation completes.
 *
 * For Global/Local memory: values are read from registers
 *   - Load instructions: read from destination register (value is in reg after load)
 *   - Store instructions: read from source register (source reg is not modified by store)
 *
 * For Shared memory: values are read directly from memory using address space conversion
 *
 * @param instr The instruction to instrument
 * @param opcode_id The opcode identifier for this instruction
 * @param ctx_state The context state containing channel information
 * @param mref_idx Memory reference index
 * @param mem_space Memory space type (obtained via instr->getMemorySpace() in cutracer.cu)
 */
void instrument_memory_value_trace(Instr* instr, int opcode_id, CTXstate* ctx_state, int mref_idx, int mem_space) {
  bool is_load = instr->isLoad();
  int access_size = instr->getSize();

  // Use IPOINT_AFTER for consistent timing semantics
  nvbit_insert_call(instr, "instrument_mem_value", IPOINT_AFTER);

  // Guard predicate value
  nvbit_add_call_arg_guard_pred_val(instr);
  // Opcode id
  nvbit_add_call_arg_const_val32(instr, opcode_id);
  // Memory reference 64 bit address
  nvbit_add_call_arg_mref_addr64(instr, mref_idx);
  // Access size in bytes
  nvbit_add_call_arg_const_val32(instr, access_size);
  // Memory space type (GLOBAL=1, SHARED=4, LOCAL=5)
  nvbit_add_call_arg_const_val32(instr, mem_space);
  // Is load operation (1=load, 0=store)
  nvbit_add_call_arg_const_val32(instr, is_load ? 1 : 0);

  // For Global/Local memory, we need to pass register values
  // For Shared memory, device function will read directly from memory
  int num_regs = 0;
  std::vector<int> value_reg_nums;

  // InstrType::MemorySpace::SHARED = 4
  if (mem_space != 4) {
    // Find the register operand that holds the value
    // For load: destination register (usually first REG operand)
    // For store: source register (usually last REG operand that's not part of address)
    int num_operands = instr->getNumOperands();
    int regs_needed = (access_size + 3) / 4;  // Number of 32-bit registers needed
    if (regs_needed > 4) regs_needed = 4;     // Cap at 4 (128 bits)

    for (int i = 0; i < num_operands && (int)value_reg_nums.size() < regs_needed; i++) {
      const InstrType::operand_t* op = instr->getOperand(i);
      if (op->type == InstrType::OperandType::REG) {
        // For load: first REG operand is the destination
        // For store: we want the source data register, which is typically at a different position
        // The MREF operand contains address, REG operands are data
        if (is_load) {
          // First REG operand is destination for loads
          for (int reg_idx = 0; reg_idx < regs_needed && (int)value_reg_nums.size() < regs_needed; reg_idx++) {
            value_reg_nums.push_back(op->u.reg.num + reg_idx);
          }
          break;  // Found destination register(s)
        } else {
          // For stores, we want the source data register
          // Skip if this looks like an address register (part of MREF)
          // Typically for stores like STG [R10], R8 - R8 is the source
          // We collect all REG operands that aren't part of address
          for (int reg_idx = 0; reg_idx < regs_needed && (int)value_reg_nums.size() < regs_needed; reg_idx++) {
            value_reg_nums.push_back(op->u.reg.num + reg_idx);
          }
        }
      }
    }
    num_regs = value_reg_nums.size();
  }

  // Number of register values to follow
  nvbit_add_call_arg_const_val32(instr, num_regs);
  // Pointer to channel_dev
  nvbit_add_call_arg_const_val64(instr, (uint64_t)ctx_state->channel_dev);
  // Kernel launch id (set at launch time)
  nvbit_add_call_arg_launch_val64(instr, 0);
  // Instruction offset (PC)
  nvbit_add_call_arg_const_val64(instr, instr->getOffset());

  // Add register values as variadic arguments
  for (int reg_num : value_reg_nums) {
    nvbit_add_call_arg_reg_val(instr, reg_num, true);
  }
}

```

`src/log.cu`:

```cu
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 *
 * See LICENSE file in the root directory for Meta's license terms.
 */

#include <assert.h>
#include <stdarg.h>
#include <stdint.h>
#include <stdio.h>
#include <string.h>
#include <sys/stat.h>
#include <time.h>
#include <unistd.h>

#include <sstream>
#include <string>

/* nvbit interface file */
#include "nvbit.h"

/* include environment configuration */
#include "env_config.h"

/* include log handle header */
#include "log.h"

/**
 * Builds a deterministic base filename for a kernel's trace log.
 *
 * The resulting string embeds:
 *   - The kernel_checksum (FNV-1a hash of kernel name + SASS) for identification
 *   - The iteration number (decimal)
 *   - A truncated copy (first 150 chars) of the mangled name for readability
 *
 * If trace_output_dir is set (via CUTRACER_TRACE_OUTPUT_DIR), the filename
 * will be prefixed with that directory path.
 *
 * Example: "kernel_7fa21c3_iter42__Z23my_kernelPiS_..."
 * With path: "/tmp/traces/kernel_7fa21c3_iter42__Z23my_kernelPiS_..."
 */
std::string generate_kernel_log_basename(CUcontext ctx, CUfunction func, uint32_t iteration,
                                         const std::string& kernel_checksum) {
  const char* mangled_name_raw = nvbit_get_func_name(ctx, func, true);
  if (!mangled_name_raw) {
    mangled_name_raw = "unknown_kernel";
  }

  std::string mangled_name(mangled_name_raw);

  // Truncate the name for the filename string part
  std::string truncated_name = mangled_name.substr(0, 150);

  std::stringstream ss;

  // Prepend trace_output_dir if specified
  if (!trace_output_dir.empty()) {
    ss << trace_output_dir;
    // Ensure path ends with a separator
    if (trace_output_dir.back() != '/') {
      ss << "/";
    }
  }

  // Format to hex for the hash
  ss << "kernel_" << kernel_checksum << "_iter" << std::dec << iteration << "_" << truncated_name;

  return ss.str();
}

/* ===== Global Variables ===== */

// The main log file for the entire process run
static FILE* g_main_log_file = NULL;
// The currently active log file for kernel traces
static FILE* g_kernel_log_file = NULL;

/* ===== Utility Functions for Logging ===== */

/**
 * @brief Base function for formatted output. Uses va_list to avoid re-formatting.
 *
 * @param file_output if true, output to the active log file
 * @param stdout_output if true, output to stdout
 * @param format format string
 * @param args variable argument list
 */
static void vfprintf_base(bool file_output, bool stdout_output, const char* format, va_list args) {
  if (!file_output && !stdout_output) {
    return;
  }

  va_list args_copy;
  va_copy(args_copy, args);

  int needed = vsnprintf(nullptr, 0, format, args);
  if (needed < 0) {
    va_end(args_copy);
    return;
  }

  std::string output_buffer(needed + 1, '\0');
  vsnprintf(output_buffer.data(), needed + 1, format, args_copy);
  va_end(args_copy);

  if (stdout_output) {
    fprintf(stdout, "%s", output_buffer.c_str());
  }

  if (file_output && g_main_log_file) {
    fprintf(g_main_log_file, "%s", output_buffer.c_str());
  }
}

void lprintf(const char* format, ...) {
  va_list args;
  va_start(args, format);
  vfprintf_base(true, false, format, args);
  va_end(args);
}

void oprintf(const char* format, ...) {
  va_list args;
  va_start(args, format);
  vfprintf_base(false, true, format, args);
  va_end(args);
}

void loprintf(const char* format, ...) {
  va_list args;
  va_start(args, format);
  vfprintf_base(true, true, format, args);
  // Flush the main log file if it exists
  if (!g_main_log_file) {
    oprintf("ERROR: Main log file not initialized before loprintf\n");
  }

  va_end(args);
}

void trace_lprintf(const char* format, ...) {
  if (!g_kernel_log_file) {
    oprintf("ERROR: Kernel trace log file not initialized before trace_lprintf\n");
    return;
  }

  va_list args;
  va_start(args, format);
  vfprintf(g_kernel_log_file, format, args);
  va_end(args);
}

/* ===== File Management Functions ===== */

void log_open_kernel_file(CUcontext_ptr ctx, CUfunction_ptr func, uint32_t iteration,
                          const std::string& kernel_checksum) {
  // close previous log file if it's open
  log_close_kernel_file();

  std::string basename = generate_kernel_log_basename((CUcontext)ctx, (CUfunction)func, iteration, kernel_checksum);
  std::string log_filename = basename + ".log";

  g_kernel_log_file = fopen(log_filename.c_str(), "w");
  if (g_kernel_log_file) {
    loprintf("Opened kernel trace log: %s\n", log_filename.c_str());
  } else {
    oprintf("ERROR: Failed to open kernel trace log file: %s\n", log_filename.c_str());
  }
}

void log_close_kernel_file() {
  if (g_kernel_log_file) {
    fclose(g_kernel_log_file);
    g_kernel_log_file = NULL;
  }
}

void init_log_handle() {
  // Get current timestamp for filename
  time_t now = time(0);
  struct tm* timeinfo = localtime(&now);
  char timestamp[32];
  strftime(timestamp, sizeof(timestamp), "%Y%m%d_%H%M%S", timeinfo);

  char main_log_filename[256];
  snprintf(main_log_filename, sizeof(main_log_filename), "cutracer_main_%s.log", timestamp);

  g_main_log_file = fopen(main_log_filename, "w");
  if (!g_main_log_file) {
    // Fallback to stdout if file creation fails
    g_main_log_file = stdout;
    oprintf("WARNING: Failed to create main log file. Falling back to stdout.\n");
  }

  loprintf("Log handle system initialized. Main log is %s.\n",
           (g_main_log_file == stdout) ? "stdout" : main_log_filename);
}

void cleanup_log_handle() {
  log_close_kernel_file();

  if (g_main_log_file && g_main_log_file != stdout) {
    fclose(g_main_log_file);
  }

  g_main_log_file = NULL;

  oprintf_v("Log handle system cleaned up.\n");
}

```

`src/trace_writer.cpp`:

```cpp
// (c) Meta Platforms, Inc. and affiliates. Confidential and proprietary.

/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 */

#include "trace_writer.h"

#include <errno.h>
#include <fcntl.h>
#include <unistd.h>

#include <iomanip>
#include <nlohmann/json.hpp>
#include <sstream>
#include <stdexcept>

#include "env_config.h"

// ============================================================================
// Constructor & Destructor
// ============================================================================

TraceWriter::TraceWriter(const std::string& filename, int trace_mode, size_t buffer_threshold)
    : filename_(filename),
      file_handle_(nullptr),
      fd_(-1),
      buffer_threshold_(buffer_threshold),
      trace_mode_(static_cast<TraceMode>(trace_mode)),
      enabled_(true),
      zstd_ctx_(nullptr),
      compression_level_(zstd_compression_level) {  // Use configurable compression level from env_config

  // Validate trace mode
  if (trace_mode < 0 || trace_mode > 3) {
    fprintf(stderr, "TraceWriter: Invalid trace_mode %d (must be 0, 1, 2, or 3)\n", trace_mode);
    enabled_ = false;
    return;
  }

  // Determine filename based on trace mode
  std::string actual_filename;

  if (trace_mode_ == TraceMode::TEXT) {
    // Mode 0: Text format - use FILE* for fprintf compatibility
    actual_filename = filename + ".log";
    file_handle_ = fopen(actual_filename.c_str(), "a");

    if (!file_handle_) {
      fprintf(stderr, "TraceWriter: Failed to open %s\n", actual_filename.c_str());
      enabled_ = false;
      return;
    }

  } else if (trace_mode_ == TraceMode::COMPRESSED_NDJSON) {
    // Mode 1: NDJSON + Zstd compression - use POSIX write() for reliability
    actual_filename = filename + ".ndjson.zst";

    // Open with O_CREAT | O_WRONLY | O_APPEND
    fd_ = open(actual_filename.c_str(), O_CREAT | O_WRONLY | O_APPEND, 0644);
    if (fd_ < 0) {
      fprintf(stderr, "TraceWriter: Failed to open %s (errno=%d)\n", actual_filename.c_str(), errno);
      enabled_ = false;
      return;
    }

    // Initialize Zstd compression context
    zstd_ctx_ = ZSTD_createCCtx();
    if (!zstd_ctx_) {
      fprintf(stderr, "TraceWriter: Failed to initialize Zstd compression context\n");
      close(fd_);
      fd_ = -1;
      enabled_ = false;
      return;
    }

    // Pre-allocate compression buffer to avoid runtime allocation
    size_t max_compressed_size = ZSTD_compressBound(buffer_threshold);
    compressed_buffer_.resize(max_compressed_size);

  } else {  // trace_mode == UNCOMPRESSED_NDJSON || trace_mode == CLP
    // Mode 2/3: NDJSON uncompressed - use POSIX write() for reliability
    actual_filename = filename + ".ndjson";

    // Open with O_CREAT | O_WRONLY | O_APPEND
    fd_ = open(actual_filename.c_str(), O_CREAT | O_WRONLY | O_APPEND, 0644);
    if (fd_ < 0) {
      fprintf(stderr, "TraceWriter: Failed to open %s (errno=%d)\n", actual_filename.c_str(), errno);
      enabled_ = false;
      return;
    }
  }
}

TraceWriter::~TraceWriter() {
  // Flush any remaining data
  flush();

  // Close file handle (Mode 0)
  if (file_handle_) {
    fclose(file_handle_);
    file_handle_ = nullptr;
  }

  // Close file descriptor (Mode 1/2/3)
  if (fd_ >= 0) {
    close(fd_);
    fd_ = -1;
  }

  // If in CLP mode, write to CLP archive file after fd is closed
  if (trace_mode_ == TraceMode::CLP) {
    write_clp_archive();
  }

  // Release Zstd compression context
  if (zstd_ctx_) {
    ZSTD_freeCCtx(zstd_ctx_);
    zstd_ctx_ = nullptr;
  }
}

// ============================================================================
// Public API
// ============================================================================

void TraceWriter::write_metadata(const nlohmann::json& metadata) {
  if (!enabled_) return;
  // Text mode (mode 0) does not use json_buffer_; skip.
  if (trace_mode_ == TraceMode::TEXT) return;

  json_buffer_ += metadata.dump() + "\n";
}

bool TraceWriter::write_trace(const TraceRecord& record) {
  if (!enabled_) return false;

  // Dispatch based on trace mode
  if (trace_mode_ == TraceMode::TEXT) {
    write_text_format(record);
  } else {
    write_json_format(record);
  }

  return true;
}

void TraceWriter::flush() {
  // Dispatch based on trace mode
  if (trace_mode_ == TraceMode::COMPRESSED_NDJSON) {
    write_compressed();
  } else if (trace_mode_ == TraceMode::UNCOMPRESSED_NDJSON || trace_mode_ == TraceMode::CLP) {
    write_uncompressed();
  }
  // Mode 0 (text) doesn't buffer, so no flush needed
}

// ============================================================================
// Private Helpers
// ============================================================================

template <typename T>
void serialize_common_fields(nlohmann::json& j, const T* data) {
  j["grid_launch_id"] = data->kernel_launch_id;
  j["cta"] = {data->cta_id_x, data->cta_id_y, data->cta_id_z};
  j["warp"] = data->warp_id;
  j["opcode_id"] = data->opcode_id;

  std::stringstream pc_ss;
  pc_ss << "0x" << std::hex << data->pc;
  j["pc"] = pc_ss.str();
}

bool TraceWriter::write_data(const char* data, size_t size, const char* data_type) {
  if (fd_ < 0) return false;

  size_t total_written = 0;

  // Retry until all data is written or a fatal error occurs
  while (total_written < size) {
    ssize_t written = write(fd_, data + total_written, size - total_written);

    if (written < 0) {
      // Error occurred
      if (errno == EINTR) {
        // Interrupted by signal, retry
        continue;
      }
      // Fatal error
      fprintf(stderr, "TraceWriter: Fatal write error after %zu of %zu %s (errno=%d: %s)\n", total_written, size,
              data_type, errno, strerror(errno));
      enabled_ = false;
      return false;
    }

    // Check for write() returning 0 (no progress)
    if (written == 0) {
      fprintf(stderr, "TraceWriter: write() returned 0 after %zu of %zu %s (disk full or quota exceeded?)\n",
              total_written, size, data_type);
      enabled_ = false;
      return false;
    }

    total_written += written;
  }

  // Force data to disk (optional but recommended for reliability)
  fsync(fd_);
  return true;
}

void TraceWriter::write_uncompressed() {
  if (json_buffer_.empty() || !enabled_) return;

  // CRITICAL FIX: Move json_buffer_ to temp to prevent data corruption
  //
  // Problem: Previously used json_buffer_.data() directly during write_data(),
  // which caused random NULL bytes to appear at line starts in output files.
  //
  // Root cause: If json_buffer_ internal pointer becomes invalid during write
  // (e.g., memory reallocation, or buffer state inconsistency), we'd be
  // writing from a stale pointer. This manifested as:
  //   - Random single NULL bytes replacing '{' at JSON line starts
  //   - Different error lines on each run (non-deterministic)
  //   - Mode 1 unaffected (uses separate compressed_buffer_)
  //
  // Solution: std::move() transfers ownership to temp_buffer BEFORE write,
  // ensuring json_buffer_ is immediately emptied and safe for new data,
  // regardless of write_data() success/failure.
  std::string temp_buffer = std::move(json_buffer_);

  // json_buffer_ is now empty (moved-from state)
  // Write from the temporary buffer
  write_data(temp_buffer.data(), temp_buffer.size(), "bytes");
}

void TraceWriter::write_clp_archive() {
  // Write to CLP archive file from uncompressed ndjson file
  std::string uncompressed_ndjson_file = filename_ + ".ndjson";
  std::string clp_archive_file = filename_ + ".clp";
  std::string clp_run_cmd = "clp-s c --single-file-archive " + clp_archive_file + " " + uncompressed_ndjson_file;
  // run the clp command line to compress and remove the uncompressed ndjson file
  int rc = std::system(clp_run_cmd.c_str());
  if (rc != 0) {
    fprintf(stderr, "TraceWriter: clp-s command line failed with error code %d\n", rc);
    return;
  }
  // remove the uncompressed ndjson file
  int ec = std::remove(uncompressed_ndjson_file.c_str());
  if (ec != 0) {
    fprintf(stderr, "TraceWriter: Failed to remove uncompressed ndjson file with error code %d\n", ec);
    return;
  }
}

void TraceWriter::write_compressed() {
  if (json_buffer_.empty() || !enabled_ || !zstd_ctx_) return;

  // CRITICAL FIX: Move json_buffer_ to temp to prevent data loss
  //
  // Problem: Previously compressed json_buffer_ directly, then cleared only on
  // write success. This caused Mode 1 to lose ~50% of records (e.g., 6,835 of
  // 13,008 records written, 6,173 lost).
  //
  // Root cause: If compression or write failed, json_buffer_ wasn't cleared,
  // causing data to accumulate beyond buffer_threshold_ (1MB). When buffer
  // exceeded the pre-allocated compressed_buffer_ capacity, subsequent
  // compressions failed silently, and all remaining data was lost.
  //
  // Solution: std::move() transfers ownership to temp_buffer BEFORE compression,
  // ensuring json_buffer_ is immediately emptied. This prevents buffer overflow
  // and ensures consistent behavior whether compression/write succeeds or fails.
  std::string temp_buffer = std::move(json_buffer_);

  // json_buffer_ is now empty (moved-from state)
  // Compress from the temporary buffer
  size_t compressed_size = ZSTD_compressCCtx(zstd_ctx_, compressed_buffer_.data(), compressed_buffer_.size(),
                                             temp_buffer.data(), temp_buffer.size(), compression_level_);

  // Check for compression errors
  if (ZSTD_isError(compressed_size)) {
    fprintf(stderr, "TraceWriter: Zstd compression error: %s\n", ZSTD_getErrorName(compressed_size));
    return;  // temp_buffer is automatically destroyed, json_buffer_ remains empty
  }

  // Write the compressed data
  write_data(compressed_buffer_.data(), compressed_size, "compressed bytes");
}

void TraceWriter::serialize_reg_info(nlohmann::json& j, const reg_info_t* reg, const RegIndices* indices) {
  if (!reg) return;

  using json = nlohmann::json;

  serialize_common_fields(j, reg);

  // CRITICAL: Transpose register array
  // C layout: reg_vals[thread][reg] â†’ JSON: regs[reg][thread]
  // This ensures all values for the same register across all threads
  // are grouped together in the JSON output.
  json::array_t regs_array;
  for (int reg_idx = 0; reg_idx < reg->num_regs; reg_idx++) {
    json::array_t thread_vals;
    for (int thread = 0; thread < 32; thread++) {
      thread_vals.push_back(reg->reg_vals[thread][reg_idx]);
    }
    regs_array.push_back(thread_vals);
  }
  j["regs"] = regs_array;

  // Add register indices from CPU-side static mapping
  if (indices && !indices->reg_indices.empty()) {
    json::array_t regs_indices_array;
    for (auto idx : indices->reg_indices) {
      regs_indices_array.push_back(idx);
    }
    j["regs_indices"] = regs_indices_array;
  }

  // Add unified registers if present
  if (reg->num_uregs > 0) {
    json::array_t uregs_array;
    for (int i = 0; i < reg->num_uregs; i++) {
      uregs_array.push_back(reg->ureg_vals[i]);
    }
    j["uregs"] = uregs_array;

    // Add unified register indices from CPU-side static mapping
    if (indices && !indices->ureg_indices.empty()) {
      json::array_t uregs_indices_array;
      for (auto idx : indices->ureg_indices) {
        uregs_indices_array.push_back(idx);
      }
      j["uregs_indices"] = uregs_indices_array;
    }
  }
}

void TraceWriter::serialize_mem_access(nlohmann::json& j, const mem_addr_access_t* mem) {
  if (!mem) return;

  serialize_common_fields(j, mem);

  // Convert address array (32 addresses)
  std::vector<uint64_t> addrs(mem->addrs, mem->addrs + 32);
  j["addrs"] = addrs;
}

void TraceWriter::serialize_opcode_only(nlohmann::json& j, const opcode_only_t* opcode) {
  if (!opcode) return;

  serialize_common_fields(j, opcode);
}

void TraceWriter::serialize_mem_value_access(nlohmann::json& j, const mem_value_access_t* mem) {
  if (!mem) return;

  using json = nlohmann::json;

  serialize_common_fields(j, mem);

  // Memory access metadata
  j["mem_space"] = mem->mem_space;
  j["is_load"] = (mem->is_load == 1);
  j["access_size"] = mem->access_size;

  // Convert address array (32 addresses)
  std::vector<uint64_t> addrs(mem->addrs, mem->addrs + 32);
  j["addrs"] = addrs;

  // Convert values array (32 lanes x up to 4 registers based on access_size)
  // Only include registers needed for the access size
  int regs_needed = (mem->access_size + 3) / 4;
  if (regs_needed > 4) regs_needed = 4;

  json::array_t values_array;
  for (int lane = 0; lane < 32; lane++) {
    json::array_t lane_vals;
    for (int r = 0; r < regs_needed; r++) {
      lane_vals.push_back(mem->values[lane][r]);
    }
    values_array.push_back(lane_vals);
  }
  j["values"] = values_array;
}

void TraceWriter::serialize_tma_access(nlohmann::json& j, const tma_access_t* tma) {
  if (!tma) return;

  using json = nlohmann::json;

  serialize_common_fields(j, tma);

  // Descriptor address (as hex string for readability)
  std::stringstream desc_ss;
  desc_ss << "0x" << std::hex << tma->desc_addr;
  j["desc_addr"] = desc_ss.str();

  // Raw descriptor bytes (as hex strings for debugging)
  json::array_t desc_raw_array;
  for (int i = 0; i < 16; i++) {
    std::stringstream ss;
    ss << "0x" << std::hex << std::setfill('0') << std::setw(16) << tma->desc_raw[i];
    desc_raw_array.push_back(ss.str());
  }
  j["desc_raw"] = desc_raw_array;
}

// ============================================================================
// Format-specific output methods
// ============================================================================

void TraceWriter::write_text_format(const TraceRecord& record) {
  if (!file_handle_) return;

  // Dispatch by trace type
  switch (record.type) {
    case MSG_TYPE_REG_INFO: {
      const reg_info_t* ri = record.data.reg_info;

      // Print header line
      fprintf(file_handle_, "CTX %p - CTA %d,%d,%d - warp %d - %s:\n", record.context, ri->cta_id_x, ri->cta_id_y,
              ri->cta_id_z, ri->warp_id, record.sass_instruction.c_str());

      // Print register values
      for (int reg_idx = 0; reg_idx < ri->num_regs; reg_idx++) {
        fprintf(file_handle_, "  * ");
        for (int i = 0; i < 32; i++) {
          fprintf(file_handle_, "Reg%d_T%02d: 0x%08x ", reg_idx, i, ri->reg_vals[i][reg_idx]);
        }
        fprintf(file_handle_, "\n");
      }

      // Print uniform register values (if present)
      if (ri->num_uregs > 0) {
        fprintf(file_handle_, "  * UR: ");
        for (int i = 0; i < ri->num_uregs; i++) {
          fprintf(file_handle_, "UR%d: 0x%08x ", i, ri->ureg_vals[i]);
        }
        fprintf(file_handle_, "\n");
      }

      fprintf(file_handle_, "\n");
      break;
    }

    case MSG_TYPE_MEM_ADDR_ACCESS: {
      const mem_addr_access_t* mem = record.data.mem_access;

      // Print header
      fprintf(file_handle_, "CTX %p - kernel_launch_id %ld - CTA %d,%d,%d - warp %d - PC %ld - %s:\n", record.context,
              mem->kernel_launch_id, mem->cta_id_x, mem->cta_id_y, mem->cta_id_z, mem->warp_id, mem->pc,
              record.sass_instruction.c_str());

      // Print memory addresses
      fprintf(file_handle_, "  Memory Addresses:\n  * ");
      int printed = 0;
      for (int i = 0; i < 32; i++) {
        if (mem->addrs[i] != 0) {
          fprintf(file_handle_, "T%02d: 0x%016lx ", i, mem->addrs[i]);
          printed++;
          if (printed % 4 == 0 && i < 31) {
            fprintf(file_handle_, "\n    ");
          }
        }
      }
      fprintf(file_handle_, "\n\n");
      break;
    }

    case MSG_TYPE_OPCODE_ONLY: {
      // Opcode_only typically doesn't output to trace files (only for histogram)
      // But we can add support if needed
      break;
    }

    default:
      fprintf(stderr, "TraceWriter: Unknown message type %d in text mode\n", record.type);
      break;
  }

  fflush(file_handle_);
}

void TraceWriter::write_json_format(const TraceRecord& record) {
  try {
    using json = nlohmann::json;
    json j;

    // ========== Serialize metadata ==========

    // Type string
    switch (record.type) {
      case MSG_TYPE_REG_INFO:
        j["type"] = "reg_trace";
        break;
      case MSG_TYPE_MEM_ADDR_ACCESS:
        j["type"] = "mem_addr_trace";
        j["ipoint"] = "B";  // IPOINT_BEFORE
        break;
      case MSG_TYPE_MEM_VALUE_ACCESS:
        j["type"] = "mem_value_trace";
        j["ipoint"] = "A";  // IPOINT_AFTER
        break;
      case MSG_TYPE_OPCODE_ONLY:
        j["type"] = "opcode_only";
        break;
      case MSG_TYPE_TMA_ACCESS:
        j["type"] = "tma_trace";
        break;
      default:
        fprintf(stderr, "TraceWriter: Unknown message type %d\n", record.type);
        return;
    }

    // Context pointer (as hex string)
    std::stringstream ss;
    ss << "0x" << std::hex << reinterpret_cast<uintptr_t>(record.context);
    j["ctx"] = ss.str();

    // SASS instruction (if available)
    if (!record.sass_instruction.empty()) {
      j["sass"] = record.sass_instruction;
    }

    // Trace index
    j["trace_index"] = record.trace_index;

    // Timestamp
    j["timestamp"] = record.timestamp;

    // ========== Serialize data (dispatch by type) ==========

    switch (record.type) {
      case MSG_TYPE_REG_INFO:
        serialize_reg_info(j, record.data.reg_info, record.reg_indices);
        break;
      case MSG_TYPE_MEM_ADDR_ACCESS:
        serialize_mem_access(j, record.data.mem_access);
        break;
      case MSG_TYPE_MEM_VALUE_ACCESS:
        serialize_mem_value_access(j, record.data.mem_value_access);
        break;
      case MSG_TYPE_OPCODE_ONLY:
        serialize_opcode_only(j, record.data.opcode_only);
        break;
      case MSG_TYPE_TMA_ACCESS:
        serialize_tma_access(j, record.data.tma_access);
        break;
    }

    // ========== Buffer and flush ==========

    // Append to JSON buffer (NDJSON format - newline is critical!)
    json_buffer_ += j.dump() + "\n";

    // Check if buffer threshold reached
    if (json_buffer_.size() >= buffer_threshold_) {
      // Dispatch based on trace mode
      if (trace_mode_ == TraceMode::COMPRESSED_NDJSON) {
        write_compressed();
      } else {
        write_uncompressed();
      }
    }

  } catch (const std::exception& e) {
    fprintf(stderr, "TraceWriter: JSON error in write_json_format: %s\n", e.what());
  }
}

```

`tests/datarace_test/blackwell-fa-ws_data_race_test.py`:

```py
"""
Data race reproducer for the Blackwell flash-attention warp-specialized kernel.

Kernel: _attn_fwd_ws
Arch: sm100 (Blackwell)

Original kernel source:
https://github.com/facebookexperimental/triton/blob/b8832e29bc966cf392ca8fdb9b90c9ea8340401e/third_party/tlx/tutorials/blackwell-fa-ws-pipelined-persistent_test.py

Generated by TritonParse reproducer, then simplified.
"""

import logging
import math

import torch
import triton
import triton.language as tl
import triton.language.extra.tlx as tlx
from triton.tools.tensor_descriptor import TensorDescriptor

logger = logging.getLogger(__name__)


def _disable_triton_autotune() -> None:
    """
    Monkey patch the triton.autotune decorator to skip autotuning entirely.
    """
    logger.info("Disabling triton autotune")

    def dummy_autotune(configs, key=None, **kwargs):
        def decorator(func):
            return func  # Just pass through, let @triton.jit handle compilation

        return decorator

    import triton

    triton.autotune = dummy_autotune
    logger.info("Disabled triton autotune")


_disable_triton_autotune()


# Source: https://github.com/facebookexperimental/triton/blob/b8832e29bc966cf392ca8fdb9b90c9ea8340401e/third_party/tlx/tutorials/blackwell-fa-ws-pipelined-persistent_test.py#L341-L890
@triton.jit
def _attn_fwd_ws(
    sm_scale,
    M,  #
    Z,
    H,
    desc_q,
    desc_k,
    desc_v,
    desc_o,
    N_CTX,  #
    HEAD_DIM: tl.constexpr,  #
    BLOCK_M: tl.constexpr,  #
    BLOCK_N: tl.constexpr,  #
    FP8_OUTPUT: tl.constexpr,  #
    STAGE: tl.constexpr,  #
    NUM_BUFFERS_Q: tl.constexpr,  #
    NUM_BUFFERS_KV: tl.constexpr,  #
    NUM_BUFFERS_QK: tl.constexpr,  #
    NUM_MMA_GROUPS: tl.constexpr,  #
    NUM_MMA_SLICES: tl.constexpr,  #
    GROUP_SIZE_N: tl.constexpr,  #
):
    tl.static_assert(NUM_MMA_GROUPS == 2)
    tl.static_assert(NUM_BUFFERS_QK == 1)
    tl.static_assert(NUM_BUFFERS_Q == 1)

    BLOCK_M_SPLIT: tl.constexpr = BLOCK_M // 2

    # original grid
    #   triton.cdiv(q.shape[2], META["BLOCK_M"]),
    #   q.shape[0] * q.shape[1],
    prog_id = tl.program_id(0)
    num_progs = tl.num_programs(0)
    num_pid_m = tl.cdiv(N_CTX, BLOCK_M)
    num_pid_n = Z * H
    num_pid_in_group = num_pid_m * GROUP_SIZE_N
    total_tiles = num_pid_m * Z * H

    tiles_per_sm = total_tiles // num_progs
    if prog_id < total_tiles % num_progs:
        tiles_per_sm += 1

    tile_idx = prog_id

    # allocate SMEM buffers and barriers
    q_tiles = tlx.local_alloc(
        (BLOCK_M_SPLIT, HEAD_DIM), tlx.dtype_of(desc_q), NUM_MMA_GROUPS * NUM_BUFFERS_Q
    )
    kv_tiles = tlx.local_alloc(
        (BLOCK_N, HEAD_DIM), tlx.dtype_of(desc_k), NUM_BUFFERS_KV
    )
    o_tiles = tlx.local_alloc(
        (BLOCK_M_SPLIT, HEAD_DIM), tlx.dtype_of(desc_o), NUM_MMA_GROUPS
    )

    q_fulls = tlx.alloc_barriers(num_barriers=NUM_MMA_GROUPS * NUM_BUFFERS_Q)
    q_empties = tlx.alloc_barriers(num_barriers=NUM_MMA_GROUPS * NUM_BUFFERS_Q)
    kv_fulls = tlx.alloc_barriers(num_barriers=NUM_BUFFERS_KV)
    kv_empties = tlx.alloc_barriers(num_barriers=NUM_BUFFERS_KV)
    o_fulls = tlx.alloc_barriers(num_barriers=NUM_MMA_GROUPS)
    o_empties = tlx.alloc_barriers(num_barriers=NUM_MMA_GROUPS)

    # allocate TMEM buffers and barriers
    qk_tiles = tlx.local_alloc(
        (BLOCK_M_SPLIT, BLOCK_N), tl.float32, NUM_MMA_GROUPS, tlx.storage_kind.tmem
    )
    # Shared buffer for QK, P and Alpha, l, and m.
    # A single QK buffer is split evenly:
    #   - First half  : stores P
    #   - Second half  : stores Alpha, l, and m
    #     QK : |                              BLK_M/2 * BLOCK_N * fp32                  |
    #     P:                                                |  BLK_M/2 * BLOCK_N * fp16 |
    #  Alpha : |BLK_M/2*1*fp32|
    #     l :                 |BLK_M/2*1*fp32|
    #     m :                                |BLK_M/2*1*fp32|
    p_tiles = tlx.local_alloc(
        (BLOCK_M_SPLIT, BLOCK_N // NUM_MMA_SLICES),
        tlx.dtype_of(desc_v),
        NUM_MMA_GROUPS * NUM_MMA_SLICES * 2,
        tlx.storage_kind.tmem,
        reuse=qk_tiles,
    )
    alpha_tiles = tlx.local_alloc(
        (BLOCK_M_SPLIT, 1),
        tl.float32,
        BLOCK_N * NUM_MMA_GROUPS * NUM_BUFFERS_QK,
        tlx.storage_kind.tmem,
        reuse=qk_tiles,
    )
    l_tiles = tlx.local_alloc(
        (BLOCK_M_SPLIT, 1),
        tl.float32,
        BLOCK_N * NUM_MMA_GROUPS * NUM_BUFFERS_QK,
        tlx.storage_kind.tmem,
        reuse=qk_tiles,
    )
    m_tiles = tlx.local_alloc(
        (BLOCK_M_SPLIT, 1),
        tl.float32,
        BLOCK_N * NUM_MMA_GROUPS * NUM_BUFFERS_QK,
        tlx.storage_kind.tmem,
        reuse=qk_tiles,
    )

    acc_tiles = tlx.local_alloc(
        (BLOCK_M_SPLIT, HEAD_DIM), tl.float32, NUM_MMA_GROUPS, tlx.storage_kind.tmem
    )

    qk_fulls = tlx.alloc_barriers(num_barriers=NUM_MMA_GROUPS)
    qk_empties = tlx.alloc_barriers(num_barriers=NUM_MMA_GROUPS)
    p_fulls = tlx.alloc_barriers(num_barriers=NUM_MMA_GROUPS * NUM_MMA_SLICES)
    acc_fulls = tlx.alloc_barriers(num_barriers=NUM_MMA_GROUPS)
    acc_empties = tlx.alloc_barriers(num_barriers=NUM_MMA_GROUPS)

    alpha_fulls = tlx.alloc_barriers(num_barriers=NUM_MMA_GROUPS)
    alpha_empties = tlx.alloc_barriers(num_barriers=NUM_MMA_GROUPS)
    l_fulls = tlx.alloc_barriers(num_barriers=NUM_MMA_GROUPS)

    with tlx.async_tasks():
        # correction group
        with tlx.async_task("default"):
            accum_cnt = 0
            phase = 0
            for i in range(0, tiles_per_sm):
                # initialize offsets
                start_m, off_hz, lo, hi, qo_offset_y, kv_offset_y = _compute_offsets(
                    tile_idx,
                    H,
                    num_pid_n,
                    num_pid_in_group,
                    N_CTX,
                    BLOCK_M,
                    STAGE,
                    GROUP_SIZE_N,
                )
                for _ in tl.range(lo, hi, BLOCK_N):
                    _, phase = _get_bufidx_phase(accum_cnt, 1)
                    for cid in tl.static_range(0, NUM_MMA_GROUPS):
                        # -- update output accumulator --
                        tlx.barrier_wait(alpha_fulls[cid], phase)
                        # Use alpha[0] for cid=0, and alpha[BLOCK_N] for cid=1
                        alpha_1 = tlx.local_load(alpha_tiles[cid * BLOCK_N])
                        tlx.barrier_arrive(alpha_empties[cid])
                        for slice_id in tl.static_range(0, NUM_MMA_SLICES):
                            subslice = tlx.subslice(
                                acc_tiles[cid],
                                HEAD_DIM * slice_id // NUM_MMA_SLICES,
                                HEAD_DIM // NUM_MMA_SLICES,
                            )
                            acc = tlx.local_load(subslice)
                            # acc = acc * alpha_1
                            acc = _mul_f32x2(acc, alpha_1)
                            tlx.local_store(subslice, acc)
                        tlx.barrier_arrive(acc_fulls[cid])
                    accum_cnt += 1

                _, phase = _get_bufidx_phase(i, 1)
                for cid in tl.static_range(0, NUM_MMA_GROUPS):
                    # epilogue
                    tlx.barrier_wait(l_fulls[cid], phase)
                    # Use l[1]/l[1+BLOCK_N] and m[2][2 + BLOCK_N]
                    # to disambigulate from alpha[0]/alpha[BLOCK_N]
                    l = tlx.local_load(l_tiles[cid * BLOCK_N + 1])  # noqa: E741
                    tlx.barrier_arrive(qk_empties[cid])
                    # DATA RACE REPRODUCER NOTE:
                    # The clock wait below is intentionally commented out so the
                    # test PASSES (no data race observed) under normal timing.
                    #
                    # Root cause: l_tiles and m_tiles reuse the same TMEM storage
                    # as qk_tiles (see `reuse=qk_tiles` in their allocation).
                    # After barrier_arrive(qk_empties) signals the softmax group
                    # that qk_tiles is free, there is a latent WAR (write-after-
                    # read) data race: the softmax group may overwrite TMEM before
                    # the epilogue finishes reading m_tiles below.
                    #
                    # Without the clock wait (commented out, current state), the
                    # epilogue reads m_tiles quickly enough that the race does not
                    # manifest â†’ test PASSES (M_diff ~0.000002).
                    #
                    # When uncommented, the 10M-cycle clock wait deliberately
                    # delays the m_tiles read, widening the race window so the
                    # softmax group has time to overwrite the shared TMEM â†’
                    # test FAILS (M_diff ~14.25), confirming the data race.
                    #
                    # start_clock = tlx.clock64()
                    # while tlx.clock64() - start_clock < 10000000:
                    #     pass
                    m = tlx.local_load(m_tiles[cid * BLOCK_N + 2])
                    m += tl.math.log2(l)
                    offs_m = (
                        start_m * BLOCK_M
                        + cid * BLOCK_M_SPLIT
                        + tl.arange(0, BLOCK_M_SPLIT)
                    )
                    m_ptrs = M + off_hz * N_CTX + offs_m
                    tl.store(m_ptrs, tl.reshape(m, [BLOCK_M_SPLIT]))

                    tlx.barrier_wait(acc_empties[cid], phase)
                    tlx.barrier_wait(o_empties[cid], phase ^ 1)
                    scale = 1 / l
                    for slice_id in tl.static_range(0, NUM_MMA_SLICES):
                        subslice = tlx.subslice(
                            acc_tiles[cid],
                            HEAD_DIM * slice_id // NUM_MMA_SLICES,
                            HEAD_DIM // NUM_MMA_SLICES,
                        )
                        acc = tlx.local_load(subslice)
                        acc = _mul_f32x2(acc, scale)
                        acc = acc.to(tlx.dtype_of(desc_o))
                        subslice_o = tlx.local_slice(
                            o_tiles[cid],
                            [0, HEAD_DIM * slice_id // NUM_MMA_SLICES],
                            [BLOCK_M_SPLIT, HEAD_DIM // NUM_MMA_SLICES],
                        )
                        tlx.local_store(subslice_o, acc)
                    tlx.barrier_arrive(o_fulls[cid])

                tile_idx += num_progs

        # softmax groups
        with tlx.async_task(num_warps=4, registers=168, replicate=NUM_MMA_GROUPS):
            accum_cnt_qk = 0
            for i in range(0, tiles_per_sm):
                # initialize offsets
                start_m, off_hz, lo, hi, qo_offset_y, kv_offset_y = _compute_offsets(
                    tile_idx,
                    H,
                    num_pid_n,
                    num_pid_in_group,
                    N_CTX,
                    BLOCK_M,
                    STAGE,
                    GROUP_SIZE_N,
                )
                # initialize pointer to m and l
                m_i = tl.zeros([BLOCK_M_SPLIT], dtype=tl.float32) - float("inf")
                l_i = tl.zeros([BLOCK_M_SPLIT], dtype=tl.float32) + 1.0
                acc = tl.zeros([BLOCK_M_SPLIT, HEAD_DIM], dtype=tl.float32)
                qk_scale = sm_scale
                qk_scale *= 1.44269504  # 1/log(2)
                out_dtype = tlx.dtype_of(desc_v)

                cid = tlx.async_task_replica_id()
                offs_m = (start_m * BLOCK_M) + (
                    (cid * BLOCK_M_SPLIT) + tl.arange(0, BLOCK_M_SPLIT)
                )
                if STAGE & 1:
                    m_i, l_i, accum_cnt_qk = _softmax_inner_loop(
                        qk_fulls,
                        qk_tiles,
                        p_fulls,
                        p_tiles,
                        alpha_empties,
                        alpha_fulls,
                        alpha_tiles,
                        cid,
                        accum_cnt_qk,
                        qk_scale,
                        offs_m,
                        m_i,
                        l_i,
                        start_m,
                        N_CTX,
                        out_dtype,
                        BLOCK_M,
                        BLOCK_N,
                        HEAD_DIM,
                        NUM_MMA_SLICES,
                        NUM_MMA_GROUPS,
                        STAGE=4 - STAGE,
                    )

                if STAGE & 2:
                    m_i, l_i, accum_cnt_qk = _softmax_inner_loop(
                        qk_fulls,
                        qk_tiles,
                        p_fulls,
                        p_tiles,
                        alpha_empties,
                        alpha_fulls,
                        alpha_tiles,
                        cid,
                        accum_cnt_qk,
                        qk_scale,
                        offs_m,
                        m_i,
                        l_i,
                        start_m,
                        N_CTX,
                        out_dtype,
                        BLOCK_M,
                        BLOCK_N,
                        HEAD_DIM,
                        NUM_MMA_SLICES,
                        NUM_MMA_GROUPS,
                        STAGE=2,
                    )

                # prepare l_i for the epilog
                # Use l[1]/l[1+BLOCK_N] and m[2][2 + BLOCK_N]
                # to disambigulate from alpha[0]/alpha[BLOCK_N]
                tlx.local_store(l_tiles[cid * BLOCK_N + 1], l_i[:, None])
                tlx.local_store(m_tiles[cid * BLOCK_N + 2], m_i[:, None])
                tlx.barrier_arrive(l_fulls[cid])
                tile_idx += num_progs

            # mma group
        with tlx.async_task(num_warps=1, registers=24):
            accum_cnt_kv = 0
            accum_cnt_qk = 0

            for j in range(0, tiles_per_sm):
                # initialize offsets
                _, _, lo, hi, _, _ = _compute_offsets(
                    tile_idx,
                    H,
                    num_pid_n,
                    num_pid_in_group,
                    N_CTX,
                    BLOCK_M,
                    STAGE,
                    GROUP_SIZE_N,
                )

                q_bufIdx, q_phase = _get_bufidx_phase(j, NUM_BUFFERS_Q)
                k_bufIdx, k_phase = _get_bufidx_phase(accum_cnt_kv, NUM_BUFFERS_KV)
                v_bufIdx, v_phase = _get_bufidx_phase(accum_cnt_kv + 1, NUM_BUFFERS_KV)

                # wait for the K buffer to be populated by the producer
                tlx.barrier_wait(kv_fulls[k_bufIdx], k_phase)

                # wait for the Q buffer to be populated by the producer
                tlx.barrier_wait(q_fulls[q_bufIdx], q_phase)

                # -- compute q0 @ k ----
                k_tile = tlx.local_trans(kv_tiles[k_bufIdx])
                tlx.barrier_wait(qk_empties[0], q_phase ^ 1)
                tlx.async_dot(
                    q_tiles[0],
                    k_tile,
                    qk_tiles[0],
                    use_acc=False,
                    mBarriers=[qk_fulls[0]],
                )

                # -- compute q1 @ k ----
                tlx.barrier_wait(q_fulls[q_bufIdx + NUM_BUFFERS_Q], q_phase)
                tlx.barrier_wait(qk_empties[1], q_phase ^ 1)
                tlx.async_dot(
                    q_tiles[1],
                    k_tile,
                    qk_tiles[1],
                    use_acc=False,
                    mBarriers=[qk_fulls[1], kv_empties[k_bufIdx]],
                )

                _, qk_phase = _get_bufidx_phase(accum_cnt_qk, 1)

                # -- compute p0 @ v ----
                # wait for the V buffer to be populated by the producer
                tlx.barrier_wait(kv_fulls[v_bufIdx], v_phase)
                tlx.barrier_wait(acc_fulls[0], qk_phase)
                # Use p[NUM_MMA_SLICES + slice_id] for cid=0, and
                # p[NUM_MMA_GROUPS * NUM_MMA_SLICES + NUM_MMA_SLICES + slice_id] for cid=1
                for slice_id in tl.static_range(0, NUM_MMA_SLICES):
                    tlx.barrier_wait(p_fulls[slice_id + 0 * NUM_MMA_SLICES], qk_phase)
                    kv_slice = tlx.local_slice(
                        kv_tiles[v_bufIdx],
                        [BLOCK_N * slice_id // NUM_MMA_SLICES, 0],
                        [BLOCK_N // NUM_MMA_SLICES, HEAD_DIM],
                    )
                    p_bufIdx = NUM_MMA_SLICES + slice_id
                    tlx.async_dot(
                        p_tiles[p_bufIdx],
                        kv_slice,
                        acc_tiles[0],
                        use_acc=slice_id > 0,
                    )

                acc1_init = False

                for i in tl.range(lo + BLOCK_N, hi, BLOCK_N):
                    v_bufIdx_prev = v_bufIdx
                    qk_phase_prev = qk_phase

                    accum_cnt_qk += 1
                    accum_cnt_kv += 2
                    k_bufIdx, k_phase = _get_bufidx_phase(accum_cnt_kv, NUM_BUFFERS_KV)
                    v_bufIdx, v_phase = _get_bufidx_phase(
                        accum_cnt_kv + 1, NUM_BUFFERS_KV
                    )

                    # -- compute q0 @ k ----
                    # wait for the K buffer to be populated by the producer
                    tlx.barrier_wait(kv_fulls[k_bufIdx], k_phase)
                    k_tile = tlx.local_trans(kv_tiles[k_bufIdx])
                    _, qk_phase = _get_bufidx_phase(accum_cnt_qk, 1)

                    tlx.async_dot(
                        q_tiles[0],
                        k_tile,
                        qk_tiles[0],
                        use_acc=False,
                        mBarriers=[qk_fulls[0]],
                    )

                    # -- compute p1 @ v from the previous iteration----
                    tlx.barrier_wait(acc_fulls[1], qk_phase_prev)
                    for slice_id in tl.static_range(0, NUM_MMA_SLICES):
                        tlx.barrier_wait(
                            p_fulls[slice_id + 1 * NUM_MMA_SLICES], qk_phase_prev
                        )
                        kv_slice = tlx.local_slice(
                            kv_tiles[v_bufIdx_prev],
                            [BLOCK_N * slice_id // NUM_MMA_SLICES, 0],
                            [BLOCK_N // NUM_MMA_SLICES, HEAD_DIM],
                        )
                        p_bufIdx = (
                            1 * NUM_MMA_GROUPS * NUM_MMA_SLICES
                            + NUM_MMA_SLICES
                            + slice_id
                        )
                        use_acc = acc1_init if slice_id == 0 else True
                        mBarriers = (
                            [kv_empties[v_bufIdx_prev]]
                            if slice_id == NUM_MMA_SLICES - 1
                            else []
                        )
                        tlx.async_dot(
                            p_tiles[p_bufIdx],
                            kv_slice,
                            acc_tiles[1],
                            use_acc=use_acc,
                            mBarriers=mBarriers,
                        )

                    acc1_init = True

                    # -- compute q1 @ k ----
                    tlx.async_dot(
                        q_tiles[1],
                        k_tile,
                        qk_tiles[1],
                        use_acc=False,
                        mBarriers=[qk_fulls[1], kv_empties[k_bufIdx]],
                    )

                    # -- compute p0 @ v ----
                    # wait for the V buffer to be populated by the producer
                    tlx.barrier_wait(kv_fulls[v_bufIdx], v_phase)

                    tlx.barrier_wait(acc_fulls[0], qk_phase)
                    for slice_id in tl.static_range(0, NUM_MMA_SLICES):
                        tlx.barrier_wait(
                            p_fulls[slice_id + 0 * NUM_MMA_SLICES], qk_phase
                        )
                        # Use p[1] for cid=0, and p[3] for cid=1
                        kv_slice = tlx.local_slice(
                            kv_tiles[v_bufIdx],
                            [BLOCK_N * slice_id // NUM_MMA_SLICES, 0],
                            [BLOCK_N // NUM_MMA_SLICES, HEAD_DIM],
                        )
                        p_bufIdx = NUM_MMA_SLICES + slice_id
                        tlx.async_dot(
                            p_tiles[p_bufIdx],
                            kv_slice,
                            acc_tiles[0],
                            use_acc=True,
                        )

                tlx.tcgen05_commit(q_empties[q_bufIdx])
                tlx.tcgen05_commit(q_empties[q_bufIdx + NUM_BUFFERS_Q])
                tlx.tcgen05_commit(acc_empties[0])

                # -- compute p1 @ v ----
                tlx.barrier_wait(acc_fulls[1], qk_phase)
                for slice_id in tl.static_range(0, NUM_MMA_SLICES):
                    tlx.barrier_wait(p_fulls[slice_id + NUM_MMA_SLICES], qk_phase)
                    # Use p[1] for cid=0, and p[3] for cid=1
                    kv_slice = tlx.local_slice(
                        kv_tiles[v_bufIdx],
                        [BLOCK_N * slice_id // NUM_MMA_SLICES, 0],
                        [BLOCK_N // NUM_MMA_SLICES, HEAD_DIM],
                    )
                    p_bufIdx = (
                        1 * NUM_MMA_GROUPS * NUM_MMA_SLICES + NUM_MMA_SLICES + slice_id
                    )
                    use_acc = acc1_init if slice_id == 0 else True
                    mBarriers = (
                        [acc_empties[1], kv_empties[v_bufIdx]]
                        if slice_id == NUM_MMA_SLICES - 1
                        else []
                    )
                    tlx.async_dot(
                        p_tiles[p_bufIdx],
                        kv_slice,
                        acc_tiles[1],
                        use_acc=use_acc,
                        mBarriers=mBarriers,
                    )

                accum_cnt_qk += 1
                accum_cnt_kv += 2
                tile_idx += num_progs

        # load
        with tlx.async_task(num_warps=1, registers=24):
            accum_cnt_kv = 0
            for i in range(0, tiles_per_sm):
                # initialize offsets
                _, _, lo, hi, qo_offset_y, kv_offset_y = _compute_offsets(
                    tile_idx,
                    H,
                    num_pid_n,
                    num_pid_in_group,
                    N_CTX,
                    BLOCK_M,
                    STAGE,
                    GROUP_SIZE_N,
                )

                # load q0
                q_bufIdx, q_phase = _get_bufidx_phase(i, NUM_BUFFERS_Q)
                tlx.barrier_wait(q_empties[q_bufIdx], q_phase ^ 1)
                tlx.barrier_expect_bytes(
                    q_fulls[q_bufIdx], 2 * BLOCK_M_SPLIT * HEAD_DIM
                )  # float16
                qo_offset_y_split = qo_offset_y
                tlx.async_descriptor_load(
                    desc_q, q_tiles[q_bufIdx], [qo_offset_y_split, 0], q_fulls[q_bufIdx]
                )

                # loop over loading k, v
                k_bufIdx, k_phase = _get_bufidx_phase(accum_cnt_kv, NUM_BUFFERS_KV)
                # wait for the K buffer to be released by the consumer
                k_empty = tlx.local_view(kv_empties, k_bufIdx)
                tlx.barrier_wait(k_empty, k_phase ^ 1)

                # load K
                k_full = tlx.local_view(kv_fulls, k_bufIdx)
                k_tile = tlx.local_view(kv_tiles, k_bufIdx)
                tlx.barrier_expect_bytes(k_full, 2 * BLOCK_N * HEAD_DIM)  # float16
                tlx.async_descriptor_load(desc_k, k_tile, [kv_offset_y, 0], k_full)

                # load q1
                q_bufIdx += NUM_BUFFERS_Q
                tlx.barrier_wait(q_empties[q_bufIdx], q_phase ^ 1)
                tlx.barrier_expect_bytes(
                    q_fulls[q_bufIdx], 2 * BLOCK_M_SPLIT * HEAD_DIM
                )  # float16
                qo_offset_y_split = qo_offset_y + BLOCK_M_SPLIT
                tlx.async_descriptor_load(
                    desc_q, q_tiles[q_bufIdx], [qo_offset_y_split, 0], q_fulls[q_bufIdx]
                )

                v_bufIdx, v_phase = _get_bufidx_phase(accum_cnt_kv + 1, NUM_BUFFERS_KV)
                # wait for the V buffer to be released by the consumer
                v_empty = tlx.local_view(kv_empties, v_bufIdx)
                tlx.barrier_wait(v_empty, v_phase ^ 1)
                # load V
                v_full = tlx.local_view(kv_fulls, v_bufIdx)
                v_tile = tlx.local_view(kv_tiles, v_bufIdx)
                tlx.barrier_expect_bytes(v_full, 2 * BLOCK_N * HEAD_DIM)  # float16
                tlx.async_descriptor_load(desc_v, v_tile, [kv_offset_y, 0], v_full)

                kv_offset_y += BLOCK_N
                accum_cnt_kv += 2

                for _ in tl.range(lo + BLOCK_N, hi, BLOCK_N):
                    k_bufIdx, k_phase = _get_bufidx_phase(accum_cnt_kv, NUM_BUFFERS_KV)
                    # wait for the K buffer to be released by the consumer
                    k_empty = tlx.local_view(kv_empties, k_bufIdx)
                    tlx.barrier_wait(k_empty, k_phase ^ 1)
                    # load K
                    k_full = tlx.local_view(kv_fulls, k_bufIdx)
                    k_tile = tlx.local_view(kv_tiles, k_bufIdx)
                    tlx.barrier_expect_bytes(k_full, 2 * BLOCK_N * HEAD_DIM)  # float16
                    tlx.async_descriptor_load(desc_k, k_tile, [kv_offset_y, 0], k_full)

                    v_bufIdx, v_phase = _get_bufidx_phase(
                        accum_cnt_kv + 1, NUM_BUFFERS_KV
                    )
                    # wait for the V buffer to be released by the consumer
                    v_empty = tlx.local_view(kv_empties, v_bufIdx)
                    tlx.barrier_wait(v_empty, v_phase ^ 1)
                    # load V
                    v_full = tlx.local_view(kv_fulls, v_bufIdx)
                    v_tile = tlx.local_view(kv_tiles, v_bufIdx)
                    tlx.barrier_expect_bytes(v_full, 2 * BLOCK_N * HEAD_DIM)  # float16
                    tlx.async_descriptor_load(desc_v, v_tile, [kv_offset_y, 0], v_full)

                    kv_offset_y += BLOCK_N
                    accum_cnt_kv += 2

                tile_idx += num_progs

        # epilog group
        with tlx.async_task(num_warps=1, registers=24):
            # initialize offsets
            for i in range(0, tiles_per_sm):
                # initialize offsets
                _, _, _, _, qo_offset_y, _ = _compute_offsets(
                    tile_idx,
                    H,
                    num_pid_n,
                    num_pid_in_group,
                    N_CTX,
                    BLOCK_M,
                    STAGE,
                    GROUP_SIZE_N,
                )
                _, phase = _get_bufidx_phase(i, 1)
                for cid in tl.static_range(0, NUM_MMA_GROUPS):
                    tlx.barrier_wait(o_fulls[cid], phase)
                    tlx.fence_async_shared()
                    qo_offset_y_split = qo_offset_y + cid * BLOCK_M_SPLIT
                    tlx.async_descriptor_store(
                        desc_o, o_tiles[cid], [qo_offset_y_split, 0]
                    )
                    tlx.async_descriptor_store_wait(0)
                    tlx.barrier_arrive(o_empties[cid])

                tile_idx += num_progs


# Dependent functions extracted from source file


# Source: https://github.com/facebookexperimental/triton/blob/b8832e29bc966cf392ca8fdb9b90c9ea8340401e/third_party/tlx/tutorials/blackwell-fa-ws-pipelined-persistent_test.py#L226-L232
@triton.jit
def _split_n(x, SPLIT_FACTOR: tl.constexpr):
    if SPLIT_FACTOR == 1:
        return (x,)
    else:
        x0, x1 = x.reshape([x.shape[0], 2, x.shape[1] // 2]).permute(0, 2, 1).split()
        return _split_n(x0, SPLIT_FACTOR // 2) + _split_n(x1, SPLIT_FACTOR // 2)


# Source: https://github.com/facebookexperimental/triton/blob/b8832e29bc966cf392ca8fdb9b90c9ea8340401e/third_party/tlx/tutorials/blackwell-fa-ws-pipelined-persistent_test.py#L201-L223
@triton.jit
def _compute_offsets(
    tile_idx,
    H,
    num_pid_n,
    num_pid_in_group,
    N_CTX,
    BLOCK_M: tl.constexpr,
    STAGE: tl.constexpr,
    GROUP_SIZE_N: tl.constexpr,
):
    group_id = tile_idx // num_pid_in_group
    first_pid_n = group_id * GROUP_SIZE_N
    group_size_n = min(num_pid_n - first_pid_n, GROUP_SIZE_N)
    start_m = tile_idx % num_pid_in_group // group_size_n
    off_hz = first_pid_n + tile_idx % group_size_n
    off_z = off_hz // H
    off_h = off_hz % H
    offset_y = off_z * (N_CTX * H) + off_h * N_CTX
    qo_offset_y = offset_y + start_m * BLOCK_M
    lo, hi = _get_fused_loop_bounds(start_m, N_CTX, BLOCK_M, STAGE)
    kv_offset_y = offset_y + lo
    return (start_m, off_hz, lo, hi, qo_offset_y, kv_offset_y)


# Source: https://github.com/facebookexperimental/triton/blob/b8832e29bc966cf392ca8fdb9b90c9ea8340401e/third_party/tlx/tutorials/blackwell-fa-ws-pipelined-persistent_test.py#L154-L166
@triton.jit
def _get_unfused_loop_bounds(start_m, N_CTX, BLOCK_M, STAGE: tl.constexpr):
    if STAGE == 1:
        lo, hi = (0, start_m * BLOCK_M)
    elif STAGE == 2:
        lo, hi = (start_m * BLOCK_M, (start_m + 1) * BLOCK_M)
    else:
        tl.static_assert(STAGE == 3)
        lo, hi = (0, N_CTX)
    return (lo, hi)


# Source: https://github.com/facebookexperimental/triton/blob/b8832e29bc966cf392ca8fdb9b90c9ea8340401e/third_party/tlx/tutorials/blackwell-fa-ws-pipelined-persistent_test.py#L113-L130
@triton.jit
def _mul_f32x2(a, b):
    return tl.inline_asm_elementwise(
        "\n        {\n            .reg .b64 ra, rb, rc;\n            mov.b64 ra, { $2, $3 };\n            mov.b64 rb, { $4, $5 };\n            mul.f32x2 rc, ra, rb;\n            mov.b64 { $0, $1 }, rc;\n        }\n        ",
        "=r,=r,r,r,r,r",
        [a, b],
        dtype=tl.float32,
        is_pure=True,
        pack=2,
    )


# Source: https://github.com/facebookexperimental/triton/blob/b8832e29bc966cf392ca8fdb9b90c9ea8340401e/third_party/tlx/tutorials/blackwell-fa-ws-pipelined-persistent_test.py#L106-L110
@triton.jit
def _get_bufidx_phase(accum_cnt, NUM_BUFFERS_KV):
    bufIdx = accum_cnt % NUM_BUFFERS_KV
    phase = accum_cnt // NUM_BUFFERS_KV & 1
    return (bufIdx, phase)


# Source: https://github.com/facebookexperimental/triton/blob/b8832e29bc966cf392ca8fdb9b90c9ea8340401e/third_party/tlx/tutorials/blackwell-fa-ws-pipelined-persistent_test.py#L235-L243
@triton.jit
def _join_n(xs):
    if len(xs) == 1:
        return xs[0]
    else:
        x0 = _join_n(xs[: len(xs) // 2])
        x1 = _join_n(xs[len(xs) // 2 :])
        x = tl.join(x0, x1).permute(0, 2, 1).reshape([x0.shape[0], x0.shape[1] * 2])
        return x


# Source: https://github.com/facebookexperimental/triton/blob/b8832e29bc966cf392ca8fdb9b90c9ea8340401e/third_party/tlx/tutorials/blackwell-fa-ws-pipelined-persistent_test.py#L270-L338
@triton.jit
def _softmax_inner_loop(
    qk_fulls,
    qk_tiles,
    p_fulls,
    p_tiles,
    alpha_empties,
    alpha_fulls,
    alpha_tiles,
    cid,
    accum_cnt_qk,
    qk_scale,
    offs_m,
    m_i,
    l_i,
    start_m,
    N_CTX,
    out_dtype,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    HEAD_DIM: tl.constexpr,
    NUM_MMA_SLICES: tl.constexpr,
    NUM_MMA_GROUPS: tl.constexpr,
    STAGE: tl.constexpr,
):
    lo, hi = _get_unfused_loop_bounds(start_m, N_CTX, BLOCK_M, STAGE)
    for start_n in tl.range(lo, hi, BLOCK_N):
        _, qk_phase = _get_bufidx_phase(accum_cnt_qk, 1)
        tlx.barrier_wait(tlx.local_view(qk_fulls, cid), qk_phase)
        qk = tlx.local_load(tlx.local_view(qk_tiles, cid))
        if STAGE == 2:
            col_limit_right = (offs_m - start_n + 1)[:, None]
            qk = _apply_causal_mask(qk, col_limit_right, BLOCK_N)
        m_ij = tl.maximum(m_i, tl.max(qk, 1) * qk_scale)
        alpha = tl.math.exp2(m_i - m_ij)
        tlx.barrier_wait(tlx.local_view(alpha_empties, cid), qk_phase ^ 1)
        tlx.local_store(tlx.local_view(alpha_tiles, cid * BLOCK_N), alpha[:, None])
        tlx.barrier_arrive(tlx.local_view(alpha_fulls, cid))
        qk = _fma_f32x2(qk, qk_scale, -m_ij[:, None])
        qks = _split_n(qk, NUM_MMA_SLICES)
        ps = ()
        for slice_id in tl.static_range(0, NUM_MMA_SLICES):
            p_bufIdx = cid * NUM_MMA_GROUPS * NUM_MMA_SLICES + NUM_MMA_SLICES + slice_id
            p_i = tl.math.exp2(qks[slice_id])
            tlx.local_store(tlx.local_view(p_tiles, p_bufIdx), p_i.to(out_dtype))
            tlx.fence_async_shared()
            tlx.barrier_arrive(tlx.local_view(p_fulls, slice_id + cid * NUM_MMA_SLICES))
            ps = ps + (p_i,)
        p = _join_n(ps)
        l_ij = tl.sum(p, 1)
        l_i = l_i * alpha + l_ij
        m_i = m_ij
        accum_cnt_qk += 1
    return (m_i, l_i, accum_cnt_qk)


# Source: https://github.com/facebookexperimental/triton/blob/b8832e29bc966cf392ca8fdb9b90c9ea8340401e/third_party/tlx/tutorials/blackwell-fa-ws-pipelined-persistent_test.py#L192-L198
@triton.jit
def _get_fused_loop_bounds(start_m, N_CTX, BLOCK_M, STAGE: tl.constexpr):
    if STAGE == 1:
        return (0, N_CTX)
    else:
        tl.static_assert(STAGE == 3)
        return (0, (start_m + 1) * BLOCK_M)


# Source: https://github.com/facebookexperimental/triton/blob/b8832e29bc966cf392ca8fdb9b90c9ea8340401e/third_party/tlx/tutorials/blackwell-fa-ws-pipelined-persistent_test.py#L246-L252
@triton.jit
def _mask_scalar(qk, col_limit_right, s, i):
    col_lim_right_s = col_limit_right - s
    col_lim_right_cur = max(col_lim_right_s, 0)
    mask = -1 << col_lim_right_cur
    mask_i_bit = (mask & (1 << i)) == 0
    return tl.where(mask_i_bit, qk, -float("inf"))


# Source: https://github.com/facebookexperimental/triton/blob/b8832e29bc966cf392ca8fdb9b90c9ea8340401e/third_party/tlx/tutorials/blackwell-fa-ws-pipelined-persistent_test.py#L255-L267
@triton.jit
def _apply_causal_mask(qk, col_limit_right, BLOCK_N: tl.constexpr):
    offs_n = tl.arange(0, BLOCK_N)[None, :]
    s = offs_n & ~15
    i = offs_n & 15
    return tl.map_elementwise(_mask_scalar, qk, col_limit_right, s, i)


# Source: https://github.com/facebookexperimental/triton/blob/b8832e29bc966cf392ca8fdb9b90c9ea8340401e/third_party/tlx/tutorials/blackwell-fa-ws-pipelined-persistent_test.py#L133-L151
@triton.jit
def _fma_f32x2(a, b, c):
    return tl.inline_asm_elementwise(
        "\n        {\n            .reg .b64 ra, rb, rc, rd;\n            mov.b64 ra, { $2, $3 };\n            mov.b64 rb, { $4, $5 };\n            mov.b64 rc, { $6, $7 };\n            fma.rn.f32x2 rd, ra, rb, rc;\n            mov.b64 { $0, $1 }, rd;\n        }\n        ",
        "=r,=r,r,r,r,r,r,r",
        [a, b, c],
        dtype=tl.float32,
        is_pure=True,
        pack=2,
    )


def launch_kernel():
    # ===== Kernel parameters (from original test) =====
    Z = 8
    H = 16
    N_CTX = 1024
    HEAD_DIM = 64
    BLOCK_M = 256
    BLOCK_N = 128
    NUM_MMA_GROUPS = 2
    BLOCK_M_SPLIT = BLOCK_M // NUM_MMA_GROUPS  # 128
    FP8_OUTPUT = False
    STAGE = 3
    NUM_BUFFERS_Q = 1
    NUM_BUFFERS_KV = 6
    NUM_BUFFERS_QK = 1
    NUM_MMA_SLICES = 2
    GROUP_SIZE_N = 4
    sm_scale = 0.5
    grid = (148, 1, 1)

    y_dim = Z * H * N_CTX

    # ===== Create tensors =====
    torch.manual_seed(20)
    dtype = torch.float16
    device = "cuda:0"

    q = torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=device).normal_(
        mean=0.0, std=0.5
    )
    k = torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=device).normal_(
        mean=0.0, std=0.5
    )
    v = torch.empty((Z, H, N_CTX, HEAD_DIM), dtype=dtype, device=device).normal_(
        mean=0.0, std=0.5
    )
    o = torch.empty_like(q)
    M = torch.empty((Z, H, N_CTX), device=device, dtype=torch.float32)

    # ===== Create TensorDescriptors =====
    desc_q = TensorDescriptor(
        q,
        shape=[y_dim, HEAD_DIM],
        strides=[HEAD_DIM, 1],
        block_shape=[BLOCK_M_SPLIT, HEAD_DIM],
    )
    desc_k = TensorDescriptor(
        k,
        shape=[y_dim, HEAD_DIM],
        strides=[HEAD_DIM, 1],
        block_shape=[BLOCK_N, HEAD_DIM],
    )
    desc_v = TensorDescriptor(
        v,
        shape=[y_dim, HEAD_DIM],
        strides=[HEAD_DIM, 1],
        block_shape=[BLOCK_N, HEAD_DIM],
    )
    desc_o = TensorDescriptor(
        o,
        shape=[y_dim, HEAD_DIM],
        strides=[HEAD_DIM, 1],
        block_shape=[BLOCK_M_SPLIT, HEAD_DIM],
    )

    # ===== Allocator (required by the kernel) =====
    def alloc_fn(size: int, align: int, _):  # noqa: B007
        return torch.empty(size, dtype=torch.int8, device="cuda")

    triton.set_allocator(alloc_fn)

    # ===== Compute reference M (log-sum-exp in log2 scale) =====
    causal = STAGE == 3
    RCP_LN2 = 1.0 / math.log(2)
    qk_scale = sm_scale * RCP_LN2

    S = torch.matmul(q.float(), k.float().transpose(-2, -1)) * qk_scale
    if causal:
        causal_mask = torch.triu(
            torch.ones(N_CTX, N_CTX, device=device, dtype=torch.bool), diagonal=1
        )
        S.masked_fill_(causal_mask, float("-inf"))

    S_max = S.max(dim=-1).values
    ref_M = S_max + torch.log2(torch.exp2(S - S_max.unsqueeze(-1)).sum(dim=-1))

    # ===== Multi-run data race detection =====
    NUM_RUNS = 5
    num_passed = 0
    num_failed = 0

    for run_idx in range(NUM_RUNS):
        M.zero_()

        _attn_fwd_ws[grid](
            sm_scale,
            M,
            Z,
            H,
            desc_q,
            desc_k,
            desc_v,
            desc_o,
            N_CTX=N_CTX,
            HEAD_DIM=HEAD_DIM,
            BLOCK_M=BLOCK_M,
            BLOCK_N=BLOCK_N,
            FP8_OUTPUT=FP8_OUTPUT,
            STAGE=STAGE,
            NUM_BUFFERS_Q=NUM_BUFFERS_Q,
            NUM_BUFFERS_KV=NUM_BUFFERS_KV,
            NUM_BUFFERS_QK=NUM_BUFFERS_QK,
            NUM_MMA_GROUPS=NUM_MMA_GROUPS,
            NUM_MMA_SLICES=NUM_MMA_SLICES,
            GROUP_SIZE_N=GROUP_SIZE_N,
            num_warps=4,
            num_stages=0,
            num_ctas=1,
        )
        torch.cuda.synchronize()

        max_M_diff = (M - ref_M).abs().max().item()
        if max_M_diff > 0.5:
            num_failed += 1
            print(f"âŒ Run {run_idx}: FAILED, M_diff={max_M_diff:.6f}")
        else:
            num_passed += 1
            print(f"âœ… Run {run_idx}: PASSED, M_diff={max_M_diff:.6f}")

    print(f"\n{'=' * 50}")
    print(f"Results: {num_passed} passed, {num_failed} failed out of {NUM_RUNS} runs")
    if num_failed > 0:
        print("DATA RACE DETECTED")
    else:
        print("No data race detected")


if __name__ == "__main__":
    launch_kernel()

```

`tests/datarace_test/cutracer_reduce_test.sh`:

```sh
#!/bin/bash
# =============================================================================
# cutracer_reduce_test.sh - Demo test script for cutracer reduce
# =============================================================================
# Exit 0 = Race detected (interesting)
# Exit 1 = No race (not interesting)
#
# IMPORTANT: The python reducer sets CUTRACER_DELAY_LOAD_PATH env var to pass
# the modified config path. Do NOT set CUTRACER_DELAY_LOAD_PATH in this script
# as it will be overwritten by the reducer.
#
# Example:
#   cutracer reduce --config cutracer_config.json \
#     --test ./tests/datarace_test/cutracer_reduce_test.sh -v
# =============================================================================

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
CUTRACER_ROOT="$(cd "$SCRIPT_DIR/../.." && pwd)"
LOG="/tmp/cutracer_reduce_test.log"

# Run test with delay config replay mode
# Note: CUTRACER_DELAY_LOAD_PATH is set by the python reducer
CUTRACER_ANALYSIS=random_delay \
CUTRACER_DELAY_NS=10000 \
CUTRACER_KERNEL_FILTERS=matmul_kernel_bug1_late_barrier_a \
CUDA_INJECTION64_PATH="$CUTRACER_ROOT/lib/cutracer.so" \
python3 "$SCRIPT_DIR/hopper-gemm-ws_data_race_test.py" --bug 1 --iters 10 > "$LOG" 2>&1

# Parse failures from output (matches "Bug 1:", "Bug 2:", etc.)
FAILURES=$(grep -A1 "Bug.*:" "$LOG" | grep -oP 'Result: \K\d+(?=/)' || echo 0)
echo "Failures: $FAILURES/10"

# Race detected if >= 30% failure rate (3 out of 10 iterations)
if [[ "$FAILURES" -ge 3 ]]; then
    echo "âœ“ Race detected"
    exit 0
else
    echo "âœ— No race"
    exit 1
fi

```

`tests/datarace_test/hopper-gemm-ws_data_race_test.py`:

```py
"""
Data Race Test for Hopper GEMM Warp-Specialized Kernel

This file contains intentionally buggy versions of the warp-specialized GEMM kernel
to test data race detectors. Each bug is designed to:
1. Have high probability of passing in normal execution
2. Be triggered by random delay injection

The correct pattern is:
    barrier_wait(full_a)       # Wait for TMA to complete
    barrier_wait(full_b)       # Wait for TMA to complete
    async_dot(data_a, data_b)  # MMA uses ready data

Bug Pattern 1: Late Barrier Wait (Moved After MMA Load)
=======================================================
The barrier_wait for A is MOVED to AFTER async_dot:
    spin_loop()                # Spin to give TMA time (usually enough)
    barrier_wait(full_b)       # Only wait for B
    async_dot(data_a, data_b)  # DATA RACE: A may not be ready yet!
    barrier_wait(full_a)       # TOO LATE: MMA already used the data!

Bug Pattern 2: Missing Barrier Wait (Spin-Wait Only)
====================================================
The barrier_wait for B is REMOVED entirely:
    barrier_wait(full_a)       # Only wait for A
    spin_loop()                # Spin to give TMA time for B (usually enough)
    async_dot(data_a, data_b)  # DATA RACE: B may not be ready yet!
                               # No barrier_wait for B at all!

Why they usually pass:
- TMA loads are very fast (~100-200 cycles for 64KB)
- The spin loop provides actual delay cycles (clock64 cannot be optimized away)
- This is usually MORE than enough time for TMA to complete

Why random delay injection triggers the bugs:
- Delays slow down TMA initiation or data arrival
- The spin loop becomes insufficient
- async_dot reads partially loaded or stale data
"""

from typing import Optional

import click
import pytest
import torch
import triton
import triton.language as tl
import triton.language.extra.tlx as tlx
from triton._internal_testing import is_cuda
from triton.tools.tensor_descriptor import TensorDescriptor

DEVICE = triton.runtime.driver.active.get_active_torch_device()

# Use smaller matrices for faster testing
# CRITICAL: Smaller matrices = fewer loop iterations = less chance for race to manifest
# This makes the race condition timing-dependent rather than guaranteed
M, N, K = (512, 512, 512)  # Small size: only 8 K-loop iterations with BK=64


def alloc_fn(size: int, align: int, stream: Optional[int]):
    assert align == 128
    assert stream == 0
    return torch.empty(size, dtype=torch.int8, device=DEVICE)


def matmul_tma_set_block_size_hook(nargs):
    BLOCK_M = nargs["BM"]
    BLOCK_N = nargs["BN"]
    BLOCK_K = nargs["BK"]
    NUM_MMA_GROUPS = nargs["NUM_MMA_GROUPS"]
    BLOCK_M_SPLIT = BLOCK_M // NUM_MMA_GROUPS
    nargs["a_desc"].block_shape = [BLOCK_M_SPLIT, BLOCK_K]
    nargs["b_desc"].block_shape = [BLOCK_K, BLOCK_N]
    EPILOGUE_SUBTILE = nargs.get("EPILOGUE_SUBTILE", False)
    if EPILOGUE_SUBTILE:
        nargs["c_desc"].block_shape = [BLOCK_M_SPLIT, BLOCK_N // 2]
    else:
        nargs["c_desc"].block_shape = [BLOCK_M_SPLIT, BLOCK_N]


# ==============================================================================
# CORRECT REFERENCE KERNEL (for comparison)
# ==============================================================================
@triton.jit
def matmul_kernel_correct(
    a_desc,
    b_desc,
    c_desc,
    M,
    N,
    K,
    BM: tl.constexpr,
    BN: tl.constexpr,
    BK: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    NUM_STAGES: tl.constexpr,
    NUM_MMA_WARPS: tl.constexpr,
    NUM_MMA_GROUPS: tl.constexpr,
    EPILOGUE_SUBTILE: tl.constexpr,
):
    """Correct kernel - should always pass."""
    BLOCK_M_SPLIT: tl.constexpr = BM // NUM_MMA_GROUPS

    a = tlx.local_alloc(
        (BLOCK_M_SPLIT, BK), tlx.dtype_of(a_desc), NUM_STAGES * NUM_MMA_GROUPS
    )
    b = tlx.local_alloc((BK, BN), tlx.dtype_of(b_desc), NUM_STAGES)

    bars_empty_a = tlx.alloc_barriers(
        num_barriers=NUM_STAGES * NUM_MMA_GROUPS, arrive_count=1
    )
    bars_full_a = tlx.alloc_barriers(
        num_barriers=NUM_STAGES * NUM_MMA_GROUPS, arrive_count=1
    )
    bars_empty_b = tlx.alloc_barriers(
        num_barriers=NUM_STAGES, arrive_count=NUM_MMA_GROUPS
    )
    bars_full_b = tlx.alloc_barriers(num_barriers=NUM_STAGES, arrive_count=1)

    with tlx.async_tasks():
        with tlx.async_task("default"):
            pid = tl.program_id(axis=0)
            num_pid_m = tl.cdiv(M, BM)
            num_pid_n = tl.cdiv(N, BN)
            num_pid_in_group = GROUP_SIZE_M * num_pid_n
            group_id = pid // num_pid_in_group
            first_pid_m = group_id * GROUP_SIZE_M
            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
            pid_m = first_pid_m + (pid % group_size_m)
            pid_n = (pid % num_pid_in_group) // group_size_m
            offset_am = pid_m * BM
            offset_bn = pid_n * BN

            p = 1
            for k in range(0, tl.cdiv(K, BK)):
                buf = k % NUM_STAGES
                offset_k = k * BK

                empty_a_1st = tlx.local_view(bars_empty_a, buf)
                full_a_1st = tlx.local_view(bars_full_a, buf)
                tlx.barrier_wait(bar=empty_a_1st, phase=p)
                tlx.barrier_expect_bytes(
                    full_a_1st, BLOCK_M_SPLIT * BK * tlx.size_of(tlx.dtype_of(a_desc))
                )
                data_a_1st = tlx.local_view(a, buf)
                tlx.async_descriptor_load(
                    a_desc, data_a_1st, [offset_am, offset_k], full_a_1st
                )

                empty_b = tlx.local_view(bars_empty_b, buf)
                full_b = tlx.local_view(bars_full_b, buf)
                tlx.barrier_wait(bar=empty_b, phase=p)
                tlx.barrier_expect_bytes(
                    full_b, BN * BK * tlx.size_of(tlx.dtype_of(a_desc))
                )
                data_b = tlx.local_view(b, buf)
                tlx.async_descriptor_load(b_desc, data_b, [offset_k, offset_bn], full_b)

                empty_a_2nd = tlx.local_view(bars_empty_a, buf + NUM_STAGES)
                full_a_2nd = tlx.local_view(bars_full_a, buf + NUM_STAGES)
                tlx.barrier_wait(bar=empty_a_2nd, phase=p)
                tlx.barrier_expect_bytes(
                    bar=full_a_2nd,
                    size=BLOCK_M_SPLIT * BK * tlx.size_of(tlx.dtype_of(a_desc)),
                )
                data_a_2nd = tlx.local_view(a, buf + NUM_STAGES)
                tlx.async_descriptor_load(
                    a_desc,
                    data_a_2nd,
                    [offset_am + BLOCK_M_SPLIT, offset_k],
                    full_a_2nd,
                )

                p = p ^ (buf == (NUM_STAGES - 1))

        with tlx.async_task(num_warps=4, replicate=2):
            pid = tl.program_id(axis=0)
            num_pid_m = tl.cdiv(M, BM)
            num_pid_n = tl.cdiv(N, BN)
            num_pid_in_group = GROUP_SIZE_M * num_pid_n
            group_id = pid // num_pid_in_group
            first_pid_m = group_id * GROUP_SIZE_M
            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
            pid_m = first_pid_m + (pid % group_size_m)
            pid_n = (pid % num_pid_in_group) // group_size_m
            offset_am = pid_m * BM
            offset_bn = pid_n * BN

            p = 0
            acc = tl.zeros([BM // 2, BN], dtype=tl.float32)

            for k in range(0, tl.cdiv(K, BK)):
                buf = k % NUM_STAGES

                full_a = tlx.local_view(
                    bars_full_a, buf + NUM_STAGES * tlx.async_task_replica_id()
                )
                full_b = tlx.local_view(bars_full_b, buf)
                tlx.barrier_wait(bar=full_a, phase=p)
                tlx.barrier_wait(bar=full_b, phase=p)  # CORRECT: wait for B

                data_a = tlx.local_view(
                    a, buf + NUM_STAGES * tlx.async_task_replica_id()
                )
                data_b = tlx.local_view(b, buf)

                acc = tlx.async_dot(data_a, data_b, acc)
                acc = tlx.async_dot_wait(tl.constexpr(0), acc)

                empty_a = tlx.local_view(
                    bars_empty_a, buf + NUM_STAGES * tlx.async_task_replica_id()
                )
                empty_b = tlx.local_view(bars_empty_b, buf)
                tlx.barrier_arrive(empty_a)
                tlx.barrier_arrive(empty_b)

                p = p ^ (buf == (NUM_STAGES - 1))

            offset_cm = offset_am + BLOCK_M_SPLIT * tlx.async_task_replica_id()

            if EPILOGUE_SUBTILE:
                acc = tl.reshape(acc, (BLOCK_M_SPLIT, 2, BN // 2))
                acc = tl.permute(acc, (0, 2, 1))
                acc0, acc1 = tl.split(acc)
                c0 = acc0.to(tlx.dtype_of(c_desc))
                c_desc.store([offset_cm, offset_bn], c0)
                c1 = acc1.to(tlx.dtype_of(c_desc))
                c_desc.store([offset_cm, offset_bn + BN // 2], c1)
            else:
                c_desc.store([offset_cm, offset_bn], acc.to(tlx.dtype_of(c_desc)))


# ==============================================================================
# BUG 1: Late barrier_wait for A (moved after MMA load)
# ==============================================================================
# The barrier_wait for matrix A is MOVED to AFTER the async_dot call.
# This creates a data race where MMA loads potentially unready data.
#
# Correct pattern:
#     barrier_wait(full_a)       # Wait for TMA to complete
#     barrier_wait(full_b)
#     async_dot(data_a, data_b)  # MMA uses ready data
#
# Buggy pattern (barrier_wait moved after async_dot):
#     spin_loop()                # Spin to give TMA time (usually enough)
#     barrier_wait(full_b)       # Only wait for B
#     async_dot(data_a, data_b)  # DATA RACE: A may not be ready yet!
#     barrier_wait(full_a)       # TOO LATE: MMA already used the data!
#
# The spin loop uses tlx.clock64() which:
#   1. Cannot be optimized away by the compiler (hardware register read)
#   2. Actually burns GPU cycles
#   3. Provides "usually enough" delay for TMA to complete
#
# Why it usually passes without delays:
#   - TMA loads are extremely fast (~100-200 cycles for 64KB)
#   - The spin loop provides actual delay cycles
#   - This is usually MORE than enough time
#
# Why random delay injection triggers the bug:
#   - Delays slow down TMA initiation or data arrival
#   - The spin loop becomes insufficient
#   - async_dot reads partially loaded or stale data
#   - barrier_wait after async_dot doesn't help - damage is already done
# ==============================================================================
@triton.jit
def matmul_kernel_bug1_late_barrier_a(
    a_desc,
    b_desc,
    c_desc,
    M,
    N,
    K,
    BM: tl.constexpr,
    BN: tl.constexpr,
    BK: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    NUM_STAGES: tl.constexpr,
    NUM_MMA_WARPS: tl.constexpr,
    NUM_MMA_GROUPS: tl.constexpr,
    EPILOGUE_SUBTILE: tl.constexpr,
    SPIN_COUNT: tl.constexpr,  # Number of clock cycles to spin
):
    BLOCK_M_SPLIT: tl.constexpr = BM // NUM_MMA_GROUPS

    a = tlx.local_alloc(
        (BLOCK_M_SPLIT, BK), tlx.dtype_of(a_desc), NUM_STAGES * NUM_MMA_GROUPS
    )
    b = tlx.local_alloc((BK, BN), tlx.dtype_of(b_desc), NUM_STAGES)

    bars_empty_a = tlx.alloc_barriers(
        num_barriers=NUM_STAGES * NUM_MMA_GROUPS, arrive_count=1
    )
    bars_full_a = tlx.alloc_barriers(
        num_barriers=NUM_STAGES * NUM_MMA_GROUPS, arrive_count=1
    )
    bars_empty_b = tlx.alloc_barriers(
        num_barriers=NUM_STAGES, arrive_count=NUM_MMA_GROUPS
    )
    bars_full_b = tlx.alloc_barriers(num_barriers=NUM_STAGES, arrive_count=1)

    with tlx.async_tasks():
        with tlx.async_task("default"):
            pid = tl.program_id(axis=0)
            num_pid_m = tl.cdiv(M, BM)
            num_pid_n = tl.cdiv(N, BN)
            num_pid_in_group = GROUP_SIZE_M * num_pid_n
            group_id = pid // num_pid_in_group
            first_pid_m = group_id * GROUP_SIZE_M
            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
            pid_m = first_pid_m + (pid % group_size_m)
            pid_n = (pid % num_pid_in_group) // group_size_m
            offset_am = pid_m * BM
            offset_bn = pid_n * BN

            p = 1
            for k in range(0, tl.cdiv(K, BK)):
                buf = k % NUM_STAGES
                offset_k = k * BK

                # Load A for consumer 0
                empty_a_1st = tlx.local_view(bars_empty_a, buf)
                full_a_1st = tlx.local_view(bars_full_a, buf)
                tlx.barrier_wait(bar=empty_a_1st, phase=p)
                tlx.barrier_expect_bytes(
                    full_a_1st, BLOCK_M_SPLIT * BK * tlx.size_of(tlx.dtype_of(a_desc))
                )
                data_a_1st = tlx.local_view(a, buf)
                tlx.async_descriptor_load(
                    a_desc, data_a_1st, [offset_am, offset_k], full_a_1st
                )

                # Load B
                empty_b = tlx.local_view(bars_empty_b, buf)
                full_b = tlx.local_view(bars_full_b, buf)
                tlx.barrier_wait(bar=empty_b, phase=p)
                tlx.barrier_expect_bytes(
                    full_b, BN * BK * tlx.size_of(tlx.dtype_of(a_desc))
                )
                data_b = tlx.local_view(b, buf)
                tlx.async_descriptor_load(b_desc, data_b, [offset_k, offset_bn], full_b)

                # Load A for consumer 1
                empty_a_2nd = tlx.local_view(bars_empty_a, buf + NUM_STAGES)
                full_a_2nd = tlx.local_view(bars_full_a, buf + NUM_STAGES)
                tlx.barrier_wait(bar=empty_a_2nd, phase=p)
                tlx.barrier_expect_bytes(
                    bar=full_a_2nd,
                    size=BLOCK_M_SPLIT * BK * tlx.size_of(tlx.dtype_of(a_desc)),
                )
                data_a_2nd = tlx.local_view(a, buf + NUM_STAGES)
                tlx.async_descriptor_load(
                    a_desc,
                    data_a_2nd,
                    [offset_am + BLOCK_M_SPLIT, offset_k],
                    full_a_2nd,
                )

                p = p ^ (buf == (NUM_STAGES - 1))

        with tlx.async_task(num_warps=4, replicate=2):
            pid = tl.program_id(axis=0)
            num_pid_m = tl.cdiv(M, BM)
            num_pid_n = tl.cdiv(N, BN)
            num_pid_in_group = GROUP_SIZE_M * num_pid_n
            group_id = pid // num_pid_in_group
            first_pid_m = group_id * GROUP_SIZE_M
            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
            pid_m = first_pid_m + (pid % group_size_m)
            pid_n = (pid % num_pid_in_group) // group_size_m
            offset_am = pid_m * BM
            offset_bn = pid_n * BN

            p = 0
            acc = tl.zeros([BM // 2, BN], dtype=tl.float32)

            for k in range(0, tl.cdiv(K, BK)):
                buf = k % NUM_STAGES

                full_a = tlx.local_view(
                    bars_full_a, buf + NUM_STAGES * tlx.async_task_replica_id()
                )
                full_b = tlx.local_view(bars_full_b, buf)

                # BUG: barrier_wait for A is MOVED to AFTER async_dot!
                # We use spin-wait to give TMA time, but MMA reads A before
                # we actually confirm it's ready.
                #
                # CORRECT pattern:
                #   tlx.barrier_wait(bar=full_a, phase=p)  # Wait for A
                #   tlx.barrier_wait(bar=full_b, phase=p)  # Wait for B
                #   async_dot(data_a, data_b, acc)         # MMA uses ready data
                #
                # BUG pattern (barrier moved after async_dot):
                #   spin_loop()                            # Give TMA time (usually enough)
                #   barrier_wait(bar=full_b, phase=p)      # Only wait for B
                #   async_dot(data_a, data_b, acc)         # DATA RACE: A may not be ready!
                #   barrier_wait(bar=full_a, phase=p)      # TOO LATE: damage done
                #
                # Spin for a fixed number of cycles using clock64():
                start_clock = tlx.clock64()
                while tlx.clock64() - start_clock < SPIN_COUNT:
                    pass  # Spin until enough cycles have passed
                # The spin is usually enough, but with delays it won't be!

                tlx.barrier_wait(
                    bar=full_b, phase=p
                )  # B barrier is correct (before MMA)

                data_a = tlx.local_view(
                    a, buf + NUM_STAGES * tlx.async_task_replica_id()
                )
                data_b = tlx.local_view(b, buf)

                acc = tlx.async_dot(
                    data_a, data_b, acc
                )  # DATA RACE: A may not be ready!
                tlx.barrier_wait(
                    bar=full_a, phase=p
                )  # TOO LATE: MMA already used the data!

                acc = tlx.async_dot_wait(tl.constexpr(0), acc)

                empty_a = tlx.local_view(
                    bars_empty_a, buf + NUM_STAGES * tlx.async_task_replica_id()
                )
                empty_b = tlx.local_view(bars_empty_b, buf)
                tlx.barrier_arrive(empty_a)
                tlx.barrier_arrive(empty_b)

                p = p ^ (buf == (NUM_STAGES - 1))

            offset_cm = offset_am + BLOCK_M_SPLIT * tlx.async_task_replica_id()

            if EPILOGUE_SUBTILE:
                acc = tl.reshape(acc, (BLOCK_M_SPLIT, 2, BN // 2))
                acc = tl.permute(acc, (0, 2, 1))
                acc0, acc1 = tl.split(acc)
                c0 = acc0.to(tlx.dtype_of(c_desc))
                c_desc.store([offset_cm, offset_bn], c0)
                c1 = acc1.to(tlx.dtype_of(c_desc))
                c_desc.store([offset_cm, offset_bn + BN // 2], c1)
            else:
                c_desc.store([offset_cm, offset_bn], acc.to(tlx.dtype_of(c_desc)))


# ==============================================================================
# BUG 2: Missing barrier_wait for B (spin-wait only, no barrier)
# ==============================================================================
# The barrier_wait for matrix B is REMOVED entirely.
# We use only a spin loop instead of proper synchronization.
#
# Correct pattern:
#     barrier_wait(full_a)       # Wait for TMA to complete
#     barrier_wait(full_b)       # Wait for TMA to complete
#     async_dot(data_a, data_b)  # MMA uses ready data
#
# Buggy pattern (barrier_wait removed, spin only):
#     barrier_wait(full_a)       # Only wait for A
#     spin_loop()                # Spin to give TMA time for B (usually enough)
#     async_dot(data_a, data_b)  # DATA RACE: B may not be ready yet!
#                                # No barrier_wait for B at all!
#
# The spin loop uses tlx.clock64() which:
#   1. Cannot be optimized away by the compiler (hardware register read)
#   2. Actually burns GPU cycles
#   3. Provides "usually enough" delay for TMA to complete
#
# Why it usually passes without delays:
#   - TMA loads are extremely fast (~100-200 cycles for 64KB)
#   - The spin loop provides actual delay cycles
#   - This is usually MORE than enough time
#
# Why random delay injection triggers the bug:
#   - Delays slow down TMA initiation or data arrival
#   - The spin loop becomes insufficient
#   - async_dot reads partially loaded or stale data
# ==============================================================================
@triton.jit
def matmul_kernel_bug2_missing_barrier_b(
    a_desc,
    b_desc,
    c_desc,
    M,
    N,
    K,
    BM: tl.constexpr,
    BN: tl.constexpr,
    BK: tl.constexpr,
    GROUP_SIZE_M: tl.constexpr,
    NUM_STAGES: tl.constexpr,
    NUM_MMA_WARPS: tl.constexpr,
    NUM_MMA_GROUPS: tl.constexpr,
    EPILOGUE_SUBTILE: tl.constexpr,
    SPIN_COUNT: tl.constexpr,  # Number of clock cycles to spin
):
    BLOCK_M_SPLIT: tl.constexpr = BM // NUM_MMA_GROUPS

    a = tlx.local_alloc(
        (BLOCK_M_SPLIT, BK), tlx.dtype_of(a_desc), NUM_STAGES * NUM_MMA_GROUPS
    )
    b = tlx.local_alloc((BK, BN), tlx.dtype_of(b_desc), NUM_STAGES)

    bars_empty_a = tlx.alloc_barriers(
        num_barriers=NUM_STAGES * NUM_MMA_GROUPS, arrive_count=1
    )
    bars_full_a = tlx.alloc_barriers(
        num_barriers=NUM_STAGES * NUM_MMA_GROUPS, arrive_count=1
    )
    bars_empty_b = tlx.alloc_barriers(
        num_barriers=NUM_STAGES, arrive_count=NUM_MMA_GROUPS
    )
    bars_full_b = tlx.alloc_barriers(num_barriers=NUM_STAGES, arrive_count=1)

    with tlx.async_tasks():
        with tlx.async_task("default"):
            pid = tl.program_id(axis=0)
            num_pid_m = tl.cdiv(M, BM)
            num_pid_n = tl.cdiv(N, BN)
            num_pid_in_group = GROUP_SIZE_M * num_pid_n
            group_id = pid // num_pid_in_group
            first_pid_m = group_id * GROUP_SIZE_M
            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
            pid_m = first_pid_m + (pid % group_size_m)
            pid_n = (pid % num_pid_in_group) // group_size_m
            offset_am = pid_m * BM
            offset_bn = pid_n * BN

            p = 1
            for k in range(0, tl.cdiv(K, BK)):
                buf = k % NUM_STAGES
                offset_k = k * BK

                empty_a_1st = tlx.local_view(bars_empty_a, buf)
                full_a_1st = tlx.local_view(bars_full_a, buf)
                tlx.barrier_wait(bar=empty_a_1st, phase=p)
                tlx.barrier_expect_bytes(
                    full_a_1st, BLOCK_M_SPLIT * BK * tlx.size_of(tlx.dtype_of(a_desc))
                )
                data_a_1st = tlx.local_view(a, buf)
                tlx.async_descriptor_load(
                    a_desc, data_a_1st, [offset_am, offset_k], full_a_1st
                )

                empty_b = tlx.local_view(bars_empty_b, buf)
                full_b = tlx.local_view(bars_full_b, buf)
                tlx.barrier_wait(bar=empty_b, phase=p)
                tlx.barrier_expect_bytes(
                    full_b, BN * BK * tlx.size_of(tlx.dtype_of(a_desc))
                )
                data_b = tlx.local_view(b, buf)
                tlx.async_descriptor_load(b_desc, data_b, [offset_k, offset_bn], full_b)

                empty_a_2nd = tlx.local_view(bars_empty_a, buf + NUM_STAGES)
                full_a_2nd = tlx.local_view(bars_full_a, buf + NUM_STAGES)
                tlx.barrier_wait(bar=empty_a_2nd, phase=p)
                tlx.barrier_expect_bytes(
                    bar=full_a_2nd,
                    size=BLOCK_M_SPLIT * BK * tlx.size_of(tlx.dtype_of(a_desc)),
                )
                data_a_2nd = tlx.local_view(a, buf + NUM_STAGES)
                tlx.async_descriptor_load(
                    a_desc,
                    data_a_2nd,
                    [offset_am + BLOCK_M_SPLIT, offset_k],
                    full_a_2nd,
                )

                p = p ^ (buf == (NUM_STAGES - 1))

        with tlx.async_task(num_warps=4, replicate=2):
            pid = tl.program_id(axis=0)
            num_pid_m = tl.cdiv(M, BM)
            num_pid_n = tl.cdiv(N, BN)
            num_pid_in_group = GROUP_SIZE_M * num_pid_n
            group_id = pid // num_pid_in_group
            first_pid_m = group_id * GROUP_SIZE_M
            group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
            pid_m = first_pid_m + (pid % group_size_m)
            pid_n = (pid % num_pid_in_group) // group_size_m
            offset_am = pid_m * BM
            offset_bn = pid_n * BN

            p = 0
            acc = tl.zeros([BM // 2, BN], dtype=tl.float32)

            for k in range(0, tl.cdiv(K, BK)):
                buf = k % NUM_STAGES

                full_a = tlx.local_view(
                    bars_full_a, buf + NUM_STAGES * tlx.async_task_replica_id()
                )
                full_b = tlx.local_view(bars_full_b, buf)
                tlx.barrier_wait(
                    bar=full_a, phase=p
                )  # A barrier is correct (before MMA)

                # BUG: barrier_wait for B is REMOVED entirely!
                # We use only a spin loop instead of proper synchronization.
                #
                # CORRECT pattern:
                #   tlx.barrier_wait(bar=full_b, phase=p)  # Wait for B
                #   async_dot(data_a, data_b, acc)         # MMA uses ready data
                #
                # BUG pattern (barrier_wait removed, spin only):
                #   spin_loop()                            # Spin to give TMA time (usually enough)
                #   async_dot(data_a, data_b, acc)         # DATA RACE: B may not be ready!
                #                                          # No barrier_wait for B at all!
                #
                # Spin for a fixed number of cycles using clock64():
                start_clock = tlx.clock64()
                while tlx.clock64() - start_clock < SPIN_COUNT:
                    pass  # Spin until enough cycles have passed
                # The spin is usually enough, but with delays it won't be!

                data_a = tlx.local_view(
                    a, buf + NUM_STAGES * tlx.async_task_replica_id()
                )
                data_b = tlx.local_view(b, buf)

                acc = tlx.async_dot(
                    data_a, data_b, acc
                )  # DATA RACE: B may not be ready!
                # NO barrier_wait for B here - completely removed!
                acc = tlx.async_dot_wait(tl.constexpr(0), acc)

                empty_a = tlx.local_view(
                    bars_empty_a, buf + NUM_STAGES * tlx.async_task_replica_id()
                )
                empty_b = tlx.local_view(bars_empty_b, buf)
                tlx.barrier_arrive(empty_a)
                tlx.barrier_arrive(empty_b)

                p = p ^ (buf == (NUM_STAGES - 1))

            offset_cm = offset_am + BLOCK_M_SPLIT * tlx.async_task_replica_id()

            if EPILOGUE_SUBTILE:
                acc = tl.reshape(acc, (BLOCK_M_SPLIT, 2, BN // 2))
                acc = tl.permute(acc, (0, 2, 1))
                acc0, acc1 = tl.split(acc)
                c0 = acc0.to(tlx.dtype_of(c_desc))
                c_desc.store([offset_cm, offset_bn], c0)
                c1 = acc1.to(tlx.dtype_of(c_desc))
                c_desc.store([offset_cm, offset_bn + BN // 2], c1)
            else:
                c_desc.store([offset_cm, offset_bn], acc.to(tlx.dtype_of(c_desc)))


# ==============================================================================
# Test Harness
# ==============================================================================


def run_matmul_with_kernel(
    kernel, a, b, check_correctness=True, spin_count_a=1200, spin_count_b=200
):
    """Run matrix multiplication with a specific kernel."""
    M_dim, K_dim = a.shape
    K_dim2, N_dim = b.shape
    assert K_dim == K_dim2, "Dimension mismatch"

    c = torch.zeros((M_dim, N_dim), dtype=torch.float16, device=DEVICE)

    dummy_block = [1, 1]
    desc_in_1 = TensorDescriptor(
        a,
        shape=[M_dim, K_dim],
        strides=[K_dim, 1],
        block_shape=dummy_block,
    )
    desc_in_2 = TensorDescriptor(
        b,
        shape=[K_dim, N_dim],
        strides=[N_dim, 1],
        block_shape=dummy_block,
    )
    desc_out = TensorDescriptor(
        c,
        shape=[M_dim, N_dim],
        strides=[N_dim, 1],
        block_shape=dummy_block,
    )

    # Fixed config for testing
    BM, BN, BK = 128, 256, 64
    GROUP_SIZE_M = 8
    NUM_STAGES = 4
    NUM_MMA_WARPS = 8
    NUM_MMA_GROUPS = 2
    EPILOGUE_SUBTILE = True

    # Set block shapes
    BLOCK_M_SPLIT = BM // NUM_MMA_GROUPS
    desc_in_1.block_shape = [BLOCK_M_SPLIT, BK]
    desc_in_2.block_shape = [BK, BN]
    desc_out.block_shape = (
        [BLOCK_M_SPLIT, BN // 2] if EPILOGUE_SUBTILE else [BLOCK_M_SPLIT, BN]
    )

    grid = (triton.cdiv(M_dim, BM) * triton.cdiv(N_dim, BN),)

    # Check if this kernel needs SPIN_COUNT parameter
    if kernel == matmul_kernel_bug1_late_barrier_a:
        kernel[grid](
            desc_in_1,
            desc_in_2,
            desc_out,
            M_dim,
            N_dim,
            K_dim,
            BM=BM,
            BN=BN,
            BK=BK,
            GROUP_SIZE_M=GROUP_SIZE_M,
            NUM_STAGES=NUM_STAGES,
            NUM_MMA_WARPS=NUM_MMA_WARPS,
            NUM_MMA_GROUPS=NUM_MMA_GROUPS,
            EPILOGUE_SUBTILE=EPILOGUE_SUBTILE,
            SPIN_COUNT=spin_count_a,  # Spin cycles before MMA (for A timing)
            num_stages=1,
            num_warps=4,
        )
    elif kernel == matmul_kernel_bug2_missing_barrier_b:
        kernel[grid](
            desc_in_1,
            desc_in_2,
            desc_out,
            M_dim,
            N_dim,
            K_dim,
            BM=BM,
            BN=BN,
            BK=BK,
            GROUP_SIZE_M=GROUP_SIZE_M,
            NUM_STAGES=NUM_STAGES,
            NUM_MMA_WARPS=NUM_MMA_WARPS,
            NUM_MMA_GROUPS=NUM_MMA_GROUPS,
            EPILOGUE_SUBTILE=EPILOGUE_SUBTILE,
            SPIN_COUNT=spin_count_b,  # Spin cycles before MMA (for B timing)
            num_stages=1,
            num_warps=4,
        )
    else:
        kernel[grid](
            desc_in_1,
            desc_in_2,
            desc_out,
            M_dim,
            N_dim,
            K_dim,
            BM=BM,
            BN=BN,
            BK=BK,
            GROUP_SIZE_M=GROUP_SIZE_M,
            NUM_STAGES=NUM_STAGES,
            NUM_MMA_WARPS=NUM_MMA_WARPS,
            NUM_MMA_GROUPS=NUM_MMA_GROUPS,
            EPILOGUE_SUBTILE=EPILOGUE_SUBTILE,
            num_stages=1,
            num_warps=4,
        )

    if check_correctness:
        output_ref = torch.matmul(a, b)
        if not torch.allclose(c, output_ref, atol=1e-2, rtol=1e-2):
            max_diff = (c - output_ref).abs().max().item()
            mean_diff = (c - output_ref).abs().mean().item()
            print(f"  Max diff: {max_diff:.6f}, Mean diff: {mean_diff:.6f}")
            return False, c, output_ref
        return True, c, output_ref

    return True, c, None


def run_multiple_iterations(kernel, num_iterations=10, matrix_size=None):
    """Run kernel multiple times to increase chance of triggering race.

    Returns a dict with test results for summary report.
    """
    if matrix_size is None:
        matrix_size = (M, N, K)
    m, n, k = matrix_size

    triton.set_allocator(alloc_fn)

    results = {
        "failures": 0,
        "total": num_iterations,
        "failed_iterations": [],
        "exception_iterations": [],
    }

    for i in range(num_iterations):
        # Use different random seed each iteration for variety
        torch.manual_seed(i * 42)
        a = torch.randn((m, k), dtype=torch.float16, device=DEVICE)
        b = torch.randn((k, n), dtype=torch.float16, device=DEVICE)

        try:
            correct, output, ref = run_matmul_with_kernel(kernel, a, b)
            if not correct:
                results["failures"] += 1
                max_diff = (output - ref).abs().max().item()
                results["failed_iterations"].append((i, max_diff))
        except Exception as e:
            results["failures"] += 1
            results["exception_iterations"].append((i, str(e)))

    return results


# ==============================================================================
# Pytest Tests
# ==============================================================================


@pytest.mark.skipif(
    not is_cuda() or torch.cuda.get_device_capability()[0] != 9,
    reason="Requires Hopper GPU",
)
def test_bug1_late_barrier_a():
    """Test Bug 1: Late barrier_wait for A (moved after MMA load).

    The barrier_wait for matrix A is MOVED to AFTER the async_dot call.
    A spin loop provides "usually enough" delay for TMA to complete.
    async_dot reads potentially unready data - DATA RACE!
    barrier_wait after async_dot is TOO LATE - damage is already done.

    With random delays, this should fail more consistently.
    """
    results = run_multiple_iterations(
        matmul_kernel_bug1_late_barrier_a,
        num_iterations=100,
    )
    return results


@pytest.mark.skipif(
    not is_cuda() or torch.cuda.get_device_capability()[0] != 9,
    reason="Requires Hopper GPU",
)
def test_bug2_missing_barrier_b():
    """Test Bug 2: Missing barrier_wait for B (spin-wait only, no barrier).

    The barrier_wait for matrix B is REMOVED entirely.
    A spin loop provides "usually enough" delay for TMA to complete.
    async_dot reads potentially unready data - DATA RACE!
    No barrier_wait for B at all - completely removed!

    With random delays, this should fail more consistently.
    """
    results = run_multiple_iterations(
        matmul_kernel_bug2_missing_barrier_b,
        num_iterations=100,
    )
    return results


def print_summary_report(all_results):
    """Print a summary report of all test results."""
    print("\n" + "=" * 70)
    print("TEST SUMMARY REPORT")
    print("=" * 70)

    for test_name, results in all_results.items():
        failures = results["failures"]
        total = results["total"]
        status = "PASS" if failures == 0 else "FAIL"

        print(f"\n{test_name}:")
        print(f"  Result: {failures}/{total} failures [{status}]")

        if results["failed_iterations"]:
            print("  Failed iterations (with max diff):")
            for iter_num, max_diff in results["failed_iterations"]:
                print(f"    - Iteration {iter_num}: max_diff = {max_diff:.6f}")

        if results["exception_iterations"]:
            print("  Exception iterations:")
            for iter_num, exc in results["exception_iterations"]:
                print(f"    - Iteration {iter_num}: {exc}")

    print("\n" + "=" * 70)


# ==============================================================================
# Main: Run all tests with verbose output
# ==============================================================================


@click.command()
@click.option(
    "--iters",
    "-i",
    default=100,
    type=int,
    help="Number of iterations for all tests (default: 100)",
)
@click.option(
    "--bug",
    "-b",
    default="all",
    type=click.Choice(["all", "1", "2"]),
    help="Which bug test to run: 'all' (default), '1' (late barrier), or '2' (missing barrier)",
)
@click.option(
    "--tritonparse",
    "tritonparse_path",
    default=None,
    is_flag=False,
    flag_value="/tmp/tritonparse_logs/",
    help="Enable tritonparse structured logging. Optionally specify output path (default: /tmp/tritonparse_logs/)",
)
def main(iters, bug, tritonparse_path):
    if not is_cuda() or torch.cuda.get_device_capability()[0] != 9:
        print("Skipping: No Hopper GPU found")
        return

    if tritonparse_path:
        import tritonparse.structured_logging

        tritonparse.structured_logging.init(
            tritonparse_path,
            enable_trace_launch=True,
            enable_sass_dump=True,
        )

    print("=" * 70)
    print("Data Race Detection Test Suite for Hopper GEMM")
    print("=" * 70)
    print(f"Matrix size: {M} x {N} x {K}")
    print(f"Device: {DEVICE}")
    print(f"Bug filter: {bug}")
    print()
    print("These tests contain intentional bugs that cause data races.")
    print("Without random delay injection, they may pass due to timing luck.")
    print("With your random delay detector, they should fail consistently.")
    print()
    print("Running tests... (results will be shown at the end)")
    print("=" * 70)

    # Run tests and collect results
    all_results = {}
    if bug in ("all", "1"):
        all_results["Bug 1: Late barrier_wait for A"] = run_multiple_iterations(
            matmul_kernel_bug1_late_barrier_a,
            num_iterations=iters,
        )
    if bug in ("all", "2"):
        all_results["Bug 2: Missing barrier_wait for B"] = run_multiple_iterations(
            matmul_kernel_bug2_missing_barrier_b,
            num_iterations=iters,
        )

    # Print summary report at the end
    print_summary_report(all_results)

    print("Test suite complete!")
    print("=" * 70)


if __name__ == "__main__":
    main()

```

`tests/hang_test/test_hang.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.
import os

import torch
import triton
import triton.language as tl


@triton.jit
def add_kernel(
    a_ptr,
    b_ptr,
    c_ptr,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    if pid == 0:
        while pid == pid:
            tl.atomic_add(a_ptr, 1)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements

    a = tl.load(a_ptr + offsets, mask=mask)
    b = tl.load(b_ptr + offsets, mask=mask)
    c = a + b
    tl.store(c_ptr + offsets, c, mask=mask)


def tensor_add(a, b):
    n_elements = a.numel()
    c = torch.empty_like(a)
    BLOCK_SIZE = 1024
    grid = (triton.cdiv(n_elements, BLOCK_SIZE),)
    add_kernel[grid](a, b, c, n_elements, BLOCK_SIZE)
    return c


def test_tensor_add():
    torch.manual_seed(0)
    size = (1024, 1024)
    a = torch.randn(size, device="cuda", dtype=torch.float32)
    b = torch.randn(size, device="cuda", dtype=torch.float32)

    # Test Triton kernel
    c_triton = tensor_add(a, b)
    c_triton.sum()
    tensor_add(a, b)
    print("Triton kernel executed successfully")


if __name__ == "__main__":
    pid = os.getpid()
    print("=" * 60)
    print(f"Process ID: {pid}")
    print("=" * 60)
    test_tensor_add()

```

`tests/proton_tests/vector-add-instrumented.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.
from typing import NamedTuple

import torch
import triton
import triton.language as tl
import triton.profiler as proton
import triton.profiler.language as pl

DEVICE = triton.runtime.driver.active.get_active_torch_device()

pl.enable_semantic("triton")


def metadata_fn(grid: tuple, metadata: NamedTuple, args: dict):
    BLOCK_SIZE = args["BLOCK_SIZE"]
    return {"name": f"add_{BLOCK_SIZE}"}


@triton.jit(launch_metadata=metadata_fn)
def add_kernel(
    x_ptr,  # *Pointer* to first input vector.
    y_ptr,  # *Pointer* to second input vector.
    output_ptr,  # *Pointer* to output vector.
    n_elements,  # Size of the vector.
    BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
    # NOTE: `constexpr` so it can be used as a shape value.
):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements
    with pl.scope("load_ops"):
        x = tl.load(x_ptr + offsets, mask=mask)
        y = tl.load(y_ptr + offsets, mask=mask)
        output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)


def add(x: torch.Tensor, y: torch.Tensor):
    output = torch.empty_like(x)
    assert x.device == DEVICE and y.device == DEVICE and output.device == DEVICE
    n_elements = output.numel()

    def grid(meta):
        return (triton.cdiv(n_elements, meta["BLOCK_SIZE"]),)

    mode = proton.mode.Default(metric_type="cycle", optimizations="clock32")
    proton.start("vector", data="trace", backend="instrumentation", mode=mode)

    add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024, num_warps=1)

    proton.finalize()
    return output


torch.manual_seed(0)
size = 98432
x = torch.rand(size, device=DEVICE)
y = torch.rand(size, device=DEVICE)
output_torch = x + y
output_triton = add(x, y)
output_triton = add(x, y)

```

`tests/py_add/test_add.py`:

```py
# Copyright (c) Meta Platforms, Inc. and affiliates.
"""
Test script for CUTracer trace format validation.

Uses a PT2 compiled kernel to ensure deterministic kernel generation.
This guarantees the same Triton kernel is traced across different runs,
enabling reliable cross-format validation (Mode 0/1/2 comparison).
"""

import torch


@torch.compile
def simple_add(a: torch.Tensor, b: torch.Tensor) -> torch.Tensor:
    """Simple addition compiled to a single Triton kernel."""
    return a + b - 1.0


def test_compiled_add():
    """Test with PT2 compiled kernel for deterministic tracing."""
    device = torch.device("cuda")

    # Use fixed size for deterministic kernel generation
    # 1024 elements is small enough to be fast, large enough for meaningful traces
    a = torch.randn(1024, dtype=torch.float32, device=device)
    b = torch.randn(1024, dtype=torch.float32, device=device)

    print("Testing PT2 compiled kernel (simple_add)")
    print(f"  Input shape: {a.shape}")
    print(f"  Input dtype: {a.dtype}")
    compiled_function = torch.compile(simple_add)
    result = compiled_function(a, b)
    torch.cuda.synchronize()

    print(f"  Result shape: {result.shape}")
    print(f"  Result sum: {result.sum().item():.4f}")
    print("  âœ… PT2 compiled kernel executed successfully")

    return result


# Keep the old eager mode test for backward compatibility
def test_tensor_addition_on_gpu():
    """Legacy eager mode test (deprecated, use test_compiled_add instead)."""
    device = torch.device("cuda")

    a = torch.tensor([1, 2, 3], dtype=torch.float32, device=device)
    b = torch.tensor([2, 3, 4], dtype=torch.float32, device=device)

    print("Tensor A:", a)
    print("Tensor B:", b)
    for _ in range(3):
        a = a + b

    print("Result (A + B):", a)

    return a


if __name__ == "__main__":
    # Use PT2 compiled kernel by default for deterministic tracing
    test_compiled_add()

```

`tests/unit/Makefile`:

```
# Copyright (c) Meta Platforms, Inc. and affiliates.
# Makefile for CUTracer unit tests

CXX = g++
CXXFLAGS = -std=c++17 -Wall -Wextra -I../../include

# Test executables
TESTS = test_instr_category

.PHONY: all clean test

all: $(TESTS)

test_instr_category: test_instr_category.cpp ../../include/instr_category.h
	$(CXX) $(CXXFLAGS) -o $@ $<

test: $(TESTS)
	@echo "Running unit tests..."
	@for t in $(TESTS); do \
		echo ""; \
		echo ">>> Running $$t"; \
		./$$t || exit 1; \
	done
	@echo ""
	@echo "All tests passed!"

clean:
	rm -f $(TESTS)

```

`tests/unit/test_instr_category.cpp`:

```cpp
/*
 * SPDX-FileCopyrightText: Copyright (c) Meta Platforms, Inc. and affiliates.
 * SPDX-License-Identifier: MIT
 * See LICENSE file in the root directory for Meta's license terms.
 */

/**
 * @file test_instr_category.cpp
 * @brief Unit tests for instruction category detection.
 *
 * Compile and run:
 *   g++ -std=c++17 -I../../include -o test_instr_category test_instr_category.cpp
 *   ./test_instr_category
 */

#include <cassert>
#include <cstring>
#include <iostream>
#include <string>

#include "instr_category.h"

// Test counter
static int tests_passed = 0;
static int tests_failed = 0;

#define TEST(name)                             \
  std::cout << "Testing: " << #name << "... "; \
  if (test_##name()) {                         \
    std::cout << "âœ… PASSED" << std::endl;     \
    tests_passed++;                            \
  } else {                                     \
    std::cout << "âŒ FAILED" << std::endl;     \
    tests_failed++;                            \
  }

// ============================================================================
// Test Cases
// ============================================================================

bool test_detect_utcmma() {
  // Test UTCHMMA detection (Blackwell tensor core)
  const char* sass = "UTCHMMA gdesc[UR44], gdesc[UR46], tmem[UR53], tmem[UR60], idesc[UR61], UP1 ;";
  InstrCategory cat = detect_instr_category(sass);
  return cat == InstrCategory::MMA;
}

bool test_detect_utcmma_variants() {
  // Test different UTC*MMA variants (Blackwell tensor core instructions)
  const char* variants[] = {
      // Blackwell specific patterns
      "UTCHMMA gdesc[UR44], gdesc[UR46], tmem[UR53], tmem[UR60], idesc[UR61], UP1 ;",
      "UTCIMMA gdesc[UR44], gdesc[UR46], tmem[UR53], tmem[UR60], idesc[UR61], UP1 ;",
      "UTCQMMA gdesc[UR44], gdesc[UR46], tmem[UR53], tmem[UR60], idesc[UR61], UP1 ;",
      "UTCOMMA gdesc[UR44], gdesc[UR46], tmem[UR53], tmem[UR60], idesc[UR61], UP1 ;",
      "/*0a00*/   UTCHMMA gdesc[UR44], gdesc[UR46], tmem[UR53], tmem[UR60], idesc[UR61], UP1 ;",  // With PC prefix
      // Hopper GMMA
      "HGMMA.16816.F16 desc[UR8], desc[UR12], desc[UR0], R24 ;",
  };

  for (const char* sass : variants) {
    if (detect_instr_category(sass) != InstrCategory::MMA) {
      std::cerr << "Failed for: " << sass << std::endl;
      return false;
    }
  }
  return true;
}

bool test_detect_tma_load() {
  // Test TMA load detection
  const char* sass = "UTMALDG.2D.CTA_GROUP::1 [UR8], [R16], P0";
  InstrCategory cat = detect_instr_category(sass);
  return cat == InstrCategory::TMA;
}

bool test_detect_tma_store() {
  // Test TMA store detection
  const char* sass = "UTMASTG.2D [UR8], [R16], R32";
  InstrCategory cat = detect_instr_category(sass);
  return cat == InstrCategory::TMA;
}

bool test_detect_sync() {
  // Test sync instruction detection
  const char* sass = "WARPGROUP.DEPBAR.LE 0x2";
  InstrCategory cat = detect_instr_category(sass);
  return cat == InstrCategory::SYNC;
}

bool test_detect_none() {
  // Test that non-matching instructions return NONE
  const char* sass_list[] = {
      "FADD R0, R1, R2", "LDG.E.64 R8, [R16]", "STG.E.64 [R16], R8", "MOV R0, R1", "EXIT", nullptr,
  };

  for (int i = 0; sass_list[i] != nullptr; i++) {
    if (detect_instr_category(sass_list[i]) != InstrCategory::NONE) {
      std::cerr << "Should be NONE: " << sass_list[i] << std::endl;
      return false;
    }
  }

  // Test nullptr
  if (detect_instr_category(nullptr) != InstrCategory::NONE) {
    std::cerr << "nullptr should return NONE" << std::endl;
    return false;
  }

  return true;
}

bool test_category_name() {
  // Test category name lookup
  if (strcmp(get_instr_category_name(InstrCategory::MMA), "MMA") != 0) return false;
  if (strcmp(get_instr_category_name(InstrCategory::TMA), "TMA") != 0) return false;
  if (strcmp(get_instr_category_name(InstrCategory::SYNC), "SYNC") != 0) return false;
  if (strcmp(get_instr_category_name(InstrCategory::NONE), "NONE") != 0) return false;
  return true;
}

bool test_pattern_description() {
  // Test pattern description lookup - Blackwell pattern
  const char* desc = get_instr_pattern_description("UTCHMMA gdesc[UR44], gdesc[UR46], tmem[UR53] ;");
  if (desc == nullptr) return false;
  if (strstr(desc, "Blackwell UTCHMMA") == nullptr) return false;

  // Non-matching should return nullptr
  desc = get_instr_pattern_description("FADD R0, R1, R2");
  if (desc != nullptr) return false;

  return true;
}

bool test_is_instr_category() {
  // Test isInstrCategory helper - Blackwell pattern
  const char* mma_sass = "UTCHMMA gdesc[UR44], gdesc[UR46], tmem[UR53], tmem[UR60], idesc[UR61], UP1 ;";
  if (!is_instr_category(mma_sass, InstrCategory::MMA)) return false;
  if (is_instr_category(mma_sass, InstrCategory::TMA)) return false;
  if (is_instr_category(mma_sass, InstrCategory::SYNC)) return false;

  return true;
}

bool test_get_patterns_for_category() {
  // Test getting patterns for a category
  auto mma_patterns = get_patterns_for_category(InstrCategory::MMA);
  if (mma_patterns.empty()) return false;

  // Should contain UTCHMMA (Blackwell)
  bool found_utchmma = false;
  for (const char* p : mma_patterns) {
    if (strcmp(p, "UTCHMMA") == 0) {
      found_utchmma = true;
      break;
    }
  }
  if (!found_utchmma) return false;

  // NONE category should have no patterns
  auto none_patterns = get_patterns_for_category(InstrCategory::NONE);
  if (!none_patterns.empty()) return false;

  return true;
}

// ============================================================================
// Main
// ============================================================================

int main() {
  std::cout << "========================================" << std::endl;
  std::cout << "Instruction Category Unit Tests" << std::endl;
  std::cout << "========================================" << std::endl;

  TEST(detect_utcmma);
  TEST(detect_utcmma_variants);
  TEST(detect_tma_load);
  TEST(detect_tma_store);
  TEST(detect_sync);
  TEST(detect_none);
  TEST(category_name);
  TEST(pattern_description);
  TEST(is_instr_category);
  TEST(get_patterns_for_category);

  std::cout << "========================================" << std::endl;
  std::cout << "Results: " << tests_passed << " passed, " << tests_failed << " failed" << std::endl;
  std::cout << "========================================" << std::endl;

  return tests_failed > 0 ? 1 : 0;
}

```

`tests/vectoradd/Makefile`:

```
NVCCFLAGS := -arch=all -O3 -g -lineinfo -Wno-deprecated-gpu-targets

all: vectoradd.cu
	nvcc $(NVCCFLAGS) vectoradd.cu -o vectoradd

clean:
	rm -f vectoradd

```

`tests/vectoradd/vectoradd.cu`:

```cu
// Copyright (c) Meta Platforms, Inc. and affiliates.
#include <math.h>
#include <stdio.h>
#include <stdlib.h>

#define CUDA_SAFECALL(call)                                                                                  \
  {                                                                                                          \
    call;                                                                                                    \
    cudaError err = cudaGetLastError();                                                                      \
    if (cudaSuccess != err) {                                                                                \
      fprintf(stderr, "Cuda error in function '%s' file '%s' in line %i : %s.\n", #call, __FILE__, __LINE__, \
              cudaGetErrorString(err));                                                                      \
      fflush(stderr);                                                                                        \
      exit(EXIT_FAILURE);                                                                                    \
    }                                                                                                        \
  }

// CUDA kernel. Each thread takes care of one element of c
__global__ void vecAdd(double* a, double* b, double* c, int n) {
  // Get our global thread ID
  auto id = blockIdx.x * blockDim.x + threadIdx.x;

  // Make sure we do not go out of bounds
  if (id < n) c[id] = a[id] + b[id];
}

int main(int argc, char* argv[]) {
  // Size of vectors
  int n = 1024;
  if (argc > 1) n = atoi(argv[1]);

  // Host input vectors
  double* h_a;
  double* h_b;
  // Host output vector
  double* h_c;

  // Device input vectors
  double* d_a;
  double* d_b;
  // Device output vector
  double* d_c;

  // Size, in bytes, of each vector
  size_t bytes = n * sizeof(double);

  // Allocate memory for each vector on host
  h_a = (double*)malloc(bytes);
  h_b = (double*)malloc(bytes);
  h_c = (double*)malloc(bytes);

  // Allocate memory for each vector on GPU
  cudaMalloc(&d_a, bytes);
  cudaMalloc(&d_b, bytes);
  cudaMalloc(&d_c, bytes);

  int i;
  // Initialize vectors on host
  for (i = 0; i < n; i++) {
    h_a[i] = sin(i) * sin(i);
    h_b[i] = cos(i) * cos(i);
    h_c[i] = 0;
  }

  // Copy host vectors to device
  cudaMemcpy(d_a, h_a, bytes, cudaMemcpyHostToDevice);
  cudaMemcpy(d_b, h_b, bytes, cudaMemcpyHostToDevice);
  cudaMemcpy(d_c, h_c, bytes, cudaMemcpyHostToDevice);

  int blockSize, gridSize;

  // Number of threads in each thread block
  blockSize = 128;

  // Number of thread blocks in grid
  gridSize = (n + blockSize - 1) / blockSize;

  // Execute the kernel
  CUDA_SAFECALL((vecAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n)));

  // Copy array back to host
  cudaMemcpy(h_c, d_c, bytes, cudaMemcpyDeviceToHost);

  // Sum up vector c and print result divided by n, this should equal 1 within
  // error
  double sum = 0;
  for (i = 0; i < n; i++) sum += h_c[i];
  printf("Final sum = %f; sum/n = %f (should be ~1)\n", sum, sum / n);
  // call kernel again
  CUDA_SAFECALL((vecAdd<<<gridSize, blockSize>>>(d_a, d_b, d_c, n)));
  printf("Second call to kernel\n");
  // Release device memory
  cudaFree(d_a);
  cudaFree(d_b);
  cudaFree(d_c);

  // Release host memory
  free(h_a);
  free(h_b);
  free(h_c);

  return 0;
}

```

`tests/vectoradd_smem/Makefile`:

```
NVCCFLAGS := -arch=all -O3 -g -lineinfo -Wno-deprecated-gpu-targets

all: vectoradd_smem.cu
	nvcc $(NVCCFLAGS) vectoradd_smem.cu -o vectoradd_smem

clean:
	rm -f vectoradd_smem

```

`tests/vectoradd_smem/vectoradd_smem.cu`:

```cu
// Copyright (c) Meta Platforms, Inc. and affiliates.
#include <math.h>
#include <stdio.h>
#include <stdlib.h>

#define CUDA_SAFECALL(call)                                                                                  \
  {                                                                                                          \
    call;                                                                                                    \
    cudaError err = cudaGetLastError();                                                                      \
    if (cudaSuccess != err) {                                                                                \
      fprintf(stderr, "Cuda error in function '%s' file '%s' in line %i : %s.\n", #call, __FILE__, __LINE__, \
              cudaGetErrorString(err));                                                                      \
      fflush(stderr);                                                                                        \
      exit(EXIT_FAILURE);                                                                                    \
    }                                                                                                        \
  }

#define BLOCK_SIZE 32

// CUDA kernel using shared memory for vector addition
// This kernel demonstrates shared memory usage for CUTracer testing
__global__ void vecAddSmem(double* a, double* b, double* c, int n) {
  __shared__ double s_a[BLOCK_SIZE];
  __shared__ double s_b[BLOCK_SIZE];

  auto tid = threadIdx.x;
  auto gid = blockIdx.x * blockDim.x + threadIdx.x;

  if (gid < n) {
    // Step 1: Load from global memory to shared memory
    s_a[tid] = a[gid];
    s_b[tid] = b[gid];

    __syncthreads();

    // Step 2: Read from shared memory and compute
    double sum = s_a[tid] + s_b[tid];

    // Step 3: Write result back to global memory
    c[gid] = sum;
  }
}

int main(int argc, char* argv[]) {
  int n = 32;
  if (argc > 1) n = atoi(argv[1]);

  double *h_a, *h_b, *h_c;
  double *d_a, *d_b, *d_c;
  size_t bytes = n * sizeof(double);

  // Allocate host memory
  h_a = (double*)malloc(bytes);
  h_b = (double*)malloc(bytes);
  h_c = (double*)malloc(bytes);

  // Allocate device memory
  cudaMalloc(&d_a, bytes);
  cudaMalloc(&d_b, bytes);
  cudaMalloc(&d_c, bytes);

  // Initialize host data
  for (int i = 0; i < n; i++) {
    h_a[i] = sin(i) * sin(i);
    h_b[i] = cos(i) * cos(i);
  }

  // Copy to device
  cudaMemcpy(d_a, h_a, bytes, cudaMemcpyHostToDevice);
  cudaMemcpy(d_b, h_b, bytes, cudaMemcpyHostToDevice);

  // Launch kernel
  int gridSize = (n + BLOCK_SIZE - 1) / BLOCK_SIZE;
  CUDA_SAFECALL((vecAddSmem<<<gridSize, BLOCK_SIZE>>>(d_a, d_b, d_c, n)));

  // Copy result back
  cudaMemcpy(h_c, d_c, bytes, cudaMemcpyDeviceToHost);

  // Verify result
  double sum = 0;
  for (int i = 0; i < n; i++) sum += h_c[i];
  printf("Final sum = %f; sum/n = %f (should be ~1)\n", sum, sum / n);

  // Cleanup
  cudaFree(d_a);
  cudaFree(d_b);
  cudaFree(d_c);
  free(h_a);
  free(h_b);
  free(h_c);

  return 0;
}

```