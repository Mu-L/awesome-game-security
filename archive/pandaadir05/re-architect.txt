Project Path: arc_pandaadir05_re-architect_88a5bjx1

Source Tree:

```txt
arc_pandaadir05_re-architect_88a5bjx1
├── LICENSE
├── MANIFEST.in
├── README.md
├── RELEASE_NOTES.md
├── config.yaml
├── docs
│   ├── api_reference.md
│   ├── installation.md
│   ├── quick_start.md
│   └── user_manual.md
├── main.py
├── pytest.ini
├── requirements-dev.txt
├── requirements.txt
├── setup.py
├── src
│   ├── __init__.py
│   ├── analysis
│   │   ├── __init__.py
│   │   ├── data_structure_analyzer.py
│   │   ├── dynamic_analyzer.py
│   │   ├── enhanced_static_analyzer.py
│   │   ├── static_analyzer.py
│   │   └── unified_static_analyzer.py
│   ├── auth
│   │   ├── __init__.py
│   │   └── middleware.py
│   ├── comparison
│   │   ├── __init__.py
│   │   ├── analyzer.py
│   │   ├── comparator.py
│   │   ├── models.py
│   │   ├── routes.py
│   │   └── store.py
│   ├── core
│   │   ├── __init__.py
│   │   ├── binary_loader.py
│   │   ├── config.py
│   │   ├── error_handling.py
│   │   ├── performance.py
│   │   └── pipeline.py
│   ├── decompilers
│   │   ├── __init__.py
│   │   ├── base_decompiler.py
│   │   ├── binary_ninja_decompiler.py
│   │   ├── decompiler_factory.py
│   │   ├── ghidra_decompiler.py
│   │   ├── ida_decompiler.py
│   │   ├── internal_ir_decompiler.py
│   │   └── mock_decompiler.py
│   ├── ir
│   │   ├── __init__.py
│   │   └── ir_core.py
│   ├── llm
│   │   ├── __init__.py
│   │   └── function_summarizer.py
│   ├── optimization
│   │   ├── __init__.py
│   │   └── optimizer.py
│   ├── security
│   │   └── __init__.py
│   ├── test_generation
│   │   ├── __init__.py
│   │   └── test_generator.py
│   ├── testing
│   │   └── __init__.py
│   ├── unpacking
│   │   ├── __init__.py
│   │   └── symbolic_unpacker.py
│   └── visualization
│       ├── __init__.py
│       ├── mock_data.py
│       ├── run_mock_server.py
│       └── server.py
└── tests
    ├── integration
    │   ├── test_dynamic_disabled_pipeline.py
    │   ├── test_obfuscation_pipeline.py
    │   ├── test_pipeline.py
    │   └── test_unpacking_pipeline.py
    └── unit
        ├── test_comparison_routes.py
        ├── test_core.py
        ├── test_dynamic_analyzer.py
        ├── test_llm.py
        ├── test_obfuscation_optimizer.py
        └── test_symbolic_unpacker.py

```

`LICENSE`:

```
MIT License

Copyright (c) 2025 RE-Architect Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

`MANIFEST.in`:

```in
include README.md
include LICENSE
include requirements.txt
include requirements-dev.txt
include config.yaml
include pytest.ini
recursive-include src *.py
recursive-include docs *.md
recursive-exclude * __pycache__
recursive-exclude * *.py[co]
recursive-exclude * *.so
recursive-exclude * .DS_Store
```

`README.md`:

```md
# RE-Architect

[![Build Status](https://github.com/pandaadir05/re-architect/workflows/RE-Architect%20CI/badge.svg)](https://github.com/pandaadir05/re-architect/actions)
[![Python Version](https://img.shields.io/badge/python-3.11%2B-blue)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/license-MIT-green)](LICENSE)

RE-Architect is an advanced automated reverse-engineering platform that transforms binary files into human-readable function summaries, data structure definitions, and executable test harnesses. The system leverages modern binary analysis techniques and machine learning to provide comprehensive analysis results in an efficient timeframe.

## Features

- **Binary Analysis**: Decompiles and analyzes binary files using advanced techniques
- **Function Summarization**: Generates concise, accurate summaries of function behaviors using machine learning
- **Data Structure Recovery**: Identifies and reconstructs complex data structures from binaries
- **Test Harness Generation**: Creates runnable test harnesses for recovered functions with built-in safety constraints
- **Interactive Visualization**: Presents results through an intuitive user interface with configurable views
- **Multiple Decompiler Support**: Seamlessly integrates with Ghidra, IDA Pro, and Binary Ninja
- **Cross-Platform**: Works on Windows, Linux, and macOS

## Architecture

RE-Architect consists of several integrated components working together to provide a comprehensive reverse engineering solution:

1. **Binary Loader**: Handles various binary formats (ELF, PE, Mach-O) and architectures (x86, ARM, MIPS)
2. **Decompiler Bridge**: Interfaces with leading decompilers using a uniform abstraction layer
3. **Analysis Engine**: Performs static, dynamic, and symbolic analysis to extract program behavior
4. **Machine Learning Interpreter**: Generates natural language explanations of code functionality
5. **Test Generator**: Creates safe, executable test harnesses with appropriate input generation
6. **Visualization Layer**: Provides interactive graphical representations of program structure and data flow

## Quick Start

```bash
# Clone the repository
git clone https://github.com/pandaadir05/re-architect.git
cd re-architect

# Install dependencies
pip install -r requirements.txt

# Install the package in development mode
pip install -e .

# Run analysis on a binary
python main.py binary_file.exe --config config.yaml
```

## Technologies

- **Core Analysis**: Python 3.11+ with specialized binary analysis libraries
- **Decompilation**: Integration with Ghidra, IDA Pro, and Binary Ninja
- **Machine Learning Components**: Natural language processing for code understanding
- **Symbolic Execution**: Integration with angr framework
- **Dynamic Analysis**: Sandboxed execution environments using Docker and QEMU
- **Visualization**: Flask-based web interface with interactive graphs
- **Testing**: pytest for unit and integration testing
- **CI/CD**: GitHub Actions for automated testing and deployment

## Documentation

- [Installation Guide](docs/installation.md) - Detailed setup instructions for different environments
- [Quick Start Guide](docs/quick_start.md) - Get up and running in minutes
- [User Manual](docs/user_manual.md) - Comprehensive usage guide and tutorials
- [API Reference](docs/api_reference.md) - Complete Python API documentation

## Requirements

- Python 3.11+
- 64-bit operating system (Windows, Linux, or macOS)
- 16GB+ RAM recommended for analyzing large binaries
- CUDA-compatible GPU (optional, for accelerated analysis)
- One or more supported decompilers (Ghidra, IDA Pro, or Binary Ninja)

## Example

```python
from src.core.pipeline import ReversePipeline
from src.core.config import Config

# Initialize the pipeline with configuration
config = Config.from_file("config.yaml")
pipeline = ReversePipeline(config)

# Analyze a binary
results = pipeline.analyze("path/to/binary.exe")

# Access results
functions = results["functions"]
metadata = results["metadata"]
```

## Performance

Performance varies based on binary complexity, analysis depth, and available decompilers. The system supports both lightweight analysis for quick insights and comprehensive deep analysis for detailed reverse engineering work.

## Contributing

Contributions are welcome. Please follow standard GitHub pull request procedures to submit your changes.

## License

MIT License - See [LICENSE](LICENSE) file for details.

## Acknowledgements

- The Ghidra team at NSA for their open-source decompiler
- The angr symbolic execution framework
- All open-source libraries used in this project
- The binary analysis research community

```

`RELEASE_NOTES.md`:

```md
# RE-Architect v1.0.0 Release Notes

## First Stable Release

We're excited to announce the first stable release of RE-Architect, a comprehensive reverse engineering platform that combines traditional static analysis with modern AI-powered insights.

## Key Features

### Multi-Decompiler Support
- **Ghidra Integration** - Full headless analysis support
- **IDA Pro Support** - Professional decompiler integration  
- **Binary Ninja Support** - Modern analysis platform integration
- **Auto-Detection** - Automatically selects best available decompiler

### AI-Powered Analysis
- **LLM Integration** - OpenAI GPT-4 and Azure OpenAI support
- **Function Summarization** - AI-generated function descriptions and behavior analysis
- **Security Analysis** - Automated vulnerability detection and security notes
- **Batch Processing** - Efficient analysis of multiple functions

### Comprehensive Static Analysis
- **Enhanced Function Detection** - Advanced static analysis with complexity scoring
- **Data Structure Recovery** - Automated identification of data structures and types
- **Cross-Reference Analysis** - Function call graphs and dependency mapping
- **Symbol Analysis** - Import/export table analysis

### Interactive Web Interface  
- **Real-time Visualization** - Interactive exploration of analysis results
- **Function Browser** - Navigate through analyzed functions with detailed views
- **Call Graph Visualization** - Interactive function relationship mapping
- **Export Capabilities** - Multiple output formats for reports

### Professional Testing & CI/CD
- **70 Comprehensive Tests** - Full test suite with 36% code coverage
- **GitHub Actions Integration** - Automated testing and deployment
- **Multiple Python Versions** - Support for Python 3.8 through 3.12
- **Cross-Platform Support** - Windows, Linux, and macOS compatibility

##  Technical Highlights

- **Robust Error Handling** - Comprehensive error management and logging
- **Configurable Pipeline** - YAML-based configuration system
- **Extensible Architecture** - Plugin system for custom analyzers
- **Performance Optimized** - Efficient processing of large binaries
- **Memory Management** - Smart resource utilization for complex analyses

##  Installation

```bash
# Install from source
git clone https://github.com/pandaadir05/re-architect.git
cd re-architect
pip install -r requirements.txt
pip install -e .

# Quick start
python main.py analyze binary.exe --output ./results
```

##  What's Next

This stable release provides a solid foundation for reverse engineering workflows. Future releases will focus on:

- Enhanced dynamic analysis capabilities
- Additional decompiler integrations
- Advanced AI model support
- Performance optimizations
- Extended plugin ecosystem

##  Quality Metrics

- **Test Coverage**: 36% with 70 passing tests
- **Code Quality**: Comprehensive linting and style checks
- **Documentation**: Complete API reference, user manual, and quick start guide
- **CI/CD**: Full automated testing pipeline

##  Acknowledgments

Special thanks to the reverse engineering community and open source contributors who made this project possible.

---

**Full Changelog**: Initial stable release with complete feature set
**Download**: [v1.0.0 Release](https://github.com/pandaadir05/re-architect/releases/tag/v1.0.0)
```

`config.yaml`:

```yaml
# RE-Architect Configuration File

# Decompiler settings
decompiler:
  # Default decompiler to use
  default: ghidra
  
  # Ghidra settings
  ghidra:
    path: null  # Set this to your Ghidra installation path
    headless: true
    timeout: 600  # seconds
    analyze_all: true
    analyze_data: true
  
  # IDA Pro settings
  ida:
    path: null  # Set this to your IDA Pro installation path
    headless: true
    timeout: 600  # seconds
    idc_path: null  # Path to custom IDC script
  
  # Binary Ninja settings
  binary_ninja:
    path: null  # Set this to your Binary Ninja installation path
    timeout: 600  # seconds
    analysis_mode: full  # basic, intermediate, full

# Analysis settings
analysis:
  # Static analysis settings
  static:
    function_analysis_depth: medium  # basic, medium, deep
    data_flow_analysis: true
    control_flow_analysis: true
    symbolic_execution: false
    taint_analysis: false
    string_analysis: true
    call_graph_analysis: true
  
  # Dynamic analysis settings
  dynamic:
    enable: false  # Set to true to enable dynamic analysis
    max_execution_time: 60  # seconds
    memory_limit: 2048  # MB
    sandbox_type: container  # none, container, vm
    record_syscalls: true
    record_network: true
    emulation_mode: qemu  # qemu, unicorn, native

# LLM settings (for function summarization)
llm:
  enable: true
  provider: openai  # openai, anthropic, huggingface, local
  model: gpt-4-turbo  # gpt-4-turbo, claude-3-opus, etc.
  api_key: null  # Set your API key here or use environment variable
  max_tokens: 8192
  temperature: 0.2
  cache_dir: ./cache/llm
  streaming: true
  function_templates:
    - name: standard
      prompt: "Analyze this function and provide a summary of its purpose, inputs, outputs, and potential vulnerabilities."
    - name: security
      prompt: "Analyze this function for security vulnerabilities such as buffer overflows, use-after-free, and other common issues."

# Data structure recovery settings
data_structures:
  recovery_method: hybrid  # static, dynamic, hybrid
  field_analysis: true
  pointer_analysis: true
  array_detection: true
  type_inference: true

# Test generation settings
test_generation:
  sanitizers: [address, undefined, memory, thread]
  fuzzing_time: 120  # seconds
  fuzzing_engine: libfuzzer  # libfuzzer, afl++, honggfuzz
  max_test_cases: 10
  compiler: clang  # gcc, clang, etc.
  compiler_flags: [-O0, -g, -fsanitize=address]
  coverage_threshold: 70  # percent

# Visualization settings
visualization:
  server:
    host: localhost
    port: 5000
    debug: false
    auth_required: false
  ui:
    theme: dark  # light, dark
    show_disassembly: true
    show_decompiled: true
    show_graph: true
    show_data_flow: true

# Output settings
output:
  detail_level: full  # basic, standard, full
  formats: [json, html, markdown]
  output_dir: ./results
  reports:
    generate_pdf: false
    include_graphs: true
    include_data_structures: true
    include_test_cases: true

# Performance settings
performance:
  parallelism: auto  # auto, or number of threads
  memory_limit: 8192  # MB
  disk_cache: true
  cache_dir: ./cache

# Logging settings
logging:
  level: info  # debug, info, warning, error, critical
  file: re-architect.log
  console: true
  format: "{time} [{level}] {message}"

```

`docs/api_reference.md`:

```md
# RE-Architect API Reference

This document provides detailed reference documentation for the RE-Architect Python API.

---

## Table of Contents

1. [Core Pipeline](#core-pipeline)
2. [Configuration](#configuration)
3. [Decompiler Integration](#decompiler-integration)
4. [Binary Loading](#binary-loading)
5. [Analysis Components](#analysis-components)
6. [LLM Integration](#llm-integration)
7. [Test Generation](#test-generation)
8. [Visualization](#visualization)
9. [Data Models](#data-models)
10. [Error Handling](#error-handling)
11. [Examples](#examples)
12. [Version Compatibility](#version-compatibility)
13. [Support](#support)

---

## Core Pipeline

### `ReversePipeline` Class

Main entry point for binary analysis.

```python
from src.core.pipeline import ReversePipeline
from src.core.config import Config

config = Config.from_file("config.yaml")
pipeline = ReversePipeline(config)

results = pipeline.analyze(
    binary_path="path/to/binary.exe",
    output_dir="./output",
    decompiler="ghidra",
    generate_tests=True
)
```

#### Constructor

`__init__(config: Config)`
Initialize the pipeline with a configuration.

* **Parameters:**

  * `config` (Config): Analysis configuration

#### Methods

`analyze(binary_path, output_dir=None, decompiler="auto", generate_tests=False)`
Perform complete analysis of a binary.

* **Parameters:**

  * `binary_path` (str | Path): Path to binary
  * `output_dir` (str | Path, optional): Output directory
  * `decompiler` (str): `"ghidra" | "ida" | "binja" | "auto"`
  * `generate_tests` (bool): Whether to generate test harnesses

* **Returns:**

  * `dict`: Results with functions, data structures, and metadata

* **Raises:**

  * `DecompilerError`, `AnalysisError`, `FileNotFoundError`

---

## Configuration

### `Config` Class

Manages configuration from YAML or dictionaries.

```python
from src.core.config import Config

# Load from file
config = Config.from_file("config.yaml")

# Load from dict
config = Config({
    "decompiler": {"default": "ghidra"},
    "llm": {"enable": True, "provider": "openai"}
})
```

#### Methods

* `from_file(path: str) -> Config` — load from YAML
* `get(key: str, default=None)` — retrieve using dot notation
* `set(key: str, value)` — update value
* `disable_llm()` / `enable_llm()` — toggle LLM features

---

## Decompiler Integration

### `DecompilerFactory`

Factory for decompiler instances.

```python
from src.decompilers.decompiler_factory import DecompilerFactory

factory = DecompilerFactory()
decompiler = factory.create("ghidra")
if decompiler.is_available():
    results = decompiler.decompile(binary_info)
```

* `create(name: str) -> BaseDecompiler`
* Returns a decompiler instance (`ghidra`, `ida`, `binja`, or `auto`)

### `BaseDecompiler`

Abstract base class for all decompilers.

* `is_available() -> bool`
* `decompile(binary_info: BinaryInfo) -> DecompiledCode`
* `get_decompiler_info() -> dict`

---

## Binary Loading

### `BinaryLoader`

Loads and parses binaries.

```python
from src.core.binary_loader import BinaryLoader

loader = BinaryLoader()
binary_info = loader.load("binary.exe", auto_unpack=True)

print(binary_info.format, binary_info.architecture)
```

### `BinaryInfo`

Container with attributes:

* `path`, `format`, `architecture`, `bit_width`, `endianness`
* `entry_point`, `sections`, `symbols`, `imports`, `exports`
* `compiler`, `stripped`

---

## Analysis Components

* **StaticAnalyzer** — static analysis of code
* **DataStructureAnalyzer** — recover structs
* **EnhancedStaticAnalyzer** — deeper function analysis

```python
from src.analysis.static_analyzer import StaticAnalyzer

analyzer = StaticAnalyzer(config)
results = analyzer.analyze(decompiled_code)
```

---

## LLM Integration

### `FunctionSummarizer`

Generates LLM-based summaries.

```python
from src.llm.function_summarizer import FunctionSummarizer

summarizer = FunctionSummarizer(config)
summary = summarizer.analyze_function_enhanced(func_info, context)
batch = summarizer.analyze_batch_enhanced(functions, context)
```

**Outputs:** `FunctionSummary` objects with attributes:

* `name`, `purpose`, `behavior`, `complexity_analysis`
* `arguments`, `return_value`, `side_effects`, `security_notes`
* `optimization_suggestions`, `confidence_score`

---

## Test Generation

### `TestGenerator`

Generates per-function harnesses.

```python
from src.test_generation.test_generator import TestGenerator

generator = TestGenerator(config)
tests = generator.generate(functions, data_structures)
```

---

## Visualization

### Start Web Server

```bash
python -m src.visualization.server --host 0.0.0.0 --port 8080
```

### From Code

```python
from src.visualization.server import app
app.run(host="localhost", port=5000, debug=True)
```

---

## Data Models

* **DecompiledCode**: container for functions, strings, types
* **FunctionInfo**: function attributes (name, address, size, calls, etc.)
* **Instruction**: assembly instruction details

---

## Error Handling

Custom exceptions:

* `DecompilerError` — decompilation failure
* `AnalysisError` — analysis failure

```python
from src.core.exceptions import DecompilerError, AnalysisError
```

---

## Examples

### Full Pipeline

```python
from src.core.pipeline import ReversePipeline
from src.core.config import Config

config = Config.from_file("config.yaml")
pipeline = ReversePipeline(config)

results = pipeline.analyze("sample_binary.exe", "./analysis", "ghidra", True)

print(f"Functions: {len(results['functions'])}")
print(f"Data structures: {len(results['data_structures'])}")
```

### Custom Decompiler

```python
from src.decompilers.base_decompiler import BaseDecompiler, DecompiledCode

class CustomDecompiler(BaseDecompiler):
    def is_available(self): return True
    def decompile(self, binary_info):
        dc = DecompiledCode(binary_info)
        dc.add_function(address=0x401000, code="int main(){return 0;}", name="main")
        return dc
```

---

## Version Compatibility

* **v1.0+**: Static analysis, LLM integration, web UI
* **v1.1+**: Dynamic analysis, batch mode
* **v1.2+**: Custom decompiler plugins, advanced test generation

---

## Support

* Source code: `src/` directory
* Examples: `tests/` directory
* Issues: [GitHub Issues](https://github.com/pandaadir05/re-architect/issues)
* Docs: [User Manual](user_manual.md)

---

```

`docs/installation.md`:

```md
# RE-Architect Installation Guide

This guide provides detailed instructions for setting up RE-Architect on various platforms.

## Prerequisites

RE-Architect requires the following:

- Python 3.11 or higher
- 64-bit operating system (Windows, Linux, or macOS)
- 16GB+ RAM recommended for analyzing large binaries
- CUDA-compatible GPU (optional, for accelerated analysis)

## Installation Steps

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/re-architect.git
cd re-architect
```

### 2. Set Up a Virtual Environment (Recommended)

#### On Windows:
```powershell
python -m venv venv
.\venv\Scripts\activate
```

#### On Linux/macOS:
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Install External Tools

RE-Architect can integrate with external decompilers for enhanced analysis:

#### Ghidra (Recommended)
1. Download Ghidra from [https://ghidra-sre.org/](https://ghidra-sre.org/)
2. Extract the archive to your preferred location
3. Update the `config.yaml` file with the path to your Ghidra installation

#### IDA Pro (Optional)
1. Install IDA Pro
2. Update the `config.yaml` file with the path to your IDA Pro installation

#### Binary Ninja (Optional)
1. Install Binary Ninja
2. Update the `config.yaml` file with the path to your Binary Ninja installation

### 5. Configure API Keys (Optional)

For LLM-based function summarization, you'll need to set up API keys:

1. Obtain an API key from OpenAI (https://platform.openai.com/) or Anthropic
2. Add the key to your `config.yaml` file:
   ```yaml
   llm:
     enable: true
     provider: openai
     model: gpt-4
     api_key: your_api_key_here
   ```

### 6. Verify Installation

Run the following command to verify that RE-Architect is properly installed:

```bash
python -m unittest discover tests
```

## Running RE-Architect

### Basic Usage

```bash
python main.py path/to/binary --output-dir ./output
```

### Advanced Options

```bash
# Use a specific decompiler
python main.py path/to/binary --decompiler ghidra

# Generate test harnesses
python main.py path/to/binary --generate-tests

# Start the visualization server after analysis
python main.py path/to/binary --serve

# Disable LLM-based analysis
python main.py path/to/binary --no-llm
```

## Troubleshooting

### Common Issues

1. **Missing dependencies**: Ensure all requirements are installed with `pip install -r requirements.txt`
2. **Decompiler path not found**: Update your `config.yaml` with the correct path to your decompiler
3. **LLM API errors**: Verify your API key is correct and has sufficient quota

### Getting Help

If you encounter issues not covered here, please:

1. Check the [GitHub Issues](https://github.com/your-username/re-architect/issues)
2. Join our [Discord community](https://discord.gg/your-invitation)
3. Contact support at support@re-architect.example.com

```

`docs/quick_start.md`:

```md
# RE-Architect Quick Start Guide

Get RE-Architect up and running in minutes with this step-by-step guide.

---

## Installation

### Prerequisites

* Python 3.8 or higher
* Git
* At least 4GB free disk space

### Step 1: Clone the Repository

```bash
git clone https://github.com/pandaadir05/re-architect.git
cd re-architect
```

### Step 2: Install Dependencies

```bash
pip install -r requirements.txt
pip install -e .
```

### Step 3: Configure Decompilers

Choose one or more decompilers and set their paths in `config.yaml`.

* **Ghidra (Recommended, Free)**: [Download here](https://github.com/NationalSecurityAgency/ghidra/releases)
* **IDA Pro (Commercial)**: [Install here](https://hex-rays.com/)
* **Binary Ninja (Commercial)**: [Install here](https://binary.ninja/)

Example `config.yaml`:

```yaml
decompiler:
  default: ghidra
  ghidra:
    path: /path/to/ghidra
    headless: true
  ida:
    path: /path/to/ida
  binary_ninja:
    path: /path/to/binaryninja

analysis:
  static:
    function_analysis_depth: medium
  dynamic:
    enable: false

llm:
  enable: false
  provider: openai
  api_key: your_api_key_here
```

---

## Basic Usage

### Command Line Analysis

```bash
python main.py analyze sample_binary.exe --output ./results
```

### Web Interface

Start the visualization server:

```bash
python -m src.visualization.server
```

Then open [http://localhost:5000](http://localhost:5000) in your browser.

### With LLM Features

```bash
export OPENAI_API_KEY="your_api_key_here"
python main.py sample.exe --generate-tests --serve
```

---

## Python API Usage

```python
from src.core.pipeline import ReversePipeline
from src.core.config import Config

# Load configuration
config = Config.from_file("config.yaml")

# Create pipeline
pipeline = ReversePipeline(config)

# Analyze binary
results = pipeline.analyze("sample.exe")

print(f"Found {len(results['functions'])} functions")
```

---

## Common Options

| Option             | Description           | Example                   |
| ------------------ | --------------------- | ------------------------- |
| `--config`         | Specify config file   | `--config my_config.yaml` |
| `--output`         | Set output directory  | `--output ./analysis`     |
| `--decompiler`     | Choose decompiler     | `--decompiler ghidra`     |
| `--generate-tests` | Create test harnesses | `--generate-tests`        |
| `--no-llm`         | Disable LLM analysis  | `--no-llm`                |
| `--verbose`        | Enable debug output   | `--verbose`               |
| `--serve`          | Start web server      | `--serve`                 |

---

## Example Workflow

```bash
# 1. Analyze with default settings
python main.py malware.exe --verbose

# 2. Review results
ls output/

# 3. Start web interface
python -m src.visualization.server
```

---

## Troubleshooting

**Decompiler not found**
Check `config.yaml` paths. For auto-detection:

```yaml
decompiler:
  ghidra:
    path: null
```

**No module named 'src'**

```bash
pip install -e .
```

**Memory errors**

```bash
python main.py large_binary.exe --no-llm
```

**Permission errors**

```bash
python main.py binary.exe --output ./analysis
```

---

## Next Steps

1. Explore examples in `tests/`
2. Read the [User Manual](user_manual.md)
3. Review the [API Reference](api_reference.md)
4. Enable LLM features for deeper analysis
5. Customize analysis with your own modules

---

Happy reverse engineering!

---

```

`docs/user_manual.md`:

```md
# RE-Architect User Manual

Welcome to **RE-Architect**, a reverse-engineering and binary-analysis platform that combines multi-decompiler static analysis with optional LLM-powered insights, an interactive web UI, and automated test generation.

## Table of Contents

1. [Introduction](#introduction)
2. [Prerequisites](#prerequisites)
3. [Installation](#installation)

   * [Standard](#standard-installation)
   * [Development](#development-setup)
   * [Docker](#docker-installation)
4. [Configuration](#configuration)
5. [Command Line Usage](#command-line-usage)
6. [Python API](#python-api)
7. [Web Interface](#web-interface)
8. [Advanced Features](#advanced-features)
9. [Decompiler Integration](#decompiler-integration)
10. [LLM Integration](#llm-integration)
11. [Understanding Results](#understanding-results)
12. [Troubleshooting](#troubleshooting)
13. [Best Practices](#best-practices)
14. [Performance Optimization](#performance-optimization)
15. [Test Generation](#test-generation)
16. [Integration (CI/CD & IDE)](#integration-cicd--ide)
17. [Support & Contributing](#support--contributing)

---

## Introduction

RE-Architect transforms binaries into:

* **Function summaries** and cross-references
* **Recovered data structures**
* **Automated test harnesses**
* **Comprehensive reports** and an **interactive web UI**

### Key Features

* **Multi-Decompiler Support**: Ghidra (free), IDA Pro, Binary Ninja
* **LLM-Powered Analysis** (optional): Function summaries, naming, patterns
* **Interactive Web Interface**: Search, call graphs, data types, reports
* **Automated Test Generation**: Per-function harnesses
* **Batch & Headless Modes**: For pipelines and CI

---

## Prerequisites

### System Requirements

* **OS**: Windows 10/11, Linux (Ubuntu 18.04+), or macOS 10.15+
* **Python**: 3.8+ (3.11 recommended)
* **Memory**: 8 GB minimum (16 GB recommended for large binaries)
* **Disk**: ≥ 2 GB free

### Optional Dependencies

* **Ghidra** (recommended, free)
* **IDA Pro** (commercial)
* **Binary Ninja** (commercial)
* **Docker** (for containerized runs)

---

## Installation

### Standard Installation

```bash
git clone https://github.com/pandaadir05/re-architect.git
cd re-architect
pip install -r requirements.txt
pip install -e .
```

### Development Setup

```bash
git clone https://github.com/pandaadir05/re-architect.git
cd re-architect
pip install -r requirements-dev.txt
pip install -e .
pytest     # run tests
flake8 src/ tests/
```

### Docker Installation

```bash
docker build -t re-architect .
docker run -p 5000:5000 -v "$PWD:/workspace" re-architect
```

---

## Configuration

RE-Architect uses a YAML config file (`config.yaml`). Minimal example:

```yaml
# Decompiler settings
decompiler:
  default: ghidra           # ghidra | ida | binja | auto
  ghidra:
    path: null              # auto-detect if null
    headless: true
    timeout: 600
  ida:
    path: /opt/ida
    headless: true
    timeout: 600
  binary_ninja:
    path: /opt/binaryninja
    headless: true
    timeout: 600

# Analysis configuration
analysis:
  static:
    function_analysis_depth: medium   # basic | medium | deep
    data_flow_analysis: true
    control_flow_analysis: true
    string_analysis: true
  dynamic:
    enable: false                     # set true to enable
    sandbox_type: container           # container | vm | none
    max_execution_time: 120

# LLM (optional)
llm:
  enable: false
  provider: openai                    # openai | anthropic | azure
  model: gpt-4-turbo
  api_key: your-api-key-here
  max_tokens: 8192
  temperature: 0.2

# Output
output:
  format: json                        # json | html | text
  generate_reports: true
  save_intermediate_results: false
```

### Environment Variables (optional)

* `OPENAI_API_KEY`, `ANTHROPIC_API_KEY` — LLM keys
* `GHIDRA_PATH` — override Ghidra install path

---

## Command Line Usage

Analyze a binary:

```bash
python main.py analyze binary.exe --output ./results --decompiler ghidra
```

Common options:

```
--config PATH       # config file (default: ./config.yaml)
--output PATH       # output directory (default: ./output)
--decompiler TEXT   # ghidra | ida | binja | auto (default: auto)
--generate-tests    # generate per-function test harnesses
--no-llm            # disable LLM features
--format TEXT       # json | html | text
--verbose, -v       # increase logging (repeatable)
--serve             # start web server after analysis
```

Examples:

```bash
# Default analysis (auto decompiler)
python main.py analyze sample.exe --verbose

# Specify output directory
python main.py analyze sample.exe --output ./analysis

# Choose decompiler
python main.py analyze sample.exe --decompiler ghidra

# Generate tests & serve results
python main.py analyze sample.exe --generate-tests --serve
```

---

## Python API

```python
from src.core.pipeline import ReversePipeline
from src.core.config import Config

config = Config.from_file("config.yaml")
pipeline = ReversePipeline(config)

results = pipeline.analyze(
    binary_path="sample.exe",
    output_dir="./analysis_output",
    decompiler="ghidra",
    generate_tests=True
)

functions = results["functions"]
data_structures = results["data_structures"]
print(f"Analyzed {len(functions)} functions, {len(data_structures)} structures")
```

---

## Web Interface

Start server:

```bash
python -m src.visualization.server
# or with host/port
python -m src.visualization.server --host 0.0.0.0 --port 8080
```

Open: `http://localhost:5000`

Features:

* Interactive **Function Browser**
* **Call Graph** visualization
* **Data Structures** explorer
* Source **viewer** with syntax highlighting
* Downloadable **reports**

Serve immediately after analysis:

```bash
python main.py analyze sample.exe --serve
```

---

## Advanced Features

### Multi-Decompiler Comparison

Run multiple decompilers and compare results (conceptual):

```python
decompilers = ["ghidra", "ida", "binja"]
results = {}
for d in decompilers:
    results[d] = pipeline.analyze(binary_path="target.exe", decompiler=d)

# compare_decompiler_results(...) is a hypothetical utility
comparison = compare_decompiler_results(results)
```

### Batch Processing

```bash
for f in binaries/*.exe; do
  echo "Analyzing $f"
  python main.py analyze "$f" --output "./batch/$(basename "$f" .exe)"
done
```

### Custom Analyzers (plugin example)

```python
from src.analysis.base_analyzer import BaseAnalyzer

class CustomSecurityAnalyzer(BaseAnalyzer):
    def analyze(self, decompiled_code):
        vulns = []
        for fn in decompiled_code.functions.values():
            if self.detect_buffer_overflow(fn):
                vulns.append({"type": "buffer_overflow",
                              "function": fn.name, "severity": "high"})
        return {"vulnerabilities": vulns}

pipeline.add_analyzer(CustomSecurityAnalyzer())
```

---

## Decompiler Integration

### Ghidra (recommended)

Install from NSA GitHub and set path (or leave `null` to auto-detect):

```yaml
decompiler:
  ghidra:
    path: /opt/ghidra
    headless: true
    timeout: 600
```

### IDA Pro

```yaml
decompiler:
  ida:
    path: /opt/ida
    headless: true
    timeout: 600
    batch_mode: true
    use_decompiler: true
```

### Binary Ninja

```yaml
decompiler:
  binary_ninja:
    path: /opt/binaryninja
    headless: true
    timeout: 600
```

---

## LLM Integration

Enable and configure:

```yaml
llm:
  enable: true
  provider: openai           # openai | anthropic | azure
  model: gpt-4-turbo
  api_key: ${OPENAI_API_KEY}
  max_tokens: 8192
  temperature: 0.2
```

**Tips**

* Keep token limits reasonable for speed
* Use `--no-llm` for large batch jobs
* Store keys in env vars, not in code

---

## Understanding Results

### Output Structure

```
output/
├── metadata.json          # Analysis summary & stats
├── functions/
│   ├── functions.json     # All functions overview
│   └── *.json             # Per-function details
├── data_structures/
│   ├── structures.json    # All structures overview
│   └── *.json             # Per-structure details
├── test_harnesses/        # Generated tests (if enabled)
│   ├── tests.json
│   └── func_<addr>.c
└── reports/
    ├── summary.html       # Web-viewable report
    └── analysis.md        # Markdown report
```

---

## Troubleshooting

**Decompiler not found**

* Set correct path in `config.yaml`
* Ensure tool is installed & executable
* For Ghidra, try `path: null` for auto-detect

**No module named `src`**

```bash
pip install -e .
```

**LLM API errors**

* Verify `OPENAI_API_KEY` / provider settings
* Check network and API rate limits

**Out of memory / slow**

* Use `function_analysis_depth: basic`
* Disable LLM: `--no-llm`
* Analyze smaller sections or fewer files
* Increase system memory / use SSD

**Permission errors**

* Choose a writable `--output` directory
* Check disk space

Enable debug logs:

```bash
python main.py analyze binary.exe --verbose --verbose
```

---

## Best Practices

**Security**

* Analyze untrusted binaries in VMs/containers
* Disable network access for targets
* Keep API keys out of source control

**Workflow**

1. Start with **basic** static analysis
2. Refine depth/flags iteratively
3. Validate with multiple decompilers
4. Document findings (reports)
5. Version configs and outputs

---

## Performance Optimization

**Config profile (faster runs)**

```yaml
analysis:
  static:
    function_analysis_depth: basic
    data_flow_analysis: false
llm:
  enable: false
decompiler:
  ghidra:
    timeout: 300
```

**Tips**

* Monitor CPU/RAM; prefer SSDs
* Batch by size/complexity
* Cache intermediate results if enabled

---

## Test Generation

Enable via CLI:

```bash
python main.py analyze sample.exe --generate-tests
```

Python API:

```python
results = pipeline.analyze("sample.exe", generate_tests=True)
for name, code in results.get("test_harnesses", {}).items():
    print(f"Test for {name}:\n{code}")
```

Test types: unit, integration, fuzzing templates, performance.

---

## Integration (CI/CD & IDE)

### GitHub Actions (example)

```yaml
name: Binary Analysis
on: [push]

jobs:
  analyze:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install
        run: |
          pip install -r requirements.txt
          pip install -e .
      - name: Analyze
        run: python main.py analyze test_binary.exe --output reports/
      - name: Upload Reports
        uses: actions/upload-artifact@v3
        with:
          name: analysis-reports
          path: reports/
```

### VS Code Task

```json
{
  "version": "2.0.0",
  "tasks": [
    {
      "label": "Analyze Binary",
      "type": "shell",
      "command": "python",
      "args": ["main.py", "analyze", "${input:binaryPath}", "--output", "./analysis"],
      "group": "build",
      "presentation": { "echo": true, "reveal": "always" }
    }
  ],
  "inputs": [
    { "id": "binaryPath", "description": "Path to binary file", "type": "promptString" }
  ]
}
```

---

## Support & Contributing

**Getting Help**

* `docs/` (User Manual, API Reference, Quick Start)
* Examples in `tests/`
* GitHub Issues for bugs & features

**Contributing**

1. Fork the repo
2. Create a feature branch
3. Add tests
4. Open a PR with a clear description

---

**Happy reverse engineering!**

---

```

`main.py`:

```py
"""
Main entry point for RE-Architect.

This script provides the command-line interface for RE-Architect, handling
argument parsing, binary loading, and orchestrating the reverse engineering pipeline.
"""

import argparse
import logging
import sys
import time
from pathlib import Path

from src.core.pipeline import ReversePipeline
from src.core.config import Config

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("re-architect")

def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="RE-Architect: Automated reverse engineering pipeline"
    )
    
    parser.add_argument(
        "binary_path", 
        type=str, 
        help="Path to the binary file to analyze"
    )
    
    parser.add_argument(
        "--output-dir", 
        type=str, 
        default="./output",
        help="Directory to store output files (default: ./output)"
    )
    
    parser.add_argument(
        "--config", 
        type=str, 
        default="./config.yaml",
        help="Path to configuration file (default: ./config.yaml)"
    )
    
    parser.add_argument(
        "--decompiler", 
        type=str, 
        choices=["ghidra", "ida", "binja", "mock", "auto"],
        default=None,
        help="Decompiler to use (default: from config file or auto)"
    )
    
    parser.add_argument(
        "--verbose", 
        "-v", 
        action="count", 
        default=0,
        help="Increase verbosity (can be used multiple times)"
    )
    
    parser.add_argument(
        "--no-llm", 
        action="store_true", 
        help="Disable LLM-based analysis"
    )
    
    parser.add_argument(
        "--generate-tests", 
        action="store_true", 
        help="Generate test harnesses for identified functions"
    )
    
    parser.add_argument(
        "--serve", 
        action="store_true", 
        help="Start the web visualization server after analysis"
    )
    
    return parser.parse_args()

def main():
    """Main execution function."""
    start_time = time.time()
    args = parse_args()
    
    # Set verbosity level
    if args.verbose == 0:
        logger.setLevel(logging.INFO)
    elif args.verbose == 1:
        logger.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.DEBUG)
        # Enable detailed debug logs
        logging.getLogger().setLevel(logging.DEBUG)
    
    logger.info("RE-Architect starting...")
    logger.debug(f"Arguments: {args}")
    
    # Check if binary exists
    binary_path = Path(args.binary_path)
    if not binary_path.exists() or not binary_path.is_file():
        logger.error(f"Binary file not found: {binary_path}")
        return 1
    
    # Create output directory if it doesn't exist
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Load configuration
    config = Config.from_file(args.config)
    if args.no_llm:
        config.disable_llm()
    
    # Determine which decompiler to use
    decompiler_name = args.decompiler
    if decompiler_name is None:
        # Use decompiler from config file
        decompiler_name = config.get("decompiler.default", "auto")
    
    logger.info(f"Using decompiler: {decompiler_name}")
    
    try:
        # Initialize and run the pipeline
        pipeline = ReversePipeline(config)
        
        results = pipeline.analyze(
            binary_path=binary_path,
            output_dir=output_dir,
            decompiler=decompiler_name,
            generate_tests=args.generate_tests
        )
        
        # Output processing time
        elapsed_time = time.time() - start_time
        logger.info(f"Analysis completed in {elapsed_time:.2f} seconds")
        
        # Start visualization server if requested
        if args.serve:
            from src.visualization.server import VisualizationServer
            server = VisualizationServer(host="localhost", port=5000)
            server.load_results(results)
            server.start()
        
        return 0
        
    except Exception as e:
        logger.exception(f"Error during analysis: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())

```

`pytest.ini`:

```ini
[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts = --verbose --cov=src --cov-report=term-missing
markers =
    unit: Unit tests
    integration: Integration tests
    slow: Tests that take a long time to run
    api: Tests that require API access

```

`requirements-dev.txt`:

```txt
# Development dependencies
pytest>=7.4.0
pytest-cov>=4.1.0
flake8>=6.1.0
black>=23.7.0
mypy>=1.5.0
isort>=5.12.0
pre-commit>=3.3.3
sphinx>=7.1.0
sphinx-rtd-theme>=1.3.0
build>=1.0.0
twine>=4.0.2
coverage>=7.3.0
pytest-mock>=3.11.0
tox>=4.11.0

# API documentation
PyYAML>=6.0
pyyaml-include>=1.3
redoc-cli>=1.0.0

# Security scanning
bandit>=1.7.5

```

`requirements.txt`:

```txt
# Core dependencies
PyYAML>=6.0
requests>=2.28.0
tqdm>=4.64.0

# Binary analysis
capstone>=5.0.0
lief>=0.12.0
r2pipe>=1.6.5
frida>=16.0.0
angr>=9.2.0

# Data analysis and machine learning
numpy>=1.22.0
pandas>=1.4.0
scikit-learn>=1.1.0

# Web visualization
flask>=2.3.0

# LLM integration
openai>=0.27.0
anthropic>=0.3.0
torch>=2.0.0
transformers>=4.21.0

# Testing
pytest>=7.1.0
coverage>=6.3.0

```

`setup.py`:

```py
"""
Setup script for RE-Architect.

This script allows installing RE-Architect as a Python package.
"""

from setuptools import setup, find_packages

with open("README.md", "r", encoding="utf-8") as fh:
    long_description = fh.read()

with open("requirements.txt", "r", encoding="utf-8") as fh:
    requirements = fh.read().splitlines()

setup(
    name="re-architect",
    version="1.0.0",
    author="RE-Architect Team",
    author_email="contact@re-architect.example.com",
    description="Advanced reverse engineering platform with AI-powered analysis",
    long_description=long_description,
    long_description_content_type="text/markdown",
    url="https://github.com/pandaadir05/re-architect",
    packages=find_packages(),
    classifiers=[
        "Development Status :: 5 - Production/Stable",
        "Intended Audience :: Developers",
        "Intended Audience :: Information Technology", 
        "Intended Audience :: Science/Research",
        "Topic :: Security",
        "Topic :: Software Development :: Disassemblers",
        "Topic :: Scientific/Engineering :: Information Analysis",
        "Programming Language :: Python :: 3",
        "Programming Language :: Python :: 3.8",
        "Programming Language :: Python :: 3.9", 
        "Programming Language :: Python :: 3.10",
        "Programming Language :: Python :: 3.11",
        "Programming Language :: Python :: 3.12",
        "License :: OSI Approved :: MIT License",
        "Operating System :: OS Independent",
    ],
    python_requires=">=3.8",
    install_requires=requirements,
    entry_points={
        "console_scripts": [
            "re-architect=main:main",
        ],
    },
)

```

`src/__init__.py`:

```py
"""
RE-Architect: Advanced Reverse Engineering Platform

A comprehensive reverse engineering platform that combines traditional static analysis 
with modern AI-powered insights for binary analysis and security research.
"""

__version__ = "1.0.0"
__description__ = "Advanced reverse engineering platform with AI-powered analysis"

# Core exports
from src.core.pipeline import ReversePipeline
from src.core.config import Config
from src.core.binary_loader import BinaryLoader

__all__ = [
    "ReversePipeline",
    "Config", 
    "BinaryLoader",
    "__version__",
]

```

`src/analysis/__init__.py`:

```py
"""
Analysis package for RE-Architect.
"""

from .static_analyzer import StaticAnalyzer, StaticAnalysisResults, FunctionInfo
from .enhanced_static_analyzer import EnhancedStaticAnalyzer
from .dynamic_analyzer import DynamicAnalyzer
from .data_structure_analyzer import DataStructureAnalyzer
from .unified_static_analyzer import UnifiedStaticAnalyzer

__all__ = [
    'StaticAnalyzer',
    'StaticAnalysisResults', 
    'FunctionInfo',
    'EnhancedStaticAnalyzer',
    'DynamicAnalyzer',
    'DataStructureAnalyzer',
    'UnifiedStaticAnalyzer'
]

```

`src/analysis/data_structure_analyzer.py`:

```py
"""
Data structure analyzer module for RE-Architect.

This module analyzes decompiled code to identify and reconstruct data structures.
"""

import logging
import re
from dataclasses import dataclass
from typing import Dict, List, Optional, Set, Tuple, Any

from src.core.config import Config
from src.decompilers.base_decompiler import DecompiledCode
from src.analysis.static_analyzer import StaticAnalysisResults

logger = logging.getLogger("re-architect.analysis.data_structures")

class DataStructureAnalyzer:
    """
    Data structure analyzer for RE-Architect.
    
    This class analyzes decompiled code to identify and reconstruct data structures
    used in the binary.
    """
    
    def __init__(self, config: Config):
        """
        Initialize the data structure analyzer.
        
        Args:
            config: Configuration object
        """
        self.config = config
    
    def analyze(
        self,
        decompiled_code: DecompiledCode,
        static_analysis: StaticAnalysisResults
    ) -> Dict[str, Dict[str, Any]]:
        """
        Analyze decompiled code to extract data structures.
        
        Args:
            decompiled_code: Decompiled code to analyze
            static_analysis: Results from static analysis
            
        Returns:
            Dictionary mapping structure names to structure information
        """
        logger.info("Starting data structure analysis")
        
        # Start with structures already identified by the decompiler
        structures = self._extract_defined_structures(decompiled_code)
        
        # Analyze function parameters and variables to infer additional structures
        inferred_structures = self._infer_structures(decompiled_code, static_analysis)
        
        # Merge the results (preserving already defined structures)
        for name, struct_info in inferred_structures.items():
            if name not in structures:
                structures[name] = struct_info
        
        logger.info(f"Identified {len(structures)} data structures")
        return structures
    
    def _extract_defined_structures(self, decompiled_code: DecompiledCode) -> Dict[str, Dict[str, Any]]:
        """
        Extract structures already defined in the decompiled code.
        
        Args:
            decompiled_code: Decompiled code to analyze
            
        Returns:
            Dictionary mapping structure names to structure information
        """
        structures = {}
        
        # Process types defined by the decompiler
        for name, definition in decompiled_code.types.items():
            # Only process structures
            if self._is_structure_definition(definition):
                # Parse the structure definition
                structure_info = self._parse_structure_definition(name, definition)
                if structure_info:
                    structures[name] = structure_info
        
        return structures
    
    def _is_structure_definition(self, definition: str) -> bool:
        """
        Check if a definition represents a structure.
        
        Args:
            definition: Type definition to check
            
        Returns:
            True if the definition is a structure
        """
        return definition.strip().startswith("struct ")
    
    def _parse_structure_definition(self, name: str, definition: str) -> Dict[str, Any]:
        """
        Parse a structure definition into a structured format.
        
        Args:
            name: Structure name
            definition: Structure definition
            
        Returns:
            Dictionary containing structure information
        """
        # Remove comments
        definition = re.sub(r"//.*$", "", definition, flags=re.MULTILINE)
        
        # Extract fields
        field_pattern = r"(\w+)\s+(\w+)(?:\[(\d+)\])?;"
        fields = []
        
        for line in definition.splitlines():
            line = line.strip()
            match = re.search(field_pattern, line)
            if match:
                field_type = match.group(1)
                field_name = match.group(2)
                field_array_size = match.group(3)
                
                field_info = {
                    "name": field_name,
                    "type": field_type,
                    "is_array": field_array_size is not None
                }
                
                if field_array_size:
                    field_info["array_size"] = int(field_array_size)
                
                fields.append(field_info)
        
        # Calculate size (this is approximate)
        type_sizes = {
            "char": 1,
            "byte": 1,
            "short": 2,
            "int": 4,
            "long": 4,
            "float": 4,
            "double": 8,
            "pointer": 4,  # Assume 32-bit pointers by default
            "void": 0
        }
        
        size = 0
        for field in fields:
            field_type = field["type"]
            base_size = type_sizes.get(field_type, 4)  # Default to 4 bytes if unknown
            
            if field.get("is_array", False):
                array_size = field.get("array_size", 1)
                field_size = base_size * array_size
            else:
                field_size = base_size
            
            size += field_size
        
        return {
            "name": name,
            "original_definition": definition,
            "fields": fields,
            "size": size,
            "source": "decompiler"
        }
    
    def _infer_structures(
        self,
        decompiled_code: DecompiledCode,
        static_analysis: StaticAnalysisResults
    ) -> Dict[str, Dict[str, Any]]:
        """
        Infer structures from function parameters and usage patterns.
        
        Args:
            decompiled_code: Decompiled code to analyze
            static_analysis: Results from static analysis
            
        Returns:
            Dictionary mapping inferred structure names to structure information
        """
        inferred_structures = {}
        
        # Look for structure usage patterns in functions
        for func_addr, func_info in static_analysis.functions.items():
            # Skip library functions
            if func_info.get("is_library", False):
                continue
            
            func_code = func_info.get("code", "")
            
            # Look for struct dereference patterns
            self._find_struct_dereferences(func_code, inferred_structures)
            
            # Analyze parameters that might be structures
            parameters = func_info.get("parameters", [])
            self._analyze_struct_parameters(parameters, func_code, inferred_structures)
        
        return inferred_structures
    
    def _find_struct_dereferences(
        self,
        code: str,
        inferred_structures: Dict[str, Dict[str, Any]]
    ) -> None:
        """
        Find structure dereference patterns in code.
        
        Args:
            code: Function code to analyze
            inferred_structures: Dictionary to update with inferred structures
        """
        # Look for patterns like "x->field" or "x.field"
        arrow_pattern = r"(\w+)->(\w+)"
        dot_pattern = r"(\w+)\.(\w+)"
        
        # Find all arrow dereferences
        for match in re.finditer(arrow_pattern, code):
            struct_var = match.group(1)
            field_name = match.group(2)
            
            # Generate a structure name if this looks like a structure
            struct_name = f"struct_{struct_var}"
            
            # Add to inferred structures if new
            if struct_name not in inferred_structures:
                inferred_structures[struct_name] = {
                    "name": struct_name,
                    "fields": [],
                    "size": 0,
                    "source": "inferred"
                }
            
            # Add the field if it's new
            struct_info = inferred_structures[struct_name]
            if not any(field["name"] == field_name for field in struct_info["fields"]):
                struct_info["fields"].append({
                    "name": field_name,
                    "type": "unknown",
                    "is_array": False
                })
        
        # Find all dot dereferences
        for match in re.finditer(dot_pattern, code):
            struct_var = match.group(1)
            field_name = match.group(2)
            
            # Generate a structure name if this looks like a structure
            struct_name = f"struct_{struct_var}"
            
            # Add to inferred structures if new
            if struct_name not in inferred_structures:
                inferred_structures[struct_name] = {
                    "name": struct_name,
                    "fields": [],
                    "size": 0,
                    "source": "inferred"
                }
            
            # Add the field if it's new
            struct_info = inferred_structures[struct_name]
            if not any(field["name"] == field_name for field in struct_info["fields"]):
                struct_info["fields"].append({
                    "name": field_name,
                    "type": "unknown",
                    "is_array": False
                })
    
    def _analyze_struct_parameters(
        self,
        parameters: List[Dict[str, Any]],
        code: str,
        inferred_structures: Dict[str, Dict[str, Any]]
    ) -> None:
        """
        Analyze function parameters that might be structures.
        
        Args:
            parameters: List of parameter information
            code: Function code to analyze
            inferred_structures: Dictionary to update with inferred structures
        """
        for param in parameters:
            param_name = param.get("name", "")
            param_type = param.get("dataType", "")
            
            # Check if this parameter might be a structure pointer
            if "struct" in param_type or "*" in param_type:
                # Look for dereference patterns with this parameter
                arrow_pattern = f"{param_name}->([\\w_]+)"
                
                for match in re.finditer(arrow_pattern, code):
                    field_name = match.group(1)
                    
                    # Generate structure name from the parameter type if possible
                    if "struct" in param_type:
                        # Extract name from something like "struct_name *"
                        type_match = re.search(r"struct\s+(\w+)", param_type)
                        if type_match:
                            struct_name = type_match.group(1)
                        else:
                            struct_name = f"struct_{param_name}"
                    else:
                        struct_name = f"struct_{param_name}"
                    
                    # Add to inferred structures if new
                    if struct_name not in inferred_structures:
                        inferred_structures[struct_name] = {
                            "name": struct_name,
                            "fields": [],
                            "size": 0,
                            "source": "inferred"
                        }
                    
                    # Add the field if it's new
                    struct_info = inferred_structures[struct_name]
                    if not any(field["name"] == field_name for field in struct_info["fields"]):
                        struct_info["fields"].append({
                            "name": field_name,
                            "type": "unknown",
                            "is_array": False
                        })
                        
    def _infer_field_types(
        self, 
        decompiled_code: DecompiledCode,
        static_analysis: StaticAnalysisResults,
        structures: Dict[str, Dict[str, Any]]
    ) -> None:
        """
        Infer field types based on usage patterns.
        
        Args:
            decompiled_code: Decompiled code to analyze
            static_analysis: Results from static analysis
            structures: Dictionary mapping structure names to structure information
        """
        for struct_name, struct_info in structures.items():
            if struct_info["source"] == "decompiler":
                # Skip structures with known types
                continue
                
            for field_idx, field in enumerate(struct_info["fields"]):
                if field["type"] != "unknown":
                    continue
                    
                # Try to infer type from usage
                inferred_type = self._infer_field_type(
                    struct_name, 
                    field["name"], 
                    decompiled_code, 
                    static_analysis
                )
                
                if inferred_type:
                    structures[struct_name]["fields"][field_idx]["type"] = inferred_type
    
    def _infer_field_type(
        self,
        struct_name: str,
        field_name: str,
        decompiled_code: DecompiledCode,
        static_analysis: StaticAnalysisResults
    ) -> str:
        """
        Infer the type of a structure field based on usage patterns.
        
        Args:
            struct_name: Name of the structure
            field_name: Name of the field
            decompiled_code: Decompiled code to analyze
            static_analysis: Results from static analysis
            
        Returns:
            Inferred type, or "unknown" if the type could not be inferred
        """
        # For simplicity, we'll use some common naming conventions
        
        # Number-related field names often indicate numeric types
        if re.search(r"(count|size|length|index|num|id|age|year|month|day)", field_name, re.I):
            return "int"
            
        # Fields containing "name" are often strings
        if "name" in field_name.lower():
            return "char*"
            
        # Fields with "ptr" or "pointer" are often pointers
        if re.search(r"(ptr|pointer)", field_name, re.I):
            return "void*"
            
        # Fields with "flag" or "bool" are often booleans
        if re.search(r"(flag|bool|enabled|active)", field_name, re.I):
            return "bool"
            
        # Fields with "data" might be arrays or pointers
        if "data" in field_name.lower():
            return "void*"
            
        return "unknown"

```

`src/analysis/dynamic_analyzer.py`:

```py
"""
Dynamic analyzer module for RE-Architect.

This module handles dynamic analysis of binary files.
"""

import logging
import time
from pathlib import Path
from typing import Dict, List, Optional, Any

from src.core.config import Config
from src.core.binary_loader import BinaryInfo

logger = logging.getLogger("re-architect.analysis.dynamic")

class DynamicAnalyzer:
    """
    Dynamic analyzer for RE-Architect.
    
    This class handles dynamic analysis of binary files using
    sandboxed execution and tracing.
    """
    
    def __init__(self, config: Config):
        """
        Initialize the dynamic analyzer.
        
        Args:
            config: Configuration object
        """
        self.config = config
        self.enabled = config.get("analysis.dynamic.enable", False)
        self.max_execution_time = config.get("analysis.dynamic.max_execution_time", 60)
        self.memory_limit = config.get("analysis.dynamic.memory_limit", 2048)
        self.sandbox_type = config.get("analysis.dynamic.sandbox_type", "container")
    
    def analyze(self, binary_info: BinaryInfo) -> Dict[str, Any]:
        """
        Perform dynamic analysis on a binary file.
        
        Args:
            binary_info: Information about the binary to analyze
            
        Returns:
            Dictionary containing dynamic analysis results
            
        Raises:
            RuntimeError: If dynamic analysis fails or is not enabled
        """
        if not self.enabled:
            logger.info("Dynamic analysis is disabled")
            return {"enabled": False}
        
        logger.info(f"Starting dynamic analysis of {binary_info.path}")
        
        # Initialize results
        results = {
            "enabled": True,
            "functions": {},
            "memory_access": {},
            "syscalls": [],
            "execution_paths": {}
        }
        
        # Choose and initialize the appropriate execution environment
        environment = self._create_execution_environment()
        
        try:
            # Set up the binary for analysis
            environment.setup(binary_info)
            
            # Perform function tracing
            function_results = self._trace_functions(environment, binary_info)
            results["functions"] = function_results
            
            # Collect memory access patterns
            memory_results = self._analyze_memory_access(environment)
            results["memory_access"] = memory_results
            
            # Collect system call information
            syscall_results = self._collect_syscalls(environment, binary_info)
            results["syscalls"] = syscall_results
            
            # Analyze execution paths
            path_results = self._analyze_execution_paths(environment, binary_info)
            results["execution_paths"] = path_results
            
            logger.info("Dynamic analysis completed successfully")
            
        except Exception as e:
            logger.error(f"Error during dynamic analysis: {e}")
            results["error"] = str(e)
            
        finally:
            # Clean up
            environment.cleanup()
        
        return results
    
    def _create_execution_environment(self):
        """
        Create an appropriate execution environment based on configuration.
        
        Returns:
            Execution environment instance
        """
        if self.sandbox_type == "container":
            # Use containerized execution (e.g., Docker)
            from src.analysis.execution.container_environment import ContainerEnvironment
            return ContainerEnvironment(
                max_execution_time=self.max_execution_time,
                memory_limit=self.memory_limit
            )
        elif self.sandbox_type == "vm":
            # Use virtual machine execution
            from src.analysis.execution.vm_environment import VMEnvironment
            return VMEnvironment(
                max_execution_time=self.max_execution_time,
                memory_limit=self.memory_limit
            )
        else:
            # Use local execution (less secure)
            from src.analysis.execution.local_environment import LocalEnvironment
            return LocalEnvironment(
                max_execution_time=self.max_execution_time
            )
    
    def _trace_functions(self, environment, binary_info):
        """
        Trace function execution.
        
        Args:
            environment: Execution environment
            binary_info: Information about the binary
            
        Returns:
            Dictionary containing function tracing results
        """
        # Prefer Frida-based tracing if available/configured.
        use_frida = self.config.get("analysis.dynamic.use_frida", True)
        if use_frida:
            try:
                import frida  # type: ignore
                return self._trace_functions_with_frida(binary_info)
            except Exception as e:
                logger.warning(f"Frida tracing unavailable or failed, falling back: {e}")
        
        # Fallback: try environment-provided tracing hooks
        traces: Any = None
        try:
            if hasattr(environment, "trace_functions"):
                traces = environment.trace_functions(binary_info)
            elif hasattr(environment, "run_with_tracing"):
                traces = environment.run_with_tracing(binary_info)
            elif hasattr(environment, "get_function_trace"):
                traces = environment.get_function_trace()
        except Exception as e:
            logger.warning(f"Function tracing failed: {e}")
            traces = None
        
        if traces is None:
            logger.info("Function tracing not available from the execution environment")
            return {"calls": [], "call_counts": {}, "by_function": {}}
        
        return self._normalize_function_traces(traces)

    def _normalize_function_traces(self, traces: Any) -> Dict[str, Any]:
        call_records: List[Dict[str, Any]] = []
        if isinstance(traces, dict) and "calls" in traces and isinstance(traces["calls"], list):
            call_records = traces["calls"]
        elif isinstance(traces, list):
            call_records = traces
        elif isinstance(traces, dict):
            for func, count in traces.items():
                call_records.append({"name": str(func), "count": int(count) if isinstance(count, int) else 1})
        else:
            logger.info("Unrecognized function trace format; returning empty results")
            return {"calls": [], "call_counts": {}, "by_function": {}}
        
        call_counts: Dict[str, int] = {}
        by_function: Dict[str, Dict[str, Any]] = {}
        normalized_calls: List[Dict[str, Any]] = []
        
        for rec in call_records:
            if not isinstance(rec, dict):
                continue
            name = str(rec.get("name") or rec.get("to_name") or rec.get("symbol") or rec.get("to") or "unknown")
            address = rec.get("address") or rec.get("toAddress") or rec.get("to_address")
            if isinstance(address, int):
                address_str = f"0x{address:x}"
            elif isinstance(address, str):
                address_str = address
            else:
                address_str = None
            count = rec.get("count")
            if not isinstance(count, int):
                count = 1
            call_counts[name] = call_counts.get(name, 0) + count
            if name not in by_function:
                by_function[name] = {"calls": 0, "addresses": set()}
            by_function[name]["calls"] += count
            if address_str:
                by_function[name]["addresses"].add(address_str)
            normalized_calls.append({"name": name, "address": address_str, "count": count})
        
        for name, agg in by_function.items():
            addresses = sorted(list(agg.get("addresses", [])))
            agg["addresses"] = addresses
        
        return {"calls": normalized_calls, "call_counts": call_counts, "by_function": by_function}

    def _trace_functions_with_frida(self, binary_info: Any) -> Dict[str, Any]:
        import frida  # type: ignore
        target_path = str(binary_info.path if isinstance(binary_info.path, (str, Path)) else binary_info.path)
        session = None
        pid = None
        summaries: List[Dict[str, Any]] = []
        errors: List[str] = []
        
        # Frida script using Stalker to collect call summaries periodically
        script_source = """
        var collected = {};
        function symbolFromAddress(addr) {
          try {
            var m = Process.findModuleByAddress(ptr(addr));
            if (m) {
              var exp = DebugSymbol.fromAddress(ptr(addr));
              if (exp && exp.name) return exp.name;
              return m.name + "!" + ptr(addr);
            }
          } catch (_) {}
          return ptr(addr).toString();
        }
        Stalker.follow(Process.getCurrentThreadId(), {
          events: { call: true },
          onCallSummary: function (summary) {
            var out = [];
            for (var target in summary) {
              var count = summary[target];
              out.push({ name: symbolFromAddress(target), address: target, count: count });
            }
            send({ type: 'call_summary', calls: out });
          }
        });
        """
        
        try:
            pid = frida.spawn([target_path])
            session = frida.attach(pid)
            script = session.create_script(script_source)
            
            def on_message(message, data):
                try:
                    if message.get("type") == "send":
                        payload = message.get("payload", {})
                        if payload.get("type") == "call_summary":
                            summaries.append(payload)
                    elif message.get("type") == "error":
                        errors.append(str(message))
                except Exception as _e:
                    errors.append(f"callback error: {_e}")
            
            script.on("message", on_message)
            script.load()
            frida.resume(pid)
            
            # Run for a bounded duration
            duration = int(self.max_execution_time)
            time.sleep(min(max(duration, 1), 10))  # keep short by default for safety
        except Exception as e:
            logger.warning(f"Frida execution error: {e}")
        finally:
            try:
                if session is not None:
                    session.detach()
            except Exception:
                pass
            try:
                if pid is not None:
                    import frida  # re-import for scope
                    frida.kill(pid)
            except Exception:
                pass
        
        # Merge summaries
        merged_calls: Dict[str, Dict[str, Any]] = {}
        for s in summaries:
            for rec in s.get("calls", []):
                name = str(rec.get("name") or "unknown")
                address = rec.get("address")
                count = int(rec.get("count") or 1)
                if name not in merged_calls:
                    merged_calls[name] = {"name": name, "address": address, "count": 0}
                merged_calls[name]["count"] += count
                # Prefer first non-empty address
                if not merged_calls[name].get("address") and address:
                    merged_calls[name]["address"] = address
        
        traces = {"calls": list(merged_calls.values())}
        if errors:
            traces["errors"] = errors
        return self._normalize_function_traces(traces)
    
    def _analyze_memory_access(self, environment):
        """
        Analyze memory access patterns.
        
        Args:
            environment: Execution environment
            
        Returns:
            Dictionary containing memory access analysis results
        """
        # Prefer Frida-based allocation/free tracing if available.
        use_frida = self.config.get("analysis.dynamic.use_frida", True)
        if use_frida:
            try:
                import frida  # type: ignore
                return self._analyze_memory_access_with_frida()
            except Exception as e:
                logger.warning(f"Frida memory analysis unavailable or failed, falling back: {e}")
        
        # Fallback to environment-provided memory logs
        events: Any = None
        try:
            if hasattr(environment, "get_memory_events"):
                events = environment.get_memory_events()
            elif hasattr(environment, "collect_memory_profile"):
                events = environment.collect_memory_profile()
            elif hasattr(environment, "memory_log"):
                events = environment.memory_log
        except Exception as e:
            logger.warning(f"Memory event collection failed: {e}")
            events = None
        
        if events is None:
            logger.info("Memory access data not available from the execution environment")
            return {"summary": {"reads": 0, "writes": 0, "allocations": 0, "frees": 0}, "by_address": {}, "events": []}
        
        return self._normalize_memory_events(events)

    def _normalize_memory_events(self, events: Any) -> Dict[str, Any]:
        if isinstance(events, dict) and isinstance(events.get("events"), list):
            event_list = events["events"]
        elif isinstance(events, list):
            event_list = events
        else:
            logger.info("Unrecognized memory event format; returning empty results")
            return {"summary": {"reads": 0, "writes": 0, "allocations": 0, "frees": 0}, "by_address": {}, "events": []}
        
        reads = 0
        writes = 0
        allocations = 0
        frees = 0
        by_address: Dict[str, Dict[str, int]] = {}
        normalized_events: List[Dict[str, Any]] = []
        
        for ev in event_list:
            if not isinstance(ev, dict):
                continue
            etype = str(ev.get("type") or ev.get("event") or "").lower()
            address = ev.get("address")
            if isinstance(address, int):
                address_str = f"0x{address:x}"
            elif isinstance(address, str):
                address_str = address
            else:
                address_str = "unknown"
            size = ev.get("size")
            try:
                size_int = int(size) if size is not None else 0
            except Exception:
                size_int = 0
            if etype == "read":
                reads += 1
                agg = by_address.setdefault(address_str, {"reads": 0, "writes": 0})
                agg["reads"] += 1
            elif etype == "write":
                writes += 1
                agg = by_address.setdefault(address_str, {"reads": 0, "writes": 0})
                agg["writes"] += 1
            elif etype in ("alloc", "allocate", "malloc", "new"):
                allocations += 1
            elif etype in ("free", "dealloc", "delete"):
                frees += 1
            normalized_events.append({"type": etype or "unknown", "address": address_str, "size": size_int})
        
        return {"summary": {"reads": reads, "writes": writes, "allocations": allocations, "frees": frees}, "by_address": by_address, "events": normalized_events}

    def _analyze_memory_access_with_frida(self) -> Dict[str, Any]:
        import frida  # type: ignore
        # We cannot reliably record reads/writes generically across ISAs without heavy
        # instrumentation. As a practical compromise, hook allocations and frees on
        # common libc/UCRT symbols and provide counts and sizes.
        # The actual process to inspect is already launched during function tracing;
        # for simplicity and isolation, spawn a fresh instance here as well.
        # Note: Users can increase max_execution_time for longer profiling.
        return {
            "summary": {"reads": 0, "writes": 0, "allocations": 0, "frees": 0},
            "by_address": {},
            "events": []
        }
    
    def _collect_syscalls(self, environment, binary_info: BinaryInfo):
        """
        Collect system call information.
        
        Args:
            environment: Execution environment
            
        Returns:
            List of system call records
        """
        use_frida = self.config.get("analysis.dynamic.use_frida", True)
        if use_frida:
            try:
                import frida  # type: ignore
                return self._collect_syscalls_with_frida(binary_info)
            except Exception as e:
                logger.warning(f"Frida syscall collection unavailable or failed, falling back: {e}")
        
        # Fallback to environment-provided syscall logs if any
        try:
            if hasattr(environment, "get_syscalls"):
                syscalls = environment.get_syscalls()
            elif hasattr(environment, "syscall_log"):
                syscalls = environment.syscall_log
            else:
                syscalls = []
        except Exception as e:
            logger.warning(f"Syscall collection failed: {e}")
            syscalls = []
        
        # Normalize to a list of dicts
        normalized: List[Dict[str, Any]] = []
        if isinstance(syscalls, list):
            for s in syscalls:
                if isinstance(s, dict):
                    name = str(s.get("name") or s.get("syscall") or "unknown")
                    args = s.get("args") if isinstance(s.get("args"), list) else []
                    ret = s.get("ret")
                    normalized.append({"name": name, "args": args, "ret": ret})
                elif isinstance(s, str):
                    normalized.append({"name": s, "args": [], "ret": None})
        return normalized

    def _collect_syscalls_with_frida(self, binary_info: BinaryInfo) -> List[Dict[str, Any]]:
        import frida  # type: ignore
        # Hook common libc/system APIs as an approximation of syscalls across platforms
        targets = [
            "open", "openat", "close", "read", "write", "pread", "pwrite",
            "socket", "connect", "accept", "send", "recv", "sendto", "recvfrom",
            "execve", "fork", "vfork", "clone", "mmap", "munmap",
            "CreateFileW", "CreateFileA", "ReadFile", "WriteFile", "CloseHandle",
            "CreateProcessW", "CreateProcessA"
        ]
        script_source = """
        function tryHook(name) {
          var addr = Module.findExportByName(null, name);
          if (!addr) return false;
          try {
            Interceptor.attach(addr, {
              onEnter: function (args) {
                var msg = { type: 'api_call', name: name, args: [] };
                send(msg);
              },
              onLeave: function (retval) {
                send({ type: 'api_ret', name: name, ret: retval.toString() });
              }
            });
            return true;
          } catch (_) {
            return false;
          }
        }
        var names = %NAMES%;
        for (var i = 0; i < names.length; i++) tryHook(names[i]);
        """.replace('%NAMES%', str(targets))
        
        syscalls: List[Dict[str, Any]] = []
        errors: List[str] = []
        pid = None
        session = None
        target_path = str(binary_info.path if isinstance(binary_info.path, (str, Path)) else binary_info.path)
        try:
            pid = frida.spawn([target_path])
            session = frida.attach(pid)
            script = session.create_script(script_source)
            
            def on_message(message, data):
                try:
                    if message.get("type") == "send":
                        payload = message.get("payload", {})
                        if payload.get("type") == "api_call":
                            syscalls.append({"name": payload.get("name"), "args": payload.get("args", []), "ret": None})
                        elif payload.get("type") == "api_ret":
                            # Attach return value to last matching call if any
                            name = payload.get("name")
                            ret = payload.get("ret")
                            for i in range(len(syscalls) - 1, -1, -1):
                                if syscalls[i]["name"] == name and syscalls[i]["ret"] is None:
                                    syscalls[i]["ret"] = ret
                                    break
                    elif message.get("type") == "error":
                        errors.append(str(message))
                except Exception as _e:
                    errors.append(f"callback error: {_e}")
            
            script.on("message", on_message)
            script.load()
            frida.resume(pid)
            
            duration = int(self.max_execution_time)
            time.sleep(min(max(duration, 1), 5))
        except Exception as e:
            logger.warning(f"Frida syscall hook error: {e}")
        finally:
            try:
                if session is not None:
                    session.detach()
            except Exception:
                pass
            try:
                if pid is not None:
                    frida.kill(pid)
            except Exception:
                pass
        
        # Deduplicate consecutive identical entries to reduce noise
        deduped: List[Dict[str, Any]] = []
        last = None
        for s in syscalls:
            if last is None or s["name"] != last["name"] or s.get("ret") != last.get("ret"):
                deduped.append(s)
                last = s
        return deduped
    
    def _analyze_execution_paths(self, environment, binary_info: BinaryInfo):
        """
        Analyze execution paths.
        
        Args:
            environment: Execution environment
            
        Returns:
            Dictionary containing execution path analysis results
        """
        use_frida = self.config.get("analysis.dynamic.use_frida", True)
        if use_frida:
            try:
                import frida  # type: ignore
                return self._analyze_execution_paths_with_frida(binary_info)
            except Exception as e:
                logger.warning(f"Frida path analysis unavailable or failed, falling back: {e}")
        
        # Fallback: try environment-provided coverage/paths
        try:
            if hasattr(environment, "get_coverage"):
                coverage = environment.get_coverage()
            elif hasattr(environment, "execution_paths"):
                coverage = environment.execution_paths
            else:
                coverage = {}
        except Exception as e:
            logger.warning(f"Execution path collection failed: {e}")
            coverage = {}
        
        # Normalize
        if isinstance(coverage, dict):
            return coverage
        return {"blocks": [], "edges": [], "paths": []}

    def _analyze_execution_paths_with_frida(self, binary_info: BinaryInfo) -> Dict[str, Any]:
        import frida  # type: ignore
        # Track a sample of executed basic blocks using Stalker
        session = None
        pid = None
        blocks: Dict[str, int] = {}
        errors: List[str] = []
        script_source = """
        var seen = {};
        Stalker.follow(Process.getCurrentThreadId(), {
          events: { block: true },
          onReceive: function (events) {
            var parsed = Stalker.parse(events);
            for (var i = 0; i < parsed.length; i++) {
              var ev = parsed[i];
              if (ev[0] === 'block') {
                var addr = ptr(ev[1]).toString();
                seen[addr] = (seen[addr] || 0) + 1;
              }
            }
          }
        });
        setInterval(function () {
          var out = [];
          for (var k in seen) out.push({ address: k, count: seen[k] });
          send({ type: 'blocks', blocks: out });
        }, 500);
        """
        try:
            target_path = str(binary_info.path if isinstance(binary_info.path, (str, Path)) else binary_info.path)
            pid = frida.spawn([target_path])
            session = frida.attach(pid)
            script = session.create_script(script_source)
            
            def on_message(message, data):
                try:
                    if message.get("type") == "send":
                        payload = message.get("payload", {})
                        if payload.get("type") == "blocks":
                            for b in payload.get("blocks", []):
                                addr = str(b.get("address"))
                                cnt = int(b.get("count") or 0)
                                blocks[addr] = blocks.get(addr, 0) + cnt
                    elif message.get("type") == "error":
                        errors.append(str(message))
                except Exception as _e:
                    errors.append(f"callback error: {_e}")
            
            script.on("message", on_message)
            script.load()
            frida.resume(pid)
            time.sleep(min(max(int(self.max_execution_time), 1), 5))
        except Exception as e:
            logger.warning(f"Frida execution path error: {e}")
        finally:
            try:
                if session is not None:
                    session.detach()
            except Exception:
                pass
            try:
                if pid is not None:
                    frida.kill(pid)
            except Exception:
                pass
        # Format result
        block_list = [{"address": addr, "count": cnt} for addr, cnt in blocks.items()]
        block_list.sort(key=lambda x: x["count"], reverse=True)
        return {"blocks": block_list, "edges": [], "paths": []}

```

`src/analysis/enhanced_static_analyzer.py`:

```py
"""
Enhanced static analyzer module for RE-Architect.

This module performs static analysis on binary data using Capstone disassembler
to extract function information, control flow, and dependencies.
"""

import logging
import re
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Set, Tuple, Any
from pathlib import Path

try:
    import capstone
    CAPSTONE_AVAILABLE = True
except ImportError:
    CAPSTONE_AVAILABLE = False

try:
    import lief
    LIEF_AVAILABLE = True
except ImportError:
    LIEF_AVAILABLE = False

from src.core.config import Config
from src.core.binary_loader import BinaryInfo, BinaryFormat
from src.core.binary_loader import Architecture  # Import separately to avoid issues
from src.decompilers.base_decompiler import DecompiledCode

logger = logging.getLogger("re-architect.analysis.static")

@dataclass
class Instruction:
    """Represents a disassembled instruction."""
    address: int
    mnemonic: str
    op_str: str
    size: int
    bytes: bytes
    is_call: bool = False
    is_jump: bool = False
    is_return: bool = False
    target_address: Optional[int] = None

@dataclass 
class BasicBlock:
    """Represents a basic block in the control flow."""
    start_address: int
    end_address: int
    instructions: List[Instruction] = field(default_factory=list)
    successors: List[int] = field(default_factory=list)
    predecessors: List[int] = field(default_factory=list)
    
    @property
    def size(self) -> int:
        """Get the size of this basic block."""
        return len(self.instructions)

@dataclass
class FunctionInfo:
    """Information about a function extracted from static analysis."""
    address: int
    name: str
    size: int
    instructions: List[Instruction] = field(default_factory=list)
    basic_blocks: List[BasicBlock] = field(default_factory=list)
    calls: List[int] = field(default_factory=list)  # Addresses of functions called
    called_by: List[int] = field(default_factory=list)  # Addresses of functions that call this
    parameters: List[str] = field(default_factory=list)
    return_type: str = "unknown"
    complexity: float = 0.0
    is_library: bool = False
    has_loops: bool = False
    has_switch: bool = False
    entry_point: bool = False
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            "address": self.address,
            "name": self.name,
            "size": self.size,
            "instruction_count": len(self.instructions),
            "basic_block_count": len(self.basic_blocks),
            "calls": self.calls,
            "called_by": self.called_by,
            "parameters": self.parameters,
            "return_type": self.return_type,
            "complexity": self.complexity,
            "is_library": self.is_library,
            "has_loops": self.has_loops,
            "has_switch": self.has_switch,
            "entry_point": self.entry_point
        }

@dataclass
class StaticAnalysisResults:
    """Results from static analysis of binary code."""
    functions: Dict[int, FunctionInfo]
    call_graph: Dict[int, Set[int]]
    reverse_call_graph: Dict[int, Set[int]]
    strings: List[Tuple[int, str]] = field(default_factory=list)
    
    @property
    def function_count(self) -> int:
        """Get the number of analyzed functions."""
        return len(self.functions)

class EnhancedStaticAnalyzer:
    """
    Enhanced static analyzer for RE-Architect.
    
    This class performs static analysis on binary data using Capstone disassembler
    to extract function information, detect patterns, and build a call graph.
    """
    
    def __init__(self, config: Config):
        """
        Initialize the static analyzer.
        
        Args:
            config: Configuration object
        """
        self.config = config
        self.depth = self.config.get("analysis.static.function_analysis_depth", "medium")
        
        if not CAPSTONE_AVAILABLE:
            logger.warning("Capstone not available. Static analysis will be limited.")
        
        self._disassembler = None
        self._architecture = None
        self._mode = None
    
    def analyze(self, decompiled_code: DecompiledCode) -> StaticAnalysisResults:
        """
        Analyze decompiled code (legacy interface for compatibility).
        
        Args:
            decompiled_code: Decompiled code to analyze
            
        Returns:
            StaticAnalysisResults object containing analysis results
        """
        logger.info("Using legacy static analysis (no binary data)")
        
        # Create empty results for compatibility
        return StaticAnalysisResults(
            functions={},
            call_graph={},
            reverse_call_graph={}
        )
    
    def analyze_binary(self, binary_info: BinaryInfo) -> StaticAnalysisResults:
        """
        Analyze binary data using Capstone disassembler.
        
        Args:
            binary_info: Binary information from BinaryLoader
            
        Returns:
            StaticAnalysisResults object containing analysis results
        """
        logger.info(f"Starting enhanced static analysis of {binary_info.path}")
        
        if not CAPSTONE_AVAILABLE:
            logger.error("Capstone not available for static analysis")
            return StaticAnalysisResults(functions={}, call_graph={}, reverse_call_graph={})
        
        # Initialize disassembler for this architecture
        self._setup_disassembler(binary_info.architecture, binary_info.bit_width)
        
        if not self._disassembler:
            logger.error("Failed to initialize disassembler")
            return StaticAnalysisResults(functions={}, call_graph={}, reverse_call_graph={})
        
        # Read binary data
        binary_data = self._read_binary_data(binary_info.path)
        if not binary_data:
            return StaticAnalysisResults(functions={}, call_graph={}, reverse_call_graph={})
        
        # Find executable sections
        executable_sections = self._find_executable_sections(binary_info)
        
        # Extract strings
        strings = self._extract_strings(binary_data)
        
        # Analyze functions
        functions = {}
        
        # Start with known symbols (functions)
        function_addresses = self._get_function_addresses(binary_info)
        
        # If no symbols, try to identify functions heuristically
        if not function_addresses:
            function_addresses = self._identify_functions_heuristic(binary_data, executable_sections)
        
        # Analyze each function
        for addr in function_addresses:
            try:
                func_info = self._analyze_function(binary_data, addr, binary_info)
                if func_info:
                    functions[addr] = func_info
            except Exception as e:
                logger.warning(f"Failed to analyze function at 0x{addr:08x}: {e}")
        
        # Build call graphs
        call_graph, reverse_call_graph = self._build_call_graphs(functions)
        
        logger.info(f"Static analysis completed: {len(functions)} functions analyzed")
        
        return StaticAnalysisResults(
            functions=functions,
            call_graph=call_graph,
            reverse_call_graph=reverse_call_graph,
            strings=strings
        )
    
    def _setup_disassembler(self, architecture: Architecture, bit_width: int):
        """Setup Capstone disassembler for the given architecture."""
        try:
            logger.info(f"Setting up disassembler for {architecture} ({bit_width}-bit)")
            
            # Compare by value to avoid enum identity issues
            if architecture.value == "x86":
                logger.info("Detected X86 architecture - setting up 32-bit mode")
                if bit_width == 64:
                    self._disassembler = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_64)
                else:
                    self._disassembler = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_32)
            elif architecture.value == "x86_64":
                logger.info("Detected X86_64 architecture - setting up 64-bit mode")
                self._disassembler = capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_64)
            elif architecture.value == "arm":
                logger.info("Detected ARM architecture")
                self._disassembler = capstone.Cs(capstone.CS_ARCH_ARM, capstone.CS_MODE_ARM)
            elif architecture.value == "arm64":
                logger.info("Detected ARM64 architecture")
                self._disassembler = capstone.Cs(capstone.CS_ARCH_ARM64, capstone.CS_MODE_ARM)
            else:
                logger.warning(f"Unsupported architecture for disassembly: {architecture}")
                return
            
            # Enable detailed instruction information
            self._disassembler.detail = True
            self._architecture = architecture
            logger.info(f"Disassembler setup successful for {architecture}")
            
        except Exception as e:
            logger.error(f"Failed to setup disassembler: {e}")
            import traceback
            logger.error(traceback.format_exc())
            self._disassembler = None
    
    def _read_binary_data(self, binary_path: Path) -> Optional[bytes]:
        """Read binary file data."""
        try:
            with open(binary_path, 'rb') as f:
                return f.read()
        except Exception as e:
            logger.error(f"Failed to read binary data: {e}")
            return None
    
    def _find_executable_sections(self, binary_info: BinaryInfo) -> List[Tuple[int, int, bytes]]:
        """Find executable sections in the binary."""
        executable_sections = []
        
        # Look for executable sections
        for section_name, section_info in binary_info.sections.items():
            # Common executable section names
            if (section_name.startswith('.text') or 
                section_name.startswith('.code') or
                'exec' in section_name.lower()):
                
                addr = section_info.get('virtual_address', 0)
                size = section_info.get('size', 0)
                offset = section_info.get('offset', 0)
                
                if addr > 0 and size > 0:
                    executable_sections.append((addr, size, offset))
        
        return executable_sections
    
    def _extract_strings(self, binary_data: bytes) -> List[Tuple[int, str]]:
        """Extract strings from binary data."""
        strings = []
        
        # Simple string extraction (printable ASCII strings >= 4 chars)
        current_string = ""
        start_offset = 0
        
        for i, byte in enumerate(binary_data):
            if 32 <= byte <= 126:  # Printable ASCII
                if not current_string:
                    start_offset = i
                current_string += chr(byte)
            else:
                if len(current_string) >= 4:
                    strings.append((start_offset, current_string))
                current_string = ""
        
        # Don't forget the last string
        if len(current_string) >= 4:
            strings.append((start_offset, current_string))
        
        return strings
    
    def _get_function_addresses(self, binary_info: BinaryInfo) -> List[int]:
        """Get function addresses from symbols."""
        function_addresses = []
        
        for symbol_name, symbol_info in binary_info.symbols.items():
            # Look for function symbols
            if (symbol_info.get('type') in ['static', 'dynamic'] and
                not symbol_name.startswith('.') and
                symbol_info.get('value', 0) > 0):
                function_addresses.append(symbol_info['value'])
        
        # Add entry point if available
        if binary_info.entry_point > 0:
            function_addresses.append(binary_info.entry_point)
        
        return sorted(set(function_addresses))
    
    def _identify_functions_heuristic(self, binary_data: bytes, executable_sections: List[Tuple[int, int, int]]) -> List[int]:
        """Identify function start addresses using heuristics."""
        function_addresses = []
        
        # This is a simplified heuristic - in practice, this would be much more sophisticated
        # Look for common function prologue patterns
        
        if (self._architecture and 
            (self._architecture.value == "x86" or self._architecture.value == "x86_64")):
            # Common x86/x64 function prologues
            prologues = [
                b'\x55\x8b\xec',        # push ebp; mov ebp, esp
                b'\x55\x48\x89\xe5',    # push rbp; mov rbp, rsp (x64)
                b'\x48\x83\xec',        # sub rsp, imm (x64)
                b'\x83\xec',            # sub esp, imm (x86)
            ]
            
            for addr, size, offset in executable_sections:
                if offset + size > len(binary_data):
                    continue
                
                section_data = binary_data[offset:offset + size]
                for i in range(len(section_data) - 8):
                    for prologue in prologues:
                        if section_data[i:i + len(prologue)] == prologue:
                            function_addresses.append(addr + i)
        
        return sorted(set(function_addresses))
    
    def _analyze_function(self, binary_data: bytes, address: int, binary_info: BinaryInfo) -> Optional[FunctionInfo]:
        """Analyze a single function starting at the given address."""
        
        # Find the section containing this address
        section_data, section_offset = self._get_section_data(binary_data, address, binary_info)
        if not section_data:
            return None
        
        # Calculate offset within section
        data_offset = address - section_offset
        if data_offset < 0 or data_offset >= len(section_data):
            return None
        
        # Disassemble instructions
        instructions = []
        max_instructions = 1000  # Prevent infinite loops
        current_offset = data_offset
        
        try:
            for insn in self._disassembler.disasm(section_data[data_offset:], address, max_instructions):
                instruction = Instruction(
                    address=insn.address,
                    mnemonic=insn.mnemonic,
                    op_str=insn.op_str,
                    size=insn.size,
                    bytes=insn.bytes,
                    is_call='call' in insn.mnemonic.lower(),
                    is_jump=insn.group(capstone.CS_GRP_JUMP) if hasattr(insn, 'group') else False,
                    is_return=insn.group(capstone.CS_GRP_RET) if hasattr(insn, 'group') else 'ret' in insn.mnemonic.lower()
                )
                
                instructions.append(instruction)
                
                # Stop at return instruction (simple heuristic)
                if instruction.is_return:
                    break
                
                # Stop if we hit another known function start (simple heuristic)
                if len(instructions) > 1 and insn.address in binary_info.symbols:
                    break
        
        except Exception as e:
            logger.warning(f"Disassembly failed for function at 0x{address:08x}: {e}")
            return None
        
        if not instructions:
            return None
        
        # Find function name
        func_name = f"func_{address:08x}"
        for symbol_name, symbol_info in binary_info.symbols.items():
            if symbol_info.get('value') == address:
                func_name = symbol_name
                break
        
        # Calculate function size
        func_size = instructions[-1].address - address + instructions[-1].size
        
        # Extract function calls
        calls = []
        for insn in instructions:
            if insn.is_call and insn.target_address:
                calls.append(insn.target_address)
        
        # Create basic blocks (simplified)
        basic_blocks = self._create_basic_blocks(instructions)
        
        # Calculate complexity (simplified McCabe complexity)
        complexity = self._calculate_complexity(instructions, basic_blocks)
        
        # Detect loops and switches (simplified)
        has_loops = self._detect_loops(instructions)
        has_switch = self._detect_switch(instructions)
        
        return FunctionInfo(
            address=address,
            name=func_name,
            size=func_size,
            instructions=instructions,
            basic_blocks=basic_blocks,
            calls=calls,
            complexity=complexity,
            has_loops=has_loops,
            has_switch=has_switch,
            entry_point=(address == binary_info.entry_point)
        )
    
    def _get_section_data(self, binary_data: bytes, address: int, binary_info: BinaryInfo) -> Tuple[Optional[bytes], int]:
        """Get section data containing the given address."""
        for section_name, section_info in binary_info.sections.items():
            vaddr = section_info.get('virtual_address', 0)
            size = section_info.get('size', 0)
            offset = section_info.get('offset', 0)
            
            if vaddr <= address < vaddr + size:
                # Found the section
                if offset + size > len(binary_data):
                    return None, 0
                return binary_data[offset:offset + size], vaddr
        
        return None, 0
    
    def _create_basic_blocks(self, instructions: List[Instruction]) -> List[BasicBlock]:
        """Create basic blocks from instructions (simplified)."""
        if not instructions:
            return []
        
        # For now, create one basic block per function (simplified)
        # In a full implementation, this would properly split on jumps/calls
        block = BasicBlock(
            start_address=instructions[0].address,
            end_address=instructions[-1].address,
            instructions=instructions
        )
        
        return [block]
    
    def _calculate_complexity(self, instructions: List[Instruction], basic_blocks: List[BasicBlock]) -> float:
        """Calculate cyclomatic complexity (simplified)."""
        # Simplified: count decision points (jumps, calls)
        decision_points = sum(1 for insn in instructions if insn.is_jump or insn.is_call)
        return float(decision_points + 1)
    
    def _detect_loops(self, instructions: List[Instruction]) -> bool:
        """Detect if function contains loops (simplified)."""
        # Look for backward jumps (simplified heuristic)
        for insn in instructions:
            if insn.is_jump and insn.target_address and insn.target_address < insn.address:
                return True
        return False
    
    def _detect_switch(self, instructions: List[Instruction]) -> bool:
        """Detect if function contains switch statements (simplified)."""
        # Look for jump tables or multiple jumps (simplified heuristic)
        jump_count = sum(1 for insn in instructions if insn.is_jump)
        return jump_count > 3
    
    def _build_call_graphs(self, functions: Dict[int, FunctionInfo]) -> Tuple[Dict[int, Set[int]], Dict[int, Set[int]]]:
        """Build call graph and reverse call graph."""
        call_graph = {}
        reverse_call_graph = {}
        
        for addr, func_info in functions.items():
            call_graph[addr] = set(func_info.calls)
            
            # Build reverse call graph
            for called_addr in func_info.calls:
                if called_addr not in reverse_call_graph:
                    reverse_call_graph[called_addr] = set()
                reverse_call_graph[called_addr].add(addr)
        
        return call_graph, reverse_call_graph


# Alias for backwards compatibility
StaticAnalyzer = EnhancedStaticAnalyzer
```

`src/analysis/static_analyzer.py`:

```py
"""
Static analyzer module for RE-Architect.

This module performs static analysis on decompiled code to extract
function information and dependencies.
"""

import logging
import re
from dataclasses import dataclass
from typing import Dict, List, Optional, Set, Tuple, Any

from src.core.config import Config
from src.decompilers.base_decompiler import DecompiledCode

logger = logging.getLogger("re-architect.analysis.static")

@dataclass
class FunctionInfo:
    """Information about a function extracted from static analysis."""
    address: int
    name: str
    code: str
    signature: str
    parameters: List[Dict[str, Any]]
    return_type: str
    calls: List[Dict[str, Any]]
    called_by: List[int]
    complexity: float
    size: int
    is_library: bool
    has_loops: bool
    has_switch: bool
    variables: List[Dict[str, Any]]
    basic_blocks: int
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            "address": self.address,
            "name": self.name,
            "code": self.code,
            "signature": self.signature,
            "parameters": self.parameters,
            "return_type": self.return_type,
            "calls": self.calls,
            "called_by": self.called_by,
            "complexity": self.complexity,
            "size": self.size,
            "is_library": self.is_library,
            "has_loops": self.has_loops,
            "has_switch": self.has_switch,
            "variables": self.variables,
            "basic_blocks": self.basic_blocks
        }

@dataclass
class StaticAnalysisResults:
    """Results from static analysis of decompiled code."""
    functions: Dict[int, Dict[str, Any]]
    call_graph: Dict[int, Set[int]]
    reverse_call_graph: Dict[int, Set[int]]
    
    @property
    def function_count(self) -> int:
        """Get the number of analyzed functions."""
        return len(self.functions)

class StaticAnalyzer:
    """
    Static analyzer for RE-Architect.
    
    This class performs static analysis on decompiled code to extract
    function information, detect patterns, and build a call graph.
    """
    
    def __init__(self, config: Config):
        """
        Initialize the static analyzer.
        
        Args:
            config: Configuration object
        """
        self.config = config
        self.depth = self.config.get("analysis.static.function_analysis_depth", "medium")
        
    def analyze(self, decompiled_code: DecompiledCode) -> StaticAnalysisResults:
        """
        Analyze decompiled code to extract information.
        
        Args:
            decompiled_code: Decompiled code to analyze
            
        Returns:
            StaticAnalysisResults object containing analysis results
        """
        logger.info("Starting static analysis")
        
        # Extract information about each function
        functions = {}
        for addr, code in decompiled_code.functions.items():
            name = decompiled_code.function_names.get(addr, f"func_{addr:x}")
            metadata = decompiled_code.function_metadata.get(addr, {})
            
            # Analyze the function
            func_info = self._analyze_function(addr, name, code, metadata)
            functions[addr] = func_info.to_dict()
        
        logger.info(f"Analyzed {len(functions)} functions")
        
        # Build call graph
        call_graph, reverse_call_graph = self._build_call_graph(functions)
        
        # Update called_by information
        for addr, calls in reverse_call_graph.items():
            if addr in functions:
                functions[addr]["called_by"] = list(calls)
        
        return StaticAnalysisResults(
            functions=functions,
            call_graph=call_graph,
            reverse_call_graph=reverse_call_graph
        )
    
    def _analyze_function(
        self,
        address: int,
        name: str,
        code: str,
        metadata: Dict[str, Any]
    ) -> FunctionInfo:
        """
        Analyze a single function.
        
        Args:
            address: Function address
            name: Function name
            code: Decompiled function code
            metadata: Additional function metadata
            
        Returns:
            FunctionInfo object containing analysis results
        """
        logger.debug(f"Analyzing function {name} at 0x{address:x}")
        
        # Extract signature
        signature = metadata.get("signature", "")
        
        # Get parameters and return type
        parameters = metadata.get("parameters", [])
        return_type = metadata.get("returnType", "")
        
        # Get calls
        calls = metadata.get("calls", [])
        
        # Initially empty called_by list (will be populated later)
        called_by = []
        
        # Estimate size by counting lines
        size = len(code.splitlines())
        
        # Check if it's a library function
        is_library = self._is_library_function(name)
        
        # Detect loops
        has_loops = self._has_loops(code)
        
        # Detect switch statements
        has_switch = self._has_switch(code)
        
        # Extract variables
        variables = self._extract_variables(code)
        
        # Estimate number of basic blocks
        basic_blocks = self._estimate_basic_blocks(code)
        
        # Calculate cyclomatic complexity
        complexity = self._calculate_complexity(code)
        
        return FunctionInfo(
            address=address,
            name=name,
            code=code,
            signature=signature,
            parameters=parameters,
            return_type=return_type,
            calls=calls,
            called_by=called_by,
            complexity=complexity,
            size=size,
            is_library=is_library,
            has_loops=has_loops,
            has_switch=has_switch,
            variables=variables,
            basic_blocks=basic_blocks
        )
    
    def _is_library_function(self, name: str) -> bool:
        """
        Check if a function is likely a standard library function.
        
        Args:
            name: Function name
            
        Returns:
            True if the function is likely a standard library function
        """
        # Common library function prefixes
        lib_prefixes = ["std::", "str", "mem", "print", "malloc", "free", "calloc", "realloc"]
        
        # Common library functions
        lib_functions = [
            "printf", "sprintf", "scanf", "sscanf", "fprintf", "fscanf",
            "malloc", "calloc", "realloc", "free",
            "strcpy", "strncpy", "strcmp", "strncmp", "strcat", "strncat",
            "memcpy", "memmove", "memset", "memcmp",
            "fopen", "fclose", "fread", "fwrite", "fseek",
            "atoi", "atof", "atol", "strtol", "strtod",
            "exit", "abort", "assert"
        ]
        
        # Check for exact match
        if name in lib_functions:
            return True
        
        # Check for prefix match
        for prefix in lib_prefixes:
            if name.startswith(prefix):
                return True
        
        return False
    
    def _has_loops(self, code: str) -> bool:
        """
        Check if a function contains loops.
        
        Args:
            code: Decompiled function code
            
        Returns:
            True if the function contains loops
        """
        # Look for common loop keywords
        loop_keywords = ["for", "while", "do"]
        
        for keyword in loop_keywords:
            pattern = fr"\b{keyword}\s*\("
            if re.search(pattern, code):
                return True
        
        return False
    
    def _has_switch(self, code: str) -> bool:
        """
        Check if a function contains switch statements.
        
        Args:
            code: Decompiled function code
            
        Returns:
            True if the function contains switch statements
        """
        return "switch" in code
    
    def _extract_variables(self, code: str) -> List[Dict[str, Any]]:
        """
        Extract variable declarations from function code.
        
        Args:
            code: Decompiled function code
            
        Returns:
            List of dictionaries containing variable information
        """
        variables = []
        
        # Simple regex to match variable declarations
        # This is a basic implementation and may miss complex declarations
        var_pattern = r"(\w+)\s+(\w+)\s*(?:=\s*([^;]+))?\s*;"
        
        for match in re.finditer(var_pattern, code):
            var_type = match.group(1)
            var_name = match.group(2)
            initial_value = match.group(3)
            
            variables.append({
                "name": var_name,
                "type": var_type,
                "initial_value": initial_value
            })
        
        return variables
    
    def _estimate_basic_blocks(self, code: str) -> int:
        """
        Estimate the number of basic blocks in a function.
        
        Args:
            code: Decompiled function code
            
        Returns:
            Estimated number of basic blocks
        """
        # This is a simple heuristic: count statements that likely start new blocks
        block_starters = [
            "{", "}", "if", "else", "for", "while", "do", "switch", "case", "default", "return"
        ]
        
        count = 1  # Start with one block
        
        for starter in block_starters:
            pattern = fr"\b{starter}\b"
            count += len(re.findall(pattern, code))
        
        return count
    
    def _calculate_complexity(self, code: str) -> float:
        """
        Calculate cyclomatic complexity of a function.
        
        Args:
            code: Decompiled function code
            
        Returns:
            Estimated cyclomatic complexity
        """
        # McCabe's cyclomatic complexity is: E - N + 2P
        # Where E is the number of edges, N is the number of nodes, and P is the number of connected components
        # For a simple approximation, we'll count decision points and add 1
        
        decision_points = [
            r"\bif\s*\(",
            r"\bfor\s*\(",
            r"\bwhile\s*\(",
            r"\bcase\s+",
            r"\b&&\b",
            r"\b\|\|\b",
            r"\?",  # Ternary operator
            r"\bcatch\s*\("
        ]
        
        complexity = 1  # Base complexity
        
        for pattern in decision_points:
            complexity += len(re.findall(pattern, code))
        
        return complexity
    
    def _build_call_graph(
        self,
        functions: Dict[int, Dict[str, Any]]
    ) -> Tuple[Dict[int, Set[int]], Dict[int, Set[int]]]:
        """
        Build a call graph from function information.
        
        Args:
            functions: Dictionary mapping function addresses to function information
            
        Returns:
            Tuple containing forward and reverse call graphs
        """
        # Forward call graph: function -> called functions
        call_graph = {}
        
        # Reverse call graph: function -> functions that call it
        reverse_call_graph = {}
        
        # Initialize graphs
        for addr in functions:
            call_graph[addr] = set()
            reverse_call_graph[addr] = set()
        
        # Build the graphs
        for addr, func_info in functions.items():
            # Process each call made by this function
            for call in func_info.get("calls", []):
                to_addr_str = call.get("toAddress", "")
                
                # Extract address from the string (e.g., "00401234" from "ram:00401234")
                match = re.search(r"([0-9a-fA-F]+)$", to_addr_str)
                if match:
                    to_addr = int(match.group(1), 16)
                    
                    # Add to forward graph
                    if addr in call_graph:
                        call_graph[addr].add(to_addr)
                    
                    # Add to reverse graph
                    if to_addr in reverse_call_graph:
                        reverse_call_graph[to_addr].add(addr)
        
        return call_graph, reverse_call_graph

```

`src/analysis/unified_static_analyzer.py`:

```py
"""
Unified static analyzer that can work with both binary data and decompiled code.

This module provides a single interface for static analysis that automatically
chooses the best approach based on available data and capabilities.
"""

import logging
from typing import Optional, Union
from pathlib import Path

from src.core.config import Config
from src.core.binary_loader import BinaryInfo
from src.decompilers.base_decompiler import DecompiledCode
from src.analysis.static_analyzer import StaticAnalyzer, StaticAnalysisResults as LegacyResults
from src.analysis.enhanced_static_analyzer import EnhancedStaticAnalyzer, StaticAnalysisResults as EnhancedResults

logger = logging.getLogger("re-architect.analysis.unified")

class UnifiedStaticAnalyzer:
    """
    Unified static analyzer that can analyze both binary data and decompiled code.
    
    This analyzer automatically selects the best analysis approach:
    - If binary data is available and Capstone is installed: Enhanced binary-level analysis
    - If only decompiled code is available: Legacy decompiled code analysis
    - If both are available: Enhanced analysis with legacy as fallback
    """
    
    def __init__(self, config: Config):
        """
        Initialize the unified static analyzer.
        
        Args:
            config: Configuration object
        """
        self.config = config
        
        # Initialize both analyzers
        self.legacy_analyzer = StaticAnalyzer(config)
        self.enhanced_analyzer = EnhancedStaticAnalyzer(config)
        
        # Check capabilities
        self.enhanced_available = self._check_enhanced_capabilities()
        
    def analyze(
        self,
        binary_info: Optional[BinaryInfo] = None,
        decompiled_code: Optional[DecompiledCode] = None
    ) -> Union[EnhancedResults, LegacyResults]:
        """
        Perform static analysis using the best available method.
        
        Args:
            binary_info: Optional binary information for enhanced analysis
            decompiled_code: Optional decompiled code for legacy analysis
            
        Returns:
            Analysis results from the selected analyzer
            
        Raises:
            ValueError: If no valid input is provided
        """
        if not binary_info and not decompiled_code:
            raise ValueError("Either binary_info or decompiled_code must be provided")
        
        # Prefer enhanced analysis if available and binary info is provided
        if self.enhanced_available and binary_info:
            logger.info("Using enhanced binary-level static analysis")
            try:
                return self.enhanced_analyzer.analyze_binary(binary_info)
            except Exception as e:
                logger.warning(f"Enhanced analysis failed: {e}")
                if decompiled_code:
                    logger.info("Falling back to legacy decompiled code analysis")
                    return self.legacy_analyzer.analyze(decompiled_code)
                else:
                    raise
        
        # Use legacy analysis if enhanced is not available or failed
        if decompiled_code:
            logger.info("Using legacy decompiled code static analysis")
            return self.legacy_analyzer.analyze(decompiled_code)
        
        # If we get here, we have binary info but enhanced analysis is not available
        logger.warning("Enhanced analysis not available and no decompiled code provided")
        raise ValueError("Cannot perform analysis without enhanced capabilities or decompiled code")
    
    def analyze_binary(self, binary_info: BinaryInfo) -> EnhancedResults:
        """
        Perform enhanced binary-level static analysis.
        
        Args:
            binary_info: Binary information to analyze
            
        Returns:
            Enhanced analysis results
            
        Raises:
            RuntimeError: If enhanced analysis is not available
        """
        if not self.enhanced_available:
            raise RuntimeError("Enhanced binary analysis not available - missing dependencies")
        
        logger.info("Performing enhanced binary-level static analysis")
        return self.enhanced_analyzer.analyze_binary(binary_info)
    
    def analyze_decompiled(self, decompiled_code: DecompiledCode) -> LegacyResults:
        """
        Perform legacy decompiled code static analysis.
        
        Args:
            decompiled_code: Decompiled code to analyze
            
        Returns:
            Legacy analysis results
        """
        logger.info("Performing legacy decompiled code static analysis")
        return self.legacy_analyzer.analyze(decompiled_code)
    
    def _check_enhanced_capabilities(self) -> bool:
        """
        Check if enhanced binary analysis is available.
        
        Returns:
            True if enhanced analysis dependencies are available
        """
        try:
            import capstone
            return True
        except ImportError:
            logger.warning("Enhanced static analysis not available - Capstone not installed")
            return False
    
    def get_analysis_info(self) -> dict:
        """
        Get information about available analysis capabilities.
        
        Returns:
            Dictionary with capability information
        """
        return {
            "enhanced_available": self.enhanced_available,
            "legacy_available": True,
            "preferred_method": "enhanced" if self.enhanced_available else "legacy",
            "capstone_available": self.enhanced_available
        }
```

`src/auth/__init__.py`:

```py
"""Authentication module for RE-Architect."""

from .middleware import login_required

__all__ = ['login_required']
```

`src/auth/middleware.py`:

```py
"""Authentication middleware for RE-Architect web interface."""

from functools import wraps
from flask import request, jsonify, session, g


def login_required(f):
    """
    Decorator that requires authentication for API endpoints.
    For now, this is a placeholder that allows all requests.
    In a production environment, this should implement proper authentication.
    """
    @wraps(f)
    def decorated_function(*args, **kwargs):
        # TODO: Implement proper authentication logic
        # For now, we'll allow all requests to pass through
        # In production, check for valid session, API key, or JWT token
        
        # Mock user for development
        g.user = {
            'id': 'dev_user',
            'username': 'developer',
            'role': 'admin'
        }
        
        return f(*args, **kwargs)
    return decorated_function


def check_api_key():
    """Check if request has valid API key."""
    # TODO: Implement API key validation
    api_key = request.headers.get('X-API-Key')
    return True  # Allow all for now


def authenticate_user(username, password):
    """Authenticate user credentials."""
    # TODO: Implement proper user authentication
    # This is a placeholder for development
    return {
        'id': 'dev_user',
        'username': username,
        'role': 'admin'
    }


def create_session(user):
    """Create user session."""
    # TODO: Implement session management
    session['user_id'] = user['id']
    session['username'] = user['username']
    return True


def destroy_session():
    """Destroy user session."""
    session.clear()
    return True
```

`src/comparison/__init__.py`:

```py
"""Binary comparison package for RE-ARCHITECT.

This package contains modules for comparing binary analysis results
between different versions of a program.
"""

from src.comparison.models import AnalysisProject, ComparisonResult
from src.comparison.comparator import BinaryComparator
from src.comparison.store import ComparisonStore
from src.comparison.analyzer import (
    BinaryComparisonAnalyzer,
    BinaryDiff,
    FunctionDiff,
    StructureDiff
)

__all__ = [
    'AnalysisProject', 
    'ComparisonResult', 
    'BinaryComparator',
    'ComparisonStore',
    'BinaryComparisonAnalyzer',
    'BinaryDiff',
    'FunctionDiff',
    'StructureDiff'
]
```

`src/comparison/analyzer.py`:

```py
"""Binary comparison analyzer module.

This module implements algorithms for comparing binary analysis results
between different versions of a program.
"""

from typing import Dict, List, Any, Optional, Tuple, Set
import difflib
from dataclasses import dataclass


@dataclass
class FunctionDiff:
    """Represents differences between two versions of a function."""
    
    name: str
    address_before: str
    address_after: str
    size_before: int
    size_after: int
    complexity_before: int
    complexity_after: int
    size_change: int
    complexity_change: int
    code_similarity: float  # 0-1 score of similarity
    match_confidence: float  # 0-1 confidence in the match
    
    # Lists of call addresses that were added/removed
    added_calls: List[str]
    removed_calls: List[str]
    
    # If available, detailed instruction differences
    instruction_changes: Optional[Dict[str, Any]] = None


@dataclass
class StructureDiff:
    """Represents differences between two versions of a data structure."""
    
    name: str
    type_before: str
    type_after: str
    size_before: int
    size_after: int
    size_change: int
    
    # Track changes to members
    added_members: List[Dict[str, Any]]
    removed_members: List[Dict[str, Any]]
    modified_members: List[Dict[str, Any]]
    
    match_confidence: float  # 0-1 confidence in the match


@dataclass
class BinaryDiff:
    """Top-level diff between two binary analyses."""
    
    # Basic binary info
    name: str
    version_before: str
    version_after: str
    
    # Function changes
    added_functions: List[Dict[str, Any]]
    removed_functions: List[Dict[str, Any]]
    modified_functions: List[FunctionDiff]
    
    # Structure changes
    added_structures: List[Dict[str, Any]]
    removed_structures: List[Dict[str, Any]]
    modified_structures: List[StructureDiff]
    
    # Overall statistics
    function_count_before: int
    function_count_after: int
    structure_count_before: int
    structure_count_after: int
    
    # Security changes
    security_issues_added: List[Dict[str, Any]]
    security_issues_removed: List[Dict[str, Any]]


class BinaryComparisonAnalyzer:
    """Analyzes and compares two binary analysis results."""
    
    def __init__(self, threshold: float = 0.7):
        """Initialize with similarity threshold.
        
        Args:
            threshold: Minimum similarity score to consider two functions a match
        """
        self.similarity_threshold = threshold
    
    def compare_binaries(self, before: Dict[str, Any], after: Dict[str, Any]) -> BinaryDiff:
        """Compare two binary analysis results.
        
        Args:
            before: Analysis results from first binary version
            after: Analysis results from second binary version
            
        Returns:
            BinaryDiff object with comparison details
        """
        # Match functions between the two binaries
        function_matches = self._match_functions(before.get('functions', []), 
                                                after.get('functions', []))
        
        # Compare matched functions
        modified_functions = []
        for before_func, after_func, confidence in function_matches:
            diff = self._compare_functions(before_func, after_func, confidence)
            modified_functions.append(diff)
        
        # Find added/removed functions
        before_matched = {f['id'] for f, _, _ in function_matches}
        after_matched = {f['id'] for _, f, _ in function_matches}
        
        added_functions = [f for f in after.get('functions', []) 
                           if f['id'] not in after_matched]
        removed_functions = [f for f in before.get('functions', []) 
                             if f['id'] not in before_matched]
        
        # Match and compare data structures
        struct_matches = self._match_structures(before.get('data_structures', []),
                                               after.get('data_structures', []))
        
        # Compare matched structures
        modified_structures = []
        for before_struct, after_struct, confidence in struct_matches:
            diff = self._compare_structures(before_struct, after_struct, confidence)
            modified_structures.append(diff)
        
        # Find added/removed structures
        before_struct_matched = {s['name'] for s, _, _ in struct_matches}
        after_struct_matched = {s['name'] for _, s, _ in struct_matches}
        
        added_structures = [s for s in after.get('data_structures', []) 
                           if s['name'] not in after_struct_matched]
        removed_structures = [s for s in before.get('data_structures', []) 
                             if s['name'] not in before_struct_matched]
        
        # Compare security issues
        security_issues_added, security_issues_removed = self._compare_security_issues(
            before.get('security_issues', []),
            after.get('security_issues', [])
        )
        
        # Create the binary diff
        return BinaryDiff(
            name=after.get('name', 'Unknown Binary'),
            version_before=before.get('version', 'Unknown'),
            version_after=after.get('version', 'Unknown'),
            added_functions=added_functions,
            removed_functions=removed_functions,
            modified_functions=modified_functions,
            added_structures=added_structures,
            removed_structures=removed_structures,
            modified_structures=modified_structures,
            function_count_before=len(before.get('functions', [])),
            function_count_after=len(after.get('functions', [])),
            structure_count_before=len(before.get('data_structures', [])),
            structure_count_after=len(after.get('data_structures', [])),
            security_issues_added=security_issues_added,
            security_issues_removed=security_issues_removed
        )
    
    def _match_functions(self, before: List[Dict[str, Any]], 
                        after: List[Dict[str, Any]]) -> List[Tuple[Dict[str, Any], Dict[str, Any], float]]:
        """Match functions between two binaries using name and signature similarity.
        
        Args:
            before: List of functions from first binary
            after: List of functions from second binary
            
        Returns:
            List of tuples (before_func, after_func, confidence)
        """
        matches = []
        
        # First, try to match by name (exact match)
        name_matches = {}
        for before_func in before:
            for after_func in after:
                if before_func['name'] == after_func['name']:
                    name_matches.setdefault(before_func['id'], []).append(
                        (after_func, 0.8)  # Base confidence for name match
                    )
        
        # For each before_func with name matches, find the best match
        matched_after_funcs = set()
        
        for before_id, candidates in name_matches.items():
            before_func = next(f for f in before if f['id'] == before_id)
            
            best_match = None
            best_confidence = 0
            
            for after_func, base_confidence in candidates:
                if after_func['id'] in matched_after_funcs:
                    continue
                
                # Calculate similarity based on code content if available
                if ('decompiled_code' in before_func and 
                    'decompiled_code' in after_func):
                    code_sim = self._calculate_code_similarity(
                        before_func['decompiled_code'],
                        after_func['decompiled_code']
                    )
                    # Weighted combination of name match and code similarity
                    confidence = 0.4 * base_confidence + 0.6 * code_sim
                else:
                    confidence = base_confidence
                
                if confidence > best_confidence and confidence >= self.similarity_threshold:
                    best_match = after_func
                    best_confidence = confidence
            
            if best_match:
                matches.append((before_func, best_match, best_confidence))
                matched_after_funcs.add(best_match['id'])
        
        # For functions without name matches, try to match by code similarity
        unmatched_before = [f for f in before if f['id'] not in name_matches]
        unmatched_after = [f for f in after if f['id'] not in matched_after_funcs]
        
        if unmatched_before and unmatched_after:
            # Only proceed with expensive comparison if both sets are non-empty
            for before_func in unmatched_before:
                best_match = None
                best_confidence = 0
                
                for after_func in unmatched_after:
                    # Calculate similarity based on available metrics
                    metrics_sim = self._calculate_metrics_similarity(before_func, after_func)
                    
                    # If code is available, use it to refine the similarity
                    if ('decompiled_code' in before_func and 
                        'decompiled_code' in after_func):
                        code_sim = self._calculate_code_similarity(
                            before_func['decompiled_code'],
                            after_func['decompiled_code']
                        )
                        # Weight code similarity higher than metrics
                        confidence = 0.3 * metrics_sim + 0.7 * code_sim
                    else:
                        confidence = metrics_sim
                    
                    if confidence > best_confidence and confidence >= self.similarity_threshold:
                        best_match = after_func
                        best_confidence = confidence
                
                if best_match:
                    matches.append((before_func, best_match, best_confidence))
                    unmatched_after.remove(best_match)
        
        return matches
    
    def _calculate_code_similarity(self, code1: str, code2: str) -> float:
        """Calculate similarity between two code snippets.
        
        Args:
            code1: First code snippet
            code2: Second code snippet
            
        Returns:
            Similarity score between 0 and 1
        """
        # Use difflib's SequenceMatcher for string similarity
        matcher = difflib.SequenceMatcher(None, code1, code2)
        return matcher.ratio()
    
    def _calculate_metrics_similarity(self, func1: Dict[str, Any], 
                                     func2: Dict[str, Any]) -> float:
        """Calculate similarity based on function metrics.
        
        Args:
            func1: First function data
            func2: Second function data
            
        Returns:
            Similarity score between 0 and 1
        """
        # Compare based on complexity, size, and call patterns
        total_score = 0
        total_weight = 0
        
        # Compare complexity if available
        if 'complexity' in func1 and 'complexity' in func2:
            max_complexity = max(func1['complexity'], func2['complexity'])
            if max_complexity > 0:
                complexity_diff = abs(func1['complexity'] - func2['complexity']) / max_complexity
                complexity_sim = 1 - min(complexity_diff, 1.0)
                total_score += 0.3 * complexity_sim
                total_weight += 0.3
        
        # Compare size if available
        if 'size' in func1 and 'size' in func2:
            max_size = max(func1['size'], func2['size'])
            if max_size > 0:
                size_diff = abs(func1['size'] - func2['size']) / max_size
                size_sim = 1 - min(size_diff, 1.0)
                total_score += 0.3 * size_sim
                total_weight += 0.3
        
        # Compare call counts if available
        if 'callsTo' in func1 and 'callsTo' in func2:
            max_calls = max(func1['callsTo'], func2['callsTo'])
            if max_calls > 0:
                calls_diff = abs(func1['callsTo'] - func2['callsTo']) / max_calls
                calls_sim = 1 - min(calls_diff, 1.0)
                total_score += 0.2 * calls_sim
                total_weight += 0.2
        
        # If we have no metrics to compare, return low similarity
        if total_weight == 0:
            return 0.1
            
        # Normalize score by total weight
        return total_score / total_weight
    
    def _compare_functions(self, before: Dict[str, Any], 
                          after: Dict[str, Any], 
                          confidence: float) -> FunctionDiff:
        """Compare two functions in detail.
        
        Args:
            before: Function data from first binary
            after: Function data from second binary
            confidence: Match confidence score
            
        Returns:
            FunctionDiff object with detailed comparison
        """
        # Extract added and removed calls
        added_calls = []
        removed_calls = []
        
        # If we have call graph data
        before_calls = set()
        after_calls = set()
        
        if 'call_graph' in before and 'links' in before['call_graph']:
            before_calls = {link['target'] for link in before['call_graph']['links'] 
                          if link['source'] == before['id']}
        
        if 'call_graph' in after and 'links' in after['call_graph']:
            after_calls = {link['target'] for link in after['call_graph']['links'] 
                         if link['source'] == after['id']}
        
        added_calls = list(after_calls - before_calls)
        removed_calls = list(before_calls - after_calls)
        
        # Calculate code similarity if available
        code_similarity = 0.0
        if 'decompiled_code' in before and 'decompiled_code' in after:
            code_similarity = self._calculate_code_similarity(
                before['decompiled_code'],
                after['decompiled_code']
            )
        
        # Compare instructions if available
        instruction_changes = None
        if 'disassembly' in before and 'disassembly' in after:
            instruction_changes = self._compare_instructions(
                before['disassembly'],
                after['disassembly']
            )
        
        # Create function diff
        return FunctionDiff(
            name=after['name'],
            address_before=before.get('address', 'Unknown'),
            address_after=after.get('address', 'Unknown'),
            size_before=before.get('size', 0),
            size_after=after.get('size', 0),
            complexity_before=before.get('complexity', 0),
            complexity_after=after.get('complexity', 0),
            size_change=after.get('size', 0) - before.get('size', 0),
            complexity_change=after.get('complexity', 0) - before.get('complexity', 0),
            code_similarity=code_similarity,
            match_confidence=confidence,
            added_calls=added_calls,
            removed_calls=removed_calls,
            instruction_changes=instruction_changes
        )
    
    def _compare_instructions(self, before: List[Dict[str, Any]], 
                             after: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Compare function instructions.
        
        Args:
            before: List of instructions from first function
            after: List of instructions from second function
            
        Returns:
            Dictionary with instruction diff details
        """
        # Extract instruction text for comparison
        before_instr = [f"{i['instruction']} {i.get('comment', '')}" for i in before]
        after_instr = [f"{i['instruction']} {i.get('comment', '')}" for i in after]
        
        # Get unified diff
        diff = list(difflib.unified_diff(before_instr, after_instr, n=1))
        
        # Count added, modified, and removed instructions
        added = sum(1 for d in diff if d.startswith('+') and not d.startswith('+++'))
        removed = sum(1 for d in diff if d.startswith('-') and not d.startswith('---'))
        
        return {
            'added_count': added,
            'removed_count': removed,
            'modified_count': min(added, removed),  # Conservative estimate of modifications
            'total_before': len(before),
            'total_after': len(after),
            'change_ratio': (added + removed) / max(len(before) + len(after), 1)
        }
    
    def _match_structures(self, before: List[Dict[str, Any]], 
                         after: List[Dict[str, Any]]) -> List[Tuple[Dict[str, Any], Dict[str, Any], float]]:
        """Match data structures between two binaries.
        
        Args:
            before: List of data structures from first binary
            after: List of data structures from second binary
            
        Returns:
            List of tuples (before_struct, after_struct, confidence)
        """
        matches = []
        
        # First match by exact name
        matched_after = set()
        
        for before_struct in before:
            for after_struct in after:
                if (after_struct['name'] == before_struct['name'] and 
                    after_struct['name'] not in matched_after):
                    # Calculate similarity based on members
                    confidence = self._calculate_structure_similarity(before_struct, after_struct)
                    
                    if confidence >= self.similarity_threshold:
                        matches.append((before_struct, after_struct, confidence))
                        matched_after.add(after_struct['name'])
                        break
        
        # For unmatched structures, try fuzzy matching
        unmatched_before = [s for s in before if s['name'] not in {b['name'] for b, _, _ in matches}]
        unmatched_after = [s for s in after if s['name'] not in matched_after]
        
        for before_struct in unmatched_before:
            best_match = None
            best_confidence = 0
            
            for after_struct in unmatched_after:
                # Try name similarity and member similarity
                name_sim = difflib.SequenceMatcher(None, 
                                                  before_struct['name'], 
                                                  after_struct['name']).ratio()
                
                if name_sim > 0.7:  # Only consider if names are somewhat similar
                    member_sim = self._calculate_structure_similarity(before_struct, after_struct)
                    confidence = 0.4 * name_sim + 0.6 * member_sim
                    
                    if confidence > best_confidence and confidence >= self.similarity_threshold:
                        best_match = after_struct
                        best_confidence = confidence
            
            if best_match:
                matches.append((before_struct, best_match, best_confidence))
                unmatched_after.remove(best_match)
                
        return matches
    
    def _calculate_structure_similarity(self, struct1: Dict[str, Any], 
                                       struct2: Dict[str, Any]) -> float:
        """Calculate similarity between two data structures.
        
        Args:
            struct1: First structure data
            struct2: Second structure data
            
        Returns:
            Similarity score between 0 and 1
        """
        # Compare based on size and members
        if 'members' not in struct1 or 'members' not in struct2:
            # If no member info, compare basic properties
            if struct1['type'] == struct2['type']:
                # Same type, give some base similarity
                return 0.7
            return 0.3
        
        # Count matching members (by name and type)
        members1 = {m['name']: m['type'] for m in struct1['members']}
        members2 = {m['name']: m['type'] for m in struct2['members']}
        
        # Count exact matches (same name and type)
        exact_matches = sum(1 for name, type_ in members1.items() 
                           if name in members2 and members2[name] == type_)
        
        # Count name matches (same name, different type)
        name_matches = sum(1 for name in members1 
                          if name in members2 and members2[name] != members1[name])
        
        # Total possible matches
        total = max(len(members1), len(members2))
        
        if total == 0:
            return 0.5  # Empty structures are somewhat similar
            
        # Weight exact matches higher than name-only matches
        similarity = (exact_matches + 0.5 * name_matches) / total
        
        # If the types match, boost the similarity
        if struct1['type'] == struct2['type']:
            similarity = min(1.0, similarity + 0.2)
            
        return similarity
    
    def _compare_structures(self, before: Dict[str, Any], 
                           after: Dict[str, Any], 
                           confidence: float) -> StructureDiff:
        """Compare two data structures in detail.
        
        Args:
            before: Structure data from first binary
            after: Structure data from second binary
            confidence: Match confidence score
            
        Returns:
            StructureDiff object with detailed comparison
        """
        # Extract added, removed, and modified members
        added_members = []
        removed_members = []
        modified_members = []
        
        if 'members' in before and 'members' in after:
            before_members = {m['name']: m for m in before['members']}
            after_members = {m['name']: m for m in after['members']}
            
            # Find added and removed members
            added_names = set(after_members.keys()) - set(before_members.keys())
            removed_names = set(before_members.keys()) - set(after_members.keys())
            common_names = set(before_members.keys()) & set(after_members.keys())
            
            added_members = [after_members[name] for name in added_names]
            removed_members = [before_members[name] for name in removed_names]
            
            # Find modified members
            for name in common_names:
                if before_members[name]['type'] != after_members[name]['type']:
                    modified_members.append({
                        'name': name,
                        'type_before': before_members[name]['type'],
                        'type_after': after_members[name]['type'],
                        'offset_before': before_members[name].get('offset', 0),
                        'offset_after': after_members[name].get('offset', 0)
                    })
                elif before_members[name].get('offset', 0) != after_members[name].get('offset', 0):
                    modified_members.append({
                        'name': name,
                        'type': before_members[name]['type'],
                        'offset_before': before_members[name].get('offset', 0),
                        'offset_after': after_members[name].get('offset', 0)
                    })
        
        # Create structure diff
        return StructureDiff(
            name=after['name'],
            type_before=before.get('type', 'unknown'),
            type_after=after.get('type', 'unknown'),
            size_before=before.get('size', 0),
            size_after=after.get('size', 0),
            size_change=after.get('size', 0) - before.get('size', 0),
            added_members=added_members,
            removed_members=removed_members,
            modified_members=modified_members,
            match_confidence=confidence
        )
    
    def _compare_security_issues(self, before: List[Dict[str, Any]], 
                                after: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """Compare security issues between two binaries.
        
        Args:
            before: Security issues from first binary
            after: Security issues from second binary
            
        Returns:
            Tuple of (added_issues, removed_issues)
        """
        # Convert to sets for easy comparison
        # Use tuples of (type, description) as unique identifiers
        before_issues = {(issue['type'], issue.get('description', '')): issue for issue in before}
        after_issues = {(issue['type'], issue.get('description', '')): issue for issue in after}
        
        # Find added and removed issues
        added_keys = set(after_issues.keys()) - set(before_issues.keys())
        removed_keys = set(before_issues.keys()) - set(after_issues.keys())
        
        added_issues = [after_issues[key] for key in added_keys]
        removed_issues = [before_issues[key] for key in removed_keys]
        
        return added_issues, removed_issues
```

`src/comparison/comparator.py`:

```py
"""Binary comparison utilities."""

import difflib
from typing import Dict, List, Set, Tuple, Optional, Any

from src.comparison.models import (
    AnalysisVersion,
    ComparisonResult,
    ChangeType,
    FunctionInfo,
    StructureInfo,
)


class BinaryComparator:
    """Compares two binary analysis versions to find differences."""

    def __init__(self, 
                 name_similarity_threshold: float = 0.85,
                 code_similarity_threshold: float = 0.75):
        """Initialize comparator with similarity thresholds."""
        self.name_similarity_threshold = name_similarity_threshold
        self.code_similarity_threshold = code_similarity_threshold
    
    def compare(
        self, 
        base_version: AnalysisVersion,
        target_version: AnalysisVersion,
    ) -> ComparisonResult:
        """Compare two binary analysis versions."""
        result = ComparisonResult(
            base_version_id=base_version.version_id,
            target_version_id=target_version.version_id,
            base_version_name=base_version.version_name,
            target_version_name=target_version.version_name,
        )
        
        # Compare functions
        function_matches = self._match_functions(
            base_version.functions, 
            target_version.functions
        )
        
        self._analyze_function_changes(
            base_version.functions,
            target_version.functions,
            function_matches,
            result,
        )
        
        # Compare structures
        structure_matches = self._match_structures(
            base_version.structures,
            target_version.structures,
        )
        
        self._analyze_structure_changes(
            base_version.structures,
            target_version.structures,
            structure_matches,
            result,
        )
        
        # Compare call graphs
        self._analyze_call_graph_changes(
            base_version.call_graph,
            target_version.call_graph,
            function_matches,
            result,
        )
        
        # Compare performance metrics
        self._analyze_performance_changes(
            base_version.performance_metrics,
            target_version.performance_metrics,
            function_matches,
            result,
        )
        
        # Calculate overall similarity scores
        self._calculate_similarity_scores(
            base_version,
            target_version,
            function_matches,
            structure_matches,
            result,
        )
        
        return result
    
    def _match_functions(
        self,
        base_functions: Dict[str, FunctionInfo],
        target_functions: Dict[str, FunctionInfo],
    ) -> Dict[str, str]:
        """Match functions between versions based on similarity."""
        matches: Dict[str, str] = {}  # base_id -> target_id
        
        # First, try to match by exact name
        for base_id, base_func in base_functions.items():
            for target_id, target_func in target_functions.items():
                if (base_func.name == target_func.name and 
                    target_id not in matches.values()):
                    matches[base_id] = target_id
                    break
        
        # For remaining functions, try to match by name similarity and signature
        unmatched_base_ids = [
            fid for fid in base_functions.keys() if fid not in matches
        ]
        
        matched_target_ids = set(matches.values())
        unmatched_target_ids = [
            fid for fid in target_functions.keys() 
            if fid not in matched_target_ids
        ]
        
        for base_id in unmatched_base_ids:
            base_func = base_functions[base_id]
            best_match = None
            best_score = 0.0
            
            for target_id in unmatched_target_ids:
                target_func = target_functions[target_id]
                
                # Calculate name similarity
                name_similarity = difflib.SequenceMatcher(
                    None, base_func.name, target_func.name
                ).ratio()
                
                # Calculate code similarity if available
                code_similarity = 0.0
                if (base_func.decompiled_code and 
                    target_func.decompiled_code):
                    code_similarity = difflib.SequenceMatcher(
                        None, 
                        base_func.decompiled_code, 
                        target_func.decompiled_code
                    ).ratio()
                
                # Weight and combine similarity scores
                combined_score = (
                    0.7 * name_similarity + 
                    0.3 * code_similarity
                )
                
                if (combined_score > best_score and 
                    combined_score > self.name_similarity_threshold):
                    best_score = combined_score
                    best_match = target_id
            
            if best_match and best_match not in matches.values():
                matches[base_id] = best_match
                unmatched_target_ids.remove(best_match)
        
        return matches
    
    def _match_structures(
        self,
        base_structures: Dict[str, StructureInfo],
        target_structures: Dict[str, StructureInfo],
    ) -> Dict[str, str]:
        """Match structures between versions based on similarity."""
        matches: Dict[str, str] = {}  # base_id -> target_id
        
        # First, try to match by exact name
        for base_id, base_struct in base_structures.items():
            for target_id, target_struct in target_structures.items():
                if (base_struct.name == target_struct.name and 
                    target_id not in matches.values()):
                    matches[base_id] = target_id
                    break
        
        # For remaining structures, try to match by field similarity
        unmatched_base_ids = [
            sid for sid in base_structures.keys() if sid not in matches
        ]
        
        matched_target_ids = set(matches.values())
        unmatched_target_ids = [
            sid for sid in target_structures.keys() 
            if sid not in matched_target_ids
        ]
        
        for base_id in unmatched_base_ids:
            base_struct = base_structures[base_id]
            best_match = None
            best_score = 0.0
            
            for target_id in unmatched_target_ids:
                target_struct = target_structures[target_id]
                
                # Calculate name similarity
                name_similarity = difflib.SequenceMatcher(
                    None, base_struct.name, target_struct.name
                ).ratio()
                
                # Calculate field similarity
                field_similarity = self._calculate_field_similarity(
                    base_struct, target_struct
                )
                
                # Weight and combine similarity scores
                combined_score = (
                    0.4 * name_similarity + 
                    0.6 * field_similarity
                )
                
                if (combined_score > best_score and 
                    combined_score > self.name_similarity_threshold):
                    best_score = combined_score
                    best_match = target_id
            
            if best_match and best_match not in matches.values():
                matches[base_id] = best_match
                unmatched_target_ids.remove(best_match)
        
        return matches
    
    def _calculate_field_similarity(
        self,
        base_struct: StructureInfo,
        target_struct: StructureInfo,
    ) -> float:
        """Calculate similarity between structure fields."""
        if not base_struct.fields or not target_struct.fields:
            return 0.0
        
        # Compare field names and types
        base_field_names = [f.name for f in base_struct.fields]
        target_field_names = [f.name for f in target_struct.fields]
        
        base_field_types = [f.type_name for f in base_struct.fields]
        target_field_types = [f.type_name for f in target_struct.fields]
        
        # Calculate Jaccard similarity for names and types
        common_names = set(base_field_names) & set(target_field_names)
        name_similarity = len(common_names) / (
            len(set(base_field_names) | set(target_field_names))
        )
        
        common_types = set(base_field_types) & set(target_field_types)
        type_similarity = len(common_types) / (
            len(set(base_field_types) | set(target_field_types))
        )
        
        # Weight and combine similarities
        return 0.7 * name_similarity + 0.3 * type_similarity
    
    def _analyze_function_changes(
        self,
        base_functions: Dict[str, FunctionInfo],
        target_functions: Dict[str, FunctionInfo],
        function_matches: Dict[str, str],
        result: ComparisonResult,
    ) -> None:
        """Analyze changes between functions in two versions."""
        # Find added, removed, modified, and unchanged functions
        for base_id, base_func in base_functions.items():
            if base_id in function_matches:
                target_id = function_matches[base_id]
                target_func = target_functions[target_id]
                
                if self._is_function_modified(base_func, target_func):
                    result.add_function_change(
                        base_id, ChangeType.MODIFIED, target_id
                    )
                elif base_func.name != target_func.name:
                    result.add_function_change(
                        base_id, ChangeType.RENAMED, target_id
                    )
                else:
                    result.add_function_change(
                        base_id, ChangeType.UNCHANGED, target_id
                    )
            else:
                result.add_function_change(base_id, ChangeType.REMOVED)
                
        # Find added functions in target version
        matched_target_ids = set(function_matches.values())
        for target_id, target_func in target_functions.items():
            if target_id not in matched_target_ids:
                result.add_function_change(target_id, ChangeType.ADDED)
    
    def _is_function_modified(
        self,
        base_func: FunctionInfo,
        target_func: FunctionInfo,
    ) -> bool:
        """Check if a function has been modified."""
        # If the size or complexity changed, consider it modified
        if base_func.size != target_func.size:
            return True
            
        if (base_func.complexity is not None and 
            target_func.complexity is not None and
            base_func.complexity != target_func.complexity):
            return True
            
        # If the signature changed, consider it modified
        if base_func.signature != target_func.signature:
            return True
            
        # If parameters changed, consider it modified
        if len(base_func.parameters) != len(target_func.parameters):
            return True
            
        # If the decompiled code is available, compare it
        if (base_func.decompiled_code and target_func.decompiled_code):
            code_similarity = difflib.SequenceMatcher(
                None, 
                base_func.decompiled_code, 
                target_func.decompiled_code
            ).ratio()
            
            if code_similarity < self.code_similarity_threshold:
                return True
                
        return False
    
    def _analyze_structure_changes(
        self,
        base_structures: Dict[str, StructureInfo],
        target_structures: Dict[str, StructureInfo],
        structure_matches: Dict[str, str],
        result: ComparisonResult,
    ) -> None:
        """Analyze changes between structures in two versions."""
        # Find added, removed, modified, and unchanged structures
        for base_id, base_struct in base_structures.items():
            if base_id in structure_matches:
                target_id = structure_matches[base_id]
                target_struct = target_structures[target_id]
                
                if self._is_structure_modified(base_struct, target_struct):
                    result.add_structure_change(
                        base_id, ChangeType.MODIFIED, target_id
                    )
                elif base_struct.name != target_struct.name:
                    result.add_structure_change(
                        base_id, ChangeType.RENAMED, target_id
                    )
                else:
                    result.add_structure_change(
                        base_id, ChangeType.UNCHANGED, target_id
                    )
            else:
                result.add_structure_change(base_id, ChangeType.REMOVED)
                
        # Find added structures in target version
        matched_target_ids = set(structure_matches.values())
        for target_id, target_struct in target_structures.items():
            if target_id not in matched_target_ids:
                result.add_structure_change(target_id, ChangeType.ADDED)
    
    def _is_structure_modified(
        self,
        base_struct: StructureInfo,
        target_struct: StructureInfo,
    ) -> bool:
        """Check if a structure has been modified."""
        # If the size or is_union changed, consider it modified
        if base_struct.size != target_struct.size:
            return True
            
        if base_struct.is_union != target_struct.is_union:
            return True
            
        # If the number of fields changed, consider it modified
        if len(base_struct.fields) != len(target_struct.fields):
            return True
            
        # Check for field differences
        base_fields = {
            (f.name, f.type_name, f.offset, f.size) 
            for f in base_struct.fields
        }
        
        target_fields = {
            (f.name, f.type_name, f.offset, f.size) 
            for f in target_struct.fields
        }
        
        if base_fields != target_fields:
            return True
            
        return False
    
    def _analyze_call_graph_changes(
        self,
        base_call_graph: Dict[str, List[str]],
        target_call_graph: Dict[str, List[str]],
        function_matches: Dict[str, str],
        result: ComparisonResult,
    ) -> None:
        """Analyze changes in call graph between versions."""
        # Create reverse mapping for faster lookup
        target_to_base = {
            target_id: base_id 
            for base_id, target_id in function_matches.items()
        }
        
        # Check for removed calls
        for base_caller_id, base_callees in base_call_graph.items():
            # Skip if the caller function itself was removed
            if base_caller_id not in function_matches:
                continue
                
            target_caller_id = function_matches[base_caller_id]
            target_callees = target_call_graph.get(target_caller_id, [])
            
            # Map base callee IDs to target IDs if they exist
            mapped_target_callees = []
            for base_callee_id in base_callees:
                if base_callee_id in function_matches:
                    mapped_target_callees.append(
                        function_matches[base_callee_id]
                    )
            
            # Find removed calls
            for base_callee_id in base_callees:
                if (base_callee_id in function_matches and
                    function_matches[base_callee_id] not in target_callees):
                    result.add_call_graph_change(
                        base_caller_id,
                        base_callee_id,
                        ChangeType.REMOVED
                    )
        
        # Check for added calls
        for target_caller_id, target_callees in target_call_graph.items():
            # Skip if the caller function is new (was added)
            if target_caller_id not in target_to_base:
                continue
                
            base_caller_id = target_to_base[target_caller_id]
            base_callees = base_call_graph.get(base_caller_id, [])
            
            # Map target callee IDs to base IDs if they exist
            mapped_base_callees = []
            for target_callee_id in target_callees:
                if target_callee_id in target_to_base:
                    mapped_base_callees.append(
                        target_to_base[target_callee_id]
                    )
            
            # Find added calls
            for target_callee_id in target_callees:
                if (target_callee_id in target_to_base and
                    target_to_base[target_callee_id] not in base_callees):
                    result.add_call_graph_change(
                        base_caller_id,
                        target_to_base[target_callee_id],
                        ChangeType.ADDED
                    )
    
    def _analyze_performance_changes(
        self,
        base_metrics: Dict[str, Dict[str, Any]],
        target_metrics: Dict[str, Dict[str, Any]],
        function_matches: Dict[str, str],
        result: ComparisonResult,
    ) -> None:
        """Analyze changes in performance metrics between versions."""
        for base_func_id, base_func_metrics in base_metrics.items():
            # Skip if the function was removed
            if base_func_id not in function_matches:
                continue
                
            target_func_id = function_matches[base_func_id]
            target_func_metrics = target_metrics.get(target_func_id, {})
            
            # Compare metrics
            for metric_name, base_value in base_func_metrics.items():
                if metric_name in target_func_metrics:
                    target_value = target_func_metrics[metric_name]
                    
                    # Skip if not numeric
                    if not (isinstance(base_value, (int, float)) and 
                           isinstance(target_value, (int, float))):
                        continue
                        
                    # Calculate percentage change
                    if base_value != 0:
                        change_pct = (
                            (target_value - base_value) / abs(base_value) * 100
                        )
                    else:
                        # Avoid division by zero
                        change_pct = float('inf') if target_value != 0 else 0.0
                        
                    result.add_metric_change(
                        base_func_id,
                        metric_name,
                        base_value,
                        target_value,
                        change_pct
                    )
    
    def _calculate_similarity_scores(
        self,
        base_version: AnalysisVersion,
        target_version: AnalysisVersion,
        function_matches: Dict[str, str],
        structure_matches: Dict[str, str],
        result: ComparisonResult,
    ) -> None:
        """Calculate various similarity scores between versions."""
        # Function similarity
        if base_version.functions:
            function_similarity = len(function_matches) / len(base_version.functions)
        else:
            function_similarity = 1.0 if not target_version.functions else 0.0
            
        # Structure similarity
        if base_version.structures:
            structure_similarity = len(structure_matches) / len(base_version.structures)
        else:
            structure_similarity = 1.0 if not target_version.structures else 0.0
            
        # Call graph similarity
        call_graph_similarity = self._calculate_call_graph_similarity(
            base_version.call_graph,
            target_version.call_graph,
            function_matches
        )
        
        # Overall similarity (weighted average)
        overall_similarity = (
            0.5 * function_similarity +
            0.3 * structure_similarity +
            0.2 * call_graph_similarity
        )
        
        result.set_similarity_scores(
            overall=overall_similarity,
            function=function_similarity,
            structure=structure_similarity,
            call_graph=call_graph_similarity
        )
    
    def _calculate_call_graph_similarity(
        self,
        base_call_graph: Dict[str, List[str]],
        target_call_graph: Dict[str, List[str]],
        function_matches: Dict[str, str],
    ) -> float:
        """Calculate similarity between call graphs."""
        if not base_call_graph:
            return 1.0 if not target_call_graph else 0.0
        
        # Count preserved edges
        preserved_edges = 0
        total_base_edges = 0
        
        for base_caller_id, base_callees in base_call_graph.items():
            total_base_edges += len(base_callees)
            
            # Skip if the caller was removed
            if base_caller_id not in function_matches:
                continue
                
            target_caller_id = function_matches[base_caller_id]
            target_callees = target_call_graph.get(target_caller_id, [])
            
            for base_callee_id in base_callees:
                # Skip if the callee was removed
                if base_callee_id not in function_matches:
                    continue
                    
                target_callee_id = function_matches[base_callee_id]
                
                if target_callee_id in target_callees:
                    preserved_edges += 1
        
        if total_base_edges == 0:
            return 1.0
            
        return preserved_edges / total_base_edges
```

`src/comparison/models.py`:

```py
"""Models for binary analysis comparisons."""

import os
import json
import uuid
from datetime import datetime
from enum import Enum, auto
from typing import Dict, List, Optional, Set, Any, Union, Tuple


class ChangeType(Enum):
    """Types of changes between binary versions."""

    ADDED = auto()
    REMOVED = auto()
    MODIFIED = auto()
    RENAMED = auto()
    UNCHANGED = auto()


class AnalysisProject:
    """Represents a binary analysis project with versioning capabilities."""

    def __init__(
        self,
        name: str,
        description: str = "",
        binary_path: Optional[str] = None,
        project_id: Optional[str] = None,
    ):
        """Initialize a project."""
        self.project_id = project_id or str(uuid.uuid4())
        self.name = name
        self.description = description
        self.binary_path = binary_path
        self.created_at = datetime.utcnow()
        self.updated_at = self.created_at
        self.versions: Dict[str, "AnalysisVersion"] = {}
        
    def add_version(
        self,
        version_name: str,
        binary_path: str,
        description: str = "",
        metadata: Optional[Dict[str, Any]] = None,
    ) -> "AnalysisVersion":
        """Add a new version to the project."""
        if version_name in self.versions:
            raise ValueError(f"Version '{version_name}' already exists")

        version = AnalysisVersion(
            project_id=self.project_id,
            version_name=version_name,
            binary_path=binary_path,
            description=description,
            metadata=metadata or {},
        )
        
        self.versions[version_name] = version
        self.updated_at = datetime.utcnow()
        return version
    
    def get_version(self, version_name: str) -> "AnalysisVersion":
        """Get a specific version by name."""
        if version_name not in self.versions:
            raise ValueError(f"Version '{version_name}' does not exist")
        
        return self.versions[version_name]
    
    def list_versions(self) -> List["AnalysisVersion"]:
        """List all versions in the project, sorted by creation date."""
        return sorted(
            self.versions.values(),
            key=lambda v: v.created_at
        )
    
    def compare_versions(
        self,
        base_version: str,
        target_version: str,
    ) -> "ComparisonResult":
        """Compare two versions and generate a comparison result."""
        if base_version not in self.versions:
            raise ValueError(f"Base version '{base_version}' does not exist")
            
        if target_version not in self.versions:
            raise ValueError(f"Target version '{target_version}' does not exist")
            
        base = self.versions[base_version]
        target = self.versions[target_version]
        
        from src.comparison.comparator import BinaryComparator
        comparator = BinaryComparator()
        return comparator.compare(base, target)
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            "project_id": self.project_id,
            "name": self.name,
            "description": self.description,
            "binary_path": self.binary_path,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat(),
            "versions": {
                name: version.to_dict()
                for name, version in self.versions.items()
            },
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "AnalysisProject":
        """Create from dictionary representation."""
        project = cls(
            name=data["name"],
            description=data["description"],
            binary_path=data.get("binary_path"),
            project_id=data["project_id"],
        )
        
        project.created_at = datetime.fromisoformat(data["created_at"])
        project.updated_at = datetime.fromisoformat(data["updated_at"])
        
        # Recreate versions
        for version_name, version_data in data.get("versions", {}).items():
            version = AnalysisVersion.from_dict(version_data)
            project.versions[version_name] = version
            
        return project
    
    def save(self, output_dir: str) -> str:
        """Save project to a JSON file."""
        os.makedirs(output_dir, exist_ok=True)
        file_path = os.path.join(output_dir, f"{self.project_id}.json")
        
        with open(file_path, 'w') as f:
            json.dump(self.to_dict(), f, indent=2)
            
        return file_path
    
    @classmethod
    def load(cls, file_path: str) -> "AnalysisProject":
        """Load project from a JSON file."""
        with open(file_path, 'r') as f:
            data = json.load(f)
            
        return cls.from_dict(data)


class AnalysisVersion:
    """Represents a single version of a binary analysis."""

    def __init__(
        self,
        project_id: str,
        version_name: str,
        binary_path: str,
        description: str = "",
        metadata: Optional[Dict[str, Any]] = None,
        version_id: Optional[str] = None,
    ):
        """Initialize a version."""
        self.version_id = version_id or str(uuid.uuid4())
        self.project_id = project_id
        self.version_name = version_name
        self.binary_path = binary_path
        self.description = description
        self.metadata = metadata or {}
        self.created_at = datetime.utcnow()
        
        # Analysis results
        self.functions: Dict[str, "FunctionInfo"] = {}
        self.structures: Dict[str, "StructureInfo"] = {}
        self.call_graph: Dict[str, List[str]] = {}  # function_id -> [called_function_ids]
        self.performance_metrics: Dict[str, Dict[str, Any]] = {}
        
    def add_function(self, function: "FunctionInfo") -> None:
        """Add or update a function in the analysis."""
        self.functions[function.function_id] = function
        
    def add_structure(self, structure: "StructureInfo") -> None:
        """Add or update a structure in the analysis."""
        self.structures[structure.structure_id] = structure
        
    def add_call(self, caller_id: str, callee_id: str) -> None:
        """Add a function call to the call graph."""
        if caller_id not in self.call_graph:
            self.call_graph[caller_id] = []
            
        if callee_id not in self.call_graph[caller_id]:
            self.call_graph[caller_id].append(callee_id)
            
    def set_performance_metrics(
        self, 
        function_id: str, 
        metrics: Dict[str, Any]
    ) -> None:
        """Set performance metrics for a function."""
        self.performance_metrics[function_id] = metrics
        
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            "version_id": self.version_id,
            "project_id": self.project_id,
            "version_name": self.version_name,
            "binary_path": self.binary_path,
            "description": self.description,
            "metadata": self.metadata,
            "created_at": self.created_at.isoformat(),
            "functions": {
                fid: f.to_dict() for fid, f in self.functions.items()
            },
            "structures": {
                sid: s.to_dict() for sid, s in self.structures.items()
            },
            "call_graph": self.call_graph,
            "performance_metrics": self.performance_metrics,
        }
        
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "AnalysisVersion":
        """Create from dictionary representation."""
        version = cls(
            project_id=data["project_id"],
            version_name=data["version_name"],
            binary_path=data["binary_path"],
            description=data["description"],
            metadata=data.get("metadata", {}),
            version_id=data["version_id"],
        )
        
        version.created_at = datetime.fromisoformat(data["created_at"])
        
        # Recreate functions
        for fid, func_data in data.get("functions", {}).items():
            function = FunctionInfo.from_dict(func_data)
            version.functions[fid] = function
            
        # Recreate structures
        for sid, struct_data in data.get("structures", {}).items():
            structure = StructureInfo.from_dict(struct_data)
            version.structures[sid] = structure
            
        # Copy call graph and performance metrics
        version.call_graph = data.get("call_graph", {})
        version.performance_metrics = data.get("performance_metrics", {})
        
        return version


class FunctionInfo:
    """Information about a function in a binary."""

    def __init__(
        self,
        name: str,
        address: int,
        size: int,
        signature: Optional[str] = None,
        function_id: Optional[str] = None,
        complexity: Optional[int] = None,
        decompiled_code: Optional[str] = None,
        summary: Optional[str] = None,
        tags: Optional[List[str]] = None,
    ):
        """Initialize function information."""
        self.function_id = function_id or str(uuid.uuid4())
        self.name = name
        self.address = address
        self.size = size
        self.signature = signature
        self.complexity = complexity
        self.decompiled_code = decompiled_code
        self.summary = summary
        self.tags = tags or []
        self.parameters: List["ParameterInfo"] = []
        self.local_vars: List["VariableInfo"] = []
        
    def add_parameter(self, param: "ParameterInfo") -> None:
        """Add a parameter to the function."""
        self.parameters.append(param)
        
    def add_local_var(self, var: "VariableInfo") -> None:
        """Add a local variable to the function."""
        self.local_vars.append(var)
        
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            "function_id": self.function_id,
            "name": self.name,
            "address": self.address,
            "size": self.size,
            "signature": self.signature,
            "complexity": self.complexity,
            "decompiled_code": self.decompiled_code,
            "summary": self.summary,
            "tags": self.tags,
            "parameters": [p.to_dict() for p in self.parameters],
            "local_vars": [v.to_dict() for v in self.local_vars],
        }
        
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "FunctionInfo":
        """Create from dictionary representation."""
        function = cls(
            name=data["name"],
            address=data["address"],
            size=data["size"],
            signature=data.get("signature"),
            function_id=data["function_id"],
            complexity=data.get("complexity"),
            decompiled_code=data.get("decompiled_code"),
            summary=data.get("summary"),
            tags=data.get("tags", []),
        )
        
        # Recreate parameters and local variables
        for param_data in data.get("parameters", []):
            param = ParameterInfo.from_dict(param_data)
            function.add_parameter(param)
            
        for var_data in data.get("local_vars", []):
            var = VariableInfo.from_dict(var_data)
            function.add_local_var(var)
            
        return function


class ParameterInfo:
    """Information about a function parameter."""

    def __init__(
        self,
        name: str,
        type_name: str,
        position: int,
        size: int,
    ):
        """Initialize parameter information."""
        self.name = name
        self.type_name = type_name
        self.position = position
        self.size = size
        
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            "name": self.name,
            "type_name": self.type_name,
            "position": self.position,
            "size": self.size,
        }
        
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ParameterInfo":
        """Create from dictionary representation."""
        return cls(
            name=data["name"],
            type_name=data["type_name"],
            position=data["position"],
            size=data["size"],
        )


class VariableInfo:
    """Information about a local variable."""

    def __init__(
        self,
        name: str,
        type_name: str,
        size: int,
        is_stack: bool = True,
    ):
        """Initialize variable information."""
        self.name = name
        self.type_name = type_name
        self.size = size
        self.is_stack = is_stack
        
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            "name": self.name,
            "type_name": self.type_name,
            "size": self.size,
            "is_stack": self.is_stack,
        }
        
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "VariableInfo":
        """Create from dictionary representation."""
        return cls(
            name=data["name"],
            type_name=data["type_name"],
            size=data["size"],
            is_stack=data["is_stack"],
        )


class StructureInfo:
    """Information about a data structure in a binary."""

    def __init__(
        self,
        name: str,
        size: int,
        structure_id: Optional[str] = None,
        is_union: bool = False,
    ):
        """Initialize structure information."""
        self.structure_id = structure_id or str(uuid.uuid4())
        self.name = name
        self.size = size
        self.is_union = is_union
        self.fields: List["StructureField"] = []
        
    def add_field(self, field: "StructureField") -> None:
        """Add a field to the structure."""
        self.fields.append(field)
        
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            "structure_id": self.structure_id,
            "name": self.name,
            "size": self.size,
            "is_union": self.is_union,
            "fields": [f.to_dict() for f in self.fields],
        }
        
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "StructureInfo":
        """Create from dictionary representation."""
        structure = cls(
            name=data["name"],
            size=data["size"],
            structure_id=data["structure_id"],
            is_union=data["is_union"],
        )
        
        # Recreate fields
        for field_data in data.get("fields", []):
            field = StructureField.from_dict(field_data)
            structure.add_field(field)
            
        return structure


class StructureField:
    """Information about a field in a data structure."""

    def __init__(
        self,
        name: str,
        type_name: str,
        offset: int,
        size: int,
    ):
        """Initialize field information."""
        self.name = name
        self.type_name = type_name
        self.offset = offset
        self.size = size
        
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            "name": self.name,
            "type_name": self.type_name,
            "offset": self.offset,
            "size": self.size,
        }
        
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "StructureField":
        """Create from dictionary representation."""
        return cls(
            name=data["name"],
            type_name=data["type_name"],
            offset=data["offset"],
            size=data["size"],
        )


class ComparisonResult:
    """Results of comparing two binary analysis versions."""

    def __init__(
        self,
        base_version_id: str,
        target_version_id: str,
        base_version_name: str,
        target_version_name: str,
    ):
        """Initialize comparison result."""
        self.base_version_id = base_version_id
        self.target_version_id = target_version_id
        self.base_version_name = base_version_name
        self.target_version_name = target_version_name
        self.created_at = datetime.utcnow()
        
        # Changes between versions
        self.function_changes: Dict[str, Tuple[ChangeType, Optional[str]]] = {}
        self.structure_changes: Dict[str, Tuple[ChangeType, Optional[str]]] = {}
        self.call_graph_changes: List[Dict[str, Any]] = []
        self.metric_changes: Dict[str, Dict[str, Any]] = {}
        
        # Similarity scores (0.0 to 1.0)
        self.overall_similarity = 0.0
        self.function_similarity = 0.0
        self.structure_similarity = 0.0
        self.call_graph_similarity = 0.0
        
    def add_function_change(
        self,
        function_id: str,
        change_type: ChangeType,
        corresponding_id: Optional[str] = None,
    ) -> None:
        """Add a function change to the result."""
        self.function_changes[function_id] = (change_type, corresponding_id)
        
    def add_structure_change(
        self,
        structure_id: str,
        change_type: ChangeType,
        corresponding_id: Optional[str] = None,
    ) -> None:
        """Add a structure change to the result."""
        self.structure_changes[structure_id] = (change_type, corresponding_id)
        
    def add_call_graph_change(
        self,
        caller_id: str,
        callee_id: str,
        change_type: ChangeType,
    ) -> None:
        """Add a call graph change to the result."""
        self.call_graph_changes.append({
            "caller_id": caller_id,
            "callee_id": callee_id,
            "change_type": change_type.name,
        })
        
    def add_metric_change(
        self,
        function_id: str,
        metric_name: str,
        base_value: Any,
        target_value: Any,
        change_percentage: float,
    ) -> None:
        """Add a performance metric change to the result."""
        if function_id not in self.metric_changes:
            self.metric_changes[function_id] = {}
            
        self.metric_changes[function_id][metric_name] = {
            "base_value": base_value,
            "target_value": target_value,
            "change_percentage": change_percentage,
        }
        
    def set_similarity_scores(
        self,
        overall: float,
        function: float,
        structure: float,
        call_graph: float,
    ) -> None:
        """Set similarity scores for the comparison."""
        self.overall_similarity = overall
        self.function_similarity = function
        self.structure_similarity = structure
        self.call_graph_similarity = call_graph
        
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        # Convert ChangeType enum to string for serialization
        function_changes = {
            f_id: (ct.name, corr_id)
            for f_id, (ct, corr_id) in self.function_changes.items()
        }
        
        structure_changes = {
            s_id: (ct.name, corr_id)
            for s_id, (ct, corr_id) in self.structure_changes.items()
        }
        
        return {
            "base_version_id": self.base_version_id,
            "target_version_id": self.target_version_id,
            "base_version_name": self.base_version_name,
            "target_version_name": self.target_version_name,
            "created_at": self.created_at.isoformat(),
            "function_changes": function_changes,
            "structure_changes": structure_changes,
            "call_graph_changes": self.call_graph_changes,
            "metric_changes": self.metric_changes,
            "overall_similarity": self.overall_similarity,
            "function_similarity": self.function_similarity,
            "structure_similarity": self.structure_similarity,
            "call_graph_similarity": self.call_graph_similarity,
        }
        
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "ComparisonResult":
        """Create from dictionary representation."""
        result = cls(
            base_version_id=data["base_version_id"],
            target_version_id=data["target_version_id"],
            base_version_name=data["base_version_name"],
            target_version_name=data["target_version_name"],
        )
        
        result.created_at = datetime.fromisoformat(data["created_at"])
        
        # Convert string back to ChangeType enum
        for f_id, (ct_str, corr_id) in data.get("function_changes", {}).items():
            result.function_changes[f_id] = (ChangeType[ct_str], corr_id)
            
        for s_id, (ct_str, corr_id) in data.get("structure_changes", {}).items():
            result.structure_changes[s_id] = (ChangeType[ct_str], corr_id)
            
        result.call_graph_changes = data.get("call_graph_changes", [])
        result.metric_changes = data.get("metric_changes", {})
        
        # Set similarity scores
        result.overall_similarity = data.get("overall_similarity", 0.0)
        result.function_similarity = data.get("function_similarity", 0.0)
        result.structure_similarity = data.get("structure_similarity", 0.0)
        result.call_graph_similarity = data.get("call_graph_similarity", 0.0)
        
        return result
```

`src/comparison/routes.py`:

```py
"""API routes for project comparison."""

from flask import Blueprint, jsonify, request, send_file, g
import os
from datetime import datetime
import json

from src.auth.middleware import login_required
from src.comparison.store import ComparisonStore
from src.comparison.models import (
    AnalysisProject, 
    AnalysisVersion,
    ComparisonResult, 
    FunctionInfo,
    StructureInfo,
    ParameterInfo,
    VariableInfo,
    StructureField,
    ChangeType
)
from src.comparison.comparator import BinaryComparator

# Create blueprint
comparison_bp = Blueprint('comparison', __name__)

# Initialize the comparison store
STORAGE_DIR = os.environ.get('RE_ARCHITECT_DATA_DIR', os.path.expanduser('~/.re-architect/data'))
store = ComparisonStore(os.path.join(STORAGE_DIR, 'comparisons'))

@comparison_bp.route('/projects', methods=['GET'])
@login_required
def list_projects():
    """List all saved analysis projects."""
    projects = store.list_projects()
    return jsonify(projects)

@comparison_bp.route('/project/<project_id>', methods=['GET'])
@login_required
def get_project(project_id):
    """Get a specific analysis project."""
    project = store.get_project(project_id)
    if not project:
        return jsonify({"error": f"Project {project_id} not found"}), 404
    
    # Convert to dictionary
    project_dict = {
        "id": project.project_id,
        "name": project.name,
        "binary_path": project.binary_path,
        "timestamp": project.timestamp.isoformat() if project.timestamp else None,
        "version": getattr(project, 'version', '1.0'),
        "description": project.description,
        "tags": getattr(project, 'tags', [])
    }
    
    # Include analysis_data only if explicitly requested
    if request.args.get('include_analysis') == 'true':
        project_dict["analysis_data"] = project.analysis_data
    
    return jsonify(project_dict)

@comparison_bp.route('/project/<project_id>/functions', methods=['GET'])
@login_required
def get_project_functions(project_id):
    """Get all functions from a specific project."""
    project = store.get_project(project_id)
    if not project:
        return jsonify({"error": f"Project {project_id} not found"}), 404
    
    functions = project.analysis_data.get('functions', [])
    
    # Apply optional filtering
    name_filter = request.args.get('name')
    if name_filter:
        functions = [f for f in functions if name_filter.lower() in f.get('name', '').lower()]
    
    # Apply optional sorting
    sort_by = request.args.get('sort')
    if sort_by == 'name':
        functions = sorted(functions, key=lambda f: f.get('name', ''))
    elif sort_by == 'size':
        functions = sorted(functions, key=lambda f: f.get('size', 0), reverse=True)
    elif sort_by == 'complexity':
        functions = sorted(functions, key=lambda f: f.get('complexity', 0), reverse=True)
    
    # Apply optional pagination
    try:
        page = int(request.args.get('page', 1))
        page_size = int(request.args.get('page_size', 50))
        start = (page - 1) * page_size
        end = start + page_size
        paginated_functions = functions[start:end]
    except ValueError:
        paginated_functions = functions
    
    return jsonify({
        "project_id": project_id,
        "total_count": len(functions),
        "functions": paginated_functions
    })

@comparison_bp.route('/project/<project_id>/structures', methods=['GET'])
@login_required
def get_project_structures(project_id):
    """Get all data structures from a specific project."""
    project = store.get_project(project_id)
    if not project:
        return jsonify({"error": f"Project {project_id} not found"}), 404
    
    structures = project.analysis_data.get('structures', [])
    
    # Apply optional filtering
    name_filter = request.args.get('name')
    if name_filter:
        structures = [s for s in structures if name_filter.lower() in s.get('name', '').lower()]
    
    # Apply optional sorting
    sort_by = request.args.get('sort')
    if sort_by == 'name':
        structures = sorted(structures, key=lambda s: s.get('name', ''))
    elif sort_by == 'size':
        structures = sorted(structures, key=lambda s: s.get('size', 0), reverse=True)
    
    # Apply optional pagination
    try:
        page = int(request.args.get('page', 1))
        page_size = int(request.args.get('page_size', 50))
        start = (page - 1) * page_size
        end = start + page_size
        paginated_structures = structures[start:end]
    except ValueError:
        paginated_structures = structures
    
    return jsonify({
        "project_id": project_id,
        "total_count": len(structures),
        "structures": paginated_structures
    })

@comparison_bp.route('/project', methods=['POST'])
@login_required
def create_project():
    """Create a new analysis project."""
    data = request.json
    
    # Check required fields
    if not data.get('name') or not data.get('binary_path'):
        return jsonify({"error": "Name and binary_path are required"}), 400
    
    # Create project
    project = AnalysisProject(
        project_id=data.get('id'),
        name=data['name'],
        description=data.get('description', ''),
        binary_path=data['binary_path']
    )
    
    # Add optional fields
    if 'timestamp' in data:
        try:
            project.timestamp = datetime.fromisoformat(data['timestamp'])
        except ValueError:
            pass
    if 'version' in data:
        project.version = data['version']
    if 'description' in data:
        project.description = data['description']
    if 'tags' in data:
        project.tags = data['tags']
    
    # Save project
    project_id = store.save_project(project)
    
    return jsonify({
        "id": project_id,
        "message": "Project created successfully"
    })

@comparison_bp.route('/project/<project_id>', methods=['DELETE'])
@login_required
def delete_project(project_id):
    """Delete an analysis project."""
    success = store.delete_project(project_id)
    if not success:
        return jsonify({"error": f"Project {project_id} not found"}), 404
    
    return jsonify({"message": f"Project {project_id} deleted successfully"})

@comparison_bp.route('/comparisons', methods=['GET'])
@login_required
def list_comparisons():
    """List all saved comparisons."""
    comparisons = store.list_comparisons()
    return jsonify(comparisons)

@comparison_bp.route('/comparison/<comparison_id>', methods=['GET'])
@login_required
def get_comparison(comparison_id):
    """Get a specific comparison."""
    comparison = store.get_comparison(comparison_id)
    if not comparison:
        return jsonify({"error": f"Comparison {comparison_id} not found"}), 404
    
    # Convert to dictionary
    comparison_dict = {
        "id": comparison.id,
        "name": comparison.name,
        "timestamp": comparison.timestamp.isoformat() if comparison.timestamp else None,
        "project1_id": comparison.base_version_id,
        "project2_id": comparison.target_version_id,
        "description": comparison.description,
        "tags": comparison.tags,
    }
    
    # Include result_data only if explicitly requested
    if request.args.get('include_results') == 'true':
        comparison_dict["result_data"] = comparison.result_data
    
    return jsonify(comparison_dict)

@comparison_bp.route('/comparison/<comparison_id>/functions', methods=['GET'])
@login_required
def get_comparison_functions(comparison_id):
    """Get function changes from a specific comparison."""
    comparison = store.get_comparison(comparison_id)
    if not comparison:
        return jsonify({"error": f"Comparison {comparison_id} not found"}), 404
    
    # Get function changes from result data
    function_changes = []
    if comparison.function_changes:
        # Convert ChangeType enum to string for serialization
        for func_id, (change_type, corresponding_id) in comparison.function_changes.items():
            change = {
                "function_id": func_id,
                "change_type": change_type.name,
                "corresponding_id": corresponding_id,
            }
            
            # Get function details from project1 or project2
            if change_type in [ChangeType.REMOVED, ChangeType.MODIFIED, ChangeType.RENAMED, ChangeType.UNCHANGED]:
                # Function exists in project1
                project1 = store.get_project(comparison.project1_id)
                if project1:
                    for func in project1.analysis_data.get('functions', []):
                        if func.get('id') == func_id:
                            change["function_details"] = func
                            break
            
            if change_type in [ChangeType.ADDED, ChangeType.MODIFIED, ChangeType.RENAMED, ChangeType.UNCHANGED]:
                # Function exists in project2
                if corresponding_id:
                    project2 = store.get_project(comparison.project2_id)
                    if project2:
                        for func in project2.analysis_data.get('functions', []):
                            if func.get('id') == corresponding_id:
                                change["corresponding_function_details"] = func
                                break
            
            function_changes.append(change)
    
    # Apply optional filtering
    change_type_filter = request.args.get('change_type')
    if change_type_filter:
        try:
            change_type = ChangeType[change_type_filter.upper()]
            function_changes = [c for c in function_changes 
                                if c['change_type'] == change_type.name]
        except KeyError:
            pass
    
    # Apply optional name filtering
    name_filter = request.args.get('name')
    if name_filter:
        function_changes = [c for c in function_changes 
                            if ('function_details' in c and 
                                name_filter.lower() in c['function_details'].get('name', '').lower())]
    
    # Apply optional sorting
    sort_by = request.args.get('sort')
    if sort_by == 'name':
        function_changes = sorted(
            function_changes, 
            key=lambda c: c.get('function_details', {}).get('name', '') 
                if 'function_details' in c 
                else c.get('corresponding_function_details', {}).get('name', '')
        )
    elif sort_by == 'change_type':
        function_changes = sorted(function_changes, key=lambda c: c['change_type'])
    
    # Apply optional pagination
    try:
        page = int(request.args.get('page', 1))
        page_size = int(request.args.get('page_size', 50))
        start = (page - 1) * page_size
        end = start + page_size
        paginated_changes = function_changes[start:end]
    except ValueError:
        paginated_changes = function_changes
    
    return jsonify({
        "comparison_id": comparison_id,
        "total_count": len(function_changes),
        "function_changes": paginated_changes
    })

@comparison_bp.route('/comparison/<comparison_id>/structures', methods=['GET'])
@login_required
def get_comparison_structures(comparison_id):
    """Get structure changes from a specific comparison."""
    comparison = store.get_comparison(comparison_id)
    if not comparison:
        return jsonify({"error": f"Comparison {comparison_id} not found"}), 404
    
    # Get structure changes from result data
    structure_changes = []
    if comparison.structure_changes:
        # Convert ChangeType enum to string for serialization
        for struct_id, (change_type, corresponding_id) in comparison.structure_changes.items():
            change = {
                "structure_id": struct_id,
                "change_type": change_type.name,
                "corresponding_id": corresponding_id,
            }
            
            # Get structure details from project1 or project2
            if change_type in [ChangeType.REMOVED, ChangeType.MODIFIED, ChangeType.RENAMED, ChangeType.UNCHANGED]:
                # Structure exists in project1
                project1 = store.get_project(comparison.project1_id)
                if project1:
                    for struct in project1.analysis_data.get('structures', []):
                        if struct.get('id') == struct_id:
                            change["structure_details"] = struct
                            break
            
            if change_type in [ChangeType.ADDED, ChangeType.MODIFIED, ChangeType.RENAMED, ChangeType.UNCHANGED]:
                # Structure exists in project2
                if corresponding_id:
                    project2 = store.get_project(comparison.project2_id)
                    if project2:
                        for struct in project2.analysis_data.get('structures', []):
                            if struct.get('id') == corresponding_id:
                                change["corresponding_structure_details"] = struct
                                break
            
            structure_changes.append(change)
    
    # Apply optional filtering
    change_type_filter = request.args.get('change_type')
    if change_type_filter:
        try:
            change_type = ChangeType[change_type_filter.upper()]
            structure_changes = [c for c in structure_changes 
                                 if c['change_type'] == change_type.name]
        except KeyError:
            pass
    
    # Apply optional name filtering
    name_filter = request.args.get('name')
    if name_filter:
        structure_changes = [c for c in structure_changes 
                             if ('structure_details' in c and 
                                 name_filter.lower() in c['structure_details'].get('name', '').lower())]
    
    # Apply optional sorting
    sort_by = request.args.get('sort')
    if sort_by == 'name':
        structure_changes = sorted(
            structure_changes, 
            key=lambda c: c.get('structure_details', {}).get('name', '') 
                if 'structure_details' in c 
                else c.get('corresponding_structure_details', {}).get('name', '')
        )
    elif sort_by == 'change_type':
        structure_changes = sorted(structure_changes, key=lambda c: c['change_type'])
    
    # Apply optional pagination
    try:
        page = int(request.args.get('page', 1))
        page_size = int(request.args.get('page_size', 50))
        start = (page - 1) * page_size
        end = start + page_size
        paginated_changes = structure_changes[start:end]
    except ValueError:
        paginated_changes = structure_changes
    
    return jsonify({
        "comparison_id": comparison_id,
        "total_count": len(structure_changes),
        "structure_changes": paginated_changes
    })

@comparison_bp.route('/comparison/<comparison_id>/metrics', methods=['GET'])
@login_required
def get_comparison_metrics(comparison_id):
    """Get performance metric changes from a specific comparison."""
    comparison = store.get_comparison(comparison_id)
    if not comparison:
        return jsonify({"error": f"Comparison {comparison_id} not found"}), 404
    
    # Get metric changes
    metric_changes = []
    if comparison.metric_changes:
        for func_id, metrics in comparison.metric_changes.items():
            func_name = "Unknown"
            
            # Try to get function name from project1
            project1 = store.get_project(comparison.project1_id)
            if project1:
                for func in project1.analysis_data.get('functions', []):
                    if func.get('id') == func_id:
                        func_name = func.get('name', 'Unknown')
                        break
            
            for metric_name, values in metrics.items():
                change = {
                    "function_id": func_id,
                    "function_name": func_name,
                    "metric_name": metric_name,
                    "base_value": values["base_value"],
                    "target_value": values["target_value"],
                    "change_percentage": values["change_percentage"]
                }
                metric_changes.append(change)
    
    # Apply optional function filtering
    func_filter = request.args.get('function')
    if func_filter:
        metric_changes = [c for c in metric_changes 
                          if func_filter.lower() in c['function_name'].lower()]
    
    # Apply optional metric filtering
    metric_filter = request.args.get('metric')
    if metric_filter:
        metric_changes = [c for c in metric_changes 
                         if metric_filter.lower() == c['metric_name'].lower()]
    
    # Apply optional sorting
    sort_by = request.args.get('sort')
    if sort_by == 'function_name':
        metric_changes = sorted(metric_changes, key=lambda c: c['function_name'])
    elif sort_by == 'metric_name':
        metric_changes = sorted(metric_changes, key=lambda c: c['metric_name'])
    elif sort_by == 'change':
        metric_changes = sorted(metric_changes, 
                              key=lambda c: abs(c['change_percentage']), 
                              reverse=True)
    
    # Apply optional pagination
    try:
        page = int(request.args.get('page', 1))
        page_size = int(request.args.get('page_size', 50))
        start = (page - 1) * page_size
        end = start + page_size
        paginated_changes = metric_changes[start:end]
    except ValueError:
        paginated_changes = metric_changes
    
    return jsonify({
        "comparison_id": comparison_id,
        "total_count": len(metric_changes),
        "metric_changes": paginated_changes
    })

@comparison_bp.route('/comparison/<comparison_id>/function/<function_id>', methods=['GET'])
@login_required
def get_comparison_function_detail(comparison_id, function_id):
    """Get detailed comparison of a specific function."""
    comparison = store.get_comparison(comparison_id)
    if not comparison:
        return jsonify({"error": f"Comparison {comparison_id} not found"}), 404
    
    # Find the function change entry
    corresponding_id = None
    change_type = None
    
    if comparison.function_changes:
        if function_id in comparison.function_changes:
            change_type, corresponding_id = comparison.function_changes[function_id]
    
    if not change_type:
        return jsonify({"error": f"Function {function_id} not found in comparison"}), 404
    
    # Get function details
    base_function = None
    target_function = None
    
    # Get base function from project1
    if change_type in [ChangeType.REMOVED, ChangeType.MODIFIED, ChangeType.RENAMED, ChangeType.UNCHANGED]:
        project1 = store.get_project(comparison.project1_id)
        if project1:
            for func in project1.analysis_data.get('functions', []):
                if func.get('id') == function_id:
                    base_function = func
                    break
    
    # Get target function from project2
    if change_type in [ChangeType.ADDED, ChangeType.MODIFIED, ChangeType.RENAMED, ChangeType.UNCHANGED]:
        if corresponding_id:
            project2 = store.get_project(comparison.project2_id)
            if project2:
                for func in project2.analysis_data.get('functions', []):
                    if func.get('id') == corresponding_id:
                        target_function = func
                        break
    
    # Get call graph changes
    call_changes = []
    for change in comparison.call_graph_changes:
        if change["caller_id"] == function_id:
            call_changes.append(change)
    
    # Get performance metric changes
    metric_changes = {}
    if function_id in comparison.metric_changes:
        metric_changes = comparison.metric_changes[function_id]
    
    return jsonify({
        "comparison_id": comparison_id,
        "function_id": function_id,
        "corresponding_id": corresponding_id,
        "change_type": change_type.name if change_type else None,
        "base_function": base_function,
        "target_function": target_function,
        "call_changes": call_changes,
        "metric_changes": metric_changes
    })

@comparison_bp.route('/compare', methods=['POST'])
@login_required
def create_comparison():
    """Create a new comparison between two projects."""
    data = request.json
    
    # Check required fields
    if not data.get('project1_id') or not data.get('project2_id'):
        return jsonify({"error": "Both project1_id and project2_id are required"}), 400
    
    # Get projects
    project1 = store.get_project(data['project1_id'])
    project2 = store.get_project(data['project2_id'])
    
    if not project1:
        return jsonify({"error": f"Project {data['project1_id']} not found"}), 404
    if not project2:
        return jsonify({"error": f"Project {data['project2_id']} not found"}), 404
    
    try:
        # Create analysis versions from the projects
        base_version = AnalysisVersion(
            project_id=project1.id,
            version_name=project1.version,
            binary_path=project1.binary_path,
            description=project1.description,
            metadata={"timestamp": project1.timestamp.isoformat() if project1.timestamp else ""},
        )
        
        target_version = AnalysisVersion(
            project_id=project2.id,
            version_name=project2.version,
            binary_path=project2.binary_path,
            description=project2.description,
            metadata={"timestamp": project2.timestamp.isoformat() if project2.timestamp else ""},
        )
        
        # Convert project analysis_data to functions and structures
        _convert_project_to_version(project1.analysis_data, base_version)
        _convert_project_to_version(project2.analysis_data, target_version)
        
        # Initialize comparator
        name_threshold = data.get('name_similarity_threshold', 0.85)
        code_threshold = data.get('code_similarity_threshold', 0.75)
        comparator = BinaryComparator(
            name_similarity_threshold=name_threshold,
            code_similarity_threshold=code_threshold
        )
        
        # Perform comparison
        result = comparator.compare(base_version, target_version)
        
        # Convert to serializable format
        result_data = result.to_dict()
        
        # Create comparison result
        comparison = ComparisonResult(
            base_version_id=base_version.version_id,
            target_version_id=target_version.version_id,
            base_version_name=project1.name,
            target_version_name=project2.name,
        )
        
        # Set similarity scores
        comparison.set_similarity_scores(
            result.overall_similarity,
            result.function_similarity,
            result.structure_similarity,
            result.call_graph_similarity
        )
        
        # Add function changes
        for func_id, (change_type, target_id) in result.function_changes.items():
            # Only add if it's in base_version (target-only functions are handled separately)
            if func_id in base_version.functions:
                comparison.add_function_change(func_id, change_type, target_id)
        
        # Add structure changes
        for struct_id, (change_type, target_id) in result.structure_changes.items():
            # Only add if it's in base_version (target-only structures are handled separately)
            if struct_id in base_version.structures:
                comparison.add_structure_change(struct_id, change_type, target_id)
        
        # Add call graph changes
        for change in result.call_graph_changes:
            comparison.add_call_graph_change(
                change["caller_id"],
                change["callee_id"],
                ChangeType[change["change_type"]]
            )
        
        # Add performance metric changes
        for func_id, metrics in result.metric_changes.items():
            for metric_name, values in metrics.items():
                comparison.add_metric_change(
                    func_id,
                    metric_name,
                    values["base_value"],
                    values["target_value"],
                    values["change_percentage"]
                )
        
        # Create comparison for storage
        storage_comparison = ComparisonResult(
            base_version_id=result.base_version_id,
            target_version_id=result.target_version_id,
            base_version_name=result.base_version_name,
            target_version_name=result.target_version_name
        )
        
        # Optional fields from request
        if 'name' in data:
            storage_comparison.name = data['name']
        else:
            storage_comparison.name = f"Comparison of {project1.name} and {project2.name}"
            
        if 'description' in data:
            storage_comparison.description = data['description']
        else:
            storage_comparison.description = f"Comparing {project1.name} ({project1.version}) with {project2.name} ({project2.version})"
            
        if 'tags' in data:
            storage_comparison.tags = data['tags']
            
        # Save the serialized result
        comparison_id = store.save_comparison(storage_comparison)
        
        return jsonify({
            "id": comparison_id,
            "message": "Comparison created successfully",
            "comparison": result_data
        })
    except Exception as e:
        return jsonify({"error": f"Comparison failed: {str(e)}"}), 500


def _convert_project_to_version(analysis_data, version):
    """Convert project analysis data to AnalysisVersion format."""
    # Add functions
    if 'functions' in analysis_data:
        for func_data in analysis_data['functions']:
            function = FunctionInfo(
                name=func_data.get('name', 'Unknown'),
                address=func_data.get('address', 0),
                size=func_data.get('size', 0),
                signature=func_data.get('signature'),
                function_id=func_data.get('id'),
                complexity=func_data.get('complexity'),
                decompiled_code=func_data.get('decompiled_code'),
                summary=func_data.get('summary'),
                tags=func_data.get('tags', [])
            )
            
            # Add parameters if available
            if 'parameters' in func_data:
                for i, param in enumerate(func_data['parameters']):
                    param_info = ParameterInfo(
                        name=param.get('name', f'param{i}'),
                        type_name=param.get('type', 'unknown'),
                        position=i,
                        size=param.get('size', 0)
                    )
                    function.add_parameter(param_info)
            
            # Add local variables if available
            if 'local_vars' in func_data:
                for var in func_data['local_vars']:
                    var_info = VariableInfo(
                        name=var.get('name', 'var'),
                        type_name=var.get('type', 'unknown'),
                        size=var.get('size', 0),
                        is_stack=var.get('is_stack', True)
                    )
                    function.add_local_var(var_info)
            
            version.add_function(function)
    
    # Add structures
    if 'structures' in analysis_data:
        for struct_data in analysis_data['structures']:
            structure = StructureInfo(
                name=struct_data.get('name', 'Unknown'),
                size=struct_data.get('size', 0),
                structure_id=struct_data.get('id'),
                is_union=struct_data.get('is_union', False)
            )
            
            # Add fields if available
            if 'fields' in struct_data:
                for field in struct_data['fields']:
                    field_info = StructureField(
                        name=field.get('name', 'field'),
                        type_name=field.get('type', 'unknown'),
                        offset=field.get('offset', 0),
                        size=field.get('size', 0)
                    )
                    structure.add_field(field_info)
            
            version.add_structure(structure)
    
    # Add call graph
    if 'call_graph' in analysis_data:
        for caller, callees in analysis_data['call_graph'].items():
            for callee in callees:
                version.add_call(caller, callee)
    
    # Add performance metrics
    if 'performance_metrics' in analysis_data:
        for func_id, metrics in analysis_data['performance_metrics'].items():
            version.set_performance_metrics(func_id, metrics)

@comparison_bp.route('/comparison/<comparison_id>', methods=['DELETE'])
@login_required
def delete_comparison(comparison_id):
    """Delete a comparison."""
    success = store.delete_comparison(comparison_id)
    if not success:
        return jsonify({"error": f"Comparison {comparison_id} not found"}), 404
    
    return jsonify({"message": f"Comparison {comparison_id} deleted successfully"})

@comparison_bp.route('/analysis/export/<project_id>', methods=['GET'])
@login_required
def export_analysis(project_id):
    """Export an analysis project as a JSON file."""
    project = store.get_project(project_id)
    if not project:
        return jsonify({"error": f"Project {project_id} not found"}), 404
    
    # Create temporary file
    import tempfile
    
    fd, temp_path = tempfile.mkstemp(suffix='.json')
    os.close(fd)
    
    # Write project data to file
    with open(temp_path, 'w') as f:
        json.dump({
            "id": project.project_id,
            "name": project.name,
            "binary_path": project.binary_path,
            "timestamp": project.timestamp.isoformat() if project.timestamp else None,
            "version": getattr(project, 'version', '1.0'),
            "description": project.description,
            "tags": getattr(project, 'tags', []),
            "analysis_data": getattr(project, 'analysis_data', {})
        }, f, indent=2)
    
    # Send file
    return send_file(
        temp_path,
        as_attachment=True,
        download_name=f"{project.name.replace(' ', '_')}_analysis.json",
        mimetype='application/json'
    )

@comparison_bp.route('/analysis/import', methods=['POST'])
@login_required
def import_analysis():
    """Import an analysis project from a JSON file."""
    if 'file' not in request.files:
        return jsonify({"error": "No file provided"}), 400
    
    file = request.files['file']
    if not file.filename:
        return jsonify({"error": "No file selected"}), 400
    
    # Check file extension
    if not file.filename.endswith('.json'):
        return jsonify({"error": "File must be a JSON file"}), 400
    
    # Load file content
    try:
        data = json.load(file)
        
        # Create project
        project = AnalysisProject(
            name=data['name'],
            description=data.get('description', ''),
            binary_path=data.get('binary_path', '')
        )
        
        # Add optional fields
        if 'timestamp' in data:
            try:
                project.timestamp = datetime.fromisoformat(data['timestamp'])
            except ValueError:
                pass
        if 'version' in data:
            project.version = data['version']
        if 'description' in data:
            project.description = data['description']
        if 'tags' in data:
            project.tags = data['tags']
        
        # Save project
        project_id = store.save_project(project)
        
        return jsonify({
            "id": project_id,
            "message": "Analysis imported successfully"
        })
    except json.JSONDecodeError:
        return jsonify({"error": "Invalid JSON file"}), 400
    except KeyError as e:
        return jsonify({"error": f"Missing required field: {str(e)}"}), 400
```

`src/comparison/store.py`:

```py
"""Storage for binary analysis comparisons."""

import os
import json
import shutil
from typing import Dict, List, Optional, Any
from datetime import datetime

from src.comparison.models import AnalysisProject, ComparisonResult


class ComparisonStore:
    """Manages storage and retrieval of binary analysis comparisons."""
    
    def __init__(self, storage_dir: str):
        """Initialize with storage directory.
        
        Args:
            storage_dir: Directory path to store comparison data
        """
        self.storage_dir = storage_dir
        self.projects_dir = os.path.join(storage_dir, "projects")
        self.comparisons_dir = os.path.join(storage_dir, "comparisons")
        
        # Create directories if they don't exist
        os.makedirs(self.projects_dir, exist_ok=True)
        os.makedirs(self.comparisons_dir, exist_ok=True)
    
    def save_project(self, project: AnalysisProject) -> str:
        """Save a project analysis to storage.
        
        Args:
            project: The analysis project to save
            
        Returns:
            The project ID
        """
        # Ensure project has an ID
        if not project.id:
            project.id = self._generate_id()
        
        # Create project directory
        project_dir = os.path.join(self.projects_dir, project.id)
        os.makedirs(project_dir, exist_ok=True)
        
        # Save project metadata
        metadata = {
            "id": project.id,
            "name": project.name,
            "binary_path": project.binary_path,
            "timestamp": project.timestamp.isoformat() if project.timestamp else datetime.now().isoformat(),
            "version": project.version,
            "description": project.description,
            "tags": project.tags
        }
        
        with open(os.path.join(project_dir, "metadata.json"), "w") as f:
            json.dump(metadata, f, indent=2)
        
        # Save analysis data
        with open(os.path.join(project_dir, "analysis.json"), "w") as f:
            json.dump(project.analysis_data, f, indent=2)
        
        return project.id
    
    def get_project(self, project_id: str) -> Optional[AnalysisProject]:
        """Retrieve a project analysis from storage.
        
        Args:
            project_id: The ID of the project to retrieve
            
        Returns:
            The retrieved project or None if not found
        """
        project_dir = os.path.join(self.projects_dir, project_id)
        
        if not os.path.exists(project_dir):
            return None
        
        # Load metadata
        try:
            with open(os.path.join(project_dir, "metadata.json"), "r") as f:
                metadata = json.load(f)
            
            # Load analysis data
            with open(os.path.join(project_dir, "analysis.json"), "r") as f:
                analysis_data = json.load(f)
            
            # Create project object
            project = AnalysisProject(
                id=metadata["id"],
                name=metadata["name"],
                binary_path=metadata.get("binary_path", ""),
                analysis_data=analysis_data,
            )
            
            # Add optional fields
            if "timestamp" in metadata:
                project.timestamp = datetime.fromisoformat(metadata["timestamp"])
            if "version" in metadata:
                project.version = metadata["version"]
            if "description" in metadata:
                project.description = metadata["description"]
            if "tags" in metadata:
                project.tags = metadata["tags"]
            
            return project
        except (json.JSONDecodeError, FileNotFoundError, KeyError) as e:
            print(f"Error loading project {project_id}: {str(e)}")
            return None
    
    def list_projects(self) -> List[Dict[str, Any]]:
        """List all available analysis projects.
        
        Returns:
            List of project metadata dictionaries
        """
        projects = []
        
        # List directories in projects directory
        if not os.path.exists(self.projects_dir):
            return projects
            
        for project_id in os.listdir(self.projects_dir):
            project_dir = os.path.join(self.projects_dir, project_id)
            
            # Skip if not a directory
            if not os.path.isdir(project_dir):
                continue
            
            # Load metadata
            try:
                with open(os.path.join(project_dir, "metadata.json"), "r") as f:
                    metadata = json.load(f)
                projects.append(metadata)
            except (json.JSONDecodeError, FileNotFoundError):
                # Skip projects with missing or invalid metadata
                continue
        
        # Sort by timestamp (newest first)
        projects.sort(key=lambda p: p.get("timestamp", ""), reverse=True)
        return projects
    
    def delete_project(self, project_id: str) -> bool:
        """Delete a project analysis from storage.
        
        Args:
            project_id: The ID of the project to delete
            
        Returns:
            True if deletion was successful, False otherwise
        """
        project_dir = os.path.join(self.projects_dir, project_id)
        
        if not os.path.exists(project_dir):
            return False
        
        try:
            shutil.rmtree(project_dir)
            return True
        except OSError:
            return False
    
    def save_comparison(self, comparison: ComparisonResult) -> str:
        """Save a comparison result to storage.
        
        Args:
            comparison: The comparison result to save
            
        Returns:
            The comparison ID
        """
        # Ensure comparison has an ID
        if not comparison.id:
            comparison.id = self._generate_id()
        
        # Create comparison file
        comparison_path = os.path.join(self.comparisons_dir, f"{comparison.id}.json")
        
        # Convert to dictionary
        comparison_data = {
            "id": comparison.id,
            "name": comparison.name,
            "timestamp": comparison.timestamp.isoformat() if comparison.timestamp else datetime.now().isoformat(),
            "project1_id": comparison.project1_id,
            "project2_id": comparison.project2_id,
            "description": comparison.description,
            "tags": comparison.tags,
            "result_data": comparison.result_data
        }
        
        # Save to file
        with open(comparison_path, "w") as f:
            json.dump(comparison_data, f, indent=2)
        
        return comparison.id
    
    def get_comparison(self, comparison_id: str) -> Optional[ComparisonResult]:
        """Retrieve a comparison result from storage.
        
        Args:
            comparison_id: The ID of the comparison to retrieve
            
        Returns:
            The retrieved comparison or None if not found
        """
        comparison_path = os.path.join(self.comparisons_dir, f"{comparison_id}.json")
        
        if not os.path.exists(comparison_path):
            return None
        
        try:
            with open(comparison_path, "r") as f:
                data = json.load(f)
            
            # Create comparison object
            comparison = ComparisonResult(
                id=data["id"],
                name=data["name"],
                project1_id=data["project1_id"],
                project2_id=data["project2_id"],
                result_data=data["result_data"]
            )
            
            # Add optional fields
            if "timestamp" in data:
                comparison.timestamp = datetime.fromisoformat(data["timestamp"])
            if "description" in data:
                comparison.description = data["description"]
            if "tags" in data:
                comparison.tags = data["tags"]
            
            return comparison
        except (json.JSONDecodeError, FileNotFoundError, KeyError) as e:
            print(f"Error loading comparison {comparison_id}: {str(e)}")
            return None
    
    def list_comparisons(self) -> List[Dict[str, Any]]:
        """List all available comparison results.
        
        Returns:
            List of comparison metadata dictionaries
        """
        comparisons = []
        
        # List files in comparisons directory
        if not os.path.exists(self.comparisons_dir):
            return comparisons
            
        for filename in os.listdir(self.comparisons_dir):
            if not filename.endswith(".json"):
                continue
                
            comparison_path = os.path.join(self.comparisons_dir, filename)
            
            # Load comparison data
            try:
                with open(comparison_path, "r") as f:
                    data = json.load(f)
                
                # Extract metadata
                metadata = {
                    "id": data["id"],
                    "name": data["name"],
                    "timestamp": data.get("timestamp", ""),
                    "project1_id": data["project1_id"],
                    "project2_id": data["project2_id"],
                    "description": data.get("description", ""),
                    "tags": data.get("tags", [])
                }
                
                comparisons.append(metadata)
            except (json.JSONDecodeError, FileNotFoundError, KeyError):
                # Skip comparisons with invalid data
                continue
        
        # Sort by timestamp (newest first)
        comparisons.sort(key=lambda c: c.get("timestamp", ""), reverse=True)
        return comparisons
    
    def delete_comparison(self, comparison_id: str) -> bool:
        """Delete a comparison result from storage.
        
        Args:
            comparison_id: The ID of the comparison to delete
            
        Returns:
            True if deletion was successful, False otherwise
        """
        comparison_path = os.path.join(self.comparisons_dir, f"{comparison_id}.json")
        
        if not os.path.exists(comparison_path):
            return False
        
        try:
            os.remove(comparison_path)
            return True
        except OSError:
            return False
    
    def _generate_id(self) -> str:
        """Generate a unique ID.
        
        Returns:
            A unique ID string
        """
        import uuid
        return str(uuid.uuid4())
```

`src/core/__init__.py`:

```py
"""
Core package for RE-Architect.
"""

from .config import Config
from .pipeline import ReversePipeline
from .binary_loader import BinaryLoader, BinaryInfo, BinaryFormat, Architecture, CompilerType

__all__ = [
    'Config',
    'ReversePipeline', 
    'BinaryLoader',
    'BinaryInfo',
    'BinaryFormat',
    'Architecture',
    'CompilerType'
]
```

`src/core/binary_loader.py`:

```py
"""
Binary loader module for RE-Architect.

This module handles loading and initial analysis of binary files.
"""

import logging
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
import subprocess
import tempfile
from typing import Dict, List, Optional, Tuple, Union

try:
    import lief
    LIEF_AVAILABLE = True
except ImportError:
    LIEF_AVAILABLE = False

try:
    from src.unpacking.symbolic_unpacker import SymbolicUnpacker
    SYMBOLIC_UNPACKER_AVAILABLE = True
except ImportError:
    SymbolicUnpacker = None
    SYMBOLIC_UNPACKER_AVAILABLE = False

logger = logging.getLogger("re-architect.binary_loader")

class Architecture(Enum):
    """Supported binary architectures."""
    X86 = "x86"
    X86_64 = "x86_64"
    ARM = "arm"
    ARM64 = "arm64"
    MIPS = "mips"
    MIPS64 = "mips64"
    PPC = "powerpc"
    PPC64 = "powerpc64"
    UNKNOWN = "unknown"

class BinaryFormat(Enum):
    """Supported binary file formats."""
    ELF = "elf"
    PE = "pe"
    MACHO = "macho"
    RAW = "raw"
    UNKNOWN = "unknown"

class CompilerType(Enum):
    """Detected compiler types."""
    GCC = "gcc"
    CLANG = "clang"
    MSVC = "msvc"
    GO = "go"
    RUST = "rust"
    UNKNOWN = "unknown"

@dataclass
class BinaryInfo:
    """Information about a loaded binary file."""
    path: Path
    format: BinaryFormat
    architecture: Architecture
    bit_width: int
    endianness: str  # "little" or "big"
    entry_point: int
    sections: Dict[str, Dict]
    symbols: Dict[str, Dict]
    compiler: CompilerType
    stripped: bool
    is_library: bool
    imports: Dict[str, List[str]]
    exports: List[str]
    
    def __str__(self) -> str:
        """String representation of binary information."""
        return (
            f"BinaryInfo(path={self.path}, "
            f"format={self.format.value}, "
            f"architecture={self.architecture.value}, "
            f"bit_width={self.bit_width}, "
            f"endianness={self.endianness}, "
            f"compiler={self.compiler.value}, "
            f"stripped={self.stripped})"
        )

class BinaryLoader:
    """
    Binary loader for RE-Architect.
    
    This class handles loading and initial analysis of binary files.
    """
    
    def __init__(self):
        """Initialize the binary loader."""
        # Check for available tools
        self._check_tools()
        
        # Define supported binary formats
        self.supported_formats = ["elf", "pe", "macho"]
        
        # Check LIEF availability
        if not LIEF_AVAILABLE:
            logger.warning("LIEF not available. Some features may be limited.")
    
    def _check_tools(self) -> None:
        """Check for available binary analysis tools."""
        self.available_tools = {
            "file": self._check_command("file --version"),
            "objdump": self._check_command("objdump --version"),
            "readelf": self._check_command("readelf --version"),
            "nm": self._check_command("nm --version"),
            "strings": self._check_command("strings --version"),
            "ldd": self._check_command("ldd --version"),
            "lief": LIEF_AVAILABLE,
        }
        
        logger.debug(f"Available tools: {[k for k, v in self.available_tools.items() if v]}")
    
    def _check_command(self, command: str) -> bool:
        """
        Check if a command is available.
        
        Args:
            command: Command to check
            
        Returns:
            True if the command is available, False otherwise
        """
        try:
            # Import security module for safe subprocess execution
            from src.security import SecurityValidator
            
            # Safely split and execute command
            cmd_list = command.split()
            SecurityValidator.safe_subprocess_run(
                cmd_list,
                timeout=10,  # Short timeout for availability check
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=False
            )
            return True
        except Exception:
            return False
    
    def load(self, binary_path: Union[str, Path], auto_unpack: bool = True) -> BinaryInfo:
        """
        Load and analyze a binary file.
        
        Args:
            binary_path: Path to the binary file
            auto_unpack: Whether to automatically unpack if binary is detected as packed
            
        Returns:
            BinaryInfo object containing information about the binary
            
        Raises:
            SecurityError: If the binary file is unsafe
            ValueError: If the binary format is not supported
        """
        # Import security module
        from src.security import SecurityValidator
        
        # Validate and sanitize the binary path
        validated_path = SecurityValidator.validate_binary_file(binary_path)
        
        logger.info(f"Loading binary: {validated_path}")
        
        # Calculate file hash for integrity checking
        file_hash = SecurityValidator.calculate_file_hash(validated_path)
        logger.info(f"Binary hash (SHA256): {file_hash}")
        
        # Check if binary is packed and unpack if needed
        if auto_unpack:
            binary_path = self._check_and_unpack(binary_path)
        
        # Use LIEF if available for better analysis
        if LIEF_AVAILABLE:
            return self._load_with_lief(binary_path)
        else:
            return self._load_with_fallback(binary_path)
    
    def _load_with_lief(self, binary_path: Path) -> BinaryInfo:
        """
        Load and analyze a binary file using LIEF.
        
        Args:
            binary_path: Path to the binary file
            
        Returns:
            BinaryInfo object containing information about the binary
        """
        try:
            # Parse the binary with LIEF
            binary = lief.parse(str(binary_path))
            if binary is None:
                logger.warning("LIEF failed to parse binary, falling back to basic analysis")
                return self._load_with_fallback(binary_path)
            
            # Determine format
            binary_format = self._lief_determine_format(binary)
            
            # Get architecture info
            architecture, bit_width, endianness = self._lief_get_architecture(binary)
            
            # Get entry point
            entry_point = binary.entrypoint if hasattr(binary, 'entrypoint') else 0
            
            # Get sections
            sections = self._lief_get_sections(binary)
            
            # Get symbols
            symbols = self._lief_get_symbols(binary)
            
            # Detect compiler
            compiler = self._lief_detect_compiler(binary, sections)
            
            # Check if stripped
            stripped = len(symbols) == 0 or all(not sym.get('name', '') for sym in symbols.values())
            
            # Check if library
            is_library = self._lief_is_library(binary)
            
            # Get imports and exports
            imports = self._lief_get_imports(binary)
            exports = self._lief_get_exports(binary)
            
            return BinaryInfo(
                path=binary_path,
                format=binary_format,
                architecture=architecture,
                bit_width=bit_width,
                endianness=endianness,
                entry_point=entry_point,
                sections=sections,
                symbols=symbols,
                compiler=compiler,
                stripped=stripped,
                is_library=is_library,
                imports=imports,
                exports=exports
            )
            
        except Exception as e:
            logger.error(f"Error using LIEF to parse {binary_path}: {e}")
            logger.info("Falling back to basic analysis")
            return self._load_with_fallback(binary_path)
    
    def _check_and_unpack(self, binary_path: Path) -> Path:
        """
        Check if binary is packed and unpack if necessary.
        
        Args:
            binary_path: Path to the binary file
            
        Returns:
            Path to the binary (original or unpacked)
        """
        try:
            if not SYMBOLIC_UNPACKER_AVAILABLE or SymbolicUnpacker is None:
                logger.debug("SymbolicUnpacker not available")
                return binary_path
            
            unpacker = SymbolicUnpacker()
            if not unpacker.is_available():
                logger.debug("Angr not available for unpacking")
                return binary_path
            
            # Check if binary is packed
            packer = unpacker.detect_packer(binary_path)
            if not packer:
                logger.debug("Binary does not appear to be packed")
                return binary_path
            
            logger.info(f"Detected packed binary with packer: {packer}")
            
            # Attempt to unpack
            result = unpacker.unpack(binary_path)
            if result.success and result.unpacked_path:
                logger.info(f"Successfully unpacked binary to: {result.unpacked_path}")
                return result.unpacked_path
            else:
                logger.warning(f"Failed to unpack binary: {result.error_message}")
                return binary_path
                
        except ImportError:
            logger.debug("Unpacking module not available")
            return binary_path
        except Exception as e:
            logger.error(f"Error during unpacking: {e}")
            return binary_path
    
    def _load_with_fallback(self, binary_path: Path) -> BinaryInfo:
        """
        Load and analyze a binary file using basic analysis.
        
        Args:
            binary_path: Path to the binary file
            
        Returns:
            BinaryInfo object containing information about the binary
        """
        # Get basic file information
        file_info = self._get_file_info(binary_path)
        
        # Determine binary format
        binary_format = self._determine_format(file_info)
        
        # Determine architecture
        architecture, bit_width, endianness = self._determine_architecture(file_info)
        
        # Get entry point (basic implementation)
        entry_point = 0
        
        # Get sections (basic implementation)
        sections = {}
        
        # Get symbols (basic implementation)
        symbols = {}
        
        # Detect compiler (basic implementation)
        compiler = CompilerType.UNKNOWN
        
        # Check if stripped (basic implementation)
        stripped = True
        
        # Check if library (basic implementation)
        is_library = False
        
        # Get imports and exports (basic implementation)
        imports = {}
        exports = []
        
        return BinaryInfo(
            path=binary_path,
            format=binary_format,
            architecture=architecture,
            bit_width=bit_width,
            endianness=endianness,
            entry_point=entry_point,
            sections=sections,
            symbols=symbols,
            compiler=compiler,
            stripped=stripped,
            is_library=is_library,
            imports=imports,
            exports=exports
        )
    
    def _get_file_info(self, binary_path: Path) -> str:
        """
        Get basic file information using the 'file' command.
        
        Args:
            binary_path: Path to the binary file
            
        Returns:
            Output of the 'file' command
        """
        if not self.available_tools.get("file", False):
            logger.warning("'file' command not available, falling back to basic analysis")
            return ""
        
        try:
            # Import security module for safe subprocess execution
            from src.security import SecurityValidator
            
            # Validate binary path first
            validated_path = SecurityValidator.validate_binary_file(binary_path)
            
            result = SecurityValidator.safe_subprocess_run(
                ["file", str(validated_path)],
                timeout=30,
                check=True
            )
            return result.stdout.strip() if result.stdout else ""
        except Exception as e:
            logger.warning(f"Error running 'file' command: {e}")
            return ""
    
    def _determine_format(self, file_info: str) -> BinaryFormat:
        """
        Determine the binary format based on file information.
        
        Args:
            file_info: Output of the 'file' command
            
        Returns:
            Detected binary format
        """
        file_info_lower = file_info.lower()
        
        if "elf" in file_info_lower:
            return BinaryFormat.ELF
        elif "pe" in file_info_lower or "executable for ms windows" in file_info_lower:
            return BinaryFormat.PE
        elif "mach-o" in file_info_lower:
            return BinaryFormat.MACHO
        else:
            return BinaryFormat.UNKNOWN
    
    def _determine_architecture(self, file_info: str) -> Tuple[Architecture, int, str]:
        """
        Determine the architecture, bit width, and endianness based on file information.
        
        Args:
            file_info: Output of the 'file' command
            
        Returns:
            Tuple of (architecture, bit_width, endianness)
        """
        file_info_lower = file_info.lower()
        
        # Determine architecture
        if "x86-64" in file_info_lower or "x86_64" in file_info_lower:
            architecture = Architecture.X86_64
            bit_width = 64
        elif "i386" in file_info_lower or "80386" in file_info_lower:
            architecture = Architecture.X86
            bit_width = 32
        elif "arm64" in file_info_lower or "aarch64" in file_info_lower:
            architecture = Architecture.ARM64
            bit_width = 64
        elif "arm" in file_info_lower:
            architecture = Architecture.ARM
            bit_width = 32
        else:
            architecture = Architecture.UNKNOWN
            bit_width = 32
        
        # Determine endianness
        if "msb" in file_info_lower or "big-endian" in file_info_lower:
            endianness = "big"
        else:
            endianness = "little"
        
        return architecture, bit_width, endianness
    
    # LIEF-specific helper methods
    def _lief_determine_format(self, binary) -> BinaryFormat:
        """Determine binary format using LIEF."""
        if hasattr(binary, 'format'):
            if binary.format == lief.Binary.FORMATS.ELF:
                return BinaryFormat.ELF
            elif binary.format == lief.Binary.FORMATS.PE:
                return BinaryFormat.PE
            elif binary.format == lief.Binary.FORMATS.MACHO:
                return BinaryFormat.MACHO
        return BinaryFormat.UNKNOWN
    
    def _lief_get_architecture(self, binary) -> Tuple[Architecture, int, str]:
        """Get architecture information using LIEF."""
        architecture = Architecture.UNKNOWN
        bit_width = 32
        endianness = "little"
        
        if hasattr(binary, 'header'):
            header = binary.header
            
            # ELF specific
            if binary.format == lief.Binary.FORMATS.ELF:
                machine = header.machine_type if hasattr(header, 'machine_type') else None
                try:
                    if machine == lief.ELF.ARCH.x86_64:
                        architecture = Architecture.X86_64
                        bit_width = 64
                    elif machine == lief.ELF.ARCH.i386:
                        architecture = Architecture.X86
                        bit_width = 32
                    elif machine == lief.ELF.ARCH.ARM:
                        architecture = Architecture.ARM
                        bit_width = 32
                    elif machine == lief.ELF.ARCH.AARCH64:
                        architecture = Architecture.ARM64
                        bit_width = 64
                    
                    # Get endianness
                    endianness = "little" if header.identity_data == lief.ELF.ELF_DATA.LSB else "big"
                except AttributeError:
                    # Fallback for different LIEF versions
                    pass
            
            # PE specific
            elif binary.format == lief.Binary.FORMATS.PE:
                machine = header.machine if hasattr(header, 'machine') else None
                try:
                    # Handle different LIEF API versions
                    if hasattr(lief.PE, 'MACHINE_TYPES'):
                        machine_types = lief.PE.MACHINE_TYPES
                    else:
                        # Fallback for newer LIEF versions
                        machine_types = lief.PE.MACHINE_TYPE
                    
                    if hasattr(machine_types, 'AMD64') and machine == machine_types.AMD64:
                        architecture = Architecture.X86_64
                        bit_width = 64
                    elif hasattr(machine_types, 'I386') and machine == machine_types.I386:
                        architecture = Architecture.X86
                        bit_width = 32
                    elif hasattr(machine_types, 'ARM64') and machine == machine_types.ARM64:
                        architecture = Architecture.ARM64
                        bit_width = 64
                except AttributeError:
                    # Fallback - try to determine from binary class
                    if hasattr(binary, 'optional_header') and hasattr(binary.optional_header, 'magic'):
                        magic = binary.optional_header.magic
                        if magic == 0x20b:  # PE32+
                            architecture = Architecture.X86_64
                            bit_width = 64
                        elif magic == 0x10b:  # PE32
                            architecture = Architecture.X86
                            bit_width = 32
            
            # Mach-O specific
            elif binary.format == lief.Binary.FORMATS.MACHO:
                cpu_type = header.cpu_type if hasattr(header, 'cpu_type') else None
                try:
                    if hasattr(lief.MachO, 'CPU_TYPES'):
                        cpu_types = lief.MachO.CPU_TYPES
                    else:
                        cpu_types = lief.MachO.CPU_TYPE
                    
                    if hasattr(cpu_types, 'x86_64') and cpu_type == cpu_types.x86_64:
                        architecture = Architecture.X86_64
                        bit_width = 64
                    elif hasattr(cpu_types, 'x86') and cpu_type == cpu_types.x86:
                        architecture = Architecture.X86
                        bit_width = 32
                    elif hasattr(cpu_types, 'ARM64') and cpu_type == cpu_types.ARM64:
                        architecture = Architecture.ARM64
                        bit_width = 64
                    elif hasattr(cpu_types, 'ARM') and cpu_type == cpu_types.ARM:
                        architecture = Architecture.ARM
                        bit_width = 32
                except AttributeError:
                    pass
        
        return architecture, bit_width, endianness
    
    def _lief_get_sections(self, binary) -> Dict[str, Dict]:
        """Get section information using LIEF."""
        sections = {}
        
        if hasattr(binary, 'sections'):
            for idx, section in enumerate(binary.sections):
                section_info = {
                    "name": section.name if hasattr(section, 'name') else f"section_{idx}",
                    "virtual_address": section.virtual_address if hasattr(section, 'virtual_address') else 0,
                    "size": section.size if hasattr(section, 'size') else 0,
                    "offset": section.offset if hasattr(section, 'offset') else 0,
                    "entropy": section.entropy if hasattr(section, 'entropy') else 0.0,
                }
                
                # Add format-specific information
                if binary.format == lief.Binary.FORMATS.ELF and hasattr(section, 'type'):
                    section_info["type"] = str(section.type)
                    section_info["flags"] = section.flags if hasattr(section, 'flags') else 0
                elif binary.format == lief.Binary.FORMATS.PE and hasattr(section, 'characteristics'):
                    section_info["characteristics"] = section.characteristics
                
                sections[section_info["name"]] = section_info
        
        return sections
    
    def _lief_get_symbols(self, binary) -> Dict[str, Dict]:
        """Get symbol information using LIEF."""
        symbols = {}
        
        # Get static symbols
        if hasattr(binary, 'symbols'):
            for symbol in binary.symbols:
                if hasattr(symbol, 'name') and symbol.name:
                    symbol_info = {
                        "name": symbol.name,
                        "value": symbol.value if hasattr(symbol, 'value') else 0,
                        "size": symbol.size if hasattr(symbol, 'size') else 0,
                        "type": "static"
                    }
                    
                    # Add format-specific info
                    if binary.format == lief.Binary.FORMATS.ELF:
                        if hasattr(symbol, 'type'):
                            symbol_info["symbol_type"] = str(symbol.type)
                        if hasattr(symbol, 'binding'):
                            symbol_info["binding"] = str(symbol.binding)
                    
                    symbols[symbol.name] = symbol_info
        
        # Get dynamic symbols for ELF
        if binary.format == lief.Binary.FORMATS.ELF and hasattr(binary, 'dynamic_symbols'):
            for symbol in binary.dynamic_symbols:
                if hasattr(symbol, 'name') and symbol.name:
                    symbol_info = {
                        "name": symbol.name,
                        "value": symbol.value if hasattr(symbol, 'value') else 0,
                        "size": symbol.size if hasattr(symbol, 'size') else 0,
                        "type": "dynamic"
                    }
                    symbols[symbol.name] = symbol_info
        
        return symbols
    
    def _lief_detect_compiler(self, binary, sections: Dict) -> CompilerType:
        """Detect compiler using LIEF analysis."""
        # Look for compiler-specific sections or strings
        if ".gcc_except_table" in sections or ".eh_frame" in sections:
            return CompilerType.GCC
        
        if binary.format == lief.Binary.FORMATS.PE:
            # Check for MSVC-specific sections
            if ".rdata" in sections and ".pdata" in sections:
                return CompilerType.MSVC
        
        # Look for Go-specific patterns
        if ".gopclntab" in sections or ".go.buildinfo" in sections:
            return CompilerType.GO
        
        # Look for Rust-specific patterns
        rust_sections = [".text.startup", ".rustc"]
        if any(section in sections for section in rust_sections):
            return CompilerType.RUST
        
        return CompilerType.UNKNOWN
    
    def _lief_is_library(self, binary) -> bool:
        """Check if binary is a library using LIEF."""
        try:
            if binary.format == lief.Binary.FORMATS.ELF:
                return hasattr(binary, 'header') and hasattr(binary.header, 'file_type') and \
                       str(binary.header.file_type).endswith('DYNAMIC')
            elif binary.format == lief.Binary.FORMATS.PE:
                return hasattr(binary, 'header') and hasattr(binary.header, 'characteristics') and \
                       (binary.header.characteristics & 0x2000) != 0  # IMAGE_FILE_DLL
            elif binary.format == lief.Binary.FORMATS.MACHO:
                return hasattr(binary, 'header') and hasattr(binary.header, 'file_type') and \
                       str(binary.header.file_type).endswith('DYLIB')
        except AttributeError:
            pass
        return False
    
    def _lief_get_imports(self, binary) -> Dict[str, List[str]]:
        """Get import information using LIEF."""
        imports = {}
        
        if hasattr(binary, 'imports'):
            for imported_library in binary.imports:
                library_name = imported_library.name if hasattr(imported_library, 'name') else "unknown"
                import_list = []
                
                if hasattr(imported_library, 'entries'):
                    for entry in imported_library.entries:
                        if hasattr(entry, 'name') and entry.name:
                            import_list.append(entry.name)
                
                if import_list:
                    imports[library_name] = import_list
        
        return imports
    
    def _lief_get_exports(self, binary) -> List[str]:
        """Get export information using LIEF."""
        exports = []
        
        if hasattr(binary, 'exported_functions'):
            for export in binary.exported_functions:
                if hasattr(export, 'name') and export.name:
                    exports.append(export.name)
        
        return exports
```

`src/core/config.py`:

```py
"""
Configuration module for RE-Architect.

This module handles loading, validating, and accessing configuration settings.
"""

import logging
import os
from pathlib import Path
from typing import Any, Dict, List, Optional, Union, Type, TypeVar
import yaml

logger = logging.getLogger("re-architect.config")

T = TypeVar('T')

class Config:
    """
    Configuration manager for RE-Architect.
    
    Handles loading and accessing all configuration settings for the pipeline.
    """
    
    # Default configuration values
    DEFAULT_CONFIG = {
        "auth": {
            "jwt_secret_key": "change-this-in-production",
            "jwt_algorithm": "HS256",
            "jwt_expiration_delta": 86400,  # seconds (24 hours)
            "token_location": ["headers"],
            "enable_registration": True,
            "default_role": "analyst"
        },
        "decompiler": {
            "ghidra": {
                "path": None,
                "headless": True,
                "timeout": 300  # seconds
            },
            "ida": {
                "path": None,
                "headless": True,
                "timeout": 300  # seconds
            },
            "binary_ninja": {
                "path": None,
                "timeout": 300  # seconds
            }
        },
        "analysis": {
            "static": {
                "function_analysis_depth": "medium",  # basic, medium, deep
                "data_flow_analysis": True,
                "control_flow_analysis": True
            },
            "dynamic": {
                "enable": False,
                "max_execution_time": 60,  # seconds
                "memory_limit": 2048,  # MB
                "sandbox_type": "container"  # none, container, vm
            }
        },
        "llm": {
            "enable": True,
            "provider": "openai",
            "model": "gpt-4",
            "api_key": None,
            "max_tokens": 8192,
            "temperature": 0.2,
            "cache_dir": "./cache/llm"
        },
        "test_generation": {
            "sanitizers": ["address", "undefined"],
            "fuzzing_time": 60,  # seconds
            "max_test_cases": 10,
            "compiler": "gcc",
            "compiler_flags": ["-O0", "-g"]
        },
        "visualization": {
            "host": "localhost",
            "port": 8000,
            "theme": "light"
        },
        "output": {
            "detail_level": "full",  # basic, standard, full
            "formats": ["json", "html"]
        }
    }
    
    def __init__(self, config_data: Union[Dict[str, Any], str, Path] = None):
        """
        Initialize configuration with provided data or defaults.
        
        Args:
            config_data: Dictionary containing configuration values or path to config file
        """
        # Start with default configuration
        self._config = self.DEFAULT_CONFIG.copy()
        
        # If config_data is a string or Path, assume it's a file path and load it
        if isinstance(config_data, (str, Path)):
            config_data = self._load_config_file(config_data)
        
        # Update with provided configuration if available
        if config_data:
            self._update_recursive(self._config, config_data)
        
        # Flag to track if LLM is enabled
        self.use_llm = self._config["llm"]["enable"]
        
    def _load_config_file(self, config_path: Union[str, Path]) -> Dict[str, Any]:
        """
        Load configuration from a YAML file.
        
        Args:
            config_path: Path to the configuration file
            
        Returns:
            Dictionary containing the loaded configuration
            
        Raises:
            FileNotFoundError: If the configuration file doesn't exist
            yaml.YAMLError: If the configuration file contains invalid YAML
        """
        config_path = Path(config_path)
        
        # If file doesn't exist, return empty dict
        if not config_path.exists():
            logger.warning(f"Configuration file not found: {config_path}")
            return {}
        
        try:
            with open(config_path, "r") as f:
                config_data = yaml.safe_load(f)
            
            logger.info(f"Loaded configuration from {config_path}")
            return config_data or {}
            
        except yaml.YAMLError as e:
            logger.error(f"Error parsing configuration file: {e}")
            raise
    
    @classmethod
    def from_file(cls, config_path: Union[str, Path]) -> "Config":
        """
        Load configuration from a YAML file.
        
        Args:
            config_path: Path to the configuration file
            
        Returns:
            Config object initialized with the loaded configuration
            
        Raises:
            FileNotFoundError: If the configuration file doesn't exist
            yaml.YAMLError: If the configuration file contains invalid YAML
        """
        config_path = Path(config_path)
        
        # If file doesn't exist, return default configuration
        if not config_path.exists():
            logger.warning(f"Configuration file not found: {config_path}")
            logger.info("Using default configuration")
            return cls()
        
        try:
            with open(config_path, "r") as f:
                config_data = yaml.safe_load(f)
            
            logger.info(f"Loaded configuration from {config_path}")
            return cls(config_data)
            
        except yaml.YAMLError as e:
            logger.error(f"Error parsing configuration file: {e}")
            raise
    
    def get(self, key_path: str, default: Optional[T] = None) -> Union[T, Any]:
        """
        Get a configuration value using a dot-separated path.
        
        Args:
            key_path: Dot-separated path to the configuration value (e.g., "llm.model")
            default: Default value to return if the key doesn't exist
            
        Returns:
            Configuration value at the specified path, or the default value if not found
        """
        keys = key_path.split(".")
        value = self._config
        
        try:
            for key in keys:
                value = value[key]
            return value
        except (KeyError, TypeError):
            return default
    
    def set(self, key_path: str, value: Any) -> None:
        """
        Set a configuration value using a dot-separated path.
        
        Args:
            key_path: Dot-separated path to the configuration value (e.g., "llm.model")
            value: Value to set
        """
        keys = key_path.split(".")
        config = self._config
        
        # Navigate to the innermost dictionary
        for key in keys[:-1]:
            if key not in config:
                config[key] = {}
            config = config[key]
        
        # Set the value
        config[keys[-1]] = value
        
        # Update use_llm flag if the llm.enable setting is changed
        if key_path == "llm.enable":
            self.use_llm = value
    
    def disable_llm(self) -> None:
        """Disable LLM-based analysis."""
        self.set("llm.enable", False)
        self.use_llm = False
    
    def enable_llm(self) -> None:
        """Enable LLM-based analysis."""
        self.set("llm.enable", True)
        self.use_llm = True
    
    def _update_recursive(self, base_dict: Dict[str, Any], update_dict: Dict[str, Any]) -> None:
        """
        Recursively update a dictionary with values from another dictionary.
        
        Args:
            base_dict: Dictionary to update
            update_dict: Dictionary containing values to update with
        """
        for key, value in update_dict.items():
            if key in base_dict and isinstance(base_dict[key], dict) and isinstance(value, dict):
                self._update_recursive(base_dict[key], value)
            else:
                base_dict[key] = value

```

`src/core/error_handling.py`:

```py
"""
Advanced error handling and monitoring for RE-Architect.

This module provides enterprise-grade error handling, structured logging,
and monitoring capabilities for the RE-Architect application.
"""

import logging
import traceback
import functools
import time
import json
import sys
import threading
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Optional, Callable, Union
from dataclasses import dataclass, field
from enum import Enum
import uuid

# Performance monitoring
import psutil


class ErrorSeverity(Enum):
    """Error severity levels."""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high" 
    CRITICAL = "critical"


class ErrorCategory(Enum):
    """Error category types."""
    SECURITY = "security"
    PERFORMANCE = "performance"
    RESOURCE = "resource"
    VALIDATION = "validation"
    NETWORK = "network"
    FILE_SYSTEM = "file_system"
    DECOMPILER = "decompiler"
    ANALYSIS = "analysis"
    CONFIGURATION = "configuration"
    UNKNOWN = "unknown"


@dataclass
class ErrorReport:
    """Structured error report."""
    error_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    severity: ErrorSeverity = ErrorSeverity.MEDIUM
    category: ErrorCategory = ErrorCategory.UNKNOWN
    component: str = ""
    message: str = ""
    exception_type: Optional[str] = None
    stack_trace: Optional[str] = None
    context: Dict[str, Any] = field(default_factory=dict)
    user_action: Optional[str] = None
    resolution_steps: List[str] = field(default_factory=list)
    performance_impact: Optional[Dict[str, float]] = None


class PerformanceMonitor:
    """System performance monitoring."""
    
    def __init__(self):
        self.start_time = time.time()
        self.metrics = {}
        self._lock = threading.Lock()
    
    def start_operation(self, operation_name: str) -> str:
        """Start monitoring an operation."""
        operation_id = str(uuid.uuid4())
        
        with self._lock:
            self.metrics[operation_id] = {
                'name': operation_name,
                'start_time': time.time(),
                'start_memory': self.get_memory_usage(),
                'start_cpu': psutil.cpu_percent(),
            }
        
        return operation_id
    
    def end_operation(self, operation_id: str) -> Dict[str, float]:
        """End monitoring an operation and return metrics."""
        end_time = time.time()
        end_memory = self.get_memory_usage()
        end_cpu = psutil.cpu_percent()
        
        with self._lock:
            if operation_id not in self.metrics:
                return {}
            
            start_metrics = self.metrics[operation_id]
            
            performance_data = {
                'duration': end_time - start_metrics['start_time'],
                'memory_delta': end_memory - start_metrics['start_memory'],
                'avg_cpu': (start_metrics['start_cpu'] + end_cpu) / 2,
                'peak_memory': end_memory,
            }
            
            # Clean up
            del self.metrics[operation_id]
            
        return performance_data
    
    @staticmethod
    def get_memory_usage() -> float:
        """Get current memory usage in MB."""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    @staticmethod
    def get_system_metrics() -> Dict[str, float]:
        """Get system-wide metrics."""
        return {
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_usage': psutil.disk_usage('/').percent,
            'load_average': psutil.getloadavg()[0] if hasattr(psutil, 'getloadavg') else 0.0,
        }


class StructuredLogger:
    """Structured logging with JSON output and context management."""
    
    def __init__(self, name: str, log_level: int = logging.INFO):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(log_level)
        
        # Configure JSON formatter
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = self.JSONFormatter()
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
        
        self.context = threading.local()
        self.performance_monitor = PerformanceMonitor()
    
    def set_context(self, **context):
        """Set logging context for current thread."""
        if not hasattr(self.context, 'data'):
            self.context.data = {}
        self.context.data.update(context)
    
    def clear_context(self):
        """Clear logging context."""
        if hasattr(self.context, 'data'):
            self.context.data.clear()
    
    def _get_context(self) -> Dict[str, Any]:
        """Get current logging context."""
        if hasattr(self.context, 'data'):
            return self.context.data.copy()
        return {}
    
    def log_structured(self, level: int, message: str, **extra):
        """Log a structured message with context."""
        context = self._get_context()
        context.update(extra)
        
        # Add system metrics for important events
        if level >= logging.WARNING:
            context['system_metrics'] = self.performance_monitor.get_system_metrics()
        
        self.logger.log(level, message, extra=context)
    
    def info(self, message: str, **extra):
        """Log info message."""
        self.log_structured(logging.INFO, message, **extra)
    
    def warning(self, message: str, **extra):
        """Log warning message."""
        self.log_structured(logging.WARNING, message, **extra)
    
    def error(self, message: str, **extra):
        """Log error message."""
        self.log_structured(logging.ERROR, message, **extra)
    
    def critical(self, message: str, **extra):
        """Log critical message."""
        self.log_structured(logging.CRITICAL, message, **extra)
    
    class JSONFormatter(logging.Formatter):
        """JSON formatter for structured logging."""
        
        def format(self, record):
            log_obj = {
                'timestamp': datetime.fromtimestamp(record.created, timezone.utc).isoformat(),
                'level': record.levelname,
                'logger': record.name,
                'message': record.getMessage(),
                'module': record.module,
                'function': record.funcName,
                'line': record.lineno,
            }
            
            # Add extra context
            if hasattr(record, 'extra'):
                log_obj.update(record.extra)
            
            # Add exception info if present
            if record.exc_info:
                log_obj['exception'] = {
                    'type': record.exc_info[0].__name__,
                    'message': str(record.exc_info[1]),
                    'traceback': traceback.format_exception(*record.exc_info),
                }
            
            return json.dumps(log_obj)


class ErrorHandler:
    """Advanced error handling and reporting."""
    
    def __init__(self, logger: Optional[StructuredLogger] = None):
        self.logger = logger or StructuredLogger("re-architect.errors")
        self.error_reports: List[ErrorReport] = []
        self.error_callbacks: List[Callable[[ErrorReport], None]] = []
        self._lock = threading.Lock()
    
    def add_error_callback(self, callback: Callable[[ErrorReport], None]):
        """Add callback for error notifications."""
        self.error_callbacks.append(callback)
    
    def handle_error(self, 
                    exception: Exception,
                    severity: ErrorSeverity = ErrorSeverity.MEDIUM,
                    category: ErrorCategory = ErrorCategory.UNKNOWN,
                    component: str = "",
                    context: Optional[Dict[str, Any]] = None,
                    user_action: Optional[str] = None,
                    resolution_steps: Optional[List[str]] = None) -> ErrorReport:
        """
        Handle an error with comprehensive reporting.
        
        Args:
            exception: The exception that occurred
            severity: Error severity level
            category: Error category
            component: Component where error occurred
            context: Additional context information
            user_action: User action that triggered the error
            resolution_steps: Suggested resolution steps
            
        Returns:
            ErrorReport object
        """
        # Create error report
        report = ErrorReport(
            severity=severity,
            category=category,
            component=component,
            message=str(exception),
            exception_type=type(exception).__name__,
            stack_trace=traceback.format_exc(),
            context=context or {},
            user_action=user_action,
            resolution_steps=resolution_steps or []
        )
        
        # Add performance impact if it's a performance-related error
        if category == ErrorCategory.PERFORMANCE:
            report.performance_impact = PerformanceMonitor.get_system_metrics()
        
        # Store report
        with self._lock:
            self.error_reports.append(report)
        
        # Log the error
        self.logger.error(
            f"Error in {component}: {exception}",
            error_id=report.error_id,
            severity=severity.value,
            category=category.value,
            exception_type=report.exception_type,
            context=report.context
        )
        
        # Notify callbacks
        for callback in self.error_callbacks:
            try:
                callback(report)
            except Exception as e:
                self.logger.warning(f"Error callback failed: {e}")
        
        return report
    
    def get_error_summary(self, hours: int = 24) -> Dict[str, Any]:
        """Get error summary for the last N hours."""
        cutoff_time = datetime.now(timezone.utc).timestamp() - (hours * 3600)
        
        recent_errors = [
            report for report in self.error_reports
            if report.timestamp.timestamp() > cutoff_time
        ]
        
        # Group by category and severity
        by_category = {}
        by_severity = {}
        
        for report in recent_errors:
            category = report.category.value
            severity = report.severity.value
            
            by_category[category] = by_category.get(category, 0) + 1
            by_severity[severity] = by_severity.get(severity, 0) + 1
        
        return {
            'total_errors': len(recent_errors),
            'by_category': by_category,
            'by_severity': by_severity,
            'time_window_hours': hours,
        }
    
    def export_error_reports(self, output_path: Union[str, Path]) -> None:
        """Export error reports to JSON file."""
        output_path = Path(output_path)
        
        reports_data = []
        for report in self.error_reports:
            reports_data.append({
                'error_id': report.error_id,
                'timestamp': report.timestamp.isoformat(),
                'severity': report.severity.value,
                'category': report.category.value,
                'component': report.component,
                'message': report.message,
                'exception_type': report.exception_type,
                'context': report.context,
                'user_action': report.user_action,
                'resolution_steps': report.resolution_steps,
                'performance_impact': report.performance_impact,
            })
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(reports_data, f, indent=2, default=str)
        
        self.logger.info(f"Exported {len(reports_data)} error reports to {output_path}")


def monitored_operation(component: str = "", 
                       category: ErrorCategory = ErrorCategory.UNKNOWN,
                       severity: ErrorSeverity = ErrorSeverity.MEDIUM):
    """
    Decorator for monitoring operations with automatic error handling.
    
    Args:
        component: Component name for error reporting
        category: Error category
        severity: Error severity level
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # Get or create error handler
            error_handler = getattr(wrapper, '_error_handler', None)
            if error_handler is None:
                error_handler = ErrorHandler()
                wrapper._error_handler = error_handler
            
            # Start performance monitoring
            perf_monitor = PerformanceMonitor()
            op_id = perf_monitor.start_operation(func.__name__)
            
            try:
                # Set logging context
                logger = StructuredLogger(f"re-architect.{component}")
                logger.set_context(
                    operation=func.__name__,
                    component=component,
                    operation_id=op_id
                )
                
                logger.info(f"Starting operation: {func.__name__}")
                
                # Execute function
                result = func(*args, **kwargs)
                
                # Log success
                performance_data = perf_monitor.end_operation(op_id)
                logger.info(
                    f"Operation completed: {func.__name__}",
                    performance=performance_data
                )
                
                return result
                
            except Exception as e:
                # Get performance impact
                performance_data = perf_monitor.end_operation(op_id)
                
                # Handle error
                error_handler.handle_error(
                    exception=e,
                    severity=severity,
                    category=category,
                    component=component,
                    context={
                        'function': func.__name__,
                        'args_count': len(args),
                        'kwargs_keys': list(kwargs.keys()),
                        'performance_impact': performance_data,
                    }
                )
                
                # Re-raise the exception
                raise
            
            finally:
                # Clear logging context
                if 'logger' in locals():
                    logger.clear_context()
        
        return wrapper
    return decorator


def setup_global_error_handling(log_file: Optional[Union[str, Path]] = None):
    """
    Set up global error handling for the application.
    
    Args:
        log_file: Optional log file path
    """
    # Create global error handler
    global_logger = StructuredLogger("re-architect.global")
    global_error_handler = ErrorHandler(global_logger)
    
    # Set up file logging if specified
    if log_file:
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(global_logger.JSONFormatter())
        global_logger.logger.addHandler(file_handler)
    
    # Handle uncaught exceptions
    def handle_exception(exc_type, exc_value, exc_traceback):
        if issubclass(exc_type, KeyboardInterrupt):
            sys.__excepthook__(exc_type, exc_value, exc_traceback)
            return
        
        global_error_handler.handle_error(
            exception=exc_value,
            severity=ErrorSeverity.CRITICAL,
            category=ErrorCategory.UNKNOWN,
            component="global",
            context={
                'uncaught_exception': True,
                'exception_type': exc_type.__name__,
            }
        )
    
    sys.excepthook = handle_exception
    
    return global_error_handler
```

`src/core/performance.py`:

```py
"""
Performance optimization module for RE-Architect.

This module provides memory management, CPU optimization, async/await patterns,
and performance improvements for binary analysis operations.
"""

import asyncio
import concurrent.futures
import functools
import gc
import multiprocessing
import os
import psutil
import threading
import time
import weakref
from contextlib import contextmanager
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Union, AsyncGenerator, Tuple
import mmap
import tempfile
import lru

# For memory profiling
try:
    import tracemalloc
    TRACEMALLOC_AVAILABLE = True
except ImportError:
    TRACEMALLOC_AVAILABLE = False

# For CPU profiling
try:
    import cProfile
    import pstats
    PROFILING_AVAILABLE = True
except ImportError:
    PROFILING_AVAILABLE = False


@dataclass
class PerformanceMetrics:
    """Performance metrics container."""
    execution_time: float
    memory_usage_mb: float
    peak_memory_mb: float
    cpu_usage_percent: float
    cache_hits: int = 0
    cache_misses: int = 0
    disk_io_mb: float = 0.0
    network_io_mb: float = 0.0


class MemoryManager:
    """Advanced memory management for large binary analysis."""
    
    def __init__(self, max_memory_mb: int = 4096):
        self.max_memory_mb = max_memory_mb
        self.memory_pools = {}
        self.object_cache = weakref.WeakKeyDictionary()
        self._lock = threading.Lock()
        
        # Set up memory monitoring
        self.memory_threshold = max_memory_mb * 0.8  # 80% threshold
        
    @contextmanager
    def memory_pool(self, pool_name: str):
        """Context manager for memory pool allocation."""
        pool_start_memory = self.get_current_memory_mb()
        
        try:
            with self._lock:
                self.memory_pools[pool_name] = {
                    'start_memory': pool_start_memory,
                    'objects': []
                }
            
            yield self.memory_pools[pool_name]
            
        finally:
            # Cleanup pool
            with self._lock:
                if pool_name in self.memory_pools:
                    pool = self.memory_pools[pool_name]
                    
                    # Force garbage collection of pool objects
                    for obj in pool.get('objects', []):
                        del obj
                    
                    del self.memory_pools[pool_name]
            
            # Force garbage collection
            gc.collect()
    
    def register_object(self, pool_name: str, obj: Any) -> None:
        """Register an object in a memory pool."""
        with self._lock:
            if pool_name in self.memory_pools:
                self.memory_pools[pool_name]['objects'].append(obj)
    
    @staticmethod
    def get_current_memory_mb() -> float:
        """Get current memory usage in MB."""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024
    
    def check_memory_pressure(self) -> bool:
        """Check if memory usage is approaching limits."""
        current_memory = self.get_current_memory_mb()
        return current_memory > self.memory_threshold
    
    def force_cleanup(self) -> float:
        """Force memory cleanup and return freed memory."""
        before_memory = self.get_current_memory_mb()
        
        # Clear all pools
        with self._lock:
            for pool_name, pool in list(self.memory_pools.items()):
                for obj in pool.get('objects', []):
                    del obj
                del self.memory_pools[pool_name]
        
        # Force garbage collection
        gc.collect()
        
        after_memory = self.get_current_memory_mb()
        return before_memory - after_memory


class AsyncFileProcessor:
    """Async file processing for large binaries."""
    
    def __init__(self, max_workers: int = None):
        self.max_workers = max_workers or min(32, multiprocessing.cpu_count() + 4)
        self.executor = concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers)
    
    async def read_file_chunks(self, 
                              file_path: Path, 
                              chunk_size: int = 1024 * 1024) -> AsyncGenerator[bytes, None]:
        """
        Async generator for reading file in chunks.
        
        Args:
            file_path: Path to file
            chunk_size: Size of each chunk in bytes
            
        Yields:
            File chunks as bytes
        """
        def _read_chunk(f, size):
            return f.read(size)
        
        loop = asyncio.get_event_loop()
        
        with open(file_path, 'rb') as f:
            while True:
                chunk = await loop.run_in_executor(
                    self.executor, 
                    _read_chunk, 
                    f, 
                    chunk_size
                )
                if not chunk:
                    break
                yield chunk
    
    async def memory_mapped_analysis(self, 
                                   file_path: Path,
                                   analysis_func: Callable[[mmap.mmap], Any]) -> Any:
        """
        Perform memory-mapped file analysis for large files.
        
        Args:
            file_path: Path to file
            analysis_func: Function to analyze the memory-mapped file
            
        Returns:
            Analysis result
        """
        def _analyze_mmap():
            with open(file_path, 'rb') as f:
                with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:
                    return analysis_func(mm)
        
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(self.executor, _analyze_mmap)
    
    async def parallel_processing(self, 
                                 items: List[Any],
                                 process_func: Callable[[Any], Any],
                                 max_concurrent: int = None) -> List[Any]:
        """
        Process items in parallel with concurrency control.
        
        Args:
            items: List of items to process
            process_func: Function to process each item
            max_concurrent: Maximum concurrent operations
            
        Returns:
            List of results
        """
        if max_concurrent is None:
            max_concurrent = self.max_workers
        
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def _process_item(item):
            async with semaphore:
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(self.executor, process_func, item)
        
        tasks = [_process_item(item) for item in items]
        return await asyncio.gather(*tasks)


class CacheManager:
    """Advanced caching with LRU eviction and memory management."""
    
    def __init__(self, max_size: int = 1000, max_memory_mb: int = 512):
        self.max_size = max_size
        self.max_memory_mb = max_memory_mb
        self.cache = {}
        self.access_order = []
        self.memory_usage = 0
        self.hits = 0
        self.misses = 0
        self._lock = threading.RLock()
    
    def get(self, key: str) -> Optional[Any]:
        """Get item from cache."""
        with self._lock:
            if key in self.cache:
                # Move to end (most recently used)
                self.access_order.remove(key)
                self.access_order.append(key)
                self.hits += 1
                return self.cache[key]['value']
            else:
                self.misses += 1
                return None
    
    def put(self, key: str, value: Any, size_mb: float = 0.0) -> None:
        """Put item in cache with memory tracking."""
        with self._lock:
            # If key already exists, remove old entry
            if key in self.cache:
                old_size = self.cache[key]['size_mb']
                self.memory_usage -= old_size
                self.access_order.remove(key)
            
            # Check if we need to evict items
            self._evict_if_needed(size_mb)
            
            # Add new entry
            self.cache[key] = {
                'value': value,
                'size_mb': size_mb,
                'timestamp': time.time()
            }
            self.access_order.append(key)
            self.memory_usage += size_mb
    
    def _evict_if_needed(self, new_size_mb: float) -> None:
        """Evict items if cache limits are exceeded."""
        # Evict by size limit
        while len(self.cache) >= self.max_size and self.access_order:
            oldest_key = self.access_order[0]
            self._remove_item(oldest_key)
        
        # Evict by memory limit
        while (self.memory_usage + new_size_mb > self.max_memory_mb and 
               self.access_order):
            oldest_key = self.access_order[0]
            self._remove_item(oldest_key)
    
    def _remove_item(self, key: str) -> None:
        """Remove item from cache."""
        if key in self.cache:
            size_mb = self.cache[key]['size_mb']
            self.memory_usage -= size_mb
            del self.cache[key]
            self.access_order.remove(key)
    
    def clear(self) -> None:
        """Clear all cache entries."""
        with self._lock:
            self.cache.clear()
            self.access_order.clear()
            self.memory_usage = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics."""
        total_requests = self.hits + self.misses
        hit_rate = self.hits / total_requests if total_requests > 0 else 0.0
        
        return {
            'size': len(self.cache),
            'max_size': self.max_size,
            'memory_usage_mb': self.memory_usage,
            'max_memory_mb': self.max_memory_mb,
            'hits': self.hits,
            'misses': self.misses,
            'hit_rate': hit_rate,
        }


def performance_monitor(track_memory: bool = True, 
                       track_cpu: bool = True,
                       enable_profiling: bool = False):
    """
    Decorator for performance monitoring.
    
    Args:
        track_memory: Track memory usage
        track_cpu: Track CPU usage  
        enable_profiling: Enable detailed profiling
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            # Start monitoring
            start_time = time.time()
            start_memory = MemoryManager.get_current_memory_mb() if track_memory else 0
            
            # CPU monitoring
            process = psutil.Process() if track_cpu else None
            cpu_start = process.cpu_percent() if process else 0
            
            # Memory tracing
            if track_memory and TRACEMALLOC_AVAILABLE:
                tracemalloc.start()
            
            # Profiling
            profiler = None
            if enable_profiling and PROFILING_AVAILABLE:
                profiler = cProfile.Profile()
                profiler.enable()
            
            try:
                # Execute function
                result = func(*args, **kwargs)
                
                # Calculate metrics
                end_time = time.time()
                execution_time = end_time - start_time
                
                metrics = PerformanceMetrics(
                    execution_time=execution_time,
                    memory_usage_mb=0,
                    peak_memory_mb=0,
                    cpu_usage_percent=0
                )
                
                if track_memory:
                    end_memory = MemoryManager.get_current_memory_mb()
                    metrics.memory_usage_mb = end_memory - start_memory
                    
                    if TRACEMALLOC_AVAILABLE:
                        current, peak = tracemalloc.get_traced_memory()
                        metrics.peak_memory_mb = peak / 1024 / 1024
                        tracemalloc.stop()
                
                if track_cpu and process:
                    cpu_end = process.cpu_percent()
                    metrics.cpu_usage_percent = (cpu_start + cpu_end) / 2
                
                # Store metrics in function
                if not hasattr(func, '_performance_metrics'):
                    func._performance_metrics = []
                func._performance_metrics.append(metrics)
                
                # Save profiling results
                if profiler:
                    profiler.disable()
                    stats = pstats.Stats(profiler)
                    
                    # Create profile directory if it doesn't exist
                    profile_dir = Path("performance_profiles")
                    profile_dir.mkdir(exist_ok=True)
                    
                    profile_file = profile_dir / f"{func.__name__}_{int(time.time())}.prof"
                    stats.dump_stats(str(profile_file))
                
                return result
                
            except Exception as e:
                # Stop profiling on exception
                if profiler:
                    profiler.disable()
                if track_memory and TRACEMALLOC_AVAILABLE:
                    tracemalloc.stop()
                raise
        
        return wrapper
    return decorator


class ResourceManager:
    """System resource management and optimization."""
    
    def __init__(self):
        self.cpu_count = multiprocessing.cpu_count()
        self.memory_gb = psutil.virtual_memory().total / (1024**3)
        self.optimal_workers = min(self.cpu_count * 2, 32)
        
    @contextmanager
    def cpu_affinity(self, cpu_cores: List[int] = None):
        """Set CPU affinity for the current process."""
        if cpu_cores is None:
            cpu_cores = list(range(min(4, self.cpu_count)))  # Use first 4 cores
        
        process = psutil.Process()
        original_affinity = process.cpu_affinity()
        
        try:
            process.cpu_affinity(cpu_cores)
            yield
        finally:
            process.cpu_affinity(original_affinity)
    
    @contextmanager
    def memory_limit(self, limit_mb: int):
        """Set memory limit for the current process."""
        try:
            import resource
            # Set memory limit (soft limit)
            original_limit = resource.getrlimit(resource.RLIMIT_AS)
            new_limit = (limit_mb * 1024 * 1024, original_limit[1])
            resource.setrlimit(resource.RLIMIT_AS, new_limit)
            yield
        except (ImportError, OSError):
            # Resource module not available on Windows
            yield
        finally:
            try:
                resource.setrlimit(resource.RLIMIT_AS, original_limit)
            except (NameError, OSError):
                pass
    
    def optimize_for_binary_analysis(self) -> Dict[str, Any]:
        """
        Optimize system settings for binary analysis.
        
        Returns:
            Dictionary with optimization settings
        """
        settings = {
            'gc_threshold': gc.get_threshold(),
            'gc_disabled': False,
            'thread_count': self.optimal_workers,
            'memory_limit_mb': min(int(self.memory_gb * 1024 * 0.8), 8192),
            'cpu_cores': list(range(min(self.cpu_count, 8))),
        }
        
        # Optimize garbage collection for large objects
        gc.set_threshold(700, 10, 10)
        
        # Set process priority (if possible)
        try:
            process = psutil.Process()
            if hasattr(process, 'nice'):
                process.nice(-5)  # Higher priority
        except (PermissionError, AttributeError):
            pass
        
        return settings
    
    def get_system_info(self) -> Dict[str, Any]:
        """Get comprehensive system information."""
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')
        
        return {
            'cpu_count': self.cpu_count,
            'memory_total_gb': self.memory_gb,
            'memory_available_gb': memory.available / (1024**3),
            'memory_percent_used': memory.percent,
            'disk_total_gb': disk.total / (1024**3),
            'disk_free_gb': disk.free / (1024**3),
            'disk_percent_used': (disk.used / disk.total) * 100,
            'load_average': psutil.getloadavg() if hasattr(psutil, 'getloadavg') else [0, 0, 0],
            'optimal_workers': self.optimal_workers,
        }


# Global instances
_memory_manager = MemoryManager()
_cache_manager = CacheManager()
_resource_manager = ResourceManager()

# Export convenience functions
def get_memory_manager() -> MemoryManager:
    """Get global memory manager instance."""
    return _memory_manager

def get_cache_manager() -> CacheManager:
    """Get global cache manager instance.""" 
    return _cache_manager

def get_resource_manager() -> ResourceManager:
    """Get global resource manager instance."""
    return _resource_manager
```

`src/core/pipeline.py`:

```py
"""
Core pipeline module for RE-Architect.

This module defines the main reverse engineering pipeline that coordinates
the different analysis stages with advanced error handling and monitoring.
"""

import logging
import time
from pathlib import Path
from typing import Dict, List, Optional, Any

from src.core.binary_loader import BinaryLoader
from src.core.config import Config
from src.core.error_handling import (
    StructuredLogger, ErrorHandler, monitored_operation,
    ErrorSeverity, ErrorCategory
)
from src.security import SecurityValidator
from src.decompilers.decompiler_factory import DecompilerFactory
from src.analysis.static_analyzer import StaticAnalyzer
from src.analysis.dynamic_analyzer import DynamicAnalyzer
from src.analysis.data_structure_analyzer import DataStructureAnalyzer
from src.llm.function_summarizer import FunctionSummarizer
from src.test_generation.test_generator import TestGenerator

try:
    from src.optimization import ObfuscationOptimizer
    OPTIMIZATION_AVAILABLE = True
except ImportError:
    ObfuscationOptimizer = None
    OPTIMIZATION_AVAILABLE = False

logger = StructuredLogger("re-architect.pipeline")

class ReversePipeline:
    """
    Main pipeline for the reverse engineering process.
    
    This class orchestrates the entire reverse engineering workflow, from binary loading
    to test harness generation.
    """
    
    def __init__(self, config: Config):
        """
        Initialize the reverse engineering pipeline.
        
        Args:
            config: Configuration object
        """
        self.config = config
        
        # Initialize error handling and monitoring
        self.error_handler = ErrorHandler()
        
        # Other fields will be initialized when analyze() is called
        self.binary_path = None
        self.output_dir = None
        self.decompiler_name = "auto"
        self.generate_tests = False
        
        self.binary_loader = None
        self.decompiler = None
        self.static_analyzer = None
        self.dynamic_analyzer = None
        self.data_structure_analyzer = None
        self.function_summarizer = None
        self.test_generator = None
        
        self.results = {
            "metadata": {},
            "functions": {},
            "data_structures": {},
            "test_harnesses": {},
            "performance_metrics": {},
            "security_report": {},
            "error_summary": {}
        }
        
        # Set up logging context
        logger.set_context(pipeline_id=id(self))
    
    @monitored_operation(
        component="pipeline",
        category=ErrorCategory.ANALYSIS,
        severity=ErrorSeverity.HIGH
    )
    def analyze(self, binary_path, output_dir=None, decompiler="auto", generate_tests=False):
        """
        Analyze a binary file with comprehensive security and error handling.
        
        Args:
            binary_path: Path to the binary file to analyze
            output_dir: Directory to store output files, defaults to a directory next to the binary
            decompiler: Decompiler to use (ghidra, ida, binja, auto)
            generate_tests: Whether to generate test harnesses
            
        Returns:
            Dictionary containing analysis results
            
        Raises:
            SecurityError: If the binary file is unsafe
            ValueError: If parameters are invalid
        """
        try:
            # Validate and sanitize inputs using security module
            validated_binary = SecurityValidator.validate_binary_file(binary_path)
            
            if output_dir:
                validated_output = SecurityValidator.validate_output_directory(output_dir)
            else:
                # Default to a directory next to the binary
                default_output = validated_binary.parent / f"{validated_binary.stem}_analysis"
                validated_output = SecurityValidator.validate_output_directory(default_output)
            
            # Validate decompiler choice
            if decompiler not in ["ghidra", "ida", "binja", "auto", "mock"]:
                raise ValueError(f"Invalid decompiler choice: {decompiler}")
            
            # Set validated paths
            self.binary_path = validated_binary
            self.output_dir = validated_output
            self.decompiler_name = decompiler
            self.generate_tests = generate_tests
            
            # Set logging context
            logger.set_context(
                binary_path=str(self.binary_path),
                output_dir=str(self.output_dir),
                decompiler=self.decompiler_name
            )
            
            logger.info("Starting binary analysis", binary_size=self.binary_path.stat().st_size)
            
            # Run the pipeline
            return self._run()
            
        except Exception as e:
            self.error_handler.handle_error(
                exception=e,
                severity=ErrorSeverity.HIGH,
                category=ErrorCategory.ANALYSIS,
                component="pipeline.analyze",
                context={
                    "binary_path": str(binary_path),
                    "output_dir": str(output_dir) if output_dir else None,
                    "decompiler": decompiler,
                    "generate_tests": generate_tests
                },
                resolution_steps=[
                    "Verify the binary file exists and is readable",
                    "Check that the output directory is writable",
                    "Ensure the selected decompiler is installed and available",
                    "Check system resources (memory, disk space)"
                ]
            )
            raise
        
    def _run(self) -> Dict[str, Any]:
        """
        Internal method to run the pipeline after parameters are set.
        
        Returns:
            Dictionary containing analysis results
        """
        logger.info(f"Starting analysis of {self.binary_path}")
        
        # Record start time for performance metrics
        stage_times = {}
        
        # Initialize components
        self._initialize_components()
        
        # Load binary
        start_time = time.time()
        binary_info = self.binary_loader.load(self.binary_path, auto_unpack=True)
        stage_times["binary_loading"] = time.time() - start_time
        
        # Store binary metadata
        self.results["metadata"] = {
            "file_path": str(self.binary_path),
            "file_size": self.binary_path.stat().st_size,
            "architecture": binary_info.architecture.value if hasattr(binary_info.architecture, 'value') else str(binary_info.architecture),
            "compiler": binary_info.compiler.value if hasattr(binary_info.compiler, 'value') else str(binary_info.compiler),
            "entry_point": binary_info.entry_point
        }
        
        # Decompile binary
        start_time = time.time()
        decompiled_code = self.decompiler.decompile(binary_info)
        stage_times["decompilation"] = time.time() - start_time
        
        # Perform static analysis
        start_time = time.time()
        static_analysis_results = self.static_analyzer.analyze(decompiled_code)
        stage_times["static_analysis"] = time.time() - start_time
        
        # Extract functions and their details
        self.results["functions"] = static_analysis_results.functions
        
        # Analyze data structures
        start_time = time.time()
        data_structures = self.data_structure_analyzer.analyze(
            decompiled_code, 
            static_analysis_results
        )
        stage_times["data_structure_analysis"] = time.time() - start_time
        
        # Store data structure information
        self.results["data_structures"] = data_structures

        # Obfuscation optimization (iterative, post-decompilation)
        try:
            if not OPTIMIZATION_AVAILABLE or ObfuscationOptimizer is None:
                logger.warning("Optimization not available - skipping obfuscation removal")
            else:
                optimizer = ObfuscationOptimizer()
            if optimizer.is_available():
                start_time = time.time()
                report = optimizer.optimize(self.binary_path)
                stage_times["obfuscation_optimization"] = time.time() - start_time
                self.results["obfuscation_optimization"] = {
                    "iterations": report.iterations,
                    "changes_applied": report.changes_applied,
                    "passes_run": report.passes_run,
                    "details": report.details,
                }
        except Exception:
            # Non-fatal: continue pipeline if optimizer unavailable or fails
            pass
        
        # Generate function summaries with LLM if enabled
        if self.function_summarizer:
            start_time = time.time()
            for func_id, func_info in self.results["functions"].items():
                summary = self.function_summarizer.summarize(func_info)
                self.results["functions"][func_id]["summary"] = summary
            stage_times["function_summarization"] = time.time() - start_time
        
        # Generate test harnesses if requested
        if self.test_generator:
            start_time = time.time()
            test_harnesses = self.test_generator.generate(
                self.results["functions"],
                self.results["data_structures"]
            )
            stage_times["test_generation"] = time.time() - start_time
            
            # Store test harnesses
            self.results["test_harnesses"] = test_harnesses
        
        # Store performance metrics
        self.results["performance_metrics"] = stage_times
        
        # Save results to output directory
        self._save_results()
        
        logger.info("Analysis completed successfully")
        return self.results
        
        self.binary_loader = None
        self.decompiler = None
        self.static_analyzer = None
        self.dynamic_analyzer = None
        self.data_structure_analyzer = None
        self.function_summarizer = None
        self.test_generator = None
        
        self.results = {
            "metadata": {},
            "functions": {},
            "data_structures": {},
            "test_harnesses": {},
            "performance_metrics": {}
        }
    
    def _initialize_components(self):
        """Initialize all pipeline components."""
        logger.info("Initializing pipeline components...")
        
        # Initialize binary loader
        self.binary_loader = BinaryLoader()
        
        # Initialize decompiler
        decompiler_factory = DecompilerFactory()
        self.decompiler = decompiler_factory.create(self.decompiler_name)
        
        # Initialize analyzers
        self.static_analyzer = StaticAnalyzer(self.config)
        self.dynamic_analyzer = DynamicAnalyzer(self.config)
        self.data_structure_analyzer = DataStructureAnalyzer(self.config)
        
        # Initialize LLM components if enabled
        if self.config.use_llm:
            self.function_summarizer = FunctionSummarizer(self.config)
        
        # Initialize test generator if requested
        if self.generate_tests:
            self.test_generator = TestGenerator(self.config)
    
    def run(self) -> Dict[str, Any]:
        """
        Run the complete reverse engineering pipeline.
        
        Returns:
            Dictionary containing all analysis results
        """
        logger.info(f"Starting analysis of {self.binary_path}")
        
        # Record start time for performance metrics
        stage_times = {}
        
        # Initialize components
        self._initialize_components()
        
        # Load binary
        start_time = time.time()
        binary_info = self.binary_loader.load(self.binary_path, auto_unpack=True)
        stage_times["binary_loading"] = time.time() - start_time
        
        # Store binary metadata
        self.results["metadata"] = {
            "file_path": str(self.binary_path),
            "file_size": self.binary_path.stat().st_size,
            "architecture": binary_info.architecture.value if hasattr(binary_info.architecture, 'value') else str(binary_info.architecture),
            "compiler": binary_info.compiler.value if hasattr(binary_info.compiler, 'value') else str(binary_info.compiler),
            "entry_point": binary_info.entry_point
        }
        
        # Decompile binary
        start_time = time.time()
        decompiled_code = self.decompiler.decompile(binary_info)
        stage_times["decompilation"] = time.time() - start_time
        
        # Perform static analysis
        start_time = time.time()
        static_analysis_results = self.static_analyzer.analyze(decompiled_code)
        stage_times["static_analysis"] = time.time() - start_time
        
        # Extract functions and their details
        self.results["functions"] = static_analysis_results.functions
        
        # Analyze data structures
        start_time = time.time()
        data_structures = self.data_structure_analyzer.analyze(
            decompiled_code, 
            static_analysis_results
        )
        stage_times["data_structure_analysis"] = time.time() - start_time
        
        # Store data structure information
        self.results["data_structures"] = data_structures
        
        # Generate function summaries with LLM if enabled
        if self.function_summarizer:
            start_time = time.time()
            for func_id, func_info in self.results["functions"].items():
                summary = self.function_summarizer.summarize(func_info)
                self.results["functions"][func_id]["summary"] = summary
            stage_times["function_summarization"] = time.time() - start_time
        
        # Generate test harnesses if requested
        if self.test_generator:
            start_time = time.time()
            test_harnesses = self.test_generator.generate(
                self.results["functions"],
                self.results["data_structures"]
            )
            stage_times["test_generation"] = time.time() - start_time
            
            # Store test harnesses
            self.results["test_harnesses"] = test_harnesses
        
        # Store performance metrics
        self.results["performance_metrics"] = stage_times
        
        # Save results to output directory
        self._save_results()
        
        logger.info("Analysis completed successfully")
        return self.results
    
    def _save_results(self):
        """Save all results to the output directory."""
        import json
        
        # Ensure output directory exists
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Save overview JSON
        with open(self.output_dir / "results.json", "w") as f:
            json.dump(self.results, f, indent=2)
        
        # Save function summaries
        functions_dir = self.output_dir / "functions"
        functions_dir.mkdir(exist_ok=True)
        
        for func_id, func_info in self.results["functions"].items():
            func_file = functions_dir / f"{func_id}.json"
            with open(func_file, "w") as f:
                json.dump(func_info, f, indent=2)
        
        # Save data structure definitions
        data_structures_dir = self.output_dir / "data_structures"
        data_structures_dir.mkdir(exist_ok=True)
        
        for struct_id, struct_info in self.results["data_structures"].items():
            struct_file = data_structures_dir / f"{struct_id}.json"
            with open(struct_file, "w") as f:
                json.dump(struct_info, f, indent=2)
        
        # Save test harnesses if available
        if self.results["test_harnesses"]:
            tests_dir = self.output_dir / "tests"
            tests_dir.mkdir(exist_ok=True)
            
            for test_id, test_info in self.results["test_harnesses"].items():
                # Save test source code
                test_file = tests_dir / f"{test_id}.c"  # Using C as default
                with open(test_file, "w") as f:
                    f.write(test_info["source_code"])
                
                # Save test metadata
                test_meta_file = tests_dir / f"{test_id}_meta.json"
                with open(test_meta_file, "w") as f:
                    json.dump(
                        {k: v for k, v in test_info.items() if k != "source_code"}, 
                        f, 
                        indent=2
                    )
        
        logger.info(f"Results saved to {self.output_dir}")
```

`src/decompilers/__init__.py`:

```py
"""
Decompilers package for RE-Architect.
"""

from .base_decompiler import BaseDecompiler, DecompiledCode, DecompiledFunction
from .decompiler_factory import DecompilerFactory
from .ghidra_decompiler import GhidraDecompiler
from .ida_decompiler import IDADecompiler
from .binary_ninja_decompiler import BinaryNinjaDecompiler
from .mock_decompiler import MockDecompiler
from .internal_ir_decompiler import InternalIRDecompiler

__all__ = [
    'BaseDecompiler',
    'DecompiledCode',
    'DecompiledFunction',
    'DecompilerFactory',
    'GhidraDecompiler',
    'IDADecompiler',
    'BinaryNinjaDecompiler',
    'MockDecompiler',
    'InternalIRDecompiler'
]

```

`src/decompilers/base_decompiler.py`:

```py
"""
Base decompiler interface for RE-Architect.

This module defines the base interface for all decompilers.
"""

import abc
import logging
from pathlib import Path
from typing import Dict, List, Optional, Union

from src.core.binary_loader import BinaryInfo

logger = logging.getLogger("re-architect.decompilers.base")

class DecompiledFunction:
    """
    Container for a decompiled function.
    
    This class represents a single decompiled function with its code and metadata.
    """
    
    def __init__(self, address: int, name: str, code: str, signature: str = None):
        """
        Initialize decompiled function.
        
        Args:
            address: Function start address
            name: Function name
            code: Decompiled function code
            signature: Function signature if available
        """
        self.address = address
        self.name = name
        self.code = code
        self.signature = signature
        self.calls = []  # List of functions called by this function
        self.called_by = []  # List of functions that call this function
        self.metadata = {}  # Additional metadata
        
    def add_call(self, target_address: int, target_name: str):
        """
        Add a function call.
        
        Args:
            target_address: Address of called function
            target_name: Name of called function
        """
        self.calls.append((target_address, target_name))
        
    def add_called_by(self, source_address: int, source_name: str):
        """
        Add a function that calls this function.
        
        Args:
            source_address: Address of calling function
            source_name: Name of calling function
        """
        self.called_by.append((source_address, source_name))
        
    def add_metadata(self, key: str, value):
        """
        Add metadata to the function.
        
        Args:
            key: Metadata key
            value: Metadata value
        """
        self.metadata[key] = value
        
    def __str__(self) -> str:
        """
        String representation of the function.
        
        Returns:
            Summary string
        """
        return f"DecompiledFunction({self.name} @ {hex(self.address)})"

class DecompiledCode:
    """
    Container for decompiled code and related information.
    
    This class stores the decompiled code and associated metadata
    from a decompilation process.
    """
    
    def __init__(self, binary_info: BinaryInfo):
        """
        Initialize decompiled code container.
        
        Args:
            binary_info: Information about the decompiled binary
        """
        self.binary_info = binary_info
        self.functions = {}  # Function address -> decompiled code
        self.function_names = {}  # Function address -> function name
        self.function_metadata = {}  # Function address -> metadata dict
        self.data_segments = {}  # Start address -> (data, size, name)
        self.strings = {}  # Address -> string value
        self.comments = {}  # Address -> comment
        self.types = {}  # Type name -> type definition
        
    def add_function(self, address: int, code: str, name: str, metadata: Dict = None):
        """
        Add a decompiled function.
        
        Args:
            address: Function start address
            code: Decompiled function code
            name: Function name
            metadata: Additional function metadata
        """
        self.functions[address] = code
        self.function_names[address] = name
        self.function_metadata[address] = metadata or {}
        
    def add_data_segment(self, address: int, data: bytes, size: int, name: str):
        """
        Add a data segment.
        
        Args:
            address: Start address
            data: Raw data bytes
            size: Size of the segment
            name: Segment name
        """
        self.data_segments[address] = (data, size, name)
        
    def add_string(self, address: int, value: str):
        """
        Add a string constant.
        
        Args:
            address: String address
            value: String value
        """
        self.strings[address] = value
        
    def add_comment(self, address: int, comment: str):
        """
        Add a comment.
        
        Args:
            address: Comment address
            comment: Comment text
        """
        self.comments[address] = comment
        
    def add_type(self, name: str, definition: str):
        """
        Add a type definition.
        
        Args:
            name: Type name
            definition: Type definition
        """
        self.types[name] = definition
        
    def get_function_count(self) -> int:
        """
        Get the number of decompiled functions.
        
        Returns:
            Number of functions
        """
        return len(self.functions)
        
    def __str__(self) -> str:
        """
        String representation of decompiled code.
        
        Returns:
            Summary string
        """
        return (
            f"DecompiledCode(binary={self.binary_info.path.name}, "
            f"functions={len(self.functions)}, "
            f"data_segments={len(self.data_segments)}, "
            f"strings={len(self.strings)}, "
            f"types={len(self.types)})"
        )

class BaseDecompiler(abc.ABC):
    """
    Base interface for all decompilers.
    
    This abstract class defines the common interface that all decompiler
    implementations must provide.
    """
    
    def __init__(self):
        """Initialize the decompiler."""
        self.name = self.__class__.__name__
        self.logger = logging.getLogger(f"re-architect.decompilers.{self.name.lower()}")
    
    @abc.abstractmethod
    def is_available(self) -> bool:
        """
        Check if the decompiler is available for use.
        
        Returns:
            True if the decompiler is available, False otherwise
        """
        pass
    
    @abc.abstractmethod
    def decompile(self, binary_info: BinaryInfo) -> DecompiledCode:
        """
        Decompile a binary file.
        
        Args:
            binary_info: Information about the binary to decompile
            
        Returns:
            DecompiledCode object containing decompilation results
            
        Raises:
            RuntimeError: If decompilation fails
        """
        pass
    
    @abc.abstractmethod
    def get_decompiler_info(self) -> Dict:
        """
        Get information about the decompiler.
        
        Returns:
            Dictionary containing decompiler information
        """
        pass
    
    def __str__(self) -> str:
        """
        String representation of the decompiler.
        
        Returns:
            Decompiler name
        """
        return self.name

```

`src/decompilers/binary_ninja_decompiler.py`:

```py
"""
Binary Ninja decompiler implementation for RE-Architect.

This module provides the integration with Binary Ninja for decompilation.
"""

import logging
import os
import json
import tempfile
from pathlib import Path
from typing import Dict, List, Optional, Any

from src.core.binary_loader import BinaryInfo
from src.decompilers.base_decompiler import BaseDecompiler, DecompiledCode, DecompiledFunction

logger = logging.getLogger("re-architect.decompilers.binary_ninja")

class BinaryNinjaDecompiler(BaseDecompiler):
    """
    Binary Ninja decompiler implementation.
    
    This class provides integration with Binary Ninja using its Python API.
    """
    
    def __init__(self, binja_path: Optional[str] = None):
        """
        Initialize the Binary Ninja decompiler.
        
        Args:
            binja_path: Path to Binary Ninja installation directory (optional)
        """
        super().__init__()
        self.name = "BinaryNinjaDecompiler"
        
        # Try to find Binary Ninja path if not provided
        self.binja_path = binja_path or self._find_binja_path()
        
        # Try to import binaryninja module
        self.binja_available = False
        try:
            if self.binja_path:
                import sys
                sys.path.append(os.path.join(self.binja_path, "python"))
            
            import binaryninja
            self.binaryninja = binaryninja
            self.binja_available = True
            logger.info("Binary Ninja Python API available")
        except ImportError as e:
            logger.warning(f"Binary Ninja Python API not available: {e}")
            self.binaryninja = None
        
        # Cache decompiler info
        self._decompiler_info = None
    
    def _find_binja_path(self) -> Optional[str]:
        """
        Find the Binary Ninja installation directory.
        
        Looks for Binary Ninja in common installation locations.
        
        Returns:
            Path to Binary Ninja installation directory, or None if not found
        """
        # Check environment variable
        if "BINARYNINJADIR" in os.environ:
            path = os.environ["BINARYNINJADIR"]
            if os.path.exists(path):
                return path
        
        # Check common installation locations
        common_paths = []
        
        if os.name == "nt":  # Windows
            common_paths.extend([
                "C:/Program Files/Vector35/BinaryNinja",
                "C:/Program Files (x86)/Vector35/BinaryNinja",
                "C:/BinaryNinja",
                os.path.expanduser("~/BinaryNinja")
            ])
        elif os.name == "posix":  # Unix-like
            if "darwin" in os.sys.platform:  # macOS
                common_paths.extend([
                    "/Applications/Binary Ninja.app",
                    "/Applications/Binary Ninja.app/Contents/MacOS",
                    os.path.expanduser("~/Applications/Binary Ninja.app")
                ])
            else:  # Linux
                common_paths.extend([
                    "/opt/binaryninja",
                    "/usr/local/binaryninja", 
                    os.path.expanduser("~/binaryninja"),
                    os.path.expanduser("~/Binary Ninja")
                ])
        
        for path in common_paths:
            if os.path.exists(path):
                # Check for python directory
                python_dir = os.path.join(path, "python")
                if os.path.exists(python_dir):
                    return path
        
        return None
    
    def is_available(self) -> bool:
        """
        Check if Binary Ninja is available on the system.
        
        Returns:
            True if Binary Ninja is available, False otherwise
        """
        return self.binja_available and self.binaryninja is not None
    
    def decompile(self, binary_info: BinaryInfo) -> DecompiledCode:
        """
        Decompile a binary using Binary Ninja.
        
        Args:
            binary_info: Information about the binary to decompile
            
        Returns:
            Object containing the decompiled code
            
        Raises:
            RuntimeError: If decompilation fails or Binary Ninja is not available
        """
        if not self.is_available():
            raise RuntimeError("Binary Ninja is not available")
        
        logger.info(f"Decompiling {binary_info.path} using Binary Ninja")
        
        try:
            # Open the binary
            bv = self.binaryninja.open_view(str(binary_info.path))
            if not bv:
                raise RuntimeError("Failed to open binary in Binary Ninja")
            
            # Wait for analysis to complete
            bv.update_analysis_and_wait()
            
            # Create DecompiledCode object
            decompiled_code = DecompiledCode(binary_info)
            
            # Export functions
            self._export_functions(bv, decompiled_code)
            
            # Export strings
            self._export_strings(bv, decompiled_code)
            
            # Export data types
            self._export_data_types(bv, decompiled_code)
            
            return decompiled_code
            
        except Exception as e:
            logger.exception(f"Error during Binary Ninja decompilation: {e}")
            raise RuntimeError(f"Binary Ninja decompilation failed: {str(e)}")
    
    def _export_functions(self, bv, decompiled_code: DecompiledCode):
        """
        Export functions from Binary Ninja.
        
        Args:
            bv: Binary Ninja BinaryView object
            decompiled_code: DecompiledCode object to populate
        """
        logger.info("Exporting functions from Binary Ninja")
        
        count = 0
        for func in bv.functions:
            try:
                # Get basic function information
                address = func.start
                name = func.name
                
                # Get decompiled code using high-level IL
                hlil = func.hlil
                if hlil and hlil.root:
                    # Convert HLIL to pseudo-C code
                    code_lines = []
                    
                    # Add function signature
                    return_type = str(func.return_type) if func.return_type else "void"
                    params = []
                    for param in func.parameter_vars:
                        param_type = str(param.type) if param.type else "int"
                        params.append(f"{param_type} {param.name}")
                    
                    signature = f"{return_type} {name}({', '.join(params)})"
                    code_lines.append(signature + " {")
                    
                    # Add HLIL representation
                    for instruction in hlil.instructions:
                        code_lines.append(f"    {instruction}")
                    
                    code_lines.append("}")
                    code = "\\n".join(code_lines)
                else:
                    # Fallback to disassembly if no HLIL
                    code_lines = [f"// Disassembly for {name}"]
                    for basic_block in func.basic_blocks:
                        for instruction in basic_block:
                            addr_str = f"0x{instruction.address:x}"
                            code_lines.append(f"{addr_str}: {instruction}")
                    code = "\\n".join(code_lines)
                
                # Extract metadata
                metadata = {
                    "signature": signature if 'signature' in locals() else "",
                    "return_type": str(func.return_type) if func.return_type else "unknown",
                    "parameters": [
                        {
                            "name": param.name,
                            "type": str(param.type) if param.type else "unknown"
                        }
                        for param in func.parameter_vars
                    ],
                    "calls": [
                        {
                            "address": f"0x{ref.address:x}",
                            "name": bv.get_function_at(ref.address).name if bv.get_function_at(ref.address) else "unknown"
                        }
                        for ref in func.call_sites
                    ],
                    "size": len(func),
                    "basic_blocks": len(func.basic_blocks),
                    "calling_convention": str(func.calling_convention) if func.calling_convention else "unknown"
                }
                
                decompiled_code.add_function(address, code, name, metadata)
                count += 1
                
            except Exception as e:
                logger.warning(f"Failed to process function {func.name}: {e}")
        
        logger.info(f"Exported {count} functions")
    
    def _export_strings(self, bv, decompiled_code: DecompiledCode):
        """
        Export strings from Binary Ninja.
        
        Args:
            bv: Binary Ninja BinaryView object
            decompiled_code: DecompiledCode object to populate
        """
        logger.info("Exporting strings from Binary Ninja")
        
        count = 0
        for string in bv.strings:
            try:
                address = string.start
                value = string.value
                decompiled_code.add_string(address, value)
                count += 1
            except Exception as e:
                logger.warning(f"Failed to process string at 0x{string.start:x}: {e}")
        
        logger.info(f"Exported {count} strings")
    
    def _export_data_types(self, bv, decompiled_code: DecompiledCode):
        """
        Export data types from Binary Ninja.
        
        Args:
            bv: Binary Ninja BinaryView object  
            decompiled_code: DecompiledCode object to populate
        """
        logger.info("Exporting data types from Binary Ninja")
        
        count = 0
        for name, type_obj in bv.types.items():
            try:
                # Convert Binary Ninja type to C-like definition
                definition = str(type_obj)
                decompiled_code.add_type(name, definition)
                count += 1
            except Exception as e:
                logger.warning(f"Failed to process type {name}: {e}")
        
        logger.info(f"Exported {count} data types")
    
    def get_decompiler_info(self) -> Dict:
        """
        Get information about the Binary Ninja decompiler.
        
        Returns:
            Dictionary containing decompiler information
        """
        if self._decompiler_info is not None:
            return self._decompiler_info
        
        info = {
            "name": self.name,
            "available": self.is_available(),
            "path": self.binja_path,
            "version": "unknown"
        }
        
        # Try to get version information
        if self.is_available():
            try:
                info["version"] = self.binaryninja.version()
                info["build_id"] = getattr(self.binaryninja, "build_id", "unknown")
            except Exception as e:
                logger.warning(f"Error getting Binary Ninja version: {e}")
        
        self._decompiler_info = info
        return info
        
    def get_decompiler_info(self) -> Dict:
        """
        Get information about the decompiler.
        
        Returns:
            Dictionary containing decompiler information
        """
        return {
            "name": self.name,
            "version": "Not available",
            "capabilities": []
        }
```

`src/decompilers/decompiler_factory.py`:

```py
"""
Decompiler factory for RE-Architect.

This module provides a factory for creating decompiler instances.
"""

import logging
from typing import Optional

from src.core.binary_loader import BinaryInfo
from src.decompilers.base_decompiler import BaseDecompiler
from src.decompilers.ghidra_decompiler import GhidraDecompiler
from src.decompilers.ida_decompiler import IDADecompiler
from src.decompilers.binary_ninja_decompiler import BinaryNinjaDecompiler
from src.decompilers.mock_decompiler import MockDecompiler
from src.decompilers.internal_ir_decompiler import InternalIRDecompiler

logger = logging.getLogger("re-architect.decompilers.factory")

class DecompilerFactory:
    """
    Factory for creating decompiler instances.
    
    This class handles creating the appropriate decompiler instance based on the
    requested decompiler name or binary information.
    """
    
    def get_decompiler(self, decompiler_name: str = "auto") -> BaseDecompiler:
        """
        Get a decompiler instance.
        
        Args:
            decompiler_name: Name of the decompiler to create (ghidra, ida, binja, auto)
            
        Returns:
            Initialized decompiler instance
            
        Raises:
            ValueError: If the requested decompiler is not supported
        """
        return self.create(decompiler_name)
    
    def create(self, decompiler_name: str = "auto") -> BaseDecompiler:
        """
        Create a decompiler instance.
        
        Args:
            decompiler_name: Name of the decompiler to create (ghidra, ida, binja, auto)
            
        Returns:
            Initialized decompiler instance
            
        Raises:
            ValueError: If the requested decompiler is not supported
        """
        decompiler_name = decompiler_name.lower()
        
        if decompiler_name == "ghidra":
            logger.info("Creating Ghidra decompiler")
            return GhidraDecompiler()
        elif decompiler_name == "ida" or decompiler_name == "ida_pro":
            logger.info("Creating IDA Pro decompiler")
            return IDADecompiler()
        elif decompiler_name == "binja" or decompiler_name == "binary_ninja":
            logger.info("Creating Binary Ninja decompiler")
            return BinaryNinjaDecompiler()
        elif decompiler_name == "mock":
            logger.info("Creating Mock decompiler for testing")
            return MockDecompiler()
        elif decompiler_name == "internal" or decompiler_name == "ir":
            logger.info("Creating Internal IR decompiler")
            return InternalIRDecompiler()
        elif decompiler_name == "auto":
            # We'll pick one based on availability later when we have binary info
            logger.info("Creating auto-selected decompiler (will choose when binary is available)")
            return self._create_auto_decompiler()
        else:
            logger.error(f"Unsupported decompiler: {decompiler_name}")
            raise ValueError(f"Unsupported decompiler: {decompiler_name}")
    
    def _create_auto_decompiler(self) -> BaseDecompiler:
        """
        Create an automatically selected decompiler.
        
        Returns:
            Decompiler instance (defaults to Ghidra if available)
        """
        # Try to create decompilers in order of preference
        for decompiler_class in [InternalIRDecompiler, GhidraDecompiler, IDADecompiler, BinaryNinjaDecompiler]:
            try:
                decompiler = decompiler_class()
                if decompiler.is_available():
                    logger.info(f"Auto-selected decompiler: {decompiler.name}")
                    return decompiler
            except Exception as e:
                logger.debug(f"Error creating {decompiler_class.__name__}: {e}")
        
        # If we get here, none of the decompilers are available
        # Default to Ghidra (which will handle the error on actual decompilation)
        logger.warning("No available decompilers found, defaulting to Ghidra")
        return GhidraDecompiler()

```

`src/decompilers/ghidra_decompiler.py`:

```py
"""
Ghidra decompiler integration for RE-Architect.

This module provides integration with the Ghidra decompiler.
"""

import logging
import os
import subprocess
import tempfile
import time
from pathlib import Path
from typing import Dict, List, Optional, Union, Any
import json

from src.core.binary_loader import BinaryInfo
from src.decompilers.base_decompiler import BaseDecompiler, DecompiledCode

logger = logging.getLogger("re-architect.decompilers.ghidra")

class GhidraDecompiler(BaseDecompiler):
    """
    Ghidra decompiler integration.
    
    This class provides integration with the Ghidra decompiler,
    using Ghidra's headless analyzer and decompiler.
    """
    
    def __init__(self, ghidra_path: Optional[str] = None):
        """
        Initialize the Ghidra decompiler.
        
        Args:
            ghidra_path: Path to Ghidra installation directory (optional)
        """
        super().__init__()
        self.name = "GhidraDecompiler"
        
        # Try to find Ghidra path if not provided
        self.ghidra_path = ghidra_path or self._find_ghidra_path()
        
        # Cache decompiler info
        self._decompiler_info = None
    
    def _find_ghidra_path(self) -> Optional[str]:
        """
        Find the Ghidra installation directory.
        
        Looks for Ghidra in common installation locations or via environment variables.
        
        Returns:
            Path to Ghidra installation directory, or None if not found
        """
        # Check environment variable
        if "GHIDRA_INSTALL_DIR" in os.environ:
            path = os.environ["GHIDRA_INSTALL_DIR"]
            if os.path.exists(path):
                return path
        
        # Check common installation locations
        common_paths = [
            # Windows paths
            "C:/Program Files/Ghidra",
            "C:/Ghidra",
            os.path.expanduser("~/Ghidra"),
            
            # Unix paths
            "/opt/ghidra",
            "/usr/local/ghidra",
            os.path.expanduser("~/ghidra")
        ]
        
        for base_path in common_paths:
            if os.path.exists(base_path):
                # Look for support/analyzeHeadless script
                if os.path.exists(os.path.join(base_path, "support", "analyzeHeadless")):
                    return base_path
                
                # Check subdirectories (for versioned installs)
                for item in os.listdir(base_path):
                    sub_path = os.path.join(base_path, item)
                    if os.path.isdir(sub_path) and os.path.exists(os.path.join(sub_path, "support", "analyzeHeadless")):
                        return sub_path
        
        return None
    
    def is_available(self) -> bool:
        """
        Check if Ghidra is available for use.
        
        Returns:
            True if Ghidra is available, False otherwise
        """
        if not self.ghidra_path:
            logger.warning("Ghidra path not found")
            return False
        
        headless_script = self._get_headless_script_path()
        if not os.path.exists(headless_script):
            logger.warning(f"Ghidra headless script not found at {headless_script}")
            return False
        
        return True
    
    def _get_headless_script_path(self) -> str:
        """
        Get the path to the Ghidra headless script.
        
        Returns:
            Path to the analyzeHeadless script
        """
        if os.name == "nt":  # Windows
            return os.path.join(self.ghidra_path, "support", "analyzeHeadless.bat")
        else:  # Unix-like
            return os.path.join(self.ghidra_path, "support", "analyzeHeadless")
    
    def decompile(self, binary_info: BinaryInfo) -> DecompiledCode:
        """
        Decompile a binary file using Ghidra.
        
        Args:
            binary_info: Information about the binary to decompile
            
        Returns:
            DecompiledCode object containing decompilation results
            
        Raises:
            RuntimeError: If decompilation fails
        """
        if not self.is_available():
            raise RuntimeError("Ghidra is not available")
        
        logger.info(f"Decompiling {binary_info.path} using Ghidra")
        
        # Create a temporary project directory
        with tempfile.TemporaryDirectory(prefix="re-architect-ghidra-") as temp_dir:
            project_dir = os.path.join(temp_dir, "project")
            project_name = "re-architect"
            
            # Import security module
            from src.security import SecurityValidator
            
            # Create output directory securely
            output_dir = SecurityValidator.validate_output_directory(
                os.path.join(temp_dir, "output"), 
                create_if_missing=True
            )
            
            # Path to the export script
            export_script = self._create_export_script(temp_dir, str(output_dir))
            
            # Run Ghidra headless analyzer
            headless_script = self._get_headless_script_path()
            
            # Validate binary path
            validated_binary = SecurityValidator.validate_binary_file(binary_info.path)
            
            cmd = [
                headless_script,
                project_dir,
                project_name,
                "-import", str(validated_binary),
                "-postScript", export_script,
                "-scriptPath", temp_dir,
                "-deleteProject"
            ]
            
            logger.debug(f"Running Ghidra command (sanitized): {SecurityValidator._sanitize_command_for_logging(cmd)}")
            
            try:
                # Use secure subprocess execution
                result = SecurityValidator.safe_subprocess_run(
                    cmd,
                    timeout=600,  # 10-minute timeout
                    cwd=temp_dir
                )
                
                # Check if execution was successful
                if result.returncode != 0:
                    logger.error(f"Ghidra decompilation failed with code {result.returncode}")
                    if result.stdout:
                        logger.error(f"Stdout: {result.stdout}")
                    if result.stderr:
                        logger.error(f"Stderr: {result.stderr}")
                    raise RuntimeError(f"Ghidra decompilation failed with code {result.returncode}")
                
                # Parse the output files
                return self._parse_output(binary_info, str(output_dir))
                
            except Exception as e:
                logger.exception(f"Error running Ghidra: {e}")
                raise RuntimeError(f"Error running Ghidra: {str(e)}")
    
    def _create_export_script(self, script_dir: str, output_dir: str) -> str:
        """
        Create a Ghidra script to export decompiled code.
        
        Args:
            script_dir: Directory to write the script to
            output_dir: Directory to write the output to
            
        Returns:
            Path to the created script
        """
        script_path = os.path.join(script_dir, "ExportDecompiledCode.java")
        
        script_content = f"""
import ghidra.app.script.GhidraScript;
import ghidra.program.model.listing.*;
import ghidra.program.model.pcode.*;
import ghidra.program.model.symbol.*;
import ghidra.program.model.data.*;
import ghidra.program.model.address.*;
import ghidra.util.task.TaskMonitor;
import java.io.*;
import java.util.*;
import com.google.gson.*;

public class ExportDecompiledCode extends GhidraScript {{
    private static final String OUTPUT_DIR = "{output_dir.replace(os.sep, '/')}";
    
    
    @Override
    public void run() throws Exception {{
        println("Starting decompilation export script");
        
        // Create output directory if it doesn't exist
        File outDir = new File(OUTPUT_DIR);
        if (!outDir.exists()) {{
            outDir.mkdirs();
        }}
        
        // Export program info
        exportProgramInfo();
        
        // Export functions
        exportFunctions();
        
        // Export strings
        exportStrings();
        
        // Export data types
        exportDataTypes();
        
        println("Export complete");
    }}
    
    private void exportProgramInfo() throws Exception {{
        println("Exporting program info");
        
        JsonObject json = new JsonObject();
        json.addProperty("name", currentProgram.getName());
        json.addProperty("language", currentProgram.getLanguage().getLanguageID().toString());
        json.addProperty("compiler", currentProgram.getCompiler());
        json.addProperty("creationDate", new Date().toString());
        
        // Add memory layout info
        JsonArray memoryArray = new JsonArray();
        for (MemoryBlock block : currentProgram.getMemory().getBlocks()) {{
            JsonObject memBlock = new JsonObject();
            memBlock.addProperty("name", block.getName());
            memBlock.addProperty("start", block.getStart().toString());
            memBlock.addProperty("end", block.getEnd().toString());
            memBlock.addProperty("size", block.getSize());
            memBlock.addProperty("readable", block.isRead());
            memBlock.addProperty("writable", block.isWrite());
            memBlock.addProperty("executable", block.isExecute());
            memoryArray.add(memBlock);
        }}
        json.add("memoryBlocks", memoryArray);
        
        // Write to file
        try (FileWriter writer = new FileWriter(new File(OUTPUT_DIR, "program_info.json"))) {{
            writer.write(json.toString());
        }}
    }}
    
    private void exportFunctions() throws Exception {{
        println("Exporting functions");
        
        // Create functions directory
        File functionsDir = new File(OUTPUT_DIR, "functions");
        if (!functionsDir.exists()) {{
            functionsDir.mkdirs();
        }}
        
        DecompileOptions options = new DecompileOptions();
        DecompInterface decompInterface = new DecompInterface();
        decompInterface.setOptions(options);
        
        if (!decompInterface.openProgram(currentProgram)) {{
            println("Decompiler error: " + decompInterface.getLastMessage());
            return;
        }}
        
        // Create a JSON array for all functions
        JsonArray allFunctionsArray = new JsonArray();
        
        // Process all functions
        FunctionIterator functions = currentProgram.getFunctionManager().getFunctions(true);
        int count = 0;
        
        for (Function function : functions) {{
            monitor.checkCancelled();
            
            Address entryPoint = function.getEntryPoint();
            String address = entryPoint.toString();
            String name = function.getName();
            
            println("Decompiling function: " + name + " at " + address);
            
            // Create JSON object for function info
            JsonObject functionJson = new JsonObject();
            functionJson.addProperty("address", address);
            functionJson.addProperty("name", name);
            functionJson.addProperty("signature", function.getSignature().toString());
            functionJson.addProperty("entryPoint", entryPoint.toString());
            
            // Add parameter info
            JsonArray paramsArray = new JsonArray();
            for (Parameter param : function.getParameters()) {{
                JsonObject paramJson = new JsonObject();
                paramJson.addProperty("name", param.getName());
                paramJson.addProperty("dataType", param.getDataType().toString());
                paramJson.addProperty("length", param.getLength());
                paramJson.addProperty("ordinal", param.getOrdinal());
                paramsArray.add(paramJson);
            }}
            functionJson.add("parameters", paramsArray);
            
            // Add return type info
            functionJson.addProperty("returnType", function.getReturnType().toString());
            
            // Add calling convention
            functionJson.addProperty("callingConvention", function.getCallingConvention().toString());
            
            // Add references (calls made by this function)
            JsonArray referencesArray = new JsonArray();
            ReferenceIterator refs = currentProgram.getReferenceManager().getReferences(function.getBody());
            for (Reference ref : refs) {{
                if (ref.getReferenceType().isCall()) {{
                    Function calledFunction = currentProgram.getFunctionManager().getFunctionAt(ref.getToAddress());
                    if (calledFunction != null) {{
                        JsonObject refJson = new JsonObject();
                        refJson.addProperty("fromAddress", ref.getFromAddress().toString());
                        refJson.addProperty("toAddress", ref.getToAddress().toString());
                        refJson.addProperty("toFunction", calledFunction.getName());
                        referencesArray.add(refJson);
                    }}
                }}
            }}
            functionJson.add("calls", referencesArray);
            
            // Decompile the function
            DecompileResults results = decompInterface.decompileFunction(function, 30, monitor);
            if (results.decompileCompleted()) {{
                // Get the C code
                ClangTokenGroup tokens = results.getCCodeMarkup();
                String code = tokens.toString();
                functionJson.addProperty("code", code);
                
                // Export to individual file
                String safeAddress = address.replace(":", "_");
                try (FileWriter writer = new FileWriter(new File(functionsDir, safeAddress + ".c"))) {{
                    writer.write(code);
                }}
            }} else {{
                println("Failed to decompile function: " + name);
                functionJson.addProperty("decompilationError", results.getErrorMessage());
            }}
            
            allFunctionsArray.add(functionJson);
            count++;
        }}
        
        println("Decompiled " + count + " functions");
        
        // Write all function info to a single file
        try (FileWriter writer = new FileWriter(new File(OUTPUT_DIR, "functions.json"))) {{
            writer.write(allFunctionsArray.toString());
        }}
    }}
    
    private void exportStrings() throws Exception {{
        println("Exporting strings");
        
        DataIterator dataIterator = currentProgram.getListing().getDefinedData(true);
        JsonArray stringsArray = new JsonArray();
        int count = 0;
        
        while (dataIterator.hasNext()) {{
            Data data = dataIterator.next();
            if (data.isString()) {{
                monitor.checkCancelled();
                
                JsonObject stringJson = new JsonObject();
                stringJson.addProperty("address", data.getAddress().toString());
                stringJson.addProperty("value", data.getValue().toString());
                stringJson.addProperty("length", data.getLength());
                stringJson.addProperty("dataType", data.getDataType().getName());
                
                stringsArray.add(stringJson);
                count++;
            }}
        }}
        
        println("Found " + count + " strings");
        
        // Write strings to file
        try (FileWriter writer = new FileWriter(new File(OUTPUT_DIR, "strings.json"))) {{
            writer.write(stringsArray.toString());
        }}
    }}
    
    private void exportDataTypes() throws Exception {{
        println("Exporting data types");
        
        DataTypeManager dtm = currentProgram.getDataTypeManager();
        JsonArray typesArray = new JsonArray();
        int count = 0;
        
        // Export structures
        CategoryPath structPath = new CategoryPath("/Structure");
        if (dtm.containsCategory(structPath)) {{
            Category structCategory = dtm.getCategory(structPath);
            exportCategory(structCategory, typesArray);
            count += typesArray.size();
        }}
        
        println("Exported " + count + " data types");
        
        // Write data types to file
        try (FileWriter writer = new FileWriter(new File(OUTPUT_DIR, "data_types.json"))) {{
            writer.write(typesArray.toString());
        }}
    }}
    
    private void exportCategory(Category category, JsonArray typesArray) {{
        for (DataType dt : category.getDataTypes()) {{
            if (dt instanceof Structure) {{
                Structure struct = (Structure) dt;
                JsonObject structJson = new JsonObject();
                structJson.addProperty("name", struct.getName());
                structJson.addProperty("path", struct.getCategoryPath().getPath());
                structJson.addProperty("size", struct.getLength());
                
                JsonArray fieldsArray = new JsonArray();
                for (int i = 0; i < struct.getNumComponents(); i++) {{
                    DataTypeComponent component = struct.getComponent(i);
                    JsonObject fieldJson = new JsonObject();
                    fieldJson.addProperty("name", component.getFieldName());
                    fieldJson.addProperty("dataType", component.getDataType().getName());
                    fieldJson.addProperty("offset", component.getOffset());
                    fieldJson.addProperty("size", component.getLength());
                    fieldsArray.add(fieldJson);
                }}
                structJson.add("fields", fieldsArray);
                
                typesArray.add(structJson);
            }}
        }}
        
        // Process subcategories
        for (Category subCategory : category.getCategories()) {{
            exportCategory(subCategory, typesArray);
        }}
    }}
}}
"""
        
        with open(script_path, "w") as f:
            f.write(script_content)
        
        return script_path
    
    def _parse_output(self, binary_info: BinaryInfo, output_dir: str) -> DecompiledCode:
        """
        Parse the output files from Ghidra.
        
        Args:
            binary_info: Information about the decompiled binary
            output_dir: Directory containing the output files
            
        Returns:
            DecompiledCode object containing decompilation results
        """
        logger.info("Parsing Ghidra output")
        
        decompiled_code = DecompiledCode(binary_info)
        
        # Parse program info
        program_info_file = os.path.join(output_dir, "program_info.json")
        if os.path.exists(program_info_file):
            with open(program_info_file, "r") as f:
                program_info = json.load(f)
                logger.debug(f"Loaded program info: {program_info}")
        
        # Parse functions
        functions_file = os.path.join(output_dir, "functions.json")
        if os.path.exists(functions_file):
            with open(functions_file, "r") as f:
                functions_data = json.load(f)
                
                for func_data in functions_data:
                    address = int(func_data["address"].split(":")[-1], 16)
                    name = func_data["name"]
                    
                    # Get the function code
                    if "code" in func_data:
                        code = func_data["code"]
                    else:
                        # Try to load from individual file
                        safe_address = func_data["address"].replace(":", "_")
                        func_file = os.path.join(output_dir, "functions", f"{safe_address}.c")
                        if os.path.exists(func_file):
                            with open(func_file, "r") as func_f:
                                code = func_f.read()
                        else:
                            code = "// Decompilation failed"
                    
                    # Extract metadata
                    metadata = {
                        "signature": func_data.get("signature", ""),
                        "returnType": func_data.get("returnType", ""),
                        "callingConvention": func_data.get("callingConvention", ""),
                        "parameters": func_data.get("parameters", []),
                        "calls": func_data.get("calls", []),
                        "decompilationError": func_data.get("decompilationError", None)
                    }
                    
                    decompiled_code.add_function(address, code, name, metadata)
                
                logger.info(f"Loaded {len(functions_data)} functions")
        
        # Parse strings
        strings_file = os.path.join(output_dir, "strings.json")
        if os.path.exists(strings_file):
            with open(strings_file, "r") as f:
                strings_data = json.load(f)
                
                for string_data in strings_data:
                    address = int(string_data["address"].split(":")[-1], 16)
                    value = string_data["value"]
                    decompiled_code.add_string(address, value)
                
                logger.info(f"Loaded {len(strings_data)} strings")
        
        # Parse data types
        data_types_file = os.path.join(output_dir, "data_types.json")
        if os.path.exists(data_types_file):
            with open(data_types_file, "r") as f:
                types_data = json.load(f)
                
                for type_data in types_data:
                    name = type_data["name"]
                    # Convert to C-like structure definition
                    definition = self._convert_to_c_struct(type_data)
                    decompiled_code.add_type(name, definition)
                
                logger.info(f"Loaded {len(types_data)} data types")
        
        return decompiled_code
    
    def _convert_to_c_struct(self, struct_data: Dict) -> str:
        """
        Convert a struct definition from Ghidra's JSON format to C code.
        
        Args:
            struct_data: Structure data from Ghidra
            
        Returns:
            C structure definition
        """
        name = struct_data["name"]
        fields = struct_data.get("fields", [])
        
        lines = [f"struct {name} {{"]
        
        for field in fields:
            field_name = field.get("name", "field")
            field_type = field.get("dataType", "undefined")
            lines.append(f"    {field_type} {field_name};")
        
        lines.append("};")
        
        return "\n".join(lines)
    
    def get_decompiler_info(self) -> Dict:
        """
        Get information about the Ghidra decompiler.
        
        Returns:
            Dictionary containing decompiler information
        """
        if self._decompiler_info is not None:
            return self._decompiler_info
        
        info = {
            "name": self.name,
            "available": self.is_available(),
            "path": self.ghidra_path,
            "version": "unknown"
        }
        
        # Try to get version information
        if self.is_available():
            try:
                # Version file is usually in the Ghidra root directory
                version_file = os.path.join(self.ghidra_path, "Ghidra", "application.properties")
                if os.path.exists(version_file):
                    with open(version_file, "r") as f:
                        for line in f:
                            if line.startswith("application.version="):
                                info["version"] = line.split("=")[1].strip()
                                break
            except Exception as e:
                logger.warning(f"Error getting Ghidra version: {e}")
        
        self._decompiler_info = info
        return info

```

`src/decompilers/ida_decompiler.py`:

```py
"""
IDA Pro decompiler implementation for RE-Architect.

This module provides the integration with IDA Pro for decompilation.
"""

import logging
import os
import subprocess
import tempfile
import json
import time
from pathlib import Path
from typing import Dict, List, Optional, Any

from src.core.binary_loader import BinaryInfo
from src.decompilers.base_decompiler import BaseDecompiler, DecompiledCode, DecompiledFunction

logger = logging.getLogger("re-architect.decompilers.ida")

class IDADecompiler(BaseDecompiler):
    """
    IDA Pro decompiler implementation.
    
    This class provides integration with IDA Pro using IDAPython scripts
    and IDA's headless mode for automated decompilation.
    """
    
    def __init__(self, ida_path: Optional[str] = None):
        """
        Initialize the IDA Pro decompiler.
        
        Args:
            ida_path: Path to IDA Pro installation directory (optional)
        """
        super().__init__()
        self.name = "IDADecompiler"
        
        # Try to find IDA path if not provided
        self.ida_path = ida_path or self._find_ida_path()
        
        # Cache decompiler info
        self._decompiler_info = None
    
    def _find_ida_path(self) -> Optional[str]:
        """
        Find the IDA Pro installation directory.
        
        Looks for IDA Pro in common installation locations.
        
        Returns:
            Path to IDA Pro installation directory, or None if not found
        """
        # Check environment variable
        if "IDADIR" in os.environ:
            path = os.environ["IDADIR"]
            if os.path.exists(path):
                return path
        
        # Check common installation locations
        common_paths = []
        
        if os.name == "nt":  # Windows
            common_paths.extend([
                "C:/Program Files/IDA Pro 8.4",
                "C:/Program Files/IDA Pro 8.3", 
                "C:/Program Files/IDA Pro 8.2",
                "C:/Program Files/IDA Pro 8.1",
                "C:/Program Files/IDA Pro 8.0",
                "C:/Program Files/IDA Pro 7.7",
                "C:/Program Files (x86)/IDA Pro 8.4",
                "C:/Program Files (x86)/IDA Pro 8.3",
                "C:/Program Files (x86)/IDA Pro 8.2",
                "C:/Program Files (x86)/IDA Pro 8.1", 
                "C:/Program Files (x86)/IDA Pro 8.0",
                "C:/Program Files (x86)/IDA Pro 7.7",
                "C:/IDA",
                os.path.expanduser("~/IDA")
            ])
        else:  # Unix-like
            common_paths.extend([
                "/opt/ida",
                "/usr/local/ida",
                os.path.expanduser("~/ida"),
                os.path.expanduser("~/idapro")
            ])
        
        for path in common_paths:
            if os.path.exists(path):
                # Look for ida64 or idaq executable
                if self._find_ida_executable(path):
                    return path
        
        return None
    
    def _find_ida_executable(self, ida_dir: str) -> Optional[str]:
        """
        Find the IDA executable in the given directory.
        
        Args:
            ida_dir: IDA Pro installation directory
            
        Returns:
            Path to IDA executable, or None if not found
        """
        if os.name == "nt":  # Windows
            executables = ["ida64.exe", "idaq64.exe", "ida.exe", "idaq.exe"]
        else:  # Unix-like
            executables = ["ida64", "idaq64", "ida", "idaq"]
        
        for exe in executables:
            exe_path = os.path.join(ida_dir, exe)
            if os.path.exists(exe_path):
                return exe_path
        
        return None
    
    def is_available(self) -> bool:
        """
        Check if IDA Pro is available on the system.
        
        Returns:
            True if IDA Pro is available, False otherwise
        """
        if not self.ida_path:
            logger.warning("IDA Pro path not found")
            return False
        
        ida_exe = self._find_ida_executable(self.ida_path)
        if not ida_exe:
            logger.warning(f"IDA Pro executable not found in {self.ida_path}")
            return False
        
        return True
    
    def decompile(self, binary_info: BinaryInfo) -> DecompiledCode:
        """
        Decompile a binary using IDA Pro.
        
        Args:
            binary_info: Information about the binary to decompile
            
        Returns:
            Object containing the decompiled code
            
        Raises:
            RuntimeError: If decompilation fails or IDA Pro is not available
        """
        if not self.is_available():
            raise RuntimeError("IDA Pro is not available")
        
        logger.info(f"Decompiling {binary_info.path} using IDA Pro")
        
        # Create a temporary directory for IDA output
        with tempfile.TemporaryDirectory(prefix="re-architect-ida-") as temp_dir:
            output_dir = os.path.join(temp_dir, "output")
            os.makedirs(output_dir, exist_ok=True)
            
            # Create IDAPython script
            script_path = self._create_ida_script(temp_dir, output_dir)
            
            # Run IDA in headless mode
            ida_exe = self._find_ida_executable(self.ida_path)
            binary_path = str(binary_info.path)
            
            cmd = [
                ida_exe,
                "-A",  # Autonomous mode
                "-S" + script_path,  # Run script
                "-L" + os.path.join(temp_dir, "ida.log"),  # Log file
                binary_path
            ]
            
            logger.debug(f"Running IDA command: {cmd}")
            
            try:
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    cwd=temp_dir
                )
                
                # Wait for the process to complete with a timeout
                try:
                    stdout, stderr = process.communicate(timeout=600)  # 10-minute timeout
                    
                    if process.returncode != 0:
                        logger.error(f"IDA decompilation failed with code {process.returncode}")
                        logger.error(f"Stdout: {stdout}")
                        logger.error(f"Stderr: {stderr}")
                        raise RuntimeError(f"IDA decompilation failed with code {process.returncode}")
                    
                except subprocess.TimeoutExpired:
                    process.kill()
                    logger.error("IDA decompilation timed out")
                    raise RuntimeError("IDA decompilation timed out after 10 minutes")
                
                # Parse the output files
                return self._parse_output(binary_info, output_dir)
                
            except Exception as e:
                logger.exception(f"Error running IDA Pro: {e}")
                raise RuntimeError(f"Error running IDA Pro: {str(e)}")
    
    def _create_ida_script(self, script_dir: str, output_dir: str) -> str:
        """
        Create an IDAPython script to export decompiled code.
        
        Args:
            script_dir: Directory to write the script to
            output_dir: Directory to write the output to
            
        Returns:
            Path to the created script
        """
        script_path = os.path.join(script_dir, "export_decompiled.py")
        
        # Generate IDAPython script content
        script_template = '''
import idaapi
import idautils
import idc
import ida_hexrays
import ida_funcs
import ida_name
import ida_bytes
import ida_struct
import json
import os

OUTPUT_DIR = r"{output_dir}"

def main():
    """Main decompilation export function."""
    print("Starting IDA Pro decompilation export")
    
    # Wait for analysis to complete
    idaapi.auto_wait()
    
    # Ensure output directory exists
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
    
    # Export program information
    export_program_info()
    
    # Export functions
    export_functions()
    
    # Export strings
    export_strings()
    
    # Export structures
    export_structures()
    
    print("Export complete")
    idc.qexit(0)

def export_program_info():
    """Export basic program information."""
    print("Exporting program info")
    
    info = {{
        "name": idc.get_root_filename(),
        "imagebase": idc.get_imagebase(),
        "entry_point": idc.get_inf_attr(idc.INF_START_IP),
        "architecture": idaapi.get_inf_structure().procName,
        "creation_date": idaapi.get_file_type_name()
    }}
    
    # Export memory segments
    segments = []
    for seg in idautils.Segments():
        seg_info = {{
            "name": idc.get_segm_name(seg),
            "start": seg,
            "end": idc.get_segm_end(seg),
            "size": idc.get_segm_end(seg) - seg,
            "class": idc.get_segm_attr(seg, idc.SEGATTR_CLASS)
        }}
        segments.append(seg_info)
    
    info["segments"] = segments
    
    # Write to file
    with open(os.path.join(OUTPUT_DIR, "program_info.json"), "w") as f:
        json.dump(info, f, indent=2)

def export_functions():
    """Export decompiled functions."""
    print("Exporting functions")
    
    functions = []
    functions_dir = os.path.join(OUTPUT_DIR, "functions")
    if not os.path.exists(functions_dir):
        os.makedirs(functions_dir)
    
    # Initialize decompiler
    if not ida_hexrays.init_hexrays_plugin():
        print("Hex-Rays decompiler not available")
        return
    
    count = 0
    for func_addr in idautils.Functions():
        func_name = idc.get_func_name(func_addr)
        if not func_name:
            func_name = "sub_" + format(func_addr, 'X')
        
        print("Processing function: " + func_name + " at 0x" + format(func_addr, 'X'))
        
        func_info = {{
            "address": "0x" + format(func_addr, 'X'),
            "name": func_name,
            "start": func_addr,
            "end": idc.get_func_attr(func_addr, idc.FUNCATTR_END),
            "size": idc.get_func_attr(func_addr, idc.FUNCATTR_END) - func_addr
        }}
        
        # Get function signature
        func_type = idc.get_type(func_addr)
        if func_type:
            func_info["signature"] = func_type
        
        # Get function parameters and return type
        tif = idaapi.tinfo_t()
        if idaapi.get_tinfo(tif, func_addr):
            func_details = idaapi.func_type_data_t()
            if tif.get_func_details(func_details):
                # Return type
                ret_type = str(func_details.rettype)
                func_info["return_type"] = ret_type
                
                # Parameters
                params = []
                for i in range(func_details.size()):
                    param = func_details[i]
                    param_info = {{
                        "name": param.name if param.name else "arg_" + str(i),
                        "type": str(param.type)
                    }}
                    params.append(param_info)
                func_info["parameters"] = params
        
        # Get function calls (outgoing references)
        calls = []
        for ref in idautils.CodeRefsFrom(func_addr, True):
            ref_name = idc.get_func_name(ref)
            if ref_name:
                calls.append({{
                    "address": "0x" + format(ref, 'X'),
                    "name": ref_name
                }})
        func_info["calls"] = calls
        
        # Attempt to decompile the function
        try:
            cfunc = ida_hexrays.decompile(func_addr)
            if cfunc:
                # Get the decompiled C code
                decompiled_code = str(cfunc)
                func_info["code"] = decompiled_code
                
                # Save to individual file
                safe_name = func_name.replace(":", "_").replace("?", "_")
                func_file = os.path.join(functions_dir, safe_name + ".c")
                with open(func_file, "w") as f:
                    f.write(decompiled_code)
            else:
                func_info["decompilation_error"] = "Failed to decompile"
                
        except Exception as e:
            func_info["decompilation_error"] = str(e)
        
        functions.append(func_info)
        count += 1
    
    print("Processed " + str(count) + " functions")
    
    # Write all functions info
    with open(os.path.join(OUTPUT_DIR, "functions.json"), "w") as f:
        json.dump(functions, f, indent=2)

def export_strings():
    """Export string constants."""
    print("Exporting strings")
    
    strings = []
    
    # Get all strings
    for string_addr in idautils.Strings():
        string_info = {{
            "address": "0x" + format(string_addr.ea, 'X'),
            "value": str(string_addr),
            "length": string_addr.length,
            "type": string_addr.strtype
        }}
        strings.append(string_info)
    
    print("Found " + str(len(strings)) + " strings")
    
    # Write strings to file
    with open(os.path.join(OUTPUT_DIR, "strings.json"), "w") as f:
        json.dump(strings, f, indent=2)

def export_structures():
    """Export structure definitions."""
    print("Exporting structures")
    
    structures = []
    
    # Get all structures
    for struct_idx in range(ida_struct.get_struc_qty()):
        struct_id = ida_struct.get_struc_by_idx(struct_idx)
        if struct_id != idaapi.BADADDR:
            struct_ptr = ida_struct.get_struc(struct_id)
            if struct_ptr:
                struct_name = ida_struct.get_struc_name(struct_id)
                struct_size = ida_struct.get_struc_size(struct_id)
                
                struct_info = {{
                    "name": struct_name,
                    "size": struct_size,
                    "id": struct_id
                }}
                
                # Get structure members
                members = []
                for member_idx in range(struct_ptr.memqty):
                    member = struct_ptr.get_member(member_idx)
                    if member:
                        member_info = {{
                            "name": ida_struct.get_member_name(member.id),
                            "offset": member.soff,
                            "size": ida_struct.get_member_size(member),
                            "type": idc.get_type(member.id) or "unknown"
                        }}
                        members.append(member_info)
                
                struct_info["members"] = members
                structures.append(struct_info)
    
    print("Found " + str(len(structures)) + " structures")
    
    # Write structures to file
    with open(os.path.join(OUTPUT_DIR, "structures.json"), "w") as f:
        json.dump(structures, f, indent=2)

if __name__ == "__main__":
    main()
'''
        
        script_content = script_template.format(output_dir=output_dir)
        
        with open(script_path, "w") as f:
            f.write(script_content)
        
        return script_path
    
    def _parse_output(self, binary_info: BinaryInfo, output_dir: str) -> DecompiledCode:
        """
        Parse the output files from IDA Pro.
        
        Args:
            binary_info: Information about the decompiled binary
            output_dir: Directory containing the output files
            
        Returns:
            DecompiledCode object containing decompilation results
        """
        logger.info("Parsing IDA Pro output")
        
        decompiled_code = DecompiledCode(binary_info)
        
        # Parse functions
        functions_file = os.path.join(output_dir, "functions.json")
        if os.path.exists(functions_file):
            try:
                with open(functions_file, "r") as f:
                    functions_data = json.load(f)
                    
                    for func_data in functions_data:
                        address = int(func_data["address"], 16)
                        name = func_data["name"]
                        code = func_data.get("code", "// Decompilation failed")
                        
                        # Extract metadata
                        metadata = {
                            "signature": func_data.get("signature", ""),
                            "return_type": func_data.get("return_type", ""),
                            "parameters": func_data.get("parameters", []),
                            "calls": func_data.get("calls", []),
                            "size": func_data.get("size", 0),
                            "decompilation_error": func_data.get("decompilation_error", None)
                        }
                        
                        decompiled_code.add_function(address, code, name, metadata)
                    
                    logger.info(f"Loaded {len(functions_data)} functions")
            except Exception as e:
                logger.error(f"Error parsing functions.json: {{e}}")
        
        # Parse strings
        strings_file = os.path.join(output_dir, "strings.json")
        if os.path.exists(strings_file):
            try:
                with open(strings_file, "r") as f:
                    strings_data = json.load(f)
                    
                    for string_data in strings_data:
                        address = int(string_data["address"], 16)
                        value = string_data["value"]
                        decompiled_code.add_string(address, value)
                    
                    logger.info(f"Loaded {len(strings_data)} strings")
            except Exception as e:
                logger.error(f"Error parsing strings.json: {{e}}")
        
        # Parse structures
        structures_file = os.path.join(output_dir, "structures.json")
        if os.path.exists(structures_file):
            try:
                with open(structures_file, "r") as f:
                    structures_data = json.load(f)
                    
                    for struct_data in structures_data:
                        name = struct_data["name"]
                        definition = self._convert_to_c_struct(struct_data)
                        decompiled_code.add_type(name, definition)
                    
                    logger.info(f"Loaded {len(structures_data)} structures")
            except Exception as e:
                logger.error(f"Error parsing structures.json: {{e}}")
        
        return decompiled_code
    
    def _convert_to_c_struct(self, struct_data: Dict) -> str:
        """
        Convert a struct definition from IDA's JSON format to C code.
        
        Args:
            struct_data: Structure data from IDA
            
        Returns:
            C structure definition
        """
        name = struct_data["name"]
        members = struct_data.get("members", [])
        
        lines = [f"struct {{name}} {{"]
        
        for member in members:
            member_name = member.get("name", "field")
            member_type = member.get("type", "undefined")
            lines.append(f"    {member_type} {member_name};")
        
        lines.append("}};")
        
        return "\n".join(lines)
    
    def get_decompiler_info(self) -> Dict:
        """
        Get information about the IDA Pro decompiler.
        
        Returns:
            Dictionary containing decompiler information
        """
        if self._decompiler_info is not None:
            return self._decompiler_info
        
        info = {
            "name": self.name,
            "available": self.is_available(),
            "path": self.ida_path,
            "executable": self._find_ida_executable(self.ida_path) if self.ida_path else None,
            "version": "unknown"
        }
        
        self._decompiler_info = info
        return info
        
    def get_decompiler_info(self) -> Dict:
        """
        Get information about the decompiler.
        
        Returns:
            Dictionary containing decompiler information
        """
        return {
            "name": self.name,
            "version": "Not available",
            "capabilities": []
        }
```

`src/decompilers/internal_ir_decompiler.py`:

```py
"""
Internal IR Decompiler for RE-Architect.

This decompiler produces a two-level Intermediate Representation (IR):

1) Ground-Level IR: a low-level, architecture-agnostic encoding of machine
   instruction semantics with explicit, verbose operation names and explicit
   side effects.

2) Sky-Level IR: a higher-level abstraction rendering control flow and data
   flow in structures resembling C/C++ (function definitions, statements,
   expressions), while preserving traceability to the ground level.

The design takes inspiration from Valgrind's VEX and Ghidra's P-Code. All
IR names are deliberately verbose and self-describing.
"""

import logging
from typing import Dict, Optional, List

from src.core.binary_loader import BinaryInfo
from src.decompilers.base_decompiler import BaseDecompiler, DecompiledCode
from src.ir import (
    IntermediateRepresentationProgram,
    IntermediateRepresentationFunction,
    IntermediateRepresentationBasicBlock,
    GroundLevelInstruction,
    GroundLevelOperand,
    IRAddressingForm,
    IRIntegerSignedness,
    IRScalarElementType,
    IRVectorShape,
    IRRoundingMode,
    IRMemoryOrdering,
    IRMemoryAddressExpression,
    IROperationCategory,
    EnterpriseAbstractSyntaxTreeNode,
    EnterpriseFunctionAbstractSyntaxTree,
)

logger = logging.getLogger("re-architect.decompilers.internal_ir")


class InternalIRDecompiler(BaseDecompiler):
    """Decompiler that emits the custom two-level IR and a C-like rendering."""

    def __init__(self):
        super().__init__()
        self.name = "InternalIRDecompiler"

    def is_available(self) -> bool:
        # Always available as it is internal
        return True

    def decompile(self, binary_info: BinaryInfo) -> DecompiledCode:
        """Decompile the binary into IR, and also produce a C-like rendering per function.

        The return type is still DecompiledCode for compatibility with the
        existing pipeline. We store the IR in special fields on the instance
        for downstream components that are IR-aware.
        """
        logger.info(f"Decompiling {binary_info.path} using Internal IR decompiler")

        # Construct IR program container and lift using Capstone if available
        ir_program = self._lift_program(binary_info)

        # Bridge to existing interface: create DecompiledCode where function bodies
        # are the "sky-level" C-like renderings. We will also attach the IR to
        # the object for IR-aware consumers.
        decompiled = DecompiledCode(binary_info)
        for addr, ir_func in ir_program.functions_by_address.items():
            c_like = self._render_sky_level_function_as_c(ir_func)
            metadata = {
                "ir_available": True,
                "ground_instruction_count": sum(len(bb.ground_level_instructions) for bb in ir_func.basic_blocks),
                "sky_ast": True,
            }
            decompiled.add_function(addr, c_like, ir_func.function_symbolic_name, metadata)

        # Attach IR program so that other components can access it
        setattr(decompiled, "internal_ir_program", ir_program)
        return decompiled

    def _render_sky_level_function_as_c(self, ir_func: IntermediateRepresentationFunction) -> str:
        """Render a minimal C-like representation from the sky-level AST.

        This renderer is intentionally simple and conservative; it ensures
        human-readable output even if the sky-level AST is rudimentary.
        """
        ret_type = ir_func.function_return_type_description or "int"
        name = ir_func.function_symbolic_name
        params = ", ".join(ir_func.function_parameter_descriptions) if ir_func.function_parameter_descriptions else "void"
        body_lines = ["// Auto-generated C-like representation from Sky-Level IR"]

        ast = ir_func.sky_level_function_ast
        if ast and ast.root_node:
            for node in ast.root_node.child_nodes:
                if node.node_kind_name == "return_statement":
                    expr = node.node_properties.get("return_expression", 0)
                    body_lines.append(f"return {expr};")
        else:
            body_lines.append("// No AST available; returning 0 as placeholder")
            body_lines.append("return 0;")

        body = "\n    ".join(body_lines)
        return f"{ret_type} {name}({params})\n{{\n    {body}\n}}\n"

    def get_decompiler_info(self) -> Dict:
        return {
            "name": self.name,
            "available": True,
            "version": "0.1-internal-ir",
            "ir_levels": ["ground", "sky"],
        }

    # --- Lifting implementation ---
    def _lift_program(self, binary_info: BinaryInfo) -> IntermediateRepresentationProgram:
        """Lift binary code into IR. Currently focuses on entry point function.

        This implementation uses Capstone to disassemble a bounded number of
        instructions from the entry point, translates each instruction into our
        ground-level IR, and builds a single basic block for a proof of concept.
        """
        ir_program = IntermediateRepresentationProgram()
        if not binary_info.entry_point:
            return ir_program
        try:
            import capstone
        except ImportError:
            # Fallback: synthesize a trivial function if disassembly is unavailable
            func_addr = binary_info.entry_point
            function_name = f"function_at_0x{func_addr:x}"
            basic_block = IntermediateRepresentationBasicBlock(
                basic_block_start_address=func_addr,
                basic_block_end_address=func_addr,
                ground_level_instructions=[
                    GroundLevelInstruction(
                        originating_address=func_addr,
                        operation_semantic_name="control_flow_return",
                        operation_category=IROperationCategory.CONTROL_FLOW,
                    )
                ],
            )
            ir_function = IntermediateRepresentationFunction(
                function_start_address=func_addr,
                function_symbolic_name=function_name,
                function_return_type_description="int",
                function_parameter_descriptions=[],
                basic_blocks=[basic_block],
            )
            ir_function.sky_level_function_ast = self._build_minimal_sky_ast(function_name, 0, [func_addr])
            ir_program.functions_by_address[func_addr] = ir_function
            return ir_program

        # Setup Capstone for the architecture
        md = self._create_capstone(binary_info)
        if md is None:
            return ir_program

        # Read binary bytes
        try:
            with open(binary_info.path, 'rb') as f:
                binary_bytes = f.read()
        except Exception:
            return ir_program

        # Compute section containing entry point if possible
        entry = binary_info.entry_point
        section_bytes, section_base = self._get_section_bytes_containing_address(binary_bytes, binary_info, entry)
        if section_bytes is None:
            return ir_program

        # Disassemble a limited window from the entry point
        max_instructions = 256
        ir_instructions: List[GroundLevelInstruction] = []
        end_address = entry
        try:
            for insn in md.disasm(section_bytes[entry - section_base:], entry, count=max_instructions):
                ir_instr = self._translate_insn_to_ir(md, insn)
                ir_instructions.append(ir_instr)
                end_address = insn.address
                # Stop at RET to keep it simple for PoC
                if insn.mnemonic.lower().startswith('ret'):
                    break
        except Exception as e:
            logger.debug(f"Disassembly error during IR lift: {e}")

        # Build function and basic block
        function_name = f"function_at_0x{entry:x}"
        basic_block = IntermediateRepresentationBasicBlock(
            basic_block_start_address=entry,
            basic_block_end_address=end_address,
            ground_level_instructions=ir_instructions,
        )
        ir_function = IntermediateRepresentationFunction(
            function_start_address=entry,
            function_symbolic_name=function_name,
            function_return_type_description="int",
            function_parameter_descriptions=[],
            basic_blocks=[basic_block],
        )

        # Build naïve sky-level AST (recognize simple "return 0" pattern)
        return_value = 0
        return_addresses = [i.originating_address for i in ir_instructions[-1:]] if ir_instructions else [entry]
        ir_function.sky_level_function_ast = self._build_minimal_sky_ast(function_name, return_value, return_addresses)

        ir_program.functions_by_address[entry] = ir_function
        return ir_program

    def _create_capstone(self, binary_info: BinaryInfo):
        try:
            import capstone
        except ImportError:
            return None
        arch = getattr(binary_info.architecture, 'value', str(binary_info.architecture)).lower()
        try:
            if arch == 'x86_64' or arch == 'x86-64':
                return capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_64)
            elif arch == 'x86':
                return capstone.Cs(capstone.CS_ARCH_X86, capstone.CS_MODE_32)
            elif arch == 'arm64':
                return capstone.Cs(capstone.CS_ARCH_ARM64, capstone.CS_MODE_ARM)
            elif arch == 'arm':
                return capstone.Cs(capstone.CS_ARCH_ARM, capstone.CS_MODE_ARM)
            else:
                return None
        except Exception:
            return None

    def _get_section_bytes_containing_address(self, binary_bytes: bytes, binary_info: BinaryInfo, address: int):
        for _name, info in binary_info.sections.items():
            vaddr = info.get('virtual_address', 0)
            size = info.get('size', 0)
            offset = info.get('offset', 0)
            if vaddr <= address < vaddr + size and offset + size <= len(binary_bytes):
                return binary_bytes[offset:offset + size], vaddr
        return None, 0

    def _build_minimal_enterprise_ast(self, function_name: str, return_value: int, source_addrs: List[int]) -> EnterpriseFunctionAbstractSyntaxTree:
        return_stmt = EnterpriseAbstractSyntaxTreeNode(
            node_kind_name="return_statement",
            node_properties={"return_expression": return_value},
            source_ground_level_addresses=source_addrs,
        )
        root = EnterpriseAbstractSyntaxTreeNode(
            node_kind_name="function_definition",
            node_properties={},
            child_nodes=[return_stmt],
            source_ground_level_addresses=source_addrs,
        )
        return EnterpriseFunctionAbstractSyntaxTree(
            function_name=function_name,
            return_type_description="int",
            parameter_descriptions=[],
            root_node=root,
        )

    def _translate_insn_to_ir(self, md, insn) -> GroundLevelInstruction:
        """Translate a Capstone instruction to GroundLevelInstruction.

        This covers x86/x64 including vector and x87 mnemonics at a category level.
        """
        mnem = insn.mnemonic.lower()
        opcat, sem_name = self._map_mnemonic_to_category_and_semantics(mnem)

        # Prefixes
        lock = False
        repeat = None
        try:
            # Capstone exposes x86 prefixes as a list of ints
            if getattr(insn, 'prefix', None):
                pf = [p for p in insn.prefix if p]
                if 0xF0 in pf:
                    lock = True
                if 0xF3 in pf:
                    repeat = 'repe'
                if 0xF2 in pf:
                    repeat = 'repne'
        except Exception:
            pass

        # Operands
        operands: List[GroundLevelOperand] = []
        try:
            operands_list = insn.operands
            for idx, op in enumerate(operands_list):
                role = self._infer_operand_role(mnem, idx)
                if op.type == md.x86.OP_REG:
                    reg_name = md.reg_name(op.reg)
                    operands.append(GroundLevelOperand(
                        operand_role_name=role,
                        addressing_form=IRAddressingForm.REGISTER_DIRECT,
                        operand_value_representation=reg_name,
                    ))
                elif op.type == md.x86.OP_IMM:
                    operands.append(GroundLevelOperand(
                        operand_role_name=role,
                        addressing_form=IRAddressingForm.IMMEDIATE,
                        operand_value_representation=int(op.imm),
                    ))
                elif op.type == md.x86.OP_MEM:
                    mem = op.mem
                    addr_expr = IRMemoryAddressExpression(
                        segment_register_name=(md.reg_name(mem.segment) if mem.segment != 0 else None),
                        base_register_name=(md.reg_name(mem.base) if mem.base != 0 else None),
                        index_register_name=(md.reg_name(mem.index) if mem.index != 0 else None),
                        index_scale_factor=int(mem.scale) if getattr(mem, 'scale', 0) else 0,
                        displacement_value=int(mem.disp) if getattr(mem, 'disp', 0) else 0,
                    )
                    operands.append(GroundLevelOperand(
                        operand_role_name=role,
                        addressing_form=IRAddressingForm.MEMORY_EFFECTIVE_ADDRESS,
                        operand_value_representation=addr_expr,
                    ))
        except Exception:
            pass

        # Vector hints based on register names
        vec_shape = IRVectorShape.SCALAR
        vec_etype = None
        vec_ebits = None
        vec_ecount = None
        if any(isinstance(o.operand_value_representation, str) and o.operand_value_representation.startswith(('xmm', 'ymm', 'zmm')) for o in operands):
            vec_shape = IRVectorShape.VECTOR
            # Heuristic: assume 32-bit float elements for *ps, 64-bit for *pd, else integer 32
            if mnem.endswith('ps'):
                vec_etype = IRScalarElementType.FLOAT_IEEE754
                vec_ebits = 32
            elif mnem.endswith('pd'):
                vec_etype = IRScalarElementType.FLOAT_IEEE754
                vec_ebits = 64
            else:
                vec_etype = IRScalarElementType.INTEGER
                vec_ebits = 32

        instr = GroundLevelInstruction(
            originating_address=insn.address,
            operation_semantic_name=sem_name,
            operation_category=opcat,
            operands=operands,
            vector_shape=vec_shape,
            vector_element_type=vec_etype,
            vector_element_bit_width=vec_ebits,
            vector_element_count=vec_ecount,
            lock_prefix_applied=lock,
            repeat_prefix=repeat,
        )

        # Flags effects (heuristic for common ALU ops)
        if opcat == IROperationCategory.ARITHMETIC_INTEGER or opcat == IROperationCategory.COMPARE_TEST:
            instr.writes_flags = ["CF", "ZF", "SF", "OF", "AF", "PF"]

        # Memory ordering fences
        if mnem in ("mfence", "sfence", "lfence"):
            instr.memory_ordering = IRMemoryOrdering.SEQ_CST if mnem == 'mfence' else (
                IRMemoryOrdering.RELEASE if mnem == 'sfence' else IRMemoryOrdering.ACQUIRE
            )

        # x87 stack effect hints
        if mnem.startswith('f') and any(isinstance(o.operand_value_representation, str) and o.operand_value_representation.startswith('st') for o in operands):
            instr.operation_category = IROperationCategory.X87_STACK_FLOATING
            if mnem.startswith('fld'):
                instr.x87_stack_effect_description = 'push'
            elif mnem.startswith('fstp'):
                instr.x87_stack_effect_description = 'pop'
            elif mnem.startswith('fxch'):
                instr.x87_stack_effect_description = 'exchange_top'

        return instr

    def _map_mnemonic_to_category_and_semantics(self, mnem: str):
        # Common integer arithmetic
        if mnem in ("add", "adc"):
            return IROperationCategory.ARITHMETIC_INTEGER, "arithmetic_add_unsigned"
        if mnem in ("sub", "sbb"):
            return IROperationCategory.ARITHMETIC_INTEGER, "arithmetic_subtract_unsigned"
        if mnem in ("mul", "imul"):
            return IROperationCategory.ARITHMETIC_INTEGER, "arithmetic_multiply"
        if mnem in ("div", "idiv"):
            return IROperationCategory.ARITHMETIC_INTEGER, "arithmetic_divide"
        if mnem in ("inc",):
            return IROperationCategory.ARITHMETIC_INTEGER, "arithmetic_increment"
        if mnem in ("dec",):
            return IROperationCategory.ARITHMETIC_INTEGER, "arithmetic_decrement"

        # Bitwise, shifts, rotates
        if mnem in ("and", "or", "xor", "not"):
            return IROperationCategory.BIT_MANIPULATION, f"bitwise_{mnem}"
        if mnem in ("shl", "sal"):
            return IROperationCategory.SHIFT_ROTATE, "shift_left_arithmetic"
        if mnem == "shr":
            return IROperationCategory.SHIFT_ROTATE, "shift_right_logical"
        if mnem == "sar":
            return IROperationCategory.SHIFT_ROTATE, "shift_right_arithmetic"
        if mnem in ("rol", "ror", "rcl", "rcr"):
            return IROperationCategory.SHIFT_ROTATE, f"rotate_{mnem}"

        # Compare/test
        if mnem == "cmp":
            return IROperationCategory.COMPARE_TEST, "compare_subtract_sets_flags"
        if mnem == "test":
            return IROperationCategory.COMPARE_TEST, "compare_test_and_sets_flags"

        # Control flow
        if mnem.startswith('j'):
            return IROperationCategory.CONTROL_FLOW, ("control_flow_branch_conditional" if mnem != 'jmp' else "control_flow_jump_unconditional")
        if mnem == "jmp":
            return IROperationCategory.CONTROL_FLOW, "control_flow_jump_unconditional"
        if mnem == "call":
            return IROperationCategory.CONTROL_FLOW, "control_flow_call"
        if mnem.startswith("ret"):
            return IROperationCategory.CONTROL_FLOW, "control_flow_return"

        # Moves and LEA
        if mnem.startswith('mov'):
            return IROperationCategory.MEMORY_MOVE, "memory_move_transfer"
        if mnem == 'lea':
            return IROperationCategory.MEMORY_MOVE, "address_calculation_lea"

        # Fences
        if mnem in ("mfence", "sfence", "lfence"):
            return IROperationCategory.MEMORY_FENCE_CACHE, f"memory_{mnem}"

        # SSE/AVX: heuristic based on leading 'v' or xmm/ymm/zmm usage
        if mnem.startswith('v'):
            if mnem.startswith('vmov'):
                return IROperationCategory.SSE_AVX_DATA_MOVEMENT, "vector_data_move"
            if any(s in mnem for s in ("add", "sub", "mul", "div", "and", "or", "xor")):
                return IROperationCategory.SSE_AVX_VECTOR, "vector_arithmetic_operation"
            return IROperationCategory.SSE_AVX_MISC, "vector_misc_operation"

        if mnem.startswith('f'):
            return IROperationCategory.X87_STACK_FLOATING, "x87_floating_operation"

        # Default catch-all
        return IROperationCategory.SYSTEM, f"system_or_unclassified_{mnem}"

    def _infer_operand_role(self, mnem: str, index: int) -> str:
        # Simple heuristic: many x86 ops are dest, src1, src2 order
        if index == 0:
            return "destination_operand"
        elif index == 1:
            return "source_operand_primary"
        else:
            return f"source_operand_{index}"



```

`src/decompilers/mock_decompiler.py`:

```py
"""
Mock decompiler for testing RE-Architect.

This module provides a mock decompiler that generates synthetic decompiled code
for testing purposes when real decompilers are not available.
"""

import logging
from typing import Dict, Any, List

from src.core.binary_loader import BinaryInfo
from src.decompilers.base_decompiler import BaseDecompiler, DecompiledCode

logger = logging.getLogger("re-architect.decompilers.mock")

class MockDecompiler(BaseDecompiler):
    """
    Mock decompiler for testing purposes.
    
    This decompiler generates synthetic decompiled code based on static analysis
    of the binary, allowing the pipeline to be tested without requiring actual
    decompiler installations.
    """
    
    name = "Mock Decompiler"
    version = "1.0.0"
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Initialize the mock decompiler.
        
        Args:
            config: Decompiler configuration (not used for mock)
        """
        super().__init__()
        self.config = config or {}
        logger.info("Mock decompiler initialized for testing")
    
    def is_available(self) -> bool:
        """
        Check if the mock decompiler is available.
        
        Returns:
            Always True for mock decompiler
        """
        return True
    
    def get_decompiler_info(self) -> Dict:
        """
        Get information about the mock decompiler.
        
        Returns:
            Dictionary containing mock decompiler information
        """
        return {
            "name": self.name,
            "version": self.version,
            "type": "mock",
            "available": True,
            "capabilities": ["decompilation", "testing"],
            "supported_architectures": ["x86", "x86_64", "arm", "arm64"],
            "supported_formats": ["PE", "ELF", "Mach-O"],
            "description": "Mock decompiler for testing RE-Architect pipeline"
        }
    
    def decompile(self, binary_info: BinaryInfo) -> DecompiledCode:
        """
        Generate mock decompiled code.
        
        Args:
            binary_info: Binary information object
            
        Returns:
            DecompiledCode object with synthetic decompiled code
        """
        logger.info(f"Mock decompiling {binary_info.path}")
        
        # Generate synthetic function data based on the binary
        functions = self._generate_mock_functions(binary_info)
        
        # Generate synthetic decompiled code
        decompiled_code_str = self._generate_mock_code(functions)
        
        # Create DecompiledCode object
        decompiled_result = DecompiledCode(binary_info)
        
        # Add functions to the result
        for func in functions:
            # Extract function-specific code from the full decompiled code
            func_code = self._extract_function_code(decompiled_code_str, func["name"])
            decompiled_result.add_function(
                address=func["address"],
                code=func_code,
                name=func["name"],
                metadata={
                    "signature": func["signature"],
                    "parameters": func["parameters"],
                    "return_type": func["return_type"],
                    "complexity": func["complexity"],
                    "call_graph": func["call_graph"],
                    "strings": func["strings"],
                    "mock": True
                }
            )
        
        # Add metadata
        decompiled_result.metadata = {
            "decompiler": "mock",
            "version": self.version,
            "binary_path": str(binary_info.path),
            "architecture": str(binary_info.architecture.value if hasattr(binary_info.architecture, 'value') else binary_info.architecture),
            "format": str(binary_info.format.value if hasattr(binary_info.format, 'value') else binary_info.format),
            "generated": True,
            "test_mode": True,
            "full_code": decompiled_code_str
        }
        
        return decompiled_result
    
    def _generate_mock_functions(self, binary_info: BinaryInfo) -> List[Dict[str, Any]]:
        """
        Generate mock function data based on binary analysis.
        
        Args:
            binary_info: Binary information object
            
        Returns:
            List of mock function dictionaries
        """
        # Extract some basic info from the binary
        arch = binary_info.architecture.value if hasattr(binary_info.architecture, 'value') else str(binary_info.architecture)
        entry_point = binary_info.entry_point
        
        # Generate mock functions based on common patterns
        mock_functions = [
            {
                "name": "main",
                "address": entry_point,
                "size": 256,
                "signature": "int main(int argc, char** argv)",
                "parameters": [
                    {"name": "argc", "type": "int"},
                    {"name": "argv", "type": "char**"}
                ],
                "return_type": "int",
                "complexity": "medium",
                "call_graph": ["add_numbers", "multiply_numbers", "greet_user"],
                "strings": ["Addition result: %d\\n", "Multiplication result: %d\\n"],
                "mock": True
            },
            {
                "name": "add_numbers",
                "address": entry_point + 0x100,
                "size": 64,
                "signature": "int add_numbers(int a, int b)",
                "parameters": [
                    {"name": "a", "type": "int"},
                    {"name": "b", "type": "int"}
                ],
                "return_type": "int",
                "complexity": "low",
                "call_graph": [],
                "strings": [],
                "mock": True
            },
            {
                "name": "multiply_numbers",
                "address": entry_point + 0x140,
                "size": 64,
                "signature": "int multiply_numbers(int x, int y)",
                "parameters": [
                    {"name": "x", "type": "int"},
                    {"name": "y", "type": "int"}
                ],
                "return_type": "int",
                "complexity": "low",
                "call_graph": [],
                "strings": [],
                "mock": True
            },
            {
                "name": "greet_user",
                "address": entry_point + 0x180,
                "size": 128,
                "signature": "void greet_user(char* name)",
                "parameters": [
                    {"name": "name", "type": "char*"}
                ],
                "return_type": "void",
                "complexity": "low",
                "call_graph": ["printf"],
                "strings": ["Hello, %s!\\n"],
                "mock": True
            }
        ]
        
        return mock_functions
    
    def _generate_mock_code(self, functions: List[Dict[str, Any]]) -> str:
        """
        Generate synthetic decompiled C code.
        
        Args:
            functions: List of function metadata
            
        Returns:
            Generated C code as string
        """
        code_parts = [
            "// Mock decompiled code generated for testing",
            "#include <stdio.h>",
            "#include <string.h>",
            "",
            "// Global variables (mock)",
            "static int global_counter = 0;",
            ""
        ]
        
        # Generate function implementations
        for func in functions:
            if func["name"] == "main":
                code_parts.extend([
                    f"int {func['name']}(int argc, char** argv) {{",
                    "    // Mock implementation of main function",
                    "    int result1 = add_numbers(5, 3);",
                    "    int result2 = multiply_numbers(4, 6);",
                    "    ",
                    "    printf(\"Addition result: %d\\n\", result1);",
                    "    printf(\"Multiplication result: %d\\n\", result2);",
                    "    ",
                    "    char name[] = \"World\";",
                    "    greet_user(name);",
                    "    ",
                    "    return 0;",
                    "}",
                    ""
                ])
            elif func["name"] == "add_numbers":
                params = ', '.join([f"{p['type']} {p['name']}" for p in func['parameters']])
                code_parts.extend([
                    f"{func['return_type']} {func['name']}({params}) {{",
                    "    // Mock implementation of addition function",
                    "    return a + b;",
                    "}",
                    ""
                ])
            elif func["name"] == "multiply_numbers":
                params = ', '.join([f"{p['type']} {p['name']}" for p in func['parameters']])
                code_parts.extend([
                    f"{func['return_type']} {func['name']}({params}) {{",
                    "    // Mock implementation of multiplication function",
                    "    return x * y;",
                    "}",
                    ""
                ])
            elif func["name"] == "greet_user":
                params = ', '.join([f"{p['type']} {p['name']}" for p in func['parameters']])
                code_parts.extend([
                    f"{func['return_type']} {func['name']}({params}) {{",
                    "    // Mock implementation of greeting function",
                    "    printf(\"Hello, %s!\\n\", name);",
                    "}",
                    ""
                ])
        
        return "\n".join(code_parts)
    
    def _extract_function_code(self, full_code: str, function_name: str) -> str:
        """
        Extract code for a specific function from the full decompiled code.
        
        Args:
            full_code: Complete decompiled C code
            function_name: Name of function to extract
            
        Returns:
            Code for the specific function
        """
        lines = full_code.split('\n')
        in_function = False
        function_lines = []
        
        for line in lines:
            if f"{function_name}(" in line and "{" in line:
                in_function = True
                function_lines.append(line)
            elif in_function:
                function_lines.append(line)
                if line.strip() == "}" and len([l for l in function_lines if "{" in l]) == len([l for l in function_lines if "}" in l]):
                    break
        
        return "\n".join(function_lines)
```

`src/ir/__init__.py`:

```py
"""
Intermediate Representation (IR) framework for RE-Architect.

This package defines a two-level IR inspired by Valgrind's VEX and
Ghidra's P-Code models:

- Ground-Level IR: A low-level, architecture-agnostic representation that
  preserves the semantics of machine instructions using explicit, verbose
  operation names and explicit side effects.

- Sky-Level IR: A higher-level, language-like abstraction that captures
  control flow and data flow in constructs resembling C/C++ for improved
  readability and analysis while retaining links back to the ground level.

All IR component names intentionally use descriptive and verbose names
to enable self-documenting structures during debugging and analysis.
"""

from .ir_core import (
    # Core IR classes
    IntermediateRepresentationProgram,
    IntermediateRepresentationFunction,
    IntermediateRepresentationBasicBlock,
    GroundLevelInstruction,
    GroundLevelOperand,
    EnterpriseAbstractSyntaxTreeNode,
    EnterpriseFunctionAbstractSyntaxTree,
    # Enums and supporting classes
    IRIntegerSignedness,
    IRScalarElementType,
    IRVectorShape,
    IRRoundingMode,
    IRMemoryOrdering,
    IRAddressingForm,
    IRMemoryAddressExpression,
    IROperationCategory,
)

__all__ = [
    # Core IR classes
    "IntermediateRepresentationProgram",
    "IntermediateRepresentationFunction", 
    "IntermediateRepresentationBasicBlock",
    "GroundLevelInstruction",
    "GroundLevelOperand",
    "EnterpriseAbstractSyntaxTreeNode",
    "EnterpriseFunctionAbstractSyntaxTree",
    # Enums and supporting classes
    "IRIntegerSignedness",
    "IRScalarElementType", 
    "IRVectorShape",
    "IRRoundingMode",
    "IRMemoryOrdering",
    "IRAddressingForm",
    "IRMemoryAddressExpression",
    "IROperationCategory",
]



```

`src/ir/ir_core.py`:

```py
"""
Core Intermediate Representation (IR) data structures.

This module defines the core classes for a two-level IR used by
the internal decompiler:

- Ground-Level IR captures semantics of machine instructions as explicit
  operations with fully named side-effects.

- Sky-Level IR constructs an abstract syntax tree (AST) with C/C++-like
  control and data structures while maintaining back-references to the
  underlying ground-level instructions for traceability.

All names are intentionally verbose and self-descriptive.
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Any, Tuple, Union


class IRIntegerSignedness(Enum):
    SIGNED = "signed"
    UNSIGNED = "unsigned"


class IRScalarElementType(Enum):
    INTEGER = "integer"
    FLOAT_IEEE754 = "float_ieee754"
    BIT = "bit"
    BCD = "bcd"


class IRVectorShape(Enum):
    SCALAR = "scalar"
    VECTOR = "vector"  # use element_count and element_width for shape


class IRRoundingMode(Enum):
    NEAREST_EVEN = "nearest_even"
    TOWARD_ZERO = "toward_zero"
    TOWARD_POSITIVE = "toward_positive"
    TOWARD_NEGATIVE = "toward_negative"
    CURRENT_ENVIRONMENT = "current_environment"


class IRMemoryOrdering(Enum):
    NONE = "none"
    ACQUIRE = "acquire"
    RELEASE = "release"
    ACQ_REL = "acq_rel"
    SEQ_CST = "seq_cst"


class IRAddressingForm(Enum):
    REGISTER_DIRECT = "register_direct"
    IMMEDIATE = "immediate"
    MEMORY_EFFECTIVE_ADDRESS = "memory_effective_address"  # base+index*scale+disp with optional segment


@dataclass
class IRMemoryAddressExpression:
    """Explicit x86 effective address structure with segment overrides."""
    segment_register_name: Optional[str]
    base_register_name: Optional[str]
    index_register_name: Optional[str]
    index_scale_factor: int
    displacement_value: int


@dataclass
class GroundLevelOperand:
    """A single operand in the ground-level IR with explicit type and value.

    This structure supports x86 addressing (segment overrides, SIB) and vector
    element typing to cover AVX/AVX-512 and x87 usages.
    """
    operand_role_name: str  # e.g., "source_register", "destination_memory"
    addressing_form: IRAddressingForm
    operand_value_representation: Union[str, int, IRMemoryAddressExpression]
    operand_bit_width: Optional[int] = None
    operand_integer_signedness: Optional[IRIntegerSignedness] = None
    scalar_element_type: Optional[IRScalarElementType] = None
    vector_shape: IRVectorShape = IRVectorShape.SCALAR
    vector_element_bit_width: Optional[int] = None
    vector_element_count: Optional[int] = None


class IROperationCategory(Enum):
    """High-level categories that cover the entire x86/x64 ISA surface."""
    ARITHMETIC_INTEGER = "arithmetic_integer"          # add, sub, adc, sbb, mul, imul, div, idiv, neg, inc, dec
    BIT_MANIPULATION = "bit_manipulation"              # and, or, xor, not, bt/bts/btr/btc, bsf, bsr, popcnt, pdep, pext, lzcnt, tzcnt
    SHIFT_ROTATE = "shift_rotate"                      # shl, shr, sal, sar, rol, ror, rcl, rcr, shld, shrd
    COMPARE_TEST = "compare_test"                      # cmp, test, cmpsx/mmx/sse compares, setcc, cmovcc
    CONTROL_FLOW = "control_flow"                      # jcc, jmp, call, ret, loop*, sysenter/sysexit, int/iret, xbegin/xend/xabort
    MEMORY_MOVE = "memory_move"                        # mov*, cmov*, xchg, xadd, movsx/movzx, lea, string ops (movs, cmps, stos, lods, scas)
    MEMORY_FENCE_CACHE = "memory_fence_cache"          # mfence, lfence, sfence, clflush*, wbinvd, invlpg
    FLAGS_MANAGEMENT = "flags_management"              # lahf, sahf, pushf, popf, stc, clc, cmc, std, cld
    SYSTEM = "system"                                  # cpuid, rdtsc, rdtscp, rdmsr, wrmsr, xgetbv, xsetbv, cli, sti, hlt
    CRYPTO = "crypto"                                  # AES*, PCLMULQDQ, SHA*
    SSE_AVX_SCALAR = "sse_avx_scalar"                  # scalar float ops
    SSE_AVX_VECTOR = "sse_avx_vector"                  # packed float/integer ops incl. AVX/AVX2/AVX-512 (blend, permute, shuffle)
    SSE_AVX_DATA_MOVEMENT = "sse_avx_data_movement"    # movd, movq, movaps, movups, vmovdqa*, broadcasts, masks, compress/expand
    SSE_AVX_CONVERSION = "sse_avx_conversion"          # cvt* conversions between types and sizes
    SSE_AVX_MISC = "sse_avx_misc"                      # testps, ptest, crc32, etc.
    X87_STACK_FLOATING = "x87_stack_floating"          # fld, fstp, fadd, fsub, fmul, fdiv, fcom*, fxch, finit, fsave/frstor
    ATOMIC_SYNCHRONIZATION = "atomic_synchronization"  # lock-prefixed operations, xchg-based atomics


@dataclass
class GroundLevelInstruction:
    """A ground-level IR instruction with explicit operation semantics covering x86/x64.

    Rather than enumerating every opcode, we provide a parameterized operation
    model with:
    - operation_semantic_name: verbose operation (e.g., "arithmetic_add_unsigned")
    - operation_category: category enum to simplify analysis
    - vector and floating metadata for AVX/AVX-512 and x87
    - flag effects and reads/writes sets
    - prefix semantics (lock, rep/repe/repne, segment override already in addressing)
    - memory ordering semantics for fences and atomics
    """
    originating_address: int
    operation_semantic_name: str
    operation_category: IROperationCategory
    operands: List[GroundLevelOperand] = field(default_factory=list)
    side_effect_descriptions: List[str] = field(default_factory=list)
    comment_explanations: List[str] = field(default_factory=list)

    # Vector and FP metadata (for SSE/AVX/AVX-512/x87)
    vector_shape: IRVectorShape = IRVectorShape.SCALAR
    vector_element_type: Optional[IRScalarElementType] = None
    vector_element_bit_width: Optional[int] = None
    vector_element_count: Optional[int] = None
    predicate_mask_register_name: Optional[str] = None  # e.g., k0-k7 for AVX-512
    predicate_mask_is_merging: Optional[bool] = None    # AVX-512 merging vs zeroing
    rounding_mode: Optional[IRRoundingMode] = None     # EVEX/legacy rounding control
    floating_exception_behavior: Optional[str] = None  # "suppress_all", "default", etc.

    # x87 stack oriented metadata
    x87_stack_effect_description: Optional[str] = None  # e.g., "push", "pop", "exchange_top", "no_change"

    # Flags and atomic semantics
    reads_flags: List[str] = field(default_factory=list)
    writes_flags: List[str] = field(default_factory=list)
    memory_ordering: IRMemoryOrdering = IRMemoryOrdering.NONE
    lock_prefix_applied: bool = False
    repeat_prefix: Optional[str] = None  # None | "rep" | "repe" | "repne"


@dataclass
class IntermediateRepresentationBasicBlock:
    """A basic block grouping ground-level instructions with explicit edges."""
    basic_block_start_address: int
    basic_block_end_address: int
    ground_level_instructions: List[GroundLevelInstruction] = field(default_factory=list)
    successor_block_addresses: List[int] = field(default_factory=list)
    predecessor_block_addresses: List[int] = field(default_factory=list)


@dataclass
class IntermediateRepresentationFunction:
    """A function containing basic blocks and optional sky-level AST."""
    function_start_address: int
    function_symbolic_name: str
    function_return_type_description: str = "unknown"
    function_parameter_descriptions: List[str] = field(default_factory=list)
    basic_blocks: List[IntermediateRepresentationBasicBlock] = field(default_factory=list)
    enterprise_function_ast: Optional["EnterpriseFunctionAbstractSyntaxTree"] = None


@dataclass
class IntermediateRepresentationProgram:
    """IR container for a complete program with function mapping."""
    functions_by_address: Dict[int, IntermediateRepresentationFunction] = field(default_factory=dict)
    program_strings_by_address: Dict[int, str] = field(default_factory=dict)
    program_types_by_name: Dict[str, str] = field(default_factory=dict)


# Sky-Level IR types

@dataclass
class EnterpriseAbstractSyntaxTreeNode:
    """A node in the sky-level AST representing a high-level construct.

    node_kind_name examples: "function_definition", "if_statement",
    "while_loop", "variable_declaration", "return_statement",
    "binary_expression", "call_expression".
    """
    node_kind_name: str
    node_properties: Dict[str, Any] = field(default_factory=dict)
    child_nodes: List["EnterpriseAbstractSyntaxTreeNode"] = field(default_factory=list)
    source_ground_level_addresses: List[int] = field(default_factory=list)


@dataclass
class EnterpriseFunctionAbstractSyntaxTree:
    """A sky-level AST for a function, similar to a simplified C/C++ AST."""
    function_name: str
    return_type_description: str
    parameter_descriptions: List[str]
    root_node: EnterpriseAbstractSyntaxTreeNode



```

`src/llm/__init__.py`:

```py
"""
LLM integration package for RE-Architect.
"""

```

`src/llm/function_summarizer.py`:

```py
"""
Function summarizer module for RE-Architect.

This module uses LLM-based techniques to generate human-readable summaries of decompiled functions.
"""

import logging
import json
import os
import time
import hashlib
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, asdict
from pathlib import Path

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

from src.core.config import Config
from src.analysis.enhanced_static_analyzer import FunctionInfo, Instruction

logger = logging.getLogger("re-architect.llm.summarizer")

@dataclass
class FunctionSummary:
    """Comprehensive summary of a function generated by LLM analysis."""
    name: str
    purpose: str
    behavior: str
    complexity_analysis: str
    arguments: List[Dict[str, str]]
    return_value: str
    side_effects: List[str]
    security_notes: List[str]
    optimization_suggestions: List[str]
    confidence_score: float
    analysis_method: str  # "enhanced" or "legacy"
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return asdict(self)

@dataclass
class BatchSummaryResults:
    """Results from batch function summarization."""
    summaries: Dict[int, FunctionSummary]  # address -> summary
    total_functions: int
    successful_summaries: int
    failed_summaries: int
    total_time: float
    average_time_per_function: float

class FunctionSummarizer:
    """
    Advanced function summarizer for RE-Architect.
    
    This class uses language models to generate comprehensive, human-readable summaries 
    of binary functions using both static analysis data and decompiled code.
    """
    
    def __init__(self, config: Union[Dict, Config]):
        """
        Initialize the function summarizer.
        
        Args:
            config: Configuration object or dictionary
        """
        # Handle both Config objects and dictionaries
        if isinstance(config, dict):
            self.config = config
            self.provider = config.get("provider", "openai")
            self.model = config.get("model", "gpt-4")
            self.api_key = config.get("api_key")
            self.max_tokens = config.get("max_tokens", 4000)
            self.temperature = config.get("temperature", 0.2)
            self.cache_dir = config.get("cache_dir", "./cache/llm")
            self.use_cache = config.get("use_cache", True)
        else:
            self.config = config
            self.provider = config.get("llm.provider", "openai")
            self.model = config.get("llm.model", "gpt-4")
            self.api_key = config.get("llm.api_key")
            self.max_tokens = config.get("llm.max_tokens", 4000)
            self.temperature = config.get("llm.temperature", 0.2)
            self.cache_dir = config.get("llm.cache_dir", "./cache/llm")
            self.use_cache = config.get("llm.use_cache", True)
        
        # Create cache directory
        if self.use_cache:
            os.makedirs(self.cache_dir, exist_ok=True)
        
        # Initialize API client
        self._initialize_client()
        
        # In-memory cache for function summaries
        self._cache = {}
        
        # Statistics
        self.total_requests = 0
        self.cache_hits = 0
        
    def _initialize_client(self):
        """Initialize the appropriate LLM client."""
        self.client = None
        
        if not self.api_key:
            logger.warning("No API key provided - using mock responses for testing")
            return
            
        if self.provider == "openai" and OPENAI_AVAILABLE:
            try:
                self.client = openai.OpenAI(api_key=self.api_key)
                logger.info(f"Initialized OpenAI client with model {self.model}")
            except Exception as e:
                logger.error(f"Failed to initialize OpenAI client: {e}")
                
        elif self.provider == "anthropic" and ANTHROPIC_AVAILABLE:
            try:
                self.client = anthropic.Anthropic(api_key=self.api_key)
                logger.info(f"Initialized Anthropic client with model {self.model}")
            except Exception as e:
                logger.error(f"Failed to initialize Anthropic client: {e}")
        else:
            logger.warning(f"Provider '{self.provider}' not available or not installed")
        
    def analyze_function_enhanced(
        self, 
        func_info: FunctionInfo, 
        context: Optional[Dict[str, Any]] = None
    ) -> FunctionSummary:
        """
        Generate comprehensive analysis of a function using enhanced static analysis data.
        
        Args:
            func_info: Enhanced function information from static analysis
            context: Optional context information (call graph, binary info, etc.)
            
        Returns:
            Comprehensive function summary
        """
        logger.info(f"Analyzing function {func_info.name} with enhanced data")
        
        # Check cache first
        cache_key = self._get_cache_key_enhanced(func_info, context)
        if self.use_cache and cache_key in self._cache:
            self.cache_hits += 1
            return self._cache[cache_key]
        
        # Build analysis prompt
        prompt = self._build_enhanced_analysis_prompt(func_info, context)
        
        # Call LLM API
        try:
            response = self._call_llm_api(prompt)
            summary = self._parse_llm_response(response, func_info.name, "enhanced")
            
            # Cache result
            if self.use_cache:
                self._cache[cache_key] = summary
                
            self.total_requests += 1
            return summary
            
        except Exception as e:
            logger.error(f"Failed to analyze function {func_info.name}: {e}")
            return self._create_fallback_summary(func_info.name, "enhanced", str(e))
    
    def analyze_batch_enhanced(
        self, 
        functions: Dict[int, FunctionInfo], 
        context: Optional[Dict[str, Any]] = None,
        max_functions: Optional[int] = None
    ) -> BatchSummaryResults:
        """
        Perform batch analysis of multiple functions.
        
        Args:
            functions: Dictionary of address -> FunctionInfo
            context: Optional context information
            max_functions: Optional limit on number of functions to analyze
            
        Returns:
            Batch analysis results
        """
        start_time = time.time()
        
        # Limit functions if requested
        functions_to_analyze = dict(list(functions.items())[:max_functions]) if max_functions else functions
        
        logger.info(f"Starting batch analysis of {len(functions_to_analyze)} functions")
        
        summaries = {}
        successful = 0
        failed = 0
        
        for addr, func_info in functions_to_analyze.items():
            try:
                summary = self.analyze_function_enhanced(func_info, context)
                summaries[addr] = summary
                successful += 1
            except Exception as e:
                logger.error(f"Failed to analyze function at 0x{addr:x}: {e}")
                failed += 1
        
        total_time = time.time() - start_time
        avg_time = total_time / len(functions_to_analyze) if functions_to_analyze else 0
        
        logger.info(f"Batch analysis completed: {successful} successful, {failed} failed, {total_time:.2f}s total")
        
        return BatchSummaryResults(
            summaries=summaries,
            total_functions=len(functions_to_analyze),
            successful_summaries=successful,
            failed_summaries=failed,
            total_time=total_time,
            average_time_per_function=avg_time
        )
    
    def _build_enhanced_analysis_prompt(
        self, 
        func_info: FunctionInfo, 
        context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Build a comprehensive analysis prompt using enhanced function data."""
        
        # Basic function information
        prompt = f"""Please analyze this binary function and provide a comprehensive summary.

FUNCTION INFORMATION:
Name: {func_info.name}
Address: 0x{func_info.address:08x}
Size: {func_info.size} bytes
Number of Instructions: {len(func_info.instructions)}
Complexity Score: {func_info.complexity:.1f}
Has Loops: {func_info.has_loops}
Entry Point: {func_info.entry_point}

DISASSEMBLED INSTRUCTIONS:
"""
        
        # Add first 20 instructions for analysis
        max_instructions = min(20, len(func_info.instructions))
        for i, insn in enumerate(func_info.instructions[:max_instructions]):
            prompt += f"  0x{insn.address:08x}: {insn.mnemonic} {insn.op_str}\n"
        
        if len(func_info.instructions) > max_instructions:
            prompt += f"  ... and {len(func_info.instructions) - max_instructions} more instructions\n"
        
        # Add function calls if available
        if func_info.calls:
            prompt += f"\nFUNCTION CALLS:\n"
            for call_addr in func_info.calls[:10]:  # Show first 10 calls
                prompt += f"  -> 0x{call_addr:08x}\n"
            if len(func_info.calls) > 10:
                prompt += f"  ... and {len(func_info.calls) - 10} more calls\n"
        
        # Add context if available
        if context:
            if "binary_info" in context:
                binary_info = context["binary_info"]
                prompt += f"\nBINARY CONTEXT:\n"
                prompt += f"  Format: {binary_info.format.value}\n"
                prompt += f"  Architecture: {binary_info.architecture.value}\n"
            
            if "strings" in context:
                relevant_strings = context["strings"][:5]  # Show first 5 strings
                if relevant_strings:
                    prompt += f"\nRELEVANT STRINGS:\n"
                    for addr, string in relevant_strings:
                        prompt += f"  \"{string}\"\n"

            # If Internal IR is available, include a concise description of the sky-level
            # representation and ground-level instruction count to improve summaries.
            if "internal_ir" in context:
                ir_info = context["internal_ir"]
                prompt += f"\nINTERNAL IR CONTEXT:\n"
                if ir_info.get("sky_c_like"):
                    prompt += "  Sky-Level C-like Rendering (truncated):\n"
                    sky = ir_info["sky_c_like"]
                    truncated = sky[:400]
                    prompt += "  " + truncated.replace("\n", "\n  ") + ("...\n" if len(sky) > 400 else "\n")
                if ir_info.get("ground_instruction_count") is not None:
                    prompt += f"  Ground-Level Instruction Count: {ir_info['ground_instruction_count']}\n"
        
        prompt += """
ANALYSIS REQUEST:
Please provide a detailed analysis in the following JSON format:
{
    "purpose": "Brief description of what this function does",
    "behavior": "Detailed explanation of the function's behavior and logic flow",
    "complexity_analysis": "Analysis of why the complexity score is what it is",
    "arguments": [{"name": "arg1", "type": "int", "description": "description"}],
    "return_value": "Description of what the function returns",
    "side_effects": ["List of side effects like file I/O, memory allocation, etc."],
    "security_notes": ["Any potential security concerns or vulnerabilities"],
    "optimization_suggestions": ["Suggestions for code optimization if any"],
    "confidence_score": 0.85
}

Focus on:
1. Understanding the assembly instructions and their purpose
2. Identifying common patterns (loops, conditionals, function calls)
3. Determining the function's role in the larger program
4. Noting any security-relevant operations
5. Providing actionable insights for reverse engineering

Respond ONLY with the JSON object, no additional text."""
        
        return prompt
    def _call_llm_api(self, prompt: str) -> str:
        """Call the appropriate LLM API based on the provider."""
        try:
            if self.provider == "openai":
                return self._call_openai_api(prompt)
            elif self.provider == "anthropic":
                return self._call_anthropic_api(prompt)
            else:
                raise ValueError(f"Unsupported provider: {self.provider}")
        except Exception as e:
            logger.error(f"LLM API call failed: {e}")
            # Return mock response only if no client is available
            if not self.client:
                return self._get_mock_response()
            raise
    
    def _call_openai_api(self, prompt: str) -> str:
        """Call the OpenAI API to analyze a function."""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system", 
                        "content": "You are an expert reverse engineer and binary analysis specialist. Analyze assembly code and provide detailed, accurate insights."
                    },
                    {"role": "user", "content": prompt}
                ],
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                response_format={"type": "json_object"}
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"OpenAI API error: {e}")
            raise
    
    def _call_anthropic_api(self, prompt: str) -> str:
        """Call the Anthropic API to analyze a function."""
        try:
            response = self.client.messages.create(
                model=self.model,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                system="You are an expert reverse engineer and binary analysis specialist. Analyze assembly code and provide detailed, accurate insights. Always respond with valid JSON.",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
        except Exception as e:
            logger.error(f"Anthropic API error: {e}")
            raise
    
    def _parse_llm_response(self, response: str, function_name: str, method: str) -> FunctionSummary:
        """Parse the LLM response into a FunctionSummary object."""
        try:
            data = json.loads(response)
            
            return FunctionSummary(
                name=function_name,
                purpose=data.get("purpose", "Unknown purpose"),
                behavior=data.get("behavior", "Behavior analysis unavailable"),
                complexity_analysis=data.get("complexity_analysis", "Complexity analysis unavailable"),
                arguments=data.get("arguments", []),
                return_value=data.get("return_value", "Unknown return value"),
                side_effects=data.get("side_effects", []),
                security_notes=data.get("security_notes", []),
                optimization_suggestions=data.get("optimization_suggestions", []),
                confidence_score=data.get("confidence_score", 0.5),
                analysis_method=method
            )
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM response as JSON: {e}")
            return self._create_fallback_summary(function_name, method, f"JSON parse error: {e}")
        except Exception as e:
            logger.error(f"Unexpected error parsing LLM response: {e}")
            return self._create_fallback_summary(function_name, method, f"Parse error: {e}")
    
    def _get_mock_response(self) -> str:
        """Return a mock response for testing without API access."""
        # For enhanced analysis, return JSON
        if hasattr(self, '_current_prompt') and 'JSON format' in self._current_prompt:
            return json.dumps({
                "purpose": "Function purpose analysis (mock mode - no API key provided)",
                "behavior": "Detailed behavior analysis would be provided by LLM with proper API access",
                "complexity_analysis": "Complexity analysis based on instruction count and control flow",
                "arguments": [],
                "return_value": "Return value analysis requires LLM API access",
                "side_effects": ["Mock analysis mode"],
                "security_notes": ["API key required for real security analysis"],
                "optimization_suggestions": ["Enable LLM API for optimization suggestions"],
                "confidence_score": 0.1
            })
        
        # For legacy analysis, return simple text
        return "This function performs basic operations and returns a result."
    
    def _create_fallback_summary(self, name: str, method: str, error: str) -> FunctionSummary:
        """Create a fallback summary when LLM analysis fails."""
        return FunctionSummary(
            name=name,
            purpose=f"Analysis failed: {error}",
            behavior="Unable to analyze behavior due to LLM error",
            complexity_analysis="Complexity analysis unavailable",
            arguments=[],
            return_value="Unknown",
            side_effects=["Analysis failed"],
            security_notes=[f"Security analysis failed: {error}"],
            optimization_suggestions=[],
            confidence_score=0.0,
            analysis_method=f"{method}_fallback"
        )
    
    def _get_cache_key_enhanced(self, func_info: FunctionInfo, context: Optional[Dict[str, Any]] = None) -> str:
        """Generate cache key for enhanced analysis."""
        # Create hash based on function content and relevant context
        content = f"{func_info.name}_{func_info.address}_{func_info.size}_{len(func_info.instructions)}"
        
        if context:
            # Only include serializable context elements
            context_key = ""
            if "function_count" in context:
                context_key += f"_fc_{context['function_count']}"
            if "strings" in context and context["strings"]:
                # Just include count of strings to avoid huge cache keys
                context_key += f"_sc_{len(context['strings'])}"
            if "binary_info" in context:
                binary_info = context["binary_info"]
                context_key += f"_bi_{binary_info.format.value}_{binary_info.architecture.value}"
            content += context_key
            
        return hashlib.md5(content.encode()).hexdigest()
    
    # Legacy methods for backward compatibility
    def summarize_function(self, function_code: str) -> str:
        """
        Legacy method for backward compatibility.
        Generate a basic summary for a function code string.
        """
        # Check cache
        cache_key = hashlib.md5(function_code.encode()).hexdigest()
        if self.use_cache and cache_key in self._cache and isinstance(self._cache[cache_key], str):
            self.cache_hits += 1
            return self._cache[cache_key]
            
        # Default behavior - create simple prompt for legacy code
        prompt = f"""Analyze this function code and provide a brief summary:

{function_code}

Respond with a single sentence describing what this function does."""
        
        try:
            response = self._call_llm_api(prompt)
            # For legacy method, just return the text response
            result = response.strip().replace('"', '')
            if self.use_cache:
                self._cache[cache_key] = result
            self.total_requests += 1
            return result
        except Exception as e:
            logger.error(f"Legacy analysis failed: {e}")
            return f"Function analysis failed: {e}"
    
    def summarize(self, function_info: Dict[str, Any]) -> Dict[str, Any]:
        """
        Legacy method for backward compatibility.
        Generate detailed information about a function from a dictionary.
        """
        result = {
            "summary": f"Function {function_info.get('name', 'unknown')} analyzed via legacy method",
            "purpose": "Legacy analysis - upgrade to enhanced analysis for better results",
            "arguments": function_info.get('parameters', []),
            "return_value": function_info.get('return_type', 'unknown')
        }
        # If sky-level IR code is present in the metadata (from InternalIRDecompiler), append a preview.
        ir_meta = function_info.get('ir_available') or function_info.get('sky_ast')
        if ir_meta and 'decompiled_code' in function_info:
            preview = function_info['decompiled_code'][:120]
            result['ir_preview'] = preview
        return result
    
    def get_statistics(self) -> Dict[str, Any]:
        """Get summarizer statistics."""
        cache_hit_rate = (self.cache_hits / self.total_requests) if self.total_requests > 0 else 0
        return {
            "total_requests": self.total_requests,
            "cache_hits": self.cache_hits,
            "cache_hit_rate": cache_hit_rate,
            "provider": self.provider,
            "model": self.model,
            "cache_enabled": self.use_cache,
            "api_available": self.client is not None
        }

```

`src/optimization/__init__.py`:

```py
"""
Obfuscation optimization subsystem.

Provides iterative passes to clean junk code, resolve opaque predicates,
reduce mixed boolean arithmetic, and detect VM-based virtualization handlers.

Backed by Angr for symbolic reasoning and CFG reconstruction.
"""

from .optimizer import ObfuscationOptimizer, OptimizationReport

__all__ = ["ObfuscationOptimizer", "OptimizationReport"]



```

`src/optimization/optimizer.py`:

```py
"""
Obfuscation optimizer using Angr for symbolic reasoning.

Passes:
- JunkCodeRemovalPass: removes dead blocks and no-op sequences.
- OpaquePredicateSolvingPass: solves always-true/false branches.
- MixedBooleanArithmeticSimplificationPass: simplifies MBA expressions.
- VMHandlerIdentificationPass: heuristically identifies VM handlers (e.g., VMProtect-like).

The optimizer runs passes iteratively until convergence or pass/iteration limits.
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

logger = logging.getLogger("re-architect.optimization")

try:
    import angr  # type: ignore
    import claripy  # type: ignore
    ANGR_AVAILABLE = True
except Exception:
    angr = None
    claripy = None
    ANGR_AVAILABLE = False


@dataclass
class OptimizationReport:
    iterations: int
    changes_applied: int
    passes_run: List[str] = field(default_factory=list)
    details: Dict[str, Any] = field(default_factory=dict)


class BaseOptimizationPass:
    name: str = "base"

    def run(self, project: "angr.Project", cfg: Any, state: Optional["angr.SimState"]) -> Tuple[int, Dict[str, Any]]:
        raise NotImplementedError


class JunkCodeRemovalPass(BaseOptimizationPass):
    name = "junk_code_removal"

    def run(self, project, cfg, state):
        changes = 0
        details: Dict[str, Any] = {"removed_blocks": []}
        try:
            # Identify unreachable nodes after pruning returns and errors
            unreachable = [n for n in cfg.graph.nodes() if cfg.graph.in_degree(n) == 0 and getattr(n, 'addr', None) != project.entry]
            for n in unreachable:
                details["removed_blocks"].append(hex(getattr(n, 'addr', 0)))
                changes += 1
        except Exception as e:
            logger.debug(f"JunkCodeRemovalPass error: {e}")
        return changes, details


class OpaquePredicateSolvingPass(BaseOptimizationPass):
    name = "opaque_predicate_solving"

    def run(self, project, cfg, state):
        changes = 0
        details: Dict[str, Any] = {"resolved_branches": []}
        try:
            for node in cfg.graph.nodes():
                # Walk successors: if an edge condition is satisfiable only one way, mark as resolved
                for succ in cfg.graph.successors(node):
                    cond = getattr(succ, 'condition', None)
                    if cond is not None and state is not None:
                        try:
                            sat_true = state.solver.satisfiable(extra_constraints=[cond == 1])
                            sat_false = state.solver.satisfiable(extra_constraints=[cond == 0])
                            if sat_true != sat_false:
                                details["resolved_branches"].append({
                                    "at": hex(getattr(node, 'addr', 0)),
                                    "taken": bool(sat_true and not sat_false)
                                })
                                changes += 1
                        except Exception:
                            continue
        except Exception as e:
            logger.debug(f"OpaquePredicateSolvingPass error: {e}")
        return changes, details


class MixedBooleanArithmeticSimplificationPass(BaseOptimizationPass):
    name = "mixed_boolean_arithmetic_simplification"

    def _simplify_expr(self, expr: "claripy.ast.Base") -> "claripy.ast.Base":
        try:
            expr = claripy.simplify(expr)
        except Exception:
            return expr
        try:
            if getattr(expr, 'op', None) == '__xor__':
                a, b = expr.args
                if claripy.is_same_ast(a, b):
                    return claripy.BVV(0, expr.size())
            if getattr(expr, 'op', None) == 'If':
                c, t, f = expr.args
                if getattr(c, 'concrete', False):
                    return t if c.is_true() else f
        except Exception:
            pass
        return expr

    def run(self, project, cfg, state):
        changes = 0
        details: Dict[str, Any] = {"simplified": 0}
        try:
            for node in cfg.graph.nodes():
                block = project.factory.block(getattr(node, 'addr', 0))
                irsb = getattr(block, 'vex', None)
                if not irsb:
                    continue
                for stmt in getattr(irsb, 'statements', []):
                    guard = getattr(stmt, 'guard', None)
                    if guard is not None and state is not None:
                        simplified = self._simplify_expr(guard)
                        if simplified is not guard:
                            changes += 1
                            details["simplified"] += 1
        except Exception as e:
            logger.debug(f"MixedBooleanArithmeticSimplificationPass error: {e}")
        return changes, details


class VMHandlerIdentificationPass(BaseOptimizationPass):
    name = "vm_handler_identification"

    def run(self, project, cfg, state):
        changes = 0
        details: Dict[str, Any] = {"handlers": []}
        try:
            for node in cfg.graph.nodes():
                block = project.factory.block(getattr(node, 'addr', 0))
                vex = getattr(block, 'vex', None)
                if not vex:
                    continue
                # Heuristic: many indirect branches around a dispatcher region
                if getattr(vex, 'jumpkind', '') in ('Ijk_Boring', 'Ijk_Call') and getattr(vex, 'next', None) is not None:
                    details["handlers"].append(hex(getattr(node, 'addr', 0)))
                    changes += 1
        except Exception as e:
            logger.debug(f"VMHandlerIdentificationPass error: {e}")
        return changes, details


class ObfuscationOptimizer:
    def __init__(self, max_iterations: int = 5):
        self.max_iterations = max_iterations
        self.passes: List[BaseOptimizationPass] = [
            JunkCodeRemovalPass(),
            OpaquePredicateSolvingPass(),
            MixedBooleanArithmeticSimplificationPass(),
            VMHandlerIdentificationPass(),
        ]

    def is_available(self) -> bool:
        return ANGR_AVAILABLE

    def optimize(self, binary_path: Path) -> OptimizationReport:
        if not ANGR_AVAILABLE:
            logger.info("Angr not available; skipping obfuscation optimization")
            return OptimizationReport(iterations=0, changes_applied=0)

        project = angr.Project(str(binary_path), auto_load_libs=False)
        cfg = project.analyses.CFGFast(normalize=True)
        state = project.factory.entry_state()

        total_changes = 0
        passes_run: List[str] = []
        details: Dict[str, Any] = {"per_iteration": []}

        for iteration in range(1, self.max_iterations + 1):
            iteration_changes = 0
            iteration_details: Dict[str, Any] = {}

            for p in self.passes:
                changes, pass_details = p.run(project, cfg, state)
                iteration_changes += changes
                total_changes += changes
                passes_run.append(p.name)
                iteration_details[p.name] = pass_details

            details["per_iteration"].append(iteration_details)

            if iteration_changes == 0:
                return OptimizationReport(
                    iterations=iteration,
                    changes_applied=total_changes,
                    passes_run=passes_run,
                    details=details,
                )

            # Rebuild CFG to reflect any inferred changes in reachability/structure
            cfg = project.analyses.CFGFast(normalize=True)

        return OptimizationReport(
            iterations=self.max_iterations,
            changes_applied=total_changes,
            passes_run=passes_run,
            details=details,
        )



```

`src/security/__init__.py`:

```py
"""
Security utilities for RE-Architect.

This module provides security-focused utilities for safe file operations,
input validation, and subprocess execution to prevent common security
vulnerabilities.
"""

__all__ = [
    'SecurityValidator',
    'SecurityAudit', 
    'SecurityError',
    'PathTraversalError',
    'InputValidationError',
    'UnsafeOperationError'
]

import os
import re
import shlex
import subprocess
import logging
import sys
from pathlib import Path, PurePath
from typing import List, Dict, Any, Optional, Union
import hashlib
import tempfile

logger = logging.getLogger("re-architect.security")

class SecurityError(Exception):
    """Raised when a security validation fails."""
    pass

class PathTraversalError(SecurityError):
    """Raised when path traversal is detected."""
    pass

class InputValidationError(SecurityError):
    """Raised when input validation fails."""
    pass

class UnsafeOperationError(SecurityError):
    """Raised when an unsafe operation is attempted."""
    pass

class SecurityValidator:
    """
    Security validator for RE-Architect operations.
    
    Provides methods to validate inputs, sanitize paths, and ensure
    safe operations throughout the application.
    """
    
    # Allowed file extensions for analysis
    ALLOWED_BINARY_EXTENSIONS = {
        '.exe', '.dll', '.so', '.dylib', '.bin', '.elf', 
        '.o', '.obj', '.a', '.lib', '.sys', '.ko'
    }
    
    # Dangerous characters in filenames (OS-aware)
    @property
    def DANGEROUS_FILENAME_CHARS(self):
        chars = ['..', '~', '$', '`', '|', '&', ';', '<', '>', '(', ')', '[', ']', '{', '}', '"', "'"]
        # Only include backslash as dangerous on non-Windows systems
        if sys.platform != 'win32':
            chars.append('\\')
        return chars
    
    # Maximum file size for analysis (500MB)
    MAX_FILE_SIZE = 500 * 1024 * 1024
    
    # Safe characters for identifiers
    SAFE_IDENTIFIER_PATTERN = re.compile(r'^[a-zA-Z_][a-zA-Z0-9_]*$')
    
    @staticmethod
    def validate_file_path(file_path: Union[str, Path], base_dir: Optional[Union[str, Path]] = None) -> Path:
        """
        Validate and sanitize a file path to prevent path traversal attacks.
        
        Args:
            file_path: Path to validate
            base_dir: Optional base directory to restrict path to
            
        Returns:
            Validated Path object
            
        Raises:
            PathTraversalError: If path traversal is detected
            FileNotFoundError: If file doesn't exist
            SecurityError: If path is unsafe
        """
        try:
            # Convert to Path object
            path = Path(file_path).resolve()
            
            # Check for dangerous patterns
            path_str = str(path)
            validator_instance = SecurityValidator()
            for dangerous_char in validator_instance.DANGEROUS_FILENAME_CHARS:
                if dangerous_char in path_str:
                    raise PathTraversalError(f"Dangerous character '{dangerous_char}' in path: {path}")
            
            # If base_dir is provided, ensure path is within it
            if base_dir:
                base_path = Path(base_dir).resolve()
                try:
                    path.relative_to(base_path)
                except ValueError:
                    raise PathTraversalError(f"Path {path} is outside base directory {base_path}")
            
            # Check if file exists
            if not path.exists():
                raise FileNotFoundError(f"File not found: {path}")
            
            # Check if it's actually a file (not a directory or special file)
            if not path.is_file():
                raise SecurityError(f"Path is not a regular file: {path}")
            
            return path
            
        except Exception as e:
            if isinstance(e, (PathTraversalError, FileNotFoundError, SecurityError)):
                raise
            raise SecurityError(f"Invalid path: {file_path}") from e
    
    @staticmethod
    def validate_binary_file(file_path: Union[str, Path], check_size: bool = True) -> Path:
        """
        Validate a binary file for analysis.
        
        Args:
            file_path: Path to binary file
            check_size: Whether to check file size limits
            
        Returns:
            Validated Path object
            
        Raises:
            SecurityError: If file is unsafe for analysis
        """
        path = SecurityValidator.validate_file_path(file_path)
        
        # Check file extension
        if path.suffix.lower() not in SecurityValidator.ALLOWED_BINARY_EXTENSIONS:
            logger.warning(f"File has unexpected extension: {path.suffix}")
            # Don't raise an error, but log the warning
        
        # Check file size
        if check_size:
            file_size = path.stat().st_size
            if file_size > SecurityValidator.MAX_FILE_SIZE:
                raise SecurityError(
                    f"File too large: {file_size} bytes (max: {SecurityValidator.MAX_FILE_SIZE})"
                )
            
            if file_size == 0:
                raise SecurityError("File is empty")
        
        return path
    
    @staticmethod
    def validate_output_directory(dir_path: Union[str, Path], create_if_missing: bool = True) -> Path:
        """
        Validate and create output directory.
        
        Args:
            dir_path: Directory path to validate
            create_if_missing: Whether to create directory if it doesn't exist
            
        Returns:
            Validated Path object
            
        Raises:
            SecurityError: If directory is unsafe
        """
        try:
            path = Path(dir_path).resolve()
            
            # Check for dangerous patterns
            path_str = str(path)
            validator_instance = SecurityValidator()
            for dangerous_char in validator_instance.DANGEROUS_FILENAME_CHARS:
                if dangerous_char in path_str:
                    raise PathTraversalError(f"Dangerous character '{dangerous_char}' in path: {path}")
            
            if path.exists():
                if not path.is_dir():
                    raise SecurityError(f"Path exists but is not a directory: {path}")
            elif create_if_missing:
                path.mkdir(parents=True, exist_ok=True)
                logger.info(f"Created output directory: {path}")
            
            return path
            
        except Exception as e:
            if isinstance(e, (PathTraversalError, SecurityError)):
                raise
            raise SecurityError(f"Invalid directory path: {dir_path}") from e
    
    @staticmethod
    def validate_identifier(identifier: str, max_length: int = 255) -> str:
        """
        Validate an identifier (function name, variable name, etc.).
        
        Args:
            identifier: Identifier to validate
            max_length: Maximum allowed length
            
        Returns:
            Validated identifier
            
        Raises:
            InputValidationError: If identifier is invalid
        """
        if not isinstance(identifier, str):
            raise InputValidationError("Identifier must be a string")
        
        if len(identifier) == 0:
            raise InputValidationError("Identifier cannot be empty")
        
        if len(identifier) > max_length:
            raise InputValidationError(f"Identifier too long: {len(identifier)} > {max_length}")
        
        if not SecurityValidator.SAFE_IDENTIFIER_PATTERN.match(identifier):
            raise InputValidationError(f"Identifier contains unsafe characters: {identifier}")
        
        return identifier
    
    @staticmethod
    def safe_subprocess_run(cmd: List[str], 
                           timeout: Optional[int] = None,
                           cwd: Optional[Union[str, Path]] = None,
                           env: Optional[Dict[str, str]] = None,
                           **kwargs) -> subprocess.CompletedProcess:
        """
        Safely execute a subprocess command with security controls.
        
        Args:
            cmd: Command and arguments as a list
            timeout: Timeout in seconds
            cwd: Working directory
            env: Environment variables
            **kwargs: Additional subprocess arguments
            
        Returns:
            CompletedProcess result
            
        Raises:
            UnsafeOperationError: If command is unsafe
            subprocess.SubprocessError: If execution fails
        """
        if not isinstance(cmd, list):
            raise UnsafeOperationError("Command must be a list, not a string")
        
        if not cmd:
            raise UnsafeOperationError("Command list cannot be empty")
        
        # Validate executable path
        executable = Path(cmd[0])
        if not executable.is_absolute():
            # Try to find the executable in PATH
            executable = SecurityValidator._find_executable(cmd[0])
            if not executable:
                raise UnsafeOperationError(f"Executable not found: {cmd[0]}")
        
        # Validate working directory
        if cwd:
            cwd_path = SecurityValidator.validate_output_directory(cwd, create_if_missing=False)
            cwd = str(cwd_path)
        
        # Sanitize environment variables
        if env:
            env = SecurityValidator._sanitize_env(env)
        
        # Set safe defaults
        safe_kwargs = {
            'stdout': subprocess.PIPE,
            'stderr': subprocess.PIPE,
            'text': True,
            'timeout': timeout or 300,  # Default 5-minute timeout
            'check': False,  # Don't raise exception on non-zero exit
        }
        safe_kwargs.update(kwargs)
        
        # Log the command (but sanitize sensitive data)
        safe_cmd_log = SecurityValidator._sanitize_command_for_logging(cmd)
        logger.info(f"Executing command: {' '.join(safe_cmd_log)}")
        
        try:
            return subprocess.run(cmd, cwd=cwd, env=env, **safe_kwargs)
        except subprocess.TimeoutExpired as e:
            logger.error(f"Command timed out after {timeout} seconds: {cmd[0]}")
            raise
        except Exception as e:
            logger.error(f"Command execution failed: {cmd[0]} - {e}")
            raise
    
    @staticmethod
    def _find_executable(name: str) -> Optional[Path]:
        """Find executable in PATH."""
        import shutil
        exec_path = shutil.which(name)
        return Path(exec_path) if exec_path else None
    
    @staticmethod
    def _sanitize_env(env: Dict[str, str]) -> Dict[str, str]:
        """Sanitize environment variables."""
        sanitized = {}
        for key, value in env.items():
            # Only allow safe environment variables
            if key.startswith(('PATH', 'HOME', 'USER', 'TEMP', 'TMP')):
                sanitized[key] = value
        return sanitized
    
    @staticmethod
    def _sanitize_command_for_logging(cmd: List[str]) -> List[str]:
        """Sanitize command for safe logging (remove sensitive data)."""
        sanitized = []
        for arg in cmd:
            # Hide potential passwords, keys, tokens
            if any(sensitive in arg.lower() for sensitive in ['password', 'key', 'token', 'secret']):
                sanitized.append('[REDACTED]')
            else:
                sanitized.append(arg)
        return sanitized
    
    @staticmethod
    def calculate_file_hash(file_path: Union[str, Path], algorithm: str = 'sha256') -> str:
        """
        Calculate cryptographic hash of a file.
        
        Args:
            file_path: Path to file
            algorithm: Hash algorithm ('md5', 'sha1', 'sha256', 'sha512')
            
        Returns:
            Hexadecimal hash string
        """
        path = SecurityValidator.validate_file_path(file_path)
        
        hasher = hashlib.new(algorithm)
        with open(path, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hasher.update(chunk)
        
        return hasher.hexdigest()
    
    @staticmethod
    def create_secure_temp_file(suffix: str = '', prefix: str = 're_architect_') -> Path:
        """
        Create a secure temporary file.
        
        Args:
            suffix: File suffix
            prefix: File prefix
            
        Returns:
            Path to temporary file
        """
        fd, temp_path = tempfile.mkstemp(suffix=suffix, prefix=prefix)
        os.close(fd)
        return Path(temp_path)
    
    @staticmethod
    def create_secure_temp_dir(prefix: str = 're_architect_') -> Path:
        """
        Create a secure temporary directory.
        
        Args:
            prefix: Directory prefix
            
        Returns:
            Path to temporary directory
        """
        temp_dir = tempfile.mkdtemp(prefix=prefix)
        return Path(temp_dir)


class SecurityAudit:
    """Security audit utilities for RE-Architect."""
    
    @staticmethod
    def audit_file_permissions(file_path: Union[str, Path]) -> Dict[str, Any]:
        """
        Audit file permissions and ownership.
        
        Args:
            file_path: Path to audit
            
        Returns:
            Dictionary with permission information
        """
        path = Path(file_path)
        stat_info = path.stat()
        
        return {
            'path': str(path),
            'mode': oct(stat_info.st_mode),
            'uid': stat_info.st_uid,
            'gid': stat_info.st_gid,
            'size': stat_info.st_size,
            'is_readable': os.access(path, os.R_OK),
            'is_writable': os.access(path, os.W_OK),
            'is_executable': os.access(path, os.X_OK),
        }
    
    @staticmethod
    def scan_for_suspicious_patterns(file_path: Union[str, Path]) -> List[Dict[str, Any]]:
        """
        Scan a file for suspicious patterns that might indicate malware.
        
        Args:
            file_path: Path to scan
            
        Returns:
            List of suspicious patterns found
        """
        path = SecurityValidator.validate_file_path(file_path)
        
        # Basic suspicious patterns (this is a simplified implementation)
        suspicious_patterns = [
            (rb'CreateRemoteThread', 'Code injection function'),
            (rb'VirtualAllocEx', 'Memory allocation in other process'),
            (rb'WriteProcessMemory', 'Memory writing in other process'),
            (rb'SetWindowsHookEx', 'Hook installation'),
            (rb'GetProcAddress', 'Dynamic API resolution'),
            (rb'LoadLibrary', 'Dynamic library loading'),
        ]
        
        findings = []
        try:
            with open(path, 'rb') as f:
                content = f.read()
                
                for pattern, description in suspicious_patterns:
                    matches = []
                    start = 0
                    while True:
                        pos = content.find(pattern, start)
                        if pos == -1:
                            break
                        matches.append(pos)
                        start = pos + 1
                    
                    if matches:
                        findings.append({
                            'pattern': pattern.decode('utf-8', errors='ignore'),
                            'description': description,
                            'positions': matches,
                            'count': len(matches)
                        })
        except Exception as e:
            logger.error(f"Error scanning file for suspicious patterns: {e}")
        
        return findings
```

`src/test_generation/__init__.py`:

```py
"""
Test generation package for RE-Architect.
"""

```

`src/test_generation/test_generator.py`:

```py
"""
Test generator module for RE-Architect.

This module generates safe test harnesses for decompiled functions.
"""

import logging
import os
from typing import Dict, List, Optional, Any, Set

from src.core.config import Config

logger = logging.getLogger("re-architect.test_generation")

class TestGenerator:
    """
    Test generator for RE-Architect.
    
    This class generates safe test harnesses for decompiled functions,
    allowing users to execute and verify function behavior.
    """
    
    def __init__(self, config: Config):
        """
        Initialize the test generator.
        
        Args:
            config: Configuration object
        """
        self.config = config
        self.sanitizers = config.get("test_generation.sanitizers", ["address", "undefined"])
        self.fuzzing_time = config.get("test_generation.fuzzing_time", 60)
        self.max_test_cases = config.get("test_generation.max_test_cases", 10)
        self.compiler = config.get("test_generation.compiler", "gcc")
        self.compiler_flags = config.get("test_generation.compiler_flags", ["-O0", "-g"])
    
    def generate(
        self,
        functions: Dict[int, Dict[str, Any]],
        data_structures: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Dict[str, Any]]:
        """
        Generate test harnesses for functions.
        
        Args:
            functions: Dictionary of functions to generate tests for
            data_structures: Dictionary of data structures used by the functions
            
        Returns:
            Dictionary mapping function IDs to test harness information
        """
        logger.info("Generating test harnesses")
        
        test_harnesses = {}
        
        # Sort functions by complexity (less complex first)
        sorted_functions = sorted(
            functions.items(),
            key=lambda x: x[1].get("complexity", 999)
        )
        
        # Generate tests for the most promising functions
        count = 0
        for func_id, func_info in sorted_functions:
            # Skip if it's a library function
            if func_info.get("is_library", False):
                continue
            
            # Skip if it's too complex
            if func_info.get("complexity", 0) > 20:
                continue
                
            # Skip if it has too many dependencies
            if len(func_info.get("calls", [])) > 5:
                continue
                
            logger.info(f"Generating test for {func_info['name']}")
            
            try:
                # Generate the test harness
                test_info = self._generate_test_harness(func_id, func_info, functions, data_structures)
                
                if test_info:
                    test_harnesses[func_id] = test_info
                    count += 1
                    
                    # Limit the number of test harnesses
                    if count >= self.max_test_cases:
                        break
                        
            except Exception as e:
                logger.warning(f"Error generating test for {func_info['name']}: {e}")
        
        logger.info(f"Generated {len(test_harnesses)} test harnesses")
        return test_harnesses
    
    def _generate_test_harness(
        self,
        func_id: int,
        func_info: Dict[str, Any],
        all_functions: Dict[int, Dict[str, Any]],
        data_structures: Dict[str, Dict[str, Any]]
    ) -> Optional[Dict[str, Any]]:
        """
        Generate a test harness for a specific function.
        
        Args:
            func_id: Function ID
            func_info: Function information
            all_functions: Dictionary of all functions
            data_structures: Dictionary of data structures
            
        Returns:
            Dictionary containing test harness information, or None if generation fails
        """
        # Extract function name and code
        func_name = func_info["name"]
        func_code = func_info["code"]
        
        # Get function signature
        signature = func_info.get("signature", "")
        
        # Get return type
        return_type = func_info.get("return_type", "int")
        if not return_type:
            return_type = "int"
        
        # Get parameters
        parameters = func_info.get("parameters", [])
        
        # Identify dependencies
        dependencies = self._identify_dependencies(func_info, all_functions, data_structures)
        
        # Generate test source code
        source_code = self._generate_test_source(
            func_name,
            func_code,
            return_type,
            parameters,
            dependencies
        )
        
        # Generate build script
        build_script = self._generate_build_script(func_name, dependencies)
        
        return {
            "function_id": func_id,
            "function_name": func_name,
            "source_code": source_code,
            "build_script": build_script,
            "dependencies": dependencies,
            "has_fuzz_target": self._can_generate_fuzz_target(func_info)
        }
    
    def _identify_dependencies(
        self,
        func_info: Dict[str, Any],
        all_functions: Dict[int, Dict[str, Any]],
        data_structures: Dict[str, Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Identify dependencies for a function.
        
        Args:
            func_info: Function information
            all_functions: Dictionary of all functions
            data_structures: Dictionary of data structures
            
        Returns:
            Dictionary containing dependency information
        """
        # Identify called functions
        called_functions = []
        for call in func_info.get("calls", []):
            target_addr_str = call.get("toAddress", "")
            target_name = call.get("toFunction", "")
            
            # Try to find the actual function info
            target_info = None
            for addr, info in all_functions.items():
                if info["name"] == target_name:
                    target_info = info
                    break
            
            if target_info:
                called_functions.append({
                    "name": target_name,
                    "is_library": target_info.get("is_library", False)
                })
            else:
                called_functions.append({
                    "name": target_name,
                    "is_library": True  # Assume external if not found
                })
        
        # Identify used data structures
        used_structures = []
        for struct_name, struct_info in data_structures.items():
            # Check if the structure is used in the function code
            if struct_name in func_info.get("code", ""):
                used_structures.append(struct_name)
        
        return {
            "called_functions": called_functions,
            "used_structures": used_structures
        }
    
    def _generate_test_source(
        self,
        func_name: str,
        func_code: str,
        return_type: str,
        parameters: List[Dict[str, Any]],
        dependencies: Dict[str, Any]
    ) -> str:
        """
        Generate test source code.
        
        Args:
            func_name: Function name
            func_code: Function code
            return_type: Return type
            parameters: Parameter information
            dependencies: Dependency information
            
        Returns:
            Test source code
        """
        # Start with includes
        lines = [
            "/* Test harness for function {} */".format(func_name),
            "#include <stdio.h>",
            "#include <stdlib.h>",
            "#include <string.h>",
            "#include <stdint.h>",
            "#include <stdbool.h>",
            ""
        ]
        
        # Add any necessary structure definitions
        for struct_name in dependencies["used_structures"]:
            lines.append("/* Structure {} definition (placeholder) */".format(struct_name))
            lines.append("typedef struct {} {{".format(struct_name))
            lines.append("    int placeholder;")
            lines.append("    /* Add actual fields here */")
            lines.append("}} {};".format(struct_name))
            lines.append("")
        
        # Add function declaration
        lines.append("/* Original function declaration */")
        
        # Try to extract function declaration from the code
        declaration = self._extract_function_declaration(func_code, func_name, return_type, parameters)
        lines.append(declaration + ";")
        lines.append("")
        
        # Add function implementation (commented out as reference)
        lines.append("/* Original function implementation (for reference) */")
        lines.append("/*")
        for line in func_code.splitlines():
            lines.append(" * " + line)
        lines.append(" */")
        lines.append("")
        
        # Create test main function
        lines.append("int main(int argc, char **argv) {")
        lines.append("    printf(\"Testing function {}...\\n\");".format(func_name))
        lines.append("")
        
        # Create parameter variables
        lines.append("    /* Create test parameters */")
        for i, param in enumerate(parameters):
            param_name = param.get("name", "param{}".format(i))
            param_type = param.get("dataType", "int")
            
            # Generate appropriate initialization based on type
            if "char*" in param_type or "char *" in param_type or "string" in param_type.lower():
                lines.append("    {} = \"test_string\";".format(param_name))
            elif "int" in param_type:
                lines.append("    {} = 42;".format(param_name))
            elif "float" in param_type or "double" in param_type:
                lines.append("    {} = 3.14;".format(param_name))
            elif "bool" in param_type:
                lines.append("    {} = true;".format(param_name))
            elif "*" in param_type:  # Pointer type
                lines.append("    {} = ({}) malloc(sizeof({}));".format(
                    param_name, param_type, param_type.replace("*", "")
                ))
                lines.append("    if (!{}) {{".format(param_name))
                lines.append("        printf(\"Memory allocation failed\\n\");")
                lines.append("        return 1;")
                lines.append("    }")
            else:
                lines.append("    {} = ({}) 0;  /* Initialize with default value */".format(
                    param_name, param_type
                ))
        
        lines.append("")
        
        # Call the function
        lines.append("    /* Call the function */")
        if return_type != "void":
            lines.append("    {} result = {}({});".format(
                return_type,
                func_name,
                ", ".join(p.get("name", "param{}".format(i)) for i, p in enumerate(parameters))
            ))
            lines.append("    printf(\"Result: %d\\n\", result);")
        else:
            lines.append("    {}({});".format(
                func_name,
                ", ".join(p.get("name", "param{}".format(i)) for i, p in enumerate(parameters))
            ))
        
        lines.append("")
        
        # Free any allocated memory
        lines.append("    /* Clean up */")
        for i, param in enumerate(parameters):
            param_name = param.get("name", "param{}".format(i))
            param_type = param.get("dataType", "int")
            
            if "*" in param_type and "char*" not in param_type and "char *" not in param_type:
                lines.append("    free({});".format(param_name))
        
        lines.append("")
        lines.append("    return 0;")
        lines.append("}")
        
        return "\n".join(lines)
    
    def _extract_function_declaration(
        self,
        func_code: str,
        func_name: str,
        return_type: str,
        parameters: List[Dict[str, Any]]
    ) -> str:
        """
        Extract or construct a function declaration.
        
        Args:
            func_code: Function code
            func_name: Function name
            return_type: Return type
            parameters: Parameter information
            
        Returns:
            Function declaration string
        """
        # Try to extract the declaration from the code
        lines = func_code.splitlines()
        for i, line in enumerate(lines):
            if func_name in line and "(" in line and (i == 0 or "{" not in lines[i-1]):
                # This might be the declaration line
                end_line = i
                while end_line < len(lines) and ")" not in lines[end_line]:
                    end_line += 1
                
                if end_line < len(lines):
                    declaration = " ".join(lines[i:end_line+1])
                    # Remove any trailing '{' and surrounding whitespace
                    declaration = declaration.split("{")[0].strip()
                    return declaration
        
        # If we couldn't extract it, construct one
        param_str = ", ".join([
            "{} {}".format(p.get("dataType", "int"), p.get("name", "param{}".format(i)))
            for i, p in enumerate(parameters)
        ])
        
        if not param_str:
            param_str = "void"
            
        return "{} {}({})".format(return_type, func_name, param_str)
    
    def _generate_build_script(self, func_name: str, dependencies: Dict[str, Any]) -> str:
        """
        Generate a build script for the test harness.
        
        Args:
            func_name: Function name
            dependencies: Dependency information
            
        Returns:
            Build script content
        """
        compiler = self.compiler
        flags = " ".join(self.compiler_flags)
        
        # Add sanitizer flags if enabled
        san_flags = ""
        for sanitizer in self.sanitizers:
            san_flags += f" -fsanitize={sanitizer}"
        
        lines = [
            "#!/bin/bash",
            "# Build script for {} test harness".format(func_name),
            "",
            "# Compile the test",
            "{} {} {} {}_test.c -o {}_test".format(compiler, flags, san_flags, func_name, func_name),
            "",
            "# Check if compilation was successful",
            "if [ $? -eq 0 ]; then",
            "    echo \"Build successful\"",
            "    echo \"Run with: ./{}_test\"".format(func_name),
            "else",
            "    echo \"Build failed\"",
            "fi"
        ]
        
        return "\n".join(lines)
    
    def _can_generate_fuzz_target(self, func_info: Dict[str, Any]) -> bool:
        """
        Determine if we can generate a fuzzing target for this function.
        
        Args:
            func_info: Function information
            
        Returns:
            True if a fuzz target can be generated
        """
        # Check if the function has string or array parameters
        # which are good candidates for fuzzing
        parameters = func_info.get("parameters", [])
        
        for param in parameters:
            param_type = param.get("dataType", "")
            if "*" in param_type or "[]" in param_type:
                return True
        
        return False

```

`src/testing/__init__.py`:

```py
"""
Advanced testing infrastructure for RE-Architect.

This module provides comprehensive testing capabilities including unit tests,
integration tests, property-based testing, fuzzing, and benchmark tests.
"""

import asyncio
import functools
import hypothesis
import hypothesis.strategies as st
import pytest
import pytest_benchmark
import tempfile
import time
import unittest
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Type, Union
from unittest.mock import Mock, patch, MagicMock
import concurrent.futures
import threading

# Import testing utilities
try:
    import hypothesis
    from hypothesis import given, strategies as st
    HYPOTHESIS_AVAILABLE = True
except ImportError:
    HYPOTHESIS_AVAILABLE = False

try:
    import pytest_benchmark
    BENCHMARK_AVAILABLE = True
except ImportError:
    BENCHMARK_AVAILABLE = False

# Import fuzzing capabilities
try:
    import atheris
    FUZZING_AVAILABLE = True
except ImportError:
    FUZZING_AVAILABLE = False


class TestDataGenerator:
    """Generate test data for various RE-Architect components."""
    
    @staticmethod
    def generate_binary_data(size: int = 1024) -> bytes:
        """Generate random binary data for testing."""
        import random
        return bytes([random.randint(0, 255) for _ in range(size)])
    
    @staticmethod
    def generate_pe_header() -> bytes:
        """Generate a minimal valid PE header."""
        # DOS header
        dos_header = b'MZ' + b'\x00' * 58 + b'\x80\x00\x00\x00'
        
        # PE signature
        pe_sig = b'PE\x00\x00'
        
        # File header
        file_header = (
            b'\x4c\x01'  # Machine (i386)
            b'\x03\x00'  # NumberOfSections
            b'\x00\x00\x00\x00'  # TimeDateStamp
            b'\x00\x00\x00\x00'  # PointerToSymbolTable
            b'\x00\x00\x00\x00'  # NumberOfSymbols
            b'\xe0\x00'  # SizeOfOptionalHeader
            b'\x0f\x01'  # Characteristics
        )
        
        # Optional header (minimal)
        opt_header = b'\x0b\x01' + b'\x00' * 222  # Magic + padding
        
        return dos_header + pe_sig + file_header + opt_header
    
    @staticmethod
    def generate_elf_header() -> bytes:
        """Generate a minimal valid ELF header."""
        return (
            b'\x7fELF' +   # ELF magic
            b'\x01' +      # 32-bit
            b'\x01' +      # Little endian
            b'\x01' +      # ELF version
            b'\x00' * 9 +  # Padding
            b'\x02\x00' +  # Executable file
            b'\x03\x00' +  # i386
            b'\x01\x00\x00\x00' +  # ELF version
            b'\x00' * 36   # Rest of header
        )
    
    @staticmethod
    def create_test_binary(format_type: str = "pe", size: int = 2048) -> bytes:
        """Create a test binary with specified format."""
        if format_type.lower() == "pe":
            header = TestDataGenerator.generate_pe_header()
        elif format_type.lower() == "elf":
            header = TestDataGenerator.generate_elf_header()
        else:
            header = b'\x00' * 64  # Generic header
        
        # Pad with random data
        remaining_size = max(0, size - len(header))
        padding = TestDataGenerator.generate_binary_data(remaining_size)
        
        return header + padding


class MockFactory:
    """Factory for creating mock objects and test doubles."""
    
    @staticmethod
    def create_mock_binary_info(
        path: str = "/test/binary.exe",
        architecture: str = "x86_64",
        format_type: str = "PE",
        entry_point: int = 0x401000
    ) -> Mock:
        """Create a mock BinaryInfo object."""
        mock_info = Mock()
        mock_info.path = Path(path)
        mock_info.architecture = architecture
        mock_info.format = format_type
        mock_info.entry_point = entry_point
        mock_info.size = 2048
        mock_info.compiler = "MSVC"
        return mock_info
    
    @staticmethod
    def create_mock_decompiled_code(
        functions: Optional[Dict[str, str]] = None
    ) -> Mock:
        """Create a mock DecompiledCode object."""
        if functions is None:
            functions = {
                "main": "int main() { return 0; }",
                "func1": "void func1() { printf(\"Hello\"); }"
            }
        
        mock_code = Mock()
        mock_code.functions = functions
        mock_code.success = True
        mock_code.metadata = {"decompiler": "mock", "version": "1.0"}
        return mock_code
    
    @staticmethod
    def create_mock_config(overrides: Optional[Dict[str, Any]] = None) -> Mock:
        """Create a mock Config object."""
        config = Mock()
        config.get = Mock(side_effect=lambda key, default=None: overrides.get(key, default) if overrides else default)
        config.use_llm = False
        return config


class TestFixtures:
    """Common test fixtures and utilities."""
    
    @staticmethod
    def create_temp_binary(content: Optional[bytes] = None) -> Path:
        """Create a temporary binary file for testing."""
        if content is None:
            content = TestDataGenerator.create_test_binary()
        
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix='.exe')
        temp_file.write(content)
        temp_file.close()
        
        return Path(temp_file.name)
    
    @staticmethod
    def create_temp_directory() -> Path:
        """Create a temporary directory for testing."""
        temp_dir = tempfile.mkdtemp(prefix='re_architect_test_')
        return Path(temp_dir)


class AsyncTestCase(unittest.TestCase):
    """Base class for async test cases."""
    
    def setUp(self) -> None:
        """Set up async test environment."""
        self.loop = asyncio.new_event_loop()
        asyncio.set_event_loop(self.loop)
    
    def tearDown(self) -> None:
        """Clean up async test environment."""
        self.loop.close()
    
    def run_async(self, coro: Any) -> Any:
        """Run an async coroutine in the test loop."""
        return self.loop.run_until_complete(coro)


def async_test(func: Callable) -> Callable:
    """Decorator for async test functions."""
    @functools.wraps(func)
    def wrapper(self, *args, **kwargs):
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            return loop.run_until_complete(func(self, *args, **kwargs))
        finally:
            loop.close()
    return wrapper


class PerformanceTestCase(unittest.TestCase):
    """Base class for performance tests."""
    
    def setUp(self) -> None:
        """Set up performance monitoring."""
        self.performance_data = []
    
    def measure_performance(self, func: Callable, *args, **kwargs) -> Dict[str, float]:
        """Measure function performance."""
        import psutil
        
        process = psutil.Process()
        start_time = time.time()
        start_memory = process.memory_info().rss / 1024 / 1024
        
        try:
            result = func(*args, **kwargs)
        finally:
            end_time = time.time()
            end_memory = process.memory_info().rss / 1024 / 1024
        
        metrics = {
            'execution_time': end_time - start_time,
            'memory_delta': end_memory - start_memory,
            'peak_memory': end_memory
        }
        
        self.performance_data.append(metrics)
        return metrics
    
    def assert_performance(self, 
                          max_time: Optional[float] = None,
                          max_memory_mb: Optional[float] = None) -> None:
        """Assert performance constraints."""
        if not self.performance_data:
            self.fail("No performance data collected")
        
        latest_metrics = self.performance_data[-1]
        
        if max_time is not None:
            self.assertLessEqual(
                latest_metrics['execution_time'], 
                max_time,
                f"Execution time {latest_metrics['execution_time']:.3f}s exceeds limit {max_time}s"
            )
        
        if max_memory_mb is not None:
            self.assertLessEqual(
                latest_metrics['memory_delta'],
                max_memory_mb,
                f"Memory usage {latest_metrics['memory_delta']:.1f}MB exceeds limit {max_memory_mb}MB"
            )


class PropertyBasedTest:
    """Property-based testing utilities."""
    
    @staticmethod
    def binary_path_strategy() -> Any:
        """Hypothesis strategy for generating binary paths."""
        if not HYPOTHESIS_AVAILABLE:
            return None
        
        return st.builds(
            Path,
            st.text(
                alphabet=st.characters(whitelist_categories=('Lu', 'Ll', 'Nd')),
                min_size=1,
                max_size=50
            ).map(lambda x: f"/test/{x}.exe")
        )
    
    @staticmethod
    def binary_data_strategy(min_size: int = 64, max_size: int = 4096) -> Any:
        """Hypothesis strategy for generating binary data."""
        if not HYPOTHESIS_AVAILABLE:
            return None
        
        return st.binary(min_size=min_size, max_size=max_size)
    
    @staticmethod
    def config_data_strategy() -> Any:
        """Hypothesis strategy for generating config data."""
        if not HYPOTHESIS_AVAILABLE:
            return None
        
        return st.dictionaries(
            keys=st.text(min_size=1, max_size=20),
            values=st.one_of(
                st.text(),
                st.integers(),
                st.booleans(),
                st.floats(allow_nan=False, allow_infinity=False)
            )
        )


class FuzzingFramework:
    """Fuzzing framework for RE-Architect components."""
    
    def __init__(self):
        self.corpus_dir = Path("fuzzing_corpus")
        self.corpus_dir.mkdir(exist_ok=True)
    
    def create_binary_corpus(self, count: int = 100) -> List[Path]:
        """Create a corpus of binary files for fuzzing."""
        corpus_files = []
        
        formats = ["pe", "elf"]
        sizes = [512, 1024, 2048, 4096, 8192]
        
        for i in range(count):
            format_type = formats[i % len(formats)]
            size = sizes[i % len(sizes)]
            
            binary_data = TestDataGenerator.create_test_binary(format_type, size)
            
            corpus_file = self.corpus_dir / f"test_binary_{i:03d}_{format_type}.bin"
            with open(corpus_file, 'wb') as f:
                f.write(binary_data)
            
            corpus_files.append(corpus_file)
        
        return corpus_files
    
    def fuzz_binary_loader(self, iterations: int = 1000) -> None:
        """Fuzz the binary loader component."""
        if not FUZZING_AVAILABLE:
            print("Atheris not available, skipping fuzzing")
            return
        
        def fuzz_target(data: bytes) -> None:
            """Fuzzing target for binary loader."""
            try:
                # Create temporary file with fuzzing data
                temp_file = tempfile.NamedTemporaryFile(delete=False)
                temp_file.write(data)
                temp_file.close()
                
                # Test binary loading
                from src.core.binary_loader import BinaryLoader
                loader = BinaryLoader()
                
                try:
                    loader.load(Path(temp_file.name))
                except Exception:
                    # Expected for malformed binaries
                    pass
                
            finally:
                # Cleanup
                if 'temp_file' in locals():
                    Path(temp_file.name).unlink(missing_ok=True)
        
        # Run fuzzing
        atheris.Setup([], fuzz_target)
        atheris.Fuzz()


class TestReporter:
    """Advanced test reporting and metrics."""
    
    def __init__(self, output_dir: Path):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.test_results = []
        self.performance_data = []
    
    def add_test_result(self, 
                       test_name: str,
                       status: str,
                       duration: float,
                       error_message: Optional[str] = None) -> None:
        """Add a test result."""
        self.test_results.append({
            'test_name': test_name,
            'status': status,
            'duration': duration,
            'error_message': error_message,
            'timestamp': time.time()
        })
    
    def add_performance_data(self, 
                           test_name: str,
                           metrics: Dict[str, float]) -> None:
        """Add performance metrics."""
        self.performance_data.append({
            'test_name': test_name,
            'metrics': metrics,
            'timestamp': time.time()
        })
    
    def generate_report(self) -> Dict[str, Any]:
        """Generate comprehensive test report."""
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result['status'] == 'passed')
        failed_tests = sum(1 for result in self.test_results if result['status'] == 'failed')
        
        avg_duration = sum(result['duration'] for result in self.test_results) / total_tests if total_tests > 0 else 0
        
        return {
            'summary': {
                'total_tests': total_tests,
                'passed': passed_tests,
                'failed': failed_tests,
                'success_rate': (passed_tests / total_tests * 100) if total_tests > 0 else 0,
                'average_duration': avg_duration
            },
            'test_results': self.test_results,
            'performance_data': self.performance_data,
            'generated_at': time.time()
        }
    
    def save_report(self, filename: str = "test_report.json") -> Path:
        """Save report to file."""
        import json
        
        report_path = self.output_dir / filename
        report = self.generate_report()
        
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
        
        return report_path


# Utility decorators
def skip_if_no_hypothesis(func: Callable) -> Callable:
    """Skip test if hypothesis is not available."""
    return unittest.skipIf(not HYPOTHESIS_AVAILABLE, "Hypothesis not available")(func)

def skip_if_no_benchmark(func: Callable) -> Callable:
    """Skip test if pytest-benchmark is not available."""
    return unittest.skipIf(not BENCHMARK_AVAILABLE, "pytest-benchmark not available")(func)

def requires_external_tool(tool_name: str) -> Callable:
    """Skip test if external tool is not available."""
    def decorator(func: Callable) -> Callable:
        # Check if tool is available
        import shutil
        available = shutil.which(tool_name) is not None
        return unittest.skipIf(not available, f"{tool_name} not available")(func)
    return decorator
```

`src/unpacking/__init__.py`:

```py
"""
Unpacking module for RE-Architect.

This module handles unpacking of packed binaries using symbolic execution
with Angr. It can detect common packers and automatically unpack binaries
to enable static analysis.
"""

from .symbolic_unpacker import SymbolicUnpacker, UnpackingResult

__all__ = ["SymbolicUnpacker", "UnpackingResult"]

```

`src/unpacking/symbolic_unpacker.py`:

```py
"""
Symbolic execution unpacker for RE-Architect.

This module uses Angr to symbolically execute packed binaries and extract
the unpacked payload. It can detect common packers and automatically
unpack binaries to enable static analysis.
"""

import logging
import math
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass

# Try to import angr and claripy at module level
try:
    import angr
    import claripy
    ANGR_AVAILABLE = True
except ImportError:
    ANGR_AVAILABLE = False
    # Create placeholder classes to avoid undefined name errors
    class MockAngr:
        pass
    class MockClaripy:
        class ast:
            class BV:
                pass
    angr = MockAngr()
    claripy = MockClaripy()

logger = logging.getLogger("re-architect.unpacking.symbolic")

@dataclass
class UnpackingResult:
    """Result from symbolic unpacking."""
    success: bool
    unpacked_path: Optional[Path] = None
    original_path: Optional[Path] = None
    packer_detected: Optional[str] = None
    unpacking_method: Optional[str] = None
    execution_steps: int = 0
    memory_dumps: List[Tuple[int, int, bytes]] = None  # (address, size, data)
    error_message: Optional[str] = None
    metadata: Dict[str, Any] = None

    def __post_init__(self):
        if self.memory_dumps is None:
            self.memory_dumps = []
        if self.metadata is None:
            self.metadata = {}


class SymbolicUnpacker:
    """
    Symbolic execution unpacker using Angr.
    
    This class can detect common packers and automatically unpack binaries
    using symbolic execution to find the unpacked payload in memory.
    """
    
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize the symbolic unpacker.
        
        Args:
            config: Configuration dictionary for unpacking parameters
        """
        self.config = config or {}
        self.max_execution_steps = self.config.get("unpacking.max_execution_steps", 10000)
        self.timeout_seconds = self.config.get("unpacking.timeout_seconds", 300)
        self.memory_dump_threshold = self.config.get("unpacking.memory_dump_threshold", 1024 * 1024)  # 1MB
        
        # Common packer signatures
        self.packer_signatures = {
            "UPX": [b"UPX!", b"$Info: This file is packed with the UPX"],
            "PECompact": [b"PECompact"],
            "ASPack": [b"aPLib", b"ASPack"],
            "FSG": [b"FSG!"],
            "Mew": [b"Mew11"],
            "Themida": [b"Themida"],
            "VMProtect": [b"VMProtect"],
            "Armadillo": [b"Armadillo"],
            "Enigma": [b"Enigma"],
        }
    
    def detect_packer(self, binary_path: Path) -> Optional[str]:
        """
        Detect if a binary is packed and identify the packer.
        
        Args:
            binary_path: Path to the binary file
            
        Returns:
            Name of detected packer or None if not packed
        """
        try:
            with open(binary_path, 'rb') as f:
                data = f.read()
            
            for packer_name, signatures in self.packer_signatures.items():
                for signature in signatures:
                    if signature in data:
                        logger.info(f"Detected packer: {packer_name}")
                        return packer_name
            
            # Additional heuristics for packed binaries
            if self._is_likely_packed(data):
                return "Unknown"
                
            return None
            
        except Exception as e:
            logger.error(f"Error detecting packer: {e}")
            return None
    
    def _is_likely_packed(self, data: bytes) -> bool:
        """
        Heuristic check for packed binaries.
        
        Args:
            data: Binary data to analyze
            
        Returns:
            True if binary appears to be packed
        """
        # Check for common packed binary characteristics
        if len(data) < 1024:  # Too small to be meaningful
            return False
            
        # High entropy (compressed/encrypted data)
        entropy = self._calculate_entropy(data)
        if entropy > 7.5:  # Very high entropy suggests packed data
            return True
            
        # Check for suspicious section characteristics
        # This is a simplified check - in practice, you'd parse PE/ELF headers
        suspicious_patterns = [
            b".upx", b".packed", b".encrypted", b".compressed",
            b"UPX", b"aPLib", b"FSG", b"Mew"
        ]
        
        for pattern in suspicious_patterns:
            if pattern in data:
                return True
                
        return False
    
    def _calculate_entropy(self, data: bytes) -> float:
        """Calculate Shannon entropy of data."""
        if not data:
            return 0.0
            
        # Count byte frequencies
        byte_counts = [0] * 256
        for byte in data:
            byte_counts[byte] += 1
            
        # Calculate entropy
        entropy = 0.0
        data_len = len(data)
        for count in byte_counts:
            if count > 0:
                probability = count / data_len
                entropy -= probability * math.log2(probability)
                
        return entropy
    
    def unpack(self, binary_path: Path, output_path: Optional[Path] = None) -> UnpackingResult:
        """
        Unpack a binary using symbolic execution.
        
        Args:
            binary_path: Path to the packed binary
            output_path: Optional path for unpacked binary output
            
        Returns:
            UnpackingResult with unpacking status and results
        """
        logger.info(f"Starting symbolic unpacking of {binary_path}")
        
        # Detect packer
        packer = self.detect_packer(binary_path)
        if not packer:
            return UnpackingResult(
                success=False,
                original_path=binary_path,
                error_message="No packer detected"
            )
        
        # Check if Angr is available
        if not ANGR_AVAILABLE:
            return UnpackingResult(
                success=False,
                original_path=binary_path,
                error_message="Angr not available - install with: pip install angr"
            )
        
        try:
            # Create Angr project
            project = angr.Project(str(binary_path), auto_load_libs=False)
            
            # Set up symbolic execution
            state = project.factory.entry_state()
            simgr = project.factory.simulation_manager(state)
            
            # Track execution steps
            execution_steps = 0
            memory_dumps = []
            
            # Symbolic execution loop
            while simgr.active and execution_steps < self.max_execution_steps:
                simgr.step()
                execution_steps += 1
                
                # Check for unpacking completion
                if self._check_unpacking_complete(simgr, project):
                    logger.info("Unpacking appears complete")
                    break
                
                # Dump memory periodically to look for unpacked code
                if execution_steps % 1000 == 0:
                    self._dump_memory_regions(simgr, memory_dumps)
            
            # Final memory dump
            self._dump_memory_regions(simgr, memory_dumps)
            
            # Extract unpacked binary
            unpacked_data = self._extract_unpacked_binary(simgr, project)
            
            if unpacked_data:
                # Save unpacked binary
                if output_path is None:
                    output_path = binary_path.parent / f"{binary_path.stem}_unpacked{binary_path.suffix}"
                
                with open(output_path, 'wb') as f:
                    f.write(unpacked_data)
                
                logger.info(f"Successfully unpacked binary to {output_path}")
                
                return UnpackingResult(
                    success=True,
                    unpacked_path=output_path,
                    original_path=binary_path,
                    packer_detected=packer,
                    unpacking_method="symbolic_execution",
                    execution_steps=execution_steps,
                    memory_dumps=memory_dumps,
                    metadata={
                        "angr_version": getattr(angr, '__version__', 'unknown'),
                        "project_arch": str(project.arch),
                        "entry_point": hex(project.entry),
                    }
                )
            else:
                return UnpackingResult(
                    success=False,
                    original_path=binary_path,
                    packer_detected=packer,
                    execution_steps=execution_steps,
                    error_message="Failed to extract unpacked binary"
                )
                
        except Exception as e:
            logger.error(f"Error during symbolic unpacking: {e}")
            return UnpackingResult(
                success=False,
                original_path=binary_path,
                packer_detected=packer,
                error_message=f"Unpacking failed: {str(e)}"
            )
    
    def _check_unpacking_complete(self, simgr, project) -> bool:
        """
        Check if unpacking appears to be complete.
        
        This is a heuristic check - in practice, you'd look for specific
        patterns that indicate unpacking is done.
        """
        # Look for common unpacking completion patterns
        for state in simgr.active:
            # Check if we're in a region that looks like unpacked code
            if hasattr(state, 'addr') and state.addr:
                # Simple heuristic: if we're executing in a region with
                # typical executable characteristics, unpacking might be complete
                try:
                    # This is a simplified check - real implementation would
                    # analyze memory regions and instruction patterns
                    return False  # Placeholder
                except:
                    pass
        return False
    
    def _dump_memory_regions(self, simgr, memory_dumps: List[Tuple[int, int, bytes]]):
        """Dump interesting memory regions during execution."""
        for state in simgr.active:
            try:
                # Dump executable memory regions
                if hasattr(state, 'memory') and hasattr(state.memory, 'regions'):
                    for region in state.memory.regions:
                        if region.is_executable and region.size < self.memory_dump_threshold:
                            try:
                                data = state.memory.load(region.start, region.size)
                                if isinstance(data, claripy.ast.BV):
                                    # Convert symbolic data to concrete if possible
                                    concrete_data = state.solver.eval(data, cast_to=bytes)
                                    memory_dumps.append((region.start, region.size, concrete_data))
                            except:
                                pass
            except:
                pass
    
    def _extract_unpacked_binary(self, simgr, project) -> Optional[bytes]:
        """
        Extract the unpacked binary from memory.
        
        This is a simplified implementation - a real unpacker would
        need to identify the correct memory regions and reconstruct
        the binary format.
        """
        # For now, return a placeholder
        # Real implementation would:
        # 1. Identify the unpacked code region
        # 2. Extract the code and data sections
        # 3. Reconstruct the binary format (PE/ELF)
        # 4. Fix up imports and relocations
        
        logger.warning("Unpacked binary extraction not fully implemented")
        return None
    
    def is_available(self) -> bool:
        """Check if Angr is available for symbolic execution."""
        return ANGR_AVAILABLE
    
    def get_unpacker_info(self) -> Dict[str, Any]:
        """Get information about the unpacker."""
        return {
            "name": "SymbolicUnpacker",
            "available": self.is_available(),
            "max_execution_steps": self.max_execution_steps,
            "timeout_seconds": self.timeout_seconds,
            "supported_packers": list(self.packer_signatures.keys()),
        }

```

`src/visualization/__init__.py`:

```py
"""
Visualization package for RE-Architect.

Provides web-based visualization and exploration of binary analysis results.
"""
```

`src/visualization/mock_data.py`:

```py
"""
Mock data generation for RE-Architect visualization and testing.

This module provides functions to generate realistic mock data for 
testing the visualization components and development purposes.
"""

import random
import time
from typing import Dict, List, Any

def generate_function_mock(func_id: int, name: str = None) -> Dict[str, Any]:
    """
    Generate mock data for a function.
    
    Args:
        func_id: Function ID
        name: Function name (optional)
        
    Returns:
        Dict with mock function data
    """
    if name is None:
        name = f"func_{func_id:04x}"
        
    return {
        "id": str(func_id),
        "name": name,
        "address": 0x400000 + func_id * 0x100,
        "size": random.randint(16, 512),
        "summary": f"Function {name} performs computation and returns a result",
        "complexity": round(random.uniform(1.0, 10.0), 1),
        "has_loops": random.choice([True, False]),
        "parameters": [
            {"name": "arg1", "type": "int", "description": "First argument"},
            {"name": "arg2", "type": "void*", "description": "Pointer argument"}
        ],
        "return_type": random.choice(["int", "void", "char*", "float"]),
        "call_count": random.randint(0, 20),
        "decompiled_code": f"// Decompiled code for {name}\nint {name}(int arg1, void* arg2) {{\n    // Function implementation\n    return 0;\n}}",
        "confidence": random.uniform(0.7, 1.0)
    }

def generate_data_structure_mock(struct_id: int, name: str = None) -> Dict[str, Any]:
    """
    Generate mock data for a data structure.
    
    Args:
        struct_id: Structure ID  
        name: Structure name (optional)
        
    Returns:
        Dict with mock data structure data
    """
    if name is None:
        name = f"struct_{struct_id}"
        
    return {
        "id": str(struct_id),
        "name": name,
        "size": random.choice([8, 16, 32, 64, 128]),
        "alignment": random.choice([4, 8]),
        "fields": [
            {"name": "field1", "type": "int", "offset": 0, "size": 4},
            {"name": "field2", "type": "char*", "offset": 8, "size": 8},
            {"name": "field3", "type": "float", "offset": 16, "size": 4}
        ],
        "usage_count": random.randint(1, 10),
        "confidence": random.uniform(0.6, 0.95)
    }

def generate_test_harness_mock(func_id: int, name: str = None) -> Dict[str, Any]:
    """
    Generate mock data for a test harness.
    
    Args:
        func_id: Function ID
        name: Function name (optional)
        
    Returns:
        Dict with mock test harness data
    """
    if name is None:
        name = f"func_{func_id:04x}"
        
    return {
        "function_id": str(func_id),
        "function_name": name,
        "test_code": f"""// Test harness for {name}
#include <stdio.h>
#include <stdlib.h>
#include <assert.h>

// External declaration of the target function
extern int {name}(int arg1, void* arg2);

int main() {{
    // Prepare test data
    int test_value = 42;
    int* ptr_value = (int*)malloc(sizeof(int));
    *ptr_value = 100;
    
    // Call the function
    int result = {name}(test_value, ptr_value);
    
    // Check the result
    printf("Result: %d\\n", result);
    assert(result == (test_value + 10 + *ptr_value));
    
    // Clean up
    free(ptr_value);
    return 0;
}}""",
        "test_cases": [
            {
                "inputs": {"arg1": 42, "arg2": {"type": "pointer", "value": 100}},
                "expected_output": 152,
                "description": "Standard test case"
            },
            {
                "inputs": {"arg1": 0, "arg2": {"type": "pointer", "value": 50}},
                "expected_output": 60,
                "description": "Zero input test case"
            }
        ],
        "coverage": random.uniform(0.5, 1.0),
        "execution_result": random.choice(["Success", "Failure", "Timeout", "Crash"]),
        "confidence": random.uniform(0.5, 1.0)
    }

def generate_mock_analysis_results(
    num_functions: int = 50,
    num_data_structures: int = 15,
    binary_path: str = "/path/to/example.exe"
) -> Dict[str, Any]:
    """
    Generate complete mock analysis results.
    
    Args:
        num_functions: Number of functions to generate
        num_data_structures: Number of data structures to generate
        binary_path: Path to the binary (used in metadata)
        
    Returns:
        Dict with complete mock analysis results
    """
    # Generate functions
    functions = {}
    for i in range(1, num_functions + 1):
        func = generate_function_mock(i)
        functions[str(i)] = func
    
    # Generate data structures
    data_structures = {}
    for i in range(1, num_data_structures + 1):
        struct = generate_data_structure_mock(i)
        data_structures[str(i)] = struct
    
    # Generate test harnesses for subset of functions
    test_harnesses = {}
    for i in range(1, min(10, num_functions) + 1):
        test = generate_test_harness_mock(i)
        test_harnesses[str(i)] = test
    
    # Generate metadata
    metadata = {
        "binary_path": binary_path,
        "analysis_start_time": time.strftime("%Y-%m-%d %H:%M:%S"),
        "analysis_duration": random.uniform(30, 300),  # seconds
        "total_functions": num_functions,
        "analyzed_functions": num_functions,
        "total_data_structures": num_data_structures,
        "binary_size": random.randint(1024, 1024*1024*10),  # bytes
        "architecture": random.choice(["x86_64", "x86", "arm64", "arm"]),
        "format": random.choice(["PE", "ELF", "Mach-O"]),
        "compiler": random.choice(["gcc", "clang", "msvc", "unknown"]),
        "analysis_settings": {
            "decompiler": "ghidra",
            "static_analysis": True,
            "dynamic_analysis": False,
            "llm_enabled": True
        }
    }
    
    # Analysis statistics
    stats = {
        "static": {
            "executed": True,
            "findings": random.randint(0, 50) if random.choice([True, False]) else 0,
            "execution_time": random.uniform(10.0, 100.0) if random.choice([True, False]) else 0.0
        },
        "symbolic": {
            "executed": random.choice([True, False]),
            "findings": random.randint(0, 30) if random.choice([True, False]) else 0,
            "execution_time": random.uniform(20.0, 200.0) if random.choice([True, False]) else 0.0
        }
    }
    
    # Combine all results
    results = {
        "metadata": metadata,
        "functions": functions,
        "data_structures": data_structures,
        "test_harnesses": test_harnesses,
        "statistics": stats
    }
    
    return results
```

`src/visualization/run_mock_server.py`:

```py
#!/usr/bin/env python3
"""
Script to run the visualization server with mock data for testing.
"""

import sys
from pathlib import Path

# Add src to path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.visualization.server import create_mock_server

def main():
    """Run the mock server."""
    print("Starting RE-Architect visualization server with mock data...")
    print("Server will be available at: http://localhost:5000")
    
    server = create_mock_server()
    server.start()

if __name__ == "__main__":
    main()
```

`src/visualization/server.py`:

```py
"""
Web server module for RE-Architect visualization.

Provides a Flask-based web interface for exploring binary analysis results.
"""

import logging
import os
from pathlib import Path
from typing import Dict, Any, Optional

try:
    from flask import Flask, render_template, jsonify, request, send_from_directory
    FLASK_AVAILABLE = True
except ImportError:
    FLASK_AVAILABLE = False

logger = logging.getLogger("re-architect.visualization.server")

class VisualizationServer:
    """
    Web server for visualizing RE-Architect results.
    
    Provides a Flask-based interface for exploring binary analysis results
    including functions, data structures, and test harnesses.
    """
    
    def __init__(self, host: str = "localhost", port: int = 5000, debug: bool = False):
        """
        Initialize the visualization server.
        
        Args:
            host: Host address to bind to
            port: Port to listen on
            debug: Enable debug mode
        """
        if not FLASK_AVAILABLE:
            raise ImportError("Flask is required for visualization server. Install with: pip install flask")
            
        self.host = host
        self.port = port
        self.debug = debug
        self.app = Flask(__name__, template_folder="templates", static_folder="static")
        self.results = None
        
        # Setup routes
        self._setup_routes()
        
    def _setup_routes(self):
        """Setup Flask routes."""
        
        @self.app.route('/')
        def index():
            """Main dashboard."""
            if not self.results:
                return jsonify({"error": "No analysis results loaded"}), 404
            return render_template('dashboard.html', results=self.results)
        
        @self.app.route('/api/functions')
        def api_functions():
            """Get all functions."""
            if not self.results or 'functions' not in self.results:
                return jsonify({"error": "No functions available"}), 404
            return jsonify(self.results['functions'])
        
        @self.app.route('/api/function/<function_id>')
        def api_function_detail(function_id):
            """Get detailed information about a specific function."""
            if not self.results or 'functions' not in self.results:
                return jsonify({"error": "No functions available"}), 404
                
            function = self.results['functions'].get(function_id)
            if not function:
                return jsonify({"error": "Function not found"}), 404
                
            return jsonify(function)
        
        @self.app.route('/api/data-structures')
        def api_data_structures():
            """Get all data structures."""
            if not self.results or 'data_structures' not in self.results:
                return jsonify({"error": "No data structures available"}), 404
            return jsonify(self.results['data_structures'])
        
        @self.app.route('/api/test-harnesses')
        def api_test_harnesses():
            """Get all test harnesses."""
            if not self.results or 'test_harnesses' not in self.results:
                return jsonify({"error": "No test harnesses available"}), 404
            return jsonify(self.results['test_harnesses'])
        
        @self.app.route('/api/metadata')
        def api_metadata():
            """Get analysis metadata."""
            if not self.results or 'metadata' not in self.results:
                return jsonify({"error": "No metadata available"}), 404
            return jsonify(self.results['metadata'])
        
        @self.app.route('/health')
        def health():
            """Health check endpoint."""
            return jsonify({"status": "ok", "results_loaded": self.results is not None})
    
    def load_results(self, results: Dict[str, Any]):
        """
        Load analysis results for visualization.
        
        Args:
            results: Dictionary containing analysis results
        """
        self.results = results
        logger.info("Analysis results loaded for visualization")
    
    def load_results_from_file(self, results_path: str):
        """
        Load analysis results from a JSON file.
        
        Args:
            results_path: Path to the results JSON file
        """
        import json
        
        try:
            with open(results_path, 'r') as f:
                self.results = json.load(f)
            logger.info(f"Analysis results loaded from {results_path}")
        except Exception as e:
            logger.error(f"Failed to load results from {results_path}: {e}")
            raise
    
    def start(self):
        """Start the web server."""
        if not self.results:
            logger.warning("Starting server without loaded results")
        
        logger.info(f"Starting visualization server at http://{self.host}:{self.port}")
        self.app.run(host=self.host, port=self.port, debug=self.debug)
    
    def get_app(self):
        """Get the Flask app instance for testing."""
        return self.app


def create_mock_server(host: str = "localhost", port: int = 5000):
    """
    Create a visualization server with mock data for testing.
    
    Args:
        host: Host address to bind to
        port: Port to listen on
        
    Returns:
        Configured VisualizationServer instance
    """
    server = VisualizationServer(host, port)
    
    # Load mock results
    mock_results = {
        "metadata": {
            "binary_path": "/path/to/example.exe",
            "analysis_time": "2025-01-01T12:00:00",
            "total_functions": 10,
            "total_data_structures": 3
        },
        "functions": {
            "1": {
                "name": "main",
                "address": 4096,
                "size": 64,
                "summary": "Main entry point function",
                "complexity": 2.5
            },
            "2": {
                "name": "helper_func", 
                "address": 4160,
                "size": 32,
                "summary": "Helper function for data processing",
                "complexity": 1.2
            }
        },
        "data_structures": {
            "1": {
                "name": "data_struct",
                "size": 16,
                "fields": ["field1", "field2"]
            }
        },
        "test_harnesses": {
            "1": {
                "function_name": "main",
                "test_code": "// Test code for main function",
                "coverage": 0.85
            }
        }
    }
    
    server.load_results(mock_results)
    return server
```

`tests/integration/test_dynamic_disabled_pipeline.py`:

```py
import os
import sys
import tempfile
from pathlib import Path

import pytest

# Add the src directory to the Python path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.core.pipeline import ReversePipeline
from src.core.config import Config


class TestPipelineDynamicDisabled:
    @pytest.fixture
    def temp_config_file(self):
        config_content = """
        decompiler:
          default: ghidra
          ghidra:
            path: null
            headless: true
            timeout: 60
        analysis:
          static:
            function_analysis_depth: basic
          dynamic:
            enable: false
        llm:
          enable: false
        output:
          detail_level: basic
          formats: [json]
        """
        with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.yaml') as f:
            f.write(config_content)
            config_path = f.name
        yield config_path
        os.unlink(config_path)

    @pytest.fixture
    def mock_binary_path(self):
        with tempfile.NamedTemporaryFile(delete=False, mode='wb', suffix='.bin') as f:
            f.write(b'\x7fELF\x02\x01\x01')
            binary_path = f.name
        yield binary_path
        os.unlink(binary_path)

    @pytest.fixture
    def pipeline(self, temp_config_file, monkeypatch):
        import src.decompilers.ghidra_decompiler
        monkeypatch.setattr(src.decompilers.ghidra_decompiler.GhidraDecompiler,
                            'decompile',
                            lambda self, binary_info: type('D', (), {
                                'functions': {0x1000: 'int main() { return 0; }'},
                                'function_names': {0x1000: 'main'},
                                'function_metadata': {0x1000: {}}
                            })())
        config = Config(temp_config_file)
        return ReversePipeline(config)

    def test_pipeline_runs_with_dynamic_disabled(self, pipeline, mock_binary_path, monkeypatch):
        import src.analysis.static_analyzer
        class MockAnalysisResults:
            def __init__(self):
                self.functions = {'main': {'name': 'main', 'code': 'int main() { return 0; }'}}
        monkeypatch.setattr(src.analysis.static_analyzer.StaticAnalyzer,
                            'analyze',
                            lambda self, decompiled: MockAnalysisResults())
        import src.analysis.data_structure_analyzer
        monkeypatch.setattr(src.analysis.data_structure_analyzer.DataStructureAnalyzer,
                            'analyze',
                            lambda self, decompiled, static_analysis: {})
        monkeypatch.setattr(ReversePipeline, '_save_results', lambda self: None)
        results = pipeline.analyze(mock_binary_path)
        assert 'functions' in results
        assert 'data_structures' in results

```

`tests/integration/test_obfuscation_pipeline.py`:

```py
from pathlib import Path
from unittest.mock import patch, Mock

from src.core.pipeline import ReversePipeline
from src.core.config import Config


@patch('src.core.pipeline.ObfuscationOptimizer')
def test_pipeline_reports_obfuscation_optimization(mock_opt_class, tmp_path):
    # Prepare a dummy binary file
    binary = tmp_path / 'dummy.bin'
    binary.write_bytes(b'not a real binary')

    # Mock optimizer to return a deterministic report
    mock_opt = Mock()
    mock_opt.is_available.return_value = True
    mock_report = Mock()
    mock_report.iterations = 2
    mock_report.changes_applied = 10
    mock_report.passes_run = ["junk_code_removal", "opaque_predicate_solving"]
    mock_report.details = {"per_iteration": [{}, {}]}
    mock_opt.optimize.return_value = mock_report
    mock_opt_class.return_value = mock_opt

    config = Config()
    pipeline = ReversePipeline(config)
    result = pipeline.analyze(binary)

    assert "obfuscation_optimization" in result
    info = result["obfuscation_optimization"]
    assert info["iterations"] == 2
    assert info["changes_applied"] == 10
    assert info["passes_run"] == ["junk_code_removal", "opaque_predicate_solving"]



```

`tests/integration/test_pipeline.py`:

```py
import os
import pytest
import tempfile
import sys
from pathlib import Path

# Add the src directory to the Python path
sys.path.append(str(Path(__file__).parent.parent.parent))

# Import necessary components for integration testing
from src.core.pipeline import ReversePipeline
from src.core.config import Config


class TestPipelineIntegration:
    @pytest.fixture
    def temp_config_file(self):
        # Create a temporary config file for testing
        config_content = """
        decompiler:
          default: ghidra
          ghidra:
            path: null
            headless: true
            timeout: 60
        analysis:
          static:
            function_analysis_depth: basic
          dynamic:
            enable: false
        llm:
          enable: true
          provider: mock
          model: mock-model
        output:
          detail_level: basic
          formats: [json]
        """
        with tempfile.NamedTemporaryFile(delete=False, mode='w', suffix='.yaml') as f:
            f.write(config_content)
            config_path = f.name
            
        yield config_path
        
        # Clean up the temporary file
        os.unlink(config_path)
    
    @pytest.fixture
    def mock_binary_path(self):
        # Create a temporary binary-like file for testing
        with tempfile.NamedTemporaryFile(delete=False, mode='wb', suffix='.bin') as f:
            f.write(b'\x7fELF\x02\x01\x01\x00\x00\x00\x00\x00\x00\x00\x00\x00\x02\x00\x3e\x00')
            binary_path = f.name
            
        yield binary_path
        
        # Clean up the temporary file
        os.unlink(binary_path)
        
    @pytest.fixture
    def pipeline(self, temp_config_file, monkeypatch):
        # Mock external dependencies
        import src.decompilers.ghidra_decompiler
        monkeypatch.setattr(src.decompilers.ghidra_decompiler.GhidraDecompiler, 
                           'decompile', 
                           lambda self, binary_path: {'functions': {'main': 'int main() { return 0; }'}})
        
        # Create pipeline with config
        config = Config(temp_config_file)
        pipeline = ReversePipeline(config)
        return pipeline
    
    def test_pipeline_initialization(self, pipeline):
        assert pipeline is not None
        assert pipeline.config is not None
        
    def test_pipeline_analysis(self, pipeline, mock_binary_path, monkeypatch):
        # Mock the analysis components to avoid actual execution
        # Create a class with the expected properties to mock StaticAnalyzer's return value
        class MockAnalysisResults:
            def __init__(self):
                self.functions = {'main': {'name': 'main', 'code': 'int main() { return 0; }'}}
        
        # Mock the analyze method to return our mock object
        import src.analysis.static_analyzer
        monkeypatch.setattr(src.analysis.static_analyzer.StaticAnalyzer,
                           'analyze',
                           lambda self, decompiled: MockAnalysisResults())
                           
        # Mock the data structure analyzer
        import src.analysis.data_structure_analyzer
        monkeypatch.setattr(src.analysis.data_structure_analyzer.DataStructureAnalyzer,
                           'analyze',
                           lambda self, decompiled, static_analysis: {})
                           
        # Mock BinaryLoader load method to return a simple dict
        import src.core.binary_loader
        
        class MockBinaryInfo:
            def __init__(self):
                self.path = mock_binary_path
                self.architecture = "x86_64"
                self.compiler = "gcc"
                self.entry_point = 0x1000
                self.sections = {'.text': {'start': 0x1000, 'size': 0x1000, 'data': b'MOCK_CODE'}}
                self.symbols = {}
                self.format = "ELF"
                self.bit_width = 64
                self.endianness = "little"
                self.stripped = False
                self.is_library = False
                self.imports = {}
                self.exports = []
        
        monkeypatch.setattr(src.core.binary_loader.BinaryLoader,
                           'load',
                           lambda self, path, **kwargs: MockBinaryInfo())
                           
        # Mock _save_results to do nothing
        monkeypatch.setattr(src.core.pipeline.ReversePipeline,
                          '_save_results',
                          lambda self: None)
                           
        import src.llm.function_summarizer
        monkeypatch.setattr(src.llm.function_summarizer.FunctionSummarizer,
                           'summarize_function',
                           lambda self, function_code: 'This function returns zero.')
                           
        # Run the pipeline
        results = pipeline.analyze(mock_binary_path)
        
        # Check results
        assert results is not None
        assert 'functions' in results
        assert 'data_structures' in results
        # More assertions based on expected pipeline output

```

`tests/integration/test_unpacking_pipeline.py`:

```py
"""
Integration tests for unpacking in the analysis pipeline.
"""

import pytest
import tempfile
import os
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock

from src.core.binary_loader import BinaryLoader
from src.core.pipeline import ReversePipeline
from src.core.config import Config
from src.unpacking.symbolic_unpacker import SymbolicUnpacker


class TestUnpackingPipeline:
    """Test cases for unpacking integration in the analysis pipeline."""
    
    def setup_method(self):
        """Set up test fixtures."""
        self.temp_dir = tempfile.mkdtemp()
        self.temp_path = Path(self.temp_dir)
        self.binary_loader = BinaryLoader()
    
    def teardown_method(self):
        """Clean up test fixtures."""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_binary_loader_auto_unpack_enabled(self):
        """Test binary loader with auto-unpack enabled."""
        # Create a mock packed binary
        packed_data = b"UPX! This is a packed binary"
        test_binary = self.temp_path / "test_packed.exe"
        test_binary.write_bytes(packed_data)
        
        # Mock the unpacker to return a successful result
        with patch('src.core.binary_loader.SymbolicUnpacker') as mock_unpacker_class:
            mock_unpacker = Mock()
            mock_unpacker.is_available.return_value = True
            mock_unpacker.detect_packer.return_value = "UPX"
            
            # Create a mock unpacked binary
            unpacked_binary = self.temp_path / "test_packed_unpacked.exe"
            unpacked_binary.write_bytes(b"Unpacked binary data")
            
            mock_result = Mock()
            mock_result.success = True
            mock_result.unpacked_path = unpacked_binary
            mock_unpacker.unpack.return_value = mock_result
            
            mock_unpacker_class.return_value = mock_unpacker
            
            # Test loading with auto-unpack enabled
            binary_info = self.binary_loader.load(test_binary, auto_unpack=True)
            
            # Verify unpacker was called
            mock_unpacker.detect_packer.assert_called_once_with(test_binary)
            mock_unpacker.unpack.assert_called_once_with(test_binary)
            
            # Verify the unpacked binary was used
            assert binary_info.path == unpacked_binary
    
    def test_binary_loader_auto_unpack_disabled(self):
        """Test binary loader with auto-unpack disabled."""
        # Create a mock packed binary
        packed_data = b"UPX! This is a packed binary"
        test_binary = self.temp_path / "test_packed.exe"
        test_binary.write_bytes(packed_data)
        
        # Test loading with auto-unpack disabled
        binary_info = self.binary_loader.load(test_binary, auto_unpack=False)
        
        # Verify the original binary was used
        assert binary_info.path == test_binary
    
    def test_binary_loader_unpacker_not_available(self):
        """Test binary loader when unpacker is not available."""
        # Create a mock packed binary
        packed_data = b"UPX! This is a packed binary"
        test_binary = self.temp_path / "test_packed.exe"
        test_binary.write_bytes(packed_data)
        
        # Mock the unpacker to be unavailable
        with patch('src.core.binary_loader.SymbolicUnpacker') as mock_unpacker_class:
            mock_unpacker = Mock()
            mock_unpacker.is_available.return_value = False
            mock_unpacker_class.return_value = mock_unpacker
            
            # Test loading with auto-unpack enabled
            binary_info = self.binary_loader.load(test_binary, auto_unpack=True)
            
            # Verify unpacker was called but no unpacking occurred
            mock_unpacker.is_available.assert_called_once()
            mock_unpacker.detect_packer.assert_not_called()
            
            # Verify the original binary was used
            assert binary_info.path == test_binary
    
    def test_binary_loader_no_packer_detected(self):
        """Test binary loader when no packer is detected."""
        # Create a normal binary
        normal_data = b"Normal binary data"
        test_binary = self.temp_path / "test_normal.exe"
        test_binary.write_bytes(normal_data)
        
        # Mock the unpacker to detect no packer
        with patch('src.core.binary_loader.SymbolicUnpacker') as mock_unpacker_class:
            mock_unpacker = Mock()
            mock_unpacker.is_available.return_value = True
            mock_unpacker.detect_packer.return_value = None
            mock_unpacker_class.return_value = mock_unpacker
            
            # Test loading with auto-unpack enabled
            binary_info = self.binary_loader.load(test_binary, auto_unpack=True)
            
            # Verify unpacker was called but no unpacking occurred
            mock_unpacker.detect_packer.assert_called_once_with(test_binary)
            mock_unpacker.unpack.assert_not_called()
            
            # Verify the original binary was used
            assert binary_info.path == test_binary
    
    def test_binary_loader_unpacking_failed(self):
        """Test binary loader when unpacking fails."""
        # Create a mock packed binary
        packed_data = b"UPX! This is a packed binary"
        test_binary = self.temp_path / "test_packed.exe"
        test_binary.write_bytes(packed_data)
        
        # Mock the unpacker to fail unpacking
        with patch('src.core.binary_loader.SymbolicUnpacker') as mock_unpacker_class:
            mock_unpacker = Mock()
            mock_unpacker.is_available.return_value = True
            mock_unpacker.detect_packer.return_value = "UPX"
            
            mock_result = Mock()
            mock_result.success = False
            mock_result.error_message = "Unpacking failed"
            mock_unpacker.unpack.return_value = mock_result
            
            mock_unpacker_class.return_value = mock_unpacker
            
            # Test loading with auto-unpack enabled
            binary_info = self.binary_loader.load(test_binary, auto_unpack=True)
            
            # Verify unpacker was called
            mock_unpacker.detect_packer.assert_called_once_with(test_binary)
            mock_unpacker.unpack.assert_called_once_with(test_binary)
            
            # Verify the original binary was used (fallback)
            assert binary_info.path == test_binary
    
    def test_binary_loader_unpacking_import_error(self):
        """Test binary loader when unpacking module import fails."""
        # Create a mock packed binary
        packed_data = b"UPX! This is a packed binary"
        test_binary = self.temp_path / "test_packed.exe"
        test_binary.write_bytes(packed_data)
        
        # Mock import error for unpacking module
        with patch('src.core.binary_loader.SymbolicUnpacker', side_effect=ImportError):
            # Test loading with auto-unpack enabled
            binary_info = self.binary_loader.load(test_binary, auto_unpack=True)
            
            # Verify the original binary was used (fallback)
            assert binary_info.path == test_binary
    
    def test_binary_loader_unpacking_exception(self):
        """Test binary loader when unpacking raises an exception."""
        # Create a mock packed binary
        packed_data = b"UPX! This is a packed binary"
        test_binary = self.temp_path / "test_packed.exe"
        test_binary.write_bytes(packed_data)
        
        # Mock the unpacker to raise an exception
        with patch('src.core.binary_loader.SymbolicUnpacker') as mock_unpacker_class:
            mock_unpacker = Mock()
            mock_unpacker.is_available.return_value = True
            mock_unpacker.detect_packer.side_effect = Exception("Unpacker error")
            mock_unpacker_class.return_value = mock_unpacker
            
            # Test loading with auto-unpack enabled
            binary_info = self.binary_loader.load(test_binary, auto_unpack=True)
            
            # Verify the original binary was used (fallback)
            assert binary_info.path == test_binary
    
    def test_pipeline_with_unpacking(self):
        """Test the full pipeline with unpacking enabled."""
        # Create a mock packed binary
        packed_data = b"UPX! This is a packed binary"
        test_binary = self.temp_path / "test_packed.exe"
        test_binary.write_bytes(packed_data)
        
        # Create a mock unpacked binary
        unpacked_binary = self.temp_path / "test_packed_unpacked.exe"
        unpacked_binary.write_bytes(b"Unpacked binary data")
        
        # Mock the unpacker
        with patch('src.core.binary_loader.SymbolicUnpacker') as mock_unpacker_class:
            mock_unpacker = Mock()
            mock_unpacker.is_available.return_value = True
            mock_unpacker.detect_packer.return_value = "UPX"
            
            mock_result = Mock()
            mock_result.success = True
            mock_result.unpacked_path = unpacked_binary
            mock_unpacker.unpack.return_value = mock_result
            
            mock_unpacker_class.return_value = mock_unpacker
            
            # Mock the pipeline components
            with patch('src.core.pipeline.StaticAnalyzer') as mock_static_analyzer_class:
                with patch('src.core.pipeline.DynamicAnalyzer') as mock_dynamic_analyzer_class:
                    with patch('src.core.pipeline.DecompilerFactory') as mock_decompiler_factory_class:
                        # Set up mocks
                        mock_static_analyzer = Mock()
                        mock_analysis_result = Mock()
                        mock_analysis_result.functions = {}
                        mock_static_analyzer.analyze.return_value = mock_analysis_result
                        
                        mock_dynamic_analyzer = Mock()
                        mock_dynamic_analyzer.analyze.return_value = {"traces": []}
                        
                        mock_decompiler = Mock()
                        mock_decompiled_code = Mock()
                        mock_decompiled_code.types = {}
                        mock_decompiler.decompile.return_value = mock_decompiled_code
                        
                        mock_static_analyzer_class.return_value = mock_static_analyzer
                        mock_dynamic_analyzer_class.return_value = mock_dynamic_analyzer
                        mock_decompiler_factory_class.return_value.create.return_value = mock_decompiler
                        
                        # Create pipeline
                        config = Config()
                        pipeline = ReversePipeline(config)
                        
                        # Run pipeline
                        result = pipeline.analyze(test_binary)
                        
                        # Verify unpacking occurred
                        mock_unpacker.detect_packer.assert_called_once_with(test_binary)
                        mock_unpacker.unpack.assert_called_once_with(test_binary)
                        
                        # Verify the unpacked binary was used in analysis
                        mock_static_analyzer.analyze.assert_called_once()
                        # Note: Dynamic analyzer is not called by default (dynamic.enable=false in config)
                        mock_decompiler.decompile.assert_called_once()
                        
                        # Verify result is a valid pipeline result
                        assert "metadata" in result
                        assert "functions" in result
                        assert "data_structures" in result
                        
                        # Unpacking info validation is done through the mock assertions above
                        # The actual unpacking result integration is not yet implemented in pipeline

```

`tests/unit/test_comparison_routes.py`:

```py
import json
import pytest
import tempfile
from unittest.mock import Mock, patch
from datetime import datetime

from flask import Flask

# Add the src directory to the Python path
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.comparison.routes import comparison_bp
from src.comparison.models import AnalysisProject, ComparisonResult, ChangeType


@pytest.fixture
def app():
    """Create a test Flask app with the comparison blueprint."""
    app = Flask(__name__)
    app.register_blueprint(comparison_bp)
    app.config['TESTING'] = True
    return app


@pytest.fixture
def client(app):
    """Create a test client."""
    return app.test_client()


@pytest.fixture
def mock_auth():
    """Mock the login_required decorator to always allow access."""
    with patch('src.comparison.routes.login_required', lambda f: f):
        yield


@pytest.fixture
def mock_store():
    """Mock the comparison store."""
    with patch('src.comparison.routes.store') as mock:
        yield mock


class TestComparisonRoutes:
    """Test cases for comparison API routes."""

    def test_list_projects_success(self, client, mock_auth, mock_store):
        """Test listing projects returns 200 with project list."""
        mock_store.list_projects.return_value = [
            {"id": "proj1", "name": "Test Project 1"},
            {"id": "proj2", "name": "Test Project 2"}
        ]
        
        response = client.get('/projects')
        assert response.status_code == 200
        
        data = json.loads(response.data)
        assert isinstance(data, list)
        assert len(data) == 2
        assert data[0]["id"] == "proj1"

    def test_get_project_success(self, client, mock_auth, mock_store):
        """Test getting a specific project returns 200 with project data."""
        project = AnalysisProject(
            project_id="proj1",
            name="Test Project",
            binary_path="/path/to/binary",
            description="Test description"
        )
        project.timestamp = datetime.now()
        project.version = "1.0"
        project.tags = ["test", "example"]
        
        mock_store.get_project.return_value = project
        
        response = client.get('/project/proj1')
        assert response.status_code == 200
        
        data = json.loads(response.data)
        assert data["id"] == "proj1"
        assert data["name"] == "Test Project"
        assert "timestamp" in data
        assert data["version"] == "1.0"

    def test_get_project_not_found(self, client, mock_auth, mock_store):
        """Test getting non-existent project returns 404."""
        mock_store.get_project.return_value = None
        
        response = client.get('/project/nonexistent')
        assert response.status_code == 404
        
        data = json.loads(response.data)
        assert "error" in data
        assert "not found" in data["error"]

    def test_get_project_functions_success(self, client, mock_auth, mock_store):
        """Test getting project functions returns 200 with function list."""
        project = AnalysisProject(
            project_id="proj1",
            name="Test Project",
            binary_path="/path/to/binary",
            description="Test project with functions"
        )
        # Mock the functions data directly on the project
        project.analysis_data = {
            "functions": [
                {"id": "func1", "name": "main", "size": 100},
                {"id": "func2", "name": "helper", "size": 50}
            ]
        }
        mock_store.get_project.return_value = project
        
        response = client.get('/project/proj1/functions')
        assert response.status_code == 200
        
        data = json.loads(response.data)
        assert data["project_id"] == "proj1"
        assert "total_count" in data
        assert "functions" in data
        assert len(data["functions"]) == 2

    def test_get_project_functions_with_filters(self, client, mock_auth, mock_store):
        """Test getting project functions with filtering and pagination."""
        project = AnalysisProject(
            project_id="proj1",
            name="Test Project",
            binary_path="/path/to/binary",
            description="Test project with multiple functions"
        )
        # Mock the functions data directly on the project
        project.analysis_data = {
            "functions": [
                {"id": "func1", "name": "main_function", "size": 100},
                {"id": "func2", "name": "helper_function", "size": 50},
                {"id": "func3", "name": "other", "size": 25}
            ]
        }
        mock_store.get_project.return_value = project
        
        # Test name filtering
        response = client.get('/project/proj1/functions?name=function')
        assert response.status_code == 200
        
        data = json.loads(response.data)
        assert len(data["functions"]) == 2  # main_function and helper_function
        
        # Test sorting by name
        response = client.get('/project/proj1/functions?sort=name')
        assert response.status_code == 200
        
        data = json.loads(response.data)
        function_names = [f["name"] for f in data["functions"]]
        assert function_names == sorted(function_names)

    def test_create_project_success(self, client, mock_auth, mock_store):
        """Test creating a project returns 200 with project ID."""
        mock_store.save_project.return_value = "new_proj_id"
        
        project_data = {
            "name": "New Project",
            "binary_path": "/path/to/binary",
            "description": "Test project",
            "tags": ["test"]
        }
        
        response = client.post('/project', 
                             data=json.dumps(project_data),
                             content_type='application/json')
        assert response.status_code == 200  # Note: route returns 200, not 201
        
        data = json.loads(response.data)
        assert "id" in data
        assert "message" in data
        assert data["id"] == "new_proj_id"

    def test_create_project_missing_fields(self, client, mock_auth, mock_store):
        """Test creating project with missing required fields returns 400."""
        project_data = {
            "name": "New Project"
            # Missing binary_path
        }
        
        response = client.post('/project',
                             data=json.dumps(project_data),
                             content_type='application/json')
        assert response.status_code == 400
        
        data = json.loads(response.data)
        assert "error" in data
        assert "required" in data["error"]

    def test_delete_project_success(self, client, mock_auth, mock_store):
        """Test deleting a project returns 200 with success message."""
        mock_store.delete_project.return_value = True
        
        response = client.delete('/project/proj1')
        assert response.status_code == 200
        
        data = json.loads(response.data)
        assert "message" in data
        assert "deleted successfully" in data["message"]

    def test_delete_project_not_found(self, client, mock_auth, mock_store):
        """Test deleting non-existent project returns 404."""
        mock_store.delete_project.return_value = False
        
        response = client.delete('/project/nonexistent')
        assert response.status_code == 404
        
        data = json.loads(response.data)
        assert "error" in data
        assert "not found" in data["error"]

    def test_list_comparisons_success(self, client, mock_auth, mock_store):
        """Test listing comparisons returns 200 with comparison list."""
        mock_store.list_comparisons.return_value = [
            {"id": "comp1", "name": "Comparison 1"},
            {"id": "comp2", "name": "Comparison 2"}
        ]
        
        response = client.get('/comparisons')
        assert response.status_code == 200
        
        data = json.loads(response.data)
        assert isinstance(data, list)
        assert len(data) == 2

    def test_get_comparison_success(self, client, mock_auth, mock_store):
        """Test getting a specific comparison returns 200 with comparison data."""
        comparison = ComparisonResult(
            base_version_id="ver1",
            target_version_id="ver2",
            base_version_name="Project 1",
            target_version_name="Project 2"
        )
        comparison.id = "comp1"
        comparison.name = "Test Comparison"
        comparison.timestamp = datetime.now()
        comparison.description = "Test description"
        comparison.tags = ["test"]
        
        mock_store.get_comparison.return_value = comparison
        
        response = client.get('/comparison/comp1')
        assert response.status_code == 200
        
        data = json.loads(response.data)
        assert data["id"] == "comp1"
        assert data["name"] == "Test Comparison"
        assert "timestamp" in data

    def test_get_comparison_not_found(self, client, mock_auth, mock_store):
        """Test getting non-existent comparison returns 404."""
        mock_store.get_comparison.return_value = None
        
        response = client.get('/comparison/nonexistent')
        assert response.status_code == 404
        
        data = json.loads(response.data)
        assert "error" in data
        assert "not found" in data["error"]

    def test_export_analysis_success(self, client, mock_auth, mock_store):
        """Test exporting analysis returns file download."""
        project = AnalysisProject(
            project_id="proj1",
            name="Test Project",
            binary_path="/path/to/binary",
            description="Test description"
        )
        project.timestamp = datetime.now()
        project.version = "1.0"
        project.analysis_data = {"functions": []}
        project.tags = ["test"]
        
        mock_store.get_project.return_value = project
        
        response = client.get('/analysis/export/proj1')
        assert response.status_code == 200
        assert response.headers['Content-Type'] == 'application/json'
        assert 'attachment' in response.headers['Content-Disposition']

    def test_export_analysis_not_found(self, client, mock_auth, mock_store):
        """Test exporting non-existent project returns 404."""
        mock_store.get_project.return_value = None
        
        response = client.get('/analysis/export/nonexistent')
        assert response.status_code == 404
        
        data = json.loads(response.data)
        assert "error" in data
        assert "not found" in data["error"]

    def test_import_analysis_success(self, client, mock_auth, mock_store):
        """Test importing analysis from JSON file returns 200."""
        mock_store.save_project.return_value = "imported_proj_id"
        
        project_data = {
            "name": "Imported Project",
            "binary_path": "/path/to/binary",
            "analysis_data": {"functions": []}
        }
        
        # Create a temporary JSON file
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            json.dump(project_data, f)
            temp_path = f.name
        
        try:
            with open(temp_path, 'rb') as f:
                response = client.post('/analysis/import',
                                     data={'file': (f, 'test.json')},
                                     content_type='multipart/form-data')
                assert response.status_code == 200
                
                data = json.loads(response.data)
                assert "id" in data
                assert "message" in data
                assert data["id"] == "imported_proj_id"
        finally:
            import os
            os.unlink(temp_path)

    def test_import_analysis_no_file(self, client, mock_auth, mock_store):
        """Test importing analysis without file returns 400."""
        response = client.post('/analysis/import')
        assert response.status_code == 400
        
        data = json.loads(response.data)
        assert "error" in data
        assert "No file provided" in data["error"]

    def test_import_analysis_invalid_json(self, client, mock_auth, mock_store):
        """Test importing invalid JSON file returns 400."""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
            f.write("invalid json content")
            temp_path = f.name
        
        try:
            with open(temp_path, 'rb') as f:
                response = client.post('/analysis/import',
                                     data={'file': (f, 'test.json')},
                                     content_type='multipart/form-data')
                assert response.status_code == 400
                
                data = json.loads(response.data)
                assert "error" in data
                assert "Invalid JSON file" in data["error"]
        finally:
            import os
            os.unlink(temp_path)

    def test_import_analysis_wrong_extension(self, client, mock_auth, mock_store):
        """Test importing non-JSON file returns 400."""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write("some text content")
            temp_path = f.name
        
        try:
            with open(temp_path, 'rb') as f:
                response = client.post('/analysis/import',
                                     data={'file': (f, 'test.txt')},
                                     content_type='multipart/form-data')
                assert response.status_code == 400
                
                data = json.loads(response.data)
                assert "error" in data
                assert "JSON file" in data["error"]
        finally:
            import os
            os.unlink(temp_path)

```

`tests/unit/test_core.py`:

```py
import pytest
import os
import sys
from pathlib import Path

# Add the src directory to the Python path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.core.binary_loader import BinaryLoader


class TestBinaryLoader:
    def test_binary_loader_initialization(self):
        loader = BinaryLoader()
        assert loader is not None

    def test_supported_formats(self):
        loader = BinaryLoader()
        assert hasattr(loader, 'supported_formats')
        assert isinstance(loader.supported_formats, list)
        assert len(loader.supported_formats) > 0
        
    @pytest.mark.parametrize("format_name", ["elf", "pe", "macho"])
    def test_format_support(self, format_name):
        loader = BinaryLoader()
        assert format_name in loader.supported_formats


class TestDecompilerFactory:
    @pytest.fixture
    def decompiler_factory(self):
        from src.decompilers.decompiler_factory import DecompilerFactory
        return DecompilerFactory()

    def test_factory_initialization(self, decompiler_factory):
        assert decompiler_factory is not None

    def test_get_decompiler(self, decompiler_factory):
        from src.decompilers.ghidra_decompiler import GhidraDecompiler
        decompiler = decompiler_factory.get_decompiler("ghidra")
        assert decompiler is not None
        assert isinstance(decompiler, GhidraDecompiler)

    def test_invalid_decompiler(self, decompiler_factory):
        with pytest.raises(ValueError):
            decompiler_factory.get_decompiler("invalid_decompiler_name")

```

`tests/unit/test_dynamic_analyzer.py`:

```py
import sys
import types
import pytest

from pathlib import Path

# Ensure src is importable in tests
import os
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.analysis.dynamic_analyzer import DynamicAnalyzer
from src.core.config import Config
from src.core.binary_loader import BinaryInfo, BinaryFormat, Architecture, CompilerType


class DummyEnv:
    def __init__(self, traces=None, mem_events=None, syscalls=None, coverage=None):
        self._traces = traces
        self._mem = mem_events
        self._sys = syscalls
        self._cov = coverage

    def setup(self, binary_info):
        return None

    def cleanup(self):
        return None

    def trace_functions(self, binary_info):
        return self._traces

    def get_memory_events(self):
        return self._mem

    def get_syscalls(self):
        return self._sys

    @property
    def execution_paths(self):
        return self._cov


def make_binary_info(tmp_path: Path) -> BinaryInfo:
    p = tmp_path / "dummy.bin"
    p.write_bytes(b"\x7fELF")
    return BinaryInfo(
        path=p,
        format=BinaryFormat.ELF,
        architecture=Architecture.X86_64,
        bit_width=64,
        endianness="little",
        entry_point=0,
        sections={},
        symbols={},
        compiler=CompilerType.GCC,
        stripped=True,
        is_library=False,
        imports={},
        exports=[]
    )


def test_trace_normalization_without_frida(monkeypatch, tmp_path):
    cfg = Config(None)
    cfg._config = {"analysis": {"dynamic": {"enable": True, "use_frida": False}}}
    da = DynamicAnalyzer(cfg)

    env = DummyEnv(traces=[{"name": "foo", "address": 0x401000, "count": 3}, {"name": "bar", "count": 1}])
    monkeypatch.setattr(da, "_create_execution_environment", lambda: env)

    res = da.analyze(make_binary_info(tmp_path))
    assert res["enabled"] is True
    assert "functions" in res
    calls = res["functions"].get("calls", [])
    assert any(c["name"] == "foo" and c["count"] == 3 for c in calls)
    assert any(c["name"] == "bar" for c in calls)


def test_memory_normalization_without_frida(monkeypatch, tmp_path):
    cfg = Config(None)
    cfg._config = {"analysis": {"dynamic": {"enable": True, "use_frida": False}}}
    da = DynamicAnalyzer(cfg)

    events = [
        {"type": "read", "address": 0x1000, "size": 4},
        {"type": "write", "address": 0x1000, "size": 8},
        {"type": "alloc", "address": 0x2000, "size": 16},
        {"type": "free", "address": 0x2000}
    ]
    env = DummyEnv(mem_events=events)
    monkeypatch.setattr(da, "_create_execution_environment", lambda: env)

    res = da.analyze(make_binary_info(tmp_path))
    summary = res["memory_access"]["summary"]
    assert summary["reads"] == 1
    assert summary["writes"] == 1
    assert summary["allocations"] == 1
    assert summary["frees"] == 1
    assert res["memory_access"]["by_address"]["0x1000"]["reads"] == 1


def test_syscalls_normalization_without_frida(monkeypatch, tmp_path):
    cfg = Config(None)
    cfg._config = {"analysis": {"dynamic": {"enable": True, "use_frida": False}}}
    da = DynamicAnalyzer(cfg)

    syscalls = [
        {"name": "open", "args": ["/etc/passwd"], "ret": 3},
        "close"
    ]
    env = DummyEnv(syscalls=syscalls)
    monkeypatch.setattr(da, "_create_execution_environment", lambda: env)

    res = da.analyze(make_binary_info(tmp_path))
    sc = res["syscalls"]
    assert any(s["name"] == "open" and s["ret"] == 3 for s in sc)
    assert any(s["name"] == "close" for s in sc)


def test_paths_normalization_without_frida(monkeypatch, tmp_path):
    cfg = Config(None)
    cfg._config = {"analysis": {"dynamic": {"enable": True, "use_frida": False}}}
    da = DynamicAnalyzer(cfg)

    coverage = {"blocks": [{"address": "0x1000", "count": 5}]}
    env = DummyEnv(coverage=coverage)
    monkeypatch.setattr(da, "_create_execution_environment", lambda: env)

    res = da.analyze(make_binary_info(tmp_path))
    assert res["execution_paths"].get("blocks")

```

`tests/unit/test_llm.py`:

```py
import pytest
import os
import sys
from pathlib import Path
from unittest.mock import Mock, patch

# Add the src directory to the Python path
sys.path.append(str(Path(__file__).parent.parent.parent))

from src.llm.function_summarizer import FunctionSummarizer


class TestFunctionSummarizer:
    @pytest.fixture
    def config(self):
        return {
            "provider": "openai",
            "model": "gpt-4-turbo",
            "max_tokens": 8192,
            "temperature": 0.2,
            "cache_dir": "./cache/llm"
        }
        
    @pytest.fixture
    def summarizer(self, config):
        return FunctionSummarizer(config)
        
    def test_summarizer_initialization(self, summarizer):
        assert summarizer is not None
        assert summarizer.provider == "openai"
        assert summarizer.model == "gpt-4-turbo"
        
    def test_cache_management(self, summarizer):
        # Create mock function for testing
        test_function = """
        int calculate_factorial(int n) {
            if (n <= 1) {
                return 1;
            }
            return n * calculate_factorial(n - 1);
        }
        """
        
        # First call should not use cache
        with patch.object(summarizer, '_call_llm_api') as mock_api:
            mock_api.return_value = "Calculates the factorial of a number recursively."
            result1 = summarizer.summarize_function(test_function)
            assert mock_api.called
            
        # Second call with same function should use cache
        with patch.object(summarizer, '_call_llm_api') as mock_api:
            mock_api.return_value = "Different summary"  # Should not be used
            result2 = summarizer.summarize_function(test_function)
            assert not mock_api.called
            assert result1 == result2
            
    @patch('src.llm.function_summarizer.FunctionSummarizer._call_openai_api')
    def test_provider_selection(self, mock_openai_api, config):
        mock_openai_api.return_value = "OpenAI summary"
        
        # Test OpenAI
        summarizer = FunctionSummarizer(config)
        result = summarizer.summarize_function("void test() {}")
        assert result == "OpenAI summary"
        assert mock_openai_api.called
        
        # Test Anthropic
        with patch('src.llm.function_summarizer.FunctionSummarizer._call_anthropic_api') as mock_anthropic_api:
            mock_anthropic_api.return_value = "Anthropic summary"
            config["provider"] = "anthropic"
            summarizer = FunctionSummarizer(config)
            result = summarizer.summarize_function("void test() {}")
            assert result == "Anthropic summary"
            assert mock_anthropic_api.called

```

`tests/unit/test_obfuscation_optimizer.py`:

```py
import tempfile
from pathlib import Path
from unittest.mock import patch, Mock

from src.optimization.optimizer import ObfuscationOptimizer, OptimizationReport


def test_optimizer_availability():
    opt = ObfuscationOptimizer()
    assert isinstance(opt.is_available(), bool)


@patch('src.optimization.optimizer.ANGR_AVAILABLE', True)
@patch('src.optimization.optimizer.angr')
def test_optimizer_runs_iterations(mock_angr):
    # Mock angr project, cfg, state
    mock_project = Mock()
    mock_cfg = Mock()
    mock_cfg.graph.nodes.return_value = []
    mock_project.analyses.CFGFast.return_value = mock_cfg
    mock_state = Mock()
    mock_project.factory.entry_state.return_value = mock_state
    mock_angr.Project.return_value = mock_project

    opt = ObfuscationOptimizer(max_iterations=3)
    report = opt.optimize(Path('dummy'))

    assert isinstance(report, OptimizationReport)
    assert report.iterations >= 1
    assert isinstance(report.changes_applied, int)
    assert isinstance(report.passes_run, list)


@patch('src.optimization.optimizer.ANGR_AVAILABLE', True)
@patch('src.optimization.optimizer.angr')
def test_optimizer_converges(mock_angr):
    # Configure passes to produce zero changes (graph has no nodes)
    mock_project = Mock()
    mock_cfg = Mock()
    mock_cfg.graph.nodes.return_value = []
    mock_project.analyses.CFGFast.return_value = mock_cfg
    mock_state = Mock()
    mock_project.factory.entry_state.return_value = mock_state
    mock_angr.Project.return_value = mock_project

    opt = ObfuscationOptimizer(max_iterations=5)
    report = opt.optimize(Path('dummy'))
    assert report.iterations == 1



```

`tests/unit/test_symbolic_unpacker.py`:

```py
"""
Unit tests for the symbolic unpacker module.
"""

import pytest
import tempfile
import os
from pathlib import Path
from unittest.mock import Mock, patch, MagicMock

from src.unpacking.symbolic_unpacker import SymbolicUnpacker, UnpackingResult


class TestSymbolicUnpacker:
    """Test cases for SymbolicUnpacker."""
    
    def setup_method(self):
        """Set up test fixtures."""
        self.unpacker = SymbolicUnpacker()
        self.temp_dir = tempfile.mkdtemp()
        self.temp_path = Path(self.temp_dir)
    
    def teardown_method(self):
        """Clean up test fixtures."""
        import shutil
        shutil.rmtree(self.temp_dir, ignore_errors=True)
    
    def test_init_default_config(self):
        """Test unpacker initialization with default config."""
        unpacker = SymbolicUnpacker()
        assert unpacker.max_execution_steps == 10000
        assert unpacker.timeout_seconds == 300
        assert unpacker.memory_dump_threshold == 1024 * 1024
    
    def test_init_custom_config(self):
        """Test unpacker initialization with custom config."""
        config = {
            "unpacking.max_execution_steps": 5000,
            "unpacking.timeout_seconds": 150,
            "unpacking.memory_dump_threshold": 512 * 1024
        }
        unpacker = SymbolicUnpacker(config)
        assert unpacker.max_execution_steps == 5000
        assert unpacker.timeout_seconds == 150
        assert unpacker.memory_dump_threshold == 512 * 1024
    
    def test_detect_packer_upx(self):
        """Test UPX packer detection."""
        # Create a mock UPX-packed binary
        upx_data = b"UPX! This is a test binary"
        test_binary = self.temp_path / "test_upx.exe"
        test_binary.write_bytes(upx_data)
        
        packer = self.unpacker.detect_packer(test_binary)
        assert packer == "UPX"
    
    def test_detect_packer_pecompact(self):
        """Test PECompact packer detection."""
        pecompact_data = b"PECompact packed binary data"
        test_binary = self.temp_path / "test_pecompact.exe"
        test_binary.write_bytes(pecompact_data)
        
        packer = self.unpacker.detect_packer(test_binary)
        assert packer == "PECompact"
    
    def test_detect_packer_aspack(self):
        """Test ASPack packer detection."""
        aspack_data = b"aPLib compressed data with ASPack"
        test_binary = self.temp_path / "test_aspack.exe"
        test_binary.write_bytes(aspack_data)
        
        packer = self.unpacker.detect_packer(test_binary)
        assert packer == "ASPack"
    
    def test_detect_packer_unknown(self):
        """Test detection of unknown packed binary."""
        # High entropy data that looks packed
        high_entropy_data = bytes(range(256)) * 10  # High entropy
        test_binary = self.temp_path / "test_unknown.exe"
        test_binary.write_bytes(high_entropy_data)
        
        packer = self.unpacker.detect_packer(test_binary)
        assert packer == "Unknown"
    
    def test_detect_packer_not_packed(self):
        """Test detection of non-packed binary."""
        # Low entropy data that looks like normal code
        normal_data = b"Hello, World!" * 100
        test_binary = self.temp_path / "test_normal.exe"
        test_binary.write_bytes(normal_data)
        
        packer = self.unpacker.detect_packer(test_binary)
        assert packer is None
    
    def test_detect_packer_file_not_found(self):
        """Test packer detection with non-existent file."""
        non_existent = self.temp_path / "does_not_exist.exe"
        packer = self.unpacker.detect_packer(non_existent)
        assert packer is None
    
    def test_calculate_entropy(self):
        """Test entropy calculation."""
        # Test with uniform distribution (high entropy)
        uniform_data = bytes(range(256))
        entropy = self.unpacker._calculate_entropy(uniform_data)
        assert entropy > 7.0
        
        # Test with repeated data (low entropy)
        repeated_data = b"A" * 1000
        entropy = self.unpacker._calculate_entropy(repeated_data)
        assert entropy < 1.0
        
        # Test with empty data
        entropy = self.unpacker._calculate_entropy(b"")
        assert entropy == 0.0
    
    def test_is_likely_packed_high_entropy(self):
        """Test packed binary detection with high entropy."""
        high_entropy_data = bytes(range(256)) * 10
        assert self.unpacker._is_likely_packed(high_entropy_data)
    
    def test_is_likely_packed_low_entropy(self):
        """Test packed binary detection with low entropy."""
        low_entropy_data = b"A" * 1000
        assert not self.unpacker._is_likely_packed(low_entropy_data)
    
    def test_is_likely_packed_suspicious_patterns(self):
        """Test packed binary detection with suspicious patterns."""
        suspicious_data = b"Normal data with .upx section" + b"A" * 1000  # Make it large enough
        assert self.unpacker._is_likely_packed(suspicious_data)
    
    def test_is_likely_packed_too_small(self):
        """Test packed binary detection with very small file."""
        small_data = b"Hi"
        assert not self.unpacker._is_likely_packed(small_data)
    
    @patch('src.unpacking.symbolic_unpacker.ANGR_AVAILABLE', False)
    def test_unpack_angr_unavailable(self):
        """Test unpacking when Angr is not available."""
        # Create test binary
        test_binary = self.temp_path / "test_packed.exe"
        test_binary.write_bytes(b"UPX! packed data")
        
        result = self.unpacker.unpack(test_binary)
        
        assert not result.success
        assert "Angr not available" in result.error_message
    
    def test_unpack_no_packer_detected(self):
        """Test unpacking when no packer is detected."""
        test_binary = self.temp_path / "test_normal.exe"
        test_binary.write_bytes(b"Normal binary data")
        
        result = self.unpacker.unpack(test_binary)
        
        assert not result.success
        assert result.error_message == "No packer detected"
    
    def test_unpack_angr_not_available(self):
        """Test unpacking when Angr is not available."""
        with patch('src.unpacking.symbolic_unpacker.ANGR_AVAILABLE', False):
            test_binary = self.temp_path / "test_packed.exe"
            test_binary.write_bytes(b"UPX! packed data")
            
            result = self.unpacker.unpack(test_binary)
            
            assert not result.success
            assert "Angr not available" in result.error_message
    
    def test_unpack_extraction_failed(self):
        """Test unpacking when extraction fails."""
        with patch('src.unpacking.symbolic_unpacker.angr'):
            with patch('src.unpacking.symbolic_unpacker.ANGR_AVAILABLE', True):
                with patch.object(self.unpacker, '_extract_unpacked_binary', return_value=None):
                    test_binary = self.temp_path / "test_packed.exe"
                    test_binary.write_bytes(b"UPX! packed data")
                    
                    result = self.unpacker.unpack(test_binary)
                    
                    assert not result.success
                    assert "Failed to extract unpacked binary" in result.error_message
    
    def test_is_available_with_angr(self):
        """Test availability check when Angr is available."""
        with patch('src.unpacking.symbolic_unpacker.ANGR_AVAILABLE', True):
            assert self.unpacker.is_available()
    
    def test_is_available_without_angr(self):
        """Test availability check when Angr is not available."""
        with patch('src.unpacking.symbolic_unpacker.ANGR_AVAILABLE', False):
            assert not self.unpacker.is_available()
    
    def test_get_unpacker_info(self):
        """Test unpacker info retrieval."""
        info = self.unpacker.get_unpacker_info()
        
        assert info["name"] == "SymbolicUnpacker"
        assert "available" in info
        assert info["max_execution_steps"] == 10000
        assert info["timeout_seconds"] == 300
        assert "supported_packers" in info
        assert "UPX" in info["supported_packers"]
    
    def test_unpacking_result_initialization(self):
        """Test UnpackingResult initialization."""
        result = UnpackingResult(success=True)
        
        assert result.success
        assert result.unpacked_path is None
        assert result.original_path is None
        assert result.packer_detected is None
        assert result.unpacking_method is None
        assert result.execution_steps == 0
        assert result.memory_dumps == []
        assert result.error_message is None
        assert result.metadata == {}
    
    def test_unpacking_result_with_data(self):
        """Test UnpackingResult with data."""
        memory_dumps = [(0x1000, 1024, b"data")]
        metadata = {"test": "value"}
        
        result = UnpackingResult(
            success=True,
            unpacked_path=Path("/tmp/unpacked.exe"),
            original_path=Path("/tmp/packed.exe"),
            packer_detected="UPX",
            unpacking_method="symbolic_execution",
            execution_steps=1000,
            memory_dumps=memory_dumps,
            metadata=metadata
        )
        
        assert result.success
        assert result.unpacked_path == Path("/tmp/unpacked.exe")
        assert result.original_path == Path("/tmp/packed.exe")
        assert result.packer_detected == "UPX"
        assert result.unpacking_method == "symbolic_execution"
        assert result.execution_steps == 1000
        assert result.memory_dumps == memory_dumps
        assert result.metadata == metadata

```