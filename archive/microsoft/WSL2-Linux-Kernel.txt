# WSL2 Linux Kernel Overview

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [.gitignore](.gitignore)
- [.mailmap](.mailmap)
- [Documentation/core-api/symbol-namespaces.rst](Documentation/core-api/symbol-namespaces.rst)
- [Documentation/devicetree/bindings/crypto/atmel,at91sam9g46-aes.yaml](Documentation/devicetree/bindings/crypto/atmel,at91sam9g46-aes.yaml)
- [Documentation/devicetree/bindings/crypto/atmel,at91sam9g46-sha.yaml](Documentation/devicetree/bindings/crypto/atmel,at91sam9g46-sha.yaml)
- [Documentation/devicetree/bindings/crypto/atmel,at91sam9g46-tdes.yaml](Documentation/devicetree/bindings/crypto/atmel,at91sam9g46-tdes.yaml)
- [Documentation/devicetree/bindings/sound/google,cros-ec-codec.yaml](Documentation/devicetree/bindings/sound/google,cros-ec-codec.yaml)
- [Documentation/devicetree/bindings/sound/realtek,rt1015p.yaml](Documentation/devicetree/bindings/sound/realtek,rt1015p.yaml)
- [Documentation/devicetree/bindings/spi/atmel,at91rm9200-spi.yaml](Documentation/devicetree/bindings/spi/atmel,at91rm9200-spi.yaml)
- [Documentation/devicetree/bindings/spi/atmel,quadspi.yaml](Documentation/devicetree/bindings/spi/atmel,quadspi.yaml)
- [Documentation/livepatch/livepatch.rst](Documentation/livepatch/livepatch.rst)
- [Documentation/livepatch/module-elf-format.rst](Documentation/livepatch/module-elf-format.rst)
- [Documentation/process/maintainer-handbooks.rst](Documentation/process/maintainer-handbooks.rst)
- [Documentation/process/maintainer-kvm-x86.rst](Documentation/process/maintainer-kvm-x86.rst)
- [Documentation/process/maintainer-soc-clean-dts.rst](Documentation/process/maintainer-soc-clean-dts.rst)
- [Documentation/process/maintainer-soc.rst](Documentation/process/maintainer-soc.rst)
- [Documentation/translations/zh_CN/core-api/kernel-api.rst](Documentation/translations/zh_CN/core-api/kernel-api.rst)
- [Documentation/translations/zh_CN/core-api/symbol-namespaces.rst](Documentation/translations/zh_CN/core-api/symbol-namespaces.rst)
- [MAINTAINERS](MAINTAINERS)
- [MSFT-Merge/log](MSFT-Merge/log)
- [Makefile](Makefile)
- [arch/um/include/asm/Kbuild](arch/um/include/asm/Kbuild)
- [arch/x86/lib/csum-partial_64.c](arch/x86/lib/csum-partial_64.c)
- [arch/x86/um/Kconfig](arch/x86/um/Kconfig)
- [drivers/crypto/atmel-ecc.c](drivers/crypto/atmel-ecc.c)
- [drivers/crypto/atmel-i2c.c](drivers/crypto/atmel-i2c.c)
- [drivers/crypto/atmel-i2c.h](drivers/crypto/atmel-i2c.h)
- [drivers/crypto/atmel-sha204a.c](drivers/crypto/atmel-sha204a.c)
- [drivers/hv/Kconfig](drivers/hv/Kconfig)
- [drivers/hv/Makefile](drivers/hv/Makefile)
- [drivers/vhost/Kconfig](drivers/vhost/Kconfig)
- [include/linux/export.h](include/linux/export.h)
- [include/linux/kallsyms.h](include/linux/kallsyms.h)
- [include/linux/module.h](include/linux/module.h)
- [include/linux/module_symbol.h](include/linux/module_symbol.h)
- [include/linux/rcuref.h](include/linux/rcuref.h)
- [include/linux/types.h](include/linux/types.h)
- [kernel/Makefile](kernel/Makefile)
- [kernel/kallsyms.c](kernel/kallsyms.c)
- [kernel/kallsyms_internal.h](kernel/kallsyms_internal.h)
- [kernel/kallsyms_selftest.c](kernel/kallsyms_selftest.c)
- [kernel/kallsyms_selftest.h](kernel/kallsyms_selftest.h)
- [kernel/ksyms_common.c](kernel/ksyms_common.c)
- [kernel/module/kallsyms.c](kernel/module/kallsyms.c)
- [kernel/module/livepatch.c](kernel/module/livepatch.c)
- [kernel/params.c](kernel/params.c)
- [lib/Kconfig.debug](lib/Kconfig.debug)
- [lib/Kconfig.kcsan](lib/Kconfig.kcsan)
- [lib/Makefile](lib/Makefile)
- [lib/checksum_kunit.c](lib/checksum_kunit.c)
- [lib/fortify_kunit.c](lib/fortify_kunit.c)
- [lib/kunit_iov_iter.c](lib/kunit_iov_iter.c)
- [lib/rcuref.c](lib/rcuref.c)
- [lib/strcat_kunit.c](lib/strcat_kunit.c)
- [mm/kmemleak.c](mm/kmemleak.c)
- [rust/macros/module.rs](rust/macros/module.rs)
- [scripts/Kbuild.include](scripts/Kbuild.include)
- [scripts/Makefile.debug](scripts/Makefile.debug)
- [scripts/Makefile.modfinal](scripts/Makefile.modfinal)
- [scripts/Makefile.modinst](scripts/Makefile.modinst)
- [scripts/Makefile.modpost](scripts/Makefile.modpost)
- [scripts/Makefile.package](scripts/Makefile.package)
- [scripts/basic/fixdep.c](scripts/basic/fixdep.c)
- [scripts/check-git](scripts/check-git)
- [scripts/clang-tools/gen_compile_commands.py](scripts/clang-tools/gen_compile_commands.py)
- [scripts/depmod.sh](scripts/depmod.sh)
- [scripts/kallsyms.c](scripts/kallsyms.c)
- [scripts/mksysmap](scripts/mksysmap)
- [scripts/mod/file2alias.c](scripts/mod/file2alias.c)
- [scripts/mod/modpost.c](scripts/mod/modpost.c)
- [scripts/mod/modpost.h](scripts/mod/modpost.h)
- [scripts/modules-check.sh](scripts/modules-check.sh)
- [scripts/package/builddeb](scripts/package/builddeb)
- [scripts/package/buildtar](scripts/package/buildtar)
- [scripts/package/deb-build-option](scripts/package/deb-build-option)
- [scripts/package/debian/rules](scripts/package/debian/rules)
- [scripts/package/gen-diff-patch](scripts/package/gen-diff-patch)
- [scripts/package/install-extmod-build](scripts/package/install-extmod-build)
- [scripts/package/kernel.spec](scripts/package/kernel.spec)
- [scripts/package/mkdebian](scripts/package/mkdebian)
- [scripts/package/mkspec](scripts/package/mkspec)
- [scripts/remove-stale-files](scripts/remove-stale-files)
- [scripts/setlocalversion](scripts/setlocalversion)
- [tools/testing/selftests/bpf/progs/bpf_iter_ksym.c](tools/testing/selftests/bpf/progs/bpf_iter_ksym.c)

</details>



The WSL2 Linux Kernel is Microsoft's Linux kernel distribution that powers the Windows Subsystem for Linux 2 (WSL2) environment. This repository contains the standard Linux kernel with WSL2-specific modifications and development infrastructure optimized for integration with Windows through Hyper-V virtualization.

The kernel maintains full Linux compatibility while incorporating Windows-specific components such as `dxgkrnl` for GPU virtualization, Hyper-V integration drivers, and specialized build infrastructure. The repository includes comprehensive development tools, maintainer systems, and packaging scripts that support both standard Linux kernel development and WSL2-specific customizations.

## Architecture Overview

The WSL2 Linux Kernel architecture encompasses core Linux subsystems, development infrastructure, and Windows integration components. The kernel supports multiple virtualization technologies including KVM implementations for x86 and ARM64, alongside Microsoft Hyper-V integration.

### Overall System Architecture

```mermaid
graph TB
    subgraph "Development Infrastructure"
        MAINTAINERS_SYS["MAINTAINERS System<br/>790.19 - Contributor Management"]
        BUILD_SYS["Build System<br/>Makefile, scripts/"]
        STATIC_ANALYSIS["Static Analysis<br/>scripts/mod/modpost.c"]
    end
    
    subgraph "Core Kernel Services"
        SCHED_CFS["CFS Scheduler<br/>kernel/sched/fair.c"]
        WORKQUEUE_SYS["Workqueue Subsystem<br/>kernel/workqueue.c"]
        IOURING_FRAMEWORK["io_uring Framework<br/>io_uring/, fs/io_uring.c"]
    end
    
    subgraph "Virtualization Infrastructure"
        KVM_X86["KVM x86<br/>arch/x86/kvm/x86.c"]
        KVM_ARM64["KVM ARM64<br/>arch/arm64/kvm/"]
        AMD_SVM["AMD SVM<br/>arch/x86/kvm/svm/svm.c"]
        HYPERV_INTEGRATION["Hyper-V Integration<br/>drivers/hv/"]
    end
    
    subgraph "Windows Integration"
        DXGKRNL_DRV["DXGKRNL Driver<br/>DirectX Graphics Kernel"]
        HDA_AUDIO["HDA Audio<br/>sound/pci/hda/"]
        VMBUS_CHANNEL["VM Bus Channel<br/>drivers/hv/vmbus_drv.c"]
    end
    
    subgraph "Storage & I/O"
        EXT4_FS["EXT4 Filesystem<br/>fs/ext4/"]
        BTRFS_FS["Btrfs Filesystem<br/>fs/btrfs/"]
        NVME_DRV["NVMe Driver<br/>drivers/nvme/"]
        IOMMU_SUBSYS["IOMMU Subsystem<br/>drivers/iommu/"]
    end
    
    BUILD_SYS --> STATIC_ANALYSIS
    MAINTAINERS_SYS --> BUILD_SYS
    
    SCHED_CFS --> WORKQUEUE_SYS
    WORKQUEUE_SYS --> IOURING_FRAMEWORK
    
    KVM_X86 --> AMD_SVM
    KVM_ARM64 --> HYPERV_INTEGRATION
    
    HYPERV_INTEGRATION --> VMBUS_CHANNEL
    VMBUS_CHANNEL --> DXGKRNL_DRV
    VMBUS_CHANNEL --> HDA_AUDIO
    
    IOURING_FRAMEWORK --> EXT4_FS
    IOURING_FRAMEWORK --> BTRFS_FS
    IOMMU_SUBSYS --> NVME_DRV
```

Sources:
- [MAINTAINERS:1-100]()
- [Makefile:1-100]()
- [scripts/mod/modpost.c:1-50]()
- [arch/x86/kvm/x86.c]()
- [arch/x86/kvm/svm/svm.c]()
- [drivers/hv/Kconfig:1-25]()

## Virtualization Infrastructure

The kernel includes comprehensive virtualization support through KVM implementations for multiple architectures, alongside Hyper-V integration for Windows environments. The virtualization layer supports nested virtualization scenarios and provides hardware-assisted virtualization capabilities.

### Virtualization Ecosystem Detail

```mermaid
graph TD
    subgraph "Host Environment"
        HOST_KERNEL["Linux Host Kernel"]
        HOST_DRIVERS["Host Device Drivers"]
    end
    
    subgraph "KVM Hypervisor Layer"
        KVM_CORE["kvm_main.c<br/>KVM Core"]
        KVM_MMU["kvm_mmu.c<br/>Memory Virtualization"]
        
        subgraph "Hardware Virtualization"
            INTEL_VMX["vmx.c<br/>Intel VMX"]
            AMD_SVM["svm.c<br/>AMD SVM<br/>arch/x86/kvm/svm/"]
            ARM_KVM["kvm_main.c<br/>ARM64 KVM"]
        end
    end
    
    subgraph "Microsoft Integration"
        HYPERV_VMBUS["vmbus_drv.c<br/>VM Bus Channel"]
        DXGKRNL_GPU["DXGKRNL<br/>DirectX Graphics Kernel"]
        HYPERV_DRIVERS["hv_utils.c<br/>Hyper-V Utilities"]
    end
    
    subgraph "I/O Virtualization"
        IOMMU_INTEL["intel-iommu.c<br/>Intel VT-d"]
        IOMMU_AMD["amd_iommu.c<br/>AMD-Vi"]
        VFIO_FRAMEWORK["vfio.c<br/>Device Passthrough"]
    end
    
    subgraph "Guest Environments"
        GUEST_LINUX["Linux Guest VMs"]
        GUEST_WINDOWS["Windows Guest VMs"]
        WSL2_INSTANCE["WSL2 Instance"]
    end
    
    HOST_KERNEL --> KVM_CORE
    KVM_CORE --> KVM_MMU
    KVM_CORE --> INTEL_VMX
    KVM_CORE --> AMD_SVM
    KVM_CORE --> ARM_KVM
    
    HOST_KERNEL --> HYPERV_VMBUS
    HYPERV_VMBUS --> DXGKRNL_GPU
    HYPERV_VMBUS --> HYPERV_DRIVERS
    
    HOST_DRIVERS --> IOMMU_INTEL
    HOST_DRIVERS --> IOMMU_AMD
    IOMMU_INTEL --> VFIO_FRAMEWORK
    IOMMU_AMD --> VFIO_FRAMEWORK
    
    INTEL_VMX --> GUEST_LINUX
    AMD_SVM --> GUEST_WINDOWS
    ARM_KVM --> WSL2_INSTANCE
    DXGKRNL_GPU --> WSL2_INSTANCE
    VFIO_FRAMEWORK --> GUEST_LINUX
```

### KVM Implementation Components

The kernel provides KVM implementations for multiple architectures with hardware virtualization support:

| Component | Location | Description |
|-----------|----------|-------------|
| KVM x86 Core | `arch/x86/kvm/x86.c` | Core x86 virtualization logic |
| Intel VMX | `arch/x86/kvm/vmx/vmx.c` | Intel VMX hardware support |
| AMD SVM | `arch/x86/kvm/svm/svm.c` | AMD SVM hardware support |
| KVM ARM64 | `arch/arm64/kvm/` | ARM64 virtualization implementation |
| Memory Management | `arch/x86/kvm/mmu/` | Virtual memory management |
| SEV Support | `arch/x86/kvm/svm/sev.c` | AMD Secure Encrypted Virtualization |

### Hyper-V Integration Components

Windows integration is provided through Hyper-V guest drivers and communication channels:

| Component | Location | Function |
|-----------|----------|----------|
| VM Bus Driver | `drivers/hv/vmbus_drv.c` | Primary communication channel |
| Hyper-V Utilities | `drivers/hv/hv_utils.c` | Guest services and utilities |
| Balloon Driver | `drivers/hv/hv_balloon.c` | Dynamic memory management |
| Storage VSC | `drivers/scsi/storvsc_drv.c` | Virtual storage controller |
| Network VSC | `drivers/net/hyperv/netvsc_drv.c` | Virtual network adapter |

Sources:
- [arch/x86/kvm/x86.c]()
- [arch/x86/kvm/vmx/vmx.c]()
- [arch/x86/kvm/svm/svm.c]()
- [arch/arm64/kvm/]()
- [drivers/hv/Kconfig:1-25]()
- [drivers/hv/Makefile:1-10]()

## Development Infrastructure

The WSL2 Linux Kernel repository includes comprehensive development infrastructure supporting both standard Linux kernel development and WSL2-specific customizations.

### Build System Architecture

```mermaid
graph TB
    subgraph "Source Management"
        MAINTAINERS_DB["MAINTAINERS<br/>Contributor Database<br/>790.19 importance"]
        MAILMAP[".mailmap<br/>Email Mapping"]
        GITIGNORE[".gitignore<br/>Version Control Rules"]
    end
    
    subgraph "Build Infrastructure"
        MAIN_MAKEFILE["Makefile<br/>213.58 importance<br/>Build Orchestration"]
        KBUILD_INCLUDE["scripts/Kbuild.include<br/>Build Definitions"]
        
        subgraph "Module Processing"
            MODPOST["scripts/mod/modpost.c<br/>51.11 importance<br/>Symbol Processing"]
            FILE2ALIAS["scripts/mod/file2alias.c<br/>Device ID Tables"]
            MODPOST_H["scripts/mod/modpost.h<br/>Shared Definitions"]
        end
        
        subgraph "Package Generation"
            MKSPEC["scripts/package/mkspec<br/>RPM Spec Generation"]
            MKDEBIAN["scripts/package/mkdebian<br/>Debian Package Generation"]
            BUILDDEB["scripts/package/builddeb<br/>DEB Build Process"]
            PKG_MAKEFILE["scripts/Makefile.package<br/>Package Targets"]
        end
    end
    
    subgraph "Code Analysis"
        KALLSYMS["scripts/kallsyms.c<br/>Symbol Table Generation"]
        MKSYSMAP["scripts/mksysmap<br/>System Map Creation"]
        DEBUG_CONFIG["lib/Kconfig.debug<br/>Debug Configuration"]
    end
    
    subgraph "Installation"
        MODINST["scripts/Makefile.modinst<br/>Module Installation"]
        MODFINAL["scripts/Makefile.modfinal<br/>Module Final Linking"]
        MODPOST_MK["scripts/Makefile.modpost<br/>Module Post-Processing"]
    end
    
    MAINTAINERS_DB --> MAIN_MAKEFILE
    MAIN_MAKEFILE --> KBUILD_INCLUDE
    MAIN_MAKEFILE --> MODPOST
    MAIN_MAKEFILE --> PKG_MAKEFILE
    
    MODPOST --> FILE2ALIAS
    MODPOST --> MODPOST_H
    
    PKG_MAKEFILE --> MKSPEC
    PKG_MAKEFILE --> MKDEBIAN
    PKG_MAKEFILE --> BUILDDEB
    
    MAIN_MAKEFILE --> KALLSYMS
    KALLSYMS --> MKSYSMAP
    
    MAIN_MAKEFILE --> MODINST
    MODINST --> MODFINAL
    MODFINAL --> MODPOST_MK
```

### Build System Components

The build system provides comprehensive infrastructure for compiling, packaging, and distributing the kernel:

| Component | Location | Importance | Function |
|-----------|----------|------------|----------|
| Main Makefile | `Makefile` | 213.58 | Primary build orchestration |
| Module Post-processor | `scripts/mod/modpost.c` | 51.11 | Symbol and dependency processing |
| Package Tools | `scripts/package/` | 29.81+ | RPM/DEB package generation |
| Kbuild Includes | `scripts/Kbuild.include` | 4.21 | Shared build definitions |
| Debug Configuration | `lib/Kconfig.debug` | 59.12 | Kernel debugging options |

### Microsoft-Specific Features

The repository includes WSL2-specific feature branches and modifications:

| Feature Branch | SHA | Purpose |
|----------------|-----|---------|
| `feature/dxgkrnl/6.6` | d4ec8a2 | DirectX Graphics Kernel integration |
| `feature/arm64-hyperv-hypercall-interface/6.6` | 8db985f | ARM64 Hyper-V hypercall support |
| `feature/hvlite_virtio_pmem/6.6` | ed18213 | Hyper-V Lite VirtIO persistent memory |
| `product/wsl/security/6.6` | c9cb753 | WSL security enhancements |

Sources:
- [MAINTAINERS:1-100]()
- [Makefile:1-100]()
- [scripts/mod/modpost.c:1-50]()
- [scripts/package/mkspec:1-25]()
- [scripts/package/mkdebian:1-50]()
- [MSFT-Merge/log:1-12]()
- [lib/Kconfig.debug:1-50]()

## I/O and Storage Subsystems

The kernel provides high-performance I/O subsystems including asynchronous I/O frameworks, advanced storage drivers, and comprehensive filesystem support optimized for both traditional and virtualized environments.

### I/O and Storage System Architecture

```mermaid
graph TB
    subgraph "User Applications"
        USER_APPS["Applications"]
        SYSTEM_TOOLS["System Tools"]
    end
    
    subgraph "System Call Interface"
        SYSCALL_LAYER["System Call Layer<br/>kernel/sys_ni.c"]
        IOURING_API["io_uring Interface<br/>io_uring/, fs/io_uring.c<br/>264.63 importance"]
    end
    
    subgraph "Virtual File System"
        VFS_LAYER["VFS Layer<br/>fs/"]
        PAGE_CACHE["Page Cache<br/>mm/filemap.c"]
    end
    
    subgraph "Filesystems"
        EXT4_FS["EXT4 Filesystem<br/>fs/ext4/<br/>188.40 importance"]
        BTRFS_FS["Btrfs Filesystem<br/>fs/btrfs/<br/>193.85 importance"]
        OTHER_FS["9P, ProcFS, SysFS<br/>fs/9p/, fs/proc/, fs/sysfs/"]
    end
    
    subgraph "Block Layer"
        BLOCK_CORE["Block Layer Core<br/>block/"]
        BLK_MQ["blk-mq Multi-Queue<br/>block/blk-mq.c"]
        IO_SCHEDULERS["I/O Schedulers<br/>block/"]
    end
    
    subgraph "Storage Drivers"
        NVME_DRIVER["NVMe Driver<br/>drivers/nvme/<br/>160.80 importance"]
        SCSI_SUBSYS["SCSI Subsystem<br/>drivers/scsi/"]
        HYPERV_STORAGE["Hyper-V Storage<br/>drivers/scsi/storvsc_drv.c"]
    end
    
    subgraph "Memory Management"
        IOMMU_SUBSYS["IOMMU Subsystem<br/>drivers/iommu/<br/>236.69 importance"]
        DMA_MAPPING["DMA Mapping<br/>kernel/dma/"]
    end
    
    subgraph "Hardware"
        NVME_HW["NVMe SSDs"]
        TRADITIONAL_STORAGE["SATA/SAS Storage"]
        VIRTUAL_STORAGE["Hyper-V Virtual Disks"]
    end
    
    USER_APPS --> SYSCALL_LAYER
    SYSTEM_TOOLS --> IOURING_API
    SYSCALL_LAYER --> VFS_LAYER
    IOURING_API --> VFS_LAYER
    
    VFS_LAYER --> PAGE_CACHE
    VFS_LAYER --> EXT4_FS
    VFS_LAYER --> BTRFS_FS
    VFS_LAYER --> OTHER_FS
    
    EXT4_FS --> BLOCK_CORE
    BTRFS_FS --> BLOCK_CORE
    OTHER_FS --> BLOCK_CORE
    
    BLOCK_CORE --> BLK_MQ
    BLK_MQ --> IO_SCHEDULERS
    IO_SCHEDULERS --> NVME_DRIVER
    IO_SCHEDULERS --> SCSI_SUBSYS
    IO_SCHEDULERS --> HYPERV_STORAGE
    
    NVME_DRIVER --> IOMMU_SUBSYS
    SCSI_SUBSYS --> DMA_MAPPING
    HYPERV_STORAGE --> DMA_MAPPING
    
    IOMMU_SUBSYS --> NVME_HW
    DMA_MAPPING --> TRADITIONAL_STORAGE
    DMA_MAPPING --> VIRTUAL_STORAGE
```

### Key I/O Components

The I/O subsystem provides multiple high-performance interfaces and storage support:

| Component | Location | Importance | Description |
|-----------|----------|------------|-------------|
| io_uring Framework | `io_uring/`, `fs/io_uring.c` | 264.63 | High-performance asynchronous I/O |
| IOMMU Subsystem | `drivers/iommu/` | 236.69 | DMA remapping and device isolation |
| EXT4 Filesystem | `fs/ext4/` | 188.40 | Journaling filesystem with fast commit |
| Btrfs Filesystem | `fs/btrfs/` | 193.85 | Copy-on-write filesystem with snapshots |
| NVMe Driver | `drivers/nvme/` | 160.80 | High-speed NVMe storage support |

### Filesystem Support Matrix

The kernel provides comprehensive filesystem support for various use cases:

| Filesystem | Location | Key Features |
|------------|----------|--------------|
| EXT4 | `fs/ext4/` | Journaling, extent-based allocation, fast commit |
| Btrfs | `fs/btrfs/` | CoW, snapshots, RAID support, checksumming |
| 9P | `fs/9p/` | Plan 9 protocol for WSL2 Windows file sharing |
| ProcFS | `fs/proc/` | Process and kernel information interface |
| SysFS | `fs/sysfs/` | Kernel object and attribute interface |
| DebugFS | `fs/debugfs/` | Kernel debugging filesystem |

Sources:
- [io_uring/]()
- [fs/ext4/]()
- [fs/btrfs/]()
- [fs/9p/]()
- [drivers/iommu/]()
- [drivers/nvme/]()
- [drivers/scsi/storvsc_drv.c]()

## Process Scheduling and Resource Management

The kernel implements multiple scheduling classes and resource management systems optimized for both desktop and server workloads, with support for containerization and virtualization scenarios.

### Process Management Architecture

```mermaid
graph TB
    subgraph "Scheduler Infrastructure"
        SCHED_CORE["scheduler/core.c<br/>Scheduler Core"]
        SCHED_FAIR["scheduler/fair.c<br/>CFS Scheduler<br/>258.33 importance"]
        SCHED_RT["scheduler/rt.c<br/>RT Scheduler"]
        SCHED_DL["scheduler/deadline.c<br/>Deadline Scheduler"]
    end
    
    subgraph "Workqueue Subsystem"
        WORKQUEUE_CORE["workqueue.c<br/>Async Task Execution<br/>181.34 importance"]
        WORKER_THREADS["Worker Threads"]
        WORK_ITEMS["Work Items"]
    end
    
    subgraph "Resource Management"
        CGROUP_CORE["cgroup/cgroup.c<br/>Control Groups"]
        CPU_CONTROLLER["cgroup/cpuset.c<br/>CPU Controller"]
        MEM_CONTROLLER["cgroup/memcg.c<br/>Memory Controller"]
        IO_CONTROLLER["cgroup/io.c<br/>I/O Controller"]
    end
    
    subgraph "Process Structures"
        TASK_STRUCT["include/linux/sched.h<br/>Task Structure"]
        RUN_QUEUES["Per-CPU Run Queues"]
        LOAD_BALANCER["Load Balancing"]
    end
    
    subgraph "Synchronization"
        FUTEX_SYS["futex.c<br/>Fast Userspace Mutexes"]
        RWSEM["rwsem.c<br/>Reader-Writer Semaphores"]
        MUTEX["mutex.c<br/>Mutual Exclusion"]
    end
    
    SCHED_CORE --> SCHED_FAIR
    SCHED_CORE --> SCHED_RT
    SCHED_CORE --> SCHED_DL
    
    SCHED_FAIR --> WORKQUEUE_CORE
    WORKQUEUE_CORE --> WORKER_THREADS
    WORKER_THREADS --> WORK_ITEMS
    
    CGROUP_CORE --> CPU_CONTROLLER
    CGROUP_CORE --> MEM_CONTROLLER
    CGROUP_CORE --> IO_CONTROLLER
    
    SCHED_FAIR --> TASK_STRUCT
    TASK_STRUCT --> RUN_QUEUES
    RUN_QUEUES --> LOAD_BALANCER
    
    TASK_STRUCT --> FUTEX_SYS
    TASK_STRUCT --> RWSEM
    TASK_STRUCT --> MUTEX
```

### Scheduling Components

The kernel provides multiple scheduling policies and resource management mechanisms:

| Component | Location | Importance | Description |
|-----------|----------|------------|-------------|
| CFS Scheduler | `kernel/sched/fair.c` | 258.33 | Completely Fair Scheduler with EEVDF algorithm |
| Workqueue Subsystem | `kernel/workqueue.c` | 181.34 | Asynchronous task execution framework |
| Real-Time Scheduler | `kernel/sched/rt.c` | - | Priority-based real-time scheduling |
| Deadline Scheduler | `kernel/sched/deadline.c` | - | Earliest Deadline First scheduling |
| Control Groups | `kernel/cgroup/` | - | Resource limiting and accounting |

### Resource Control Mechanisms

The kernel provides comprehensive resource management through cgroups and other mechanisms:

| Controller | Location | Function |
|------------|----------|----------|
| CPU Controller | `kernel/cgroup/cpuset.c` | CPU usage limits and NUMA topology |
| Memory Controller | `mm/memcontrol.c` | Memory allocation limits and accounting |
| I/O Controller | `block/blk-cgroup.c` | I/O bandwidth throttling and prioritization |
| Device Controller | `security/device_cgroup.c` | Device access control |
| PID Controller | `kernel/cgroup/pids.c` | Process count limits |

Sources:
- [kernel/sched/fair.c]()
- [kernel/workqueue.c]()
- [kernel/sched/core.c]()
- [kernel/cgroup/cgroup.c]()
- [include/linux/sched.h]()

## Windows Integration Components

The kernel includes specialized components for seamless integration with Windows through Hyper-V and DirectX graphics virtualization, enabling high-performance graphics and audio support in WSL2 environments.

### Windows Integration Architecture

```mermaid
graph TB
    subgraph "Graphics Integration"
        DXGKRNL_CORE["DXGKRNL Driver<br/>DirectX Graphics Kernel<br/>428.45 importance"]
        DXG_ADAPTER["dxgadapter.c<br/>Graphics Adapter Management"]
        DXG_DEVICE["dxgdevice.c<br/>Graphics Device Interface"]
        DXG_CONTEXT["dxgcontext.c<br/>Graphics Context Management"]
        DXG_ALLOC["dxgalloc.c<br/>Memory Allocation"]
    end
    
    subgraph "Audio Subsystem"
        HDA_INTEL["hda_intel.c<br/>Intel HDA Controller<br/>172.50 importance"]
        HDA_CODEC["hda_codec.c<br/>HDA Codec Framework"]
        REALTEK_ALC["patch_realtek.c<br/>Realtek ALC Codecs"]
        CIRRUS_CS35L41["cs35l41_hda.c<br/>Cirrus Logic CS35L41"]
    end
    
    subgraph "Hyper-V Communication"
        VMBUS_DRV["vmbus_drv.c<br/>VM Bus Driver"]
        HV_UTILS["hv_utils.c<br/>Hyper-V Utilities"]
        HV_BALLOON["hv_balloon.c<br/>Dynamic Memory"]
        HV_VSOCK["hv_sock.c<br/>Hyper-V Sockets"]
    end
    
    subgraph "Storage Integration"
        STORVSC["storvsc_drv.c<br/>Storage VSC Driver"]
        NETVSC["netvsc_drv.c<br/>Network VSC Driver"]
    end
    
    VMBUS_DRV --> DXGKRNL_CORE
    VMBUS_DRV --> HDA_INTEL
    VMBUS_DRV --> STORVSC
    VMBUS_DRV --> NETVSC
    
    DXGKRNL_CORE --> DXG_ADAPTER
    DXGKRNL_CORE --> DXG_DEVICE
    DXGKRNL_CORE --> DXG_CONTEXT
    DXGKRNL_CORE --> DXG_ALLOC
    
    HDA_INTEL --> HDA_CODEC
    HDA_CODEC --> REALTEK_ALC
    HDA_CODEC --> CIRRUS_CS35L41
    
    VMBUS_DRV --> HV_UTILS
    HV_UTILS --> HV_BALLOON
    HV_UTILS --> HV_VSOCK
```

### DirectX Graphics Kernel (DXGKRNL)

The DXGKRNL driver provides GPU virtualization capabilities, enabling Linux applications to access Windows GPU resources through DirectX interfaces:

| Component | Function |
|-----------|----------|
| Graphics Adapter | Hardware abstraction for GPU devices |
| Device Management | D3D device lifecycle and resource management |
| Context Management | Graphics execution context handling |
| Memory Allocation | GPU memory management and sharing |
| Command Submission | Graphics command buffer processing |

### Audio Driver Support

The kernel includes comprehensive audio support through HD-Audio infrastructure:

| Driver | Location | Description |
|--------|----------|-------------|
| Intel HDA | `sound/pci/hda/hda_intel.c` | Intel HD-Audio controller driver |
| Realtek Codecs | `sound/pci/hda/patch_realtek.c` | Realtek ALC codec support |
| Cirrus Logic | `sound/pci/hda/cs35l41_hda.c` | CS35L41 amplifier driver |
| Generic HDA | `sound/pci/hda/hda_codec.c` | HDA codec framework |

### Hyper-V Integration Services

Core Hyper-V integration is provided through VM Bus and utility services:

| Service | Location | Function |
|---------|----------|----------|
| VM Bus Driver | `drivers/hv/vmbus_drv.c` | Primary communication channel |
| Utilities | `drivers/hv/hv_utils.c` | Time sync, shutdown, heartbeat |
| Balloon Driver | `drivers/hv/hv_balloon.c` | Dynamic memory adjustment |
| Storage VSC | `drivers/scsi/storvsc_drv.c` | Virtual SCSI controller |
| Network VSC | `drivers/net/hyperv/netvsc_drv.c` | Virtual network adapter |

Sources:
- [drivers/gpu/drm/]()
- [sound/pci/hda/hda_intel.c]()
- [sound/pci/hda/patch_realtek.c]()
- [drivers/hv/vmbus_drv.c]()
- [drivers/hv/hv_utils.c]()
- [drivers/scsi/storvsc_drv.c]()

## Kernel Symbol Management and Debugging

The kernel includes comprehensive symbol management and debugging infrastructure essential for kernel development, module loading, and system debugging.

### Kernel Symbol Infrastructure

```mermaid
graph TB
    subgraph "Symbol Generation"
        KALLSYMS_SCRIPT["scripts/kallsyms.c<br/>17.74 importance<br/>Symbol Table Generator"]
        MKSYSMAP["scripts/mksysmap<br/>12.09 importance<br/>System Map Creation"]
        MODPOST["scripts/mod/modpost.c<br/>Symbol Processing"]
    end
    
    subgraph "Runtime Symbol Support"
        KALLSYMS_CORE["kernel/kallsyms.c<br/>12.80 importance<br/>Runtime Symbol Resolution"]
        KALLSYMS_SELFTEST["kernel/kallsyms_selftest.c<br/>5.05 importance<br/>Symbol System Testing"]
        MODULE_KALLSYMS["kernel/module/kallsyms.c<br/>6.63 importance<br/>Module Symbol Support"]
    end
    
    subgraph "Symbol Tables"
        KALLSYMS_NAMES["kallsyms_names[]<br/>Compressed Symbol Names"]
        KALLSYMS_ADDRS["kallsyms_addresses[]<br/>Symbol Addresses"]
        KALLSYMS_TOKENS["kallsyms_token_table[]<br/>Compression Tokens"]
        SYSTEM_MAP["System.map<br/>Human-readable Symbol Map"]
    end
    
    subgraph "Memory Debugging"
        KMEMLEAK["mm/kmemleak.c<br/>5.43 importance<br/>Memory Leak Detection"]
        DEBUG_CONFIG["lib/Kconfig.debug<br/>59.12 importance<br/>Debug Configuration"]
        KCSAN_CONFIG["lib/Kconfig.kcsan<br/>4.24 importance<br/>Concurrency Sanitizer"]
    end
    
    KALLSYMS_SCRIPT --> KALLSYMS_NAMES
    KALLSYMS_SCRIPT --> KALLSYMS_ADDRS
    KALLSYMS_SCRIPT --> KALLSYMS_TOKENS
    MKSYSMAP --> SYSTEM_MAP
    
    KALLSYMS_CORE --> KALLSYMS_NAMES
    KALLSYMS_CORE --> KALLSYMS_ADDRS
    KALLSYMS_CORE --> KALLSYMS_TOKENS
    
    MODULE_KALLSYMS --> KALLSYMS_CORE
    KALLSYMS_SELFTEST --> KALLSYMS_CORE
    
    DEBUG_CONFIG --> KMEMLEAK
    DEBUG_CONFIG --> KCSAN_CONFIG
```

### Symbol System Components

The kernel provides comprehensive symbol management for debugging and introspection:

| Component | Location | Importance | Function |
|-----------|----------|------------|----------|
| Symbol Generator | `scripts/kallsyms.c` | 17.74 | Generates compressed symbol tables |
| Runtime Resolution | `kernel/kallsyms.c` | 12.80 | Resolves symbols at runtime |
| System Map | `scripts/mksysmap` | 12.09 | Creates human-readable symbol maps |
| Module Symbols | `kernel/module/kallsyms.c` | 6.63 | Module symbol support |
| Symbol Testing | `kernel/kallsyms_selftest.c` | 5.05 | Symbol system validation |

### Debugging Infrastructure

The kernel includes extensive debugging and analysis capabilities:

| Feature | Location | Purpose |
|---------|----------|---------|
| Memory Leak Detection | `mm/kmemleak.c` | Tracks memory allocation/deallocation |
| Kernel Concurrency Sanitizer | `lib/Kconfig.kcsan` | Detects data races |
| Debug Configuration | `lib/Kconfig.debug` | Comprehensive debug options |
| Dynamic Debug | `lib/dynamic_debug.c` | Runtime debug message control |
| Fault Injection | `lib/fault-inject.c` | Error condition testing |

### Build and Package Infrastructure

The kernel build system supports multiple packaging formats and deployment scenarios:

| Tool | Location | Function |
|------|----------|----------|
| RPM Generation | `scripts/package/mkspec` | Creates RPM package specifications |
| Debian Packaging | `scripts/package/mkdebian` | Generates Debian package metadata |
| Build Orchestration | `scripts/package/builddeb` | Debian package build process |
| Module Installation | `scripts/Makefile.modinst` | Module installation infrastructure |

Sources:
- [scripts/kallsyms.c:1-50]()
- [kernel/kallsyms.c:1-50]()
- [scripts/mksysmap:1-20]()
- [kernel/module/kallsyms.c:1-30]()
- [mm/kmemleak.c:1-50]()
- [lib/Kconfig.debug:1-50]()
- [scripts/package/mkspec:1-25]()
- [scripts/package/mkdebian:1-50]()

## Security Features

The WSL2 kernel includes multiple security features to ensure safe operation within the Windows environment:

| Feature | Description |
|---------|-------------|
| Memory Protection | Prevents unauthorized memory access |
| Secure Boot | Validates kernel integrity during boot |
| SEV (Secure Encrypted Virtualization) | Memory encryption for AMD platforms |
| Spectre/Meltdown Mitigations | Protections against CPU side-channel attacks |

Sources:
- [arch/x86/kernel/cpu/bugs.c]()
- [arch/x86/include/asm/nospec-branch.h]()
- [arch/x86/kvm/svm/sev.c]()

## Conclusion

The WSL2 Linux Kernel is a specialized adaptation of the standard Linux kernel designed to run efficiently in a Hyper-V environment while providing integration with Windows. It combines the power of Linux with optimizations for Windows interoperability, making it possible to run Linux applications with high performance on Windows systems.

The kernel maintains compatibility with standard Linux, allowing most Linux applications to run unmodified, while adding specific features to improve the user experience within the Windows Subsystem for Linux 2 environment.18:T5ef3,# Virtualization

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [Documentation/admin-guide/hw-vuln/gather_data_sampling.rst](Documentation/admin-guide/hw-vuln/gather_data_sampling.rst)
- [Documentation/admin-guide/hw-vuln/spectre.rst](Documentation/admin-guide/hw-vuln/spectre.rst)
- [Documentation/admin-guide/hw-vuln/srso.rst](Documentation/admin-guide/hw-vuln/srso.rst)
- [Documentation/virt/kvm/api.rst](Documentation/virt/kvm/api.rst)
- [Documentation/virt/kvm/devices/arm-vgic-its.rst](Documentation/virt/kvm/devices/arm-vgic-its.rst)
- [Documentation/virt/kvm/devices/vfio.rst](Documentation/virt/kvm/devices/vfio.rst)
- [Documentation/virt/kvm/devices/vm.rst](Documentation/virt/kvm/devices/vm.rst)
- [Documentation/virt/kvm/locking.rst](Documentation/virt/kvm/locking.rst)
- [arch/arm64/include/asm/acpi.h](arch/arm64/include/asm/acpi.h)
- [arch/arm64/include/asm/assembler.h](arch/arm64/include/asm/assembler.h)
- [arch/arm64/include/asm/el2_setup.h](arch/arm64/include/asm/el2_setup.h)
- [arch/arm64/include/asm/kvm_arm.h](arch/arm64/include/asm/kvm_arm.h)
- [arch/arm64/include/asm/kvm_emulate.h](arch/arm64/include/asm/kvm_emulate.h)
- [arch/arm64/include/asm/kvm_host.h](arch/arm64/include/asm/kvm_host.h)
- [arch/arm64/include/asm/kvm_hyp.h](arch/arm64/include/asm/kvm_hyp.h)
- [arch/arm64/include/asm/kvm_nested.h](arch/arm64/include/asm/kvm_nested.h)
- [arch/arm64/include/asm/kvm_ras.h](arch/arm64/include/asm/kvm_ras.h)
- [arch/arm64/include/asm/pgtable-hwdef.h](arch/arm64/include/asm/pgtable-hwdef.h)
- [arch/arm64/include/asm/pgtable-prot.h](arch/arm64/include/asm/pgtable-prot.h)
- [arch/arm64/include/asm/sysreg.h](arch/arm64/include/asm/sysreg.h)
- [arch/arm64/include/uapi/asm/kvm.h](arch/arm64/include/uapi/asm/kvm.h)
- [arch/arm64/kvm/Kconfig](arch/arm64/kvm/Kconfig)
- [arch/arm64/kvm/Makefile](arch/arm64/kvm/Makefile)
- [arch/arm64/kvm/arch_timer.c](arch/arm64/kvm/arch_timer.c)
- [arch/arm64/kvm/arm.c](arch/arm64/kvm/arm.c)
- [arch/arm64/kvm/debug.c](arch/arm64/kvm/debug.c)
- [arch/arm64/kvm/emulate-nested.c](arch/arm64/kvm/emulate-nested.c)
- [arch/arm64/kvm/fpsimd.c](arch/arm64/kvm/fpsimd.c)
- [arch/arm64/kvm/handle_exit.c](arch/arm64/kvm/handle_exit.c)
- [arch/arm64/kvm/hyp/entry.S](arch/arm64/kvm/hyp/entry.S)
- [arch/arm64/kvm/hyp/include/hyp/switch.h](arch/arm64/kvm/hyp/include/hyp/switch.h)
- [arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h](arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h)
- [arch/arm64/kvm/hyp/include/nvhe/fixed_config.h](arch/arm64/kvm/hyp/include/nvhe/fixed_config.h)
- [arch/arm64/kvm/hyp/nvhe/debug-sr.c](arch/arm64/kvm/hyp/nvhe/debug-sr.c)
- [arch/arm64/kvm/hyp/nvhe/hyp-main.c](arch/arm64/kvm/hyp/nvhe/hyp-main.c)
- [arch/arm64/kvm/hyp/nvhe/pkvm.c](arch/arm64/kvm/hyp/nvhe/pkvm.c)
- [arch/arm64/kvm/hyp/nvhe/switch.c](arch/arm64/kvm/hyp/nvhe/switch.c)
- [arch/arm64/kvm/hyp/nvhe/sys_regs.c](arch/arm64/kvm/hyp/nvhe/sys_regs.c)
- [arch/arm64/kvm/hyp/nvhe/timer-sr.c](arch/arm64/kvm/hyp/nvhe/timer-sr.c)
- [arch/arm64/kvm/hyp/vgic-v3-sr.c](arch/arm64/kvm/hyp/vgic-v3-sr.c)
- [arch/arm64/kvm/hyp/vhe/switch.c](arch/arm64/kvm/hyp/vhe/switch.c)
- [arch/arm64/kvm/hypercalls.c](arch/arm64/kvm/hypercalls.c)
- [arch/arm64/kvm/inject_fault.c](arch/arm64/kvm/inject_fault.c)
- [arch/arm64/kvm/nested.c](arch/arm64/kvm/nested.c)
- [arch/arm64/kvm/pmu-emul.c](arch/arm64/kvm/pmu-emul.c)
- [arch/arm64/kvm/pvtime.c](arch/arm64/kvm/pvtime.c)
- [arch/arm64/kvm/reset.c](arch/arm64/kvm/reset.c)
- [arch/arm64/kvm/sys_regs.c](arch/arm64/kvm/sys_regs.c)
- [arch/arm64/kvm/sys_regs.h](arch/arm64/kvm/sys_regs.h)
- [arch/arm64/kvm/trace_arm.h](arch/arm64/kvm/trace_arm.h)
- [arch/arm64/tools/sysreg](arch/arm64/tools/sysreg)
- [arch/mips/kvm/Kconfig](arch/mips/kvm/Kconfig)
- [arch/powerpc/kvm/Kconfig](arch/powerpc/kvm/Kconfig)
- [arch/powerpc/kvm/powerpc.c](arch/powerpc/kvm/powerpc.c)
- [arch/riscv/kvm/Kconfig](arch/riscv/kvm/Kconfig)
- [arch/riscv/kvm/mmu.c](arch/riscv/kvm/mmu.c)
- [arch/riscv/kvm/vmid.c](arch/riscv/kvm/vmid.c)
- [arch/s390/include/asm/kvm_host.h](arch/s390/include/asm/kvm_host.h)
- [arch/s390/include/uapi/asm/kvm.h](arch/s390/include/uapi/asm/kvm.h)
- [arch/s390/kvm/Kconfig](arch/s390/kvm/Kconfig)
- [arch/s390/kvm/gaccess.c](arch/s390/kvm/gaccess.c)
- [arch/s390/kvm/gaccess.h](arch/s390/kvm/gaccess.h)
- [arch/s390/kvm/kvm-s390.c](arch/s390/kvm/kvm-s390.c)
- [arch/s390/kvm/kvm-s390.h](arch/s390/kvm/kvm-s390.h)
- [arch/s390/pci/Makefile](arch/s390/pci/Makefile)
- [arch/s390/pci/pci_kvm_hook.c](arch/s390/pci/pci_kvm_hook.c)
- [arch/x86/events/intel/pt.c](arch/x86/events/intel/pt.c)
- [arch/x86/include/asm/asm-prototypes.h](arch/x86/include/asm/asm-prototypes.h)
- [arch/x86/include/asm/cpufeatures.h](arch/x86/include/asm/cpufeatures.h)
- [arch/x86/include/asm/intel_pt.h](arch/x86/include/asm/intel_pt.h)
- [arch/x86/include/asm/kvm-x86-ops.h](arch/x86/include/asm/kvm-x86-ops.h)
- [arch/x86/include/asm/kvm-x86-pmu-ops.h](arch/x86/include/asm/kvm-x86-pmu-ops.h)
- [arch/x86/include/asm/kvm_host.h](arch/x86/include/asm/kvm_host.h)
- [arch/x86/include/asm/kvm_page_track.h](arch/x86/include/asm/kvm_page_track.h)
- [arch/x86/include/asm/msr-index.h](arch/x86/include/asm/msr-index.h)
- [arch/x86/include/asm/nops.h](arch/x86/include/asm/nops.h)
- [arch/x86/include/asm/nospec-branch.h](arch/x86/include/asm/nospec-branch.h)
- [arch/x86/include/asm/page.h](arch/x86/include/asm/page.h)
- [arch/x86/include/asm/processor.h](arch/x86/include/asm/processor.h)
- [arch/x86/include/asm/required-features.h](arch/x86/include/asm/required-features.h)
- [arch/x86/include/asm/smap.h](arch/x86/include/asm/smap.h)
- [arch/x86/include/asm/svm.h](arch/x86/include/asm/svm.h)
- [arch/x86/include/asm/vmx.h](arch/x86/include/asm/vmx.h)
- [arch/x86/kernel/alternative.c](arch/x86/kernel/alternative.c)
- [arch/x86/kernel/asm-offsets.c](arch/x86/kernel/asm-offsets.c)
- [arch/x86/kernel/cpu/amd.c](arch/x86/kernel/cpu/amd.c)
- [arch/x86/kernel/cpu/bugs.c](arch/x86/kernel/cpu/bugs.c)
- [arch/x86/kernel/cpu/common.c](arch/x86/kernel/cpu/common.c)
- [arch/x86/kernel/cpu/cpu.h](arch/x86/kernel/cpu/cpu.h)
- [arch/x86/kernel/cpu/cpuid-deps.c](arch/x86/kernel/cpu/cpuid-deps.c)
- [arch/x86/kernel/cpu/hygon.c](arch/x86/kernel/cpu/hygon.c)
- [arch/x86/kernel/cpu/scattered.c](arch/x86/kernel/cpu/scattered.c)
- [arch/x86/kernel/cpu/tsx.c](arch/x86/kernel/cpu/tsx.c)
- [arch/x86/kernel/vmlinux.lds.S](arch/x86/kernel/vmlinux.lds.S)
- [arch/x86/kvm/.gitignore](arch/x86/kvm/.gitignore)
- [arch/x86/kvm/Kconfig](arch/x86/kvm/Kconfig)
- [arch/x86/kvm/Makefile](arch/x86/kvm/Makefile)
- [arch/x86/kvm/cpuid.c](arch/x86/kvm/cpuid.c)
- [arch/x86/kvm/cpuid.h](arch/x86/kvm/cpuid.h)
- [arch/x86/kvm/governed_features.h](arch/x86/kvm/governed_features.h)
- [arch/x86/kvm/ioapic.c](arch/x86/kvm/ioapic.c)
- [arch/x86/kvm/kvm-asm-offsets.c](arch/x86/kvm/kvm-asm-offsets.c)
- [arch/x86/kvm/kvm_cache_regs.h](arch/x86/kvm/kvm_cache_regs.h)
- [arch/x86/kvm/kvm_onhyperv.c](arch/x86/kvm/kvm_onhyperv.c)
- [arch/x86/kvm/kvm_onhyperv.h](arch/x86/kvm/kvm_onhyperv.h)
- [arch/x86/kvm/mmu.h](arch/x86/kvm/mmu.h)
- [arch/x86/kvm/mmu/mmu.c](arch/x86/kvm/mmu/mmu.c)
- [arch/x86/kvm/mmu/mmu_internal.h](arch/x86/kvm/mmu/mmu_internal.h)
- [arch/x86/kvm/mmu/page_track.c](arch/x86/kvm/mmu/page_track.c)
- [arch/x86/kvm/mmu/page_track.h](arch/x86/kvm/mmu/page_track.h)
- [arch/x86/kvm/mmu/paging_tmpl.h](arch/x86/kvm/mmu/paging_tmpl.h)
- [arch/x86/kvm/mmu/spte.c](arch/x86/kvm/mmu/spte.c)
- [arch/x86/kvm/mmu/spte.h](arch/x86/kvm/mmu/spte.h)
- [arch/x86/kvm/mmu/tdp_iter.c](arch/x86/kvm/mmu/tdp_iter.c)
- [arch/x86/kvm/mmu/tdp_iter.h](arch/x86/kvm/mmu/tdp_iter.h)
- [arch/x86/kvm/mmu/tdp_mmu.c](arch/x86/kvm/mmu/tdp_mmu.c)
- [arch/x86/kvm/mmu/tdp_mmu.h](arch/x86/kvm/mmu/tdp_mmu.h)
- [arch/x86/kvm/mtrr.c](arch/x86/kvm/mtrr.c)
- [arch/x86/kvm/pmu.c](arch/x86/kvm/pmu.c)
- [arch/x86/kvm/pmu.h](arch/x86/kvm/pmu.h)
- [arch/x86/kvm/reverse_cpuid.h](arch/x86/kvm/reverse_cpuid.h)
- [arch/x86/kvm/svm/avic.c](arch/x86/kvm/svm/avic.c)
- [arch/x86/kvm/svm/nested.c](arch/x86/kvm/svm/nested.c)
- [arch/x86/kvm/svm/pmu.c](arch/x86/kvm/svm/pmu.c)
- [arch/x86/kvm/svm/sev.c](arch/x86/kvm/svm/sev.c)
- [arch/x86/kvm/svm/svm.c](arch/x86/kvm/svm/svm.c)
- [arch/x86/kvm/svm/svm.h](arch/x86/kvm/svm/svm.h)
- [arch/x86/kvm/svm/svm_onhyperv.h](arch/x86/kvm/svm/svm_onhyperv.h)
- [arch/x86/kvm/svm/svm_ops.h](arch/x86/kvm/svm/svm_ops.h)
- [arch/x86/kvm/svm/vmenter.S](arch/x86/kvm/svm/vmenter.S)
- [arch/x86/kvm/vmx/capabilities.h](arch/x86/kvm/vmx/capabilities.h)
- [arch/x86/kvm/vmx/hyperv.c](arch/x86/kvm/vmx/hyperv.c)
- [arch/x86/kvm/vmx/hyperv.h](arch/x86/kvm/vmx/hyperv.h)
- [arch/x86/kvm/vmx/nested.c](arch/x86/kvm/vmx/nested.c)
- [arch/x86/kvm/vmx/nested.h](arch/x86/kvm/vmx/nested.h)
- [arch/x86/kvm/vmx/pmu_intel.c](arch/x86/kvm/vmx/pmu_intel.c)
- [arch/x86/kvm/vmx/run_flags.h](arch/x86/kvm/vmx/run_flags.h)
- [arch/x86/kvm/vmx/vmcs.h](arch/x86/kvm/vmx/vmcs.h)
- [arch/x86/kvm/vmx/vmenter.S](arch/x86/kvm/vmx/vmenter.S)
- [arch/x86/kvm/vmx/vmx.c](arch/x86/kvm/vmx/vmx.c)
- [arch/x86/kvm/vmx/vmx.h](arch/x86/kvm/vmx/vmx.h)
- [arch/x86/kvm/vmx/vmx_ops.h](arch/x86/kvm/vmx/vmx_ops.h)
- [arch/x86/kvm/x86.c](arch/x86/kvm/x86.c)
- [arch/x86/kvm/x86.h](arch/x86/kvm/x86.h)
- [arch/x86/kvm/xen.c](arch/x86/kvm/xen.c)
- [arch/x86/kvm/xen.h](arch/x86/kvm/xen.h)
- [arch/x86/lib/retpoline.S](arch/x86/lib/retpoline.S)
- [arch/x86/mm/maccess.c](arch/x86/mm/maccess.c)
- [drivers/acpi/processor_idle.c](drivers/acpi/processor_idle.c)
- [drivers/acpi/utils.c](drivers/acpi/utils.c)
- [drivers/gpu/drm/i915/gvt/debugfs.c](drivers/gpu/drm/i915/gvt/debugfs.c)
- [drivers/gpu/drm/i915/gvt/dmabuf.c](drivers/gpu/drm/i915/gvt/dmabuf.c)
- [drivers/gpu/drm/i915/gvt/gtt.c](drivers/gpu/drm/i915/gvt/gtt.c)
- [drivers/gpu/drm/i915/gvt/gtt.h](drivers/gpu/drm/i915/gvt/gtt.h)
- [drivers/gpu/drm/i915/gvt/gvt.h](drivers/gpu/drm/i915/gvt/gvt.h)
- [drivers/gpu/drm/i915/gvt/interrupt.c](drivers/gpu/drm/i915/gvt/interrupt.c)
- [drivers/gpu/drm/i915/gvt/kvmgt.c](drivers/gpu/drm/i915/gvt/kvmgt.c)
- [drivers/gpu/drm/i915/gvt/page_track.c](drivers/gpu/drm/i915/gvt/page_track.c)
- [drivers/gpu/drm/i915/gvt/vgpu.c](drivers/gpu/drm/i915/gvt/vgpu.c)
- [drivers/hwtracing/coresight/coresight-trbe.c](drivers/hwtracing/coresight/coresight-trbe.c)
- [drivers/hwtracing/coresight/coresight-trbe.h](drivers/hwtracing/coresight/coresight-trbe.h)
- [drivers/perf/apple_m1_cpu_pmu.c](drivers/perf/apple_m1_cpu_pmu.c)
- [drivers/perf/arm_pmu.c](drivers/perf/arm_pmu.c)
- [drivers/perf/arm_pmu_acpi.c](drivers/perf/arm_pmu_acpi.c)
- [drivers/s390/crypto/vfio_ap_ops.c](drivers/s390/crypto/vfio_ap_ops.c)
- [drivers/s390/crypto/vfio_ap_private.h](drivers/s390/crypto/vfio_ap_private.h)
- [drivers/vfio/pci/vfio_pci_zdev.c](drivers/vfio/pci/vfio_pci_zdev.c)
- [include/kvm/arm_arch_timer.h](include/kvm/arm_arch_timer.h)
- [include/kvm/arm_hypercalls.h](include/kvm/arm_hypercalls.h)
- [include/kvm/arm_pmu.h](include/kvm/arm_pmu.h)
- [include/kvm/iodev.h](include/kvm/iodev.h)
- [include/linux/kvm_dirty_ring.h](include/linux/kvm_dirty_ring.h)
- [include/linux/kvm_host.h](include/linux/kvm_host.h)
- [include/linux/kvm_irqfd.h](include/linux/kvm_irqfd.h)
- [include/linux/kvm_types.h](include/linux/kvm_types.h)
- [include/linux/perf/arm_pmu.h](include/linux/perf/arm_pmu.h)
- [include/uapi/linux/kvm.h](include/uapi/linux/kvm.h)
- [tools/arch/arm64/include/uapi/asm/kvm.h](tools/arch/arm64/include/uapi/asm/kvm.h)
- [tools/arch/x86/include/asm/cpufeatures.h](tools/arch/x86/include/asm/cpufeatures.h)
- [tools/arch/x86/include/asm/disabled-features.h](tools/arch/x86/include/asm/disabled-features.h)
- [tools/arch/x86/include/asm/msr-index.h](tools/arch/x86/include/asm/msr-index.h)
- [tools/arch/x86/include/asm/nops.h](tools/arch/x86/include/asm/nops.h)
- [tools/arch/x86/include/asm/required-features.h](tools/arch/x86/include/asm/required-features.h)
- [tools/arch/x86/include/uapi/asm/kvm.h](tools/arch/x86/include/uapi/asm/kvm.h)
- [tools/include/uapi/linux/kvm.h](tools/include/uapi/linux/kvm.h)
- [tools/testing/selftests/kvm/x86_64/sev_migrate_tests.c](tools/testing/selftests/kvm/x86_64/sev_migrate_tests.c)
- [virt/kvm/Kconfig](virt/kvm/Kconfig)
- [virt/kvm/coalesced_mmio.c](virt/kvm/coalesced_mmio.c)
- [virt/kvm/dirty_ring.c](virt/kvm/dirty_ring.c)
- [virt/kvm/eventfd.c](virt/kvm/eventfd.c)
- [virt/kvm/kvm_main.c](virt/kvm/kvm_main.c)
- [virt/kvm/kvm_mm.h](virt/kvm/kvm_mm.h)
- [virt/kvm/pfncache.c](virt/kvm/pfncache.c)
- [virt/kvm/vfio.c](virt/kvm/vfio.c)

</details>



## Purpose and Scope

This document provides a comprehensive overview of the virtualization capabilities and implementation in the WSL2 Linux Kernel. It covers the core architecture of the Kernel-based Virtual Machine (KVM) subsystem that enables hardware-accelerated virtualization across multiple CPU architectures, including x86 (both Intel VMX and AMD SVM) and ARM64. For information about specific virtualization features like Intel VMX implementation, see [KVM x86 Implementation](#2.1), and for ARM64 implementation details, see [KVM ARM64 Implementation](#2.2).

## Overview

Virtualization in the WSL2 Linux Kernel is implemented through the KVM (Kernel-based Virtual Machine) subsystem, which allows the kernel to function as a hypervisor. KVM leverages hardware virtualization extensions available in modern processors to run virtual machines efficiently with near-native performance.

```mermaid
flowchart TD
    subgraph "Hardware"
        CPU["CPU with Virtualization Extensions"]
    end

    subgraph "Kernel"
        KVM["KVM Core"]
        KVMARCH["Architecture-specific KVM<br>(x86/ARM64)"]
        VCPU["Virtual CPU Management"]
        MEMMGMT["Memory Management<br>(EPT/NPT/Stage-2)"]
        IO["I/O and MMIO Handling"]
    end

    subgraph "Guest"
        GUESTOS["Guest Operating System"]
    end

    CPU --> KVM
    KVM --> KVMARCH
    KVMARCH --> VCPU
    KVMARCH --> MEMMGMT
    KVMARCH --> IO
    VCPU --> GUESTOS
    MEMMGMT --> GUESTOS
    IO --> GUESTOS
```

Sources: [arch/x86/kvm/x86.c:1-137](). [arch/arm64/kvm/arm.c:48-64]()

## Architecture-Specific Implementations

KVM in the WSL2 Linux Kernel supports three major virtualization technologies:

1. **Intel VT-x (VMX)**: Intel's hardware virtualization technology
2. **AMD-V (SVM)**: AMD's hardware virtualization technology
3. **ARM Virtualization Extensions**: ARM's approach to hardware virtualization

### Comparison of x86 Virtualization Technologies

| Feature | Intel VMX | AMD SVM |
|---------|-----------|---------|
| Hardware Virtualization | VT-x | AMD-V |
| Memory Virtualization | EPT (Extended Page Tables) | NPT (Nested Page Tables) |
| Nested Virtualization | Supported (`nested=1`) | Supported (`nested=1`) |
| CPU Feature Control | MSR_IA32_FEATURE_CONTROL | EFER.SVME |
| VMM Entry/Exit | VMLAUNCH/VMRESUME, VMEXIT | VMRUN, #VMEXIT |
| Interrupt Virtualization | Posted Interrupts | AVIC |
| Pause Loop Exiting | `ple_gap`, `ple_window` | `pause_filter_count`, `pause_filter_thresh` |
| Trusted Execution | TDX | SEV (Secure Encrypted Virtualization) |

Sources: [arch/x86/kvm/vmx/vmx.c:82-213](), [arch/x86/kvm/svm/svm.c:176-193]()

## Memory Virtualization

Memory virtualization is a critical component of KVM that allows efficient translation from guest virtual addresses to host physical addresses while maintaining isolation between VMs.

```mermaid
flowchart TD
    subgraph "Guest Address Translation"
        GVA["Guest Virtual Address"]
        GPA["Guest Physical Address"]
    end

    subgraph "Host Address Translation"
        HVA["Host Virtual Address"]
        HPA["Host Physical Address"]
    end

    subgraph "Translation Technologies"
        EPT["Intel EPT<br>(Extended Page Tables)"]
        NPT["AMD NPT<br>(Nested Page Tables)"]
        S2T["ARM Stage-2 Translation"]
    end

    GVA -->|"Guest Page Tables"| GPA
    GPA -->|"Intel CPUs"| EPT
    GPA -->|"AMD CPUs"| NPT
    GPA -->|"ARM CPUs"| S2T
    EPT --> HPA
    NPT --> HPA
    S2T --> HPA
    HVA -->|"Host Page Tables"| HPA
```

### Extended Page Tables (EPT)

Intel's EPT technology provides hardware assistance for virtualizing memory, enabling the guest to use its own page tables without hypervisor intervention for most memory accesses.

Key features:
- Configured through `enable_ept` parameter (default: enabled)
- Supports accessed and dirty bit tracking via `enable_ept_ad_bits`
- Enables unrestricted guest execution with `enable_unrestricted_guest`

Sources: [arch/x86/kvm/vmx/vmx.c:92-100]()

### Nested Page Tables (NPT)

AMD's equivalent to EPT, providing similar functionality for efficient memory virtualization:

- Configured through `npt_enabled` parameter (default: enabled)
- Implemented as part of the AMD SVM architecture
- Reduces hypervisor overhead for memory operations

Sources: [arch/x86/kvm/svm/svm.c:198-200]()

### ARM Stage-2 Translation

ARM's approach to memory virtualization:

- Implemented within the ARM MMU architecture
- Supports multiple translation stages
- Configured through the virtual memory control registers

Sources: [arch/arm64/kvm/arm.c:163-167](), [arch/arm64/include/asm/kvm_host.h:143-177]()

## CPU Features and Control

### System Registers

The virtualization subsystem manages various system registers and MSRs to control the behavior of guest VMs.

#### x86 MSRs

KVM intercepts access to Model Specific Registers (MSRs) and either emulates them or passes them through to the hardware, depending on the specific MSR and the virtualization configuration.

Key MSRs include:
- MSR_IA32_FEATURE_CONTROL: Controls Intel VMX execution
- EFER: Extended Feature Enable Register
- MSR_IA32_SPEC_CTRL: Controls CPU speculative execution behavior

Sources: [arch/x86/kvm/x86.c:184-184](), [arch/x86/kvm/vmx/vmx.c:165-184]()

#### ARM System Registers

ARM virtualization involves managing system registers that control the execution environment:

```mermaid
flowchart LR
    subgraph "ARM System Registers"
        HCR_EL2["HCR_EL2<br>(Hypervisor Control Register)"]
        VTTBR_EL2["VTTBR_EL2<br>(Virtualization Translation Control)"]
        VTCR_EL2["VTCR_EL2<br>(Virtualization Translation Control)"]
        ESR_EL2["ESR_EL2<br>(Exception Syndrome Register)"]
    end

    subgraph "Guest Context"
        SCTLR_EL1["SCTLR_EL1<br>(System Control Register)"]
        TTBR0_EL1["TTBR0_EL1<br>(Translation Table Base Register)"]
        SPSR_EL2["SPSR_EL2<br>(Saved Program Status Register)"]
    end

    HCR_EL2 -->|"Controls"| SCTLR_EL1
    VTTBR_EL2 -->|"Stage-2 Translation"| TTBR0_EL1
    VTCR_EL2 -->|"Configure"| VTTBR_EL2
    ESR_EL2 -->|"Exception Return"| SPSR_EL2
```

Sources: [arch/arm64/include/asm/sysreg.h:168-592](), [arch/arm64/kvm/sys_regs.c:93-122]()

### CPUID Virtualization

KVM virtualizes the CPUID instruction to present a consistent set of CPU features to the guest, which may differ from the host CPU's capabilities.

Key aspects:
- Features can be masked or emulated
- Architecture-specific features are managed based on host capabilities
- Nested virtualization requires careful CPUID management

Sources: [arch/x86/kvm/cpuid.c:36-83]()

## Nested Virtualization

Nested virtualization allows running a hypervisor inside a virtual machine, effectively enabling VMs within VMs. This feature is useful for development and testing scenarios, as well as for compatibility with applications that require hardware virtualization.

```mermaid
flowchart TD
    subgraph "L0 (Host Hypervisor)"
        KVM_L0["KVM on Hardware<br>vmx_handle_external_intr()<br>svm_handle_exit()"]
    end

    subgraph "L1 (Guest Hypervisor)"
        KVM_L1["KVM in VM<br>nested_vmx_run()<br>nested_svm_vmrun()"]
        VCPU_L1["L1 Virtual CPU State<br>vmcs12/vmcb12"]
    end

    subgraph "L2 (Nested Guest)"
        VM_L2["L2 Guest VM<br>nested_vmx_vmexit()<br>nested_svm_vmexit()"]
    end

    KVM_L0 -->|"Hardware virtualization"| KVM_L1
    KVM_L1 -->|"Handle L1L2 VM-Enter"| VCPU_L1
    VCPU_L1 -->|"Shadow VMCS/VMCB"| VM_L2
    VM_L2 -->|"Exit handling"| KVM_L0
```

### Nested Intel VMX

Intel's nested virtualization uses shadow VMCS structures to manage the state transition between L1 and L2:

- Enabled via `nested=1` parameter
- Uses VMCS shadowing for efficient transitions
- Requires EPT support for optimal performance

Sources: [arch/x86/kvm/vmx/nested.c:1-17]()

### Nested AMD SVM

AMD's nested virtualization:

- Enabled via `nested=1` parameter
- Uses nested VMCB (Virtual Machine Control Block) structures
- Supports virtual GIF (Global Interrupt Flag) via `vgif=1`

Sources: [arch/x86/kvm/svm/svm.c:203-216](), [arch/x86/kvm/svm/nested.c:1-6]()

## Security Features

### Speculative Execution Controls

KVM implements various mitigations for CPU vulnerabilities such as Spectre and Meltdown:

- L1 Terminal Fault (L1TF) mitigation: `vmx_setup_l1d_flush()`
- Spectre V2 mitigation: MSR_IA32_SPEC_CTRL management
- Conditional IBPB (Indirect Branch Prediction Barrier) on context switch

Sources: [arch/x86/kernel/cpu/bugs.c:226-317](), [arch/x86/kernel/cpu/common.c:71-85]()

### AMD SEV (Secure Encrypted Virtualization)

AMD SEV provides memory encryption capabilities to protect VM data from physical attacks:

- Enabled via `sev_enabled=1` parameter
- SEV-ES (Encrypted State) extends protection to CPU registers
- Support for SEV-SNP (Secure Nested Paging) in newer platforms

Sources: [arch/x86/kvm/svm/sev.c:51-57]()

## Configuration Parameters

Virtualization behavior can be customized through various kernel parameters:

### Intel VMX Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| `kvm-intel.nested` | Enable nested virtualization | 1 (enabled) |
| `kvm-intel.ept` | Enable Extended Page Tables | 1 (enabled) |
| `kvm-intel.vpid` | Enable Virtual Processor IDs | 1 (enabled) |
| `kvm-intel.flexpriority` | Enable FlexPriority feature | 1 (enabled) |
| `kvm-intel.ple_gap` | Gap for pause-loop exiting detection | Platform-dependent |
| `kvm-intel.ple_window` | Window for pause-loop exiting | Platform-dependent |
| `kvm-intel.unrestricted_guest` | Allow unrestricted guest execution | 1 (enabled) |
| `kvm-intel.vmentry_l1d_flush` | Flush L1D on VM entry | "auto" |

Sources: [arch/x86/kvm/vmx/vmx.c:82-213]()

### AMD SVM Parameters

| Parameter | Description | Default |
|-----------|-------------|---------|
| `kvm-amd.nested` | Enable nested virtualization | 1 (enabled) |
| `kvm-amd.npt` | Enable Nested Page Tables | 1 (enabled) |
| `kvm-amd.avic` | Enable Advanced Virtual Interrupt Controller | 0 (disabled) |
| `kvm-amd.vls` | Enable Virtual VMLOAD/VMSAVE | 1 (enabled) |
| `kvm-amd.vgif` | Enable Virtual GIF | 1 (enabled) |
| `kvm-amd.nrips` | Enable Next RIP Save | 1 (enabled) |
| `kvm-amd.sev` | Enable Secure Encrypted Virtualization | 1 (enabled) |
| `kvm-amd.sev_es` | Enable SEV Encrypted State | 1 (enabled) |
| `kvm-amd.pause_filter_thresh` | Threshold for pause-loop exiting | Platform-dependent |
| `kvm-amd.pause_filter_count` | Count for pause-loop exiting | Platform-dependent |

Sources: [arch/x86/kvm/svm/svm.c:176-240]()

## Performance Optimizations

### Pause-Loop Exiting

Both Intel and AMD provide mechanisms to detect and optimize for CPU spinning loops (common in spin locks):

- Intel's PLE (Pause-Loop Exiting) with configurable `ple_gap` and `ple_window`
- AMD's Pause Filter with configurable `pause_filter_thresh` and `pause_filter_count`

Sources: [arch/x86/kvm/vmx/vmx.c:196-213](), [arch/x86/kvm/svm/svm.c:176-193]()

### PMU Virtualization

Performance Monitoring Unit virtualization allows guests to access hardware performance counters:

- Intel: vPMU controlled via `enable_pmu` parameter
- ARM: Implementation in `pmu-emul.c`

Sources: [arch/x86/kvm/vmx/pmu_intel.c:1-4](), [arch/arm64/kvm/pmu-emul.c:1-10]()

## Conclusion

The virtualization subsystem in the WSL2 Linux Kernel provides a robust and flexible foundation for running virtual machines with near-native performance. It leverages hardware virtualization extensions across multiple architectures and includes advanced features like nested virtualization and security enhancements. The modular design allows for comprehensive configuration to meet various performance, security, and compatibility requirements.19:T39a1,# KVM x86 Implementation

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [Documentation/virt/kvm/devices/arm-vgic-its.rst](Documentation/virt/kvm/devices/arm-vgic-its.rst)
- [Documentation/virt/kvm/locking.rst](Documentation/virt/kvm/locking.rst)
- [arch/arm64/kvm/Kconfig](arch/arm64/kvm/Kconfig)
- [arch/arm64/kvm/pvtime.c](arch/arm64/kvm/pvtime.c)
- [arch/mips/kvm/Kconfig](arch/mips/kvm/Kconfig)
- [arch/powerpc/kvm/Kconfig](arch/powerpc/kvm/Kconfig)
- [arch/riscv/kvm/Kconfig](arch/riscv/kvm/Kconfig)
- [arch/riscv/kvm/mmu.c](arch/riscv/kvm/mmu.c)
- [arch/riscv/kvm/vmid.c](arch/riscv/kvm/vmid.c)
- [arch/s390/kvm/Kconfig](arch/s390/kvm/Kconfig)
- [arch/x86/include/asm/kvm-x86-pmu-ops.h](arch/x86/include/asm/kvm-x86-pmu-ops.h)
- [arch/x86/include/asm/kvm_host.h](arch/x86/include/asm/kvm_host.h)
- [arch/x86/include/asm/kvm_page_track.h](arch/x86/include/asm/kvm_page_track.h)
- [arch/x86/kvm/Kconfig](arch/x86/kvm/Kconfig)
- [arch/x86/kvm/ioapic.c](arch/x86/kvm/ioapic.c)
- [arch/x86/kvm/mmu/mmu.c](arch/x86/kvm/mmu/mmu.c)
- [arch/x86/kvm/mmu/mmu_internal.h](arch/x86/kvm/mmu/mmu_internal.h)
- [arch/x86/kvm/mmu/page_track.c](arch/x86/kvm/mmu/page_track.c)
- [arch/x86/kvm/mmu/page_track.h](arch/x86/kvm/mmu/page_track.h)
- [arch/x86/kvm/mmu/paging_tmpl.h](arch/x86/kvm/mmu/paging_tmpl.h)
- [arch/x86/kvm/mmu/spte.c](arch/x86/kvm/mmu/spte.c)
- [arch/x86/kvm/mmu/spte.h](arch/x86/kvm/mmu/spte.h)
- [arch/x86/kvm/mmu/tdp_iter.c](arch/x86/kvm/mmu/tdp_iter.c)
- [arch/x86/kvm/mmu/tdp_iter.h](arch/x86/kvm/mmu/tdp_iter.h)
- [arch/x86/kvm/mmu/tdp_mmu.c](arch/x86/kvm/mmu/tdp_mmu.c)
- [arch/x86/kvm/mmu/tdp_mmu.h](arch/x86/kvm/mmu/tdp_mmu.h)
- [arch/x86/kvm/pmu.c](arch/x86/kvm/pmu.c)
- [arch/x86/kvm/pmu.h](arch/x86/kvm/pmu.h)
- [arch/x86/kvm/svm/pmu.c](arch/x86/kvm/svm/pmu.c)
- [arch/x86/kvm/vmx/pmu_intel.c](arch/x86/kvm/vmx/pmu_intel.c)
- [arch/x86/kvm/x86.c](arch/x86/kvm/x86.c)
- [arch/x86/kvm/xen.c](arch/x86/kvm/xen.c)
- [arch/x86/kvm/xen.h](arch/x86/kvm/xen.h)
- [drivers/gpu/drm/i915/gvt/debugfs.c](drivers/gpu/drm/i915/gvt/debugfs.c)
- [drivers/gpu/drm/i915/gvt/dmabuf.c](drivers/gpu/drm/i915/gvt/dmabuf.c)
- [drivers/gpu/drm/i915/gvt/gtt.c](drivers/gpu/drm/i915/gvt/gtt.c)
- [drivers/gpu/drm/i915/gvt/gtt.h](drivers/gpu/drm/i915/gvt/gtt.h)
- [drivers/gpu/drm/i915/gvt/gvt.h](drivers/gpu/drm/i915/gvt/gvt.h)
- [drivers/gpu/drm/i915/gvt/interrupt.c](drivers/gpu/drm/i915/gvt/interrupt.c)
- [drivers/gpu/drm/i915/gvt/kvmgt.c](drivers/gpu/drm/i915/gvt/kvmgt.c)
- [drivers/gpu/drm/i915/gvt/page_track.c](drivers/gpu/drm/i915/gvt/page_track.c)
- [drivers/gpu/drm/i915/gvt/vgpu.c](drivers/gpu/drm/i915/gvt/vgpu.c)
- [drivers/s390/crypto/vfio_ap_ops.c](drivers/s390/crypto/vfio_ap_ops.c)
- [drivers/s390/crypto/vfio_ap_private.h](drivers/s390/crypto/vfio_ap_private.h)
- [include/kvm/iodev.h](include/kvm/iodev.h)
- [include/linux/kvm_dirty_ring.h](include/linux/kvm_dirty_ring.h)
- [include/linux/kvm_host.h](include/linux/kvm_host.h)
- [include/linux/kvm_irqfd.h](include/linux/kvm_irqfd.h)
- [include/linux/kvm_types.h](include/linux/kvm_types.h)
- [virt/kvm/Kconfig](virt/kvm/Kconfig)
- [virt/kvm/coalesced_mmio.c](virt/kvm/coalesced_mmio.c)
- [virt/kvm/dirty_ring.c](virt/kvm/dirty_ring.c)
- [virt/kvm/eventfd.c](virt/kvm/eventfd.c)
- [virt/kvm/kvm_main.c](virt/kvm/kvm_main.c)
- [virt/kvm/kvm_mm.h](virt/kvm/kvm_mm.h)
- [virt/kvm/pfncache.c](virt/kvm/pfncache.c)

</details>



## Purpose and Scope

This document covers the x86-specific implementation of the Kernel-based Virtual Machine (KVM) hypervisor, focusing on Intel x86 and x86_64 architectures with VMX (Virtual Machine Extensions) support. The implementation provides hardware-assisted virtualization capabilities for x86 guest operating systems.

For AMD SVM-specific features and nested virtualization, see [AMD SVM Implementation](#2.3). For ARM64 virtualization, see [KVM ARM64 Implementation](#2.2). For high-level KVM concepts and generic functionality, see [WSL2 Linux Kernel Overview](#1).

## Core Architecture Overview

The KVM x86 implementation consists of several key subsystems that work together to provide efficient hardware virtualization:

```mermaid
graph TB
    subgraph "User Space"
        QEMU["QEMU/User VMM"]
        IOCTL["KVM ioctls"]
    end
    
    subgraph "KVM Core (x86)"
        X86_MAIN["kvm_x86_ops<br/>arch/x86/kvm/x86.c"]
        VCPU_OPS["vCPU Operations<br/>kvm_vcpu"]
        VM_OPS["VM Operations<br/>struct kvm"]
    end
    
    subgraph "Hardware Virtualization"
        VMX["Intel VMX<br/>arch/x86/kvm/vmx/"]
        GENERIC["Generic x86<br/>Hardware Abstraction"]
    end
    
    subgraph "Memory Management"
        MMU["Shadow MMU<br/>arch/x86/kvm/mmu/mmu.c"]
        TDP_MMU["TDP MMU<br/>arch/x86/kvm/mmu/tdp_mmu.c"] 
        PAGE_TRACK["Page Tracking<br/>arch/x86/kvm/mmu/page_track.c"]
    end
    
    subgraph "Supporting Subsystems"
        PMU["Performance Monitoring<br/>arch/x86/kvm/pmu.c"]
        LAPIC["Local APIC<br/>arch/x86/kvm/lapic.c"]
        XEN["Xen Emulation<br/>arch/x86/kvm/xen.c"]
    end
    
    QEMU --> IOCTL
    IOCTL --> X86_MAIN
    X86_MAIN --> VCPU_OPS
    X86_MAIN --> VM_OPS
    X86_MAIN --> VMX
    X86_MAIN --> GENERIC
    
    VMX --> MMU
    VMX --> TDP_MMU
    GENERIC --> PAGE_TRACK
    
    X86_MAIN --> PMU
    X86_MAIN --> LAPIC
    X86_MAIN --> XEN
```

Sources: [arch/x86/kvm/x86.c:1-4404](), [arch/x86/include/asm/kvm_host.h:1-1652](), [virt/kvm/kvm_main.c:1-6644]()

## Key Data Structures and Operations

The x86 KVM implementation centers around several critical data structures that manage virtualization state:

```mermaid
graph LR
    subgraph "Core Structures"
        KVM_VCPU["struct kvm_vcpu<br/>Per-vCPU state"]
        KVM_VM["struct kvm<br/>Per-VM state"]
        KVM_X86_OPS["struct kvm_x86_ops<br/>Hardware abstraction"]
    end
    
    subgraph "x86 Specific State"
        VCPU_ARCH["vcpu->arch<br/>kvm_vcpu_arch"]
        MMU_STATE["vcpu->arch.mmu<br/>struct kvm_mmu"]
        REGS["vcpu->arch registers<br/>GPRs, Control regs"]
    end
    
    subgraph "Hardware Interface"
        STATIC_CALLS["Static calls<br/>kvm_x86_*"]
        VMX_IMPL["VMX Implementation<br/>vmx_x86_ops"]
    end
    
    KVM_VCPU --> VCPU_ARCH
    VCPU_ARCH --> MMU_STATE
    VCPU_ARCH --> REGS
    
    KVM_X86_OPS --> STATIC_CALLS
    STATIC_CALLS --> VMX_IMPL
    
    KVM_VM --> KVM_VCPU
```

Sources: [arch/x86/include/asm/kvm_host.h:322-395](), [arch/x86/kvm/x86.c:136-146](), [arch/x86/kvm/x86.c:95-98]()

## Hardware Virtualization Support

### VMX Integration and Hardware Abstraction

The x86 KVM implementation provides hardware abstraction through the `kvm_x86_ops` structure, which defines function pointers for hardware-specific operations:

| Operation Category | Key Functions | Purpose |
|-------------------|---------------|---------|
| vCPU Management | `vcpu_create`, `vcpu_free`, `vcpu_reset` | vCPU lifecycle management |
| VM Control | `vm_init`, `vm_destroy`, `vm_alloc`, `vm_free` | VM lifecycle management |
| Register Access | `get_cpl`, `cache_reg`, `set_cr0`, `set_cr4` | Control register management |
| Memory Management | `set_cr3`, `flush_tlb_*`, `get_pdptr` | Memory virtualization |
| Interrupt Handling | `queue_exception`, `cancel_injection` | Exception and interrupt delivery |

The hardware abstraction uses static calls for performance, defined in [arch/x86/kvm/x86.c:138-146]():

```c
#define KVM_X86_OP(func) \
    DEFINE_STATIC_CALL_NULL(kvm_x86_##func, \
                *(((struct kvm_x86_ops *)0)->func));
```

Sources: [arch/x86/kvm/x86.c:138-146](), [arch/x86/include/asm/kvm_host.h:1440-1652]()

### Control Register Management

The implementation handles x86 control registers through dedicated functions that validate and manage state transitions:

- **CR0 Management**: [arch/x86/kvm/x86.c:929-1010]() - Handles paging enable/disable, protection mode changes
- **CR4 Management**: [arch/x86/kvm/x86.c:1150-1221]() - Manages advanced processor features like PAE, SMEP, SMAP
- **Extended Features**: [arch/x86/kvm/x86.c:1074-1130]() - XSETBV emulation for extended state management

Sources: [arch/x86/kvm/x86.c:913-1010](), [arch/x86/kvm/x86.c:1132-1221]()

## Memory Management Architecture

### Shadow MMU vs. TDP MMU

KVM x86 supports two primary memory management approaches:

#### Shadow MMU
Traditional approach that intercepts guest page table modifications:
- **Page Fault Handling**: [arch/x86/kvm/mmu/mmu.c:4421-4450]()
- **Shadow Page Management**: [arch/x86/kvm/mmu/mmu.c:2850-3100]()  
- **Write Protection**: [arch/x86/kvm/mmu/page_track.c:77-95]()

#### TDP MMU (Two Dimensional Paging)
Hardware-assisted approach using Intel EPT or AMD NPT:
- **Root Management**: [arch/x86/kvm/mmu/tdp_mmu.c:220-256]()
- **SPTE Handling**: [arch/x86/kvm/mmu/tdp_mmu.c:419-557]()
- **Lockless Walking**: [arch/x86/kvm/mmu/tdp_mmu.c:649-681]()

```mermaid
graph TB
    subgraph "Guest Memory Access"
        GVA["Guest Virtual Address"]
        GPA["Guest Physical Address"]
        HPA["Host Physical Address"]
    end
    
    subgraph "Shadow MMU Path"
        GUEST_PT["Guest Page Tables"]
        SHADOW_PT["Shadow Page Tables"]
        MMU_SYNC["mmu_sync"]
    end
    
    subgraph "TDP MMU Path"  
        HARDWARE["Hardware Translation<br/>(EPT/NPT)"]
        TDP_PT["TDP Page Tables"]
        DIRECT["Direct Mapping"]
    end
    
    GVA --> GPA
    
    GPA --> GUEST_PT
    GUEST_PT --> SHADOW_PT
    SHADOW_PT --> MMU_SYNC
    MMU_SYNC --> HPA
    
    GPA --> HARDWARE
    HARDWARE --> TDP_PT
    TDP_PT --> DIRECT
    DIRECT --> HPA
```

Sources: [arch/x86/kvm/mmu/mmu.c:100-120](), [arch/x86/kvm/mmu/tdp_mmu.c:14-52]()

### Page Tracking and Write Protection

The page tracking subsystem monitors guest memory access patterns:

- **Write Tracking**: [arch/x86/kvm/mmu/page_track.c:62-95]() - Tracks writes to specific guest pages  
- **External Tracking**: [arch/x86/include/asm/kvm_page_track.h:7-49]() - Support for external memory tracking
- **Notification System**: [arch/x86/kvm/mmu/page_track.c:180-220]() - Notifies subscribers of memory events

Sources: [arch/x86/kvm/mmu/page_track.c:23-95](), [arch/x86/include/asm/kvm_page_track.h:1-49]()

## vCPU Lifecycle and State Management

### vCPU Creation and Initialization

The vCPU lifecycle involves several phases of initialization and state setup:

```mermaid
stateDiagram-v2
    [*] --> vcpu_create: "kvm_arch_vcpu_create()"
    vcpu_create --> mmu_init: "kvm_init_mmu()"
    mmu_init --> register_init: "Initialize registers"
    register_init --> pmu_init: "PMU initialization"
    pmu_init --> ready: "vCPU ready"
    
    ready --> running: "vcpu_run()"
    running --> exit: "VM exit"
    exit --> running: "Handle exit, resume"
    exit --> destroy: "vcpu_destroy()"
    
    destroy --> [*]
```

Key initialization functions:
- **MMU Setup**: [arch/x86/kvm/mmu/mmu.c:5500-5600]() - Initialize memory management
- **Register State**: [arch/x86/kvm/x86.c:12000-12100]() - Set up initial register values
- **PMU Configuration**: [arch/x86/kvm/pmu.c:300-400]() - Performance monitoring setup

Sources: [arch/x86/kvm/x86.c:11500-12500](), [arch/x86/kvm/mmu/mmu.c:5483-5642]()

### Exception and Interrupt Handling

Exception handling is managed through a queuing system that respects x86 exception prioritization:

- **Exception Queuing**: [arch/x86/kvm/x86.c:646-729]() - Queue exceptions with proper prioritization
- **Double Fault Logic**: [arch/x86/kvm/x86.c:706-729]() - Handle exception collisions  
- **Payload Delivery**: [arch/x86/kvm/x86.c:573-622]() - Deliver exception payloads (CR2, DR6)

Exception types handled:
- Page faults with error codes and CR2 values
- Debug exceptions with DR6 state updates  
- General protection faults
- Double fault synthesis for exception collisions

Sources: [arch/x86/kvm/x86.c:573-735](), [arch/x86/kvm/x86.c:779-816]()

## Performance Monitoring Unit (PMU) Support

### Intel PMU Implementation

The x86 KVM implementation includes comprehensive PMU virtualization:

```mermaid
graph LR
    subgraph "PMU Components"
        GP_COUNTERS["General Purpose<br/>Counters (8)"]
        FIXED_COUNTERS["Fixed Function<br/>Counters (3)"] 
        EVENT_SELECT["Event Selection<br/>MSRs"]
    end
    
    subgraph "Host Integration"
        PERF_EVENTS["Linux perf_events"]
        HW_COUNTERS["Hardware Counters"]
        PERF_SCHED["perf Scheduler"]
    end
    
    subgraph "Guest Interface"
        RDPMC["RDPMC Instruction"]
        PMC_MSRS["PMC MSRs"]
        PERFMON_MSRS["PERFMON MSRs"]
    end
    
    GP_COUNTERS --> PERF_EVENTS
    FIXED_COUNTERS --> PERF_EVENTS
    EVENT_SELECT --> PERF_EVENTS
    
    PERF_EVENTS --> HW_COUNTERS
    HW_COUNTERS --> PERF_SCHED
    
    RDPMC --> PMC_MSRS
    PMC_MSRS --> PERFMON_MSRS
```

Key PMU features:
- **Architectural Events**: [arch/x86/kvm/vmx/pmu_intel.c:25-70]() - Standard performance events
- **Counter Virtualization**: [arch/x86/kvm/pmu.c:200-300]() - Hardware counter multiplexing
- **RDPMC Emulation**: [arch/x86/kvm/vmx/pmu_intel.c:128-159]() - Fast counter reads

Sources: [arch/x86/kvm/pmu.c:29-800](), [arch/x86/kvm/vmx/pmu_intel.c:1-650]()

## Integration Points and External Interfaces

### Hypervisor Integration Support

The x86 KVM implementation includes support for various hypervisor interfaces:

- **Xen Emulation**: [arch/x86/kvm/xen.c:1-2000]() - Xen hypercall and event channel emulation
- **Hyper-V Enlightenments**: [arch/x86/kvm/hyperv.c]() - Microsoft Hyper-V compatibility features
- **VMware Backdoor**: [arch/x86/kvm/x86.c:176-178]() - VMware guest tools support

### Memory Caching and Performance

The implementation includes sophisticated caching mechanisms:

- **GFN to PFN Cache**: [virt/kvm/pfncache.c:1-500]() - Guest-to-host page number caching
- **Memory Cache Management**: [virt/kvm/kvm_main.c:402-482]() - Object allocation caching
- **User Return MSRs**: [arch/x86/kvm/x86.c:208-473]() - Deferred MSR restoration

Sources: [arch/x86/kvm/x86.c:208-473](), [virt/kvm/pfncache.c:25-50]()

### Statistics and Debugging

Comprehensive statistics collection enables monitoring and debugging:

- **VM Statistics**: [arch/x86/kvm/x86.c:244-268]() - Per-VM performance metrics
- **vCPU Statistics**: [arch/x86/kvm/x86.c:270-315]() - Per-vCPU event counters  
- **MMU Statistics**: [arch/x86/kvm/mmu/mmu.c:244-315]() - Memory management metrics

The statistics cover areas including:
- Page fault handling and resolution
- TLB flush operations and efficiency  
- Exception injection and handling
- Nested virtualization events
- PMU counter programming and overflow

Sources: [arch/x86/kvm/x86.c:244-315](), [arch/x86/kvm/mmu/mmu.c:180-315]()1a:T7339,# KVM ARM64 Implementation

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [Documentation/virt/kvm/api.rst](Documentation/virt/kvm/api.rst)
- [Documentation/virt/kvm/devices/vfio.rst](Documentation/virt/kvm/devices/vfio.rst)
- [Documentation/virt/kvm/devices/vm.rst](Documentation/virt/kvm/devices/vm.rst)
- [arch/arm64/include/asm/acpi.h](arch/arm64/include/asm/acpi.h)
- [arch/arm64/include/asm/assembler.h](arch/arm64/include/asm/assembler.h)
- [arch/arm64/include/asm/el2_setup.h](arch/arm64/include/asm/el2_setup.h)
- [arch/arm64/include/asm/kvm_arm.h](arch/arm64/include/asm/kvm_arm.h)
- [arch/arm64/include/asm/kvm_emulate.h](arch/arm64/include/asm/kvm_emulate.h)
- [arch/arm64/include/asm/kvm_host.h](arch/arm64/include/asm/kvm_host.h)
- [arch/arm64/include/asm/kvm_hyp.h](arch/arm64/include/asm/kvm_hyp.h)
- [arch/arm64/include/asm/kvm_nested.h](arch/arm64/include/asm/kvm_nested.h)
- [arch/arm64/include/asm/kvm_ras.h](arch/arm64/include/asm/kvm_ras.h)
- [arch/arm64/include/asm/pgtable-hwdef.h](arch/arm64/include/asm/pgtable-hwdef.h)
- [arch/arm64/include/asm/pgtable-prot.h](arch/arm64/include/asm/pgtable-prot.h)
- [arch/arm64/include/asm/sysreg.h](arch/arm64/include/asm/sysreg.h)
- [arch/arm64/include/uapi/asm/kvm.h](arch/arm64/include/uapi/asm/kvm.h)
- [arch/arm64/kvm/Makefile](arch/arm64/kvm/Makefile)
- [arch/arm64/kvm/arch_timer.c](arch/arm64/kvm/arch_timer.c)
- [arch/arm64/kvm/arm.c](arch/arm64/kvm/arm.c)
- [arch/arm64/kvm/debug.c](arch/arm64/kvm/debug.c)
- [arch/arm64/kvm/emulate-nested.c](arch/arm64/kvm/emulate-nested.c)
- [arch/arm64/kvm/fpsimd.c](arch/arm64/kvm/fpsimd.c)
- [arch/arm64/kvm/handle_exit.c](arch/arm64/kvm/handle_exit.c)
- [arch/arm64/kvm/hyp/entry.S](arch/arm64/kvm/hyp/entry.S)
- [arch/arm64/kvm/hyp/include/hyp/switch.h](arch/arm64/kvm/hyp/include/hyp/switch.h)
- [arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h](arch/arm64/kvm/hyp/include/hyp/sysreg-sr.h)
- [arch/arm64/kvm/hyp/include/nvhe/fixed_config.h](arch/arm64/kvm/hyp/include/nvhe/fixed_config.h)
- [arch/arm64/kvm/hyp/nvhe/debug-sr.c](arch/arm64/kvm/hyp/nvhe/debug-sr.c)
- [arch/arm64/kvm/hyp/nvhe/hyp-main.c](arch/arm64/kvm/hyp/nvhe/hyp-main.c)
- [arch/arm64/kvm/hyp/nvhe/pkvm.c](arch/arm64/kvm/hyp/nvhe/pkvm.c)
- [arch/arm64/kvm/hyp/nvhe/switch.c](arch/arm64/kvm/hyp/nvhe/switch.c)
- [arch/arm64/kvm/hyp/nvhe/sys_regs.c](arch/arm64/kvm/hyp/nvhe/sys_regs.c)
- [arch/arm64/kvm/hyp/nvhe/timer-sr.c](arch/arm64/kvm/hyp/nvhe/timer-sr.c)
- [arch/arm64/kvm/hyp/vgic-v3-sr.c](arch/arm64/kvm/hyp/vgic-v3-sr.c)
- [arch/arm64/kvm/hyp/vhe/switch.c](arch/arm64/kvm/hyp/vhe/switch.c)
- [arch/arm64/kvm/hypercalls.c](arch/arm64/kvm/hypercalls.c)
- [arch/arm64/kvm/inject_fault.c](arch/arm64/kvm/inject_fault.c)
- [arch/arm64/kvm/nested.c](arch/arm64/kvm/nested.c)
- [arch/arm64/kvm/pmu-emul.c](arch/arm64/kvm/pmu-emul.c)
- [arch/arm64/kvm/reset.c](arch/arm64/kvm/reset.c)
- [arch/arm64/kvm/sys_regs.c](arch/arm64/kvm/sys_regs.c)
- [arch/arm64/kvm/sys_regs.h](arch/arm64/kvm/sys_regs.h)
- [arch/arm64/kvm/trace_arm.h](arch/arm64/kvm/trace_arm.h)
- [arch/arm64/tools/sysreg](arch/arm64/tools/sysreg)
- [arch/powerpc/kvm/powerpc.c](arch/powerpc/kvm/powerpc.c)
- [arch/s390/include/asm/kvm_host.h](arch/s390/include/asm/kvm_host.h)
- [arch/s390/include/uapi/asm/kvm.h](arch/s390/include/uapi/asm/kvm.h)
- [arch/s390/kvm/gaccess.c](arch/s390/kvm/gaccess.c)
- [arch/s390/kvm/gaccess.h](arch/s390/kvm/gaccess.h)
- [arch/s390/kvm/kvm-s390.c](arch/s390/kvm/kvm-s390.c)
- [arch/s390/kvm/kvm-s390.h](arch/s390/kvm/kvm-s390.h)
- [arch/s390/pci/Makefile](arch/s390/pci/Makefile)
- [arch/s390/pci/pci_kvm_hook.c](arch/s390/pci/pci_kvm_hook.c)
- [drivers/acpi/processor_idle.c](drivers/acpi/processor_idle.c)
- [drivers/acpi/utils.c](drivers/acpi/utils.c)
- [drivers/hwtracing/coresight/coresight-trbe.c](drivers/hwtracing/coresight/coresight-trbe.c)
- [drivers/hwtracing/coresight/coresight-trbe.h](drivers/hwtracing/coresight/coresight-trbe.h)
- [drivers/perf/apple_m1_cpu_pmu.c](drivers/perf/apple_m1_cpu_pmu.c)
- [drivers/perf/arm_pmu.c](drivers/perf/arm_pmu.c)
- [drivers/perf/arm_pmu_acpi.c](drivers/perf/arm_pmu_acpi.c)
- [drivers/vfio/pci/vfio_pci_zdev.c](drivers/vfio/pci/vfio_pci_zdev.c)
- [include/kvm/arm_arch_timer.h](include/kvm/arm_arch_timer.h)
- [include/kvm/arm_hypercalls.h](include/kvm/arm_hypercalls.h)
- [include/kvm/arm_pmu.h](include/kvm/arm_pmu.h)
- [include/linux/perf/arm_pmu.h](include/linux/perf/arm_pmu.h)
- [include/uapi/linux/kvm.h](include/uapi/linux/kvm.h)
- [tools/arch/arm64/include/uapi/asm/kvm.h](tools/arch/arm64/include/uapi/asm/kvm.h)
- [tools/arch/x86/include/uapi/asm/kvm.h](tools/arch/x86/include/uapi/asm/kvm.h)
- [tools/include/uapi/linux/kvm.h](tools/include/uapi/linux/kvm.h)
- [virt/kvm/vfio.c](virt/kvm/vfio.c)

</details>



This page documents the ARM64 implementation of the Kernel-based Virtual Machine (KVM) subsystem in the Linux kernel. It describes how KVM leverages ARM64 hardware virtualization features to enable efficient virtualization on ARM64 platforms, with focus on the specific code structures and implementation details. For x86-specific KVM implementation details, see [KVM x86 Implementation](2.1).

## ARM64 Virtualization Architecture

ARM64 processors include hardware virtualization extensions that allow a hypervisor to efficiently run guest operating systems. The KVM ARM64 implementation leverages these hardware features to provide virtualization capabilities.

### Exception Levels

ARM64 architecture defines multiple privilege levels called Exception Levels (EL0-EL3):

```mermaid
flowchart TD
    subgraph "Privilege Hierarchy"
        EL3["EL3: Secure Monitor"]
        EL2["EL2: Hypervisor"]
        EL1["EL1: OS Kernel"]
        EL0["EL0: Applications"]
        
        EL3 --> EL2
        EL2 --> EL1
        EL1 --> EL0
    end
```

- **EL0**: User applications
- **EL1**: Operating system kernel
- **EL2**: Hypervisor (where KVM runs)
- **EL3**: Secure monitor

KVM runs at EL2 (the hypervisor exception level) and guest operating systems run at EL1/EL0. The ARM64 virtualization extensions allow efficient transitions between these exception levels.

Sources: [arch/arm64/include/asm/kvm_host.h](), [arch/arm64/include/asm/kvm_arm.h](), [arch/arm64/include/asm/kvm_emulate.h]()

### Implementation Modes

The ARM64 KVM implementation supports different operational modes defined by the `enum kvm_mode` in `kvm_host.h`:

```mermaid
flowchart TD
    subgraph KVM_Modes["KVM Operation Modes"]
        kvm_mode_default["KVM_MODE_DEFAULT"]
        kvm_mode_protected["KVM_MODE_PROTECTED"]
        kvm_mode_nv["KVM_MODE_NV"]
        kvm_mode_none["KVM_MODE_NONE"]
    end
    
    subgraph Implementation["Implementation Details"]
        has_vhe["has_vhe()"]
        kvm_nvhe["kvm_nvhe switch"]
        kvm_vhe["kvm_vhe switch"]
        pkvm["Protected KVM"]
    end
    
    kvm_mode_default --> has_vhe
    has_vhe --> |"true"| kvm_vhe
    has_vhe --> |"false"| kvm_nvhe
    kvm_mode_protected --> pkvm
```

- **KVM_MODE_DEFAULT**: Standard virtualization mode, uses VHE if available
- **KVM_MODE_PROTECTED**: Isolated hypervisor with `pkvm_handle_t` for VM management
- **KVM_MODE_NV**: Nested virtualization mode
- **VHE vs nVHE**: Determined by `has_vhe()`, affects hypervisor context switching implementation

Sources: [arch/arm64/include/asm/kvm_host.h:63-68](), [arch/arm64/kvm/arm.c:48](), [arch/arm64/kvm/hyp/nvhe/switch.c](), [arch/arm64/kvm/hyp/vhe/switch.c]()

## System Register Virtualization

ARM64 KVM implements comprehensive system register virtualization through the `sys_regs.c` subsystem. The implementation handles register trapping, emulation, and context switching using automated tooling and runtime handlers.

### System Register Encoding and Metadata

ARM64 system registers use a standardized encoding scheme implemented in `sysreg.h`:

System Register Encoding Format
```mermaid
flowchart TD
    subgraph encoding["System Register Encoding"]
        sys_reg_macro["sys_reg(op0, op1, crn, crm, op2)"]
        Op0_shift["Op0_shift = 19"]
        Op1_shift["Op1_shift = 16"] 
        CRn_shift["CRn_shift = 12"]
        CRm_shift["CRm_shift = 8"]
        Op2_shift["Op2_shift = 5"]
    end
    
    subgraph tooling["Automated Register Generation"]
        sysreg_tool["arch/arm64/tools/sysreg"]
        sysreg_defs["sysreg-defs.h"]
        field_extraction["SYS_FIELD_GET/PREP macros"]
    end
    
    sys_reg_macro --> sysreg_tool
    sysreg_tool --> sysreg_defs
    sysreg_defs --> field_extraction
```

The `tools/sysreg` file defines register metadata with structured blocks:
- `Sysreg` blocks define register encoding and fields
- `Field`, `Enum`, `Res0`, `Res1` define field properties  
- Generates accessor macros and validation code

Sources: [arch/arm64/include/asm/sysreg.h:28-50](), [arch/arm64/tools/sysreg:1-50](), [arch/arm64/include/asm/sysreg.h:164]()

### Register Access Control and Trapping

KVM implements register access control through multiple trap configuration registers and runtime handlers:

Trap Configuration Architecture
```mermaid
flowchart TD
    subgraph trap_config["Trap Configuration Registers"]
        HCR_EL2["HCR_EL2<br/>Hypervisor Configuration"]
        CPTR_EL2["CPTR_EL2<br/>Architectural Feature Trap"]
        HSTR_EL2["HSTR_EL2<br/>Hypervisor System Trap"]
        HFGRTR_EL2["HFGRTR_EL2<br/>Fine Grained Read Trap"]
        HFGWTR_EL2["HFGWTR_EL2<br/>Fine Grained Write Trap"]
    end
    
    subgraph handlers["Access Handlers"]
        kvm_handle_sys_reg["kvm_handle_sys_reg()"]
        access_rw["access_rw()"]
        access_vm_reg["access_vm_reg()"]
        trap_raz_wi["trap_raz_wi()"]
        trap_undef["trap_undef()"]
    end
    
    subgraph vcpu_context["vCPU Context"]
        vcpu_sys_regs["__vcpu_sys_reg(vcpu, reg)"]
        vcpu_read_sys_reg["vcpu_read_sys_reg()"]
        vcpu_write_sys_reg["vcpu_write_sys_reg()"]
    end
    
    trap_config --> kvm_handle_sys_reg
    kvm_handle_sys_reg --> handlers
    handlers --> vcpu_context
```

Key trap mechanisms:
- **Fine-Grained Traps**: `__activate_traps_hfgxtr()` configures per-register trapping
- **CP15/AArch32 Traps**: `HSTR_EL2` set to `1 << 15` for impdef sysregs
- **Feature Traps**: SME, SVE, and floating-point register access control

Sources: [arch/arm64/kvm/hyp/include/hyp/switch.h:82-155](), [arch/arm64/kvm/sys_regs.c:69-87](), [arch/arm64/kvm/hyp/include/hyp/switch.h:175-184]()

### Register Emulation Implementation

The system register emulation is implemented through a table-driven approach in `sys_regs.c` with specialized handler functions:

Register Handler Architecture
```mermaid
flowchart TD
    subgraph sys_reg_table["System Register Table"]
        sys_reg_desc["struct sys_reg_desc"]
        reg_encoding["register encoding"]
        access_fn["access function pointer"]
        reset_fn["reset function pointer"]
        get_user_fn["get_user function"]
        set_user_fn["set_user function"]
    end
    
    subgraph handler_types["Handler Function Types"]
        access_rw["access_rw()<br/>Standard read/write"]
        access_vm_reg["access_vm_reg()<br/>VM control registers"]
        access_gic_sgi["access_gic_sgi()<br/>GIC SGI generation"]
        access_dcsw["access_dcsw()<br/>Cache operations"]
        trap_raz_wi["trap_raz_wi()<br/>Read-as-zero/Write-ignore"]
        set_id_reg["set_id_reg()<br/>ID register validation"]
    end
    
    subgraph vcpu_storage["vCPU Register Storage"]
        NR_SYS_REGS["NR_SYS_REGS = 414"]
        sys_regs_array["vcpu->arch.ctxt.sys_regs[]"]
        ccsidr_array["vcpu->arch.ccsidr[]"]
        id_regs_array["kvm->arch.id_regs[]"]
    end
    
    sys_reg_table --> handler_types
    handler_types --> vcpu_storage
```

Specialized register categories:
- **ID registers**: VM-wide emulated CPU identification with `IDREG(kvm, id)` macro
- **Cache registers**: `get_ccsidr()` and `set_ccsidr()` for cache geometry emulation
- **Debug registers**: Separate host/guest debug state in `kvm_guest_debug_arch`
- **PMU registers**: Performance counter emulation via `kvm_pmu_*` functions

Sources: [arch/arm64/kvm/sys_regs.c:45-56](), [arch/arm64/kvm/sys_regs.c:182-192](), [arch/arm64/include/asm/kvm_host.h:271-274](), [arch/arm64/include/asm/kvm_host.h:414]()

## Memory Management

ARM64 KVM implements two-stage address translation through the `kvm_s2_mmu` structure and stage-2 page table management.

### Two-Stage Translation Implementation

Stage-2 Memory Management Unit
```mermaid
flowchart TD
    subgraph kvm_s2_mmu["struct kvm_s2_mmu"]
        kvm_vmid["struct kvm_vmid vmid"]
        pgd_phys["phys_addr_t pgd_phys"]
        kvm_pgtable["struct kvm_pgtable *pgt"]
        last_vcpu_ran["int __percpu *last_vcpu_ran"]
        split_page_cache["struct kvm_mmu_memory_cache"]
    end
    
    subgraph translation_control["Translation Control"]
        vtcr["kvm->arch.vtcr"]
        vttbr_el2["VTTBR_EL2 register"]
        vtcr_el2["VTCR_EL2 register"]
    end
    
    subgraph address_translation["Address Translation Flow"]
        gva["Guest Virtual Address"]
        gpa["Guest Physical Address"] 
        hpa["Host Physical Address"]
        stage1["Stage 1: Guest page tables"]
        stage2["Stage 2: kvm_pgtable"]
    end
    
    gva --> stage1
    stage1 --> gpa
    gpa --> stage2
    stage2 --> hpa
    
    kvm_s2_mmu --> translation_control
    translation_control --> stage2
```

Key implementation components:
- **VMID management**: `atomic64_t id` for TLB isolation between VMs
- **Page table structure**: `kvm_pgtable` with hardware page table at `pgd_phys`
- **Eager page splitting**: `split_page_cache` for `KVM_CAP_ARM_EAGER_SPLIT_CHUNK_SIZE`
- **CPU tracking**: `last_vcpu_ran` per-CPU array for TLB/I-cache invalidation

Sources: [arch/arm64/include/asm/kvm_host.h:144-179](), [arch/arm64/kvm/arm.c:163](), [arch/arm64/kvm/arm.c:420-440]()

## Exception and Interrupt Handling

KVM ARM64 implements exception handling through a table-driven dispatch system in `handle_exit.c` with specific handlers for each exception class.

### Exception Handler Dispatch

Exception Handler Architecture
```mermaid
flowchart TD
    subgraph vm_exit["VM Exit Reasons"]
        ESR_ELx_EC_SVC64["ESR_ELx_EC_SVC64<br/>handle_svc()"]
        ESR_ELx_EC_HVC64["ESR_ELx_EC_HVC64<br/>handle_hvc()"]
        ESR_ELx_EC_SMC64["ESR_ELx_EC_SMC64<br/>handle_smc()"]
        ESR_ELx_EC_SYS64["ESR_ELx_EC_SYS64<br/>kvm_handle_sys_reg()"]
        ESR_ELx_EC_SVE["ESR_ELx_EC_SVE<br/>handle_sve()"]
        ESR_ELx_EC_IABT_LOW["ESR_ELx_EC_IABT_LOW<br/>kvm_handle_guest_abort()"]
        ESR_ELx_EC_DABT_LOW["ESR_ELx_EC_DABT_LOW<br/>kvm_handle_guest_abort()"]
    end
    
    subgraph handler_functions["Handler Functions"]
        handle_hvc["handle_hvc()<br/>Hypervisor calls"]
        handle_smc["handle_smc()<br/>Secure monitor calls"] 
        kvm_handle_wfx["kvm_handle_wfx()<br/>WFI/WFE instructions"]
        kvm_smccc_call_handler["kvm_smccc_call_handler()<br/>SMCCC dispatch"]
        kvm_inject_nested_sync["kvm_inject_nested_sync()<br/>Nested virtualization"]
    end
    
    subgraph fault_info["Fault Information"]
        kvm_vcpu_fault_info["struct kvm_vcpu_fault_info"]
        esr_el2["u64 esr_el2"]
        far_el2["u64 far_el2"] 
        hpfar_el2["u64 hpfar_el2"]
        disr_el1["u64 disr_el1"]
    end
    
    vm_exit --> handler_functions
    handler_functions --> fault_info
```

Key exception handling mechanisms:
- **Hypercall routing**: `handle_hvc()` checks for nested virtualization with `vcpu_has_nv()`
- **SMCCC compliance**: SMC instructions with nonzero immediate return `~0UL`
- **System error handling**: `kvm_handle_guest_serror()` for RAS-compliant error injection
- **Nested exceptions**: `kvm_inject_nested_sync()` for L1 hypervisor exception forwarding

Sources: [arch/arm64/kvm/handle_exit.c:31-54](), [arch/arm64/kvm/handle_exit.c:56-87](), [arch/arm64/include/asm/kvm_host.h:283-288]()

## Virtual GIC (Generic Interrupt Controller)

The Virtual GIC implementation provides hardware-accelerated interrupt virtualization through the `vgic_cpu` and `vgic_dist` structures integrated into KVM's vCPU and VM contexts.

### Virtual GIC Implementation

VGIC Architecture and Integration
```mermaid
flowchart TD
    subgraph vgic_structures["VGIC Data Structures"]
        vgic_dist["struct vgic_dist<br/>kvm->arch.vgic"]
        vgic_cpu["struct vgic_cpu<br/>vcpu->arch.vgic_cpu"]
        vgic_v3["vgic_cpu.vgic_v3"]
        vgic_sre["vgic_v3.vgic_sre"]
    end
    
    subgraph hardware_interface["Hardware Interface"]
        ich_lr_registers["ICH_LR0_EL2 - ICH_LR15_EL2<br/>List Registers"]
        ich_vmcr["ICH_VMCR_EL2<br/>Virtual Machine Control"]
        ich_hcr["ICH_HCR_EL2<br/>Hypervisor Control"]
        gic_sgi_generation["ICC_SGI1R_EL1 access<br/>vgic_v3_dispatch_sgi()"]
    end
    
    subgraph interrupt_handling["Interrupt Processing"]
        kvm_vgic_vcpu_pending_irq["kvm_vgic_vcpu_pending_irq()"]
        kvm_vgic_load["kvm_vgic_load()"]
        kvm_vgic_put["kvm_vgic_put()"]
        access_gic_sgi["access_gic_sgi()<br/>SGI emulation"]
        access_gic_sre["access_gic_sre()<br/>SRE register"]
    end
    
    vgic_structures --> hardware_interface
    hardware_interface --> interrupt_handling
```

Key VGIC implementation details:
- **Hardware List Registers**: Direct mapping to `ICH_LR*_EL2` for interrupt injection
- **SGI Generation**: `access_gic_sgi()` handles Group0/Group1 SGI dispatch via `vgic_v3_dispatch_sgi()`
- **SRE Emulation**: `access_gic_sre()` returns fixed `vgic_sre` value for GICv3 system register interface
- **Load/Put Cycles**: `kvm_vgic_load()`/`kvm_vgic_put()` manage hardware state during vCPU scheduling

Sources: [arch/arm64/kvm/sys_regs.c:299-347](), [arch/arm64/kvm/sys_regs.c:349-358](), [arch/arm64/kvm/arm.c:444](), [arch/arm64/include/asm/kvm_host.h:550-551]()

## Timer Virtualization

ARM64 KVM implements comprehensive timer virtualization through the `arch_timer_cpu` structure and hardware timer management with support for nested virtualization.

### Timer Implementation Architecture

Timer Virtualization System
```mermaid
flowchart TD
    subgraph timer_types["Timer Types"]
        TIMER_PTIMER["TIMER_PTIMER<br/>Physical Timer"]
        TIMER_VTIMER["TIMER_VTIMER<br/>Virtual Timer"] 
        TIMER_HPTIMER["TIMER_HPTIMER<br/>Hypervisor Physical"]
        TIMER_HVTIMER["TIMER_HVTIMER<br/>Hypervisor Virtual"]
    end
    
    subgraph vcpu_timers["vCPU Timer Context"]
        arch_timer_cpu["struct arch_timer_cpu<br/>vcpu->arch.timer_cpu"]
        arch_timer_context["struct arch_timer_context"]
        hrtimer["struct hrtimer"]
        host_timer_irq["host timer IRQ"]
    end
    
    subgraph timer_operations["Timer Operations"]
        kvm_timer_vcpu_load["kvm_timer_vcpu_load()"]
        kvm_timer_vcpu_put["kvm_timer_vcpu_put()"]
        kvm_timer_init_vm["kvm_timer_init_vm()"]
        arch_timer_vm_data["struct arch_timer_vm_data<br/>kvm->arch.timer_data"]
    end
    
    subgraph hardware_integration["Hardware Integration"]
        cntp_ctl_el0["CNTP_CTL_EL0"]
        cntp_cval_el0["CNTP_CVAL_EL0"]
        cntv_ctl_el0["CNTV_CTL_EL0"]
        cntv_cval_el0["CNTV_CVAL_EL0"]
        cntvoff_el2["CNTVOFF_EL2<br/>Virtual offset"]
    end
    
    timer_types --> vcpu_timers
    vcpu_timers --> timer_operations
    timer_operations --> hardware_integration
```

Key timer implementation features:
- **Default PPI Assignment**: Physical timer PPI 30, Virtual timer PPI 27, Hypervisor timers PPI 26/29
- **Timer Context Management**: Each timer type has dedicated `arch_timer_context` with control/compare values
- **Host Timer Integration**: `host_vtimer_irq` and `host_ptimer_irq` for host timer interrupt handling
- **VM-wide Timer Data**: `arch_timer_vm_data` manages timer configuration and offsets per VM

Sources: [arch/arm64/kvm/arch_timer.c:34-40](), [arch/arm64/include/asm/kvm_host.h:552](), [arch/arm64/kvm/arm.c:169](), [arch/arm64/kvm/arm.c:445]()

## ARM64-Specific Features

KVM ARM64 implements virtualization support for advanced ARM64 architectural features through dedicated subsystems and capability management.

### PMU (Performance Monitoring Unit) Virtualization

The PMU implementation provides hardware performance counter virtualization through the `kvm_pmu` structure:

PMU Virtualization Architecture
```mermaid
flowchart TD
    subgraph pmu_structures["PMU Data Structures"]
        kvm_pmu["struct kvm_pmu<br/>vcpu->arch.pmu"]
        kvm_pmc["struct kvm_pmc<br/>pmu.pmc[32]"]
        perf_event["struct perf_event"]
        overflow_work["struct irq_work"]
    end
    
    subgraph pmu_registers["PMU Register Emulation"]
        PMCR_EL0["PMCR_EL0<br/>Control Register"]
        PMEVCNTR_EL0["PMEVCNTR0_EL0-30<br/>Event Counters"]
        PMEVTYPER_EL0["PMEVTYPER0_EL0-30<br/>Event Type Registers"]
        PMCCNTR_EL0["PMCCNTR_EL0<br/>Cycle Counter"]
        PMCNTENSET_EL0["PMCNTENSET_EL0<br/>Counter Enable"]
    end
    
    subgraph pmu_operations["PMU Operations"]
        kvm_pmu_enable_counter_mask["kvm_pmu_enable_counter_mask()"]
        kvm_pmu_get_counter_value["kvm_pmu_get_counter_value()"]
        kvm_pmu_set_counter_value["kvm_pmu_set_counter_value()"]
        kvm_pmu_valid_counter_mask["kvm_pmu_valid_counter_mask()"]
        kvm_pmu_event_mask["kvm_pmu_event_mask()"]
    end
    
    pmu_structures --> pmu_registers
    pmu_registers --> pmu_operations
```

### FPSIMD and SVE Context Management

Floating-point state management through dedicated context switching:

FPSIMD/SVE State Management
```mermaid
flowchart TD
    subgraph fp_state["FP State Management"]
        fp_type["enum fp_type<br/>vcpu->arch.fp_type"]
        sve_state["void *sve_state<br/>vcpu->arch.sve_state"]
        sve_max_vl["unsigned int sve_max_vl"]
        svcr["u64 svcr"]
    end
    
    subgraph fp_operations["FP Operations"]
        kvm_arch_vcpu_load_fp["kvm_arch_vcpu_load_fp()"]
        kvm_arch_vcpu_put_fp["kvm_arch_vcpu_put_fp()"]
        kvm_arch_vcpu_run_map_fp["kvm_arch_vcpu_run_map_fp()"]
        guest_owns_fp_regs["guest_owns_fp_regs()"]
    end
    
    subgraph fp_context_states["FP Context States"]
        FP_STATE_FREE["FP_STATE_FREE"]
        FP_STATE_HOST_OWNED["FP_STATE_HOST_OWNED"]
        FP_STATE_GUEST_OWNED["FP_STATE_GUEST_OWNED"]
    end
    
    fp_state --> fp_operations
    fp_operations --> fp_context_states
```

### Memory Tagging Extension (MTE)

MTE support through capability flags and register emulation:
- **KVM_CAP_ARM_MTE**: Capability for MTE guest support
- **KVM_ARCH_FLAG_MTE_ENABLED**: VM-level MTE enablement flag
- **Cache operations**: `access_dcgsw()` handles MTE-aware cache maintenance

### Power State Coordination Interface (PSCI)

PSCI implementation through hypercall handling:
- **kvm_host_psci_config**: Host PSCI version and function ID management
- **Multi-processor management**: `__kvm_arm_vcpu_power_off()` and CPU state transitions
- **System suspend**: `KVM_ARCH_FLAG_SYSTEM_SUSPEND_ENABLED` for guest system suspend

Sources: [arch/arm64/kvm/pmu-emul.c:29-37](), [arch/arm64/kvm/fpsimd.c:39-64](), [arch/arm64/include/asm/kvm_host.h:466-482](), [arch/arm64/kvm/arm.c:86-99]()

## KVM API Extensions for ARM64

KVM implements ARM64-specific capabilities and ioctl extensions through the `kvm_vm_ioctl_check_extension()` function:

### Capability Support Matrix

ARM64 KVM Capabilities
```mermaid
flowchart TD
    subgraph core_caps["Core Capabilities"]
        KVM_CAP_IRQCHIP["KVM_CAP_IRQCHIP<br/>vgic_present"]
        KVM_CAP_ARM_PSCI["KVM_CAP_ARM_PSCI<br/>Power management"]
        KVM_CAP_ONE_REG["KVM_CAP_ONE_REG<br/>Register access"]
        KVM_CAP_ARM_VM_IPA_SIZE["KVM_CAP_ARM_VM_IPA_SIZE<br/>get_kvm_ipa_limit()"]
    end
    
    subgraph feature_caps["Feature Capabilities"]
        KVM_CAP_ARM_MTE["KVM_CAP_ARM_MTE<br/>system_supports_mte()"]
        KVM_CAP_ARM_SVE["KVM_CAP_ARM_SVE<br/>system_supports_sve()"]
        KVM_CAP_ARM_PTRAUTH["KVM_CAP_ARM_PTRAUTH_*<br/>system_has_full_ptr_auth()"]
        KVM_CAP_ARM_PMU_V3["KVM_CAP_ARM_PMU_V3<br/>kvm_arm_support_pmu_v3()"]
    end
    
    subgraph debug_caps["Debug Capabilities"]
        KVM_CAP_GUEST_DEBUG_HW_BPS["KVM_CAP_GUEST_DEBUG_HW_BPS<br/>get_num_brps()"]
        KVM_CAP_GUEST_DEBUG_HW_WPS["KVM_CAP_GUEST_DEBUG_HW_WPS<br/>get_num_wrps()"]
        KVM_CAP_SET_GUEST_DEBUG2["KVM_CAP_SET_GUEST_DEBUG2<br/>KVM_GUESTDBG_VALID_MASK"]
    end
    
    subgraph vm_caps["VM Management"]
        KVM_CAP_ARM_EAGER_SPLIT_CHUNK_SIZE["KVM_CAP_ARM_EAGER_SPLIT_CHUNK_SIZE<br/>split_page_chunk_size"]
        KVM_CAP_ARM_SUPPORTED_BLOCK_SIZES["KVM_CAP_ARM_SUPPORTED_BLOCK_SIZES<br/>kvm_supported_block_sizes()"]
        KVM_CAP_COUNTER_OFFSET["KVM_CAP_COUNTER_OFFSET<br/>Timer offset support"]
    end
    
    core_caps --> feature_caps
    feature_caps --> debug_caps
    debug_caps --> vm_caps
```

### Enable Capability Implementation

The `kvm_vm_ioctl_enable_cap()` function handles capability enablement:

| Capability | Implementation | Effect |
|------------|----------------|---------|
| `KVM_CAP_ARM_NISV_TO_USER` | `KVM_ARCH_FLAG_RETURN_NISV_IO_ABORT_TO_USER` | Return unhandled data aborts to userspace |
| `KVM_CAP_ARM_MTE` | `KVM_ARCH_FLAG_MTE_ENABLED` | Enable Memory Tagging Extension for VM |
| `KVM_CAP_ARM_SYSTEM_SUSPEND` | `KVM_ARCH_FLAG_SYSTEM_SUSPEND_ENABLED` | Allow PSCI system suspend |
| `KVM_CAP_ARM_EAGER_SPLIT_CHUNK_SIZE` | `kvm->arch.mmu.split_page_chunk_size` | Configure eager page splitting |

Sources: [arch/arm64/kvm/arm.c:214-324](), [arch/arm64/kvm/arm.c:71-124](), [arch/arm64/include/asm/kvm_host.h:228-245]()

## Guest to Host Communication

ARM64 KVM implements standardized hypercall interfaces through the SMCCC (Secure Monitor Call Calling Convention) framework with comprehensive service filtering and routing.

### SMCCC Hypercall Implementation

Hypercall Processing Architecture
```mermaid
flowchart TD
    subgraph hypercall_entry["Hypercall Entry Points"]
        handle_hvc["handle_hvc()<br/>Hypervisor calls"]
        handle_smc["handle_smc()<br/>Secure monitor calls"]
        kvm_vcpu_hvc_get_imm["kvm_vcpu_hvc_get_imm()<br/>Immediate value check"]
    end
    
    subgraph smccc_framework["SMCCC Framework"]
        kvm_smccc_call_handler["kvm_smccc_call_handler()"]
        kvm_smccc_features["struct kvm_smccc_features"]
        std_bmap["std_bmap"]
        std_hyp_bmap["std_hyp_bmap"] 
        vendor_hyp_bmap["vendor_hyp_bmap"]
    end
    
    subgraph hypercall_services["Hypercall Services"]
        kvm_arm_init_hypercalls["kvm_arm_init_hypercalls()"]
        kvm_arm_teardown_hypercalls["kvm_arm_teardown_hypercalls()"]
        smccc_filter["maple_tree smccc_filter"]
        KVM_ARCH_FLAG_SMCCC_FILTER_CONFIGURED["SMCCC_FILTER_CONFIGURED"]
    end
    
    subgraph nested_handling["Nested Virtualization"]
        vcpu_has_nv["vcpu_has_nv(vcpu)"]
        kvm_inject_nested_sync["kvm_inject_nested_sync()"]
        is_hyp_ctxt["is_hyp_ctxt(vcpu)"]
    end
    
    hypercall_entry --> smccc_framework
    smccc_framework --> hypercall_services
    hypercall_entry --> nested_handling
```

### Power State Coordination Interface (PSCI)

PSCI implementation integrates with the SMCCC framework for standardized power management:

PSCI Integration Points
```mermaid
flowchart TD
    subgraph psci_config["PSCI Configuration"]
        kvm_host_psci_config["struct kvm_host_psci_config"]
        psci_version["u32 version"]
        smccc_version["u32 smccc_version"]
        psci_0_1_function_ids["struct psci_0_1_function_ids"]
    end
    
    subgraph vcpu_power_management["vCPU Power Management"]
        kvm_arm_vcpu_power_off["kvm_arm_vcpu_power_off()"]
        kvm_arm_vcpu_stopped["kvm_arm_vcpu_stopped()"]
        kvm_arm_vcpu_suspend["kvm_arm_vcpu_suspend()"]
        mp_state_lock["spin_lock mp_state_lock"]
    end
    
    subgraph mp_states["MP States"]
        KVM_MP_STATE_RUNNABLE["KVM_MP_STATE_RUNNABLE"]
        KVM_MP_STATE_STOPPED["KVM_MP_STATE_STOPPED"]
        KVM_MP_STATE_SUSPENDED["KVM_MP_STATE_SUSPENDED"]
    end
    
    psci_config --> vcpu_power_management
    vcpu_power_management --> mp_states
```

Key implementation features:
- **Hypercall Filtering**: `smccc_filter` maple tree for per-VM hypercall access control
- **Immediate Value Validation**: SMC with nonzero immediate returns `~0UL` per SMCCC specification
- **Nested HVC Routing**: `vcpu_has_nv()` check routes HVC to L1 hypervisor when appropriate
- **PSCI Power Management**: Integration with vCPU `mp_state` for CPU on/off/suspend operations

Sources: [arch/arm64/kvm/handle_exit.c:37-87](), [arch/arm64/kvm/hypercalls.c](), [arch/arm64/include/asm/kvm_host.h:191-195](), [arch/arm64/kvm/arm.c:481-543]()

## Nested Virtualization

ARM64 KVM supports nested virtualization, allowing a guest hypervisor to run its own guests:

```mermaid
flowchart TD
    subgraph "Nested Virtualization"
        L0["L0: Host Hypervisor<br>(KVM)"]
        L1["L1: Guest Hypervisor"]
        L2["L2: Nested Guest"]
        
        L0 --> |"Virtualizes"| L1
        L1 --> |"Virtualizes"| L2
    end
```

This is implemented by emulating the virtualization hardware features for the L1 hypervisor, including:
- Emulating EL2 registers
- Handling nested traps
- Two-level address translation

Sources: [arch/arm64/kvm/emulate-nested.c](), [arch/arm64/include/asm/kvm_nested.h]()1b:T3b8e,# AMD SVM Implementation

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [Documentation/admin-guide/hw-vuln/gather_data_sampling.rst](Documentation/admin-guide/hw-vuln/gather_data_sampling.rst)
- [Documentation/admin-guide/hw-vuln/spectre.rst](Documentation/admin-guide/hw-vuln/spectre.rst)
- [Documentation/admin-guide/hw-vuln/srso.rst](Documentation/admin-guide/hw-vuln/srso.rst)
- [arch/x86/events/intel/pt.c](arch/x86/events/intel/pt.c)
- [arch/x86/include/asm/asm-prototypes.h](arch/x86/include/asm/asm-prototypes.h)
- [arch/x86/include/asm/cpufeatures.h](arch/x86/include/asm/cpufeatures.h)
- [arch/x86/include/asm/intel_pt.h](arch/x86/include/asm/intel_pt.h)
- [arch/x86/include/asm/kvm-x86-ops.h](arch/x86/include/asm/kvm-x86-ops.h)
- [arch/x86/include/asm/msr-index.h](arch/x86/include/asm/msr-index.h)
- [arch/x86/include/asm/nops.h](arch/x86/include/asm/nops.h)
- [arch/x86/include/asm/nospec-branch.h](arch/x86/include/asm/nospec-branch.h)
- [arch/x86/include/asm/page.h](arch/x86/include/asm/page.h)
- [arch/x86/include/asm/processor.h](arch/x86/include/asm/processor.h)
- [arch/x86/include/asm/required-features.h](arch/x86/include/asm/required-features.h)
- [arch/x86/include/asm/smap.h](arch/x86/include/asm/smap.h)
- [arch/x86/include/asm/svm.h](arch/x86/include/asm/svm.h)
- [arch/x86/include/asm/vmx.h](arch/x86/include/asm/vmx.h)
- [arch/x86/kernel/alternative.c](arch/x86/kernel/alternative.c)
- [arch/x86/kernel/asm-offsets.c](arch/x86/kernel/asm-offsets.c)
- [arch/x86/kernel/cpu/amd.c](arch/x86/kernel/cpu/amd.c)
- [arch/x86/kernel/cpu/bugs.c](arch/x86/kernel/cpu/bugs.c)
- [arch/x86/kernel/cpu/common.c](arch/x86/kernel/cpu/common.c)
- [arch/x86/kernel/cpu/cpu.h](arch/x86/kernel/cpu/cpu.h)
- [arch/x86/kernel/cpu/cpuid-deps.c](arch/x86/kernel/cpu/cpuid-deps.c)
- [arch/x86/kernel/cpu/hygon.c](arch/x86/kernel/cpu/hygon.c)
- [arch/x86/kernel/cpu/scattered.c](arch/x86/kernel/cpu/scattered.c)
- [arch/x86/kernel/cpu/tsx.c](arch/x86/kernel/cpu/tsx.c)
- [arch/x86/kernel/vmlinux.lds.S](arch/x86/kernel/vmlinux.lds.S)
- [arch/x86/kvm/.gitignore](arch/x86/kvm/.gitignore)
- [arch/x86/kvm/Makefile](arch/x86/kvm/Makefile)
- [arch/x86/kvm/cpuid.c](arch/x86/kvm/cpuid.c)
- [arch/x86/kvm/cpuid.h](arch/x86/kvm/cpuid.h)
- [arch/x86/kvm/governed_features.h](arch/x86/kvm/governed_features.h)
- [arch/x86/kvm/kvm-asm-offsets.c](arch/x86/kvm/kvm-asm-offsets.c)
- [arch/x86/kvm/kvm_cache_regs.h](arch/x86/kvm/kvm_cache_regs.h)
- [arch/x86/kvm/kvm_onhyperv.c](arch/x86/kvm/kvm_onhyperv.c)
- [arch/x86/kvm/kvm_onhyperv.h](arch/x86/kvm/kvm_onhyperv.h)
- [arch/x86/kvm/mmu.h](arch/x86/kvm/mmu.h)
- [arch/x86/kvm/mtrr.c](arch/x86/kvm/mtrr.c)
- [arch/x86/kvm/reverse_cpuid.h](arch/x86/kvm/reverse_cpuid.h)
- [arch/x86/kvm/svm/avic.c](arch/x86/kvm/svm/avic.c)
- [arch/x86/kvm/svm/nested.c](arch/x86/kvm/svm/nested.c)
- [arch/x86/kvm/svm/sev.c](arch/x86/kvm/svm/sev.c)
- [arch/x86/kvm/svm/svm.c](arch/x86/kvm/svm/svm.c)
- [arch/x86/kvm/svm/svm.h](arch/x86/kvm/svm/svm.h)
- [arch/x86/kvm/svm/svm_onhyperv.h](arch/x86/kvm/svm/svm_onhyperv.h)
- [arch/x86/kvm/svm/svm_ops.h](arch/x86/kvm/svm/svm_ops.h)
- [arch/x86/kvm/svm/vmenter.S](arch/x86/kvm/svm/vmenter.S)
- [arch/x86/kvm/vmx/capabilities.h](arch/x86/kvm/vmx/capabilities.h)
- [arch/x86/kvm/vmx/hyperv.c](arch/x86/kvm/vmx/hyperv.c)
- [arch/x86/kvm/vmx/hyperv.h](arch/x86/kvm/vmx/hyperv.h)
- [arch/x86/kvm/vmx/nested.c](arch/x86/kvm/vmx/nested.c)
- [arch/x86/kvm/vmx/nested.h](arch/x86/kvm/vmx/nested.h)
- [arch/x86/kvm/vmx/run_flags.h](arch/x86/kvm/vmx/run_flags.h)
- [arch/x86/kvm/vmx/vmcs.h](arch/x86/kvm/vmx/vmcs.h)
- [arch/x86/kvm/vmx/vmenter.S](arch/x86/kvm/vmx/vmenter.S)
- [arch/x86/kvm/vmx/vmx.c](arch/x86/kvm/vmx/vmx.c)
- [arch/x86/kvm/vmx/vmx.h](arch/x86/kvm/vmx/vmx.h)
- [arch/x86/kvm/vmx/vmx_ops.h](arch/x86/kvm/vmx/vmx_ops.h)
- [arch/x86/kvm/x86.h](arch/x86/kvm/x86.h)
- [arch/x86/lib/retpoline.S](arch/x86/lib/retpoline.S)
- [arch/x86/mm/maccess.c](arch/x86/mm/maccess.c)
- [tools/arch/x86/include/asm/cpufeatures.h](tools/arch/x86/include/asm/cpufeatures.h)
- [tools/arch/x86/include/asm/disabled-features.h](tools/arch/x86/include/asm/disabled-features.h)
- [tools/arch/x86/include/asm/msr-index.h](tools/arch/x86/include/asm/msr-index.h)
- [tools/arch/x86/include/asm/nops.h](tools/arch/x86/include/asm/nops.h)
- [tools/arch/x86/include/asm/required-features.h](tools/arch/x86/include/asm/required-features.h)
- [tools/testing/selftests/kvm/x86_64/sev_migrate_tests.c](tools/testing/selftests/kvm/x86_64/sev_migrate_tests.c)

</details>



## Purpose and Scope

This document covers the AMD SVM (Secure Virtual Machine) implementation within KVM, providing hardware-assisted virtualization support for AMD processors. The SVM implementation enables efficient execution of virtual machines through AMD-V technology, including advanced features like nested virtualization, SEV (Secure Encrypted Virtualization), and AVIC (Advanced Virtual Interrupt Controller).

For Intel VMX virtualization support, see [KVM x86 Implementation](#2.1). For ARM64 virtualization, see [KVM ARM64 Implementation](#2.2).

## Architecture Overview

The AMD SVM implementation provides a complete virtualization solution leveraging AMD-V hardware extensions. The architecture centers around the VMCB (Virtual Machine Control Block) which contains guest state and control information.

```mermaid
graph TB
    subgraph "KVM Core"
        KVM_CORE["kvm"]
        VCPU["kvm_vcpu"]
        VM["kvm"]
    end
    
    subgraph "SVM Implementation"
        SVM_MAIN["svm.c"]
        VCPU_SVM["vcpu_svm"]
        KVM_SVM["kvm_svm"] 
        VMCB["vmcb"]
    end
    
    subgraph "Hardware Features"
        NPT["Nested Page Tables"]
        AVIC_HW["AVIC Hardware"]
        SEV_HW["SEV Hardware"]
        SVM_HW["AMD-V Hardware"]
    end
    
    subgraph "Advanced Features"
        NESTED["nested.c"]
        SEV["sev.c"] 
        AVIC["avic.c"]
    end
    
    KVM_CORE --> SVM_MAIN
    VCPU --> VCPU_SVM
    VM --> KVM_SVM
    VCPU_SVM --> VMCB
    
    SVM_MAIN --> NPT
    SVM_MAIN --> SVM_HW
    
    NESTED --> SVM_MAIN
    SEV --> SEV_HW
    AVIC --> AVIC_HW
    
    VMCB --> SVM_HW
```

**SVM Architecture Within KVM**

Sources: [arch/x86/kvm/svm/svm.c:1-100](), [arch/x86/kvm/svm/svm.h:1-50]()

## Core Data Structures

The SVM implementation revolves around several key data structures that manage virtualization state and hardware interaction.

```mermaid
graph TB
    subgraph "Per-VM State"
        KVM_SVM["kvm_svm"]
        SEV_INFO["kvm_sev_info"]
        AVIC_VM["AVIC VM State"]
    end
    
    subgraph "Per-vCPU State"
        VCPU_SVM["vcpu_svm"]
        VMCB01["vmcb01"]
        VMCB02["vmcb02"]
        NESTED_STATE["svm_nested_state"]
    end
    
    subgraph "Hardware Interface"
        VMCB_PTR["vmcb *ptr"]
        VMCB_PA["vmcb_pa"]
        MSRPM["msrpm"]
        IOPM["iopm"]
    end
    
    KVM_SVM --> SEV_INFO
    KVM_SVM --> AVIC_VM
    
    VCPU_SVM --> VMCB01
    VCPU_SVM --> VMCB02
    VCPU_SVM --> NESTED_STATE
    
    VMCB01 --> VMCB_PTR
    VMCB01 --> VMCB_PA
    VCPU_SVM --> MSRPM
    VCPU_SVM --> IOPM
```

**Core SVM Data Structures**

The `vcpu_svm` structure contains per-vCPU state including VMCB management, MSR permission maps, and nested virtualization state. The `kvm_svm` structure holds per-VM state for advanced features like SEV and AVIC.

Sources: [arch/x86/kvm/svm/svm.h:192-280](), [arch/x86/kvm/svm/svm.h:96-106]()

### VMCB Structure and Management

The Virtual Machine Control Block (VMCB) is the central hardware interface for SVM operations:

| Component | Purpose | Key Fields |
|-----------|---------|------------|
| `vmcb_control_area` | VM execution control | intercepts, ASID, NPT settings |
| `vmcb_save_area` | Guest CPU state | registers, segment state, EFER |
| Clean bits | Optimization | tracks modified VMCB sections |

The VMCB management includes two instances per vCPU: `vmcb01` for L1 guest state and `vmcb02` for L2 nested guest state.

Sources: [arch/x86/kvm/svm/svm.h:110-125](), [arch/x86/include/asm/svm.h:1-50]()

## Hardware Support and Initialization

### CPU Feature Detection

The SVM implementation performs comprehensive CPU feature detection during initialization:

```mermaid
graph LR
    BOOT["Boot CPU"] --> DETECT["__kvm_is_svm_supported()"]
    DETECT --> VENDOR_CHECK["AMD/Hygon Check"]
    VENDOR_CHECK --> SVM_FEATURE["X86_FEATURE_SVM"]
    SVM_FEATURE --> VM_CR["MSR_VM_CR Check"]
    VM_CR --> SEV_CHECK["SEV Guest Check"]
    SEV_CHECK --> READY["SVM Ready"]
```

**SVM Hardware Detection Flow**

The detection process validates AMD/Hygon processors, checks for SVM CPUID feature bit, verifies SVM is not disabled in `MSR_VM_CR`, and ensures the system is not running as an SEV guest.

Sources: [arch/x86/kvm/svm/svm.c:531-572]()

### Hardware Initialization

SVM hardware initialization involves multiple components:

```c
// Key initialization functions
static int svm_hardware_enable(void)     // Per-CPU enablement
static int svm_cpu_init(int cpu)         // Per-CPU data setup  
static void init_vmcb(struct kvm_vcpu *vcpu)  // VMCB initialization
```

The initialization process includes:
- EFER.SVME bit enablement
- VMCB memory allocation and setup
- MSR permission map configuration
- ASID (Address Space ID) management setup

Sources: [arch/x86/kvm/svm/svm.c:625-704](), [arch/x86/kvm/svm/svm.c:1251-1400]()

## VM Entry and Exit Handling

### VM Entry Flow

The SVM implementation provides optimized VM entry through hardware-specific assembly routines:

```mermaid
graph TD
    PREP["VM Entry Preparation"]
    PREP --> STATE_LOAD["Load Guest State"]
    STATE_LOAD --> VMCB_SETUP["Setup VMCB"]
    VMCB_SETUP --> VMRUN["VMRUN Instruction"]
    VMRUN --> GUEST["Guest Execution"]
    
    GUEST --> VMEXIT["VM Exit"]
    VMEXIT --> EXIT_REASON["Determine Exit Reason"]
    EXIT_REASON --> HANDLER["Exit Handler"]
    HANDLER --> HOST["Return to Host"]
```

**VM Entry/Exit Cycle**

Key functions in the VM entry/exit flow:
- `svm_vcpu_run()` - Main entry point for guest execution
- `__svm_vcpu_run()` - Assembly routine for state switching
- VM exit handlers for different exit reasons (NPF, MSR access, etc.)

Sources: [arch/x86/kvm/svm/svm.c:4000-4100](), [arch/x86/kvm/svm/vmenter.S:1-200]()

### Exit Code Processing

SVM handles numerous VM exit reasons through a comprehensive dispatch system:

| Exit Code | Handler | Purpose |
|-----------|---------|---------|
| `SVM_EXIT_NPF` | `svm_handle_npf()` | Nested page fault handling |
| `SVM_EXIT_MSR` | `svm_handle_msr()` | MSR access emulation |
| `SVM_EXIT_HLT` | `svm_handle_halt()` | Guest halt instruction |
| `SVM_EXIT_CPUID` | `kvm_emulate_cpuid()` | CPUID emulation |

Sources: [arch/x86/kvm/svm/svm.c:2800-3200]()

## Memory Management - NPT Support

### Nested Page Tables

NPT (Nested Page Tables) provides hardware-assisted memory virtualization:

```mermaid
graph TB
    subgraph "Guest Memory Access"
        GVA["Guest Virtual Address"]
        GPA["Guest Physical Address"] 
        HPA["Host Physical Address"]
    end
    
    subgraph "Translation Process"
        GUEST_PT["Guest Page Tables"]
        NPT_WALK["NPT Walk"]
        HOST_PT["Host Page Tables"]
    end
    
    GVA --> GUEST_PT
    GUEST_PT --> GPA
    GPA --> NPT_WALK
    NPT_WALK --> HPA
    
    NPT_WALK --> HOST_PT
```

**NPT Memory Translation**

NPT enables two-level address translation where guest page tables translate GVAGPA and NPT translates GPAHPA, eliminating the need for shadow page tables.

Sources: [arch/x86/kvm/svm/svm.c:288-295](), [arch/x86/kvm/svm/svm.c:199-208]()

## Advanced Features

### SEV (Secure Encrypted Virtualization)

SEV provides VM memory encryption and guest attestation capabilities:

```mermaid
graph TB
    subgraph "SEV Components"
        SEV_INIT["sev_guest_init()"]
        SEV_LAUNCH["sev_launch_start()"] 
        SEV_MEASURE["sev_launch_measure()"]
        SEV_FINISH["sev_launch_finish()"]
    end
    
    subgraph "Hardware Integration"
        PSP["Platform Security Processor"]
        MEMORY_ENC["Memory Encryption"]
        ASID_MGR["ASID Management"]
    end
    
    SEV_INIT --> PSP
    SEV_LAUNCH --> MEMORY_ENC
    SEV_MEASURE --> PSP
    SEV_FINISH --> ASID_MGR
```

**SEV Implementation Architecture**

SEV support includes:
- Memory encryption key management
- Guest attestation and measurement
- ASID-based memory protection
- SEV-ES (Encrypted State) support for register encryption

Sources: [arch/x86/kvm/svm/sev.c:253-290](), [arch/x86/kvm/svm/sev.c:328-398]()

### AVIC (Advanced Virtual Interrupt Controller)

AVIC provides hardware-accelerated interrupt delivery:

```c
// Key AVIC structures and functions
struct avic_logical_id_entry;    // Logical APIC ID mapping
struct avic_physical_id_entry;   // Physical APIC ID mapping
int avic_init_vcpu(struct vcpu_svm *svm);
void avic_vcpu_load(struct kvm_vcpu *vcpu, int cpu);
```

AVIC reduces VM exits for interrupt handling by allowing direct interrupt delivery to guests through hardware tables.

Sources: [arch/x86/kvm/svm/avic.c:200-400]()

### Nested Virtualization

Nested SVM enables running hypervisors within SVM guests:

```mermaid
graph TB
    subgraph "L0 Hypervisor"
        L0_KVM["KVM"]
        L0_SVM["SVM Implementation"]
    end
    
    subgraph "L1 Guest Hypervisor" 
        L1_HV["Guest Hypervisor"]
        L1_VMCB["L1 VMCB"]
    end
    
    subgraph "L2 Guest"
        L2_GUEST["Nested Guest"]
        L2_VMCB["L2 VMCB"]
    end
    
    L0_SVM --> L1_HV
    L1_HV --> L1_VMCB
    L1_VMCB --> L2_VMCB
    L2_VMCB --> L2_GUEST
```

**Nested SVM Architecture**

Key nested virtualization functions:
- `nested_svm_vmrun()` - Handle L1L2 transitions
- `nested_svm_vmexit()` - Handle L2L1 transitions  
- `nested_svm_init_mmu_context()` - Setup nested MMU

Sources: [arch/x86/kvm/svm/nested.c:500-700](), [arch/x86/kvm/svm/nested.c:84-104]()

## Integration Points

### KVM Core Integration

The SVM implementation integrates with KVM core through the `kvm_x86_ops` interface:

| Operation | SVM Implementation | Purpose |
|-----------|-------------------|---------|
| `hardware_enable` | `svm_hardware_enable()` | Enable SVM on CPU |
| `vcpu_create` | `svm_vcpu_create()` | Create SVM vCPU |
| `vcpu_run` | `svm_vcpu_run()` | Execute guest |
| `handle_exit` | `svm_handle_exit()` | Process VM exits |

This provides a uniform interface allowing KVM to support both Intel VMX and AMD SVM through the same core APIs.

Sources: [arch/x86/kvm/svm/svm.c:4900-5000]()

### Module Parameters and Configuration

SVM behavior is configurable through various module parameters:

```c
// Key module parameters
module_param_named(npt, npt_enabled, bool, 0444);        // NPT enable/disable
module_param(nested, int, S_IRUGO);                      // Nested virtualization
module_param_named(avic, avic, bool, 0444);             // AVIC support
module_param_named(sev, sev_enabled, bool, 0444);       // SEV support
```

These parameters allow runtime configuration of SVM features based on workload requirements and hardware capabilities.

Sources: [arch/x86/kvm/svm/svm.c:199-240](), [arch/x86/kvm/svm/sev.c:52-66]()1c:T59e2,# Process Management and Scheduling

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [Documentation/accounting/psi.rst](Documentation/accounting/psi.rst)
- [Documentation/admin-guide/cgroup-v2.rst](Documentation/admin-guide/cgroup-v2.rst)
- [Documentation/admin-guide/kernel-parameters.txt](Documentation/admin-guide/kernel-parameters.txt)
- [Documentation/core-api/workqueue.rst](Documentation/core-api/workqueue.rst)
- [Documentation/translations/zh_CN/core-api/workqueue.rst](Documentation/translations/zh_CN/core-api/workqueue.rst)
- [block/blk-cgroup-fc-appid.c](block/blk-cgroup-fc-appid.c)
- [drivers/powercap/dtpm_cpu.c](drivers/powercap/dtpm_cpu.c)
- [include/linux/cgroup-defs.h](include/linux/cgroup-defs.h)
- [include/linux/cgroup.h](include/linux/cgroup.h)
- [include/linux/cgroup_refcnt.h](include/linux/cgroup_refcnt.h)
- [include/linux/cpuset.h](include/linux/cpuset.h)
- [include/linux/fault-inject.h](include/linux/fault-inject.h)
- [include/linux/gfp.h](include/linux/gfp.h)
- [include/linux/kernel_stat.h](include/linux/kernel_stat.h)
- [include/linux/memcontrol.h](include/linux/memcontrol.h)
- [include/linux/misc_cgroup.h](include/linux/misc_cgroup.h)
- [include/linux/psi.h](include/linux/psi.h)
- [include/linux/psi_types.h](include/linux/psi_types.h)
- [include/linux/resume_user_mode.h](include/linux/resume_user_mode.h)
- [include/linux/sched.h](include/linux/sched.h)
- [include/linux/sched/cputime.h](include/linux/sched/cputime.h)
- [include/linux/sched/sysctl.h](include/linux/sched/sysctl.h)
- [include/linux/sched/topology.h](include/linux/sched/topology.h)
- [include/linux/start_kernel.h](include/linux/start_kernel.h)
- [include/linux/swap.h](include/linux/swap.h)
- [include/linux/workqueue.h](include/linux/workqueue.h)
- [init/main.c](init/main.c)
- [kernel/bpf/cgroup.c](kernel/bpf/cgroup.c)
- [kernel/bpf/cgroup_iter.c](kernel/bpf/cgroup_iter.c)
- [kernel/bpf/local_storage.c](kernel/bpf/local_storage.c)
- [kernel/cgroup/cgroup-internal.h](kernel/cgroup/cgroup-internal.h)
- [kernel/cgroup/cgroup-v1.c](kernel/cgroup/cgroup-v1.c)
- [kernel/cgroup/cgroup.c](kernel/cgroup/cgroup.c)
- [kernel/cgroup/cpuset.c](kernel/cgroup/cpuset.c)
- [kernel/cgroup/misc.c](kernel/cgroup/misc.c)
- [kernel/cgroup/rstat.c](kernel/cgroup/rstat.c)
- [kernel/power/Makefile](kernel/power/Makefile)
- [kernel/power/process.c](kernel/power/process.c)
- [kernel/sched/Makefile](kernel/sched/Makefile)
- [kernel/sched/autogroup.c](kernel/sched/autogroup.c)
- [kernel/sched/build_policy.c](kernel/sched/build_policy.c)
- [kernel/sched/build_utility.c](kernel/sched/build_utility.c)
- [kernel/sched/core.c](kernel/sched/core.c)
- [kernel/sched/core_sched.c](kernel/sched/core_sched.c)
- [kernel/sched/cpudeadline.c](kernel/sched/cpudeadline.c)
- [kernel/sched/cpufreq_schedutil.c](kernel/sched/cpufreq_schedutil.c)
- [kernel/sched/cpupri.c](kernel/sched/cpupri.c)
- [kernel/sched/cputime.c](kernel/sched/cputime.c)
- [kernel/sched/deadline.c](kernel/sched/deadline.c)
- [kernel/sched/debug.c](kernel/sched/debug.c)
- [kernel/sched/fair.c](kernel/sched/fair.c)
- [kernel/sched/features.h](kernel/sched/features.h)
- [kernel/sched/idle.c](kernel/sched/idle.c)
- [kernel/sched/pelt.c](kernel/sched/pelt.c)
- [kernel/sched/pelt.h](kernel/sched/pelt.h)
- [kernel/sched/psi.c](kernel/sched/psi.c)
- [kernel/sched/rt.c](kernel/sched/rt.c)
- [kernel/sched/sched.h](kernel/sched/sched.h)
- [kernel/sched/smp.h](kernel/sched/smp.h)
- [kernel/sched/stats.h](kernel/sched/stats.h)
- [kernel/sched/stop_task.c](kernel/sched/stop_task.c)
- [kernel/sched/topology.c](kernel/sched/topology.c)
- [kernel/stop_machine.c](kernel/stop_machine.c)
- [kernel/trace/trace_preemptirq.c](kernel/trace/trace_preemptirq.c)
- [kernel/workqueue.c](kernel/workqueue.c)
- [kernel/workqueue_internal.h](kernel/workqueue_internal.h)
- [lib/fault-inject.c](lib/fault-inject.c)
- [mm/failslab.c](mm/failslab.c)
- [mm/memcontrol.c](mm/memcontrol.c)
- [mm/page_alloc.c](mm/page_alloc.c)
- [mm/percpu-internal.h](mm/percpu-internal.h)
- [mm/percpu-stats.c](mm/percpu-stats.c)
- [mm/percpu.c](mm/percpu.c)
- [mm/vmpressure.c](mm/vmpressure.c)
- [mm/vmscan.c](mm/vmscan.c)
- [tools/perf/util/bpf_skel/bperf_cgroup.bpf.c](tools/perf/util/bpf_skel/bperf_cgroup.bpf.c)
- [tools/workqueue/wq_dump.py](tools/workqueue/wq_dump.py)
- [tools/workqueue/wq_monitor.py](tools/workqueue/wq_monitor.py)

</details>



This document provides an overview of the process management and scheduling subsystems in the WSL2 Linux kernel. It explains how processes are represented, managed, and scheduled within the kernel, covering the scheduler architecture, the different scheduling classes, and how control groups integrate with the scheduling system.

## 1. Process Representation and States

Linux represents processes via the `task_struct` structure, which contains all information the kernel needs to manage a process, including its scheduling parameters, memory mappings, file descriptors, and more.

### Process States

Processes can be in various states, defined in [include/linux/sched.h:84-105]():

```
TASK_RUNNING           - Process is either running or ready to run
TASK_INTERRUPTIBLE     - Process is sleeping but can be woken by signals
TASK_UNINTERRUPTIBLE   - Process is in uninterruptible sleep
__TASK_STOPPED         - Process execution has been stopped
__TASK_TRACED          - Process is being traced
TASK_PARKED            - Process is parked (special sleep state)
TASK_DEAD              - Process is exiting
```

A process transitions between states based on scheduler decisions, system calls, and events like I/O completion.

```mermaid
stateDiagram-v2
    direction LR
    [*] --> RUNNING: fork()
    RUNNING --> INTERRUPTIBLE: wait for event
    RUNNING --> UNINTERRUPTIBLE: Uninterruptible I/O
    INTERRUPTIBLE --> RUNNING: Event occurs or signal
    UNINTERRUPTIBLE --> RUNNING: Event completes
    RUNNING --> STOPPED: SIGSTOP
    STOPPED --> RUNNING: SIGCONT
    RUNNING --> [*]: exit()
```

Sources: [include/linux/sched.h:84-105](), [kernel/sched/core.c:1042-1065]()

## 2. Scheduler Architecture

The Linux scheduler uses a modular architecture with multiple scheduling classes to handle different types of tasks with different requirements.

### Multi-Class Scheduler Design

The scheduler operates through a hierarchy of scheduling classes, each specialized for specific workload types:

#### Core Scheduler Functions and Classes

```mermaid
flowchart TD
    subgraph sched_classes["Scheduling Classes (sched_class)"]
        direction TB
        stop_sched_class["stop_sched_class\n(highest priority)"] --> dl_sched_class["dl_sched_class\n(SCHED_DEADLINE)"]
        dl_sched_class --> rt_sched_class["rt_sched_class\n(SCHED_FIFO/RR)"]
        rt_sched_class --> fair_sched_class["fair_sched_class\n(SCHED_NORMAL/BATCH)"]
        fair_sched_class --> idle_sched_class["idle_sched_class\n(SCHED_IDLE)"]
    end

    task_struct["task_struct"] --> __schedule["__schedule()"]
    __schedule --> pick_next_task["pick_next_task()"]
    pick_next_task --> sched_classes
    
    subgraph methods["Class Methods"]
        enqueue_task["enqueue_task()"]
        dequeue_task["dequeue_task()"]
        pick_next_task_method["pick_next_task()"]
        task_tick["task_tick()"]
    end
    
    sched_classes --> methods
```

Each `sched_class` implements function pointers for operations like `enqueue_task`, `dequeue_task`, and `pick_next_task`. The core scheduler in `__schedule()` iterates through classes in priority order to select the next task.

Sources: [kernel/sched/sched.h:2203-2270](), [kernel/sched/core.c:6700-6750]()

### Run Queues and Per-CPU Data Structures

Each CPU has a run queue (`struct rq`) that maintains lists of runnable tasks for each scheduling class. The run queue is the central data structure for the scheduler:

#### Run Queue Architecture

```mermaid
classDiagram
    class rq {
        +raw_spinlock_t __lock
        +clock: u64
        +nr_running: unsigned int
        +cfs: struct cfs_rq
        +rt: struct rt_rq
        +dl: struct dl_rq
        +curr: struct task_struct
        +idle: struct task_struct
        +load: struct rq_load
    }
    
    class cfs_rq {
        +tasks_timeline: struct rb_root_cached
        +min_vruntime: u64
        +avg_vruntime: s64
        +avg_load: unsigned long
        +nr_running: int
        +h_nr_running: int
        +idle: int
    }
    
    class rt_rq {
        +active: struct rt_prio_array
        +rt_nr_running: int
        +highest_prio: struct rt_prio
        +rt_throttled: int
    }
    
    class dl_rq {
        +root: struct rb_root_cached
        +dl_nr_running: int
        +earliest_dl: struct dl_bw
    }
    
    rq --> cfs_rq : "contains"
    rq --> rt_rq : "contains"
    rq --> dl_rq : "contains"
```

The global `runqueues` per-CPU variable provides access to each CPU's run queue, defined as `DEFINE_PER_CPU_SHARED_ALIGNED(struct rq, runqueues)`.

Sources: [kernel/sched/sched.h:1027-1123](), [kernel/sched/core.c:118]()

## 3. Completely Fair Scheduler (CFS)

The Completely Fair Scheduler (CFS) is the default scheduler for normal (non-realtime) tasks. It aims to provide fair CPU time allocation among competing processes.

### EEVDF Algorithm Implementation

Recent Linux kernels, including WSL2, implement the Earliest Eligible Virtual Deadline First (EEVDF) algorithm in CFS through the `pick_eevdf()` function:

#### EEVDF Task Selection Process

```mermaid
flowchart TD
    __pick_eevdf["__pick_eevdf(cfs_rq)"] --> check_curr{"curr && curr->on_rq &&\nentity_eligible(cfs_rq, curr)?"}
    check_curr -- "Yes" --> best_curr["best = curr"]
    check_curr -- "No" --> traverse["Traverse tasks_timeline\nrb_tree"]
    
    traverse --> entity_eligible{"entity_eligible(cfs_rq, se)?"}
    entity_eligible -- "No" --> rb_left["node = node->rb_left"]
    entity_eligible -- "Yes" --> deadline_gt{"deadline_gt(deadline,\nbest, se)?"}
    deadline_gt -- "Yes" --> update_best["best = se"]
    deadline_gt -- "No" --> continue_search["Continue search"]
    
    rb_left --> entity_eligible
    update_best --> check_min_deadline["Check min_deadline\nin subtrees"]
    continue_search --> check_min_deadline
    check_min_deadline --> return_best["return best"]
```

A task is eligible when `entity_eligible(cfs_rq, se)` returns true, determined by comparing the entity's lag with the current average vruntime. The algorithm selects the eligible task with the earliest deadline.

Sources: [kernel/sched/fair.c:881-988](), [kernel/sched/fair.c:744-758]()

### Virtual Runtime and Deadline Management

CFS tracks virtual runtime (vruntime) and deadlines for each scheduling entity. The core functions managing these are:

#### Virtual Runtime and Deadline Update Flow

```mermaid
flowchart TD
    update_curr["update_curr(cfs_rq)"] --> calc_delta_fair["calc_delta_fair(delta_exec, se)"]
    calc_delta_fair --> __calc_delta["__calc_delta(delta_exec,\nNICE_0_LOAD, &se->load)"]
    __calc_delta --> se_vruntime["se->vruntime += delta"]
    se_vruntime --> update_deadline["update_deadline(cfs_rq, se)"]
    
    update_deadline --> check_deadline{"(s64)(se->vruntime -\nse->deadline) < 0?"}
    check_deadline -- "No" --> set_slice["se->slice = sysctl_sched_base_slice"]
    set_slice --> calc_deadline["se->deadline = se->vruntime +\ncalc_delta_fair(se->slice, se)"]
    calc_deadline --> resched_check{"cfs_rq->nr_running > 1?"}
    resched_check -- "Yes" --> resched_curr["resched_curr(rq_of(cfs_rq))"]
    
    check_deadline -- "Yes" --> update_min_vruntime["update_min_vruntime(cfs_rq)"]
    resched_curr --> update_min_vruntime
    update_min_vruntime --> avg_vruntime_update["avg_vruntime_update(cfs_rq, delta)"]
```

The `calc_delta_fair()` function adjusts time accounting based on task weight, where lower nice values (higher priority) result in slower vruntime progression. The `update_deadline()` function implements EEVDF deadline assignment when a task's vruntime exceeds its current deadline.

Sources: [kernel/sched/fair.c:1025-1049](), [kernel/sched/fair.c:311-317](), [kernel/sched/fair.c:774-798]()

## 4. Real-Time and Deadline Schedulers

The kernel supports real-time scheduling for tasks with strict timing requirements.

### Real-Time Scheduler

The real-time scheduler handles tasks with `SCHED_FIFO` (first-in, first-out) and `SCHED_RR` (round-robin) policies:

- SCHED_FIFO tasks run until they yield, block, or are preempted by higher-priority RT tasks
- SCHED_RR tasks receive a time slice and are moved to the back of the queue when it expires
- RT tasks have static priorities from 1 (lowest) to 99 (highest)

```mermaid
flowchart TD
    rtqueue["RT Priority Array\n(1-99)"] --> pick["Pick Highest Priority\nNon-Empty Queue"]
    pick --> fifo{"Policy?"}
    fifo -- "SCHED_FIFO" --> runfifo["Run Until Complete\nor Preempted"]
    fifo -- "SCHED_RR" --> runrr["Run for Time Slice"]
    runrr --> requeue["Requeue at End\nof Same Priority"]
```

Sources: [kernel/sched/rt.c:1-26](), [kernel/sched/rt.c:1201-1253]()

### Deadline Scheduler

SCHED_DEADLINE tasks specify three parameters:
- Runtime: maximum execution time
- Period: task frequency
- Deadline: time by which execution must complete (relative to start of period)

The scheduler guarantees that each task receives its requested runtime within each period, selecting tasks with the earliest absolute deadline first.

Sources: [kernel/sched/deadline.c:1-17](), [kernel/sched/deadline.c:1071-1126]()

## 5. Control Groups (cgroups)

Control groups provide a mechanism to organize processes into hierarchical groups and manage their resource usage.

### CPU Controller

The CPU controller allows limiting and prioritizing CPU usage among groups of processes:

```mermaid
flowchart TD
    root["Root cgroup"] --> group1["Group A\nCPU Weight: 100"]
    root --> group2["Group B\nCPU Weight: 200"]
    group1 --> subA1["Subgroup A1\nCPU Weight: 50"]
    group1 --> subA2["Subgroup A2\nCPU Weight: 50"]
    
    style root fill:white,stroke:black
    style group1 fill:white,stroke:black
    style group2 fill:white,stroke:black
    style subA1 fill:white,stroke:black
    style subA2 fill:white,stroke:black
```

The CPU controller provides two interfaces:
1. **Weights**: Specify relative shares of CPU time (default: 100)
2. **Limits**: Restrict CPU usage through maximum bandwidth or quotas

Sources: [kernel/cgroup/cgroup.c:116-176](), [kernel/cgroup/cpuset.c:75-187]()

### Scheduler Integration with cgroups

The scheduler integrates with cgroups through:

- Task group structures that track groups of tasks
- Group scheduling entities that represent cgroups in the CFS scheduler
- Hierarchical load tracking and distribution
- Throttling mechanisms to enforce bandwidth limits

```mermaid
classDiagram
    class task_group {
        +shares
        +cfs_bandwidth
        +se[]
        +cfs_rq[]
    }
    
    class sched_entity {
        +load
        +vruntime
        +avg
        +cfs_rq
        +my_q
    }
    
    class cfs_rq {
        +tasks_timeline
        +tg
    }
    
    task_group --> sched_entity : contains
    task_group --> cfs_rq : contains
    sched_entity --> cfs_rq : runs on
    cfs_rq --> task_group : belongs to
```

Sources: [kernel/sched/fair.c:324-397](), [kernel/sched/sched.h:362-415]()

## 6. Load Balancing

Load balancing distributes tasks across CPUs to maximize throughput and minimize latency.

### CPU Topology Awareness

The scheduler organizes CPUs into a hierarchy of scheduling domains based on the hardware topology:

```mermaid
flowchart TD
    system["System Domain"] --> socket1["Socket Domain 1"]
    system --> socket2["Socket Domain 2"]
    socket1 --> core11["Core Domain 1-1"]
    socket1 --> core12["Core Domain 1-2"]
    socket2 --> core21["Core Domain 2-1"]
    socket2 --> core22["Core Domain 2-2"]
    core11 --> cpu1["CPU 0"]
    core11 --> cpu2["CPU 1"]
    core12 --> cpu3["CPU 2"]
    core12 --> cpu4["CPU 3"]
    core21 --> cpu5["CPU 4"]
    core21 --> cpu6["CPU 5"]
    core22 --> cpu7["CPU 6"]
    core22 --> cpu8["CPU 7"]
```

Load balancing happens at each domain level, ensuring tasks are balanced first among SMT siblings, then among cores, then among sockets.

Sources: [kernel/sched/topology.c:1-30](), [kernel/sched/fair.c:1368-1403]()

### Task Migration

Tasks are migrated between CPUs to maintain balance. The scheduler considers:
- CPU capacity and utilization
- Task affinity and constraints (cgroups, cpusets)
- Cache locality
- NUMA memory placement

Sources: [kernel/sched/fair.c:9516-9578]()

## 7. Workqueue Integration and Asynchronous Task Execution

The workqueue subsystem provides asynchronous execution contexts that integrate closely with the process scheduler for deferred work processing.

### Workqueue Architecture

The workqueue subsystem manages pools of worker threads that execute work items asynchronously:

#### Workqueue System Components

```mermaid
flowchart TD
    subgraph workqueue_struct["workqueue_struct"]
        pwqs["pwqs: pool_workqueue list"]
        flags["flags: WQ_* flags"]
        cpu_pwq["cpu_pwq: per-cpu pwqs"]
    end
    
    subgraph worker_pool["worker_pool"]
        lock["lock: raw_spinlock_t"]
        worklist["worklist: pending works"]
        workers["workers: worker threads"]
        nr_running["nr_running: active workers"]
        idle_list["idle_list: idle workers"]
    end
    
    subgraph work_struct["work_struct"]
        func["func: work_func_t"]
        data["data: work flags + pwq"]
        entry["entry: list_head"]
    end
    
    workqueue_struct --> worker_pool : "routes to"
    worker_pool --> work_struct : "executes"
    
    subgraph system_wqs["System Workqueues"]
        system_wq["system_wq"]
        system_highpri_wq["system_highpri_wq"]
        system_unbound_wq["system_unbound_wq"]
        system_long_wq["system_long_wq"]
    end
```

Workers are kernel threads created by `worker_thread()` that continuously process work items from their pool's worklist. The scheduler treats these as normal `SCHED_NORMAL` tasks.

Sources: [kernel/workqueue.c:285-326](), [kernel/workqueue.c:402-436]()

### Integration with Scheduler

Workqueue workers interact with the scheduler through several mechanisms:

- **Concurrency Management**: The workqueue subsystem monitors `nr_running` to maintain optimal worker thread counts
- **CPU Intensive Work**: Tasks exceeding `wq_cpu_intensive_thresh_us` are marked with `WORKER_CPU_INTENSIVE` and excluded from concurrency management
- **NUMA Awareness**: Worker pools are organized by NUMA nodes, respecting scheduler topology

Sources: [kernel/workqueue.c:360-361](), [kernel/workqueue.c:87-88]()

## 8. Pressure Stall Information (PSI)

Pressure Stall Information monitors resource pressure in the system and provides metrics to userspace through the `psi_group` infrastructure.

### PSI State Tracking

PSI tracks resource contention through state changes recorded in `psi_group_cpu` structures:

#### PSI Resource States and Transitions

```mermaid
flowchart TD
    subgraph psi_states["PSI States (psi_states)"]
        PSI_IO_SOME["PSI_IO_SOME\n(1 task waiting I/O)"]
        PSI_IO_FULL["PSI_IO_FULL\n(all tasks waiting I/O)"]
        PSI_MEM_SOME["PSI_MEM_SOME\n(1 task in reclaim)"]
        PSI_MEM_FULL["PSI_MEM_FULL\n(all tasks in reclaim)"]
        PSI_CPU_SOME["PSI_CPU_SOME\n(1 task on runqueue)"]
    end
    
    psi_task_change["psi_task_change(task, prev, next)"] --> psi_group_change["psi_group_change(group, cpu, clear, set)"]
    psi_group_change --> record_times["record_times(groupc, now)"]
    record_times --> update_averages["calc_avgs(group, now)"]
    
    subgraph triggers["PSI Triggers"]
        psi_trigger["psi_trigger structs"]
        psi_poll["psi_poll workqueue"]
        eventfd["eventfd notifications"]
    end
    
    update_averages --> triggers
```

The `psi_task_change()` function is called from scheduler paths to update pressure state when tasks transition between running, sleeping, and waiting states.

Sources: [kernel/sched/psi.c:260-300](), [kernel/sched/psi.c:140-200]()

## 9. Process Creation and Termination

### Process Creation Flow

New processes are created through the `clone()` system call family, implemented by `kernel_clone()` and `copy_process()`:

#### Process Creation and Scheduler Integration

```mermaid
flowchart TD
    sys_clone["SYSCALL_DEFINE5(clone, ...)"] --> kernel_clone["kernel_clone(args)"]
    kernel_clone --> copy_process["copy_process(args)"]
    
    copy_process --> dup_task_struct["dup_task_struct(current, node)"]
    dup_task_struct --> sched_fork["sched_fork(clone_flags, p)"]
    
    sched_fork --> __sched_fork["__sched_fork(clone_flags, p)"]
    __sched_fork --> init_entity_runnable_average["init_entity_runnable_average(&p->se)"]
    init_entity_runnable_average --> task_fork_fair["task_fork_fair(p)"]
    
    subgraph scheduler_setup["Scheduler Setup"]
        set_task_cpu["set_task_cpu(p, cpu)"]
        set_load_weight["set_load_weight(p, false)"]
        init_numa_balancing["init_numa_balancing(clone_flags, p)"]
    end
    
    task_fork_fair --> scheduler_setup
    scheduler_setup --> wake_up_new_task["wake_up_new_task(p)"]
    wake_up_new_task --> activate_task["activate_task(rq, p, flags)"]
```

The `sched_fork()` function initializes scheduler-specific fields, while `wake_up_new_task()` places the new process on a run queue through `activate_task()`.

### Process Termination and Cleanup

Process termination involves scheduler cleanup through `sched_exit()`:

```mermaid
flowchart TD
    do_exit["do_exit(code)"] --> exit_notify["exit_notify(current)"]
    exit_notify --> sched_exit["sched_exit(current, group_dead)"]
    
    sched_exit --> sched_autogroup_exit_task["sched_autogroup_exit_task(current)"]
    sched_autogroup_exit_task --> sched_dead["sched_dead(current)"]
    
    sched_dead --> deactivate_task["deactivate_task(rq, current, flags)"]
    deactivate_task --> context_switch["context_switch(rq, prev, next)"]
    context_switch --> finish_task_switch["finish_task_switch(prev)"]
```

The scheduler removes the dying task from run queues and handles final cleanup in `finish_task_switch()`.

Sources: [kernel/fork.c:2400-2500](), [kernel/exit.c:750-850](), [kernel/sched/core.c:4550-4600]()

## Summary

The Linux scheduling subsystem in WSL2 is a complex but efficient system that:

1. Supports various task types through specialized scheduling classes
2. Provides fair distribution of CPU time with the CFS scheduler
3. Ensures real-time guarantees for time-critical tasks
4. Integrates with cgroups for resource control
5. Balances load across CPUs based on system topology
6. Monitors and reacts to resource pressure through PSI

This architecture ensures good performance and responsiveness for a wide range of workloads, from interactive desktop applications to server workloads and real-time systems.1d:T38f9,# CFS Scheduler

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [Documentation/accounting/psi.rst](Documentation/accounting/psi.rst)
- [Documentation/admin-guide/cgroup-v2.rst](Documentation/admin-guide/cgroup-v2.rst)
- [block/blk-cgroup-fc-appid.c](block/blk-cgroup-fc-appid.c)
- [drivers/powercap/dtpm_cpu.c](drivers/powercap/dtpm_cpu.c)
- [include/linux/cgroup-defs.h](include/linux/cgroup-defs.h)
- [include/linux/cgroup.h](include/linux/cgroup.h)
- [include/linux/cgroup_refcnt.h](include/linux/cgroup_refcnt.h)
- [include/linux/cpuset.h](include/linux/cpuset.h)
- [include/linux/fault-inject.h](include/linux/fault-inject.h)
- [include/linux/gfp.h](include/linux/gfp.h)
- [include/linux/kernel_stat.h](include/linux/kernel_stat.h)
- [include/linux/memcontrol.h](include/linux/memcontrol.h)
- [include/linux/misc_cgroup.h](include/linux/misc_cgroup.h)
- [include/linux/psi.h](include/linux/psi.h)
- [include/linux/psi_types.h](include/linux/psi_types.h)
- [include/linux/resume_user_mode.h](include/linux/resume_user_mode.h)
- [include/linux/sched.h](include/linux/sched.h)
- [include/linux/sched/cputime.h](include/linux/sched/cputime.h)
- [include/linux/sched/sysctl.h](include/linux/sched/sysctl.h)
- [include/linux/sched/topology.h](include/linux/sched/topology.h)
- [include/linux/swap.h](include/linux/swap.h)
- [kernel/bpf/cgroup.c](kernel/bpf/cgroup.c)
- [kernel/bpf/cgroup_iter.c](kernel/bpf/cgroup_iter.c)
- [kernel/bpf/local_storage.c](kernel/bpf/local_storage.c)
- [kernel/cgroup/cgroup-internal.h](kernel/cgroup/cgroup-internal.h)
- [kernel/cgroup/cgroup-v1.c](kernel/cgroup/cgroup-v1.c)
- [kernel/cgroup/cgroup.c](kernel/cgroup/cgroup.c)
- [kernel/cgroup/cpuset.c](kernel/cgroup/cpuset.c)
- [kernel/cgroup/misc.c](kernel/cgroup/misc.c)
- [kernel/cgroup/rstat.c](kernel/cgroup/rstat.c)
- [kernel/sched/Makefile](kernel/sched/Makefile)
- [kernel/sched/autogroup.c](kernel/sched/autogroup.c)
- [kernel/sched/build_policy.c](kernel/sched/build_policy.c)
- [kernel/sched/build_utility.c](kernel/sched/build_utility.c)
- [kernel/sched/core.c](kernel/sched/core.c)
- [kernel/sched/core_sched.c](kernel/sched/core_sched.c)
- [kernel/sched/cpudeadline.c](kernel/sched/cpudeadline.c)
- [kernel/sched/cpufreq_schedutil.c](kernel/sched/cpufreq_schedutil.c)
- [kernel/sched/cpupri.c](kernel/sched/cpupri.c)
- [kernel/sched/cputime.c](kernel/sched/cputime.c)
- [kernel/sched/deadline.c](kernel/sched/deadline.c)
- [kernel/sched/debug.c](kernel/sched/debug.c)
- [kernel/sched/fair.c](kernel/sched/fair.c)
- [kernel/sched/features.h](kernel/sched/features.h)
- [kernel/sched/idle.c](kernel/sched/idle.c)
- [kernel/sched/pelt.c](kernel/sched/pelt.c)
- [kernel/sched/pelt.h](kernel/sched/pelt.h)
- [kernel/sched/psi.c](kernel/sched/psi.c)
- [kernel/sched/rt.c](kernel/sched/rt.c)
- [kernel/sched/sched.h](kernel/sched/sched.h)
- [kernel/sched/smp.h](kernel/sched/smp.h)
- [kernel/sched/stats.h](kernel/sched/stats.h)
- [kernel/sched/stop_task.c](kernel/sched/stop_task.c)
- [kernel/sched/topology.c](kernel/sched/topology.c)
- [kernel/stop_machine.c](kernel/stop_machine.c)
- [kernel/trace/trace_preemptirq.c](kernel/trace/trace_preemptirq.c)
- [lib/fault-inject.c](lib/fault-inject.c)
- [mm/failslab.c](mm/failslab.c)
- [mm/memcontrol.c](mm/memcontrol.c)
- [mm/page_alloc.c](mm/page_alloc.c)
- [mm/percpu-internal.h](mm/percpu-internal.h)
- [mm/percpu-stats.c](mm/percpu-stats.c)
- [mm/percpu.c](mm/percpu.c)
- [mm/vmpressure.c](mm/vmpressure.c)
- [mm/vmscan.c](mm/vmscan.c)
- [tools/perf/util/bpf_skel/bperf_cgroup.bpf.c](tools/perf/util/bpf_skel/bperf_cgroup.bpf.c)

</details>



The Completely Fair Scheduler (CFS) is the default process scheduling class in the Linux kernel for `SCHED_NORMAL` and `SCHED_BATCH` tasks. This document covers the CFS implementation including the EEVDF (Earliest Eligible Virtual Deadline First) algorithm, virtual runtime tracking, group scheduling with cgroups, and load balancing mechanisms.

For information about real-time scheduling classes, see RT Scheduler. For deadline scheduling, see DL Scheduler. For workqueue task execution, see [Workqueue Subsystem](#3.2).

## Architecture Overview

CFS implements a completely fair scheduling algorithm using virtual runtime tracking and the EEVDF placement strategy. Tasks are organized in a red-black tree ordered by virtual runtime, with additional deadline-based selection for latency guarantees.

### CFS Core Components

```mermaid
graph TB
    subgraph "CFS Scheduling Class"
        fair_sched_class["fair_sched_class"]
        cfs_rq["struct cfs_rq"]
        sched_entity["struct sched_entity"]
    end
    
    subgraph "EEVDF Algorithm"
        pick_eevdf["__pick_eevdf()"]
        entity_eligible["entity_eligible()"]
        update_deadline["update_deadline()"]
    end
    
    subgraph "Virtual Runtime"
        vruntime["se->vruntime"]
        avg_vruntime["avg_vruntime()"]
        calc_delta_fair["calc_delta_fair()"]
    end
    
    subgraph "Red-Black Tree"
        tasks_timeline["cfs_rq->tasks_timeline"]
        enqueue_entity["__enqueue_entity()"]
        dequeue_entity["__dequeue_entity()"]
    end
    
    subgraph "Load Balancing"
        load_balance["load_balance()"]
        find_busiest_group["find_busiest_group()"]
        migrate_task["migrate_task()"]
    end
    
    fair_sched_class --> cfs_rq
    fair_sched_class --> sched_entity
    cfs_rq --> tasks_timeline
    sched_entity --> vruntime
    
    pick_eevdf --> entity_eligible
    pick_eevdf --> update_deadline
    avg_vruntime --> calc_delta_fair
    
    tasks_timeline --> enqueue_entity
    tasks_timeline --> dequeue_entity
    
    load_balance --> find_busiest_group
    load_balance --> migrate_task
```

Sources: [kernel/sched/fair.c:319](), [kernel/sched/fair.c:881](), [kernel/sched/fair.c:1025]()

### Task Lifecycle in CFS

```mermaid
graph LR
    new_task["New Task"] --> enqueue["enqueue_task_fair()"]
    enqueue --> place_entity["place_entity()"]
    place_entity --> rb_tree["Red-Black Tree"]
    rb_tree --> pick_next["pick_next_task_fair()"]
    pick_next --> pick_eevdf["__pick_eevdf()"]
    pick_eevdf --> running["Running Task"]
    running --> update_curr["update_curr()"]
    update_curr --> vruntime_update["vruntime += delta"]
    vruntime_update --> preempt_check["Preemption Check"]
    preempt_check --> dequeue["dequeue_task_fair()"]
    dequeue --> rb_remove["Remove from RB-tree"]
```

Sources: [kernel/sched/fair.c:5956](), [kernel/sched/fair.c:4906](), [kernel/sched/fair.c:1201]()

## EEVDF Algorithm Implementation

CFS uses the Earliest Eligible Virtual Deadline First (EEVDF) algorithm for task selection, providing both fairness and latency guarantees.

### Virtual Runtime and Deadlines

The scheduler maintains virtual runtime (`vruntime`) and deadlines for each scheduling entity:

| Field | Purpose | Location |
|-------|---------|----------|
| `se->vruntime` | Virtual runtime for fairness | [kernel/sched/fair.c:1213]() |
| `se->deadline` | Virtual deadline for latency | [kernel/sched/fair.c:1040]() |
| `se->slice` | Time slice allocation | [kernel/sched/fair.c:1035]() |
| `se->vlag` | Lag from average vruntime | [kernel/sched/fair.c:724]() |

### EEVDF Task Selection

```mermaid
graph TD
    pick_eevdf["__pick_eevdf()"] --> check_curr["Check current task"]
    check_curr --> curr_eligible{"Current eligible?"}
    curr_eligible -->|Yes| run_to_parity{"RUN_TO_PARITY?"}
    run_to_parity -->|Yes| return_curr["Return current"]
    curr_eligible -->|No| tree_search["Search RB-tree"]
    run_to_parity -->|No| tree_search
    
    tree_search --> check_eligible{"Node eligible?"}
    check_eligible -->|No| go_left["Go left"]
    check_eligible -->|Yes| update_best["Update best candidate"]
    update_best --> check_deadline{"Deadline == min_deadline?"}
    check_deadline -->|Yes| return_best["Return best"]
    check_deadline -->|No| continue_search["Continue search"]
    
    go_left --> check_eligible
    continue_search --> check_eligible
```

Sources: [kernel/sched/fair.c:881](), [kernel/sched/fair.c:744]()

### Virtual Runtime Calculation

Virtual runtime advances based on task execution time, weighted by task priority:

```mermaid
graph LR
    delta_exec["delta_exec"] --> calc_delta_fair["calc_delta_fair()"]
    calc_delta_fair --> weight_factor["Apply weight factor"]
    weight_factor --> vruntime_update["vruntime += weighted_delta"]
    vruntime_update --> update_deadline["update_deadline()"]
    update_deadline --> min_vruntime["update_min_vruntime()"]
```

The core calculation uses: `vruntime += calc_delta_fair(delta_exec, se)`

Sources: [kernel/sched/fair.c:311](), [kernel/sched/fair.c:1025](), [kernel/sched/fair.c:774]()

## Data Structures

### CFS Run Queue

The `struct cfs_rq` represents a CFS scheduling queue:

| Field | Purpose | Type |
|-------|---------|------|
| `tasks_timeline` | Red-black tree of tasks | `struct rb_root_cached` |
| `curr` | Currently running entity | `struct sched_entity *` |
| `nr_running` | Number of runnable tasks | `unsigned int` |
| `min_vruntime` | Minimum vruntime baseline | `u64` |
| `avg_vruntime` | Weighted average vruntime | `s64` |
| `avg_load` | Total weight of entities | `unsigned long` |

Sources: [kernel/sched/sched.h:541]()

### Scheduling Entity

The `struct sched_entity` represents a schedulable entity (task or group):

| Field | Purpose | Usage |
|-------|---------|-------|
| `vruntime` | Virtual runtime | Fairness tracking |
| `deadline` | Virtual deadline | EEVDF selection |
| `min_deadline` | Subtree minimum deadline | RB-tree augmentation |
| `load` | Task weight | Load balancing |
| `run_node` | RB-tree node | Tree organization |
| `on_rq` | Enqueue status | State tracking |

Sources: [include/linux/sched.h:548]()

## Group Scheduling and Cgroups

CFS supports hierarchical group scheduling through cgroups, allowing resource isolation and bandwidth control.

### Group Scheduling Architecture

```mermaid
graph TB
    subgraph "Root Group"
        root_tg["root_task_group"]
        root_cfs_rq["Root CFS RQ"]
        root_se["Root SE"]
    end
    
    subgraph "Child Group A"
        tg_a["task_group A"]
        cfs_rq_a["CFS RQ A"]
        se_a["SE A"]
        task_a1["Task A1"]
        task_a2["Task A2"]
    end
    
    subgraph "Child Group B"
        tg_b["task_group B"]
        cfs_rq_b["CFS RQ B"]
        se_b["SE B"]
        task_b1["Task B1"]
    end
    
    root_tg --> tg_a
    root_tg --> tg_b
    root_cfs_rq --> se_a
    root_cfs_rq --> se_b
    se_a --> cfs_rq_a
    se_b --> cfs_rq_b
    cfs_rq_a --> task_a1
    cfs_rq_a --> task_a2
    cfs_rq_b --> task_b1
```

Sources: [kernel/sched/sched.h:362](), [kernel/sched/fair.c:325]()

### CFS Bandwidth Control

CFS provides bandwidth throttling for cgroups through the CFS bandwidth controller:

| Parameter | Purpose | Default |
|-----------|---------|---------|
| `cpu.cfs_period_us` | Bandwidth period | 100ms |
| `cpu.cfs_quota_us` | Bandwidth quota | Unlimited |
| `cpu.cfs_burst_us` | Burst allowance | 0 |

Sources: [kernel/sched/sched.h:336](), [kernel/sched/fair.c:5138]()

## Load Balancing

CFS implements sophisticated load balancing across CPU cores and NUMA domains.

### Load Balancing Hierarchy

```mermaid
graph TD
    scheduler_tick["scheduler_tick()"] --> trigger_load_balance["trigger_load_balance()"]
    trigger_load_balance --> rebalance_domains["rebalance_domains()"]
    rebalance_domains --> load_balance["load_balance()"]
    
    load_balance --> find_busiest_group["find_busiest_group()"]
    find_busiest_group --> find_busiest_queue["find_busiest_queue()"]
    find_busiest_queue --> detach_tasks["detach_tasks()"]
    detach_tasks --> attach_tasks["attach_tasks()"]
    
    subgraph "Migration"
        can_migrate_task["can_migrate_task()"]
        set_task_rq["set_task_rq()"]
        migrate_task_rq_fair["migrate_task_rq_fair()"]
    end
    
    attach_tasks --> can_migrate_task
    can_migrate_task --> set_task_rq
    set_task_rq --> migrate_task_rq_fair
```

Sources: [kernel/sched/fair.c:10854](), [kernel/sched/fair.c:9416](), [kernel/sched/fair.c:8109]()

### NUMA Balancing

CFS includes NUMA-aware load balancing to optimize memory locality:

```mermaid
graph LR
    numa_fault["NUMA Page Fault"] --> task_numa_fault["task_numa_fault()"]
    task_numa_fault --> update_stats["Update NUMA stats"]
    update_stats --> numa_migrate_preferred["numa_migrate_preferred()"]
    numa_migrate_preferred --> migrate_swap["migrate_swap()"]
    
    periodic_balance["Periodic Balance"] --> task_numa_migrate["task_numa_migrate()"]
    task_numa_migrate --> find_numa_target["Find NUMA target"]
    find_numa_target --> migrate_swap
```

Sources: [kernel/sched/fair.c:1656](), [kernel/sched/fair.c:1773]()

## Configuration and Tunables

### System-wide Tunables

| Parameter | Purpose | Default | Location |
|-----------|---------|---------|----------|
| `sched_base_slice` | Base time slice | 0.75ms | [kernel/sched/fair.c:78]() |
| `sched_child_runs_first` | Child runs before parent | 0 | [kernel/sched/fair.c:85]() |
| `sched_migration_cost` | Migration cost threshold | 500s | [kernel/sched/fair.c:87]() |

### CFS Features

The scheduler supports various feature flags controlled through `/sys/kernel/debug/sched/features`:

| Feature | Purpose | Default |
|---------|---------|---------|
| `PLACE_LAG` | Preserve lag across sleep/wake | Enabled |
| `RUN_TO_PARITY` | Run task until vlag == deadline | Enabled |
| `NEXT_BUDDY` | Prefer recently woken task | Disabled |
| `WAKEUP_PREEMPTION` | Allow wakeup preemption | Enabled |

Sources: [kernel/sched/features.h:7](), [kernel/sched/features.h:16]()

### Per-Task Controls

Tasks can be controlled through various interfaces:

- **Nice values**: Priority adjustment (-20 to +19)
- **CPU affinity**: `sched_setaffinity()` system call
- **Cgroup membership**: CPU controller in cgroups v2
- **Scheduling policy**: `sched_setscheduler()` for SCHED_NORMAL/BATCH

Sources: [kernel/sched/core.c:7611](), [kernel/sched/core.c:8023]()

## Integration Points

CFS integrates with multiple kernel subsystems:

- **Memory management**: NUMA balancing and memory pressure
- **Control groups**: Resource isolation and bandwidth control  
- **CPU frequency**: `schedutil` governor integration
- **Power management**: CPU idle state selection
- **Pressure stall information**: PSI metrics for scheduling delays

Sources: [kernel/sched/fair.c:10854](), [kernel/sched/cpufreq_schedutil.c](), [kernel/sched/psi.c]()1e:T30ee,# Workqueue Subsystem

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [Documentation/admin-guide/kernel-parameters.txt](Documentation/admin-guide/kernel-parameters.txt)
- [Documentation/core-api/workqueue.rst](Documentation/core-api/workqueue.rst)
- [Documentation/translations/zh_CN/core-api/workqueue.rst](Documentation/translations/zh_CN/core-api/workqueue.rst)
- [include/linux/start_kernel.h](include/linux/start_kernel.h)
- [include/linux/workqueue.h](include/linux/workqueue.h)
- [init/main.c](init/main.c)
- [kernel/power/Makefile](kernel/power/Makefile)
- [kernel/power/process.c](kernel/power/process.c)
- [kernel/workqueue.c](kernel/workqueue.c)
- [kernel/workqueue_internal.h](kernel/workqueue_internal.h)
- [tools/workqueue/wq_dump.py](tools/workqueue/wq_dump.py)
- [tools/workqueue/wq_monitor.py](tools/workqueue/wq_monitor.py)

</details>



The workqueue subsystem provides an asynchronous task execution framework for deferred work processing in the Linux kernel. It enables kernel subsystems and drivers to queue work items (functions) to be executed later in process context by worker threads, with automatic concurrency management and CPU affinity control.

For information about process scheduling, see [CFS Scheduler](#3.1). For high-performance I/O mechanisms, see [io_uring Framework](#4.1).

## Architecture Overview

The workqueue subsystem implements a Concurrency Managed Workqueue (CMWQ) design that separates user-facing workqueues from backend worker pools. This architecture provides flexible concurrency levels while minimizing resource usage.

```mermaid
graph TB
    subgraph "User Interface Layer"
        WQ1["system_wq"] 
        WQ2["system_highpri_wq"]
        WQ3["system_unbound_wq"]
        WQ4["Custom Workqueues"]
    end
    
    subgraph "Work Items"
        WI1["work_struct"]
        WI2["delayed_work"] 
        WI3["rcu_work"]
    end
    
    subgraph "Backend Worker Pools"
        subgraph "Per-CPU Pools"
            CP1["cpu0_normal_pool"]
            CP2["cpu0_highpri_pool"]
            CP3["cpu1_normal_pool"]
            CP4["cpu1_highpri_pool"]
        end
        subgraph "Unbound Pools"
            UP1["unbound_pool_1"]
            UP2["unbound_pool_2"]
        end
    end
    
    subgraph "Worker Threads"
        W1["kworker/0:0"]
        W2["kworker/0:1H"]
        W3["kworker/1:0"]
        W4["kworker/u16:0"]
    end
    
    WQ1 --> WI1
    WQ2 --> WI2
    WQ3 --> WI3
    WQ4 --> WI1
    
    WI1 --> CP1
    WI2 --> CP2
    WI3 --> UP1
    
    CP1 --> W1
    CP2 --> W2
    CP3 --> W3
    UP1 --> W4
```

Sources: [kernel/workqueue.c:423-436](), [Documentation/core-api/workqueue.rst:85-92]()

## Core Data Structures

The workqueue subsystem is built around several key data structures that manage work items, queues, pools, and worker threads.

```mermaid
graph TD
    subgraph "workqueue_struct"
        WQ["workqueue_struct<br/>- name: char[24]<br/>- flags: unsigned int<br/>- cpu_pwq: per-cpu pwqs<br/>- mutex: struct mutex"]
    end
    
    subgraph "pool_workqueue" 
        PWQ["pool_workqueue<br/>- pool: worker_pool*<br/>- wq: workqueue_struct*<br/>- nr_active: int<br/>- max_active: int<br/>- inactive_works: list"]
    end
    
    subgraph "worker_pool"
        POOL["worker_pool<br/>- lock: raw_spinlock_t<br/>- cpu: int<br/>- worklist: list_head<br/>- nr_workers: int<br/>- nr_idle: int<br/>- busy_hash: hashtable"]
    end
    
    subgraph "worker"
        WORKER["worker<br/>- current_work: work_struct*<br/>- current_func: work_func_t<br/>- pool: worker_pool*<br/>- task: task_struct*<br/>- flags: unsigned int"]
    end
    
    subgraph "work_struct"
        WORK["work_struct<br/>- data: atomic_long_t<br/>- entry: list_head<br/>- func: work_func_t"]
    end
    
    WQ -->|"pwqs"| PWQ
    PWQ -->|"pool"| POOL
    POOL -->|"workers"| WORKER
    POOL -->|"worklist"| WORK
    WORKER -->|"current_work"| WORK
```

Sources: [kernel/workqueue.c:153-202](), [kernel/workqueue.c:227-268](), [kernel/workqueue.c:285-326](), [kernel/workqueue_internal.h:24-62](), [include/linux/workqueue.h:98-105]()

### Key Data Structure Details

**`work_struct`**: The fundamental unit representing a work item containing a function pointer and metadata. The `data` field encodes flags and pool/workqueue information.

**`workqueue_struct`**: Represents a workqueue that serves as a domain for work items, providing forward progress guarantees and flush semantics. Contains per-CPU pool workqueues.

**`worker_pool`**: Manages a pool of worker threads serving work items. Each CPU has two pools (normal and high priority), plus dynamic unbound pools.

**`worker`**: Represents a kernel thread (`kworker`) that executes work items from a pool's worklist.

Sources: [include/linux/workqueue.h:98-105](), [kernel/workqueue.c:285-326](), [kernel/workqueue.c:153-202](), [kernel/workqueue_internal.h:24-62]()

## Work Execution Flow

The workqueue subsystem follows a well-defined flow from work item queuing to execution, with automatic concurrency management.

```mermaid
sequenceDiagram
    participant Client as "Kernel Code"
    participant WQ as "workqueue_struct"
    participant PWQ as "pool_workqueue" 
    participant Pool as "worker_pool"
    participant Worker as "worker thread"
    
    Client->>WQ: queue_work(wq, work)
    WQ->>PWQ: determine target pwq
    PWQ->>Pool: add to pool->worklist
    
    alt need more workers
        Pool->>Pool: wake_up_worker()
        Pool->>Worker: wake existing idle worker
    else no idle workers
        Pool->>Pool: create_worker()
        Pool->>Worker: spawn new kworker thread
    end
    
    Worker->>Pool: grab work from worklist
    Worker->>Worker: execute work->func()
    Worker->>Pool: update concurrency counters
    
    alt no more work
        Worker->>Worker: go idle (with timeout)
    else too many idle workers  
        Worker->>Worker: destroy self
    end
```

Sources: [kernel/workqueue.c:1372-1425](), [kernel/workqueue.c:2438-2507](), [kernel/workqueue.c:827-857]()

### Concurrency Management

The workqueue subsystem implements sophisticated concurrency management to maintain optimal worker thread counts:

- **Need More Worker**: Determined by `need_more_worker()` - true if worklist is non-empty and no workers are running
- **Worker Creation**: New workers are created when existing workers can't handle the workload
- **Idle Worker Management**: Workers become idle when no work is available, with a timeout mechanism to destroy excess workers
- **CPU Intensive Detection**: Work items running longer than `wq_cpu_intensive_thresh_us` are excluded from concurrency management

Sources: [kernel/workqueue.c:827-857](), [kernel/workqueue.c:360-361]()

## Workqueue Types and Configuration

The workqueue subsystem supports different types of workqueues with various configuration flags to control behavior.

### System Workqueues

The kernel provides several pre-defined system workqueues for common use cases:

| Workqueue | Purpose | Characteristics |
|-----------|---------|-----------------|
| `system_wq` | General purpose work | Multi-CPU, short-running tasks |
| `system_highpri_wq` | High priority work | Elevated nice level, urgent tasks |
| `system_long_wq` | Long-running work | Suitable for tasks that may run longer |
| `system_unbound_wq` | Unbound work | Not tied to specific CPUs |
| `system_freezable_wq` | Freezable work | Participates in system suspend |
| `system_power_efficient_wq` | Power-aware work | May become unbound for power savings |

Sources: [kernel/workqueue.c:423-436](), [include/linux/workqueue.h:438-444]()

### Workqueue Flags

Workqueues can be configured with various flags that control their behavior:

- **`WQ_UNBOUND`**: Work items not bound to specific CPUs, served by unbound worker pools
- **`WQ_FREEZABLE`**: Workqueue participates in system freeze operations during suspend
- **`WQ_MEM_RECLAIM`**: Has rescue worker for memory reclaim scenarios, prevents deadlocks
- **`WQ_HIGHPRI`**: Uses high-priority worker pools with elevated nice levels
- **`WQ_CPU_INTENSIVE`**: Work items don't contribute to concurrency management
- **`WQ_POWER_EFFICIENT`**: May become unbound when power efficiency is enabled

Sources: [include/linux/workqueue.h:364-408](), [Documentation/core-api/workqueue.rst:164-218]()

### Affinity Scopes

Unbound workqueues support different CPU affinity scopes to optimize cache locality:

- **`WQ_AFFN_CPU`**: One worker pool per CPU
- **`WQ_AFFN_SMT`**: One pool per SMT (Simultaneous Multi-Threading) group  
- **`WQ_AFFN_CACHE`**: One pool per Last Level Cache boundary (default)
- **`WQ_AFFN_NUMA`**: One pool per NUMA node
- **`WQ_AFFN_SYSTEM`**: Single pool for entire system

Sources: [include/linux/workqueue.h:128-137](), [kernel/workqueue.c:334-351]()

## Integration with Kernel Systems

The workqueue subsystem integrates deeply with various kernel systems to provide comprehensive asynchronous execution support.

### Kernel Initialization

During kernel boot, the workqueue subsystem is initialized early to support other subsystems:

```mermaid
graph TD
    A["start_kernel()"] --> B["workqueue_init_early()"]
    B --> C["Create initial worker pools"]
    C --> D["workqueue_init()"] 
    D --> E["Create system workqueues"]
    E --> F["Start kworker threads"]
```

The initialization happens in two phases:
- **Early init**: Creates basic infrastructure before scheduler is fully operational
- **Full init**: Creates system workqueues and starts worker threads

Sources: [kernel/workqueue.c:6700-6780](), [init/main.c:875-1100]()

### Power Management Integration

The workqueue subsystem integrates with the power management framework to support system suspend and resume:

- **Freezing**: `freeze_workqueues_begin()` and `freeze_workqueues_busy()` coordinate with suspend operations
- **Thawing**: `thaw_workqueues()` resumes workqueue operations after resume
- **Freezable workqueues**: Marked with `WQ_FREEZABLE` participate in the freeze process

Sources: [kernel/power/process.c:47-48](), [kernel/power/process.c:62-63](), [kernel/power/process.c:195](), [kernel/power/process.c:224]()

### Scheduler Integration

Worker threads integrate with the kernel scheduler through hooks that enable concurrency management:

- **`wq_worker_running()`**: Called when worker thread starts running
- **`wq_worker_sleeping()`**: Called when worker thread goes to sleep
- **`wq_worker_tick()`**: Called on scheduler ticks to track CPU usage

Sources: [kernel/workqueue_internal.h:78-81]()

## Monitoring and Debugging

The workqueue subsystem provides comprehensive monitoring and debugging capabilities through various interfaces.

### Statistics and Monitoring

Each pool workqueue maintains detailed statistics accessible through monitoring tools:

- **PWQ_STAT_STARTED**: Number of work items that started execution
- **PWQ_STAT_COMPLETED**: Number of work items that completed execution  
- **PWQ_STAT_CPU_TIME**: Total CPU time consumed by the workqueue
- **PWQ_STAT_CPU_INTENSIVE**: Number of CPU intensive threshold violations
- **PWQ_STAT_CM_WAKEUP**: Concurrency management wakeups
- **PWQ_STAT_MAYDAY**: Rescue worker activations

Sources: [kernel/workqueue.c:208-219](), [tools/workqueue/wq_monitor.py:66-74]()

### Debugging Tools

The kernel provides several tools for monitoring and debugging workqueue behavior:

**`wq_monitor.py`**: Real-time monitoring of workqueue statistics and performance metrics

**`wq_dump.py`**: Dumps current workqueue configuration including affinity scopes, worker pools, and workqueue-to-pool mappings

**Sysfs Interface**: Workqueues marked with `WQ_SYSFS` expose configuration files under `/sys/devices/virtual/workqueue/`

**Tracing**: The workqueue subsystem supports tracing through the kernel's tracing infrastructure

Sources: [tools/workqueue/wq_monitor.py:1-176](), [tools/workqueue/wq_dump.py:1-178](), [Documentation/core-api/workqueue.rst:392-399]()

### Kernel Parameters

Several kernel parameters control workqueue behavior:

- **`workqueue.cpu_intensive_thresh_us`**: Threshold for detecting CPU-intensive work items
- **`workqueue.power_efficient`**: Enables power-efficient mode for workqueues
- **`workqueue.debug_force_rr_cpu`**: Forces round-robin CPU selection for debugging

Sources: [kernel/workqueue.c:360-361](), [kernel/workqueue.c:364-365](), [kernel/workqueue.c:396-400](), [Documentation/admin-guide/kernel-parameters.txt:1-100]()1f:T4948,# I/O Subsystems

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [Documentation/ABI/testing/debugfs-pktcdvd](Documentation/ABI/testing/debugfs-pktcdvd)
- [Documentation/ABI/testing/sysfs-bus-event_source-devices-iommu](Documentation/ABI/testing/sysfs-bus-event_source-devices-iommu)
- [Documentation/ABI/testing/sysfs-class-pktcdvd](Documentation/ABI/testing/sysfs-class-pktcdvd)
- [Documentation/ABI/testing/sysfs-kernel-iommu_groups](Documentation/ABI/testing/sysfs-kernel-iommu_groups)
- [block/Makefile](block/Makefile)
- [drivers/acpi/viot.c](drivers/acpi/viot.c)
- [drivers/block/Kconfig](drivers/block/Kconfig)
- [drivers/block/Makefile](drivers/block/Makefile)
- [drivers/block/pktcdvd.c](drivers/block/pktcdvd.c)
- [drivers/block/ublk_drv.c](drivers/block/ublk_drv.c)
- [drivers/gpu/drm/exynos/exynos_drm_dma.c](drivers/gpu/drm/exynos/exynos_drm_dma.c)
- [drivers/iommu/Kconfig](drivers/iommu/Kconfig)
- [drivers/iommu/Makefile](drivers/iommu/Makefile)
- [drivers/iommu/amd/Kconfig](drivers/iommu/amd/Kconfig)
- [drivers/iommu/amd/Makefile](drivers/iommu/amd/Makefile)
- [drivers/iommu/amd/amd_iommu.h](drivers/iommu/amd/amd_iommu.h)
- [drivers/iommu/amd/amd_iommu_types.h](drivers/iommu/amd/amd_iommu_types.h)
- [drivers/iommu/amd/init.c](drivers/iommu/amd/init.c)
- [drivers/iommu/amd/io_pgtable.c](drivers/iommu/amd/io_pgtable.c)
- [drivers/iommu/amd/io_pgtable_v2.c](drivers/iommu/amd/io_pgtable_v2.c)
- [drivers/iommu/amd/iommu.c](drivers/iommu/amd/iommu.c)
- [drivers/iommu/apple-dart.c](drivers/iommu/apple-dart.c)
- [drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3-sva.c](drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3-sva.c)
- [drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c](drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c)
- [drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.h](drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.h)
- [drivers/iommu/arm/arm-smmu/Makefile](drivers/iommu/arm/arm-smmu/Makefile)
- [drivers/iommu/arm/arm-smmu/arm-smmu-qcom-debug.c](drivers/iommu/arm/arm-smmu/arm-smmu-qcom-debug.c)
- [drivers/iommu/arm/arm-smmu/arm-smmu-qcom.c](drivers/iommu/arm/arm-smmu/arm-smmu-qcom.c)
- [drivers/iommu/arm/arm-smmu/arm-smmu-qcom.h](drivers/iommu/arm/arm-smmu/arm-smmu-qcom.h)
- [drivers/iommu/arm/arm-smmu/arm-smmu.c](drivers/iommu/arm/arm-smmu/arm-smmu.c)
- [drivers/iommu/arm/arm-smmu/arm-smmu.h](drivers/iommu/arm/arm-smmu/arm-smmu.h)
- [drivers/iommu/arm/arm-smmu/qcom_iommu.c](drivers/iommu/arm/arm-smmu/qcom_iommu.c)
- [drivers/iommu/dma-iommu.c](drivers/iommu/dma-iommu.c)
- [drivers/iommu/dma-iommu.h](drivers/iommu/dma-iommu.h)
- [drivers/iommu/exynos-iommu.c](drivers/iommu/exynos-iommu.c)
- [drivers/iommu/fsl_pamu.c](drivers/iommu/fsl_pamu.c)
- [drivers/iommu/fsl_pamu_domain.c](drivers/iommu/fsl_pamu_domain.c)
- [drivers/iommu/intel/Kconfig](drivers/iommu/intel/Kconfig)
- [drivers/iommu/intel/Makefile](drivers/iommu/intel/Makefile)
- [drivers/iommu/intel/cap_audit.c](drivers/iommu/intel/cap_audit.c)
- [drivers/iommu/intel/dmar.c](drivers/iommu/intel/dmar.c)
- [drivers/iommu/intel/iommu.c](drivers/iommu/intel/iommu.c)
- [drivers/iommu/intel/iommu.h](drivers/iommu/intel/iommu.h)
- [drivers/iommu/intel/pasid.c](drivers/iommu/intel/pasid.c)
- [drivers/iommu/intel/pasid.h](drivers/iommu/intel/pasid.h)
- [drivers/iommu/intel/perfmon.c](drivers/iommu/intel/perfmon.c)
- [drivers/iommu/intel/perfmon.h](drivers/iommu/intel/perfmon.h)
- [drivers/iommu/intel/svm.c](drivers/iommu/intel/svm.c)
- [drivers/iommu/io-pgfault.c](drivers/iommu/io-pgfault.c)
- [drivers/iommu/io-pgtable-arm-v7s.c](drivers/iommu/io-pgtable-arm-v7s.c)
- [drivers/iommu/io-pgtable-arm.c](drivers/iommu/io-pgtable-arm.c)
- [drivers/iommu/io-pgtable-dart.c](drivers/iommu/io-pgtable-dart.c)
- [drivers/iommu/io-pgtable.c](drivers/iommu/io-pgtable.c)
- [drivers/iommu/iommu-priv.h](drivers/iommu/iommu-priv.h)
- [drivers/iommu/iommu-sva.c](drivers/iommu/iommu-sva.c)
- [drivers/iommu/iommu-sva.h](drivers/iommu/iommu-sva.h)
- [drivers/iommu/iommu.c](drivers/iommu/iommu.c)
- [drivers/iommu/iova.c](drivers/iommu/iova.c)
- [drivers/iommu/ipmmu-vmsa.c](drivers/iommu/ipmmu-vmsa.c)
- [drivers/iommu/msm_iommu.c](drivers/iommu/msm_iommu.c)
- [drivers/iommu/mtk_iommu.c](drivers/iommu/mtk_iommu.c)
- [drivers/iommu/mtk_iommu_v1.c](drivers/iommu/mtk_iommu_v1.c)
- [drivers/iommu/omap-iommu.c](drivers/iommu/omap-iommu.c)
- [drivers/iommu/rockchip-iommu.c](drivers/iommu/rockchip-iommu.c)
- [drivers/iommu/sprd-iommu.c](drivers/iommu/sprd-iommu.c)
- [drivers/iommu/sun50i-iommu.c](drivers/iommu/sun50i-iommu.c)
- [drivers/iommu/tegra-gart.c](drivers/iommu/tegra-gart.c)
- [drivers/iommu/tegra-smmu.c](drivers/iommu/tegra-smmu.c)
- [drivers/iommu/virtio-iommu.c](drivers/iommu/virtio-iommu.c)
- [drivers/nvme/host/Makefile](drivers/nvme/host/Makefile)
- [drivers/nvme/host/core.c](drivers/nvme/host/core.c)
- [drivers/nvme/host/fabrics.c](drivers/nvme/host/fabrics.c)
- [drivers/nvme/host/fabrics.h](drivers/nvme/host/fabrics.h)
- [drivers/nvme/host/hwmon.c](drivers/nvme/host/hwmon.c)
- [drivers/nvme/host/ioctl.c](drivers/nvme/host/ioctl.c)
- [drivers/nvme/host/multipath.c](drivers/nvme/host/multipath.c)
- [drivers/nvme/host/nvme.h](drivers/nvme/host/nvme.h)
- [drivers/nvme/host/pci.c](drivers/nvme/host/pci.c)
- [drivers/nvme/host/rdma.c](drivers/nvme/host/rdma.c)
- [drivers/nvme/host/sysfs.c](drivers/nvme/host/sysfs.c)
- [drivers/nvme/host/tcp.c](drivers/nvme/host/tcp.c)
- [drivers/nvme/host/trace.h](drivers/nvme/host/trace.h)
- [drivers/nvme/target/admin-cmd.c](drivers/nvme/target/admin-cmd.c)
- [drivers/nvme/target/configfs.c](drivers/nvme/target/configfs.c)
- [drivers/nvme/target/core.c](drivers/nvme/target/core.c)
- [drivers/nvme/target/fc.c](drivers/nvme/target/fc.c)
- [drivers/nvme/target/fcloop.c](drivers/nvme/target/fcloop.c)
- [drivers/nvme/target/io-cmd-file.c](drivers/nvme/target/io-cmd-file.c)
- [drivers/nvme/target/loop.c](drivers/nvme/target/loop.c)
- [drivers/nvme/target/nvmet.h](drivers/nvme/target/nvmet.h)
- [drivers/nvme/target/passthru.c](drivers/nvme/target/passthru.c)
- [drivers/nvme/target/rdma.c](drivers/nvme/target/rdma.c)
- [drivers/nvme/target/tcp.c](drivers/nvme/target/tcp.c)
- [drivers/nvme/target/zns.c](drivers/nvme/target/zns.c)
- [include/linux/amd-iommu.h](include/linux/amd-iommu.h)
- [include/linux/io-pgtable.h](include/linux/io-pgtable.h)
- [include/linux/io_uring.h](include/linux/io_uring.h)
- [include/linux/io_uring_types.h](include/linux/io_uring_types.h)
- [include/linux/iommu.h](include/linux/iommu.h)
- [include/linux/iova.h](include/linux/iova.h)
- [include/linux/nvme-tcp.h](include/linux/nvme-tcp.h)
- [include/linux/nvme.h](include/linux/nvme.h)
- [include/linux/pktcdvd.h](include/linux/pktcdvd.h)
- [include/linux/sched/mm.h](include/linux/sched/mm.h)
- [include/soc/mediatek/smi.h](include/soc/mediatek/smi.h)
- [include/uapi/linux/io_uring.h](include/uapi/linux/io_uring.h)
- [include/uapi/linux/pktcdvd.h](include/uapi/linux/pktcdvd.h)
- [include/uapi/linux/ublk_cmd.h](include/uapi/linux/ublk_cmd.h)
- [io_uring/advise.c](io_uring/advise.c)
- [io_uring/alloc_cache.h](io_uring/alloc_cache.h)
- [io_uring/cancel.c](io_uring/cancel.c)
- [io_uring/cancel.h](io_uring/cancel.h)
- [io_uring/epoll.c](io_uring/epoll.c)
- [io_uring/filetable.c](io_uring/filetable.c)
- [io_uring/filetable.h](io_uring/filetable.h)
- [io_uring/fs.c](io_uring/fs.c)
- [io_uring/io_uring.c](io_uring/io_uring.c)
- [io_uring/io_uring.h](io_uring/io_uring.h)
- [io_uring/kbuf.c](io_uring/kbuf.c)
- [io_uring/kbuf.h](io_uring/kbuf.h)
- [io_uring/msg_ring.c](io_uring/msg_ring.c)
- [io_uring/net.c](io_uring/net.c)
- [io_uring/net.h](io_uring/net.h)
- [io_uring/notif.c](io_uring/notif.c)
- [io_uring/notif.h](io_uring/notif.h)
- [io_uring/opdef.c](io_uring/opdef.c)
- [io_uring/poll.c](io_uring/poll.c)
- [io_uring/poll.h](io_uring/poll.h)
- [io_uring/rsrc.c](io_uring/rsrc.c)
- [io_uring/rsrc.h](io_uring/rsrc.h)
- [io_uring/rw.c](io_uring/rw.c)
- [io_uring/rw.h](io_uring/rw.h)
- [io_uring/splice.c](io_uring/splice.c)
- [io_uring/statx.c](io_uring/statx.c)
- [io_uring/sync.c](io_uring/sync.c)
- [io_uring/timeout.c](io_uring/timeout.c)
- [io_uring/uring_cmd.c](io_uring/uring_cmd.c)
- [io_uring/uring_cmd.h](io_uring/uring_cmd.h)
- [io_uring/xattr.c](io_uring/xattr.c)

</details>



## Purpose and Scope

This document covers the high-performance I/O subsystems within the WSL2 Linux kernel, focusing on asynchronous I/O frameworks, hardware abstraction, and storage interfaces. The I/O subsystems provide efficient data transfer mechanisms between user applications, kernel space, and hardware devices while ensuring security and isolation.

The primary components covered include the `io_uring` asynchronous I/O framework, IOMMU (Input-Output Memory Management Unit) hardware abstraction layer, and high-speed storage drivers like NVMe. For process scheduling and workqueue management, see [Process Management and Scheduling](#3). For specific filesystem implementations, see [File Systems](#5).

## High-Level Architecture

```mermaid
flowchart TD
    subgraph "User Space"
        APP["Applications"]
        LIBURING["liburing"]
    end
    
    subgraph "Kernel I/O Layer"
        IOURING["io_uring Framework<br/>io_uring/io_uring.c"]
        VFS["Virtual File System"]
        BLOCK["Block Layer"]
    end
    
    subgraph "Hardware Abstraction"
        IOMMU_CORE["IOMMU Core<br/>drivers/iommu/iommu.c"]
        INTEL_IOMMU["Intel VT-d<br/>drivers/iommu/intel/iommu.c"]
        AMD_IOMMU["AMD-Vi<br/>drivers/iommu/amd/iommu.c"]
        ARM_SMMU["ARM SMMU<br/>drivers/iommu/arm/arm-smmu-v3/"]
    end
    
    subgraph "Storage Drivers"
        NVME_PCI["NVMe PCI<br/>drivers/nvme/host/pci.c"]
        NVME_CORE["NVMe Core<br/>drivers/nvme/host/core.c"]
        NVME_TCP["NVMe-oF TCP<br/>drivers/nvme/host/tcp.c"]
    end
    
    subgraph "Hardware"
        NVME_HW["NVMe SSDs"]
        DMA_HW["DMA Controllers"]
        NETWORK["Network Adapters"]
    end
    
    APP --> LIBURING
    LIBURING --> IOURING
    IOURING --> VFS
    IOURING --> BLOCK
    BLOCK --> NVME_CORE
    NVME_CORE --> NVME_PCI
    NVME_CORE --> NVME_TCP
    
    IOMMU_CORE --> INTEL_IOMMU
    IOMMU_CORE --> AMD_IOMMU
    IOMMU_CORE --> ARM_SMMU
    
    INTEL_IOMMU --> DMA_HW
    NVME_PCI --> NVME_HW
    NVME_TCP --> NETWORK
```

Sources: [io_uring/io_uring.c:1-4800](), [drivers/iommu/iommu.c:1-500](), [drivers/nvme/host/core.c:1-500]()

## io_uring Framework

The `io_uring` subsystem provides a high-performance asynchronous I/O interface using shared memory rings between user and kernel space, eliminating the overhead of traditional system calls for I/O operations.

### Core Architecture

```mermaid
flowchart TD
    subgraph "User Space"
        APP["Application"]
        SQ["Submission Queue<br/>io_uring_sqe"]
        CQ["Completion Queue<br/>io_uring_cqe"]
    end
    
    subgraph "Kernel Space"
        CTX["io_ring_ctx"]
        KIOCB["io_kiocb"]
        RSRC["Resource Management<br/>io_rsrc_data"]
        SQPOLL["SQ Poll Thread"]
    end
    
    subgraph "I/O Operations"
        RW["Read/Write<br/>io_uring/rw.c"]
        NET["Network I/O<br/>io_uring/net.c"]
        POLL["Polling<br/>io_uring/poll.c"]
    end
    
    APP --> SQ
    SQ --> CTX
    CTX --> KIOCB
    KIOCB --> RW
    KIOCB --> NET
    KIOCB --> POLL
    RW --> CQ
    NET --> CQ
    POLL --> CQ
    CQ --> APP
    
    CTX --> RSRC
    CTX --> SQPOLL
```

The core structure `io_ring_ctx` manages the entire I/O ring context, defined in [io_uring/io_uring.c:284-349](). Key operations are handled through `io_kiocb` structures that represent individual I/O requests.

### Resource Management

The `io_rsrc_data` structure manages registered files and buffers to avoid repeated setup costs:

- **File Registration**: Files are registered once and referenced by index via `io_fixed_file_slot`
- **Buffer Registration**: Memory buffers are pinned and mapped through `io_mapped_ubuf` structures
- **Resource Nodes**: `io_rsrc_node` provides reference counting and cleanup coordination

Sources: [io_uring/rsrc.c:19-50](), [io_uring/rsrc.h:20-58](), [io_uring/io_uring.c:284-349]()

### Operation Types

The framework supports multiple I/O operation types through specialized handlers:

| Operation Type | Handler File | Key Functions |
|---|---|---|
| File I/O | `io_uring/rw.c` | `io_read`, `io_write` |
| Network I/O | `io_uring/net.c` | `io_sendmsg`, `io_recvmsg` |
| Polling | `io_uring/poll.c` | `io_poll_add`, `io_poll_wake` |

Sources: [io_uring/rw.c:23-30](), [io_uring/net.c:53-72](), [io_uring/poll.c:1-50]()

## IOMMU Subsystem

The IOMMU subsystem provides hardware-level DMA remapping, device isolation, and memory protection. It abstracts different IOMMU implementations under a common interface defined in `drivers/iommu/iommu.c`.

### Core Framework

```mermaid
flowchart TD
    subgraph "Generic IOMMU Layer"
        IOMMU_DEVICE["iommu_device"]
        IOMMU_GROUP["iommu_group"]
        IOMMU_DOMAIN["iommu_domain"]
        IOMMU_OPS["iommu_ops"]
    end
    
    subgraph "Intel VT-d Implementation"
        INTEL_IOMMU["intel_iommu"]
        DMAR_DOMAIN["dmar_domain"]
        CONTEXT_ENTRY["context_entry"]
        DMA_PTE["dma_pte"]
    end
    
    subgraph "AMD-Vi Implementation"
        AMD_IOMMU["amd_iommu"]
        PROTECTION_DOMAIN["protection_domain"]
        DEV_TABLE_ENTRY["dev_table_entry"]
        IOMMU_CMD["iommu_cmd"]
    end
    
    subgraph "ARM SMMU Implementation"
        ARM_SMMU["arm_smmu_device"]
        ARM_SMMU_DOMAIN["arm_smmu_domain"]
        ARM_SMMU_MASTER["arm_smmu_master"]
    end
    
    IOMMU_DEVICE --> IOMMU_OPS
    IOMMU_GROUP --> IOMMU_DOMAIN
    
    IOMMU_OPS --> INTEL_IOMMU
    IOMMU_OPS --> AMD_IOMMU  
    IOMMU_OPS --> ARM_SMMU
    
    INTEL_IOMMU --> DMAR_DOMAIN
    DMAR_DOMAIN --> CONTEXT_ENTRY
    CONTEXT_ENTRY --> DMA_PTE
    
    AMD_IOMMU --> PROTECTION_DOMAIN
    PROTECTION_DOMAIN --> DEV_TABLE_ENTRY
    AMD_IOMMU --> IOMMU_CMD
    
    ARM_SMMU --> ARM_SMMU_DOMAIN
    ARM_SMMU --> ARM_SMMU_MASTER
```

### Intel VT-d Implementation

The Intel implementation uses a hierarchical page table structure:

- **Root Entry**: Points to context tables for PCI bus/device/function
- **Context Entry**: Contains page table root and domain information  
- **Page Tables**: Multi-level page tables for address translation

Key functions include `intel_iommu_map` and `intel_iommu_unmap` for managing address translations, implemented in [drivers/iommu/intel/iommu.c:4500-4600]().

### AMD-Vi Implementation  

AMD's implementation uses command queues and event logs:

- **Command Ring**: Circular buffer for issuing IOMMU commands
- **Event Log**: Records IOMMU faults and events
- **Device Table**: Per-device configuration and domain assignment

The `amd_iommu_ops` structure provides the interface implementation in [drivers/iommu/amd/iommu.c:2100-2200]().

Sources: [drivers/iommu/iommu.c:244-283](), [drivers/iommu/intel/iommu.c:122-200](), [drivers/iommu/amd/iommu.c:67-83](), [include/linux/iommu.h:34-61]()

## Storage Driver Integration

### NVMe Driver Architecture

The NVMe driver provides high-performance access to NVMe storage devices through multiple transport mechanisms.

```mermaid
flowchart TD
    subgraph "NVMe Core Layer"
        NVME_CTRL["nvme_ctrl"]
        NVME_NS["nvme_ns"] 
        NVME_QUEUE["nvme_queue"]
        BLK_MQ["blk_mq integration"]
    end
    
    subgraph "Transport Layer"
        PCI_TRANSPORT["PCI Transport<br/>nvme_dev"]
        TCP_TRANSPORT["TCP Transport<br/>nvme_tcp_ctrl"]
        FABRICS["NVMe-oF"]
    end
    
    subgraph "Hardware Interface"
        NVME_PCI_HW["PCIe NVMe Controller"]
        NVME_TCP_HW["Remote NVMe Target"]
        DMA_ENGINE["DMA Engine"]
    end
    
    NVME_CTRL --> NVME_NS
    NVME_CTRL --> NVME_QUEUE
    NVME_QUEUE --> BLK_MQ
    
    NVME_CTRL --> PCI_TRANSPORT
    NVME_CTRL --> TCP_TRANSPORT
    TCP_TRANSPORT --> FABRICS
    
    PCI_TRANSPORT --> NVME_PCI_HW
    TCP_TRANSPORT --> NVME_TCP_HW
    PCI_TRANSPORT --> DMA_ENGINE
```

### PCI Transport Implementation

The PCI transport uses direct memory access and interrupt handling:

- **Submission Queues**: Ring buffers for commands in host memory
- **Completion Queues**: Ring buffers for completions  
- **MSI-X Interrupts**: Per-queue interrupt vectors for completion notification
- **DMA Mapping**: Direct memory access for data transfers

The `nvme_dev` structure encapsulates the PCI device state in [drivers/nvme/host/pci.c:118-163]().

### Command Processing Pipeline

```mermaid
sequenceDiagram
    participant App as "Application"
    participant BLK as "Block Layer" 
    participant NVME as "NVMe Core"
    participant PCI as "PCI Transport"
    participant HW as "NVMe Hardware"
    
    App->>BLK: submit_bio()
    BLK->>NVME: nvme_queue_rq()
    NVME->>PCI: nvme_pci_submit_cmd()
    PCI->>HW: Write to SQ doorbell
    HW-->>PCI: MSI-X interrupt
    PCI->>NVME: nvme_pci_complete_rq()
    NVME->>BLK: blk_mq_complete_request()
    BLK-->>App: bio completion
```

Sources: [drivers/nvme/host/core.c:102-150](), [drivers/nvme/host/pci.c:118-163](), [drivers/nvme/host/tcp.c:100-200]()

## System Integration and Data Flow

### DMA and Memory Management Integration

The I/O subsystems integrate closely with memory management and DMA subsystems:

```mermaid
flowchart LR
    subgraph "Application Layer"
        USER_BUF["User Buffer"]
        IO_SUBMIT["io_uring_enter()"]
    end
    
    subgraph "Kernel I/O Layer"  
        IO_RING["io_ring_ctx"]
        BIO["bio structure"]
        SG_TABLE["Scatter-Gather List"]
    end
    
    subgraph "Memory Management"
        PIN_PAGES["pin_user_pages()"]
        DMA_MAP["dma_map_sg()"]
        IOMMU_MAP["iommu_map()"]
    end
    
    subgraph "Hardware"
        DMA_CTRL["DMA Controller"]
        STORAGE["Storage Device"]
    end
    
    USER_BUF --> IO_SUBMIT
    IO_SUBMIT --> IO_RING
    IO_RING --> BIO
    BIO --> SG_TABLE
    SG_TABLE --> PIN_PAGES
    PIN_PAGES --> DMA_MAP
    DMA_MAP --> IOMMU_MAP
    IOMMU_MAP --> DMA_CTRL
    DMA_CTRL --> STORAGE
```

### Performance Optimizations

Key performance optimizations across the I/O subsystems include:

- **Zero-Copy Operations**: Direct DMA between user buffers and devices
- **Polling Mode**: Bypass interrupt-driven completion for low latency
- **Queue Depth**: Multiple outstanding operations per queue
- **CPU Affinity**: Pin queues to specific CPU cores for cache locality

The integration ensures efficient data flow while maintaining security boundaries through IOMMU translation and access controls.

Sources: [io_uring/io_uring.c:178-205](), [drivers/iommu/iommu.c:1825-1900](), [drivers/nvme/host/pci.c:440-467]()20:T3698,# io_uring Framework

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [Documentation/ABI/testing/debugfs-pktcdvd](Documentation/ABI/testing/debugfs-pktcdvd)
- [Documentation/ABI/testing/sysfs-class-pktcdvd](Documentation/ABI/testing/sysfs-class-pktcdvd)
- [drivers/block/Kconfig](drivers/block/Kconfig)
- [drivers/block/Makefile](drivers/block/Makefile)
- [drivers/block/pktcdvd.c](drivers/block/pktcdvd.c)
- [drivers/block/ublk_drv.c](drivers/block/ublk_drv.c)
- [include/linux/io_uring.h](include/linux/io_uring.h)
- [include/linux/io_uring_types.h](include/linux/io_uring_types.h)
- [include/linux/pktcdvd.h](include/linux/pktcdvd.h)
- [include/uapi/linux/io_uring.h](include/uapi/linux/io_uring.h)
- [include/uapi/linux/pktcdvd.h](include/uapi/linux/pktcdvd.h)
- [include/uapi/linux/ublk_cmd.h](include/uapi/linux/ublk_cmd.h)
- [io_uring/advise.c](io_uring/advise.c)
- [io_uring/alloc_cache.h](io_uring/alloc_cache.h)
- [io_uring/cancel.c](io_uring/cancel.c)
- [io_uring/cancel.h](io_uring/cancel.h)
- [io_uring/epoll.c](io_uring/epoll.c)
- [io_uring/filetable.c](io_uring/filetable.c)
- [io_uring/filetable.h](io_uring/filetable.h)
- [io_uring/fs.c](io_uring/fs.c)
- [io_uring/io_uring.c](io_uring/io_uring.c)
- [io_uring/io_uring.h](io_uring/io_uring.h)
- [io_uring/kbuf.c](io_uring/kbuf.c)
- [io_uring/kbuf.h](io_uring/kbuf.h)
- [io_uring/msg_ring.c](io_uring/msg_ring.c)
- [io_uring/net.c](io_uring/net.c)
- [io_uring/net.h](io_uring/net.h)
- [io_uring/notif.c](io_uring/notif.c)
- [io_uring/notif.h](io_uring/notif.h)
- [io_uring/opdef.c](io_uring/opdef.c)
- [io_uring/poll.c](io_uring/poll.c)
- [io_uring/poll.h](io_uring/poll.h)
- [io_uring/rsrc.c](io_uring/rsrc.c)
- [io_uring/rsrc.h](io_uring/rsrc.h)
- [io_uring/rw.c](io_uring/rw.c)
- [io_uring/rw.h](io_uring/rw.h)
- [io_uring/splice.c](io_uring/splice.c)
- [io_uring/statx.c](io_uring/statx.c)
- [io_uring/sync.c](io_uring/sync.c)
- [io_uring/timeout.c](io_uring/timeout.c)
- [io_uring/uring_cmd.c](io_uring/uring_cmd.c)
- [io_uring/uring_cmd.h](io_uring/uring_cmd.h)
- [io_uring/xattr.c](io_uring/xattr.c)

</details>



## Purpose and Scope

The io_uring framework provides a high-performance asynchronous I/O interface that enables applications to efficiently submit I/O operations and receive completions through shared memory rings. This framework serves as a unified interface for various I/O operations including file I/O, network operations, polling, and other kernel operations, designed to minimize system call overhead and maximize throughput.

This document covers the core io_uring implementation, shared memory ring mechanics, and resource management. For network-specific operations using io_uring, see the networking integration. For block device integration, see [NVMe Storage Driver](#4.3). For process management aspects, see [Workqueue Subsystem](#3.2).

## Architecture Overview

io_uring implements a shared memory architecture where applications and the kernel communicate through two lock-free rings: a Submission Queue (SQ) for submitting I/O requests and a Completion Queue (CQ) for receiving completions.

### Shared Memory Ring Architecture

```mermaid
graph TB
    subgraph "User Space"
        APP["Application"]
        LIBURING["liburing Library"]
        SQ_TAIL["SQ Tail<br/>(App Controls)"]
        CQ_HEAD["CQ Head<br/>(App Controls)"]
    end
    
    subgraph "Shared Memory Region"
        RINGS["io_rings Structure"]
        SQ_ARRAY["SQ Index Array"]
        SQE_RING["SQE Ring<br/>(io_uring_sqe[])"]
        CQE_RING["CQE Ring<br/>(io_uring_cqe[])"]
    end
    
    subgraph "Kernel Space"
        IO_RING_CTX["io_ring_ctx"]
        SQ_HEAD["SQ Head<br/>(Kernel Controls)"]
        CQ_TAIL["CQ Tail<br/>(Kernel Controls)"]
        IO_SUBMIT["io_submit_sqes()"]
        IO_COMPLETE["io_req_complete_post()"]
    end
    
    APP --> LIBURING
    LIBURING --> SQ_TAIL
    LIBURING --> CQ_HEAD
    
    SQ_TAIL -.-> RINGS
    CQ_HEAD -.-> RINGS
    RINGS --> SQ_ARRAY
    RINGS --> CQE_RING
    SQ_ARRAY --> SQE_RING
    
    SQ_HEAD -.-> RINGS
    CQ_TAIL -.-> RINGS
    IO_RING_CTX --> IO_SUBMIT
    IO_RING_CTX --> IO_COMPLETE
    IO_SUBMIT --> SQE_RING
    IO_COMPLETE --> CQE_RING
```

Sources: [io_uring/io_uring.c:1-100](), [include/linux/io_uring_types.h:83-152](), [include/uapi/linux/io_uring.h:71-151]()

## Core Data Structures

### Ring Context and Request Management

```mermaid
graph TB
    subgraph "io_ring_ctx"
        CTX_FLAGS["flags<br/>setup options"]
        CTX_RINGS["rings*<br/> io_rings"]
        CTX_SUBMIT["submit_state<br/> io_submit_state"]
        CTX_RSRC["rsrc_node*<br/> io_rsrc_node"]
        CTX_FILE_TABLE["file_table<br/> io_file_table"]
        CTX_USER_BUFS["user_bufs**<br/> io_mapped_ubuf"]
    end
    
    subgraph "io_kiocb"
        REQ_OPCODE["opcode<br/>IORING_OP_*"]
        REQ_FLAGS["flags<br/>REQ_F_*"]
        REQ_CQE["cqe<br/>result data"]
        REQ_FILE["file*<br/>target file"]
        REQ_CTX["ctx*<br/> io_ring_ctx"]
    end
    
    subgraph "io_rings"
        RINGS_SQ["sq<br/>submission queue"]
        RINGS_CQ["cq<br/>completion queue"]
        RINGS_SQ_MASK["sq_ring_mask"]
        RINGS_CQ_MASK["cq_ring_mask"]
        RINGS_CQES["cqes[]<br/>completion entries"]
    end
    
    CTX_RINGS --> RINGS_SQ
    CTX_RINGS --> RINGS_CQ
    CTX_RINGS --> RINGS_CQES
    REQ_CTX --> CTX_FLAGS
    REQ_CTX --> CTX_SUBMIT
```

Sources: [include/linux/io_uring_types.h:196-342](), [io_uring/io_uring.c:284-349]()

## Submission and Completion Flow

### Request Lifecycle

```mermaid
graph TD
    SQE_SUBMIT["io_uring_enter()<br/>syscall"] --> IO_SUBMIT_SQES["io_submit_sqes()"]
    IO_SUBMIT_SQES --> IO_GET_SQE["get SQE from ring"]
    IO_GET_SQE --> IO_ALLOC_REQ["io_alloc_req()"]
    IO_ALLOC_REQ --> IO_INIT_REQ["io_init_req()"]
    IO_INIT_REQ --> IO_PREP_ASYNC["io_prep_async_work()"]
    
    IO_PREP_ASYNC --> IO_QUEUE_SQE["io_queue_sqe()"]
    IO_QUEUE_SQE --> SYNC_PATH{"sync execution?"}
    
    SYNC_PATH -->|Yes| IO_ISSUE_SQE["io_issue_sqe()"]
    SYNC_PATH -->|No| IO_QUEUE_IOWQ["io_queue_iowq()"]
    
    IO_ISSUE_SQE --> IO_COMPLETE["io_req_complete_post()"]
    IO_QUEUE_IOWQ --> IO_WQ_SUBMIT["io-wq execution"]
    IO_WQ_SUBMIT --> IO_COMPLETE
    
    IO_COMPLETE --> IO_FILL_CQE["io_fill_cqe_req()"]
    IO_FILL_CQE --> IO_COMMIT_CQRING["io_commit_cqring()"]
    IO_COMMIT_CQRING --> IO_CQRING_WAKE["io_cqring_wake()"]
```

Sources: [io_uring/io_uring.c:2676-2789](), [io_uring/io_uring.c:993-1056](), [io_uring/io_uring.c:623-670]()

### Operation Dispatch

```mermaid
graph LR
    subgraph "Operation Types"
        IORING_OP_READ["IORING_OP_READ<br/>io_read()"]
        IORING_OP_WRITE["IORING_OP_WRITE<br/>io_write()"]
        IORING_OP_READV["IORING_OP_READV<br/>io_readv()"]
        IORING_OP_WRITEV["IORING_OP_WRITEV<br/>io_writev()"]
        IORING_OP_SENDMSG["IORING_OP_SENDMSG<br/>io_sendmsg()"]
        IORING_OP_RECVMSG["IORING_OP_RECVMSG<br/>io_recvmsg()"]
        IORING_OP_POLL_ADD["IORING_OP_POLL_ADD<br/>io_poll_add()"]
    end
    
    subgraph "Issue Handlers"
        IO_ISSUE_DEFS["io_issue_defs[]<br/>dispatch table"]
        IO_ISSUE_SQE["io_issue_sqe()"]
    end
    
    subgraph "Implementation Modules"
        RW_MODULE["io_uring/rw.c<br/>read/write ops"]
        NET_MODULE["io_uring/net.c<br/>network ops"]
        POLL_MODULE["io_uring/poll.c<br/>polling ops"]
    end
    
    IO_ISSUE_SQE --> IO_ISSUE_DEFS
    IO_ISSUE_DEFS --> IORING_OP_READ
    IO_ISSUE_DEFS --> IORING_OP_SENDMSG
    IO_ISSUE_DEFS --> IORING_OP_POLL_ADD
    
    IORING_OP_READ --> RW_MODULE
    IORING_OP_SENDMSG --> NET_MODULE
    IORING_OP_POLL_ADD --> POLL_MODULE
```

Sources: [io_uring/opdef.c:1-469](), [io_uring/rw.c:76-124](), [io_uring/net.c:403-463](), [io_uring/poll.c:1-50]()

## Resource Management

### File and Buffer Registration

```mermaid
graph TB
    subgraph "Resource Registration"
        IO_REGISTER["io_uring_register()<br/>syscall"]
        REG_FILES["IORING_REGISTER_FILES<br/>io_sqe_files_register()"]
        REG_BUFFERS["IORING_REGISTER_BUFFERS<br/>io_sqe_buffers_register()"]
    end
    
    subgraph "File Management"
        FILE_TABLE["io_file_table"]
        FIXED_FILES["fixed_files[]<br/>io_fixed_file"]
        FILE_BITMAP["bitmap<br/>allocation tracking"]
    end
    
    subgraph "Buffer Management"
        USER_BUFS["user_bufs[]<br/>io_mapped_ubuf*"]
        MAPPED_UBUF["io_mapped_ubuf<br/>pinned pages"]
        BIO_VEC["bvec[]<br/>bio_vec array"]
    end
    
    subgraph "Resource Lifecycle"
        RSRC_NODE["io_rsrc_node"]
        RSRC_DATA["io_rsrc_data"]
        RSRC_PUT["io_rsrc_put<br/>cleanup handler"]
    end
    
    IO_REGISTER --> REG_FILES
    IO_REGISTER --> REG_BUFFERS
    
    REG_FILES --> FILE_TABLE
    FILE_TABLE --> FIXED_FILES
    FILE_TABLE --> FILE_BITMAP
    
    REG_BUFFERS --> USER_BUFS
    USER_BUFS --> MAPPED_UBUF
    MAPPED_UBUF --> BIO_VEC
    
    FIXED_FILES --> RSRC_NODE
    MAPPED_UBUF --> RSRC_NODE
    RSRC_NODE --> RSRC_DATA
    RSRC_NODE --> RSRC_PUT
```

Sources: [io_uring/rsrc.c:523-553](), [io_uring/rsrc.c:970-1022](), [io_uring/rsrc.h:52-58](), [io_uring/filetable.c:1-150]()

## Ring Buffer Management

### Submission and Completion Queues

| Component | Purpose | Control | Memory Ordering |
|-----------|---------|---------|----------------|
| SQ Tail | Submission index | Application | `smp_wmb()` before update |
| SQ Head | Consumption index | Kernel | `smp_load_acquire()` to read |
| CQ Head | Consumption index | Application | `smp_rmb()` after read |
| CQ Tail | Production index | Kernel | `smp_store_release()` to update |

### Memory Barrier Implementation

```mermaid
graph LR
    subgraph "Application Side"
        APP_SQ_WRITE["Write SQE entries"]
        APP_SQ_BARRIER["smp_wmb()<br/>ordering barrier"]
        APP_SQ_TAIL["Update SQ tail"]
        APP_CQ_HEAD["Read CQ head"]
        APP_CQ_BARRIER["smp_rmb()<br/>ordering barrier"]
        APP_CQE_READ["Read CQE entries"]
    end
    
    subgraph "Kernel Side"
        KERN_SQ_HEAD["Read SQ head<br/>smp_load_acquire()"]
        KERN_SQE_READ["Process SQE entries"]
        KERN_CQE_WRITE["Write CQE entries"]
        KERN_CQ_BARRIER["smp_store_release()"]
        KERN_CQ_TAIL["Update CQ tail"]
    end
    
    APP_SQ_WRITE --> APP_SQ_BARRIER
    APP_SQ_BARRIER --> APP_SQ_TAIL
    
    APP_CQ_HEAD --> APP_CQ_BARRIER
    APP_CQ_BARRIER --> APP_CQE_READ
    
    KERN_SQ_HEAD --> KERN_SQE_READ
    KERN_CQE_WRITE --> KERN_CQ_BARRIER
    KERN_CQ_BARRIER --> KERN_CQ_TAIL
```

Sources: [io_uring/io_uring.c:6-41](), [io_uring/io_uring.h:234-238](), [io_uring/io_uring.c:185-193]()

## Integration with I/O Subsystems

### Block Layer Integration

```mermaid
graph TB
    subgraph "io_uring Operations"
        IO_READ["io_read()"]
        IO_WRITE["io_write()"]
        IO_READ_FIXED["io_read_fixed()"]
        IO_WRITE_FIXED["io_write_fixed()"]
    end
    
    subgraph "Block Layer Interface"
        KIOCB["struct kiocb"]
        IOV_ITER["struct iov_iter"]
        VFS_READ["vfs_read()"]
        VFS_WRITE["vfs_write()"]
    end
    
    subgraph "Storage Drivers"
        NVME_DRIVER["NVMe Driver"]
        UBLK_DRIVER["ublk_drv.c<br/>User Block Device"]
        OTHER_DRIVERS["Other Block Drivers"]
    end
    
    IO_READ --> KIOCB
    IO_WRITE --> KIOCB
    KIOCB --> IOV_ITER
    KIOCB --> VFS_READ
    KIOCB --> VFS_WRITE
    
    VFS_READ --> NVME_DRIVER
    VFS_WRITE --> UBLK_DRIVER
    UBLK_DRIVER --> OTHER_DRIVERS
```

Sources: [io_uring/rw.c:133-169](), [io_uring/rw.c:297-309](), [drivers/block/ublk_drv.c:1-50]()

### Network Subsystem Integration

```mermaid
graph TB
    subgraph "Network Operations"
        IO_SENDMSG["io_sendmsg()"]
        IO_RECVMSG["io_recvmsg()"]
        IO_SEND["io_send()"]
        IO_RECV["io_recv()"]
        IO_ACCEPT["io_accept()"]
        IO_CONNECT["io_connect()"]
    end
    
    subgraph "Socket Interface"
        MSGHDR["struct msghdr"]
        SOCKET["struct socket"]
        SOCK_SENDMSG["sock_sendmsg()"]
        SOCK_RECVMSG["sock_recvmsg()"]
    end
    
    subgraph "Network Stack"
        TCP_STACK["TCP/IP Stack"]
        UDP_STACK["UDP Stack"]
        UNIX_SOCKET["Unix Sockets"]
    end
    
    IO_SENDMSG --> MSGHDR
    IO_RECVMSG --> MSGHDR
    MSGHDR --> SOCKET
    SOCKET --> SOCK_SENDMSG
    SOCKET --> SOCK_RECVMSG
    
    SOCK_SENDMSG --> TCP_STACK
    SOCK_RECVMSG --> UDP_STACK
    TCP_STACK --> UNIX_SOCKET
```

Sources: [io_uring/net.c:403-463](), [io_uring/net.c:815-881](), [io_uring/net.c:465-537]()

## Performance Optimizations

### Polling and Batching

| Feature | Implementation | Purpose |
|---------|---------------|---------|
| SQPOLL | `io_sq_thread()` | Kernel polling thread eliminates syscalls |
| IOPOLL | `io_do_iopoll()` | Hardware polling for low-latency devices |
| Batching | `IO_COMPL_BATCH` | Process multiple completions together |
| Task Work | `tctx_task_work()` | Defer work to task context |

### Buffer Management Optimizations

```mermaid
graph LR
    subgraph "Buffer Selection"
        BUFFER_SELECT["REQ_F_BUFFER_SELECT"]
        BUFFER_RING["REQ_F_BUFFER_RING"]
        FIXED_BUFFER["Fixed Buffers"]
    end
    
    subgraph "Buffer Pools"
        IO_BUFFERS_CACHE["io_buffers_cache<br/>per-ring cache"]
        IO_BUFFERS_COMP["io_buffers_comp<br/>completion cache"]
        KBUF_LIST["io_buffer_list<br/>provided buffers"]
    end
    
    subgraph "Memory Management"
        BUFFER_PINNING["io_pin_pages()"]
        BUFFER_UNPIN["unpin_user_pages()"]
        ACCT_PAGES["account pages<br/>rlimit tracking"]
    end
    
    BUFFER_SELECT --> IO_BUFFERS_CACHE
    BUFFER_RING --> KBUF_LIST
    FIXED_BUFFER --> BUFFER_PINNING
    
    BUFFER_PINNING --> ACCT_PAGES
    BUFFER_UNPIN --> ACCT_PAGES
```

Sources: [io_uring/kbuf.c:81-112](), [io_uring/kbuf.c:114-130](), [io_uring/rsrc.c:844-874]()21:T500a,# IOMMU Subsystem

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [Documentation/ABI/testing/sysfs-bus-event_source-devices-iommu](Documentation/ABI/testing/sysfs-bus-event_source-devices-iommu)
- [Documentation/ABI/testing/sysfs-kernel-iommu_groups](Documentation/ABI/testing/sysfs-kernel-iommu_groups)
- [drivers/acpi/viot.c](drivers/acpi/viot.c)
- [drivers/gpu/drm/exynos/exynos_drm_dma.c](drivers/gpu/drm/exynos/exynos_drm_dma.c)
- [drivers/iommu/Kconfig](drivers/iommu/Kconfig)
- [drivers/iommu/Makefile](drivers/iommu/Makefile)
- [drivers/iommu/amd/Kconfig](drivers/iommu/amd/Kconfig)
- [drivers/iommu/amd/Makefile](drivers/iommu/amd/Makefile)
- [drivers/iommu/amd/amd_iommu.h](drivers/iommu/amd/amd_iommu.h)
- [drivers/iommu/amd/amd_iommu_types.h](drivers/iommu/amd/amd_iommu_types.h)
- [drivers/iommu/amd/init.c](drivers/iommu/amd/init.c)
- [drivers/iommu/amd/io_pgtable.c](drivers/iommu/amd/io_pgtable.c)
- [drivers/iommu/amd/io_pgtable_v2.c](drivers/iommu/amd/io_pgtable_v2.c)
- [drivers/iommu/amd/iommu.c](drivers/iommu/amd/iommu.c)
- [drivers/iommu/apple-dart.c](drivers/iommu/apple-dart.c)
- [drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3-sva.c](drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3-sva.c)
- [drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c](drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.c)
- [drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.h](drivers/iommu/arm/arm-smmu-v3/arm-smmu-v3.h)
- [drivers/iommu/arm/arm-smmu/Makefile](drivers/iommu/arm/arm-smmu/Makefile)
- [drivers/iommu/arm/arm-smmu/arm-smmu-qcom-debug.c](drivers/iommu/arm/arm-smmu/arm-smmu-qcom-debug.c)
- [drivers/iommu/arm/arm-smmu/arm-smmu-qcom.c](drivers/iommu/arm/arm-smmu/arm-smmu-qcom.c)
- [drivers/iommu/arm/arm-smmu/arm-smmu-qcom.h](drivers/iommu/arm/arm-smmu/arm-smmu-qcom.h)
- [drivers/iommu/arm/arm-smmu/arm-smmu.c](drivers/iommu/arm/arm-smmu/arm-smmu.c)
- [drivers/iommu/arm/arm-smmu/arm-smmu.h](drivers/iommu/arm/arm-smmu/arm-smmu.h)
- [drivers/iommu/arm/arm-smmu/qcom_iommu.c](drivers/iommu/arm/arm-smmu/qcom_iommu.c)
- [drivers/iommu/dma-iommu.c](drivers/iommu/dma-iommu.c)
- [drivers/iommu/dma-iommu.h](drivers/iommu/dma-iommu.h)
- [drivers/iommu/exynos-iommu.c](drivers/iommu/exynos-iommu.c)
- [drivers/iommu/fsl_pamu.c](drivers/iommu/fsl_pamu.c)
- [drivers/iommu/fsl_pamu_domain.c](drivers/iommu/fsl_pamu_domain.c)
- [drivers/iommu/intel/Kconfig](drivers/iommu/intel/Kconfig)
- [drivers/iommu/intel/Makefile](drivers/iommu/intel/Makefile)
- [drivers/iommu/intel/cap_audit.c](drivers/iommu/intel/cap_audit.c)
- [drivers/iommu/intel/dmar.c](drivers/iommu/intel/dmar.c)
- [drivers/iommu/intel/iommu.c](drivers/iommu/intel/iommu.c)
- [drivers/iommu/intel/iommu.h](drivers/iommu/intel/iommu.h)
- [drivers/iommu/intel/pasid.c](drivers/iommu/intel/pasid.c)
- [drivers/iommu/intel/pasid.h](drivers/iommu/intel/pasid.h)
- [drivers/iommu/intel/perfmon.c](drivers/iommu/intel/perfmon.c)
- [drivers/iommu/intel/perfmon.h](drivers/iommu/intel/perfmon.h)
- [drivers/iommu/intel/svm.c](drivers/iommu/intel/svm.c)
- [drivers/iommu/io-pgfault.c](drivers/iommu/io-pgfault.c)
- [drivers/iommu/io-pgtable-arm-v7s.c](drivers/iommu/io-pgtable-arm-v7s.c)
- [drivers/iommu/io-pgtable-arm.c](drivers/iommu/io-pgtable-arm.c)
- [drivers/iommu/io-pgtable-dart.c](drivers/iommu/io-pgtable-dart.c)
- [drivers/iommu/io-pgtable.c](drivers/iommu/io-pgtable.c)
- [drivers/iommu/iommu-priv.h](drivers/iommu/iommu-priv.h)
- [drivers/iommu/iommu-sva.c](drivers/iommu/iommu-sva.c)
- [drivers/iommu/iommu-sva.h](drivers/iommu/iommu-sva.h)
- [drivers/iommu/iommu.c](drivers/iommu/iommu.c)
- [drivers/iommu/iova.c](drivers/iommu/iova.c)
- [drivers/iommu/ipmmu-vmsa.c](drivers/iommu/ipmmu-vmsa.c)
- [drivers/iommu/msm_iommu.c](drivers/iommu/msm_iommu.c)
- [drivers/iommu/mtk_iommu.c](drivers/iommu/mtk_iommu.c)
- [drivers/iommu/mtk_iommu_v1.c](drivers/iommu/mtk_iommu_v1.c)
- [drivers/iommu/omap-iommu.c](drivers/iommu/omap-iommu.c)
- [drivers/iommu/rockchip-iommu.c](drivers/iommu/rockchip-iommu.c)
- [drivers/iommu/sprd-iommu.c](drivers/iommu/sprd-iommu.c)
- [drivers/iommu/sun50i-iommu.c](drivers/iommu/sun50i-iommu.c)
- [drivers/iommu/tegra-gart.c](drivers/iommu/tegra-gart.c)
- [drivers/iommu/tegra-smmu.c](drivers/iommu/tegra-smmu.c)
- [drivers/iommu/virtio-iommu.c](drivers/iommu/virtio-iommu.c)
- [include/linux/amd-iommu.h](include/linux/amd-iommu.h)
- [include/linux/io-pgtable.h](include/linux/io-pgtable.h)
- [include/linux/iommu.h](include/linux/iommu.h)
- [include/linux/iova.h](include/linux/iova.h)
- [include/linux/sched/mm.h](include/linux/sched/mm.h)
- [include/soc/mediatek/smi.h](include/soc/mediatek/smi.h)

</details>



The IOMMU (Input/Output Memory Management Unit) subsystem in the Linux kernel provides hardware-based isolation and protection for DMA operations performed by peripheral devices. This page explains the architecture, key components, and operation of the IOMMU subsystem within the WSL2 Linux kernel, with a focus on how it enables secure memory access for devices.

## Purpose and Scope

The IOMMU subsystem serves several critical functions:

1. **DMA Address Translation**: Translates device-visible I/O addresses to physical memory addresses
2. **Memory Protection**: Prevents devices from accessing unauthorized memory regions
3. **Device Isolation**: Isolates devices in separate address domains
4. **Virtualization Support**: Enables device assignment to virtual machines

This document focuses on the kernel's IOMMU framework and its implementations, particularly Intel VT-d and AMD-Vi (AMD IOMMU). For information about using IOMMU through userspace, see the IOMMUFD documentation.

Sources: [drivers/iommu/iommu.c:1-578](), [include/linux/iommu.h:1-297]()

## Architecture Overview

The IOMMU subsystem consists of a generic API layer and hardware-specific implementations.

**IOMMU Subsystem Architecture**

```mermaid
flowchart TD
    subgraph "Core IOMMU Framework"
        IOMMU_C["drivers/iommu/iommu.c<br/>iommu_device_register()<br/>iommu_probe_device()<br/>iommu_group_alloc()"]
        IOMMU_H["include/linux/iommu.h<br/>struct iommu_ops<br/>struct iommu_domain<br/>struct iommu_domain_ops"]
        DMA_IOMMU_C["drivers/iommu/dma-iommu.c<br/>iommu_dma_map_page()<br/>iommu_dma_alloc()"]
        IOVA_C["drivers/iommu/iova.c<br/>alloc_iova()<br/>free_iova()"]
    end
    
    subgraph "Hardware-Specific Drivers"
        INTEL_IOMMU["drivers/iommu/intel/iommu.c<br/>intel_iommu_ops<br/>struct intel_iommu<br/>struct dmar_domain"]
        AMD_IOMMU["drivers/iommu/amd/iommu.c<br/>amd_iommu_ops<br/>struct amd_iommu<br/>struct protection_domain"]
        ARM_SMMU_V3["drivers/iommu/arm/arm-smmu-v3/<br/>arm-smmu-v3.c<br/>arm_smmu_ops"]
        MTK_IOMMU["drivers/iommu/mtk_iommu.c<br/>mtk_iommu_ops"]
        APPLE_DART["drivers/iommu/apple-dart.c<br/>apple_dart_ops"]
    end
    
    subgraph "Hardware Registers & Tables"
        INTEL_HW["Intel VT-d Registers<br/>DMAR_GCMD_REG<br/>DMAR_GSTS_REG<br/>Root/Context Tables"]
        AMD_HW["AMD-Vi Registers<br/>MMIO_CONTROL_OFFSET<br/>Device Table<br/>Command Buffer"]
        ARM_HW["ARM SMMU Registers<br/>Command Queue<br/>Stream Tables"]
    end
    
    IOMMU_C --> |"Calls ops from"| INTEL_IOMMU
    IOMMU_C --> |"Calls ops from"| AMD_IOMMU
    IOMMU_C --> |"Calls ops from"| ARM_SMMU_V3
    IOMMU_C --> |"Calls ops from"| MTK_IOMMU
    IOMMU_C --> |"Calls ops from"| APPLE_DART
    
    DMA_IOMMU_C --> |"Uses"| IOMMU_C
    DMA_IOMMU_C --> |"Uses"| IOVA_C
    
    INTEL_IOMMU --> |"Programs"| INTEL_HW
    AMD_IOMMU --> |"Programs"| AMD_HW
    ARM_SMMU_V3 --> |"Programs"| ARM_HW
```

Sources: [drivers/iommu/iommu.c:251-282](), [include/linux/iommu.h:264-297](), [drivers/iommu/intel/iommu.c:1-300](), [drivers/iommu/amd/iommu.c:1-200](), [drivers/iommu/dma-iommu.c:1-100]()

## Key Components

### IOMMU Domains

IOMMU domains represent distinct address spaces that can be assigned to one or more device groups. They define the memory that devices can access.

```c
struct iommu_domain {
    unsigned type;                       // Domain type (DMA, identity, etc.)
    const struct iommu_domain_ops *ops;  // Domain operations
    unsigned long pgsize_bitmap;         // Supported page sizes
    struct iommu_domain_geometry geometry; // Address space geometry
    struct iommu_dma_cookie *iova_cookie; // DMA mapping information
};
```

The Linux kernel supports the following domain types:

| Domain Type | Description |
|-------------|-------------|
| `IOMMU_DOMAIN_BLOCKED` | All DMA is blocked, used for isolation |
| `IOMMU_DOMAIN_IDENTITY` | DMA addresses are system physical addresses |
| `IOMMU_DOMAIN_UNMANAGED` | DMA mappings managed by user (e.g., VMs) |
| `IOMMU_DOMAIN_DMA` | Used for DMA-API implementations |
| `IOMMU_DOMAIN_DMA_FQ` | DMA with batched TLB invalidation |
| `IOMMU_DOMAIN_SVA` | Shared process address space domains |

Sources: [include/linux/iommu.h:95-114](), [include/linux/iommu.h:85-93]()

### Device Groups

Device groups represent sets of devices that share the same IOMMU address space and cannot be separated for the purpose of DMA isolation.

```mermaid
flowchart LR
    subgraph "System"
        subgraph "Device Group 1"
            D1["Device A"]
            D2["Device B"]
        end
        
        subgraph "Device Group 2"
            D3["Device C"]
            D4["Device D"]
        end
        
        subgraph "IOMMU Domain 1"
            DG1["Address Space 1"]
        end
        
        subgraph "IOMMU Domain 2"
            DG2["Address Space 2"]
        end
    end
    
    D1 --> DG1
    D2 --> DG1
    D3 --> DG2
    D4 --> DG2
```

The kernel automatically identifies which devices must be grouped together based on hardware topology and IOMMU capabilities.

Sources: [drivers/iommu/iommu.c:924-982](), [drivers/iommu/iommu.c:50-72]()

### IOVA Management

IOVA (I/O Virtual Address) management handles the allocation and deallocation of I/O virtual addresses used for DMA operations. This ensures that devices have a consistent view of memory addresses.

```c
struct iova_domain {
    struct mutex iova_rbtree_lock; // Lock for the IOVA tree
    struct rb_root rbroot;         // Red-black tree of IOVAs
    struct rb_node *cached32_node; // Cached node for 32-bit lookups
    struct rb_node *cached_node;   // Cached node for regular lookups
    unsigned long granule;         // IOVA granularity
    dma_addr_t start_pfn;          // Start page frame number
    dma_addr_t dma_32bit_pfn;      // 32-bit DMA boundary
    struct iova *anchor;           // Anchor IOVA
};
```

Sources: [drivers/iommu/iova.c:1-200]()

### DMA-IOMMU Integration

The DMA-IOMMU layer integrates the IOMMU with the DMA API, providing transparent DMA address translation for device drivers.

```mermaid
flowchart TD
    Driver["Device Driver"] --> DMA_API["DMA API"]
    DMA_API --> DMA_IOMMU["DMA-IOMMU Layer"]
    DMA_IOMMU --> IOMMU_API["IOMMU API"]
    IOMMU_API --> IOMMU_HW["IOMMU Hardware"]
    IOMMU_HW --> RAM["System RAM"]
```

Sources: [drivers/iommu/dma-iommu.c:1-100]()

### Shared Virtual Addressing (SVA)

SVA allows devices to directly use process virtual addresses, enabling more efficient interaction between devices and applications.

Sources: [drivers/iommu/iommu-sva.c:1-100](), [drivers/iommu/intel/svm.c:1-100]()

## IOMMU Hardware Implementations

### Intel IOMMU (VT-d)

Intel's VT-d technology implements IOMMU functionality on Intel platforms. Key features include:

1. **DMA Remapping**: Translates device DMA operations to proper physical addresses
2. **Interrupt Remapping**: Controls and isolates device interrupts
3. **Queued Invalidation**: Efficient cache invalidation mechanism
4. **Support for Shared Virtual Memory (SVM)**: Enables advanced virtualization features

The Intel IOMMU driver structure:

```mermaid
classDiagram
    class intel_iommu {
        +struct device *dev
        +void __iomem *reg
        +u64 cap
        +u64 ecap
        +u32 gcmd
        +struct pci_seg *pci_seg
        +int seq_id
        +u16 devid
        +struct root_entry *root_entry
        +struct dmar_drhd_unit *drhd
        +unsigned int irq
        +u32 flags
    }
    
    class dmar_domain {
        +struct iommu_domain domain
        +struct iommu_domain_geometry geometry
        +struct intel_iommu *iommu
        +struct dma_pte *pgd
        +int agaw
        +int gaw
        +int pgtable_level
        +bool use_first_level
        +bool iommu_coherency
        +int iommu_superpage
    }
    
    intel_iommu --> dmar_domain : "Manages domains"
```

Sources: [drivers/iommu/intel/iommu.c:1-300](), [drivers/iommu/intel/iommu.h:1-150]()

### AMD IOMMU

AMD's IOMMU technology provides similar functionality to Intel's VT-d, including:

1. **Device Table**: Maps devices to protection domains
2. **Protection Domains**: Isolated memory spaces for devices
3. **Page Tables**: Translation tables for address mapping
4. **Command Buffer**: Interface for programming the IOMMU

```mermaid
classDiagram
    class amd_iommu {
        +struct device *dev
        +void __iomem *mmio_base
        +u32 cap
        +u64 features
        +struct pci_dev *pdev
        +int devid
        +int index
        +struct amd_iommu_pci_seg *pci_seg
    }
    
    class protection_domain {
        +struct iommu_domain domain
        +struct iommu_domain_geometry geometry
        +struct io_pgtable_ops *iop
        +struct io_pgtable_cfg cfg
        +u64 *pt_root
        +int level
        +int mode
    }
    
    amd_iommu --> protection_domain : "Manages domains"
```

Sources: [drivers/iommu/amd/iommu.c:1-200](), [drivers/iommu/amd/init.c:1-200](), [drivers/iommu/amd/amd_iommu_types.h:1-100]()

## Address Translation Process

The IOMMU translates I/O virtual addresses (IOVAs) to physical addresses using multi-level page tables, similar to CPU MMUs.

```mermaid
flowchart LR
    Device["Device"] -->|"Issues DMA with IOVA"| IOMMU["IOMMU"]
    IOMMU -->|"1. Look up domain"| Domain["Domain"]
    IOMMU -->|"2. Consult page tables"| PageTables["Page Tables"]
    PageTables -->|"Return physical address"| IOMMU
    IOMMU -->|"Access with physical address"| Memory["System Memory"]
```

The translation process:

1. Device initiates a DMA operation with an IOVA
2. IOMMU determines which domain the device belongs to
3. IOMMU consults the domain's page tables to translate IOVA to physical address
4. If translation succeeds, the access proceeds to physical memory
5. If translation fails, the IOMMU generates a fault

Sources: [drivers/iommu/intel/iommu.c:900-1100](), [drivers/iommu/amd/iommu.c:500-700]()

## Fault Handling

When a device attempts to access memory that is not properly mapped, the IOMMU generates a fault. The Linux kernel provides mechanisms to report and handle these faults.

```c
struct iommu_fault {
    struct iommu_fault_event event;  // Fault event details
    struct kref refcount;            // Reference count
};
```

Faults can be:
- **Read Faults**: Device attempted unauthorized read
- **Write Faults**: Device attempted unauthorized write
- **Translation Faults**: Address couldn't be translated
- **Permission Faults**: Permission violation

Sources: [drivers/iommu/intel/iommu.c:453-591](), [drivers/iommu/amd/iommu.c:544-591]()

## IOMMU API

The IOMMU API provides a unified interface for kernel subsystems to interact with different IOMMU hardware implementations.

### Core Functions

| Function | Description |
|----------|-------------|
| `iommu_domain_alloc(bus)` | Allocate an IOMMU domain for a specific bus type |
| `iommu_domain_free(domain)` | Free an IOMMU domain |
| `iommu_attach_device(domain, dev)` | Attach a device to an IOMMU domain |
| `iommu_detach_device(domain, dev)` | Detach a device from an IOMMU domain |
| `iommu_map(domain, iova, paddr, size, prot)` | Map physical memory in an IOMMU domain |
| `iommu_unmap(domain, iova, size)` | Unmap memory in an IOMMU domain |
| `iommu_get_dma_cookie(domain)` | Get DMA cookie for a domain (for DMA ops) |

Sources: [include/linux/iommu.h:200-400](), [drivers/iommu/iommu.c:200-400]()

### Domain Operations

IOMMU domains support various operations through an ops structure:

```c
struct iommu_domain_ops {
    int (*attach_dev)(struct iommu_domain *domain, struct device *dev);
    void (*detach_dev)(struct iommu_domain *domain, struct device *dev);
    int (*map)(struct iommu_domain *domain, unsigned long iova,
               phys_addr_t paddr, size_t size, int prot);
    size_t (*unmap)(struct iommu_domain *domain, unsigned long iova,
                    size_t size, struct iommu_iotlb_gather *gather);
    void (*flush_iotlb_all)(struct iommu_domain *domain);
    void (*iotlb_sync)(struct iommu_domain *domain,
                      struct iommu_iotlb_gather *gather);
    phys_addr_t (*iova_to_phys)(struct iommu_domain *domain, dma_addr_t iova);
    // Additional operations...
};
```

Sources: [include/linux/iommu.h:300-350]()

## Configuration and Boot Parameters

The IOMMU subsystem can be configured through various kernel command-line parameters:

| Parameter | Description |
|-----------|-------------|
| `intel_iommu=on/off` | Enable/disable Intel IOMMU support |
| `amd_iommu=on/off` | Enable/disable AMD IOMMU support |
| `iommu=on/off/pt` | General IOMMU control (on, off, passthrough) |
| `iommu.strict=0/1` | Control DMA TLB invalidation policy |
| `iommu.passthrough=0/1` | Set IOMMU default domain type |

Sources: [drivers/iommu/intel/iommu.c:324-367](), [drivers/iommu/amd/init.c:160-215]()

## Intel IOMMU Implementation Details

The Intel IOMMU (VT-d) implementation uses several key data structures:

1. **Root Tables**: Map PCI bus/device/function to context entries
2. **Context Tables**: Map devices to page tables and define domain settings
3. **Page Tables**: Multi-level tables that translate IOVAs to physical addresses

```mermaid
flowchart TD
    RootEntry["Root Entry\n(Per PCI bus)"] --> ContextTable["Context Table\n(Per device)"]
    ContextTable --> PageTables["Page Tables\n(Per domain)"]
    PageTables --> |"IOVAPA Translation"| PhysicalMemory["Physical Memory"]
    
    BDF["Device BDF\n(Bus/Device/Function)"] --> |"Index into"| RootEntry
    DeviceID["Device ID"] --> |"Index into"| ContextTable
    IOVA["I/O Virtual Address"] --> |"Translated through"| PageTables
```

Sources: [drivers/iommu/intel/iommu.c:150-220](), [drivers/iommu/intel/iommu.h:30-140]()

## AMD IOMMU Implementation Details

The AMD IOMMU uses a different architecture:

1. **Device Table**: Maps device IDs to protection domains
2. **Protection Domains**: Contains page tables for address translation
3. **Command Buffer**: Interface for programing the IOMMU
4. **Event Log**: Records events such as faults and page requests

```mermaid
flowchart TD
    DeviceTable["Device Table\n(Maps device to domain)"] --> ProtectionDomain["Protection Domain\n(Contains page tables)"]
    ProtectionDomain --> PageTables["Page Tables\n(Translation tables)"]
    PageTables --> |"IOVAPA Translation"| PhysicalMemory["Physical Memory"]
    
    DeviceID["Device ID"] --> |"Index into"| DeviceTable
    IOVA["I/O Virtual Address"] --> |"Translated through"| PageTables
    
    IOMMU["AMD IOMMU"] --> |"Reads"| CommandBuffer["Command Buffer"]
    IOMMU --> |"Writes"| EventLog["Event Log"]
```

Sources: [drivers/iommu/amd/iommu.c:1-100](), [drivers/iommu/amd/amd_iommu_types.h:1-100]()

## DMA API Integration

The IOMMU subsystem integrates with the DMA API to provide transparent address translation for device drivers.

```mermaid
flowchart TD
    Driver["Device Driver"] --> DMA_API["DMA API\n(dma_map_*,\ndma_alloc_*)"]
    
    DMA_API --> DMA_OPS["DMA Operations"]
    DMA_OPS --> IOMMU_OPS["IOMMU Operations\n(map, unmap, etc.)"]
    
    subgraph "IOMMU Subsystem"
        IOMMU_OPS --> IOMMU_DRV["IOMMU Driver\n(Intel/AMD/ARM)"]
        IOMMU_DRV --> IOMMU_HW["IOMMU Hardware"]
    end
    
    IOMMU_HW --> |"Translates addresses"| MEMORY["System Memory"]
```

Sources: [drivers/iommu/dma-iommu.c:1-200]()

## Conclusion

The IOMMU subsystem provides essential functionality for secure and efficient I/O operations in modern computing systems. By translating addresses and enforcing access controls, it protects the system memory from potentially malicious or faulty devices. The modular architecture of the Linux IOMMU subsystem allows for different hardware implementations while providing a consistent API for the rest of the kernel.22:T3a42,# NVMe Storage Driver

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [block/Makefile](block/Makefile)
- [drivers/nvme/host/Makefile](drivers/nvme/host/Makefile)
- [drivers/nvme/host/core.c](drivers/nvme/host/core.c)
- [drivers/nvme/host/fabrics.c](drivers/nvme/host/fabrics.c)
- [drivers/nvme/host/fabrics.h](drivers/nvme/host/fabrics.h)
- [drivers/nvme/host/hwmon.c](drivers/nvme/host/hwmon.c)
- [drivers/nvme/host/ioctl.c](drivers/nvme/host/ioctl.c)
- [drivers/nvme/host/multipath.c](drivers/nvme/host/multipath.c)
- [drivers/nvme/host/nvme.h](drivers/nvme/host/nvme.h)
- [drivers/nvme/host/pci.c](drivers/nvme/host/pci.c)
- [drivers/nvme/host/rdma.c](drivers/nvme/host/rdma.c)
- [drivers/nvme/host/sysfs.c](drivers/nvme/host/sysfs.c)
- [drivers/nvme/host/tcp.c](drivers/nvme/host/tcp.c)
- [drivers/nvme/host/trace.h](drivers/nvme/host/trace.h)
- [drivers/nvme/target/admin-cmd.c](drivers/nvme/target/admin-cmd.c)
- [drivers/nvme/target/configfs.c](drivers/nvme/target/configfs.c)
- [drivers/nvme/target/core.c](drivers/nvme/target/core.c)
- [drivers/nvme/target/fc.c](drivers/nvme/target/fc.c)
- [drivers/nvme/target/fcloop.c](drivers/nvme/target/fcloop.c)
- [drivers/nvme/target/io-cmd-file.c](drivers/nvme/target/io-cmd-file.c)
- [drivers/nvme/target/loop.c](drivers/nvme/target/loop.c)
- [drivers/nvme/target/nvmet.h](drivers/nvme/target/nvmet.h)
- [drivers/nvme/target/passthru.c](drivers/nvme/target/passthru.c)
- [drivers/nvme/target/rdma.c](drivers/nvme/target/rdma.c)
- [drivers/nvme/target/tcp.c](drivers/nvme/target/tcp.c)
- [drivers/nvme/target/zns.c](drivers/nvme/target/zns.c)
- [include/linux/nvme-tcp.h](include/linux/nvme-tcp.h)
- [include/linux/nvme.h](include/linux/nvme.h)

</details>



## Purpose and Scope

The NVMe Storage Driver provides high-performance storage support for NVMe (Non-Volatile Memory Express) devices in the Linux kernel. This driver implements the NVMe specification for both PCIe-attached NVMe SSDs and NVMe-over-Fabrics devices, enabling efficient I/O operations with advanced features like multiple I/O queues, asynchronous command processing, and hardware-accelerated completion handling.

This document covers the host-side NVMe driver implementation. For information about the I/O subsystem framework that NVMe integrates with, see [I/O Subsystems](#4). For details about the io_uring asynchronous I/O interface that works with NVMe, see [io_uring Framework](#4.1).

## Architecture Overview

The NVMe driver follows a layered architecture that abstracts NVMe controllers and provides a unified interface to the Linux block layer:

```mermaid
graph TB
    subgraph "Application Layer"
        APP["Applications"]
        IOCTL["ioctl Interface"]
    end
    
    subgraph "Block Layer"
        BLKMQ["blk-mq Multi-Queue"]
        GENDISK["gendisk"]
    end
    
    subgraph "NVMe Core Layer"
        NVME_CORE["nvme_ctrl"]
        NVME_NS["nvme_ns"]
        NVME_REQ["nvme_request"]
    end
    
    subgraph "Transport Layer"
        NVME_PCI["nvme_dev (PCI)"]
        NVME_TCP["nvme_tcp_ctrl"]
        NVME_RDMA["nvme_rdma_ctrl"]
    end
    
    subgraph "Hardware/Network"
        PCI_HW["PCIe NVMe SSD"]
        TCP_NET["TCP/IP Network"]
        RDMA_NET["RDMA Network"]
    end
    
    APP --> BLKMQ
    IOCTL --> NVME_CORE
    BLKMQ --> NVME_CORE
    GENDISK --> NVME_NS
    NVME_CORE --> NVME_REQ
    NVME_NS --> NVME_PCI
    NVME_NS --> NVME_TCP
    NVME_NS --> NVME_RDMA
    NVME_PCI --> PCI_HW
    NVME_TCP --> TCP_NET
    NVME_RDMA --> RDMA_NET
```

Sources: [drivers/nvme/host/core.c:279-412](), [drivers/nvme/host/pci.c:118-163](), [drivers/nvme/host/nvme.h:279-412]()

## Core Data Structures

The NVMe driver uses several key data structures to manage devices, queues, and I/O operations:

```mermaid
graph TB
    subgraph "Controller Management"
        nvme_ctrl["nvme_ctrl<br/> admin_q: request_queue<br/> tagset: blk_mq_tag_set<br/> state: nvme_ctrl_state<br/> ops: nvme_ctrl_ops"]
        nvme_dev["nvme_dev<br/> queues: nvme_queue*<br/> ctrl: nvme_ctrl<br/> bar: void __iomem*<br/> db_stride: u32"]
    end
    
    subgraph "Queue Management" 
        nvme_queue["nvme_queue<br/> sq_cmds: void*<br/> cqes: nvme_completion*<br/> q_depth: u32<br/> qid: u16"]
        blk_mq_hw_ctx["blk_mq_hw_ctx<br/> driver_data: nvme_queue"]
    end
    
    subgraph "Request Processing"
        nvme_request["nvme_request<br/> cmd: nvme_command*<br/> result: nvme_result<br/> status: u16<br/> ctrl: nvme_ctrl*"]
        nvme_iod["nvme_iod<br/> req: nvme_request<br/> cmd: nvme_command<br/> sgt: sg_table<br/> dma_len: unsigned int"]
    end
    
    subgraph "Namespace Management"
        nvme_ns["nvme_ns<br/> queue: request_queue*<br/> disk: gendisk*<br/> ctrl: nvme_ctrl*<br/> head: nvme_ns_head*"]
    end
    
    nvme_ctrl --> nvme_dev
    nvme_dev --> nvme_queue
    nvme_queue --> blk_mq_hw_ctx
    blk_mq_hw_ctx --> nvme_request
    nvme_request --> nvme_iod
    nvme_ctrl --> nvme_ns
```

Sources: [drivers/nvme/host/nvme.h:279-412](), [drivers/nvme/host/pci.c:118-163](), [drivers/nvme/host/pci.c:190-218](), [drivers/nvme/host/pci.c:231-242]()

## Queue Management

NVMe devices use a queue-pair model with separate submission queues (SQ) and completion queues (CQ) for high-performance I/O:

### Queue Initialization

The driver creates multiple I/O queues to maximize parallelism across CPU cores:

```mermaid
graph LR
    subgraph "Queue Allocation"
        nvme_dev_add["nvme_dev_add()"]
        nvme_setup_io_queues["nvme_setup_io_queues()"]
        nvme_alloc_queue["nvme_alloc_queue()"]
    end
    
    subgraph "Queue Configuration"
        adapter_alloc_cq["adapter_alloc_cq()"]
        adapter_alloc_sq["adapter_alloc_sq()"]
        nvme_init_hctx["nvme_init_hctx()"]
    end
    
    subgraph "Hardware Programming"
        writel_db["writel(nvmeq->q_db)"]
        nvme_write_sq_db["nvme_write_sq_db()"]
    end
    
    nvme_dev_add --> nvme_setup_io_queues
    nvme_setup_io_queues --> nvme_alloc_queue
    nvme_alloc_queue --> adapter_alloc_cq
    adapter_alloc_cq --> adapter_alloc_sq
    adapter_alloc_sq --> nvme_init_hctx
    nvme_init_hctx --> writel_db
    writel_db --> nvme_write_sq_db
```

### Queue Types and CPU Mapping

The driver supports different queue types optimized for various workloads:

| Queue Type | Purpose | CPU Mapping |
|------------|---------|-------------|
| Admin Queue | Controller management commands | CPU 0 |
| Default I/O Queues | Read/write operations | Round-robin across CPUs |
| Write Queues | Dedicated write operations | Configured via `write_queues` parameter |
| Poll Queues | Low-latency polling operations | Configured via `poll_queues` parameter |

Sources: [drivers/nvme/host/pci.c:2094-2145](), [drivers/nvme/host/pci.c:440-467](), [drivers/nvme/host/pci.c:1154-1204]()

## Command Processing

### Command Submission Path

The NVMe driver processes I/O requests through a multi-stage pipeline:

```mermaid
graph TB
    subgraph "Block Layer Entry"
        nvme_queue_rq["nvme_queue_rq()"]
        nvme_prep_rq["nvme_prep_rq()"]
        nvme_setup_cmd["nvme_setup_cmd()"]
    end
    
    subgraph "DMA Mapping"
        nvme_map_data["nvme_map_data()"]
        nvme_setup_prp_simple["nvme_setup_prp_simple()"]
        nvme_pci_setup_prps["nvme_pci_setup_prps()"]
    end
    
    subgraph "Command Submission"
        nvme_sq_copy_cmd["nvme_sq_copy_cmd()"]
        nvme_write_sq_db["nvme_write_sq_db()"]
        nvme_dbbuf_update_and_check_event["nvme_dbbuf_update_and_check_event()"]
    end
    
    nvme_queue_rq --> nvme_prep_rq
    nvme_prep_rq --> nvme_setup_cmd
    nvme_setup_cmd --> nvme_map_data
    nvme_map_data --> nvme_setup_prp_simple
    nvme_map_data --> nvme_pci_setup_prps
    nvme_setup_prp_simple --> nvme_sq_copy_cmd
    nvme_pci_setup_prps --> nvme_sq_copy_cmd
    nvme_sq_copy_cmd --> nvme_write_sq_db
    nvme_write_sq_db --> nvme_dbbuf_update_and_check_event
```

### Command Completion Path

Completed commands are processed via interrupt handlers or polling mechanisms:

```mermaid
graph TB
    subgraph "Completion Detection"
        nvme_irq["nvme_irq()"]
        nvme_poll["nvme_poll()"]
        nvme_cqe_pending["nvme_cqe_pending()"]
    end
    
    subgraph "Completion Processing"
        nvme_poll_cq["nvme_poll_cq()"]
        nvme_handle_cqe["nvme_handle_cqe()"]
        nvme_try_complete_req["nvme_try_complete_req()"]
    end
    
    subgraph "Request Finalization"
        nvme_pci_complete_rq["nvme_pci_complete_rq()"]
        nvme_unmap_data["nvme_unmap_data()"]
        nvme_complete_rq["nvme_complete_rq()"]
    end
    
    nvme_irq --> nvme_cqe_pending
    nvme_poll --> nvme_cqe_pending
    nvme_cqe_pending --> nvme_poll_cq
    nvme_poll_cq --> nvme_handle_cqe
    nvme_handle_cqe --> nvme_try_complete_req
    nvme_try_complete_req --> nvme_pci_complete_rq
    nvme_pci_complete_rq --> nvme_unmap_data
    nvme_unmap_data --> nvme_complete_rq
```

Sources: [drivers/nvme/host/pci.c:877-904](), [drivers/nvme/host/pci.c:1010-1042](), [drivers/nvme/host/pci.c:1077-1088](), [drivers/nvme/host/pci.c:959-973]()

## Device Initialization and Management

### Controller Discovery and Setup

The driver follows a structured initialization sequence for NVMe controllers:

```mermaid
graph TB
    subgraph "PCI Device Probe"
        nvme_probe["nvme_probe()"]
        nvme_dev_map["nvme_dev_map()"]
        nvme_reset_ctrl["nvme_reset_ctrl()"]
    end
    
    subgraph "Controller Reset"
        nvme_reset_work["nvme_reset_work()"]
        nvme_pci_enable["nvme_pci_enable()"]
        nvme_dev_disable["nvme_dev_disable()"]
    end
    
    subgraph "Controller Initialization"
        nvme_init_ctrl["nvme_init_ctrl()"]
        nvme_init_identify["nvme_init_identify()"]
        nvme_setup_io_queues["nvme_setup_io_queues()"]
    end
    
    subgraph "Namespace Discovery"
        nvme_scan_work["nvme_scan_work()"]
        nvme_validate_ns["nvme_validate_ns()"]
        nvme_alloc_ns["nvme_alloc_ns()"]
    end
    
    nvme_probe --> nvme_dev_map
    nvme_dev_map --> nvme_reset_ctrl
    nvme_reset_ctrl --> nvme_reset_work
    nvme_reset_work --> nvme_pci_enable
    nvme_pci_enable --> nvme_dev_disable
    nvme_reset_work --> nvme_init_ctrl
    nvme_init_ctrl --> nvme_init_identify
    nvme_init_identify --> nvme_setup_io_queues
    nvme_setup_io_queues --> nvme_scan_work
    nvme_scan_work --> nvme_validate_ns
    nvme_validate_ns --> nvme_alloc_ns
```

### Power Management and Error Handling

The driver implements comprehensive power management and error recovery:

| Feature | Implementation | Key Functions |
|---------|----------------|---------------|
| Runtime PM | Automatic power state transitions | `nvme_configure_apst()` |
| Error Recovery | Controller reset on fatal errors | `nvme_reset_ctrl()`, `nvme_dev_disable()` |
| Timeout Handling | Command timeout and abort | `nvme_timeout()`, `nvme_abort_req()` |
| Hot Removal | Clean device removal | `nvme_remove()`, `nvme_dev_disable()` |

Sources: [drivers/nvme/host/pci.c:2690-2745](), [drivers/nvme/host/pci.c:2746-2850](), [drivers/nvme/host/pci.c:1278-1384]()

## Advanced Features

### Doorbell Buffer Optimization

NVMe devices support doorbell buffer (DBBUF) optimization to reduce PCIe traffic:

```mermaid
graph LR
    subgraph "DBBUF Setup"
        nvme_dbbuf_dma_alloc["nvme_dbbuf_dma_alloc()"]
        nvme_dbbuf_set["nvme_dbbuf_set()"]
        nvme_dbbuf_init["nvme_dbbuf_init()"]
    end
    
    subgraph "Runtime Usage"
        nvme_dbbuf_update_and_check_event["nvme_dbbuf_update_and_check_event()"]
        memory_barrier["wmb() / mb()"]
        conditional_mmio["Conditional MMIO Write"]
    end
    
    nvme_dbbuf_dma_alloc --> nvme_dbbuf_set
    nvme_dbbuf_set --> nvme_dbbuf_init
    nvme_dbbuf_init --> nvme_dbbuf_update_and_check_event
    nvme_dbbuf_update_and_check_event --> memory_barrier
    memory_barrier --> conditional_mmio
```

### Multi-Path Support

The driver supports NVMe multipath for high availability and load balancing across multiple controllers accessing the same namespace:

```mermaid
graph TB
    subgraph "Path Management"
        nvme_ns_head["nvme_ns_head"]
        nvme_current_path["current_path[node]"]
        nvme_mpath_set_live["nvme_mpath_set_live()"]
    end
    
    subgraph "Path Selection"
        nvme_find_path["nvme_find_path()"]
        nvme_round_robin_path["nvme_round_robin_path()"]
        nvme_numa_path["NUMA-aware selection"]
    end
    
    subgraph "Failover Handling"
        nvme_failover_req["nvme_failover_req()"]
        nvme_mpath_clear_current_path["nvme_mpath_clear_current_path()"]
        nvme_kick_requeue_lists["nvme_kick_requeue_lists()"]
    end
    
    nvme_ns_head --> nvme_current_path
    nvme_current_path --> nvme_mpath_set_live
    nvme_find_path --> nvme_round_robin_path
    nvme_find_path --> nvme_numa_path
    nvme_failover_req --> nvme_mpath_clear_current_path
    nvme_mpath_clear_current_path --> nvme_kick_requeue_lists
```

Sources: [drivers/nvme/host/pci.c:249-300](), [drivers/nvme/host/pci.c:325-382](), [drivers/nvme/host/multipath.c:86-127](), [drivers/nvme/host/multipath.c:257-296]()

## Integration with Block Layer

The NVMe driver integrates tightly with the Linux block multi-queue (blk-mq) framework:

### Block Multi-Queue Integration

```mermaid
graph TB
    subgraph "Block Layer"
        request_queue["request_queue"]
        blk_mq_tag_set["blk_mq_tag_set"]
        blk_mq_ops["blk_mq_ops"]
    end
    
    subgraph "NVMe Operations"
        nvme_queue_rq["nvme_queue_rq"]
        nvme_complete_rq["nvme_complete_rq"]
        nvme_init_hctx["nvme_init_hctx"]
        nvme_map_queues["nvme_pci_map_queues"]
    end
    
    subgraph "Hardware Context"
        blk_mq_hw_ctx["blk_mq_hw_ctx"]
        nvme_queue_driver_data["driver_data: nvme_queue*"]
    end
    
    blk_mq_tag_set --> blk_mq_ops
    blk_mq_ops --> nvme_queue_rq
    blk_mq_ops --> nvme_complete_rq
    blk_mq_ops --> nvme_init_hctx
    blk_mq_ops --> nvme_map_queues
    request_queue --> blk_mq_hw_ctx
    blk_mq_hw_ctx --> nvme_queue_driver_data
```

### Performance Optimization Features

| Feature | Description | Configuration |
|---------|-------------|---------------|
| `io_queue_depth` | Commands per I/O queue | Module parameter, default 1024 |
| `write_queues` | Dedicated write queues | Module parameter |
| `poll_queues` | Low-latency polling queues | Module parameter |
| `use_cmb_sqes` | Controller Memory Buffer usage | Module parameter, default true |
| `sgl_threshold` | Scatter-gather list threshold | Module parameter, default 32KB |

Sources: [drivers/nvme/host/pci.c:44-76](), [drivers/nvme/host/pci.c:94-102](), [drivers/nvme/host/pci.c:396-418](), [drivers/nvme/host/pci.c:440-467]()23:T4c89,# File Systems

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [arch/mips/include/uapi/asm/ucontext.h](arch/mips/include/uapi/asm/ucontext.h)
- [arch/s390/include/uapi/asm/hwctrset.h](arch/s390/include/uapi/asm/hwctrset.h)
- [arch/x86/include/uapi/asm/kvm.h](arch/x86/include/uapi/asm/kvm.h)
- [drivers/net/wireless/broadcom/brcm80211/brcmfmac/pno.c](drivers/net/wireless/broadcom/brcm80211/brcmfmac/pno.c)
- [fs/btrfs/accessors.c](fs/btrfs/accessors.c)
- [fs/btrfs/accessors.h](fs/btrfs/accessors.h)
- [fs/btrfs/backref.c](fs/btrfs/backref.c)
- [fs/btrfs/backref.h](fs/btrfs/backref.h)
- [fs/btrfs/bio.c](fs/btrfs/bio.c)
- [fs/btrfs/bio.h](fs/btrfs/bio.h)
- [fs/btrfs/block-group.c](fs/btrfs/block-group.c)
- [fs/btrfs/block-group.h](fs/btrfs/block-group.h)
- [fs/btrfs/btrfs_inode.h](fs/btrfs/btrfs_inode.h)
- [fs/btrfs/compression.c](fs/btrfs/compression.c)
- [fs/btrfs/compression.h](fs/btrfs/compression.h)
- [fs/btrfs/ctree.c](fs/btrfs/ctree.c)
- [fs/btrfs/ctree.h](fs/btrfs/ctree.h)
- [fs/btrfs/delayed-inode.c](fs/btrfs/delayed-inode.c)
- [fs/btrfs/delayed-inode.h](fs/btrfs/delayed-inode.h)
- [fs/btrfs/delayed-ref.c](fs/btrfs/delayed-ref.c)
- [fs/btrfs/delayed-ref.h](fs/btrfs/delayed-ref.h)
- [fs/btrfs/disk-io.c](fs/btrfs/disk-io.c)
- [fs/btrfs/disk-io.h](fs/btrfs/disk-io.h)
- [fs/btrfs/extent-io-tree.c](fs/btrfs/extent-io-tree.c)
- [fs/btrfs/extent-tree.c](fs/btrfs/extent-tree.c)
- [fs/btrfs/extent-tree.h](fs/btrfs/extent-tree.h)
- [fs/btrfs/extent_io.c](fs/btrfs/extent_io.c)
- [fs/btrfs/extent_io.h](fs/btrfs/extent_io.h)
- [fs/btrfs/extent_map.c](fs/btrfs/extent_map.c)
- [fs/btrfs/extent_map.h](fs/btrfs/extent_map.h)
- [fs/btrfs/file-item.c](fs/btrfs/file-item.c)
- [fs/btrfs/file-item.h](fs/btrfs/file-item.h)
- [fs/btrfs/file.c](fs/btrfs/file.c)
- [fs/btrfs/free-space-cache.c](fs/btrfs/free-space-cache.c)
- [fs/btrfs/free-space-tree.c](fs/btrfs/free-space-tree.c)
- [fs/btrfs/free-space-tree.h](fs/btrfs/free-space-tree.h)
- [fs/btrfs/fs.h](fs/btrfs/fs.h)
- [fs/btrfs/inode.c](fs/btrfs/inode.c)
- [fs/btrfs/ioctl.c](fs/btrfs/ioctl.c)
- [fs/btrfs/locking.c](fs/btrfs/locking.c)
- [fs/btrfs/locking.h](fs/btrfs/locking.h)
- [fs/btrfs/messages.c](fs/btrfs/messages.c)
- [fs/btrfs/messages.h](fs/btrfs/messages.h)
- [fs/btrfs/ordered-data.c](fs/btrfs/ordered-data.c)
- [fs/btrfs/ordered-data.h](fs/btrfs/ordered-data.h)
- [fs/btrfs/print-tree.c](fs/btrfs/print-tree.c)
- [fs/btrfs/qgroup.c](fs/btrfs/qgroup.c)
- [fs/btrfs/raid56.c](fs/btrfs/raid56.c)
- [fs/btrfs/raid56.h](fs/btrfs/raid56.h)
- [fs/btrfs/rcu-string.h](fs/btrfs/rcu-string.h)
- [fs/btrfs/reflink.c](fs/btrfs/reflink.c)
- [fs/btrfs/relocation.c](fs/btrfs/relocation.c)
- [fs/btrfs/relocation.h](fs/btrfs/relocation.h)
- [fs/btrfs/root-tree.c](fs/btrfs/root-tree.c)
- [fs/btrfs/scrub.c](fs/btrfs/scrub.c)
- [fs/btrfs/scrub.h](fs/btrfs/scrub.h)
- [fs/btrfs/send.c](fs/btrfs/send.c)
- [fs/btrfs/send.h](fs/btrfs/send.h)
- [fs/btrfs/space-info.c](fs/btrfs/space-info.c)
- [fs/btrfs/space-info.h](fs/btrfs/space-info.h)
- [fs/btrfs/subpage.c](fs/btrfs/subpage.c)
- [fs/btrfs/subpage.h](fs/btrfs/subpage.h)
- [fs/btrfs/super.c](fs/btrfs/super.c)
- [fs/btrfs/sysfs.c](fs/btrfs/sysfs.c)
- [fs/btrfs/tests/btrfs-tests.c](fs/btrfs/tests/btrfs-tests.c)
- [fs/btrfs/tests/extent-map-tests.c](fs/btrfs/tests/extent-map-tests.c)
- [fs/btrfs/tests/inode-tests.c](fs/btrfs/tests/inode-tests.c)
- [fs/btrfs/transaction.c](fs/btrfs/transaction.c)
- [fs/btrfs/transaction.h](fs/btrfs/transaction.h)
- [fs/btrfs/tree-checker.c](fs/btrfs/tree-checker.c)
- [fs/btrfs/tree-checker.h](fs/btrfs/tree-checker.h)
- [fs/btrfs/tree-log.c](fs/btrfs/tree-log.c)
- [fs/btrfs/tree-log.h](fs/btrfs/tree-log.h)
- [fs/btrfs/tree-mod-log.c](fs/btrfs/tree-mod-log.c)
- [fs/btrfs/volumes.c](fs/btrfs/volumes.c)
- [fs/btrfs/volumes.h](fs/btrfs/volumes.h)
- [fs/btrfs/zoned.c](fs/btrfs/zoned.c)
- [fs/btrfs/zoned.h](fs/btrfs/zoned.h)
- [fs/ext2/ialloc.c](fs/ext2/ialloc.c)
- [fs/ext4/Makefile](fs/ext4/Makefile)
- [fs/ext4/acl.c](fs/ext4/acl.c)
- [fs/ext4/balloc.c](fs/ext4/balloc.c)
- [fs/ext4/block_validity.c](fs/ext4/block_validity.c)
- [fs/ext4/crypto.c](fs/ext4/crypto.c)
- [fs/ext4/ext4.h](fs/ext4/ext4.h)
- [fs/ext4/ext4_jbd2.c](fs/ext4/ext4_jbd2.c)
- [fs/ext4/ext4_jbd2.h](fs/ext4/ext4_jbd2.h)
- [fs/ext4/extents.c](fs/ext4/extents.c)
- [fs/ext4/extents_status.c](fs/ext4/extents_status.c)
- [fs/ext4/extents_status.h](fs/ext4/extents_status.h)
- [fs/ext4/fast_commit.c](fs/ext4/fast_commit.c)
- [fs/ext4/fast_commit.h](fs/ext4/fast_commit.h)
- [fs/ext4/fsync.c](fs/ext4/fsync.c)
- [fs/ext4/hash.c](fs/ext4/hash.c)
- [fs/ext4/ialloc.c](fs/ext4/ialloc.c)
- [fs/ext4/indirect.c](fs/ext4/indirect.c)
- [fs/ext4/inline.c](fs/ext4/inline.c)
- [fs/ext4/inode.c](fs/ext4/inode.c)
- [fs/ext4/ioctl.c](fs/ext4/ioctl.c)
- [fs/ext4/mballoc.c](fs/ext4/mballoc.c)
- [fs/ext4/mballoc.h](fs/ext4/mballoc.h)
- [fs/ext4/migrate.c](fs/ext4/migrate.c)
- [fs/ext4/mmp.c](fs/ext4/mmp.c)
- [fs/ext4/move_extent.c](fs/ext4/move_extent.c)
- [fs/ext4/namei.c](fs/ext4/namei.c)
- [fs/ext4/orphan.c](fs/ext4/orphan.c)
- [fs/ext4/page-io.c](fs/ext4/page-io.c)
- [fs/ext4/resize.c](fs/ext4/resize.c)
- [fs/ext4/super.c](fs/ext4/super.c)
- [fs/ext4/symlink.c](fs/ext4/symlink.c)
- [fs/ext4/sysfs.c](fs/ext4/sysfs.c)
- [fs/ext4/verity.c](fs/ext4/verity.c)
- [fs/jbd2/checkpoint.c](fs/jbd2/checkpoint.c)
- [fs/jbd2/commit.c](fs/jbd2/commit.c)
- [fs/jbd2/journal.c](fs/jbd2/journal.c)
- [fs/jbd2/recovery.c](fs/jbd2/recovery.c)
- [fs/jbd2/transaction.c](fs/jbd2/transaction.c)
- [fs/ocfs2/journal.c](fs/ocfs2/journal.c)
- [fs/ocfs2/journal.h](fs/ocfs2/journal.h)
- [fs/ocfs2/ocfs2.h](fs/ocfs2/ocfs2.h)
- [fs/ocfs2/slot_map.c](fs/ocfs2/slot_map.c)
- [fs/ocfs2/super.c](fs/ocfs2/super.c)
- [include/linux/jbd2.h](include/linux/jbd2.h)
- [include/linux/sbitmap.h](include/linux/sbitmap.h)
- [include/trace/events/btrfs.h](include/trace/events/btrfs.h)
- [include/trace/events/ext4.h](include/trace/events/ext4.h)
- [include/trace/events/jbd2.h](include/trace/events/jbd2.h)
- [include/uapi/linux/blkzoned.h](include/uapi/linux/blkzoned.h)
- [include/uapi/linux/btrfs.h](include/uapi/linux/btrfs.h)
- [include/uapi/linux/btrfs_tree.h](include/uapi/linux/btrfs_tree.h)
- [include/uapi/linux/can/bcm.h](include/uapi/linux/can/bcm.h)
- [lib/sbitmap.c](lib/sbitmap.c)
- [lib/test_vmalloc.c](lib/test_vmalloc.c)
- [net/rds/bind.c](net/rds/bind.c)
- [net/sched/sch_sfb.c](net/sched/sch_sfb.c)

</details>



This document covers the file system implementations in the WSL2 Linux kernel, focusing on the two major filesystems: EXT4 and Btrfs. It explains their architecture, key data structures, and integration with the Virtual File System (VFS) layer.

For information about I/O subsystems and storage drivers, see [I/O Subsystems](#4). For details about device drivers, see [Device Drivers](#7).

## Overview

The WSL2 kernel implements two primary filesystems that provide different approaches to data storage and management:

- **EXT4**: A traditional journaling filesystem with robust metadata consistency via JBD2
- **Btrfs**: A copy-on-write filesystem with advanced features like snapshots, checksumming, and RAID support

Both filesystems integrate with the kernel's Virtual File System layer and provide POSIX-compliant file operations while implementing distinct on-disk formats and internal architectures.

## Filesystem Architecture

```mermaid
graph TB
    subgraph "User Space"
        APPS["Applications"]
        TOOLS["File System Tools"]
    end
    
    subgraph "System Call Interface" 
        SYSCALLS["System Calls<br/>open, read, write, etc"]
    end
    
    subgraph "Virtual File System Layer"
        VFS["VFS Layer"]
        DCACHE["Dentry Cache"]
        ICACHE["Inode Cache"]
        PAGECACHE["Page Cache"]
    end
    
    subgraph "EXT4 Filesystem"
        EXT4_SUPER["ext4_super_operations<br/>fs/ext4/super.c"]
        EXT4_INODE["ext4_inode_operations<br/>fs/ext4/inode.c"]
        EXT4_FILE["ext4_file_operations<br/>fs/ext4/file.c"]
        EXT4_MBALLOC["mballoc<br/>fs/ext4/mballoc.c"]
        JBD2["JBD2 Journaling<br/>fs/jbd2/journal.c"]
    end
    
    subgraph "Btrfs Filesystem"
        BTRFS_SUPER["btrfs_super_operations<br/>fs/btrfs/super.c"]
        BTRFS_INODE["btrfs_inode_operations<br/>fs/btrfs/inode.c"]
        BTRFS_FILE["btrfs_file_operations<br/>fs/btrfs/file.c"]
        EXTENT_TREE["extent_tree<br/>fs/btrfs/extent-tree.c"]
        COW_TREE["ctree<br/>fs/btrfs/ctree.c"]
    end
    
    subgraph "Block Layer"
        BLOCK["Block Layer"]
        BIO["Bio Operations"]
    end
    
    APPS --> SYSCALLS
    TOOLS --> SYSCALLS
    SYSCALLS --> VFS
    VFS --> DCACHE
    VFS --> ICACHE
    VFS --> PAGECACHE
    
    VFS --> EXT4_SUPER
    VFS --> BTRFS_SUPER
    
    EXT4_SUPER --> EXT4_INODE
    EXT4_SUPER --> EXT4_FILE
    EXT4_INODE --> EXT4_MBALLOC
    EXT4_INODE --> JBD2
    
    BTRFS_SUPER --> BTRFS_INODE
    BTRFS_SUPER --> BTRFS_FILE
    BTRFS_INODE --> EXTENT_TREE
    BTRFS_INODE --> COW_TREE
    
    EXT4_MBALLOC --> BLOCK
    JBD2 --> BLOCK
    EXTENT_TREE --> BLOCK
    COW_TREE --> BLOCK
    
    BLOCK --> BIO
```

Sources: [fs/ext4/super.c:125-130](), [fs/btrfs/super.c:1-20](), [fs/ext4/inode.c:1-50](), [fs/btrfs/inode.c:116-122]()

## EXT4 Filesystem

EXT4 is a mature journaling filesystem that extends EXT3 with improved performance and larger file support. It uses the JBD2 (Journaling Block Device v2) subsystem for metadata consistency.

### EXT4 Components

```mermaid
graph TB
    subgraph "EXT4 Core Components"
        SUPER["ext4_super_block<br/>Superblock Management"]
        INODE_CACHE["ext4_inode_info<br/>In-memory Inodes"]
        EXTENT["ext4_extent<br/>Extent Trees"]
    end
    
    subgraph "Allocation Subsystem"
        MBALLOC["ext4_mb_*<br/>Multi-block Allocator"]
        BUDDY["ext4_buddy<br/>Buddy Bitmaps"]
        PREALLOC["ext4_prealloc_space<br/>Preallocation"]
    end
    
    subgraph "Journaling (JBD2)"
        JOURNAL["journal_t<br/>Journal Handle"]
        TRANSACTION["transaction_t<br/>Transaction State"]
        JH["journal_head<br/>Buffer Journaling"]
    end
    
    subgraph "Directory Operations"
        NAMEI["ext4_dir_operations<br/>Directory Lookup"]
        HTREE["dx_root<br/>Hash Tree Directories"]
    end
    
    subgraph "Fast Commit"
        FC["ext4_fc_*<br/>Fast Commit Log"]
        FC_TRACK["ext4_fc_track_*<br/>Change Tracking"]
    end
    
    SUPER --> INODE_CACHE
    INODE_CACHE --> EXTENT
    INODE_CACHE --> MBALLOC
    MBALLOC --> BUDDY
    MBALLOC --> PREALLOC
    
    INODE_CACHE --> JOURNAL
    JOURNAL --> TRANSACTION
    TRANSACTION --> JH
    
    SUPER --> NAMEI
    NAMEI --> HTREE
    
    JOURNAL --> FC
    FC --> FC_TRACK
```

Sources: [fs/ext4/super.c:63-98](), [fs/ext4/mballoc.c:42-221](), [fs/ext4/inode.c:52-120](), [fs/ext4/namei.c:1-40](), [fs/ext4/fast_commit.c:1-10]()

### EXT4 Key Data Structures

The EXT4 filesystem uses several critical data structures:

| Structure | Purpose | Location |
|-----------|---------|----------|
| `ext4_super_block` | On-disk superblock format | [fs/ext4/ext4.h:1000-1200]() |
| `ext4_sb_info` | In-memory superblock info | [fs/ext4/ext4.h:1400-1600]() |
| `ext4_inode_info` | In-memory inode extension | [fs/ext4/ext4.h:900-1000]() |
| `ext4_extent` | Extent tree nodes | [fs/ext4/ext4.h:300-400]() |
| `ext4_allocation_context` | Block allocation context | [fs/ext4/mballoc.c:200-300]() |

### EXT4 Block Allocation

EXT4 uses a sophisticated multi-block allocator (`mballoc`) that implements buddy allocation algorithms for efficient space management:

```mermaid
graph LR
    subgraph "Allocation Request Flow"
        REQ["ext4_mb_new_blocks()"]
        NORM["ext4_mb_normalize_request()"]
        GROUP["ext4_mb_find_good_group_*()"]
        BUDDY["ext4_mb_find_by_goal()"]
        ALLOC["ext4_mb_mark_used()"]
    end
    
    subgraph "Preallocation"  
        INODE_PA["Inode Preallocation"]
        GROUP_PA["Locality Group PA"]
    end
    
    subgraph "Buddy System"
        BITMAP["Block Bitmaps"]
        BUDDY_DATA["Buddy Information"]
    end
    
    REQ --> NORM
    NORM --> GROUP
    GROUP --> BUDDY
    BUDDY --> ALLOC
    
    NORM --> INODE_PA
    NORM --> GROUP_PA
    
    BUDDY --> BITMAP
    BUDDY --> BUDDY_DATA
```

Sources: [fs/ext4/mballoc.c:4000-4200](), [fs/ext4/mballoc.c:2500-2700](), [fs/ext4/mballoc.c:1500-1700]()

## Btrfs Filesystem

Btrfs is a modern copy-on-write filesystem designed for advanced features like snapshots, online defragmentation, and built-in RAID support.

### Btrfs Architecture

```mermaid
graph TB
    subgraph "Btrfs Core Trees"
        FS_TREE["btrfs_root<br/>File System Tree"]
        EXTENT_TREE["btrfs_root<br/>Extent Tree"]
        CHUNK_TREE["btrfs_root<br/>Chunk Tree"]
        DEV_TREE["btrfs_root<br/>Device Tree"]
        ROOT_TREE["btrfs_root<br/>Root Tree"]
    end
    
    subgraph "Copy-on-Write Engine"
        COW["btrfs_cow_block()"]
        TRANSACTION["btrfs_transaction"]
        COMMIT["btrfs_commit_transaction()"]
    end
    
    subgraph "Extent Management"
        EXTENT_MAP["extent_map"]
        EXTENT_BUFFER["extent_buffer"]
        EXTENT_IO["extent_io_tree"]
    end
    
    subgraph "Block Groups"
        BG["btrfs_block_group"]
        SPACE_INFO["btrfs_space_info"]
        FREE_SPACE["btrfs_free_space_cache"]
    end
    
    subgraph "Multi-device Support"
        FS_DEVICES["btrfs_fs_devices"]
        DEVICE["btrfs_device"]
        CHUNK_MAP["btrfs_chunk_map"]
    end
    
    FS_TREE --> COW
    EXTENT_TREE --> COW
    COW --> TRANSACTION
    TRANSACTION --> COMMIT
    
    FS_TREE --> EXTENT_MAP
    EXTENT_MAP --> EXTENT_BUFFER
    EXTENT_BUFFER --> EXTENT_IO
    
    EXTENT_TREE --> BG
    BG --> SPACE_INFO
    BG --> FREE_SPACE
    
    CHUNK_TREE --> FS_DEVICES
    FS_DEVICES --> DEVICE
    DEVICE --> CHUNK_MAP
```

Sources: [fs/btrfs/ctree.h:100-200](), [fs/btrfs/extent-tree.c:1-50](), [fs/btrfs/block-group.c:1-30](), [fs/btrfs/volumes.c:200-300]()

### Btrfs Key Data Structures

| Structure | Purpose | Location |
|-----------|---------|----------|
| `btrfs_fs_info` | Filesystem instance | [fs/btrfs/ctree.h:800-1000]() |
| `btrfs_root` | B-tree root structure | [fs/btrfs/ctree.h:300-400]() |
| `btrfs_inode` | Btrfs inode extension | [fs/btrfs/btrfs_inode.h:1-100]() |
| `extent_buffer` | Tree node buffer | [fs/btrfs/extent_io.c:100-200]() |
| `btrfs_block_group` | Block group management | [fs/btrfs/block-group.c:100-200]() |

### Btrfs I/O and Extent Management

```mermaid
graph LR
    subgraph "I/O Path"
        READ_REQ["btrfs_read_folio()"]
        WRITE_REQ["btrfs_write_folio()"]
        BIO_CTRL["btrfs_bio_ctrl"]
        SUBMIT["btrfs_submit_bio()"]
    end
    
    subgraph "Extent Processing"
        DELALLOC["btrfs_run_delalloc_range()"]
        COW_RANGE["cow_file_range()"]
        COMPRESS["compress_file_range()"]
        ORDERED["btrfs_ordered_extent"]
    end
    
    subgraph "Checksumming"
        CSUM_TREE["Checksum Tree"]
        VERIFY["btrfs_verify_data_csum()"]
        ERROR_CORRECT["btrfs_repair_io_failure()"]
    end
    
    READ_REQ --> BIO_CTRL
    WRITE_REQ --> DELALLOC
    DELALLOC --> COW_RANGE
    COW_RANGE --> COMPRESS
    COMPRESS --> ORDERED
    
    BIO_CTRL --> SUBMIT
    ORDERED --> SUBMIT
    
    SUBMIT --> VERIFY
    VERIFY --> CSUM_TREE
    VERIFY --> ERROR_CORRECT
```

Sources: [fs/btrfs/inode.c:600-800](), [fs/btrfs/extent_io.c:400-600](), [fs/btrfs/inode.c:320-400]()

## Advanced Btrfs Features

### Scrubbing and Data Integrity

Btrfs implements comprehensive data verification through its scrubbing subsystem:

```mermaid
graph TB
    subgraph "Scrub Infrastructure"
        SCRUB_CTX["scrub_ctx<br/>Scrub Context"]
        SCRUB_STRIPE["scrub_stripe<br/>Stripe Processing"] 
        SCRUB_SECTOR["scrub_sector_verification<br/>Sector Verification"]
    end
    
    subgraph "Error Detection"
        CSUM_ERROR["Checksum Errors"]
        IO_ERROR["I/O Errors"] 
        META_ERROR["Metadata Errors"]
    end
    
    subgraph "Repair Operations"
        REPAIR_BIO["Repair Bio Submission"]
        MIRROR_READ["Mirror Reading"]
        REWRITE["Block Rewriting"]
    end
    
    SCRUB_CTX --> SCRUB_STRIPE
    SCRUB_STRIPE --> SCRUB_SECTOR
    
    SCRUB_SECTOR --> CSUM_ERROR
    SCRUB_SECTOR --> IO_ERROR
    SCRUB_SECTOR --> META_ERROR
    
    CSUM_ERROR --> REPAIR_BIO
    IO_ERROR --> MIRROR_READ
    META_ERROR --> REWRITE
    
    REPAIR_BIO --> REWRITE
    MIRROR_READ --> REWRITE
```

Sources: [fs/btrfs/scrub.c:185-221](), [fs/btrfs/scrub.c:67-102](), [fs/btrfs/scrub.c:400-500]()

### RAID and Multi-Device Support

Btrfs provides built-in RAID functionality across multiple devices:

```mermaid
graph TB
    subgraph "Device Management"
        FS_DEVICES["btrfs_fs_devices<br/>Device Set"]
        DEVICE["btrfs_device<br/>Individual Device"]
        DEV_REPLACE["btrfs_dev_replace<br/>Device Replacement"]
    end
    
    subgraph "RAID Implementation"
        RAID_ATTR["btrfs_raid_attr[]<br/>RAID Configurations"]
        RAID56["raid56.c<br/>RAID5/6 Implementation"]
        CHUNK_MAP["btrfs_chunk_map<br/>Logical to Physical"]
    end
    
    subgraph "Volume Management"
        CHUNK_TREE["Chunk Tree"]
        BLOCK_GROUP["Block Groups"]
        STRIPE_MAP["Stripe Mapping"]
    end
    
    FS_DEVICES --> DEVICE
    FS_DEVICES --> DEV_REPLACE
    
    DEVICE --> RAID_ATTR
    RAID_ATTR --> RAID56
    RAID56 --> CHUNK_MAP
    
    CHUNK_MAP --> CHUNK_TREE
    CHUNK_TREE --> BLOCK_GROUP
    BLOCK_GROUP --> STRIPE_MAP
```

Sources: [fs/btrfs/volumes.c:43-161](), [fs/btrfs/raid56.c:1-50](), [fs/btrfs/volumes.c:5000-5200]()

## VFS Integration

Both filesystems integrate with the Linux VFS through standardized operation structures:

### File Operations Interface

```mermaid
graph LR
    subgraph "VFS Layer"
        VFS_OPS["file_operations<br/>inode_operations<br/>super_operations"]
    end
    
    subgraph "EXT4 Implementation"
        EXT4_FILE_OPS["ext4_file_operations"]
        EXT4_INODE_OPS["ext4_file_inode_operations"]  
        EXT4_SUPER_OPS["ext4_sops"]
    end
    
    subgraph "Btrfs Implementation"
        BTRFS_FILE_OPS["btrfs_file_operations"]
        BTRFS_INODE_OPS["btrfs_file_inode_operations"]
        BTRFS_SUPER_OPS["btrfs_super_ops"]
    end
    
    VFS_OPS --> EXT4_FILE_OPS
    VFS_OPS --> EXT4_INODE_OPS
    VFS_OPS --> EXT4_SUPER_OPS
    
    VFS_OPS --> BTRFS_FILE_OPS
    VFS_OPS --> BTRFS_INODE_OPS
    VFS_OPS --> BTRFS_SUPER_OPS
```

Sources: [fs/ext4/file.c:800-900](), [fs/btrfs/file.c:3500-3600](), [fs/ext4/super.c:1500-1600](), [fs/btrfs/super.c:2000-2100]()

## Transaction and Consistency Models

### EXT4 Journaling with JBD2

EXT4 uses JBD2 for atomic metadata updates:

- **Ordered Mode**: Data written before metadata journal commit
- **Writeback Mode**: Data written independently of metadata  
- **Journal Mode**: Both data and metadata journaled
- **Fast Commit**: Optimized commit path for small transactions

### Btrfs Copy-on-Write Transactions

Btrfs uses COW semantics for consistency:

- **Atomic Updates**: All changes within a transaction are atomic
- **Snapshot Consistency**: Snapshots represent consistent filesystem state
- **Multi-Root**: Multiple subvolume roots within single transaction
- **Tree Log**: Write-ahead logging for `fsync()` operations

Sources: [fs/jbd2/journal.c:1-50](), [fs/ext4/fast_commit.c:100-200](), [fs/btrfs/transaction.c:1-50](), [fs/btrfs/tree-log.c:1-50]()24:T61cd,# EXT4 Filesystem

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [drivers/net/wireless/broadcom/brcm80211/brcmfmac/pno.c](drivers/net/wireless/broadcom/brcm80211/brcmfmac/pno.c)
- [fs/ext2/ialloc.c](fs/ext2/ialloc.c)
- [fs/ext4/Makefile](fs/ext4/Makefile)
- [fs/ext4/acl.c](fs/ext4/acl.c)
- [fs/ext4/balloc.c](fs/ext4/balloc.c)
- [fs/ext4/block_validity.c](fs/ext4/block_validity.c)
- [fs/ext4/crypto.c](fs/ext4/crypto.c)
- [fs/ext4/ext4.h](fs/ext4/ext4.h)
- [fs/ext4/ext4_jbd2.c](fs/ext4/ext4_jbd2.c)
- [fs/ext4/ext4_jbd2.h](fs/ext4/ext4_jbd2.h)
- [fs/ext4/extents.c](fs/ext4/extents.c)
- [fs/ext4/extents_status.c](fs/ext4/extents_status.c)
- [fs/ext4/extents_status.h](fs/ext4/extents_status.h)
- [fs/ext4/fast_commit.c](fs/ext4/fast_commit.c)
- [fs/ext4/fast_commit.h](fs/ext4/fast_commit.h)
- [fs/ext4/fsync.c](fs/ext4/fsync.c)
- [fs/ext4/hash.c](fs/ext4/hash.c)
- [fs/ext4/ialloc.c](fs/ext4/ialloc.c)
- [fs/ext4/indirect.c](fs/ext4/indirect.c)
- [fs/ext4/inline.c](fs/ext4/inline.c)
- [fs/ext4/inode.c](fs/ext4/inode.c)
- [fs/ext4/ioctl.c](fs/ext4/ioctl.c)
- [fs/ext4/mballoc.c](fs/ext4/mballoc.c)
- [fs/ext4/mballoc.h](fs/ext4/mballoc.h)
- [fs/ext4/migrate.c](fs/ext4/migrate.c)
- [fs/ext4/mmp.c](fs/ext4/mmp.c)
- [fs/ext4/move_extent.c](fs/ext4/move_extent.c)
- [fs/ext4/namei.c](fs/ext4/namei.c)
- [fs/ext4/orphan.c](fs/ext4/orphan.c)
- [fs/ext4/page-io.c](fs/ext4/page-io.c)
- [fs/ext4/resize.c](fs/ext4/resize.c)
- [fs/ext4/super.c](fs/ext4/super.c)
- [fs/ext4/symlink.c](fs/ext4/symlink.c)
- [fs/ext4/sysfs.c](fs/ext4/sysfs.c)
- [fs/ext4/verity.c](fs/ext4/verity.c)
- [fs/jbd2/checkpoint.c](fs/jbd2/checkpoint.c)
- [fs/jbd2/commit.c](fs/jbd2/commit.c)
- [fs/jbd2/journal.c](fs/jbd2/journal.c)
- [fs/jbd2/recovery.c](fs/jbd2/recovery.c)
- [fs/jbd2/transaction.c](fs/jbd2/transaction.c)
- [fs/ocfs2/journal.c](fs/ocfs2/journal.c)
- [fs/ocfs2/journal.h](fs/ocfs2/journal.h)
- [fs/ocfs2/ocfs2.h](fs/ocfs2/ocfs2.h)
- [fs/ocfs2/slot_map.c](fs/ocfs2/slot_map.c)
- [fs/ocfs2/super.c](fs/ocfs2/super.c)
- [include/linux/jbd2.h](include/linux/jbd2.h)
- [include/linux/sbitmap.h](include/linux/sbitmap.h)
- [include/trace/events/ext4.h](include/trace/events/ext4.h)
- [include/trace/events/jbd2.h](include/trace/events/jbd2.h)
- [lib/sbitmap.c](lib/sbitmap.c)
- [lib/test_vmalloc.c](lib/test_vmalloc.c)
- [net/rds/bind.c](net/rds/bind.c)
- [net/sched/sch_sfb.c](net/sched/sch_sfb.c)

</details>



## Purpose and Scope

This document provides a comprehensive overview of the EXT4 filesystem implementation in the WSL2 Linux kernel. EXT4 (Fourth Extended Filesystem) is the evolution of the ext3 filesystem, featuring significant improvements in terms of performance, reliability, and capacity. This page covers the on-disk structure, key components, core algorithms, and advanced features of EXT4.

## Introduction to EXT4

EXT4 was designed as a backward-compatible extension of EXT3 with significant improvements including extents, delayed allocation, persistent pre-allocation, improved file allocation, and enhanced journal support. Its primary goal is to improve performance, reliability, and scalability while maintaining backward compatibility with its predecessors.

```mermaid
flowchart TD
    classDef primary fill:#f9f9f9,stroke:#333,stroke-width:1px
    classDef secondary fill:#f0f0f0,stroke:#666,stroke-width:1px
    
    U["User Space Applications"] --> VFS["Virtual File System Layer"]
    VFS --> EXT4["EXT4 Filesystem Driver"]:::primary
    
    subgraph "EXT4 Components"
        SuperBlock["Superblock Management"]:::primary
        Inodes["Inode Management"]:::primary
        Journal["Journaling System"]:::primary
        Extents["Extent Tree Management"]:::primary
        Alloc["Block Allocation"]:::primary
        Dir["Directory Operations"]:::primary
        
        SuperBlock <--> Inodes
        SuperBlock <--> Journal
        Inodes <--> Extents
        Inodes <--> Alloc
        Inodes <--> Dir
        Journal <--> Alloc
    end
    
    EXT4 --> SuperBlock
    
    SuperBlock --> Disk["Physical Storage"]
```

Sources: [fs/ext4/super.c:1-134](). [fs/ext4/ext4.h:1-100]()

## EXT4 On-Disk Structure

EXT4 organizes its data in a hierarchical structure starting with a superblock that contains filesystem-wide information, followed by block groups which divide the filesystem into manageable chunks.

```mermaid
flowchart TD
    subgraph "Filesystem Structure"
        direction TB
        SB["Superblock"]
        BGD["Block Group Descriptors"]
        
        SB --> BGD
        
        subgraph "Block Group 0"
            direction TB
            BG0SB["Superblock Copy"]
            BG0BGD["Block Group Descriptor Copy"]
            BG0BB["Block Bitmap"]
            BG0IB["Inode Bitmap"]
            BG0IT["Inode Table"]
            BG0Data["Data Blocks"]
            
            BG0SB --> BG0BGD --> BG0BB --> BG0IB --> BG0IT --> BG0Data
        end
        
        subgraph "Block Group N"
            direction TB
            BGnBB["Block Bitmap"]
            BGnIB["Inode Bitmap"]
            BGnIT["Inode Table"]
            BGnData["Data Blocks"]
            
            BGnBB --> BGnIB --> BGnIT --> BGnData
        end
        
        BGD --> BG0SB
        BGD --> BGnBB
    end
```

Sources: [fs/ext4/super.c:137-312](). [fs/ext4/ext4.h:386-424]()

### Superblock Structure

The superblock contains critical filesystem metadata including block and inode counts, block size, and feature flags that indicate which EXT4 capabilities are in use.

```
struct ext4_super_block {
    __le32  s_inodes_count;         /* Inodes count */
    __le32  s_blocks_count_lo;      /* Blocks count */
    __le32  s_r_blocks_count_lo;    /* Reserved blocks count */
    __le32  s_free_blocks_count_lo; /* Free blocks count */
    __le32  s_free_inodes_count;    /* Free inodes count */
    __le32  s_first_data_block;     /* First Data Block */
    __le32  s_log_block_size;       /* Block size */
    /* Additional fields... */
}
```

Sources: [fs/ext4/ext4.h:386-415](). [fs/ext4/super.c:275-312]()

### Block Groups

The filesystem is divided into block groups to improve performance and reliability. Each block group contains:
- A copy of the superblock and block group descriptors (in some groups)
- Block bitmap: Tracks allocated/free blocks
- Inode bitmap: Tracks allocated/free inodes
- Inode table: Array of inode data structures
- Data blocks: Actual file data

The block group descriptor structure stores metadata about each block group:

```
struct ext4_group_desc {
    __le32  bg_block_bitmap_lo;     /* Blocks bitmap block */
    __le32  bg_inode_bitmap_lo;     /* Inodes bitmap block */
    __le32  bg_inode_table_lo;      /* Inodes table block */
    __le16  bg_free_blocks_count_lo;/* Free blocks count */
    __le16  bg_free_inodes_count_lo;/* Free inodes count */
    __le16  bg_used_dirs_count_lo;  /* Directories count */
    __le16  bg_flags;               /* EXT4_BG_flags */
    /* Additional fields... */
}
```

Sources: [fs/ext4/ext4.h:390-414](). [fs/ext4/balloc.c:32-61]()

## Inode Management

Inodes (index nodes) are the fundamental data structures that store metadata about files. EXT4 enhanced the inode structure with new features and optimizations.

### Inode Structure

Each inode in EXT4 contains:
- File permissions, ownership, and timestamps
- File size and block counts
- Direct, indirect, or extent pointers to data blocks
- Extended attributes

```
struct ext4_inode {
    __le16  i_mode;         /* File mode */
    __le16  i_uid;          /* Low 16 bits of Owner Uid */
    __le32  i_size_lo;      /* Size in bytes */
    __le32  i_atime;        /* Access time */
    __le32  i_ctime;        /* Inode Change time */
    __le32  i_mtime;        /* Modification time */
    __le32  i_dtime;        /* Deletion Time */
    __le16  i_gid;          /* Low 16 bits of Group Id */
    __le16  i_links_count;  /* Links count */
    __le32  i_blocks_lo;    /* Blocks count */
    __le32  i_flags;        /* File flags */
    /* Block references or extent tree root */
    __le32  i_block[EXT4_N_BLOCKS];
    /* Additional fields... */
}
```

Sources: [fs/ext4/inode.c:142-159](). [fs/ext4/ext4.h:458-500]()

### Inode Allocation

EXT4 uses bitmap-based allocation for inodes through the `ext4_new_inode()` function, which implements sophisticated allocation policies:

#### Inode Allocation Process

```mermaid
flowchart TD
    START["ext4_new_inode()"] --> CHECK_QUOTA["dquot_initialize()"]
    CHECK_QUOTA --> FIND_GROUP["ext4_find_good_group()"]
    
    FIND_GROUP --> GROUP_POLICY{"Directory vs File?"}
    GROUP_POLICY -->|"Directory"| DIR_POLICY["Use Orlov allocator<br/>find_group_orlov()"]
    GROUP_POLICY -->|"File"| FILE_POLICY["Use parent directory group<br/>find_group_dir()"]
    
    DIR_POLICY --> READ_BITMAP["ext4_read_inode_bitmap()"]
    FILE_POLICY --> READ_BITMAP
    
    READ_BITMAP --> FIND_FREE["ext4_find_next_zero_bit()"]
    FIND_FREE --> TEST_SET["ext4_test_and_set_bit()"]
    TEST_SET -->|"Success"| UPDATE_BITMAP["Mark inode in bitmap"]
    TEST_SET -->|"Conflict"| FIND_FREE
    
    UPDATE_BITMAP --> UPDATE_STATS["ext4_free_inodes_set()<br/>ext4_used_dirs_set()"]
    UPDATE_STATS --> INIT_INODE["Initialize ext4_inode structure"]
    INIT_INODE --> INSERT_INODE["insert_inode_locked()"]
    INSERT_INODE --> RETURN["Return new inode"]
```

#### Group Selection Algorithms

EXT4 uses different allocation strategies based on file type:

- **Orlov Allocator** (`find_group_orlov()`): For directories, spreads them across groups to balance load
- **Parent Group** (`find_group_dir()`): For files, tries to allocate in parent directory's group for locality
- **Linear Search** (`find_group_flex()`): Fallback when preferred groups are full

Sources: [fs/ext4/ialloc.c:380-450](). [fs/ext4/ialloc.c:451-520](). [fs/ext4/ialloc.c:750-850]()

## Extent-Based File Storage

One of the most significant improvements in EXT4 over EXT3 is the replacement of the indirect block mapping with an extent-based system. An extent is a contiguous range of blocks, which is more efficient for storing large files.

### Extent Tree Structure

```mermaid
flowchart TD
    subgraph "Inode"
        i_data["i_block[EXT4_N_BLOCKS]"]
    end
    
    subgraph "Extent Header"
        eh_magic["eh_magic"]
        eh_entries["eh_entries"]
        eh_max["eh_max"]
        eh_depth["eh_depth"]
    end
    
    subgraph "Extent Index (Non-leaf)"
        ei_block["ei_block"]
        ei_leaf["ei_leaf_lo/hi"]
    end
    
    subgraph "Extent (Leaf)"
        ee_block["ee_block"]
        ee_len["ee_len"]
        ee_start["ee_start_lo/hi"]
    end
    
    i_data --> eh_magic
    eh_magic --> eh_entries --> eh_max --> eh_depth
    
    eh_depth -- "depth > 0" --> ei_block
    ei_block --> ei_leaf -- "points to" --> ExtentBlock["Extent Block"]
    
    eh_depth -- "depth = 0" --> ee_block
    ee_block --> ee_len --> ee_start -- "points to" --> DataBlocks["Data Blocks"]
    
    ExtentBlock --> ExtentHeader2["Another Extent Header"]
    ExtentHeader2 --> ExtentEntries["Extent Entries"]
```

Sources: [fs/ext4/extents.c:9-83](). [fs/ext4/ext4_extents.h]()

### Extent Operations

EXT4 provides a comprehensive set of operations for managing extents:
- Insertion and deletion of extents
- Splitting and merging extents
- Conversion between written and unwritten extents

Unwritten extents (pre-allocated but not yet written) help improve performance for applications that benefit from pre-allocation while ensuring data consistency.

Sources: [fs/ext4/extents.c:84-260](). [fs/ext4/ext4_extents.h]()

## Block Allocation System

EXT4 implements sophisticated block allocation algorithms to optimize disk usage and performance.

### Multi-Block Allocator (mballoc)

The multi-block allocator (`mballoc`) is implemented in `ext4_mb_regular_allocator()` and provides efficient allocation of multiple contiguous blocks through buddy bitmap algorithms and preallocation strategies.

#### MBalloc Core Components

```mermaid
flowchart TD
    subgraph "Allocation_Context"
        ext4_allocation_context["ext4_allocation_context"]
        ac_criteria["ac_criteria (CR_POWER2_ALIGNED, etc.)"]
        ac_g_ex["ac_g_ex (goal extent)"]
        ac_b_ex["ac_b_ex (best extent)"]
    end
    
    subgraph "Buddy_System"
        ext4_buddy["ext4_buddy structure"]
        bd_buddy["bd_buddy (buddy bitmap)"]
        bd_bitmap["bd_bitmap (block bitmap)"]
        mb_find_buddy["mb_find_buddy()"]
    end
    
    subgraph "Preallocation"
        ext4_prealloc_space["ext4_prealloc_space"]
        pa_inode_list["i_prealloc_list"]
        pa_group_list["lg_prealloc_list"]
        ext4_mb_use_preallocated["ext4_mb_use_preallocated()"]
    end
    
    START["ext4_mb_regular_allocator()"] --> CHECK_PA["ext4_mb_use_preallocated()"]
    CHECK_PA -->|"Hit"| USE_PA["Use preallocation"]
    CHECK_PA -->|"Miss"| NORMALIZE["ext4_mb_normalize_request()"]
    
    NORMALIZE --> CRITERIA_LOOP["Loop through criteria"]
    CRITERIA_LOOP --> LOAD_BUDDY["ext4_mb_load_buddy()"]
    LOAD_BUDDY --> FIND_BY_GOAL["ext4_mb_find_by_goal()"]
    FIND_BY_GOAL --> SCAN_BITMAP["ext4_mb_scan_and_measure()"]
    SCAN_BITMAP --> NEW_PA["ext4_mb_new_preallocation()"]
    
    USE_PA --> RETURN_BLOCKS["Return allocated blocks"]
    NEW_PA --> RETURN_BLOCKS
```

#### Allocation Criteria and Optimization

MBalloc uses multiple allocation criteria (`enum criteria`) in order of preference:

1. **CR_POWER2_ALIGNED**: For power-of-2 requests, uses `s_mb_largest_free_orders` lists
2. **CR_GOAL_LEN_FAST**: Uses `s_mb_avg_fragment_size` lists for quick matching
3. **CR_BEST_AVAIL_LEN**: Proactively trims goal length for better matches
4. **CR_GOAL_LEN_SLOW**: Sequential group scanning with disk I/O
5. **CR_ANY_FREE**: Last resort allocation of any available blocks

#### Buddy Bitmap Algorithm

The buddy system (`ext4_buddy`) manages free space efficiently:

```c
// Buddy bitmap orders from 0 to sb->s_blocksize_bits
// Order 0: individual blocks
// Order N: 2^N contiguous blocks  
void *mb_find_buddy(struct ext4_buddy *e4b, int order, int *max)
```

Sources: [fs/ext4/mballoc.c:131-221](). [fs/ext4/mballoc.c:1800-1900](). [fs/ext4/mballoc.c:4100-4200]()

### Delayed Allocation

EXT4 uses delayed allocation to improve performance by deferring block allocation decisions until data is written to disk:

1. When files are modified, changes are cached in memory
2. Blocks are not allocated immediately
3. When data is flushed to disk, blocks are allocated in an optimal way
4. This reduces fragmentation and allows for more efficient block allocation

Sources: [fs/ext4/inode.c:326-375](). [fs/ext4/mballoc.c:222-316]()

## Journaling and JBD2 Integration

EXT4 uses the JBD2 (Journaling Block Device 2) subsystem to ensure filesystem consistency after system crashes by recording changes in a journal before applying them to the main filesystem.

### JBD2 Architecture Integration

EXT4 integrates with JBD2 through several key components:

```mermaid
flowchart TD
    subgraph "EXT4_Integration"
        ext4_journal_start["ext4_journal_start()"]
        ext4_journal_get_write_access["ext4_journal_get_write_access()"]
        ext4_handle_dirty_metadata["ext4_handle_dirty_metadata()"]
        ext4_journal_stop["ext4_journal_stop()"]
    end
    
    subgraph "JBD2_Core"
        jbd2_journal_start["jbd2_journal_start()"]
        jbd2_journal_get_write_access["jbd2_journal_get_write_access()"]
        jbd2_journal_dirty_metadata["jbd2_journal_dirty_metadata()"]
        jbd2_journal_stop["jbd2_journal_stop()"]
        kjournald2["kjournald2 thread"]
    end
    
    subgraph "Transaction_Management"
        transaction_t["transaction_t"]
        handle_t["handle_t"]
        journal_t["journal_t"]
    end
    
    ext4_journal_start --> jbd2_journal_start
    ext4_journal_get_write_access --> jbd2_journal_get_write_access
    ext4_handle_dirty_metadata --> jbd2_journal_dirty_metadata
    ext4_journal_stop --> jbd2_journal_stop
    
    jbd2_journal_start --> transaction_t
    transaction_t --> handle_t
    handle_t --> journal_t
    kjournald2 --> journal_t
```

Sources: [fs/ext4/ext4_jbd2.h](). [fs/jbd2/journal.c:160-262](). [fs/jbd2/transaction.c:80-106]()

### Journal Modes

EXT4 supports three journal modes controlled by the `data` mount option:

| Mode | Description | Data Journaling | Ordering |
|------|-------------|-----------------|----------|
| `data=journal` | Both metadata and data are written to journal | Yes | Full consistency |
| `data=ordered` | Only metadata journaled, data written before commit | No | Data before metadata |
| `data=writeback` | Only metadata journaled, no ordering constraints | No | No guarantees |

```mermaid
flowchart TD
    TX_BEGIN["jbd2_journal_start()"] --> CHECK_MODE{"Journal Mode?"}
    
    CHECK_MODE -->|"data=journal"| JOURNAL_DATA["Journal both data and metadata"]
    CHECK_MODE -->|"data=ordered"| ORDER_DATA["Write data blocks first"]
    CHECK_MODE -->|"data=writeback"| JOURNAL_META["Journal metadata only"]
    
    JOURNAL_DATA --> COMMIT_JOURNAL["jbd2_journal_commit_transaction()"]
    ORDER_DATA --> WAIT_DATA["Wait for data I/O"]
    WAIT_DATA --> COMMIT_JOURNAL
    JOURNAL_META --> COMMIT_JOURNAL
    
    COMMIT_JOURNAL --> UPDATE_FS["Update filesystem structures"]
    UPDATE_FS --> TX_END["jbd2_journal_stop()"]
```

Sources: [fs/ext4/super.c:1168-1300](). [fs/jbd2/commit.c:1-100](). [fs/ext4/ext4_jbd2.c]()

### JBD2 Transaction Lifecycle

The JBD2 transaction system manages atomic updates through the following lifecycle:

1. **Transaction Creation**: `jbd2_get_transaction()` initializes new `transaction_t`
2. **Handle Allocation**: `jbd2_journal_start()` creates `handle_t` for operations
3. **Buffer Access**: `jbd2_journal_get_write_access()` prepares buffers for modification
4. **Metadata Updates**: `jbd2_journal_dirty_metadata()` marks buffers as dirty
5. **Commit Process**: `kjournald2` thread commits transactions to journal
6. **Checkpoint**: Old transactions are checkpointed to main filesystem

Sources: [fs/jbd2/transaction.c:80-106](). [fs/jbd2/journal.c:160-275](). [fs/jbd2/checkpoint.c:49-100]()

### Fast Commit

Fast commit is an optimization to reduce journaling overhead by implementing fine-grained journaling using a tag-length-value (TLV) log structure. It operates alongside regular JBD2 commits to improve performance for common operations.

#### Fast Commit TLV Structure

Fast commit uses a TLV format with the following tag types:

| Tag Type | Purpose | Structure |
|----------|---------|-----------|
| `EXT4_FC_TAG_ADD_RANGE` | Records addition of new blocks | `ext4_fc_add_range` |
| `EXT4_FC_TAG_DEL_RANGE` | Records deletion of blocks | `ext4_fc_del_range` |
| `EXT4_FC_TAG_CREAT` | Records inode and dentry creation | `ext4_fc_dentry_info` |
| `EXT4_FC_TAG_LINK` | Records directory entry link | `ext4_fc_dentry_info` |
| `EXT4_FC_TAG_UNLINK` | Records directory entry unlink | `ext4_fc_dentry_info` |
| `EXT4_FC_TAG_INODE` | Records inode metadata | `ext4_inode` |
| `EXT4_FC_TAG_TAIL` | Marks fast commit completion | `ext4_fc_tail` |

#### Fast Commit Process Flow

```mermaid
flowchart TD
    START["ext4_fc_commit()"] --> LOCK_INODES["ext4_fc_commit_dentry_updates()"]
    LOCK_INODES --> SUBMIT_DATA["Submit data buffers via ext4_journal_submit_inode_data_buffers()"]
    SUBMIT_DATA --> WAIT_IO["Wait for I/O completion"]
    WAIT_IO --> COMMIT_DENTRY["Process i_fc_dilist entries"]
    
    COMMIT_DENTRY --> WRITE_TLV_CREAT["Write EXT4_FC_TAG_CREAT TLVs"]
    WRITE_TLV_CREAT --> WRITE_TLV_LINK["Write EXT4_FC_TAG_LINK TLVs"]
    WRITE_TLV_LINK --> WRITE_TLV_UNLINK["Write EXT4_FC_TAG_UNLINK TLVs"]
    
    WRITE_TLV_UNLINK --> COMMIT_INODES["Process i_fc_list entries"]
    COMMIT_INODES --> WRITE_TLV_INODE["Write EXT4_FC_TAG_INODE TLVs"]
    WRITE_TLV_INODE --> WRITE_TLV_RANGE["Write EXT4_FC_TAG_ADD_RANGE/DEL_RANGE TLVs"]
    
    WRITE_TLV_RANGE --> WRITE_TAIL["Write EXT4_FC_TAG_TAIL with CRC"]
    WRITE_TAIL --> WAIT_COMMIT["Wait for journal commit"]
    WAIT_COMMIT --> END["ext4_fc_cleanup()"]
```

#### Fast Commit Replay and Recovery

During mount, `ext4_fc_replay()` processes fast commit logs:

1. **Scan Phase**: `ext4_fc_replay_scan()` validates TLV structure and tail tags
2. **Replay Phase**: `ext4_fc_replay_add_range()`, `ext4_fc_replay_unlink()`, etc. apply changes
3. **Cleanup Phase**: Remove processed fast commit data

Fast commit ensures idempotency by storing filesystem state outcomes rather than operations, making replay safe even after crashes.

Sources: [fs/ext4/fast_commit.c:15-166](). [fs/ext4/fast_commit.c:245-400](). [fs/ext4/super.c:511-533]()

## Directory Indexing

EXT4 uses an HTree (hashed B-tree) structure for directory indexing which significantly improves performance for directories with many files.

```mermaid
flowchart TD
    subgraph "Directory HTree"
        Root["Root Block"]
        Index1["Index Block 1"]
        Index2["Index Block 2"]
        Leaf1["Leaf Block 1"]
        Leaf2["Leaf Block 2"]
        Leaf3["Leaf Block 3"]
        Leaf4["Leaf Block 4"]
        
        Root --> Index1
        Root --> Index2
        Index1 --> Leaf1
        Index1 --> Leaf2
        Index2 --> Leaf3
        Index2 --> Leaf4
    end
    
    subgraph "Entry Format"
        direction LR
        inode["Inode Number"]
        reclen["Record Length"]
        namelen["Name Length"]
        filetype["File Type"]
        name["Filename"]
    end
```

Sources: [fs/ext4/namei.c:214-293](). [fs/ext4/namei.c:294-376]()

## Other Advanced Features

### Inline Data

For small files or directories, EXT4 can store data directly in the inode structure, saving disk space and improving performance.

Sources: [fs/ext4/inline.c:23-132](). [fs/ext4/inline.c:133-217]()

### Extent Status Tree

EXT4 maintains an in-memory extent status tree to track the status of file extents (written, unwritten, delayed, hole):

```mermaid
flowchart TD
    subgraph "Extent Status Tree"
        ESTATROOT["Root"]
        ES1["Extent Status 1\nblk: 0-9\nstatus: written"]
        ES2["Extent Status 2\nblk: 10-19\nstatus: unwritten"]
        ES3["Extent Status 3\nblk: 20-29\nstatus: delayed"]
        ES4["Extent Status 4\nblk: 30-39\nstatus: hole"]
        
        ESTATROOT --> ES1
        ESTATROOT --> ES2
        ESTATROOT --> ES3
        ESTATROOT --> ES4
    end
```

Sources: [fs/ext4/extents_status.c:54-101](). [fs/ext4/extents_status.c:102-176]()

### Metadata Checksumming

EXT4 incorporates checksumming for critical metadata to detect corruption:
- Superblock
- Group descriptors
- Extent tree blocks
- Directory entries
- Inode tables

Sources: [fs/ext4/super.c:274-313](). [fs/ext4/namei.c:333-376]()

## System Integration and Interfaces

### Mount Options

EXT4 supports numerous mount options that control its behavior:
- Journal modes: `data=journal`, `data=ordered`, `data=writeback`
- Performance options: `noatime`, `nodiratime`, `delalloc`
- Feature enablement: `acl`, `user_xattr`, `barrier`

Sources: [fs/ext4/super.c:1168-1300]()

### System Calls and IOCTLs

EXT4 implements various system calls and IOCTLs for filesystem management:
- `ioctl(EXT4_IOC_GETFLAGS)`: Get file flags
- `ioctl(EXT4_IOC_SETFLAGS)`: Set file flags
- `ioctl(EXT4_IOC_RESIZE_FS)`: Resize filesystem
- `ioctl(EXT4_IOC_MIGRATE)`: Migrate file to extents

Sources: [fs/ext4/ioctl.c:1-104](). [fs/ext4/ioctl.c:105-223]()

### Sysfs Interface

EXT4 exposes various filesystem statistics and parameters through sysfs:

```
/sys/fs/ext4/<device>/mb_groups           # Multiblock allocator group information
/sys/fs/ext4/<device>/mb_max_to_scan      # Maximum blocks to scan for allocation
/sys/fs/ext4/<device>/mb_min_to_scan      # Minimum blocks to scan for allocation
/sys/fs/ext4/<device>/mb_order2_req       # Minimum size for buddy allocation
/sys/fs/ext4/<device>/mb_stream_req       # Stream request size
/sys/fs/ext4/<device>/mb_group_prealloc   # Group preallocation size
```

Sources: [fs/ext4/sysfs.c:1-101](). [fs/ext4/sysfs.c:102-188]()

## Conclusion

EXT4 represents a significant evolution in the lineage of Linux filesystems, bringing advanced features while maintaining backward compatibility with EXT3. Its extent-based storage, improved allocation algorithms, and journaling optimizations make it a robust and performant filesystem for a wide range of workloads.

Key strengths include:
- Efficient handling of large files and directories through extent mapping
- Improved performance with delayed and multiblock allocation
- Enhanced reliability through metadata checksumming
- Optimized journaling mechanisms including fast commit
- Support for large filesystems (up to 1 EiB theoretical limit)

These features make EXT4 a solid default filesystem choice for many Linux distributions and use cases, offering a good balance of performance, reliability, and compatibility.

Sources: [fs/ext4/super.c:3-62]()25:T42b2,# Btrfs Filesystem

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [arch/mips/include/uapi/asm/ucontext.h](arch/mips/include/uapi/asm/ucontext.h)
- [arch/s390/include/uapi/asm/hwctrset.h](arch/s390/include/uapi/asm/hwctrset.h)
- [arch/x86/include/uapi/asm/kvm.h](arch/x86/include/uapi/asm/kvm.h)
- [fs/btrfs/accessors.c](fs/btrfs/accessors.c)
- [fs/btrfs/accessors.h](fs/btrfs/accessors.h)
- [fs/btrfs/backref.c](fs/btrfs/backref.c)
- [fs/btrfs/backref.h](fs/btrfs/backref.h)
- [fs/btrfs/bio.c](fs/btrfs/bio.c)
- [fs/btrfs/bio.h](fs/btrfs/bio.h)
- [fs/btrfs/block-group.c](fs/btrfs/block-group.c)
- [fs/btrfs/block-group.h](fs/btrfs/block-group.h)
- [fs/btrfs/btrfs_inode.h](fs/btrfs/btrfs_inode.h)
- [fs/btrfs/compression.c](fs/btrfs/compression.c)
- [fs/btrfs/compression.h](fs/btrfs/compression.h)
- [fs/btrfs/ctree.c](fs/btrfs/ctree.c)
- [fs/btrfs/ctree.h](fs/btrfs/ctree.h)
- [fs/btrfs/delayed-inode.c](fs/btrfs/delayed-inode.c)
- [fs/btrfs/delayed-inode.h](fs/btrfs/delayed-inode.h)
- [fs/btrfs/delayed-ref.c](fs/btrfs/delayed-ref.c)
- [fs/btrfs/delayed-ref.h](fs/btrfs/delayed-ref.h)
- [fs/btrfs/disk-io.c](fs/btrfs/disk-io.c)
- [fs/btrfs/disk-io.h](fs/btrfs/disk-io.h)
- [fs/btrfs/extent-io-tree.c](fs/btrfs/extent-io-tree.c)
- [fs/btrfs/extent-tree.c](fs/btrfs/extent-tree.c)
- [fs/btrfs/extent-tree.h](fs/btrfs/extent-tree.h)
- [fs/btrfs/extent_io.c](fs/btrfs/extent_io.c)
- [fs/btrfs/extent_io.h](fs/btrfs/extent_io.h)
- [fs/btrfs/extent_map.c](fs/btrfs/extent_map.c)
- [fs/btrfs/extent_map.h](fs/btrfs/extent_map.h)
- [fs/btrfs/file-item.c](fs/btrfs/file-item.c)
- [fs/btrfs/file-item.h](fs/btrfs/file-item.h)
- [fs/btrfs/file.c](fs/btrfs/file.c)
- [fs/btrfs/free-space-cache.c](fs/btrfs/free-space-cache.c)
- [fs/btrfs/free-space-tree.c](fs/btrfs/free-space-tree.c)
- [fs/btrfs/free-space-tree.h](fs/btrfs/free-space-tree.h)
- [fs/btrfs/fs.h](fs/btrfs/fs.h)
- [fs/btrfs/inode.c](fs/btrfs/inode.c)
- [fs/btrfs/ioctl.c](fs/btrfs/ioctl.c)
- [fs/btrfs/locking.c](fs/btrfs/locking.c)
- [fs/btrfs/locking.h](fs/btrfs/locking.h)
- [fs/btrfs/messages.c](fs/btrfs/messages.c)
- [fs/btrfs/messages.h](fs/btrfs/messages.h)
- [fs/btrfs/ordered-data.c](fs/btrfs/ordered-data.c)
- [fs/btrfs/ordered-data.h](fs/btrfs/ordered-data.h)
- [fs/btrfs/print-tree.c](fs/btrfs/print-tree.c)
- [fs/btrfs/qgroup.c](fs/btrfs/qgroup.c)
- [fs/btrfs/raid56.c](fs/btrfs/raid56.c)
- [fs/btrfs/raid56.h](fs/btrfs/raid56.h)
- [fs/btrfs/rcu-string.h](fs/btrfs/rcu-string.h)
- [fs/btrfs/reflink.c](fs/btrfs/reflink.c)
- [fs/btrfs/relocation.c](fs/btrfs/relocation.c)
- [fs/btrfs/relocation.h](fs/btrfs/relocation.h)
- [fs/btrfs/root-tree.c](fs/btrfs/root-tree.c)
- [fs/btrfs/scrub.c](fs/btrfs/scrub.c)
- [fs/btrfs/scrub.h](fs/btrfs/scrub.h)
- [fs/btrfs/send.c](fs/btrfs/send.c)
- [fs/btrfs/send.h](fs/btrfs/send.h)
- [fs/btrfs/space-info.c](fs/btrfs/space-info.c)
- [fs/btrfs/space-info.h](fs/btrfs/space-info.h)
- [fs/btrfs/subpage.c](fs/btrfs/subpage.c)
- [fs/btrfs/subpage.h](fs/btrfs/subpage.h)
- [fs/btrfs/super.c](fs/btrfs/super.c)
- [fs/btrfs/sysfs.c](fs/btrfs/sysfs.c)
- [fs/btrfs/tests/btrfs-tests.c](fs/btrfs/tests/btrfs-tests.c)
- [fs/btrfs/tests/extent-map-tests.c](fs/btrfs/tests/extent-map-tests.c)
- [fs/btrfs/tests/inode-tests.c](fs/btrfs/tests/inode-tests.c)
- [fs/btrfs/transaction.c](fs/btrfs/transaction.c)
- [fs/btrfs/transaction.h](fs/btrfs/transaction.h)
- [fs/btrfs/tree-checker.c](fs/btrfs/tree-checker.c)
- [fs/btrfs/tree-checker.h](fs/btrfs/tree-checker.h)
- [fs/btrfs/tree-log.c](fs/btrfs/tree-log.c)
- [fs/btrfs/tree-log.h](fs/btrfs/tree-log.h)
- [fs/btrfs/tree-mod-log.c](fs/btrfs/tree-mod-log.c)
- [fs/btrfs/volumes.c](fs/btrfs/volumes.c)
- [fs/btrfs/volumes.h](fs/btrfs/volumes.h)
- [fs/btrfs/zoned.c](fs/btrfs/zoned.c)
- [fs/btrfs/zoned.h](fs/btrfs/zoned.h)
- [include/trace/events/btrfs.h](include/trace/events/btrfs.h)
- [include/uapi/linux/blkzoned.h](include/uapi/linux/blkzoned.h)
- [include/uapi/linux/btrfs.h](include/uapi/linux/btrfs.h)
- [include/uapi/linux/btrfs_tree.h](include/uapi/linux/btrfs_tree.h)
- [include/uapi/linux/can/bcm.h](include/uapi/linux/can/bcm.h)

</details>



This document covers the Btrfs (B-tree File System) implementation in the WSL2 Linux kernel, focusing on its copy-on-write architecture, advanced data management features, and integration with the kernel's I/O subsystem. Btrfs provides enterprise-grade features including snapshots, checksumming, RAID support, and transparent compression.

For information about general filesystem abstractions, see [File Systems](#5). For storage device management, see [NVMe Storage Driver](#4.3).

## Core Architecture

Btrfs implements a copy-on-write filesystem built around multiple B-trees that manage different aspects of the filesystem. The core design centers on the `btrfs_fs_info` structure which coordinates all filesystem operations.

### Btrfs Filesystem Components

```mermaid
graph TB
    subgraph "Filesystem Core"
        FS_INFO["btrfs_fs_info<br/>Filesystem State"]
        SUPER_BLOCK["btrfs_super_block<br/>Superblock Management"]
        TREE_ROOT["btrfs_root<br/>Tree Root Management"]
    end
    
    subgraph "B-tree Management"
        CTREE["ctree.c<br/>B-tree Operations"]
        EXTENT_TREE["extent-tree.c<br/>Extent Allocation"]
        ROOT_TREE["Root Tree<br/>Subvolume Management"]
        CSUM_TREE["Checksum Tree<br/>Data Integrity"]
    end
    
    subgraph "Storage Layer"
        DISK_IO["disk-io.c<br/>Block I/O Operations"]
        EXTENT_IO["extent_io.c<br/>Extent I/O Management"]
        BIO_LAYER["bio.c<br/>Block I/O Abstraction"]
    end
    
    subgraph "Volume Management"
        VOLUMES["volumes.c<br/>Multi-device Support"]
        BLOCK_GROUP["block-group.c<br/>Block Group Management"]
        RAID56["raid56.c<br/>RAID 5/6 Implementation"]
    end
    
    FS_INFO --> SUPER_BLOCK
    FS_INFO --> TREE_ROOT
    TREE_ROOT --> CTREE
    CTREE --> EXTENT_TREE
    CTREE --> ROOT_TREE
    CTREE --> CSUM_TREE
    
    EXTENT_TREE --> DISK_IO
    DISK_IO --> EXTENT_IO
    EXTENT_IO --> BIO_LAYER
    
    FS_INFO --> VOLUMES
    VOLUMES --> BLOCK_GROUP
    VOLUMES --> RAID56
```

Sources: [fs/btrfs/ctree.h:621-895](), [fs/btrfs/disk-io.c:621-697](), [fs/btrfs/volumes.c:352-428]()

### Copy-on-Write Implementation

The copy-on-write mechanism is implemented through the extent allocation system and transaction management. When data is modified, new extents are allocated and the old data remains accessible until the transaction commits.

```mermaid
graph TB
    subgraph "COW Write Path"
        WRITE_REQUEST["Write Request"]
        COW_FILE_RANGE["cow_file_range<br/>fs/btrfs/inode.c:612-694"]
        ALLOC_EXTENT["btrfs_reserve_extent<br/>Allocate New Extent"]
        CREATE_ORDERED["btrfs_alloc_ordered_extent<br/>Create Ordered Extent"]
    end
    
    subgraph "Transaction Management"
        TRANS_HANDLE["btrfs_trans_handle<br/>Transaction Context"]
        COMMIT_TRANS["btrfs_commit_transaction<br/>Atomic Commit"]
        UPDATE_REFS["Update Reference Counts"]
    end
    
    subgraph "Extent Management"
        EXTENT_BUFFER["extent_buffer<br/>Cached Tree Blocks"]
        EXTENT_MAP["extent_map<br/>Extent Mapping Cache"]
        ORDERED_DATA["btrfs_ordered_extent<br/>Ordered I/O Tracking"]
    end
    
    WRITE_REQUEST --> COW_FILE_RANGE
    COW_FILE_RANGE --> ALLOC_EXTENT
    ALLOC_EXTENT --> CREATE_ORDERED
    CREATE_ORDERED --> TRANS_HANDLE
    
    TRANS_HANDLE --> COMMIT_TRANS
    COMMIT_TRANS --> UPDATE_REFS
    
    ALLOC_EXTENT --> EXTENT_BUFFER
    CREATE_ORDERED --> EXTENT_MAP
    CREATE_ORDERED --> ORDERED_DATA
```

Sources: [fs/btrfs/inode.c:612-694](), [fs/btrfs/extent-tree.c:4200-4350](), [fs/btrfs/transaction.c:2200-2400]()

## B-tree Structure and Operations

Btrfs uses multiple specialized B-trees to organize filesystem metadata. Each tree serves a specific purpose and is managed through the `btrfs_root` structure.

### B-tree Types and Operations

```mermaid
graph TB
    subgraph "Core B-tree Operations"
        SEARCH_SLOT["btrfs_search_slot<br/>fs/btrfs/ctree.c:2800-3000"]
        INSERT_ITEM["btrfs_insert_item<br/>Insert Tree Item"]
        DELETE_ITEM["btrfs_delete_item<br/>Delete Tree Item"]
        SPLIT_NODE["split_node<br/>B-tree Rebalancing"]
    end
    
    subgraph "Tree Types"
        EXTENT_TREE_NODE["Extent Tree<br/>BTRFS_EXTENT_TREE_OBJECTID<br/>Space Allocation"]
        ROOT_TREE_NODE["Root Tree<br/>BTRFS_ROOT_TREE_OBJECTID<br/>Subvolume References"]
        CHUNK_TREE_NODE["Chunk Tree<br/>BTRFS_CHUNK_TREE_OBJECTID<br/>Physical Layout"]
        FS_TREE_NODE["FS Tree<br/>BTRFS_FS_TREE_OBJECTID<br/>File Metadata"]
    end
    
    subgraph "Tree Management"
        TREE_MOD_LOG["btrfs_tree_mod_log<br/>Tree Modification Tracking"]
        EXTENT_BUFFER_OPS["extent_buffer Operations<br/>Node Caching"]
        TREE_BALANCE["balance_level<br/>Tree Balancing"]
    end
    
    SEARCH_SLOT --> EXTENT_TREE_NODE
    SEARCH_SLOT --> ROOT_TREE_NODE
    SEARCH_SLOT --> CHUNK_TREE_NODE
    SEARCH_SLOT --> FS_TREE_NODE
    
    INSERT_ITEM --> SPLIT_NODE
    DELETE_ITEM --> TREE_BALANCE
    
    SPLIT_NODE --> TREE_MOD_LOG
    TREE_BALANCE --> EXTENT_BUFFER_OPS
```

Sources: [fs/btrfs/ctree.c:2800-3000](), [fs/btrfs/ctree.h:1200-1300](), [fs/btrfs/disk-io.c:578-617]()

## Advanced Features

### Snapshot and Subvolume Management

Btrfs implements snapshots through reference counting and copy-on-write semantics. Subvolumes are independent file trees with their own root.

| Feature | Implementation | Key Functions |
|---------|---------------|---------------|
| Snapshots | COW + Reference Counting | `btrfs_ioctl_snap_create_v2` |
| Subvolumes | Independent Root Trees | `btrfs_create_subvol_root` |
| Clones | Shared Extent References | `btrfs_clone_files` |

### Data Integrity and Checksumming

```mermaid
graph TB
    subgraph "Checksum Management"
        CSUM_TREE["Checksum Tree<br/>Per-block Checksums"]
        CSUM_VERIFY["btrfs_verify_data_csum<br/>fs/btrfs/inode.c:3500-3600"]
        CSUM_CALC["btrfs_csum_data<br/>Calculate Checksums"]
    end
    
    subgraph "Checksum Types"
        CRC32C["CRC32C<br/>BTRFS_CSUM_TYPE_CRC32<br/>Default Algorithm"]
        XXHASH["XXHASH64<br/>BTRFS_CSUM_TYPE_XXHASH<br/>Fast Hashing"]
        SHA256["SHA256<br/>BTRFS_CSUM_TYPE_SHA256<br/>Cryptographic Hash"]
        BLAKE2["BLAKE2B<br/>BTRFS_CSUM_TYPE_BLAKE2<br/>Modern Algorithm"]
    end
    
    subgraph "Error Handling"
        SCRUB_OPS["scrub.c<br/>Data Verification"]
        REPAIR_BIO["btrfs_repair_io_failure<br/>Automatic Repair"]
        MIRROR_SELECT["btrfs_map_block<br/>Mirror Selection"]
    end
    
    CSUM_VERIFY --> CSUM_TREE
    CSUM_CALC --> CRC32C
    CSUM_CALC --> XXHASH
    CSUM_CALC --> SHA256
    CSUM_CALC --> BLAKE2
    
    CSUM_VERIFY --> SCRUB_OPS
    SCRUB_OPS --> REPAIR_BIO
    REPAIR_BIO --> MIRROR_SELECT
```

Sources: [fs/btrfs/ctree.c:41-51](), [fs/btrfs/inode.c:3500-3600](), [fs/btrfs/scrub.c:389-456]()

## I/O Subsystem Integration

Btrfs integrates with the kernel I/O subsystem through the `btrfs_bio` structure and extent I/O management.

### Extent I/O Management

```mermaid
graph TB
    subgraph "I/O Request Processing"
        SUBMIT_BIO["btrfs_submit_bio<br/>fs/btrfs/bio.c:200-300"]
        BIO_CTRL["btrfs_bio_ctrl<br/>I/O Control Structure"]
        EXTENT_IO_TREE["extent_io_tree<br/>Page State Tracking"]
    end
    
    subgraph "Compression Integration"
        COMPRESS_PAGES["btrfs_compress_pages<br/>Data Compression"]
        DECOMPRESS_BIO["btrfs_submit_compressed_read<br/>Decompression I/O"]
        COMPRESS_TYPES["BTRFS_COMPRESS_ZLIB<br/>BTRFS_COMPRESS_LZO<br/>BTRFS_COMPRESS_ZSTD"]
    end
    
    subgraph "Direct I/O Support"
        DIO_PRIVATE["btrfs_dio_private<br/>fs/btrfs/inode.c:88-95"]
        DIO_SUBMIT["btrfs_submit_direct<br/>Direct I/O Submission"]
        ORDERED_EXTENT["btrfs_ordered_extent<br/>I/O Ordering"]
    end
    
    SUBMIT_BIO --> BIO_CTRL
    BIO_CTRL --> EXTENT_IO_TREE
    
    SUBMIT_BIO --> COMPRESS_PAGES
    COMPRESS_PAGES --> DECOMPRESS_BIO
    DECOMPRESS_BIO --> COMPRESS_TYPES
    
    SUBMIT_BIO --> DIO_PRIVATE
    DIO_PRIVATE --> DIO_SUBMIT
    DIO_SUBMIT --> ORDERED_EXTENT
```

Sources: [fs/btrfs/bio.c:200-300](), [fs/btrfs/inode.c:88-95](), [fs/btrfs/extent_io.c:108-146]()

## Volume Management and RAID Support

Btrfs supports multiple devices and various RAID configurations through its volume management layer.

### Multi-Device Architecture

| RAID Level | Implementation | Redundancy | Key Structure |
|------------|---------------|------------|---------------|
| RAID 0 | Striping | None | `BTRFS_RAID_RAID0` |
| RAID 1 | Mirroring | 1 copy | `BTRFS_RAID_RAID1` |
| RAID 5 | Parity | 1 disk failure | `BTRFS_RAID_RAID5` |
| RAID 6 | Double Parity | 2 disk failures | `BTRFS_RAID_RAID6` |
| RAID 10 | Stripe + Mirror | 1 disk per stripe | `BTRFS_RAID_RAID10` |

### Device and Chunk Management

```mermaid
graph TB
    subgraph "Device Management"
        FS_DEVICES["btrfs_fs_devices<br/>Device Collection"]
        DEVICE["btrfs_device<br/>Individual Device"]
        DEV_REPLACE["btrfs_dev_replace<br/>Device Replacement"]
    end
    
    subgraph "Chunk Management"
        CHUNK_MAP["btrfs_chunk_map<br/>Physical Mapping"]
        BLOCK_GROUP_MGMT["btrfs_block_group<br/>Space Management"]
        CHUNK_ALLOC["btrfs_chunk_alloc<br/>Chunk Allocation"]
    end
    
    subgraph "RAID Implementation"
        RAID56_RBIO["btrfs_raid_bio<br/>fs/btrfs/raid56.c:100-200"]
        PARITY_CALC["raid6_gen_syndrome<br/>Parity Calculation"]
        STRIPE_HASH["btrfs_stripe_hash<br/>Stripe Coordination"]
    end
    
    FS_DEVICES --> DEVICE
    DEVICE --> DEV_REPLACE
    
    DEVICE --> CHUNK_MAP
    CHUNK_MAP --> BLOCK_GROUP_MGMT
    BLOCK_GROUP_MGMT --> CHUNK_ALLOC
    
    CHUNK_MAP --> RAID56_RBIO
    RAID56_RBIO --> PARITY_CALC
    RAID56_RBIO --> STRIPE_HASH
```

Sources: [fs/btrfs/volumes.c:368-393](), [fs/btrfs/block-group.c:179-213](), [fs/btrfs/raid56.c:45-100]()

## Zoned Device Support

Btrfs includes specialized support for zoned block devices (SMR/ZNS) through the zoned device infrastructure.

### Zoned Device Integration

```mermaid
graph TB
    subgraph "Zoned Device Info"
        ZONED_INFO["btrfs_zoned_device_info<br/>fs/btrfs/zoned.c:363-431"]
        ZONE_CACHE["zone_cache<br/>Zone Information Cache"]
        ACTIVE_ZONES["active_zones_left<br/>Zone Management"]
    end
    
    subgraph "Zone Operations"
        ZONE_FINISH["do_zone_finish<br/>Zone Completion"]
        ZONE_RESET["btrfs_reset_device_zone<br/>Zone Reset"]
        SB_LOG_ZONES["sb_zones<br/>Superblock Zone Management"]
    end
    
    subgraph "Write Pointer Management"
        WP_TRACKING["write_pointer<br/>Zone Write Pointer"]
        SEQ_ZONES["seq_zones<br/>Sequential Zone Bitmap"]
        EMPTY_ZONES["empty_zones<br/>Empty Zone Tracking"]
    end
    
    ZONED_INFO --> ZONE_CACHE
    ZONE_CACHE --> ACTIVE_ZONES
    
    ZONE_CACHE --> ZONE_FINISH
    ZONE_FINISH --> ZONE_RESET
    ZONE_RESET --> SB_LOG_ZONES
    
    ACTIVE_ZONES --> WP_TRACKING
    WP_TRACKING --> SEQ_ZONES
    SEQ_ZONES --> EMPTY_ZONES
```

Sources: [fs/btrfs/zoned.c:360-431](), [fs/btrfs/zoned.c:69-160](), [fs/btrfs/zoned.c:441-517]()

## Data Scrubbing and Verification

The scrub subsystem provides comprehensive data verification and repair capabilities across all storage devices.

### Scrub Architecture

```mermaid
graph TB
    subgraph "Scrub Context"
        SCRUB_CTX["scrub_ctx<br/>fs/btrfs/scrub.c:185-221"]
        SCRUB_STRIPE["scrub_stripe<br/>Stripe Processing Unit"]
        SCRUB_PROGRESS["btrfs_scrub_progress<br/>Progress Tracking"]
    end
    
    subgraph "Verification Process"
        SECTOR_VERIFY["scrub_sector_verification<br/>fs/btrfs/scrub.c:68-86"]
        CSUM_CHECK["Checksum Verification"]
        META_CHECK["Metadata Verification"]
        ERROR_BITMAP["Error Tracking Bitmaps"]
    end
    
    subgraph "Repair Operations"
        READ_REPAIR["scrub_handle_errored_block<br/>Error Recovery"]
        MIRROR_REPAIR["scrub_repair_block_from_good_copy<br/>Mirror Repair"]
        WRITE_REPAIR["scrub_write_block_to_dev_replace<br/>Device Replace"]
    end
    
    SCRUB_CTX --> SCRUB_STRIPE
    SCRUB_STRIPE --> SCRUB_PROGRESS
    
    SCRUB_STRIPE --> SECTOR_VERIFY
    SECTOR_VERIFY --> CSUM_CHECK
    SECTOR_VERIFY --> META_CHECK
    CSUM_CHECK --> ERROR_BITMAP
    
    ERROR_BITMAP --> READ_REPAIR
    READ_REPAIR --> MIRROR_REPAIR
    MIRROR_REPAIR --> WRITE_REPAIR
```

Sources: [fs/btrfs/scrub.c:185-221](), [fs/btrfs/scrub.c:68-86](), [fs/btrfs/scrub.c:1800-2000]()

Btrfs provides a comprehensive copy-on-write filesystem implementation with advanced features for data integrity, multi-device support, and enterprise-grade reliability. The modular architecture allows for efficient integration with kernel I/O subsystems while maintaining consistency across complex storage configurations.26:T41fc,# Architecture-Specific Implementations

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [Documentation/riscv/acpi.rst](Documentation/riscv/acpi.rst)
- [Documentation/riscv/boot-image-header.rst](Documentation/riscv/boot-image-header.rst)
- [Documentation/riscv/boot.rst](Documentation/riscv/boot.rst)
- [Documentation/riscv/hwprobe.rst](Documentation/riscv/hwprobe.rst)
- [Documentation/riscv/index.rst](Documentation/riscv/index.rst)
- [Documentation/riscv/vector.rst](Documentation/riscv/vector.rst)
- [arch/powerpc/include/uapi/asm/elf.h](arch/powerpc/include/uapi/asm/elf.h)
- [arch/powerpc/kernel/ptrace/ptrace-decl.h](arch/powerpc/kernel/ptrace/ptrace-decl.h)
- [arch/powerpc/kernel/ptrace/ptrace-view.c](arch/powerpc/kernel/ptrace/ptrace-view.c)
- [arch/riscv/Kconfig](arch/riscv/Kconfig)
- [arch/riscv/Kconfig.errata](arch/riscv/Kconfig.errata)
- [arch/riscv/Makefile](arch/riscv/Makefile)
- [arch/riscv/Makefile.postlink](arch/riscv/Makefile.postlink)
- [arch/riscv/boot/.gitignore](arch/riscv/boot/.gitignore)
- [arch/riscv/boot/Makefile](arch/riscv/boot/Makefile)
- [arch/riscv/errata/Makefile](arch/riscv/errata/Makefile)
- [arch/riscv/errata/andes/Makefile](arch/riscv/errata/andes/Makefile)
- [arch/riscv/errata/andes/errata.c](arch/riscv/errata/andes/errata.c)
- [arch/riscv/errata/sifive/errata.c](arch/riscv/errata/sifive/errata.c)
- [arch/riscv/errata/thead/errata.c](arch/riscv/errata/thead/errata.c)
- [arch/riscv/include/asm/alternative-macros.h](arch/riscv/include/asm/alternative-macros.h)
- [arch/riscv/include/asm/alternative.h](arch/riscv/include/asm/alternative.h)
- [arch/riscv/include/asm/asm-prototypes.h](arch/riscv/include/asm/asm-prototypes.h)
- [arch/riscv/include/asm/asm.h](arch/riscv/include/asm/asm.h)
- [arch/riscv/include/asm/barrier.h](arch/riscv/include/asm/barrier.h)
- [arch/riscv/include/asm/cfi.h](arch/riscv/include/asm/cfi.h)
- [arch/riscv/include/asm/cpufeature.h](arch/riscv/include/asm/cpufeature.h)
- [arch/riscv/include/asm/elf.h](arch/riscv/include/asm/elf.h)
- [arch/riscv/include/asm/entry-common.h](arch/riscv/include/asm/entry-common.h)
- [arch/riscv/include/asm/hwcap.h](arch/riscv/include/asm/hwcap.h)
- [arch/riscv/include/asm/hwprobe.h](arch/riscv/include/asm/hwprobe.h)
- [arch/riscv/include/asm/insn.h](arch/riscv/include/asm/insn.h)
- [arch/riscv/include/asm/kfence.h](arch/riscv/include/asm/kfence.h)
- [arch/riscv/include/asm/mmu.h](arch/riscv/include/asm/mmu.h)
- [arch/riscv/include/asm/mmu_context.h](arch/riscv/include/asm/mmu_context.h)
- [arch/riscv/include/asm/processor.h](arch/riscv/include/asm/processor.h)
- [arch/riscv/include/asm/ptrace.h](arch/riscv/include/asm/ptrace.h)
- [arch/riscv/include/asm/signal.h](arch/riscv/include/asm/signal.h)
- [arch/riscv/include/asm/stacktrace.h](arch/riscv/include/asm/stacktrace.h)
- [arch/riscv/include/asm/switch_to.h](arch/riscv/include/asm/switch_to.h)
- [arch/riscv/include/asm/syscall.h](arch/riscv/include/asm/syscall.h)
- [arch/riscv/include/asm/syscall_wrapper.h](arch/riscv/include/asm/syscall_wrapper.h)
- [arch/riscv/include/asm/thread_info.h](arch/riscv/include/asm/thread_info.h)
- [arch/riscv/include/asm/tlbflush.h](arch/riscv/include/asm/tlbflush.h)
- [arch/riscv/include/asm/vdso.h](arch/riscv/include/asm/vdso.h)
- [arch/riscv/include/asm/vdso/data.h](arch/riscv/include/asm/vdso/data.h)
- [arch/riscv/include/asm/vdso/gettimeofday.h](arch/riscv/include/asm/vdso/gettimeofday.h)
- [arch/riscv/include/asm/vdso/processor.h](arch/riscv/include/asm/vdso/processor.h)
- [arch/riscv/include/asm/vector.h](arch/riscv/include/asm/vector.h)
- [arch/riscv/include/uapi/asm/auxvec.h](arch/riscv/include/uapi/asm/auxvec.h)
- [arch/riscv/include/uapi/asm/hwcap.h](arch/riscv/include/uapi/asm/hwcap.h)
- [arch/riscv/include/uapi/asm/hwprobe.h](arch/riscv/include/uapi/asm/hwprobe.h)
- [arch/riscv/include/uapi/asm/ptrace.h](arch/riscv/include/uapi/asm/ptrace.h)
- [arch/riscv/include/uapi/asm/sigcontext.h](arch/riscv/include/uapi/asm/sigcontext.h)
- [arch/riscv/include/uapi/asm/unistd.h](arch/riscv/include/uapi/asm/unistd.h)
- [arch/riscv/kernel/Makefile](arch/riscv/kernel/Makefile)
- [arch/riscv/kernel/alternative.c](arch/riscv/kernel/alternative.c)
- [arch/riscv/kernel/cfi.c](arch/riscv/kernel/cfi.c)
- [arch/riscv/kernel/compat_syscall_table.c](arch/riscv/kernel/compat_syscall_table.c)
- [arch/riscv/kernel/compat_vdso/Makefile](arch/riscv/kernel/compat_vdso/Makefile)
- [arch/riscv/kernel/copy-unaligned.S](arch/riscv/kernel/copy-unaligned.S)
- [arch/riscv/kernel/copy-unaligned.h](arch/riscv/kernel/copy-unaligned.h)
- [arch/riscv/kernel/cpu.c](arch/riscv/kernel/cpu.c)
- [arch/riscv/kernel/cpufeature.c](arch/riscv/kernel/cpufeature.c)
- [arch/riscv/kernel/crash_core.c](arch/riscv/kernel/crash_core.c)
- [arch/riscv/kernel/entry.S](arch/riscv/kernel/entry.S)
- [arch/riscv/kernel/head.h](arch/riscv/kernel/head.h)
- [arch/riscv/kernel/mcount-dyn.S](arch/riscv/kernel/mcount-dyn.S)
- [arch/riscv/kernel/probes/decode-insn.c](arch/riscv/kernel/probes/decode-insn.c)
- [arch/riscv/kernel/probes/simulate-insn.c](arch/riscv/kernel/probes/simulate-insn.c)
- [arch/riscv/kernel/probes/simulate-insn.h](arch/riscv/kernel/probes/simulate-insn.h)
- [arch/riscv/kernel/process.c](arch/riscv/kernel/process.c)
- [arch/riscv/kernel/ptrace.c](arch/riscv/kernel/ptrace.c)
- [arch/riscv/kernel/signal.c](arch/riscv/kernel/signal.c)
- [arch/riscv/kernel/smpboot.c](arch/riscv/kernel/smpboot.c)
- [arch/riscv/kernel/stacktrace.c](arch/riscv/kernel/stacktrace.c)
- [arch/riscv/kernel/sys_riscv.c](arch/riscv/kernel/sys_riscv.c)
- [arch/riscv/kernel/syscall_table.c](arch/riscv/kernel/syscall_table.c)
- [arch/riscv/kernel/traps.c](arch/riscv/kernel/traps.c)
- [arch/riscv/kernel/vdso.c](arch/riscv/kernel/vdso.c)
- [arch/riscv/kernel/vdso/Makefile](arch/riscv/kernel/vdso/Makefile)
- [arch/riscv/kernel/vdso/hwprobe.c](arch/riscv/kernel/vdso/hwprobe.c)
- [arch/riscv/kernel/vdso/rt_sigreturn.S](arch/riscv/kernel/vdso/rt_sigreturn.S)
- [arch/riscv/kernel/vdso/sys_hwprobe.S](arch/riscv/kernel/vdso/sys_hwprobe.S)
- [arch/riscv/kernel/vdso/vdso.lds.S](arch/riscv/kernel/vdso/vdso.lds.S)
- [arch/riscv/kernel/vector.c](arch/riscv/kernel/vector.c)
- [arch/riscv/mm/context.c](arch/riscv/mm/context.c)
- [arch/riscv/mm/fault.c](arch/riscv/mm/fault.c)
- [arch/riscv/mm/tlbflush.c](arch/riscv/mm/tlbflush.c)
- [arch/riscv/tools/relocs_check.sh](arch/riscv/tools/relocs_check.sh)
- [include/linux/sched/coredump.h](include/linux/sched/coredump.h)
- [include/uapi/linux/elf.h](include/uapi/linux/elf.h)
- [include/uapi/linux/prctl.h](include/uapi/linux/prctl.h)

</details>



This document covers the kernel's architecture-specific implementation layer, which provides hardware abstraction and CPU architecture support. The primary focus is on how different CPU architectures are detected, configured, and supported at runtime, with particular emphasis on ISA extension management and hardware capability detection.

For detailed RISC-V implementation specifics, see [RISC-V Support](#6.1).

## Architecture Abstraction Overview

The kernel provides a layered approach to architecture-specific functionality, separating generic kernel services from hardware-specific implementations.

```mermaid
graph TB
    subgraph "Generic Kernel Layer"
        SCHED["Process Scheduler"]
        MM["Memory Management"] 
        VFS["Virtual File System"]
        NET["Networking Stack"]
    end
    
    subgraph "Architecture Abstraction Layer"
        ARCH_DETECT["Architecture Detection"]
        HWCAP_MGR["Hardware Capability Management"]
        ISA_EXT["ISA Extension Support"]
        TRAP_HANDLER["Trap/Exception Handling"]
    end
    
    subgraph "Architecture-Specific Implementations"
        RISCV_IMPL["RISC-V Implementation"]
        X86_IMPL["x86 Implementation"] 
        ARM64_IMPL["ARM64 Implementation"]
    end
    
    subgraph "Hardware Layer"
        RISCV_HW["RISC-V Hardware"]
        X86_HW["x86 Hardware"]
        ARM64_HW["ARM64 Hardware"]
    end
    
    SCHED --> ARCH_DETECT
    MM --> HWCAP_MGR
    VFS --> ISA_EXT
    NET --> TRAP_HANDLER
    
    ARCH_DETECT --> RISCV_IMPL
    HWCAP_MGR --> RISCV_IMPL
    ISA_EXT --> RISCV_IMPL
    TRAP_HANDLER --> RISCV_IMPL
    
    ARCH_DETECT --> X86_IMPL
    ARCH_DETECT --> ARM64_IMPL
    
    RISCV_IMPL --> RISCV_HW
    X86_IMPL --> X86_HW
    ARM64_IMPL --> ARM64_HW
```

Sources: [arch/riscv/kernel/cpufeature.c:1-758](), [arch/riscv/kernel/cpu.c:1-341](), [arch/riscv/include/asm/hwcap.h:1-159]()

## CPU Identification and Feature Detection

Architecture-specific implementations rely on robust CPU identification and feature detection mechanisms to provide appropriate hardware abstraction.

### RISC-V Implementation Details

```mermaid
graph TD
    subgraph "Boot Process"
        BOOT_START["Kernel Boot"]
        FILL_HWCAP["riscv_fill_hwcap()"]
        PARSE_ISA["riscv_parse_isa_string()"]
        CHECK_EXT["riscv_isa_extension_check()"]
    end
    
    subgraph "CPU Information Storage"
        HART_ISA["hart_isa[NR_CPUS]"]
        RISCV_ISA["riscv_isa bitmap"]
        ELF_HWCAP["elf_hwcap"]
        RISCV_CPUINFO["riscv_cpuinfo per_cpu"]
    end
    
    subgraph "ISA Extension Data"
        ISA_EXT_ARRAY["riscv_isa_ext[]"]
        ISA_EXT_COUNT["riscv_isa_ext_count"] 
        ISA_EXT_DATA["riscv_isa_ext_data"]
    end
    
    subgraph "Runtime Functions"
        ISA_AVAILABLE["__riscv_isa_extension_available()"]
        HAS_EXTENSION["riscv_has_extension_likely()"]
        CPU_HAS_EXT["riscv_cpu_has_extension_likely()"]
        GET_HWCAP["riscv_get_elf_hwcap()"]
    end
    
    BOOT_START --> FILL_HWCAP
    FILL_HWCAP --> PARSE_ISA
    PARSE_ISA --> CHECK_EXT
    
    CHECK_EXT --> HART_ISA
    CHECK_EXT --> RISCV_ISA
    CHECK_EXT --> ELF_HWCAP
    
    PARSE_ISA --> ISA_EXT_ARRAY
    ISA_EXT_ARRAY --> ISA_EXT_COUNT
    ISA_EXT_ARRAY --> ISA_EXT_DATA
    
    HART_ISA --> ISA_AVAILABLE
    RISCV_ISA --> HAS_EXTENSION
    HART_ISA --> CPU_HAS_EXT
    ELF_HWCAP --> GET_HWCAP
```

Sources: [arch/riscv/kernel/cpufeature.c:508-564](), [arch/riscv/kernel/cpufeature.c:154-187](), [arch/riscv/kernel/cpufeature.c:72-81]()

## ISA Extension Management

The kernel maintains detailed information about supported ISA extensions through a structured approach that combines compile-time configuration with runtime detection.

### Extension Categories and Definitions

| Extension Type | Examples | Configuration | Runtime Detection |
|----------------|----------|---------------|-------------------|
| Base ISA | `RISCV_ISA_EXT_i`, `RISCV_ISA_EXT_m`, `RISCV_ISA_EXT_a` | Always enabled | Static |
| Standard Extensions | `RISCV_ISA_EXT_c`, `RISCV_ISA_EXT_f`, `RISCV_ISA_EXT_d` | `CONFIG_RISCV_ISA_C`, `CONFIG_FPU` | Device tree parsing |
| Vector Extensions | `RISCV_ISA_EXT_v` | `CONFIG_RISCV_ISA_V` | Hardware probing |
| Bit Manipulation | `RISCV_ISA_EXT_ZBB`, `RISCV_ISA_EXT_ZBA`, `RISCV_ISA_EXT_ZBS` | `CONFIG_RISCV_ISA_ZBB` | Alternative patching |
| Cache Management | `RISCV_ISA_EXT_ZICBOM`, `RISCV_ISA_EXT_ZICBOZ` | `CONFIG_RISCV_ISA_ZICBOM` | Block size validation |

Sources: [arch/riscv/include/asm/hwcap.h:16-62](), [arch/riscv/kernel/cpufeature.c:154-185]()

### Extension Detection Process

The `riscv_parse_isa_string()` function handles the complex task of parsing ISA strings from device tree or ACPI tables:

- Validates base ISA requirements (`rv32ima` or `rv64ima`)
- Parses single-letter extensions (e.g., `c`, `f`, `d`, `v`)
- Handles multi-letter extensions with version numbers
- Updates per-CPU `hart_isa` bitmaps and global `riscv_isa` bitmap
- Performs extension-specific validation through `riscv_isa_extension_check()`

Sources: [arch/riscv/kernel/cpufeature.c:189-344](), [arch/riscv/kernel/cpufeature.c:83-107]()

## Hardware Capability Exposure

The kernel exposes detected hardware capabilities to userspace through multiple interfaces:

### ELF Auxiliary Vector

The `elf_hwcap` global variable contains capability flags exposed via `AT_HWCAP`:

```c
// Populated during riscv_fill_hwcap()
unsigned long elf_hwcap;

// Filtered for userspace consumption
unsigned long riscv_get_elf_hwcap(void)
```

### Hardware Probing Interface

The `sys_riscv_hwprobe()` system call provides detailed hardware information:

- `RISCV_HWPROBE_KEY_MVENDORID`: CPU vendor identification
- `RISCV_HWPROBE_KEY_MARCHID`: Architecture identification  
- `RISCV_HWPROBE_KEY_MIMPID`: Implementation identification
- `RISCV_HWPROBE_KEY_IMA_EXT_0`: ISA extension capabilities
- `RISCV_HWPROBE_KEY_CPUPERF_0`: Performance characteristics

Sources: [arch/riscv/kernel/sys_riscv.c:85-267](), [arch/riscv/kernel/cpufeature.c:566-576]()

## Architecture Configuration System

### Build-Time Configuration

The Kconfig system provides extensive architecture-specific configuration options:

```mermaid
graph LR
    subgraph "Base Architecture Selection"
        ARCH_RV32I["CONFIG_ARCH_RV32I"]
        ARCH_RV64I["CONFIG_ARCH_RV64I"] 
        BITS_32["32BIT"]
        BITS_64["64BIT"]
    end
    
    subgraph "ISA Extension Options"
        ISA_C["CONFIG_RISCV_ISA_C"]
        ISA_V["CONFIG_RISCV_ISA_V"]
        ISA_ZBB["CONFIG_RISCV_ISA_ZBB"]
        ISA_ZICBOM["CONFIG_RISCV_ISA_ZICBOM"]
        FPU["CONFIG_FPU"]
    end
    
    subgraph "Platform Features"
        SMP["CONFIG_SMP"]
        MMU["CONFIG_MMU"]
        NUMA["CONFIG_NUMA"]
        HOTPLUG_CPU["CONFIG_HOTPLUG_CPU"]
    end
    
    subgraph "Build Configuration"
        CMODEL_MEDLOW["CONFIG_CMODEL_MEDLOW"]
        CMODEL_MEDANY["CONFIG_CMODEL_MEDANY"]
        RELOCATABLE["CONFIG_RELOCATABLE"]
        XIP_KERNEL["CONFIG_XIP_KERNEL"]
    end
    
    ARCH_RV32I --> BITS_32
    ARCH_RV64I --> BITS_64
    
    BITS_64 --> ISA_V
    BITS_64 --> NUMA
    MMU --> ISA_ZBB
    
    SMP --> HOTPLUG_CPU
    MMU --> RELOCATABLE
```

Sources: [arch/riscv/Kconfig:7-933]()

### Compiler and Toolchain Configuration

The build system adapts compilation flags based on selected features:

- ISA string construction: `riscv-march-$(CONFIG_ARCH_RV64I) := rv64ima`
- Extension inclusion: `riscv-march-$(CONFIG_RISCV_ISA_V) := $(riscv-march-y)v`
- Toolchain compatibility: `CONFIG_TOOLCHAIN_HAS_V`, `CONFIG_TOOLCHAIN_HAS_ZBB`
- Code generation flags: `-mcmodel=medany`, `-mstrict-align`

Sources: [arch/riscv/Makefile:57-78](), [arch/riscv/Makefile:103-107]()

## Alternative Instruction Patching

The kernel uses runtime code patching to optimize for detected hardware features:

### Alternative Framework Components

```mermaid
graph TD
    subgraph "Alternative Infrastructure"
        ALT_ENTRY["struct alt_entry"]
        ALT_SECTION[".alternative section"]
        PATCH_FUNC["patch functions"]
    end
    
    subgraph "CPU Feature Patches"
        CPUFEATURE_PATCH["riscv_cpufeature_patch_func()"]
        FEATURE_CHECK["riscv_cpufeature_patch_check()"]
        ISA_EXT_CHECK["__riscv_isa_extension_available()"]
    end
    
    subgraph "Vendor-Specific Patches"
        THEAD_PATCH["thead_errata_patch_func()"]
        SIFIVE_PATCH["sifive_errata_patch_func()"]
        VENDOR_DETECT["riscv_fill_cpu_mfr_info()"]
    end
    
    subgraph "Patching Stages"
        EARLY_BOOT["RISCV_ALTERNATIVES_EARLY_BOOT"]
        BOOT["RISCV_ALTERNATIVES_BOOT"]
        MODULE["RISCV_ALTERNATIVES_MODULE"]
    end
    
    ALT_ENTRY --> CPUFEATURE_PATCH
    ALT_SECTION --> CPUFEATURE_PATCH
    
    CPUFEATURE_PATCH --> FEATURE_CHECK
    FEATURE_CHECK --> ISA_EXT_CHECK
    
    VENDOR_DETECT --> THEAD_PATCH
    VENDOR_DETECT --> SIFIVE_PATCH
    
    EARLY_BOOT --> CPUFEATURE_PATCH
    BOOT --> CPUFEATURE_PATCH
    MODULE --> CPUFEATURE_PATCH
```

Sources: [arch/riscv/kernel/alternative.c:1-223](), [arch/riscv/kernel/cpufeature.c:700-756]()

## Exception and Trap Handling

Architecture-specific trap handling provides the foundation for system call dispatch, fault handling, and interrupt processing:

### Exception Vector Table

The kernel maintains a function pointer table for different exception types:

```c
// Exception handlers in excp_vect_table
do_trap_insn_misaligned    // Instruction address misalignment  
do_trap_insn_fault         // Instruction access fault
do_trap_insn_illegal       // Illegal instruction
do_trap_break              // Breakpoint
do_trap_load_misaligned    // Load address misalignment
do_trap_load_fault         // Load access fault
do_trap_store_misaligned   // Store address misalignment  
do_trap_store_fault        // Store access fault
do_trap_ecall_u            // User environment call (syscall)
```

Sources: [arch/riscv/kernel/entry.S:282-301](), [arch/riscv/kernel/traps.c:147-241]()

The trap handling infrastructure bridges hardware exceptions to kernel services, enabling proper system call dispatch, memory fault handling, and debugging support across all supported architectures.27:T4c5c,# RISC-V Support

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [Documentation/riscv/acpi.rst](Documentation/riscv/acpi.rst)
- [Documentation/riscv/boot-image-header.rst](Documentation/riscv/boot-image-header.rst)
- [Documentation/riscv/boot.rst](Documentation/riscv/boot.rst)
- [Documentation/riscv/hwprobe.rst](Documentation/riscv/hwprobe.rst)
- [Documentation/riscv/index.rst](Documentation/riscv/index.rst)
- [Documentation/riscv/vector.rst](Documentation/riscv/vector.rst)
- [arch/powerpc/include/uapi/asm/elf.h](arch/powerpc/include/uapi/asm/elf.h)
- [arch/powerpc/kernel/ptrace/ptrace-decl.h](arch/powerpc/kernel/ptrace/ptrace-decl.h)
- [arch/powerpc/kernel/ptrace/ptrace-view.c](arch/powerpc/kernel/ptrace/ptrace-view.c)
- [arch/riscv/Kconfig](arch/riscv/Kconfig)
- [arch/riscv/Kconfig.errata](arch/riscv/Kconfig.errata)
- [arch/riscv/Makefile](arch/riscv/Makefile)
- [arch/riscv/Makefile.postlink](arch/riscv/Makefile.postlink)
- [arch/riscv/boot/.gitignore](arch/riscv/boot/.gitignore)
- [arch/riscv/boot/Makefile](arch/riscv/boot/Makefile)
- [arch/riscv/errata/Makefile](arch/riscv/errata/Makefile)
- [arch/riscv/errata/andes/Makefile](arch/riscv/errata/andes/Makefile)
- [arch/riscv/errata/andes/errata.c](arch/riscv/errata/andes/errata.c)
- [arch/riscv/errata/sifive/errata.c](arch/riscv/errata/sifive/errata.c)
- [arch/riscv/errata/thead/errata.c](arch/riscv/errata/thead/errata.c)
- [arch/riscv/include/asm/alternative-macros.h](arch/riscv/include/asm/alternative-macros.h)
- [arch/riscv/include/asm/alternative.h](arch/riscv/include/asm/alternative.h)
- [arch/riscv/include/asm/asm-prototypes.h](arch/riscv/include/asm/asm-prototypes.h)
- [arch/riscv/include/asm/asm.h](arch/riscv/include/asm/asm.h)
- [arch/riscv/include/asm/barrier.h](arch/riscv/include/asm/barrier.h)
- [arch/riscv/include/asm/cfi.h](arch/riscv/include/asm/cfi.h)
- [arch/riscv/include/asm/cpufeature.h](arch/riscv/include/asm/cpufeature.h)
- [arch/riscv/include/asm/elf.h](arch/riscv/include/asm/elf.h)
- [arch/riscv/include/asm/entry-common.h](arch/riscv/include/asm/entry-common.h)
- [arch/riscv/include/asm/hwcap.h](arch/riscv/include/asm/hwcap.h)
- [arch/riscv/include/asm/hwprobe.h](arch/riscv/include/asm/hwprobe.h)
- [arch/riscv/include/asm/insn.h](arch/riscv/include/asm/insn.h)
- [arch/riscv/include/asm/kfence.h](arch/riscv/include/asm/kfence.h)
- [arch/riscv/include/asm/mmu.h](arch/riscv/include/asm/mmu.h)
- [arch/riscv/include/asm/mmu_context.h](arch/riscv/include/asm/mmu_context.h)
- [arch/riscv/include/asm/processor.h](arch/riscv/include/asm/processor.h)
- [arch/riscv/include/asm/ptrace.h](arch/riscv/include/asm/ptrace.h)
- [arch/riscv/include/asm/signal.h](arch/riscv/include/asm/signal.h)
- [arch/riscv/include/asm/stacktrace.h](arch/riscv/include/asm/stacktrace.h)
- [arch/riscv/include/asm/switch_to.h](arch/riscv/include/asm/switch_to.h)
- [arch/riscv/include/asm/syscall.h](arch/riscv/include/asm/syscall.h)
- [arch/riscv/include/asm/syscall_wrapper.h](arch/riscv/include/asm/syscall_wrapper.h)
- [arch/riscv/include/asm/thread_info.h](arch/riscv/include/asm/thread_info.h)
- [arch/riscv/include/asm/tlbflush.h](arch/riscv/include/asm/tlbflush.h)
- [arch/riscv/include/asm/vdso.h](arch/riscv/include/asm/vdso.h)
- [arch/riscv/include/asm/vdso/data.h](arch/riscv/include/asm/vdso/data.h)
- [arch/riscv/include/asm/vdso/gettimeofday.h](arch/riscv/include/asm/vdso/gettimeofday.h)
- [arch/riscv/include/asm/vdso/processor.h](arch/riscv/include/asm/vdso/processor.h)
- [arch/riscv/include/asm/vector.h](arch/riscv/include/asm/vector.h)
- [arch/riscv/include/uapi/asm/auxvec.h](arch/riscv/include/uapi/asm/auxvec.h)
- [arch/riscv/include/uapi/asm/hwcap.h](arch/riscv/include/uapi/asm/hwcap.h)
- [arch/riscv/include/uapi/asm/hwprobe.h](arch/riscv/include/uapi/asm/hwprobe.h)
- [arch/riscv/include/uapi/asm/ptrace.h](arch/riscv/include/uapi/asm/ptrace.h)
- [arch/riscv/include/uapi/asm/sigcontext.h](arch/riscv/include/uapi/asm/sigcontext.h)
- [arch/riscv/include/uapi/asm/unistd.h](arch/riscv/include/uapi/asm/unistd.h)
- [arch/riscv/kernel/Makefile](arch/riscv/kernel/Makefile)
- [arch/riscv/kernel/alternative.c](arch/riscv/kernel/alternative.c)
- [arch/riscv/kernel/cfi.c](arch/riscv/kernel/cfi.c)
- [arch/riscv/kernel/compat_syscall_table.c](arch/riscv/kernel/compat_syscall_table.c)
- [arch/riscv/kernel/compat_vdso/Makefile](arch/riscv/kernel/compat_vdso/Makefile)
- [arch/riscv/kernel/copy-unaligned.S](arch/riscv/kernel/copy-unaligned.S)
- [arch/riscv/kernel/copy-unaligned.h](arch/riscv/kernel/copy-unaligned.h)
- [arch/riscv/kernel/cpu.c](arch/riscv/kernel/cpu.c)
- [arch/riscv/kernel/cpufeature.c](arch/riscv/kernel/cpufeature.c)
- [arch/riscv/kernel/crash_core.c](arch/riscv/kernel/crash_core.c)
- [arch/riscv/kernel/entry.S](arch/riscv/kernel/entry.S)
- [arch/riscv/kernel/head.h](arch/riscv/kernel/head.h)
- [arch/riscv/kernel/mcount-dyn.S](arch/riscv/kernel/mcount-dyn.S)
- [arch/riscv/kernel/probes/decode-insn.c](arch/riscv/kernel/probes/decode-insn.c)
- [arch/riscv/kernel/probes/simulate-insn.c](arch/riscv/kernel/probes/simulate-insn.c)
- [arch/riscv/kernel/probes/simulate-insn.h](arch/riscv/kernel/probes/simulate-insn.h)
- [arch/riscv/kernel/process.c](arch/riscv/kernel/process.c)
- [arch/riscv/kernel/ptrace.c](arch/riscv/kernel/ptrace.c)
- [arch/riscv/kernel/signal.c](arch/riscv/kernel/signal.c)
- [arch/riscv/kernel/smpboot.c](arch/riscv/kernel/smpboot.c)
- [arch/riscv/kernel/stacktrace.c](arch/riscv/kernel/stacktrace.c)
- [arch/riscv/kernel/sys_riscv.c](arch/riscv/kernel/sys_riscv.c)
- [arch/riscv/kernel/syscall_table.c](arch/riscv/kernel/syscall_table.c)
- [arch/riscv/kernel/traps.c](arch/riscv/kernel/traps.c)
- [arch/riscv/kernel/vdso.c](arch/riscv/kernel/vdso.c)
- [arch/riscv/kernel/vdso/Makefile](arch/riscv/kernel/vdso/Makefile)
- [arch/riscv/kernel/vdso/hwprobe.c](arch/riscv/kernel/vdso/hwprobe.c)
- [arch/riscv/kernel/vdso/rt_sigreturn.S](arch/riscv/kernel/vdso/rt_sigreturn.S)
- [arch/riscv/kernel/vdso/sys_hwprobe.S](arch/riscv/kernel/vdso/sys_hwprobe.S)
- [arch/riscv/kernel/vdso/vdso.lds.S](arch/riscv/kernel/vdso/vdso.lds.S)
- [arch/riscv/kernel/vector.c](arch/riscv/kernel/vector.c)
- [arch/riscv/mm/context.c](arch/riscv/mm/context.c)
- [arch/riscv/mm/fault.c](arch/riscv/mm/fault.c)
- [arch/riscv/mm/tlbflush.c](arch/riscv/mm/tlbflush.c)
- [arch/riscv/tools/relocs_check.sh](arch/riscv/tools/relocs_check.sh)
- [include/linux/sched/coredump.h](include/linux/sched/coredump.h)
- [include/uapi/linux/elf.h](include/uapi/linux/elf.h)
- [include/uapi/linux/prctl.h](include/uapi/linux/prctl.h)

</details>



This document covers the RISC-V architecture implementation within the WSL2 Linux kernel, including CPU feature detection, ISA extension handling, memory management, exception processing, and SMP support. For information about other architecture-specific implementations, see [Architecture-Specific Implementations](#6).

The RISC-V support provides a complete implementation of the open-source RISC-V instruction set architecture, supporting both RV32 and RV64 variants with various standard and vendor-specific extensions. The implementation handles hardware abstraction, boot processes, memory management, and integration with core kernel subsystems.

## Architecture Overview

The RISC-V kernel implementation is organized around several key subsystems that handle different aspects of the architecture support:

```mermaid
graph TB
    subgraph "Boot and Initialization"
        KCONFIG["arch/riscv/Kconfig<br/>Architecture Configuration"]
        MAKEFILE["arch/riscv/Makefile<br/>Build System"]
        ENTRY["arch/riscv/kernel/entry.S<br/>Low-level Entry Points"]
    end
    
    subgraph "CPU Management"
        CPUFEATURE["cpufeature.c<br/>riscv_fill_hwcap()"]
        CPU["cpu.c<br/>riscv_of_processor_hartid()"]
        HWCAP["hwcap.h<br/>RISCV_ISA_EXT_*"]
    end
    
    subgraph "Memory Management"
        FAULT["mm/fault.c<br/>handle_page_fault()"]
        CONTEXT["mm/context.c<br/>set_mm_asid()"]
        TLBFLUSH["mm/tlbflush.c<br/>flush_tlb_*()"]
    end
    
    subgraph "Exception Handling"
        TRAPS["traps.c<br/>do_trap_*()"]
        SIGNAL["signal.c<br/>setup_rt_frame()"]
        VECTOR["vector.h<br/>riscv_v_*()"]
    end
    
    subgraph "SMP Support"
        SMPBOOT["smpboot.c<br/>smp_prepare_cpus()"]
        SMP_ENTRY["smp_callin()"]
    end
    
    KCONFIG --> CPUFEATURE
    MAKEFILE --> ENTRY
    CPUFEATURE --> CPU
    CPU --> HWCAP
    FAULT --> CONTEXT
    TRAPS --> SIGNAL
    SMPBOOT --> SMP_ENTRY
    ENTRY --> TRAPS
```

Sources: [arch/riscv/Kconfig:1-933](), [arch/riscv/Makefile:1-191](), [arch/riscv/kernel/cpufeature.c:1-758](), [arch/riscv/kernel/cpu.c:1-341]()

## CPU Feature Detection and ISA Extensions

The RISC-V implementation provides comprehensive ISA extension detection through the `cpufeature` subsystem:

### Hardware Capability Detection

```mermaid
graph TD
    BOOTCPU["Boot CPU"]
    FILL_HWCAP["riscv_fill_hwcap()"]
    ISA_STRING["riscv_fill_hwcap_from_isa_string()"]
    PARSE_ISA["riscv_parse_isa_string()"]
    
    EXT_LIST["riscv_fill_hwcap_from_ext_list()"]
    ISA_DATA["riscv_isa_ext[]<br/>Extension Definitions"]
    
    HWCAP_RESULT["elf_hwcap<br/>Final Hardware Capabilities"]
    ISA_BITMAP["riscv_isa<br/>ISA Extension Bitmap"]
    
    BOOTCPU --> FILL_HWCAP
    FILL_HWCAP --> ISA_STRING
    FILL_HWCAP --> EXT_LIST
    ISA_STRING --> PARSE_ISA
    EXT_LIST --> ISA_DATA
    PARSE_ISA --> HWCAP_RESULT
    EXT_LIST --> ISA_BITMAP
    ISA_DATA --> ISA_BITMAP
```

The system supports detection of standard extensions including:
- **Base Extensions**: I, M, A, F, D, Q, C, V, H
- **Zbb Extensions**: Basic bit manipulation (`RISCV_ISA_EXT_ZBB`)
- **Vector Extensions**: V extension with dynamic sizing (`RISCV_ISA_EXT_v`)
- **Cache Management**: Zicbom, Zicboz for cache operations
- **Supervisor Extensions**: Svnapot, Svpbmt for memory management

Sources: [arch/riscv/kernel/cpufeature.c:508-564](), [arch/riscv/kernel/cpufeature.c:154-187](), [arch/riscv/include/asm/hwcap.h:42-62]()

### Runtime Feature Checking

The kernel provides efficient runtime feature detection through static keys and alternatives:

| Function | Purpose | Usage |
|----------|---------|-------|
| `riscv_has_extension_likely()` | Fast path for common extensions | Hot code paths |
| `riscv_has_extension_unlikely()` | Slow path for rare extensions | Cold code paths |
| `riscv_cpu_has_extension_likely()` | Per-CPU extension check | SMP-aware code |

Sources: [arch/riscv/include/asm/hwcap.h:95-155]()

## Memory Management

The RISC-V memory management implementation handles page tables, TLB management, and ASID allocation:

### Page Fault Handling

```mermaid
graph TD
    PAGEFAULT["Page Fault Exception"]
    HANDLE_PAGE_FAULT["handle_page_fault()"]
    
    VMALLOC_CHECK{"Address in<br/>VMALLOC range?"}
    VMALLOC_FAULT["vmalloc_fault()"]
    
    USER_CHECK{"User mode<br/>access?"}
    VMA_LOCK["lock_vma_under_rcu()"]
    MM_LOCK["lock_mm_and_find_vma()"]
    
    ACCESS_CHECK["access_error()"]
    HANDLE_MM_FAULT["handle_mm_fault()"]
    
    ERROR_HANDLING["mm_fault_error()"]
    
    PAGEFAULT --> HANDLE_PAGE_FAULT
    HANDLE_PAGE_FAULT --> VMALLOC_CHECK
    VMALLOC_CHECK -->|Yes| VMALLOC_FAULT
    VMALLOC_CHECK -->|No| USER_CHECK
    USER_CHECK -->|Yes| VMA_LOCK
    USER_CHECK -->|No| MM_LOCK
    VMA_LOCK --> ACCESS_CHECK
    MM_LOCK --> ACCESS_CHECK
    ACCESS_CHECK --> HANDLE_MM_FAULT
    HANDLE_MM_FAULT --> ERROR_HANDLING
```

The fault handler supports:
- **VMA Lock Optimization**: Per-VMA locking for reduced contention
- **VMALLOC Fault Handling**: Kernel virtual memory synchronization
- **Access Permission Checking**: Execute, read, write validation
- **Fault Type Classification**: `EXC_INST_PAGE_FAULT`, `EXC_LOAD_PAGE_FAULT`, `EXC_STORE_PAGE_FAULT`

Sources: [arch/riscv/mm/fault.c:223-377](), [arch/riscv/mm/fault.c:194-217]()

### ASID Management

The ASID allocator provides efficient TLB management across multiple harts:

| Component | Function | Description |
|-----------|----------|-------------|
| `context_asid_map` | Bitmap tracking | Available ASID allocation |
| `current_version` | Version counter | Rollover detection |
| `set_mm_asid()` | ASID assignment | Per-MM ASID management |
| `__flush_context()` | Global flush | Complete TLB invalidation |

Sources: [arch/riscv/mm/context.c:145-218]()

## Exception and Trap Handling

RISC-V exception handling provides comprehensive trap processing and signal delivery:

### Exception Vector Table

```mermaid
graph LR
    EXCEPTION["Exception/Interrupt"]
    HANDLE_EXCEPTION["handle_exception"]
    
    subgraph "Exception Dispatch"
        EXCP_VECT_TABLE["excp_vect_table"]
        INSN_MISALIGNED["do_trap_insn_misaligned"]
        INSN_FAULT["do_trap_insn_fault"]
        INSN_ILLEGAL["do_trap_insn_illegal"]
        BREAKPOINT["do_trap_break"]
        LOAD_MISALIGNED["do_trap_load_misaligned"]
        LOAD_FAULT["do_trap_load_fault"]
        STORE_MISALIGNED["do_trap_store_misaligned"]
        STORE_FAULT["do_trap_store_fault"]
        ECALL_U["do_trap_ecall_u"]
        PAGE_FAULT["do_page_fault"]
    end
    
    EXCEPTION --> HANDLE_EXCEPTION
    HANDLE_EXCEPTION --> EXCP_VECT_TABLE
    EXCP_VECT_TABLE --> INSN_MISALIGNED
    EXCP_VECT_TABLE --> INSN_FAULT
    EXCP_VECT_TABLE --> INSN_ILLEGAL
    EXCP_VECT_TABLE --> BREAKPOINT
    EXCP_VECT_TABLE --> LOAD_MISALIGNED
    EXCP_VECT_TABLE --> LOAD_FAULT
    EXCP_VECT_TABLE --> STORE_MISALIGNED
    EXCP_VECT_TABLE --> STORE_FAULT
    EXCP_VECT_TABLE --> ECALL_U
    EXCP_VECT_TABLE --> PAGE_FAULT
```

### Signal Frame Handling

The signal implementation supports both standard and vector contexts:

| Structure | Purpose | Key Fields |
|-----------|---------|------------|
| `rt_sigframe` | Signal frame layout | `siginfo`, `ucontext` |
| `sigcontext` | Register context | `sc_regs`, `sc_fpregs`, `sc_extdesc` |
| `__sc_riscv_v_state` | Vector context | Vector registers and CSRs |

The signal handler supports dynamic frame sizing based on enabled extensions:
- Base frame: Integer and FP registers
- Vector extension: Additional vector state (`riscv_v_sc_size`)
- Dynamic sizing: `get_rt_frame_size()` calculates required space

Sources: [arch/riscv/kernel/signal.c:202-219](), [arch/riscv/kernel/signal.c:33-39](), [arch/riscv/kernel/traps.c:282-301]()

## SMP Support

RISC-V SMP implementation handles multi-hart initialization and coordination:

### SMP Boot Process

```mermaid
sequenceDiagram
    participant BOOT as "Boot Hart"
    participant SEC as "Secondary Hart"
    participant SBI as "SBI/Firmware"
    
    BOOT->>BOOT: setup_smp()
    BOOT->>BOOT: smp_prepare_cpus()
    
    loop For each CPU
        BOOT->>SBI: cpu_ops[cpu]->cpu_start()
        SBI->>SEC: Start secondary hart
        SEC->>SEC: smp_callin()
        SEC->>SEC: store_cpu_topology()
        SEC->>SEC: riscv_ipi_enable()
        SEC->>SEC: numa_add_cpu()
        SEC->>SEC: check_unaligned_access()
        SEC->>BOOT: complete(&cpu_running)
    end
```

Key SMP functions:
- `smp_prepare_cpus()`: Initialize CPU topology and NUMA mapping
- `smp_callin()`: Secondary CPU initialization entry point
- `check_unaligned_access()`: Per-CPU performance characterization
- CPU operations through `cpu_ops[]` array for different boot methods

Sources: [arch/riscv/kernel/smpboot.c:49-77](), [arch/riscv/kernel/smpboot.c:235-272]()

### CPU Operations Framework

| Boot Method | Implementation | Configuration |
|-------------|----------------|---------------|
| SBI HSM | `cpu_ops_sbi.c` | Modern SBI implementations |
| Spin-wait | `cpu_ops_spinwait.c` | Legacy firmware support |
| ACPI | ACPI RINTC tables | ACPI-based systems |

Sources: [arch/riscv/kernel/smpboot.c:126-140](), [arch/riscv/kernel/smpboot.c:145-189]()

## Vector Extension Support

The RISC-V Vector (RVV) extension provides SIMD capabilities with dynamic vector length:

### Vector State Management

```mermaid
graph TD
    VECTOR_TRAP["Vector Instruction Trap"]
    FIRST_USE["riscv_v_first_use_handler()"]
    
    SETUP_VSIZE["riscv_v_setup_vsize()"]
    VSTATE_ON["riscv_v_vstate_on()"]
    
    CONTEXT_SWITCH["Context Switch"]
    VSTATE_SAVE["riscv_v_vstate_save()"]
    VSTATE_RESTORE["riscv_v_vstate_restore()"]
    
    SIGNAL_HANDLER["Signal Delivery"]
    SAVE_V_STATE["save_v_state()"]
    RESTORE_V_STATE["__restore_v_state()"]
    
    VECTOR_TRAP --> FIRST_USE
    FIRST_USE --> SETUP_VSIZE
    SETUP_VSIZE --> VSTATE_ON
    
    CONTEXT_SWITCH --> VSTATE_SAVE
    CONTEXT_SWITCH --> VSTATE_RESTORE
    
    SIGNAL_HANDLER --> SAVE_V_STATE
    SIGNAL_HANDLER --> RESTORE_V_STATE
```

Vector support includes:
- **Dynamic Vector Length**: Runtime `vl` determination via `vsetvli`
- **Lazy Initialization**: First-use trap handling for efficiency
- **Context Switching**: Complete vector register save/restore
- **Signal Context**: Vector state preservation across signals

Sources: [arch/riscv/include/asm/vector.h:22-158](), [arch/riscv/kernel/signal.c:71-144]()

## Build System and Configuration

The RISC-V build system handles ISA string generation, toolchain requirements, and architecture-specific compilation:

### ISA String Construction

The Makefile dynamically constructs ISA strings based on configuration:

| Configuration Option | ISA Component | Makefile Variable |
|---------------------|---------------|-------------------|
| `CONFIG_ARCH_RV64I` | `rv64ima` | `riscv-march-$(CONFIG_ARCH_RV64I)` |
| `CONFIG_FPU` | `fd` | `riscv-march-$(CONFIG_FPU)` |  
| `CONFIG_RISCV_ISA_C` | `c` | `riscv-march-$(CONFIG_RISCV_ISA_C)` |
| `CONFIG_RISCV_ISA_V` | `v` | `riscv-march-$(CONFIG_RISCV_ISA_V)` |

### Toolchain Compatibility

The build system handles various toolchain incompatibilities:

```mermaid
graph TD
    TOOLCHAIN_CHECK["Toolchain Detection"]
    
    OLD_ISA_SPEC["TOOLCHAIN_NEEDS_OLD_ISA_SPEC"]
    EXPLICIT_ZICSR["TOOLCHAIN_NEEDS_EXPLICIT_ZICSR_ZIFENCEI"]
    
    ISA_SPEC_22["-Wa,-misa-spec=2.2"]
    ZICSR_ZIFENCEI["_zicsr_zifencei"]
    
    FINAL_MARCH["Final -march string"]
    
    TOOLCHAIN_CHECK --> OLD_ISA_SPEC
    TOOLCHAIN_CHECK --> EXPLICIT_ZICSR
    
    OLD_ISA_SPEC -->|Yes| ISA_SPEC_22
    EXPLICIT_ZICSR -->|Yes| ZICSR_ZIFENCEI
    
    ISA_SPEC_22 --> FINAL_MARCH
    ZICSR_ZIFENCEI --> FINAL_MARCH
```

Sources: [arch/riscv/Makefile:57-78](), [arch/riscv/Kconfig:587-613]()

## Errata and Vendor Extensions

The RISC-V implementation includes comprehensive errata handling for various CPU vendors:

### Alternative Patching System

```mermaid
graph LR
    ALT_ENTRY["struct alt_entry"]
    VENDOR_ID["vendor_id"]
    PATCH_ID["patch_id"]
    
    subgraph "Vendor Handlers"
        THEAD["thead_errata_patch_func()"]
        SIFIVE["sifive_errata_patch_func()"]  
        ANDES["andes_errata_patch_func()"]
    end
    
    PATCH_TEXT["patch_text_nosync()"]
    
    ALT_ENTRY --> VENDOR_ID
    ALT_ENTRY --> PATCH_ID
    
    VENDOR_ID -->|THEAD_VENDOR_ID| THEAD
    VENDOR_ID -->|SIFIVE_VENDOR_ID| SIFIVE
    VENDOR_ID -->|ANDESTECH_VENDOR_ID| ANDES
    
    THEAD --> PATCH_TEXT
    SIFIVE --> PATCH_TEXT
    ANDES --> PATCH_TEXT
```

Supported errata categories:
- **T-Head**: PBMT, CMO, PMU errata (`ERRATA_THEAD_*`)
- **SiFive**: CIP-453, CIP-1200 errata handling
- **Andes**: AX45MP-specific errata

Sources: [arch/riscv/errata/thead/errata.c:72-122](), [arch/riscv/kernel/alternative.c:32-63](), [arch/riscv/errata/sifive/errata.c:15-87]()28:T3e24,# Device Drivers

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [arch/x86/kernel/amd_nb.c](arch/x86/kernel/amd_nb.c)
- [drivers/gpu/drm/amd/amdgpu/amdgpu.h](drivers/gpu/drm/amd/amdgpu/amdgpu.h)
- [drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c](drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c)
- [drivers/gpu/drm/amd/amdgpu/amdgpu_device.c](drivers/gpu/drm/amd/amdgpu/amdgpu_device.c)
- [drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c](drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c)
- [drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c](drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c)
- [drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c](drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c)
- [drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.h](drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.h)
- [drivers/gpu/drm/amd/amdgpu/amdgpu_virt.c](drivers/gpu/drm/amd/amdgpu/amdgpu_virt.c)
- [drivers/gpu/drm/amd/amdgpu/sdma_v5_0.c](drivers/gpu/drm/amd/amdgpu/sdma_v5_0.c)
- [drivers/gpu/drm/amd/amdgpu/sdma_v6_0.c](drivers/gpu/drm/amd/amdgpu/sdma_v6_0.c)
- [drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.c](drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.c)
- [drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h](drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h)
- [drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c](drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c)
- [drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v10.c](drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v10.c)
- [drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v11.c](drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v11.c)
- [drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c](drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c)
- [drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c](drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.h](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.h)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_debugfs.c](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_debugfs.c)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_helpers.c](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_helpers.c)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_mst_types.c](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_mst_types.c)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_mst_types.h](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_mst_types.h)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_plane.c](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_plane.c)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_plane.h](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_plane.h)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_replay.c](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_replay.c)
- [drivers/gpu/drm/amd/display/dc/clk_mgr/dcn31/dcn31_clk_mgr.c](drivers/gpu/drm/amd/display/dc/clk_mgr/dcn31/dcn31_clk_mgr.c)
- [drivers/gpu/drm/amd/display/dc/clk_mgr/dcn314/dcn314_clk_mgr.c](drivers/gpu/drm/amd/display/dc/clk_mgr/dcn314/dcn314_clk_mgr.c)
- [drivers/gpu/drm/amd/display/dc/clk_mgr/dcn314/dcn314_clk_mgr.h](drivers/gpu/drm/amd/display/dc/clk_mgr/dcn314/dcn314_clk_mgr.h)
- [drivers/gpu/drm/amd/display/dc/clk_mgr/dcn315/dcn315_clk_mgr.c](drivers/gpu/drm/amd/display/dc/clk_mgr/dcn315/dcn315_clk_mgr.c)
- [drivers/gpu/drm/amd/display/dc/clk_mgr/dcn316/dcn316_clk_mgr.c](drivers/gpu/drm/amd/display/dc/clk_mgr/dcn316/dcn316_clk_mgr.c)
- [drivers/gpu/drm/amd/display/dc/dcn10/dcn10_dpp_dscl.c](drivers/gpu/drm/amd/display/dc/dcn10/dcn10_dpp_dscl.c)
- [drivers/gpu/drm/amd/display/dc/dml/dcn314/dcn314_fpu.c](drivers/gpu/drm/amd/display/dc/dml/dcn314/dcn314_fpu.c)
- [drivers/gpu/drm/amd/display/dc/dml/display_mode_lib.c](drivers/gpu/drm/amd/display/dc/dml/display_mode_lib.c)
- [drivers/gpu/drm/amd/display/dc/dml/display_mode_lib.h](drivers/gpu/drm/amd/display/dc/dml/display_mode_lib.h)
- [drivers/gpu/drm/amd/display/dc/inc/hw/clk_mgr.h](drivers/gpu/drm/amd/display/dc/inc/hw/clk_mgr.h)
- [drivers/gpu/drm/amd/include/amd_shared.h](drivers/gpu/drm/amd/include/amd_shared.h)
- [include/linux/pci_ids.h](include/linux/pci_ids.h)
- [sound/pci/hda/cs35l41_hda.c](sound/pci/hda/cs35l41_hda.c)
- [sound/pci/hda/cs35l41_hda.h](sound/pci/hda/cs35l41_hda.h)
- [sound/pci/hda/cs35l41_hda_i2c.c](sound/pci/hda/cs35l41_hda_i2c.c)
- [sound/pci/hda/cs35l41_hda_spi.c](sound/pci/hda/cs35l41_hda_spi.c)
- [sound/pci/hda/hda_auto_parser.c](sound/pci/hda/hda_auto_parser.c)
- [sound/pci/hda/hda_component.h](sound/pci/hda/hda_component.h)
- [sound/pci/hda/hda_cs_dsp_ctl.c](sound/pci/hda/hda_cs_dsp_ctl.c)
- [sound/pci/hda/hda_cs_dsp_ctl.h](sound/pci/hda/hda_cs_dsp_ctl.h)
- [sound/pci/hda/hda_local.h](sound/pci/hda/hda_local.h)
- [sound/pci/hda/patch_realtek.c](sound/pci/hda/patch_realtek.c)

</details>



This document covers the device driver implementations in the WSL2 Linux kernel, focusing on hardware support for graphics and audio devices. The primary subsystems documented here are AMD GPU display drivers and HD-Audio codec support, which provide essential hardware abstraction layers for multimedia functionality in virtualized Windows environments.

For information about virtualization-specific drivers and Windows integration components, see [Windows Integration Components](#8).

## AMD GPU Display Driver Subsystem

The AMD GPU display driver subsystem provides comprehensive graphics hardware support through the AMDGPU kernel module and Display Manager (DM) components. The architecture centers around the `amdgpu_device` structure and display core (`dc`) library integration.

```mermaid
graph TB
    subgraph user_space ["User Space"]
        drm_apps["DRM Applications"]
        xorg["X.Org Server"]
    end
    
    subgraph kernel_drm ["Kernel DRM Layer"]
        drm_core["DRM Core"]
        drm_atomic["drm_atomic_helper"]
    end
    
    subgraph amdgpu_driver ["AMDGPU Driver"]
        amdgpu_drv["amdgpu_drv.c<br/>KMS Driver Entry"]
        amdgpu_device["amdgpu_device.c<br/>Device Management"]
        amdgpu_kms["amdgpu_kms_driver<br/>Structure"]
    end
    
    subgraph display_manager ["AMD Display Manager"]
        amdgpu_dm["amdgpu_dm.c<br/>Display Manager Core"]
        amdgpu_dm_connector["amdgpu_dm_connector<br/>Connector Management"]
        amdgpu_dm_crtc["amdgpu_dm_crtc<br/>CRTC Operations"]
        amdgpu_dm_plane["amdgpu_dm_plane<br/>Plane Management"]
    end
    
    subgraph display_core ["Display Core Library"]
        dc_core["dc.h<br/>DC Interface"]
        dc_resource["dc_resource<br/>Resource Management"]
        dc_stream["dc_stream_state<br/>Stream Configuration"]
    end
    
    subgraph hardware ["AMD GPU Hardware"]
        dcn_blocks["DCN Display Blocks"]
        gpu_engines["GPU Engines"]
    end
    
    drm_apps --> drm_core
    xorg --> drm_core
    drm_core --> drm_atomic
    drm_atomic --> amdgpu_drv
    
    amdgpu_drv --> amdgpu_device
    amdgpu_device --> amdgpu_dm
    
    amdgpu_dm --> amdgpu_dm_connector
    amdgpu_dm --> amdgpu_dm_crtc
    amdgpu_dm --> amdgpu_dm_plane
    
    amdgpu_dm --> dc_core
    dc_core --> dc_resource
    dc_core --> dc_stream
    
    dc_resource --> dcn_blocks
    dc_stream --> gpu_engines
```

### Core Device Management

The `amdgpu_device` structure serves as the central device context, initialized through `amdgpu_device_init()` and managed via the `amdgpu_kms_driver` operations. Device initialization follows the pattern:

1. PCI device detection via `amdgpu_pci_probe()`
2. Hardware identification and ASIC-specific setup
3. IP block initialization for graphics, display, and multimedia engines
4. Memory manager and virtual memory setup
5. Display Manager initialization via `amdgpu_dm_init()`

The driver supports multiple AMD GPU families through IP block abstractions, with each ASIC type providing specific implementations for graphics (`gfx`), display controller (`dc`), video (`vcn`), and system DMA (`sdma`) blocks.

Sources: [drivers/gpu/drm/amd/amdgpu/amdgpu_device.c:1-200](), [drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c:1-100](), [drivers/gpu/drm/amd/amdgpu/amdgpu.h:1-150]()

### Display Manager Architecture

The AMD Display Manager (`amdgpu_dm`) acts as an abstraction layer between the Linux DRM subsystem and AMD's Display Core library. Key components include:

| Component | File | Purpose |
|-----------|------|---------|
| `amdgpu_display_manager` | amdgpu_dm.c | Main DM context and initialization |
| `amdgpu_dm_connector` | amdgpu_dm.c | Display output management |
| `dm_crtc_state` | amdgpu_dm_crtc.c | CRTC state management |
| `dm_plane_state` | amdgpu_dm_plane.c | Overlay plane operations |
| `amdgpu_dm_mst_types` | amdgpu_dm_mst_types.c | Multi-Stream Transport |

```mermaid
graph LR
    subgraph drm_subsystem ["DRM Subsystem"]
        drm_connector["drm_connector"]
        drm_crtc["drm_crtc"]
        drm_plane["drm_plane"]
    end
    
    subgraph amdgpu_dm_layer ["AMDGPU DM Layer"]
        dm_connector["amdgpu_dm_connector<br/>to_amdgpu_dm_connector()"]
        dm_crtc["amdgpu_crtc<br/>dm_crtc_state"]
        dm_plane["dm_plane_state<br/>dm_plane_helper_funcs"]
    end
    
    subgraph dc_interface ["Display Core Interface"]
        dc_link["dc_link<br/>Link Management"]
        dc_stream["dc_stream_state<br/>Stream Config"]
        dc_surface["dc_plane_state<br/>Surface Properties"]
    end
    
    drm_connector -.-> dm_connector
    drm_crtc -.-> dm_crtc
    drm_plane -.-> dm_plane
    
    dm_connector --> dc_link
    dm_crtc --> dc_stream
    dm_plane --> dc_surface
```

The Display Manager handles atomic modesetting through `amdgpu_dm_atomic_check()` and `amdgpu_dm_atomic_commit_tail()`, coordinating with the Display Core to validate and apply display configurations.

Sources: [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c:150-300](), [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c:1-50](), [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_plane.c:1-50]()

### Multi-Stream Transport (MST) Support

DisplayPort MST functionality is implemented through `amdgpu_dm_mst_types.c`, providing support for daisy-chained display configurations. The MST manager integrates with DRM's MST helpers:

- `dm_dp_aux_transfer()` - AUX channel communication
- `dm_dp_mst_connector_funcs` - MST connector operations  
- `validate_dsc_caps_on_connector()` - Display Stream Compression validation
- `amdgpu_dm_mst_connector_late_register()` - MST connector registration

Sources: [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_mst_types.c:50-200](), [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_mst_types.h:1-50]()

## HD-Audio Support Subsystem

The HD-Audio subsystem provides comprehensive audio codec support through the ALSA framework, with specialized drivers for Realtek and Cirrus Logic audio controllers commonly found in laptop and desktop systems.

```mermaid
graph TB
    subgraph alsa_core ["ALSA Core"]
        snd_hda_codec["snd_hda_codec<br/>Codec Framework"]
        hda_generic["hda_generic<br/>Generic Parser"]
        hda_jack["hda_jack<br/>Jack Detection"]
    end
    
    subgraph codec_drivers ["Codec Drivers"]
        patch_realtek["patch_realtek.c<br/>Realtek ALC Codecs"]
        cs35l41_hda["cs35l41_hda.c<br/>Cirrus Logic Smart Amp"]
        hda_cs_dsp_ctl["hda_cs_dsp_ctl.c<br/>DSP Control Interface"]
    end
    
    subgraph codec_specific ["Codec-Specific Implementation"]
        alc_spec["alc_spec<br/>Realtek Configuration"]
        cs35l41_hda_spec["cs35l41_hda<br/>CS35L41 Context"]
        coef_access["COEF Access<br/>alc_read_coef_idx()"]
    end
    
    subgraph hardware_layer ["Hardware Layer"]
        hda_controller["HDA Controller"]
        realtek_codec["Realtek ALC Codec"]
        cs35l41_amp["CS35L41 Smart Amplifier"]
    end
    
    snd_hda_codec --> patch_realtek
    snd_hda_codec --> cs35l41_hda
    hda_generic --> patch_realtek
    hda_jack --> patch_realtek
    
    patch_realtek --> alc_spec
    cs35l41_hda --> cs35l41_hda_spec
    cs35l41_hda --> hda_cs_dsp_ctl
    
    alc_spec --> coef_access
    coef_access --> realtek_codec
    cs35l41_hda_spec --> cs35l41_amp
    
    realtek_codec --> hda_controller
    cs35l41_amp --> hda_controller
```

### Realtek ALC Codec Support

The Realtek codec driver (`patch_realtek.c`) provides support for the extensive ALC (Audio Logic Controller) codec family. The driver architecture centers around the `alc_spec` structure, which extends the generic HDA specification with Realtek-specific features.

Key components include:

- **COEF (Coefficient) Management**: Functions like `alc_read_coef_idx()` and `alc_write_coef_idx()` provide access to vendor-specific coefficient registers for fine-tuning audio parameters
- **Fixup System**: Extensive quirk handling through `hda_fixup` structures to address hardware-specific issues across different laptop models
- **GPIO Control**: LED and amplifier control via `alc_setup_gpio()` and related functions
- **Auto-Configuration**: Pin configuration parsing via `alc_parse_auto_config()` with SSID-based system identification

The driver supports automatic codec detection and configuration through the `alc_codec_rename_from_preset()` system, which maps hardware IDs to specific codec variants.

Sources: [sound/pci/hda/patch_realtek.c:1-300](), [sound/pci/hda/patch_realtek.c:79-137]()

### Cirrus Logic CS35L41 Smart Amplifier

The CS35L41 driver (`cs35l41_hda.c`) implements support for Cirrus Logic's smart amplifier with integrated DSP capabilities. The architecture provides both firmware-based and direct register control modes:

```mermaid
graph LR
    subgraph cs35l41_driver ["CS35L41 Driver"]
        cs35l41_hda_comp["cs35l41_hda<br/>Component Structure"]
        firmware_load["cs35l41_request_firmware<br/>Firmware Loading"]
        dsp_control["hda_cs_dsp_ctl<br/>DSP Control Interface"]
    end
    
    subgraph firmware_mgmt ["Firmware Management"]
        wmfw_file["WMFW Firmware<br/>DSP Code"]
        coeff_file["Coefficient File<br/>Tuning Parameters"]
        cs_dsp["cs_dsp<br/>DSP Runtime"]
    end
    
    subgraph hardware_config ["Hardware Configuration"]
        reg_sequences["Register Sequences<br/>cs35l41_hda_config"]
        gpio_config["GPIO Configuration<br/>cs35l41_gpio_config"]
        amp_control["Amplifier Control<br/>Mute/Unmute"]
    end
    
    cs35l41_hda_comp --> firmware_load
    firmware_load --> wmfw_file
    firmware_load --> coeff_file
    wmfw_file --> cs_dsp
    coeff_file --> cs_dsp
    
    cs35l41_hda_comp --> dsp_control
    dsp_control --> reg_sequences
    reg_sequences --> gpio_config
    gpio_config --> amp_control
```

The driver supports automatic firmware selection based on system configuration, using ACPI properties and subsystem IDs to locate appropriate tuning files. DSP control is managed through the `hda_cs_dsp_ctl` interface, providing ALSA controls for real-time parameter adjustment.

Configuration sequences like `cs35l41_hda_config_dsp` and `cs35l41_hda_unmute_dsp` handle hardware initialization and power management.

Sources: [sound/pci/hda/cs35l41_hda.c:1-150](), [sound/pci/hda/cs35l41_hda.c:42-94](), [sound/pci/hda/hda_cs_dsp_ctl.c:1-50]()

### DSP Control Interface

The HDA DSP control system (`hda_cs_dsp_ctl.c`) provides a bridge between ALSA mixer controls and Cirrus Logic DSP firmware parameters. Key functions include:

- `hda_cs_dsp_add_controls()` - Register DSP parameters as ALSA controls
- `hda_cs_dsp_control_remove()` - Clean up controls during device removal
- `hda_cs_dsp_write()` / `hda_cs_dsp_read()` - Parameter access interface

The system enables real-time audio tuning through standard ALSA mixer interfaces while maintaining synchronization with DSP firmware state.

Sources: [sound/pci/hda/hda_cs_dsp_ctl.c:1-100]()29:T3610,# AMD GPU Display Driver

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [arch/x86/kernel/amd_nb.c](arch/x86/kernel/amd_nb.c)
- [drivers/gpu/drm/amd/amdgpu/amdgpu.h](drivers/gpu/drm/amd/amdgpu/amdgpu.h)
- [drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c](drivers/gpu/drm/amd/amdgpu/amdgpu_acpi.c)
- [drivers/gpu/drm/amd/amdgpu/amdgpu_device.c](drivers/gpu/drm/amd/amdgpu/amdgpu_device.c)
- [drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c](drivers/gpu/drm/amd/amdgpu/amdgpu_drv.c)
- [drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c](drivers/gpu/drm/amd/amdgpu/amdgpu_irq.c)
- [drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c](drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.c)
- [drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.h](drivers/gpu/drm/amd/amdgpu/amdgpu_sdma.h)
- [drivers/gpu/drm/amd/amdgpu/amdgpu_virt.c](drivers/gpu/drm/amd/amdgpu/amdgpu_virt.c)
- [drivers/gpu/drm/amd/amdgpu/sdma_v5_0.c](drivers/gpu/drm/amd/amdgpu/sdma_v5_0.c)
- [drivers/gpu/drm/amd/amdgpu/sdma_v6_0.c](drivers/gpu/drm/amd/amdgpu/sdma_v6_0.c)
- [drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.c](drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.c)
- [drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h](drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager.h)
- [drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c](drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_cik.c)
- [drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v10.c](drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v10.c)
- [drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v11.c](drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v11.c)
- [drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c](drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_v9.c)
- [drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c](drivers/gpu/drm/amd/amdkfd/kfd_mqd_manager_vi.c)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.h](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.h)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_crtc.c)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_debugfs.c](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_debugfs.c)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_helpers.c](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_helpers.c)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_mst_types.c](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_mst_types.c)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_mst_types.h](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_mst_types.h)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_plane.c](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_plane.c)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_plane.h](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_plane.h)
- [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_replay.c](drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm_replay.c)
- [drivers/gpu/drm/amd/display/dc/clk_mgr/dcn31/dcn31_clk_mgr.c](drivers/gpu/drm/amd/display/dc/clk_mgr/dcn31/dcn31_clk_mgr.c)
- [drivers/gpu/drm/amd/display/dc/clk_mgr/dcn314/dcn314_clk_mgr.c](drivers/gpu/drm/amd/display/dc/clk_mgr/dcn314/dcn314_clk_mgr.c)
- [drivers/gpu/drm/amd/display/dc/clk_mgr/dcn314/dcn314_clk_mgr.h](drivers/gpu/drm/amd/display/dc/clk_mgr/dcn314/dcn314_clk_mgr.h)
- [drivers/gpu/drm/amd/display/dc/clk_mgr/dcn315/dcn315_clk_mgr.c](drivers/gpu/drm/amd/display/dc/clk_mgr/dcn315/dcn315_clk_mgr.c)
- [drivers/gpu/drm/amd/display/dc/clk_mgr/dcn316/dcn316_clk_mgr.c](drivers/gpu/drm/amd/display/dc/clk_mgr/dcn316/dcn316_clk_mgr.c)
- [drivers/gpu/drm/amd/display/dc/dcn10/dcn10_dpp_dscl.c](drivers/gpu/drm/amd/display/dc/dcn10/dcn10_dpp_dscl.c)
- [drivers/gpu/drm/amd/display/dc/dml/dcn314/dcn314_fpu.c](drivers/gpu/drm/amd/display/dc/dml/dcn314/dcn314_fpu.c)
- [drivers/gpu/drm/amd/display/dc/dml/display_mode_lib.c](drivers/gpu/drm/amd/display/dc/dml/display_mode_lib.c)
- [drivers/gpu/drm/amd/display/dc/dml/display_mode_lib.h](drivers/gpu/drm/amd/display/dc/dml/display_mode_lib.h)
- [drivers/gpu/drm/amd/display/dc/inc/hw/clk_mgr.h](drivers/gpu/drm/amd/display/dc/inc/hw/clk_mgr.h)
- [drivers/gpu/drm/amd/include/amd_shared.h](drivers/gpu/drm/amd/include/amd_shared.h)
- [include/linux/pci_ids.h](include/linux/pci_ids.h)

</details>



## Purpose and Scope

This document covers the AMD GPU Display Driver subsystem within the Linux kernel, specifically the `amdgpu_dm` (Display Manager) component that provides display functionality for AMD graphics hardware. The display driver acts as an interface layer between the Linux DRM (Direct Rendering Manager) subsystem and AMD's Display Core (DC) library, handling display output, connector management, audio integration, and display timing.

For information about AMD GPU compute and graphics functionality, see other relevant subsystem documentation. This document focuses specifically on display output capabilities.

## Architecture Overview

The AMD GPU Display Driver follows a layered architecture where the Display Manager (`amdgpu_dm`) serves as the primary interface between the Linux DRM subsystem and AMD's hardware-specific Display Core library.

```mermaid
graph TB
    subgraph "User Space"
        USERSPACE["User Applications"]
        XORG["X.Org / Wayland"]
    end
    
    subgraph "Kernel DRM Subsystem"
        DRM["DRM Core"]
        DRM_ATOMIC["DRM Atomic API"]
        DRM_CONNECTOR["DRM Connector"]
        DRM_CRTC["DRM CRTC"]
        DRM_PLANE["DRM Plane"]
    end
    
    subgraph "AMD Display Manager (amdgpu_dm)"
        AMDGPU_DM["amdgpu_dm.c"]
        DM_CONNECTOR["amdgpu_dm_connector"]
        DM_CRTC["amdgpu_dm_crtc"]
        DM_PLANE["amdgpu_dm_plane"]
        DM_MST["amdgpu_dm_mst_types"]
        DM_HELPERS["amdgpu_dm_helpers"]
    end
    
    subgraph "Display Core (DC)"
        DC_CORE["DC Core Library"]
        DC_LINK["dc_link"]
        DC_STREAM["dc_stream_state"]
        DC_SURFACE["dc_surface_update"]
    end
    
    subgraph "Hardware Abstraction"
        DCN["DCN (Display Controller Next)"]
        DMCU["DMCU (Display Microcontroller)"]
        DMUB["DMUB (Display Microcontroller Unit B)"]
    end
    
    USERSPACE --> DRM
    XORG --> DRM
    DRM --> DRM_ATOMIC
    DRM_ATOMIC --> AMDGPU_DM
    DRM_CONNECTOR --> DM_CONNECTOR
    DRM_CRTC --> DM_CRTC
    DRM_PLANE --> DM_PLANE
    
    AMDGPU_DM --> DC_CORE
    DM_CONNECTOR --> DC_LINK
    DM_MST --> DC_LINK
    DM_HELPERS --> DC_CORE
    
    DC_CORE --> DCN
    DC_CORE --> DMCU
    DC_CORE --> DMUB
```

Sources: [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c:150-158](), [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.h:26-31]()

## Core Components

### Display Manager (amdgpu_dm)

The Display Manager is the central component that manages the interface between DRM and DC. It maintains the main control structure and handles display operations.

```mermaid
graph LR
    subgraph "amdgpu_dm Core Functions"
        INIT["amdgpu_dm_init()"]
        FINI["amdgpu_dm_fini()"]
        ATOMIC_CHECK["amdgpu_dm_atomic_check()"]
        ATOMIC_COMMIT["amdgpu_dm_atomic_commit_tail()"]
    end
    
    subgraph "Key Data Structures"
        DM_STRUCT["amdgpu_display_manager"]
        DM_CONNECTOR_STRUCT["amdgpu_dm_connector"]
        DM_CRTC_STATE["dm_crtc_state"]
        DM_PLANE_STATE["dm_plane_state"]
    end
    
    subgraph "Hardware Interface"
        DC_CTX["dc_context"]
        DC_INSTANCE["dc (Display Core instance)"]
        DMUB_SRV["dmub_srv"]
    end
    
    INIT --> DM_STRUCT
    DM_STRUCT --> DC_CTX
    DC_CTX --> DC_INSTANCE
    ATOMIC_CHECK --> DM_CRTC_STATE
    ATOMIC_COMMIT --> DC_INSTANCE
    DM_CONNECTOR_STRUCT --> DMUB_SRV
```

The main initialization occurs in `amdgpu_dm_init()` which sets up the Display Core context, creates connectors for detected displays, and registers interrupt handlers.

Sources: [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c:161-162](), [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c:4671-4899]()

### Interrupt Management

The driver handles several types of display-related interrupts for proper timing and synchronization:

| Interrupt Type | Handler Function | Purpose |
|----------------|------------------|---------|
| Page Flip | `dm_pflip_high_irq()` | Handles page flip completion |
| VBlank | `dm_crtc_high_irq()` | Processes vertical blank interrupts |
| VUpdate | `dm_vupdate_high_irq()` | Manages variable refresh rate timing |
| HPD | `handle_hpd_irq_helper()` | Hot plug detection events |
| DMUB Outbox | `dm_dmub_outbox1_low_irq()` | Display microcontroller notifications |

Sources: [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c:411-506](), [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c:574-646]()

### Connector Management

Display connectors are managed through the `amdgpu_dm_connector` structure, which maintains the connection between DRM connectors and DC links:

```mermaid
graph TB
    subgraph "Connector Components"
        DRM_CONN["drm_connector"]
        AMDGPU_CONN["amdgpu_dm_connector"]
        DC_LINK["dc_link"]
        DC_SINK["dc_sink"]
    end
    
    subgraph "Connector Types"
        HDMI["HDMI Connector"]
        DP["DisplayPort Connector"]
        MST["MST Port"]
        EDP["eDP Connector"]
    end
    
    subgraph "Detection & EDID"
        HPD_WORK["hpd_work"]
        EDID_MGR["EDID Management"]
        LINK_DETECT["link_detect()"]
    end
    
    DRM_CONN --> AMDGPU_CONN
    AMDGPU_CONN --> DC_LINK
    DC_LINK --> DC_SINK
    
    AMDGPU_CONN --> HDMI
    AMDGPU_CONN --> DP
    AMDGPU_CONN --> MST
    AMDGPU_CONN --> EDP
    
    HPD_WORK --> LINK_DETECT
    LINK_DETECT --> EDID_MGR
    EDID_MGR --> DC_SINK
```

Sources: [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c:212-218](), [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c:8239-8455]()

### Audio Integration

The display driver includes comprehensive audio support for HDMI and DisplayPort outputs through the `amdgpu_dm_audio` subsystem:

```mermaid
graph LR
    subgraph "Audio Components"
        AUDIO_COMP["drm_audio_component"]
        AUDIO_OPS["amdgpu_dm_audio_component_ops"]
        ELD_FUNC["amdgpu_dm_audio_component_get_eld()"]
    end
    
    subgraph "Audio Management"
        AUDIO_INIT["amdgpu_dm_audio_init()"]
        AUDIO_FINI["amdgpu_dm_audio_fini()"]
        ELD_NOTIFY["amdgpu_dm_audio_eld_notify()"]
    end
    
    subgraph "Hardware Interface"
        AUDIO_PINS["mode_info.audio.pin[]"]
        AUDIO_LOCK["dm.audio_lock"]
        AUDIO_INST["aconnector->audio_inst"]
    end
    
    AUDIO_COMP --> AUDIO_OPS
    AUDIO_OPS --> ELD_FUNC
    AUDIO_INIT --> AUDIO_PINS
    ELD_NOTIFY --> AUDIO_LOCK
    AUDIO_INST --> ELD_FUNC
```

Sources: [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c:1013-1073](), [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c:940-976]()

## Display Pipeline

The display pipeline processes frame updates through several stages, coordinating between DRM atomic operations and Display Core hardware programming:

```mermaid
graph TB
    subgraph "DRM Atomic Operation"
        ATOMIC_CHECK["amdgpu_dm_atomic_check()"]
        ATOMIC_COMMIT["amdgpu_dm_atomic_commit_tail()"]
        PLANE_UPDATE["dm_plane_helper_check_state()"]
    end
    
    subgraph "DC Stream Management"
        STREAM_CREATE["dc_create_stream_for_sink()"]
        STREAM_UPDATE["dc_stream_update"]
        SURFACE_UPDATE["dc_surface_update"]
    end
    
    subgraph "Hardware Programming"
        UPDATE_PLANES["update_planes_and_stream_adapter()"]
        DC_UPDATE["dc_update_planes_and_stream()"]
        DC_COMMIT["dc_commit_streams()"]
    end
    
    subgraph "Interrupt Completion"
        PFLIP_IRQ["dm_pflip_high_irq()"]
        VBLANK_IRQ["dm_crtc_high_irq()"]
        EVENT_COMPLETE["drm_crtc_send_vblank_event()"]
    end
    
    ATOMIC_CHECK --> PLANE_UPDATE
    ATOMIC_COMMIT --> STREAM_CREATE
    STREAM_CREATE --> STREAM_UPDATE
    STREAM_UPDATE --> SURFACE_UPDATE
    SURFACE_UPDATE --> UPDATE_PLANES
    UPDATE_PLANES --> DC_UPDATE
    DC_UPDATE --> DC_COMMIT
    DC_COMMIT --> PFLIP_IRQ
    PFLIP_IRQ --> VBLANK_IRQ
    VBLANK_IRQ --> EVENT_COMPLETE
```

The pipeline ensures proper synchronization between frame updates and display hardware through interrupt-driven completion events.

Sources: [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c:365-402](), [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c:222-225](), [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.c:10888-11180]()

## Key Data Structures

### amdgpu_display_manager

The root control structure manages the overall display subsystem state:

| Field | Type | Purpose |
|-------|------|---------|
| `dc` | `struct dc *` | Display Core instance |
| `adev` | `struct amdgpu_device *` | Parent GPU device |
| `ddev` | `struct drm_device *` | DRM device reference |
| `display_indexes_num` | `int` | Number of display outputs |
| `irq_handler_list_table_lock` | `struct mutex` | IRQ handler synchronization |
| `backlight_caps` | `struct amdgpu_dm_backlight_caps` | Backlight control capabilities |
| `freesync_module` | `struct mod_freesync *` | Variable refresh rate support |

### amdgpu_dm_connector

Represents individual display outputs and their properties:

| Field | Type | Purpose |
|-------|------|---------|
| `base` | `struct drm_connector` | Base DRM connector |
| `connector_id` | `uint32_t` | Unique connector identifier |
| `dc_link` | `struct dc_link *` | Display Core link object |
| `dc_sink` | `struct dc_sink *` | Display Core sink object |
| `edid` | `struct edid *` | Extended Display Identification Data |
| `audio_inst` | `int` | Associated audio instance |
| `mst_mgr` | `struct drm_dp_mst_topology_mgr` | MST topology manager |

Sources: [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.h:343-500](), [drivers/gpu/drm/amd/display/amdgpu_dm/amdgpu_dm.h:501-650]()2a:T310d,# HD-Audio Support

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [sound/pci/hda/cs35l41_hda.c](sound/pci/hda/cs35l41_hda.c)
- [sound/pci/hda/cs35l41_hda.h](sound/pci/hda/cs35l41_hda.h)
- [sound/pci/hda/cs35l41_hda_i2c.c](sound/pci/hda/cs35l41_hda_i2c.c)
- [sound/pci/hda/cs35l41_hda_spi.c](sound/pci/hda/cs35l41_hda_spi.c)
- [sound/pci/hda/hda_auto_parser.c](sound/pci/hda/hda_auto_parser.c)
- [sound/pci/hda/hda_component.h](sound/pci/hda/hda_component.h)
- [sound/pci/hda/hda_cs_dsp_ctl.c](sound/pci/hda/hda_cs_dsp_ctl.c)
- [sound/pci/hda/hda_cs_dsp_ctl.h](sound/pci/hda/hda_cs_dsp_ctl.h)
- [sound/pci/hda/hda_local.h](sound/pci/hda/hda_local.h)
- [sound/pci/hda/patch_realtek.c](sound/pci/hda/patch_realtek.c)

</details>



## Purpose and Scope

This document covers the High Definition Audio (HD-Audio) codec driver infrastructure in the WSL2 Linux kernel, focusing on hardware-specific codec implementations and their supporting frameworks. HD-Audio codecs handle digital-to-analog and analog-to-digital conversion for audio playback and recording.

The system primarily consists of two major codec driver families: Realtek ALC codecs and Cirrus Logic CS35L41 codecs, along with shared infrastructure for automatic hardware configuration, DSP control, and component binding. For general I/O subsystem information, see [I/O Subsystems](#4).

## Architecture Overview

The HD-Audio support system follows a layered architecture where codec-specific drivers implement hardware interfaces while sharing common infrastructure for configuration parsing and control management.

**HD-Audio Codec Driver Architecture**
```mermaid
graph TB
    subgraph "ALSA Framework"
        ALSA["ALSA Core"]
        HDA["HDA Core"]
    end
    
    subgraph "Common Infrastructure"
        AUTO["hda_auto_parser"]
        LOCAL["hda_local"]
        COMP["hda_component"]
    end
    
    subgraph "Realtek ALC Codecs"
        ALC["patch_realtek"]
        ALC_SPEC["alc_spec"]
        ALC_OPS["alc_patch_ops"]
    end
    
    subgraph "Cirrus Logic CS35L41"
        CS35L41["cs35l41_hda"]
        CS35L41_DSP["hda_cs_dsp_ctl"]
        CS35L41_I2C["cs35l41_hda_i2c"]
        CS35L41_SPI["cs35l41_hda_spi"]
    end
    
    ALSA --> HDA
    HDA --> AUTO
    HDA --> LOCAL
    HDA --> COMP
    
    AUTO --> ALC
    LOCAL --> ALC
    COMP --> ALC
    
    AUTO --> CS35L41
    LOCAL --> CS35L41
    COMP --> CS35L41
    
    CS35L41 --> CS35L41_DSP
    CS35L41 --> CS35L41_I2C
    CS35L41 --> CS35L41_SPI
```

Sources: [sound/pci/hda/patch_realtek.c:1-50](), [sound/pci/hda/cs35l41_hda.c:1-30](), [sound/pci/hda/hda_auto_parser.c:1-20](), [sound/pci/hda/hda_component.h:1-22]()

## Realtek ALC Codec Support

The Realtek ALC codec driver provides comprehensive support for a wide range of Realtek audio codecs through the `patch_realtek.c` implementation. This driver handles codec initialization, configuration, and hardware-specific fixups.

**Realtek ALC Driver Components**
```mermaid
graph TB
    subgraph "Core Structures"
        ALC_SPEC["alc_spec<br/>Main codec state"]
        ALC_CUSTOMIZE["alc_customize_define<br/>Hardware configuration"]
        ALC_COEF["alc_coef_led<br/>LED control coefficients"]
    end
    
    subgraph "Operations"
        PATCH_OPS["alc_patch_ops<br/>Codec operations"]
        BUILD_CTRL["alc_build_controls"]
        INIT["alc_init"]
        SUSPEND["alc_suspend"]
        RESUME["alc_resume"]
    end
    
    subgraph "Hardware Management"
        COEF_MGT["COEF Management<br/>alc_read_coef_idx<br/>alc_write_coef_idx"]
        GPIO_CTRL["GPIO Control<br/>alc_setup_gpio<br/>alc_write_gpio"]
        FIXUPS["Hardware Fixups<br/>Device-specific patches"]
    end
    
    ALC_SPEC --> PATCH_OPS
    ALC_CUSTOMIZE --> ALC_SPEC
    ALC_COEF --> ALC_SPEC
    
    PATCH_OPS --> BUILD_CTRL
    PATCH_OPS --> INIT
    PATCH_OPS --> SUSPEND
    PATCH_OPS --> RESUME
    
    INIT --> COEF_MGT
    INIT --> GPIO_CTRL
    BUILD_CTRL --> FIXUPS
```

The `alc_spec` structure contains the complete codec state including GPIO configuration, LED control, and headset detection parameters. The coefficient (COEF) management system provides low-level hardware register access for codec-specific features.

Sources: [sound/pci/hda/patch_realtek.c:79-137](), [sound/pci/hda/patch_realtek.c:999-1010](), [sound/pci/hda/patch_realtek.c:143-260]()

### Hardware Fixups and Device Support

The driver implements an extensive fixup system to handle hardware variations and vendor-specific configurations. Each fixup addresses specific issues with particular hardware combinations.

Key fixup categories include:
- **GPIO Configuration**: `alc_fixup_gpio1()`, `alc_fixup_gpio2()` for hardware control signals
- **Pin Configuration**: Hardware-specific pin mappings and EAPD control
- **LED Control**: Microphone mute and power LEDs via `alc_fixup_micmute_led()`
- **Power Management**: PLL fixes and depop delay adjustments

Sources: [sound/pci/hda/patch_realtek.c:314-350](), [sound/pci/hda/patch_realtek.c:1272-1400]()

## Cirrus Logic CS35L41 Codec Support

The CS35L41 driver provides support for Cirrus Logic's smart amplifier codecs with integrated DSP capabilities. The driver supports both I2C and SPI interfaces and includes firmware management and power control.

**CS35L41 Driver Architecture**
```mermaid
graph TB
    subgraph "Transport Layer"
        I2C["cs35l41_hda_i2c<br/>I2C interface"]
        SPI["cs35l41_hda_spi<br/>SPI interface"]
    end
    
    subgraph "Core Driver"
        CS35L41_CORE["cs35l41_hda<br/>Main driver"]
        CS35L41_STRUCT["cs35l41_hda struct<br/>Device state"]
    end
    
    subgraph "DSP Framework"
        DSP_CTL["hda_cs_dsp_ctl<br/>DSP control interface"]
        CS_DSP["cs_dsp<br/>DSP management"]
        FIRMWARE["Firmware loading<br/>cs35l41_request_firmware_files"]
    end
    
    subgraph "Hardware Control"
        PLAYBACK["Playback Control<br/>cs35l41_hda_play_start<br/>cs35l41_hda_pause_start"]
        POWER["Power Management<br/>cs35l41_runtime_suspend<br/>cs35l41_system_suspend"]
        CALIBRATION["Calibration<br/>cs35l41_save_calibration"]
    end
    
    I2C --> CS35L41_CORE
    SPI --> CS35L41_CORE
    CS35L41_CORE --> CS35L41_STRUCT
    
    CS35L41_CORE --> DSP_CTL
    DSP_CTL --> CS_DSP
    CS_DSP --> FIRMWARE
    
    CS35L41_CORE --> PLAYBOOK
    CS35L41_CORE --> POWER
    CS35L41_CORE --> CALIBRATION
```

The `cs35l41_hda` structure maintains device state including regmap, GPIO descriptors, firmware status, and DSP configuration. The driver supports automatic firmware loading based on system configuration.

Sources: [sound/pci/hda/cs35l41_hda.c:48-74](), [sound/pci/hda/cs35l41_hda_i2c.c:15-35](), [sound/pci/hda/cs35l41_hda_spi.c:15-30]()

### DSP Control Framework

The CS35L41 codecs include integrated DSP capabilities managed through the `hda_cs_dsp_ctl` framework. This system provides ALSA control interfaces for DSP parameters and firmware management.

**DSP Control Architecture**
```mermaid
graph TB
    subgraph "ALSA Controls"
        KCTL["snd_kcontrol<br/>ALSA control interface"]
        INFO["hda_cs_dsp_coeff_info<br/>Control information"]
        GET["hda_cs_dsp_coeff_get<br/>Read DSP parameter"]
        PUT["hda_cs_dsp_coeff_put<br/>Write DSP parameter"]
    end
    
    subgraph "DSP Management"
        CS_DSP_CTL["cs_dsp_coeff_ctl<br/>DSP coefficient control"]
        CS_DSP_CORE["cs_dsp<br/>DSP core management"]
        WMFW["WMFW firmware<br/>DSP firmware format"]
    end
    
    subgraph "Firmware Types"
        SPK_PROT["HDA_CS_DSP_FW_SPK_PROT<br/>Speaker protection"]
        SPK_CALI["HDA_CS_DSP_FW_SPK_CALI<br/>Speaker calibration"]
        SPK_DIAG["HDA_CS_DSP_FW_SPK_DIAG<br/>Speaker diagnostics"]
        MISC["HDA_CS_DSP_FW_MISC<br/>Miscellaneous"]
    end
    
    KCTL --> INFO
    KCTL --> GET  
    KCTL --> PUT
    
    GET --> CS_DSP_CTL
    PUT --> CS_DSP_CTL
    CS_DSP_CTL --> CS_DSP_CORE
    
    CS_DSP_CORE --> SPK_PROT
    CS_DSP_CORE --> SPK_CALI
    CS_DSP_CORE --> SPK_DIAG
    CS_DSP_CORE --> MISC
```

The framework supports multiple firmware types for different operating modes including speaker protection, calibration, and diagnostics. Controls are automatically generated based on DSP firmware capabilities.

Sources: [sound/pci/hda/hda_cs_dsp_ctl.c:18-37](), [sound/pci/hda/hda_cs_dsp_ctl.c:111-156](), [sound/pci/hda/hda_cs_dsp_ctl.h:16-22]()

## Auto-Parser Functionality

The HD-Audio auto-parser automatically detects and configures audio hardware based on BIOS pin configuration data. This system eliminates the need for manual hardware configuration in most cases.

The `snd_hda_parse_pin_defcfg()` function serves as the main entry point for pin configuration parsing. It processes pin widgets and categorizes them into outputs (line-out, speaker, headphone) and inputs (microphone, line-in, auxiliary).

**Pin Configuration Parsing Flow**
```mermaid
graph TB
    START["snd_hda_parse_pin_defcfg()"]
    
    subgraph "Pin Detection"
        SCAN["Scan codec nodes<br/>for_each_hda_codec_node"]
        WIDGET["Check widget type<br/>AC_WID_PIN"]
        DEFCFG["Read pin config<br/>snd_hda_codec_get_pincfg"]
    end
    
    subgraph "Pin Classification"
        CONNECT["Check connection<br/>get_defcfg_connect"]
        DEVICE["Identify device type<br/>get_defcfg_device"]
        LOCATION["Get location<br/>get_defcfg_location"]
        SEQUENCE["Get sequence<br/>get_defcfg_sequence"]
    end
    
    subgraph "Configuration"
        LINE_OUT["Line outputs<br/>line_out_pins[]"]
        SPEAKERS["Speakers<br/>speaker_pins[]"]
        HEADPHONES["Headphones<br/>hp_pins[]"]
        INPUTS["Inputs<br/>inputs[]"]
    end
    
    START --> SCAN
    SCAN --> WIDGET
    WIDGET --> DEFCFG
    DEFCFG --> CONNECT
    CONNECT --> DEVICE
    DEVICE --> LOCATION
    LOCATION --> SEQUENCE
    
    SEQUENCE --> LINE_OUT
    SEQUENCE --> SPEAKERS
    SEQUENCE --> HEADPHONES
    SEQUENCE --> INPUTS
```

The parser handles complex scenarios including multiple output types, headset microphone detection, and fallback configurations when primary outputs are unavailable.

Sources: [sound/pci/hda/hda_auto_parser.c:172-185](), [sound/pci/hda/hda_auto_parser.c:194-313](), [sound/pci/hda/hda_auto_parser.c:373-410]()

### Input Pin Management

Input pin management includes automatic detection of microphone types (internal, external, headset) and proper labeling for user interfaces. The `hda_get_autocfg_input_label()` function generates appropriate labels considering location and type.

Key input types handled:
- **AUTO_PIN_MIC**: Various microphone inputs with location-based labeling
- **AUTO_PIN_LINE_IN**: Line input connections
- **AUTO_PIN_CD**: CD audio inputs  
- **AUTO_PIN_AUX**: Auxiliary inputs

Sources: [sound/pci/hda/hda_auto_parser.c:575-591](), [sound/pci/hda/hda_auto_parser.c:486-532]()

## Component Binding System

The HD-Audio component binding system provides a framework for codec drivers to interact with external amplifiers and other audio components through standardized interfaces.

The `hda_component` structure defines callback hooks for playback events, allowing external components to synchronize with codec operations. This system is particularly important for smart amplifiers that require coordination with the main audio codec.

**Component Binding Interface**
```mermaid
graph TB
    subgraph "HDA Component Framework" 
        HDA_COMP["hda_component<br/>Component interface"]
        HOOKS["Playback hooks<br/>pre_playbook_hook<br/>playbook_hook<br/>post_playbook_hook"]
    end
    
    subgraph "CS35L41 Integration"
        CS35L41_HOOKS["CS35L41 Hooks<br/>cs35l41_hda_pre_playbook_hook<br/>cs35l41_hda_playbook_hook<br/>cs35l41_hda_post_playbook_hook"]
        POWER_CTL["Power Control<br/>cs35l41_hda_play_start<br/>cs35l41_hda_pause_done"]
    end
    
    subgraph "Codec Integration"
        CODEC["HDA Codec Driver"]
        PCM_OPS["PCM Operations<br/>HDA_GEN_PCM_ACT_OPEN<br/>HDA_GEN_PCM_ACT_PREPARE<br/>HDA_GEN_PCM_ACT_CLEANUP"]
    end
    
    HDA_COMP --> HOOKS
    HOOKS --> CS35L41_HOOKS
    CS35L41_HOOKS --> POWER_CTL
    
    CODEC --> PCM_OPS
    PCM_OPS --> HOOKS
```

Component binding enables proper sequencing of operations across multiple audio devices, ensuring synchronized startup and shutdown procedures.

Sources: [sound/pci/hda/hda_component.h:14-21](), [sound/pci/hda/cs35l41_hda.c:616-674](), [sound/pci/hda/cs35l41_hda.c:676-689]()2b:T2d9f,# Windows Integration Components

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [drivers/hv/dxgkrnl/Kconfig](drivers/hv/dxgkrnl/Kconfig)
- [drivers/hv/dxgkrnl/Makefile](drivers/hv/dxgkrnl/Makefile)
- [drivers/hv/dxgkrnl/dxgadapter.c](drivers/hv/dxgkrnl/dxgadapter.c)
- [drivers/hv/dxgkrnl/dxgkrnl.h](drivers/hv/dxgkrnl/dxgkrnl.h)
- [drivers/hv/dxgkrnl/dxgmodule.c](drivers/hv/dxgkrnl/dxgmodule.c)
- [drivers/hv/dxgkrnl/dxgprocess.c](drivers/hv/dxgkrnl/dxgprocess.c)
- [drivers/hv/dxgkrnl/dxgsyncfile.c](drivers/hv/dxgkrnl/dxgsyncfile.c)
- [drivers/hv/dxgkrnl/dxgsyncfile.h](drivers/hv/dxgkrnl/dxgsyncfile.h)
- [drivers/hv/dxgkrnl/dxgvmbus.c](drivers/hv/dxgkrnl/dxgvmbus.c)
- [drivers/hv/dxgkrnl/dxgvmbus.h](drivers/hv/dxgkrnl/dxgvmbus.h)
- [drivers/hv/dxgkrnl/hmgr.c](drivers/hv/dxgkrnl/hmgr.c)
- [drivers/hv/dxgkrnl/hmgr.h](drivers/hv/dxgkrnl/hmgr.h)
- [drivers/hv/dxgkrnl/ioctl.c](drivers/hv/dxgkrnl/ioctl.c)
- [drivers/hv/dxgkrnl/misc.c](drivers/hv/dxgkrnl/misc.c)
- [drivers/hv/dxgkrnl/misc.h](drivers/hv/dxgkrnl/misc.h)
- [include/uapi/misc/d3dkmthk.h](include/uapi/misc/d3dkmthk.h)

</details>



This document covers the Windows Integration Components subsystem, which provides DirectX graphics and compute capabilities in Linux environments running on Microsoft Hyper-V, particularly in WSL2. The system enables Linux processes to access Windows GPU drivers and hardware through paravirtualized interfaces.

For information about the core DXGKRNL graphics driver implementation, see [DXGKRNL Graphics Driver](#8.1).

## Purpose and Scope

The Windows Integration Components implement a paravirtualized GPU driver (`dxgkrnl`) that allows Linux applications to utilize DirectX graphics and compute APIs by communicating with Windows host GPU drivers through Hyper-V VM bus channels. The system provides D3DKMT API compatibility and handles the virtualization of GPU resources, memory management, and synchronization between the Linux guest and Windows host.

Sources: [drivers/hv/dxgkrnl/Kconfig:1-29](), [drivers/hv/dxgkrnl/dxgmodule.c:1-12]()

## Architecture Overview

```mermaid
graph TB
    subgraph "Linux Guest (WSL2)"
        subgraph "User Space"
            APP["Linux Applications"]
            LIBDXCORE["libdxcore Library"]
        end
        
        subgraph "Kernel Space"
            DXGDEV["/dev/dxg Device"]
            IOCTL["IOCTL Handlers<br/>ioctl.c"]
            DXGGLOBAL["dxgglobal<br/>Global State"]
            DXGPROCESS["dxgprocess<br/>Process Management"]
            DXGADAPTER["dxgadapter<br/>Adapter Management"]
            DXGDEVICE["dxgdevice<br/>Device Management"]
            HMGR["hmgr<br/>Handle Manager"]
            DXGVMBUS["dxgvmbus<br/>VM Bus Communication"]
        end
    end
    
    subgraph "Hyper-V Infrastructure"
        VMBUS["VM Bus Channels"]
    end
    
    subgraph "Windows Host"
        HOSTDRV["Host GPU Drivers"]
        HOSTGPU["Physical GPU Hardware"]
    end
    
    APP --> LIBDXCORE
    LIBDXCORE --> DXGDEV
    DXGDEV --> IOCTL
    IOCTL --> DXGPROCESS
    IOCTL --> DXGADAPTER
    IOCTL --> DXGDEVICE
    DXGPROCESS --> HMGR
    DXGADAPTER --> DXGVMBUS
    DXGDEVICE --> DXGVMBUS
    DXGVMBUS --> VMBUS
    VMBUS --> HOSTDRV
    HOSTDRV --> HOSTGPU
    
    DXGGLOBAL -.-> DXGPROCESS
    DXGGLOBAL -.-> DXGADAPTER
```

**Architecture Components Overview**

The system consists of a Linux kernel driver that exposes a `/dev/dxg` character device to userspace applications. Applications communicate through the libdxcore library which translates D3DKMT API calls to driver IOCTLs. The kernel driver manages processes, adapters, and devices while communicating with the Windows host through Hyper-V VM bus channels.

Sources: [drivers/hv/dxgkrnl/dxgkrnl.h:280-325](), [drivers/hv/dxgkrnl/dxgmodule.c:424-430](), [drivers/hv/dxgkrnl/ioctl.c:28-31]()

## VM Bus Communication Layer

```mermaid
graph LR
    subgraph "Linux Guest Commands"
        CREATEPROC["DXGK_VMBCOMMAND_CREATEPROCESS"]
        OPENADAPTER["DXGK_VMBCOMMAND_OPENADAPTER"] 
        CREATEDEV["DXGK_VMBCOMMAND_CREATEDEVICE"]
        CREATEALLOC["DXGK_VMBCOMMAND_CREATEALLOCATION"]
        CREATESYNC["DXGK_VMBCOMMAND_CREATESYNCOBJECT"]
    end
    
    subgraph "VM Bus Infrastructure"
        GLOBALCH["Global VM Bus Channel<br/>dxgglobal->channel"]
        ADAPTERCH["Per-Adapter Channels<br/>adapter->channel"]
        MSGPROC["Message Processing<br/>dxgvmbuschannel_receive()"]
    end
    
    subgraph "Host Responses"
        HOSTCMD["Host Command Processing"]
        SETGUESTDATA["DXGK_VMBCOMMAND_SETGUESTDATA"]
        SIGNALEVENT["DXGK_VMBCOMMAND_SIGNALGUESTEVENT"]
    end
    
    CREATEPROC --> GLOBALCH
    OPENADAPTER --> ADAPTERCH  
    CREATEDEV --> ADAPTERCH
    CREATEALLOC --> ADAPTERCH
    CREATESYNC --> GLOBALCH
    
    GLOBALCH --> HOSTCMD
    ADAPTERCH --> HOSTCMD
    
    HOSTCMD --> SETGUESTDATA
    HOSTCMD --> SIGNALEVENT
    
    SETGUESTDATA --> MSGPROC
    SIGNALEVENT --> MSGPROC
```

**VM Bus Message Flow**

The driver uses two types of VM bus channels: a global channel for process-level operations and per-adapter channels for device-specific operations. Commands are sent synchronously with completion tracking, while host-initiated messages are processed asynchronously.

Sources: [drivers/hv/dxgkrnl/dxgvmbus.h:36-53](), [drivers/hv/dxgkrnl/dxgvmbus.h:62-132](), [drivers/hv/dxgkrnl/dxgvmbus.c:390-411]()

## Core Components

### Process Management

The `dxgprocess` component manages per-process state and handle tables for graphics resources.

| Component | Purpose | Key Functions |
|-----------|---------|---------------|
| `dxgprocess` | Per-process graphics state | `dxgprocess_create()`, `dxgprocess_destroy()` |
| `handle_table` | Main resource handles | `hmgrtable_*()` functions |
| `local_handle_table` | Adapter handles | Separate table for adapters |

Each Linux process that opens `/dev/dxg` gets a corresponding `dxgprocess` structure created, which maintains handle tables for graphics objects and communicates with the host through a process handle.

Sources: [drivers/hv/dxgkrnl/dxgkrnl.h:381-414](), [drivers/hv/dxgkrnl/dxgprocess.c:25-59]()

### Adapter Management

The `dxgadapter` represents virtual GPU adapters exposed by the Windows host.

```mermaid
graph TB
    subgraph "Adapter Lifecycle"
        PCIDISC["PCI Device Discovery<br/>dxg_pci_probe()"]
        ADAPTERCREATE["Adapter Creation<br/>dxgglobal_create_adapter()"]
        VMBUSCONN["VM Bus Connection<br/>dxgadapter_set_vmbus()"]
        ADAPTERSTART["Adapter Start<br/>dxgadapter_start()"]
    end
    
    subgraph "Adapter State"
        WAITING["DXGADAPTER_STATE_WAITING_VMBUS"]
        ACTIVE["DXGADAPTER_STATE_ACTIVE"] 
        STOPPED["DXGADAPTER_STATE_STOPPED"]
    end
    
    subgraph "Adapter Objects"
        DEVICES["dxgdevice List<br/>Device Management"]
        SYNCOBJS["Sync Objects<br/>syncobj_list_head"]
        RESOURCES["Shared Resources<br/>shared_resource_list_head"]
    end
    
    PCIDISC --> ADAPTERCREATE
    ADAPTERCREATE --> WAITING
    VMBUSCONN --> WAITING
    WAITING --> ADAPTERSTART
    ADAPTERSTART --> ACTIVE
    ACTIVE --> DEVICES
    ACTIVE --> SYNCOBJS
    ACTIVE --> RESOURCES
```

**Adapter State Management**

Adapters go through a lifecycle from PCI discovery to active state. They maintain lists of associated devices, synchronization objects, and shared resources.

Sources: [drivers/hv/dxgkrnl/dxgkrnl.h:456-485](), [drivers/hv/dxgkrnl/dxgadapter.c:55-99](), [drivers/hv/dxgkrnl/dxgmodule.c:263-303]()

### Device and Context Management

Graphics devices and execution contexts are managed hierarchically under adapters.

| Object Type | Purpose | Creation Function |
|-------------|---------|-------------------|
| `dxgdevice` | GPU device instance | `dxgkio_create_device()` |
| `dxgcontext` | Execution context | `dxgkio_create_context_virtual()` |
| `dxghwqueue` | Hardware queue | `dxgkio_create_hwqueue()` |
| `dxgallocation` | GPU memory allocation | `dxgkio_create_allocation()` |

Sources: [drivers/hv/dxgkrnl/dxgkrnl.h:516-540](), [drivers/hv/dxgkrnl/ioctl.c:674-758]()

### Synchronization Objects

The system provides various synchronization primitives for GPU operations.

```mermaid
graph LR
    subgraph "Sync Object Types"
        FENCE["D3DDDI_FENCE<br/>Basic Fence"]
        MONITORED["D3DDDI_MONITORED_FENCE<br/>CPU Accessible"]
        MUTEX["D3DDDI_SYNCHRONIZATION_MUTEX<br/>Mutual Exclusion"]
        SEMAPHORE["D3DDDI_SEMAPHORE<br/>Counting Semaphore"]
        CPUNOTIF["D3DDDI_CPU_NOTIFICATION<br/>CPU Event"]
    end
    
    subgraph "Sync Management"
        DXGSYNC["dxgsyncobject<br/>Local Sync Object"]
        SHAREDSYNC["dxgsharedsyncobject<br/>Cross-Process Sharing"]
        HOSTEVENTS["Host Event System<br/>dxghostevent"]
    end
    
    FENCE --> DXGSYNC
    MONITORED --> DXGSYNC
    MUTEX --> DXGSYNC
    SEMAPHORE --> DXGSYNC
    CPUNOTIF --> HOSTEVENTS
    
    DXGSYNC --> SHAREDSYNC
    SHAREDSYNC --> HOSTEVENTS
```

**Synchronization Architecture**

The driver supports multiple synchronization object types from the D3DDDI specification. Monitored fences provide CPU-accessible memory for fence values, while shared sync objects enable cross-process synchronization.

Sources: [drivers/hv/dxgkrnl/dxgkrnl.h:170-212](), [include/uapi/misc/d3dkmthk.h:476-485](), [drivers/hv/dxgkrnl/dxgkrnl.h:228-247]()

## User-Mode Interface

The driver exposes its functionality through IOCTL commands that mirror the Windows D3DKMT API.

| IOCTL Category | Key Functions | Purpose |
|----------------|---------------|---------|
| Adapter Management | `dxgkio_enum_adapters()`, `dxgkio_open_adapter_from_luid()` | Adapter discovery and opening |
| Device Operations | `dxgkio_create_device()`, `dxgkio_destroy_device()` | Device lifecycle |
| Memory Management | `dxgkio_create_allocation()`, `dxgkio_make_resident()` | GPU memory allocation |
| Synchronization | `dxgkio_create_sync_object()`, `dxgkio_wait_for_sync_object()` | GPU synchronization |
| Context Management | `dxgkio_create_context_virtual()`, `dxgkio_submit_command()` | Command submission |

The IOCTL interface is defined in `d3dkmthk.h` and implemented in `ioctl.c` with functions following the naming pattern `dxgkio_*()`.

Sources: [include/uapi/misc/d3dkmthk.h:1-50](), [drivers/hv/dxgkrnl/ioctl.c:92-149](), [drivers/hv/dxgkrnl/ioctl.c:1375-1450]()

## Message Processing Flow

```mermaid
sequenceDiagram
    participant App as "Linux Application"
    participant LibDX as "libdxcore"
    participant DXGDev as "/dev/dxg"
    participant IOCTL as "IOCTL Handler" 
    participant VMBus as "VM Bus"
    participant Host as "Windows Host"
    
    App->>LibDX: D3DKMT API Call
    LibDX->>DXGDev: ioctl()
    DXGDev->>IOCTL: dxgkio_*()
    IOCTL->>VMBus: dxgvmb_send_*()
    VMBus->>Host: DXGK_VMBCOMMAND_*
    Host-->>VMBus: Response
    VMBus-->>IOCTL: Result
    IOCTL-->>DXGDev: Return Code
    DXGDev-->>LibDX: ioctl result
    LibDX-->>App: D3DKMT result
    
    Note over Host,VMBus: Async host events
    Host->>VMBus: SIGNALGUESTEVENT
    VMBus->>IOCTL: dxgvmbuschannel_receive()
    IOCTL->>App: Event notification
```

**Message Processing Sequence**

The typical flow involves userspace API calls being translated to IOCTLs, which generate VM bus messages to the Windows host. The host processes requests and sends responses back through the same channel, with additional support for asynchronous host-initiated events.

Sources: [drivers/hv/dxgkrnl/dxgvmbus.c:413-498](), [drivers/hv/dxgkrnl/dxgvmbus.c:320-351](), [drivers/hv/dxgkrnl/dxgmodule.c:351-381]()2c:T3d45,# DXGKRNL Graphics Driver

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [drivers/hv/dxgkrnl/Kconfig](drivers/hv/dxgkrnl/Kconfig)
- [drivers/hv/dxgkrnl/Makefile](drivers/hv/dxgkrnl/Makefile)
- [drivers/hv/dxgkrnl/dxgadapter.c](drivers/hv/dxgkrnl/dxgadapter.c)
- [drivers/hv/dxgkrnl/dxgkrnl.h](drivers/hv/dxgkrnl/dxgkrnl.h)
- [drivers/hv/dxgkrnl/dxgmodule.c](drivers/hv/dxgkrnl/dxgmodule.c)
- [drivers/hv/dxgkrnl/dxgprocess.c](drivers/hv/dxgkrnl/dxgprocess.c)
- [drivers/hv/dxgkrnl/dxgsyncfile.c](drivers/hv/dxgkrnl/dxgsyncfile.c)
- [drivers/hv/dxgkrnl/dxgsyncfile.h](drivers/hv/dxgkrnl/dxgsyncfile.h)
- [drivers/hv/dxgkrnl/dxgvmbus.c](drivers/hv/dxgkrnl/dxgvmbus.c)
- [drivers/hv/dxgkrnl/dxgvmbus.h](drivers/hv/dxgkrnl/dxgvmbus.h)
- [drivers/hv/dxgkrnl/hmgr.c](drivers/hv/dxgkrnl/hmgr.c)
- [drivers/hv/dxgkrnl/hmgr.h](drivers/hv/dxgkrnl/hmgr.h)
- [drivers/hv/dxgkrnl/ioctl.c](drivers/hv/dxgkrnl/ioctl.c)
- [drivers/hv/dxgkrnl/misc.c](drivers/hv/dxgkrnl/misc.c)
- [drivers/hv/dxgkrnl/misc.h](drivers/hv/dxgkrnl/misc.h)
- [include/uapi/misc/d3dkmthk.h](include/uapi/misc/d3dkmthk.h)

</details>



The DXGKRNL Graphics Driver enables GPU virtualization in Hyper-V environments by providing paravirtualized GPU access from Linux guests to Windows host GPU resources. This driver acts as a bridge between Linux applications and the Windows DirectX Graphics Kernel (DXGK) subsystem, enabling GPU compute and graphics workloads to execute on the host while maintaining process isolation and resource management in the virtualized environment.

## Purpose and Architecture

The DXGKRNL driver implements GPU virtualization by proxying graphics and compute operations from Linux guests to the Windows host's DirectX Graphics Kernel. The `dxgglobal` structure maintains the global driver state, while individual `dxgadapter` objects represent virtualized GPU adapters. Each Linux process (`dxgprocess`) can open multiple GPU devices (`dxgdevice`) to execute workloads through execution contexts (`dxgcontext`).

```mermaid
flowchart TD
    subgraph "Windows Host"
        WinGPUDrivers["Host GPU Drivers"]
        DXG["DirectX Graphics Kernel"]
    end
    
    subgraph "Linux Guest (WSL2)"
        LinuxApp["Linux Application"]
        UserLib["libdxcore.so"]
        DXGDev["/dev/dxg Device"]
        DXGKrnl["DXGKRNL Driver"]
        VMBus["Hyper-V VM Bus"]
    end
    
    LinuxApp --> UserLib
    UserLib --> DXGDev
    DXGDev --> DXGKrnl
    DXGKrnl --> VMBus
    VMBus --> DXG
    DXG --> WinGPUDrivers
```

**Sources:** 
- [drivers/hv/dxgkrnl/Kconfig]()
- [drivers/hv/dxgkrnl/dxgkrnl.h]()
- [drivers/hv/dxgkrnl/dxgmodule.c]()

## VM Bus Communication

Communication between Linux and Windows occurs through the Hyper-V VM Bus. The DXGKRNL driver establishes two types of channels:

1. **Global Channel**: Shared across all adapters for system-wide operations
2. **Adapter Channels**: Dedicated channels for each GPU adapter

Commands flow bidirectionally through these channels, with Linux sending requests and Windows responding or initiating events.

```mermaid
sequenceDiagram
    participant LinuxProcess as "dxgprocess"
    participant DXGKrnl as "DXGKRNL Driver"
    participant VMBus as "dxgvmbuschannel"
    participant Windows as "Windows Host"
    
    LinuxProcess->>DXGKrnl: IOCTL Request
    DXGKrnl->>DXGKrnl: Translate to VM Bus Command
    DXGKrnl->>VMBus: dxgvmb_send_sync_msg()
    VMBus->>Windows: Command via VM Bus
    Note over Windows: Process Command
    Windows->>VMBus: Response via VM Bus
    VMBus->>DXGKrnl: Process Response
    DXGKrnl->>LinuxProcess: Return Result
    
    Windows->>VMBus: Asynchronous Event
    VMBus->>DXGKrnl: dxgvmbuschannel_receive()
    DXGKrnl->>LinuxProcess: Signal Event
```

### Command Structure

The VM Bus protocol uses structured messages with type-specific payloads:

| Command Channel Type | Purpose |
|----------------------|---------|
| `DXGKVMB_VM_TO_HOST` | Global commands from VM to host |
| `DXGKVMB_VGPU_TO_HOST` | Adapter-specific commands from VM to host |
| `DXGKVMB_HOST_TO_VM` | Commands from host to VM |

**Sources:**
- [drivers/hv/dxgkrnl/dxgvmbus.c]()
- [drivers/hv/dxgkrnl/dxgvmbus.h]()

## Core Objects and Relationships

DXGKRNL implements a hierarchy of objects to represent graphics resources. The following diagram illustrates the relationship between these core objects:

```mermaid
classDiagram
    class dxgglobal {
        +dxgvmbuschannel channel
        +list_head adapter_list_head
        +list_head plisthead
    }
    
    class dxgadapter {
        +dxgvmbuschannel channel
        +list_head adapter_process_list_head
        +list_head shared_resource_list_head
        +d3dkmthandle host_handle
    }
    
    class dxgprocess {
        +hmgrtable handle_table
        +hmgrtable local_handle_table
        +list_head process_adapter_list_head
        +d3dkmthandle host_handle
    }
    
    class dxgprocess_adapter {
        +list_head device_list_head
        +dxgadapter* adapter
        +dxgprocess* process
    }
    
    class dxgdevice {
        +dxgadapter* adapter
        +dxgprocess* process
        +list_head context_list_head
        +list_head alloc_list_head
        +list_head resource_list_head
        +d3dkmthandle handle
    }
    
    class dxgcontext {
        +dxgdevice* device
        +dxgprocess* process
        +list_head hwqueue_list_head
        +d3dkmthandle handle
    }
    
    dxgglobal "1" -- "many" dxgadapter: manages
    dxgglobal "1" -- "many" dxgprocess: tracks
    dxgprocess "1" -- "many" dxgprocess_adapter: contains
    dxgadapter "1" -- "many" dxgprocess_adapter: referenced_by
    dxgprocess_adapter "1" -- "many" dxgdevice: contains
    dxgdevice "1" -- "many" dxgcontext: contains
```

### Global State (dxgglobal)

The `dxgglobal` structure maintains the global driver state defined in [drivers/hv/dxgkrnl/dxgkrnl.h:281-326](). Key components include:
- `adapter_list_head`: List of all GPU adapters
- `plisthead`: List of active processes  
- `channel`: Global VM bus channel for system-wide operations
- `mmiospace_base`/`mmiospace_size`: Memory-mapped I/O region
- `device_state_counter`: GPU device state tracking

Key functions include `dxgglobal_create_adapter()` in [drivers/hv/dxgkrnl/dxgmodule.c:263-303]() for adapter enumeration and `dxgglobal_init_global_channel()` for VM bus initialization.

### Adapters (dxgadapter)

The `dxgadapter` structure defined in [drivers/hv/dxgkrnl/dxgkrnl.h:456-485]() represents a virtualized GPU adapter:
- `channel`: Dedicated VM bus channel for adapter-specific commands
- `luid`/`host_adapter_luid`: Unique identifiers for guest and host adapter mapping
- `adapter_process_list_head`: List of processes using this adapter
- `adapter_state`: Current state (active/stopped/waiting)

Key functions include `dxgadapter_start()` in [drivers/hv/dxgkrnl/dxgadapter.c:55-99]() for adapter activation and `dxgvmb_send_open_adapter()` in [drivers/hv/dxgkrnl/dxgvmbus.c:1015-1046]() for establishing host communication.

### Processes (dxgprocess)

The `dxgprocess` structure tracks a user process using the driver:
- Maintains handle tables for objects
- Maps to a Windows host process
- Tracks process-specific adapter relationships

Key operations:
- Creating/destroying process
- Opening/closing adapters
- Managing process-specific resources

### Process-Adapter Relationship (dxgprocess_adapter)

The `dxgprocess_adapter` structure links processes with adapters:
- Contains device list for the process-adapter pair
- Maintains reference counts
- Links to both process and adapter

### Devices (dxgdevice)

The `dxgdevice` structure represents a GPU device opened by a process:
- Links to adapter and process
- Contains lists of contexts, allocations, and resources
- Tracks device state and execution status

Key operations:
- Creating/destroying device
- Managing device resources
- Handling device execution state

### Contexts (dxgcontext)

The `dxgcontext` structure represents a GPU execution context:
- Links to device and process
- Contains hardware queue list
- Manages execution state

**Sources:**
- [drivers/hv/dxgkrnl/dxgkrnl.h]()
- [drivers/hv/dxgkrnl/dxgadapter.c]()
- [drivers/hv/dxgkrnl/dxgprocess.c]()

## Resource Management

DXGKRNL manages various GPU resources including allocations, resources, and paging queues.

### Allocations and Resources

```mermaid
flowchart TD
    Resource["dxgresource"] --> Allocation1["dxgallocation"]
    Resource --> Allocation2["dxgallocation"]
    Resource --> AllocationN["dxgallocation..."]
    
    SharedResource["dxgsharedresource"] --> Resource1["dxgresource"]
    SharedResource --> Resource2["dxgresource"]
    SharedResource --> ResourceN["dxgresource..."]
    
    Allocation1 --> Memory["GPU Memory"]
    Allocation2 --> Memory
    AllocationN --> Memory
```

- **Allocations** (`dxgallocation`): Represent memory allocations used by the GPU
- **Resources** (`dxgresource`): Group related allocations into a single resource
- **Shared Resources** (`dxgsharedresource`): Enable resources to be shared between processes

### Paging Queues

Paging queues (`dxgpagingqueue`) manage memory residency operations:
- Making allocations resident in GPU memory
- Evicting allocations from GPU memory
- Handling memory pressure situations

**Sources:**
- [drivers/hv/dxgkrnl/ioctl.c]()
- [drivers/hv/dxgkrnl/dxgkrnl.h]()

## Synchronization Mechanisms

DXGKRNL implements various synchronization mechanisms to coordinate GPU-CPU operations.

### Synchronization Objects

```mermaid
flowchart TD
    SyncObj["dxgsyncobject"] -- "References" --> SharedSyncObj["dxgsharedsyncobject"]
    SharedSyncObj -- "Referenced by" --> SyncObj1["dxgsyncobject"]
    SharedSyncObj -- "Referenced by" --> SyncObj2["dxgsyncobject"]
    
    SyncPoint["dxgsyncpoint"] -- "Uses" --> DMAFence["Linux DMA Fence"]
    SyncPoint -- "References" --> SharedSyncObj
    
    SyncObj -- "Creates" --> HostEvent["dxghostevent"]
```

Types of synchronization objects:
- **Fence objects**: Used for GPU-CPU synchronization
- **Monitored fence objects**: Memory-backed fences for efficient polling
- **CPU notification objects**: Event-based synchronization

### Sync File Integration

The driver integrates with the Linux sync file framework for interoperability with other Linux subsystems:
- `dxgsyncpoint` encapsulates a DMA fence with GPU synchronization
- Sync files can be created from driver synchronization objects
- Sync files can be waited on using standard Linux mechanisms

**Sources:**
- [drivers/hv/dxgkrnl/dxgkrnl.h]()
- [drivers/hv/dxgkrnl/dxgsyncfile.c]()
- [drivers/hv/dxgkrnl/dxgsyncfile.h]()

## Handle Management

DXGKRNL implements a handle manager to track references to driver objects. Handles provide a safe way for user-mode applications to reference kernel objects.

```mermaid
flowchart TD
    HandleTable["hmgrtable"] -- "Contains" --> Entries["hmgrentry[]"]
    Entries -- "References" --> Objects["Driver Objects"]
    
    HandleAlloc["hmgrtable_alloc_handle()"] -- "Creates" --> Handle["d3dkmthandle"]
    Handle -- "Refers to" --> Entries
    
    GetObject["hmgrtable_get_object()"] -- "Resolves" --> Handle
    GetObject -- "Returns" --> Objects
```

The handle table structure:
- Maintains a table of entries, each containing an object pointer
- Tracks free and used entries in the table
- Implements unique identifiers to prevent handle reuse issues
- Supports handle validation and type checking

Each process has two handle tables:
- `handle_table`: Used for most objects
- `local_handle_table`: Used specifically for adapter objects

**Sources:**
- [drivers/hv/dxgkrnl/hmgr.c]()
- [drivers/hv/dxgkrnl/hmgr.h]()

## Command Processing Flow

DXGKRNL processes user requests through IOCTL commands defined in [drivers/hv/dxgkrnl/ioctl.c](), translating them to VM bus messages for the Windows host.

### IOCTL Command Processing Flow

```mermaid
flowchart TD
    UserApp["User Application"] -- "IOCTL Call" --> DXGDev["/dev/dxg Device"]
    DXGDev -- "dxgk_unlocked_ioctl()" --> IOCTLRouter["IOCTL Command Router"]
    
    IOCTLRouter -- "dxgkio_open_adapter_from_luid()" --> AdapterOps["Adapter Operations"]
    IOCTLRouter -- "dxgkio_create_device()" --> DeviceOps["Device Operations"] 
    IOCTLRouter -- "dxgkio_create_context_virtual()" --> ContextOps["Context Operations"]
    
    AdapterOps -- "hmgrtable_get_object()" --> HandleTable["hmgrtable"]
    DeviceOps -- "dxgvmb_send_create_device()" --> VMBusMsg["VM Bus Message"]
    ContextOps -- "dxgvmb_send_create_context()" --> VMBusMsg
    
    VMBusMsg -- "dxgvmb_send_sync_msg()" --> VMBusChannel["dxgvmbuschannel"]
    VMBusChannel -- "vmbus_sendpacket()" --> WindowsHost["Windows DirectX Graphics Kernel"]
    
    WindowsHost -- "Completion Packet" --> VMBusChannel
    VMBusChannel -- "dxgvmbuschannel_receive()" --> IOCTLRouter
    IOCTLRouter -- "copy_to_user()" --> UserApp
```

### Key IOCTL Commands

The IOCTL handler implements commands including:
- `dxgkio_open_adapter_from_luid()` [drivers/hv/dxgkrnl/ioctl.c:92-149](): Opens GPU adapter by LUID
- `dxgkio_create_device()` [drivers/hv/dxgkrnl/ioctl.c:675-758](): Creates GPU device contexts
- `dxgkio_create_allocation()` [drivers/hv/dxgkrnl/ioctl.c:1375-1683](): Manages GPU memory allocations
- `dxgkio_submit_command()` [drivers/hv/dxgkrnl/ioctl.c:4287-4376](): Submits GPU command buffers

Commands use VM bus message structures from [drivers/hv/dxgkrnl/dxgvmbus.h]() like `dxgkvmb_command_createdevice` and `dxgkvmb_command_createallocation` to communicate with the Windows host.

**Sources:**
- [drivers/hv/dxgkrnl/ioctl.c:92-149]()
- [drivers/hv/dxgkrnl/ioctl.c:675-758]()
- [drivers/hv/dxgkrnl/ioctl.c:1375-1683]()
- [drivers/hv/dxgkrnl/dxgvmbus.c:413-498]()
- [drivers/hv/dxgkrnl/dxgvmbus.h:366-376]()

## IO Memory Mapping

DXGKRNL enables mapping of GPU memory into user-mode process address space. This mapping allows efficient access to GPU memory and synchronization structures.

Key functions:
- `dxg_map_iospace`: Maps IO space to user virtual address space
- `dxg_unmap_iospace`: Unmaps previously mapped IO space

The mapping process involves:
1. Reserving virtual address space in the process
2. Setting up appropriate page protections
3. Mapping physical GPU memory to the virtual address space

This mechanism is primarily used for monitored fence objects, where the CPU can efficiently poll a memory location for status changes.

**Sources:**
- [drivers/hv/dxgkrnl/dxgvmbus.c]()

## Device and Driver Initialization

The DXGKRNL driver initializes through standard Linux kernel mechanisms:

1. Module initialization creates the `/dev/dxg` device node
2. PCI and VM bus subsystems are registered
3. GPU adapters are discovered through PCI and VM bus channels
4. Adapter channels are established for each GPU
5. The global VM bus channel is established

The driver presents itself to userspace through a miscellaneous device (`/dev/dxg`), which applications can open to interact with the driver through IOCTL commands.

**Sources:**
- [drivers/hv/dxgkrnl/dxgmodule.c]()
- [drivers/hv/dxgkrnl/Makefile]()
- [drivers/hv/dxgkrnl/Kconfig]()

## Conclusion

The DXGKRNL Graphics Driver enables Linux applications in WSL2 environments to access the GPU capabilities of the Windows host system. It implements a paravirtualized approach, where GPU commands are forwarded to the Windows DirectX Graphics Kernel subsystem via VM bus channels. The driver manages adapters, devices, contexts, resources, and synchronization objects, providing a comprehensive interface for GPU access from Linux applications.2d:T2253,# Kernel Boot and Parameters

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [Documentation/admin-guide/kernel-parameters.txt](Documentation/admin-guide/kernel-parameters.txt)
- [Documentation/core-api/workqueue.rst](Documentation/core-api/workqueue.rst)
- [Documentation/translations/zh_CN/core-api/workqueue.rst](Documentation/translations/zh_CN/core-api/workqueue.rst)
- [include/linux/start_kernel.h](include/linux/start_kernel.h)
- [include/linux/workqueue.h](include/linux/workqueue.h)
- [init/main.c](init/main.c)
- [kernel/power/Makefile](kernel/power/Makefile)
- [kernel/power/process.c](kernel/power/process.c)
- [kernel/workqueue.c](kernel/workqueue.c)
- [kernel/workqueue_internal.h](kernel/workqueue_internal.h)
- [tools/workqueue/wq_dump.py](tools/workqueue/wq_dump.py)
- [tools/workqueue/wq_monitor.py](tools/workqueue/wq_monitor.py)

</details>



This document covers the kernel boot process, parameter handling mechanisms, and workqueue subsystem configuration during system initialization. It focuses on how kernel command-line parameters are parsed and processed, and how they influence system behavior, particularly for workqueue configuration.

For information about the workqueue subsystem's runtime behavior and API usage, see [Workqueue Subsystem](#3.2).

## Boot Process Overview

The kernel boot process follows a well-defined sequence where parameters are parsed at multiple stages to configure system behavior before full initialization.

### Boot Parameter Flow

```mermaid
graph TD
    bootloader["Bootloader"] --> cmdline["boot_command_line"]
    cmdline --> setup["setup_command_line()"]
    setup --> static["static_command_line"]
    setup --> saved["saved_command_line"]
    static --> early["parse_early_param()"]
    early --> do_early["do_early_param()"]
    static --> main_parse["parse_args()"]
    main_parse --> unknown["unknown_bootoption()"]
    main_parse --> module_params["Module Parameters"]
    do_early --> early_init["Early Subsystem Init"]
    unknown --> init_args["Init Arguments"]
    module_params --> wq_init["Workqueue Init"]
```

The kernel maintains multiple copies of the command line for different purposes: `boot_command_line` contains the original parameters, `static_command_line` is used for parsing, and `saved_command_line` is preserved for `/proc/cmdline`.

Sources: [init/main.c:139-148](), [init/main.c:617-673]()

## Parameter Parsing Mechanism

### Early Parameter Processing

Early parameters are processed before most kernel subsystems are initialized. This allows critical configuration to occur during the earliest boot stages.

```mermaid
graph LR
    start_kernel["start_kernel()"] --> parse_early["parse_early_param()"]
    parse_early --> do_early["do_early_param()"]
    do_early --> setup_check["obsolete_checksetup()"]
    do_early --> early_setup["Early Setup Functions"]
    
    subgraph "Parameter Types"
        console_param["console="]
        debug_param["debug"]  
        quiet_param["quiet"]
        loglevel_param["loglevel="]
    end
    
    early_setup --> console_param
    early_setup --> debug_param
    early_setup --> quiet_param
    early_setup --> loglevel_param
```

The `do_early_param()` function iterates through the `__setup_start` to `__setup_end` section, matching parameters with their handlers based on the `early` flag in `obs_kernel_param` structures.

Sources: [init/main.c:736-752](), [init/main.c:193-220]()

### Main Parameter Processing

After early initialization, the kernel processes remaining parameters through the `parse_args()` function, which handles both known kernel parameters and module parameters.

```mermaid
graph TD
    parse_args["parse_args()"] --> known_param["Known Parameter"]
    parse_args --> unknown_param["Unknown Parameter"]
    
    known_param --> kernel_param["Kernel Parameter Handler"]
    unknown_param --> unknown_boot["unknown_bootoption()"]
    
    unknown_boot --> sysctl_alias["sysctl_is_alias()"]
    unknown_boot --> obsolete_check["obsolete_checksetup()"]
    unknown_boot --> module_param["Module Parameter"]
    unknown_boot --> init_arg["Init Argument"]
    unknown_boot --> env_var["Environment Variable"]
    
    sysctl_alias --> handled["Handled as sysctl"]
    obsolete_check --> legacy["Legacy Parameter"]
    module_param --> dot_notation["Contains '.'"]
    init_arg --> argv_init["argv_init[]"]
    env_var --> envp_init["envp_init[]"]
```

The `unknown_bootoption()` function serves as a catch-all for parameters not handled by the kernel directly, routing them to appropriate handlers or storing them for init.

Sources: [init/main.c:529-575](), [init/main.c:911-914]()

## Workqueue Subsystem Configuration

The workqueue subsystem accepts several module parameters that control its behavior during and after initialization.

### Workqueue Module Parameters

```mermaid
graph LR
    subgraph "Workqueue Parameters"
        cpu_thresh["cpu_intensive_thresh_us"]
        power_eff["power_efficient"] 
        debug_rr["debug_force_rr_cpu"]
    end
    
    cpu_thresh --> wq_cpu_intensive_thresh_us["wq_cpu_intensive_thresh_us"]
    power_eff --> wq_power_efficient["wq_power_efficient"]
    debug_rr --> wq_debug_force_rr_cpu["wq_debug_force_rr_cpu"]
    
    wq_cpu_intensive_thresh_us --> concurrency_mgmt["Concurrency Management"]
    wq_power_efficient --> power_saving["Power Saving Mode"]
    wq_debug_force_rr_cpu --> round_robin["Force Round Robin"]
```

These parameters are defined using the `module_param_named()` macro and affect workqueue behavior globally:

- `cpu_intensive_thresh_us`: Controls when work items are considered CPU-intensive (default: calculated in `wq_cpu_intensive_thresh_init()`)
- `power_efficient`: Enables power-efficient workqueue mode  
- `debug_force_rr_cpu`: Forces round-robin CPU selection for debugging

Sources: [kernel/workqueue.c:360-365](), [kernel/workqueue.c:395-400]()

## Key Boot Parameters

### System Configuration Parameters

The kernel supports thousands of boot parameters documented in `kernel-parameters.txt`. Key categories include:

| Parameter Category | Examples | Purpose |
|-------------------|----------|---------|
| Console/Debug | `console=`, `debug`, `loglevel=` | Output control |
| Memory | `crashkernel=`, `cma=` | Memory allocation |
| CPU/SMP | `maxcpus=`, `isolcpus=` | CPU configuration |
| Hardware | `acpi=`, `pci=` | Hardware subsystem control |
| Workqueue | `workqueue.cpu_intensive_thresh_us` | Workqueue behavior |

### Parameter Format and Processing

Parameters follow specific format conventions:
- Boolean parameters: `param` or `param=1/0/on/off/y/n`
- Value parameters: `param=value`
- List parameters: `param=value1,value2,value3`
- Range parameters: `param=start-end`

Sources: [Documentation/admin-guide/kernel-parameters.txt:1-100]()

## Boot Configuration Integration

### Bootconfig Support

The kernel supports extended configuration through bootconfig, allowing structured parameter specification:

```mermaid
graph TD
    initrd["initrd/initramfs"] --> bootconfig_data["Bootconfig Data"]
    bootconfig_data --> xbc_init["xbc_init()"]
    xbc_init --> kernel_params["kernel.* parameters"]
    xbc_init --> init_params["init.* parameters"]
    
    kernel_params --> extra_command_line["extra_command_line"]
    init_params --> extra_init_args["extra_init_args"]
    
    extra_command_line --> setup_command_line["setup_command_line()"]
    extra_init_args --> init_argv["Init Arguments"]
```

The bootconfig system extracts structured configuration from initrd and converts it to command-line format for processing by the standard parameter parsing mechanisms.

Sources: [init/main.c:405-461](), [init/main.c:317-388]()

### Workqueue Initialization Sequence

```mermaid
graph TD
    start_kernel["start_kernel()"] --> workqueue_init_early["workqueue_init_early()"]
    workqueue_init_early --> early_pools["Create Early Worker Pools"]
    
    start_kernel --> rest_init["rest_init()"]
    rest_init --> kernel_init["kernel_init()"]
    kernel_init --> workqueue_init["workqueue_init()"]
    
    workqueue_init --> system_wq["system_wq"]
    workqueue_init --> system_highpri_wq["system_highpri_wq"] 
    workqueue_init --> system_long_wq["system_long_wq"]
    workqueue_init --> system_unbound_wq["system_unbound_wq"]
    workqueue_init --> system_freezable_wq["system_freezable_wq"]
    workqueue_init --> system_power_efficient_wq["system_power_efficient_wq"]
```

The workqueue subsystem initializes in two phases: early initialization creates basic worker pools, while full initialization creates the standard system workqueues used throughout the kernel.

Sources: [kernel/workqueue.c:423-436](), [init/main.c:876-1000]()2e:T5f1c,# Build System

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [.gitignore](.gitignore)
- [Documentation/core-api/symbol-namespaces.rst](Documentation/core-api/symbol-namespaces.rst)
- [Documentation/livepatch/livepatch.rst](Documentation/livepatch/livepatch.rst)
- [Documentation/livepatch/module-elf-format.rst](Documentation/livepatch/module-elf-format.rst)
- [Documentation/translations/zh_CN/core-api/kernel-api.rst](Documentation/translations/zh_CN/core-api/kernel-api.rst)
- [Documentation/translations/zh_CN/core-api/symbol-namespaces.rst](Documentation/translations/zh_CN/core-api/symbol-namespaces.rst)
- [MSFT-Merge/log](MSFT-Merge/log)
- [Makefile](Makefile)
- [arch/Kconfig](arch/Kconfig)
- [arch/powerpc/Kconfig](arch/powerpc/Kconfig)
- [arch/powerpc/include/asm/asm.h](arch/powerpc/include/asm/asm.h)
- [arch/x86/Kconfig](arch/x86/Kconfig)
- [arch/x86/include/asm/Kbuild](arch/x86/include/asm/Kbuild)
- [arch/x86/include/asm/orc_header.h](arch/x86/include/asm/orc_header.h)
- [arch/x86/include/asm/orc_types.h](arch/x86/include/asm/orc_types.h)
- [arch/x86/include/asm/topology.h](arch/x86/include/asm/topology.h)
- [arch/x86/include/asm/unwind_hints.h](arch/x86/include/asm/unwind_hints.h)
- [arch/x86/kernel/unwind_orc.c](arch/x86/kernel/unwind_orc.c)
- [drivers/vhost/Kconfig](drivers/vhost/Kconfig)
- [include/linux/cpu.h](include/linux/cpu.h)
- [include/linux/cpu_smt.h](include/linux/cpu_smt.h)
- [include/linux/cpuhotplug.h](include/linux/cpuhotplug.h)
- [include/linux/export.h](include/linux/export.h)
- [include/linux/hrtimer.h](include/linux/hrtimer.h)
- [include/linux/kallsyms.h](include/linux/kallsyms.h)
- [include/linux/module.h](include/linux/module.h)
- [include/linux/module_symbol.h](include/linux/module_symbol.h)
- [include/linux/objtool.h](include/linux/objtool.h)
- [kernel/Makefile](kernel/Makefile)
- [kernel/cpu.c](kernel/cpu.c)
- [kernel/kallsyms.c](kernel/kallsyms.c)
- [kernel/kallsyms_internal.h](kernel/kallsyms_internal.h)
- [kernel/kallsyms_selftest.c](kernel/kallsyms_selftest.c)
- [kernel/kallsyms_selftest.h](kernel/kallsyms_selftest.h)
- [kernel/ksyms_common.c](kernel/ksyms_common.c)
- [kernel/module/kallsyms.c](kernel/module/kallsyms.c)
- [kernel/module/livepatch.c](kernel/module/livepatch.c)
- [kernel/params.c](kernel/params.c)
- [kernel/smpboot.c](kernel/smpboot.c)
- [kernel/time/hrtimer.c](kernel/time/hrtimer.c)
- [rust/macros/module.rs](rust/macros/module.rs)
- [scripts/Kbuild.include](scripts/Kbuild.include)
- [scripts/Makefile.build](scripts/Makefile.build)
- [scripts/Makefile.lib](scripts/Makefile.lib)
- [scripts/Makefile.modfinal](scripts/Makefile.modfinal)
- [scripts/Makefile.modinst](scripts/Makefile.modinst)
- [scripts/Makefile.modpost](scripts/Makefile.modpost)
- [scripts/Makefile.package](scripts/Makefile.package)
- [scripts/Makefile.vmlinux_o](scripts/Makefile.vmlinux_o)
- [scripts/basic/fixdep.c](scripts/basic/fixdep.c)
- [scripts/check-git](scripts/check-git)
- [scripts/clang-tools/gen_compile_commands.py](scripts/clang-tools/gen_compile_commands.py)
- [scripts/depmod.sh](scripts/depmod.sh)
- [scripts/kallsyms.c](scripts/kallsyms.c)
- [scripts/link-vmlinux.sh](scripts/link-vmlinux.sh)
- [scripts/mksysmap](scripts/mksysmap)
- [scripts/mod/file2alias.c](scripts/mod/file2alias.c)
- [scripts/mod/modpost.c](scripts/mod/modpost.c)
- [scripts/mod/modpost.h](scripts/mod/modpost.h)
- [scripts/modules-check.sh](scripts/modules-check.sh)
- [scripts/orc_hash.sh](scripts/orc_hash.sh)
- [scripts/package/builddeb](scripts/package/builddeb)
- [scripts/package/buildtar](scripts/package/buildtar)
- [scripts/package/deb-build-option](scripts/package/deb-build-option)
- [scripts/package/debian/rules](scripts/package/debian/rules)
- [scripts/package/gen-diff-patch](scripts/package/gen-diff-patch)
- [scripts/package/install-extmod-build](scripts/package/install-extmod-build)
- [scripts/package/kernel.spec](scripts/package/kernel.spec)
- [scripts/package/mkdebian](scripts/package/mkdebian)
- [scripts/package/mkspec](scripts/package/mkspec)
- [scripts/remove-stale-files](scripts/remove-stale-files)
- [scripts/setlocalversion](scripts/setlocalversion)
- [tools/arch/x86/include/asm/orc_types.h](tools/arch/x86/include/asm/orc_types.h)
- [tools/objtool/arch/powerpc/Build](tools/objtool/arch/powerpc/Build)
- [tools/objtool/arch/powerpc/decode.c](tools/objtool/arch/powerpc/decode.c)
- [tools/objtool/arch/powerpc/include/arch/cfi_regs.h](tools/objtool/arch/powerpc/include/arch/cfi_regs.h)
- [tools/objtool/arch/powerpc/include/arch/elf.h](tools/objtool/arch/powerpc/include/arch/elf.h)
- [tools/objtool/arch/powerpc/include/arch/special.h](tools/objtool/arch/powerpc/include/arch/special.h)
- [tools/objtool/arch/powerpc/special.c](tools/objtool/arch/powerpc/special.c)
- [tools/objtool/arch/x86/decode.c](tools/objtool/arch/x86/decode.c)
- [tools/objtool/arch/x86/include/arch/elf.h](tools/objtool/arch/x86/include/arch/elf.h)
- [tools/objtool/arch/x86/special.c](tools/objtool/arch/x86/special.c)
- [tools/objtool/builtin-check.c](tools/objtool/builtin-check.c)
- [tools/objtool/check.c](tools/objtool/check.c)
- [tools/objtool/elf.c](tools/objtool/elf.c)
- [tools/objtool/include/objtool/arch.h](tools/objtool/include/objtool/arch.h)
- [tools/objtool/include/objtool/builtin.h](tools/objtool/include/objtool/builtin.h)
- [tools/objtool/include/objtool/cfi.h](tools/objtool/include/objtool/cfi.h)
- [tools/objtool/include/objtool/check.h](tools/objtool/include/objtool/check.h)
- [tools/objtool/include/objtool/elf.h](tools/objtool/include/objtool/elf.h)
- [tools/objtool/include/objtool/endianness.h](tools/objtool/include/objtool/endianness.h)
- [tools/objtool/include/objtool/objtool.h](tools/objtool/include/objtool/objtool.h)
- [tools/objtool/include/objtool/warn.h](tools/objtool/include/objtool/warn.h)
- [tools/objtool/objtool.c](tools/objtool/objtool.c)
- [tools/objtool/orc_dump.c](tools/objtool/orc_dump.c)
- [tools/objtool/orc_gen.c](tools/objtool/orc_gen.c)
- [tools/objtool/special.c](tools/objtool/special.c)
- [tools/testing/selftests/bpf/progs/bpf_iter_ksym.c](tools/testing/selftests/bpf/progs/bpf_iter_ksym.c)

</details>



This document covers the kernel build infrastructure, including compilation orchestration, static analysis tools, module processing, and package generation. The build system transforms source code into bootable kernel images and loadable modules through a complex pipeline of Makefiles, configuration systems, and verification tools.

For information about architecture-specific build configurations, see [Architecture-Specific Implementations](#6). For details about static analysis during the build process, see [Static Analysis Tools](#10.2).

## Core Build Infrastructure

The kernel build system is orchestrated by the main `Makefile` which coordinates compilation, linking, and packaging across multiple architectures and configurations. The build process supports both in-tree and out-of-tree builds, cross-compilation, and various output formats.

### Build Orchestration

```mermaid
graph TD
    MAIN_MAKEFILE["Makefile"] --> CONFIG_SYSTEM["Configuration System"]
    MAIN_MAKEFILE --> BUILD_SCRIPTS["scripts/Makefile.*"]
    MAIN_MAKEFILE --> ARCH_MAKEFILES["arch/*/Makefile"]
    
    CONFIG_SYSTEM --> KCONFIG["Kconfig Files"]
    CONFIG_SYSTEM --> AUTO_CONF["include/config/auto.conf"]
    
    BUILD_SCRIPTS --> MAKEFILE_BUILD["scripts/Makefile.build"]
    BUILD_SCRIPTS --> MAKEFILE_LIB["scripts/Makefile.lib"]
    BUILD_SCRIPTS --> MAKEFILE_MODPOST["scripts/Makefile.modpost"]
    
    MAKEFILE_BUILD --> COMPILATION["Object Compilation"]
    MAKEFILE_MODPOST --> MODULE_PROCESSING["Module Symbol Processing"]
    
    ARCH_MAKEFILES --> ARCH_SPECIFIC["Architecture-Specific Rules"]
    
    COMPILATION --> VMLINUX_LINK["vmlinux Linking"]
    MODULE_PROCESSING --> MODULE_INSTALL["Module Installation"]
    
    VMLINUX_LINK --> LINK_VMLINUX_SH["scripts/link-vmlinux.sh"]
    LINK_VMLINUX_SH --> KALLSYMS["scripts/kallsyms"]
    LINK_VMLINUX_SH --> MKSYSMAP["scripts/mksysmap"]
```

The main `Makefile` defines critical build variables and targets. Key components include the `KERNELRELEASE` variable constructed from `VERSION`, `PATCHLEVEL`, `SUBLEVEL`, and `EXTRAVERSION` at [Makefile:2-5](), and the primary build logic that handles both in-tree and out-of-tree builds through the `__sub-make` target at [Makefile:233-235]().

### Build Process Flow

```mermaid
graph TD
    START["make"] --> CHECK_SUBMAKE["need-sub-make?"]
    CHECK_SUBMAKE -->|yes| SUB_MAKE["__sub-make"]
    CHECK_SUBMAKE -->|no| MAIN_BUILD["Main Build Process"]
    
    SUB_MAKE --> MAIN_BUILD
    
    MAIN_BUILD --> CONFIG_CHECK["Configuration Check"]
    CONFIG_CHECK --> PREPARE_PHASE["prepare0"]
    
    PREPARE_PHASE --> SCRIPTS_BASIC["scripts_basic"]
    SCRIPTS_BASIC --> ARCH_PREPARE["Arch-specific prepare"]
    
    ARCH_PREPARE --> BUILD_TARGETS["Build Targets"]
    BUILD_TARGETS --> VMLINUX_TARGET["vmlinux"]
    BUILD_TARGETS --> MODULES_TARGET["modules"]
    BUILD_TARGETS --> OTHERS["Other Targets"]
    
    VMLINUX_TARGET --> COMPILE_OBJECTS["Compile Objects"]
    COMPILE_OBJECTS --> LINK_VMLINUX["Link vmlinux"]
    LINK_VMLINUX --> GENERATE_SYMBOLS["Generate Symbols"]
    
    MODULES_TARGET --> MODULE_COMPILE["Module Compilation"]
    MODULE_COMPILE --> MODULE_POSTPROCESS["modpost Processing"]
    MODULE_POSTPROCESS --> MODULE_INSTALL_TARGET["modules_install"]
```

The build process begins with environment setup and configuration validation. The `scripts_basic` target at [Makefile:632-633]() ensures basic build tools are available. The main compilation phase is controlled by `KBUILD_BUILTIN` and `KBUILD_MODULES` variables at [Makefile:718-719](), determining whether built-in objects or modules are built.

Sources: [Makefile:1-1200](), [scripts/Makefile.build:1-500]()

## Static Analysis and Code Verification

The build system incorporates sophisticated static analysis tools, primarily through `objtool`, which performs instruction-level analysis and generates metadata for runtime features like stack unwinding and control flow integrity.

### Objtool Architecture

```mermaid
graph TD
    OBJTOOL_MAIN["tools/objtool/check.c"] --> ELF_PARSER["tools/objtool/elf.c"]
    OBJTOOL_MAIN --> ARCH_DECODE["tools/objtool/arch/*/decode.c"]
    
    ELF_PARSER --> SECTION_ANALYSIS["Section Analysis"]
    ELF_PARSER --> SYMBOL_MANAGEMENT["Symbol Management"]
    ELF_PARSER --> RELOC_PROCESSING["Relocation Processing"]
    
    SECTION_ANALYSIS --> find_section_by_name["find_section_by_name()"]
    SYMBOL_MANAGEMENT --> find_symbol_by_name["find_symbol_by_name()"]
    RELOC_PROCESSING --> find_reloc_by_dest["find_reloc_by_dest()"]
    
    ARCH_DECODE --> INSN_DECODE["arch_decode_instruction()"]
    INSN_DECODE --> CFI_ANALYSIS["CFI State Analysis"]
    INSN_DECODE --> STACK_VALIDATION["Stack Validation"]
    
    OBJTOOL_MAIN --> decode_instructions["decode_instructions()"]
    decode_instructions --> INSTRUCTION_HASH["insn_hash Table"]
    
    CFI_ANALYSIS --> create_cfi_sections["create_cfi_sections()"]
    STACK_VALIDATION --> create_retpoline_sites_sections["create_retpoline_sites_sections()"]
```

The `objtool` system performs comprehensive analysis of object files during compilation. The main analysis engine in `decode_instructions()` at [tools/objtool/check.c:368-487]() processes each executable section, creating a hash table of instructions for efficient lookup. The tool maintains CFI (Call Frame Information) state through the `cfi_state` structure and generates specialized sections for runtime features.

### Code Analysis Pipeline

```mermaid
graph TD
    INPUT_OBJ["Object Files"] --> READ_ELF["elf_open_read()"]
    READ_ELF --> PARSE_SECTIONS["read_sections()"]
    PARSE_SECTIONS --> PARSE_SYMBOLS["read_symbols()"]
    
    PARSE_SYMBOLS --> DECODE_INSNS["decode_instructions()"]
    DECODE_INSNS --> VALIDATE_FUNCTIONS["Function Validation"]
    
    VALIDATE_FUNCTIONS --> CHECK_UNREACHABLE["add_dead_ends()"]
    CHECK_UNREACHABLE --> STACK_ANALYSIS["Stack Frame Analysis"]
    STACK_ANALYSIS --> CFI_GENERATION["CFI Metadata Generation"]
    
    CFI_GENERATION --> create_static_call_sections["create_static_call_sections()"]
    CFI_GENERATION --> create_retpoline_sites_sections["create_retpoline_sites_sections()"]
    CFI_GENERATION --> create_return_sites_sections["create_return_sites_sections()"]
    CFI_GENERATION --> create_ibt_endbr_seal_sections["create_ibt_endbr_seal_sections()"]
    
    create_static_call_sections --> OUTPUT_SECTIONS["Modified Object Files"]
    create_retpoline_sites_sections --> OUTPUT_SECTIONS
    create_return_sites_sections --> OUTPUT_SECTIONS
    create_ibt_endbr_seal_sections --> OUTPUT_SECTIONS
```

The analysis pipeline processes each object file through multiple phases. Symbol resolution occurs in `read_symbols()` at [tools/objtool/elf.c:435-574](), which builds interval trees for efficient symbol lookup. The instruction decoder at [tools/objtool/check.c:368-487]() creates instruction metadata that enables advanced static analysis features.

Sources: [tools/objtool/check.c:1-4000](), [tools/objtool/elf.c:1-1000](), [tools/objtool/include/objtool/elf.h:1-351]()

## Module Processing System

The module processing system handles symbol resolution, versioning, and metadata generation for loadable kernel modules through the `modpost` tool and associated infrastructure.

### Module Symbol Processing

```mermaid
graph TD
    MODPOST["scripts/mod/modpost.c"] --> PARSE_ELF["parse_elf()"]
    PARSE_ELF --> SYMBOL_HASH["symbolhash[SYMBOL_HASH_SIZE]"]
    
    SYMBOL_HASH --> sym_add_exported["sym_add_exported()"]
    SYMBOL_HASH --> sym_add_unresolved["sym_add_unresolved()"]
    
    MODPOST --> MODULE_LIST["modules list"]
    MODULE_LIST --> new_module["new_module()"]
    new_module --> MODULE_STRUCT["struct module"]
    
    MODULE_STRUCT --> exported_symbols["exported_symbols"]
    MODULE_STRUCT --> unresolved_symbols["unresolved_symbols"]
    MODULE_STRUCT --> missing_namespaces["missing_namespaces"]
    
    PARSE_ELF --> handle_symbol["handle_symbol()"]
    handle_symbol --> get_modinfo["get_modinfo()"]
    get_modinfo --> MODULE_LICENSE["MODULE_LICENSE"]
    get_modinfo --> MODULE_AUTHOR["MODULE_AUTHOR"]
    
    SYMBOL_HASH --> SYMBOL_RESOLUTION["Symbol Resolution"]
    SYMBOL_RESOLUTION --> check_exports["check_exports()"]
    check_exports --> MISSING_EXPORTS["Missing Export Warnings"]
```

The `modpost` system maintains a global symbol hash table with `SYMBOL_HASH_SIZE` entries at [scripts/mod/modpost.c:213]() for efficient symbol lookup. Each module is represented by a `struct module` at [scripts/mod/modpost.c:182-208]() containing lists of exported symbols, unresolved symbols, and missing namespaces.

### Module Build Integration

```mermaid
graph TD
    MAKEFILE_MODPOST["scripts/Makefile.modpost"] --> MODPOST_EXEC["modpost execution"]
    MODPOST_EXEC --> VMLINUX_ANALYSIS["vmlinux.o analysis"]
    MODPOST_EXEC --> MODULE_ANALYSIS["*.o module analysis"]
    
    VMLINUX_ANALYSIS --> EXTRACT_SYMBOLS["Extract kernel symbols"]
    MODULE_ANALYSIS --> RESOLVE_SYMBOLS["Resolve module dependencies"]
    
    EXTRACT_SYMBOLS --> SYMBOL_TABLE["Global symbol table"]
    RESOLVE_SYMBOLS --> SYMBOL_TABLE
    
    SYMBOL_TABLE --> GENERATE_MODINFO["Generate .modinfo"]
    SYMBOL_TABLE --> GENERATE_VERSIONS["Generate Module.symvers"]
    
    GENERATE_MODINFO --> MOD_FILES[".mod files"]
    GENERATE_VERSIONS --> MODULE_SYMVERS["Module.symvers"]
    
    MOD_FILES --> MODULE_INSTALL["modules_install"]
    MODULE_SYMVERS --> KBUILD_EXTMOD["External module builds"]
```

Module processing integrates with the main build system through `scripts/Makefile.modpost`. The system processes both `vmlinux.o` for kernel symbols and individual module object files. Symbol version generation uses CRC checksums calculated in `sym_set_crc()` at [scripts/mod/modpost.c:390-394]() to ensure ABI compatibility.

Sources: [scripts/mod/modpost.c:1-2000](), [scripts/Makefile.modpost:1-150](), [scripts/Makefile.modinst:1-100]()

## Package Generation System

The kernel build system supports multiple package formats including RPM, DEB, and TAR archives through a unified packaging infrastructure that handles cross-compilation and architecture-specific requirements.

### Package Generation Architecture

```mermaid
graph TD
    MAKEFILE_PACKAGE["scripts/Makefile.package"] --> RPM_TARGETS["RPM Generation"]
    MAKEFILE_PACKAGE --> DEB_TARGETS["DEB Generation"]
    MAKEFILE_PACKAGE --> TAR_TARGETS["TAR Generation"]
    
    RPM_TARGETS --> rpm_pkg["rpm-pkg"]
    RPM_TARGETS --> srcrpm_pkg["srcrpm-pkg"]
    RPM_TARGETS --> binrpm_pkg["binrpm-pkg"]
    
    DEB_TARGETS --> deb_pkg["deb-pkg"]
    DEB_TARGETS --> srcdeb_pkg["srcdeb-pkg"]
    DEB_TARGETS --> bindeb_pkg["bindeb-pkg"]
    
    TAR_TARGETS --> tar_pkg["tar-pkg"]
    TAR_TARGETS --> targz_pkg["targz-pkg"]
    TAR_TARGETS --> tarxz_pkg["tarxz-pkg"]
    
    rpm_pkg --> MKSPEC["scripts/package/mkspec"]
    MKSPEC --> kernel_spec["kernel.spec"]
    
    deb_pkg --> MKDEBIAN["scripts/package/mkdebian"]
    MKDEBIAN --> debian_dir["debian/"]
    
    tar_pkg --> BUILDTAR["scripts/package/buildtar"]
    BUILDTAR --> tar_install["tar-install"]
```

The packaging system provides standardized targets for different package formats. RPM generation uses `scripts/package/mkspec` at [scripts/package/mkspec:1-25]() to create spec files, while DEB packages use `scripts/package/mkdebian` for Debian control files. The system handles architecture detection through `set_debarch()` at [scripts/package/mkdebian:21-74]().

### Source Packaging Pipeline

```mermaid
graph TD
    SOURCE_TREE["Source Tree"] --> GIT_ARCHIVE["git archive"]
    GIT_ARCHIVE --> TAR_CONTENT["TAR_CONTENT"]
    
    TAR_CONTENT --> DOCUMENTATION["Documentation"]
    TAR_CONTENT --> ARCH_DIRS["arch/"]
    TAR_CONTENT --> KERNEL_DIR["kernel/"]
    TAR_CONTENT --> DRIVERS_DIR["drivers/"]
    TAR_CONTENT --> OTHER_DIRS["Other Directories"]
    
    GIT_ARCHIVE --> COMPRESSION["Compression"]
    COMPRESSION --> KGZIP["$(KGZIP)"]
    COMPRESSION --> KBZIP2["$(KBZIP2)"]
    COMPRESSION --> XZ["$(XZ)"]
    COMPRESSION --> ZSTD["$(ZSTD)"]
    
    COMPRESSION --> linux_tar_gz["linux.tar.gz"]
    COMPRESSION --> linux_tar_bz2["linux.tar.bz2"]
    COMPRESSION --> linux_tar_xz["linux.tar.xz"]
    
    linux_tar_gz --> PACKAGE_DIST["Package Distribution"]
```

Source packaging creates archives from the Git repository using the `TAR_CONTENT` variable at [scripts/Makefile.package:9-13]() which defines included directories. The system supports multiple compression formats through configurable tools and generates checksums for integrity verification.

Sources: [scripts/Makefile.package:1-275](), [scripts/package/mkspec:1-25](), [scripts/package/mkdebian:1-200]()

## Configuration System Integration

The build system integrates closely with the Kconfig system to handle architecture-specific options, feature selection, and build-time configuration validation across multiple supported architectures.

### Configuration Processing Flow

```mermaid
graph TD
    KCONFIG_FILES["Kconfig files"] --> MENUCONFIG["make menuconfig"]
    KCONFIG_FILES --> DEFCONFIG["make defconfig"]
    KCONFIG_FILES --> OLDCONFIG["make oldconfig"]
    
    MENUCONFIG --> DOT_CONFIG[".config"]
    DEFCONFIG --> DOT_CONFIG
    OLDCONFIG --> DOT_CONFIG
    
    DOT_CONFIG --> SYNCCONFIG["syncconfig"]
    SYNCCONFIG --> AUTO_CONF["include/config/auto.conf"]
    SYNCCONFIG --> AUTOCONF_H["include/generated/autoconf.h"]
    SYNCCONFIG --> RUSTC_CFG["include/generated/rustc_cfg"]
    
    AUTO_CONF --> MAKEFILE_INCLUDE["Makefile inclusion"]
    AUTOCONF_H --> C_COMPILATION["C compilation"]
    RUSTC_CFG --> RUST_COMPILATION["Rust compilation"]
    
    MAKEFILE_INCLUDE --> CONFIG_CHECKS["CONFIG_* checks"]
    CONFIG_CHECKS --> CONDITIONAL_COMPILATION["Conditional Compilation"]
```

The configuration system uses `syncconfig` to generate multiple output files from `.config`. The main Makefile includes configuration through the `auto.conf` mechanism at [Makefile:741](), enabling conditional compilation based on selected features. Architecture-specific configurations are processed through `arch/$(SRCARCH)/Makefile` inclusion.

### Architecture Configuration Integration

```mermaid
graph TD
    ARCH_KCONFIG["arch/$(SRCARCH)/Kconfig"] --> ARCH_OPTIONS["Architecture Options"]
    ARCH_OPTIONS --> CPU_SELECTION["CPU Selection"]
    ARCH_OPTIONS --> PLATFORM_SUPPORT["Platform Support"]
    ARCH_OPTIONS --> FEATURE_SELECTION["Feature Selection"]
    
    CPU_SELECTION --> X86_CPU_TYPES["X86 CPU Types"]
    CPU_SELECTION --> ARM_CPU_TYPES["ARM CPU Types"]
    CPU_SELECTION --> PPC_CPU_TYPES["PowerPC CPU Types"]
    
    PLATFORM_SUPPORT --> X86_PLATFORMS["X86 Platforms"]
    PLATFORM_SUPPORT --> ARM_PLATFORMS["ARM Platforms"]
    
    FEATURE_SELECTION --> VIRTUALIZATION["Virtualization Support"]
    FEATURE_SELECTION --> SECURITY_FEATURES["Security Features"]
    FEATURE_SELECTION --> DEBUG_OPTIONS["Debug Options"]
    
    ARCH_KCONFIG --> ARCH_MAKEFILE["arch/$(SRCARCH)/Makefile"]
    ARCH_MAKEFILE --> COMPILER_FLAGS["Compiler Flags"]
    ARCH_MAKEFILE --> LINKER_SCRIPT["Linker Script"]
```

Architecture-specific configuration integrates through dedicated Kconfig files. The x86 configuration at [arch/x86/Kconfig:56-307]() demonstrates extensive feature selection including virtualization support, security mitigations, and debug options. These configurations influence compiler flags and linker scripts through architecture Makefiles.

Sources: [Makefile:740-815](), [arch/x86/Kconfig:1-2500](), [arch/Kconfig:1-1500]()

## Build Optimization and Caching

The build system implements sophisticated caching and optimization mechanisms to minimize rebuild times and efficiently handle incremental builds across large codebases with thousands of source files.

### Dependency Tracking System

```mermaid
graph TD
    SOURCE_FILES["Source Files"] --> DEPENDENCY_GEN["Dependency Generation"]
    DEPENDENCY_GEN --> DEP_FILES[".d files"]
    
    DEP_FILES --> MAKEFILE_INCLUDE["Makefile inclusion"]
    MAKEFILE_INCLUDE --> REBUILD_DECISIONS["Rebuild Decisions"]
    
    REBUILD_DECISIONS --> if_changed["if_changed()"]
    REBUILD_DECISIONS --> if_changed_dep["if_changed_dep()"]
    
    if_changed --> COMMAND_COMPARISON["Command Line Comparison"]
    if_changed_dep --> DEPENDENCY_CHECK["Dependency Check"]
    
    COMMAND_COMPARISON --> CMD_FILES[".cmd files"]
    DEPENDENCY_CHECK --> PREREQUISITE_CHECK["Prerequisite Check"]
    
    CMD_FILES --> INCREMENTAL_BUILD["Incremental Build"]
    PREREQUISITE_CHECK --> INCREMENTAL_BUILD
```

The build system tracks dependencies through `.d` files generated during compilation and `.cmd` files storing command line information. The `if_changed` and `if_changed_dep` functions in `scripts/Kbuild.include` determine when objects need rebuilding based on source changes or command line modifications.

### Parallel Build Support

```mermaid
graph TD
    MAKE_J["make -j N"] --> JOBSERVER["GNU Make Jobserver"]
    JOBSERVER --> PARALLEL_TARGETS["Parallel Target Execution"]
    
    PARALLEL_TARGETS --> SUBDIR_PARALLEL["Subdirectory Builds"]
    PARALLEL_TARGETS --> MODULE_PARALLEL["Module Compilation"]
    PARALLEL_TARGETS --> OBJTOOL_PARALLEL["Objtool Analysis"]
    
    SUBDIR_PARALLEL --> BUILT_IN_A["built-in.a Archives"]
    MODULE_PARALLEL --> MODULE_O["*.o Module Objects"]
    OBJTOOL_PARALLEL --> OBJTOOL_METADATA["Objtool Metadata"]
    
    BUILT_IN_A --> VMLINUX_LINK["vmlinux Linking"]
    MODULE_O --> MODPOST_PHASE["modpost Phase"]
    OBJTOOL_METADATA --> VMLINUX_LINK
    
    VMLINUX_LINK --> FINAL_VMLINUX["Final vmlinux"]
    MODPOST_PHASE --> MODULE_INSTALL_TARGETS["Module Installation"]
```

The build system leverages GNU Make's jobserver for parallel execution. Critical synchronization points include the `modpost` phase which must wait for all module compilation to complete, and `vmlinux` linking which requires all `built-in.a` archives and objtool metadata.

Sources: [scripts/Kbuild.include:1-400](), [scripts/Makefile.build:1-600](), [scripts/link-vmlinux.sh:1-300]()2f:T3e6b,# Build Infrastructure

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [.gitignore](.gitignore)
- [Documentation/core-api/symbol-namespaces.rst](Documentation/core-api/symbol-namespaces.rst)
- [Documentation/livepatch/livepatch.rst](Documentation/livepatch/livepatch.rst)
- [Documentation/livepatch/module-elf-format.rst](Documentation/livepatch/module-elf-format.rst)
- [Documentation/translations/zh_CN/core-api/kernel-api.rst](Documentation/translations/zh_CN/core-api/kernel-api.rst)
- [Documentation/translations/zh_CN/core-api/symbol-namespaces.rst](Documentation/translations/zh_CN/core-api/symbol-namespaces.rst)
- [MSFT-Merge/log](MSFT-Merge/log)
- [Makefile](Makefile)
- [drivers/vhost/Kconfig](drivers/vhost/Kconfig)
- [include/linux/export.h](include/linux/export.h)
- [include/linux/kallsyms.h](include/linux/kallsyms.h)
- [include/linux/module.h](include/linux/module.h)
- [include/linux/module_symbol.h](include/linux/module_symbol.h)
- [kernel/Makefile](kernel/Makefile)
- [kernel/kallsyms.c](kernel/kallsyms.c)
- [kernel/kallsyms_internal.h](kernel/kallsyms_internal.h)
- [kernel/kallsyms_selftest.c](kernel/kallsyms_selftest.c)
- [kernel/kallsyms_selftest.h](kernel/kallsyms_selftest.h)
- [kernel/ksyms_common.c](kernel/ksyms_common.c)
- [kernel/module/kallsyms.c](kernel/module/kallsyms.c)
- [kernel/module/livepatch.c](kernel/module/livepatch.c)
- [kernel/params.c](kernel/params.c)
- [rust/macros/module.rs](rust/macros/module.rs)
- [scripts/Kbuild.include](scripts/Kbuild.include)
- [scripts/Makefile.modfinal](scripts/Makefile.modfinal)
- [scripts/Makefile.modinst](scripts/Makefile.modinst)
- [scripts/Makefile.modpost](scripts/Makefile.modpost)
- [scripts/Makefile.package](scripts/Makefile.package)
- [scripts/basic/fixdep.c](scripts/basic/fixdep.c)
- [scripts/check-git](scripts/check-git)
- [scripts/clang-tools/gen_compile_commands.py](scripts/clang-tools/gen_compile_commands.py)
- [scripts/depmod.sh](scripts/depmod.sh)
- [scripts/kallsyms.c](scripts/kallsyms.c)
- [scripts/mksysmap](scripts/mksysmap)
- [scripts/mod/file2alias.c](scripts/mod/file2alias.c)
- [scripts/mod/modpost.c](scripts/mod/modpost.c)
- [scripts/mod/modpost.h](scripts/mod/modpost.h)
- [scripts/modules-check.sh](scripts/modules-check.sh)
- [scripts/package/builddeb](scripts/package/builddeb)
- [scripts/package/buildtar](scripts/package/buildtar)
- [scripts/package/deb-build-option](scripts/package/deb-build-option)
- [scripts/package/debian/rules](scripts/package/debian/rules)
- [scripts/package/gen-diff-patch](scripts/package/gen-diff-patch)
- [scripts/package/install-extmod-build](scripts/package/install-extmod-build)
- [scripts/package/kernel.spec](scripts/package/kernel.spec)
- [scripts/package/mkdebian](scripts/package/mkdebian)
- [scripts/package/mkspec](scripts/package/mkspec)
- [scripts/remove-stale-files](scripts/remove-stale-files)
- [scripts/setlocalversion](scripts/setlocalversion)
- [tools/testing/selftests/bpf/progs/bpf_iter_ksym.c](tools/testing/selftests/bpf/progs/bpf_iter_ksym.c)

</details>



## Purpose and Scope

This document covers the core build system implementation of the WSL2 Linux kernel, including the primary Makefile infrastructure, module processing pipeline, symbol management system, and package generation mechanisms. The build infrastructure orchestrates compilation, linking, module processing, and packaging across multiple architectures and configurations.

For information about static analysis tools that work with the build system, see [Static Analysis Tools](#10.2).

## Core Build System Architecture

The kernel build system is centered around a recursive make architecture with the main [Makefile:1-1500]() serving as the primary orchestrator. The build process handles cross-compilation, architecture-specific configurations, and multiple output formats.

### Build System Flow

```mermaid
graph TD
    makefile["Makefile"] --> version_check["Version Detection"]
    makefile --> arch_setup["Architecture Setup"]
    makefile --> toolchain["Toolchain Configuration"]
    
    version_check --> kernel_release["KERNELRELEASE"]
    arch_setup --> srcarch["SRCARCH"]
    arch_setup --> cross_compile["CROSS_COMPILE"]
    
    toolchain --> cc["CC Compilation"]
    toolchain --> ld["LD Linking"] 
    toolchain --> ar["AR Archiving"]
    
    cc --> vmlinux_build["vmlinux Build"]
    ld --> vmlinux_build
    ar --> vmlinux_build
    
    vmlinux_build --> modules_build["Modules Build"]
    modules_build --> modpost["scripts/mod/modpost"]
    modpost --> module_symvers["Module.symvers"]
    
    vmlinux_build --> kallsyms_gen["scripts/kallsyms"]
    kallsyms_gen --> system_map["System.map"]
```

The main build process begins with version detection [Makefile:2-6]() where `VERSION`, `PATCHLEVEL`, `SUBLEVEL`, and `EXTRAVERSION` are defined. Architecture setup occurs through `SRCARCH` determination [Makefile:397-418](), handling architecture aliases like `i386` and `x86_64` both mapping to `x86`.

Toolchain configuration [Makefile:435-524]() supports both GCC and LLVM/Clang through the `LLVM` variable, with automatic tool prefix handling for cross-compilation scenarios.

Sources: Makefile:1-524, scripts/Kbuild.include:1-100

### Build Targets and Phases

The build system operates in distinct phases with specific targets:

```mermaid
graph LR
    prepare["prepare"] --> scripts_basic["scripts_basic"]
    scripts_basic --> vmlinux_deps["vmlinux-deps"]
    vmlinux_deps --> vmlinux["vmlinux"]
    
    vmlinux --> modules["modules"]
    modules --> modules_install["modules_install"]
    
    prepare --> config_targets["*config"]
    config_targets --> syncconfig["syncconfig"]
    
    vmlinux --> package_targets["Package Targets"]
    package_targets --> rpm_pkg["rpm-pkg"]
    package_targets --> deb_pkg["deb-pkg"]
    package_targets --> tar_pkg["tar-pkg"]
```

The `prepare` target [Makefile:1102]() establishes the build environment, while `scripts_basic` [Makefile:631-633]() builds essential host programs. The `vmlinux` target represents the core kernel image, and `modules` handles loadable kernel modules.

Sources: Makefile:631-756, scripts/Makefile.modinst:1-164

## Module Processing Pipeline

The module processing system transforms compiled object files into loadable kernel modules through a multi-stage pipeline centered around the `modpost` utility.

### Module Processing Flow

```mermaid
graph TD
    mod_o["module.o files"] --> modpost_stage1["modpost Stage 1"]
    vmlinux_o["vmlinux.o"] --> modpost_stage1
    
    modpost_stage1 --> module_symvers["Module.symvers"]
    modpost_stage1 --> mod_c["module.mod.c files"]
    
    mod_c --> compile_mod_c["Compile .mod.c"]
    compile_mod_c --> mod_obj["module.mod.o"]
    
    mod_obj --> ld_ko["Link .ko"]
    mod_o --> ld_ko
    ld_ko --> final_ko["module.ko"]
    
    final_ko --> sign_modules["Module Signing"]
    sign_modules --> compress_modules["Module Compression"]
    compress_modules --> install_modules["Module Installation"]
```

The `modpost` utility [scripts/mod/modpost.c:1-2500]() serves as the central component, processing ELF files to resolve symbols and generate module metadata. The main processing function `main()` [scripts/mod/modpost.c:810-843]() orchestrates symbol table analysis and cross-reference generation.

### Symbol Resolution and Dependencies

```mermaid
graph TB
    parse_elf["parse_elf()"] --> symbol_extraction["Symbol Extraction"]
    symbol_extraction --> handle_symbol["handle_symbol()"]
    
    handle_symbol --> exported_symbols["Exported Symbols"]
    handle_symbol --> unresolved_symbols["Unresolved Symbols"]
    
    exported_symbols --> symbol_hash["Symbol Hash Table"]
    unresolved_symbols --> dependency_analysis["Dependency Analysis"]
    
    symbol_hash --> sym_add_exported["sym_add_exported()"]
    dependency_analysis --> sym_add_unresolved["sym_add_unresolved()"]
    
    sym_add_exported --> module_symvers_out["Module.symvers Output"]
    sym_add_unresolved --> missing_deps["Missing Dependencies"]
```

The `parse_elf()` function [scripts/mod/modpost.c:423-582]() processes ELF headers and section tables, while `handle_symbol()` [scripts/mod/modpost.c:624-664]() categorizes symbols into exported and unresolved categories. Symbol resolution uses a hash table implementation [scripts/mod/modpost.c:229-266]() for efficient lookup.

Sources: scripts/mod/modpost.c:1-2500, scripts/Makefile.modpost:1-156, scripts/Makefile.modfinal:1-82

## Symbol Management System

The kernel symbol management system generates compressed symbol tables and provides runtime symbol lookup capabilities through the `kallsyms` subsystem.

### Symbol Table Generation Pipeline

```mermaid
graph TD
    nm_output["nm vmlinux"] --> mksysmap["scripts/mksysmap"]
    mksysmap --> system_map["System.map"]
    
    system_map --> kallsyms_script["scripts/kallsyms"]
    kallsyms_script --> read_map["read_map()"]
    
    read_map --> symbol_validation["Symbol Validation"]
    symbol_validation --> shrink_table["shrink_table()"]
    
    shrink_table --> token_optimization["optimize_token_table()"]
    token_optimization --> compression["Symbol Compression"]
    
    compression --> write_src["write_src()"]
    write_src --> kallsyms_s["kallsyms.S"]
    
    kallsyms_s --> assembly["Assembler"]
    assembly --> kallsyms_o["kallsyms.o"]
```

The `scripts/kallsyms` utility [scripts/kallsyms.c:1-844]() implements a sophisticated compression algorithm. The `read_map()` function [scripts/kallsyms.c:257-292]() parses symbol addresses and names, while `optimize_token_table()` [scripts/kallsyms.c:695-702]() applies compression through token substitution.

### Runtime Symbol Lookup Architecture

```mermaid
graph LR
    kallsyms_lookup_name["kallsyms_lookup_name()"] --> kallsyms_lookup_names["kallsyms_lookup_names()"]
    kallsyms_lookup_names --> binary_search["Binary Search"]
    
    binary_search --> get_symbol_seq["get_symbol_seq()"]
    get_symbol_seq --> get_symbol_offset["get_symbol_offset()"]
    get_symbol_offset --> kallsyms_expand_symbol["kallsyms_expand_symbol()"]
    
    kallsyms_expand_symbol --> token_table["kallsyms_token_table"]
    token_table --> decompressed_name["Decompressed Symbol Name"]
    
    kallsyms_sym_address["kallsyms_sym_address()"] --> address_calculation["Address Calculation"]
    address_calculation --> final_address["Symbol Address"]
```

Runtime symbol lookup [kernel/kallsyms.c:265-279]() uses binary search through compressed symbol tables. The `kallsyms_expand_symbol()` function [kernel/kallsyms.c:42-96]() decompresses symbol names using the token table generated during build time.

Sources: scripts/kallsyms.c:1-844, kernel/kallsyms.c:1-700, scripts/mksysmap:1-105

## Package Generation System

The build system supports multiple package formats through a unified packaging infrastructure that handles distribution-specific requirements and metadata generation.

### Package Format Support Matrix

| Format | Script | Spec Generator | Build Script |
|--------|--------|----------------|--------------|
| RPM | `scripts/package/mkspec` | `kernel.spec` | `rpmbuild` |
| DEB | `scripts/package/mkdebian` | `debian/` | `scripts/package/builddeb` |
| TAR | `scripts/Makefile.package` | N/A | Built-in |
| SNAP | `scripts/package/snapcraft.template` | `snapcraft.yaml` | `snapcraft` |

### RPM Package Generation Flow

```mermaid
graph TD
    make_rpm["make rpm-pkg"] --> mkspec["scripts/package/mkspec"]
    mkspec --> kernel_spec["kernel.spec"]
    
    kernel_spec --> rpm_sources["rpm-sources target"]
    rpm_sources --> linux_tar["linux.tar.gz"]
    rpm_sources --> config_copy["Config Copy"]
    
    linux_tar --> rpmbuild_sources["rpmbuild/SOURCES/"]
    config_copy --> rpmbuild_sources
    
    rpmbuild_sources --> rpmbuild_cmd["rpmbuild command"]
    rpmbuild_cmd --> binary_rpm["Binary RPM"]
    rpmbuild_cmd --> source_rpm["Source RPM"]
```

The `mkspec` script [scripts/package/mkspec:1-25]() generates RPM specifications dynamically, incorporating kernel configuration and version information. The package generation process [scripts/Makefile.package:77-99]() handles source preparation and build orchestration.

### Debian Package Generation Flow

```mermaid
graph TD
    make_deb["make deb-pkg"] --> mkdebian["scripts/package/mkdebian"]
    mkdebian --> debian_dir["debian/ directory"]
    
    debian_dir --> control_file["debian/control"]
    debian_dir --> changelog["debian/changelog"]
    debian_dir --> rules["debian/rules"]
    
    control_file --> builddeb["scripts/package/builddeb"]
    rules --> builddeb
    
    builddeb --> install_linux_image["install_linux_image()"]
    builddeb --> install_kernel_headers["install_kernel_headers()"]
    
    install_linux_image --> create_package["create_package()"]
    install_kernel_headers --> create_package
    create_package --> deb_files[".deb files"]
```

The `mkdebian` script [scripts/package/mkdebian:1-274]() creates Debian packaging metadata, while `builddeb` [scripts/package/builddeb:1-220]() handles the actual package construction through functions like `install_linux_image()` [scripts/package/builddeb:54-122]().

Sources: scripts/Makefile.package:1-275, scripts/package/mkspec:1-25, scripts/package/mkdebian:1-274, scripts/package/builddeb:1-220

## Build Utilities and Helper Infrastructure

The build system includes numerous utility scripts and helper programs that support compilation, dependency tracking, and build optimization.

### Core Build Utilities

```mermaid
graph TB
    fixdep["scripts/basic/fixdep"] --> dependency_tracking["Dependency Tracking"]
    setlocalversion["scripts/setlocalversion"] --> version_string["Version String Generation"]
    remove_stale["scripts/remove-stale-files"] --> cleanup["Build Cleanup"]
    
    dependency_tracking --> dot_d_files[".d files"]
    version_string --> kernelrelease["KERNELRELEASE"]
    cleanup --> stale_removal["Stale File Removal"]
    
    kbuild_include["scripts/Kbuild.include"] --> helper_functions["Helper Functions"]
    helper_functions --> if_changed["if_changed"]
    helper_functions --> cmd_and_fixdep["cmd_and_fixdep"]
    helper_functions --> filechk["filechk"]
```

The `fixdep` utility [scripts/basic/fixdep.c:1-400]() optimizes dependency files by converting GCC-generated dependencies into make-compatible format. The `setlocalversion` script [scripts/setlocalversion:1-200]() generates version strings incorporating VCS information.

### Kbuild Helper Functions

Key helper functions in [scripts/Kbuild.include:1-450]() provide build system primitives:

- `if_changed` [scripts/Kbuild.include:150-160](): Conditional command execution based on prerequisite changes
- `filechk` [scripts/Kbuild.include:86-100](): File content verification and update mechanism  
- `cmd_and_fixdep` [scripts/Kbuild.include:200-220](): Command execution with dependency processing

### Module Installation and Processing

```mermaid
graph LR
    modules_install["make modules_install"] --> modinst_makefile["scripts/Makefile.modinst"]
    modinst_makefile --> module_copy["Module Copying"]
    modinst_makefile --> module_strip["Module Stripping"]
    
    module_copy --> modlib_dir["$(MODLIB) Directory"]
    module_strip --> strip_debug["Strip Debug Info"]
    
    modlib_dir --> depmod["scripts/depmod.sh"]
    strip_debug --> module_signing["Module Signing"]
    
    module_signing --> compressed_modules["Module Compression"]
    compressed_modules --> final_install["Final Installation"]
```

The module installation process [scripts/Makefile.modinst:1-164]() handles copying, stripping, signing, and compression of kernel modules. The `depmod.sh` script generates module dependency information for runtime loading.

Sources: scripts/basic/fixdep.c:1-400, scripts/setlocalversion:1-200, scripts/Kbuild.include:1-450, scripts/Makefile.modinst:1-164, scripts/remove-stale-files:1-5030:T37df,# Static Analysis Tools

<details>
<summary>Relevant source files</summary>

The following files were used as context for generating this wiki page:

- [arch/Kconfig](arch/Kconfig)
- [arch/powerpc/Kconfig](arch/powerpc/Kconfig)
- [arch/powerpc/include/asm/asm.h](arch/powerpc/include/asm/asm.h)
- [arch/x86/Kconfig](arch/x86/Kconfig)
- [arch/x86/include/asm/Kbuild](arch/x86/include/asm/Kbuild)
- [arch/x86/include/asm/orc_header.h](arch/x86/include/asm/orc_header.h)
- [arch/x86/include/asm/orc_types.h](arch/x86/include/asm/orc_types.h)
- [arch/x86/include/asm/topology.h](arch/x86/include/asm/topology.h)
- [arch/x86/include/asm/unwind_hints.h](arch/x86/include/asm/unwind_hints.h)
- [arch/x86/kernel/unwind_orc.c](arch/x86/kernel/unwind_orc.c)
- [include/linux/cpu.h](include/linux/cpu.h)
- [include/linux/cpu_smt.h](include/linux/cpu_smt.h)
- [include/linux/cpuhotplug.h](include/linux/cpuhotplug.h)
- [include/linux/hrtimer.h](include/linux/hrtimer.h)
- [include/linux/objtool.h](include/linux/objtool.h)
- [kernel/cpu.c](kernel/cpu.c)
- [kernel/smpboot.c](kernel/smpboot.c)
- [kernel/time/hrtimer.c](kernel/time/hrtimer.c)
- [scripts/Makefile.build](scripts/Makefile.build)
- [scripts/Makefile.lib](scripts/Makefile.lib)
- [scripts/Makefile.vmlinux_o](scripts/Makefile.vmlinux_o)
- [scripts/link-vmlinux.sh](scripts/link-vmlinux.sh)
- [scripts/orc_hash.sh](scripts/orc_hash.sh)
- [tools/arch/x86/include/asm/orc_types.h](tools/arch/x86/include/asm/orc_types.h)
- [tools/objtool/arch/powerpc/Build](tools/objtool/arch/powerpc/Build)
- [tools/objtool/arch/powerpc/decode.c](tools/objtool/arch/powerpc/decode.c)
- [tools/objtool/arch/powerpc/include/arch/cfi_regs.h](tools/objtool/arch/powerpc/include/arch/cfi_regs.h)
- [tools/objtool/arch/powerpc/include/arch/elf.h](tools/objtool/arch/powerpc/include/arch/elf.h)
- [tools/objtool/arch/powerpc/include/arch/special.h](tools/objtool/arch/powerpc/include/arch/special.h)
- [tools/objtool/arch/powerpc/special.c](tools/objtool/arch/powerpc/special.c)
- [tools/objtool/arch/x86/decode.c](tools/objtool/arch/x86/decode.c)
- [tools/objtool/arch/x86/include/arch/elf.h](tools/objtool/arch/x86/include/arch/elf.h)
- [tools/objtool/arch/x86/special.c](tools/objtool/arch/x86/special.c)
- [tools/objtool/builtin-check.c](tools/objtool/builtin-check.c)
- [tools/objtool/check.c](tools/objtool/check.c)
- [tools/objtool/elf.c](tools/objtool/elf.c)
- [tools/objtool/include/objtool/arch.h](tools/objtool/include/objtool/arch.h)
- [tools/objtool/include/objtool/builtin.h](tools/objtool/include/objtool/builtin.h)
- [tools/objtool/include/objtool/cfi.h](tools/objtool/include/objtool/cfi.h)
- [tools/objtool/include/objtool/check.h](tools/objtool/include/objtool/check.h)
- [tools/objtool/include/objtool/elf.h](tools/objtool/include/objtool/elf.h)
- [tools/objtool/include/objtool/endianness.h](tools/objtool/include/objtool/endianness.h)
- [tools/objtool/include/objtool/objtool.h](tools/objtool/include/objtool/objtool.h)
- [tools/objtool/include/objtool/warn.h](tools/objtool/include/objtool/warn.h)
- [tools/objtool/objtool.c](tools/objtool/objtool.c)
- [tools/objtool/orc_dump.c](tools/objtool/orc_dump.c)
- [tools/objtool/orc_gen.c](tools/objtool/orc_gen.c)
- [tools/objtool/special.c](tools/objtool/special.c)

</details>



This document covers the static analysis infrastructure in the WSL2 Linux kernel, primarily focusing on `objtool` - the primary static analysis tool used during kernel compilation. This tool performs instruction-level analysis, control flow integrity checking, and generates metadata for runtime features like stack unwinding.

For information about build system infrastructure, see [Build Infrastructure](#10.1). For kernel boot processes, see [Kernel Boot and Parameters](#9).

## Overview

The static analysis tools in the kernel are primarily centered around `objtool`, a sophisticated binary analysis tool that operates on compiled object files during the build process. The tool performs multiple types of analysis including stack validation, control flow integrity verification, and metadata generation for features like ORC stack unwinding and kernel live patching.

**Primary Tool Architecture**

```mermaid
graph TB
    subgraph "Build Process Integration"
        MAKEFILE["Makefile.build"]
        SCRIPTS["Build Scripts"]
    end
    
    subgraph "Objtool Core"
        MAIN["objtool.c<br/>Main Entry Point"]
        BUILTIN["builtin-check.c<br/>Command Processing"]
        CHECK["check.c<br/>Analysis Engine"]
    end
    
    subgraph "Analysis Components"
        ELF["elf.c<br/>ELF File Processing"]
        ARCH_X86["arch/x86/decode.c<br/>x86 Instruction Decoder"]
        ARCH_PPC["arch/powerpc/decode.c<br/>PowerPC Decoder"]
        ORC["orc_gen.c<br/>ORC Metadata Generation"]
        SPECIAL["special.c<br/>Special Section Handling"]
    end
    
    subgraph "Output Artifacts"
        ORC_DATA[".orc_unwind sections"]
        VALIDATION["Stack Validation Results"]
        CFI_METADATA["CFI Information"]
    end
    
    MAKEFILE --> MAIN
    SCRIPTS --> MAIN
    MAIN --> BUILTIN
    BUILTIN --> CHECK
    CHECK --> ELF
    CHECK --> ARCH_X86
    CHECK --> ARCH_PPC
    CHECK --> ORC
    CHECK --> SPECIAL
    
    CHECK --> ORC_DATA
    CHECK --> VALIDATION
    CHECK --> CFI_METADATA
```

Sources: [tools/objtool/objtool.c:1-50](), [tools/objtool/builtin-check.c:1-100](), [tools/objtool/check.c:1-50]()

## Core Analysis Engine

The heart of the static analysis system is the `check.c` module, which implements a comprehensive instruction analysis engine. This engine performs control flow analysis, stack state tracking, and various validation checks on compiled kernel code.

**Instruction Analysis Pipeline**

```mermaid
graph LR
    subgraph "Input Processing"
        OBJFILE["Object File<br/>(.o files)"]
        ELF_PARSE["elf_open_read()<br/>ELF Parser"]
        DECODE["decode_instructions()<br/>Architecture Decoder"]
    end
    
    subgraph "Analysis Phases"
        CFG["Control Flow Graph<br/>Construction"]
        STACK["Stack State<br/>Validation"]
        SPECIAL_ANALYSIS["Special Section<br/>Analysis"]
        DEAD_END["Dead End<br/>Detection"]
    end
    
    subgraph "Validation Checks"
        UACCESS["User Access<br/>Validation"]
        NOINSTR["No Instrumentation<br/>Checks"]
        RET_VALIDATION["Return Path<br/>Validation"]
    end
    
    OBJFILE --> ELF_PARSE
    ELF_PARSE --> DECODE
    DECODE --> CFG
    CFG --> STACK
    CFG --> SPECIAL_ANALYSIS
    CFG --> DEAD_END
    STACK --> UACCESS
    STACK --> NOINSTR
    STACK --> RET_VALIDATION
```

The analysis engine maintains state for each instruction through the `insn_state` structure, tracking control flow information (`cfi_state`), user access context, and various flags for different validation modes.

Sources: [tools/objtool/check.c:264-290](), [tools/objtool/check.c:368-487](), [tools/objtool/include/objtool/check.h:13-21]()

## Architecture-Specific Instruction Decoding

The static analysis system supports multiple architectures through pluggable instruction decoders. Each architecture provides specific knowledge about instruction formats, calling conventions, and platform-specific validation rules.

**x86 Instruction Analysis**

```mermaid
graph TB
    subgraph "x86 Decoder Components"
        X86_DECODE["arch_decode_instruction()"]
        INSN_DECODE["insn_decode()<br/>x86 Instruction Parser"]
        STACK_OPS["Stack Operation<br/>Detection"]
        CFI_ANALYSIS["CFI State<br/>Tracking"]
    end
    
    subgraph "Instruction Categories"
        JUMPS["Jump Instructions<br/>Static/Dynamic"]
        CALLS["Call Instructions<br/>Direct/Indirect"]
        STACK_INSNS["Stack Operations<br/>push/pop/mov"]
        SPECIAL_INSNS["Special Instructions<br/>ENDBR/UNRET"]
    end
    
    subgraph "Analysis Results"
        STACK_EFFECTS["Stack Effects<br/>CFI Updates"]
        CONTROL_FLOW["Control Flow<br/>Destinations"]
        VALIDATION_FLAGS["Validation Flags<br/>retpoline_safe, etc."]
    end
    
    X86_DECODE --> INSN_DECODE
    INSN_DECODE --> STACK_OPS
    INSN_DECODE --> CFI_ANALYSIS
    
    STACK_INSNS --> STACK_EFFECTS
    JUMPS --> CONTROL_FLOW
    CALLS --> CONTROL_FLOW
    SPECIAL_INSNS --> VALIDATION_FLAGS
    
    STACK_OPS --> STACK_EFFECTS
    CFI_ANALYSIS --> STACK_EFFECTS
```

The x86 decoder in `arch/x86/decode.c` handles complex x86 instruction formats using the kernel's instruction decoder library. It identifies stack-affecting operations, control flow transfers, and special security-related instructions like Intel CET's `ENDBR` instructions.

Sources: [tools/objtool/arch/x86/decode.c:147-200](), [tools/objtool/arch/x86/decode.c:26-42](), [tools/objtool/arch/x86/decode.c:135-145]()

## Build System Integration

The static analysis tools are deeply integrated into the kernel build process through the kbuild system. Object files are automatically processed by objtool during compilation, with different analysis modes enabled based on kernel configuration options.

**Build Process Flow**

```mermaid
graph LR
    subgraph "Source Compilation"
        C_FILES[".c source files"]
        CC_COMPILE["$(CC) compilation"]
        OBJ_FILES[".o object files"]
    end
    
    subgraph "Objtool Processing"
        OBJTOOL_CMD["objtool check"]
        CONFIG_CHECK["Configuration<br/>Analysis Mode Selection"]
        ANALYSIS["Static Analysis<br/>Execution"]
    end
    
    subgraph "Configuration Options"
        STACK_VAL["CONFIG_STACK_VALIDATION"]
        ORC_UNWINDER["CONFIG_UNWINDER_ORC"]
        RETPOLINE["CONFIG_RETPOLINE"]
        IBT["CONFIG_X86_KERNEL_IBT"]
    end
    
    subgraph "Output Processing"
        MODIFIED_OBJ["Modified .o files<br/>with metadata"]
        ORC_SECTIONS[".orc_unwind sections"]
        LINK_VMLINUX["Link into vmlinux"]
    end
    
    C_FILES --> CC_COMPILE
    CC_COMPILE --> OBJ_FILES
    OBJ_FILES --> OBJTOOL_CMD
    
    STACK_VAL --> CONFIG_CHECK
    ORC_UNWINDER --> CONFIG_CHECK
    RETPOLINE --> CONFIG_CHECK
    IBT --> CONFIG_CHECK
    CONFIG_CHECK --> ANALYSIS
    
    OBJTOOL_CMD --> ANALYSIS
    ANALYSIS --> MODIFIED_OBJ
    ANALYSIS --> ORC_SECTIONS
    MODIFIED_OBJ --> LINK_VMLINUX
    ORC_SECTIONS --> LINK_VMLINUX
```

The integration occurs through Makefile rules that conditionally invoke objtool based on configuration options. The `cmd_objtool` variable in the build system determines when and how objtool is executed.

Sources: [scripts/Makefile.build:212-219](), [scripts/Makefile.build:158-161](), [arch/x86/Kconfig:250-253]()

## Analysis Capabilities

The static analysis system provides several key capabilities that enhance kernel security, debugging, and runtime features.

### Stack Validation and CFI Tracking

The tool maintains detailed control flow information (CFI) for each instruction, tracking register states and stack pointer modifications. This enables detection of stack corruption vulnerabilities and ensures proper stack unwinding metadata.

| Analysis Type | Function | Purpose |
|---------------|----------|---------|
| Stack Validation | `validate_branch()` | Ensures consistent stack state across control flow paths |
| CFI State Tracking | `update_cfi_state()` | Maintains register and stack pointer state |
| Dead End Detection | `add_dead_ends()` | Identifies unreachable code paths |
| User Access Validation | `validate_uaccess()` | Checks proper user/kernel space boundaries |

### ORC Unwinder Metadata Generation

The ORC (Oops Rewind Capability) unwinder requires precise metadata about stack frame layouts. Objtool generates this information by analyzing function prologues and stack manipulation instructions.

```mermaid
graph TB
    subgraph "ORC Generation Process"
        INSN_ANALYSIS["Instruction Analysis<br/>Stack Effect Detection"]
        CFI_TRACKING["CFI State Tracking<br/>Per-Instruction State"]
        ORC_CREATION["create_orc_entry()<br/>Metadata Generation"]
        ORC_OUTPUT[".orc_unwind sections<br/>Runtime Metadata"]
    end
    
    subgraph "Runtime Usage"
        KERNEL_OOPS["Kernel Oops/Panic"]
        STACK_UNWIND["Stack Unwinding<br/>orc_unwind_init()"]
        STACK_TRACE["Stack Trace<br/>Generation"]
    end
    
    INSN_ANALYSIS --> CFI_TRACKING
    CFI_TRACKING --> ORC_CREATION
    ORC_CREATION --> ORC_OUTPUT
    
    ORC_OUTPUT --> KERNEL_OOPS
    KERNEL_OOPS --> STACK_UNWIND
    STACK_UNWIND --> STACK_TRACE
```

### Security Feature Validation

Modern security features like Intel CET (Control-flow Enforcement Technology) and retpoline mitigations require static analysis to ensure proper implementation.

The tool validates:
- **ENDBR Instruction Placement**: Ensures indirect branch targets have proper landing pad instructions
- **Retpoline Usage**: Validates that indirect branches use retpoline thunks when enabled
- **User Access Boundaries**: Checks that user space access functions properly manage SMAP/SMEP flags
- **No-Instrumentation Sections**: Ensures critical code sections remain uninstrumented

Sources: [tools/objtool/check.c:188-257](), [tools/objtool/orc_gen.c:1-50](), [tools/objtool/check.c:1055-1255](), [arch/x86/kernel/unwind_orc.c:1-50]()

## Configuration and Usage

The static analysis system is controlled through various kernel configuration options and build-time flags. The analysis behavior can be customized based on the target kernel configuration and security requirements.

**Key Configuration Options**

| Option | Purpose | Analysis Impact |
|--------|---------|-----------------|
| `CONFIG_STACK_VALIDATION` | Enable stack validation | Performs comprehensive stack state analysis |
| `CONFIG_UNWINDER_ORC` | ORC unwinder support | Generates ORC metadata sections |
| `CONFIG_RETPOLINE` | Retpoline mitigation | Validates indirect branch mitigations |
| `CONFIG_X86_KERNEL_IBT` | Intel CET support | Validates ENDBR instruction placement |
| `OBJECT_FILES_NON_STANDARD` | Skip analysis | Excludes specific files from analysis |

The tool's behavior is controlled through the `opts` structure defined in `builtin.h`, which contains flags for different analysis modes, output options, and validation settings.

Sources: [tools/objtool/include/objtool/builtin.h:10-39](), [tools/objtool/builtin-check.c:39-120](), [arch/x86/Kconfig:271-278]()5:["$","$L15",null,{"repoName":"microsoft/WSL2-Linux-Kernel","hasConfig":false,"canSteer":true,"children":["$","$L16",null,{"wiki":{"metadata":{"repo_name":"microsoft/WSL2-Linux-Kernel","commit_hash":"427645e3","generated_at":"2025-08-01T07:04:44.733864","config":null,"config_source":"none"},"pages":[{"page_plan":{"id":"1","title":"WSL2 Linux Kernel Overview"},"content":"$17"},{"page_plan":{"id":"2","title":"Virtualization"},"content":"$18"},{"page_plan":{"id":"2.1","title":"KVM x86 Implementation"},"content":"$19"},{"page_plan":{"id":"2.2","title":"KVM ARM64 Implementation"},"content":"$1a"},{"page_plan":{"id":"2.3","title":"AMD SVM Implementation"},"content":"$1b"},{"page_plan":{"id":"3","title":"Process Management and Scheduling"},"content":"$1c"},{"page_plan":{"id":"3.1","title":"CFS Scheduler"},"content":"$1d"},{"page_plan":{"id":"3.2","title":"Workqueue Subsystem"},"content":"$1e"},{"page_plan":{"id":"4","title":"I/O Subsystems"},"content":"$1f"},{"page_plan":{"id":"4.1","title":"io_uring Framework"},"content":"$20"},{"page_plan":{"id":"4.2","title":"IOMMU Subsystem"},"content":"$21"},{"page_plan":{"id":"4.3","title":"NVMe Storage Driver"},"content":"$22"},{"page_plan":{"id":"5","title":"File Systems"},"content":"$23"},{"page_plan":{"id":"5.1","title":"EXT4 Filesystem"},"content":"$24"},{"page_plan":{"id":"5.2","title":"Btrfs Filesystem"},"content":"$25"},{"page_plan":{"id":"6","title":"Architecture-Specific Implementations"},"content":"$26"},{"page_plan":{"id":"6.1","title":"RISC-V Support"},"content":"$27"},{"page_plan":{"id":"7","title":"Device Drivers"},"content":"$28"},{"page_plan":{"id":"7.1","title":"AMD GPU Display Driver"},"content":"$29"},{"page_plan":{"id":"7.2","title":"HD-Audio Support"},"content":"$2a"},{"page_plan":{"id":"8","title":"Windows Integration Components"},"content":"$2b"},{"page_plan":{"id":"8.1","title":"DXGKRNL Graphics Driver"},"content":"$2c"},{"page_plan":{"id":"9","title":"Kernel Boot and Parameters"},"content":"$2d"},{"page_plan":{"id":"10","title":"Build System"},"content":"$2e"},{"page_plan":{"id":"10.1","title":"Build Infrastructure"},"content":"$2f"},{"page_plan":{"id":"10.2","title":"Static Analysis Tools"},"content":"$30"}]},"children":"$L31"}]}]

