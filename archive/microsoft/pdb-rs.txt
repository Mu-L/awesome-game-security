Project Path: arc_microsoft_pdb-rs_c_nwdxnw

Source Tree:

```txt
arc_microsoft_pdb-rs_c_nwdxnw
├── CODE_OF_CONDUCT.md
├── Cargo.lock
├── Cargo.toml
├── LICENSE
├── README.md
├── SECURITY.md
├── SUPPORT.md
├── codeview
│   ├── Cargo.toml
│   └── src
│       ├── arch
│       │   ├── amd64
│       │   │   └── regs.rs
│       │   ├── amd64.rs
│       │   ├── arm64
│       │   │   └── regs.rs
│       │   ├── arm64.rs
│       │   ├── x86
│       │   │   └── regs.rs
│       │   └── x86.rs
│       ├── arch.rs
│       ├── encoder.rs
│       ├── lib.rs
│       ├── parser
│       │   └── tests.rs
│       ├── parser.rs
│       ├── syms
│       │   ├── builder.rs
│       │   ├── iter.rs
│       │   ├── kind.rs
│       │   └── offset_segment.rs
│       ├── syms.rs
│       ├── types
│       │   ├── fields.rs
│       │   ├── iter.rs
│       │   ├── kind.rs
│       │   ├── number.rs
│       │   ├── primitive.rs
│       │   ├── records.rs
│       │   └── visitor.rs
│       ├── types.rs
│       ├── utils
│       │   └── iter.rs
│       └── utils.rs
├── coff
│   ├── Cargo.toml
│   └── src
│       ├── dll_characteristics.rs
│       ├── image.rs
│       ├── lib.rs
│       ├── machine.rs
│       ├── reloc.rs
│       └── section.rs
├── docs
│   ├── codeview
│   │   ├── calling_convention.md
│   │   ├── codeview.md
│   │   ├── items
│   │   │   ├── items.md
│   │   │   ├── lf_buildinfo.md
│   │   │   ├── lf_func_id.md
│   │   │   ├── lf_mfunc_id.md
│   │   │   ├── lf_string_id.md
│   │   │   ├── lf_substr_list.md
│   │   │   ├── lf_udt_mod_src_line.md
│   │   │   └── lf_udt_src_line.md
│   │   ├── line_data.md
│   │   ├── number.md
│   │   ├── primitive_types.md
│   │   ├── symbols
│   │   │   ├── s_annotation.md
│   │   │   ├── s_armswitchtable.md
│   │   │   ├── s_block.md
│   │   │   ├── s_buildinfo.md
│   │   │   ├── s_coffgroup.md
│   │   │   ├── s_compile.md
│   │   │   ├── s_constant.md
│   │   │   ├── s_data.md
│   │   │   ├── s_end.md
│   │   │   ├── s_frameproc.md
│   │   │   ├── s_inlinesite.md
│   │   │   ├── s_label.md
│   │   │   ├── s_local.md
│   │   │   ├── s_namespace.md
│   │   │   ├── s_objname.md
│   │   │   ├── s_procs.md
│   │   │   ├── s_pub.md
│   │   │   ├── s_refsyms.md
│   │   │   ├── s_section.md
│   │   │   ├── s_thread.md
│   │   │   ├── s_thunk.md
│   │   │   ├── s_trampoline.md
│   │   │   ├── s_udt.md
│   │   │   └── symbols.md
│   │   └── types
│   │       ├── lf_arglist.md
│   │       ├── lf_array.md
│   │       ├── lf_bitfield.md
│   │       ├── lf_class.md
│   │       ├── lf_endprecomp.md
│   │       ├── lf_enum.md
│   │       ├── lf_fieldlist.md
│   │       ├── lf_methodlist.md
│   │       ├── lf_mfunction.md
│   │       ├── lf_modifier.md
│   │       ├── lf_pointer.md
│   │       ├── lf_precomp.md
│   │       ├── lf_procedure.md
│   │       ├── lf_skip.md
│   │       ├── lf_union.md
│   │       ├── lf_vtshape.md
│   │       └── types.md
│   ├── data_types.md
│   ├── determinism.md
│   ├── index.md
│   ├── intro.md
│   ├── pdb
│   │   ├── dbi_fixups.md
│   │   ├── dbi_modules.md
│   │   ├── dbi_opt_debug.md
│   │   ├── dbi_sections.md
│   │   ├── dbi_sources.md
│   │   ├── dbi_stream.md
│   │   ├── globals.md
│   │   ├── hashing.md
│   │   ├── ipi_stream.md
│   │   ├── mini_pdb.md
│   │   ├── module_stream.md
│   │   ├── msf.md
│   │   ├── msfz.md
│   │   ├── names_stream.md
│   │   ├── pdb.md
│   │   ├── pdbi_stream.md
│   │   ├── relationships.md
│   │   └── tpi_stream.md
│   ├── references.md
│   └── terminology.md
├── msf
│   ├── Cargo.toml
│   ├── README.md
│   └── src
│       ├── check.rs
│       ├── commit.rs
│       ├── lib.rs
│       ├── open.rs
│       ├── pages.rs
│       ├── read.rs
│       ├── stream_reader.rs
│       ├── stream_writer.rs
│       ├── tests.rs
│       └── write.rs
├── msfz
│   ├── Cargo.toml
│   └── src
│       ├── compress_utils.rs
│       ├── lib.rs
│       ├── reader.rs
│       ├── stream_data.rs
│       ├── tests.rs
│       └── writer.rs
├── pdb
│   ├── Cargo.toml
│   ├── README.md
│   ├── src
│   │   ├── coff_groups.rs
│   │   ├── container.rs
│   │   ├── dbi
│   │   │   ├── fixups.rs
│   │   │   ├── modules.rs
│   │   │   ├── optional_dbg.rs
│   │   │   ├── section_contrib.rs
│   │   │   ├── section_map.rs
│   │   │   └── sources.rs
│   │   ├── dbi.rs
│   │   ├── embedded_sources.rs
│   │   ├── globals
│   │   │   ├── gsi.rs
│   │   │   ├── gss.rs
│   │   │   ├── name_table
│   │   │   │   └── tests.rs
│   │   │   ├── name_table.rs
│   │   │   ├── psi.rs
│   │   │   └── tests.rs
│   │   ├── globals.rs
│   │   ├── guid.rs
│   │   ├── hash.rs
│   │   ├── lib.rs
│   │   ├── lines
│   │   │   ├── checksum.rs
│   │   │   └── subsection.rs
│   │   ├── lines.rs
│   │   ├── modi.rs
│   │   ├── names
│   │   │   └── tests.rs
│   │   ├── names.rs
│   │   ├── pdbi
│   │   │   └── tests.rs
│   │   ├── pdbi.rs
│   │   ├── stream_index.rs
│   │   ├── taster.rs
│   │   ├── tpi
│   │   │   └── hash.rs
│   │   ├── tpi.rs
│   │   ├── utils
│   │   │   ├── align.rs
│   │   │   ├── io.rs
│   │   │   ├── iter.rs
│   │   │   ├── path.rs
│   │   │   ├── swizzle.rs
│   │   │   └── vec.rs
│   │   ├── utils.rs
│   │   └── writer.rs
│   └── tests
│       ├── cpp_check
│       │   └── types.cpp
│       └── cpp_check.rs
└── pdbtool
    ├── Cargo.toml
    ├── README.md
    ├── src
    │   ├── addsrc.rs
    │   ├── check.rs
    │   ├── compare.rs
    │   ├── container.rs
    │   ├── copy.rs
    │   ├── counts.rs
    │   ├── dump
    │   │   ├── lines.rs
    │   │   ├── names.rs
    │   │   ├── sections.rs
    │   │   ├── sources.rs
    │   │   ├── streams.rs
    │   │   ├── sym.rs
    │   │   └── types.rs
    │   ├── dump.rs
    │   ├── dump_utils.rs
    │   ├── find.rs
    │   ├── glob_pdbs.rs
    │   ├── hexdump.rs
    │   ├── main.rs
    │   ├── pdz
    │   │   └── encode.rs
    │   ├── pdz.rs
    │   ├── save.rs
    │   └── util.rs
    └── tests
        └── encode.rs

```

`CODE_OF_CONDUCT.md`:

```md
# Microsoft Open Source Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).

Resources:

- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)
- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns

```

`Cargo.lock`:

```lock
# This file is automatically @generated by Cargo.
# It is not intended for manual editing.
version = 4

[[package]]
name = "adler2"
version = "2.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "512761e0bb2578dd7380c6baaa0f4ce03e84f95e960231d1dec8bf4d7d6e2627"

[[package]]
name = "aho-corasick"
version = "1.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8e60d3430d3a69478ad0993f19238d2df97c507009a52b3c10addcd7f6bcb916"
dependencies = [
 "memchr",
]

[[package]]
name = "anstream"
version = "0.6.18"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8acc5369981196006228e28809f761875c0327210a891e941f4c683b3a99529b"
dependencies = [
 "anstyle",
 "anstyle-parse",
 "anstyle-query",
 "anstyle-wincon",
 "colorchoice",
 "is_terminal_polyfill",
 "utf8parse",
]

[[package]]
name = "anstyle"
version = "1.0.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "55cc3b69f167a1ef2e161439aa98aed94e6028e5f9a59be9a6ffb47aef1651f9"

[[package]]
name = "anstyle-parse"
version = "0.2.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3b2d16507662817a6a20a9ea92df6652ee4f94f914589377d69f3b21bc5798a9"
dependencies = [
 "utf8parse",
]

[[package]]
name = "anstyle-query"
version = "1.1.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "79947af37f4177cfead1110013d678905c37501914fba0efea834c3fe9a8d60c"
dependencies = [
 "windows-sys",
]

[[package]]
name = "anstyle-wincon"
version = "3.0.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ca3534e77181a9cc07539ad51f2141fe32f6c3ffd4df76db8ad92346b003ae4e"
dependencies = [
 "anstyle",
 "once_cell",
 "windows-sys",
]

[[package]]
name = "anyhow"
version = "1.0.95"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "34ac096ce696dc2fcabef30516bb13c0a68a11d30131d3df6f04711467681b04"

[[package]]
name = "autocfg"
version = "1.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ace50bade8e6234aa140d9a2f552bbee1db4d353f69b8217bc503490fc1a9f26"

[[package]]
name = "bitfield"
version = "0.14.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2d7e60934ceec538daadb9d8432424ed043a904d8e0243f3c6446bce549a46ac"

[[package]]
name = "bitflags"
version = "1.3.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bef38d45163c2f1dde094a7dfd33ccf595c92905c8f8f4fdc18d06fb1037718a"

[[package]]
name = "bitflags"
version = "2.8.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8f68f53c83ab957f72c32642f3868eec03eb974d1fb82e453128456482613d36"

[[package]]
name = "bitvec"
version = "1.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1bc2832c24239b0141d5674bb9174f9d68a8b5b3f2753311927c172ca46f7e9c"
dependencies = [
 "funty",
 "radium",
 "tap",
 "wyz",
]

[[package]]
name = "bstr"
version = "1.11.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "531a9155a481e2ee699d4f98f43c0ca4ff8ee1bfd55c31e9e98fb29d2b176fe0"
dependencies = [
 "memchr",
 "regex-automata",
 "serde",
]

[[package]]
name = "bumpalo"
version = "3.17.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1628fb46dfa0b37568d12e5edd512553eccf6a22a78e8bde00bb4aed84d5bdbf"

[[package]]
name = "byteorder"
version = "1.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1fd0f2584146f6f2ef48085050886acf353beff7305ebd1ae69500e27c67f64b"

[[package]]
name = "cc"
version = "1.2.11"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e4730490333d58093109dc02c23174c3f4d490998c3fed3cc8e82d57afedb9cf"
dependencies = [
 "jobserver",
 "libc",
 "shlex",
]

[[package]]
name = "cfg-if"
version = "1.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "baf1de4339761588bc0619e3cbc0120ee582ebb74b53b4efbf79117bd2da40fd"

[[package]]
name = "cfg_aliases"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fd16c4719339c4530435d38e511904438d07cce7950afa3718a84ac36c10e89e"

[[package]]
name = "cfg_aliases"
version = "0.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "613afe47fcd5fac7ccf1db93babcb082c5994d996f20b8b159f2ad1658eb5724"

[[package]]
name = "clap"
version = "4.5.27"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "769b0145982b4b48713e01ec42d61614425f27b7058bda7180a3a41f30104796"
dependencies = [
 "clap_builder",
 "clap_derive",
]

[[package]]
name = "clap_builder"
version = "4.5.27"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1b26884eb4b57140e4d2d93652abfa49498b938b3c9179f9fc487b0acc3edad7"
dependencies = [
 "anstream",
 "anstyle",
 "clap_lex",
 "strsim",
]

[[package]]
name = "clap_derive"
version = "4.5.24"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "54b755194d6389280185988721fffba69495eed5ee9feeee9a599b53db80318c"
dependencies = [
 "heck",
 "proc-macro2",
 "quote",
 "syn 2.0.96",
]

[[package]]
name = "clap_lex"
version = "0.7.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f46ad14479a25103f283c0f10005961cf086d8dc42205bb44c46ac563475dca6"

[[package]]
name = "colorchoice"
version = "1.0.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5b63caa9aa9397e2d9480a9b13673856c78d8ac123288526c37d7839f2a86990"

[[package]]
name = "crc32fast"
version = "1.4.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a97769d94ddab943e4510d138150169a2758b5ef3eb191a9ee688de3e23ef7b3"
dependencies = [
 "cfg-if",
]

[[package]]
name = "dbg-ranges"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2311ff68817d43b944d0d2c424f68d4be1573662aa53479ddcb4cc163507b73c"

[[package]]
name = "flate2"
version = "1.0.35"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c936bfdafb507ebbf50b8074c54fa31c5be9a1e7e5f467dd659697041407d07c"
dependencies = [
 "crc32fast",
 "miniz_oxide",
]

[[package]]
name = "friendly"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dab7f6883e5b8794393b23def04af02a7c056210d0879f43367e8b186783fc12"
dependencies = [
 "num-traits",
]

[[package]]
name = "funty"
version = "2.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e6d5a32815ae3f33302d95fdcb2ce17862f8c65363dcfd29360480ba1001fc9c"

[[package]]
name = "generator"
version = "0.8.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "cc6bd114ceda131d3b1d665eba35788690ad37f5916457286b32ab6fd3c438dd"
dependencies = [
 "cfg-if",
 "libc",
 "log",
 "rustversion",
 "windows",
]

[[package]]
name = "glob"
version = "0.3.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a8d1add55171497b4705a648c6b583acafb01d58050a51727785f0b2c8e0a2b2"

[[package]]
name = "heck"
version = "0.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2304e00983f87ffb38b55b444b5e3b60a884b5d30c0fca7d82fe33449bbe55ea"

[[package]]
name = "is_terminal_polyfill"
version = "1.70.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7943c866cc5cd64cbc25b2e01621d07fa8eb2a1a23160ee81ce38704e97b8ecf"

[[package]]
name = "jobserver"
version = "0.1.32"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "48d1dbcbbeb6a7fec7e059840aa538bd62aaccf972c7346c4d9d2059312853d0"
dependencies = [
 "libc",
]

[[package]]
name = "lazy_static"
version = "1.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bbd2bcb4c963f2ddae06a2efc7e9f3591312473c50c6685e1f298068316e66fe"

[[package]]
name = "libc"
version = "0.2.169"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b5aba8db14291edd000dfcc4d620c7ebfb122c613afb886ca8803fa4e128a20a"

[[package]]
name = "lock_api"
version = "0.4.14"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "224399e74b87b5f3557511d98dff8b14089b3dadafcab6bb93eab67d3aace965"
dependencies = [
 "scopeguard",
]

[[package]]
name = "log"
version = "0.4.25"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "04cbf5b083de1c7e0222a7a51dbfdba1cbe1c6ab0b15e29fff3f6c077fd9cd9f"

[[package]]
name = "loom"
version = "0.7.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "419e0dc8046cb947daa77eb95ae174acfbddb7673b4151f56d1eed8e93fbfaca"
dependencies = [
 "cfg-if",
 "generator",
 "scoped-tls",
 "tracing",
 "tracing-subscriber",
]

[[package]]
name = "matchers"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d1525a2a28c7f4fa0fc98bb91ae755d1e2d1505079e05539e35bc876b5d65ae9"
dependencies = [
 "regex-automata",
]

[[package]]
name = "memchr"
version = "2.7.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "78ca9ab1a0babb1e7d5695e3530886289c18cf2f87ec19a575a0abdce112e3a3"

[[package]]
name = "miniz_oxide"
version = "0.8.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b8402cab7aefae129c6977bb0ff1b8fd9a04eb5b51efc50a70bea51cda0c7924"
dependencies = [
 "adler2",
]

[[package]]
name = "ms-codeview"
version = "0.1.6"
dependencies = [
 "anyhow",
 "bitfield",
 "bitflags 2.8.0",
 "bstr",
 "ms-coff",
 "pretty-hex",
 "tracing",
 "uuid",
 "zerocopy 0.8.14",
 "zerocopy-derive 0.8.14",
]

[[package]]
name = "ms-coff"
version = "0.1.0"
dependencies = [
 "bitflags 2.8.0",
 "bstr",
 "static_assertions",
 "zerocopy 0.8.14",
 "zerocopy-derive 0.8.14",
]

[[package]]
name = "ms-pdb"
version = "0.1.20"
dependencies = [
 "anyhow",
 "bitfield",
 "bitflags 2.8.0",
 "bitvec",
 "bstr",
 "ms-codeview",
 "ms-coff",
 "ms-pdb-msf",
 "ms-pdb-msfz",
 "pow2",
 "pretty-hex",
 "static_assertions",
 "static_init",
 "sync_file",
 "tracing",
 "tracing-subscriber",
 "uuid",
 "zerocopy 0.8.14",
 "zerocopy-derive 0.8.14",
]

[[package]]
name = "ms-pdb-msf"
version = "0.1.7"
dependencies = [
 "anyhow",
 "bitvec",
 "dbg-ranges",
 "pow2",
 "pretty-hex",
 "static_assertions",
 "static_init",
 "sync_file",
 "tracing",
 "tracing-subscriber",
 "uuid",
 "zerocopy 0.8.14",
 "zerocopy-derive 0.8.14",
]

[[package]]
name = "ms-pdb-msfz"
version = "0.1.10"
dependencies = [
 "anyhow",
 "bstr",
 "flate2",
 "pow2",
 "pretty-hex",
 "static_assertions",
 "static_init",
 "sync_file",
 "tracing",
 "tracing-subscriber",
 "tracing-tracy",
 "tracy-client",
 "zerocopy 0.8.14",
 "zerocopy-derive 0.8.14",
 "zstd",
]

[[package]]
name = "nu-ansi-term"
version = "0.50.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7957b9740744892f114936ab4a57b3f487491bbeafaf8083688b16841a4240e5"
dependencies = [
 "windows-sys",
]

[[package]]
name = "num-traits"
version = "0.2.19"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "071dfc062690e90b734c0b2273ce72ad0ffa95f0c74596bc250dcfd960262841"
dependencies = [
 "autocfg",
]

[[package]]
name = "once_cell"
version = "1.20.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1261fe7e33c73b354eab43b1273a57c8f967d0391e80353e51f764ac02cf6775"

[[package]]
name = "parking_lot"
version = "0.12.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "93857453250e3077bd71ff98b6a65ea6621a19bb0f559a85248955ac12c45a1a"
dependencies = [
 "lock_api",
 "parking_lot_core",
]

[[package]]
name = "parking_lot_core"
version = "0.9.12"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2621685985a2ebf1c516881c026032ac7deafcda1a2c9b7850dc81e3dfcb64c1"
dependencies = [
 "cfg-if",
 "libc",
 "redox_syscall",
 "smallvec",
 "windows-link",
]

[[package]]
name = "pdbtool"
version = "0.1.20"
dependencies = [
 "anyhow",
 "bitvec",
 "bstr",
 "bumpalo",
 "clap",
 "dbg-ranges",
 "friendly",
 "glob",
 "ms-coff",
 "ms-pdb",
 "regex",
 "static_init",
 "tracing",
 "tracing-subscriber",
 "tracing-tracy",
 "zerocopy 0.8.14",
 "zerocopy-derive 0.8.14",
 "zstd",
]

[[package]]
name = "pin-project-lite"
version = "0.2.16"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3b3cff922bd51709b605d9ead9aa71031d81447142d828eb4a6eba76fe619f9b"

[[package]]
name = "pkg-config"
version = "0.3.31"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "953ec861398dccce10c670dfeaf3ec4911ca479e9c02154b3a215178c5f566f2"

[[package]]
name = "pow2"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9ae2effd3c9b3ac6ee244ab9add985d6e1be66c318bad1fac682666f42578d88"
dependencies = [
 "zerocopy 0.7.35",
 "zerocopy-derive 0.7.35",
]

[[package]]
name = "pretty-hex"
version = "0.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bbc83ee4a840062f368f9096d80077a9841ec117e17e7f700df81958f1451254"

[[package]]
name = "proc-macro2"
version = "1.0.93"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "60946a68e5f9d28b0dc1c21bb8a97ee7d018a8b322fa57838ba31cc878e22d99"
dependencies = [
 "unicode-ident",
]

[[package]]
name = "quote"
version = "1.0.38"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0e4dccaaaf89514f546c693ddc140f729f958c247918a13380cccc6078391acc"
dependencies = [
 "proc-macro2",
]

[[package]]
name = "radium"
version = "0.7.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dc33ff2d4973d518d823d61aa239014831e521c75da58e3df4840d3f47749d09"

[[package]]
name = "redox_syscall"
version = "0.5.18"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ed2bf2547551a7053d6fdfafda3f938979645c44812fbfcda098faae3f1a362d"
dependencies = [
 "bitflags 2.8.0",
]

[[package]]
name = "regex"
version = "1.11.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b544ef1b4eac5dc2db33ea63606ae9ffcfac26c1416a2806ae0bf5f56b201191"
dependencies = [
 "aho-corasick",
 "memchr",
 "regex-automata",
 "regex-syntax",
]

[[package]]
name = "regex-automata"
version = "0.4.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "809e8dc61f6de73b46c85f4c96486310fe304c434cfa43669d7b40f711150908"
dependencies = [
 "aho-corasick",
 "memchr",
 "regex-syntax",
]

[[package]]
name = "regex-syntax"
version = "0.8.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2b15c43186be67a4fd63bee50d0303afffcef381492ebe2c5d87f324e1b8815c"

[[package]]
name = "rustversion"
version = "1.0.19"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f7c45b9784283f1b2e7fb61b42047c2fd678ef0960d4f6f1eba131594cc369d4"

[[package]]
name = "scoped-tls"
version = "1.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e1cf6437eb19a8f4a6cc0f7dca544973b0b78843adbfeb3683d1a94a0024a294"

[[package]]
name = "scopeguard"
version = "1.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "94143f37725109f92c262ed2cf5e59bce7498c01bcc1502d7b9afe439a4e9f49"

[[package]]
name = "serde"
version = "1.0.217"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "02fc4265df13d6fa1d00ecff087228cc0a2b5f3c0e87e258d8b94a156e984c70"
dependencies = [
 "serde_derive",
]

[[package]]
name = "serde_derive"
version = "1.0.217"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5a9bf7cf98d04a2b28aead066b7496853d4779c9cc183c440dbac457641e19a0"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.96",
]

[[package]]
name = "sharded-slab"
version = "0.1.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f40ca3c46823713e0d4209592e8d6e826aa57e928f09752619fc696c499637f6"
dependencies = [
 "lazy_static",
]

[[package]]
name = "shlex"
version = "1.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0fda2ff0d084019ba4d7c6f371c95d8fd75ce3524c3cb8fb653a3023f6323e64"

[[package]]
name = "smallvec"
version = "1.13.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3c5e1a9a646d36c3599cd173a41282daf47c44583ad367b8e6837255952e5c67"

[[package]]
name = "static_assertions"
version = "1.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a2eb9349b6444b326872e140eb1cf5e7c522154d69e7a0ffb0fb81c06b37543f"

[[package]]
name = "static_init"
version = "1.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8bae1df58c5fea7502e8e352ec26b5579f6178e1fdb311e088580c980dee25ed"
dependencies = [
 "bitflags 1.3.2",
 "cfg_aliases 0.2.1",
 "libc",
 "parking_lot",
 "parking_lot_core",
 "static_init_macro",
 "winapi",
]

[[package]]
name = "static_init_macro"
version = "1.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1389c88ddd739ec6d3f8f83343764a0e944cd23cfbf126a9796a714b0b6edd6f"
dependencies = [
 "cfg_aliases 0.1.1",
 "memchr",
 "proc-macro2",
 "quote",
 "syn 1.0.109",
]

[[package]]
name = "strsim"
version = "0.11.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7da8b5736845d9f2fcb837ea5d9e2628564b3b043a70948a3f0b778838c5fb4f"

[[package]]
name = "syn"
version = "1.0.109"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "72b64191b275b66ffe2469e8af2c1cfe3bafa67b529ead792a6d0160888b4237"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "syn"
version = "2.0.96"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d5d0adab1ae378d7f53bdebc67a39f1f151407ef230f0ce2883572f5d8985c80"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "sync_file"
version = "0.2.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9cfc24238213e42ffb35314aad7a7f5d46649c5fbba3ea95ef24f7debb95874e"
dependencies = [
 "wasi",
]

[[package]]
name = "tap"
version = "1.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "55937e1799185b12863d447f42597ed69d9928686b8d88a1df17376a097d8369"

[[package]]
name = "thread_local"
version = "1.1.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8b9ef9bad013ada3808854ceac7b46812a6465ba368859a37e2100283d2d719c"
dependencies = [
 "cfg-if",
 "once_cell",
]

[[package]]
name = "tracing"
version = "0.1.41"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "784e0ac535deb450455cbfa28a6f0df145ea1bb7ae51b821cf5e7927fdcfbdd0"
dependencies = [
 "pin-project-lite",
 "tracing-attributes",
 "tracing-core",
]

[[package]]
name = "tracing-attributes"
version = "0.1.28"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "395ae124c09f9e6918a2310af6038fba074bcf474ac352496d5910dd59a2226d"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.96",
]

[[package]]
name = "tracing-core"
version = "0.1.33"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e672c95779cf947c5311f83787af4fa8fffd12fb27e4993211a84bdfd9610f9c"
dependencies = [
 "once_cell",
 "valuable",
]

[[package]]
name = "tracing-log"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ee855f1f400bd0e5c02d150ae5de3840039a3f54b025156404e34c23c03f47c3"
dependencies = [
 "log",
 "once_cell",
 "tracing-core",
]

[[package]]
name = "tracing-subscriber"
version = "0.3.20"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2054a14f5307d601f88daf0553e1cbf472acc4f2c51afab632431cdcd72124d5"
dependencies = [
 "matchers",
 "nu-ansi-term",
 "once_cell",
 "regex-automata",
 "sharded-slab",
 "smallvec",
 "thread_local",
 "tracing",
 "tracing-core",
 "tracing-log",
]

[[package]]
name = "tracing-tracy"
version = "0.11.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0eaa1852afa96e0fe9e44caa53dc0bd2d9d05e0f2611ce09f97f8677af56e4ba"
dependencies = [
 "tracing-core",
 "tracing-subscriber",
 "tracy-client",
]

[[package]]
name = "tracy-client"
version = "0.18.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d90a2c01305b02b76fdd89ac8608bae27e173c829a35f7d76a345ab5d33836db"
dependencies = [
 "loom",
 "once_cell",
 "tracy-client-sys",
]

[[package]]
name = "tracy-client-sys"
version = "0.24.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "69fff37da548239c3bf9e64a12193d261e8b22b660991c6fd2df057c168f435f"
dependencies = [
 "cc",
 "windows-targets",
]

[[package]]
name = "unicode-ident"
version = "1.0.16"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a210d160f08b701c8721ba1c726c11662f877ea6b7094007e1ca9a1041945034"

[[package]]
name = "utf8parse"
version = "0.2.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "06abde3611657adf66d383f00b093d7faecc7fa57071cce2578660c9f1010821"

[[package]]
name = "uuid"
version = "1.12.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b3758f5e68192bb96cc8f9b7e2c2cfdabb435499a28499a42f8f984092adad4b"

[[package]]
name = "valuable"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ba73ea9cf16a25df0c8caa16c51acb937d5712a8429db78a3ee29d5dcacd3a65"

[[package]]
name = "wasi"
version = "0.11.0+wasi-snapshot-preview1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9c8d87e72b64a3b4db28d11ce29237c246188f4f51057d65a7eab63b7987e423"

[[package]]
name = "winapi"
version = "0.3.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5c839a674fcd7a98952e593242ea400abe93992746761e38641405d28b00f419"
dependencies = [
 "winapi-i686-pc-windows-gnu",
 "winapi-x86_64-pc-windows-gnu",
]

[[package]]
name = "winapi-i686-pc-windows-gnu"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ac3b87c63620426dd9b991e5ce0329eff545bccbbb34f3be09ff6fb6ab51b7b6"

[[package]]
name = "winapi-x86_64-pc-windows-gnu"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "712e227841d057c1ee1cd2fb22fa7e5a5461ae8e48fa2ca79ec42cfc1931183f"

[[package]]
name = "windows"
version = "0.58.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dd04d41d93c4992d421894c18c8b43496aa748dd4c081bac0dc93eb0489272b6"
dependencies = [
 "windows-core",
 "windows-targets",
]

[[package]]
name = "windows-core"
version = "0.58.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6ba6d44ec8c2591c134257ce647b7ea6b20335bf6379a27dac5f1641fcf59f99"
dependencies = [
 "windows-implement",
 "windows-interface",
 "windows-result",
 "windows-strings",
 "windows-targets",
]

[[package]]
name = "windows-implement"
version = "0.58.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2bbd5b46c938e506ecbce286b6628a02171d56153ba733b6c741fc627ec9579b"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.96",
]

[[package]]
name = "windows-interface"
version = "0.58.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "053c4c462dc91d3b1504c6fe5a726dd15e216ba718e84a0e46a88fbe5ded3515"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.96",
]

[[package]]
name = "windows-link"
version = "0.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f0805222e57f7521d6a62e36fa9163bc891acd422f971defe97d64e70d0a4fe5"

[[package]]
name = "windows-result"
version = "0.2.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1d1043d8214f791817bab27572aaa8af63732e11bf84aa21a45a78d6c317ae0e"
dependencies = [
 "windows-targets",
]

[[package]]
name = "windows-strings"
version = "0.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4cd9b125c486025df0eabcb585e62173c6c9eddcec5d117d3b6e8c30e2ee4d10"
dependencies = [
 "windows-result",
 "windows-targets",
]

[[package]]
name = "windows-sys"
version = "0.59.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1e38bc4d79ed67fd075bcc251a1c39b32a1776bbe92e5bef1f0bf1f8c531853b"
dependencies = [
 "windows-targets",
]

[[package]]
name = "windows-targets"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9b724f72796e036ab90c1021d4780d4d3d648aca59e491e6b98e725b84e99973"
dependencies = [
 "windows_aarch64_gnullvm",
 "windows_aarch64_msvc",
 "windows_i686_gnu",
 "windows_i686_gnullvm",
 "windows_i686_msvc",
 "windows_x86_64_gnu",
 "windows_x86_64_gnullvm",
 "windows_x86_64_msvc",
]

[[package]]
name = "windows_aarch64_gnullvm"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "32a4622180e7a0ec044bb555404c800bc9fd9ec262ec147edd5989ccd0c02cd3"

[[package]]
name = "windows_aarch64_msvc"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "09ec2a7bb152e2252b53fa7803150007879548bc709c039df7627cabbd05d469"

[[package]]
name = "windows_i686_gnu"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "8e9b5ad5ab802e97eb8e295ac6720e509ee4c243f69d781394014ebfe8bbfa0b"

[[package]]
name = "windows_i686_gnullvm"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "0eee52d38c090b3caa76c563b86c3a4bd71ef1a819287c19d586d7334ae8ed66"

[[package]]
name = "windows_i686_msvc"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "240948bc05c5e7c6dabba28bf89d89ffce3e303022809e73deaefe4f6ec56c66"

[[package]]
name = "windows_x86_64_gnu"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "147a5c80aabfbf0c7d901cb5895d1de30ef2907eb21fbbab29ca94c5b08b1a78"

[[package]]
name = "windows_x86_64_gnullvm"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "24d5b23dc417412679681396f2b49f3de8c1473deb516bd34410872eff51ed0d"

[[package]]
name = "windows_x86_64_msvc"
version = "0.52.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "589f6da84c646204747d1270a2a5661ea66ed1cced2631d546fdfb155959f9ec"

[[package]]
name = "wyz"
version = "0.5.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "05f360fc0b24296329c78fda852a1e9ae82de9cf7b27dae4b7f62f118f77b9ed"
dependencies = [
 "tap",
]

[[package]]
name = "zerocopy"
version = "0.7.35"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1b9b4fd18abc82b8136838da5d50bae7bdea537c574d8dc1a34ed098d6c166f0"
dependencies = [
 "byteorder",
 "zerocopy-derive 0.7.35",
]

[[package]]
name = "zerocopy"
version = "0.8.14"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a367f292d93d4eab890745e75a778da40909cab4d6ff8173693812f79c4a2468"
dependencies = [
 "zerocopy-derive 0.8.14",
]

[[package]]
name = "zerocopy-derive"
version = "0.7.35"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fa4f8080344d4671fb4e831a13ad1e68092748387dfc4f55e356242fae12ce3e"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.96",
]

[[package]]
name = "zerocopy-derive"
version = "0.8.14"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d3931cb58c62c13adec22e38686b559c86a30565e16ad6e8510a337cedc611e1"
dependencies = [
 "proc-macro2",
 "quote",
 "syn 2.0.96",
]

[[package]]
name = "zstd"
version = "0.13.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "fcf2b778a664581e31e389454a7072dab1647606d44f7feea22cd5abb9c9f3f9"
dependencies = [
 "zstd-safe",
]

[[package]]
name = "zstd-safe"
version = "7.2.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "54a3ab4db68cea366acc5c897c7b4d4d1b8994a9cd6e6f841f8964566a419059"
dependencies = [
 "zstd-sys",
]

[[package]]
name = "zstd-sys"
version = "2.0.13+zstd.1.5.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "38ff0f21cfee8f97d94cef41359e0c89aa6113028ab0291aa8ca0038995a95aa"
dependencies = [
 "cc",
 "pkg-config",
]

```

`Cargo.toml`:

```toml
[workspace]
resolver = "2"

members = [
    "codeview",
    "coff",
    "msf",
    "msfz",
    "pdb",
    "pdbtool",
]

[workspace.dependencies]
anyhow = "1.0.71"
bitfield = "0.14.0"
bitflags = "2.3.2"
bitvec = "1"
bstr = "1.8.0"
bumpalo = "3.13.0"
cc = "1.0.79"
clap = "4.5.27"
dbg-ranges = "0.1.0"
flate2 = "1.0.27"
pow2 = "0.1.1"
pretty-hex = "0.4.1"
static_assertions = "1.0"
static_init = "1.0.3"
sync_file = "0.2.6"
tracing = "0.1.41"
tracing-subscriber = "0.3.20"
tracing-tracy = "0.11.4"
uuid = "1.4.0"
zerocopy = "0.8.14"
zerocopy-derive = "0.8.14"
zstd = "0.13.2"

mspdb = { path = "mspdb" }

[profile.release]
debug = 2

# This compiles external packages with optimizations. Compression libraries
# especially sensitive to optimization. This makes it a lot easier to do
# development and run tests with local code in debug mode but external dependencies
# fully optimized.
[profile.dev.package."*"]
opt-level = 2

[profile.coverage]
inherits = "dev"

```

`LICENSE`:

```
    MIT License

    Copyright (c) Microsoft Corporation.

    Permission is hereby granted, free of charge, to any person obtaining a copy
    of this software and associated documentation files (the "Software"), to deal
    in the Software without restriction, including without limitation the rights
    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
    copies of the Software, and to permit persons to whom the Software is
    furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in all
    copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
    SOFTWARE

```

`README.md`:

```md
# PDB tools

This repository contains libraries and tools for working with Microsoft Program
Database (PDB) files. All of the code is in Rust.

* The `ms-pdb-msf` crate contains code for reading, creating, and modifying PDB
  files that use the MSF container format. Currently, all PDBs produced by
  Microsoft tools use the MSF container format.
  
  This is a lower-level building block for PDBs, and most developers will never
  need to directly use the `ms-pdb-msf` crate. Instead, they should use the
  `ms-pdb` crate.

* The `ms-pdb-msfz` crate contains code for reading and writing PDB files that
  use the MSFZ container format. MSFZ is a new container format that is
  optimized for "cold storage"; PDB/MSFZ files cannot be modified in-place in
  the way PDB/MSF files can, but MSFZ files use an efficient form of compression
  that allows data to be accessed without decompressing the entire file. MSFZ is
  intended to be a format for storing PDBs, not for local development.

  Most developers will not need to use the `ms-pdb-msfz` crate directly.
  Instead, they should use the `ms-pdb` crate.

* The `ms-pdb` crate supports reading, creating, and modifying PDB files. It
  builds on the `ms-pdb-msf` and `ms-pdb-msfz` crate. The `ms-pdb-msf` and
  `ms-pdb-msfz` crates provide the container format for PDB, but they do not
  contain any code for working with the contents of PDBs. That is the job of the
  `ms-pdb` crate -- it provides methods for reading specific PDB data
  structures, such as debug symbols, line mappings, module records, etc.

* The `pdbtool` crate is a standalone command-line tool for reading PDB files
  and for doing some minor edits to PDB files. It also supports transcoding
  to PDZ (MSFZ).  It can be installed directly from Cargo:

```bash
cargo install pdbtool
```

## PDB and CodeView Documentation

[PDB and CodeView Documentation](./docs/index.md) is a synthesis of public
resources on PDB and CodeView descriptions. It describes the structure of PDB
files and many CodeView data structures with enough detail to implement some
useful tools.

## All information in this implementation is based on publicly-available information

With the exception of MSFZ, this implementation is based solely on public
sources that describe the PDB and MSF data structures. This repository does not
contain any confidential Microsoft intellectual property.

## MSFZ describes an **experimental** data format and is subject to change without notice

The MSFZ specification in this repository describes an experimental container
format for PDBs. **Microsoft makes no commitment to the stability or support for
MSFZ.** The MSFZ format may be changed or discontinued at any time, without any
notice or obligation for Microsoft to take any action. At some future point,
Microsoft _may_ make a support commitment to MSFZ (possibly with incompatible
changes made to it), but this repository does not imply any commitment or
agreement to do so.

## **THIS IMPLEMENTATION IS NOT AUTHORITATIVE AND IS NOT A REFERENCE IMPLEMENTATION**

This implementation is **NOT** an authoritative reference. It may contain
defects or inaccuracies. As the `LICENSE` states, this implementation is
provided "as is", without warranty of any kind. Specifically, this
implementation **DOES NOT** make any guarantees about compatibility or
interoperability with any other toolset, including (but not limited to)
Microsoft Visual C++ (MSVC) and Clang.

The authors of this effort may make a good-faith effort to fix bugs, including
bugs discovered by the authors or the community. However, as the license states,
this repository is provided "as is" and there is absolutely no obligation or
expectation on levels of service for the implementation provided in this
repository.

## Contributing

This project welcomes contributions and suggestions. Most contributions require
you to agree to a Contributor License Agreement (CLA) declaring that you have
the right to, and actually do, grant us the rights to use your contribution. For
details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether
you need to provide a CLA and decorate the PR appropriately (e.g., status check,
comment). Simply follow the instructions provided by the bot. You will only need
to do this once across all repos using our CLA.

This project has adopted the
[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the
[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any
additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or
services. Authorized use of Microsoft trademarks or logos is subject to and must
follow
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must
not cause confusion or imply Microsoft sponsorship. Any use of third-party
trademarks or logos are subject to those third-party's policies.

## Repository

* <https://github.com/microsoft/pdb-rs>

## Contacts

* `sivadeilra` on GitHub
* Arlie Davis ardavis@microsoft.com

```

`SECURITY.md`:

```md
<!-- BEGIN MICROSOFT SECURITY.MD V0.0.9 BLOCK -->

## Security

Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/Microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet) and [Xamarin](https://github.com/xamarin).

If you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://aka.ms/security.md/definition), please report it to us as described below.

## Reporting Security Issues

**Please do not report security vulnerabilities through public GitHub issues.**

Instead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://aka.ms/security.md/msrc/create-report).

If you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://aka.ms/security.md/msrc/pgp).

You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://www.microsoft.com/msrc). 

Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:

  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)
  * Full paths of source file(s) related to the manifestation of the issue
  * The location of the affected source code (tag/branch/commit or direct URL)
  * Any special configuration required to reproduce the issue
  * Step-by-step instructions to reproduce the issue
  * Proof-of-concept or exploit code (if possible)
  * Impact of the issue, including how an attacker might exploit the issue

This information will help us triage your report more quickly.

If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://aka.ms/security.md/msrc/bounty) page for more details about our active programs.

## Preferred Languages

We prefer all communications to be in English.

## Policy

Microsoft follows the principle of [Coordinated Vulnerability Disclosure](https://aka.ms/security.md/cvd).

<!-- END MICROSOFT SECURITY.MD BLOCK -->

```

`SUPPORT.md`:

```md
# Unsupported open source

This repository contains open-source code. It is shared freely, but without any support
obligation whatsoever from Microsoft. As the `LICENSE` file specifies, Microsoft does not
make any claims about the suitability, reliability, or accuracy of the code in this repository.

```

`codeview/Cargo.toml`:

```toml
[package]
name = "ms-codeview"
version = "0.1.6"
edition = "2024"
description = "Definitions for use with CodeView debugging symbols"
authors = ["Arlie Davis <ardavis@microsoft.com>"]
license = "MIT OR Apache-2.0"
repository = "https://github.com/microsoft/pdb-rs/"
categories = ["parsing", "development-tools", "development-tools::debugging"]

[dependencies]
anyhow.workspace = true
bitfield.workspace = true
bitflags.workspace = true
bstr.workspace = true
tracing.workspace = true
pretty-hex.workspace = true
uuid.workspace = true
zerocopy = { workspace = true, features = ["alloc", "derive"] }
zerocopy-derive.workspace = true

[dependencies.ms-coff]
version = "0.1.0"
path = "../coff"

```

`codeview/src/arch.rs`:

```rs
//! Architecture-specific definitions

macro_rules! register_set {
    (
        $( #[$a:meta] )*
        $v:vis enum $ty_name:ident;
        $( $reg_name:ident = $reg_value:expr, )*
    ) => {
        $( #[$a] )*
        #[allow(missing_docs)]
        #[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
        $v struct $ty_name(pub u16);

        #[allow(missing_docs)]
        impl $ty_name {
            $(
                pub const $reg_name: $ty_name = $ty_name($reg_value);
            )*

            #[inline(never)]
            pub fn get_name(self) -> Option<&'static str> {
                match self {
                    $(
                        Self::$reg_name => Some(stringify!($reg_name)),
                    )*
                    _ => None,
                }
            }

            #[inline(never)]
            pub fn from_name(name: &str) -> Option<Self> {
                match name {
                    $(
                        stringify!($reg_name) => Some(Self::$reg_name),
                    )*
                    _ => None,
                }
            }
        }

        impl core::fmt::Display for $ty_name {
            fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {
                if let Some(s) = self.get_name() {
                    f.write_str(s)
                } else {
                    write!(f, "??(0x{:x})", self.0)
                }
            }
        }

        impl core::fmt::Debug for $ty_name {
            fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {
                <Self as core::fmt::Display>::fmt(self, f)
            }
        }
    }
}

/// Identifies COFF CPU architectures.
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash, Debug)]
pub enum Arch {
    /// AMD64
    AMD64,
    /// ARM64, including ARM64EC, ARM64X
    ARM64,
    /// X86
    X86,
}

pub mod amd64;
pub mod arm64;
pub mod x86;

/// Identifies a register in a specific architecture
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub struct ArchReg {
    /// Target architecture
    pub arch: Arch,
    /// The untyped register index
    pub reg: u16,
}

impl ArchReg {
    /// Ties an arch and a reg
    pub fn new(arch: Arch, reg: u16) -> Self {
        Self { arch, reg }
    }
}

use core::fmt::{Debug, Display};

impl Debug for ArchReg {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        <Self as Display>::fmt(self, f)
    }
}

impl Display for ArchReg {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self.arch {
            Arch::AMD64 => Display::fmt(&amd64::Amd64Reg(self.reg), f),
            Arch::X86 => Display::fmt(&x86::X86Reg(self.reg), f),
            Arch::ARM64 => Display::fmt(&arm64::Arm64Reg(self.reg), f),
        }
    }
}

```

`codeview/src/arch/amd64.rs`:

```rs
//! AMD64-specific definitions

mod regs;

pub use regs::*;

```

`codeview/src/arch/amd64/regs.rs`:

```rs
register_set! {
    pub enum Amd64Reg;

    AL       =   1,
    CL       =   2,
    DL       =   3,
    BL       =   4,
    AH       =   5,
    CH       =   6,
    DH       =   7,
    BH       =   8,
    AX       =   9,
    CX       =  10,
    DX       =  11,
    BX       =  12,
    SP       =  13,
    BP       =  14,
    SI       =  15,
    DI       =  16,
    EAX      =  17,
    ECX      =  18,
    EDX      =  19,
    EBX      =  20,
    ESP      =  21,
    EBP      =  22,
    ESI      =  23,
    EDI      =  24,
    ES       =  25,
    CS       =  26,
    SS       =  27,
    DS       =  28,
    FS       =  29,
    GS       =  30,
    FLAGS    =  32,
    RIP      =  33,
    EFLAGS   =  34,

    // Control registers
    CR0      =  80,
    CR1      =  81,
    CR2      =  82,
    CR3      =  83,
    CR4      =  84,
    CR8      =  88,

    // Debug registers
    DR0      =  90,
    DR1      =  91,
    DR2      =  92,
    DR3      =  93,
    DR4      =  94,
    DR5      =  95,
    DR6      =  96,
    DR7      =  97,
    DR8      =  98,
    DR9      =  99,
    DR10     =  100,
    DR11     =  101,
    DR12     =  102,
    DR13     =  103,
    DR14     =  104,
    DR15     =  105,

    GDTR     =  110,
    GDTL     =  111,
    IDTR     =  112,
    IDTL     =  113,
    LDTR     =  114,
    TR       =  115,

    ST0      =  128,
    ST1      =  129,
    ST2      =  130,
    ST3      =  131,
    ST4      =  132,
    ST5      =  133,
    ST6      =  134,
    ST7      =  135,
    CTRL     =  136,
    STAT     =  137,
    TAG      =  138,
    FPIP     =  139,
    FPCS     =  140,
    FPDO     =  141,
    FPDS     =  142,
    ISEM     =  143,
    FPEIP    =  144,
    FPEDO    =  145,

    MM0      =  146,
    MM1      =  147,
    MM2      =  148,
    MM3      =  149,
    MM4      =  150,
    MM5      =  151,
    MM6      =  152,
    MM7      =  153,

    XMM0     =  154,   // KATMAI registers
    XMM1     =  155,
    XMM2     =  156,
    XMM3     =  157,
    XMM4     =  158,
    XMM5     =  159,
    XMM6     =  160,
    XMM7     =  161,

    XMM0_0   =  162,   // KATMAI sub-registers
    XMM0_1   =  163,
    XMM0_2   =  164,
    XMM0_3   =  165,
    XMM1_0   =  166,
    XMM1_1   =  167,
    XMM1_2   =  168,
    XMM1_3   =  169,
    XMM2_0   =  170,
    XMM2_1   =  171,
    XMM2_2   =  172,
    XMM2_3   =  173,
    XMM3_0   =  174,
    XMM3_1   =  175,
    XMM3_2   =  176,
    XMM3_3   =  177,
    XMM4_0   =  178,
    XMM4_1   =  179,
    XMM4_2   =  180,
    XMM4_3   =  181,
    XMM5_0   =  182,
    XMM5_1   =  183,
    XMM5_2   =  184,
    XMM5_3   =  185,
    XMM6_0   =  186,
    XMM6_1   =  187,
    XMM6_2   =  188,
    XMM6_3   =  189,
    XMM7_0   =  190,
    XMM7_1   =  191,
    XMM7_2   =  192,
    XMM7_3   =  193,

    XMM0L    =  194,
    XMM1L    =  195,
    XMM2L    =  196,
    XMM3L    =  197,
    XMM4L    =  198,
    XMM5L    =  199,
    XMM6L    =  200,
    XMM7L    =  201,

    XMM0H    =  202,
    XMM1H    =  203,
    XMM2H    =  204,
    XMM3H    =  205,
    XMM4H    =  206,
    XMM5H    =  207,
    XMM6H    =  208,
    XMM7H    =  209,

    MXCSR    =  211,   // XMM status register

    EMM0L    =  220,   // XMM sub-registers (WNI integer)
    EMM1L    =  221,
    EMM2L    =  222,
    EMM3L    =  223,
    EMM4L    =  224,
    EMM5L    =  225,
    EMM6L    =  226,
    EMM7L    =  227,

    EMM0H    =  228,
    EMM1H    =  229,
    EMM2H    =  230,
    EMM3H    =  231,
    EMM4H    =  232,
    EMM5H    =  233,
    EMM6H    =  234,
    EMM7H    =  235,

    // do not change the order of these regs, first one must be even too
    MM00     =  236,
    MM01     =  237,
    MM10     =  238,
    MM11     =  239,
    MM20     =  240,
    MM21     =  241,
    MM30     =  242,
    MM31     =  243,
    MM40     =  244,
    MM41     =  245,
    MM50     =  246,
    MM51     =  247,
    MM60     =  248,
    MM61     =  249,
    MM70     =  250,
    MM71     =  251,

    // Extended KATMAI registers
    XMM8     =  252,   // KATMAI registers
    XMM9     =  253,
    XMM10    =  254,
    XMM11    =  255,
    XMM12    =  256,
    XMM13    =  257,
    XMM14    =  258,
    XMM15    =  259,

    XMM8_0   =  260,   // KATMAI sub-registers
    XMM8_1   =  261,
    XMM8_2   =  262,
    XMM8_3   =  263,
    XMM9_0   =  264,
    XMM9_1   =  265,
    XMM9_2   =  266,
    XMM9_3   =  267,
    XMM10_0  =  268,
    XMM10_1  =  269,
    XMM10_2  =  270,
    XMM10_3  =  271,
    XMM11_0  =  272,
    XMM11_1  =  273,
    XMM11_2  =  274,
    XMM11_3  =  275,
    XMM12_0  =  276,
    XMM12_1  =  277,
    XMM12_2  =  278,
    XMM12_3  =  279,
    XMM13_0  =  280,
    XMM13_1  =  281,
    XMM13_2  =  282,
    XMM13_3  =  283,
    XMM14_0  =  284,
    XMM14_1  =  285,
    XMM14_2  =  286,
    XMM14_3  =  287,
    XMM15_0  =  288,
    XMM15_1  =  289,
    XMM15_2  =  290,
    XMM15_3  =  291,

    XMM8L    =  292,
    XMM9L    =  293,
    XMM10L   =  294,
    XMM11L   =  295,
    XMM12L   =  296,
    XMM13L   =  297,
    XMM14L   =  298,
    XMM15L   =  299,

    XMM8H    =  300,
    XMM9H    =  301,
    XMM10H   =  302,
    XMM11H   =  303,
    XMM12H   =  304,
    XMM13H   =  305,
    XMM14H   =  306,
    XMM15H   =  307,

    EMM8L    =  308,   // XMM sub-registers (WNI integer)
    EMM9L    =  309,
    EMM10L   =  310,
    EMM11L   =  311,
    EMM12L   =  312,
    EMM13L   =  313,
    EMM14L   =  314,
    EMM15L   =  315,

    EMM8H    =  316,
    EMM9H    =  317,
    EMM10H   =  318,
    EMM11H   =  319,
    EMM12H   =  320,
    EMM13H   =  321,
    EMM14H   =  322,
    EMM15H   =  323,

    // Low byte forms of some standard registers
    SIL      =  324,
    DIL      =  325,
    BPL      =  326,
    SPL      =  327,

    // 64-bit regular registers
    RAX      =  328,
    RBX      =  329,
    RCX      =  330,
    RDX      =  331,
    RSI      =  332,
    RDI      =  333,
    RBP      =  334,
    RSP      =  335,

    // 64-bit integer registers with 8-, 16-, and 32-bit forms (B, W, and D)
    R8       =  336,
    R9       =  337,
    R10      =  338,
    R11      =  339,
    R12      =  340,
    R13      =  341,
    R14      =  342,
    R15      =  343,

    R8B      =  344,
    R9B      =  345,
    R10B     =  346,
    R11B     =  347,
    R12B     =  348,
    R13B     =  349,
    R14B     =  350,
    R15B     =  351,

    R8W      =  352,
    R9W      =  353,
    R10W     =  354,
    R11W     =  355,
    R12W     =  356,
    R13W     =  357,
    R14W     =  358,
    R15W     =  359,

    R8D      =  360,
    R9D      =  361,
    R10D     =  362,
    R11D     =  363,
    R12D     =  364,
    R13D     =  365,
    R14D     =  366,
    R15D     =  367,

    // AVX registers 256 bits
    YMM0     =  368,
    YMM1     =  369,
    YMM2     =  370,
    YMM3     =  371,
    YMM4     =  372,
    YMM5     =  373,
    YMM6     =  374,
    YMM7     =  375,
    YMM8     =  376,
    YMM9     =  377,
    YMM10    =  378,
    YMM11    =  379,
    YMM12    =  380,
    YMM13    =  381,
    YMM14    =  382,
    YMM15    =  383,

    // AVX registers upper 128 bits
    YMM0H    =  384,
    YMM1H    =  385,
    YMM2H    =  386,
    YMM3H    =  387,
    YMM4H    =  388,
    YMM5H    =  389,
    YMM6H    =  390,
    YMM7H    =  391,
    YMM8H    =  392,
    YMM9H    =  393,
    YMM10H   =  394,
    YMM11H   =  395,
    YMM12H   =  396,
    YMM13H   =  397,
    YMM14H   =  398,
    YMM15H   =  399,

    //Lower/upper 8 bytes of XMM registers.  Unlike XMM<regnum><H/L>, these
    //values reprsesent the bit patterns of the registers as 64-bit integers, not
    //the representation of these registers as a double.
    XMM0IL    = 400,
    XMM1IL    = 401,
    XMM2IL    = 402,
    XMM3IL    = 403,
    XMM4IL    = 404,
    XMM5IL    = 405,
    XMM6IL    = 406,
    XMM7IL    = 407,
    XMM8IL    = 408,
    XMM9IL    = 409,
    XMM10IL    = 410,
    XMM11IL    = 411,
    XMM12IL    = 412,
    XMM13IL    = 413,
    XMM14IL    = 414,
    XMM15IL    = 415,

    XMM0IH    = 416,
    XMM1IH    = 417,
    XMM2IH    = 418,
    XMM3IH    = 419,
    XMM4IH    = 420,
    XMM5IH    = 421,
    XMM6IH    = 422,
    XMM7IH    = 423,
    XMM8IH    = 424,
    XMM9IH    = 425,
    XMM10IH    = 426,
    XMM11IH    = 427,
    XMM12IH    = 428,
    XMM13IH    = 429,
    XMM14IH    = 430,
    XMM15IH    = 431,

    YMM0I0    =  432,        // AVX integer registers
    YMM0I1    =  433,
    YMM0I2    =  434,
    YMM0I3    =  435,
    YMM1I0    =  436,
    YMM1I1    =  437,
    YMM1I2    =  438,
    YMM1I3    =  439,
    YMM2I0    =  440,
    YMM2I1    =  441,
    YMM2I2    =  442,
    YMM2I3    =  443,
    YMM3I0    =  444,
    YMM3I1    =  445,
    YMM3I2    =  446,
    YMM3I3    =  447,
    YMM4I0    =  448,
    YMM4I1    =  449,
    YMM4I2    =  450,
    YMM4I3    =  451,
    YMM5I0    =  452,
    YMM5I1    =  453,
    YMM5I2    =  454,
    YMM5I3    =  455,
    YMM6I0    =  456,
    YMM6I1    =  457,
    YMM6I2    =  458,
    YMM6I3    =  459,
    YMM7I0    =  460,
    YMM7I1    =  461,
    YMM7I2    =  462,
    YMM7I3    =  463,
    YMM8I0    =  464,
    YMM8I1    =  465,
    YMM8I2    =  466,
    YMM8I3    =  467,
    YMM9I0    =  468,
    YMM9I1    =  469,
    YMM9I2    =  470,
    YMM9I3    =  471,
    YMM10I0    =  472,
    YMM10I1    =  473,
    YMM10I2    =  474,
    YMM10I3    =  475,
    YMM11I0    =  476,
    YMM11I1    =  477,
    YMM11I2    =  478,
    YMM11I3    =  479,
    YMM12I0    =  480,
    YMM12I1    =  481,
    YMM12I2    =  482,
    YMM12I3    =  483,
    YMM13I0    =  484,
    YMM13I1    =  485,
    YMM13I2    =  486,
    YMM13I3    =  487,
    YMM14I0    =  488,
    YMM14I1    =  489,
    YMM14I2    =  490,
    YMM14I3    =  491,
    YMM15I0    =  492,
    YMM15I1    =  493,
    YMM15I2    =  494,
    YMM15I3    =  495,

    YMM0F0    =  496,        // AVX floating-point single precise registers
    YMM0F1    =  497,
    YMM0F2    =  498,
    YMM0F3    =  499,
    YMM0F4    =  500,
    YMM0F5    =  501,
    YMM0F6    =  502,
    YMM0F7    =  503,
    YMM1F0    =  504,
    YMM1F1    =  505,
    YMM1F2    =  506,
    YMM1F3    =  507,
    YMM1F4    =  508,
    YMM1F5    =  509,
    YMM1F6    =  510,
    YMM1F7    =  511,
    YMM2F0    =  512,
    YMM2F1    =  513,
    YMM2F2    =  514,
    YMM2F3    =  515,
    YMM2F4    =  516,
    YMM2F5    =  517,
    YMM2F6    =  518,
    YMM2F7    =  519,
    YMM3F0    =  520,
    YMM3F1    =  521,
    YMM3F2    =  522,
    YMM3F3    =  523,
    YMM3F4    =  524,
    YMM3F5    =  525,
    YMM3F6    =  526,
    YMM3F7    =  527,
    YMM4F0    =  528,
    YMM4F1    =  529,
    YMM4F2    =  530,
    YMM4F3    =  531,
    YMM4F4    =  532,
    YMM4F5    =  533,
    YMM4F6    =  534,
    YMM4F7    =  535,
    YMM5F0    =  536,
    YMM5F1    =  537,
    YMM5F2    =  538,
    YMM5F3    =  539,
    YMM5F4    =  540,
    YMM5F5    =  541,
    YMM5F6    =  542,
    YMM5F7    =  543,
    YMM6F0    =  544,
    YMM6F1    =  545,
    YMM6F2    =  546,
    YMM6F3    =  547,
    YMM6F4    =  548,
    YMM6F5    =  549,
    YMM6F6    =  550,
    YMM6F7    =  551,
    YMM7F0    =  552,
    YMM7F1    =  553,
    YMM7F2    =  554,
    YMM7F3    =  555,
    YMM7F4    =  556,
    YMM7F5    =  557,
    YMM7F6    =  558,
    YMM7F7    =  559,
    YMM8F0    =  560,
    YMM8F1    =  561,
    YMM8F2    =  562,
    YMM8F3    =  563,
    YMM8F4    =  564,
    YMM8F5    =  565,
    YMM8F6    =  566,
    YMM8F7    =  567,
    YMM9F0    =  568,
    YMM9F1    =  569,
    YMM9F2    =  570,
    YMM9F3    =  571,
    YMM9F4    =  572,
    YMM9F5    =  573,
    YMM9F6    =  574,
    YMM9F7    =  575,
    YMM10F0    =  576,
    YMM10F1    =  577,
    YMM10F2    =  578,
    YMM10F3    =  579,
    YMM10F4    =  580,
    YMM10F5    =  581,
    YMM10F6    =  582,
    YMM10F7    =  583,
    YMM11F0    =  584,
    YMM11F1    =  585,
    YMM11F2    =  586,
    YMM11F3    =  587,
    YMM11F4    =  588,
    YMM11F5    =  589,
    YMM11F6    =  590,
    YMM11F7    =  591,
    YMM12F0    =  592,
    YMM12F1    =  593,
    YMM12F2    =  594,
    YMM12F3    =  595,
    YMM12F4    =  596,
    YMM12F5    =  597,
    YMM12F6    =  598,
    YMM12F7    =  599,
    YMM13F0    =  600,
    YMM13F1    =  601,
    YMM13F2    =  602,
    YMM13F3    =  603,
    YMM13F4    =  604,
    YMM13F5    =  605,
    YMM13F6    =  606,
    YMM13F7    =  607,
    YMM14F0    =  608,
    YMM14F1    =  609,
    YMM14F2    =  610,
    YMM14F3    =  611,
    YMM14F4    =  612,
    YMM14F5    =  613,
    YMM14F6    =  614,
    YMM14F7    =  615,
    YMM15F0    =  616,
    YMM15F1    =  617,
    YMM15F2    =  618,
    YMM15F3    =  619,
    YMM15F4    =  620,
    YMM15F5    =  621,
    YMM15F6    =  622,
    YMM15F7    =  623,

    YMM0D0    =  624,        // AVX floating-point double precise registers
    YMM0D1    =  625,
    YMM0D2    =  626,
    YMM0D3    =  627,
    YMM1D0    =  628,
    YMM1D1    =  629,
    YMM1D2    =  630,
    YMM1D3    =  631,
    YMM2D0    =  632,
    YMM2D1    =  633,
    YMM2D2    =  634,
    YMM2D3    =  635,
    YMM3D0    =  636,
    YMM3D1    =  637,
    YMM3D2    =  638,
    YMM3D3    =  639,
    YMM4D0    =  640,
    YMM4D1    =  641,
    YMM4D2    =  642,
    YMM4D3    =  643,
    YMM5D0    =  644,
    YMM5D1    =  645,
    YMM5D2    =  646,
    YMM5D3    =  647,
    YMM6D0    =  648,
    YMM6D1    =  649,
    YMM6D2    =  650,
    YMM6D3    =  651,
    YMM7D0    =  652,
    YMM7D1    =  653,
    YMM7D2    =  654,
    YMM7D3    =  655,
    YMM8D0    =  656,
    YMM8D1    =  657,
    YMM8D2    =  658,
    YMM8D3    =  659,
    YMM9D0    =  660,
    YMM9D1    =  661,
    YMM9D2    =  662,
    YMM9D3    =  663,
    YMM10D0    =  664,
    YMM10D1    =  665,
    YMM10D2    =  666,
    YMM10D3    =  667,
    YMM11D0    =  668,
    YMM11D1    =  669,
    YMM11D2    =  670,
    YMM11D3    =  671,
    YMM12D0    =  672,
    YMM12D1    =  673,
    YMM12D2    =  674,
    YMM12D3    =  675,
    YMM13D0    =  676,
    YMM13D1    =  677,
    YMM13D2    =  678,
    YMM13D3    =  679,
    YMM14D0    =  680,
    YMM14D1    =  681,
    YMM14D2    =  682,
    YMM14D3    =  683,
    YMM15D0    =  684,
    YMM15D1    =  685,
    YMM15D2    =  686,
    YMM15D3    =  687,
}

```

`codeview/src/arch/arm64.rs`:

```rs
//! ARM64 / AArch64

mod regs;
pub use regs::Arm64Reg;

```

`codeview/src/arch/arm64/regs.rs`:

```rs
register_set! {
    pub enum Arm64Reg;
    // General purpose 32-bit integer registers

    W0     =  10,
    W1     =  11,
    W2     =  12,
    W3     =  13,
    W4     =  14,
    W5     =  15,
    W6     =  16,
    W7     =  17,
    W8     =  18,
    W9     =  19,
    W10    =  20,
    W11    =  21,
    W12    =  22,
    W13    =  23,
    W14    =  24,
    W15    =  25,
    W16    =  26,
    W17    =  27,
    W18    =  28,
    W19    =  29,
    W20    =  30,
    W21    =  31,
    W22    =  32,
    W23    =  33,
    W24    =  34,
    W25    =  35,
    W26    =  36,
    W27    =  37,
    W28    =  38,
    W29    =  39,
    W30    =  40,
    WZR    =  41,

    // General purpose 64-bit integer registers

    X0     =  50,
    X1     =  51,
    X2     =  52,
    X3     =  53,
    X4     =  54,
    X5     =  55,
    X6     =  56,
    X7     =  57,
    X8     =  58,
    X9     =  59,
    X10    =  60,
    X11    =  61,
    X12    =  62,
    X13    =  63,
    X14    =  64,
    X15    =  65,
    IP0    =  66,
    IP1    =  67,
    X18    =  68,
    X19    =  69,
    X20    =  70,
    X21    =  71,
    X22    =  72,
    X23    =  73,
    X24    =  74,
    X25    =  75,
    X26    =  76,
    X27    =  77,
    X28    =  78,
    FP     =  79,
    LR     =  80,
    SP     =  81,
    ZR     =  82,

    // statue register

    NZCV   =  90,

    // 32-bit floating point registers

    S0     =  100,
    S1     =  101,
    S2     =  102,
    S3     =  103,
    S4     =  104,
    S5     =  105,
    S6     =  106,
    S7     =  107,
    S8     =  108,
    S9     =  109,
    S10    =  110,
    S11    =  111,
    S12    =  112,
    S13    =  113,
    S14    =  114,
    S15    =  115,
    S16    =  116,
    S17    =  117,
    S18    =  118,
    S19    =  119,
    S20    =  120,
    S21    =  121,
    S22    =  122,
    S23    =  123,
    S24    =  124,
    S25    =  125,
    S26    =  126,
    S27    =  127,
    S28    =  128,
    S29    =  129,
    S30    =  130,
    S31    =  131,

    // 64-bit floating point registers

    D0     =  140,
    D1     =  141,
    D2     =  142,
    D3     =  143,
    D4     =  144,
    D5     =  145,
    D6     =  146,
    D7     =  147,
    D8     =  148,
    D9     =  149,
    D10    =  150,
    D11    =  151,
    D12    =  152,
    D13    =  153,
    D14    =  154,
    D15    =  155,
    D16    =  156,
    D17    =  157,
    D18    =  158,
    D19    =  159,
    D20    =  160,
    D21    =  161,
    D22    =  162,
    D23    =  163,
    D24    =  164,
    D25    =  165,
    D26    =  166,
    D27    =  167,
    D28    =  168,
    D29    =  169,
    D30    =  170,
    D31    =  171,

    // 128-bit SIMD registers

    Q0     =  180,
    Q1     =  181,
    Q2     =  182,
    Q3     =  183,
    Q4     =  184,
    Q5     =  185,
    Q6     =  186,
    Q7     =  187,
    Q8     =  188,
    Q9     =  189,
    Q10    =  190,
    Q11    =  191,
    Q12    =  192,
    Q13    =  193,
    Q14    =  194,
    Q15    =  195,
    Q16    =  196,
    Q17    =  197,
    Q18    =  198,
    Q19    =  199,
    Q20    =  200,
    Q21    =  201,
    Q22    =  202,
    Q23    =  203,
    Q24    =  204,
    Q25    =  205,
    Q26    =  206,
    Q27    =  207,
    Q28    =  208,
    Q29    =  209,
    Q30    =  210,
    Q31    =  211,

    // Floating point status register

    FPSR   =  220,
}

```

`codeview/src/arch/x86.rs`:

```rs
//! x86

mod regs;

pub use regs::X86Reg;

```

`codeview/src/arch/x86/regs.rs`:

```rs
register_set! {
    pub enum X86Reg;

    NONE     =   0,
    AL       =   1,
    CL       =   2,
    DL       =   3,
    BL       =   4,
    AH       =   5,
    CH       =   6,
    DH       =   7,
    BH       =   8,
    AX       =   9,
    CX       =  10,
    DX       =  11,
    BX       =  12,
    SP       =  13,
    BP       =  14,
    SI       =  15,
    DI       =  16,
    EAX      =  17,
    ECX      =  18,
    EDX      =  19,
    EBX      =  20,
    ESP      =  21,
    EBP      =  22,
    ESI      =  23,
    EDI      =  24,
    ES       =  25,
    CS       =  26,
    SS       =  27,
    DS       =  28,
    FS       =  29,
    GS       =  30,
    IP       =  31,
    FLAGS    =  32,
    EIP      =  33,
    EFLAGS   =  34,
    TEMP     =  40,          // PCODE Temp
    TEMPH    =  41,          // PCODE TempH
    QUOTE    =  42,          // PCODE Quote
    PCDR3    =  43,          // PCODE reserved
    PCDR4    =  44,          // PCODE reserved
    PCDR5    =  45,          // PCODE reserved
    PCDR6    =  46,          // PCODE reserved
    PCDR7    =  47,          // PCODE reserved
    CR0      =  80,          // CR0 -- control registers
    CR1      =  81,
    CR2      =  82,
    CR3      =  83,
    CR4      =  84,          // Pentium
    DR0      =  90,          // Debug register
    DR1      =  91,
    DR2      =  92,
    DR3      =  93,
    DR4      =  94,
    DR5      =  95,
    DR6      =  96,
    DR7      =  97,
    GDTR     =  110,
    GDTL     =  111,
    IDTR     =  112,
    IDTL     =  113,
    LDTR     =  114,
    TR       =  115,

    PSEUDO1  =  116,
    PSEUDO2  =  117,
    PSEUDO3  =  118,
    PSEUDO4  =  119,
    PSEUDO5  =  120,
    PSEUDO6  =  121,
    PSEUDO7  =  122,
    PSEUDO8  =  123,
    PSEUDO9  =  124,

    ST0      =  128,
    ST1      =  129,
    ST2      =  130,
    ST3      =  131,
    ST4      =  132,
    ST5      =  133,
    ST6      =  134,
    ST7      =  135,
    CTRL     =  136,
    STAT     =  137,
    TAG      =  138,
    FPIP     =  139,
    FPCS     =  140,
    FPDO     =  141,
    FPDS     =  142,
    ISEM     =  143,
    FPEIP    =  144,
    FPEDO    =  145,

    MM0      =  146,
    MM1      =  147,
    MM2      =  148,
    MM3      =  149,
    MM4      =  150,
    MM5      =  151,
    MM6      =  152,
    MM7      =  153,

    XMM0     =  154, // KATMAI registers
    XMM1     =  155,
    XMM2     =  156,
    XMM3     =  157,
    XMM4     =  158,
    XMM5     =  159,
    XMM6     =  160,
    XMM7     =  161,

    XMM00    =  162, // KATMAI sub-registers
    XMM01    =  163,
    XMM02    =  164,
    XMM03    =  165,
    XMM10    =  166,
    XMM11    =  167,
    XMM12    =  168,
    XMM13    =  169,
    XMM20    =  170,
    XMM21    =  171,
    XMM22    =  172,
    XMM23    =  173,
    XMM30    =  174,
    XMM31    =  175,
    XMM32    =  176,
    XMM33    =  177,
    XMM40    =  178,
    XMM41    =  179,
    XMM42    =  180,
    XMM43    =  181,
    XMM50    =  182,
    XMM51    =  183,
    XMM52    =  184,
    XMM53    =  185,
    XMM60    =  186,
    XMM61    =  187,
    XMM62    =  188,
    XMM63    =  189,
    XMM70    =  190,
    XMM71    =  191,
    XMM72    =  192,
    XMM73    =  193,

    XMM0L    =  194,
    XMM1L    =  195,
    XMM2L    =  196,
    XMM3L    =  197,
    XMM4L    =  198,
    XMM5L    =  199,
    XMM6L    =  200,
    XMM7L    =  201,

    XMM0H    =  202,
    XMM1H    =  203,
    XMM2H    =  204,
    XMM3H    =  205,
    XMM4H    =  206,
    XMM5H    =  207,
    XMM6H    =  208,
    XMM7H    =  209,

    MXCSR    =  211, // XMM status register

    EDXEAX   =  212, // EDX:EAX pair

    EMM0L    =  220, // XMM sub-registers (WNI integer)
    EMM1L    =  221,
    EMM2L    =  222,
    EMM3L    =  223,
    EMM4L    =  224,
    EMM5L    =  225,
    EMM6L    =  226,
    EMM7L    =  227,

    EMM0H    =  228,
    EMM1H    =  229,
    EMM2H    =  230,
    EMM3H    =  231,
    EMM4H    =  232,
    EMM5H    =  233,
    EMM6H    =  234,
    EMM7H    =  235,

    // do not change the order of these regs, first one must be even too
    MM00     =  236,
    MM01     =  237,
    MM10     =  238,
    MM11     =  239,
    MM20     =  240,
    MM21     =  241,
    MM30     =  242,
    MM31     =  243,
    MM40     =  244,
    MM41     =  245,
    MM50     =  246,
    MM51     =  247,
    MM60     =  248,
    MM61     =  249,
    MM70     =  250,
    MM71     =  251,

    YMM0     =  252, // AVX registers
    YMM1     =  253,
    YMM2     =  254,
    YMM3     =  255,
    YMM4     =  256,
    YMM5     =  257,
    YMM6     =  258,
    YMM7     =  259,

    YMM0H    =  260,
    YMM1H    =  261,
    YMM2H    =  262,
    YMM3H    =  263,
    YMM4H    =  264,
    YMM5H    =  265,
    YMM6H    =  266,
    YMM7H    =  267,

    YMM0I0     =    268,    // AVX integer registers
    YMM0I1     =    269,
    YMM0I2     =    270,
    YMM0I3     =    271,
    YMM1I0     =    272,
    YMM1I1     =    273,
    YMM1I2     =    274,
    YMM1I3     =    275,
    YMM2I0     =    276,
    YMM2I1     =    277,
    YMM2I2     =    278,
    YMM2I3     =    279,
    YMM3I0     =    280,
    YMM3I1     =    281,
    YMM3I2     =    282,
    YMM3I3     =    283,
    YMM4I0     =    284,
    YMM4I1     =    285,
    YMM4I2     =    286,
    YMM4I3     =    287,
    YMM5I0     =    288,
    YMM5I1     =    289,
    YMM5I2     =    290,
    YMM5I3     =    291,
    YMM6I0     =    292,
    YMM6I1     =    293,
    YMM6I2     =    294,
    YMM6I3     =    295,
    YMM7I0     =    296,
    YMM7I1     =    297,
    YMM7I2     =    298,
    YMM7I3     =    299,

    YMM0F0    =  300,     // AVX floating-point single precise registers
    YMM0F1    =  301,
    YMM0F2    =  302,
    YMM0F3    =  303,
    YMM0F4    =  304,
    YMM0F5    =  305,
    YMM0F6    =  306,
    YMM0F7    =  307,
    YMM1F0    =  308,
    YMM1F1    =  309,
    YMM1F2    =  310,
    YMM1F3    =  311,
    YMM1F4    =  312,
    YMM1F5    =  313,
    YMM1F6    =  314,
    YMM1F7    =  315,
    YMM2F0    =  316,
    YMM2F1    =  317,
    YMM2F2    =  318,
    YMM2F3    =  319,
    YMM2F4    =  320,
    YMM2F5    =  321,
    YMM2F6    =  322,
    YMM2F7    =  323,
    YMM3F0    =  324,
    YMM3F1    =  325,
    YMM3F2    =  326,
    YMM3F3    =  327,
    YMM3F4    =  328,
    YMM3F5    =  329,
    YMM3F6    =  330,
    YMM3F7    =  331,
    YMM4F0    =  332,
    YMM4F1    =  333,
    YMM4F2    =  334,
    YMM4F3    =  335,
    YMM4F4    =  336,
    YMM4F5    =  337,
    YMM4F6    =  338,
    YMM4F7    =  339,
    YMM5F0    =  340,
    YMM5F1    =  341,
    YMM5F2    =  342,
    YMM5F3    =  343,
    YMM5F4    =  344,
    YMM5F5    =  345,
    YMM5F6    =  346,
    YMM5F7    =  347,
    YMM6F0    =  348,
    YMM6F1    =  349,
    YMM6F2    =  350,
    YMM6F3    =  351,
    YMM6F4    =  352,
    YMM6F5    =  353,
    YMM6F6    =  354,
    YMM6F7    =  355,
    YMM7F0    =  356,
    YMM7F1    =  357,
    YMM7F2    =  358,
    YMM7F3    =  359,
    YMM7F4    =  360,
    YMM7F5    =  361,
    YMM7F6    =  362,
    YMM7F7    =  363,

    YMM0D0     =    364,    // AVX floating-point double precise registers
    YMM0D1     =    365,
    YMM0D2     =    366,
    YMM0D3     =    367,
    YMM1D0     =    368,
    YMM1D1     =    369,
    YMM1D2     =    370,
    YMM1D3     =    371,
    YMM2D0     =    372,
    YMM2D1     =    373,
    YMM2D2     =    374,
    YMM2D3     =    375,
    YMM3D0     =    376,
    YMM3D1     =    377,
    YMM3D2     =    378,
    YMM3D3     =    379,
    YMM4D0     =    380,
    YMM4D1     =    381,
    YMM4D2     =    382,
    YMM4D3     =    383,
    YMM5D0     =    384,
    YMM5D1     =    385,
    YMM5D2     =    386,
    YMM5D3     =    387,
    YMM6D0     =    388,
    YMM6D1     =    389,
    YMM6D2     =    390,
    YMM6D3     =    391,
    YMM7D0     =    392,
    YMM7D1     =    393,
    YMM7D2     =    394,
    YMM7D3     =    395,

    BND0       =    396,
    BND1       =    397,
    BND2       =    398,
    BND3       =    399,
}

```

`codeview/src/encoder.rs`:

```rs
//! Support for encoding primitives and blittable types into output buffers.
#![allow(missing_docs)]

use bstr::BStr;
use uuid::Uuid;
use zerocopy::{Immutable, IntoBytes};

/// A simple type which helps encode CodeView records into a buffer.
pub struct Encoder<'a> {
    pub buf: &'a mut Vec<u8>,
}

impl<'a> Encoder<'a> {
    pub fn new(buf: &'a mut Vec<u8>) -> Self {
        Self { buf }
    }

    pub fn len(&self) -> usize {
        self.buf.len()
    }

    pub fn is_empty(&self) -> bool {
        self.buf.is_empty()
    }

    pub fn u8(&mut self, x: u8) {
        self.buf.push(x);
    }

    pub fn bytes(&mut self, b: &[u8]) {
        self.buf.extend_from_slice(b);
    }

    pub fn u16(&mut self, x: u16) {
        self.bytes(&x.to_le_bytes());
    }

    pub fn u32(&mut self, x: u32) {
        self.bytes(&x.to_le_bytes());
    }

    pub fn t<T: IntoBytes + Immutable>(&mut self, x: &T) {
        self.buf.extend_from_slice(x.as_bytes());
    }

    pub fn strz(&mut self, s: &BStr) {
        self.buf.extend_from_slice(s);
        self.buf.push(0);
    }

    pub fn uuid(&mut self, u: &Uuid) {
        self.bytes(&u.to_bytes_le())
    }
}

```

`codeview/src/lib.rs`:

```rs
//! CodeView definitions
//!
//! This defines types and constants for CodeView debugging tools. CodeView is the debugging
//! format used for PE/COFF images on Windows.
//!
//! This crate does not provide any I/O capabilities. It does not read or write PDBs, `.debug`
//! sections in PE/COFF objects, etc.
//!
//! * [`ms-pdb`](https://crates.io/crates/ms-pdb) - Use this crate for reading and writing PDB files.
//!
//! # References
//!
//! * [CodeView Symbols](https://llvm.org/docs/PDB/CodeViewSymbols.html)
//! * [`cvinfo.h`](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/include/cvinfo.h)

#![forbid(unsafe_code)]
#![forbid(unused_must_use)]
#![warn(missing_docs)]
#![allow(clippy::needless_lifetimes)]

pub mod arch;
pub mod encoder;
pub mod parser;
pub mod syms;
pub mod types;
mod utils;

pub use utils::iter::*;

```

`codeview/src/parser.rs`:

```rs
//! Support for parsing byte-oriented data

#[cfg(test)]
mod tests;

use crate::types::TypeIndex;
use bstr::{BStr, ByteSlice};
use std::mem::{size_of, take};
use zerocopy::byteorder::{I16, I32, I64, LE, U16, U32, U64};
use zerocopy::{FromBytes, I128, Immutable, IntoBytes, KnownLayout, U128, Unaligned};

pub use crate::types::number::Number;

/// A byte-oriented parser, for use in decoding CodeView records.
#[derive(Clone)]
pub struct Parser<'a> {
    /// The bytes that have not yet been parsed.
    pub bytes: &'a [u8],
}

impl<'a> Parser<'a> {
    /// Starts a new parser.
    pub fn new(bytes: &'a [u8]) -> Self {
        Self { bytes }
    }

    /// Gets the rest of the unparsed bytes in the parser. The parser still retains a reference to
    /// the same data.
    pub fn peek_rest(&self) -> &'a [u8] {
        self.bytes
    }

    /// Gets the rest of the unparsed
    pub fn take_rest(&mut self) -> &'a [u8] {
        take(&mut self.bytes)
    }

    /// Consumes this `Parser` and returns the unparsed bytes within it.
    ///
    /// This should be used in situations where there is no valid reason to use the `Parser`
    /// after taking the rest of the bytes within it. In situations where a `parse()` method only
    /// has access to `&mut Parser`, then this function cannot be used, and the caller should use
    /// `Parser::take_rest`.
    pub fn into_rest(self) -> &'a [u8] {
        self.bytes
    }

    /// Indicates whether there are any bytes left to parse.
    pub fn is_empty(&self) -> bool {
        self.bytes.is_empty()
    }

    /// Returns the number of unparsed bytes in the parser.
    pub fn len(&self) -> usize {
        self.bytes.len()
    }

    /// Checks that the buffer has at least `n` bytes.
    ///
    /// This can be used as an optimization improvement in some situations. Ordinarily, code like
    /// this will compile to a series of bounds checks:
    ///
    /// ```ignore
    /// let mut p = Parser::new(bytes);
    /// let a = p.u32()?;
    /// let b = p.u16()?;
    /// let c = p.u16()?;
    /// let d = p.u32()?;
    /// ```
    ///
    /// Inserting a `a.needs(12)?` statement can sometimes enable the compiler to collapse a
    /// series of bounds checks (4, in this case) to a single bounds check.
    #[inline(always)]
    pub fn needs(&self, n: usize) -> Result<(), ParserError> {
        if n <= self.bytes.len() {
            Ok(())
        } else {
            Err(ParserError::new())
        }
    }

    /// Takes the next `n` bytes of input and returns a slice to it. The parser is advanced by `n`.
    #[inline(always)]
    pub fn bytes(&mut self, n: usize) -> Result<&'a [u8], ParserError> {
        if self.bytes.len() < n {
            return Err(ParserError::new());
        }

        let (lo, hi) = self.bytes.split_at(n);
        self.bytes = hi;
        Ok(lo)
    }

    /// Skips `n` bytes.
    pub fn skip(&mut self, n: usize) -> Result<(), ParserError> {
        if self.bytes.len() < n {
            return Err(ParserError::new());
        }

        self.bytes = &self.bytes[n..];
        Ok(())
    }

    /// Parses a reference to a structure. The input must contain at least [`size_of::<T>()`] bytes.
    #[inline(always)]
    pub fn get<T: FromBytes + Unaligned + KnownLayout + Immutable>(
        &mut self,
    ) -> Result<&'a T, ParserError> {
        if let Ok((value, rest)) = T::ref_from_prefix(self.bytes) {
            self.bytes = rest;
            Ok(value)
        } else {
            Err(ParserError::new())
        }
    }

    /// Parses a copy of a structure. The input must contain at least [`size_of::<T>()`] bytes.
    #[inline(always)]
    pub fn copy<T: FromBytes + Unaligned>(&mut self) -> Result<T, ParserError> {
        let item = self.bytes(size_of::<T>())?;
        Ok(T::read_from_bytes(item).unwrap())
    }

    /// Parses a `T` from the input, if `T` knows how to read from a `Parser`.
    ///
    /// This exists mainly to allow more succinct calls, using type inference.
    #[inline(always)]
    pub fn parse<T: Parse<'a>>(&mut self) -> Result<T, ParserError> {
        T::from_parser(self)
    }

    /// Parses a slice of items. The input must contain at least [`size_of::<T>() * n`] bytes.
    pub fn slice<T: FromBytes + Unaligned + Immutable>(
        &mut self,
        len: usize,
    ) -> Result<&'a [T], ParserError> {
        if let Ok((lo, hi)) = <[T]>::ref_from_prefix_with_elems(self.bytes, len) {
            self.bytes = hi;
            Ok(lo)
        } else {
            Err(ParserError::new())
        }
    }

    /// Copies an array of items with a constant size and advances the parser.
    pub fn array<const N: usize>(&mut self) -> Result<[u8; N], ParserError> {
        let s = self.bytes(N)?;
        Ok(<[u8; N]>::try_from(s).unwrap())
    }

    /// Reads one byte and advances.
    pub fn u8(&mut self) -> Result<u8, ParserError> {
        let b = self.bytes(1)?;
        Ok(b[0])
    }

    /// Reads one signed byte and advances.
    pub fn i8(&mut self) -> Result<i8, ParserError> {
        let b = self.bytes(1)?;
        Ok(b[0] as i8)
    }

    /// Reads an `i16` (in little-endian order) and advances.
    pub fn i16(&mut self) -> Result<i16, ParserError> {
        Ok(self.copy::<I16<LE>>()?.get())
    }

    /// Reads an `i32` (in little-endian order) and advances.
    pub fn i32(&mut self) -> Result<i32, ParserError> {
        Ok(self.copy::<I32<LE>>()?.get())
    }

    /// Reads an `i64` (in little-endian order) and advances.
    pub fn i64(&mut self) -> Result<i64, ParserError> {
        Ok(self.copy::<I64<LE>>()?.get())
    }

    /// Reads an `u16` (in little-endian order) and advances.
    pub fn u16(&mut self) -> Result<u16, ParserError> {
        Ok(self.copy::<U16<LE>>()?.get())
    }

    /// Reads an `u32` (in little-endian order) and advances.
    pub fn u32(&mut self) -> Result<u32, ParserError> {
        Ok(self.copy::<U32<LE>>()?.get())
    }

    /// Reads an `u64` (in little-endian order) and advances.
    pub fn u64(&mut self) -> Result<u64, ParserError> {
        Ok(self.copy::<U64<LE>>()?.get())
    }

    /// Reads an `u128` (in little-endian order) and advances.
    pub fn u128(&mut self) -> Result<u128, ParserError> {
        Ok(self.copy::<U128<LE>>()?.get())
    }

    /// Reads an `i128` (in little-endian order) and advances.
    pub fn i128(&mut self) -> Result<i128, ParserError> {
        Ok(self.copy::<I128<LE>>()?.get())
    }

    /// Reads an `f32` (in little-endian order) and advances.
    pub fn f32(&mut self) -> Result<f32, ParserError> {
        let bytes: [u8; 4] = self.copy()?;
        Ok(f32::from_le_bytes(bytes))
    }

    /// Reads an `f64` (in little-endian order) and advances.
    pub fn f64(&mut self) -> Result<f64, ParserError> {
        let bytes: [u8; 8] = self.copy()?;
        Ok(f64::from_le_bytes(bytes))
    }

    /// Skips over a NUL-terminated string.
    pub fn skip_strz(&mut self) -> Result<(), ParserError> {
        for i in 0..self.bytes.len() {
            if self.bytes[i] == 0 {
                self.bytes = &self.bytes[i + 1..];
                return Ok(());
            }
        }

        Err(ParserError::new())
    }

    /// Reads a NUL-terminated string, without checking that it is UTF-8 encoded.
    pub fn strz(&mut self) -> Result<&'a BStr, ParserError> {
        for i in 0..self.bytes.len() {
            if self.bytes[i] == 0 {
                let str_bytes = &self.bytes[..i];
                self.bytes = &self.bytes[i + 1..];
                return Ok(BStr::new(str_bytes));
            }
        }

        Err(ParserError::new())
    }

    /// Reads a length-prefixed string, without checking that it is UTF-8 encoded.
    pub fn strt_raw(&mut self) -> Result<&'a BStr, ParserError> {
        let len = self.u8()?;
        let bytes = self.bytes(len as usize)?;
        Ok(BStr::new(bytes))
    }

    /// Reads a length-prefixed string.
    pub fn strt(&mut self) -> Result<&'a str, ParserError> {
        let bytes = self.strt_raw()?;
        if let Ok(s) = core::str::from_utf8(bytes.as_ref()) {
            Ok(s)
        } else {
            Err(ParserError::new())
        }
    }

    /// Parses a 32-bit TypeIndex.
    pub fn type_index(&mut self) -> Result<TypeIndex, ParserError> {
        Ok(TypeIndex(self.u32()?))
    }

    /// Parses a generic number value.
    ///
    /// See Section 4, numeric leaves
    pub fn number(&mut self) -> Result<crate::types::number::Number<'a>, ParserError> {
        self.parse()
    }
}

/// A parser that can return mutable references to the data that it parses.
///
/// Most of the methods defined on `ParserMut` are equivalent to the same methods on `Parser`.
pub struct ParserMut<'a> {
    /// The remaining, unparsed data.
    pub bytes: &'a mut [u8],
}

#[allow(missing_docs)]
impl<'a> ParserMut<'a> {
    pub fn new(bytes: &'a mut [u8]) -> Self {
        Self { bytes }
    }

    pub fn peek_rest(&self) -> &[u8] {
        self.bytes
    }

    pub fn peek_rest_mut(&mut self) -> &mut [u8] {
        self.bytes
    }

    pub fn into_rest(self) -> &'a mut [u8] {
        self.bytes
    }

    pub fn is_empty(&self) -> bool {
        self.bytes.is_empty()
    }

    pub fn len(&self) -> usize {
        self.bytes.len()
    }

    pub fn skip(&mut self, n: usize) -> Result<(), ParserError> {
        if n <= self.bytes.len() {
            let b = take(&mut self.bytes);
            self.bytes = &mut b[n..];
            Ok(())
        } else {
            Err(ParserError::new())
        }
    }

    #[inline(always)]
    pub fn bytes(&mut self, n: usize) -> Result<&'a [u8], ParserError> {
        if self.bytes.len() < n {
            return Err(ParserError::new());
        }

        let (lo, hi) = take(&mut self.bytes).split_at_mut(n);
        self.bytes = hi;

        Ok(lo)
    }

    #[inline(always)]
    pub fn bytes_mut(&mut self, n: usize) -> Result<&'a mut [u8], ParserError> {
        if self.bytes.len() < n {
            return Err(ParserError::new());
        }

        let (lo, hi) = take(&mut self.bytes).split_at_mut(n);
        self.bytes = hi;

        Ok(lo)
    }

    #[inline(always)]
    pub fn get<T: FromBytes + Unaligned + Immutable + KnownLayout>(
        &mut self,
    ) -> Result<&'a T, ParserError> {
        let bytes = self.bytes(size_of::<T>())?;
        Ok(T::ref_from_bytes(bytes).unwrap())
    }

    #[inline(always)]
    pub fn get_mut<T: FromBytes + IntoBytes + Unaligned + Immutable + KnownLayout>(
        &mut self,
    ) -> Result<&'a mut T, ParserError> {
        let bytes = self.bytes_mut(size_of::<T>())?;
        Ok(T::mut_from_bytes(bytes).unwrap())
    }

    #[inline(always)]
    pub fn copy<T: FromBytes + Unaligned + Immutable>(&mut self) -> Result<T, ParserError> {
        let item = self.bytes(size_of::<T>())?;
        Ok(T::read_from_bytes(item).unwrap())
    }

    pub fn slice_mut<T: FromBytes + IntoBytes + Unaligned>(
        &mut self,
        len: usize,
    ) -> Result<&'a mut [T], ParserError> {
        let d = take(&mut self.bytes);
        if let Ok((lo, hi)) = <[T]>::mut_from_prefix_with_elems(d, len) {
            self.bytes = hi;
            Ok(lo)
        } else {
            Err(ParserError::new())
        }
    }

    pub fn array<const N: usize>(&mut self) -> Result<[u8; N], ParserError> {
        let s = self.bytes(N)?;
        Ok(<[u8; N]>::try_from(s).unwrap())
    }

    pub fn u8(&mut self) -> Result<u8, ParserError> {
        let b = self.bytes(1)?;
        Ok(b[0])
    }

    pub fn i8(&mut self) -> Result<i8, ParserError> {
        let b = self.bytes(1)?;
        Ok(b[0] as i8)
    }

    pub fn i16(&mut self) -> Result<i16, ParserError> {
        Ok(self.copy::<I16<LE>>()?.get())
    }

    pub fn i32(&mut self) -> Result<i32, ParserError> {
        Ok(self.copy::<I32<LE>>()?.get())
    }

    pub fn i64(&mut self) -> Result<i64, ParserError> {
        Ok(self.copy::<I64<LE>>()?.get())
    }

    pub fn u16(&mut self) -> Result<u16, ParserError> {
        Ok(self.copy::<U16<LE>>()?.get())
    }

    pub fn u32(&mut self) -> Result<u32, ParserError> {
        Ok(self.copy::<U32<LE>>()?.get())
    }

    pub fn u64(&mut self) -> Result<u64, ParserError> {
        Ok(self.copy::<U64<LE>>()?.get())
    }

    pub fn skip_strz(&mut self) -> Result<(), ParserError> {
        for i in 0..self.bytes.len() {
            if self.bytes[i] == 0 {
                let stolen_bytes = take(&mut self.bytes);
                self.bytes = &mut stolen_bytes[i + 1..];
                return Ok(());
            }
        }

        Err(ParserError::new())
    }

    pub fn strz(&mut self) -> Result<&'a mut BStr, ParserError> {
        for i in 0..self.bytes.len() {
            if self.bytes[i] == 0 {
                let stolen_bytes = take(&mut self.bytes);
                let (str_bytes, hi) = stolen_bytes.split_at_mut(i);
                self.bytes = &mut hi[1..];
                return Ok(str_bytes.as_bstr_mut());
            }
        }

        Err(ParserError::new())
    }

    pub fn type_index(&mut self) -> Result<TypeIndex, ParserError> {
        Ok(TypeIndex(self.u32()?))
    }

    pub fn skip_number(&mut self) -> Result<(), ParserError> {
        let mut p = Parser::new(self.bytes);
        let len_before = p.len();
        let _ = p.number()?;
        let num_len = len_before - p.len();
        self.skip(num_len)?;
        Ok(())
    }
}

/// Zero-sized type for representing parsing errors.
#[derive(Copy, Clone, Debug, Eq, PartialEq)]
pub struct ParserError;

impl ParserError {
    /// Constructor for ParserError, also logs an event. This is useful for setting breakpoints.
    #[cfg_attr(debug_assertions, inline(never))]
    #[cfg_attr(not(debug_assertions), inline(always))]
    pub fn new() -> Self {
        #[cfg(debug_assertions)]
        {
            tracing::debug!("ParserError");
        }
        Self
    }
}

impl Default for ParserError {
    fn default() -> Self {
        Self::new()
    }
}

impl std::error::Error for ParserError {}

impl std::fmt::Display for ParserError {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        fmt.write_str("Parsing error")
    }
}

/// Defines types that can parse from a byte stream
pub trait Parse<'a>
where
    Self: Sized,
{
    /// Parses an instance of `Self` from a `Parser`.
    /// This allows the caller to detect which bytes were not consumed at the end of the input.
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError>;

    /// Parses an instance of `Self` from a byte slice.
    fn parse(bytes: &'a [u8]) -> Result<Self, ParserError> {
        let mut p = Parser::new(bytes);
        Self::from_parser(&mut p)
    }
}

```

`codeview/src/parser/tests.rs`:

```rs
#![allow(clippy::redundant_pattern_matching)]

use super::*;
use bstr::ByteSlice;
use std::borrow::Cow;
use zerocopy::{Immutable, KnownLayout};

#[test]
fn empty() {
    assert!(Parser::new(&[]).is_empty());
    assert!(!Parser::new(&[42]).is_empty());
}

#[test]
fn len() {
    assert_eq!(Parser::new(&[]).len(), 0);
    assert_eq!(Parser::new(&[42]).len(), 1);
}

#[test]
fn ints() {
    let bytes = &[
        0x12, 0x34, // u16
        0x56, 0x78, 0xaa, 0xee, // u32
        0x55, 0x33, // u16
    ];

    let mut p = Parser::new(bytes);
    assert_eq!(p.len(), 8);
    assert_eq!(p.u16().unwrap(), 0x3412);
    assert_eq!(p.len(), 6);
    assert_eq!(p.u32().unwrap(), 0xeeaa_7856);
    assert_eq!(p.len(), 2);
    assert_eq!(p.u8().unwrap(), 0x55);
    assert_eq!(p.len(), 1);
    assert_eq!(p.i8().unwrap(), 0x33);
    assert_eq!(p.len(), 0);
    assert!(p.is_empty());

    // Integers do not need to be aligned.  Read some misaligned stuff.
    let mut p = Parser::new(bytes);
    p.u8().unwrap();
    assert_eq!(p.u16().unwrap(), 0x5634); // at index 1
    assert_eq!(p.u32().unwrap(), 0x55_ee_aa_78); // at index 3

    let mut p = Parser::new(&[1, 2, 3, 4]);
    assert_eq!(p.u32().unwrap(), 0x04_03_02_01);
    assert!(p.is_empty());

    let mut p = Parser::new(&[1, 2, 3, 4]);
    assert_eq!(p.type_index().unwrap(), TypeIndex(0x04_03_02_01));
    assert!(p.is_empty());
}

#[test]
fn strz() {
    assert!(Parser::new(&[]).strz().is_err());
    assert!(Parser::new(b"x").strz().is_err());

    assert_eq!(
        Parser::new(&[b'f', b'o', b'o', 0])
            .strz()
            .unwrap()
            .to_str()
            .unwrap(),
        "foo"
    );
}

#[test]
fn strz_bad_utf8() {
    // 0x80 is a continuation byte in UTF-8. It must be preceded by a leading byte.
    let buf: &[u8] = b"\x80 bad\0second string \xe2\x9c\x85\0";
    let mut p = Parser::new(buf);

    let bad_raw = p.strz().unwrap();
    assert!(bad_raw.to_str().is_err()); // is not valid UTF-8
    assert_eq!(bad_raw as &[u8], &[0x80, b' ', b'b', b'a', b'd']);
    let bad_lossy: Cow<str> = bad_raw.to_str_lossy();
    assert!(matches!(bad_lossy, Cow::Owned(_)));
    assert_eq!(bad_lossy, "\u{fffd} bad"); // U+FFFD is the replacement character

    let good = p.strz().unwrap();
    let good_str: &str = good.to_str().unwrap();
    assert_eq!(good_str, "second string ✅");

    assert!(p.is_empty());
}

#[test]
fn strt() {
    let buf = b"\x03abc123";
    let mut p = Parser::new(buf);
    let s = p.strt().unwrap();
    assert_eq!(s, "abc");
    assert_eq!(p.into_rest(), b"123");
}

#[test]
fn rest() {
    let mut p = Parser::new(&[1, 2, 3, 4, 5]);
    assert_eq!(p.u8().unwrap(), 1);

    assert_eq!(p.peek_rest(), &[2, 3, 4, 5]);

    let rest = p.take_rest();
    assert_eq!(rest, &[2, 3, 4, 5]);
    assert!(p.is_empty());
}

#[test]
fn into_rest() {
    let mut p = Parser::new(&[1, 2, 3, 4, 5]);
    assert_eq!(p.u8().unwrap(), 1);
    let rest = p.into_rest();
    assert_eq!(rest, &[2, 3, 4, 5]);
}

#[test]
fn needs() {
    let p = Parser::new(&[10, 20]);
    assert!(matches!(p.needs(0), Ok(_)));
    assert!(matches!(p.needs(2), Ok(_)));
    assert!(matches!(p.needs(3), Err(_)));
}

#[test]
fn skip() {
    let mut p = Parser::new(&[1, 2, 3, 4, 5]);
    p.skip(2).unwrap();
    assert_eq!(p.u16().unwrap(), 0x403);
}

#[derive(IntoBytes, FromBytes, Unaligned, KnownLayout, Immutable, PartialEq, Eq, Debug)]
#[repr(C)]
struct Bar {
    b: u8,
    a: u8,
    r: u8,
}

#[test]
fn get() {
    let buf = &[b'b', b'a', b'r', 4, 5];
    let mut p = Parser::new(buf);

    let bar: &Bar = p.get().unwrap();
    assert_eq!(bar.b, b'b');
    assert_eq!(bar.a, b'a');
    assert_eq!(bar.r, b'r');

    assert!(p.get::<Bar>().is_err());
}

#[test]
fn copy() {
    let buf = &[b'b', b'a', b'r', 4, 5];
    let mut p = Parser::new(buf);

    let bar: Bar = p.copy().unwrap();
    assert_eq!(bar.b, b'b');
    assert_eq!(bar.a, b'a');
    assert_eq!(bar.r, b'r');

    assert!(p.copy::<Bar>().is_err());
}

#[test]
fn slice() {
    let buf = b"ABC123()!zap";
    let mut p = Parser::new(buf);

    let bars: &[Bar] = p.slice(3).unwrap();
    assert_eq!(bars.len(), 3);
    assert_eq!(
        bars[0],
        Bar {
            b: b'A',
            a: b'B',
            r: b'C'
        }
    );
    assert_eq!(
        bars[1],
        Bar {
            b: b'1',
            a: b'2',
            r: b'3'
        }
    );
    assert_eq!(
        bars[2],
        Bar {
            b: b'(',
            a: b')',
            r: b'!'
        }
    );
    assert_eq!(p.into_rest(), b"zap");
}

```

`codeview/src/syms.rs`:

```rs
//! Decodes symbols records. Reads the "Global Symbols" stream and per-module symbol streams.
//!
//! # References
//!
//! * [`cvinfo.h`](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/include/cvinfo.h)
//! * [CodeView Symbols](https://llvm.org/docs/PDB/CodeViewSymbols.html)

pub mod builder;
mod iter;
mod kind;
mod offset_segment;

#[doc(inline)]
pub use self::{iter::*, kind::SymKind, offset_segment::*};

use crate::parser::{Number, Parse, Parser, ParserError, ParserMut};
use crate::types::{ItemId, ItemIdLe, TypeIndex, TypeIndexLe};
use bitflags::bitflags;
use bstr::BStr;
use std::fmt::Debug;
use std::mem::size_of;
use zerocopy::{FromBytes, I32, Immutable, IntoBytes, KnownLayout, LE, U16, U32, Unaligned};

/// This header is shared by many records that can start a symbol scope.
#[derive(IntoBytes, FromBytes, Unaligned, Immutable, KnownLayout, Default, Clone, Debug)]
#[repr(C)]
#[allow(missing_docs)]
pub struct BlockHeader {
    /// If the record containing this `BlockHeader` is a top-level symbol record (not nested within
    /// another symbol), then this value is 0.
    ///
    /// If the record containing this `BlockHeader` is nested within another symbol, then this
    /// value is the offset in the symbol stream of the parent record.
    pub p_parent: U32<LE>,

    /// Offset in symbol stream of the `P_END` which terminates this block scope.
    pub p_end: U32<LE>,
}

/// Used for the header of procedure symbols. This is used for `S_LPROC32`, `S_GPROC32`,
/// `S_LPROC32_ID`, etc.
///
/// See `PROCSYM32` in `cvinfo.h`.
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned, Debug)]
#[repr(C)]
#[allow(missing_docs)]
pub struct ProcFixed {
    /// This field is always zero; procedure symbols never have parents.
    pub p_parent: U32<LE>,

    /// The byte offset, relative to the start of this procedure record, of the `S_END` symbol that
    /// closes the scope of this symbol record.
    pub p_end: U32<LE>,

    pub p_next: U32<LE>,

    /// The length in bytes of the procedure instruction stream.
    pub proc_len: U32<LE>,

    /// The offset in bytes from the start of the procedure to the point where the stack frame has
    /// been set up. Parameter and frame variables can be viewed at this point.
    pub debug_start: U32<LE>,

    /// The offset in bytes from the start of the procedure to the point where the procedure is
    /// ready to return and has calculated its return value, if any. Frame and register variables
    /// can still be viewed.
    pub debug_end: U32<LE>,

    /// This field is either a `TypeIndex` that points into the TPI or is an `ItemId` that
    /// points into the IPI.
    ///
    /// This field is a `TypeIndex` for the following symbols: `S_GPROC32`, `S_LPROC32`,
    /// `S_LPROC32EX`, `S_LPROC32_DPC`, `S_GPROC32EX`.
    ///
    /// This field is a `ItemId` for `S_LPROC32_ID`, `S_GPROC32_ID`, `S_LPROC32_DPC_ID`,
    /// `S_GPROC32EX_ID`, `S_LPROC32EX_ID`.
    pub proc_type: TypeIndexLe,

    pub offset_segment: OffsetSegment,
    pub flags: u8,
}

bitflags! {
    /// Flags describing a procedure symbol.
    ///
    /// See: `CV_PROCFLAGS` in `cvinfo.h`.
    #[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
    pub struct ProcFlags: u8 {
        /// Frame pointer present.
        const NOFPO = 1 << 0;

        /// Interrupt return.
        const INT = 1 << 1;

        /// Far return.
        const FAR = 1 << 2;

        /// Does not return.
        const NEVER = 1 << 3;

        /// Label isn't fallen into.
        const NOTREACHED = 1 << 4;

        /// Custom calling convention.
        const CUST_CALL = 1 << 5;

        /// Marked as `noinline`.
        const NOINLINE = 1 << 6;

        /// Has debug information for optimized code.
        const OPTDBGINFO = 1 << 7;
    }
}

/// Used for `S_LPROC32` and `S_GPROC32`.
///
/// These records are found in Module Symbol Streams. They are very important; they describe the
/// beginning of a function (procedure), and they contain other symbols recursively (are a
/// "symbol scope"). The end of the sequence is terminated with an `S_END` symbol.
///
/// This is equivalent to the `PROCSYM32` type defined in `cvinfo.h`. This symbol begins with a
/// `BLOCKSYM` header
///
/// # References
/// * See `PROCSYM32` in `cvinfo.h`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Proc<'a> {
    pub fixed: &'a ProcFixed,
    pub name: &'a BStr,
}

impl<'a> Proc<'a> {
    /// View the procedure `flags` field as bit flags.
    pub fn flags(&self) -> ProcFlags {
        ProcFlags::from_bits_retain(self.fixed.flags)
    }
}

impl<'a> Parse<'a> for Proc<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

// Basic framing and decoding test
#[test]
fn test_parse_proc() {
    #[rustfmt::skip]
    let data = &[
        /* 0x0000 */ 0x2e, 0, 0x10, 0x11,       // size and S_GPROC32
        /* 0x0004 */ 0, 0, 0, 0,                // p_parent
        /* 0x0008 */ 0x40, 0, 0, 0,             // p_end
        /* 0x000c */ 0, 0, 0, 0,                // p_next
        /* 0x0010 */ 42, 0, 0, 0,               // proc_len
        /* 0x0014 */ 10, 0, 0, 0,               // debug_start
        /* 0x0018 */ 20, 0, 0, 0,               // debug_end
        /* 0x001c */ 0xee, 0x10, 0, 0,          // proc_type
        /* 0x0020 */ 0xcc, 0x1, 0, 0,           // offset
        /* 0x0024 */ 1, 0, 0x50, b'm',          // segment, flags, beginning of name
        /* 0x0028 */ b'e', b'm', b's', b'e',    // name
        /* 0x002c */ b't', 0, 0xf1, 0xf2,       // end and padding
        /* 0x0030 */ 2, 0, 6, 0                 // size = 2 and S_END
        /* 0x0034 */
    ];

    let mut i = SymIter::new(data);

    let s0 = i.next().unwrap();
    assert_eq!(s0.kind, SymKind::S_GPROC32);
    assert_eq!(s0.data.len(), 0x2c);

    match s0.parse().unwrap() {
        SymData::Proc(proc) => {
            assert_eq!(proc.fixed.p_parent.get(), 0);
            assert_eq!(proc.fixed.p_end.get(), 0x40);
            assert_eq!(proc.name, "memset");
        }
        _ => panic!(),
    }

    let s1 = i.next().unwrap();
    assert_eq!(s1.kind, SymKind::S_END);
    assert!(s1.data.is_empty());
}

/// `S_GMANPROC`, `S_LMANPROC` - Managed Procedure Start
///
/// See `MANPROCSYM` in `cvinfo.h`.
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned, Debug)]
#[repr(C)]
#[allow(missing_docs)]
pub struct ManagedProcFixed {
    pub p_parent: U32<LE>,
    pub p_end: U32<LE>,
    pub p_next: U32<LE>,
    pub proc_len: U32<LE>,
    pub debug_start: U32<LE>,
    pub debug_end: U32<LE>,
    pub token: U32<LE>,
    pub offset_segment: OffsetSegment,
    pub flags: u8,
    pub return_reg: U16<LE>,
}

/// `S_GMANPROC`, `S_LMANPROC` - Managed Procedure Start
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct ManagedProc<'a> {
    pub fixed: &'a ManagedProcFixed,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for ManagedProc<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Clone, Default)]
#[allow(missing_docs)]
pub struct ThunkFixed {
    pub block: BlockHeader,
    pub p_next: U32<LE>,
    pub offset_segment: OffsetSegment,
    pub length: U16<LE>,
    pub thunk_ordinal: u8,
    // name: strz
    // variant: [u8]
}

#[allow(missing_docs)]
pub struct Thunk<'a> {
    pub fixed: &'a ThunkFixed,
    pub name: &'a BStr,
    pub variant: &'a [u8],
}

impl<'a> Parse<'a> for Thunk<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
            variant: p.take_rest(),
        })
    }
}

/// Describes the start of every symbol record.
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Clone, Default)]
pub struct SymHeader {
    /// The length in bytes of the record.
    ///
    /// This length _does not_ count the length itself, but _does_ count the `kind` that follows it.
    /// Therefore, all well-formed symbol records have `len >= 2`.
    pub len: U16<LE>,

    /// The kind of the symbol.  See `SymKind`.
    pub kind: U16<LE>,
}

/// Points to one symbol record in memory and gives its kind.
#[derive(Clone)]
pub struct Sym<'a> {
    /// The kind of the symbol.
    pub kind: SymKind,
    /// The contents of the record. This slice does _not_ include the `len` or `kind` fields.
    pub data: &'a [u8],
}

impl<'a> Sym<'a> {
    /// Parse the payload of the symbol.
    pub fn parse(&self) -> Result<SymData<'a>, ParserError> {
        SymData::parse(self.kind, self.data)
    }

    /// Parses the payload of the symbol with a type chosen by the caller.
    ///
    /// This is useful when the caller has already tested `Sym::kind` and knows the type of the
    /// payload.
    pub fn parse_as<T>(&self) -> Result<T, ParserError>
    where
        T: Parse<'a>,
    {
        let mut p = Parser::new(self.data);
        p.parse::<T>()
    }
}

impl<'a> Debug for Sym<'a> {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(fmt, "{:?}", self.kind)
    }
}

/// Points to one symbol record in memory and gives its kind. Allows mutation of the contents of
/// the symbol record.
pub struct SymMut<'a> {
    /// The kind of the symbol.
    pub kind: SymKind,
    /// The contents of the record. This slice does _not_ include the `len` or `kind` fields.
    pub data: &'a mut [u8],
}

/// PUBSYM32
///
/// ```text
/// typedef struct PUBSYM32 {
///     unsigned short  reclen;     // Record length
///     unsigned short  rectyp;     // S_PUB32
///     CV_PUBSYMFLAGS  pubsymflags;
///     CV_uoff32_t     off;
///     unsigned short  seg;
///     unsigned char   name[1];    // Length-prefixed name
/// } PUBSYM32;
/// ```
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Pub<'a> {
    pub fixed: &'a PubFixed,
    pub name: &'a BStr,
}

impl<'a> Pub<'a> {
    /// Gets the `segment:offset` of this symbol.
    pub fn offset_segment(&self) -> OffsetSegment {
        self.fixed.offset_segment
    }
}

#[allow(missing_docs)]
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct PubFixed {
    pub flags: U32<LE>,
    pub offset_segment: OffsetSegment,
    // name: &str
}

#[allow(missing_docs)]
impl<'a> Parse<'a> for Pub<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

impl<'a> Pub<'a> {
    /// Parses `S_PUB32_ST`
    pub fn parse_st(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strt_raw()?,
        })
    }
}

/// Parsed form of `S_CONSTANT`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Constant<'a> {
    pub type_: TypeIndex,
    pub value: Number<'a>,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Constant<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            type_: p.type_index()?,
            value: p.number()?,
            name: p.strz()?,
        })
    }
}

/// Parsed form of `S_CONSTANT`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct ManagedConstant<'a> {
    pub token: u32,
    pub value: Number<'a>,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for ManagedConstant<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            token: p.u32()?,
            value: p.number()?,
            name: p.strz()?,
        })
    }
}

/// Several symbols use this structure: `S_PROCREF`, `S_LPROCREF`, `S_DATAREF`. These symbols
/// are present in the Global Symbol Stream, not in module symbol streams.
///
/// These `S_*REF` symbols tell you where to find a specific global symbol, but they do not directly
/// describe the symbol. Instead, you have to load the corresponding module
///
///
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct RefSym2<'a> {
    pub header: &'a RefSym2Fixed,
    pub name: &'a BStr,
}

#[allow(missing_docs)]
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct RefSym2Fixed {
    /// Checksum of the name (called `SUC` in C++ code)
    ///
    /// This appears to be set to zero.
    pub name_checksum: U32<LE>,

    /// Offset of actual symbol in $$Symbols
    ///
    /// This is the byte offset into the module symbol stream for this symbol. The `module_index`
    /// field tells you which symbol stream to load, to resolve this value.
    pub symbol_offset: U32<LE>,

    /// The 1-based index of the module containing the actual symbol.
    ///
    /// This value is 1-based. Subtract 1 from this value before indexing into a zero-based module array.
    pub module_index: U16<LE>,
    // pub name: strz, // hidden name made a first class member
}

impl<'a> Parse<'a> for RefSym2<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            header: p.get()?,
            name: p.strz()?,
        })
    }
}

#[allow(missing_docs)]
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct ThreadStorageFixed {
    pub type_: TypeIndexLe,
    pub offset_segment: OffsetSegment,
}

/// Record data for `S_LTHREAD32` and `S_GTHREAD32`. These describes thread-local storage.
///
/// Thread-local storage is declared using `__declspec(thread)` or `thread_static`, in C++.
#[derive(Clone, Debug)]
pub struct ThreadStorageData<'a> {
    #[allow(missing_docs)]
    pub header: &'a ThreadStorageFixed,
    #[allow(missing_docs)]
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for ThreadStorageData<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            header: p.get()?,
            name: p.strz()?,
        })
    }
}

#[allow(missing_docs)]
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct DataFixed {
    pub type_: TypeIndexLe,
    pub offset_segment: OffsetSegment,
}

/// Record data for `S_LDATA32` and `S_GDATA32`. These describe global storage.
#[allow(missing_docs)]
#[derive(Clone)]
pub struct Data<'a> {
    pub header: &'a DataFixed,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Data<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            header: p.get()?,
            name: p.strz()?,
        })
    }
}

impl<'a> Debug for Data<'a> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(
            f,
            "Data: {} {:?} {}",
            self.header.offset_segment,
            self.header.type_.get(),
            self.name
        )
    }
}

/// Record data for `S_UDT` symbols
#[derive(Clone, Debug)]
pub struct Udt<'a> {
    /// The type of the UDT
    pub type_: TypeIndex,
    /// Name
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Udt<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            type_: p.type_index()?,
            name: p.strz()?,
        })
    }
}

/// `S_OBJNAME`
#[derive(Clone, Debug)]
pub struct ObjectName<'a> {
    /// A robust signature that will change every time that the module will be compiled or
    /// different in any way. It should be at least a CRC32 based upon module name and contents.
    pub signature: u32,
    /// Full path of the object file.
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for ObjectName<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            signature: p.u32()?,
            name: p.strz()?,
        })
    }
}

/// `S_COMPILE3`
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[repr(C)]
#[allow(missing_docs)]
pub struct Compile3Fixed {
    pub flags: U32<LE>,
    pub machine: U16<LE>,
    pub frontend_major: U16<LE>,
    pub frontend_minor: U16<LE>,
    pub frontend_build: U16<LE>,
    pub frontend_qfe: U16<LE>,
    pub ver_major: U16<LE>,
    pub ver_minor: U16<LE>,
    pub ver_build: U16<LE>,
    pub ver_qfe: U16<LE>,
    // name: strz
}

/// `S_COMPILE3`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Compile3<'a> {
    pub fixed: &'a Compile3Fixed,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Compile3<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

/// `S_FRAMEPROC`: This symbol is used for indicating a variety of extra information regarding a
/// procedure and its stack frame. If any of the flags are non-zero, this record should be added
/// to the symbols for that procedure.
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct FrameProc {
    /// Count of bytes in the whole stack frame.
    frame_size: U32<LE>,
    /// Count of bytes in the frame allocated as padding.
    pad_size: U32<LE>,
    /// Offset of pad bytes from the base of the frame.
    pad_offset: U32<LE>,
    /// Count of bytes in frame allocated for saved callee-save registers.
    save_regs_size: U32<LE>,
    offset_exception_handler: U32<LE>,
    exception_handler_section: U16<LE>,
    padding: U16<LE>,
    flags: U32<LE>,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct RegRelFixed {
    pub offset: U32<LE>,
    pub ty: TypeIndexLe,
    pub register: U16<LE>,
    // name: strz
}

/// `S_REGREGL32`: This symbol specifies symbols that are allocated relative to a register.
/// This should be used on all platforms besides x86 and on x86 when the register is not a form of EBP.
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct RegRel<'a> {
    pub fixed: &'a RegRelFixed,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for RegRel<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

/// Block Start: This symbol specifies the start of an inner block of lexically scoped symbols.
/// The lexical scope is terminated by a matching `S_END` symbol.
///
/// This symbol should only be nested (directly or indirectly) within a function symbol
/// (`S_GPROC32`, `S_LPROC32`, etc.).
///
/// See `BLOCKSYM32`
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct Block<'a> {
    pub fixed: &'a BlockFixed,
    pub name: &'a BStr,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct BlockFixed {
    /// Header of the block
    pub header: BlockHeader,

    /// Length in bytes of the scope of this block within the executable code stream.
    pub length: U32<LE>,

    pub offset_segment: OffsetSegment,
}

impl<'a> Parse<'a> for Block<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

/// Local Symbol: This symbol defines a local variable.
///
/// This symbol must be nested (directly or indirectly) within a function symbol. It must be
/// followed by more range descriptions.
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct Local<'a> {
    pub fixed: &'a LocalFixed,
    pub name: &'a BStr,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct LocalFixed {
    pub ty: TypeIndexLe,
    /// The spec says this is a 32-bit flags field, but the actual records show that this is 16-bit.
    pub flags: U16<LE>,
    // name: strz
}

impl<'a> Parse<'a> for Local<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

/// Represents an address range, used for optimized code debug info
///
/// See `CV_LVAR_ADDR_RANGE`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct LVarAddrRange {
    /// Start of the address range
    pub start: OffsetSegment,
    /// Size of the range in bytes.
    pub range_size: U16<LE>,
}

/// Represents the holes in overall address range, all address is pre-bbt.
/// it is for compress and reduce the amount of relocations need.
///
/// See `CV_LVAR_ADDR_GAP`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct LVarAddrGap {
    /// relative offset from the beginning of the live range.
    pub gap_start_offset: U16<LE>,
    /// length of this gap, in bytes
    pub range_size: U16<LE>,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct DefRangeFixed {
    /// DIA program to evaluate the value of the symbol
    pub program: U32<LE>,

    pub range: LVarAddrRange,
    // gaps: [LVAddrGap]
}

/// `S_DEFRANGE`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct DefRange<'a> {
    pub fixed: &'a DefRangeFixed,
    pub gaps: &'a [LVarAddrGap],
}

impl<'a> Parse<'a> for DefRange<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let fixed = p.get()?;
        let gaps = p.slice(p.len() / size_of::<LVarAddrGap>())?;
        Ok(Self { fixed, gaps })
    }
}

/// `S_DEFRANGE_FRAMEPOINTER_REL`: A live range of frame variable
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct DefRangeSymFramePointerRelFixed {
    pub offset_to_frame_pointer: U32<LE>,

    /// Range of addresses where this program is valid
    pub range: LVarAddrRange,
}

/// `S_DEFRANGE_FRAMEPOINTER_REL`: A live range of frame variable
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct DefRangeSymFramePointerRel<'a> {
    pub fixed: &'a DefRangeSymFramePointerRelFixed,
    // The value is not available in following gaps.
    pub gaps: &'a [LVarAddrGap],
}

impl<'a> Parse<'a> for DefRangeSymFramePointerRel<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let fixed = p.get()?;
        let gaps = p.slice(p.len() / size_of::<LVarAddrGap>())?;
        Ok(Self { fixed, gaps })
    }
}

/// Attributes for a register range
///
/// See `CV_RANGEATTR`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct RangeAttrLe {
    // unsigned short  maybe : 1;    // May have no user name on one of control flow path.
    // unsigned short  padding : 15; // Padding for future use.
    pub value: U16<LE>,
}

/// `S_DEFRANGE_REGISTER` - A live range of en-registed variable
///
/// See `DEFRANGESYMREGISTER`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct DefRangeRegisterFixed {
    /// Register to hold the value of the symbol
    pub reg: U16<LE>,
    // Attribute of the register range.
    pub attr: RangeAttrLe,
}

/// `S_DEFRANGE_REGISTER`
///
/// See `DEFRANGESYMREGISTER`
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct DefRangeRegister<'a> {
    pub fixed: &'a DefRangeRegisterFixed,
    pub gaps: &'a [u8],
}

impl<'a> Parse<'a> for DefRangeRegister<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            gaps: p.take_rest(),
        })
    }
}

/// `S_DEFRANGE_REGISTER_REL`
#[allow(missing_docs)]
#[derive(Debug, Clone)]
pub struct DefRangeRegisterRel<'a> {
    pub fixed: &'a DefRangeRegisterRelFixed,

    /// The value is not available in following gaps.
    pub gaps: &'a [u8],
}

/// `S_DEFRANGE_REGISTER_REL`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct DefRangeRegisterRelFixed {
    /// Register to hold the base pointer of the symbol
    pub base_reg: U16<LE>,

    /// ```text
    /// unsigned short  spilledUdtMember : 1;   // Spilled member for s.i.
    /// unsigned short  padding          : 3;   // Padding for future use.
    /// unsigned short  offsetParent     : CV_OFFSET_PARENT_LENGTH_LIMIT;  // Offset in parent variable.
    /// ```
    pub flags: U16<LE>,

    /// offset to register
    pub base_pointer_offset: I32<LE>,

    /// Range of addresses where this program is valid
    pub range: LVarAddrRange,
}

impl<'a> Parse<'a> for DefRangeRegisterRel<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            gaps: p.take_rest(),
        })
    }
}

/// `S_DEFRANGE_FRAMEPOINTER_REL_FULL_SCOPE`
///
/// A frame variable valid in all function scope.
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Clone, Debug)]
#[repr(C)]
pub struct DefRangeFramePointerRelFullScope {
    /// offset to frame pointer
    pub frame_pointer_offset: I32<LE>,
}

/// `S_DEFRANGE_SUBFIELD_REGISTER`
///
/// See `DEFRANGESYMSUBFIELDREGISTER`
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct DefRangeSubFieldRegister<'a> {
    pub fixed: &'a DefRangeSubFieldRegisterFixed,
    pub gaps: &'a [u8],
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct DefRangeSubFieldRegisterFixed {
    pub reg: U16<LE>,
    pub attr: RangeAttrLe,
    pub flags: U32<LE>,
    pub range: LVarAddrRange,
}

impl<'a> Parse<'a> for DefRangeSubFieldRegister<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            gaps: p.take_rest(),
        })
    }
}

/// `S_GMANPROC`, `S_LMANPROC`, `S_GMANPROCIA64`, `S_LMANPROCIAC64`
///
/// See `MANPROCSYM`
pub struct ManProcSym<'a> {
    #[allow(missing_docs)]
    pub fixed: &'a ManProcSymFixed,
    #[allow(missing_docs)]
    pub name: &'a BStr,
}

/// MSIL / CIL token value
pub type TokenIdLe = U32<LE>;

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct ManProcSymFixed {
    pub block: BlockHeader,
    /// pointer to next symbol
    pub pnext: U32<LE>,
    /// Proc length
    pub len: U32<LE>,
    /// Debug start offset
    pub dbg_start: U32<LE>,
    /// Debug end offset
    pub dbg_end: U32<LE>,
    // COM+ metadata token for method
    pub token: TokenIdLe,
    pub off: U32<LE>,
    pub seg: U16<LE>,
    pub flags: u8, // CV_PROCFLAGS: Proc flags
    pub padding: u8,
    // Register return value is in (may not be used for all archs)
    pub ret_reg: U16<LE>,
    // name: strz    // optional name field
}

impl<'a> Parse<'a> for ManProcSym<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

/// `S_TRAMPOLINE`
#[derive(Clone, Debug)]
pub struct Trampoline<'a> {
    /// Fixed header
    pub fixed: &'a TrampolineFixed,

    /// Data whose interpretation depends on `tramp_type`
    pub rest: &'a [u8],
}

/// `S_TRAMPOLINE`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Clone, Debug)]
pub struct TrampolineFixed {
    /// trampoline sym subtype
    pub tramp_type: U16<LE>,
    /// size of the thunk
    pub cb_thunk: U16<LE>,
    /// offset of the thunk
    pub off_thunk: U32<LE>,
    /// offset of the target of the thunk
    pub off_target: U32<LE>,
    /// section index of the thunk
    pub sect_thunk: U16<LE>,
    /// section index of the target of the thunk
    pub sect_target: U16<LE>,
}

impl<'a> Parse<'a> for Trampoline<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            rest: p.take_rest(),
        })
    }
}

/// `S_BUILDINFO` - Build info for a module
///
/// This record is present only in module symbol streams.
#[derive(Clone, Debug)]
pub struct BuildInfo {
    /// ItemId points to an `LF_BUILDINFO` record in IPI
    pub item: ItemId,
}

impl<'a> Parse<'a> for BuildInfo {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self { item: p.u32()? })
    }
}

/// `S_UNAMESPACE` - Using Namespace
#[derive(Clone, Debug)]
pub struct UsingNamespace<'a> {
    /// The namespace, e.g. `std`
    pub namespace: &'a BStr,
}

impl<'a> Parse<'a> for UsingNamespace<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            namespace: p.strz()?,
        })
    }
}

/// `S_LABEL32`
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct Label<'a> {
    pub fixed: &'a LabelFixed,
    pub name: &'a BStr,
}

/// `S_LABEL32`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct LabelFixed {
    pub offset_segment: OffsetSegment,
    pub flags: u8,
}

impl<'a> Parse<'a> for Label<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

/// Data for `S_CALLERS`, `S_CALLEES`, `S_INLINEES`.
#[derive(Clone, Debug)]
pub struct FunctionList<'a> {
    /// The list of functions, in the IPI. Each is either `LF_FUNC_ID` or `LF_MFUNC_ID`.
    pub funcs: &'a [ItemIdLe],

    /// Counts for each function.
    ///
    /// The values in `counts` parallel the items in `funcs`, but the length of `invocations` can be
    /// less than the length of `funcs`. Unmatched counts are assumed to be zero.
    ///
    /// This is empty for `S_INLINEES`.
    pub counts: &'a [U32<LE>],
}

impl<'a> Parse<'a> for FunctionList<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let num_funcs = p.u32()? as usize;
        let funcs: &[ItemIdLe] = p.slice(num_funcs)?;
        let num_counts = num_funcs.min(p.len() / size_of::<U32<LE>>());
        let counts = p.slice(num_counts)?;
        Ok(Self { funcs, counts })
    }
}

/// `S_INLINESITE`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct InlineSite<'a> {
    pub fixed: &'a InlineSiteFixed,
    /// an array of compressed binary annotations.
    pub binary_annotations: &'a [u8],
}

/// `S_INLINESITE`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct InlineSiteFixed {
    pub block: BlockHeader,
    pub inlinee: ItemIdLe,
}

impl<'a> Parse<'a> for InlineSite<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            binary_annotations: p.take_rest(),
        })
    }
}

/// `S_INLINESITE2`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct InlineSite2<'a> {
    pub fixed: &'a InlineSite2Fixed,
    /// an array of compressed binary annotations.
    pub binary_annotations: &'a [u8],
}

/// `S_INLINESITE`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct InlineSite2Fixed {
    pub block: BlockHeader,
    pub inlinee: ItemIdLe,
    pub invocations: U32<LE>,
}

impl<'a> Parse<'a> for InlineSite2<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            binary_annotations: p.take_rest(),
        })
    }
}

/// `S_FRAMECOOKIE`: Symbol for describing security cookie's position and type
// (raw, xor'd with esp, xor'd with ebp).
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct FrameCookie {
    /// Frame relative offset
    pub offset: I32<LE>,
    pub reg: U16<LE>,
    pub cookie_type: u8,
    pub flags: u8,
}

/// `S_CALLSITEINFO`
///
/// Symbol for describing indirect calls when they are using
/// a function pointer cast on some other type or temporary.
/// Typical content will be an LF_POINTER to an LF_PROCEDURE
/// type record that should mimic an actual variable with the
/// function pointer type in question.
///
/// Since the compiler can sometimes tail-merge a function call
/// through a function pointer, there may be more than one
/// S_CALLSITEINFO record at an address.  This is similar to what
/// you could do in your own code by:
///
/// ```text
///  if (expr)
///      pfn = &function1;
///  else
///      pfn = &function2;
///
///  (*pfn)(arg list);
/// ```
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct CallSiteInfo {
    pub offset: OffsetSegment,
    pub padding: U16<LE>,
    pub func_type: TypeIndexLe,
}

/// `S_HEAPALLOCSITE`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct HeapAllocSite {
    pub offset: OffsetSegment,
    /// length of heap allocation call instruction
    pub instruction_size: U16<LE>,
    pub func_type: TypeIndexLe,
}

/// `S_ANNOTATION`
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Annotation<'a> {
    pub fixed: &'a AnnotationFixed,
    pub strings: &'a [u8],
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct AnnotationFixed {
    pub offset: OffsetSegment,
    pub num_strings: U16<LE>,
}

impl<'a> Parse<'a> for Annotation<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            strings: p.take_rest(),
        })
    }
}

impl<'a> Annotation<'a> {
    /// Iterates the strings stored in the annotation.
    pub fn iter_strings(&self) -> AnnotationIterStrings<'a> {
        AnnotationIterStrings {
            num_strings: self.fixed.num_strings.get(),
            bytes: self.strings,
        }
    }
}

/// Iterator state for [`Annotation::iter_strings`].
#[allow(missing_docs)]
pub struct AnnotationIterStrings<'a> {
    pub num_strings: u16,
    pub bytes: &'a [u8],
}

impl<'a> Iterator for AnnotationIterStrings<'a> {
    type Item = &'a BStr;

    fn next(&mut self) -> Option<Self::Item> {
        if self.num_strings == 0 {
            return None;
        }

        self.num_strings -= 1;
        let mut p = Parser::new(self.bytes);
        let s = p.strz().ok()?;
        self.bytes = p.into_rest();
        Some(s)
    }
}

/// Hot-patched function
#[derive(Clone, Debug)]
pub struct HotPatchFunc<'a> {
    /// ID of the function
    pub func: ItemId,

    /// The name of the function
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for HotPatchFunc<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            func: p.u32()?,
            name: p.strz()?,
        })
    }
}

/// Data for `S_ARMSWITCHTABLE`.
///
/// This describes a switch table (jump table).
///
/// MSVC generates this symbol only when targeting ARM64.
/// LLVM generates this symbol for all target architectures.
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Clone)]
pub struct ArmSwitchTable {
    /// Section-relative offset to the base for switch offsets
    pub offset_base: U32<LE>,
    /// Section index of the base for switch offsets
    pub sect_base: U16<LE>,
    /// type of each entry
    pub switch_type: U16<LE>,
    /// Section-relative offset to the table branch instruction
    pub offset_branch: U32<LE>,
    /// Section-relative offset to the start of the table
    pub offset_table: U32<LE>,
    /// Section index of the table branch instruction
    pub sect_branch: U16<LE>,
    /// Section index of the table
    pub sect_table: U16<LE>,
    /// number of switch table entries
    pub num_entries: U32<LE>,
}

impl ArmSwitchTable {
    /// The `[segment:offset]` of the jump base.
    ///
    /// This is the base address of the target of the jump. The value stored within the jump table
    /// entry is added to this base.
    ///
    /// LLVM often generates tables where `base` and `table` have the same address, but this is
    /// not necessarily true for all tables.
    pub fn base(&self) -> OffsetSegment {
        OffsetSegment {
            offset: self.offset_base,
            segment: self.sect_base,
        }
    }

    /// The `[segment:offset]` of the branch instruction.
    pub fn branch(&self) -> OffsetSegment {
        OffsetSegment {
            offset: self.offset_branch,
            segment: self.sect_branch,
        }
    }

    /// The `[segment:offset]` of the jump table.
    pub fn table(&self) -> OffsetSegment {
        OffsetSegment {
            offset: self.offset_table,
            segment: self.sect_table,
        }
    }

    /// The type of switch table (2-byte, 4-byte, etc.).
    pub fn switch_type(&self) -> ArmSwitchType {
        ArmSwitchType(self.switch_type.get())
    }

    /// The number of entries in the jump table.
    pub fn num_entries(&self) -> u32 {
        self.num_entries.get()
    }
}

impl core::fmt::Debug for ArmSwitchTable {
    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
        f.debug_struct("ArmSwitchTable")
            .field("base", &self.base())
            .field("branch", &self.branch())
            .field("table", &self.table())
            .field("switch_type", &self.switch_type())
            .field("num_entries", &self.num_entries())
            .finish()
    }
}

/// The type of switch table, as defined by `S_ARMSWITCHTABLE`.
#[derive(Copy, Clone, Eq, PartialEq)]
pub struct ArmSwitchType(pub u16);

impl ArmSwitchType {
    /// Signed 1-byte offset
    pub const INT1: ArmSwitchType = ArmSwitchType(0);
    /// Unsigned 1-byte offset
    pub const UINT1: ArmSwitchType = ArmSwitchType(1);
    /// Signed 2-byte offset
    pub const INT2: ArmSwitchType = ArmSwitchType(2);
    /// Unsigned 2-byte offset
    pub const UINT2: ArmSwitchType = ArmSwitchType(3);
    /// Signed 4-byte offset
    pub const INT4: ArmSwitchType = ArmSwitchType(4);
    /// Unsigned 4-byte offset
    pub const UINT4: ArmSwitchType = ArmSwitchType(5);
    /// Absolute pointer (no base)
    pub const POINTER: ArmSwitchType = ArmSwitchType(6);
    /// Unsigned 1-byte offset, shift left by 1
    pub const UINT1SHL1: ArmSwitchType = ArmSwitchType(7);
    /// Unsigned 2-byte offset, shift left by 2
    pub const UINT2SHL1: ArmSwitchType = ArmSwitchType(8);
    /// Signed 1-byte offset, shift left by 1
    pub const INT1SHL1: ArmSwitchType = ArmSwitchType(9);
    /// Signed 2-byte offset, shift left by 1
    pub const INT2SHL1: ArmSwitchType = ArmSwitchType(10);
    // CV_SWT_TBB          = CV_SWT_UINT1SHL1,
    // CV_SWT_TBH          = CV_SWT_UINT2SHL1,
}

impl core::fmt::Debug for ArmSwitchType {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        static NAMES: [&str; 11] = [
            "INT1",
            "UINT1",
            "INT2",
            "UINT2",
            "INT4",
            "UINT4",
            "POINTER",
            "UINT1SHL1",
            "UINT2SHL1",
            "INT1SHL1",
            "INT2SHL1",
        ];

        if let Some(&s) = NAMES.get(self.0 as usize) {
            f.write_str(s)
        } else {
            write!(f, "??{}", self.0)
        }
    }
}

// Trampoline subtypes

/// Incremental thunks
pub const TRAMPOLINE_KIND_INCREMENTAL: u16 = 0;
/// Branch island thunks
pub const TRAMPOLINE_KIND_BRANCH_ISLAND: u16 = 1;

/// The fixed header of `S_COFFGROUP` symbols.
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct CoffGroupFixed {
    /// Size in bytes of the coff group
    pub cb: U32<LE>,
    /// Characteristics flags. These are the same as the COFF section characteristics.
    ///
    /// See: <https://learn.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-image_section_header>
    pub characteristics: U32<LE>,
    /// Location of the COFF group
    pub off_seg: OffsetSegment,
}

/// For `S_COFFGROUP`.
///
/// `S_COFFGROUP` records are present in the `* Linker *` special module. These records describe
/// contiguous subsections within COFF sections. For example, `.text$mn` is a COFF group within
/// the `.text` segment.
#[derive(Clone, Debug)]
pub struct CoffGroup<'a> {
    /// The fixed-size header
    pub fixed: &'a CoffGroupFixed,
    /// The name of the COFF group
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for CoffGroup<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

/// For `S_SECTION`
#[derive(Clone, Debug)]
pub struct Section<'a> {
    /// The fixed-size header
    pub fixed: &'a SectionFixed,
    /// The name of the section
    pub name: &'a BStr,
}

/// The fixed header of `S_SECTION` symbols.
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct SectionFixed {
    /// Section number
    pub section: U16<LE>,
    /// Alignment of this section (power of 2)
    pub align: u8,
    /// Reserved
    pub reserved: u8,
    /// RVA of this section base
    pub rva: U32<LE>,
    /// Size in bytes of this section
    pub cb: U32<LE>,
    /// Section characteristics (bit flags)
    pub characteristics: U32<LE>,
}

impl<'a> Parse<'a> for Section<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
        })
    }
}

/// Parsed data from a symbol record
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub enum SymData<'a> {
    Unknown,
    ObjName(ObjectName<'a>),
    Compile3(Compile3<'a>),
    Proc(Proc<'a>),
    Udt(Udt<'a>),
    Constant(Constant<'a>),
    ManagedConstant(ManagedConstant<'a>),
    RefSym2(RefSym2<'a>),
    Data(Data<'a>),
    ThreadData(ThreadStorageData<'a>),
    Pub(Pub<'a>),
    End,
    FrameProc(&'a FrameProc),
    RegRel(RegRel<'a>),
    Block(Block<'a>),
    Local(Local<'a>),
    DefRange(DefRange<'a>),
    DefRangeFramePointerRel(DefRangeSymFramePointerRel<'a>),
    DefRangeRegister(DefRangeRegister<'a>),
    DefRangeRegisterRel(DefRangeRegisterRel<'a>),
    DefRangeFramePointerRelFullScope(&'a DefRangeFramePointerRelFullScope),
    DefRangeSubFieldRegister(DefRangeSubFieldRegister<'a>),
    Trampoline(Trampoline<'a>),
    BuildInfo(BuildInfo),
    UsingNamespace(UsingNamespace<'a>),
    InlineSiteEnd,
    Label(Label<'a>),
    FunctionList(FunctionList<'a>),
    InlineSite(InlineSite<'a>),
    InlineSite2(InlineSite2<'a>),
    FrameCookie(&'a FrameCookie),
    CallSiteInfo(&'a CallSiteInfo),
    HeapAllocSite(&'a HeapAllocSite),
    ManagedProc(ManagedProc<'a>),
    Annotation(Annotation<'a>),
    HotPatchFunc(HotPatchFunc<'a>),
    CoffGroup(CoffGroup<'a>),
    ArmSwitchTable(&'a ArmSwitchTable),
    Section(Section<'a>),
}

impl<'a> SymData<'a> {
    /// Parses a symbol record. The caller has already parsed the length and kind of the record.
    /// The `data` parameter does not include the length or kind.
    pub fn parse(kind: SymKind, data: &'a [u8]) -> Result<Self, ParserError> {
        let mut p = Parser::new(data);
        Self::from_parser(kind, &mut p)
    }

    /// Parses a symbol record. The caller has already parsed the length and kind of the record.
    /// The `p` parameter does not include the length or kind.
    ///
    /// This function allows the caller to observe how many bytes were actually consumed from
    /// the input stream.
    pub fn from_parser(kind: SymKind, p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(match kind {
            SymKind::S_OBJNAME => Self::ObjName(p.parse()?),
            SymKind::S_GPROC32 | SymKind::S_LPROC32 => Self::Proc(p.parse()?),
            SymKind::S_COMPILE3 => Self::Compile3(p.parse()?),
            SymKind::S_UDT => Self::Udt(p.parse()?),
            SymKind::S_CONSTANT => Self::Constant(p.parse()?),
            SymKind::S_MANCONSTANT => Self::Constant(p.parse()?),
            SymKind::S_PUB32 => Self::Pub(p.parse()?),
            SymKind::S_PUB32_ST => Self::Pub(Pub::parse_st(p)?),

            SymKind::S_PROCREF
            | SymKind::S_LPROCREF
            | SymKind::S_DATAREF
            | SymKind::S_ANNOTATIONREF => Self::RefSym2(p.parse()?),

            SymKind::S_LDATA32 | SymKind::S_GDATA32 | SymKind::S_LMANDATA | SymKind::S_GMANDATA => {
                Self::Data(p.parse()?)
            }

            SymKind::S_LTHREAD32 | SymKind::S_GTHREAD32 => Self::ThreadData(p.parse()?),
            SymKind::S_END => Self::End,
            SymKind::S_FRAMEPROC => Self::FrameProc(p.get()?),
            SymKind::S_REGREL32 => Self::RegRel(p.parse()?),
            SymKind::S_BLOCK32 => Self::Block(p.parse()?),
            SymKind::S_LOCAL => Self::Local(p.parse()?),
            SymKind::S_DEFRANGE => Self::DefRange(p.parse()?),
            SymKind::S_DEFRANGE_FRAMEPOINTER_REL => Self::DefRangeFramePointerRel(p.parse()?),
            SymKind::S_DEFRANGE_REGISTER => Self::DefRangeRegister(p.parse()?),
            SymKind::S_DEFRANGE_REGISTER_REL => Self::DefRangeRegisterRel(p.parse()?),
            SymKind::S_DEFRANGE_FRAMEPOINTER_REL_FULL_SCOPE => {
                Self::DefRangeFramePointerRelFullScope(p.get()?)
            }
            SymKind::S_DEFRANGE_SUBFIELD_REGISTER => Self::DefRangeSubFieldRegister(p.parse()?),
            SymKind::S_TRAMPOLINE => Self::Trampoline(p.parse()?),
            SymKind::S_BUILDINFO => Self::BuildInfo(p.parse()?),
            SymKind::S_UNAMESPACE => Self::UsingNamespace(p.parse()?),
            SymKind::S_INLINESITE_END => Self::InlineSiteEnd,
            SymKind::S_LABEL32 => Self::Label(p.parse()?),
            SymKind::S_CALLEES | SymKind::S_CALLERS => Self::FunctionList(p.parse()?),
            SymKind::S_INLINESITE => Self::InlineSite(p.parse()?),
            SymKind::S_INLINESITE2 => Self::InlineSite2(p.parse()?),
            SymKind::S_INLINEES => Self::FunctionList(p.parse()?),
            SymKind::S_FRAMECOOKIE => Self::FrameCookie(p.get()?),
            SymKind::S_CALLSITEINFO => Self::CallSiteInfo(p.get()?),
            SymKind::S_HEAPALLOCSITE => Self::HeapAllocSite(p.get()?),
            SymKind::S_GMANPROC | SymKind::S_LMANPROC => Self::ManagedProc(p.parse()?),
            SymKind::S_ANNOTATION => Self::Annotation(p.parse()?),
            SymKind::S_HOTPATCHFUNC => Self::HotPatchFunc(p.parse()?),
            SymKind::S_ARMSWITCHTABLE => Self::ArmSwitchTable(p.get()?),
            SymKind::S_COFFGROUP => Self::CoffGroup(p.parse()?),
            SymKind::S_SECTION => Self::Section(p.parse()?),

            _ => Self::Unknown,
        })
    }

    /// If this symbol record has a "name" field, return it. Else, `None`.
    pub fn name(&self) -> Option<&'a BStr> {
        match self {
            Self::Proc(proc) => Some(proc.name),
            Self::Data(data) => Some(data.name),
            Self::ThreadData(thread_data) => Some(thread_data.name),
            Self::Udt(udt) => Some(udt.name),
            Self::Local(local) => Some(local.name),
            Self::RefSym2(refsym) => Some(refsym.name),
            Self::Constant(c) => Some(c.name),
            Self::ManagedConstant(c) => Some(c.name),
            _ => None,
        }
    }
}

```

`codeview/src/syms/builder.rs`:

```rs
//! Supports building new symbol streams

use super::SymKind;
use crate::encoder::Encoder;
use crate::types::TypeIndex;
use bstr::BStr;

/// Writes symbol records into a buffer.
#[derive(Default)]
pub struct SymBuilder {
    /// Contains the symbol stream
    pub buffer: Vec<u8>,
}

impl SymBuilder {
    /// Creates a new empty symbol stream builder.
    pub fn new() -> Self {
        Self { buffer: Vec::new() }
    }

    /// Consumes this builder and returns the symbol stream.
    pub fn finish(self) -> Vec<u8> {
        self.buffer
    }

    /// Starts adding a new record to the builder.
    pub fn record(&mut self, kind: SymKind) -> RecordBuilder<'_> {
        let record_start = self.buffer.len();
        self.buffer.extend_from_slice(&[0, 0]); // placeholder for record length
        self.buffer.extend_from_slice(&kind.0.to_le_bytes());
        RecordBuilder {
            enc: Encoder::new(&mut self.buffer),
            record_start,
        }
    }

    /// Adds an `S_UDT` record.
    pub fn udt(&mut self, ty: TypeIndex, name: &BStr) {
        let mut r = self.record(SymKind::S_UDT);
        r.enc.u32(ty.0);
        r.enc.strz(name);
    }

    /// Adds an `S_PUB32` record.
    pub fn pub32(&mut self, flags: u32, offset: u32, segment: u16, name: &str) {
        let mut r = self.record(SymKind::S_PUB32);
        r.enc.u32(flags);
        r.enc.u32(offset);
        r.enc.u16(segment);
        r.enc.strz(name.into());
    }
}

/// State for writing a single record. When this is dropped, it will terminate the record.
pub struct RecordBuilder<'a> {
    /// Encoder which can write the payload of the current record.
    pub enc: Encoder<'a>,
    /// Byte offset of the start of the current record. We use this to patch the record length
    /// when we're done writing the record.
    record_start: usize,
}

impl<'a> Drop for RecordBuilder<'a> {
    fn drop(&mut self) {
        // Align the buffer to a 4-byte boundary
        match self.enc.buf.len() & 3 {
            1 => self.enc.buf.push(0xf1),
            2 => self.enc.buf.extend_from_slice(&[0xf1, 0xf2]),
            3 => self.enc.buf.extend_from_slice(&[0xf1, 0xf2, 0xf3]),
            _ => {}
        }

        let record_len = self.enc.buf.len() - self.record_start - 2;
        let record_field = &mut self.enc.buf[self.record_start..];
        record_field[0] = record_len as u8;
        record_field[1] = (record_len >> 8) as u8;
    }
}

```

`codeview/src/syms/iter.rs`:

```rs
use super::*;
use crate::utils::iter::HasRestLen;
use std::mem::take;
use tracing::error;

/// Parses [`Sym`] records from a symbol stream.
#[derive(Clone)]
pub struct SymIter<'a> {
    data: &'a [u8],
}

impl<'a> HasRestLen for SymIter<'a> {
    fn rest_len(&self) -> usize {
        self.data.len()
    }
}

/// Parses [`SymMut`] records from a symbol stream.
///
/// This iterator allows you to modify the payload of a symbol record but not to change its length
/// or its kind.
pub struct SymIterMut<'a> {
    data: &'a mut [u8],
}

impl<'a> SymIterMut<'a> {
    /// Parses the 4-byte CodeView signature that is at the start of a module symbol stream.
    pub fn get_signature(&mut self) -> Result<[u8; 4], ParserError> {
        let mut p = ParserMut::new(take(&mut self.data));
        let sig = p.copy()?;
        self.data = p.into_rest();
        Ok(sig)
    }
}

impl<'a> HasRestLen for SymIterMut<'a> {
    fn rest_len(&self) -> usize {
        self.data.len()
    }
}

impl<'a> SymIter<'a> {
    /// Creates a new symbol iterator.
    pub fn new(data: &'a [u8]) -> Self {
        Self { data }
    }

    /// Skip 4 bytes for the header of a module stream
    pub fn skip_module_prefix(&mut self) {
        if self.data.len() >= 4 {
            self.data = &self.data[4..];
        }
    }

    /// Creates a new symbol iterator for symbols stored in a module stream.
    ///
    /// The symbol data in a module stream begins with a 4-byte header. This function ignores
    /// the 4-byte header.
    pub fn for_module_syms(data: &'a [u8]) -> Self {
        Self {
            data: if data.len() < 4 { &[] } else { &data[4..] },
        }
    }

    /// Parses the 4-byte CodeView signature that is at the start of a module symbol stream.
    pub fn get_signature(&mut self) -> Result<[u8; 4], ParserError> {
        let mut p = Parser::new(self.data);
        let sig = p.copy()?;
        self.data = p.into_rest();
        Ok(sig)
    }

    /// The remaining unparsed bytes in the symbol stream.
    pub fn rest(&self) -> &'a [u8] {
        self.data
    }

    /// Parses a single record from `data`.
    pub fn one(data: &'a [u8]) -> Option<Sym<'a>> {
        Self::new(data).next()
    }
}

impl<'a> Iterator for SymIter<'a> {
    type Item = Sym<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.data.is_empty() {
            return None;
        }

        let mut p = Parser::new(self.data);
        let record_len = p.u16().ok()?;
        if record_len < 2 {
            error!(
                invalid_record_len = record_len,
                iterator_pos = self.data.len(),
                "type record has invalid len"
            );
            return None;
        }

        let kind = SymKind(p.u16().ok()?);
        let record_data = p.bytes(record_len as usize - 2).ok()?;

        self.data = p.into_rest();

        Some(Sym {
            kind,
            data: record_data,
        })
    }
}

#[test]
fn test_sym_iter() {
    #[rustfmt::skip]
    let data: &[u8] = &[
        // record 0, total size = 8
        /* 0x0000 */ 6, 0,                              // size
        /* 0x0002 */ 0x4c, 0x11,                        // S_BUILDINFO
        /* 0x0004 */ 1, 2, 3, 4,                        // payload (ItemId)

        // record 1, total size = 12
        /* 0x0008 */ 10, 0,                              // size
        /* 0x000a */ 0x24, 0x11,                        // S_UNAMESPACE
        /* 0x000c */ b'b', b'o', b'o', b's',            // payload (6 bytes)
        /* 0x0010 */ b't', 0,
        /* 0x0012 */ 0xf1, 0xf2,                        // alignment padding (inside payload)

        // record 2, total size = 12
        /* 0x0014 */ 10, 0,                             // size
        /* 0x0016 */ 0x24, 0x11,                        // S_UNAMESPACE
        /* 0x0018 */ b'a', b'b', b'c', b'd',            // payload
        /* 0x001c */ b'e', b'f', b'g', 0,               // no alignment padding

        /* 0x0020 : end */
    ];

    let mut i = SymIter::new(data);

    // parse record 0
    assert_eq!(i.rest_len(), 0x20);
    let s0 = i.next().unwrap();
    assert_eq!(s0.kind, SymKind::S_BUILDINFO);
    let s0_data = s0.parse().unwrap();
    assert!(matches!(s0_data, SymData::BuildInfo(_)));

    // parse record 1
    assert_eq!(i.rest_len(), 0x18);
    let s1 = i.next().unwrap();
    assert_eq!(s1.kind, SymKind::S_UNAMESPACE);
    match s1.parse() {
        Ok(SymData::UsingNamespace(ns)) => assert_eq!(ns.namespace, "boost"),
        sd => panic!("wrong: {sd:?}"),
    }

    // parse record 2
    assert_eq!(i.rest_len(), 0xc);
    let s1 = i.next().unwrap();
    assert_eq!(s1.kind, SymKind::S_UNAMESPACE);
    match s1.parse() {
        Ok(SymData::UsingNamespace(ns)) => assert_eq!(ns.namespace, "abcdefg"),
        sd => panic!("wrong: {sd:?}"),
    }

    // end
    assert_eq!(i.rest_len(), 0);
    assert!(i.next().is_none());
}

impl<'a> SymIterMut<'a> {
    /// Creates a new symbol iterator.
    pub fn new(data: &'a mut [u8]) -> Self {
        Self { data }
    }

    /// The remaining unparsed bytes in the symbol stream.
    pub fn rest(&self) -> &[u8] {
        self.data
    }

    /// The remaining unparsed bytes in the symbol stream, with mutable access.
    pub fn rest_mut(&mut self) -> &mut [u8] {
        self.data
    }

    /// Converts this iterator into a mutable reference to the unparsed bytes in the symbol stream.
    pub fn into_rest(self) -> &'a mut [u8] {
        self.data
    }
}

impl<'a> Iterator for SymIterMut<'a> {
    type Item = SymMut<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.data.len() < 4 {
            return None;
        }

        // We steal self.data because it is the only way that split_at_mut() can work.
        let d = core::mem::take(&mut self.data);

        let mut p = Parser::new(d);
        let record_len = p.u16().ok()?;
        if record_len < 2 {
            error!(
                record_len,
                iterator_len = self.data.len(),
                "type record has invalid len"
            );
            self.data = d;
            return None;
        }

        let kind = SymKind(p.u16().ok()?);

        let (entire_record_data, hi) = d.split_at_mut(2 + record_len as usize);
        self.data = hi;

        let record_data = &mut entire_record_data[4..];

        Some(SymMut {
            kind,
            data: record_data,
        })
    }
}

```

`codeview/src/syms/kind.rs`:

```rs
//! Symbol kind enumeration

#[cfg(doc)]
use super::BlockHeader;

/// Identifies symbol records.
///
/// Symbol records are stored in the Global Symbol Stream and in each per-module symbol stream.
///
/// Many symbols can only appear in the Global Symbol Stream or in a per-module symbol stream.
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub struct SymKind(pub u16);

macro_rules! sym_kinds {
    (
        $( $code:expr, $name:ident; )*
    ) => {
        #[allow(missing_docs)]
        impl SymKind {
            $(
                pub const $name: SymKind = SymKind($code);
            )*
        }

        static SYM_NAMES: &[(SymKind, &str)] = &[
            $(
                (SymKind($code), stringify!($name)),
            )*
        ];
    }
}

sym_kinds! {
    0x0001, S_COMPILE;
    0x0006, S_END;
    0x0007, S_SKIP;
    0x0008, S_CVRESERVE;
    0x0009, S_OBJNAME_ST;
    0x000d, S_RETURN;
    // 0x0100..0x0400 is for 16-bit types
    0x0400, S_PROCREF_ST;
    0x0401, S_DATAREF_ST;
    0x0402, S_ALIGN;
    0x0403, S_LPROCREF_ST;
    0x0404, S_OEM;

    0x1000, S_TI16_MAX;
    0x1001, S_REGISTER_ST;
    0x1002, S_CONSTANT_ST;
    0x1003, S_UDT_ST;
    0x1004, S_COBOLUDT_ST;
    0x1005, S_MANYREG_ST;
    0x1006, S_BPREL32_ST;
    0x1007, S_LDATA32_ST;
    0x1008, S_GDATA32_ST;
    0x1009, S_PUB32_ST;
    0x100a, S_LPROC32_ST;
    0x100b, S_GPROC32_ST;
    0x100c, S_VFTABLE32;
    0x100d, S_REGREL32_ST;
    0x100e, S_LTHREAD32_ST;
    0x100f, S_GTHREAD32_ST;
    0x1012, S_FRAMEPROC;
    0x1019, S_ANNOTATION;

    0x1101, S_OBJNAME;
    0x1102, S_THUNK32;
    0x1103, S_BLOCK32;
    0x1104, S_WITH32;
    0x1105, S_LABEL32;
    0x1106, S_REGISTER;
    0x1107, S_CONSTANT;
    0x1108, S_UDT;
    0x1109, S_COBOLUDT;
    0x110a, S_MANYREG;
    0x110b, S_BPREL32;
    0x110c, S_LDATA32;
    0x110d, S_GDATA32;
    0x110e, S_PUB32;
    0x110f, S_LPROC32;

    0x1110, S_GPROC32;
    0x1111, S_REGREL32;
    0x1112, S_LTHREAD32;
    0x1113, S_GTHREAD32;
    0x1116, S_COMPILE2;
    0x1117, S_MANYREG2;
    0x1118, S_LPROCIA64;
    0x1119, S_GPROCIA64;
    0x111a, S_LOCALSLOT;
    0x111b, S_PARAMSLOT;
    0x111c, S_LMANDATA;
    0x111d, S_GMANDATA;
    0x111e, S_MANFRAMEREL;
    0x111f, S_MANREGISTER;

    0x1120, S_MANSLOT;
    0x1121, S_MANMANYREG;
    0x1122, S_MANREGREL;
    0x1123, S_MANMANYREG2;
    0x1124, S_UNAMESPACE;
    0x1125, S_PROCREF;
    0x1126, S_DATAREF;
    0x1127, S_LPROCREF;
    0x1128, S_ANNOTATIONREF;
    0x1129, S_TOKENREF;
    0x112a, S_GMANPROC;
    0x112b, S_LMANPROC;
    0x112c, S_TRAMPOLINE;
    0x112d, S_MANCONSTANT;
    0x112e, S_ATTR_FRAMEREL;
    0x112f, S_ATTR_REGISTER;

    0x1130, S_ATTR_REGREL;
    0x1131, S_ATTR_MANYREG;
    0x1132, S_SEPCODE;
    0x1133, S_LOCAL_2005;
    0x1134, S_DEFRANGE_2005;
    0x1135, S_DEFRANGE2_2005;
    0x1136, S_SECTION;
    0x1137, S_COFFGROUP;
    0x1138, S_EXPORT;
    0x1139, S_CALLSITEINFO;
    0x113a, S_FRAMECOOKIE;
    0x113b, S_DISCARDED;
    0x113c, S_COMPILE3;
    0x113d, S_ENVBLOCK;
    0x113e, S_LOCAL;
    0x113f, S_DEFRANGE;

    0x1140, S_DEFRANGE_SUBFIELD;
    0x1141, S_DEFRANGE_REGISTER;
    0x1142, S_DEFRANGE_FRAMEPOINTER_REL;
    0x1143, S_DEFRANGE_SUBFIELD_REGISTER;
    0x1144, S_DEFRANGE_FRAMEPOINTER_REL_FULL_SCOPE;
    0x1145, S_DEFRANGE_REGISTER_REL;
    0x1146, S_LPROC32_ID;
    0x1147, S_GPROC32_ID;
    0x1148, S_LPROCMIPS_ID;
    0x1149, S_GPROCMIPS_ID;
    0x114a, S_LPROCIA64_ID;
    0x114b, S_GPROCIA64_ID;
    0x114c, S_BUILDINFO;
    0x114d, S_INLINESITE;
    0x114e, S_INLINESITE_END;
    0x114f, S_PROC_ID_END;

    0x1150, S_DEFRANGE_HLSL;
    0x1151, S_GDATA_HLSL;
    0x1152, S_LDATA_HLSL;
    0x1153, S_FILESTATIC;
    0x1154, S_LOCAL_DPC_GROUPSHARED;
    0x1155, S_LPROC32_DPC;
    0x1156, S_LPROC32_DPC_ID;
    0x1157, S_DEFRANGE_DPC_PTR_TAG;
    0x1158, S_DPC_SYM_TAG_MAP;
    0x1159, S_ARMSWITCHTABLE;
    0x115a, S_CALLEES;
    0x115b, S_CALLERS;
    0x115c, S_POGODATA;
    0x115d, S_INLINESITE2;
    0x115e, S_HEAPALLOCSITE;
    0x115f, S_MOD_TYPEREF;

    0x1160, S_REF_MINIPDB;
    0x1161, S_PDBMAP;
    0x1162, S_GDATA_HLSL32;
    0x1163, S_LDATA_HLSL32;
    0x1164, S_GDATA_HLSL32_EX;
    0x1165, S_LDATA_HLSL32_EX;
    0x1167, S_FASTLINK;
    0x1168, S_INLINEES;
    0x1169, S_HOTPATCHFUNC;
}

impl std::fmt::Debug for SymKind {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        if let Ok(index) = SYM_NAMES.binary_search_by_key(self, |ii| ii.0) {
            <str as std::fmt::Display>::fmt(SYM_NAMES[index].1, f)
        } else {
            let b0 = (self.0 & 0xff) as u8;
            let b1 = (self.0 >> 8) as u8;
            fn to_c(b: u8) -> char {
                if (32..=126).contains(&b) {
                    char::from(b)
                } else {
                    '_'
                }
            }

            write!(f, "S_(??{:04x} {}{})", self.0, to_c(b1), to_c(b0))
        }
    }
}

#[test]
fn test_sym_kind_debug() {
    assert_eq!(format!("{:?}", SymKind::S_GPROC32), "S_GPROC32");
    assert_eq!(format!("{:?}", SymKind(0x31aa)), "S_(??31aa 1_)");
}

impl SymKind {
    /// True if this `SymKind` starts a scope. All symbols that start a block begin with
    /// [`BlockHeader`].
    pub fn starts_scope(self) -> bool {
        matches!(
            self,
            SymKind::S_GPROC32
                | SymKind::S_LPROC32
                | SymKind::S_LPROC32_DPC
                | SymKind::S_LPROC32_DPC_ID
                | SymKind::S_GPROC32_ID
                | SymKind::S_BLOCK32
                | SymKind::S_THUNK32
                | SymKind::S_INLINESITE
                | SymKind::S_INLINESITE2
                | SymKind::S_SEPCODE
                | SymKind::S_GMANPROC
                | SymKind::S_LMANPROC
        )
    }

    /// True if this `SymKind` is a procedure definition.
    pub fn is_proc(self) -> bool {
        matches!(
            self,
            SymKind::S_GPROC32
                | SymKind::S_GPROC32_ID
                | SymKind::S_GPROC32_ST
                | SymKind::S_LPROC32
                | SymKind::S_LPROC32_DPC
                | SymKind::S_LPROC32_DPC_ID
                | SymKind::S_LPROC32_ID
                | SymKind::S_LPROC32_ST
        )
    }

    /// Indicates whether this `SymKind` ends a scope.
    ///
    /// There are no `SymKind` values that both start and end a scope.
    ///
    /// In all well-formed symbol streams, every symbol that starts a scope has a matching symbol
    /// that ends that scope.
    pub fn ends_scope(self) -> bool {
        matches!(
            self,
            SymKind::S_END | SymKind::S_PROC_ID_END | SymKind::S_INLINESITE_END
        )
    }

    /// Returns `true` if this symbol can be the _target_ of a "reference to symbol" in the
    /// Global Symbol Stream.
    pub fn is_refsym_target(self) -> bool {
        matches!(
            self,
            SymKind::S_GPROC32
                | SymKind::S_LPROC32
                | SymKind::S_GMANPROC
                | SymKind::S_LMANPROC
                | SymKind::S_GDATA32
                | SymKind::S_LDATA32
                | SymKind::S_ANNOTATION
        )
    }

    /// Returns `true` if this symbol can be the _source_ of a "reference to symbol"
    /// in the Global Symbol Stream.
    pub fn is_refsym_source(self) -> bool {
        matches!(
            self,
            SymKind::S_LPROCREF
                | SymKind::S_PROCREF
                | SymKind::S_ANNOTATIONREF
                | SymKind::S_TOKENREF
                | SymKind::S_DATAREF
        )
    }
}

```

`codeview/src/syms/offset_segment.rs`:

```rs
use super::*;

/// Stores an `offset` and `segment` pair, in that order. This structure is directly embedded in
/// on-disk structures.
#[repr(C)]
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned, Default, Clone, Copy, Eq)]
pub struct OffsetSegment {
    /// The offset in bytes of a symbol within a segment.
    pub offset: U32<LE>,

    /// The segment (section) index.
    pub segment: U16<LE>,
}

impl PartialEq for OffsetSegment {
    #[inline]
    fn eq(&self, other: &Self) -> bool {
        self.as_u64() == other.as_u64()
    }
}

impl PartialOrd for OffsetSegment {
    #[inline]
    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for OffsetSegment {
    #[inline]
    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
        Ord::cmp(&self.as_u64(), &other.as_u64())
    }
}

impl OffsetSegment {
    /// Constructor
    pub fn new(offset: u32, segment: u16) -> Self {
        Self {
            offset: offset.into(),
            segment: segment.into(),
        }
    }

    /// The offset in bytes of a symbol within a segment.
    #[inline]
    pub fn offset(&self) -> u32 {
        self.offset.get()
    }

    /// The segment (section) index.
    #[inline]
    pub fn segment(&self) -> u16 {
        self.segment.get()
    }

    /// Combines the segment and offset into a tuple. The segment is the first element and the
    /// offset is the second element. This order gives a sorting order that sorts by segment first.
    #[inline]
    pub fn as_tuple(&self) -> (u16, u32) {
        (self.segment.get(), self.offset.get())
    }

    /// Combines the segment and offset into a single `u64` value, with the segment in the
    /// higher-order bits. This allows for efficient comparisons.
    #[inline]
    pub fn as_u64(&self) -> u64 {
        ((self.segment.get() as u64) << 32) | (self.offset.get() as u64)
    }
}

impl std::fmt::Display for OffsetSegment {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "[{:04x}:{:08x}]", self.segment.get(), self.offset.get())
    }
}

impl Debug for OffsetSegment {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        <Self as std::fmt::Display>::fmt(self, f)
    }
}

impl std::hash::Hash for OffsetSegment {
    #[inline]
    fn hash<H: std::hash::Hasher>(&self, state: &mut H) {
        state.write_u16(self.segment());
        state.write_u32(self.offset());
    }
}

```

`codeview/src/types.rs`:

```rs
//! Code for decoding type record streams (the TPI and IPI streams).
//!
//! # References
//! * [CodeView Type Records](https://llvm.org/docs/PDB/CodeViewTypes.html)

mod iter;
#[doc(inline)]
pub use iter::*;

mod kind;
#[doc(inline)]
pub use kind::*;

pub mod fields;
pub mod number;
pub mod primitive;
pub mod visitor;

mod records;
#[doc(inline)]
pub use records::*;

pub use fields::FieldList;

use self::primitive::dump_primitive_type_index;
use crate::parser::{Number, Parse, Parser, ParserError};
use bitfield::bitfield;
use bstr::BStr;
use std::fmt::Debug;
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, LE, U16, U32, Unaligned};

/// A type index refers to another type record, or to a primitive type.
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub struct TypeIndex(pub u32);

impl TypeIndex {
    /// The minimum value for a `type_index_begin` value.
    ///
    /// This value comes from the fact that the first 0x1000 values are reserved for primitive
    /// types.  See `primitive_types.md` in the specification.
    pub const MIN_BEGIN: TypeIndex = TypeIndex(0x1000);
}

impl std::fmt::Debug for TypeIndex {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        if self.0 < TypeIndex::MIN_BEGIN.0 {
            dump_primitive_type_index(fmt, *self)
        } else {
            write!(fmt, "T#0x{:x}", self.0)
        }
    }
}

/// The serialized form of [`TypeIndex`]. This can be embedded directly in data structures
/// stored on disk.
#[derive(
    Copy, Clone, Eq, PartialEq, Hash, FromBytes, IntoBytes, Immutable, KnownLayout, Unaligned,
)]
#[repr(transparent)]
pub struct TypeIndexLe(pub U32<LE>);

impl From<TypeIndex> for TypeIndexLe {
    #[inline(always)]
    fn from(value: TypeIndex) -> TypeIndexLe {
        TypeIndexLe(U32::new(value.0))
    }
}

impl Debug for TypeIndexLe {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        let ti = self.get();
        Debug::fmt(&ti, fmt)
    }
}

impl TypeIndexLe {
    /// Converts this value to host byte-order.
    #[inline(always)]
    pub fn get(self) -> TypeIndex {
        TypeIndex(self.0.get())
    }
}

/// Parsed details of a type record.
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub enum TypeData<'a> {
    Array(Array<'a>),
    Struct(Struct<'a>),
    Union(Union<'a>),
    Enum(Enum<'a>),
    Proc(&'a Proc),
    MemberFunc(&'a MemberFunc),
    VTableShape(VTableShapeData<'a>),
    Pointer(Pointer<'a>),
    Modifier(TypeModifier),
    Bitfield(&'a Bitfield),
    FieldList(FieldList<'a>),
    MethodList(MethodListData<'a>),
    ArgList(ArgList<'a>),
    Alias(Alias<'a>),
    UdtSrcLine(&'a UdtSrcLine),
    UdtModSrcLine(&'a UdtModSrcLine),
    FuncId(FuncId<'a>),
    MFuncId(MFuncId<'a>),
    StringId(StringId<'a>),
    SubStrList(SubStrList<'a>),
    BuildInfo(BuildInfo<'a>),
    VFTable(&'a VFTable),
    Unknown,
}

impl<'a> TypeData<'a> {
    /// Parses the payload of a type record.
    pub fn parse_bytes(kind: Leaf, bytes: &'a [u8]) -> Result<Self, ParserError> {
        let mut p = Parser::new(bytes);
        Self::parse(kind, &mut p)
    }

    /// Parses the payload of a type record, using a [`Parser`].
    pub fn parse(kind: Leaf, p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(match kind {
            Leaf::LF_ARRAY => Self::Array(p.parse()?),
            Leaf::LF_CLASS | Leaf::LF_STRUCTURE | Leaf::LF_INTERFACE => Self::Struct(p.parse()?),
            Leaf::LF_UNION => Self::Union(p.parse()?),
            Leaf::LF_ENUM => Self::Enum(p.parse()?),
            Leaf::LF_PROCEDURE => Self::Proc(p.get()?),
            Leaf::LF_MEMBER => Self::MemberFunc(p.get()?),

            Leaf::LF_VTSHAPE => {
                let fixed: &VTableShapeFixed = p.get()?;
                Self::VTableShape(VTableShapeData {
                    count: fixed.count.get(),
                    descriptors: p.take_rest(),
                })
            }

            Leaf::LF_VFTABLE => Self::VFTable(p.get()?),

            Leaf::LF_POINTER => {
                let fixed = p.get()?;
                let variant = p.take_rest();
                Self::Pointer(Pointer { fixed, variant })
            }

            Leaf::LF_MFUNCTION => Self::MemberFunc(p.get()?),
            Leaf::LF_MODIFIER => Self::Modifier(p.copy()?),
            Leaf::LF_BITFIELD => Self::Bitfield(p.get()?),

            Leaf::LF_FIELDLIST => Self::FieldList(FieldList {
                bytes: p.take_rest(),
            }),

            Leaf::LF_METHODLIST => Self::MethodList(MethodListData {
                bytes: p.take_rest(),
            }),

            Leaf::LF_ARGLIST => Self::ArgList(p.parse()?),
            Leaf::LF_ALIAS => Self::Alias(Alias::from_parser(p)?),
            Leaf::LF_UDT_SRC_LINE => Self::UdtSrcLine(p.get()?),
            Leaf::LF_UDT_MOD_SRC_LINE => Self::UdtModSrcLine(p.get()?),
            Leaf::LF_FUNC_ID => Self::FuncId(p.parse()?),
            Leaf::LF_MFUNC_ID => Self::MFuncId(p.parse()?),
            Leaf::LF_STRING_ID => Self::StringId(p.parse()?),
            Leaf::LF_SUBSTR_LIST => Self::SubStrList(p.parse()?),
            Leaf::LF_BUILDINFO => Self::BuildInfo(p.parse()?),

            _ => Self::Unknown,
        })
    }

    /// If this record has a primary "name" field, return it. Else, return `None`.
    pub fn name(&self) -> Option<&'a BStr> {
        match self {
            // From TPI
            Self::Struct(t) => Some(t.name),
            Self::Union(t) => Some(t.name),
            Self::Enum(t) => Some(t.name),
            Self::Alias(t) => Some(t.name),

            // From IPI
            Self::FuncId(t) => Some(t.name),
            Self::StringId(t) => Some(t.name),

            _ => None,
        }
    }

    /// Returns the name of this type definition, if it is a UDT (user-defined type) definition.
    pub fn udt_name(&self) -> Option<&'a BStr> {
        match self {
            Self::Struct(t) => Some(t.name),
            Self::Union(t) => Some(t.name),
            Self::Enum(t) => Some(t.name),
            Self::Alias(t) => Some(t.name),
            _ => None,
        }
    }
}

```

`codeview/src/types/fields.rs`:

```rs
//! Decodes items in a `LF_FIELDLIST` complex list.

use super::*;
use tracing::error;

/// Represents the data stored within an `LF_FIELDLIST` type string. This can be decoded using
/// the `iter()` method.
#[derive(Clone)]
pub struct FieldList<'a> {
    #[allow(missing_docs)]
    pub bytes: &'a [u8],
}

impl<'a> FieldList<'a> {
    /// Iterates the fields within an `LF_FIELDLIST` type string.
    pub fn iter(&self) -> IterFields<'a> {
        IterFields { bytes: self.bytes }
    }
}

impl<'a> Debug for FieldList<'a> {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        if f.alternate() {
            let mut list = f.debug_list();
            for f in self.iter() {
                list.entry(&f);
            }
            list.finish()
        } else {
            f.write_str("FieldList")
        }
    }
}

/// Iterates the fields within an `LF_FIELDLIST` type string.
pub struct IterFields<'a> {
    #[allow(missing_docs)]
    pub bytes: &'a [u8],
}

/// Represents one field within an `LF_FIELDLIST` type string.
#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub enum Field<'a> {
    BaseClass(BaseClass<'a>),
    DirectVirtualBaseClass(DirectVirtualBaseClass<'a>),
    IndirectVirtualBaseClass(IndirectVirtualBaseClass<'a>),
    Enumerate(Enumerate<'a>),
    FriendFn(FriendFn<'a>),
    Index(TypeIndex),
    Member(Member<'a>),
    StaticMember(StaticMember<'a>),
    Method(Method<'a>),
    NestedType(NestedType<'a>),
    VFuncTable(TypeIndex),
    FriendClass(TypeIndex),
    OneMethod(OneMethod<'a>),
    VFuncOffset(VFuncOffset),
    NestedTypeEx(NestedTypeEx<'a>),
}

#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct NestedType<'a> {
    pub nested_ty: TypeIndex,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for NestedType<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        p.skip(2)?; // padding
        Ok(Self {
            nested_ty: p.type_index()?,
            name: p.strz()?,
        })
    }
}

#[derive(Clone, Debug)]
#[allow(missing_docs)]
pub struct BaseClass<'a> {
    pub attr: u16,
    pub ty: TypeIndex,
    pub offset: Number<'a>,
}

impl<'a> Parse<'a> for BaseClass<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let attr = p.u16()?;
        let ty = p.type_index()?;
        let offset = p.number()?;
        Ok(BaseClass { attr, ty, offset })
    }
}

/// This is used by both DirectVirtualBaseClass and IndirectVirtualBaseClass.
#[allow(missing_docs)]
#[repr(C)]
#[derive(Clone, Debug, IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned)]
pub struct VirtualBaseClassFixed {
    pub attr: U16<LE>,
    pub btype: TypeIndexLe,
    pub vbtype: TypeIndexLe,
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct DirectVirtualBaseClass<'a> {
    pub fixed: &'a VirtualBaseClassFixed,
    pub vbpoff: Number<'a>,
    pub vboff: Number<'a>,
}

impl<'a> Parse<'a> for DirectVirtualBaseClass<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            vbpoff: p.number()?,
            vboff: p.number()?,
        })
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct IndirectVirtualBaseClass<'a> {
    pub fixed: &'a VirtualBaseClassFixed,
    pub vbpoff: Number<'a>,
    pub vboff: Number<'a>,
}

impl<'a> Parse<'a> for IndirectVirtualBaseClass<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let fixed = p.get()?;
        let vbpoff = p.number()?;
        let vboff = p.number()?;
        Ok(Self {
            fixed,
            vbpoff,
            vboff,
        })
    }
}

#[allow(missing_docs)]
#[derive(Clone)]
pub struct Enumerate<'a> {
    pub attr: u16,
    pub value: Number<'a>,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Enumerate<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            attr: p.u16()?,
            value: p.number()?,
            name: p.strz()?,
        })
    }
}

impl<'a> Debug for Enumerate<'a> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "{} = {}", self.name, self.value)
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct FriendFn<'a> {
    pub ty: TypeIndex,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for FriendFn<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        p.skip(2)?; // padding
        Ok(Self {
            ty: p.type_index()?,
            name: p.strz()?,
        })
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct OneMethod<'a> {
    pub attr: u16,
    pub ty: TypeIndex,
    pub vbaseoff: u32,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for OneMethod<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let attr = p.u16()?;
        let ty = p.type_index()?;
        let vbaseoff = if introduces_virtual(attr) {
            p.u32()?
        } else {
            0
        };
        let name = p.strz()?;
        Ok(OneMethod {
            attr,
            ty,
            vbaseoff,
            name,
        })
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct VFuncOffset {
    pub vtable_ty: TypeIndex,
    pub offset: u32,
}

impl<'a> Parse<'a> for VFuncOffset {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        p.skip(2)?; // padding
        let vtable_ty = p.type_index()?;
        let offset = p.u32()?;
        Ok(Self { vtable_ty, offset })
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct NestedTypeEx<'a> {
    pub attr: u16,
    pub ty: TypeIndex,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for NestedTypeEx<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let attr = p.u16()?;
        let ty = p.type_index()?;
        let name = p.strz()?;
        Ok(Self { attr, ty, name })
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Member<'a> {
    pub attr: u16,
    pub ty: TypeIndex,
    pub offset: Number<'a>,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Member<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            attr: p.u16()?,
            ty: p.type_index()?,
            offset: p.number()?,
            name: p.strz()?,
        })
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct StaticMember<'a> {
    pub attr: u16,
    pub ty: TypeIndex,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for StaticMember<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            attr: p.u16()?,
            ty: p.type_index()?,
            name: p.strz()?,
        })
    }
}

#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Method<'a> {
    pub count: u16,
    pub methods: TypeIndex,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Method<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            count: p.u16()?,
            methods: p.type_index()?,
            name: p.strz()?,
        })
    }
}

impl<'a> Iterator for IterFields<'a> {
    type Item = Field<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.bytes.is_empty() {
            return None;
        }
        let mut p = Parser::new(self.bytes);

        let rest = p.peek_rest();

        // Check for padding (alignment) bytes.
        let mut padding_len = 0;
        while padding_len < rest.len() && rest[padding_len] >= 0xf0 {
            padding_len += 1;
        }
        if padding_len > 0 {
            let _ = p.skip(padding_len);
        }

        if p.is_empty() {
            return None;
        }

        match Field::parse(&mut p) {
            Ok(f) => {
                self.bytes = p.into_rest();
                Some(f)
            }
            Err(ParserError) => None,
        }
    }
}

impl<'a> Field<'a> {
    /// Parses one field within an `LF_FIELDLIST` type string.
    ///
    /// Unlike most of the `parse()` methods defined in this library, this function requires a
    /// `Parser` instance, rather than just working directly with `&[u8]`. This is because the
    /// field records do not have a length field; the type of the field is required to know how
    /// many bytes to decode in each field.
    ///
    /// So the act of parsing a field is what is needed for locating the next field.
    pub fn parse(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let item_kind = Leaf(p.u16()?);

        Ok(match item_kind {
            Leaf::LF_BCLASS => Self::BaseClass(p.parse()?),
            Leaf::LF_VBCLASS => Self::DirectVirtualBaseClass(p.parse()?),
            Leaf::LF_IVBCLASS => Self::IndirectVirtualBaseClass(p.parse()?),
            Leaf::LF_ENUMERATE => Self::Enumerate(p.parse()?),
            Leaf::LF_FRIENDFCN => Self::FriendFn(p.parse()?),

            Leaf::LF_INDEX => {
                p.skip(2)?; // padding
                let ty = p.type_index()?;
                Self::Index(ty)
            }

            Leaf::LF_MEMBER => Self::Member(p.parse()?),
            Leaf::LF_STMEMBER => Self::StaticMember(p.parse()?),
            Leaf::LF_METHOD => Self::Method(p.parse()?),
            Leaf::LF_NESTEDTYPE => Self::NestedType(p.parse()?),

            Leaf::LF_VFUNCTAB => {
                p.skip(2)?; // padding
                let vtable_ty = p.type_index()?;
                Self::VFuncTable(vtable_ty)
            }

            Leaf::LF_FRIENDCLS => {
                p.skip(2)?; // padding
                let ty = p.type_index()?; // friend class type
                Self::FriendClass(ty)
            }

            Leaf::LF_ONEMETHOD => Self::OneMethod(p.parse()?),
            Leaf::LF_VFUNCOFF => Self::VFuncOffset(p.parse()?),
            Leaf::LF_NESTEDTYPEEX => Self::NestedTypeEx(p.parse()?),

            unknown_item_kind => {
                error!(?unknown_item_kind, "unrecognized item within LF_FIELDLIST",);
                return Err(ParserError::new());
            }
        })
    }
}

```

`codeview/src/types/iter.rs`:

```rs
//! Code for iterating through type streams

use super::Leaf;
use crate::parser::{Parser, ParserError, ParserMut};
use crate::utils::iter::{HasRestLen, IteratorWithRangesExt};
use std::mem::take;

/// Parses a type record stream and iterates `TypeRecord` values.
#[derive(Clone)]
pub struct TypesIter<'a> {
    buffer: &'a [u8],
}

impl<'a> TypesIter<'a> {
    /// Starts a new iterator.
    pub fn new(buffer: &'a [u8]) -> Self {
        Self { buffer }
    }

    /// Returns the "rest" of the data that has not been parsed.
    pub fn rest(&self) -> &'a [u8] {
        self.buffer
    }
}

impl<'a> HasRestLen for TypesIter<'a> {
    fn rest_len(&self) -> usize {
        self.buffer.len()
    }
}

impl<'a> Iterator for TypesIter<'a> {
    type Item = TypeRecord<'a>;

    /// Finds the next type record
    ///
    /// This implementation makes an important guarantee: If it cannot decode the next record,
    /// it _will not_ change `self.buffer`. This is important because it allows an application
    /// to detect the exact length and contents of an unparseable record.
    fn next(&mut self) -> Option<TypeRecord<'a>> {
        if self.buffer.is_empty() {
            return None;
        }

        let mut p = Parser::new(self.buffer);

        let record_len = p.u16().ok()?;
        if record_len < 2 {
            // Type record has length that is too short to be valid
            return None;
        }

        let type_kind = p.u16().ok()?;

        let Ok(record_data) = p.bytes(record_len as usize - 2) else {
            // Type record is too short to be valid.
            return None;
        };

        self.buffer = p.into_rest();

        Some(TypeRecord {
            data: record_data,
            kind: Leaf(type_kind),
        })
    }
}

/// Represents a record that was enumerated within a type record stream (the TPI or IPI).
#[derive(Clone)]
pub struct TypeRecord<'a> {
    /// Indicates how to interpret the payload (the `data` field).
    pub kind: Leaf,
    /// Record data. This does NOT include `kind` and the record data length.
    pub data: &'a [u8],
}

impl<'a> TypeRecord<'a> {
    /// Parses the payload of this type record.
    pub fn parse(&self) -> Result<crate::types::TypeData<'a>, ParserError> {
        crate::types::TypeData::parse(self.kind, &mut Parser::new(self.data))
    }
}

/// Builds a "starts" table that gives the starting location of each type record.
pub fn build_types_starts(num_records_expected: usize, type_records: &[u8]) -> Vec<u32> {
    let mut starts: Vec<u32> = Vec::with_capacity(num_records_expected + 1);
    let mut iter = TypesIter::new(type_records).with_ranges();

    // This loop pushes a byte offset (pos) for the start of every record, plus 1 additional
    // value at the end of the sequence.  This will correctly handle the case where the last
    // record has some undecodable garbage at the end.
    loop {
        let pos = iter.pos();
        starts.push(pos as u32);

        if iter.next().is_none() {
            break;
        }
    }

    starts.shrink_to_fit();
    starts
}

/// Parses a type record stream and iterates `TypeRecord` values.
pub struct TypesIterMut<'a> {
    buffer: &'a mut [u8],
}

impl<'a> TypesIterMut<'a> {
    /// Starts a new iterator.
    pub fn new(buffer: &'a mut [u8]) -> Self {
        Self { buffer }
    }
}

impl<'a> HasRestLen for TypesIterMut<'a> {
    fn rest_len(&self) -> usize {
        self.buffer.len()
    }
}

impl<'a> Iterator for TypesIterMut<'a> {
    type Item = TypeRecordMut<'a>;

    fn next(&mut self) -> Option<TypeRecordMut<'a>> {
        if self.buffer.is_empty() {
            return None;
        }

        let mut parser = ParserMut::new(take(&mut self.buffer));

        let record_len = parser.u16().ok()?;
        if record_len < 2 {
            // Type record has length that is too short to be valid
            return None;
        }

        let type_kind = parser.u16().ok()?;

        let Ok(record_data) = parser.bytes_mut(record_len as usize - 2) else {
            // Type record is too short to be valid.
            return None;
        };

        self.buffer = parser.into_rest();

        Some(TypeRecordMut {
            data: record_data,
            kind: Leaf(type_kind),
        })
    }
}

/// Represents a record that was enumerated within a type record stream (the TPI or IPI).
/// Allows mutable access.
pub struct TypeRecordMut<'a> {
    /// Indicates how to interpret the payload (the `data` field).
    pub kind: Leaf,
    /// Record data. This does NOT include `kind` and the record data length.
    pub data: &'a mut [u8],
}

```

`codeview/src/types/kind.rs`:

```rs
/// Identifies type records. Also called "leaf" records.
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub struct Leaf(pub u16);

macro_rules! cv_leaf {
    (
        $(
            $code:expr, $name:ident ;
        )*
    ) => {
        #[allow(non_upper_case_globals)]
        #[allow(missing_docs)]
        impl Leaf {
            $(
                pub const $name: Leaf = Leaf($code);
            )*
        }

        static LEAF_NAMES: &[(Leaf, &str)] = &[
            $(
                (Leaf($code), stringify!($name)),
            )*
        ];
    }
}

cv_leaf! {
    0x0001, LF_MODIFIER_16t;
    0x0002, LF_POINTER_16t;
    0x0003, LF_ARRAY_16t;
    0x0004, LF_CLASS_16t;
    0x0005, LF_STRUCTURE_16t;
    0x0006, LF_UNION_16t;
    0x0007, LF_ENUM_16t;
    0x0008, LF_PROCEDURE_16t;
    0x0009, LF_MFUNCTION_16t;
    0x000a, LF_VTSHAPE;
    0x000c, LF_COBOL1;
    0x000e, LF_LABEL;
    0x000f, LF_NULL;
    0x0014, LF_ENDPRECOMP;
    0x020c, LF_REFSYM;
    0x040b, LF_FRIENDCLS;   // (in field list) friend class
    0x1001, LF_MODIFIER;
    0x1002, LF_POINTER;
    0x1008, LF_PROCEDURE;
    0x1009, LF_MFUNCTION;
    0x100a, LF_COBOL0;
    0x100b, LF_BARRAY;
    0x100d, LF_VFTPATH;
    0x100f, LF_OEM;
    0x1011, LF_OEM2;
    0x1200, LF_SKIP;
    0x1201, LF_ARGLIST;
    0x1203, LF_FIELDLIST;
    0x1204, LF_DERIVED;
    0x1205, LF_BITFIELD;
    0x1206, LF_METHODLIST;
    0x1207, LF_DIMCONU;
    0x1208, LF_DIMCONLU;
    0x1209, LF_DIMVARU;
    0x120a, LF_DIMVARLU;
    0x1400, LF_BCLASS;      // (in field list) real (non-virtual) base class
    0x1401, LF_VBCLASS;     // (in field list) direct virtual base class
    0x1402, LF_IVBCLASS;    // (in field list) indirect virtual base class
    0x1404, LF_INDEX;       // (in field list) index to another type record
    0x1409, LF_VFUNCTAB;    // (in field list) virtual function table pointer
    0x140c, LF_VFUNCOFF;    // (in field list) virtual function offset
    0x1502, LF_ENUMERATE;   // (in field list) an enumerator value
    0x1503, LF_ARRAY;
    0x1504, LF_CLASS;
    0x1505, LF_STRUCTURE;
    0x1506, LF_UNION;
    0x1507, LF_ENUM;
    0x1508, LF_DIMARRAY;
    0x1509, LF_PRECOMP;
    0x150a, LF_ALIAS;
    0x150b, LF_DEFARG;
    0x150c, LF_FRIENDFCN;   // (in field list) friend function
    0x150d, LF_MEMBER;      // (in field list) data member
    0x150e, LF_STMEMBER;    // (in field list) static data member
    0x150f, LF_METHOD;      // (in field list) method group (overloaded methods), not single method
    0x1510, LF_NESTEDTYPE;  // (in field list) nested type definition
    0x1511, LF_ONEMETHOD;   // (in field list) a single method
    0x1512, LF_NESTEDTYPEEX;// (in field list) nested type extended definition
    0x1514, LF_MANAGED;
    0x1515, LF_TYPESERVER2;
    0x1519, LF_INTERFACE;
    0x151d, LF_VFTABLE;

    // --- end of types ---

    // 0x1601..=0x1607 are only present in IPI stream, not TPI stream.

    0x1601, LF_FUNC_ID;         // global func ID
    0x1602, LF_MFUNC_ID;        // member func ID
    0x1603, LF_BUILDINFO;       // build info: tool, version, command line, src/pdb file
    0x1604, LF_SUBSTR_LIST;     // similar to LF_ARGLIST, for list of sub strings
    0x1605, LF_STRING_ID;       // string ID

    // source and line on where an UDT is defined
    // only generated by compiler
    0x1606, LF_UDT_SRC_LINE;

    // module, source and line on where an UDT is defined
    // only generated by linker
    0x1607, LF_UDT_MOD_SRC_LINE;

    // The following four kinds were added to the wrong place in this enumeration.
    // They should have been added befor LF_TYPE_LAST.
    // But now it has been too late to change this :-(

    0x1608, LF_CLASS2;       // LF_CLASS with 32bit property field
    0x1609, LF_STRUCTURE2;   // LF_STRUCTURE with 32bit property field
    0x160a, LF_UNION2;       // LF_UNION with 32bit property field
    0x160b, LF_INTERFACE2;   // LF_INTERFACE with 32bit property field

    // These values are used for encoding numeric constants.
//    0x8000, LF_NUMERIC;
    0x8000, LF_CHAR;            // i8
    0x8001, LF_SHORT;           // i16
    0x8002, LF_USHORT;          // u16
    0x8003, LF_LONG;            // i32
    0x8004, LF_ULONG;           // u32
    0x8005, LF_REAL32;          // f32
    0x8006, LF_REAL64;          // f64
    0x8007, LF_REAL80;
    0x8008, LF_REAL128;
    0x8009, LF_QUADWORD;        // i64
    0x800a, LF_UQUADWORD;       // u64
    0x800b, LF_REAL48;
    0x800c, LF_COMPLEX32;
    0x800d, LF_COMPLEX64;
    0x800e, LF_COMPLEX80;
    0x800f, LF_COMPLEX128;
    0x8010, LF_VARSTRING;       // string prefixed with u16 length
    0x8017, LF_OCTWORD;         // i128
    0x8018, LF_UOCTWORD;        // u128
    0x8019, LF_DECIMAL;
    0x801a, LF_DATE;            // 8 bytes
    0x801b, LF_UTF8STRING;      // NUL-terminated UTF-8 string
    0x801c, LF_REAL16;
}

impl std::fmt::Debug for Leaf {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        if let Ok(index) = LEAF_NAMES.binary_search_by_key(self, |ii| ii.0) {
            fmt.write_str(LEAF_NAMES[index].1)
        } else {
            let b0 = (self.0 & 0xff) as u8;
            let b1 = (self.0 >> 8) as u8;
            fn to_c(b: u8) -> char {
                if (32..=126).contains(&b) {
                    char::from(b)
                } else {
                    '_'
                }
            }

            write!(fmt, "Leaf(??{:04x} {}{})", self.0, to_c(b0), to_c(b1))
        }
    }
}

impl Leaf {
    /// True if this `Leaf` codes for an immediate numeric constant.
    pub fn is_immediate_numeric(self) -> bool {
        self.0 < 0x8000
    }

    /// Checks whether this `Leaf` can be used as a type record.
    pub fn can_start_record(self) -> bool {
        matches!(
            self,
            Leaf::LF_MODIFIER
                | Leaf::LF_POINTER
                | Leaf::LF_ARRAY
                | Leaf::LF_CLASS
                | Leaf::LF_STRUCTURE
                | Leaf::LF_UNION
                | Leaf::LF_ENUM
                | Leaf::LF_PROCEDURE
                | Leaf::LF_MFUNCTION
                | Leaf::LF_VTSHAPE
                | Leaf::LF_COBOL0
                | Leaf::LF_COBOL1
                | Leaf::LF_BARRAY
                | Leaf::LF_LABEL
                | Leaf::LF_NULL
                | Leaf::LF_DIMARRAY
                | Leaf::LF_VFTPATH
                | Leaf::LF_PRECOMP
                | Leaf::LF_ENDPRECOMP
                | Leaf::LF_OEM
                | Leaf::LF_OEM2
                | Leaf::LF_ALIAS
                | Leaf::LF_MANAGED
                | Leaf::LF_TYPESERVER2
        )
    }

    /// Checks whether this `Leaf` can be used within a field list record.
    pub fn is_nested_leaf(self) -> bool {
        matches!(
            self,
            Leaf::LF_SKIP
                | Leaf::LF_ARGLIST
                | Leaf::LF_DEFARG
                | Leaf::LF_FIELDLIST
                | Leaf::LF_DERIVED
                | Leaf::LF_BITFIELD
                | Leaf::LF_METHODLIST
                | Leaf::LF_DIMCONU
                | Leaf::LF_DIMCONLU
                | Leaf::LF_DIMVARU
                | Leaf::LF_DIMVARLU
                | Leaf::LF_REFSYM
        )
    }

    /// Indicates whether a given type record can contain references to other type records.
    pub fn can_reference_types(self) -> bool {
        matches!(
            self,
            Leaf::LF_MODIFIER
                | Leaf::LF_POINTER
                | Leaf::LF_ARRAY
                | Leaf::LF_CLASS
                | Leaf::LF_UNION
                | Leaf::LF_ENUM
                | Leaf::LF_PROCEDURE
        )
    }
}

```

`codeview/src/types/number.rs`:

```rs
//! Section 4, numeric leaves

use super::Leaf;
use crate::parser::{Parse, Parser, ParserError};
use bstr::BStr;
use pretty_hex::PrettyHex;
use std::fmt::{Debug, Display};
use std::num::TryFromIntError;
use tracing::warn;

/// A numeric constant defined within a CodeView type or symbol record.
///
/// # References
/// * "Numeric Leaves" section of PDB specification.
#[derive(Copy, Clone)]
#[repr(transparent)]
pub struct Number<'a> {
    bytes: &'a [u8],
}

impl<'a> Number<'a> {
    /// Gets the raw bytes of this `Number`.
    pub fn as_bytes(&self) -> &'a [u8] {
        self.bytes
    }

    /// Gets the kind (representation) of this value.
    /// If this is an immediate value (integer in `0..=0x7fff`), gets the actual value.
    pub fn kind(&self) -> Leaf {
        let mut p = Parser::new(self.bytes);
        Leaf(p.u16().unwrap())
    }
}

impl<'a> Parse<'a> for Number<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let start = p.peek_rest();

        let more_len = match Leaf(p.u16()?) {
            lf if lf.is_immediate_numeric() => 0,
            Leaf::LF_CHAR => 1,
            Leaf::LF_SHORT => 2,
            Leaf::LF_USHORT => 2,
            Leaf::LF_LONG => 4,
            Leaf::LF_ULONG => 4,
            Leaf::LF_REAL32 => 4,
            Leaf::LF_REAL64 => 8,
            Leaf::LF_REAL80 => 10,
            Leaf::LF_REAL128 => 16,
            Leaf::LF_QUADWORD => 8,
            Leaf::LF_UQUADWORD => 8,
            Leaf::LF_REAL48 => 6,
            Leaf::LF_COMPLEX32 => 8,
            Leaf::LF_COMPLEX64 => 16,
            Leaf::LF_COMPLEX80 => 20,
            Leaf::LF_COMPLEX128 => 32,
            Leaf::LF_VARSTRING => p.u16()? as usize,
            Leaf::LF_OCTWORD => 16,
            Leaf::LF_UOCTWORD => 16,
            Leaf::LF_DECIMAL => 16,
            Leaf::LF_DATE => 8,
            Leaf::LF_UTF8STRING => {
                p.skip_strz()?;
                0
            }
            Leaf::LF_REAL16 => 2,
            lf => {
                warn!(leaf = ?lf, "unrecognized numeric leaf");
                // We don't know how many bytes to consume, so we can't keep parsing.
                return Err(ParserError::new());
            }
        };

        p.skip(more_len)?;
        Ok(Self {
            bytes: &start[..start.len() - p.len()],
        })
    }
}

impl<'a> Number<'a> {}

macro_rules! try_from_number {
    (
        $t:ty
    ) => {
        impl<'a> TryFrom<Number<'a>> for $t {
            type Error = TryFromIntError;

            #[inline(never)]
            fn try_from(value: Number<'a>) -> Result<Self, Self::Error> {
                use map_parser_error_to_int_error as e;

                let mut p = Parser::new(value.bytes);
                Ok(match Leaf(e(p.u16())?) {
                    lf if lf.is_immediate_numeric() => Self::try_from(lf.0)?,
                    Leaf::LF_USHORT => Self::try_from(e(p.u16())?)?,
                    Leaf::LF_ULONG => Self::try_from(e(p.u32())?)?,
                    Leaf::LF_UQUADWORD => Self::try_from(e(p.u64())?)?,
                    Leaf::LF_CHAR => Self::try_from(e(p.i8())?)?,
                    Leaf::LF_SHORT => Self::try_from(e(p.i16())?)?,
                    Leaf::LF_LONG => Self::try_from(e(p.i32())?)?,
                    Leaf::LF_QUADWORD => Self::try_from(e(p.i64())?)?,
                    Leaf::LF_OCTWORD => Self::try_from(e(p.i128())?)?,
                    Leaf::LF_UOCTWORD => Self::try_from(e(p.u128())?)?,
                    _ => return Err(try_from_int_error()),
                })
            }
        }
    };
}

try_from_number!(i8);
try_from_number!(i16);
try_from_number!(i32);
try_from_number!(i64);
try_from_number!(i128);

try_from_number!(u8);
try_from_number!(u16);
try_from_number!(u32);
try_from_number!(u64);
try_from_number!(u128);

fn map_parser_error_to_int_error<T>(r: Result<T, ParserError>) -> Result<T, TryFromIntError> {
    match r {
        Ok(x) => Ok(x),
        Err(ParserError) => Err(try_from_int_error()),
    }
}

/// Error type for conversions from `Number` to `f32`
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
pub struct TryFromFloatError;

impl From<ParserError> for TryFromFloatError {
    fn from(_: ParserError) -> Self {
        Self
    }
}

impl<'a> TryFrom<Number<'a>> for f32 {
    type Error = TryFromFloatError;

    fn try_from(value: Number<'a>) -> Result<Self, Self::Error> {
        let mut p = Parser::new(value.bytes);
        Ok(match Leaf(p.u16()?) {
            Leaf::LF_REAL32 => f32::from_le_bytes(p.array()?),
            _ => return Err(TryFromFloatError),
        })
    }
}

impl<'a> TryFrom<Number<'a>> for f64 {
    type Error = TryFromFloatError;

    fn try_from(value: Number<'a>) -> Result<Self, Self::Error> {
        let mut p = Parser::new(value.bytes);
        Ok(match Leaf(p.u16()?) {
            Leaf::LF_REAL32 => f32::from_le_bytes(p.array::<4>()?) as f64,
            Leaf::LF_REAL64 => f64::from_le_bytes(p.array::<8>()?),
            _ => return Err(TryFromFloatError),
        })
    }
}

/// Error type for conversions from `Number` to string
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
pub struct TryFromStrError;

impl From<ParserError> for TryFromStrError {
    fn from(_: ParserError) -> Self {
        Self
    }
}
impl<'a> TryFrom<Number<'a>> for &'a BStr {
    type Error = TryFromStrError;

    fn try_from(value: Number<'a>) -> Result<Self, Self::Error> {
        let mut p = Parser::new(value.bytes);
        Ok(match Leaf(p.u16()?) {
            Leaf::LF_UTF8STRING => p.strz()?,
            Leaf::LF_VARSTRING => {
                let len = p.u16()?;
                let bytes = p.bytes(len as usize)?;
                BStr::new(bytes)
            }
            _ => return Err(TryFromStrError),
        })
    }
}

impl<'a> Debug for Number<'a> {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        Display::fmt(self, f)
    }
}

impl<'a> Display for Number<'a> {
    #[inline(never)]
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        fn e<T>(
            f: &mut std::fmt::Formatter<'_>,
            r: Result<T, ParserError>,
        ) -> Result<T, std::fmt::Error> {
            match r {
                Ok(x) => Ok(x),
                Err(ParserError) => {
                    f.write_str("??(parser error)")?;
                    Err(std::fmt::Error)
                }
            }
        }

        let mut p = Parser::new(self.bytes);

        match Leaf(p.u16().unwrap()) {
            lf if lf.is_immediate_numeric() => Display::fmt(&lf.0, f),
            Leaf::LF_CHAR => Display::fmt(&e(f, p.i8())?, f),
            Leaf::LF_SHORT => Display::fmt(&e(f, p.i16())?, f),
            Leaf::LF_USHORT => Display::fmt(&e(f, p.u16())?, f),
            Leaf::LF_LONG => Display::fmt(&e(f, p.i32())?, f),
            Leaf::LF_ULONG => Display::fmt(&e(f, p.u32())?, f),
            Leaf::LF_REAL32 => Display::fmt(&e(f, p.f32())?, f),
            Leaf::LF_REAL64 => Display::fmt(&e(f, p.f64())?, f),
            Leaf::LF_QUADWORD => Display::fmt(&e(f, p.i64())?, f),
            Leaf::LF_UQUADWORD => Display::fmt(&e(f, p.u64())?, f),
            Leaf::LF_VARSTRING => {
                // This uses a 2-byte length prefix, not 1-byte.
                let len = p.u16().unwrap();
                let s = BStr::new(p.bytes(len as usize).unwrap());
                <BStr as Display>::fmt(s, f)
            }
            Leaf::LF_OCTWORD => Display::fmt(&e(f, p.i128())?, f),
            Leaf::LF_UOCTWORD => Display::fmt(&e(f, p.u128())?, f),
            Leaf::LF_UTF8STRING => {
                let s = p.strz().unwrap();
                <BStr as Display>::fmt(s, f)
            }

            lf => {
                write!(f, "?? {lf:?} {:?}", self.bytes.hex_dump())
            }
        }
    }
}

fn try_from_int_error() -> TryFromIntError {
    u32::try_from(-1i8).unwrap_err()
}

#[cfg(test)]
fn parse_number(bytes: &[u8]) -> Number<'_> {
    let mut p = Parser::new(bytes);
    let n = p.number().unwrap();
    assert!(p.is_empty());
    n
}

#[test]
fn number_error() {
    assert!(Number::parse(&[]).is_err()); // too short
    assert!(Number::parse(&[0]).is_err()); // also too short
    assert!(Number::parse(&[0xff, 0xff]).is_err()); // unrecognized kind
}

#[test]
fn number_immediate() {
    // Values below 0x8000 are literal uint16 constants.
    let n = parse_number(&[0xaa, 0x70]);
    assert_eq!(n.as_bytes(), &[0xaa, 0x70]);
    assert_eq!(u32::try_from(n).unwrap(), 0x70aa);
}

#[test]
fn number_char() {
    // LF_CHAR
    let n = parse_number(&[0x00, 0x80, (-33i8) as u8]);
    assert_eq!(i32::try_from(n).unwrap(), -33);

    assert!(f32::try_from(n).is_err());
    assert!(f64::try_from(n).is_err());
    assert!(<&BStr>::try_from(n).is_err());
}

#[test]
fn number_short() {
    // LF_SHORT
    let n = parse_number(&[0x01, 0x80, 0xaa, 0x55]);
    assert_eq!(i32::try_from(n).unwrap(), 0x55aa_i32);
    assert_eq!(u32::try_from(n).unwrap(), 0x55aa_u32);

    let n = parse_number(&[0x01, 0x80, 0x55, 0xaa]);
    assert_eq!(i32::try_from(n).unwrap(), -21931_i32);
    assert!(u32::try_from(n).is_err());

    assert!(f32::try_from(n).is_err());
    assert!(f64::try_from(n).is_err());
    assert!(<&BStr>::try_from(n).is_err());
}

#[test]
fn number_long() {
    // LF_LONG
    let n = parse_number(&[0x03, 0x80, 1, 2, 3, 4]);
    assert_eq!(u32::try_from(n).unwrap(), 0x04030201_u32);
    assert_eq!(i32::try_from(n).unwrap(), 0x04030201_i32);
    assert!(u16::try_from(n).is_err());
    assert!(i16::try_from(n).is_err());
    assert!(u8::try_from(n).is_err());
    assert!(i8::try_from(n).is_err());

    // unsigned cannot decode negative numbers
    let n = parse_number(&[0x03, 0x80, 0xfe, 0xff, 0xff, 0xff]);
    assert!(u8::try_from(n).is_err());
    assert!(u16::try_from(n).is_err());
    assert!(u32::try_from(n).is_err());
    assert!(u64::try_from(n).is_err());
    assert!(u128::try_from(n).is_err());
    assert_eq!(i8::try_from(n).unwrap(), -2);
    assert_eq!(i16::try_from(n).unwrap(), -2);
    assert_eq!(i32::try_from(n).unwrap(), -2);
    assert_eq!(i64::try_from(n).unwrap(), -2);
    assert_eq!(i128::try_from(n).unwrap(), -2);

    assert!(f32::try_from(n).is_err());
    assert!(f64::try_from(n).is_err());
    assert!(<&BStr>::try_from(n).is_err());
}

#[test]
fn number_real32() {
    use std::f32::consts::PI;

    let b: [u8; 4] = PI.to_le_bytes();
    assert_eq!(b, [0xdb, 0x0f, 0x49, 0x40]); // 0x400490fdb, pi in f32
    println!("f32 PI bytes: {b:#x?}");

    // 8005 is LF_REAL32
    let n = parse_number(&[0x05, 0x80, 0xdb, 0x0f, 0x49, 0x40]);

    // LF_REAL32 is not convertible to any of the integer types
    assert!(u8::try_from(n).is_err());
    assert!(u16::try_from(n).is_err());
    assert!(u32::try_from(n).is_err());
    assert!(u64::try_from(n).is_err());
    assert!(u128::try_from(n).is_err());

    assert!(i8::try_from(n).is_err());
    assert!(i16::try_from(n).is_err());
    assert!(i32::try_from(n).is_err());
    assert!(i64::try_from(n).is_err());
    assert!(i128::try_from(n).is_err());

    // Floating-point exact equality can be weird.
    assert_eq!(f32::try_from(n).unwrap(), PI);

    // We convert to f64 but do not verify the value, because again, floating-point is weird.
    let _ = f64::try_from(n).unwrap();
}

#[test]
fn number_real64() {
    use std::f64::consts::PI;

    let b: [u8; 8] = PI.to_le_bytes();
    assert_eq!(b, [0x18, 0x2d, 0x44, 0x54, 0xfb, 0x21, 0x9, 0x40]);
    // assert_eq!(b, [0xdb, 0x0f, 0x49, 0x40]); // 0x400921fb54442d18, pi in f64
    println!("f64 PI bytes: {b:#x?}");

    // 8006 is LF_REAL64
    let n = parse_number(&[0x06, 0x80, 0x18, 0x2d, 0x44, 0x54, 0xfb, 0x21, 0x9, 0x40]);

    // LF_REAL64 is not convertible to any of the integer types
    assert!(u8::try_from(n).is_err());
    assert!(u16::try_from(n).is_err());
    assert!(u32::try_from(n).is_err());
    assert!(u64::try_from(n).is_err());
    assert!(u128::try_from(n).is_err());

    assert!(i8::try_from(n).is_err());
    assert!(i16::try_from(n).is_err());
    assert!(i32::try_from(n).is_err());
    assert!(i64::try_from(n).is_err());
    assert!(i128::try_from(n).is_err());

    // Floating-point exact equality can be weird.
    assert_eq!(f64::try_from(n).unwrap(), PI);
}

#[test]
fn number_strz() {
    let n = parse_number(b"\x1b\x80Hello, world\0");
    assert_eq!(n.kind(), Leaf::LF_UTF8STRING);
    assert_eq!(<&BStr>::try_from(n).unwrap(), "Hello, world");

    let n = parse_number(&[0x00, 0x80, (-33i8) as u8]);
    assert!(<&BStr>::try_from(n).is_err());
}

#[test]
fn number_varstring() {
    let s = parse_number(b"\x10\x80\x0c\x00Hello, world");
    assert_eq!(s.kind(), Leaf::LF_VARSTRING);
    assert_eq!(<&BStr>::try_from(s).unwrap(), "Hello, world");
}

#[test]
fn number_unsupported_types() {
    // We can test decoding the prefix for these types, even if we can't currently display them
    // or convert them to something useful.
    let cases: &[(Leaf, usize)] = &[
        (Leaf::LF_REAL80, 10),
        (Leaf::LF_REAL128, 16),
        (Leaf::LF_REAL48, 6),
        (Leaf::LF_COMPLEX32, 8),
        (Leaf::LF_COMPLEX64, 16),
        (Leaf::LF_COMPLEX80, 20),
        (Leaf::LF_COMPLEX128, 32),
        (Leaf::LF_DECIMAL, 16),
        (Leaf::LF_DATE, 8),
        (Leaf::LF_REAL16, 2),
    ];

    for &(kind, num_zeroes) in cases.iter() {
        let mut input = vec![0; 2 + num_zeroes];
        input[0] = kind.0 as u8;
        input[1] = (kind.0 >> 8) as u8;
        let n = parse_number(&input);
        assert_eq!(kind, n.kind());
    }
}

#[test]
fn display() {
    let cases: &[(&[u8], &str)] = &[
        (&[0x01, 0x04], "immediate 1025"),
        (&[0x00, 0x80, 0xff], "LF_CHAR -1"),
        (&[0x01, 0x80, 0xfe, 0xff], "LF_SHORT -2"),
        (&[0x02, 0x80, 0xfd, 0xff], "LF_USHORT 65533"),
        (&[0x03, 0x80, 0xfc, 0xff, 0xff, 0xff], "LF_LONG -4"),
        (&[0x04, 0x80, 0x00, 0x00, 0x02, 0x00], "LF_ULONG 131072"),
        (&[0x05, 0x80, 0xdb, 0x0f, 0x49, 0x40], "LF_REAL32 3.1415927"),
        (
            &[0x06, 0x80, 0x18, 0x2d, 0x44, 0x54, 0xfb, 0x21, 0x9, 0x40],
            "LF_REAL64 3.141592653589793",
        ),
        (
            &[0x09, 0x80, 0xfb, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff],
            "LF_QUADWORD -5",
        ),
        (
            &[0x0a, 0x80, 0x00, 0xe4, 0x0b, 0x54, 0x02, 0x00, 0x00, 0x00],
            "LF_UQUADWORD 10000000000",
        ),
        (
            &[
                0x17, 0x80, 0xfa, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,
                0xff, 0xff, 0xff, 0xff,
            ],
            "LF_OCTWORD -6",
        ),
        (
            &[
                0x18, 0x80, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,
                0x00, 0x00, 0x00, 0x00,
            ],
            "LF_UOCTWORD 1",
        ),
    ];

    for &(input, expected_output) in cases.iter() {
        let mut p = Parser::new(input);
        let leaf = Leaf(p.u16().unwrap());

        let n = parse_number(input);

        let actual_output = if leaf.is_immediate_numeric() {
            format!("immediate {n}")
        } else {
            format!("{leaf:?} {n}")
        };

        assert_eq!(actual_output, expected_output, "bytes: {input:#x?}");

        // Cover Debug::fmt. It just trivially defers to Display.
        let _ = format!("{n:?}");
    }
}

#[test]
fn display_bogus() {
    // Because the byte slice within Number is private and Number::parse() does not construct
    // a Number for kinds it does not recognize, it is impossible (outside of this module)
    // to construct a Number over an invalid, non-immediate Leaf value.  But the Display code
    // has to have a case for that, so we construct a bogus Number just so we can display it.
    let bogus_num = Number {
        bytes: &[0xff, 0xff, 0xaa, 0xaa, 0xaa],
    };
    println!("bogus_num = {bogus_num}");
}

```

`codeview/src/types/primitive.rs`:

```rs
//! Primitive types

use super::TypeIndex;

#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_SPECIAL: u32 = 0;
#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_SIGNED_INT: u32 = 1;
#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_UNSIGNED_INT: u32 = 2;
#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_BOOL: u32 = 3;
#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_REAL: u32 = 4;
#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_COMPLEX: u32 = 5;
#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_SPECIAL2: u32 = 6;
#[allow(missing_docs)]
pub const PRIMITIVE_TYPE_REALLY_INT: u32 = 7;

macro_rules! primitives {
    (
        $(
            (
                $value:expr,
                $name:ident,
                $description:expr
            ),
        )*
    ) => {
        /// Contains the names and descriptions of all primitive types
        pub static PRIMITIVES: &[(u32, &str, &str)] = &[
            $(
                ($value, stringify!($name), $description),
            )*
        ];

        impl TypeIndex {
            $(
                #[doc = concat!("Primitive type: `", $description, "`")]
                pub const $name: TypeIndex = TypeIndex($value);
            )*
        }
    }
}

primitives! {
    // number, spec name, C/C++ name
    (0x0000, T_NOTYPE, "none"),
    (0x0001, T_ABS, "absolute symbol"),
    (0x0002, T_SEGMENT, "segment type"),
    (0x0003, T_VOID, "void"),
    (0x0004, T_CURRENCY, "BASIC 8 byte currency value"),
    (0x0005, T_NBASICSTR, "Near BASIC string"),
    (0x0006, T_FBASICSTR, "Far BASIC string"),
    (0x0007, T_NOTTRANS, "<type-not-translated>"),
    (0x0008, T_HRESULT, "HRESULT"),
    (0x0010, T_CHAR, "char"),
    (0x0011, T_SHORT, "short"),
    (0x0012, T_LONG, "long"),
    (0x0013, T_QUAD, "long long"),
    (0x0014, T_OCT, "__int128"),
    (0x0020, T_UCHAR, "unsigned char"),
    (0x0021, T_USHORT, "unsigned short"),
    (0x0022, T_ULONG, "unsigned long"),
    (0x0023, T_UQUAD, "unsigned long long"),
    (0x0024, T_UOCT, "unsigned __int128"),
    (0x0030, T_BOOL8, "bool"),
    (0x0031, T_BOOL16, "bool16"),
    (0x0032, T_BOOL32, "bool32"),
    (0x0033, T_BOOL64, "bool64"),
    (0x0040, T_REAL32, "float"),
    (0x0041, T_REAL64, "double"),
    (0x0042, T_REAL80, "80 bit real"),
    (0x0043, T_REAL128, "128 bit real"),
    (0x0044, T_REAL48, "48 bit real"),
    (0x0045, T_REAL32PP, "32 bit PP real"),
    (0x0046, T_REAL16, "16 bit real"),
    (0x0050, T_CPLX32, "32 bit complex"),
    (0x0051, T_CPLX64, "64 bit complex"),
    (0x0052, T_CPLX80, "80 bit complex"),
    (0x0053, T_CPLX128, "128 bit complex"),
    (0x0060, T_BIT, "bit"),
    (0x0061, T_PASCHAR, "Pascal CHAR"),
    (0x0062, T_BOOL32FF, "32-bit BOOL where true is 0xffffffff"),
    (0x0068, T_INT1, "__int8"),
    (0x0069, T_UINT1, "unsigned __int8"),
    (0x0070, T_RCHAR, "char"), // really a character. This is "char", which is distinct from "signed char" and "unsigned char"
    (0x0071, T_WCHAR, "wchar_t"),
    (0x0072, T_INT2, "__int16"),
    (0x0073, T_UINT2, "unsigned __int16"),
    (0x0074, T_INT4, "__int32"), // really 32-bit
    (0x0075, T_UINT4, "unsigned __int32"),
    (0x0076, T_INT8, "__int64"),
    (0x0077, T_UINT8, "unsigned __int64"),
    (0x0078, T_INT16, "128 bit signed int"),
    (0x0079, T_UINT16, "128 bit unsigned int"),
    (0x007a, T_CHAR16, "char16"),
    (0x007b, T_CHAR32, "char32"),
    (0x007c, T_CHAR8, "char8"),
    // 32-bit pointer types
    (0x0103, T_PVOID, "near pointer to void"),
    (0x0110, T_PCHAR, "16 bit pointer to 8 bit signed"),
    (0x0111, T_PSHORT, "16 bit pointer to 16 bit signed"),
    (0x0112, T_PLONG, "16 bit pointer to 32 bit signed"),
    (0x0113, T_PQUAD, "16 bit pointer to 64 bit signed"),
    (0x0114, T_POCT, "16 bit pointer to 128 bit signed"),
    (0x0120, T_PUCHAR, "16 bit pointer to 8 bit unsigned"),
    (0x0121, T_PUSHORT, "16 bit pointer to 16 bit unsigned"),
    (0x0122, T_PULONG, "16 bit pointer to 32 bit unsigned"),
    (0x0123, T_PUQUAD, "16 bit pointer to 64 bit unsigned"),
    (0x0124, T_PUOCT, "16 bit pointer to 128 bit unsigned"),
    (0x0130, T_PBOOL08, "16 bit pointer to  8 bit boolean"),
    (0x0131, T_PBOOL16, "16 bit pointer to 16 bit boolean"),
    (0x0132, T_PBOOL32, "16 bit pointer to 32 bit boolean"),
    (0x0133, T_PBOOL64, "16 bit pointer to 64 bit boolean"),
    (0x0140, T_PREAL32, "16 bit pointer to 32 bit real"),
    (0x0141, T_PREAL64, "16 bit pointer to 64 bit real"),
    (0x0142, T_PREAL80, "16 bit pointer to 80 bit real"),
    (0x0143, T_PREAL128, "16 bit pointer to 128 bit real"),
    (0x0144, T_PREAL48, "16 bit pointer to 48 bit real"),
    (0x0145, T_PREAL32PP, "16 bit pointer to 32 bit PP real"),
    (0x0146, T_PREAL16, "16 bit pointer to 16 bit real"),
    (0x0150, T_PCPLX32, "16 bit pointer to 32 bit complex"),
    (0x0151, T_PCPLX64, "16 bit pointer to 64 bit complex"),
    (0x0152, T_PCPLX80, "16 bit pointer to 80 bit complex"),
    (0x0153, T_PCPLX128, "16 bit pointer to 128 bit complex"),
    (0x0168, T_PINT1, "16 bit pointer to 8 bit signed int"),
    (0x0169, T_PUINT1, "16 bit pointer to 8 bit unsigned int"),
    (0x0170, T_PRCHAR, "16 bit pointer to a real char"),
    (0x0171, T_PWCHAR, "16 bit pointer to a wide char"),
    (0x0172, T_PINT2, "16 bit pointer to 16 bit signed int"),
    (0x0173, T_PUINT2, "16 bit pointer to 16 bit unsigned int"),
    (0x0174, T_PINT4, "16 bit pointer to 32 bit signed int"),
    (0x0175, T_PUINT4, "16 bit pointer to 32 bit unsigned int"),
    (0x0176, T_PINT8, "16 bit pointer to 64 bit signed int"),
    (0x0177, T_PUINT8, "16 bit pointer to 64 bit unsigned int"),
    (0x0178, T_PINT16, "16 bit pointer to 128 bit signed int"),
    (0x0179, T_PUINT16, "16 bit pointer to 128 bit unsigned int"),
    (0x0203, T_PFVOID, "far pointer to void"),
    (0x0210, T_PFCHAR, "16:16 far pointer to 8 bit signed"),
    (0x0211, T_PFSHORT, "16:16 far pointer to 16 bit signed"),
    (0x0212, T_PFLONG, "16:16 far pointer to 32 bit signed"),
    (0x0213, T_PFQUAD, "16:16 far pointer to 64 bit signed"),
    (0x0214, T_PFOCT, "16:16 far pointer to 128 bit signed"),
    (0x0220, T_PFUCHAR, "16:16 far pointer to 8 bit unsigned"),
    (0x0221, T_PFUSHORT, "16:16 far pointer to 16 bit unsigned"),
    (0x0222, T_PFULONG, "16:16 far pointer to 32 bit unsigned"),
    (0x0223, T_PFUQUAD, "16:16 far pointer to 64 bit unsigned"),
    (0x0224, T_PFUOCT, "16:16 far pointer to 128 bit unsigned"),
    (0x0230, T_PFBOOL08, "16:16 far pointer to  8 bit boolean"),
    (0x0231, T_PFBOOL16, "16:16 far pointer to 16 bit boolean"),
    (0x0232, T_PFBOOL32, "16:16 far pointer to 32 bit boolean"),
    (0x0233, T_PFBOOL64, "16:16 far pointer to 64 bit boolean"),
    (0x0240, T_PFREAL32, "16:16 far pointer to 32 bit real"),
    (0x0241, T_PFREAL64, "16:16 far pointer to 64 bit real"),
    (0x0242, T_PFREAL80, "16:16 far pointer to 80 bit real"),
    (0x0243, T_PFREAL128, "16:16 far pointer to 128 bit real"),
    (0x0244, T_PFREAL48, "16:16 far pointer to 48 bit real"),
    (0x0245, T_PFREAL32PP, "16:16 far pointer to 32 bit PP real"),
    (0x0246, T_PFREAL16, "16:16 far pointer to 16 bit real"),
    (0x0250, T_PFCPLX32, "16:16 far pointer to 32 bit complex"),
    (0x0251, T_PFCPLX64, "16:16 far pointer to 64 bit complex"),
    (0x0252, T_PFCPLX80, "16:16 far pointer to 80 bit complex"),
    (0x0253, T_PFCPLX128, "16:16 far pointer to 128 bit complex"),
    (0x0268, T_PFINT1, "16:16 far pointer to 8 bit signed int"),
    (0x0269, T_PFUINT1, "16:16 far pointer to 8 bit unsigned int"),
    (0x0270, T_PFRCHAR, "16:16 far pointer to a real char"),
    (0x0271, T_PFWCHAR, "16:16 far pointer to a wide char"),
    (0x0272, T_PFINT2, "16:16 far pointer to 16 bit signed int"),
    (0x0273, T_PFUINT2, "16:16 far pointer to 16 bit unsigned int"),
    (0x0274, T_PFINT4, "16:16 far pointer to 32 bit signed int"),
    (0x0275, T_PFUINT4, "16:16 far pointer to 32 bit unsigned int"),
    (0x0276, T_PFINT8, "16:16 far pointer to 64 bit signed int"),
    (0x0277, T_PFUINT8, "16:16 far pointer to 64 bit unsigned int"),
    (0x0278, T_PFINT16, "16:16 far pointer to 128 bit signed int"),
    (0x0279, T_PFUINT16, "16:16 far pointer to 128 bit unsigned int"),
    (0x0303, T_PHVOID, "huge pointer to void"),
    (0x0310, T_PHCHAR, "16:16 huge pointer to 8 bit signed"),
    (0x0311, T_PHSHORT, "16:16 huge pointer to 16 bit signed"),
    (0x0312, T_PHLONG, "16:16 huge pointer to 32 bit signed"),
    (0x0313, T_PHQUAD, "16:16 huge pointer to 64 bit signed"),
    (0x0314, T_PHOCT, "16:16 huge pointer to 128 bit signed"),
    (0x0320, T_PHUCHAR, "16:16 huge pointer to 8 bit unsigned"),
    (0x0321, T_PHUSHORT, "16:16 huge pointer to 16 bit unsigned"),
    (0x0322, T_PHULONG, "16:16 huge pointer to 32 bit unsigned"),
    (0x0323, T_PHUQUAD, "16:16 huge pointer to 64 bit unsigned"),
    (0x0324, T_PHUOCT, "16:16 huge pointer to 128 bit unsigned"),
    (0x0330, T_PHBOOL08, "16:16 huge pointer to  8 bit boolean"),
    (0x0331, T_PHBOOL16, "16:16 huge pointer to 16 bit boolean"),
    (0x0332, T_PHBOOL32, "16:16 huge pointer to 32 bit boolean"),
    (0x0333, T_PHBOOL64, "16:16 huge pointer to 64 bit boolean"),
    (0x0340, T_PHREAL32, "16:16 huge pointer to 32 bit real"),
    (0x0341, T_PHREAL64, "16:16 huge pointer to 64 bit real"),
    (0x0342, T_PHREAL80, "16:16 huge pointer to 80 bit real"),
    (0x0343, T_PHREAL128, "16:16 huge pointer to 128 bit real"),
    (0x0344, T_PHREAL48, "16:16 huge pointer to 48 bit real"),
    (0x0345, T_PHREAL32PP, "16:16 huge pointer to 32 bit PP real"),
    (0x0346, T_PHREAL16, "16:16 huge pointer to 16 bit real"),
    (0x0350, T_PHCPLX32, "16:16 huge pointer to 32 bit complex"),
    (0x0351, T_PHCPLX64, "16:16 huge pointer to 64 bit complex"),
    (0x0352, T_PHCPLX80, "16:16 huge pointer to 80 bit complex"),
    (0x0353, T_PHCPLX128, "16:16 huge pointer to 128 bit real"),
    (0x0368, T_PHINT1, "16:16 huge pointer to 8 bit signed int"),
    (0x0369, T_PHUINT1, "16:16 huge pointer to 8 bit unsigned int"),
    (0x0370, T_PHRCHAR, "16:16 huge pointer to a real char"),
    (0x0371, T_PHWCHAR, "16:16 huge pointer to a wide char"),
    (0x0372, T_PHINT2, "16:16 huge pointer to 16 bit signed int"),
    (0x0373, T_PHUINT2, "16:16 huge pointer to 16 bit unsigned int"),
    (0x0374, T_PHINT4, "16:16 huge pointer to 32 bit signed int"),
    (0x0375, T_PHUINT4, "16:16 huge pointer to 32 bit unsigned int"),
    (0x0376, T_PHINT8, "16:16 huge pointer to 64 bit signed int"),
    (0x0377, T_PHUINT8, "16:16 huge pointer to 64 bit unsigned int"),
    (0x0378, T_PHINT16, "16:16 huge pointer to 128 bit signed int"),
    (0x0379, T_PHUINT16, "16:16 huge pointer to 128 bit unsigned int"),
    (0x0403, T_32PVOID, "void *"),
    (0x0408, T_32PHRESULT, "HRESULT *"),
    (0x0410, T_32PCHAR, "char *"),
    (0x0411, T_32PSHORT, "short *"),
    (0x0412, T_32PLONG, "long *"),
    (0x0413, T_32PQUAD, "long long *"),
    (0x0414, T_32POCT, "__int128 *"),
    (0x0420, T_32PUCHAR, "unsigned char *"),
    (0x0421, T_32PUSHORT, "unsigned short *"),
    (0x0422, T_32PULONG, "unsigned __int32 *"),
    (0x0423, T_32PUQUAD, "long long *"),
    (0x0424, T_32PUOCT, "unsigned __int128 *"),
    (0x0430, T_32PBOOL08, "bool *"),
    (0x0431, T_32PBOOL16, "bool16 *"),
    (0x0432, T_32PBOOL32, "bool32 *"),
    (0x0433, T_32PBOOL64, "bool64 *"),
    (0x0440, T_32PREAL32, "float *"),
    (0x0441, T_32PREAL64, "double *"),
    (0x0442, T_32PREAL80, "32 bit pointer to 80 bit real"),
    (0x0443, T_32PREAL128, "32 bit pointer to 128 bit real"),
    (0x0444, T_32PREAL48, "32 bit pointer to 48 bit real"),
    (0x0445, T_32PREAL32PP, "32 bit pointer to 32 bit PP real"),
    (0x0446, T_32PREAL16, "32 bit pointer to 16 bit real"),
    (0x0450, T_32PCPLX32, "32 bit pointer to 32 bit complex"),
    (0x0451, T_32PCPLX64, "32 bit pointer to 64 bit complex"),
    (0x0452, T_32PCPLX80, "32 bit pointer to 80 bit complex"),
    (0x0453, T_32PCPLX128, "32 bit pointer to 128 bit complex"),
    (0x0468, T_32PINT1, "__int8 *"),
    (0x0469, T_32PUINT1, "unsigned __int8 *"),
    (0x0470, T_32PRCHAR, "char *"), // really a character
    (0x0471, T_32PWCHAR, "wchar_t *"),
    (0x0472, T_32PINT2, "__int16 *"),
    (0x0473, T_32PUINT2, "unsigned __int16 *"),
    (0x0474, T_32PINT4, "__int32 *"),
    (0x0475, T_32PUINT4, "unsigned __int32 *"),
    (0x0476, T_32PINT8, "__int64 *"),
    (0x0477, T_32PUINT8, "unsigned __int64 *"),
    (0x0478, T_32PINT16, "32 bit pointer to 128 bit signed int"),
    (0x0479, T_32PUINT16, "32 bit pointer to 128 bit unsigned int"),
    (0x047a, T_32PCHAR16, "char16 *"),
    (0x047b, T_32PCHAR32, "char32 *"),
    (0x0503, T_32PFVOID, "16:32 pointer to void"),
    (0x0510, T_32PFCHAR, "16:32 pointer to 8 bit signed"),
    (0x0511, T_32PFSHORT, "16:32 pointer to 16 bit signed"),
    (0x0512, T_32PFLONG, "16:32 pointer to 32 bit signed"),
    (0x0513, T_32PFQUAD, "16:32 pointer to 64 bit signed"),
    (0x0514, T_32PFOCT, "16:32 pointer to 128 bit signed"),
    (0x0520, T_32PFUCHAR, "16:32 pointer to 8 bit unsigned"),
    (0x0521, T_32PFUSHORT, "16:32 pointer to 16 bit unsigned"),
    (0x0522, T_32PFULONG, "16:32 pointer to 32 bit unsigned"),
    (0x0523, T_32PFUQUAD, "16:32 pointer to 64 bit unsigned"),
    (0x0524, T_32PFUOCT, "16:32 pointer to 128 bit unsigned"),
    (0x0530, T_32PFBOOL08, "16:32 pointer to 8 bit boolean"),
    (0x0531, T_32PFBOOL16, "16:32 pointer to 16 bit boolean"),
    (0x0532, T_32PFBOOL32, "16:32 pointer to 32 bit boolean"),
    (0x0533, T_32PFBOOL64, "16:32 pointer to 64 bit boolean"),
    (0x0540, T_32PFREAL32, "16:32 pointer to 32 bit real"),
    (0x0541, T_32PFREAL64, "16:32 pointer to 64 bit real"),
    (0x0542, T_32PFREAL80, "16:32 pointer to 80 bit real"),
    (0x0543, T_32PFREAL128, "16:32 pointer to 128 bit real"),
    (0x0544, T_32PFREAL48, "16:32 pointer to 48 bit real"),
    (0x0545, T_32PFREAL32PP, "16:32 pointer to 32 bit PP real"),
    (0x0546, T_32PFREAL16, "16:32 pointer to 16 bit real"),
    (0x0550, T_32PFCPLX32, "16:32 pointer to 32 bit complex"),
    (0x0551, T_32PFCPLX64, "16:32 pointer to 64 bit complex"),
    (0x0552, T_32PFCPLX80, "16:32 pointer to 80 bit complex"),
    (0x0553, T_32PFCPLX128, "16:32 pointer to 128 bit complex"),
    (0x0568, T_32PFINT1, "16:32 pointer to 8 bit signed int"),
    (0x0569, T_32PFUINT1, "16:32 pointer to 8 bit unsigned int"),
    (0x0570, T_32PFRCHAR, "16:32 pointer to a real char"),
    (0x0571, T_32PFWCHAR, "16:32 pointer to a wide char"),
    (0x0572, T_32PFINT2, "16:32 pointer to 16 bit signed int"),
    (0x0573, T_32PFUINT2, "16:32 pointer to 16 bit unsigned int"),
    (0x0574, T_32PFINT4, "16:32 pointer to 32 bit signed int"),
    (0x0575, T_32PFUINT4, "16:32 pointer to 32 bit unsigned int"),
    (0x0576, T_32PFINT8, "16:32 pointer to 64 bit signed int"),
    (0x0577, T_32PFUINT8, "16:32 pointer to 64 bit unsigned int"),
    (0x0578, T_32PFINT16, "16:32 pointer to 128 bit signed int"),
    (0x0579, T_32PFUINT16, "16:32 pointer to 128 bit unsigned int"),
    // 64-bit pointer types
    (0x0603, T_64PVOID, "void *"),
    (0x0608, T_64PHRESULT, "HRESULT *"),
    (0x0610, T_64PCHAR, "char *"),
    (0x0611, T_64PSHORT, "short *"),
    (0x0612, T_64PLONG, "long *"),
    (0x0613, T_64PQUAD, "long long *"),
    (0x0614, T_64POCT, "__int128 *"),
    (0x0620, T_64PPUCHAR, "unsigned char *"),
    (0x0621, T_64PUSHORT, "unsigned short *"),
    (0x0622, T_64PULONG, "unsigned __int32 *"),
    (0x0623, T_64PUQUAD, "long long *"),
    (0x0624, T_64PUOCT, "unsigned __int128 *"),
    (0x0630, T_64PBOOL08, "bool *"),
    (0x0631, T_64PBOOL16, "bool16 *"),
    (0x0632, T_64PBOOL32, "bool32 *"),
    (0x0633, T_64PBOOL64, "bool64 *"),
    (0x0640, T_64PREAL32, "float *"),
    (0x0641, T_64PREAL64, "double *"),
    (0x0642, T_64PREAL80, "64 bit pointer to 80 bit real"),
    (0x0643, T_64PREAL128, "64 bit pointer to 128 bit real"),
    (0x0644, T_64PREAL48, "64 bit pointer to 48 bit real"),
    (0x0645, T_64PREAL32PP, "64 bit pointer to 32 bit PP real"),
    (0x0646, T_64PREAL16, "64 bit pointer to 16 bit real"),
    (0x0650, T_64PCPLX32, "64 bit pointer to 32 bit complex"),
    (0x0651, T_64PCPLX64, "64 bit pointer to 64 bit complex"),
    (0x0652, T_64PCPLX80, "64 bit pointer to 80 bit complex"),
    (0x0653, T_64PCPLX128, "64 bit pointer to 128 bit complex"),
    (0x0668, T_64PINT1, "__int8 *"),
    (0x0669, T_64PUINT1, "unsigned __int8 *"),
    (0x0670, T_64PRCHAR, "char *"), // really a character
    (0x0671, T_64PWCHAR, "wchar_t *"),
    (0x0672, T_64PINT2, "__int16 *"),
    (0x0673, T_64PUINT2, "unsigned __int16 *"),
    (0x0674, T_64PINT4, "__int32 *"),
    (0x0675, T_64PUINT4, "unsigned __int32 *"),
    (0x0676, T_64PINT8, "__int64 *"),
    (0x0677, T_64PUINT8, "unsigned __int64 *"),
    (0x0678, T_64PINT16, "64 bit pointer to 128 bit signed int"),
    (0x0679, T_64PUINT16, "64 bit pointer to 128 bit unsigned int"),
    (0x067a, T_64PCHAR16, "char16 *"),
    (0x067b, T_64PCHAR32, "char32 *"),
    (0x067c, T_64PCHAR8, "char8 *"),
}

/// Dumps a `TypeIndex`. For use only with primitive types.
pub fn dump_primitive_type_index(
    out: &mut dyn std::fmt::Write,
    type_index: TypeIndex,
) -> std::fmt::Result {
    let mode = (type_index.0 >> 8) & 7;
    let prim_ty = (type_index.0 >> 4) & 0xf;
    let size = type_index.0 & 7;

    if let Ok(i) = PRIMITIVES.binary_search_by_key(&type_index.0, |entry| entry.0) {
        let s = PRIMITIVES[i].1;
        write!(out, "{s}")?;
    } else {
        write!(out, "??PRIM(0x{:04x}) {{ ty: ", type_index.0)?;

        'a: {
            let ty_str = match prim_ty {
                PRIMITIVE_TYPE_SPECIAL => "special",
                PRIMITIVE_TYPE_SIGNED_INT => "signed_integer",
                PRIMITIVE_TYPE_UNSIGNED_INT => "unsigned_integer",
                PRIMITIVE_TYPE_BOOL => "bool",
                PRIMITIVE_TYPE_REAL => "real",
                PRIMITIVE_TYPE_COMPLEX => "complex",
                PRIMITIVE_TYPE_SPECIAL2 => "special2",
                PRIMITIVE_TYPE_REALLY_INT => "really_integer",
                _ => {
                    write!(out, "??{prim_ty}")?;
                    break 'a;
                }
            };
            write!(out, "{ty_str}")?;
        }

        write!(out, ", mode: {mode}, size: {size} }}")?;
    }

    Ok(())
}

#[test]
fn test_dump() {
    let mut s = String::new();
    dump_primitive_type_index(&mut s, TypeIndex::T_REAL32).unwrap();
    assert_eq!(s, "T_REAL32");

    s.clear();
    dump_primitive_type_index(&mut s, TypeIndex(0x067c)).unwrap();
}

```

`codeview/src/types/records.rs`:

```rs
#![allow(missing_docs)]

use super::*;
use bstr::BStr;
use zerocopy::U64;

bitfield::bitfield! {
    /// Bit field structure describing class/struct/union/enum properties
    ///
    /// See `CV_prop_t` in `cvinfo.h`.
    pub struct UdtProperties(u16);
    impl Debug;

    pub packed,        set_packed:        0;      // true if structure is packed
    pub ctor,          set_ctor:          1;      // true if constructors or destructors present
    pub ovlops,        set_ovlops:        2;      // true if overloaded operators present
    pub isnested,      set_isnested:      3;      // true if this is a nested class
    pub cnested,       set_cnested:       4;      // true if this class contains nested types
    pub opassign,      set_opassign:      5;      // true if overloaded assignment (=)
    pub opcast,        set_opcast:        6;      // true if casting methods
    pub fwdref,        set_fwdref:        7;      // true if forward reference (incomplete defn)
    pub scoped,        set_scoped:        8;      // scoped definition
    pub hasuniquename, set_hasuniquename: 9;      // true if there is a decorated name following the regular name
    pub sealed,        set_sealed:        10;     // true if class cannot be used as a base class
    pub hfa,           set_hfa:           12, 11; // CV_HFA
    pub intrinsic,     set_intrinsic:     13;     // true if class is an intrinsic type (e.g. __m128d)
    pub mocom,         set_mocom:         15, 14; // CV_MOCOM_UDT
}

#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned)]
#[repr(transparent)]
pub struct UdtPropertiesLe(pub U16<LE>);

impl Debug for UdtPropertiesLe {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        Debug::fmt(&self.get(), fmt)
    }
}

impl UdtPropertiesLe {
    #[inline(always)]
    pub fn get(&self) -> UdtProperties {
        UdtProperties(self.0.get())
    }
}

#[derive(Clone, Debug)]
pub struct Enum<'a> {
    pub fixed: &'a EnumFixed,
    pub name: &'a BStr,
    pub unique_name: Option<&'a BStr>,
}

#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[repr(C)]
pub struct EnumFixed {
    pub count: U16<LE>,
    pub property: UdtPropertiesLe,
    pub underlying_type: TypeIndexLe,
    pub fields: TypeIndexLe,
}

impl<'a> Parse<'a> for Enum<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let fixed: &EnumFixed = p.get()?;
        let name = p.strz()?;
        let property = fixed.property.get();
        let unique_name = if property.hasuniquename() {
            Some(p.strz()?)
        } else {
            None
        };
        Ok(Self {
            fixed,
            name,
            unique_name,
        })
    }
}

/// For `LF_ARRAY`
#[derive(Clone, Debug)]
pub struct Array<'a> {
    pub fixed: &'a ArrayFixed,
    pub len: Number<'a>,
    pub name: &'a BStr,
}

#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[repr(C)]
pub struct ArrayFixed {
    pub element_type: TypeIndexLe,
    pub index_type: TypeIndexLe,
}

impl<'a> Parse<'a> for Array<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Array {
            fixed: p.get()?,
            len: p.number()?,
            name: p.strz()?,
        })
    }
}

/// For `LF_CLASS`, `LF_STRUCTURE`, and `LF_INTERFACE`.
#[derive(Clone, Debug)]
pub struct Struct<'a> {
    pub fixed: &'a StructFixed,
    pub length: Number<'a>,
    pub name: &'a BStr,
    pub unique_name: Option<&'a BStr>,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct StructFixed {
    /// Number of elements in the class or structure. This count includes direct, virtual, and
    /// indirect virtual bases, and methods including overloads, data members, static data members,
    /// friends, etc.
    pub num_elements: U16<LE>,

    /// Bit flags
    pub property: UdtPropertiesLe,

    pub field_list: TypeIndexLe,

    // Docs say this should always be zero.
    pub derivation_list: TypeIndexLe,

    pub vtable_shape: TypeIndexLe,
    // numeric leaf
}

impl<'a> Parse<'a> for Struct<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let fixed: &StructFixed = p.get()?;
        let length = p.number()?;
        let name = p.strz()?;
        let unique_name = if fixed.property.get().hasuniquename() {
            Some(p.strz()?)
        } else {
            None
        };
        Ok(Struct {
            fixed,
            length,
            name,
            unique_name,
        })
    }
}

#[derive(Clone, Debug)]
pub struct Union<'a> {
    pub fixed: &'a UnionFixed,
    pub length: Number<'a>,
    pub name: &'a BStr,
    pub unique_name: Option<&'a BStr>,
}

#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[repr(C)]
pub struct UnionFixed {
    pub count: U16<LE>,
    pub property: UdtPropertiesLe,
    pub fields: TypeIndexLe,
}

impl<'a> Parse<'a> for Union<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let fixed: &UnionFixed = p.get()?;
        let length = p.number()?;
        let name = p.strz()?;
        let unique_name = if fixed.property.get().hasuniquename() {
            Some(p.strz()?)
        } else {
            None
        };
        Ok(Union {
            fixed,
            length,
            name,
            unique_name,
        })
    }
}

/// Type modifier record (`LF_MODIFIER`)
///
/// This record defines a qualified variation of another type. Bits indicate whether the qualifier
/// uses `const`, `volatile`, `unaligned`, or a combination of these flags.
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned, Clone, Debug)]
#[repr(C)]
pub struct TypeModifier {
    pub underlying_type: TypeIndexLe,
    pub attributes: U16<LE>,
}

impl<'a> Parse<'a> for TypeModifier {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        p.copy()
    }
}

impl TypeModifier {
    pub fn attributes(&self) -> TypeModifierBits {
        TypeModifierBits(self.attributes.get())
    }

    pub fn is_const(&self) -> bool {
        self.attributes().is_const()
    }

    pub fn is_volatile(&self) -> bool {
        self.attributes().is_volatile()
    }

    pub fn is_unaligned(&self) -> bool {
        self.attributes().is_unaligned()
    }
}

bitfield! {
    #[repr(transparent)]
    #[derive(Clone)]
    pub struct TypeModifierBits(u16);
    impl Debug;

    pub is_const, set_is_const: 0;
    pub is_volatile, set_is_volatile: 1;
    pub is_unaligned, set_is_unaligned: 2;
    pub reserved, set_reserved: 15, 3;
}

/// `LF_BITFIELD`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct Bitfield {
    pub underlying_type: TypeIndexLe,
    pub length: u8,
    pub position: u8,
}

/// `LF_PROCEDURE`
#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct Proc {
    pub return_value: TypeIndexLe,
    pub call: u8,
    pub reserved: u8,
    pub num_params: U16<LE>,
    pub arg_list: TypeIndexLe,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct MemberFunc {
    pub return_value: TypeIndexLe,
    pub class: TypeIndexLe,
    pub this: TypeIndexLe,
    pub call: u8,
    pub reserved: u8,
    pub num_params: U16<LE>,
    pub arg_list: TypeIndexLe,
    pub this_adjust: U32<LE>,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned)]
pub struct VTableShapeFixed {
    pub count: U16<LE>,
}

#[derive(Clone, Debug)]
pub struct VTableShapeData<'a> {
    pub count: u16,
    pub descriptors: &'a [u8],
}

pub struct MethodList<'a> {
    pub rest: &'a [u8],
}

impl<'a> MethodList<'a> {
    pub fn parse(record_data: &'a [u8]) -> Result<Self, ParserError> {
        Ok(Self { rest: record_data })
    }

    #[allow(clippy::should_implement_trait)]
    pub fn next(&mut self) -> Result<Option<MethodListItem>, ParserError> {
        if self.rest.is_empty() {
            return Ok(None);
        }

        let mut p = Parser::new(self.rest);
        let attr = p.u16()?;
        p.u16()?; // discard padding
        let ty = p.type_index()?;
        let vtab_offset = if introduces_virtual(attr) {
            Some(p.u32()?)
        } else {
            None
        };

        self.rest = p.into_rest();

        Ok(Some(MethodListItem {
            attr,
            ty,
            vtab_offset,
        }))
    }
}

/// Indicates whether a method type introduces a new virtual function slot.
///
/// `attr` is the `attr` field of a `LF_ONEMETHOD`, etc. record.
pub fn introduces_virtual(attr: u16) -> bool {
    // This field is only present if this method introduces a new vtable slot.
    matches!((attr >> 2) & 0xf, 4 | 6)
}

pub struct MethodListItem {
    pub attr: u16,
    pub ty: TypeIndex,
    pub vtab_offset: Option<u32>,
}

#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
#[repr(C)]
pub struct PointerFixed {
    pub ty: TypeIndexLe,
    pub attr: U32<LE>,
}

impl PointerFixed {
    pub fn attr(&self) -> PointerFlags {
        PointerFlags::from_bits(self.attr.get())
    }
}

#[derive(Clone)]
pub struct Pointer<'a> {
    pub fixed: &'a PointerFixed,
    pub variant: &'a [u8],
}

impl<'a> Parse<'a> for Pointer<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let fixed = p.get()?;
        let variant = p.take_rest();
        Ok(Self { fixed, variant })
    }
}

impl<'a> Debug for Pointer<'a> {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        let attr = self.fixed.attr();
        write!(fmt, "ty: {:?}", self.fixed.ty.get())?;
        write!(fmt, " attr: 0x{:08x} {:?}", attr.0, attr)?;
        write!(fmt, " mode: {}", attr.mode())?;
        Ok(())
    }
}

bitfield::bitfield! {
    pub struct PointerFlags(u32);
    impl Debug;
    pub pointer_kind, set_pointer_kind: 4, 0;
    pub mode, set_mode: 7, 5;
    pub flat32, set_flat32: 8;
    pub volatile, set_volatile: 9;
    pub r#const, set_const: 10;
    pub unaligned, set_unaligned: 11;
    pub restrict, set_restrict: 12;
    pub size, set_size: 18, 13;
    pub ismocom, set_ismocom: 19;
    pub islref, set_islref: 20;
    pub isrref, set_isrref: 21;
    pub unused, set_unused: 31, 22;
}

impl PointerFlags {
    #[allow(missing_docs)]
    pub fn from_bits(bits: u32) -> Self {
        Self(bits)
    }
}

/// Payload for `LF_METHODLIST`
#[derive(Clone, Debug)]
pub struct MethodListData<'a> {
    /// Contains a repeated sequence of:
    ///
    /// ```text
    /// struct {
    ///   attr: u16,
    ///   pad0: u16,
    ///   ty: TypeIndex,
    ///   vtab_offset: u32,         // optional, present only if attr indicates it starts a vtable slot
    /// }
    /// ```
    pub bytes: &'a [u8],
}

/// `LF_ARGLIST`
#[derive(Clone, Debug)]
pub struct ArgList<'a> {
    /// Arguments of the function signature
    pub args: &'a [TypeIndexLe],
}

impl<'a> Parse<'a> for ArgList<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let arg_count = p.u32()?;
        let args = p.slice(arg_count as usize)?;
        Ok(Self { args })
    }
}

/// `LF_ALIAS` record
#[allow(missing_docs)]
#[derive(Clone, Debug)]
pub struct Alias<'a> {
    pub utype: TypeIndex,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for Alias<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            utype: p.type_index()?,
            name: p.strz()?,
        })
    }
}

/// `LF_UDT_SRC_LINE`
///
/// See `lfUdtSrcLine` in `cvinfo.h`
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Clone, Debug)]
#[repr(C)]
pub struct UdtSrcLine {
    /// UDT's type index
    pub ty: TypeIndexLe,

    /// The source file which contains this UDT definition.
    /// This is a `NameIndex` value in the `/names` stream.
    pub src: U32<LE>,

    /// Line number
    pub line: U32<LE>,
}

/// `LF_UDT_MOD_SRC_LINE`
///
/// See `lfUdtModSrcLine` in `cvinfo.h`
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Clone, Debug)]
#[repr(C)]
pub struct UdtModSrcLine {
    /// UDT's type index
    pub ty: TypeIndexLe,

    /// The source file which contains this UDT definition.
    /// This is a `NameIndex` value in the `/names` stream.
    pub src: U32<LE>,

    /// Line number, 1-based
    pub line: U32<LE>,

    /// Module that contributes this UDT definition
    pub imod: U16<LE>,
}

/// CV_ItemId
pub type ItemIdLe = U32<LE>;

/// Identifies a record within the IPI Stream.
pub type ItemId = u32;

/// `LF_FUNC_ID`
#[derive(Clone, Debug)]
pub struct FuncId<'a> {
    pub fixed: &'a FuncIdFixed,
    pub name: &'a BStr,
    pub decorated_name_hash: Option<&'a U64<LE>>,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct FuncIdFixed {
    /// Parent scope of the ID, 0 if global. This is used for the namespace that contains a symbol.
    /// The value points into the IPI.
    pub scope: ItemIdLe,

    /// The type of the function.
    pub func_type: TypeIndexLe,
}

impl<'a> Parse<'a> for FuncId<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
            decorated_name_hash: if p.len() >= 8 { Some(p.get()?) } else { None },
        })
    }
}

/// `LF_MFUNC_ID`
#[derive(Clone, Debug)]
pub struct MFuncId<'a> {
    pub fixed: &'a MFuncIdFixed,
    pub name: &'a BStr,
    pub decorated_name_hash: Option<&'a U64<LE>>,
}

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct MFuncIdFixed {
    /// type index of parent
    pub parent_type: TypeIndexLe,
    /// function type
    pub func_type: TypeIndexLe,
}

impl<'a> Parse<'a> for MFuncId<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        Ok(Self {
            fixed: p.get()?,
            name: p.strz()?,
            decorated_name_hash: if p.len() >= 8 { Some(p.get()?) } else { None },
        })
    }
}

/// `LF_STRING_ID`
#[derive(Clone, Debug)]
pub struct StringId<'a> {
    pub id: ItemId,
    pub name: &'a BStr,
}

impl<'a> Parse<'a> for StringId<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let id = p.u32()?;
        let name = p.strz()?;
        Ok(Self { id, name })
    }
}

/// `LF_SUBSTR_LIST` - A list of substrings
#[derive(Clone, Debug)]
pub struct SubStrList<'a> {
    pub ids: &'a [ItemIdLe],
}

impl<'a> Parse<'a> for SubStrList<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let n = p.u32()?;
        let ids = p.slice(n as usize)?;
        Ok(Self { ids })
    }
}

/// `LF_BUILDINFO`
#[derive(Clone, Debug)]
pub struct BuildInfo<'a> {
    pub args: &'a [ItemIdLe],
}

impl<'a> BuildInfo<'a> {
    pub fn arg(&self, index: BuildInfoIndex) -> Option<ItemId> {
        let a = self.args.get(index as usize)?;
        Some(a.get())
    }
}

impl<'a> Parse<'a> for BuildInfo<'a> {
    fn from_parser(p: &mut Parser<'a>) -> Result<Self, ParserError> {
        let n = p.u16()?;
        let args = p.slice(n as usize)?;
        Ok(Self { args })
    }
}

/// Identifies indexes into `BuildInfo::args`.
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash, Debug)]
#[repr(u32)]
pub enum BuildInfoIndex {
    CurrentDirectory = 0,
    BuildTool = 1,           // Cl.exe
    SourceFile = 2,          // foo.cpp
    ProgramDatabaseFile = 3, // foo.pdb
    CommandArguments = 4,    // -I etc
}

/// Short strings for `BuildInfoIndex`
pub const BUILD_INFO_ARG_NAMES: [&str; 5] = ["cwd", "tool", "source_file", "pdb", "args"];

#[repr(C)]
#[derive(IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned, Debug)]
pub struct VFTable {
    /// type index of the root of path
    pub root: TypeIndexLe,
    /// type index of the path record
    pub path: TypeIndexLe,
    /// offset of virtual function table
    pub off: U32<LE>,
    /// segment of virtual function table
    pub seg: U16<LE>,
}

```

`codeview/src/types/visitor.rs`:

```rs
//! Algorithm for traversing ("visiting") the dependency graph within a type stream or between a
//! symbol stream and a type stream.

use super::{ItemId, ItemIdLe};
use crate::parser::{Parser, ParserError, ParserMut};
use crate::types::{Leaf, TypeIndex, TypeIndexLe};
use crate::types::{PointerFlags, introduces_virtual};
use anyhow::Context;
use tracing::error;
use zerocopy::{LE, U32};

/// Defines the functions needed for generically visiting type indexes within a type record or a
/// symbol record.
///
/// This trait exists in order to allow a single generic function to handle visiting the `TypeIndex`
/// values within a buffer, generic over the mutability of the access.
pub trait RecordVisitor {
    /// True if the parser is empty
    fn is_empty(&self) -> bool;

    /// Provides access to the rest of the record
    fn peek_rest(&self) -> &[u8];

    /// Parses a `u16` value
    fn u16(&mut self) -> Result<u16, ParserError>;

    /// Parses a `u32` value
    fn u32(&mut self) -> Result<u32, ParserError>;

    /// Skips `n` bytes of input
    fn skip(&mut self, n: usize) -> Result<(), ParserError>;

    /// Parses the next `ItemId` value and visits the location or value.
    fn item(&mut self) -> Result<(), ParserError>;

    /// Parses the next `TypeIndex` value and visits the location or value.
    fn ty(&mut self) -> Result<(), ParserError>;

    /// Parses a `NameIndex` value and visits the location or value.
    fn name_index(&mut self) -> Result<(), ParserError>;

    /// Parses a `Number`.
    fn number(&mut self) -> Result<(), ParserError>;

    /// Parses a NUL-terminated string.
    fn strz(&mut self) -> Result<(), ParserError>;
}

/// Defines a visitor that visits every ItemId and TypeIndexLe in a record. Allows modification.
#[allow(missing_docs)]
pub trait IndexVisitorMut {
    #[allow(unused_variables)]
    fn type_index(&mut self, offset: usize, value: &mut TypeIndexLe) -> Result<(), ParserError> {
        Ok(())
    }

    #[allow(unused_variables)]
    fn item_id(&mut self, offset: usize, value: &mut ItemIdLe) -> Result<(), ParserError> {
        Ok(())
    }

    #[allow(unused_variables)]
    fn name_index(&mut self, offset: usize, value: &mut U32<LE>) -> Result<(), ParserError> {
        Ok(())
    }
}

/// Defines a visitor that visits every ItemId and TypeIndexLe in a record.
#[allow(missing_docs)]
pub trait IndexVisitor {
    #[allow(unused_variables)]
    fn type_index(&mut self, offset: usize, value: TypeIndex) -> Result<(), ParserError> {
        Ok(())
    }

    #[allow(unused_variables)]
    fn item_id(&mut self, offset: usize, value: ItemId) -> Result<(), ParserError> {
        Ok(())
    }

    #[allow(unused_variables)]
    fn name_index(&mut self, offset: usize, value: u32) -> Result<(), ParserError> {
        Ok(())
    }
}

struct RefVisitor<'a, IV: IndexVisitor> {
    parser: Parser<'a>,
    original_len: usize,
    index_visitor: IV,
}

impl<'a, IV: IndexVisitor> RecordVisitor for RefVisitor<'a, IV> {
    fn is_empty(&self) -> bool {
        self.parser.is_empty()
    }

    fn peek_rest(&self) -> &[u8] {
        self.parser.peek_rest()
    }

    fn u16(&mut self) -> Result<u16, ParserError> {
        self.parser.u16()
    }

    fn u32(&mut self) -> Result<u32, ParserError> {
        self.parser.u32()
    }

    fn skip(&mut self, n: usize) -> Result<(), ParserError> {
        self.parser.skip(n)
    }

    fn ty(&mut self) -> Result<(), ParserError> {
        let offset = self.original_len - self.parser.len();
        let ti = self.parser.type_index()?;
        self.index_visitor.type_index(offset, ti)?;
        Ok(())
    }

    fn item(&mut self) -> Result<(), ParserError> {
        let offset = self.original_len - self.parser.len();
        let ii = self.parser.u32()?;
        self.index_visitor.item_id(offset, ii)?;
        Ok(())
    }

    fn number(&mut self) -> Result<(), ParserError> {
        self.parser.number()?;
        Ok(())
    }

    fn strz(&mut self) -> Result<(), ParserError> {
        self.parser.skip_strz()
    }

    fn name_index(&mut self) -> Result<(), ParserError> {
        let offset = self.original_len - self.parser.len();
        let ni = self.parser.u32()?;
        self.index_visitor.name_index(offset, ni)?;
        Ok(())
    }
}

struct MutVisitor<'a, IV: IndexVisitorMut> {
    parser: ParserMut<'a>,
    original_len: usize,
    index_visitor: IV,
}

impl<'a, IV: IndexVisitorMut> RecordVisitor for MutVisitor<'a, IV> {
    fn is_empty(&self) -> bool {
        self.parser.is_empty()
    }

    fn peek_rest(&self) -> &[u8] {
        self.parser.peek_rest()
    }

    fn u16(&mut self) -> Result<u16, ParserError> {
        self.parser.u16()
    }

    fn u32(&mut self) -> Result<u32, ParserError> {
        self.parser.u32()
    }

    fn skip(&mut self, n: usize) -> Result<(), ParserError> {
        self.parser.skip(n)
    }

    fn ty(&mut self) -> Result<(), ParserError> {
        let offset = self.original_len - self.parser.len();
        let ti: &mut TypeIndexLe = self.parser.get_mut()?;
        self.index_visitor.type_index(offset, ti)?;
        Ok(())
    }

    fn item(&mut self) -> Result<(), ParserError> {
        let offset = self.original_len - self.parser.len();
        let ii: &mut ItemIdLe = self.parser.get_mut()?;
        self.index_visitor.item_id(offset, ii)?;
        Ok(())
    }

    fn number(&mut self) -> Result<(), ParserError> {
        self.parser.skip_number()
    }

    fn strz(&mut self) -> Result<(), ParserError> {
        self.parser.skip_strz()
    }

    fn name_index(&mut self) -> Result<(), ParserError> {
        let offset = self.original_len - self.parser.len();
        let ni: &mut U32<LE> = self.parser.get_mut()?;
        self.index_visitor.name_index(offset, ni)?;
        Ok(())
    }
}

/// Scans the type indexes within a type record and calls `f` for each type index. This function
/// can only read data.
#[inline(never)]
pub fn visit_type_indexes_in_record_slice<IV: IndexVisitor>(
    type_kind: Leaf,
    record_data: &[u8],
    index_visitor: IV,
) -> Result<(), anyhow::Error> {
    let record_data_len = record_data.len();

    let mut v = RefVisitor {
        original_len: record_data.len(),
        parser: Parser::new(record_data),
        index_visitor,
    };

    visit_type_indexes_in_record(type_kind, &mut v).with_context(|| {
        let offset = record_data_len - v.parser.len();
        format!("at byte offset 0x{offset:x} {offset} within type record")
    })
}

/// Scans the type indexes within a type record and calls `f` for each type index. This function
/// can modify the type indexes within the record.
#[inline(never)]
pub fn visit_type_indexes_in_record_slice_mut<IV>(
    type_kind: Leaf,
    record_data: &mut [u8],
    index_visitor: IV,
) -> Result<(), anyhow::Error>
where
    IV: IndexVisitorMut,
{
    let record_data_len = record_data.len();

    let mut v = MutVisitor {
        original_len: record_data.len(),
        parser: ParserMut::new(record_data),
        index_visitor,
    };

    visit_type_indexes_in_record(type_kind, &mut v).with_context(|| {
        let offset = record_data_len - v.parser.len();
        format!("at byte offset 0x{offset:x} {offset} within type record")
    })
}

/// This function examines a type record and traverses the type indexes within it.
///
/// The caller provides an implementation of the visitor trait.  The visitor trait provides the
/// ability to read fields from the type record, and receives notifications that the visitor is
/// positioned on a type index.
pub fn visit_type_indexes_in_record<V: RecordVisitor>(
    type_kind: Leaf,
    p: &mut V,
) -> Result<(), ParserError> {
    match type_kind {
        Leaf::LF_LABEL => {}

        Leaf::LF_MODIFIER => {
            p.ty()?;
        }

        Leaf::LF_POINTER => {
            p.ty()?; // underlying type
            let attr = PointerFlags(p.u32()?);
            match attr.mode() {
                2 => {
                    // 2 is pointer to data member
                    p.ty()?;
                }
                3 => {
                    // 3 is pointer to method
                    p.ty()?;
                }
                _ => {}
            }
        }

        Leaf::LF_ALIAS => {
            p.ty()?;
        }

        Leaf::LF_ARRAY => {
            p.ty()?; // element type
            p.ty()?; // index type
        }

        Leaf::LF_CLASS | Leaf::LF_STRUCTURE => {
            p.u16()?; // count
            p.skip(2)?; // property
            p.ty()?; // field list
            p.ty()?; // derivation list
            p.ty()?; // vtable shape
        }

        Leaf::LF_ENUM => {
            p.u16()?; // count
            p.skip(2)?; // property
            p.ty()?; // type
            p.ty()?; // field list
        }

        Leaf::LF_UNION => {
            p.u16()?; // count
            p.skip(2)?; // property
            p.ty()?; // field list
        }

        Leaf::LF_PROCEDURE => {
            p.ty()?; // return type
            p.skip(4)?; // calling convention, reserved, and num params
            p.ty()?; // arg list
        }

        Leaf::LF_MFUNCTION => {
            p.ty()?; // return type
            p.ty()?; // class definition
            p.ty()?; // this type
            p.skip(4)?; // calling convention, reserved, and num params
            p.ty()?; // arg list
        }

        Leaf::LF_ARGLIST => {
            let num_args = p.u32()?;
            for _ in 0..num_args {
                p.ty()?;
            }
        }

        Leaf::LF_FIELDLIST => {
            let mut prev_item_kind = None;
            loop {
                let rest = p.peek_rest();
                if rest.is_empty() {
                    break;
                }

                // Check for padding (alignment) bytes.
                let mut padding_len = 0;
                while padding_len < rest.len() && rest[padding_len] >= 0xf0 {
                    padding_len += 1;
                }
                if padding_len > 0 {
                    p.skip(padding_len)?;
                }

                if p.is_empty() {
                    break;
                }

                let item_kind = Leaf(p.u16()?);
                let after = prev_item_kind.replace(item_kind);

                match item_kind {
                    Leaf::LF_BCLASS => {
                        let _attr = p.u16()?;
                        p.ty()?; // class type
                        p.number()?; // offset
                    }

                    Leaf::LF_VBCLASS => {
                        let _attr = p.u16()?;
                        p.ty()?; // base class
                        p.ty()?; // vbtype
                        p.number()?; // vbpoff
                        p.number()?; // vbpff
                    }

                    Leaf::LF_IVBCLASS => {
                        let _attr = p.u16()?;
                        p.ty()?; // base class
                        p.ty()?; // virtual base type
                        p.number()?; // vbpoff
                        p.number()?; // vbpff
                    }

                    Leaf::LF_ENUMERATE => {
                        // nothing needed
                        let _attr = p.u16()?;
                        p.number()?; // value
                        p.strz()?; // name
                    }

                    Leaf::LF_FRIENDFCN => {
                        p.skip(2)?; // padding
                        p.ty()?; // type
                        p.strz()?; // name
                    }

                    Leaf::LF_INDEX => {
                        p.skip(2)?; // padding
                        p.ty()?; // index
                    }

                    Leaf::LF_MEMBER => {
                        let _attr = p.u16()?;
                        p.ty()?; // type
                        p.number()?; // offset
                        p.strz()?; // name
                    }

                    Leaf::LF_STMEMBER => {
                        let _attr = p.u16()?;
                        p.ty()?; // type
                        p.strz()?; // name
                    }

                    Leaf::LF_METHOD => {
                        let _count = p.u16()?;
                        p.ty()?; // method list
                        p.strz()?; // name
                    }

                    Leaf::LF_NESTEDTYPE => {
                        p.skip(2)?; // padding
                        p.ty()?; // index
                        p.strz()?; // name
                    }

                    Leaf::LF_VFUNCTAB => {
                        p.skip(2)?; // padding
                        p.ty()?; // vtable type
                    }

                    Leaf::LF_FRIENDCLS => {
                        p.skip(2)?; // padding
                        p.ty()?; // friend class type
                    }

                    Leaf::LF_ONEMETHOD => {
                        let attr = p.u16()?; // attribute
                        p.ty()?; // type of method
                        if introduces_virtual(attr) {
                            p.u32()?; // vbaseoff
                        }
                        p.strz()?; // name
                    }

                    Leaf::LF_VFUNCOFF => {
                        p.u16()?; // padding
                        p.ty()?; // vtable type
                        p.u32()?; // offset
                    }

                    Leaf::LF_NESTEDTYPEEX => {
                        p.u16()?; // attribute
                        p.ty()?; // nested type
                        p.strz()?; // name
                    }

                    unknown_item_kind => {
                        error!(
                            ?unknown_item_kind,
                            ?after,
                            "unrecognized item within LF_FIELDLIST"
                        );
                        break;
                    }
                }
            }
        }

        Leaf::LF_DERIVED => {
            let count = p.u32()?;
            for _ in 0..count {
                p.ty()?;
            }
        }

        Leaf::LF_BITFIELD => {
            p.ty()?;
        }

        Leaf::LF_METHODLIST => {
            while !p.is_empty() {
                let attr = p.u16()?;
                p.skip(2)?; // padding
                p.ty()?;
                if introduces_virtual(attr) {
                    p.skip(4)?; // vtable offset
                }
            }
        }

        Leaf::LF_DIMCONU => {
            p.ty()?; // index type
        }

        Leaf::LF_DIMCONLU => {
            p.ty()?; // index type
        }

        Leaf::LF_DIMVARU => {
            let rank = p.u32()?;
            p.ty()?; // index type
            for _ in 0..rank {
                p.ty()?; // upper bound for this dimension
            }
        }

        // These types do not contain any pointers to other types.
        Leaf::LF_VTSHAPE | Leaf::LF_PRECOMP | Leaf::LF_ENDPRECOMP | Leaf::LF_SKIP => {}

        Leaf::LF_VFTPATH => {
            let count = p.u32()?;
            for _ in 0..count {
                p.ty()?;
            }
        }

        Leaf::LF_VFTABLE => {
            p.ty()?; // type
            p.ty()?; // base_vftable
        }

        Leaf::LF_CLASS2 | Leaf::LF_STRUCTURE2 | Leaf::LF_UNION2 | Leaf::LF_INTERFACE2 => {
            p.skip(4)?; // property
            p.ty()?; // field
            p.ty()?; // derived
            p.ty()?; // vshape
        }

        Leaf::LF_FUNC_ID => {
            p.item()?; // parent scope of the ID, 0 if global
            p.ty()?; // function type
        }

        Leaf::LF_MFUNC_ID => {
            p.ty()?; // parent type
            p.ty()?; // function type
        }

        Leaf::LF_BUILDINFO => {
            let n = p.u16()?;
            for _ in 0..n {
                p.item()?;
            }
        }

        Leaf::LF_SUBSTR_LIST => {
            let count = p.u32()?;
            for _ in 0..count {
                p.item()?;
            }
        }

        Leaf::LF_STRING_ID => {
            p.item()?; // ID to list of sub string IDs
        }

        Leaf::LF_UDT_SRC_LINE => {
            p.ty()?;
            p.name_index()?; // NameIndex of source file name
        }

        Leaf::LF_UDT_MOD_SRC_LINE => {
            p.ty()?;
            p.name_index()?; // NameIndex of source file name
        }

        _ => {
            error!("unrecognized type kind: {:?}", type_kind);
            return Err(ParserError::new());
        }
    }

    Ok(())
}

```

`codeview/src/utils.rs`:

```rs
pub mod iter;

```

`codeview/src/utils/iter.rs`:

```rs
use std::ops::Range;

/// Allows iterators to report the remaining, unparsed bytes within an iterator.
///
/// This is for iterators that parse items from `&[u8]` buffers or similar.
pub trait HasRestLen {
    /// Returns the number of bytes (or elements, abstractly) that have not yet been parsed by this
    /// iterator.
    fn rest_len(&self) -> usize;
}

/// An iterator adapter which reports the byte ranges of the items that are iterated by the
/// underlying iterator. The underlying iterator must implement `HasRestLen`.
pub struct IterWithRange<I> {
    original_len: usize,
    inner: I,
}

impl<I> IterWithRange<I> {
    /// The number of items (usually bytes) that were present in the inner iterator when this
    /// `IterWithRange` was created. This value allows us to convert the "bytes remaining" value
    /// (`rest_len()`, which is what the inner iterator operates directly on) to an offset from the
    /// beginning of a buffer.
    pub fn original_len(&self) -> usize {
        self.original_len
    }

    /// Gets access to the inner iterator.
    pub fn inner(&self) -> &I {
        &self.inner
    }

    /// Gets mutable access to the inner iterator.
    ///
    /// Be warned!  If you modify this iterator, make sure you don't break its relationship with
    /// the `original_len` value.  Iterating items from it is fine, because that should never
    /// break the relationship with `original_len`.
    ///
    /// What would break it would be replacing the inner iterator with one that has a length that
    /// is greater than `original_len`.
    pub fn inner_mut(&mut self) -> &mut I {
        &mut self.inner
    }

    /// The current position in the iteration range.
    #[inline(always)]
    pub fn pos(&self) -> usize
    where
        I: HasRestLen,
    {
        self.original_len - self.inner.rest_len()
    }
}

impl<I: Iterator> Iterator for IterWithRange<I>
where
    I: HasRestLen,
{
    type Item = (Range<usize>, I::Item);

    fn next(&mut self) -> Option<Self::Item> {
        let pos_before = self.pos();
        let item = self.inner.next()?;
        let pos_after = self.pos();
        Some((pos_before..pos_after, item))
    }
}

/// An extension trait for iterators that converts an `Iterator` into an `IterWithRange`.
/// Use `foo.with_ranges()` to convert (augment) the iterator.
pub trait IteratorWithRangesExt: Sized {
    /// Augments this iterator with information about the byte range of each underlying item.
    fn with_ranges(self) -> IterWithRange<Self>;
}

impl<I> IteratorWithRangesExt for I
where
    I: Iterator + HasRestLen,
{
    fn with_ranges(self) -> IterWithRange<Self> {
        IterWithRange {
            original_len: self.rest_len(),
            inner: self,
        }
    }
}

```

`coff/Cargo.toml`:

```toml
[package]
name = "ms-coff"
version = "0.1.0"
edition = "2024"
description = "Definitions for Windows COFF binaries"
authors = ["Arlie Davis <ardavis@microsoft.com>"]
license = "MIT OR Apache-2.0"
repository = "https://github.com/microsoft/pdb-rs/"
categories = ["parsing", "development-tools", "development-tools::debugging"]

[lib]
doctest = false

[dependencies]
bitflags.workspace = true
bstr.workspace = true
static_assertions.workspace = true
zerocopy.workspace = true
zerocopy-derive.workspace = true

```

`coff/src/dll_characteristics.rs`:

```rs
use zerocopy_derive::*;

#[repr(transparent)]
#[derive(
    Clone,
    Default,
    Eq,
    PartialEq,
    Ord,
    PartialOrd,
    Hash,
    IntoBytes,
    FromBytes,
    Immutable,
    KnownLayout,
)]
pub struct IMAGE_DLLCHARACTERISTICS(pub u16);

//      IMAGE_LIBRARY_PROCESS_INIT            0x0001     // Reserved.
//      IMAGE_LIBRARY_PROCESS_TERM            0x0002     // Reserved.
//      IMAGE_LIBRARY_THREAD_INIT             0x0004     // Reserved.
//      IMAGE_LIBRARY_THREAD_TERM             0x0008     // Reserved.

/// Image can handle a high entropy 64-bit virtual address space.
pub const IMAGE_DLLCHARACTERISTICS_HIGH_ENTROPY_VA: u16 = 0x0020;
/// DLL can move.
pub const IMAGE_DLLCHARACTERISTICS_DYNAMIC_BASE: u16 = 0x0040;
/// Code Integrity Image
pub const IMAGE_DLLCHARACTERISTICS_FORCE_INTEGRITY: u16 = 0x0080;
/// Image is NX compatible
pub const IMAGE_DLLCHARACTERISTICS_NX_COMPAT: u16 = 0x0100;
/// Image understands isolation and doesn't want it
pub const IMAGE_DLLCHARACTERISTICS_NO_ISOLATION: u16 = 0x0200;
/// Image does not use SEH.  No SE handler may reside in this image
pub const IMAGE_DLLCHARACTERISTICS_NO_SEH: u16 = 0x0400;
/// Do not bind this image.
pub const IMAGE_DLLCHARACTERISTICS_NO_BIND: u16 = 0x0800;
/// Image should execute in an AppContainer
pub const IMAGE_DLLCHARACTERISTICS_APPCONTAINER: u16 = 0x1000;
/// Driver uses WDM model
pub const IMAGE_DLLCHARACTERISTICS_WDM_DRIVER: u16 = 0x2000;
/// Image supports Control Flow Guard.
pub const IMAGE_DLLCHARACTERISTICS_GUARD_CF: u16 = 0x4000;
pub const IMAGE_DLLCHARACTERISTICS_TERMINAL_SERVER_AWARE: u16 = 0x8000;

```

`coff/src/image.rs`:

```rs
use zerocopy_derive::*;

use crate::IMAGE_DLLCHARACTERISTICS;

#[repr(C)]
#[derive(
    Clone,
    Default,
    Eq,
    PartialEq,
    Ord,
    PartialOrd,
    Hash,
    IntoBytes,
    FromBytes,
    Immutable,
    KnownLayout,
)]
pub struct IMAGE_FILE_HEADER {
    pub machine: u16,
    pub number_of_sections: u16,
    pub time_date_stamp: u32,
    pub pointer_to_symbol_table: u32,
    pub number_of_symbols: u32,
    pub size_of_optional_header: u16,
    pub characteristics: u16,
}

#[repr(C)]
#[derive(
    Clone,
    Default,
    Eq,
    PartialEq,
    Ord,
    PartialOrd,
    Hash,
    IntoBytes,
    FromBytes,
    Immutable,
    KnownLayout,
)]
pub struct IMAGE_DATA_DIRECTORY {
    pub virtual_address: u32,
    pub size: u32,
}

pub const IMAGE_NUMBEROF_DIRECTORY_ENTRIES: usize = 16;

#[repr(C)]
#[derive(
    Clone,
    Default,
    Eq,
    PartialEq,
    Ord,
    PartialOrd,
    Hash,
    IntoBytes,
    FromBytes,
    Immutable,
    KnownLayout,
)]
pub struct IMAGE_OPTIONAL_HEADER32 {
    pub magic: u16,
    pub major_linker_version: u8,
    pub minor_linker_version: u8,
    pub size_of_code: u32,
    pub size_of_initialized_data: u32,
    pub size_of_uninitialized_data: u32,
    pub address_of_entry_point: u32,
    pub base_of_code: u32,
    pub base_of_data: u32,
    pub image_base: u32,
    pub section_alignment: u32,
    pub file_alignment: u32,
    pub major_operating_system_version: u16,
    pub minor_operating_system_version: u16,
    pub major_image_version: u16,
    pub minor_image_version: u16,
    pub major_subsystem_version: u16,
    pub minor_subsystem_version: u16,
    pub win32_version_value: u32,
    pub size_of_image: u32,
    pub size_of_headers: u32,
    pub check_sum: u32,
    pub subsystem: u16,
    pub dll_characteristics: IMAGE_DLLCHARACTERISTICS,
    pub size_of_stack_reserve: u32,
    pub size_of_stack_commit: u32,
    pub size_of_heap_reserve: u32,
    pub size_of_heap_commit: u32,
    pub loader_flags: u32,
    pub number_of_rva_and_sizes: u32,
    pub data_directory: [IMAGE_DATA_DIRECTORY; IMAGE_NUMBEROF_DIRECTORY_ENTRIES],
}

#[repr(C)]
#[derive(
    Clone,
    Default,
    Eq,
    PartialEq,
    Ord,
    PartialOrd,
    Hash,
    IntoBytes,
    FromBytes,
    Immutable,
    KnownLayout,
)]
pub struct IMAGE_OPTIONAL_HEADER64 {
    pub magic: u16,
    pub major_linker_version: u8,
    pub minor_linker_version: u8,
    pub size_of_code: u32,
    pub size_of_initialized_data: u32,
    pub size_of_uninitialized_data: u32,
    pub address_of_entry_point: u32,
    pub base_of_code: u32,
    pub image_base: u64,
    pub section_alignment: u32,
    pub file_alignment: u32,
    pub major_operating_system_version: u16,
    pub minor_operating_system_version: u16,
    pub major_image_version: u16,
    pub minor_image_version: u16,
    pub major_subsystem_version: u16,
    pub minor_subsystem_version: u16,
    pub win32_version_value: u32,
    pub size_of_image: u32,
    pub size_of_headers: u32,
    pub check_sum: u32,
    pub subsystem: u16,
    pub dll_characteristics: IMAGE_DLLCHARACTERISTICS,
    pub size_of_stack_reserve: u64,
    pub size_of_stack_commit: u64,
    pub size_of_heap_reserve: u64,
    pub size_of_heap_commit: u64,
    pub loader_flags: u32,
    pub number_of_rva_and_sizes: u32,
    pub data_directory: [IMAGE_DATA_DIRECTORY; IMAGE_NUMBEROF_DIRECTORY_ENTRIES],
}

pub const IMAGE_NT_OPTIONAL_HDR32_MAGIC: u16 = 0x10b;
pub const IMAGE_NT_OPTIONAL_HDR64_MAGIC: u16 = 0x20b;
pub const IMAGE_ROM_OPTIONAL_HDR_MAGIC: u16 = 0x107;

pub struct IMAGE_NT_HEADERS64 {
    pub signature: u32,
    pub file_header: IMAGE_FILE_HEADER,
    pub optional_header: IMAGE_OPTIONAL_HEADER64,
}

pub struct IMAGE_NT_HEADERS32 {
    pub signature: u32,
    pub file_header: IMAGE_FILE_HEADER,
    pub optional_header: IMAGE_OPTIONAL_HEADER32,
}

pub struct IMAGE_ROM_OPTIONAL_HEADER {
    pub magic: u16,
    pub major_linker_version: u8,
    pub minor_linker_version: u8,
    pub size_of_code: u32,
    pub size_of_initialized_data: u32,
    pub size_of_uninitialized_data: u32,
    pub address_of_entry_point: u32,
    pub base_of_code: u32,
    pub base_of_data: u32,
    pub base_of_bss: u32,
    pub gpr_mask: u32,
    pub cpr_mask: [u32; 4],
    pub gp_value: u32,
}

pub struct IMAGE_ROM_HEADERS {
    pub file_header: IMAGE_FILE_HEADER,
    pub optional_header: IMAGE_ROM_OPTIONAL_HEADER,
}

```

`coff/src/lib.rs`:

```rs
//! Definitions for Portable Executable (PE) COFF binaries (Windows binaries)

#![allow(non_camel_case_types)]
#![forbid(unsafe_code)]

mod dll_characteristics;
mod image;
mod machine;
mod reloc;
mod section;

pub use dll_characteristics::*;
pub use image::*;
pub use machine::*;
pub use reloc::*;
pub use section::*;

```

`coff/src/machine.rs`:

```rs
#[derive(Copy, Clone, Eq, PartialEq, Hash, Ord, PartialOrd)]
#[allow(non_camel_case_types)]
pub struct IMAGE_FILE_MACHINE(pub u16);

impl IMAGE_FILE_MACHINE {
    pub const IMAGE_FILE_MACHINE_UNKNOWN: Self = Self(0);
    /// Useful for indicating we want to interact with the host and not a WoW guest.
    pub const IMAGE_FILE_MACHINE_TARGET_HOST: Self = Self(0x0001);
    /// Intel 386.
    pub const IMAGE_FILE_MACHINE_I386: Self = Self(0x014c);
    /// MIPS little-endian, 0x160 big-endian
    pub const IMAGE_FILE_MACHINE_R3000: Self = Self(0x0162);
    /// MIPS little-endian
    pub const IMAGE_FILE_MACHINE_R4000: Self = Self(0x0166);
    /// MIPS little-endian
    pub const IMAGE_FILE_MACHINE_R10000: Self = Self(0x0168);
    /// MIPS little-endian WCE v2
    pub const IMAGE_FILE_MACHINE_WCEMIPSV2: Self = Self(0x0169);
    /// Alpha_AXP
    pub const IMAGE_FILE_MACHINE_ALPHA: Self = Self(0x0184);
    /// SH3 little-endian
    pub const IMAGE_FILE_MACHINE_SH3: Self = Self(0x01a2);
    pub const IMAGE_FILE_MACHINE_SH3DSP: Self = Self(0x01a3);
    /// SH3E little-endian
    pub const IMAGE_FILE_MACHINE_SH3E: Self = Self(0x01a4);
    /// SH4 little-endian
    pub const IMAGE_FILE_MACHINE_SH4: Self = Self(0x01a6);
    /// SH5
    pub const IMAGE_FILE_MACHINE_SH5: Self = Self(0x01a8);
    /// ARM Little-Endian
    pub const IMAGE_FILE_MACHINE_ARM: Self = Self(0x01c0);
    /// ARM Thumb/Thumb-2 Little-Endian
    pub const IMAGE_FILE_MACHINE_THUMB: Self = Self(0x01c2);
    /// ARM Thumb-2 Little-Endian
    pub const IMAGE_FILE_MACHINE_ARMNT: Self = Self(0x01c4);
    pub const IMAGE_FILE_MACHINE_AM33: Self = Self(0x01d3);
    /// IBM PowerPC Little-Endian
    pub const IMAGE_FILE_MACHINE_POWERPC: Self = Self(0x01F0);
    pub const IMAGE_FILE_MACHINE_POWERPCFP: Self = Self(0x01f1);
    /// Intel 64
    pub const IMAGE_FILE_MACHINE_IA64: Self = Self(0x0200);
    /// MIPS
    pub const IMAGE_FILE_MACHINE_MIPS16: Self = Self(0x0266);
    /// ALPHA64
    pub const IMAGE_FILE_MACHINE_ALPHA64: Self = Self(0x0284);
    /// MIPS
    pub const IMAGE_FILE_MACHINE_MIPSFPU: Self = Self(0x0366);
    /// MIPS
    pub const IMAGE_FILE_MACHINE_MIPSFPU16: Self = Self(0x0466);
    pub const IMAGE_FILE_MACHINE_AXP64: Self = Self::IMAGE_FILE_MACHINE_ALPHA64;
    /// Infineon
    pub const IMAGE_FILE_MACHINE_TRICORE: Self = Self(0x0520);
    pub const IMAGE_FILE_MACHINE_CEF: Self = Self(0x0CEF);
    /// EFI Byte Code
    pub const IMAGE_FILE_MACHINE_EBC: Self = Self(0x0EBC);
    /// AMD64 (K8)
    pub const IMAGE_FILE_MACHINE_AMD64: Self = Self(0x8664);
    /// M32R little-endian
    pub const IMAGE_FILE_MACHINE_M32R: Self = Self(0x9041);
    /// ARM64 Little-Endian
    pub const IMAGE_FILE_MACHINE_ARM64: Self = Self(0xAA64);
    pub const IMAGE_FILE_MACHINE_CEE: Self = Self(0xC0EE);

    pub fn to_str_opt(self) -> Option<&'static str> {
        Some(match self.0 {
            0x0000 => "IMAGE_FILE_MACHINE_UNKNOWN",
            0x0001 => "IMAGE_FILE_MACHINE_TARGET_HOST",
            0x014c => "IMAGE_FILE_MACHINE_I386",
            0x0162 => "IMAGE_FILE_MACHINE_R3000",
            0x0166 => "IMAGE_FILE_MACHINE_R4000",
            0x0168 => "IMAGE_FILE_MACHINE_R10000",
            0x0169 => "IMAGE_FILE_MACHINE_WCEMIPSV2",
            0x0184 => "IMAGE_FILE_MACHINE_ALPHA",
            0x01a2 => "IMAGE_FILE_MACHINE_SH3",
            0x01a3 => "IMAGE_FILE_MACHINE_SH3DSP",
            0x01a4 => "IMAGE_FILE_MACHINE_SH3E",
            0x01a6 => "IMAGE_FILE_MACHINE_SH4",
            0x01a8 => "IMAGE_FILE_MACHINE_SH5",
            0x01c0 => "IMAGE_FILE_MACHINE_ARM",
            0x01c2 => "IMAGE_FILE_MACHINE_THUMB",
            0x01c4 => "IMAGE_FILE_MACHINE_ARMNT",
            0x01d3 => "IMAGE_FILE_MACHINE_AM33",
            0x01F0 => "IMAGE_FILE_MACHINE_POWERPC",
            0x01f1 => "IMAGE_FILE_MACHINE_POWERPCFP",
            0x0200 => "IMAGE_FILE_MACHINE_IA64",
            0x0266 => "IMAGE_FILE_MACHINE_MIPS16",
            0x0284 => "IMAGE_FILE_MACHINE_ALPHA64",
            0x0366 => "IMAGE_FILE_MACHINE_MIPSFPU",
            0x0466 => "IMAGE_FILE_MACHINE_MIPSFPU16",
            0x0520 => "IMAGE_FILE_MACHINE_TRICORE",
            0x0CEF => "IMAGE_FILE_MACHINE_CEF",
            0x0EBC => "IMAGE_FILE_MACHINE_EBC",
            0x8664 => "IMAGE_FILE_MACHINE_AMD64",
            0x9041 => "IMAGE_FILE_MACHINE_M32R",
            0xAA64 => "IMAGE_FILE_MACHINE_ARM64",
            0xC0EE => "IMAGE_FILE_MACHINE_CEE",
            _ => return None,
        })
    }

    pub fn to_str(self) -> &'static str {
        self.to_str_opt().unwrap_or("??")
    }
}

impl core::fmt::Debug for IMAGE_FILE_MACHINE {
    fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {
        if let Some(s) = self.to_str_opt() {
            f.write_str(s)
        } else {
            write!(f, "??0x{:x}", self.0)
        }
    }
}

```

`coff/src/reloc.rs`:

```rs
use crate::IMAGE_FILE_MACHINE;

pub const IMAGE_REL_I386_ABSOLUTE: u16 = 0x0000; // Reference is absolute, no relocation is necessary
pub const IMAGE_REL_I386_DIR16: u16 = 0x0001; // Direct 16-bit reference to the symbols virtual address
pub const IMAGE_REL_I386_REL16: u16 = 0x0002; // PC-relative 16-bit reference to the symbols virtual address
pub const IMAGE_REL_I386_DIR32: u16 = 0x0006; // Direct 32-bit reference to the symbols virtual address
pub const IMAGE_REL_I386_DIR32NB: u16 = 0x0007; // Direct 32-bit reference to the symbols virtual address, base not included
pub const IMAGE_REL_I386_SEG12: u16 = 0x0009; // Direct 16-bit reference to the segment-selector bits of a 32-bit virtual address
pub const IMAGE_REL_I386_SECTION: u16 = 0x000A;
pub const IMAGE_REL_I386_SECREL: u16 = 0x000B;
pub const IMAGE_REL_I386_TOKEN: u16 = 0x000C; // clr token
pub const IMAGE_REL_I386_SECREL7: u16 = 0x000D; // 7 bit offset from base of section containing target
pub const IMAGE_REL_I386_REL32: u16 = 0x0014; // PC-relative 32-bit reference to the symbols virtual address

pub fn reloc_type_str_i386(reloc: u16) -> Option<&'static str> {
    match reloc {
        IMAGE_REL_I386_ABSOLUTE => Some("IMAGE_REL_I386_ABSOLUTE"),
        IMAGE_REL_I386_DIR16 => Some("IMAGE_REL_I386_DIR16"),
        IMAGE_REL_I386_REL16 => Some("IMAGE_REL_I386_REL16"),
        IMAGE_REL_I386_DIR32 => Some("IMAGE_REL_I386_DIR32"),
        IMAGE_REL_I386_DIR32NB => Some("IMAGE_REL_I386_DIR32NB"),
        IMAGE_REL_I386_SEG12 => Some("IMAGE_REL_I386_SEG12"),
        IMAGE_REL_I386_SECTION => Some("IMAGE_REL_I386_SECTION"),
        IMAGE_REL_I386_SECREL => Some("IMAGE_REL_I386_SECREL"),
        IMAGE_REL_I386_TOKEN => Some("IMAGE_REL_I386_TOKEN"),
        IMAGE_REL_I386_SECREL7 => Some("IMAGE_REL_I386_SECREL7"),
        IMAGE_REL_I386_REL32 => Some("IMAGE_REL_I386_REL32"),
        _ => None,
    }
}

pub fn reloc_type_str_short_i386(reloc: u16) -> Option<&'static str> {
    match reloc {
        IMAGE_REL_I386_ABSOLUTE => Some("ABSOLUTE"),
        IMAGE_REL_I386_DIR16 => Some("DIR16"),
        IMAGE_REL_I386_REL16 => Some("REL16"),
        IMAGE_REL_I386_DIR32 => Some("DIR32"),
        IMAGE_REL_I386_DIR32NB => Some("DIR32NB"),
        IMAGE_REL_I386_SEG12 => Some("SEG12"),
        IMAGE_REL_I386_SECTION => Some("SECTION"),
        IMAGE_REL_I386_SECREL => Some("SECREL"),
        IMAGE_REL_I386_TOKEN => Some("TOKEN"),
        IMAGE_REL_I386_SECREL7 => Some("SECREL7"),
        IMAGE_REL_I386_REL32 => Some("REL32"),
        _ => None,
    }
}

//
// ARM64 relocations types.
//

pub const IMAGE_REL_ARM64_ABSOLUTE: u16 = 0x0000; // No relocation required
pub const IMAGE_REL_ARM64_ADDR32: u16 = 0x0001; // 32 bit address. Review! do we need it?
pub const IMAGE_REL_ARM64_ADDR32NB: u16 = 0x0002; // 32 bit address w/o image base (RVA: for Data/PData/XData)
pub const IMAGE_REL_ARM64_BRANCH26: u16 = 0x0003; // 26 bit offset << 2 & sign ext. for B & BL
pub const IMAGE_REL_ARM64_PAGEBASE_REL21: u16 = 0x0004; // ADRP
pub const IMAGE_REL_ARM64_REL21: u16 = 0x0005; // ADR
pub const IMAGE_REL_ARM64_PAGEOFFSET_12A: u16 = 0x0006; // ADD/ADDS (immediate) with zero shift, for page offset
pub const IMAGE_REL_ARM64_PAGEOFFSET_12L: u16 = 0x0007; // LDR (indexed, unsigned immediate), for page offset
pub const IMAGE_REL_ARM64_SECREL: u16 = 0x0008; // Offset within section
pub const IMAGE_REL_ARM64_SECREL_LOW12A: u16 = 0x0009; // ADD/ADDS (immediate) with zero shift, for bit 0:11 of section offset
pub const IMAGE_REL_ARM64_SECREL_HIGH12A: u16 = 0x000A; // ADD/ADDS (immediate) with zero shift, for bit 12:23 of section offset
pub const IMAGE_REL_ARM64_SECREL_LOW12L: u16 = 0x000B; // LDR (indexed, unsigned immediate), for bit 0:11 of section offset
pub const IMAGE_REL_ARM64_TOKEN: u16 = 0x000C;
pub const IMAGE_REL_ARM64_SECTION: u16 = 0x000D; // Section table index
pub const IMAGE_REL_ARM64_ADDR64: u16 = 0x000E; // 64 bit address
pub const IMAGE_REL_ARM64_BRANCH19: u16 = 0x000F; // 19 bit offset << 2 & sign ext. for conditional B

pub fn reloc_type_str_arm64(reloc: u16) -> Option<&'static str> {
    match reloc {
        IMAGE_REL_ARM64_ABSOLUTE => Some("IMAGE_REL_ARM64_ABSOLUTE"),
        IMAGE_REL_ARM64_ADDR32 => Some("IMAGE_REL_ARM64_ADDR32"),
        IMAGE_REL_ARM64_ADDR32NB => Some("IMAGE_REL_ARM64_ADDR32NB"),
        IMAGE_REL_ARM64_BRANCH26 => Some("IMAGE_REL_ARM64_BRANCH26"),
        IMAGE_REL_ARM64_PAGEBASE_REL21 => Some("IMAGE_REL_ARM64_PAGEBASE_REL21"),
        IMAGE_REL_ARM64_REL21 => Some("IMAGE_REL_ARM64_REL21"),
        IMAGE_REL_ARM64_PAGEOFFSET_12A => Some("IMAGE_REL_ARM64_PAGEOFFSET_12A"),
        IMAGE_REL_ARM64_PAGEOFFSET_12L => Some("IMAGE_REL_ARM64_PAGEOFFSET_12L"),
        IMAGE_REL_ARM64_SECREL => Some("IMAGE_REL_ARM64_SECREL"),
        IMAGE_REL_ARM64_SECREL_LOW12A => Some("IMAGE_REL_ARM64_SECREL_LOW12A"),
        IMAGE_REL_ARM64_SECREL_HIGH12A => Some("IMAGE_REL_ARM64_SECREL_HIGH12A"),
        IMAGE_REL_ARM64_SECREL_LOW12L => Some("IMAGE_REL_ARM64_SECREL_LOW12L"),
        IMAGE_REL_ARM64_TOKEN => Some("IMAGE_REL_ARM64_TOKEN"),
        IMAGE_REL_ARM64_SECTION => Some("IMAGE_REL_ARM64_SECTION"),
        IMAGE_REL_ARM64_ADDR64 => Some("IMAGE_REL_ARM64_ADDR64"),
        IMAGE_REL_ARM64_BRANCH19 => Some("IMAGE_REL_ARM64_BRANCH19"),
        _ => None,
    }
}

pub fn reloc_type_str_short_arm64(reloc: u16) -> Option<&'static str> {
    match reloc {
        IMAGE_REL_ARM64_ABSOLUTE => Some("ABSOLUTE"),
        IMAGE_REL_ARM64_ADDR32 => Some("ADDR32"),
        IMAGE_REL_ARM64_ADDR32NB => Some("ADDR32NB"),
        IMAGE_REL_ARM64_BRANCH26 => Some("BRANCH26"),
        IMAGE_REL_ARM64_PAGEBASE_REL21 => Some("PAGEBASE_REL21"),
        IMAGE_REL_ARM64_REL21 => Some("REL21"),
        IMAGE_REL_ARM64_PAGEOFFSET_12A => Some("PAGEOFFSET_12A"),
        IMAGE_REL_ARM64_PAGEOFFSET_12L => Some("PAGEOFFSET_12L"),
        IMAGE_REL_ARM64_SECREL => Some("SECREL"),
        IMAGE_REL_ARM64_SECREL_LOW12A => Some("SECREL_LOW12A"),
        IMAGE_REL_ARM64_SECREL_HIGH12A => Some("SECREL_HIGH12A"),
        IMAGE_REL_ARM64_SECREL_LOW12L => Some("SECREL_LOW12L"),
        IMAGE_REL_ARM64_TOKEN => Some("TOKEN"),
        IMAGE_REL_ARM64_SECTION => Some("SECTION"),
        IMAGE_REL_ARM64_ADDR64 => Some("ADDR64"),
        IMAGE_REL_ARM64_BRANCH19 => Some("BRANCH19"),
        _ => None,
    }
}

//
// x64 relocations
//
pub const IMAGE_REL_AMD64_ABSOLUTE: u16 = 0x0000; // Reference is absolute, no relocation is necessary
pub const IMAGE_REL_AMD64_ADDR64: u16 = 0x0001; // 64-bit address (VA).
pub const IMAGE_REL_AMD64_ADDR32: u16 = 0x0002; // 32-bit address (VA).
pub const IMAGE_REL_AMD64_ADDR32NB: u16 = 0x0003; // 32-bit address w/o image base (RVA).
pub const IMAGE_REL_AMD64_REL32: u16 = 0x0004; // 32-bit relative address from byte following reloc
pub const IMAGE_REL_AMD64_REL32_1: u16 = 0x0005; // 32-bit relative address from byte distance 1 from reloc
pub const IMAGE_REL_AMD64_REL32_2: u16 = 0x0006; // 32-bit relative address from byte distance 2 from reloc
pub const IMAGE_REL_AMD64_REL32_3: u16 = 0x0007; // 32-bit relative address from byte distance 3 from reloc
pub const IMAGE_REL_AMD64_REL32_4: u16 = 0x0008; // 32-bit relative address from byte distance 4 from reloc
pub const IMAGE_REL_AMD64_REL32_5: u16 = 0x0009; // 32-bit relative address from byte distance 5 from reloc
pub const IMAGE_REL_AMD64_SECTION: u16 = 0x000A; // Section index
pub const IMAGE_REL_AMD64_SECREL: u16 = 0x000B; // 32 bit offset from base of section containing target
pub const IMAGE_REL_AMD64_SECREL7: u16 = 0x000C; // 7 bit unsigned offset from base of section containing target
pub const IMAGE_REL_AMD64_TOKEN: u16 = 0x000D; // 32 bit metadata token
pub const IMAGE_REL_AMD64_SREL32: u16 = 0x000E; // 32 bit signed span-dependent value emitted into object
pub const IMAGE_REL_AMD64_PAIR: u16 = 0x000F;
pub const IMAGE_REL_AMD64_SSPAN32: u16 = 0x0010; // 32 bit signed span-dependent value applied at link time
pub const IMAGE_REL_AMD64_EHANDLER: u16 = 0x0011;
pub const IMAGE_REL_AMD64_IMPORT_BR: u16 = 0x0012; // Indirect branch to an import
pub const IMAGE_REL_AMD64_IMPORT_CALL: u16 = 0x0013; // Indirect call to an import
pub const IMAGE_REL_AMD64_CFG_BR: u16 = 0x0014; // Indirect branch to a CFG check
pub const IMAGE_REL_AMD64_CFG_BR_REX: u16 = 0x0015; // Indirect branch to a CFG check, with REX.W prefix
pub const IMAGE_REL_AMD64_CFG_CALL: u16 = 0x0016; // Indirect call to a CFG check
pub const IMAGE_REL_AMD64_INDIR_BR: u16 = 0x0017; // Indirect branch to a target in RAX (no CFG)
pub const IMAGE_REL_AMD64_INDIR_BR_REX: u16 = 0x0018; // Indirect branch to a target in RAX, with REX.W prefix (no CFG)
pub const IMAGE_REL_AMD64_INDIR_CALL: u16 = 0x0019; // Indirect call to a target in RAX (no CFG)
pub const IMAGE_REL_AMD64_INDIR_BR_SWITCHTABLE_FIRST: u16 = 0x0020; // Indirect branch for a switch table using Reg 0 (RAX)
pub const IMAGE_REL_AMD64_INDIR_BR_SWITCHTABLE_LAST: u16 = 0x002F; // Indirect branch for a switch table using Reg 15 (R15)

pub fn reloc_type_str_amd64(reloc: u16) -> Option<&'static str> {
    match reloc {
        IMAGE_REL_AMD64_ABSOLUTE => Some("IMAGE_REL_AMD64_ABSOLUTE"),
        IMAGE_REL_AMD64_ADDR64 => Some("IMAGE_REL_AMD64_ADDR64"),
        IMAGE_REL_AMD64_ADDR32 => Some("IMAGE_REL_AMD64_ADDR32"),
        IMAGE_REL_AMD64_ADDR32NB => Some("IMAGE_REL_AMD64_ADDR32NB"),
        IMAGE_REL_AMD64_REL32 => Some("IMAGE_REL_AMD64_REL32"),
        IMAGE_REL_AMD64_REL32_1 => Some("IMAGE_REL_AMD64_REL32_1"),
        IMAGE_REL_AMD64_REL32_2 => Some("IMAGE_REL_AMD64_REL32_2"),
        IMAGE_REL_AMD64_REL32_3 => Some("IMAGE_REL_AMD64_REL32_3"),
        IMAGE_REL_AMD64_REL32_4 => Some("IMAGE_REL_AMD64_REL32_4"),
        IMAGE_REL_AMD64_REL32_5 => Some("IMAGE_REL_AMD64_REL32_5"),
        IMAGE_REL_AMD64_SECTION => Some("IMAGE_REL_AMD64_SECTION"),
        IMAGE_REL_AMD64_SECREL => Some("IMAGE_REL_AMD64_SECREL"),
        IMAGE_REL_AMD64_SECREL7 => Some("IMAGE_REL_AMD64_SECREL7"),
        IMAGE_REL_AMD64_TOKEN => Some("IMAGE_REL_AMD64_TOKEN"),
        IMAGE_REL_AMD64_SREL32 => Some("IMAGE_REL_AMD64_SREL32"),
        IMAGE_REL_AMD64_PAIR => Some("IMAGE_REL_AMD64_PAIR"),
        IMAGE_REL_AMD64_SSPAN32 => Some("IMAGE_REL_AMD64_SSPAN32"),
        IMAGE_REL_AMD64_EHANDLER => Some("IMAGE_REL_AMD64_EHANDLER"),
        IMAGE_REL_AMD64_IMPORT_BR => Some("IMAGE_REL_AMD64_IMPORT_BR"),
        IMAGE_REL_AMD64_IMPORT_CALL => Some("IMAGE_REL_AMD64_IMPORT_CALL"),
        IMAGE_REL_AMD64_CFG_BR => Some("IMAGE_REL_AMD64_CFG_BR"),
        IMAGE_REL_AMD64_CFG_BR_REX => Some("IMAGE_REL_AMD64_CFG_BR_REX"),
        IMAGE_REL_AMD64_CFG_CALL => Some("IMAGE_REL_AMD64_CFG_CALL"),
        IMAGE_REL_AMD64_INDIR_BR => Some("IMAGE_REL_AMD64_INDIR_BR"),
        IMAGE_REL_AMD64_INDIR_BR_REX => Some("IMAGE_REL_AMD64_INDIR_BR_REX"),
        IMAGE_REL_AMD64_INDIR_CALL => Some("IMAGE_REL_AMD64_INDIR_CALL"),
        IMAGE_REL_AMD64_INDIR_BR_SWITCHTABLE_FIRST => {
            Some("IMAGE_REL_AMD64_INDIR_BR_SWITCHTABLE_FIRST")
        }
        IMAGE_REL_AMD64_INDIR_BR_SWITCHTABLE_LAST => {
            Some("IMAGE_REL_AMD64_INDIR_BR_SWITCHTABLE_LAST")
        }
        _ => None,
    }
}

pub fn reloc_type_str_short_amd64(reloc: u16) -> Option<&'static str> {
    match reloc {
        IMAGE_REL_AMD64_ABSOLUTE => Some("ABSOLUTE"),
        IMAGE_REL_AMD64_ADDR64 => Some("ADDR64"),
        IMAGE_REL_AMD64_ADDR32 => Some("ADDR32"),
        IMAGE_REL_AMD64_ADDR32NB => Some("ADDR32NB"),
        IMAGE_REL_AMD64_REL32 => Some("REL32"),
        IMAGE_REL_AMD64_REL32_1 => Some("REL32_1"),
        IMAGE_REL_AMD64_REL32_2 => Some("REL32_2"),
        IMAGE_REL_AMD64_REL32_3 => Some("REL32_3"),
        IMAGE_REL_AMD64_REL32_4 => Some("REL32_4"),
        IMAGE_REL_AMD64_REL32_5 => Some("REL32_5"),
        IMAGE_REL_AMD64_SECTION => Some("SECTION"),
        IMAGE_REL_AMD64_SECREL => Some("SECREL"),
        IMAGE_REL_AMD64_SECREL7 => Some("SECREL7"),
        IMAGE_REL_AMD64_TOKEN => Some("TOKEN"),
        IMAGE_REL_AMD64_SREL32 => Some("SREL32"),
        IMAGE_REL_AMD64_PAIR => Some("PAIR"),
        IMAGE_REL_AMD64_SSPAN32 => Some("SSPAN32"),
        IMAGE_REL_AMD64_EHANDLER => Some("EHANDLER"),
        IMAGE_REL_AMD64_IMPORT_BR => Some("IMPORT_BR"),
        IMAGE_REL_AMD64_IMPORT_CALL => Some("IMPORT_CALL"),
        IMAGE_REL_AMD64_CFG_BR => Some("CFG_BR"),
        IMAGE_REL_AMD64_CFG_BR_REX => Some("CFG_BR_REX"),
        IMAGE_REL_AMD64_CFG_CALL => Some("CFG_CALL"),
        IMAGE_REL_AMD64_INDIR_BR => Some("INDIR_BR"),
        IMAGE_REL_AMD64_INDIR_BR_REX => Some("INDIR_BR_REX"),
        IMAGE_REL_AMD64_INDIR_CALL => Some("INDIR_CALL"),
        IMAGE_REL_AMD64_INDIR_BR_SWITCHTABLE_FIRST => Some("INDIR_BR_SWITCHTABLE_FIRST"),
        IMAGE_REL_AMD64_INDIR_BR_SWITCHTABLE_LAST => Some("INDIR_BR_SWITCHTABLE_LAST"),
        _ => None,
    }
}

pub fn reloc_type_str(machine: IMAGE_FILE_MACHINE, reloc: u16) -> Option<&'static str> {
    match machine {
        IMAGE_FILE_MACHINE::IMAGE_FILE_MACHINE_I386 => reloc_type_str_i386(reloc),
        IMAGE_FILE_MACHINE::IMAGE_FILE_MACHINE_AMD64 => reloc_type_str_amd64(reloc),
        IMAGE_FILE_MACHINE::IMAGE_FILE_MACHINE_ARM64 => reloc_type_str_arm64(reloc),
        _ => None,
    }
}

pub fn reloc_type_str_short(machine: IMAGE_FILE_MACHINE, reloc: u16) -> Option<&'static str> {
    match machine {
        IMAGE_FILE_MACHINE::IMAGE_FILE_MACHINE_I386 => reloc_type_str_short_i386(reloc),
        IMAGE_FILE_MACHINE::IMAGE_FILE_MACHINE_AMD64 => reloc_type_str_short_amd64(reloc),
        IMAGE_FILE_MACHINE::IMAGE_FILE_MACHINE_ARM64 => reloc_type_str_short_arm64(reloc),
        _ => None,
    }
}

```

`coff/src/section.rs`:

```rs
//! Image section
//!
//! # References
//! * <https://learn.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-image_section_header>

use bstr::BStr;
use core::fmt::Debug;
use core::mem::size_of;
use static_assertions::const_assert_eq;
use zerocopy_derive::*;

#[derive(
    Copy,
    Clone,
    Eq,
    PartialEq,
    Default,
    Hash,
    Ord,
    PartialOrd,
    IntoBytes,
    FromBytes,
    Immutable,
    KnownLayout,
)]
#[repr(transparent)]
pub struct SectionCharacteristics(pub u32);

bitflags::bitflags! {
    impl SectionCharacteristics: u32 {
        //
        // Section characteristics.
        //
        //      IMAGE_SCN_TYPE_REG                   0x00000000  // Reserved.
        //      IMAGE_SCN_TYPE_DSECT                 0x00000001  // Reserved.
        //      IMAGE_SCN_TYPE_NOLOAD                0x00000002  // Reserved.
        //      IMAGE_SCN_TYPE_GROUP                 0x00000004  // Reserved.
        const IMAGE_SCN_TYPE_NO_PAD = 8; // obsolete
        //      IMAGE_SCN_TYPE_COPY                  0x00000010  // Reserved.

        const IMAGE_SCN_CNT_CODE                   = 0x00000020;  // Section contains code.
        const IMAGE_SCN_CNT_INITIALIZED_DATA       = 0x00000040;  // Section contains initialized data.
        const IMAGE_SCN_CNT_UNINITIALIZED_DATA     = 0x00000080;  // Section contains uninitialized data.

        const IMAGE_SCN_LNK_OTHER                  = 0x00000100;  // Reserved.
        const IMAGE_SCN_LNK_INFO                   = 0x00000200;  // Section contains comments or some other type of information.
        //      IMAGE_SCN_TYPE_OVER                  0x00000400  // Reserved.
        const IMAGE_SCN_LNK_REMOVE                 = 0x00000800;  // Section contents will not become part of image.
        const IMAGE_SCN_LNK_COMDAT                 = 0x00001000;  // Section contents comdat.
        //                                           0x00002000  // Reserved.
        //      IMAGE_SCN_MEM_PROTECTED - Obsolete   0x00004000
        const IMAGE_SCN_NO_DEFER_SPEC_EXC          = 0x00004000;  // Reset speculative exceptions handling bits in the TLB entries for this section.
        const IMAGE_SCN_GPREL                      = 0x00008000;  // Section content can be accessed relative to GP
        const IMAGE_SCN_MEM_FARDATA                = 0x00008000;
        //      IMAGE_SCN_MEM_SYSHEAP  - Obsolete    0x00010000
        const IMAGE_SCN_MEM_PURGEABLE              = 0x00020000;
        const IMAGE_SCN_MEM_16BIT                  = 0x00020000;
        const IMAGE_SCN_MEM_LOCKED                 = 0x00040000;
        const IMAGE_SCN_MEM_PRELOAD                = 0x00080000;

        const IMAGE_SCN_ALIGN_1BYTES               = 0x00100000;  //
        const IMAGE_SCN_ALIGN_2BYTES               = 0x00200000;  //
        const IMAGE_SCN_ALIGN_4BYTES               = 0x00300000;  //
        const IMAGE_SCN_ALIGN_8BYTES               = 0x00400000;  //
        const IMAGE_SCN_ALIGN_16BYTES              = 0x00500000;  // Default alignment if no others are specified.
        const IMAGE_SCN_ALIGN_32BYTES              = 0x00600000;  //
        const IMAGE_SCN_ALIGN_64BYTES              = 0x00700000;  //
        const IMAGE_SCN_ALIGN_128BYTES             = 0x00800000;  //
        const IMAGE_SCN_ALIGN_256BYTES             = 0x00900000;  //
        const IMAGE_SCN_ALIGN_512BYTES             = 0x00A00000;  //
        const IMAGE_SCN_ALIGN_1024BYTES            = 0x00B00000;  //
        const IMAGE_SCN_ALIGN_2048BYTES            = 0x00C00000;  //
        const IMAGE_SCN_ALIGN_4096BYTES            = 0x00D00000;  //
        const IMAGE_SCN_ALIGN_8192BYTES            = 0x00E00000;  //
        // Unused                                    0x00F00000
        const IMAGE_SCN_ALIGN_MASK                 = 0x00F00000;

        const IMAGE_SCN_LNK_NRELOC_OVFL            = 0x01000000;  // Section contains extended relocations.
        const IMAGE_SCN_MEM_DISCARDABLE            = 0x02000000;  // Section can be discarded.
        const IMAGE_SCN_MEM_NOT_CACHED             = 0x04000000;  // Section is not cachable.
        const IMAGE_SCN_MEM_NOT_PAGED              = 0x08000000;  // Section is not pageable.
        const IMAGE_SCN_MEM_SHARED                 = 0x10000000;  // Section is shareable.
        const IMAGE_SCN_MEM_EXECUTE                = 0x20000000;  // Section is executable.
        const IMAGE_SCN_MEM_READ                   = 0x40000000;  // Section is readable.
        const IMAGE_SCN_MEM_WRITE                  = 0x80000000;  // Section is writeable.
    }
}

impl SectionCharacteristics {
    /// Returns true if this contains `IMAGE_SCN_MEM_READ`
    pub fn is_read(self) -> bool {
        self.intersects(Self::IMAGE_SCN_MEM_READ)
    }

    /// Returns true if this contains `IMAGE_SCN_MEM_WRITE`
    pub fn is_write(self) -> bool {
        self.intersects(Self::IMAGE_SCN_MEM_WRITE)
    }

    /// Returns true if this contains `IMAGE_SCN_MEM_EXECUTE`
    pub fn is_exec(self) -> bool {
        self.intersects(Self::IMAGE_SCN_MEM_EXECUTE)
    }
}

impl Debug for SectionCharacteristics {
    fn fmt(&self, f: &mut core::fmt::Formatter<'_>) -> core::fmt::Result {
        write!(f, "[{:08x}", self.0)?;

        write!(f, "]")
    }
}

pub const IMAGE_SIZEOF_SHORT_NAME: usize = 8;

#[allow(non_camel_case_types)]
#[repr(C)]
#[derive(
    Clone,
    Default,
    Eq,
    PartialEq,
    Ord,
    PartialOrd,
    Hash,
    FromBytes,
    IntoBytes,
    Immutable,
    KnownLayout,
)]
pub struct IMAGE_SECTION_HEADER {
    pub name: [u8; IMAGE_SIZEOF_SHORT_NAME],
    pub physical_address_or_virtual_size: u32,
    pub virtual_address: u32,
    pub size_of_raw_data: u32,
    pub pointer_to_raw_data: u32,
    pub pointer_to_relocations: u32,
    pub pointer_to_linenumbers: u32,
    pub number_of_relocations: u16,
    pub number_of_linenumbers: u16,
    pub characteristics: SectionCharacteristics,
}

impl IMAGE_SECTION_HEADER {
    pub fn name(&self) -> &BStr {
        BStr::new(if let Some(i) = self.name.iter().position(|&b| b == 0) {
            &self.name[..i]
        } else {
            &self.name
        })
    }
}

pub const IMAGE_SIZEOF_SECTION_HEADER: usize = 40;

const_assert_eq!(
    size_of::<IMAGE_SECTION_HEADER>(),
    IMAGE_SIZEOF_SECTION_HEADER
);

```

`docs/codeview/calling_convention.md`:

```md
# Calling Convention

T
This document specifies constants (named values and enums) that are used by
CodeView.

The values in this file come from these sources:

* <https://github.com/microsoft/microsoft-pdb/blob/master/include/cvinfo.h>
* <https://github.com/microsoft/microsoft-pdb/blob/master/include/cvconst.h>

# `CV_call` - Function Calling Convention

`CV_call` specifies the calling convention of the procedure. It can take one of
the following values:

| Value | Name           | Description                                                      |
|-------|----------------|------------------------------------------------------------------|
| 0x00  | `NEAR_C`       | near right to left push, caller pops stack                      |
| 0x01  | `FAR_C`        | far right to left push, caller pops stack                       |
| 0x02  | `NEAR_PASCAL`  | near left to right push, callee pops stack                      |
| 0x03  | `FAR_PASCAL`   | far left to right push, callee pops stack                       |
| 0x04  | `NEAR_FAST`    | near left to right push with regs, callee pops stack            |
| 0x05  | `FAR_FAST`     | far left to right push with regs, callee pops stack             |
| 0x06  | `SKIPPED`      | skipped (unused) call index                                      |
| 0x07  | `NEAR_STD`     | near standard call                                               |
| 0x08  | `FAR_STD`      | far standard call                                                |
| 0x09  | `NEAR_SYS`     | near sys call                                                    |
| 0x0a  | `FAR_SYS`      | far sys call                                                     |
| 0x0b  | `THISCALL`     | this call (this passed in register)                             |
| 0x0c  | `MIPSCALL`     | Mips call                                                        |
| 0x0d  | `GENERIC`      | Generic call sequence                                            |
| 0x0e  | `ALPHACALL`    | Alpha call                                                       |
| 0x0f  | `PPCCALL`      | PPC call                                                         |
| 0x10  | `SHCALL`       | Hitachi SuperH call                                              |
| 0x11  | `ARMCALL`      | ARM call                                                         |
| 0x12  | `AM33CALL`     | AM33 call                                                        |
| 0x13  | `TRICALL`      | TriCore Call                                                     |
| 0x14  | `SH5CALL`      | Hitachi SuperH-5 call                                            |
| 0x15  | `M32RCALL`     | M32R Call                                                        |
| 0x16  | `CLRCALL`      | clr call                                                         |
| 0x17  | `INLINE`       | Marker for routines always inlined and thus lacking a convention|
| 0x18  | `NEAR_VECTOR`  | near left to right push with regs, callee pops stack            |
| 0x19  | `RESERVED`     | first unused call enumeration                                    |

```

`docs/codeview/codeview.md`:

```md
# CodeView

CodeView was originally a standalone debugger for early versions of Windows. The
name "Code View" now refers to the system of type records, symbol records, and
line records that have evolved from the original Code View debugger.

CodeView data structures may be stored in several places:

* Within PDB files that describe fully-linked executable images. These are
  produced by a linker.

* Within "compiler PDB" files, which describe a collection of OBJ files which
  have not yet been linked into an executable image. The MSVC `/Zi` compiler
  switch enables this.

* Within COFF OBJs in `.debug$S` sections. The MSVC `/Z7` compiler switch
  enables this.

This directory contains documents that describe these parts of Code View:

* [Primitive Types](primitive_types.md) describes how primitive types, such as `int`,
  `char`, `void` are represented

* [Types](types/types.md) describes how composed (non-primitive) types are
  represented

* [Items](items/items.md) describes how "items" are represented. Items are
  metadata about programs, such as build environment info, compiler
  command-lines, etc.

* [Symbols](symbols/symbols.md) describes how procedures, global variables, and
  other program elements, collectively known as "symbols", are represented.

* [Line Data](./line_data.md) describes how source code line mappings work.

* [Number](./number.md) describes the `Number` type, which represents literal
  constant values in a variety of types.

```

`docs/codeview/items/items.md`:

```md
# CodeView Item Records

CodeView defines a set of _item records_, which contain a variety of information
about programs. Item records are stored in the
[IPI Stream](../../pdb/ipi_stream.md).

To distinguish between type records and item records, we use `ItemId` (an alias
for `uint32_t`) for identifying item records.

Item records use the same record framing as type records. Briefly repeated:

```c
struct TypeRecord {
  uint16_t size;
  uint16_t kind;
  uint8_t payload[size - 2];
};
```

## `ItemId`

Item records are identified by the `ItemId` type alias:

```c
typedef uint32_t ItemId;
```

The value zero is reserved for a nil `ItemId`, meaning nil points to no record
at all. All other `ItemId` values must be within the range of `type_index_begin`
(inclusive lower bound) to `type_index_end` (exclusive upper bound), which is
specified in the IPI Stream Header.

To find a specific record in the IPI given its `ItemId`, first subtract
`type_index_begin` from the `ItemId`. This gives the 0-based index of the record
within the stream; let this be the value `R`. Then, begin decoding records
within the IPI Stream, counting them as they are decoded. When `R` records have
been decoded, the next record is the desired record.

The value of `type_index_begin` (in the IPI Stream Header) is typically 0x1000.
No other value has been observed.

## Item Record Summary

These are the defined item records. Although the names of these records share
the same `LF_*` prefix with "type" records (and the same numeric identifier
space), they are not type records.

Kind   | Name                                               | Description
-------|----------------------------------------------------|------------
0x1601 | [`LF_FUNC_ID`](./lf_func_id.md.md)                 | Identifies a function.
0x1602 | [`LF_MFUNC_ID`](./lf_mfunc_id.md)                  | Identifies a member function. This includes both static and non-static member functions.
0x1603 | [`LF_BUILDINFO`](./lf_buildinfo.md)                | Describes the environment and arguments to an invocation of a tool or compiler.
0x1604 | [`LF_SUBSTR_LIST`](./lf_substr_list.md)            | Contains a list of string IDs, forming a concatenated string.
0x1605 | [`LF_STRING_ID`](./lf_string_id.md)                | A string, identified by an `ItemId`.
0x1607 | [`LF_UDT_MOD_SRC_LINE`](./lf_udt_mod_src_line.md)  | Specifies the source location for the definition of a user-defined type (UDT), in a module.
0x1606 | [`LF_UDT_SRC_LINE`](./lf_udt_src_line.md)          | Specifies the source location for the definition of a user-defined type (UDT).

```

`docs/codeview/items/lf_buildinfo.md`:

```md
# `LF_BUILDINFO` (0x1603)

Describes the environment and arguments to an invocation of a tool or compiler.

```c
struct BuildInfo {
    ItemId cwd;
    ItemId build_tool;
    ItemId source_file;
    ItemId pdb_file;
    ItemId args;
};
```

Unlike most records, this record can be truncated after any field. The record
can also be seen as a single array of type `ItemId`, with meanings assigned to
each fixed array index.

Each field in `BuildInfo` is an `ItemId` that refers to an `LF_STRING_ID` or
`LF_SUBSTR_LIST` record. See `LF_SUBSTR_LIST` for details on how string records
are concatenated to form whole strings.

Each module stream may contain at most one
[`S_BUILDINFO`](../symbols/symbols.md#s_buildinfo-0x114c---build-info) record. If present,
the `S_BUILDINFO` contains the `ItemId` that points to the `LF_BUILDINFO` record
in the IPI Stream. This is the only way to associate a module with an
`LF_BUILDINFO` record.

* `cwd` - The current directory when the tool ran.

* `build_tool` - The path to the tool executable, e.g. `d:\...\cl.exe`.

* `source_file` - The primary source file that was passed to the tool. For
  C/C++, this is usually the source file that was passed on the command-line to
  the compiler. For Rust, this is the path to the root module source file.

* `pdb_file` - The path to the compiler PDB (not linker PDB), if applicable. For
  MSVC, this will only be non-empty if the compiler was invoked with `/Zi` or
  `/ZI`. See: [Debug Information
  Format](https://learn.microsoft.com/en-us/cpp/build/reference/z7-zi-zi-debug-information-format)

* `args` - Command-line arguments that were passed to the tool.

> TODO: It appears that MSVC replaces response file arguments (e.g.
> `@d:\foo\args.rsp`) with their contents, when generating the string records
> that `LF_BUILDINFO` points to. However, we should confirm (or disprove) this.

Fields in `LF_BUILDINFO` may be absent entirely (because the structure is too
small to contain the field), may have a value of 0, or may point to an empty
`LF_STRING_ID` record. Decoders should make very few assumptions about the
information in this record.

## Example

```
00000000 : 05 00 4c 41 00 00 ed 10 00 00 4d 41 00 00 4e 41 : ..LA......MA..NA
00000010 : 00 00 54 41 00 00 f2 f1                         : ..TA....
```

* `cwd` = `LF_STRING_ID` : `D:\\dw.main\\.build\\Windows\\x64\\src\\Binding\\FontBindingShared`
* `tool` = `LF_STRING_ID` : `C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.36.32532\\bin\\HostX64\\x64\\CL.exe`
* `source_file` = `LF_STRING_ID` : `D:\\dw.main\\src\\Binding\\FontBindingShared\\FontBindingShared.cpp`
* `pdb` = `LF_STRING_ID` : `D:\\dw.main\\.build\\Windows\\x64\\src\\Binding\\FontBindingShared\\Debug\\FontBinding.pdb`
* `args` = `LF_STRING_ID` : ` -external:I\"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\cppwinrt\" -external:I\"C:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\Include\\um\" -X`

## Example

First, we find an `LF_BUILDINFO` record. The record begins at offset 0x1d82.

```text
00001d80 : 00 f1 1a 00 03 16 05 00 ec 10 00 00 ed 10 00 00 : ................
00001d90 : ef 10 00 00 f0 10 00 00 fc 10 00 00 f2 f1 1a 00 : ................
00001da0 : 03 16 05 00 ec 10 00 00 ed 10 00 00 ee 10 00 00 : ................
```

The `num_strings` field is 5. Each of the string IDs decodes as follows:

Index | Name     | Value
------|----------|------
0     | `cwd`    | 0x10ec
1     | `tool`   | 0x10ed
2     | `source` | 0x10ef
3     | `pdb`    | 0x10f0
4     | `args`   | 0x10fc

The record ends with two alignment padding bytes (F2 F1).

To decode the strings, we subtract the `type_index_begin` value from the IPI
Stream Header from the string ID, then count records from the start of the IPI
record stream. The `type_index_begin` value is typically 0x1000; no other value
has been observed.

For `cwd` at `ItemID` 0x10ec, we find this `LF_STRING_ID` record:

```
00001230 : 00 00 20 01 00 00 02 00 32 00 05 16 00 00 00 00 : .. .....2.......
00001240 : 44 3a 5c 64 77 2e 6d 61 69 6e 5c 2e 62 75 69 6c : D:\dw.main\.buil
00001250 : 64 5c 57 69 6e 64 6f 77 73 5c 78 36 34 5c 73 72 : d\Windows\x64\sr
00001260 : 63 5c 43 6f 6d 6d 6f 6e 00 f3 f2 f1 72 00 05 16 : c\Common....r...
```

Note that the record offset (0x1238) cannot be directly computed from the
`ItemID` (0x10ec). It is necessary to sequentially scan records from the
beginning of the IPI Stream. Note also that `ItemId` values are subject to the
same "backward-pointing" constraint as `TypeIndex` values in the TPI; an
`ItemId` in a record `R` must point to an `ItemId` that is numerically less than
the ItemId of `R` itself.

This gives the current working directory of the tool when it was executed, which
is `D:\dw.main\.build\Windows\x64\src\Common` in this example.

Let's also examine the args `ItemId`, whose value is 0x10FC. In this case,
0x10FC points to this `LF_STRING_ID` record:

```
00001cd0 : 00 00 f9 10 00 00 fa 10 00 00 a6 00 05 16 fb 10 : ................
00001ce0 : 00 00 20 2d 65 78 74 65 72 6e 61 6c 3a 49 22 43 : .. -external:I"C
00001cf0 : 3a 5c 50 72 6f 67 72 61 6d 20 46 69 6c 65 73 20 : :\Program Files 
00001d00 : 28 78 38 36 29 5c 57 69 6e 64 6f 77 73 20 4b 69 : (x86)\Windows Ki
00001d10 : 74 73 5c 31 30 5c 49 6e 63 6c 75 64 65 5c 31 30 : ts\10\Include\10
00001d20 : 2e 30 2e 31 39 30 34 31 2e 30 5c 63 70 70 77 69 : .0.19041.0\cppwi
00001d30 : 6e 72 74 22 20 2d 65 78 74 65 72 6e 61 6c 3a 49 : nrt" -external:I
00001d40 : 22 43 3a 5c 50 72 6f 67 72 61 6d 20 46 69 6c 65 : "C:\Program File
00001d50 : 73 20 28 78 38 36 29 5c 57 69 6e 64 6f 77 73 20 : s (x86)\Windows 
00001d60 : 4b 69 74 73 5c 4e 45 54 46 58 53 44 4b 5c 34 2e : Kits\NETFXSDK\4.
00001d70 : 38 5c 49 6e 63 6c 75 64 65 5c 75 6d 22 20 2d 58 : 8\Include\um" -X
00001d80 : 00 f1 1a 00 03 16 05 00 ec 10 00 00 ed 10 00 00 : ................
```

The command-line arguments many C/C++ compiler invocations is quite long. In
this case, the `LF_STRING_ID` field has a non-zero "substring pointer" field,
which points to 0x10FB. This is the 0x10FB record, which starts at 0x1CAE:

```
00001ca0 : 34 31 2e 30 5c 77 69 6e 72 74 22 00 f2 f1 2a 00 : 41.0\winrt"...*.
00001cb0 : 04 16 09 00 00 00 f1 10 00 00 f2 10 00 00 f3 10 : ................
00001cc0 : 00 00 f4 10 00 00 f6 10 00 00 f7 10 00 00 f8 10 : ................
00001cd0 : 00 00 f9 10 00 00 fa 10 00 00 a6 00 05 16 fb 10 : ................
```

This record is `LF_SUBSTR_LIST` (0x1604), and contains a list of substrings. To
reconstruct the full command-line string, it is necessary to traverse these
substring pointers.

```

`docs/codeview/items/lf_func_id.md`:

```md
# `LF_FUNC_ID` (0x1601)

Identifies a function. This is used for global functions, regardless of linkage
visibility. It is not used for member functions; member functions use
`LF_MFUNC_ID`.

```c
struct FuncId {
    ItemId scope;
    TypeIndex func_type;
    strz name;
    uint64_t decorated_name_hash;       // optional; may not be present
};
```

`scope` specifies the scope that contains this function definition. It is 0 for
the global scope. If it is non-zero, then it points to an `LF_STRING` record
that gives the scope. For C++, the scope is a C++ namespace. In C++, if the
scope contains nested namespaces, e.g. `namespace foo { namespace bar { ... }}`,
then the `LF_STRING` record will contain the namespaces, separated by `::`,
e.g. `foo::bar`.

`func_type` specifies the function signature type.

`name` is the undecorated name of the function, e.g. `CreateWindowExW`.

`decorated_name_hash` is a hash of the full decorated name of a function. This
field is optional; it was added as a later extension to the `LF_FUNC_ID` record.
Because symbol records are required to have a size that is a multiple of 4, and
because `LF_FUNC_ID` records contain a NUL-terminated string, it may be
necessary to insert padding bytes at the end of the record. However, we need to
be able to distinguish between padding bytes and the presence of
`decorated_name_hash`.

To do so, decode the record up to and including the `name` field. If the size of
the remaining data is at least 8 bytes, then `decorated_name_hash` is present
and should be decoded. The remainder of the record should be padded (as all
symbol records are padded) to a multiple of 4 bytes.

> TODO: clarify what hash function is used for `decorated_name_hash`.

## Examples

```
00000000 : 00 00 00 00 80 10 00 00 52 74 6c 43 61 70 74 75 : ........RtlCaptu
00000010 : 72 65 43 6f 6e 74 65 78 74 00 32 a1 6d 2c 95 ab : reContext.2.m,..
00000020 : 82 0d f2 f1                                     : ....
```

* `scope` is 0 (global)
* `type` is 0x1046
* `name` is `RtlCaptureContext`
* `decorated_name_hash` is 0x0d82ab95_2c6da132
* Note the presence of the `f2 f1` padding bytes at the end of the record. They
  are not part of `decorated_name_hash`.

```
00000000 : 06 10 00 00 78 13 00 00 43 72 65 61 74 65 43 61 : ....x...CreateCa
00000010 : 63 68 65 43 6f 6e 74 65 78 74 00 dd 56 10 27 2b : cheContext..V.'+
00000020 : 85 cd 21 f1                                     : ..!.
```

* `scope` is 0x0610 and points to an `LF_STRING_ID` record whose value is
  `DWriteCore::ApiImpl`.
* `type` is 0x1378
* `name` is `CreateCacheContext`.
* `decorated_name_hash` is 0x21cd852b_271056dd.
* Note the presence of the `f1` padding byte at the end of the record. It is not
  part of `decorated_name_hash`.

## Example

This example shows several `LF_FUNC_ID` records.

```
00000040 : 44 57 72 69 74 65 43 6f 72 65 00 f1 2a 00 01 16 : DWriteCore..*...
00000050 : 00 00 00 00 09 10 00 00 44 57 72 69 74 65 43 6f : ........DWriteCo
00000060 : 72 65 43 72 65 61 74 65 46 61 63 74 6f 72 79 00 : reCreateFactory.
00000070 : 04 18 80 14 ff 26 2f 8e 10 00 07 16 32 10 00 00 : .....&/.....2...
00000080 : 01 00 00 00 fe 01 00 00 01 00 22 00 01 16 00 00 : ..........".....
00000090 : 00 00 38 10 00 00 43 72 65 61 74 65 42 69 6e 64 : ..8...CreateBind
000000a0 : 69 6e 67 00 b4 83 61 8b 64 cc d4 15 f2 f1 2a 00 : ing...a.d.....*.
000000b0 : 01 16 00 00 00 00 38 10 00 00 43 72 65 61 74 65 : ......8...Create
000000c0 : 52 65 73 74 72 69 63 74 65 64 42 69 6e 64 69 6e : RestrictedBindin
000000d0 : 67 00 e3 71 3f 18 ba 7e a5 f6 10 00 07 16 3a 10 : g..q?..~......:.
000000e0 : 00 00 2b 00 00 00 b4 05 00 00 01 00 1a 00 05 16 : ..+.............
000000f0 : 00 00 00 00 44 57 72 69 74 65 43 6f 72 65 3a 3a : ....DWriteCore::
```

In this example, there are 3 LF_FUNC_ID records:

* The first `LF_FUNC_ID` record begins at offset 0x4c and has `scope == 0`,
  `name == "DWriteCoreCreateFactory"`. The `decorated_name_hash` is
  0x8E2F26FF_14801804. The record ends on natural alignment, so there are no
  alignment padding bytes.

* Between the first `LF_FUNC_ID` record and the second is an
  `LF_UDT_MOD_SRC_LINE` record, which is not described here.

* The second `LF_FUNC_ID` record begins at 0x8a. The `scope` is 0, func_type is
  0x1038 name is CreateBinding. The decorated_name_hash is 0x15d4cc64_8b6183b4.
  This record has two alignment padding bytes (F2 F1).

* The third `LF_FUNC_ID` record begins at 0xae. The `scope` is 0, `func_type` is
  0x1038, name is CreateRestrictedBinding. The decorated_name_hash is
  0xf6a57eba_183f71e3. The record ends on natural alignment, so there is no
  padding.

It is not understood why some records have alignment padding bytes after the
`decorated_name_hash` but others do not. It may be that record payload
(excluding the record length field but including the record kind field) is
padded to a length that is a multiple of 4, for `LF_FUNC_ID` records. Note that
the `decorated_name_hash` field does not begin on an alignment boundary because
it immediately follows a NUL-terminated UTF-8 string.

```

`docs/codeview/items/lf_mfunc_id.md`:

```md
# `LF_MFUNC_ID` (0x1602)

Identifies a member function. This includes both static and non-static member
functions.

```c
struct MFuncId {
    TypeIndex parent_type;
    TypeIndex func_type;
    strz name;
    uint64_t decorated_name_hash;       // optional; may not be present
};
```

`parent_type` is the type of the class or struct that this member function is
defined on. This field points into the TPI. The type record that it points to
should be one of `LF_STRUCTURE`, `LF_CLASS`, `LF_UNION`, or `LF_ENUM`.
(`LF_ENUM` is used only by Rust.)

`func_type` field is the type of the member function. It may identify static
methods and instance methods. It is not known whether `LF_MFUNC_ID` is used for
constructors, destructors, or other special methods.

`name` is the undecorated name of the function, e.g. `AddRef`. For special
functions, such as constructors and conversion operations, each language has its
own conventions for how to encode those names. The following is a non-exhaustive
list of the special function names that have been observed in `LF_MFUNC_ID`
records:

* `{ctor}` - constructors
* `{dtor}` - destructors
* `operator unsigned __int64` - conversion method
* `operator new`
* `operator=`
* `operator==`
* `operator!=`
* `operator++`

`decorated_name_hash` has the same meaning as in the `LF_FUNC_ID` record,
including its interaction with padding bytes.

## Example

```
00000000 : 7f 10 00 00 a3 10 00 00 47 65 74 4e 65 78 74 45 : ........GetNextE
00000010 : 76 65 6e 74 53 6f 75 72 63 65 4f 62 6a 65 63 74 : ventSourceObject
00000020 : 49 64 00 46 48 d4 bc ac 43 c4 43 f1             : Id.FH...C.C.
```

* `parent_type` is 0x107f
* `func_type` is 0x10a3
* `name` is `GetNextEventSourceObjectId`
* `decorated_name_hash` is 0x43c443ac_bcd44846

Note the presence of the `f1` padding byte at the end of the record. It is not
part of `decorated_name_hash`.

## Example

```text
00000400 : 98 10 00 00 e8 00 00 00 1a 12 00 00 01 00 2e 00 : ................
00000410 : 02 16 7f 10 00 00 a3 10 00 00 47 65 74 4e 65 78 : ..........GetNex
00000420 : 74 45 76 65 6e 74 53 6f 75 72 63 65 4f 62 6a 65 : tEventSourceObje
00000430 : 63 74 49 64 00 46 48 d4 bc ac 43 c4 43 f1 1e 00 : ctId.FH...C.C...
```

In this example, `parent_type` is 0x107F, `func_type` is 0x10A3, `name` is
`GetNextEventSourceObjectId`, and the `decorated_name_hash` is
0x43c443ac_bcd44846. Note the presence of a single alignment padding byte (F1).

```

`docs/codeview/items/lf_string_id.md`:

```md
# `LF_STRING_ID` (0x1605)

Contains a single string, and optionally a pointer to another list of
substrings.

```c
struct StringId {
  ItemId substrings;
  strz string;
};
```

The substrings field is the `ItemId` of an `LF_SUBSTR_LIST` record, or 0 if
there is none. If this field is non-zero, then this string is concatenated with
the substrings identified by the `LF_SUBSTR_LIST`.

### Example

At `ItemId` 0x10ec, we find this `LF_STRING_ID` record:

```
00001230 : 00 00 20 01 00 00 02 00 32 00 05 16 00 00 00 00 : .. .....2.......
00001240 : 44 3a 5c 64 77 2e 6d 61 69 6e 5c 2e 62 75 69 6c : D:\dw.main\.buil
00001250 : 64 5c 57 69 6e 64 6f 77 73 5c 78 36 34 5c 73 72 : d\Windows\x64\sr
00001260 : 63 5c 43 6f 6d 6d 6f 6e 00 f3 f2 f1 72 00 05 16 : c\Common....r...
```

The substrings field is zero (no substring list). The value of the string is `D:\dw.main\.build\Windows\x64\src\Common`.

```

`docs/codeview/items/lf_substr_list.md`:

```md
# `LF_SUBSTR_LIST` (0x1604)

Contains a list of `ItemId` values that point to `LF_STRING_ID` records.

```c
struct SubStrList {
    ItemId substrs[];
};
```

The items in the `substrs` list should be dereferenced and concatenated into one
large string, in the order implied by `substr`. This is similar to a
[Rope](https://en.wikipedia.org/wiki/Rope_(data_structure)).

The boundaries between the items in `substr` do not have any meaning. The
divisions are simply necessary in order to keep large strings from overflowing
the size limitations of the item record format.

## Example

```
00000000 : 09 00 00 00 f1 10 00 00 f2 10 00 00 f3 10 00 00 : ................
00000010 : f4 10 00 00 f6 10 00 00 f7 10 00 00 f8 10 00 00 : ................
00000020 : f9 10 00 00 fa 10 00 00                         : ........
```

This record contains 9 `ItemId` values, all of which point to `LF_STRING_ID` records. They are listed below:

* `-c -ID:\\dw.main\\Inc -ID:\\dw.main\\Inc\\public -ID:\\dw.main\\Inc\\internal -ID:\\dw.main\\src -Zi -nologo -W3 -WX- -diagnostics:column -Od -Ob0 -D_MBCS -DWIN32 -D_WINDOWS -DDWRITE_SUBSET_MIN=0 -DDWRITE_SUBSET_CORE=1 -DDWRITE_SUBSET=1 -DDWRITE_TARGET_WINDOWS=1`
* ` -DCMAKE_INTDIR=\\\"Debug\\\" -Gm- -EHs -EHc -RTC1 -MTd -GS -guard:cf -fp:precise -Qspectre -Zc:wchar_t- -Zc:forScope -Zc:inline -GR- -std:c++17 -permissive- -YuD:/dw.main/.build/Windows/x64/src/Common/CMakeFiles/Common.dir/Debug/cmake_pch.hxx`
* ` -FpD:\\dw.main\\.build\\Windows\\x64\\src\\Common\\Common.dir\\Debug\\cmake_pch.pch -external:W3 -Gz -TP -FID:/dw.main/.build/Windows/x64/src/Common/CMakeFiles/Common.dir/Debug/cmake_pch.hxx -errorreport:queue -validate-charset -I\"C:\\Program`
* ` Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.36.32532\\include\" -I\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.36.32532\\atlmfc\\include\" -I\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Auxiliary\\VS`
* `\\include\" -I\"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\" -I\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Auxiliary\\VS\\UnitTest\\include\" -I\"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\"`
* ` -I\"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\shared\" -I\"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\winrt\" -I\"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\cppwinrt\" -I\"C:\\Program`
* ` Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\Include\\um\" -external:I\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.36.32532\\include\" -external:I\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Tools\\MSVC\\14.36.32532\\atlmfc\\in`
* `clude\" -external:I\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Auxiliary\\VS\\include\" -external:I\"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\ucrt\" -external:I\"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Auxiliar`
* `y\\VS\\UnitTest\\include\" -external:I\"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\um\" -external:I\"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\shared\" -external:I\"C:\\Program Files (x86)\\Windows Kits\\10\\Include\\10.0.19041.0\\winrt\"`

As you can see, command-line arguments are split among different `LF_STRING_ID`
records.

> TODO: It is not known whether `LF_SUBSTR_ID` can point to yet more
> `LF_SUBSTR_ID` records, forming a tree. This has not been observed in Windows
> PDBs.

```

`docs/codeview/items/lf_udt_mod_src_line.md`:

```md
# `LF_UDT_MOD_SRC_LINE` (0x1607) - UDT Source Line in Module

Specifies the source location for the definition of a user-defined type (UDT).
This record is the same as `LF_MOD_SRC_LINE` except that it adds a
`module_index` field.

```c
struct UdtModSrcLine {
    TypeIndex type;
    NameIndex file_name;
    uint32_t line;
    uint16_t module_index;
};
```

Source code comments in `cvinfo.h` imply that this record is generated by the
linker, not the compiler.

`type` is the type being described.

`file_name` points into the [Names Stream](../../pdb/names_stream.md) and
specifies the file name.

`line` is the 1-based line number of the definition of the UDT.

`module_index` is the module index of a module which defined this type.

> TODO: There's a big problem here! What is the proper value for `module_index`
> if a type was defined in more than one module? It is expected that _most_
> types are defined in more than one module. If the linker collects
> `LF_UDT_SRC_LINE` records from different modules that point to the same
> `TypeIndex`, which one does it pick? The choice is likely non-deterministic.
>
> For determinism, should we squash `module_index` and set it to some fixed
> value? What actually reads `module_index`?

> Determinism: It is assumed that this is the index of the module which defined
> this UDT. However, there is a problem: What if the same UDT is defined in more
> than one module? It is not clear if there is a deterministic result. Because
> that information (the set of modules which defined the same UDT) is not
> present in the linker, we cannot replace the module index with a deterministic
> selection from that set. Instead, we can only set the module index to some
> well-known value, like 0.

```

`docs/codeview/items/lf_udt_src_line.md`:

```md
# `LF_UDT_SRC_LINE` (0x1606) - UDT Source Line

Specifies the source location for the definition of a user-defined type (UDT).

```c
struct UdtSrcLine {
    TypeIndex type;
    NameIndex file_name;
    uint32_t line;
};
```

Source code comments in `cvinfo.h` imply that this record is generated by the
compiler, not the linker. There is some suggestion that `LF_UDT_SRC_LINE`
records do not appear in linker PDBs, but instead are found in compiler PDBs,
and that the linker converts `LF_UDT_SRC_LINE` records to `LF_UDT_MOD_SRC_LINE`
during linking.

`type` is the type being described.

`file_name` points into the [Names Stream](../../pdb/names_stream.md) and
specifies the file name.

`line` is the 1-based line number of the definition of the UDT.

```

`docs/codeview/line_data.md`:

```md
# Line Data (C13)

Module Streams may contain line number information, which describes the mapping
between source locations and instruction streams. The length of the C13 Line
Data section is found in the Module Information structure in the DBI Modules
substream, not in the corresponding Module Stream.

Older versions of MSVC use C11 Line Data. C11 Line Data is obsolete and will not
be described.

The location of the C13 Line Data is specified in the Module Information record
in the [DBI Modules Substream](../pdb/dbi_streams.md#dbi-modules-substream). See
the `DbiModuleInfo` record. Each module may have a corresponding module stream,
and thus a C13 Line Data. The `stream` field specifies the stream that contains
the C13 Line Data (as well as several other substreams). The offset of the C13
Line Data is computed by `sym_byte_size + c11_byte_size`, and the size of the
C13 Line Data is given by `c13_byte_size`.

> Invariant: The offset of the Line Data within the Module Information Stream is
> a multiple of 4.

> Invariant: The size of the C13 Line Data is a multiple of 4.

The C13 Line Data substream contains a sequence of variable-length subsections:

```c
struct Subsection {
  uint32_t subsection_kind;
  uint32_t subsection_size;
  uint8_t subsection_data[subsection_size];
  uint8_t alignment_padding[];
};
```

If bit 31 of `subsection_kind` is 1 (mask 0x8000_0000), then the subsection
should be ignored.

These are the known values for `subsection_kind`:

Name                           | Value | Description
-------------------------------|-------|------------
`DEBUG_S_SYMBOLS`              |  0xF1 | 
`DEBUG_S_LINES`                |  0xF2 | Line information
`DEBUG_S_STRINGTABLE`          |  0xF3 | 
`DEBUG_S_FILECHKSMS`           |  0xF4 | File checksums
`DEBUG_S_FRAMEDATA`            |  0xF5 | 
`DEBUG_S_INLINEELINES`         |  0xF6 | 
`DEBUG_S_CROSSSCOPEIMPORTS`    |  0xF7 | 
`DEBUG_S_CROSSSCOPEEXPORTS`    |  0xF8 | 
`DEBUG_S_IL_LINES`             |  0xF9 | 
`DEBUG_S_FUNC_MDTOKEN_MAP`     |  0xFA | 
`DEBUG_S_TYPE_MDTOKEN_MAP`     |  0xFB | 
`DEBUG_S_MERGED_ASSEMBLYINPUT` |  0xFC | 
`DEBUG_S_COFF_SYMBOL_RVA`      |  0xFD | 
`DEBUG_S_INLINEE_MDTOKEN_MAP`  |  0xFE | 
`DEBUG_S_XFGHASH_TYPE`         |  0xFF | 
`DEBUG_S_XFGHASH_VIRTUAL`      | 0x100 | 

There is no defined order for the subsections; subsections may appear in any
order.

> Determinism: Sort the subsections of the Line Data table. Place the
> `DEBUG_S_FILE_CHECKSUMS` record before all `DEBUG_S_LINES` subsections. Sort
> the `DEBUG_S_LINES` subsections by their contribution segment and offset, and
> then by their byte contents.

> Invariant: The total size in bytes of each subsection is a multiple of 4.
> Alignment padding bytes are added after `subsection_data` if `subsection_size`
> is not a multiple of 4.

This invariant guarantees 4-byte alignment for all subsections and their data.

# Subsection: DEBUG_S_LINES (0xF2)

A `DEBUG_S_LINES` subsection contains line mapping data. A C13 Line Data section
may contain any number of `DEBUG_S_LINES` subsections. It describes the line
number mappings for a single "contribution". The subsection data has this
header:

```c
struct DebugSLinesHeader {
    uint32_t contribution_offset;       // address of the instruction stream
    uint16_t contribution_section;
    uint16_t contribution_flags;
    uint32_t contribution_size;         // size in the instruction stream
};
```

The `contribution_section` and `contribution_offset` are the COFF section and
offset of this contribution. The `contribution_size` field is the size in bytes
of this contribution in the COFF section; it is not related to the size of the
data in the `DebugSLinesHeader` record.

Immediately after `DebugSLinesHeader` is a sequence of variable-length `Block`
records:

```c
// sizeof = dynamic
struct Block {
    uint32_t file_offset;           // points into DEBUG_S_FILE_CHECKSUMS
    uint32_t num_lines;
    uint32_t block_size;
    LineRecord lines[num_lines];
    ColumnRecord columns[contribution_flags & HAVE_COLUMNS ? num_lines : 0];
};
```

Each `Block` describes a set of line mappings to a single file. The line
mappings use byte offsets that are relative to the `contribution_offset`
specified in the `DebugSLinesHeader` header.

`file_offset` is the byte offset into a File Checksums subsection. This is
required even if the file is not using a checksum because the File Checksum
record contains the pointer to the file name string.

> TODO: Is the previous statement true? If file checksums are not being used, is
> `file_offset` actually a `NameIndex` or some other reference to the DBI
> Sources Substream?

> Invariant: If any Block record exists in a Lines Data substream, then there
> must be exactly one File Checksums subsection in the Lines Data substream, and
> the file_offset of the Block Record must point to a valid File Checksum
> record.

> Invariant: `num_lines` is greater than zero. There is no purpose in a Block
> that describes no lines.

> Invariant: The value of `num_lines` must always specify a set of line records
> (and optional column records) that fits within the size of the subsection.

> Invariant: In the sequence of Block records within a `DEBUG_S_LINES` record,
> the `Block` `offset` field must be sorted in increasing order.

```c
// sizeof = 8
struct LineRecord {
    uint32_t offset;
    uint32_t flags;
};
```

The `offset` field specifies the byte offset from the start of this contribution
(in the instruction stream, not the Lines Data) for this line.

The `flags` field encodes three bit-fields:

* Bits 0-23 are `line_num_start`. This is the 1-based starting line number
  within the source file of this line record.
* Bits 24-30 are `delta_line_end`. It specifies a value to add to line_num_start
  to find the ending line. If this value is zero, then this line record encodes
  only a single line, not a span of lines.
* Bit 31 is the `statement` bit field. If set to 1, it indicates that this line
  record describes a statement.

# Columns

```c
// sizeof = 4
struct ColumnRecord {
    uint16_t start_offset;
    uint16_t end_offset;
};
```

The header of `DEBUG_S_LINES` specifies whether it contains column records, by
setting a bit within the `contribution_flags` field. If the subsection contains
column records, then each block will contain an array of ColumnRecord,
immediately following the `LineRecord` array. The number of elements in the
ColumnRecord is num_lines, which is the same as the LineRecord array. These two
arrays are parallel arrays; entries at the same index are related.

The `start_offset` field specifies the offset in bytes from the start of a line
in the source file. Similarly, `end_offset` gives the end of the region in
bytes.

# Example

This is an example of a `DEBUG_S_LINES` subsection. The entire subsection is
highlighted, including the subsection header.

This is the `DEBUG_S_LINES` header:
 
* `contribution_offset` is 0x000B_4DB0.
* `contribution_section` is 1.
* `contribution_flags` is 0. This means there are no Column records.
* `contribution_size` is 0x4E.

This `DEBUG_S_LINES` subsection contains a single Block. The header of that
Block is highlighted:
 
* `file_offset` is 0xE88. This is an offset into the File Checksums Subsection
  for this module.
* `num_lines` is 0xB (11)
* `block_size` is 0x64 (100)

As mentioned above, this `DEBUG_S_LINES` subsection does not have Column
records, so we see only Line records in this block. The Line records are
highlighted:
 
Decoding these Line Records gives:

Offset (hex) | Statement? | `line_num_start`
-------------|------------|-----------------
0            | Yes        | 0x157
0x0004       | Yes        | 0x15D
0x000F       | Yes        | 0x15E
0x0013       | Yes        | 0x160
0x0018       | Yes        | 0x165
0x002E       | Yes        | 0x166
0x0032       | Yes        | 0x16A
0x003F       | Yes        | 0x171
0x0045       | Yes        | 0x172
0x0047       | Yes        | 0x167
0x0049       | Yes        | 0x173

# Subsection: DEBUG_S_FILE_CHECKSUMS (0xF4)

The `DEBUG_S_FILE_CHECKSUMS` subsection contains file checksum entries. Each
entry identifies a file using an index into another table and contains the
file's checksum, if any. Each entry has this structure:

```c
// sizeof = dynamic
// offset within DEBUG_S_FILE_CHECKSUMS is always aligned at multiple of 4
struct FileChecksum {
    NameIndex file_name;
    uint8_t checksum_size;
    uint8_t checksum_kind;
    uint8_t checksum[checksum_size];
};
```

`file_name` is a `NameIndex` value which points into the
[Names Stream](../pdb/names_stream.md).

`checksum_kind` identifies the algorithm which computed the checksum.
`checksum_size` is the size in bytes of the checksum. The following table lists
the known checksum algorithms, the `checksum_kind` which identifies them, and
the expected value of `checksum_size`.

Algorithm | `checksum_kind`  | `checksum_size` | MSVC compiler arg
----------|------------------|-----------------|------------------
None      | 0                | 0               | (none)
MD5       | 1                | 16              | `/ZH:MD5`
SHA1      | 2                | 20              | `/ZH:SHA1`
SHA256    | 3                | 32              | `/ZH:SHA_256`

Decoders should always use `checksum_size` when reading records, rather than
making assumptions about the size of `checksum` based on `checksum_kind`.

## Single instance

Within each module's C13 Line Data there should be at most one
`DEBUG_S_FILE_CHECKSUMS` subsection. This is an implication of the fact that
there are pointers (byte offsets) from `DEBUG_S_FILE_CHECKSUMS` into
`DEBUG_S_FILE_CHECKSUMS`. If there were more than one `DEBUG_S_FILE_CHECKSUMS`
subsection, then these pointers would be ambiguous.

If a C13 Line Data section contains any `DEBUG_S_LINES` subsections, then the
`DEBUG_S_FILE_CHECKSUMS` subsection must be present.

## Addressing and iterating checksum records

`DEBUG_S_LINES` subsections contain byte offsets that point into the
`DEBUG_S_FILE_CHECKSUMS` subsection. These pointers are the `file_offset` field
within each `Block` record. Because of this, it is required that these
`file_offset` values point to valid locations within the
`DEBUG_S_FILE_CHECKSUMS` table. The pointer should point to a complete
`FileChecksum` record.

> Invariant: Each `Block.file_offset` pointer points to a valid location within
> a `DEBUG_S_FILE_CHECKSUMS` subsection. The `FileChecksum` record must be
> completely within the bounds of the subsection; it cannot overlap the end of
> the subsection or start outside of it.

> Invariant: `FileChecksum` records must not overlap (share bytes). For two
> `Block.file_offset` values `f1` and `f2`, if `f1 != f2`, then the
> `FileChecksum` records for `f1` and `f2` must not overlap (must not share any
> bytes in common).

> Invariant: `FileChecksum` records within the `DEBUG_S_FILE_CHECKSUMS`
> subsection must not have any unused bytes between them. This does not apply to
> alignment bytes at the end of the subsection.

> Invariant: The size of the `DEBUG_S_FILE_CHECKSUMS` subsection must be a
> multiple of 4. If necessary, padding bytes are added to the end of the
> subsection.

> Determinism: If padding bytes are present at the end of
> `DEBUG_S_FILE_CHECKSUMS`, then they must have a value of zero.

## Alignment

> Invariant: The offset of each `FileChecksum` record within the
> `DEBUG_S_FILE_CHECKSUMS` section is a multiple of 4.

Most `FileChecksum` records will not naturally have a size that is a multiple of
4 because the fixed-size portion of the record has a length of 6 and all of the
current checksum sizes have lengths that are a multiple of 4. Decoders must skip
padding bytes after each `FileChecksum` so that the current decoding position
(relative to the start of `DEBUG_S_FILE_CHECKSUMS`) is a multiple of 4. Encoders
must insert these padding bytes.

> Determinism: Set the padding bytes to zero.

## Correlation of `FileChecksum` and DBI Sources Substream

There is an unusual implementation detail, which associates the order of
`FileChecksum` entries with the entries of source code locations within the
[DBI Sources Substream](../pdb/dbi_stream.md#dbi-sources-substream).

# References

* <https://github.com/microsoft/microsoft-pdb/blob/master/include/cvinfo.h>
  + `DEBUG_S_SUBSECTION_TYPE`
  + `CV_DebugSSubectionHeader_t`
  + `CV_DebugSLinesHeader_t`
  + `CV_Line_t`
  + `CV_Column_t`

```

`docs/codeview/number.md`:

```md
# Numbers

Type records and symbol records may contain instances of the `Number` type. In
the PDB documentation, these are called "numeric leaves".

> Example: The `S_CONSTANT` record contains a `Number` field which is the
> value of the constant.

`Number` contains a numeric value of a given value and type. The type of the
number is represented in its encoding, so there is no need for an external
specification of the type of the number.

`Number` is a variable-length data type. Its size can be determined from its
encoded form.

The representation of `Number` always requires at least two bytes. These first
two bytes either specify the literal value of the number (with an implied type),
or they specify the type and size of the `Number`. The first two bytes are
encoded as a little-endian `uint16_t` value, and we refer to this `uint16_t`
value as the _leaf_.

If _leaf_ is less than 0x8000, then the type is implicitly `LF_USHORT` (an
unsigned 16-bit value), the value is simply the leaf value itself, and there are
no additional bytes after the first two bytes. This is the simplest case and is
very common.

If _leaf_ is greater or equal to 0x8000, then the leaf specifies the type and
size of the `Number`:

Leaf   | Leaf Type       | Size<br>(bytes) | Description
-------|-----------------|----|------------
0x8000 | `LF_CHAR`       | 1  | signed 8-bit integer
0x8001 | `LF_SHORT`      | 2  | signed 16-bit integer
0x8002 | `LF_USHORT`     | 2  | unsigned 16-bit integer
0x8003 | `LF_LONG`       | 4  | signed 32-bit integer
0x8004 | `LF_ULONG`      | 4  | unsigned 32-bit integer
0x8005 | `LF_REAL32`     | 4  | 32-bit floating-point
0x8006 | `LF_REAL64`     | 8  | 64-bit floating-point
0x8007 | `LF_REAL80`     | 10 | 80-bit floating-point
0x8008 | `LF_REAL128`    | 16 | 128-bit floating-point
0x8009 | `LF_QUADWORD`   | 8  | signed 64-bit integer
0x800a | `LF_UQUADWORD`  | 8  | unsigned 64-bit integer
0x800b | `LF_REAL48`     | 6  | 48-bit floating-point
0x800c | `LF_COMPLEX32`  | 8  | 32-bit floating-point complex (real, imaginary)
0x800d | `LF_COMPLEX64`  | 16 | 64-bit floating-point complex (real, imaginary)
0x800e | `LF_COMPLEX80`  | 20 | 80-bit floating-point complex (real, imaginary)
0x800f | `LF_COMPLEX128` | 32 | 128-bit floating-point complex (real, imaginary)
0x8010 | `LF_VARSTRING`  | \* | Variable-length string; see below
0x8017 | `LF_OCTWORD`    | 16 | signed 128-bit integer
0x8018 | `LF_UOCTWORD`   | 16 | unsigned 128-bit integer
0x8019 | `LF_DECIMAL`    | 16 | OLE variant decimal value
0x801a | `LF_DATE`       | 8  | OLE variant date value
0x801b | `LF_UTF8STRING` | \* | UTF-8 NUL-terminated string

In this table, the "Size (Bytes)" column specifies the number of bytes that
_follow_ the leaf value; the size does not count the number of bytes in the leaf
value itself. For example, the total size of an `LF_ULONG` number is 6 bytes,
not 4 bytes.

There is no leaf value for an unsigned 8-bit value because all unsigned 8-bit
values can be stored directly in _leaf_.

## `LF_VARSTRING` (0x8010)

For `LF_VARSTRING`, the leaf value is followed by a `uint16_t` value that
specifies the length in bytes of the string data, followed by the length data.
There is no alignment padding after the string data.

## Examples

Encoded bytes           | Leaf Type       | Value
------------------------|-----------------|--------------
00 00                   | `LF_USHORT`     | 0
0a 01                   | `LF_USHORT`     | 266 (0x010a)
00 80 41                | `LF_CHAR`       | 'A'
01 80 fe ff             | `LF_SHORT`      | -2
02 80 fe ff             | `LF_USHORT`     | 0xfffe
03 80 11 22 00 00       | `LF_LONG`       | 0x2211
04 80 fe ff ff ff       | `LF_LONG`       | -2
04 80 fe ff ff ff       | `LF_ULONG`      | 0xfffffffe
1b 80 41 42 43 00       | `LF_UTF8STRING` | "ABC"
10 80 04 00 44 45 46    | `LF_VARSTRING`  | "DEF"

```

`docs/codeview/primitive_types.md`:

```md
# Primitive Types

CodeView defines a set of _primitive types_. Primitive types have specified
meanings and do not require type records within a PDB to define the primitive
type. For example, the C/C++ types `void`, `unsigned int`, and `char` are
primitive types.

Primitive types are identified by `TypeIndex` values that are less than the
`type_index_begin` value defined in the TPI Stream Header. `type_index_begin` is
required to be at least 4096, so all `TypeIndex` values that are less than 4096
are primitive types.

The `TypeIndex` value for primitive types is composed of the following bit
fields. Because primitive types are in the reserved range of 0-4095, the
`TypeIndex` value has 12 significant bits.

Field        | Bits     | Description
-------------|----------|------------
`size`       | 0-2      | Specifies the size of the primitive; see below.
(reserved)   | 3        | Reserved; always zero.
`type`       | 4-7      | Specifies the type of the primitive; see below.
`mode`       | 8-10     | Specifies the _mode_ of the primitive; see below.
(reserved)   | 11       | Reserved; always zero.

The `type` field can take on these values:

`type` value  | `type` name    | Usage
--------------|----------------|------
0x00          | `TY_SPECIAL`   | Special
0x01          | `TY_SINT`      | Signed integer
0x02          | `TY_UINT`      | Unsigned integer
0x03          | `TY_BOOLEAN`   | Boolean
0x04          | `TY_REAL`      | Real (floating-point)
0x05          | `TY_COMPLEX`   | Complex
0x06          | `TY_SPECIAL2`  | Special2
0x07          | `TY_REALLYINT` | "Really" int value
0x08 - 0x0f   |                | Reserved

The interpretation of the `size` field depends on the `type` field. It can taken
on these values:

`size` value | `TY_SPECIAL` usage | `TY_SINT`, `TY_UINT`,<br>`TY_BOOLEAN` usage | `TY_REALLYINT` | `TY_REAL` and <br>`TY_COMPLEX` usage | `TY_SPECIAL2` usage
---------|--------------------|-----------|--------------|-----------|--------------
0        | No type            | 1 byte    | `char`       | 32-bit    | bit
1        | absolute symbol    | 2 bytes   | `wchar_t`    | 64-bit    | Pascal `CHAR`
2        | segment            | 4 bytes   | `int16_t`    | 80-bit    | reserved
3        | C/C++ `void` type  | 8 bytes   | `uint16_t`   | 128-bit   | reserved
4        | obsolete           | reserved  | `int32_t`    | 48-bit    | reserved
5        | obsolete           | reserved  | `uint32_t`   | reserved  | reserved
6        | obsolete           | reserved  | `int64_t`    | reserved  | reserved
7        | obsolete           | reserved  | `uint64_t`   | reserved  | reserved

The `mode` field specifies the pointer mode for pointer types. It can take on
thse values:

`mode` value | `mode` name    | Usage
-------------|----------------|------
0            | `MODE_DIRECT`  | Direct value; not a pointer
1            | `MODE_NEAR`    | Near pointer
2            | `MODE_FAR`     | Far pointer
3            | `MODE_HUGE`    | Huge pointer
4            | `MODE_NEAR32`  | 32-bit near pointer
5            | `MODE_FAR32`   | 32-bit far pointer
6            | `MODE_NEAR64`  | 64-bit near pointer

The distinctions between "near", "far", and "huge" pointers date back to
segmented 80x86 CPU architectures. They are no longer relevant, since modern
systems always use flat memory models. For 32-bit architectures (x86, ARM32),
`mode` is `MODE_NEAR32`. For 64-bit architectures (AMD64, ARM64), `mode` is
`MODE_NEAR64`.

CodeView defines types for segmented architectures (such as 8086), which are
obsolete. This specification omits these obsolete variants.

> TODO: actually delete obsolete variants from the lists below

# C/C++ types and explicitly-sized ("really") types

C/C++ defines a family of integer types, such as `int`, `short int`, `long int`,
`long long int`, and their unsigned variants. The C/C++ language specification
does not mandate a specific bit size for each type. Instead, the C/C++
specification defines that the bit size (and therefore range) of `int` is equal
to or greater than that of `short int`, and that `long int` is equal to or
greater than that of `int`, and similar for `long long int`.

MSVC uses specific bit sizes for these types, but MSVC also defines a separate
set of integer types which have explicit sizes. These are separate types, even
if they are trivially convertible between the C/C++ types of the same bit size.
These explicitly-sized types are the "really" types. In C/C++ source code, they
are identified by `__int8`, `__int32`, `unsigned __int64`, etc.

# Types summary

Most types also have a 32-bit pointer type and a 64-bit pointer type. There are
many obsolete types that are not shown in this table.

Most compilers, such as the MSVC C/C++ compiler, MASM, and Rust, can generate
only a subset of the types in this table.

Name          |Value | 32-bit<br>Pointer Name | Value  | 64-bit<br>Pointer Name | Value | Size<br>(bytes) | C/C++ Type | Description
--------------|------|--------------|------|--------------|------|--|-------------------|------------------------
`T_NOTYPE`    |0x0000|              |      |              |      |  |                   | Uncharacterized type
`T_ABS`       |0x0001|              |      |              |      |  |                   | Absolute symbol
`T_SEGMENT`   |0x0002|              |      |              |      |  |                   | Segment type (obsolete)
`T_VOID`      |0x0003|`T_32PVOID`   |0x0403|`T_64PVOID`   |0x0603|  |`void`             | The C/C++ `void` type
`T_CHAR`      |0x0010|`T_32PCHAR`   |0x0410|`T_64PCHAR`   |0x0610| 1|`char`             | 8-bit signed 
`T_QUAD`      |0x0013|`T_32PQUAD`   |0x0413|`T_64PQUAD`   |0x0613| 8|`long long`        | 64-bit signed 
`T_SHORT`     |0x0011|`T_32PSHORT`  |0x0411|`T_64PSHORT`  |0x0611| 2|`short`            | 16-bit signed
`T_LONG`      |0x0012|`T_32PLONG`   |0x0412|`T_64PLONG`   |0x0612| 4|`long`             | 32-bit signed 
`T_UCHAR`     |0x0020|`T_32PUCHAR`  |0x0420|`T_64PUCHAR`  |0x0620| 1|`unsigned char`    | 8-bit unsigned
`T_USHORT`    |0x0021|`T_32PUSHORT` |0x0421|`T_64PUSHORT` |0x0621| 2|`unsigned short`   | 16-bit unsigned
`T_ULONG`     |0x0022|`T_32PULONG`  |0x0422|`T_64PULONG`  |0x0622| 4|`unsigned long`    | 32-bit unsigned
`T_UQUAD`     |0x0023|`T_32PUQUAD`  |0x0423|`T_64PUQUAD`  |0x0623| 8|`unsigned long long` | 64-bit unsigned
`T_BOOL08`    |0x0030|`T_32PBOOL08` |0x0430|`T_64PBOOL08` |0x0630| 1|`bool`             | 8-bit Boolean value
`T_BOOL16`    |0x0031|`T_32PBOOL16` |0x0431|`T_64PBOOL16` |0x0631| 2|                   | 16-bit Boolean value
`T_BOOL32`    |0x0032|`T_32PBOOL32` |0x0432|`T_64PBOOL32` |0x0632| 4|                   | 32-bit Boolean value
`T_BOOL64`    |0x0033|`T_32PBOOL64` |0x0433|`T_64PBOOL64` |0x0633| 8|                   | 64-bit Boolean value
`T_RCHAR`     |0x0070|`T_32PRCHAR`  |0x0470|`T_64PRCHAR`  |0x0670|  |                   | real char: is this `__int8`?
`T_WCHAR`     |0x0071|`T_32PWCHAR`  |0x0471|`T_64PWCHAR`  |0x0671| 2|`wchar_t`          | wide char
`T_INT2`      |0x0072|`T_32PINT2`   |0x0472|`T_64PINT2`   |0x0672| 2|`__int16`          | 16 bit signed int
`T_UINT2`     |0x0073|`T_32PUINT2`  |0x0473|`T_64PUINT2`  |0x0673| 2|`unsigned __int16` | 16 bit unsigned int
`T_INT4`      |0x0074|`T_32PINT4`   |0x0474|`T_64PINT4`   |0x0674| 4|`__int32`          | really 32 bit signed int
`T_UINT4`     |0x0075|`T_32PUINT4`  |0x0475|`T_64PUINT4`  |0x0675| 4|`unsigned __int32` | really 32 bit unsigned int
`T_INT8`      |0x0076|`T_32PINT8`   |0x0476|`T_64PINT8`   |0x0676| 8|`__int64`          | 64-bit signed  int
`T_UINT8`     |0x0077|`T_32PUINT8`  |0x0477|`T_64PUINT8`  |0x0677| 8|`unsigned __int64` | 64-bit unsigned int
`T_REAL32`    |0x0040|`T_32PREAL32` |0x0440|`T_64PREAL32` |0x0640| 4|`float`            | 32-bit real 
`T_REAL48`    |0x0044|`T_32PREAL48` |0x0444|`T_64PREAL48` |0x0644| 6|                   | 48-bit real
`T_REAL64`    |0x0041|`T_32PREAL64` |0x0441|`T_64PREAL64` |0x0641| 8|`double`           | 64-bit real 
`T_REAL80`    |0x0042|`T_32PREAL80` |0x0442|`T_64PREAL80` |0x0642|10|                   | 80-bit real 
`T_REAL128`   |0x0043|`T_32PREAL128`|0x0443|`T_64PREAL128`|0x0643|16|                   | 128-bit real 
`T_CPLX32`    |0x0050|`T_32PCPLX32` |0x0450|`T_64PCPLX32` |0x0650| 8|                   | 32-bit complex 
`T_CPLX64`    |0x0051|`T_32PCPLX64` |0x0451|`T_64PCPLX64` |0x0651|16|                   | 64-bit complex 
`T_CPLX80`    |0x0052|`T_32PCPLX80` |0x0452|`T_64PCPLX80` |0x0652|20|                   | 80-bit complex 
`T_CPLX128`   |0x0053|`T_32PCPLX128`|0x0453|`T_64PCPLX128`|0x0653|32|                   | 128-bit complex

# Special types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_NOTYPE`    | 0x0000 |                       | Uncharacterized type (no type)
`T_ABS`       | 0x0001 |                       | Absolute symbol
`T_SEGMENT`   | 0x0002 |                       | Segment type (obsolete)
`T_VOID`      | 0x0003 | `void`                | The C/C++ `void` type
`T_32PVOID`   | 0x0403 | `void *`              | 32 bit near pointer to `void`
`T_64PVOID`   | 0x0603 | `void *`              | 64-bit pointer to `void`

# Character types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_CHAR`      | 0x0010 |                       | 8-bit signed 
`T_UCHAR`     | 0x0020 |                       | 8-bit unsigned
`T_32PCHAR`   | 0x0410 |                       | 32-bit pointer to 8-bit signed
`T_32PUCHAR`  | 0x0420 |                       | 32-bit pointer to 8-bit unsigned
`T_64PCHAR`   | 0x0610 |                       | 64-bit pointer to 8 bit signed
`T_64PUCHAR`  | 0x0620 |                       | 64-bit pointer to 8 bit unsigned

# Really a character types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_RCHAR`     | 0x0070 |                       | real char
`T_32PRCHAR`  | 0x0470 |                       | 32-bit pointer to a real char
`T_64PRCHAR`  | 0x0670 |                       | 64-bit pointer to a real char

# Wide character types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_WCHAR`     | 0x0071 | `wchar_t`             | wide char
`T_32PWCHAR`  | 0x0471 | `wchar_t *`           | 32-bit pointer to a wide char
`T_64PWCHAR`  | 0x0671 | `wchar_t *`           | 64-bit pointer to a wide char

# Really 16 bit integer types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_INT2`      | 0x0072 | `__int16`             | 16 bit signed int
`T_UINT2`     | 0x0073 | `unsigned __int16`    | 16 bit unsigned int
`T_32PINT2`   | 0x0472 | `__int16 *`           | near pointer to 16 bit signed int
`T_32PUINT2`  | 0x0473 | `unsigned __int16 *`  | near pointer to 16 bit unsigned int
`T_64PINT2`   | 0x0672 | `__int16 *`           | 64-bit pointer to 16 bit signed int
`T_64PUINT2`  | 0x0673 | `unsigned __int16 *`  | 64-bit pointer to 16 bit unsigned int

# 16-bit short types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_SHORT`     | 0x0011 | `short`               | 16-bit signed
`T_USHORT`    | 0x0021 | `unsigned short`      | 16-bit unsigned
`T_32PSHORT`  | 0x0411 | `short *`             | 32-bit pointer to 16 bit signed
`T_32PUSHORT` | 0x0421 | `unsigned short *`    | 32-bit pointer to 16 bit unsigned
`T_64PSHORT`  | 0x0611 | `short *`             | 64-bit pointer to 16 bit signed
`T_64PUSHORT` | 0x0621 | `unsigned short *`    | 64-bit pointer to 16 bit unsigned

# Really 32 bit integer types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_INT4`      | 0x0074 |                       | really 32 bit signed int
`T_UINT4`     | 0x0075 |                       | really 32 bit unsigned int
`T_32PINT4`   | 0x0474 |                       | 32-bit pointer to 32 bit signed int
`T_32PUINT4`  | 0x0475 |                       | 32-bit pointer to 32 bit unsigned int
`T_64PINT4`   | 0x0674 |                       | 64-bit pointer to 32 bit signed int
`T_64PUINT4`  | 0x0675 |                       | 64-bit pointer to 32 bit unsigned int

# 32-bit long types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_LONG`      | 0x0012 |                       | 32-bit signed 
`T_ULONG`     | 0x0022 |                       | 32-bit unsigned
`T_32PLONG`   | 0x0412 |                       | 32-bit pointer to 32 bit signed 
`T_32PULONG`  | 0x0422 |                       | 32-bit pointer to 32 bit unsigned 
`T_64PLONG`   | 0x0612 |                       | 64-bit pointer to 32 bit signed 
`T_64PULONG`  | 0x0622 |                       | 64-bit pointer to 32 bit unsigned

# Really 64-bit integer types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_INT8`      | 0x0076 |                       | 64-bit signed  int
`T_UINT8`     | 0x0077 |                       | 64-bit unsigned int
`T_32PINT8`   | 0x0476 |                       | 32-bit pointer to 64 bit signed int
`T_32PUINT8`  | 0x0477 |                       | 32-bit pointer to 64 bit unsigned int
`T_64PINT8`   | 0x0676 |                       | 64-bit pointer to 64 bit signed int
`T_64PUINT8`  | 0x0677 |                       | 64-bit pointer to 64 bit unsigned int

# 64-bit integral types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_QUAD`      | 0x0013 |                       | 64-bit signed 
`T_UQUAD`     | 0x0023 |                       | 64-bit unsigned
`T_32PQUAD`   | 0x0413 |                       | 32-bit pointer to 64 bit signed 
`T_32PUQUAD`  | 0x0423 |                       | 32-bit pointer to 64 bit unsigned 
`T_64PQUAD`   | 0x0613 |                       | 64-bit pointer to 64 bit signed 
`T_64PUQUAD`  | 0x0623 |                       | 64-bit pointer to 64 bit unsigned 

# 32-bit real types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_REAL32`    | 0x0040 | `float`               | 32-bit real 
`T_32PREAL32` | 0x0440 | `float *`             | 32-bit pointer to 32 bit real
`T_64PREAL32` | 0x0640 | `float *`             | 64 pointer to 32 bit real

# 48-bit real types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_REAL48`    | 0x0044 | 48-bit real 
`T_32PREAL48` | 0x0444 | 32-bit pointer to 48 bit real
`T_64PREAL48` | 0x0644 | 64-bit pointer to 48 bit real

# 64-bit real types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_REAL64`    | 0x0041 | 64-bit real 
`T_32PREAL64` | 0x0441 | 32-bit pointer to 64 bit real
`T_64PREAL64` | 0x0641 | 64-bit pointer to 64 bit real

# 80-bit real types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_REAL80`    | 0x0042 | 80-bit real 
`T_32PREAL80` | 0x0442 | 32-bit pointer to 80 bit real
`T_64PREAL80` | 0x0642 | 64-bit pointer to 80 bit real

# 128-bit real types

Name            | Value  | C/C++ Type            | Description
----------------|--------|-----------------------|------------
`T_REAL128`     | 0x0043 | 128-bit real 
`T_32PREAL128`  | 0x0443 | 32-bit pointer to 128 bit real
`T_64PREAL128`  | 0x0643 | 64-bit pointer to 128 bit real

# 32-bit complex types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_CPLX32`    | 0x0050 | 32-bit complex 
`T_32PCPLX32` | 0x0450 | 32-bit pointer to 32 bit complex
`T_64PCPLX32` | 0x0650 | 64-bit pointer to 32 bit complex

# 64-bit complex types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_CPLX64`    | 0x0051 | 64-bit complex 
`T_32PCPLX64` | 0x0451 | 32-bit pointer to 64 bit complex
`T_64PCPLX64` | 0x0651 | 64-bit pointer to 64 bit complex

# 80-bit complex types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_CPLX80`    | 0x0052 | 80-bit complex 
`T_32PCPLX80` | 0x0452 | 32-bit pointer to 80 bit complex
`T_64PCPLX80` | 0x0652 | 64-bit pointer to 80 bit complex

# 128-bit complex types

Name            | Value  | C/C++ Type            | Description
----------------|--------|-----------------------|------------
`T_CPLX128`     | 0x0053 |                       | 128-bit complex 
`T_32PCPLX128`  | 0x0453 |                       | 32-bit pointer to 128 bit complex
`T_64PCPLX128`  | 0x0653 |                       | 64-bit pointer to 128 bit complex

# Boolean types

Name          | Value  | C/C++ Type            | Description
--------------|--------|-----------------------|------------
`T_BOOL08`    | 0x0030 |                       | 8-bit boolean
`T_BOOL16`    | 0x0031 |                       | 16-bit boolean
`T_BOOL32`    | 0x0032 |                       | 32-bit boolean
`T_BOOL64`    | 0x0033 |                       | 64-bit boolean
`T_32PBOOL08` | 0x0430 |                       | 32-bit pointer to 8 bit boolean
`T_32PBOOL16` | 0x0431 |                       | 32-bit pointer to 16 bit boolean
`T_32PBOOL32` | 0x0432 |                       | 32-bit pointer to 32 bit boolean
`T_32PBOOL64` | 0x0433 |                       | 32-bit pointer to 64-bit boolean
`T_64PBOOL08` | 0x0630 |                       | 64-bit pointer to 8 bit boolean
`T_64PBOOL16` | 0x0631 |                       | 64-bit pointer to 16 bit boolean
`T_64PBOOL32` | 0x0632 |                       | 64-bit pointer to 32 bit boolean
`T_64PBOOL64` | 0x0633 |                       | 64-bit pointer to 64-bit boolean

```

`docs/codeview/symbols/s_annotation.md`:

```md
# `S_ANNOTATION` (0x1019) - Annotation

```c
struct Annotation {
    uint32_t offset;
    uint16_t section;
    uint16_t count;
    strz strings[count];
};
```

This symbol stores annotations that point to a specific location in code
streams. This allows for analysis tools, such as debuggers, instrumentation
systems (ETW), etc. to process annotations.

`strings` contains a sequence of strings, whose count is given by `count`. PDB
does not specify how to interpret these strings.

The MSVC compiler provides an extension which allows it to insert `S_ANNOTATION`
records into module streams. This extension is invoked using the
`__annotation("format", ... args ...)` syntax.

For example, when MSVC compiles this program:

```c
int main(int argc, char** argv) {
    __annotation(L"Hello!", L"World!");
    return 0;
}
```

it produces this `S_ANNOTATION` symbol:

```
00000784 :   S_ANNOTATION: [0001:00006202]
    Hello!
    World!

00000784 :  1a 00 19 10 02 62 00 00 01 00 02 00 48 65 6c 6c : .....b......Hell
00000794 :  6f 21 00 57 6f 72 6c 64 21 00 00 00             : o!.World!...
```

Note the `segment:offset` value of `[0001:00006202]`. This shows that the
`S_ANNOTATION` points to a specific location in the code stream, even though the
annotation has no effect on code generation.

```

`docs/codeview/symbols/s_armswitchtable.md`:

```md
# `S_ARMSWITCHTABLE` (0x1159) - ARM Switch Table

Describes a switch table (jump table), which is used to implement `switch`
statements or similar control flow constructs.

MSVC generates this symbol only when targeting ARM64. LLVM generates this symbol
for all target architectures.

```c
struct ArmSwitchTable {
    uint32_t offset_base;
    uint16_t sect_base;
    uint16_t switch_type;
    uint32_t offset_branch;
    uint32_t offset_table;
    uint16_t sect_branch;
    uint16_t sect_table;
    uint32_t num_entries;
};
```

`offset_base` is the section-relative offset to the base address for switch
offsets. This is the base address of the target of the jump. The value stored
within the jump table entry is added to this base.

`sect_base` is the section index of the base for switch offsets.

`switch_type` specifies the type of each entry in the jump table. See [Switch
Types](#switch-types) below.

`offset_branch` is the section-relative offset to the table branch instruction.

`offset_table` is the section-relative offset to the start of the table.

`sect_branch` is the section index of the table branch instruction.

`sect_table` is the section index of the table.

`num_entries` is the number of switch table entries.

## Switch Types

The `switch_type` field can have the following values:

Value | Name          | Description
------|---------------|------------
`0`   | `INT1`        | Signed 1-byte offset
`1`   | `UINT1`       | Unsigned 1-byte offset
`2`   | `INT2`        | Signed 2-byte offset
`3`   | `UINT2`       | Unsigned 2-byte offset
`4`   | `INT4`        | Signed 4-byte offset
`5`   | `UINT4`       | Unsigned 4-byte offset
`6`   | `POINTER`     | Absolute pointer (no base)
`7`   | `UINT1SHL1`   | Unsigned 1-byte offset, shift left by 1
`8`   | `UINT2SHL1`   | Unsigned 2-byte offset, shift left by 1

## Notes

LLVM often generates tables where the base address and the table address are the
same, but this is not necessarily true for all tables.

This symbol can only appear in module symbol streams, never in the global symbol
stream.

```

`docs/codeview/symbols/s_block.md`:

```md
# `S_BLOCK32` (0x1103) - Block Start

Describes the start of an inner block of lexically scoped symbols within a
[Procedure](./s_procs.md). The lexical scope is terminated by a matching `S_END`
symbol.

```c
struct Block {
    uint32_t p_parent;
    uint32_t p_end;
    uint32_t length;
    uint32_t offset;
    uint16_t segment;
    strz name;
};
```

This symbol must be nested (directly or indirectly) within a procedure. It may
be nested within another `S_BLOCK32` or inline call site. This also implies that
`S_BLOCK32` can only occur within module symbol streams.

```

`docs/codeview/symbols/s_buildinfo.md`:

```md
# `S_BUILDINFO` (0x114c) - Build Info

```c
struct BuildInfoSym {
    ItemId id;
};
```

This record associates the current module with an
[`LF_BUILDINFO`](../../codeview/items/lf_buildinfo.md) record in the IPI Stream.
The `BuildInfoSym` record does not directly contain the build information; use
`id` to look up the corresponding record in the IPI Stream.

```

`docs/codeview/symbols/s_coffgroup.md`:

```md
# `S_COFFGROUP` (0x1137) - COFF Group

```c
struct CoffGroup {
    uint32_t cb;
    uint32_t characteristics;
    uint32_t offset;
    uint16_t segment;
    strz name;
};
```

This symbol describes a COFF group (also called a subsection), which is a
contiguous region within a COFF section.

`S_COFFGROUP` records are present only in the `* Linker *` special module.
These records are generated by the linker to describe how the linker has
organized code and data into contiguous subsections.

## Fields

`cb` is the size in bytes of the COFF group.

`characteristics` contains bit flags that describe the properties of this COFF
group. These flags are the same as the COFF section characteristics defined by
the Windows PE format. See [IMAGE_SECTION_HEADER in the Windows
documentation](https://learn.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-image_section_header)
for details on the flag values.

`offset` is the section-relative offset where this COFF group begins.

`segment` is the section index that contains this COFF group.

`name` is the name of the COFF group. For example, `.text$mn` is a COFF group
within the `.text` segment.

## Example

COFF groups allow the linker to subdivide a section into multiple contiguous
regions. For example, the `.text` section might be divided into several COFF
groups:

- `.text$di` - Dynamic initializers
- `.text$mn` - Main code
- `.text$x` - Exception handling code
- `.text$zz` - Termination code

Each of these groups would be described by a separate `S_COFFGROUP` symbol in
the `* Linker *` module.

## Notes

This symbol can only appear in the `* Linker *` special module, never in regular
module symbol streams or the global symbol stream.

COFF groups are entirely contained within a single COFF section. They represent
a linker-level organization of code and data, and are useful for understanding
how the linker has laid out the executable.

```

`docs/codeview/symbols/s_compile.md`:

```md
# `S_COMPILE2` (0x1116) and `S_COMPILE3` (0x113c) - Compile

This symbol provides a rich set of information regarding the compiler that was
used to produce the object file that it is embedded in. It is expected that
there is exactly one of these records in each object file and that it resides in
the first, non-optional, `.debug$S` section.

In a PDB, the module stream for a given module should contain exactly one
`S_COMPILE` symbol (either `S_COMPILE2` or `S_COMPILE3`).

The `S_COMPILE2` symbol is the older variant. Encoders should use the
`S_COMPILE3` symbol. Decoders should be prepared to process both `S_COMPILE2`
and `S_COMPILE3` symbols. These symbols may only appear in module symbol
streams, not the global symbol stream. These symbols are mutually-exclusive
within a given module symbol stream; only one of them may be used in a given
module stream. However, a linked executable may contain modules from different
tools, and so it is legal for some modules to use `S_COMPILE2` and others to use
`S_COMPILE3`.

```c
struct Compile2 {
    uint32_t flags;
    uint16_t machine;
    uint16_t frontend_major_version;
    uint16_t frontend_minor_version;
    uint16_t frontend_build_version;
    uint16_t major_version;
    uint16_t minor_version;
    uint16_t build_version;
    strz name;
};

struct Compile3 {
    uint32_t flags;
    uint16_t machine;
    uint16_t frontend_major_version;
    uint16_t frontend_minor_version;
    uint16_t frontend_build_version;
    uint16_t frontend_qfe_version;
    uint16_t major_version;
    uint16_t minor_version;
    uint16_t build_version;
    uint16_t qfe_version;
    strz name;
};
```

`flags` contains language and per-module attributes, encoded in these bit fields:

Name            | Bits   | Description
----------------|--------|------------
language        | 0-7    | language index; see below
EC              | 8      | compiled for edit and continue
NoDbgInfo       | 9      | not compiled with debug information
LTCG            | 10     | compiled with link-time code generation
NoDataAlign     | 11     | no data alignment on globals
ManagedPresent  | 12     | managed code/data present
SecurityChecks  | 13     | compiled with security checks (/GS)
HotPatch        | 14     | compiled with hotpatch support  (/hotpatch)
CVTCIL          | 15     | converted with CVTCIL
MSILModule      | 16     | MSIL netmodule
pad             | 16-31  | reserved, must be zero

Language enumeration:

Value  | Language
-------|---------
0      | C
1      | C++
2      | Fortran
3      | MASM
4      | Pascal
5      | Basic
6      | Cobol
7      | Linker
8      | CvtRes
9      | CvtPgd
10     | C#
11     | Visual Basic .NET
12     | CIL (e.g. `ilasm`)
13     | Java
14     | JScript
15     | MSIL (LTCG of `.NETMODULE`)
16     | HLSL
17     | Objective-C
18     | Objective-C++
19     | Swift
20     | ALIASOBJ
21     | Rust
22     | Go
23-255 | Reserved

```

`docs/codeview/symbols/s_constant.md`:

```md
# `S_CONSTANT` (0x1107) - Constant

```c
struct Constant {
  TypeIndex type;
  Number value;
  strz name;
};
```

Defines a named constant. This symbol can appear in both the global symbol
stream and in module symbol streams.

See [Number](../number.md) for the encoding of `value`.

## `S_MANCONSTANT` (0x112d) - Managed Constant

```c
struct ManagedConstant {
    uint32_t metadata_token;
    Number value;
    strz name;
};
```

Defines a named constant whose type is defined by MSIL metadata. This symbol has
been observed only in module streams.

```

`docs/codeview/symbols/s_data.md`:

```md
# Data symbols: `S_GDATA32` (0x1007) and `S_LDATA32` (0x1008)

```c
struct Data {
    TypeIndex type;
    uint32_t offset;
    uint16_t segment;
    strz name;
};
```

`S_GDATA32` describes a global variable, usually with external visibility across
all translation units. For example, `int g_foo;` would be described with an
`S_GDATA32` record. `S_GDATA32` records are stored in the Global Symbol Stream,
never in module symbol streams.

`S_LDATA32` describes a variable whose lifetime duration is global, but whose
visibility is limited to a single module. `S_LDATA32` rercords can be stored in
either the Global Symbol Stream or in module symbol streams. (It is not clear
why the linker decides the place `S_LDATA32` in global vs. module symbol
streams.)

In C, `static` variables are described using `S_LDATA32`. Example:

```c
// Static variables use S_LDATA32
static int g_foo;
```

In C++, variables at global scope defined within an anonymous namespace
are described using `S_LDATA32`:

```cpp
namespace {
    int g_foo;
}
```

The `type` field describes the type of the variable, and implicitly, its size.
`offset` and `segment` describe its location in the image.

```

`docs/codeview/symbols/s_end.md`:

```md
# `S_END` (0x0006) - End of Scope

The `S_END` record terminates a nested scope. Nested scopes are created by
`S_LPROC32`, `S_GPROC32`, `S_THUNK32`, `S_INLINESITE`, etc. See
[Procedures](./s_procs.md).

The `S_END` symbol has no payload.

This symbol can appear only within module symbol streams.

```

`docs/codeview/symbols/s_frameproc.md`:

```md
# `S_FRAMEPROC` (0x1012) - Frame Procedure Information

Describes the stack frame layout of a [Procedure](./s_procs.md). 

The `S_FRAMEPROC` symbol is optional, for any given procedure. If any of the
`flags` are non-zero, this record should be added to the symbols for that
procedure. If present, there should be only one `S_FRAMEPROC` record for a given
procedure; it is not legal to have two or more. Also, if present, the
`S_FRAMEPROC` symbol should be nested directly within the procedure symbol, not
contained within a nested scope such as `S_BLOCK32`.

```c
struct FrameProc {
    uint32_t frame_size;
    uint32_t pad_size;
    uint32_t pad_offset;
    uint32_t save_regs_size;
    uint32_t exception_handler_offset;
    uint16_t exception_handler_segment;
    uint32_t flags;
};
```

`flags` describes various attributes of the function:

Name               | Bits | Description
-------------------|------|------------
`has_alloca`       | 0    | function uses `_alloca()`
`has_set_jmp`      | 1    | function uses `setjmp()`
`has_long_jmp`     | 2    | function uses `longjmp()`
`has_inl_asm`      | 3    | function uses inline asm
`has_eh`           | 4    | function has EH states
`inl_spec`         | 5    | function was specified as inline
`has_seh`          | 6    | function has SEH
`naked`            | 7    | function is `__declspec(naked)`
`security_checks`  | 8    | function has buffer security check
`pad`              | 9-31 | must be zero

```

`docs/codeview/symbols/s_inlinesite.md`:

```md
# `S_INLINESITE` (0x114d) and `S_INLINESITE2` (0x115c)

`S_INLINESITE` describes where one procedure was inlined into another.
`S_INLINESITE2` is a refinement of `S_INLINESITE`.

`S_INLINESITE` records may only appear within [Procedures](./s_procs.md).

```c
struct InlineSite {
    uint32_t p_parent;
    uint32_t p_end;
    ItemId inlinee;
};

struct InlineSite2 {
    uint32_t p_parent;
    uint32_t p_end;
    ItemId inlinee;
    uint8_t binary_annotations[];
};
```

```

`docs/codeview/symbols/s_label.md`:

```md
# `S_LABEL32` (0x1105) - Code Label

```c
struct Label {
    uint32_t offset;
    uint16_t segment;
    uint8_t flags;
    strz name;
};
```

Identifies a named label, such as jump targets within machine code.

`S_LABEL32` records are always nested within [Procedures](./s_procs.md).

```

`docs/codeview/symbols/s_local.md`:

```md
# Local variables: `S_LOCAL` (0x113e) and related records

Symbol records describe the names and locations of local variables. The records
that describe local variables can be nested at any level within a procedure
symbol.

## `S_REGREL32` (0x1111) - Register-Relative Local Variable

```c
struct RegRel {
    uint32_t offset;
    TypeIndex type;
    uint16_t register;
    strz name;
};
```

This symbol specifies symbols that are allocated relative to a register. This
should be used on all platforms besides x86 and on x86 when the register is not
a form of `EBP`.

This symbol can only occur within procedure symbol scopes (`S_LPROC32` and
`S_GPROC32`). It may be nested within inlined call sites with procedure symbol
scopes.

**Unlike** the rest of the local variable symbols described in this document,
`S_REGREL32` **is not** paired with an `S_LOCAL` record. This is because
`S_REGREL32` already contains a `name` field.

## Paired symbols

Most local variables are described by a _pair_ of symbol records. The first
record is always `S_LOCAL` and this record specifies the name of the variable,
e.g. `result` or `windowHandle`. The second record describes the location of the
value of the local and there are many different kinds of records for the
different locations for locals.

For example:

```text
S_GPROC32: add_two_numbers
  S_LOCAL: x
  S_REGREL32: rcx                 <-- location of x
  S_LOCAL: y
  S_REGREL32: rdx                 <-- location of y
  ...
  S_END
```

## `S_LOCAL` (0x113e) - Local Variable

The `S_LOCAL` symbol introduces a local variable.

```c
struct Local {
    TypeIndex type;
    uint32_t flags;
    strz name;
};
```

Immediately after the `S_LOCAL` record is a record that describes the location
of the value of the variable. The following record kinds are known to describe
local variables. The list is not exhaustive and

There are known to be records that Microsoft considers to be its proprietary
intellectual property, relating largely to debugging of optimized code. This
document may list the numeric symbol IDs for these records but it will not
describe their structure or their usage.

## Definition-Range Symbols

These symbols specify the location of a local variable within a range of
instruction addresses. These symbols must immediately follow an `S_LOCAL`
symbol.

## `S_DEFRANGE` (0x113F)

Specifies a DIA program for interpreting a variable.

### `S_DEFRANGE_SUBFIELD` (0x1140)

Specifies a DIA program and an offset in a parent variable for interpreting a
variable.

### `S_DEFRANGE_REGISTER` (0x1141) - Definition Range: Register

A live range of an en-registered variable.

```c
struct DefRangeRegister {
    uint16_t register;
    RangeAttr attr;
    uint8_t gaps[];
};

// See cvinfo.h for CV_RANGEATTR
struct RangeAttr {
    uint16_t bits;
};
```

### `S_DEFRANGE_FRAMEPOINTER_REL` (0x1142) - Definition Range: Frame-Pointer Relative

> TODO

### `S_DEFRANGE_SUBFIELD_REGISTER` (0x1143) - Definition Range: Sub-Field Register

> TODO

### `S_DEFRANGE_FRAMEPOINTER_REL_FULL_SCOPE` (0x1144) - Definition Range: Frame-Pointer Relative, Full Scope

A frame variable valid in the entire function scope.

```
struct DefRangeFramePointerRelFullScope {
    uint32 offset_to_frame_pointer;
};
```

### `S_DEFRANGE_REGISTER_REL` (0x1145) - Definition Range: Register-Relative

> TODO

```

`docs/codeview/symbols/s_namespace.md`:

```md
# `S_UNAMESPACE` (0x1124) - Using Namespace

```c
struct UsingNamespace {
    strz namespace;
};
```

This symbol is used to indicate that the compiler has added a namespace to the
lookup-scope of the lexical scope that contains this symbol. The use of this
symbol is restricted to procedures and blocks, as we felt it unreasonable to
burden the consumer side of the debugging information with having to search all
of the module's symbols to find them. Consequently, all functions that fall
under a module level `using namespace` directive will each have a `S_UNAMESPACE`
symbol record.

```

`docs/codeview/symbols/s_objname.md`:

```md
# `S_OBJNAME` (0x1101) - Object Name

```c
struct ObjectName {
    uint32_t signature;
    strz name;
};
```

`signature` is a robust signature that will change every time that the module
will be compiled or different in any way. It should be at least a CRC32 based
upon module name and contents.

`name` is the full path of the object file.

```

`docs/codeview/symbols/s_procs.md`:

```md
# Procedures: `S_LPROC32` (0x110F), `S_GPROC32` (0x1110), and more

Procedures (executable code such as functions and methods) are defined using a
variety of symbols records. Procedures are described by a _sequence_ of symbol
records. These records are organized into a nested hierarchy (a tree) of
_scopes_.

The scopes within a procedure describe semantic scopes (such as compound
statements) within source code. However, there are no constraints or guarantees
about the exact number of scopes or their nesting relationships, only that the
scopes form a tree.

The procedure symbol (`S_LPROC32`, `S_GPROC32`, etc.) starts the root scope.
The following is the full list of procedure symbols; all of them start a
procedure scope:

* `S_LPROC32`
* `S_GPROC32`
* `S_LPROC32_DPC`
* `S_LPROC32_DPC_ID`
* `S_LPROC32_ST`
* `S_GMANPROC`
* `S_LMANPROC`

The following symbols create a nested scope. These symbols _should not_ be
present in symbol streams unless they are nested (directly or indirectly) within
a procedure scope.

* `S_BLOCK32`: defines a block of code
* `S_INLINESITE`: describes code that was inlined
* `S_INLINESITE2`: describes code that was inlined

All scopes (procedure scopes and nested scopes) are terminated with an `S_END`
record.

Within this document, the term "procedure symbol" or simply "procedure" refers
to the entire tree of symbol records, starting with the procedure record
(e.g. `S_GPROC32`) and ending with the matching `S_END` record. If necessary,
we will use "procedure symbol" or `G_PROC32` to clarify the distinction between
the procedure (the actual executable code), the root symbol record, or the
tree of records associated with the procedure.

Example:

```text
S_GPROC32: paint_house
  S_LOCAL: favorite_color
  S_REGREL32: rax
  S_BLOCK32:                    <-- starts a new scope
    S_LOCAL: x
    S_REGREL32: rcx             <-- location of x
    S_LOCAL: y
    S_REGREL32: rdx             <-- location of y
    S_END                       <-- ends the scope of S_BLOCK32
  S_END                         <-- ends the scope of S_GPROC32
```

This shows the nesting relationship of the scopes.

Procedure symbols are stored in module streams. They are never stored in the
Global Symbol Stream. However, the Global Symbol Stream may contain references
to procedure symbols; see [RefSym](s_refsyms.md).

## Local variables

See [Local variables](s_local.md).

## Procedure Start Records

Each procedure symbol begins with a _procedure start_ record. There are several
symbol record kinds that start a procedure symbol:

* `S_GPROC32`: A global procedure, visible to all modules.
* `S_LPROC32`: A "local" procedure (i.e.g `static`), visible only within a
    single * `S_LPROC32_DPC`: A local procedure with DPC semantics (for Windows
    kernel development)
* `S_LPROC32_DPC_ID`: A local procedure with DPC semantics (for Windows kernel
    development)
* `S_LPROC32_ST`: A local procedure; older (obsolete) record format.
* `S_GMANPROC`: A global procedure defined in an MSIL manifest.
* `S_LMANPROC`: A local procedure defined in an MSIL manifest.

## `S_LPROC32` (0x110f) and `S_GPROC32` (0x1110) - Procedure Start

The `S_LPROC32` and `S_GPROC32` records have the same field layout and (aside
from visibility) have the same semantics.

```c
struct Procedure {
    uint32_t p_parent;
    uint32_t p_end;
    uint32_t p_next;
    uint32_t proc_length;
    uint32_t debug_start;
    uint32_t debug_end;
    TypeIndex proc_type;
    uint32_t offset;
    uint16_t segment;
    uint8 flags;
    strz name;
};
```

The `S_LPROC32` and `S_GPROC32` symbols define "free" functions (functions at
global scope or defined within a namespace); they are not used for static or
instance methods of classes. In this section of the specification, `S_LPROC32`
will be assumed to apply to both `S_LPROC32` and `S_GPROC32`, unless clarified.

Procedure symbols can only appear in a module symbol stream. It cannot appear in
the global symbol stream. However, the global symbol stream can point to a
procedure symbol by using `S_PROCREF`.

Each `S_LPROC32` symbol starts a _symbol scope_, and the records that follow it
are associated with that procedure. For example, `S_REGREL32` defines a
register-relative local variable, and it is implicitly associated with the
containing `S_LPROC32` scope. The procedure definition scope is terminated by an
`S_END` record.

The `p_parent`, `p_end`, and `p_next` fields are byte offsets within the symbol
stream, relative to the `S_LPROC32` record. Because `S_LPROC32` cannot be nested
within another symbol scope, `s_parent` is always zero (meaning "no parent").
The `p_end` field is the byte offset (relative to the `S_LPROC32` symbol) of the
start of the `S_END` symbol. The `p_next` field is deprecated; decoders should
ignore it and encoders should always set it to zero.

`proc_length` is the size in bytes of the procedure (the machine code
instructions, not the symbol record). This only applies to procedures whose
instructions form a single contiguous block.

`debug_start` is the offset in bytes from the start of the procedure to the
point where the stack frame has been set up. Parameter and frame variables can
be viewed at this point.

`debug_end` is the offset in bytes from the start of the procedure to the point
where the procedure is ready to return and has calculated its return value, if
any. Frame and register variables can still be viewed.

`proc_type` is the type of the function signature.

`offset` and `segment` give the address of the procedure.

`flags` has these bits:

| Name         | Bit | Description                                              |
|--------------|-----|----------------------------------------------------------|
| `fpo`        | 0   | true if function has frame pointer omitted               |
| `interrupt`  | 1   | true if function is interrupt routine                    |
| `unused`     | 2   | must be zero                                             |
| `never`      | 3   | true if function does not return (eg: exit())            |
| `unused`     | 4   | must be zero                                             |
| `custcall`   | 5   | true if custom calling convention used                   |
| `noinline`   | 6   | true if function marked as noinline                      |
| `optdbginfo` | 7   | true if function has optimized code debugging info       |

## `S_LMANPROC` (0x112b) and `S_GMANPROC` (0x112a) - Managed Procedure Start

Defines the start of a managed (MSIL) procedure. This symbol definition is
similar to `S_GDATA32` but uses MSIL tokens instead of `TypeIndex`.

```c
struct Procedure {
        uint32_t p_parent;
        uint32_t p_end;
        uint32_t p_next;
        uint32_t proc_length;
        uint32_t debug_start;
        uint32_t debug_end;
        uint32_t proc_type;         // MSIL token
        uint32_t offset;
        uint16_t segment;
        uint8 flags;
        strz name;
};
```

## Nested symbols

Procedures may contain the following nested symbols:

| Hex    | Symbol          | Link                                     | Description
|--------|-----------------|------------------------------------------|---------------------------
| 0x1103 | `S_BLOCK32`     | [s_block.md](s_block.md)                 | Block scope
| 0x114d | `S_INLINESITE`  | [s_inlinesite.md](s_inlinesite.md)       | Inlined code
| 0x115c | `S_INLINESITE2` | [s_inlinesite.md](s_inlinesite.md)       | Inlined code (v2)
| 0x113e | `S_LOCAL`       | [s_local.md](s_local.md)                 | Local variable
| 0x1111 | `S_REGREL32`    | [s_local.md](s_local.md)                 | Register-relative variable
| 0x0006 | `S_END`         | [s_end.md](s_end.md)                     | Scope terminator

```

`docs/codeview/symbols/s_pub.md`:

```md
# `S_PUB32` (0x110e) - Public Symbol

```c
struct PubSym {
    uint32_t flags;
    uint32_t offset;
    uint16_t segment;
    strz name;
};
```

`S_PUB32` should only appear in the Global Symbol Stream.

```

`docs/codeview/symbols/s_refsyms.md`:

```md
# Reference Symbols: `S_PROCREF`, `S_DATAREF`, etc.

Several symbols (e.g. `S_PROCREF`) in the Global Symbol Stream use this
definition. These symbols point from the GSS into the symbol stream of a
specific module.

```c
struct RefSym2 {
    uint32_t name_checksum;
    uint32_t symbol_offset;
    uint16_t module_index;
    strz name;
};
```

`name_checksum` appears to be set to 0 in all records found.

Important! `module_index` is the 1-based index of the module, e.g. 1 is the
first module. This is unlike most other PDB data structures, where module
indexes are numbered starting at 0.

`symbol_offset` is the offset in bytes in the module symbol stream.

* `name_checksum` is a checksum computed over the `name` field.

  > TODO: Which hash algorithm computes this checksum?

* `symbol_offset` is the byte offset within the symbol stream of a module,
  identified by `module_index`. This byte offset starts from the beginning of
  the symbol stream, and the count includes the 4-byte header at the start of
  the module stream. That is, if this record points to the first symbol record
  in the module's symbol stream, then the value of `symbol_offset` will be 4,
  not 0.

* `module_index` is the 1-based index of the module.

> Invariant: `module_index` is in the range 1 to `num_modules` (inclusive),
> where `num_modules` is the number of modules as determined by counting
> `ModuleInfo` records in the DBI Modules Substream.

> Invariant: `symbol_offset` is the byte offset of a valid symbol record stored
> in the symbol stream of the module identified by `module_index`. It points to
> the beginning of a symbol record, not the interior of a record.

## `S_PROCREF` (0x1125) - Procedure Reference

Describes a reference to an `S_GPROC32` record, which is stored in a module
symbol stream. It uses the `RefSym2` definition.

`S_PROCREF` should only appear in the GSS.

## `S_DATAREF` (0x1126) - Data Reference

> Important: This record is _not present_ in linker PDBs. The record may be
> obsolete.

Describes a reference to an `S_GDATA32` or `S_LDATA32` record, which is stored
in a module symbol stream. It uses the `RefSym2` definition.

## `S_LPROCREF` (0x1127) - Local Procedure Reference

Describes a reference to an `S_LPROC32` record, which is stored in a module
symbol stream. It uses the `RefSym2` definition.

`S_LPROCREF` should only appear in the GSS.

## `S_TOKENREF` (0x1129) - MSIL Token Reference

Describes a reference to a symbol related to an MSIL metadata token. It uses the
`RefSym2` definition.

> TODO: Clarify how MSIL integration works, and what set of symbol records
> `S_TOKENREF` can point to.

`S_TOKENREF` should only appear in the GSS.

## `S_ANNOTATIONREF` (0x1128) - Annotation Reference

Describes a reference to an `S_ANNOTATION` symbol, which is stored in a module
symbol stream. It uses the `RefSym2` definition.

`S_ANNOTATIONREF` should only appear in the GSS.

The `name` field in `S_ANNOTATIONREF` is not used and should be empty.

```

`docs/codeview/symbols/s_section.md`:

```md
# `S_SECTION` (0x1136) - COFF Section

```c
struct Section {
    uint16_t section;          // Section number
    uint8_t  align;            // Alignment of this section (power of 2)  
    uint8_t  reserved;         // Reserved, must be zero
    uint32_t rva;              // RVA of this section base
    uint32_t cb;               // Size in bytes of this section
    uint32_t characteristics;  // Section characteristics (bit flags)
    strz name;                 // NUL-terminated section name
};
```

The `S_SECTION` symbol describes a COFF section in a PE executable. COFF
sections are contiguous regions of memory that contain code, data, or other
information in an executable file.

## Fields

`section` is the 1-based section number within the PE executable. This number is
used to reference the section from other parts of the debug information.

`align` specifies the alignment of this section as a power of 2. For example, a
value of 12 means the section is aligned to 2^12 = 4096 bytes.

`reserved` is a reserved field that must be zero.

`rva` is the Relative Virtual Address (RVA) of the base of this section. This is
the address where the section will be loaded in memory, relative to the base
address of the executable.

`cb` is the size of this section in bytes.

`characteristics` contains the section characteristics as bit flags. These flags
are the same as the COFF section characteristics defined by the Windows PE
format. See [IMAGE_SECTION_HEADER in the Windows documentation](https://learn.microsoft.com/en-us/windows/win32/api/winnt/ns-winnt-image_section_header)
for details on the specific bit flags.

`name` is the NUL-terminated name of the section, such as `.text`, `.data`,
`.rdata`, etc.

## Usage

The `S_SECTION` symbol provides mapping information between section numbers used
in other debug symbols and the actual sections in the PE executable. This
information is essential for debuggers and other tools to correctly resolve
addresses and understand the layout of the executable.

Section symbols are typically found in module symbol streams and provide the
foundation for interpreting other symbols that reference sections by number,
such as procedure symbols that specify which section contains their code.

## Examples

Common section names include:
- `.text` - executable code
- `.data` - initialized data
- `.rdata` - read-only data
- `.bss` - uninitialized data
- `.pdata` - exception handling information
- `.idata` - import table

This symbol appears only in the special `* Linker *` module symbol stream, not
in the global symbol stream.

```

`docs/codeview/symbols/s_thread.md`:

```md
# `S_LTHREAD32` (0x1112) and `S_GTHREAD32` (0x1113) - Thread Storage

```c
struct ThreadStorage {
    TypeIndex type;
    uint32_t offset;
    uint16_t segment;
    strz name;
};
```

These symbols are used for data declared with the `__declspec(thread)` or
`thread_static` storage attribute. The `S_LTHREAD32` symbol is used for
variables that are local to a module (no external linkage) and `S_GTHREAD32` is
used for variables that have external linkage.

These symbols can appear in both module symbol streams and global symbol
streams. They are never nested within another symbol scope.

```

`docs/codeview/symbols/s_thunk.md`:

```md
# `S_THUNK32` (0x1102) - Thunk Start

```c
struct Thunk {
    uint32_t p_parent;
    uint32_t p_end;
    uint32_t p_next;
    uint32_t offset;
    uint16_t segment;
    uint16_t thunk_length;
    uint8_t ordinal;
    strz name;
    // variant data follows
};
```

This record is used to specify any piece of code that exists outside a
procedure. It is followed by an `S_END` record. The thunk record is intended for
small code fragments. and a two byte length field is sufficient for its intended
purpose.

The `p_parent`, `p_end`, `p_next`, `offset`, and `segment` fields have the same
meaning as the fields with the same name within `S_LPROC32`.

```

`docs/codeview/symbols/s_trampoline.md`:

```md
# `S_TRAMPOLINE` (0x112c) - Trampoline

```c
struct Trampoline {
    uint16_t trampoline_kind;
    uint16_t thunk_size;
    uint32_t thunk_offset;
    uint32_t target_offset;
    uint16_t thunk_section;
    uint16_t target_section;
};
```

This symbol is emitted only by a linker to indicate a fairly simple and short,
light-weight thunk to the debugger. It was introduced due to the more complex
code requirements of the RISC platforms whereas on x86, a thunk typically
doesn't need any more code that a single instruction where it is simple to
decode the destination. These are typically used when the debugger is expected
to step through the thunk to the other side. Hence, there is a need for the
target information in the debug symbols to locate the target in a machine
independent manner.

`trampoline_kind` is 0 for linker incremental thunks, and 1 for linker
branch-island thunks.

`thunk_size` is the size of the thunk's code.

```

`docs/codeview/symbols/s_udt.md`:

```md
# `S_UDT` (0x1108) - User-Defined Type

```c
struct Udt {
  TypeIndex type;
  strz name;
};
```

A user-defined type (UDT). This symbol is usually found in the global symbol
stream, but in rare cases can also be found in a module symbol stream.

`type` points into the TPI Stream. The pointed-to type should be `LF_ENUM`,
`LF_STRUCTURE`, `LF_CLASS`, or `LF_INTERFACE`.

> TODO: Can `S_UDT` point to primitives?  What about typedefs?

The `name` field of the `S_UDT` symbol record should be equal to the name field
of the pointed-to type. The `S_UDT` symbol can appear in the global symbol
stream and in module symbol streams.

```

`docs/codeview/symbols/symbols.md`:

```md
# CodeView Symbols

Many streams encode sequences of CodeView "symbols". Symbols are variable-length
records which describe certain elements of high-level languages, such as C, C++,
Rust, etc.

CodeView was defined in the 1980s and has been extended and modified in nearly
every release of MSVC. This document does not describe all of the possible
symbol record kinds. Instead, it focuses on symbols used by modern compilers
(including, but not limited to, MSVC and LLVM).

# Symbol record framing

A "symbol stream" is a sequence of variable-length symbol records. Each symbol
record starts with a 4-byte header, which is followed by the payload for that
symbol. The 4-byte header specifies the size of the symbol payload and a 16-bit
"symbol kind," which specifies how to interpret this symbol record.

Each symbol record has this structure:

```c
struct SymbolRecord {
  uint16_t size;              // size of the entire record (including header)
  uint16_t kind;              // S_GPROC32, S_ANNOTATION, etc.
  uint8_t payload[size - 2];
};
```

The `size` field specifies the size in bytes of this record, excluding the
`size` field itself, but including the size of the `kind` and payload fields.
Because the `size` field includes the size of the `kind` field, the smallest
legal value for `size` is 2.

We use the term `kind` to identify the meaning and structure of symbols instead
of `type` to avoid confusion with `TypeIndex` and CodeView type descriptions.

The starting offset of each symbol record is required to be aligned to a
multiple of 4. The size of each symbol record, including the size, kind, and
payload fields, is also required to be a multiple of 4. Because the size field
does not count the size of the size field itself, this means that the size field
is always 2 less than a multiple of 4.

The size of each symbol record is required to be a multiple of 4, which implies
that the payload size is also a multiple of 4. However, many symbol records
contain variable-length data within their payload, and the size of this payload
is not necessarily a multiple of 4. In that case, padding bytes are inserted
into the payload. By convention, the framing bytes use the values 0xF1, 0xF2,
and 0xF3 (in hex). That is, if a single byte needs to be padded, the value is
0xF1. If two bytes are needed, then the values 0xF1 and 0xF2 are inserted, etc.

The `kind` field specifies how to interpret a symbol. It is a named constant,
such as `S_CONSTANT`, `S_COFFGROUP`, etc. The symbol kinds are summarized and
described in detail in this section.

# Symbol Streams: Global vs. Module

Within a PDB there are two kinds of symbol streams. Each PDB contains one Global
Symbol Stream (GSS). Some PDBs do not contain a GSS; it is not possible for a
PDB to contain more than one GSS. Each module within a PDB may contain its own
"module symbol stream".

The Global Symbol Stream and the module symbol streams use the same symbol
stream format, but there are many differences in how symbols are stored and
interpreted. Many symbol kinds can appear only in the GSS, or appear only in a
module symbol stream. Nested symbols may appear only in module streams. For
example, symbols that describe the local variables of a function may occur only
in module symbol streams, never in global symbol streams.
 
# Symbols Summary

This table summarizes the known symbol record kinds. It also lists whether each
kind of symbol may appear in the Global Symbol Stream and/or the Module Symbol
Stream, and whether the symbol can be used as a "root" scope or can only be used
within a nested symbol scope. These terms will be explained in later sections.

Code (Hex) | Name                                         | Location      | Description
-----------|----------------------------------------------|---------------|------------
`0006`     | [`S_END`](s_end.md)                          | module        | Ends a nested scope
`1007`     | [`S_GDATA32`](s_data.md)                     | global        | Global data with external linkage
`1008`     | [`S_LDATA32`](s_data.md)                     | global/module | Global data with private module visibility
`1012`     | [`S_FRAMEPROC`](s_frameproc.md)              | module        | Frame procedure information
`1019`     | [`S_ANNOTATION`](s_annotation.md)            | module        | Annotation
`1101`     | [`S_OBJNAME`](s_objname.md)                  | module        | Gives the name of the object file
`1102`     | [`S_THUNK32`](s_thunk.md)                    | module        | Code outside a procedure (thunk)
`1103`     | [`S_BLOCK32`](s_block.md)                    | module        | Block of code within a procedure
`1105`     | [`S_LABEL32`](s_label.md)                    | module        | Code label within procedures
`1107`     | [`S_CONSTANT`](s_constant.md)                | global/module | Named constant
`1108`     | [`S_UDT`](s_udt.md)                          | global/module | User-defined type (struct, enum, etc.)
`110E`     | [`S_PUB32`](s_pub.md)                        | global        | Public symbol
`110F`     | [`S_LPROC32`](s_procs.md)                    | module        | Procedure private to a module
`1110`     | [`S_GPROC32`](s_procs.md)                    | module        | Procedure with global linkage
`1111`     | [`S_REGREL32`](s_local.md)                   | module        | Register-relative local variable
`1112`     | [`S_LTHREAD32`](s_thread.md)                 | global/module | Thread-local storage (module-local)
`1113`     | [`S_GTHREAD32`](s_thread.md)                 | global/module | Thread-local storage (global)
`1116`     | [`S_COMPILE2`](s_compile.md)                 | module        | Compiler version and flags
`1124`     | [`S_UNAMESPACE`](s_namespace.md)             | module        | Using namespace
`1125`     | [`S_PROCREF`](s_refsyms.md)                  | global        | Procedure reference
`1126`     | [`S_DATAREF`](s_refsyms.md)                  | global        | Data reference
`1127`     | [`S_LPROCREF`](s_refsyms.md)                 | global        | Local procedure reference
`1128`     | [`S_ANNOTATIONREF`](s_refsyms.md)            | global        | Annotation reference
`1129`     | [`S_TOKENREF`](s_refsyms.md)                 | global        | MSIL token reference
`112A`     | [`S_GMANPROC`](s_procs.md)                   | module        | Managed procedure with global linkage
`112B`     | [`S_LMANPROC`](s_procs.md)                   | module        | Managed procedure with module-local linkage
`112C`     | [`S_TRAMPOLINE`](s_trampoline.md)            | module        | Trampoline
`112D`     | [`S_MANCONSTANT`](s_constant.md)             | module        | Managed constant
`1137`     | [`S_COFFGROUP`](s_coffgroup.md)              | module        | COFF group (subsection)
`1136`     | [`S_SECTION`](s_section.md)                  | module        | COFF section in a PE executable
`113C`     | [`S_COMPILE3`](s_compile.md)                 | module        | Compiler version and flags (extended)
`113E`     | [`S_LOCAL`](s_local.md)                      | module        | Local variable
`113F`     | [`S_DEFRANGE`](s_local.md)                   | module        | Define range for local variable
`1140`     | [`S_DEFRANGE_SUBFIELD`](s_local.md)          | module        | Define range with offset in parent variable
`1141`     | [`S_DEFRANGE_REGISTER`](s_local.md)          | module        | Define range for enregistered variable
`1142`     | [`S_DEFRANGE_FRAMEPOINTER_REL`](s_local.md)  | module        | Define range for frame-pointer relative variable
`1143`     | [`S_DEFRANGE_SUBFIELD_REGISTER`](s_local.md) | module        | Define range for sub-field register
`1144`     | [`S_DEFRANGE_FRAMEPOINTER_REL_FULL_SCOPE`](s_local.md) | module | Define range for frame-pointer relative variable (full scope)
`1145`     | [`S_DEFRANGE_REGISTER_REL`](s_local.md)      | module        | Define range for register-relative variable
`114C`     | [`S_BUILDINFO`](s_buildinfo.md)              | module        | Build info
`114D`     | [`S_INLINESITE`](s_inlinesite.md)            | module        | Inline site
`1159`     | [`S_ARMSWITCHTABLE`](s_armswitchtable.md)    | module        | ARM switch table (jump table)
`115C`     | [`S_INLINESITE2`](s_inlinesite.md)           | module        | Inline site (version 2) 

# Nested Symbol Scopes

Some symbol records are related to other symbol records, by arranging them
within nested scopes. These typically indicate a parent/child relationship of
some kind.

Nested symbols can only occur within module symbol streams. They cannot appear
in the global symbol stream. The following symbols start a new nested scope when
they appear within a symbol stream:

Symbol         | Top-level? | Nested?
---------------------|------|---------
`S_GPROC32`          | Yes  | No
`S_LPROC32`          | Yes  | No
`S_LPROC32_DPC_ID`   | Yes  | No
`S_GPROC32_ID`       | Yes  | No
`S_BLOCK32`          | No   | Yes
`S_THUNK32`          | Yes  | No
`S_INLINESITE`       | No   | Yes
`S_INLINESITE2`      | No   | Yes
`S_SEPCODE`          | Yes  | No
`S_GMANPROC`         | Yes  | No
`S_LMANPROC`         | Yes  | No

Nesting relationships imply a tree. The "top-level" column indicates whether a
specific symbol can appear as the root of a tree of nesting relationships. The
"nested" column indicates whether a specific symbol can be nested within another
symbol record.

The `S_END` symbol ends a symbol scope. The `S_END` symbol itself does not
contain any fields (its payload is zero-length).

When processing a symbol stream, when a decoder encounters a symbol that starts
a nesting scope, it should push a record onto a scope stack (or increment a
stack depth counter). When the processor encounters an S_END record, it should
pop a record from the scope stack (or decrement a counter). This allows the
processor to see the full symbol scope during traversal. The full symbol scope
is often relevant when interpreting a symbol.

# TypeIndex: Pointers into the Type Database (TPI)

Many symbol records contain `TypeIndex` values. `TypeIndex` is an alias for
`uint32_t`. `TypeIndex` values point into the Type Database (TPI Stream), or
identify primitive (intrinsic) types.

See [Types](../types/types.md).

# Global symbols

Global symbols are stored in the [Global Symbol Stream (GSS)](../../pdb/globals.md).

# Module symbols

Module symbols (symbols that describe code and data within a particular
module) are stored in [Module Streams](../../pdb/module_stream.md).

```

`docs/codeview/types/lf_arglist.md`:

```md
## `LF_ARGLIST` (0x1201) - Argument List

```c
struct ArgList {
    uint32_t arg_count;
    TypeIndex args[arg_count];
};
```

Specifies the arguments for `LF_PROCEDURE` or `LF_MFUNCTION`.

This record should only be pointed-to by `LF_PROCEDURE` and `LF_MFUNCTION`,
within the TPI stream.

> Invariant: Each `LF_ARGLIST` record is pointed-to by exactly one
> `LF_PROCEDURE` or `LF_MFUNCTION` record, and is pointed-to by the `arg_list`
> field.

Effectively, the `LF_PROCEDURE` (or `LF_MFUNCTION`) record "owns" the
`LF_ARGLIST`.
```

`docs/codeview/types/lf_array.md`:

```md
## `LF_ARRAY` (0x1503)

```c
struct Array {
    TypeIndex element_type;
    TypeIndex index_type;
    Number length;
    strz name;
};
```

Specifies an array type. The array type has a fixed size.

`element_type` is the type of the element, e.g. `int` for `int FOO[100]`.

`index_type` is the type of index used. TODO: what is this used for?

`length` is the size of the array.

`name` is the name of the array type. This is often an empty string.

```

`docs/codeview/types/lf_bitfield.md`:

```md
## `LF_BITFIELD` (0x1205) - Bit-field

Describes a bitfield within a structure.

```c
struct BitField {
    TypeIndex type;
    uint8_t length;
    uint8_t position;
};
```

`type` is the type of the field that contains the bitfield. For example,
`T_ULONG`.

`length` is the length of the bitfield, in bits.

`position` is the index of the lowest bit occupied by this bitfield.

```

`docs/codeview/types/lf_class.md`:

```md
# `LF_CLASS` (0x1504), `LF_STRUCTURE` (0x1505), and `LF_INTERFACE` (0x1519)

Describes a user-defined type defined with `class` (`LF_CLASS`), `struct`
(`LF_STRUCTURE`), or `interface` (`LF_INTERFACE`).

```c
struct Class {
    uint16_t count;             // total number of members defined (see below)
    uint16_t property;          // bit fields describing type
    TypeIndex fields;           // pointer to LF_FIELDLIST
    TypeIndex derivation_list;  // obsolete; should be zero
    TypeIndex vshape;           // pointer to LF_VTSHAPE (vtable shape), or zero if none
    Number length;              // length of instances in memory, in bytes
    strz name;                  // name of the type
    strz unique_name;           // present only if `hasuniquename` is set in `property`
};
```

`count` specifies the number of members defined on the class (or structure).
This includes base classes (if any), data members (static and non-static),
non-static data fields, member functions (static and non-static), friend
declarations, etc. The `count` should not be considered authoritative; it can be
used as an allocation hint, but it is possible for a class to have more than
65,535 fields.

`property` specifies a set of bit fields:

Name           | Bits  | Description
---------------|-------|------------
`packed`       | 0     | Structure is packed (has no alignment padding)
`ctor`         | 1     | Class has constructors and/or destructors
`overops`      | 2     | Class has overloaded operators
`nested`       | 3     | Class is a nested class (is nested within another class)
`cnested`      | 4     | Class contains other nested classes
`opassign`     | 5     | Class has overloaded assignment
`opcast`       | 6     | Class has casting methods
`fwdref`       | 7     | Class is a forward declaration (is incomplete; has no field list)
`scoped`       | 8     | This is a scoped definition
`hasuniquename`| 9     | true if there is a decorated name following the regular name
`sealed`       | 10    | true if class cannot be used as a base class
`hfa`          | 11-12 | See `CV_HFA`, below.
`intrinsic`    | 13    | true if class is an intrinsic type (e.g. `__m128d`)
`mocom`        | 14-15 | See `CV_MOCOM_UDT`, below.

The `hfa` bitfield is defined by this enum:

```c
enum CV_HFA {
    none = 0,
    float = 1,
    double = 2,
    other = 3,
};
```

HFA (Homogeneous Floating-point Aggregate) is a concept defined by ARM 
architectures. See [Overview of ARM64 ABI conventions](https://learn.microsoft.com/en-us/cpp/build/arm64-windows-abi-conventions?view=msvc-170).

The `CV_MOCOM_UDT` bitfield is defined by this enum:

```c
enum CV_MOCOM_UDT {
    none = 0,
    ref = 1,
    value = 2,
    interface = 3,
};
```

The `fields` field points to an `LF_FIELDLIST` or is 0 if there is no field
list. Each `LF_FIELDLIST` record contains a list of fields and optionally a
pointer to another `LF_FIELDLIST` if the fields cannot be stored within a single
`LF_FIELDLIST` record. This allows fields to be stored in a list-of-lists.

`derivation_list` is obsolete and unused and should always be set to zero.

`vshape` points to a virtual function table shape descriptor (`LF_VTSHAPE`), or
0 if there is none.

`length` specifies the size of instances of the class in memory, in bytes.

`name` is the name of this type.

## Unique Name

The `unique_name` field is an extension of the original record structure. If the
`hasuniquename` bit is set then the `unique_name` field is present and
immediately follows the `name` field. If `hasuniquename` is not set then the
`unique_name` field is not present _at all_; there is not even a NUL terminator
for `unique_name`. This is because `unique_name` was a later extension of the
specification.

```

`docs/codeview/types/lf_endprecomp.md`:

```md
# `LF_ENDPRECOMP` (0x0014) - End of Precompiled Types 

This record specifies that the preceding type records in this module can be
referenced by another module in the executable. A module that contains this type
record is considered to be the creator of the precompiled types. The subsection
index for the `.debug$T` segment for a precompiled types creator is emitted as
`.debug$P` instead of `.debug$T` so that the packing processing can pack the
precompiled types creators before the users.

Precompiled types must be emitted as the first type records within the
`.debug$T` segment and must be self-contained. That is, they cannot reference a
type record whose index is greater than or equal to the type index of the
`LF_ENDPRECOMP` type record.

```c
struct EndPrecompiledTypes {
    uint32_t signature;
};
```

`signature` is the signature of the precompiled types. The signatures in the
`S_OBJNAME` symbol record, the `LF_PRECOMP` type record and this signature must
match.

# Records referenced from other type records

These records define parts of types, but they do not define types. For example,
`LF_ARGLIST` specifies the argument list for a procedure, but the argument list
alone does not specify a procedure type.

None of the records described in this section should be pointed-to by records
outside of the type stream (TPI Stream).

```

`docs/codeview/types/lf_enum.md`:

```md
# `LF_ENUM` (0x1507)

```c
struct Enum {
    uint16_t count;
    uint16_t property;
    TypeIndex underlying_type;
    TypeIndex fields;
    strz name;
    strz unique_name;     // present only if `hasuniquename` is set in `property`
};
```

Defines an `enum` type. The `count`, `property`, `fields`, `name`, and
`unique_name` fields have the same meaning as the corresponding fields defined
on the `LF_CLASS` record.

The `underlying_type` field specifies the type the stores the values of the
enum. This should always be a primitive type, such as `T_ULONG`; it should not
point to another type record.

When C/C++ compilers emit this record, the field list will contain only
`LF_ENUMERATE` fields. However, the Rust compiler may generate `LF_FIELDLIST`
field lists that contain methods, even when generating field lists for `LF_ENUM`
types. This is because Rust supports methods associated with all types,
including enums, while C/C++ do not.

```

`docs/codeview/types/lf_fieldlist.md`:

```md
# `LF_FIELDLIST` (0x1203) - Field List

The `LF_FIELDLIST` record contains a list of fields defined on a type. The type
can be `LF_CLASS`, `LF_STRUCTURE`, `LF_INTERFACE`, `LF_UNION`, or `LF_ENUM`.

Each field within an `LF_FIELDLIST` record uses a leaf value to identify the
kind of field. However, the leaf values are disjoint from those used for type
records. Also, the field records do not use the same header as that used for
type records. The length of each field is not stored; it is implied by the leaf
value. For this reason, decoders must know how to decode all possible fields,
because if the decoder does not recognize a field then it cannot know the size
of the field.

This table summarizes the different field kinds. They use the same `LF_XXX`
naming convention, but these values are disjoint from the `LF_XXX` values used
for type records and should not be confused with them.

After the summary, each field is described in detail.

Value  | Name               | Description
-------|--------------------|-----------------------------------------------------------------
0x040b | `LF_FRIENDCLS`     | A friend class
0x1400 | `LF_BCLASS`        | A non-virtual (real) base class of a class.
0x1401 | `LF_VBCLASS`       | A directly-inherited virtual base class.
0x1402 | `LF_IVBCLASS`      | An indirectly-inherited virtual base class.
0x1404 | `LF_INDEX`         | An index to another `LF_FIELDLIST` which contains more fields
0x1409 | `LF_VFUNCTAB`      | A virtual function table pointer
0x140c | `LF_VFUNCOFF`      | A virtual function table pointer at a non-zero offset
0x1502 | `LF_ENUMERATE`     | An enumerator (named constant) defined on an `LF_ENUM` type
0x150c | `LF_FRIENDFCN`     | A friend function
0x150d | `LF_MEMBER`        | A non-static data member (field)
0x150e | `LF_STMEMBER`      | A static data member (static field)
0x150f | `LF_METHOD`        | A single method of a class, or a pointer to a `LF_METHODLIST`
0x1510 | `LF_NESTEDTYPE`    | A nested type
0x1511 | `LF_ONEMETHOD`     | A single non-overloaded method
0x1512 | `LF_NESTEDTYPEX`   | Nested type extended definition

## Field Attribute Structure

Many field types include an `attr` field that describes access protection and other properties. This is a 16-bit value with the following bit fields:

Bits  | Field        | Description
------|--------------|-------------------------------------------------------------
0-1   | `access`     | Access protection: 1=private, 2=protected, 3=public
2-4   | `mprop`      | Method properties (see below)
5     | `pseudo`     | Compiler-generated function that doesn't exist
6     | `noinherit`  | Class cannot be inherited
7     | `noconstruct`| Class cannot be constructed
8     | `compgenx`   | Compiler-generated function that does exist
9     | `sealed`     | Method cannot be overridden
10-15 | `unused`     | Unused bits

The `mprop` field (bits 2-4) encodes method properties:
- 0: Non-virtual method
- 1: Virtual method
- 2: Static method
- 3: Friend method
- 4: Introducing virtual method
- 5: Pure virtual method
- 6: Pure introducing virtual method

## Field Type Details

### `LF_BCLASS` (0x1400) - Base Class

```c
struct BClass {
    uint16_t attr;        // Field attributes
    TypeIndex type;       // Type index of the base class
    Number offset;        // Offset to base class subobject within derived class
};
```

Describes a non-virtual base class of a class. The `offset` specifies where the
base class subobject appears within instances of the derived class.

### `LF_VBCLASS` (0x1401) - Virtual Base Class (Direct)

```c
struct VBClass {
    uint16_t attr;        // Field attributes
    TypeIndex btype;      // Type of the virtual base class
    TypeIndex vbtype;     // Type of virtual base pointer
    Number vbpoff;        // Offset to virtual base pointer within derived class
    Number vboff;         // Offset within virtual base pointer table
};
```

Describes a virtual base class that is directly inherited by this class.

### `LF_IVBCLASS` (0x1402) - Virtual Base Class (Indirect)

```c
struct IVBClass {
    uint16_t attr;        // Field attributes
    TypeIndex btype;      // Type of the virtual base class
    TypeIndex vbtype;     // Type of virtual base pointer
    Number vbpoff;        // Offset to virtual base pointer within derived class
    Number vboff;         // Offset within virtual base pointer table
};
```

Describes a virtual base class that is indirectly inherited by this class
(inherited by a base class of this class).

### `LF_FRIENDFCN` (0x150c) - Friend Function

```c
struct FriendFcn {
    uint16_t padding;     // Padding (must be zero)
    TypeIndex type;       // Type of the friend function
    strz name;            // Name of the friend function
};
```

Describes a friend function declaration. Friend functions have access to private
and protected members of the class.

### `LF_INDEX` (0x1404) - Continuation Index

```c
struct Index {
    uint16_t padding;     // Padding (must be zero)
    TypeIndex index;      // Type index of another LF_FIELDLIST
};
```

Points to another `LF_FIELDLIST` record that contains additional fields. This
allows field lists to be chained together when they exceed the size limits of a
single record.

### `LF_MEMBER` (0x150d) - Data Member

```c
struct Member {
    uint16_t attr;        // Field attributes
    TypeIndex type;       // Type of the member
    Number offset;        // Offset within the containing type
    strz name;            // Name of the member
};
```

Describes a non-static data member of a class or structure. The `offset`
specifies where this member appears within instances of the containing type.

### `LF_STMEMBER` (0x150e) - Static Member

```c
struct STMember {
    uint16_t attr;        // Field attributes
    TypeIndex type;       // Type of the static member
    strz name;            // Name of the static member
};
```

Describes a static data member of a class or structure. Static members are
shared among all instances of the type and do not have an offset within
individual instances.

### `LF_METHOD` (0x150f) - Method List

```c
struct Method {
    uint16_t count;       // Number of overloads of this method
    TypeIndex mList;      // Type index of LF_METHODLIST containing the overloads
    strz name;            // Name of the method
};
```

Describes a method that may have multiple overloads. The `mList` field points to
an `LF_METHODLIST` record that contains information about each overload.

### `LF_ONEMETHOD` (0x1511) - Single Method

```c
struct OneMethod {
    uint16_t attr;        // Field attributes
    TypeIndex type;       // Type index of the method (LF_MFUNCTION or LF_PROCEDURE)
    uint32_t vbaseoff;    // Virtual base offset (present only if introduces virtual)
    strz name;            // Name of the method
};
```

Describes a single method without overloads. The `vbaseoff` field is present
only if the method introduces a new virtual function slot (when `mprop` is 4 or
6).

### `LF_NESTEDTYPE` (0x1510) - Nested Type

```c
struct NestedType {
    uint16_t padding;     // Padding (must be zero)
    TypeIndex type;       // Type index of the nested type
    strz name;            // Name of the nested type
};
```

Describes a type that is nested within another type. This is commonly used for
classes defined within other classes.

### `LF_NESTEDTYPEX` (0x1512) - Extended Nested Type

```c
struct NestedTypeEx {
    uint16_t attr;        // Field attributes
    TypeIndex type;       // Type index of the nested type
    strz name;            // Name of the nested type
};
```

An extended version of `LF_NESTEDTYPE` that includes attribute information.

### `LF_VFUNCTAB` (0x1409) - Virtual Function Table

```c
struct VFuncTab {
    uint16_t padding;     // Padding (must be zero)
    TypeIndex type;       // Type index of the virtual function table shape
};
```

Describes the virtual function table (vtable) for a class. The `type` field
points to an `LF_VFTSHAPE` record that describes the layout of the vtable.

### `LF_VFUNCOFF` (0x140c) - Virtual Function Offset

```c
struct VFuncOff {
    uint16_t padding;     // Padding (must be zero)
    TypeIndex type;       // Type index of the virtual function table type
    uint32_t offset;      // Offset within the virtual function table
};
```

Describes a virtual function table pointer that appears at a non-zero offset
within the containing type.

### `LF_ENUMERATE` (0x1502) - Enumerator

```c
struct Enumerate {
    uint16_t attr;        // Field attributes
    Number value;         // Value of this enumerator
    strz name;            // Name of this enumerator
};
```

Describes a named constant within an enumeration type. The `value` field
contains the integer value associated with this enumerator name.

### `LF_FRIENDCLS` (0x040b) - Friend Class

```c
struct FriendCls {
    uint16_t padding;     // Padding (must be zero)
    TypeIndex type;       // Type index of the friend class
};
```

Describes a friend class declaration. Friend classes have access to private and
protected members of the class that declares them as friends.

```

`docs/codeview/types/lf_methodlist.md`:

```md
# `LF_METHODLIST` (0x1206) - Method List

```c
struct MethodList {
    MethodEntry methods[];
};

struct MethodEntry {
    uint16_t attribute;
    uint16_t padding;
    TypeIndex type;
    uint32_t vtab_offset;     // This field is only present if 'attribute' introduces a new vtable slot
};
```

The `LF_METHODLIST` record contains a list of method overloads that share the same name. This record is referenced by [`LF_METHOD`](lf_fieldlist.md) fields within `LF_FIELDLIST` records to describe all the different overloads of a method.

## Structure

The record consists of a variable-length array of `MethodEntry` structures. Each entry describes one method overload with the same name but potentially different signatures, access levels, or virtual characteristics.

## Fields

### MethodEntry Fields

`attribute` is a 16-bit field containing method properties and access control. This follows the same bit field structure as other field attributes:

Bits  | Field        | Description
------|--------------|------------------------------------------------------------
0-1   | `access`     | Access protection: 1=private, 2=protected, 3=public
2-4   | `mprop`      | Method properties (see below)
5     | `pseudo`     | Compiler-generated function that doesn't exist
6     | `noinherit`  | Method cannot be inherited
7     | `noconstruct`| Method cannot be used for construction
8     | `compgenx`   | Compiler-generated function that does exist
9     | `sealed`     | Method cannot be overridden
10-15 | `unused`     | Unused bits

The `mprop` field (bits 2-4) encodes method properties:
- 0: Non-virtual method
- 1: Virtual method  
- 2: Static method
- 3: Friend method
- 4: Introducing virtual method (introduces new vtable slot)
- 5: Pure virtual method
- 6: Pure introducing virtual method

`padding` is a 16-bit padding field that must be zero.

`type` is a `TypeIndex` pointing to the method's type. This typically points to an `LF_MFUNCTION` record that describes the method's signature, calling convention, and parameter types.

`vtab_offset` is a 32-bit offset within the virtual function table. This field is **only present** when the method introduces a new virtual function slot (when `mprop` is 4 or 6). For non-virtual methods or virtual methods that override existing slots, this field is omitted entirely.

## Usage

`LF_METHODLIST` is used to group method overloads together. When a class has multiple methods with the same name but different signatures (overloaded methods), a single `LF_METHOD` field in the class's `LF_FIELDLIST` points to an `LF_METHODLIST` that contains all the overloads.

For example, a C++ class with multiple constructors:
```cpp
class MyClass {
public:
    MyClass();                    // Default constructor
    MyClass(int x);              // Int constructor  
    MyClass(const std::string& s); // String constructor
};
```

Would have a single `LF_METHOD` field named "MyClass" that points to an `LF_METHODLIST` containing three `MethodEntry` records, one for each constructor overload.

## Iteration

The Rust implementation provides an iterator interface for parsing method list entries:

```rust
let mut method_list = MethodList::parse(record_data)?;
while let Some(entry) = method_list.next()? {
    println!("Method type: {:?}, Access: {}", entry.ty, entry.attr & 0x3);
    if let Some(offset) = entry.vtab_offset {
        println!("  Virtual table offset: {}", offset);
    }
}
```

The iterator automatically handles the conditional presence of the `vtab_offset` field based on the method attributes.

## Virtual Method Handling

The presence of the `vtab_offset` field depends on whether the method introduces a new virtual function slot. This is determined by examining bits 2-4 of the `attribute` field:

- If `mprop` is 4 (introducing virtual) or 6 (pure introducing virtual), the `vtab_offset` field is present
- For all other method types, the `vtab_offset` field is absent

This conditional field presence is why decoders must check the attribute bits before attempting to read the virtual table offset.

```

`docs/codeview/types/lf_mfunction.md`:

```md
# `LF_MFUNCTION` (0x1009)

```c
struct MFunction {
    TypeIndex return_value_type;
    TypeIndex class;
    TypeIndex this;
    uint8_t calling_convention;
    uint8_t reserved;
    uint16_t num_params;
    TypeIndex arg_list;
    uint32_t this_adjust;
};
```

Defines the type of a non-static instance method. The `return_value_type`,
`calling_convention`, `num_params`, and `arg_list` values have the same meaning
as the fields with same name defined on `LF_PROCEDURE`.

The `this` field specifies the type of the implicit `this` argument. The
`arg_list` _does not_ include an argument for the implicit `this` argument. The
`num_params` field _does not_ count the implicit `this` field.

The `class` field specifies the type of the class that defines the method.

> TODO: clarify `this_adjust`

```

`docs/codeview/types/lf_modifier.md`:

```md
# `LF_MODIFIER` (0x1001)

Modifies another type record by qualifying it with `const`, `volatile`, or
`unaligned` modifiers.

For example, `const char *` would be described with an `LF_MODIFIER` type
(applied to the primitive type `T_CHAR`), and a `LF_POINTER` type that points to
the `LD_MODIFIER`.

```c
struct Modifier {
    TypeIndex index;
    uint16_t attribute;
};
```

`index` identifies the type that this type is based on.

`attribute` is a set of bit fields. If a bit is set to 1, then that qualifier
applies:

Field       | Bits      | Description
------------|-----------|------------
`const`     | 0         | `const` qualifier
`volatile`  | 1         | `volatile` qualifier
`unaligned` | 2         | `unaligned` qualifier
(reserved)  | 3-15      | reserved; must be zero

> **Determinism**: `LF_MODIFIER` records should not point to another
> `LF_MODIFIER` record. Instead, each `LF_MODIFIER` record should point directly
> to the unqualified type, and should contain the union of all qualifiers that
> are needed.

```

`docs/codeview/types/lf_pointer.md`:

```md
# `LF_POINTER` (0x1002)

Defines a type that is a pointer to another type. This is used for C-style
pointers, such as `FOO *`, as well as C++ references such as `FOO &`.

```c
struct Pointer {
    TypeIndex type;        // The type that is being pointed-to
    uint32_t attributes;   // Flags; see below
    // more data follows; structure depends on attributes
};
```

The `attribute` field contains several bit fields:

`attribute`<br>bit field | Bits  | Description
------------|---------|--
`ptrtype`   | 0-4     | Specifies the mode of the pointer; see below
`ptrmode`   | 5-7     | Specifies mode; see below
`is_flat32` | 8       | True if is a flat 32-bit pointer
`volatile`  | 9       | True if pointer (not the pointed-to) is volatile, e.g. `int * volatile x;`
`const`     | 10      | True if pointer (not the pointed-to) is const, e.g. `int * const x;`
`unaligned` | 11      | True if pointer (not the pointed-to) is unaligned, e.g. `int * unaligned x;`
`restrict`  | 12      | True if pointer is restricted (is not aliased)
`size`      | 13-18   | Size of pointer in bytes
`ismocom`   | 19      | True if it is a MoCOM pointer (`^` or `%`)
`islref`    | 20      | Ttrue if it is this pointer of member function with `&` ref-qualifier
`lsrref`    | 21      | True if it is this pointer of member function with `&&` ref-qualifier
(reserved)  | 22-31   | reserved

The `ptrtype` bit field can take these values:

Value | Name           | Description
------|----------------|---------------------------------------
0x00  | `NEAR`         | 16 bit pointer
0x01  | `FAR`          | 16:16 far pointer
0x02  | `HUGE`         | 16:16 huge pointer
0x03  | `BASE_SEG`     | based on segment
0x04  | `BASE_VAL`     | based on value of base
0x05  | `BASE_SEGVAL`  | based on segment value of base
0x06  | `BASE_ADDR`    | based on address of base
0x07  | `BASE_SEGADDR` | based on segment address of base
0x08  | `BASE_TYPE`    | based on type
0x09  | `BASE_SELF`    | based on self
0x0a  | `NEAR32`       | 32 bit pointer
0x0b  | `FAR32`        | 16:32 pointer
0x0c  | `64`           | 64 bit pointer
0x0d  | `UNUSEDPTR`    | first unused pointer type

The `ptrmode` bit field can take these values:

Value | Name      | Description
------|-----------|---------------------------------------
0x00  | `PTR`     | "normal" pointer, e.g. `FOO *`
0x01  | `REF`     | "old" reference, e.g. `FOO &`
0x01  | `LVREF`   | l-value reference, e.g. `FOO &`
0x02  | `PMEM`    | pointer to data member
0x03  | `PMFUNC`  | pointer to member function
0x04  | `RVREF`   | r-value reference, e.g. `FOO &&`

The data after the `Pointer` structure depends on `ptrtype`, and is called the "variant data".

### Variant data for pointer to type

If the pointer is based on a type (`ptrtype == CV_PTR_BASE_TYPE`), then the
variant data consists of a single `TypeIndex`.

### Variant data for pointer to data member

If the pointer is a pointer to a data member, then the variant data has this
structure:

```c
struct PointerToDataMemberVariant {
    TypeIndex class;        // The pointed-to class
    uint16_t format;
};
```

where `format` has one of these values:

`format` | Description
---------|------------
0        | 16:16 data for class with no virtual functions or virtual bases.
1        | 16:16 data for class with virtual functions.
2        | 16:16 data for class with virtual bases.
3        | 16:32 data for classes w/wo virtual functions and no virtual bases
4        | 16:32 data for class with virtual bases.
5        | 16:16 near method nonvirtual bases with single address point
6        | 16:16 near method nonvirtual bases with multiple address points
7        | 16:16 near method with virtual bases
8        | 16:16 far method nonvirtual bases with single address point
9        | 16:16 far method nonvirtual bases with multiple address points
10       | 16:16 far method with virtual bases
11       | 16:32 method nonvirtual bases with single address point
12       | 16:32 method nonvirtual bases with multiple address points
13       | 16:32 method with virtual bases

The pointer to data member and pointer to method have the following formats in
memory. In the following descriptions of the format and value of the NULL
pointer, `*` means any value.

> TODO: convert these; they are quite complicated

```

`docs/codeview/types/lf_precomp.md`:

```md
# `LF_PRECOMP` (0x1509)

This type is only present in compiler PDBs and debug information embedded in
object files. It is not present in linker PDBs.

```c
struct Precomp {
    uint32_t start;
    uint32_t count;
    uint32_t signature;
    strz name;
};
```

This record specifies that the type records are included from the precompiled
types contained in another module in the executable. A module that contains this
type record is considered to be a user of the precompiled types. When emitting
to a COFF object the section name must be `.debug$P` rather than `.debug$T`. All
other attributes should be the same.

`start` is the starting type index that is included. This number must correspond
to the current type index in the current module.

`count` is the count of the number of type indices included. After including the
precompiled types, the type index must be `start + count`.

`signature` is the signature for the precompiled types being referenced by this
module. The signature will be checked against the signature in the `S_OBJNAME`
symbol record and the `LF_ENDPRECOMP` type record contained in the `.debug$T`
table of the creator of the precompiled types. The signature check is used to
detect recompilation of the supplier of the precompiled types without
recompilation of all of the users of the precompiled types. The method for
computing the signature is unspecified. It should be sufficiently robust to
detect failures to recompile.

`name` is the full path name of the module containing the precompiled types.
This name must match the module name in the `S_OBJNAME` symbol emitted by the
compiler for the object file containing the precompiled types.

```

`docs/codeview/types/lf_procedure.md`:

```md
# `LF_PROCEDURE` (0x1008)

Defines the type of a function or of a static method. Instance methods use the
`LF_MFUNCTION` record.

```c
struct Procedure {
    TypeIndex return_value_type;
    uint8_t calling_convention;
    uint8_t reserved;
    uint16_t num_params;
    TypeIndex arg_list;
};
```

`return_value_type` specifies the return value type, e.g. `T_VOID` for `void`.

`calling_convention` specifies the calling convention of the procedure. See
[`CV_call`](../calling_convention.md).

`arg_list` points to an [`LF_ARGLIST`](./lf_arglist.md) record, which describes the arguments
of the procedure type.

```

`docs/codeview/types/lf_skip.md`:

```md
# `LF_SKIP` (0x1200)

This record reserve space within a type stream but does not contain any
information. The payload for this record can be non-empty but its contents are
ignored.

This record should never be referenced by any other record.

```

`docs/codeview/types/lf_union.md`:

```md
# `LF_UNION` (0x1506)

```c
struct Union {
    uint16_t count;
    uint16_t property;
    TypeIndex fields;
    Number length;
    strz name;
    strz unique_name;     // present only if `hasuniquename` is set in `property`
};
```

Defines a `union` type. All fields of this record have the same meaning as the
corresponding fields defined on the `LF_CLASS` record.

The `LF_UNION` record is similar to `LF_CLASS`, but does not support inheritance
(derivation) or virtual functions. The `property` bit fields are the same as
those used for `LF_CLASS`.

```

`docs/codeview/types/lf_vtshape.md`:

```md
# `LF_VTSHAPE` (0x000a)

Defines the format of a virtual function table.

```c
struct VTShape {
    uint16_t count;
    // An array of 4-bit descriptors follow, whose size is given by 'count'
};
```

This record is accessed by the `vfunctabptr` in the member list of the class
which introduces the virtual function. The `vfunctabptr` is defined either by
the `LF_VFUNCTAB` or `LF_VFUNCOFF` member record. If `LF_VFUNCTAB` record is
used, then `vfunctabptr` is at the address point of the class. If `LF_VFUNCOFF`
record is used, then `vfunctabptr` is at the specified offset from the class
address point. The underlying type of the pointer is a `VTShape` type record.
This record describes how to interpret the memory at the location pointed to by
the virtual function table pointer.

`count` specifies the number of descriptors. Each value in `descriptor`
describes an entry in the virtual function table. Each descriptor is 4 bits and
can take one of the following values:

Value       | Description
------------|------------
0           | Near
1           | Far
2           | Thin
3           | address point displacement to outermost class. This is at `entry[-1]` from table address.
4           | far pointer to metaclass descriptor. This is at `entry[-2]` from table address.
5           | Near32
6           | Far32
7-15        | reserved

```

`docs/codeview/types/types.md`:

```md
# CodeView Type Records

This file describes the type records used by CodeView.

Type records are variable-length records. Each record begins with a 4-byte
header which specifies the length and the "kind" of the record.

```c
struct TypeRecord {
   uint16_t size;
   uint16_t kind;
   uint8_t payload[size - 2];
};
```

The `size` field specifies the size in bytes of the record. The `size` field
_does not_ count the `size` field itself, but it _does_ count the `kind` field
and the payload bytes.

> Invariant: The `size` field is a multiple of 2 and is greater than or equal to
> 2.

Type records are aligned at 2-byte boundaries. Unfortunately, many type records
contain fields that have 4-byte alignment, such as `uint32_t`. Encoders and
decoders must handle misaligned access to those fields, either using unaligned
memory accesses or must copy the entire record to a buffer that has a guaranteed
alignment.

The `kind` field specifies how to interpret a type record. In the PDB
documentation, this `kind` field uses the "Leaf Type" enumeration.

# Summary of type records

There are several disjoint categories of record kinds:

* `type` category: Records that are pointed-to by other parts of the PDB file,
  such as symbol records. These are the "top-level" type records. These records
  define a complete type, which is what allows them to be used by symbol
  records.

* `internal` category: Records that are pointed-to by other type records, but
  which are _not_ pointed-to by any symbol record outside of the TPI stream.
  This is why these records are called "internal"; these records (along with the
  records that point to them) describe complex hierarchical structures. Only the
  "external" records, such as `LF_CLASS` or `LF_ENUM`, are directly pointed-to
  by symbol records.

  These internal records are not, by themselves, complete records. They only
  have meaning in the context of the hierarchy of records that they occur
  within. For example, `LF_METHODLIST` gives a list of methods, but must be
  associated with (pointed-to by) a record such as `LF_CLASS`.

* `special` records, such as `LF_PRECOMP`, which are produced by the compiler
  and are read (and consumed) by the linker.

There are also data structures which use `LF_*` naming prefixes and the same
numbering system as type records, but which are embedded within `LF_FIELDLIST`
records. These "field records" **are not** type records. They **do not** use the
`TypeRecord` layout and cannot be found by simply enumerating the type records
in the TPI. See [`LF_FIELDLIST`](./lf_fieldlist.md).

Kind   | Name                                     | Category | Description
-------|------------------------------------------|----------|------------
0x000a | [`LF_VTSHAPE`](lf_vtshape.md)            | internal | Shape of a virtual function table
0x0014 | [`LF_ENDPRECOMP`](lf_endprecomp.md)      | special  | Specifies end of precompiled types
0x1001 | [`LF_MODIFIER`](lf_modifier.md)          | type     | Modifies a type by applying `volatile`, `const`, or `unaligned` to it
0x1002 | [`LF_POINTER`](lf_pointer.md)            | type     | Defines a pointer to another type, e.g. `FOO*`
0x1008 | [`LF_PROCEDURE`](lf_procedure.md)        | type     | A function signature type
0x1009 | [`LF_MFUNCTION`](lf_mfunction.md)        | type     | A member function signature type
0x1502 | [`LF_ARRAY`](lf_array.md)                | type     | A fixed-size array, e.g. `char FOO[100]`
0x1504 | [`LF_CLASS`](lf_class.md)                | type     | A `class` definition
0x1505 | [`LF_STRUCTURE`](lf_class.md)            | type     | A `struct` definition
0x1506 | [`LF_UNION`](lf_union.md)                | type     | A `union` definition
0x1507 | [`LF_ENUM`](lf_enum.md)                  | type     | An `enum` definition
0x1509 | [`LF_PRECOMP`](lf_precomp.md)            | special  | Specifies types come from precompiled module
0x1200 | [`LF_SKIP`](lf_skip.md)                  | internal | Reserves space in the type stream, but contains no information
0x1201 | [`LF_ARGLIST`](lf_arglist.md)            | internal | Specifies arguments for `LF_PROCEDURE` or `LF_MFUNCTION`
0x1203 | [`LF_FIELDLIST`](lf_fieldlist.md)        | internal | Contains field records for `LF_CLASS`, `LF_STRUCTURE`, `LF_ENUM`, etc.
0x1205 | [`LF_BITFIELDS`](lf_bitfield.md)         | internal | Specifies a bitfield within another field
0x1206 | [`LF_METHODLIST`](lf_methodlist.md)      | internal | Specifies a list of methods in an overload group (methods that have the same name but differing signatures)

# Summary of item records

# Leaf indices referenced from symbols

These are the top-level type record kinds. These records can be pointed-to by
symbol records, or by other top-level type records. They define types.

```

`docs/data_types.md`:

```md
# Data Structures and Types

This document defines many data structures. This section describes the notation
used for those data structures.

Structures are defined using pseudo-C code, using the `struct { ... }` syntax.
Unlike C, structures defined in this document may be variable-length and may
contain variable-length fields. Variable-length fields of structures may be
arrays of fixed-size structures, arrays of variable-size structures,
NUL-terminated strings, the Number type, or a reference to another
variable-length structure defined in this document.

# Range syntax

Numeric ranges are specified using `start-end`, where both `start` and `end` are
inclusive bounds. For example, "bits 3-5" means "bits 3, 4, and 5".

# Fixed-size Integers

This document uses the C/C++ integer types defined in `<stdint.h>`. All integer
values are in LSB-first (little-endian) encoding, unless specified otherwise.

These are the integer types used in this document:

Name        | Usage
------------|------
`uint8_t`   | (obvious)
`uint16_t`  |
`uint32_t`  |
`uint64_t`  |
`int8_t`    |
`int16_t`   |
`int32_t`   |
`int64_t`   |
`char8_t`   | 8-bit value used for encoding code units of UTF-8 strings.

# GUID

The well-known `GUID` type is imported from Windows. It is a 16-byte quantity.

# Embedded arrays

Variable-length arrays may be directly embedded within a structure. It is not
the case that a fixed amount of storage is reserved. Instead, the elements are
directly embedded within the structure, followed by the next field (if any). The
number of elements in the array is usually given as an expression within the
array’s square brackets.

Example:

```
struct Garden {
  uint32_t num_apples;
  Apple apples[num_apples];
};
```

If the last field of a structure is an array field, and the number of elements
is implied by the length of the entire encoded structure, then the length may be
omitted. Example:

```
struct MoreGarden {
  uint32_t num_apples;
  Apple apples[num_apples];
  Tomato tomatoes[];   // the rest of the storage is tomatoes
};
```

It is not legal to have another field follow an array with implicit length,
unless the meaning is clarified by context. The following is an example of an
illegal structure definition:

```
struct BadDefinition {
  uint32_t numbers[];
  uint32_t x;
};
```

# Strings: strz

The data type `strz` represents a NUL-terminated string, using UTF-8 encoding.
For example:

```
struct Foo {
    uint32_t month;   // fixed-length header
    uint32_t day;
    uint32_t year;

    strz flavor;      // variable-length fields
    strz build;
}
```

This syntax means that the string data is directly stored in the structure, and
that the structure is variable-length. It is not the case that a fixed number of
characters are allocated for the string; instead, character data is written,
followed by a NUL (a zero byte), followed by the next field (if any).

# The `Number` type

The `Number` type represents a numeric constant. Numbers may have different
sizes and representations. Each Number value is at least 2 bytes in size. These
first 2 bytes specify either an immediate value or specify the encoding to use
for the entire value.

See: [Numbers](codeview/number.md)

## Example of using `Number`:

```
struct NamedConstant {
    strz name;
    Number value;
}
```

Both of the fields of `NamedConstant` are variable-length.

# `TypeIndex`: Pointer into TPI Stream

`TypeIndex` is an alias for `uint32_t`, with a specific interpretation. Each
`TypeIndex` either refers to a record within the Type Database (TPI Stream),
which is described later in this document, or refers to a primitive (well-known
/ intrinsic) type. See the Type Database section for more information.

# `ItemId`: Pointer into IPI Stream

`ItemId` is an alias for `uint32_t`, with a specific interpretation. Each
`ItemId` refers to a record within the IPI Stream, or to `NIL` (no record). The
value of `NIL` is zero. `ItemId` counts records, not bytes.

See [`ItemId`](pdb/ipi_stream.md#itemid).

# `NameIndex`

 `NameIndex` is an alias for `uint32_t` which points into the [Names
 Stream](pdb/names_stream.md).

```

`docs/determinism.md`:

```md
# Determinism and Normalization 

There are many benefits to determinism (aka reproducible builds).

> See: [Reproducible builds on Wikipedia](https://en.wikipedia.org/wiki/Reproducible_builds).

When a compiler produces an executable (assuming the compiler is deterministic),
the corresponding PDB should also be deterministic.

Throughout this document, as the file structures are described, many sections
will also specify what rules could be used to impose a deterministic order on a
given file structure. For example, a table might contain records that could
occur in any order. A determinism requirement would state one arbitrary sorting
order that could be applied to the table. The sorting order does not affect the
semantics of the information, but it does remove a degree of freedom from its
encoding.

These determinism requirements will be called out in each section that describes
a file structure. Again, determinism requirements are stronger than invariants.
For example, let’s assume we are dealing with a hash table, where each hash
record contains a hash code and a reference count field. Our correctness
requirement only governs the ordering of the records with respect to their hash
codes:

> Invariant: The `hash_records` array must be sorted in increasing order by hash
> code.

Unfortunately, this leaves us with an unwanted degree of freedom because two
consecutive hash records could have the same hash code but have different
reference counts. We specify this as a determinism requirement:

> Determinism: The `hash_records` array must also be sorted in increasing order
> by reference count.

```

`docs/index.md`:

```md
# PDB and CodeView

This document describes the PDB file format. It describes the structure of the
PDBs with sufficient detail to read, create, and modify PDBs for some purposes.
It describes the relationships between different sections of the PDB. It also
describes sources of non-determinism, and a set of normalization rules that may
be applied to PDBs to reach determinism.

This document is a **non-authoritative** decriptions of PDB, CodeView, and the
data structures used by them. It is the synthesis of public information drawn from
many sources.

## Contents

- [Introduction](intro.md)
- [Terminology](terminology.md) - Lists terms used within this specification.
- [Data Types](data_types.md) - Data types that are used within this specification.
- [PDB](pdb/pdb.md) - Describes Program Database (PDB) files.
- [CodeView](codeview/codeview.md) - Describes debugging types and symbol records.
- [Determinism](./determinism.md) - Discusses deterministic (reproducible) builds.
- [References](references.md)

## Maintainers

* Arlie Davis - ardavis@microsoft.com

```

`docs/intro.md`:

```md
# Introduction

## Publicly-sourced information

All of the information described in this document comes from existing documents
and source code published by Microsoft. This document does not describe any
trade secrets or confidential intellectual property of Microsoft; all of it has
been made public by Microsoft in other forms.

## Goals and Scope

The PDB file format is an essential part of development workflows for Microsoft
and the developer community that uses Microsoft platforms. Unfortunately,
documentation for PDB is inconsistent and sparse. This document is intended to
describe the structure of PDBs, clearly and unambiguously, to a degree that it
can serve as a primary reference for implementation of tools that work with
PDBs.

This document should be sufficient for developers to implement _some_
PDB-related tools without reference to other documents or sources.
Unfortunately, that level of detail will not be feasible for some of the debug
records, but nonetheless the aim of this document is to be as comprehensive as
possible.

These goals are in scope for this document:

* Describing the PDB/MSF container file format, including how streams are stored
  on disk, how the 2-phase commit protocol works, how the Free Page Map works,
  and how to correctly create or modify a PDB.
* Describing the well-known streams in the PDB. Each stream should be described
  at a level of detail sufficient for implementing a decoder or encoder, without
  reference to other implementations.
* Describing (or referencing) the set of CodeView type and symbol records that
  are produced by current-generation MSVC and LLVM tools (circa 2023). LLVM is
  explicitly in-scope because Microsoft makes extensive use of LLVM-based
  compilers (Clang and Rust) and these compilers must be able to interoperate
  with MSVC when describing debug information.
* Describing the records that contain compiler flags (CFLAGS).
* Describing how NatVis files are embedded within PDBs.
* Describing how SourceLink information is embedded within PDBs.
* Describing the Module Information streams.

The following are not within the scope of this document:

* CodeView records for older versions of the MSVC toolset, such as 16-bit CPUs,
  IA64, MIPS, etc.
* "Portable PDBs" are out of scope. Portable PDBs use a very different file
  format, which is based on the .NET platform’s CIL metadata.
* "Compiler PDBs" (PDBs produced by the MSVC compiler) are out of scope. This
  document focuses only on "linker PDBs".

# Invariants

When possible, this document specifies invariants for file structures that it
specifies. For example, if a table of values must be sorted in increasing order,
then this will be called out:

> Invariant: The `foo_entries` table must be sorted in increasing order, using
> the `bar_offset` field.

For a PDB file to be well-formed (valid) all invariants must be met. If an
invariant is conditional then the invariant will give the condition that governs
it. For example:

> Invariant: If the `foo_stream` header is using the Paired Encoding, then the
> number of items in the `foo_items` table must be a multiple of 2.

Some invariants apply only to specific versions of the PDB file format, or to
specific versions of streams, substreams, etc. These version dependencies will
be explicitly called out in invariants.

```

`docs/pdb/dbi_fixups.md`:

```md
# DBI Subsection: Fixups

The DBI stream may contain an Optional Debug Substream containing fixups.
See [dbi_sections.md] for locating this substream.

This substream contains fixup records described by this structure:

```
struct Fixup {
    uint16_t fixup_type;
    uint16_t fixup_extra;
    uint32_t rva;
    uint32_t rva_target;
}
```

# References

See `pdbdump.cpp` sources.


```

`docs/pdb/dbi_modules.md`:

```md
# DBI Modules Substream

The DBI Modules Substream lists the modules of the executable. A module is a
compiland / translation unit, such as an *.OBJ file or a *.RES file (produced by
the Resource Compiler). The DBI Modules Substream is very important; many PDB
data structures refer to it.

The DBI Modules Substream is a sequence of contiguous bytes within the 
[DBI Stream](dbi_stream.md). The DBI Modules Substream begins immediately after the DBI Stream Header, which has a fixed size of 64 bytes, so the DBI Modules
Substream always begins at stream offset 64. The size of the DBI Modules
Substream is specified by the `DbiStreamHeader.module_info_size` field.

> Invariant: `DbiStreamHeader.module_info_size` is non-negative and is a
> multiple of 4.

The DBI Modules Substream contains a sequence of DBI Module Info records. These
records have a fixed-size header and a variable-length tail. The size of DBI
Module Info record is a multiple of 4, and records always start at a byte offset
(within the DBI Modules Substream) that is a multiple of 4.

```c
// variable-length
struct ModuleInfo {
    uint32_t old_module_index;
    SectionContrib section_contrib;
    uint16_t flags;
    uint16_t stream;
    uint32_t sym_byte_size;
    uint32_t c11_byte_size;
    uint32_t c13_byte_size;
    uint16_t source_file_count;
    uint16_t padding;
    uint32_t unused2;
    uint32_t source_file_name_index;
    uint32_t pdb_file_path_name_index;
    // end of fixed-length header (size = 64)

    // variable-length fields:
    strz module_name;
    strz obj_file;
    uint8_t alignment_padding[];        // 0-3 bytes of alignment padding
};
```

> Invariant: Within the DBI Module Substream, each `ModuleInfo` record starts at
> a byte offset that is a multiple of 4.

> Invariant: The size of each `ModuleInfo` record, including the variable-length
> fields and the `alignment_padding`, is a multiple of 4.

## Module Index

The order of the `ModuleInfo` records determines the _module index_. Module
indices are zero-based; the first `ModuleInfo` record is module index 0. Many
PDB data structures contain a module index field, which points to a specific
module within the DBI Modules Substream. In some cases, PDB data structures
contain arrays of records which have the same number and order as the modules in
the DBI Modules Substream. These relationships must be preserved when making
changes to a PDB, such as changing the order of `ModuleInfo` records.

The `old_module_index` field is deprecated. Some encoders (such as the MSVC
linker) set this value to module index, but some encoders set it to
unpredictable values or to zero. For this reason, the `old_module_index`
field should be considered obsolete; encoders should always set it to the
module index (implied by the order of a given `ModuleInfo` structure in the
array of all modules); decoders should ignore the `old_module_index` field.

> Determinism: Set `old_module_index` to the implied value of the module index.

## First section contribution

The `section_contrib` field describes the contributions of this module to the
executable. See [`SectionContrib`](dbi_sections.md).

> Invariant: `section_contrib` contains a `module_index` field, which must be
> equal to the module index, or have the value 0xffff.

> Determinism: `section_contrib.module_index` must be updated if the DBI Modules
> Substream is sorted.

## Flags

`flags` specifies several bit fields:

Name         | Bits  | Description
-------------|-------|------------
`written`    | 0     | True if this module has been written since DBI opened
`ec_enabled` | 1     | True if this module has Edit-and-Continue symbolic information
(unused)     | 2-7   |
`tsm_index`  | 8-15  | Index into TSM list for this module's server

> Determinism: Set the `written` bit to 0.

`source_file_count` specifies the number of source files used when compiling
this module.

> Invariant: The value of `source_file_count` is equal to the value of
> `module_file_counts[m]` for the corresponding entry in the
> [DBI Sources Substream](dbi_sources.md).

## Edit-and-Continue Fields

The `source_file_name_index` and `pdb_file_path_name_index` values are
used by the MSVC Edit-and-Continue feature. This document does not specify
the meaning or usage of these fields.

## Module Stream

The `stream` field is the stream index of this module's Module Stream, if the
module has one. If a Module does not have a Module Stream then this field will
be set to 0xFFFF. See [Module Stream](module_stream.md) for details on the
contents of this stream. The `sym_byte_size`, `c11_byte_size`, and
`c13_byte_size` fields describe the size of substreams within the Module Stream;
these values are necessary for correctly interpreting the module stream.

> Invariant: No two `ModuleInfo` records should have the same value for `stream`
> (unless the value is 0xffff).

> Invariant: `sym_byte_size` is a multiple of 4.

> Invariant: `c11_byte_size` is a multiple of 4.

> Invariant: `c13_byte_size` is a multiple of 4.

> Invariant: `c11_byte_size` and `c13_byte_size` are not both non-zero.

> Invariant: If `stream` is 0xffff, then `sym_byte_size` is equal to zero,
> `c11_byte_size` is equal to zero, and `c13_byte_size` is equal to zero.

> Invariant: The sum of `sym_byte_size`, `c11_byte_size`, and `c13_byte_size` is
> less than or equal to the size of the module stream.

The `source_file_count` field specifies the number of source files that were
read by the compiler when compiling this module. The DBI Sources Substream also
specifies the number of source files for each module; these values are required
to be equal in any well-formed PDB.

The meaning of the `unused2` field is unclear. We see some PDBs with non-zero
values, but they appear to be meaningless, and may be simply uninitialized data
being written to disk. Encoders should set this field to zero.

> Determinism: Set `unused2` to zero.

## Module Name and Object File Path

The `module_name` and `obj_file` fields are both NUL-terminated UTF-8 strings.
They immediately follow the fixed-length portion of the Module Info record.

The `module_name` and `obj_file` fields, taken together, uniquely identify this
module in a way that is meaningful outside of the PDB (i.e. not simply an index
in an array). Because an executable can be composed of different kinds of
modules, these fields can be used in different ways.

In general, the `obj_file` field specifies the path of the file that was
submitted to the linker. This could be an object file (`*.obj`), a static
library (`*.lib`), a DLL import lib (`*.lib`), a compiled Windows resource file
(`*.res`), etc. The `module_name` field specifies a string that uniquely
identifies that module within the scope of the file it was read from (the
`obj_file`). For example, if a module was linked in from a static library, then
`module_name` is the original object file, such as `printf.obj`, at the time
that the static library was created (not when the linker was run for the current
executable).

## Examples

`module_name`         | `obj_file`     | Description
----------------------|----------------|------------
`printf.obj`          | `msvcrtd.lib`  | Module pulled from static library
`my_app.obj`          | `my_app.obj`   | OBJ file submitted directly to linker
`version.res`         | `version.res`  | Compiled resource file (RC) submitted directly to linker
`Import:KERNEL32.dll` | `kernel32.lib` | DLL import entries pulled from a DLL import library

## Alignment

The `ModuleInfo` record is variable-length, due to the string fields that follow
the fixed-length header. The `ModuleInfo` record must be padded to an alignment
of 4. This must be done not only so that fields fall on their natural alignment,
but also so that the Module Info record decoder can correctly locate the start
of the next record.

The `alignment_padding` pseudo-field exists to pad the size of the entire
`ModuleInfo` to a size that is a multiple of 4. Decoders should compute the
number of alignment padding bytes based on the size of the variable-length
string fields. Similarly, encoders *must* insert padding bytes so that the
complete `ModuleInfo` record has a size that is a multiple of 4. This is
required even for the last `ModuleInfo` record in the DBI Modules Substream
because the DBI Modules Substream is required to have a size that is a multiple
of 4.

> Determinism: If padding bytes are present, use a fixed value, such as zero.

> Invariant: The stream offset of the DBI Modules Substream is a multiple of 4.
> This is trivially met because the DBI Modules Substream is located after the
> DBI Stream Header, whose size is a multiple of 4.

## Determinism

> Determinism: `ModuleInfo` records should be sorted by the tuple of
> (`module_name`, `obj_file`). These are the only identifiers that are
> meaningful outside of the contents of the PDB file itself. The tuple should
> always be unique.
>
> After sorting the list of modules, many PDB data structures will need to be
> updated. The `module_index` field within the `section_contrib` field will need
> to be updated.

> Determinism: Set the alignment padding bytes (if any) to zero.

# References

* [`struct MODI_60_Persist`](https://github.com/microsoft/microsoft-pdb/blob/master/PDB/dbi/dbi.h)

```

`docs/pdb/dbi_opt_debug.md`:

```md
# DBI Optional Debug Header Substream

The DBI Optional Debug Header Substream contains a list of stream indexes. Each
stream, if present, contains a copy of a data structure from the corresponding
executable.

```c
struct OptionalDebugHeaderSubstream {
    uint16_t streams[];
};
```

The size of the Optional Debug Header Substream is determined by the number of
stream indexes stored in it. The number of elements in `streams` is implied by
the size of the DBI Optional Debug Header Substream; simply divide the size
of the `OptionalDebugHeaderSubstream` by `sizeof(uint16_t)` to compute the
number of streams.

The order of the items in `streams` is significant. The index of each entry is
described in this list:

Index | Name                            | Description
------|---------------------------------|------------
0     | `FPO_DATA`                      | Frame-pointer omission data
1     | `EXCEPTION_DATA`                | Contains a debug data directory of type `IMAGE_DEBUG_TYPE_EXCEPTION`.
2     | `FIXUP_DATA`                    | Contains a debug data directory of type `IMAGE_DEBUG_TYPE_FIXUP`.
3     | `OMAP_TO_SRC_DATA`              | Contains a debug data directory of type `IMAGE_DEBUG_TYPE_OMAP_TO_SRC`. This is used for mapping addresses from instrumented code to uninstrumented code.
4     | `OMAP_FROM_SRC_DATA`            | Contains a debug data directory of type `IMAGE_DEBUG_TYPE_OMAP_FROM_SRC`. This is used for mapping addresses from uninstrumented code to instrumented code.
5     | `SECTION_HEADER_DATA`           | A dump of all section headers from the original executable.
6     | `TOKEN_TO_RECORD_ID_MAP`        | 
7     | `XDATA`                         | Exception handler data
8     | `PDATA`                         | Procedure data
9     | `NEW_FPO_DATA`                  |
10    | `ORIGINAL_SECTION_HEADER_DATA`  |

The `streams` array may contain stream indices whose value is 0xffff. If an
entry in this list has the value 0xffff, then it means that the stream is not
present. This is necessary because the `streams` array may be sparse. For
example, if a PDB does not contain an `EXCEPTION_DATA` stream but _does_ contain
a `SECTION_HEADER_DATA` stream, then the `EXCEPTION_DATA` entry should be set to
0xffff.

This document does not specify the contents of the optional debug headers. Some
of them are publicly documented while others are not.

## Alignment

It is not clear if there is an alignment requirement for the starting offset of
the DBI Optional Debug Header Substream or for its length. To be on the safe
side, encoders should pad this substream to an alignment of 4, by adding a
stream index of 0xFFFF if necessary.

## Determinism

The contents of the streams appear to be deterministic, since they are copies of
data structures found in the executable. We assume the executable is
deterministic.

Because stream indexes are not deterministic it is necessary to copy streams in
a deterministic order. It is recommended to use the order of the streams as
found in this table.

## References

* [DBGTYPE](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/langapi/include/pdb.h#L438)
* [DBI1::fGetDbgData](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/PDB/dbi/dbi.cpp#L3860)


```

`docs/pdb/dbi_sections.md`:

```md
# DBI Sections

The DBI Section Map Substream and DBI Section Contributions Substream together
form the DBI Sections data structure. DBI Sections describes _contributions_,
which are fragments of object files that contribute to the executable.

# DBI Section Map Substream

The DBI Section Map Substream describes the number of physical segments and
logical segments within a binary and provides an array that describes these
segments.

```
// sizeof = dynamic
struct SectionMap {
    uint16_t num_segments;
    uint16_t num_logical_segments;
    SectionMapEntry entries[];
}

// sizeof = 20
struct SectionMapEntry {
    uint16_t flags;
    uint16_t overlay;
    uint16_t group;
    uint16_t frame;
    uint16_t section_name;
    uint16_t class_name;
    uint32_t offset;
    uint32_t section_length;
}
```

The Section Map Substream begins with a small fixed-size header, which specifies
the number of physical segments and logical segments. The remainder of the
section map is an array of `SectionMapEntry` records.

The `section_name` and `class_name` fields are byte offsets into a name table,
but it is not clear what that name table is. In all observed PDBs, these fields
are 0xFFFF.

## Alignment

> Invariant: The stream offset of the DBI Section Map is a multiple of 4.

The stream offset of the DBI Section Map is computed by
`module_info_size + section_contributions_size` and both `module_info_size` and
`section_contributions_size` also have an invariants that they are multiples of
4, so this invariant should always be met.

> The size of the DBI Section Map (`section_map_size`) is a multiple of 4.

## Determinism

The DBI Section Map Substream appears to already be deterministic. Nothing needs
to be done.

There do not appear to be pointers from the DBI Section Map into other tables.

Many other tables (such as the DBI Section Contributions Substream and DBI
Modules Substream) refer to sections by index, and so they point into the DBI
Section Map. However, since we do not need to reorder the section table, we do
not need to update section indexes in other tables.

# DBI Section Contributions Substream

The DBI Section Contributions Substream lists the contributions (code and data)
of each module to the executable. It is one of the largest substreams of the DBI
Stream. Its size typically accounts for 50% to 70% of the size of the DBI
Stream.

The DBI Section Contributions Substream consists of a fixed-size header and an
array of fixed-size "contribution" structures which describe the section
contributions. The definition and size of the contribution structures can vary,
depending on the version field in the header.

> Invariant: The stream offset of the DBI Section Contributions Substream is a
> multiple of 4.

> Invariant: The size of the DBI Section Contributions Substream is a multiple
> of 4. The size may be 0, indicating an empty substream.

```c
struct SectionContribSubstreamHeader {
    uint32_t version;
};
```

`version` specifies the version of this structure:

Name            | Value expression       | Value
----------------|------------------------|------------
`DBISCImpvV60`  | 0xeffe_0000 + 19970605 | 0xf12e_ba2d
`DBISCImpv2`    | 0xeffe_0000 + 20140516 | 0xf131_51e4

## Version `DBISCImpvV60`

All observed PDBs have used the `DBISCImpvV60` version. It uses `SectionContrib` for the records that follow `SectionContribSubstreamHeader`:

```c
struct SectionContribSubstream {
    uint32_t version;
    SectionContrib contribs[];      // when version == DBISCImpvV60
};

// sizeof = 28
struct SectionContrib {
    uint16_t section;
    uint16_t padding1;
    int32_t offset;
    int32_t size;
    uint32_t characteristics;
    uint16_t module_index;
    uint16_t padding2;
    uint32_t data_crc;
    uint32_t reloc_crc;
};
```

## Version `DBISCImpV2`

This version appears not to be used; no PDBs have been found that use
`DBISCImpV2`. It uses `SectionContrib2` for the records that follow
`SectionContribSubstreamHeader`:

```c
struct SectionContribSubstream {
    uint32_t version;
    SectionContrib2 contribs[];     // when version == DBISCImpvV2
};

// sizeof = 32
struct SectionContrib2 {
    uint16_t section;
    uint16_t padding1;
    int32_t offset;
    int32_t size;
    uint32_t characteristics;
    uint16_t module_index;
    uint16_t padding2;
    uint32_t data_crc;
    uint32_t reloc_crc;
    uint32_t coff_section;      // new field
}
```

## Contribution records

Clearly, most of the fields in `SectionContrib` and `SectionContrib2` are
shared; only the additional `coff_section` field is different.

> Invariant: The `contribs` array is sorted by `(section, offset)`, in
> increasing order.

This invariant allows binary searching of the contributions. Contributions
_should_ be unique; no two records should have the same `(section, offset)`
values.

> Determinism: Set `padding1` and `padding2` to zero.

`module_index` is the zero-based index of the module for this section
contribution. If `SectionContrib` is embedded in a `ModuleInfo` record (in the
DBI Modules Substream), and the module does not have any section contributions,
then the `module_index` field can be 0xffff. If `SectionContrib` is embedded in
the DBI Section Contributions Substream, then the `module_index` cannot be
0xffff, and must be a valid zero-based module index.

## Determinism

The records in the Section Contributions Substream are already sorted and the
sorting key is stable (is determined by information in the corresponding
executable). Therefore, we do not need to sort the records.

```

`docs/pdb/dbi_sources.md`:

```md

# DBI Sources Substream

The DBI Sources Substream provides a list of the source files that were read
when compiling each module. Because many executables contain modules that read
from the same source files (such as header files), the DBI Sources substream
only lists each unique source file once. It stores a list of file names that
were read in each compiland, using offsets into a string table to avoid storing
the file names more than once.

The size and location of the DBI Sources Substream is specified in the DBI
Stream Header. The size is specified explicitly; its location is computed by
summing the sizes of the substreams that precede it.

> Invariant: The starting byte offset of the DBI Sources Substream is a multiple
> of 4. The starting byte offset is computed from
> `module_info_size + section_contributions_size + section_map_size`, from the
> DBI Stream Header. Each of those sections has a corresponding invariant that
> those sizes are multiplies of 4.

> Invariant: The size of the DBI Sources Substream (`source_info_size`) is a
> multiple of 4.

The DBI Sources Substream has this structure:

```c
struct DbiSourcesSubstream {
   uint16_t num_modules;
   uint16_t num_sources;                        // obsolete; do not read
   uint16_t module_file_starts[num_modules];
   uint16_t module_file_counts[num_modules];
   uint32_t file_offsets[num_file_offsets];
   uint8_t names_buffer[];
   uint8_t alignment_padding[];                 // indistinguishable from `names_buffer`
};
```

`num_modules` specifies the number of modules in this substream. The order of
module records in the [DBI Modules Substream](dbi_modules.md) corresponds to the
order of module records in the DBI Sources Substream.

> Invariant: `num_modules` is equal to the number of `ModuleInfo` records in the
> [DBI Modules Substream](dbi_modules.md).

The size of the `file_offsets` array should be computed by summing the values in
the `module_file_counts` array. Let `num_file_offsets` be the sum of all values
in `module_file_counts`. This is not the number of _unique_ file names; it is
the length of the `file_offsets` array. `file_offsets` will usually contain
duplicates because different modules will often include the same header files.

The `num_sources` field is **obsolete** and **must be ignored** when reading the
DBI Sources Substream. In the past, it specified the number of entries in the
`file_offsets` array. However, because the field only has 16 bits of precision,
and many PDBs exist that link together modules that read from more than 2^16
source files, it can no longer be used as an accurate count of the number of
source files in the stream. Encoders should set `num_sources` to the truncated
value (the lower 16 bits) of the length of `num_file_offsets`.

The `module_file_starts` and `module_file_counts` arrays, taken together,
specify the range of values within `file_offsets` that correspond to each
module. For a module `m`, `module_file_starts[m]` is the index within
`file_offsets` of the first file in the list of files for `m` and
`module_file_counts[m]` specifies the number of files for `m`. In Rust notation,
`file_offsets[module_file_starts[m] .. module_file_starts[m] + module_file_counts[m]]`
is the slice of `file_offsets` for `m`.

> Invariant:
> `module_file_starts[m] + module_file_counts[m] <= num_file_offsets` for all
> `m` in `0..num_modules`.

> Guideline: Every entry in `file_offsets` is covered by exactly one module.
> There are no unused gaps and no entries covered by more than one module.

The following determinism requirements state this more directly. They ensure
that every entry in `file_offsets` is used by exactly one module, and that the
order of the per-module ranges within `file_offsets` follows the same order as
the modules themselves.

> Determinism: `module_file_starts[0] == 0`.

> Determinism:
> `module_file_starts[i] = module_file_starts[i - 1] + module_file_counts[i - 1]`
> for all `i > 0`.

`file_offsets` contains offsets into the names_buffer array. The values are
relative to the start of `names_buffer`; e.g. a value of zero references the
first character in `names_buffer`. The values stored in `file_name_offsets` are
organized as a set contiguous sequences, where each sequence corresponds to one
module and the length of that sequence is specified in module_file_counts.

`names_buffer` contains the character data for the file names. Each string is
UTF-8 and is NUL-terminated. Values in the `file_name_offsets` array point to
the start of strings in `names_buffer`.

In all observed PDBs the values in `file_name_offsets` always point to the start
of a string, never to the middle of a string. That is, `file_name_offsets[i]` is
either 0, or `names_buffer[file_name_offsets[i] – 1] == 0`. PDB writers should
adhere to this requirement, but it is not stated as an invariant. PDB readers
should be prepared for decoding DBI Sources Substreams where file name offsets
point into the middle of strings.

Decoders _should not_ assume that strings within `names_buffer` have no gaps
between them. An encoder could place strings at any offset within
`names_buffer`, as long as `file_offsets` points to valid offsets within
`names_buffer`. This is implied by the fact that there is alignment padding at
the end of `DbiSourcesSubstream` and that nothing indicates where `names_buffer`
ends and the alignment padding begins. Decoders should only decode strings that
correspond to an entry in `file_offsets`.

## Example

This is an example of the start of a DBI Sources Substream. In this example, the
DBI Sources Substream begins at stream offset 0x1ee4c0, which is aligned to our
16-byte rows, so all of the data shown is from the DBI Sources Substream.

```
001ee4c0 : 15 09 2f d0 00 00 01 00 06 00 63 00 bf 00 c2 00 : ../.......c.....
001ee4d0 : 14 01 14 01 18 01 71 01 cc 01 27 02 8a 02 3a 03 : ......q...'...:.
001ee4e0 : 9a 03 fa 03 a8 04 56 05 b8 05 18 06 b2 06 73 07 : ......V.......s.
001ee4f0 : 16 08 e6 08 8f 09 8f 09 38 0a d3 0a d3 0a 86 0b : ........8.......
001ee500 : 8a 0b e9 0b 48 0c 9c 0c 42 0d e8 0d 8e 0e 54 0f : ....H...B.....T.
001ee510 : fc 0f a4 10 4c 11 f4 11 9c 12 44 13 ec 13 94 14 : ....L.....D.....
001ee520 : 3c 15 e4 15 8c 16 32 17 d8 17 7e 18 24 19 ca 19 : <.....2...~.$...
001ee530 : 70 1a 16 1b bc 1b 62 1c 08 1d ae 1d 54 1e 1b 1f : p.....b.....T...
001ee540 : dd 1f 9f 20 47 21 ef 21 97 22 3f 23 e7 23 8f 24 : ... G!.!."?#.#.$
001ee550 : 37 25 df 25 87 26 2f 27 d7 27 7f 28 27 29 cf 29 : 7%.%.&/'.'.(').)
001ee560 : 77 2a 1e 2b c5 2b 6c 2c 12 2d c0 2d 6e 2e 1c 2f : w*.+.+l,.-.-n../
001ee570 : cb 2f 7a 30 29 31 d8 31 87 32 36 33 e5 33 90 34 : ./z0)1.1.263.3.4
001ee580 : 3b 35 e6 35 8e 36 50 37 12 38 d4 38 96 39 58 3a : ;5.5.6P7.8.8.9X:
001ee590 : 1a 3b ef 3b c4 3c 99 3d 6e 3e 43 3f e9 3f 90 40 : .;.;.<.=n>C?.?.@
```
 
In this example, `num_modules` is 0x915 (2325) and `num_sources` is 53295
(0xD02F). However, we cannot trust the `num_sources` value, as we will see below
when we recompute it from `module_file_counts`.

Using the `num_modules` value (0x915), we can find the offsets of the
`module_file_counts`, `module_file_starts`, and `file_name_offsets` fields:

Field                | Stream offset | Size expression                       | Size value
---------------------|---------------|---------------------------------------|-----------
`num_modules`        | 0x001ee4c0    | `sizeof(uint16_t)`                    | 2
`num_sources`        | 0x001ee4c2    | `sizeof(uint16_t)`                    | 2
`module_file_starts` | 0x001ee4c4    | `sizeof(uint16_t) * num_modules`      | 0x122a
`module_file_counts` | 0x001ef6ee    | `sizeof(uint16_t) * num_modules`      | 0x122a
`file_name_offsets`  | 0x001f0918    | `sizeof(uint32_t) * num_file_offsets` | unknown
`names_buffer`       | unknown       | consumes rest of record               | implicit

Because we do not yet know the value of `num_file_offsets` we cannot yet compute
the location of `names_buffer`.

Using the stream offsets that we just computed, we can read the first few
entries of the `module_file_counts` and `module_file_starts` arrays.

At stream offset 0x1ee4c4 (which is visible in the hex dump above), we see the
beginning of the `module_file_starts` array. At stream offset 0x1ef6ee we see
the beginning of the `module_file_counts` array:

```
001ef6e0 :                                           01 00 : -.-.-.-.-.-.-...
001ef6f0 : 05 00 5d 00 5c 00 03 00 52 00 00 00 04 00 59 00 : ..].\...R.....Y.
001ef700 : 5b 00 5b 00 63 00 b0 00 60 00 60 00 ae 00 ae 00 : [.[.c...`.`.....
001ef710 : 62 00 60 00 9a 00 c1 00 a3 00 d0 00 a9 00 00 00 : b.`.............
001ef720 : a9 00 9b 00 00 00 b3 00 04 00 5f 00 5f 00 54 00 : .........._._.T.
001ef730 : a6 00 a6 00 a6 00 c6 00 a8 00 a8 00 a8 00 a8 00 : ................
001ef740 : a8 00 a8 00 a8 00 a8 00 a8 00 a8 00 a8 00 a6 00 : ................
001ef750 : a6 00 a6 00 a6 00 a6 00 a6 00 a6 00 a6 00 a6 00 : ................
001ef760 : a6 00 a6 00 a6 00 c7 00 c2 00 c2 00 a8 00 a8 00 : ................
001ef770 : a8 00 a8 00 a8 00 a8 00 a8 00 a8 00 a8 00 a8 00 : ................
001ef780 : a8 00 a8 00 a8 00 a8 00 a8 00 a7 00 a7 00 a7 00 : ................
```

`module_file_starts`<br>stream offset | `module_file_starts[i]`<br>value | `module_file_counts`<br>stream offset | `module_file_counts[i]`<br>value
-----------|--------|------------|-------
0x001ee4c4 | 0x0000 | 0x001ef6ee | 0x0001
0x001ee4c6 | 0x0001 | 0x001ef6f0 | 0x0005
0x001ee4c8 | 0x0006 | 0x001ef6f2 | 0x005d
0x001ee4ca | 0x0063 | 0x001ef6f4 | 0x005c
0x001ee4cc | 0x00bf | 0x001ef6f6 | 0x0003
0x001ee4ce | 0x00c2 | 0x001ef6f8 | 0x0052
0x001ee4d0 | 0x0114 | 0x001ef6fa | 0x0000
0x001ee4d2 | 0x0114 | 0x001ef6fc | 0x0004
0x001ee4d4 | 0x0118 | 0x001ef6fe | 0x0059

Note that each `module_file_starts[i + 1]` can be computed from
`module_file_starts[i] + module_file_counts[i]`. This is because the
`module_file_starts` values point into the `file_name_offsets` array, and these
values are "packed". There is no overlap between the files for different modules
and the slice of values for each module have been appended in the same order as
the modules themselves.

Now that we have found the `module_file_counts` array, we scan it and compute
the sum of all values in it. Let `num_file_offsets` be the sum of all values in
`module_file_counts`. In our example, that value is 0x4d02f (315,439). This
allows us to find the remaining offsets of our fields:

Field                | Stream offset | Size expression                       | Size value
---------------------|---------------|---------------------------------------|-----------
`num_modules`        | 0x001ee4c0    | `sizeof(uint16_t)`                    | 2
`num_sources`        | 0x001ee4c2    | `sizeof(uint16_t)`                    | 2
`module_file_starts` | 0x001ee4c4    | `sizeof(uint16_t) * num_modules`      | 0x122a
`module_file_counts` | 0x001ef6ee    | `sizeof(uint16_t) * num_modules`      | 0x122a
`file_name_offsets`  | 0x001f0918    | `sizeof(uint32_t) * num_file_offsets` | 0x1340bc
`names_buffer`       | 0x003249d4    | consumes rest of record               | implicit

At 0x1f0918 we find the beginning of `file_name_offsets`:

```
001f0910 :                         4a a7 02 00 29 c5 02 00 : ........J...)...
001f0920 : 57 c5 02 00 8f c5 02 00 b7 c5 02 00 fd 92 02 00 : W...............
001f0930 : 15 1b 00 00 3a 1b 00 00 66 5e 00 00 94 1c 00 00 : ....:...f^......
001f0940 : b7 1c 00 00 db 1c 00 00 5e 1d 00 00 d8 1d 00 00 : ........^.......
001f0950 : ea 1e 00 00 1b 1a 00 00 00 00 00 00 72 1f 00 00 : ............r...
001f0960 : 35 1a 00 00 23 00 00 00 53 5c 00 00 8b 5e 00 00 : 5...#...S\...^..
001f0970 : 77 1a 00 00 f8 1f 00 00 ab 29 00 00 82 13 00 00 : w........)......
```

At 0x3249d4 we find the beginning of `names_buffer`:

```
003249d0 :             6f 6e 65 63 6f 72 65 5c 69 6e 74 65 : )...onecore\inte
003249e0 : 72 6e 61 6c 5c 73 64 6b 5c 69 6e 63 5c 77 61 72 : rnal\sdk\inc\war
003249f0 : 6e 69 6e 67 2e 68 00 6f 6e 65 63 6f 72 65 5c 69 : ning.h.onecore\i
00324a00 : 6e 74 65 72 6e 61 6c 5c 73 64 6b 5c 69 6e 63 5c : nternal\sdk\inc\
00324a10 : 73 75 70 70 72 65 73 73 5f 78 2e 68 00 6d 69 6e : suppress_x.h.min
00324a20 : 6b 65 72 6e 65 6c 5c 6e 74 6f 73 5c 72 74 6c 5c : kernel\ntos\rtl\
00324a30 : 68 6f 74 70 61 74 63 68 68 65 6c 70 65 72 5c 67 : hotpatchhelper\g
00324a40 : 6c 6f 62 61 6c 73 2e 63 00 6f 6e 65 63 6f 72 65 : lobals.c.onecore
00324a50 : 5c 69 6e 74 65 72 6e 61 6c 5c 73 64 6b 5c 69 6e : \internal\sdk\in
00324a60 : 63 5c 4d 69 6e 57 69 6e 5c 77 6f 77 36 34 74 6c : c\MinWin\wow64tl
00324a70 : 73 2e 68 00 6f 6e 65 63 6f 72 65 5c 65 78 74 65 : s.h.onecore\exte
00324a80 : 72 6e 61 6c 5c 73 64 6b 5c 69 6e 63 5c 4d 69 6e : rnal\sdk\inc\Min
00324a90 : 57 69 6e 5c 6c 69 62 6c 6f 61 64 65 72 61 70 69 : Win\libloaderapi
00324aa0 : 2e 68 00 4f 6e 65 43 6f 72 65 5c 49 6e 74 65 72 : .h.OneCore\Inter
```

Decoding the first few values from `file_name_offsets` and using the value to look up a string in `names_buffer` gives us this:

`i`    | `file_name_offsets[i]`  | Stream offset<br>of string | String
-------|-------------------------|----------------------------|-------
0      | 0x0002a74a              | 0x0034f11e                 | `d:\os\obj\amd64fre\mincore\kernelbase\daytona\objfre\amd64\kernelbase.def`
1      | 0x0002c529              | 0x00350efd                 | `minkernel\tools\gs_support\amd64\amdsecgs.asm`
2      | 0x0002c557              | 0x00350f2b                 | `OneCore\Private\MinWin\Priv_Sdk\Inc\gs\crt_amdsecgs.asm`
3      | 0x0002c58f              | 0x00350f63                 | `onecore\external\shared\inc\ksamd64.inc`
4      | 0x0002c5b7              | 0x00350f8b                 | `onecore\external\shared\inc\kxamd64.inc`

## Invariants

> Invariant: In the DbiStreamHeader, the fields which give the size of
> substreams must never be negative. If a substream is empty, its length should
> be zero.

> Invariant: The Sources substream must begin on a 4-byte aligned boundary, and
> its length must be a multiple of 4.

> Invariant: The size of the entire DBI stream should be equal to the sum of the
> size of the DbiStreamHeader and all of the substreams.

> Limit: The number of modules is limited to 65,534 (0xfffe). The value 0xffff
> is not available because it is used to mean "no module" in some data
> structures.

> Limit: The number of unique source files for a given module is limited to
> 65,535 (0xffff).

## Determinism

> Determinism: The strings in `names_buffer` are sorted and unique, using
> case-sensitive rules.

> Determinism: Every string in `names_buffer` is referenced by at least one
> entry in `file_name_offsets`.

> Determinism: There are no gaps (unused bytes) between strings within
> `names_buffer`.

> Determinism: Every value in `file_name_offsets` points to the start of a
> string, not the middle of a string.

> Determinism: For a given sequence of values in `file_name_offsets` that
> correspond to a single module M, the order of the file name offsets should be
> deterministic. Since the strings are required to be sorted, the simplest
> deterministic order would be to sort the file name offsets for M (which is the
> same as sorting them by the strings they refer to).

> Determinism: The modules themselves should be sorted, but this order is
> provided by the DBI Modules Substream.

> Determinism: `num_modules` is set to the low 16 bits of `num_file_offsets`.

> Determinism: Alignment padding bytes are set to zero, if present.

```

`docs/pdb/dbi_stream.md`:

```md
# Debug Information Stream (DBI) (Fixed Stream 3)
 
The Debug Information Stream (DBI) contains many important fields and substreams. It is a central data structure for PDBs.

## DBI Stream Layout

```
DBI Stream (Stream 3)
│
├─ DBI Stream Header (64 bytes)
│  ├─ signature, version, age
│  ├─ global_symbol_index_stream
│  ├─ public_symbol_index_stream
│  ├─ global_symbol_stream
│  └─ substream sizes (module_info_size, section_contributions_size, etc.)
│
├─ Modules Substream [module_info_size]
│  ├─ Module Info 1
│  ├─ Module Info 2
│  └─ Module Info N
│
├─ Section Contributions Substream [section_contributions_size]
│  ├─ Section Contribution 1
│  ├─ Section Contribution 2
│  └─ ...
│
├─ Section Map Substream [section_map_size]
│  ├─ Section Map Header
│  └─ Section Map Entries
│
├─ Sources Substream [source_info_size]
│  ├─ Source Info Header
│  ├─ Module Source Info
│  └─ Source File Names
│
├─ Type Server Map Substream [type_server_map_size] (obsolete)
│
├─ Edit-and-Continue Substream [edit_and_continue_size]
│
└─ Optional Debug Header Substream [optional_debug_header_size]
   ├─ FPO Data Stream Index
   ├─ Exception Data Stream Index
   ├─ Fixup Data Stream Index
   ├─ OMap To Source Stream Index
   ├─ OMap From Source Stream Index
   ├─ Section Header Data Stream Index
   ├─ Token RID Map Stream Index
   ├─ X Data Stream Index
   └─ P Data Stream Index
```

## DBI Stream Header

The DBI Stream begins with the DBI Stream Header:

```c
// sizeof = 64
struct DbiStreamHeader {
    int32_t signature;
    uint32_t version;
    uint32_t age;
    uint16_t global_symbol_index_stream;
    uint16_t build_number;
    uint16_t public_symbol_index_stream;
    uint16_t pdb_dll_version;
    uint16_t global_symbol_stream;
    uint16_t pdb_dll_rbld;
    int32_t module_info_size;                 // size of the Modules Substream
    int32_t section_contributions_size;       // size of the Section Contributions Substream
    int32_t section_map_size;                 // size of the Section Map Substream
    int32_t source_info_size;                 // size of the Sources Substream
    int32_t type_server_map_size;             // size of the Type Server Map Substream
    uint32_t mfc_type_server_index;
    int32_t optional_debug_header_size;       // size of the Optional Debug Header Substream
    int32_t edit_and_continue_size;           // size of the Edit-and-Continue Substream
    uint16_t flags;
    uint16_t machine;
    uint32_t padding;
};
```

## Signature and version

The signature field is always -1 (0xFFFF_FFFF). The version field specifies
which version of the DBI stream is being used:

Value (decimal) | Identifier   | Description
----------------|--------------|------------
930803          | `DBIImpvV41` | MSVC 4.1
19960307        | `DBIImpvV50` | MSVC 5.0
19970606        | `DBIImpvV60` | MSVC 6.0
19990903        | `DBIImpvV70` | MSVC 7.0
20091201        | `DBIImpV110` | MSVC 11.0 (current)

This specification covers only version MSVC 11.0+ (`DBIImpV110`).

## Global symbols

The DBI Stream Header contains three fields that point to streams which are
relevant to the Global Symbols Stream.

* The `global_symbol_stream` field is the stream index of the Global Symbol
  Stream (GSS). It contains a sequence of symbol records.

* The `public_symbol_index_stream` field is the stream index of the Public
  Symbol Index (PSI), which contains lookup tables that accelerate finding
  S_PUB32 symbols. It does not directly contain symbol records; instead, it
  contains pointers (byte offsets) that point into the Global Symbol Stream
  (GSS).

* The `global_symbol_index_stream` field is the stream index of the Global
  Symbol Index (GSI), which contains lookup tables that accelerate finding
  global symbols, including `S_CONSTANT`, `S_PROCREF`, and many more. This
  stream does not directly contain symbol records; instead, it contains pointers
  (byte offsets) that point into the Global Symbol Stream (GSS).

For a description of the GSS, GSI, and PSI, see [Global Symbols](globals.md).

## Substreams

The DBI Stream contains several substreams. The size of each substream is
specified in the `DbiStreamHeader`. The byte offset within the DBI Stream of
each substream is given by adding the size of the previous substream to the
start of the previous substream. The byte offset of the first substream (the DBI
Modules Substream) is fixed at 64; it immediately follows the `DbiStreamHeader`.

> Invariant: `module_info_size` is non-negative and is a multiple of 4.

> Invariant: `section_contributions_size` is non-negative and is a multiple of 4.

> Invariant: `section_map_size` is non-negative and is a multiple of 4.

> Invariant: `source_info_size` is non-negative and is a multiple of 4.

> Invariant: The sum of the size of `DbiStreamHeader` and the values of
> substream size fields (`module_info_size`, `section_contributions_size`,
> `section_map_size`, `source_info_size`, `type_server_map_size`,
> `optional_debug_header_size`, and `edit_and_continue_size`) is less than or
> equal to the size of the DBI Stream.

The above invariants imply that the starting byte offset of each these
substreams is a multiple of 4:

* Modules
* Section Contributions
* Section Map
* Sources
* Type Server Map

```c
struct DbiStream {
    DbiStreamHeader header;
    uint8_t modules               [module_info_size];
    uint8_t section_contributions [section_contributions_size];
    uint8_t section_map           [section_map_size];
    uint8_t source_info           [source_info_size];
    uint8_t type_server_map       [type_server_map_size];
    uint8_t edit_and_continue     [edit_and_continue_size];
    uint8_t optional_debug_header [optional_debug_header_size];
}
```

> Encoders should guarantee that _all_ DBI Substreams have a size that is a
> multiple of 4, even the `edit_and_continue` and `optional_debug_header`
> substreams. It is believed that these substreams also have alignment
> requirements.

The DBI Substreams immediately follow the DBI stream header. Note that the order
of the substreams is similar but not identical to the order of the corresponding
"size" fields. The last two substreams, `edit_and_continue` and
`optional_debug_header_size`, have different orders for the "size" fields and
the corresponding substream.

> NOTE: Pay careful attention to the order of the `edit_and_continue` and
> `optional_debug_header` substreams! The order of the _size_ fields is
> different from the order of the substream contents.

The byte offset of each substream is found by adding the size and offset of the
preceding substream. The byte offset of the first substream (the Module Info
Substream) is fixed at 64.

The size of the DBI Stream should be equal to the sum of the size of the DBI
Stream Header and the sizes of the substreams. However, it is legal for the sum
of the sizes of the substreams to be less than the size of the DBI Stream.
Decoders should ignore any extra data beyond the end of the last substream.
Encoders should not write any information beyond the last substream.

There are no gaps between the substreams. Some of the DBI substreams have
alignment requirements. These are specified in each of the following sections.

Each substream is described in its own section in this file, or in separate
files.

* [DBI Modules](dbi_modules.md) - Describes the DBI Modules Substream.
* [DBI Sources](dbi_sources.md) - Describes the DBI Sources Substream.
* [DBI Sections](dbi_sections.md) - Describes the DBI Section Map Substream and
  DBI Section Contributions Substream.
* [DBI Optional Debug Headers](dbi_opt_debug.md) - Describes the DBI Optional
  Debug Headers Substream.

# Determinism

Most of the issues concerning determinism are described in each of the DBI
Substreams sections. The DBI Header itself has only a few requirements:

> Determinism: Assign stream indexes in a deterministic order.

This applies to all stream indexes that are listed in the DBI and its
substreams. The exact stream index does not matter; all that matters is that the
stream index is chosen deterministically.

> Determinism: Set the `padding` field in the DBI Stream Header to zero.

# DBI Type Server Map Substream

The DBI Type Server Map Substream is not specified and is likely obsolete.
Decoders should ignore it. Encoders should set its size to zero.

# DBI Edit-and-Continue Substream

The DBI Edit-and-Continue Substream is not specified. Decoders should ignore it.
Encoders should set its size to zero.

```

`docs/pdb/globals.md`:

```md
- [Global Symbols: GSS, GSI, and PSI](#global-symbols-gss-gsi-and-psi)
- [Global Symbol Stream (GSS)](#global-symbol-stream-gss)
  - [Reference Symbols](#reference-symbols)
- [Symbol Name Table (used in GSI and PSI)](#symbol-name-table-used-in-gsi-and-psi)
  - [Hash Records](#hash-records)
  - [Hash Buckets](#hash-buckets)
- [Example Name Table Header](#example-name-table-header)
- [Hash Records](#hash-records-1)
- [Example of hash records](#example-of-hash-records)
- [Hash Buckets](#hash-buckets-1)
- [Building the Name Table](#building-the-name-table)
- [Loading the Name Table](#loading-the-name-table)
- [Querying the Name Table](#querying-the-name-table)
- [Global Symbol Index (GSI) Stream](#global-symbol-index-gsi-stream)
- [Public Symbol Index (PSI) Stream](#public-symbol-index-psi-stream)
  - [Address Table](#address-table)
  - [Querying the PSI Address Table](#querying-the-psi-address-table)
  - [Pointer relationships](#pointer-relationships)

# Global Symbols: GSS, GSI, and PSI

Each PDB contains a set of global symbols. Three streams contain the global
symbols stream and indexed views of the global symbols:

* The Global Symbol Stream (GSS) contains the global symbol records. These are
  encoded using the CodeView symbol stream format, which is described in
  [Symbols](../codeview/symbols/symbols.md).

* The Global Symbol Index (GSI) contains a name-to-symbol lookup table for a
  certain set global symbol kinds; that set will be described below. The GSI
  does not directly contain symbol records; it only contains pointers to them.

* The Public Symbol Index (PSI) contains a name-to-symbol lookup table for
  `S_PUB32` symbols. It also contains an address-to-symbol lookup table. Similar
  to the GSI, the PSI does not directly contain symbol records.

The stream indexes for the GSS, GSI, and PSI are specified in the DBI Stream
Header.

The GSI and PSI use the same format for the name-to-symbol lookup table. It is
specified below.

The GSI and PSI are functions of the GSS; that is, the entire contents of the
GSI and PSI can be reconstructed from the GSS. The GSI and PSI aid in query
performance.

# Global Symbol Stream (GSS)

The Global Symbol Stream contains a sequence of variable-length symbol records.
This stream does not have a header; all of the stream data consists of CodeView
symbol records. See [Symbols](../codeview/symbols/symbols.md) for the framing of the records and the
description of their structure.

> Invariant: The size of the Global Symbol Stream is a multiple of 4.

The Global Symbol Stream may only contain records in the following list. All
other symbol records are not permitted in the GSS.

Record Kind<br> Value (hex) | Record Kind Name | Description
-------|--------------------|---
0x1107 | `S_CONSTANT`       | Global named constants and enum values
0x1108 | `S_UDT`            | User-defined types, such as structs, classes, unions, enums, and type aliases (`using`).
0x110C | `S_LDATA32`        | Local data declarations with global scope.
0x110D | `S_GDATA32`        | Global variables
0x110E | `S_PUB32`          | Public symbols
0x1112 | `S_LTHREAD32`      | Module-private variables with thread-local storage
0x1113 | `S_GTHREAD32`      | Global variables with thread-local storage
0x111d | `S_GMANDATA`       | Global managed variables (MSIL)
0x1125 | `S_PROCREF`        | A reference to a global procedure symbol
0x1127 | `S_LPROCREF`       | A reference to a local procedure symbol
0x1128 | `S_ANNOTATIONREF`  | A reference to an `S_ANNOTATION` symbol
0x1129 | `S_TOKENREF`       | A reference to a managed token (MSIL)

> Encoders should only encode symbols that are listed in this table.

> Decoders should ignore any symbols that are not listed in this table.

## Reference Symbols

Some symbols in the GSS are references to symbols stored in module streams.
These are the reference symbols:

* `S_PROCREF` - Refers to `S_GPROC32` in a module stream
* `S_LPROCREF` - Refers to `S_LPROC32` in a module stream
* `S_DATAREF` - Refers to `S_LDATA32` or `S_GDATA32` in a module stream.
  However, we have never observed `S_DATAREF` symbols in PDBs, so this symbol
  may be obsolete.
* `S_ANNOTATIONREF` - Refers to `S_ANNOTATION` in a module stream
* `S_TOKENREF` - Refers to one of several symbol records that identify MSIL
  tokens

These symbols use the [`RefSym2`](../codeview/symbols/s_refsyms.md) record
definition.

These records allow an application to find a symbol definition using the name of
the symbol, such as a procedure. First, the application scans the records in the
GSS and finds an `S_PROCREF` or `S_LPROCREF` structure, using the `name` field
stored in those records for comparison. If a match is found, then the record
gives the module index and byte offset within the module's symbol stream. This
search does require an unindexed sequential scan through the GSS, but it avoids
scanning through all module symbol streams. The GSI can be used to further
accelerate procedure lookups.

# Symbol Name Table (used in GSI and PSI)

Before we specify the structure of the GSI and PSI, we first define a Symbol
Name Table. The GSI and PSI both use the same Name Table format.

The Name Table allows finding global symbols using their symbol name. The
lookups are case-insensitive. Lookups require the entire symbol name; partial
(substring) lookups are not supported because the lookups use name hashing. The
hash function is case-insensitive, for ASCII.

The Name Table can use one of two encodings: the "large" encoding or the "small"
encoding. The large encoding appears to have been the earlier one, with the
"small" encoding being a later optimization of it. Only the small encoding has
been observed. This document only specifies the small encoding.

The Name Table is a hash table. It has the following structure. Again, this
describes only the "small table" representation; the "large" representation is
not specified.

```c
struct SymbolNameTable {
    // Always 0xFFFF_FFFF. This value indicates that the "small" representation is being used.
    uint32_t signature;

    // Always 0xF12F_091A.  This value is 0xEFFE_0000 + 19990810.
    // This suggests that this version was created on August 10, 1999.
    uint32_t version;

    // The size in bytes of hash_records. Since each hash record has a fixed
    // size of 8 bytes, this determines the number of hash records.
    uint32_t hash_records_size;

    // The size in bytes of the hash_buckets region.
    uint32_t hash_buckets_size;

    // Contains one record for each symbol in the Name Table.
    HashRecord hash_records[hash_records_size / 8];

    // Contains a bitmap which describes which hash buckets are present
    uint8_t hash_buckets[hash_buckets_size];
};
```

> Invariant: `hash_records_size` is a multiple of 8.

Let `num_hash_records = hash_records_size / 8`.

## Hash Records

```c
struct HashRecord {
    // This field specifies the byte offset of this symbol within the Global
    // Symbol Stream (GSS), plus 1.
    int32_t offset; 

    // This appears to be a reference count, but is only relevant to in-memory
    // data structures. Decoders should ignore this field. Encoders should set
    // this field 1.
    int32_t c_refs;
};
```

Each entry in `hash_records` describes a symbol record stored in the Global
Symbol Stream. `hash_records` is sorted by hash code, in increasing order. The
hash code is not stored directly in the `HashRecord`, but is computed by
accessing the symbol record stored in the GSS.

> Invariant: `offset` is positive. `offset - 1` points to a valid byte offset of
> the start of a symbol record within the Global Symbol Stream (GSS), and that
> record has a `name` field.

> Limit: The previous invariant implies that the GSS cannot be larger than
> approximately 2^31.

> Determinism: The `c_ref` field should always be set to 1.

## Hash Buckets

A _hash bucket_ is a sequence of contiguous hash records within `hash_records`
that have the same hash value.

The `hash_buckets` array contains a compressed array of `int32_t` values, one
for each hash bucket. It must be decompressed before it can be used. The
compression scheme uses a bitmap which indicates which of the hash buckets are
non-empty. After decompression, each `int32_t` value points into the
`hash_records` array and specifies the start of the hash records for a given
hash bucket. The following `int32_t` value (if there is one) implies the end of
the hash bucket.

Let `DecompressedHashBuckets` be the in-memory representation of the hash
buckets:

```c
struct DecompressedHashBuckets {
    int32_t buckets[num_buckets];
};
```

`num_buckets` is a parameter that is chosen when the hash table is constructed.
Unfortunately, this parameter is **not** stored in the Name Table. The value is
a function of bit flags stored in the [PDB Information Stream](pdbi_stream.md).

# Example Name Table Header

In this example, the PSI Stream Header is at offset 0. The Name Table
immediately follows the PSI Stream Header and is at offset 0x1A (28). The
`ver_signature` field is -1; the `ver_header` field is 0xF12F091A. The
`hash_records_size` field is 0x88500; the `hash_buckets_size` is 0x4204.

# Hash Records

There is one `HashRecord` structure for each name in the Name Table.
`HashRecord` has this structure:

> Invariant: The `offset` field is positive.

> Invariant: The offset field of `HashRecord` (after subtracting 1) points to a
> valid byte offset of the start of a symbol record within the Global Symbol
> Stream (GSS).

Each hash record points to the Global Symbol Stream (GSS). The hash record does
not store the hash value of a symbol; applications which read or write PDBs are
expected to generate those hash values during table construction or table
querying.

# Example of hash records

The first record has an offset value of 0x66_2121. Removing the +1 bias gives a
value of 0x66_2120. In the GSS of this PDB at offset 0x66_2120 we see:

The 2A 00 encodes the symbol length (42 bytes). The 0E 11 encodes the symbol
kind (S_PUB32). The symbol name is `__imp_RtlInitUnicodeStringEx`.

The hash records are sorted in order of increasing hash code. Each contiguous
run of hash records that have the same hash code form a single hash bucket.

Conceptually, the hash records look like this:

Hash record index | Hash code (bucket) | Symbol name
------------------|--------------------|------------
0                 | 0                  | `DllMain`
1                 | 0                  | `FooBar`
2                 | 0                  | `ZapfDingbats`
3                 | 1                  | `MessageBoxW`
4                 | 1                  | `GetProcAddress`
5                 | 2                  | `CreateWindowW`
6                 | 3                  | `CloseHandle`
7                 | 3                  | `HeapAlloc`
8                 | 4                  | `CreateThread`
9                 | 4                  | `GetCurrentThread`
...               | ...                | ...
10,571            | 4095               | `ExitProcess`
10,572            | 4095               | `__chkstk`

The encoded form of the hash records does not contain these fields directly. The
hash record index is implicit (it is simply the location of the hash record
itself). The hash code was computed when the table was generated but is not
stored. The only field in `HashRecord` that is needed is the offset field, which
points to the symbol record in the Global Symbol Stream (GSS).

The hash buckets array looks like this:

Bucket index | Hash record offset | Covers these symbols
-------------|--------------------|---------------------
0            | 0                  | `DllMain`, `Foobar`, `ZapfDingbats`
1            | 3                  | `MessageBoxW`, `GetProcAddress`
2            | 5                  | `CreateWindowW`
3            | 6                  | `CloseHandle`, `HeapAlloc`
4            | 8                  | `CreateThread`, `GetCurrentThread`
...          | ...
4095         | 10,571             | `ExitProcess`, `_chkstk`

# Hash Buckets

The `hash_buckets` region contains the hash buckets for the Name Table. As
mentioned above, each hash bucket is a sequence of contiguous hash records. This
is an implication of another invariant: The hash records are sorted by their
hash.

To describe a hash bucket, we need the index of the first hash record in this
hash bucket and the number of entries in that hash bucket. This is what the
`hash_buckets` region tells us. Conceptually, the `hash_buckets` region contains
one `int32_t` value for each hash bucket, which is the offset within the
`hash_records` region where this hash bucket starts. The offset in
`hash_buckets` for the next bucket tells us where the next hash bucket starts,
and implicitly where the current hash bucket ends.

> Invariant: The hash offsets are sorted in increasing order. Duplicates are
> permitted.

For historical reasons, the values stored in `hash_buckets` are multiplied by
12, not by 8. To find the index of the hash record, you must first divide
`hash_buckets[i]` by 12. To find the byte offset of a hash record, you then
multiply the value by 8. (Apparently the values were multiplied by 12 because
that was the size of the in-memory data structure used for hash buckets, many
years ago.)

> Invariant: The first hash offset is always zero.

> Invariant: Each hash offset is a non-negative multiple of 12.

> Invariant: Each hash offset (divided by 12) must be less than
> `num_hash_records`.

The `hash_buckets` region uses an encoding that minimizes the encoding size of
the hash buckets. It only encodes the hash record offset for hash buckets that
are not empty, because many hash tables will have a large number of empty hash
buckets. The encoding uses a bitmap that indicates which hash buckets are
non-empty (1 in the bitmap) vs. empty (0 in the bitmap).

To encode the `hash_buckets` region, you must know the number of hash buckets
(`num_buckets`). It is not stored directly in the Name Table, or even in the GSI
Header. Instead, it is determined by the presence of the `MinimalDebugInfo`
feature code in the PDB Information Stream. See
[Feature Codes](pdbi_stream.md#feature-codes). If the `MinimalDebugInfo` feature
is _absent_, then the value of `num_buckets` is 0x1000. If the
`MinimalDebugInfo` feature is _present_, then the value of `num_buckets` is
0x3ffff.

The `hash_buckets` region consists of a bitmap which indicates which hash
buckets are non-empty, followed by an array of `int32_t` values for the
non-empty hash buckets.

The following is an example of a `hash_buckets` region. The `FF` values (and the
`00 00 00 00` immediately after them) are the "non-empty buckets" bitmap.
Following that are the bucket-index values.

```text
00088520 : 01 00 00 00 19 63 5b 00 01 00 00 00 ff ff ff ff
00088530 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088540 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088550 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088560 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088570 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088580 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088590 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
000885a0 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
000885b0 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
000885c0 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
000885d0 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
000885e0 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
000885f0 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088600 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088610 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088620 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088630 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088640 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088650 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088660 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088670 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088680 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088690 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
000886a0 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
000886b0 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
000886c0 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
000886d0 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
000886e0 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
000886f0 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088700 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088710 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088720 : ff ff ff ff ff ff ff ff ff ff ff ff 00 00 00 00
00088730 : 00 00 00 00 d8 00 00 00 d4 01 00 00 88 02 00 00
00088740 : 30 03 00 00 a8 03 00 00 38 04 00 00 40 05 00 00
00088750 : 00 06 00 00 c0 06 00 00 50 07 00 00 d4 07 00 00
```

The first part is the `nonempty_buckets` bitmap. In this example, `num_buckets`
is 4096 (0x1000), which is the default value. The length in bits of
`nonempty_buckets` is equal to `num_buckets`. It is easy to see that all 4096
bits in this table have been set.

However, the size in bytes of `nonempty_buckets` is a bit strange. It relies on
an implementation detail of the PDB reader/writer library. Let `num_buckets` be
the number of hash buckets. Then let
`non_empty_bitmask_size_in_bytes =(num_buckets + 32) / 8`. The C++ PDB library adds 1 to `num_buckets` and then computes the 32-bit aligned size of the bitmask, because it stores it in `uint32` values. Then it serializes the mask as
a byte array, but it uses the full `uint32_t` array.

In our example, since `num_buckets` is 4096 (0x1000), we compute the size in
bytes of `nonempty_buckets` as 516 (0x204), not 512. You can see the unused 32
bits at the end of the `nonempty_buckets` bitmap:

```c
000886f0 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088700 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088710 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088720 : ff ff ff ff ff ff ff ff ff ff ff ff 00 00 00 00
00088730 : 00 00 00 00 d8 00 00 00 d4 01 00 00 88 02 00 00
00088740 : 30 03 00 00 a8 03 00 00 38 04 00 00 40 05 00 00
00088750 : 00 06 00 00 c0 06 00 00 50 07 00 00 d4 07 00 00
```

Immediately following this bitmap is the array of hash record offsets that start
each bucket. This highlights the first 16 hash bucket values:

```
00088710 : ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff ff
00088720 : ff ff ff ff ff ff ff ff ff ff ff ff 00 00 00 00
00088730 : 00 00 00 00 d8 00 00 00 d4 01 00 00 88 02 00 00
00088740 : 30 03 00 00 a8 03 00 00 38 04 00 00 40 05 00 00
00088750 : 00 06 00 00 c0 06 00 00 50 07 00 00 d4 07 00 00
00088760 : b8 08 00 00 9c 09 00 00 80 0a 00 00 4c 0b 00 00
00088770 : 54 0c 00 00 38 0d 00 00 58 0e 00 00 dc 0e 00 00
00088780 : 78 0f 00 00 68 10 00 00 1c 11 00 00 c4 11 00 00
00088790 : d8 12 00 00 10 14 00 00 d0 14 00 00 18 15 00 00
```

In table format:

Bytes         | Hex        | Decimal,<br>divided by 12 | Difference from next bucket offset<br>(number of hash records in this bucket)
--------------|------------|-----|----
`00 00 00 00` | `00000000` |   0 | 18
`D8 00 00 00` | `000000D8` |  18 | 21
`D4 01 00 00` | `000001D4` |  39 | 15
`88 02 00 00` | `00000288` |  54 | 14
`30 03 00 00` | `00000330` |  68 | 10
`A8 03 00 00` | `000003A8` |  78 | 12
`38 04 00 00` | `00000438` |  90 | 22
`40 05 00 00` | `00000540` | 112 | 16
`00 06 00 00` | `00000600` | 128 | 16
`C0 06 00 00` | `000006C0` | 144 | 12
`50 07 00 00` | `00000750` | 156 | 11
`D4 07 00 00` | `000007D4` | 167 | 19
`B8 08 00 00` | `000008B8` | 186 | ...
...           | ...        | ... | ...

# Building the Name Table

To build a GSI Name Table, you need to have the encoded byte stream of the
Global Symbol Stream (GSS) and know the number of hash buckets to use
(num_buckets).

1. Inputs:
   1. Let `gss` be the Global Symbol Stream.
   1. Let `num_buckets` be the number of hash buckets (usually 4096).
   1. Let `output_bytes` be the byte stream that is the output of this function.
2. Let `hash_names_temp` be a vector (array) and initialize it to empty. Each
   record in `hash_names_temp` holds the byte offset within the GSS and the hash
   code of a symbol.
3. Scan through the GSS and locate all relevant public symbols. This is
   `S_PUB32` for the PSI, and `S_GPROC32` etc. for the GSI. Compute the hash
   code for the symbol’s name. Add an entry to `hash_names_temp` for this
   symbol, containing the hash code and the byte offset of this record.
4. Sort `hash_names_temp` in order of increasing hash code, and then increasing
   byte offset.
5. Let `hash_records_size = 8 * length of hash_names_temp`.
6. Let `hash_buckets` be a vector of `int32_t`, whose length is `num_buckets`.
   Scan through `hash_names_temp` and find the pairs of consecutive records
   where the hash code (bucket index) is different. Use this to initialize
   hash_buckets to the starting offset of each bucket.
7. Let `nonempty_buckets` be a bitmap, whose length is `num_buckets`. Initialize
   it so that each entry `nonempty_bucket[j]` is 1 if the corresponding value in
   `hash_buckets[j]` is less than `hash_buckets[j – 1]` (indicating that it is
   not empty). For the last `hash_buckets[j]`, compare it to `num_hash_records`.
8. Let `num_nonempty_buckets` = count the number of bits in nonempty_buckets
   that are 1.
9. Let `nonempty_buckets_size = (num_buckets + 32) / 32 * 4`.
10. Let `hash_buckets_size = nonempty_buckets_size + num_nonempty_buckets * 4`.
11. Write the Name Table Header to `output_bytes`, using `hash_records_size` and
    `hash_buckets_size` as computed above.
12. Write the hash records to output_bytes. For each hash record, write a
    `HashRecord`, containing the symbol’s GSS stream offset and setting
    `c_ref = 1`.
13. Write the `nonempty_buckets` bitmap to `output_bytes`. Ensure that exactly
    `nonempty_buckets_size` bytes are written.
14. Scan through `hash_buckets` and write the offset for those hash buckets that
    are not empty. This should write exactly `num_nonempty_buckets * 4` bytes.

# Loading the Name Table

The description above should be sufficient for decoding the GSI Name Table. The
only interesting bit is decoding the hash buckets, using the nonempty_buckets
bitmap. The goal is to produce a hash_buckets array that contains non-decreasing
values. If the nonempty_buckets bitmap indicates that a bucket is empty, then
its offset is equal to the offset of the next bucket.

# Querying the Name Table

Each query begins with a symbol name to locate.

1. Compute the hash of the symbol name, using the PDB Hash Function (V1). This
   gives the bucket index. Let `h` be the hash value.
2. Use the hash_buckets table to find the hash records that are in this hash
   bucket. The starting index is given by hash_buckets[h] and the ending
   (exclusive bound) index is given by `hash_buckets[h + 1]`. In Rust syntax,
   the records are: `&hash_records[hash_buckets[h] .. hash_buckets[h + 1]]`.
3. For each hash record in the hash bucket:
   1. Use the symbol offset value in the hash record to look up the symbol in
      the GSS. It is a byte offset (biased by 1).
   1. Read / decode the symbol record.
   1. Compare the string to the name being searched for. If it matches, the
      query terminates (succeeds).
4. If no matches are found, then there is no global symbol with that name.

# Global Symbol Index (GSI) Stream

The Global Symbol Index (GSI) Stream provides a name-to-symbol lookup table for
global symbols that have a name. The GSI contains entries only for the following
symbol kinds:

Record Kind<br>(hex) | Record Kind
---------------------|------------
0x1107               | `S_CONSTANT`
0x1108               | `S_UDT`
0x110C               | `S_LDATA32`
0x110D               | `S_GDATA32`
0x1113               | `S_GTHREAD32`
0x1127               | `S_LPROCREF`
0x1128               | `S_ANNOTATIONREF`
0x1129               | `S_TOKENREF`

Note that `S_PUB32` is **not** included in this list. `S_PUB32` symbols are
indexed in the PSI, not the GSI.

The GSI Stream simply contains an instance of the Name Table described above.
The GSI Stream does not have a header of its own; instead, it starts immediately
with the Name Table. The GSI Stream does not have an address lookup table.

# Public Symbol Index (PSI) Stream

The Public Symbol Index (PSI) provides several look-up tables that accelerate
finding information in the Global Symbol Stream. The PSI indexes only `S_PUB32`
symbols in the GSS; all other symbol kinds are indexed in the GSI.

* Name-to-symbol table: This table allows finding global symbols using a symbol
  name.
* Address-to-symbol table: This table allows finding global symbols using a
  symbol address. The address is represented as a segment and an offset within a
  segment.
* Thunk table: TODO

The PSI stream has this structure:

```c
struct PsiStream {
  PsiStreamHeader header;
  uint8_t name_table[];
  uint8_t address_table[];
};
```

`name_table` contains the name-to-symbol lookup table and is specified by the
`SymbolNameTable` structure, above. This section immediately follows the header
and is variable-length. Its size is specified in the `PsiStreamHeader`.

`address_table` contains the address-to-symbol lookup table. This section
immediately follows `name_table` and is variable-length. Its size is specified
in the `PsiStreamHeader`.

`PsiStreamHeader` has this structure:

```c
struct PsiStreamHeader {
    uint32_t name_table_size;  // Size in bytes of name_table
    uint32_t addr_table_size;  // Size in bytes of address_table
    uint32_t num_thunks;       // The number of thunk records
    uint32_t thunk_size;       // Size in bytes of each thunk record.
    uint16_t thunk_table_section;
    uint16_t padding;
    uint32_t thunk_table_offset;
    uint32_t num_sections;
};
```

> TODO: Specify the thunk table.

Example PSI Stream Header:

```
00000000 : 14 c7 08 00 80 42 04 00 00 00 00 00 00 00 00 00
00000010 : 00 00 00 00 00 00 00 00 00 00 00 00 ff ff ff ff
00000020 : 1a 09 2f f1 00 85 08 00 04 42 00 00 21 21 66 00
00000030 : 01 00 00 00 65 2a 92 00 01 00 00 00 49 07 70 00
00000040 : 01 00 00 00 29 c9 75 00 01 00 00 00 95 60 62 00
00000050 : 01 00 00 00 5d d8 74 00 01 00 00 00 ad e4 89 00
```

## Address Table

The Address Table allows finding a global symbol based on its address. The size
in bytes of the Address Table is specified in the PSI Stream Header as
`addr_table_size`.

The PSI Address Table is an array of `int32` values. Let
`num_addr_records = addr_table_size / 4`. Each value is the byte offset of a
symbol record in the GSS.

The values in the Address Table are in a sorted order. The order is determined
by the `(segment, offset)` values of the `S_PUB32` symbols that are referenced
by the GSI Address Table. The address records are not sorted by their own value;
the sorting algorithm must use the address record to dereference the GSS in
order to compare entries.

The entries in the Address Map are not sorted by their value, but by the `name`
field of the global symbol that those values point to. That is, sorting must
dereference the pointer (stream offset) into the GSS.

It is meaningless for a value (GSS stream offset) to appear more than once in
the Address Map. This is not stated as an invariant; it would not be harmful for
a value to be repeated, but it would have no benefit.

> Determinism: The values in the Address Map are unique.

This is an example of the GSI Address Table, starting at 0x8_C730:

```text
0008c710 : 8c c1 0c 00 f8 c1 0c 00 d0 c2 0c 00 b4 c3 0c 00 : ................
0008c720 : 44 c4 0c 00 64 c5 0c 00 b8 c5 0c 00 9c c6 0c 00 : D...d...........
0008c730 : a8 d5 58 00 9c 29 8c 00 fc 74 8e 00 80 d8 68 00 : ..X..)...t....h.
0008c740 : c0 59 50 00 38 1a 66 00 8c 4b 8b 00 e4 26 6c 00 : .YP.8.f..K...&l.
0008c750 : 5c 5c 63 00 34 69 6c 00 c4 31 8c 00 14 0f 7b 00 : \\c.4il..1....{.
0008c760 : 88 69 70 00 a4 fd 7b 00 5c 9b 7a 00 c0 c2 5c 00 : .ip...{.\.z...\.
0008c770 : 6c 56 79 00 18 24 55 00 a0 16 59 00 5c e8 5a 00 : lVy..$U...Y.\.Z.
0008c780 : 48 19 65 00 f0 5f 70 00 40 6c 5a 00 50 33 76 00 : H.e.._p.@lZ.P3v.
0008c790 : d0 ed 7a 00 10 01 51 00 38 ea 61 00 48 6f 59 00 : ..z...Q.8.a.HoY.
```

The first few offsets in this table:

Bytes         | Hex value  | Symbol name
--------------|------------|------------
`A8 D5 58 00` | `0058D5A8` | `EnumSystemGeoID`
`9C 29 8C 00` | `008C299C` | `DeleteStateAtomValue`
`FC 74 8E 00` | `008E74FC` | `?DeleteSetting@StateAtom@@QEAAJPEBG@Z`

We can manually find these records in the GSS table:

```text
0058d5a0 : 61 63 6b 61 67 65 40 00 1e 00 0e 11 02 00 00 00 : ackage@.........
0058d5b0 : 10 00 00 00 01 00 45 6e 75 6d 53 79 73 74 65 6d : ......EnumSystem
0058d5c0 : 47 65 6f 49 44 00 00 00 42 00 0e 11 02 00 00 00 : GeoID...B.......
...
008c2990 : 65 61 64 42 61 74 63 68 00 00 00 00 22 00 0e 11 : eadBatch...."...
008c29a0 : 02 00 00 00 f0 00 00 00 01 00 44 65 6c 65 74 65 : ..........Delete
008c29b0 : 53 74 61 74 65 41 74 6f 6d 56 61 6c 75 65 00 00 : StateAtomValue..
...
008e74f0 : 6d 61 70 68 6f 72 65 45 78 41 00 00 32 00 0e 11 : maphoreExA..2...
008e7500 : 02 00 00 00 70 01 00 00 01 00 3f 44 65 6c 65 74 : ....p.....?Delet
008e7510 : 65 53 65 74 74 69 6e 67 40 53 74 61 74 65 41 74 : eSetting@StateAt
008e7520 : 6f 6d 40 40 51 45 41 41 4a 50 45 42 47 40 5a 00 : om@@QEAAJPEBG@Z.
008e7530 : 3a 00 0e 11 00 00 00 00 b8 c3 15 00 02 00 3f 3f : :.............??
```

## Querying the PSI Address Table

Querying the Address Table is simply a binary-search process. Use a standard
binary-search algorithm to search the Address Table, but keep in mind that the
Address Table does not directly contain the field that is being tested for the
binary search (the search key). The search key is the `segment:offset` value
stored in the `S_PUB32` record. Each entry in the Address Table contains the
offset within the GSS that contains the `S_PUB32`, so the binary search needs to
use that offset to dereference the GSS.

## Pointer relationships

The GSI and PSI both contain pointers (stream offsets) into the Global Symbol
Stream (GSS). Therefore, if symbol records within the GSS are moved, added, or
deleted, the GSI and PSI will both need to be reconstructed.

The GSS contains `TypeIndex` values that point into the TPI Type Stream. If type
records in the TPI are moved (added, deleted, etc.), then the GSS will need to
updated. If this update also reorders symbol records in the GSS, then the GSI
and PSI will also need to be rebuilt.

* GSS contains `TypeIndex` values &rarr; TPI Type Stream
* GSS contains Module index values &rarr; DBI Modules Substream
* GSS contains byte offsets that point into Module symbol streams &rarr; Module
  Streams
 
```

`docs/pdb/hashing.md`:

```md
# Hash functions

## PDB Hash Function: `LHashPbCb` (32-bit)

Several PDB tables use a hash function. The hash function is simplistic, and is
based on XOR’ing input bytes in groups of 4. For inputs whose length is not a
multiple of 4, the hash function XOR’s the last 1, 2, or 3 bytes, but there is
an accidental special-case that must be considered carefully.

The hash function takes a byte array and produces a `uint32`. It then divides
that value by a modulus (usually a hash bucket count) and returns the remainder.

To compute the hash function:

1. Let B be the input byte array, and L be its length. Let M be the modulus
   parameter (hash bucket count).
2. Let H be a uint32 value, and initialize it to zero.
3. Let L4 = L / 4, with integer division, i.e. drop the remainder. This gives
   the number of whole uint32 units that we will read.
4. Read the first L4 * 4 bytes from the input array, and read each group of 4
   bytes as a uint32 value (in little-endian byte order). For each uint32 value,
   XOR the value into H.
5. Examine the remaining bytes, after the L4 * 4 prefix.
   a. If there are no remaining bytes, do nothing.
   b. If there are 2 or 3 remaining bytes, then read the first 2 as a uint16
      value (in little-endian byte order), cast it to uint32, and XOR it into H.
   c. If there are 3 remaining bytes or 1 remaining byte, then read the last
      byte and cast it to a uint32 value, in the low bits. XOR this value into H.
6. Set bit 5 in each byte of H to 1. That is, set bits 5, 13, 21, and 29 to 1.
   This is done to support case-insensitive hashing.
7. Set H = H XOR (H >> 11).
8. Set H = H XOR (H >> 16).
9. Divide H by M and return the remainder as the result of the has function.

## Hash function: `SigForPbCb`

PDB uses CRC-32 for hashing some strings. It is the standard, well-known CRC-32 algorithm.

## References

* [`HashPbCb`](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/langapi/shared/crc32.h#L8)

* [`SigForPbCb`](https://github.com/microsoft/microsoft-pdb/blob/master/langapi/shared/crc32.h)

```

`docs/pdb/ipi_stream.md`:

```md
# IPI Stream (Fixed Stream 4)

The IPI Stream uses many of the same data structures as the TPI Stream and the
[TPI Stream](tpi_stream.md) specification should serve as the specification for
the IPI Stream. However, The IPI Stream and TPI Stream store different kinds
records and serve different purposes.

These aspects of the TPI Stream and IPI Stream are identical:

* Stream header
* Record framing, but not record contents
* Hash value substream
* Hash stream

See [CodeView Item Records](../codeview/items/items.md) for a description of
the records that can be stored in the IPI Stream.  **Only item records can be
stored in the IPI Stream**.  Although the IPI Stream uses many of the same
data structures as the TPI Stream, they do not contain the same records and
serve different purposes.

To find a specific record in the IPI given its `ItemId`, first subtract
`type_index_begin` from the `ItemId`. This gives the 0-based index of the record
within the stream; let this be the value `R`. Then, begin decoding records
within the IPI Stream, counting them as they are decoded. When `R` records have
been decoded, the next record is the desired record.

The value of `type_index_begin` (in the IPI Stream Header) is typically 0x1000.
No other value has been observed.

## IPI Hash Value Substream and IPI Hash Stream

The IPI Stream contains an IPI Hash Value Substream, which has the same
structure as the TPI Hash Value Substream.

The IPI Stream also has a corresponding IPI Hash Stream, which has the same
structure as the TPI Hash Stream but it describes records in the IPI Stream, not
TPI Stream.

```

`docs/pdb/mini_pdb.md`:

```md
# Mini PDBs

The MSVC linker provides a feature known as "fast PDBs" or "mini PDBs". It is
enabled using the `/DEBUG:FASTLINK` linker option. This option generates PDBs
that contain only minimal debug information, some of which consists of pointers
to debug information in other files.

Mini PDBs were intended to reduce developer inner loop times by shortening
linking times. However, recent improvements in linker performance have reduced
the performance gap between full PDBs and mini PDBs. `/DEBUG:FASTLINK` is now
considered a deprecated feature.

A mini PDB file can be identified by the presence of the `MinimalDebugInfo`
feature code in the PDB Information Stream.

The `MinimalDebugInfo` feature code determines the value of the `num_buckets`
parameter in the GSI and PSI Name Tables. It is therefore required in order to
decode those tables. If the `MinimalDebugInfo` feature is _absent_, then the
value of `num_buckets` is 0x1000. If the `MinimalDebugInfo` feature is
_present_, then the value of `num_buckets` is 0x3ffff.

# References

* [/DEBUG (Generate debug info)](https://learn.microsoft.com/en-us/cpp/build/reference/debug-generate-debug-info)

```

`docs/pdb/module_stream.md`:

```md
# Module Streams

- [Module Streams](#module-streams)
- [Line Data (C13)](#line-data-c13)
- [Module Symbols](#module-symbols)
- [Global Refs](#global-refs)

Each module may have its own Module Stream. Modules are not required to have a
Module Stream; a "stripped" (public) PDB is a PDB whose module streams have been
removed. Different modules do not share a Module Stream; it is a 1:1
relationship (or 1:none).

The [DBI Modules Substream](dbi_stream.md#dbi-modules-substream) describes the
modules within an executable. For each module there is a `ModuleInfo` record
which specifies the stream index of the Module Stream and the size of the
substreams within the Module Stream. The `ModuleInfo` record is necessary to
locate the Module Stream and its substreams.

Each Module Stream has this structure:

```c
struct ModuleStream {
    uint8_t symbols[sym_byte_size];
    uint8_t c11_lines[c11_byte_size];
    uint8_t c13_lines[c13_byte_size];
    uint8_t global_refs[];
};
```

where `sym_byte_size`, `c11_byte_size`, and `c13_byte_size` are taken from
`ModuleInfo` record.

# Line Data (C13)

Module Streams may contain line number information, which describes the mapping
between source locations and instruction streams.

`c11_lines` contains C11 Line Data. C11 Line Data is obsolete and will not be
described in this document. `c11_byte_size` is expected to be zero in all modern
PDBs.

`c13_lines` contains C13 Line Data. See [C13 Line Data](../codeview/line_data.md) for the
specification of the contents of this substream.

# Module Symbols

`sym_byte_size` specifies the length of the symbols substream for this module.
The symbols stream has a `uint32_t` (4-byte) prefix, followed by symbols
records.

# Global Refs

`global_refs` contains the locations of global symbols that are relevant to this
module. Global Refs is an extension, and was added in such a way that older
tools could safely ignore it. It is legal for `global_refs` to have a length of
zero, meaning this Module Stream does not contain Global Refs.

If Global Refs is present, then it has this structure:

```c
struct GlobalRefs {
    uint32_t size;                     // size in bytes of the global refs table
    uint32_t global_refs[size / 4];
};
```

Each value in `global_refs` is a byte offset into the
[Global Symbol Stream](globals.md). Each entry identifies a global symbol that
is relevant to this module.

```

`docs/pdb/msf.md`:

```md
- [Multi-Stream File (MSF) Container](#multi-stream-file-msf-container)
- [Basic structure](#basic-structure)
  - [Pages](#pages)
  - [Streams](#streams)
  - [Small MSF vs Big MSF](#small-msf-vs-big-msf)
- [Big MSF Encoding](#big-msf-encoding)
  - [MSF File Header](#msf-file-header)
  - [Stream Directory](#stream-directory)
  - [Stream Directory location on disk](#stream-directory-location-on-disk)
  - [Free Page Map](#free-page-map)
  - [Intervals (Big MSF only)](#intervals-big-msf-only)
    - [Example](#example)
    - [Wasted space in the FPMs](#wasted-space-in-the-fpms)
- [Small MSF Encoding](#small-msf-encoding)
  - [MSF File Header - Small MSF](#msf-file-header---small-msf)
- [Stream Directory](#stream-directory-1)
  - [Small MSF](#small-msf)
  - [Determinism](#determinism)
  - [Location on disk - Small MSF](#location-on-disk---small-msf)
- [Stream usage](#stream-usage)
- [2-phase commit protocol](#2-phase-commit-protocol)
- [Example - Big MSF](#example---big-msf)
- [References](#references)

# Multi-Stream File (MSF) Container

PDBs store information in _streams_ where each stream is analogous to a file and
an MSF container is analogous to a filesystem volume (e.g. a raw disk image).
MSF allows processes to create, delete, and modify streams without updating the
entire MSF file. Updates to existing PDBs are efficient because a writer can
update only those on-disk pages that need to be modified. Modifying a stream,
including changing the size of the stream, does not require moving other
streams.

This section will use the term MSF for the overall file structure of the PDB.
MSF is the container format for all of the information stored within PDBs.
However, the MSF format itself does not contain any PDB-specific information,
aside from a signature field in its header that identifies it as being a PDB.

# Basic structure

The MSF is organized as:

* The MSF File Header, which describes global parameters that control the file
  layout.
* An array of fixed-size pages, each of which is typically 4096 bytes in length.
* An ordered set of streams, where each stream is a logical sequence of bytes,
  similar to an ordinary file. Streams are the main abstraction that the MSF
  creates. The Stream Directory specifies the length (in bytes) of each stream
  and the pages that compose each stream.
* The Free Page Map, which is used for allocating pages. The Free Page Map also
  organizes pages into regions called intervals. Intervals are fixed-size
  sequences of pages, related to a specific portion of the Free Page Map.
  Intervals are relevant only to the page allocator and are not meaningful to
  the user of the PDB/MSF.

## Pages

The MSF file is organized as a sequence of fixed-size pages. Pages are the unit
of allocation and organization; all stream data is stored in pages. Pages are
identify by _page number_, which is a `uint32_t` value.

All pages have the same page size. The `page_size` field in the MSF File Header
specifies this value. The most common value for `page_size` size is 4096 bytes.

> Invariant: `page_size` is a power of 2 and is in the inclusive range 512-65536
> (0x200-0x10000).

> Invariant: Each page is stored at a file offset within the MSF that is a
> multiple of `page_size`.

The default and most common value for page_size is 4096 (0x1000). Many tools
that process PDBs, such as older builds of the MSPDB and DIA libraries, can only
work with PDBs that have a page_size of 4096.

> Determinism: Use a `page_size` of 4096, unless the total PDB data size is
> large enough that it requires a larger page size. Or allow `page_size` to be
> an input parameter, not a free variable.

The `num_pages` field of the MSF File Header specifies the number of valid pages
in the file. Valid means that the bits in the FPM are meaningful and that a
valid page may be assigned to a stream. In the example above, the num_pages
field is 20923 (0x51BB). The total file length is 85,700,608 (0x51B_B000) bytes,
so the `num_pages` field covers exactly the file size.

> Invariant: The num_pages field must be less than or equal to the byte length
> of the MSF file, divided by the page size.

> Determinism: The byte length of the MSF file must be a multiple of `page_size`
> size and the `num_pages` field must be equal to the byte length of the MSF
> file divided by `page_size`.

## Streams

Streams store the important information within PDBs. A stream is a logical
sequence of bytes, similar to a file. A stream can be any size, as long as the
size can be represented as a `uint32_t`. The contents of each stream are stored
in a sequence of pages. The pages in a stream may be stored at any valid page
offset within the MSF file; there is no requirement that the pages be stored
sequentially (contiguously). Pages from different streams may be intermixed in
arbitrary orders.

Streams are identified by `stream index`. Small MSF uses `uint16_t` for
`num_streams`, so Small MSF has a limit of 65535 usable streams (stream 0 is
reserved for the Old Stream Directory). Big MSF uses `uint32_t` for
`num_streams`, so it can encode a much larger number of streams, although due to
encoding requirements for the Stream Directory it is not possible to create
`2^32 - 1` streams. Unfortunately, while MSF itself allows for a large number of
streams (when using Big MSF), many of the data structures defined by PDB (e.g.
the Named Streams Table) use `uint16_t` for stream indexes. This limits the
number of streams to 65535.

Each stream "owns" the pages that are assigned to it. If a page is allocated to
a stream then that page may not be allocated to any other stream. The page may
occur at only one position within the stream.

The Free Page Map stores a bitmap of the set of pages that are free (available
for allocation to a stream). The Free Page Map is an important data structure
for the implementation of the MSF, but is not directly visible or relevant to
the user of the MSF.

## Small MSF vs Big MSF

There are two variants of the MSF file format: "Small MSF" and "Big MSF". Big
MSF is an improvement on Small MSF which enabled PDBs to be much larger. Small
MSF is obsolete; all encoders should use Big MSF. Small MSF is documented here
because there are still some PDBs in storage that are encoded using Small MSF.

Big MSF and Small MSF differ in the structure of the Stream Directory, Free Page
Map, MSF File Header, and in storage limits, but provide the same abstraction
(streams).

Decoders should be capable of reading Small MSF files, or reliably identifying
them and rejecting them. New encoder implementations should not implement
support for Small MSF.

# Big MSF Encoding

## MSF File Header

The MSF File Header is stored at file offset 0. It contains a `magic` field
which unambiguously identifies this file as an MSF file and specifies either the
Big MSF or Small MSF encoding.

The structure of the MSF File Header, Stream Directory, and Free Page Map are
dependent on whether the MSF file is encoded using Big MSF or Small MSF. This
document will describe each part of the MSF file using the Big MSF encoding,
then specify each part of the file using Small MSF encoding. Some of the
descriptions of the Small MSF encoding are given in terms of Big MSF.

The MSF File Header is stored at file offset 0. When using Big MSF the MSF File
Header has this structure:

```text
// sizeof = 52 bytes (fixed-size prefix)
struct BigMsfFileHeader {
    uint8_t magic[32];
    uint32_t page_size;
    uint32_t active_fpm;
    uint32_t num_pages;
    uint32_t stream_dir_size;
    uint32_t small_stream_pages_map;      // not used; do not read
    uint32_t big_stream_pages_map[];
}
```

The `magic` field has the following contents:

```text
const uint8_t MSF_BIG_MAGIC[32] = {
    0x4d, 0x69, 0x63, 0x72, 0x6f, 0x73, 0x6f, 0x66, // Microsof
    0x74, 0x20, 0x43, 0x2f, 0x43, 0x2b, 0x2b, 0x20, // t C/C++
    0x4d, 0x53, 0x46, 0x20, 0x37, 0x2e, 0x30, 0x30, // MSF 7.00
    0x0d, 0x0a, 0x1a, 0x44, 0x53, 0x00, 0x00, 0x00, // ...DS...
}
```

`page_size` specifies the size in bytes of each page.

> Invariant: When using Big MSF, `page_size` is a power of 2 and is in the range
> 512-65536 (0x200-0x10000).

`active_fpm` specifies the Active Free Page Map.

> Invariant: `active_fpm` is 1 or 2. No other values are valid.

`num_pages` specifies the total number of pages in the MSF file. This value
includes all pages: free pages, the MSF Header Page, FPM pages, stream pages,
etc. Normally, `num_pages = file_size / page_size`, where `file_size` is the
size of the MSF file in bytes.

`small_stream_pages_map` is obsolete. Decoders should ignore this field. Encoders should set it to 0.

`stream_dir_size` specifies the size in bytes of the Stream Directory.
`big_stream_pages_map` specifies the location of the Stream Directory. They will
be described in the Stream Directory section, below. Note that
`big_stream_pages_map` is an array, not just a single `uint32_t` field.

## Stream Directory

The Stream Directory is a variable-length data structure that lists the streams
defined in the MSF file, their size in bytes, and the page numbers for the pages
assigned to each stream. When using Big MSF the Stream Directory has this
structure:

```text
struct BigStreamDirectory {
    uint32_t num_streams;
    uint32_t stream_sizes[num_streams];
    uint32_t stream_pages[num_pages_in_all_streams];  // num_pages_in_all_streams is defined below
}
```

The `stream_dir_size` field of the MSF File Header specifies the size in bytes
of the Stream Directory.

> Invariant: `stream_dir_size` is a multiple of 4.

`num_streams` specifies the number of streams in the MSF file.

> Invariant: `num_streams` is at least 1.

`stream_sizes` specifies the size in bytes of each stream. A stream may be a
"nil stream". Nil streams have a `stream_size[i]` value of `NIL_STREAM_SIZE`,
whose value is 0xFFFF_FFFF. Nil streams do not store any data and do not have
any pages assigned to them.

The number of pages in a non-nil stream is computed by dividing
`stream_sizes[i]` by `page_size` and rounding up. Rounding up accounts for the
last page in the stream which will be partially-filled if the stream size is not
a multiple of `page_size`. A nil stream does not have any pages assigned to it.
Let
`num_pages_in_stream(s) = stream_sizes[s] == NIL_STREAM_SIZE ? 0 : divide_round_up(stream_sizes[s], page_size)`
be the number of pages in stream `s`.

> Determinism: If `stream_sizes[s]` is not a multiple of `page_size`, then the
> unused bytes in the last page are set to zero.

`stream_pages` contains the list of pages that are assigned to each stream,
concatenated, and in the same order as the streams. To traverse (or build) this
array, use the `num_pages_in_stream(s)` function to compute the number of pages
for each stream, starting at `stream_sizes[0]` and ending at
`stream_sizes[num_streams - 1]`. Use the value of `num_pages_in_stream(s)` to
traverse the correct number of values in `stream_pages`.

Let `num_pages_in_all_streams` be the sum of `num_pages_in_stream(s)` for all
streams. This gives the size of `stream_pages`. Since `stream_pages` contains
the concatenated list of pages in each stream, `stream_pages` begins with the
list of pages in stream 0, followed by the pages in stream 1, then stream 2, ...
then the pages for stream `num_streams - 1`.

> Invariant: The contents of the Stream Directory must be consistent with the
> value of `stream_dir_size`.
> `stream_dir_size = sizeof(uint32_t) * (1 + num_streams + num_pages_in_all_streams)`.

Stream 0 is special; it points to the "Old Stream Directory" and is part of the
2-phase protocol. Stream 0 must always be present (even if it is zero-length or
invalid).

> Invariant: If a page is allocated to a stream then it is not used in any other
> stream or used again within the same stream. Stated differently, the
> `stream_pages` array does not contain any duplicates.

## Stream Directory location on disk

The Stream Directory is stored in a 3-level hierarchy of pages:

1. The MSF File Header contains a list of page numbers which point to the Stream
   Directory Page Map.
2. The Stream Directory Page Map is a set of pages which contain page numbers
   which point to the Stream Directory.
3. The Stream Directory contains the byte representation, described in the
   previous section.

It is easiest to understand this hierarchy by walking through the process of
buiding the Stream Directory "from the bottom up". That is, first you construct
the byte representation of the Stream Directory, then you build the Page Map,
then you store the pointers to the Page Map in MSF Page 0.

* First, build the Stream Directory as described in the previous section. This
  is a variable-size data structure which specifies the number of streams, their
  size in bytes, and the pages assigned to each stream.

* Next, we define several values:

  * Let `stream_dir_size` be the size in bytes of the Stream Directory that we
    have just constructed.
  * Let `stream_dir_pages` be an array of `uint32_t` page numbers that contains
    the page numbers that were allocated for storing the Stream Directory.
  * Let `num_stream_dir_pages = divide_round_up(stream_dir_size, page_size)`.
    This is the number of pages that contain the Stream Directory.
  * Let
    `num_page_map_pages = divide_round_up(num_stream_dir_pages * sizeof(uint32_t), page_size)`.
    This is the number of pages in the Stream Directory Page Map.

* Next, allocate pages for storing the Stream Directory and write the Stream
  Directory to those pages. This step allocates `num_stream_dir_pages` pages.
  Let `stream_dir_pages` be an array of `uint32_t` page numbers that contains
  the page numbers that were allocated for storing the Stream Directory.

* Next, allocate pages and write `stream_dir_pages` to those pages. Let
  `page_map` be the list of page numbers that were just allocated.

* Finally, write `page_map` into the `big_stream_pages_map` field of the
  `BigMsfFileHader`.

> Determinism: If `stream_dir_size` is not a multiple of `page_size`, then the
> unused bytes in the last page of the Stream Directory should be set to zero.

> Determinism: If `num_stream_dir_pages` is not a multiple of `page_size` then
> the unused bytes in the last page of the Stream Directory Page Map should be
> set to zero.

Decoders should ignore the `small_stream_pages_map` field. Encoders should set
it to zero.

## Free Page Map

Each MSF file contains two Free Page Maps (FPMs), called FPM1 and FPM2. One FPM
is the _active FPM_ while the other is the _inactive FPM_. The `active_fpm`
field of the MSF File Header specifies which FPM is the active FPM.

The Active FPM describes the committed state of the MSF file. In the Big MSF
encoding, the only legal values for `active_fpm` are 1 and 2.

The inactive FPM has the same size and structure as the Active FPM, but its
contents are undefined. The `active_fpm` field specifies which FPM is currently
in use.

The reason for storing two FPMs is to enable MSF to support a 2-phase commit
protocol works. When a PDB is being modified, changes are written to the
Inactive FPM. To commit changes, a single write to the MSF File Header (page 0)
reverses the roles of the active and inactive FPMs. The FPM that was active
becomes inactive, and now describes the state of the MSF file before the commit
point. The FPM that was inactive becomes active, and now describes the new
committed state of the MSF.

In this document, "Free Page Map" (or FPM) refers to the Active FPM, unless
clarified by context.

> Invariant: When using Big MSF the `active_fpm` field can only be 1 or 2.

> Determinism: The `active_fpm` field must be 1.

> Determinism: The inactive FPM must have its contents set to some deterministic
> contents, such as "all busy" or "all free".

Each FPM is stored in a set of pages that are reserved for its use. The pages
that store the FPM are not allocated using the FPM; that would be a
self-referential definition. Instead, pages are allocated for the FPM using a
simpler approach, called _intervals_. Intervals are described below.

Each FPM is stored as a bitmap. The bits are assigned in little-endian format,
so bit 0 in byte 0 of the FPM corresponds to page 0. The byte offset within the
FPM for page `p` is `p / 8`. The bit index within that byte for a page `p` is
`p % 8` (remainder).

Each bit in the FPM specifies whether the corresponding page is free (FPM bit is
1) or busy (FPM bit is 0). The definition of "free" or "busy" requires
clarification, depending on what is stored in that page.

FPM bit value   | Page usage
----------------|-----------
0 (busy)        | The page is part of the Stream Directory.
0 (busy)        | The page is part of the Stream Directory Page Map, which contains page numbers of the Stream Directory.
1 (free)        | The page is part of the Old Stream Directory (stream 0).
0 (busy)        | The page is part of a stream other than the Stream Directory (streams 1 and higher).
0 (busy)        | The page is part of the FPM itself. See the "Intervals" section for how FPM pages are assigned.
1 (free)        | The page is not used for anything, and is not part of the FPM itself.

It may seem surprising that pages that are assigned to the Stream Directory are
considered "free". This is because the MS-PDB implementation reads the Stream
Directory into memory, then marks these pages as "deleted" in an in-memory
bitmask. This protects the pages from being overwritten, until the next commit.

## Intervals (Big MSF only)

An interval is a fixed-size sequence of pages. Every MSF file consists of 1 or
more intervals. The number of pages in each interval is equal to the number of
bytes in each page.

Each FPM is composed of one or more pages. Those pages come from the reserved
pages in each interval. Page 1 within each interval is reserved for FPM1 and
page 2 within each interval is reserved for FPM2. Page 0 within interval 0 is
reserved for the MSF File Header; page 0 within all other intervals is available
for storing stream data.

We compute the number of intervals in a file from the number of pages in a file
by dividing the `num_pages` field (from the MSF File Header) by `page_size` and
rounding up.

> Let `num_intervals = divide_round_up(num_pages, page_size)`

Intervals are quite large, since their size is the square of `page_size`. For
the typical page size of 4096, the interval size would be 16,777,216 bytes (16
MiB). For this reason, there is no requirement that a complete interval be
stored on disk.

### Example

Let's examine the intervals within our example file, whose header is show above.
The page size is 4096 bytes and `num_pages` = 20,923 (0x51B). Then
`num_intervals = divide_round_up(20923, 4096)`, giving 6 intervals. Numbers are
shown in hex.

Interval | Page number of start of interval = `page_size * interval` | File byte offset of start of interval = `page_size * page_size * interval` | FPM1 byte offset = `(page_size + 1) * page_size` | FPM2 byte offset = `(page_size + 2) * page_size`
--|--------|-------------|-------------|------------
0 | 0      | 0           |      0x1000 |      0x2000
1 | 0x1000 | 0x0100_0000 | 0x0100_1000 | 0x0100_2000
2 | 0x2000 | 0x0200_0000 | 0x0200_1000 | 0x0200_2000
3 | 0x3000 | 0x0300_0000 | 0x0300_1000 | 0x0300_2000
4 | 0x4000 | 0x0400_0000 | 0x0400_1000 | 0x0400_2000
5 | 0x5000 | 0x0500_0000 | 0x0500_1000 | 0x0500_2000

### Wasted space in the FPMs

The design of the intervals allocation scheme for FPMs implies that there is one
_byte_ allocated in each FPM for each page. This is more storage space than,
since the FPM is a _bitmap_.

The FPM bitmap is stored within the pages that are allocated to it, in the
sequence of pages that are allocated to it. This means that the pages that store
the FPM are not necessarily in the same interval as the pages that are described
by the FPM.

We will use an example to illustrate this. We will continue using the example
above, where `num_pages` is 20,923 (0x51B). In this example:

* FPM1 is stored in 6 pages, whose page numbers are 1, 0x1001, 0x2001, 0x3001,
  0x4001, and 0x5001.
* The number of **bits** needed to store the FPM bitmap is equal to `num_pages`,
  which is 0x51B.
* The number of **bytes** needed to store the FPM bitmap is equal to
  `divide_round_up(num_pages, 8)`, which is 164.
* Because 164 is less than 4096 (our `page_size`), the entire FPM is stored
  **only** in page 1. The rest of the storage space in page 1, as well as _all_
  of the storage space in pages 0x1001, 0x2001, 0x3001, 0x4001, and 0x5001, is
  wasted.
* Be aware that FPM bits that are _stored_ in page 1 (interval 0) nevertheless
  _describe_ pages in intervals 1, 2, 3, 4, and 5.

As we see, the storage space assigned to the FPM is always 8 times as large as
it actually needs to be. This wasted space is not substantial, but it is often
confusing when trying to understand the design of the FPM.

# Small MSF Encoding

## MSF File Header - Small MSF

The Small MSF variant uses this structure for the MSF File Header:

```text
struct SmallMsfFileHeader {
    uint8_t magic[44];
    uint32_t page_size;
    uint16_t active_fpm;
    uint16_t num_pages;
    uint32_t stream_dir_size;
    uint32_t stream_dir_ptr;      // obsolete; do not read
    uint32_t stream_dir_pages[];
}
```

The `magic` field must have exactly the following contents:

```text
const uint8_t MSF_SMALL_MAGIC[44] = {
    0x4d, 0x69, 0x63, 0x72, 0x6f, 0x73, 0x6f, 0x66, // Microsof
    0x74, 0x20, 0x43, 0x2f, 0x43, 0x2b, 0x2b, 0x20, // t C/C++
    0x70, 0x72, 0x6f, 0x67, 0x72, 0x61, 0x6d, 0x20, // program
    0x64, 0x61, 0x74, 0x61, 0x62, 0x61, 0x73, 0x65, // database
    0x20, 0x32, 0x2e, 0x30, 0x30, 0x0d, 0x0a, 0x1a, //  2.00...
    0x4a, 0x47, 0x00, 0x00,                         // JG..
}
```

`page_size` has the same meaning as in Big MSF.

> Invariant: When using Small MSF, `page_size` is a power of 2 and is in the
> range 512-4096 (0x200-0x1000).

`active_fpm` specifies the page number of the Active FPM. Unlike Big MSF,
`active_fpm` can have several different values.

> TODO: Document what the `active_fpm` constraints are. It's not important right
> now, because we do not need to generate new Small MSF files, only read them.

`num_pages` has the same meaning as in Big MSF.

`stream_dir_size` has the same meaning as in Big MSF.

# Stream Directory

The Stream Directory specifies the number of streams, the size in bytes of each
stream, and the page numbers that make up each stream.

We will first describe the structure of the contents of the Stream Directory.
Then we will describe the on-disk location of the Stream Directory.

There are two variants of the Stream Directory: "Small MSF" and "Big MSF". Both
store the same semantic information, but represent that information differently
and have different limitations. Old MSF is obsolete; all encoders should encode
Big MSF.

## Small MSF

When using Small MSF, the Stream Directory has this structure:

```text
struct SmallStreamDirectory {
    uint16_t num_streams;
    uint16_t padding;
    SmallStreamEntry streams[num_streams];
    uint16_t stream_pages[num_pages_in_all_streams];
}

struct SmallStreamEntry {
    uint32_t stream_size;   // size in bytes of this stream
    uint32_t ignored;       // This used to be an in-memory pointer, so it was meaningless.
}
```

In Small MSF, finding the size of a stream requires indexing into `streams` and
accessing the `stream_size` filed, i.e. `streams[s].stream_size`. Aside from
that difference, the stream size is interpreted in the same way as
`stream_sizes` as defined in Big MSF.

Note that `stream_pages` uses `uint16_t`, not `uint32_t`. Aside from that
detail, `stream_pages` works the same way as defined in Big MSF.

Big MSF improved on Small MSF by upgrading page numbers and `num_streams` from
`uint16_t` to `uint32_t` and by eliminating the useless `ignored` field.

## Determinism

> Determinism: There are no "nil" streams (streams with
> `stream_sizes[i] == NIL_STREAM_SIZE`).

> Determinism: The page numbers of all pages allocated to all streams are
> assigned in strictly-increasing order. That is, the elements in `stream_pages`
> are sorted in ascending order.

These determinism rules are guidelines, not invariants. They are one way to
achieve determinism, but apps that generate PDB/MSF files are not required to
follow these guidelines, as long as the apps still generate data in a
deterministics order.

## Location on disk - Small MSF

# Stream usage

The MSF Container format enables the storage of streams, which store the
significant data of a PDB. This lists some of the important streams in a PDB:

* [PDB Information Stream](pdbi_stream.md) - Contains global information about the
  PDB. This is usually the first stream that any tool reads. It provides the
  binding key (unique GUID, age), PDB version number, and contains the Named
  Streams Table.
* [Debug Information Stream](dbi_stream.md) - Contains pointers to debugging
  information
* [Module Streams](module_stream.md) - Each describes a given module
  (compiland).
* [Names Stream](names_stream.md) - Contains a set of strings, which are
  referenced by other data structures using a small integer (`NameIndex`).
* [TPI Stream](tpi_stream.md) - Describes the types used by a program.
* [IPI Stream](ipi_stream.md) - Contains identifiers and metadata about
  compilation.

# 2-phase commit protocol

MSF provides a 2-phase commit protocol, which allows making a series of
modifications to a PDB file and only committing them with a single update to the
PDB header file.

> TODO: expand on the commit protocol

# Example - Big MSF

This is an example of page 0 of a PDB file:

```text
00000000 : 4d 69 63 72 6f 73 6f 66 74 20 43 2f 43 2b 2b 20 : Microsoft C/C++
00000010 : 4d 53 46 20 37 2e 30 30 0d 0a 1a 44 53 00 00 00 : MSF 7.00...DS...
00000020 : 00 10 00 00 01 00 00 00 bb 51 00 00 00 6b 01 00 : .........Q...k..
00000030 : 00 00 00 00 ba 51 00 00 00 00 00 00 00 00 00 00 : .....Q..........
00000040 : ... zeroes ...
00001000 : (end)
```

* The `magic` field is clearly visible. It is using the "Big MSF" magic value.
* At offset 0x20, `page_size` is 0x1000.
* At offset 0x24, `active_fpm` is 1.
* At offset 0x28, `num_pages` is 0x51bb (20,923). When multipied by `page_size`,
  this gives a file size of 0x51bb000 (85,700,608).
* At offset 0x2c, `stream_dir_size` is 0x16b00 (92,928).
* At offset 0x30, `small_stream_pages_map` is 0. Since `magic` specifies "Big
  MSF", it is expected that this field is zero.
* At offset 0x3c, `big_stream_pages_map` contains one page pointer, whose value
  is 0x51ba.

The page pointer 0x51ba points to a page that contains page numbers that contain
the Stream Directory. Remember that we multiply page numbers by `page_size`
(which is 0x1000 in this example) to get file offsets. At page 0x51ba we see:

```text
051ba000 : a3 51 00 00 a4 51 00 00 a5 51 00 00 a6 51 00 00 : .Q...Q...Q...Q..
051ba010 : a7 51 00 00 a8 51 00 00 a9 51 00 00 aa 51 00 00 : .Q...Q...Q...Q..
051ba020 : ab 51 00 00 ac 51 00 00 ad 51 00 00 ae 51 00 00 : .Q...Q...Q...Q..
051ba030 : af 51 00 00 b0 51 00 00 b1 51 00 00 b2 51 00 00 : .Q...Q...Q...Q..
051ba040 : b3 51 00 00 b4 51 00 00 b5 51 00 00 b6 51 00 00 : .Q...Q...Q...Q..
051ba050 : b7 51 00 00 b8 51 00 00 b9 51 00 00 00 00 00 00 : .Q...Q...Q......
051ba060 : ... zeroes ...
051bb000 : (end)
```

This shows the page numbers for the Stream Directory. The first few page numbers
are 0x51a3, 0x51a4, 0x51a5, etc.

Page 0x51a3 contains the beginning of the Stream Directory:

```text
051a3000 : 2b 09 00 00 e0 66 01 00 db 00 00 00 64 6d ed 00 : +....f......dm..
051a3010 : a2 f1 38 00 70 5a 3a 00 00 00 00 00 00 00 00 00 : ..8.pZ:.........
051a3020 : 7d d1 07 00 b4 38 00 00 4c 8d 01 00 40 fb 00 00 : }....8..L...@...
051a3030 : bc 9d 24 00 18 01 00 00 48 00 00 00 28 01 00 00 : ..$.....H...(...
051a3040 : a4 49 02 00 1c 4d 00 00 9c 32 00 00 2c 3a 00 00 : .I...M...2..,:..
051a3050 : 44 8e 00 00 1c 36 00 00 18 51 00 00 14 37 00 00 : D....6...Q...7..
051a3060 : d4 1f 00 00 b0 34 00 00 b8 2e 00 00 60 4e 00 00 : .....4......`N..
051a3070 : 00 35 00 00 3c 23 00 00 94 4c 00 00 78 3c 00 00 : .5..<#...L..x<..
051a3080 : 80 23 00 00 ec 7d 00 00 78 2c 00 00 ec 28 00 00 : .#...}..x,...(..
051a3090 : 2c 1c 00 00 e4 48 00 00 90 c8 00 00 80 77 02 00 : ,....H.......w..
051a30a0 : b4 15 00 00 d4 2c 00 00 a4 20 00 00 80 d4 00 00 : .....,... ......
051a30b0 : e0 3c 00 00 f4 22 00 00 14 78 00 00 04 36 00 00 : .<..."...x...6..
051a30c0 : f4 2a 00 00 dc 34 00 00 e4 10 00 00 40 44 00 00 : .*...4......@D..
051a30d0 : 7c 25 00 00 00 34 00 00 a8 10 00 00 50 42 02 00 : |%...4......PB..
051a30e0 : 58 58 00 00 08 eb 00 00 4c 6d 00 00 dc 2e 00 00 : XX......Lm......
051a30f0 : ec 3e 00 00 94 54 00 00 44 1f 00 00 44 c6 00 00 : .>...T..D...D...
051a3100 : 08 25 00 00 40 58 00 00 d8 3e 00 00 fc f4 00 00 : .%..@X...>......
051a3110 : bc 69 00 00 bc b8 00 00 7c 21 00 00 f4 39 00 00 : .i......|!...9..
051a3120 : 74 fd 03 00 00 43 00 00 dc 39 00 00 64 4f 00 00 : t....C...9..dO..
051a3130 : fc 77 00 00 98 70 00 00 6c 60 00 00 c4 15 00 00 : .w...p..l`......
051a3140 : 54 2b 00 00 2c e5 01 00 c8 2e 00 00 e0 77 00 00 : T+..,........w..
051a3150 : e0 25 00 00 6c 33 00 00 1c 36 00 00 34 23 00 00 : .%..l3...6..4#..
051a3160 : 7c 58 00 00 ec 10 00 00 a0 38 00 00 3c 3a 00 00 : |X.......8..<:..
051a3170 : d4 09 00 00 58 e6 00 00 40 22 00 00 ec 3d 00 00 : ....X...@"...=..
051a3180 : 3c 1d 00 00 20 57 00 00 e4 31 00 00 e0 45 00 00 : <... W...1...E..
051a3190 : e0 61 00 00 c4 b6 00 00 04 09 00 00 ac 5c 00 00 : .a...........\..
051a31a0 : fc 54 00 00 64 26 00 00 e0 20 00 00 30 47 00 00 : .T..d&... ..0G..
051a31b0 : 28 22 00 00 2c 6e 00 00 64 21 00 00 9c 2c 00 00 : ("..,n..d!...,..
051a31c0 : d4 4c 00 00 74 33 00 00 28 12 00 00 9c 18 00 00 : .L..t3..(.......
051a31d0 : ec 31 00 00 dc 1f 00 00 90 2d 00 00 40 30 00 00 : .1.......-..@0..
051a31e0 : 80 10 00 00 0c 3b 00 00 2c 22 00 00 a0 29 00 00 : .....;..,"...)..
051a31f0 : a8 3d 00 00 a0 25 00 00 7c 2c 00 00 54 50 00 00 : .=...%..|,..TP..
```

At file offset 0x51a3000 we see the `uint32_t` value 0x92b (2347), which is
`num_streams`. The next 2347 values are the contents of `stream_sizes`. Here are
the first few entries from `stream_sizes`:

File Offset | Stream | `stream_sizes[i]` | Num pages needed for this stream | Description
------------|--------|-------------------|----------------------------------|------------
051a3004    | 0      | 000166e0          | 00017                            | "Old Stream Directory"
051a3008    | 1      | 000000db          | 00001                            | [PDB Stream](pdbi_stream.md)
051a300c    | 2      | 00ed6d64          | 00ed7                            | [TPI Stream](tpi_stream.md)
051a3010    | 3      | 0038f1a2          | 00390                            | [DBI Stream](dbi_stream.md)
051a3014    | 4      | 003a5a70          | 003a6                            | [IPI Stream](ipi_stream.md)

The "num pages needed for this stream" is computed by dividing
`stream_sizes[i]` by `page_size` and rounding up.

To find `stream_pages`, we add `num_streams * sizeof(uint32_t)` to the offset of
`stream_sizes`, which is `0x51a3004 + 0x92b * 4 = 0x51a54b0`. At that location
we see:

```text
051a54b0 : 04 00 00 00 05 00 00 00 06 00 00 00 07 00 00 00 : ................
051a54c0 : 0a 00 00 00 a0 50 00 00 a1 50 00 00 a2 50 00 00 : .....P...P...P..
051a54d0 : a3 50 00 00 a4 50 00 00 a5 50 00 00 a6 50 00 00 : .P...P...P...P..
051a54e0 : a7 50 00 00 a8 50 00 00 a9 50 00 00 aa 50 00 00 : .P...P...P...P..
051a54f0 : ab 50 00 00 ac 50 00 00 ad 50 00 00 ae 50 00 00 : .P...P...P...P..
051a5500 : af 50 00 00 b0 50 00 00 b1 50 00 00 a2 51 00 00 : .P...P...P...Q..  <-- stream 1 pages
051a5510 : 85 50 00 00 2b 3c 00 00 2c 3c 00 00 2d 3c 00 00 : .P..+<..,<..-<..
051a5520 : 2e 3c 00 00 2f 3c 00 00 30 3c 00 00 31 3c 00 00 : .<../<..0<..1<..
051a5530 : 32 3c 00 00 33 3c 00 00 34 3c 00 00 35 3c 00 00 : 2<..3<..4<..5<..
051a5540 : 36 3c 00 00 37 3c 00 00 38 3c 00 00 39 3c 00 00 : 6<..7<..8<..9<..
051a5550 : 3a 3c 00 00 3b 3c 00 00 3c 3c 00 00 3d 3c 00 00 : :<..;<..<<..=<..
```

As seen from the table above, the first 0x17 values store the pages for the Old
Stream Directory. The Old Stream Directory contains data from the previous
version of the stream (before the most recent commit). The information in the
Old Stream Directory is not relevant for decoding the current version of the
PDB, so we ignore it.

If we seek ahead by `0x17 * 4` bytes to stream offset 0x51a550c, then we will
find the pages for stream 1, the "PDB Stream". Stream 1 is small (0xdb bytes),
so it occupies only a single stream. That page number is 0x51a2.

At page 0x51a2 we find:

```text
051a2000 : 94 2e 31 01 3d 27 f1 8e 02 00 00 00 63 b7 fc 1c : ..1.='......c...
051a2010 : 72 76 f1 91 c2 b1 f0 28 b6 29 60 bb 63 00 00 00 : rv.....(.)`.c...
051a2020 : 2f 4c 69 6e 6b 49 6e 66 6f 00 2f 54 4d 43 61 63 : /LinkInfo./TMCac
051a2030 : 68 65 00 2f 6e 61 6d 65 73 00 2f 55 44 54 53 52 : he./names./UDTSR
051a2040 : 43 4c 49 4e 45 55 4e 44 4f 4e 45 00 73 6f 75 72 : CLINEUNDONE.sour
051a2050 : 63 65 6c 69 6e 6b 24 31 00 73 6f 75 72 63 65 6c : celink$1.sourcel
051a2060 : 69 6e 6b 24 32 00 73 6f 75 72 63 65 6c 69 6e 6b : ink$2.sourcelink
051a2070 : 24 32 00 65 6d 62 65 64 73 70 64 00 73 72 63 73 : $2.embedspd.srcs
051a2080 : 72 76 00 07 00 00 00 0e 00 00 00 01 00 00 00 f4 : rv..............
051a2090 : 05 00 00 01 00 00 00 01 00 00 00 2c 00 00 00 28 : ...........,...(
051a20a0 : 09 00 00 1a 00 00 00 26 09 00 00 13 00 00 00 07 : .......&........
051a20b0 : 00 00 00 46 00 00 00 2a 09 00 00 00 00 00 00 05 : ...F...*........
051a20c0 : 00 00 00 0a 00 00 00 06 00 00 00 5c 00 00 00 29 : ...........\...)
051a20d0 : 09 00 00 00 00 00 00 dc 51 33 01 00 00 00 00 00 : ........Q3......
051a20e0 : ... zeroes ...
051a3000 : (end)
```

This is the contents of Stream 1, the PDB Stream.

# References
 
* [LLVM: The MSF File Format](https://releases.llvm.org/8.0.0/docs/PDB/MsfFile.html)
* [`msf.h`](https://github.com/microsoft/microsoft-pdb/blob/master/PDB/include/msf.h)

```

`docs/pdb/msfz.md`:

```md
# MSFZ Container Specification

This document specifies the MSFZ Container file format. MSFZ replaces one part
of the design of PDB in order to improve the storage and access costs associated
with PDB files.

PDB files store information in _streams_, which are similar to ordinary files.
PDB uses the Multi-Stream File (MSF) container format for storing streams. MSF
allows for easy modification of existing streams; streams may be added, deleted,
or modified without rebuilding the entire file or accessing unrelated parts of
the file.

Unfortunately, MSF's capabilities for modifying streams come with a significant
cost. MSF files divide streams into fixed-size pages, manage allocation state
for those pages, etc. Because most streams' length is not a multiple of the page
size, space is wasted in the last page of each stream due to fragmentation.
(Losses due to fragmentation have been observed to be between 5% and 8% of
typical PDB size.) MSF's complex structure also complicates the process of
decoding a PDB file because the pages that compose a particular stream can be
stored at any location in the file; a stream's pages are not contiguous.

MSFZ makes a different set of trade-offs. It optimizes for the reader, not the
writer. MSFZ files are intended to be written and then never modified
(in-place); if a tool needs to modify information within an MSFZ file then an
entirely new file generally needs to be created. This allows MSFZ to eliminate
the concept of pages and the bookkeeping related to them (the Free Page Map).

MSFZ also supports transparent, "chunked" compression. MSFZ may compress stream
data, and when it does, it may group together parts of different streams into
the same compression chunk. This allows very large streams to be broken into
fragments, where each fragment can be loaded from disk and decompressed without
needing to load and decompress other chunks. This also allows many small streams
to be grouped together into a single chunk and compressed together.

MSFZ uses a more efficient representation of its Stream Directory, compared to
that of MSF. MSF uses lists of page numbers; for many streams, these page
numbers are sequential. MSFZ does not use pages at all, but MSFZ does allow a
stream to be composed of multiple _fragments_. A fragment is a contiguous
sequence of bytes within a stream; a fragment may be compressed or uncompressed.
This allows contiguous streams to be described very efficiently in the Stream
Directory, even streams that are quite large (such as the TPI Stream).

MSFZ replaces only the MSF layer of PDBs. It does not change the model of
streams or the data stored within those streams. PDB files can be converted from
MSF to MSFZ container format, and the reverse, without losing any information.
MSFZ is thus a simple and low-risk change to how PDB files work, with
substantial benefits in size reduction, while still allowing tools to read only
those parts of a PDB that they require.

For the sake of brevity, we will refer to PDB files that use the MSFZ container
format as "PDZ files". These files use the same PDB data structures, but they
are stored using the MSFZ container format rather than MSF.

PDZ files cannot be read directly by tools (debuggers, etc.) that have not been
updated to read PDZ files. These tools will need to be updated, or the PDZ file
will need to be converted to PDB before it is used by the tool.

Many parts of the software ecosystem rely on the `*.PDB` file extension to
identify symbol files. It would be costly to update the ecosystem (ADO
pipelines, customer workflows, etc.) just to use a new file extension for
PDB/MSF files. However, when it is necessary to distinguish between PDB/MSF and
PDB/MSFZ file, we suggest using the `*.PDZ` file extension for PDB/MSFZ files.

## MSFZ concepts: Streams, Fragments, Chunks

A _stream_ is a sequence of bytes that contain PDB-related data. Streams are
stored within MSF and MSFZ files, but are not necessarily stored "directly" in
the sense of a 1:1 mapping between the bytes in the stream and the bytes in the
container. The concept of a stream is the same in MSF and MSFZ, although the
representation is quite different.

A _fragment_ is a contiguous portion of a stream. A stream is composed of zero
or more concatenated fragments. Each byte of stream data is stored in some
fragment and fragments are non-overlapping. Fragments may be stored _compressed_
or _uncompressed_; these will both be described below. Fragments can be stored
in different locations on disk, in the MSFZ file. (Fragment is a concept only in
MSFZ.) Each stream contains a list of its fragments.

A _chunk_ is the unit of compression for compressed stream data. If a fragment
of a stream is compressed, then it is stored within a chunk. Fragments from more
than one stream may be stored within the same chunk. The Chunk Table lists all
of the chunks in a given MSFZ file.

## MSFZ data structures

The MSFZ file consists of these data structures:

* The MSFZ File Header, which identifies the file format, specifies global
  parameters, and specifies the locations of other data structures. The MSFZ
  File Header is located at file offset 0 and is the only data structure that
  has a fixed location. All other data structures are stored at offsets that are
  specified as fields in other data structures.

* The Stream Directory, which specifies the list of fragments that compose the
  contents of each stream. The list of fragments implicitly specifies the size
  of the stream.

* The Chunk Table, which specifies the size and location of each compressed
  chunk.

* Uncompressed stream data fragments, which contains the contents of streams
  that do not use compression.

* Compressed chunks, which contain stream data that has been compressed.
  Compressed chunks can contain stream data from more than one stream; the
  contents of a single stream may be stored in multiple chunks. The relationship
  between streams and chunks is clarified in later sections of this document.

All MSFZ structures use little-endian (LSB first) byte order. The `a-b` notation
for ranges specifies an inclusive range, e.g. the range `0-31` _includes_ values
0 and 31 (and all values between them).

### MSFZ File Header

An MSFZ file begins with this header, at file offset 0:

```c
struct MsfzFileHeader {
    uint8_t signature[32];                  // Identifies this as an MSFZ file.
    uint64_t version;                       // specifies the version number of the MSFZ file format
    uint64_t stream_dir_offset;             // file offset of the stream directory
    uint64_t chunk_table_offset;            // file offset of the chunk table
    uint32_t num_streams;                   // the number of streams stored within this MSFZ file
    uint32_t stream_dir_compression;        // compression algorithm used for the stream directory
    uint32_t stream_dir_size_compressed;    // size in bytes of the stream directory when compressed (on disk)
    uint32_t stream_dir_size_uncompressed;  // size in bytes of the stream directory when uncompressed (in memory)
    uint32_t num_chunks;                    // number of compressed chunks
    uint32_t chunk_table_size;              // size in bytes of the chunk table
};
// sizeof(MsfzFileHeader) == 80
```

All fields in `MsfzFileHeader` are at offsets that are naturally aligned for the type of the field.

The `signature` field identifies a file as being an MSFZ file. It has this value:

```text
00000000 :  4d 69 63 72 6f 73 6f 66 74 20 4d 53 46 5a 20 43 : Microsoft MSFZ C
00000010 :  6f 6e 74 61 69 6e 65 72 0d 0a 1a 41 4c 44 00 00 : ontainer...ALD..
```

The `version` field specifies the file format version used for the MSFZ container format. It is
not related to the version fields of the data stored within PDB streams; it specifies only the
format of the data specified by this specification. The only supported value is:

```c++
const uint64_t MSFZ_FILE_VERSION_V0 = 0;
```

New version numbers will be assigned if the MSFZ container format changes, such
as moving fields or changing their type. All changes to version are assumed to
be incompatible; a MSFZ reader must only decode an MSFZ file if it supports the
specified version number exactly.

`num_streams` field specifies the number of streams in the file. This value has
the same meaning as the number of streams stored in the PDB/MSF Stream
Directory. This value must always be greater than or equal to 1.

`stream_dir_compression` specifies the compression algorithm used for the Stream
Directory. This is described below.

The `stream_dir_size_compressed` and `stream_dir_offset` fields specify the
location and size in bytes of the compressed Stream Directory within the MSFZ
file. `stream_dir_compression` specifies the compression algorithm used to
compress the Stream Directory.

`stream_dir_size_uncompressed` specifies the size in bytes of the Stream
Directory after it has been decompressed in memory.

`num_chunks` specifies the number of compression chunks, which are described in
the Chunk Table. Each entry in the Chunk Table is described by a `ChunkEntry`
record. Chunk compression is described below. The `chunk_table_size` and
`chunk_table_offset` fields specify the location of the Chunk Table. The Chunk
Table itself is never compressed because it is usually small enough that
compression has an insignificant impact.

### Stream Directory

The Stream Directory describes the streams stored in the MSFZ file. It is
encoded using a variable-length encoding scheme because each stream may be
composed of a variable number of fragments.

The MSFZ File Header specifies number of streams as `num_streams`.

Streams may be _nil streams_. A nil stream does not contain any data but is
distinguishable from non-nil stream with zero-length. (Nil streams in MSFZ exist
for compatibility with MSF.) Nil streams are encoded by a `u32` with the value
0xffff_ffff, and are _not_ followed by any fragment records at all, not even a 0
terminator for the fragment list:

```text
00000000 : ff ff ff ff
```

Non-nil streams are encoded by a sequence of _fragment records_. Each fragment
record describes a contiguous sequence of bytes within a stream. Fragments can
be either _compressed_ or _uncompressed_. See below for a description of
compressed and uncompressed fragments.

Each fragment record starts with a `u32` value which specifies the size in bytes
of the fragment. If the fragment is compressed, then this value gives the
_uncompressed_ size of the fragment. That is, the value always describes the
number of bytes of stream data. The fragment size is always non-zero; the value
0 encodes "end of fragment list", not "zero-length fragment".

The fragment size is followed by a `u64` value that specifies the location and
encoding of the fragment.

The following pseudo-structure describes the structure of a non-nil stream
record:

```c
struct Stream {
    Fragment fragments[];       // variable-length list of fragments
    uint32_t end;               // value is always zero
};

struct Fragment {
    uint32_t size;              // size (uncompressed) of this fragment; is never zero
    uint64_t location;          // bit-packed field containing location of this fragment
};
```

Fragments may be stored in _compressed_ or _uncompressed_ form. The `location`
field specifies whether the fragment is compressed or uncompressed mode, and for
each mode, where to find the contents of the fragment. Bit 63 of `location` is
set to 0 for uncompressed mode and 1 for compressed mode.

The `location` field within `Fragment` is _not_ aligned. The size of `Fragment`
is 12 bytes, not 16 bytes.

The `location` field for uncompressed fragments:

```text
+----+---------------+--------------------------------+
| 63 | 62 ... 48     | 47 ... 0                       |
+----+---------------+--------------------------------+
|  0 | reserved (0)  | file_offset                    |
+----+---------------+--------------------------------+
```

The `location` field for compressed fragments:

```text

+----+---------------------+--------------------------+
| 63 | 62 ... 32           | 31 ... 0                 |
+----+---------------------+--------------------------+
|  1 | first_chunk         | offset_within_chunk      |
+----+---------------------+--------------------------+
```

Keep in mind that the `location` field is stored as 64-bit LSB-first value. Bit
63 is stored in bit 7 of byte 7, not in bit 7 of byte 0.

Fragments always have a non-zero size. We use the value 0 to indicate the end of
a sequence of fragments. There is _not_ a `location` field after the `size` if
`size` is 0.

The size of each stream is computed as the sum of the size of the stream's
fragments. The stream size is not directly represented in the encoded form of
the Stream Directory. Typically, most streams have only a single fragment.

Because the size of a stream is computed as the sum of the size of the stream's
fragments, stream size is not limited to 32-bit lengths. Large streams (those
larger than `1 << 32`) can be encoded using multiple fragments. Decoders should
be prepared for recognizing large streams, even if their implementation cannot
otherwise handle large streams.

If a fragment record describes a compressed fragment, then the `first_chunk`
field is the index, within the Chunk Table, of the first fragment that
contributes to this stream. If the size of the fragment and the
`offset_within_chunk` field exceed the size of that first chunk, then the reader
advances to the next chunk, and reads data from its decompressed form. This
repeats until we reach the end of the fragment records for this stream.

For this reason, the order of entries in the Chunk Table is very important
because it determines the order of the uncompressed data that is stored within
the chunks. You can think of the decompressed form of the chunks as forming a
single "virtual address space" and the fragment records as reading from that
address space.

The location of the _compressed_ form of the chunks on-disk is not important,
however. The encoder may write chunks in any order, as long as the bytes that
compose each chunk are contiguous within that chunk (compressed chunks cannot be
interleaved) and the order of the entries in the Chunk Table is correct.

### Example

This is an example of a stream directory.

```text
offset   : contents
00000000 : 00 00 00 00            // stream 0; has no fragments, size is 0
00000004 : a0 01 00 00            // stream 1, fragment 0, fragment size is 0x1a0
00000008 : 50 00 00 00            // fragment 0 location: uncompressed, file offset 0x50
0000000c : 00 00 00 00
00000010 : 00 00 00 00            // end of stream 1
00000014 : ff ff ff ff            // stream 2 is nil
00000018 : 84 ac 06 00            // stream 3, fragment 0, fragment size is 0x6ac84
0000001c : e0 10 00 00            //     offset_within_chunk: 0x10e0
00000020 : 80 00 00 e5            //     chunk_index e5, with bit 63 set (meaning: compressed chunk)
```

It is expected that most streams will be stored compressed. Compression not only
reduces the size of PDZ files, but typically makes I/O more efficient.

The Stream Directory itself can be compressed. The `stream_dir_compression`
field of the MSFZ File Header specifies the compression algorithm used for the
Stream Directory.

### Uncompressed streams

In uncompressed form, the stream data is stored in a single contiguous sequence
of bytes in the MSF file. The `location` field sets bit 63 to 0. Bits 0-47
contains the file offset of the stream data. Bits 48-62 of `location` are
reserved and are set to 0. The contents of the stream are stored contiguously in
the MSFZ file; MSFZ does not use pages. The stream data may be read directly
from the MSFZ file without any processing.

### Chunk-compressed streams

If a stream contains compressed fragments, then the `location` field of
`Fragment` has bit 63 set to 1 and also contains two bit-packed subfields:
`chunk` (stored in bits 32-62) and `offset_within_chunk` (stored in bits 0-31).
`chunk` specifies the index of the chunk that contains the fragment's data.
`offset_within_chunk` specifies the _decompressed_ byte offset within the chunk
where the stream's data begins. The Chunk Table is described below.

Chunk boundaries are not required to be aligned to fragment boundaries. A chunk
may contain data from more than one fragment and these fragments may come from
different streams. The contents of a single stream may be spread across multiple
fragments, possibly using different compression modes (compressed vs.
uncompressed).

Compressed chunks may be stored in any order on disk.

This diagram illustrates some of the cases:

```text
                               --> layout of file contents -->

-------------------+------------------------------------+---------------------+-----------------
    compressed     | compressed                         | uncompressed        | compressed
... chunk 42       | chunk 43                           | stream data         | chunk 44 ...
-------------------+------------------------------------+---------------------+-----------------
       ↑                        ↑                       ↑                     ↑
       |                        |                       |                     |
------/ \---- stream 10 -------/ \-- stream 11 --------/ \--- stream 12 -----/ \--- stream 13 --
```

* Stream 10 is compressed and spans chunks 42 and 43.
* Stream 11 is compressed and is stored entirely within chunk 43.
* Stream 12 is uncompressed. Its contents are stored in the MSFZ file but there is no entry in the
  Chunk Table for it.
* Stream 13 is compressed and begins in chunk 44.

### Chunk Table

The Chunk Table lists the compressed chunks within the MSFZ file. The location
and size of the Chunk Table is specified in the MSFZ File Header. The Chunk
Table is an array of `ChunkEntry` records:

```c
struct ChunkEntry {
    uint64_t file_offset;               // file offset of the compressed data for this chunk
    uint32_t compression;               // compression algorithm for this chunk
    uint32_t compressed_size;           // size in bytes of the compressed (on-disk) chunk data
    uint32_t uncompressed_size;         // size in bytes of the uncompressed (in-memory) chunk data
};
```

Note that `ChunkEntry` contains unaligned data. The size of `ChunkEntry` is 20
bytes, not 24, and the alignment is 1, not 8.

Each chunk specifies its compression algorithm, and hence different chunks may
use different compression algorithms. Currently, the only supported algorithm is
[`Zstd`](https://github.com/facebook/zstd).

To read any data from a chunk, usually the entire compressed chunk must be read
from disk and decompressed. For this reason, the encoder of an MSFZ file chooses
a chunk size that is a good tradeoff between encoding efficiency and the cost of
reading and decompressing chunks. There is no requirement that different chunks
have the same size, either before or after decompression. This gives encoders a
lot of freedom.

The _uncompressed_ form of consecutive chunks forms a virtual byte array.
Streams that cross chunk boundaries rely on this. If a stream crosses one or
more chunk boundaries, then all of the chunks that contribute to the stream must
be contiguous in the Chunk Table.

Each entry in the Chunk Table specifies the file offset where the compressed
form of that chunk begins, the size in bytes of the compressed chunk (on-disk),
and the size in bytes of the decompressed chunk (in memory). This allows
decoders to allocate buffers of the correct size before reading data from disk.

### Order of chunk contents and uncompressed stream contents

The location and size of each compressed chunk is specified by that chunk's
entry in the Chunk Table. Similarly, the size and location of each uncompressed
stream is stored in that stream's entry in the Stream Directory. Let _fragment_
refer to either a portion of a compressed chunk or to a contiguous sequence of bytes that
are stored in the MSFZ file without any compression.

Fragments may be stored anywhere in the MSFZ file, except for the following
constraints:

* No two fragments may overlap.
* No fragment may overlap the MSFZ File Header.
* No fragment may overlap the Stream Directory.
* No fragment may overlap the Chunk Table.

These constraints _allow_ the following:

* Uncompressed fragments and compressed chunks may appear in any order within
  the MSFZ file.

* The order of records in the Chunk Table is significant, but the location of
  the compressed chunk data within the MSFZ file is not significant. Encoders
  may write compressed chunks in any order, as long as the Chunk Table correctly
  describes their location.

* There may be unused bytes between pieces. These unused bytes can be used for
  alignment padding. Compressed chunks do not need alignment padding. However,
  encoders may choose to align the starting offset of uncompressed file
  structures, to enable direct memory mapping of uncompressed streams.

  Encoders should ensure that unused bytes do not contain data that was not
  intended to be written to disk, such as accidentally-copied data from memory.
  Encoders are encouraged to always zero-fill unused bytes.

This gives encoders considerable freedom when writing MSFZ files. The following
lists some (non-normative, non-exclusive) ideas for using this freedom when
encoding MSFZ files:

* Simplicity: Since encoders may write pieces in any order, they can simply
  write data as it becomes available.
* Locality: Encoders may write the data that they expect to be accessed most
  frequently in neighboring locations within the file. For example, the Stream
  Directory could be written immediately after the MSF File Header, followed by
  the contents of the PDBI Stream and the DBI Stream. This would allow a reader
  to read a lot of useful data in a single read, at the start of the file,
  without decoding any of the contents of file.
* Encoding efficiency: An encoder may compress chunks using multiple threads
  (CPUs). As each CPU finishes writing a chunk, it may directly append the chunk
  data to the output file. This would minimize lock contention and serialization
  within the encoding process. However, this does introduce non-determinism,
  since different CPUs would race for access to the file.

There are other hypothetical ways to use this freedom. The point is, the
specification permits encoders to write pieces in any order, as long as the
Stream Directory and Chunk Table correctly describe the streams.

## Compression algorithms

Several fields identify a compression algorithm. This section enumerates the specified
compression algorithms.

```c++
const uint32_t COMPRESSION_NONE = 0;
const uint32_t COMPRESSION_ZSTD = 1;
const uint32_t COMPRESSION_DEFLATE = 2;
```

## Procedure for opening and validating an MSFZ file

* Read the MSFZ File Header.
  * Verify that `signature` matches `MSFZ_FILE_SIGNATURE`.
  * Verify that `version` matches a supported version. (Currently, the only supported version is `MSFZ_FILE_VERSION_V0`.)
  * Verify that `stream_dir_compression` is a supported compression algorithm.
  * Verify that
    `num_streams * size_of::<StreamEntry>() == stream_dir_size_uncompressed`.
  * Verify that `num_chunks * size_of::<ChunkEntry>() == chunk_table_size`.
  * Verify that the file range for the Stream Directory is valid, given the
    length of the MSFZ file.
  * Verify that the file range for the Chunk Table is valid, given the length of
    the MSFZ file.
  * Read the Stream Directory.
  * Read the Chunk Table.
* For each `ChunkEntry` in the Chunk Table:
  * Verify that the `uncompressed_size` is non-zero
  * Verify that the `compressed_size` is non-zero and that the file range
    implied by the `file_offset` and `compressed_size` is valid, given the
    length of the MSFZ file.
* For each `StreamEntry` in the Stream Directory:
  * If the stream is a nil stream (`size == NIL_STREAM_SIZE`) or a zero-length stream (`size == 0`), then verify that `location == 0`.
  * If the stream is uncompressed, verify that the file range implied by the `location` and `uncompressed_size` is valid.
  * If the stream is compressed, verify that the range of chunks implied by the
    `location` (when unpacked into `first_chunk` and `offset_within_chunk`) and
    `uncompressed_size` is valid. This requires checking the contents of the
    Chunk Table. Also verify that the `uncompressed_size` is non-zero.
* Verify that the byte ranges for all compressed chunks and uncompressed streams
  are non-overlapping and that they do not overlap the File Header, Stream
  Directory, or Chunk Table.

## Procedure for reading data from a stream

Given a stream `s`, an `offset` within the stream to read, and `len` bytes to read:

* Locate `s` in the `StreamEntry` in the Stream Directory. `s` is required to be
  less than `num_streams`.
* If `compression` is `COMPRESSION_NONE`, then use the `location` and
  `uncompressed_size` to read the contents of the stream directly from the MSFZ
  file.
* If `compression` is `COMPRESSION_CHUNKED`, then:
  * Decompose the `location` into `first_chunk` and `offset_within_chunk`. These
    identify the chunk that contains the data at the start of the stream.
    Remember that the chunk may contain data from other streams, which is why
    the `offset_within_chunk` value is necessary.
  * Use the `offset` and `offset_within_chunk` values to scan forward within the
    chunk list. The goal is to find the first chunk that contains the desired
    data (the data at `offset`), rather than simply the first chunk that
    contains data at stream offset 0.
  * If necessary, read chunks from disk and decompress them.
  * Scan forward in the chunk list. Consume `len` bytes of data from chunks,
    crossing chunk boundaries if necessary.

## Author

* [Arlie Davis](ardavis@microsoft.com)

```

`docs/pdb/names_stream.md`:

```md
# Names Stream

The Names Stream stores a set of unique strings (names). This allows other data
structures to refer to strings using an integer index, rather than storing
copies of the same string in many different places.

The stream index for the Names Stream is found in the PDB Information Stream, in
the Named Streams section. The key is `/names`.

The Names Stream has this structure:

```c
struct NamesStream {
    uint32_t signature;
    uint32_t version;
    uint32_t strings_size;
    uint8_t string_data[strings_size];

    // Hash table
    uint32_t num_hashes;                // 'cit' in nmt.h
    NameIndex hash_table[num_hashes];   // `mphashni` in nmt.h
    uint32_t num_names;                 // 'cni' in nmt.h 
};
```

The `signature` field should always be 0xEFFE_EFFE. The `version` field can take
on one of these values:

Name | Value | Description
-----|-------|------------
`V1` |     1 | Uses `LHashPbCb` for hashing strings.
`V2` |     2 | Uses `LHashPbCbV2` for hashing strings.

All observed PDBs use version `V1`.

`strings_size` specifies the size of `string_data`, in bytes. There is no
alignment requirement for `strings_size`.

`string_data` contains the strings stored in each table. Each string is a
NUL-terminated UTF-8 string. These strings are identified by absolute stream
offset from other tables. The `strings_size` field indicates the size in bytes
of the string data that follows the header. Because the size of `string_data`
must be a multiple of 4, there may be padding bytes at the end of `string_data`.

The first string in `string_data` is the empty string. This allows a `NameIndex`
value of 0 to mean "the empty string" without consulting the table and without
needing to subtract 1 from `NameIndex` values before indexing into
`string_data`. This empty string is _not_ counted in `num_names`.

## `NameIndex`

A `NameIndex` value is a `uint32_t` that points into the Names Stream, starting
at the `string_data` array. `NameIndex` identifies strings by location. By
convention, the empty string is always stored first, which means that the
`NameIndex` value 0 always means the empty string.

There are no guarantees made about `NameIndex` values pointing into the middle
of a string. We have not observed any PDBs that contain string indexes that
point to the middle of strings. Decoders should be prepared to handle pointers
to the middle of strings. Encoders should conservatively avoid generating
pointers to the middle of strings, even if this requires duplicating some string
data.

These are the known locations in other parts of the PDB that contain `NameIndex`
values. If the `NameIndex` table is modified (existing strings are moved), then
the `NameIndex` values that point to them will need to be updated to point to
the new location.

* [C13 Line Data](../codeview/line_data.md), File Checksums Subsection. Each
  File Checksum contains a `NameIndex` for the file name.
* `LF_UDT_SRC_LINE` and `LF_MOD_UDT_SRC_LINE` records (in the IPI Stream)
  contain `NameIndex` values that point to file names.

## Hash Table

The Names Stream contains a hash table which accelerates finding strings. The
hash table is stored in `hash_table`; each entry contains a `NameIndex` value or
0 to indicate that the slot is not used. `num_hashes` specifies the number of
elements in the `hash_table` array. The hash table uses
[open addressing](https://en.wikipedia.org/wiki/Open_addressing), also known as
linear probing, to resolve collisions.

`num_names` value specifies the number of names in the table. This value _does
not_ count the empty string at the start of `string_data`.

> TODO: How does this handle hashes that are zero? How do we distinguish an
> empty hash slot from a hash slot that points to a string whose hash code is
> zero?

> Invariant: `num_hashes` must be greater than or equal to `num_names`.

## Querying the hash table

Querying the table begins with a query string `q` and returns either "not found" or the `NameIndex` value that points to `q`.

1. Let `q` be the query string. `q` cannot be the empty string.
2. Let `h = hash(q)`, where `hash` is the hash function implied by `version`.
3. Let `p = h`, where `p` is the current probe location.
4. Loop:
   1. Let `i = hash_table[p]`.
   2. If `i` is zero, then `q` is not in the table, and the search halts with
      "not found".
   3. Read the string at `string_data[i]` and compare it to `q`. If it is equal,
      then we have found the string in the table, and the search halts with
      "found" with a `NameIndex` value of `i`.
   4. Let `i = (i + 1) % num_hashes`.
   5. If `i == h`, then the table is completely full and does not contain `q`;
      the search halts with "not found". Else, continue the loop.

## Building

This section describes how to build the byte representation for a Names Stream.

1. Input: The input is an ordered list of UTF-8 strings. Let `num_strings` be
   the number of strings in the list.
   * Determinism: There are no duplicates in this list.
   * Determinism: The list is sorted. The empty string is the first element in
     the list.

2. Choose a value for `num_hashes`. This value must be greater than or equal to
   `num_strings`. Generally, the larger this number is, the fewer hash
   collisions there will be, but the more space is taken up with unused hash
   slots.

3. Let hashes be a new array for the hash table. Each element is a `uint32_t`
   value and the length of the table is `num_hashes`. Initialize all elements to
   0.

4. Iterate through the set of strings to add to the table. For each string S:
   * Compute `H = hash(S) % num_hashes`.
   * Loop:
     + If `hashes[H] == 0`, then set `hashes[H]` to the `NameIndex` of this
       string. Remember that the `NameIndex` is the byte offset into the strings
       data section for this string.  Exit this loop.
     + If `hashes[H] != 0`, then set `H = (H + 1) % num_hashes` and continue
       this loop.

The requirement that `num_hashes >= num_strings` guarantees that the inner loop
will terminate.

## Determinism

> Determinism: `NameIndex` values should never point into the "middle" of a
> string. If a `NameIndex` value `n` is non-zero, then
> `string_data[n - 1] == 0` (i.e. the previous string has been terminated).

> Determinism: The strings in `string_data` are sorted and are unique.

> Determinism: If `string_data` has padding bytes at the end, then they have the
> value 0.

Strings should be written in a deterministic order, with case-sensitive sorting
being the most obvious option. Take care to ensure that the empty string is
still placed at the start of the stream (at stream offset 8). Duplicate strings
should be eliminated.

## Example

This is the beginning of a Names Stream, showing the header and the first part
of `string_data`. Note the `strings_size` field, which is `c9 13 07 00`, or
0x713c9.

```
00000000 : fe ef fe ef 01 00 00 00 c9 13 07 00 00 6f 6e 65 : .............one
00000010 : 63 6f 72 65 5c 65 78 74 65 72 6e 61 6c 5c 64 64 : core\external\dd
00000020 : 6b 5c 69 6e 63 5c 6e 74 70 6f 61 70 69 2e 68 00 : k\inc\ntpoapi.h.
00000030 : 6f 6e 65 63 6f 72 65 5c 69 6e 74 65 72 6e 61 6c : onecore\internal
00000040 : 5c 73 64 6b 5c 69 6e 63 5c 6e 74 70 73 61 70 69 : \sdk\inc\ntpsapi
00000050 : 5f 78 2e 68 00 6f 6e 65 63 6f 72 65 5c 69 6e 74 : _x.h.onecore\int
00000060 : 65 72 6e 61 6c 5c 73 64 6b 5c 69 6e 63 5c 6e 74 : ernal\sdk\inc\nt
00000070 : 74 70 61 70 69 2e 68 00 6f 6e 65 63 6f 72 65 5c : tpapi.h.onecore\
00000080 : 69 6e 74 65 72 6e 61 6c 5c 73 64 6b 5c 69 6e 63 : internal\sdk\inc
00000090 : 5c 6e 74 70 73 61 70 69 2e 68 00 6f 6e 65 63 6f : \ntpsapi.h.oneco
000000a0 : 72 65 5c 69 6e 74 65 72 6e 61 6c 5c 73 64 6b 5c : re\internal\sdk\
000000b0 : 69 6e 63 5c 72 74 6c 66 65 61 74 75 72 65 63 6f : inc\rtlfeatureco
000000c0 : 6e 66 69 67 74 79 70 65 73 2e 68 00 6f 6e 65 63 : nfigtypes.h.onec
000000d0 : 6f 72 65 5c 65 78 74 65 72 6e 61 6c 5c 64 64 6b : ore\external\ddk
000000e0 : 5c 69 6e 63 5c 6e 74 69 6d 61 67 65 2e 68 00 6f : \inc\ntimage.h.o
000000f0 : 6e 65 63 6f 72 65 5c 69 6e 74 65 72 6e 61 6c 5c : necore\internal\
00000100 : 73 64 6b 5c 69 6e 63 5c 6e 74 65 78 61 70 69 2e : sdk\inc\ntexapi.
00000110 : 68 00 6f 6e 65 63 6f 72 65 5c 69 6e 74 65 72 6e : h.onecore\intern
00000120 : 61 6c 5c 73 64 6b 5c 69 6e 63 5c 6e 74 78 63 61 : al\sdk\inc\ntxca
00000130 : 70 69 5f 78 2e 68 00 6f 6e 65 63 6f 72 65 5c 69 : pi_x.h.onecore\i
00000140 : 6e 74 65 72 6e 61 6c 5c 73 64 6b 5c 69 6e 63 5c : nternal\sdk\inc\
```

The end of the `strings_data` is at 0x713c9 + 0xc (where 0xc is the size of the
header), which is 0x713d5, we find the `num_hashes` and `hash_table` fields:

```
00071370 : 74 2d 6d 73 2d 6f 6e 65 63 6f 72 65 2d 61 70 70 : t-ms-onecore-app
00071380 : 6d 6f 64 65 6c 2d 73 74 61 74 65 72 65 70 6f 73 : model-staterepos
00071390 : 69 74 6f 72 79 2d 69 6e 74 65 72 6e 61 6c 2d 6c : itory-internal-l
000713a0 : 31 2d 31 2d 36 5f 71 75 65 72 79 2e 6f 62 6a 00 : 1-1-6_query.obj.
000713b0 : 6d 69 6e 6b 65 72 6e 65 6c 5c 6b 65 72 6e 65 6c : minkernel\kernel
000713c0 : 62 61 73 65 5c 61 6d 64 36 34 5c 66 69 62 65 72 : base\amd64\fiber
000713d0 : 2e 61 73 6d 00 68 2f 00 00 00 00 00 00 00 00 00 : .asm.h/.........   <-- num_hashes is 68 2f 00 00
000713e0 : ... zeroes ...
000713f0 : 00 7a 0f 01 00 00 00 00 00 00 00 00 00 00 00 00 : .z..............
00071400 : ... zeroes ...
00071410 : 00 43 29 05 00 bd a3 04 00 e1 cc 00 00 09 46 03 : .C)...........F.
00071420 : 00 ed f0 04 00 fd d7 03 00 31 ff 04 00 1c 42 05 : .........1....B.
00071430 : 00 cf 60 02 00 a2 78 06 00 af f0 00 00 6b b6 01 : ..`...x......k..
00071440 : 00 98 3e 03 00 22 98 00 00 00 00 00 00 00 00 00 : ..>.."..........
00071450 : 00 00 00 00 00 00 00 00 00 00 00 00 00 d2 01 03 : ................
00071460 : 00 00 00 00 00 b3 b8 01 00 00 00 00 00 24 aa 01 : .............$..
00071470 : 00 e5 e7 01 00 a4 28 04 00 c9 12 05 00 13 de 02 : ......(.........
00071480 : 00 73 8f 03 00 c5 ad 00 00 00 00 00 00 00 00 00 : .s..............
00071490 : 00 00 00 00 00 00 00 00 00 00 00 00 00 5c 88 00 : .............\..
000714a0 : 00 01 e5 00 00 00 00 00 00 c3 69 03 00 cc 43 03 : ..........i...C.
000714b0 : 00 25 f5 06 00 0d dd 03 00 60 80 00 00 9e e7 05 : .%.......`......
000714c0 : 00 d2 35 03 00 00 00 00 00 00 00 00 00 00 00 00 : ..5.............
```

We find that `num_hashes` is 0x2f68. The first few entries in `hash_table` are
given in the table below; entires with zero values are omitted. Remember that
the values are `NameIndex` values, which point into `string_data`.

Stream Offset | `NameIndex`
--------------|------------
000713f1      | 00010f7a
00071411      | 00052943
00071415      | 0004a3bd
00071419      | 0000cce1
0007141d      | 00034609

Remember that the stream offset of the `string_data` field is 12 (the size of
the fields that precede `string_data`), so we add 12 to each of these
`NameIndex` values and look up their strings. For example, at 0x10f7a + 12 =
0x10f86, we find:

00010f80 : 61 70 69 2e 68 00 6f 6e 65 63 6f 72 65 5c 65 78 : api.h.onecore\ex
00010f90 : 74 65 72 6e 61 6c 5c 73 64 6b 5c 69 6e 63 5c 69 : ternal\sdk\inc\i
00010fa0 : 73 6f 6c 61 74 69 6f 6e 2e 68 00 6f 6e 65 63 6f : solation.h.oneco

So the first string, whose `NameIndex` is 0x10f7a, decodes as
`onecore\external\sdk\inc\isolation.h`.

## References

* [`class NMT`](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/PDB/include/nmt.h#L26)

```

`docs/pdb/pdb.md`:

```md
# Program Database (PDB) Files

This directory describes Program Database (PDB) files. PDBs contain a variety of
information about executable images, including:

* CodeView debugging information, 
* the list of modules (OBJ files)
* linker sections and subsections
* source file references
* NatVis debugger extensions
* etc.

## Container and Streams

PDB files are stored in a _container_ format. Containers provide the abstraction
of _streams_, which are similar to files within a ZIP archive. Most streams are
identified by number, not by file name. Some streams are identified by name; the
mapping from stream name to stream number is stored in the 
[PDB Information Stream](./pdbi_stream.md).

The [Multi-Stream File (MSF) Container Format](msf.md) is used by most compilers
and debuggers. This document is essential for understanding PDBs at the lowest
level.

The [MSF Compressed (MSFZ) Container Format](msfz.md) is a container format that
is optimized for storage efficiency. It provides the same stream abstraction as
the MSF container format but uses a different on-disk representation.

## PDB Information Stream - Fixed Stream 1

The [PDB Information Stream](pdbi_stream.md) contains important information
about the entire PDB, such as the binding key (GUID and age), the named streams
table, and the PDB version. Most programs that read PDBs will read the PDB
Information Stream as one of the first steps.

The PDB Information Stream also contains a table of _named streams_. Named
streams allow tools to insert arbitrary information into PDBs, such as NatVis
files, source code, SourceLink metadata, etc.

The PDB Information Stream is always stream 1.

## Debug Information Stream (DBI) - Fixed Stream 3

The [Debug Information Stream (DBI)](dbi.md) is a central data structure for
debugging. It is always stream 3.

The DBI contains:

* the list of all modules (OBJ files) that were linked into the program
* the list of all sources files that were compiled

- [DBI Modules Substream](dbi_modules.md) - Lists all of the modules (OBJ files)
  that were linked into this program. Many data structures refer to modules by
  index. This table defines the meaning of those module indexes.

- [DBI Sources Substream](dbi_sources.md) - DBI subsection listing source files
  used when compiling each module

- [Optional Debug Streams](dbi_opt_debug.md) - Contains a set of optional debug
  streams. These describe [fixups](dbi_fixups.md), exception data, COFF section
  headers, and a variety of other data.

- [Section Map and Contributions](dbi_sections.md) - Describes the contributions
  (fragments) of each module and how they map to the executable image sections.

## Global Symbols Stream

The [Global Symbols Stream (GSS)](globals.md) contains information about symbols
that are used across the entire program, or are exported by an executable (DLL
exports).

The stream number of the Global Symbol Stream (and its related index streams)
can be found in the Debug Information Stream Header.

The debug records (symbols) stored within the Global Symbols Stream are
described in [CodeView Debugging Records](../codeview/codeview.md).

## Module Streams

[Module Streams](module_stream.md) describes the code and data within each
module (OBJ file). Each module (OBJ file) has its own module stream. Module
streams are optional.

Use the [DBI Modules Substream](dbi_modules.md) to find the list of modules and
their stream indexes.

The debug records (symbols) stored within Module Streams are described in
[CodeView Debugging Records](../codeview/codeview.md).

## Type Stream (TPI) - Fixed Stream 2

The [Type Stream (TPI)](tpi_stream.md) describes the TPI Stream, which contains
a sequence of related type records.

## Items Stream (IPI) - Fixed Stream 4

The [IPI Stream](ipi_stream.md) describes the IPI Stream, which contains various
ids instruction addresses to source locations

## Names Streams - `/names`

The [Names Stream](names_stream.md) describes the `/names` stream, which
contains a set of names (mostly file names) that are referenced by many data
structures. Several record types within the IPI point into the Names Stream.
The integers which point into the Names Stream use the `NameIndex` alias.

## Named Streams

PDB files may also contain named streams, which are identified by name and are
listed within the Named Stream Table within the
[PDB Information Stream](./pdbi_stream.md).

* NatVis streams, which contain XML type descriptions for visualizing types
  during debugging. These are identified by file name as named streams, e.g.
  `my_types.natvis`.

* Source file contents (not merely source file names but their entire contents)
  may be stored within streams. These are identified by file name as named
  streams, e.g. `my_generated_code.cpp`.

* Source code linking information, which allows debuggers to find
  the correct source code for a given binary. See [Source Link](https://github.com/dotnet/designs/blob/main/accepted/2020/diagnostics/source-link.md#source-link-file-specification).  Source Link information is stored in
  a named stream.

## Hash Algorithms

[Hash Algorithms](hashing.md) describes hash functions used by several PDB
tables and records

## Relationships

[Relationships](relationships.md) describes values that "point" from one
data structure into another. These relationships must be preserved when PDBs are
modified.

## Mini PDB (obsolete)

[mini_pdb.md](mini_pdb.md) - Mini PDBs (fast PDBs) generated with
`/DEBUG:FASTLINK`

```

`docs/pdb/pdbi_stream.md`:

```md
# PDB Information Stream (Fixed Stream #1)

The PDB Information Stream contains a variety of global information. It is
generally the first stream that tools read when they open a PDB file. It
contains:

* The binding key (GUID and age) link the PDB to its corresponding executable
  image. This is what tools like WinDbg use to verify that a PDB describes a
  loaded module.
* The version of the MSVC tools that generated this PDB.
* The Named Stream Map.
* The Feature List.

The PDB Information Stream is usually fairly small.

```c
// sizeof = dynamic
struct PdbStream {
    PdbStreamHeader header;
    NamedStreams named_streams;
    uint32_t feature_codes[];
};

// sizeof = dynamic (depends on version)
struct PdbStreamHeader {
    uint32_t version;
    uint32_t signature;
    uint32_t age;
    GUID unique_guid;           // present only in version PDBImpvVC70 and later; see below
};
```

## Version

The version field identifies how to interpret many of the data structures in this file. This version number changes very infrequently; changing the version number generally means that all existing code needs to be updated to read the new PDB format.

Known version numbers (from pdb.h in [MS-PDB]):

Name             | Value (decimal) | Has GUID? | Notes
-----------------|-----------------|-----------|-----------
`PDBImpvVC2`     | 19941610        | No        | The PDBI contains only the `PdbStreamHeader`. There is no named streams table after it. This version is no longer supported by MS tools.
`PDBImpvVC4`     | 19950623        | No        | 
`PDBImpvVC41`    | 19950814        | No        | 
`PDBImpvVC50`    | 19960307        | No        | 
`PDBImpvVC98`    | 19970604        | No        | 
`PDBImpvVC70Dep` | 19990604        | No        | Deprecated VC 7.0 version
`PDBImpvVC70`    | 20000404        | Yes       | 
`PDBImpvVC80`    | 20030901        | Yes       | 
`PDBImpvVC110`   | 20091201        | Yes       | 
`PDBImpvVC140`   | 20140508        | Yes       | 

Before VC70, `PdbStreamHeader` did not contain a GUID field.

See <https://github.com/microsoft/microsoft-pdb/blob/master/langapi/include/pdb.h> for version number definitions.

## Named Streams

The Named Stream Map immediately follows the `PdbStreamHeader` and has this structure:

```c
struct NamedStreams {
    uint32_t keys_size;                  // The size of the keys data, in bytes
    uint8_t keys_data[keys_size];        // String data for the keys
    uint32_t num_names;                  // The number of entries in hash_entries.
    uint32_t hash_size;                  // The number of entries in the hash table, in memory.
    uint32_t present_mask_size;          // The number of values in present_mask
    uint32_t present_mask[num_present];  // Bit mask of the values that are "present"
    uint32_t deleted_mask_size;          // The number of values in deleted_mask
    uint32_t deleted_mask[num_deleted];  // Bit mask of the values that are "deleted"
    HashEntry hash_entries[name_count];

    // The following are obsolete. It is expected that num_name_index is always zero.
    uint32_t num_name_index;
    NameIndex name_indexes[num_name_index];
};

struct HashEntry {
    uint32_t key;                        // byte offset into `keys` of NUL-terminated string
    uint32_t value;                      // stream index
};
```

`NamedStreams` is a map from a stream name to a stream index. In memory, it is
implemented as a hash table, and the on-disk representation preserves some
information about the hash table. Understanding the on-disk representation,
especially when generating or modifying `NamedStreams`, requires understanding
the in-memory representation.

The in-memory representation consists of these elements:
* An array of `keys` (strings, in this case)
* A corresponding array of `values` (stream indexes, in this case)
* A bitmask of `present` items
* A bitmask of `deleted` items

 The hash table uses
 [open addressing](https://en.wikipedia.org/wiki/Open_addressing), also known as
 linear probing, to resolve collisions. To add an item `(k, v)` to the table:

   1. Let `h = hash(k)`
   2. Let `p = h`, where `p` is the current probing location.
   3. Loop:
      1. If `present[p] == false`, then set `keys[p] = k`, set
         `values[p] = value`, set `present[p] = true`, **and** set
         `deleted[p] = false`. The algorithm halts.
      2. Set `p = (p + 1) % hash_size`.
      3. If `p == h`, then the hash table is full, and it must be resized.
         (Resizing is out of the scope of this description.)
      4. Continue looking for available entries.

To delete an item `k` from the table:
  1. Let `h = hash(k)`
  2. Let `p = h`, where `p` is the current probing location.
  3. Loop:
     1. If `present[p] == true`, use `keys[p]` to read the key string for `p`
        and compare it to `k`. If they are equal, then set `present[p] = false`
        and set `deleted[p] = true`. The algorithm halts.
     2. If `deleted[p] == false`, then `k` is not present in the table. The algorithm halts.
     3. Set `p = (p + 1) % hash_size`.
     4. If `p == h`, then the hash table is full and `k` was never found. The algorithm halts.
     4. Continue looking for available entries.

Thus, the `deleted` bitmask enables the linear probing algorithm to continue
searching for items, even when an item has been deleted. This is necessary
because the algorithm that deletes an item does not move existing items, to
repair damage from hash collisions.

This clarifies the purpose of the `hash_size` field in the on-disk
`NamedStreams` record. This is the size (number of entries) in the `keys` and
`values` tables, in memory. The `present_mask` and `deleted_mask` arrays are the
serialized forms of the bitmasks.

`present_mask` and `deleted_mask` are both LSB-ordered bitmasks (bit vectors).
Both of these bitmasks define a property of the hash table.

> Invariant: `present_mask_size >= num_hashes * 4`

> Invariant: `present_mask_size == deleted_mask_size`

> Invariant: The `present_mask` and `deleted_mask` bitmasks are disjoint. A hash
> table entry cannot be both "deleted" and "present".

The `keys_size` field specifies the size in bytes of `keys_data`, which contains
NUL-terminated UTF-8 strings.

Each `HashEntry` is an entry in the Named Streams Table. The `key` field within
`HashEntry` points into the `keys_data` array, and points to the start of a
NUL-terminated UTF-8 string. The `value` entry is the stream index (the named
stream). All of the entries in `HashEntry` are valid.

The order of items in `HashEntry` is determined by two things:
  1. The hash of the stream name.
  2. The order of items as they were added to the Named Streams Table, because
     the order determines how hash collisions are resolved.

### Simple decoding

Decoders may choose to ignore the hashing scheme entirely. The number of named
streams is usually fairly small and changes infrequently. To load the
`NamedStreams` table, decoders will need to find `hash_entries` data structure,
which requires using the `present_mask_size` and `deleted_mask_size` to find the
offset of `hash_entries`. Decoders can then iterate the entries in
`hash_entries` and ignore the contents of `present_mask` and `deleted_mask`.

### Encoding

Correctly encoding `NamedStreams` requires understanding the linear probing hash
algorithm and generating correct values for `present_mask`. Because the number
of named streams is generally small, encoders can simply regenerate
`NamedStreams` whenever the table is modified.

To do so, start with the following inputs:

1. Let `names` be a vector of strings. Let `num_names` be the length of this vector.
2. Let `values` be a vector of string indexes. The length of this vector is
   equal to the length of `names`.

Then:
1. Choose a value for `num_hashes`. The value must be greater than or equal to
   `num_names`. A larger value will reduce hash collisions, at the cost of
   larger sizes for the `present_mask` and `deleted_mask`. A simple choice would
   be `num_hashes * 1.5`.
2. Create a `present_mask` bit vector whose size is `num_hashes`.
3. Let `hash_entries` be a new empty vector, whose item type is `HashEntry`.
4. Let `keys_data` be a new empty vector of bytes. This will contain key data (strings).
5. For each string `keys[i]` in `keys`:
   1. Let `offset` = the current length of `keys_data`. Append `keys[i]`
      (including the NUL terminator) to `keys_data`.
   1. Let `h = hash(keys[i])`.
   2. Let `p` = `h`, the current probing location.
   3. Loop:
      1. If `present_mask[p] == false`, then set `present_mask[p] == true`,
         append a new record to `keys_data` and break this loop, continuing to
         the `keys[i]` loop.
      2. Set `p = (p + 1) % num_hashes`.
      3. Continue the loop.

The contents of `deleted_mask` is a vector with the same size as `present_mask`,
but whose contents is all zeroes (no deleted items).

### Name index table (obsolete)

The `num_name_index` and `name_indexes` fields are obsolete. `num_name_index`
should always be zero.

## Feature Codes

After the Named Streams Table is an optional list of feature codes. Each feature
code is a `uint32_t` value that specifies whether a specific feature is enabled.
If the Feature Codes table is present, then it covers all data within the PDB
Information Stream that follows the Named Streams Table. (It appears to be a
backward-compatible addition to the PDB file format.)

These are the known feature codes. Feature codes in the table below may be in
decimal or hexadecimal.

Code                 | Name                | Description
---------------------|---------------------|------------
20091201             | `VC110`             |
20140508             | `VC140`             |
0x4D544F4E           | `NoTypeMerge`       |
0x494E494D (`MINI`)  | `MinimalDebugInfo`  | See [Mini PDBs](mini_pdb.md)

## Example

The following is an example of the entire PDB Information Stream from an example PDB.

```
00000000 : 94 2e 31 01 3d 27 f1 8e 02 00 00 00 63 b7 fc 1c : ..1.='......c...
00000010 : 72 76 f1 91 c2 b1 f0 28 b6 29 60 bb 63 00 00 00 : rv.....(.)`.c...
00000020 : 2f 4c 69 6e 6b 49 6e 66 6f 00 2f 54 4d 43 61 63 : /LinkInfo./TMCac
00000030 : 68 65 00 2f 6e 61 6d 65 73 00 2f 55 44 54 53 52 : he./names./UDTSR
00000040 : 43 4c 49 4e 45 55 4e 44 4f 4e 45 00 73 6f 75 72 : CLINEUNDONE.sour
00000050 : 63 65 6c 69 6e 6b 24 31 00 73 6f 75 72 63 65 6c : celink$1.sourcel
00000060 : 69 6e 6b 24 32 00 73 6f 75 72 63 65 6c 69 6e 6b : ink$2.sourcelink
00000070 : 24 32 00 65 6d 62 65 64 73 70 64 00 73 72 63 73 : $2.embedspd.srcs
00000080 : 72 76 00 07 00 00 00 0e 00 00 00 01 00 00 00 f4 : rv..............
00000090 : 05 00 00 01 00 00 00 01 00 00 00 2c 00 00 00 28 : ...........,...(
000000a0 : 09 00 00 1a 00 00 00 26 09 00 00 13 00 00 00 07 : .......&........
000000b0 : 00 00 00 46 00 00 00 2a 09 00 00 00 00 00 00 05 : ...F...*........
000000c0 : 00 00 00 0a 00 00 00 06 00 00 00 5c 00 00 00 29 : ...........\...)
000000d0 : 09 00 00 00 00 00 00 dc 51 33 01                : ........Q3.
```

* At stream offset 0 is the `PdbStreamHeader`.
  + `version` is `94 2e 31 01`, or 0x01312e94, or 20,000,404 in decimal, which is `PDBImpvVC70`.
  + `signature` is `3d 27 f1 8e`, or 0x8ef1273d.
  + `age` is `02 00 00 00`, or 2.
  + `unique_id` is `63 b7 fc 1c 72 76 f1 91 c2 b1 f0 28 b6 29 60 bb`.

The `NamedStreams` table begins at stream offset 0x1c. The `keys_size` is
`63 00 00 00`, or 0x63. Note that this value is not aligned (not a multiple of
4). This means that many of the `uint32_t` values in the table after it are not
aligned.

The `keys_data` ranges starts at stream offset 0x20 and ends at 0x20 + 0x63 =
0x83:

```
00000020 : 2f 4c 69 6e 6b 49 6e 66 6f 00 2f 54 4d 43 61 63 : /LinkInfo./TMCac
00000030 : 68 65 00 2f 6e 61 6d 65 73 00 2f 55 44 54 53 52 : he./names./UDTSR
00000040 : 43 4c 49 4e 45 55 4e 44 4f 4e 45 00 73 6f 75 72 : CLINEUNDONE.sour
00000050 : 63 65 6c 69 6e 6b 24 31 00 73 6f 75 72 63 65 6c : celink$1.sourcel
00000060 : 69 6e 6b 24 32 00 73 6f 75 72 63 65 6c 69 6e 6b : ink$2.sourcelink
00000070 : 24 32 00 65 6d 62 65 64 73 70 64 00 73 72 63 73 : $2.embedspd.srcs
00000080 : 72 76 00 07 00 00 00 0e 00 00 00 01 00 00 00 f4 : rv..............
```

* At stream offset 0x83 is `num_names`, whose value is 7.
* At stream offset 0x87 is `hash_size`, whose value is 0xe = 14.
* At stream offset 0x8b is `present_mask_size`, whose value is 1.
* The `present_mask` starts at stream offset 0x8f and ends at 0x93 (exclusive).
  It contains the byte values `f4 05 00 00`, implying a bit vector of
  `0 0 0 1 1 1 1 1, 1 0 1 0 0 0 0 0, 0 0 0 0 0 0 0 0, 0 0 0 0 0 0 0 0`
* At stream offset 0x93 is `deleted_mask_size`, whose value is 1.
* At stream offset 0x97 is `deleted_mask`. It contains the byte values
  `01 00 00 00`, implying a bit vector of `1 0 0 ... 0`.
* At stream offset 0x9b is `hash_entries`. By using the bitmasks in
  `present_mask` and `deleted_mask`, and by looking up the key strings in
  `keys_data`, we can build the full hash entry:

Stream offset |Index| Key      | Key string         | Value (Stream)<br>(hex)
--------------|-----|----------|--------------------|---------------
0000009b      | 0   | 0000002c | `sourcelink$1`     | 00000928
000000a3      | 1   | 0000001a | `/UDTSRCLINEUNDONE`| 00000926
000000ab      | 2   | 00000013 | `/names`           | 00000007
000000b3      | 3   | 00000046 | `sourcelink$2`     | 0000092a
000000bb      | 4   | 00000000 | `/LinkInfo`        | 00000005
000000c3      | 5   | 0000000a | `/TMCache`         | 00000006
000000cb      | 6   | 0000005c | `srcsrv`           | 00000929

# References

* [LLVM: The PDB Info Stream](https://llvm.org/docs/PDB/PdbStream.html)
* [`class Map`](https://github.com/microsoft/microsoft-pdb/blob/master/PDB/include/map.h) - equivalent to `NamedStreams`
* [`PDBCommon::featNoTypeMerge`](https://github.com/microsoft/microsoft-pdb/blob/master/PDB/include/pdbcommon.h) - describes the feature codes
* [`GSI1::GSI1`](https://github.com/microsoft/microsoft-pdb/blob/master/PDB/dbi/gsi.cpp)
  - The `GSI1::GSI1` constructor chooses the value for `num_buckets` (called
  `iphrHash`) based on the presence of the `MinimalDebugInfo` feature.

```

`docs/pdb/relationships.md`:

```md
# Relationships between data: References 

Many of the data structures in PDBs are related to other each other. This can take many forms:

* Some streams contain stream indices of other streams. If information is moved
  from one stream index to another (during a PDB rebuild), then the streams
  which pointed to the old stream numbers need to be updated to point to the new
  ones.

  + Example: The Debug Information (DBI) stream contains stream indices for
    Module Information streams, the Global Symbol Stream (GSS), the Global
    Symbol Index (GSI), etc.

  + Example: The PDB Information Stream contains stream indexes for named
    streams.

* Some records contain byte offsets that point into other streams.

  + Example: The GSI contains byte offsets that point into the contents of the
    GSS.

* Some records contain record indices that refer to records in other streams.

  + Example: Symbol records refer to type records using a TypeIndex.

* Some bounds (array sizes) or other parameters are stored in headers or
  separate streams.

For a PDB reader, these references are obviously needed to get the job done;
it’s always obvious that you need to read the DBI before you can find the right
stream that contains the GSI.

However, these references are much more important for apps that create or modify
PDBs, because editing information may invalidate information. It is important to
understand the relationships between all of the data in PDBs, so that the
relationships can be preserved when creating or modifying PDBs. In some cases
the edits desired are minor, such as adding a new NatVis XML file to an existing
PDB. In other cases, like achieving determinism, we need a rigorous and formal
approach to guarantee correctness.

Formally, we define the reference graph of all data structures within a PDB. The
reference graph is acyclic; it does not make sense for a data structure to
depend on itself, directly or indirectly.

The reference graph allows us to reason about how to make changes to a PDB while
preserving all the invariants of the PDB. The reference graph will answer
questions, such as: If I change the order of the records in table T, what other
tables do I need to update? And transitively, what other tables (indirect
dependencies) do I need to update?

Achieving determinism requires a topology-walk through these dependencies in one
direction.

This diagram illustrates many of the references that connect the modules,
symbols, and types data structures.

```mermaid
flowchart TB;

dbi[["DBI"]]
dbi_modules["DBI Module Info"]
dbi_sources["DBI Sources"]

subgraph "TPI and IPI"
    ipi[["IPI Stream"]]
    ipi_hash[["IPI Hash Stream"]]
    ipi_hash --"byte<br>offset"--> ipi

    tpi[["TPI Stream"]]
    tpi_hash[["TPI Hash Stream"]]
    tpi_hash --"byte<br>offset"--> tpi
end

gss[["GSS"]]
gsi[["GSI"]]
psi[["PSI"]]
names_stream[["Names Stream"]]

dbi_opt_headers["DBI Optional Headers"]
optional_dbg_header_stream[["Optional Debug Headers"]]

dbi --contains--> dbi_modules
dbi --contains--> dbi_sources
dbi --contains--> dbi_opt_headers

dbi_opt_headers --"StreamIndex"--> optional_dbg_header_stream

dbi --StreamIndex--> gss
dbi --StreamIndex--> gsi
dbi --StreamIndex--> psi

subgraph Globals
    gss
    gsi
    psi
end

subgraph module_stream_graph [Module Stream]
    module_stream[["Module Stream"]]
    module_stream_globalrefs["Module<br>GlobalRefs<br>Substream"]
    module_symbols["Module Symbols<br>Substream"]
    c13line["C13 Line Data<br>Substream"]

    module_stream --contains--> module_symbols
    module_stream --contains--> c13line
    module_stream --contains--> module_stream_globalrefs
end

dbi_modules --StreamIndex--> module_stream
dbi_sources --NameIndex--> names_stream

gsi --"byte offset"--> gss
psi --"byte offset"--> gss
gss --TypeIndex----> tpi

module_symbols --TypeIndex--> tpi
module_symbols --ItemId--> ipi

module_stream_globalrefs --"byte offset"--> gss

c13line --NameIndex--> names_stream
c13line --"Mod#,File"--> dbi_sources

ipi --TypeIndex--> tpi

pdbi[["PDB Info"]]
named["Named Streams"]
tmcache_map[["TMCache Map"]]
tmcache_stream[["TMCache Stream"]]
natvis_stream["NatVis Streams"]

pdbi --contains--> named
named --StreamIndex--> natvis_stream
named --StreamIndex--> tmcache_map
tmcache_map --"StreamIndex"--> tmcache_stream

named --StreamIndex--> srcsrv_stream["SrcSrv"]
named --StreamIndex--> names_stream
```

* Each node is a data structure.

* Edges represent pointers from one data structure to another, which need to be
  updated when the pointed-to data structure is modified.

* Double-boxed nodes represent streams.

References within nodes are _not_ shown. For example, the TPI Stream contains
`TypeIndex` values, which point into the TPI Stream. These self-edges are not
shown in the diagram above, for the sake of clarity.

```

`docs/pdb/tpi_stream.md`:

```md
# TPI Stream: Type Database (Fixed Stream 2)

Each PDB contains a TPI Stream, also called the Type Database. The TPI Stream is
always stored in stream 2. The TPI is one of the central data structures of the
PDB; many other structures refer to it, using `TypeIndex` values.

Symbol records stored in the Global Symbol Stream (GSS) and in module symbol
streams refer to type definitions stored in the TPI Stream using `TypeIndex`
values.

For the set of type records which can be stored in the TPI, see
[Types](../codeview/types/types.md).

# Type Stream Header

The Type Stream consists of a header, followed by a series of variable-length
type records.

The header:

```c
// sizeof = 56
struct TypeStreamHeader {
    uint32_t version;
    uint32_t header_size;

    // Fields for Type Records
    uint32_t type_index_begin;      // TypeIndex of the first record; almost always 0x1000
    uint32_t type_index_end;        // Exclusive upper bound of the TypeIndex of the last type record
    uint32_t type_record_bytes;     // The size, in bytes, of the type record data following the header.

    // Fields for Type Hash Stream
    uint16_t hash_stream_index;         // Stream index of the Type Hash Stream, or 0xffff
    uint16_t hash_aux_stream_index;     // Stream index of the Type Hash Aux Stream, or 0xffff
    uint32_t hash_key_size;
    uint32_t num_hash_buckets;
    int32_t  hash_value_buffer_offset;
    uint32_t hash_value_buffer_length;
    int32_t  index_offset_buffer_offset;
    uint32_t index_offset_buffer_length;
    int32_t  hash_adj_buffer_offset;
    uint32_t hash_adj_buffer_length;
};
```

An example of a Type Stream Header:

```
00000000 : 0b ca 31 01 38 00 00 00 00 10 00 00 af 1a 04 00 : ..1.8...........
00000010 : 2c 6d ed 00 25 09 ff ff 04 00 00 00 ff ff 03 00 : ,m..%...........
00000020 : 00 00 00 00 bc 2a 10 00 bc 2a 10 00 80 3a 00 00 : .....*...*...:..
00000030 : 3c 65 10 00 00 00 00 00 06 00 01 12 00 00 00 00 : <e..............
00000040 : 0e 00 08 10 03 00 00 00 00 00 00 00 00 10 00 00 : ................
00000050 : 7e 00 03 12 02 15 03 00 00 00 50 6f 77 65 72 55 : ~.........PowerU
00000060 : 73 65 72 50 72 65 73 65 6e 74 00 f1 02 15 03 00 : serPresent......
00000070 : 01 00 50 6f 77 65 72 55 73 65 72 4e 6f 74 50 72 : ..PowerUserNotPr
00000080 : 65 73 65 6e 74 00 f2 f1 02 15 03 00 02 00 50 6f : esent.........Po
00000090 : 77 65 72 55 73 65 72 49 6e 61 63 74 69 76 65 00 : werUserInactive.
000000a0 : 02 15 03 00 03 00 50 6f 77 65 72 55 73 65 72 4d : ......PowerUserM
000000b0 : 61 78 69 6d 75 6d 00 f1 02 15 03 00 03 00 50 6f : aximum........Po
000000c0 : 77 65 72 55 73 65 72 49 6e 76 61 6c 69 64 00 f1 : werUserInvalid..
```

The `header_size` field specifies the actual size of this header. The Type
Records immediately follow the Type Stream Header, at the offset given by
`header_size`. Readers should use the `header_size` value, rather than assuming
that the header has a fixed (compile-time) size.

> Invariant: The `header_size` field cannot be less than 56.

> Invariant: The `header_size` field must be a multiple of 4. This is required
> so that the type records begin on an aligned boundary.

> Invariant: Although the `hash_value_buffer_length`,
> `index_offset_buffer_length`, and `hash_adj_buffer_length` fields are typed as
> signed integers, these fields should never be negative.
Version

The version field specifies the structure of the Type Stream. These are the
defined values for version, in decimal. The names and values come from the
`/PDB/dbi/tpi.h` header file [MS-PDB].

Name             | Value (decimal) | Description
-----------------|----------|-----------
`impv40`         | 19950410 |
`impv41`         | 19951122 |
`impv50Interim`  | 19960307 |
`impv50`         | 19961031 |
`impv70`         | 19990903 |
`impv80`         | 20040203 | The current version. Uses `uint32_t` for the hash keys, and uses CRC-32 for the hashes.

All decoders should expect `impv80` version. All encoders should generate `impv80`.

# Type Index Range

The `type_index_begin` and `type_index_end` fields specify the range of Type
Index values within the type stream, and implicitly specify the number of type
records in the Type Stream.

The `type_index_begin` value specifies the Type Index of the first type record
stored in the stream. Type Index values that are less than `type_index_begin`
are reserved for primitive types, such as void, unsigned int, etc. The value of
`type_index_begin` must be greater than or equal to 4096 (0x1000), because the
first 4096 values are reserved for encoding primitive types. In fact, all
observed PDBs use a value of 4096 for `type_index_begin`; encoders should always
use a value of 4096, because many decoders may assume a fixed value.

`type_index_begin` is a very important parameter. Interpreting many data
structures within the PDB requires knowing the value of `type_index_begin`.

`TypeIndex` values that are numerically less than `type_index_begin` identify
[primitive types](../codeview/primitive_types.md). Primitive types are those
that have a fixed interpretation, such as `int`, and do not require a type
record to describe them.

> Invariant: `type_index_begin` must be greater than or equal to 4096.

The `type_index_end` field specifies the exclusive upper bound of Type Index
values. Thus, the number of type records is equal to
`type_index_end - type_index_begin`.

> Invariant: `type_index_end` must be greater than or equal to `type_index_begin`.

> Invariant: The number of type records stored in the Type Stream must be equal
> to `type_index_end - type_index_begin`.

# Type Records Substream

After the Type Stream Header, the rest of the data in the Type Stream stores
Type Records. The `type_record_bytes` field specifies the length of the type
records, in bytes. The type records are stored within the Type Stream, starting
at the byte offset given by `header_size`.

> Invariant: type_record_bytes must be a multiple of 2.

> Invariant: All type records begin on a 2-byte aligned boundary, within the stream.

> Invariant: header_size + type_record_bytes must be less than or equal to the size of the Type Stream

In all PDBs observed, `header_size + type_record_bytes` is equal to the size of
the Type Stream. It would be possible to store additional data at the end of the
Type Stream after the Type Records, but this has not been observed in practice.

> Determinism: No extra data should be written to the Type Stream after the Type
> Records. The size of the Type Stream must be equal to header_size +
> type_record_bytes.

Type records are variable-length records. Each record begins with a 4-byte
header, which specifies the length and the "kind" of the record.

```c
struct TypeRecordHeader {
  uint16_t size;
  uint16_t kind;
  // followed by kind - 2 bytes of kind-specific data
};
```

The size field specifies the size in bytes of the record. The size field does
not count the size field itself, but it does count the kind field and the
payload bytes.

> Invariant: The TypeRecordHeader.size field must be a multiple of 2, and must
> be greater than or equal to 2.

Type records are aligned at 2-byte boundaries. Unfortunately, many type records
contain fields that have 4-byte alignment, such as `uint32_t`. Encoders and
decoders must handle misaligned access to those fields, either using unaligned
memory accesses or must copy the entire record to a buffer that has a guaranteed
alignment.

The kind field specifies how to interpret a type record. In the PDB
documentation, this kind field uses the "Leaf Type" enumeration. The details of
these records are outside of the scope of this document. See these references:

* LLVM: CodeView Type Records
* `cvinfo.h` in MS public and internal sources, specifically the `LEAF_ENUM_e` type. (`cvinfo.h` in microsoft-pdb public GitHub repo)

This is an example of a single type record:

```
000000d0 : 46 00 07 15 05 00 00 02 74 00 00 00 02 10 00 00 : F.......t.......
000000e0 : 5f 55 53 45 52 5f 41 43 54 49 56 49 54 59 5f 50 : _USER_ACTIVITY_P
000000f0 : 52 45 53 45 4e 43 45 00 2e 3f 41 57 34 5f 55 53 : RESENCE..?AW4_US
00000100 : 45 52 5f 41 43 54 49 56 49 54 59 5f 50 52 45 53 : ER_ACTIVITY_PRES
00000110 : 45 4e 43 45 40 40 00 f1 ba 00 03 12 02 15 03 00 : ENCE@@..........
00000120 : 01 00 4a 4f 42 5f 4f 42 4a 45 43 54 5f 4e 45 54 : ..JOB_OBJECT_NET
00000130 : 5f 52 41 54 45 5f 43 4f 4e 54 52 4f 4c 5f 45 4e : _RATE_CONTROL_EN
```

It begins with the length (0x0046) and the kind (0x1507). In the CodeView
documentation, 0x1507 identifies an `LF_ENUM` type record. Decoding that record
shows that it points to a field list (another type record) using type index
0x1002, and that its name is `_USER_ACTIVITY_PRESENCE`.

To ensure that records are padded to 2-byte alignment, MS PDB writers often
insert 0xF1 bytes at the end of type records. This is visible at offset 0x117 in
the example above. The 0xF1 byte is included in the record length, so the code
that finds record boundaries does not need to know about the 0xF1 byte. When
decoding records, implementations must ignore these 0xF1 bytes. When encoding
records, implementations should use a value of 0xF1 to pad record payloads to an
alignment boundary of 2.

# Hash Stream

The TPI Stream may have a corresponding TPI Hash Stream. The TPI Hash Stream
contains several forms of lookup tables that point into the TPI Stream, which
enables faster searching.

The TPI Stream Header specifies the stream number of the TPI Hash Stream in the
`hash_stream_index` field of the TPI Stream Header. The reserved value 0xFFFF
means that there is no hash stream. All observed PDBs have a valid hash stream.

The Type Hash Stream contains the following substreams:

* Hash Value Buffer
* Index Offset Buffer
* Hash Adjustment Buffer

The byte offset within the Hash Stream and length of each of these substreams is
specified in the TPI Stream Header, not in the Type Hash Stream itself.

Because the Type Stream Header explicitly specifies the starting offset of each
of the substreams, encoders have a degree of freedom in the order of the
substreams. Observationally, the MSVC linker writes these substreams in the
order given above (Hash Value Buffer, Index Offset Buffer, Hash Adjustment
Buffer), with no gaps between them. Also, if a given substream has a length of
zero (which is common for the Hash Adjustment Buffer), then the MSVC linker will
still write the offset where the substream would have been written, rather than
writing a meaningless value for the offset.

PDB encoders should use the same behavior as the MSVC linker:

> Determinism: Encoders should write the substreams in the order specified
> above, with no gaps between the substreams.

> Determinism: If a substream is zero-length, then encoders should still write
> an offset (into the Type Stream Header) where the substream would logically be
> (i.e. the end of the previous substream).

# Hash Value Substream

The Hash Value Substream is a substream of the Type Hash Stream. It contains
hash values for all type records. It is not clear what the purpose of the Hash
Value Substream is.

The Type Stream Header specifies the stream offset and size of the Hash Value
Substream. The Hash Value Substream is optional; it may be empty (zero-length)
even if the Type Stream contains type records. However, if the Hash Value
Substream is present, then it must contain the same number of entries as the
number of type records in the Type Stream.

> Invariant: If `TypeStreamHeader.hash_value_buffer_length` is non-zero then it
> must be a multiple of `hash_key_size` and its value must be
> `num_type_records * hash_key_size`, where `num_type_records` is
> `type_index_end – type_index_end`.

The number of hash values in the Hash Value Substream is equal to
`hash_value_buffer_length / hash_key_size`. This value should either be zero, or
should be equal to the number of types in the Type Record Stream. Each value
corresponds to a type record, by ordering hash values and type records
sequentially. The hash values are computed using CRC-32 (with an initializer of
zero) over the bytes of the entire type record. This includes the type record's
size field, kind field, and record payload.

# Type Record Hash Function

The Hash Value Substream contains hash values computed for each type record. The
hash function is complex because the function depends on the type record kind.
(See `TPI1::hashPrec` defined in `tpi.cpp` in [MS-PDB].)

* For global UDT definitions, use the `LHashPbCb` function over the `name` field
  of the UDT type. The definition of a "global UDT definition" is moderately
  complicated; see `REC::fIsGlobalDefnUdt` in `tpi.cpp`.
  * The type kind must be one of `LF_ALIAS`, `LF_CLASS`, `LF_STRUCTURE`,
    `LF_UNION`, `LF_ENUM`, or `LF_INTERFACE`.
  * If the type kind is LF_ALIAS, then the record is a global UDT definition.
  * If the type kind is other than LF_ALIAS, then these requirements apply:
* The type record must not be a forward declaration. Forward declarations are
  identified by a bit set in the property field of the type record. All of the
  relevant types have the same property bitmask set the same offset, which makes
  testing this requirement easy.
* The type record must not have the scoped bit set in its property bitmask.
* The type record must not be an "anonymous UDT".
   * The type name must not be `<unnamed-tag>` or `__unnamed`.
   * The type name must not end with `::<unnamed-tag>` or `::__unnamed`.

> TODO: Expand this section and fully specify the behavior of `TPI1::hashPrec`.
> That function also uses CRC-32 for hashing some names.

# Hash Index Substream

The Hash Index Substream is stored in the Type Hash Stream. It consists of an
array of fixed-size records with this definition:

```c
// sizeof = 8
struct HashIndexPair {
  uint32_t type_index;
  uint32_t stream_offset;
};
```

This array of `HashIndexPair` structures is sorted in strictly-increasing order
by `type_index`. Each entry specifies the offset within the Type Records
Substream of the corresponding type record. That is, you will need to add the
header_size field to stream_offset to get the absolute byte offset within the
Type Stream.

Observationally, the Hash Index Substream is usually fairly small. There is no
clear pattern to the increases in the type_index of `stream_offset` fields.
Typical values observed are an increase of 80 to 100 Type Index values, and 8000
to 10000 bytes for stream_offset.

The first entry in the array must have `type_index == type_index_begin`, and
must have `stream_offset == 0`.

> Invariant: The byte size of the Hash Index Substream is a multiple of 8.

> Invariant: The entries in Hash Index Substream are ordered with type_index
> strictly increasing, and stream_offset strictly increasing.

# Example

This is an example of the Hash Index Substream. The Hash Index Substream is
highlighted; it is preceded (in this example) by the Hash Value Substream.
 
The first few entries of the Hash Index Substream decode as:

Type Index  | Stream Offset
------------|--------------
0x0000_1000 | 0x0000_0000
0x0000_101F | 0x0000_4022
0x0000_102F | 0x0000_8450
0x0000_1056 | 0x0000_604C
0x0000_10D0 | 0x0000_81CC
0x0000_10FC | 0x0000_A034
0x0000_1157 | 0x0000_C04C

# Hash Adjustment Substream

The purpose and structure of the Hash Adjustment Substream is not
well-understood. [LLVM] suggests that it is related to Edit-and-Continue.

```

`docs/references.md`:

```md
# References

This document brings together many of the existing resources that describe PDB
and describes the file format at a level of detail sufficient for implementing
some tools.

The primary sources for this document:

* Microsoft published a GitHub repository:
  <https://github.com/microsoft/microsoft-pdb>. This contains a snapshot of some
  of the PDB sources, including header files that describe data structures and a
  pdbdump tool. Microsoft has not updated this repo in years, and recently
  marked it as "archived." It does not accept pull requests.

  The following paths with the `microsoft-pdb` repo contain useful information:

  + `langapi/include/cvinfo.h` – Many essential definitions for CodeView debug
    info, including the `LF_*` and `S_*` constants and structures which define
    the fixed-size portion of many type and symbol records.
  
  + `langapi/include/cvexefmt.h` – CodeView definitions for executables.
  
  + `langapi/include/pdb.h` – The API for the PDB reader/writer library. This is
    an implementation detail and does not contain descriptions of the file
    structures. However, it is useful for seeing the operations defined by the
    implementation.
  
  + `PDB/src/tools/cvdump/dumppdb.cpp` – Dumps information in PDBs.
  
  + `PDB/dbi/pdb.cpp` – The PDB reader/writer library.
  
  + `PDB/dbi/dbi.cpp` and `dbi.h` – Code which can read the Debug Information
    (DBI) Stream.
  
  + `PDB/dbi/gsi.cpp` and `gsi.h` – Code which can read the Global Symbol Index
    (GSI) Stream.
  
  + `PDB/dbi/mod.cpp` and `mod.h` – Code which can read Module Information
    Streams (aka `mod` or `modi` streams).

  As a resource, this repository is useful but incomplete. It does not describe
  all of the information in PDBs and it does not describe invariants or
  relationships. Still, it has been a valuable resource for the external
  developers who created the LLVM web site, and various other tools for reading
  PDBs.

* The LLVM Project provides [The PDB File Format](https://llvm.org/docs/PDB/index.html).
  This web site covers some of the basics of PDB, enough to write a simple PDB
  reader for a subset of the information in PDBs. It is not comprehensive. Some
  of the information in this web site is incorrect.

* The <https://github.com/willglynn/pdb> repository. This is an open source
  library (MIT and Apache dual-licensed), implemented in Rust, which can read
  some information from PDBs.

* The <https://github.com/MolecularMatters/raw_pdb> repository. This is an open
  source library (BSD 2-clause), implemented in C++, which can read some
  information from PDBs.

* The [microsoft-pdb](https://github.com/microsoft/microsoft-pdb) repository.
  Contains documentation and implementation for PDB and CodeView. In many ways,
  this is the most authoritative public _implementation_ resource for PDB,
  but it is not a document and not a reference. The repository was provided as
  a resource by Microsoft, primarily as an aid for the LLVM Project. The source
  code in it is not complete (not buildable) and the repository has been
  archived.

  In the `microsoft-pdb` repository, the `cvinfo.h` header contains many
  CodeView definitions but does not document their semantics.

* [CODEVIEW] The CodeView specification for type records and symbol records.
  This information has been published by Microsoft previously, but is not
  currently offered.

* <https://en.wikipedia.org/wiki/Program_database>

* [CodeView, the MS debug info format, in LLVM](https://llvm.org/devmtg/2016-11/Slides/Kleckner-CodeViewInLLVM.pdf)

  (Reid Kleckner) Describes the progress and status of the LLVM Project's effort to support
  CodeView and PDB.

  [Reid's YouTube talk at 2016 LLVM Developers' Meeting](https://www.youtube.com/watch?v=5twzd06NqGU)

* Microsoft published `MS_Symbol_Type_v1.0.pdf` in 2004, which contains
  extensive documentation on the CodeView type and symbol records. It has been
  archived repeatedly on the Internet.

  * Example: <https://github.com/mfichman/jogo/blob/master/notes/MS_Symbol_Type_v1.0.pdf>

```

`docs/terminology.md`:

```md

# Terminology

age
: A `uint32_t` value which is initialized to 1, and is incremented whenever an
executable and a PDB are modified as related pair. Tools which read PDBs, such
as debuggers, should verify that the `age` stored in an executable matches the
`age` stored in the PDB.

binding key
: The combination of a GUID (`unique_id`) and `age` value, which uniquely
identifies a PDB and links it to the executable that it describes.

COFF, PE/COFF
: The Common Object File Format, which describes the structure of compiler
outputs, linker outputs, executables. This is the executable format used by
Windows. Also known as PE/COFF (Portable Executable).

contribution
: A fragment of a module that was included into an executable by the linker,
such as a function, vtable, global variable, etc. Each module typically contains
many contributions to the final executable.

DIA, MS-DIA
: Microsoft Debug Interface Access, a library for accessing symbolic debugging
information within PDBs and other sources. Used by tools such as debuggers.

executable
: A PE/COFF executable, such as a `*.exe`, `*.dll`, `*.sys` file (etc), which
has been produced by the linker and can be loaded and executed. The term
"executable" is _not_ limited to `*.exe` files.

[Global Symbol Stream (GSS)](pdb/globals.md)
: A sequence of symbol records that describe global functions, types,
annotations, etc. This is an "index" of the per-module symbols, or in PDBs that
have been stripped, this is the only description of global symbols.

interval
: A fixed-size group of pages in the MSF, which have 2 pages reserved for the
Free Page Maps (FPM1 and FPM2). See [Free Page Map](pdb/msf.md#free-page-map).

module index
: A `uint16_t` value that identifies a module within a PDB. The order of the
records in the [DBI Modules Substream](pdb/dbi_modules.md) determines the range
and meaning of the module index. Module indexes are 0-based. The value 0xffff is
reserved and cannot identify a valid module.

MSF
: Multi-Stream File, a file container format which can contain multiple internal
file streams.

Free Page Map (FPM)
: A set of pages that contain a bitmap. The elements of the bitmap correspond to
pages, and indicate which pages are free (1) or allocated (0). See
[MSF](pdb/msf.md).

`ItemId`
: A 32-bit integer which identifies a record in the
[IPI Stream](pdb/ipi_stream.md).

module
: An object file that was linked into an executable by the linker. Also called a
compiland or a translation unit. This sense of "module" is not related to Rust
modules or C++20 Modules.

`NameIndex`
: A 32-bit integer which identifies a string stored in the
[Names Stream](pdb/names_stream.md).

page
: The unit of allocation / storage of data within a PDB/MSF file. All pages
within a PDB file are the same size and are stored at PDB file offsets that are
a multiple of the page size. The most common page size is 4096 bytes.

stream
: A logical sequence of bytes stored within a PDB/MSF file. Analogous to a file
stored within a ZIP or TAR file.

Type Database, TPI Stream
: A stream which contains records that describe C/C++ types. Also known as TPI
Stream.

`TypeIndex`
: A 32-bit integer which identifies either a primitive type (such as
`unsigned short`) or identifies a type record defined in the
[TPI Stream](./pdb/tpi_stream.md).

UDT
: A _user-defined type_, such as a `class`, `struct`, or `union`.

```

`msf/Cargo.toml`:

```toml
[package]
name = "ms-pdb-msf"
version = "0.1.7"
edition = "2024"
description = "Reads Multi-Stream Files, which are used in the Microsoft Program Database (PDB) file format"
authors = ["Arlie Davis <ardavis@microsoft.com>"]
license = "MIT OR Apache-2.0"
repository = "https://github.com/microsoft/pdb-rs/"
categories = ["parsing", "development-tools", "development-tools::debugging"]

[lib]
doctest = false

[dependencies]
anyhow.workspace = true
bitvec.workspace = true
pow2.workspace = true
static_assertions.workspace = true
sync_file.workspace = true
tracing.workspace = true
uuid.workspace = true
zerocopy = { workspace = true, features = ["alloc", "derive"] }
zerocopy-derive.workspace = true

[dev-dependencies]
dbg-ranges = "0.1.0"
pretty-hex = "0.4.1"
static_init.workspace = true
tracing-subscriber = { workspace = true, features = ["fmt"] }

```

`msf/README.md`:

```md
# MSF container layer for Program Database (PDB) library

This crate contains code for reading, creating, and modifying PDB files that use
the MSF container format. Currently, all PDBs produced by Microsoft tools use
the MSF container format.
  
This is a lower-level building block for PDBs. Most developers should use the
[`ms-pdb`](https://crates.io/crates/ms-pdb) crate, instead of directly using the `ms-pdb-msf` crate.
The `ms-pdb-msf` crate is published separately to aid in minimizing dependencies and enforcing
good layering.

## All information in this implementation is based on publicly-available information

This implementation is based solely on public sources that describe the PDB and
MSF data structures. This repository does not contain any confidential Microsoft
intellectual property.

## **THIS IMPLEMENTATION IS NOT AUTHORITATIVE AND IS NOT A REFERENCE IMPLEMENTATION**

This implementation is **NOT** an authoritative reference. It may contain
defects or inaccuracies. As the `LICENSE` states, this implementation is
provided "as is", without warranty of any kind. Specifically, this
implementation **DOES NOT** make any guarantees about compatibility or
interoperability with any other toolset, including (but not limited to)
Microsoft Visual C++ (MSVC) and Clang.

The authors of this effort may make a good-faith effort to fix bugs, including
bugs discovered by the authors or the community. However, as the license states,
this repository is provided "as is" and there is absolutely no obligation or
expectation on levels of service for the implementation provided in this
repository.

## Contributing

This project welcomes contributions and suggestions. Most contributions require
you to agree to a Contributor License Agreement (CLA) declaring that you have
the right to, and actually do, grant us the rights to use your contribution. For
details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether
you need to provide a CLA and decorate the PR appropriately (e.g., status check,
comment). Simply follow the instructions provided by the bot. You will only need
to do this once across all repos using our CLA.

This project has adopted the
[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the
[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any
additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or
services. Authorized use of Microsoft trademarks or logos is subject to and must
follow
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must
not cause confusion or imply Microsoft sponsorship. Any use of third-party
trademarks or logos are subject to those third-party's policies.

## Repository

* <https://github.com/microsoft/pdb-rs>

## Contacts

* `sivadeilra` on GitHub
* Arlie Davis ardavis@microsoft.com

```

`msf/src/check.rs`:

```rs
//! Consistency checks for MSF, both in-memory and on-disk

use super::*;

impl<F> Msf<F> {
    #[cfg(not(test))]
    #[inline(always)]
    pub(super) fn assert_invariants(&self) {}

    #[cfg(test)]
    #[inline(never)]
    pub(super) fn assert_invariants(&self) {
        // There is always at least one stream, because stream 0 is special.
        assert!(!self.stream_sizes.is_empty());

        // This is implied by stream_sizes not being empty.
        assert!(self.committed_stream_page_starts.len() >= 2);

        // The pages assigned to Stream 0 are marked "free" and are not marked "deleted".
        {
            let stream0_pages = &self.committed_stream_pages[self.committed_stream_page_starts[0]
                as usize
                ..self.committed_stream_page_starts[1] as usize];

            for &page in stream0_pages.iter() {
                assert!(self.pages.fpm[page as usize]);
                assert!(!self.pages.fpm_freed[page as usize]);
            }
        }

        // The pages assigned to all streams in the committed state are disjoint.
        // The pages assigned to all streams are < num_pages.
        {
            let mut busy_pages: BitVec<u32, Lsb0> = BitVec::new();
            busy_pages.resize(self.pages.num_pages as usize, false);
            for &page in self.committed_stream_pages.iter() {
                assert!(page <= self.pages.num_pages, "page {page} is out of range",);
                assert!(
                    !busy_pages[page as usize],
                    "page {page} is used by more than one stream",
                );
                assert!(
                    !self.pages.fresh[page as usize],
                    "page {page} cannot be fresh if it is used by a committed stream"
                );

                busy_pages.set(page as usize, true);
            }
        }

        // All entries in modified_streams have a stream index that is valid.
        for &stream_index in self.modified_streams.keys() {
            assert!(stream_index < self.stream_sizes.len() as u32);
        }
    }
}

```

`msf/src/commit.rs`:

```rs
//! Commits all pending changes to an MSF file.

use super::*;
use anyhow::Result;
use tracing::{debug, debug_span, info, info_span, trace, trace_span};

impl<F: ReadAt + WriteAt> Msf<F> {
    /// Commits all changes to the MSF file to disk.
    ///
    /// The MSF file format is designed to support a very limited form of transactional commits.
    /// A single block write to the first page (located at the start of the file) can atomically
    /// commit all outstanding modifications to the MSF file.
    ///
    /// This commit design does require that the underlying operating system handle the write to
    /// page 0 in a single operation.  Operating systems generally don't _contractually_
    /// guarantee atomicity of such writes, but in practice it's true enough to be reasonably
    /// reliable. The commit functionality of MSF is really designed to guard against application
    /// failures, not failures of the operating system or its storage stack. So don't rely on
    /// `commit()` for anything more than best-effort service.
    ///
    /// This `commit()` implementation _does not_ permit multiple concurrent writers (or concurrent
    /// readers and writers) to the underlying MSF/PDB file. If you manage to circumvent this,
    /// you may damage the underlying MSF/PDB file.
    ///
    /// This `commit()` implementation does not buffer modifications to stream contents. All
    /// writes to stream contents are handled by immediately writing the data to the underlying
    /// MSF file. However, these writes do not _overwrite_ the data stored in the stream; instead,
    /// new pages are allocated in the file for the new or modified stream contents. If an
    /// application deletes a range of a stream (by resizing it), then the existing pages are
    /// protected and cannot be overwritten until `commit()` is called.
    ///
    /// Commit operations are moderately expensive. Applications that modify PDBs should generally
    /// perform a single commit operation, not a series of commit operations. Each call to
    /// `commit()` writes a new copy of the Stream Directory (the list of streams and their page
    /// numbers) to disk. Because the existing Stream Directory cannot be overwritten (until
    /// _after_ the `commit()` operation completes), this means that all `commit()` operations
    /// require that two complete copies of the Stream Directory persist on disk.
    ///
    /// While it would be possible to reduce the size of an MSF file (after a commit completes) by
    /// trimming the unused pages at the end of the MSF file, in practice this does not help much
    /// because of page fragmentation. For this reason, `commit()` never reduces the size of the
    /// underlying MSF file.
    ///
    /// Returns `Ok(true)` if this `Msf` contained uncommitted changes and these changes have now
    /// been committed.
    ///
    /// Returns `Ok(false)` if this `Msf` did not contain any uncomitted changes. In this case,
    /// no `write()` calls are issued to the underlying storage.
    ///
    /// If this function returns `Err`, then the underlying MSF file may have had new pages written,
    /// but the existing Stream Directory and header page should be intact. However, if the
    /// underlying operating system did not write Page 0 atomically, then the underlying MSF
    /// file may be irrecoverably damaged.
    ///
    /// Also, if this function returns `Err`, the in-memory data structures that represent the
    /// state of the `Msf` editor are not guaranteed to be in a consistent state.
    pub fn commit(&mut self) -> Result<bool> {
        let _span = info_span!("Msf::commit").entered();

        self.assert_invariants();

        // If this was not opened for write access then there are no pending changes at all.
        if self.access_mode != AccessMode::ReadWrite {
            info!("this Msf is not opened for read-write access");
            debug_assert!(self.modified_streams.is_empty());
            return Ok(false);
        };

        // We only support modifying Big MSF files.
        assert_eq!(self.kind, MsfKind::Big);

        // If no streams have been modified, then there is nothing to do.
        if self.modified_streams.is_empty() {
            info!("there are no modified streams; nothing to commit");
            return Ok(false);
        }

        let new_fpm_number: u32 = match self.active_fpm {
            FPM_NUMBER_1 => FPM_NUMBER_2,
            FPM_NUMBER_2 => FPM_NUMBER_1,
            _ => panic!("Active FPM has invalid value"),
        };
        info!(
            old_fpm = self.active_fpm,
            new_fpm = new_fpm_number,
            "beginning commit"
        );

        let stream_dir_info = self.write_new_stream_dir()?;

        // NOTE: The call to merge_freed_into_free irreversibly alters state. If we fail after
        // this point, then Msf will be left in an inconsistent state. This could be improved by
        // building a new FPM vector in-memory without modifying any state.
        self.pages.merge_freed_into_free();
        fill_last_word_of_fpm(&mut self.pages.fpm);

        self.write_fpm(new_fpm_number)?;

        let page_size = self.pages.page_size;
        let page_size_usize = usize::from(page_size);

        // Build the new Page 0.
        let mut page0: Vec<u8> = vec![0; page_size_usize];

        let msf_header = MsfHeader {
            magic: MSF_BIG_MAGIC,
            page_size: U32::new(u32::from(page_size)),
            active_fpm: U32::new(new_fpm_number),
            num_pages: U32::new(self.pages.num_pages),
            stream_dir_size: U32::new(stream_dir_info.dir_size),
            stream_dir_small_page_map: U32::new(0),
            // The stream directory page map pointers follows the MsfHeader.
        };
        msf_header.write_to_prefix(page0.as_mut_slice()).unwrap();

        // Copy the stream dir page map into Page 0.
        let page_map_pages_bytes = stream_dir_info.map_pages.as_bytes();
        page0[STREAM_DIR_PAGE_MAP_FILE_OFFSET as usize..][..page_map_pages_bytes.len()]
            .copy_from_slice(page_map_pages_bytes);

        // ------------------------ THE BIG COMMIT ----------------------

        info!("writing MSF File Header");
        self.file.write_all_at(page0.as_bytes(), 0)?;

        // After this point, _nothing can fail_.
        // Any operation that could have failed should have been moved above the commit point.

        // --------------------- CLEANUP AFTER THE COMMIT ---------------

        // Update in-memory state to reflect the commit.
        //
        // This code runs after we write the new Page 0 to disk. That commits the changes to the
        // PDB. This function modifies in-memory state to reflect the successful commit. For this
        // reason, after this point, this function must NEVER return a failure code.
        {
            // Build the new in-memory stream directory. This is very similar to the version that we
            // just wrote to disk, so maybe we should unify the two.

            let _span = trace_span!("post_commit").entered();

            let page_size = self.pages.page_size;

            // We can easily determine the right size for allocating 'stream_pages'.
            let mut num_stream_pages: usize = 0;
            for &stream_size in self.stream_sizes.iter() {
                if stream_size != NIL_STREAM_SIZE {
                    num_stream_pages += num_pages_for_stream_size(stream_size, page_size) as usize;
                }
            }

            let mut stream_pages: Vec<Page> = Vec::with_capacity(num_stream_pages);
            let mut stream_page_starts: Vec<u32> = Vec::with_capacity(self.stream_sizes.len() + 1);

            for (stream, &stream_size) in self.stream_sizes.iter().enumerate() {
                stream_page_starts.push(stream_pages.len() as u32);

                if stream_size == NIL_STREAM_SIZE {
                    trace!(stream, "stream is nil");
                    continue;
                }

                let num_stream_pages = num_pages_for_stream_size(stream_size, page_size) as usize;

                // If this stream has been modified, then return the modified page list.
                let is_modified;
                let pages: &[Page] =
                    if let Some(pages) = self.modified_streams.get(&(stream as u32)) {
                        is_modified = true;
                        pages
                    } else {
                        is_modified = false;
                        let start = self.committed_stream_page_starts[stream] as usize;
                        &self.committed_stream_pages[start..start + num_stream_pages]
                    };
                assert_eq!(num_stream_pages, pages.len());

                trace!(stream, stream_size, num_stream_pages, is_modified);

                stream_pages.extend_from_slice(pages);
            }

            stream_page_starts.push(stream_pages.len() as u32);

            // Now that we have written the Stream Directory (and the map pages, above it), we
            // need to mark the pages that contain the Stream Directory (and the map pages) as
            // *freed*.  Not free, but *freed*.  Fortunately, we still have this information, since
            // we built it above when we called write_new_stream_dir().
            //
            // The multiple_commits() and many_commits() tests verify this.
            {
                trace!("marking stream dir pages as free");

                for list in [
                    stream_dir_info.dir_pages.as_slice(),
                    stream_dir_info.map_pages.as_slice(),
                ] {
                    for &p in list.iter() {
                        let pi = p.get() as usize;
                        assert!(!self.pages.fpm_freed[pi]);
                        assert!(!self.pages.fpm[pi]);
                        self.pages.fpm_freed.set(pi, true);
                    }
                }
            }

            // Update state
            self.committed_stream_pages = stream_pages;
            self.committed_stream_page_starts = stream_page_starts;
            self.modified_streams.clear();

            self.pages.fresh.set_elements(0);
            self.pages.next_free_page_hint = 3; // positioned after file header and FPM1 and FPM2

            trace!(new_fpm_number, "setting active FPM");
            self.active_fpm = new_fpm_number;
        }

        info!("commit complete");

        self.assert_invariants();

        Ok(true)
    }

    /// Builds the new stream directory.
    fn build_new_stream_dir(&self) -> Vec<U32<LE>> {
        let page_size = self.pages.page_size;

        let num_streams = self.stream_sizes.len();

        let mut stream_dir: Vec<U32<LE>> = Vec::new();
        stream_dir.push(U32::new(num_streams as u32));

        // Push a size of 0 for Stream 0.
        stream_dir.push(U32::new(0));

        for &stream_size in self.stream_sizes[1..].iter() {
            stream_dir.push(U32::new(stream_size));
        }

        for (stream, &stream_size) in self.stream_sizes.iter().enumerate() {
            if stream_size == NIL_STREAM_SIZE {
                debug!(stream, "stream is nil");
                continue;
            }

            let num_stream_pages = num_pages_for_stream_size(stream_size, page_size) as usize;

            // If this stream has been modified, then return the modified page list.
            let pages: &[Page] = if let Some(pages) = self.modified_streams.get(&(stream as u32)) {
                pages
            } else {
                let start = self.committed_stream_page_starts[stream] as usize;
                &self.committed_stream_pages[start..start + num_stream_pages]
            };
            assert_eq!(num_stream_pages, pages.len());
            debug!(stream, stream_size);

            stream_dir.reserve(pages.len());
            for &p in pages.iter() {
                stream_dir.push(U32::new(p));
            }
        }

        stream_dir
    }

    /// Builds the new stream directory and writes it to disk.
    ///
    /// This builds the stream directory and the page map pages and writes it to disk. It returns
    /// the size in bytes of the stream directory and the page numbers of the page map.
    fn write_new_stream_dir(&mut self) -> anyhow::Result<StreamDirInfo> {
        let _span = debug_span!("Msf::write_new_stream_dir").entered();

        let page_size = self.pages.page_size;
        let page_size_usize = usize::from(page_size);

        // "Dir" pages contain the contents of the Stream Directory.
        // "Map" pages contain pointers to "dir" pages. They are "above" the dir pages.

        let stream_dir = self.build_new_stream_dir();
        let stream_dir_bytes = stream_dir.as_bytes();

        let mut reusable_page_data: Vec<u8> = vec![0; usize::from(page_size)];

        // The number of pages needed to store the Stream Directory.
        let num_stream_dir_pages =
            num_pages_for_stream_size(stream_dir_bytes.len() as u32, page_size) as usize;
        let mut dir_pages: Vec<U32<LE>> = Vec::with_capacity(num_stream_dir_pages);

        for stream_dir_chunk in stream_dir_bytes.chunks(page_size_usize) {
            // Allocate a page for the next stream dir page.
            let page = self.pages.alloc_page();
            dir_pages.push(U32::new(page));

            let page_bytes = if stream_dir_chunk.len() == page_size_usize {
                // It's a complete page, so there is no need for the bounce buffer.
                stream_dir_chunk
            } else {
                let (lo, hi) = reusable_page_data.split_at_mut(stream_dir_chunk.len());
                lo.copy_from_slice(stream_dir_chunk);
                hi.fill(0);
                reusable_page_data.as_slice()
            };

            let page_offset = page_to_offset(page, page_size);
            debug!(page, page_offset, "writing stream dir page");
            self.file.write_all_at(page_bytes, page_offset)?;
        }

        // Now we build the next level of indirection (the "page map"), and allocate pages for them
        // and write them.
        let mut map_pages: Vec<U32<LE>> = Vec::new();

        let num_u32s_per_page = u32::from(page_size) / 4;
        for map_page_contents in dir_pages.chunks(num_u32s_per_page as usize) {
            let map_page_index = self.pages.alloc_page();
            let map_file_offset = page_to_offset(map_page_index, page_size);
            let map_page_bytes = map_page_contents.as_bytes();
            let (lo, hi) = reusable_page_data.split_at_mut(map_page_bytes.len());
            lo.copy_from_slice(map_page_bytes);
            hi.fill(0);

            debug!(
                map_page_index,
                map_file_offset, "writing stream dir page map page"
            );
            self.file
                .write_all_at(&reusable_page_data, map_file_offset)?;

            map_pages.push(U32::new(map_page_index));
        }

        Ok(StreamDirInfo {
            dir_size: stream_dir_bytes.len() as u32,
            dir_pages,
            map_pages,
        })
    }

    /// Writes the FPM for the new transaction state.
    fn write_fpm(&mut self, new_fpm_number: u32) -> anyhow::Result<()> {
        let _span = debug_span!("write_fpm").entered();

        let page_size = self.pages.page_size;
        let page_size_usize = usize::from(page_size);
        let num_intervals = self.pages.num_pages.div_round_up(page_size);

        assert_eq!(self.pages.num_pages as usize, self.pages.fpm.len());
        let fpm_words: &[u32] = self.pages.fpm.as_raw_slice();
        let fpm_bytes: &[u8] = fpm_words.as_bytes();

        // This iterates the contents of the pages of the FPM. Each item iterated is a &[u8]
        // containing the piece of the FPM that should be written to a single on-disk page.
        // The last page iterated can be a partial (incomplete) page.
        //
        // For example: page_size = 4096, so there are 4096 bytes in each FPM page within
        // an interval.  That means there are 4096 * 8 bits in each FPM page, or 32,768 bits.
        // These bits cover _much_ more than a single interval; each FPM page covers 8
        // intervals worth of pages.
        //
        // This is basically a bug in the design of the FPM; the FPM is 8x larger than it
        // needs to be. But the design is frozen, so we must do it this way.

        let mut fpm_pages_data_iter = fpm_bytes.chunks(page_size_usize);

        // This is a buffer where we assemble complete FPM pages before writing them to disk.
        // This ensures that we always write a complete page. This is more efficient for storage
        // stacks, since pages are usually larger than on-disk block sizes and are block-size
        // aligned, so this avoids the need for a read-modify-write cycle in the underlying
        // filesystem. This is only necessary for the last (partial) page.
        let mut fpm_page_buffer: Vec<u8> = vec![0; page_size_usize];

        for interval_index in 0..num_intervals {
            let this_fpm_page_data = fpm_pages_data_iter.next().unwrap_or(&[]);
            assert!(this_fpm_page_data.len() <= fpm_page_buffer.len());

            let slice_to_write = if this_fpm_page_data.len() < page_size_usize {
                fpm_page_buffer[..this_fpm_page_data.len()].copy_from_slice(this_fpm_page_data);
                fpm_page_buffer[this_fpm_page_data.len()..].fill(0xff); // fill the rest with "free"
                fpm_page_buffer.as_slice()
            } else {
                // We already have a perfectly-sized slice. Just use it.
                this_fpm_page_data
            };

            let interval_page = interval_to_page(interval_index, page_size);
            let new_fpm_page = interval_page + new_fpm_number;

            debug!(interval = interval_index, "writing fpm chunk");

            self.file
                .write_all_at(slice_to_write, page_to_offset(new_fpm_page, page_size))?;
        }

        Ok(())
    }
}

/// This ensures that the last few bits of the FPM are set to "free".
///
/// The MSPDB library uses a bit vector implementation that packs bits into an array of `u32`
/// values, just as this Rust implementation does. However, if the number of _bits_ in the FPM
/// is not a multiple of 32, then the MSPDB library accidentally reads the unaligned bits in the
/// last `u32` and expects them to be "free".
fn fill_last_word_of_fpm(fpm: &mut BitVec<u32, Lsb0>) {
    let unaligned_len = fpm.len() & 0x1f;
    if unaligned_len == 0 {
        return;
    }

    let fpm_words = fpm.as_raw_mut_slice();
    let last = fpm_words.last_mut().unwrap();

    // Because unaligned_len is the result of masking with 0x1f, we know that the shift count
    // cannot overflow.
    *last |= 0xffff_ffff << unaligned_len;
}

/// Information about the new Stream Directory that we just constructed and wrote to disk.
struct StreamDirInfo {
    /// Size in bytes of the Stream Directory
    dir_size: u32,

    /// The list of pages that contain the Stream Directory
    dir_pages: Vec<U32<LE>>,

    /// The list of pages that contain the Stream Directory Map (the level _above_ `dir_pages`)
    map_pages: Vec<U32<LE>>,
}

```

`msf/src/lib.rs`:

```rs
//! Reads and writes Multi-Stream Files (MSF). MSF is the underlying container format used by
//! Program Database (PDB) files.
//!
//! MSF files contain a set of numbered _streams_. Each stream is like a file; a stream is a
//! sequence of bytes.
//!
//! The bytes stored within a single stream are usually not stored sequentially on disk. The
//! organization of the disk file and the mapping from stream locations to MSF file locations is
//! similar to a traditional file system; managing that mapping is the main purpose of the MSF
//! file format.
//!
//! MSF files are used as the container format for Program Database (PDB) files. PDB files are used
//! by compilers, debuggers, and other tools when targeting Windows.
//!
//! Most developers should not use this crate directly. This crate is a building block for tools
//! that read and write PDBs. This crate does not provide any means for building or parsing the
//! data structures of PDB files; it only handles storing files in the MSF container format.
//!
//! The `mspdb` crate uses this crate for reading and writing PDB files. It provides an interface
//! for reading PDB data structures, and in some cases for creating or modifying them. Most
//! developers should use `mspdb` instead of using `msf` directly.
//!
//! # References
//!
//! * [The MSF File Format](https://llvm.org/docs/PDB/MsfFile.html)
//! * [The PDB File Format](https://llvm.org/docs/PDB/index.html)
//! * [`microsoft-pdb` repository](https://github.com/microsoft/microsoft-pdb): Many of the comments
//!   in this Rust crate reference C++ source files and header files from this `microsoft-pdb`
//!   repository. If a C++ file is referenced in a comment without more context, such as `dbi.h`,
//!   then check for it in the `microsoft-pdb` repository.

#![forbid(unused_must_use)]
#![forbid(unsafe_code)]
#![warn(missing_docs)]
#![allow(clippy::collapsible_if)]
#![allow(clippy::needless_late_init)]
#![allow(clippy::needless_lifetimes)]

mod check;
mod commit;
mod open;
mod pages;
mod read;
mod stream_reader;
mod stream_writer;
mod write;

#[cfg(test)]
mod tests;

pub use open::CreateOptions;
pub use stream_reader::StreamReader;
pub use stream_writer::StreamWriter;

use anyhow::bail;
use bitvec::prelude::{BitVec, Lsb0};
use pow2::{IntOnlyPow2, Pow2};
use std::collections::HashMap;
use std::fs::{File, OpenOptions};
use std::io::{Read, Seek, SeekFrom};
use std::mem::size_of;
use std::path::Path;
use sync_file::{RandomAccessFile, ReadAt, WriteAt};
use zerocopy::{FromBytes, FromZeros, Immutable, IntoBytes, KnownLayout, LE, U16, U32, Unaligned};

use self::pages::{PageAllocator, num_pages_for_stream_size};

/// Identifies a page number in the MSF file. Not to be confused with `StreamPage`.
type Page = u32;

/// Identifies a page within a stream. `StreamPage` can be translated to `Page` by using the
/// stream page mapper.
type StreamPage = u32;

const FPM_NUMBER_1: u32 = 1;
const FPM_NUMBER_2: u32 = 2;

/// The value of `magic` for "big" MSF files.
const MSF_BIG_MAGIC: [u8; 32] = *b"Microsoft C/C++ MSF 7.00\r\n\x1a\x44\x53\x00\x00\x00";

/// This identifies MSF files before the transition to "big" MSF files.
const MSF_SMALL_MAGIC: [u8; 0x2c] = *b"Microsoft C/C++ program database 2.00\r\n\x1a\x4a\x47\0\0";

#[test]
fn show_magics() {
    use pretty_hex::PrettyHex;

    println!("MSF_SMALL_MAGIC:");
    println!("{:?}", MSF_SMALL_MAGIC.hex_dump());

    println!("MSF_BIG_MAGIC:");
    println!("{:?}", MSF_BIG_MAGIC.hex_dump());
}

/// The header of the PDB/MSF file, before the transition to "big" MSF files.
/// This is at file offset 0.
#[allow(missing_docs)]
#[derive(IntoBytes, FromBytes, Unaligned, Immutable, KnownLayout)]
#[repr(C)]
struct SmallMsfHeader {
    /// Identifies this file as a PDB. Value must be [`MSF_SMALL_MAGIC`].
    magic: [u8; 0x2c],
    page_size: U32<LE>,
    active_fpm: U16<LE>,
    num_pages: U16<LE>,
    stream_dir_size: U32<LE>,
    /// This field contains a pointer to an in-memory data structure, and hence is meaningless.
    /// Decoders should ignore this field. Encoders should set this field to 0.
    stream_dir_ptr: U32<LE>,
}

/// The header of the PDB/MSF file. This is at file offset 0.
#[derive(IntoBytes, FromBytes, Unaligned, Immutable, KnownLayout)]
#[repr(C)]
struct MsfHeader {
    /// Identifies this file as a PDB.
    magic: [u8; 32],

    /// The size of each page, in bytes.
    page_size: U32<LE>,

    /// Page number of the active FPM. This can only be 1 or 2. In the C++ implementation (in the
    /// `microsoft-pdb` repository), this is `pnFpm`.
    active_fpm: U32<LE>,

    /// The number of pages in this MSF file. In the C++ implementation, this is `pnMac`.
    num_pages: U32<LE>,

    /// Size of the Stream Directory, in bytes. In the C++ implementation, this is `siSt.cb`.
    stream_dir_size: U32<LE>,

    /// The page which contains the Stream Directory Map. This page contains a list of pages
    /// which contain the Stream Directory.
    ///
    /// This field is only used for "Small MSF" (pre-"Big MSF") encoding. When using Big MSF,
    /// this field is expected to be zero.
    ///
    /// In the C++ implementation, this is `mpspnpn` (map of stream page number to page number).
    stream_dir_small_page_map: U32<LE>,
    // When using "Big MSF", there is an array of u32 values that immediately follow
    // the MSfHeader. The size of the array is a function of stream_dir_size and num_pages:
    //
    //     divide_round_up(divide_round_up(stream_dir_size, num_pages) * 4), num_pages)
    //
    // pub stream_dir_big_page_map: [U32<LE>],
}

/// The length of the MSF File Header.
const MSF_HEADER_LEN: usize = size_of::<MsfHeader>();

/// The byte offset of the stream directory page map. This is a small array of page indices that
/// point to pages that contain the stream directory. This is used only with the Big MSF encoding.
const STREAM_DIR_PAGE_MAP_FILE_OFFSET: u64 = MSF_HEADER_LEN as u64;
static_assertions::const_assert_eq!(MSF_HEADER_LEN, 52);

/// The minimum page size.
pub const MIN_PAGE_SIZE: PageSize = PageSize::from_exponent(9);

/// The default page size.
pub const DEFAULT_PAGE_SIZE: PageSize = PageSize::from_exponent(12);

/// A large page size. This is less than the largest supported page size.
pub const LARGE_PAGE_SIZE: PageSize = PageSize::from_exponent(13);

/// The largest supported page size.
pub const MAX_PAGE_SIZE: PageSize = PageSize::from_exponent(16);

/// This size is used to mark a stream as "invalid". An invalid stream is different from a
/// stream with a length of zero bytes.
pub const NIL_STREAM_SIZE: u32 = 0xffff_ffff;

/// Specifies a page size used in an MSF file. This value is always a power of 2.
pub type PageSize = Pow2;

/// The stream index of the Stream Directory stream. This is reserved and cannot be used by
/// applications.
pub const STREAM_DIR_STREAM: u32 = 0;

/// The maximum valid stream index.
///
/// Although this library uses `u32` for stream indices, many MSF and PDB data structures assume
/// that stream numbers cab be stored in `u16`. Also, `0xffff` is used as a sentinel value in
/// some data structures, so that value cannot be a valid stream index.
pub const MAX_STREAM: u32 = 0xfffe;

/// Converts a page number to a file offset.
fn page_to_offset(page: u32, page_size: PageSize) -> u64 {
    (page as u64) << page_size.exponent()
}

/// Given an interval number, returns the page number of the first page of the interval.
fn interval_to_page(interval: u32, page_size: PageSize) -> u32 {
    interval << page_size.exponent()
}

/// Gets the byte offset within a page, for a given offset within a stream.
pub fn offset_within_page(offset: u32, page_size: PageSize) -> u32 {
    let page_low_mask = (1u32 << page_size.exponent()) - 1u32;
    offset & page_low_mask
}

/// Allows reading and writing the contents of a PDB/MSF file.
///
/// The [`Msf::open`] function opens an MSF file for read access, given a file. This is the most
/// commonly-used way to open a file.
pub struct Msf<F = RandomAccessFile> {
    /// The data source.
    file: F,

    kind: MsfKind,

    /// The FPM number for the committed (active) FPM.
    ///
    /// The `commit()` function can change this number.
    active_fpm: u32,

    /// Contains the sizes of all streams. The length of `stream_sizes` implicitly defines
    /// the number of streams.
    ///
    /// Values in this vector may be [`NIL_STREAM_SIZE`], indicating that the stream is present
    /// but is a nil stream.
    ///
    /// As streams are modified, this vector changes. It contains a combination of both committed
    /// and uncommitted state.
    stream_sizes: Vec<u32>,

    /// The maximum number of streams that we will allow to be created using `new_stream` or
    /// `nil_stream`. The default value is 0xfffe, which prevents overflowing the 16-bit stream
    /// indexes that are used in PDB (or confusing them with the "nil" stream index).
    max_streams: u32,

    /// Contains the page numbers for all streams in the committed state.
    committed_stream_pages: Vec<Page>,

    /// Vector contains offsets into `committed_stream_pages` where the pages for a given stream start.
    committed_stream_page_starts: Vec<u32>,

    /// Handles allocating pages.
    pages: PageAllocator,

    /// If a stream has been modified then there is an entry in this table for it. The key for
    /// each entry is the stream number. The value is the sequence of pages for that stream.
    ///
    /// One of the side-effects of the [`Msf::commit`] function is that the `modified_streams`
    /// table is cleared.
    ///
    /// This table is always empty if `access_mode == AccessMode::Read`.
    modified_streams: HashMap<u32, Vec<Page>>,

    access_mode: AccessMode,
}

/// Specifies the versions used for the MSF.
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
pub enum MsfKind {
    /// The obsolete, pre-Big MSF encoding. This library does not support creating or modifying
    /// MSF files that use this encoding, but it does support reading them.
    Small,
    /// The Big MSF encoding, which is the encoding currently used by most tools that target
    /// Windows.
    Big,
}

/// Specifies the access mode for opening a PDB/MSF file.
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
enum AccessMode {
    /// Read-only access
    Read,
    /// Read-write access
    ReadWrite,
}

impl<F> Msf<F> {
    /// Returns the page size used for this file.
    pub fn page_size(&self) -> PageSize {
        self.pages.page_size
    }

    /// Gets access to the stream page pointers for a given stream. The stream page pointers
    /// provide the mapping from offsets within a stream to offsets within the entire PDB (MSF) file.
    ///
    /// If the stream is a NIL stream, then this returns `(NIL_STREAM_SIZE, &[])`.
    pub fn stream_size_and_pages(&self, stream: u32) -> Result<(u32, &[Page]), anyhow::Error> {
        let Some(&stream_size) = self.stream_sizes.get(stream as usize) else {
            bail!("Stream index is out of range.  Index: {stream}");
        };

        if stream_size == NIL_STREAM_SIZE {
            // This is a NIL stream.
            return Ok((NIL_STREAM_SIZE, &[]));
        }

        // The stream index is valid and the stream is not a NIL stream.
        let num_stream_pages =
            num_pages_for_stream_size(stream_size, self.pages.page_size) as usize;

        if num_stream_pages == 0 {
            // The stream is valid (is not nil) and is a zero-length stream.
            // It has no pages assigned to it.
            return Ok((0, &[]));
        }

        // If this stream has been modified, then return the modified page list.
        if let Some(pages) = self.modified_streams.get(&stream) {
            assert_eq!(num_stream_pages, pages.len());
            return Ok((stream_size, pages.as_slice()));
        }

        let start = self.committed_stream_page_starts[stream as usize] as usize;
        let pages = &self.committed_stream_pages[start..start + num_stream_pages];
        Ok((stream_size, pages))
    }

    /// The total number of streams in this PDB, including nil streams.
    pub fn num_streams(&self) -> u32 {
        self.stream_sizes.len() as u32
    }

    /// Gets the size of a given stream, in bytes.
    ///
    /// The `stream` value must be in a valid range of `0..num_streams()`.
    ///
    /// If `stream` is a NIL stream, this function returns 0.
    pub fn stream_size(&self, stream: u32) -> u32 {
        assert!((stream as usize) < self.stream_sizes.len());
        let stream_size = self.stream_sizes[stream as usize];
        if stream_size == NIL_STREAM_SIZE {
            0
        } else {
            stream_size
        }
    }

    /// Indicates whether a given stream index is valid.
    pub fn is_valid_stream_index(&self, stream: u32) -> bool {
        (stream as usize) < self.stream_sizes.len()
    }

    /// Indicates that a stream index is valid, and that its length is valid.
    pub fn is_stream_valid(&self, stream: u32) -> bool {
        if stream != 0 && (stream as usize) < self.stream_sizes.len() {
            self.stream_sizes[stream as usize] != NIL_STREAM_SIZE
        } else {
            false
        }
    }

    /// Return the nominal length of this file, in bytes.
    ///
    /// This is the number of pages multiplied by the page size. It is not guaranteed to be equal to
    /// the on-disk size of the file, but in practice it usually is.
    pub fn nominal_size(&self) -> u64 {
        page_to_offset(self.pages.num_pages, self.pages.page_size)
    }

    /// Return the total number of pages allocated to the file, including all pages (allocated,
    /// unallocated, etc.).
    ///
    /// This count includes pages allocated to streams, Page 0, FPM pages, pages that are free
    /// (not allocated), and pages allocated to the Stream Directory.
    pub fn num_total_pages(&self) -> u32 {
        self.pages.num_pages
    }

    /// Returns the number of free pages.
    ///
    /// This number counts the pages that are _less than_ `num_pages`. There may be pages assigned
    /// to the MSF file beyond `num_pages`, but if there are then this does not count that space.
    ///
    /// This value does not count Page 0, pages assigned to the FPM, streams, or the current
    /// Stream Directory. It does count pages assigned to the old stream directory.
    pub fn num_free_pages(&self) -> u32 {
        self.pages.fpm.count_ones() as u32
    }

    /// Extracts the underlying file for this MSF. **All pending modifications are dropped**.
    pub fn into_file(self) -> F {
        self.file
    }

    /// Gets access to the contained file
    pub fn file(&self) -> &F {
        &self.file
    }

    /// Gets mutable access to the contained file
    pub fn file_mut(&mut self) -> &mut F {
        &mut self.file
    }

    /// Indicates whether this [`Msf`] was opened for read/write access.
    pub fn is_writable(&self) -> bool {
        self.access_mode == AccessMode::ReadWrite
    }
}

impl<F: ReadAt> Msf<F> {
    /// Reads a portion of a stream to a vector.
    pub fn read_stream_section_to_box(
        &self,
        stream: u32,
        start: u32,
        size: u32,
    ) -> anyhow::Result<Box<[u8]>>
    where
        F: ReadAt,
    {
        let reader = self.get_stream_reader(stream)?;
        let mut stream_data = FromZeros::new_box_zeroed_with_elems(size as usize)
            .map_err(|_| std::io::Error::from(std::io::ErrorKind::OutOfMemory))?;
        reader.read_exact_at(&mut stream_data, u64::from(start))?;
        Ok(stream_data)
    }

    /// Reads the entire stream into a `Box<[u8]>`.
    pub fn read_stream_to_box(&self, stream: u32) -> anyhow::Result<Box<[u8]>> {
        let reader = self.get_stream_reader(stream)?;
        let mut stream_data = FromZeros::new_box_zeroed_with_elems(reader.len() as usize)
            .map_err(|_| std::io::Error::from(std::io::ErrorKind::OutOfMemory))?;
        reader.read_exact_at(&mut stream_data, 0)?;
        Ok(stream_data)
    }

    /// Reads an entire stream to a vector.
    pub fn read_stream_to_vec(&self, stream: u32) -> anyhow::Result<Vec<u8>> {
        let stream_data = self.read_stream_to_box(stream)?;

        // This conversion does not reallocate data. It just adds a 'capacity' field.
        Ok(stream_data.into_vec())
    }

    /// Reads an entire stream into an existing vector.
    pub fn read_stream_to_vec_mut(
        &self,
        stream: u32,
        stream_data: &mut Vec<u8>,
    ) -> anyhow::Result<()> {
        let reader = self.get_stream_reader(stream)?;

        // Do not clear and resize. Doing so requires zeroing all the data in the vector.
        // Since we are going to read into the vector, that means we would modify every byte twice.
        // That's expensive when you're working with a lot of data.
        stream_data.resize(reader.len() as usize, 0);

        reader.read_exact_at(stream_data, 0)?;
        Ok(())
    }

    /// Returns an object which can read from a given stream.  The returned object implements
    /// the [`Read`], [`Seek`], and [`ReadAt`] traits.
    pub fn get_stream_reader(&self, stream: u32) -> anyhow::Result<StreamReader<'_, F>>
    where
        F: ReadAt,
    {
        let (stream_size, stream_pages) = self.stream_size_and_pages(stream)?;
        Ok(StreamReader::new(
            self,
            stream,
            stream_size,
            stream_pages,
            0,
        ))
    }
}

/// Checks whether the header of a file appears to be a valid MSF file.
///
/// This only looks at the signature; it does not read anything else in the file. This is useful
/// for quickly determining whether a file could be an MSF file, but without any validation.
pub fn is_file_header_msf(header: &[u8]) -> bool {
    header.starts_with(&MSF_BIG_MAGIC) || header.starts_with(&MSF_SMALL_MAGIC)
}

/// The absolute minimum size of a slice that could contain a valid MSF file header, as tested by
/// [`is_file_header_msf`].
///
/// This does not specify the minimum valid size of an MSF file. It is only a recommended minimum
/// for callers of [`is_file_header_msf`].
pub const MIN_FILE_HEADER_SIZE: usize = 0x100;

#[doc(hidden)]
pub fn open_options_shared(options: &mut OpenOptions) -> &mut OpenOptions {
    #[cfg(windows)]
    {
        use std::os::windows::fs::OpenOptionsExt;
        const FILE_SHARE_READ: u32 = 1;
        options.share_mode(FILE_SHARE_READ)
    }
    #[cfg(not(windows))]
    {
        options
    }
}

#[doc(hidden)]
pub fn open_options_exclusive(options: &mut OpenOptions) -> &mut OpenOptions {
    #[cfg(windows)]
    {
        use std::os::windows::fs::OpenOptionsExt;
        options.share_mode(0)
    }
    #[cfg(not(windows))]
    {
        options
    }
}

```

`msf/src/open.rs`:

```rs
//! Code for opening or creating MSF files.

use super::*;
use sync_file::RandomAccessFile;
use tracing::{error, trace, trace_span, warn};
use zerocopy::IntoBytes;

/// Options for creating a new PDB/MSF file.
#[derive(Clone, Debug)]
pub struct CreateOptions {
    /// The page size to use. This must be in the range [`MIN_PAGE_SIZE..=MAX_PAGE_SIZE`].
    pub page_size: PageSize,

    /// The maximum number of streams that we will allow to be created using `new_stream` or
    /// `nil_stream`. The default value is 0xfffe, which prevents overflowing the 16-bit stream
    /// indexes that are used in PDB (or confusing them with the "nil" stream index).
    ///
    /// Applications may increase this value beyond the default, but this will produce MSF files
    /// that are not usable by most PDB tools.
    pub max_streams: u32,
}

/// The maximum number of streams that PDB can tolerate.
const DEFAULT_MAX_STREAMS: u32 = 0xfffe;

impl Default for CreateOptions {
    fn default() -> Self {
        Self {
            page_size: DEFAULT_PAGE_SIZE,
            max_streams: DEFAULT_MAX_STREAMS,
        }
    }
}

impl Msf<RandomAccessFile> {
    /// Opens an MSF file for read access, given a file name.
    pub fn open(file_name: &Path) -> anyhow::Result<Self> {
        let file = open_options_shared(File::options().read(true)).open(file_name)?;
        let random_file = RandomAccessFile::from(file);
        Self::new_with_access_mode(random_file, AccessMode::Read)
    }

    /// Creates a new MSF file on disk (**truncating any existing file!**) and creates a new
    /// [`Msf`] object in-memory object with read/write access.
    ///
    /// This function does not write anything to disk until stream data is written or
    /// [`Self::commit`] is called.
    pub fn create(file_name: &Path, options: CreateOptions) -> anyhow::Result<Self> {
        let file = open_options_exclusive(File::options().read(true).write(true).create(true))
            .open(file_name)?;
        let random_file = RandomAccessFile::from(file);
        Self::create_with_file(random_file, options)
    }

    /// Opens an existing MSF file for read/write access, given a file name.
    pub fn modify(file_name: &Path) -> anyhow::Result<Self> {
        let file =
            open_options_exclusive(File::options().read(true).write(true)).open(file_name)?;
        let random_file = RandomAccessFile::from(file);
        Self::modify_with_file(random_file)
    }
}

impl<F: ReadAt> Msf<F> {
    /// Opens an MSF file for read access, given a [`File`] that has already been opened.
    pub fn open_with_file(file: F) -> anyhow::Result<Self> {
        Self::new_with_access_mode(file, AccessMode::Read)
    }

    /// Creates a new MSF file, given a file handle that has already been opened.
    ///
    /// **This function destroys the contents of the existing file.**
    pub fn create_with_file(file: F, options: CreateOptions) -> anyhow::Result<Self> {
        Self::create_for(file, options)
    }

    /// Opens an existing MSF file for read/write access, given an [`File`] that has already
    /// been opened.
    ///
    /// The `file` handle will be used for absolute reads and writes. The caller should never use
    /// this same file handle for reads (and especially not for writes) while also using [`Msf`]
    /// because the operating system's read/write file position may be updated by [`Msf`].
    pub fn modify_with_file(file: F) -> anyhow::Result<Self> {
        Self::new_with_access_mode(file, AccessMode::ReadWrite)
    }

    /// Reads the header of a PDB file and provides access to the streams contained within the
    /// PDB file.
    ///
    /// This function reads the MSF File Header, which is the header for the entire file.
    /// It also reads the stream directory, so it knows how to find each of the streams
    /// and the pages of the streams.
    fn new_with_access_mode(file: F, access_mode: AccessMode) -> anyhow::Result<Self> {
        // Read the MSF File Header.

        let _span = trace_span!("Msf::new_with_access_mode").entered();

        const MIN_PAGE_SIZE_USIZE: usize = 1usize << MIN_PAGE_SIZE.exponent();

        let mut page0: [u8; MIN_PAGE_SIZE_USIZE] = [0; MIN_PAGE_SIZE_USIZE];

        // If this read fails, then the file is too small to be a valid PDB of any kind.
        file.read_exact_at(&mut page0, 0)?;

        let msf_kind: MsfKind;
        let page_size: u32;
        let active_fpm: u32;
        let num_pages: u32;
        let stream_dir_size: u32;

        if page0.starts_with(&MSF_BIG_MAGIC) {
            // unwrap() cannot fail because page0 has a fixed size that is larger than MsfHeader
            let (msf_header, _) = MsfHeader::ref_from_prefix(page0.as_slice()).unwrap();
            page_size = msf_header.page_size.get();
            active_fpm = msf_header.active_fpm.get();
            num_pages = msf_header.num_pages.get();
            stream_dir_size = msf_header.stream_dir_size.get();
            msf_kind = MsfKind::Big;

            // The active FPM can only be 1 or 2.
            if !matches!(active_fpm, 1 | 2) {
                bail!("The PDB header is invalid.  The active FPM is invalid.");
            }
        } else if page0.starts_with(&MSF_SMALL_MAGIC) {
            // Found an "old" MSF header.
            // unwrap() cannot fail because page0 has a fixed size that is larger than SmallMsfHeader
            let (msf_header, _) = SmallMsfHeader::ref_from_prefix(page0.as_slice()).unwrap();
            page_size = msf_header.page_size.get();
            active_fpm = msf_header.active_fpm.get() as u32;
            num_pages = msf_header.num_pages.get() as u32;
            stream_dir_size = msf_header.stream_dir_size.get();
            msf_kind = MsfKind::Small;
        } else if page0[16..24] == *b"PDB v1.0" {
            bail!("This file is a Portable PDB, which is not supported.");
        } else {
            bail!("PDB file does not have the correct header (magic is wrong).");
        }

        let Ok(page_size_pow2) = PageSize::try_from(page_size) else {
            bail!("The PDB header is invalid. The page size ({page_size}) is not a power of 2.",);
        };

        if num_pages == 0 {
            bail!("PDB specifies invalid value for num_pages (zero).");
        }

        let mut stream_sizes: Vec<u32>;

        // The number of pages in the stream directory.
        let stream_dir_num_pages = stream_dir_size.div_round_up(page_size_pow2);

        // Create the PageAllocator. This initializes the fpm vector to "everything is free"
        // and then sets Page 0 and the FPM pages as "free". Nothing is marked as "freed".
        let mut page_allocator = PageAllocator::new(num_pages as usize, page_size_pow2);

        let mut committed_stream_pages: Vec<Page>;
        let mut committed_stream_page_starts: Vec<u32>;

        match msf_kind {
            MsfKind::Big => {
                // "Big MSF" uses a 3-level hierarchy for the Stream Directory:
                //
                // stream_dir_map        <-- contains u32 pages to ↓
                // stream_dir_pages      <-- contains u32 pages to ↓
                // stream_dir_bytes      <-- bottom level, stored in pages
                //
                // stream_dir_map is an array of u32 page pointers. It is stored directly in
                // page 0, immediately after MsfHeader. These pointers point to pages that contain
                // the stream_dir_pages, which is the next level down.
                // The number of pages allocated to stream_dir_map = ceil(stream_dir_pages.len() * 4 / page_size).
                // The number of bytes used within stream_dir_map = stream_dir_pages.len() * 4.
                //
                // stream_dir_pages is a set of pages. When concatenated, they contain the page
                // pointers that point to the stream directory bytes.
                // The number of pages in stream_dir_pages = ceil(stream_dir_size / page_size).
                // The number of bytes used within stream_dir_pages is stream_dir_pages * 4.

                if stream_dir_size % 4 != 0 {
                    bail!("MSF Stream Directory has an invalid size; it is not a multiple of 4.");
                }

                // We are going to read the stream directory into this vector.
                let mut stream_dir: Vec<u32> = vec![0; stream_dir_size as usize / 4];

                // Read the page map for the stream directory.
                let stream_dir_l1_num_pages =
                    num_pages_for_stream_size(4 * stream_dir_num_pages, page_size_pow2) as usize;
                let Ok((page_map_l1_ptrs, _)) = <[U32<LE>]>::ref_from_prefix_with_elems(
                    &page0[STREAM_DIR_PAGE_MAP_FILE_OFFSET as usize..],
                    stream_dir_l1_num_pages,
                ) else {
                    bail!("Stream dir size is invalid (exceeds design limits)");
                };

                let stream_dir_bytes: &mut [u8] = stream_dir.as_mut_bytes();
                let mut stream_dir_chunks = stream_dir_bytes.chunks_mut(page_size as usize);
                // Now read the stream pages for the stream dir.
                let mut l1_page: Vec<u8> = vec![0; page_size as usize];
                'l1_loop: for &page_map_l1_ptr in page_map_l1_ptrs.iter() {
                    let page_map_l1_ptr: u32 = page_map_l1_ptr.get();

                    page_allocator.init_mark_stream_dir_page_busy(page_map_l1_ptr)?;
                    if is_special_page_big_msf(page_size_pow2, page_map_l1_ptr) {
                        bail!(
                            "Stream dir contains invalid page number: {page_map_l1_ptr}. \
                             Page points to Page 0 or to an FPM page."
                        );
                    }

                    // Read the page pointers.
                    let file_offset = page_to_offset(page_map_l1_ptr, page_size_pow2);
                    file.read_exact_at(l1_page.as_mut_slice(), file_offset)?;

                    // Now read the individual pages, as long as we have more.
                    let l2_page_u32 = <[U32<LE>]>::ref_from_bytes(l1_page.as_slice()).unwrap();

                    for &l2_page in l2_page_u32.iter() {
                        let l2_page: u32 = l2_page.get();

                        let Some(stream_dir_chunk) = stream_dir_chunks.next() else {
                            break 'l1_loop;
                        };

                        page_allocator.init_mark_stream_dir_page_busy(l2_page)?;
                        if is_special_page_big_msf(page_size_pow2, l2_page) {
                            bail!(
                                "Stream dir contains invalid page number: {l2_page}. \
                                 Page points to Page 0 or to an FPM page."
                            );
                        }

                        let l2_file_offset = page_to_offset(l2_page, page_size_pow2);
                        file.read_exact_at(stream_dir_chunk, l2_file_offset)?;
                    }
                }

                if stream_dir.is_empty() {
                    bail!("Stream directory is invalid (zero-length)");
                }

                // Bulk-convert the stream directory to host endian, if necessary.
                if !cfg!(target_endian = "little") {
                    for x in stream_dir.iter_mut() {
                        *x = u32::from_le(*x);
                    }
                }

                let num_streams = stream_dir[0] as usize;

                // Stream 0 is special and must exist.
                if num_streams == 0 {
                    bail!("MSF file is invalid, because num_streams = 0.");
                }

                let Some(stream_sizes_src) = stream_dir.get(1..1 + num_streams) else {
                    bail!("Stream directory is invalid (num_streams is not consistent with size)");
                };
                stream_sizes = stream_sizes_src.to_vec();

                let mut stream_pages_iter = &stream_dir[1 + num_streams..];

                // Build committed_stream_pages and committed_stream_page_starts.
                committed_stream_pages = Vec::with_capacity(stream_dir.len() - num_streams - 1);
                committed_stream_page_starts = Vec::with_capacity(num_streams + 1);

                for (stream, &stream_size) in stream_sizes_src.iter().enumerate() {
                    committed_stream_page_starts.push(committed_stream_pages.len() as u32);

                    if stream_size != NIL_STREAM_SIZE {
                        let num_stream_pages =
                            num_pages_for_stream_size(stream_size, page_size_pow2) as usize;
                        if num_stream_pages > stream_pages_iter.len() {
                            bail!(
                                "Stream directory is invalid.  Stream {stream} has size {stream_size}, \
                                 which exceeds the size of the stream directory."
                            );
                        }
                        let (this_stream_pages, next) =
                            stream_pages_iter.split_at(num_stream_pages);
                        stream_pages_iter = next;
                        committed_stream_pages.extend_from_slice(this_stream_pages);
                    }
                }
                committed_stream_page_starts.push(committed_stream_pages.len() as u32);

                // Now that we have finished reading the stream directory, we set the length
                // of stream 0 (the "Old Stream Directory") to 0. Nothing should ever read Stream 0.
                // If we modify a PDB/MSF file, then we want to write no pages at all for Stream 0.
                // Doing this here is the most convenient way to handle this.
                stream_sizes[0] = 0;
            }

            MsfKind::Small => {
                // Before Big MSF files, the stream directory was stored in a set of pages.
                // These pages were listed directly within page 0. Keep in mind that page numbers
                // are 16-bit in old MSF files.
                let page_pointers_size_bytes = stream_dir_num_pages * 2;

                let mut pages_u16: Vec<U16<LE>> = vec![U16::new(0); stream_dir_num_pages as usize];
                if page_pointers_size_bytes + size_of::<SmallMsfHeader>() as u32 > page_size {
                    bail!(
                        "The MSF header is invalid. The page pointers for the stream directory \
                         exceed the range of the first page. \
                         Stream dir size (in bytes): {stream_dir_size}  Page size: {page_size}"
                    );
                }

                file.read_exact_at(pages_u16.as_mut_bytes(), size_of::<SmallMsfHeader>() as u64)?;

                // Read the pages of the stream directory. Be careful with the last page.
                let mut page_iter = pages_u16.iter();
                let mut old_stream_dir_bytes: Vec<u8> = vec![0; stream_dir_size as usize];
                for stream_dir_chunk in old_stream_dir_bytes.chunks_mut(page_size as usize) {
                    // This unwrap should succeed because we computed the length of pages_u16
                    // based on the byte size of the stream directory.
                    let page = page_iter.next().unwrap().get() as u32;
                    page_allocator.init_mark_stream_dir_page_busy(page)?;
                    file.read_exact_at(stream_dir_chunk, page_to_offset(page, page_size_pow2))?;
                }

                let Ok((header, rest)) =
                    OldMsfStreamDirHeader::read_from_prefix(old_stream_dir_bytes.as_slice())
                else {
                    bail!("Invalid stream directory: too small");
                };

                let num_streams = header.num_streams.get() as usize;
                stream_sizes = Vec::with_capacity(num_streams);

                let Ok((entries, mut rest)) =
                    <[OldMsfStreamEntry]>::ref_from_prefix_with_elems(rest, num_streams)
                else {
                    bail!("Invalid stream directory: too small")
                };

                for entry in entries.iter() {
                    let stream_size = entry.stream_size.get();
                    stream_sizes.push(stream_size);
                }

                committed_stream_page_starts = Vec::with_capacity(num_streams + 1);
                committed_stream_pages = Vec::new(); // TODO: precompute capacity

                for &stream_size in stream_sizes.iter() {
                    committed_stream_page_starts.push(committed_stream_pages.len() as u32);
                    if stream_size != NIL_STREAM_SIZE {
                        let num_pages = stream_size.div_round_up(page_size_pow2);

                        let Ok((pages, r)) =
                            <[U16<LE>]>::ref_from_prefix_with_elems(rest, num_pages as usize)
                        else {
                            bail!("Invalid stream directory: too small");
                        };

                        rest = r; // update iterator state
                        for page in pages.iter() {
                            committed_stream_pages.push(page.get() as u32);
                        }
                    }
                }

                committed_stream_page_starts.push(committed_stream_pages.len() as u32);

                if !rest.is_empty() {
                    warn!(
                        unused_bytes = rest.len(),
                        "old-style stream dir contained unused bytes"
                    );
                }
            }
        }

        // Mark the pages in all streams (except for stream 0) as busy. This will also detect
        // page numbers that are invalid (0 or FPM).
        mark_stream_pages_busy(
            &committed_stream_page_starts,
            &committed_stream_pages,
            &mut page_allocator.fpm,
            page_size_pow2,
            msf_kind,
            access_mode,
        )?;

        // We have finished building the in-memory FPM, including both the fpm and fpm_freed
        // vectors. We expect that every page is either FREE, BUSY, or DELETED. Check that now.
        page_allocator.check_vector_consistency()?;

        // Read the FPM from disk and compare it to the FPM that we just constructed. They should
        // be identical.
        // TODO: implement for small MSF

        // If we are opening this MSF file for read-write access, then check that it meets our
        // consistency requirements. Some tools generate PDBs that violate some consistency
        // requirements, mainly the bits set in the FPM. We want to avoid checking checking
        // requirements so that we can read data from those tools, even if they don't set the FPM
        // bits correctly.
        if access_mode == AccessMode::ReadWrite {
            let fpm_on_disk = read_fpm_big_msf(&file, active_fpm, num_pages, page_size_pow2)?;

            assert_eq!(fpm_on_disk.len(), page_allocator.fpm.len()); // because num_pages defines both

            if page_allocator.fpm != fpm_on_disk {
                warn!("FPM computed from Stream Directory is not equal to FPM found on disk.");
                warn!(
                    "Num pages = {num_pages} (0x{num_pages:x} bytes, bit offset: 0x{:x}:{})",
                    num_pages / 8,
                    num_pages % 8
                );

                for i in 0..num_pages as usize {
                    if fpm_on_disk[i] != page_allocator.fpm[i] {
                        warn!(
                            "  bit 0x{:04x} is different. disk = {}, computed = {}",
                            i, fpm_on_disk[i], page_allocator.fpm[i]
                        );
                    }
                }

                // Clang's PDB writer sometimes places stream pages at illegal locations,
                // such as in the pages reserved for the FPM. We tolerate this for reading
                // but not for writing.
                if access_mode == AccessMode::ReadWrite {
                    bail!(
                        "FPM is corrupted; FPM computed from Stream Directory is not equal to FPM found on disk."
                    );
                }
            }
        }

        // We have finished checking all the data that we have read from disk.
        // Now check the consistency of our in-memory data structures.
        page_allocator.assert_invariants();

        match (access_mode, msf_kind) {
            (AccessMode::ReadWrite, MsfKind::Small) => {
                bail!(
                    "This PDB file uses the obsolete 'Small MSF' encoding. \
                     This library does not support read-write mode with Small MSF files."
                );
            }

            (AccessMode::ReadWrite, MsfKind::Big) => {}

            (AccessMode::Read, _) => {}
        }

        Ok(Self {
            file,
            access_mode,
            active_fpm,
            committed_stream_pages,
            committed_stream_page_starts,
            stream_sizes,
            kind: msf_kind,
            pages: page_allocator,
            modified_streams: HashMap::new(),
            max_streams: DEFAULT_MAX_STREAMS,
        })
    }

    /// Creates a new MSF object in memory. The on-disk file is not modified until `commit()` is
    /// called.
    pub fn create_for(file: F, options: CreateOptions) -> anyhow::Result<Self> {
        let _span = trace_span!("Msf::create_for").entered();

        assert!(options.page_size >= MIN_PAGE_SIZE);
        assert!(options.page_size <= MAX_PAGE_SIZE);

        let num_pages: usize = 3;

        let mut this = Self {
            file,
            access_mode: AccessMode::ReadWrite,
            committed_stream_pages: vec![],
            committed_stream_page_starts: vec![0; 2],
            kind: MsfKind::Big,
            pages: PageAllocator::new(num_pages, options.page_size),
            modified_streams: HashMap::new(),
            stream_sizes: vec![0],
            active_fpm: 2,
            max_streams: options.max_streams,
        };

        // Set up the 4 fixed-index streams. They are created as nil streams.
        for _ in 1..=4 {
            let _stream_index = this.nil_stream()?;
        }

        Ok(this)
    }
}

/// Read each page of the FPM. Each page of the FPM is stored in a different interval;
/// they are not contiguous.
///
/// num_pages is the total number of pages in the FPM.
fn read_fpm_big_msf<F: ReadAt>(
    file: &F,
    active_fpm: u32,
    num_pages: u32,
    page_size: PageSize,
) -> anyhow::Result<BitVec<u32, Lsb0>> {
    let _span = trace_span!("read_fpm_big_msf").entered();

    assert!(num_pages > 0);

    let mut free_page_map: BitVec<u32, Lsb0> = BitVec::new();
    free_page_map.resize(num_pages as usize, false);
    let fpm_bytes: &mut [u8] = free_page_map.as_raw_mut_slice().as_mut_bytes();
    let page_size_usize = usize::from(page_size);

    for (interval, fpm_page_bytes) in fpm_bytes.chunks_mut(page_size_usize).enumerate() {
        let interval_page = interval_to_page(interval as u32, page_size);
        let file_pos = page_to_offset(interval_page + active_fpm, page_size);

        trace!(
            interval,
            interval_page,
            file_pos,
            "reading FPM page, interval_page = 0x{interval_page:x}, file_pos = 0x{file_pos:x}"
        );
        file.read_exact_at(fpm_page_bytes, file_pos)?;
    }

    // Check our invariants for the FPM. If these checks fail then we return Err because we
    // are validating data that we read from disk. After these checks succeed, we switch to using
    // assert_invariants(), which uses assert!(). That verifies that we preserve our invariants.

    // Check that page 0, which stores the MSF File Header, is busy.
    if free_page_map[0] {
        bail!("FPM is invalid: Page 0 should always be BUSY");
    }

    // Check that the pages assigned to the FPM are marked "busy" in all intervals.

    let mut interval: u32 = 0;
    loop {
        let interval_page = interval_to_page(interval, page_size) as usize;
        let fpm1_index = interval_page + 1;
        let fpm2_index = interval_page + 2;

        if fpm1_index < free_page_map.len() {
            if free_page_map[fpm1_index] {
                bail!("All FPM pages should be marked BUSY");
            }
        }

        if fpm2_index < free_page_map.len() {
            if free_page_map[fpm2_index] {
                bail!("All FPM pages should be marked BUSY");
            }
            interval += 1;
        } else {
            break;
        }
    }

    Ok(free_page_map)
}

/// Computes the low-bits-on mask for the page mask.
fn low_page_mask(page_size: PageSize) -> u32 {
    (1u32 << page_size.exponent()).wrapping_sub(1u32)
}

/// Tests whether `page` contributes to either FPM1 or FPM2.
fn is_fpm_page_big_msf(page_size: PageSize, page: u32) -> bool {
    let page_within_interval = page & low_page_mask(page_size);
    matches!(page_within_interval, 1 | 2)
}

/// Tests whether `page` is one of the special pages (Page 0, FPM1, or FPM2)
fn is_special_page_big_msf(page_size: PageSize, page: u32) -> bool {
    page == 0 || is_fpm_page_big_msf(page_size, page)
}

/// Describes the "old" MSF Stream Directory Header.
#[derive(Clone, IntoBytes, FromBytes, Unaligned, KnownLayout, Immutable)]
#[repr(C)]
struct OldMsfStreamDirHeader {
    num_streams: U16<LE>,
    ignored: U16<LE>,
}

/// An entry in the "old" MSF Stream Directory.
#[derive(Clone, IntoBytes, FromBytes, Unaligned, KnownLayout, Immutable)]
#[repr(C)]
struct OldMsfStreamEntry {
    stream_size: U32<LE>,
    ignored: U32<LE>,
}

/// Mark all of the pages that are assigned to streams (except for Stream 0) as being "busy" in
/// the Free Page Map.
///
/// This is used only when loading the Stream Directory.
///
/// `all_stream_pages` contains the list of pages assigned to all streams. The pages are listed
/// in order by stream, then in order by page within stream. The `stream_page_starts` table
/// gives the starting index within `all_stream_pages` for each stream. `stream_page_starts.len()`
/// is equal to `num_streams + 1`.
///
/// `fpm` is the Free Page Map. `fpm.len()` is equal to `num_pages` from the MSF File Header.
/// This function validates that all stream page indices are valid, where valid means:
///
/// * less than `num_pages`
/// * not Page 0
/// * not assigned to the Free Page Map (although this requirement is relaxed for read-only mode)
/// * not already marked busy
///
/// This ignores pages in Stream 0, which is the Old Stream Directory. The corresponding bits
/// in the Free Page Map are *not* modified.
fn mark_stream_pages_busy(
    stream_page_starts: &[u32],
    all_stream_pages: &[u32],
    fpm: &mut BitVec<u32, Lsb0>,
    page_size: Pow2,
    msf_kind: MsfKind,
    access_mode: AccessMode,
) -> anyhow::Result<()> {
    let page_within_interval_mask = low_page_mask(page_size);
    let strict_mode = access_mode == AccessMode::ReadWrite;

    // The skip(1) skips the Old Stream Directory.
    for (stream, range) in stream_page_starts.windows(2).enumerate().skip(1) {
        let stream_pages = &all_stream_pages[range[0] as usize..range[1] as usize];

        for (stream_page, &page) in stream_pages.iter().enumerate() {
            if page == 0 {
                bail!(
                    "Page cannot be marked busy because it points to the first page of the file. Stream {0} is invalid.",
                    stream
                );
            }

            // Clang's PDB writer currently generates PDBs that assign stream pages to pages
            // reserved for the FPM. That's illegal, but the MSVC implementation of PDB/MSF
            // does not detect that problem. We check for it here and report an error (and
            // refuse to open the PDB/MSF file) if the access mode is read/write. If the access
            // mode is read-only then we report a warning but still open the file.

            if msf_kind == MsfKind::Big {
                let page_within_interval = page & page_within_interval_mask;
                if page_within_interval == 1 || page_within_interval == 2 {
                    warn!(
                        "Page {page} is invalid; it is assigned to a page reserved for the Free Page Map. Stream {0} is invalid.",
                        stream
                    );
                    if strict_mode {
                        bail!(
                            "Page {page} is invalid; it is assigned to a page reserved for the Free Page Map. Stream {0} is invalid.",
                            stream
                        );
                    }
                    // The page will already have been marked busy. Skip the code below which
                    // marks the page as busy, so that don't report two warnings.
                    continue;
                }
            }

            if let Some(mut page_is_free) = fpm.get_mut(page as usize) {
                if !*page_is_free {
                    error!(
                        page,
                        stream,
                        stream_page,
                        "Page cannot be marked busy, because it is already marked busy. It may be used by more than one stream."
                    );
                    if strict_mode {
                        bail!(
                            "Page {page} cannot be marked busy, because it is already marked busy. It may be used by more than one stream. Stream #{stream}"
                        );
                    } else {
                        continue;
                    }
                }

                page_is_free.set(false);
            } else {
                error!(
                    page,
                    stream, stream_page, "Page is invalid; it is out of range (exceeds num_pages)"
                );
                bail!(
                    "Page {} is invalid; it is out of range (exceeds num_pages)",
                    page
                );
            }
        }
    }

    Ok(())
}

```

`msf/src/pages.rs`:

```rs
//! Page management code

use super::*;
use tracing::{trace, trace_span};
use zerocopy::FromZeros;

/// Given the size of a stream in bytes, returns the number of pages needed to store it.
///
/// This function correctly handles the case where the stream size is [`NIL_STREAM_SIZE`].
/// In this case, it returns 0.
pub(crate) fn num_pages_for_stream_size(stream_size: u32, page_size: PageSize) -> u32 {
    if stream_size == NIL_STREAM_SIZE {
        0
    } else {
        stream_size.div_round_up(page_size)
    }
}

/// Maps ranges of bytes within a stream to contiguous ranges of bytes in the containing MSF file.
pub(crate) struct StreamPageMapper<'a> {
    pages: &'a [Page],
    page_size: PageSize,
    stream_size: u32,
}

impl<'a> StreamPageMapper<'a> {
    pub(crate) fn new(pages: &'a [Page], page_size: PageSize, stream_size: u32) -> Self {
        assert_eq!(
            num_pages_for_stream_size(stream_size, page_size) as usize,
            pages.len()
        );

        Self {
            pages,
            page_size,
            stream_size,
        }
    }

    /// Maps a byte offset and a length within a stream to a contiguous run of bytes within the MSF file.
    ///
    /// Repeated calls to this function (with increasing values of `pos`) can be used to read/write
    /// the contents of a stream using the smallest number of read/write calls to the underlying
    /// MSF file.
    ///
    /// Returns `(file_offset, transfer_len)` where `file_offset` is the byte offset within the MSF
    /// file and `transfer_len` is the length of the longest contiguous sub-range of the requested
    /// range.
    ///
    /// If this returns `None` then no bytes can be mapped. This occurs when `pos >= stream_size`.
    ///
    /// Invariants:
    ///
    /// * if returned `Some`, then `transfer_len <= bytes_wanted`
    /// * if returned `Some`, then `transfer_len > 0`
    pub(crate) fn map(&self, pos: u32, bytes_wanted: u32) -> Option<(u64, u32)> {
        if self.stream_size == NIL_STREAM_SIZE {
            return None;
        }

        if pos >= self.stream_size {
            return None;
        }

        let bytes_available = self.stream_size - pos;
        let max_transfer_size = bytes_available.min(bytes_wanted);

        if max_transfer_size == 0 {
            return None;
        }

        // We will reduce transfer_size as needed.
        let transfer_size: u32;

        // Find the position within the file where the read will start.
        let first_page_index = pos >> self.page_size.exponent();
        let first_page_pointer = self.pages[first_page_index as usize];
        let first_page_file_offset = (first_page_pointer as u64) << self.page_size.exponent();

        let offset_within_first_page = pos - self.page_size.align_down(pos);
        let file_offset = first_page_file_offset + offset_within_first_page as u64;

        // Find the longest read we can execute in a single underlying read call.
        // If pages are numbered consecutively, then cover as many pages as we can.

        // Does the beginning of the read cross a page boundary?
        let bytes_available_first_page = u32::from(self.page_size) - offset_within_first_page;
        if max_transfer_size > bytes_available_first_page {
            // Yes, this read crosses a page boundary.
            // Set transfer_size to just the bytes in the first page.
            // Then, keep advancing through the page list as long as pages are sequential.
            let mut p = pos + bytes_available_first_page;
            assert!(self.page_size.is_aligned(p));

            let mut last_page_ptr = first_page_pointer;

            loop {
                assert!(
                    p - pos <= max_transfer_size,
                    "p = {p}, max_transfer_size = {max_transfer_size}"
                );
                let want_bytes = max_transfer_size - (p - pos);

                if p - pos == max_transfer_size {
                    // Reached max transfer size.
                    break;
                }

                let p_page = p >> self.page_size.exponent();

                let p_ptr = self.pages[p_page as usize];
                assert!(p_page > first_page_index);

                if p_ptr != last_page_ptr + 1 {
                    // The pages are not contiguous, so we stop here.
                    break;
                }

                // Advance over this page.
                p += want_bytes.min(u32::from(self.page_size));
                last_page_ptr += 1;
            }

            transfer_size = p - pos;
        } else {
            // This range does not cross a page boundary; it fits within a single page.
            transfer_size = max_transfer_size;
        }

        assert!(transfer_size > 0);

        assert!(
            transfer_size <= bytes_wanted,
            "transfer_size = {transfer_size}, bytes_wanted = {bytes_wanted}"
        );

        Some((file_offset, transfer_size))
    }
}

#[test]
fn test_page_mapper_nil() {
    const PAGE_SIZE: PageSize = PageSize::from_exponent(12); // 0x1000

    let mapper = StreamPageMapper::new(&[], PAGE_SIZE, NIL_STREAM_SIZE);
    assert_eq!(mapper.map(0, 0), None);
    assert_eq!(mapper.map(0x1000, 0x1000), None);
}

#[test]
fn test_page_mapper_basic() {
    const PAGE_SIZE: PageSize = PageSize::from_exponent(12); // 0x1000

    let mapper = StreamPageMapper::new(&[5, 6, 7, 300, 301], PAGE_SIZE, 0x4abc);

    assert_eq!(mapper.map(0, 0), None, "empty read within stream boundary");

    assert_eq!(
        mapper.map(0x1000_0000, 0),
        None,
        "empty read outside stream boundary"
    );

    assert_eq!(
        mapper.map(0x1000_0000, 0x1000),
        None,
        "outside stream boundary"
    );

    assert_eq!(
        mapper.map(0, 0x10),
        Some((0x5000, 0x10)),
        "aligned start, unaligned end, within first page"
    );

    assert_eq!(
        mapper.map(0, 0x1000),
        Some((0x5000, 0x1000)),
        "aligned start, aligned end, single page"
    );

    assert_eq!(
        mapper.map(0, 0x1eee),
        Some((0x5000, 0x1eee)),
        "aligned start, crosses page boundary, unaligned end"
    );

    assert_eq!(
        mapper.map(0, 0x3eee),
        Some((0x5000, 0x3000)),
        "aligned start, crosses page boundary, unaligned end, clipped at page boundary"
    );

    assert_eq!(
        mapper.map(0, 0x1000_0000),
        Some((0x5000, 0x3000)),
        "aligned start, aligned end beyond stream size, max contiguous span"
    );

    assert_eq!(
        mapper.map(0xccc, 0x10),
        Some((0x5ccc, 0x10)),
        "unaligned start, ends within first page"
    );

    assert_eq!(
        mapper.map(0xccc, 0x1000),
        Some((0x5ccc, 0x1000)),
        "unaligned start, crosses page boundary, unaligned end"
    );

    assert_eq!(
        mapper.map(0xccc, 0x1000_0000),
        Some((0x5ccc, 0x2334)),
        "unaligned start, crosses page boundary, clipped at page boundary"
    );
}

/// Contains state for allocating pages.
///
/// The `fpm`, `fpm_freed` and `fresh` bit vectors all describe the state of pages. These are
/// parallel vectors; the same index in each vector is related to the same page. Only certain
/// combinations of values for these vectors are legal.
///
/// Each page `p` can be in one of the following states:
///
/// `fpm[p]`  | `fpm_freed[p]` | State    | Description
/// ----------|----------------|----------|------------------
/// `true`    | `false`        | FREE     | The page is available for use. This page is not used by any stream.
/// `false`   | `false`        | BUSY     | The page is being used by a stream.
/// `false`   | `true`         | DELETING | The page is used by the previous (committed) state of some stream, but has been deleted in the next (uncommitted) state.
/// `true`    | `true`         | (illegal) | (illegal)
pub(super) struct PageAllocator {
    /// Free Page Map (FPM): A bit vector that lists the pages in the MSF that are free.
    pub(super) fpm: BitVec<u32, Lsb0>,

    /// A bit vector that lists the pages in the MSF that are valid in the committed state
    /// but which have been deleted in the uncommitted state.
    pub(super) fpm_freed: BitVec<u32, Lsb0>,

    /// A bit vector that tells us whether we have copied a page and it is now writable.
    /// This vector is set to all-false. Whenever we allocate a new page and copy some contents to
    /// it, or when we allocate a new page, we set the corresponding bit in `fresh`.
    pub(super) fresh: BitVec<u32, Lsb0>,

    /// The index of the next free page to check in the Free Page Map, when allocating pages.
    ///
    /// This index is not guaranteed to point to an entry that is free. It points to the next index
    /// to check.
    ///
    /// Invariant: `next_free_page <= num_pages`
    pub(super) next_free_page_hint: Page,

    /// The number of valid pages in the file. This value comes from the MSF File Header.
    pub(super) num_pages: u32,

    pub(super) page_size: PageSize,

    /// A reusable buffer whose length is `page_size`.
    #[allow(dead_code)]
    pub(super) page_buffer: Box<[u8]>,
}

impl PageAllocator {
    /// Allocate a bit vector for the FPM. We are going to compute this FPM from num_pages
    /// and the contents of the stream directory. After we finish computing it, we will also
    /// read the active FPM from disk and verify that it is exactly the same as the one that
    /// we computed.
    ///
    /// We begin with setting _all_ pages free. Then we mark Page 0 and the FPM pages as busy.
    /// It is the caller's responsibility to set other bits in the FPM accordingly.
    pub(crate) fn new(num_pages: usize, page_size: PageSize) -> Self {
        let mut fpm: BitVec<u32, Lsb0> = BitVec::with_capacity(num_pages);
        fpm.resize(num_pages, true);
        fpm.set(0, false);

        // Mark FPM pages as busy.
        for interval in 0u32.. {
            let fpm1_page = (interval << page_size.exponent()) + 1u32;
            let fpm2_page = fpm1_page + 1u32;
            if let Some(mut b) = fpm.get_mut(fpm1_page as usize) {
                b.set(false);
            }
            if let Some(mut b) = fpm.get_mut(fpm2_page as usize) {
                b.set(false);
            } else {
                break;
            }
        }

        let mut fpm_freed: BitVec<u32, Lsb0> = BitVec::with_capacity(num_pages);
        fpm_freed.resize(num_pages, false);

        let mut fresh: BitVec<u32, Lsb0> = BitVec::with_capacity(num_pages);
        fresh.resize(num_pages, false);

        Self {
            fpm,
            fpm_freed,
            fresh,
            next_free_page_hint: 0,
            num_pages: num_pages as u32,
            page_size,
            // unwrap() is for OOM handling
            page_buffer: FromZeros::new_box_zeroed_with_elems(usize::from(page_size)).unwrap(),
        }
    }

    pub(crate) fn alloc_page_buffer(&self) -> Box<[u8]> {
        // unwrap() is for OOM handling
        FromZeros::new_box_zeroed_with_elems(usize::from(self.page_size)).unwrap()
    }

    /// This marks a page as "busy but freed". This is called for pages of the Stream Directory,
    /// and when using Big MSF, for pages of the Page Map.
    ///
    /// These pages are marked "free pending" (freed) because these pages become unused after
    /// the next successful call to `commit()`.
    pub(crate) fn init_mark_stream_dir_page_busy(&mut self, page: Page) -> anyhow::Result<()> {
        // We mark the page as "freed". It is still marked "free" in the FPM.
        // The page allocator will not use this page, because it is marked "freed".

        let Some(mut b) = self.fpm.get_mut(page as usize) else {
            bail!(
                "Page {} is invalid; it is out of range (exceeds num_pages)",
                page
            );
        };

        if !*b {
            bail!(
                "Page {page} cannot be marked busy, because it is already marked busy. It may be used by more than one stream."
            );
        }
        b.set(false);

        // Now mark the page as "freed".
        let Some(mut freed) = self.fpm_freed.get_mut(page as usize) else {
            bail!(
                "Page {} is invalid; it is out of range (exceeds num_pages)",
                page
            );
        };
        if *freed {
            bail!(
                "Page {page} cannot be marked 'freed', because it is already marked freed. This indicates that the page was used more than once in the Stream Directory or Page Map."
            );
        }
        freed.set(true);

        Ok(())
    }

    /// Allocates a single page.
    ///
    /// This function does not do any disk I/O. It only updates in-memory state.
    pub(crate) fn alloc_page(&mut self) -> Page {
        let (page, run_len) = self.alloc_pages(1);
        debug_assert_eq!(run_len, 1);
        page
    }

    /// Allocates one or more contiguous pages.
    ///
    /// This function handles crossing interval boundaries. If `num_pages_wanted` is large enough that
    /// it crosses an interval boundary, then this function will return a run length that is
    /// smaller than `num_pages_wanted`.  If this function does not cross an interval boundary, then
    /// the returned `run_len` value will be equal to `num_pages_wanted`.
    ///
    /// This function does not do any disk I/O. It only updates in-memory state.
    pub(crate) fn alloc_pages(&mut self, num_pages_wanted: u32) -> (Page, u32) {
        let _span = trace_span!("alloc_pages").entered();
        trace!(num_pages_wanted);

        assert!(num_pages_wanted > 0);
        assert_eq!(self.num_pages as usize, self.fpm.len());
        assert_eq!(self.num_pages as usize, self.fpm_freed.len());
        assert!(self.next_free_page_hint <= self.num_pages);

        // First, check to see whether an existing page is free.
        if self.next_free_page_hint < self.fpm.len() as u32 {
            if let Some(i) = self.fpm.as_bitslice()[self.next_free_page_hint as usize..].first_one()
            {
                let p0: Page = self.next_free_page_hint + i as u32;

                // We found an existing free page. Mark the page as busy.
                self.fpm.set(p0 as usize, false);
                self.fresh.set(p0 as usize, true);
                self.next_free_page_hint = p0 + 1;

                let mut run_len: u32 = 1;

                // See if the pages immediately following this first page are also free.
                // If they are, then claim them, too.
                while run_len < num_pages_wanted
                    && p0 + run_len < self.num_pages
                    && self.fpm[self.next_free_page_hint as usize]
                {
                    self.fpm.set(self.next_free_page_hint as usize, false);
                    self.fresh.set(self.next_free_page_hint as usize, true);
                    self.next_free_page_hint += 1;
                    run_len += 1;
                }

                trace!(first_page = p0, run_len, "allocated pages");
                return (p0, run_len);
            }

            // There are no more free pages. Fast-forward to the end of the FPM so we don't
            // waste time re-scanning this part of the FPM on future calls.
            trace!("there are no free pages");
            self.next_free_page_hint = self.fpm.len() as u32;
        }

        // We need to add new pages to the MSF file.
        trace!(num_pages = self.num_pages, "adding new pages to MSF file");
        assert_eq!(self.next_free_page_hint, self.num_pages);
        let low_mask = (1u32 << self.page_size.exponent()) - 1;
        let page_size = u32::from(self.page_size);
        let num_pages_available = match self.num_pages & low_mask {
            0 => {
                // This is an unusual but legal case. num_pages is currently positioned exactly at
                // the beginning of an interval. There is exactly 1 usable page at the start of an
                // interval; after that page is the FPM1 and then the FPM2. So we can only allocate
                // a single page.
                trace!(
                    "num_pages is positioned on first page of an interval; can only allocate 1 page"
                );
                1
            }
            1 => {
                // num_pages is positioned on FPM1. That's fine.  Step over FPM1 and FPM2.
                // Increment phase to pretend like that's how we got here in the first place.
                trace!(
                    "num_pages is positioned on FPM1; incrementing by 2 and using remainder of interval"
                );
                self.fpm_freed.push(false); // FPM1
                self.fpm_freed.push(false); // FPM2
                self.fpm.push(false); // FPM1
                self.fpm.push(false); // FPM2
                self.num_pages += 2;
                self.next_free_page_hint += 2;
                page_size - 2
            }
            2 => {
                // We are positioned on FPM2. That's unusual but OK. Step over FPM2 and mark it
                // as busy.
                trace!(
                    "num_pages is positioned on FPM2; incrementing by 1 and using remainder of interval"
                );
                self.fpm_freed.push(false);
                self.fpm.push(false);
                self.num_pages += 1;
                self.next_free_page_hint += 1;
                page_size - 2
            }
            phase => page_size - phase + 1,
        };

        assert_eq!(self.next_free_page_hint, self.num_pages);

        let num_pages_allocated = num_pages_available.min(num_pages_wanted);
        assert!(num_pages_allocated > 0);

        let start_page = self.num_pages;
        self.num_pages += num_pages_allocated;

        // Extend the bitmaps and set their new values.
        self.fpm_freed.resize(self.num_pages as usize, false);
        self.fpm.resize(self.num_pages as usize, false);
        self.fresh.resize(self.num_pages as usize, true);

        // Advance next_free_page so that we don't keep re-scanning the same region of
        // the FPM repeatedly.
        self.next_free_page_hint = self.num_pages;

        assert_eq!(self.num_pages as usize, self.fpm.len());
        assert_eq!(self.num_pages as usize, self.fpm_freed.len());
        assert_eq!(self.num_pages as usize, self.fresh.len());

        trace!(start_page, num_pages_allocated, "allocated pages");
        (start_page, num_pages_allocated)
    }

    /// Ensures that a given page is mutable (is "fresh", i.e. can be modified in the uncommitted
    /// state).
    ///
    /// * If `*page` is not fresh, then this function allocates a new page and assigns its page
    ///   number to `*page`. This function _does not_ do any disk I/O; it does not copy the
    ///   contents of the old page to the new.
    ///
    /// * If `*page` is already fresh, then this function does nothing.
    pub(crate) fn make_page_fresh(&mut self, page: &mut Page) -> Page {
        let p = *page as usize;

        if self.fresh[p] {
            *page
        } else {
            let (new_page, _) = self.alloc_pages(1);
            self.fpm_freed.set(p, true);
            *page = new_page;
            new_page
        }
    }

    /// Ensures that a sequence of pages are "fresh" (can be modified in the uncommitted state).
    ///
    /// This function ensures that each page number in `pages` points to a fresh page. If an
    /// existing page number is not fresh, then this function will allocate a new page and replace
    /// the old page number with the new one.
    ///
    /// This function _does not_ do any disk I/O. It does not copy the contents of old pages to
    /// new pages.
    pub(crate) fn make_pages_fresh(&mut self, pages: &mut [Page]) {
        for p in pages.iter_mut() {
            self.make_page_fresh(p);
        }
    }

    /// Checks that the `fpm` and `fpm_freed` vectors are consistent.
    pub(crate) fn check_vector_consistency(&self) -> anyhow::Result<()> {
        let num_pages = self.num_pages as usize;
        assert_eq!(num_pages, self.fpm.len());
        assert_eq!(num_pages, self.fpm_freed.len());

        for i in 0..num_pages {
            let free = self.fpm[i];
            let freed = self.fpm_freed[i];

            match (free, freed) {
                (true, false) => {} // FREE
                (true, true) => {
                    bail!("Page {i} is in illegal state: marked 'free' and 'freed' (free pending)")
                }
                (false, false) => {} // BUSY
                (false, true) => {}  // FREED
            }
        }

        Ok(())
    }

    /// Merges the "freed" bit map into the "free" bitmap and clears the "freed" bitmap.
    ///
    /// This is part of the commit protocol.
    pub fn merge_freed_into_free(&mut self) {
        let fpm_words: &mut [u32] = self.fpm.as_raw_mut_slice();
        let freed_words: &mut [u32] = self.fpm_freed.as_raw_mut_slice();

        for (free, freed) in fpm_words.iter_mut().zip(freed_words.iter_mut()) {
            *free |= *freed;
            *freed = 0;
        }
    }

    /// Checks invariants that are visible at this scope.
    #[inline(never)]
    pub fn assert_invariants(&self) {
        assert!(self.num_pages > 0);
        assert_eq!(self.num_pages as usize, self.fpm.len());
        assert_eq!(self.num_pages as usize, self.fpm_freed.len());

        // Check that page 0, which stores the MSF File Header, is busy.
        assert!(!self.fpm[0], "Page 0 should always be BUSY");
        assert!(!self.fpm_freed[0], "Page 0 should never be deleted");

        // Check that the pages assigned to the FPM are marked "busy" in all intervals.

        let mut interval: u32 = 0;
        loop {
            let p = (interval << self.page_size.exponent()) as usize;
            let fpm1_index = p + 1;
            let fpm2_index = p + 2;

            if fpm1_index < self.fpm.len() {
                assert!(!self.fpm[fpm1_index], "All FPM pages should be marked BUSY");
                assert!(
                    !self.fpm_freed[fpm1_index],
                    "FPM pages should never be deleted"
                );
            }

            if fpm2_index < self.fpm.len() {
                assert!(!self.fpm[fpm2_index], "All FPM pages should be marked BUSY");
                assert!(
                    !self.fpm_freed[fpm2_index],
                    "FPM pages should never be deleted"
                );
                interval += 1;
            } else {
                break;
            }
        }

        // Check that the free/deleted bit vectors are consistent.
        for page in 0..self.num_pages {
            let is_free = self.fpm[page as usize];
            let is_freed = self.fpm_freed[page as usize];
            assert!(
                !(is_free && is_freed),
                "page {page} is in illegal state (both 'free' and 'freed')"
            );
        }
    }
}

```

`msf/src/read.rs`:

```rs
//! Code for reading data from streams

use sync_file::ReadAt;
use tracing::{trace, trace_span};

use crate::pages::StreamPageMapper;
use crate::{Page, PageSize};

/// This reads data from a stream. It maps byte offsets within a stream to byte offsets within the
/// containing MSF file.
///
/// It will read as much data in a single `read()` call (to the underlying storage) as it can,
/// provided the pages within the stream are contiguous.
///
/// Returns `(bytes_transferred, new_pos)`, where `new_pos` is the position within the stream
/// after the last byte was read. If no bytes were transferred, then this is the same as `pos`.
/// Note that it is possible for `pos` (and thus `new_pos`) to be greater than `stream_size`.
pub(super) fn read_stream_core<F: ReadAt>(
    stream: u32,
    file: &F,
    page_size: PageSize,
    stream_size: u32,
    pages: &[Page],
    stream_pos: u64,
    dst: &mut [u8],
) -> std::io::Result<(usize, u64)> {
    let _span = trace_span!("read_stream_core").entered();

    // Early out for a read at the end. This also handles checking the 64-bit stream position
    // vs 32-bit, so we can safely cast to u32 after this check.
    if stream_pos >= stream_size as u64 {
        return Ok((0, stream_pos));
    }

    let mut stream_pos = stream_pos as u32;

    let original_len = dst.len();
    let mut remaining_dst = dst;

    let mapper = StreamPageMapper::new(pages, page_size, stream_size);

    while !remaining_dst.is_empty() && stream_pos < stream_size {
        let Some((file_offset, transfer_size)) = mapper.map(stream_pos, remaining_dst.len() as u32)
        else {
            break;
        };

        let (dst_this_transfer, dst_next) = remaining_dst.split_at_mut(transfer_size as usize);

        trace!(
            stream,
            stream_pos, transfer_size, file_offset, "reading stream data"
        );

        file.read_exact_at(dst_this_transfer, file_offset)?;

        stream_pos += transfer_size;
        remaining_dst = dst_next;
    }

    let total_bytes_transferred = original_len - remaining_dst.len();
    Ok((total_bytes_transferred, stream_pos as u64))
}

```

`msf/src/stream_reader.rs`:

```rs
use super::*;

/// Allows reading a stream using the [`Read`], [`Seek`], and [`ReadAt`] traits.
pub struct StreamReader<'a, F> {
    /// The stream index. This is used only for diagnostics.
    stream: u32,
    /// Size in bytes of the stream. This value _is never_ equal to [`NIL_STREAM_SIZE`].
    stream_size: u32,
    is_nil: bool,
    /// Page size of the MSF file.
    page_size: PageSize,
    /// Maps page indices within the stream to page indices within the MSF file.
    page_map: &'a [Page],
    /// Provides access to the MSF file contents.
    file: &'a F,
    /// The seek position of the stream reader.
    pos: u64,
}

impl<'a, F: ReadAt> StreamReader<'a, F> {
    pub(crate) fn new(
        pdb: &'a Msf<F>,
        stream: u32,
        stream_size: u32,
        page_map: &'a [Page],
        pos: u64,
    ) -> Self {
        Self {
            stream,
            stream_size: if stream_size == NIL_STREAM_SIZE {
                0
            } else {
                stream_size
            },
            is_nil: stream_size == NIL_STREAM_SIZE,
            page_size: pdb.pages.page_size,
            page_map,
            file: &pdb.file,
            pos,
        }
    }

    /// Size in bytes of the stream.
    ///
    /// This will be zero for nil streams.
    pub fn len(&self) -> u32 {
        self.stream_size
    }

    /// Tests whether this stream is empty (zero-length)
    pub fn is_empty(&self) -> bool {
        self.len() == 0
    }

    /// Returns `true` if this is a nil stream.
    pub fn is_nil(&self) -> bool {
        self.is_nil
    }
}

impl<'a, F: ReadAt> Seek for StreamReader<'a, F> {
    fn seek(&mut self, from: SeekFrom) -> std::io::Result<u64> {
        let new_pos: i64 = match from {
            SeekFrom::Start(offset) => offset as i64,
            SeekFrom::End(signed_offset) => signed_offset + self.stream_size as i64,
            SeekFrom::Current(signed_offset) => self.pos as i64 + signed_offset,
        };

        if new_pos < 0 {
            return Err(std::io::ErrorKind::InvalidInput.into());
        }
        self.pos = new_pos as u64;
        Ok(self.pos)
    }
}

impl<'a, F: ReadAt> Read for StreamReader<'a, F> {
    fn read(&mut self, dst: &mut [u8]) -> std::io::Result<usize> {
        let (n, new_pos) = super::read::read_stream_core(
            self.stream,
            self.file,
            self.page_size,
            self.stream_size,
            self.page_map,
            self.pos,
            dst,
        )?;

        self.pos = new_pos;
        Ok(n)
    }
}

impl<'a, F: ReadAt> ReadAt for StreamReader<'a, F> {
    fn read_exact_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<()> {
        let (n, _new_pos) = super::read::read_stream_core(
            self.stream,
            self.file,
            self.page_size,
            self.stream_size,
            self.page_map,
            offset,
            buf,
        )?;
        if n != buf.len() {
            return Err(std::io::Error::from(std::io::ErrorKind::UnexpectedEof));
        }
        Ok(())
    }

    fn read_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<usize> {
        let (n, _new_pos) = super::read::read_stream_core(
            self.stream,
            self.file,
            self.page_size,
            self.stream_size,
            self.page_map,
            offset,
            buf,
        )?;
        Ok(n)
    }
}

```

`msf/src/stream_writer.rs`:

```rs
use super::*;
use std::cell::RefCell;
use tracing::{trace, trace_span};

/// Provides read/write access for a stream within an MSF (PDB) file.
pub struct StreamWriter<'a, F> {
    /// The stream number. This is used only for diagnostics.
    pub(super) stream: u32,

    pub(super) file: &'a F,

    /// The current byte size of this stream. Points directly into the `stream_sizes` vector.
    ///
    /// This value can be [`NIL_STREAM_SIZE`].
    pub(super) size: &'a mut u32,

    pub(super) page_allocator: &'a mut PageAllocator,

    /// The set of pages owned by this stream.
    pub(super) pages: &'a mut Vec<Page>,

    /// The seek position of this `StreamWriter`.
    ///
    /// `pos` may be greater than `size`. If data is written to a stream when `pos` is greater than
    /// (or equal to) `size`, then the stream is extended to contain the data.
    pub(super) pos: u64,
}

impl<'a, F> StreamWriter<'a, F> {
    /// The length in bytes of this stream. This will return zero for nil streams.
    pub fn len(&self) -> u32 {
        if *self.size == NIL_STREAM_SIZE {
            0
        } else {
            *self.size
        }
    }

    /// Returns `true` if the stream is zero-length or is a nil stream.
    pub fn is_empty(&self) -> bool {
        *self.size == 0 || *self.size == NIL_STREAM_SIZE
    }

    /// Replaces the contents of a stream. The length of the stream is set to the length of `data`.
    pub fn set_contents(&mut self, data: &[u8]) -> std::io::Result<()>
    where
        F: ReadAt + WriteAt,
    {
        let _span = trace_span!("StreamWriter::set_contents").entered();

        if data.len() as u64 >= NIL_STREAM_SIZE as u64 {
            return Err(std::io::ErrorKind::InvalidInput.into());
        }
        let data_len = data.len() as u32;

        // If the existing stream has more data than the caller, then truncate now.  This is a
        // half-hearted attempt to avoid unnecessary read-modify-write cycles.
        if *self.size > data_len {
            self.set_len(data_len)?;
        }

        self.write_core(data, 0)?;
        self.set_len(data.len() as u32)?;
        Ok(())
    }

    /// Writes data to a given position in the file.
    ///
    /// This method exists to work around the limitation of `WriteAt`, which takes `&self`
    /// instead of `&mut self`.
    pub fn write_at_mut(&mut self, buf: &[u8], offset: u64) -> std::io::Result<usize>
    where
        F: ReadAt + WriteAt,
    {
        let _span = trace_span!("StreamWriter::write_at_mut").entered();

        self.write_core(buf, offset)?;
        Ok(buf.len())
    }

    /// Writes data to a given position in the file.
    ///
    /// This method exists to work around the limitation of `WriteAt`, which takes `&self`
    /// instead of `&mut self`.
    pub fn write_all_at_mut(&mut self, buf: &[u8], offset: u64) -> std::io::Result<()>
    where
        F: ReadAt + WriteAt,
    {
        self.write_core(buf, offset)
    }

    /// Sets the length of this stream, in bytes.
    ///
    /// If the stream is a nil stream, this changes it to a non-nil stream, even if `len` is zero.
    ///
    /// If you are writing data to a stream, try to avoid using `set_len` to preallocate storage.
    /// It will allocate pages (yay) but it will waste I/O time by writing zero-fill data to
    /// all of the pages.  There is currently no optimized path for preallocating pages that relies
    /// on the underlying storage provide zero-filling pages.
    ///
    /// If you are replacing the contents of a large stream, just write everything from page zero
    /// and then call `set_len` at the end of the write.
    pub fn set_len(&mut self, mut len: u32) -> std::io::Result<()>
    where
        F: ReadAt + WriteAt,
    {
        use std::cmp::Ordering;

        let _span = trace_span!("StreamWriter::set_len").entered();
        trace!(new_len = len);

        if *self.size == NIL_STREAM_SIZE {
            trace!("stream changes from nil to non-nil");
            *self.size = 0;
        }

        let page_size = self.page_allocator.page_size;

        match Ord::cmp(&len, self.size) {
            Ordering::Equal => {
                trace!(len = self.size, "no change in stream size");
            }

            Ordering::Less => {
                // Truncating the stream. Find the number of pages that need to be freed.
                trace!(old_len = self.size, new_len = len, "reducing stream size");

                let num_pages_old = num_pages_for_stream_size(*self.size, page_size) as usize;
                let num_pages_new = num_pages_for_stream_size(len, page_size) as usize;
                assert!(num_pages_new <= num_pages_old);

                for &page in self.pages[num_pages_new..num_pages_old].iter() {
                    self.page_allocator.fpm_freed.set(page as usize, true);
                }

                self.pages.truncate(num_pages_new);
                *self.size = len;
            }

            Ordering::Greater => {
                // Zero-extend the stream.
                trace!(
                    old_len = self.size,
                    new_len = len,
                    "increasing stream size (zero-filling)"
                );

                let end_phase = offset_within_page(*self.size, page_size);
                if end_phase != 0 {
                    // Total number of bytes we need to fill
                    let total_zx_bytes = len - *self.size;

                    // zero-extend partial page
                    let end_spage = *self.size / page_size;
                    let num_zx_bytes = (u32::from(page_size) - end_phase).min(total_zx_bytes);

                    let mut page_buffer = self.page_allocator.alloc_page_buffer();
                    self.read_page(end_spage, &mut page_buffer)?;
                    page_buffer[end_phase as usize..].fill(0);
                    self.cow_page_and_write(end_spage, &page_buffer)?;

                    *self.size += num_zx_bytes;

                    len -= num_zx_bytes;
                    if len == 0 {
                        // We may have finished without reaching the end of this page.
                        return Ok(());
                    }
                }

                // The code above should have handled aligning the current size of the stream
                // (or returning if we are done).
                assert!(page_size.is_aligned(*self.size));

                let mut page_buffer = self.page_allocator.alloc_page_buffer();
                page_buffer.fill(0);

                assert!(page_size.is_aligned(*self.size));

                // num_zx_pages includes any partial page at the end.
                let num_zx_pages_wanted = (len - *self.size).div_round_up(page_size);

                let (first_page, run_len) = self.page_allocator.alloc_pages(num_zx_pages_wanted);
                assert!(run_len > 0);

                let old_num_pages = self.pages.len() as u32;

                for i in 0..run_len {
                    self.pages.push(first_page + i);
                }

                // This size increase may cover a partial page.
                *self.size += len;

                // TODO: If the app calls set_len() with a large size, this will be inefficient,
                // since we issue one write per page.  We could avoid that in the case where we
                // are extending the MSF file with fresh pages, at the end, and rely on a single
                // "set length" call to the underlying file.
                for i in 0..run_len {
                    self.write_page(old_num_pages + i, &page_buffer)?;
                }
            }
        }

        Ok(())
    }

    /// Converts this `StreamWriter` into a `RandomStreamWriter`.
    pub fn into_random(self) -> RandomStreamWriter<'a, F> {
        RandomStreamWriter {
            cell: RefCell::new(self),
        }
    }
}

impl<'a, F: ReadAt> std::io::Seek for StreamWriter<'a, F> {
    fn seek(&mut self, from: SeekFrom) -> std::io::Result<u64> {
        let new_pos: i64 = match from {
            SeekFrom::Start(offset) => offset as i64,
            SeekFrom::End(signed_offset) => signed_offset + *self.size as i64,
            SeekFrom::Current(signed_offset) => self.pos as i64 + signed_offset,
        };

        if new_pos < 0 {
            return Err(std::io::ErrorKind::InvalidInput.into());
        }

        self.pos = new_pos as u64;
        Ok(self.pos)
    }
}

impl<'a, F: ReadAt> std::io::Read for StreamWriter<'a, F> {
    fn read(&mut self, dst: &mut [u8]) -> std::io::Result<usize> {
        let (n, new_pos) = super::read::read_stream_core(
            self.stream,
            self.file,
            self.page_allocator.page_size,
            *self.size,
            self.pages,
            self.pos,
            dst,
        )?;
        self.pos = new_pos;
        Ok(n)
    }
}

impl<'a, F: ReadAt + WriteAt> std::io::Write for StreamWriter<'a, F> {
    fn write(&mut self, buf: &[u8]) -> std::io::Result<usize> {
        self.write_core(buf, self.pos)?;
        self.pos += buf.len() as u64;
        Ok(buf.len())
    }

    fn flush(&mut self) -> std::io::Result<()> {
        Ok(())
    }
}

pub struct RandomStreamWriter<'a, F> {
    cell: RefCell<StreamWriter<'a, F>>,
}

impl<'a, F: ReadAt> ReadAt for RandomStreamWriter<'a, F> {
    fn read_exact_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<()> {
        let sw = self.cell.borrow();
        let (n, _new_pos) = super::read::read_stream_core(
            sw.stream,
            &sw.file,
            sw.page_allocator.page_size,
            *sw.size,
            sw.pages,
            offset,
            buf,
        )?;
        if n != buf.len() {
            return Err(std::io::Error::from(std::io::ErrorKind::UnexpectedEof));
        }
        Ok(())
    }

    fn read_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<usize> {
        let sw = self.cell.borrow();
        let (n, _new_pos) = super::read::read_stream_core(
            sw.stream,
            &sw.file,
            sw.page_allocator.page_size,
            *sw.size,
            sw.pages,
            offset,
            buf,
        )?;
        Ok(n)
    }
}

impl<'a, F: ReadAt + WriteAt> WriteAt for RandomStreamWriter<'a, F> {
    fn write_at(&self, buf: &[u8], offset: u64) -> std::io::Result<usize> {
        let mut sw = self.cell.borrow_mut();
        sw.write_core(buf, offset)?;
        Ok(buf.len())
    }

    fn write_all_at(&self, buf: &[u8], offset: u64) -> std::io::Result<()> {
        let mut sw = self.cell.borrow_mut();
        sw.write_core(buf, offset)
    }
}

```

`msf/src/tests.rs`:

```rs
#![allow(clippy::useless_vec)]

use super::*;
use anyhow::Result;
use pretty_hex::PrettyHex;
use std::io::{Cursor, Seek, SeekFrom, Write};
use std::sync::Mutex;
use sync_file::{ReadAt, WriteAt};
use tracing::{debug, debug_span, trace, trace_span};

#[static_init::dynamic]
static INIT_LOGGER: () = {
    use tracing_subscriber::fmt::format::FmtSpan;

    tracing_subscriber::fmt::fmt()
        .compact()
        .with_max_level(tracing_subscriber::filter::LevelFilter::TRACE)
        .with_level(false)
        .with_file(true)
        .with_line_number(true)
        .with_span_events(FmtSpan::ENTER | FmtSpan::EXIT)
        .with_test_writer()
        .without_time()
        .with_ansi(false)
        .init();
};

macro_rules! assert_bytes_eq {
    ($a:expr, $b:expr) => {
        match (&($a), &($b)) {
            (a, b) => {
                let a_bytes: &[u8] = a.as_ref();
                let b_bytes: &[u8] = b.as_ref();

                if a_bytes != b_bytes {
                    panic!(
                        "Bytes do not match:\n{}\n{}",
                        a_bytes.hex_dump(),
                        b_bytes.hex_dump()
                    );
                }
            }
        }
    };

    ($a:expr, $b:expr, $($msg:tt)*) => {
        match (&($a), &($b)) {
            (a, b) => {
                let a_bytes: &[u8] = a.as_ref();
                let b_bytes: &[u8] = b.as_ref();

                if a_bytes != b_bytes {
                    let msg = format!($($msg)*);
                    panic!(
                        "Bytes do not match: {msg}\n{:?}\n{:?}",
                        a_bytes.hex_dump(),
                        b_bytes.hex_dump()
                    );
                }
            }
        }
    };
}

struct WritePair<Test, Good> {
    test: Test,
    good: Good,
}

impl<Test: Write, Good: Write> std::io::Write for WritePair<Test, Good> {
    fn flush(&mut self) -> std::io::Result<()> {
        self.test.flush()?;
        self.good.flush()?;
        Ok(())
    }

    fn write(&mut self, buf: &[u8]) -> std::io::Result<usize> {
        let len_test = self.test.write(buf)?;
        let len_good = self.good.write(buf)?;
        assert_eq!(len_test, len_good);
        Ok(len_test)
    }

    fn write_all(&mut self, buf: &[u8]) -> std::io::Result<()> {
        self.test.write_all(buf)?;
        self.good.write_all(buf)?;
        Ok(())
    }
}

impl<A: Seek, B: Seek> std::io::Seek for WritePair<A, B> {
    fn seek(&mut self, pos: SeekFrom) -> std::io::Result<u64> {
        let pos_test = self.test.seek(pos)?;
        let pos_good = self.good.seek(pos)?;
        assert_eq!(pos_test, pos_good);
        Ok(pos_test)
    }
}

#[derive(Default)]
struct TestFile {
    data: Mutex<Vec<u8>>,
}

impl ReadAt for TestFile {
    fn read_exact_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<()> {
        let _span = trace_span!("TestFile::read_exact_at").entered();
        debug!(offset, buf_len = buf.len(), "TestFile::read_exact_at");
        let lock = self.data.lock().unwrap();
        lock.read_exact_at(buf, offset)?;
        debug!(data = buf, "read received");
        Ok(())
    }

    fn read_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<usize> {
        let lock = self.data.lock().unwrap();
        let n = lock.read_at(buf, offset)?;
        debug!(data = &buf[..n], "TestFile::read_at: read received");
        Ok(n)
    }
}

impl WriteAt for TestFile {
    fn write_at(&self, buf: &[u8], offset: u64) -> std::io::Result<usize> {
        self.write_all_at(buf, offset)?;
        Ok(buf.len())
    }

    fn write_all_at(&self, buf: &[u8], offset: u64) -> std::io::Result<()> {
        debug!(
            offset = offset,
            len = buf.len(),
            data = buf,
            "TestFile: write_all_at"
        );

        let mut lock = self.data.lock().unwrap();
        let vec: &mut Vec<u8> = &mut lock;

        let offset = offset as usize;

        if offset == vec.len() {
            vec.extend_from_slice(buf);
        } else {
            let new_len = buf.len() + offset;
            if new_len > vec.len() {
                vec.resize(new_len, 0);
            }
            vec[offset..offset + buf.len()].copy_from_slice(buf);
        }
        Ok(())
    }
}

struct Tester {
    msf: Msf<TestFile>,
}

fn tester() -> Tester {
    println!();

    let f = TestFile::default();
    let msf = Msf::create_for(f, CreateOptions::default()).unwrap();
    Tester { msf }
}

/// Contains enough state from an MSF file that we can fake up a StreamWriter for testing.
struct StreamTester {
    file: TestFile,
    stream_size: u32,
    page_allocator: PageAllocator,
    pages: Vec<Page>,
    expected_stream_data: Vec<u8>,
}

impl StreamTester {
    fn new() -> Self {
        Self {
            file: Default::default(),
            stream_size: 0,
            page_allocator: PageAllocator::new(0x100, DEFAULT_PAGE_SIZE),
            pages: Vec::new(),
            expected_stream_data: Vec::new(),
        }
    }

    fn writer(&mut self) -> WritePair<StreamWriter<'_, TestFile>, Cursor<&mut Vec<u8>>> {
        let good = Cursor::new(&mut self.expected_stream_data);

        let test = StreamWriter {
            stream: 100, // fake stream number
            file: &mut self.file,
            size: &mut self.stream_size,
            page_allocator: &mut self.page_allocator,
            pages: &mut self.pages,
            pos: 0,
        };

        WritePair { good, test }
    }

    #[inline(never)]
    #[track_caller]
    fn write_at(&mut self, pos: u64, data: &[u8]) {
        let _span = debug_span!("StreamTester::write_at");
        debug!(
            pos,
            current_stream_size = self.stream_size,
            piece_contents = data,
            "write_at"
        );

        let mut w = self.writer();
        w.seek(SeekFrom::Start(pos)).unwrap();
        w.write_all(data).unwrap();
        self.check_data();
    }

    // Verifies that the data stored in the stream is consistent with the data that we also wrote
    // into expected_stream_data.
    #[track_caller]
    fn check_data(&self) {
        assert_eq!(
            self.stream_size as usize,
            self.expected_stream_data.len(),
            "stream sizes should be same"
        );

        let page_size = self.page_allocator.page_size;

        assert_eq!(
            num_pages_for_stream_size(self.stream_size, page_size) as usize,
            self.pages.len(),
            "number of pages should be consistent with stream size"
        );

        let file = self.file.data.lock().unwrap();

        for (spage, &page) in self.pages.iter().enumerate() {
            let whole_page_data =
                &file[page_to_offset(page, page_size) as usize..][..usize::from(page_size)];
            let page_start = (spage as u32) << page_size.exponent();
            let len_within_page = (self.stream_size - page_start).min(u32::from(page_size));

            // Page data from the MSF "file"
            let page_data = &whole_page_data[..len_within_page as usize];

            // Page data from our parallel file contents
            let expected_page_data = &self.expected_stream_data[(spage << page_size.exponent())..]
                [..len_within_page as usize];

            assert_bytes_eq!(expected_page_data, page_data, "Stream page {page}");
        }
    }
}

/// Create a stream but don't write anything to it.
#[test]
fn test_write_empty_stream() {
    let mut t = tester();

    let (si, _s) = t.msf.new_stream().unwrap();
    assert_eq!(si, 5);
    assert_eq!(t.msf.num_streams(), 6);

    t.msf.commit().unwrap();
}

/// Create a stream, do a single zero-length write to it.
#[test]
fn test_write_empty_buffer() {
    let mut t = tester();

    let (si, mut s) = t.msf.new_stream().unwrap();
    assert_eq!(si, 5);
    s.write_all(&[]).unwrap();

    assert_eq!(t.msf.num_streams(), 6);

    t.msf.commit().unwrap();
}

/// Create a stream, write a small amount of data into it
#[test]
fn test_write_hello_world() {
    let mut st = StreamTester::new();
    st.write_at(0, b"Hello, world!");
}

#[test]
fn test_write_simple() {
    let mut st = StreamTester::new();
    st.write_at(0, b"Alpha_");
    st.write_at(6, b"Bravo_");
    st.write_at(12, b"Charlie_");
    st.write_at(6, b"Delta_");
}

// Zero-extend with a small amount of data that does not cross the page boundary where zero-extend starts.
#[test]
fn test_zero_extend_unaligned_start_1() {
    let mut st = StreamTester::new();
    st.write_at(10, b"Hello!");
}

// Zero-extend with a small amount of data that DOES cross the page boundary where zero-extend starts.
// This also zero-extends several complete pages.
#[test]
fn test_zero_extend_unaligned_start_cross_page_many() {
    let mut st = StreamTester::new();
    st.write_at(0, b"Hello");
    st.write_at(0x2ffe, b"World!");
}

// unaligned start, finishes within a single page
#[test]
fn test_zero_extend_unaligned_start_single_page() {
    let mut st = StreamTester::new();
    st.write_at(0, b"old");
    // <-- zero extend 7 bytes
    st.write_at(10, b"new");
}

#[test]
fn test_zero_extend_unaligned_start_cross_pages_aligned_end() {
    let mut st = StreamTester::new();
    st.write_at(0, b"old");
    st.write_at(10, &vec![0xaa; 0x1ff6]); // ends at page-aligned boundary
    assert_eq!(st.stream_size, 0x2000);
}

#[test]
fn test_zero_extend_unaligned_start_cross_pages_unaligned_end() {
    let mut st = StreamTester::new();
    st.write_at(0, b"old");
    st.write_at(10, &vec![0xaa; 0x2000]);
    assert_eq!(st.stream_size, 0x200a);
}

#[test]
fn test_zero_extend_aligned_start_unaligned_end() {
    let mut st = StreamTester::new();
    st.write_at(0x2000, b"alpha");
}

#[test]
fn test_zero_extend_aligned_start_pages_unaligned_end() {
    let mut st = StreamTester::new();
    st.write_at(0x0000, &vec![0xaa; 0x1000]);
    st.write_at(0x2010, b"alpha");
}

// aligned start, does not extend stream, existing stream page is unaligned
#[test]
fn test_overwrite_aligned_start_single_page() {
    let mut st = StreamTester::new();
    st.write_at(0, b"alpha bravo charlie delta");
    st.write_at(0, b"TANGO");
}

// unaligned start, does not extend stream, existing stream page is unaligned
#[test]
fn test_overwrite_unaligned_start_single_page() {
    let mut st = StreamTester::new();
    st.write_at(0, b"alpha bravo charlie delta");
    st.write_at(6, b"TANGO");
}

// unaligned start, extends stream, existing stream page is unaligned
#[test]
fn test_overwrite_case_unaligned_extend_within_page() {
    let mut st = StreamTester::new();
    st.write_at(0, b"alpha bravo");
    st.write_at(12, b"TANGO");
}

// unaligned start, does not extend stream, existing stream page is unaligned
#[test]
fn test_overwrite_case_unaligned_extend_across_pages() {
    let mut st = StreamTester::new();
    st.write_at(0, b"alpha bravo");
    let big = FRIENDS_ROMANS.repeat(10);
    println!("big length = 0x{:x}", big.len());
    st.write_at(12, big.as_bytes());
}

#[test]
fn test_overwrite_case_many_pages() {
    let mut st = StreamTester::new();
    st.write_at(0, &vec![0xcc; 0x10_000]); // write lots of data
    st.write_at(0x0_f00, FRIENDS_ROMANS.as_bytes()); // get some shakespeare
    st.write_at(0x1_f00, FRIENDS_ROMANS.repeat(10).as_bytes());
}

// This tests the case in write_overwrite_aligned_pages() where we overwrite an unaligned portion
// of a page. buf.len() is too small to cover the page, but the existing stream does have enough
// pages assigned to it to cover it.
#[test]
fn test_overwrite_case_unaligned_end() {
    let mut st = StreamTester::new();
    st.write_at(0, &vec![0xcc; 0x2_000]);
    st.write_at(0xffe, b"abcd");
}

#[test]
fn test_overwrite_case_zzz_1() {
    let mut st = StreamTester::new();
    st.write_at(0, &vec![0xcc; 0x1_005]);
    st.write_at(0xffe, b"__abc");
}

#[test]
fn test_overwrite_case_zzz_2() {
    let mut st = StreamTester::new();
    st.write_at(0, &vec![0xcc; 0x1_005]);
    st.write_at(0xffe, b"__abcde");
}

#[test]
fn test_overwrite_case_zzz_3() {
    let mut st = StreamTester::new();
    st.write_at(0, &vec![0xcc; 0x1_005]);
    st.write_at(0xffe, b"__abcdefgh");
}

#[test]
fn test_overwrite_case_c() {
    let mut st = StreamTester::new();
    st.write_at(0, &vec![0xcc; 0xc]);
    st.write_at(0, &vec![0xaa; 0xaaaa]);
}

const FRIENDS_ROMANS: &str = r#"
Friends, Romans, countrymen, lend me your ears;
I come to bury Caesar, not to praise him.
The evil that men do lives after them;
The good is oft interred with their bones;
So let it be with Caesar. The noble Brutus
Hath told you Caesar was ambitious:
If it were so, it was a grievous fault,
And grievously hath Caesar answer'd it.
Here, under leave of Brutus and the rest-
For Brutus is an honourable man;
So are they all, all honourable men-
Come I to speak in Caesar's funeral.
He was my friend, faithful and just to me:
But Brutus says he was ambitious;
And Brutus is an honourable man.
He hath brought many captives home to Rome
Whose ransoms did the general coffers fill:
Did this in Caesar seem ambitious?
When that the poor have cried, Caesar hath wept:
Ambition should be made of sterner stuff:
Yet Brutus says he was ambitious;
And Brutus is an honourable man.
You all did see that on the Lupercal
I thrice presented him a kingly crown,
Which he did thrice refuse: was this ambition?
Yet Brutus says he was ambitious;
And, sure, he is an honourable man.
I speak not to disprove what Brutus spoke,
But here I am to speak what I do know.
You all did love him once, not without cause:
What cause withholds you then, to mourn for him?
O judgment! thou art fled to brutish beasts,
And men have lost their reason. Bear with me;
My heart is in the coffin there with Caesar,
And I must pause till it come back to me.
"#;

#[test]
fn test_write_many_pieces() {
    let mut st = StreamTester::new();
    st.write_at(0, b"Alpha_");
    st.write_at(6, b"Bravo_");
    st.write_at(12, b"Charlie_");
    st.write_at(6, b"Delta_");
    st.write_at(50, b"Zulu");
    st.write_at(0, b"__Wiffleball__");
    st.write_at(5, b"__Garrus__");
}

#[test]
fn test_write_x() {
    let mut st = StreamTester::new();
    st.write_at(0x35, b"!");
    st.write_at(0, b"zzz");
}

#[test]
fn msf_write_multi_streams() {
    let mut t = tester();

    {
        let (_si1, mut sw1) = t.msf.new_stream().unwrap();
        sw1.write_all(b"Hello, world!").unwrap();
    }

    {
        let (_si2, mut sw2) = t.msf.new_stream().unwrap();
        sw2.write_all(b"Hallo Welt!").unwrap();
    }

    {
        let (_si2, mut sw2) = t.msf.new_stream().unwrap();
        sw2.write_all(b"Salut tout le monde!").unwrap();
    }
}

fn writer() -> Msf<TestFile> {
    let f = TestFile::default();
    Msf::create_for(f, Default::default()).unwrap()
}

fn finish_and_dump(mut w: Msf<TestFile>) {
    match w.commit() {
        Err(e) => {
            panic!("PdbWriter::commit failed: {e}");
        }
        Ok(_wrote_any) => {
            let data_guard = w.file.data.lock().unwrap();
            let data: &[u8] = &data_guard;

            println!(
                "Finished PDB.  Size = 0x{:x} {}:\n{:#?}",
                data.len(),
                data.len(),
                data.hex_dump()
            );
        }
    }
}

/// Commits changes in a writer, then closes it and re-opens it as a new `Msf`.
#[track_caller]
fn finish_and_read(mut w: Msf<TestFile>) -> Msf<TestFile> {
    w.commit().unwrap();
    let file = w.into_file();

    // Now re-open the file.
    Msf::open_with_file(file).unwrap()
}

#[track_caller]
fn commit_and_read(w: &mut Msf<TestFile>) -> Msf<TestFile> {
    let _span = trace_span!("commit_and_read").entered();
    w.commit().unwrap();

    let cloned_file_data = w.file_mut().data.get_mut().unwrap().clone();
    Msf::open_with_file(TestFile {
        data: Mutex::new(cloned_file_data),
    })
    .unwrap()
}

#[test]
fn page_size() {
    let w = writer();
    assert_eq!(usize::from(w.page_size()), 4096);
}

#[test]
fn empty_pdb() {
    let w = writer();
    finish_and_dump(w);
}

#[test]
fn read_stream_out_of_range() {
    let w = writer();
    let r = finish_and_read(w);
    let s = r.get_stream_reader(100);
    assert!(s.is_err());
}

#[test]
fn read_stream_dir_stream() {
    let w = writer();
    let r = finish_and_read(w);
    let s = r.get_stream_reader(STREAM_DIR_STREAM).unwrap();
    assert!(!s.is_nil());
}

#[test]
fn read_nil_stream() {
    let mut w = writer();
    let si = w.nil_stream().unwrap();
    debug!(nil_stream_index = si);
    let r = finish_and_read(w);
    let s = r.get_stream_reader(si).unwrap();
    assert!(s.is_nil());
    assert_eq!(s.len(), 0);
}

#[test]
fn one_stream_hello_world() -> anyhow::Result<()> {
    let mut w = writer();

    let (_, mut s) = w.new_stream()?;
    s.write_all("Hello, world!".as_bytes())?;

    finish_and_dump(w);
    Ok(())
}

#[test]
fn simple_multiple_streams() -> anyhow::Result<()> {
    let mut w = writer();

    let (si, mut s) = w.new_stream()?;
    assert_eq!(si, 5);
    s.write_all("Friends, Romans, countrymen, lend me your ears.".as_bytes())?;

    let (si, mut s) = w.new_stream()?;
    assert_eq!(si, 6);
    s.write_all("I come to bury Caesar, not to praise him.".as_bytes())?;

    let (si, mut s) = w.new_stream()?;
    assert_eq!(si, 7);
    s.write_all("The evil that men do lives after them.".as_bytes())?;

    let (si, mut s) = w.new_stream()?;
    assert_eq!(si, 8);
    s.write_all("I come to bury Caesar, not to praise him.".as_bytes())?;

    let (si, mut s) = w.new_stream()?;
    assert_eq!(si, 9);
    s.write_all("So let it be with Caesar.".as_bytes())?;

    finish_and_dump(w);
    Ok(())
}

#[test]
fn mix_and_match() -> Result<()> {
    let mut w = writer();

    let (si0, _s0) = w.new_stream()?;
    let (si1, _s1) = w.new_stream()?;
    let (si2, _s2) = w.new_stream()?;

    w.write_stream(si0)?.write_all("Sponge Bob!".as_bytes())?;
    w.write_stream(si1)?.write_all("Squidward!".as_bytes())?;
    w.write_stream(si2)?.write_all("Mr Crabs!".as_bytes())?;
    w.write_stream(si0)?.write_all("Square Pants!".as_bytes())?; // should land on same page

    let mut w1 = w.write_stream(si1)?;
    w1.seek(SeekFrom::Start(0x2000))?;
    w1.write_all("Peace and Quiet...".as_bytes())?; // new page

    assert_eq!(w.write_stream(si0)?.pages, &[3]);
    assert_eq!(w.write_stream(si1)?.pages, &[4, 6, 7]);
    assert_eq!(w.write_stream(si2)?.pages, &[5]);

    finish_and_dump(w);

    Ok(())
}

#[test]
fn commit_on_read_only() -> Result<()> {
    let mut w = writer();

    let (_si, mut sw) = w.new_stream().unwrap();
    sw.write_all(b"Hello!").unwrap();

    let mut r = finish_and_read(w);

    // This commit() call should do nothing (but should succeed).
    assert!(!r.commit().unwrap());
    Ok(())
}

#[test]
fn commit_no_writes() {
    let mut w = writer();
    // First call should write the initial MSF file.
    assert!(w.commit().unwrap());
    // Second call should have no writes at all, though.
    assert!(!w.commit().unwrap());
}

#[test]
fn single_commit() {
    let mut w = writer();
    let (si1, mut sw1) = w.new_stream().unwrap();
    sw1.write_all(b"Alpha").unwrap();

    {
        let r = commit_and_read(&mut w);
        let contents1 = r.read_stream_to_vec(si1).unwrap();
        assert_eq!(contents1, b"Alpha");
    }
}

#[test]
fn multiple_commit() {
    let mut w = writer();

    trace!("multi_commit: writing first stream");
    let (si1, mut sw1) = w.new_stream().unwrap();
    sw1.write_all(b"Alpha").unwrap();

    {
        trace!("multi_commit: first commit");
        let r = commit_and_read(&mut w);
        let contents1 = r.read_stream_to_vec(si1).unwrap();
        assert_eq!(contents1, b"Alpha");
    }

    trace!("multi_commit: writing second stream");
    let (si2, mut sw2) = w.new_stream().unwrap();
    sw2.write_all(b"Bravo").unwrap();

    {
        trace!("multi_commit: second commit");
        let r = commit_and_read(&mut w);

        let contents1 = r.read_stream_to_vec(si1).unwrap();
        assert_bytes_eq!(contents1, b"Alpha");

        let contents2 = r.read_stream_to_vec(si2).unwrap();
        assert_bytes_eq!(contents2, b"Bravo");
    }
}

#[test]
fn many_commits() {
    let num_commits: usize = 37;

    let mut w = writer();

    let (si1, _sw1) = w.new_stream().unwrap();

    let mut expected_stream_contents: Vec<u8> = Vec::new();

    for i in 0..num_commits {
        let sw = w.write_stream(si1).unwrap();
        let pos = i * 2039; // 2039 is next-lower prime under 2048

        let text = format!("i{i} pos{pos};");
        let buf = text.as_bytes();

        // Write the buffer to expected_stream_contents.
        let text_end = pos + text.len();
        if expected_stream_contents.len() < text_end {
            // Extend, if necessary.
            expected_stream_contents.resize(text_end, 0);
        }
        expected_stream_contents[pos..][..buf.len()].copy_from_slice(buf);

        sw.into_random().write_at(buf, pos as u64).unwrap();

        {
            let r = commit_and_read(&mut w);
            let read_buf = r.read_stream_to_vec(si1).unwrap();
            assert_bytes_eq!(expected_stream_contents, read_buf);
        }
    }
}

/// Test the WriteAt impl for StreamReader
#[test]
fn stream_writer_random_write_at() {
    let mut w = writer();

    let (si, sw) = w.new_stream().unwrap();
    let sw = sw.into_random();
    assert_eq!(sw.write_at(b"Hello", 0).unwrap(), 5);
    sw.write_all_at(b"World", 5).unwrap();

    let r = commit_and_read(&mut w);
    let data = r.read_stream_to_vec(si).unwrap();
    assert_eq!(data, b"HelloWorld");
}

/// Test the WriteAt and ReadAt impl for RandomStreamWriter
#[test]
fn stream_writer_random_read_at() {
    let mut w = writer();

    let (_si, sw) = w.new_stream().unwrap();
    let sw = sw.into_random();

    sw.write_at(b"012345", 5).unwrap();
    sw.write_all_at(b"AlphaBravo", 5).unwrap();

    {
        let mut buf: [u8; 5] = [0; 5];
        assert_eq!(sw.read_at(&mut buf, 12).unwrap(), 3);
        assert_eq!(&buf, b"avo\0\0");
    }

    {
        let mut buf: [u8; 5] = [0; 5];
        sw.read_exact_at(&mut buf, 7).unwrap();
        assert_eq!(&buf, b"phaBr");
    }

    {
        // attempting to read beyond the end of the buffer
        let mut buf: [u8; 5] = [0; 5];
        assert!(sw.read_exact_at(&mut buf, 12).is_err());
    }
}

#[test]
fn stream_writer_flush() {
    let mut w = writer();
    let (_si, mut sw) = w.new_stream().unwrap();
    sw.flush().unwrap();
}

#[test]
fn stream_writer_write_at() {
    let mut w = writer();
    let (si, mut sw) = w.new_stream().unwrap();
    assert_eq!(sw.write_at_mut(b"Alpha", 0).unwrap(), 5);
    sw.write_all_at_mut(b"Bravo", 5).unwrap();

    let r = commit_and_read(&mut w);
    let data = r.read_stream_to_vec(si).unwrap();
    assert_bytes_eq!(data, b"AlphaBravo");
}

/// Test set_contents() on a newly-created stream.
#[test]
fn stream_writer_set_contents_new() {
    let mut w = writer();
    let (si, mut sw) = w.new_stream().unwrap();
    sw.set_contents(FRIENDS_ROMANS.as_bytes()).unwrap();

    let r = commit_and_read(&mut w);
    let data = r.read_stream_to_vec(si).unwrap();
    assert_bytes_eq!(data, FRIENDS_ROMANS);
}

/// Test set_contents() on a stream that has not been modified yet.
/// Extend the buffer.
#[test]
fn stream_writer_set_contents_modifying_extending() {
    let mut w = writer();
    let (si, mut sw) = w.new_stream().unwrap();
    sw.set_contents(b"this will get overwritten").unwrap();

    let _r = commit_and_read(&mut w);

    let mut sw = w.write_stream(si).unwrap();
    sw.set_contents(b"this string is a lot longer than the first string so it extends the buffer")
        .unwrap();

    let r2 = commit_and_read(&mut w);
    let data = r2.read_stream_to_vec(si).unwrap();
    assert_bytes_eq!(
        data,
        b"this string is a lot longer than the first string so it extends the buffer"
    );
}

/// Test set_contents() on a stream that has not been modified yet.
/// Shrink the buffer.
#[test]
fn stream_writer_set_contents_modifying_shrinking() {
    let mut w = writer();
    let (si, mut sw) = w.new_stream().unwrap();
    sw.set_contents(b"this is a moderately long string which will get overwritten")
        .unwrap();

    let _r = commit_and_read(&mut w);

    let mut sw = w.write_stream(si).unwrap();
    sw.set_contents(b"goodbye!").unwrap();

    let r2 = commit_and_read(&mut w);
    let data = r2.read_stream_to_vec(si).unwrap();
    assert_bytes_eq!(data, b"goodbye!");
}

#[test]
fn stream_writer_write_at_empty() {
    let mut w = writer();
    let (si, mut sw) = w.new_stream().unwrap();

    // This should succeed, but...
    sw.write_all_at_mut(&[], 100).unwrap();

    // ...it shouldn't extend the stream size.
    assert_eq!(sw.len(), 0);

    let r = commit_and_read(&mut w);
    let data = r.read_stream_to_vec(si).unwrap();
    assert!(data.is_empty());
}

/// Test extending a stream with a large number of pages, which should
/// test the path in `StreamWriter::write_page`.
#[test]
fn stream_writer_extend_large() {
    let mut w = writer();
    let (si, mut sw) = w.new_stream().unwrap();

    let mut large_data = vec![0; 0x10000];
    large_data[0xffff] = 0xff;

    sw.set_contents(&large_data).unwrap();

    let r = commit_and_read(&mut w);
    let data = r.read_stream_to_vec(si).unwrap();

    assert_bytes_eq!(large_data, data);
}

#[test]
fn read_stream_to_vec_mut_modified() {
    let mut w = writer();
    let si = w.new_stream_data(b"Hello!").unwrap();
    let mut read_back = Vec::new();
    w.read_stream_to_vec_mut(si, &mut read_back).unwrap();
    assert_bytes_eq!(read_back, b"Hello!");
}

#[test]
fn read_stream_to_vec_mut_committed() {
    let mut w = writer();
    let si = w.new_stream_data(b"Hello!").unwrap();
    let r = commit_and_read(&mut w);
    let mut read_back = Vec::new();
    r.read_stream_to_vec_mut(si, &mut read_back).unwrap();
    assert_bytes_eq!(read_back, b"Hello!");
}

#[test]
fn read_stream_to_box() {
    let mut w = writer();
    let si = w.new_stream_data(b"Hello!").unwrap();
    let r = commit_and_read(&mut w);
    let read_back = r.read_stream_to_box(si).unwrap();
    assert_bytes_eq!(read_back, b"Hello!");
}

```

`msf/src/write.rs`:

```rs
use super::*;
use std::collections::hash_map::Entry;
use std::io::Write;
use tracing::{trace, trace_span};

impl<'a, F: ReadAt + WriteAt> StreamWriter<'a, F> {
    /// Writes data to a stream at a given offset. This is the main driver for all `write()` calls
    /// and their variants.
    ///
    /// This function has to handle a lot of complexity:
    /// * alignment of the starting position to a page boundary
    /// * alignment of the ending position to a page boundary
    /// * allocating pages for new pages
    /// * allocating pages for copy-on-write
    /// * updating the size of a stream
    /// * writing zeroes into regions that were implicitly created
    ///
    /// Returns the new write position, which is immediately after the buffer that was provided.
    ///
    /// This implementation is input-dependent. That is, we will drive all of our state transitions
    /// by "walking" through the offsets in the stream, from 0 to the end of the stream or the end
    /// of the transfer, whichever is greater.
    ///
    /// For some operations (read-modify-write cycles), we use a temporary page buffer.
    ///
    /// This function always writes all of the data in `buf`. If it cannot, it returns `Err`.
    #[inline(never)]
    pub(super) fn write_core(&mut self, mut buf: &[u8], offset: u64) -> std::io::Result<()> {
        let _span = trace_span!("StreamWriter::write_core").entered();

        if buf.is_empty() {
            return Ok(());
        }

        if *self.size == NIL_STREAM_SIZE {
            *self.size = 0;
        }

        let page_size = self.page_allocator.page_size;

        // Validate the ranges of our inputs. We validate these now so that we can compute values
        // that depend on them without worrying about overflow.
        let Ok(buf_len) = u32::try_from(buf.len()) else {
            return Err(std::io::ErrorKind::InvalidInput.into());
        };
        let Ok(mut pos) = u32::try_from(offset) else {
            return Err(std::io::ErrorKind::InvalidInput.into());
        };
        let Some(_buf_end) = pos.checked_add(buf_len) else {
            return Err(std::io::ErrorKind::InvalidInput.into());
        };

        // Is there any implicit zero extension happening? If so, handle the zero extension now.
        // Note that this may transfer a small prefix of buf, if the end of the zero-extension
        // region is unaligned (i.e. pos is unaligned). If that consumes all of the data in buf,
        // then we finish early.
        if *self.size < pos {
            self.write_zero_extend(&mut buf, &mut pos)?;
            if buf.is_empty() {
                return Ok(());
            }
            assert_eq!(pos, *self.size);
        }

        assert!(!buf.is_empty());
        assert!(pos <= *self.size);

        // Are we doing any overwrite?
        if pos < *self.size {
            self.write_overwrite(&mut buf, &mut pos)?;
            if buf.is_empty() {
                return Ok(());
            }
            assert_eq!(pos, *self.size);
        }

        assert!(!buf.is_empty());
        assert_eq!(pos, *self.size);

        // Does the write position start at an unaligned page boundary?
        if !page_size.is_aligned(pos) {
            self.write_unaligned_start_page(&mut buf, &mut pos)?;
            if buf.is_empty() {
                return Ok(());
            }
        }

        assert!(!buf.is_empty());
        assert_eq!(pos, *self.size);
        assert!(page_size.is_aligned(pos));

        // From this point on, we no longer need to cow pages.
        // All pages that we write will be newly-allocated pages.

        self.write_append_complete_pages(&mut buf, &mut pos)?;
        if buf.is_empty() {
            return Ok(());
        }

        self.write_append_final_unaligned_page(&mut buf, &mut pos)?;
        assert!(buf.is_empty());

        Ok(())
    }

    /// Append complete pages to the file.
    fn write_append_complete_pages(
        &mut self,
        buf: &mut &[u8],
        pos: &mut u32,
    ) -> std::io::Result<()> {
        let page_size = self.page_allocator.page_size;

        assert_eq!(*self.size, *pos);
        assert!(page_size.is_aligned(*pos));

        // Append complete pages. We do this iteratively so that we can minimize our calls to
        // alloc_pages(). Each iteration of this loop allocates a contiguous run of pages and
        // uses a single write() call to the lower level for transferring data.
        //
        // This is expected to be the main "hot path" for generating new PDBs (not just editing
        // existing ones). The page allocator should usually give us a long run of pages; it only
        // needs to break up runs when we cross interval boundaries (because of the FPM pages).

        loop {
            let num_pages_wanted = (buf.len() as u32) / page_size;
            if num_pages_wanted == 0 {
                break;
            }

            // Allocate pages, add them to our stream, and update the stream size.
            // These must all be done before I/O so that our in-memory state is consistent.
            let (first_page, run_len) = self.page_allocator.alloc_pages(num_pages_wanted);
            assert!(run_len > 0);
            let xfer_len: u32 = run_len << page_size.exponent();
            for i in 0..run_len {
                self.pages.push(first_page + i);
            }

            let buf_head = take_n(buf, xfer_len as usize);

            let file_offset = page_to_offset(first_page, page_size);

            trace!(
                stream_pos = *pos,
                first_page = first_page,
                file_offset,
                xfer_len,
                "write_append_complete_pages"
            );

            self.file.write_all_at(buf_head, file_offset)?;

            *self.size += xfer_len;
            *pos += xfer_len;
        }

        Ok(())
    }

    /// Append the final unaligned page to the file, if any.
    fn write_append_final_unaligned_page(
        &mut self,
        buf: &mut &[u8],
        pos: &mut u32,
    ) -> std::io::Result<()> {
        let page_size = self.page_allocator.page_size;

        assert_eq!(*self.size, *pos);
        assert!(page_size.is_aligned(*pos));

        // The only thing left is a single partial page at the end. We use the page buffer so that we
        // are sending down complete page writes to the lower-level storage device.
        assert!(buf.len() < usize::from(page_size));
        if buf.is_empty() {
            return Ok(());
        }

        let page = self.page_allocator.alloc_page();

        let mut page_buffer = self.page_allocator.alloc_page_buffer();
        page_buffer[..buf.len()].copy_from_slice(buf);
        page_buffer[buf.len()..].fill(0);

        self.pages.push(page);
        *self.size += buf.len() as u32;

        let file_offset = page_to_offset(page, page_size);

        trace!(
            stream_pos = *pos,
            page = page,
            file_offset,
            unaligned_len = buf.len(),
            "write_append_final_unaligned_page"
        );

        self.file.write_all_at(&page_buffer, file_offset)?;

        *pos += buf.len() as u32;
        *buf = &[];

        Ok(())
    }

    /// Handles zero-extending a stream. This occurs when the write position is beyond the
    /// current size of the stream. This implicitly writes zeroes from the old end of the stream
    /// to the start of the write request.
    ///
    /// This implementation will transfer the prefix of `buf` if `pos` is unaligned. If `buf` fits
    /// entirely on one page, then this finishes the entire transfer. If some portion of `buf` is
    /// transferred, then it will be written to 1 or 2 pages. It will be written to 2 pages if it
    /// crosses a page boundary.
    fn write_zero_extend(&mut self, buf: &mut &[u8], pos: &mut u32) -> std::io::Result<()> {
        let page_size = self.page_allocator.page_size;

        self.write_zero_extend_unaligned_start(buf, pos)?;
        if buf.is_empty() {
            return Ok(());
        }

        // If we have more bytes to write, then write_zero_extend_unaligned_start() should
        // have aligned the stream size to a page boundary.
        assert!(page_size.is_aligned(*self.size));

        if *self.size < *pos {
            self.write_zero_extend_whole_pages(*pos)?;
        }

        if *self.size < *pos {
            self.write_zero_extend_unaligned_end(buf, pos)?;
        }

        Ok(())
    }

    fn write_zero_extend_unaligned_start(
        &mut self,
        buf: &mut &[u8],
        pos: &mut u32,
    ) -> std::io::Result<()> {
        assert!(*self.size < *pos); // caller should have already checked this
        let num_zx = *pos - *self.size; // number of zero-extend bytes we need

        let page_size = self.page_allocator.page_size;

        // current end of the stream
        let end_spage: StreamPage = *self.size >> page_size.exponent();
        let end_phase = offset_within_page(*self.size, page_size);

        // where the new data begins
        let pos_spage: StreamPage = *pos >> page_size.exponent();
        let pos_phase = offset_within_page(*pos, page_size);

        if end_phase != 0 {
            // The end of the stream is not page-aligned and we are extending the end of the stream
            // by one or more bytes.

            // Prepare the page buffer that we are assembling. Zero-fill it.
            let mut page_buffer = self.page_allocator.alloc_page_buffer();
            page_buffer.fill(0);

            // Read the old data from the page. The read starts at phase 0 within the page and ends
            // at end_phase. If this fails, that's OK, we haven't made any state changes yet and the
            // error propagates.
            {
                let file_page = self.pages[end_spage as usize];
                let file_offset = page_to_offset(file_page, page_size);
                self.file
                    .read_exact_at(&mut page_buffer[..end_phase as usize], file_offset)?;
            }

            if end_spage == pos_spage {
                // The stream ends on the same page that new-data begins on, and the end of
                // the stream is unaligned. This means that we have a very complicated page to
                // deal with. It has old stream data, zero-extend bytes, and 1 or more bytes of
                // new data. We also have to deal with copy-on-write for the page.
                //
                // We expect zero-extending to be a rare case. We implement this by allocating a
                // page buffer, reading the unaligned piece of the old page, zeroing the middle,
                // copying the unaligned piece of the new data, and optionally zeroing the tail.
                // Then we allocate a fresh page (if needed) and write the page data that we built.
                // Then we write the page to disk and update the stream page pointer.
                //
                // There are two subcases to consider:
                // 1) the new data does not reach the end of this page (is unaligned), so there are
                //    some undefined bytes at the end of the page
                // 2) the new data reaches or crosses the end of this page (is aligned), so we
                //    "paint" the entire page to its end with new data.

                // within end_spage:
                // |------old-data-------|-------zeroes------|-------new-data-----|----
                //                       |                   |
                //               end_phase                   |
                //                                           |
                //                                           pos_phase

                assert!(end_phase <= pos_phase);

                // Copy the new data into the page buffer. The new data may end within this page
                // or may cross the boundary to the next page, which is what the min() call handles.
                let buf_head_len = (usize::from(page_size) - pos_phase as usize).min(buf.len());
                let buf_head = take_n(buf, buf_head_len);
                page_buffer[pos_phase as usize..][..buf_head.len()].copy_from_slice(buf_head);

                // Move pos because we have consumed data from 'buf'
                *pos += buf_head_len as u32;

                // In this case, all of the zero-extend bytes have been handled in this first page,
                // so we can advance pos by num_zx.
                *self.size += num_zx;
                *self.size += buf_head_len as u32;
            } else {
                // The new data does not overlap the page we are working on. That means the
                // zero-extend region reaches to the end of the page.

                // within end_spage:
                // |------old-data-------|-------zeroes-------------------------------|
                //                       |                                            |
                //               end_phase                                            |
                //                                                                    |
                //                                                               page_size

                let num_zx_this_page = u32::from(page_size) - end_phase;
                *self.size += num_zx_this_page;
            }

            // COW the page and write it.
            self.cow_page_and_write(end_spage, &page_buffer)?;
        }

        Ok(())
    }

    /// Writes zero or more complete zero pages during zero-extension. The size of the stream has
    /// already been aligned to a page boundary.
    ///
    /// This does not read data from the current transfer request, so it does not need `buf`.
    /// It also does not change `pos`.
    fn write_zero_extend_whole_pages(&mut self, pos: u32) -> std::io::Result<()> {
        let page_size = self.page_allocator.page_size;

        assert!(*self.size <= pos);
        assert!(page_size.is_aligned(*self.size));

        if (pos - *self.size) / page_size == 0 {
            return Ok(());
        }

        let mut page_buffer = self.page_allocator.alloc_page_buffer();
        page_buffer.fill(0); // probably redundant

        loop {
            let num_pages_wanted = (pos - *self.size) / page_size;
            if num_pages_wanted == 0 {
                break;
            }

            let (first_page, run_len) = self.page_allocator.alloc_pages(num_pages_wanted);
            assert!(run_len > 0);
            for i in 0..run_len {
                self.pages.push(first_page + i);
            }

            let run_size_bytes = run_len << page_size.exponent();
            *self.size += run_size_bytes;

            assert!(*self.size <= pos);

            // Write the zeroed pages.
            for i in 0..run_len {
                let page = first_page + i;
                self.file
                    .write_at(&page_buffer, page_to_offset(page, page_size))?;
            }
        }

        Ok(())
    }

    /// If the zero-extend region ends with an unaligned final page, then this will write that page.
    /// This may transfer data from `buf`.
    fn write_zero_extend_unaligned_end(
        &mut self,
        buf: &mut &[u8],
        pos: &mut u32,
    ) -> std::io::Result<()> {
        let page_size = self.page_allocator.page_size;

        assert!(*self.size <= *pos);
        assert!(page_size.is_aligned(*self.size));

        // We should have at most a partial page.
        let num_zx_bytes = *pos - *self.size;
        assert!(num_zx_bytes < u32::from(page_size));

        if num_zx_bytes == 0 {
            return Ok(());
        }

        let mut page_buffer = self.page_allocator.alloc_page_buffer();

        page_buffer[0..num_zx_bytes as usize].fill(0);

        let num_data_len = buf
            .len()
            .min(usize::from(page_size) - num_zx_bytes as usize);
        let buf_head = take_n(buf, num_data_len);
        *pos += num_data_len as u32;

        page_buffer[num_zx_bytes as usize..num_zx_bytes as usize + num_data_len]
            .copy_from_slice(buf_head);

        let end_of_data = num_zx_bytes as usize + num_data_len;
        page_buffer[end_of_data..usize::from(page_size)].fill(0);

        let page = self.page_allocator.alloc_page();
        self.pages.push(page);
        *self.size += end_of_data as u32;

        self.file
            .write_at(&page_buffer, page_to_offset(page, page_size))?;

        Ok(())
    }

    /// Handles _most_ of overwrite.
    ///
    /// This handles:
    /// * the unaligned page at the start of overwrite (if any)
    /// * the complete, aligned pages in the middle of overwrite (if any)
    ///
    /// When this returns, even if buf still has data, we do not guarantee that pos == stream_size.
    fn write_overwrite(&mut self, buf: &mut &[u8], pos: &mut u32) -> std::io::Result<()> {
        assert!(*pos < *self.size);

        self.write_overwrite_unaligned_start(buf, pos)?;
        if buf.is_empty() {
            return Ok(());
        }

        assert!(self.page_allocator.page_size.is_aligned(*pos));

        self.write_overwrite_aligned_pages(buf, pos)?;
        if buf.is_empty() {
            return Ok(());
        }

        assert!(self.page_allocator.page_size.is_aligned(*pos));
        assert!(self.page_allocator.page_size.is_aligned(*self.size));
        Ok(())
    }

    /// Handles writing the first page during overwrite, if the first page is unaligned.
    ///
    /// # Requires
    /// * `pos <= stream_size`
    ///
    /// # Ensures
    /// * `buf.is_empty() || pos == stream_size`
    fn write_overwrite_unaligned_start(
        &mut self,
        buf: &mut &[u8],
        pos: &mut u32,
    ) -> std::io::Result<()> {
        assert!(*pos <= *self.size);

        let page_size = self.page_allocator.page_size;

        let pos_spage: StreamPage = *pos >> page_size.exponent();
        let pos_phase = offset_within_page(*pos, page_size);
        if pos_phase == 0 {
            // The overwrite starts on an aligned page boundary. This function has no work to do.
            return Ok(());
        }

        // In this case, we need to assemble a page from some old data and some new data.
        // And because we need to cow the page, it is easier to reassemble everything into a
        // page buffer.
        //
        // Contents of this page:
        //
        //
        //
        //        not empty            not empty             can be empty          can be empty
        // |----old-data----------|------new-data--------|-------old-data------|-----garbage-----
        // 0                      |                      |                     |
        //                        |                      |                     |
        //                  pos_phase
        //
        // At this point, we don't know which subcase we're in. It depends on whether new_data
        // reaches the end of this page and whether writing the new-data extends the stream size.

        // Read the old page data into our page buffer. This makes cow'ing the page easier.
        // We read the entire page because that simplifies the case where the new-data ends
        // before stream_size.
        let mut page_buffer = self.page_allocator.alloc_page_buffer();
        self.read_page(pos_spage, &mut page_buffer)?;

        // Copy the data from 'buf' (new-data) into the page and advance 'buf' and 'pos'.
        let new_data_len = (usize::from(page_size) - pos_phase as usize).min(buf.len());
        assert!(new_data_len > 0);
        let buf_head = take_n(buf, new_data_len);
        page_buffer[pos_phase as usize..][..buf_head.len()].copy_from_slice(buf_head);
        *pos += new_data_len as u32;

        // Cow the page and write its new contents.
        self.cow_page_and_write(pos_spage, &page_buffer)?;

        // We may have written enough data that we extended stream_size. This only happens if we
        // drag in some of the prefix of buf.
        if *pos > *self.size {
            *self.size = *pos;
        }

        Ok(())
    }

    /// If we are doing overwrite and the remaining buffer contains one or more whole pages,
    /// then this function transfers those.
    ///
    /// This function may extend the stream size. If it does, then it will not extend it enough
    /// to cross a page boundary. This form of extension happens when we are overwriting beyond
    /// the existing end of the stream.
    ///
    /// This function will cow pages, but will not allocate new page slots in the stream.
    ///
    /// # Requires
    /// * `pos` is page-aligned
    /// * `pos <= stream_size`
    ///
    /// # Ensures
    /// * `pos` is page-aligned
    /// * `buf.is_empty() || stream_size is page-aligned`
    fn write_overwrite_aligned_pages(
        &mut self,
        buf: &mut &[u8],
        pos: &mut u32,
    ) -> std::io::Result<()> {
        let page_size = self.page_allocator.page_size;
        assert!(*pos <= *self.size);
        assert!(page_size.is_aligned(*pos));

        if *pos == *self.size {
            return Ok(());
        }

        // The stream page where this transfer will begin (if any).
        let pos_spage = *pos / page_size;

        // Number of complete pages that can be read from buf.
        let num_buf_pages = buf.len() as u32 / page_size;

        // Number of pages at our write position that are assigned to the stream.
        // This includes the partial page at the end, if any.
        let num_pages_total = self.size.div_round_up(page_size);
        assert_eq!(num_pages_total, self.pages.len() as u32);
        let num_pages_at_pos = *pos / page_size;
        let num_pages_owned = num_pages_total - num_pages_at_pos;

        // Number of complete pages we are going to transfer from buf to disk.
        // Since we are writing whole pages, we do not need the old contents of any page.
        // Cow the pages, just so we get fresh pages.
        let num_xfer_pages = num_pages_owned.min(num_buf_pages);
        if num_xfer_pages != 0 {
            trace!(num_pages = num_xfer_pages, "writing whole pages");

            let num_xfer_bytes = num_xfer_pages << page_size.exponent();
            let buf_head = take_n(buf, num_xfer_bytes as usize);
            *pos += num_xfer_bytes;

            let pages = &mut self.pages[pos_spage as usize..][..num_xfer_pages as usize];
            self.page_allocator.make_pages_fresh(pages);
            write_runs(&self.file, buf_head, pages, page_size)?;

            // If the last page that we overwrite was a partial page, then we will have extended
            // the size of the stream. This will not extend stream_size beyond a page boundary.
            if *pos > *self.size {
                *self.size = *pos;
            }

            if buf.is_empty() {
                return Ok(());
            }
        }

        assert!(page_size.is_aligned(*pos));
        assert!(*pos <= *self.size);

        // We may have gotten here because buf.len() is now less than a full page size, but we still
        // have another page assigned in the stream. Cow it now.
        if *self.size - *pos > 0 {
            trace!(buf_len = buf.len(), "buffer has partial page remaining.");
            assert!(
                buf.len() < usize::from(page_size),
                "size = {:x}, pos = {:x}, buf.len = 0x{:x}",
                *self.size,
                *pos,
                buf.len()
            );

            let spage = *pos / page_size;
            let old_len = *self.size - *pos;

            let mut page_buffer = self.page_allocator.alloc_page_buffer();
            if old_len > buf.len() as u32 {
                self.read_page(spage, &mut page_buffer)?;
            }

            page_buffer[..buf.len()].copy_from_slice(buf);

            self.cow_page_and_write(spage, &page_buffer)?;
            *pos += buf.len() as u32;
            if *pos > *self.size {
                *self.size = *pos;
            }

            *buf = &[];
            return Ok(());
        }

        assert_eq!(*pos, *self.size);

        Ok(())
    }

    /// Writes the unaligned start page at the beginning of the new-data. This page is only
    /// present if `pos` is not aligned.
    ///
    /// # Requires
    /// * `stream_size == pos`
    fn write_unaligned_start_page(
        &mut self,
        buf: &mut &[u8],
        pos: &mut u32,
    ) -> std::io::Result<()> {
        assert_eq!(*pos, *self.size);

        // In this case, we need to assemble a page from some old data and some new data.
        // And because we need to cow the page, it is easier to reassemble everything into a
        // page buffer.
        //
        // Contents of this page:
        //
        //
        //
        //        not empty            not empty             can be empty          can be empty
        // |----old-data----------|------new-data--------|-------old-data------|-----garbage-----
        // 0                      |                      |                     |
        //                        |                      |                     |
        //                  pos_phase
        //
        // At this point, we don't know which subcase we're in. It depends on whether new_data
        // reaches the end of this page and whether writing the new-data extends the stream size.

        let page_size = self.page_allocator.page_size;

        // where the new data begins
        let pos_spage: StreamPage = *pos >> page_size.exponent();
        let pos_phase = offset_within_page(*pos, page_size); // <-- we know this is non-zero

        // Read the old page data into our page buffer. This makes cow'ing the page easier.
        // We read the entire page because that simplifies the case where the new-data ends
        // before stream_size.
        let mut page_buffer = self.page_allocator.alloc_page_buffer();

        let file_offset = page_to_offset(self.pages[pos_spage as usize], page_size);
        trace!(
            stream_pos = *pos,
            file_offset,
            len = u32::from(page_size),
            "write_unaligned_start_page: reading existing unaligned data"
        );

        self.file.read_exact_at(&mut page_buffer, file_offset)?;

        // Copy the data from 'buf' (new-data) into the page and advance 'buf' and 'pos'.
        let new_data_len = (usize::from(page_size) - pos_phase as usize).min(buf.len());
        assert!(new_data_len > 0);
        let buf_head = take_n(buf, new_data_len);
        page_buffer[pos_phase as usize..][..buf_head.len()].copy_from_slice(buf_head);
        *pos += new_data_len as u32;

        // Cow the page and write its new contents.
        self.cow_page_and_write(pos_spage, &page_buffer)?;

        // We may have written enough data that we extended stream_size.
        if *pos > *self.size {
            *self.size = *pos;
        }

        Ok(())
    }

    /// Ensures that a stream page is writable (is "fresh"). If the page is not writable, then it
    /// allocates a new page. This function returns the page number of the writable page.
    fn cow_page(&mut self, spage: StreamPage) -> Page {
        self.page_allocator
            .make_page_fresh(&mut self.pages[spage as usize])
    }

    /// Ensures that a stream page is writable and then writes it.
    ///
    /// `data` should contain at most one page of data.
    pub(super) fn cow_page_and_write(
        &mut self,
        spage: StreamPage,
        data: &[u8],
    ) -> std::io::Result<()> {
        // At most one full page of data can be written.
        debug_assert!(
            data.len() <= usize::from(self.page_allocator.page_size),
            "buffer cannot exceed size of a single page"
        );

        let page = self.cow_page(spage);
        let file_offset = page_to_offset(page, self.page_allocator.page_size);

        trace!(
            stream_page = spage,
            file_offset,
            len = u32::from(self.page_allocator.page_size),
            "cow_page_and_write"
        );

        self.file.write_all_at(data, file_offset)
    }

    /// Reads a stream page.  The length of `data` must be exactly one page, or less. It cannot
    /// cross page boundaries.
    pub(super) fn read_page(
        &self,
        stream_page: StreamPage,
        data: &mut [u8],
    ) -> std::io::Result<()> {
        debug_assert!(
            data.len() <= usize::from(self.page_allocator.page_size),
            "buffer cannot exceed size of a single page"
        );

        let page = self.pages[stream_page as usize];
        let offset = page_to_offset(page, self.page_allocator.page_size);
        self.file.read_exact_at(data, offset)
    }

    /// The caller **must** guarantee that this page is already writable.
    pub(super) fn write_page(&self, stream_page: StreamPage, data: &[u8]) -> std::io::Result<()> {
        let page = self.pages[stream_page as usize];
        assert!(
            self.page_allocator.fresh[page as usize],
            "page is required to be fresh (writable)"
        );

        let file_offset = page_to_offset(page, self.page_allocator.page_size);

        trace!(
            stream_page,
            page = page,
            file_offset,
            len = u32::from(self.page_allocator.page_size),
            "write_page"
        );

        self.file.write_all_at(data, file_offset)
    }
}

/// Finds the length of the prefix of pages in `pages` that are numbered sequentially.
///
/// This function assumes that there are no entries with the value 0xffff_ffff. (That would cause
/// overflow.)
fn find_longest_page_run(pages: &[Page]) -> usize {
    if pages.is_empty() {
        0
    } else {
        let mut prev = pages[0];
        let mut i = 1;
        while i < pages.len() && pages[i] == prev + 1 {
            prev = pages[i];
            i += 1;
        }
        i
    }
}

#[test]
fn test_find_longest_page_run() {
    assert_eq!(find_longest_page_run(&[]), 0);
    assert_eq!(find_longest_page_run(&[1]), 1);
    assert_eq!(find_longest_page_run(&[1, 2, 3]), 3);
    assert_eq!(find_longest_page_run(&[1, 3, 2]), 1);
    assert_eq!(find_longest_page_run(&[1, 2, 3, 9, 9, 9]), 3);
}

/// Given a page map that corresponds to a buffer of data to write, write all of the data.
/// Write it in a sequence of function calls that group together consecutive pages, so that
/// we minimize the number of write() calls.
fn write_runs<F: WriteAt>(
    file: &F,
    mut buf: &[u8],
    pages: &[Page],
    page_size: PageSize,
) -> std::io::Result<()> {
    let mut region_pages = pages;

    assert_eq!(buf.len(), pages.len() << page_size.exponent());

    loop {
        let run_len = find_longest_page_run(region_pages);
        if run_len == 0 {
            break;
        }

        let page0: Page = region_pages[0];
        let xfer_len: usize = run_len << page_size.exponent();
        let buf_head = take_n(&mut buf, xfer_len);

        // Write the run of pages. If this write fails, then the contents of the stream are
        // now in an undefined state.
        file.write_at(buf_head, page_to_offset(page0, page_size))?;

        // Advance iterator state
        region_pages = &region_pages[run_len..];
    }

    Ok(())
}

impl<F> Msf<F> {
    /// Adds a new stream to the MSF file. The stream has a length of zero.
    pub fn new_stream(&mut self) -> anyhow::Result<(u32, StreamWriter<'_, F>)> {
        let _span = trace_span!("new_stream").entered();

        self.requires_writeable()?;
        self.check_can_add_stream()?;

        let new_stream_index = self.stream_sizes.len() as u32;
        trace!(new_stream_index);

        self.stream_sizes.push(0);
        let size = self.stream_sizes.last_mut().unwrap();

        let pages = match self.modified_streams.entry(new_stream_index) {
            Entry::Occupied(_) => {
                panic!("Found entry in modified streams table that should not be present.")
            }
            Entry::Vacant(v) => v.insert(Vec::new()),
        };

        Ok((
            new_stream_index,
            StreamWriter {
                stream: new_stream_index,
                file: &self.file,
                size,
                page_allocator: &mut self.pages,
                pos: 0,
                pages,
            },
        ))
    }

    fn check_can_add_stream(&self) -> anyhow::Result<()> {
        if self.stream_sizes.len() as u32 >= self.max_streams {
            bail!(
                "A new stream cannot be created because the maximum number of streams has been reached."
            );
        }
        Ok(())
    }

    /// Adds a new stream to the MSF file, given the byte contents. This function returns the
    /// stream index of the new stream.
    pub fn new_stream_data(&mut self, data: &[u8]) -> anyhow::Result<u32>
    where
        F: ReadAt + WriteAt,
    {
        let (stream_index, mut writer) = self.new_stream()?;
        writer.set_contents(data)?;
        Ok(stream_index)
    }

    /// Adds a new nil stream to the MSF file.
    pub fn nil_stream(&mut self) -> anyhow::Result<u32> {
        self.requires_writeable()?;
        self.check_can_add_stream()?;

        let new_stream_index = self.stream_sizes.len() as u32;
        self.stream_sizes.push(NIL_STREAM_SIZE);

        match self.modified_streams.entry(new_stream_index) {
            Entry::Occupied(_) => {
                panic!("Found entry in modified streams table that should be present.")
            }
            Entry::Vacant(v) => {
                v.insert(Vec::new());
            }
        }

        Ok(new_stream_index)
    }

    /// Given the stream index for a stream, returns a `StreamWriter` that allows read/write
    /// for the stream.
    ///
    /// If `stream` is out of range for the current set of streams, then the set of streams is
    /// increased until `stream` is in range. For example, if a new MSF file is created, then
    /// it is legal to immediately call `msf.write_stream(10)` on it. This will expand the Stream
    /// Directory so that `num_streams()` returns 11 (because it must include the new stream index).
    /// All streams lower than `stream` will be allocated as nil streams.
    ///
    /// If `stream` is currently a nil stream, then this function promotes the stream to a
    /// non-nil stream.
    pub fn write_stream(&mut self, stream: u32) -> anyhow::Result<StreamWriter<'_, F>> {
        assert!(stream <= MAX_STREAM);
        self.requires_writeable()?;

        while (self.stream_sizes.len() as u32) <= stream {
            _ = self.nil_stream()?;
        }

        let Some(size) = self.stream_sizes.get_mut(stream as usize) else {
            bail!("Stream index is out of range");
        };

        // If the stream is currently a nil stream, then promote it to a zero-length stream.
        if *size == NIL_STREAM_SIZE {
            *size = 0;
        }

        let pages = match self.modified_streams.entry(stream) {
            Entry::Occupied(occ) => occ.into_mut(),
            Entry::Vacant(v) => {
                // Copy the existing page list to a new page list.
                //
                // Copying the page list _does not_ imply that we can safely write to those pages,
                // because they may still be owned by the previous committed state. Copy-on-write
                // is handled elsewhere.
                let starts = &self.committed_stream_page_starts[stream as usize..];
                let old_pages =
                    &self.committed_stream_pages[starts[0] as usize..starts[1] as usize];
                v.insert(old_pages.to_vec())
            }
        };

        Ok(StreamWriter {
            stream,
            file: &self.file,
            size,
            page_allocator: &mut self.pages,
            pos: 0,
            pages,
        })
    }

    pub(crate) fn requires_writeable(&self) -> anyhow::Result<()> {
        match self.access_mode {
            AccessMode::ReadWrite => Ok(()),
            AccessMode::Read => bail!("This PDB was not opened for read/write access."),
        }
    }

    /// Copies a stream from another PDB/MSF into this one.
    pub fn copy_stream<Input: ReadAt>(
        &mut self,
        source: &Msf<Input>,
        source_stream: u32,
    ) -> anyhow::Result<u32>
    where
        F: ReadAt + WriteAt,
    {
        const BUFFER_LEN: usize = 16 << 20; // 16 MiB

        let mut source_reader = source.get_stream_reader(source_stream)?;
        let source_len = source_reader.len();

        let mut buffer = vec![0; (source_len as usize).min(BUFFER_LEN)];
        let (dest_stream_index, mut dest_writer) = self.new_stream()?;

        loop {
            let n = source_reader.read(&mut buffer)?;
            if n == 0 {
                break;
            }

            dest_writer.write_all(&buffer[..n])?;
        }

        Ok(dest_stream_index)
    }

    /// Copies a stream that implements `Read` into this PDB/MSF file.
    pub fn copy_stream_read<Input: Read>(&mut self, source: &mut Input) -> anyhow::Result<u32>
    where
        F: ReadAt + WriteAt,
    {
        const BUFFER_LEN: usize = 16 << 20; // 16 MiB

        let mut buffer = vec![0; BUFFER_LEN];
        let (dest_stream_index, mut dest_writer) = self.new_stream()?;

        loop {
            let n = source.read(&mut buffer)?;
            if n == 0 {
                break;
            }

            dest_writer.write_all(&buffer[..n])?;
        }

        Ok(dest_stream_index)
    }

    /// Copies a stream that implements `ReadAt` into this PDB/MSF file.
    pub fn copy_stream_read_at<Input: ReadAt>(&mut self, source: &Input) -> anyhow::Result<u32>
    where
        F: ReadAt + WriteAt,
    {
        const BUFFER_LEN: usize = 16 << 20; // 16 MiB

        let mut buffer = vec![0; BUFFER_LEN];
        let (dest_stream_index, mut dest_writer) = self.new_stream()?;

        let mut pos: u64 = 0;

        loop {
            let n = source.read_at(&mut buffer, pos)?;
            if n == 0 {
                break;
            }

            dest_writer.write_all(&buffer[..n])?;
            pos += n as u64;
        }

        Ok(dest_stream_index)
    }
}

/// Splits a slice `items` at a given index `n`. The slice is modified to point to the items
/// after `n`. The function returns the items up to `n`.
fn take_n<'a, T>(items: &mut &'a [T], n: usize) -> &'a [T] {
    let (lo, hi) = items.split_at(n);
    *items = hi;
    lo
}

```

`msfz/Cargo.toml`:

```toml
[package]
name = "ms-pdb-msfz"
version = "0.1.10"
edition = "2024"
description = "Reads Compressed Multi-Stream Files, which is part of the Microsoft PDB file format"
authors = ["Arlie Davis <ardavis@microsoft.com>"]
license = "MIT OR Apache-2.0"
repository = "https://github.com/microsoft/pdb-rs/"
categories = ["parsing", "development-tools", "development-tools::debugging"]

[lib]
doctest = false

[dependencies]
anyhow.workspace = true
flate2.workspace = true
static_assertions.workspace = true
sync_file.workspace = true
tracing.workspace = true
zerocopy = { workspace = true, features = ["alloc", "derive"] }
zerocopy-derive.workspace = true
zstd.workspace = true
pow2.workspace = true

[dev-dependencies]
bstr.workspace = true
pretty-hex.workspace = true
static_init.workspace = true
tracing-subscriber = { workspace = true, features = ["fmt"] }

[dev-dependencies.tracing-tracy]
workspace = true

[dev-dependencies.tracy-client]
version = "0.18.0"
features = ["manual-lifetime", "only-localhost"]

```

`msfz/src/compress_utils.rs`:

```rs
use std::io::{Read, Write};
use tracing::{trace, trace_span};

use crate::Compression;

pub(crate) fn compress_to_vec(compression: Compression, input: &[u8]) -> std::io::Result<Vec<u8>> {
    let mut output = Vec::new();
    compress_to_vec_mut(compression, input, &mut output)?;
    Ok(output)
}

pub(crate) fn compress_to_vec_mut(
    compression: Compression,
    input: &[u8],
    output: &mut Vec<u8>,
) -> std::io::Result<()> {
    let _span = trace_span!("compress_to_vec_mut").entered();

    let output_original_len = output.len();
    let out = &mut *output; // reborrow

    match compression {
        Compression::Zstd => {
            let mut enc = zstd::Encoder::new(out, 0)?;
            enc.write_all(input)?;
            enc.finish()?;
        }

        Compression::Deflate => {
            let mut enc = flate2::write::DeflateEncoder::new(
                std::io::Cursor::new(out),
                flate2::Compression::default(),
            );

            enc.write_all(input)?;
            enc.finish()?;
        }
    }

    trace!(
        ?compression,
        uncompressed_len = input.len(),
        compressed_len = output.len() - output_original_len
    );

    Ok(())
}

/// Decompresses a compressed buffer using the given compression algorithm.
///
/// `output.len()` specifies the expected size of the decoded stream. Returns `Err` if the
/// decompression algorithm returned the wrong number of bytes.
pub(crate) fn decompress_to_slice(
    compression: Compression,
    input: &[u8],
    output: &mut [u8],
) -> std::io::Result<()> {
    let _span = trace_span!("decompress_to_slice").entered();

    match compression {
        Compression::Zstd => {
            let mut dec = zstd::Decoder::new(input)?;
            dec.read_exact(output)?;
        }

        Compression::Deflate => {
            let mut dec = flate2::read::DeflateDecoder::new(input);
            dec.read_exact(output)?;
        }
    };

    trace!(
        ?compression,
        compressed_len = input.len(),
        uncompressed_len = output.len()
    );

    Ok(())
}

```

`msfz/src/lib.rs`:

```rs
//! Multi-Stream File - Compressed
//!
//! This crate allows reading and writing PDZ/MSFZ files. PDZ/MSFZ files are similar to PDB/MSF
//! files. They contain a set of streams, which are indexed by number. Each stream is a sequence
//! of bytes, similar to an ordinary file.
//!
//! See the [MSFZ Container Specification](https://github.com/microsoft/pdb-rs/blob/main/docs/pdb/msfz.md)

#![forbid(unsafe_code)]
#![forbid(unused_must_use)]
#![warn(missing_docs)]
#![allow(clippy::needless_lifetimes)]

use std::fs::OpenOptions;

use anyhow::Result;
use zerocopy::{FromBytes, FromZeros, Immutable, IntoBytes, KnownLayout, LE, U32, U64, Unaligned};

mod compress_utils;
mod reader;
mod stream_data;
#[cfg(test)]
mod tests;
mod writer;

pub use reader::{Fragment, FragmentLocation, Msfz, StreamReader};
pub use stream_data::StreamData;
pub use writer::*;

/// Describes the header at the start of the MSFZ file.
///
/// This describes the on-disk layout of the file header. It is stored at the beginning of the
/// MSFZ file.
#[derive(IntoBytes, FromBytes, Unaligned, Immutable, KnownLayout)]
#[repr(C)]
pub struct MsfzFileHeader {
    /// Identifies this as an MSFZ file. The value must always be [`MSFZ_FILE_SIGNATURE`].
    pub signature: [u8; 32],

    /// Specifies the version of the MSFZ file layout.
    pub version: U64<LE>,

    /// The file offset of the stream directory.
    pub stream_dir_offset: U64<LE>,

    /// The file offset of the Chunk Table, which has type `[ChunkEntry; num_chunks]`.
    pub chunk_table_offset: U64<LE>,

    /// The number of streams stored within this MSFZ file.
    pub num_streams: U32<LE>,

    /// The compression algorithm applied to the stream directory.
    pub stream_dir_compression: U32<LE>,

    /// The size in bytes of the stream directory, compressed (on-disk).
    pub stream_dir_size_compressed: U32<LE>,

    /// The size in bytes of the stream directory after decompression (in-memory).
    pub stream_dir_size_uncompressed: U32<LE>,

    /// The number of compression chunks.
    pub num_chunks: U32<LE>,

    /// The size in bytes of the Chunk Table.
    pub chunk_table_size: U32<LE>,
}

/// Describes one compressed chunk.
#[derive(IntoBytes, FromBytes, Unaligned, Immutable, KnownLayout)]
#[repr(C)]
pub struct ChunkEntry {
    /// File offset (within the MSFZ file) the compressed chunk.
    pub file_offset: U64<LE>,

    /// The compression algorithm for this chunk.
    pub compression: U32<LE>,

    /// Size in bytes of the compressed data on disk.
    ///
    /// This value should be non-zero.
    pub compressed_size: U32<LE>,

    /// Number of bytes after decompression; this is the in-memory size.
    ///
    /// This value should be non-zero.
    pub uncompressed_size: U32<LE>,
}

/// The special value used for stream size to indicate a nil stream.
pub const NIL_STREAM_SIZE: u32 = 0xffff_ffff;

/// Indicates that no compression is used.
pub const COMPRESSION_NONE: u32 = 0;

/// Identifies the [`Zstd`](https://github.com/facebook/zstd) compression algorithm.
pub const COMPRESSION_ZSTD: u32 = 1;

/// Identifies the [`Deflate`](https://en.wikipedia.org/wiki/Deflate) compression algorithm.
///
/// This uses the "raw" Deflate stream. It _does not_ use the GZIP encapsulation header.
pub const COMPRESSION_DEFLATE: u32 = 2;

/// This is the maximum file offset where an uncompressed fragment be be stored.
///
/// The MSFZ specification provides 48 bits for storing the file offset of an uncompressed fragment.
pub const MAX_UNCOMPRESSED_FILE_OFFSET: u64 = (1u64 << 48) - 1;

/// Specifies the compression algorithms that are supported by this library.
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
#[non_exhaustive]
pub enum Compression {
    /// Identifies the [`Zstd`](https://github.com/facebook/zstd) compression algorithm.
    Zstd,
    /// Identifies the [`Deflate`](https://en.wikipedia.org/wiki/Deflate) compression algorithm.
    Deflate,
}

impl Compression {
    fn to_code(self) -> u32 {
        match self {
            Self::Zstd => COMPRESSION_ZSTD,
            Self::Deflate => COMPRESSION_DEFLATE,
        }
    }

    fn try_from_code(code: u32) -> Result<Self, UnsupportedCompressionError> {
        match code {
            COMPRESSION_ZSTD => Ok(Self::Zstd),
            COMPRESSION_DEFLATE => Ok(Self::Deflate),
            _ => Err(UnsupportedCompressionError),
        }
    }

    fn try_from_code_opt(code: u32) -> Result<Option<Self>, UnsupportedCompressionError> {
        if code != COMPRESSION_NONE {
            Ok(Some(Self::try_from_code(code)?))
        } else {
            Ok(None)
        }
    }
}

#[derive(Copy, Clone, Debug)]
struct UnsupportedCompressionError;

impl std::error::Error for UnsupportedCompressionError {}

impl std::fmt::Display for UnsupportedCompressionError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(
            f,
            "The specified compression mode is not recognized or supported."
        )
    }
}

/// The signature of a MSFZ/PDZ file.
pub const MSFZ_FILE_SIGNATURE: [u8; 32] = *b"Microsoft MSFZ Container\r\n\x1aALD\0\0";

#[test]
fn print_file_signature() {
    use pretty_hex::PrettyHex;
    println!("\n{:?}", MSFZ_FILE_SIGNATURE.hex_dump());
}

/// The current version of the PDZ specification being developed.
pub const MSFZ_FILE_VERSION_V0: u64 = 0;

/// Checks whether the header of a file appears to be a valid MSFZ/PDZ file.
///
/// This only looks at the signature; it doens't read anything else in the file.
pub fn is_header_msfz(header: &[u8]) -> bool {
    header.starts_with(&MSFZ_FILE_SIGNATURE)
}

fn open_options_shared(options: &mut OpenOptions) -> &mut OpenOptions {
    #[cfg(windows)]
    {
        use std::os::windows::fs::OpenOptionsExt;
        const FILE_SHARE_READ: u32 = 1;
        options.share_mode(FILE_SHARE_READ)
    }
    #[cfg(not(windows))]
    {
        options
    }
}

fn open_options_exclusive(options: &mut OpenOptions) -> &mut OpenOptions {
    #[cfg(windows)]
    {
        use std::os::windows::fs::OpenOptionsExt;
        options.share_mode(0)
    }
    #[cfg(not(windows))]
    {
        options
    }
}

```

`msfz/src/reader.rs`:

```rs
use crate::*;
use anyhow::{Result, bail};
use core::mem::size_of;
use std::fs::File;
use std::io::{Read, Seek, SeekFrom};
use std::path::Path;
use std::sync::{Arc, OnceLock};
use sync_file::{RandomAccessFile, ReadAt};
use tracing::{debug, debug_span, info_span, trace, trace_span};
use zerocopy::IntoBytes;

/// Reads MSFZ files.
pub struct Msfz<F = RandomAccessFile> {
    file: F,
    /// The list of all fragments in all streams.
    ///
    /// `fragments` is sorted by stream index, then by the order of the fragments in each stream.
    /// Each stream has zero or more fragments associated with it. The set of fragments for a stream `s` is
    /// `&fragments[stream_fragments[s] .. stream_fragments[s + 1]]`.
    fragments: Vec<Fragment>,

    /// Contains the index of the first entry in `fragments` for a given stream.
    ///
    /// The last entry in this list does not point to a stream. It simply points to the end of
    /// the `fragments` list.
    ///
    /// Invariant: `stream_fragments.len() > 0`
    /// Invariant: `stream_fragments.len() == num_streams() + 1`.
    stream_fragments: Vec<u32>,

    chunk_table: Box<[ChunkEntry]>,
    chunk_cache: Vec<OnceLock<Arc<[u8]>>>,
}

/// Describes a region within a stream.
#[derive(Clone)]
pub struct Fragment {
    /// The size in bytes of the fragment
    pub size: u32,
    /// The location of the fragment
    pub location: FragmentLocation,
}

impl std::fmt::Debug for Fragment {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        write!(f, "size 0x{:05x} at {:?}", self.size, self.location)
    }
}

impl std::fmt::Debug for FragmentLocation {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        if self.is_nil() {
            f.write_str("nil")
        } else if self.is_compressed() {
            write!(
                f,
                "chunk {} : 0x{:04x}",
                self.compressed_first_chunk(),
                self.compressed_offset_within_chunk()
            )
        } else {
            write!(
                f,
                "uncompressed at 0x{:06x}",
                self.uncompressed_file_offset()
            )
        }
    }
}

const FRAGMENT_LOCATION_32BIT_IS_COMPRESSED_MASK: u32 = 1u32 << 31;

/// A bit mask for the bits within a packed `FragmentLocation` which encode
/// the file offset of an uncompressed fragment.
const UNCOMPRESSED_FRAGMENT_FILE_OFFSET_MASK: u64 = (1u64 << 48) - 1;

/// Represents the location of a fragment, either compressed or uncompressed.
#[derive(Copy, Clone)]
pub struct FragmentLocation {
    /// bits 0-31
    lo: u32,
    /// bits 32-63
    hi: u32,
}

impl FragmentLocation {
    /// This is a sentinel value for `FragmentLocation` that means "this stream is a nil stream".
    /// It is not an actual fragment.
    const NIL: Self = Self {
        lo: u32::MAX,
        hi: u32::MAX,
    };

    fn is_nil(&self) -> bool {
        self.lo == u32::MAX && self.hi == u32::MAX
    }

    /// Returns `true` if this is a compressed fragment
    pub fn is_compressed(&self) -> bool {
        (self.hi & FRAGMENT_LOCATION_32BIT_IS_COMPRESSED_MASK) != 0
    }

    /// Returns the chunk index for this compressed fragment.
    ///
    /// You must check `is_compressed()` before calling this function.
    pub fn compressed_first_chunk(&self) -> u32 {
        debug_assert!(!self.is_nil());
        debug_assert!(self.is_compressed());
        self.hi & !FRAGMENT_LOCATION_32BIT_IS_COMPRESSED_MASK
    }

    fn compressed_offset_within_chunk(&self) -> u32 {
        debug_assert!(!self.is_nil());
        debug_assert!(self.is_compressed());
        self.lo
    }

    fn uncompressed_file_offset(&self) -> u64 {
        debug_assert!(!self.is_nil());
        debug_assert!(!self.is_compressed());
        (((self.hi as u64) << 32) | (self.lo as u64)) & UNCOMPRESSED_FRAGMENT_FILE_OFFSET_MASK
    }
}

impl Msfz<RandomAccessFile> {
    /// Opens an MSFZ file and validates its header.
    pub fn open<P: AsRef<Path>>(path: P) -> Result<Self> {
        let f = open_options_shared(File::options().read(true)).open(path)?;
        let raf = RandomAccessFile::from(f);
        Self::from_file(raf)
    }
}

impl<F: ReadAt> Msfz<F> {
    /// Opens an MSFZ file using an implementation of the [`ReadAt`] trait.
    pub fn from_file(file: F) -> Result<Self> {
        let _span = info_span!("Msfz::from_file").entered();

        let mut header: MsfzFileHeader = MsfzFileHeader::new_zeroed();
        file.read_exact_at(header.as_mut_bytes(), 0)?;

        if header.signature != MSFZ_FILE_SIGNATURE {
            bail!("This file does not have a PDZ file signature.");
        }

        if header.version.get() != MSFZ_FILE_VERSION_V0 {
            bail!("This PDZ file uses a version number that is not supported.");
        }

        // Load the stream directory.
        let num_streams = header.num_streams.get();
        if num_streams == 0 {
            bail!("The stream directory is invalid; it is empty.");
        }

        let stream_dir_size_uncompressed = header.stream_dir_size_uncompressed.get() as usize;
        let stream_dir_size_compressed = header.stream_dir_size_compressed.get() as usize;
        let stream_dir_file_offset = header.stream_dir_offset.get();
        let stream_dir_compression = header.stream_dir_compression.get();
        debug!(
            num_streams,
            stream_dir_size_uncompressed,
            stream_dir_size_compressed,
            stream_dir_compression,
            stream_dir_file_offset,
            "reading stream directory"
        );

        let mut stream_dir_bytes: Vec<u8> =
            map_alloc_error(FromZeros::new_vec_zeroed(stream_dir_size_uncompressed))?;
        if let Some(compression) = Compression::try_from_code_opt(stream_dir_compression)? {
            let mut compressed_stream_dir: Vec<u8> =
                map_alloc_error(FromZeros::new_vec_zeroed(stream_dir_size_compressed))?;
            file.read_exact_at(
                compressed_stream_dir.as_mut_bytes(),
                header.stream_dir_offset.get(),
            )?;

            debug!("decompressing stream directory");

            crate::compress_utils::decompress_to_slice(
                compression,
                &compressed_stream_dir,
                &mut stream_dir_bytes,
            )?;
        } else {
            if stream_dir_size_uncompressed != stream_dir_size_compressed {
                bail!(
                    "This PDZ file is invalid. The Stream Directory is not compressed, but has inconsistent compressed vs. uncompressed sizes."
                );
            }
            file.read_exact_at(stream_dir_bytes.as_mut_bytes(), stream_dir_file_offset)?;
        }

        // Load the chunk table.
        let num_chunks = header.num_chunks.get() as usize;
        let chunk_index_size = header.chunk_table_size.get() as usize;
        if chunk_index_size != num_chunks * size_of::<ChunkEntry>() {
            bail!("This PDZ file is invalid. num_chunks and chunk_index_size are not consistent.");
        }

        let chunk_table_offset = header.chunk_table_offset.get();
        let mut chunk_table: Box<[ChunkEntry]> =
            map_alloc_error(FromZeros::new_box_zeroed_with_elems(num_chunks))?;
        if num_chunks != 0 {
            debug!(
                num_chunks,
                chunk_table_offset, "reading compressed chunk table"
            );
            file.read_exact_at(chunk_table.as_mut_bytes(), chunk_table_offset)?;
        } else {
            // Don't issue a read. The writer code may not have actually extended the file.
        }

        let mut chunk_cache = Vec::with_capacity(num_chunks);
        chunk_cache.resize_with(num_chunks, Default::default);

        // Decode the Stream Directory. We do this after loading the chunk table so that we can
        // validate fragment records within the Stream Directory now.
        let stream_dir = decode_stream_dir(&stream_dir_bytes, num_streams, &chunk_table)?;

        Ok(Self {
            file,
            fragments: stream_dir.fragments,
            stream_fragments: stream_dir.stream_fragments,
            chunk_table,
            chunk_cache,
        })
    }

    /// The total number of streams in this MSFZ file. This count includes nil streams.
    pub fn num_streams(&self) -> u32 {
        (self.stream_fragments.len() - 1) as u32
    }

    fn stream_fragments_result(&self, stream: u32) -> Result<&[Fragment]> {
        self.stream_fragments(stream)
            .ok_or_else(|| anyhow::anyhow!("Stream index is out of range"))
    }

    /// Gets the fragments for a given stream.
    ///
    /// If `stream` is out of range, returns `None`.
    pub fn stream_fragments(&self, stream: u32) -> Option<&[Fragment]> {
        let i = stream as usize;
        if i < self.stream_fragments.len() - 1 {
            let start = self.stream_fragments[i] as usize;
            let end = self.stream_fragments[i + 1] as usize;
            let fragments = &self.fragments[start..end];
            match fragments {
                [f, ..] if f.location.is_nil() => Some(&[]),
                _ => Some(fragments),
            }
        } else {
            None
        }
    }

    /// Gets the size of a given stream, in bytes.
    ///
    /// The `stream` value must be in a valid range of `0..num_streams()`.
    ///
    /// If `stream` is a NIL stream, this function returns 0.
    pub fn stream_size(&self, stream: u32) -> Result<u64> {
        let fragments = self.stream_fragments_result(stream)?;
        Ok(fragments.iter().map(|f| f.size as u64).sum())
    }

    /// Returns `true` if `stream` is a valid stream index and the stream is non-nil.
    ///
    /// * If `stream` is 0, returns `false`.
    /// * if `stream` is greater than `num_streams()`, returns false.
    /// * If `stream` is a nil stream, this returns `false`.
    /// * Else returns `true`.
    #[allow(clippy::match_like_matches_macro)]
    pub fn is_stream_valid(&self, stream: u32) -> bool {
        assert!(!self.stream_fragments.is_empty());

        if stream == 0 {
            return false;
        }

        let i = stream as usize;
        if i < self.stream_fragments.len() - 1 {
            let start = self.stream_fragments[i] as usize;
            let end = self.stream_fragments[i + 1] as usize;
            let fragments = &self.fragments[start..end];
            match fragments {
                [f, ..] if f.location.is_nil() => false,
                _ => true,
            }
        } else {
            false
        }
    }

    /// Gets a slice of a chunk. `offset` is the offset within the chunk and `size` is the
    /// length in bytes of the slice. The chunk is loaded and decompressed, if necessary.
    fn get_chunk_slice(&self, chunk: u32, offset: u32, size: u32) -> std::io::Result<&[u8]> {
        let chunk_data = self.get_chunk_data(chunk)?;
        if let Some(slice) = chunk_data.get(offset as usize..offset as usize + size as usize) {
            Ok(slice)
        } else {
            Err(std::io::Error::new(
                std::io::ErrorKind::InvalidData,
                "PDZ file contains invalid byte ranges within a chunk",
            ))
        }
    }

    fn get_chunk_data(&self, chunk_index: u32) -> std::io::Result<&Arc<[u8]>> {
        let _span = trace_span!("get_chunk_data").entered();
        trace!(chunk_index);

        debug_assert_eq!(self.chunk_cache.len(), self.chunk_table.len());

        let Some(slot) = self.chunk_cache.get(chunk_index as usize) else {
            return Err(std::io::Error::new(
                std::io::ErrorKind::InvalidInput,
                "Chunk index is out of range.",
            ));
        };

        if let Some(arc) = slot.get() {
            trace!(chunk_index, "found chunk in cache");
            return Ok(arc);
        }

        let arc = self.load_chunk_data(chunk_index)?;
        Ok(slot.get_or_init(move || arc))
    }

    /// This is the slow path for `get_chunk_data`, which loads the chunk data from disk and
    /// decompresses it.
    #[inline(never)]
    fn load_chunk_data(&self, chunk_index: u32) -> std::io::Result<Arc<[u8]>> {
        assert_eq!(self.chunk_cache.len(), self.chunk_table.len());

        let _span = debug_span!("load_chunk_data").entered();

        // We may race with another read that is loading the same entry.
        // For now, that's OK, but in the future we should be smarter about de-duping
        // cache fill requests.

        // We have already implicitly validated the chunk index.
        let entry = &self.chunk_table[chunk_index as usize];

        let compression_opt =
            Compression::try_from_code_opt(entry.compression.get()).map_err(|_| {
                std::io::Error::new(
                    std::io::ErrorKind::Unsupported,
                    "Chunk uses an unrecognized compression algorithm",
                )
            })?;

        // Read the data from disk.
        let mut compressed_data: Box<[u8]> =
            FromZeros::new_box_zeroed_with_elems(entry.compressed_size.get() as usize)
                .map_err(|_| std::io::Error::from(std::io::ErrorKind::OutOfMemory))?;
        self.file
            .read_exact_at(&mut compressed_data, entry.file_offset.get())?;

        let uncompressed_data: Box<[u8]> = if let Some(compression) = compression_opt {
            let mut uncompressed_data: Box<[u8]> =
                FromZeros::new_box_zeroed_with_elems(entry.uncompressed_size.get() as usize)
                    .map_err(|_| std::io::Error::from(std::io::ErrorKind::OutOfMemory))?;

            self::compress_utils::decompress_to_slice(
                compression,
                &compressed_data,
                &mut uncompressed_data,
            )?;
            uncompressed_data
        } else {
            // This chunk is not compressed.
            compressed_data
        };

        // This conversion should not need to allocate memory for the buffer.  The conversion from
        // Box to Arc should allocate a new Arc object, but the backing allocation for the buffer
        // should simply be transferred.
        Ok(Arc::from(uncompressed_data))
    }

    /// Reads an entire stream to a vector.
    ///
    /// If the stream data fits entirely within a single decompressed chunk, then this function
    /// returns a slice to the data, without copying it.
    pub fn read_stream(&self, stream: u32) -> anyhow::Result<StreamData> {
        let _span = trace_span!("read_stream_to_cow").entered();
        trace!(stream);

        let mut fragments = self.stream_fragments_result(stream)?;

        match fragments.first() {
            Some(f) if f.location.is_nil() => fragments = &[],
            _ => {}
        }

        // If the stream is zero-length, then things are really simple.
        if fragments.is_empty() {
            return Ok(StreamData::empty());
        }

        // If this stream fits in a single fragment and the fragment is compressed, then we can
        // return a single borrowed reference to it. This is common, and is one of the most
        // important optimizations.
        if fragments.len() == 1 && fragments[0].location.is_compressed() {
            let chunk_index = fragments[0].location.compressed_first_chunk();
            let offset_within_chunk = fragments[0].location.compressed_offset_within_chunk();

            let chunk_data = self.get_chunk_data(chunk_index)?;
            let fragment_range = offset_within_chunk as usize
                ..offset_within_chunk as usize + fragments[0].size as usize;

            // Validate the fragment range.
            if chunk_data.get(fragment_range.clone()).is_none() {
                bail!("PDZ data is invalid. Stream fragment byte range is out of range.");
            }

            return Ok(StreamData::ArcSlice(Arc::clone(chunk_data), fragment_range));
        }

        let stream_size: u32 = fragments.iter().map(|f| f.size).sum();
        let stream_usize = stream_size as usize;

        // Allocate a buffer and copy data from each chunk.
        let mut output_buffer: Box<[u8]> = FromZeros::new_box_zeroed_with_elems(stream_usize)
            .map_err(|_| std::io::Error::from(std::io::ErrorKind::OutOfMemory))?;
        let mut output_slice: &mut [u8] = &mut output_buffer;

        for fragment in fragments.iter() {
            let stream_offset = stream_usize - output_slice.len();

            // Because we computed stream_usize by summing the fragment sizes, this
            // split_at_mut() call should not fail.
            let (fragment_output_slice, rest) = output_slice.split_at_mut(fragment.size as usize);
            output_slice = rest;

            if fragment.location.is_compressed() {
                let chunk_index = fragment.location.compressed_first_chunk();
                let offset_within_chunk = fragment.location.compressed_offset_within_chunk();

                let chunk_data = self.get_chunk_data(chunk_index)?;
                if let Some(chunk_slice) = chunk_data.get(
                    offset_within_chunk as usize
                        ..offset_within_chunk as usize + fragment.size as usize,
                ) {
                    fragment_output_slice.copy_from_slice(chunk_slice);
                } else {
                    bail!("PDZ data is invalid. Stream fragment byte range is out of range.");
                }
            } else {
                let file_offset = fragment.location.uncompressed_file_offset();
                // Read an uncompressed fragment.
                trace!(
                    file_offset,
                    stream_offset,
                    fragment_len = fragment_output_slice.len(),
                    "reading uncompressed fragment"
                );
                self.file
                    .read_exact_at(fragment_output_slice, file_offset)?;
            }
        }

        assert!(output_slice.is_empty());

        Ok(StreamData::Box(output_buffer))
    }

    /// Returns an object which can read from a given stream.  The returned object implements
    /// the [`Read`], [`Seek`], and [`ReadAt`] traits.
    ///
    /// If `stream` is out of range (greater than or equal to `num_streams()`) then this function
    /// returns an error.
    ///
    /// If `stream` is a nil stream then this function returns a `StreamReader` whose size is 0.
    pub fn get_stream_reader(&self, stream: u32) -> Result<StreamReader<'_, F>> {
        let fragments = self.stream_fragments_result(stream)?;
        Ok(StreamReader {
            msfz: self,
            size: fragments.iter().map(|f| f.size as u64).sum(),
            fragments,
            pos: 0,
        })
    }

    /// The total number of fragments in the MSFZ file.
    pub fn num_fragments(&self) -> usize {
        self.fragments.len()
    }

    /// Raw access to the Fragments table
    pub fn fragments(&self) -> &[Fragment] {
        &self.fragments
    }

    /// The total number of compressed chunks.
    pub fn num_chunks(&self) -> usize {
        self.chunk_table.len()
    }

    /// Raw access to the Chunks table
    pub fn chunks(&self) -> &[ChunkEntry] {
        &self.chunk_table
    }
}

/// Allows reading a stream using the [`Read`], [`Seek`], and [`ReadAt`] traits.
pub struct StreamReader<'a, F> {
    msfz: &'a Msfz<F>,
    size: u64,
    fragments: &'a [Fragment],
    pos: u64,
}

impl<'a, F> StreamReader<'a, F> {
    /// Returns `true` if this is a zero-length stream or a nil stream.
    pub fn is_empty(&self) -> bool {
        self.stream_size() == 0
    }

    /// Size in bytes of the stream.
    ///
    /// This returns zero for nil streams.
    pub fn stream_size(&self) -> u64 {
        self.size
    }
}

impl<'a, F: ReadAt> ReadAt for StreamReader<'a, F> {
    fn read_at(&self, mut buf: &mut [u8], offset: u64) -> std::io::Result<usize> {
        if buf.is_empty() {
            return Ok(0);
        }

        let original_buf_len = buf.len();
        let mut current_offset: u64 = offset;

        for fragment in self.fragments.iter() {
            debug_assert!(!buf.is_empty());

            if current_offset >= fragment.size as u64 {
                current_offset -= fragment.size as u64;
                continue;
            }

            // Because of the range check above, we know that this cannot overflow.
            let fragment_bytes_available = fragment.size - current_offset as u32;

            let num_bytes_xfer = buf.len().min(fragment_bytes_available as usize);
            let (buf_xfer, buf_rest) = buf.split_at_mut(num_bytes_xfer);
            buf = buf_rest;

            if fragment.location.is_compressed() {
                let chunk_index = fragment.location.compressed_first_chunk();
                let offset_within_chunk = fragment.location.compressed_offset_within_chunk();

                let chunk_slice = self.msfz.get_chunk_slice(
                    chunk_index,
                    offset_within_chunk + current_offset as u32,
                    num_bytes_xfer as u32,
                )?;
                buf_xfer.copy_from_slice(chunk_slice);
            } else {
                // Read the stream data directly from disk.  We don't validate the file offset or
                // the length of the transfer. Instead, we just allow the underlying file system
                // to report errors.
                let file_offset = fragment.location.uncompressed_file_offset();
                self.msfz
                    .file
                    .read_exact_at(buf_xfer, file_offset + current_offset)?;
            }

            if buf.is_empty() {
                break;
            }

            // Since we have finished reading from one chunk and there is more data to read in the
            // next chunk, the "offset within chunk" becomes zero.
            current_offset = 0;
        }

        Ok(original_buf_len - buf.len())
    }
}

impl<'a, F: ReadAt> Read for StreamReader<'a, F> {
    fn read(&mut self, buf: &mut [u8]) -> std::io::Result<usize> {
        let n = self.read_at(buf, self.pos)?;
        self.pos += n as u64;
        Ok(n)
    }
}

impl<'a, F> Seek for StreamReader<'a, F> {
    fn seek(&mut self, pos: SeekFrom) -> std::io::Result<u64> {
        match pos {
            SeekFrom::Start(p) => self.pos = p,
            SeekFrom::End(offset) => {
                let new_pos = self.stream_size() as i64 + offset;
                if new_pos < 0 {
                    return Err(std::io::ErrorKind::InvalidInput.into());
                }
                self.pos = new_pos as u64;
            }
            SeekFrom::Current(offset) => {
                let new_pos = self.pos as i64 + offset;
                if new_pos < 0 {
                    return Err(std::io::ErrorKind::InvalidInput.into());
                }
                self.pos = new_pos as u64;
            }
        }
        Ok(self.pos)
    }
}

struct DecodedStreamDir {
    fragments: Vec<Fragment>,
    stream_fragments: Vec<u32>,
}

fn decode_stream_dir(
    stream_dir_bytes: &[u8],
    num_streams: u32,
    chunk_table: &[ChunkEntry],
) -> anyhow::Result<DecodedStreamDir> {
    let mut dec = Decoder {
        bytes: stream_dir_bytes,
    };

    let mut fragments: Vec<Fragment> = Vec::new();
    let mut stream_fragments: Vec<u32> = Vec::with_capacity(num_streams as usize + 1);

    for _ in 0..num_streams {
        stream_fragments.push(fragments.len() as u32);

        let mut fragment_size = dec.u32()?;

        if fragment_size == NIL_STREAM_SIZE {
            // Nil stream. We synthesize a fake fragment record so that we can distinguish
            // nil streams and non-nil streams, and yet optimize for the case where nearly all
            // streams are non-nil.
            fragments.push(Fragment {
                size: 0,
                location: FragmentLocation::NIL,
            });
            continue;
        }

        while fragment_size != 0 {
            debug_assert_ne!(fragment_size, NIL_STREAM_SIZE);

            let location_lo = dec.u32()?;
            let location_hi = dec.u32()?;

            if location_lo == u32::MAX && location_hi == u32::MAX {
                bail!("The Stream Directory contains an invalid fragment record.");
            }

            let location = FragmentLocation {
                lo: location_lo,
                hi: location_hi,
            };

            if location.is_compressed() {
                let first_chunk = location.compressed_first_chunk();
                let offset_within_chunk = location.compressed_offset_within_chunk();

                let Some(chunk) = chunk_table.get(first_chunk as usize) else {
                    bail!(
                        "The Stream Directory contains an invalid fragment record. Chunk index {first_chunk} exceeds the size of the chunk table."
                    );
                };

                let uncompressed_chunk_size = chunk.uncompressed_size.get();

                // Testing for greater-than-or-equal instead of greater-than is correct. Fragments
                // always have a size that is non-zero, so at least one byte must come from the
                // first chunk identified by a compressed fragment.
                if offset_within_chunk >= uncompressed_chunk_size {
                    bail!(
                        "The Stream Directory contains an invalid fragment record. offset_within_chunk {offset_within_chunk} exceeds the size of the chunk."
                    );
                };

                // We could go further and validate that the current fragment extends beyond a
                // valid number of chunks. The stream reader code handles that, though.
            } else {
                // We could validate that the uncompressed fragment lies entirely within the MSFZ
                // file, if we knew the length of the file. Unfortunately, ReadAt does not provide
                // the length of the file, so we will not validate the fragment here. If the
                // fragment is invalid it will cause a read failure within the StreamReader,
                // which will be propagated to the application.
            }

            fragments.push(Fragment {
                size: fragment_size,
                location,
            });

            // Read the fragment size for the next fragment. A value of zero terminates the list,
            // which is handled at the start of the while loop.
            fragment_size = dec.u32()?;
            if fragment_size == NIL_STREAM_SIZE {
                bail!(
                    "Stream directory is malformed. It contains a non-initial fragment with size = NIL_STREAM_SIZE."
                );
            }
            // continue for more
        }
    }

    stream_fragments.push(fragments.len() as u32);

    fragments.shrink_to_fit();

    Ok(DecodedStreamDir {
        fragments,
        stream_fragments,
    })
}

struct Decoder<'a> {
    bytes: &'a [u8],
}

impl<'a> Decoder<'a> {
    fn next_n<const N: usize>(&mut self) -> anyhow::Result<&'a [u8; N]> {
        if self.bytes.len() < N {
            bail!("Buffer ran out of bytes");
        }

        let (lo, hi) = self.bytes.split_at(N);
        self.bytes = hi;
        // This unwrap() should never fail because we just tested the length, above.
        // The optimizer should eliminate the unwrap() call.
        Ok(<&[u8; N]>::try_from(lo).unwrap())
    }

    fn u32(&mut self) -> anyhow::Result<u32> {
        Ok(u32::from_le_bytes(*self.next_n()?))
    }
}

fn map_alloc_error<T>(result: Result<T, zerocopy::AllocError>) -> anyhow::Result<T> {
    match result {
        Ok(value) => Ok(value),
        Err(zerocopy::AllocError) => {
            Err(std::io::Error::from(std::io::ErrorKind::OutOfMemory).into())
        }
    }
}

```

`msfz/src/stream_data.rs`:

```rs
use core::ops::Range;
use std::sync::Arc;

use zerocopy::FromZeros;

#[cfg(doc)]
use crate::Msfz;

/// Contains the contents of an entire stream.
///
/// This is used as the return type for [`Msfz::read_stream`] function. This type either contains
/// an owned buffer (`Vec`) or a counted reference to a slice of an `Arc<[u8]>`.
///
/// See the `[Msfz::read_stream]` function for more details.
pub enum StreamData {
    /// Owned contents of stream data
    Box(Box<[u8]>),
    /// Shared contents of stream data.  The `Range` gives the range of bytes within the `Arc`.
    ArcSlice(Arc<[u8]>, Range<usize>),
}

impl StreamData {
    /// Gets a slice over the contained stream data.
    #[inline(always)]
    pub fn as_slice(&self) -> &[u8] {
        match self {
            Self::Box(v) => v,
            Self::ArcSlice(arc, range) => &arc[range.clone()],
        }
    }

    /// Returns `true` if the stream contains no data.
    pub fn is_empty(&self) -> bool {
        self.as_slice().is_empty()
    }

    /// Converts this `StreamData` into an owned `Vec<u8>`.
    pub fn into_vec(self) -> Vec<u8> {
        self.into_boxed().into()
    }

    /// Converts this `StreamData` into an owned `Box<[u8]>`.
    pub fn into_boxed(self) -> Box<[u8]> {
        match self {
            Self::Box(b) => b,
            Self::ArcSlice(arc, range) => {
                let mut b: Box<[u8]> = FromZeros::new_box_zeroed_with_elems(range.len()).unwrap();
                b.copy_from_slice(&arc[range]);
                b
            }
        }
    }
}

impl From<StreamData> for Box<[u8]> {
    fn from(s: StreamData) -> Self {
        s.into_boxed()
    }
}

impl core::ops::Deref for StreamData {
    type Target = [u8];

    #[inline(always)]
    fn deref(&self) -> &[u8] {
        self.as_slice()
    }
}

impl AsRef<[u8]> for StreamData {
    #[inline(always)]
    fn as_ref(&self) -> &[u8] {
        self.as_slice()
    }
}

impl Default for StreamData {
    fn default() -> Self {
        Self::empty()
    }
}

impl StreamData {
    /// An empty value for `StreamData`
    pub fn empty() -> Self {
        Self::Box(Box::from(&[] as &[u8]))
    }
}

```

`msfz/src/tests.rs`:

```rs
#![allow(clippy::format_collect)]

use super::*;
use bstr::BStr;
use pow2::Pow2;
use std::io::{Cursor, Read, Seek, SeekFrom, Write};
use sync_file::ReadAt;
use tracing::{debug_span, info, info_span, instrument};

#[static_init::dynamic(drop)]
static mut INIT_LOGGER: Option<tracy_client::Client> = {
    use tracing_subscriber::fmt::format::FmtSpan;
    use tracing_subscriber::layer::SubscriberExt;

    if let Ok(s) = std::env::var("ENABLE_TRACY") {
        if s == "1" {
            let client = tracy_client::Client::start();

            eprintln!("Enabling Tracy");
            tracing::subscriber::set_global_default(
                tracing_subscriber::registry().with(tracing_tracy::TracyLayer::default()),
            )
            .expect("setup tracy layer");
            return Some(client);
        }
    }

    tracing_subscriber::fmt::fmt()
        .compact()
        .with_max_level(tracing_subscriber::filter::LevelFilter::TRACE)
        .with_level(false)
        .with_file(true)
        .with_line_number(true)
        .with_span_events(FmtSpan::ENTER | FmtSpan::EXIT)
        .with_test_writer()
        .without_time()
        .with_ansi(false)
        .init();

    None
};

#[track_caller]
fn make_msfz<F>(f: F) -> Msfz<Vec<u8>>
where
    F: FnOnce(&mut MsfzWriter<Cursor<Vec<u8>>>),
{
    let _span = info_span!("make_msfz").entered();

    let cursor: Cursor<Vec<u8>> = Cursor::new(Vec::new());
    let mut w = MsfzWriter::new(cursor).unwrap();

    f(&mut w);

    info!("Streams:");
    for (i, stream_opt) in w.streams.iter().enumerate() {
        if let Some(stream) = stream_opt {
            info!(stream_index = i, fragments = ?stream.fragments);
        } else {
            info!(stream_index = i, "nil stream");
        }
    }

    info!("Encoded stream directory:");
    let stream_dir_bytes = encode_stream_dir(&w.streams);
    info!(stream_dir_bytes = stream_dir_bytes.as_slice());

    let (summary, returned_file) = w.finish().unwrap();
    info!(%summary);
    let msfz_data = returned_file.into_inner();

    info!(msfz_data = msfz_data.as_slice());

    match Msfz::from_file(msfz_data) {
        Ok(msfz) => msfz,
        Err(e) => {
            panic!("Failed to decode MSFZ file during test: {e:?}");
        }
    }
}

#[test]
#[instrument]
fn basic_compressed() {
    let r = make_msfz(|w| {
        w.reserve_num_streams(10);

        // write streams out of order
        w.stream_writer(4)
            .unwrap()
            .write_all(b"Hello, world!")
            .unwrap();
        w.stream_writer(1)
            .unwrap()
            .write_all(b"Friends, Romans, yadda yadda")
            .unwrap();

        // Write a "large" stream to stream 2
        let mut big_stream: Vec<u8> = vec![0; 0x1_0000];
        let mut big = Cursor::new(&mut big_stream);
        big.seek(SeekFrom::Start(0x1000)).unwrap();
        big.write_all(b"way out in the hinterlands").unwrap();
        big.into_inner();
        w.stream_writer(2).unwrap().write_all(&big_stream).unwrap();
    });

    // Now read it back.

    assert_eq!(r.num_streams(), 10);

    assert!(!r.is_stream_valid(0)); // stream directory; reserved
    assert!(r.is_stream_valid(1)); // we wrote to it
    assert!(r.is_stream_valid(2)); // we wrote to it
    assert!(!r.is_stream_valid(3));
    assert!(r.is_stream_valid(4)); // we wrote to it
    assert!(!r.is_stream_valid(5));
    assert!(!r.is_stream_valid(6));
    assert!(!r.is_stream_valid(7));
    assert!(!r.is_stream_valid(8));
    assert!(!r.is_stream_valid(9));

    assert_eq!(r.stream_size(0).unwrap(), 0); // stream dir stream should always be zero-length

    {
        let stream1 = r.read_stream(1).unwrap();
        assert_eq!(
            BStr::new(&stream1),
            BStr::new(b"Friends, Romans, yadda yadda")
        );
    }

    {
        let stream4 = r.read_stream(4).unwrap();
        assert_eq!(BStr::new(&stream4), BStr::new(b"Hello, world!"));
    }

    {
        // test Seek + Read
        let msg = b"way out in the hinterlands";
        let mut buf: Vec<u8> = vec![0; msg.len()];
        let mut s = r.get_stream_reader(2).unwrap();
        s.seek(SeekFrom::Start(0x1000)).unwrap();
        s.read_exact(buf.as_mut_slice()).unwrap();
        assert_eq!(BStr::new(&buf), BStr::new(msg));

        // test ReadAt
        buf.clear();
        buf.resize(msg.len(), 0);
        s.read_at(buf.as_mut_slice(), 0x1000).unwrap();
        assert_eq!(BStr::new(&buf), BStr::new(msg));
    }

    // Verify that reading a nil stream works.
    assert_eq!(r.stream_size(3).unwrap(), 0); // check that nil stream is zero-length
    assert!(r.read_stream(3).unwrap().is_empty());
    let mut sr = r.get_stream_reader(3).unwrap();
    assert_eq!(seek_read_span(&mut sr, 0, 0).unwrap(), &[]);
    assert_eq!(read_span_at(&sr, 0, 0).unwrap(), &[]);
}

/// Test the code for crossing chunk boundaries.
#[test]
#[instrument]
fn multi_chunks() {
    let r = make_msfz(|w| {
        w.reserve_num_streams(2);

        let mut sw = w.stream_writer(1).unwrap();
        sw.write_all(b"alpha ").unwrap(); // 0..6
        sw.end_chunk().unwrap();
        sw.write_all(b"bravo ").unwrap(); // 6..12
        sw.end_chunk().unwrap();
        sw.write_all(b"charlie").unwrap(); // 12..19
    });

    assert_eq!(r.num_streams(), 2);
    assert!(!r.is_stream_valid(0)); // stream directory; reserved
    assert!(r.is_stream_valid(1)); // we wrote to it
    assert_eq!(r.stream_size(1).unwrap(), 19);

    // Verify that reading the entire stream works correctly.
    assert_eq!(
        BStr::new(&r.read_stream(1).unwrap()),
        BStr::new(b"alpha bravo charlie")
    );

    // Verify that Read works correctly at various offsets.

    let mut sr = r.get_stream_reader(1).unwrap();

    let cases: &[(u64, usize, &[u8])] = &[
        (0, 4, b"alph"),          // within a chunk
        (0, 6, b"alpha "),        // complete chunk
        (0, 8, b"alpha br"),      // spans 2 chunks
        (1, 3, b"lph"),           // within a single chunk
        (1, 6, b"lpha b"),        // spans 2 chunks
        (0, 12, b"alpha bravo "), // exactly 2 chunks
    ];

    for &(offset, len, expected) in cases.iter() {
        let data = seek_read_span(&mut sr, offset, len).unwrap();
        assert_eq!(
            BStr::new(&data),
            BStr::new(expected),
            "offset: {offset}, len: {len}"
        );

        let data_at = read_span_at(&sr, offset, len).unwrap();
        assert_eq!(
            BStr::new(&data_at),
            BStr::new(expected),
            "offset: {offset}, len: {len}"
        );
    }
}

#[test]
#[instrument]
fn basic_uncompressed() {
    let big_text: String = (0..100)
        .map(|i| format!("This should compress quite well #{i}\n"))
        .collect();

    let r = make_msfz(|w| {
        w.reserve_num_streams(10);

        // Write a compressed stream.
        let mut sw = w.stream_writer(1).unwrap();
        sw.set_compression_enabled(false);
        sw.write_all(big_text.as_bytes()).unwrap();

        // Write an uncompressed stream.
        let mut sw = w.stream_writer(2).unwrap();
        sw.set_compression_enabled(false);
        sw.write_all(b"This text should not be compressed.")
            .unwrap();
    });

    let mut sr = r.get_stream_reader(1).unwrap();
    check_read_ranges(
        &mut sr,
        big_text.as_bytes(),
        &[
            (0, 0),
            (0, 50),
            (0, 100),
            (100, 50),
            (100, 50),
            (1000, 10),
            // keep on multiple lines
        ],
    );
}

#[test]
#[instrument]
fn uncompressed_stream_alignment() {
    let r = make_msfz(|w| {
        w.reserve_num_streams(5);

        assert_eq!(w.file.out.stream_position().unwrap() & 0xf, 0);

        // Write 3 bytes with no alignment requirement.
        let mut sw = w.stream_writer(1).unwrap();
        sw.set_compression_enabled(false);
        sw.set_alignment(Pow2::from_exponent(0));
        sw.write_all(b"alpha").unwrap();

        assert_eq!(w.file.out.stream_position().unwrap() & 0xf, 5);

        let mut sw = w.stream_writer(2).unwrap();
        sw.set_compression_enabled(false);
        sw.set_alignment(Pow2::from_exponent(16));
        sw.write_all(b"zzzz").unwrap();

        assert_eq!(w.file.out.stream_position().unwrap() & 0xf, 4);
    });

    drop(r);
}

#[test]
#[instrument]
fn interleaving() {
    // variant specifies bits for whether compression is enabled for various pieces.
    for variant in 0u64..16u64 {
        let vbit = |i: u32| variant & (1u64 << i) != 0;

        let r = make_msfz(|w| {
            w.reserve_num_streams(5);

            let mut sw = w.stream_writer(1).unwrap();
            sw.set_compression_enabled(vbit(0));
            sw.write_all(b"Hello, world!\n").unwrap();

            let mut sw = w.stream_writer(2).unwrap();
            sw.set_compression_enabled(vbit(1));
            sw.write_all(
                b"
The universe (which others call the Library) is composed of an indefinite,
perhaps infinite number of hexagonal galleries. In the center of each gallery is a ventillation
shaft, bounded by a low railing.
",
            )
            .unwrap();

            let mut sw = w.stream_writer(1).unwrap();
            sw.set_compression_enabled(vbit(2));
            sw.write_all(b"Goodbye, world!\n").unwrap();

            let mut sw = w.stream_writer(2).unwrap();
            sw.set_compression_enabled(vbit(3));
            sw.write_all(
                b"
From any hexagon one can see the floors above and below -- one after another, endlessly.
The arrangement of the galleries is always the same: Twenty bookshelves, five to each side,
line four of the hexagon's six sides; the height of the bookshelves, floor to ceiling, is
hardly greater than the height of a normal librarian.
",
            )
            .unwrap();

            let mut sw = w.stream_writer(1).unwrap();
            sw.write_all(b"Hello, again!\n").unwrap();

            let mut sw = w.stream_writer(2).unwrap();
            sw.write_all(
                b"
One of the hexagon's free sides opens onto a narrow sort of vestibule, which in turn opens onto
another gallery, identical to the first -- identical in fact to all.",
            )
            .unwrap();
        });

        let s1 = r.read_stream(1).unwrap();
        assert_eq!(
            std::str::from_utf8(&s1).unwrap(),
            "Hello, world!\n\
             Goodbye, world!\n\
             Hello, again!\n"
        );

        let s2 = r.read_stream(2).unwrap();
        assert_eq!(
            std::str::from_utf8(&s2).unwrap(),
            "
The universe (which others call the Library) is composed of an indefinite,
perhaps infinite number of hexagonal galleries. In the center of each gallery is a ventillation
shaft, bounded by a low railing.

From any hexagon one can see the floors above and below -- one after another, endlessly.
The arrangement of the galleries is always the same: Twenty bookshelves, five to each side,
line four of the hexagon's six sides; the height of the bookshelves, floor to ceiling, is
hardly greater than the height of a normal librarian.

One of the hexagon's free sides opens onto a narrow sort of vestibule, which in turn opens onto
another gallery, identical to the first -- identical in fact to all."
        );
    }
}

fn check_read_ranges<F: ReadAt>(
    sr: &mut StreamReader<'_, F>,
    known_good_data: &[u8],
    ranges: &[(u64, usize)],
) {
    let _span = debug_span!("check_read_ranges").entered();

    for &(offset, len) in ranges.iter() {
        let expected = &known_good_data[offset as usize..offset as usize + len];

        let data = seek_read_span(sr, offset, len).unwrap();
        assert_eq!(
            BStr::new(&data),
            BStr::new(expected),
            "offset: {offset}, len: {len}"
        );

        let data_at = read_span_at(sr, offset, len).unwrap();
        assert_eq!(
            BStr::new(&data_at),
            BStr::new(expected),
            "offset: {offset}, len: {len}"
        );
    }
}

fn seek_read_span<R: Read + Seek>(r: &mut R, offset: u64, len: usize) -> Result<Vec<u8>> {
    let mut buf = vec![0; len];
    r.seek(SeekFrom::Start(offset))?;
    r.read_exact(buf.as_mut_slice())?;
    Ok(buf)
}

fn read_span_at<R: ReadAt>(r: &R, offset: u64, len: usize) -> Result<Vec<u8>> {
    let mut buf = vec![0; len];
    r.read_at(buf.as_mut_slice(), offset)?;
    Ok(buf)
}

```

`msfz/src/writer.rs`:

```rs
use super::*;
use anyhow::anyhow;
use pow2::Pow2;
use std::fs::File;
use std::io::{Seek, SeekFrom, Write};
use std::path::Path;
use tracing::{debug, debug_span, error, trace, trace_span};
use zerocopy::IntoBytes;

/// The default threshold for compressing a chunk of data.
pub const DEFAULT_CHUNK_THRESHOLD: u32 = 0x10_0000; // 1 MiB

/// The minimum value for the uncompressed chunk size threshold.
pub const MIN_CHUNK_SIZE: u32 = 0x1000;

/// The maximum value for the uncompressed chunk size threshold.
pub const MAX_CHUNK_SIZE: u32 = 1 << 30;

/// Allows writing a new MSFZ file.
pub struct MsfzWriter<F: Write + Seek = File> {
    pub(crate) file: MsfzWriterFile<F>,

    /// The list of streams. This includes nil streams and non-nil streams. Nil streams are
    /// represented with `None`.
    pub(crate) streams: Vec<Option<Stream>>,
}

pub(crate) struct MsfzWriterFile<F: Write + Seek> {
    /// Max number of bytes to write into `uncompressed_chunk_data` before finishing (compressing
    /// and writing to disk) a chunk.
    uncompressed_chunk_size_threshold: u32,

    /// Holds data for the current chunk that we are building.
    uncompressed_chunk_data: Vec<u8>,

    /// A reusable buffer used for compressing the current chunk. This exists only to reduce
    /// memory allocation churn.
    compressed_chunk_buffer: Vec<u8>,
    /// The list of complete compressed chunks that have been written to disk.
    chunks: Vec<ChunkEntry>,

    /// Compression mode to use for the next chunk.
    chunk_compression_mode: Compression,

    /// The output file.
    pub(crate) out: F,
}

impl std::fmt::Debug for FragmentLocation {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Self::Uncompressed { file_offset } => {
                write!(f, "uncompressed at 0x{file_offset:06x}")
            }
            Self::Compressed {
                chunk_index,
                offset_within_chunk,
            } => write!(f, "chunk {chunk_index} : 0x{offset_within_chunk:04x}"),
        }
    }
}

// Describes a region within a stream.
#[derive(Clone, Debug)]
pub(crate) struct Fragment {
    pub(crate) size: u32,
    pub(crate) location: FragmentLocation,
}

#[derive(Default)]
pub(crate) struct Stream {
    pub(crate) fragments: Vec<Fragment>,
}

const FRAGMENT_LOCATION_CHUNK_BIT: u32 = 63;
const FRAGMENT_LOCATION_CHUNK_MASK: u64 = 1 << FRAGMENT_LOCATION_CHUNK_BIT;

#[derive(Clone)]
pub(crate) enum FragmentLocation {
    Uncompressed {
        file_offset: u64,
    },
    Compressed {
        chunk_index: u32,
        offset_within_chunk: u32,
    },
}

/// Describes the results of writing an MSFZ file.
#[non_exhaustive]
pub struct Summary {
    /// Number of chunks
    pub num_chunks: u32,
    /// Number of streams
    pub num_streams: u32,
}

impl std::fmt::Display for Summary {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        writeln!(f, "Number of chunks: {}", self.num_chunks)?;
        writeln!(f, "Number of streams: {}", self.num_streams)?;
        Ok(())
    }
}

impl MsfzWriter<File> {
    /// Creates a new writer on a file at a given path.
    ///
    /// This will *truncate* any existing file.
    pub fn create(file_name: &Path) -> Result<Self> {
        let f = open_options_exclusive(File::options().write(true).create(true).truncate(true))
            .open(file_name)?;
        Self::new(f)
    }
}

impl<F: Write + Seek> MsfzWriter<F> {
    /// Creates a new writer on an object that implements [`Write`] (and [`Seek`]), such as
    /// [`File`].
    pub fn new(mut file: F) -> Result<Self> {
        let _span = trace_span!("MsfzWriter::new").entered();

        file.seek(SeekFrom::Start(0))?;

        // Write a meaningless (zero-filled) file header, just so we get the file position that we
        // want.  We will re-write this header at the end.
        let fake_file_header = MsfzFileHeader::new_zeroed();
        file.write_all(fake_file_header.as_bytes())?;

        // We do not know how many streams the writer will write. We reserve a small fixed number,
        // just guessing at the size.
        let mut streams = Vec::with_capacity(0x40);

        // Reserve stream 0 for the stream directory. The stream is zero-length.
        // Setting the size to 0 also prevents this stream from being written, which is good.
        streams.push(Some(Stream {
            fragments: Vec::new(),
        }));

        let mut this = Self {
            streams,
            file: MsfzWriterFile {
                uncompressed_chunk_size_threshold: DEFAULT_CHUNK_THRESHOLD,
                uncompressed_chunk_data: Vec::with_capacity(DEFAULT_CHUNK_THRESHOLD as usize),
                compressed_chunk_buffer: Vec::new(),
                out: file,
                chunks: Vec::new(),
                chunk_compression_mode: Compression::Zstd,
            },
        };
        this.file.write_align(Pow2::from_exponent(4))?;
        Ok(this)
    }

    /// Sets the compression mode that is used for chunked compression.
    pub fn set_chunk_compression_mode(&mut self, compression: Compression) {
        // If the current chunk buffer contains data, then leave it there. It will be compressed
        // with the new algorithm.
        self.file.chunk_compression_mode = compression;
    }

    /// Sets the maximum uncompressed size for each chunk.
    ///
    /// This is an optimization hint. The implementation will do its best to keep chunks below this
    /// size, but there are cases where the chunk has already exceeded the specified size.
    ///
    /// The value is clamped to `MIN_CHUNK_SIZE..=MAX_CHUNK_SIZE`.
    pub fn set_uncompressed_chunk_size_threshold(&mut self, value: u32) {
        self.file.uncompressed_chunk_size_threshold = value.clamp(MIN_CHUNK_SIZE, MAX_CHUNK_SIZE);
    }

    /// Gets the maximum uncompressed size for each chunk.
    pub fn uncompressed_chunk_size_threshold(&self) -> u32 {
        self.file.uncompressed_chunk_size_threshold
    }

    /// Reserves `num_streams` streams.
    ///
    /// If `num_streams` is less than or equal to the current number of streams, then this
    /// function has no effect.
    ///
    /// If `num_streams` is greater than the current number of streams, then new "nil" streams are
    /// added to the stream directory. These streams can be written by using the `stream_writer`
    /// function. The `stream_writer` function can only be called once for each stream index.
    pub fn reserve_num_streams(&mut self, num_streams: usize) {
        if num_streams <= self.streams.len() {
            return;
        }

        self.streams.resize_with(num_streams, Option::default);
    }

    /// Ends the current chunk, if any.
    ///
    /// This function is a performance hint for compression. It is not necessary to call this
    /// function. If you are writing two different streams that have very different contents, then
    /// it may be beneficial to put the streams into different compression chunks. This allows
    /// the compressor to adapt to the different contents of each stream.
    pub fn end_chunk(&mut self) -> std::io::Result<()> {
        self.file.finish_current_chunk()
    }

    /// Writes an existing stream.
    ///
    /// This function can only be called once for each stream index. Calling it more than once
    /// for the same stream is permitted. Note that settings on [`StreamWriter`] do not persist
    /// across multiple calls to `stream_writer()`, such as enabling/disabling chunked compression.
    pub fn stream_writer(&mut self, stream: u32) -> std::io::Result<StreamWriter<'_, F>> {
        assert!((stream as usize) < self.streams.len());

        Ok(StreamWriter {
            file: &mut self.file,
            stream: self.streams[stream as usize].get_or_insert_with(Stream::default),
            chunked_compression_enabled: true,
            alignment: Pow2::from_exponent(2), // default is 4-byte alignment
        })
    }

    /// Creates a new stream and returns a [`StreamWriter`] for it.
    pub fn new_stream_writer(&mut self) -> Result<(u32, StreamWriter<'_, F>)> {
        let stream = self.streams.len() as u32;
        self.streams.push(Some(Stream::default()));
        let w = self.stream_writer(stream)?;
        Ok((stream, w))
    }

    /// Finishes writing the MSFZ file.
    ///
    /// This writes the Stream Directory, the Chunk Table, and then writes the MSFZ file header.
    /// It then returns the inner file object. However, the caller should not write more data to
    /// the returned file object.
    pub fn finish(self) -> Result<(Summary, F)> {
        self.finish_with_options(MsfzFinishOptions::default())
    }

    /// Finishes writing the MSFZ file.
    ///
    /// This writes the Stream Directory, the Chunk Table, and then writes the MSFZ file header.
    /// It then returns the inner file object. However, the caller should not write more data to
    /// the returned file object.
    ///
    /// This function also allows the caller to pass `MsfzFinishOptions`.
    pub fn finish_with_options(mut self, options: MsfzFinishOptions) -> Result<(Summary, F)> {
        let _span = debug_span!("MsfzWriter::finish").entered();

        self.file.finish_current_chunk()?;

        // Write the stream directory, and optionally compress it.
        let directory_offset = self.file.write_align(Pow2::from_exponent(4))?;

        let stream_dir_bytes: Vec<u8> = encode_stream_dir(&self.streams);
        let stream_dir_size_uncompressed = u32::try_from(stream_dir_bytes.len())
            .map_err(|_| anyhow!("The stream directory is too large."))?;
        let stream_dir_size_compressed: u32;
        let stream_dir_compression: u32;
        if let Some(compression) = options.stream_dir_compression {
            stream_dir_compression = compression.to_code();
            let stream_dir_compressed_bytes =
                crate::compress_utils::compress_to_vec(compression, &stream_dir_bytes)?;
            stream_dir_size_compressed = stream_dir_compressed_bytes.len() as u32;
            self.file.out.write_all(&stream_dir_compressed_bytes)?;
        } else {
            self.file.out.write_all(&stream_dir_bytes)?;
            stream_dir_size_compressed = stream_dir_size_uncompressed;
            stream_dir_compression = COMPRESSION_NONE;
        }

        // Write the chunk list.
        let chunk_table_offset = self.file.write_align(Pow2::from_exponent(4))?;
        let chunk_table_bytes = self.file.chunks.as_bytes();
        let chunk_table_size = u32::try_from(chunk_table_bytes.len())
            .map_err(|_| anyhow!("The chunk index is too large."))?;
        self.file.out.write_all(chunk_table_bytes)?;

        // Rewind and write the real file header.
        let file_header = MsfzFileHeader {
            signature: MSFZ_FILE_SIGNATURE,
            version: U64::new(MSFZ_FILE_VERSION_V0),
            num_streams: U32::new(self.streams.len() as u32),
            stream_dir_compression: U32::new(stream_dir_compression),
            stream_dir_offset: U64::new(directory_offset),
            stream_dir_size_compressed: U32::new(stream_dir_size_compressed),
            stream_dir_size_uncompressed: U32::new(stream_dir_size_uncompressed),
            num_chunks: U32::new(self.file.chunks.len() as u32),
            chunk_table_size: U32::new(chunk_table_size),
            chunk_table_offset: U64::new(chunk_table_offset),
        };
        self.file.out.seek(SeekFrom::Start(0))?;
        self.file.out.write_all(file_header.as_bytes())?;

        if options.min_file_size != 0 {
            let file_length = self.file.out.seek(SeekFrom::End(0))?;
            if file_length < options.min_file_size {
                debug!(
                    file_length,
                    options.min_file_size, "Extending file to meet minimum length requirement"
                );
                // Write a single byte at the end of the file. We do this because there is no
                // way to set the stream length without writing some bytes.
                self.file
                    .out
                    .seek(SeekFrom::Start(options.min_file_size - 1))?;
                self.file.out.write_all(&[0u8])?;
            }
        }

        let summary = Summary {
            num_chunks: self.file.chunks.len() as u32,
            num_streams: self.streams.len() as u32,
        };

        Ok((summary, self.file.out))
    }
}

/// Handles packing and unpacking the `file_offset` for compressed streams.
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
struct ChunkAndOffset {
    chunk: u32,
    offset: u32,
}

impl<F: Write + Seek> MsfzWriterFile<F> {
    /// Writes `data` to the compressed chunk stream and returns the location of the start of the
    /// data.
    ///
    /// This function does its best to keep chunks below the `uncompressed_chunk_size_threshold`.
    /// If `data.len()` would cause the current chunk to overflow the threshold, then this function
    /// finishes the current chunk and starts a new one.
    ///
    /// If `data.len()` itself is larger than `uncompressed_chunk_size_threshold`, then this
    /// function will write a chunk that is larger than `uncompressed_chunk_size_threshold`.
    /// This is common for very large data streams, such as the TPI or GSS.  Writers that want to
    /// avoid encoding very large chunks will need to break up the data and call
    /// `write_to_chunks()` repeatedly.
    ///
    /// All of the bytes of `data` will be written to a single chunk; this function never splits
    /// the data across multiple chunks.
    fn write_to_chunks(&mut self, data: &[u8]) -> std::io::Result<ChunkAndOffset> {
        let _span = debug_span!("write_to_chunks").entered();

        if data.len() + self.uncompressed_chunk_data.len()
            >= self.uncompressed_chunk_size_threshold as usize
        {
            self.finish_current_chunk()?;
        }

        // There is no guarantee that the input data fits below our threshold, of course. If we
        // receive a buffer whose size exceeds our threshold, we'll just write a larger-than-usual
        // chunk. That's ok, everything should still work.

        let chunk = self.chunks.len() as u32;
        let offset_within_chunk = self.uncompressed_chunk_data.len();

        self.uncompressed_chunk_data.extend_from_slice(data);
        Ok(ChunkAndOffset {
            chunk,
            offset: offset_within_chunk as u32,
        })
    }

    fn bytes_available_in_chunk_buffer(&self) -> usize {
        (self.uncompressed_chunk_size_threshold as usize)
            .saturating_sub(self.uncompressed_chunk_data.len())
    }

    #[inline(never)]
    fn finish_current_chunk(&mut self) -> std::io::Result<()> {
        let _span = debug_span!("finish_current_chunk").entered();

        if self.uncompressed_chunk_data.is_empty() {
            return Ok(());
        }

        let _span = trace_span!("MsfzWriter::finish_current_chunk").entered();

        {
            let _span = trace_span!("compress chunk").entered();
            self.compressed_chunk_buffer.clear();
            crate::compress_utils::compress_to_vec_mut(
                self.chunk_compression_mode,
                &self.uncompressed_chunk_data,
                &mut self.compressed_chunk_buffer,
            )?;
        }

        let file_pos;
        {
            let _span = trace_span!("write to disk").entered();
            file_pos = self.out.stream_position()?;
            self.out.write_all(&self.compressed_chunk_buffer)?;
        }

        trace!(
            file_pos,
            compressed_size = self.compressed_chunk_buffer.len(),
            uncompressed_size = self.uncompressed_chunk_data.len()
        );

        self.chunks.push(ChunkEntry {
            compressed_size: U32::new(self.compressed_chunk_buffer.len() as u32),
            uncompressed_size: U32::new(self.uncompressed_chunk_data.len() as u32),
            file_offset: U64::new(file_pos),
            compression: U32::new(self.chunk_compression_mode.to_code()),
        });

        self.uncompressed_chunk_data.clear();
        self.compressed_chunk_buffer.clear();

        Ok(())
    }

    /// Ensures that the current stream write position on the output file is aligned to a multiple
    /// of the given alignment.
    fn write_align(&mut self, alignment: Pow2) -> std::io::Result<u64> {
        let pos = self.out.stream_position()?;
        if alignment.is_aligned(pos) {
            return Ok(pos);
        }

        let Some(aligned_pos) = alignment.align_up(pos) else {
            return Err(std::io::ErrorKind::InvalidInput.into());
        };

        self.out.seek(SeekFrom::Start(aligned_pos))?;
        Ok(aligned_pos)
    }
}

/// Allows writing data to a stream.
///
/// This object does not implement [`Seek`] and there is no variant of this object that allows
/// seeking or writing to arbitrary offsets. Stream data must be written sequentially.
///
/// After a [`StreamWriter`] is closed (dropped), it is not possible to create a new `StreamWriter`
/// for the same stream.
///
/// # Write calls are never split across chunks
///
/// The [`Write`] implementation of this type makes a guarantee: For a given call to
/// [`StreamWriter::write`], if the current stream is using chunked compression, then the data will
/// be written to a single compressed chunk. This is an implementation guarantee; it is not required
/// by the MSFZ specification.
///
/// This allows readers to rely on complete records being stored within a single chunk. For example,
/// when copying the TPI, an encoder _could_ issue a sequence of `write()` calls whose boundaries
/// align with the boundaries of the records within the TPI. This would allow the reader to read
/// records directly from the chunk decompressed buffer, without needing to allocate a separate
/// buffer or copy the records. (We do not currently implement that behavior; this is describing a
/// hypothetical.)
pub struct StreamWriter<'a, F: Write + Seek> {
    file: &'a mut MsfzWriterFile<F>,
    stream: &'a mut Stream,
    alignment: Pow2,
    chunked_compression_enabled: bool,
}

impl<'a, F: Write + Seek> StreamWriter<'a, F> {
    /// Ends the current chunk, if any.
    ///
    /// This function is a performance hint for compression. It is not necessary to call this
    /// function.
    pub fn end_chunk(&mut self) -> std::io::Result<()> {
        self.file.finish_current_chunk()
    }

    /// The number of bytes that can be written to the current chunk buffer, without exceeding
    /// the configured maximum.
    pub fn bytes_available_in_chunk_buffer(&self) -> usize {
        self.file.bytes_available_in_chunk_buffer()
    }

    /// Specifies whether to use chunked compression or not. The default value for this setting is
    /// `true` (chunked compression is enabled).
    ///
    /// This does not have any immediate effect. It controls the behavior of the `write()`
    /// implementation for this stream.
    ///
    /// If this is called with `false`, then `write()` calls that follow this will cause stream
    /// data to be written to disk without compression.
    pub fn set_compression_enabled(&mut self, value: bool) {
        self.chunked_compression_enabled = value;
    }

    /// Specifies the on-disk alignment requirement for the start of the stream data.
    ///
    /// This only applies to uncompressed streams. Compressed stream data is always stored within
    /// compressed chunks, so the alignment is meaningless.
    pub fn set_alignment(&mut self, value: Pow2) {
        self.alignment = value;
    }
}

impl<'a, F: Write + Seek> Write for StreamWriter<'a, F> {
    fn flush(&mut self) -> std::io::Result<()> {
        Ok(())
    }

    fn write(&mut self, buf: &[u8]) -> std::io::Result<usize> {
        let _span = trace_span!("StreamWriter::write").entered();
        trace!(buf_len = buf.len());

        if buf.is_empty() {
            return Ok(0);
        }

        let old_stream_size: u32 = self.stream.fragments.iter().map(|f| f.size).sum();
        let is_first_write = old_stream_size == 0;
        let max_new_bytes = NIL_STREAM_SIZE - old_stream_size;

        // Check that buf.len() can be converted to u32, that the increase in size does not
        // overflow u32, and that writing the new data will not cause the stream size to erroneously
        // become NIL_STREAM_SIZE.
        let buf_len = match u32::try_from(buf.len()) {
            Ok(buf_len) if buf_len < max_new_bytes => buf_len,
            _ => {
                return Err(std::io::Error::new(
                    std::io::ErrorKind::InvalidInput,
                    "The input is too large for an MSFZ stream.",
                ));
            }
        };

        if self.chunked_compression_enabled {
            let chunk_at = self.file.write_to_chunks(buf)?;

            add_fragment_compressed(
                &mut self.stream.fragments,
                buf_len,
                chunk_at.chunk,
                chunk_at.offset,
            );
        } else {
            let fragment_file_offset: u64 = if is_first_write {
                self.file.write_align(self.alignment)?
            } else {
                self.file.out.stream_position()?
            };

            // The MSFZ spec allocates 48 bytes for the file offset of uncompressed fragments.
            if fragment_file_offset > MAX_UNCOMPRESSED_FILE_OFFSET {
                error!("The uncompressed file fragment ");
                return Err(std::io::ErrorKind::FileTooLarge.into());
            };

            self.file.out.write_all(buf)?;

            add_fragment_uncompressed(&mut self.stream.fragments, buf_len, fragment_file_offset);
        }

        Ok(buf.len())
    }
}

/// Adds a fragment record to a fragment list for a compressed fragment. If possible, the new
/// fragment is coalesced with the last record.
fn add_fragment_compressed(
    fragments: &mut Vec<Fragment>,
    new_fragment_size: u32,
    new_chunk: u32,
    new_offset_within_chunk: u32,
) {
    debug!(
        new_fragment_size,
        new_chunk, new_offset_within_chunk, "add_fragment_compressed"
    );

    // Either create a new fragment for this write or coalesce it with the previous fragment.
    match fragments.last_mut() {
        Some(Fragment {
            size: last_fragment_size,
            location:
                FragmentLocation::Compressed {
                    chunk_index: last_chunk,
                    offset_within_chunk: last_offset_within_chunk,
                },
        }) if *last_chunk == new_chunk
            && *last_offset_within_chunk + new_fragment_size == new_offset_within_chunk =>
        {
            *last_fragment_size += new_fragment_size;
        }

        _ => {
            // We cannot extend the last fragment, or there is no last fragment.
            fragments.push(Fragment {
                size: new_fragment_size,
                location: FragmentLocation::Compressed {
                    chunk_index: new_chunk,
                    offset_within_chunk: new_offset_within_chunk,
                },
            });
        }
    }
}

/// Adds a fragment record to a fragment list for an uncompressed fragment. If possible, the new
/// fragment is coalesced with the last record.
fn add_fragment_uncompressed(
    fragments: &mut Vec<Fragment>,
    new_fragment_size: u32,
    new_file_offset: u64,
) {
    debug!(
        new_fragment_size,
        new_file_offset, "add_fragment_uncompressed"
    );

    match fragments.last_mut() {
        Some(Fragment {
            size: last_fragment_size,
            location:
                FragmentLocation::Uncompressed {
                    file_offset: last_fragment_file_offset,
                },
        }) if *last_fragment_file_offset + new_fragment_size as u64 == new_file_offset => {
            *last_fragment_size += new_fragment_size;
        }

        _ => {
            // We cannot extend the last fragment, or there is no last fragment.
            fragments.push(Fragment {
                size: new_fragment_size,
                location: FragmentLocation::Uncompressed {
                    file_offset: new_file_offset,
                },
            });
        }
    }
}

/// Encodes a stream directory to its byte representation.
pub(crate) fn encode_stream_dir(streams: &[Option<Stream>]) -> Vec<u8> {
    let _span = trace_span!("encode_stream_dir").entered();

    let mut stream_dir_encoded: Vec<u8> = Vec::new();
    let mut enc = Encoder {
        vec: &mut stream_dir_encoded,
    };

    for stream_opt in streams.iter() {
        if let Some(stream) = stream_opt {
            for fragment in stream.fragments.iter() {
                assert_ne!(fragment.size, 0);
                assert_ne!(fragment.size, NIL_STREAM_SIZE);
                enc.u32(fragment.size);

                let location: u64 = match fragment.location {
                    FragmentLocation::Compressed {
                        chunk_index,
                        offset_within_chunk,
                    } => {
                        ((chunk_index as u64) << 32)
                            | (offset_within_chunk as u64)
                            | FRAGMENT_LOCATION_CHUNK_MASK
                    }
                    FragmentLocation::Uncompressed { file_offset } => file_offset,
                };

                enc.u64(location)
            }

            // Write 0 to the list to terminate the list of fragments.
            enc.u32(0);
        } else {
            // It's a nil stream. Our encoding writes a single NIL_STREAM_SIZE value to the
            // stream directory. It is _not_ followed by a fragment list.
            enc.u32(NIL_STREAM_SIZE);
        }
    }

    stream_dir_encoded.as_bytes().to_vec()
}

struct Encoder<'a> {
    vec: &'a mut Vec<u8>,
}

impl<'a> Encoder<'a> {
    fn u32(&mut self, value: u32) {
        self.vec.extend_from_slice(&value.to_le_bytes());
    }
    fn u64(&mut self, value: u64) {
        self.vec.extend_from_slice(&value.to_le_bytes());
    }
}

/// Defines options for finishing an MSFZ file.
#[derive(Clone, Debug, Default)]
pub struct MsfzFinishOptions {
    /// The minimum output file size. Use `MIN_FILE_SIZE_16K` to guarantee compatibility with
    /// MSVC tools that can read PDZ files.
    pub min_file_size: u64,

    /// If `Some`, then the Stream Directory will be compressed.
    pub stream_dir_compression: Option<Compression>,
}

/// This is the minimum file size that is guaranteed to avoid triggering a bug in the first
/// version of the PDZ decoder compiled into DIA (and other MSVC-derived tools).
pub const MIN_FILE_SIZE_16K: u64 = 0x4000;

```

`pdb/Cargo.toml`:

```toml
[package]
name = "ms-pdb"
version = "0.1.20"
edition = "2024"
description = "Reads Microsoft Program Database (PDB) files"
authors = ["Arlie Davis <ardavis@microsoft.com>"]
license = "MIT OR Apache-2.0"
repository = "https://github.com/microsoft/pdb-rs/"
categories = ["parsing", "development-tools", "development-tools::debugging"]

[lib]
doctest = false

[dependencies]
anyhow.workspace = true
bitfield.workspace = true
bitflags.workspace = true
bitvec.workspace = true
pow2.workspace = true
bstr.workspace = true
pretty-hex.workspace = true
static_assertions.workspace = true
sync_file.workspace = true
tracing.workspace = true
uuid.workspace = true
zerocopy = { workspace = true, features = ["alloc", "derive"] }
zerocopy-derive.workspace = true

[dependencies.ms-codeview]
version = "0.1.6"
path = "../codeview"

[dependencies.ms-coff]
version = "0.1.0"
path = "../coff"

[dependencies.ms-pdb-msf]
version = "0.1.7"
path = "../msf"

[dependencies.ms-pdb-msfz]
version = "0.1.10"
path = "../msfz"

[dev-dependencies]
static_init.workspace = true
tracing-subscriber.workspace = true

```

`pdb/README.md`:

```md
# Rust support for reading and writing Program Database (PDB) files

This crate provides the ability to read and write Program Database (PDB) files. PDB files
contain debugging symbols and other information used by debuggers and development tools,
when targeting the Windows operating system.

## All information in this implementation is based on publicly-available information

With the exception of MSFZ, this implementation is based solely on public
sources that describe the PDB and MSF data structures. This repository does not
contain any confidential Microsoft intellectual property.

## **THIS IMPLEMENTATION IS NOT AUTHORITATIVE AND IS NOT A REFERENCE IMPLEMENTATION**

This implementation is **NOT** an authoritative reference. It may contain
defects or inaccuracies. As the `LICENSE` states, this implementation is
provided "as is", without warranty of any kind. Specifically, this
implementation **DOES NOT** make any guarantees about compatibility or
interoperability with any other toolset, including (but not limited to)
Microsoft Visual C++ (MSVC) and Clang.

The authors of this effort may make a good-faith effort to fix bugs, including
bugs discovered by the authors or the community. However, as the license states,
this repository is provided "as is" and there is absolutely no obligation or
expectation on levels of service for the implementation provided in this
repository.

## Contributing

This project welcomes contributions and suggestions. Most contributions require
you to agree to a Contributor License Agreement (CLA) declaring that you have
the right to, and actually do, grant us the rights to use your contribution. For
details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether
you need to provide a CLA and decorate the PR appropriately (e.g., status check,
comment). Simply follow the instructions provided by the bot. You will only need
to do this once across all repos using our CLA.

This project has adopted the
[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the
[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any
additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or
services. Authorized use of Microsoft trademarks or logos is subject to and must
follow
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must
not cause confusion or imply Microsoft sponsorship. Any use of third-party
trademarks or logos are subject to those third-party's policies.

## Repository

* <https://github.com/microsoft/pdb-rs>

## Contacts

* `sivadeilra` on GitHub
* Arlie Davis ardavis@microsoft.com

```

`pdb/src/coff_groups.rs`:

```rs
use std::cmp::Ordering;

use ms_codeview::syms::OffsetSegment;

/// Contains a list of COFF groups.
#[derive(Clone, Default, Debug)]
pub struct CoffGroups {
    /// The COFF groups
    pub vec: Vec<CoffGroup>,
}

/// Describes a single COFF group.
///
/// A COFF group is a contiguous region within a COFF section. For this reason, they are sometimes
/// called "subsections".
#[derive(Clone, Debug)]
pub struct CoffGroup {
    /// Name of the section
    pub name: String,
    /// Bit flags
    pub characteristics: u32,
    /// The location where this COFF group begins. The COFF group is contained entirely within
    /// a single COFF section.
    pub offset_segment: OffsetSegment,
    /// The size in bytes of the COFF group.
    pub size: u32,
}

impl CoffGroups {
    /// Find the COFF group which contains `offset_segment`.
    pub fn find_group_at(&self, offset_segment: OffsetSegment) -> Option<&CoffGroup> {
        let seg = offset_segment.segment.get();
        let off = offset_segment.offset.get();

        match self.vec.binary_search_by(|g| {
            let g_seg: u16 = g.offset_segment.segment();
            let c = g_seg.cmp(&seg);
            if c.is_ne() {
                return c;
            }

            let g_off = g.offset_segment.offset();
            if off < g_off {
                return Ordering::Greater;
            }

            let offset_within_group = off - g_off;
            if offset_within_group < g.size {
                Ordering::Equal
            } else {
                Ordering::Less
            }
        }) {
            Ok(i) => Some(&self.vec[i]),
            Err(_) => None,
        }
    }
}

```

`pdb/src/container.rs`:

```rs
//! Provides an abstraction over MSF and MSFZ files.

use super::*;
use std::io::{Read, Seek, SeekFrom};

/// An abstraction over MSF and MSFZ files. Both types of files contain a set of streams.
pub enum Container<F> {
    /// The underlying file is an MSF file.
    Msf(msf::Msf<F>),
    /// The underlying file is an MSFZ file.
    Msfz(msfz::Msfz<F>),
}

impl<F: ReadAt> Container<F> {
    /// Provides direct access to the MSF layer. If this PDB file is using MSFZ instead of MSF,
    /// then this function returns `None`.
    pub fn msf(&self) -> Option<&msf::Msf<F>> {
        match self {
            Container::Msf(msf) => Some(msf),
            _ => None,
        }
    }

    /// Provides direct, mutable access to the MSF layer. If this PDB file is using MSFZ instead of
    /// MSF, then this function returns `None`.
    pub fn msf_mut(&mut self) -> Option<&mut msf::Msf<F>> {
        match self {
            Container::Msf(msf) => Some(msf),
            _ => None,
        }
    }

    /// Provides direct, mutable access to the MSF layer. If this PDB file is using MSFZ instead of
    /// MSF, then this function returns `None`.
    pub fn msf_mut_err(&mut self) -> anyhow::Result<&mut msf::Msf<F>> {
        match self {
            Container::Msf(msf) => Ok(msf),
            _ => bail!("This operation requires a PDB/MSF file. It cannot use a PDB/MSFZ file."),
        }
    }

    /// The total number of streams in this PDB.
    ///
    /// Some streams may be NIL.
    pub fn num_streams(&self) -> u32 {
        match self {
            Self::Msf(m) => m.num_streams(),
            Self::Msfz(m) => m.num_streams(),
        }
    }

    /// Returns an object which can read from a given stream.  The returned object implements
    /// the [`Read`], [`Seek`], and [`ReadAt`] traits.
    pub fn get_stream_reader(&self, stream: u32) -> anyhow::Result<StreamReader<'_, F>> {
        match self {
            Self::Msf(m) => Ok(StreamReader::Msf(m.get_stream_reader(stream)?)),
            Self::Msfz(m) => Ok(StreamReader::Msfz(m.get_stream_reader(stream)?)),
        }
    }

    /// Reads an entire stream to a vector.
    pub fn read_stream_to_vec(&self, stream: u32) -> anyhow::Result<Vec<u8>> {
        match self {
            Self::Msf(m) => m.read_stream_to_vec(stream),
            Self::Msfz(m) => Ok(m.read_stream(stream)?.into_vec()),
        }
    }

    /// Reads an entire stream to a vector.
    ///
    /// If the stream data is stored within a single compressed chunk, then this function returns
    /// a reference to the decompressed stream data.
    pub fn read_stream(&self, stream: u32) -> anyhow::Result<StreamData> {
        match self {
            Self::Msf(m) => Ok(StreamData::Box(m.read_stream_to_box(stream)?)),
            Self::Msfz(m) => m.read_stream(stream),
        }
    }

    /// Reads an entire stream into an existing vector.
    pub fn read_stream_to_vec_mut(
        &self,
        stream: u32,
        stream_data: &mut Vec<u8>,
    ) -> anyhow::Result<()> {
        match self {
            Self::Msf(m) => m.read_stream_to_vec_mut(stream, stream_data),
            Self::Msfz(m) => {
                let src = m.read_stream(stream)?;
                stream_data.clear();
                stream_data.extend_from_slice(&src);
                Ok(())
            }
        }
    }

    /// Gets the length of a given stream, in bytes.
    ///
    /// The `stream` value must be in a valid range of `0..num_streams()`.
    ///
    /// If `stream` is a NIL stream, this function returns 0.
    pub fn stream_len(&self, stream: u32) -> u64 {
        match self {
            Self::Msf(m) => m.stream_size(stream) as u64,
            Self::Msfz(m) => m.stream_size(stream).unwrap_or_default(),
        }
    }

    /// Returns `true` if `stream` is a valid stream index and is not a nil stream.
    pub fn is_stream_valid(&self, stream: u32) -> bool {
        match self {
            Self::Msf(m) => m.is_stream_valid(stream),
            Self::Msfz(m) => m.is_stream_valid(stream),
        }
    }
}

/// Allows reading a stream using the [`Read`], [`Seek`], and [`ReadAt`] traits.
pub enum StreamReader<'a, F> {
    /// A stream stored within an MSF file.
    Msf(msf::StreamReader<'a, F>),
    /// A stream stored within an MSFZ file.
    Msfz(msfz::StreamReader<'a, F>),
}

impl<'a, F: ReadAt> StreamReader<'a, F> {
    /// Tests whether this stream is empty (zero-length)
    pub fn is_empty(&self) -> bool {
        match self {
            Self::Msf(s) => s.is_empty(),
            Self::Msfz(s) => s.is_empty(),
        }
    }

    /// Returns the length in bytes of the stream.
    ///
    /// If the stream is a nil stream, this returns 0.
    pub fn stream_size(&self) -> u64 {
        match self {
            Self::Msf(s) => s.len() as u64,
            Self::Msfz(s) => s.stream_size(),
        }
    }
}

impl<'a, F: ReadAt> ReadAt for StreamReader<'a, F> {
    fn read_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<usize> {
        match self {
            Self::Msf(s) => s.read_at(buf, offset),
            Self::Msfz(s) => s.read_at(buf, offset),
        }
    }

    fn read_exact_at(&self, buf: &mut [u8], offset: u64) -> std::io::Result<()> {
        match self {
            Self::Msf(s) => s.read_exact_at(buf, offset),
            Self::Msfz(s) => s.read_exact_at(buf, offset),
        }
    }
}

impl<'a, F: ReadAt> Read for StreamReader<'a, F> {
    fn read(&mut self, buf: &mut [u8]) -> std::io::Result<usize> {
        match self {
            Self::Msf(s) => s.read(buf),
            Self::Msfz(s) => s.read(buf),
        }
    }

    fn read_exact(&mut self, buf: &mut [u8]) -> std::io::Result<()> {
        match self {
            Self::Msf(s) => s.read_exact(buf),
            Self::Msfz(s) => s.read_exact(buf),
        }
    }
}

impl<'a, F: ReadAt> Seek for StreamReader<'a, F> {
    fn seek(&mut self, pos: SeekFrom) -> std::io::Result<u64> {
        match self {
            Self::Msf(s) => s.seek(pos),
            Self::Msfz(s) => s.seek(pos),
        }
    }
}

```

`pdb/src/dbi.rs`:

```rs
//! Provides access to the DBI Stream (Debug Information).
//!
//! The DBI Stream is a central data structure of the PDB. It contains many vital fields, and
//! points to other streams that contain other important information. The DBI is stream 3.
//!
//! Briefly, the DBI contains these substreams:
//!
//! * Modules: This lists the modules (compilands / translation units) that compose an executable.
//!   Each Module Info structure contains many important fields, including the stream number for
//!   a Module Stream.
//!
//! * Section Contributions Substream
//!
//! * Section Map Substream
//!
//! * Sources Substream: This lists the source files that were inputs to all of the translation units.
//!
//! * Type Server Map Substream
//!
//! * Optional Debug Header Substream
//!
//! * Edit-and-Continue Substream
//!
//! The `Dbi` stream holds section contributions and the list of modules (compilands).
//!
//! * <https://llvm.org/docs/PDB/DbiStream.html>
//! * <https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/langapi/include/pdb.h#L860>

use crate::dbi::optional_dbg::{OptionalDebugHeaders, OptionalDebugStream};
use crate::{Container, NIL_STREAM_INDEX};
use crate::{Stream, get_or_init_err};
use crate::{StreamIndexIsNilError, StreamIndexU16};
use anyhow::{Result, bail};
use ms_codeview::parser::{Parser, ParserError, ParserMut};
use ms_coff::IMAGE_SECTION_HEADER;
use std::mem::size_of;
use std::ops::Range;
use sync_file::ReadAt;
use tracing::{error, warn};
use zerocopy::{
    FromBytes, FromZeros, I32, Immutable, IntoBytes, KnownLayout, LE, U16, U32, Unaligned,
};

#[cfg(doc)]
use crate::Pdb;

pub mod fixups;
pub mod modules;
pub mod optional_dbg;
pub mod section_contrib;
pub mod section_map;
pub mod sources;

pub use modules::*;
#[doc(inline)]
pub use section_contrib::*;
#[doc(inline)]
pub use sources::*;

/// The header of the DBI (Debug Information) stream.
#[repr(C)]
#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Debug, Clone)]
#[allow(missing_docs)]
pub struct DbiStreamHeader {
    /// Always -1
    pub signature: I32<LE>,

    /// One of the `DBI_STREAM_VERSION_*` values; typically, `DBI_STREAM_VERSION_V110`.
    pub version: U32<LE>,

    /// The number of times this PDB has been modified. The value is set to 1 when a PDB is
    /// first created. This value must match the same field within the PE header.
    pub age: U32<LE>,

    /// The index of the Global Symbol Index, which contains a name-to-symbol lookup table for
    /// global symbols. The symbol records are not stored in this stream; they are stored in the
    /// Global Symbol Stream.
    pub global_symbol_index_stream: StreamIndexU16,

    pub build_number: U16<LE>,

    /// The index of the stream that contains the Public Symbol Index (GSI). This contains a
    /// name-to-symbol map and an address-to-symbol map. See [`crate::globals::gsi`].
    pub public_symbol_index_stream: StreamIndexU16,

    /// The version of the MSPDB DLL which produced this DBI stream.
    pub pdb_dll_version: U16<LE>,

    /// The stream that contains the Global Symbol Stream. This contains symbol records, which can
    /// be decoded using [`crate::syms::SymIter`].
    pub global_symbol_stream: StreamIndexU16,

    pub pdb_dll_rbld: U16<LE>,

    // Substreams
    pub mod_info_size: I32<LE>,
    pub section_contribution_size: I32<LE>,
    pub section_map_size: I32<LE>,
    pub source_info_size: I32<LE>,
    pub type_server_map_size: I32<LE>,
    /// This field is _not_ a substream size. Not sure what it is.
    pub mfc_type_server_index: U32<LE>,
    pub optional_dbg_header_size: I32<LE>,
    pub edit_and_continue_size: I32<LE>,

    pub flags: U16<LE>,
    pub machine: U16<LE>,
    pub padding: U32<LE>,
}

/// Data for an empty DBI stream
pub static EMPTY_DBI_STREAM_HEADER: [u8; DBI_STREAM_HEADER_LEN] = [
    0xFF, 0xFF, 0xFF, 0xFF, // signature
    0x77, 0x09, 0x31, 0x01, // version
    0x01, 0x00, 0x00, 0x00, // age
    0xFF, 0xFF, // global_stream_index
    0x00, 0x00, // build_number
    0xFF, 0xFF, // public_stream_index
    0x00, 0x00, // pdb_dll_version
    0xFF, 0xFF, // sym_record_stream
    0x00, 0x00, // pdb_dll_rbld
    0x00, 0x00, 0x00, 0x00, // mod_info_size
    0x00, 0x00, 0x00, 0x00, // section_contribution_size
    0x00, 0x00, 0x00, 0x00, // section_map_size
    0x00, 0x00, 0x00, 0x00, // source_info_size
    0x00, 0x00, 0x00, 0x00, // type_server_map_size
    0x00, 0x00, 0x00, 0x00, // mfc_type_server_index
    0x00, 0x00, 0x00, 0x00, // optional_dbg_header_size
    0x00, 0x00, 0x00, 0x00, // edit_and_continue_size
    0x00, 0x00, // flags
    0x00, 0x00, // machine
    0x00, 0x00, 0x00, 0x00, // padding
];

#[test]
fn test_parse_empty_dbi_stream_header() {
    let h = DbiStreamHeader::read_from_bytes(EMPTY_DBI_STREAM_HEADER.as_slice()).unwrap();
    assert!(h.global_symbol_index_stream.get().is_none());
}

impl DbiStreamHeader {
    /// Gets the stream index for the Global Symbol Stream.
    pub fn sym_record_stream(&self) -> Result<u32, StreamIndexIsNilError> {
        self.global_symbol_stream.get_err()
    }

    /// Gets the stream index for the Public Symbol Index.
    pub fn public_stream_index(&self) -> Result<u32, StreamIndexIsNilError> {
        self.public_symbol_index_stream.get_err()
    }

    /// Gets the stream index for the Global Symbol Index.
    pub fn global_stream_index(&self) -> Result<u32, StreamIndexIsNilError> {
        self.global_symbol_index_stream.get_err()
    }

    /// Byte range of the Modules substream.
    pub fn modules_range(&self) -> anyhow::Result<Range<usize>> {
        let start = DBI_STREAM_HEADER_LEN;
        let size = self.mod_info_size.get() as usize;
        Ok(start..start + size)
    }

    /// Byte range of the Modules substream.
    pub fn sources_range(&self) -> anyhow::Result<Range<usize>> {
        let start = DBI_STREAM_HEADER_LEN
            + self.mod_info_size.get() as usize
            + self.section_contribution_size.get() as usize
            + self.section_map_size.get() as usize;
        let size = self.source_info_size.get() as usize;
        Ok(start..start + size)
    }

    /// The total length of all substreams, or None if this value cannot be computed.
    ///
    /// In a well-formed DBI stream, this value can be computed and the value is less than
    /// the size of the data that follows the DBI Stream Header.
    pub fn total_substreams_len(&self) -> Option<u32> {
        // Read the fields and (where necessary) convert them from i32 to u32.
        // If a value is negative, then we return None.
        u32::try_from(self.mod_info_size.get())
            .ok()?
            .checked_add(u32::try_from(self.section_contribution_size.get()).ok()?)?
            .checked_add(u32::try_from(self.section_map_size.get()).ok()?)?
            .checked_add(u32::try_from(self.source_info_size.get()).ok()?)?
            .checked_add(u32::try_from(self.type_server_map_size.get()).ok()?)?
            .checked_add(u32::try_from(self.optional_dbg_header_size.get()).ok()?)?
            .checked_add(u32::try_from(self.edit_and_continue_size.get()).ok()?)
    }
}

static_assertions::const_assert_eq!(size_of::<DbiStreamHeader>(), DBI_STREAM_HEADER_LEN);
/// The size of the DBI stream header.
pub const DBI_STREAM_HEADER_LEN: usize = 64;

/// MSVC version 4.1
pub const DBI_STREAM_VERSION_VC41: u32 = 930803;
/// MSVC version 5.0
pub const DBI_STREAM_VERSION_V50: u32 = 19960307;
/// MSVC version 6.0
pub const DBI_STREAM_VERSION_V60: u32 = 19970606;
/// MSVC version 7.0
pub const DBI_STREAM_VERSION_V70: u32 = 19990903;
/// MSVC version 11.0
pub const DBI_STREAM_VERSION_V110: u32 = 20091201;

/// Holds or refers to the DBI stream.
///
/// The `StreamData` type parameter can be any type that can contain `[u8]`.
///
/// This type contains (or refers to) the _entire_ DBI stream, not just the header.
#[derive(Clone)]
pub struct DbiStream<StreamData = Vec<u8>>
where
    StreamData: AsRef<[u8]>,
{
    /// The contents of the stream.
    pub stream_data: StreamData,

    /// The byte ranges of the substreams.
    pub substreams: DbiSubstreamRanges,
}

// The DBI stream contains a fixed number of "substreams". The DBI header specifies the
// length of each substream.  The position of each substream is found by computing the
// sum of all previous substreams (and the header).
macro_rules! dbi_substreams {
    (
        $(
            $name:ident,
            $mut_name:ident,
            $size_field:ident ;
        )*
    ) => {
        /// Contains the byte ranges of the substreams within the DBI stream.
        #[derive(Clone, Debug, Default)]
        pub struct DbiSubstreamRanges {
            $(
                #[doc = concat!("The range of the ", stringify!($name), " substream.")]
                pub $name: Range<usize>,
            )*
        }

        impl<StreamData: AsRef<[u8]>> DbiStream<StreamData> {
            $(
                #[doc = concat!("The unparsed contents of the ", stringify!($name), " substream.")]
                pub fn $name(&self) -> &[u8] {
                    self.substream_data(self.substreams.$name.clone())
                }

                #[doc = concat!("The unparsed contents of the ", stringify!($name), " substream.")]
                pub fn $mut_name(&mut self) -> &mut [u8]
                where
                    StreamData: AsMut<[u8]>,
                {
                    self.substream_data_mut(self.substreams.$name.clone())
                }

            )*
        }

        impl DbiSubstreamRanges {
            pub(crate) fn from_sizes(sizes: &DbiStreamHeader, stream_len: usize) -> anyhow::Result<Self> {
                let mut pos: usize = DBI_STREAM_HEADER_LEN;
                if pos > stream_len {
                    bail!("DBI stream is too short; pos = {}, stream_len = {}", pos, stream_len);
                }

                $(
                    assert!(pos <= stream_len);
                    let size: i32 = sizes.$size_field.get();
                    if size < 0 {
                        bail!("Substream {} length in DBI header is invalid (is negative)", stringify!($size_field));
                    }

                    let len = size as usize;
                    let available = stream_len - pos;
                    if len > available {
                        bail!("Substream {} length in DBI header is invalid. It extends beyond the end of the stream.", stringify!($size_field));
                    }
                    let start = pos;
                    pos += len;

                    let $name = start..pos;
                )*

                if pos < stream_len {
                    warn!(pos, stream_len, "Something is wrong with the code that finds the ranges of substreams. Expected pos to be equal to stream_len.");
                } else if pos > stream_len {
                    error!(pos, stream_len, "Something is very wrong with the DBI header. The sum of the subtream lengths (pos) exceeds the stream len.");
                } else {
                    // Substream sizes look good.
                }

                Ok(Self {
                    $( $name, )*
                })
            }
        }
    }
}

dbi_substreams! {
    // The order of these determines the order of the substream data in the stream.
    modules_bytes, modules_bytes_mut, mod_info_size;
    section_contributions_bytes, section_contributions_bytes_mut, section_contribution_size;
    section_map_bytes, section_map_bytes_mut, section_map_size;
    source_info, source_info_mut, source_info_size;
    type_server_map, type_server_map_mut, type_server_map_size;
    edit_and_continue, edit_and_continue_mut, edit_and_continue_size;
    optional_debug_header_bytes, optional_debug_header_bytes_mut, optional_dbg_header_size;
}

impl<StreamData: AsRef<[u8]>> DbiStream<StreamData> {
    /// Returns the DBI stream header.
    pub fn header(&self) -> Result<&DbiStreamHeader> {
        if let Ok((header, _)) = DbiStreamHeader::ref_from_prefix(self.stream_data.as_ref()) {
            Ok(header)
        } else {
            bail!("The DBI stream is too small to contain a valid header.")
        }
    }

    /// Provides mutable access to the DBI stream header.
    pub fn header_mut(&mut self) -> Result<&mut DbiStreamHeader>
    where
        StreamData: AsMut<[u8]>,
    {
        if let Ok((header, _)) = DbiStreamHeader::mut_from_prefix(self.stream_data.as_mut()) {
            Ok(header)
        } else {
            bail!("The DBI stream is too small to contain a valid header.")
        }
    }

    fn substream_data(&self, range: Range<usize>) -> &[u8] {
        &self.stream_data.as_ref()[range]
    }

    fn substream_data_mut(&mut self, range: Range<usize>) -> &mut [u8]
    where
        StreamData: AsMut<[u8]>,
    {
        &mut self.stream_data.as_mut()[range]
    }

    /// Reads the Module Information substream.
    pub fn modules(&self) -> ModInfoSubstream<&[u8]> {
        ModInfoSubstream {
            substream_data: self.modules_bytes(),
        }
    }

    /// Iterates the Module records in the Module Information Substream.
    pub fn iter_modules(&self) -> IterModuleInfo<'_> {
        IterModuleInfo::new(self.modules_bytes())
    }

    /// Iterates the Module records in the Module Information Substream, with mutable access.
    pub fn iter_modules_mut(&mut self) -> IterModuleInfoMut<'_>
    where
        StreamData: AsMut<[u8]>,
    {
        IterModuleInfoMut::new(self.modules_bytes_mut())
    }

    /// Return a DbiStream over just a a reference
    pub fn as_slice(&self) -> DbiStream<&[u8]> {
        DbiStream {
            stream_data: self.stream_data.as_ref(),
            substreams: self.substreams.clone(),
        }
    }

    /// Read the DBI Stream header and validate it.
    pub fn parse(stream_data: StreamData) -> anyhow::Result<Self> {
        let stream_bytes: &[u8] = stream_data.as_ref();

        if stream_bytes.is_empty() {
            return Ok(Self {
                substreams: Default::default(),
                stream_data,
            });
        }

        let mut p = Parser::new(stream_bytes);
        let dbi_header: &DbiStreamHeader = p.get()?;

        let substreams = DbiSubstreamRanges::from_sizes(dbi_header, stream_bytes.len())?;

        // We just computed the ranges for each of the substreams, and we verified that the end of
        // the substreams is equal to the size of the entire stream. That implicitly validates all
        // of the range checks for the substreams, so we don't need explicit / verbose checks.
        // We can simply use normal range indexing.

        Ok(Self {
            stream_data,
            substreams,
        })
    }

    /// Parses the DBI Sources Substream section.
    pub fn sources(&self) -> anyhow::Result<sources::DbiSourcesSubstream<'_>> {
        DbiSourcesSubstream::parse(self.source_info())
    }

    /// Parses the header of the Section Contributions Substream and returns an object which can
    /// query it.
    pub fn section_contributions(
        &self,
    ) -> anyhow::Result<section_contrib::SectionContributionsSubstream<'_>> {
        let substream_bytes = self.section_contributions_bytes();
        section_contrib::SectionContributionsSubstream::parse(substream_bytes)
    }

    /// Parses the header of the Section Map Substream and returns an object which can query it.
    pub fn section_map(&self) -> anyhow::Result<section_map::SectionMap<'_>> {
        let section_map_bytes = self.section_map_bytes();
        section_map::SectionMap::parse(section_map_bytes)
    }

    /// Parses the Optional Debug Header Substream and returns an object which can query it.
    pub fn optional_debug_header(&self) -> anyhow::Result<optional_dbg::OptionalDebugHeader<'_>> {
        optional_dbg::OptionalDebugHeader::parse(self.optional_debug_header_bytes())
    }

    /// Gets a mutable reference to the Optional Debug Header substream.
    pub fn optional_debug_header_mut(&mut self) -> anyhow::Result<&mut [U16<LE>]>
    where
        StreamData: AsMut<[u8]>,
    {
        if self.substreams.optional_debug_header_bytes.is_empty() {
            Ok(&mut [])
        } else {
            let substream_bytes =
                &mut self.stream_data.as_mut()[self.substreams.optional_debug_header_bytes.clone()];

            if let Ok(slice) = <[U16<LE>]>::mut_from_bytes(substream_bytes) {
                Ok(slice)
            } else {
                bail!(
                    "The Optional Debug Header substream within the DBI stream is malformed (length is not valid)."
                );
            }
        }
    }
}

/// Reads the header of the DBI stream. This does **not** validate the header.
///
/// This is a free function because we need to use it before constructing an instance of [`Pdb`].
pub fn read_dbi_stream_header<F: ReadAt>(msf: &Container<F>) -> anyhow::Result<DbiStreamHeader> {
    let stream_reader = msf.get_stream_reader(Stream::DBI.into())?;
    if !stream_reader.is_empty() {
        let mut dbi_header = DbiStreamHeader::new_zeroed();
        stream_reader.read_exact_at(dbi_header.as_mut_bytes(), 0)?;
        Ok(dbi_header)
    } else {
        Ok(DbiStreamHeader::read_from_bytes(EMPTY_DBI_STREAM_HEADER.as_slice()).unwrap())
    }
}

/// Reads the entire DBI Stream, validates the header, and then returns an object that
/// can be used for further queries of the DBI Stream.
///
/// This is a free function because we need to use it before constructing an instance of [`Pdb`].
pub fn read_dbi_stream<F: ReadAt>(
    container: &Container<F>,
) -> Result<DbiStream<Vec<u8>>, anyhow::Error> {
    let mut dbi_stream_data = container.read_stream_to_vec(Stream::DBI.into())?;
    if dbi_stream_data.is_empty() {
        dbi_stream_data = EMPTY_DBI_STREAM_HEADER.to_vec();
    }

    DbiStream::parse(dbi_stream_data)
}

impl<F: ReadAt> crate::Pdb<F> {
    /// Reads the header of the DBI stream. This does **not** validate the header.
    pub fn read_dbi_stream_header(&self) -> anyhow::Result<DbiStreamHeader> {
        read_dbi_stream_header(&self.container)
    }

    /// Reads the entire DBI Stream, validates the header, and then returns an object that
    /// can be used for further queries of the DBI Stream.
    pub fn read_dbi_stream(&self) -> Result<DbiStream<Vec<u8>>, anyhow::Error> {
        read_dbi_stream(&self.container)
    }

    fn read_dbi_substream(&self, range: Range<usize>) -> anyhow::Result<Vec<u8>> {
        let len = range.len();
        let mut substream_data = vec![0; len];
        let reader = self.container.get_stream_reader(Stream::DBI.into())?;
        reader.read_exact_at(&mut substream_data, range.start as u64)?;
        Ok(substream_data)
    }

    fn read_dbi_substream_u32(&self, range: Range<usize>) -> anyhow::Result<Vec<u32>> {
        let len_bytes = range.len();
        let len_u32 = len_bytes / 4;
        let mut substream_data = vec![0u32; len_u32];
        let reader = self.container.get_stream_reader(Stream::DBI.into())?;
        reader.read_exact_at(substream_data.as_mut_bytes(), range.start as u64)?;
        Ok(substream_data)
    }

    /// Reads the module substream data from the DBI stream.
    ///
    /// This function always reads the data from the file. It does not cache the data.
    pub fn read_modules(&self) -> anyhow::Result<ModInfoSubstream<Vec<u8>>> {
        let substream_data = self.read_dbi_substream(self.dbi_substreams.modules_bytes.clone())?;
        Ok(ModInfoSubstream { substream_data })
    }

    /// Gets access to the DBI Modules Substream. This will read the DBI Modules Substream
    /// on-demand, and will cache it.
    pub fn modules(&self) -> anyhow::Result<&ModInfoSubstream<Vec<u8>>> {
        get_or_init_err(&self.cached.dbi_modules_cell, || self.read_modules())
    }

    /// Reads the DBI Sources Substream. This always reads the data, and does not cache it.
    pub fn read_sources_data(&self) -> Result<Vec<u8>> {
        self.read_dbi_substream(self.dbi_substreams.source_info.clone())
    }

    /// Gets access to the DBI Sources Substream data.
    pub fn sources_data(&self) -> Result<&[u8]> {
        let sources_data =
            get_or_init_err(&self.cached.dbi_sources_cell, || self.read_sources_data())?;
        Ok(sources_data)
    }

    /// Gets access to the DBI Sources Substream and parses the header.
    pub fn sources(&self) -> Result<sources::DbiSourcesSubstream<'_>> {
        let sources_data = self.sources_data()?;
        sources::DbiSourcesSubstream::parse(sources_data)
    }

    /// Drops the cached DBI Sources Substream data, if any.
    pub fn drop_sources(&mut self) {
        self.cached.dbi_sources_cell = Default::default();
    }

    /// Reads the contents of the DBI Section Contributions Substream. This function never caches
    /// the data; it is always read unconditionally.
    ///
    /// The returned buffer is `Vec<u32>` instead of `Vec<u8>` so that natural alignment is
    /// guaranteed.
    pub fn read_section_contributions(&self) -> Result<Vec<u32>> {
        self.read_dbi_substream_u32(self.dbi_substreams.section_contributions_bytes.clone())
    }

    /// Reads (uncached) the DBI Optional Debug Streams Substream.
    pub fn read_optional_debug_streams(&self) -> anyhow::Result<OptionalDebugHeaders> {
        let num_opt_streams = self.dbi_substreams.optional_debug_header_bytes.len() / 2;
        if num_opt_streams == 0 {
            return Ok(OptionalDebugHeaders {
                streams: Vec::new(),
            });
        }

        let mut streams: Vec<u16> = vec![0; num_opt_streams];
        let sr = self.get_stream_reader(Stream::DBI.value() as u32)?;
        sr.read_exact_at(
            streams.as_mut_bytes(),
            self.dbi_substreams.optional_debug_header_bytes.start as u64,
        )?;

        Ok(OptionalDebugHeaders { streams })
    }

    /// Gets the stream index of the `FIXUP_DATA` optional debug stream.
    pub fn fixup_stream(&self) -> anyhow::Result<Option<u32>> {
        self.optional_debug_stream(OptionalDebugStream::FIXUP_DATA)
    }

    /// Gets the DBI Optional Debug Streams Substream.
    pub fn optional_debug_streams(&self) -> anyhow::Result<&OptionalDebugHeaders> {
        get_or_init_err(&self.cached.optional_dbg_streams, || {
            self.read_optional_debug_streams()
        })
    }

    /// Gets the stream index of a specific Optional Debug Stream.
    pub fn optional_debug_stream(&self, i: OptionalDebugStream) -> anyhow::Result<Option<u32>> {
        let streams = self.optional_debug_streams()?;
        if let Some(&s) = streams.streams.get(i.0 as usize) {
            if s != NIL_STREAM_INDEX {
                Ok(Some(s as u32))
            } else {
                Ok(None)
            }
        } else {
            Ok(None)
        }
    }

    /// Reads (uncached) the contents of the "Section Headers" Optional Debug Stream.
    pub fn read_section_headers(&self) -> anyhow::Result<Box<[IMAGE_SECTION_HEADER]>> {
        let Some(stream) = self.optional_debug_stream(OptionalDebugStream::SECTION_HEADER_DATA)?
        else {
            // This information is not available.
            return Ok(Box::from([]));
        };

        let sr = self.get_stream_reader(stream)?;
        let stream_size = sr.stream_size();
        let num_sections = stream_size as usize / core::mem::size_of::<IMAGE_SECTION_HEADER>();
        let mut section_headers: Box<[IMAGE_SECTION_HEADER]> =
            <[IMAGE_SECTION_HEADER]>::new_box_zeroed_with_elems(num_sections).unwrap();
        sr.read_exact_at(section_headers.as_mut_bytes(), 0)?;
        Ok(section_headers)
    }

    /// Gets the contents of the "Section Headers" Optional Debug Stream.
    pub fn section_headers(&self) -> anyhow::Result<&[IMAGE_SECTION_HEADER]> {
        get_or_init_err(&self.cached.section_headers, || self.read_section_headers()).map(|v| &**v)
    }
}

/// Reads fields of the DBI Stream and validates them for consistency with the specification.
pub fn validate_dbi_stream(stream_data: &[u8]) -> anyhow::Result<()> {
    let dbi_stream = DbiStream::parse(stream_data)?;

    // For now, the only validation that we do in this function is decoding the ModuleInfo records.
    let num_modules: usize = dbi_stream.modules().iter().count();

    let sources = DbiSourcesSubstream::parse(dbi_stream.source_info())?;
    if sources.num_modules() != num_modules {
        bail!(
            "Number of modules found in Sources substream ({}) does not match number of Module Info structs found in Modules substream ({}).",
            sources.num_modules(),
            num_modules
        );
    }

    Ok(())
}

```

`pdb/src/dbi/fixups.rs`:

```rs
//! Definitions relating to the `FIXUP_DATA` Optional Debug Substream.

use super::*;
use crate::Pdb;

/// Describes a fixup record, stored in the `FIXUP_DATA` Optional Debug Substream.
///
/// This _does not_ describe the structure of a relocation record (stored in the PE binary itself).
/// Instead, this structure describes "fixups" (which are essentially relocation records) that are
/// stored in the `FIXUP_DATA` Optional Debug Substream. These records allow binary analysis tools
/// to discover relationships between code (and between code and data) that are not otherwise
/// represented in the PE binary itself.
#[allow(missing_docs)]
#[repr(C)]
#[derive(Clone, Default, Debug, FromBytes, Immutable, IntoBytes, KnownLayout)]
pub struct Fixup {
    /// Relocation type
    ///
    /// This is one of the `IMAGE_REL_*` constants. See the documentation for PE relocations.
    /// These values are architecture-dependent; the same numeric values may have different
    /// meanings on different architectures.
    pub fixup_type: u16,
    pub extra: u16,
    pub rva: u32,
    pub rva_target: u32,
}

impl<F: ReadAt> Pdb<F> {
    /// Reads (uncached) the `FIXUP_DATA` optional debug stream.
    pub fn read_fixups(&self) -> anyhow::Result<Option<Vec<Fixup>>> {
        let Some(fixup_stream_index) = self.fixup_stream()? else {
            return Ok(None);
        };

        let sr = self.get_stream_reader(fixup_stream_index)?;
        let num_records = sr.stream_size() as usize / size_of::<Fixup>();
        let mut fixups: Vec<Fixup> = Fixup::new_vec_zeroed(num_records).unwrap();
        sr.read_exact_at(fixups.as_mut_bytes(), 0)?;
        Ok(Some(fixups))
    }
}

```

`pdb/src/dbi/modules.rs`:

```rs
//! DBI Modules Substream

use super::*;
use crate::StreamIndexU16;
use bstr::BStr;
use ms_codeview::HasRestLen;
use std::mem::take;
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned};

/// The header of a Module Info record. Module Info records are stored in the DBI stream.
///
/// See `dbi.h`, `MODI_60_Persist`
#[derive(Unaligned, IntoBytes, FromBytes, Immutable, KnownLayout, Clone, Debug)]
#[repr(C)]
pub struct ModuleInfoFixed {
    /// This appears to be a module index field, but it is not always set.
    ///
    /// In some PDBs, we see this field being set to the zero-based index of this Module Info record
    /// in the DBI Modules Substream.  In other PDBs, this value is 0.  Set this to 0.
    pub unused1: U32<LE>,

    /// This module's first section contribution.
    pub section_contrib: SectionContribEntry,

    /// Various flags
    ///
    /// * bit 0: set to 1 if this module has been written since DBI opened
    /// * bit 1: set to 1 if this module has EC symbolic information
    /// * bits 2-7: not used
    /// * bits 8-15: index into TSM list for this mods server
    pub flags: U16<LE>,

    /// Stream index of the Module Stream for this module, which contains the symbols and line data
    /// for this module. If this is 0xffff, then this module does not have a module info stream.
    pub stream: StreamIndexU16,

    /// Specifies the size of the symbols substream within the Module Stream.
    pub sym_byte_size: U32<LE>,

    /// Specifies the length of the C11 Line Data in a Module Information Stream.
    /// C11 line data is obsolete and is not supported.
    pub c11_byte_size: U32<LE>,

    /// Specifies the length of the C13 Line Data in a Module Information Stream.
    pub c13_byte_size: U32<LE>,

    /// Number of files contributing to this module.
    pub source_file_count: U16<LE>,

    /// Alignment padding.
    pub padding: [u8; 2],

    /// Do not read. Set to 0 when encoding.
    pub unused2: U32<LE>,

    /// Unknown; possible that this relates to Edit-and-Continue.
    pub source_file_name_index: U32<LE>,

    /// Unknown; possible that this relates to Edit-and-Continue.
    pub pdb_file_path_name_index: U32<LE>,
}

impl ModuleInfoFixed {
    /// Gets the stream for this module, if any. This stream contains the symbol data and C13 Line
    /// Data for the module.
    pub fn stream(&self) -> Option<u32> {
        self.stream.get()
    }
}

/// Holds or refers to the data of a substream within a Module Info record.
#[derive(Clone)]
pub struct ModInfoSubstream<D: AsRef<[u8]>> {
    /// The substream data.
    pub substream_data: D,
}

impl<D: AsRef<[u8]>> ModInfoSubstream<D> {
    /// Iterates the Module Info records contained within the DBI Stream.
    pub fn iter(&self) -> IterModuleInfo<'_> {
        IterModuleInfo {
            rest: self.substream_data.as_ref(),
        }
    }
}

/// An in-memory representation of a Module Info record.
///
/// The `IterModInfo` iterator produces these items.
#[allow(missing_docs)]
pub struct ModuleInfo<'a> {
    pub header: &'a ModuleInfoFixed,
    pub module_name: &'a BStr,
    pub obj_file: &'a BStr,
}

/// A mutable view of a Module Info record.
#[allow(missing_docs)]
pub struct ModuleInfoMut<'a> {
    pub header: &'a mut ModuleInfoFixed,
    pub module_name: &'a BStr,
    pub obj_file: &'a BStr,
}

impl<'a> ModuleInfo<'a> {
    /// The name of the module.
    ///
    /// * For simple object files, this is the same as `file_name()`.
    /// * For DLL import libraries, this is the name of the DLL, e.g. `KernelBase.dll`.
    /// * For static libraries, this is the name (and possibly path) of the object file within the
    ///   static library, not the static library itself.
    pub fn module_name(&self) -> &'a BStr {
        self.module_name
    }

    /// The file name of this module.
    ///
    /// * For individual `*.obj` files that are passed directly to the linker (not in a static
    ///   library), this is the filename.
    /// * For static libraries, this is the `*.lib` file, not the modules within it.
    /// * For DLL import libraries, this is the import library, e.g. `KernelBase.lib`.
    pub fn obj_file(&self) -> &'a BStr {
        self.obj_file
    }

    /// The header of this Module Info record.
    pub fn header(&self) -> &'a ModuleInfoFixed {
        self.header
    }

    /// The stream index of the stream which contains the symbols defined by this module.
    ///
    /// Some modules do not have a symbol stream. In that case, this function will return `None`.
    pub fn stream(&self) -> Option<u32> {
        self.header.stream()
    }

    /// Gets the size in bytes of the C11 Line Data.
    pub fn c11_size(&self) -> u32 {
        self.header.c11_byte_size.get()
    }

    /// Gets the size in bytes of the C13 Line Data.
    pub fn c13_size(&self) -> u32 {
        self.header.c13_byte_size.get()
    }

    /// Gets the size in bytes of the symbol stream for this module. This value includes the size
    /// of the 4-byte symbol stream header.
    pub fn sym_size(&self) -> u32 {
        self.header.sym_byte_size.get()
    }
}

/// Iterates module info records
pub struct IterModuleInfo<'a> {
    rest: &'a [u8],
}

impl<'a> IterModuleInfo<'a> {
    #[allow(missing_docs)]
    pub fn new(data: &'a [u8]) -> Self {
        Self { rest: data }
    }

    /// Returns the data in the iterator that has not yet been parsed.
    pub fn rest(&self) -> &'a [u8] {
        self.rest
    }
}

impl<'a> HasRestLen for IterModuleInfo<'a> {
    fn rest_len(&self) -> usize {
        self.rest.len()
    }
}

impl<'a> Iterator for IterModuleInfo<'a> {
    type Item = ModuleInfo<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.rest.is_empty() {
            return None;
        }

        let mut p = Parser::new(self.rest);

        let len_before = p.len();
        let header: &ModuleInfoFixed = p.get().ok()?;
        let module_name = p.strz().ok()?;
        let obj_file = p.strz().ok()?;

        // Each ModInfo structures is variable-length. It ends with two NUL-terminated strings.
        // However, the ModInfo structures have an alignment requirement, so if the strings
        // did not land us on an aligned boundary, we have to skip a few bytes to restore
        // alignment.

        // Find the number of bytes that were used for this structure.
        let mod_record_bytes = len_before - p.len();
        let alignment = (4 - (mod_record_bytes & 3)) & 3;
        p.bytes(alignment).ok()?;

        // Save iterator position.
        self.rest = p.into_rest();

        Some(ModuleInfo {
            header,
            module_name,
            obj_file,
        })
    }
}

/// Mutable iterator
pub struct IterModuleInfoMut<'a> {
    rest: &'a mut [u8],
}

impl<'a> IterModuleInfoMut<'a> {
    #[allow(missing_docs)]
    pub fn new(data: &'a mut [u8]) -> Self {
        Self { rest: data }
    }
}

impl<'a> Iterator for IterModuleInfoMut<'a> {
    type Item = ModuleInfoMut<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.rest.is_empty() {
            return None;
        }

        // TODO: note that we steal the byte slice, which means that
        // if anything goes wrong, we'll never put it back.
        let mut p = ParserMut::new(take(&mut self.rest));

        let len_before = p.len();
        let header: &mut ModuleInfoFixed = p.get_mut().ok()?;
        let module_name = p.strz().ok()?;
        let obj_file = p.strz().ok()?;

        // Each ModInfo structures is variable-length. It ends with two NUL-terminated strings.
        // However, the ModInfo structures have an alignment requirement, so if the strings
        // did not land us on an aligned boundary, we have to skip a few bytes to restore
        // alignment.

        // Find the number of bytes that were used for this structure.
        let mod_record_bytes = len_before - p.len();
        let alignment = (4 - (mod_record_bytes & 3)) & 3;
        p.bytes(alignment).ok()?;

        // Save iterator position.
        self.rest = p.into_rest();
        Some(ModuleInfoMut {
            header,
            module_name,
            obj_file,
        })
    }
}

```

`pdb/src/dbi/optional_dbg.rs`:

```rs
//! Decodes the Optional Debug Header Substream.
//!
//! This substream contains an array of stream indexes. The order of the array is significant;
//! each has a specific purpose. They are enumerated by the [`OptionalDebugHeaderStream`] type.
//!
//! # References
//! * <https://llvm.org/docs/PDB/DbiStream.html#id10>
//! * [`DBGTYPE` in `pdb.h`](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/langapi/include/pdb.h#L438)

use super::*;

/// Provides access to the Optional Debug Header.
pub struct OptionalDebugHeader<'a> {
    /// Raw access to the stream indexes
    pub stream_indexes: &'a [StreamIndexU16],
}

impl<'a> OptionalDebugHeader<'a> {
    /// Parses the Optional Debug Header Substream.
    pub fn parse(bytes: &'a [u8]) -> anyhow::Result<Self> {
        let Ok(stream_indexes) = <[StreamIndexU16]>::ref_from_bytes(bytes) else {
            bail!(
                "The OptionalDebugHeader has an invalid size. The size is required to be a multiple of 2. Size: {}",
                bytes.len()
            );
        };

        Ok(Self { stream_indexes })
    }

    /// Gets a stream index, given an index into the Optional Debug Header.
    pub fn stream_by_index(&self, i: usize) -> Option<u32> {
        self.stream_indexes.get(i)?.get()
    }

    /// Gets a stream index, given an identifier for a stream within the Optional Debug Header.
    pub fn stream(&self, s: OptionalDebugHeaderStream) -> Option<u32> {
        self.stream_by_index(s as usize)
    }

    /// The number of stream indexes in the Optional Debug Header Substream.
    pub fn num_streams(&self) -> usize {
        self.stream_indexes.len()
    }

    /// Iterates the streams within the Optional Debug Header. The iterated values are
    /// `(i, stream)` where `i` is an index into the Optional Debug Header.
    /// `OptionalDebugHeaderStream::try_from(i)`.
    pub fn iter_streams(&self) -> IterStreams<'_> {
        IterStreams {
            stream_indexes: self.stream_indexes,
            next: 0,
        }
    }
}

/// Contains the list of Optional Debug Streams.
#[derive(Clone)]
pub struct OptionalDebugHeaders {
    /// Contains stream indices, indexed by OptionalDebugStreamIndex.
    pub streams: Vec<u16>,
}

impl OptionalDebugHeaders {
    /// Iterates the Optional Debug Streams.
    pub fn iter(&self) -> impl Iterator<Item = (OptionalDebugStream, u32)> + '_ {
        self.streams.iter().enumerate().filter_map(|(i, &s)| {
            if s != NIL_STREAM_INDEX {
                Some((OptionalDebugStream(i as u32), s as u32))
            } else {
                None
            }
        })
    }
}

/// Identifies one of the Optional Debug Streams.
#[derive(Copy, Clone, Eq, PartialEq, Debug, Hash, Ord, PartialOrd)]
pub struct OptionalDebugStream(pub u32);

#[allow(missing_docs)]
impl OptionalDebugStream {
    pub const FPO_DATA: Self = Self(0);
    pub const EXCEPTION_DATA: Self = Self(1);
    pub const FIXUP_DATA: Self = Self(2);
    pub const OMAP_TO_SRC_DATA: Self = Self(3);
    pub const OMAP_FROM_SRC_DATA: Self = Self(4);
    pub const SECTION_HEADER_DATA: Self = Self(5);
    pub const TOKEN_TO_RECORD_ID_MAP: Self = Self(6);
    pub const XDATA: Self = Self(7);
    pub const PDATA: Self = Self(8);
    pub const NEW_FPO_DATA: Self = Self(9);
    pub const ORIGINAL_SECTION_HEADER_DATA: Self = Self(10);
}

impl OptionalDebugStream {
    /// Returns the name of this optional debug stream.
    pub fn name(&self) -> Option<&'static str> {
        OPTIONAL_DEBUG_HEADER_STREAM_NAME
            .get(self.0 as usize)
            .copied()
    }
}

/// Iterates streams
pub struct IterStreams<'a> {
    stream_indexes: &'a [StreamIndexU16],
    next: usize,
}

impl<'a> Iterator for IterStreams<'a> {
    type Item = (usize, u32);

    fn next(&mut self) -> Option<Self::Item> {
        while self.next < self.stream_indexes.len() {
            let i = self.next;
            let stream_index_or_nil = self.stream_indexes[i].get();
            self.next += 1;

            if let Some(stream_index) = stream_index_or_nil {
                return Some((i, stream_index));
            }
        }
        None
    }
}

macro_rules! optional_debug_header_streams {
    (
        $(
            $( #[$a:meta] )*
            $index:literal, $name:ident, $description:expr;
        )*
    ) => {
        /// Identifies the stream indexes stored in the Optional Debug Header.
        #[derive(Copy, Clone, Eq, PartialEq, Debug)]
        #[repr(u8)]
        #[allow(non_camel_case_types)]
        #[allow(missing_docs)]
        pub enum OptionalDebugHeaderStream {
            $(
                $( #[$a] )*
                $name = $index,
            )*
        }

        /// The short name (identifier) for each of the names in `OptionalDebugHeaderStream`.
        pub static OPTIONAL_DEBUG_HEADER_STREAM_NAME: [&str; 11] = [
            $(
                stringify!($name),
            )*
        ];

        /// The for each of the names in `OptionalDebugHeaderStream`.
        pub static OPTIONAL_DEBUG_HEADER_STREAM_DESCRIPTION: [&str; 11] = [
            $(
                $description,
            )*
        ];

        impl TryFrom<usize> for OptionalDebugHeaderStream {
            type Error = ();

            fn try_from(i: usize) -> std::result::Result<Self, Self::Error> {
                match i {
                    $( $index => Ok(Self::$name), )*
                    _ => Err(()),
                }
            }
        }
    }
}

optional_debug_header_streams! {
    /// Stream contains an array of `FPO_DATA` structures. This contains the relocated contents of
    /// any `.debug$F` section from any of the linker inputs.
    0, fpo_data, "";
    /// Stream contains a debug data directory of type `IMAGE_DEBUG_TYPE_EXCEPTION`.
    1, exception_data, "";
    /// Stream contains a debug data directory of type `IMAGE_DEBUG_TYPE_FIXUP`.
    2, fixup_data, "";
    /// Stream contains a debug data directory of type `IMAGE_DEBUG_TYPE_OMAP_TO_SRC`.
    /// This is used for mapping addresses from instrumented code to uninstrumented code.
    3, omap_to_src_data, "";
    /// Stream contains a debug data directory of type `IMAGE_DEBUG_TYPE_OMAP_FROM_SRC`.
    /// This is used for mapping addresses from uninstrumented code to instrumented code.
    4, omap_from_src_data, "";
    /// A dump of all section headers from the original executable.
    5, section_header_data, "";
    6, token_to_record_id_map, "";
    /// Exception handler data
    7, xdata, "";
    /// Procedure data
    8, pdata, "";
    9, new_fpo_data, "";
    10, original_section_header_data, "";
}

```

`pdb/src/dbi/section_contrib.rs`:

```rs
//! DBI Section Contribution Substream
//!
//! The Section Contributions Substream describes the COFF sections that contributed to a linked
//! binary. Section contributions come from object files that are submitted to the linker.
//!
//! The Section Contributions table is usually quite large, especially for large binaries.
//!
//! # References
//! * [`SC2` in `dbicommon.h`](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/PDB/include/dbicommon.h#L107)

use super::*;
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned};

/// Describes one section contribution.
#[allow(missing_docs)]
#[derive(Unaligned, IntoBytes, FromBytes, Immutable, KnownLayout, Clone, Debug)]
#[repr(C)]
pub struct SectionContribEntry {
    /// The section index
    pub section: U16<LE>,
    /// Alignment padding
    pub padding1: [u8; 2],
    pub offset: I32<LE>,
    pub size: I32<LE>,
    pub characteristics: U32<LE>,
    /// The zero-based module index of the module containing this section contribution.
    pub module_index: U16<LE>,
    /// Alignment padding
    pub padding2: [u8; 2],
    pub data_crc: U32<LE>,
    pub reloc_crc: U32<LE>,
}

/// Describes one section contribution.
#[allow(missing_docs)]
#[derive(Unaligned, IntoBytes, FromBytes, Immutable, KnownLayout, Clone, Debug)]
#[repr(C)]
pub struct SectionContribEntry2 {
    pub base: SectionContribEntry,
    pub coff_section: U32<LE>,
}

impl SectionContribEntry {
    /// Tests whether `offset` falls within this section contribution.
    pub fn contains_offset(&self, offset: i32) -> bool {
        let self_offset = self.offset.get();
        if offset < self_offset {
            return false;
        }

        let overshoot = offset - self_offset;
        if overshoot >= self.size.get() {
            return false;
        }

        true
    }
}

/// Decodes the Section Contribution Substream.
pub struct SectionContributionsSubstream<'a> {
    /// The array of section contributions.
    pub contribs: &'a [SectionContribEntry],
}

/// Version 6.0 of the Section Contributions Substream. This is the only supported version.
pub const SECTION_CONTRIBUTIONS_SUBSTREAM_VER60: u32 = 0xeffe0000 + 19970605;

impl<'a> SectionContributionsSubstream<'a> {
    /// Parses the header of the Section Contributions Substream.
    ///
    /// It is legal for a Section Contributions Substream to be entirely empty.
    pub fn parse(bytes: &'a [u8]) -> anyhow::Result<Self> {
        let mut p = Parser::new(bytes);
        if p.is_empty() {
            return Ok(Self { contribs: &[] });
        }

        let version = p.u32()?;

        match version {
            SECTION_CONTRIBUTIONS_SUBSTREAM_VER60 => {}
            _ => {
                bail!(
                    "The Section Contributions Substream has a version number that is not supported. Version: 0x{:08x}",
                    version
                );
            }
        }

        let records_bytes = p.into_rest();
        let Ok(contribs) = <[SectionContribEntry]>::ref_from_bytes(records_bytes) else {
            bail!(
                "The Section Contributions stream has an invalid size. It is not a multiple of the section contribution record size.  Size: 0x{:x}",
                bytes.len()
            );
        };
        Ok(SectionContributionsSubstream { contribs })
    }

    /// Searches for a section contribution that contains the given offset.
    /// The `section` must match exactly. This uses binary search.
    pub fn find(&self, section: u16, offset: i32) -> Option<&SectionContribEntry> {
        let i = self.find_index(section, offset)?;
        Some(&self.contribs[i])
    }

    /// Searches for the index of a section contribution that contains the given offset.
    /// The `section` must match exactly. This uses binary search.
    pub fn find_index(&self, section: u16, offset: i32) -> Option<usize> {
        match self
            .contribs
            .binary_search_by_key(&(section, offset), |con| {
                (con.section.get(), con.offset.get())
            }) {
            Ok(i) => Some(i),
            Err(i) => {
                // We didn't find it, but i is close to it.
                if i > 0 {
                    let previous = &self.contribs[i - 1];
                    if previous.contains_offset(offset) {
                        return Some(i - 1);
                    }
                }

                if i + 1 < self.contribs.len() {
                    let next = &self.contribs[i + 1];
                    if next.contains_offset(offset) {
                        return Some(i + 1);
                    }
                }

                None
            }
        }
    }

    /// Searches for a section contribution that contains the given offset.
    /// The `section` must match exactly. This uses sequential scan (brute force).
    pub fn find_brute(&self, section: u16, offset: i32) -> Option<&SectionContribEntry> {
        let i = self.find_index_brute(section, offset)?;
        Some(&self.contribs[i])
    }

    /// Searches for the index of a section contribution that contains the given offset.
    /// The `section` must match exactly. This uses sequential scan (brute force).
    pub fn find_index_brute(&self, section: u16, offset: i32) -> Option<usize> {
        self.contribs
            .iter()
            .position(|c| c.section.get() == section && c.contains_offset(offset))
    }
}

/// Decodes the Section Contribution Substream.
pub struct SectionContributionsSubstreamMut<'a> {
    /// The array of section contributions.
    pub contribs: &'a mut [SectionContribEntry],
}

impl<'a> SectionContributionsSubstreamMut<'a> {
    /// Parses the header of the Section Contributions Substream.
    pub fn parse(bytes: &'a mut [u8]) -> anyhow::Result<Self> {
        let bytes_len = bytes.len();

        let mut p = ParserMut::new(bytes);
        if p.is_empty() {
            return Ok(Self { contribs: &mut [] });
        }

        let version = p.u32()?;

        match version {
            SECTION_CONTRIBUTIONS_SUBSTREAM_VER60 => {}
            _ => {
                bail!(
                    "The Section Contributions Substream has a version number that is not supported. Version: 0x{:08x}",
                    version
                );
            }
        }

        let records_bytes = p.into_rest();

        let Ok(contribs) = <[SectionContribEntry]>::mut_from_bytes(records_bytes) else {
            bail!(
                "The Section Contributions stream has an invalid size. It is not a multiple of the section contribution record size.  Size: 0x{:x}",
                bytes_len
            );
        };
        Ok(Self { contribs })
    }

    /// Given a lookup table that maps module indexes from old to new, this edits a
    /// Section Contributions table and converts module indexes.
    pub fn remap_module_indexes(&mut self, modules_old_to_new: &[u32]) -> anyhow::Result<()> {
        for (i, contrib) in self.contribs.iter_mut().enumerate() {
            let old = contrib.module_index.get();
            if let Some(&new) = modules_old_to_new.get(old as usize) {
                contrib.module_index.set(new as u16);
            } else {
                bail!(
                    "Section contribution record (at contribution index #{i} has module index {old}, \
                       which is out of range (num modules is {})",
                    modules_old_to_new.len()
                );
            }

            // While we're at it, make sure that the padding fields are cleared.
            contrib.padding1 = [0; 2];
            contrib.padding2 = [0; 2];
        }
        Ok(())
    }
}

```

`pdb/src/dbi/section_map.rs`:

```rs
//! DBI Section Map Substream
#![allow(missing_docs)]

use super::*;
use bitflags::bitflags;
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned};

#[derive(IntoBytes, KnownLayout, Immutable, FromBytes, Unaligned)]
#[repr(C)]
pub struct SectionMapHeader {
    /// Total number of segment descriptors
    pub num_segments: U16<LE>,
    /// Number of logical segment descriptors
    pub num_logical_segments: U16<LE>,
}

#[derive(IntoBytes, KnownLayout, Immutable, FromBytes, Unaligned, Debug)]
#[repr(C)]
pub struct SectionMapEntry {
    /// Descriptor flags bit field. See `SectionMapEntryFlags`.
    pub flags: U16<LE>,
    /// The logical overlay number
    pub overlay: U16<LE>,
    /// Group index into the descriptor array
    pub group: U16<LE>,
    /// Logical segment index, interpreted via flags
    pub frame: U16<LE>,
    /// Byte index of segment / group name in string table, or 0xFFFF.
    pub section_name: U16<LE>,
    /// Byte index of class in string table, or 0xFFFF.
    pub class_name: U16<LE>,
    /// Byte offset of the logical segment within physical segment.
    /// If group is set in flags, this is the offset of the group.
    pub offset: U32<LE>,
    /// Byte count of the segment or group.
    pub section_length: U32<LE>,
}

bitflags! {
    #[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
    pub struct SectionMapEntryFlags: u16 {
        /// Segment is readable.
        const READ = 1 << 0;
        /// Segment is writable.
        const WRITE = 1 << 1;
        /// Segment is executable.
        const EXECUTE = 1 << 2;
        /// Descriptor describes a 32-bit linear address.
        const ADDRESS_IS32_BIT = 1 << 3;
        /// Frame represents a selector.
        const IS_SELECTOR = 1 << 8;
        /// Frame represents an absolute address.
        const IS_ABSOLUTE_ADDRESS = 1 << 9;
        /// If set, descriptor represents a group. (obsolete)
        const IS_GROUP = 1 << 10;
    }
}

pub struct SectionMap<'a> {
    pub header: SectionMapHeader,
    pub entries: &'a [SectionMapEntry],
}

impl<'a> SectionMap<'a> {
    pub fn parse(bytes: &'a [u8]) -> anyhow::Result<Self> {
        let mut p = Parser::new(bytes);
        if p.is_empty() {
            return Ok(Self {
                entries: &[],
                header: SectionMapHeader {
                    num_logical_segments: U16::ZERO,
                    num_segments: U16::ZERO,
                },
            });
        }

        let header: SectionMapHeader = p.copy()?;

        let Ok(entries) = <[SectionMapEntry]>::ref_from_bytes(p.take_rest()) else {
            bail!(
                "Section map has invalid length (is not a multiple of SectionMapEntry size). Length (including 4-byte header): 0x{:x}",
                bytes.len()
            );
        };
        Ok(Self { header, entries })
    }
}

```

`pdb/src/dbi/sources.rs`:

```rs
//! DBI Sources Substream

use super::*;
use crate::BStr;
use std::collections::HashMap;

/// The "Sources" substream of the DBI stream. This stream describes the merged set of source
/// files that were the inputs (compilands) of all modules.
///
/// See: <https://llvm.org/docs/PDB/DbiStream.html#file-info-substream>
pub struct DbiSourcesSubstream<'a> {
    /// The `module_file_starts` array gives the index within `file_name_offsets` where the file
    /// names for each module begin. That is, `file_name_offsets[module_file_starts[m]]` is the file
    /// name offset for the first file in the set of files for module `m`.
    ///
    /// When combined with the `module_file_counts` array, you can easily find the slice within
    /// `file_name_offsets` of files for a specific module.
    ///
    /// The length of this slice is equal to `num_modules`. This slice _does not_ have an extra
    /// entry at the end, so you must use `file_name_offsets.len()` as the end of the per-module
    /// slice for the last entry in this slice.
    module_file_starts: &'a [U16<LE>],

    /// For each module, gives the number of source files that contribute to that module.
    module_file_counts: &'a [U16<LE>],

    /// Contains the concatenated list of file name lists, one list per module. For each module
    /// `m`, the set of items within `file_name_offsets` is given by
    /// `file_name_offsets[module_file_starts[m]..][..module_file_counts[m]]`.
    ///
    /// Each item in this list is an offset into `names_buffer` and points to the start of a
    /// NUL-terminated UTF-8 string.
    ///
    /// This array can (and usually does) contain duplicate values. The values are ordered by the
    /// module which referenced a given set of source files. Since many modules will read a shared
    /// set of header files (e.g. `windows.h`), those shared header files will appear many times
    /// in this list.
    ///
    /// The length of `file_name_offsets` is usually higher than the number of _unique_ source files
    /// because many source files (header files) are referenced by more than one module.
    ///
    /// The length of this slice is equal to the sum of the values in the `module_file_counts`.
    /// The on-disk file format stores a field that counts the number of source files, but the field
    /// is only 16-bit, so it can easily overflow on large executables. That is why this
    /// value is computed when the substream is parsed, instead of using the on-disk version.
    file_name_offsets: &'a [U32<LE>],

    /// Contains the file name strings, encoded in UTF-8 and NUL-terminated.
    names_buffer: &'a [u8],
}

impl<'a> DbiSourcesSubstream<'a> {
    /// The number of modules
    pub fn num_modules(&self) -> usize {
        self.module_file_starts.len()
    }

    /// Provides access to the file name offsets slice. Each value is a file name offset, and can
    /// be used with `get_source_name_at()`.
    pub fn file_name_offsets(&self) -> &'a [U32<LE>] {
        self.file_name_offsets
    }

    /// Parses the file info substream.
    ///
    /// This does not parse or validate every part of the substream. It only parses enough to find
    /// the module indices and file names.
    pub fn parse(substream_data: &'a [u8]) -> anyhow::Result<Self> {
        let mut p = Parser::new(substream_data);
        let num_modules = p.u16()? as usize;

        // In theory this is supposed to contain the number of source files for which this substream
        // contains information. But that would present a problem in that the width of this field
        // being 16-bits would prevent one from having more than 64K source files in a program. In
        // early versions of the file format, this seems to have been the case. In order to support
        // more than this, this field of the is simply ignored, and computed dynamically by summing
        // up the values of the ModFileCounts array (discussed below).
        //
        // In short, this value should be ignored. However, we still have to read the value in
        // order to parse the header correctly.
        let _obsolete_num_source_files = p.u16()? as usize;

        let module_file_starts: &[U16<LE>] = p.slice(num_modules)?;

        // An array of num_modules integers, each one containing the number of source files which
        // contribute to the module at the specified index. While each individual module is limited
        // to 64K contributing source files, the union of all modules' source files may be greater
        // than 64K. The real number of source files is thus computed by summing this array.
        //
        // Note that summing this array does not give the number of _unique source files_, only the
        // total number of source file contributions to modules.
        let module_file_counts: &[U16<LE>] = p.slice(num_modules)?;

        let num_file_offsets = module_file_counts.iter().map(|c| c.get() as usize).sum();

        // At this point, we could scan module_file_starts + module_file_counts and validate that
        // no entry exceeds num_file_offsets.

        let file_name_offsets = p.slice(num_file_offsets)?;
        let names_buffer = p.into_rest();

        Ok(Self {
            module_file_starts,
            module_file_counts,
            file_name_offsets,
            names_buffer,
        })
    }

    /// Given a source file index, returns the source file name.
    pub fn get_source_file_name(&self, source_file_index: usize) -> Result<&'a BStr, ParserError> {
        let offset = self.file_name_offsets[source_file_index].get();
        self.get_source_file_name_at(offset)
    }

    /// Given a file name offset (within `name_buffer`), returns the source file name.
    pub fn get_source_file_name_at(&self, file_name_offset: u32) -> Result<&'a BStr, ParserError> {
        let Some(string_data) = self.names_buffer.get(file_name_offset as usize..) else {
            return Err(ParserError);
        };
        let mut p = Parser::new(string_data);
        let file_name = p.strz()?;
        Ok(file_name)
    }

    /// Caller is expected to validate module_index (against `num_modules()`) before calling
    pub fn name_offsets_for_module(&self, module_index: usize) -> anyhow::Result<&[U32<LE>]> {
        let start = self.module_file_starts[module_index].get() as usize;
        let count = self.module_file_counts[module_index].get() as usize;
        let Some(s) = self.file_name_offsets.get(start..start + count) else {
            bail!(
                "File name offsets for module #{module_index} are invalid.  start: {start}, count: {count}, len available: {}",
                self.file_name_offsets.len()
            );
        };
        Ok(s)
    }

    /// Iterates source files in the DBI Sources Substream.
    pub fn iter_sources(&self) -> IterSources<'_> {
        IterSources {
            names_buffer: self.names_buffer,
            file_name_offsets: self.file_name_offsets.iter(),
        }
    }

    /// Builds a HashMap that maps from file name offsets to strings.
    pub fn sources_map(&self) -> anyhow::Result<HashMap<u32, &BStr>> {
        let mut unique_offsets: Vec<u32> = self.file_name_offsets.iter().map(|i| i.get()).collect();
        unique_offsets.sort_unstable();
        unique_offsets.dedup();

        let mut map = HashMap::new();
        for &offset in unique_offsets.iter() {
            let name = self.get_source_file_name_at(offset)?;
            map.insert(offset, name);
        }

        Ok(map)
    }
}

/// Iterates source files in the DBI Sources Substream.
pub struct IterSources<'a> {
    names_buffer: &'a [u8],
    file_name_offsets: std::slice::Iter<'a, U32<LE>>,
}

impl<'a> Iterator for IterSources<'a> {
    /// name_offset (in bytes), name
    type Item = (u32, &'a BStr);

    fn next(&mut self) -> Option<Self::Item> {
        let offset = self.file_name_offsets.next()?.get();
        let mut p = Parser::new(self.names_buffer);
        p.skip(offset as usize).ok()?;
        let name = p.strz().ok()?;
        Some((offset, name))
    }
}

#[cfg(test)]
#[rustfmt::skip]
static TEST_SOURCES_DATA: &[u8] = &[
    /* 0x0000 */ 4, 0,                     // num_modules = 4
    /* 0x0004 */ 0xee, 0xee,               // obsolete num_sources (bogus)
    /* 0x0008 */ 0, 0, 2, 0, 3, 0, 3, 0,   // module_file_starts = [0, 2, 3, 3]
    /* 0x0010 */ 2, 0, 1, 0, 0, 0, 3, 0,   // module_file_counts = [2, 1, 0, 3] sum = 6

    /* 0x0018 */                           // file_offsets, len = 6 items, 24 bytes
    /* 0x0018 */ 0x00, 0, 0, 0,            // module 0, file_offsets[0] = 0x00, points to "foo.c",
    /* 0x0018 */ 0x14, 0, 0, 0,            // module 0, file_offsets[1] = 0x14, points to "windows.h"
    /* 0x0018 */ 0x06, 0, 0, 0,            // module 1, file_offsets[2] = 0x06, points to "bar.rs"
    /* 0x0018 */ 0x00, 0, 0, 0,            // module 3, file_offsets[3] = 0x00, points to "foo.c"
    /* 0x0018 */ 0x14, 0, 0, 0,            // module 3, file_offsets[4] = 0x14, points to "windows.h"
    /* 0x0018 */ 0x0d, 0, 0, 0,            // module 3, file_offsets[5] = 0x0d, points to "main.c"

    // names_buffer; contains (at relative offsets):
    //      name offset 0x0000 : "foo.c"
    //      name offset 0x0006 : "bar.rs"
    //      name offset 0x000d : "main.c"
    //      name offset 0x0014 : "windows.h"
    /* 0x0030 */                                // names_buffer
    /* 0x0030 */ b'f', b'o', b'o', b'.',
    /* 0x0034 */ b'c', 0,    b'b', b'a',
    /* 0x0038 */ b'r', b'.', b'r', b's',
    /* 0x003c */ 0,    b'm', b'a', b'i',
    /* 0x0040 */ b'n', b'.', b'c', 0,
    /* 0x0044 */ b'w', b'i', b'n', b'd',
    /* 0x0048 */ b'o', b'w', b's', b'.',
    /* 0x004c */ b'h', 0,    0,    0,

    /* 0x0050 : end */
];

#[test]
fn basic_parse() {
    let s = DbiSourcesSubstream::parse(TEST_SOURCES_DATA).unwrap();
    assert_eq!(s.num_modules(), 4);

    assert_eq!(s.file_name_offsets.len(), 6);

    let module_file_starts: Vec<u16> = s.module_file_starts.iter().map(|x| x.get()).collect();
    assert_eq!(&module_file_starts, &[0, 2, 3, 3]);

    let module_file_counts: Vec<u16> = s.module_file_counts.iter().map(|x| x.get()).collect();
    assert_eq!(&module_file_counts, &[2, 1, 0, 3]);

    let file_name_offsets: Vec<u32> = s.file_name_offsets.iter().map(|x| x.get()).collect();
    assert_eq!(&file_name_offsets, &[0x00, 0x14, 0x06, 0x00, 0x14, 0x0d]);

    // Read the file names. Remember that there are duplicates in this list.
    assert_eq!(s.get_source_file_name(0).unwrap(), "foo.c");
    assert_eq!(s.get_source_file_name(1).unwrap(), "windows.h");
    assert_eq!(s.get_source_file_name(2).unwrap(), "bar.rs");
    assert_eq!(s.get_source_file_name(3).unwrap(), "foo.c");
    assert_eq!(s.get_source_file_name(4).unwrap(), "windows.h");
    assert_eq!(s.get_source_file_name(5).unwrap(), "main.c");

    let modsrcs0 = s.name_offsets_for_module(0).unwrap();
    assert_eq!(modsrcs0.len(), 2);
    assert_eq!(modsrcs0[0].get(), 0);
    assert_eq!(modsrcs0[1].get(), 0x14);

    // Test bounds check on get_source_file_name_at()
    assert!(s.get_source_file_name_at(0xeeee).is_err());
}

#[test]
fn test_iter_sources() {
    use bstr::ByteSlice;

    let s = DbiSourcesSubstream::parse(TEST_SOURCES_DATA).unwrap();

    let sources: Vec<(u32, &str)> = s
        .iter_sources()
        .map(|(i, s)| (i, s.to_str().unwrap()))
        .collect();

    assert_eq!(
        &sources,
        &[
            (0x00, "foo.c"),
            (0x14, "windows.h"),
            (0x06, "bar.rs"),
            (0x00, "foo.c"),
            (0x14, "windows.h"),
            (0x0d, "main.c"),
        ]
    );
}

#[test]
fn test_sources_map() {
    let s = DbiSourcesSubstream::parse(TEST_SOURCES_DATA).unwrap();
    let map = s.sources_map().unwrap();
    assert_eq!(map.len(), 4); // 4 unique file names
    assert_eq!(*map.get(&0x00).unwrap(), "foo.c");
    assert_eq!(*map.get(&0x06).unwrap(), "bar.rs");
    assert_eq!(*map.get(&0x0d).unwrap(), "main.c");
    assert_eq!(*map.get(&0x14).unwrap(), "windows.h");
}

```

`pdb/src/embedded_sources.rs`:

```rs
use super::*;
use anyhow::Result;
use std::io::Write;

impl<F: ReadAt> Pdb<F> {
    /// Embeds the contents of a source file into the PDB.
    pub fn add_embedded_source(&mut self, file_path: &str, file_contents: &[u8]) -> Result<bool>
    where
        F: WriteAt,
    {
        let stream_name = format!("/src/{file_path}");
        self.add_or_replace_named_stream(&stream_name, file_contents)
    }

    /// Sets the contents of a named stream to the given value.
    ///
    /// If there is already a named stream with the given name, then the stream's contents
    /// are replaced with `stream_contents`.  First, though, this function reads the contents of
    /// the existing stream and compares them to `stream_contents`. If they are identical, then
    /// the stream is not modified and this function will return `Ok(true)`.  If the contents are
    /// not identical, then this function returns `Ok(true)`.
    ///
    /// If there is not already a named stream with given name, then a new stream is created
    /// and an entry is added to the Named Streams Map. In this case, the function returns
    /// `Ok(false)`.
    pub fn add_or_replace_named_stream(
        &mut self,
        stream_name: &str,
        stream_contents: &[u8],
    ) -> Result<bool>
    where
        F: WriteAt,
    {
        if let Some(existing_stream) = self.named_streams().get(stream_name) {
            // No need to update the named stream directory.

            // Are the stream contents identical?
            let existing_len = self.stream_len(existing_stream);
            if existing_len == stream_contents.len() as u64 {
                let existing_contents = self.read_stream_to_vec(existing_stream)?;
                if existing_contents == stream_contents {
                    return Ok(false);
                }
            }

            let mut w = self.msf_mut_err()?.write_stream(existing_stream)?;
            w.set_len(0)?;
            w.write_all(stream_contents)?;
            Ok(true)
        } else {
            let (new_stream, mut w) = self.msf_mut_err()?.new_stream()?;
            w.write_all(stream_contents)?;
            self.named_streams_mut().insert(stream_name, new_stream);
            Ok(true)
        }
    }
}

```

`pdb/src/globals.rs`:

```rs
//! Global Symbols
//!
//! This module contains code for reading the public / global symbol streams. This is a
//! moderately-complicated set of data structures, and requires reading several streams and
//! correlating data between them.
//!
//! Global symbols are stored in several streams. The stream indexes are stored in the DBI
//! stream header; the stream indexes are not fixed.

pub mod gsi;
pub mod gss;
pub mod name_table;
pub mod psi;

#[cfg(test)]
mod tests;

use crate::ReadAt;
use crate::syms::{self, Constant, OffsetSegment, Pub, SymIter, SymKind};
use anyhow::Context;
use bstr::BStr;
use ms_codeview::IteratorWithRangesExt;
use ms_codeview::parser::{Parse, ParserError};
use std::collections::HashMap;
use tracing::{debug, warn};

#[cfg(doc)]
use crate::dbi::DbiStreamHeader;

impl<F: ReadAt> crate::Pdb<F> {
    /// Reads the Global Symbol Stream (GSS). This stream contains global symbol records.
    ///
    /// This function does not validate the contents of the stream.
    pub fn read_gss(&self) -> anyhow::Result<gss::GlobalSymbolStream> {
        if let Some(gss_stream) = self.dbi_header.global_symbol_stream.get() {
            let stream_data = self.read_stream_to_vec(gss_stream)?;
            Ok(gss::GlobalSymbolStream { stream_data })
        } else {
            Ok(gss::GlobalSymbolStream::empty())
        }
    }

    /// Reads the Global Symbol Index (GSI). This stream contains a name-to-symbol lookup table.
    /// It indexes many global symbols, such as `S_GPROCREF`, `S_CONSTANT`, etc.
    pub fn read_gsi(&self) -> anyhow::Result<gsi::GlobalSymbolIndex> {
        if let Some(gsi_stream) = self.dbi_header.global_symbol_index_stream.get() {
            let num_buckets = self.num_buckets_for_name_table();
            let gsi_stream_data = self.read_stream_to_vec(gsi_stream)?;
            gsi::GlobalSymbolIndex::parse(num_buckets, gsi_stream_data)
        } else {
            Ok(gsi::GlobalSymbolIndex::empty())
        }
    }

    /// Returns the number of buckets to use in `NameTable`, for use by the GSI and PSI.
    pub(crate) fn num_buckets_for_name_table(&self) -> usize {
        let minimal_dbg_info = self.mini_pdb();
        name_table::get_v1_default_bucket(minimal_dbg_info)
    }

    /// Reads the Public Symbol Index.
    pub fn read_psi(&self) -> anyhow::Result<psi::PublicSymbolIndex> {
        if let Ok(psi_stream) = self.dbi_header.public_stream_index() {
            let num_buckets = self.num_buckets_for_name_table();
            let public_stream_data = self.read_stream_to_vec(psi_stream)?;
            psi::PublicSymbolIndex::parse(num_buckets, public_stream_data)
        } else {
            Ok(psi::PublicSymbolIndex::empty())
        }
    }
}

/// If `kind` is a global symbol that should be indexed in the GSI or PSI, then this returns the
/// name of that global symbol (within `Some`).
///
/// A "global symbol" in this context is any symbol that can appear in the Global Symbol Stream
/// and be indexed in the Global Symbol Index or Public Symbol Index. The list of global symbols:
///
/// * `S_PUB32`
/// * `S_CONSTANT`
/// * `S_PROCREF`
/// * `S_LPROCREF`
/// * `S_DATAREF`
/// * `S_ANNOTATIONREF`
/// * `S_UDT`
/// * `S_LDATA32`
/// * `S_GDATA32`
/// * `S_LTHREAD32`
/// * `S_GTHREAD32`
pub fn get_global_symbol_name(kind: SymKind, data: &[u8]) -> Result<Option<&BStr>, ParserError> {
    match kind {
        SymKind::S_PUB32 => {
            let pub_data = Pub::parse(data)?;
            Ok(Some(pub_data.name))
        }

        SymKind::S_CONSTANT => {
            let constant_record = Constant::parse(data)?;
            Ok(Some(constant_record.name))
        }

        // These symbols have the same structure.
        SymKind::S_PROCREF
        | SymKind::S_LPROCREF
        | SymKind::S_DATAREF
        | SymKind::S_ANNOTATIONREF => {
            let ref_sym = syms::RefSym2::parse(data)?;
            Ok(Some(ref_sym.name))
        }

        SymKind::S_UDT => {
            let udt_data = syms::Udt::parse(data)?;
            Ok(Some(udt_data.name))
        }

        SymKind::S_LDATA32 | SymKind::S_GDATA32 | SymKind::S_LMANDATA | SymKind::S_GMANDATA => {
            let data = syms::Data::parse(data)?;
            Ok(Some(data.name))
        }

        SymKind::S_LTHREAD32 | SymKind::S_GTHREAD32 => {
            let thread_storage = syms::ThreadStorageData::parse(data)?;
            Ok(Some(thread_storage.name))
        }

        SymKind::S_LMANPROC | SymKind::S_GMANPROC => {
            let man_proc = syms::ManProcSym::parse(data)?;
            Ok(Some(man_proc.name))
        }

        // TODO
        SymKind::S_TOKENREF => Ok(None),

        _ => Ok(None),
    }
}

/// Output of `build_global_symbols_index`
pub struct BuildGlobalSymbolsIndexesOutput {
    /// The new GSI contents
    pub global_symbol_index_stream_data: Vec<u8>,
    /// The new PSI contents
    pub public_symbol_index_stream_data: Vec<u8>,
}

/// Reads a Global Symbol Stream and constructs a new Global Symbol Index (GSI) and
/// Public Symbol Index (PSI).
pub fn build_global_symbols_index(
    symbol_records: &[u8],
    num_buckets: usize,
) -> anyhow::Result<BuildGlobalSymbolsIndexesOutput> {
    debug!("Rebuilding Global Symbol Index (GSI) and Public Symbol Index (PSI)");

    let mut public_hash_records = name_table::NameTableBuilder::new(num_buckets);
    let mut global_hash_records = name_table::NameTableBuilder::new(num_buckets);

    // contains (byte offset in symbol stream, SegmentOffset)
    let mut public_addr_map: Vec<(u32, OffsetSegment)> = Vec::new();

    let mut unrecognized_symbols: HashMap<SymKind, u32> = HashMap::new();

    for (sym_range, sym) in SymIter::new(symbol_records).with_ranges() {
        let sym_offset = sym_range.start;

        // If the symbol is S_PUB32, then add an entry to both public_hash_records and
        // global_hash_records.
        if sym.kind == SymKind::S_PUB32 {
            let pub_data =
                Pub::parse(sym.data).with_context(|| "failed to parse S_PUB32 record")?;
            public_hash_records.push(pub_data.name, (sym_offset + 1) as i32);
            public_addr_map.push((sym_offset as u32, pub_data.offset_segment()));
            continue;
        }

        if matches!(sym.kind, SymKind::S_TOKENREF | SymKind::S_DATAREF) {
            continue;
        }

        if let Some(sym_name) = get_global_symbol_name(sym.kind, sym.data)? {
            global_hash_records.push(sym_name, (sym_offset + 1) as i32);
        } else {
            *unrecognized_symbols.entry(sym.kind).or_default() += 1;
        }
    }

    if !unrecognized_symbols.is_empty() {
        warn!(
            "Number of unrecognized symbol types found in Global Symbol Stream: {}",
            unrecognized_symbols.len()
        );
        let mut sorted_unrecognized: Vec<(SymKind, u32)> =
            unrecognized_symbols.iter().map(|(&k, &v)| (k, v)).collect();
        sorted_unrecognized.sort_unstable_by_key(|(k, _)| *k);
        for (kind, count) in sorted_unrecognized.iter() {
            warn!(
                "    {count:6} - [{raw_kind:04x}] {kind:?}",
                raw_kind = kind.0
            );
        }
    }

    psi::sort_address_records(&mut public_addr_map);

    debug!("Building Global Symbol Index (GSI)");
    let global_symbol_stream_data = gsi::build_gsi(&mut global_hash_records);

    debug!("Building Public Symbol Index (PSI)");
    let public_symbol_stream_data = psi::build_psi(&mut public_hash_records, &public_addr_map);

    Ok(BuildGlobalSymbolsIndexesOutput {
        global_symbol_index_stream_data: global_symbol_stream_data,
        public_symbol_index_stream_data: public_symbol_stream_data,
    })
}

```

`pdb/src/globals/gsi.rs`:

```rs
//! Global Symbol Index
//!
//! The Global Symbol Index (GSI) Stream provides a name-to-symbol lookup table for global symbols
//! that have a name.
//!
//! The GSI does not have a fixed stream number. The stream number is found in the DBI Stream
//! Header.
//!
//! The GSI contains entries only for the following symbol kinds:
//!
//! * `S_CONSTANT`
//! * `S_UDT`
//! * `S_LDATA32`
//! * `S_GDATA32`
//! * `S_LTHREAD32`
//! * `S_GTHREAD32`
//! * `S_LMANDATA`
//! * `S_GMANDATA`
//! * `S_PROCREF`
//! * `S_LPROCREF`
//! * `S_ANNOTATIONREF`
//! * `S_TOKENREF`
//!
//! Note that `S_PUB32` is not included in this list.  `S_PUB32` symbols are indexed in the PSI, not
//! the GSI.
//!
//! The GSI does not provide an address-to-name lookup table.

use super::name_table::*;
use crate::syms::Sym;
use crate::utils::is_aligned_4;
use bstr::BStr;
use std::mem::size_of;
use tracing::{debug, trace_span};

/// Contains the Global Symbol Index
pub struct GlobalSymbolIndex {
    name_table: NameTable,
}

impl GlobalSymbolIndex {
    /// Parses the Global Symbol Index from stream data. The caller must specify `num_buckets`
    /// because the value is not specified in the GSI itself.
    pub fn parse(num_buckets: usize, stream_data: Vec<u8>) -> anyhow::Result<GlobalSymbolIndex> {
        if stream_data.is_empty() {
            return Ok(Self::empty());
        }

        let name_table = NameTable::parse(num_buckets, 0, &stream_data)?;
        Ok(Self { name_table })
    }

    /// Constructs an empty instance of the GSI.
    pub fn empty() -> Self {
        Self {
            name_table: NameTable::empty(),
        }
    }

    /// Find a symbol within the GSI by name.
    pub fn find_symbol<'a>(
        &self,
        gss: &'a crate::globals::gss::GlobalSymbolStream,
        name: &BStr,
    ) -> anyhow::Result<Option<Sym<'a>>> {
        self.name_table.find_symbol(gss, name)
    }

    /// Gets direct access to the name-to-symbol table.
    pub fn names(&self) -> &NameTable {
        &self.name_table
    }
}

/// Builds the Global Symbol Index (GSI) table.
///
/// The GSI contains only a name table. It does not contain an address table.
pub fn build_gsi(sorted_hash_records: &mut NameTableBuilder) -> Vec<u8> {
    let _span = trace_span!("build_gsi").entered();

    let name_table_info = sorted_hash_records.prepare();
    let mut stream_data: Vec<u8> = vec![0; name_table_info.table_size_bytes];
    sorted_hash_records.encode(&name_table_info, &mut stream_data);

    // Make it easy to understand the output.
    {
        let mut pos = 0;
        let mut region = |name: &str, len: usize| {
            debug!("    {pos:08x} +{len:08x} : {name}");
            pos += len;
        };
        debug!("GSI Stream layout:");
        region("Name Table - Header", size_of::<NameTableHeader>());
        region(
            "Name Table - Hash Records",
            sorted_hash_records.num_names() * size_of::<HashRecord>(),
        );
        region(
            "Name Table - Buckets Bitmap",
            nonempty_bitmap_size_bytes(sorted_hash_records.num_buckets()),
        );
        region(
            "Name Table - Buckets",
            name_table_info.num_nonempty_buckets * 4,
        );
        region("(end)", 0);
        assert_eq!(pos, stream_data.len());
    }

    assert!(is_aligned_4(stream_data.len()));

    stream_data
}

```

`pdb/src/globals/gss.rs`:

```rs
//! Global Symbol Stream
//!
//! The Global Symbol Stream contains a sequence of variable-length symbol records. This stream does
//! not have a header of any kind; all of the stream data consists of CodeView symbol records.
//!
//! The GSS does not have a fixed stream number. The stream number is found in the DBI Stream
//! Header.
//!
//! Many other parts of the PDB contain pointers (byte offsets) that point into the GSS:
//! * PSI: Contains lookup tables for `S_PUB32` symbols
//! * GSI: Contains a lookup table for all other named global symbols
//! * Module Streams: Contains a Global Refs section that points to entries in the GSS that are
//!   referenced by that module.

use crate::syms::{Pub, SymIter, SymKind};
use anyhow::bail;
use ms_codeview::parser::Parse;

/// Contains the Global Symbol Stream (GSS). This contains symbol records.
///
/// The GSI and the PSI both point into this stream.
pub struct GlobalSymbolStream {
    /// Contains the stream data.
    pub stream_data: Vec<u8>,
}

impl GlobalSymbolStream {
    /// Constructor. This does not validate the contents.
    pub fn new(stream_data: Vec<u8>) -> Self {
        Self { stream_data }
    }

    /// Constructs an empty GSS.
    pub fn empty() -> Self {
        Self {
            stream_data: vec![],
        }
    }

    /// Gets a reference to a symbol at a given record offset.
    ///
    /// This function validates `record_offset`. If it is out of range, this function will return
    /// `Err` instead of panicking.
    pub fn get_sym_at(&self, record_offset: u32) -> anyhow::Result<crate::syms::Sym<'_>> {
        let Some(record_bytes) = self.stream_data.get(record_offset as usize..) else {
            bail!("Invalid record offset into GSS: {record_offset}.  Out of range for the GSS.");
        };

        let mut sym_iter = SymIter::new(record_bytes);
        let Some(sym) = sym_iter.next() else {
            bail!(
                "Invalid record offset into GSS: {record_offset}. Failed to decode symbol data at that offset."
            );
        };

        Ok(sym)
    }

    /// Gets a reference to a symbol at a given record offset, and then parses it as an `S_PUB32`
    /// record.
    ///
    /// This function validates `record_offset`. If it is out of range, this function will return
    /// `Err` instead of panicking.
    ///
    /// If the symbol at `record_offset` is not an `S_PUB32` symbol, this function returns `Err`.
    pub fn get_pub32_at(&self, record_offset: u32) -> anyhow::Result<Pub<'_>> {
        let sym = self.get_sym_at(record_offset)?;

        if sym.kind != SymKind::S_PUB32 {
            bail!(
                "Invalid record offset into GSS: {record_offset}. Found a symbol with the wrong type.  Expected S_PUB32, found {:?}",
                sym.kind
            );
        };

        let Ok(pub_sym) = crate::syms::Pub::parse(sym.data) else {
            bail!(
                "Invalid record offset into GSS: {record_offset}. Failed to decode S_PUB32 record."
            );
        };

        Ok(pub_sym)
    }

    /// Iterates the symbol records in the Global Symbol Stream.
    pub fn iter_syms(&self) -> SymIter<'_> {
        SymIter::new(&self.stream_data)
    }
}

```

`pdb/src/globals/name_table.rs`:

```rs
//! Name-to-Symbol Lookup Table

#[cfg(test)]
mod tests;

use super::gss::GlobalSymbolStream;
use crate::syms::Sym;
use anyhow::bail;
use bitvec::prelude::{BitSlice, Lsb0};
use bstr::BStr;
use ms_codeview::parser::{Parser, ParserMut};
use std::mem::size_of;
use tracing::{debug, debug_span, error, info, trace, warn};
use zerocopy::{FromBytes, I32, Immutable, IntoBytes, KnownLayout, LE, U32, Unaligned};

/// This is the size used for calculating hash indices. It was the size of the in-memory form
/// of the hash records, on 32-bit machines. It is not the same as the length of the hash records
/// that are stored on disk (which is 8).
pub const HASH_RECORD_CALC_LEN: i32 = 12;

// section contribution structure
/// section contribution version, before V70 there was no section version
pub const GSI_HASH_SC_IMPV_V70: u32 = 0xeffe0000 + 19990810;

/// Signature value for `NameTableHeader::ver_signature`.
pub const GSI_HASH_HEADER_SIGNATURE: u32 = 0xffff_ffff;
/// The current value to use (implementation version) for the GSI Hash.
pub const GSI_HASH_HEADER_VERSION: u32 = GSI_HASH_SC_IMPV_V70;

/// This header is present at the start of the Name Table.
///
/// Immediately after this header follows the hash records, whose length is given by
/// `cb_hash_records`. This section contains an array of [`HashRecord`] structs.
///
/// Immediately after the hash records is the hash buckets, whose length is given by `cb_buckets`.
///
/// Called `GSIHashHdr` in C++.
#[derive(IntoBytes, FromBytes, Unaligned, KnownLayout, Immutable, Debug)]
#[repr(C)]
pub struct NameTableHeader {
    /// Constant which identifies this as a `NameTableHeader`. This value (0xffff_ffff) was chosen
    /// because the previous version of the `NameTable` did not use this header, and this
    /// signature value would never have occurred in the same position in the previous version.
    pub signature: U32<LE>,

    /// Version of the name hash table.
    pub version: U32<LE>,

    /// Size in bytes of the hash records. This should always be a multiple of
    /// `size_of::<HashRecord>()`, not `HASH_RECORD_CALC_LEN`.
    pub hash_records_size: U32<LE>,

    /// Size in bytes of the hash buckets.
    pub buckets_size: U32<LE>,
}

/// An entry in the GSI hash table. This is in the `cb_hash_records` region.
#[derive(IntoBytes, FromBytes, Unaligned, KnownLayout, Immutable, Debug, Clone)]
#[repr(C)]
pub struct HashRecord {
    /// The byte offset of a symbol record within the Global Symbol Stream, plus 1.
    /// It should point to the beginning of a global symbol record, e.g. `S_PUB32`.
    ///
    /// The value should never be negative or zero.
    pub offset: I32<LE>,

    /// This appears to be a reference-count field, use for the in-memory representation.
    /// On disk, the values are nearly always 1.
    pub c_ref: I32<LE>,
}

/// Contains a Name Table, as used in the GSI and PSI.
pub struct NameTable {
    /// Each value in this vector is an index into `hash_records`.
    /// There is an extra index at the end, to make range calculations easier.
    /// This is a "starts" vector that points into `hash_records`.
    ///
    /// `hash_buckets` contains a list of offsets into an array of `HROffsetCalc`
    /// structures, each of which is 12 bytes long. So all of the offsets should be a multiple
    /// of 12.  (Entries can also be -1, meaning there are no entries in that bucket.)
    ///
    /// But here's where it gets weird!  The value 12 was the size of a Win32 (32-bit) structure
    /// that contained memory pointers.  It is not the size of the structure that gets written
    /// to disk.  _That_ structure is 8 bytes!  So when you read these values, you must first
    /// divide them by 12, then multiply by 8, to get the actual byte offset into the on-disk
    /// data structure.
    hash_buckets: Vec<u32>,

    /// Each record in this table corresponds to one symbol record (e.g. `S_PUB32`). It contains
    /// the offset in bytes into the global symbol stream, plus 1.  The value 0 is reserved.
    hash_records: Vec<HashRecord>,
}

impl NameTable {
    /// Parses a `NameTable`. For the GSI, the entire GSI stream contains only a `NameTable`.
    /// For the `PSI, the PSI stream begins with a `PsiStreamHeader`, followed by a `NameTable`.
    pub fn parse(
        num_buckets: usize,
        stream_offset: usize,
        sym_hash_bytes: &[u8],
    ) -> anyhow::Result<Self> {
        // Read the hash table from the stream data. The hash table may be in one of two forms:
        // "large" or "small".  The "large" format was the original format. The "small" format was
        // added later.
        //
        // The hash records are the same for the small and large hash table formats. They are
        // different in how the hash buckets are stored.

        let _span = debug_span!("NameTable::parse").entered();

        let original_len = stream_offset + sym_hash_bytes.len();

        // See gsi.cpp, GSI1::readHash()
        let hash_records: Vec<HashRecord>;
        let hash_buckets: Vec<u32>;
        {
            let mut p = Parser::new(sym_hash_bytes);
            let stream_offset_table_header = original_len - p.len();
            let hash_header: &NameTableHeader = p.get()?;

            if hash_header.signature.get() == GSI_HASH_HEADER_SIGNATURE
                && hash_header.version.get() == GSI_HASH_HEADER_VERSION
            {
                debug!(
                    hash_records_size = hash_header.hash_records_size.get(),
                    buckets_size = hash_header.buckets_size.get(),
                    "Hash table format: small buckets"
                );

                let hash_records_size = hash_header.hash_records_size.get() as usize;
                let buckets_size = hash_header.buckets_size.get() as usize;

                let stream_offset_hash_records = original_len - p.len();
                let hash_records_bytes = p.bytes(hash_records_size)?;
                let stream_offset_hash_buckets = original_len - p.len();
                let buckets_bytes = p.bytes(buckets_size)?;
                let stream_offset_end = original_len - p.len();

                // We expect that the size of the hash table is equal to the size of the header +
                // cb_hash_records + cb_buckets.
                if !p.is_empty() {
                    warn!("Found extra bytes in hash table, len = {}", p.len());
                }

                if hash_records_size % size_of::<HashRecord>() != 0 {
                    warn!(
                        "GSI/PSI name table contains hash table with a length that is not a multiple of the hash record size."
                    );
                }
                let num_hash_records = hash_records_size / size_of::<HashRecord>();
                let hash_records_slice: &[HashRecord] =
                    Parser::new(hash_records_bytes).slice(num_hash_records)?;

                debug!(num_hash_records, "Number of records in name table");

                hash_records = hash_records_slice.to_vec();

                debug!(num_buckets, "Number of hash buckets in name table");

                debug!("[........] Stream offsets:");
                debug!("[{:08x}] : NameTableHeader", stream_offset_table_header);
                debug!("[{:08x}] : hash records", stream_offset_hash_records);
                debug!("[{:08x}] : hash buckets", stream_offset_hash_buckets);
                debug!("[{:08x}] : (end)", stream_offset_end);

                hash_buckets = expand_buckets(
                    buckets_bytes,
                    num_buckets,
                    num_hash_records,
                    stream_offset_hash_buckets,
                )?;
            } else {
                // We did not find a GsiHashHeader, so this is an old-style hash.
                error!("Hash table format: old-style normal buckets");
                bail!("Old-style hash table is not supported");
            }
        }

        Ok(Self {
            hash_buckets,
            hash_records,
        })
    }

    /// Constructs an empty instance
    pub fn empty() -> Self {
        Self {
            hash_buckets: vec![0],
            hash_records: vec![],
        }
    }

    /// Check all of the hash records in the name table and verify that the hash of the name for
    /// this record matches the hash bucket that the hash record is in.
    pub fn check_hashes(&self, global_symbols: &GlobalSymbolStream) -> anyhow::Result<()> {
        // Verify that hash buckets point to symbol records, and that the hashes of the symbol
        // names in those symbol records matches the hash code for this bucket.

        let mut num_hashes_incorrect: u32 = 0;

        for (bucket_index, hash_index_window) in self.hash_buckets.windows(2).enumerate() {
            trace!(
                "Checking hash bucket #{bucket_index}, hash records at {} .. {}",
                hash_index_window[0], hash_index_window[1]
            );
            let Some(bucket_hash_records) = self
                .hash_records
                .get(hash_index_window[0] as usize..hash_index_window[1] as usize)
            else {
                error!("hash record range is invalid");
                continue;
            };

            for hash_record in bucket_hash_records.iter() {
                let hash_record_offset = hash_record.offset.get();

                // The hash record offset should always be positive.
                if hash_record_offset <= 0 {
                    continue;
                }

                let record_offset_in_symbol_stream = hash_record_offset - 1;
                let pub_sym =
                    match global_symbols.get_pub32_at(record_offset_in_symbol_stream as u32) {
                        Err(e) => {
                            error!("{e}");
                            continue;
                        }
                        Ok(s) => s,
                    };

                let name_hash =
                    crate::hash::hash_mod_u32(pub_sym.name, self.hash_buckets.len() as u32 - 1);

                if name_hash != bucket_index as u32 {
                    if num_hashes_incorrect < 50 {
                        error!(
                            "bucket #{} has symbol {} with wrong hash code {}",
                            bucket_index, pub_sym.name, name_hash
                        );
                    }
                    num_hashes_incorrect += 1;
                }
            }
        }

        if num_hashes_incorrect != 0 {
            error!("Found {num_hashes_incorrect} hash records with the wrong hash value");
        } else {
            info!("All name hashes are correct.");
        }

        Ok(())
    }

    /// Gets the hash records for a specific bucket index. The caller is responsible for using
    /// a valid bucket index.
    pub fn hash_records_for_bucket(&self, bucket: usize) -> &[HashRecord] {
        let start = self.hash_buckets[bucket] as usize;
        let end = self.hash_buckets[bucket + 1] as usize;
        &self.hash_records[start..end]
    }

    /// Gets the hash records that might contain `name`.
    pub fn hash_records_for_name<'a>(&'a self, name: &BStr) -> &'a [HashRecord] {
        // If hash_buckets is empty (i.e. has a length of 1, because it is a "starts" table),
        // then the table is empty and the hash modulus is zero. We need to avoid dividing by zero.
        if self.hash_buckets.len() <= 1 {
            return &[];
        }

        let name_hash = crate::hash::hash_mod_u32(name, self.hash_buckets.len() as u32 - 1);
        self.hash_records_for_bucket(name_hash as usize)
    }

    /// Searches for `name` in the Name Table. The caller must provide access to the GSS.
    pub fn find_symbol<'a>(
        &self,
        gss: &'a GlobalSymbolStream,
        name: &BStr,
    ) -> anyhow::Result<Option<Sym<'a>>> {
        let bucket_entries = self.hash_records_for_name(name);
        for entry in bucket_entries.iter() {
            let entry_offset = entry.offset.get();
            if entry_offset <= 0 {
                warn!("found invalid hash record; entry.offset <= 0");
                continue;
            }

            let sym = gss.get_sym_at(entry_offset as u32 - 1)?;

            if let Some(sym_name) = super::get_global_symbol_name(sym.kind, sym.data)? {
                if sym_name.eq_ignore_ascii_case(name) {
                    return Ok(Some(sym));
                }
            }
        }

        Ok(None)
    }

    /// Iterates the names stored within a `NameTable`.
    pub fn iter<'i, 'a: 'i>(&'a self, gss: &'a GlobalSymbolStream) -> NameTableIter<'i> {
        NameTableIter {
            hash_record_iter: self.hash_records.iter(),
            gss,
        }
    }
}

/// Iterator state for iterating the names within a `NameTable`.
pub struct NameTableIter<'a> {
    hash_record_iter: std::slice::Iter<'a, HashRecord>,
    gss: &'a GlobalSymbolStream,
}

impl<'a> Iterator for NameTableIter<'a> {
    type Item = Sym<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        loop {
            let hash_record = self.hash_record_iter.next()?;

            let sym_offset = hash_record.offset.get();
            if sym_offset <= 0 {
                // Whatever, just ignore it.
                continue;
            }

            if let Ok(sym) = self.gss.get_sym_at((sym_offset - 1) as u32) {
                return Some(sym);
            } else {
                error!("failed to decode symbol in GSS at offset {sym_offset}");
                return None;
            }
        }
    }
}

/// Gets the default number of buckets to use.
///
/// The values are hard-coded. 0x1000 is used for normal PDBs and 0x3ffff is used for "mini PDBs".
/// Mini PDBs are produced using the `/DEBUG:FASTLINK` linker option.
pub fn get_v1_default_bucket(minimal_dbg_info: bool) -> usize {
    if minimal_dbg_info { 0x3ffff } else { 0x1000 }
}

/// Expands a compressed bucket. Returns a vector of offsets.
///
/// The input contains a bitmap, followed by an array of offsets. The bitmap determines how many
/// items there are in the array of offsets. The length of the bitmap is specified by num_buckets.
///
/// This function returns a vector that contains hash indices. The hash records for a given hash
/// bucket can be found as:
///
/// ```text
/// let buckets = expand_buckets(...)?;
/// let bucket_index = 10;
/// let hash_records: &[HashRecord] = &hash_records[buckets[bucket_index]..buckets[bucket_index + 1]];
/// ```
fn expand_buckets(
    buckets_bytes: &[u8],
    num_buckets: usize,
    num_hash_records: usize,
    stream_offset: usize, // offset of this data structure in stream; for diagnostics only
) -> anyhow::Result<Vec<u32>> {
    let _span = debug_span!("expand_buckets").entered();

    trace!(num_buckets, num_hash_records, "expanding buckets");

    let original_len = stream_offset + buckets_bytes.len();
    let mut p = Parser::new(buckets_bytes);

    let output_len = num_buckets + 1;

    let bitmap_len_in_bytes = nonempty_bitmap_size_bytes(num_buckets);
    let bitmap_bytes = p.bytes(bitmap_len_in_bytes)?;
    let bv: &BitSlice<u8, Lsb0> = BitSlice::from_slice(bitmap_bytes);
    trace!(bitmap_bytes, bitmap_len_in_bytes);

    // Count the number of 1 bits set in the non-empty bucket bitmap.
    // Use min(num_buckets) so that we ignore any extra bits in the bitmap.
    let num_nonempty_buckets = bv.count_ones().min(num_buckets);
    trace!(num_nonempty_buckets);

    let nonempty_pointers_stream_offset = original_len - p.len();
    let nonempty_pointers: &[I32<LE>] = p.slice(num_nonempty_buckets)?;

    trace!(
        nonempty_pointers_stream_offset,
        non_empty_pointers = nonempty_pointers.as_bytes(),
        "non-empty pointers"
    );

    let mut nonempty_pointers_iter = nonempty_pointers.iter();

    let mut hash_buckets = Vec::with_capacity(output_len);
    for bucket_index in bv.iter_ones() {
        // Be careful to avoid processing 1 bits in the bitmap. It is possible for the bitmap
        // to contain more bits than there are buckets, due to bit alignment.
        if bucket_index >= num_buckets {
            break;
        }

        // The unwrap() cannot fail because we computed the slice length (num_nonempty_buckets)
        // from the number of 1 bits in the non-empty mask (bv).
        let offset_x12 = nonempty_pointers_iter.next().unwrap().get();
        if offset_x12 < 0 {
            bail!("found a negative offset in hash buckets");
        }
        if offset_x12 % HASH_RECORD_CALC_LEN != 0 {
            bail!("hash record offset {offset_x12} is not a multiple of 12 (as required)");
        }
        let offset = (offset_x12 / HASH_RECORD_CALC_LEN) as u32;

        // It would be strange for offset to be equal to num_hash_records because that would
        // imply an empty "non-empty" hash bucket.
        if offset as usize >= num_hash_records {
            bail!("hash record offset {offset_x12} is beyond range of hash records table");
        }

        // Record offsets must be non-decreasing. They should actually be strictly increasing,
        // but we tolerate repeated values.
        if let Some(&prev_offset) = hash_buckets.last() {
            if offset < prev_offset {
                bail!("hash record offset {offset} is less than previous offset {prev_offset}");
            }
        } else if offset != 0 {
            bail!("First hash record offset should be zero, but instead it is: 0x{offset:x}");
        }

        // Add offsets for previous buckets, which were all empty.
        if hash_buckets.len() < bucket_index {
            trace!(
                "    bucket: 0x{:08x} .. 0x{bucket_index:08x} : range is empty, pushing offset: 0x{offset:8x} {offset:10}",
                hash_buckets.len()
            );
            hash_buckets.resize(bucket_index, offset);
        }
        trace!(
            "    bucket: 0x{b:08x} --> offset: 0x{offset:08x}",
            b = hash_buckets.len()
        );
        hash_buckets.push(offset);
        assert!(hash_buckets.len() <= num_buckets);
    }

    // Fill in the offsets for the remaining empty buckets (if any), and push an extra offset for
    // the end of the hash records array.
    assert!(hash_buckets.len() <= num_buckets);
    hash_buckets.resize(num_buckets + 1, num_hash_records as u32);

    trace!(
        "hash bucket offsets: {:?}",
        &hash_buckets[..hash_buckets.len().min(100)]
    );

    if !nonempty_pointers_iter.as_slice().is_empty() {
        warn!(
            num_extra_bytes = p.len(),
            rest = p.peek_rest(),
            "Compressed hash buckets table contains extra byte(s)"
        );
    }

    if tracing::event_enabled!(tracing::Level::TRACE) {
        trace!("Non-empty buckets: (within expand_buckets)");
        for i in 0..hash_buckets.len() - 1 {
            let start = hash_buckets[i];
            let end = hash_buckets[i + 1];
            if start != end {
                trace!(i, start, end);
            }
        }
    }

    Ok(hash_buckets)
}

/// Used when rebuilding a Name Table
///
/// The order of the fields is significant because it is used for sorting.
#[derive(Eq, PartialEq, PartialOrd, Ord)]
pub struct HashEntry {
    /// computed hash code
    pub hash: u32,
    /// Symbol offset within the GSS. This value has a bias of +1. It will never be 0.
    pub symbol_offset: i32,
}

/// Scan hash_records and figure out how many hash buckets are _not_ empty. Because hash_records
/// is sorted by hash, we can do a single scan through it and find all of the "edges" (places where
/// the `hash` value changes).
pub fn count_nonempty_buckets(sorted_hash_records: &[HashEntry]) -> usize {
    iter_nonempty_buckets(sorted_hash_records).count()
}

/// Compute the size in bytes of the bitmap of non-empty buckets.
pub fn nonempty_bitmap_size_bytes(num_buckets: usize) -> usize {
    let compressed_bitvec_size_u32s = (num_buckets + 1).div_ceil(32);
    compressed_bitvec_size_u32s * 4
}

/// Compute the size in bytes of the name hash table. This includes the header.
pub fn compute_hash_table_size_bytes(
    num_hash_records: usize,
    num_buckets: usize,
    num_nonempty_buckets: usize,
) -> usize {
    size_of::<NameTableHeader>()
        + num_hash_records * size_of::<HashRecord>()
        + nonempty_bitmap_size_bytes(num_buckets)
        + num_nonempty_buckets * size_of::<i32>()
}

/// Output of `build_name_table_prepare`
pub struct NameTableInfo {
    /// The number of buckets to use. This is an input parameter for the table building code, but
    /// it is preserved here to simplify control flow.
    pub num_buckets: usize,
    /// Number of non-empty buckets.  Always less than or equal to `num_buckets.
    pub num_nonempty_buckets: usize,
    /// Size of the entire table, in bytes.
    pub table_size_bytes: usize,
}

impl NameTableBuilder {
    /// Reads a set of sorted hash records and computes the number of non-empty hash buckets and the
    /// total size of the Name Table in bytes.
    pub fn prepare(&mut self) -> NameTableInfo {
        self.sort();

        let num_nonempty_buckets = count_nonempty_buckets(&self.hash_records);
        let table_size_bytes = compute_hash_table_size_bytes(
            self.hash_records.len(),
            self.num_buckets,
            num_nonempty_buckets,
        );
        NameTableInfo {
            num_buckets: self.num_buckets,
            num_nonempty_buckets,
            table_size_bytes,
        }
    }

    /// The number of names inserted into this builder.
    pub fn num_names(&self) -> usize {
        self.hash_records.len()
    }

    /// Writes the hash records, the hash header, and the buckets to output. The output size must be
    /// equal to the expected size.
    pub fn encode(&self, prepared_info: &NameTableInfo, output: &mut [u8]) {
        debug!(
            "Number of symbols found (in name table): {n} 0x{n:x}",
            n = self.hash_records.len()
        );
        debug!(
            "Size in bytes of hash records (in name table): {s} 0x{s:x}",
            s = self.hash_records.len() * 8,
        );

        let compressed_bitvec_size_bytes = nonempty_bitmap_size_bytes(self.num_buckets);
        let num_nonempty_buckets = prepared_info.num_nonempty_buckets;
        let hash_buckets_size_bytes =
            compressed_bitvec_size_bytes + num_nonempty_buckets * size_of::<i32>();
        debug!(
            "hash_buckets_size_bytes = 0x{0:x} {0}",
            hash_buckets_size_bytes
        );

        // Build the name-to-symbol hash map.
        // The structure of the name-to-symbol hash map is:
        //
        //      GsiHashHeader (fixed size)
        //      [HashRecord; num_hash_records]
        //      u32-aligned bitmap of num_buckets + 1 length (in bits), indicating whether a given bucket is non-empty.
        //      [i32; num_non_empty_buckets]
        {
            let mut hash_output_cursor = ParserMut::new(output);

            // Write the hash header.
            *hash_output_cursor.get_mut().unwrap() = NameTableHeader {
                signature: U32::new(GSI_HASH_HEADER_SIGNATURE),
                version: U32::new(GSI_HASH_HEADER_VERSION),
                buckets_size: U32::new(hash_buckets_size_bytes as u32),
                hash_records_size: U32::new(
                    (self.hash_records.len() * size_of::<HashRecord>()) as u32,
                ),
            };

            // Write the hash records.
            {
                let output_slice: &mut [HashRecord] = hash_output_cursor
                    .slice_mut(self.hash_records.len())
                    .unwrap();
                for (from, to) in self.hash_records.iter().zip(output_slice.iter_mut()) {
                    *to = HashRecord {
                        offset: I32::new(from.symbol_offset),
                        c_ref: I32::new(1),
                    };
                }
            }

            // Set all bits in the presence bitmap.
            {
                let sym_hash_bitvec_bytes = hash_output_cursor
                    .bytes_mut(compressed_bitvec_size_bytes)
                    .unwrap();
                let bv: &mut BitSlice<u8, Lsb0> = BitSlice::from_slice_mut(sym_hash_bitvec_bytes);

                for (_record_index, bucket_hashes) in iter_nonempty_buckets(&self.hash_records) {
                    let hash = bucket_hashes[0].hash;
                    bv.set(hash as usize, true);
                }

                assert_eq!(bv.count_ones(), num_nonempty_buckets);
            }

            // Write the hash record offsets of each of the non-empty hash buckets.
            // These values are non-decreasing. The end of each hash bucket is the beginning of the
            // next hash bucket.
            {
                let mut num_nonempty_written = 0;
                let output_offsets_slice: &mut [U32<LE>] =
                    hash_output_cursor.slice_mut(num_nonempty_buckets).unwrap();
                let mut output_iter = output_offsets_slice.iter_mut();

                for (record_index, _hashes) in iter_nonempty_buckets(&self.hash_records) {
                    *output_iter.next().unwrap() = U32::new((record_index as u32) * 12);
                    num_nonempty_written += 1;
                }

                assert_eq!(num_nonempty_written, num_nonempty_buckets);
                assert!(output_iter.as_slice().is_empty());
            }

            assert!(hash_output_cursor.is_empty());
        }
    }
}

/// Sorts hash records
#[inline(never)]
pub fn sort_hash_records(hash_records: &mut [HashEntry]) {
    // This fully determines the order, since we are comparing all fields.
    hash_records.sort_unstable();
}

/// Iterates non-empty buckets. Each item is `(record_index, bucket)`, where `record_index` is
/// the index within `hashes` (the input of this function) where `bucket` begins.  The iterated
/// `bucket` value will always contain values (will not be empty).
fn iter_nonempty_buckets(hashes: &[HashEntry]) -> IterNonEmptyBuckets<'_> {
    IterNonEmptyBuckets {
        record_index: 0,
        hashes,
    }
}

struct IterNonEmptyBuckets<'a> {
    record_index: usize,
    hashes: &'a [HashEntry],
}

impl<'a> Iterator for IterNonEmptyBuckets<'a> {
    // (index into hash records, slice of hash records in bucket)
    type Item = (usize, &'a [HashEntry]);

    fn next(&mut self) -> Option<Self::Item> {
        if self.hashes.is_empty() {
            return None;
        }

        let record_index = self.record_index;

        let hash = self.hashes[0].hash;
        let mut end = 1;
        while end < self.hashes.len() && self.hashes[end].hash == hash {
            end += 1;
        }

        let (lo, hi) = self.hashes.split_at(end);
        self.record_index += end;
        self.hashes = hi;
        Some((record_index, lo))
    }
}

/// This type constructs a new name table, for the GSI/PSI.
///
/// Example:
///
/// ```
/// # use ms_pdb::globals::name_table::NameTableBuilder;
/// # use bstr::BStr;
/// let mut builder = NameTableBuilder::new(0x1000);
/// builder.push("hello".into(), 1);
/// builder.push("world".into(), 2);
/// let prepared_info = builder.prepare();
/// let mut encoded_bytes: Vec<u8> = vec![0; prepared_info.table_size_bytes];
/// builder.encode(&prepared_info, &mut encoded_bytes);
/// ```
pub struct NameTableBuilder {
    num_buckets: usize,
    hash_records: Vec<HashEntry>,
}

impl NameTableBuilder {
    /// Starts building a new name table.  Specify the number of hash buckets to use.
    pub fn new(num_buckets: usize) -> Self {
        assert!(num_buckets > 0);
        Self {
            num_buckets,
            hash_records: Vec::new(),
        }
    }

    /// The number of hash buckets.
    pub fn num_buckets(&self) -> usize {
        self.num_buckets
    }

    /// Adds a new string entry to the builder. Specify the hash of the string and its symbol
    /// offset. The hash _must_ already have been computed and the remainder taken using
    /// `num_buckets()`.
    pub fn push_hash(&mut self, hash: u32, symbol_offset: i32) {
        assert!((hash as usize) < self.num_buckets);
        self.hash_records.push(HashEntry {
            hash,
            symbol_offset,
        });
    }

    /// Hashes a string and adds it to the table. The contents of the string are not retained.
    pub fn push(&mut self, name: &BStr, symbol_offset: i32) {
        let name_hash = crate::hash::hash_mod_u32(name, self.num_buckets as u32);
        self.push_hash(name_hash, symbol_offset);
    }

    fn sort(&mut self) {
        sort_hash_records(&mut self.hash_records);
    }
}

```

`pdb/src/globals/name_table/tests.rs`:

```rs
use pretty_hex::PrettyHex;

use super::*;

fn test_names(names: &[String]) -> NameTable {
    println!();

    let num_buckets = 0x1000;

    let mut builder = NameTableBuilder::new(num_buckets);

    for (i, name) in names.iter().enumerate() {
        let symbol_offset = i as i32 + 1;
        builder.push(BStr::new(name), symbol_offset);

        let entry = builder.hash_records.last().unwrap();
        println!(
            "  {i:4} : hash 0x{:08x}, symbol_offset 0x{:08x}, name: {name:?}",
            entry.hash, entry.symbol_offset as u32
        );
    }

    // Add two entries that test our requirements during decoding.
    builder.push("bad_symbol_zero_offset".into(), 0);
    builder.push("bad_symbol_negative_offset".into(), -1);

    let prepared_info = builder.prepare();

    let mut encoded_bytes = vec![0u8; prepared_info.table_size_bytes];
    builder.encode(&prepared_info, &mut encoded_bytes);
    println!("Encoded name table:\n{:?}", encoded_bytes.hex_dump());

    // Decode the table.
    let rt_table = NameTable::parse(num_buckets, 0, &encoded_bytes)
        .expect("Expected table to decode successfully");

    println!("Hash records in decoded table:");
    for (i, hr) in rt_table.hash_records.iter().enumerate() {
        println!("  {i:4} : symbol_offset 0x{:08x}", hr.offset.get() as u32);
    }

    println!("Non-empty buckets:");
    for i in 0..rt_table.hash_buckets.len() - 1 {
        let start = rt_table.hash_buckets[i];
        let end = rt_table.hash_buckets[i + 1];
        if start != end {
            println!("  {i:4} : {start:4} .. {end:4}");
        }
    }

    println!("Checking names:");

    // Make sure that all of the names can be found in the table.
    for name in names.iter() {
        let bucket = rt_table.hash_records_for_name(BStr::new(name));
        println!(
            "searching for {name:?}, num entries in bucket = {}",
            bucket.len()
        );

        let mut num_found: u32 = 0;
        for entry in bucket {
            let symbol_offset = entry.offset.get();
            assert!(symbol_offset > 0);
            assert!(symbol_offset as usize - 1 < names.len());
            if names[symbol_offset as usize - 1] == *name {
                num_found += 1;
            }
        }

        assert_eq!(
            num_found, 1,
            "expected to find {name:?} in the table exactly once"
        );
    }

    rt_table
}

#[test]
fn build_empty() {
    test_names(&[]);
}

// Verify that the code that checks for a record offset <= 0 is working.
#[test]
fn build_and_check_bad_names() {
    let names = test_names(&[]);
    let gss = GlobalSymbolStream::new(Vec::new());
    let name_opt = names
        .find_symbol(&gss, "bad_symbol_negative_offset".into())
        .unwrap();
    assert!(name_opt.is_none());
}

#[test]
fn build_simple() {
    let names = vec![
        "achilles".to_string(),
        "castor".to_string(),
        "pollux".to_string(),
    ];
    test_names(&names);
}

#[test]
fn build_many() {
    let mut names: Vec<String> = Vec::new();
    for i in 0..100 {
        names.push(format!("name{i}"));
    }

    test_names(&names);
}

```

`pdb/src/globals/psi.rs`:

```rs
//! Public Symbol Index
//!
//! The Public Symbol Index (PSI) provides several look-up tables that accelerate finding
//! information in the Global Symbol Stream. The PSI indexes only `S_PUB32` symbols in the GSS; all
//! other symbol kinds are indexed in the GSI.
//!
//! The PSI does not have a fixed stream number. The DBI Stream Header contains the stream number
//! of the PSI.

use super::gss::*;
use super::name_table::*;
use crate::syms::{OffsetSegment, Pub};
use crate::utils::is_aligned_4;
use anyhow::{Context, bail};
use bstr::BStr;
use ms_codeview::parser::{Parse, Parser, ParserMut};
use std::mem::size_of;
use tracing::{debug, error, info};
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, LE, U16, U32, Unaligned};

/// The header of the GSI stream.
///
/// See `PSGSIHDR` in `microsoft-pdb/PDB/dbi/gsi.h`.
#[repr(C)]
#[derive(IntoBytes, FromBytes, Unaligned, Immutable, KnownLayout, Clone, Debug)]
#[allow(missing_docs)]
pub struct PsiStreamHeader {
    /// Length in bytes of the symbol hash table.  This region immediately follows PSGSIHDR.
    pub name_table_size: U32<LE>,

    /// Length in bytes of the address map.  This region immediately follows the symbol hash.
    pub addr_table_size: U32<LE>,

    /// The number of thunk records.
    pub num_thunks: U32<LE>,
    /// Size in bytes of each thunk record.
    pub thunk_size: U32<LE>,
    pub thunk_table_section: U16<LE>,
    pub padding: U16<LE>,
    pub thunk_table_offset: U32<LE>,
    pub num_sections: U32<LE>,
}
static_assertions::const_assert_eq!(core::mem::size_of::<PsiStreamHeader>(), 28);

/// Contains the Public Symbol Index
///
/// The Public Symbol Index (PSI) contains a name-to-symbol lookup table and an address-to-symbol
/// lookup table.
pub struct PublicSymbolIndex {
    /// Allows name-to-symbol look for `S_PUB32` symbols.
    name_table: NameTable,

    /// Each entry in this table is a byte offset of one `S_PUB32` symbol in the GSS.
    /// All of the values are sorted by `(segment, offset)`, which allows binary search.
    addr_map: Vec<u32>,
}

impl PublicSymbolIndex {
    /// Parses the PSI from stream data. The caller must specify `num_buckets` because that value
    /// is not stored within the stream.
    pub fn parse(num_buckets: usize, public_stream_data: Vec<u8>) -> anyhow::Result<Self> {
        let mut p = Parser::new(&public_stream_data);
        if p.is_empty() {
            return Ok(Self::empty());
        }

        let psgsi_header: &PsiStreamHeader = p.get()?;
        debug!("PsiStreamHeader: {:#?}", psgsi_header);

        let sym_hash_size = psgsi_header.name_table_size.get() as usize;
        let addr_map_size = psgsi_header.addr_table_size.get() as usize;

        debug!("Size of symbol hash table: {} bytes", sym_hash_size);
        debug!("Size of address map: {} bytes", addr_map_size);

        let sym_hash_bytes = p
            .bytes(sym_hash_size)
            .with_context(|| "Failed to locate symbol hash table within Publics stream")?;
        let addr_map_bytes = p
            .bytes(addr_map_size)
            .with_context(|| "Failed to locate address map within Publics stream")?;

        let name_table =
            NameTable::parse(num_buckets, size_of::<PsiStreamHeader>(), sym_hash_bytes)?;

        // Load the address map. The address map is an array of u32 values, each of which is an
        // offset into the global symbol stream. I'm _guessing_ that the array is sorted by
        // [segment:offset].
        let addr_map: Vec<u32>;
        {
            let num_addrs = addr_map_bytes.len() / 4;
            info!("Number of entries in address map: {}", num_addrs);

            let mut addr_parser = Parser::new(addr_map_bytes);
            let addr_map_u32_slice: &[U32<LE>] = addr_parser.slice(num_addrs)?;

            addr_map = addr_map_u32_slice.iter().map(|i| i.get()).collect();
        }

        Ok(PublicSymbolIndex {
            name_table,
            addr_map,
        })
    }

    /// Constructs an empty instance of the PSI.
    pub fn empty() -> Self {
        Self {
            addr_map: vec![],
            name_table: NameTable::empty(),
        }
    }

    /// Check invariants for the PSI. This requires having access to the GSS, since the PSI
    /// points into the GSS.
    pub fn check_consistency(&self, gss: &GlobalSymbolStream) -> anyhow::Result<()> {
        // Verify that all entries in the address map are in non-decreasing order.
        let mut prev_sym: Option<Pub<'_>> = None;
        let mut num_bad_order: u32 = 0;
        for &offset in self.addr_map.iter() {
            let sym = gss.get_pub32_at(offset)?;
            if let Some(prev_sym) = &prev_sym {
                if prev_sym.offset_segment() > sym.offset_segment() {
                    if num_bad_order < 20 {
                        error!("found addr map in bad order");
                    }
                    num_bad_order += 1;
                }
            }
            prev_sym = Some(sym);
        }

        if num_bad_order != 0 {
            bail!(
                "Found {} address map entries that were out of order.",
                num_bad_order
            );
        }
        info!("All address map entries are correctly ordered.");

        Ok(())
    }

    /// Gets direct access to the name-to-symbol table.
    pub fn names(&self) -> &NameTable {
        &self.name_table
    }

    /// Searches for an `S_PUB32` symbol by name.
    pub fn find_symbol_by_name<'a>(
        &self,
        gss: &'a GlobalSymbolStream,
        name: &BStr,
    ) -> anyhow::Result<Option<Pub<'a>>> {
        if let Some(sym) = self.name_table.find_symbol(gss, name)? {
            Ok(Some(Pub::parse(sym.data)?))
        } else {
            Ok(None)
        }
    }

    /// Searches for an `S_PUB32` symbol by address.
    pub fn find_symbol_by_addr<'a>(
        &self,
        gss: &'a GlobalSymbolStream,
        segment: u16,
        offset: u32,
    ) -> anyhow::Result<Option<(Pub<'a>, u32)>> {
        use std::cmp::Ordering;

        let addr_map = self.addr_map.as_slice();

        let mut items = addr_map;
        while !items.is_empty() {
            let mid_index = items.len() / 2;
            let mid_rec = gss.get_pub32_at(items[mid_index])?;
            let mid_segment = mid_rec.fixed.offset_segment.segment();
            let mid_offset = mid_rec.fixed.offset_segment.offset();

            match segment.cmp(&mid_segment) {
                Ordering::Less => {
                    // info!("segment is less, moving low");
                    items = &items[..mid_index];
                    continue;
                }
                Ordering::Greater => {
                    // info!("segment is greater, moving high");
                    items = &items[mid_index + 1..];
                    continue;
                }
                Ordering::Equal => {}
            }

            // Same segment. Compare the offsets.

            if offset < mid_offset {
                items = &items[..mid_index];
                continue;
            }

            if offset == mid_offset {
                // Bullseye!
                return Ok(Some((mid_rec, 0)));
            }

            // The address we are looking for is higher than the address of the symbol that we are
            // currently looking at.
            // TODO: Implement best-so-far search.
            items = &items[mid_index + 1..];
            continue;
        }

        Ok(None)
    }
}

/// Sorts an address map slice.
#[inline(never)]
pub fn sort_address_records(addr_map: &mut [(u32, OffsetSegment)]) {
    addr_map.sort_unstable_by_key(|(record_offset, os)| (*os, *record_offset));
}

/// Builds the Public Symbol Index (PSI).
///
/// The PSI contains both a name-to-symbol table and an address-to-symbol table.
pub fn build_psi(
    sorted_hash_records: &mut NameTableBuilder,
    sorted_addr_map: &[(u32, OffsetSegment)],
) -> Vec<u8> {
    assert_eq!(sorted_hash_records.num_names(), sorted_addr_map.len());

    debug!(
        "Number of entries in address table: {n} 0x{n:x}",
        n = sorted_addr_map.len()
    );
    debug!(
        "Size in bytes of address table: {s} 0x{s:x}",
        s = sorted_addr_map.len() * 4
    );

    let name_table_info = sorted_hash_records.prepare();
    let addr_map_size_bytes = sorted_addr_map.len() * size_of::<i32>();

    let stream_size_bytes =
        size_of::<PsiStreamHeader>() + name_table_info.table_size_bytes + addr_map_size_bytes;

    let mut stream_data: Vec<u8> = vec![0; stream_size_bytes];
    let mut p = ParserMut::new(&mut stream_data);

    let stream_header = PsiStreamHeader {
        name_table_size: U32::new(name_table_info.table_size_bytes as u32),
        addr_table_size: U32::new(addr_map_size_bytes as u32),
        num_thunks: U32::new(0), // TODO
        thunk_size: U32::new(0), // TODO
        padding: U16::new(0),
        thunk_table_section: U16::new(0), // TODO
        thunk_table_offset: U32::new(0),  // TODO
        num_sections: U32::new(0),        // TODO
    };
    *p.get_mut::<PsiStreamHeader>().unwrap() = stream_header;

    let name_table_bytes = p.bytes_mut(name_table_info.table_size_bytes).unwrap();
    sorted_hash_records.encode(&name_table_info, name_table_bytes);

    let addr_map_bytes = p.bytes_mut(addr_map_size_bytes).unwrap();
    let addr_map_output = <[U32<LE>]>::mut_from_bytes(addr_map_bytes).unwrap();
    // Write the address map. This converts from the array that we used for sorting, which contains
    // the symbol record byte offset and the segment:offset, to just the symbol record byte offset.
    {
        for (from, to) in sorted_addr_map.iter().zip(addr_map_output.iter_mut()) {
            *to = U32::new(from.0);
        }
    }

    assert!(p.is_empty());

    // Make it easy to understand the output.
    {
        let mut pos = 0;
        let mut region = |name: &str, len: usize| {
            debug!("    {pos:08x} +{len:08x} : {name}");
            pos += len;
        };
        debug!("PSI Stream layout:");
        region("PSI Stream Header", size_of::<PsiStreamHeader>());
        region("Name Table - Header", size_of::<NameTableHeader>());
        region(
            "Name Table - Hash Records",
            sorted_hash_records.num_names() * size_of::<HashRecord>(),
        );
        region(
            "Name Table - Buckets Bitmap",
            nonempty_bitmap_size_bytes(sorted_hash_records.num_buckets()),
        );
        region(
            "Name Table - Buckets",
            name_table_info.num_nonempty_buckets * 4,
        );
        region("Address Table", addr_map_size_bytes);
        region("(end)", 0);
        assert_eq!(pos, stream_data.len());
    }

    assert!(is_aligned_4(stream_data.len()));

    stream_data
}

```

`pdb/src/globals/tests.rs`:

```rs
use pretty_hex::PrettyHex;

use super::build_global_symbols_index;
use super::gsi::GlobalSymbolIndex;
use super::gss::GlobalSymbolStream;
use super::psi::PublicSymbolIndex;
use crate::syms::SymKind;
use crate::types::TypeIndex;
use ms_codeview::encoder::Encoder;

const NUM_BUCKETS: usize = 0x1000;

#[derive(Default)]
struct SymBuilder {
    buffer: Vec<u8>,
    record_start: usize,
}

impl SymBuilder {
    fn finish(self) -> GlobalSymbolStream {
        GlobalSymbolStream::new(self.buffer)
    }

    /// Starts adding a new record to the builder.
    fn start_record(&mut self, kind: SymKind) -> Encoder<'_> {
        self.record_start = self.buffer.len();
        self.buffer.extend_from_slice(&[0, 0]); // placeholder for record length
        self.buffer.extend_from_slice(&kind.0.to_le_bytes());
        Encoder::new(&mut self.buffer)
    }

    /// Finishes adding a record to the builder.
    fn end_record(&mut self) {
        match self.buffer.len() & 3 {
            1 => self.buffer.push(0xf1),
            2 => self.buffer.extend_from_slice(&[0xf1, 0xf2]),
            3 => self.buffer.extend_from_slice(&[0xf1, 0xf2, 0xf3]),
            _ => {}
        }

        let record_len = self.buffer.len() - self.record_start - 2;

        let record_field = &mut self.buffer[self.record_start..];
        record_field[0] = record_len as u8;
        record_field[1] = (record_len >> 8) as u8;
    }

    /// Adds an `S_UDT` record.
    fn udt(&mut self, ty: TypeIndex, name: &str) {
        let mut e = self.start_record(SymKind::S_UDT);
        e.u32(ty.0);
        e.strz(name.into());
        self.end_record();
    }

    /// Adds an `S_PUB32` record.
    fn pub32(&mut self, flags: u32, offset: u32, segment: u16, name: &str) {
        let mut e = self.start_record(SymKind::S_PUB32);
        e.u32(flags);
        e.u32(offset);
        e.u16(segment);
        e.strz(name.into());
        self.end_record();
    }
}

/// Builds a GSS with some example records
fn build_test_gss() -> Vec<u8> {
    let mut sb = SymBuilder::default();

    sb.udt(TypeIndex(0x1001), "FOO");
    sb.udt(TypeIndex(0x1001), "BAR");
    sb.udt(
        TypeIndex(0x1002),
        "AugmentedMultiThreadedSymbolExpanderServiceProviderSingletonAbstractBaseFacet",
    );

    // Add some S_PUB32 records. Put records out-of-order, with respect to their segment:offset,
    // so that we test the sorting code.
    sb.pub32(0, 100, 1, "main");
    sb.pub32(0, 200, 1, "memset");
    sb.pub32(0, 40, 1, "memcpy");
    sb.pub32(0, 30, 1, "CreateWindowEx");

    sb.buffer
}

#[test]
fn build_and_search_globals() {
    println!();

    let gss = GlobalSymbolStream::new(build_test_gss());
    println!("GSS:\n{:?}", &gss.stream_data.hex_dump());

    let indexes = build_global_symbols_index(&gss.stream_data, NUM_BUCKETS).unwrap();

    {
        let gsi =
            GlobalSymbolIndex::parse(NUM_BUCKETS, indexes.global_symbol_index_stream_data).unwrap();

        // Check consistency of name hashes
        gsi.names().check_hashes(&gss).unwrap();

        println!("Dumping names from GSI:");
        for name_sym in gsi.names().iter(&gss) {
            println!("{name_sym:?}");
        }

        let gsi_names = gsi.names();
        assert!(
            gsi_names
                .find_symbol(&gss, "bad_name_not_found".into())
                .unwrap()
                .is_none()
        );
        assert!(gsi_names.find_symbol(&gss, "FOO".into()).unwrap().is_some());
        assert!(gsi_names.find_symbol(&gss, "BAR".into()).unwrap().is_some());
        assert!(
            gsi_names
                .find_symbol(
                    &gss,
                    "AugmentedMultiThreadedSymbolExpanderServiceProviderSingletonAbstractBaseFacet"
                        .into()
                )
                .unwrap()
                .is_some()
        );
    }

    {
        let psi =
            PublicSymbolIndex::parse(NUM_BUCKETS, indexes.public_symbol_index_stream_data).unwrap();

        // Check consistency of name hashes
        psi.names().check_hashes(&gss).unwrap();

        psi.check_consistency(&gss).unwrap();

        assert!(
            psi.find_symbol_by_name(&gss, "bad_name_not_found".into())
                .unwrap()
                .is_none()
        );

        {
            let memset = psi
                .find_symbol_by_name(&gss, "memset".into())
                .unwrap()
                .unwrap();
            assert_eq!(memset.name, "memset");
            assert_eq!(memset.fixed.offset_segment.offset(), 200);
        }

        {
            let memcpy = psi.find_symbol_by_addr(&gss, 1, 40).unwrap().unwrap();
            assert_eq!(memcpy.0.name, "memcpy");
        }
    }
}

#[test]
fn empty_psi() {
    let gss = GlobalSymbolStream::empty();
    let psi = PublicSymbolIndex::parse(NUM_BUCKETS, Vec::new()).unwrap();
    psi.check_consistency(&gss).unwrap();
}

#[test]
fn empty_gsi() {
    let gss = GlobalSymbolStream::empty();
    let gsi = GlobalSymbolIndex::parse(NUM_BUCKETS, Vec::new()).unwrap();
    assert!(
        gsi.find_symbol(&gss, bstr::BStr::new("none"))
            .unwrap()
            .is_none()
    );

    // Check bad offset: Outside of bounds
    assert!(gss.get_sym_at(0xbadbad).is_err());

    // Check bad offset: The slice operation succeeds, but the symbol cannot be decoded.
    assert!(gss.get_sym_at(0).is_err());

    assert_eq!(gss.iter_syms().count(), 0);
}

#[test]
fn gss_get_pub32_wrong_type() {
    let mut sb = SymBuilder::default();
    sb.udt(TypeIndex(0x1001), "FOO");

    let gss = sb.finish();

    // Symbol exists, but has wrong type.
    assert!(gss.get_pub32_at(0).is_err());
}

#[test]
fn gss_get_pub32_invalid_symbol() {
    let mut sb = SymBuilder::default();

    // S_PUB32 record with invalid contents
    let _e = sb.start_record(SymKind::S_PUB32);
    sb.end_record();

    let gss = sb.finish();

    // Found record at offset, but it could not be decoded as S_PUB32 because its contents are bogus.
    assert!(gss.get_pub32_at(0).is_err());
}

```

`pdb/src/guid.rs`:

```rs
//! Standard Windows type

use std::fmt::Debug;
use uuid::Uuid;
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, LE, U16, U32, Unaligned};

/// Standard Windows type
#[repr(C)]
#[derive(Clone, Eq, PartialEq, IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Debug)]
#[allow(missing_docs)]
pub struct GuidLe {
    pub data1: U32<LE>,
    pub data2: U16<LE>,
    pub data3: U16<LE>,
    pub data4: [u8; 8],
}

impl GuidLe {
    /// Convert the on-disk format to in-memory format.
    pub fn get(&self) -> Uuid {
        Uuid::from_fields(
            self.data1.get(),
            self.data2.get(),
            self.data3.get(),
            &self.data4,
        )
    }
}

impl From<&Uuid> for GuidLe {
    fn from(uuid: &Uuid) -> Self {
        let f = uuid.as_fields();
        GuidLe {
            data1: U32::new(f.0),
            data2: U16::new(f.1),
            data3: U16::new(f.2),
            data4: *f.3,
        }
    }
}

```

`pdb/src/hash.rs`:

```rs
//! MSVC hash algorithms

use zerocopy::{FromBytes, LE, U16, U32};

#[cfg(test)]
use pretty_hex::PrettyHex;

/// Computes a 32-bit hash. This produces the same results as the hash function used in the
/// MSVC PDB reader library.
///
/// This is a port of the `LHashPbCb` function.
///
/// # WARNING! WARNING! WARNING!
///
/// This is a **VERY POOR HASH FUNCTION** and it should not be used for *ANY* new code. This
/// function should only be used for compatibility with PDB data structures.
///
/// # References
///
/// * [`misc.h](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/PDB/include/misc.h#L15)
pub fn hash_mod_u32(pb: &[u8], m: u32) -> u32 {
    hash_u32(pb) % m
}

/// Computes a 32-bit hash, but does not compute a remainder (modulus).
#[inline(never)]
pub fn hash_u32(mut pb: &[u8]) -> u32 {
    let mut h: u32 = 0;
    if let Ok((u32s, tail)) = <[U32<LE>]>::ref_from_prefix(pb) {
        for u in u32s.iter() {
            h ^= u.get();
        }
        pb = tail;
    }

    // The tail is handled differently.
    if let Ok((tail_u16, rest)) = <U16<LE>>::read_from_prefix(pb) {
        h ^= tail_u16.get() as u32;
        pb = rest;
    }

    debug_assert!(pb.is_empty() || pb.len() == 1);

    if !pb.is_empty() {
        h ^= pb[0] as u32;
    }

    h |= 0x20202020;
    h ^= h >> 11;
    h ^ (h >> 16)
}

/// Computes a 16-bit hash
///
/// This is a port of the `HashPbCb` function.
pub fn hash_mod_u16(pb: &[u8], m: u32) -> u16 {
    hash_mod_u32(pb, m) as u16
}

#[test]
fn test_hash() {
    static INPUTS: &[(u32, &[u8])] = &[
        (0x00000c09, b""),
        (0x00000c09, b" "),
        (0x00000c09, b"  "),
        (0x00000c09, b"   "),
        (0x00000c09, b"    "),
        (0x00019fe2, b"hello"),
        (0x00019fe2, b"HELLO"),
        (0x0003c00b, b"Hello, World"),
        (0x0003c00b, b"hello, world"),
        (0x000068e2, b"hello_world::main"),
        (0x0000b441, b"std::vector<std::basic_string<wchar_t>>"),
        (0x000372ae, b"__chkstk"),
        (0x0001143b, b"WelsEmms"),
        (0x00000c0a, &[1]),
        (0x00000e0a, &[1, 2]),
        (0x00000e0b, &[1, 2, 3]),
        (0x00038b6b, &[1, 2, 3, 4]),
        (0x00038b70, &[1, 2, 3, 4, 5]),
        (0x00038d70, &[1, 2, 3, 4, 5, 6]),
        (0x00038d69, &[1, 2, 3, 4, 5, 6, 7]),
        (0x00019789, &[1, 2, 3, 4, 5, 6, 7, 8]),
        (0x00019790, &[1, 2, 3, 4, 5, 6, 7, 8, 9]),
        (0x00019191, &[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]),
        (0x0001918a, &[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]),
        (0x000313ed, &[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]),
        (0x000313f8, &[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]),
        (0x000214eb, &[5, 6, 7, 8]),
    ];

    for &(expected_output, input) in INPUTS.iter() {
        let m = 0x3_ffff;
        let actual_output = hash_mod_u32(input, m);
        assert_eq!(
            expected_output, actual_output,
            "expected: 0x{expected_output:08x}, actual: 0x{actual_output:08x}, input: {input:02x?}"
        );
    }
}

/// Computes a hash code using an algorithm equivalent to the `SigForPbCb` function.
/// This is a CRC-32 checksum with an initial value of `sig`.
///
/// See `SigForPbCb` in <https://github.com/microsoft/microsoft-pdb/blob/master/langapi/shared/crc32.h>
pub fn hash_sig(pb: &[u8], mut sig: u32) -> u32 {
    static RGCRC: [u32; 0x100] = [
        0x00000000, 0x77073096, 0xEE0E612C, 0x990951BA, 0x076DC419, 0x706AF48F, 0xE963A535,
        0x9E6495A3, 0x0EDB8832, 0x79DCB8A4, 0xE0D5E91E, 0x97D2D988, 0x09B64C2B, 0x7EB17CBD,
        0xE7B82D07, 0x90BF1D91, 0x1DB71064, 0x6AB020F2, 0xF3B97148, 0x84BE41DE, 0x1ADAD47D,
        0x6DDDE4EB, 0xF4D4B551, 0x83D385C7, 0x136C9856, 0x646BA8C0, 0xFD62F97A, 0x8A65C9EC,
        0x14015C4F, 0x63066CD9, 0xFA0F3D63, 0x8D080DF5, 0x3B6E20C8, 0x4C69105E, 0xD56041E4,
        0xA2677172, 0x3C03E4D1, 0x4B04D447, 0xD20D85FD, 0xA50AB56B, 0x35B5A8FA, 0x42B2986C,
        0xDBBBC9D6, 0xACBCF940, 0x32D86CE3, 0x45DF5C75, 0xDCD60DCF, 0xABD13D59, 0x26D930AC,
        0x51DE003A, 0xC8D75180, 0xBFD06116, 0x21B4F4B5, 0x56B3C423, 0xCFBA9599, 0xB8BDA50F,
        0x2802B89E, 0x5F058808, 0xC60CD9B2, 0xB10BE924, 0x2F6F7C87, 0x58684C11, 0xC1611DAB,
        0xB6662D3D, 0x76DC4190, 0x01DB7106, 0x98D220BC, 0xEFD5102A, 0x71B18589, 0x06B6B51F,
        0x9FBFE4A5, 0xE8B8D433, 0x7807C9A2, 0x0F00F934, 0x9609A88E, 0xE10E9818, 0x7F6A0DBB,
        0x086D3D2D, 0x91646C97, 0xE6635C01, 0x6B6B51F4, 0x1C6C6162, 0x856530D8, 0xF262004E,
        0x6C0695ED, 0x1B01A57B, 0x8208F4C1, 0xF50FC457, 0x65B0D9C6, 0x12B7E950, 0x8BBEB8EA,
        0xFCB9887C, 0x62DD1DDF, 0x15DA2D49, 0x8CD37CF3, 0xFBD44C65, 0x4DB26158, 0x3AB551CE,
        0xA3BC0074, 0xD4BB30E2, 0x4ADFA541, 0x3DD895D7, 0xA4D1C46D, 0xD3D6F4FB, 0x4369E96A,
        0x346ED9FC, 0xAD678846, 0xDA60B8D0, 0x44042D73, 0x33031DE5, 0xAA0A4C5F, 0xDD0D7CC9,
        0x5005713C, 0x270241AA, 0xBE0B1010, 0xC90C2086, 0x5768B525, 0x206F85B3, 0xB966D409,
        0xCE61E49F, 0x5EDEF90E, 0x29D9C998, 0xB0D09822, 0xC7D7A8B4, 0x59B33D17, 0x2EB40D81,
        0xB7BD5C3B, 0xC0BA6CAD, 0xEDB88320, 0x9ABFB3B6, 0x03B6E20C, 0x74B1D29A, 0xEAD54739,
        0x9DD277AF, 0x04DB2615, 0x73DC1683, 0xE3630B12, 0x94643B84, 0x0D6D6A3E, 0x7A6A5AA8,
        0xE40ECF0B, 0x9309FF9D, 0x0A00AE27, 0x7D079EB1, 0xF00F9344, 0x8708A3D2, 0x1E01F268,
        0x6906C2FE, 0xF762575D, 0x806567CB, 0x196C3671, 0x6E6B06E7, 0xFED41B76, 0x89D32BE0,
        0x10DA7A5A, 0x67DD4ACC, 0xF9B9DF6F, 0x8EBEEFF9, 0x17B7BE43, 0x60B08ED5, 0xD6D6A3E8,
        0xA1D1937E, 0x38D8C2C4, 0x4FDFF252, 0xD1BB67F1, 0xA6BC5767, 0x3FB506DD, 0x48B2364B,
        0xD80D2BDA, 0xAF0A1B4C, 0x36034AF6, 0x41047A60, 0xDF60EFC3, 0xA867DF55, 0x316E8EEF,
        0x4669BE79, 0xCB61B38C, 0xBC66831A, 0x256FD2A0, 0x5268E236, 0xCC0C7795, 0xBB0B4703,
        0x220216B9, 0x5505262F, 0xC5BA3BBE, 0xB2BD0B28, 0x2BB45A92, 0x5CB36A04, 0xC2D7FFA7,
        0xB5D0CF31, 0x2CD99E8B, 0x5BDEAE1D, 0x9B64C2B0, 0xEC63F226, 0x756AA39C, 0x026D930A,
        0x9C0906A9, 0xEB0E363F, 0x72076785, 0x05005713, 0x95BF4A82, 0xE2B87A14, 0x7BB12BAE,
        0x0CB61B38, 0x92D28E9B, 0xE5D5BE0D, 0x7CDCEFB7, 0x0BDBDF21, 0x86D3D2D4, 0xF1D4E242,
        0x68DDB3F8, 0x1FDA836E, 0x81BE16CD, 0xF6B9265B, 0x6FB077E1, 0x18B74777, 0x88085AE6,
        0xFF0F6A70, 0x66063BCA, 0x11010B5C, 0x8F659EFF, 0xF862AE69, 0x616BFFD3, 0x166CCF45,
        0xA00AE278, 0xD70DD2EE, 0x4E048354, 0x3903B3C2, 0xA7672661, 0xD06016F7, 0x4969474D,
        0x3E6E77DB, 0xAED16A4A, 0xD9D65ADC, 0x40DF0B66, 0x37D83BF0, 0xA9BCAE53, 0xDEBB9EC5,
        0x47B2CF7F, 0x30B5FFE9, 0xBDBDF21C, 0xCABAC28A, 0x53B39330, 0x24B4A3A6, 0xBAD03605,
        0xCDD70693, 0x54DE5729, 0x23D967BF, 0xB3667A2E, 0xC4614AB8, 0x5D681B02, 0x2A6F2B94,
        0xB40BBE37, 0xC30C8EA1, 0x5A05DF1B, 0x2D02EF8D,
    ];

    for &b in pb.iter() {
        sig = (sig >> 8) ^ RGCRC[((sig & 0xff) ^ (b as u32)) as usize];
    }
    sig
}

#[test]
fn test_hash_sig() {
    static CASES: &[(u32, u32, &[u8])] = &[
        // expected_hash, input_sig, input_bytes
        (0x00000000, 0x00000000, &[]),
        (0x01234567, 0x01234567, &[]),
        (0x57eccb91, 0x00000000, b"hello, world!"),
        (0x29b1c6ec, 0xabababab, b"hello, world!"),
        (0x2b4468c3, 0x00000000, b"hello!"),
        (0x102f0bec, 0xabababab, b"hello!"),
    ];

    for &(expected_hash, input_sig, input_bytes) in CASES.iter() {
        let actual_hash = hash_sig(input_bytes, input_sig);
        assert_eq!(
            actual_hash,
            expected_hash,
            "actual: 0x{actual_hash:08x}, expected: 0x{expected_hash:08x}, input_sig: 0x{input_sig:08x}, bytes: {:?}",
            input_bytes.hex_dump()
        );
    }
}

/// Computes a CRC-32 with an initializer value, then computes the modulus of it.
pub fn hash_sig_mod(pb: &[u8], sig: u32, modulus: u32) -> u32 {
    let h = hash_sig(pb, sig);
    h % modulus
}

```

`pdb/src/lib.rs`:

```rs
//! Reads and writes Program Database (PDB) files.
//!
//! # References
//! * <https://llvm.org/docs/PDB/index.html>
//! * <https://github.com/microsoft/microsoft-pdb>

#![forbid(unused_must_use)]
#![forbid(unsafe_code)]
#![warn(missing_docs)]
#![allow(clippy::collapsible_if)]
#![allow(clippy::single_match)]
#![allow(clippy::manual_flatten)]
#![allow(clippy::needless_lifetimes)]
#![allow(clippy::needless_late_init)]

pub mod container;
pub mod dbi;
pub mod globals;
pub mod guid;
pub mod hash;
pub mod lines;
pub mod modi;
pub mod taster;
pub use ::uuid::Uuid;
use ms_codeview::arch::Arch;
use ms_codeview::syms::{SymIter, SymKind};
use ms_coff::IMAGE_FILE_MACHINE;
pub use ms_pdb_msf as msf;
pub use ms_pdb_msfz as msfz;
use tracing::warn;
mod coff_groups;
mod embedded_sources;
pub mod names;
pub mod pdbi;
mod stream_index;
pub mod tpi;
pub mod utils;
pub mod writer;

pub use bstr::BStr;
pub use coff_groups::{CoffGroup, CoffGroups};
pub use container::{Container, StreamReader};
pub use ms_codeview::{self as codeview, syms, types};
pub use ms_coff::{self as coff, IMAGE_SECTION_HEADER};
pub use msfz::StreamData;
pub use stream_index::{NIL_STREAM_INDEX, Stream, StreamIndexIsNilError, StreamIndexU16};
pub use sync_file::{RandomAccessFile, ReadAt, WriteAt};

use anyhow::bail;
use globals::gsi::GlobalSymbolIndex;
use globals::gss::GlobalSymbolStream;
use globals::psi::PublicSymbolIndex;
use names::{NameIndex, NamesStream};
use std::cell::OnceCell;
use std::fmt::Debug;
use std::fs::File;
use std::path::Path;
use syms::{Pub, Sym};
use zerocopy::{FromZeros, IntoBytes};

use crate::dbi::ModuleInfo;
use crate::dbi::optional_dbg::OptionalDebugHeaders;

#[cfg(test)]
#[static_init::dynamic]
static INIT_LOGGER: () = {
    tracing_subscriber::fmt()
        .with_ansi(false)
        .with_test_writer()
        .with_file(true)
        .with_line_number(true)
        .with_max_level(tracing::Level::DEBUG)
        .compact()
        .without_time()
        .finish();
};

/// Allows reading the contents of a PDB file.
///
/// This type provides read-only access. It does not provide any means to modify a PDB file or
/// to create a new one.
pub struct Pdb<F = sync_file::RandomAccessFile> {
    container: Container<F>,

    /// The header of the DBI Stream. The DBI Stream contains many of the important data structures
    /// for PDB, or has pointers (stream indexes) for them. Nearly all programs that read PDBs
    /// need to read the DBI, so we always load the header.
    dbi_header: dbi::DbiStreamHeader,
    dbi_substreams: dbi::DbiSubstreamRanges,

    pdbi: pdbi::PdbiStream,

    cached: PdbCached,
}

#[derive(Default)]
struct PdbCached {
    names: OnceCell<NamesStream<Vec<u8>>>,

    tpi_header: OnceCell<tpi::CachedTypeStreamHeader>,
    ipi_header: OnceCell<tpi::CachedTypeStreamHeader>,

    /// Cached contents of DBI Modules Substream.
    dbi_modules_cell: OnceCell<dbi::ModInfoSubstream<Vec<u8>>>,
    /// Cached contents of DBI Sources Substream.
    dbi_sources_cell: OnceCell<Vec<u8>>,

    gss: OnceCell<Box<GlobalSymbolStream>>,
    gsi: OnceCell<Box<GlobalSymbolIndex>>,
    psi: OnceCell<Box<PublicSymbolIndex>>,

    coff_groups: OnceCell<CoffGroups>,
    optional_dbg_streams: OnceCell<OptionalDebugHeaders>,
    section_headers: OnceCell<Box<[IMAGE_SECTION_HEADER]>>,
}

#[derive(Copy, Clone, Eq, PartialEq)]
enum AccessMode {
    Read,
    ReadWrite,
}

impl<F: ReadAt> Pdb<F> {
    /// Reads the header of a PDB file and provides access to the streams contained within the
    /// PDB file. Allows read/write access, if using an MSF container format.
    ///
    /// This function reads the MSF File Header, which is the header for the entire file.
    /// It also reads the stream directory, so it knows how to find each of the streams
    /// and the pages of the streams.
    fn from_file_access(file: F, access_mode: AccessMode) -> anyhow::Result<Box<Self>> {
        use crate::taster::{Flavor, what_flavor};

        let Some(flavor) = what_flavor(&file)? else {
            bail!("The file is not a recognized PDB or PDZ format.");
        };

        let container = match (flavor, access_mode) {
            (Flavor::PortablePdb, _) => bail!("Portable PDBs are not supported."),
            (Flavor::Pdb, AccessMode::Read) => Container::Msf(msf::Msf::open_with_file(file)?),
            (Flavor::Pdb, AccessMode::ReadWrite) => {
                Container::Msf(msf::Msf::modify_with_file(file)?)
            }
            (Flavor::Pdz, AccessMode::Read) => Container::Msfz(msfz::Msfz::from_file(file)?),
            (Flavor::Pdz, AccessMode::ReadWrite) => {
                bail!("The MSFZ file format is read-only.")
            }
        };

        let dbi_header = dbi::read_dbi_stream_header(&container)?;
        let stream_len = container.stream_len(Stream::DBI.into());
        let dbi_substreams = if stream_len != 0 {
            dbi::DbiSubstreamRanges::from_sizes(&dbi_header, stream_len as usize)?
        } else {
            dbi::DbiSubstreamRanges::default()
        };

        let pdbi_stream_data = container.read_stream_to_vec(Stream::PDB.into())?;
        let pdbi = pdbi::PdbiStream::parse(&pdbi_stream_data)?;

        Ok(Box::new(Self {
            container,
            dbi_header,
            dbi_substreams,
            pdbi,
            cached: Default::default(),
        }))
    }

    /// Gets access to the PDB Information Stream.
    ///
    /// This loads the PDBI on-demand. The PDBI is usually fairly small.
    pub fn pdbi(&self) -> &pdbi::PdbiStream {
        &self.pdbi
    }

    /// Gets access to the Named Streams table.
    pub fn named_streams(&self) -> &pdbi::NamedStreams {
        &self.pdbi.named_streams
    }

    /// Gets mutable access to the Named Streams table.
    pub fn named_streams_mut(&mut self) -> &mut pdbi::NamedStreams {
        &mut self.pdbi.named_streams
    }

    /// Searches the Named Streams table for a stream with a given name.
    /// Returns `None` if the stream is not found.
    pub fn named_stream(&self, name: &str) -> Option<u32> {
        self.pdbi.named_streams().get(name)
    }

    /// Searches the Named Streams table for a stream with a given name.
    /// Returns an error if the stream is not found.
    pub fn named_stream_err(&self, name: &str) -> anyhow::Result<u32> {
        if let Some(s) = self.pdbi.named_streams().get(name) {
            Ok(s)
        } else {
            anyhow::bail!("There is no stream with the name {:?}.", name);
        }
    }

    /// The header of the DBI Stream.
    pub fn dbi_header(&self) -> &dbi::DbiStreamHeader {
        &self.dbi_header
    }

    /// The byte ranges of the DBI substreams.
    pub fn dbi_substreams(&self) -> &dbi::DbiSubstreamRanges {
        &self.dbi_substreams
    }

    /// Gets the TPI Stream Header.
    ///
    /// This loads the TPI Stream Header on-demand. This does not load the rest of the TPI Stream.
    pub fn tpi_header(&self) -> anyhow::Result<&tpi::CachedTypeStreamHeader> {
        self.tpi_or_ipi_header(Stream::TPI, &self.cached.tpi_header)
    }

    /// Gets the IPI Stream Header.
    ///
    /// This loads the IPI Stream Header on-demand. This does not load the rest of the TPI Stream.
    pub fn ipi_header(&self) -> anyhow::Result<&tpi::CachedTypeStreamHeader> {
        self.tpi_or_ipi_header(Stream::IPI, &self.cached.ipi_header)
    }

    fn tpi_or_ipi_header<'s>(
        &'s self,
        stream: Stream,
        cell: &'s OnceCell<tpi::CachedTypeStreamHeader>,
    ) -> anyhow::Result<&'s tpi::CachedTypeStreamHeader> {
        get_or_init_err(cell, || {
            let r = self.get_stream_reader(stream.into())?;
            let mut header = tpi::TypeStreamHeader::new_zeroed();
            let header_bytes = header.as_mut_bytes();
            let bytes_read = r.read_at(header_bytes, 0)?;
            if bytes_read == 0 {
                // This stream is zero-length.
                return Ok(tpi::CachedTypeStreamHeader { header: None });
            }

            if bytes_read < header_bytes.len() {
                bail!(
                    "The type stream (stream {}) does not contain enough data for a valid header.",
                    stream
                );
            }

            Ok(tpi::CachedTypeStreamHeader {
                header: Some(header),
            })
        })
    }

    /// Gets the Names Stream
    ///
    /// This loads the Names Stream on-demand.
    pub fn names(&self) -> anyhow::Result<&NamesStream<Vec<u8>>> {
        get_or_init_err(&self.cached.names, || {
            if let Some(stream) = self.named_stream(names::NAMES_STREAM_NAME) {
                let stream_data = self.read_stream_to_vec(stream)?;
                Ok(NamesStream::parse(stream_data)?)
            } else {
                let stream_data = names::EMPTY_NAMES_STREAM_DATA.to_vec();
                Ok(NamesStream::parse(stream_data)?)
            }
        })
    }

    /// Gets a name from the Names Stream.
    pub fn get_name(&self, offset: NameIndex) -> anyhow::Result<&BStr> {
        let names = self.names()?;
        names.get_string(offset)
    }

    /// The binding key that associates this PDB with a given PE executable.
    pub fn binding_key(&self) -> BindingKey {
        let pdbi = self.pdbi();
        pdbi.binding_key()
    }

    /// Checks whether this PDB has a given feature enabled.
    pub fn has_feature(&self, feature_code: pdbi::FeatureCode) -> bool {
        self.pdbi.has_feature(feature_code)
    }

    /// Indicates that this PDB was built using the "Mini PDB" option, i.e. `/DEBUG:FASTLINK`.
    pub fn mini_pdb(&self) -> bool {
        self.has_feature(pdbi::FeatureCode::MINI_PDB)
    }

    /// Gets a reference to the Global Symbol Stream (GSS). This loads the GSS on-demand.
    #[inline]
    pub fn gss(&self) -> anyhow::Result<&GlobalSymbolStream> {
        if let Some(gss) = self.cached.gss.get() {
            Ok(gss)
        } else {
            self.gss_slow()
        }
    }

    /// Gets a reference to the Global Symbol Stream (GSS). This loads the GSS on-demand.
    #[inline(never)]
    fn gss_slow(&self) -> anyhow::Result<&GlobalSymbolStream> {
        let box_ref = get_or_init_err(
            &self.cached.gss,
            || -> anyhow::Result<Box<GlobalSymbolStream>> { Ok(Box::new(self.read_gss()?)) },
        )?;
        Ok(box_ref)
    }

    /// If the GSS has been loaded by using the `gss()` function, then this method frees it.
    pub fn gss_drop(&mut self) {
        self.cached.gss.take();
    }

    /// Gets a reference to the Global Symbol Index (GSI). This loads the GSI on-demand.
    #[inline(never)]
    pub fn gsi(&self) -> anyhow::Result<&GlobalSymbolIndex> {
        if let Some(gsi) = self.cached.gsi.get() {
            Ok(gsi)
        } else {
            self.gsi_slow()
        }
    }

    #[inline(never)]
    fn gsi_slow(&self) -> anyhow::Result<&GlobalSymbolIndex> {
        let box_ref = get_or_init_err(
            &self.cached.gsi,
            || -> anyhow::Result<Box<GlobalSymbolIndex>> { Ok(Box::new(self.read_gsi()?)) },
        )?;
        Ok(box_ref)
    }

    /// If the GSI has been loaded by using the `gsi()` function, then this method frees it.
    pub fn gsi_drop(&mut self) {
        self.cached.gsi.take();
    }

    /// Gets a reference to the Public Symbol Index (PSI). This loads the PSI on-demand.
    #[inline]
    pub fn psi(&self) -> anyhow::Result<&PublicSymbolIndex> {
        if let Some(psi) = self.cached.psi.get() {
            Ok(psi)
        } else {
            self.psi_slow()
        }
    }

    #[inline(never)]
    fn psi_slow(&self) -> anyhow::Result<&PublicSymbolIndex> {
        let box_ref = get_or_init_err(
            &self.cached.psi,
            || -> anyhow::Result<Box<PublicSymbolIndex>> { Ok(Box::new(self.read_psi()?)) },
        )?;
        Ok(box_ref)
    }

    /// If the PSI has been loaded by using the `psi()` function, then this method frees it.
    pub fn psi_drop(&mut self) {
        self.cached.psi.take();
    }

    /// Searches for an `S_PUB32` symbol by name.
    pub fn find_public_by_name(&self, name: &BStr) -> anyhow::Result<Option<Pub<'_>>> {
        let gss = self.gss()?;
        let psi = self.psi()?;
        psi.find_symbol_by_name(gss, name)
    }

    /// Searches for a global symbol symbol by name.
    ///
    /// This uses the Global Symbol Index (GSI). This index _does not_ contain `S_PUB32` records.
    /// Use `find_public_by_name` to search for `S_PUB32` records.
    pub fn find_global_by_name(&self, name: &'_ BStr) -> anyhow::Result<Option<Sym<'_>>> {
        let gss = self.gss()?;
        let gsi = self.gsi()?;
        gsi.find_symbol(gss, name)
    }

    /// Writes any changes that have been buffered in memory to disk. However, this does not commit
    /// the changes. It is still necessary to call the `commit()` method.
    ///
    /// The return value indicates whether any changes were written to disk. `Ok(true)` indicates
    /// that some change were written to disk.  `Ok(false)` indicates that there were no buffered
    /// changes and nothing has been written to disk.
    pub fn flush_all(&mut self) -> anyhow::Result<bool>
    where
        F: WriteAt,
    {
        let mut any = false;

        if self.pdbi.named_streams.modified {
            let pdbi_data = self.pdbi.to_bytes()?;
            let mut w = self.msf_mut_err()?.write_stream(Stream::PDB.into())?;
            w.set_contents(&pdbi_data)?;
            self.pdbi.named_streams.modified = false;
            any = true;
        }

        Ok(any)
    }

    /// Gets access to the underlying container.
    pub fn container(&self) -> &Container<F> {
        &self.container
    }

    /// Find the `"* Linker *"` module, which contains the S_COFFGROUP symbols.
    ///
    /// If the PDB does not contain a linker module then this returns `Err`.
    pub fn linker_module(&self) -> anyhow::Result<ModuleInfo<'_>> {
        if let Some(module) = self.linker_module_opt()? {
            Ok(module)
        } else {
            bail!("This PDB does not contain a linker module.");
        }
    }

    /// Find the `"* Linker *"` module, which contains the S_COFFGROUP symbols.
    ///
    /// If the PDB does not contain a linker module then this returns `Ok(None)`.
    pub fn linker_module_opt(&self) -> anyhow::Result<Option<ModuleInfo<'_>>> {
        let modules = self.modules()?;
        for module in modules.iter() {
            if module.module_name == LINKER_MODULE_NAME {
                return Ok(Some(module));
            }
        }
        Ok(None)
    }

    /// Gets the list of COFF groups defined in this binary.
    pub fn coff_groups(&self) -> anyhow::Result<&CoffGroups> {
        get_or_init_err(&self.cached.coff_groups, || self.read_coff_groups())
    }

    /// Reads (uncached) the list of COFF groups defined in this binary.
    pub fn read_coff_groups(&self) -> anyhow::Result<CoffGroups> {
        // S_COFFGROUP symbols are defined in the linker module.
        let Some(linker_module) = self.linker_module_opt()? else {
            return Ok(CoffGroups { vec: Vec::new() });
        };

        let Some(linker_module_stream) = linker_module.stream() else {
            bail!("The linker module does not contain any symbols.");
        };

        let mut linker_module_syms: Vec<u8> = vec![0; linker_module.sym_size() as usize];
        let sr = self.get_stream_reader(linker_module_stream)?;
        sr.read_exact_at(&mut linker_module_syms, 0)?;

        // Count the number of S_COFFGROUP symbols. We can use this to do a precise allocation.
        let mut num_coff_groups: usize = 0;
        for sym in SymIter::for_module_syms(&linker_module_syms) {
            if sym.kind == SymKind::S_COFFGROUP {
                num_coff_groups += 1;
            }
        }

        let mut groups = Vec::with_capacity(num_coff_groups);

        for sym in SymIter::for_module_syms(&linker_module_syms) {
            if sym.kind == SymKind::S_COFFGROUP {
                match sym.parse_as::<ms_codeview::syms::CoffGroup>() {
                    Ok(group) => {
                        groups.push(CoffGroup {
                            name: group.name.to_string(),
                            characteristics: group.fixed.characteristics.get(),
                            offset_segment: group.fixed.off_seg,
                            size: group.fixed.cb.get(),
                        });
                    }
                    Err(_) => {
                        warn!("failed to parse S_COFFGROUP symbol");
                    }
                }
            }
        }

        groups.sort_unstable_by_key(|g| g.offset_segment);

        Ok(CoffGroups { vec: groups })
    }

    /// Returns the target architecture for this PE binary.
    pub fn machine(&self) -> IMAGE_FILE_MACHINE {
        IMAGE_FILE_MACHINE(self.dbi_header.machine.get())
    }

    /// Returns the target CPU architecture.
    pub fn arch(&self) -> anyhow::Result<Arch> {
        match self.machine() {
            IMAGE_FILE_MACHINE::IMAGE_FILE_MACHINE_AMD64 => Ok(Arch::AMD64),
            IMAGE_FILE_MACHINE::IMAGE_FILE_MACHINE_ARM64 => Ok(Arch::ARM64),
            IMAGE_FILE_MACHINE::IMAGE_FILE_MACHINE_I386 => Ok(Arch::X86),
            _ => bail!("target machine not supported"),
        }
    }
}

fn get_or_init_err<T, E, F: FnOnce() -> Result<T, E>>(cell: &OnceCell<T>, f: F) -> Result<&T, E> {
    if let Some(value) = cell.get() {
        return Ok(value);
    }

    match f() {
        Ok(value) => {
            let _ = cell.set(value);
            Ok(cell.get().unwrap())
        }
        Err(e) => Err(e),
    }
}

impl Pdb<RandomAccessFile> {
    /// Opens a PDB file.
    pub fn open(file_name: &Path) -> anyhow::Result<Box<Pdb<RandomAccessFile>>> {
        let f = ms_pdb_msf::open_options_shared(File::options().read(true)).open(file_name)?;
        let random_file = RandomAccessFile::from(f);
        Self::from_file_access(random_file, AccessMode::Read)
    }

    /// Reads the header of a PDB file and provides access to the streams contained within the
    /// PDB file.
    ///
    /// This function reads the MSF File Header, which is the header for the entire file.
    /// It also reads the stream directory, so it knows how to find each of the streams
    /// and the pages of the streams.
    pub fn open_from_file(file: File) -> anyhow::Result<Box<Self>> {
        let random_file = RandomAccessFile::from(file);
        Self::from_file_access(random_file, AccessMode::Read)
    }

    /// Opens a PDB file for editing. The file must use the MSF container format.
    pub fn modify(filename: &Path) -> anyhow::Result<Box<Pdb<sync_file::RandomAccessFile>>> {
        let file = File::options().read(true).write(true).open(filename)?;
        let random_file = sync_file::RandomAccessFile::from(file);
        Self::from_file_access(random_file, AccessMode::ReadWrite)
    }

    /// Opens an existing PDB file for read/write access, given a file name.
    ///
    /// The file _must_ use the MSF container format. MSFZ is not supported for read/write access.
    pub fn modify_from_file(file: File) -> anyhow::Result<Box<Self>> {
        let random_file = RandomAccessFile::from(file);
        Self::from_file_access(random_file, AccessMode::ReadWrite)
    }
}

impl<F: ReadAt> Pdb<F> {
    /// Reads the header of a PDB file and provides access to the streams contained within the
    /// PDB file.
    ///
    /// This function reads the MSF File Header, which is the header for the entire file.
    /// It also reads the stream directory, so it knows how to find each of the streams
    /// and the pages of the streams.
    pub fn open_from_random_file(random_file: F) -> anyhow::Result<Box<Self>> {
        Self::from_file_access(random_file, AccessMode::Read)
    }

    /// Opens an existing PDB file for read/write access, given a file name.
    ///
    /// The file _must_ using the MSF container format. MSFZ is not supported for read/write access.
    pub fn modify_from_random_file(random_file: F) -> anyhow::Result<Box<Self>> {
        Self::from_file_access(random_file, AccessMode::ReadWrite)
    }
}

impl<F> std::ops::Deref for Pdb<F> {
    type Target = Container<F>;

    fn deref(&self) -> &Self::Target {
        &self.container
    }
}

impl<F> std::ops::DerefMut for Pdb<F> {
    fn deref_mut(&mut self) -> &mut Self::Target {
        &mut self.container
    }
}

/// This is the key used to associate a given PE executable (DLL or EXE) with a PDB.
/// All values come from the PDBI stream.
#[derive(Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub struct BindingKey {
    /// The GUID. When MSVC tools are run in deterministic mode, this value is a hash of the PE
    /// image, rather than being assigned using an RNG.
    pub guid: uuid::Uuid,
    /// The age of the executable. This is incremented every time the DLL + PDB are modified.
    pub age: u32,
}

impl Debug for BindingKey {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        if self.age > 0x1000 {
            write!(f, "{:?} age 0x{:x}", self.guid, self.age)
        } else {
            write!(f, "{:?} age {}", self.guid, self.age)
        }
    }
}

/// The name of the special "linker" module.
///
/// The linker module is created by the linker and is not an input to the linker. It contains
/// special / well-known symbols, such as `S_COFFGROUP`.
pub const LINKER_MODULE_NAME: &str = "* Linker *";

```

`pdb/src/lines.rs`:

```rs
//! Decodes line information found in Module Streams.
//!
//! # References
//! * [/ZH (Hash algorithm for calculation of file checksum in debug info)](https://learn.microsoft.com/en-us/cpp/build/reference/zh?view=msvc-170)

mod checksum;
mod subsection;

pub use checksum::*;
pub use subsection::*;

use crate::codeview::syms::OffsetSegment;
use crate::names::NameIndex;
use anyhow::{Context, bail};
use ms_codeview::parser::{Parser, ParserError, ParserMut};
use ms_codeview::{HasRestLen, IteratorWithRangesExt};
use std::mem::{size_of, take};
use tracing::{trace, warn};
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, LE, U16, U32, Unaligned};

/// Enumerates the kind of subsections found in C13 Line Data.
///
/// See `cvinfo.h`, `DEBUG_S_SUBSECTION_TYPE`.
#[derive(Copy, Clone, Eq, PartialEq)]
#[repr(transparent)]
pub struct SubsectionKind(pub u32);

macro_rules! subsections {
    ($( $(#[$a:meta])*  $name:ident = $value:expr;)*) => {
        impl SubsectionKind {
            $(
                $(#[$a])*
                #[allow(missing_docs)]
                pub const $name: SubsectionKind = SubsectionKind($value);
            )*
        }

        impl std::fmt::Debug for SubsectionKind {
            fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
                let s: &str = match *self {
                    $( SubsectionKind::$name => stringify!($name), )*
                    _ => return write!(fmt, "??(0x{:x})", self.0),
                };
                fmt.write_str(s)
            }
        }
    }
}

subsections! {
    SYMBOLS = 0xf1;
    /// Contains C13 Line Data
    LINES = 0xf2;
    STRING_TABLE = 0xf3;
    /// Contains file checksums and pointers to file names. For a given module, there should be
    /// at most one `FILE_CHECKSUMS` subsection.
    FILE_CHECKSUMS = 0xf4;

    FRAMEDATA = 0xF5;
    INLINEELINES = 0xF6;
    CROSSSCOPEIMPORTS = 0xF7;
    CROSSSCOPEEXPORTS = 0xF8;

    IL_LINES = 0xF9;
    FUNC_MDTOKEN_MAP = 0xFA;
    TYPE_MDTOKEN_MAP = 0xFB;
    MERGED_ASSEMBLYINPUT = 0xFC;

    COFF_SYMBOL_RVA = 0xFD;
}

/// Enables decoding of the line data stored in a Module Stream. This decodes the "C13 line data"
/// substream.
pub struct LineData<'a> {
    bytes: &'a [u8],
}

impl<'a> LineData<'a> {
    /// Use this to create a new decoder for the C13 line data. Usually, you want to pass the
    /// result of calling `ModiStreamData::c13_line_data_bytes()` to this function.
    pub fn new(bytes: &'a [u8]) -> Self {
        Self { bytes }
    }

    /// Iterates subsections
    pub fn subsections(&self) -> SubsectionIter<'a> {
        SubsectionIter::new(self.bytes)
    }

    /// Finds the `FILE_CHECKSUMS` subsection. There should only be one.
    pub fn find_checksums_bytes(&self) -> Option<&'a [u8]> {
        for subsection in self.subsections() {
            if subsection.kind == SubsectionKind::FILE_CHECKSUMS {
                return Some(subsection.data);
            }
        }
        None
    }

    /// Finds the `FILE_CHECKSUMS` subsection. There should only be one.
    pub fn find_checksums(&self) -> Option<FileChecksumsSubsection<'a>> {
        let subsection_bytes = self.find_checksums_bytes()?;
        Some(FileChecksumsSubsection::new(subsection_bytes))
    }

    /// Iterates the `NameIndex` values that appear in this Line Data section.
    ///
    /// This may iterate the same `NameIndex` value more than once.
    pub fn iter_name_index<F>(&self, mut f: F) -> anyhow::Result<()>
    where
        F: FnMut(NameIndex),
    {
        if let Some(checksums) = self.find_checksums() {
            for subsection in self.subsections() {
                match subsection.kind {
                    SubsectionKind::LINES => {
                        let lines_subsection = LinesSubsection::parse(subsection.data)?;
                        for block in lines_subsection.blocks() {
                            let file = checksums.get_file(block.header.file_index.get())?;
                            let ni = file.header.name.get();
                            f(NameIndex(ni));
                        }
                    }
                    _ => {}
                }
            }
        } else {
            for subsection in self.subsections() {
                match subsection.kind {
                    SubsectionKind::LINES => {
                        bail!(
                            "This C13 Line Data substream contains LINES subsections, but does not contain a FILE_CHECKSUMS subsection."
                        );
                    }
                    _ => {}
                }
            }
        };

        Ok(())
    }
}

/// Enables decoding of the line data stored in a Module Stream. This decodes the "C13 line data"
/// substream.
pub struct LineDataMut<'a> {
    bytes: &'a mut [u8],
}

impl<'a> LineDataMut<'a> {
    /// Initializes a new `LineDataMut`. This does not validate the contents of the data.
    pub fn new(bytes: &'a mut [u8]) -> Self {
        Self { bytes }
    }

    /// Iterates subsections, with mutable access.
    pub fn subsections_mut(&mut self) -> SubsectionIterMut<'_> {
        SubsectionIterMut::new(self.bytes)
    }

    /// Iterates through all of the name indexes stored within this Line Data.
    /// Remaps all entries using `f` as the remapping function.
    ///
    /// `NameIndex` values are found in the `FILE_CHECKSUMS` debug subsections. However, it is not
    /// possible to directly enumerate the entries stored within a `FILE_CHECKSUMS` subsection,
    /// because they are not at guaranteed positions. There may be gaps.
    ///
    /// To find the `NameIndex` values within each `FILE_CHECKSUMS` debug subsection, we first scan
    /// the `LINES` subsections that point to them, and use a `HashSet` to avoid modifying the
    /// same `NameIndex` more than once.
    pub fn remap_name_indexes<F>(&mut self, name_remapping: F) -> anyhow::Result<()>
    where
        F: Fn(NameIndex) -> anyhow::Result<NameIndex>,
    {
        for subsection in self.subsections_mut() {
            match subsection.kind {
                SubsectionKind::FILE_CHECKSUMS => {
                    let mut checksums = FileChecksumsSubsectionMut::new(subsection.data);
                    for checksum in checksums.iter_mut() {
                        // This `name_offset` value points into the Names stream (/names).
                        let old_name = NameIndex(checksum.header.name.get());
                        let new_name = name_remapping(old_name)
                            .with_context(|| format!("old_name: {old_name}"))?;
                        checksum.header.name = U32::new(new_name.0);
                    }
                }

                _ => {}
            }
        }

        Ok(())
    }
}

/// Represents one contribution. Each contribution consists of a sequence of variable-length
/// blocks.
///
/// Each `LINES` subsection represents one "contribution", which has a `ContributionHeader`,
/// followed by a sequence of blocks. Each block is a variable-length record.
pub struct LinesSubsection<'a> {
    /// The fixed-size header of the `Lines` subsection.
    pub contribution: &'a Contribution,
    /// Contains a sequence of variable-sized "blocks". Each block specifies a source file
    /// and a set of mappings from instruction offsets to line numbers within that source file.
    pub blocks_data: &'a [u8],
}

impl<'a> LinesSubsection<'a> {
    /// Parses the contribution header and prepares for iteration of blocks.
    pub fn parse(bytes: &'a [u8]) -> Result<Self, ParserError> {
        let mut p = Parser::new(bytes);
        Ok(Self {
            contribution: p.get()?,
            blocks_data: p.into_rest(),
        })
    }

    /// Iterates through the line number blocks.
    pub fn blocks(&self) -> IterBlocks<'a> {
        IterBlocks {
            bytes: self.blocks_data,
            have_columns: self.contribution.have_columns(),
        }
    }
}

/// Represents one contribution. Each contribution consists of a sequence of variable-length
/// blocks.
///
/// Each `LINES` subsection represents one "contribution", which has a `ContributionHeader`,
/// followed by a sequence of blocks. Each block is a variable-length record.
pub struct LinesSubsectionMut<'a> {
    /// The fixed-size header of the `Lines` subsection.
    pub contribution: &'a mut Contribution,
    /// Contains a sequence of variable-sized "blocks". Each block specifies a source file
    /// and a set of mappings from instruction offsets to line numbers within that source file.
    pub blocks_data: &'a mut [u8],
}

impl<'a> LinesSubsectionMut<'a> {
    /// Parses the contribution header and prepares for iteration of blocks.
    pub fn parse(bytes: &'a mut [u8]) -> Result<Self, ParserError> {
        let mut p = ParserMut::new(bytes);
        Ok(Self {
            contribution: p.get_mut()?,
            blocks_data: p.into_rest(),
        })
    }

    /// Iterates through the line number blocks.
    pub fn blocks(&self) -> IterBlocks<'_> {
        IterBlocks {
            bytes: self.blocks_data,
            have_columns: self.contribution.have_columns(),
        }
    }

    /// Iterates through the line number blocks, with mutable access.
    pub fn blocks_mut(&mut self) -> IterBlocksMut<'_> {
        IterBlocksMut {
            bytes: self.blocks_data,
            have_columns: self.contribution.have_columns(),
        }
    }
}

/// Iterator state for `LinesSubsection::blocks`.
pub struct IterBlocks<'a> {
    bytes: &'a [u8],
    have_columns: bool,
}

impl<'a> HasRestLen for IterBlocks<'a> {
    fn rest_len(&self) -> usize {
        self.bytes.len()
    }
}

impl<'a> Iterator for IterBlocks<'a> {
    type Item = Block<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.bytes.is_empty() {
            return None;
        }

        let mut p = Parser::new(self.bytes);
        let Ok(header) = p.get::<BlockHeader>() else {
            warn!("failed to read BlockHeader");
            return None;
        };

        let block_size: usize = header.block_size.get() as usize;
        let Some(data_len) = block_size.checked_sub(size_of::<BlockHeader>()) else {
            warn!("invalid block; block_size is less than size of block header");
            return None;
        };

        trace!(
            file_index = header.file_index.get(),
            num_lines = header.num_lines.get(),
            block_size = header.block_size.get(),
            data_len,
            "block header"
        );

        let Ok(data) = p.bytes(data_len) else {
            warn!(
                needed_bytes = data_len,
                have_bytes = p.len(),
                "invalid block: need more bytes for block contents"
            );
            return None;
        };

        self.bytes = p.into_rest();
        Some(Block {
            header,
            data,
            have_columns: self.have_columns,
        })
    }
}

/// Iterator state for `LinesSubsection::blocks`.
pub struct IterBlocksMut<'a> {
    bytes: &'a mut [u8],
    have_columns: bool,
}

impl<'a> HasRestLen for IterBlocksMut<'a> {
    fn rest_len(&self) -> usize {
        self.bytes.len()
    }
}

impl<'a> Iterator for IterBlocksMut<'a> {
    type Item = BlockMut<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.bytes.is_empty() {
            return None;
        }

        let mut p = ParserMut::new(take(&mut self.bytes));
        let Ok(header) = p.get_mut::<BlockHeader>() else {
            warn!("failed to read BlockHeader");
            return None;
        };

        let block_size: usize = header.block_size.get() as usize;
        let Some(data_len) = block_size.checked_sub(size_of::<BlockHeader>()) else {
            warn!("invalid block; block_size is less than size of block header");
            return None;
        };

        trace!(
            "block header: file_index = {}, num_lines = {}, block_size = {}, data_len = {}",
            header.file_index.get(),
            header.num_lines.get(),
            header.block_size.get(),
            data_len
        );

        let Ok(data) = p.bytes_mut(data_len) else {
            warn!(
                "invalid block: need {} bytes for block contents, only have {}",
                data_len,
                p.len()
            );
            return None;
        };

        self.bytes = p.into_rest();
        Some(BlockMut {
            header,
            data,
            have_columns: self.have_columns,
        })
    }
}

/// One block of line data. Each block has a header which points to a source file. All of the line
/// locations within the block point to line numbers (and potentially column numbers) within that
/// source file.
pub struct Block<'a> {
    /// Fixed-size header for the block.
    pub header: &'a BlockHeader,
    /// If `true`, then this block has column numbers as well as line numbers.
    pub have_columns: bool,
    /// Contains the encoded line numbers, followed by column numbers. The number of entries is
    /// specified by `header.num_lines`.
    pub data: &'a [u8],
}

impl<'a> Block<'a> {
    /// Gets the line records for this block.
    pub fn lines(&self) -> &'a [LineRecord] {
        let num_lines = self.header.num_lines.get() as usize;
        if let Ok((lines, _)) = <[LineRecord]>::ref_from_prefix_with_elems(self.data, num_lines) {
            lines
        } else {
            warn!("failed to get lines_data for a block; wrong size");
            &[]
        }
    }

    /// Gets the column records for this block, if it has any.
    pub fn columns(&self) -> Option<&'a [ColumnRecord]> {
        if !self.have_columns {
            return None;
        }

        let num_lines = self.header.num_lines.get() as usize;
        let lines_size = num_lines * size_of::<LineRecord>();
        let Some(column_data) = self.data.get(lines_size..) else {
            warn!("failed to get column data for a block; wrong size");
            return None;
        };

        let Ok((columns, _)) = <[ColumnRecord]>::ref_from_prefix_with_elems(column_data, num_lines)
        else {
            warn!("failed to get column data for a block; byte size is wrong");
            return None;
        };

        Some(columns)
    }
}

/// One block of line data. Each block has a header which points to a source file. All of the line
/// locations within the block point to line numbers (and potentially column numbers) within that
/// source file.
pub struct BlockMut<'a> {
    /// Fixed-size header for the block.
    pub header: &'a mut BlockHeader,
    /// If `true`, then this block has column numbers as well as line numbers.
    pub have_columns: bool,
    /// Contains the encoded line numbers, followed by column numbers. The number of entries is
    /// specified by `header.num_lines`.
    pub data: &'a mut [u8],
}

/// A single line record
///
/// See `CV_Line_t` in `cvinfo.h`
#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Clone)]
#[repr(C)]
pub struct LineRecord {
    /// The byte offset from the start of this contribution (in the instruction stream, not the
    /// Lines Data) for this line
    pub offset: U32<LE>,

    /// Encodes three bit-fields
    ///
    /// * Bits 0-23 are `line_num_start`. This is the 1-based starting line number within the source
    ///   file of this line record.
    /// * Bits 24-30 are `delta_line_end`. It specifies a value to add to line_num_start to find the
    ///   ending line. If this value is zero, then this line record encodes only a single line, not
    ///   a span of lines.
    /// * Bit 31 is the `statement` bit field. If set to 1, it indicates that this line record describes a statement.
    pub flags: U32<LE>,
}

impl LineRecord {
    /// The line number of this location. This value is 1-based.
    pub fn line_num_start(&self) -> u32 {
        self.flags.get() & 0x00_ff_ff_ff
    }

    /// If non-zero, then this indicates the delta in bytes within the source file from the start
    /// of the source location to the end of the source location.
    pub fn delta_line_end(&self) -> u8 {
        ((self.flags.get() >> 24) & 0x7f) as u8
    }

    /// True if this location points to a statement.
    pub fn statement(&self) -> bool {
        (self.flags.get() >> 31) != 0
    }
}

impl std::fmt::Debug for LineRecord {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        write!(fmt, "+{} L{}", self.offset.get(), self.line_num_start())?;

        let delta_line_end = self.delta_line_end();
        if delta_line_end != 0 {
            write!(fmt, "..+{delta_line_end}")?;
        }

        if self.statement() {
            write!(fmt, " S")?;
        }

        Ok(())
    }
}

/// A single column record
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned)]
#[repr(C)]
pub struct ColumnRecord {
    /// byte offset in a source line
    pub start_offset: U16<LE>,
    /// byte offset in a source line
    pub end_offset: U16<LE>,
}

#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned)]
#[repr(C)]
#[allow(missing_docs)]
pub struct Contribution {
    pub offset: U32<LE>,
    pub segment: U16<LE>,
    pub flags: U16<LE>,
    pub size: U32<LE>,
    // Followed by a sequence of block records. Each block is variable-length and begins with
    // BlockHeader.
}

impl Contribution {
    /// Indicates whether this block (contribution) also has column numbers.
    pub fn have_columns(&self) -> bool {
        (self.flags.get() & CV_LINES_HAVE_COLUMNS) != 0
    }

    /// Get the `segment:offset` of this contribution.
    pub fn offset_segment(&self) -> OffsetSegment {
        OffsetSegment {
            offset: self.offset,
            segment: self.segment,
        }
    }
}

/// Bit flag for `Contribution::flags` field
pub const CV_LINES_HAVE_COLUMNS: u16 = 0x0001;

#[allow(missing_docs)]
pub struct LinesEntry<'a> {
    pub header: &'a Contribution,
    pub blocks: &'a [u8],
}

/// Header for a variable-length Block record.
///
/// Each block contains a sequence of line records, and optionally column records.
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned)]
#[repr(C)]
pub struct BlockHeader {
    /// The byte offset into the file checksums subsection for this file.
    pub file_index: U32<LE>,
    /// The number of `LineRecord` entries that immediately follow this structure. Also, if the
    /// contribution header indicates that the contribution has column values, this specifies
    /// the number of column records that follow the file records.
    pub num_lines: U32<LE>,
    /// Size of the data for this block. This value includes the size of the block header itself,
    /// so the minimum value value is 12.
    pub block_size: U32<LE>,
    // Followed by [u8; block_size - 12]. This data contains [LineRecord; num_lines], optionally
    // followed by [ColumnRecord; num_lines].
}

/// Updates a C13 Line Data substream after NameIndex values have been updated and after
/// file lists for a given module have been rearranged (sorted).
pub fn fixup_c13_line_data(
    file_permutation: &[u32], // maps new-->old for files within a module
    sorted_names: &crate::names::NameIndexMapping,
    c13_line_data: &mut crate::lines::LineDataMut<'_>,
) -> anyhow::Result<()> {
    // maps old --> new, for the file_index values in DEBUG_S_LINES blocks
    let mut checksum_files_mapping: Vec<(u32, u32)> = Vec::with_capacity(file_permutation.len());

    for subsection in c13_line_data.subsections_mut() {
        match subsection.kind {
            SubsectionKind::FILE_CHECKSUMS => {
                let mut checksums = FileChecksumsSubsectionMut::new(subsection.data);
                let mut checksum_ranges = Vec::with_capacity(file_permutation.len());
                for (checksum_range, checksum) in checksums.iter_mut().with_ranges() {
                    // This `name_offset` value points into the Names stream (/names).
                    let old_name = NameIndex(checksum.header.name.get());
                    let new_name = sorted_names
                        .map_old_to_new(old_name)
                        .with_context(|| format!("old_name: {old_name}"))?;
                    checksum.header.name = U32::new(new_name.0);
                    checksum_ranges.push(checksum_range);
                }

                // Next, we are going to rearrange the FileChecksum records within this
                // section, using the permutation that was generated in dbi::sources::sort_sources().

                let mut new_checksums: Vec<u8> = Vec::with_capacity(subsection.data.len());
                for &old_file_index in file_permutation.iter() {
                    let old_range = checksum_ranges[old_file_index as usize].clone();
                    checksum_files_mapping
                        .push((old_range.start as u32, new_checksums.len() as u32));
                    let old_checksum_data = &subsection.data[old_range];
                    new_checksums.extend_from_slice(old_checksum_data);
                }
                checksum_files_mapping.sort_unstable();

                assert_eq!(new_checksums.len(), subsection.data.len());
                subsection.data.copy_from_slice(&new_checksums);
            }

            _ => {}
        }
    }

    // There is a data-flow dependency (on checksum_files_mapping) between these two loops; the
    // loops cannot be combined. The first loop builds checksum_files_mapping; the second loop
    // reads from it.

    for subsection in c13_line_data.subsections_mut() {
        match subsection.kind {
            SubsectionKind::LINES => {
                // We need to rewrite the file_index values within each line block.
                let mut lines = LinesSubsectionMut::parse(subsection.data)?;
                for block in lines.blocks_mut() {
                    let old_file_index = block.header.file_index.get();
                    match checksum_files_mapping
                        .binary_search_by_key(&old_file_index, |&(old, _new)| old)
                    {
                        Ok(i) => {
                            let (_old, new) = checksum_files_mapping[i];
                            block.header.file_index = U32::new(new);
                        }
                        Err(_) => {
                            bail!(
                                "DEBUG_S_LINES section contains invalid file index: {old_file_index}"
                            );
                        }
                    }
                }
            }

            _ => {}
        }
    }

    Ok(())
}

/// This special line number is part of the "Just My Code" MSVC compiler feature.
///
/// Debuggers that implement the "Just My Code" feature look for this constant when handling
/// "Step Into" requests. If the user asks to "step into" a function call, the debugger will look
/// up the line number of the start of the function. If the line number is `JMC_LINE_NO_STEP_INTO`,
/// then the debugger will _not_ step into the function. Instead, it will step over it.
///
/// This is useful for implementations of standard library functions, like
/// `std::vector<T>::size()`. Often calls to such functions are embedded in complex statements,
/// and the user wants to debug other parts of the complex statement, not the `size()` call.
///
/// # References
/// * <https://learn.microsoft.com/en-us/cpp/build/reference/jmc?view=msvc-170>
/// * <https://learn.microsoft.com/en-us/visualstudio/debugger/just-my-code>
pub const JMC_LINE_NO_STEP_INTO: u32 = 0xf00f00;

/// This special line number is part of the "Just My Code" MSVC compiler feature.
pub const JMC_LINE_FEE_FEE: u32 = 0xfeefee;

/// Returns true if `line` is a number that is used by the "Just My Code" MSVC compiler feature.
pub fn is_jmc_line(line: u32) -> bool {
    line == JMC_LINE_NO_STEP_INTO || line == JMC_LINE_FEE_FEE
}

```

`pdb/src/lines/checksum.rs`:

```rs
//! Code for the `FILE_CHECKSUMS` subsection.

use super::*;

/// The hash algorithm used for the checksum.
#[derive(
    Copy,
    Clone,
    Eq,
    PartialEq,
    Ord,
    PartialOrd,
    IntoBytes,
    FromBytes,
    Unaligned,
    KnownLayout,
    Immutable,
)]
#[repr(transparent)]
pub struct ChecksumKind(pub u8);

impl ChecksumKind {
    /// No checksum at all
    pub const NONE: ChecksumKind = ChecksumKind(0);
    /// MD-5 checksum. See `/ZH:MD5` for MSVC.
    pub const MD5: ChecksumKind = ChecksumKind(1);
    /// SHA-1 checksum. See `/ZH:SHA1` for MSVC
    pub const SHA_1: ChecksumKind = ChecksumKind(2);
    /// SHA-256 checksum.  See `/ZH:SHA_256` for MSVC.
    pub const SHA_256: ChecksumKind = ChecksumKind(3);
}

impl std::fmt::Debug for ChecksumKind {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        static NAMES: [&str; 4] = ["NONE", "MD5", "SHA_1", "SHA_256"];

        if let Some(name) = NAMES.get(self.0 as usize) {
            f.write_str(name)
        } else {
            write!(f, "??({})", self.0)
        }
    }
}

#[test]
fn checksum_kind_debug() {
    assert_eq!(format!("{:?}", ChecksumKind::SHA_256), "SHA_256");
    assert_eq!(format!("{:?}", ChecksumKind(42)), "??(42)");
}

/// The File Checksums Subection
///
/// The file checksums subsection contains records for the source files referenced by Line Data.
pub struct FileChecksumsSubsection<'a> {
    #[allow(missing_docs)]
    pub bytes: &'a [u8],
}

impl<'a> FileChecksumsSubsection<'a> {
    #[allow(missing_docs)]
    pub fn new(bytes: &'a [u8]) -> Self {
        Self { bytes }
    }

    /// Iterates the `FileChecksum` records within this subsection.
    pub fn iter(&self) -> FileChecksumIter<'a> {
        FileChecksumIter { bytes: self.bytes }
    }

    /// Given a file index, which is a byte offset into the `FileChecksums` section, gets a
    /// `FileChecksum` value.
    pub fn get_file(&self, file_index: u32) -> anyhow::Result<FileChecksum<'a>> {
        if let Some(b) = self.bytes.get(file_index as usize..) {
            if let Some(c) = FileChecksumIter::new(b).next() {
                Ok(c)
            } else {
                bail!("failed to decode FileChecksum record");
            }
        } else {
            bail!("file index is out of range of file checksums subsection");
        }
    }
}

/// Like `FileChecksums`, but with mutable access
pub struct FileChecksumsSubsectionMut<'a> {
    #[allow(missing_docs)]
    pub bytes: &'a mut [u8],
}

impl<'a> HasRestLen for FileChecksumsSubsectionMut<'a> {
    fn rest_len(&self) -> usize {
        self.bytes.len()
    }
}

impl<'a> FileChecksumsSubsectionMut<'a> {
    #[allow(missing_docs)]
    pub fn new(bytes: &'a mut [u8]) -> Self {
        Self { bytes }
    }

    /// Iterates the `FileChecksumMut` records within this subsection.
    pub fn iter_mut(&mut self) -> FileChecksumMutIter<'_> {
        FileChecksumMutIter { bytes: self.bytes }
    }

    /// Given a file index, which is a byte offset into the `FileChecksums` section, gets a
    /// `FileChecksumMut` value.
    pub fn get_file_mut(&mut self, file_index: u32) -> anyhow::Result<FileChecksumMut<'_>> {
        if let Some(b) = self.bytes.get_mut(file_index as usize..) {
            if let Some(c) = FileChecksumMutIter::new(b).next() {
                Ok(c)
            } else {
                bail!("failed to decode FileChecksum record");
            }
        } else {
            bail!("file index is out of range of file checksums subsection");
        }
    }
}

/// Points to a single file checksum record.
pub struct FileChecksum<'a> {
    /// The fixed-size header.
    pub header: &'a FileChecksumHeader,
    /// The checksum bytes.
    pub checksum_data: &'a [u8],
}

/// Points to a single file checksum record, with mutable access.
pub struct FileChecksumMut<'a> {
    /// The fixed-size header.
    pub header: &'a mut FileChecksumHeader,
    /// The checksum bytes.
    pub checksum_data: &'a mut [u8],
}

impl<'a> FileChecksum<'a> {
    /// Gets the `NameIndex` of the file name for this record. To dereference the `NameIndex value,
    /// use [`crate::names::NamesStream`].
    pub fn name(&self) -> NameIndex {
        NameIndex(self.header.name.get())
    }
}

/// The header at the start of a file checksum record.
///
/// Each file checksum record specifies the name of the file (using an offset into the Names Stream),
/// the kind of checksum (none, SHA1, SHA256, MD5, etc.), the size of the checksum, and the
/// checksum bytes.
///
/// The checksum record is variable-length.
#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Clone, Debug)]
#[repr(C)]
pub struct FileChecksumHeader {
    /// Offset into the global string table (the `/names` stream) of the PDB.
    pub name: U32<LE>,

    /// The size in bytes of the checksum. The checksum bytes immediately follow the `FileChecksumHeader`.
    pub checksum_size: u8,

    /// The hash algorithm used for the checksum.
    pub checksum_kind: ChecksumKind,
}

/// Iterates FileChecksum values from a byte stream.
pub struct FileChecksumIter<'a> {
    /// The unparsed data
    pub bytes: &'a [u8],
}

impl<'a> HasRestLen for FileChecksumIter<'a> {
    fn rest_len(&self) -> usize {
        self.bytes.len()
    }
}

/// Iterator state. Iterates `FileChecksumMut` values.
pub struct FileChecksumMutIter<'a> {
    /// The unparsed data
    pub bytes: &'a mut [u8],
}

impl<'a> HasRestLen for FileChecksumMutIter<'a> {
    fn rest_len(&self) -> usize {
        self.bytes.len()
    }
}

impl<'a> FileChecksumIter<'a> {
    /// Starts a new iterator
    pub fn new(bytes: &'a [u8]) -> Self {
        Self { bytes }
    }
}

impl<'a> Iterator for FileChecksumIter<'a> {
    type Item = FileChecksum<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.bytes.is_empty() {
            return None;
        }

        let mut p = Parser::new(self.bytes);
        let len_before = p.len();
        let header: &FileChecksumHeader = p.get().ok()?;
        let checksum_data = p.bytes(header.checksum_size as usize).ok()?;

        // Align to 4-byte boundaries.
        let record_len = len_before - p.len();
        let _ = p.skip((4 - (record_len & 3)) & 3);

        self.bytes = p.into_rest();
        Some(FileChecksum {
            header,
            checksum_data,
        })
    }
}

impl<'a> FileChecksumMutIter<'a> {
    /// Starts a new iterator
    pub fn new(bytes: &'a mut [u8]) -> Self {
        Self { bytes }
    }
}

impl<'a> Iterator for FileChecksumMutIter<'a> {
    type Item = FileChecksumMut<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.bytes.is_empty() {
            return None;
        }

        let mut p = ParserMut::new(take(&mut self.bytes));
        let len_before = p.len();
        let header: &mut FileChecksumHeader = p.get_mut().ok()?;
        let checksum_data = p.bytes_mut(header.checksum_size as usize).ok()?;

        // Align to 4-byte boundaries.
        let record_len = len_before - p.len();
        let _ = p.skip((4 - (record_len & 3)) & 3);

        self.bytes = p.into_rest();
        Some(FileChecksumMut {
            header,
            checksum_data,
        })
    }
}

/// Test iteration of records with byte ranges.
#[test]
fn iter_ranges() {
    const PAD: u8 = 0xaa;

    #[rustfmt::skip]
    let data = &[
        200, 0, 0, 0,               // name
        0,                          // checksum_size
        0,                          // no checksum
        // <-- offset = 6
        PAD, PAD,
        // <-- offset = 8

        42, 0, 0, 0,                // name
        16,                         // checksum_size
        1,                          // MD5
        0xc0, 0xc1, 0xc2, 0xc3,     // checksum
        0xc4, 0xc5, 0xc6, 0xc7,
        0xc8, 0xc9, 0xca, 0xcb,
        0xcc, 0xcd, 0xce, 0xcf,
        // <-- offset = 30
        PAD, PAD,

        // <-- offset = 32
    ];

    let sums = FileChecksumsSubsection::new(data);
    let mut iter = sums.iter().with_ranges();

    let (sub0_range, _) = iter.next().unwrap();
    assert_eq!(sub0_range, 0..8);

    let (sub1_range, _) = iter.next().unwrap();
    assert_eq!(sub1_range, 8..32);

    assert!(iter.next().is_none());
}

/// Tests that FileChecksumMutIter allows us to modify checksum records.
#[test]
fn iter_mut() {
    const PAD: u8 = 0xaa;

    #[rustfmt::skip]
    let data = &[
        42, 0, 0, 0,                // name
        16,                         // checksum_size
        1,                          // MD5
        0xc0, 0xc1, 0xc2, 0xc3,     // checksum
        0xc4, 0xc5, 0xc6, 0xc7,
        0xc8, 0xc9, 0xca, 0xcb,
        0xcc, 0xcd, 0xce, 0xcf,
        // <-- offset = 22
        PAD, PAD,

        // <-- offset = 24
    ];

    let mut data_mut = data.to_vec();
    let mut sums = FileChecksumsSubsectionMut::new(&mut data_mut);
    let mut iter = sums.iter_mut();
    assert_eq!(iter.rest_len(), 24); // initial amount of data in iterator

    let sum0 = iter.next().unwrap();
    assert_eq!(iter.rest_len(), 0); // initial amount of data in iterator
    assert_eq!(sum0.header.name.get(), 42);
    assert_eq!(
        sum0.checksum_data,
        &[
            0xc0, 0xc1, 0xc2, 0xc3, 0xc4, 0xc5, 0xc6, 0xc7, 0xc8, 0xc9, 0xca, 0xcb, 0xcc, 0xcd,
            0xce, 0xcf
        ]
    );

    sum0.header.name = U32::new(0xcafef00d);
    sum0.checksum_data[4] = 0xff;

    #[rustfmt::skip]
    let expected_new_data = &[
        0x0d, 0xf0, 0xfe, 0xca,     // name (modified)
        16,                         // checksum_size
        1,                          // MD5
        0xc0, 0xc1, 0xc2, 0xc3,     // checksum
        0xff, 0xc5, 0xc6, 0xc7,     // <-- modified
        0xc8, 0xc9, 0xca, 0xcb,
        0xcc, 0xcd, 0xce, 0xcf,
        // <-- offset = 22
        PAD, PAD,

        // <-- offset = 24
    ];

    assert_eq!(data_mut.as_slice(), expected_new_data);
}

/// Tests FileChecksumIter and FileChecksumMutIter.
#[test]
fn basic_iter() {
    const PAD: u8 = 0xaa;

    #[rustfmt::skip]
    let data = &[
        42, 0, 0, 0,                // name
        16,                         // checksum_size
        1,                          // MD5
        0xc0, 0xc1, 0xc2, 0xc3,     // checksum
        0xc4, 0xc5, 0xc6, 0xc7,
        0xc8, 0xc9, 0xca, 0xcb,
        0xcc, 0xcd, 0xce, 0xcf,
        // <-- offset = 22
        PAD, PAD,

        // <-- offset = 24
        0, 1, 0, 0,                 // name
        16,                         // checksum_size
        1,                          // MD5
        0xd0, 0xd1, 0xd2, 0xd3,     // checksum
        0xd4, 0xd5, 0xd6, 0xd7,
        0xd8, 0xd9, 0xda, 0xdb,
        0xdc, 0xdd, 0xde, 0xdf,
        // <-- offset = 46
        PAD, PAD,
        // <-- offset = 48
    ];

    // Test FileChecksumIter (immutable iterator)
    {
        let mut iter = FileChecksumIter::new(data);
        assert_eq!(iter.rest_len(), 48); // initial amount of data in iterator
        let sum0 = iter.next().unwrap();
        assert_eq!(sum0.name(), NameIndex(42));
        assert_eq!(
            sum0.checksum_data,
            &[
                0xc0, 0xc1, 0xc2, 0xc3, 0xc4, 0xc5, 0xc6, 0xc7, 0xc8, 0xc9, 0xca, 0xcb, 0xcc, 0xcd,
                0xce, 0xcf
            ]
        );

        assert_eq!(iter.rest_len(), 24); // record 0 is 24 bytes (including padding)

        let sum1 = iter.next().unwrap();
        assert_eq!(sum1.name(), NameIndex(0x100));
        assert_eq!(
            sum1.checksum_data,
            &[
                0xd0, 0xd1, 0xd2, 0xd3, 0xd4, 0xd5, 0xd6, 0xd7, 0xd8, 0xd9, 0xda, 0xdb, 0xdc, 0xdd,
                0xde, 0xdf,
            ]
        );

        assert_eq!(iter.rest_len(), 0); // record 1 is 24 bytes (including padding), leaving nothing in buffer
        assert!(iter.next().is_none());
    }

    // Test FileChecksumMutIter (mutable iterator)
    // We duplicate this because we can't do generics over mutability.
    {
        let mut data_mut = data.to_vec();
        let mut iter = FileChecksumMutIter::new(&mut data_mut);
        assert_eq!(iter.rest_len(), 48); // initial amount of data in iterator
        let sum0 = iter.next().unwrap();
        assert_eq!(sum0.header.name.get(), 42);
        assert_eq!(
            sum0.checksum_data,
            &[
                0xc0, 0xc1, 0xc2, 0xc3, 0xc4, 0xc5, 0xc6, 0xc7, 0xc8, 0xc9, 0xca, 0xcb, 0xcc, 0xcd,
                0xce, 0xcf
            ]
        );

        assert_eq!(iter.rest_len(), 24); // record 0 is 24 bytes (including padding)

        let sum1 = iter.next().unwrap();
        assert_eq!(sum1.header.name.get(), 0x100);
        assert_eq!(
            sum1.checksum_data,
            &[
                0xd0, 0xd1, 0xd2, 0xd3, 0xd4, 0xd5, 0xd6, 0xd7, 0xd8, 0xd9, 0xda, 0xdb, 0xdc, 0xdd,
                0xde, 0xdf,
            ]
        );

        assert_eq!(iter.rest_len(), 0); // record 1 is 24 bytes (including padding), leaving nothing in buffer
        assert!(iter.next().is_none());
    }
}

#[test]
fn test_get_file() {
    const PAD: u8 = 0xaa;

    #[rustfmt::skip]
    let data = &[
        42,   0,    0, 0, 0, 0, PAD, PAD,      // record 0 at 0
        0xee, 0,    0, 0, 0, 0, PAD, PAD,      // record 1 at 8
        0,    0xcc, 0, 0, 0, 0, PAD, PAD,      // record 2 at 0x10
        // len = 0x18
    ];

    // Test immutable access
    {
        let sums = FileChecksumsSubsection::new(data);

        let sum0 = sums.get_file(0).unwrap();
        assert_eq!(sum0.name(), NameIndex(42));

        let sum1 = sums.get_file(8).unwrap();
        assert_eq!(sum1.name(), NameIndex(0xee));

        let sum2 = sums.get_file(0x10).unwrap();
        assert_eq!(sum2.name(), NameIndex(0xcc00));

        // Test bad index (way outside of data)
        assert!(sums.get_file(0x1000).is_err());

        // Test bad index (invalid header)
        assert!(sums.get_file(0x16).is_err());
    }

    // Test mutable access
    {
        let mut data_mut = data.to_vec();
        let mut sums = FileChecksumsSubsectionMut::new(&mut data_mut);

        let sum0 = sums.get_file_mut(0).unwrap();
        assert_eq!(sum0.header.name.get(), 42);

        let sum1 = sums.get_file_mut(8).unwrap();
        assert_eq!(sum1.header.name.get(), 0xee);

        let sum2 = sums.get_file_mut(0x10).unwrap();
        assert_eq!(sum2.header.name.get(), 0xcc00);

        // Modify one of the records
        sum2.header.name = U32::new(0xcafe);

        // Test bad index (way outside of data)
        assert!(sums.get_file_mut(0x1000).is_err());

        // Test bad index (invalid header)
        assert!(sums.get_file_mut(0x16).is_err());

        #[rustfmt::skip]
        let expected_data = &[
            42,   0,    0, 0, 0, 0, PAD, PAD,      // record 0 at 0
            0xee, 0,    0, 0, 0, 0, PAD, PAD,      // record 1 at 8
            0xfe, 0xca, 0, 0, 0, 0, PAD, PAD,      // record 2 at 0x10
        ];

        assert_eq!(data_mut.as_slice(), expected_data);
    }
}

```

`pdb/src/lines/subsection.rs`:

```rs
//! Iteration logic for subsections

#[cfg(test)]
use pretty_hex::PrettyHex;

use super::*;

/// Iterator state for subsections
pub struct SubsectionIter<'a> {
    rest: &'a [u8],
}

impl<'a> SubsectionIter<'a> {
    /// Start iteration
    pub fn new(rest: &'a [u8]) -> Self {
        Self { rest }
    }

    /// The remaining unparsed data.
    pub fn rest(&self) -> &'a [u8] {
        self.rest
    }
}

impl<'a> HasRestLen for SubsectionIter<'a> {
    fn rest_len(&self) -> usize {
        self.rest.len()
    }
}

/// Iterator state for subsections with mutable access
pub struct SubsectionIterMut<'a> {
    rest: &'a mut [u8],
}

impl<'a> SubsectionIterMut<'a> {
    /// Begins iteration
    pub fn new(rest: &'a mut [u8]) -> Self {
        Self { rest }
    }

    /// The remaining unparsed data.
    pub fn rest(&self) -> &[u8] {
        self.rest
    }
}

impl<'a> HasRestLen for SubsectionIterMut<'a> {
    fn rest_len(&self) -> usize {
        self.rest.len()
    }
}

/// A reference to one subsection
pub struct Subsection<'a> {
    /// The kind of data in this subsection.
    pub kind: SubsectionKind,
    /// The contents of the subsection.
    pub data: &'a [u8],
}

/// A reference to one subsection, with mutable access
pub struct SubsectionMut<'a> {
    /// The kind of data in this subsection.
    pub kind: SubsectionKind,
    /// The contents of the subsection.
    pub data: &'a mut [u8],
}

/// The header of a subsection.
#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned)]
#[repr(C)]
pub struct SubsectionHeader {
    /// The kind of data in this subsection.
    pub kind: U32<LE>,
    /// The size of the subsection, in bytes. This value does not count the size of the `kind` field.
    pub size: U32<LE>,
}

impl<'a> Iterator for SubsectionIter<'a> {
    type Item = Subsection<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.rest.is_empty() {
            return None;
        }

        let mut p = Parser::new(self.rest);
        let header: &SubsectionHeader = if let Ok(h) = p.get::<SubsectionHeader>() {
            h
        } else {
            warn!(
                "Failed to decode subsection data (incomplete header)!  rest_len = {}",
                self.rest.len()
            );
            return None;
        };
        let size = header.size.get() as usize;

        let data = if let Ok(d) = p.bytes(size) {
            d
        } else {
            warn!(
                "Failed to decode subsection data (incomplete payload)!  rest_len = {}",
                self.rest.len()
            );
            return None;
        };

        // If 'size' is not 4-byte aligned, then skip the alignment bytes.
        let alignment_len = (4 - (size & 3)) & 3;
        let _ = p.skip(alignment_len);

        self.rest = p.into_rest();

        Some(Subsection {
            kind: SubsectionKind(header.kind.get()),
            data,
        })
    }
}

impl<'a> Iterator for SubsectionIterMut<'a> {
    type Item = SubsectionMut<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.rest.is_empty() {
            return None;
        }

        let mut p = ParserMut::new(core::mem::take(&mut self.rest));
        let header: &SubsectionHeader = p.get::<SubsectionHeader>().ok()?;
        let size = header.size.get() as usize;
        let data = p.bytes_mut(size).ok()?;

        let alignment_len = (4 - (size & 3)) & 3;
        let _ = p.skip(alignment_len);

        self.rest = p.into_rest();

        Some(SubsectionMut {
            kind: SubsectionKind(header.kind.get()),
            data,
        })
    }
}

// Test that empty input or malformed line data (too little data) does not cause the iterator to fail.
// The iterator will return `None`.
#[test]
fn empty_or_malformed_input() {
    static CASES: &[(&str, &[u8])] = &[
        ("empty input", &[]),
        ("incomplete subsection_kind", &[0xf1, 0]),
        ("incomplete subsection_size", &[0xf1, 0, 0, 0, 0xff, 0xff]),
        (
            "incomplete subsection_data",
            &[
                0xf1, 0, 0, 0, // subsection_Kind
                0, 0, 1, 0, // subsection_size (0x10000)
            ],
        ),
    ];

    for &(case_name, case_data) in CASES.iter() {
        // Test the subsection iterator
        println!("case: {}\n{:?}", case_name, case_data.hex_dump());
        let ld = LineData::new(case_data);
        assert_eq!(ld.subsections().count(), 0);
        assert!(ld.find_checksums().is_none());
        assert!(ld.find_checksums_bytes().is_none());
        ld.iter_name_index(|_name| panic!("should never be called"))
            .unwrap();

        // Do the same thing with a mutable iterator.
        let mut case_data_mut = case_data.to_vec();
        let mut ld = LineDataMut::new(&mut case_data_mut);
        assert_eq!(ld.subsections_mut().count(), 0);
    }
}

/// Test the alignment padding code in the subsection iterator.
#[test]
fn test_subsection_alignment() {
    const PAD: u8 = 0xaa;

    #[rustfmt::skip]
    static DATA: &[u8] = &[
                                // -----subsection 0 -----
        0xf4, 0, 0, 0,          // subsection_kind: DEBUG_S_FILECHKSMS
        0x2, 0, 0, 0,           // subsection_size (unaligned len = 2)
        0xab, 0xcd,             // subsection_data
        PAD, PAD,               // 2 padding bytes
                                // ----- subsection 1 -----
        0xf5, 0, 0, 0,          // subsection_kind: FRAMEDATA
        7, 0, 0, 0,             // subsection_size: 7 (unaligned len = 3)
        1, 2, 3, 4, 5, 6, 7,    // subsection_data
        PAD,                    // 1 padding byte
                                // ----- subsection 2 -----
        0xf6, 0, 0, 0,          // subsection_kind: INLINEELINES
        8, 0, 0, 0,             // subsection_size: 8 (unaligned len = 0)
        8, 7, 6, 5, 4, 3, 2, 1, // subsection_data
                                // no padding bytes
                                // ----- subsection 3 -----
        0xf7, 0, 0, 0,          // subsection_kind: CROSSSCOPEIMPORTS
        5, 0, 0, 0,             // subsection_size: 5 (unaligned len = 1)
        10, 11, 12, 13, 14,     // subsection_data
        PAD, PAD, PAD,          // 3 padding bytes


    ];

    // Test SubsectionsIter
    {
        let mut iter = LineData::new(DATA).subsections();

        let sub0 = iter.next().unwrap();
        assert_eq!(sub0.kind, SubsectionKind::FILE_CHECKSUMS);
        assert_eq!(sub0.data, &[0xab, 0xcd]);

        let sub1 = iter.next().unwrap();
        assert_eq!(sub1.kind, SubsectionKind::FRAMEDATA);
        assert_eq!(sub1.data, &[1, 2, 3, 4, 5, 6, 7]);

        let sub2 = iter.next().unwrap();
        assert_eq!(sub2.kind, SubsectionKind::INLINEELINES);
        assert_eq!(sub2.data, &[8, 7, 6, 5, 4, 3, 2, 1]);

        let sub3 = iter.next().unwrap();
        assert_eq!(sub3.kind, SubsectionKind::CROSSSCOPEIMPORTS);
        assert_eq!(sub3.data, &[10, 11, 12, 13, 14]);

        assert!(iter.rest().is_empty());
    }

    // Test SubsectionIterMut
    // We repeat the tests because we can't do generics over mutability, and the implementations of
    // SubsectionIter and SubsectionIterMut
    {
        let mut data_mut = DATA.to_vec();
        let mut iter = SubsectionIterMut::new(&mut data_mut);

        let sub0 = iter.next().unwrap();
        assert_eq!(sub0.kind, SubsectionKind::FILE_CHECKSUMS);
        assert_eq!(sub0.data, &[0xab, 0xcd]);

        let sub1 = iter.next().unwrap();
        assert_eq!(sub1.kind, SubsectionKind::FRAMEDATA);
        assert_eq!(sub1.data, &[1, 2, 3, 4, 5, 6, 7]);

        let sub2 = iter.next().unwrap();
        assert_eq!(sub2.kind, SubsectionKind::INLINEELINES);
        assert_eq!(sub2.data, &[8, 7, 6, 5, 4, 3, 2, 1]);

        let sub3 = iter.next().unwrap();
        assert_eq!(sub3.kind, SubsectionKind::CROSSSCOPEIMPORTS);
        assert_eq!(sub3.data, &[10, 11, 12, 13, 14]);

        assert!(iter.rest().is_empty());
    }
}

```

`pdb/src/modi.rs`:

```rs
//! Reads data from Module Info (`modi`) streams.
//!
//! # References
//! * <https://llvm.org/docs/PDB/ModiStream.html>
//! * [`MODI_60_Persist` in `dbi.h`]

use crate::StreamData;
use crate::dbi::ModuleInfoFixed;
use crate::utils::vec::replace_range_copy;
use crate::{dbi::ModuleInfo, syms::SymIter};
use anyhow::{Result, anyhow, bail};
use ms_codeview::parser::Parser;
use std::mem::size_of;
use std::ops::Range;
use sync_file::ReadAt;
use tracing::{debug, warn};
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, LE, U32, Unaligned};

/// The Module Symbols substream begins with this header. It is located at stream offset 0 in the
/// Module Stream.
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned)]
#[repr(C)]
pub struct ModuleSymbolsHeader {
    /// Indicates the version of the module symbol stream. Use the `CV_SIGNATURE_*` constants.
    /// The expected value is `CV_SIGNATURE_C13`.
    pub signature: U32<LE>,
}

const MODULE_SYMBOLS_HEADER_LEN: usize = 4;
static_assertions::const_assert_eq!(size_of::<ModuleSymbolsHeader>(), MODULE_SYMBOLS_HEADER_LEN);

/// Actual signature is >64K
pub const CV_SIGNATURE_C6: u32 = 0;
/// First explicit signature
pub const CV_SIGNATURE_C7: u32 = 1;
/// C11 (vc5.x) 32-bit types
pub const CV_SIGNATURE_C11: u32 = 2;
/// C13 (vc7.x) zero terminated names
pub const CV_SIGNATURE_C13: u32 = 4;
/// All signatures from 5 to 64K are reserved
pub const CV_SIGNATURE_RESERVED: u32 = 5;

impl<F: ReadAt> crate::Pdb<F> {
    /// Reads a Module Info stream. The caller must provide a [`ModuleInfo`] structure, which comes
    /// from the DBI Stream.  Use [`crate::dbi::read_dbi_stream`] to enumerate [`ModuleInfo`] values.
    ///
    /// If the Module Info record has a NIL stream, then this function returns `Ok(None)`.
    pub fn read_module_stream(
        &self,
        mod_info: &ModuleInfo,
    ) -> Result<Option<ModiStreamData<StreamData>>, anyhow::Error> {
        let Some(stream) = mod_info.stream() else {
            return Ok(None);
        };

        let stream_data = self.read_stream(stream)?;
        Ok(Some(ModiStreamData::new(stream_data, mod_info.header())?))
    }

    /// Reads the symbol data for a specific module.
    ///
    /// The returned buffer will contain a 4-byte header. The caller can use `SymIter::for_module_syms`
    /// on the returned buffer.
    ///
    /// If the given module does not have a module stream, then this function will return `Ok`
    /// with a zero-length buffer, so callers should be prepared to deal with the presence
    /// _or absence_ of the 4-byte header.
    pub fn read_module_symbols(&self, module: &ModuleInfo) -> Result<Vec<u32>> {
        let len_u32 = module.sym_size() as usize / 4;
        if len_u32 < 4 {
            return Ok(Vec::new());
        }

        let Some(module_stream) = module.stream() else {
            return Ok(Vec::new());
        };

        let mut syms: Vec<u32> = vec![0; len_u32];
        let sr = self.get_stream_reader(module_stream)?;
        sr.read_exact_at(syms.as_mut_bytes(), 0)?;
        Ok(syms)
    }
}

/// Contains the stream data for a Module Info stream.
#[allow(missing_docs)]
pub struct ModiStreamData<Data> {
    /// The contents of the stream.
    pub stream_data: Data,
    pub sym_byte_size: u32,
    pub c11_byte_size: u32,
    pub c13_byte_size: u32,
    pub global_refs_size: u32,
}

impl<Data: AsRef<[u8]>> ModiStreamData<Data> {
    /// Initializes a new `ModiStreamData`. This validates the byte sizes of the substreams,
    /// which are specified in the [`ModuleInfo`] structure, not within the Module Stream itself.
    pub fn new(stream_data: Data, module: &ModuleInfoFixed) -> anyhow::Result<Self> {
        let stream_bytes: &[u8] = stream_data.as_ref();

        // Validate the byte sizes against the size of the stream data.
        let sym_byte_size = module.sym_byte_size.get();
        let c11_byte_size = module.c11_byte_size.get();
        let c13_byte_size = module.c13_byte_size.get();

        let mut p = Parser::new(stream_bytes);

        p.skip(sym_byte_size as usize).map_err(|_| {
            anyhow!("Module info has a sym_byte_size that exceeds the size of the stream.")
        })?;
        p.skip(c11_byte_size as usize).map_err(|_| {
            anyhow!("Module info has a c11_byte_size that exceeds the size of the stream.")
        })?;
        p.skip(c13_byte_size as usize).map_err(|_| {
            anyhow!("Module info has a c13_byte_size that exceeds the size of the stream.")
        })?;

        let mut global_refs_size;
        if !p.is_empty() {
            global_refs_size = p
                .u32()
                .map_err(|_| anyhow!("Failed to decode global_refs_size. There are {} bytes after the module symbols substream.", p.len()))?;

            if global_refs_size == 0xffff_ffff {
                warn!("Module has global_refs_size = 0xffff_ffff");
                global_refs_size = 0;
            } else {
                p.skip(global_refs_size as usize)
                .map_err(|_| anyhow!("Failed to decode global_refs substream. global_refs_size = 0x{:x}, but there are only 0x{:x} bytes left.",
                global_refs_size,
                p.len()
            ))?;
            }

            if !p.is_empty() {
                debug!(stream_len = p.len(), "Module stream has extra bytes at end");
            }
        } else {
            global_refs_size = 0;
        }

        Ok(Self {
            stream_data,
            sym_byte_size,
            c11_byte_size,
            c13_byte_size,
            global_refs_size,
        })
    }

    /// Returns an iterator for the symbol data for this module.
    pub fn iter_syms(&self) -> SymIter<'_> {
        if let Ok(sym_data) = self.sym_data() {
            SymIter::new(sym_data)
        } else {
            SymIter::new(&[])
        }
    }

    fn nested_slice(&self, range: Range<usize>) -> Result<&[u8]> {
        if let Some(b) = self.stream_data.as_ref().get(range) {
            Ok(b)
        } else {
            bail!("Range within module stream is invalid")
        }
    }

    fn nested_slice_mut(&mut self, range: Range<usize>) -> Result<&mut [u8]>
    where
        Data: AsMut<[u8]>,
    {
        if let Some(b) = self.stream_data.as_mut().get_mut(range) {
            Ok(b)
        } else {
            bail!("Range within module stream is invalid")
        }
    }

    /// Returns a reference to the encoded symbol data for this module.
    ///
    /// This _does not_ include the CodeView signature.
    pub fn sym_data(&self) -> Result<&[u8]> {
        self.nested_slice(MODULE_SYMBOLS_HEADER_LEN..self.sym_byte_size as usize)
    }

    /// Returns a mutable reference to the encoded symbol data for this module.
    ///
    /// This _does not_ include the CodeView signature.
    pub fn sym_data_mut(&mut self) -> Result<&mut [u8]>
    where
        Data: AsMut<[u8]>,
    {
        self.nested_slice_mut(MODULE_SYMBOLS_HEADER_LEN..self.sym_byte_size as usize)
    }

    /// Returns a reference to the encoded symbol data for this module.
    ///
    /// This _does_ include the CodeView signature.
    pub fn full_sym_data(&self) -> Result<&[u8]> {
        self.nested_slice(0..self.sym_byte_size as usize)
    }

    /// Returns a mutable reference to the encoded symbol data for this module.
    ///
    /// This _does_ include the CodeView signature.
    pub fn full_sym_data_mut(&mut self) -> Result<&mut [u8]>
    where
        Data: AsMut<[u8]>,
    {
        self.nested_slice_mut(0..self.sym_byte_size as usize)
    }

    /// Returns the byte range of the C13 Line Data within this Module Information Stream.
    pub fn c13_line_data_range(&self) -> Range<usize> {
        if self.c13_byte_size == 0 {
            return 0..0;
        }

        let start = self.sym_byte_size as usize + self.c11_byte_size as usize;
        start..start + self.c13_byte_size as usize
    }

    /// Returns the byte data for the C13 line data.
    pub fn c13_line_data_bytes(&self) -> &[u8] {
        if self.c13_byte_size == 0 {
            return &[];
        }

        // The range has already been validated.
        let stream_data: &[u8] = self.stream_data.as_ref();
        let range = self.c13_line_data_range();
        &stream_data[range]
    }

    /// Returns a mutable reference to the byte data for the C13 Line Data.
    pub fn c13_line_data_bytes_mut(&mut self) -> &mut [u8]
    where
        Data: AsMut<[u8]>,
    {
        if self.c13_byte_size == 0 {
            return &mut [];
        }

        // The range has already been validated.
        let range = self.c13_line_data_range();
        let stream_data: &mut [u8] = self.stream_data.as_mut();
        &mut stream_data[range]
    }

    /// Returns an object which can decode the C13 Line Data.
    pub fn c13_line_data(&self) -> crate::lines::LineData<'_> {
        crate::lines::LineData::new(self.c13_line_data_bytes())
    }

    /// Returns an object which can decode and modify the C13 Line Data.
    pub fn c13_line_data_mut(&mut self) -> crate::lines::LineDataMut<'_>
    where
        Data: AsMut<[u8]>,
    {
        crate::lines::LineDataMut::new(self.c13_line_data_bytes_mut())
    }

    /// Gets the byte range within the stream data for the global refs
    pub fn global_refs_range(&self) -> Range<usize> {
        if self.global_refs_size == 0 {
            return 0..0;
        }

        // The Global Refs start after the C13 line data.
        // This offset was validated in Self::new().
        // The size_of::<u32>() is for the global_refs_size field itself.
        let global_refs_offset = self.sym_byte_size as usize
            + self.c11_byte_size as usize
            + self.c13_byte_size as usize
            + size_of::<U32<LE>>();
        global_refs_offset..global_refs_offset + self.global_refs_size as usize
    }

    /// Returns a reference to the global refs stored in this Module Stream.
    ///
    /// Each value in the returned slice is a byte offset into the Global Symbol Stream of
    /// a global symbol that this module references.
    pub fn global_refs(&self) -> Result<&[U32<LE>]> {
        let range = self.global_refs_range();
        let stream_data: &[u8] = self.stream_data.as_ref();
        if let Some(global_refs_bytes) = stream_data.get(range) {
            if let Ok(global_refs) = FromBytes::ref_from_bytes(global_refs_bytes) {
                Ok(global_refs)
            } else {
                bail!("Invalid size for global refs")
            }
        } else {
            bail!("Invalid range for global refs")
        }
    }

    /// Returns a mutable reference to the global refs stored in this Module Stream.
    pub fn global_refs_mut(&mut self) -> Result<&mut [U32<LE>]>
    where
        Data: AsMut<[u8]>,
    {
        let range = self.global_refs_range();
        let stream_data: &mut [u8] = self.stream_data.as_mut();
        if let Some(global_refs_bytes) = stream_data.get_mut(range) {
            if let Ok(global_refs) = FromBytes::mut_from_bytes(global_refs_bytes) {
                Ok(global_refs)
            } else {
                bail!("Invalid size for global refs")
            }
        } else {
            bail!("Invalid range for global refs")
        }
    }
}

impl ModiStreamData<Vec<u8>> {
    /// Replace the symbol data for this module.  `new_sym_data` includes the CodeView signature.
    pub fn replace_sym_data(&mut self, new_sym_data: &[u8]) {
        if new_sym_data.len() == self.sym_byte_size as usize {
            self.stream_data[..new_sym_data.len()].copy_from_slice(new_sym_data);
        } else {
            replace_range_copy(
                &mut self.stream_data,
                0,
                self.sym_byte_size as usize,
                new_sym_data,
            );
            self.sym_byte_size = new_sym_data.len() as u32;
        }
    }

    /// Remove the Global Refs section, if present.
    pub fn truncate_global_refs(&mut self) {
        if self.global_refs_size == 0 {
            return;
        }

        let global_refs_offset =
            self.sym_byte_size as usize + self.c11_byte_size as usize + self.c13_byte_size as usize;

        self.stream_data.truncate(global_refs_offset);
        self.global_refs_size = 0;
    }
}

```

`pdb/src/names.rs`:

```rs
//! Parses the Names Stream (`/names`).
//!
//! The Names Stream stores a set of unique strings (names). This allows other data structures to
//! refer to strings using an integer index ([`NameIndex`]), rather than storing copies of the same
//! string in many different places.
//!
//! The stream index for the Names Stream is found in the PDB Information Stream, in the Named
//! Streams section.  The key is "/names".
//!
//! The Names Stream begins with `NamesStreamHeader`, which specifies the size in bytes of the
//! string data substream. The string data substream immediately follows the stream header.
//! It consists of NUL-terminated UTF-8 strings.
//!
//! After the string data there is a hash table. The hash table is an array, one for each string
//! in the table. The value of each array entry is a byte offset that points into the string data.
//! The index of each array entry is chosen using a hash of the corresponding string value.
//!
//! Hash collisions are resolved using linear probing. That is, during table construction, the
//! hash table is allocated and initialized, with each entry pointing to nothing (nil). For each
//! string, we compute the hash of the string (modulo the size of the hash table). If the
//! corresponding entry in the hash table is empty, then we write the `NameIndex` value into that
//! slot. If that slot is already busy, then we check the next slot; if we reach the end of the
//! table then we wrap around to slot 0. For this reason, the number of hash entries must be
//! greater than or equal to the number of strings in the table.
//!
//! The overall organization of the stream is:
//!
//! name             | type                 | usage
//! -----------------|----------------------|------
//! `signature`      | `u32`                | should always be 0xEFFE_EFFE
//! `version`        | `u32`                | should be 1
//! `strings_size`   | `u32`                | size of the string data
//! `strings_data`   | `[u8; strings_size]` | contains the UTF-8 string data, with NUL terminators
//! `num_hashes`     | `u32`                | specifies the number of hash entries
//! `hashes`         | `[u32; num_hashes]`  | contains hash entries for all strings
//! `num_strings`    | `u32`                | number of non-empty strings in the table

#[cfg(test)]
mod tests;

use crate::ReadAt;
use crate::utils::align_4;
use anyhow::bail;
use bstr::BStr;
use ms_codeview::parser::{Parser, ParserMut};
use ms_codeview::{HasRestLen, IteratorWithRangesExt};
use std::ops::Range;
use tracing::{debug, trace, trace_span, warn};
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, LE, U32, Unaligned};

/// The name of the `/names` stream. This identifies the stream in the Named Streams Table,
/// in the PDB Information Stream.
pub const NAMES_STREAM_NAME: &str = "/names";

/// A byte offset into the Names Stream.
///
/// This value does not include the size of the stream header, so the size of the stream header
/// must be added to it when dereferencing a string.
#[derive(Copy, Clone, Eq, PartialEq, Debug, Hash, Ord, PartialOrd)]
pub struct NameIndex(pub u32);

impl std::fmt::Display for NameIndex {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        std::fmt::Display::fmt(&self.0, f)
    }
}

#[test]
fn name_index_display() {
    assert_eq!(format!("{}", NameIndex(42)), "42");
}

/// Represents a `NameIndex` value in LE byte order.
#[derive(
    Copy, Clone, Eq, PartialEq, Debug, IntoBytes, Immutable, KnownLayout, FromBytes, Unaligned,
)]
#[repr(transparent)]
pub struct NameIndexLe(pub U32<LE>);

impl NameIndexLe {
    /// Converts the value to the in-memory byte order.
    #[inline(always)]
    pub fn get(self) -> NameIndex {
        NameIndex(self.0.get())
    }
}

/// Value for `NamesStreamHeader::signature`.
pub const NAMES_STREAM_SIGNATURE: u32 = 0xEFFE_EFFE;

/// Value for `NamesStreamHeader::version`.
pub const NAMES_STREAM_VERSION_V1: u32 = 1;

/// The header of the Names Stream.
#[repr(C)]
#[derive(IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned)]
pub struct NamesStreamHeader {
    /// Signature identifies this as a Names Stream. Should always be `NAMES_STREAM_SIGNATURE`.
    pub signature: U32<LE>,
    /// Version of the Names Stream, which determines the hash function.
    pub version: U32<LE>,
    /// Size in bytes of the string data, which immediately follows this header.
    pub strings_size: U32<LE>,
}

/// Stream data for an empty Names stream.
pub static EMPTY_NAMES_STREAM_DATA: &[u8] = &[
    0xFE, 0xEF, 0xFE, 0xEF, // signature
    0x01, 0x00, 0x00, 0x00, // version
    0x04, 0x00, 0x00, 0x00, // strings_size
    0x00, 0x00, 0x00, 0x00, // string data
    0x01, 0x00, 0x00, 0x00, // num_hashes
    0x00, 0x00, 0x00, 0x00, // hash[0]
    0x00, 0x00, 0x00, 0x00, // num_strings
];

#[test]
fn parse_empty_names_stream() {
    let names = NamesStream::parse(EMPTY_NAMES_STREAM_DATA).unwrap();
    assert_eq!(names.num_strings, 0);
    assert_eq!(names.num_hashes, 1);
}

/// The size of the Names Stream Header, in bytes.
pub const NAMES_STREAM_HEADER_LEN: usize = 12;

/// Reads the `/names` stream.
pub struct NamesStream<StreamData>
where
    StreamData: AsRef<[u8]>,
{
    /// Contains the stream data of the `/names` stream.
    pub stream_data: StreamData,

    /// The size of the string data. This value comes from the stream header.
    pub strings_size: usize,

    /// The number of entries in the hash table.
    pub num_hashes: usize,

    /// The byte offset within `stream_data` where the hash records begin. Each hash record
    /// contains a `NameIndex` value. The number of elements is `num_hashes`.
    pub hashes_offset: usize,

    /// The is the number of strings from the stream trailer. Nothing guarantees that this value
    /// correctly reflects the number of strings in the string data.
    pub num_strings: usize,
}

impl<StreamData> NamesStream<StreamData>
where
    StreamData: AsRef<[u8]>,
{
    /// Parses and validates the stream header.
    ///
    /// This function does not validate all of the strings in the table.
    /// The `check()` function performs extensive checks.
    pub fn parse(stream_data: StreamData) -> anyhow::Result<Self> {
        let stream_data_slice: &[u8] = stream_data.as_ref();
        let mut p = Parser::new(stream_data_slice);
        let header: &NamesStreamHeader = p.get()?;

        if header.signature.get() != NAMES_STREAM_SIGNATURE {
            bail!(
                "The `/names` stream has an invalid signature: 0x{:08x}.",
                header.signature.get()
            );
        }

        if header.version.get() != NAMES_STREAM_VERSION_V1 {
            bail!(
                "The `/names` stream is using an unsupported version: {}.",
                header.version.get()
            );
        }

        let strings_size = header.strings_size.get() as usize;
        let _string_data = p.bytes(strings_size)?;

        // Read the header of the hash table. The only value in the fixed-size portion is a u32
        // that specifies the number of hashes in the table.
        let num_hashes = p.u32()? as usize;

        let hashes_offset = stream_data_slice.len() - p.len();
        let _hashed_names: &[U32<LE>] = p.slice(num_hashes)?;

        // The last item is a u32 that specifies the number of strings in the table.
        let num_strings = p.u32()? as usize;

        Ok(Self {
            stream_data,
            strings_size,
            num_hashes,
            hashes_offset,
            num_strings,
        })
    }

    /// Returns the byte range within the stream of the string data.
    pub fn strings_range(&self) -> Range<usize> {
        NAMES_STREAM_HEADER_LEN..NAMES_STREAM_HEADER_LEN + self.strings_size
    }

    /// Gets the strings data
    pub fn strings_bytes(&self) -> &[u8] {
        &self.stream_data.as_ref()[self.strings_range()]
    }

    /// Gets the hash table. Each entry contains NameIndex, or 0.  The entries are arranged in
    /// the order of the hash of the strings.
    pub fn hashes(&self) -> &[U32<LE>] {
        let stream_data = self.stream_data.as_ref();
        <[U32<LE>]>::ref_from_prefix_with_elems(&stream_data[self.hashes_offset..], self.num_hashes)
            .unwrap()
            .0
    }

    /// Retrieves one string from the string table.
    pub fn get_string(&self, offset: NameIndex) -> anyhow::Result<&BStr> {
        let strings_bytes = self.strings_bytes();
        if let Some(s_bytes) = strings_bytes.get(offset.0 as usize..) {
            let mut p = Parser::new(s_bytes);
            let s = p.strz()?;
            trace!("found string at {offset:?} : {s:?}");
            Ok(s)
        } else {
            bail!("String offset {offset:?} is invalid (out of range)");
        }
    }

    /// Iterates the strings in the table, by reading the character data directly.
    ///
    /// By convention, the string table usually begins with the empty string. However, this is not
    /// a guarantee of this implementation.
    ///
    /// This iterator may iterate empty strings at the end of the sequence, due to alignment bytes
    /// at the end of the string data.
    pub fn iter(&self) -> IterNames<'_> {
        IterNames {
            rest: self.strings_bytes(),
        }
    }

    /// Sorts the Names Stream and removes duplicates. This also eliminates duplicate strings.
    ///
    /// Returns `(remapping_table, new_stream_data)`. The `remapping_table` contains tuples of
    /// `(old_offset, new_offset)` and is sorted by `old_offset`. The caller can use a binary
    /// search to remap entries.
    pub fn rebuild(&self) -> (NameIndexMapping, Vec<u8>) {
        let _span = trace_span!("NamesStream::rebuild").entered();

        let old_stream_data: &[u8] = self.stream_data.as_ref();
        // We verified the length of the stream in NamesStream::parse().
        let old_string_data = self.strings_bytes();

        // Check for the degenerate case of an empty names table, which does not even contain
        // the empty string. This should never happen, but protect against it anyway. Return
        // a copy of the current table, such as it is. The remapping_table is empty.
        if old_string_data.is_empty() {
            return (
                NameIndexMapping { table: Vec::new() },
                old_stream_data.to_vec(),
            );
        }

        // First pass, count the non-empty strings.
        let num_strings = self.iter().filter(|s| !s.is_empty()).count();
        debug!("Number of strings found: {num_strings}");

        // Second pass, build a string table.
        let mut strings: Vec<(Range<usize>, &BStr)> = Vec::with_capacity(num_strings);
        strings.extend(self.iter().with_ranges().filter(|(_, s)| !s.is_empty()));

        // Sort the strings.
        strings.sort_unstable_by_key(|i| i.1);
        strings.dedup_by_key(|i| i.1);

        let num_unique_strings = strings.len();
        if num_unique_strings != num_strings {
            debug!(
                "Removed {} duplicate strings.",
                num_strings - num_unique_strings
            );
        } else {
            debug!("Did not find duplicate strings.");
        }

        // Find the size of the new stream.
        // The 1+ at the start is for the empty string.
        let new_strings_len_unaligned = 1 + strings.iter().map(|(_, s)| s.len() + 1).sum::<usize>();
        let new_strings_len = align_4(new_strings_len_unaligned);

        // Choose the number of hashes.
        let num_hashes = num_unique_strings * 6 / 4;
        assert!(num_hashes >= num_unique_strings);
        debug!(
            "Using {} hashes for {} strings with linear probing.",
            num_hashes, num_unique_strings
        );

        let new_hash_size_bytes = 4   // for the num_hashes field
            + num_hashes * 4 // for the hashes array
            + 4; // for the num_strings field

        let new_stream_data_len = NAMES_STREAM_HEADER_LEN + new_strings_len + new_hash_size_bytes;
        debug!(
            "Old name stream size (strings only): {}",
            old_string_data.len()
        );
        debug!("New name stream size (strings only): {}", new_strings_len);

        let mut new_stream_data: Vec<u8> = vec![0; new_stream_data_len];
        let mut p = ParserMut::new(&mut new_stream_data);
        *p.get_mut().unwrap() = NamesStreamHeader {
            signature: U32::new(NAMES_STREAM_SIGNATURE),
            version: U32::new(NAMES_STREAM_VERSION_V1),
            strings_size: U32::new(new_strings_len as u32),
        };

        // Write the string data into the output table, and build the remapping table as we go.
        let mut remapping_table: Vec<(NameIndex, NameIndex)> = Vec::with_capacity(num_strings + 1);
        // Add mapping for empty
        remapping_table.push((NameIndex(0), NameIndex(0)));
        {
            let new_strings_data_with_alignment = p.bytes_mut(new_strings_len).unwrap();
            let out_bytes = &mut new_strings_data_with_alignment[..new_strings_len_unaligned];
            let out_bytes_len = out_bytes.len();
            let mut out_iter = out_bytes;

            // Write empty string.
            out_iter[0] = 0;
            out_iter = &mut out_iter[1..];

            for (old_range, s) in strings.iter() {
                let old_ni = NameIndex(old_range.start as u32);
                let new_ni = NameIndex((out_bytes_len - out_iter.len()) as u32);
                remapping_table.push((old_ni, new_ni));
                let sb: &[u8] = s;

                trace!(
                    "string: old_ni: 0x{old_ni:08x}, new_ni: 0x{new_ni:08x}, old_range: {:08x}..{:08x} s: {:?}",
                    old_range.start,
                    old_range.end,
                    s,
                    old_ni = old_ni.0,
                    new_ni = new_ni.0,
                );

                out_iter[..sb.len()].copy_from_slice(sb);
                out_iter = &mut out_iter[sb.len() + 1..]; // +1 for NUL
            }

            assert!(out_iter.is_empty());
            remapping_table.sort_unstable_by_key(|&(old, _)| old);
        }

        // Build the hash table. We rely on the table contain all zeroes before we begin writing.
        // We iterate through the strings, in the sorted order, and compute their hashes. Then we
        // insert the NameIndex into the table, using linear probing. If we get to the end, we
        // wrap around.
        let stream_offset_num_hashes = new_stream_data_len - p.len();
        *p.get_mut::<U32<LE>>().unwrap() = U32::new(num_hashes as u32);
        let stream_offset_hash_table = new_stream_data_len - p.len();

        {
            debug!("Building hash table, num_hashes = {}", num_hashes);
            let hash_table: &mut [U32<LE>] = p.slice_mut(num_hashes).unwrap();
            let mut new_ni: u32 = 1; // 1 is for empty string length
            for &(_, sb) in strings.iter() {
                let h = crate::hash::hash_mod_u32(sb, num_hashes as u32);
                trace!("ni {:08x}, hash {:08x}, {:?}", new_ni, h, sb);

                let mut hi = h;
                let mut wrapped = false;
                loop {
                    let slot = &mut hash_table[hi as usize];
                    if slot.get() == 0 {
                        *slot = U32::new(new_ni);
                        break;
                    }
                    hi += 1;
                    if hi as usize == hash_table.len() {
                        hi = 0;
                        assert!(!wrapped, "should not wrap around the table more than once");
                        wrapped = true;
                    }
                }

                new_ni += (sb.len() + 1) as u32;
            }
        }

        let stream_offset_num_strings = new_stream_data_len - p.len();
        *p.get_mut::<U32<LE>>().unwrap() = U32::new(strings.len() as u32);

        assert!(p.is_empty());

        debug!("Stream offsets:");
        debug!(
            "    [{:08x}] - Names Stream header",
            NAMES_STREAM_HEADER_LEN
        );
        debug!("    [{:08x}] - string data", NAMES_STREAM_HEADER_LEN);
        debug!(
            "    [{:08x}] - hash table header (num_hashes)",
            stream_offset_num_hashes
        );
        debug!(
            "    [{:08x}] - hash table, size in bytes = {}",
            stream_offset_hash_table,
            num_hashes * 4
        );
        debug!(
            "    [{:08x}] - num_strings field",
            stream_offset_num_strings
        );
        debug!("    [{:08x}] - (end)", new_stream_data_len);

        (
            NameIndexMapping {
                table: remapping_table,
            },
            new_stream_data,
        )
    }
}

/// Contains a mapping from old `NameIndex` to new `NameIndex. The mapping is sparse.
#[derive(Default)]
pub struct NameIndexMapping {
    /// the mapping table; use binary search for it
    ///
    /// This always starts with `(0, 0)`.
    pub table: Vec<(NameIndex, NameIndex)>,
}

impl NameIndexMapping {
    /// Looks up `name` in the mapping table and returns the mapping for it.
    pub fn map_old_to_new(&self, name: NameIndex) -> anyhow::Result<NameIndex> {
        // Perf optimization: Avoid the binary search for 0, which is never remapped.
        if name.0 == 0 {
            return Ok(name);
        }

        let table = self.table.as_slice();
        match table.binary_search_by_key(&name, |(old, _)| *old) {
            Ok(i) => Ok(table[i].1),
            Err(_) => bail!(
                "The NameIndex value 0x{:x} cannot be remapped because it was not present in the old Names stream.",
                name.0
            ),
        }
    }
}

/// Given an index `i` into a hash table `hashes`, where `hashes[i]` is already known to be used
/// (non-empty), find the range or ranges of contiguous non-empty entries in `hashes` that cover `i`.
///
/// The reason this function can return two ranges is that linear probing wraps around at the end
/// of the hash table. We have to account for wrap-around at both the start and end of `hashes`.
/// The unit tests (below) illustrate this.
///
/// We use the ranges returned from this function to verify that a given hash entry is at a legal
/// index within the hash table. The hash table may place hash entries adjacent to each other either
/// because the hash functions were numerically 1 different from each other (e.g. `foo` hashes to
/// 42 and `bar` hashes to 43) or because a hash collision occurred. This function does not
/// (cannot) distinguish between those two cases, because it does not have the original strings.
/// Instead, it just computes the places where a given string could legally be. The caller then
/// verifies that each hash entry is in a range that is valid for it.
#[allow(dead_code)]
fn find_collision_ranges(hashes: &[U32<LE>], i: usize) -> (Range<usize>, Range<usize>) {
    assert!(i < hashes.len());
    assert!(hashes[i].get() != 0);

    let mut start = i;
    while start > 0 && hashes[start - 1].get() != 0 {
        start -= 1;
    }

    let mut end = i + 1;
    while end < hashes.len() && hashes[end].get() != 0 {
        end += 1;
    }

    if start == 0 {
        // Special case: The entire hash table is one collision range.
        // We check for this because there are no unused slots in the table.
        if end == hashes.len() {
            return (start..end, 0..0);
        }

        let mut r2_start = hashes.len();
        while r2_start > 0 && hashes[r2_start - 1].get() != 0 {
            r2_start -= 1;
            assert!(r2_start > end); // prevent infinite loops
        }
        if r2_start != hashes.len() {
            (start..end, r2_start..hashes.len())
        } else {
            (start..end, 0..0)
        }
    } else if end == hashes.len() {
        // The end of the main range is aligned at the end of the buffer.
        // Wrap around to the beginning and find the range at the beginning, if any.
        let mut r2_end = 0;
        while r2_end < hashes.len() && hashes[r2_end].get() != 0 {
            assert!(r2_end < start); // prevent infinite loops
            r2_end += 1;
        }

        (start..end, 0..r2_end)
    } else {
        (start..end, 0..0)
    }
}

#[test]
fn test_find_collision_range() {
    const EMPTY: U32<LE> = U32::from_bytes([0; 4]);
    const BUSY: U32<LE> = U32::from_bytes([0xff; 4]);

    let hashes_full: Vec<U32<LE>> = vec![BUSY, BUSY, BUSY, BUSY, BUSY];
    assert_eq!(find_collision_ranges(&hashes_full, 0), (0..5, 0..0));
    assert_eq!(find_collision_ranges(&hashes_full, 2), (0..5, 0..0));

    {
        let hashes_2 = vec![
            BUSY,  // 0 - wraps around
            EMPTY, // 1
            BUSY,  // 2
            EMPTY, // 3
            EMPTY, // 4
            BUSY,  // 5
            BUSY,  // 6 - wraps around
        ];
        assert_eq!(find_collision_ranges(&hashes_2, 0), (0..1, 5..7));
        assert_eq!(find_collision_ranges(&hashes_2, 2), (2..3, 0..0));
        assert_eq!(find_collision_ranges(&hashes_2, 5), (5..7, 0..1));
    }

    {
        let hashes_3 = vec![
            BUSY,  // 0 - wraps around
            EMPTY, // 1
            BUSY,  // 2
            EMPTY, // 3
            EMPTY, // 4
            BUSY,  // 5
            EMPTY, // 6 - no wrap around
        ];
        assert_eq!(find_collision_ranges(&hashes_3, 0), (0..1, 0..0));
        assert_eq!(find_collision_ranges(&hashes_3, 2), (2..3, 0..0));
        assert_eq!(find_collision_ranges(&hashes_3, 5), (5..6, 0..0));
    }
    {
        let hashes_4 = vec![
            EMPTY, // 0 - no wrap around
            EMPTY, // 1
            BUSY,  // 2
            EMPTY, // 3
            EMPTY, // 4
            BUSY,  // 5
            BUSY,  // 6 - wraps around
        ];
        assert_eq!(find_collision_ranges(&hashes_4, 2), (2..3, 0..0));
        assert_eq!(find_collision_ranges(&hashes_4, 5), (5..7, 0..0));
        assert_eq!(find_collision_ranges(&hashes_4, 6), (5..7, 0..0));
    }
}

/// Iterator state
pub struct IterNames<'a> {
    rest: &'a [u8],
}

impl<'a> HasRestLen for IterNames<'a> {
    fn rest_len(&self) -> usize {
        self.rest.len()
    }
}

impl<'a> Iterator for IterNames<'a> {
    type Item = &'a BStr;

    fn next(&mut self) -> Option<Self::Item> {
        if self.rest.is_empty() {
            return None;
        }

        let mut p = Parser::new(self.rest);
        let Ok(s) = p.strz() else {
            warn!(
                rest_len = self.rest.len(),
                "Found malformed string in /names stream"
            );
            return None;
        };

        self.rest = p.into_rest();
        Some(s)
    }
}

impl NamesStream<Vec<u8>> {
    /// Reads the Names Stream and parses its header.
    pub fn load_and_parse<F: ReadAt>(
        pdb: &crate::msf::Msf<F>,
        named_streams: &crate::pdbi::NamedStreams,
    ) -> anyhow::Result<Self> {
        let named_stream_index = named_streams.get_err(NAMES_STREAM_NAME)?;
        let named_stream_data = pdb.read_stream_to_vec(named_stream_index)?;
        Self::parse(named_stream_data)
    }
}

```

`pdb/src/names/tests.rs`:

```rs
use super::*;

#[rustfmt::skip]
static NAMES_DATA: &[u8] = &[
    /* 0x0000 */ 0xfe, 0xef, 0xfe, 0xef,                 // signature
    /* 0x0004 */ 1, 0, 0, 0,                             // version
    /* 0x0008 */ 0x18, 0, 0, 0,                          // strings_size
    /* 0x000c */ 0,                                      // empty string
    /* 0x000d */ b'f', b'o', b'o', b'.', b'c', 0,        // (ni 0x0001) "foo.c\0" (len 6)
    /* 0x0013 */ b'b', b'a', b'r', b'.', b'r', b's', 0,  // (ni 0x0007) "bar.rs\0" (len 7)
    /* 0x001a */ b'm', b'a', b'i', b'n', b'.', b'c', 0,  // (ni 0x000e) "main.c\0" (len 7)
    /* 0x0021 */ 0, 0, 0,                                // padding bytes
    /* 0x0024 */ 0, 0, 0, 0,                             // num_hashes
    /* 0x0028 */                                         // hashes (none!)
    /* 0x0028 */ 3, 0, 0, 0,                             // num_strings
];

#[test]
fn test_basic() {
    let names = NamesStream::parse(&NAMES_DATA).unwrap();
    assert_eq!(names.get_string(NameIndex(1)).unwrap(), "foo.c");
    assert_eq!(names.get_string(NameIndex(7)).unwrap(), "bar.rs");
    assert_eq!(names.get_string(NameIndex(0xe)).unwrap(), "main.c");

    // Sort the name table.  After sorting, we should have:
    //      old ni 0x0007, new ni 0x0001 - "bar.rs"
    //      old ni 0x0001, new ni 0x0008 - "foo.c"
    //      old ni 0x000e, new ni 0x000e - "main.c"
    let (mapping, new_names_bytes) = names.rebuild();
    let new_names = NamesStream::parse(new_names_bytes).unwrap();

    assert_eq!(new_names.get_string(NameIndex(1)).unwrap(), "bar.rs");
    assert_eq!(new_names.get_string(NameIndex(8)).unwrap(), "foo.c");
    assert_eq!(new_names.get_string(NameIndex(0xe)).unwrap(), "main.c");

    assert_eq!(mapping.map_old_to_new(NameIndex(7)).unwrap(), NameIndex(1));
    assert_eq!(mapping.map_old_to_new(NameIndex(1)).unwrap(), NameIndex(8));
    assert_eq!(
        mapping.map_old_to_new(NameIndex(0xe)).unwrap(),
        NameIndex(0xe)
    );
}

#[test]
fn rebuild() {
    println!("parsing old names table");
    let old_names = NamesStream::parse(NAMES_DATA).unwrap();

    println!("rebuilding names table");
    let (remapping, new_names_bytes) = old_names.rebuild();
    assert!(!remapping.table.is_empty());
    assert_eq!(remapping.table[0], (NameIndex(0), NameIndex(0)));

    // The old_name_index values should be strictly increasing.
    for w in remapping.table.windows(2) {
        assert!(
            w[0].0 < w[1].0,
            "The old_name_index values should be strictly increasing."
        );
    }

    println!("parsing new names table");
    let new_names = NamesStream::parse(new_names_bytes.as_slice())
        .expect("expected rebuild Names table to successfully parse");

    // All entries in remapping should be valid in both old and new table.
    // The string value should be equal, for both.
    println!("validating mapping");
    for &(old_index, new_index) in remapping.table.iter() {
        let old_str = match old_names.get_string(old_index) {
            Ok(s) => s,
            Err(_) => panic!("Did not find mapping for {old_index} in old name table"),
        };

        let new_str = match new_names.get_string(new_index) {
            Ok(s) => s,
            Err(_) => panic!("Did not find mapping for {new_index} in new name table"),
        };

        assert_eq!(
            old_str, new_str,
            "old_index = {old_index}, new_index = {new_index}"
        );
    }

    // Rebuilding the names table _again_ should produce the exact same bytes.
    let (roundtrip_remapping, roundtrip_names_bytes) = new_names.rebuild();

    // We do not expect the remapping table to be the same, but we do expect the old/new values to be the same.
    assert_eq!(remapping.table.len(), roundtrip_remapping.table.len());
    for (i, &(old_name_index, new_name_index)) in roundtrip_remapping.table.iter().enumerate() {
        assert_eq!(old_name_index, new_name_index, "i = {i}");
    }

    assert_eq!(
        roundtrip_names_bytes, new_names_bytes,
        "Round-trip name table should be identical."
    );
}

```

`pdb/src/pdbi.rs`:

```rs
//! PDB Info Stream (aka the PDB Stream)
//!
//! # References
//! * <https://llvm.org/docs/PDB/PdbStream.html>

#[cfg(test)]
mod tests;

use std::collections::BTreeMap;

use super::*;
use crate::guid::GuidLe;
use anyhow::bail;
use bitvec::prelude::{BitSlice, Lsb0};
use bstr::ByteSlice;
use ms_codeview::encoder::Encoder;
use ms_codeview::parser::Parser;
use tracing::{trace, trace_span, warn};
use uuid::Uuid;
use zerocopy::{FromBytes, Immutable, IntoBytes, KnownLayout, LE, U32, Unaligned};

/// Contains the PDB Information Stream.
///
/// This implementation reads all of the data from the PDBI Stream and converts it to in-memory
/// data structures. This is not typical for most of the data within the PDB. We do this because
/// the PDBI is fairly small, is needed for reading most PDBs, and will often need to be edited
/// for generating or rebuilding PDBs.
#[allow(missing_docs)]
#[derive(Clone)]
pub struct PdbiStream {
    pub signature: u32,
    pub version: u32,
    pub age: u32,
    pub unique_id: Option<Uuid>,
    pub named_streams: NamedStreams,
    pub features: Vec<FeatureCode>,
}

impl PdbiStream {
    /// Parses the stream.
    pub fn parse(stream_data: &[u8]) -> anyhow::Result<Self> {
        let mut p = Parser::new(stream_data);

        let header: &PdbiStreamHeader = p.get()?;
        let version = header.version.get();

        // Older PDBs (pre-VC7, i.e. before 2000) do not contain a GUID.
        let unique_id = if pdbi_has_unique_id(version) {
            // Check that the stream data is large enough to contain the unique ID.
            // We use slices, below, relying on bounds checking here.
            Some(p.get::<GuidLe>()?.get())
        } else {
            None
        };

        let named_streams = NamedStreams::parse(&mut p)?;

        // The last part of the PDBI stream is a list of "features". Features are u32 values, and
        // the feature values are defined as constants. If a feature is present in this list, then
        // that feature is enabled.
        let mut features: Vec<FeatureCode> = Vec::with_capacity(p.len() / 4);
        while p.len() >= 4 {
            let feature = FeatureCode(p.u32()?);
            features.push(feature);
        }

        Ok(Self {
            signature: header.signature.get(),
            version,
            age: header.age.get(),
            unique_id,
            named_streams,
            features,
        })
    }

    /// Serializes this to a stream.
    pub fn to_bytes(&self) -> anyhow::Result<Vec<u8>> {
        let mut out = Vec::new();

        let mut e = Encoder::new(&mut out);

        let header = PdbiStreamHeader {
            signature: U32::new(self.signature),
            version: U32::new(self.version),
            age: U32::new(self.age),
        };

        e.t(&header);
        if pdbi_has_unique_id(self.version) {
            if let Some(unique_id) = &self.unique_id {
                e.uuid(unique_id);
            } else {
                bail!("The PDBI version requires a unique ID, but none has been provided.");
            }
        } else if self.unique_id.is_some() {
            warn!(
                "PDBI version is too old to have a unique ID, but this PdbiStream has a unique ID. It will be ignored."
            );
        }

        self.named_streams.to_bytes(&mut e);

        // Write the features.
        for &feature in self.features.iter() {
            e.u32(feature.0);
        }

        Ok(out)
    }

    /// Gets the 'age' value of the PDB. This links the PDB with the executable; a PDB must have
    /// the same age as its related executable.
    pub fn age(&self) -> u32 {
        self.age
    }

    /// Version from the PDBI header, e.g. [`PDBI_VERSION_VC110`].
    pub fn version(&self) -> u32 {
        self.version
    }

    /// The binding key that associates this PDB with a given PE executable.
    pub fn binding_key(&self) -> BindingKey {
        BindingKey {
            guid: self.unique_id.unwrap_or(Uuid::nil()),
            age: self.age,
        }
    }

    /// Provides access to the named streams table.
    pub fn named_streams(&self) -> &NamedStreams {
        &self.named_streams
    }

    /// Provides mutable access to the named streams table.
    pub fn named_streams_mut(&mut self) -> &mut NamedStreams {
        &mut self.named_streams
    }

    /// Checks whether this PDB has a given feature enabled.
    pub fn has_feature(&self, feature_code: FeatureCode) -> bool {
        self.features.contains(&feature_code)
    }
}

#[allow(missing_docs)]
pub const PDBI_VERSION_VC2: u32 = 19941610;
#[allow(missing_docs)]
pub const PDBI_VERSION_VC4: u32 = 19950623;
#[allow(missing_docs)]
pub const PDBI_VERSION_VC41: u32 = 19950814;
#[allow(missing_docs)]
pub const PDBI_VERSION_VC50: u32 = 19960307;
#[allow(missing_docs)]
pub const PDBI_VERSION_VC98: u32 = 19970604;
#[allow(missing_docs)]
pub const PDBI_VERSION_VC70_DEPRECATED: u32 = 19990604; // deprecated vc70 implementation version
#[allow(missing_docs)]
pub const PDBI_VERSION_VC70: u32 = 20000404; // <-- first version that has unique id
#[allow(missing_docs)]
pub const PDBI_VERSION_VC80: u32 = 20030901;
#[allow(missing_docs)]
pub const PDBI_VERSION_VC110: u32 = 20091201;
#[allow(missing_docs)]
pub const PDBI_VERSION_VC140: u32 = 20140508;

fn pdbi_has_unique_id(version: u32) -> bool {
    version > PDBI_VERSION_VC70_DEPRECATED
}

/// The header of the PDB Info stream.
#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Debug)]
#[repr(C)]
#[allow(missing_docs)]
pub struct PdbiStreamHeader {
    pub version: U32<LE>,
    pub signature: U32<LE>,
    pub age: U32<LE>,
    // This is only present if the version number is higher than impvVC70Dep.
    // pub unique_id: GuidLe,
}

#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Debug)]
#[repr(C)]
#[allow(missing_docs)]
pub struct HashTableHeader {
    pub size: U32<LE>,
    pub capacity: U32<LE>,
    // present bit vector
    // deleted bit vector
    // (key, value) pairs
}

#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Debug)]
#[repr(C)]
#[allow(missing_docs)]
pub struct HashEntry {
    pub key: U32<LE>,
    pub value: U32<LE>,
}

/// Provides access to the Named Streams Table.
#[derive(Default, Clone)]
pub struct NamedStreams {
    /// If true, the named streams set has been modified since it was loaded.
    pub(crate) modified: bool,

    /// Stores the mapping.
    ///
    /// We use `BTreeMap` so that the names are ordered.
    map: BTreeMap<String, u32>,
}

impl NamedStreams {
    /// Iterates the named streams.
    pub fn iter(&self) -> impl Iterator<Item = (&String, &u32)> {
        self.map.iter()
    }

    /// Searches the list of named strings for `name`. If found, returns the stream index.
    ///
    /// This does _not_ use a hash function. It just sequentially searches.
    /// This uses a case-sensitive comparison.
    pub fn get(&self, name: &str) -> Option<u32> {
        self.map.get(name).copied()
    }

    /// Searches the list of named strings for `name`. If found, returns the stream index.
    /// If not found, returns a descriptive error.
    ///
    /// This does _not_ use a hash function. It just sequentially searches.
    /// This uses a case-sensitive comparison.
    pub fn get_err(&self, name: &str) -> anyhow::Result<u32> {
        if let Some(&stream) = self.map.get(name) {
            Ok(stream)
        } else {
            bail!("Failed to find a named stream {:?}", name);
        }
    }

    /// Parses a `NamedStreams` table.
    pub fn parse(p: &mut Parser) -> anyhow::Result<Self> {
        let names_size = p.u32()?;
        let names_data = p.bytes(names_size as usize)?;

        // This is the "cdr" (cardinality) field in pdb.cpp.
        let name_count = p.u32()?;
        let _name_hash_size = p.u32()?;

        let present_u32_count = p.u32()?;
        let present_mask = p.bytes(present_u32_count as usize * 4)?;
        let present_num_items: u32 = present_mask.iter().map(|&b| b.count_ones()).sum();

        let deleted_u32_count = p.u32()?;
        let deleted_mask = p.bytes(deleted_u32_count as usize * 4)?;
        let _deleted_num_items: u32 = deleted_mask.iter().map(|&b| b.count_ones()).sum();

        if present_num_items != name_count {
            bail!(
                "The PDBI name table contains inconsistent values.  Name count is {}, but present bitmap count is {}.",
                name_count,
                present_num_items
            );
        }

        let items: &[HashEntry] = p.slice(name_count as usize)?;

        let mut names: BTreeMap<String, u32> = BTreeMap::new();

        for item in items.iter() {
            let key = item.key.get();
            let stream = item.value.get();
            // Key is a byte offset into names_data.
            // Value is a stream index.

            let mut kp = Parser::new(names_data);
            kp.skip(key as usize)?;
            let name = kp.strz()?.to_str_lossy();

            if let Some(existing_stream) = names.get(&*name) {
                warn!(
                    "The PDBI contains more than one stream with the same name {:?}: stream {} vs stream {}",
                    name, existing_stream, stream
                );
                continue;
            }

            names.insert(name.to_string(), stream);
        }

        // Parse the "number of NameIndex" values at the end (niMac).
        let num_name_index = p.u32()?;
        if num_name_index != 0 {
            warn!(
                "The Named Streams table contains a non-zero value for the 'niMac' field. This is not supported"
            );
        }

        Ok(Self {
            modified: false,
            map: names,
        })
    }

    /// Inserts a new named stream.
    ///
    /// Returns `true` if the mapping was inserted.
    ///
    /// Returns `false` if there was already a mapping with the given name. In this case, the
    /// named stream table is not modified.
    pub fn insert(&mut self, name: &str, value: u32) -> bool {
        if self.map.contains_key(name) {
            false
        } else {
            self.modified = true;
            self.map.insert(name.to_string(), value);
            true
        }
    }

    /// Removes all entries from the named stream map.
    pub fn clear(&mut self) {
        self.modified = true;
        self.map.clear();
    }

    /// Encode this table to a byte stream
    pub fn to_bytes(&self, e: &mut Encoder) {
        let _span = trace_span!("NamedStreams::to_bytes").entered();

        // Sort the names in the table, so that we have a deterministic order.
        let mut sorted_names: Vec<(&String, u32)> = Vec::with_capacity(self.map.len());
        for (name, stream) in self.map.iter() {
            sorted_names.push((name, *stream));
        }
        sorted_names.sort_unstable();
        let num_names = sorted_names.len();

        // Find the size of the string data table and find the position of every string in that
        // table. We have to do this after sorting the strings.
        let mut strings_len: usize = 0;
        let name_offsets: Vec<u32> = sorted_names
            .iter()
            .map(|(name, _)| {
                let this_pos = strings_len;
                strings_len += name.len() + 1;
                this_pos as u32
            })
            .collect();

        // Write the string data. This is prefixed by the length of the string data.
        e.u32(strings_len as u32);
        for &(name, _) in sorted_names.iter() {
            e.strz(BStr::new(name));
        }

        // We are going to encode this hash table using the format defined by PDBI.  This format
        // is a hash table that uses linear probing.  We choose a load factor of 2x, then hash all
        // the items and place them in the table.
        //
        // Choose a hash size that is larger than our list of names.
        let hash_size = if sorted_names.is_empty() {
            10
        } else {
            sorted_names.len() * 2
        };

        // Find the size of the "present" and "deleted" bitmaps. These bitmaps have the same size.
        let bitmap_size_u32s = hash_size.div_ceil(32);
        let mut present_bitmap_bytes: Vec<u8> = vec![0; bitmap_size_u32s * 4];
        let present_bitmap: &mut BitSlice<u8, Lsb0> =
            BitSlice::from_slice_mut(present_bitmap_bytes.as_mut_slice());

        // hash_slots contains (string_index, stream)
        let mut hash_slots: Vec<Option<(u32, u32)>> = Vec::new();
        hash_slots.resize_with(hash_size, Default::default);

        trace!(num_names, hash_size);

        // Assign all strings to hash slots.
        for (i, &(name, stream)) in sorted_names.iter().enumerate() {
            let name_offset = name_offsets[i];
            let h = crate::hash::hash_mod_u16(name.as_bytes(), 0xffff_ffff) as usize % hash_size;
            let mut slot = h;
            loop {
                if hash_slots[slot].is_none() {
                    hash_slots[slot] = Some((name_offset, stream));
                    present_bitmap.set(slot, true);
                    trace!(
                        assigned_name = name,
                        hash = h,
                        slot = slot,
                        name_offset,
                        stream
                    );
                    break;
                }
                slot += 1;
                assert_ne!(
                    slot, h,
                    "linear probing should not wrap around to starting slot"
                );
                if slot == hash_slots.len() {
                    slot = 0;
                }
            }
        }

        // Write the "cardinality" (number of elements in the table) field.
        e.u32(num_names as u32);

        // Write the number of hashes field.
        e.u32(hash_size as u32);

        // Write the "present" bitmap.
        e.u32(bitmap_size_u32s as u32);
        e.bytes(&present_bitmap_bytes);

        // Write the "deleted" bitmap.
        e.u32(bitmap_size_u32s as u32);
        for _ in 0..bitmap_size_u32s {
            e.u32(0);
        }

        // Write the entries from the hash table that are present.
        for slot in hash_slots.iter() {
            if let Some(slot) = slot {
                e.u32(slot.0);
                e.u32(slot.1);
            }
        }

        // Write the "number of NameIndex values" (niMac).
        e.u32(0);
    }
}

/// A feature code is a `u32` value that indicates that an optional feature is enabled for a given PDB.
#[derive(Copy, Clone, Eq, PartialEq, Debug, Hash, Ord, PartialOrd)]
pub struct FeatureCode(pub u32);

impl FeatureCode {
    /// Indicates that this PDB is a "mini PDB", produced by using the `/DEBUG:FASTLINK` parameter.
    ///
    /// See: <https://learn.microsoft.com/en-us/cpp/build/reference/debug-generate-debug-info?view=msvc-170>
    pub const MINI_PDB: FeatureCode = FeatureCode(0x494E494D);
}

```

`pdb/src/pdbi/tests.rs`:

```rs
use pretty_hex::PrettyHex;

use super::*;

fn names_build(names: &NamedStreams) {
    let mut bytes = Vec::new();
    names.to_bytes(&mut Encoder::new(&mut bytes));
    println!("\n{:?}", bytes.hex_dump());

    // Round-trip testing: Decode the stream that we just built.
    let mut p = Parser::new(&bytes);
    let rt_names =
        NamedStreams::parse(&mut p).expect("expected to successfully parse names stream");

    assert_eq!(names.map, rt_names.map);
    assert!(
        p.is_empty(),
        "found unparsed bytes at the end:\n{:?}",
        p.peek_rest().hex_dump()
    );

    // Round-trip testing *again*.  Encode the round-trip table into bytes again, and verify that
    // we got the exact same bytes.
    let mut rt_bytes = Vec::new();
    names.to_bytes(&mut Encoder::new(&mut rt_bytes));
    assert_eq!(bytes, rt_bytes, "expected round-trip bytes to be the same");
}

#[test]
fn names_build_empty() {
    let names = NamedStreams::default();
    names_build(&names);
}

#[test]
fn names_build_simple() {
    let mut names = NamedStreams::default();
    names.map.insert("/foo".to_string(), 100);
    names.map.insert("/bar".to_string(), 200);
    names_build(&names);
}

#[test]
fn names_build_many() {
    let n = 100;
    let mut names = NamedStreams::default();
    for i in 0..n {
        names.map.insert(format!("/num/{i:04}"), 1000 + i as u32);
    }
    names_build(&names);
}

```

`pdb/src/stream_index.rs`:

```rs
use std::fmt::Display;
use zerocopy::{LE, U16};
use zerocopy_derive::{FromBytes, Immutable, IntoBytes, KnownLayout, Unaligned};

/// Identifies a stream in a PDB/MSF file.
///
/// This type guards against NIL stream values. The value stored in `Stream` should never be
/// a NIL value (0xFFFF).
#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Debug, Hash)]
#[repr(transparent)]
pub struct Stream(u16);

impl Stream {
    // Some streams have a fixed index.

    /// Fixed stream index 0 is the Previous MSF Stream Directory
    pub const OLD_STREAM_DIR: Stream = Stream(0);

    /// Index of the PDB Information Stream. It contains version information and information to
    /// connect this PDB to the executable.
    pub const PDB: Stream = Stream(1);

    /// Index of the Type Information Stream. It contains type records.
    pub const TPI: Stream = Stream(2);

    /// Debug Information Stream (DBI).
    pub const DBI: Stream = Stream(3);

    /// CodeView type records, index of IPI hash stream
    pub const IPI: Stream = Stream(4);

    /// Validates that `index` is non-NIL and converts it to a `Stream` value.
    ///
    /// If `index` is NIL (0xffff), then this returns `None`.
    pub fn new(index: u16) -> Option<Stream> {
        if index == NIL_STREAM_INDEX {
            None
        } else {
            Some(Stream(index))
        }
    }

    /// Returns the value of the stream index.
    pub fn value(self) -> u16 {
        self.0
    }

    /// Returns the value of the stream index, cast to `usize`. Use this when indexing slices.
    pub fn index(self) -> usize {
        debug_assert!(self.0 != NIL_STREAM_INDEX);
        self.0 as usize
    }
}

impl From<Stream> for u32 {
    fn from(value: Stream) -> Self {
        value.value() as u32
    }
}

impl Display for Stream {
    fn fmt(&self, f: &mut std::fmt::Formatter) -> std::fmt::Result {
        Display::fmt(&self.0, f)
    }
}

/// A reserved stream index meaning "no stream at all", in `u16`.
pub const NIL_STREAM_INDEX: u16 = 0xffff;

/// Error type for `Stream::try_from` implementations.
#[derive(Clone, Debug)]
pub struct StreamIndexIsNilError;

impl std::error::Error for StreamIndexIsNilError {}

impl Display for StreamIndexIsNilError {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        fmt.write_str("The given stream index is NIL.")
    }
}

#[derive(Clone, Debug)]
pub struct StreamIndexOverflow;

impl std::error::Error for StreamIndexOverflow {}

impl Display for StreamIndexOverflow {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        fmt.write_str("The value is out of range for 16-bit stream indexes.")
    }
}

impl TryFrom<u16> for Stream {
    type Error = StreamIndexIsNilError;

    fn try_from(i: u16) -> Result<Self, Self::Error> {
        if i != NIL_STREAM_INDEX {
            Ok(Self(i))
        } else {
            Err(StreamIndexIsNilError)
        }
    }
}

/// This structure can be embedded directly in structure definitions.
#[derive(
    Copy, Clone, Eq, PartialEq, Debug, IntoBytes, FromBytes, Immutable, KnownLayout, Unaligned,
)]
#[repr(transparent)]
pub struct StreamIndexU16(pub U16<LE>);

impl StreamIndexU16 {
    /// The value of a nil stream index.
    pub const NIL: Self = Self(U16::from_bytes(NIL_STREAM_INDEX.to_le_bytes()));

    /// Checks whether this value is a nil stream index. Returns `Ok` if the value is not a nil
    /// stream index, or `Err` if it is a nil stream index.
    pub fn get(self) -> Option<u32> {
        let s = self.0.get();
        if s != NIL_STREAM_INDEX {
            Some(s as u32)
        } else {
            None
        }
    }

    /// Checks whether this value is a nil stream index. Returns `Ok` if the value is not a nil
    /// stream index, or `Err` if it is a nil stream index.
    pub fn get_err(self) -> Result<u32, StreamIndexIsNilError> {
        let s = self.0.get();
        if s != NIL_STREAM_INDEX {
            Ok(s as u32)
        } else {
            Err(StreamIndexIsNilError)
        }
    }
}

impl TryFrom<u32> for StreamIndexU16 {
    type Error = StreamIndexOverflow;

    fn try_from(s: u32) -> Result<Self, Self::Error> {
        if s < NIL_STREAM_INDEX as u32 {
            Ok(StreamIndexU16(U16::new(s as u16)))
        } else {
            Err(StreamIndexOverflow)
        }
    }
}

impl TryFrom<Option<u32>> for StreamIndexU16 {
    type Error = StreamIndexOverflow;

    fn try_from(s_opt: Option<u32>) -> Result<Self, Self::Error> {
        if let Some(s) = s_opt {
            if s < NIL_STREAM_INDEX as u32 {
                Ok(StreamIndexU16(U16::new(s as u16)))
            } else {
                Err(StreamIndexOverflow)
            }
        } else {
            Ok(Self::NIL)
        }
    }
}

```

`pdb/src/taster.rs`:

```rs
//! Determines whether a given file header is a PDB/MSF file, PDBZ/MSFZ file, or a Portable PDB file.

use sync_file::ReadAt;

/// Enumerates the kind of PDBs files that are recognized.
#[derive(Copy, Clone, Eq, PartialEq)]
pub enum Flavor {
    /// An ordinary PDB file.
    Pdb,
    /// A compressed PDB (PDZ) file.
    Pdz,
    /// A "Portable PDB" file.
    PortablePdb,
}

/// Determines whether a given file header is a PDB/MSF file, PDBZ/MSFZ file, or a Portable PDB file.
pub fn what_flavor<F: ReadAt>(f: &F) -> Result<Option<Flavor>, std::io::Error> {
    let mut header = [0u8; 0x100];
    let _n = f.read_at(&mut header, 0)?;
    if ms_pdb_msf::is_file_header_msf(&header) {
        Ok(Some(Flavor::Pdb))
    } else if ms_pdb_msfz::is_header_msfz(&header) {
        Ok(Some(Flavor::Pdz))
    } else if is_header_portable_pdb(&header) {
        Ok(Some(Flavor::PortablePdb))
    } else {
        Ok(None)
    }
}

fn is_header_portable_pdb(header: &[u8]) -> bool {
    header.len() >= 24 && header[16..24] == *b"PDB v1.0"
}

```

`pdb/src/tpi.rs`:

```rs
//! Type Information Stream (TPI)
//!
//! Layout of a Type Stream:
//!
//! * `TypeStreamHeader` - specifies lots of important parameters
//! * Type Record Data
//!
//! Each Type Stream may also have an associated Type Hash Stream. The Type Hash Stream contains
//! indexing information that helps find records within the main Type Stream. The Type Stream
//! Header specifies several parameters that are needed for finding and decoding the Type Hash
//! Stream.
//!
//! The Type Hash Stream contains:
//!
//! * Hash Value Buffer: Contains a list of hash values, one for each Type Record in the
//!   Type Stream.
//!
//!   The offset and size of the Hash Value Buffer is specified in the `TypeStreamHeader`, in the
//!   `hash_value_buffer_offset` and `hash_value_buffer_length` fields, respectively.
//!
//!   It should be assumed that there are either 0 hash values, or a number equal to the number of
//!   type records in the TPI stream (`type_index_end - type_end_begin`). Thus, if
//!   `hash_value_buffer_length` is not equal to `(type_index_end - type_end_begin) * hash_key_size`
//!   we can consider the PDB malformed.
//!
//! * Type Index Offset Buffer - A list of pairs of `u32` values where the first is a Type Index
//!   and the second is the offset within Type Record Data of the type with this index.
//!   This enables a binary search to find a given Type Index record.
//!
//!   The offset and size of the Type Index Offset Buffer is specified in the `TypeStreamHeader`,
//!   in the `index_offset_buffer_offset` and `index_offset_buffer_length` fields, respectively.
//!
//! * Hash Adjustment Buffer - A hash table whose keys are the hash values in the hash value
//!   buffer and whose values are type indices.
//!
//!   The offset and size of the Type Index Offset BUffer is specified in the `TypeStreamHeader`,
//!   in the `index_offset_buffer_offset` and `index_offset_buffer_length` fields, respectively.

pub mod hash;

use super::*;
use crate::types::fields::{Field, IterFields};
use crate::types::{TypeData, TypeIndex, TypeIndexLe, TypeRecord, TypesIter, build_types_starts};
use anyhow::bail;
use ms_codeview::parser::Parser;
use std::fmt::Debug;
use std::mem::size_of;
use std::ops::Range;
use zerocopy::{FromBytes, I32, Immutable, IntoBytes, KnownLayout, LE, U32, Unaligned};

/// The header of the TPI stream.
#[allow(missing_docs)]
#[derive(Clone, Eq, PartialEq, IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Debug)]
#[repr(C)]
pub struct TypeStreamHeader {
    pub version: U32<LE>,
    pub header_size: U32<LE>,
    pub type_index_begin: TypeIndexLe,
    pub type_index_end: TypeIndexLe,
    /// The number of bytes of type record data following the `TypeStreamHeader`.
    pub type_record_bytes: U32<LE>,

    pub hash_stream_index: StreamIndexU16,
    pub hash_aux_stream_index: StreamIndexU16,

    /// The size of each hash key in the Hash Value Substream. For the current version of TPI,
    /// this value should always be 4.
    pub hash_key_size: U32<LE>,
    /// The number of hash buckets. This is used when calculating the record hashes. Each hash
    /// is computed, and then it is divided by num_hash_buckets and the remainder becomes the
    /// final hash.
    ///
    /// If `hash_value_buffer_length` is non-zero, then `num_hash_buckets` must also be non-zero.
    pub num_hash_buckets: U32<LE>,
    pub hash_value_buffer_offset: I32<LE>,
    pub hash_value_buffer_length: U32<LE>,

    pub index_offset_buffer_offset: I32<LE>,
    pub index_offset_buffer_length: U32<LE>,

    pub hash_adj_buffer_offset: I32<LE>,
    pub hash_adj_buffer_length: U32<LE>,
}

impl TypeStreamHeader {
    /// Makes an empty one
    pub fn empty() -> Self {
        Self {
            version: Default::default(),
            header_size: U32::new(size_of::<TypeStreamHeader>() as u32),
            type_index_begin: TypeIndexLe(U32::new(TypeIndex::MIN_BEGIN.0)),
            type_index_end: TypeIndexLe(U32::new(TypeIndex::MIN_BEGIN.0)),
            type_record_bytes: Default::default(),
            hash_stream_index: StreamIndexU16::NIL,
            hash_aux_stream_index: StreamIndexU16::NIL,
            hash_key_size: Default::default(),
            num_hash_buckets: Default::default(),
            hash_value_buffer_offset: Default::default(),
            hash_value_buffer_length: Default::default(),
            index_offset_buffer_offset: Default::default(),
            index_offset_buffer_length: Default::default(),
            hash_adj_buffer_offset: Default::default(),
            hash_adj_buffer_length: Default::default(),
        }
    }
}

/// The size of the `TpiStreamHeader` structure.
pub const TPI_STREAM_HEADER_LEN: usize = size_of::<TypeStreamHeader>();

/// The expected value of `TypeStreamHeader::version`.
pub const TYPE_STREAM_VERSION_2004: u32 = 20040203;

/// Contains a TPI Stream or IPI Stream.
pub struct TypeStream<StreamData>
where
    StreamData: AsRef<[u8]>,
{
    /// The stream data. This contains the entire type stream, including header and type records.
    pub stream_data: StreamData,

    type_index_begin: TypeIndex,
    type_index_end: TypeIndex,

    /// A starts vector for type record offsets. This is created on-demand, since many users of
    /// `TypeStream` do not need this.
    record_starts: OnceCell<Vec<u32>>,
}

/// Distinguishes the TPI and IPI streams.
#[derive(Copy, Clone, Eq, PartialEq, Debug)]
pub enum TypeStreamKind {
    /// The primary type stream
    TPI,
    /// The ID stream
    IPI,
}

impl TypeStreamKind {
    /// Get the stream index. Fortunately, the stream indexes are fixed.
    pub fn stream(self) -> Stream {
        match self {
            Self::IPI => Stream::IPI,
            Self::TPI => Stream::TPI,
        }
    }
}

/// Represents an entry in the Hash Index Offset Substream.
#[repr(C)]
#[derive(IntoBytes, FromBytes, KnownLayout, Immutable, Unaligned, Debug)]
pub struct HashIndexPair {
    /// The type index at the start of this range.
    pub type_index: TypeIndexLe,
    /// The offset within the Type Records Substream (not the entire Type Stream) where this
    /// record begins.
    pub offset: U32<LE>,
}

impl<StreamData> TypeStream<StreamData>
where
    StreamData: AsRef<[u8]>,
{
    /// Gets a reference to the stream header
    pub fn header(&self) -> Option<&TypeStreamHeader> {
        let stream_data: &[u8] = self.stream_data.as_ref();
        let (header, _) = TypeStreamHeader::ref_from_prefix(stream_data).ok()?;
        Some(header)
    }

    /// Returns the version of the stream, or `TYPE_STREAM_VERSION_2004` if this is an empty stream.
    pub fn version(&self) -> u32 {
        if let Some(header) = self.header() {
            header.version.get()
        } else {
            TYPE_STREAM_VERSION_2004
        }
    }

    /// Returns the stream index of the related hash stream, if any.
    pub fn hash_stream(&self) -> Option<u32> {
        self.header()?.hash_stream_index.get()
    }

    /// Checks whether this is a degenerate empty stream.
    pub fn is_empty(&self) -> bool {
        self.stream_data.as_ref().is_empty()
    }

    /// Gets a mutable reference to the stream header
    pub fn header_mut(&mut self) -> Option<&mut TypeStreamHeader>
    where
        StreamData: AsMut<[u8]>,
    {
        let (header, _) = TypeStreamHeader::mut_from_prefix(self.stream_data.as_mut()).ok()?;
        Some(header)
    }

    /// The type index of the first type record.
    pub fn type_index_begin(&self) -> TypeIndex {
        self.type_index_begin
    }

    /// The type index of the last type record, plus 1.
    pub fn type_index_end(&self) -> TypeIndex {
        self.type_index_end
    }

    /// The number of types defined in the type stream.
    pub fn num_types(&self) -> u32 {
        self.type_index_end.0 - self.type_index_begin.0
    }

    /// Gets the byte offset within the stream of the record data.
    pub fn records_offset(&self) -> usize {
        if let Some(header) = self.header() {
            header.header_size.get() as usize
        } else {
            0
        }
    }

    /// Returns the encoded type records found in the TPI or IPI stream.
    ///
    /// The type records immediately follow the type stream. The length is given by the
    /// header field type_record_bytes. The values in the header were validated in
    /// read_tpi_or_ipi_stream(), so we do not need to check them again, here.
    pub fn type_records_bytes(&self) -> &[u8] {
        let records_range = self.type_records_range();
        if records_range.is_empty() {
            &[]
        } else {
            &self.stream_data.as_ref()[records_range]
        }
    }

    /// Returns the encoded type records found in the type stream.
    pub fn type_records_bytes_mut(&mut self) -> &mut [u8]
    where
        StreamData: AsMut<[u8]>,
    {
        let records_range = self.type_records_range();
        if records_range.is_empty() {
            &mut []
        } else {
            &mut self.stream_data.as_mut()[records_range]
        }
    }

    /// Returns the byte range of the encoded type records found in the type stream.
    pub fn type_records_range(&self) -> std::ops::Range<usize> {
        if let Some(header) = self.header() {
            let size = header.type_record_bytes.get();
            if size == 0 {
                return 0..0;
            }
            let type_records_start = header.header_size.get();
            let type_records_end = type_records_start + size;
            type_records_start as usize..type_records_end as usize
        } else {
            0..0
        }
    }

    /// Iterates the types contained within this type stream.
    pub fn iter_type_records(&self) -> TypesIter<'_> {
        TypesIter::new(self.type_records_bytes())
    }

    /// Parses the header of a Type Stream and validates it.
    pub fn parse(stream_index: Stream, stream_data: StreamData) -> anyhow::Result<Self> {
        let stream_bytes: &[u8] = stream_data.as_ref();

        if stream_bytes.is_empty() {
            return Ok(Self {
                stream_data,
                type_index_begin: TypeIndex::MIN_BEGIN,
                type_index_end: TypeIndex::MIN_BEGIN,
                record_starts: OnceCell::new(),
            });
        }

        let mut p = Parser::new(stream_bytes);
        let tpi_stream_header: TypeStreamHeader = p.copy()?;

        let type_index_begin = tpi_stream_header.type_index_begin.get();
        let type_index_end = tpi_stream_header.type_index_end.get();
        if type_index_end < type_index_begin {
            bail!(
                "Type stream (stream {stream_index}) has invalid values in header.  \
                 The type_index_begin field is greater than the type_index_end field."
            );
        }

        if type_index_begin < TypeIndex::MIN_BEGIN {
            bail!(
                "The Type Stream has an invalid value for type_index_begin ({type_index_begin:?}). \
                 It is less than the minimum required value ({}).",
                TypeIndex::MIN_BEGIN.0
            );
        }

        let type_data_start = tpi_stream_header.header_size.get();
        if type_data_start < TPI_STREAM_HEADER_LEN as u32 {
            bail!(
                "Type stream (stream {stream_index}) has invalid values in header.  \
                 The header_size field is smaller than the definition of the actual header."
            );
        }

        let type_data_end = type_data_start + tpi_stream_header.type_record_bytes.get();
        if type_data_end > stream_bytes.len() as u32 {
            bail!(
                "Type stream (stream {stream_index}) has invalid values in header.  \
                   The header_size and type_record_bytes fields exceed the size of the stream."
            );
        }

        Ok(TypeStream {
            stream_data,
            type_index_begin,
            type_index_end,
            record_starts: OnceCell::new(),
        })
    }

    /// Builds a "starts" table that gives the starting location of each type record.
    pub fn build_types_starts(&self) -> TypeIndexMap {
        let starts =
            crate::types::build_types_starts(self.num_types() as usize, self.type_records_bytes());

        TypeIndexMap {
            type_index_begin: self.type_index_begin,
            type_index_end: self.type_index_end,
            starts,
        }
    }

    /// Creates a new `TypeStream` that referenced the stream data of this `TypeStream`.
    /// This is typically used for temporarily creating a `TypeStream<&[u8]>` from a
    /// `TypeStream<Vec<u8>>`.
    pub fn to_ref(&self) -> TypeStream<&[u8]> {
        TypeStream {
            stream_data: self.stream_data.as_ref(),
            type_index_begin: self.type_index_begin,
            type_index_end: self.type_index_end,
            record_starts: OnceCell::new(),
        }
    }

    /// Gets the "starts" vector for the byte offsets of the records in this `TypeStream`.
    ///
    /// This function will create the starts vector on-demand.
    pub fn record_starts(&self) -> &[u32] {
        self.record_starts.get_or_init(|| {
            let type_records = self.type_records_bytes();
            build_types_starts(self.num_types() as usize, type_records)
        })
    }

    /// Returns `true` if `type_index` refers to a primitive type.
    pub fn is_primitive(&self, type_index: TypeIndex) -> bool {
        type_index < self.type_index_begin
    }

    /// Retrieves the type record identified by `type_index`.
    ///
    /// This should only be used for non-primitive `TypeIndex` values. If this is called with a
    /// primitive `TypeIndex` then it will return `Err`.
    pub fn record(&self, type_index: TypeIndex) -> anyhow::Result<TypeRecord<'_>> {
        let Some(relative_type_index) = type_index.0.checked_sub(self.type_index_begin.0) else {
            bail!("The given TypeIndex is a primitive type index, not a type record.");
        };

        let starts = self.record_starts();
        let Some(&record_start) = starts.get(relative_type_index as usize) else {
            bail!(
                "The given TypeIndex {type_index:?} is out of bounds (exceeds maximum allowed TypeIndex)"
            );
        };

        let all_type_records = self.type_records_bytes();
        let Some(this_type_record_slice) = all_type_records.get(record_start as usize..) else {
            // This should never happen, but let's be cautious.
            bail!("Internal error: record offset is out of range.");
        };

        let mut iter = TypesIter::new(this_type_record_slice);
        if let Some(record) = iter.next() {
            Ok(record)
        } else {
            bail!("Failed to decode type record");
        }
    }

    /// Iterate the fields of an `LF_STRUCTURE`, `LF_CLASS`, `LF_ENUM`, etc. This correctly
    /// iterates across chains of `LF_FIELDLIST`.
    pub fn iter_fields(&self, field_list: TypeIndex) -> IterFieldChain<'_, StreamData> {
        // We initialize `fields` to an empty iterator so that the first iteration of
        // IterFieldChain::next() will find no records and will then check next_field_list.
        IterFieldChain {
            type_stream: self,
            next_field_list: if field_list.0 != 0 {
                Some(field_list)
            } else {
                None
            },
            fields: IterFields { bytes: &[] },
        }
    }
}

/// Iterator state for `iter_fields`
pub struct IterFieldChain<'a, StreamData>
where
    StreamData: AsRef<[u8]>,
{
    /// The current `LF_FIELDLIST` record that we are decoding.
    fields: IterFields<'a>,

    /// Allows us to read `LF_FIELDLIST` records.
    type_stream: &'a TypeStream<StreamData>,

    /// The pointer to the next `LF_FIELDLIST` that we will decode.
    next_field_list: Option<TypeIndex>,
}

impl<'a, StreamData> Iterator for IterFieldChain<'a, StreamData>
where
    StreamData: AsRef<[u8]>,
{
    type Item = Field<'a>;

    fn next(&mut self) -> Option<Self::Item> {
        loop {
            if let Some(field) = self.fields.next() {
                if let Field::Index(index) = &field {
                    // The full field list is split across more than one LF_FIELDLIST record.
                    // Store the link to the next field list and do not return this item to the caller.
                    self.next_field_list = Some(*index);
                    continue;
                }

                return Some(field);
            }

            // We have run out of fields in the current LF_FIELDLIST record.
            // See if there is a pointer to another LF_FIELDLIST.
            let next_field_list = self.next_field_list.take()?;
            let next_record = self.type_stream.record(next_field_list).ok()?;
            match next_record.parse().ok()? {
                TypeData::FieldList(fl) => {
                    // Restart iteration on the new field list.
                    self.fields = fl.iter();
                }
                _ => {
                    // Wrong record type!
                    return None;
                }
            }
        }
    }
}

impl<F: ReadAt> crate::Pdb<F> {
    /// Reads the TPI stream.
    pub fn read_type_stream(&self) -> anyhow::Result<TypeStream<Vec<u8>>> {
        self.read_tpi_or_ipi_stream(Stream::TPI)
    }

    /// Reads the IPI stream.
    pub fn read_ipi_stream(&self) -> anyhow::Result<TypeStream<Vec<u8>>> {
        self.read_tpi_or_ipi_stream(Stream::IPI)
    }

    /// Reads the TPI or IPI stream.
    pub fn read_tpi_or_ipi_stream(
        &self,
        stream_index: Stream,
    ) -> anyhow::Result<TypeStream<Vec<u8>>> {
        let stream_data = self.read_stream_to_vec(stream_index.into())?;
        TypeStream::parse(stream_index, stream_data)
    }
}

/// Maps `TypeIndex` values to the byte range of records within a type stream.
pub struct TypeIndexMap {
    /// Copied from type stream header.
    pub type_index_begin: TypeIndex,

    /// Copied from type stream header.
    pub type_index_end: TypeIndex,

    /// Contains a "starts" vector for the byte offsets of each type record.
    ///
    /// This vector has an additional value at the end, which gives the size in bytes of the
    /// type stream.
    pub starts: Vec<u32>,
}

impl TypeIndexMap {
    /// Tests whether a `TypeIndex` is a primitive type.
    pub fn is_primitive(&self, ti: TypeIndex) -> bool {
        ti < self.type_index_begin
    }

    /// Given a `TypeIndex`, returns the byte range within a `TypeStream` where that record
    /// is stored.
    ///
    /// If `ti` is a primitive type then this function will return `Err`. The caller should
    /// use the `is_primitive` method to check whether a `TypeIndex` is a primitive type.
    pub fn record_range(&self, ti: TypeIndex) -> anyhow::Result<Range<usize>> {
        let Some(i) = ti.0.checked_sub(self.type_index_begin.0) else {
            bail!("The TypeIndex is a primitive type, not a type record.");
        };

        if let Some(w) = self.starts.get(i as usize..i as usize + 2) {
            Ok(w[0] as usize..w[1] as usize)
        } else {
            bail!("The TypeIndex is out of range.");
        }
    }
}

/// Represents the cached state of a Type Stream header.
pub struct CachedTypeStreamHeader {
    pub(crate) header: Option<TypeStreamHeader>,
}

impl CachedTypeStreamHeader {
    /// Gets direct access to the type stream header, if any.
    pub fn header(&self) -> Option<&TypeStreamHeader> {
        self.header.as_ref()
    }

    /// Gets the beginning of the type index space, or `TypeIndex::MIN_BEGIN` if this type stream
    /// does not contain any data.
    pub fn type_index_begin(&self) -> TypeIndex {
        if let Some(h) = &self.header {
            h.type_index_begin.get()
        } else {
            TypeIndex::MIN_BEGIN
        }
    }

    /// Gets the end of the type index space, or `TypeIndex::MIN_BEGIN` if this type stream does
    /// not contain any data.
    pub fn type_index_end(&self) -> TypeIndex {
        if let Some(h) = &self.header {
            h.type_index_end.get()
        } else {
            TypeIndex::MIN_BEGIN
        }
    }
}

```

`pdb/src/tpi/hash.rs`:

```rs
//! Hashing functions for Type Records
//!
//! # References
//!
//! * [`TPI1::hashPrec` in `tpi.cpp`](https://github.com/microsoft/microsoft-pdb/blob/805655a28bd8198004be2ac27e6e0290121a5e89/PDB/dbi/tpi.cpp#L1296)

use crate::hash::hash_u32;
use crate::types::{Leaf, TypeData, UdtProperties};
use bstr::BStr;
use ms_codeview::parser::ParserError;
use zerocopy::IntoBytes;

/// Hash a type record, using the same rules as `TPI1::hashPrec`.
pub fn hash_type_record(
    kind: Leaf,
    record_bytes: &[u8],
    payload: &[u8],
) -> Result<u32, ParserError> {
    let tdata = TypeData::parse_bytes(kind, payload)?;

    match &tdata {
        TypeData::Alias(t) => Ok(hash_u32(t.name)),

        // This handles LF_CLASS, LF_STRUCTURE, and LF_INTERFACE.
        TypeData::Struct(t) => Ok(hash_udt_name(
            t.fixed.property.get(),
            record_bytes,
            t.name,
            t.unique_name,
        )),

        TypeData::Union(t) => Ok(hash_udt_name(
            t.fixed.property.get(),
            record_bytes,
            t.name,
            t.unique_name,
        )),

        TypeData::Enum(t) => Ok(hash_udt_name(
            t.fixed.property.get(),
            record_bytes,
            t.name,
            t.unique_name,
        )),

        TypeData::UdtSrcLine(t) => Ok(hash_u32(t.ty.as_bytes())),
        TypeData::UdtModSrcLine(t) => Ok(hash_u32(t.ty.as_bytes())),
        _ => Ok(crate::hash::hash_sig(record_bytes, 0)),
    }
}

fn hash_udt_name(
    prop: UdtProperties,
    record_bytes: &[u8],
    name: &BStr,
    unique_name: Option<&BStr>,
) -> u32 {
    if !prop.fwdref() && !is_udt_anon_name(name) {
        if prop.scoped() {
            if let Some(unique_name) = unique_name {
                return hash_u32(unique_name);
            }
        } else {
            // This branch is equivalent to the case handled by fIsGlobalDefUdt().
            return hash_u32(name);
        }
    }

    crate::hash::hash_sig(record_bytes, 0)
}

/// Tests if `name` is indicates that this UDT is an anonymous UDT.
pub fn is_udt_anon_name(name: &BStr) -> bool {
    name == "<unnamed-tag>"
        || name == "__unnamed"
        || name.ends_with(b"::<unnamed-tag>")
        || name.ends_with(b"::__unnamed")
}

```

`pdb/src/utils.rs`:

```rs
//! Misc utilities

pub mod align;
pub mod io;
pub mod iter;
pub mod path;
pub mod swizzle;
pub mod vec;

use std::ops::Range;
use zerocopy::{FromBytes, Immutable, IntoBytes};

/// Copies a value that implements `FromBytes`, by simply copying its byte representation.
pub fn copy_from_bytes<T>(t: &T) -> T
where
    T: IntoBytes + FromBytes + Immutable,
{
    FromBytes::read_from_bytes(t.as_bytes()).unwrap()
}

/// Helps decode records that are indexed using "starts" arrays.
pub struct StartsOf<'a, T> {
    /// The "starts" array
    pub starts: &'a [u32],
    /// The items that are being indexed.
    pub items: &'a [T],
}

impl<'a, T> StartsOf<'a, T> {
    /// Initializes a new starts-based array accessor.
    pub fn new(starts: &'a [u32], items: &'a [T]) -> Self {
        debug_assert!(!starts.is_empty());
        debug_assert_eq!(starts[0], 0);
        debug_assert_eq!(*starts.last().unwrap() as usize, items.len());
        debug_assert!(starts.windows(2).all(|w| w[0] <= w[1]));

        Self { starts, items }
    }
}

impl<'a, T> std::ops::Index<usize> for StartsOf<'a, T> {
    type Output = [T];

    fn index(&self, i: usize) -> &[T] {
        let start = self.starts[i] as usize;
        let end = self.starts[i + 1] as usize;
        &self.items[start..end]
    }
}

/// True if `n` is a multiple of 4.
pub fn is_aligned_4(n: usize) -> bool {
    (n & 3) == 0
}

/// Align n up to the next multiple of 4, if it is not already a multiple of 4.
pub fn align_4(n: usize) -> usize {
    (n + 3) & !3
}

/// Iterates ranges of items within a slice that share a common property.
pub fn iter_similar_ranges<'a, T, F>(items: &'a [T], is_eq: F) -> IterSimilarRanges<'a, T, F> {
    IterSimilarRanges {
        items,
        is_eq,
        start: 0,
    }
}

/// Iterates ranges of items within a slice that share a common property.
pub struct IterSimilarRanges<'a, T, F> {
    items: &'a [T],
    is_eq: F,
    start: usize,
}

impl<'a, T, F> Iterator for IterSimilarRanges<'a, T, F>
where
    F: FnMut(&T, &T) -> bool,
{
    type Item = Range<usize>;

    fn next(&mut self) -> Option<Self::Item> {
        if self.items.is_empty() {
            return None;
        }

        let first = &self.items[0];

        let mut i = 1;
        while i < self.items.len() && (self.is_eq)(first, &self.items[i]) {
            i += 1;
        }

        let start = self.start;
        self.start += i;
        self.items = &self.items[i..];

        Some(start..start + i)
    }
}

/// Iterates ranges of items within a slice that share a common property.
pub fn iter_similar_slices<'a, T, F>(items: &'a [T], is_eq: F) -> IterSimilarSlices<'a, T, F> {
    IterSimilarSlices { items, is_eq }
}

/// Iterates slices of items within a slice that share a common property.
pub struct IterSimilarSlices<'a, T, F> {
    items: &'a [T],
    is_eq: F,
}

impl<'a, T, F> Iterator for IterSimilarSlices<'a, T, F>
where
    F: FnMut(&T, &T) -> bool,
{
    type Item = &'a [T];

    fn next(&mut self) -> Option<Self::Item> {
        if self.items.is_empty() {
            return None;
        }

        let first = &self.items[0];

        let mut i = 1;
        while i < self.items.len() && (self.is_eq)(first, &self.items[i]) {
            i += 1;
        }

        let (lo, hi) = self.items.split_at(i);
        self.items = hi;
        Some(lo)
    }
}

/// Iterates ranges of items within a slice that share a common property.
pub fn iter_similar_slices_mut<'a, T, F>(
    items: &'a mut [T],
    is_eq: F,
) -> IterSimilarSlicesMut<'a, T, F> {
    IterSimilarSlicesMut { items, is_eq }
}

/// Iterates slices of items within a slice that share a common property.
pub struct IterSimilarSlicesMut<'a, T, F> {
    items: &'a mut [T],
    is_eq: F,
}

impl<'a, T, F> Iterator for IterSimilarSlicesMut<'a, T, F>
where
    F: FnMut(&T, &T) -> bool,
{
    type Item = &'a mut [T];

    fn next(&mut self) -> Option<Self::Item> {
        if self.items.is_empty() {
            return None;
        }

        let items = std::mem::take(&mut self.items);
        let first = &items[0];

        let mut i = 1;
        while i < items.len() && (self.is_eq)(first, &items[i]) {
            i += 1;
        }

        let (lo, hi) = items.split_at_mut(i);
        self.items = hi;
        Some(lo)
    }
}

```

`pdb/src/utils/align.rs`:

```rs
//! Alignment and padding utilities

/// Returns the number of alignment padding bytes that are needed to reach an alignment of 4,
/// given the number of bytes already in a buffer.
pub fn alignment_bytes_needed_4(n: usize) -> usize {
    (4 - (n & 3)) & 3
}

```

`pdb/src/utils/io.rs`:

```rs
#![allow(missing_docs)]

use std::io::{Read, Seek, SeekFrom, Write};
use sync_file::ReadAt;
use zerocopy::{FromBytes, FromZeros, IntoBytes};

pub fn read_struct_at<T: FromBytes + IntoBytes, R: ReadAt>(
    r: &R,
    offset: u64,
) -> std::io::Result<T> {
    let mut value = T::new_zeroed();
    r.read_exact_at(value.as_mut_bytes(), offset)?;
    Ok(value)
}

pub fn read_struct<T: FromBytes + IntoBytes, R: Read>(r: &mut R) -> std::io::Result<T> {
    let mut value: T = T::new_zeroed();
    let value_bytes = value.as_mut_bytes();
    r.read_exact(value_bytes)?;
    Ok(value)
}

pub fn read_boxed_slice<T: FromBytes + IntoBytes, R: Read>(
    r: &mut R,
    n: usize,
) -> std::io::Result<Box<[T]>> {
    let mut value = <[T]>::new_box_zeroed_with_elems(n).unwrap();
    r.read_exact(value.as_mut_bytes())?;
    Ok(value)
}

pub fn read_boxed_slice_at<T: FromBytes + IntoBytes, R: ReadAt>(
    r: &mut R,
    offset: u64,
    n: usize,
) -> std::io::Result<Box<[T]>> {
    let mut value = <[T]>::new_box_zeroed_with_elems(n).unwrap();
    r.read_exact_at(value.as_mut_bytes(), offset)?;
    Ok(value)
}

pub fn write_at<W: Write + Seek>(w: &mut W, pos: u64, data: &[u8]) -> std::io::Result<()> {
    w.seek(SeekFrom::Start(pos))?;
    w.write_all(data)
}

```

`pdb/src/utils/iter.rs`:

```rs
//! Iterator utilities

use std::collections::BTreeMap;

/// Reads a slice of items and groups them using a function over the items.
pub fn group_by<'a, T, F, K>(s: &'a [T], f: F) -> BTreeMap<K, Vec<&'a T>>
where
    F: Fn(&T) -> K,
    K: Ord + Eq,
{
    let mut out: BTreeMap<K, Vec<&'a T>> = BTreeMap::new();

    for item in s.iter() {
        let key = f(item);
        if let Some(list) = out.get_mut(&key) {
            list.push(item);
        } else {
            out.insert(key, vec![item]);
        }
    }

    out
}

/// Reads a sequence of items and groups them using a function over the items.
pub fn group_by_iter_ref<'a, T, F, I, K>(iter: I, f: F) -> BTreeMap<K, Vec<&'a T>>
where
    I: Iterator<Item = &'a T>,
    F: Fn(&T) -> K,
    K: Ord + Eq,
{
    let mut out: BTreeMap<K, Vec<&'a T>> = BTreeMap::new();

    for item in iter {
        let key = f(item);
        if let Some(list) = out.get_mut(&key) {
            list.push(item);
        } else {
            out.insert(key, vec![item]);
        }
    }

    out
}

/// Reads a sequence of items and groups them using a function over the items.
pub fn group_by_iter<I, F, K>(iter: I, f: F) -> BTreeMap<K, Vec<I::Item>>
where
    I: Iterator,
    F: Fn(&I::Item) -> K,
    K: Ord + Eq,
{
    let mut out: BTreeMap<K, Vec<I::Item>> = BTreeMap::new();

    for item in iter {
        let key = f(&item);
        if let Some(list) = out.get_mut(&key) {
            list.push(item);
        } else {
            out.insert(key, vec![item]);
        }
    }

    out
}

```

`pdb/src/utils/path.rs`:

```rs
//! Utilities for working with filesystem paths

use std::path::Path;

/// Tests whether `container_path` is equal to `nested_path` or is an ancestor of `nested_path`.
pub fn path_contains(container_path: &str, nested_path: &str) -> bool {
    let c_path = Path::new(container_path);
    let n_path = Path::new(nested_path);

    if c_path.is_absolute() != n_path.is_absolute() {
        return false;
    }

    let mut ci = c_path.components();
    let mut ni = n_path.components();

    loop {
        match (ci.next(), ni.next()) {
            (Some(ce), Some(ne)) => {
                // Ignore case, because Windows.
                if !ce.as_os_str().eq_ignore_ascii_case(ne.as_os_str()) {
                    return false;
                }
            }

            // We ran out of nested elements, but still have more container elements. Not a match.
            (Some(_), None) => return false,

            // We ran out of container elements, so it's a match.
            (None, _) => return true,
        }
    }
}

#[test]
#[cfg(windows)]
fn test_path_contains() {
    assert!(!path_contains(r"d:\src", r"foo.c"));

    assert!(path_contains(r"d:\src", r"d:\src\foo.c"));
    assert!(path_contains(r"d:\src", r"D:\SRC\\foo.c"));
    assert!(path_contains(r"d:\src\", r"d:\src"));
    assert!(path_contains(r"d:\src", r"d:\src\"));

    // negative cases
    assert!(!path_contains(r"d:\src", r"e:\src\foo.c"));
    assert!(!path_contains(r"d:\src", r"d:\bar"));
}

```

`pdb/src/utils/swizzle.rs`:

```rs
//! Traits and support for in-place byte-order swizzling

/// Defines the behavior of converting from the host byte order to specific external byte orders
/// (LE and BE).
pub trait Swizzle {
    /// Converts values within this value from LE order to host order.
    /// On LE architectures, this does nothing.
    fn le_to_host(&mut self);
}

macro_rules! int_swizzle {
    ($t:ty) => {
        impl Swizzle for $t {
            fn le_to_host(&mut self) {
                if cfg!(target_endian = "big") {
                    *self = Self::from_le(*self);
                }
            }
        }
    };
}

int_swizzle!(u16);
int_swizzle!(u32);
int_swizzle!(u64);
int_swizzle!(u128);

int_swizzle!(i16);
int_swizzle!(i32);
int_swizzle!(i64);
int_swizzle!(i128);

impl Swizzle for u8 {
    fn le_to_host(&mut self) {}
}

impl Swizzle for i8 {
    fn le_to_host(&mut self) {}
}

impl<T: Swizzle> Swizzle for [T] {
    fn le_to_host(&mut self) {
        for i in self.iter_mut() {
            i.le_to_host();
        }
    }
}

```

`pdb/src/utils/vec.rs`:

```rs
//! Utilities for `Vec`

use std::cmp::Ordering;

/// Replace a range of values in a vector with a new range. The old and new ranges can be
/// different sizes.
pub fn replace_range_copy<T: Copy>(v: &mut Vec<T>, start: usize, old_len: usize, values: &[T]) {
    assert!(start <= v.len());
    assert!(old_len <= v.len() - start);

    match values.len().cmp(&old_len) {
        Ordering::Equal => {
            v[start..start + values.len()].copy_from_slice(values);
        }

        Ordering::Less => {
            // The new values are shorter than the old values.
            // Copy the overlap, then drain the remainder.
            v[start..start + values.len()].copy_from_slice(values);
            v.drain(start + values.len()..start + old_len);
        }

        Ordering::Greater => {
            // Copy the overlapping values.
            // Then append the other values.
            // Then rotate them into position.
            let (lo, hi) = values.split_at(old_len);
            v.extend_from_slice(hi);
            v[start..start + old_len].copy_from_slice(lo);
            v[start + old_len..].rotate_right(lo.len());
        }
    }
}

```

`pdb/src/writer.rs`:

```rs
//! Utilties for copying data between streams.

use std::io::{Read, Write};

/// Copies all data from `Src` to `Dst`
pub fn copy_stream_with_buffer<Dst, Src>(
    mut dst: Dst,
    mut src: Src,
    buffer: &mut [u8],
) -> std::io::Result<()>
where
    Dst: Write,
    Src: Read,
{
    loop {
        let n = src.read(buffer)?;
        if n == 0 {
            break;
        }
        dst.write_all(&buffer[..n])?;
    }

    Ok(())
}

/// Copies all data from `Src` to `Dst`
pub fn copy_stream<Dst, Src>(dst: Dst, src: Src) -> std::io::Result<()>
where
    Dst: Write,
    Src: Read,
{
    const BUFFER_LEN: usize = 16 << 20; // 16 MiB

    let mut buffer = vec![0; BUFFER_LEN];
    copy_stream_with_buffer(dst, src, &mut buffer)
}

```

`pdb/tests/cpp_check.rs`:

```rs
//! This integration test runs the MSVC compiler and linker to generate complete executables and
//! and PDBs, and then reads the PDBs and verifies that they contain the expected information.

#![cfg(windows)]
#![allow(clippy::single_match)]
#![allow(clippy::useless_vec)]

use bstr::BStr;
use ms_pdb::Pdb;
use ms_pdb::syms::{Data, SymData, SymKind};
use ms_pdb::types::fields::Field;
use ms_pdb::types::{TypeData, TypeIndex};
use std::collections::HashMap;
use std::path::Path;
use std::process::Command;
use tracing::{error, info, trace};

const CARGO_MANIFEST_DIR: &str = env!("CARGO_MANIFEST_DIR");
const CARGO_TARGET_TMPDIR: &str = env!("CARGO_TARGET_TMPDIR");

#[static_init::dynamic]
static INIT_LOGGER: () = {
    tracing_subscriber::fmt()
        .with_ansi(false)
        .with_test_writer()
        .with_file(true)
        .with_line_number(true)
        .with_max_level(tracing::Level::DEBUG)
        .compact()
        .without_time()
        .finish();
};

// id should be the base name of a C++ source file in the "cpp_check" directory.
// e.g. id == `types`
fn run_test(id: &str) -> Box<Pdb> {
    info!("case: {id}");

    let cargo_manifest_dir = Path::new(CARGO_MANIFEST_DIR);
    let cargo_target_tmpdir = Path::new(CARGO_TARGET_TMPDIR);
    let cases_tmpdir = cargo_target_tmpdir.join("cases");

    let cases_dir = Path::new(cargo_manifest_dir)
        .join("tests")
        .join("cpp_check");
    let source_file_name = format!("{id}.cpp");
    let source_file_path = cases_dir.join(&source_file_name);

    let this_output_dir = cases_tmpdir.join(id);

    info!("source file: {}", source_file_name);
    info!("output dir: {}", this_output_dir.display());

    let dll_file_name = format!("{id}.dll");
    let pdb_file_name = format!("{id}.pdb");

    let dll_path = this_output_dir.join(dll_file_name);
    let pdb_path = this_output_dir.join(pdb_file_name);

    info!("target: {}", dll_path.display());
    info!("pdb:    {}", pdb_path.display());

    std::fs::create_dir_all(&this_output_dir).unwrap();

    let obj_file_path = this_output_dir.join(format!("{id}.obj"));

    {
        let mut cmd = Command::new("cl.exe");
        cmd.current_dir(&this_output_dir);
        cmd.arg("/nologo");
        cmd.arg("/Z7");
        cmd.arg("/c");
        cmd.arg("/O2");
        cmd.arg(source_file_path);
        cmd.arg(format!(
            "/Fo{}",
            obj_file_path.as_os_str().to_string_lossy()
        ));

        let mut child = cmd.spawn().unwrap();
        let child_exit = child.wait().unwrap();
        assert!(child_exit.success(), "cl.exe failed");
    }

    {
        let mut cmd = Command::new("link.exe");
        cmd.current_dir(&this_output_dir);
        cmd.arg("/nologo");
        cmd.arg("/dll");
        cmd.arg("/debug:full");
        cmd.arg(format!("/out:{}", dll_path.display()));
        cmd.arg(format!("/pdb:{}", pdb_path.display()));
        cmd.arg(format!("{}", obj_file_path.as_os_str().to_string_lossy()));

        let mut child = cmd.spawn().unwrap();
        let child_exit = child.wait().unwrap();
        assert!(child_exit.success(), "link.exe failed");
    }

    Pdb::open(&pdb_path).unwrap()
}

#[test]
#[ignore] // Ignored for now because MSVC is not installed in GitHub actions
fn types() -> anyhow::Result<()> {
    let pdb = run_test("types");

    let gss = pdb.read_gss()?;
    let gsi = pdb.read_gsi()?;

    let get_global = |kind: SymKind, name: &str| -> SymData {
        let sym = gsi
            .find_symbol(&gss, name.into())
            .expect("expected find_symbol to succeed")
            .unwrap_or_else(|| panic!("expected find_symbol to succeed: {name}"));
        assert_eq!(sym.kind, kind, "expected symbol kind {kind:?} : {name}");
        let sym_data = sym
            .parse()
            .unwrap_or_else(|e| panic!("expected symbol parse to succeed: {name} : {e:?}"));
        sym_data
    };

    let get_global_data = |kind: SymKind, name: &str| -> Data {
        match get_global(kind, name) {
            SymData::Data(d) => d,
            unknown => panic!("expected SymData::Data for {name}, got: {unknown:?}"),
        }
    };

    let names = vec![
        "__acrt_initial_locale_pointers",
        "__xi_a",
        "get_initial_environment",
        "FEOFLAG",
        "StructWithManyEnums",
        "enums_export",
    ];

    // Dump some stuff for fun.
    for name in names.iter() {
        let s = gsi.find_symbol(&gss, BStr::new(name)).unwrap();
        info!("{:?} --> {:?}", name, s);
    }

    let enums_export = gsi.find_symbol(&gss, BStr::new("enums_export"))?.unwrap();
    assert_eq!(enums_export.kind, SymKind::S_PROCREF);

    let type_stream = pdb.read_type_stream()?;

    // Check that primitive types match the values we're expecting.
    {
        let primitives_data = get_global_data(SymKind::S_GDATA32, "g_structWithPrimitiveTypes");
        let primitives_ty_record = type_stream.record(primitives_data.header.type_.get())?;
        let primitives_ty_struct = match primitives_ty_record.parse()? {
            TypeData::Struct(s) => s,
            unknown => panic!("Expected StructWithPrimitiveTypes to be a struct: {unknown:?}"),
        };

        // Index the member (data) fields by name
        let mut fields: HashMap<&BStr, TypeIndex> = HashMap::new();
        for f in type_stream.iter_fields(primitives_ty_struct.fixed.field_list.get()) {
            match f {
                Field::Member(m) => {
                    // Turn this on when adding new fields in types.cpp
                    if false {
                        if m.ty.0 < 0x1000 {
                            println!("  (TypeIndex::{:?}, \"{}\"),", m.ty, m.name);
                        } else {
                            println!("  // non-primitive field: {}", m.name);
                        }
                    }
                    fields.insert(m.name, m.ty);
                }
                _ => {}
            }
        }

        // Validate our expectations
        //
        // TODO: This will fail if the C++ code is compiled for a 32-bit architecture because the
        // pointer types encode the size of the pointer.
        let expectations: &[(TypeIndex, &str)] = &[
            (TypeIndex::T_RCHAR, "f_char"),
            (TypeIndex::T_RCHAR, "f_const_char"),
            (TypeIndex::T_CHAR, "f_signed_char"),
            (TypeIndex::T_UCHAR, "f_unsigned_char"),
            (TypeIndex::T_64PRCHAR, "f_char_ptr"),
            // non-primitive field: f_const_char_ptr
            (TypeIndex::T_64PRCHAR, "f_char_const_ptr"),
            // non-primitive field: f_const_char_const_ptr
            (TypeIndex::T_INT4, "f_int"),
            (TypeIndex::T_INT4, "f_const_int"),
            (TypeIndex::T_INT4, "f_signed_int"),
            (TypeIndex::T_UINT4, "f_unsigned_int"),
            (TypeIndex::T_64PINT4, "f_int_ptr"),
            // non-primitive field: f_const_int_ptr
            (TypeIndex::T_64PINT4, "f_int_const_ptr"),
            // non-primitive field: f_const_int_const_ptr
            (TypeIndex::T_LONG, "f_long"),
            (TypeIndex::T_LONG, "f_const_long"),
            (TypeIndex::T_LONG, "f_signed_long"),
            (TypeIndex::T_ULONG, "f_unsigned_long"),
            (TypeIndex::T_64PLONG, "f_long_ptr"),
            // non-primitive field: f_const_long_ptr
            (TypeIndex::T_64PLONG, "f_long_const_ptr"),
            // non-primitive field: f_const_long_const_ptr
            (TypeIndex::T_SHORT, "f_short"),
            (TypeIndex::T_SHORT, "f_const_short"),
            (TypeIndex::T_SHORT, "f_signed_short"),
            (TypeIndex::T_USHORT, "f_unsigned_short"),
            (TypeIndex::T_64PSHORT, "f_short_ptr"),
            // non-primitive field: f_const_short_ptr
            (TypeIndex::T_64PSHORT, "f_short_const_ptr"),
            // non-primitive field: f_const_short_const_ptr
            (TypeIndex::T_QUAD, "f__long_long"),
            (TypeIndex::T_QUAD, "f_const__long_long"),
            (TypeIndex::T_QUAD, "f_signed__long_long"),
            (TypeIndex::T_UQUAD, "f_unsigned__long_long"),
            (TypeIndex::T_64PQUAD, "f__long_long_ptr"),
            // non-primitive field: f_const__long_long_ptr
            (TypeIndex::T_64PQUAD, "f__long_long_const_ptr"),
            // non-primitive field: f_const__long_long_const_ptr
            (TypeIndex::T_RCHAR, "f_int8"),
            (TypeIndex::T_RCHAR, "f_const_int8"),
            (TypeIndex::T_CHAR, "f_signed_int8"),
            (TypeIndex::T_UCHAR, "f_unsigned_int8"),
            (TypeIndex::T_64PRCHAR, "f_int8_ptr"),
            // non-primitive field: f_const_int8_ptr
            (TypeIndex::T_64PRCHAR, "f_int8_const_ptr"),
            // non-primitive field: f_const_int8_const_ptr
            (TypeIndex::T_SHORT, "f_int16"),
            (TypeIndex::T_SHORT, "f_const_int16"),
            (TypeIndex::T_SHORT, "f_signed_int16"),
            (TypeIndex::T_USHORT, "f_unsigned_int16"),
            (TypeIndex::T_64PSHORT, "f_int16_ptr"),
            // non-primitive field: f_const_int16_ptr
            (TypeIndex::T_64PSHORT, "f_int16_const_ptr"),
            // non-primitive field: f_const_int16_const_ptr
            (TypeIndex::T_INT4, "f_int32"),
            (TypeIndex::T_INT4, "f_const_int32"),
            (TypeIndex::T_INT4, "f_signed_int32"),
            (TypeIndex::T_UINT4, "f_unsigned_int32"),
            (TypeIndex::T_64PINT4, "f_int32_ptr"),
            // non-primitive field: f_const_int32_ptr
            (TypeIndex::T_64PINT4, "f_int32_const_ptr"),
            // non-primitive field: f_const_int32_const_ptr
            (TypeIndex::T_QUAD, "f_int64"),
            (TypeIndex::T_QUAD, "f_const_int64"),
            (TypeIndex::T_QUAD, "f_signed_int64"),
            (TypeIndex::T_UQUAD, "f_unsigned_int64"),
            (TypeIndex::T_64PQUAD, "f_int64_ptr"),
            // non-primitive field: f_const_int64_ptr
            (TypeIndex::T_64PQUAD, "f_int64_const_ptr"),
            // non-primitive field: f_const_int64_const_ptr
            (TypeIndex::T_BOOL8, "f_bool"),
            (TypeIndex::T_64PBOOL08, "f_bool_ptr"),
            (TypeIndex::T_64PVOID, "f_void_ptr"),
            (TypeIndex::T_BOOL8, "f_bool"),
            (TypeIndex::T_64PBOOL08, "f_bool_ptr"),
            (TypeIndex::T_REAL32, "f_float"),
            (TypeIndex::T_64PREAL32, "f_float_ptr"),
            (TypeIndex::T_REAL64, "f_double"),
            (TypeIndex::T_64PREAL64, "f_double_ptr"),
        ];

        let mut error = false;
        for &(expected_type, name) in expectations.iter() {
            if let Some(&actual_type) = fields.get(BStr::new(name)) {
                if expected_type == actual_type {
                    trace!("field has correct type: {expected_type:?} - {name}");
                } else {
                    error!(
                        "expected field {name} to have type {expected_type:?}, but it had type {actual_type:?}"
                    );
                    error = true;
                }
            } else {
                error!("did not find field: {name}");
                error = true;
            }
        }

        assert!(!error, "Found one or more fields with the wrong type");
    }

    // Find an S_LPROCREF symbol.
    {
        let _gf = match get_global(SymKind::S_PROCREF, "global_function") {
            SymData::RefSym2(r) => r,
            unknown => panic!("wrong symbol data for global_function: {unknown:?}"),
        };
        // TODO: look up the actual S_GDATA32 symbol and check things about it
    }

    // Find an S_CONSTANT symbol at global scope.
    {
        let c = match get_global(SymKind::S_CONSTANT, "WHAT_IS_SIX_TIMES_SEVEN") {
            SymData::Constant(c) => c,
            unknown => panic!("expected S_CONSTANT, got: {unknown:?}"),
        };
        let value: i32 = c.value.try_into().unwrap();
        assert_eq!(value, 42);
    }

    // Find an S_CONSTANT symbol that is nested within a class.
    {
        let c = match get_global(SymKind::S_CONSTANT, "Zebra::NUMBER_OF_STRIPES") {
            SymData::Constant(c) => c,
            unknown => panic!("expected S_CONSTANT, got: {unknown:?}"),
        };
        let value: i32 = c.value.try_into().unwrap();
        assert_eq!(value, 80);
    }

    // Find an S_CONSTANT symbol that is within nested C++ namespaces.
    {
        let c = match get_global(SymKind::S_CONSTANT, "foo::bar::CONSTANT_INSIDE_NAMESPACE") {
            SymData::Constant(c) => c,
            unknown => panic!("expected S_CONSTANT, got: {unknown:?}"),
        };
        let value: i32 = c.value.try_into().unwrap();
        assert_eq!(value, -333);
    }

    Ok(())
}

```

`pdb/tests/cpp_check/types.cpp`:

```cpp
// #include <stdint.h>

#include <stdlib.h>

enum EnumSimple
{
    Simple_A = 100,
    Simple_B = 200,
};
typedef EnumSimple EnumSimple;

__declspec(dllexport) EnumSimple g_enumSimpleValue = EnumSimple::Simple_B;

enum class EnumClass
{
    A = 100,
    B = 200,
};

enum EnumOverInt : int
{
    EnumOverInt_A = 100,
    EnumOverInt_B = 200,
};

enum class EnumClassOverInt : int
{
    A = 100,
    B = 200,
};

enum class EnumClassOverUInt8 : unsigned __int8
{
    Z = 10,
};

struct StructWithManyEnums
{
    EnumSimple enum_simple;
    EnumClass enum_class;
    EnumOverInt enum_over_int;
    EnumClassOverInt enum_class_over_int;
    EnumClassOverUInt8 enum_class_over_uint8;
};

struct StructWithPrimitiveTypes
{
#define INT_VARIANTS(ty, name)      \
    ty f_##name;                    \
    ty f_const_##name;              \
    signed ty f_signed_##name;      \
    unsigned ty f_unsigned_##name;  \
    ty *f_##name##_ptr;             \
    const ty *f_const_##name##_ptr; \
    ty *f_##name##_const_ptr;       \
    const ty *f_const_##name##_const_ptr;

    INT_VARIANTS(char, char)
    INT_VARIANTS(int, int)
    INT_VARIANTS(long, long)
    INT_VARIANTS(short, short)
    INT_VARIANTS(long long, _long_long)
    INT_VARIANTS(__int8, int8)
    INT_VARIANTS(__int16, int16)
    INT_VARIANTS(__int32, int32)
    INT_VARIANTS(__int64, int64)

#undef INT_VARIANTS

    void *f_void_ptr;

    bool f_bool;
    bool *f_bool_ptr;

    float f_float;
    float *f_float_ptr;
    double f_double;
    double *f_double_ptr;
};

const int WHAT_IS_SIX_TIMES_SEVEN = 42;

class Zebra
{
public:
    static constexpr short NUMBER_OF_STRIPES = 80;
};

namespace foo
{
    namespace bar
    {
        const long long CONSTANT_INSIDE_NAMESPACE = -333;
    }
}


class __declspec(dllexport) ExportedClass
{
public:
    int x_;
    bool live_;

    ExportedClass();
    ExportedClass(int x);
    ExportedClass(const ExportedClass &other);
    ExportedClass(ExportedClass &&other);
    ExportedClass &operator=(const ExportedClass &other);
    ExportedClass &operator=(ExportedClass &&other);
    operator int() const;
    void operator()() const;
};

ExportedClass::ExportedClass() {
    abort();
}

ExportedClass::ExportedClass(int x) : x_(x), live_(true) {}

ExportedClass::ExportedClass(const ExportedClass &other) : x_(other.x_) {}

ExportedClass::ExportedClass(ExportedClass &&other) : x_(other.x_)
{
    if (other.live_)
    {
        live_ = true;
        x_ = other.x_;
        other.live_ = false;
        other.x_ = 0;
    }
    else
    {
        live_ = false;
        x_ = 0;
    }
}

__declspec(dllexport)
    ExportedClass &ExportedClass::operator=(const ExportedClass &)
{
    return *this;
}

__declspec(dllexport)
    ExportedClass &ExportedClass::operator=(ExportedClass &&)
{
    return *this;
}

__declspec(dllexport)
    ExportedClass::operator int() const
{
    return 0;
}

__declspec(dllexport) void ExportedClass::operator()() const
{
    abort();
}

__declspec(dllexport) ExportedClass *newExportedClass()
{
    return new ExportedClass();
}

__declspec(dllexport) StructWithPrimitiveTypes g_structWithPrimitiveTypes;

__declspec(noinline) int global_function()
{
    return 0;
}

__declspec(noinline) extern "C" int global_function_c_linkage()
{
    return WHAT_IS_SIX_TIMES_SEVEN;
}

__declspec(dllexport) void enums_export(StructWithManyEnums *s)
{
    __annotation(L"Hello!", L"World!");

    s->enum_simple = Simple_A;
    s->enum_class = EnumClass::A;
    s->enum_over_int = EnumOverInt_A;
    s->enum_class_over_int = EnumClassOverInt::A;
    s->enum_class_over_uint8 = EnumClassOverUInt8::Z;

    global_function();
    global_function_c_linkage();
}


```

`pdbtool/Cargo.toml`:

```toml
[package]
name = "pdbtool"
version = "0.1.20"
edition = "2024"
description = "A tool for reading Program Database (PDB) files and displaying information about them."
authors = ["Arlie Davis <ardavis@microsoft.com>"]
license = "MIT OR Apache-2.0"
repository = "https://github.com/microsoft/pdb-rs/"
categories = ["parsing", "development-tools", "development-tools::debugging"]

[features]
tracy = ["dep:tracing-tracy"]

[dependencies]
anyhow.workspace = true
bitvec.workspace = true
bstr.workspace = true
bumpalo.workspace = true
dbg-ranges = "0.1.1"
friendly = "0.2"
glob = "0.3.2"
regex = "1.0"
static_init.workspace = true
clap = { workspace = true, features = ["derive"] }
tracing-subscriber = { workspace = true }
tracing-tracy = { workspace = true, optional = true, features = ["enable", "flush-on-exit"] }
tracing = { workspace = true, features = ["release_max_level_debug"] }
zerocopy-derive.workspace = true
zerocopy.workspace = true
zstd.workspace = true

[dependencies.ms-coff]
version = "0.1.0"
path = "../coff"

[dependencies.ms-pdb]
version = "0.1.20"
path = "../pdb"

```

`pdbtool/README.md`:

```md
# Program Database (PDB) tool

This is a simple tool for reading Program Database files. It uses the
[`ms-pdb`](https://crates.io/crates/ms-pdb) crate to read and write PDB.

This tool is published as a separate crate in order to minimize the dependencies of the `ms-pdb`
crate.

## Installation

```batch
cargo install pdbtool
```

## Examples

```
pdbtool dump hello_world.pdb streams
```

produces:

```text
Stream #     0 : (size          0) OldStreamDir
Stream #     1 : (size        665) PDB
Stream #     2 : (size   15341432) TPI
Stream #     3 : (size    1668945) DBI
Stream #     4 : (size    1871280) IPI
Stream #     5 : (size          0) Named { name: "/LinkInfo" }
Stream #     6 : (size          0) Named { name: "/TMCache" }
Stream #     7 : (size     140696) Named { name: "/names" }
[...]
```

Use `pdbtool --help` for a list of commands. Use `pdbtool dump --help` for help with the `dump`
command, which has many subcommands.

## Help

```batch
pdbtool --help
```

```text
Usage: pdbtool [OPTIONS] <COMMAND>

Commands:
  add-src     Adds source file contents to the PDB. The contents are embedded directly within the PDB. WinDbg and Visual Studio can both extract the source files
  copy        Copies a PDB from one file to another. All stream contents are preserved exactly, byte-for-byte. The blocks within streams are laid out sequentially
  test        
  dump        
  save        
  find        Searches the DBI Section Contributions table
  find-name   Searches the TPI Stream for a given type
  counts      Counts the number of records and record sizes for a given set of PDBs
  hexdump     Dumps part of a file (any file, not just a PDB) as a hex dump. If you want to dump a specific stream, then use the `dump <filename> hex` command instead
  pdz-encode  
  help        Print this message or the help of the given subcommand(s)

Options:
      --quiet       Reduce logging to just warnings and errors in `mspdb` and `pdbtool` modules
      --verbose     Turn on debug output in all `mspdb` and `pdbtool` modules. Noisy!
      --timestamps  Show timestamps in log messages
      --tracy       Connect to Tracy (diagnostics tool). Requires that the `tracy` Cargo feature be enabled
  -h, --help        Print help
```

## Help - dump

The `dump` subcommand takes a path to a PDB file as its next argument, followed by a subcommand,
as seen in the example above.

```batch
pdbtool dump --help
```

```text
Usage: pdbtool dump [OPTIONS] <PDB> <COMMAND>

Commands:
  names                
  globals              
  tpi                  Dump the Type Stream (TPI)
  ipi                  Dump the Id Stream (TPI)
  dbi                  Dump DBI header
  dbi-enc              Dump DBI Edit-and-Continue Substream
  dbi-type-server-map  
  gsi                  Global Symbol Index. Loads the GSI and iterates through its hash records. For each one, finds the symbol record in the GSS and displays it
  psi                  Public Symbol Index. Loads the PSI and iterates through its hash records. For each one, finds the symbol record in the GSS and displays it
  modules              
  streams              Dump the Stream Directory
  lines                Dumps C13 Line Data for a given module
  sources              Dump the DBI Stream - Sources substream
  section-map          
  section-contribs     Dump section contributions (quite large!)
  pdbi                 Dump the PDB Info Stream
  module-symbols       Displays the symbols for a specific module
  hex                  Dump the contents of a stream, or a subsection of it, using a hexadecimal dump format. By default, this will only show a portion of the stream; use `--len` to increase it
  help                 Print this message or the help of the given subcommand(s)

Arguments:
  <PDB>  The PDB to dump

Options:
      --lines-like-cvdump  
  -h, --help               Print help
```

## Converting PDB files to Compressed PDB (PDZ/MSFZ)

> **WARNING** **WARNING** **WARNING** **WARNING** **WARNING**
>
> This tool provides an _experimental_ capability for compressing PDBs into a new PDB container
> format, called MSFZ. See the [MSFZ Container Specification](https://github.com/microsoft/pdb-rs/blob/main/msfz/src/msfz.md).
> This container specification is _experimental_ and is not guaranteed to be stable. It may change
> at any time; there is no guarantee of compatibility, stability, or reliability. It is defined
> for the purposes of experimentation.
>
> **WARNING** **WARNING** **WARNING** **WARNING** **WARNING**

MSFZ is intended to reduce development costs, by defining a compression format for PDB, which
still allows tools (such as debuggers) to read from compressed PDBs by decompressing only those
parts of the PDB that are needed. This avoids the need to decompress the entire PDB. See the
specification (linked above) for details.

To convert a PDB file to a compressed PDB:

```batch
pdbtool pdz-encode hello_world.pdb d:\compressed_pdbs\hello_world.pdb
```

This reads the `hello_world.pdb`, compresses its contents, and writes it to
`d:\compressed_pdbs\hello_world.pdb`. Recent versions of the Windows Debugger
(WinDbg) can directly read compressed PDB files. _It cannot be over-emphasized
that this support is experimental, and subject to change without notice._

## Contributing

This project welcomes contributions and suggestions. Most contributions require
you to agree to a Contributor License Agreement (CLA) declaring that you have
the right to, and actually do, grant us the rights to use your contribution. For
details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether
you need to provide a CLA and decorate the PR appropriately (e.g., status check,
comment). Simply follow the instructions provided by the bot. You will only need
to do this once across all repos using our CLA.

This project has adopted the
[Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the
[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any
additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or
services. Authorized use of Microsoft trademarks or logos is subject to and must
follow
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must
not cause confusion or imply Microsoft sponsorship. Any use of third-party
trademarks or logos are subject to those third-party's policies.

## Repository

* <https://github.com/microsoft/pdb-rs>

## Contacts

* `sivadeilra` on GitHub
* Arlie Davis ardavis@microsoft.com


```

`pdbtool/src/addsrc.rs`:

```rs
use anyhow::{Context, Result, bail};
use bstr::ByteSlice;
use ms_pdb::{BStr, Pdb};

#[derive(clap::Parser)]
pub struct AddSrcOptions {
    /// The PDB to modify.
    pub pdb: String,

    /// A list of source files to embed into the PDB.
    pub source_files: Vec<String>,

    /// A list of directories (path prefixes). If this list contains any values, then
    /// this tool will scan the sources table within the PDB and look for any source file
    /// that was compiled and is underneath any path in the `under` list. If so, then the
    /// source file will be read and embedded into the PDB.
    ///
    /// For example: `pdbtool add-src foo.pdb --under=d:\some\dir`
    #[arg(long)]
    pub under: Vec<String>,
}

pub fn command(options: AddSrcOptions) -> Result<()> {
    if options.source_files.is_empty() && options.under.is_empty() {
        bail!("You must specify at least one source file to add to the PDB.");
    }

    let mut pdb = ms_pdb::Pdb::modify(options.pdb.as_ref())?;

    for source_file in options.source_files.iter() {
        let (fake_source_file, real_source_file): (&str, &str) =
            if let Some(s) = source_file.split_once('=') {
                s
            } else {
                (source_file, source_file)
            };

        embed_source_file(&mut pdb, fake_source_file, real_source_file)?;
    }

    if !options.under.is_empty() {
        let sources = pdb.sources()?;

        // Build a list of the unique source files. This should really be moved into Pdb.
        let mut source_files: Vec<(u32, &BStr)> = sources.iter_sources().collect();
        source_files.sort_unstable_by_key(|&(offset, _name)| offset);
        source_files.dedup();

        // Scan each source file.
        for &(_, file_name) in source_files.iter() {
            let Ok(file_name) = file_name.to_str() else {
                // icky file name
                continue;
            };

            for under in options.under.iter() {
                if strip_prefix_ignore_ascii_case(under, file_name).is_some() {
                    println!("prefix matched: {under} : {file_name}");
                }
            }
        }
    }

    pdb.flush_all()?;
    let committed = pdb.msf_mut_err()?.commit()?;
    if committed {
        println!("Changes successfully committed to PDB.");
    } else {
        println!(
            "No changes were written to disk. The given files are already embedded in the PDB."
        );
    }

    Ok(())
}

/// Embeds a source file into the PDB.
///
/// `file_name` specifies the path to the file to embed. This function uses `file_name` to open
/// the file and read its contents.
///
/// `path_within_pdb` specifies the file name to use within the PDB. This can be different from
/// `file_name`. For example, source root directories (or object directories) can be chopped off
/// and replaced with a well-known prefix.
///
/// If there is already a source file embedded in the PDB with the same file name (as specified by `path_within_pdb`), then this will update
/// the existing stream instead of modifying it.
fn embed_source_file(pdb: &mut Pdb, path_within_pdb: &str, file_name: &str) -> Result<()> {
    let file_contents =
        std::fs::read(file_name).with_context(|| format!("Failed to open file: {file_name}"))?;
    if pdb.add_embedded_source(path_within_pdb, &file_contents)? {
        println!("{file_name} : embedded");
    } else {
        println!("{file_name} : already embedded (no change)");
    }

    Ok(())
}

fn strip_prefix_ignore_ascii_case<'a>(prefix: &str, s: &'a str) -> Option<&'a str> {
    if s.is_char_boundary(prefix.len()) {
        let (lo, hi) = s.split_at(prefix.len());
        if prefix.eq_ignore_ascii_case(lo) {
            Some(hi)
        } else {
            None
        }
    } else {
        None
    }
}

```

`pdbtool/src/check.rs`:

```rs
use anyhow::{Context, Result, bail};
use ms_pdb::{Pdb, RandomAccessFile};
use std::path::{Path, PathBuf};
use tracing::{error, info, warn};

/// Checks whether a given PDB is well-formed (not corrupted).
/// Can check more than one PDB at a time.
///
/// The default behavior is to open the PDB and do nothing else. This simulates
/// the most basic behavior of any tool that reads a PDB. Additional checks
/// may be enabled by setting flags.
#[derive(clap::Parser)]
pub(crate) struct CheckOptions {
    /// The files to check.
    pub(crate) files: Vec<String>,

    #[arg(long)]
    pub check_modules: bool,
}

pub(crate) fn command(mut options: CheckOptions) -> Result<()> {
    if options.files.is_empty() {
        bail!("You must specify at least one file name (or file pattern) to check.");
    }

    let mut all_files: Vec<PathBuf> = Vec::with_capacity(options.files.len());

    for file_or_glob in std::mem::take(&mut options.files) {
        if file_or_glob.contains(['*', '?']) {
            let mut found_any = false;

            for file_name in glob::glob(&file_or_glob)
                .with_context(|| format!("File pattern: {file_or_glob}"))?
            {
                let file_name =
                    file_name.with_context(|| format!("File pattern: {file_or_glob}"))?;
                all_files.push(file_name);
                found_any = true;
            }

            if !found_any {
                warn!("File pattern did not match any files: {file_or_glob}");
            }
        } else {
            all_files.push(file_or_glob.into());
        }
    }

    let show_stat = |name: &str, value: u32| {
        info!("{:<40} : {:8}", name, value);
    };

    if all_files.len() > 1 {
        show_stat("Number of PDBs to check", all_files.len() as u32);
    }

    let mut stats = Stats::default();

    for file_name in all_files.iter() {
        stats.num_files_checked += 1;
        check_one(&options, &mut stats, Path::new(file_name));
    }

    info!("Results:");
    show_stat("Number of PDBs checked", stats.num_files_checked);
    show_stat("Number of PDBs with errors", stats.num_files_failed);

    if stats.num_portable_pdbs != 0 {
        show_stat("Number of portable PDBs (ignored)", stats.num_portable_pdbs);
    }
    if stats.num_unknown_files != 0 {
        show_stat(
            "Number of unrecognized files (ignored)",
            stats.num_unknown_files,
        );
    }

    Ok(())
}

#[derive(Default)]
struct Stats {
    pub num_files_checked: u32,
    pub num_files_failed: u32,
    pub num_portable_pdbs: u32,
    pub num_unknown_files: u32,
}

fn check_one(options: &CheckOptions, stats: &mut Stats, file_name: &Path) {
    let mut errors: Vec<String> = Vec::new();

    match check_one_err(options, stats, &mut errors, file_name) {
        Ok(()) => {
            if !errors.is_empty() {
                stats.num_files_failed += 1;

                let mut all_errors_text = String::new();
                for error in errors.iter() {
                    all_errors_text.push_str(error);
                    all_errors_text.push('\n');
                }

                error!("{} : has errors:\n", all_errors_text);
            }
        }
        Err(e) => {
            error!("{} : failed: {:?}", file_name.display(), e);
            stats.num_files_failed += 1;
        }
    }
}

fn check_one_err(
    options: &CheckOptions,
    stats: &mut Stats,
    errors: &mut Vec<String>,
    file_name: &Path,
) -> Result<()> {
    let f = RandomAccessFile::open(file_name)?;

    use ms_pdb::taster::{Flavor, what_flavor};
    match what_flavor(&f)? {
        Some(Flavor::Pdb | Flavor::Pdz) => {}

        Some(Flavor::PortablePdb) => {
            stats.num_portable_pdbs += 1;
            return Ok(());
        }

        None => {
            stats.num_unknown_files += 1;
            return Ok(());
        }
    }

    let pdb = Pdb::open_from_random_file(f)?;

    if options.check_modules {
        let modules = pdb.modules().with_context(|| "failed to get modules")?;
        let sources = pdb.sources().with_context(|| "failed to get sources")?;

        let mod_vec: Vec<_> = modules.iter().collect();

        if mod_vec.len() != sources.num_modules() {
            errors.push(format!("The number of DBI modules is not the same as the number of entries in the DBI Sources map.  {} vs {}", mod_vec.len(), sources.num_modules()));
        }
    }

    Ok(())
}

```

`pdbtool/src/compare.rs`:

```rs
//! Compares two PDB (or PDZ) files to check that their stream directories and
//! stream contents are identical.

use anyhow::{Context, Result, bail};
use ms_pdb::{Pdb, RandomAccessFile};
use std::path::Path;
use tracing::{error, info};

/// Compares two PDB (or PDZ) files and verifies that their stream contents
/// are identical.
#[derive(clap::Parser)]
pub(crate) struct CompareOptions {
    /// The first PDB file or PDZ file to compare.
    pub(crate) first_file: String,

    /// The second PDB file or PDZ file to compare.
    pub(crate) second_file: String,

    /// Continue comparison even after finding differences (don't stop at first difference).
    #[arg(long)]
    pub continue_on_differences: bool,
}

pub(crate) fn command(options: CompareOptions) -> Result<()> {
    let first_path = Path::new(&options.first_file);
    let second_path = Path::new(&options.second_file);

    info!("Comparing files:");
    info!("  First:  {}", first_path.display());
    info!("  Second: {}", second_path.display());

    // Open both files
    let first_pdb = Pdb::open(first_path)
        .with_context(|| format!("Failed to open first file: {}", options.first_file))?;
    let second_pdb = Pdb::open(second_path)
        .with_context(|| format!("Failed to open second file: {}", options.second_file))?;

    let mut comparison_stats = ComparisonStats::default();
    let success = compare_pdbs(&first_pdb, &second_pdb, &options, &mut comparison_stats)?;

    // Print summary
    info!("Comparison completed:");
    info!(
        "  Total streams compared:   {}",
        comparison_stats.streams_compared
    );
    info!(
        "  Streams with differences: {}",
        comparison_stats.streams_different
    );
    info!(
        "  Nil streams matched:      {}",
        comparison_stats.nil_streams_matched
    );

    if !success {
        info!("Result: Files are DIFFERENT");
        bail!("Files are different");
    } else {
        info!("Result: Files are IDENTICAL");
    }

    Ok(())
}

#[derive(Default)]
struct ComparisonStats {
    streams_compared: u32,
    streams_different: u32,
    nil_streams_matched: u32,
}

fn compare_pdbs(
    first_pdb: &Pdb<RandomAccessFile>,
    second_pdb: &Pdb<RandomAccessFile>,
    options: &CompareOptions,
    stats: &mut ComparisonStats,
) -> Result<bool> {
    let first_container = first_pdb.container();
    let second_container = second_pdb.container();

    // Compare number of streams
    let first_num_streams = first_container.num_streams();
    let second_num_streams = second_container.num_streams();

    if first_num_streams != second_num_streams {
        error!(
            "Files have different number of streams. First: {}, Second: {}",
            first_num_streams, second_num_streams
        );
        if !options.continue_on_differences {
            return Ok(false);
        }
    }

    let min_num_streams = first_num_streams.min(second_num_streams);
    let mut has_differences = first_num_streams != second_num_streams;

    let mut first_stream_data: Vec<u8> = Vec::new();
    let mut second_stream_data: Vec<u8> = Vec::new();

    // Compare each stream (starting from stream 1, as stream 0 is special)
    for stream in 1..min_num_streams {
        let first_valid = stream < first_num_streams && first_container.is_stream_valid(stream);
        let second_valid = stream < second_num_streams && second_container.is_stream_valid(stream);

        // Check if both streams have same nil-ness
        if first_valid != second_valid {
            error!(
                "Stream {} has different validity. First: {}, Second: {}",
                stream, first_valid, second_valid
            );
            has_differences = true;
            stats.streams_different += 1;

            if !options.continue_on_differences {
                return Ok(false);
            }
            continue;
        }

        // If both streams are nil, they match
        if !first_valid && !second_valid {
            stats.nil_streams_matched += 1;
            continue;
        }

        // Both streams are valid, compare their contents
        stats.streams_compared += 1;

        // Check stream sizes first
        let first_size = first_container.stream_len(stream);
        let second_size = second_container.stream_len(stream);

        if first_size != second_size {
            error!(
                "Stream {} has different sizes. First: {} bytes, Second: {} bytes",
                stream, first_size, second_size
            );
            has_differences = true;
            stats.streams_different += 1;

            if !options.continue_on_differences {
                return Ok(false);
            }
            continue;
        }

        // Read and compare stream contents
        match compare_stream_contents(
            first_container,
            second_container,
            stream,
            &mut first_stream_data,
            &mut second_stream_data,
        ) {
            Ok(true) => {
                // Streams are identical
            }
            Ok(false) => {
                has_differences = true;
                stats.streams_different += 1;

                if !options.continue_on_differences {
                    return Ok(false);
                }
            }
            Err(e) => {
                error!("Failed to compare stream {}: {}", stream, e);
                has_differences = true;
                stats.streams_different += 1;

                if !options.continue_on_differences {
                    return Ok(false);
                }
            }
        }
    }

    Ok(!has_differences)
}

fn compare_stream_contents(
    first_container: &ms_pdb::Container<RandomAccessFile>,
    second_container: &ms_pdb::Container<RandomAccessFile>,
    stream: u32,
    first_stream_data: &mut Vec<u8>,
    second_stream_data: &mut Vec<u8>,
) -> Result<bool> {
    // Read first stream
    first_stream_data.clear();
    first_container.read_stream_to_vec_mut(stream, first_stream_data)?;

    // Read second stream
    second_stream_data.clear();
    second_container.read_stream_to_vec_mut(stream, second_stream_data)?;

    // Find the first different byte
    if let Some(byte_offset) =
        find_index_of_first_different_byte(first_stream_data, second_stream_data)
    {
        error!(
            "Stream {} differs at byte offset {} ({:#x})",
            stream, byte_offset, byte_offset
        );
        return Ok(false);
    }

    Ok(true)
}

/// Compares two byte slices for equality. If they are different, then returns the offset of the
/// first byte that is wrong.
///
/// * Returns `Some(0)` if the byte slices have different lengths.
/// * Returns `Some(i)` if the byte slices have different contents, and `i` is the offset of the
///   first byte that is different.
/// * Returns `None` if the contents are identical.
#[inline(never)]
pub(crate) fn find_index_of_first_different_byte(mut a: &[u8], mut b: &[u8]) -> Option<usize> {
    if a.len() != b.len() {
        return Some(0);
    }

    const BLOCK_SIZE: usize = 4096;

    // This first loop uses memcmp to find the first block which has different contents.
    // memcmp is generally going to be faster than a manually-indexed loop.
    let mut skipped_len: usize = 0;
    loop {
        assert_eq!(a.len(), b.len());
        if a.len() < BLOCK_SIZE {
            break;
        }
        let block_a = &a[..BLOCK_SIZE];
        let block_b = &b[..BLOCK_SIZE];

        if block_a != block_b {
            break;
        }

        a = &a[BLOCK_SIZE..];
        b = &b[BLOCK_SIZE..];
        skipped_len += BLOCK_SIZE;
    }

    // Finish checking the ragged last block -or- repeat the scan of the block which
    // contained different contents.
    for i in 0..a.len() {
        if a[i] != b[i] {
            return Some(skipped_len + i);
        }
    }

    None
}

```

`pdbtool/src/container.rs`:

```rs
use anyhow::Result;
use std::path::Path;

#[derive(clap::Parser)]
pub struct ContainerOptions {
    /// The PDB file or PDZ file to read.
    pdb: String,

    /// Show the chunks table (applicable only to PDZ)
    #[arg(long)]
    chunks: bool,

    /// Show the streams table
    #[arg(long)]
    streams: bool,
}

pub fn container_command(options: &ContainerOptions) -> Result<()> {
    let pdb = ms_pdb::Pdb::open(Path::new(&options.pdb))?;

    let container = pdb.container();
    match container {
        ms_pdb::Container::Msf(msf) => {
            println!("Container format: MSF (uncompressed)");
            println!("  Number of streams:           {:8}", pdb.num_streams());
            println!(
                "  Page size:                   {:8} bytes per page",
                u32::from(msf.page_size())
            );
            println!("  Number of pages:             {:8}", msf.num_total_pages());
            println!("  Number of pages * page size: {:8}", msf.nominal_size());
            println!("  Number of free pages:        {:8}", msf.num_free_pages());
        }

        ms_pdb::Container::Msfz(msfz) => {
            println!("Container format: MSFZ (compressed)");
            println!("  Number of streams:           {:8}", pdb.num_streams());
            println!("  Number of compressed chunks: {:8}", msfz.num_chunks());
            println!("  Number of stream fragments:  {:8}", msfz.num_fragments());

            if options.chunks {
                // Build a mapping of streams to chunks, so we can display it.
                // We need to know, for each chunk, which streams it contains.
                // Contains (chunk, stream) pairs.
                let mut chunks_to_streams: Vec<(u32, u32)> = Vec::new();

                for stream in 1..msfz.num_streams() {
                    if let Some(fragments) = msfz.stream_fragments(stream) {
                        for f in fragments {
                            if f.location.is_compressed() {
                                let chunk = f.location.compressed_first_chunk();
                                chunks_to_streams.push((chunk, stream));
                            }
                        }
                    }
                }

                chunks_to_streams.sort_unstable();

                println!();
                println!("Chunks table:");
                println!();

                println!("Chunk    | File       | Compressed | Uncompressed | Streams");
                println!("         | offset     | size       | size         |");
                println!("---------------------------------------------------------------");

                let mut j: usize = 0; // index into chunks_to_streams

                let mut streams_list = String::new();

                for (chunk_index, chunk) in msfz.chunks().iter().enumerate() {
                    use std::fmt::Write;

                    streams_list.clear();

                    while j < chunks_to_streams.len() && chunks_to_streams[j].0 < chunk_index as u32
                    {
                        j += 1;
                    }

                    // (start, last)
                    let mut current_range: Option<(u32, u32)> = None;

                    while j < chunks_to_streams.len()
                        && chunks_to_streams[j].0 == chunk_index as u32
                    {
                        let stream = chunks_to_streams[j].1;
                        j += 1;

                        match current_range {
                            // Extend the range, if possible
                            Some((range_start, range_last)) if range_last + 1 == stream => {
                                current_range = Some((range_start, stream));
                                continue;
                            }

                            Some((range_start, range_last)) => {
                                // Previous range could not be extended
                                if range_start != range_last {
                                    _ = write!(streams_list, "{range_start}-{range_last} ");
                                } else {
                                    _ = write!(streams_list, "{range_start} ");
                                }
                            }

                            None => {
                                current_range = Some((stream, stream));
                            }
                        }
                    }

                    if let Some((range_start, range_last)) = current_range {
                        if range_start != range_last {
                            _ = write!(streams_list, "{range_start}-{range_last} ");
                        } else {
                            _ = write!(streams_list, "{range_start} ");
                        }
                    }

                    println!(
                        "  {:6} | {:10} | {:10} | {:12} | {}",
                        chunk_index,
                        chunk.file_offset,
                        chunk.compressed_size,
                        chunk.uncompressed_size,
                        streams_list
                    );
                }
            }
        }
    }

    Ok(())
}

```

`pdbtool/src/copy.rs`:

```rs
use ms_pdb::Pdb;
use ms_pdb::msf::{CreateOptions, PageSize};
use std::io::Write;
use std::path::PathBuf;

#[derive(clap::Parser)]
pub struct Options {
    /// The PDB to read.
    source_pdb: PathBuf,

    /// The PDB to write.
    dest_pdb: PathBuf,

    /// The size in bytes of the pages to use. Must be a power of two.
    #[arg(long)]
    page_size: Option<u32>,
}

pub fn copy_command(options: &Options) -> anyhow::Result<()> {
    let src = Pdb::open(&options.source_pdb)?;
    let mut create_options = CreateOptions::default();

    if let Some(page_size) = options.page_size {
        create_options.page_size = PageSize::try_from(page_size)
            .map_err(|_| anyhow::anyhow!("The page size must be a power of 2."))?;
    }

    let mut dst = ms_pdb::msf::Msf::create(&options.dest_pdb, create_options)?;

    for stream_index in 1..src.num_streams() {
        if src.is_stream_valid(stream_index) {
            let stream_data = src.read_stream_to_vec(stream_index)?;
            let mut s = dst.write_stream(stream_index)?;
            s.write_all(&stream_data)?;
        }
    }

    dst.commit()?;

    Ok(())
}

```

`pdbtool/src/counts.rs`:

```rs
use anyhow::Result;
use ms_pdb::msf::offset_within_page;
use ms_pdb::syms::{SymIter, SymKind};
use ms_pdb::tpi::TypeStreamHeader;
use ms_pdb::types::{Leaf, TypesIter};
use ms_pdb::{Pdb, Stream};
use std::collections::{BTreeMap, HashMap};
use std::io::Read;
use std::mem::size_of;
use zerocopy::{FromZeros, IntoBytes};

/// Counts the number of records and record sizes for a given set of PDBs.
#[derive(clap::Parser)]
pub struct CountsOptions {
    /// The set of PDBs to read.
    #[command(flatten)]
    pdbs: crate::glob_pdbs::PdbList,

    /// Count type records in the Global Symbol Stream.
    #[arg(long)]
    global_symbols: bool,

    /// Count symbol records in the TPI Stream.
    #[arg(long)]
    tpi: bool,

    /// Count symbol records in the IPI Stream.
    #[arg(long)]
    ipi: bool,

    /// Count symbol records in each module symbol stream.
    #[arg(long)]
    module_symbols: bool,
}

#[derive(Default)]
struct Counts {
    ipi: TypeStreamCounts,
    tpi: TypeStreamCounts,
    module_sym_counts: HashMap<SymKind, PerRecord>,

    module_sym_sizes: Vec<(u64, u32)>, // (byte_size, module_index)
    global_syms_counts: HashMap<SymKind, PerRecord>,

    num_pdbs_failed: u32,

    sc: StreamCounts,
}

#[derive(Default)]
struct StreamCounts {
    total_file_size: u64,

    tpi: u64,
    tpi_hash: u64,
    ipi: u64,
    ipi_hash: u64,
    gsi: u64,
    psi: u64,

    named: BTreeMap<String, u64>,

    dbi: u64,
    dbi_contribs: u64,
    dbi_modules: u64,
    dbi_sources: u64,

    pdbi: u64,

    /// Size in bytes of all module streams combined
    modules: u64,

    modules_c13_lines: u64,
    modules_syms: u64,

    /// Size in bytes of GSS
    gss: u64,

    old_stream_dir: u64,

    /// Fragmentation in the last page of streams.
    stream_frag: u64,

    /// Number of bytes in pages that are free.
    free_pages_bytes: u64,
}

#[derive(Default)]
struct TypeStreamCounts {
    records: HashMap<Leaf, PerRecord>,
}

pub fn counts_command(options: CountsOptions) -> Result<()> {
    let mut counts = Counts::default();

    for file_name in options.pdbs.get_paths()? {
        match ms_pdb::Pdb::open(&file_name) {
            Ok(pdb) => match count_one_pdb(&options, &pdb, &mut counts) {
                Ok(()) => {}
                Err(e) => {
                    eprintln!(
                        "Error occurred while processing PDB: {}\n  {}",
                        file_name.display(),
                        e
                    );
                    counts.num_pdbs_failed += 1;
                }
            },
            Err(e) => {
                eprintln!("Failed to open {} : {}", file_name.display(), e);
                counts.num_pdbs_failed += 1;
            }
        }
    }

    show_counts(&mut counts);

    Ok(())
}

// Count records in global symbol stream
fn count_global_symbols(pdb: &Pdb, counts: &mut Counts) -> anyhow::Result<()> {
    let global_syms_stream = pdb.dbi_header().sym_record_stream()?;
    let global_syms_stream_data = pdb.read_stream_to_vec(global_syms_stream)?;
    count_sym_records(&global_syms_stream_data, &mut counts.global_syms_counts);
    Ok(())
}

fn count_one_pdb(options: &CountsOptions, pdb: &Pdb, counts: &mut Counts) -> Result<()> {
    if options.tpi {
        read_and_count_type_records(pdb, &mut counts.tpi, Stream::TPI)?;
    }
    if options.ipi {
        read_and_count_type_records(pdb, &mut counts.ipi, Stream::IPI)?;
    }

    if let Some(msf) = pdb.msf() {
        // TODO: Do something smarter for PDZ.
        counts.sc.total_file_size += msf.nominal_size();
    }

    let modules_substream = pdb.read_modules()?;
    for (module_index, module) in modules_substream.iter().enumerate() {
        if let Some(module_stream) = module.stream() {
            if options.module_symbols {
                let module_sym_size: u64;
                if let Some(modi) = pdb.read_module_stream(&module)? {
                    count_sym_records(modi.sym_data()?, &mut counts.module_sym_counts);
                    module_sym_size = modi.sym_byte_size as u64;
                } else {
                    module_sym_size = 0;
                }
                counts
                    .module_sym_sizes
                    .push((module_sym_size, module_index as u32));
            }

            // counts.module_infos.push(module);
            counts.sc.modules += pdb.stream_len(module_stream);
            counts.sc.modules_c13_lines += module.header().c13_byte_size.get() as u64;
            counts.sc.modules_syms += module.header().sym_byte_size.get() as u64;
        }
    }

    counts.sc.dbi += pdb.stream_len(Stream::DBI.into());
    counts.sc.dbi_contribs += pdb.dbi_header().section_contribution_size.get() as u64;
    counts.sc.dbi_modules += pdb.dbi_header().mod_info_size.get() as u64;
    counts.sc.dbi_sources += pdb.dbi_header().source_info_size.get() as u64;

    counts.sc.pdbi += pdb.stream_len(Stream::PDB.into());

    // TPI
    {
        let tpi_len = pdb.stream_len(Stream::TPI.into());
        counts.sc.tpi += tpi_len;
        if tpi_len as usize >= size_of::<TypeStreamHeader>() {
            let mut header = TypeStreamHeader::new_zeroed();
            let mut reader = pdb.get_stream_reader(Stream::TPI.into())?;
            reader.read_exact(header.as_mut_bytes())?;

            if let Some(s) = header.hash_aux_stream_index.get() {
                // TODO: yes, yes, I know, it's the wrong stream count
                counts.sc.tpi_hash += pdb.stream_len(s);
            }

            if let Some(s) = header.hash_stream_index.get() {
                counts.sc.tpi_hash += pdb.stream_len(s);
            }
        }
    }

    // IPI
    {
        let ipi_len = pdb.stream_len(Stream::IPI.into());
        counts.sc.ipi += ipi_len;
        if ipi_len as usize >= size_of::<TypeStreamHeader>() {
            let mut header = TypeStreamHeader::new_zeroed();
            let mut reader = pdb.get_stream_reader(Stream::IPI.into())?;
            reader.read_exact(header.as_mut_bytes())?;

            if let Some(s) = header.hash_aux_stream_index.get() {
                // TODO: yes, yes, I know, it's the wrong stream count
                counts.sc.ipi_hash += pdb.stream_len(s);
            }

            if let Some(s) = header.hash_stream_index.get() {
                counts.sc.ipi_hash += pdb.stream_len(s);
            }
        }
    }

    if let Ok(gss) = pdb.dbi_header().sym_record_stream() {
        counts.sc.gss += pdb.stream_len(gss);
    }

    if let Ok(gsi) = pdb.dbi_header().global_stream_index() {
        counts.sc.gsi += pdb.stream_len(gsi);
    }

    if let Ok(psi) = pdb.dbi_header().public_stream_index() {
        counts.sc.psi += pdb.stream_len(psi);
    }

    if options.global_symbols {
        count_global_symbols(pdb, counts)?;
    }

    for (name, stream) in pdb.named_streams().iter() {
        let stream_len = pdb.stream_len(*stream);

        let name = name.to_ascii_lowercase();

        let chopped_name: &str = if name.ends_with(".cs") {
            "*.cs"
        } else if name.ends_with(".cpp") || name.ends_with(".CPP") {
            "*.cpp"
        } else if name.ends_with(".natvis") {
            "*.natvis"
        } else if name.ends_with(".xaml") {
            "*.xaml"
        } else {
            &name
        };

        if let Some(slot) = counts.sc.named.get_mut(chopped_name) {
            *slot += stream_len;
        } else {
            counts.sc.named.insert(chopped_name.to_string(), stream_len);
        }
    }

    counts.sc.old_stream_dir += pdb.stream_len(Stream::OLD_STREAM_DIR.into());

    // Count the space wasted due to fragmentation in the final page of streams.
    if let Some(msf) = pdb.msf() {
        let page_size = msf.page_size();
        for i in 1..msf.num_streams() {
            let stream_size_bytes = msf.stream_size(i);
            let stream_size_phase = offset_within_page(stream_size_bytes, page_size);
            if stream_size_phase != 0 {
                counts.sc.stream_frag += (u32::from(page_size) - stream_size_phase) as u64;
            }
        }

        let num_free_pages = msf.num_free_pages();
        counts.sc.free_pages_bytes += (num_free_pages as u64) << page_size.exponent();
    }

    Ok(())
}

#[derive(Default, Clone)]
struct PerRecord {
    count: u32,
    bytes: u32,
}

fn read_and_count_type_records(
    pdb: &Pdb,
    counts: &mut TypeStreamCounts,
    stream: Stream,
) -> Result<()> {
    let tpi_type_stream = pdb.read_tpi_or_ipi_stream(stream)?;
    count_type_records(tpi_type_stream.type_records_bytes(), counts);
    Ok(())
}

fn count_type_records(type_records: &[u8], counts: &mut TypeStreamCounts) {
    for type_record in TypesIter::new(type_records) {
        let per_record = counts.records.entry(type_record.kind).or_default();
        per_record.count += 1;
        per_record.bytes += type_record.data.len() as u32 + 4; // 4 for the header
    }
}

fn count_sym_records(sym_records: &[u8], counts: &mut HashMap<SymKind, PerRecord>) {
    for sym_record in SymIter::new(sym_records) {
        let per_record = counts.entry(sym_record.kind).or_default();
        per_record.count += 1;
        per_record.bytes += sym_record.data.len() as u32 + 4; // 4 for the header
    }
}

fn dump_type_counts_map(record_counts: &TypeStreamCounts) {
    let mut record_counts_vec: Vec<(Leaf, PerRecord)> = record_counts
        .records
        .iter()
        .map(|(&kind, per_record)| (kind, per_record.clone()))
        .collect();
    record_counts_vec.sort_unstable_by_key(|&(key, _)| key);

    println!("    {:>8}  {:>12}", "records", "bytes");
    println!("    {:>8}  {:>12}", "-------", "-----");
    let mut total_count = 0;
    let mut total_bytes = 0;
    for &(kind, ref per_record) in record_counts_vec.iter() {
        let raw_kind = kind.0;
        let count = per_record.count;
        let bytes = per_record.bytes;
        println!("    {count:8}  {bytes:12} : [{raw_kind:04x}] {kind:?}");
        total_count += count;
        total_bytes += bytes;
    }

    println!("    {total_count:8}  {total_bytes:12} : (total)");
}

fn dump_sym_counts_map(record_counts: &HashMap<SymKind, PerRecord>) {
    let mut record_counts_vec: Vec<(SymKind, PerRecord)> = record_counts
        .iter()
        .map(|(&kind, per_record)| (kind, per_record.clone()))
        .collect();
    record_counts_vec.sort_unstable_by_key(|&(key, _)| key);

    println!("    {:>8}  {:>12}", "records", "bytes");
    println!("    {:>8}  {:>12}", "-------", "-----");
    let mut total_count = 0;
    let mut total_bytes = 0;
    for &(kind, ref per_record) in record_counts_vec.iter() {
        let raw_kind = kind.0;
        let count = per_record.count;
        let bytes = per_record.bytes;
        println!("    {count:8}  {bytes:12} : [{raw_kind:04x}] {kind:?}");
        total_count += count;
        total_bytes += bytes;
    }
    println!("    {total_count:8}  {total_bytes:12} : (total)");
}

fn show_counts(counts: &mut Counts) {
    println!("TPI Stream:");
    dump_type_counts_map(&counts.tpi);

    println!();

    println!("IPI Stream:");
    dump_type_counts_map(&counts.ipi);

    let module_sym_counts = &counts.module_sym_counts;

    println!("Record counts for module symbols (all modules):");
    dump_sym_counts_map(module_sym_counts);

    println!();

    counts
        .module_sym_sizes
        .sort_unstable_by_key(|(size, _)| *size);

    println!();

    println!("Record counts for Global Symbol Stream:");
    dump_sym_counts_map(&counts.global_syms_counts);

    println!();

    if !counts.module_sym_sizes.is_empty() {
        println!();

        let module_sym_size_percentile5 =
            counts.module_sym_sizes[counts.module_sym_sizes.len() * 5 / 100].0;
        let module_sym_size_median = counts.module_sym_sizes[counts.module_sym_sizes.len() / 2].0;
        let module_sym_size_percentile95 =
            counts.module_sym_sizes[counts.module_sym_sizes.len() * 95 / 100].0;
        println!("Number of modules: {}", counts.module_sym_sizes.len());
        println!("Module symbol stream sizes:");
        println!("    percentile  5%  : {module_sym_size_percentile5:8}");
        println!("    percentile 50%  : {module_sym_size_median:8}");
        println!("    percentile 95%  : {module_sym_size_percentile95:8}");
        println!();
    }

    println!();
    println!(
        "Total size of all PDB files:       {}",
        friendly::bytes(counts.sc.total_file_size)
    );
    let pct = |size: u64| -> Percent { Percent(size as f64, counts.sc.total_file_size as f64) };

    let sc = &counts.sc;
    let total_named_stream_size: u64 = sc.named.values().copied().sum();

    let accounted_bytes = sc.tpi
        + sc.tpi_hash
        + sc.ipi
        + sc.ipi_hash
        + sc.gsi
        + sc.psi
        + total_named_stream_size
        + sc.pdbi
        + sc.modules
        + sc.gss
        + sc.old_stream_dir
        + sc.stream_frag
        + sc.free_pages_bytes;

    let unaccounted_bytes = sc.total_file_size - accounted_bytes;

    let show_one = |name: &str, size: u64| {
        println!(
            "    {:-20}     : {}, {}",
            name,
            friendly::bytes(size),
            pct(size)
        );
    };

    let show_level2 = |name: &str, size: u64| {
        println!(
            "        {:-20} : {}, {}",
            name,
            friendly::bytes(size),
            pct(size)
        );
    };

    show_one("PDBI streams", sc.pdbi);

    show_one("DBI streams", sc.dbi);
    show_level2("DBI Contribs", sc.dbi_contribs);
    show_level2("DBI Modules", sc.dbi_modules);
    show_level2("DBI Sources", sc.dbi_sources);

    show_one("TPI streams", sc.tpi);
    show_one("TPI hash streams", sc.tpi_hash);
    show_one("IPI streams", sc.ipi);
    show_one("IPI hash streams", sc.ipi_hash);

    show_one("Module streams", sc.modules);
    show_level2("Module symbols", sc.modules_syms);
    show_level2("Module line data", sc.modules_c13_lines);

    show_one("GSS streams", sc.gss);
    show_one("GSI streams", sc.gsi);
    show_one("PSI streams", sc.psi);

    show_one("Named streams", total_named_stream_size);
    for (name, count) in sc.named.iter() {
        show_level2(name, *count);
    }

    show_one("Old Stream Dir", sc.old_stream_dir);
    show_one("Page fragmentation", sc.stream_frag);
    show_one("Free pages", sc.free_pages_bytes);
    show_one("Unaccounted", unaccounted_bytes);
}

struct Percent(pub f64, pub f64);

impl std::fmt::Display for Percent {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        if self.1 > 0.0 {
            let pct = self.0 / self.1 * 100.0;
            write!(f, "{pct:2.1} %")
        } else {
            write!(f, "n/a")
        }
    }
}

```

`pdbtool/src/dump.rs`:

```rs
use crate::dump_utils::{HexDump, HexStr};
use anyhow::Result;
use bstr::BStr;
use bstr::ByteSlice;
use ms_pdb::codeview::IteratorWithRangesExt;
use ms_pdb::codeview::parser::Parser;
use ms_pdb::dbi::optional_dbg::OptionalDebugHeaderStream;
use ms_pdb::dbi::{DbiSourcesSubstream, DbiStream, ModuleInfo};
use ms_pdb::names::NamesStream;
use ms_pdb::syms::{OffsetSegment, SymData, SymIter, SymKind};
use ms_pdb::tpi::TypeStreamKind;
use ms_pdb::types::TypeIndex;
use ms_pdb::{Pdb, Stream};
use std::borrow::Cow;
use std::fmt::Write;
use std::ops::Range;
use std::path::Path;
use tracing::error;

use self::sym::DumpSymsContext;
use self::types::dump_type_index_short;

mod lines;
mod names;
mod sections;
mod sources;
mod streams;
pub mod sym;
mod types;

#[derive(clap::Parser)]
pub struct DumpOptions {
    /// The PDB to dump
    pub pdb: String,

    #[arg(long)]
    pub lines_like_cvdump: bool,

    #[command(subcommand)]
    pub subcommand: Subcommand,
}

#[derive(clap::Subcommand)]
pub enum Subcommand {
    Names(names::DumpNamesOptions),
    /// Dump global symbol stream (GSS)
    Globals {
        max: Option<usize>,
        skip: Option<usize>,
        /// Show types
        #[arg(short, long)]
        types: bool,
    },
    /// Dump the Type Stream (TPI)
    Tpi(types::DumpTypeStreamOptions),
    /// Dump the Id Stream (TPI)
    Ipi(types::DumpTypeStreamOptions),

    /// Dump DBI header
    Dbi(DbiOptions),

    /// Dump fixups, which are located in the DBI stream.
    Fixups,

    /// Dump DBI Edit-and-Continue Substream
    DbiEnc,

    DbiTypeServerMap,

    /// Global Symbol Index. Loads the GSI and iterates through its hash records. For each one,
    /// finds the symbol record in the GSS and displays it.
    Gsi,
    /// Public Symbol Index. Loads the PSI and iterates through its hash records. For each one,
    /// finds the symbol record in the GSS and displays it.
    Psi,

    Modules(ModulesOptions),

    /// Dump the Stream Directory.
    Streams(streams::StreamsOptions),

    Lines(lines::LinesOptions),

    /// Dump the DBI Stream - Sources substream
    Sources(sources::SourcesOptions),

    SectionMap,

    /// Dump section contributions (quite large!)
    SectionContribs,

    /// Dump the PDB Info Stream
    Pdbi,

    ModuleSymbols(sym::DumpModuleSymbols),

    /// Dump the contents of a stream, or a subsection of it, using a hexadecimal dump format.
    /// By default, this will only show a portion of the stream; use `--len` to increase it.
    Hex {
        stream: String,
        #[arg(long)]
        offset: Option<String>,
        #[arg(long)]
        len: Option<String>,
    },

    /// Dumps the COFF groups, which are contiguous segments within a section.
    /// For example, `.text$mn` is a COFF group within the `.text` section.
    CoffGroups,

    /// Dumps the COFF section headers. This information comes from an Optional Debug Stream.
    Sections(sections::DumpSectionsOptions),
}

#[derive(clap::Parser)]
pub struct DbiOptions {
    /// Show the "Optional Debug Headers" substream
    #[arg(long)]
    pub optional_dbg: bool,
}

#[derive(clap::Parser)]
pub struct ModulesOptions {
    /// Filter results to those where the object name matches this regex.
    #[arg(long)]
    pub obj: Option<String>,

    /// Display a specific module by module number. This value is zero-based.
    pub module: Option<u32>,

    /// Show source file paths for each module.
    #[arg(long)]
    pub sources: bool,

    /// Show information about the tool (compiler, etc.) that created each module.
    #[arg(long)]
    pub tool: bool,
}

pub fn dump_main(options: DumpOptions) -> anyhow::Result<()> {
    let p = ms_pdb::Pdb::open(Path::new(&options.pdb))?;

    let dbi_stream = p.read_dbi_stream()?;

    if options.lines_like_cvdump {
        lines::dump_lines_like_cvdump(&p, &dbi_stream)?;
    }

    match options.subcommand {
        Subcommand::Globals { skip, max, types } => {
            sym::dump_globals(&p, skip, max, false, types)?;
        }

        Subcommand::Names(args) => {
            names::dump_names(&p, args)?;
        }

        Subcommand::Dbi(opts) => {
            dump_dbi(&p, opts)?;
        }

        Subcommand::Fixups => dump_fixups(&p)?,

        Subcommand::DbiEnc => {
            let enc = dbi_stream.edit_and_continue();
            println!("Edit-and-Continue substream:");
            if enc.is_empty() {
                println!("(empty)");
            } else {
                println!("{}", HexDump::new(enc).max(0x1000));
                let enc_names = NamesStream::parse(enc)?;
                for (range, name) in enc_names.iter().with_ranges() {
                    println!("[{:08x}] {}", range.start, name);
                }
            }
        }

        Subcommand::DbiTypeServerMap => {
            let tsm = dbi_stream.type_server_map();
            if tsm.is_empty() {
                println!("(empty)");
            } else {
                println!("{:?}", HexDump::new(tsm).max(0x1000));
            }
        }

        Subcommand::Pdbi => dump_pdbi(&p)?,

        Subcommand::ModuleSymbols(args) => sym::dump_module_symbols(&p, args)?,

        Subcommand::Tpi(opts) => {
            let arch = p.arch()?;
            let type_stream = p.read_type_stream()?;
            let id_stream = p.read_ipi_stream()?;
            let type_dump_syms_context = DumpSymsContext::new(arch, &type_stream, &id_stream);
            types::dump_type_stream(
                TypeStreamKind::TPI,
                &type_stream,
                &mut |out, t| dump_type_index_short(out, &type_dump_syms_context, t),
                &mut |_out, _id| Ok(()),
                None,
                &opts,
            )?;
        }

        Subcommand::Ipi(opts) => {
            let arch = p.arch()?;
            let type_stream = p.read_type_stream()?;
            let id_stream = p.read_ipi_stream()?;
            let type_dump_syms_context = DumpSymsContext::new(arch, &type_stream, &id_stream);
            let id_dump_syms_context = DumpSymsContext::new(arch, &id_stream, &id_stream); // TODO: not even remotely right

            let names = p.names()?;
            types::dump_type_stream(
                TypeStreamKind::IPI,
                &id_stream,
                &mut |out, t| dump_type_index_short(out, &type_dump_syms_context, t),
                &mut |out, id| dump_type_index_short(out, &id_dump_syms_context, TypeIndex(id)),
                Some(names),
                &opts,
            )?;
        }
        Subcommand::Gsi => sym::dump_gsi(&p)?,
        Subcommand::Psi => sym::dump_psi(&p)?,
        Subcommand::Lines(args) => lines::dump_lines(args, &p, &dbi_stream)?,
        Subcommand::Streams(args) => streams::dump_streams(&p, args)?,
        Subcommand::Modules(args) => dump_modules(&p, &dbi_stream, args)?,
        Subcommand::Sources(args) => sources::dump_dbi_sources(&dbi_stream, args)?,
        Subcommand::SectionContribs => sections::dump_section_contribs(&p, &dbi_stream)?,
        Subcommand::SectionMap => dump_section_map(&p, &dbi_stream)?,

        Subcommand::Hex {
            stream,
            offset,
            len,
        } => {
            let mut offset = if let Some(offset_str) = &offset {
                str_to_u32(offset_str)? as usize
            } else {
                0
            };

            let mut len = if let Some(len_str) = &len {
                str_to_u32(len_str)? as usize
            } else {
                0x200
            };

            let (stream_index, stream_range_opt) = crate::save::get_stream_index(&p, &stream)?;
            let stream_data = p.read_stream_to_vec(stream_index)?;

            if let Some(r) = stream_range_opt {
                println!("range = {r:?}");
                len = len.min(r.len());
                offset += r.start;
            };

            if let Some(bytes) = stream_data.get(offset..) {
                println!("{:?}", HexDump::new(bytes).max(len).at(offset).header(true));
            } else {
                println!("Offset 0x{offset:x} ({offset}) is out of range for the stream size.");
                println!("Stream length: 0x{len:x} ({len}).", len = stream_data.len());
            }
        }

        Subcommand::CoffGroups => sections::dump_coff_groups(&p)?,
        Subcommand::Sections(opts) => sections::dump_sections(&p, opts)?,
    }

    Ok(())
}

fn dump_pdbi(pdb: &Pdb) -> Result<()> {
    let pdbi = pdb.pdbi();

    let binding_key = pdbi.binding_key();
    println!("PDBI version: 0x{0:08x}  {0}", pdbi.version());
    println!();
    println!("Binding key:");
    println!("    Unique ID: {}", binding_key.guid.braced());
    println!("    Age: {}", binding_key.age);
    println!(
        "    symsrv file.ptr path: {:?}{}",
        HexStr::new(binding_key.guid.as_bytes()).packed(),
        binding_key.age.wrapping_sub(1)
    );

    Ok(())
}

fn dump_modules(pdb: &Pdb, dbi: &DbiStream, args: ModulesOptions) -> Result<()> {
    let modules = dbi.modules();

    let mut num_modules: u32 = 0;

    let obj_rx = if let Some(obj_filter) = args.obj.as_ref() {
        Some(regex::bytes::Regex::new(obj_filter)?)
    } else {
        None
    };

    let modules_records_start = dbi.substreams.modules_bytes.start;

    let sources = pdb.sources()?;
    let mut source_file_names: Vec<&BStr> = Vec::new();

    for (module_index, (module_record_range, module)) in modules.iter().with_ranges().enumerate() {
        if let Some(mi) = args.module {
            if module_index != mi as usize {
                continue;
            }
        }

        if let Some(obj_rx) = &obj_rx {
            if !obj_rx.is_match(module.obj_file()) {
                continue;
            }
        }

        println!("Module #{} : {}", module_index, module.module_name());
        println!(
            "    [{:08x} .. {:08x}] Module Info record in DBI Stream",
            modules_records_start + module_record_range.start,
            modules_records_start + module_record_range.end
        );
        println!("    {}", module.obj_file());
        if let Some(stream) = module.stream() {
            println!("    Stream: {stream}");
            let sym_start = 0;
            let sym_byte_size = module.header().sym_byte_size.get();
            let sym_end = sym_byte_size;
            let c11_byte_size = module.header().c11_byte_size.get();
            let c11_start = sym_byte_size;
            let c11_end = sym_end + c11_byte_size;
            let c13_byte_size = module.header().c13_byte_size.get();
            let c13_start = sym_byte_size + c11_byte_size;
            let c13_end = c13_start + c13_byte_size;
            println!("        [{sym_start:08x} .. {sym_end:08x}] module symbols");
            if c11_byte_size != 0 {
                println!("        [{c11_start:08x} .. {c11_end:08x}] c11 line data");
            }
            if c13_byte_size != 0 {
                println!("        [{c13_start:08x} .. {c13_end:08x}] c13 line data");
            }
            let sym_stream_len = pdb.stream_len(stream);
            if sym_stream_len > c13_byte_size as u64 {
                println!("        [{c13_end:08x} .. {sym_stream_len:08x}] global refs");
            }
        } else {
            println!("    Stream: (none)");
        }
        println!();

        if args.sources {
            let name_offsets = sources.name_offsets_for_module(module_index)?;

            println!("    Sources:");
            if !name_offsets.is_empty() {
                for &name_offset in name_offsets.iter() {
                    if let Ok(source_file_name) = sources.get_source_file_name_at(name_offset.get())
                    {
                        source_file_names.push(source_file_name);
                    }
                }

                source_file_names.sort_unstable();

                for name in source_file_names.iter() {
                    println!("        {name}");
                }
                source_file_names.clear();
            } else {
                println!("        (none)");
            }
            println!();
        }

        if args.tool {
            if let Some(module_stream) = pdb.read_module_stream(&module)? {
                let mut found = false;
                let sym_data = module_stream.sym_data()?;
                for sym in SymIter::new(sym_data) {
                    match sym.kind {
                        SymKind::S_COMPILE => {
                            println!("    Tool: uses obsolete S_COMPILE, not supported");
                            found = true;
                            break;
                        }

                        SymKind::S_COMPILE2 => {
                            println!("    Tool: uses obsolete S_COMPILE2, not supported");
                            found = true;
                            break;
                        }

                        SymKind::S_COMPILE3 => {
                            match sym.parse()? {
                                SymData::Compile3(compile) => {
                                    println!("    Tool:");
                                    println!("        Name:               {}", compile.name);
                                    println!(
                                        "        Front-end version:  {}.{}, build {}, qfe {}",
                                        compile.fixed.frontend_major.get(),
                                        compile.fixed.frontend_minor.get(),
                                        compile.fixed.frontend_build.get(),
                                        compile.fixed.frontend_qfe.get()
                                    );
                                    println!(
                                        "        Version:            {}.{}, build {}, qfe {}",
                                        compile.fixed.ver_major.get(),
                                        compile.fixed.ver_minor.get(),
                                        compile.fixed.ver_build.get(),
                                        compile.fixed.ver_qfe.get()
                                    );
                                }

                                _ => {}
                            }
                            found = true;
                            break;
                        }

                        _ => {}
                    }
                }

                if !found {
                    println!("    Tool: (unavailable)");
                }
            } else {
                println!("    Tool: (unavailable)");
            }
            println!();
        }

        num_modules += 1;
    }

    println!("Number of modules found: {num_modules}");

    Ok(())
}

fn dump_section_map(_p: &Pdb, dbi_stream: &DbiStream) -> Result<()> {
    use ms_pdb::dbi::section_map::SectionMapEntryFlags;

    let section_map = dbi_stream.section_map()?;

    println!(
        "Number of entries in section map: {}",
        section_map.entries.len()
    );

    for (i, entry) in section_map.entries.iter().enumerate() {
        println!(
            "  {:6} : section_name {:04x}, class_name {:04x}, offset {:08x}, length {:08x}, flags {:04x} {:?}",
            i,
            entry.section_name.get(),
            entry.class_name.get(),
            entry.offset.get(),
            entry.section_length.get(),
            entry.flags.get(),
            SectionMapEntryFlags::from_bits_truncate(entry.flags.get())
        );
    }

    Ok(())
}

fn str_to_u32(s: &str) -> anyhow::Result<u32> {
    if let Some(after) = s.strip_prefix("0x") {
        Ok(u32::from_str_radix(after, 16)?)
    } else {
        Ok(s.parse()?)
    }
}

fn dump_dbi(pdb: &Pdb, options: DbiOptions) -> Result<()> {
    let header = pdb.dbi_header();

    println!("Signature: 0x{:08x}", header.signature.get());
    println!(
        "Version:   0x{version:08x}  {version}",
        version = header.version.get()
    );
    println!("Age:       0x{age:08x}  {age}", age = header.age.get());
    println!();
    println!("Global Symbols:");
    println!(
        "    Global Symbol Stream (GSS):  {:?}",
        header.global_symbol_stream.get()
    );
    println!(
        "    Global Symbol Index Stream (GSI): {:?}",
        header.global_symbol_index_stream.get()
    );
    println!(
        "    Public Symbol Index Stream (PSI): {:?}",
        header.public_symbol_index_stream.get()
    );

    println!("Substreams:");

    let subs = pdb.dbi_substreams();

    let show_sub = |range: &Range<usize>, name: &str| {
        println!(
            "    [{:08x} .. {:08x}] size 0x{:08x} : {name}",
            range.start,
            range.end,
            range.len()
        );
    };

    show_sub(&subs.modules_bytes, "Modules");
    show_sub(&subs.section_contributions_bytes, "Section Contributions");
    show_sub(&subs.section_map_bytes, "Section Map");
    show_sub(&subs.source_info, "Sources");
    show_sub(&subs.type_server_map, "Type Server Map");
    show_sub(&subs.optional_debug_header_bytes, "Optional Debug Headers");
    show_sub(&subs.edit_and_continue, "Edit-and-Continue");

    if options.optional_dbg {
        println!();
        println!("Optional debug header streams:");

        let opt_streams = pdb.optional_debug_streams()?;

        if !opt_streams.streams.is_empty() {
            for (i, stream) in opt_streams.iter() {
                let name = i.name().unwrap_or("???");
                if stream >= pdb.num_streams() {
                    println!("error: stream '{name}' out of range: {stream}");
                    continue;
                }
                let stream_len = pdb.stream_len(stream);

                println!("    {name:<20} : stream {stream:6}, len {stream_len:6}");
            }
        } else {
            println!("    (none)");
        }
    }

    Ok(())
}

fn dump_fixups(pdb: &Pdb) -> Result<()> {
    let Some(fixups) = pdb.read_fixups()? else {
        println!("This PDB does not contain fixups.");
        return Ok(());
    };

    println!("Fixups:");
    println!();
    println!("i        type        extra      rva   target");
    println!("-------- -------- -------- -------- --------");

    let machine = pdb.machine();

    let mut fixup_type_string: String = String::new();

    for (i, fixup) in fixups.iter().enumerate() {
        let fixup_type_str: &str =
            if let Some(s) = ms_coff::reloc_type_str_short(machine, fixup.fixup_type) {
                s
            } else {
                fixup_type_string.clear();
                _ = write!(fixup_type_string, "{:04x}", fixup.fixup_type);
                fixup_type_string.as_str()
            };

        println!(
            "[{i:6}] {fixup_type_str:<8} {:8x} {:8x} {:8x}",
            fixup.extra, fixup.rva, fixup.rva_target
        );
    }

    Ok(())
}

```

`pdbtool/src/dump/lines.rs`:

```rs
use super::*;
use crate::dump_utils::HexStr;
use ms_pdb::lines::{FileChecksum, FileChecksumsSubsection, LinesSubsection, SubsectionKind};
use std::collections::HashMap;

/// Dumps C13 Line Data for a given module.
#[derive(clap::Parser)]
pub struct LinesOptions {
    /// The module index of the module to dump.
    #[arg(long)]
    pub module: Option<usize>,
}

fn dump_module_lines(
    pdb: &Pdb,
    module_index: usize,
    module: &ModuleInfo,
    names: &ms_pdb::names::NamesStream<Vec<u8>>,
    sources: &ms_pdb::dbi::sources::DbiSourcesSubstream,
) -> Result<()> {
    println!("Module: {}", module.module_name());
    println!("    Obj file: {}", module.obj_file());

    if let Some(module_stream) = module.stream() {
        println!("    Stream: {module_stream}",);
    } else {
        println!("    Stream: (none)");
        return Ok(());
    };

    if module.header().c11_byte_size.get() != 0 {
        println!("    *** Module has obsolete C11 line data, which is not supported ***");
        return Ok(());
    }

    if module.header().c13_byte_size.get() == 0 {
        println!("    Module has no line data.");
        return Ok(());
    }

    // unwrap() is ok because we tested module.stream() above.
    let module_stream = pdb.read_module_stream(module)?.unwrap();
    let c13_stream_offset = module_stream.c13_line_data_range().start as u32;
    let c13_line_data = module_stream.c13_line_data();

    let mut checksums: HashMap<u32, FileChecksum<'_>> = HashMap::new();

    if let Some(checksums_subsection) = c13_line_data.find_checksums() {
        for (range, checksum) in checksums_subsection.iter().with_ranges() {
            checksums.insert(range.start as u32, checksum);
        }
    }

    println!();

    let mut iter = c13_line_data.subsections().with_ranges();
    for (subsection_range, subsection) in iter.by_ref() {
        println!(
            "[{:08x}] Subsection: {:?}, len {}",
            c13_stream_offset + subsection_range.start as u32,
            subsection.kind,
            subsection.data.len()
        );

        match subsection.kind {
            SubsectionKind::LINES => {
                let contribution = ms_pdb::lines::LinesSubsection::parse(subsection.data)?;
                println!(
                    "    contribution: offset 0x{:x}, segment {}, size {}",
                    contribution.contribution.offset,
                    contribution.contribution.segment,
                    contribution.contribution.size
                );

                for block in contribution.blocks() {
                    println!(
                        "        block: file {}, num_lines {}",
                        block.header.file_index, block.header.num_lines
                    );
                    if let Some(checksum) = checksums.get(&block.header.file_index.get()) {
                        let name = names.get_string(checksum.name())?;
                        println!("            file: {name}");
                    } else {
                        println!(
                            "            file: unknown: {}",
                            block.header.file_index.get()
                        );
                    }

                    print!("            lines: ");
                    for (i, line) in block.lines().iter().enumerate() {
                        if i != 0 {
                            print!(", ");
                        }

                        let line_num_start = line.line_num_start();
                        if ms_pdb::lines::is_jmc_line(line_num_start) {
                            print!("<no-step>");
                        } else {
                            print!("{line_num_start}");
                        }
                    }
                    println!();
                }
            }

            SubsectionKind::FILE_CHECKSUMS => {
                let name_offsets_for_module = if module_index < sources.num_modules() {
                    sources.name_offsets_for_module(module_index)?
                } else {
                    &[]
                };

                let checksums = ms_pdb::lines::FileChecksumsSubsection {
                    bytes: subsection.data,
                };

                for (i, checksum) in checksums.iter().enumerate() {
                    let name = names.get_string(checksum.name())?;

                    println!(
                        "  checksum: file_offset {:08x}, kind {:?} : {:?} : {name}",
                        checksum.header.name.get(),
                        checksum.header.checksum_kind,
                        HexStr::new(checksum.checksum_data).packed()
                    );

                    if let Some(&name_offset) = name_offsets_for_module.get(i) {
                        let name2 = sources.get_source_file_name_at(name_offset.get())?;
                        if name != name2 {
                            println!("    different name: {name2}");
                        }
                    } else {
                        println!("    index is out of range");
                    }
                }
            }

            _ => {
                println!("{:?}", HexDump::new(subsection.data).max(0x200));
            }
        }

        println!();
    }

    if !iter.inner().rest().is_empty() {
        println!();
        println!("Found unparsed data at the end:");
        println!("{:?}", HexDump::new(iter.inner().rest()).at(iter.pos()));
    }

    Ok(())
}

pub fn dump_lines(options: LinesOptions, p: &Pdb, dbi_stream: &DbiStream<Vec<u8>>) -> Result<()> {
    let names = p.names()?;
    let sources = dbi_stream.sources()?;

    if let Some(module_index) = options.module {
        if let Some(module) = dbi_stream.modules().iter().nth(module_index) {
            dump_module_lines(p, module_index, &module, names, &sources)?;
        } else {
            println!("There is no module with the requested index.");
        }
    } else {
        for (module_index, module) in dbi_stream.modules().iter().enumerate().take(20) {
            dump_module_lines(p, module_index, &module, names, &sources)?;
        }
    }

    Ok(())
}

pub fn dump_lines_like_cvdump(p: &Pdb, dbi_stream: &DbiStream<Vec<u8>>) -> Result<()> {
    let names_stream = p.names()?;

    println!("*** LINES");
    println!();

    for module in dbi_stream.modules().iter() {
        if module.module_name() == module.obj_file() {
            println!("** Module: \"{}\"", module.module_name());
        } else {
            println!(
                "** Module: \"{}\" from \"{}\"",
                module.module_name(),
                module.obj_file()
            );
        }
        println!();

        let Some(module_stream) = p.read_module_stream(&module)? else {
            continue;
        };
        let line_data = module_stream.c13_line_data();

        // Find the Checksums subsection. There should be at most one.
        let checksums_subsection_data = line_data.find_checksums_bytes();
        let checksums = if let Some(chk) = &checksums_subsection_data {
            Some(FileChecksumsSubsection { bytes: chk })
        } else {
            None
        };

        for subsection in line_data.subsections() {
            match subsection.kind {
                SubsectionKind::LINES => {
                    let lines = LinesSubsection::parse(subsection.data)?;
                    let contribution_offset = lines.contribution.offset.get();

                    for block in lines.blocks() {
                        if let Some(checksums) = &checksums {
                            if let Ok(f) = checksums.get_file(block.header.file_index.get()) {
                                let file_name = names_stream.get_string(f.name())?;
                                println!(
                                    "  {file_name} ({:?}: {:?})",
                                    f.header.checksum_kind,
                                    HexStr::new(f.checksum_data).packed()
                                );
                            } else {
                                println!("warning: failed to get file name");
                            }
                            println!();

                            const NUM_COLUMNS: usize = 4;
                            let mut column = 0;

                            for line in block.lines() {
                                print!(
                                    " {:6} {:08X}",
                                    line.line_num_start(),
                                    contribution_offset + line.offset.get()
                                );

                                column += 1;
                                if column == NUM_COLUMNS {
                                    println!();
                                    column = 0;
                                }
                            }
                            if column != 0 {
                                println!();
                            }
                            println!();
                        } else {
                            println!("warning: This module has no file checksums!");
                        }
                    }
                }

                _ => {}
            }
        }
    }

    Ok(())
}

```

`pdbtool/src/dump/names.rs`:

```rs
use super::*;
use ms_pdb::codeview::IteratorWithRangesExt;
use ms_pdb::hash;
use ms_pdb::names::NameIndex;

#[derive(clap::Parser)]
pub struct DumpNamesOptions {
    #[arg(long)]
    max: Option<usize>,

    /// Show hex offsets (NameIndex) for each string.
    #[arg(long)]
    show_offsets: bool,

    /// Show the contents of the name hash table
    #[arg(long)]
    show_hashes: bool,
}

pub fn dump_names(pdb: &Pdb, options: DumpNamesOptions) -> anyhow::Result<()> {
    let names_stream = pdb.names()?;
    let names_stream_index = pdb.named_stream_err(ms_pdb::names::NAMES_STREAM_NAME)?;
    println!("Names Stream Index: {names_stream_index}");

    println!(
        "Number of names in table (as declared in the stream): {:6}",
        names_stream.num_strings
    );
    println!(
        "Number of hash entries:                               {:6}",
        names_stream.num_hashes
    );

    println!();
    println!("Strings:");
    println!();

    for (i, (range, name)) in names_stream.iter().with_ranges().enumerate() {
        if let Some(max) = options.max {
            if i >= max {
                println!("(stopping because we reached max)");
                break;
            }
        }

        if options.show_offsets {
            println!("[{:08x}] {name:?}", range.start);
        } else {
            println!("{name:?}");
        }
    }

    if options.show_hashes {
        println!();
        println!("Hash buckets:");
        println!();

        let hashes = names_stream.hashes();
        let mut num_hashes_good: usize = 0;
        let mut num_hashes_bad: usize = 0;
        let mut num_hashes_unused: usize = 0;
        let mut probing_hash_base: u32 = 0;

        for (i, &ni) in hashes.iter().enumerate() {
            if ni.get() == 0 {
                println!("  hash 0x{i:08x} : none");
                num_hashes_unused += 1;
                probing_hash_base = i as u32 + 1;
                continue;
            }

            let s = names_stream.get_string(NameIndex(ni.get()))?;
            let computed_hash = hash::hash_mod_u32(s, names_stream.num_hashes as u32);
            println!("  hash 0x{i:08x} : computed hash 0x{computed_hash:08x} : {s}");

            let hash_is_good = computed_hash == i as u32
                || (computed_hash >= probing_hash_base && computed_hash < i as u32);
            if hash_is_good {
                num_hashes_good += 1;
            } else {
                num_hashes_bad += 1;
            }
        }

        println!();
        println!("Number of hashes that are correct:    {num_hashes_good:8}");
        println!("Number of hashes that are wrong:      {num_hashes_bad:8}");
        println!("Number of hash slots that are unused: {num_hashes_unused:8}");

        let num_hashes_used = num_hashes_good + num_hashes_bad;
        if num_hashes_used == names_stream.num_strings {
            println!("Number of hashes used is equal to total number of strings (good).");
        } else {
            println!(
                "error: Number of hashes used is {}, which is not equal to the total number of strings ({}).",
                num_hashes_used, names_stream.num_strings
            );
        }
    }
    Ok(())
}

```

`pdbtool/src/dump/sections.rs`:

```rs
use ms_pdb::coff::SectionCharacteristics;
use ms_pdb::dbi::SectionContributionsSubstream;
use ms_pdb::syms::{Data, Proc};
use tracing::warn;
use zerocopy::IntoBytes;

use crate::dump::sym::dump_sym;

use super::*;

pub(crate) fn dump_coff_groups(pdb: &Pdb) -> anyhow::Result<()> {
    let coff_groups = pdb.coff_groups()?;

    println!("COFF groups:");
    println!();

    for (i, group) in coff_groups.vec.iter().enumerate() {
        println!(
            "  [{i:4}]  {off_seg} + {size:08x}, {char:08x} : {name:<16}",
            off_seg = group.offset_segment,
            size = group.size,
            char = group.characteristics,
            name = group.name
        );
    }

    Ok(())
}

static SECTION_DESCRIPTIONS: &[(&str, &str)] = &[
    (".text", "executable code"),
    (".idata", "import tables"),
    (".rdata", "read-only data"),
    (".data", "read-write data"),
    (".pdata", "procedure description tables"),
    (".xdata", "exception unwinding data"),
];

fn section_description(name: &str) -> Option<&'static str> {
    SECTION_DESCRIPTIONS
        .iter()
        .find_map(|&(n, d)| if n == name { Some(d) } else { None })
}

/// Descriptions for COFF groups
///
/// # References
/// * [CRT initialization](https://learn.microsoft.com/en-us/cpp/c-runtime-library/crt-initialization?view=msvc-170)
static COFF_GROUP_DESCRIPTIONS: &[(&str, &str)] = &[
    // Keep these sorted
    (".00cfg", "Control-Flow Guard (CFG)"),
    (".CRT$XCA", "(__xc_a) C++ initializer function list: start"),
    (".CRT$XCAA", "pre-C++ initializers"),
    (".CRT$XCU", "debug code masquerading as CRT code"),
    (".CRT$XCU65534", ""),
    (".CRT$XCU65535", ""),
    (".CRT$XCZ", "(__xc_z) C++ initializer function list: end"),
    (".CRT$XDA", "(__xd_a) TLS initializer function list: start"),
    (".CRT$XDZ", "(__xd_z) TLS initializer function list: end"),
    (".CRT$XIA", "(__xi_a) C initializer function list: start"),
    (".CRT$XIAA", "pre-C initializers"),
    (".CRT$XIAB", "PGO initializers"),
    (".CRT$XIAC", "post-PGO initializers"),
    (".CRT$XIZ", "(__xi_z) C initializer function list: end"),
    (".CRT$XLA", "loader TLS callback list: start"),
    (".CRT$XLB", "(__zl_a) pointer to TLS callback array"),
    (".CRT$XLC", "(__xl_c) TLS initializers"),
    (".CRT$XLD", "(__xl_d) TLS destructors"),
    (".CRT$XLZ", "(__xl_z) loader TLS callback list: end"),
    (".CRT$XPA", "(__xp_a) C pre-terminator list: start"),
    (".CRT$XPTZ65535", ""),
    (".CRT$XPZ", "(__xp_z) C pre-terminator list: end"),
    (".CRT$XTA", "(__xt_a) terminator function list: start"),
    (".CRT$XTZ", "(__xt_z) terminator function list: end"),
    (".edata", ""),
    (".gehcont", "EHCONT guard"),
    (".gehcont$y", "EHCONT guard target"),
    (".gfids$y", "relates to Control Flow Guard (CFG)"),
    (".idata$2", "import descriptors"),
    (".idata$3", "import descriptors null terminator"),
    (".idata$4", "Import Name Table (INT)"),
    (".idata$5", "Import Address Table (IAT)"),
    (".idata$6", "import data strings"),
    (".rdata$CastGuardVftablesA", ""),
    (".rdata$CastGuardVftablesC", ""),
    (".rdata$T", ""),
    (".rdata$r", "RTTI read-only data"),
    (
        ".rdata$voltmd",
        "(__volatile_metadata) volatile metadata for CFG",
    ),
    (".rdata$zzzdbg", "read-only data 'dead' from PGO training"),
    (".rtc$IAA", "run-time checks (RTC) initializer list: start"),
    (".rtc$IZZ", "run-time checks (RTC) initializer list: end"),
    (".rtc$TAA", "run-time checks (RTC) terminator list: start"),
    (".rtc$TZZ", "run-time checks (RTC) terminator list: end"),
    (".text$di", ""),
    (".text$mn", "\"main\" code"),
    (".text$mn$00", ""),
    (".text$unlikely", "code believed to be cold"),
    (".text$x", "exception unwinding funclets (__finally, etc.)"),
    (".text$yd", "dynamic atexit destructors"),
    (".tls", "thread-local storage"),
    (".tls$", "thread-local storage"),
    (".tls$ZZZ", ""),
    (".xdata", ""),
];

#[test]
fn test_coff_group_descriptions_sorted() {
    for w in COFF_GROUP_DESCRIPTIONS.windows(2) {
        assert!(
            w[0].0 < w[1].0,
            "group descriptions should be sorted: {} >= {}",
            w[0].0,
            w[1].0
        );
    }
}

fn coff_group_description(name: &str) -> Option<&'static str> {
    if let Ok(i) = COFF_GROUP_DESCRIPTIONS.binary_search_by(|&(n, _)| n.cmp(name)) {
        Some(COFF_GROUP_DESCRIPTIONS[i].1)
    } else {
        None
    }
}

/// A helper type for showing read/write/execute bits from section characteristics.
struct Rwx(SectionCharacteristics);

impl core::fmt::Display for Rwx {
    fn fmt(&self, f: &mut core::fmt::Formatter) -> core::fmt::Result {
        f.write_char(if self.0.is_read() { 'r' } else { '-' })?;
        f.write_char(if self.0.is_write() { 'w' } else { '-' })?;
        f.write_char(if self.0.is_exec() { 'x' } else { '-' })?;
        Ok(())
    }
}

#[derive(clap::Parser)]
pub struct DumpSectionsOptions {
    /// Show COFF groups (subsections within sections)
    #[arg(long)]
    pub groups: bool,

    /// Show section contributions, nested within sections. If `--groups` is also specified, then
    /// section contributions will be nested within COFF groups.
    #[arg(long)]
    pub contribs: bool,
}

#[derive(PartialOrd, Ord, Eq, PartialEq)]
struct ModuleSymEntry {
    off_seg: OffsetSegment,
    module_index: u16,
    /// The byte offset of this symbol record within the module's symbol stream.
    /// This includes the 4-byte offset, i.e. if this is the very first record in the stream
    /// then `byte_offset == 4`.
    byte_offset: u32,
}

pub(crate) fn dump_sections(pdb: &Pdb, options: DumpSectionsOptions) -> anyhow::Result<()> {
    let section_headers = pdb.section_headers()?;

    let modules = pdb.modules()?;
    let modules: Vec<ModuleInfo<'_>> = modules.iter().collect();

    // Read modules for all symbols.
    let mut module_symbols: Vec<Vec<u32>> = Vec::with_capacity(modules.len());
    for module in modules.iter() {
        let syms = pdb.read_module_symbols(module)?;
        module_symbols.push(syms);
    }

    // Scan the symbols for each module. For those symbols that have a [section:offset],
    // add an entry to this table.
    let mut indexed_module_syms: Vec<ModuleSymEntry> = Vec::new();

    for (module_index, module_syms) in module_symbols.iter().enumerate() {
        let mut syms_iter = SymIter::new(module_syms.as_bytes()).with_ranges();
        syms_iter.inner_mut().skip_module_prefix();

        for (range, sym) in syms_iter {
            let off_seg: OffsetSegment;
            match sym.kind {
                SymKind::S_GPROC32 | SymKind::S_LPROC32 => match sym.parse_as::<Proc>() {
                    Ok(proc) => off_seg = proc.fixed.offset_segment,
                    Err(_) => {
                        warn!(
                            module_index,
                            symbol_offset = range.start,
                            "failed to decode symbol record"
                        );
                        continue;
                    }
                },
                SymKind::S_GDATA32 | SymKind::S_LDATA32 => match sym.parse_as::<Data>() {
                    Ok(proc) => off_seg = proc.header.offset_segment,
                    Err(_) => {
                        warn!(
                            module_index,
                            symbol_offset = range.start,
                            "failed to decode symbol record"
                        );
                        continue;
                    }
                },
                _ => continue,
            }

            indexed_module_syms.push(ModuleSymEntry {
                off_seg,
                byte_offset: range.start as u32,
                module_index: module_index as u16,
            });
        }
    }

    // Sort the indexed symbols by section, then offset
    indexed_module_syms.sort_unstable_by_key(|ms| ms.off_seg.as_u64());

    let mut indexed_module_syms_iter = indexed_module_syms.iter().peekable();

    // The COFF groups should be sorted by offset_segment.
    let coff_groups = if options.groups {
        Some(pdb.coff_groups()?)
    } else {
        None
    };
    let mut groups_iter = coff_groups.map(|groups| groups.vec.iter().peekable());

    // Section contribs should also be sorted by offset_segment.
    let section_contribs = if options.contribs {
        Some(pdb.read_section_contributions()?)
    } else {
        None
    };
    let contribs = if let Some(ref contribs_bytes) = section_contribs {
        SectionContributionsSubstream::parse(contribs_bytes.as_bytes()).ok()
    } else {
        None
    };
    let mut contribs_iter = contribs.map(|c| c.contribs.iter().peekable());

    let tpi = pdb.read_type_stream()?;
    let ipi = pdb.read_ipi_stream()?;
    let mut dump_syms_context = DumpSymsContext::new(pdb.arch()?, &tpi, &ipi);
    dump_syms_context.show_record_offsets = false;

    // (group_offset, group_size) describe the entire section or COFF group (subsection) that we
    // are dumping.
    // section_rva is the rva of the beginning of the entire section, not the group, so you'll need
    // to add group_offset to section_rva to get the group_rva.
    let mut show_contribs_in = |section_num: u16,
                                section_rva: u32,
                                group_offset: u32,
                                group_size: u32| {
        let Some(ref mut contribs_iter) = contribs_iter else {
            return;
        };

        let Some(group_offset_end) = group_offset.checked_add(group_size) else {
            warn!("group offset / size exceed limits");
            return;
        };

        // Walk through the contrib records for this COFF group (which is potentially the entire
        // section). For each contrib record that we find, also search the per-module symbols
        // for symbols that lie within the contribution.

        while let Some(contrib) = contribs_iter.peek() {
            let contrib_section = contrib.section.get();

            // Discard records that will never match. In theory we should never have any such records.
            if contrib_section < section_num {
                _ = contribs_iter.next();
                continue;
            }

            // Check to see if we're done with this section.
            if contrib_section > section_num {
                break;
            }

            let contrib_offset = contrib.offset.get() as u32;
            if contrib_offset < group_offset {
                // Fast forward to the relevant region. As above (with the section index), this
                // should really never happen.
                _ = contribs_iter.next();
                continue;
            }

            // Stop if we hit an offset that is outside of this COFF group (or section).
            if contrib_offset >= group_offset_end {
                break;
            }

            let contrib_size_i32: i32 = contrib.size.get();
            if contrib_size_i32 < 0 {
                // Contributions should never be this large.
                break;
            }
            let contrib_size: u32 = contrib_size_i32 as u32;
            let Some(contrib_offset_end) = contrib_offset.checked_add(contrib_size) else {
                warn!("section contribution record exceeds limits");
                break;
            };

            let contrib_module_index = contrib.module_index.get();

            // Accept this contribution record.
            _ = contribs_iter.next();

            // This should really never occur, but be paranoid and double-check.
            // Section contributions should always contribute to exactly one COFF group;
            // they shouldn't cross COFF group boundaries.
            if contrib_offset_end > group_offset_end {
                warn!("section contribution record extends beyond end of COFF group");
                break;
            }

            let module_name = if let Some(module) = modules.get(contrib_module_index as usize) {
                module.module_name().to_str_lossy()
            } else {
                "??".into()
            };

            println!(
                "c {off_seg} rva: {contrib_rva:08x} + {contrib_size:08x} : module {contrib_module_index} - {module_name}",
                off_seg = OffsetSegment::new(contrib_offset, section_num),
                contrib_rva = section_rva + contrib_offset
            );

            // Next, find any module symbols that are within the contribution record we just found.
            // The symbol records must fall within [contrib_offset..contrib_offset + contrib_size].

            while let Some(ms) = indexed_module_syms_iter.peek() {
                let ms_segment = ms.off_seg.segment();
                if ms_segment < section_num {
                    _ = indexed_module_syms_iter.next();
                    continue;
                }

                if ms_segment > section_num {
                    break;
                }

                let ms_offset = ms.off_seg.offset();
                if ms_offset < contrib_offset {
                    _ = indexed_module_syms_iter.next();
                    continue;
                }

                if ms_offset >= contrib_offset_end {
                    break;
                }

                // println!("    {} related symbol: mod {}", ms.off_seg, ms.module_index);

                // We know that module_index and byte_offset are valid, since we built this table, above.
                let this_module_symbols: &[u32] = &module_symbols[ms.module_index as usize];
                let this_sym_bytes: &[u8] =
                    &this_module_symbols.as_bytes()[ms.byte_offset as usize..];
                let sym = SymIter::one(this_sym_bytes).unwrap();
                let mut sym_text = String::new();
                dump_syms_context.scope_depth = 0;
                if dump_sym(
                    &mut sym_text,
                    &mut dump_syms_context,
                    ms.byte_offset,
                    sym.kind,
                    sym.data,
                )
                .is_ok()
                {
                    println!("    ... {}", sym_text.trim_ascii());
                }

                _ = indexed_module_syms_iter.next();
            }
        }
    };

    for (i, section) in section_headers.iter().enumerate() {
        let section_num = (i + 1) as u16;
        let section_name = section.name();
        let section_rva = section.virtual_address;

        println!(
            "s {off_seg} rva: {section_rva:08x} + {vsize:08x} : {rwx} : {section_name:<8}     {description}",
            vsize = section.physical_address_or_virtual_size,
            off_seg = OffsetSegment::new(0, section_num),
            rwx = Rwx(section.characteristics),
            description = section_description(&section_name.to_str_lossy()).unwrap_or("")
        );

        if let Some(ref mut groups_iter) = groups_iter {
            while let Some(g) = groups_iter.peek() {
                let desc = coff_group_description(&g.name).unwrap_or("");
                if g.offset_segment.segment.get() != section_num {
                    break;
                }
                let group_virtual_address = section
                    .virtual_address
                    .wrapping_add(g.offset_segment.offset());

                println!(
                    "g {off_seg} rva: {group_virtual_address:08x} + {vsize:08x} : {rwx} :     {name:<30}  {desc}",
                    off_seg = g.offset_segment,
                    rwx = Rwx(section.characteristics),
                    vsize = g.size,
                    name = g.name
                );

                // If requested, show contribs in this section.
                show_contribs_in(section_num, section_rva, g.offset_segment.offset(), g.size);

                // Advance the groups iterator
                _ = groups_iter.next();
            }

            println!();
        } else if options.contribs {
            // No COFF groups, so process all contribs in this section.
            show_contribs_in(
                section_num,
                section_rva,
                0,
                section.physical_address_or_virtual_size,
            );
        }
    }

    Ok(())
}

pub(crate) fn dump_section_contribs(
    pdb: &Pdb,
    dbi_stream: &DbiStream<Vec<u8>>,
) -> anyhow::Result<()> {
    let coff_groups = pdb.coff_groups()?;
    let modules = pdb.modules()?;
    let modules: Vec<ModuleInfo<'_>> = modules.iter().collect();

    println!("*** SECTION CONTRIBUTIONS");
    println!();

    println!("  Imod  Address        Size      Characteristics");

    let section_contribs = dbi_stream.section_contributions()?;
    for contrib in section_contribs.contribs.iter() {
        let group_name = if let Some(group) = coff_groups.find_group_at(OffsetSegment::new(
            contrib.offset.get() as u32,
            contrib.section.get(),
        )) {
            &group.name
        } else {
            "--"
        };

        let module_name: Cow<'_, str> =
            if let Some(module) = modules.get(contrib.module_index.get() as usize) {
                module.module_name.to_str_lossy()
            } else {
                Cow::Borrowed("??")
            };
        let module_file_name: &str = if let Some((_, after)) = module_name.rsplit_once(['\\', '/'])
        {
            after
        } else {
            &module_name
        };

        println!(
            "  {module_index:04X} {section:04X}:{offset:08X}  {size:08X}  {characteristics:08X}  {group_name:<20}  mod: {module_file_name}",
            module_index = contrib.module_index.get() + 1,
            section = contrib.section.get(),
            offset = contrib.offset.get(),
            size = contrib.size.get(),
            characteristics = contrib.characteristics.get(),
        );
    }

    Ok(())
}

```

`pdbtool/src/dump/sources.rs`:

```rs
use super::*;

#[derive(clap::Parser)]
pub struct SourcesOptions {
    /// Show all files
    #[arg(long)]
    pub files: bool,

    /// Show all modules and their source files. This is the default, if no other options
    /// are specified.
    #[arg(long)]
    pub modules: bool,

    /// Show one specific file, by index.
    #[arg(long, short)]
    pub file: Option<u32>,

    /// Show indexes (name offsets, etc.).
    #[arg(long)]
    pub indexes: bool,
}

pub fn dump_dbi_sources(
    dbi_stream: &DbiStream<Vec<u8>>,
    mut options: SourcesOptions,
) -> anyhow::Result<()> {
    let sources_substream = DbiSourcesSubstream::parse(dbi_stream.source_info())?;

    if !options.files && !options.modules && options.file.is_none() {
        options.modules = true;
    }

    let mut module_infos: Vec<ModuleInfo> = Vec::new();
    let modules_substream = dbi_stream.modules();
    module_infos.extend(modules_substream.iter());

    let mut file_name_offsets: Vec<u32> = sources_substream
        .file_name_offsets()
        .iter()
        .map(|x| x.get())
        .collect();
    file_name_offsets.sort_unstable();
    file_name_offsets.dedup();

    println!("DBI File Info substream:");
    println!(
        "Number of modules:      {:8}",
        sources_substream.num_modules()
    );
    println!(
        "Number of sources:      {:8} (unique)",
        file_name_offsets.len()
    );
    println!(
        "Number of file offsets: {:8} (not unique)",
        sources_substream.file_name_offsets().len()
    );
    println!();

    if options.files {
        for &name_offset in file_name_offsets.iter() {
            let name = sources_substream.get_source_file_name_at(name_offset)?;
            if options.indexes {
                println!("  [{name_offset:08x}] : {name}");
            } else {
                println!("  {name}");
            }
        }
        println!();
    }

    if let Some(file_index) = options.file {
        if let Some(&offset) = sources_substream
            .file_name_offsets()
            .get(file_index as usize)
        {
            println!("File name offset: 0x{offset:x}");
            let file_name = sources_substream.get_source_file_name_at(offset.get())?;
            println!("{file_name}");
        } else {
            println!(
                "File index {file_index} is out of range. Number of files: {}",
                sources_substream.file_name_offsets().len()
            );
        }
    }

    let num_modules = module_infos.len();
    if num_modules != sources_substream.num_modules() {
        error!("Number of modules is wrong");
    }

    if options.modules {
        for (module_index, module_info) in module_infos.iter().enumerate() {
            if options.indexes {
                println!("Module #{module_index} : {}", module_info.module_name());
            } else {
                println!("Module: {}", module_info.module_name());
            }
            println!("    object: {}", module_info.obj_file());

            for name_offset in sources_substream.name_offsets_for_module(module_index)? {
                match sources_substream.get_source_file_name_at(name_offset.get()) {
                    Ok(name) => {
                        if options.indexes {
                            println!("    [{:08x}] : {}", name_offset.get(), name);
                        } else {
                            println!("    {name}");
                        }
                    }
                    Err(e) => {
                        error!("{}", e);
                    }
                }
            }

            println!();
        }
    }

    Ok(())
}

```

`pdbtool/src/dump/streams.rs`:

```rs
use super::*;
use dbg_ranges::debug_adjacent;

#[derive(Debug)]
#[allow(dead_code)] // dead code analysis ignores Debug impls, but that's why this type exists
enum StreamUsage {
    OldStreamDir, // 0
    PDB,          // 1
    TPI,          // 2
    DBI,          // 3
    IPI,          // 4
    ModuleInfo {
        module_name: String,
        obj_name: String,
    },
    Named {
        name: String,
    },
    GlobalSymbolStream,
    GlobalSymbolIndex,
    PublicSymbolStream,
    OptionalDebugHeader {
        which: usize,
        whichs: Option<OptionalDebugHeaderStream>,
    },
    TypeStreamHashStream {
        parent_stream: Stream,
    },
    TypeStreamAuxHashStream {
        parent_stream: Stream,
    },
}

#[derive(clap::Parser)]
pub struct StreamsOptions {
    /// Show the blocks assigned to each stream
    #[arg(long)]
    pages: bool,

    /// Show only this stream (name or index)
    stream: Option<String>,
}

pub fn dump_streams(p: &Pdb, options: StreamsOptions) -> anyhow::Result<()> {
    let num_streams = p.num_streams();

    let mut streams_usage: Vec<Option<StreamUsage>> =
        (0..num_streams as usize).map(|_| None).collect();

    streams_usage[0] = Some(StreamUsage::OldStreamDir);
    streams_usage[Stream::PDB.index()] = Some(StreamUsage::PDB);
    streams_usage[Stream::TPI.index()] = Some(StreamUsage::TPI);
    streams_usage[Stream::DBI.index()] = Some(StreamUsage::DBI);
    streams_usage[Stream::IPI.index()] = Some(StreamUsage::IPI);

    let mut add_stream_usage = |stream_opt: Option<u32>, usage: StreamUsage| {
        let Some(stream_index) = stream_opt else {
            return;
        };

        if let Some(slot) = streams_usage.get_mut(stream_index as usize) {
            if let Some(existing_usage) = slot.as_ref() {
                error!(
                    "Stream index #{} has conflicting usages.\n  Usage #1: {:?}\n  Usage #2: {:?}",
                    stream_index, existing_usage, usage
                );
            } else {
                *slot = Some(usage);
            }
        } else {
            error!(
                "Stream index #{} is invalid (is out of range).  Usage is invalid: {:?}",
                stream_index, usage
            );
        }
    };

    let dbi = p.read_dbi_stream()?;
    let dbi_header = dbi.header()?;

    add_stream_usage(
        dbi_header.global_stream_index().ok(),
        StreamUsage::GlobalSymbolIndex,
    );
    add_stream_usage(
        dbi_header.sym_record_stream().ok(),
        StreamUsage::GlobalSymbolStream,
    );
    add_stream_usage(
        dbi_header.public_stream_index().ok(),
        StreamUsage::PublicSymbolStream,
    );

    if let Some(tpi_header) = p.tpi_header()?.header() {
        add_stream_usage(
            tpi_header.hash_stream_index.get(),
            StreamUsage::TypeStreamHashStream {
                parent_stream: Stream::TPI,
            },
        );
        add_stream_usage(
            tpi_header.hash_aux_stream_index.get(),
            StreamUsage::TypeStreamAuxHashStream {
                parent_stream: Stream::TPI,
            },
        );
    }

    if let Some(ipi_header) = p.ipi_header()?.header() {
        add_stream_usage(
            ipi_header.hash_stream_index.get(),
            StreamUsage::TypeStreamHashStream {
                parent_stream: Stream::IPI,
            },
        );
        add_stream_usage(
            ipi_header.hash_aux_stream_index.get(),
            StreamUsage::TypeStreamAuxHashStream {
                parent_stream: Stream::IPI,
            },
        );
    }

    let pdb_info = p.pdbi();

    for (name, stream) in pdb_info.named_streams().iter() {
        add_stream_usage(
            Some(*stream),
            StreamUsage::Named {
                name: name.to_string(),
            },
        );
    }

    for module in dbi.modules().iter() {
        add_stream_usage(
            module.stream(),
            StreamUsage::ModuleInfo {
                module_name: module.module_name().to_string(),
                obj_name: module.obj_file().to_string(),
            },
        );
    }

    let optional_debug_header = dbi.optional_debug_header()?;
    for (i, stream) in optional_debug_header.iter_streams() {
        add_stream_usage(
            Some(stream),
            StreamUsage::OptionalDebugHeader {
                which: i,
                whichs: OptionalDebugHeaderStream::try_from(i).ok(),
            },
        );
    }

    let one_stream: Option<u32> = if let Some(stream_name) = &options.stream {
        Some(crate::save::get_stream_index(p, stream_name)?.0)
    } else {
        None
    };

    let mut num_streams_unknown_usage: u32 = 0;

    for (stream_index, usage_opt) in streams_usage.iter().enumerate() {
        let stream_index = stream_index as u32;

        // Filter out streams, if desired.
        if let Some(s) = one_stream {
            if stream_index != s {
                continue;
            }
        }

        let stream_size = p.stream_len(stream_index);
        if let Some(usage) = usage_opt {
            println!("Stream #{stream_index:6} : (size {stream_size:10}) {usage:?}");
            if p.is_stream_valid(stream_index) {
            } else {
                println!("   error: Stream is nil");
            }
        } else {
            if p.is_stream_valid(stream_index) {
                println!("Stream #{stream_index:6} : (size {stream_size:10}) UNKNOWN USAGE");
                num_streams_unknown_usage += 1;
            } else {
                println!("Stream #{stream_index} is nil");
            }
        }

        if options.pages {
            if let Some(msf) = p.msf() {
                let (_stream_len, stream_pages) = msf.stream_size_and_pages(stream_index)?;
                println!("    Pages: {:?}", debug_adjacent(stream_pages));
            }
        }
    }

    if num_streams_unknown_usage != 0 {
        println!("Number of streams with unknown usage: {num_streams_unknown_usage}");
    }

    Ok(())
}

```

`pdbtool/src/dump/sym.rs`:

```rs
use super::*;
use crate::dump_utils::indent;
use anyhow::bail;
use ms_pdb::codeview::arch::{Arch, ArchReg};
use ms_pdb::syms::SymData;
use ms_pdb::tpi::TypeStream;
use ms_pdb::types::ItemId;
use tracing::warn;

pub fn dump_sym(
    out: &mut String,
    context: &mut DumpSymsContext<'_>,
    record_offset: u32,
    kind: SymKind,
    data: &[u8],
) -> anyhow::Result<()> {
    fn item_ref(
        out: &mut dyn std::fmt::Write,
        context: &super::sym::DumpSymsContext,
        item: ItemId,
    ) {
        match super::types::dump_item_short(out, context, item) {
            Ok(()) => {}
            Err(e) => {
                _ = write!(out, "(error: {e:?}");
            }
        }
    }

    fn ty_ref(
        out: &mut dyn std::fmt::Write,
        context: &super::sym::DumpSymsContext,
        type_index: TypeIndex,
    ) {
        match super::types::dump_type_index_short(out, context, type_index) {
            Ok(()) => {}
            Err(e) => {
                _ = write!(out, "(error: {e:?}");
            }
        }
    }

    if context.scope_depth == 0 && kind.starts_scope() {
        writeln!(out)?;
    }

    if context.show_record_offsets {
        write!(out, "{record_offset:08x} : ")?;
    }

    if context.scope_depth > 0 {
        write!(out, "{}", indent(context.scope_depth * 2))?;
    }

    write!(out, "{kind:?}: ")?;

    match SymData::parse(kind, data)? {
        // TODO: S_CALLEES is using a different kind of TypeIndex value.
        // Suppress these for now.
        _ if kind == SymKind::S_CALLEES || kind == SymKind::S_INLINESITE => {
            write!(out, "<unavailable>")?;
        }

        SymData::Pub(pub_data) => {
            write!(
                out,
                "{}, flags: {:08x}, {}",
                pub_data.fixed.offset_segment,
                pub_data.fixed.flags.get(),
                pub_data.name
            )?;
        }

        SymData::Udt(udt_data) => {
            ty_ref(out, context, udt_data.type_);
            write!(out, " {}", udt_data.name)?;
        }

        SymData::Constant(constant_data) => {
            ty_ref(out, context, constant_data.type_);
            write!(out, " {} = {}", constant_data.name, constant_data.value)?;
        }

        SymData::ManagedConstant(constant_data) => {
            write!(out, "Token 0x{:x}", constant_data.token)?;
            write!(out, " {} = {}", constant_data.name, constant_data.value)?;
        }

        SymData::RefSym2(sym_ref) => {
            write!(
                out,
                "({}, {:08x}) {}",
                sym_ref.header.module_index.get(),
                sym_ref.header.symbol_offset.get(),
                sym_ref.name
            )?;
        }

        SymData::Data(data) => {
            write!(out, "{} ", data.header.offset_segment,)?;
            ty_ref(out, context, data.header.type_.get());
            write!(out, " {}", data.name)?;
        }

        SymData::ThreadData(thread_storage) => {
            write!(
                out,
                "{}, Type: 0x{:04X}, {}",
                thread_storage.header.offset_segment,
                thread_storage.header.type_.0,
                thread_storage.name
            )?;
        }

        SymData::ObjName(obj_name) => {
            write!(out, "sig: 0x{:08x} {}", obj_name.signature, obj_name.name)?;
        }

        SymData::Compile3(compile3) => {
            write!(out, "{}", compile3.name)?;
        }

        SymData::Proc(proc) => {
            write!(
                out,
                "{} ..+ 0x{:x}, ",
                proc.fixed.offset_segment, proc.fixed.proc_len
            )?;
            ty_ref(out, context, proc.fixed.proc_type.get());
            write!(out, " {}", proc.name)?;
        }

        SymData::ManagedProc(proc) => {
            write!(out, "Token 0x{:x} {}", proc.fixed.token.get(), proc.name)?;
        }

        SymData::End => {}

        SymData::Unknown => {
            write!(out, "Unknown")?;
        }

        SymData::Annotation(ann) => {
            writeln!(out, "{}", ann.fixed.offset)?;
            for s in ann.iter_strings() {
                writeln!(out, "    {s}")?;
            }
        }

        SymData::FrameProc(_) => {}

        SymData::RegRel(reg_rel) => {
            let reg = reg_rel.fixed.register.get();
            let arch_reg = ArchReg::new(context.arch, reg);
            write!(
                out,
                "{arch_reg} + 0x{offset:x}, ",
                offset = reg_rel.fixed.offset.get()
            )?;
            ty_ref(out, context, reg_rel.fixed.ty.get());
            write!(out, " {}", reg_rel.name)?;
        }

        SymData::Block(block) => {
            write!(out, "length: 0x{:x}", block.fixed.length.get())?;

            if !block.name.is_empty() {
                write!(out, " name: {}", block.name)?;
            }
        }

        SymData::Local(local) => {
            ty_ref(out, context, local.fixed.ty.get());
            write!(out, " {}", local.name)?;
        }

        SymData::DefRange(def_range) => {
            write!(out, " program: {}", def_range.fixed.program.get())?;
        }

        SymData::DefRangeFramePointerRel(def_range) => {
            write!(
                out,
                "bp+ 0x{:x}, {} ..+ 0x{:x}",
                def_range.fixed.offset_to_frame_pointer,
                def_range.fixed.range.start,
                def_range.fixed.range.range_size.get()
            )?;
            if !def_range.gaps.is_empty() {
                write!(out, ", num_gaps: {}", def_range.gaps.len())?;
            }
        }

        SymData::Trampoline(_) => {}

        SymData::UsingNamespace(ns) => {
            write!(out, "using {}", ns.namespace)?;
        }

        SymData::BuildInfo(b) => {
            item_ref(out, context, b.item);
        }

        SymData::InlineSite(site) => {
            item_ref(out, context, site.fixed.inlinee.get());
        }

        SymData::InlineSite2(site) => {
            item_ref(out, context, site.fixed.inlinee.get());
        }

        SymData::InlineSiteEnd => {}

        SymData::DefRangeRegister(r) => {
            let reg = ArchReg::new(context.arch, r.fixed.reg.get());
            write!(out, "register: {reg}")?;
        }

        SymData::DefRangeRegisterRel(r) => {
            write!(
                out,
                "base register: 0x{:x}, base pointer offset: {}",
                r.fixed.base_reg, r.fixed.base_pointer_offset
            )?;
        }

        SymData::DefRangeSubFieldRegister(_) => {}

        SymData::DefRangeFramePointerRelFullScope(r) => {
            write!(out, "frame pointer offset: {}", r.frame_pointer_offset)?;
        }

        SymData::Label(label) => {
            write!(out, "{} : {}", label.fixed.offset_segment, label.name)?;
        }

        SymData::FunctionList(funcs) => {
            if !funcs.funcs.is_empty() {
                writeln!(out)?;
                for f in funcs.funcs.iter() {
                    item_ref(out, context, f.get());
                    writeln!(out)?;
                }
            }
        }

        SymData::FrameCookie(_) => {}

        SymData::CallSiteInfo(site) => {
            write!(out, "{} ", site.offset)?;
            ty_ref(out, context, site.func_type.get());
        }

        SymData::HeapAllocSite(site) => {
            write!(out, "{} ", site.offset)?;
            ty_ref(out, context, site.func_type.get());
        }

        SymData::HotPatchFunc(hp) => {
            write!(out, "0x{:x} : {}", hp.func, hp.name)?;
        }

        SymData::CoffGroup(group) => {
            write!(
                out,
                "{} : {} + {}",
                group.name,
                group.fixed.off_seg,
                group.fixed.cb.get()
            )?;
        }

        SymData::ArmSwitchTable(table) => {
            writeln!(out)?;
            writeln!(
                out,
                "    switch_type:                 {:?}",
                table.switch_type()
            )?;
            writeln!(
                out,
                "    num_entries:                 {}",
                table.num_entries
            )?;
            writeln!(out, "    base location:               {}", table.base())?;
            writeln!(out, "    branch instruction location: {}", table.branch())?;
            writeln!(out, "    jump table location:         {}", table.table())?;
        }

        SymData::Section(section) => {
            write!(
                out,
                "section {:4}, rva {:#08x}, size {:#08x} : {:?}",
                section.fixed.section.get(),
                section.fixed.rva.get(),
                section.fixed.cb.get(),
                section.name
            )?;
        }
    }

    writeln!(out)?;

    if kind.starts_scope() {
        context.scope_depth += 1;
    }

    if kind.ends_scope() {
        if context.scope_depth > 0 {
            context.scope_depth -= 1;
        } else {
            warn!("scope depth is mismatched");
        }
    }

    Ok(())
}

pub struct DumpSymsContext<'a> {
    pub scope_depth: u32,
    pub type_stream: &'a TypeStream<Vec<u8>>,
    pub show_record_offsets: bool,
    pub show_type_index: bool,
    pub ipi: &'a TypeStream<Vec<u8>>,
    pub arch: Arch,
}

impl<'a> DumpSymsContext<'a> {
    pub fn new(
        arch: Arch,
        type_stream: &'a TypeStream<Vec<u8>>,
        ipi: &'a TypeStream<Vec<u8>>,
    ) -> Self {
        Self {
            scope_depth: 0,
            type_stream,
            show_record_offsets: true,
            show_type_index: false,
            ipi,
            arch,
        }
    }
}

pub fn dump_globals(
    p: &Pdb,
    skip_opt: Option<usize>,
    max_opt: Option<usize>,
    show_bytes: bool,
    show_types: bool,
) -> anyhow::Result<()> {
    println!("Global symbols:");
    let arch = p.arch()?;
    let gss = p.gss()?;
    let tpi = p.read_type_stream()?;
    let ipi = p.read_ipi_stream()?;
    dump_symbol_stream(
        arch,
        &tpi,
        &ipi,
        &gss.stream_data,
        skip_opt,
        max_opt,
        0,
        show_bytes,
        show_types,
    )?;
    Ok(())
}

pub fn dump_symbol_stream(
    arch: Arch,
    type_stream: &TypeStream<Vec<u8>>,
    ipi: &TypeStream<Vec<u8>>,
    symbol_records: &[u8],
    skip_opt: Option<usize>,
    max_opt: Option<usize>,
    stream_offset: u32,
    show_bytes: bool,
    show_types: bool,
) -> anyhow::Result<()> {
    let mut iter = SymIter::new(symbol_records).with_ranges();

    // We have to manually decode all the records that we are skipping;
    // there is no index structure.
    if let Some(skip) = skip_opt {
        for _ in 0..skip {
            if iter.next().is_none() {
                break;
            }
        }
    }

    let mut num_found = 0;
    let mut out = String::new();
    let mut context = DumpSymsContext::new(arch, type_stream, ipi);
    context.show_type_index = show_types;

    for (record_range, sym) in iter {
        out.clear();

        dump_sym(
            &mut out,
            &mut context,
            stream_offset + record_range.start as u32,
            sym.kind,
            sym.data,
        )?;
        print!("{out}");

        if show_bytes {
            let record_bytes = &symbol_records[record_range.clone()];
            println!(
                "{:?}",
                HexDump::new(record_bytes).at(stream_offset as usize + record_range.start)
            );
        }

        num_found += 1;
        if let Some(max) = max_opt {
            if num_found >= max {
                break;
            }
        }
    }

    Ok(())
}

/// Displays module symbols, for all modules or for a specific module.
#[derive(clap::Parser, Debug)]
pub struct DumpModuleSymbols {
    /// The module to dump
    pub module_index: Option<u32>,

    /// Skip this many symbol records before beginning the dump.
    #[arg(long)]
    pub skip: Option<usize>,

    /// Stop after this many symbol records have been displayed.
    #[arg(long)]
    pub max: Option<usize>,

    /// Dump the hex bytes of each symbol record.
    #[arg(long)]
    pub bytes: bool,

    /// Show the contents of the Global Refs section.
    #[arg(long)]
    pub global_refs: bool,
}

pub fn dump_module_symbols(pdb: &Pdb, options: DumpModuleSymbols) -> anyhow::Result<()> {
    let dbi = pdb.read_dbi_stream()?;

    let mut found_wanted_module = false;

    let tpi = pdb.read_type_stream()?;
    let ipi = pdb.read_ipi_stream()?;

    println!("Types:");
    println!("    type_index_begin: {:?}", tpi.type_index_begin());
    println!("    type_index_end:   {:?}", tpi.type_index_end());
    println!(
        "    num_types:        {:8}",
        tpi.type_index_end().0 - tpi.type_index_begin().0
    );
    println!("    num_starts:       {:8}", tpi.record_starts().len());

    println!();

    for (module_index, module) in dbi.iter_modules().enumerate() {
        if let Some(wanted_module_index) = options.module_index {
            if wanted_module_index as usize == module_index {
                found_wanted_module = true;
            } else {
                continue;
            }
        }

        println!("Module #{module_index}");
        println!("-------------------");
        println!();

        let Some(module_stream) = pdb.read_module_stream(&module)? else {
            println!("Module does not have a module stream (no symbols for module)");
            continue;
        };

        let arch = pdb.arch()?;

        dump_symbol_stream(
            arch,
            &tpi,
            &ipi,
            module_stream.sym_data()?,
            options.skip,
            options.max,
            4,
            options.bytes,
            false,
        )?;

        println!();

        if options.global_refs {
            println!("Global Refs");
            println!("-----------");
            println!();

            let module_global_refs = module_stream.global_refs()?;
            if !module_global_refs.is_empty() {
                let gss = pdb.gss()?;

                let mut out = String::new();
                let mut context = DumpSymsContext::new(arch, &tpi, &ipi);

                for &global_ref in module_global_refs.iter() {
                    let global_ref = global_ref.get();
                    // global_ref is an index into the GSS

                    let global_sym = gss.get_sym_at(global_ref)?;
                    dump_sym(
                        &mut out,
                        &mut context,
                        global_ref,
                        global_sym.kind,
                        global_sym.data,
                    )?;
                    print!("{out}");
                }
            } else {
                println!("(none)");
            }
        }

        if found_wanted_module {
            break;
        }
    }

    if let Some(wanted_module_index) = options.module_index {
        if !found_wanted_module {
            bail!(
                "Could not find a module with index #{}",
                wanted_module_index
            );
        }
    }

    Ok(())
}

pub fn dump_gsi(p: &Pdb) -> Result<()> {
    let arch = p.arch()?;
    let gsi = p.gsi()?;
    let gss = p.gss()?;
    let tpi = p.read_type_stream()?;
    let ipi = p.read_ipi_stream()?;

    println!("*** GLOBALS");
    println!();

    let mut context = DumpSymsContext::new(arch, &tpi, &ipi);

    let mut out = String::new();
    for sym in gsi.names().iter(gss) {
        out.clear();
        // TODO: show the correct record offset, instead of 0
        dump_sym(&mut out, &mut context, 0, sym.kind, sym.data)?;
        println!("{out}");
    }

    println!();

    Ok(())
}

pub fn dump_psi(p: &Pdb) -> Result<()> {
    let arch = p.arch()?;
    let psi = p.read_psi()?;
    let gss = p.gss()?;
    let tpi = p.read_type_stream()?;
    let ipi = p.read_ipi_stream()?;

    println!("*** PUBLICS");
    println!();

    let mut context = DumpSymsContext::new(arch, &tpi, &ipi);

    let mut out = String::new();
    for sym in psi.names().iter(gss) {
        out.clear();
        // TODO: show the correct record offset, instead of 0
        dump_sym(&mut out, &mut context, 0, sym.kind, sym.data)?;
        println!("{out}");
    }

    println!();

    Ok(())
}

```

`pdbtool/src/dump/types.rs`:

```rs
use super::*;
use ms_pdb::names::NameIndex;
use ms_pdb::tpi::TypeStreamKind;
use ms_pdb::types::fields::Field;
use ms_pdb::types::primitive::dump_primitive_type_index;
use ms_pdb::types::{BUILD_INFO_ARG_NAMES, ItemId, Leaf, TypeData, TypeIndex, UdtProperties};

#[derive(clap::Parser)]
pub struct DumpTypeStreamOptions {
    /// Skip this many type records before beginning the dump.
    #[arg(long)]
    pub skip: Option<usize>,

    /// Stop after this many records have been dumped.
    #[arg(long)]
    pub max: Option<usize>,

    /// Show a hex dump of each type record.
    #[arg(long)]
    pub show_bytes: bool,

    /// Show the value of `TypeIndex` references.
    #[arg(long)]
    pub show_type_indexes: bool,
}

pub fn dump_type_stream(
    type_stream_kind: TypeStreamKind,
    type_stream: &ms_pdb::tpi::TypeStream<Vec<u8>>, // records to decode and display
    dump_type_index: &mut dyn FnMut(&mut dyn std::fmt::Write, TypeIndex) -> anyhow::Result<()>,
    dump_item: &mut dyn FnMut(&mut dyn std::fmt::Write, ItemId) -> anyhow::Result<()>,
    names: Option<&NamesStream<Vec<u8>>>,
    options: &DumpTypeStreamOptions,
) -> anyhow::Result<()> {
    println!("Type Stream");
    println!("-----------");
    println!();

    let Some(header) = type_stream.header() else {
        println!("Stream is empty (no header)");
        return Ok(());
    };

    println!(
        "type_index_begin = 0x{:08x}",
        type_stream.type_index_begin().0
    );
    println!(
        "type_index_end =   0x{:08x}",
        type_stream.type_index_end().0
    );
    println!(
        "Number of types:   0x{n:08x} {n:8}",
        n = type_stream.num_types()
    );

    println!(
        "Number of hash buckets: {n} 0x{n:x}",
        n = header.num_hash_buckets.get()
    );
    println!("Hash key size: {n} 0x{n:x}", n = header.hash_key_size.get());

    println!("{:#?}", type_stream.header());

    let index_prefix = match type_stream_kind {
        TypeStreamKind::TPI => 'T',
        TypeStreamKind::IPI => 'I',
    };

    let type_index_begin = type_stream.type_index_begin();
    let mut iter = type_stream.iter_type_records().with_ranges();
    let mut next_type_index = type_index_begin;

    if let Some(skip) = options.skip {
        // We have to brute-force the iterator, since there is no way to seek to a specific type record.
        for _ in 0..skip {
            let item = iter.next();
            if item.is_none() {
                break;
            }
            next_type_index.0 += 1;
        }
    }

    let mut num_found: usize = 0;

    let mut out = String::new();

    let type_stream_start = type_stream.type_records_range().start;

    for (record_range, ty) in iter {
        out.clear();

        dump_type_record(
            &mut out,
            dump_type_index,
            dump_item,
            index_prefix,
            names,
            record_range.start + type_stream_start,
            next_type_index,
            ty.kind,
            ty.data,
            options,
        )?;

        print!("{out}");

        next_type_index.0 += 1;

        num_found += 1;
        if let Some(max) = options.max {
            if num_found >= max {
                break;
            }
        }
    }

    Ok(())
}

pub fn dump_type_record(
    out: &mut dyn std::fmt::Write,
    ty_ref_in: &mut dyn FnMut(&mut dyn std::fmt::Write, TypeIndex) -> anyhow::Result<()>,
    dump_item: &mut dyn FnMut(&mut dyn std::fmt::Write, ItemId) -> anyhow::Result<()>,
    index_prefix: char,
    names: Option<&NamesStream<Vec<u8>>>,
    record_offset: usize,
    type_index: TypeIndex,
    kind: Leaf,
    data: &[u8],
    options: &DumpTypeStreamOptions,
) -> anyhow::Result<()> {
    let mut ty_ref = |out: &mut dyn std::fmt::Write, ty: TypeIndex| -> anyhow::Result<()> {
        if options.show_type_indexes {
            write!(out, "{ty:?} ")?;
        }

        ty_ref_in(out, ty)
    };

    write!(
        out,
        "[{record_offset:08x}] {index_prefix}#{:08x} [{:04x}] {kind:?} : ",
        type_index.0, kind.0,
    )?;

    let mut p = Parser::new(data);

    fn out_udt_props(out: &mut dyn std::fmt::Write, props: UdtProperties) -> std::fmt::Result {
        if props.fwdref() {
            out.write_str(" fwdref")?;
        }
        Ok(())
    }

    match TypeData::parse(kind, &mut p)? {
        TypeData::Array(t) => {
            ty_ref(out, t.fixed.element_type.get())?;
            write!(out, "[{}]", t.len)?;
        }

        TypeData::Struct(t) => {
            out_udt_props(out, t.fixed.property.get())?;
            write!(out, " {}", t.name)?;
            let field_list = t.fixed.field_list.get();
            if let Some(unique_name) = t.unique_name {
                if unique_name != t.name {
                    write!(out, " (unique: {unique_name})")?;
                }
            }
            if field_list.0 != 0 {
                write!(out, " fields: ")?;
                ty_ref(out, field_list)?;
            }
        }

        TypeData::Enum(t) => {
            out_udt_props(out, t.fixed.property.get())?;
            write!(out, " {}", t.name)?;
            if let Some(unique_name) = t.unique_name {
                if unique_name != t.name {
                    write!(out, " (unique: {unique_name})")?;
                }
            }
        }

        TypeData::Union(t) => {
            out_udt_props(out, t.fixed.property.get())?;
            write!(out, " {}", t.name)?;
            if let Some(unique_name) = t.unique_name {
                if unique_name != t.name {
                    write!(out, " (unique: {unique_name})")?;
                }
            }
        }

        TypeData::Unknown => {
            write!(out, "<UNKNOWN>")?;
        }

        TypeData::Pointer(t) => {
            let attr = t.fixed.attr();
            if attr.r#const() {
                write!(out, "const ")?;
            }
            if attr.volatile() {
                write!(out, "volatile ")?;
            }
            if attr.unaligned() {
                write!(out, "unaligned ")?;
            }
            write!(out, "* ")?;

            ty_ref(out, t.fixed.ty.get())?;
        }

        TypeData::Modifier(t) => {
            if t.is_const() {
                write!(out, "const ")?;
            }
            if t.is_unaligned() {
                write!(out, "unaligned ")?;
            }
            if t.is_volatile() {
                write!(out, "volatile ")?;
            }
            ty_ref(out, t.underlying_type.get())?;
        }

        TypeData::Bitfield(t) => {
            ty_ref(out, t.underlying_type.get())?;
            write!(out, " : {}", t.length)?;
            if t.position != 0 {
                write!(out, " at bit {}", t.position)?;
            }
        }

        TypeData::MemberFunc(t) => {
            write!(out, "    ret: ")?;
            ty_ref(out, t.return_value.get())?;
            write!(out, " class: ")?;
            ty_ref(out, t.class.get())?;
            if t.this.get().0 != 0 {
                write!(out, " this: ")?;
                ty_ref(out, t.this.get())?;
            }
        }

        TypeData::Proc(t) => {
            write!(out, "ret: ")?;
            ty_ref(out, t.return_value.get())?;
            write!(out, " args: ")?;
            ty_ref(out, t.arg_list.get())?;
        }

        TypeData::VTableShape(t) => {
            write!(out, "num_slots: {}", t.count)?;
        }

        TypeData::FieldList(fields) => {
            writeln!(out)?;
            for field in fields.iter() {
                match field {
                    Field::Member(m) => {
                        write!(out, "    at {} : {} ", m.offset, m.name)?;
                        ty_ref(out, m.ty)?;
                        writeln!(out)?;
                    }
                    Field::Enumerate(en) => {
                        writeln!(out, "    {} = {}", en.name, en.value)?;
                    }
                    Field::Method(m) => {
                        writeln!(out, "    {}() - (method group)", m.name)?;
                    }
                    Field::OneMethod(m) => {
                        writeln!(out, "    {}()", m.name)?;
                    }
                    Field::StaticMember(sm) => writeln!(out, "    static {}", sm.name)?,
                    Field::NestedType(nt) => writeln!(out, "    nested {}", nt.name)?,
                    _ => {
                        writeln!(out, "??")?;
                    }
                }
            }
        }

        TypeData::MethodList(_t) => {}

        TypeData::ArgList(t) => {
            write!(out, "num_args: {}", t.args.len())?;
            for &arg in t.args.iter() {
                write!(out, ", ")?;
                ty_ref(out, arg.get())?;
            }
        }

        TypeData::Alias(t) => {
            write!(out, "{} - ", t.name)?;
            ty_ref(out, t.utype)?;
        }

        TypeData::FuncId(t) => {
            writeln!(out, "name: {:?}", t.name)?;

            if t.fixed.scope.get() != 0 {
                write!(out, "    scope: ")?;
                dump_item(out, t.fixed.scope.get())?;
                writeln!(out)?;
            }

            write!(out, "    func: ")?;
            ty_ref(out, t.fixed.func_type.get())?;
        }

        TypeData::MFuncId(t) => {
            writeln!(out, "name: {:?}", t.name)?;

            let parent = t.fixed.parent_type.get();
            write!(out, "    parent: ")?;
            ty_ref(out, parent)?;
            writeln!(out)?;

            let func = t.fixed.func_type.get();
            write!(out, "    func: ")?;
            ty_ref(out, func)?;
            writeln!(out)?;
        }

        TypeData::StringId(t) => {
            writeln!(out, "name: {:?}", t.name)?;
            write!(out, "    ")?;
            dump_item(out, t.id)?;
            writeln!(out)?;
        }

        TypeData::UdtModSrcLine(t) => {
            let src = NameIndex(t.src.get());
            writeln!(out, " module: {}", t.imod.get())?;

            write!(out, "    ")?;
            ty_ref(out, t.ty.get())?;
            writeln!(out)?;

            let line = t.line.get();
            if let Some(names) = names {
                if let Ok(s) = names.get_string(src) {
                    writeln!(out, "    (line {line:6}) {s}")?;
                } else {
                    writeln!(out, "    (line {line:6}) ?? {src:?}")?;
                }
            }
        }

        TypeData::UdtSrcLine(t) => {
            let src = NameIndex(t.src.get());
            writeln!(out)?;

            write!(out, "    ")?;
            ty_ref(out, t.ty.get())?;
            writeln!(out)?;

            let line = t.line.get();
            if let Some(names) = names {
                if let Ok(s) = names.get_string(src) {
                    writeln!(out, "    (line {line:6}) {s}")?;
                } else {
                    writeln!(out, "    (line {line:6}) ?? {src:?}")?;
                }
            }
        }

        TypeData::SubStrList(t) => {
            writeln!(out, "n = {}", t.ids.len())?;
            for (n, id) in t.ids.iter().enumerate() {
                writeln!(out, "[{n:3}] I#{:08x}", id.get())?;
                dump_item(out, id.get())?;
                writeln!(out)?;
            }
        }

        TypeData::BuildInfo(build_info) => {
            writeln!(out)?;

            for (i, a) in build_info.args.iter().enumerate() {
                if let Some(name) = BUILD_INFO_ARG_NAMES.get(i) {
                    write!(out, "    {name} = ")?;
                } else {
                    write!(out, "    ??{i} = ")?;
                }
                dump_item(out, a.get())?;
                writeln!(out)?;
            }
        }

        TypeData::VFTable(vftable) => {
            if vftable.path.get().0 != 0 {
                write!(out, "path: ")?;
                ty_ref(out, vftable.path.get())?;
                write!(out, " ")?;
            }

            if vftable.root.get().0 != 0 {
                write!(out, "root: ")?;
                ty_ref(out, vftable.root.get())?;
                write!(out, " ")?;
            }
        }
    }

    writeln!(out)?;

    if options.show_bytes {
        write!(out, "{:?}", HexDump::new(data))?;
        writeln!(out)?;
    }

    Ok(())
}

// recursive
pub fn dump_type_index_short(
    out: &mut dyn std::fmt::Write,
    context: &super::sym::DumpSymsContext,
    type_index: TypeIndex,
) -> anyhow::Result<()> {
    if context.type_stream.is_primitive(type_index) {
        dump_primitive_type_index(out, type_index)?;
        return Ok(());
    }

    let type_record = context.type_stream.record(type_index)?;
    let kind = type_record.kind;
    let data = type_record.data;

    if context.show_type_index {
        write!(out, "T#{:08x} ", type_index.0)?;
    }

    write!(out, "{kind:?} : ")?;

    let ty_ref = |out: &mut dyn std::fmt::Write,
                  context: &super::sym::DumpSymsContext,
                  ref_ti: TypeIndex| {
        let _ = dump_type_index_short(out, context, ref_ti);
    };

    let mut p = Parser::new(data);

    match TypeData::parse(kind, &mut p)? {
        TypeData::Array(t) => {
            ty_ref(out, context, t.fixed.element_type.get());
            write!(out, "[{}]", t.len)?;
        }

        TypeData::Struct(t) => write!(out, "{}", t.name)?,
        TypeData::Enum(t) => write!(out, "{}", t.name)?,
        TypeData::Union(t) => write!(out, "{}", t.name)?,
        TypeData::Unknown => write!(out, "<UNKNOWN>")?,

        TypeData::Pointer(t) => {
            let attr = t.fixed.attr();
            if attr.r#const() {
                write!(out, "const ")?;
            }
            if attr.volatile() {
                write!(out, "volatile ")?;
            }
            if attr.unaligned() {
                write!(out, "unaligned ")?;
            }

            ty_ref(out, context, t.fixed.ty.get());
        }

        TypeData::Modifier(t) => {
            if t.is_const() {
                write!(out, "const ")?;
            }
            if t.is_unaligned() {
                write!(out, "unaligned ")?;
            }
            if t.is_volatile() {
                write!(out, "volatile ")?;
            }
            ty_ref(out, context, t.underlying_type.get());
        }

        TypeData::MemberFunc(_t) => {}
        TypeData::Proc(_t) => {}
        TypeData::VTableShape(_t) => {}
        TypeData::FieldList(_t) => {}
        TypeData::MethodList(_t) => {}
        TypeData::ArgList(t) => write!(out, "num_args: {}", t.args.len())?,
        TypeData::Alias(t) => write!(out, "{}", t.name)?,

        TypeData::VFTable(_) => {}

        _ => {
            write!(out, "error: unexpected record kind in TPI stream")?;
        }
    }

    Ok(())
}

// recursive
pub fn dump_item_short(
    out: &mut dyn std::fmt::Write,
    context: &super::sym::DumpSymsContext,
    item: ItemId,
) -> anyhow::Result<()> {
    if item == 0 {
        write!(out, "(nil)")?;
        return Ok(());
    }
    if context.ipi.is_primitive(TypeIndex(item)) {
        write!(out, "(error: item = 0x{item:x})")?;
        return Ok(());
    }

    let item_record = match context.ipi.record(TypeIndex(item)) {
        Ok(r) => r,
        Err(e) => {
            write!(out, "(error: {e:?})")?;
            return Ok(());
        }
    };
    let kind = item_record.kind;
    let data = item_record.data;

    if context.show_type_index {
        write!(out, "I#{item:08x} ")?;
    }

    write!(out, "{kind:?} : ")?;

    let ty_ref = |out: &mut dyn std::fmt::Write,
                  context: &super::sym::DumpSymsContext,
                  ref_ti: TypeIndex| {
        let _ = dump_type_index_short(out, context, ref_ti);
    };

    let mut p = Parser::new(data);

    match TypeData::parse(kind, &mut p)? {
        TypeData::UdtModSrcLine(t) => {
            write!(out, "src: 0x{:08x}, line {}, ", t.src.get(), t.line.get())?;
            ty_ref(out, context, t.ty.get());
        }

        TypeData::UdtSrcLine(t) => {
            write!(out, "src: 0x{:08x}, line {}, ", t.src.get(), t.line.get())?;
            ty_ref(out, context, t.ty.get());
        }

        TypeData::FuncId(t) => write!(out, "{:?}", t.name)?,
        TypeData::MFuncId(t) => write!(out, "{:?}", t.name)?,
        TypeData::StringId(t) => write!(out, "{:?}", t.name)?,
        TypeData::SubStrList(_) => {}
        TypeData::BuildInfo(_) => {}

        _ => {
            write!(out, "error: unexpected record kind in IPI stream")?;
        }
    }

    Ok(())
}

```

`pdbtool/src/dump_utils.rs`:

```rs
//! Utilities for dumping byte slices as hex or possibly-invalid UTF-8 strings.

#![forbid(unsafe_code)]
#![warn(missing_docs)]
#![allow(clippy::collapsible_else_if)]
#![allow(clippy::needless_lifetimes)]

use std::fmt::{Debug, Formatter, Write};

/// Dumps a byte slice. The bytes are formatted into rows, with a byte offset displayed on the
/// left, the byte values in hex in the center, and ASCII characters on the right.
pub(crate) struct HexDump<'a> {
    bytes: &'a [u8],
    start: usize,
    show_header: bool,
    row_len: usize,
}

impl<'a> HexDump<'a> {
    /// Creates a `HexDump` over a byte slice with the default display settings.
    pub fn new(bytes: &'a [u8]) -> Self {
        Self {
            bytes,
            start: 0,
            show_header: false,
            row_len: 16,
        }
    }

    /// Limits the input to a maximum length.
    pub fn max(self, max: usize) -> Self {
        Self {
            bytes: &self.bytes[..self.bytes.len().min(max)],
            ..self
        }
    }

    /// Sets the displayed byte offset to a value.
    pub fn at(self, start: usize) -> Self {
        Self { start, ..self }
    }

    /// Specifies whether to display the header line.
    pub fn header(self, show_header: bool) -> Self {
        Self {
            show_header,
            ..self
        }
    }
}

impl<'a> std::fmt::Display for HexDump<'a> {
    fn fmt(&self, fmt: &mut Formatter) -> std::fmt::Result {
        <Self as Debug>::fmt(self, fmt)
    }
}

impl<'a> Debug for HexDump<'a> {
    fn fmt(&self, f: &mut Formatter) -> std::fmt::Result {
        let mut pos = self.start;

        let mut repeat_start: usize = 0;
        let mut repeat_len: usize = 0;
        let mut repeat_byte: u8 = 0;

        if self.show_header {
            writeln!(
                f,
                "________ : 00 01 02 03 04 05 06 07-08 09 0a 0b 0c 0d 0e 0f"
            )?;
        }

        let row_len = self.row_len;

        let write_offset =
            |f: &mut Formatter, offset: usize| -> std::fmt::Result { write!(f, "{offset:08x} : ") };

        let empty_col_size = 3; // two hex values and a space

        for row in self.bytes.chunks(row_len) {
            if row.len() == row_len {
                if repeat_len != 0 {
                    // Are we extending a repeated set of rows?
                    if row.iter().all(|&b| b == repeat_byte) {
                        repeat_len += row_len;
                        pos += row_len;
                        continue;
                    }
                } else {
                    // Did we find the beginning of a new repeated row?
                    let row0 = row[0];
                    if row.iter().all(|&b| b == row0) {
                        repeat_byte = row0;
                        repeat_start = pos;
                        repeat_len = row_len;
                        pos += row_len;
                        continue;
                    }
                }
            }

            if repeat_len != 0 {
                write_offset(f, repeat_start)?;
                writeln!(f, "... {repeat_byte:02x} repeated ...")?;
                repeat_len = 0;
                repeat_start = 0;
                repeat_byte = 0;
            }

            write_offset(f, pos)?;
            for &b in row.iter() {
                write!(f, " {b:02x}")?;
            }
            for _ in 0..(row_len - row.len()) * empty_col_size {
                f.write_char(' ')?;
            }

            {
                write!(f, " : ")?;
                for &b in row.iter() {
                    let c = if matches!(b, 0x20..=0x7e) {
                        char::from(b)
                    } else {
                        '.'
                    };
                    f.write_char(c)?;
                }
            }

            f.write_char('\n')?;

            pos += row_len;
        }

        if repeat_len != 0 {
            write_offset(f, repeat_start)?;
            writeln!(f, "... {repeat_byte:02x} repeated ...")?;
            write_offset(f, pos)?;
            writeln!(f, "(end)")?;
        }

        Ok(())
    }
}

/// Displays a byte slice in hexadecimal.
pub struct HexStr<'a> {
    bytes: &'a [u8],
    packed: bool,
}

impl<'a> HexStr<'a> {
    /// Creates a new `HexStr` over a slice with the default display settings.
    pub fn new(bytes: &'a [u8]) -> Self {
        Self {
            bytes,
            packed: false,
        }
    }

    /// Specifies that the hex string should be displayed without spaces between the bytes.
    pub fn packed(self) -> Self {
        Self {
            packed: true,
            ..self
        }
    }
}

impl<'a> Debug for HexStr<'a> {
    fn fmt(&self, fmt: &mut Formatter) -> std::fmt::Result {
        for (i, &b) in self.bytes.iter().enumerate() {
            if i != 0 && !self.packed {
                fmt.write_char(' ')?;
            }
            write!(fmt, "{b:02x}")?;
        }

        Ok(())
    }
}

/// Helps display indentation in debug output
#[derive(Copy, Clone)]
pub struct Indent(pub u32);

impl std::fmt::Display for Indent {
    fn fmt(&self, fmt: &mut std::fmt::Formatter) -> std::fmt::Result {
        for _ in 0..self.0 {
            fmt.write_char(' ')?;
        }
        Ok(())
    }
}

/// Creates an `Indent`.
pub fn indent(n: u32) -> Indent {
    Indent(n)
}

```

`pdbtool/src/find.rs`:

```rs
use crate::dump::sym::dump_sym;
use anyhow::Result;
use bstr::BStr;
use ms_pdb::codeview::IteratorWithRangesExt;
use ms_pdb::syms::SymData;
use std::path::Path;

/// Searches the DBI Section Contributions table.
#[derive(clap::Parser)]
pub struct FindOptions {
    /// The PDB to search.
    pub pdb: String,

    /// COFF section index to search
    pub section: u16,

    /// The symbol name or contribution offset to search.
    pub name: String,
}

pub fn find_command(options: &FindOptions) -> Result<()> {
    let pdb = ms_pdb::Pdb::open(Path::new(&options.pdb))?;

    let dbi = pdb.read_dbi_stream()?;
    let contribs = dbi.section_contributions()?;

    if let Some(hex_str) = options.name.strip_prefix("0x") {
        let addr = u32::from_str_radix(hex_str, 0x10)?;

        if let Some(contrib) = contribs.find(options.section, addr as i32) {
            println!("Found contribution record:\n{contrib:#?}");
            println!("Module index = {}", contrib.module_index.get());
        } else {
            println!("No symbol found.");
        }
    } else {
        println!("name lookups are nyi");
    }

    Ok(())
}

/// Searches the TPI Stream for a given type.
#[derive(clap::Parser)]
pub struct FindNameOptions {
    /// The PDB to search.
    pub pdb: String,

    /// The type name to search for.
    pub name: String,

    /// Indicates that `name` is a regex.
    #[arg(long, short)]
    pub regex: bool,
}

pub fn find_name_command(options: &FindNameOptions) -> Result<()> {
    use crate::dump::sym::DumpSymsContext;

    let pdb = ms_pdb::Pdb::open(Path::new(&options.pdb))?;
    let arch = pdb.arch()?;
    let tpi = pdb.read_type_stream()?;
    let ipi = pdb.read_ipi_stream()?;
    let mut context = DumpSymsContext::new(arch, &tpi, &ipi);

    let gss = pdb.read_gss()?;

    if options.regex {
        let rx = regex::bytes::Regex::new(&options.name)?;

        let mut found_any = false;

        for (record_range, sym) in gss.iter_syms().with_ranges() {
            let sym_data = SymData::parse(sym.kind, sym.data)?;

            if let Some(sym_name) = sym_data.name() {
                if rx.is_match(sym_name) {
                    let mut out = String::new();
                    dump_sym(
                        &mut out,
                        &mut context,
                        record_range.start as u32,
                        sym.kind,
                        sym.data,
                    )?;
                    print!("{out}");
                    found_any = true;
                }
            }
        }

        if !found_any {
            println!("No matches found.");
        }
    } else {
        let gsi = pdb.read_gsi()?;
        if let Some(sym) = gsi.find_symbol(&gss, BStr::new(&options.name))? {
            let mut out = String::new();
            dump_sym(&mut out, &mut context, 0, sym.kind, sym.data)?;
            print!("{out}");
            return Ok(());
        }
        println!("Symbol not found.");
    }

    Ok(())
}

```

`pdbtool/src/glob_pdbs.rs`:

```rs
use anyhow::{Result, bail};
use std::path::PathBuf;

#[derive(clap::Parser)]
pub struct PdbList {
    /// The set of PDB files to read. This can contain globs, e.g. `*.pdb` or `foo\**\*.pdb`.
    pub pdbs: Vec<String>,
}

impl PdbList {
    pub fn get_paths(&self) -> Result<Vec<PathBuf>> {
        let paths = self.get_paths_empty_ok()?;
        if paths.is_empty() {
            bail!("This command requires that you specify one or more PDB files.");
        }
        Ok(paths)
    }

    pub fn get_paths_empty_ok(&self) -> Result<Vec<PathBuf>> {
        let mut file_names = Vec::new();
        for file_name_or_glob in self.pdbs.iter() {
            if file_name_or_glob.contains(['?', '*']) {
                for f in glob::glob(file_name_or_glob)? {
                    let f = f?;
                    if f.is_file() {
                        file_names.push(f);
                    }
                }
            } else {
                file_names.push(PathBuf::from(file_name_or_glob));
            }
        }

        Ok(file_names)
    }
}

```

`pdbtool/src/hexdump.rs`:

```rs
use crate::dump_utils::HexDump;
use anyhow::Result;
use std::fs::File;
use std::io::{Read, Seek, SeekFrom};

use crate::util::HexU64;

/// Dumps the contents of a file as a hex dump.
#[derive(clap::Parser)]
pub struct HexdumpOptions {
    /// The file to dump
    pub file: String,
    /// Offset within the file. Defaults to 0.
    pub offset: Option<HexU64>,
    /// Max length of the data to dump. Defaults to 0x1000.
    pub len: Option<HexU64>,
}

pub fn command(options: HexdumpOptions) -> Result<()> {
    let mut f = File::open(&options.file)?;

    let offset: u64 = if let Some(offset) = options.offset {
        offset.0
    } else {
        0
    };

    f.seek(SeekFrom::Start(offset))?;

    let len: usize = if let Some(len) = options.len {
        len.0 as usize
    } else {
        0x1000
    };

    let mut buffer: Vec<u8> = vec![0; len];

    let n = f.read(buffer.as_mut_slice())?;
    let bytes = &buffer[..n];

    print!("{}", HexDump::new(bytes).at(offset as usize));

    Ok(())
}

```

`pdbtool/src/main.rs`:

```rs
#![forbid(unused_must_use)]
#![allow(clippy::collapsible_else_if)]
#![allow(clippy::manual_map)]
#![allow(clippy::single_match)]
#![allow(clippy::upper_case_acronyms)]
#![allow(clippy::too_many_arguments)]
#![allow(clippy::needless_late_init)]

use clap::Parser;

mod addsrc;
mod check;
mod compare;
mod container;
mod copy;
mod counts;
mod dump;
mod dump_utils;
mod find;
mod glob_pdbs;
mod hexdump;
mod pdz;
mod save;
mod util;

#[derive(clap::Parser)]
struct CommandWithFlags {
    /// Reduce logging to just warnings and errors in `mspdb` and `pdbtool` modules.
    #[arg(long)]
    quiet: bool,

    /// Turn on debug output in all `mspdb` and `pdbtool` modules. Noisy!
    #[arg(long)]
    verbose: bool,

    /// Show timestamps in log messages
    #[arg(long)]
    timestamps: bool,

    /// Connect to Tracy (diagnostics tool). Requires that the `tracy` Cargo feature be enabled.
    #[arg(long)]
    tracy: bool,

    #[command(subcommand)]
    command: Command,
}

#[derive(clap::Subcommand)]
enum Command {
    /// Adds source file contents to the PDB. The contents are embedded directly within the PDB.
    /// WinDbg and Visual Studio can both extract the source files.
    AddSrc(addsrc::AddSrcOptions),
    /// Copies a PDB file (or a PDZ) file to another PDB file. All stream contents are preserved
    /// exactly, byte-for-byte. The blocks within streams are laid out sequentially. The output
    /// file is always a PDB file, not a PDZ. The input can be either PDB or PDZ.
    Copy(copy::Options),
    /// Show information about the file container. This indicates whether the file is using the
    /// PDB (MSF) or PDBZ (MSFZ) container format. This also shows some container-specific
    /// information.
    Container(container::ContainerOptions),
    /// Compares two PDB (or PDZ) files and verifies that their stream contents are identical.
    /// Reports differences but continues checking all streams unless stopped.
    Compare(compare::CompareOptions),
    Test,
    Dump(dump::DumpOptions),
    Save(save::SaveStreamOptions),
    Find(find::FindOptions),
    FindName(find::FindNameOptions),
    Counts(counts::CountsOptions),
    /// Dumps part of a file (any file, not just a PDB) as a hex dump. If you want to dump a
    /// specific stream, then use the `dump <filename> hex` command instead.
    Hexdump(hexdump::HexdumpOptions),
    PdzEncode(pdz::encode::PdzEncodeOptions),
    Check(check::CheckOptions),
}

fn main() -> anyhow::Result<()> {
    let command_with_flags = CommandWithFlags::parse();
    configure_tracing(&command_with_flags);

    match command_with_flags.command {
        Command::AddSrc(args) => addsrc::command(args)?,
        Command::Dump(args) => dump::dump_main(args)?,
        Command::Test => {}
        Command::Copy(args) => copy::copy_command(&args)?,
        Command::Save(args) => save::save_stream(&args)?,
        Command::Find(args) => find::find_command(&args)?,
        Command::FindName(args) => find::find_name_command(&args)?,
        Command::Counts(args) => counts::counts_command(args)?,
        Command::Hexdump(args) => hexdump::command(args)?,
        Command::PdzEncode(args) => pdz::encode::pdz_encode(args)?,
        Command::Container(args) => container::container_command(&args)?,
        Command::Compare(args) => compare::command(args)?,
        Command::Check(args) => check::command(args)?,
    }

    Ok(())
}

fn configure_tracing(args: &CommandWithFlags) {
    use tracing_subscriber::filter::LevelFilter;

    if args.tracy {
        #[cfg(feature = "tracy")]
        {
            use tracing_subscriber::layer::SubscriberExt;

            let layer = tracing_tracy::TracyLayer::default();
            tracing::subscriber::set_global_default(tracing_subscriber::registry().with(layer))
                .expect("setup tracy layer");

            return;
        }

        #[cfg(not(feature = "tracy"))]
        {
            eprintln!(
                "Tracing is not enabled in the build configuration.\n\
                 You can enable it by using 'cargo run --features \"tracy\"'."
            );
        }
    }

    let builder = tracing_subscriber::fmt();

    let max_level = if args.quiet {
        LevelFilter::ERROR
    } else if args.verbose {
        LevelFilter::DEBUG
    } else {
        LevelFilter::INFO
    };

    builder
        .with_max_level(max_level)
        .with_ansi(false)
        .without_time()
        .init();
}

```

`pdbtool/src/pdz.rs`:

```rs
pub mod encode;

```

`pdbtool/src/pdz/encode.rs`:

```rs
//! Compresses PDB files into "PDZ" (compressed PDB) files.

use anyhow::{Context, Result, bail};
use ms_pdb::codeview::HasRestLen;
use ms_pdb::dbi::DbiStreamHeader;
use ms_pdb::msf::Msf;
use ms_pdb::msfz::{self, MIN_FILE_SIZE_16K, MsfzFinishOptions, MsfzWriter, StreamWriter};
use ms_pdb::syms::SymIter;
use ms_pdb::tpi;
use ms_pdb::types::TypesIter;
use ms_pdb::{RandomAccessFile, Stream};
use std::fs::File;
use std::io::{Read, Seek, SeekFrom, Write};
use std::path::Path;
use tracing::{debug, error, info, trace, trace_span, warn};
use zerocopy::{FromBytes, IntoBytes};

#[derive(clap::Parser, Debug)]
pub(crate) struct PdzEncodeOptions {
    /// Path to the input PDB file.
    pub input_pdb: String,

    /// Path to the output PDZ file.
    pub output_pdz: String,

    /// Pad the output PDZ file to a minimum of 16 KB. This is a workaround for a bug in the
    /// original MSVC implementation of the PDZ *decoder*.
    #[arg(long)]
    pub pad16k: bool,

    /// Compress the Stream Directory. Although the PDZ specification defines stream directory
    /// compression, some PDZ readers do not yet support reading compressed stream directories.
    #[arg(long)]
    pub compress_stream_dir: bool,

    /// If a file is not a PDB, then simply copy it to the destination, unchanged. This will
    /// copy Portable PDBs and PDZs to the output without changing them.
    #[arg(long)]
    pub copy_unrecognized: bool,

    /// The maximum _uncompressed_ size of each chunk, in bytes.  (You can specify a suffix of
    /// K for 1024 or M for 1048576.). Chunk contents are accumulated in a buffer with the specified
    /// size. When the buffer is full, the chunk is compressed and written to the output.
    /// The default is 4 MB.
    #[arg(long)]
    pub max_chunk_size: Option<String>,

    /// Place each chunk into its own compression chunk (or chunks, if the stream is large).
    #[arg(long)]
    pub one_stream_per_chunk: bool,

    /// After writing the PDZ file, close it, re-open it, and verify that its stream contents
    /// match the original PDB, byte for byte.
    #[arg(long)]
    pub verify: bool,
}

pub fn pdz_encode(options: PdzEncodeOptions) -> Result<()> {
    let _span = trace_span!("pdz_encode").entered();

    let input_file = RandomAccessFile::open(Path::new(&options.input_pdb))
        .with_context(|| format!("Failed to open input PDB: {}", options.input_pdb))?;

    use ms_pdb::taster::{Flavor, what_flavor};

    if let Ok(flavor) = what_flavor(&input_file) {
        if flavor != Some(Flavor::Pdb) {
            if options.copy_unrecognized {
                drop(input_file);
                std::fs::copy(&options.input_pdb, &options.output_pdz)?;
                return Ok(());
            } else {
                bail!("The input file is not a PDB: {}", options.input_pdb);
            }
        }
    }

    // Choose the maximum chunk size. We do this before creating the MsfzWriter so that we can
    // validate the input before creating an output file.
    let max_chunk_size: u32 = if let Some(s) = &options.max_chunk_size {
        parse_bytes(s)?
    } else {
        msfz::DEFAULT_CHUNK_THRESHOLD
    };

    let pdb = Msf::open(Path::new(&options.input_pdb))
        .with_context(|| format!("Failed to open input PDB: {}", options.input_pdb))?;

    let mut writer = MsfzWriter::create(Path::new(&options.output_pdz))
        .with_context(|| format!("Failed to open output PDZ: {}", options.output_pdz))?;

    writer.set_uncompressed_chunk_size_threshold(max_chunk_size);
    info!(
        "Using maximum chunk size: {n} ({n:#x})",
        n = writer.uncompressed_chunk_size_threshold()
    );

    let mut stream_data: Vec<u8> = Vec::new();
    let num_streams = pdb.num_streams();
    writer.reserve_num_streams(num_streams as usize);

    // The PDBI is a very important stream and we want to write its uncompressed form at the
    // very beginning of the output file, so that it can be easily found using the "initial read"
    // optimization.
    {
        let _span = trace_span!("transfer PDBI stream");
        pdb.read_stream_to_vec_mut(Stream::PDB.into(), &mut stream_data)?;
        let mut sw = writer.stream_writer(Stream::PDB.into())?;
        sw.set_compression_enabled(false);
        sw.write_all(&stream_data)?;
    }

    if options.one_stream_per_chunk {
        writer.end_chunk()?;
    }

    // Write the DBI stream (3).
    let dbi_header: Option<DbiStreamHeader>;
    {
        let _span = trace_span!("transfer DBI stream");
        pdb.read_stream_to_vec_mut(Stream::DBI.into(), &mut stream_data)?;
        let mut sw = writer.stream_writer(Stream::DBI.into())?;
        dbi_header = write_dbi(&mut sw, &stream_data)?;
        writer.end_chunk()?;
    }

    // Write the TPI (2).
    let tpi_header_opt: Option<tpi::TypeStreamHeader> = if pdb.is_stream_valid(Stream::TPI.into()) {
        let _span = trace_span!("transfer TPI stream");
        pdb.read_stream_to_vec_mut(Stream::TPI.into(), &mut stream_data)?;
        let mut sw = writer.stream_writer(Stream::TPI.into())?;
        write_tpi_or_ipi(&mut sw, &stream_data, max_chunk_size as usize)?
    } else {
        None
    };

    // Write the IPI (4)
    let ipi_header_opt: Option<tpi::TypeStreamHeader> = if pdb.is_stream_valid(Stream::IPI.into()) {
        let _span = trace_span!("transfer IPI stream");
        pdb.read_stream_to_vec_mut(Stream::IPI.into(), &mut stream_data)?;
        let mut sw = writer.stream_writer(Stream::IPI.into())?;
        write_tpi_or_ipi(&mut sw, &stream_data, max_chunk_size as usize)?
    } else {
        None
    };

    // Loop through the rest of the streams and do normal chunked compression.
    for stream_index in 1..num_streams {
        let _span = trace_span!("stream").entered();
        trace!(stream_index);

        if !pdb.is_stream_valid(stream_index) {
            // This is a nil stream. We don't need to do anything because the writer has already
            // reserved this stream slot.
            continue;
        }

        if stream_index == Stream::PDB.into()
            || stream_index == Stream::DBI.into()
            || stream_index == Stream::IPI.into()
            || stream_index == Stream::TPI.into()
        {
            // We have already processed these streams, above.
            continue;
        }

        // Custom encoding for the TPI Hash Stream
        if let Some(tpi_header) = &tpi_header_opt {
            if tpi_header.hash_stream_index.get() == Some(stream_index) {
                pdb.read_stream_to_vec_mut(stream_index, &mut stream_data)?;
                let mut sw = writer.stream_writer(stream_index)?;
                write_tpi_or_ipi_hash_stream(
                    &mut sw,
                    &stream_data,
                    max_chunk_size as usize,
                    tpi_header,
                )?;
                writer.end_chunk()?;
                continue;
            }
        }

        // Custom encoding for the IPI Hash Stream
        if let Some(ipi_header) = &ipi_header_opt {
            if ipi_header.hash_stream_index.get() == Some(stream_index) {
                pdb.read_stream_to_vec_mut(stream_index, &mut stream_data)?;
                let mut sw = writer.stream_writer(stream_index)?;
                write_tpi_or_ipi_hash_stream(
                    &mut sw,
                    &stream_data,
                    max_chunk_size as usize,
                    ipi_header,
                )?;
                writer.end_chunk()?;
                continue;
            }
        }

        // Custom encoding for the Global Symbol Stream.
        // The stream number for the Global Symbol Stream is found in the DBI Stream Header.
        match &dbi_header {
            Some(dbi_header) if dbi_header.global_symbol_stream.get() == Some(stream_index) => {
                let mut sw = writer.stream_writer(stream_index)?;
                pdb.read_stream_to_vec_mut(stream_index, &mut stream_data)?;
                write_global_symbols_stream(&mut sw, &stream_data, max_chunk_size as usize)?;
                writer.end_chunk()?;
                continue;
            }
            _ => {}
        }

        {
            let _span = trace_span!("read stream").entered();
            pdb.read_stream_to_vec_mut(stream_index, &mut stream_data)?;
            trace!(stream_size = stream_data.len());
        }

        {
            let _span = trace_span!("write stream").entered();
            let mut sw = writer.stream_writer(stream_index)?;
            sw.write_all(&stream_data)?;
        }

        if options.one_stream_per_chunk {
            writer.end_chunk()?;
        }
    }

    // Finish encoding the MSFZ file. This closes the session; don't append more data to the file
    // after this line. This writes the MSFZ Stream Directory, Chunk Table, and MSFZ File Header.
    let (summary, mut file) = {
        let _span = trace_span!("finish writing").entered();
        writer.finish_with_options(MsfzFinishOptions {
            min_file_size: if options.pad16k { MIN_FILE_SIZE_16K } else { 0 },
            stream_dir_compression: if options.compress_stream_dir {
                Some(msfz::Compression::Zstd)
            } else {
                None
            },
        })?
    };

    let out_file_size = file.seek(SeekFrom::End(0))?;

    match std::fs::metadata(&options.input_pdb) {
        Ok(pdb_metadata) => {
            let before = pdb_metadata.len();
            let after = out_file_size;

            // We don't divide by zero around here.
            if before != 0 {
                let percent = (before as f64 - after as f64) / (before as f64) * 100.0;
                info!(
                    "    PDB -> PDZ compression : {:8} -> {:8} {percent:2.1} %",
                    friendly::bytes(before),
                    friendly::bytes(after)
                );
            }
        }
        Err(e) => {
            warn!("Failed to get metadata for input PDB: {e:?}");
        }
    }

    info!("Number of streams: {:8}", summary.num_streams);
    info!("Number of chunks:  {:8}", summary.num_chunks);

    // Explicitly drop our output file handle so that we can re-open it for verification.
    drop(file);

    if options.verify {
        info!("Verifying PDZ encoding");
        let is_same = verify_pdz(&pdb, &options.output_pdz)?;
        if !is_same {
            bail!("PDZ encoding failed verification.");
        }
    }

    drop(pdb);

    Ok(())
}

/// Reads all of the data from two files and compares their contents.
///
/// The first file is a PDB (MSF) file and is already open.
/// The second file is a PDZ (MSFZ) file and its filename is given.
///
/// Returns `true` if their contents are identical.
fn verify_pdz(input_pdb: &Msf, output_pdz: &str) -> anyhow::Result<bool> {
    let output = msfz::Msfz::open(output_pdz)?;

    let input_num_streams = input_pdb.num_streams();
    let output_num_streams = output.num_streams();
    if input_num_streams != output_num_streams {
        error!(
            "The output file (PDZ) has the wrong number of streams. \
             Expected value: {input_num_streams}. \
             Actual value: {output_num_streams}"
        );
        bail!("Wrong number of streams");
    }

    let mut has_errors = false;

    let mut input_stream_data: Vec<u8> = Vec::new();
    let mut output_stream_data: Vec<u8> = Vec::new();

    for stream in 1..input_num_streams {
        let input_stream_is_valid = input_pdb.is_stream_valid(stream);
        let output_stream_is_valid = output.is_stream_valid(stream);

        if input_stream_is_valid != output_stream_is_valid {
            error!(
                "Stream {stream} has wrong validity. \
                 Expected value: {input_stream_is_valid:?}. \
                 Actual value: {output_stream_is_valid:?}."
            );
            has_errors = true;
        }

        if !input_stream_is_valid {
            continue;
        }

        let input_stream_size = input_pdb.stream_size(stream) as usize;
        let output_stream_size = output.stream_size(stream)? as usize;

        if input_stream_size != output_stream_size {
            error!(
                "Stream {stream} has wrong length in stream directory.
                    Expected value: {input_stream_size}. \
                    Actual value: {output_stream_size}."
            );
            has_errors = true;
            continue;
        }

        // Read stream data in the input file.
        {
            input_stream_data.clear();
            let mut sr = input_pdb.get_stream_reader(stream)?;
            sr.read_to_end(&mut input_stream_data)?;
        }

        // Read stream data in the output file.
        {
            output_stream_data.clear();
            let mut sr = output.get_stream_reader(stream)?;
            sr.read_to_end(&mut output_stream_data)?;
        }

        if input_stream_data.len() != output_stream_data.len() {
            error!(
                "Stream {stream} has wrong length. \
                    Expected value: {}. \
                    Actual value: {}.",
                input_stream_data.len(),
                output_stream_data.len()
            );
            has_errors = true;
            continue;
        }

        if let Some(byte_offset) = crate::compare::find_index_of_first_different_byte(
            &input_stream_data,
            &output_stream_data,
        ) {
            error!(
                "Stream {stream} has wrong (different) contents, at index {byte_offset} ({byte_offset:#x})"
            );
            has_errors = true;
            continue;
        }
    }

    if has_errors {
        return Ok(false);
    }

    info!("Verification succeeded.");

    Ok(true)
}

/// Write the DBI stream. Be smart about compression and compression chunk boundaries.
fn write_dbi(
    sw: &mut StreamWriter<'_, File>,
    stream_data: &[u8],
) -> Result<Option<DbiStreamHeader>> {
    // Avoid compressing data from the DBI stream with other chunks.
    sw.end_chunk()?;

    // Read the DBI stream header. If we can't read it (because it's too small), then fall back
    // to copying the stream.
    let Ok((dbi_header, mut rest_of_stream)) = DbiStreamHeader::read_from_prefix(stream_data)
    else {
        // Something is seriously wrong with this PDB. Pass the contents through without any
        // modification or compression.
        sw.set_compression_enabled(false);
        sw.write_all(stream_data)?;
        sw.end_chunk()?;
        return Ok(None);
    };

    // Write the DBI Stream Header uncompressed. This allows symbol.exe to read it.
    sw.set_compression_enabled(false);
    sw.write_all(dbi_header.as_bytes())?;
    sw.end_chunk()?;

    // The DBI consists of a header, followed by a set of substreams. The substreams contain
    // data with different sizes, compression characteristic, and different access patterns.
    //
    // We attempt to break up the rest of the stream data and handle each substream individually.
    // If we find a substream size that doesn't make sense (is negative or exceeds the size of
    // the remaining data in the stream) then we just bail and write the rest of the data without
    // doing any more chunking.

    'fallback: {
        macro_rules! get_next_substream {
            ($substream_len_field:ident) => {
                {
                    let Ok(len_u32) = u32::try_from(dbi_header.$substream_len_field.get()) else {
                        warn!("DBI stream is invalid; the substream {} has a negative length", stringify!($substream_len_field));
                        break 'fallback;
                    };
                    let len_usize = len_u32 as usize;
                    if rest_of_stream.len() < len_usize {
                        warn!("DBI stream is invalid; the substream {} has a length that exceeds the size of the remaining stream data.", stringify!($substream_len_field));
                        break 'fallback;
                    }
                    let (lo, hi) = rest_of_stream.split_at(len_usize);
                    rest_of_stream = hi;
                    lo
                }
            }
        }

        // Module Info
        let modules = get_next_substream!(mod_info_size);
        sw.set_compression_enabled(true);
        sw.write_all(modules)?;
        sw.end_chunk()?;

        // The "Section Contributions" substream is very large and is not often used, so we place
        // it in its own chunk, too.
        let section_contributions = get_next_substream!(section_contribution_size);
        sw.set_compression_enabled(true);
        sw.write_all(section_contributions)?;
        sw.end_chunk()?;

        // The "Section Map" is very small. We write it without compression.
        let section_map = get_next_substream!(section_map_size);
        sw.set_compression_enabled(false);
        sw.write_all(section_map)?;
        sw.end_chunk()?;

        // The "Sources" substream is very commonly accessed and medium-sized. We store it in its
        // own chunk.
        let sources = get_next_substream!(source_info_size);
        sw.set_compression_enabled(true);
        sw.write_all(sources)?;
        sw.end_chunk()?;

        // The "Type Server Map", "Optional Debug Headers", and "Edit-and-Continue" substreams are
        // typically very small and rarely accessed. We store them compressed.
    }

    // If we got here, then either something is wrong with the contents of the stream, or we just
    // reached the last few substreams, and we don't do anything special with them. Write the rest
    // of the data.
    sw.set_compression_enabled(true);
    sw.write_all(rest_of_stream)?;
    sw.end_chunk()?;

    Ok(Some(dbi_header))
}

/// This is the fallback path for writing complex streams.
///
/// If we find any problem in writing a complex stream, we fall back to compressing all of it.
#[inline(never)]
fn write_stream_fallback(sw: &mut StreamWriter<'_, File>, stream_data: &[u8]) -> Result<()> {
    sw.set_compression_enabled(true);
    sw.write_all(stream_data)?;
    sw.end_chunk()?;
    Ok(())
}

fn write_global_symbols_stream(
    sw: &mut StreamWriter<'_, File>,
    stream_data: &[u8],
    mut max_chunk_len: usize,
) -> Result<()> {
    debug!("Writing Global Symbol Stream");

    sw.end_chunk()?;

    // The code below assumes that you can always put at least one type record into a chunk.
    // To prevent a forward-progress failure, we require that max_chunk_len is larger than the
    // largest type record.
    const MIN_CHUNK_LEN: usize = 0x20000;
    if max_chunk_len < MIN_CHUNK_LEN {
        warn!(
            "max_chunk_len ({}) is way too small; promoting it",
            max_chunk_len
        );
        max_chunk_len = MIN_CHUNK_LEN;
    }

    let mut current_chunk_bytes: &[u8] = stream_data;
    let mut total_bytes_written: usize = 0;

    'top: while !current_chunk_bytes.is_empty() {
        let mut iter = SymIter::new(current_chunk_bytes);
        loop {
            let rest_len_before = iter.rest_len();
            if iter.next().is_none() {
                break 'top;
            }
            let rest_len_after = iter.rest_len();

            let chunk_len_with_this_record = current_chunk_bytes.len() - rest_len_after;
            if chunk_len_with_this_record > max_chunk_len {
                let chunk_len_without_this_record = current_chunk_bytes.len() - rest_len_before;
                let (committed_chunk_bytes, next_chunk_bytes) =
                    current_chunk_bytes.split_at(chunk_len_without_this_record);

                // TODO: This could be optimized to a single, non-buffered chunk write.
                sw.write_all(committed_chunk_bytes)?;
                sw.end_chunk()?;
                total_bytes_written += committed_chunk_bytes.len();

                // This will cause us to re-parse a record at the start of the next chunk.
                // That's ok, that's cheap.  But we do need to handle the case where a record
                // is larger than max_chunk_len.  We "handle" that by requiring that max_chunk_len
                // is at least 0x10004, since that is the maximum size for any record.  That's a
                // very silly lower bound for a chunk size, so we actually require it to be higher.
                current_chunk_bytes = next_chunk_bytes;
                continue 'top;
            }
        }
    }

    if !current_chunk_bytes.is_empty() {
        sw.write_all(current_chunk_bytes)?;
        sw.end_chunk()?;
        total_bytes_written += current_chunk_bytes.len();
    }

    assert_eq!(
        total_bytes_written,
        stream_data.len(),
        "expected to write same number of record bytes"
    );

    Ok(())
}

/// Write the TPI or IPI stream. Be smart about compression and compression chunk boundaries.
///
/// This function returns `Some(header)` if the TPI header was correctly parsed. This header can
/// be used to optimize the encoding of the associated Type Hash Stream.
fn write_tpi_or_ipi(
    sw: &mut StreamWriter<'_, File>,
    stream_data: &[u8],
    mut max_chunk_len: usize,
) -> Result<Option<tpi::TypeStreamHeader>> {
    debug!("write_tpi_or_ipi");
    sw.end_chunk()?;

    // The code below assumes that you can always put at least one type record into a chunk.
    // To prevent a forward-progress failure, we require that max_chunk_len is larger than the
    // largest type record.
    const MIN_CHUNK_LEN: usize = 0x20000;
    if max_chunk_len < MIN_CHUNK_LEN {
        warn!("max_chunk_len is way too small; promoting it");
        max_chunk_len = MIN_CHUNK_LEN;
    }

    // If the stream does not even contain a full header, then fall back to writing full contents.
    let Ok((tpi_header, after_header)) = tpi::TypeStreamHeader::read_from_prefix(stream_data)
    else {
        warn!("TPI or IPI stream was too short to contain a valid header");
        write_stream_fallback(sw, stream_data)?;
        return Ok(None);
    };

    // Find the slice of the type data. There can be data following the type data and we must
    // handle it correctly.
    let type_record_bytes_len = tpi_header.type_record_bytes.get() as usize;
    if after_header.len() < type_record_bytes_len {
        warn!(
            "TPI or IPI stream contained invalid header value (type_record_bytes exceeded bounds)"
        );
        write_stream_fallback(sw, stream_data)?;
        return Ok(None);
    }

    debug!(type_record_bytes_len, "encoding TPI/IPI.");

    // type_record_bytes contains the encoded type records
    // after_records contains unknown data (if any) after the type records
    let (type_record_bytes, after_records) = after_header.split_at(type_record_bytes_len);

    // Write the header, without compression.
    // TODO: Place this in the initial read section.
    sw.set_compression_enabled(false);
    sw.write_all(tpi_header.as_bytes())?;

    // Next, we are going to scan through the type records in the TPI. Our goal is to create chunk
    // boundaries that align with record boundaries, so that no type record is split across chunks.
    sw.set_compression_enabled(true);

    // current_chunk_bytes contains the type records that will be written into the next chunk.
    let mut current_chunk_bytes: &[u8] = type_record_bytes;

    let mut record_bytes_written: usize = 0;

    // This loop runs once per "chunk". Each iteration builds a single MSFZ chunk from a
    // sequence of contiguous type records. Type records never cross chunk boundaries.
    'top: while !current_chunk_bytes.is_empty() {
        let mut iter = TypesIter::new(current_chunk_bytes);
        loop {
            let rest_len_before = iter.rest_len();
            if iter.next().is_none() {
                break 'top;
            }

            let rest_len_after = iter.rest_len();
            let record_len = rest_len_before - rest_len_after;
            assert!(record_len <= current_chunk_bytes.len());

            // Would adding this record to the current chunk exceed our threshold?
            let chunk_len_with_this_record = current_chunk_bytes.len() - rest_len_after;
            if chunk_len_with_this_record > max_chunk_len {
                let chunk_len_without_this_record = current_chunk_bytes.len() - rest_len_before;
                let (committed_chunk_bytes, next_chunk_bytes) =
                    current_chunk_bytes.split_at(chunk_len_without_this_record);

                // TODO: This could be optimized to a single, non-buffered chunk write.
                sw.write_all(committed_chunk_bytes)?;
                sw.end_chunk()?;
                record_bytes_written += committed_chunk_bytes.len();

                // This will cause us to re-parse a record at the start of the next chunk.
                // That's ok, that's cheap.  But we do need to handle the case where a record
                // is larger than max_chunk_len.  We "handle" that by requiring that max_chunk_len
                // is at least 0x10004, since that is the maximum size for any record.  That's a
                // very silly lower bound for a chunk size, so we actually require it to be higher.
                current_chunk_bytes = next_chunk_bytes;
                continue 'top;
            }

            // Keep processing records.
        }
    }

    // If we got here, then the iterator stopped reporting records. That can happen for two
    // reasons: 1) the normal case where we reach the end of the types, or 2) we failed to
    // decode a type record.  In both cases, writing current_chunk_contents will write the
    // prefix of records that have been parsed (but have not triggered our threshold
    if !current_chunk_bytes.is_empty() {
        // TODO: optimize to a single, non-buffered chunk write.
        sw.write_all(current_chunk_bytes)?;
        record_bytes_written += current_chunk_bytes.len();
    }
    sw.end_chunk()?;
    assert_eq!(
        record_bytes_written,
        type_record_bytes.len(),
        "expected to write same number of record bytes"
    );

    if !after_records.is_empty() {
        debug!(
            after_records_len = after_records.len(),
            "TPI/IPI contains data after type stream"
        );
        sw.write_all(after_records)?;
    }

    sw.end_chunk()?;

    Ok(Some(tpi_header))
}

/// Write the "Type Hash Stream" associated with the TPI or IPI stream.
/// Be smart about compression and compression chunk boundaries.
///
/// This function requires the Type Stream Header from the original TPI or IPI stream.
/// That header describes the regions within the Type Stream Header.
///
/// The Type Hash Stream consists of three regions: 1) Hash Value Buffer, 2) Index Offset Buffer,
/// and 3) Hash Adjustment Buffer.  The size and location of each of these regions is specified
/// in fields in the Type Stream Header.
///
/// We use a simple algorithm. We build a list of the start and end locations of each of these
/// buffers (if they are non-zero length). Then we sort the list and de-dup it.  Then we traverse
/// the list, writing chunks for each region.
///
/// This simple algorithm allows us to ignore all sorts of strange situations, such as regions
/// overlapping, regions occurring in an unusual order, or there being bytes within the stream
/// that are not covered by any region. All we care about is the compression boundaries.
fn write_tpi_or_ipi_hash_stream(
    sw: &mut StreamWriter<'_, File>,
    stream_data: &[u8],
    max_chunk_len: usize,
    tpi_header: &tpi::TypeStreamHeader,
) -> Result<()> {
    sw.end_chunk()?;

    let stream_len_u32 = stream_data.len() as u32;

    let mut boundaries: Vec<usize> = Vec::with_capacity(8);

    boundaries.push(stream_data.len());

    let regions: [(i32, u32); 3] = [
        (
            tpi_header.hash_value_buffer_offset.get(),
            tpi_header.hash_value_buffer_length.get(),
        ),
        (
            tpi_header.index_offset_buffer_offset.get(),
            tpi_header.index_offset_buffer_length.get(),
        ),
        (
            tpi_header.hash_adj_buffer_offset.get(),
            tpi_header.hash_adj_buffer_length.get(),
        ),
    ];

    for &(start, length) in regions.iter() {
        if length == 0 {
            continue;
        }
        if start < 0 {
            warn!("Type Hash Stream has a negative offset for one of its regions");
            continue;
        }

        let start_u: u32 = start as u32;
        if start_u > stream_len_u32 {
            warn!("Type Hash Stream has a region whose start offset is out of bounds");
            continue;
        }

        let avail = stream_len_u32 - start_u;
        if length > avail {
            warn!("Type Hash Stream has a region whose end offset is out of bounds");
            continue;
        }

        let end: u32 = start_u + length;

        boundaries.push(start_u as usize);
        boundaries.push(end as usize);
    }

    boundaries.sort_unstable();
    boundaries.dedup();

    debug!("Type Hash Stream offset boundaries: {:?}", boundaries);

    let mut previous_boundary: usize = 0;
    let mut total_bytes_written: usize = 0;

    for &next_boundary in boundaries.iter() {
        assert!(next_boundary >= previous_boundary);
        let region_data = &stream_data[previous_boundary..next_boundary];

        // We are going to _further_ chunk things, based on max_chunk_len.
        for chunk_data in region_data.chunks(max_chunk_len) {
            sw.write_all(chunk_data)?;
            sw.end_chunk()?;
            total_bytes_written += chunk_data.len();
        }

        previous_boundary = next_boundary;
    }

    assert_eq!(
        total_bytes_written,
        stream_data.len(),
        "expected to write the correct number of bytes"
    );

    Ok(())
}

fn parse_bytes(mut bytes_str: &str) -> anyhow::Result<u32> {
    let mut units: u32 = 1;

    if let Some(s) = bytes_str.strip_suffix(['k', 'K']) {
        units = 1024;
        bytes_str = s;
    }

    if let Some(s) = bytes_str.strip_suffix(['m', 'M']) {
        units = 1048576;
        bytes_str = s;
    }

    let n: u32 = bytes_str.parse()?;
    if let Some(n_scaled) = n.checked_mul(units) {
        Ok(n_scaled)
    } else {
        bail!("Size is too large")
    }
}

```

`pdbtool/src/save.rs`:

```rs
use anyhow::bail;
use ms_pdb::{Pdb, Stream, names::NAMES_STREAM_NAME};
use std::ops::Range;
use std::path::Path;

#[derive(clap::Parser)]
pub struct SaveStreamOptions {
    /// The PDB file to read.
    pdb: String,

    /// The index or name of the stream to save. Name can be one of: pdb, dbi, gsi, gss, tpi, ipi
    stream: String,

    /// The path to save the stream to.
    out: String,
}

pub fn save_stream(options: &SaveStreamOptions) -> anyhow::Result<()> {
    let reader = Pdb::open(Path::new(&options.pdb))?;

    // Support saving substreams of just a handful of streams. This could be made more general.
    if options.stream == "dbi/sources" {
        let dbi = reader.read_dbi_stream()?;
        std::fs::write(&options.out, dbi.source_info())?;
        return Ok(());
    }

    if options.stream == "dbi/section_contributions" {
        let dbi = reader.read_dbi_stream()?;
        std::fs::write(&options.out, dbi.section_contributions_bytes())?;
        return Ok(());
    }

    let (stream_index, stream_range_opt) = get_stream_index(&reader, &options.stream)?;
    let stream_data = reader.read_stream_to_vec(stream_index)?;
    let stream_slice = if let Some(stream_range) = stream_range_opt {
        if let Some(s) = stream_data.get(stream_range.clone()) {
            s
        } else {
            bail!(
                "The stream range 0x{:x}..0x{:x} is out of range. Stream length = 0x{:x}.",
                stream_range.start,
                stream_range.end,
                stream_data.len()
            );
        }
    } else {
        stream_data.as_slice()
    };
    std::fs::write(&options.out, stream_slice)?;
    Ok(())
}

pub fn get_stream_index(reader: &Pdb, name: &str) -> anyhow::Result<(u32, Option<Range<usize>>)> {
    if let Ok(stream_index) = name.parse::<u32>() {
        return Ok((stream_index, None));
    }

    if let Some(i) = get_fixed_stream(name) {
        return Ok((i, None));
    }

    let dbi_header = reader.dbi_header();

    if let Some(suffix) = name.strip_prefix("named:") {
        if let Some(s) = reader.named_streams().get(suffix) {
            return Ok((s, None));
        } else {
            bail!("There is no named stream with that name.");
        }
    }

    if let Some(suffix) = name.strip_prefix("mod:") {
        let index: usize = suffix.parse()?;
        let modules = reader.read_modules()?;
        if let Some(module) = modules.iter().nth(index) {
            if let Some(s) = module.stream() {
                return Ok((s, None));
            } else {
                bail!("Module {} does not have a stream.", index);
            }
        } else {
            bail!("Module index {} is out of valid range.", index);
        }
    }

    Ok(match name {
        "gss" => (dbi_header.sym_record_stream()?, None),
        "psi" => (dbi_header.public_stream_index()?, None),
        "gsi" => (dbi_header.global_stream_index()?, None),
        "names" => (reader.named_stream_err(NAMES_STREAM_NAME)?, None),
        "dbi/sources" => (Stream::DBI.into(), Some(dbi_header.sources_range()?)),
        "dbi/modules" => (Stream::DBI.into(), Some(dbi_header.modules_range()?)),
        _ => {
            if let Some(s) = reader.named_streams().get(name) {
                return Ok((s, None));
            }
            bail!("The name '{}' does not identify any known stream.", name);
        }
    })
}

fn get_fixed_stream(name: &str) -> Option<u32> {
    match name {
        "pdb" => Some(Stream::PDB.into()),
        "dbi" => Some(Stream::DBI.into()),
        "tpi" => Some(Stream::TPI.into()),
        "ipi" => Some(Stream::IPI.into()),
        _ => None,
    }
}

```

`pdbtool/src/util.rs`:

```rs
use std::str::FromStr;

use bitvec::prelude::BitSlice;

#[allow(dead_code)] // useful
pub fn dump_bitvec<T, O, W: std::fmt::Write + ?Sized>(
    b: &BitSlice<T, O>,
    out: &mut W,
) -> std::fmt::Result
where
    T: bitvec::prelude::BitStore,
    O: bitvec::prelude::BitOrder,
{
    let mut prev = None;
    for i in b.iter_ones() {
        if let Some((start, end)) = &mut prev {
            if *end + 1 == i {
                *end = i;
                continue;
            }
            if *start != *end {
                write!(out, "{}-{} ", *start, *end)?;
            } else {
                write!(out, "{} ", *start)?;
            }
            prev = None;
        } else {
            prev = Some((i, i));
        }
    }

    if let Some((start, end)) = prev {
        if start != end {
            write!(out, "{start}-{end} ")?;
        } else {
            write!(out, "{start} ")?;
        }
    }

    Ok(())
}

#[derive(Copy, Clone, Eq, PartialEq, Ord, PartialOrd, Hash)]
pub struct HexU64(pub u64);

impl FromStr for HexU64 {
    type Err = <u64 as FromStr>::Err;

    fn from_str(s: &str) -> Result<Self, Self::Err> {
        let value: u64 = if let Some(suffix) = s.strip_prefix("0x") {
            u64::from_str_radix(suffix, 0x10)?
        } else if let Some(suffix) = s.strip_prefix("0X") {
            u64::from_str_radix(suffix, 0x10)?
        } else {
            u64::from_str(s)?
        };
        Ok(Self(value))
    }
}

```

`pdbtool/tests/encode.rs`:

```rs
use ms_pdb::dbi::{DBI_STREAM_HEADER_LEN, DBI_STREAM_VERSION_V110, DbiStreamHeader};
use ms_pdb::msf::Msf;
use ms_pdb::msfz::Msfz;
use ms_pdb::{Stream, StreamIndexU16};
use std::io::Write;
use std::path::Path;
use std::process::Command;
use zerocopy::{FromBytes, IntoBytes};

const TMP_DIR: &str = env!("CARGO_TARGET_TMPDIR");
const PDBTOOL: &str = env!("CARGO_BIN_EXE_pdbtool");

#[track_caller]
fn run_command(mut cmd: Command) {
    let mut s = String::new();
    s.push_str(cmd.get_program().to_str().unwrap());
    for arg in cmd.get_args() {
        s.push(' ');
        s.push_str(arg.to_str().unwrap());
    }

    println!("Running: {s}");

    let status = cmd.status().expect("Failed to execute command");

    if !status.success() {
        panic!("Command failed: {}", status.code().unwrap());
    }

    println!();
}

#[test]
fn min_size_workaround() {
    let dir = Path::new(TMP_DIR).join("min_size_workaround");
    _ = std::fs::create_dir_all(&dir);

    let pdb_file_name = dir.join("test.pdb");
    let pdz_file_name = dir.join("test.pdz");

    // Create a very small test.pdb file
    {
        let mut pdb = Msf::create(&pdb_file_name, Default::default()).unwrap();
        let mut sw = pdb.write_stream(10).unwrap();
        sw.write_all_at_mut(b"Hello, world!", 0).unwrap();
        pdb.commit().unwrap();
    }

    // Directly read the PDB file
    {
        let pdb = Msf::open(&pdb_file_name).unwrap();
        let stream_contents = pdb.read_stream_to_vec(10).unwrap();
        assert_eq!(stream_contents.as_slice(), b"Hello, world!");
    }

    // Convert the PDB file to a PDZ file.
    {
        let mut cmd = Command::new(PDBTOOL);
        cmd.arg("pdz-encode");
        cmd.arg(&pdb_file_name);
        cmd.arg(&pdz_file_name);
        cmd.arg("--pad16k");
        assert!(cmd.status().unwrap().success());
    }

    // Read the PDZ file and verify the contents of the stream.
    {
        let pdz = Msfz::open(&pdz_file_name).unwrap();
        let stream_contents = pdz.read_stream(10).unwrap();
        assert_eq!(stream_contents.as_slice(), b"Hello, world!");
    }
}

fn make_good_dbi() -> Vec<u8> {
    // Make up some subsections.  The \0s are there to pad to multiples of 4.
    let module_info = b"I am the module info!\0\0\0";
    let section_contributions = b"This is totally a section contributions substream!\0\0";
    let section_map = b"What if this were a section map?";
    let source_info = b"It would be awesome if this was a source info substream.";
    let type_server_map = b"If I were a type server map, where would I be?\0\0";
    let optional_dbg_header = b"You get a debug header! And you get a debug header!\0";
    let edit_and_continue = b"I can barely edit at all, much less edit and continue!\0\0";

    // Make up a reasonable DBI stream header.
    let good_dbi_header = DbiStreamHeader {
        signature: (-1).into(),
        version: DBI_STREAM_VERSION_V110.into(),
        age: 1.into(),
        global_symbol_index_stream: StreamIndexU16::NIL,
        build_number: 0.into(),
        public_symbol_index_stream: StreamIndexU16::NIL,
        pdb_dll_version: 0.into(),
        global_symbol_stream: StreamIndexU16::NIL,
        pdb_dll_rbld: 0.into(),
        mod_info_size: (module_info.len() as i32).into(),
        section_contribution_size: (section_contributions.len() as i32).into(),
        section_map_size: (section_map.len() as i32).into(),
        source_info_size: (source_info.len() as i32).into(),
        type_server_map_size: (type_server_map.len() as i32).into(),
        mfc_type_server_index: 0.into(),
        optional_dbg_header_size: (optional_dbg_header.len() as i32).into(),
        edit_and_continue_size: (edit_and_continue.len() as i32).into(),
        flags: 0.into(),
        machine: 0.into(),
        padding: 0.into(),
    };

    let mut good_dbi: Vec<u8> = Vec::new();
    good_dbi.extend_from_slice(good_dbi_header.as_bytes());
    good_dbi.extend_from_slice(module_info);
    good_dbi.extend_from_slice(section_contributions);
    good_dbi.extend_from_slice(section_map);
    good_dbi.extend_from_slice(source_info);
    good_dbi.extend_from_slice(type_server_map);
    good_dbi.extend_from_slice(optional_dbg_header);
    good_dbi.extend_from_slice(edit_and_continue);

    good_dbi
}

// Test roundtrip with a PDB with a reasonable DBI stream
#[test]
fn dbi_good() {
    let dbi = make_good_dbi();
    dbi_case("good", &dbi);
}

// Test roundtrip with a DBI that is too small to be valid.
#[test]
fn dbi_header_too_small() {
    dbi_case("dbi_header_too_small", b"this is bad");
}

// Test roundtrip with a DBI that has a valid header, but is cut off within the Module Info substream.
#[test]
fn dbi_cut_off_in_module_info() {
    let dbi = make_good_dbi();
    dbi_case(
        "dbi_cut_off_in_module_info",
        &dbi[..DBI_STREAM_HEADER_LEN + 10],
    );
}

#[test]
fn dbi_cut_off_in_section_contributions() {
    let dbi = make_good_dbi();
    let header = DbiStreamHeader::ref_from_prefix(&dbi).unwrap().0;
    dbi_case(
        "dbi_cut_off_in_section_contributions",
        &dbi[..DBI_STREAM_HEADER_LEN + header.mod_info_size.get() as usize + 4],
    );
}

#[test]
fn dbi_cut_off_in_section_map() {
    let dbi = make_good_dbi();
    let header = DbiStreamHeader::ref_from_prefix(&dbi).unwrap().0;
    dbi_case(
        "dbi_cut_off_in_section_map",
        &dbi[..DBI_STREAM_HEADER_LEN
            + header.mod_info_size.get() as usize
            + header.section_contribution_size.get() as usize
            + 4],
    );
}

// Test roundtrip with only a header
#[test]
fn dbi_only_header() {
    let dbi = make_good_dbi();
    dbi_case("dbi_only_header", &dbi[..DBI_STREAM_HEADER_LEN]);
}

// Poke some bogus values into length fields
#[test]
fn dbi_negative_module_info() {
    let mut dbi = make_good_dbi();
    let header = DbiStreamHeader::mut_from_prefix(&mut dbi).unwrap().0;
    header.mod_info_size = (-4).into();
    dbi_case("dbi_negative_module_info", &dbi);
}

/// Write out a PDB with the DBI contents provided, convert it to PDZ, read the data back,
/// verify we got the same data.
#[inline(never)]
#[track_caller]
fn dbi_case(name: &str, original_dbi_data: &[u8]) {
    println!(
        "dbi_case: DBI stream length: {0} 0x{0:x}",
        original_dbi_data.len()
    );

    let dir = Path::new(TMP_DIR).join("dbi_chunking");
    _ = std::fs::create_dir_all(&dir);

    let pdb_file_name = dir.join(format!("{name}.pdb"));
    let pdz_file_name = dir.join(format!("{name}.pdz"));

    {
        let mut pdb = Msf::create(&pdb_file_name, Default::default()).unwrap();
        let mut sw = pdb.write_stream(Stream::DBI.into()).unwrap();
        sw.write_all(original_dbi_data).unwrap();
        pdb.commit().unwrap();
    }

    // Convert it to a PDZ
    let mut cmd = Command::new(PDBTOOL);
    cmd.arg("pdz-encode");
    cmd.arg(&pdb_file_name);
    cmd.arg(&pdz_file_name);
    run_command(cmd);

    // Open the PDZ and verify a few things
    let pdz = Msfz::open(&pdz_file_name).unwrap();
    let readback_dbi_data = pdz.read_stream(Stream::DBI.into()).unwrap();

    // Don't use assert_eq!() here; the data is too large and the debug display is not useful.
    assert!(readback_dbi_data.as_slice() == original_dbi_data);
}

```