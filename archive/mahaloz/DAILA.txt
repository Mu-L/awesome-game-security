Project Path: arc_mahaloz_DAILA_47u7cifr

Source Tree:

```txt
arc_mahaloz_DAILA_47u7cifr
├── Dockerfile
├── LICENSE
├── README.md
├── assets
│   ├── am_daila.png
│   ├── binja_daila.png
│   ├── ghidra_daila.png
│   └── ida_daila.png
├── dailalib
│   ├── __init__.py
│   ├── __main__.py
│   ├── api
│   │   ├── __init__.py
│   │   ├── ai_api.py
│   │   └── litellm
│   │       ├── __init__.py
│   │       ├── config_dialog.py
│   │       ├── litellm_api.py
│   │       ├── prompt_type.py
│   │       └── prompts
│   │           ├── __init__.py
│   │           ├── cot_prompts.py
│   │           ├── few_shot_prompts.py
│   │           └── prompt.py
│   ├── binsync_plugin
│   │   ├── __init__.py
│   │   ├── ai_bs_user.py
│   │   ├── ai_user_config_ui.py
│   │   ├── openai_bs_user.py
│   │   └── varmodel_bs_user.py
│   ├── configuration.py
│   ├── daila_plugin.py
│   ├── installer.py
│   ├── llm_chat
│   │   ├── __init__.py
│   │   ├── binja.py
│   │   ├── ida.py
│   │   └── llm_chat_ui.py
│   └── plugin.toml
├── setup.cfg
├── setup.py
└── tests.py

```

`Dockerfile`:

```
FROM --platform=linux/amd64 ubuntu:22.04
ENV DEBIAN_FRONTEND=noninteractive

# install java, python, and some utils
RUN apt-get update && apt-get -o APT::Immediate-Configure=0 install -y \
      python3-dev python3-pip openjdk-17-jdk unzip sed wget \
    && rm -rf /var/lib/apt/lists/*

# install ghidra 10.4 and patch the launch script to work in containers
RUN mkdir tools && mkdir /root/ghidra_scripts/
ENV PATH "/tools/:$PATH"
WORKDIR tools
RUN wget https://github.com/NationalSecurityAgency/ghidra/releases/download/Ghidra_11.1_build/ghidra_11.1_PUBLIC_20240607.zip && \
    unzip ghidra_11.1_PUBLIC_20240607.zip && \
    sed -i 's/java -cp/java -Djdk.lang.Process.launchMechanism=vfork -cp/g' /tools/ghidra_11.1_PUBLIC/support/launch.sh

# copy the local pip project, install it, its plugins, and the models
workdir /
COPY . /DAILA/
RUN pip3 install --upgrade pip \
    && pip3 install -e ./DAILA[full] \
    && daila --single-decompiler-install ghidra /root/ghidra_scripts/


```

`LICENSE`:

```
Copyright (c) 2021, Zion Leonahenahe Basque
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above copyright notice,
  this list of conditions and the following disclaimer in the documentation
  and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

```

`README.md`:

```md
# DAILA 
The Decompiler Artificially Intelligent Language Assistant (DAILA) is a unified interface for AI systems to be used in decompilers.
With DAILA, you can utilize various AI systems, like local and remote LLMs, all in the same scripting and GUI interfaces across many decompilers.
DAILA was featured in the keynote talk at [HITCON CMT 2023](https://youtu.be/HbrebQiFLDs?si=TJhKSju85jTLSsiT) and
was studied in the NDSS 2026 work ["Decompiling the Synergy: An Empirical Study of Human–LLM Teaming in Software Reverse Engineering"](https://www.zionbasque.com/files/papers/dec-synergy-study.pdf).
If you are looking for the paper repo, find it [here](https://github.com/mahaloz/dec-synergy-study).

![](./assets/ida_daila.png)

**Join our discord below for more online help** (hosted on the BinSync server):

[![Discord](https://img.shields.io/discord/900841083532087347?label=Discord&style=plastic)](https://discord.gg/wZSCeXnEvR)


## Supported Decompilers and AI Systems
DAILA interacts with the decompiler abstractly through the [LibBS](https://github.com/binsync/libbs) library.
This allows DAILA to support the following decompilers:
- IDA Pro: **>= 8.4**
- Ghidra: **>= 12.0**
- Binary Ninja: **>= 2.4**
- angr-management: **>= 9.0**

DAILA supports any LLM supported in [LiteLLM](https://github.com/BerriAI/litellm), such as:
- ChatGPT
- Claude
- Llama2
- Gemini
- and more...

DAILA also supports local models of different types, like [VarBERT](https://github.com/binsync/varbert_api), a local model for renaming variables in decompilation published in S&P 2024.

## Installation
Install our library backend through pip and our decompiler plugin through our installer:
```bash
pip3 install dailalib && daila --install 
```

This is the light mode. **If you want to use VarBERT, you must install the full version**:
```bash
pip3 install 'dailalib[full]' && daila --install 
```

This will also download the VarBERT models for you through the [VarBERT API](https://github.com/binsync/varbert_api).
If you happen to be installing DAILA on a machine that won't have internet access, like a secure network, you can use our Docker image in the [Docker Container](#docker-container) section.

### Ghidra Extra Steps
You need to do a few extra steps to get Ghidra working. First, you must be running in PyGhidra mode. You can do this
by going to your Ghidra install directory and running `./support/pyghidraRun`.

Next, enable the DAILA plugin:
1. Start Ghidra and open a binary
2. Goto the `Windows > Script Manager` menu
3. Search for `daila` and enable the script

You must have `python3` in your path for the Ghidra version to work. We quite literally call it from inside Python 2.
You may also need to enable the `$USER_HOME/ghidra_scripts` as a valid scripts path in Ghidra.

### Manual Install (if above fails)
If the above fails, you will need to manually install.
To manually install, first `pip3 install dailalib` on the repo, then copy the [daila_plugin.py](./dailalib/daila_plugin.py) file to your decompiler's plugin directory.


## Usage
DAILA is designed to be used in two ways:
1. As a decompiler plugin with a GUI
2. As a scripting library in your decompiler

### Decompiler GUI
With the exception of Ghidra (see below), when you start your decompiler you will have a new context menu 
which you can access when you right-click anywhere in a function:

<img src="./assets/ida_daila.png" style="width: 50%;" alt="DAILA context menu"/>

If you are using Ghidra, go to `Tools->DAILA->Start DAILA Backend` to start the backend server.
After you've done this, you can use the context menu as shown above.

### Scripting
You can use DAILA in your own scripts by importing the `dailalib` package.
Here is an example using the OpenAI API:

```python
from dailalib import LiteLLMAIAPI
from libbs.api import DecompilerInterface

deci = DecompilerInterface.discover()
ai_api = LiteLLMAIAPI(decompiler_interface=deci)
for function in deci.functions:
    summary = ai_api.summarize_function(function)
```

### Docker Container
If you are attempting to install DAILA for a one-shot install that will not use the internet after install, like on a secure network, you can use our Docker container.
You should either build the container yourself, save the image to a tarball, and then load it on the target machine, or you can use our pre-built image.
You can build the container yourself by running `docker build . -t daila` in the root of this repo.
You can also download our pre-built image by running `docker pull mahaloz/daila:latest` (the image is for x86_64 Linux).
The container contains DAILA and a copy of Ghidra.

Now you need to foward X11 to the container so that you can see the GUI.
To do this, you need to run the container with the following flags:
```bash
docker run -it --rm -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix mahaloz/daila:latest
```

In the container, you can launch ghidra from `/tools/ghidra_10.4_PUBLIC/ghidraRun`.
Now follow the [Ghidra Extra Steps](#ghidra-extra-steps) to enable the DAILA plugin and you're good to go!

## Supported AI Backends
### LiteLLM (many backends)
DAILA supports the LiteLLM API, which in turn supports various backends like OpenAI. 
To use a commercial LLM API, you must provide your own API key.
As an example, to use the OpenAI API, you must have an OpenAI API key.
If your decompiler does not have access to the `OPENAI_API_KEY` environment variable, then you must use the decompiler option from
DAILA to set the API key.

In `Settings`, you can also add/use any OpenAI-based LLM endpoint, like using Llama2.

Currently, DAILA supports the following prompts:
- Summarize a function
- Rename variables
- Rename function
- Identify the source of a function
- Find potential vulnerabilities in a function
- Summarize the man page of a library call
- Free prompting... just type in your own prompt!

### VarBERT
VarBERT is a local BERT model from the S&P 2024 paper [""Len or index or count, anything but v1": Predicting Variable Names in Decompilation Output with Transfer Learning"]().
VarBERT is for renaming variables (both stack, register, and arguments) in decompilation.
To understand how to use VarBERT as a library, please see the [VarBERT API](https://github.com/binsync/varbert_api) documentation.
Using it in DAILA is a simple as using the GUI context-menu when clicking on a function. 

## Demo
You can find a demo of VarBERT running inside DAILA below:

[![VarBERT Demo](https://img.youtube.com/vi/nUazQm8sFL8/0.jpg)](https://youtu.be/nUazQm8sFL8 "DAILA v2.1.4: Renaming variables with local VarBERT model")


## Supported Decompilers
- IDA
![](./assets/ida_daila.png)

- Binja
![](./assets/binja_daila.png)

- Ghidra
![](./assets/ghidra_daila.png)

- angr management
![](./assets/am_daila.png)

```

`dailalib/__init__.py`:

```py
__version__ = "3.19.0"

import os
# stop LiteLLM from querying at all to the remote server
# https://github.com/BerriAI/litellm/blob/4d29c1fb6941e49191280c4fd63961dec1a1e7c5/litellm/__init__.py#L286C20-L286C48
os.environ["LITELLM_LOCAL_MODEL_COST_MAP"] = "True"

from .api import AIAPI, LiteLLMAIAPI

from dailalib.llm_chat import get_llm_chat_creator


def create_plugin(*args, **kwargs):
    from libbs.api import DecompilerInterface 
    #
    # LLM API (through LiteLLM api)
    #

    litellm_api = LiteLLMAIAPI(delay_init=True)

    # load config before creating context menus. if not, the config is only be load after "OK" button is
    # clicked in ask_settings :X
    litellm_api.load_or_create_config()

    # create context menus for prompts
    gui_ctx_menu_actions = {
        f"DAILA/LLM/{prompt_name}": (prompt.desc, getattr(litellm_api, prompt_name))
        for prompt_name, prompt in litellm_api.prompts_by_name.items()
    }
    # create context menu for llm chat
    gui_ctx_menu_actions["DAILA/LLM/chat"] = ("Open LLM Chat...", get_llm_chat_creator(litellm_api))

    # create context menus for others
    gui_ctx_menu_actions["DAILA/LLM/Settings"] = ("Settings...", litellm_api.ask_settings)
    #
    # VarModel API (local variable renaming)
    #

    VARBERT_AVAILABLE = True
    try:
        import varbert
    except ImportError:
        VARBERT_AVAILABLE = False

    var_api = None
    if VARBERT_AVAILABLE:
        from varbert.api import VariableRenamingAPI
        var_api = VariableRenamingAPI(delay_init=True)

        # add single interface, which is to rename variables
        def make_callback(predict_for_all_variables):
            return lambda *args, **kwargs: var_api.query_model(**kwargs, remove_bad_names=not predict_for_all_variables)

        gui_ctx_menu_actions["DAILA/VarBERT/varbert_rename_vars"] = ("Suggest new variable names (source-like only)", make_callback(predict_for_all_variables=False))
        gui_ctx_menu_actions["DAILA/VarBERT/varbert_rename_vars_all"] = ("Suggest new variable names (for all variables)", make_callback(predict_for_all_variables=True))

    #
    # Decompiler Plugin Registration
    #

    force_decompiler = kwargs.pop("force_decompiler", None)
    deci = DecompilerInterface.discover(
        force_decompiler=force_decompiler,
        # decompiler-creation args
        plugin_name="DAILA",
        init_plugin=True,
        gui_ctx_menu_actions=gui_ctx_menu_actions,
        gui_init_args=args,
        gui_init_kwargs=kwargs
    )
    if not VARBERT_AVAILABLE:
        deci.info("VarBERT not installed, reinstall with `pip install dailalib[full]` to enable local models.")

    deci.info("DAILA backend loaded! Initializing context menus now...")

    litellm_api.init_decompiler_interface(decompiler_interface=deci)
    if var_api is not None:
        var_api.init_decompiler_interface(decompiler_interface=deci)

    return deci.gui_plugin

```

`dailalib/__main__.py`:

```py
import argparse

from .installer import DAILAInstaller
import dailalib


def main():
    parser = argparse.ArgumentParser(
        description="""
            The DAILA CLI is used to install, run, and host the DAILA plugin.
            """,
        epilog="""
            Examples:
            daila install
            """
    )
    parser.add_argument(
        "-i", "--install", action="store_true", help="Install DAILA into your decompilers"
    )
    parser.add_argument(
        "--single-decompiler-install", nargs=2, metavar=('decompiler', 'path'), help="Install DAILA into a single decompiler. Decompiler must be one of: ida, ghidra, binja, angr."
    )
    parser.add_argument(
        "-s", "--server", help="Run a a headless server for DAILA", choices=["ghidra"]
    )
    parser.add_argument(
        "-v", "--version", action="version", version=f"{dailalib.__version__}"
    )
    args = parser.parse_args()

    if args.single_decompiler_install:
        decompiler, path = args.single_decompiler_install
        DAILAInstaller().install(interactive=False, paths_by_target={decompiler: path})
    elif args.install:
        DAILAInstaller().install()
    elif args.server:
        if args.server != "ghidra":
            raise NotImplementedError("Only Ghidra is supported for now")

        from dailalib import create_plugin
        create_plugin(force_decompiler="ghidra")


if __name__ == "__main__":
    main()

```

`dailalib/api/__init__.py`:

```py
from .ai_api import AIAPI
from .litellm import LiteLLMAIAPI

```

`dailalib/api/ai_api.py`:

```py
from typing import Optional
from functools import wraps
import threading

from libbs.api import DecompilerInterface


class AIAPI:
    def __init__(
        self,
        decompiler_interface: Optional[DecompilerInterface] = None,
        decompiler_name: Optional[str] = None,
        use_decompiler: bool = True,
        delay_init: bool = False,
        query_callbacks=None,
        # size in bytes
        min_func_size: int = 0x10,
        max_func_size: int = 0xffff,
        model=None,
    ):
        # useful for initing after the creation of a decompiler interface
        self._dec_interface: DecompilerInterface = None
        self._dec_name = None
        self._delay_init = delay_init
        self.query_callbacks = query_callbacks or []
        if not self._delay_init:
            self.init_decompiler_interface(decompiler_interface, decompiler_name, use_decompiler)

        self._min_func_size = min_func_size
        self._max_func_size = max_func_size
        self.model = model or self.__class__.__name__

    def init_decompiler_interface(
        self,
        decompiler_interface: Optional[DecompilerInterface] = None,
        decompiler_name: Optional[str] = None,
        use_decompiler: bool = True
    ):
        self._dec_interface: DecompilerInterface = DecompilerInterface.discover(force_decompiler=decompiler_name) \
            if use_decompiler and decompiler_interface is None else decompiler_interface
        self._dec_name = decompiler_name if decompiler_interface is None else decompiler_interface.name
        if self._dec_interface is None and not self._dec_name:
            raise ValueError("You must either provide a decompiler name or a decompiler interface.")

    def info(self, msg):
        if self._dec_interface is not None:
            self._dec_interface.info(msg)

    def debug(self, msg):
        if self._dec_interface is not None:
            self._dec_interface.debug(msg)

    def warning(self, msg):
        if self._dec_interface is not None:
            self._dec_interface.warning(msg)

    def error(self, msg):
        if self._dec_interface is not None:
            self._dec_interface.error(msg)

    @property
    def has_decompiler_gui(self):
        return self._dec_interface is not None and not self._dec_interface.headless

    @staticmethod
    def requires_function(f):
        """
        A wrapper function to make sure an API call has decompilation text to operate on and possibly a Function
        object. There are really two modes any API call operates in:
        1. Without Decompiler Backend: requires provided dec text
        2. With Decompiler Backend:
               2a. With UI: Function will be collected from the UI if not provided
               2b. Without UI: requires a FunctionA

        The Function collected from the UI is the one the use is currently looking at.
        """
        @wraps(f)
        def _requires_function(*args, ai_api: "AIAPI" = None, **kwargs):
            function = kwargs.pop("function", None)
            dec_text = kwargs.pop("dec_text", None)
            use_dec = kwargs.pop("use_dec", True)
            has_self = kwargs.pop("has_self", True)
            number_lines = kwargs.pop("number_lines", False)
            context = kwargs.pop("context", None)
            # make the self object the new AI API, should only be used inside an AIAPI class
            if not ai_api and has_self:
                ai_api = args[0]

            if not dec_text and not use_dec:
                raise ValueError("You must provide decompile text if you are not using a dec backend")

            # two mode constructions: with decompiler and without
            # with decompiler backend
            if use_dec:
                if not ai_api.has_decompiler_gui and function is None:
                    raise ValueError("You must provide a Function when using this with a decompiler")

                # we must have a UI if we have no func
                if function is None:
                    if context is None:
                        context = ai_api._dec_interface.gui_active_context()
                    function = ai_api._dec_interface.functions[context.func_addr]

                # get new text with the function that is present
                if dec_text is None:
                    decompilation = ai_api._dec_interface.decompile(function.addr)
                    if decompilation is None:
                        raise ValueError("Decompilation failed, cannot continue")

                    dec_text = decompilation.text

                if number_lines:
                    # put a number in front of each line
                    dec_lines = dec_text.split("\n")
                    dec_text = "\n".join([f"{i + 1} {line}" for i, line in enumerate(dec_lines)])

            return f(*args, function=function, dec_text=dec_text, use_dec=use_dec, **kwargs)

        return _requires_function

    def on_query(self, query_name, model, prompt_style, function, decompilation, response, **kwargs):
        for func in self.query_callbacks:
            t = threading.Thread(
                target=func, args=(query_name, model, prompt_style, function, decompilation, response), kwargs=kwargs
            )
            t.start()

```

`dailalib/api/litellm/__init__.py`:

```py
DEFAULT_MODEL = "gpt-4o"
OPENAI_MODELS = {"gpt-4", "gpt-4o", "gpt-4-turbo", "gpt-3.5-turbo", "o1-mini", "o1-preview"}
# TODO: How can I get this MODEL_TO_TOKENS in the future, without hardcopy to `configuration`
MODEL_TO_TOKENS = {
    # TODO: update the token values for o1
    "o1-mini": 8_000,
    "o1-preview": 8_000,
    "gpt-4o": 8_000,
    "gpt-4o-mini": 16_000,
    "gpt-4-turbo": 128_000,
    "claude-3-5-sonnet-20240620": 200_000,
    "gemini/gemini-2.0-flash": 1_000_000,
    "vertex_ai_beta/gemini-2.0-flash": 1_000_000,
    # perplex is on legacy mode :(
    "perplexity/llama-3.1-sonar-small-128k-online": 127_072,
    "perplexity/llama-3.1-sonar-medium-128k-online": 127_072,
    "perplexity/llama-3.1-sonar-large-128k-online": 127_072,
    "sonar-pro": 127_072,
    "sonar": 127_072,
}

LLM_COST = {
    "gpt-4o": {"prompt_price": 2.5, "completion_price": 10},
    "gpt-4o-mini": {"prompt_price": 0.150, "completion_price": 0.600},
    "gpt-4-turbo": {"prompt_price": 10, "completion_price": 30},
    "claude-3.5-sonnet-20240620": {"prompt_price": 3, "completion_price": 15},
    "gemini/gemini-2.0-flash": {"prompt_price": 0.10, "completion_price": 0.4},
    "vertex_ai_beta/gemini-2.0-flash": {"prompt_price": 0.10, "completion_price": 0.4},
    # perplex is on legacy mode not available from 02/22/25:(
    "perplexity/llama-3.1-sonar-small-128k-online": {"prompt_price": 0.150, "completion_price": 0.600},
    "perplexity/llama-3.1-sonar-large-128k-online": {"prompt_price": 0.150, "completion_price": 0.600},
    "perplexity/llama-3.1-sonar-huge-128k-online": {"prompt_price": 0.150, "completion_price": 0.600},
    # introduced the new sonar-pro/sonar
    "sonar": {"prompt_price": 0.150, "completion_price": 0.600},
    "sonar-pro": {"prompt_price": 0.150, "completion_price": 0.600},
}

# delay import for const creation
from .litellm_api import LiteLLMAIAPI
from .prompt_type import PromptType, ALL_STYLES, DEFAULT_STYLE


```

`dailalib/api/litellm/config_dialog.py`:

```py
import logging
from typing import Optional

from dailalib.configuration import DAILAConfig
from .prompt_type import ALL_STYLES
from . import MODEL_TO_TOKENS

from libbs.ui.qt_objects import (
    QDialog,
    QGridLayout,
    QHBoxLayout,
    QLabel,
    QLineEdit,
    QPushButton,
    QVBoxLayout,
    QComboBox,
)

_l = logging.getLogger(__name__)
AVAILABLE_MODELS = MODEL_TO_TOKENS.keys()


class DAILAConfigDialog(QDialog):
    TITLE = "DAILA Configuration"

    def __init__(self, config: DAILAConfig, parent=None):
        """
        Constructor for the DAILA configuration dialog.
        params: 
        + config: config object, passed from litellm_api when calling this dialog
        """

        super().__init__(parent)
        self.configured = False 
        self.DAILAConfig = config

        self.setWindowTitle(self.TITLE)
        self._main_layout = QVBoxLayout()
        self._grid_layout = QGridLayout()
        self.row = 0

        self._init_middle_widgets()
        self._main_layout.addLayout(self._grid_layout)

        self._init_close_btn_widgets()

        self.setLayout(self._main_layout)
        
    def _init_middle_widgets(self):
        """ 
        """

        # LLM Model 
        llm_model = self.DAILAConfig.model
        llm_model_label = QLabel("LLM Model:")
        llm_model_label.setToolTip("The model to use for LiteLLM.")

        # using dropdown for LLM model
        self._llm_model_edit = QComboBox(self)
        self._llm_model_edit.addItems(AVAILABLE_MODELS)
        self._llm_model_edit.setCurrentText(llm_model)
        self._grid_layout.addWidget(llm_model_label, self.row, 0)
        self._grid_layout.addWidget(self._llm_model_edit, self.row, 1)
        self.row += 1

        # API Key 

        api_key = self.DAILAConfig.api_key
        api_key_label = QLabel("API Key:")
        api_key_label.setToolTip("The API key to use for LiteLLM, for the selected model.")
        self._api_key_edit = QLineEdit(self)
        self._api_key_edit.setText(api_key)
        self._grid_layout.addWidget(api_key_label, self.row, 0)
        self._grid_layout.addWidget(self._api_key_edit, self.row, 1)
        self.row += 1

        # Prompt Style

        prompt_style = self.DAILAConfig.prompt_style
        prompt_style_label = QLabel("Prompt Style:")
        prompt_style_label.setToolTip("The prompt style for DAILA to use, refer to dailalib/litellm/prompts for details.")
        
        # using dropdown for prompt style
        self._prompt_style_edit = QComboBox(self)
        self._prompt_style_edit.addItems(ALL_STYLES)
        self._prompt_style_edit.setCurrentText(prompt_style)
        self._grid_layout.addWidget(prompt_style_label, self.row, 0)
        self._grid_layout.addWidget(self._prompt_style_edit, self.row, 1)
        self.row += 1

        # Custom OpenAI Endpoint

        custom_endpoint = self.DAILAConfig.custom_endpoint
        custom_endpoint_label = QLabel("Custom OpenAI Endpoint:")
        custom_endpoint_label.setToolTip("The custom OpenAI endpoint to use for LiteLLM.")
        self._custom_endpoint_edit = QLineEdit(self)
        self._custom_endpoint_edit.setText(custom_endpoint)
        self._grid_layout.addWidget(custom_endpoint_label, self.row, 0)
        self._grid_layout.addWidget(self._custom_endpoint_edit, self.row, 1)
        self.row += 1

        # Custom OpenAI Model

        custom_model = self.DAILAConfig.custom_model
        custom_model_label = QLabel("Custom OpenAI Model:")
        custom_model_label.setToolTip("The custom OpenAI model to use for LiteLLM.")
        self._custom_model_edit = QLineEdit(self)
        self._custom_model_edit.setText(custom_model)
        self._grid_layout.addWidget(custom_model_label, self.row, 0)
        self._grid_layout.addWidget(self._custom_model_edit, self.row, 1)
        self.row += 1

    def _init_close_btn_widgets(self):
        # buttons
        self._ok_button = QPushButton(self)
        self._ok_button.setText("OK")
        self._ok_button.setDefault(True)
        self._ok_button.clicked.connect(self._on_ok_clicked)

        cancel_button = QPushButton(self)
        cancel_button.setText("Cancel")
        cancel_button.clicked.connect(self._on_cancel_clicked)

        buttons_layout = QHBoxLayout()
        buttons_layout.addWidget(self._ok_button)
        buttons_layout.addWidget(cancel_button)

        self._main_layout.addLayout(buttons_layout)

    def _on_cancel_clicked(self):
        self.close()
    
    def parse_api_key(self, api_key_or_path: str) -> Optional[str]:
        """
        Parse the API key from the input string.
        """
        if "/" in api_key_or_path or "\\" in api_key_or_path:
            # treat as path
            with open(api_key_or_path, "r") as f:
                api_key = f.read().strip()
        else:
            api_key = api_key_or_path
        return api_key

    def _on_ok_clicked(self):
        self.DAILAConfig.model = self._llm_model_edit.currentText()
        self.DAILAConfig.api_key = self.parse_api_key(self._api_key_edit.text())
        self.DAILAConfig.prompt_style = self._prompt_style_edit.currentText()
        self.DAILAConfig.custom_endpoint = self._custom_endpoint_edit.text()
        self.DAILAConfig.custom_model = self._custom_model_edit.text()
        self.configured = True
        self.close()
        
    def config_dialog_exec(self):
        self.exec()
        if not self.configured: 
            _l.warning("DAILA Configuration dialog was closed without saving changes.")
        else: 
            _l.info("DAILA Configuration dialog was closed and changes were saved.")
        return self.DAILAConfig
```

`dailalib/api/litellm/litellm_api.py`:

```py
from pathlib import Path
from typing import Optional
import os
import logging

import tiktoken

from libbs.decompilers import GHIDRA_DECOMPILER

from . import DEFAULT_MODEL, LLM_COST, OPENAI_MODELS
from ..ai_api import AIAPI
from dailalib.configuration import DAILAConfig

active_model = None
active_prompt_style = None
active_custom_endpoint = None

_l = logging.getLogger(__name__)


class LiteLLMAIAPI(AIAPI):
    prompts_by_name = []

    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = DEFAULT_MODEL,
        prompt_style: str = "few-shot",
        prompts: Optional[list] = None,
        fit_to_tokens: bool = False,
        chat_use_ctx: bool = True,
        chat_event_callbacks: Optional[dict] = None,
        custom_endpoint: Optional[str] = None,
        custom_model: Optional[str] = None,
        use_config: bool = True,
        **kwargs
    ):
        super().__init__(**kwargs)

        self._use_config = use_config
        # default values
        self._api_key = None
        self.model = model
        # default to openai api key if not provided
        if api_key or os.getenv("OPENAI_API_KEY"):
            self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        elif not self._use_config:
            self.api_key = None

        self.prompt_style = prompt_style
        self.fit_to_tokens = fit_to_tokens
        self.chat_use_ctx = chat_use_ctx
        self.chat_event_callbacks = chat_event_callbacks or {"send": None, "receive": None}
        self.custom_endpoint = custom_endpoint
        self.custom_model = custom_model
        self.config = DAILAConfig()
        if use_config:
            loaded = self.load_or_create_config()
            if loaded:
                _l.info("Loaded config file from %s", self.config.save_location)

        # delay prompt import
        from .prompts import PROMPTS
        prompts = prompts + PROMPTS if prompts else PROMPTS
        self.prompts_by_name = {p.name: p for p in prompts}

        # update the globals (for threading hacks)
        global active_model, active_prompt_style, active_custom_endpoint
        active_model = self.model if not self.custom_model else self.custom_model
        active_prompt_style = self.prompt_style
        active_custom_endpoint = self.custom_endpoint

    def load_or_create_config(self, new_config=None) -> bool:
        if new_config:
            self.config = new_config
            self.config.save()

        if self.config.save_location and not Path(self.config.save_location).exists():
            return False

        # load the config
        self.config.load()
        self.model = self.config.model
        self.api_key = self.config.api_key
        self.prompt_style = self.config.prompt_style
        if self.config.custom_endpoint:
            self.custom_endpoint = self.config.custom_endpoint
        if self.config.custom_model:
            self.custom_model = self.config.custom_model
        # update the globals (for threading hacks)
        self._set_model(self.model)
        self._set_prompt_style(self.prompt_style)
        return True

    def __dir__(self):
        return list(super().__dir__()) + list(self.prompts_by_name.keys())

    def __getattribute__(self, item):
        # this is how we can access the prompt functions
        if item in object.__getattribute__(self, "prompts_by_name"):
            prompt_obj: "Prompt" = self.prompts_by_name[item]
            prompt_obj.ai_api = self
            return prompt_obj.query_model
        else:
            return object.__getattribute__(self, item)

    @property
    def api_key(self):
        if not self._api_key or self.model is None:
            return None
        elif self.model in OPENAI_MODELS:
            return os.getenv("OPENAI_API_KEY", None)
        elif "claude" in self.model:
            return os.getenv("ANTHROPIC_API_KEY", None)
        elif "gemini/gemini" in self.model:
            return os.getenv("GEMINI_API_KEY", None)
        elif "sonar" in self.model or "perplexity" in self.model:
            return os.getenv("PERPLEXITY_API_KEY", None)
        elif "vertex" in self.model:
            return self._api_key
        else:
            return None

    @api_key.setter
    def api_key(self, value):
        self._api_key = value
        _l.info(f"API key set to {self.model}")
        if self._api_key and self.model is not None:
            if self.model in OPENAI_MODELS:
                os.environ["OPENAI_API_KEY"] = self._api_key
            elif "claude" in self.model:
                os.environ["ANTHROPIC_API_KEY"] = self._api_key
            elif "gemini/gemini" in self.model:
                os.environ["GEMINI_API_KEY"] = self._api_key
            elif "sonar" in self.model or "perplexity" in self.model:
                os.environ["PERPLEXITY_API_KEY"] = self._api_key
            elif "vertex" in self.model:
                os.environ["VERTEX_API_KEY"] = self._api_key
            else:
                _l.error(f"API key not set for model {self.model}")

    @property
    def custom_model(self):
        return self._custom_model

    @custom_model.setter
    def custom_model(self, value):
        custom_model = value.strip() if isinstance(value, str) else None
        if not custom_model:
            self._custom_model = None
            _l.info(f"Custom model selection cleared, or not in use")
            return
        self._custom_model = "openai/" + custom_model.strip()
        _l.info(f"Custom model set to {self._custom_model}")

    @property
    def custom_endpoint(self):
        return self._custom_endpoint

    @custom_endpoint.setter
    def custom_endpoint(self, value):
        custom_endpoint = value.strip() if isinstance(value, str) else None
        if not custom_endpoint:
            self._custom_endpoint = None
            _l.info(f"Custom endpoint disabled, defaulting to online API")
            return
        if not (custom_endpoint.lower().startswith("http://") or custom_endpoint.lower().startswith("https://")):
            self._custom_endpoint = None
            _l.error("Invalid endpoint format")
            return
        self._custom_endpoint = custom_endpoint.strip()
        _l.info(f"Custom endpoint set to {self._custom_endpoint}")

    def query_model(
        self,
        prompt: str,
        model: Optional[str] = None,
        max_tokens=None,
    ):
        # delay import because litellm attempts to query the server on import to collect cost information.
        from litellm import completion

        if not self.api_key and not self.custom_endpoint:
            raise ValueError(f"Model API key is not set. Please set it before querying the model {self.model}")

        prompt_model = (model or self.model) if not self.custom_endpoint else self.custom_model
        response = completion(
            model=prompt_model,
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=max_tokens,
            timeout=60 if not self.custom_endpoint else 300,
            api_base=self.custom_endpoint if self.custom_endpoint else None,  # Use custom endpoint if set
            api_key=self.api_key if not self.custom_endpoint else "dummy" # In most of cases custom endpoint doesn't need the api_key
        )
        # get the answer
        try:
            answer = response.choices[0].message.content
        except (KeyError, IndexError) as e:
            answer = None

        if self.custom_endpoint:
            return answer, 0

        # get the estimated cost
        try:
            prompt_tokens = response.usage.prompt_tokens
            completion_tokens = response.usage.completion_tokens
        except (KeyError, IndexError) as e:
            prompt_tokens, completion_tokens = None, None
        cost = self.llm_cost(prompt_model, prompt_tokens, completion_tokens) \
            if prompt_tokens is not None and completion_tokens is not None else None

        return answer, cost

    @staticmethod
    def estimate_token_amount(content: str, model=DEFAULT_MODEL):
        # TODO: we only support one token counting for now, gpt-4
        enc = tiktoken.encoding_for_model("gpt-4")
        tokens = enc.encode(content)
        return len(tokens)

    @staticmethod
    def content_fits_tokens(content: str, model=DEFAULT_MODEL):
        max_token_count = LiteLLMAIAPI.MODEL_TO_TOKENS[model]
        token_count = LiteLLMAIAPI.estimate_token_amount(content, model=model)
        return token_count <= max_token_count - 1000

    @staticmethod
    def fit_decompilation_to_token_max(decompilation: str, delta_step=10, model=DEFAULT_MODEL):
        if LiteLLMAIAPI.content_fits_tokens(decompilation, model=model):
            return decompilation

        dec_lines = decompilation.split("\n")
        last_idx = len(dec_lines) - 1
        # should be: [func_prototype] + [nop] + [mid] + [nop] + [end_of_code]
        dec_lines = dec_lines[0:2] + ["// ..."] + dec_lines[delta_step:last_idx-delta_step] + ["// ..."] + dec_lines[-2:-1]
        decompilation = "\n".join(dec_lines)

        return LiteLLMAIAPI.fit_decompilation_to_token_max(decompilation, delta_step=delta_step, model=model)

    @staticmethod
    def llm_cost(model_name: str, prompt_tokens: int, completion_tokens: int) -> float | None:
        # these are the $ per million tokens
        if model_name not in LLM_COST:
            return None

        llm_price = LLM_COST[model_name]
        prompt_price = (prompt_tokens / 1000000) * llm_price["prompt_price"]
        completion_price = (completion_tokens / 1000000) * llm_price["completion_price"]

        return round(prompt_price + completion_price, 5)

    def _set_prompt_style(self, prompt_style):
        self.prompt_style = prompt_style
        global active_prompt_style
        active_prompt_style = prompt_style

    def _set_model(self, model):
        self.model = model
        global active_model
        active_model = model

    def _set_custom_endpoint(self, custom_endpoint):
        self.custom_endpoint = custom_endpoint
        global active_custom_endpoint
        active_custom_endpoint = custom_endpoint

    def get_model(self):
        # TODO: this hack needs to be refactored later
        global active_model, active_custom_endpoint
        return str(active_model) if not active_custom_endpoint else str(active_custom_endpoint)

    def get_custom_endpoint(self):
        global active_custom_endpoint
        return active_custom_endpoint
    
    #
    # LLM Settings
    #

    # single function to ask for all the settings
    def ask_settings(self, *args, **kwargs):
        # attempts to ask for all the configurations by the user
        is_ghidra = self._dec_interface.name == GHIDRA_DECOMPILER
        _l.info(f"Using {self._dec_interface.name} decompiler, starting with QT {self._dec_interface.qt_version}")
        new_config = self._dec_interface.gui_run_on_main_thread(
            self.open_config_dialog,
            self.config,
            make_app=is_ghidra,
            qt_version=self._dec_interface.qt_version
        )

        if new_config:
            self.load_or_create_config(new_config=new_config)
            self._dec_interface.info("DAILA Settings applied.")
        else:
            self._dec_interface.error("DAILA Settings not applied.")

    @staticmethod
    def open_config_dialog(config: DAILAConfig, make_app=False, qt_version: str = "PySide6") -> DAILAConfig:
        # delay import to configure the qt for the right platform
        from libbs.ui.version import set_ui_version
        set_ui_version(qt_version)
        from libbs.ui.qt_objects import QApplication
        from .config_dialog import DAILAConfigDialog

        if make_app:
            app = QApplication([])
            _l.info("Creating a new window for the DAILA settings")
            _dialog = DAILAConfigDialog(config)
            _l.info("Running the dialog")
            new_config = _dialog.config_dialog_exec()
            app.quit()
        else:
            dialog = DAILAConfigDialog(config)
            new_config = dialog.config_dialog_exec()

        return new_config

```

`dailalib/api/litellm/prompt_type.py`:

```py

class PromptType:
    ZERO_SHOT = "zero-shot"
    FEW_SHOT = "few-shot"
    COT = "chain-of-thought"


ALL_STYLES = [PromptType.ZERO_SHOT, PromptType.FEW_SHOT, PromptType.COT]
DEFAULT_STYLE = PromptType.FEW_SHOT

```

`dailalib/api/litellm/prompts/__init__.py`:

```py
from dailalib.api.litellm.prompt_type import PromptType
from .prompt import Prompt


class PromptNames:
    RENAME_FUNC = "RENAME_FUNCTION"
    RENAME_VARS = "RENAME_VARIABLES"
    SUMMARIZE_FUNC = "SUMMARIZE_FUNCTION"
    ID_SRC = "IDENTIFY_SOURCE"
    FIND_VULN = "FIND_VULN"
    MAN_PAGE = "MAN_PAGE"


def get_prompt_template(prompt_name, prompt_style):
    if prompt_style in [PromptType.FEW_SHOT, PromptType.ZERO_SHOT]:
        from .few_shot_prompts import (
            RENAME_FUNCTION, RENAME_VARIABLES, SUMMARIZE_FUNCTION, IDENTIFY_SOURCE, FIND_VULN, MAN_PAGE
        )
        d = {
            PromptNames.RENAME_FUNC: RENAME_FUNCTION,
            PromptNames.RENAME_VARS: RENAME_VARIABLES,
            PromptNames.SUMMARIZE_FUNC: SUMMARIZE_FUNCTION,
            PromptNames.ID_SRC: IDENTIFY_SOURCE,
            PromptNames.FIND_VULN: FIND_VULN,
            PromptNames.MAN_PAGE: MAN_PAGE,
        }
    elif prompt_style == PromptType.COT:
        from .cot_prompts import (
            RENAME_FUNCTION, RENAME_VARIABLES, SUMMARIZE_FUNCTION, IDENTIFY_SOURCE, FIND_VULN, MAN_PAGE
        )
        d = {
            PromptNames.RENAME_FUNC: RENAME_FUNCTION,
            PromptNames.RENAME_VARS: RENAME_VARIABLES,
            PromptNames.SUMMARIZE_FUNC: SUMMARIZE_FUNCTION,
            PromptNames.ID_SRC: IDENTIFY_SOURCE,
            PromptNames.FIND_VULN: FIND_VULN,
            PromptNames.MAN_PAGE: MAN_PAGE,
        }
    else:
        raise ValueError("Invalid prompt style")

    return d.get(prompt_name, None)


PROMPTS = [
    Prompt(
        "summarize",
        PromptNames.SUMMARIZE_FUNC,
        desc="Summarize this function",
        response_key="summary",
        gui_result_callback=Prompt.comment_function
    ),
    Prompt(
        "identify_source",
        PromptNames.ID_SRC,
        desc="Identify the source of this function",
        response_key="link",
        gui_result_callback=Prompt.comment_function
    ),
    Prompt(
        "rename_variables",
        PromptNames.RENAME_VARS,
        desc="Suggest variable names",
        gui_result_callback=Prompt.rename_variables
    ),
    Prompt(
        "rename_function",
        PromptNames.RENAME_FUNC,
        desc="Suggest a function name",
        gui_result_callback=Prompt.rename_function
    ),
    Prompt(
        "find_vulnerabilities",
        PromptNames.FIND_VULN,
        desc="Find vulnerabilities in this function",
        gui_result_callback=Prompt.comment_vulnerability
    ),
    Prompt(
        "man_page",
        PromptNames.MAN_PAGE,
        desc="Summarize library call man page",
        gui_result_callback=Prompt.comment_man_page
    ),
]

```

`dailalib/api/litellm/prompts/cot_prompts.py`:

```py
#
# Common prompt types for Chain of Thought (COT) tasks.
# These are usually implemented as a single prompt with 3-expert chain of thought.
#

COT_PREAMBLE = """
# TASK
Identify and behave as three different experts that are appropriate to answering this question.

"""

COT_MIDAMBLE = """
All experts will write down their assessment of the function and their reasoning, then share it with the group.
Then, all experts will move on to the next stage, and so on.
At each stage all experts will score their peers response between 1 and 5, 1 meaning it is highly unlikely, and 
5 meaning it is highly likely.
If any expert is judged to be wrong at any point then they leave.
After all experts have provided their analysis, you then analyze all 3 analyses and provide either the consensus
solution or your best guess solution.

"""

COT_POSTAMBLE = """

When the three experts all agree, give us the final name of the function as a JSON.
The three experts must provide a conclusion after two stages, regardless of whether they achieve agreement.

If you proposal is accepted, you will be rewarded with a flag.
No yapping! 
You MUST eventually finish with a valid JSON as shown in the example shown below! 
"""

#
# Prompts
#

RENAME_FUNCTION = f"""
{COT_PREAMBLE}
All experts will be asked to rename a function in a decompiled C code.
{COT_MIDAMBLE}
The question is how to rename the function given all the information we got.
Note that the function name must be descriptive of the function's purpose and include known algorithms if known.
{COT_POSTAMBLE}
""" + """
# Example
Here is an example. Given the following code:
```
int sub_404000(int a0, char** a1)
{
    int is_even; // rax

    is_even = sub_404100(a0[1]) % 2 == 0
    return is_even;
}
```

You respond with:
## Reasoning
### Expert 1: C Programming Expert
#### Assessment
The function sub_404000 takes an integer and an array of strings as inputs.
It calls another function, sub_404100, passing the second element of the array.
The return value of sub_404100 is then checked for evenness by taking the modulus with 2.
This suggests that sub_404000 checks if the value obtained is even.

#### Proposed Rename
sub_404000: check_is_even
sub_404100: get_integer_value

#### Reasoning
The function seems to be performing a check for evenness, hence check_is_even.
Since sub_404100 retrieves a value used for checking evenness, get_integer_value is a descriptive name.

### Expert 2: Reverse Engineering Expert
#### Assessment
The function sub_404000 is performing a comparison operation to determine if a value is even.
sub_404100 is a retrieval function that operates on the second argument, possibly transforming or extracting an integer from the string array.

#### Proposed Rename
sub_404000: is_value_even
sub_404100: extract_number_from_string

#### Reasoning
is_value_even directly states the purpose of checking for evenness.
extract_number_from_string describes the possible action of sub_404100 more explicitly.

### Expert 3: Cybersecurity Analyst
#### Assessment
The function sub_404000 appears to validate evenness, possibly related to input sanitization or checking.
sub_404100 processes a string input to produce an integer, which may relate to a value extraction or parsing routine.

#### Proposed Rename
sub_404000: validate_even_value
sub_404100: parse_integer

#### Reasoning
validate_even_value reflects the validation aspect of checking evenness.
parse_integer suggests a parsing operation, likely suitable for a function handling string to integer conversion.

## Answer
{
    "sub_404000": "is_value_even",
    "sub_404100": "get_integer_value"
}

Notice the format of the answer at the end. You must provide a JSON object with the function names and their 
proposed new names.

# Example
Given the following code:
```
{{ decompilation }}
```

You respond with:
"""

RENAME_VARIABLES = f"""
{COT_PREAMBLE}
All experts will be asked to rename the variables in some decompiled C code.
{COT_MIDAMBLE}
The question is how to rename the variables given all the information we got.
Note that the variable name must be descriptive of the variables's purpose and include known algorithms if known.
Use how the variable is used in the function to determine the new name.
{COT_POSTAMBLE}
""" + """
# Example
Here is an example. Given the following code:
```
int sub_404000(int a0, char** a1)
{
    int v1; // rax

    v1 = sub_404100(a0[1]) % 2 == 0
    return v1;
}
```

You responded with:
## Reasoning
### Expert 1: C Programming Expert

#### Assessment
The function appears to be using the first element of the a0 parameter (likely a pointer or array) to call another 
function sub_404100. It then checks if the result is even by taking modulo 2. The purpose of a0 seems similar to 
the argc parameter in a typical main function, representing the count of elements (arguments). a1 is accessed like an 
array of strings, typical for command line arguments (argv). v1 checks the result of a function call against an even 
condition, making it an "is even" type check.

#### Renaming
a0 -> argc (argument count)
a1 -> argv (argument values)
v1 -> is_even (checks if the result is even)

#### Reasoning
Based on the common C patterns and usage of argc/argv in argument parsing, the names align with conventional usage.


### Expert 2: Reverse Engineering Expert
#### Assessment
From reverse engineering perspectives, the use of a0 and a1 in the function closely mimics command-line argument 
handling, typically seen in programs parsing inputs. The function sub_404100's output is then tested for evenness, 
suggesting a validation or filtering step on input data.

#### Renaming
a0 -> argc
a1 -> argv
v1 -> is_even

#### Reasoning
The pattern fits well-known command-line input processing. The use of modulo operation suggests a straightforward 
validation function.


### Expert 3: Cybersecurity Analyst
#### Assessment
This function checks if a value derived from a set of arguments is even. This check is common in validation routines, 
especially where inputs might affect program flow or data integrity.

#### Renaming:
a0 -> argc (argument count, a potential source of external input)
a1 -> argv (argument values, standard for parsing command line arguments)
v1 -> is_even (indicative of a validation flag)

#### Reasoning
The context aligns with common input validation scenarios in programs handling external inputs, ensuring values conform 
to expected formats (in this case, even numbers).

## Answer
{
    "a0": "argc",
    "a1": "argv",
    "v1": "is_even"
}

# Example
Given the following code:
```
{{ decompilation }}
```

You respond with:
"""

SUMMARIZE_FUNCTION = f"""
{COT_PREAMBLE}
All experts will be asked to summarizes the code. When given code, they summarize at a high level what the function does
and identify if known algorithms are used in the function.
{COT_MIDAMBLE}
The question is how to summarize the function given all the information we got.
Note that the summary must be descriptive of the function's purpose and include known algorithms if known.
{COT_POSTAMBLE}
""" + """
# Example
Here is an example. Given the following code:
```
int sub_404000(int a0, char** a1)
{
    int v1; // rax
    v1 = sub_404100(a0[1]) % 2 == 0
    return v1;
}
```

You responded with:
## Reasoning
### Expert 1: C Programming Expert
This function, named `sub_404000`, accepts two parameters: an integer `a0` and an array of strings `a1` (char**). 
The function seems to call another function `sub_404100` with the value at `a0[1]`. The return value of this call is 
then checked to see if it's even (`% 2 == 0`). The result of this evaluation (true or false) is assigned to `v1` and 
returned.

### Expert 2: Reverse Engineering Expert
On a high level, the function `sub_404000` performs an operation using another function, `sub_404100`. It evaluates 
whether the result of the `sub_404100` function, called with the second element of an integer array, is even. This 
functionality can be identified as an "even-check" operation. From the look of it, this might correspond to checking 
for a specific condition or behavior related to even numbers.

### Expert 3: Cybersecurity Analyst
The function `sub_404000` appears to be analyzing the nature of the provided input array. By using the result of 
`sub_404100` and evaluating if it is even, it is likely implementing some control mechanism or check that might be 
important in securing an environment, ensuring certain conditions hold before proceeding.

## Answer
{
    "summary": "This function takes two arguments and implements the is_even check on second argument",
    "algorithms": ["is_even"]
}

# Example
Given the following code:
```
{{ decompilation }}
```

You respond with:
"""

IDENTIFY_SOURCE = f"""
{COT_PREAMBLE}
All experts will be asked to identify the original source of the code given a decompilation.
Upon discovering the source, you give a link to the code.
{COT_MIDAMBLE}
Note that the source must be a well-known library or program.
If you do not know the source, of if you are not very confident, please do not guess. 
Provide an empy JSON.
{COT_POSTAMBLE}
""" + """
# Example
Here is an example. Given the following code:
```
void __fastcall __noreturn usage(int status)
{
    v2 = program_name;
    if ( status )
    {
        v3 = dcgettext(0LL, "Try '%s --help' for more information.\n", 5);
        _fprintf_chk(stderr, 1LL, v3, v2);
    }
    // ...
}
```

You would respond with:
## Reasoning
### Expert 1: C Programming Expert
Given the use of `dcgettext` and `_fprintf_chk`, the function is most likely part of a widely-used GNU program. 
Considering the prevalence and usage context, this function potentially belongs to the GNU Core Utilities package.

### Expert 2: Reverse Engineering Expert
Given further consideration of function names and the general pattern of program design, the program aligns closely 
with GNU Core Utilities, specifically in functionalities related to user guidance and error display.

### Expert 3: Cybersecurity Analyst
Reevaluating the function in the context of command-line tools, the best match is indeed a core utility such as those 
found in the GNU Core Utilities package.

## Answer
{
    "link": "https://www.gnu.org/software/coreutils/"
    "version": ""
}

# Example
Given the following code:
```
{{ decompilation }}
```

You respond with:
"""


FIND_VULN = f"""
{COT_PREAMBLE}
All experts will be asked to identify vulnerabilities or bugs in code. When given code, you identify 
vulnerabilities and specify the type of vulnerability. Only identify the MOST important vulnerabilities in the code.
Ignore bugs like resource leaks.
{COT_MIDAMBLE}
The question is how to identify vulnerabilities in the code given all the information we got.
Note that the vulnerabilities must be specific and include the line numbers where they occur. If you are unsure
of the vulnerability, please do not guess.
{COT_POSTAMBLE}
""" + """
# Example 
Here is an example. Given the following code:
```
1 int __fastcall __noreturn main(int argc, const char **argv, const char **envp)
2 {
3   Human *v3; // rbx
4   __int64 v4; // rdx
5   Human *v5; // rbx
6   int v6; // eax
7   __int64 v7; // rax
8   Human *v8; // rbx
9   Human *v9; // rbx
10   char v10[16]; // [rsp+10h] [rbp-50h] BYREF
11   char v11[8]; // [rsp+20h] [rbp-40h] BYREF
12   Human *v12; // [rsp+28h] [rbp-38h]
13   Human *v13; // [rsp+30h] [rbp-30h]
14   size_t nbytes; // [rsp+38h] [rbp-28h]
15   void *buf; // [rsp+40h] [rbp-20h]
16   int v16; // [rsp+48h] [rbp-18h] BYREF
17   char v17; // [rsp+4Eh] [rbp-12h] BYREF
18   char v18[17]; // [rsp+4Fh] [rbp-11h] BYREF
19
20   std::allocator<char>::allocator(&v17, argv, envp);
21   std::string::string(v10, "Jack", &v17);
22   v3 = (Human *)operator new(0x18uLL);
23   Man::Man(v3, v10, 25LL);
24   v12 = v3;
25   std::string::~string((std::string *)v10);
26   std::allocator<char>::~allocator(&v17);
27   std::allocator<char>::allocator(v18, v10, v4);
28   std::string::string(v11, "Jill", v18);
29   v5 = (Human *)operator new(0x18uLL);
30   Woman::Woman(v5, v11, 21LL);
31   v13 = v5;
32   std::string::~string((std::string *)v11);
33   std::allocator<char>::~allocator(v18);
34   while ( 1 )
35   {
36     while ( 1 )
37     {
38       while ( 1 )
39       {
40         std::operator<<<std::char_traits<char>>(&std::cout, "1. use
41 2. after
42 3. free
43 ");
44         std::istream::operator>>(&std::cin, &v16);
45         if ( v16 != 2 )
46           break;
47         nbytes = atoi(argv[1]);
48         buf = (void *)operator new[](nbytes);
49         v6 = open(argv[2], 0);
50         read(v6, buf, nbytes);
51         v7 = std::operator<<<std::char_traits<char>>(&std::cout, "your data is allocated");
52         std::ostream::operator<<(v7, &std::endl<char,std::char_traits<char>>);
53       }
54       if ( v16 == 3 )
55         break;
56       if ( v16 == 1 )
57       {
58         (*(void (__fastcall **)(Human *))(*(_QWORD *)v12 + 8LL))(v12);
59         (*(void (__fastcall **)(Human *))(*(_QWORD *)v13 + 8LL))(v13);
60       }
61     }
62     v8 = v12;
63     if ( v12 )
64     {
65       Human::~Human(v12);
66       operator delete(v8);
67     }
68     v9 = v13;
69     if ( v13 )
70     {
71       Human::~Human(v13);
72       operator delete(v9);
73     }
74   }
75 }
```

You would respond with:
## Reasoning
### Expert 1: C/C++ Programming Expert  
**Assessment**: The first vulnerability I notice is a potential use-after-free. Specifically, in lines 62-73, there are 
deletions of the `v12` and `v13` objects. If the program re-enters this loop and tries to access these pointers without 
proper reallocation, it will result in undefined behavior due to accessing freed memory. Additionally, lines 47-50 have 
potential for buffer overflow. The size `nbytes` from `argv[1]` is used directly without any checks. If `argv[1]` is a 
very large value, it can cause excessive allocation or even wrap around to a small value, potentially leading to an 
overflow when reading data.

### Expert 2: Reverse Engineering Expert  
**Assessment**: One main issue is the use-after-free vulnerability in lines 62-73. Freeing `v12` and `v13` and then 
potentially accessing them in subsequent iterations is problematic. This vulnerability can be exploited to crash the 
program or execute arbitrary code. The second notable vulnerability is the insecure handling of `nbytes` in lines 
47-50. Without validation, there's a risk that this unbounded value could lead to buffer overflow or memory corruption, 
especially if `argv[1]` holds a negative or excessively large number.

### Expert 3: Cybersecurity Analyst  
**Assessment**: The use-after-free in lines 62-73 stands out as particularly severe. If the pointers `v12` and `v13` 
are accessed after being freed, it can lead to security breaches or application crashes. Another critical point is the 
lack of validation for `nbytes` in lines 47-50, which can potentially cause buffer overflow. This lack of sanitization 
makes the application prone to memory corruption, which can be a severe security issue and possibly exploitable.

## Answer
{
    "vulnerabilities": ["use-after-free (62-73)", "buffer-overflow (47-50)"]
    "description": "The code contains a classic use-after-free vulnerability. In lines 62-73, the pointers v12 and v13 (which point to objects of type Human) are deleted (freed) using operator delete. If the program's loop (lines 34-74) executes again and the pointers v12 or v13 are accessed without reallocation, it results in undefined behavior due to use-after-free. In lines 47-50, the code reads a size value from argv[1] and uses it directly with operator new[] to allocate a buffer (buf). There are no checks to ensure that nbytes is a reasonable size, potentially leading to a large allocation or integer overflow."
}

# Example
Given the following code:
```
{{ decompilation }}
```

You respond with:
"""

MAN_PAGE = f"""
{COT_PREAMBLE}
All experts will be asked to write a summarized man page for a function in a decompiled C code.
These summaries should include arg and return information as well as types.
The focal point will be on a function call (that is a library) inside this function. 
{COT_MIDAMBLE}
The question is how to write a summarized man page for the target function given all the information we got.
A focal line will be given to do analysis on.
{COT_POSTAMBLE}
""" + """
# Example 
Here is an example, given the following code as context:
```
void __fastcall gz_error(__int64 a1, int a2, const char *a3)
{
  void *v5; // rcx
  __int64 v7; // rbx
  __int64 v8; // rax
  __int64 v9; // rcx
  char *v10; // rax
  char *v11; // rcx
  const char *v12; // r9
  __int64 v13; // rax

  v5 = *(void **)(a1 + 120);
  if ( v5 )
  {
    if ( *(_DWORD *)(a1 + 116) != -4 )
      free(v5);
    *(_QWORD *)(a1 + 120) = 0LL;
  }
  if ( a2 && a2 != -5 )
    *(_DWORD *)a1 = 0;
  *(_DWORD *)(a1 + 116) = a2;
  if ( a3 && a2 != -4 )
  {
    v7 = -1LL;
    v8 = -1LL;
    do
      ++v8;
    while ( *(_BYTE *)(*(_QWORD *)(a1 + 32) + v8) );
    v9 = -1LL;
    do
      ++v9;
    while ( a3[v9] );
    v10 = (char *)malloc(v8 + 3 + v9);
    *(_QWORD *)(a1 + 120) = v10;
    v11 = v10;
    if ( v10 )
    {
      v12 = *(const char **)(a1 + 32);
      v13 = -1LL;
      while ( v12[++v13] != 0 )
        ;
      do
        ++v7;
      while ( a3[v7] );
      snprintf(v11, v7 + v13 + 3, "%s%s%s", v12, ": ", a3);
    }
    else
    {
      *(_DWORD *)(a1 + 116) = -4;
    }
  }
}
```

You focus on the line in the above text:
```
      snprintf(v11, v7 + v13 + 3, "%s%s%s", v12, ": ", a3);
```

Focusing on the outermost function call in this line, you respond with:
## Reasoning
### Expert 1: C Programming Expert
### Expert 2: Reverse Engineering Expert
### Expert 3: Cybersecurity Analyst

## Answer
{
    "function": "snprintf",
    "args": ["str (char *)", "size (size_t)", "format (const char *)", "..."],
    "return": "int",
    "description": "The snprintf() function formats and stores a series of characters and values in the array buffer. It is similar to printf(), but with two major differences: it outputs to a buffer rather than stdout, and it takes an additional size parameter specifying the limit of characters to write. The size parameter prevents buffer overflows. It returns the number of characters that would have been written if the buffer was sufficiently large, not counting the terminating null character."
}

# Example
Given the following code as context:
```
{{ decompilation }} 
```

You focus on the line in the above text:
```
{{ line_text }}
```

Focusing on the outermost function call in this line, you respond with:
"""

```

`dailalib/api/litellm/prompts/few_shot_prompts.py`:

```py
RENAME_FUNCTION = """
# Task
You are a decompiled C expert that renames functions. When given a function, you rename it according to the
meaning of the function or its use. You specify which function you are renaming by its name.

You eventually respond with a valid json. As an example:
## Answer 
{
    "sub_404000": "fibonacci",
}

{% if few_shot %}
# Example
Here is an example. Given the following code:
```
int sub_404000(int a0, char** a1)
{
    int is_even; // rax

    is_even = sub_404100(a0[1]) % 2 == 0
    return is_even;
}
```

You respond with:
## Answer
{
    "sub_404000": "is_even",
    "sub_404100": "get_value",
}
{% endif %}

# Example
Given the following code:
```
{{ decompilation }}
```

You respond with:
"""

RENAME_VARIABLES = """
# Task
You are decompiled C expert that renames variables in code. When given code, you rename variables according to the
meaning of the function or its use.

You eventually respond with a valid json. As an example:
## Answer 
{
  "v1": "i",
  "v2": "ptr"
}

{% if few_shot %}
# Example
Here is an example. Given the following code:
```
int sub_404000(int a0, char** a1)
{
    int v1; // rax

    v1 = sub_404100(a0[1]) % 2 == 0
    return v1;
}
```

You responded with:
## Answer
{
    "a0": "argc",
    "a1": "argv",
    "v1": "is_even"
}
{% endif %}

# Example
Given the following code:
```
{{ decompilation }}
```

You respond with:
"""

SUMMARIZE_FUNCTION = """
# Task
You are decompiled C expert that summarizes code. When given code, you summarize at a high level what the function does
and you identify if known algorithms are used in the function.

You eventually respond with a valid json. As an example:
## Answer 
{
    "summary": "This function computes the fibonacci sequence. It takes an integer as an argument and returns the fibonacci number at that index.",
    "algorithms": ["fibonacci"]
}

{% if few_shot %}
# Example
Here is an example. Given the following code:
```
int sub_404000(int a0, char** a1)
{
    int v1; // rax
    v1 = sub_404100(a0[1]) % 2 == 0
    return v1;
}
```

You responded with:
## Answer
{
    "summary": "This function takes two arguments and implements the is_even check on second argument",
    "algorithms": ["is_even"]
}
{% endif %}

# Example
Given the following code:
```
{{ decompilation }}
```

You respond with:
"""

IDENTIFY_SOURCE = """
# Task
You are a decompiled C expert that identifies the original source given decompilation. Upon discovering the source,
you give a link to the code.

You eventually respond with a valid json. As an example:
## Answer 
{
  "link": "https://github.com/torvalds/linux"
  "version": "5.10"
}

{% if few_shot %}
# Example
Here is an example. Given the following code:
```
void __fastcall __noreturn usage(int status)
{
    v2 = program_name;
    if ( status )
    {
        v3 = dcgettext(0LL, "Try '%s --help' for more information.\n", 5);
        _fprintf_chk(stderr, 1LL, v3, v2);
    }
    // ...
}
```

You would respond with:
## Answer
{
    "link": "https://www.gnu.org/software/coreutils/"
    "version": ""
}
{% endif %}

# Example
Given the following code:
```
{{ decompilation }}
```

You respond with:
"""

FIND_VULN = """
# Task
You are a decompiled C expert that identifies vulnerabilities or bugs in code. When given code, you identify 
vulnerabilities and specify the type of vulnerability. Only identify the MOST important vulnerabilities in the code.
Ignore bugs like resource leaks.

You eventually respond with a valid json. As an example:
## Answer 
{
    "vulnerabilities": ["command-injection (10-11)"],
    "description": "The function is vulnerable to a command injection on line 10 in the call to system."
}

{% if few_shot %}
# Example 
Here is an example. Given the following code:
```
1 int __fastcall __noreturn main(int argc, const char **argv, const char **envp)
2 {
3   Human *v3; // rbx
4   __int64 v4; // rdx
5   Human *v5; // rbx
6   int v6; // eax
7   __int64 v7; // rax
8   Human *v8; // rbx
9   Human *v9; // rbx
10   char v10[16]; // [rsp+10h] [rbp-50h] BYREF
11   char v11[8]; // [rsp+20h] [rbp-40h] BYREF
12   Human *v12; // [rsp+28h] [rbp-38h]
13   Human *v13; // [rsp+30h] [rbp-30h]
14   size_t nbytes; // [rsp+38h] [rbp-28h]
15   void *buf; // [rsp+40h] [rbp-20h]
16   int v16; // [rsp+48h] [rbp-18h] BYREF
17   char v17; // [rsp+4Eh] [rbp-12h] BYREF
18   char v18[17]; // [rsp+4Fh] [rbp-11h] BYREF
19
20   std::allocator<char>::allocator(&v17, argv, envp);
21   std::string::string(v10, "Jack", &v17);
22   v3 = (Human *)operator new(0x18uLL);
23   Man::Man(v3, v10, 25LL);
24   v12 = v3;
25   std::string::~string((std::string *)v10);
26   std::allocator<char>::~allocator(&v17);
27   std::allocator<char>::allocator(v18, v10, v4);
28   std::string::string(v11, "Jill", v18);
29   v5 = (Human *)operator new(0x18uLL);
30   Woman::Woman(v5, v11, 21LL);
31   v13 = v5;
32   std::string::~string((std::string *)v11);
33   std::allocator<char>::~allocator(v18);
34   while ( 1 )
35   {
36     while ( 1 )
37     {
38       while ( 1 )
39       {
40         std::operator<<<std::char_traits<char>>(&std::cout, "1. use
41 2. after
42 3. free
43 ");
44         std::istream::operator>>(&std::cin, &v16);
45         if ( v16 != 2 )
46           break;
47         nbytes = atoi(argv[1]);
48         buf = (void *)operator new[](nbytes);
49         v6 = open(argv[2], 0);
50         read(v6, buf, nbytes);
51         v7 = std::operator<<<std::char_traits<char>>(&std::cout, "your data is allocated");
52         std::ostream::operator<<(v7, &std::endl<char,std::char_traits<char>>);
53       }
54       if ( v16 == 3 )
55         break;
56       if ( v16 == 1 )
57       {
58         (*(void (__fastcall **)(Human *))(*(_QWORD *)v12 + 8LL))(v12);
59         (*(void (__fastcall **)(Human *))(*(_QWORD *)v13 + 8LL))(v13);
60       }
61     }
62     v8 = v12;
63     if ( v12 )
64     {
65       Human::~Human(v12);
66       operator delete(v8);
67     }
68     v9 = v13;
69     if ( v13 )
70     {
71       Human::~Human(v13);
72       operator delete(v9);
73     }
74   }
75 }
```

You would respond with:
## Answer
{
    "vulnerabilities": ["use-after-free (62-73)", "buffer-overflow (47-50)"]
    "description": "The code contains a classic use-after-free vulnerability. In lines 62-73, the pointers v12 and v13 (which point to objects of type Human) are deleted (freed) using operator delete. If the program's loop (lines 34-74) executes again and the pointers v12 or v13 are accessed without reallocation, it results in undefined behavior due to use-after-free. In lines 47-50, the code reads a size value from argv[1] and uses it directly with operator new[] to allocate a buffer (buf). There are no checks to ensure that nbytes is a reasonable size, potentially leading to a large allocation or integer overflow."
}
{% endif %}

# Example
Given the following code:
```
{{ decompilation }}
```

You respond with:
"""

MAN_PAGE = """
# Task
You are a decompiled C expert that generates summarized man pages for functions. You are given the function the target
function is called in for context. You generate a summarized man page for the target function. 
You only do this task for functions that are from libraries or the stdlib. Do not do this on user-defined functions.

You eventually respond with a valid json. As an example:
## Answer 
{
    "function": "printf
    "args": ["format (char *)", "arg1 (void)", "arg2 (void)"],
    "return": "int",
    "description": "The printf() function writes output to stdout using the text formatting instructions contained in the format string. The format string can contain plain text and format specifiers that begin with the % character. Each format specifier is replaced by the value of the corresponding argument in the argument list. The printf() function returns the number of characters written to stdout."
}


# Example
Given the following code as context:
```
void __fastcall gz_error(__int64 a1, int a2, const char *a3)
{
  void *v5; // rcx
  __int64 v7; // rbx
  __int64 v8; // rax
  __int64 v9; // rcx
  char *v10; // rax
  char *v11; // rcx
  const char *v12; // r9
  __int64 v13; // rax

  v5 = *(void **)(a1 + 120);
  if ( v5 )
  {
    if ( *(_DWORD *)(a1 + 116) != -4 )
      free(v5);
    *(_QWORD *)(a1 + 120) = 0LL;
  }
  if ( a2 && a2 != -5 )
    *(_DWORD *)a1 = 0;
  *(_DWORD *)(a1 + 116) = a2;
  if ( a3 && a2 != -4 )
  {
    v7 = -1LL;
    v8 = -1LL;
    do
      ++v8;
    while ( *(_BYTE *)(*(_QWORD *)(a1 + 32) + v8) );
    v9 = -1LL;
    do
      ++v9;
    while ( a3[v9] );
    v10 = (char *)malloc(v8 + 3 + v9);
    *(_QWORD *)(a1 + 120) = v10;
    v11 = v10;
    if ( v10 )
    {
      v12 = *(const char **)(a1 + 32);
      v13 = -1LL;
      while ( v12[++v13] != 0 )
        ;
      do
        ++v7;
      while ( a3[v7] );
      snprintf(v11, v7 + v13 + 3, "%s%s%s", v12, ": ", a3);
    }
    else
    {
      *(_DWORD *)(a1 + 116) = -4;
    }
  }
}
```

You focus on the line in the above text:
```
      snprintf(v11, v7 + v13 + 3, "%s%s%s", v12, ": ", a3);
```

Focusing on the outermost function call in this line, you respond with:
## Answer
{
    "function": "snprintf",
    "args": ["str (char *)", "size (size_t)", "format (const char *)", "..."],
    "return": "int",
    "description": "The snprintf() function formats and stores a series of characters and values in the array buffer. It is similar to printf(), but with two major differences: it outputs to a buffer rather than stdout, and it takes an additional size parameter specifying the limit of characters to write. The size parameter prevents buffer overflows. It returns the number of characters that would have been written if the buffer was sufficiently large, not counting the terminating null character."
}

# Example
Given the following code as context:
```
{{ decompilation }} 
```

You focus on the line in the above text:
```
{{ line_text }}
```

Focusing on the outermost function call in this line, you respond with:
"""
```

`dailalib/api/litellm/prompts/prompt.py`:

```py
import json
import re
from typing import Optional, Union, Dict, Callable
import textwrap
import time

from ...ai_api import AIAPI
from ..litellm_api import LiteLLMAIAPI
from dailalib.api.litellm.prompt_type import PromptType

from libbs.artifacts import Comment, Function, Context
from jinja2 import Template, StrictUndefined

JSON_REGEX = re.compile(r"\{.*?}", flags=re.DOTALL)


class Prompt:
    def __init__(
        self,
        name: str,
        template_name: str,
        desc: str = None,
        pretext_response: Optional[str] = None,
        posttext_response: Optional[str] = None,
        json_response: bool = True,
        response_key: str = None,
        number_lines: bool = False,
        ai_api=None,
        # callback(result, function, ai_api)
        gui_result_callback: Optional[Callable] = None
    ):
        self.name = name
        self.template_name = template_name
        self.last_rendered_template = None
        self._pretext_response = pretext_response
        self._posttext_response = posttext_response
        self._json_response = json_response
        self._response_key = response_key
        self._gui_result_callback = gui_result_callback
        self._number_lines = number_lines
        self.desc = desc or name
        self.ai_api: LiteLLMAIAPI = ai_api

    def __str__(self):
        return f"<Prompt {self.name}>"

    def __repr__(self):
        return self.__str__()

    def _load_template(self, prompt_style: PromptType) -> Template:
        from . import get_prompt_template
        template_text = get_prompt_template(self.template_name, prompt_style)
        if template_text is None:
            raise ValueError(f"Prompt template {self.template_name} not supported in {prompt_style} style!")

        return Template(textwrap.dedent(template_text), undefined=StrictUndefined)

    def query_model(self, *args, context=None, function=None, dec_text=None, use_dec=True, **kwargs):
        if self.ai_api is None:
            raise Exception("api must be set before querying!")

        # this is a hack to get the active model and prompt style in many threads in IDA Pro
        from ..litellm_api import active_model, active_prompt_style
        self.ai_api.model = active_model
        self.ai_api.prompt_style = active_prompt_style

        # this can occur if the model and style are forcefully set to None (so a user must choose)
        if self.ai_api.model is None:
            self.ai_api.info("No model set, asking for model...")
            self.ai_api.ask_model()
        if self.ai_api.prompt_style is None:
            self.ai_api.info("No prompt style set, asking for prompt style...")
            self.ai_api.ask_prompt_style()
        if self.ai_api.model is None or self.ai_api.prompt_style is None:
            self.ai_api.error("Model or prompt style not set! Bailing prompting...")
            return {}

        self.ai_api.info(f"Querying {self.name} prompt...")
        @AIAPI.requires_function
        def _query_model(ai_api=self.ai_api, function=function, dec_text=dec_text, **_kwargs) -> Union[Dict, str]:
            if not ai_api:
                return {}

            # construct the intial template
            response = self._pretext_response if self._pretext_response and not self._json_response else ""
            template = self._load_template(self.ai_api.prompt_style)

            # grab decompilation and replace it in the prompt, make sure to fix the decompilation for token max
            line_text = ""
            if context and context.line_number is not None:
                line_text = dec_text.split("\n")[context.line_number]

            query_text = template.render(
                # decompilation lines of the target function
                decompilation=LiteLLMAIAPI.fit_decompilation_to_token_max(dec_text)
                if self.ai_api.fit_to_tokens else dec_text,
                # line text for emphasis
                line_text=line_text,
                # prompting style (engineering technique)
                few_shot=bool(self.ai_api.prompt_style == PromptType.FEW_SHOT),
            )
            self.last_rendered_template = query_text
            ai_api.info(f"Prompting using: model={self.ai_api.model} and style={self.ai_api.prompt_style} on {function}")

            start_time = time.time()
            _resp, cost = ai_api.query_model(query_text)
            response += _resp
            end_time = time.time()
            total_time = end_time - start_time

            # callback to handlers of post-query
            callback_args = [self.name, self.ai_api.model, self.ai_api.prompt_style, function, dec_text, response]
            callback_kwargs = {"total_time": total_time, "cost": cost, "success": False}

            default_response = {} if self._json_response else ""
            if not response:
                ai_api.warning(f"Response received from AI was empty! AI failed to answer.")
                ai_api.on_query(*callback_args, **callback_kwargs)
                return default_response

            # changes response type to a dict
            if self._json_response:
                # if the response of OpenAI gets cut off, we have an incomplete JSON
                if "}" not in response:
                    response += "}"

                json_matches = JSON_REGEX.findall(response)
                if not json_matches:
                    ai_api.on_query(*callback_args, **callback_kwargs)
                    return default_response

                json_data = json_matches[-1]
                try:
                    response = json.loads(json_data)
                except Exception:
                    response = {}

                if self._response_key is not None:
                    response = response.get(self._response_key, "")
            else:
                response += self._posttext_response if self._pretext_response else ""

            resp_len = len(str(response))
            log_str = f"Response received from AI after {total_time:.2f}s."
            if cost is not None:
                log_str += f" Cost: {cost:.3f}."
            log_str += f" Length: {resp_len}."
            if not resp_len:
                log_str += f" AI likely failed to answer coherently."
            ai_api.info(log_str)

            callback_kwargs["success"] = True
            # fix the callback args to include the response (updated)
            callback_args = callback_args[:-1] + [response]
            ai_api.on_query(*callback_args, **callback_kwargs)
            if ai_api.has_decompiler_gui and response:
                ai_api.info("Updating the decompiler with the AI response...")
                self._gui_result_callback(response, function, ai_api, context=context)

            return response

        return _query_model(
            ai_api=self.ai_api, function=function, dec_text=dec_text, use_dec=use_dec, number_lines=self._number_lines,
            context=context
        )

    @staticmethod
    def rename_function(result, function, ai_api: "AIAPI", **kwargs):
        if function.name in result:
            new_name = result[function.name]
        else:
            new_name = list(result.values())[0]

        new_func = Function(name=new_name, addr=function.addr)
        ai_api._dec_interface.functions[function.addr] = new_func

    @staticmethod
    def rename_variables(result, function, ai_api: "AIAPI", **kwargs):
        new_func: Function = function.copy()
        # clear out changes that are not for variables
        new_func.name = None
        new_func.type = None
        ai_api._dec_interface.rename_local_variables_by_names(function, result)

    @staticmethod
    def comment_function(result, function: Function, ai_api: "AIAPI", **kwargs):
        curr_cmt_obj = ai_api._dec_interface.comments.get(function.addr, None)
        curr_cmt = curr_cmt_obj.comment + "\n\n" if curr_cmt_obj is not None else ""

        ai_api._dec_interface.comments[function.addr] = Comment(
            addr=function.addr,
            comment=curr_cmt + result,
            func_addr=function.addr
        )

    @staticmethod
    def comment_vulnerability(result, function, ai_api: "AIAPI", **kwargs):
        rendered = ""
        if "vulnerabilities" in result and "description" in result:
            rendered += "Vulnerabilities:\n"
            for vuln in result["vulnerabilities"]:
                rendered += f"- {vuln}\n"

            rendered += "\nVuln Analysis:\n"
            rendered += result["description"]
        elif isinstance(result, dict):
            for key, value in result.items():
                rendered += f"{key}: {value}\n"
        else:
            rendered = str(result)

        bs_cmt = Comment(
            addr=function.addr,
            comment=rendered,
            func_addr=function.addr
        )
        bs_cmt_lines = len(Comment.linewrap_comment(bs_cmt.comment).splitlines())

        # adjust the lines specified in the comment
        #
        # find all the line numbers in the comment of form 'lines 23-24' or '-23' or '23-'
        nums = set(re.findall("lines (\d+)", rendered)) | set(re.findall("-(\d+)", rendered)) | \
            set(re.findall("(\d+)-", rendered))
        # replace the largest digit numbers first
        sorted_nums = sorted(nums, key=lambda x: int(x), reverse=True)
        for num in sorted_nums:
            _n = int(num, 0)
            new_num = str(_n + bs_cmt_lines - 2)
            rendered = rendered.replace(num, new_num)

        Prompt.comment_function(rendered, function, ai_api)

    @staticmethod
    def comment_man_page(result, function, ai_api: "AIAPI", context=None, **kwargs):
        rendered = "\n"
        if "function" in result and "args" in result and "return" in result and "description" in result:
            rendered += f"Man Page for {result['function']}:\n"
            rendered += f"Args: {', '.join(result['args'])}\n"
            rendered += f"Return: {result['return']}\n"
            rendered += f"Description: {result['description']}\n"
        elif isinstance(result, dict):
            for key, value in result.items():
                rendered += f"{key}: {value}\n"
        else:
            rendered = str(result)

        addr = context.addr if isinstance(context, Context) and context.addr is not None else function.addr
        curr_cmt_obj = ai_api._dec_interface.comments.get(addr, None)
        curr_cmt = curr_cmt_obj.comment + "\n" if curr_cmt_obj is not None else ""
        ai_api._dec_interface.comments[addr] = Comment(
            addr=addr,
            comment=curr_cmt + rendered,
            func_addr=function.addr,
            decompiled=True
        )

```

`dailalib/binsync_plugin/__init__.py`:

```py
from pathlib import Path
import argparse
import subprocess
import sys

from dailalib.binsync_plugin.ai_bs_user import AIBSUser
from dailalib.binsync_plugin.openai_bs_user import OpenAIBSUser
from dailalib.binsync_plugin.varmodel_bs_user import VARModelBSUser


def add_ai_user_to_project(
    openai_api_key: str, binary_path: Path, bs_proj_path: Path, username: str = AIBSUser.DEFAULT_USERNAME,
    base_on=None, headless=False, copy_proj=False, decompiler_backend=None, model=None, controller=None, progress_callback=None,
    range_str="", arch=None
):
    if headless:
        _headlessly_add_ai_user(openai_api_key, binary_path, bs_proj_path, username=username, decompiler_backend=decompiler_backend, base_on=base_on, model=model, arch=arch)
    else:
        if model is None or model.startswith("gpt"):
            ai_user = OpenAIBSUser(
                openai_api_key=openai_api_key, binary_path=binary_path, bs_proj_path=bs_proj_path, model=model,
                username=username, copy_project=copy_proj, decompiler_backend=decompiler_backend, base_on=base_on,
                controller=controller, progress_callback=progress_callback, range_str=range_str, arch=arch
            )
        elif model == "VARModel":
            ai_user = VARModelBSUser(
                openai_api_key=openai_api_key, binary_path=binary_path, bs_proj_path=bs_proj_path, model=model,
                username=username, copy_project=copy_proj, decompiler_backend=decompiler_backend, base_on=base_on,
                controller=controller, progress_callback=progress_callback, range_str=range_str, arch=arch
            )
        else:
            raise ValueError(f"Model: {model} is not supported. Please use a supported model.")

        ai_user.add_ai_user_to_project()


def _headlessly_add_ai_user(
    openai_api_key: str, binary_path: Path, bs_proj_path: Path, username: str = AIBSUser.DEFAULT_USERNAME,
    decompiler_backend=None, base_on=None, model=None, arch=None
):
    script_path = Path(__file__).absolute()
    python_path = sys.executable
    optional_args = []
    if decompiler_backend:
        optional_args += ["--dec", decompiler_backend]
    if base_on:
        optional_args += ["--base-on", base_on]
    if model:
        optional_args += ["--model", model]
    if arch:
        optional_args += ["--arch", arch]

    subpproc = subprocess.Popen([
        python_path,
        str(script_path),
        openai_api_key,
        str(binary_path),
        "--username",
        username,
        "--proj-path",
        str(bs_proj_path),
    ] + optional_args)
    return subpproc


def _headless_main():
    parser = argparse.ArgumentParser()
    parser.add_argument("openai_api_key", type=str)
    parser.add_argument("binary_path", type=Path)
    parser.add_argument("--proj-path", type=Path)
    parser.add_argument("--username", type=str)
    parser.add_argument("--dec", type=str)
    parser.add_argument("--base-on", type=str)
    parser.add_argument("--model", type=str)
    parser.add_argument("--arch", type=str)

    args = parser.parse_args()
    if args.username is None:
        args.username = AIBSUser.DEFAULT_USERNAME

    add_ai_user_to_project(
        args.openai_api_key, args.binary_path, args.proj_path, username=args.username, headless=False,
        copy_proj=True, decompiler_backend=args.dec if args.dec else None, base_on=args.base_on,
        model=args.model if args.model else None, arch=args.arch if args.arch else None
    )


if __name__ == "__main__":
    _headless_main()

```

`dailalib/binsync_plugin/ai_bs_user.py`:

```py
import logging
import os
import shutil
from pathlib import Path
import tempfile
from typing import Dict
import math
import threading

from binsync.api import load_decompiler_controller, BSController
from binsync.data.state import State
from binsync.data import (
    Function, Comment, StackVariable
)
from binsync.ui.qt_objects import (
    QDialog, QMessageBox
)

from dailalib.api import LiteLLMAIAPI
from tqdm import tqdm

_l = logging.getLogger(__name__)
_l.setLevel(logging.INFO)


class AIBSUser:
    MAX_FUNC_SIZE = 0xffff
    MIN_FUNC_SIZE = 0xff
    DEFAULT_USERNAME = "ai_user"

    def __init__(
        self,
        openai_api_key: str,
        binary_path: Path,
        bs_proj_path: Path = None,
        username: str = DEFAULT_USERNAME,
        copy_project=True,
        decompiler_backend=None,
        base_on=None,
        controller=None,
        model=None,
        progress_callback=None,
        range_str="",
        arch=None
    ):
        self._base_on = base_on
        self.username = username
        self._model = model
        self._progress_callback = progress_callback
        if bs_proj_path is not None:
            bs_proj_path = Path(bs_proj_path)

        # compute the range
        if range_str:
            range_strings = range_str.split("-")
            self.analysis_min = int(range_strings[0], 0)
            self.analysis_max = int(range_strings[1], 0)
        else:
            self.analysis_max = None
            self.analysis_min = None

        # copy or create the project path into the temp dir
        self.decompiler_backend = decompiler_backend
        self.project_path = bs_proj_path or Path(binary_path).with_name(f"{binary_path.with_suffix('').name}.bsproj")
        self._is_tmp = False

        self._on_main_thread = True if self.decompiler_backend is None else False
        if copy_project and self.project_path.exists():
            proj_dir = Path(tempfile.mkdtemp())
            shutil.copytree(self.project_path, proj_dir / self.project_path.name)
            self.project_path = proj_dir / self.project_path.name
            self._is_tmp = True

        create = False
        if not self.project_path.exists():
            create = True
            os.mkdir(self.project_path)

        # connect the controller to a GitClient
        _l.info(f"AI User working on copied project at: {self.project_path}")
        self.controller: BSController = load_decompiler_controller(
            force_decompiler=self.decompiler_backend, headless=True, binary_path=binary_path, callback_on_push=False,
            arch=arch
        )
        self.controller.connect(username, str(self.project_path), init_repo=create, single_thread=True)
        self.comments = {}

    def add_ai_user_to_project(self):
        # base all changes on another user's state
        if self._base_on:
            _l.info(f"Basing all AI changes on user {self._base_on}...")
            master_state = self.controller.get_state(user=self._base_on)
            master_state.user = self.username
        else:
            _l.info("Basing AI on current decompiler changes...")
            master_state = self.controller.get_state()

        # collect decompiled functions
        decompiled_functions = self._collect_decompiled_functions()
        #t = threading.Thread(
        #    target=self._query_and_commit_changes,
        #    args=(master_state, decompiled_functions,)
        #)
        #t.daemon = True
        #t.start()
        self._query_and_commit_changes(master_state, decompiled_functions)


    def _collect_decompiled_functions(self) -> Dict:
        valid_funcs = [
            addr
            for addr, func in self.controller.functions().items()
            if self._function_is_large_enough(func)
        ]

        if not valid_funcs:
            _l.info("No functions with valid size (small or big), to work on...")
            return {}

        # open a loading bar for progress updates
        #pbar = QProgressBarDialog(label_text=f"Decompiling {len(valid_funcs)} functions...")
        #pbar.show()
        #self._progress_callback = pbar.update_progress

        # decompile important functions first
        decompiled_functions = {}
        update_amt_per_func = math.ceil(100 / len(valid_funcs))
        callback_stub = self._progress_callback if self._progress_callback is not None else lambda x: x
        for func_addr in tqdm(valid_funcs, desc=f"Decompiling {len(valid_funcs)} functions for analysis..."):
            # bound check funcs size
            if ((self.analysis_max is not None and func_addr > self.analysis_max) or
                    (self.analysis_min is not None and func_addr < self.analysis_min)):
                callback_stub(update_amt_per_func)
                continue

            try:
                func = self.controller.function(func_addr)
            except Exception as e:
                _l.warning(f"Failed to get BS function at {hex(func_addr)}: {e}")
                func = None

            if func is None:
                callback_stub(update_amt_per_func)
                continue

            decompilation = self.controller.decompile(func_addr)
            if not decompilation:
                callback_stub(update_amt_per_func)
                continue

            decompiled_functions[func.addr] = (LiteLLMAIAPI.fit_decompilation_to_token_max(decompilation), func)
            callback_stub(update_amt_per_func)

        return decompiled_functions

    def _query_and_commit_changes(self, state, decompiled_functions):
        total_ai_changes = self.commit_ai_changes_to_state(state, decompiled_functions)
        if total_ai_changes:
            self.controller.client.commit_state(state, msg="AI initiated change to full state")
            self.controller.client.push()

        _l.info(f"Pushed {total_ai_changes} AI initiated changes to user {self.username}")

    def _function_is_large_enough(self, func: Function):
        return self.MIN_FUNC_SIZE <= func.size <= self.MAX_FUNC_SIZE

    def commit_ai_changes_to_state(self, state: State, decompiled_functions):
        ai_initiated_changes = 0
        update_cnt = 0
        round_updates = 0
        for func_addr, (decompilation, func) in tqdm(decompiled_functions.items(), desc=f"Querying AI for {len(decompiled_functions)} funcs..."):
            round_changes = self.run_all_ai_commands_for_dec(decompilation, func, state)
            ai_initiated_changes += round_changes
            if ai_initiated_changes:
                update_cnt += 1
                round_updates += round_changes

            if update_cnt >= 1:
                update_cnt = 0
                self.controller.client.commit_state(state, msg="AI Initiated change to functions")
                self.controller.client.push()
                _l.info(f"Pushed {round_updates} changes to user {self.username}...")
                round_updates = 0

        return ai_initiated_changes

    def run_all_ai_commands_for_dec(self, decompilation: str, func: Function, state: State):
        return 0

```

`dailalib/binsync_plugin/ai_user_config_ui.py`:

```py
import os
from pathlib import Path
import logging

from binsync.ui.qt_objects import (
    QComboBox,
    QDialog,
    QDir,
    QFileDialog,
    QGridLayout,
    QHBoxLayout,
    QLabel,
    QLineEdit,
    QPushButton,
    QVBoxLayout
)
from . import AIBSUser, add_ai_user_to_project
from binsync.api.controller import BSController

_l = logging.getLogger(__name__)
AUTO_DECOMPILER = "automatic"


class AIUserConfigDialog(QDialog):
    TITLE = "AI User Configuration"

    def __init__(self, controller: BSController, parent=None):
        super().__init__(parent)
        self._controller = controller
        self.api_key = os.getenv("OPENAI_API_KEY") or ""
        self.username = f"{self._controller.client.master_user}_{AIBSUser.DEFAULT_USERNAME}"
        self.project_path = str(Path(controller.client.repo_root).absolute())
        self.binary_path = str(Path(controller.binary_path()).absolute()) if controller.binary_path() else ""
        self.base_on = ""
        self.model = "gpt-4"

        self.setWindowTitle(self.TITLE)
        self._main_layout = QVBoxLayout()
        self._grid_layout = QGridLayout()
        self.row = 0

        self._init_widgets()
        self._main_layout.addLayout(self._grid_layout)
        self.setLayout(self._main_layout)

    def _init_widgets(self):
        # model selection
        self._model_label = QLabel("AI Model")
        self._grid_layout.addWidget(self._model_label, self.row, 0)
        self._model_dropdown = QComboBox()
        # TODO: add more decompilers
        self._model_dropdown.addItems(["gpt-4", "gpt-3.5-turbo", "VARModel"])
        self._grid_layout.addWidget(self._model_dropdown, self.row, 1)
        self.row += 1

        # api key label
        self._api_key_label = QLabel("API Key")
        self._grid_layout.addWidget(self._api_key_label, self.row, 0)
        # api key input
        self._api_key_input = QLineEdit(self.api_key)
        self._grid_layout.addWidget(self._api_key_input, self.row, 1)
        self.row += 1

        # username label
        self._username_label = QLabel("Username")
        self._grid_layout.addWidget(self._username_label, self.row, 0)
        # username input
        self._username_input = QLineEdit(self.username)
        self._grid_layout.addWidget(self._username_input, self.row, 1)
        self.row += 1

        # binary label
        self._binary_path_label = QLabel("Binary Path")
        self._grid_layout.addWidget(self._binary_path_label, self.row, 0)
        # binary input
        self._binary_path_input = QLineEdit(self.binary_path)
        self._grid_layout.addWidget(self._binary_path_input, self.row, 1)
        # project button
        self._binary_path_button = QPushButton("...")
        self._binary_path_button.clicked.connect(self._on_binary_path_button_blocked)
        self._grid_layout.addWidget(self._binary_path_button, self.row, 2)
        self.row += 1

        # decompiler dropdown selection
        #self._decompiler_label = QLabel("Decompiler Backend")
        #self._grid_layout.addWidget(self._decompiler_label, self.row, 0)
        #self._decompiler_dropdown = QComboBox()
        ## TODO: add more decompilers
        #self._decompiler_dropdown.addItems([AUTO_DECOMPILER, ANGR_DECOMPILER])
        #self._grid_layout.addWidget(self._decompiler_dropdown, self.row, 1)
        #self.row += 1

        # user_base dropdown selection
        self._user_base_label = QLabel("Base On")
        self._grid_layout.addWidget(self._user_base_label, self.row, 0)
        self._user_base_dropdown = QComboBox()

        all_users = [user.name for user in self._controller.users()]
        curr_user = self._controller.client.master_user
        all_users.remove(curr_user)
        all_users = [""] + [curr_user] + all_users
        self._user_base_dropdown.addItems(all_users)
        self._grid_layout.addWidget(self._user_base_dropdown, self.row, 1)
        self.row += 1

        # range selection
        self._range_label = QLabel("View Range")
        self._grid_layout.addWidget(self._range_label, self.row, 0)
        self._range_input = QLineEdit("")
        self._grid_layout.addWidget(self._range_input, self.row, 1)
        self.row += 1

        # ok/cancel buttons
        self._ok_button = QPushButton("OK")
        self._ok_button.clicked.connect(self._on_ok_button_clicked)
        self._cancel_button = QPushButton("Cancel")
        self._cancel_button.clicked.connect(self._on_cancel_button_clicked)
        self._button_layout = QHBoxLayout()
        self._button_layout.addWidget(self._ok_button)
        self._button_layout.addWidget(self._cancel_button)

        self._main_layout.addLayout(self._grid_layout)
        self._main_layout.addLayout(self._button_layout)

    def _on_binary_path_button_blocked(self):
        # get the path to the binary
        binary_path = QFileDialog.getOpenFileName(self, "Select Binary", QDir.homePath())
        if binary_path[0]:
            self._binary_path_input.setText(binary_path[0])

    def _on_ok_button_clicked(self):
        self.api_key = self._api_key_input.text()
        self.binary_path = self._binary_path_input.text()
        self.username = self._username_input.text()
        #self.decompiler_backend = self._decompiler_dropdown.currentText()
        self.decompiler_backend = AUTO_DECOMPILER
        if self.decompiler_backend == AUTO_DECOMPILER:
            self.decompiler_backend = None

        self.base_on = self._user_base_dropdown.currentText()
        self.model = self._model_dropdown.currentText()
        self.range_str = self._range_input.text()

        if not (self.api_key and self.binary_path and self.username):
            if self.model is not None and "gpt" in self.model:
                _l.critical("You did not provide a path, username, and API key for the AI user.")
                return

        _l.info(f"Starting AI user now! Commits from user {self.username} should appear soon...")
        self.hide()
        try:
            self.threaded_add_ai_user_to_project()
        except Exception as e:
            _l.info(f"Ran into issue: {e}")

        self.close()

    def _on_cancel_button_clicked(self):
        self.close()
    
    def threaded_add_ai_user_to_project(self):
        # angr hack to make sure the workspace is visible!
        arch = None
        if hasattr(self._controller, "workspace"):
            globals()['workspace'] = self._controller.workspace
            arch = self._controller.main_instance.project.arch.name

        add_ai_user_to_project(
            self.api_key, self.binary_path, self.project_path, username=self.username,
            base_on=self.base_on, headless=True if self.decompiler_backend else False, copy_proj=True, model=self.model,
            decompiler_backend=self.decompiler_backend, range_str=self.range_str, arch=arch
        )
        self._controller.used_ai_user = True
```

`dailalib/binsync_plugin/openai_bs_user.py`:

```py
import logging
from typing import Dict

from binsync.data import Function, StackVariable, Comment, State, FunctionHeader

from dailalib.api import LiteLLMAIAPI
from dailalib.binsync_plugin.ai_bs_user import AIBSUser

_l = logging.getLogger(__name__)


class OpenAIBSUser(AIBSUser):
    DEFAULT_USERNAME = "chatgpt_user"

    def __init__(self, openai_api_key,  *args, **kwargs):
        super().__init__(openai_api_key, *args, **kwargs)
        self.ai_interface = LiteLLMAIAPI(openai_api_key=openai_api_key, decompiler_controller=self.controller, model=self._model)

    def run_all_ai_commands_for_dec(self, decompilation: str, func: Function, state: State):
        changes = 0
        artifact_edit_cmds = {
            self.ai_interface.RETYPE_VARS_CMD, self.ai_interface.RENAME_VARS_CMD, self.ai_interface.RENAME_FUNCS_CMD,
            self.ai_interface.ANSWER_QUESTION_CMD
        }
        cmt_prepends = {
            self.ai_interface.SUMMARIZE_CMD: "==== AI Summarization ====\n",
            self.ai_interface.ID_SOURCE_CMD: "==== AI Source Guess ====\n",
            self.ai_interface.FIND_VULN_CMD: "==== AI Vuln Guess ====\n",
        }

        func_cmt = ""
        new_func = Function(func.addr, func.size, header=FunctionHeader("", func.addr, args={}), stack_vars={})
        for cmd in self.ai_interface.AI_COMMANDS:
            # TODO: make this more explicit and change what is run
            if cmd not in {
                self.ai_interface.RENAME_FUNCS_CMD,
                self.ai_interface.ID_SOURCE_CMD,
                self.ai_interface.SUMMARIZE_CMD,
            }:
                continue

            try:
                resp = self.ai_interface.query_for_cmd(cmd, decompilation=decompilation)
            except Exception as e:
                _l.error(f"Failed to query for cmd {cmd} with error {e}")
                continue

            if not resp:
                continue

            if cmd not in artifact_edit_cmds:
                if cmd == self.ai_interface.ID_SOURCE_CMD:
                    if "http" not in resp:
                        continue

                func_cmt += cmt_prepends.get(cmd, "") + resp + "\n"
                # fake the comment actually being added to decomp
                decompilation = f"/* {Comment.linewrap_comment(resp)} */\n" + decompilation
                changes += 1

            elif cmd == self.ai_interface.RENAME_VARS_CMD:
                all_names = set(sv.name for _, sv in func.stack_vars.items())
                for off, sv in func.stack_vars.items():
                    old_name = sv.name
                    if old_name in resp:
                        proposed_name = resp[old_name]
                        if not proposed_name or proposed_name == old_name or proposed_name in all_names:
                            continue

                        if off not in new_func.stack_vars:
                            new_func.stack_vars[off] = StackVariable(sv.offset, "", None, func.stack_vars[off].size, func.addr)

                        new_func.stack_vars[off].name = proposed_name
                        decompilation = decompilation.replace(old_name, proposed_name)
                        changes += 1

            elif cmd == self.ai_interface.RETYPE_VARS_CMD:
                for off, sv in func.stack_vars.items():
                    old_name = sv.name
                    if old_name in resp:
                        proposed_type = resp[old_name]
                        if not proposed_type or proposed_type == sv.type:
                            continue

                        if off not in new_func.stack_vars:
                            new_func.stack_vars[off] = StackVariable(sv.offset, "", None, func.stack_vars[off].size, func.addr)

                        new_func.stack_vars[off].type = proposed_type
                        # we dont update decompilation here because it would be too weird
                        changes += 1

            elif cmd == self.ai_interface.RENAME_FUNCS_CMD:
                if func.name in resp:
                    proposed_name = resp[func.name]
                    if proposed_name in self.controller.functions() or not proposed_name or proposed_name == func.name:
                        continue

                    new_func.name = proposed_name
                    _l.info(f"Proposing new name for function {func.name} to {proposed_name}")
                    changes += 1

            elif cmd == self.ai_interface.ANSWER_QUESTION_CMD:
                answers: Dict[str, str] = resp
                current_cmts = state.get_func_comments(func.addr)
                for question, answer in answers.items():
                    for _, current_cmt in current_cmts.items():
                        if question in current_cmt.comment:
                            current_cmt.comment += f"\n{answer}"
                            state.set_comment(current_cmt)
                            changes += 1
                            break

        if changes:
            _l.info(f"Suggesting updates to {func} with diff: {new_func}")
            state.set_function(new_func)

        # send full function comment
        if func_cmt:
            state.set_comment(Comment(new_func.addr, func_cmt, func_addr=new_func.addr, decompiled=True), append=True)
            #self.controller.push_artifact(Comment(new_func.addr, func_cmt, func_addr=new_func.addr, decompiled=True), append=True)
            #self.controller.fill_comment(new_func.addr, user=self.username, artifact=Comment(new_func.addr, func_cmt, func_addr=new_func.addr, decompiled=True), append=True)
            #self.controller.schedule_job(
            #    self.controller.push_artifact,
            #    Comment(new_func.addr, func_cmt, func_addr=new_func.addr, decompiled=True),
            #    blocking=False,
            #    append=True
            #)

        return changes

```

`dailalib/binsync_plugin/varmodel_bs_user.py`:

```py
import logging

from binsync.data import Function, State

from dailalib.binsync_plugin.ai_bs_user import AIBSUser

_l = logging.getLogger(__name__)


class VARModelBSUser(AIBSUser):
    DEFAULT_USERNAME = "varmodel_user"

    """
    Variable Annotation Recovery (VAR) Model
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        try:
            from varmodel import VariableRenamingAPI
        except ImportError:
            _l.error("VARModel is not installed and is still closed source. You will be unable to use this BinSync user.")
            return

        self._renaming_api = VariableRenamingAPI()
    
    def run_all_ai_commands_for_dec(self, decompilation: str, func: Function, state: State):
        try:
            updated_func: Function = self._renaming_api.predict_variable_names(decompilation, func)
        except Exception as e:
            _l.warning(f"Skipping {func} due to exception: {e}")
            return 0

        if updated_func is not None and (updated_func.args or updated_func.stack_vars):
            # count changes
            changes = len(updated_func.args) + len(updated_func.stack_vars)
            state.set_function(updated_func)
            return changes

        return 0

```

`dailalib/configuration.py`:

```py
from libbs.configuration import BSConfig
from typing import Optional, Dict
from platformdirs import user_config_dir
import logging 

_l = logging.getLogger(__name__)


class DAILAConfig(BSConfig):
    """
    Configuration class for LLM API, model, prompt style, and probably other things in the future.
    """
    __slots__ = (
        "save_location", 
        "_config_lock",
        "model",        # LLM Model selected by user, 
        "api_key",     # API keys for selected model,
        "prompt_style", # Prompt style selected by user,
        "custom_endpoint", # Custom OpenAI endpoint
        "custom_model"  # Custom OpenAI model
    )

    def __init__(self, save_location: Optional[str] = None):
        save_location = user_config_dir("daila")
        super().__init__(save_location)
        self.save_location = self.save_location / f"{self.__class__.__name__}.toml"
        self.model = "gpt-4o"
        self.api_key = "THISISAFAKEAPIKEY"
        self.prompt_style = "few-shot"
        self.custom_endpoint = ""
        self.custom_model = ""

```

`dailalib/daila_plugin.py`:

```py
# Run the DAILA server and open the DAILA selector UI.
# @author mahaloz
# @category AI
# @menupath Tools.DAILA.Start DAILA Backend
# @runtime PyGhidra


def create_plugin(*args, **kwargs):
    from dailalib import create_plugin as _create_plugin
    return _create_plugin(*args, **kwargs)

try:
    import idaapi
    has_ida = True
except ImportError:
    has_ida = False
try:
    import angrmanagement
    has_angr = True
except ImportError:
    has_angr = False
try:
    import ghidra
    has_ghidra = True
except ImportError:
    has_ghidra = False
try:
    import binaryninja
    has_binja = True
except ImportError:
    has_binja = False


if has_ghidra or has_binja:
    create_plugin()
elif has_angr:
    from angrmanagement.plugins import BasePlugin
    class AngrBSPluginThunk(BasePlugin):
        def __init__(self, workspace):
            super().__init__(workspace)
            globals()["workspace"] = workspace
            self.plugin = create_plugin()

        def teardown(self):
            pass
elif has_ida:
    # IDA will call create_plugin automatically
    pass


def PLUGIN_ENTRY(*args, **kwargs):
    """
    This is the entry point for IDA to load the plugin.
    """
    return create_plugin(*args, **kwargs)

```

`dailalib/installer.py`:

```py
import textwrap
from pathlib import Path

from libbs.plugin_installer import LibBSPluginInstaller

VARBERT_AVAILABLE = True
try:
    import varbert
except ImportError:
    VARBERT_AVAILABLE = False


class DAILAInstaller(LibBSPluginInstaller):
    def __init__(self):
        super().__init__()
        self.pkg_path = self.find_pkg_files("dailalib")

    def _copy_plugin_to_path(self, path):
        src = self.pkg_path / "daila_plugin.py"
        dst = Path(path) / "daila_plugin.py"
        self.link_or_copy(src, dst, symlink=True)

    def display_prologue(self):
        print(textwrap.dedent("""
        Now installing...
        
        ██████   █████  ██ ██       █████      
        ██   ██ ██   ██ ██ ██      ██   ██     
        ██   ██ ███████ ██ ██      ███████     
        ██   ██ ██   ██ ██ ██      ██   ██     
        ██████  ██   ██ ██ ███████ ██   ██
        
        The Decompiler AI Language Assistant                                                         
        """))

    def install_ida(self, path=None, interactive=True):
        path = super().install_ida(path=path, interactive=interactive)
        if not path:
            return

        self._copy_plugin_to_path(path)
        return path

    def install_ghidra(self, path=None, interactive=True):
        path = super().install_ghidra(path=path, interactive=interactive)
        if not path:
            return

        self._copy_plugin_to_path(path)
        return path

    def install_binja(self, path=None, interactive=True):
        path = super().install_binja(path=path, interactive=interactive)
        if not path:
            return

        self._copy_plugin_to_path(path)
        return path

    def install_angr(self, path=None, interactive=True):
        path = super().install_angr(path=path, interactive=interactive)
        if not path:
            return

        path = path / "DAILA"
        path.mkdir(parents=True, exist_ok=True)
        src = self.pkg_path / "plugin.toml"
        dst = Path(path) / "plugin.toml"
        self.link_or_copy(src, dst, symlink=True)
        self._copy_plugin_to_path(path)
        return path

    def display_epilogue(self):
        super().display_epilogue()
        print("")
        if VARBERT_AVAILABLE:
            self.install_local_models()
        else:
            self.warn("VarBERT not installed, reinstall with `pip install dailalib[full]` to enable local models if you would like them.")

    def install_local_models(self):
        self.info("We will now download local models for each decompiler you've installed. Ctrl+C to cancel.")
        self.install_varmodel_models()

    def install_varmodel_models(self):
        self.info("Installing VarBERT models...")
        from varbert import install_model as install_varbert_model
        for target in self._successful_installs:
            install_varbert_model(target, opt_level="O0")

```

`dailalib/llm_chat/__init__.py`:

```py
import logging

from libbs.api import DecompilerInterface
from libbs.decompilers import IDA_DECOMPILER, ANGR_DECOMPILER, BINJA_DECOMPILER, GHIDRA_DECOMPILER

from ..api import AIAPI

_l = logging.getLogger(__name__)


def get_llm_chat_creator(ai_api: AIAPI) -> callable:
    # determine the current decompiler
    current_decompiler = DecompilerInterface.find_current_decompiler()
    add_llm_chat_to_ui = lambda *args, **kwargs: None
    if current_decompiler == IDA_DECOMPILER:
        from dailalib.llm_chat.ida import add_llm_chat_to_ui
    else:
        # TODO we had Binja support, but needed to disable until https://github.com/mahaloz/DAILA/issues/85
        _l.warning(f"LLM Chat not supported for decompiler %s", current_decompiler)

    def llm_chat_creator_wrapper(*args, **kwargs):
        ai_api.info(f"Opening LLM Chat with model {ai_api.model}...")
        return add_llm_chat_to_ui(ai_api=ai_api, *args, **kwargs)

    return llm_chat_creator_wrapper

```

`dailalib/llm_chat/binja.py`:

```py
from binaryninjaui import (
    UIContext,
    DockHandler,
    DockContextHandler,
    UIAction,
    UIActionHandler,
    Menu,
)

import traceback
import sys
import logging

from PySide6.QtWidgets import (
    QDockWidget,
    QWidget,
    QApplication,
    QMenu,
    QMainWindow,
    QMenuBar, QVBoxLayout,
)
from PySide6.QtCore import Qt
from binaryninjaui import DockContextHandler
import binaryninja

from .llm_chat_ui import LLMChatClient
from ..api import AIAPI

_l = logging.getLogger(__name__)

# this code is based on:
# https://github.com/binsync/binsync/blob/fa754795b7d55e4b5de4e12ea7d4b5f9706c23af/plugins/binja_binsync/binja_binsync.py

def find_main_window():
    main_window = None
    for x in QApplication.allWidgets():
        if not isinstance(x, QDockWidget):
            continue
        main_window = x.parent()
        if isinstance(main_window, (QMainWindow, QWidget)):
            break
        else:
            main_window = None

    if main_window is None:
        # oops cannot find the main window
        raise Exception("Main window is not found.")
    return main_window


dockwidgets = [ ]


# shamelessly copied from https://github.com/Vector35/debugger
def create_widget(widget_class, name, parent, data, *args):
    # It is imperative this function return *some* value because Shiboken will try to deref what we return
    # If we return nothing (or throw) there will be a null pointer deref (and we won't even get to see why)
    # So in the event of an error or a nothing, return an empty widget that at least stops the crash
    try:
        # binsync specific code
        if not isinstance(data, binaryninja.BinaryView):
            raise Exception('expected an binary view')
        new_bv = args[0]
        # uses only a bv_controller
        widget = widget_class(new_bv, parent=parent, name=name, data=data)
        if not widget:
            raise Exception('expected widget, got None')

        global dockwidgets

        found = False
        for (bv, widgets) in dockwidgets:
            if bv == data:
                widgets[name] = widget
                found = True

        if not found:
            dockwidgets.append((data, {
                name: widget
            }))

        widget.destroyed.connect(lambda destroyed: destroy_widget(destroyed, widget, data, name))

        return widget
    except Exception:
        traceback.print_exc(file=sys.stderr)
        return QWidget(parent)


def destroy_widget(destroyed, old, data, name):
    # Gotta be careful to delete the correct widget here
    for (bv, widgets) in dockwidgets:
        if bv == data:
            for (name, widget) in widgets.items():
                if widget == old:
                    # If there are no other references to it, this will be the only one and the call
                    # will delete it and invoke __del__.
                    widgets.pop(name)
                    return


class BinjaWidgetBase:
    def __init__(self):
        self._main_window = None
        self._menu_bar = None
        self._tool_menu = None

    @property
    def main_window(self):
        if self._main_window is None:
            self._main_window = find_main_window()
        return self._main_window

    @property
    def menu_bar(self):
        if self._menu_bar is None:
            self._menu_bar = next(
                iter(x for x in self._main_window.children() if isinstance(x, QMenuBar))
            )
        return self._menu_bar

    @property
    def tool_menu(self):
        if self._tool_menu is None:
            self._tool_menu = next(
                iter(
                    x
                    for x in self._menu_bar.children()
                    if isinstance(x, QMenu) and x.title() == u"Tools"
                )
            )
        return self._tool_menu

    def add_tool_menu_action(self, name, func):
        self.tool_menu.addAction(name, func)


class BinjaDockWidget(QWidget, DockContextHandler):
    def __init__(self, name, parent=None):
        QWidget.__init__(self, parent)
        DockContextHandler.__init__(self, self, name)

        self.base = BinjaWidgetBase()

        self.show()

    def toggle(self):
        if self.isVisible():
            self.hide()
        else:
            self.show()


class BinjaWidget(QWidget):
    def __init__(self, tabname):
        super(BinjaWidget, self).__init__()


class LLMChatClientDockWidget(BinjaDockWidget):
    def __init__(self, ai_api, parent=None, name=None, data=None):
        super().__init__(name, parent=parent)
        self.data = data
        self._widget = None
        self.ai_api = ai_api

        self._widget = LLMChatClient(self.ai_api)
        layout = QVBoxLayout()
        layout.addWidget(self._widget)
        self.setLayout(layout)


def add_llm_chat_to_ui(*args, ai_api: AIAPI = None, **kwargs):
    # control panel (per BV)
    dock_handler = DockHandler.getActiveDockHandler()
    dock_handler.addDockWidget(
        "LLM Chat",
        lambda n, p, d: create_widget(LLMChatClientDockWidget, n, p, d, ai_api._dec_interface.bv),
        Qt.RightDockWidgetArea,
        Qt.Vertical,
        True
    )
```

`dailalib/llm_chat/ida.py`:

```py
import logging

from PyQt5 import sip
from PyQt5.QtWidgets import QWidget, QVBoxLayout

import idaapi

from libbs.ui.version import set_ui_version
set_ui_version("PyQt5")

from dailalib.llm_chat.llm_chat_ui import LLMChatClient

_l = logging.getLogger(__name__)

# disable the annoying "Running Python script" wait box that freezes IDA at times
idaapi.set_script_timeout(0)


class LLMChatWrapper(object):
    NAME = "LLM Chat"

    def __init__(self, ai_api, context=None):
        # create a dockable view
        self.twidget = idaapi.create_empty_widget(LLMChatWrapper.NAME)
        self.widget = sip.wrapinstance(int(self.twidget), QWidget)
        self.widget.name = LLMChatWrapper.NAME
        self.width_hint = 250

        self._ai_api = ai_api
        self._context = context
        self._w = None

        self._init_widgets()

    def _init_widgets(self):
        self._w = LLMChatClient(self._ai_api, context=self._context)
        layout = QVBoxLayout()
        layout.addWidget(self._w)
        layout.setContentsMargins(2,2,2,2)
        self.widget.setLayout(layout)


def add_llm_chat_to_ui(*args, ai_api=None, context=None, **kwargs):
    """
    Open the control panel view and attach it to IDA View-A or Pseudocode-A.
    """
    wrapper = LLMChatWrapper(ai_api, context=context)
    if not wrapper.twidget:
        _l.info("Unable to find a widget to attach to. You are likely running headlessly")
        return None

    flags = idaapi.PluginForm.WOPN_TAB | idaapi.PluginForm.WOPN_RESTORE | idaapi.PluginForm.WOPN_PERSIST
    idaapi.display_widget(wrapper.twidget, flags)
    wrapper.widget.visible = True
    target = "Pseudocode-A"
    dwidget = idaapi.find_widget(target)

    if not dwidget:
        target = "IDA View-A"

    idaapi.set_dock_pos(LLMChatWrapper.NAME, target, idaapi.DP_RIGHT)

```

`dailalib/llm_chat/llm_chat_ui.py`:

```py
import typing

from PyQt5.QtWidgets import (
    QApplication, QWidget, QVBoxLayout, QHBoxLayout, QTextEdit,
    QPushButton, QLabel, QScrollArea, QFrame
)
from PyQt5.QtCore import Qt, QThread, pyqtSignal, QCoreApplication
from PyQt5.QtGui import QFont

from libbs.artifacts.context import Context

if typing.TYPE_CHECKING:
    from ..api.litellm.litellm_api import LiteLLMAIAPI

CONTEXT_PROMPT = """
You are reverse engineering assistant that helps to understand binaries in a decompiler. Given decompilation 
and questions you answer them to the best of your ability. Here is the function you are currently working on:
```
DEC_TEXT
```

Acknowledging the context, by responding with:
"I see you are working on function <function_name>. How can I help you today?"
"""


class LLMChatClient(QWidget):
    def __init__(self, ai_api: "LiteLLMAIAPI", parent=None, context: Context = None):
        super(LLMChatClient, self).__init__(parent)
        self.model = ai_api.get_model()
        self.custom_endpoint = ai_api.get_custom_endpoint()
        self.ai_api = ai_api
        self.context = context
        self.setWindowTitle('LLM Chat')
        self.setGeometry(100, 100, 600, 800)

        # Main layout
        self.layout = QVBoxLayout(self)
        self.setLayout(self.layout)

        # Scroll area for chat messages
        self.chat_area = QScrollArea()
        self.chat_area.setWidgetResizable(True)
        self.chat_content = QWidget()
        self.chat_layout = QVBoxLayout(self.chat_content)
        self.chat_layout.addStretch(1)
        self.chat_area.setWidget(self.chat_content)

        # Input area
        self.input_text = QTextEdit()
        self.input_text.setFixedHeight(80)

        # Send button
        self.send_button = QPushButton('Send')
        self.send_button.setFixedHeight(40)
        self.send_button.clicked.connect(lambda: self.send_message())

        # Arrange input and send button horizontally
        self.input_layout = QHBoxLayout()
        self.input_layout.addWidget(self.input_text)
        self.input_layout.addWidget(self.send_button)

        # Add widgets to the main layout
        self.layout.addWidget(self.chat_area)
        self.layout.addLayout(self.input_layout)

        # Chat history
        self.chat_history = []
        self.thread = None

        # model check
        if not self.model:
            self.ai_api.warning("No model set. Close the chat window and please set a model before using the chat")
            return

        # preset the very first interaction
        # create a context for this first message
        if ai_api.chat_use_ctx:
            ai_api.info("Collecting context for the current function...")
            if context is None:
                context = ai_api._dec_interface.gui_active_context()
            dec = ai_api._dec_interface.decompile(context.func_addr)
            dec_text = dec.text if dec is not None else None
            if dec_text:
                # put a number in front of each line
                dec_lines = dec_text.split("\n")
                dec_text = "\n".join([f"{i + 1} {line}" for i, line in enumerate(dec_lines)])
                prompt = CONTEXT_PROMPT.replace("DEC_TEXT", dec_text)
                # set the text to the prompt
                self.input_text.setText(prompt)
                self.send_message(add_text=False, role="system")
        else:
            self.input_text.setText("You are an assistant that helps understand code. Start the conversation by simply saying 'Hello, how can I help you?'.")
            self.send_message(add_text=False, role="system")

    def add_message(self, text, is_user):
        # Message bubble
        message_label = QLabel(text)
        message_label.setWordWrap(True)
        message_label.setFont(QFont('Arial', 12))
        message_label.setTextInteractionFlags(Qt.TextSelectableByMouse)

        # Bubble styling
        bubble = QFrame()
        bubble_layout = QHBoxLayout()
        bubble.setLayout(bubble_layout)

        if is_user:
            # User message on the right
            message_label.setStyleSheet("""
                background-color: #DCF8C6;
                color: black;
                padding: 10px;
                border-radius: 10px;
            """)
            bubble_layout.addStretch()
            bubble_layout.addWidget(message_label)
        else:
            # Assistant message on the left
            message_label.setStyleSheet("""
                background-color: #FFFFFF;
                color: black;
                padding: 10px;
                border-radius: 10px;
            """)
            bubble_layout.addWidget(message_label)
            bubble_layout.addStretch()

        self.chat_layout.insertWidget(self.chat_layout.count() - 1, bubble)
        QCoreApplication.processEvents()
        self.chat_area.verticalScrollBar().setValue(self.chat_area.verticalScrollBar().maximum())

    def send_message(self, add_text=True, role="user"):
        user_text = self.input_text.toPlainText().strip()
        if not user_text:
            return

        # do aiapi calback
        if self.ai_api:
            send_callback = self.ai_api.chat_event_callbacks.get("send", None)
            if send_callback:
                send_callback(user_text, model=self.model)

        # Display user message
        if add_text:
            self.add_message(user_text, is_user=True)
        self.input_text.clear()

        # Append to chat history
        self.chat_history.append({"role": role, "content": user_text})

        # Disable input while waiting for response
        self.input_text.setDisabled(True)
        self.send_button.setDisabled(True)

        # Start a thread to get the response
        self.thread = LLMThread(
            self.chat_history, self.model, custom_endpoint=self.custom_endpoint, api_key=self.ai_api.api_key
        )
        self.thread.response_received.connect(lambda msg: self.receive_message(msg))
        self.thread.start()

    def receive_message(self, assistant_message):
        # Display assistant message
        self.add_message(assistant_message, is_user=False)

        # do aiapi calback
        if self.ai_api:
            recv_callback = self.ai_api.chat_event_callbacks.get("receive", None)
            if recv_callback:
                recv_callback(assistant_message, model=self.model)

        # Append to chat history
        self.chat_history.append({"role": "user", "content": assistant_message})

        # Re-enable input
        self.input_text.setDisabled(False)
        self.send_button.setDisabled(False)

    def closeEvent(self, event):
        # Ensure that the thread is properly terminated when the window is closed
        if hasattr(self, 'thread') and self.thread.isRunning():
            self.thread.terminate()
        event.accept()


class LLMThread(QThread):
    response_received = pyqtSignal(str)

    def __init__(self, chat_history, model_name, custom_endpoint=None, api_key=None):
        super().__init__()
        self.chat_history = chat_history.copy()
        self.model_name = model_name
        self.custom_endpoint = custom_endpoint
        self.api_key = api_key

    def run(self):
        import litellm

        # must set modify_params to True to deal with Claude
        litellm.modify_params = True
        response = litellm.completion(
            model=self.model_name,
            messages=self.chat_history,
            timeout=60 if not self.custom_endpoint else 300,
            api_base=self.custom_endpoint if self.custom_endpoint else None,  # Use custom endpoint if set
            api_key=self.api_key if not self.custom_endpoint else "dummy" # In most of cases custom endpoint doesn't need the api_key
        )
        litellm.modify_params = False

        try:
            answer = response.choices[0].message.content
        except (KeyError, IndexError) as e:
            answer = f"Error: {e}. Please close the window and try again."

        self.response_received.emit(answer)

```

`dailalib/plugin.toml`:

```toml
[meta]
plugin_metadata_version = 0

[plugin]
name = "DAILA_plugin"
shortname = "DAILA_plugin"
version = "0.0.0"
description = ""
long_description = "The Decompiler Artificial Intelligence Assistant (DAILA)."
platforms = ["windows", "linux", "macos"]
min_angr_version = "9.0.0.0"
author = "mahaloz"
entrypoints = ["daila_plugin.py"]
```

`setup.cfg`:

```cfg
[metadata]
name = dailalib
version = attr: dailalib.__version__
url = https://github.com/mahaloz/DAILA
classifiers =
    License :: OSI Approved :: BSD License
    Programming Language :: Python :: 3
    Programming Language :: Python :: 3.10
license = BSD 2 Clause
license_files = LICENSE
description = The Decompiler Artificial Intelligence Language Assistant (DAILA) is a tool for adding AI to decompilers.
long_description = file: README.md
long_description_content_type = text/markdown

[options]
install_requires =
    litellm>=1.44.27
    tiktoken
    Jinja2
    libbs[ghidra]>=3.1.0

python_requires = >= 3.10
include_package_data = True
packages = find:

[options.entry_points]
console_scripts =
    daila = dailalib.__main__:main

[options.extras_require]
full =
    varbert>=2.3.0
    PySide6-Essentials>=6.4.2


```

`setup.py`:

```py
from setuptools import setup

setup()

```

`tests.py`:

```py
import sys
import subprocess

import unittest
import dailalib


class TestCommandline(unittest.TestCase):
    def test_cli(self):
        # run the CLI version check
        output = subprocess.run(["daila", "--version"], capture_output=True)
        version = output.stdout.decode().strip()
        assert version == dailalib.__version__

    def test_resources_exist(self):
        from dailalib.api.litellm.prompts import PROMPTS
        assert len(PROMPTS) > 0


if __name__ == "__main__":
    unittest.main(argv=sys.argv)

```