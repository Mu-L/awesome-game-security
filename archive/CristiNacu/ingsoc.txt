Project Path: arc_CristiNacu_ingsoc_htaadgg1

Source Tree:

```txt
arc_CristiNacu_ingsoc_htaadgg1
├── IntelPTDriver
│   ├── IntelPTDriver
│   │   ├── Debug.h
│   │   ├── Device.c
│   │   ├── Device.h
│   │   ├── Driver.c
│   │   ├── DriverCommon.h
│   │   ├── DriverUtils.c
│   │   ├── DriverUtils.h
│   │   ├── IntelPTDriver.inf
│   │   ├── IntelPTDriver.vcxproj
│   │   ├── IntelPTDriver.vcxproj.filters
│   │   ├── IntelPTDriver.vcxproj.user
│   │   ├── IntelProcessorTraceDefs.h
│   │   ├── ProcessorTrace.c
│   │   ├── ProcessorTrace.h
│   │   ├── ProcessorTraceShared.h
│   │   ├── ProcessorTraceWindowsCommands.c
│   │   ├── ProcessorTraceWindowsCommands.h
│   │   ├── ProcessorTraceWindowsCommunication.c
│   │   ├── ProcessorTraceWindowsCommunication.h
│   │   ├── ProcessorTraceWindowsControl.c
│   │   ├── ProcessorTraceWindowsControl.h
│   │   └── Public.h
│   ├── IntelPTDriver.sln
│   ├── IntelPTUserModeApp
│   │   ├── Commands.c
│   │   ├── Commands.h
│   │   ├── Communication.c
│   │   ├── Communication.h
│   │   ├── Globals.h
│   │   ├── IntelPTUserModeApp.vcxproj
│   │   ├── IntelPTUserModeApp.vcxproj.filters
│   │   ├── IntelPTUserModeApp.vcxproj.user
│   │   ├── JsonTools.c
│   │   ├── KafkaUtils.c
│   │   ├── KafkaUtils.h
│   │   ├── Main.c
│   │   ├── UserInterface.c
│   │   ├── UserInterface.h
│   │   ├── config.cfg
│   │   └── packages.config
│   ├── TraceDecoder
│   │   ├── PacketInterpreter
│   │   │   ├── IntelPacketDefinitions.py
│   │   │   ├── InternalPacketDefinitions.py
│   │   │   ├── PacketHandlers.py
│   │   │   └── PacketInterpreter.py
│   │   ├── TraceDecoder.py
│   │   ├── TraceSamples
│   │   │   └── trace_loop_int3
│   │   ├── __pycache__
│   │   │   └── TraceDecoder.cpython-36.pyc
│   │   ├── config.json
│   │   └── file.bin
│   ├── TraceExamples
│   │   ├── T1
│   │   ├── T2
│   │   ├── T3CLEAN
│   │   ├── trace_A+B
│   │   ├── trace_A+B_2
│   │   ├── trace_bo_no_ovf
│   │   └── trace_loop_int3
│   ├── TracedApp
│   │   ├── Main.c
│   │   ├── TracedApp.vcxproj
│   │   ├── TracedApp.vcxproj.filters
│   │   └── TracedApp.vcxproj.user
│   └── packages
│       └── librdkafka.redist.1.8.2
│           ├── CONFIGURATION.md
│           ├── LICENSES.txt
│           ├── README.md
│           ├── build
│           │   ├── librdkafka.redist.props
│           │   └── native
│           │       ├── include
│           │       │   └── librdkafka
│           │       │       ├── rdkafka.h
│           │       │       ├── rdkafka_mock.h
│           │       │       └── rdkafkacpp.h
│           │       └── librdkafka.redist.targets
│           └── librdkafka.redist.1.8.2.nupkg
└── README.md

```

`IntelPTDriver/IntelPTDriver.sln`:

```sln

Microsoft Visual Studio Solution File, Format Version 12.00
# Visual Studio Version 16
VisualStudioVersion = 16.0.31112.23
MinimumVisualStudioVersion = 10.0.40219.1
Project("{8BC9CEB8-8B4A-11D0-8D11-00A0C91BC942}") = "IntelPTDriver", "IntelPTDriver\IntelPTDriver.vcxproj", "{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}"
EndProject
Project("{8BC9CEB8-8B4A-11D0-8D11-00A0C91BC942}") = "IntelPTUserModeApp", "IntelPTUserModeApp\IntelPTUserModeApp.vcxproj", "{61979C7D-EDF3-421D-B0E2-C282B7A0CB6D}"
EndProject
Project("{8BC9CEB8-8B4A-11D0-8D11-00A0C91BC942}") = "TracedApp", "TracedApp\TracedApp.vcxproj", "{F8D13A4F-146C-489B-B5EA-5B4BCCFE3DC2}"
EndProject
Global
	GlobalSection(SolutionConfigurationPlatforms) = preSolution
		Debug|ARM = Debug|ARM
		Debug|ARM64 = Debug|ARM64
		Debug|x64 = Debug|x64
		Debug|x86 = Debug|x86
		Release|ARM = Release|ARM
		Release|ARM64 = Release|ARM64
		Release|x64 = Release|x64
		Release|x86 = Release|x86
	EndGlobalSection
	GlobalSection(ProjectConfigurationPlatforms) = postSolution
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Debug|ARM.ActiveCfg = Debug|ARM
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Debug|ARM.Build.0 = Debug|ARM
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Debug|ARM.Deploy.0 = Debug|ARM
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Debug|ARM64.ActiveCfg = Debug|ARM64
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Debug|ARM64.Build.0 = Debug|ARM64
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Debug|ARM64.Deploy.0 = Debug|ARM64
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Debug|x64.ActiveCfg = Debug|x64
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Debug|x64.Build.0 = Debug|x64
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Debug|x64.Deploy.0 = Debug|x64
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Debug|x86.ActiveCfg = Debug|Win32
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Debug|x86.Build.0 = Debug|Win32
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Debug|x86.Deploy.0 = Debug|Win32
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Release|ARM.ActiveCfg = Release|ARM
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Release|ARM.Build.0 = Release|ARM
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Release|ARM.Deploy.0 = Release|ARM
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Release|ARM64.ActiveCfg = Release|ARM64
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Release|ARM64.Build.0 = Release|ARM64
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Release|ARM64.Deploy.0 = Release|ARM64
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Release|x64.ActiveCfg = Release|x64
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Release|x64.Build.0 = Release|x64
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Release|x64.Deploy.0 = Release|x64
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Release|x86.ActiveCfg = Release|Win32
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Release|x86.Build.0 = Release|Win32
		{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}.Release|x86.Deploy.0 = Release|Win32
		{61979C7D-EDF3-421D-B0E2-C282B7A0CB6D}.Debug|ARM.ActiveCfg = Debug|Win32
		{61979C7D-EDF3-421D-B0E2-C282B7A0CB6D}.Debug|ARM64.ActiveCfg = Debug|Win32
		{61979C7D-EDF3-421D-B0E2-C282B7A0CB6D}.Debug|x64.ActiveCfg = Debug|x64
		{61979C7D-EDF3-421D-B0E2-C282B7A0CB6D}.Debug|x64.Build.0 = Debug|x64
		{61979C7D-EDF3-421D-B0E2-C282B7A0CB6D}.Debug|x86.ActiveCfg = Debug|Win32
		{61979C7D-EDF3-421D-B0E2-C282B7A0CB6D}.Debug|x86.Build.0 = Debug|Win32
		{61979C7D-EDF3-421D-B0E2-C282B7A0CB6D}.Release|ARM.ActiveCfg = Release|Win32
		{61979C7D-EDF3-421D-B0E2-C282B7A0CB6D}.Release|ARM64.ActiveCfg = Release|Win32
		{61979C7D-EDF3-421D-B0E2-C282B7A0CB6D}.Release|x64.ActiveCfg = Release|x64
		{61979C7D-EDF3-421D-B0E2-C282B7A0CB6D}.Release|x64.Build.0 = Release|x64
		{61979C7D-EDF3-421D-B0E2-C282B7A0CB6D}.Release|x86.ActiveCfg = Release|Win32
		{61979C7D-EDF3-421D-B0E2-C282B7A0CB6D}.Release|x86.Build.0 = Release|Win32
		{F8D13A4F-146C-489B-B5EA-5B4BCCFE3DC2}.Debug|ARM.ActiveCfg = Debug|Win32
		{F8D13A4F-146C-489B-B5EA-5B4BCCFE3DC2}.Debug|ARM64.ActiveCfg = Debug|Win32
		{F8D13A4F-146C-489B-B5EA-5B4BCCFE3DC2}.Debug|x64.ActiveCfg = Debug|x64
		{F8D13A4F-146C-489B-B5EA-5B4BCCFE3DC2}.Debug|x64.Build.0 = Debug|x64
		{F8D13A4F-146C-489B-B5EA-5B4BCCFE3DC2}.Debug|x86.ActiveCfg = Debug|Win32
		{F8D13A4F-146C-489B-B5EA-5B4BCCFE3DC2}.Debug|x86.Build.0 = Debug|Win32
		{F8D13A4F-146C-489B-B5EA-5B4BCCFE3DC2}.Release|ARM.ActiveCfg = Release|Win32
		{F8D13A4F-146C-489B-B5EA-5B4BCCFE3DC2}.Release|ARM64.ActiveCfg = Release|Win32
		{F8D13A4F-146C-489B-B5EA-5B4BCCFE3DC2}.Release|x64.ActiveCfg = Release|x64
		{F8D13A4F-146C-489B-B5EA-5B4BCCFE3DC2}.Release|x64.Build.0 = Release|x64
		{F8D13A4F-146C-489B-B5EA-5B4BCCFE3DC2}.Release|x86.ActiveCfg = Release|Win32
		{F8D13A4F-146C-489B-B5EA-5B4BCCFE3DC2}.Release|x86.Build.0 = Release|Win32
	EndGlobalSection
	GlobalSection(SolutionProperties) = preSolution
		HideSolutionNode = FALSE
	EndGlobalSection
	GlobalSection(ExtensibilityGlobals) = postSolution
		SolutionGuid = {0434CC2A-9C12-4205-85D5-5C96E8B89721}
	EndGlobalSection
EndGlobal

```

`IntelPTDriver/IntelPTDriver/Debug.h`:

```h
#ifndef _DEBUG_H_
#define _DEBUG_H_

#define DEBUGGING_ACTIVE FALSE      //  Set to true for debugging logs / dbgbreaks

#if DEBUGGING_ACTIVE 
#define DEBUG_PRINT(...)	KdPrintEx((DPFLTR_IHVDRIVER_ID, DPFLTR_ERROR_LEVEL, __VA_ARGS__))
#define DEBUG_STOP()		DbgBreakPoint()
#else
#define DEBUG_PRINT(...)    (__VA_ARGS__)
#define DEBUG_STOP()
#endif

#endif // ! _DEBUG_H_


```

`IntelPTDriver/IntelPTDriver/Device.c`:

```c
#include "Device.h"
#include "Debug.h"

# define UninitDeviceInit(deviceInit)	WdfDeviceInitFree(deviceInit)

DECLARE_CONST_UNICODE_STRING(
COMM_QUEUE_DEVICE_PROTECTION,
L"D:P(A;;GA;;;SY)(A;;GRGWGX;;;BA)(A;;GRGWGX;;;WD)(A;;GRGWGX;;;RC)");
extern const UNICODE_STRING  COMM_QUEUE_DEVICE_PROTECTION;

DECLARE_CONST_UNICODE_STRING(
COMM_QUEUE_DEVICE_PROTECTION_FULL,
L"D:P(A;;GA;;;SY)(A;;GA;;;BA)(A;;GA;;;WD)(A;;GA;;;RC)");
extern const UNICODE_STRING  COMM_QUEUE_DEVICE_PROTECTION_FULL;

NTSTATUS 
InitDeviceInit(
	_In_	WDFDRIVER				Driver,
	_In_	DEVICE_INIT_SETTINGS	*DeviceSettings,
	_Out_	PWDFDEVICE_INIT			*DeviceInit
)
{
	NTSTATUS						status;
	PWDFDEVICE_INIT                 deviceInit = NULL;
	UNICODE_STRING                  deviceName = { 0 };

	// Initialize the deviceInit object
	deviceInit = WdfControlDeviceInitAllocate(Driver, &COMM_QUEUE_DEVICE_PROTECTION_FULL);
	if (deviceInit == NULL)
	{
		return STATUS_INSUFFICIENT_RESOURCES;
	}

	// Set native device name
	RtlInitUnicodeString(&deviceName, DeviceSettings->NativeDeviceName);
	status = WdfDeviceInitAssignName(deviceInit, &deviceName);
	if (!NT_SUCCESS(status))
	{
		goto cleanup;
	}

	// Set security string to the device
	status = WdfDeviceInitAssignSDDLString(deviceInit, &COMM_QUEUE_DEVICE_PROTECTION_FULL);
	if (!NT_SUCCESS(status))
	{
		goto cleanup;
	}

	// Set device characteristics
	WdfDeviceInitSetCharacteristics(
		deviceInit, 
		DeviceSettings->DeviceCharacteristics, 
		(!DeviceSettings->OverrideDriverCharacteristics)
	);

	// Return the device init
	*DeviceInit = deviceInit;
	return status;

cleanup:
	if (deviceInit)
	{
		UninitDeviceInit(deviceInit);
		deviceInit = NULL;
	}

	*DeviceInit = deviceInit;
	return status;
}

NTSTATUS 
InitFileConfig(
	_In_	FILE_SETTINGS *FileSettings,
	_Out_	PWDF_FILEOBJECT_CONFIG FileConfig
)
{
	WDF_FILEOBJECT_CONFIG_INIT(
		FileConfig,
		FileSettings->FileCreateCallback,
		FileSettings->FileCloseCallback,
		FileSettings->FileCleanupCallback
	);

	FileConfig->AutoForwardCleanupClose = FileSettings->AutoForwardCleanupClose ? WdfTrue : WdfFalse;
	return STATUS_SUCCESS;
}

NTSTATUS
InitDeviceControl(
	_In_	PWDFDEVICE_INIT			DeviceInit,
	_In_	PWDF_FILEOBJECT_CONFIG  FileConfig,
	_In_	DEVICE_CONFIG_SETTINGS  *DeviceConfigSettings,
	_Out_	WDFDEVICE				*WdfDevice
)
{
	NTSTATUS					status;
	UNICODE_STRING				deviceName = { 0 };
	WDFDEVICE					controlDevice = NULL;
	WDF_OBJECT_ATTRIBUTES       objAttributes;

	WDF_OBJECT_ATTRIBUTES_INIT_CONTEXT_TYPE(&objAttributes, COMM_QUEUE_DEVICE_CONTEXT);

	objAttributes.ExecutionLevel = WdfExecutionLevelPassive;
	objAttributes.EvtCleanupCallback = DeviceConfigSettings->CleanupCallback;
	objAttributes.EvtDestroyCallback = DeviceConfigSettings->DestroyCallback;

	WdfDeviceInitSetFileObjectConfig(DeviceInit, FileConfig, WDF_NO_OBJECT_ATTRIBUTES);
	status = WdfDeviceCreate(&DeviceInit, &objAttributes, &controlDevice);
	if (!NT_SUCCESS(status))
	{
		goto cleanup;
	}

	RtlInitUnicodeString(&deviceName, DeviceConfigSettings->UserDeviceName);
	status = WdfDeviceCreateSymbolicLink(controlDevice, &deviceName);
	if (!NT_SUCCESS(status))
	{
		goto cleanup;
	}

	*WdfDevice = controlDevice;
	return status;

cleanup:
	if (controlDevice)
	{
		UninitDevice(controlDevice);
	}

	*WdfDevice = NULL;
	return status;
}

NTSTATUS
InitDevice(
	_In_ WDFDRIVER WdfDriver,
	_In_ DEVICE_SETTINGS *DeviceSettings,
	_Out_ WDFDEVICE *Device
)
{
	NTSTATUS status;
	PWDFDEVICE_INIT deviceInit;
	WDF_FILEOBJECT_CONFIG fileConfig;
	WDFDEVICE device = NULL;

	DEBUG_PRINT("InitDevice\n");

	status = InitDeviceInit(
		WdfDriver,
		&DeviceSettings->DeviceInitSettings,
		&deviceInit
	);
	if (!NT_SUCCESS(status))
	{
		DEBUG_PRINT("InitDeviceInit error status %X\n", status);
		goto cleanup;
	}

	status = InitFileConfig(
		&DeviceSettings->FileSettings,
		&fileConfig
	);
	if (!NT_SUCCESS(status))
	{
		DEBUG_PRINT("InitFileConfig error status %X\n", status);
		goto cleanup;
	}

	status = InitDeviceControl(
		deviceInit,
		&fileConfig,
		&DeviceSettings->DeviceConfigSettings,
		&device
	);
	if (!NT_SUCCESS(status))
	{
		DEBUG_PRINT("InitDeviceControl error status %X\n", status);
		goto cleanup;
	}

	*Device = device;
	return status;

cleanup:
	DEBUG_PRINT("InitDevice error status %X\n", status);

	if (device)
	{
		UninitDevice(device);
		device = NULL;
	}
	if (deviceInit)
	{
		UninitDeviceInit(deviceInit);
	}

	*Device = device;
	return status;
}

/// TODO

VOID
DeviceEvtFileCreate(
	_In_ WDFDEVICE Device,
	_In_ WDFREQUEST Request,
	_In_ WDFFILEOBJECT FileObject
)
{
	UNREFERENCED_PARAMETER(Device);
	UNREFERENCED_PARAMETER(Request);
	UNREFERENCED_PARAMETER(FileObject);

	WdfRequestComplete(Request, STATUS_SUCCESS);

}

VOID
DeviceEvtFileClose(
	_In_ WDFFILEOBJECT FileObject
)
{
	UNREFERENCED_PARAMETER(FileObject);
}

VOID
DeviceIngonreOperation(
	_In_ WDFFILEOBJECT FileObject
)
{
	UNREFERENCED_PARAMETER(FileObject);
}
```

`IntelPTDriver/IntelPTDriver/Device.h`:

```h
#ifndef _DEVICE_H_
#define _DEVICE_H_

#include "DriverCommon.h"
#include "Public.h"

# define UninitDevice(device)			WdfObjectDelete(device)

typedef struct _DEVICE_INIT_SETTINGS {

	PWCHAR NativeDeviceName;
	ULONG DeviceCharacteristics;
	BOOLEAN OverrideDriverCharacteristics;

} DEVICE_INIT_SETTINGS;

typedef struct _FILE_SETTINGS {
	
	BOOLEAN AutoForwardCleanupClose;
	PFN_WDF_DEVICE_FILE_CREATE FileCreateCallback;
	PFN_WDF_FILE_CLOSE FileCloseCallback;
	PFN_WDF_FILE_CLEANUP FileCleanupCallback;

} FILE_SETTINGS;

typedef struct _DEVICE_CONFIG_SETTINGS {

	PCWSTR UserDeviceName;
	PFN_WDF_OBJECT_CONTEXT_CLEANUP CleanupCallback;
	PFN_WDF_OBJECT_CONTEXT_DESTROY DestroyCallback;

} DEVICE_CONFIG_SETTINGS;


typedef struct _DEVICE_SETTINGS {

	DEVICE_INIT_SETTINGS DeviceInitSettings;
	FILE_SETTINGS FileSettings;
	DEVICE_CONFIG_SETTINGS DeviceConfigSettings;

} DEVICE_SETTINGS;

VOID
DeviceEvtFileCreate(
	_In_ WDFDEVICE Device,
	_In_ WDFREQUEST Request,
	_In_ WDFFILEOBJECT FileObject
);

VOID
DeviceEvtFileClose(
	_In_ WDFFILEOBJECT FileObject
);

VOID
DeviceIngonreOperation(
	_In_ WDFFILEOBJECT FileObject
);

NTSTATUS
InitDevice(
	_In_ WDFDRIVER WdfDriver,
	_In_ DEVICE_SETTINGS* DeviceSettings,
	_Out_ WDFDEVICE* Device
);

#endif
```

`IntelPTDriver/IntelPTDriver/Driver.c`:

```c
#include "DriverCommon.h"
#include "Device.h"
#include "Debug.h"
#include "ProcessorTrace.h"
#include "ProcessorTraceWindowsCommands.h"
#include "ProcessorTraceWindowsCommunication.h"
#include "ProcessorTraceWindowsControl.h"

DRIVER_INITIALIZE DriverEntry;
EVT_WDF_DRIVER_DEVICE_ADD DeviceAdd;

DEVICE_SETTINGS DEFAULT_DEVICE_SETTINGS = {
    .DeviceInitSettings = {
        .NativeDeviceName = SAMPLE_DEVICE_NATIVE_NAME,
        .DeviceCharacteristics = (FILE_DEVICE_SECURE_OPEN | FILE_CHARACTERISTIC_PNP_DEVICE),
        .OverrideDriverCharacteristics = TRUE
    },
    .FileSettings = {
        .AutoForwardCleanupClose = TRUE,
        .FileCreateCallback = DeviceEvtFileCreate,
        .FileCloseCallback = DeviceEvtFileClose,
        .FileCleanupCallback = DeviceIngonreOperation
    },
    .DeviceConfigSettings = {
        .UserDeviceName = SAMPLE_DEVICE_USER_NAME,
        .CleanupCallback = NULL,
        .DestroyCallback = NULL
    }
};

IO_QUEUE_SETTINGS DEFAULT_QUEUE_SETTINGS = {
    
    .DefaultQueue = TRUE,
    .QueueType = WdfIoQueueDispatchParallel,

    .Callbacks.IoQueueIoDefault = (PFN_WDF_IO_QUEUE_IO_DEFAULT)PtwCommunicationIngonreOperation,
    .Callbacks.IoQueueIoRead = (PFN_WDF_IO_QUEUE_IO_READ)PtwCommunicationIngonreOperation,
    .Callbacks.IoQueueIoWrite = (PFN_WDF_IO_QUEUE_IO_WRITE)PtwCommunicationIngonreOperation,
    .Callbacks.IoQueueIoStop = (PFN_WDF_IO_QUEUE_IO_STOP)PtwCommunicationIngonreOperation,
    .Callbacks.IoQueueIoResume = (PFN_WDF_IO_QUEUE_IO_RESUME)PtwCommunicationIngonreOperation,
    .Callbacks.IoQueueIoCanceledOnQueue = (PFN_WDF_IO_QUEUE_IO_CANCELED_ON_QUEUE)PtwCommunicationIngonreOperation,

    .Callbacks.IoQueueIoDeviceControl = (PFN_WDF_IO_QUEUE_IO_DEVICE_CONTROL)PtwCommunicationIoControlCallback,
    .Callbacks.IoQueueIoInternalDeviceControl = (PFN_WDF_IO_QUEUE_IO_INTERNAL_DEVICE_CONTROL)PtwCommunicationIngonreOperation
};

COMM_IO_COMMAND DEFAULT_COMMAND_CALLBACKS[] = {

    // COMM_TYPE_TEST
        PtwCommandTest,

    // COMM_TYPE_QUERY_IPT_CAPABILITIES
        PtwCommandQueryCapabilities,

    // COMM_TYPE_SETUP_IPT
        PtwCommandTraceProcess,

    // COMM_TYPE_GET_BUFFER
        PtwCommandGetBuffer,

    // COMM_TYPE_FREE_BUFFER
        PtwCommandFreeBuffer
};


VOID
DriverUnload(
    IN WDFDRIVER Driver
)
{
    UNREFERENCED_PARAMETER(Driver);
    DbgBreakPoint();
}

NTSTATUS
DeviceAdd(
    _In_    WDFDRIVER       Driver,
    _Inout_ PWDFDEVICE_INIT DeviceInit
)
{
    PAGED_CODE();
    UNREFERENCED_PARAMETER(Driver);
    UNREFERENCED_PARAMETER(DeviceInit);

    KdPrintEx((DPFLTR_IHVDRIVER_ID, DPFLTR_ERROR_LEVEL, "KmdfHelloWorld: KmdfHelloWorldEvtDeviceAdd\n"));

    return STATUS_SUCCESS;
}

NTSTATUS
DriverEntry(
    _In_ PDRIVER_OBJECT     DriverObject,
    _In_ PUNICODE_STRING    RegistryPath
)
{

    // NTSTATUS variable to record success or failure
    NTSTATUS status;
    WDFDRIVER wdfdriver;

    // Allocate the driver configuration object
    WDF_DRIVER_CONFIG config;
    WDF_OBJECT_ATTRIBUTES  attributes;

    WDFDEVICE device;
    WDFQUEUE defaultQueue = {0};
    LARGE_INTEGER processorFrequency;

    // Initialize the driver configuration object to register the
    // entry point for the EvtDeviceAdd callback, KmdfHelloWorldEvtDeviceAdd
    WDF_DRIVER_CONFIG_INIT(&config, DeviceAdd);
    WDF_OBJECT_ATTRIBUTES_INIT(&attributes);
    
    config.EvtDriverUnload = DriverUnload;
    
    gDriverData.PacketIdCounter = 0;
    gDriverData.SequenceIdCounter = 0;

    KeQueryPerformanceCounter(&processorFrequency);
    gDriverData.ProcessorFrequency = processorFrequency.QuadPart;

    // Set Driver constants
    gDriverData.IoCallbacks = DEFAULT_COMMAND_CALLBACKS;

    KeInitializeMutex(
        &gDriverData.SequenceMutex,
        0
    );

    // Create the driver object
    status = WdfDriverCreate(
        DriverObject,
        RegistryPath,
        &attributes,
        &config,
        &wdfdriver
    );
    if (!NT_SUCCESS(status))
    {
        KdPrintEx((DPFLTR_IHVDRIVER_ID, DPFLTR_ERROR_LEVEL, "WdfDriverCreate error\n"));
        return status;
    }
    DEBUG_PRINT("WdfDriverCreate OK\n");

    // Create the device object
    status = InitDevice(
        wdfdriver,
        &DEFAULT_DEVICE_SETTINGS,
        &device
    );
    if (!NT_SUCCESS(status) || !device)
    {
        KdPrintEx((DPFLTR_IHVDRIVER_ID, DPFLTR_ERROR_LEVEL, "[ERROR] InitDevice exitted with status %X\n", status));
        return status;
    }
    DEBUG_PRINT("InitDevice OK\n");

    // Initialize the communication queue
    DEBUG_PRINT("Device Addr %p\n", device);

    // Initialize communication queue. The default, parallel one.
    status = PtwCommunicationInit(
        device,
        &DEFAULT_QUEUE_SETTINGS,
        &defaultQueue
    );
    if (!NT_SUCCESS(status))
    {
        // TODO: Uninitialize resources in case of an error
        KdPrintEx((DPFLTR_IHVDRIVER_ID, DPFLTR_ERROR_LEVEL, "[ERROR] InitCommQueue exitted with status %X\n", status));
        return status;
    }
    DEBUG_PRINT("InitCommQueue OK\n");

    KeInitializeEvent(
        &gPagesAvailableEvent,
        NotificationEvent,
        FALSE
    );

    KeInitializeMutex(
        &gCommMutex,
        0
    );

     
    WdfControlFinishInitializing(device);

    PtwInit();

    return status;
}
```

`IntelPTDriver/IntelPTDriver/DriverCommon.h`:

```h
#pragma once
#ifndef _DRIVER_COMMON_H_
#define _DRIVER_COMMON_H_

#include <ntddk.h>
#include <wdf.h>
#include <wdfobject.h>
#include <intrin.h>
#include "DriverUtils.h"

#include "Public.h"
#include "Debug.h"
#include "ProcessorTraceShared.h"

typedef struct _COMM_QUEUE_DEVICE_CONTEXT {
    PVOID            Data;
    WDFQUEUE         NotificationQueue;
    volatile LONG    Sequence;
} COMM_QUEUE_DEVICE_CONTEXT, * PCOMM_QUEUE_DEVICE_CONTEXT;


typedef NTSTATUS (*COMM_IO_COMMAND)(
        size_t InputBufferLength,
        size_t OutputBufferLength,
        PVOID *InputBuffer,
        PVOID *OutputBuffer,
        UINT32 *bytesWritten
    );

typedef struct _PER_CPU_DATA_STRUCTURE {

    unsigned CpuId;

} PER_CPU_DATA_STRUCTURE;

// TODO: Add driver relevant data here
typedef struct _DRIVER_GLOBAL_DATA {

    COMM_IO_COMMAND *IoCallbacks;
    KMUTEX SequenceMutex;
    volatile unsigned SequenceIdCounter;
    volatile unsigned long PacketIdCounter;
    unsigned long long ProcessorFrequency;

} DRIVER_GLOBAL_DATA;


DRIVER_GLOBAL_DATA gDriverData;



WDF_DECLARE_CONTEXT_TYPE_WITH_NAME(COMM_QUEUE_DEVICE_CONTEXT, CommGetContextFromDevice);


#endif
```

`IntelPTDriver/IntelPTDriver/DriverUtils.c`:

```c
#include "DriverUtils.h"

#define PT_QUEUE_TAG 'PTQT'

NTSTATUS 
DuAllocateBuffer(
    size_t BufferSizeInBytes,
    MEMORY_CACHING_TYPE CachingType,
    BOOLEAN ZeroBuffer,
    PVOID* BufferVa,
    PVOID* BufferPa
)
{
    PHYSICAL_ADDRESS minAddr = { .QuadPart = 0x0000000001000000 };
    PHYSICAL_ADDRESS maxAddr = { .QuadPart = 0x0000001000000000 };
    PHYSICAL_ADDRESS zero = { .QuadPart = 0 };

    PVOID buffVa = MmAllocateContiguousMemorySpecifyCache(
        BufferSizeInBytes,
        minAddr,
        maxAddr,
        zero,
        CachingType
    );
    if (buffVa == NULL)
    {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    PHYSICAL_ADDRESS buffPa = MmGetPhysicalAddress(
        buffVa
    );

    if(ZeroBuffer)
        RtlFillMemory(buffVa, BufferSizeInBytes, 0);

    if (BufferVa)
        *BufferVa = buffVa;

    if (BufferPa)
        *BufferPa = (PVOID)buffPa.QuadPart;

    return STATUS_SUCCESS;
}

NTSTATUS
DuMapBufferInUserspace(
    PVOID BufferKernelAddress,
    size_t BufferSizeInBytes,
    PMDL* Mdl,
    PVOID *BufferUserAddress
)
{
    PMDL mdl = IoAllocateMdl(
        BufferKernelAddress,
        (ULONG)BufferSizeInBytes,
        FALSE,
        FALSE,
        NULL
    );
    if (mdl == NULL)
    {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    MmBuildMdlForNonPagedPool(mdl);

    PVOID buffVaU = MmMapLockedPagesSpecifyCache(
        mdl,
        UserMode,
        MmCached,
        NULL,
        FALSE,
        0
    );
    if (buffVaU == NULL)
    {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    if (Mdl)
        *Mdl = mdl;
    if (BufferUserAddress)
        *BufferUserAddress = buffVaU;
    return STATUS_SUCCESS;
}

NTSTATUS
DuFreeUserspaceMapping(
    PVOID BaseAddress,
    PMDL Mdl
)
{
    MmUnmapLockedPages(
        BaseAddress,
        Mdl
    );

    IoFreeMdl(Mdl);


    return STATUS_SUCCESS;
}


typedef struct _INTERNAL_QUEUE_WORK_ELEMENT_STRUCTURE {

    LIST_ENTRY ListEntry;
    PVOID Data;

} INTERNAL_QUEUE_WORK_ELEMENT_STRUCTURE;

typedef struct _QUEUE_INTERLOCKED_HED_STRUCT {
    QUEUE_HEAD_STRUCTURE QueueHead;
    KSPIN_LOCK QueueLock;
} QUEUE_INTERLOCKED_HED_STRUCT;

VOID
DuQueueUninit(
    QUEUE_HEAD_STRUCTURE* QueueHead
)
{
    if (!QueueHead)
        return;

    if (QueueHead->ListEntry)
        ExFreePoolWithTag(
            QueueHead->ListEntry,
            PT_QUEUE_TAG
        );

    ExFreePoolWithTag(
        QueueHead,
        PT_QUEUE_TAG
    );

}

NTSTATUS 
DuQueueInit(
    QUEUE_HEAD_STRUCTURE **QueueHead,
    BOOLEAN Interlocked
)
{
    if (!QueueHead)
        return STATUS_INVALID_PARAMETER_1;

    NTSTATUS status = STATUS_SUCCESS;

    QUEUE_HEAD_STRUCTURE* queueHead = ExAllocatePoolWithTag(
        PagedPool,
        Interlocked? sizeof(QUEUE_INTERLOCKED_HED_STRUCT) : sizeof(QUEUE_HEAD_STRUCTURE),
        PT_QUEUE_TAG
    );
    if (!queueHead)
    {
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto cleanup;
    }

    queueHead->ListEntry = ExAllocatePoolWithTag(
        PagedPool,
        sizeof(LIST_ENTRY),
        PT_QUEUE_TAG
    );
    if (!queueHead->ListEntry)
    {
        status = STATUS_INSUFFICIENT_RESOURCES;
        goto cleanup;
    }

    queueHead->Interlocked = Interlocked;

    InitializeListHead(
        queueHead->ListEntry
    );
    
    if (Interlocked)
        KeInitializeSpinLock(
            &((QUEUE_INTERLOCKED_HED_STRUCT*)queueHead)->QueueLock
        );

cleanup:
    if (!NT_SUCCESS(status))
    {
        DuQueueUninit(
            queueHead
        );

        *QueueHead = NULL;
    }
    else
    {
        *QueueHead = queueHead;
    }

    return status;
}

BOOLEAN gFirstDataIn = TRUE;
BOOLEAN gFirstDataOut = TRUE;

NTSTATUS
DuEnqueueElement(
    QUEUE_HEAD_STRUCTURE *QueueHead,
    PVOID Data
)
{
    if(!QueueHead)
        return STATUS_INVALID_PARAMETER_1;

    INTERNAL_QUEUE_WORK_ELEMENT_STRUCTURE* element = ExAllocatePoolWithTag(
        KeGetCurrentIrql() == DISPATCH_LEVEL ? NonPagedPool : PagedPool,
        sizeof(INTERNAL_QUEUE_WORK_ELEMENT_STRUCTURE),
        PT_QUEUE_TAG
    );
    if (!element)
    {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    element->Data = Data;

    if (gFirstDataIn)
    {
        DEBUG_PRINT(
            "FIRST ENQUEUED DATA %p\n",
            element->Data
        );
        gFirstDataIn = FALSE;
    }

    if (!QueueHead->Interlocked)
    {
        InsertTailList(
            QueueHead->ListEntry,
            (PLIST_ENTRY)element
        );
    }
    else {
        ExInterlockedInsertTailList(
            QueueHead->ListEntry,
            (PLIST_ENTRY)element,
            &((QUEUE_INTERLOCKED_HED_STRUCT*)QueueHead)->QueueLock
        );
    }

    return STATUS_SUCCESS;
}

NTSTATUS
DuDequeueElement(
    QUEUE_HEAD_STRUCTURE* QueueHead,
    PVOID* Data
)
{
    if (!QueueHead)
        return STATUS_INVALID_PARAMETER_1;
    if (!Data)
        return STATUS_INVALID_PARAMETER_2;

    PLIST_ENTRY element;
    if (!QueueHead->Interlocked)
    {
        element = RemoveHeadList(
            QueueHead->ListEntry
        );
    }
    else
    {
        element = ExInterlockedRemoveHeadList(
            QueueHead->ListEntry,
            &((QUEUE_INTERLOCKED_HED_STRUCT*)QueueHead)->QueueLock
        );
    }

    if (element == NULL || element == QueueHead->ListEntry)
    {
        *Data = NULL;
        return STATUS_NO_MORE_ENTRIES;
    }

    INTERNAL_QUEUE_WORK_ELEMENT_STRUCTURE* queueElement =
        (INTERNAL_QUEUE_WORK_ELEMENT_STRUCTURE*) element;

    *Data = queueElement->Data;

    if (gFirstDataOut)
    {
        DEBUG_PRINT(
            "FIRST DEQUEUED DATA %p\n",
            queueElement->Data
        );
        gFirstDataOut = FALSE;
    }

    ExFreePoolWithTag(
        queueElement,
        PT_QUEUE_TAG
    );

    return STATUS_SUCCESS;
}

NTSTATUS
DuEnqueueElements(
    QUEUE_HEAD_STRUCTURE* QueueHead,
    unsigned NumberOfElements,
    PVOID Elements[]
)
{                                                                                   
    NTSTATUS status = STATUS_SUCCESS;
    for (unsigned i = 0; i < NumberOfElements; i++)
    {
        status = DuEnqueueElement(
            QueueHead,
            Elements[i]
        );
        if (!NT_SUCCESS(status))
        {
            DEBUG_PRINT("DuEnqueueElement returned status %X\n", status);
            return status;
        }
    }

    return status;
}

NTSTATUS
DuGetSequenceId(
    unsigned *ReturnValue
)
{
    NTSTATUS status;

    status = KeWaitForSingleObject(
        &gDriverData.SequenceMutex, 
        Executive,
        KernelMode,
        FALSE,
        NULL);

    if (status == STATUS_SUCCESS)
        *ReturnValue = gDriverData.SequenceIdCounter;
    else
    {
        DEBUG_STOP();

        KeReleaseMutex(
            &gDriverData.SequenceMutex,
            FALSE
        );

        return status;
    }
    KeReleaseMutex(
        &gDriverData.SequenceMutex,
        FALSE
    );

    return STATUS_SUCCESS;
}

BOOLEAN
DuIncreaseSequenceId()
{
    NTSTATUS status;

    status = KeWaitForSingleObject(
        &gDriverData.SequenceMutex,
        Executive,
        KernelMode,
        FALSE,
        NULL);
    if (status == STATUS_SUCCESS)
    {
        InterlockedIncrement((volatile long *)&gDriverData.SequenceIdCounter);
    }
    else
        return FALSE;
    KeReleaseMutex(
        &gDriverData.SequenceMutex,
        FALSE
    );
    return TRUE;
}

ULONG
DuGetPacketId()
{
    
    return InterlockedIncrement((LONG *)&gDriverData.PacketIdCounter) - 1;
}

void
DuDumpMemory(
    PVOID* Va,
    unsigned Size
)
{
    DEBUG_PRINT("%04X|", 0);
    for (unsigned i = 0; i < Size; i++)
    {
        DEBUG_PRINT("%02X ", ((unsigned char*)Va)[i]);
        if ((i + 1) % 0x10 == 0 && ((i + 1) < Size))
            DEBUG_PRINT("\n%04X|", i + 1);
    }
    DEBUG_PRINT("\n");
}
```

`IntelPTDriver/IntelPTDriver/DriverUtils.h`:

```h
#ifndef _DRIVER_UTILS_H_
#define _DRIVER_UTILS_H_

#include "DriverCommon.h"

#define DuFreeBuffer(BufferVa)                                      MmFreeContiguousMemory(BufferVa)
#define DuIsBufferEmpty(QueueHead)                                  IsListEmpty(QueueHead->ListEntry)

NTSTATUS
DuAllocateBuffer(
    size_t BufferSizeInBytes,
    MEMORY_CACHING_TYPE CachingType,
    BOOLEAN ZeroBuffer,
    PVOID* BufferVa,
    PVOID* BufferPa
);

NTSTATUS
DuMapBufferInUserspace(
    PVOID BufferKernelAddress,
    size_t BufferSizeInBytes,
    PMDL* Mdl,
    PVOID* BufferUserAddress
);

NTSTATUS
DuFreeUserspaceMapping(
    PVOID BaseAddress,
    PMDL Mdl
);

typedef struct _QUEUE_HEAD_STRUCTURE {

    LIST_ENTRY *ListEntry;
    BOOLEAN Interlocked;

} QUEUE_HEAD_STRUCTURE;

NTSTATUS
DuQueueInit(
    QUEUE_HEAD_STRUCTURE** QueueHead,
    BOOLEAN Interlocked
);

NTSTATUS
DuEnqueueElement(
    QUEUE_HEAD_STRUCTURE* QueueHead,
    PVOID Data
);

NTSTATUS
DuDequeueElement(
    QUEUE_HEAD_STRUCTURE* QueueHead,
    PVOID* Data
);

NTSTATUS
DuEnqueueElements(
    QUEUE_HEAD_STRUCTURE* QueueHead,
    unsigned NumberOfElements,
    PVOID Elements[]
);

void
DuDumpMemory(
    PVOID* Va,
    unsigned Size
);

ULONG DuGetPacketId();
NTSTATUS DuGetSequenceId(unsigned* ReturnValue);
BOOLEAN DuIncreaseSequenceId();


QUEUE_HEAD_STRUCTURE*   gQueueHead;
KEVENT                  gPagesAvailableEvent;
KMUTEX                  gCommMutex;

#endif
```

`IntelPTDriver/IntelPTDriver/IntelPTDriver.inf`:

```inf
;
; IntelPTDriver.inf
;

[Version]
Signature="$WINDOWS NT$"
Class=IntelPt ; TODO: edit Class
ClassGuid={78A1C341-4539-11d3-B88D-00C04FAD5122} ; TODO: edit ClassGuid
Provider=%ManufacturerName%
CatalogFile=IntelPTDriver.cat
DriverVer= ; TODO: set DriverVer in stampinf property pages
PnpLockDown=1

[DestinationDirs]
DefaultDestDir = 12
IntelPTDriver_Device_CoInstaller_CopyFiles = 11

; ================= Class section =====================

[SampleClassReg]
HKR,,,0,%ClassName%
HKR,,Icon,,-5

[SourceDisksNames]
1 = %DiskName%,,,""

[SourceDisksFiles]
IntelPTDriver.sys  = 1,,
WdfCoInstaller$KMDFCOINSTALLERVERSION$.dll=1 ; make sure the number matches with SourceDisksNames

;*****************************************
; Install Section
;*****************************************

[Manufacturer]
%ManufacturerName%=Standard,NT$ARCH$

[Standard.NT$ARCH$]
%IntelPTDriver.DeviceDesc%=IntelPTDriver_Device, Root\IntelPTDriver ; TODO: edit hw-id

[IntelPTDriver_Device.NT]
CopyFiles=Drivers_Dir

[Drivers_Dir]
IntelPTDriver.sys

;-------------- Service installation
[IntelPTDriver_Device.NT.Services]
AddService = IntelPTDriver,%SPSVCINST_ASSOCSERVICE%, IntelPTDriver_Service_Inst

; -------------- IntelPTDriver driver install sections
[IntelPTDriver_Service_Inst]
DisplayName    = %IntelPTDriver.SVCDESC%
ServiceType    = 1               ; SERVICE_KERNEL_DRIVER
StartType      = 3               ; SERVICE_DEMAND_START
ErrorControl   = 1               ; SERVICE_ERROR_NORMAL
ServiceBinary  = %12%\IntelPTDriver.sys

;
;--- IntelPTDriver_Device Coinstaller installation ------
;

[IntelPTDriver_Device.NT.CoInstallers]
AddReg=IntelPTDriver_Device_CoInstaller_AddReg
CopyFiles=IntelPTDriver_Device_CoInstaller_CopyFiles

[IntelPTDriver_Device_CoInstaller_AddReg]
HKR,,CoInstallers32,0x00010000, "WdfCoInstaller$KMDFCOINSTALLERVERSION$.dll,WdfCoInstaller"

[IntelPTDriver_Device_CoInstaller_CopyFiles]
WdfCoInstaller$KMDFCOINSTALLERVERSION$.dll

[IntelPTDriver_Device.NT.Wdf]
KmdfService =  IntelPTDriver, IntelPTDriver_wdfsect
[IntelPTDriver_wdfsect]
KmdfLibraryVersion = $KMDFVERSION$

[Strings]
SPSVCINST_ASSOCSERVICE= 0x00000002
ManufacturerName="Cristian Nacu" ;TODO: Replace with your manufacturer name
ClassName="IntelPt" ; TODO: edit ClassName
DiskName = "IntelPTDriver Installation Disk"
IntelPTDriver.DeviceDesc = "IntelPTDriver Device"
IntelPTDriver.SVCDESC = "IntelPTDriver Service"

```

`IntelPTDriver/IntelPTDriver/IntelPTDriver.vcxproj`:

```vcxproj
<?xml version="1.0" encoding="utf-8"?>
<Project DefaultTargets="Build" ToolsVersion="12.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <ItemGroup Label="ProjectConfigurations">
    <ProjectConfiguration Include="Debug|Win32">
      <Configuration>Debug</Configuration>
      <Platform>Win32</Platform>
    </ProjectConfiguration>
    <ProjectConfiguration Include="Release|Win32">
      <Configuration>Release</Configuration>
      <Platform>Win32</Platform>
    </ProjectConfiguration>
    <ProjectConfiguration Include="Debug|x64">
      <Configuration>Debug</Configuration>
      <Platform>x64</Platform>
    </ProjectConfiguration>
    <ProjectConfiguration Include="Release|x64">
      <Configuration>Release</Configuration>
      <Platform>x64</Platform>
    </ProjectConfiguration>
    <ProjectConfiguration Include="Debug|ARM">
      <Configuration>Debug</Configuration>
      <Platform>ARM</Platform>
    </ProjectConfiguration>
    <ProjectConfiguration Include="Release|ARM">
      <Configuration>Release</Configuration>
      <Platform>ARM</Platform>
    </ProjectConfiguration>
    <ProjectConfiguration Include="Debug|ARM64">
      <Configuration>Debug</Configuration>
      <Platform>ARM64</Platform>
    </ProjectConfiguration>
    <ProjectConfiguration Include="Release|ARM64">
      <Configuration>Release</Configuration>
      <Platform>ARM64</Platform>
    </ProjectConfiguration>
  </ItemGroup>
  <PropertyGroup Label="Globals">
    <ProjectGuid>{AD88C3DA-3E48-4837-A809-1259B5F0F3C3}</ProjectGuid>
    <TemplateGuid>{1bc93793-694f-48fe-9372-81e2b05556fd}</TemplateGuid>
    <TargetFrameworkVersion>v4.5</TargetFrameworkVersion>
    <MinimumVisualStudioVersion>12.0</MinimumVisualStudioVersion>
    <Configuration>Debug</Configuration>
    <Platform Condition="'$(Platform)' == ''">Win32</Platform>
    <RootNamespace>IntelPTDriver</RootNamespace>
    <WindowsTargetPlatformVersion>10.0.22000.0</WindowsTargetPlatformVersion>
  </PropertyGroup>
  <Import Project="$(VCTargetsPath)\Microsoft.Cpp.Default.props" />
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|Win32'" Label="Configuration">
    <TargetVersion>Windows10</TargetVersion>
    <UseDebugLibraries>true</UseDebugLibraries>
    <PlatformToolset>WindowsKernelModeDriver10.0</PlatformToolset>
    <ConfigurationType>Driver</ConfigurationType>
    <DriverType>KMDF</DriverType>
    <DriverTargetPlatform>Universal</DriverTargetPlatform>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|Win32'" Label="Configuration">
    <TargetVersion>Windows10</TargetVersion>
    <UseDebugLibraries>false</UseDebugLibraries>
    <PlatformToolset>WindowsKernelModeDriver10.0</PlatformToolset>
    <ConfigurationType>Driver</ConfigurationType>
    <DriverType>KMDF</DriverType>
    <DriverTargetPlatform>Universal</DriverTargetPlatform>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|x64'" Label="Configuration">
    <TargetVersion>Windows10</TargetVersion>
    <UseDebugLibraries>true</UseDebugLibraries>
    <PlatformToolset>WindowsKernelModeDriver10.0</PlatformToolset>
    <ConfigurationType>Driver</ConfigurationType>
    <DriverType>KMDF</DriverType>
    <DriverTargetPlatform>Universal</DriverTargetPlatform>
    <Driver_SpectreMitigation>false</Driver_SpectreMitigation>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|x64'" Label="Configuration">
    <TargetVersion>Windows10</TargetVersion>
    <UseDebugLibraries>false</UseDebugLibraries>
    <PlatformToolset>WindowsKernelModeDriver10.0</PlatformToolset>
    <ConfigurationType>Driver</ConfigurationType>
    <DriverType>KMDF</DriverType>
    <DriverTargetPlatform>Universal</DriverTargetPlatform>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|ARM'" Label="Configuration">
    <TargetVersion>Windows10</TargetVersion>
    <UseDebugLibraries>true</UseDebugLibraries>
    <PlatformToolset>WindowsKernelModeDriver10.0</PlatformToolset>
    <ConfigurationType>Driver</ConfigurationType>
    <DriverType>KMDF</DriverType>
    <DriverTargetPlatform>Universal</DriverTargetPlatform>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|ARM'" Label="Configuration">
    <TargetVersion>Windows10</TargetVersion>
    <UseDebugLibraries>false</UseDebugLibraries>
    <PlatformToolset>WindowsKernelModeDriver10.0</PlatformToolset>
    <ConfigurationType>Driver</ConfigurationType>
    <DriverType>KMDF</DriverType>
    <DriverTargetPlatform>Universal</DriverTargetPlatform>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|ARM64'" Label="Configuration">
    <TargetVersion>Windows10</TargetVersion>
    <UseDebugLibraries>true</UseDebugLibraries>
    <PlatformToolset>WindowsKernelModeDriver10.0</PlatformToolset>
    <ConfigurationType>Driver</ConfigurationType>
    <DriverType>KMDF</DriverType>
    <DriverTargetPlatform>Universal</DriverTargetPlatform>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|ARM64'" Label="Configuration">
    <TargetVersion>Windows10</TargetVersion>
    <UseDebugLibraries>false</UseDebugLibraries>
    <PlatformToolset>WindowsKernelModeDriver10.0</PlatformToolset>
    <ConfigurationType>Driver</ConfigurationType>
    <DriverType>KMDF</DriverType>
    <DriverTargetPlatform>Universal</DriverTargetPlatform>
  </PropertyGroup>
  <Import Project="$(VCTargetsPath)\Microsoft.Cpp.props" />
  <ImportGroup Label="ExtensionSettings">
  </ImportGroup>
  <ImportGroup Label="PropertySheets">
    <Import Project="$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props" Condition="exists('$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props')" Label="LocalAppDataPlatform" />
  </ImportGroup>
  <PropertyGroup Label="UserMacros" />
  <PropertyGroup />
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|Win32'">
    <DebuggerFlavor>DbgengKernelDebugger</DebuggerFlavor>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|Win32'">
    <DebuggerFlavor>DbgengKernelDebugger</DebuggerFlavor>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|x64'">
    <DebuggerFlavor>DbgengKernelDebugger</DebuggerFlavor>
    <EnableInf2cat>false</EnableInf2cat>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|x64'">
    <DebuggerFlavor>DbgengKernelDebugger</DebuggerFlavor>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|ARM'">
    <DebuggerFlavor>DbgengKernelDebugger</DebuggerFlavor>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|ARM'">
    <DebuggerFlavor>DbgengKernelDebugger</DebuggerFlavor>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|ARM64'">
    <DebuggerFlavor>DbgengKernelDebugger</DebuggerFlavor>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|ARM64'">
    <DebuggerFlavor>DbgengKernelDebugger</DebuggerFlavor>
  </PropertyGroup>
  <ItemDefinitionGroup Condition="'$(Configuration)|$(Platform)'=='Debug|x64'">
    <Link>
      <AdditionalOptions>/INTEGRITYCHECK %(AdditionalOptions)</AdditionalOptions>
    </Link>
  </ItemDefinitionGroup>
  <ItemGroup>
    <Inf Include="IntelPTDriver.inf" />
  </ItemGroup>
  <ItemGroup>
    <FilesToPackage Include="$(TargetPath)" />
  </ItemGroup>
  <ItemGroup>
    <ClCompile Include="Device.c" />
    <ClCompile Include="Driver.c" />
    <ClCompile Include="ProcessorTrace.c" />
    <ClCompile Include="DriverUtils.c" />
    <ClCompile Include="ProcessorTraceWindowsCommands.c" />
    <ClCompile Include="ProcessorTraceWindowsCommunication.c" />
    <ClCompile Include="ProcessorTraceWindowsControl.c" />
  </ItemGroup>
  <ItemGroup>
    <ClInclude Include="Debug.h" />
    <ClInclude Include="Device.h" />
    <ClInclude Include="DriverCommon.h" />
    <ClInclude Include="IntelProcessorTraceDefs.h" />
    <ClInclude Include="ProcessorTrace.h" />
    <ClInclude Include="DriverUtils.h" />
    <ClInclude Include="ProcessorTraceShared.h" />
    <ClInclude Include="ProcessorTraceWindowsCommands.h" />
    <ClInclude Include="ProcessorTraceWindowsCommunication.h" />
    <ClInclude Include="ProcessorTraceWindowsControl.h" />
    <ClInclude Include="Public.h" />
  </ItemGroup>
  <Import Project="$(VCTargetsPath)\Microsoft.Cpp.targets" />
  <ImportGroup Label="ExtensionTargets">
  </ImportGroup>
</Project>
```

`IntelPTDriver/IntelPTDriver/IntelPTDriver.vcxproj.filters`:

```filters
<?xml version="1.0" encoding="utf-8"?>
<Project ToolsVersion="4.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <ItemGroup>
    <Filter Include="Source Files">
      <UniqueIdentifier>{4FC737F1-C7A5-4376-A066-2A32D752A2FF}</UniqueIdentifier>
      <Extensions>cpp;c;cc;cxx;def;odl;idl;hpj;bat;asm;asmx</Extensions>
    </Filter>
    <Filter Include="Header Files">
      <UniqueIdentifier>{93995380-89BD-4b04-88EB-625FBE52EBFB}</UniqueIdentifier>
      <Extensions>h;hpp;hxx;hm;inl;inc;xsd</Extensions>
    </Filter>
    <Filter Include="Resource Files">
      <UniqueIdentifier>{67DA6AB6-F800-4c08-8B7A-83BB121AAD01}</UniqueIdentifier>
      <Extensions>rc;ico;cur;bmp;dlg;rc2;rct;bin;rgs;gif;jpg;jpeg;jpe;resx;tiff;tif;png;wav;mfcribbon-ms</Extensions>
    </Filter>
    <Filter Include="Driver Files">
      <UniqueIdentifier>{8E41214B-6785-4CFE-B992-037D68949A14}</UniqueIdentifier>
      <Extensions>inf;inv;inx;mof;mc;</Extensions>
    </Filter>
    <Filter Include="Header Files\IntelProcessorTrace">
      <UniqueIdentifier>{648ee96e-03f7-485b-8fb0-222a9b3ee5dc}</UniqueIdentifier>
    </Filter>
    <Filter Include="Header Files\PlatformConnector">
      <UniqueIdentifier>{a8be266a-2290-4f55-b7c2-491de1b6bd55}</UniqueIdentifier>
    </Filter>
    <Filter Include="Header Files\PlatformConnector\ProcessorTraceWindows">
      <UniqueIdentifier>{3ed67d06-5ba2-47ee-aac6-174db5ce5bbd}</UniqueIdentifier>
    </Filter>
    <Filter Include="Source Files\IntelProcessorTrace">
      <UniqueIdentifier>{87e5193b-81c9-4cdf-b5a0-1013ba241c5a}</UniqueIdentifier>
    </Filter>
    <Filter Include="Source Files\PlatformConnector">
      <UniqueIdentifier>{59a39399-f2bb-435b-b782-c48b42717cc3}</UniqueIdentifier>
    </Filter>
    <Filter Include="Source Files\PlatformConnector\ProcessorTraceWindows">
      <UniqueIdentifier>{b8834abb-0fc0-4e50-9538-67c0fec0682a}</UniqueIdentifier>
    </Filter>
  </ItemGroup>
  <ItemGroup>
    <Inf Include="IntelPTDriver.inf">
      <Filter>Driver Files</Filter>
    </Inf>
  </ItemGroup>
  <ItemGroup>
    <ClCompile Include="DriverUtils.c">
      <Filter>Source Files</Filter>
    </ClCompile>
    <ClCompile Include="ProcessorTrace.c">
      <Filter>Source Files\IntelProcessorTrace</Filter>
    </ClCompile>
    <ClCompile Include="Device.c">
      <Filter>Source Files\PlatformConnector\ProcessorTraceWindows</Filter>
    </ClCompile>
    <ClCompile Include="Driver.c">
      <Filter>Source Files\PlatformConnector\ProcessorTraceWindows</Filter>
    </ClCompile>
    <ClCompile Include="ProcessorTraceWindowsControl.c">
      <Filter>Source Files\PlatformConnector\ProcessorTraceWindows</Filter>
    </ClCompile>
    <ClCompile Include="ProcessorTraceWindowsCommunication.c">
      <Filter>Source Files\PlatformConnector\ProcessorTraceWindows</Filter>
    </ClCompile>
    <ClCompile Include="ProcessorTraceWindowsCommands.c">
      <Filter>Source Files\PlatformConnector\ProcessorTraceWindows</Filter>
    </ClCompile>
  </ItemGroup>
  <ItemGroup>
    <ClInclude Include="Public.h">
      <Filter>Header Files</Filter>
    </ClInclude>
    <ClInclude Include="Debug.h">
      <Filter>Header Files</Filter>
    </ClInclude>
    <ClInclude Include="ProcessorTrace.h">
      <Filter>Header Files\IntelProcessorTrace</Filter>
    </ClInclude>
    <ClInclude Include="IntelProcessorTraceDefs.h">
      <Filter>Header Files\IntelProcessorTrace</Filter>
    </ClInclude>
    <ClInclude Include="ProcessorTraceShared.h">
      <Filter>Header Files\IntelProcessorTrace</Filter>
    </ClInclude>
    <ClInclude Include="Device.h">
      <Filter>Header Files\PlatformConnector\ProcessorTraceWindows</Filter>
    </ClInclude>
    <ClInclude Include="DriverCommon.h">
      <Filter>Header Files\PlatformConnector\ProcessorTraceWindows</Filter>
    </ClInclude>
    <ClInclude Include="DriverUtils.h">
      <Filter>Header Files\PlatformConnector\ProcessorTraceWindows</Filter>
    </ClInclude>
    <ClInclude Include="ProcessorTraceWindowsControl.h">
      <Filter>Header Files\PlatformConnector\ProcessorTraceWindows</Filter>
    </ClInclude>
    <ClInclude Include="ProcessorTraceWindowsCommands.h">
      <Filter>Header Files\PlatformConnector\ProcessorTraceWindows</Filter>
    </ClInclude>
    <ClInclude Include="ProcessorTraceWindowsCommunication.h">
      <Filter>Header Files\PlatformConnector\ProcessorTraceWindows</Filter>
    </ClInclude>
  </ItemGroup>
</Project>
```

`IntelPTDriver/IntelPTDriver/IntelPTDriver.vcxproj.user`:

```user
<?xml version="1.0" encoding="utf-8"?>
<Project ToolsVersion="15.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|x64'">
    <SignMode>Off</SignMode>
    <DebuggerFlavor>WindowsLocalDebugger</DebuggerFlavor>
  </PropertyGroup>
</Project>
```

`IntelPTDriver/IntelPTDriver/IntelProcessorTraceDefs.h`:

```h
#ifndef _INTEL_PROCESSOR_TRACE_DEFS_H_
#define _INTEL_PROCESSOR_TRACE_DEFS_H_

// As described in Intel Manual Volume 4 Chapter 2 Table 2-2 pg 4630
typedef union _IA32_RTIT_CTL_STRUCTURE {

    struct {
        unsigned long long TraceEn : 1;        // 0
        unsigned long long CycEn : 1;        // 1  
        unsigned long long OS : 1;        // 2
        unsigned long long User : 1;        // 3
        unsigned long long PwrEvtEn : 1;        // 4
        unsigned long long FUPonPTW : 1;        // 5
        unsigned long long FabricEn : 1;        // 6
        unsigned long long Cr3Filter : 1;        // 7
        unsigned long long ToPA : 1;        // 8
        unsigned long long MtcEn : 1;        // 9
        unsigned long long TscEn : 1;        // 10
        unsigned long long DisRETC : 1;        // 11
        unsigned long long PTWEn : 1;        // 12
        unsigned long long BranchEn : 1;        // 13
        unsigned long long MtcFreq : 4;        // 17:14
        unsigned long long ReservedZero0 : 1;        // 18
        unsigned long long CycThresh : 4;        // 22:19
        unsigned long long ReservedZero1 : 1;        // 23
        unsigned long long PSBFreq : 4;        // 27:24
        unsigned long long ReservedZero2 : 4;        // 31:28
        unsigned long long Addr0Cfg : 4;        // 35:32
        unsigned long long Addr1Cfg : 4;        // 39:36
        unsigned long long Addr2Cfg : 4;        // 43:40
        unsigned long long Addr3Cfg : 4;        // 47:44
        unsigned long long ReservedZero3 : 8;        // 55:48
        unsigned long long InjectPsbPmiOnEnable : 1;        // 56
        unsigned long long ReservedZero4 : 7;        // 63:57
    } Fields;
    unsigned long long Raw;
} IA32_RTIT_CTL_STRUCTURE;

//////////////////////////////////////////////////// Output 

typedef enum _TOPA_ENTRY_SIZE_ENCODINGS {
    Size4K, Size8K, Size16K, Size32K,
    Size64K, Size128K, Size256K, Size512K,
    Size1M, Size2M, Size4M, Size8M,
    Size16M, Size32M, Size64M, Size128M
} TOPA_ENTRY_SIZE_ENCODINGS;

typedef struct _TOPA_ENTRY {
    unsigned long long END : 1;                                 // 0
    unsigned long long ReservedZero0 : 1;                       // 1
    unsigned long long INT : 1;                                 // 2
    unsigned long long ReservedZero1 : 1;                       // 3
    unsigned long long STOP : 1;                                // 4
    unsigned long long ReservedZero5 : 1;                       // 5
    unsigned long long Size : 4;                                // 9:6
    unsigned long long ReservedZero6 : 2;                       // 11:10
    unsigned long long OutputRegionBasePhysicalAddress : 52;    // MAXPHYADDR-1:12
} TOPA_ENTRY;

// As described in Intel Manual Volume 4 Chapter 2 Table 2-2 pg 4630
typedef union _IA32_RTIT_STATUS_STRUCTURE {
    struct {
        unsigned long long FilterEn : 1;        // 0
        unsigned long long ContextEn : 1;        // 1  
        unsigned long long TriggerEn : 1;        // 2
        unsigned long long Reserved0 : 1;        // 3
        unsigned long long Error : 1;        // 4
        unsigned long long Stopped : 1;        // 5
        unsigned long long SendPSB : 1;        // 6
        unsigned long long PendToPAPMI : 1;        // 7
        unsigned long long ReservedZero0 : 24;       // 31:8
        unsigned long long PacketByteCnt : 17;       // 48:32
        unsigned long long Reserved1 : 15;       // 10
    } Fields;
    unsigned long long Raw;
} IA32_RTIT_STATUS_STRUCTURE;


typedef enum CPUID_INDEX {
    CpuidEax,
    CpuidEbx,
    CpuidEcx,
    CpuidEdx
} CPUID_INDEX;


typedef union _PERFORMANCE_MONITOR_COUNTER_LVT_STRUCTURE {

    struct {

        unsigned long Vector : 8;
        unsigned long DeliveryMode : 3;
        unsigned long Reserved0 : 1;
        unsigned long DeliveryStatus : 1;
        unsigned long Reserved1 : 3;
        unsigned long Mask : 1;
        unsigned long Raw : 15;

    } Values;

    unsigned long Raw;

} PERFORMANCE_MONITOR_COUNTER_LVT_STRUCTURE;

typedef union _IA32_PERF_GLOBAL_STATUS_STRUCTURE {

    struct {
        unsigned long long OvfPMC0 : 1; // 0
        unsigned long long OvfPMC1 : 1; // 1
        unsigned long long OvfPMC2 : 1; // 2
        unsigned long long OvfPMC3 : 1; // 3
        unsigned long long Reserved0 : 28; // 31:4
        unsigned long long OvfFixedCtr0 : 1; // 32
        unsigned long long OvfFixedCtr1 : 1; // 33
        unsigned long long OvfFixedCtr2 : 1; // 34
        unsigned long long Reserved1 : 13; // 47:35
        unsigned long long OvfPerfMetrics : 1; // 48
        unsigned long long Reserved2 : 6; // 54:49
        unsigned long long TopaPMI : 1; // 55
        unsigned long long Reserved3 : 2; // 57:56
        unsigned long long LbrFrz : 1; // 58
        unsigned long long CrtFrz : 1; // 59
        unsigned long long Asci : 1; // 60
        unsigned long long OvfUncore : 1; // 61
        unsigned long long OvfBuf : 1; // 62
        unsigned long long CondChgd : 1; // 63
    } Values;
    unsigned long long Raw;

} IA32_PERF_GLOBAL_STATUS_STRUCTURE;

typedef union _IA32_APIC_BASE_STRUCTURE {

    struct {
        unsigned long long Reserved0 : 8;
        unsigned long long BSPFlag : 1;
        unsigned long long Reserved1 : 1;
        unsigned long long EnableX2ApicMode : 1;
        unsigned long long ApicGlobalEnbale : 1;
        unsigned long long ApicBase : 52;
    } Values;
    unsigned long long Raw;
} IA32_APIC_BASE_STRUCTURE;

#define PT_POOL_TAG                         'PTPT'
#define INTEL_PT_OUTPUT_TAG                 'IPTO'

#define PT_OPTION_IS_SUPPORTED(capability, option)  (((!capability) && (option)) ? FALSE : TRUE)

#define PT_OUTPUT_CONTIGNUOUS_BASE_MASK             0x7F
#define PT_OUTPUT_TOPA_BASE_MASK                    0xFFF

#define CPUID_INTELPT_AVAILABLE_LEAF        0x7
#define CPUID_INTELPT_AVAILABLE_SUBLEAF     0x0

#define CPUID_INTELPT_CAPABILITY_LEAF       0x14

#define BIT(x)                              (1<<x)

#define INTEL_PT_BIT                        25
#define INTEL_PT_MASK                       BIT(INTEL_PT_BIT)
#define PT_FEATURE_AVAILABLE(v)             ((v) != 0)


#define IA32_RTIT_OUTPUT_BASE               0x560
#define IA32_RTIT_OUTPUT_MASK_PTRS          0x561
#define IA32_RTIT_CTL                       0x570
#define IA32_RTIT_CR3_MATCH                 0x572

#define IA32_RTIT_ADDR0_A                   0x580
#define IA32_RTIT_ADDR0_B                   0x581
#define IA32_RTIT_ADDR1_A                   0x582
#define IA32_RTIT_ADDR1_B                   0x583
#define IA32_RTIT_ADDR2_A                   0x584
#define IA32_RTIT_ADDR2_B                   0x585
#define IA32_RTIT_ADDR3_A                   0x586
#define IA32_RTIT_ADDR3_B                   0x587

#define IA32_APIC_BASE                      0x1B
#define IA32_LVT_REGISTER                   0x834
#define MSR_IA32_PERF_GLOBAL_STATUS		    0x0000038e
#define MSR_IA32_PERF_GLOBAL_OVF_CTRL       0x00000390

#define IA32_RTIT_STATUS                    0x571
#define PtGetStatus(ia32_rtit_ctl_structure) (ia32_rtit_ctl_structure.Raw = __readmsr(IA32_RTIT_STATUS))
#define PtSetStatus(value) __writemsr(IA32_RTIT_STATUS, value)

INTEL_PT_CAPABILITIES*  gPtCapabilities = NULL;
unsigned long long      gFrequency = 3;

#endif
```

`IntelPTDriver/IntelPTDriver/ProcessorTrace.c`:

```c
#include "ProcessorTrace.h"
#include "ProcessorTraceShared.h"
#include "DriverUtils.h"
#include "IntelProcessorTraceDefs.h"

IptMemoryAllocationFunction gMemoryAllocation;

#define IPT_POOL_TAG         'IPTP'
#define DEFAULT_NUMBER_OF_TOPA_ENTRIES 10
#define IptFlush() _mm_sfence();

NTSTATUS
IptAllocateNewTopaBuffer(
    unsigned                    EntriesCount,
    TOPA_ENTRY* TopaTableVa,
    PVOID                       NextTopaPa,
    PMDL* Mdl
);

BOOLEAN
IptTopaPmiReason(
)
{
    IA32_PERF_GLOBAL_STATUS_STRUCTURE perf;
    perf.Raw = __readmsr(MSR_IA32_PERF_GLOBAL_STATUS);
    if (!perf.Values.TopaPMI)
        return FALSE;
    return TRUE;
}

void
IptResetPmi(
)
{
    IA32_PERF_GLOBAL_STATUS_STRUCTURE ctlperf = { 0 };
    ctlperf.Values.TopaPMI = 1;
    __writemsr(MSR_IA32_PERF_GLOBAL_OVF_CTRL, ctlperf.Raw);
}

NTSTATUS
IptQueryCapabilities(
    INTEL_PT_CAPABILITIES* Capabilities
)
{
    int cpuidIntelpt[4] = { 0 };

    if (!Capabilities)
    {
        return STATUS_INVALID_PARAMETER_1;
    }

    __cpuidex(
        cpuidIntelpt,
        CPUID_INTELPT_AVAILABLE_LEAF,
        CPUID_INTELPT_AVAILABLE_SUBLEAF
    );

    KdPrintEx((DPFLTR_IHVDRIVER_ID, DPFLTR_ERROR_LEVEL, "CPUID(EAX:0x7 ECX:0x0).EBX = %X\n", cpuidIntelpt[CpuidEbx]));

    Capabilities->IntelPtAvailable = PT_FEATURE_AVAILABLE(cpuidIntelpt[CpuidEbx] & INTEL_PT_MASK);

    if (!Capabilities->IntelPtAvailable)
    {
        KdPrintEx((DPFLTR_IHVDRIVER_ID, DPFLTR_ERROR_LEVEL, "IntelPT Capability NOT Available\n"));
        return STATUS_NOT_SUPPORTED;
    }

    KdPrintEx((DPFLTR_IHVDRIVER_ID, DPFLTR_ERROR_LEVEL, "IntelPT Capability Available\n"));

    /// Gather future capabilities of Intel PT
    /// CPUID function 14H is dedicated to enumerate the resource and capability of processors that report CPUID.(EAX = 07H, ECX = 0H) : EBX[bit 25] = 1.

    __cpuidex(
        (int *)&Capabilities->IptCapabilities0,
        CPUID_INTELPT_CAPABILITY_LEAF,
        0x0
    );

    if (Capabilities->IptCapabilities0.Eax.MaximumValidSubleaf >= 1)
        __cpuidex(
            (int*)&Capabilities->IptCapabilities1,
            CPUID_INTELPT_CAPABILITY_LEAF,
            0x1
        );

    Capabilities->TopaOutputEntries = Capabilities->IptCapabilities0.Ecx.TopaMultipleOutputEntriesSupport ? 
        __readmsr(IA32_RTIT_OUTPUT_MASK_PTRS) : 0;


    return STATUS_SUCCESS;
}

NTSTATUS
IptGetCapabilities(
    INTEL_PT_CAPABILITIES* Capabilities
)
{
    NTSTATUS status = STATUS_SUCCESS;
    if (!gPtCapabilities)
    {
        gPtCapabilities = (INTEL_PT_CAPABILITIES*)ExAllocatePoolWithTag(
            PagedPool,
            sizeof(INTEL_PT_CAPABILITIES),
            PT_POOL_TAG
        );
        if (!gPtCapabilities)
        {
            return STATUS_INSUFFICIENT_RESOURCES;
        }
    }

    memcpy_s(
        Capabilities,
        sizeof(INTEL_PT_CAPABILITIES),
        gPtCapabilities,
        sizeof(INTEL_PT_CAPABILITIES)
    );

    return status;
}

NTSTATUS
IptEnableTrace(
    INTEL_PT_OUTPUT_OPTIONS* CpuOptions
)
{
    IA32_RTIT_CTL_STRUCTURE ia32RtitCtlMsrShadow;

    DEBUG_PRINT("Enabling trace on cpu %d\n", KeGetCurrentProcessorNumber());

    if (IptError())
    {
        IptClearError();
    }

    ia32RtitCtlMsrShadow.Raw = __readmsr(IA32_RTIT_CTL);
    ia32RtitCtlMsrShadow.Fields.TraceEn = TRUE;

    __writemsr(IA32_RTIT_CTL, ia32RtitCtlMsrShadow.Raw);
    
    if (IptError() || !IptEnabled())
    {
        ULONG crtCpu = KeGetCurrentProcessorNumber();
        DEBUG_PRINT("Pt TraceEnable failed on cpu %d\n", crtCpu);
        DEBUG_STOP();
        return STATUS_UNSUCCESSFUL;
    }
    else
    {
        DEBUG_PRINT("Enabled trace on cpu %d\n", KeGetCurrentProcessorNumber());
    }

    CpuOptions->RunningStatus = IPT_STATUS_ENABLED;
    return STATUS_SUCCESS;
}

void
IptClearTraceEn(
)
{
    IA32_RTIT_CTL_STRUCTURE ia32RtitCtlMsrShadow;
    ia32RtitCtlMsrShadow.Raw = __readmsr(IA32_RTIT_CTL);
    ia32RtitCtlMsrShadow.Fields.TraceEn = FALSE;
    __writemsr(IA32_RTIT_CTL, ia32RtitCtlMsrShadow.Raw);
}

NTSTATUS
IptDisableTrace(
    PMDL *Mdl,
    ULONG *BufferSize,
    INTEL_PT_OUTPUT_OPTIONS* CpuOptions
)
{
    DEBUG_PRINT("Disabling trace on cpu %d\n", KeGetCurrentProcessorNumber());
    IptClearTraceEn();
    DEBUG_PRINT("Disabled trace on cpu %d\n", KeGetCurrentProcessorNumber());
    IptFlush();

    IA32_RTIT_STATUS_STRUCTURE ptStatus;
    PtGetStatus(ptStatus);
    IA32_RTIT_OUTPUT_MASK_STRUCTURE outMask = { .Raw = __readmsr(IA32_RTIT_OUTPUT_MASK_PTRS) };
    DEBUG_PRINT("IA32_RTIT_OUTPUT_MASK_PTRS Table Offset %d Output Offset %d status %X\n", outMask.Fields.MaskOrTableOffset, outMask.Fields.OutputOffset, ptStatus.Raw);
    DEBUG_PRINT("outMask.Fields.MaskOrTableOffset >> 8 = %lld\noutMask.Fields.OutputOffset = %lld\n", outMask.Fields.MaskOrTableOffset >> 8, outMask.Fields.OutputOffset);

    if(Mdl)
        *Mdl = CpuOptions->TopaMdl;
    

    if (BufferSize)
        *BufferSize = (ULONG)outMask.Fields.OutputOffset + ((ULONG)(outMask.Fields.MaskOrTableOffset >> 8) * PAGE_SIZE);

    CpuOptions->RunningStatus = IPT_STATUS_DISABLED;

    return STATUS_SUCCESS;
}

NTSTATUS
IptPauseTrace(
    INTEL_PT_OUTPUT_OPTIONS* CpuOptions
)
{
    NTSTATUS status = STATUS_SUCCESS;

    if (IptError())
    {
        DEBUG_STOP();

        if (!IptEnabled())
        {
            DEBUG_PRINT("IPT DISABLED! Walter Fuck?!\n");
        }
    }

    if (IptEnabled())
        IptClearTraceEn();

    CpuOptions->RunningStatus = IPT_STATUS_PAUSED;

    return status;

}

NTSTATUS
IptResumeTrace(
    INTEL_PT_OUTPUT_OPTIONS* CpuOptions
)
{
    NTSTATUS status = STATUS_SUCCESS;
    if (!IptEnabled())
    {
        status = IptEnableTrace(CpuOptions);
    }
    return status;

}

NTSTATUS 
IptUnlinkFullTopaBuffers(
    PMDL* Mdl,
    ULONG* DataSize,
    INTEL_PT_OUTPUT_OPTIONS* CpuOptions,
    BOOLEAN AllocateNewTopa
)
{
    if (CpuOptions->RunningStatus == IPT_STATUS_ENABLED)
        return STATUS_CANNOT_MAKE;

    DEBUG_PRINT(">>> UNLINKING ON AP %d\n", KeGetCurrentProcessorNumber());
    NTSTATUS status = STATUS_SUCCESS;
    PMDL mdl;

    if (!Mdl)
        return STATUS_INVALID_PARAMETER_1;

    if (!CpuOptions)
        return STATUS_INVALID_PARAMETER_2;

    mdl = CpuOptions->TopaMdl;
    *Mdl = CpuOptions->TopaMdl;
    if (AllocateNewTopa)
    {
        status = IptAllocateNewTopaBuffer(
            CpuOptions->TopaEntries,
            CpuOptions->TopaTableVa,
            CpuOptions->TopaTablePa,
            &(PMDL)CpuOptions->TopaMdl
        );
        if (!NT_SUCCESS(status))
        {
            DEBUG_PRINT("IptAlocateNewTopaBuffer failed with status %X\n", status);
        }

        IA32_RTIT_STATUS_STRUCTURE ptStatus;
        PtGetStatus(ptStatus);
        IA32_RTIT_OUTPUT_MASK_STRUCTURE outMask = { .Raw = __readmsr(IA32_RTIT_OUTPUT_MASK_PTRS) };

        DEBUG_PRINT("IA32_RTIT_OUTPUT_MASK_PTRS Table Offset %X Output Offset %X status %X\n", outMask.Fields.MaskOrTableOffset, outMask.Fields.OutputOffset, ptStatus.Raw);
        DEBUG_PRINT("outMask.Fields.MaskOrTableOffset >> 8 = %lld\noutMask.Fields.OutputOffset = %lld\n", outMask.Fields.MaskOrTableOffset >> 8, outMask.Fields.OutputOffset);

        if (DataSize)
            *DataSize = (ULONG)outMask.Fields.OutputOffset + ((ULONG)(outMask.Fields.MaskOrTableOffset >> 8) * PAGE_SIZE);

        __writemsr(IA32_RTIT_OUTPUT_MASK_PTRS, CpuOptions->OutputMask.Raw);
        __writemsr(IA32_RTIT_OUTPUT_BASE, (unsigned long long)CpuOptions->TopaTablePa);
    }
    return status;
}

BOOLEAN IptError()
{
    IA32_RTIT_STATUS_STRUCTURE ptStatus;
    PtGetStatus(ptStatus);
    return !!ptStatus.Fields.Error;
}

void IptClearError()
{
    IA32_RTIT_STATUS_STRUCTURE ptStatus;
    PtGetStatus(ptStatus);
    ptStatus.Fields.Error = FALSE;
    PtSetStatus(ptStatus.Raw);
}

BOOLEAN IptEnabled()
{
    IA32_RTIT_STATUS_STRUCTURE ptStatus;
    PtGetStatus(ptStatus);
    return !!ptStatus.Fields.TriggerEn;
}

NTSTATUS
IptValidateConfigurationRequest(
    INTEL_PT_CONFIGURATION* FilterConfiguration
)
{
    INTEL_PT_CAPABILITIES capabilities;
    NTSTATUS status;
    status = IptGetCapabilities(
        &capabilities
    );
    if(!NT_SUCCESS(status))
        return status;

    if (!PT_OPTION_IS_SUPPORTED( capabilities.IptCapabilities0.Ebx.ConfigurablePsbAndCycleAccurateModeSupport,
        FilterConfiguration->PacketGenerationOptions.PacketCyc.Enable))
        return STATUS_NOT_SUPPORTED;

    if (!PT_OPTION_IS_SUPPORTED( capabilities.IptCapabilities0.Ebx.PtwriteSupport,
        FilterConfiguration->PacketGenerationOptions.Misc.EnableFupPacketsAfterPtw))
        return STATUS_NOT_SUPPORTED;

    if (!PT_OPTION_IS_SUPPORTED(capabilities.IptCapabilities0.Ebx.MtcSupport,
        FilterConfiguration->PacketGenerationOptions.PackteMtc.Enable))
        return STATUS_NOT_SUPPORTED;

    if (!PT_OPTION_IS_SUPPORTED(capabilities.IptCapabilities0.Ebx.PtwriteSupport,
        FilterConfiguration->PacketGenerationOptions.PacketPtw.Enable))
        return STATUS_NOT_SUPPORTED;

    if (!PT_OPTION_IS_SUPPORTED(capabilities.IptCapabilities0.Ebx.PsbAndPmiPreservationSupport,
        FilterConfiguration->PacketGenerationOptions.Misc.InjectPsbPmiOnEnable))
        return STATUS_NOT_SUPPORTED;

    if (!PT_OPTION_IS_SUPPORTED(capabilities.IptCapabilities0.Ebx.Cr3FilteringSupport,
        FilterConfiguration->FilteringOptions.FilterCr3.Enable))
        return STATUS_NOT_SUPPORTED;

    if((capabilities.IptCapabilities0.Eax.MaximumValidSubleaf < 1) && 
        FilterConfiguration->FilteringOptions.FilterRange.Enable)
        return STATUS_NOT_SUPPORTED;

    if (capabilities.IptCapabilities1.Eax.NumberOfAddressRanges < 
        FilterConfiguration->FilteringOptions.FilterRange.NumberOfRanges)
        return STATUS_NOT_SUPPORTED;

    for (int crtAddrRng = 0; crtAddrRng < FilterConfiguration->FilteringOptions.FilterRange.NumberOfRanges; crtAddrRng++)
        if(FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[crtAddrRng].RangeType >= RangeMax)
            return STATUS_NOT_SUPPORTED;

    return STATUS_SUCCESS;
}

NTSTATUS
IptConfigureProcessorTrace(
    INTEL_PT_CONFIGURATION* FilterConfiguration,
    INTEL_PT_OUTPUT_OPTIONS* OutputOptions
)
{
    //if (FilterConfiguration->OutputOptions.OutputType == PtOutputTypeTraceTransportSubsystem)
    //    return STATUS_NOT_SUPPORTED;

    IA32_RTIT_CTL_STRUCTURE ia32RtitCtlMsrShadow = {0};

    ia32RtitCtlMsrShadow.Fields.CycEn                  = FilterConfiguration->PacketGenerationOptions.PacketCyc.Enable;
    ia32RtitCtlMsrShadow.Fields.OS                     = FilterConfiguration->FilteringOptions.FilterCpl.FilterKm;
    ia32RtitCtlMsrShadow.Fields.User                   = FilterConfiguration->FilteringOptions.FilterCpl.FilterUm;
    ia32RtitCtlMsrShadow.Fields.PwrEvtEn               = FilterConfiguration->PacketGenerationOptions.PacketPwr.Enable;
    ia32RtitCtlMsrShadow.Fields.FUPonPTW               = FilterConfiguration->PacketGenerationOptions.Misc.EnableFupPacketsAfterPtw;
    ia32RtitCtlMsrShadow.Fields.Cr3Filter              = FilterConfiguration->FilteringOptions.FilterCr3.Enable;
    ia32RtitCtlMsrShadow.Fields.MtcEn                  = FilterConfiguration->PacketGenerationOptions.PackteMtc.Enable;
    ia32RtitCtlMsrShadow.Fields.TscEn                  = FilterConfiguration->PacketGenerationOptions.PacketTsc.Enable;
    ia32RtitCtlMsrShadow.Fields.DisRETC                = FilterConfiguration->PacketGenerationOptions.Misc.DisableRetCompression;
    ia32RtitCtlMsrShadow.Fields.PTWEn                  = FilterConfiguration->PacketGenerationOptions.PacketPtw.Enable;
    ia32RtitCtlMsrShadow.Fields.BranchEn               = FilterConfiguration->PacketGenerationOptions.PacketCofi.Enable;

    ia32RtitCtlMsrShadow.Fields.MtcFreq                = FilterConfiguration->PacketGenerationOptions.PackteMtc.Frequency;
    ia32RtitCtlMsrShadow.Fields.CycThresh              = FilterConfiguration->PacketGenerationOptions.PacketCyc.Frequency;
    ia32RtitCtlMsrShadow.Fields.PSBFreq                = FilterConfiguration->PacketGenerationOptions.Misc.PsbFrequency;

    ia32RtitCtlMsrShadow.Fields.Addr0Cfg               = FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[0].RangeType;
    ia32RtitCtlMsrShadow.Fields.Addr1Cfg               = FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[1].RangeType;
    ia32RtitCtlMsrShadow.Fields.Addr2Cfg               = FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[2].RangeType;
    ia32RtitCtlMsrShadow.Fields.Addr3Cfg               = FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[3].RangeType;

    ia32RtitCtlMsrShadow.Fields.InjectPsbPmiOnEnable   = FilterConfiguration->PacketGenerationOptions.Misc.InjectPsbPmiOnEnable;

    ia32RtitCtlMsrShadow.Fields.FabricEn               = 0; // PtOutputTypeTraceTransportSubsystem is not supported
    ia32RtitCtlMsrShadow.Fields.ToPA                   = 1;/*FilterConfiguration->OutputOptions.OutputType == PtOutputTypeSingleRegion ? 0 : 1;*/

    if (FilterConfiguration->FilteringOptions.FilterCr3.Enable)
        __writemsr(IA32_RTIT_CR3_MATCH, (unsigned long long)FilterConfiguration->FilteringOptions.FilterCr3.Cr3Address);

    if (FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[0].RangeType != RangeUnused)
    {
        DEBUG_PRINT("Writing Base Address %p to IA32_RTIT_ADDR0_A\n", FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[0].BaseAddress);
        __writemsr(IA32_RTIT_ADDR0_A, (unsigned long long)FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[0].BaseAddress);
        DEBUG_PRINT("Writing End Address %p to IA32_RTIT_ADDR0_B\n", FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[0].EndAddress);
        __writemsr(IA32_RTIT_ADDR0_B, (unsigned long long)FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[0].EndAddress);
    }

    if (FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[1].RangeType != RangeUnused)
    {
        __writemsr(IA32_RTIT_ADDR1_A, (unsigned long long)FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[1].BaseAddress);
        __writemsr(IA32_RTIT_ADDR1_B, (unsigned long long)FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[1].EndAddress);
    }

    if (FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[2].RangeType != RangeUnused)
    {
        __writemsr(IA32_RTIT_ADDR2_A, (unsigned long long)FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[2].BaseAddress);
        __writemsr(IA32_RTIT_ADDR2_B, (unsigned long long)FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[2].EndAddress);
    }

    if (FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[3].RangeType != RangeUnused)
    {
        __writemsr(IA32_RTIT_ADDR3_A, (unsigned long long)FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[3].BaseAddress);
        __writemsr(IA32_RTIT_ADDR3_B, (unsigned long long)FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[3].EndAddress);
    }

    __writemsr(IA32_RTIT_CTL, ia32RtitCtlMsrShadow.Raw);
    __writemsr(IA32_RTIT_OUTPUT_MASK_PTRS, OutputOptions->OutputMask.Raw);
    __writemsr(IA32_RTIT_OUTPUT_BASE, (unsigned long long)OutputOptions->TopaTablePa);
   
    return STATUS_SUCCESS;
}

NTSTATUS
IptAllocateNewTopaBuffer(
    unsigned                    EntriesCount,
    TOPA_ENTRY*                 TopaTableVa,
    PVOID                       NextTopaPa,
    PMDL*                       Mdl
)
{
    PMDL mdl;
    PPFN_NUMBER pfnArray;
    PHYSICAL_ADDRESS lowAddress = { .QuadPart = 0 };
    PHYSICAL_ADDRESS highAddress = { .QuadPart = -1L };
    if (!Mdl)
        return STATUS_INVALID_PARAMETER_4;

    mdl = MmAllocatePagesForMdlEx(
        lowAddress,
        highAddress,
        lowAddress,
        EntriesCount * PAGE_SIZE,
        MmCached,
        MM_ALLOCATE_FULLY_REQUIRED
    );
    if (mdl == NULL)
    {
        return STATUS_INSUFFICIENT_RESOURCES; // mood 
    }

    char* buff = (char*)MmMapLockedPagesSpecifyCache(
        mdl,
        KernelMode,
        MmNonCached,
        NULL,
        FALSE,
        NormalPagePriority
    );

    RtlFillBytes(
        buff,
        mdl->ByteCount,
        0xFF
    );

    MmUnmapLockedPages(
        buff,
        mdl
    );

    pfnArray = MmGetMdlPfnArray(mdl);


    for (unsigned i = 0; i < EntriesCount; i++)
    {
        DEBUG_PRINT("New buffer in topa. Frame: %X\n", pfnArray[i]);
        TopaTableVa[i].OutputRegionBasePhysicalAddress = pfnArray[i];
        TopaTableVa[i].END = FALSE;
        TopaTableVa[i].INT = (i == (EntriesCount - 2)) ? TRUE : FALSE;
        TopaTableVa[i].STOP = FALSE;
        TopaTableVa[i].Size = Size4K;
    }

    TopaTableVa[EntriesCount].OutputRegionBasePhysicalAddress = ((unsigned long long)NextTopaPa) >> 12;
    TopaTableVa[EntriesCount].STOP = FALSE;
    TopaTableVa[EntriesCount].INT = FALSE;
    TopaTableVa[EntriesCount].END = TRUE;
    *Mdl = mdl;

    return STATUS_SUCCESS;
}

NTSTATUS
IptInitTopaOutput(
    INTEL_PT_OUTPUT_OPTIONS* Options
)
{
    NTSTATUS status = STATUS_SUCCESS;
    TOPA_ENTRY* topaTableVa;


    PHYSICAL_ADDRESS lowAddress = { .QuadPart = 0 };
    PHYSICAL_ADDRESS highAddress = { .QuadPart = -1L };
    
    if (!Options)
        return STATUS_INVALID_PARAMETER_1;

    if (gPtCapabilities->IptCapabilities0.Ecx.TopaOutputSupport)
    {
        if (gPtCapabilities->IptCapabilities0.Ecx.TopaMultipleOutputEntriesSupport)
        {
            Options->OutputType = PtOutputTypeToPA;
            Options->TopaEntries = DEFAULT_NUMBER_OF_TOPA_ENTRIES;
        }
        else
        {
            Options->OutputType = PtOutputTypeToPASingleRegion;
            Options->TopaEntries = 1;
        }
    }
    else
    {
        // TODO: find a better way to report errors
        return STATUS_NOT_SUPPORTED;
    }

    // If the hardware cannot support that many entries, error 
    if (Options->TopaEntries > gPtCapabilities->TopaOutputEntries)
    {
        return STATUS_TOO_MANY_LINKS;
    }

    // Allocate the actual topa
    topaTableVa = MmAllocateContiguousMemorySpecifyCache(
        (Options->TopaEntries + 1) * sizeof(TOPA_ENTRY),
        lowAddress,
        highAddress,
        lowAddress,
        MmCached
    );
    if (!topaTableVa)
    {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    RtlFillMemory(
        topaTableVa,
        (Options->TopaEntries + 1) * sizeof(TOPA_ENTRY),
        0
    );

    Options->TopaTablePa = (PVOID)MmGetPhysicalAddress(topaTableVa).QuadPart;
    Options->TopaTableVa = (PVOID)topaTableVa;

    DEBUG_PRINT("TopaTablePa %x\n", (unsigned long long)Options->TopaTablePa);

    status = IptAllocateNewTopaBuffer(
        Options->TopaEntries,
        topaTableVa,
        Options->TopaTablePa,
        &(PMDL)Options->TopaMdl
    );
    if (!NT_SUCCESS(status))
        return status;

    // Configure the trace to start at table 0 index 0 in the current ToPA
    Options->OutputMask.Raw = 0x7F;
    //Options->OutputMask.Fields.OutputOffset = 0x28;
    Options->RunningStatus = IPT_STATUS_INITIALIZED;

    return status;
}


NTSTATUS
IptInitSingleRegionOutput(
    INTEL_PT_OUTPUT_OPTIONS* Options
)
{
    NTSTATUS status = STATUS_SUCCESS;
    PVOID bufferVa;
    PVOID bufferPa;

    gMemoryAllocation(
        PAGE_SIZE,
        PAGE_SIZE,
        &bufferVa,
        &bufferPa
    );
    if (!bufferVa || !bufferPa)
    {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    Options->TopaTablePa = bufferPa;
    Options->TopaTableVa = bufferVa;
    Options->OutputMask.Raw = 0;
    Options->OutputMask.Fields.MaskOrTableOffset = PAGE_SIZE - 1;

    return status;
}


NTSTATUS
IptInitOutputStructure(
    INTEL_PT_OUTPUT_OPTIONS* Options
)
{
    if (gPtCapabilities->IptCapabilities0.Ecx.TopaOutputSupport)
    {
        return IptInitTopaOutput(
            Options
        );
    }
    else
    {
        return IptInitSingleRegionOutput(
            Options
        );
    }
}

NTSTATUS
IptUninitOutputStructure(
    INTEL_PT_OUTPUT_OPTIONS* Options
)
{
    if (Options->OutputType == PtOutputTypeToPA)
    {
        MmFreePagesFromMdl(
            Options->TopaMdl
        );

        ExFreePoolWithTag(
            Options->TopaTablePa,
            IPT_POOL_TAG
        );
    }
    else
    {
        return STATUS_NOT_IMPLEMENTED;
    }

    ExFreePoolWithTag(
        Options,
        IPT_POOL_TAG
    );
    return STATUS_SUCCESS;
}

NTSTATUS
IptInitPerCore(
    INTEL_PT_CONTROL_STRUCTURE *ControlStructure
)
{
    INTEL_PT_OUTPUT_OPTIONS* outputOptions;
    NTSTATUS status;

    ControlStructure->IptEnabled = IptEnabled();
    if (ControlStructure->IptEnabled)
    {
        DEBUG_PRINT("IntelPt is already in use...\n");
    }

    outputOptions = ExAllocatePoolWithTag(
        NonPagedPool,
        sizeof(INTEL_PT_OUTPUT_OPTIONS),
        IPT_POOL_TAG
    );
    if (!outputOptions)
    {
        return STATUS_INSUFFICIENT_RESOURCES;
    }

    outputOptions->TopaEntries = DEFAULT_NUMBER_OF_TOPA_ENTRIES;

    status = IptInitOutputStructure(
        outputOptions
    );
    if (!NT_SUCCESS(status))
    {
        return status;
    }

    if (IptEnabled())
    {
        status = IptDisableTrace(NULL, NULL, outputOptions);
        if (!NT_SUCCESS(status))
        {
            DEBUG_PRINT("PtDisableTrace returned status %X\n", status);
            return status;
        }
        // TODO: If ipt is already in use, implement a mechanism to maintain the old state and revert it when IPT is disabled
    }

    ControlStructure->OutputOptions = outputOptions;

    return STATUS_SUCCESS;
}

NTSTATUS
IptInit(
    IptMemoryAllocationFunction MemoryAllocationFunction
)
{
    NTSTATUS status = STATUS_SUCCESS;

    if (!MemoryAllocationFunction)
        return STATUS_INVALID_PARAMETER_1;

    gMemoryAllocation = MemoryAllocationFunction;

    if (!gPtCapabilities)
    {
        gPtCapabilities = (INTEL_PT_CAPABILITIES*)ExAllocatePoolWithTag(
            PagedPool,
            sizeof(INTEL_PT_CAPABILITIES),
            PT_POOL_TAG
        );
        if (!gPtCapabilities)
        {
            return STATUS_INSUFFICIENT_RESOURCES;
        }

        status = IptQueryCapabilities(
            gPtCapabilities
        );

        if (!NT_SUCCESS(status))
        {
            ExFreePoolWithTag(
                gPtCapabilities,
                PT_POOL_TAG
            );
            gPtCapabilities = NULL;
        }
    }

    return status;
}

void
IptUninit(
)
{
    if (gPtCapabilities)
    {
        ExFreePoolWithTag(
            gPtCapabilities,
            PT_POOL_TAG
        );
    }
}

NTSTATUS 
IptSetup(
    INTEL_PT_CONFIGURATION* FilterConfiguration,
    INTEL_PT_CONTROL_STRUCTURE *ControlStructure
)
{
    UNREFERENCED_PARAMETER(ControlStructure);

    NTSTATUS status;
    DEBUG_PRINT(">>> BA %p EA %p\n", 
        FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[0].BaseAddress,
        FilterConfiguration->FilteringOptions.FilterRange.RangeOptions[0].EndAddress);

    status = IptValidateConfigurationRequest(
        FilterConfiguration
    );
    if (!NT_SUCCESS(status))
    {
        return status;
    }

    status = IptConfigureProcessorTrace(
        FilterConfiguration,
        ControlStructure->OutputOptions
    );

    if (!NT_SUCCESS(status))
    {
        return status;
    }
    
    ULONG crtCpu = KeGetCurrentProcessorNumber();
    UNREFERENCED_PARAMETER(crtCpu);

    return status;

}
```

`IntelPTDriver/IntelPTDriver/ProcessorTrace.h`:

```h
#ifndef _PROCESSOR_TRACE_H_
#define _PROCESSOR_TRACE_H_
#include "DriverCommon.h"
#include "ProcessorTraceShared.h"
#include "DriverUtils.h"


NTSTATUS
IptInit(
    IptMemoryAllocationFunction MemoryAllocationFunction
);

NTSTATUS
IptInitPerCore(
    INTEL_PT_CONTROL_STRUCTURE* ControlStructure
);

NTSTATUS
IptSetup(
    INTEL_PT_CONFIGURATION* FilterConfiguration,
    INTEL_PT_CONTROL_STRUCTURE* ControlStructure
);

NTSTATUS
IptEnableTrace(
    INTEL_PT_OUTPUT_OPTIONS* cpuOptions
);

NTSTATUS
IptDisableTrace(
    PMDL* Mdl,
    ULONG* BufferSize,
    INTEL_PT_OUTPUT_OPTIONS* CpuOptions
);

NTSTATUS
IptPauseTrace(
    INTEL_PT_OUTPUT_OPTIONS* cpuOptions
);

NTSTATUS
IptResumeTrace(
    INTEL_PT_OUTPUT_OPTIONS* cpuOptions
);

NTSTATUS
IptUnlinkFullTopaBuffers(
    PMDL* Mdl,
    ULONG* DataSize,
    INTEL_PT_OUTPUT_OPTIONS* CpuOptions,
    BOOLEAN AllocateNewTopa
);

BOOLEAN
IptError(

);

BOOLEAN
IptEnabled(
);

void
IptUninit(
);

void
IptClearError(
);

NTSTATUS
IptGetCapabilities(
    INTEL_PT_CAPABILITIES* Capabilities
);

BOOLEAN
IptTopaPmiReason(
);

void
IptResetPmi(
);

//NTSTATUS
//PtoSwapTopaBuffer(
//    unsigned TableIndex,
//    TOPA_TABLE* TopaTable,
//    PVOID* OldBufferVa
//);

#endif // !_PROCESSOR_TRACE_H_
```

`IntelPTDriver/IntelPTDriver/ProcessorTraceShared.h`:

```h
#ifndef _PROCESSOR_TRACE_SHARED_
#define _PROCESSOR_TRACE_SHARED_

///////////////// 4338

/// Intel PT configuration 
typedef enum _INTEL_PT_RANGE_TYPE {
    RangeUnused,
    RangeFilterEn,
    RangeTraceStop,
    RangeMax
} INTEL_PT_RANGE_TYPE;

typedef struct _INTEL_PT_RANGE {
    void* BaseAddress;
    void* EndAddress;
    INTEL_PT_RANGE_TYPE RangeType;
} INTEL_PT_RANGE;

typedef struct _INTEL_PT_FILTERING_RANGE {
    unsigned short Enable;
    unsigned short NumberOfRanges;              // Will be checked to be <= CPUID_LEAF_14_SUBLEAF_1_EAX.NumberOfAddressRanges. 
    INTEL_PT_RANGE RangeOptions[4];     // At most 4 address ranges are supported by any processor in production according to Intel
} INTEL_PT_FILTERING_RANGE;

// Cr3 filtering
typedef struct _INTEL_PT_FILTERING_CR3 {
    unsigned short Enable;
    void* Cr3Address;
} INTEL_PT_FILTERING_CR3;

// Cpl filtering
typedef struct _INTEL_PT_FILTERING_CPL {
    unsigned short FilterUm;
    unsigned short FilterKm;
} INTEL_PT_FILTERING_CPL;

// Aggregate all the filtering option structures
typedef struct _INTEL_PT_FILTERING_OPTIONS {
    INTEL_PT_FILTERING_CR3 FilterCr3;
    INTEL_PT_FILTERING_CPL FilterCpl;
    INTEL_PT_FILTERING_RANGE FilterRange;
} INTEL_PT_FILTERING_OPTIONS;

// CYC
typedef enum _INTEL_PT_PACKET_CYC_FREQUENCY {
    Freq0, Freq1, Freq2, Freq4,
    Freq8, Freq16, Freq32, Freq64,
    Freq128, Freq256, Freq512, Freq1024,
    Freq2048, Freq4096, Freq8192, Freq16384
} INTEL_PT_PACKET_CYC_FREQUENCY;

typedef struct _INTEL_PT_PACKET_CYC {
    unsigned short Enable;
    INTEL_PT_PACKET_CYC_FREQUENCY Frequency;
} INTEL_PT_PACKET_CYC;

// PWR
typedef struct _INTEL_PT_PACKET_POWER_EVENTS {
    unsigned short Enable;
} INTEL_PT_PACKET_POWER_EVENTS;

// MTC
typedef enum _INTEL_PT_PACKET_MTC_FREQUENCY {
    Art0, Art1, Art2, Art3,
    Art4, Art5, Art6, Art7,
    Art8, Art9, Art10, Art11,
    Art12, Art13, Art14, Art15
} INTEL_PT_PACKET_MTC_FREQUENCY;

typedef struct _INTEL_PT_PACKET_MTC {
    unsigned short Enable;
    INTEL_PT_PACKET_MTC_FREQUENCY Frequency;
} INTEL_PT_PACKET_MTC;

// TSC
typedef struct _INTEL_PT_PACKET_TSC {
    unsigned short Enable;
} INTEL_PT_PACKET_TSC;

// COFI -- should be enabled by default?...
typedef struct _INTEL_PT_PACKET_COFI {
    unsigned short Enable;
} INTEL_PT_PACKET_COFI;

// PTW
typedef struct _INTEL_PT_PACKET_PTW {
    unsigned short Enable;
} INTEL_PT_PACKET_PTW;

// Other packet generation options
typedef enum _INTEL_PT_PACKET_PSB_FREQUENCY {
    Freq2K, Freq4K, Freq8K, Freq16K,
    Freq32K, Freq64K, Freq128K, Freq256K,
    Freq512K, Freq1M, Freq2M, Freq4M,
    Freq8M, Freq16M, Freq32M, Freq64M
} INTEL_PT_PACKET_PSB_FREQUENCY;

typedef struct _INTEL_PT_PACKET_MISC {
    unsigned short EnableFupPacketsAfterPtw;
    unsigned short DisableRetCompression;
    unsigned short InjectPsbPmiOnEnable;
    INTEL_PT_PACKET_PSB_FREQUENCY PsbFrequency;
} INTEL_PT_PACKET_MISC;

// Aggregate all the packet generation structures
typedef struct _INTEL_PT_PACKET_GENERATION_OPTIONS {
    INTEL_PT_PACKET_CYC PacketCyc;
    INTEL_PT_PACKET_POWER_EVENTS PacketPwr;
    INTEL_PT_PACKET_MTC PackteMtc;
    INTEL_PT_PACKET_TSC PacketTsc;
    INTEL_PT_PACKET_COFI PacketCofi;
    INTEL_PT_PACKET_PTW PacketPtw;
    INTEL_PT_PACKET_MISC Misc;
} INTEL_PT_PACKET_GENERATION_OPTIONS;

// Final structure for configuring IPT
typedef struct _INTEL_PT_CONFIGURATION {
    INTEL_PT_FILTERING_OPTIONS          FilteringOptions;
    INTEL_PT_PACKET_GENERATION_OPTIONS  PacketGenerationOptions;
} INTEL_PT_CONFIGURATION;





/// Subleaf 0
typedef struct _CPUID_LEAF_14_SUBLEAF_0_EAX {
    unsigned int MaximumValidSubleaf;                               // 31:0
} CPUID_LEAF_14_SUBLEAF_0_EAX;

typedef struct _CPUID_LEAF_14_SUBLEAF_0_EBX {
    unsigned int Cr3FilteringSupport : 1;                           // 0
    unsigned int ConfigurablePsbAndCycleAccurateModeSupport : 1;    // 1
    unsigned int IpFilteringAndTraceStopSupport : 1;                // 2
    unsigned int MtcSupport : 1;                                    // 3
    unsigned int PtwriteSupport : 1;                                // 4
    unsigned int PowerEventTraceSupport : 1;                        // 5
    unsigned int PsbAndPmiPreservationSupport : 1;                  // 6
    unsigned int Reserved : 25;                                     // 31:7
} CPUID_LEAF_14_SUBLEAF_0_EBX;

typedef struct _CPUID_LEAF_14_SUBLEAF_0_ECX {
    unsigned int TopaOutputSupport : 1;                             // 0
    unsigned int TopaMultipleOutputEntriesSupport : 1;              // 1
    unsigned int SingleRangeOutputSupport : 1;                      // 2
    unsigned int OutputToTraceTransportSubsystemSupport : 1;        // 3
    unsigned int Reserved : 27;                                     // 30:4
    unsigned int IpPayloadsAreLip : 1;                              // 31
} CPUID_LEAF_14_SUBLEAF_0_ECX;

typedef struct _CPUID_LEAF_14_SUBLEAF_0_EDX {
    unsigned int Reserved;                                          // 31:0
} CPUID_LEAF_14_SUBLEAF_0_EDX;

typedef struct _CPUID_LEAF_14_SUBLEAF_0 {
    CPUID_LEAF_14_SUBLEAF_0_EAX Eax;
    CPUID_LEAF_14_SUBLEAF_0_EBX Ebx;
    CPUID_LEAF_14_SUBLEAF_0_ECX Ecx;
    CPUID_LEAF_14_SUBLEAF_0_EDX Edx;
} CPUID_LEAF_14_SUBLEAF_0;

///  Subleaf 1
typedef struct _CPUID_LEAF_14_SUBLEAF_1_EAX {
    unsigned int NumberOfAddressRanges : 3;                                 // 2:0
    unsigned int Reserved : 13;                                             // 15:3    
    unsigned int BitmapOfSupportedMtcPeriodEncodings : 16;                  // 31:16
} CPUID_LEAF_14_SUBLEAF_1_EAX;

typedef struct _CPUID_LEAF_14_SUBLEAF_1_EBX {
    unsigned int BitmapOfSupportedCycleTresholdValues : 16;                 // 15:0
    unsigned int BitmapOfSupportedConfigurablePsbFrequencyEncoding : 16;    // 31:16
} CPUID_LEAF_14_SUBLEAF_1_EBX;

typedef struct _CPUID_LEAF_14_SUBLEAF_1_ECX {
    unsigned int Reserved;                                                  // 31:0
} CPUID_LEAF_14_SUBLEAF_1_ECX;

typedef struct _CPUID_LEAF_14_SUBLEAF_1_EDX {
    unsigned int Reserved;                                                  // 31:0
} CPUID_LEAF_14_SUBLEAF_1_EDX;

typedef struct _CPUID_LEAF_14_SUBLEAF_1 {
    CPUID_LEAF_14_SUBLEAF_1_EAX Eax;
    CPUID_LEAF_14_SUBLEAF_1_EBX Ebx;
    CPUID_LEAF_14_SUBLEAF_1_ECX Ecx;
    CPUID_LEAF_14_SUBLEAF_1_EDX Edx;
} CPUID_LEAF_14_SUBLEAF_1;

typedef struct _INTEL_PT_CAPABILITIES {
    unsigned short IntelPtAvailable;
    CPUID_LEAF_14_SUBLEAF_0 IptCapabilities0;
    CPUID_LEAF_14_SUBLEAF_1 IptCapabilities1;
    unsigned long long    TopaOutputEntries;
} INTEL_PT_CAPABILITIES;





typedef union _IA32_RTIT_OUTPUT_MASK_STRUCTURE {
    struct {
        // REMINDER: Lowest mask available is 128 -> last 7 bits are ALWAYS 1
        unsigned long long MaskOrTableOffset : 32;      // 31:0
        unsigned long long OutputOffset : 32;           // 64:32
    } Fields;
    unsigned long long Raw;
} IA32_RTIT_OUTPUT_MASK_STRUCTURE;

typedef enum _INTEL_PT_OUTPUT_TYPE {
    PtOutputTypeSingleRegion,
    PtOutputTypeToPA,
    PtOutputTypeToPASingleRegion,
    PtOutputTypeTraceTransportSubsystem,    // Unsupported?
} INTEL_PT_OUTPUT_TYPE;

typedef enum _IPT_RUNNING_STATUS {

    IPT_STATUS_UNINITIALIZED,
    IPT_STATUS_INITIALIZED,
    IPT_STATUS_DISABLED = IPT_STATUS_INITIALIZED,
    IPT_STATUS_PAUSED,
    IPT_STATUS_ENABLED

} IPT_RUNNING_STATUS;

typedef struct _INTEL_PT_OUTPUT_OPTIONS {

    IPT_RUNNING_STATUS RunningStatus;
    INTEL_PT_OUTPUT_TYPE OutputType;
    unsigned TopaEntries;
    unsigned EntrySize;

    BOOLEAN Flushed;

    PVOID TopaMdl;

    PVOID TopaTablePa;
    PVOID TopaTableVa;
    IA32_RTIT_OUTPUT_MASK_STRUCTURE OutputMask;


} INTEL_PT_OUTPUT_OPTIONS;

typedef struct _INTEL_PT_CONTROL_STRUCTURE {
    BOOLEAN                     IptEnabled;
    INTEL_PT_OUTPUT_OPTIONS     *OutputOptions;

} INTEL_PT_CONTROL_STRUCTURE;

typedef void (*IptMemoryAllocationFunction)(
    unsigned Size,
    unsigned Alignment,
    void* VirtualAddress,
    void* PhysicalAddress
    );

typedef void (*IptMemoryDeallocationFunction)(
    void* VirtualAddress,
    void* PhysicalAddress
    );

#endif
```

`IntelPTDriver/IntelPTDriver/ProcessorTraceWindowsCommands.c`:

```c
#include "ProcessorTraceWindowsCommands.h"
#include "ProcessorTrace.h"
#include "ProcessorTraceWindowsControl.h"

NTSTATUS
PtwCommandTest(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
)
{
    UNREFERENCED_PARAMETER(InputBuffer);
    UNREFERENCED_PARAMETER(InputBufferLength);
    UNREFERENCED_PARAMETER(BytesWritten);
    UNREFERENCED_PARAMETER(OutputBuffer);
    UNREFERENCED_PARAMETER(OutputBufferLength);
    return STATUS_SUCCESS;
}

NTSTATUS
PtwCommandQueryCapabilities(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
)
{
    UNREFERENCED_PARAMETER(InputBuffer);
    UNREFERENCED_PARAMETER(InputBufferLength);
    UNREFERENCED_PARAMETER(BytesWritten);

    NTSTATUS status;

    if (OutputBufferLength != sizeof(INTEL_PT_CAPABILITIES))
        return STATUS_INVALID_PARAMETER_2;

    status = IptGetCapabilities((INTEL_PT_CAPABILITIES *)OutputBuffer);

    return status;
}

NTSTATUS
PtwCommandSetupIpt(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
)
{
    UNREFERENCED_PARAMETER(InputBuffer);
    UNREFERENCED_PARAMETER(InputBufferLength);
    UNREFERENCED_PARAMETER(BytesWritten);
    UNREFERENCED_PARAMETER(OutputBuffer);
    UNREFERENCED_PARAMETER(OutputBufferLength);
    return STATUS_SUCCESS;
}

NTSTATUS
PtwCommandTraceProcess(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
)
{
    UNREFERENCED_PARAMETER(InputBuffer);
    UNREFERENCED_PARAMETER(InputBufferLength);
    UNREFERENCED_PARAMETER(BytesWritten);
    UNREFERENCED_PARAMETER(OutputBuffer);
    UNREFERENCED_PARAMETER(OutputBufferLength);

    NTSTATUS status;
    status = PtwHookProcessCr3();

    return status;
}

NTSTATUS
PtwCommandTraceSSDT(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
)
{
    UNREFERENCED_PARAMETER(InputBuffer);
    UNREFERENCED_PARAMETER(InputBufferLength);
    UNREFERENCED_PARAMETER(BytesWritten);
    UNREFERENCED_PARAMETER(OutputBuffer);
    UNREFERENCED_PARAMETER(OutputBufferLength);

    NTSTATUS status;

    status = PtwHookSSDT();

    return status;
}

NTSTATUS
PtwCommandGetBuffer(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
)
{
    UNREFERENCED_PARAMETER(InputBuffer);
    UNREFERENCED_PARAMETER(InputBufferLength);

    if (OutputBufferLength != sizeof(COMM_BUFFER_ADDRESS))
    {
        *OutputBuffer = NULL;
        *BytesWritten = 0;
        return STATUS_INVALID_PARAMETER_2;
    }

    PVOID address;
    COMM_BUFFER_ADDRESS* dto;
    NTSTATUS status;
    PMDL mdl;

    status = DuDequeueElement(
        gQueueHead,
        (PVOID)&dto
    );


    while (status == STATUS_NO_MORE_ENTRIES)
    {
        KeResetEvent(&gPagesAvailableEvent);

        status = KeWaitForSingleObject(
            &gPagesAvailableEvent,
            Executive,
            KernelMode,
            TRUE,
            NULL
        );
        if (!NT_SUCCESS(status))
        {
            *OutputBuffer = NULL;
            *BytesWritten = 0;
            return status;
        }

        status = DuDequeueElement(
            gQueueHead,
            (PVOID)&dto
        );
    }
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("DuDequeueElement returned status %X\n", status);
        *OutputBuffer = NULL;
        *BytesWritten = 0;
        return status;
    }

    memcpy(((COMM_BUFFER_ADDRESS*)OutputBuffer), dto, sizeof(COMM_BUFFER_ADDRESS));

    if (!dto->Header.Options.FirstPacket)
    {
        mdl = dto->Payload.GenericPacket.BufferAddress;

        address = MmMapLockedPagesSpecifyCache(
            mdl,
            UserMode,
            MmCached,
            NULL,
            FALSE,
            NormalPagePriority
        );
        if (!NT_SUCCESS(status))
        {
            *OutputBuffer = NULL;
            *BytesWritten = 0;
            return status;
        }

        ((COMM_BUFFER_ADDRESS*)OutputBuffer)->Payload.GenericPacket.BufferAddress = address;
    }

    *BytesWritten = sizeof(COMM_BUFFER_ADDRESS);

    ExFreePoolWithTag(
        (PVOID)dto,
        'ffuB'
    );

    return status;
}

NTSTATUS
PtwCommandFreeBuffer(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
)
{
    UNREFERENCED_PARAMETER(InputBuffer);
    UNREFERENCED_PARAMETER(InputBufferLength);
    UNREFERENCED_PARAMETER(BytesWritten);
    UNREFERENCED_PARAMETER(OutputBuffer);
    UNREFERENCED_PARAMETER(OutputBufferLength);
    return STATUS_SUCCESS;
}

NTSTATUS
PtwCommandDisable(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
)
{
    UNREFERENCED_PARAMETER(InputBuffer);
    UNREFERENCED_PARAMETER(InputBufferLength);
    UNREFERENCED_PARAMETER(BytesWritten);
    UNREFERENCED_PARAMETER(OutputBuffer);
    UNREFERENCED_PARAMETER(OutputBufferLength);
    return STATUS_SUCCESS;
}

```

`IntelPTDriver/IntelPTDriver/ProcessorTraceWindowsCommands.h`:

```h
#ifndef _PROCESSOR_TRACE_WINDOWS_COMMANDS_
#define _PROCESSOR_TRACE_WINDOWS_COMMANDS_

#include "DriverCommon.h"

NTSTATUS
PtwCommandTest(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
);

NTSTATUS
PtwCommandQueryCapabilities(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
);

NTSTATUS
PtwCommandSetupIpt(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
);

NTSTATUS
PtwCommandTraceProcess(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
);

NTSTATUS
PtwCommandTraceSSDT(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
);

NTSTATUS
PtwCommandGetBuffer(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
);

NTSTATUS
PtwCommandFreeBuffer(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
);

NTSTATUS
PtwCommandDisable(
    size_t InputBufferLength,
    size_t OutputBufferLength,
    PVOID* InputBuffer,
    PVOID* OutputBuffer,
    UINT32* BytesWritten
);

#endif

```

`IntelPTDriver/IntelPTDriver/ProcessorTraceWindowsCommunication.c`:

```c
#include "ProcessorTraceWindowsCommunication.h"
#include "Debug.h"

static BOOLEAN gDefaultQueueInitialized = FALSE;

NTSTATUS
PtwCommunicationGetRequestBuffers(
    _In_ WDFREQUEST Request,
    _In_ size_t InBufferSize,
    _In_ size_t OutBufferSize,
    _Out_ PVOID* InBuffer,
    _Out_ size_t* InBufferActualSize,
    _Out_ PVOID* OutBuffer,
    _Out_ size_t* OutBufferActualSize
)
{
    NTSTATUS status = STATUS_SUCCESS;

    if (InBufferSize)
    {
        status = WdfRequestRetrieveInputBuffer(
            Request,
            InBufferSize,
            InBuffer,
            InBufferActualSize
        );
        if (!NT_SUCCESS(status))
        {
            DEBUG_PRINT("Failed retrieveing input buffer, status %X\n", status);
            return status;
        }
    }
    else
    {
        *InBufferActualSize = 0;
        InBuffer = NULL;
    }

    if (OutBufferSize)
    {
        status = WdfRequestRetrieveOutputBuffer(
            Request,
            OutBufferSize,
            OutBuffer,
            OutBufferActualSize
        );
        if (!NT_SUCCESS(status))
        {
            DEBUG_PRINT("Failed retrieveing output buffer, status %X\n", status);
            return status;
        }
    }
    else
    {
        *OutBufferActualSize = 0;
        OutBuffer = NULL;
    }

    return status;
}

VOID
PtwCommunicationIoControlCallback(
    _In_ WDFQUEUE Queue,
    _In_ WDFREQUEST Request,
    _In_ size_t OutputBufferLength,
    _In_ size_t InputBufferLength,
    _In_ ULONG IoControlCode
)
{
    PVOID inBuffer, outBuffer;
    size_t inBufferSize, outBufferSize;
    UINT32 bytesWritten = 0;
    NTSTATUS status = STATUS_SUCCESS;
    UNREFERENCED_PARAMETER(Queue);

    status = PtwCommunicationGetRequestBuffers(
        Request,
        InputBufferLength,
        OutputBufferLength,
        &inBuffer,
        &inBufferSize,
        &outBuffer,
        &outBufferSize
    );
    if (!NT_SUCCESS(status))
    {
        // Todo: log error
        goto cleanup;
    }

    unsigned function;
    if (IoControlCode == COMM_TYPE_TEST)
        function = 0;
    else if (IoControlCode == COMM_TYPE_QUERY_IPT_CAPABILITIES)
        function = 1;
    else if (IoControlCode == COMM_TYPE_SETUP_IPT)
        function = 2;
    else if (IoControlCode == COMM_TYPE_GET_BUFFER)
        function = 3;
    else if (IoControlCode == COMM_TYPE_FREE_BUFFER)
        function = 4;
    else
    {
        status = STATUS_NOT_IMPLEMENTED;
        goto cleanup;
    }

    status = gDriverData.IoCallbacks[function](
        inBufferSize,
        outBufferSize,
        inBuffer,
        outBuffer,
        &bytesWritten
        );
    if (!NT_SUCCESS(status))
    {
        // Todo: log error
        goto cleanup;
    }

cleanup:
    WdfRequestCompleteWithInformation(Request, status, bytesWritten);
    return;
}

NTSTATUS
PtwCommunicationInit(
    _In_ WDFDEVICE                  ControlDevice,
    _In_ IO_QUEUE_SETTINGS*         IoQueueSettings,
    _Inout_ WDFQUEUE*               Queue
)
{
    NTSTATUS                        status;
    WDF_IO_QUEUE_CONFIG             ioQueueConfig = { 0 };

    DEBUG_PRINT("InitCommQueue\n");

    if (IoQueueSettings->DefaultQueue && !gDefaultQueueInitialized)
    {
        gDefaultQueueInitialized = TRUE;
        WDF_IO_QUEUE_CONFIG_INIT_DEFAULT_QUEUE(&ioQueueConfig, WdfIoQueueDispatchParallel);
    }
    else if (IoQueueSettings->DefaultQueue && gDefaultQueueInitialized)
    {
        KdPrintEx((DPFLTR_IHVDRIVER_ID, DPFLTR_ERROR_LEVEL, "[ERROR] Only one default queue alowed\n"));
        return STATUS_ALREADY_INITIALIZED;
    }
    else
    {
        WDF_IO_QUEUE_CONFIG_INIT(&ioQueueConfig, WdfIoQueueDispatchManual);
    }

    ioQueueConfig.Settings.Parallel.NumberOfPresentedRequests = ((ULONG)-1);

    ioQueueConfig.EvtIoDefault = IoQueueSettings->Callbacks.IoQueueIoDefault;
    ioQueueConfig.EvtIoRead = IoQueueSettings->Callbacks.IoQueueIoRead;
    ioQueueConfig.EvtIoWrite = IoQueueSettings->Callbacks.IoQueueIoWrite;
    ioQueueConfig.EvtIoStop = IoQueueSettings->Callbacks.IoQueueIoStop;
    ioQueueConfig.EvtIoResume = IoQueueSettings->Callbacks.IoQueueIoResume;
    ioQueueConfig.EvtIoCanceledOnQueue = IoQueueSettings->Callbacks.IoQueueIoCanceledOnQueue;

    ioQueueConfig.EvtIoDeviceControl = IoQueueSettings->Callbacks.IoQueueIoDeviceControl;
    ioQueueConfig.EvtIoInternalDeviceControl = IoQueueSettings->Callbacks.IoQueueIoInternalDeviceControl;

    ioQueueConfig.PowerManaged = WdfFalse;

    status = WdfIoQueueCreate(ControlDevice, &ioQueueConfig, NULL, Queue);
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("WdfIoQueueCreate error status %X\n", status);
        return status;
    }

    return status;

}

NTSTATUS
PtwCommunicationUninit(
    _In_ WDFQUEUE Queue
)
{
    WdfIoQueueStopAndPurge(
        Queue,
        NULL,
        NULL
    );

    return STATUS_SUCCESS;
}
```

`IntelPTDriver/IntelPTDriver/ProcessorTraceWindowsCommunication.h`:

```h
#ifndef _PROCESSOR_TRACE_WINDOWS_COMMUNICATION_
#define _PROCESSOR_TRACE_WINDOWS_COMMUNICATION_

#include "DriverCommon.h"

typedef struct _IO_QUEUE_SETTINGS {

    BOOLEAN DefaultQueue;
    WDF_IO_QUEUE_DISPATCH_TYPE QueueType;

    struct CALLBACKS {
        PFN_WDF_IO_QUEUE_IO_DEFAULT IoQueueIoDefault;
        PFN_WDF_IO_QUEUE_IO_READ IoQueueIoRead;
        PFN_WDF_IO_QUEUE_IO_WRITE IoQueueIoWrite;
        PFN_WDF_IO_QUEUE_IO_STOP IoQueueIoStop;
        PFN_WDF_IO_QUEUE_IO_RESUME IoQueueIoResume;
        PFN_WDF_IO_QUEUE_IO_CANCELED_ON_QUEUE IoQueueIoCanceledOnQueue;
        PFN_WDF_IO_QUEUE_IO_DEVICE_CONTROL IoQueueIoDeviceControl;
        PFN_WDF_IO_QUEUE_IO_INTERNAL_DEVICE_CONTROL IoQueueIoInternalDeviceControl;
    } Callbacks;

} IO_QUEUE_SETTINGS;

VOID
PtwCommunicationIoControlCallback(
    _In_ WDFQUEUE Queue,
    _In_ WDFREQUEST Request,
    _In_ size_t OutputBufferLength,
    _In_ size_t InputBufferLength,
    _In_ ULONG IoControlCode
);

NTSTATUS
PtwCommunicationInit(
    _In_ WDFDEVICE                  ControlDevice,
    _In_ IO_QUEUE_SETTINGS*         IoQueueSettings,
    _Inout_ WDFQUEUE*               Queue
);

NTSTATUS
PtwCommunicationUninit(
    _In_ WDFQUEUE Queue
);

VOID
__forceinline
PtwCommunicationIngonreOperation(
    _In_ WDFFILEOBJECT FileObject
)
{
    UNREFERENCED_PARAMETER(FileObject);
}

#endif

```

`IntelPTDriver/IntelPTDriver/ProcessorTraceWindowsControl.c`:

```c
#include "ProcessorTraceWindowsControl.h"
#include "ProcessorTraceShared.h"
#include "ProcessorTrace.h"
//#include "IntelProcessorTraceDefs.h"

#define PUBLIC
#define PRIVATE
#define PTW_POOL_TAG                         'PTWP'

typedef 
void (GENERIC_PER_CORE_SYNC_ROUTINE)(
    PVOID _In_ Context
);


typedef enum _PER_CORE_SYNC_EXECUTION_LEVEL {
    IptPerCoreExecutionLevelIpi,
    IptPerCoreExecutionLevelDpc
} PER_CORE_SYNC_EXECUTION_LEVEL;

typedef struct _PER_CORE_SYNC_STRUCTURE {
    volatile SHORT IpiCounter;
    volatile SHORT DpcCounter;
    PER_CORE_SYNC_EXECUTION_LEVEL ExecutionLevel;
    PVOID Context;
    GENERIC_PER_CORE_SYNC_ROUTINE *Function;
} PER_CORE_SYNC_STRUCTURE;

GENERIC_PER_CORE_SYNC_ROUTINE PtwIpiPerCoreInit;
GENERIC_PER_CORE_SYNC_ROUTINE PtwIpiPerCoreSetup;
GENERIC_PER_CORE_SYNC_ROUTINE PtwDpcPerCoreDisable;

PRIVATE
VOID
IptPmiHandler(
    PKTRAP_FRAME pTrapFrame
);

PRIVATE
VOID
IptGenericDpcFunction(
    _In_ struct _KDPC* Dpc,
    _In_opt_ PVOID DeferredContext,
    _In_opt_ PVOID SystemArgument1,
    _In_opt_ PVOID SystemArgument2
)
{
    UNREFERENCED_PARAMETER(Dpc);
    UNREFERENCED_PARAMETER(SystemArgument1);
    UNREFERENCED_PARAMETER(SystemArgument2);

    PER_CORE_SYNC_STRUCTURE* syncStructure = (PER_CORE_SYNC_STRUCTURE*)DeferredContext;

    ULONG procNumber = KeGetCurrentProcessorNumber();
    DEBUG_PRINT("Executig method on CPU %d LEVEL DPC\n", procNumber);

    if (syncStructure->ExecutionLevel == IptPerCoreExecutionLevelDpc)
        syncStructure->Function(syncStructure->Context);

    InterlockedIncrement16(&syncStructure->DpcCounter);

}

void 
IptPageAllocation(
    unsigned Size,
    unsigned Alignment,
    void* VirtualAddress,
    void* PhysicalAddress
);

void
PtwHookImageLoadCr3(
    PUNICODE_STRING FullImageName,
    HANDLE ProcessId,
    PIMAGE_INFO ImageInfo
);

void
PtwHookImageLoadCodeBase(
    PUNICODE_STRING FullImageName,
    HANDLE ProcessId,
    PIMAGE_INFO ImageInfo
);

typedef VOID(*PMIHANDLER)(PKTRAP_FRAME TrapFrame);
BOOLEAN gFirstPage = TRUE;
WCHAR* ExecutableName = L"TracedApp.exe";
HANDLE gProcessId = 0;
BOOLEAN gThreadHoodked = FALSE;
INTEL_PT_CONTROL_STRUCTURE* gIptPerCoreControl;


ULONG_PTR
PerCoreDispatcherRoutine(
    _In_ ULONG_PTR Argument
)
{
    PER_CORE_SYNC_STRUCTURE* syncStructure = (PER_CORE_SYNC_STRUCTURE*)Argument;
   
    ULONG procNumber = KeGetCurrentProcessorNumber();
    DEBUG_PRINT("Executig method on CPU %d LEVEL IPI\n", procNumber);

    if(syncStructure->ExecutionLevel == IptPerCoreExecutionLevelIpi)
        syncStructure->Function(syncStructure->Context);
    
    InterlockedIncrement16(&syncStructure->IpiCounter);
    return 0;
}

PRIVATE
NTSTATUS
PtwExecuteAndWaitPerCore(
    GENERIC_PER_CORE_SYNC_ROUTINE* PtwPerCoreRoutine,
    PER_CORE_SYNC_EXECUTION_LEVEL ExecutionLevel,
    PVOID Context
)
{
    PKDPC pProcDpc;
    LONG numberOfCores = (LONG)KeQueryActiveProcessorCount(NULL);

    PER_CORE_SYNC_STRUCTURE syncStructure = {
        .IpiCounter = 0,
        .DpcCounter = 0,
        .ExecutionLevel = ExecutionLevel,
        .Context = Context,
        .Function = PtwPerCoreRoutine
    };

    if (ExecutionLevel == IptPerCoreExecutionLevelDpc)
    {
        for (LONG i = 0; i < numberOfCores; i++)
        {
            pProcDpc = (PKDPC)ExAllocatePoolWithTag(
                NonPagedPool,
                sizeof(KDPC),
                PTW_POOL_TAG
            );
            if (pProcDpc == NULL)
            {
                DEBUG_PRINT("pProcDpc was allocated NULL. Exitting\n");
                return STATUS_INSUFFICIENT_RESOURCES;
            }

            KeInitializeDpc(
                pProcDpc,
                IptGenericDpcFunction,
                &syncStructure
            );

            // Set DPC for all processors
            KeSetTargetProcessorDpc(
                pProcDpc,
                (CCHAR)i
            );

            BOOLEAN status = KeInsertQueueDpc(
                pProcDpc,
                NULL,
                NULL
            );
            if (!status)
            {
                return STATUS_ACCESS_DENIED;
            }

            for (unsigned kk = 0; kk < 0xffff; kk++);
        }
    }
    // TODO: Is sending an IPI a good implementation????
    KeIpiGenericCall(PerCoreDispatcherRoutine, (ULONG_PTR)&syncStructure);

    // TODO: Shoud i force an interrupt on all cores to process the DPC queues?
    if (ExecutionLevel == IptPerCoreExecutionLevelDpc)
    {
        while (syncStructure.DpcCounter != numberOfCores);
        DEBUG_PRINT("Execution DPC finished successfully\n");
    }
    else
        while (syncStructure.IpiCounter != numberOfCores);

    return STATUS_SUCCESS;
}

PUBLIC
NTSTATUS 
PtwInit()
{
    NTSTATUS status;
    
    
    gIptPerCoreControl = ExAllocatePoolWithTag(
        NonPagedPool, 
        ((unsigned long)KeQueryActiveProcessorCount(NULL)) * sizeof(INTEL_PT_CONTROL_STRUCTURE), 
        PTW_POOL_TAG
    );
    if (!gIptPerCoreControl)
        return STATUS_INSUFFICIENT_RESOURCES;

    status = IptInit(
        IptPageAllocation
    );
    if (!NT_SUCCESS(status))
    {
        return status;
    }

    status = PtwExecuteAndWaitPerCore(PtwIpiPerCoreInit, IptPerCoreExecutionLevelDpc, NULL);
    if (!NT_SUCCESS(status))
    {
        return status;
    }

    status = DuQueueInit(
        &gQueueHead,
        TRUE
    );
    if (!NT_SUCCESS(status))
    {
        DEBUG_STOP();
        return status;
    }


    return status;
}

PUBLIC
void 
PtwUninit()
{
    IptUninit();
}

typedef VOID(*PMIHANDLER)(PKTRAP_FRAME TrapFrame);

PRIVATE
NTSTATUS 
PtwSetup(
    INTEL_PT_CONFIGURATION *Config
)
{
    PMIHANDLER newPmiHandler;
    NTSTATUS status;

    DEBUG_PRINT(">>> BA %p EA %p\n",
        Config->FilteringOptions.FilterRange.RangeOptions[0].BaseAddress,
        Config->FilteringOptions.FilterRange.RangeOptions[0].EndAddress);
    
    newPmiHandler = IptPmiHandler;
    status = HalSetSystemInformation(HalProfileSourceInterruptHandler, sizeof(PVOID), (PVOID)&newPmiHandler);
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("HalSetSystemInformation returned status %X\n", status);
    }
    else
    {
        DEBUG_PRINT("HalSetSystemInformation returned SUCCESS!!!!\n");
    }


    return PtwExecuteAndWaitPerCore(PtwIpiPerCoreSetup, IptPerCoreExecutionLevelDpc, Config);
}

PUBLIC
NTSTATUS 
PtwDisable()
{
    return PtwExecuteAndWaitPerCore(PtwDpcPerCoreDisable, IptPerCoreExecutionLevelDpc, NULL);
}

PUBLIC
NTSTATUS 
PtwHookProcessCr3()
{
    NTSTATUS status;

    status = PsSetLoadImageNotifyRoutine(
        PtwHookImageLoadCr3
    );
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("PsSetLoadImageNotifyRoutine error %X\n", status);
    }

    return status;
}

PUBLIC
NTSTATUS
PtwHookProcessCodeBase()
{
    NTSTATUS status;

    status = PsSetLoadImageNotifyRoutine(
        PtwHookImageLoadCodeBase
    );
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("PsSetLoadImageNotifyRoutine error %X\n", status);
    }

    return status;
}

PUBLIC
NTSTATUS 
PtwHookSSDT()
{
    return STATUS_SUCCESS;
}

PVOID gImageBasePaStart = 0;
PVOID gImageBasePaEnd = 0;

PRIVATE
void
PtwHookThreadCreation(
    _In_ HANDLE ParentId,
    _In_ HANDLE ProcessId,
    _In_ BOOLEAN Create
)
{
    UNREFERENCED_PARAMETER(ParentId);
    NTSTATUS status;

    if (gProcessId != ProcessId && ParentId != gProcessId)
        return;

    if (!Create)
        return; // Destul de rau

    if (gThreadHoodked)
        return;

    gThreadHoodked = TRUE;


    unsigned long long  cr3 = __readcr3() & (~(PAGE_SIZE - 1));
    DEBUG_PRINT("Thread CR3 %X\n", cr3);

    INTEL_PT_CONFIGURATION filterConfiguration = {
        .FilteringOptions = {
            .FilterCpl = {
                .FilterKm = FALSE,
                .FilterUm = TRUE
            },
            .FilterCr3 = {
                .Enable = TRUE,
                .Cr3Address = (PVOID)cr3
            },
            .FilterRange = {
                .Enable = gImageBasePaStart != 0 ? TRUE : FALSE,
                .NumberOfRanges = gImageBasePaStart != 0 ? 0 : 1,
                .RangeOptions[0] = {
                    .BaseAddress = gImageBasePaStart,
                    .EndAddress = gImageBasePaEnd,
                    .RangeType = gImageBasePaStart != 0 ? RangeFilterEn : RangeUnused
            }
        }
        },
        .PacketGenerationOptions = {
            .PacketCofi.Enable = TRUE,
            .PacketCyc = {0},
            .PacketPtw = {0},
            .PacketPwr = {0},
            .PacketTsc.Enable = TRUE,
            .PackteMtc = {0},
            .Misc = {
                .PsbFrequency = Freq4K,
                //.InjectPsbPmiOnEnable = TRUE,
                .DisableRetCompression = TRUE
            },
        }
    };

    status = PtwSetup(
        &filterConfiguration
    );
    if (!NT_SUCCESS(status))
    {
        return;
    }

    DEBUG_PRINT("STARTED TRACING\n");

    return;

}

PRIVATE
void
PtwHookProcessExit(
    _Inout_ PEPROCESS Process,
    _In_ HANDLE ProcessId,
    _Inout_opt_ PPS_CREATE_NOTIFY_INFO CreateInfo
)
{
    UNREFERENCED_PARAMETER(Process);
    NTSTATUS status;
    PMIHANDLER oldHandler;

    if (CreateInfo)
        return;

    if (!gProcessId)
        return;

    if (ProcessId != gProcessId)
        return;

    PtwDisable();

    status = PsRemoveCreateThreadNotifyRoutine(
        PtwHookThreadCreation
    );
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("PsRemoveCreateThreadNotifyRoutine returned status %X\n", status);
    }

    //status = PsSetCreateProcessNotifyRoutineEx(
    //    PtwHookProcessExit,
    //    TRUE
    //);
    //if (!NT_SUCCESS(status))
    //{
    //    DEBUG_STOP();
    //    DEBUG_PRINT("PsSetCreateProcessNotifyRoutineEx returned status %X\n", status);
    //}

    //DEBUG_STOP();
    /// TODO: place this code somewhere

    oldHandler = 0;
    status = HalSetSystemInformation(HalProfileSourceInterruptHandler, sizeof(PVOID), (PVOID)&oldHandler);
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("HalSetSystemInformation returned status %X\n", status);
    }
    else
    {
        DEBUG_PRINT("HalSetSystemInformation returned SUCCESS!!!!\n");
    }


    gProcessId = 0;
    gThreadHoodked = FALSE;

    return;
}

PRIVATE
void
PtwHookImageLoadCr3(
    PUNICODE_STRING FullImageName,
    HANDLE ProcessId,
    PIMAGE_INFO ImageInfo
)
{
    UNREFERENCED_PARAMETER(ImageInfo);

    NTSTATUS status;
    wchar_t* found = wcsstr(
        FullImageName->Buffer,
        ExecutableName
    );

    if (found == NULL)
    {
        goto cleanup;
    }

    gProcessId = ProcessId;

    DuIncreaseSequenceId();
    gDriverData.PacketIdCounter = 0;

    PVOID imageBasePhysicalStartAddress = (PVOID)MmGetPhysicalAddress(ImageInfo->ImageBase).QuadPart;
    PVOID imageBasePhysicalEndAddress = (PVOID)((unsigned long long)imageBasePhysicalStartAddress + ImageInfo->ImageSize);

    DEBUG_PRINT(">>>>>>>>>>>>\nProcess Image Base VA %p\nProcess Image Base Start PA %p\nImage Base End PA %p\nProcess Image Size %lld\n<<<<<<<<<<<\n",
        ImageInfo->ImageBase,
        imageBasePhysicalStartAddress,
        imageBasePhysicalEndAddress,
        ImageInfo->ImageSize);

    gImageBasePaStart = ImageInfo->ImageBase;
    gImageBasePaEnd = (PVOID)((char*)ImageInfo->ImageBase + ImageInfo->ImageSize);

    COMM_BUFFER_ADDRESS* dto =
        (COMM_BUFFER_ADDRESS*)ExAllocatePoolWithTag(
            NonPagedPool,
            sizeof(COMM_BUFFER_ADDRESS),
            'ffuB'
        );
    if (!dto)
    {
        return;
    }

    dto->Header.HeaderSize = sizeof(PACKET_HEADER_INFORMATION);
    dto->Header.Options.FirstPacket = TRUE;
    dto->Header.Options.LastPacket = FALSE;
    dto->Header.CpuId = KeGetCurrentProcessorNumber();
    status = DuGetSequenceId(&dto->Header.SequenceId);
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("DuGetSequenceId returned status%X\n", status);
        dto->Header.SequenceId = 0; // Handle the error by sengding an unused sequence ID
    }
    dto->Header.PacketId = DuGetPacketId();

    dto->Payload.FirstPacket.ImageBaseAddress = gImageBasePaStart;
    dto->Payload.FirstPacket.ProcessorFrequency = gDriverData.ProcessorFrequency;
    dto->Payload.FirstPacket.ImageSize = (unsigned long)ImageInfo->ImageSize;


    DuEnqueueElement(
        gQueueHead,
        (PVOID)dto
    );


    status = PsSetCreateThreadNotifyRoutineEx(
        PsCreateThreadNotifyNonSystem,
        (PVOID)PtwHookThreadCreation
    );
    if (!NT_SUCCESS(status)) 
    {
        return;
    }

    status = PsSetCreateProcessNotifyRoutineEx(
        PtwHookProcessExit,
        FALSE
    );
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("Failed to hook process creation. Status %X\n", status);
        return;
    }

cleanup:
    return;
}

PRIVATE
void
PtwHookImageLoadCodeBase(
    PUNICODE_STRING FullImageName,
    HANDLE ProcessId,
    PIMAGE_INFO ImageInfo
)
{
    UNREFERENCED_PARAMETER(ImageInfo);

    NTSTATUS status;
    wchar_t* found = wcsstr(
        FullImageName->Buffer,
        ExecutableName
    );

    if (found == NULL)
    {
        goto cleanup;
    }
        
    gProcessId = ProcessId;

    status = PsSetCreateThreadNotifyRoutineEx(
        PsCreateThreadNotifyNonSystem,
        (PVOID)PtwHookThreadCreation
    );
    if (!NT_SUCCESS(status))
    {
        return;
    }

    DuIncreaseSequenceId();
    gDriverData.PacketIdCounter = 0;

    PVOID imageBasePhysicalStartAddress = (PVOID)MmGetPhysicalAddress(ImageInfo->ImageBase).QuadPart;
    PVOID imageBasePhysicalEndAddress = (PVOID)((unsigned long long)imageBasePhysicalStartAddress + ImageInfo->ImageSize);

    DEBUG_PRINT(">>>>>>>>>>>>\nProcess Image Base VA %p\nProcess Image Base Start PA %p\nImage Base End PA %p\nProcess Image Size %lld\n<<<<<<<<<<<\n",
        ImageInfo->ImageBase,
        imageBasePhysicalStartAddress,
        imageBasePhysicalEndAddress,
        ImageInfo->ImageSize);

    gImageBasePaStart = ImageInfo->ImageBase;
    gImageBasePaEnd = (PVOID)((char *)ImageInfo->ImageBase + ImageInfo->ImageSize);

    COMM_BUFFER_ADDRESS* dto =
        (COMM_BUFFER_ADDRESS*)ExAllocatePoolWithTag(
            NonPagedPool,
            sizeof(COMM_BUFFER_ADDRESS),
            'ffuB'
        );
    if (!dto)
    {
        return;
    }

    dto->Header.HeaderSize = sizeof(PACKET_HEADER_INFORMATION);
    dto->Header.Options.FirstPacket = TRUE;
    dto->Header.Options.LastPacket = FALSE;
    dto->Header.CpuId = KeGetCurrentProcessorNumber();
    status = DuGetSequenceId(&dto->Header.SequenceId);
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("DuGetSequenceId returned status%X\n", status);
        dto->Header.SequenceId = 0; // Handle the error by sengding an unused sequence ID
    }

    dto->Header.PacketId = DuGetPacketId();
    dto->Payload.FirstPacket.ImageBaseAddress = gImageBasePaStart;
    dto->Payload.FirstPacket.ProcessorFrequency = gDriverData.ProcessorFrequency;
    dto->Payload.FirstPacket.ImageSize = (unsigned long)ImageInfo->ImageSize;


    DuEnqueueElement(
        gQueueHead,
        (PVOID)dto
    );

    status = PsSetCreateThreadNotifyRoutineEx(
        PsCreateThreadNotifyNonSystem,
        (PVOID)PtwHookThreadCreation
    );
    if (!NT_SUCCESS(status))
    {
        return;
    }

    status = PsSetCreateProcessNotifyRoutineEx(
        PtwHookProcessExit,
        FALSE
    );
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("Failed to hook process creation. Status %X\n", status);
        return;
    }

cleanup:
    return;
}

PRIVATE
VOID
IptDpc(
    _In_ struct _KDPC* Dpc,
    _In_opt_ PVOID DeferredContext,
    _In_opt_ PVOID SystemArgument1,
    _In_opt_ PVOID SystemArgument2
)
{
    UNREFERENCED_PARAMETER(Dpc);
    UNREFERENCED_PARAMETER(DeferredContext);
    UNREFERENCED_PARAMETER(SystemArgument1);
    UNREFERENCED_PARAMETER(SystemArgument2);

    NTSTATUS status;
    PMDL mdl;
    ULONG bufferSize;

    DEBUG_PRINT(">>> DPC ON AP %d\n", KeGetCurrentProcessorNumber());

    status = IptUnlinkFullTopaBuffers(
        &mdl,
        &bufferSize,
        gIptPerCoreControl[KeGetCurrentProcessorNumber()].OutputOptions,
        TRUE
    );
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("IptUnlinkFullTopaBuffers error %X\n", status);
        return;
    }

    COMM_BUFFER_ADDRESS* dto =
        (COMM_BUFFER_ADDRESS*)ExAllocatePoolWithTag(
            NonPagedPool,
            sizeof(COMM_BUFFER_ADDRESS),
            'ffuB'
        );
    
    if (!dto)
    {
        return;
    }


    dto->Header.HeaderSize = sizeof(PACKET_HEADER_INFORMATION);
    status = DuGetSequenceId(&dto->Header.SequenceId);
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("DuGetSequenceId returned status%X\n", status);
        dto->Header.SequenceId = 0; // Handle the error by sengding an unused sequence ID
    }
    dto->Header.PacketId = DuGetPacketId();
    dto->Header.Options.LastPacket = FALSE;
    dto->Header.Options.FirstPacket = FALSE;
    dto->Header.CpuId = KeGetCurrentProcessorNumber();

    dto->Payload.GenericPacket.BufferSize = bufferSize;
    dto->Payload.GenericPacket.BufferAddress = mdl;


    status = DuEnqueueElement(
        gQueueHead,
        (PVOID)dto
    );
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("DuEnqueueElements error %X\n", status);
    }
    else
    {
        KeSetEvent(&gPagesAvailableEvent, 0, FALSE);
    }

    IptResetPmi();
    IptResumeTrace(
        gIptPerCoreControl[KeGetCurrentProcessorNumber()].OutputOptions
    );
}

PRIVATE
VOID 
IptPmiHandler(
    PKTRAP_FRAME pTrapFrame
)
{
    PKDPC pProcDpc;
    DEBUG_PRINT(">>> PMI ON AP %d\n", KeGetCurrentProcessorNumber());
    UNREFERENCED_PARAMETER(pTrapFrame);

    if (!IptTopaPmiReason())
    {
        IptResetPmi();
        return;
    }

    DEBUG_PRINT(">>> PAUSING TRACE ON AP %d\n", KeGetCurrentProcessorNumber());
    IptPauseTrace(gIptPerCoreControl[KeGetCurrentProcessorNumber()].OutputOptions);
    
    pProcDpc = (PKDPC)ExAllocatePoolWithTag(
        NonPagedPool,
        sizeof(KDPC),
        PTW_POOL_TAG
    );
    if (pProcDpc == NULL)
    {
        DEBUG_PRINT("pProcDpc was allocated NULL. Exitting\n");
        return;
    }

    KeInitializeDpc(
        pProcDpc,
        IptDpc,
        NULL
    );

    KeSetTargetProcessorDpc(
        pProcDpc,
        (CCHAR)KeGetCurrentProcessorNumber()
    );

    KeInsertQueueDpc(
        pProcDpc,
        (PVOID)KeGetCurrentProcessorNumber(),
        NULL
    );
}


// Ipi routines:
PRIVATE
void
PtwIpiPerCoreInit(
    _In_ PVOID Context
)
{
    UNREFERENCED_PARAMETER(Context);
    
    NTSTATUS status;
    ULONG procnumber = KeGetCurrentProcessorNumber();

    status = IptInitPerCore(
        &gIptPerCoreControl[procnumber]
    );
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("IptInitPerCore %X\n", status);
    }

    return;
}

PRIVATE
void 
IptPageAllocation(
    unsigned Size,
    unsigned Alignment,
    void* VirtualAddress,
    void* PhysicalAddress
)
{
    UNREFERENCED_PARAMETER(Alignment);

    NTSTATUS status;
    status = DuAllocateBuffer(
        Size,
        MmCached,
        TRUE,
        VirtualAddress,
        PhysicalAddress
    );
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("DuAllocateBuffer returned status %X\n", status);
    }
    return;
}

void
IptPageFree(
    void* VirtualAddress,
    void* PhysicalAddress
)
{
    UNREFERENCED_PARAMETER(PhysicalAddress);
    DuFreeBuffer(
        VirtualAddress
    );
}


PRIVATE
void
PtwIpiPerCoreSetup(
    _In_ PVOID Context
)
{
    INTEL_PT_CONFIGURATION* config = (INTEL_PT_CONFIGURATION*)Context;
    //INTEL_PT_CONTROL_STRUCTURE controlStructure;
    NTSTATUS status;


    ULONG currentProcessorNumber = KeGetCurrentProcessorNumber();

    status = IptSetup(
        config,
        &gIptPerCoreControl[currentProcessorNumber]
    );
    if (!NT_SUCCESS(status))
    {
        DEBUG_STOP();
        DEBUG_PRINT("IptSetup returned status %X\n", status);
    }

    IptEnableTrace(
        gIptPerCoreControl[currentProcessorNumber].OutputOptions
    );

    return;
}

PRIVATE
void
PtwDpcPerCoreDisable(
    _In_ PVOID Context
)
{
    UNREFERENCED_PARAMETER(Context);
    PMDL mdl;
    NTSTATUS status;
    ULONG bufferSize;
    IptDisableTrace(
        &mdl,
        &bufferSize,
        gIptPerCoreControl[KeGetCurrentProcessorNumber()].OutputOptions
    );

    COMM_BUFFER_ADDRESS* dto =
        (COMM_BUFFER_ADDRESS*)ExAllocatePoolWithTag(
            NonPagedPool,
            sizeof(COMM_BUFFER_ADDRESS),
            'ffuB'
        );
    if (!dto)
    {
        return;
    }

    dto->Header.HeaderSize = sizeof(PACKET_HEADER_INFORMATION);
    dto->Header.Options.FirstPacket = FALSE;
    dto->Header.Options.LastPacket = TRUE;
    dto->Header.CpuId = KeGetCurrentProcessorNumber();
    dto->Header.PacketId = DuGetPacketId();
    status = DuGetSequenceId(&dto->Header.SequenceId);
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("DuGetSequenceId returned status%X\n", status);
        dto->Header.SequenceId = 0; // Handle the error by sengding an unused sequence ID
    }

    dto->Payload.GenericPacket.BufferSize = bufferSize;
    dto->Payload.GenericPacket.BufferAddress = mdl;

    status = DuEnqueueElement(
        gQueueHead,
        (PVOID)dto
    );
    if (!NT_SUCCESS(status))
    {
        DEBUG_PRINT("DuEnqueueElements error %X\n", status);
    }
    else
    {
        KeSetEvent(&gPagesAvailableEvent, 0, FALSE);
    }

    return;
}

```

`IntelPTDriver/IntelPTDriver/ProcessorTraceWindowsControl.h`:

```h
#ifndef _PROCESSOR_TRACE_WINDOWS_CONTROL_
#define _PROCESSOR_TRACE_WINDOWS_CONTROL_

#include <ntddk.h>

NTSTATUS
PtwInit();

void
PtwUninit();

NTSTATUS
PtwSetup();

NTSTATUS
PtwEnable();

NTSTATUS
PtwDisable();

NTSTATUS
PtwHookProcessCodeBase();

NTSTATUS
PtwHookProcessCr3();

NTSTATUS
PtwHookSSDT();



#endif

```

`IntelPTDriver/IntelPTDriver/Public.h`:

```h
#ifndef _PUBLIC_H_
#define _PUBLIC_H_

#include <guiddef.h>

DEFINE_GUID(
    GUID_DEVINTERFACE_ECHO,
    0xcdc35b6e, 0xbe4, 0x4936, 0xbf, 0x5f, 0x55, 0x37, 0x38, 0xa, 0x7c, 0x1a);

#define SAMPLE_DEVICE_NATIVE_NAME    L"\\Device\\SampleComm"
#define SAMPLE_DEVICE_USER_NAME      L"\\Global??\\SampleComm"
#define SAMPLE_DEVICE_OPEN_NAME      L"\\\\.\\SampleComm"

typedef struct _COMM_DATA_TEST {
    int Magic;
} COMM_DATA_TEST;

typedef struct _COMM_DATA_QUERY_IPT_CAPABILITIES {
    int placeholder;
} COMM_DATA_QUERY_IPT_CAPABILITIES;

typedef struct _COMM_DATA_SETUP_IPT {
    void* BufferAddress;
    unsigned long long BufferSize;
} COMM_DATA_SETUP_IPT;

#pragma pack(push, 1)

typedef struct _PACKET_HEADER_INFORMATION {
    unsigned HeaderSize;
    unsigned long long PacketId;
    unsigned CpuId;
    unsigned SequenceId;

    struct PacketInformation
    {
        short FirstPacket : 1;
        short LastPacket : 1;
        short ExecutableTrace : 1;
    } Options;
} PACKET_HEADER_INFORMATION;

typedef struct _COMM_BUFFER_ADDRESS {
    
    PACKET_HEADER_INFORMATION Header;

    union {
        struct {
            PVOID BufferAddress;
            unsigned long BufferSize;
        } GenericPacket;
        struct {
            PVOID ImageBaseAddress;
            unsigned long ImageSize;
            ULONGLONG ProcessorFrequency;
        } FirstPacket;

    } Payload;

} COMM_BUFFER_ADDRESS;

typedef struct _KAFKA_PACKET {
    PACKET_HEADER_INFORMATION Header;
    char Data[1];
} KAFKA_PACKET;

typedef struct _KAFKA_PACKET_FIRST {
    PACKET_HEADER_INFORMATION Header;
    PVOID ImageBaseAddress;
    ULONG ImageSize;
    ULONGLONG ProcessorFrequency;
} KAFKA_PACKET_FIRST;

#pragma pack(pop)

#define UM_TEST_MAGIC                       0x1234
#define KM_TEST_MAGIC                       0x4321

#define COMM_TYPE_TEST                      CTL_CODE(FILE_DEVICE_UNKNOWN, 0x8B0, METHOD_BUFFERED, FILE_ANY_ACCESS)
#define COMM_TYPE_QUERY_IPT_CAPABILITIES    CTL_CODE(FILE_DEVICE_UNKNOWN, 0x8B1, METHOD_BUFFERED, FILE_ANY_ACCESS)
#define COMM_TYPE_SETUP_IPT                 CTL_CODE(FILE_DEVICE_UNKNOWN, 0x8B2, METHOD_BUFFERED, FILE_ANY_ACCESS)
#define COMM_TYPE_GET_BUFFER                CTL_CODE(FILE_DEVICE_UNKNOWN, 0x8B3, METHOD_BUFFERED, FILE_ANY_ACCESS)
#define COMM_TYPE_FREE_BUFFER               CTL_CODE(FILE_DEVICE_UNKNOWN, 0x8B4, METHOD_BUFFERED, FILE_ANY_ACCESS)

#endif

```

`IntelPTDriver/IntelPTUserModeApp/Commands.c`:

```c
#include "Globals.h"

LARGE_INTEGER gProcessorFrequency;

NTSTATUS
CommandTest(
    PVOID   InParameter,
    PVOID*  Result
)
{
	COMMUNICATION_MESSAGE *message;
	OVERLAPPED* overlapped = NULL;
	NTSTATUS status;
	DWORD bytesWritten;

	UNREFERENCED_PARAMETER(InParameter);
	UNREFERENCED_PARAMETER(Result);

	message = (COMMUNICATION_MESSAGE*)malloc(sizeof(COMMUNICATION_MESSAGE));
	if (!message)
	{
		return STATUS_INVALID_HANDLE;
	}

	COMM_DATA_TEST* commTestBufferIn = (COMM_DATA_TEST*)malloc(sizeof(COMM_DATA_TEST));
	if (!commTestBufferIn)
	{
		printf_s("malloc error\n");
		return STATUS_INVALID_HANDLE;
	}

	COMM_DATA_TEST* commTestBufferOut = (COMM_DATA_TEST*)malloc(sizeof(COMM_DATA_TEST));
	if (!commTestBufferOut)
	{
		printf_s("malloc error\n");
		return STATUS_INVALID_HANDLE;
	}

	commTestBufferIn->Magic = UM_TEST_MAGIC;

	message->MethodType = COMM_TYPE_TEST;
	message->DataIn = commTestBufferIn;
	message->DataInSize = sizeof(COMM_DATA_TEST);
	message->DataOut = commTestBufferOut;
	message->DataOutSize = sizeof(COMM_DATA_TEST);
	message->BytesWritten = &bytesWritten;

	status = CommunicationSendMessage(
		message,
		&overlapped
	);
	if (!SUCCEEDED(status) || !overlapped)
	{
		// ...
		return STATUS_INVALID_HANDLE;
	}
	
	DWORD result = WaitForSingleObject(overlapped->hEvent, INFINITE);
	if (result == WAIT_OBJECT_0)
	{
		printf_s("DeviceIoControl successful\n");
	}
	else
	{
		printf_s("DeviceIoControl unsuccessful\n");
	}

	if (commTestBufferOut->Magic == KM_TEST_MAGIC)
	{
		printf_s("DeviceIoControl did return required status\n");
	}
	else
	{
		printf_s("DeviceIoControl did not return required status\n");
	}

	if (overlapped)
		free(overlapped);
	if (commTestBufferOut)
		free(commTestBufferOut);
	if (commTestBufferIn)
		free(commTestBufferIn);
	if (message)
		free(message);

	*Result = NULL;

	return CMC_STATUS_SUCCESS;
}

NTSTATUS
CommandQueryPtCapabilities(
    PVOID   InParameter,
    PVOID*  Result
)
{
	UNREFERENCED_PARAMETER(InParameter);
	UNREFERENCED_PARAMETER(Result);

	NTSTATUS status;
	INTEL_PT_CAPABILITIES capabilities;
	COMMUNICATION_MESSAGE message;
	DWORD bytesWritten;
	OVERLAPPED* overlapped = NULL;


	message.MethodType = COMM_TYPE_QUERY_IPT_CAPABILITIES;
	message.DataIn = NULL;
	message.DataInSize = 0;
	message.DataOut = &capabilities;
	message.DataOutSize = sizeof(capabilities);
	message.BytesWritten = &bytesWritten;

	status = CommunicationSendMessage(
		&message,
		&overlapped
	);
	if (!SUCCEEDED(status) || !overlapped)
	{
		*Result = NULL;

		// ...
		return STATUS_INVALID_HANDLE;
	}

	DWORD result = WaitForSingleObject(overlapped->hEvent, INFINITE);
	if (result == WAIT_OBJECT_0)
	{
		printf_s("	\
INTEL PT IS %s:\n		\
LEAF 0 EAX IS %X\n			\
LEAF 0 EBX IS %X\n			\
LEAF 0 ECX IS %X\n			\
LEAF 0 EDX IS %X\n			\
LEAF 1 EAX IS %X\n			\
LEAF 1 EBX IS %X\n			\
LEAF 1 ECX IS %X\n			\
LEAF 1 EDX IS %X\n			\
MaximumValidSubleaf %d\n	\
Cr3FilteringSupport %d\n	\
ConfigurablePsbAndCycleAccurateModeSupport %d\n	\
IpFilteringAndTraceStopSupport %d\n	\
MtcSupport %d\n	\
PtwriteSupport %d\n	\
PowerEventTraceSupport %d\n	\
PsbAndPmiPreservationSupport %d\n	\
TopaOutputSupport %d\n	\
TopaMultipleOutputEntriesSupport %d\n	\
SingleRangeOutputSupport %d\n	\
OutputToTraceTransportSubsystemSupport %d\n	\
IpPayloadsAreLip %d\n	\
NumberOfAddressRanges %d\n	\
BitmapOfSupportedMtcPeriodEncodings %d\n	\
BitmapOfSupportedCycleTresholdValues %d\n	\
BitmapOfSupportedConfigurablePsbFrequencyEncoding %d\n	\
NumberOfTopaOutputEntries %lld\n",
			capabilities.IntelPtAvailable ? "AVAILABLE" : "UNAVAILABLE",
			capabilities.IptCapabilities0.Eax,
			capabilities.IptCapabilities0.Ebx,
			capabilities.IptCapabilities0.Ecx,
			capabilities.IptCapabilities0.Edx,
			capabilities.IptCapabilities1.Eax,
			capabilities.IptCapabilities1.Ebx,
			capabilities.IptCapabilities1.Ecx,
			capabilities.IptCapabilities1.Edx,
			capabilities.IptCapabilities0.Eax.MaximumValidSubleaf,
			capabilities.IptCapabilities0.Ebx.Cr3FilteringSupport,
			capabilities.IptCapabilities0.Ebx.ConfigurablePsbAndCycleAccurateModeSupport,
			capabilities.IptCapabilities0.Ebx.IpFilteringAndTraceStopSupport,
			capabilities.IptCapabilities0.Ebx.MtcSupport,
			capabilities.IptCapabilities0.Ebx.PtwriteSupport,
			capabilities.IptCapabilities0.Ebx.PowerEventTraceSupport,
			capabilities.IptCapabilities0.Ebx.PsbAndPmiPreservationSupport,
			capabilities.IptCapabilities0.Ecx.TopaOutputSupport,
			capabilities.IptCapabilities0.Ecx.TopaMultipleOutputEntriesSupport,
			capabilities.IptCapabilities0.Ecx.SingleRangeOutputSupport,
			capabilities.IptCapabilities0.Ecx.OutputToTraceTransportSubsystemSupport,
			capabilities.IptCapabilities0.Ecx.IpPayloadsAreLip,
			capabilities.IptCapabilities1.Eax.NumberOfAddressRanges,
			capabilities.IptCapabilities1.Eax.BitmapOfSupportedMtcPeriodEncodings,
			capabilities.IptCapabilities1.Ebx.BitmapOfSupportedCycleTresholdValues,
			capabilities.IptCapabilities1.Ebx.BitmapOfSupportedConfigurablePsbFrequencyEncoding,
			capabilities.TopaOutputEntries
		);

	}
	else
	{
		printf_s("DeviceIoControl unsuccessful\n");
	}

	if (overlapped)
		free(overlapped);

	*Result = NULL;

	return CMC_STATUS_SUCCESS;
}

DWORD
WINAPI
ThreadProc(
	_In_ LPVOID lpParameter
);

NTSTATUS
CommandSetupPt(
    _In_    PVOID   InParameter,
    _Out_   PVOID*  Result
)
{
	UNREFERENCED_PARAMETER(InParameter);
	UNREFERENCED_PARAMETER(Result);

	NTSTATUS status;
	COMMUNICATION_MESSAGE message;
	DWORD bytesWritten;
	OVERLAPPED* overlapped = NULL;
	PVOID data;

	message.MethodType = COMM_TYPE_SETUP_IPT;
	message.DataIn = NULL;
	message.DataInSize = 0;
	message.DataOut = &data;
	message.DataOutSize = sizeof(COMM_DATA_SETUP_IPT);
	message.BytesWritten = &bytesWritten;

	printf_s("[INFO] Kafka init\n");
	status = KafkaInit(
		gApplicationGlobals->KafkaConfig.BootstrapServer,
		&gApplicationGlobals->KafkaConfig.KafkaHandler
	);
	if (status != CMC_STATUS_SUCCESS)
	{
		printf_s("[ERROR] Kafka initialization failed!\n");
		return status;
	}
	printf_s("[INFO] Kafka init successful\n");


	printf_s("[INFO] Calling the driver\n");
	status = CommunicationSendMessage(
		&message,
		&overlapped
	);
	if (!SUCCEEDED(status) || !overlapped)
	{
		*Result = NULL;

		// ...
		return STATUS_INVALID_HANDLE;
	}
	printf_s("[INFO] Driver called\n");

	
	DWORD result = WaitForSingleObject(overlapped->hEvent, INFINITE);
	printf_s("[INFO] Driver responded\n");
	if (result == WAIT_OBJECT_0)
	{
		QueryPerformanceFrequency(
			&gProcessorFrequency
		);

		printf_s("[INFO] Create worker thread\n");
		HANDLE thread = CreateThread(
			NULL,
			0,
			ThreadProc,
			data,
			0,
			NULL
		);
		if (thread == INVALID_HANDLE_VALUE || !thread)
		{
			return STATUS_FATAL_APP_EXIT;
		}
		WaitForSingleObject(thread, 0);
		printf_s("[INFO] Thread created\n");
	}
	else
	{
		printf_s("[ERROR] DeviceIoControl unsuccessful\n");
	}

	if (overlapped)
		free(overlapped);

	*Result = NULL;

	return CMC_STATUS_SUCCESS;
}

NTSTATUS
CommandGetBuffer(
	unsigned long long* BufferId,
	PVOID* Buffer
)
{
	NTSTATUS status;
	COMMUNICATION_MESSAGE message;
	DWORD bytesWritten;
	OVERLAPPED* overlapped = NULL;
	COMM_BUFFER_ADDRESS *data = calloc(1, sizeof(COMM_BUFFER_ADDRESS));

	message.MethodType = COMM_TYPE_GET_BUFFER;
	message.DataIn = NULL;
	message.DataInSize = 0;
	message.DataOut = data;
	message.DataOutSize = sizeof(COMM_BUFFER_ADDRESS);
	message.BytesWritten = &bytesWritten;

	status = CommunicationSendMessage(
		&message,
		&overlapped
	);
	if (!SUCCEEDED(status) || !overlapped)
	{
		return status;
	}

	DWORD result = WaitForSingleObject(
		overlapped->hEvent, 
		INFINITE
	);
	if (result == WAIT_OBJECT_0)
	{
		if (bytesWritten != sizeof(COMM_BUFFER_ADDRESS))
		{
			status = STATUS_INVALID_HANDLE;
			goto cleanup;
		}

		*Buffer = data;
		*BufferId = data->Header.SequenceId;
	}
	else
	{
		printf_s("WaitForSingleObject unsuccessful\n");
	}

cleanup:
	if (overlapped)
		free(overlapped);

	return status;
}


NTSTATUS
CommandFreeBuffer(
	unsigned long long BufferId
)
{
	NTSTATUS status;
	COMMUNICATION_MESSAGE message;
	DWORD bytesWritten;
	OVERLAPPED* overlapped = NULL;
	unsigned long long bufferId = BufferId;

	message.MethodType = COMM_TYPE_FREE_BUFFER;
	message.DataIn = &bufferId;
	message.DataInSize = sizeof(unsigned long long);
	message.DataOut = NULL;
	message.DataOutSize = 0;
	message.BytesWritten = &bytesWritten;

	status = CommunicationSendMessage(
		&message,
		&overlapped
	);
	if (!SUCCEEDED(status) || !overlapped)
	{
		return status;
	}

	DWORD result = WaitForSingleObject(
		overlapped->hEvent,
		INFINITE
	);
	if (result != WAIT_OBJECT_0)
	{
		printf_s("WaitForSingleObject unsuccessful\n");
	}

	if (overlapped)
		free(overlapped);

	return status;
}



DWORD
WINAPI
ThreadProc(
	_In_ LPVOID lpParameter
)
{
	UNREFERENCED_PARAMETER(lpParameter);

	NTSTATUS status;
	unsigned long long bufferId;
	COMM_BUFFER_ADDRESS *packetInfo;
	unsigned long packetSize;
	FILE* fileHandle;
	KAFKA_PACKET* packet;
	//DebugBreak();

	while (1 == 1)
	{
		//DebugBreak();

		status = CommandGetBuffer(
			&bufferId,
			&(PVOID)packetInfo
		);
		if (!SUCCEEDED(status))
		{
			continue;
		}

		long long tscInSeconds = (__rdtsc() / gProcessorFrequency.QuadPart);

		if (packetInfo->Header.Options.FirstPacket)
		{
			packetSize = sizeof(KAFKA_PACKET_FIRST);
			KAFKA_PACKET_FIRST *packetFirst = (KAFKA_PACKET_FIRST*)calloc(1, packetSize);
			if (packetFirst == NULL)
			{
				printf_s("[ERROR] Cannot allocate kafka packet!\n");
				return;
			}
			memcpy(packetFirst, &(packetInfo->Header), sizeof(PACKET_HEADER_INFORMATION));

			packetFirst->ImageBaseAddress = packetInfo->Payload.FirstPacket.ImageBaseAddress;
			packetFirst->ImageSize = packetInfo->Payload.FirstPacket.ImageSize;
			packetFirst->ProcessorFrequency = packetInfo->Payload.FirstPacket.ProcessorFrequency;


			printf_s("[INFO] Sequence %d CPU %d - first packet received. Image base %p Buffer Size %ul. TSC in ms %lld\n", packetFirst->Header.SequenceId,
				packetFirst->Header.CpuId, packetFirst->ImageBaseAddress, packetFirst->ImageSize, tscInSeconds);

			packet = (KAFKA_PACKET*)packetFirst;

		}
		else
		{
			char* iptBuffer = (char*)packetInfo->Payload.GenericPacket.BufferAddress;
			packetSize = sizeof(KAFKA_PACKET) - 1 + 10 * USN_PAGE_SIZE;
			packet = (KAFKA_PACKET*)calloc(1, packetSize);
			if (packet == NULL)
			{
				printf_s("[ERROR] Cannot allocate kafka packet!\n");
				return;
			}

			memcpy(packet, &(packetInfo->Header), sizeof(PACKET_HEADER_INFORMATION));
			memcpy(&(packet->Data), iptBuffer, 10 * USN_PAGE_SIZE);

			if (packetInfo->Header.Options.LastPacket)
			{
				printf_s("[INFO] Sequence %d - packet %ld  CPU %d Last Packet. Buffer address %p Buffer Size %lld TSC in ms %lld\n",
					packetInfo->Header.SequenceId, packetInfo->Header.PacketId, packetInfo->Header.CpuId,
					packetInfo->Payload.GenericPacket.BufferAddress, packetInfo->Payload.GenericPacket.BufferSize, 
					tscInSeconds);
			}
			else
			{
				printf_s("[INFO] Sequence %d - packet %ld  CPU %d. Buffer address %p Buffer Size %lld Tsc in ms %lld\n",
					packetInfo->Header.SequenceId, packetInfo->Header.PacketId, packetInfo->Header.CpuId,
					packetInfo->Payload.GenericPacket.BufferAddress, packetInfo->Payload.GenericPacket.BufferSize,
					tscInSeconds);
			}
		}

		
		status = KafkaSendMessage(
			gApplicationGlobals->KafkaConfig.KafkaHandler,
			gApplicationGlobals->KafkaConfig.KafkaTopicName,
			packet,
			packetSize
		);
		//if (status != CMC_STATUS_SUCCESS)
		//{
		//	printf_s("[ERROR] Could not send message to kafka!\n");
		//}
		//else
		//{
		//	printf_s("[INFO] Sent buffer %p with size %ud to Kafka!\n", iptBuffer, 10 * USN_PAGE_SIZE);
		//}
		
		/*fopen_s(
			&fileHandle,
			"ProcessorTrace",
			"a"
		);
		if (fileHandle == INVALID_HANDLE_VALUE || !fileHandle)
		{
			DebugBreak();
			return STATUS_ACCESS_VIOLATION;
		}

		for (int i = 0; i < 10 * USN_PAGE_SIZE; i++)
		{
			fprintf(fileHandle, "%c", (char*)packet[i]);
		}

		if (!FlushFileBuffers(fileHandle))
		{
			printf("Could not flush file buffers\n");
		}
		fclose(fileHandle);
		printf("Written bytes\n");*/


		status = CommandFreeBuffer(
			bufferId
		);
	}
	//DebugBreak();
	return 0;
}

NTSTATUS
CommandExit(
    _In_    PVOID   InParameter,
    _Out_   PVOID*  Result
)
{
	UNREFERENCED_PARAMETER(InParameter);
	UNREFERENCED_PARAMETER(Result);

	NTSTATUS status;

	status = KafkaUninit(
		gApplicationGlobals->KafkaConfig.KafkaHandler
	);
	if (status != CMC_STATUS_SUCCESS)
	{
		printf_s("[ERROR] Couldn't uninit Kafka!\n");
	}

	return status;
}
```

`IntelPTDriver/IntelPTUserModeApp/Commands.h`:

```h
#ifndef _COMMANDS_H_
#define _COMMANDS_H_
#include <Windows.h>

typedef 
NTSTATUS
(*COMMAND_METHOD)(
    PVOID   InParameter,
    PVOID* Result
    );

NTSTATUS
CommandTest(
    PVOID   InParameter,
    PVOID* Result
);

NTSTATUS
CommandQueryPtCapabilities(
    PVOID   InParameter,
    PVOID* Result
);

NTSTATUS
CommandSetupPt(
    PVOID   InParameter,
    PVOID* Result
);

NTSTATUS
CommandExit(
    PVOID   InParameter,
    PVOID* Result
);

#endif
```

`IntelPTDriver/IntelPTUserModeApp/Communication.c`:

```c
#include "Communication.h"
#include "Public.h"
#include <stdio.h>
#include "Globals.h"

NTSTATUS
CommunicationGetDriverHandle(
	HANDLE *Handle
)
{
	NTSTATUS status = CMC_STATUS_SUCCESS;
	if (gApplicationGlobals->Ipt.gDriverHandle == INVALID_HANDLE_VALUE)
	{
		gApplicationGlobals->Ipt.gDriverHandle = CreateFileW(
			SAMPLE_DEVICE_OPEN_NAME,
			GENERIC_READ | GENERIC_WRITE,
			FILE_SHARE_READ,
			NULL,
			OPEN_EXISTING,
			FILE_FLAG_OVERLAPPED,
			NULL);
		if (gApplicationGlobals->Ipt.gDriverHandle == INVALID_HANDLE_VALUE)
		{
			printf_s("Could not retrieve driver handle! Error %X\n", GetLastError());
			status = STATUS_INVALID_HANDLE;
		}
		printf_s("[INFO] New driver handle %p\n", gApplicationGlobals->Ipt.gDriverHandle);
	}

	*Handle = gApplicationGlobals->Ipt.gDriverHandle;

	return status;
}

NTSTATUS 
CommunicationSendMessage(
    _In_ COMMUNICATION_MESSAGE *Message, 
    _Inout_opt_ OVERLAPPED **ResponseAvailable
)
{
	NTSTATUS status;
	HANDLE driverHandle = NULL;
	OVERLAPPED* overlapped = NULL;

	overlapped = (OVERLAPPED*)malloc(sizeof(OVERLAPPED));
	if (!overlapped)
	{
		printf_s("Could not allocate overlapped structure\n");
		return STATUS_INTERRUPTED;
	}

	overlapped->hEvent = CreateEvent(NULL, TRUE, FALSE, NULL);
	if (overlapped->hEvent == NULL)
	{
		printf_s("Could not create event! Error %X\n", GetLastError());
		status = STATUS_INTERRUPTED;
		goto cleanup;
	}

	status = CommunicationGetDriverHandle(
		&driverHandle
	);
	if (!SUCCEEDED(status))
	{
		printf_s("CommunicationGetDriverHandle returned error!\n");
		goto cleanup;

	}

	BOOL controlStatus = DeviceIoControl(
		driverHandle,
		Message->MethodType,
		Message->DataIn,
		Message->DataInSize,
		Message->DataOut,
		Message->DataOutSize,
		Message->BytesWritten,
		overlapped);
	if (controlStatus && (GetLastError() != ERROR_IO_PENDING))
	{
		printf_s("DeviceIoControl failed with status %d\n", GetLastError());
	}

cleanup:

	if (ResponseAvailable && SUCCEEDED(status))
		*ResponseAvailable = overlapped;
	else if(ResponseAvailable && !SUCCEEDED(status))
		*ResponseAvailable = NULL;

	if (!SUCCEEDED(status) && overlapped)
		free(overlapped);


	return status;
}
```

`IntelPTDriver/IntelPTUserModeApp/Communication.h`:

```h
#ifndef _COMMUNICATION_H_
#define _COMMUNICATION_H_

#include <Windows.h>

typedef struct _COMMUNICATION_MESSAGE {

    DWORD MethodType;
    PVOID DataIn;
    DWORD DataInSize;
    PVOID DataOut;
    DWORD DataOutSize;
    DWORD* BytesWritten;

} COMMUNICATION_MESSAGE;

NTSTATUS
CommunicationSendMessage(
    _In_ COMMUNICATION_MESSAGE* Message,
    _Inout_opt_ OVERLAPPED** ResponseAvailable
);


#endif // !_COMMUNICATION_H_


```

`IntelPTDriver/IntelPTUserModeApp/Globals.h`:

```h
#include "Commands.h"
#include "Communication.h"
#include <stdio.h>
#include "Public.h"
#include "ProcessorTraceShared.h"
#include <fileapi.h>
#include "KafkaUtils.h"

typedef struct _IPT_CONFIG_STRUCT {
    short UseKafka;

    struct {
        char* BootstrapServer;
        char* KafkaTopicName;
        KAFKA_HANDLER KafkaHandler;
    } KafkaConfig;

    struct {

        HANDLE gDriverHandle;
        int NumberOfThreads;

    } Ipt;

} IPT_CONFIG_STRUCT;

IPT_CONFIG_STRUCT *gApplicationGlobals;
```

`IntelPTDriver/IntelPTUserModeApp/IntelPTUserModeApp.vcxproj`:

```vcxproj
<?xml version="1.0" encoding="utf-8"?>
<Project DefaultTargets="Build" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <ItemGroup Label="ProjectConfigurations">
    <ProjectConfiguration Include="Debug|Win32">
      <Configuration>Debug</Configuration>
      <Platform>Win32</Platform>
    </ProjectConfiguration>
    <ProjectConfiguration Include="Release|Win32">
      <Configuration>Release</Configuration>
      <Platform>Win32</Platform>
    </ProjectConfiguration>
    <ProjectConfiguration Include="Debug|x64">
      <Configuration>Debug</Configuration>
      <Platform>x64</Platform>
    </ProjectConfiguration>
    <ProjectConfiguration Include="Release|x64">
      <Configuration>Release</Configuration>
      <Platform>x64</Platform>
    </ProjectConfiguration>
  </ItemGroup>
  <PropertyGroup Label="Globals">
    <VCProjectVersion>16.0</VCProjectVersion>
    <Keyword>Win32Proj</Keyword>
    <ProjectGuid>{61979c7d-edf3-421d-b0e2-c282b7a0cb6d}</ProjectGuid>
    <RootNamespace>IntelPTUserModeApp</RootNamespace>
    <WindowsTargetPlatformVersion>10.0</WindowsTargetPlatformVersion>
  </PropertyGroup>
  <Import Project="$(VCTargetsPath)\Microsoft.Cpp.Default.props" />
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|Win32'" Label="Configuration">
    <ConfigurationType>Application</ConfigurationType>
    <UseDebugLibraries>true</UseDebugLibraries>
    <PlatformToolset>v142</PlatformToolset>
    <CharacterSet>Unicode</CharacterSet>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|Win32'" Label="Configuration">
    <ConfigurationType>Application</ConfigurationType>
    <UseDebugLibraries>false</UseDebugLibraries>
    <PlatformToolset>v142</PlatformToolset>
    <WholeProgramOptimization>true</WholeProgramOptimization>
    <CharacterSet>Unicode</CharacterSet>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|x64'" Label="Configuration">
    <ConfigurationType>Application</ConfigurationType>
    <UseDebugLibraries>true</UseDebugLibraries>
    <PlatformToolset>v142</PlatformToolset>
    <CharacterSet>Unicode</CharacterSet>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|x64'" Label="Configuration">
    <ConfigurationType>Application</ConfigurationType>
    <UseDebugLibraries>false</UseDebugLibraries>
    <PlatformToolset>v142</PlatformToolset>
    <WholeProgramOptimization>true</WholeProgramOptimization>
    <CharacterSet>Unicode</CharacterSet>
  </PropertyGroup>
  <Import Project="$(VCTargetsPath)\Microsoft.Cpp.props" />
  <ImportGroup Label="ExtensionSettings">
  </ImportGroup>
  <ImportGroup Label="Shared">
  </ImportGroup>
  <ImportGroup Label="PropertySheets" Condition="'$(Configuration)|$(Platform)'=='Debug|Win32'">
    <Import Project="$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props" Condition="exists('$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props')" Label="LocalAppDataPlatform" />
  </ImportGroup>
  <ImportGroup Label="PropertySheets" Condition="'$(Configuration)|$(Platform)'=='Release|Win32'">
    <Import Project="$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props" Condition="exists('$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props')" Label="LocalAppDataPlatform" />
  </ImportGroup>
  <ImportGroup Label="PropertySheets" Condition="'$(Configuration)|$(Platform)'=='Debug|x64'">
    <Import Project="$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props" Condition="exists('$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props')" Label="LocalAppDataPlatform" />
  </ImportGroup>
  <ImportGroup Label="PropertySheets" Condition="'$(Configuration)|$(Platform)'=='Release|x64'">
    <Import Project="$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props" Condition="exists('$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props')" Label="LocalAppDataPlatform" />
  </ImportGroup>
  <PropertyGroup Label="UserMacros" />
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|Win32'">
    <LinkIncremental>true</LinkIncremental>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|Win32'">
    <LinkIncremental>false</LinkIncremental>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|x64'">
    <LinkIncremental>true</LinkIncremental>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|x64'">
    <LinkIncremental>false</LinkIncremental>
  </PropertyGroup>
  <ItemDefinitionGroup Condition="'$(Configuration)|$(Platform)'=='Debug|Win32'">
    <ClCompile>
      <WarningLevel>Level3</WarningLevel>
      <SDLCheck>true</SDLCheck>
      <PreprocessorDefinitions>WIN32;_DEBUG;_CONSOLE;%(PreprocessorDefinitions)</PreprocessorDefinitions>
      <ConformanceMode>true</ConformanceMode>
    </ClCompile>
    <Link>
      <SubSystem>Console</SubSystem>
      <GenerateDebugInformation>true</GenerateDebugInformation>
    </Link>
  </ItemDefinitionGroup>
  <ItemDefinitionGroup Condition="'$(Configuration)|$(Platform)'=='Release|Win32'">
    <ClCompile>
      <WarningLevel>Level3</WarningLevel>
      <FunctionLevelLinking>true</FunctionLevelLinking>
      <IntrinsicFunctions>true</IntrinsicFunctions>
      <SDLCheck>true</SDLCheck>
      <PreprocessorDefinitions>WIN32;NDEBUG;_CONSOLE;%(PreprocessorDefinitions)</PreprocessorDefinitions>
      <ConformanceMode>true</ConformanceMode>
    </ClCompile>
    <Link>
      <SubSystem>Console</SubSystem>
      <EnableCOMDATFolding>true</EnableCOMDATFolding>
      <OptimizeReferences>true</OptimizeReferences>
      <GenerateDebugInformation>true</GenerateDebugInformation>
    </Link>
  </ItemDefinitionGroup>
  <ItemDefinitionGroup Condition="'$(Configuration)|$(Platform)'=='Debug|x64'">
    <ClCompile>
      <WarningLevel>Level3</WarningLevel>
      <SDLCheck>true</SDLCheck>
      <PreprocessorDefinitions>_DEBUG;_CONSOLE;%(PreprocessorDefinitions)</PreprocessorDefinitions>
      <ConformanceMode>true</ConformanceMode>
      <RuntimeLibrary>MultiThreadedDebug</RuntimeLibrary>
      <AdditionalIncludeDirectories>D:\disertatie\ingsoc\IntelPTDriver\IntelPTDriver;%(AdditionalIncludeDirectories)</AdditionalIncludeDirectories>
    </ClCompile>
    <Link>
      <SubSystem>Console</SubSystem>
      <GenerateDebugInformation>true</GenerateDebugInformation>
    </Link>
  </ItemDefinitionGroup>
  <ItemDefinitionGroup Condition="'$(Configuration)|$(Platform)'=='Release|x64'">
    <ClCompile>
      <WarningLevel>Level3</WarningLevel>
      <FunctionLevelLinking>true</FunctionLevelLinking>
      <IntrinsicFunctions>true</IntrinsicFunctions>
      <SDLCheck>true</SDLCheck>
      <PreprocessorDefinitions>NDEBUG;_CONSOLE;%(PreprocessorDefinitions)</PreprocessorDefinitions>
      <ConformanceMode>true</ConformanceMode>
      <RuntimeLibrary>MultiThreaded</RuntimeLibrary>
    </ClCompile>
    <Link>
      <SubSystem>Console</SubSystem>
      <EnableCOMDATFolding>true</EnableCOMDATFolding>
      <OptimizeReferences>true</OptimizeReferences>
      <GenerateDebugInformation>true</GenerateDebugInformation>
    </Link>
  </ItemDefinitionGroup>
  <ItemGroup>
    <ClCompile Include="Commands.c" />
    <ClCompile Include="Communication.c" />
    <ClCompile Include="Globals.h" />
    <ClCompile Include="KafkaUtils.c" />
    <ClCompile Include="Main.c" />
    <ClCompile Include="UserInterface.c" />
  </ItemGroup>
  <ItemGroup>
    <ClInclude Include="Commands.h" />
    <ClInclude Include="Communication.h" />
    <ClInclude Include="KafkaUtils.h" />
    <ClInclude Include="UserInterface.h" />
  </ItemGroup>
  <ItemGroup>
    <CopyFileToFolders Include="config.cfg">
      <ExcludedFromBuild Condition="'$(Configuration)|$(Platform)'=='Debug|x64'">false</ExcludedFromBuild>
      <FileType>Document</FileType>
    </CopyFileToFolders>
    <None Include="packages.config" />
  </ItemGroup>
  <Import Project="$(VCTargetsPath)\Microsoft.Cpp.targets" />
  <ImportGroup Label="ExtensionTargets">
    <Import Project="..\packages\librdkafka.redist.1.8.2\build\native\librdkafka.redist.targets" Condition="Exists('..\packages\librdkafka.redist.1.8.2\build\native\librdkafka.redist.targets')" />
  </ImportGroup>
  <Target Name="EnsureNuGetPackageBuildImports" BeforeTargets="PrepareForBuild">
    <PropertyGroup>
      <ErrorText>This project references NuGet package(s) that are missing on this computer. Use NuGet Package Restore to download them.  For more information, see http://go.microsoft.com/fwlink/?LinkID=322105. The missing file is {0}.</ErrorText>
    </PropertyGroup>
    <Error Condition="!Exists('..\packages\librdkafka.redist.1.8.2\build\native\librdkafka.redist.targets')" Text="$([System.String]::Format('$(ErrorText)', '..\packages\librdkafka.redist.1.8.2\build\native\librdkafka.redist.targets'))" />
  </Target>
</Project>
```

`IntelPTDriver/IntelPTUserModeApp/IntelPTUserModeApp.vcxproj.filters`:

```filters
<?xml version="1.0" encoding="utf-8"?>
<Project ToolsVersion="4.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <ItemGroup>
    <Filter Include="Source Files">
      <UniqueIdentifier>{4FC737F1-C7A5-4376-A066-2A32D752A2FF}</UniqueIdentifier>
      <Extensions>cpp;c;cc;cxx;c++;cppm;ixx;def;odl;idl;hpj;bat;asm;asmx</Extensions>
    </Filter>
    <Filter Include="Header Files">
      <UniqueIdentifier>{93995380-89BD-4b04-88EB-625FBE52EBFB}</UniqueIdentifier>
      <Extensions>h;hh;hpp;hxx;h++;hm;inl;inc;ipp;xsd</Extensions>
    </Filter>
    <Filter Include="Resource Files">
      <UniqueIdentifier>{67DA6AB6-F800-4c08-8B7A-83BB121AAD01}</UniqueIdentifier>
      <Extensions>rc;ico;cur;bmp;dlg;rc2;rct;bin;rgs;gif;jpg;jpeg;jpe;resx;tiff;tif;png;wav;mfcribbon-ms</Extensions>
    </Filter>
  </ItemGroup>
  <ItemGroup>
    <ClCompile Include="Main.c">
      <Filter>Source Files</Filter>
    </ClCompile>
    <ClCompile Include="Commands.c">
      <Filter>Source Files</Filter>
    </ClCompile>
    <ClCompile Include="UserInterface.c">
      <Filter>Source Files</Filter>
    </ClCompile>
    <ClCompile Include="Communication.c">
      <Filter>Source Files</Filter>
    </ClCompile>
    <ClCompile Include="KafkaUtils.c">
      <Filter>Source Files</Filter>
    </ClCompile>
    <ClCompile Include="Globals.h">
      <Filter>Header Files</Filter>
    </ClCompile>
  </ItemGroup>
  <ItemGroup>
    <ClInclude Include="Commands.h">
      <Filter>Header Files</Filter>
    </ClInclude>
    <ClInclude Include="UserInterface.h">
      <Filter>Header Files</Filter>
    </ClInclude>
    <ClInclude Include="Communication.h">
      <Filter>Header Files</Filter>
    </ClInclude>
    <ClInclude Include="KafkaUtils.h">
      <Filter>Header Files</Filter>
    </ClInclude>
  </ItemGroup>
  <ItemGroup>
    <None Include="packages.config" />
  </ItemGroup>
  <ItemGroup>
    <CopyFileToFolders Include="config.cfg">
      <Filter>Resource Files</Filter>
    </CopyFileToFolders>
  </ItemGroup>
</Project>
```

`IntelPTDriver/IntelPTUserModeApp/IntelPTUserModeApp.vcxproj.user`:

```user
<?xml version="1.0" encoding="utf-8"?>
<Project ToolsVersion="Current" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <PropertyGroup />
</Project>
```

`IntelPTDriver/IntelPTUserModeApp/JsonTools.c`:

```c



void
JsonTools()
{
    Json
}
```

`IntelPTDriver/IntelPTUserModeApp/KafkaUtils.c`:

```c
#include "librdkafka/rdkafka.h"
#include "KafkaUtils.h"

static volatile int gRun = 1;
char gErrstr[512];

// TODO: Support for multiple polling threads? 
HANDLE gPollingThreadHandle = NULL;


static void
dr_msg_cb(rd_kafka_t* rk, const rd_kafka_message_t* rkmessage, void* opaque) {
	if (rkmessage->err)
		fprintf(stderr, "%% Message delivery failed: %s\n",
			rd_kafka_err2str(rkmessage->err));
	else
		fprintf(stderr,
			"%% Message delivered (%zd bytes, "
			"partition %" PRId32 ")\n",
			rkmessage->len, rkmessage->partition);

	/* The rkmessage is destroyed automatically by librdkafka */
}

DWORD
WINAPI
PollKafkaThread(
    _In_ LPVOID lpParameter
)
{
    do {
        rd_kafka_t* rk = (rd_kafka_t*)lpParameter;
        rd_kafka_poll(rk, 0);
    } while (gRun);
    return 0;
}

NTSTATUS
StartKafkaPolling(
    KAFKA_HANDLER KafkaHandler
)
{
    if (gPollingThreadHandle != NULL)
        return CMC_STATUS_SUCCESS;

    gPollingThreadHandle = CreateThread(
        NULL,
        0,
        PollKafkaThread,
        (LPVOID)KafkaHandler,
        0,
        NULL
    );
    if (gPollingThreadHandle == INVALID_HANDLE_VALUE || !gPollingThreadHandle)
    {
        return STATUS_FATAL_APP_EXIT;
    }

    return CMC_STATUS_SUCCESS;
}

NTSTATUS
StopKafkaPolling()
{
    WaitForSingleObject(gPollingThreadHandle, INFINITE);
    return CMC_STATUS_SUCCESS;
}

NTSTATUS
KafkaInit(
	char *Brokers,
	KAFKA_HANDLER *KafkaHandler
)
{
	rd_kafka_t* rk;        /* Producer instance handle */
	rd_kafka_conf_t* conf; /* Temporary configuration object */

	if (!KafkaHandler)
		return STATUS_INVALID_PARAMETER;

	conf = rd_kafka_conf_new();

	if (rd_kafka_conf_set(conf, "bootstrap.servers", Brokers, gErrstr,
		sizeof(gErrstr)) != RD_KAFKA_CONF_OK) {
		fprintf(stderr, "%s\n",gErrstr);
		return STATUS_INTERRUPTED;
	}

	rd_kafka_conf_set_dr_msg_cb(conf, dr_msg_cb);

	rk = rd_kafka_new(RD_KAFKA_PRODUCER, conf, gErrstr, sizeof(gErrstr));
	if (!rk) {
		fprintf(stderr, "%% Failed to create new producer: %s\n",
            gErrstr);
		return STATUS_INTERRUPTED;
	}

    *KafkaHandler = rk;
	return CMC_STATUS_SUCCESS;
}

NTSTATUS
KafkaUninit(
    KAFKA_HANDLER KafkaHandler
)
{
    rd_kafka_t* rk = (rd_kafka_t*)KafkaHandler;

    /* Wait for final messages to be delivered or fail.
     * rd_kafka_flush() is an abstraction over rd_kafka_poll() which
     * waits for all messages to be delivered. */
    fprintf(stderr, "%% Flushing final messages..\n");
    StopKafkaPolling();
    rd_kafka_flush(rk, 10 * 1000 /* wait for max 10 seconds */);

    /* If the output queue is still not empty there is an issue
     * with producing messages to the clusters. */
    if (rd_kafka_outq_len(rk) > 0)
        fprintf(stderr, "%% %d message(s) were not delivered\n",
            rd_kafka_outq_len(rk));

    /* Destroy the producer instance */
    rd_kafka_destroy(rk);

    return CMC_STATUS_SUCCESS;
}

NTSTATUS
KafkaSendMessage(
    KAFKA_HANDLER KafkaHandler,
    char *Topic,
    BYTE *Buffer,
    unsigned BufferLength
)
{
    rd_kafka_resp_err_t err;
    rd_kafka_t* rk = (rd_kafka_t*)KafkaHandler;

    if(!gPollingThreadHandle)
        StartKafkaPolling(KafkaHandler);

retry:
    err = rd_kafka_producev(
        /* Producer handle */
        rk,
        /* Topic name */
        RD_KAFKA_V_TOPIC(Topic),
        /* Make a copy of the payload. */
        RD_KAFKA_V_MSGFLAGS(RD_KAFKA_MSG_F_COPY),
        /* Message value and length */
        RD_KAFKA_V_VALUE(Buffer, BufferLength),
        /* Per-Message opaque, provided in
         * delivery report callback as
         * msg_opaque. */
        RD_KAFKA_V_OPAQUE(NULL),
        /* End sentinel */
        RD_KAFKA_V_END);

    if (err) {
        /*
         * Failed to *enqueue* message for producing.
         */
        fprintf(stderr,
            "%% Failed to produce to topic %s: %s\n", Topic,
            rd_kafka_err2str(err));

        if (err == RD_KAFKA_RESP_ERR__QUEUE_FULL) {
            /* If the internal queue is full, wait for
             * messages to be delivered and then retry.
             * The internal queue represents both
             * messages to be sent and messages that have
             * been sent or failed, awaiting their
             * delivery report callback to be called.
             *
             * The internal queue is limited by the
             * configuration property
             * queue.buffering.max.messages */
            rd_kafka_poll(rk,
                1000 /*block for max 1000ms*/);
            
            // gotos are not * THE BEST * technical solution but for the moment it does the job.
            // also it is highly unlikely to get here during testing phase *HOPEFULLY* 

            goto retry;
        }
    }
    else {
        fprintf(stderr,
            "%% Enqueued message (%ud bytes) "
            "for topic %s\n",
            BufferLength, Topic);
    }

    return CMC_STATUS_SUCCESS;
}
```

`IntelPTDriver/IntelPTUserModeApp/KafkaUtils.h`:

```h
typedef PVOID KAFKA_HANDLER;

NTSTATUS
KafkaInit(
	char* Brokers,
	KAFKA_HANDLER* KafkaHandler
);

NTSTATUS
KafkaUninit(
	KAFKA_HANDLER KafkaHandler
);

NTSTATUS
KafkaSendMessage(
    KAFKA_HANDLER KafkaHandler,
    char* Topic,
    BYTE* Buffer,
    unsigned BufferLength
);
```

`IntelPTDriver/IntelPTUserModeApp/Main.c`:

```c
#define _CRT_SECURE_NO_WARNINGS

#include <stdio.h>
#include "librdkafka/rdkafka.h"

#include "Public.h"
#include "UserInterface.h"
#include "Globals.h"

char * 
TrimString(
    char* string
)
{
    while (*string == ' ' && *string != 0)
        string++;
    return string;
}

void
GatherConfigData(
    char *ConfigFilePath
)
{
    FILE *fs;

    int bufferLength = 255;
    char buffer[255];
    errno_t err;

    err = fopen_s(&fs, ConfigFilePath, "r");
    if (err != 0 || !fs)
    {
        printf_s("[ERROR] Could not open config file!\n");
        return;
    }
    
    while (fgets(buffer, bufferLength, fs)) {
        printf("%s\n", buffer);

        char *key = TrimString(strtok(buffer, "=\n"));
        char* value = TrimString(strtok(NULL, "=\n"));

        if (strstr(key, "bootstrap_server"))
        {
            char* cpyValue = malloc(sizeof(char) * strlen(value));
            if (cpyValue == NULL)
            {
                printf_s("[ERROR] Failed to allocate memory for saving config values\n");
                continue;
            }
            memcpy(cpyValue, value, strlen(value) + 1);
            gApplicationGlobals->KafkaConfig.BootstrapServer = cpyValue;
            printf_s("[INFO] Set kafka bootstrap server to %s\n", gApplicationGlobals->KafkaConfig.BootstrapServer);
            cpyValue = 0;
        }
        else if (strstr(key, "topic_name"))
        {
            char* cpyValue = malloc(sizeof(char) * strlen(value));
            if (cpyValue == NULL)
            {
                printf_s("[ERROR] Failed to allocate memory for saving config values\n");
                continue;
            }
            memcpy(cpyValue, value, strlen(value) + 1);
            gApplicationGlobals->KafkaConfig.KafkaTopicName = cpyValue;
            printf_s("[INFO] Set kafka topic to %s\n", gApplicationGlobals->KafkaConfig.KafkaTopicName);
            cpyValue = 0;
        }
    }

    fclose(fs);
}


int
main(
	int argc,
	char** argv
)
{
	UNREFERENCED_PARAMETER(argc);
	UNREFERENCED_PARAMETER(argv);

    gApplicationGlobals = calloc(1, sizeof(IPT_CONFIG_STRUCT));
    if (gApplicationGlobals == NULL)
    {
        printf_s("[ERROR] Could not allocate space for global variables");
        return 1;
    }

    gApplicationGlobals->Ipt.gDriverHandle = INVALID_HANDLE_VALUE;

    if (argc == 2)
    {
        GatherConfigData(argv[1]);
    }
    else
    {
        GatherConfigData("config.cfg");
    }

    while (1 == 1)
    {
        PrintHelp();
        InputCommand();
    }

    return 0;

}
```

`IntelPTDriver/IntelPTUserModeApp/UserInterface.c`:

```c
#include "UserInterface.h"
#include "Commands.h"
#include <stdio.h>
#include <Windows.h>
typedef enum _PARAMETER_TYPE {

    ParameterTypeInt,
    ParameterTypeString,
    ParameterTypeFloat

} PARAMETER_TYPE;


typedef struct _PARAMETER_STRUCTURE {

    PARAMETER_TYPE Type;
    const char* Explanation;

} PARAMETER_STRUCTURE;

typedef struct _COMMAND_DEFINITION_STRUCTURE {

    const char* CommandNameString;
    const char* HelpString;
    COMMAND_METHOD Method;
    PARAMETER_STRUCTURE* Parameters;

} COMMAND_DEFINITION_STRUCTURE;


COMMAND_DEFINITION_STRUCTURE Commands[] = {
    {
        "test",
        "performs a KM - UM communication test",
        CommandTest,
        NULL
    },
    {
        "query",
        "queries PT capabilities on all processors",
        CommandQueryPtCapabilities,
        NULL
    },
    {
        "setup",
        "configures PT mechanism on all processors",
        CommandSetupPt,
        NULL
    },
    {
        "help",
        "prints help",
        PrintHelp,
        NULL
    },
    {
        "exit",
        "exits the application",
        CommandExit,
        NULL
    }
};
#define NUMBER_OF_COMMANDS ((unsigned)_countof(Commands))


void
PrintHelp(
)
{
    printf_s("HELP:\n");
    for (DWORD currentCommand = 0; currentCommand < NUMBER_OF_COMMANDS; currentCommand++)
    {
        printf_s("\t%s -- %s\n", Commands[currentCommand].CommandNameString, Commands[currentCommand].HelpString);
    }
}

void
GetParameters(
    PARAMETER_STRUCTURE* parameterStructure,
    PVOID* outputParameters
)
{
    // TODO: Implement
}


void
InputCommand(
)
{
    char command[31];

    scanf_s("%s", command, (unsigned)_countof(command) - 1);

    for (DWORD currentCommand = 0; currentCommand < NUMBER_OF_COMMANDS; currentCommand++)
    {
        if (!strcmp(Commands[currentCommand].CommandNameString, command))
        {
            NTSTATUS status;
            PVOID params = NULL;
            PVOID output = NULL;
            if (Commands[currentCommand].Parameters)
            {
                GetParameters(
                    Commands[currentCommand].Parameters,
                    &params
                );
            }

            status = Commands[currentCommand].Method(
                params,
                &output
            );
            
            if (!SUCCEEDED(status))
            {
                printf_s("Method %s failed with status %d\n", Commands[currentCommand].CommandNameString, status);
            }

            // TODO: Is output necessary? If so, handle it

            return;

        }
    }

    printf_s("Unknown command\n");
    PrintHelp();

}

```

`IntelPTDriver/IntelPTUserModeApp/UserInterface.h`:

```h
#ifndef _USER_INTERFACE_H_
#define _USER_INTERFACE_H_

void
PrintHelp(
);

void
InputCommand(
);



#endif
```

`IntelPTDriver/IntelPTUserModeApp/config.cfg`:

```cfg
bootstrap_server = localhost:9092
topic_name = ana_are_mere
```

`IntelPTDriver/IntelPTUserModeApp/packages.config`:

```config
<?xml version="1.0" encoding="utf-8"?>
<packages>
  <package id="librdkafka.redist" version="1.8.2" targetFramework="native" />
</packages>
```

`IntelPTDriver/TraceDecoder/PacketInterpreter/IntelPacketDefinitions.py`:

```py
from PacketInterpreter.PacketHandlers import *


unused_packet_bytes = [0x00, 0xFF]

ip_compression_packet_size = {
    0 : 0,
    1 : 2,
    2 : 4,
    3 : 6,
    4 : 6,
    5 : None,
    6 : 8,
    7 : None
}

ptw_packet_size = {
    0 : 4,
    1 : 8,
    2 : None,
    3 : None
}

packet_definitions = [
    {
        "packet_name" : "TNT",
        "packet_header" : [lambda data: True if data & 1 == 0 and data > 0x02 else 0],
        "packet_length" : 1,
        "handle" : tnt_packet_handler
    },
    {
        "packet_name" : "TNT Extended",
        "packet_header" : ["00000010", "10100011"],
        "packet_length" : 8,
        "handle" : tnt_packet_handler
    },
    {
        "packet_name" : "TIP",
        "packet_header" : ["...01101"],
        "packet_length" : lambda header : len(header) + ip_compression_packet_size[((header[0] >> 5) & 0b0111)],
        "handle" : fup_tip_packet_handler
    },
    {
        "packet_name" : "TIP PGE",
        "packet_header" : ["...10001"],
        "packet_length" : lambda header : len(header) + ip_compression_packet_size[((header[0] >> 5) & 0b0111)],  
        "handle" : fup_tip_packet_handler
    },
    {
        "packet_name" : "TIP PGD",
        "packet_header" : ["...00001"],
        "packet_length" : lambda header : len(header) + ip_compression_packet_size[((header[0] >> 5) & 0b0111)],
        "handle" : fup_tip_packet_handler
    },
    {
        "packet_name" : "FUP",
        "packet_header" : ["...11101"],
        "packet_length" : lambda header : len(header) + ip_compression_packet_size[((header[0] >> 5) & 0b0111)],
        "handle" : fup_tip_packet_handler
    },
    {
        "packet_name" : "PIP",
        "packet_header" : ["00000010", "01000011"],
        "packet_length" : 8,
        "handle" : placeholder_function
    },
    {
        "packet_name" : "MODE",
        "packet_header" : ["10011001"],
        "packet_length" : 2,
        "handle" : mode_packet_handler
    },
    {
        "packet_name" : "TRACE STOP",
        "packet_header" : ["00000010", "10000011"],
        "packet_length" : 2,
        "handle" : placeholder_function
    },
    {
        "packet_name" : "CBR",
        "packet_header" : ["00000010", "00000011"],
        "packet_length" : 4,
        "handle" : cbr_packet_handler
    },
    {
        "packet_name" : "TSC",
        "packet_header" : ["00011001"],
        "packet_length" : 8,
        "handle" : tsc_packet_handler  
    },
    {
        "packet_name" : "MTC",
        "packet_header" : ["01011001"],
        "packet_length" : 2,
        "handle" : placeholder_function  
    },
    {
        "packet_name" : "TMA",
        "packet_header" : ["00000010", "01110011"],
        "packet_length" : 7,
        "handle" : placeholder_function  
    },
    # {
    #     "packet_name" : "CYC",
    #     "packet_header" : ["......11"],
    #     "repetitons" : 1,
    #     "handle" : placeholder_function
    # },
    {
        "packet_name" : "VMCS",
        "packet_header" : ["00000010", "11001000"],
        "packet_length" : 7,
        "handle" : placeholder_function
    },
    {
        "packet_name" : "OVF",
        "packet_header" : ["00000010", "11110011"],
        "packet_length" : 2,
        "handle" : placeholder_function
    },
    {
        "packet_name" : "PSB",
        "packet_header" : ["00000010", "10000010"] * 7,
        "packet_length" : 16,
        "handle" : psb_packet_handler  
    },
    {
        "packet_name" : "PSBEND",
        "packet_header" : ["00000010", "00100011"],
        "packet_length" : 2,
        "handle" : psbend_packet_handler  
    },
    {
        "packet_name" : "MNT",
        "packet_header" : ["00000010", "11000011", "10001000"],
        "packet_length" : 11,
        "handle" : placeholder_function  
    },
    {
        "packet_name" : "PTW",
        "packet_header" : ["00000010", "...10010"],
        "packet_length" : lambda header : len(header) + ptw_packet_size[((header[1] >> 5) & 0b011)],
        "handle" : placeholder_function
    },
    {
        "packet_name" : "EXSTOP",
        "packet_header" : ["00000010", ".1100010"],
        "packet_length" : 2,
        "handle" : placeholder_function  
    },
    {
        "packet_name" : "MWAIT",
        "packet_header" : ["00000010", "11000010"],
        "packet_length" : 10,
        "handle" : placeholder_function  
    },
    {
        "packet_name" : "PWRE",
        "packet_header" : ["00000010", "00100010"],
        "packet_length" : 4,
        "handle" : placeholder_function  
    },
    {
        "packet_name" : "PWRX",
        "packet_header" : ["00000010", "10100010"],
        "packet_length" : 7,
        "handle" : placeholder_function  
    },
    {
        "packet_name" : "BBP",
        "packet_header" : ["00000010", "01100011"],
        "packet_length" : 3,
        "handle" : placeholder_function  
    },
    # {
    #     "packet_name" : "BIP",
    #     "packet_header" : [".....100", "01100011"],
    #     "packet_length" : None,
    #     "handle" : placeholder_function  
    # },
    {
        "packet_name" : "BEP",
        "packet_header" : ["00000010", ".0110011"],
        "packet_length" : 2,
        "handle" : placeholder_function  
    }
]

```

`IntelPTDriver/TraceDecoder/PacketInterpreter/InternalPacketDefinitions.py`:

```py
# constant definitions
import json

PACKET_TNT_TAKEN = 1
PACKET_TNT_NOT_TAKEN = 2
PACKET_FUP = 3
PACKET_TIP = 4
PACKET_TIP_PGE = 5
PACKET_TIP_PGD = 6
PACKET_CBR = 7
PACKET_MODE = 8
PACKET_PSB = 9
PACKET_PSBEND = 10
PACKET_TSC = 11
PACKET_UNDEFINED = 12

PACKET_ID_TO_STRING = {
    PACKET_TNT_TAKEN: "TAKEN",
    PACKET_TNT_NOT_TAKEN: "NOT_TAKEN",
    PACKET_FUP: "FUP",
    PACKET_TIP: "TIP",
    PACKET_TIP_PGE: "TIP_PGE",
    PACKET_TIP_PGD: "TIP_PGD",
    PACKET_CBR: "CBR",
    PACKET_MODE: "MODE",
    PACKET_PSB : "PSB",
    PACKET_PSBEND : "PSBEND",
    PACKET_TSC : "TSC",
    PACKET_UNDEFINED : "UNDEFINED"
}

class PacketBase:
    def __init__(self, packet_type : int, tsc : int = None) -> None:
        self.packet_id = packet_type
        self.tsc = tsc

    def __str__(self) -> str:
        return json.dumps(self.__dict__, indent=4)

class PacketTnt(PacketBase):
    def __init__(self, taken : bool, tsc : int) -> None:
        super().__init__(PACKET_TNT_TAKEN if taken else PACKET_TNT_NOT_TAKEN, tsc)
    
class PacketTip(PacketBase):
    def __init__(self, packet_type: int = PACKET_TIP, tsc: int = None) -> None:
        super().__init__(packet_type, tsc)
        self.type = 0
        self.address = None

    # def __setattr__(self, __name: str, __value: int) -> None:
    #     if __name == "type":
    #         self.type = __value
    #     elif __name == "address":
    #         self.address = __value
    #     else:
    #         raise AttributeError(f"Attribute {__name} not available")

    def __getattr__(self, __name: str) -> int:
        if __name not in self.__dict__.keys():
            raise AttributeError(f"Attribute {__name} not available")
        else:
            return self.__dict__[__name]

class PacketTipPge(PacketTip):
    def __init__(self, tsc) -> None:
        super().__init__(PACKET_TIP_PGE, tsc)

class PacketTipPgd(PacketTip):
    def __init__(self, tsc) -> None:
        super().__init__(PACKET_TIP_PGD, tsc)

class PacketFup(PacketTip):
    def __init__(self, tsc) -> None:
        super().__init__(PACKET_FUP, tsc)

class PacketTsc(PacketBase):
    def __init__(self, data) -> None:
        super().__init__(PACKET_TSC)

        packet_data = data[1:][::-1]
        self.tsc = 0
        for d in packet_data:
            self.tsc += d
            self.tsc <<= 8
        self.tsc >>= 8
    def __getattr__(self, __name: str):
        if __name not in self.__dict__.keys():
            raise AttributeError("No such attribute")
        return self.__dict__[__name]    
class PacketCbr(PacketBase):
    def __init__(self, data, tsc) -> None:
        super().__init__(PACKET_CBR, tsc)

        self.cbr = data[0]

    def __getattr__(self, __name: str):
        if __name not in self.__dict__.keys():
            raise AttributeError("No such attribute")
        return self.__dict__[__name]    

class PacketMode(PacketBase):
    MODE_PACKET_TYPES = {
        0b000 : "Exec",
        0b001 : "TSX"
    }
    
    def __init__(self, data, tsc) -> None:
        super().__init__(PACKET_MODE, tsc)
        
        type = data >> 5 & 0b0111

        if type not in PacketMode.MODE_PACKET_TYPES.keys():
            raise Exception(f"Type {type} for mode packet not available!") 
        
        self.type = PacketMode.MODE_PACKET_TYPES[type]

        if self.type == "Exec":
            self.IF = (data & (1 << 2)) != 0
            self.CSD = (data & (1 << 1)) != 0
            self.CSL_LMA = (data & 1) != 0

            if self.CSD and self.CSL_LMA:
                raise Exception("Accordin to intel manual both CSD and CSL_LMA should not be both 1! Trace is inconsistent!")

        else:
            self.TXAbort = (data & (1 << 1)) != 0
            self.InTX = (data & 1) != 0

    def __getattr__(self, __name: str):
        if __name in self.__dict__.keys():
            return self.__dict__[__name]
        if __name == "opmode":
            if self.type == "Exec":
                if self.CSL_LMA:
                    return "64"
                if self.CSD:
                    return "32"
                return "16"
        raise AttributeError("No such attribute")

```

`IntelPTDriver/TraceDecoder/PacketInterpreter/PacketHandlers.py`:

```py
from PacketInterpreter.InternalPacketDefinitions import *

def placeholder_function(packet_data, packet_name, context):
    print(f"Intercepted {packet_name} with default handler")
    return PacketBase(PACKET_UNDEFINED)

def tsc_packet_handler(packet_data, packet_name, context):
    packet = PacketTsc(packet_data)
    
    context["tsc"] = packet.tsc
    return packet 

def psb_packet_handler(packet_data, packet_name, context):
    # print(f"Intercepted PSB")
    context["disable_interpreting"] = True
    return PacketBase(PACKET_PSB, context["tsc"])

def psbend_packet_handler(packet_data, packet_name, context):
    # print(f"Intercepted PSBEND")
    context["disable_interpreting"] = False
    return PacketBase(PACKET_PSBEND, context["tsc"])

def tnt_packet_handler(packet_data, packet_name, context) -> PacketBase:
    # TODO: Support large TNT packets

    jump_sequence = []

    aux_data = packet_data[0]
    idx = 0

    while (aux_data & 1 << 8) == 0 and idx < 8:
        idx += 1
        aux_data <<= 1
    
    idx += 1
    aux_data <<= 1

    while idx < 8:
        
        if aux_data & 1 << 8:
            jump_sequence.append(PacketTnt(True, context["tsc"]))
        else:
            jump_sequence.append(PacketTnt(False, context["tsc"]))

        idx += 1
        aux_data <<= 1

    return jump_sequence

LAST_IP_MASK = {
    0b001: 0xFFFFFFFFFFFF0000,
    0b010: 0xFFFFFFFF00000000,
    0b100: 0xFFFF000000000000
}

SIGN_EXTEND_MASK = {
    0 : 0x0000FFFFFFFFFFFF,
    1 : 0xFFFF000000000000
}

def fup_tip_packet_handler(packet_data, packet_name, context) -> PacketBase:
    if packet_name == "FUP":
        packet = PacketFup(tsc = context["tsc"])
    elif packet_name == "TIP":
        packet = PacketTip(tsc = context["tsc"])
    elif packet_name == "TIP PGE":
        packet = PacketTipPge(tsc = context["tsc"])
    elif packet_name == "TIP PGD":
        packet = PacketTipPgd(tsc = context["tsc"])
    else:
        raise Exception("Unknown packet name")

    payload_type = packet_data[0]
    payload_type = payload_type >> 5 & 0b0111
    packet.type = payload_type

    if payload_type == 0b000:
        return packet

    addr = 0
    for data in packet_data[1:][::-1]:
        addr += data
        addr <<= 8
    addr >>= 8
    if payload_type == 0b001 or payload_type == 0b010 or payload_type == 0b100:
        if "LastIP" not in context.keys():
            raise Exception("LastIP not available in context. Trace is inconsistent!")
        last_ip = context["LastIP"]

        addr = last_ip & LAST_IP_MASK[payload_type] | addr & (~LAST_IP_MASK[payload_type])
    
    elif payload_type == 0b011:
        sign_extension = 1 if (addr & 1 << 47) != 0 else 0
        addr = (addr | SIGN_EXTEND_MASK[sign_extension]) if sign_extension else (addr & SIGN_EXTEND_MASK[sign_extension])

    context["LastIP"] = addr
    packet.address = addr
    
    return packet


def mode_packet_handler(packet_data, packet_name, context):
    packet = PacketMode(packet_data[1], context["tsc"])
    if packet.type == "Exec":
        context["addressing_mode"] = packet.opmode
    return packet

def cbr_packet_handler(packet_data, packet_name, context):
    return PacketCbr(packet_data[2:], context["tsc"]) 

```

`IntelPTDriver/TraceDecoder/PacketInterpreter/PacketInterpreter.py`:

```py
from typing import Any
from PacketInterpreter.IntelPacketDefinitions import packet_definitions, unused_packet_bytes


class PacketInterpretor:
    
    def __init__(self, processor_frequency: int = None, image_base: int = None, image_size: int = None) -> None:

        # Counter for the current byte
        self.__internal_byte_idx = 0

        # Bytes accumulator for the [potential] current packet
        self.__accumulator = []
        # Possible packet types the current accumulated bytes could lead to
        self.__current_candidates = packet_definitions
        # Index of the current header byte
        self.__header_index = 0

        # Decoded packets up untill the current point
        self.__succession = []

        # A header has been matched with a packet definition
        self.__in_packet = False
        # The current matched packet
        self.__current_packet = None
        # Number of bytes in the current packet
        self.__current_packet_length = 0
        # Context for hanlders to maintain data inside
        self.__context = {"disable_interpreting": False, "tsc": None}

        self.__proc_frequency = processor_frequency
        self.__img_base =  image_base
        self.__img_size = image_size

    def __validate_header_byte(self, data_byte : int, header_definition : Any) -> bool:
        """
        Small bitwise regex implementation for matching header definition.\n
        Returns true if the pattern matches the data_byte.\n
        Supports custom functions as header_definition 
        Operations supported 1 0 .\n
        TODO: implement additional operations\n
        """

        if callable(header_definition):
            return header_definition(data_byte)

        header_definition = header_definition[::-1]

        for bit_index in range(len(header_definition)):
            if header_definition[bit_index] == '.':
                continue

            if header_definition[bit_index] == '1':
                if data_byte & (1 << bit_index) == 0:
                    return False
                else:
                    continue
            else:
                if data_byte & (1 << bit_index) != 0:
                    return False
                else:
                    continue
        return True

    def __search_header(self, byte : int):
        """
        Searches the byte in the list of potential packets header regex for a match.
        If more matches are available for a byte, the byte is accumulated until a single definitive match is found.\n

        When a packet is detected, it's length (either int or a method call is performed) to determine the number of bytes needed to be accumulated.
        All the accumulated bytes are sent to the handle and a packet definition is expected to be returned.\n

        The handle is provided with a context which can be used for storing / reading data and all the accumulated bytes\n 
        """
        # print(f"BYTE IDX {self.__internal_byte_idx} BYTE DATA {hex(byte)}")
        
        # List of new potential candidate apckets based on the current header matching
        new_candidates = []
        # Current candidate packet headers up untill the previous byte
        packet_candidates = self.__current_candidates

        # Itterate the list of candidates
        for packet_idx in range(len(packet_candidates)):

            # Cache packet and current header byte regex
            current_tested_packet = packet_candidates[packet_idx]
            packet_heders = current_tested_packet["packet_header"]

            # Test the current byte for a match with the current header byte regex
            if self.__validate_header_byte(byte, packet_heders[self.__header_index]):
                
                # If the entire header has been validated for a packet, mark it as the one in process
                if self.__header_index == len(packet_heders) - 1:

                    # Reset state
                    new_candidates = []
                    self.__header_index = 0
                    self.__in_packet = True
                    self.__current_packet = current_tested_packet
                    
                    # Get the length in bytes of the current packet
                    current_packet_length = current_tested_packet["packet_length"]
                    if callable(current_packet_length):
                        current_packet_length = current_packet_length(self.__accumulator)
                    self.__current_packet_length = current_packet_length

                    break

                # Otherwise the current packet is a potential candidate
                else:
                    new_candidates.append(current_tested_packet)
        
        # If a packet hasn't been decided upon, check the next header in the packet definition and
        # mark the next round of candidates
        if not self.__in_packet:
            self.__header_index += 1
            self.__current_candidates = new_candidates
    
    def process_byte(self, byte):
        """
        Entry point for the state machine. Receives IPT bytes as a stream, internally detects, buffers and tracks packets.\n
        """

        # Increment internal byte count
        self.__internal_byte_idx += 1

        # If no packet detected try detecting one using the current byte
        if not self.__in_packet:
            # If a restricted value was given whilst trying to detect a header, obviously the trace is flawed.
            # Acknowledge the error
            if byte in unused_packet_bytes:
                if len(self.__current_candidates) != len(packet_definitions) and len(self.__current_candidates) != 0:
                    print(f"[ERROR] while decoding bytes {self.__accumulator} detected inconsistent byte {byte} at index {self.__internal_byte_idx}")
                    self.__current_candidates = packet_definitions
                    self.__accumulator = []
                    self.__header_index = 0
                return

            # Buffer the current byte and try using it for header detection
            self.__accumulator.append(byte)
            self.__search_header(byte)
        else:
            # If a packet is currently buffering, add the new byte as data for this packet
            self.__accumulator.append(byte)
        
        # If the packet is fully buffered, process it and start detecting the next one
        if self.__current_packet is not None and len(self.__accumulator) >= self.__current_packet_length:
            
            packet_name = self.__current_packet["packet_name"]

            # Track the current packet
            # self.__succession.append([packet_name,  [hex(el) for el in self.__accumulator]])
            
            # Convert the packet from bytes to relevant internal data
            if "handle" in self.__current_packet.keys() and self.__current_packet["handle"] is not None:
                result = self.__current_packet["handle"](self.__accumulator, packet_name, self.__context)
            else:
                print(f"[WARNING] Packet type {packet_name} has no handle!")
            
            if self.__context["disable_interpreting"] is False:
                if isinstance(result, list):
                    self.__succession.extend(result)
                else:
                    self.__succession.append(result)
            else:
                print(f"dropped packet {packet_name} due to interpreter being disabled!")
            
            # Reset internal state
            self.__in_packet = False
            self.__accumulator = []
            self.__current_candidates = packet_definitions
            self.__header_index = 0

    def get_succession(self) -> list:
        aux = self.__succession
        self.__succession = []
        return aux

```

`IntelPTDriver/TraceDecoder/TraceDecoder.py`:

```py
from cProfile import label
import sys
import matplotlib 
matplotlib.use('Agg')
import matplotlib.pyplot as plt
from sympy import sequence
from PacketInterpreter.PacketInterpreter import PacketInterpretor
from PacketInterpreter.InternalPacketDefinitions import PACKET_ID_TO_STRING
import json
from kafka import KafkaConsumer
from struct import *
import numpy as np
from sklearn.neighbors.kde import KernelDensity
import os
from PacketInterpreter.IntelPacketDefinitions import *
from sklearn.cluster import KMeans
import matplotlib.ticker as ticker

PACKET_HEADER_STRUCT_CONTROL_SUM = "=I" # unsigned HeaderSize

PACKET_HEADER_STRUCT = ("=I"  # unsigned HeaderSize                          4
                        "Q"  # unsigned long long PacketId                  8
                        "I"  # unsigned CpuId                               4
                        "I"  # unsigned SequenceId                          4
                        "H") # struct PacketInformation                     2
                             # {
                             #     short FirstPacket : 1;
                             #     short LastPacket : 1;
                             #     short ExecutableTrace : 1;
                             # } Options;

PACKET_PAYLOAD_STRUCT_FIRST = ( "=Q"     # PVOID ImageBaseAddress
                                "L"     # ULONG ImageSize
                                "Q")    # ULONGLONG ProcessorFrequency

PACKET_HEADER_STRUCT_SIZE               = calcsize(PACKET_HEADER_STRUCT)
PACKET_HEADER_STRUCT_CONTROL_SUM_SIZE   = calcsize(PACKET_HEADER_STRUCT_CONTROL_SUM)

PACKET_FIRST_MASK   = 0b001
PACKET_LAST_MASK    = 0b010

SEQUENCE_INTERPRETERS = {}
PACKET_ORDER_LISTS = {}


def parser(bytes, interpretor : PacketInterpretor, sequence_id: int , packet_id: int, cpu_id: int):
    if sequence_id not in PACKET_ORDER_LISTS.keys():
        PACKET_ORDER_LISTS[sequence_id] = {"last_packet_id": 0, "out_of_order_packets" : []}

    if packet_id != PACKET_ORDER_LISTS[sequence_id]["last_packet_id"] + 1:
        print(f"[INFO] Out of order packet received. Las packet ID {PACKET_ORDER_LISTS[sequence_id]['last_packet_id']}. Current packet ID {packet_id}. Caching packet...")
        PACKET_ORDER_LISTS[sequence_id]["out_of_order_packets"].append((bytes, packet_id, cpu_id))
        PACKET_ORDER_LISTS[sequence_id]["out_of_order_packets"].sort(key = lambda x: x[1])


    for byte in bytes:
        interpretor.process_byte(byte)

    PACKET_ORDER_LISTS[sequence_id]["last_packet_id"] = packet_id
    while   PACKET_ORDER_LISTS[sequence_id]["out_of_order_packets"] is not None and\
            len(PACKET_ORDER_LISTS[sequence_id]["out_of_order_packets"]) >= 1 and\
            PACKET_ORDER_LISTS[sequence_id]["out_of_order_packets"][0] == PACKET_ORDER_LISTS[sequence_id]["last_packet_id"] + 1:
        
        bytes = PACKET_ORDER_LISTS[sequence_id]["out_of_order_packets"][0][0]
        for byte in bytes:
            interpretor.process_byte(byte)
        
        PACKET_ORDER_LISTS[sequence_id]["last_packet_id"] = PACKET_ORDER_LISTS[sequence_id]["out_of_order_packets"][0][1]
        PACKET_ORDER_LISTS[sequence_id]["out_of_order_packets"] = PACKET_ORDER_LISTS[sequence_id]["out_of_order_packets"][1:]


def read_trace():
    interpretor = PacketInterpretor()
    gCrtPlt = None
    base_image_address = 0

    topic_name = ""
    bootstrap_server = ""

    with open(os.path.dirname(os.path.realpath(__file__)) + "\\config.json", "r") as f:
        config_json = json.load(f)
        topic_name = config_json["topic"]
        bootstrap_server = config_json["bootstrap_server"]
    
    consumer = KafkaConsumer(topic_name, bootstrap_servers = bootstrap_server, auto_offset_reset = "earliest")
    packets = {}
    ips = []
    first_tsc = -1
    last_tsc = -1
    packets_by_time = {}

    for msg in consumer:
        
        msg = msg.value

        # Check the message control size against the defined header size
        control_sum_bytes = msg[:PACKET_HEADER_STRUCT_CONTROL_SUM_SIZE]
        control_sum = unpack(PACKET_HEADER_STRUCT_CONTROL_SUM, control_sum_bytes)[0]
        if control_sum != PACKET_HEADER_STRUCT_SIZE:
            print(f"[ERROR] Message header has size {control_sum}. Declared message header should have {PACKET_HEADER_STRUCT_SIZE}. The client/server structures are not in sync!")
            continue
    
        header_bytes = msg[:PACKET_HEADER_STRUCT_SIZE]
        header_values = unpack(PACKET_HEADER_STRUCT, header_bytes)
        
        # header_size = header_values[0]
        packet_id_number = header_values[1]
        cpu_id = header_values[2]
        sequence_id = header_values[3]
        options = header_values[4] 
        print(f"SEQUENCE {sequence_id} PACKET {packet_id_number} CPU ID {cpu_id}")

        packet_type = "FIRST" if (options & PACKET_FIRST_MASK) != 0 else ("LAST" if (options & PACKET_LAST_MASK) != 0 else "DATA")

        if packet_type == "FIRST":
            
            first_packet_values = unpack(PACKET_PAYLOAD_STRUCT_FIRST, msg[PACKET_HEADER_STRUCT_SIZE:])
            
            image_base_address  = first_packet_values[0]
            image_size          = first_packet_values[1]
            processor_frequency = first_packet_values[2]

            if sequence_id in SEQUENCE_INTERPRETERS.keys():
                print(f"[WARNING] An interpreter for sequence ID {sequence_id} already exists. Overwriting...")
            SEQUENCE_INTERPRETERS[sequence_id] = PacketInterpretor(processor_frequency, image_base_address, image_size)
            PACKET_ORDER_LISTS[sequence_id] = {"last_packet_id": 0, "out_of_order_packets" : []}

            # print(json.dumps(interpretor.get_succession(), indent = 4))
            packets = {}
            ips = []
            first_tsc = -1
            last_tsc = -1
            packets_by_time = {}

            print(f"Image base address {image_base_address} image size {image_size} processor frequency {processor_frequency}")
        else:
            parser(msg[PACKET_HEADER_STRUCT_SIZE:], interpretor, sequence_id, packet_id_number, cpu_id)


        for el in interpretor.get_succession():
            # print(str(el))
            if el.packet_id not in [None]:
                if PACKET_ID_TO_STRING[el.packet_id] in packets.keys():
                    packets[PACKET_ID_TO_STRING[el.packet_id]] += 1
                else:
                    packets[PACKET_ID_TO_STRING[el.packet_id]] = 1

            if el.packet_id in [PACKET_FUP, PACKET_TIP, PACKET_TIP_PGD, PACKET_TIP_PGE]:
                addr = el.address
                if addr is None:
                    continue
                ips.append(addr / (1024 * 1024))

            if el.packet_id is PACKET_TSC:
                if first_tsc == -1:
                    first_tsc = el.tsc
                last_tsc = el.tsc

            if el.tsc is not None:
                time_in_sec_relative_to_trace_start = ((el.tsc) / processor_frequency)
            else:
               time_in_sec_relative_to_trace_start = 0 
            if el.tsc < first_tsc:
                print("akscjhcgvas")
                pass

            if time_in_sec_relative_to_trace_start not in packets_by_time.keys():
                packets_by_time[time_in_sec_relative_to_trace_start] = {}
            
            if PACKET_ID_TO_STRING[el.packet_id] not in packets_by_time[time_in_sec_relative_to_trace_start].keys():
                packets_by_time[time_in_sec_relative_to_trace_start][PACKET_ID_TO_STRING[el.packet_id]] = 0

            packets_by_time[time_in_sec_relative_to_trace_start][PACKET_ID_TO_STRING[el.packet_id]] += 1

        if gCrtPlt is not None:
            plt.close(gCrtPlt)
        
        # print(json.dumps(packets, indent=4))  
        # print(json.dumps(packets_by_time, indent=4))  
        print(f"TRACED TIME {((last_tsc - first_tsc) / processor_frequency)/1024} SECONDS")

        gCrtPlt, axis = plt.subplots(2, 4)

        plt.subplots_adjust(left=0.1,
                            bottom=0.05, 
                            right=0.9, 
                            top=0.95, 
                            wspace=0.4, 
                            hspace=0.4)

        gCrtPlt.set_size_inches(20, 12.5, forward=True)
        
        # For Sine Function
        axis[0][0].bar(packets.keys(), packets.values())
        axis[0][0].set_title("Packet distribution")
        axis[0][0].tick_params(labelrotation=85)
        axis[0][0].set_xlabel("Packet type")
        axis[0][0].set_ylabel("Number of packets")

        if ips != []:
            x = np.array(ips).reshape(-1, 1)
            y = np.zeros_like(ips)
            
            axis[0][1].scatter(x, y, marker = "o")

        axis[0][1].set_title("Address distribution")
        axis[0][1].tick_params(labelrotation=45)
        axis[0][1].set_xlabel("RAM Megabyte")


        k = 2
        for packet_id in ["TIP_PGE", "TIP_PGD", "TAKEN", "NOT_TAKEN", "FUP", "TIP"]:
            au = [(packets_by_time[el][packet_id] if packet_id in packets_by_time[el].keys() else 0) for el in packets_by_time.keys()]
            axis[k // 4][k % 4].plot(list(packets_by_time.keys()), au)
            axis[k // 4][k % 4].tick_params(labelrotation=45)
            axis[k // 4][k % 4].set_title(f"{packet_id} per time")
            axis[k // 4][k % 4].set_xlabel("System runtime in ms")
            axis[k // 4][k % 4].set_ylabel("Number of packets")
            if first_tsc != last_tsc:
                axis[k // 4][k % 4].xaxis.set_major_locator(ticker.LinearLocator(numticks = 10))
            k += 1
        
        # plt.pause(0.5)
        save_path = f"D:\\disertatie\\ingsoc\\IntelPTDriver\\trace_figs\\{topic_name}"
        if not os.path.exists(save_path):
            os.makedirs(save_path)
        plt.savefig(fname = save_path + f"\\{sequence_id}_{packet_id_number}.png", format = "png")
        plt.close(gCrtPlt)

        if packet_type == "LAST":
            pass


def print_help():
    print("TraceDecoder.py <trace_file_path>")

def main():
    read_trace()
    return


if __name__ == "__main__":
    main()
```

`IntelPTDriver/TraceDecoder/config.json`:

```json
{
    "topic" : "ana_are_mere",
    "bootstrap_server" : "localhost:9092"
}
```

`IntelPTDriver/TracedApp/Main.c`:

```c
/*
    Bind socket to port 8888 on localhost
*/
#include<io.h>
#include<stdio.h>
#include<winsock2.h>

#pragma comment(lib,"ws2_32.lib") //Winsock Library

int main(int argc, char* argv[])
{
    //WSADATA wsa;
    //SOCKET s, new_socket;
    //struct sockaddr_in server, client;
    //int c;
    //char* message;

    //printf("\nInitialising Winsock...");
    //if (WSAStartup(MAKEWORD(2, 2), &wsa) != 0)
    //{
    //    printf("Failed. Error Code : %d", WSAGetLastError());
    //    return 1;
    //}

    //printf("Initialised.\n");

    ////Create a socket
    //if ((s = socket(AF_INET, SOCK_STREAM, 0)) == INVALID_SOCKET)
    //{
    //    printf("Could not create socket : %d", WSAGetLastError());
    //}

    //printf("Socket created.\n");

    ////Prepare the sockaddr_in structure
    //server.sin_family = AF_INET;
    //server.sin_addr.s_addr = INADDR_ANY;
    //server.sin_port = htons(8888);

    ////Bind
    //if (bind(s, (struct sockaddr*)&server, sizeof(server)) == SOCKET_ERROR)
    //{
    //    printf("Bind failed with error code : %d", WSAGetLastError());
    //}

    //puts("Bind done");

    ////Listen to incoming connections
    //listen(s, 3);

    //for (int i = 0; i < 10; i++)
    //{
    //    //Accept and incoming connection
    //    puts("Waiting for incoming connections...");
    //    c = sizeof(struct sockaddr_in);
    //    new_socket = accept(s, (struct sockaddr*)&client, &c);
    //    if (new_socket == INVALID_SOCKET)
    //    {
    //        printf("accept failed with error code : %d", WSAGetLastError());
    //    }

    //    puts("Connection accepted");

    //    //Reply to client
    //    message = "GET /chat HTTP/1.1 \
    //    Host: example.com : 8888 \
    //    Upgrade : websocket \
    //    Connection : Upgrade \
    //    ";
    //    send(new_socket, message, strlen(message), 0);

    //    getchar();
    //}
    //closesocket(s);
    //WSACleanup();
    LARGE_INTEGER freq;
    QueryPerformanceFrequency(
        &freq
    );

    long long tscsec = (__rdtsc() / freq.QuadPart);

    printf_s("[+] Starting exploit at TSC %lld TSC in ms %lld\n", __rdtsc(), tscsec);
    system("c:\\Python27\\python.exe c:\\Users\\test\\Desktop\\xplt.py ");

    return 0;
}
```

`IntelPTDriver/TracedApp/TracedApp.vcxproj`:

```vcxproj
<?xml version="1.0" encoding="utf-8"?>
<Project DefaultTargets="Build" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <ItemGroup Label="ProjectConfigurations">
    <ProjectConfiguration Include="Debug|Win32">
      <Configuration>Debug</Configuration>
      <Platform>Win32</Platform>
    </ProjectConfiguration>
    <ProjectConfiguration Include="Release|Win32">
      <Configuration>Release</Configuration>
      <Platform>Win32</Platform>
    </ProjectConfiguration>
    <ProjectConfiguration Include="Debug|x64">
      <Configuration>Debug</Configuration>
      <Platform>x64</Platform>
    </ProjectConfiguration>
    <ProjectConfiguration Include="Release|x64">
      <Configuration>Release</Configuration>
      <Platform>x64</Platform>
    </ProjectConfiguration>
  </ItemGroup>
  <PropertyGroup Label="Globals">
    <VCProjectVersion>16.0</VCProjectVersion>
    <Keyword>Win32Proj</Keyword>
    <ProjectGuid>{f8d13a4f-146c-489b-b5ea-5b4bccfe3dc2}</ProjectGuid>
    <RootNamespace>TracedApp</RootNamespace>
    <WindowsTargetPlatformVersion>10.0</WindowsTargetPlatformVersion>
  </PropertyGroup>
  <Import Project="$(VCTargetsPath)\Microsoft.Cpp.Default.props" />
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|Win32'" Label="Configuration">
    <ConfigurationType>Application</ConfigurationType>
    <UseDebugLibraries>true</UseDebugLibraries>
    <PlatformToolset>v142</PlatformToolset>
    <CharacterSet>Unicode</CharacterSet>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|Win32'" Label="Configuration">
    <ConfigurationType>Application</ConfigurationType>
    <UseDebugLibraries>false</UseDebugLibraries>
    <PlatformToolset>v142</PlatformToolset>
    <WholeProgramOptimization>true</WholeProgramOptimization>
    <CharacterSet>Unicode</CharacterSet>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|x64'" Label="Configuration">
    <ConfigurationType>Application</ConfigurationType>
    <UseDebugLibraries>true</UseDebugLibraries>
    <PlatformToolset>v142</PlatformToolset>
    <CharacterSet>Unicode</CharacterSet>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|x64'" Label="Configuration">
    <ConfigurationType>Application</ConfigurationType>
    <UseDebugLibraries>false</UseDebugLibraries>
    <PlatformToolset>v142</PlatformToolset>
    <WholeProgramOptimization>true</WholeProgramOptimization>
    <CharacterSet>Unicode</CharacterSet>
  </PropertyGroup>
  <Import Project="$(VCTargetsPath)\Microsoft.Cpp.props" />
  <ImportGroup Label="ExtensionSettings">
  </ImportGroup>
  <ImportGroup Label="Shared">
  </ImportGroup>
  <ImportGroup Label="PropertySheets" Condition="'$(Configuration)|$(Platform)'=='Debug|Win32'">
    <Import Project="$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props" Condition="exists('$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props')" Label="LocalAppDataPlatform" />
  </ImportGroup>
  <ImportGroup Label="PropertySheets" Condition="'$(Configuration)|$(Platform)'=='Release|Win32'">
    <Import Project="$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props" Condition="exists('$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props')" Label="LocalAppDataPlatform" />
  </ImportGroup>
  <ImportGroup Label="PropertySheets" Condition="'$(Configuration)|$(Platform)'=='Debug|x64'">
    <Import Project="$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props" Condition="exists('$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props')" Label="LocalAppDataPlatform" />
  </ImportGroup>
  <ImportGroup Label="PropertySheets" Condition="'$(Configuration)|$(Platform)'=='Release|x64'">
    <Import Project="$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props" Condition="exists('$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props')" Label="LocalAppDataPlatform" />
  </ImportGroup>
  <PropertyGroup Label="UserMacros" />
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|Win32'">
    <LinkIncremental>true</LinkIncremental>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|Win32'">
    <LinkIncremental>false</LinkIncremental>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Debug|x64'">
    <LinkIncremental>true</LinkIncremental>
  </PropertyGroup>
  <PropertyGroup Condition="'$(Configuration)|$(Platform)'=='Release|x64'">
    <LinkIncremental>false</LinkIncremental>
  </PropertyGroup>
  <ItemDefinitionGroup Condition="'$(Configuration)|$(Platform)'=='Debug|Win32'">
    <ClCompile>
      <WarningLevel>Level3</WarningLevel>
      <SDLCheck>true</SDLCheck>
      <PreprocessorDefinitions>WIN32;_DEBUG;_CONSOLE;%(PreprocessorDefinitions)</PreprocessorDefinitions>
      <ConformanceMode>true</ConformanceMode>
    </ClCompile>
    <Link>
      <SubSystem>Console</SubSystem>
      <GenerateDebugInformation>true</GenerateDebugInformation>
    </Link>
  </ItemDefinitionGroup>
  <ItemDefinitionGroup Condition="'$(Configuration)|$(Platform)'=='Release|Win32'">
    <ClCompile>
      <WarningLevel>Level3</WarningLevel>
      <FunctionLevelLinking>true</FunctionLevelLinking>
      <IntrinsicFunctions>true</IntrinsicFunctions>
      <SDLCheck>true</SDLCheck>
      <PreprocessorDefinitions>WIN32;NDEBUG;_CONSOLE;%(PreprocessorDefinitions)</PreprocessorDefinitions>
      <ConformanceMode>true</ConformanceMode>
    </ClCompile>
    <Link>
      <SubSystem>Console</SubSystem>
      <EnableCOMDATFolding>true</EnableCOMDATFolding>
      <OptimizeReferences>true</OptimizeReferences>
      <GenerateDebugInformation>true</GenerateDebugInformation>
    </Link>
  </ItemDefinitionGroup>
  <ItemDefinitionGroup Condition="'$(Configuration)|$(Platform)'=='Debug|x64'">
    <ClCompile>
      <WarningLevel>Level1</WarningLevel>
      <SDLCheck>true</SDLCheck>
      <PreprocessorDefinitions>_DEBUG;_CONSOLE;%(PreprocessorDefinitions)</PreprocessorDefinitions>
      <ConformanceMode>true</ConformanceMode>
      <RuntimeLibrary>MultiThreaded</RuntimeLibrary>
    </ClCompile>
    <Link>
      <SubSystem>Console</SubSystem>
      <GenerateDebugInformation>true</GenerateDebugInformation>
    </Link>
  </ItemDefinitionGroup>
  <ItemDefinitionGroup Condition="'$(Configuration)|$(Platform)'=='Release|x64'">
    <ClCompile>
      <WarningLevel>Level3</WarningLevel>
      <FunctionLevelLinking>true</FunctionLevelLinking>
      <IntrinsicFunctions>true</IntrinsicFunctions>
      <SDLCheck>true</SDLCheck>
      <PreprocessorDefinitions>NDEBUG;_CONSOLE;%(PreprocessorDefinitions)</PreprocessorDefinitions>
      <ConformanceMode>true</ConformanceMode>
    </ClCompile>
    <Link>
      <SubSystem>Console</SubSystem>
      <EnableCOMDATFolding>true</EnableCOMDATFolding>
      <OptimizeReferences>true</OptimizeReferences>
      <GenerateDebugInformation>true</GenerateDebugInformation>
    </Link>
  </ItemDefinitionGroup>
  <ItemGroup>
    <ClCompile Include="Main.c" />
  </ItemGroup>
  <Import Project="$(VCTargetsPath)\Microsoft.Cpp.targets" />
  <ImportGroup Label="ExtensionTargets">
  </ImportGroup>
</Project>
```

`IntelPTDriver/TracedApp/TracedApp.vcxproj.filters`:

```filters
<?xml version="1.0" encoding="utf-8"?>
<Project ToolsVersion="4.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <ItemGroup>
    <Filter Include="Source Files">
      <UniqueIdentifier>{4FC737F1-C7A5-4376-A066-2A32D752A2FF}</UniqueIdentifier>
      <Extensions>cpp;c;cc;cxx;c++;cppm;ixx;def;odl;idl;hpj;bat;asm;asmx</Extensions>
    </Filter>
    <Filter Include="Header Files">
      <UniqueIdentifier>{93995380-89BD-4b04-88EB-625FBE52EBFB}</UniqueIdentifier>
      <Extensions>h;hh;hpp;hxx;h++;hm;inl;inc;ipp;xsd</Extensions>
    </Filter>
    <Filter Include="Resource Files">
      <UniqueIdentifier>{67DA6AB6-F800-4c08-8B7A-83BB121AAD01}</UniqueIdentifier>
      <Extensions>rc;ico;cur;bmp;dlg;rc2;rct;bin;rgs;gif;jpg;jpeg;jpe;resx;tiff;tif;png;wav;mfcribbon-ms</Extensions>
    </Filter>
  </ItemGroup>
  <ItemGroup>
    <ClCompile Include="Main.c">
      <Filter>Source Files</Filter>
    </ClCompile>
  </ItemGroup>
</Project>
```

`IntelPTDriver/TracedApp/TracedApp.vcxproj.user`:

```user
<?xml version="1.0" encoding="utf-8"?>
<Project ToolsVersion="Current" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <PropertyGroup />
</Project>
```

`IntelPTDriver/packages/librdkafka.redist.1.8.2/CONFIGURATION.md`:

```md
# Configuration properties
## Global configuration properties

Property                                 | C/P | Range           |       Default | Importance | Description              
-----------------------------------------|-----|-----------------|--------------:|------------| --------------------------
builtin.features                         |  *  |                 | gzip, snappy, ssl, sasl, regex, lz4, sasl_gssapi, sasl_plain, sasl_scram, plugins, zstd, sasl_oauthbearer | low        | Indicates the builtin features for this build of librdkafka. An application can either query this value or attempt to set it with its list of required features to check for library support. <br>*Type: CSV flags*
client.id                                |  *  |                 |       rdkafka | low        | Client identifier. <br>*Type: string*
metadata.broker.list                     |  *  |                 |               | high       | Initial list of brokers as a CSV list of broker host or host:port. The application may also use `rd_kafka_brokers_add()` to add brokers during runtime. <br>*Type: string*
bootstrap.servers                        |  *  |                 |               | high       | Alias for `metadata.broker.list`: Initial list of brokers as a CSV list of broker host or host:port. The application may also use `rd_kafka_brokers_add()` to add brokers during runtime. <br>*Type: string*
message.max.bytes                        |  *  | 1000 .. 1000000000 |       1000000 | medium     | Maximum Kafka protocol request message size. Due to differing framing overhead between protocol versions the producer is unable to reliably enforce a strict max message limit at produce time and may exceed the maximum size by one message in protocol ProduceRequests, the broker will enforce the the topic's `max.message.bytes` limit (see Apache Kafka documentation). <br>*Type: integer*
message.copy.max.bytes                   |  *  | 0 .. 1000000000 |         65535 | low        | Maximum size for message to be copied to buffer. Messages larger than this will be passed by reference (zero-copy) at the expense of larger iovecs. <br>*Type: integer*
receive.message.max.bytes                |  *  | 1000 .. 2147483647 |     100000000 | medium     | Maximum Kafka protocol response message size. This serves as a safety precaution to avoid memory exhaustion in case of protocol hickups. This value must be at least `fetch.max.bytes`  + 512 to allow for protocol overhead; the value is adjusted automatically unless the configuration property is explicitly set. <br>*Type: integer*
max.in.flight.requests.per.connection    |  *  | 1 .. 1000000    |       1000000 | low        | Maximum number of in-flight requests per broker connection. This is a generic property applied to all broker communication, however it is primarily relevant to produce requests. In particular, note that other mechanisms limit the number of outstanding consumer fetch request per broker to one. <br>*Type: integer*
max.in.flight                            |  *  | 1 .. 1000000    |       1000000 | low        | Alias for `max.in.flight.requests.per.connection`: Maximum number of in-flight requests per broker connection. This is a generic property applied to all broker communication, however it is primarily relevant to produce requests. In particular, note that other mechanisms limit the number of outstanding consumer fetch request per broker to one. <br>*Type: integer*
topic.metadata.refresh.interval.ms       |  *  | -1 .. 3600000   |        300000 | low        | Period of time in milliseconds at which topic and broker metadata is refreshed in order to proactively discover any new brokers, topics, partitions or partition leader changes. Use -1 to disable the intervalled refresh (not recommended). If there are no locally referenced topics (no topic objects created, no messages produced, no subscription or no assignment) then only the broker list will be refreshed every interval but no more often than every 10s. <br>*Type: integer*
metadata.max.age.ms                      |  *  | 1 .. 86400000   |        900000 | low        | Metadata cache max age. Defaults to topic.metadata.refresh.interval.ms * 3 <br>*Type: integer*
topic.metadata.refresh.fast.interval.ms  |  *  | 1 .. 60000      |           250 | low        | When a topic loses its leader a new metadata request will be enqueued with this initial interval, exponentially increasing until the topic metadata has been refreshed. This is used to recover quickly from transitioning leader brokers. <br>*Type: integer*
topic.metadata.refresh.fast.cnt          |  *  | 0 .. 1000       |            10 | low        | **DEPRECATED** No longer used. <br>*Type: integer*
topic.metadata.refresh.sparse            |  *  | true, false     |          true | low        | Sparse metadata requests (consumes less network bandwidth) <br>*Type: boolean*
topic.metadata.propagation.max.ms        |  *  | 0 .. 3600000    |         30000 | low        | Apache Kafka topic creation is asynchronous and it takes some time for a new topic to propagate throughout the cluster to all brokers. If a client requests topic metadata after manual topic creation but before the topic has been fully propagated to the broker the client is requesting metadata from, the topic will seem to be non-existent and the client will mark the topic as such, failing queued produced messages with `ERR__UNKNOWN_TOPIC`. This setting delays marking a topic as non-existent until the configured propagation max time has passed. The maximum propagation time is calculated from the time the topic is first referenced in the client, e.g., on produce(). <br>*Type: integer*
topic.blacklist                          |  *  |                 |               | low        | Topic blacklist, a comma-separated list of regular expressions for matching topic names that should be ignored in broker metadata information as if the topics did not exist. <br>*Type: pattern list*
debug                                    |  *  | generic, broker, topic, metadata, feature, queue, msg, protocol, cgrp, security, fetch, interceptor, plugin, consumer, admin, eos, mock, assignor, conf, all |               | medium     | A comma-separated list of debug contexts to enable. Detailed Producer debugging: broker,topic,msg. Consumer: consumer,cgrp,topic,fetch <br>*Type: CSV flags*
socket.timeout.ms                        |  *  | 10 .. 300000    |         60000 | low        | Default timeout for network requests. Producer: ProduceRequests will use the lesser value of `socket.timeout.ms` and remaining `message.timeout.ms` for the first message in the batch. Consumer: FetchRequests will use `fetch.wait.max.ms` + `socket.timeout.ms`. Admin: Admin requests will use `socket.timeout.ms` or explicitly set `rd_kafka_AdminOptions_set_operation_timeout()` value. <br>*Type: integer*
socket.blocking.max.ms                   |  *  | 1 .. 60000      |          1000 | low        | **DEPRECATED** No longer used. <br>*Type: integer*
socket.send.buffer.bytes                 |  *  | 0 .. 100000000  |             0 | low        | Broker socket send buffer size. System default is used if 0. <br>*Type: integer*
socket.receive.buffer.bytes              |  *  | 0 .. 100000000  |             0 | low        | Broker socket receive buffer size. System default is used if 0. <br>*Type: integer*
socket.keepalive.enable                  |  *  | true, false     |         false | low        | Enable TCP keep-alives (SO_KEEPALIVE) on broker sockets <br>*Type: boolean*
socket.nagle.disable                     |  *  | true, false     |         false | low        | Disable the Nagle algorithm (TCP_NODELAY) on broker sockets. <br>*Type: boolean*
socket.max.fails                         |  *  | 0 .. 1000000    |             1 | low        | Disconnect from broker when this number of send failures (e.g., timed out requests) is reached. Disable with 0. WARNING: It is highly recommended to leave this setting at its default value of 1 to avoid the client and broker to become desynchronized in case of request timeouts. NOTE: The connection is automatically re-established. <br>*Type: integer*
broker.address.ttl                       |  *  | 0 .. 86400000   |          1000 | low        | How long to cache the broker address resolving results (milliseconds). <br>*Type: integer*
broker.address.family                    |  *  | any, v4, v6     |           any | low        | Allowed broker IP address families: any, v4, v6 <br>*Type: enum value*
connections.max.idle.ms                  |  *  | 0 .. 2147483647 |             0 | medium     | Close broker connections after the specified time of inactivity. Disable with 0. If this property is left at its default value some heuristics are performed to determine a suitable default value, this is currently limited to identifying brokers on Azure (see librdkafka issue #3109 for more info). <br>*Type: integer*
reconnect.backoff.jitter.ms              |  *  | 0 .. 3600000    |             0 | low        | **DEPRECATED** No longer used. See `reconnect.backoff.ms` and `reconnect.backoff.max.ms`. <br>*Type: integer*
reconnect.backoff.ms                     |  *  | 0 .. 3600000    |           100 | medium     | The initial time to wait before reconnecting to a broker after the connection has been closed. The time is increased exponentially until `reconnect.backoff.max.ms` is reached. -25% to +50% jitter is applied to each reconnect backoff. A value of 0 disables the backoff and reconnects immediately. <br>*Type: integer*
reconnect.backoff.max.ms                 |  *  | 0 .. 3600000    |         10000 | medium     | The maximum time to wait before reconnecting to a broker after the connection has been closed. <br>*Type: integer*
statistics.interval.ms                   |  *  | 0 .. 86400000   |             0 | high       | librdkafka statistics emit interval. The application also needs to register a stats callback using `rd_kafka_conf_set_stats_cb()`. The granularity is 1000ms. A value of 0 disables statistics. <br>*Type: integer*
enabled_events                           |  *  | 0 .. 2147483647 |             0 | low        | See `rd_kafka_conf_set_events()` <br>*Type: integer*
error_cb                                 |  *  |                 |               | low        | Error callback (set with rd_kafka_conf_set_error_cb()) <br>*Type: see dedicated API*
throttle_cb                              |  *  |                 |               | low        | Throttle callback (set with rd_kafka_conf_set_throttle_cb()) <br>*Type: see dedicated API*
stats_cb                                 |  *  |                 |               | low        | Statistics callback (set with rd_kafka_conf_set_stats_cb()) <br>*Type: see dedicated API*
log_cb                                   |  *  |                 |               | low        | Log callback (set with rd_kafka_conf_set_log_cb()) <br>*Type: see dedicated API*
log_level                                |  *  | 0 .. 7          |             6 | low        | Logging level (syslog(3) levels) <br>*Type: integer*
log.queue                                |  *  | true, false     |         false | low        | Disable spontaneous log_cb from internal librdkafka threads, instead enqueue log messages on queue set with `rd_kafka_set_log_queue()` and serve log callbacks or events through the standard poll APIs. **NOTE**: Log messages will linger in a temporary queue until the log queue has been set. <br>*Type: boolean*
log.thread.name                          |  *  | true, false     |          true | low        | Print internal thread name in log messages (useful for debugging librdkafka internals) <br>*Type: boolean*
enable.random.seed                       |  *  | true, false     |          true | low        | If enabled librdkafka will initialize the PRNG with srand(current_time.milliseconds) on the first invocation of rd_kafka_new() (required only if rand_r() is not available on your platform). If disabled the application must call srand() prior to calling rd_kafka_new(). <br>*Type: boolean*
log.connection.close                     |  *  | true, false     |          true | low        | Log broker disconnects. It might be useful to turn this off when interacting with 0.9 brokers with an aggressive `connection.max.idle.ms` value. <br>*Type: boolean*
background_event_cb                      |  *  |                 |               | low        | Background queue event callback (set with rd_kafka_conf_set_background_event_cb()) <br>*Type: see dedicated API*
socket_cb                                |  *  |                 |               | low        | Socket creation callback to provide race-free CLOEXEC <br>*Type: see dedicated API*
connect_cb                               |  *  |                 |               | low        | Socket connect callback <br>*Type: see dedicated API*
closesocket_cb                           |  *  |                 |               | low        | Socket close callback <br>*Type: see dedicated API*
open_cb                                  |  *  |                 |               | low        | File open callback to provide race-free CLOEXEC <br>*Type: see dedicated API*
opaque                                   |  *  |                 |               | low        | Application opaque (set with rd_kafka_conf_set_opaque()) <br>*Type: see dedicated API*
default_topic_conf                       |  *  |                 |               | low        | Default topic configuration for automatically subscribed topics <br>*Type: see dedicated API*
internal.termination.signal              |  *  | 0 .. 128        |             0 | low        | Signal that librdkafka will use to quickly terminate on rd_kafka_destroy(). If this signal is not set then there will be a delay before rd_kafka_wait_destroyed() returns true as internal threads are timing out their system calls. If this signal is set however the delay will be minimal. The application should mask this signal as an internal signal handler is installed. <br>*Type: integer*
api.version.request                      |  *  | true, false     |          true | high       | Request broker's supported API versions to adjust functionality to available protocol features. If set to false, or the ApiVersionRequest fails, the fallback version `broker.version.fallback` will be used. **NOTE**: Depends on broker version >=0.10.0. If the request is not supported by (an older) broker the `broker.version.fallback` fallback is used. <br>*Type: boolean*
api.version.request.timeout.ms           |  *  | 1 .. 300000     |         10000 | low        | Timeout for broker API version requests. <br>*Type: integer*
api.version.fallback.ms                  |  *  | 0 .. 604800000  |             0 | medium     | Dictates how long the `broker.version.fallback` fallback is used in the case the ApiVersionRequest fails. **NOTE**: The ApiVersionRequest is only issued when a new connection to the broker is made (such as after an upgrade). <br>*Type: integer*
broker.version.fallback                  |  *  |                 |        0.10.0 | medium     | Older broker versions (before 0.10.0) provide no way for a client to query for supported protocol features (ApiVersionRequest, see `api.version.request`) making it impossible for the client to know what features it may use. As a workaround a user may set this property to the expected broker version and the client will automatically adjust its feature set accordingly if the ApiVersionRequest fails (or is disabled). The fallback broker version will be used for `api.version.fallback.ms`. Valid values are: 0.9.0, 0.8.2, 0.8.1, 0.8.0. Any other value >= 0.10, such as 0.10.2.1, enables ApiVersionRequests. <br>*Type: string*
security.protocol                        |  *  | plaintext, ssl, sasl_plaintext, sasl_ssl |     plaintext | high       | Protocol used to communicate with brokers. <br>*Type: enum value*
ssl.cipher.suites                        |  *  |                 |               | low        | A cipher suite is a named combination of authentication, encryption, MAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. See manual page for `ciphers(1)` and `SSL_CTX_set_cipher_list(3). <br>*Type: string*
ssl.curves.list                          |  *  |                 |               | low        | The supported-curves extension in the TLS ClientHello message specifies the curves (standard/named, or 'explicit' GF(2^k) or GF(p)) the client is willing to have the server use. See manual page for `SSL_CTX_set1_curves_list(3)`. OpenSSL >= 1.0.2 required. <br>*Type: string*
ssl.sigalgs.list                         |  *  |                 |               | low        | The client uses the TLS ClientHello signature_algorithms extension to indicate to the server which signature/hash algorithm pairs may be used in digital signatures. See manual page for `SSL_CTX_set1_sigalgs_list(3)`. OpenSSL >= 1.0.2 required. <br>*Type: string*
ssl.key.location                         |  *  |                 |               | low        | Path to client's private key (PEM) used for authentication. <br>*Type: string*
ssl.key.password                         |  *  |                 |               | low        | Private key passphrase (for use with `ssl.key.location` and `set_ssl_cert()`) <br>*Type: string*
ssl.key.pem                              |  *  |                 |               | low        | Client's private key string (PEM format) used for authentication. <br>*Type: string*
ssl_key                                  |  *  |                 |               | low        | Client's private key as set by rd_kafka_conf_set_ssl_cert() <br>*Type: see dedicated API*
ssl.certificate.location                 |  *  |                 |               | low        | Path to client's public key (PEM) used for authentication. <br>*Type: string*
ssl.certificate.pem                      |  *  |                 |               | low        | Client's public key string (PEM format) used for authentication. <br>*Type: string*
ssl_certificate                          |  *  |                 |               | low        | Client's public key as set by rd_kafka_conf_set_ssl_cert() <br>*Type: see dedicated API*
ssl.ca.location                          |  *  |                 |               | low        | File or directory path to CA certificate(s) for verifying the broker's key. Defaults: On Windows the system's CA certificates are automatically looked up in the Windows Root certificate store. On Mac OSX this configuration defaults to `probe`. It is recommended to install openssl using Homebrew, to provide CA certificates. On Linux install the distribution's ca-certificates package. If OpenSSL is statically linked or `ssl.ca.location` is set to `probe` a list of standard paths will be probed and the first one found will be used as the default CA certificate location path. If OpenSSL is dynamically linked the OpenSSL library's default path will be used (see `OPENSSLDIR` in `openssl version -a`). <br>*Type: string*
ssl.ca.pem                               |  *  |                 |               | low        | CA certificate string (PEM format) for verifying the broker's key. <br>*Type: string*
ssl_ca                                   |  *  |                 |               | low        | CA certificate as set by rd_kafka_conf_set_ssl_cert() <br>*Type: see dedicated API*
ssl.ca.certificate.stores                |  *  |                 |          Root | low        | Comma-separated list of Windows Certificate stores to load CA certificates from. Certificates will be loaded in the same order as stores are specified. If no certificates can be loaded from any of the specified stores an error is logged and the OpenSSL library's default CA location is used instead. Store names are typically one or more of: MY, Root, Trust, CA. <br>*Type: string*
ssl.crl.location                         |  *  |                 |               | low        | Path to CRL for verifying broker's certificate validity. <br>*Type: string*
ssl.keystore.location                    |  *  |                 |               | low        | Path to client's keystore (PKCS#12) used for authentication. <br>*Type: string*
ssl.keystore.password                    |  *  |                 |               | low        | Client's keystore (PKCS#12) password. <br>*Type: string*
ssl.engine.location                      |  *  |                 |               | low        | Path to OpenSSL engine library. OpenSSL >= 1.1.0 required. <br>*Type: string*
ssl.engine.id                            |  *  |                 |       dynamic | low        | OpenSSL engine id is the name used for loading engine. <br>*Type: string*
ssl_engine_callback_data                 |  *  |                 |               | low        | OpenSSL engine callback data (set with rd_kafka_conf_set_engine_callback_data()). <br>*Type: see dedicated API*
enable.ssl.certificate.verification      |  *  | true, false     |          true | low        | Enable OpenSSL's builtin broker (server) certificate verification. This verification can be extended by the application by implementing a certificate_verify_cb. <br>*Type: boolean*
ssl.endpoint.identification.algorithm    |  *  | none, https     |          none | low        | Endpoint identification algorithm to validate broker hostname using broker certificate. https - Server (broker) hostname verification as specified in RFC2818. none - No endpoint verification. OpenSSL >= 1.0.2 required. <br>*Type: enum value*
ssl.certificate.verify_cb                |  *  |                 |               | low        | Callback to verify the broker certificate chain. <br>*Type: see dedicated API*
sasl.mechanisms                          |  *  |                 |        GSSAPI | high       | SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, OAUTHBEARER. **NOTE**: Despite the name only one mechanism must be configured. <br>*Type: string*
sasl.mechanism                           |  *  |                 |        GSSAPI | high       | Alias for `sasl.mechanisms`: SASL mechanism to use for authentication. Supported: GSSAPI, PLAIN, SCRAM-SHA-256, SCRAM-SHA-512, OAUTHBEARER. **NOTE**: Despite the name only one mechanism must be configured. <br>*Type: string*
sasl.kerberos.service.name               |  *  |                 |         kafka | low        | Kerberos principal name that Kafka runs as, not including /hostname@REALM <br>*Type: string*
sasl.kerberos.principal                  |  *  |                 |   kafkaclient | low        | This client's Kerberos principal name. (Not supported on Windows, will use the logon user's principal). <br>*Type: string*
sasl.kerberos.kinit.cmd                  |  *  |                 | kinit -R -t "%{sasl.kerberos.keytab}" -k %{sasl.kerberos.principal} \|\| kinit -t "%{sasl.kerberos.keytab}" -k %{sasl.kerberos.principal} | low        | Shell command to refresh or acquire the client's Kerberos ticket. This command is executed on client creation and every sasl.kerberos.min.time.before.relogin (0=disable). %{config.prop.name} is replaced by corresponding config object value. <br>*Type: string*
sasl.kerberos.keytab                     |  *  |                 |               | low        | Path to Kerberos keytab file. This configuration property is only used as a variable in `sasl.kerberos.kinit.cmd` as ` ... -t "%{sasl.kerberos.keytab}"`. <br>*Type: string*
sasl.kerberos.min.time.before.relogin    |  *  | 0 .. 86400000   |         60000 | low        | Minimum time in milliseconds between key refresh attempts. Disable automatic key refresh by setting this property to 0. <br>*Type: integer*
sasl.username                            |  *  |                 |               | high       | SASL username for use with the PLAIN and SASL-SCRAM-.. mechanisms <br>*Type: string*
sasl.password                            |  *  |                 |               | high       | SASL password for use with the PLAIN and SASL-SCRAM-.. mechanism <br>*Type: string*
sasl.oauthbearer.config                  |  *  |                 |               | low        | SASL/OAUTHBEARER configuration. The format is implementation-dependent and must be parsed accordingly. The default unsecured token implementation (see https://tools.ietf.org/html/rfc7515#appendix-A.5) recognizes space-separated name=value pairs with valid names including principalClaimName, principal, scopeClaimName, scope, and lifeSeconds. The default value for principalClaimName is "sub", the default value for scopeClaimName is "scope", and the default value for lifeSeconds is 3600. The scope value is CSV format with the default value being no/empty scope. For example: `principalClaimName=azp principal=admin scopeClaimName=roles scope=role1,role2 lifeSeconds=600`. In addition, SASL extensions can be communicated to the broker via `extension_NAME=value`. For example: `principal=admin extension_traceId=123` <br>*Type: string*
enable.sasl.oauthbearer.unsecure.jwt     |  *  | true, false     |         false | low        | Enable the builtin unsecure JWT OAUTHBEARER token handler if no oauthbearer_refresh_cb has been set. This builtin handler should only be used for development or testing, and not in production. <br>*Type: boolean*
oauthbearer_token_refresh_cb             |  *  |                 |               | low        | SASL/OAUTHBEARER token refresh callback (set with rd_kafka_conf_set_oauthbearer_token_refresh_cb(), triggered by rd_kafka_poll(), et.al. This callback will be triggered when it is time to refresh the client's OAUTHBEARER token. <br>*Type: see dedicated API*
plugin.library.paths                     |  *  |                 |               | low        | List of plugin libraries to load (; separated). The library search path is platform dependent (see dlopen(3) for Unix and LoadLibrary() for Windows). If no filename extension is specified the platform-specific extension (such as .dll or .so) will be appended automatically. <br>*Type: string*
interceptors                             |  *  |                 |               | low        | Interceptors added through rd_kafka_conf_interceptor_add_..() and any configuration handled by interceptors. <br>*Type: see dedicated API*
group.id                                 |  C  |                 |               | high       | Client group id string. All clients sharing the same group.id belong to the same group. <br>*Type: string*
group.instance.id                        |  C  |                 |               | medium     | Enable static group membership. Static group members are able to leave and rejoin a group within the configured `session.timeout.ms` without prompting a group rebalance. This should be used in combination with a larger `session.timeout.ms` to avoid group rebalances caused by transient unavailability (e.g. process restarts). Requires broker version >= 2.3.0. <br>*Type: string*
partition.assignment.strategy            |  C  |                 | range,roundrobin | medium     | The name of one or more partition assignment strategies. The elected group leader will use a strategy supported by all members of the group to assign partitions to group members. If there is more than one eligible strategy, preference is determined by the order of this list (strategies earlier in the list have higher priority). Cooperative and non-cooperative (eager) strategies must not be mixed. Available strategies: range, roundrobin, cooperative-sticky. <br>*Type: string*
session.timeout.ms                       |  C  | 1 .. 3600000    |         45000 | high       | Client group session and failure detection timeout. The consumer sends periodic heartbeats (heartbeat.interval.ms) to indicate its liveness to the broker. If no hearts are received by the broker for a group member within the session timeout, the broker will remove the consumer from the group and trigger a rebalance. The allowed range is configured with the **broker** configuration properties `group.min.session.timeout.ms` and `group.max.session.timeout.ms`. Also see `max.poll.interval.ms`. <br>*Type: integer*
heartbeat.interval.ms                    |  C  | 1 .. 3600000    |          3000 | low        | Group session keepalive heartbeat interval. <br>*Type: integer*
group.protocol.type                      |  C  |                 |      consumer | low        | Group protocol type. NOTE: Currently, the only supported group protocol type is `consumer`. <br>*Type: string*
coordinator.query.interval.ms            |  C  | 1 .. 3600000    |        600000 | low        | How often to query for the current client group coordinator. If the currently assigned coordinator is down the configured query interval will be divided by ten to more quickly recover in case of coordinator reassignment. <br>*Type: integer*
max.poll.interval.ms                     |  C  | 1 .. 86400000   |        300000 | high       | Maximum allowed time between calls to consume messages (e.g., rd_kafka_consumer_poll()) for high-level consumers. If this interval is exceeded the consumer is considered failed and the group will rebalance in order to reassign the partitions to another consumer group member. Warning: Offset commits may be not possible at this point. Note: It is recommended to set `enable.auto.offset.store=false` for long-time processing applications and then explicitly store offsets (using offsets_store()) *after* message processing, to make sure offsets are not auto-committed prior to processing has finished. The interval is checked two times per second. See KIP-62 for more information. <br>*Type: integer*
enable.auto.commit                       |  C  | true, false     |          true | high       | Automatically and periodically commit offsets in the background. Note: setting this to false does not prevent the consumer from fetching previously committed start offsets. To circumvent this behaviour set specific start offsets per partition in the call to assign(). <br>*Type: boolean*
auto.commit.interval.ms                  |  C  | 0 .. 86400000   |          5000 | medium     | The frequency in milliseconds that the consumer offsets are committed (written) to offset storage. (0 = disable). This setting is used by the high-level consumer. <br>*Type: integer*
enable.auto.offset.store                 |  C  | true, false     |          true | high       | Automatically store offset of last message provided to application. The offset store is an in-memory store of the next offset to (auto-)commit for each partition. <br>*Type: boolean*
queued.min.messages                      |  C  | 1 .. 10000000   |        100000 | medium     | Minimum number of messages per topic+partition librdkafka tries to maintain in the local consumer queue. <br>*Type: integer*
queued.max.messages.kbytes               |  C  | 1 .. 2097151    |         65536 | medium     | Maximum number of kilobytes of queued pre-fetched messages in the local consumer queue. If using the high-level consumer this setting applies to the single consumer queue, regardless of the number of partitions. When using the legacy simple consumer or when separate partition queues are used this setting applies per partition. This value may be overshot by fetch.message.max.bytes. This property has higher priority than queued.min.messages. <br>*Type: integer*
fetch.wait.max.ms                        |  C  | 0 .. 300000     |           500 | low        | Maximum time the broker may wait to fill the Fetch response with fetch.min.bytes of messages. <br>*Type: integer*
fetch.message.max.bytes                  |  C  | 1 .. 1000000000 |       1048576 | medium     | Initial maximum number of bytes per topic+partition to request when fetching messages from the broker. If the client encounters a message larger than this value it will gradually try to increase it until the entire message can be fetched. <br>*Type: integer*
max.partition.fetch.bytes                |  C  | 1 .. 1000000000 |       1048576 | medium     | Alias for `fetch.message.max.bytes`: Initial maximum number of bytes per topic+partition to request when fetching messages from the broker. If the client encounters a message larger than this value it will gradually try to increase it until the entire message can be fetched. <br>*Type: integer*
fetch.max.bytes                          |  C  | 0 .. 2147483135 |      52428800 | medium     | Maximum amount of data the broker shall return for a Fetch request. Messages are fetched in batches by the consumer and if the first message batch in the first non-empty partition of the Fetch request is larger than this value, then the message batch will still be returned to ensure the consumer can make progress. The maximum message batch size accepted by the broker is defined via `message.max.bytes` (broker config) or `max.message.bytes` (broker topic config). `fetch.max.bytes` is automatically adjusted upwards to be at least `message.max.bytes` (consumer config). <br>*Type: integer*
fetch.min.bytes                          |  C  | 1 .. 100000000  |             1 | low        | Minimum number of bytes the broker responds with. If fetch.wait.max.ms expires the accumulated data will be sent to the client regardless of this setting. <br>*Type: integer*
fetch.error.backoff.ms                   |  C  | 0 .. 300000     |           500 | medium     | How long to postpone the next fetch request for a topic+partition in case of a fetch error. <br>*Type: integer*
offset.store.method                      |  C  | none, file, broker |        broker | low        | **DEPRECATED** Offset commit store method: 'file' - DEPRECATED: local file store (offset.store.path, et.al), 'broker' - broker commit store (requires Apache Kafka 0.8.2 or later on the broker). <br>*Type: enum value*
isolation.level                          |  C  | read_uncommitted, read_committed | read_committed | high       | Controls how to read messages written transactionally: `read_committed` - only return transactional messages which have been committed. `read_uncommitted` - return all messages, even transactional messages which have been aborted. <br>*Type: enum value*
consume_cb                               |  C  |                 |               | low        | Message consume callback (set with rd_kafka_conf_set_consume_cb()) <br>*Type: see dedicated API*
rebalance_cb                             |  C  |                 |               | low        | Called after consumer group has been rebalanced (set with rd_kafka_conf_set_rebalance_cb()) <br>*Type: see dedicated API*
offset_commit_cb                         |  C  |                 |               | low        | Offset commit result propagation callback. (set with rd_kafka_conf_set_offset_commit_cb()) <br>*Type: see dedicated API*
enable.partition.eof                     |  C  | true, false     |         false | low        | Emit RD_KAFKA_RESP_ERR__PARTITION_EOF event whenever the consumer reaches the end of a partition. <br>*Type: boolean*
check.crcs                               |  C  | true, false     |         false | medium     | Verify CRC32 of consumed messages, ensuring no on-the-wire or on-disk corruption to the messages occurred. This check comes at slightly increased CPU usage. <br>*Type: boolean*
allow.auto.create.topics                 |  C  | true, false     |         false | low        | Allow automatic topic creation on the broker when subscribing to or assigning non-existent topics. The broker must also be configured with `auto.create.topics.enable=true` for this configuraiton to take effect. Note: The default value (false) is different from the Java consumer (true). Requires broker version >= 0.11.0.0, for older broker versions only the broker configuration applies. <br>*Type: boolean*
client.rack                              |  *  |                 |               | low        | A rack identifier for this client. This can be any string value which indicates where this client is physically located. It corresponds with the broker config `broker.rack`. <br>*Type: string*
transactional.id                         |  P  |                 |               | high       | Enables the transactional producer. The transactional.id is used to identify the same transactional producer instance across process restarts. It allows the producer to guarantee that transactions corresponding to earlier instances of the same producer have been finalized prior to starting any new transactions, and that any zombie instances are fenced off. If no transactional.id is provided, then the producer is limited to idempotent delivery (if enable.idempotence is set). Requires broker version >= 0.11.0. <br>*Type: string*
transaction.timeout.ms                   |  P  | 1000 .. 2147483647 |         60000 | medium     | The maximum amount of time in milliseconds that the transaction coordinator will wait for a transaction status update from the producer before proactively aborting the ongoing transaction. If this value is larger than the `transaction.max.timeout.ms` setting in the broker, the init_transactions() call will fail with ERR_INVALID_TRANSACTION_TIMEOUT. The transaction timeout automatically adjusts `message.timeout.ms` and `socket.timeout.ms`, unless explicitly configured in which case they must not exceed the transaction timeout (`socket.timeout.ms` must be at least 100ms lower than `transaction.timeout.ms`). This is also the default timeout value if no timeout (-1) is supplied to the transactional API methods. <br>*Type: integer*
enable.idempotence                       |  P  | true, false     |         false | high       | When set to `true`, the producer will ensure that messages are successfully produced exactly once and in the original produce order. The following configuration properties are adjusted automatically (if not modified by the user) when idempotence is enabled: `max.in.flight.requests.per.connection=5` (must be less than or equal to 5), `retries=INT32_MAX` (must be greater than 0), `acks=all`, `queuing.strategy=fifo`. Producer instantation will fail if user-supplied configuration is incompatible. <br>*Type: boolean*
enable.gapless.guarantee                 |  P  | true, false     |         false | low        | **EXPERIMENTAL**: subject to change or removal. When set to `true`, any error that could result in a gap in the produced message series when a batch of messages fails, will raise a fatal error (ERR__GAPLESS_GUARANTEE) and stop the producer. Messages failing due to `message.timeout.ms` are not covered by this guarantee. Requires `enable.idempotence=true`. <br>*Type: boolean*
queue.buffering.max.messages             |  P  | 1 .. 10000000   |        100000 | high       | Maximum number of messages allowed on the producer queue. This queue is shared by all topics and partitions. <br>*Type: integer*
queue.buffering.max.kbytes               |  P  | 1 .. 2147483647 |       1048576 | high       | Maximum total message size sum allowed on the producer queue. This queue is shared by all topics and partitions. This property has higher priority than queue.buffering.max.messages. <br>*Type: integer*
queue.buffering.max.ms                   |  P  | 0 .. 900000     |             5 | high       | Delay in milliseconds to wait for messages in the producer queue to accumulate before constructing message batches (MessageSets) to transmit to brokers. A higher value allows larger and more effective (less overhead, improved compression) batches of messages to accumulate at the expense of increased message delivery latency. <br>*Type: float*
linger.ms                                |  P  | 0 .. 900000     |             5 | high       | Alias for `queue.buffering.max.ms`: Delay in milliseconds to wait for messages in the producer queue to accumulate before constructing message batches (MessageSets) to transmit to brokers. A higher value allows larger and more effective (less overhead, improved compression) batches of messages to accumulate at the expense of increased message delivery latency. <br>*Type: float*
message.send.max.retries                 |  P  | 0 .. 2147483647 |    2147483647 | high       | How many times to retry sending a failing Message. **Note:** retrying may cause reordering unless `enable.idempotence` is set to true. <br>*Type: integer*
retries                                  |  P  | 0 .. 2147483647 |    2147483647 | high       | Alias for `message.send.max.retries`: How many times to retry sending a failing Message. **Note:** retrying may cause reordering unless `enable.idempotence` is set to true. <br>*Type: integer*
retry.backoff.ms                         |  P  | 1 .. 300000     |           100 | medium     | The backoff time in milliseconds before retrying a protocol request. <br>*Type: integer*
queue.buffering.backpressure.threshold   |  P  | 1 .. 1000000    |             1 | low        | The threshold of outstanding not yet transmitted broker requests needed to backpressure the producer's message accumulator. If the number of not yet transmitted requests equals or exceeds this number, produce request creation that would have otherwise been triggered (for example, in accordance with linger.ms) will be delayed. A lower number yields larger and more effective batches. A higher value can improve latency when using compression on slow machines. <br>*Type: integer*
compression.codec                        |  P  | none, gzip, snappy, lz4, zstd |          none | medium     | compression codec to use for compressing message sets. This is the default value for all topics, may be overridden by the topic configuration property `compression.codec`.  <br>*Type: enum value*
compression.type                         |  P  | none, gzip, snappy, lz4, zstd |          none | medium     | Alias for `compression.codec`: compression codec to use for compressing message sets. This is the default value for all topics, may be overridden by the topic configuration property `compression.codec`.  <br>*Type: enum value*
batch.num.messages                       |  P  | 1 .. 1000000    |         10000 | medium     | Maximum number of messages batched in one MessageSet. The total MessageSet size is also limited by batch.size and message.max.bytes. <br>*Type: integer*
batch.size                               |  P  | 1 .. 2147483647 |       1000000 | medium     | Maximum size (in bytes) of all messages batched in one MessageSet, including protocol framing overhead. This limit is applied after the first message has been added to the batch, regardless of the first message's size, this is to ensure that messages that exceed batch.size are produced. The total MessageSet size is also limited by batch.num.messages and message.max.bytes. <br>*Type: integer*
delivery.report.only.error               |  P  | true, false     |         false | low        | Only provide delivery reports for failed messages. <br>*Type: boolean*
dr_cb                                    |  P  |                 |               | low        | Delivery report callback (set with rd_kafka_conf_set_dr_cb()) <br>*Type: see dedicated API*
dr_msg_cb                                |  P  |                 |               | low        | Delivery report callback (set with rd_kafka_conf_set_dr_msg_cb()) <br>*Type: see dedicated API*
sticky.partitioning.linger.ms            |  P  | 0 .. 900000     |            10 | low        | Delay in milliseconds to wait to assign new sticky partitions for each topic. By default, set to double the time of linger.ms. To disable sticky behavior, set to 0. This behavior affects messages with the key NULL in all cases, and messages with key lengths of zero when the consistent_random partitioner is in use. These messages would otherwise be assigned randomly. A higher value allows for more effective batching of these messages. <br>*Type: integer*


## Topic configuration properties

Property                                 | C/P | Range           |       Default | Importance | Description              
-----------------------------------------|-----|-----------------|--------------:|------------| --------------------------
request.required.acks                    |  P  | -1 .. 1000      |            -1 | high       | This field indicates the number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: *0*=Broker does not send any response/ack to client, *-1* or *all*=Broker will block until message is committed by all in sync replicas (ISRs). If there are less than `min.insync.replicas` (broker configuration) in the ISR set the produce request will fail. <br>*Type: integer*
acks                                     |  P  | -1 .. 1000      |            -1 | high       | Alias for `request.required.acks`: This field indicates the number of acknowledgements the leader broker must receive from ISR brokers before responding to the request: *0*=Broker does not send any response/ack to client, *-1* or *all*=Broker will block until message is committed by all in sync replicas (ISRs). If there are less than `min.insync.replicas` (broker configuration) in the ISR set the produce request will fail. <br>*Type: integer*
request.timeout.ms                       |  P  | 1 .. 900000     |         30000 | medium     | The ack timeout of the producer request in milliseconds. This value is only enforced by the broker and relies on `request.required.acks` being != 0. <br>*Type: integer*
message.timeout.ms                       |  P  | 0 .. 2147483647 |        300000 | high       | Local message timeout. This value is only enforced locally and limits the time a produced message waits for successful delivery. A time of 0 is infinite. This is the maximum time librdkafka may use to deliver a message (including retries). Delivery error occurs when either the retry count or the message timeout are exceeded. The message timeout is automatically adjusted to `transaction.timeout.ms` if `transactional.id` is configured. <br>*Type: integer*
delivery.timeout.ms                      |  P  | 0 .. 2147483647 |        300000 | high       | Alias for `message.timeout.ms`: Local message timeout. This value is only enforced locally and limits the time a produced message waits for successful delivery. A time of 0 is infinite. This is the maximum time librdkafka may use to deliver a message (including retries). Delivery error occurs when either the retry count or the message timeout are exceeded. The message timeout is automatically adjusted to `transaction.timeout.ms` if `transactional.id` is configured. <br>*Type: integer*
queuing.strategy                         |  P  | fifo, lifo      |          fifo | low        | **EXPERIMENTAL**: subject to change or removal. **DEPRECATED** Producer queuing strategy. FIFO preserves produce ordering, while LIFO prioritizes new messages. <br>*Type: enum value*
produce.offset.report                    |  P  | true, false     |         false | low        | **DEPRECATED** No longer used. <br>*Type: boolean*
partitioner                              |  P  |                 | consistent_random | high       | Partitioner: `random` - random distribution, `consistent` - CRC32 hash of key (Empty and NULL keys are mapped to single partition), `consistent_random` - CRC32 hash of key (Empty and NULL keys are randomly partitioned), `murmur2` - Java Producer compatible Murmur2 hash of key (NULL keys are mapped to single partition), `murmur2_random` - Java Producer compatible Murmur2 hash of key (NULL keys are randomly partitioned. This is functionally equivalent to the default partitioner in the Java Producer.), `fnv1a` - FNV-1a hash of key (NULL keys are mapped to single partition), `fnv1a_random` - FNV-1a hash of key (NULL keys are randomly partitioned). <br>*Type: string*
partitioner_cb                           |  P  |                 |               | low        | Custom partitioner callback (set with rd_kafka_topic_conf_set_partitioner_cb()) <br>*Type: see dedicated API*
msg_order_cmp                            |  P  |                 |               | low        | **EXPERIMENTAL**: subject to change or removal. **DEPRECATED** Message queue ordering comparator (set with rd_kafka_topic_conf_set_msg_order_cmp()). Also see `queuing.strategy`. <br>*Type: see dedicated API*
opaque                                   |  *  |                 |               | low        | Application opaque (set with rd_kafka_topic_conf_set_opaque()) <br>*Type: see dedicated API*
compression.codec                        |  P  | none, gzip, snappy, lz4, zstd, inherit |       inherit | high       | Compression codec to use for compressing message sets. inherit = inherit global compression.codec configuration. <br>*Type: enum value*
compression.type                         |  P  | none, gzip, snappy, lz4, zstd |          none | medium     | Alias for `compression.codec`: compression codec to use for compressing message sets. This is the default value for all topics, may be overridden by the topic configuration property `compression.codec`.  <br>*Type: enum value*
compression.level                        |  P  | -1 .. 12        |            -1 | medium     | Compression level parameter for algorithm selected by configuration property `compression.codec`. Higher values will result in better compression at the cost of more CPU usage. Usable range is algorithm-dependent: [0-9] for gzip; [0-12] for lz4; only 0 for snappy; -1 = codec-dependent default compression level. <br>*Type: integer*
auto.commit.enable                       |  C  | true, false     |          true | low        | **DEPRECATED** [**LEGACY PROPERTY:** This property is used by the simple legacy consumer only. When using the high-level KafkaConsumer, the global `enable.auto.commit` property must be used instead]. If true, periodically commit offset of the last message handed to the application. This committed offset will be used when the process restarts to pick up where it left off. If false, the application will have to call `rd_kafka_offset_store()` to store an offset (optional). Offsets will be written to broker or local file according to offset.store.method. <br>*Type: boolean*
enable.auto.commit                       |  C  | true, false     |          true | low        | **DEPRECATED** Alias for `auto.commit.enable`: [**LEGACY PROPERTY:** This property is used by the simple legacy consumer only. When using the high-level KafkaConsumer, the global `enable.auto.commit` property must be used instead]. If true, periodically commit offset of the last message handed to the application. This committed offset will be used when the process restarts to pick up where it left off. If false, the application will have to call `rd_kafka_offset_store()` to store an offset (optional). Offsets will be written to broker or local file according to offset.store.method. <br>*Type: boolean*
auto.commit.interval.ms                  |  C  | 10 .. 86400000  |         60000 | high       | [**LEGACY PROPERTY:** This setting is used by the simple legacy consumer only. When using the high-level KafkaConsumer, the global `auto.commit.interval.ms` property must be used instead]. The frequency in milliseconds that the consumer offsets are committed (written) to offset storage. <br>*Type: integer*
auto.offset.reset                        |  C  | smallest, earliest, beginning, largest, latest, end, error |       largest | high       | Action to take when there is no initial offset in offset store or the desired offset is out of range: 'smallest','earliest' - automatically reset the offset to the smallest offset, 'largest','latest' - automatically reset the offset to the largest offset, 'error' - trigger an error (ERR__AUTO_OFFSET_RESET) which is retrieved by consuming messages and checking 'message->err'. <br>*Type: enum value*
offset.store.path                        |  C  |                 |             . | low        | **DEPRECATED** Path to local file for storing offsets. If the path is a directory a filename will be automatically generated in that directory based on the topic and partition. File-based offset storage will be removed in a future version. <br>*Type: string*
offset.store.sync.interval.ms            |  C  | -1 .. 86400000  |            -1 | low        | **DEPRECATED** fsync() interval for the offset file, in milliseconds. Use -1 to disable syncing, and 0 for immediate sync after each write. File-based offset storage will be removed in a future version. <br>*Type: integer*
offset.store.method                      |  C  | file, broker    |        broker | low        | **DEPRECATED** Offset commit store method: 'file' - DEPRECATED: local file store (offset.store.path, et.al), 'broker' - broker commit store (requires "group.id" to be configured and Apache Kafka 0.8.2 or later on the broker.). <br>*Type: enum value*
consume.callback.max.messages            |  C  | 0 .. 1000000    |             0 | low        | Maximum number of messages to dispatch in one `rd_kafka_consume_callback*()` call (0 = unlimited) <br>*Type: integer*

### C/P legend: C = Consumer, P = Producer, * = both

```

`IntelPTDriver/packages/librdkafka.redist.1.8.2/LICENSES.txt`:

```txt
LICENSE
--------------------------------------------------------------
librdkafka - Apache Kafka C driver library

Copyright (c) 2012-2020, Magnus Edenhill
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

1. Redistributions of source code must retain the above copyright notice,
   this list of conditions and the following disclaimer.
2. Redistributions in binary form must reproduce the above copyright notice,
   this list of conditions and the following disclaimer in the documentation
   and/or other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
POSSIBILITY OF SUCH DAMAGE.


LICENSE.crc32c
--------------------------------------------------------------
# For src/crc32c.c copied (with modifications) from
# http://stackoverflow.com/a/17646775/1821055

/* crc32c.c -- compute CRC-32C using the Intel crc32 instruction
 * Copyright (C) 2013 Mark Adler
 * Version 1.1  1 Aug 2013  Mark Adler
 */

/*
  This software is provided 'as-is', without any express or implied
  warranty.  In no event will the author be held liable for any damages
  arising from the use of this software.

  Permission is granted to anyone to use this software for any purpose,
  including commercial applications, and to alter it and redistribute it
  freely, subject to the following restrictions:

  1. The origin of this software must not be misrepresented; you must not
     claim that you wrote the original software. If you use this software
     in a product, an acknowledgment in the product documentation would be
     appreciated but is not required.
  2. Altered source versions must be plainly marked as such, and must not be
     misrepresented as being the original software.
  3. This notice may not be removed or altered from any source distribution.

  Mark Adler
  madler@alumni.caltech.edu
 */


LICENSE.fnv1a
--------------------------------------------------------------
parts of src/rdfnv1a.c: http://www.isthe.com/chongo/src/fnv/hash_32a.c


Please do not copyright this code.  This code is in the public domain.

LANDON CURT NOLL DISCLAIMS ALL WARRANTIES WITH REGARD TO THIS SOFTWARE,
INCLUDING ALL IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS. IN NO
EVENT SHALL LANDON CURT NOLL BE LIABLE FOR ANY SPECIAL, INDIRECT OR
CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF
USE, DATA OR PROFITS, WHETHER IN AN ACTION OF CONTRACT, NEGLIGENCE OR
OTHER TORTIOUS ACTION, ARISING OUT OF OR IN CONNECTION WITH THE USE OR
PERFORMANCE OF THIS SOFTWARE.

By:
    chongo <Landon Curt Noll> /\oo/\
    http://www.isthe.com/chongo/

Share and Enjoy!	:-)


LICENSE.hdrhistogram
--------------------------------------------------------------
This license covers src/rdhdrhistogram.c which is a C port of
Coda Hale's Golang HdrHistogram https://github.com/codahale/hdrhistogram
at revision 3a0bb77429bd3a61596f5e8a3172445844342120

-----------------------------------------------------------------------------

The MIT License (MIT)

Copyright (c) 2014 Coda Hale

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE


LICENSE.lz4
--------------------------------------------------------------
src/rdxxhash.[ch] src/lz4*.[ch]: git@github.com:lz4/lz4.git e2827775ee80d2ef985858727575df31fc60f1f3

LZ4 Library
Copyright (c) 2011-2016, Yann Collet
All rights reserved.

Redistribution and use in source and binary forms, with or without modification,
are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above copyright notice, this
  list of conditions and the following disclaimer in the documentation and/or
  other materials provided with the distribution.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND
ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR
ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;
LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON
ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


LICENSE.murmur2
--------------------------------------------------------------
parts of src/rdmurmur2.c: git@github.com:abrandoned/murmur2.git


MurMurHash2 Library
//-----------------------------------------------------------------------------
// MurmurHash2 was written by Austin Appleby, and is placed in the public
// domain. The author hereby disclaims copyright to this source code.

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


LICENSE.pycrc
--------------------------------------------------------------
The following license applies to the files rdcrc32.c and rdcrc32.h which
have been generated by the pycrc tool.
============================================================================

Copyright (c) 2006-2012, Thomas Pircher <tehpeh@gmx.net>

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in
all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
THE SOFTWARE.


LICENSE.queue
--------------------------------------------------------------
For sys/queue.h:

 * Copyright (c) 1991, 1993
 *	The Regents of the University of California.  All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 * 4. Neither the name of the University nor the names of its contributors
 *    may be used to endorse or promote products derived from this software
 *    without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE REGENTS AND CONTRIBUTORS ``AS IS'' AND
 * ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED.  IN NO EVENT SHALL THE REGENTS OR CONTRIBUTORS BE LIABLE
 * FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
 * DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS
 * OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
 * HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT
 * LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY
 * OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF
 * SUCH DAMAGE.
 *
 *	@(#)queue.h	8.5 (Berkeley) 8/20/94
 * $FreeBSD$

LICENSE.regexp
--------------------------------------------------------------
regexp.c and regexp.h from https://github.com/ccxvii/minilibs sha 875c33568b5a4aa4fb3dd0c52ea98f7f0e5ca684

"
These libraries are in the public domain (or the equivalent where that is not possible). You can do anything you want with them. You have no legal obligation to do anything else, although I appreciate attribution.
"


LICENSE.snappy
--------------------------------------------------------------
######################################################################
# LICENSE.snappy covers files: snappy.c, snappy.h, snappy_compat.h   #
# originally retrieved from http://github.com/andikleen/snappy-c     #
# git revision 8015f2d28739b9a6076ebaa6c53fe27bc238d219              #
######################################################################

The snappy-c code is under the same license as the original snappy source

Copyright 2011 Intel Corporation All Rights Reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:

    * Redistributions of source code must retain the above copyright
notice, this list of conditions and the following disclaimer.
    * Redistributions in binary form must reproduce the above
copyright notice, this list of conditions and the following disclaimer
in the documentation and/or other materials provided with the
distribution.
    * Neither the name of Intel Corporation nor the names of its
contributors may be used to endorse or promote products derived from
this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.



LICENSE.tinycthread
--------------------------------------------------------------
From https://github.com/tinycthread/tinycthread/README.txt c57166cd510ffb5022dd5f127489b131b61441b9

License
-------

Copyright (c) 2012 Marcus Geelnard
              2013-2014 Evan Nemerson

This software is provided 'as-is', without any express or implied
warranty. In no event will the authors be held liable for any damages
arising from the use of this software.

Permission is granted to anyone to use this software for any purpose,
including commercial applications, and to alter it and redistribute it
freely, subject to the following restrictions:

    1. The origin of this software must not be misrepresented; you must not
    claim that you wrote the original software. If you use this software
    in a product, an acknowledgment in the product documentation would be
    appreciated but is not required.

    2. Altered source versions must be plainly marked as such, and must not be
    misrepresented as being the original software.

    3. This notice may not be removed or altered from any source
    distribution.


LICENSE.wingetopt
--------------------------------------------------------------
For the files wingetopt.c wingetopt.h downloaded from https://github.com/alex85k/wingetopt

/*
 * Copyright (c) 2002 Todd C. Miller <Todd.Miller@courtesan.com>
 *
 * Permission to use, copy, modify, and distribute this software for any
 * purpose with or without fee is hereby granted, provided that the above
 * copyright notice and this permission notice appear in all copies.
 *
 * THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
 * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
 * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
 * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
 * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
 * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
 * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.
 *
 * Sponsored in part by the Defense Advanced Research Projects
 * Agency (DARPA) and Air Force Research Laboratory, Air Force
 * Materiel Command, USAF, under agreement number F39502-99-1-0512.
 */
/*-
 * Copyright (c) 2000 The NetBSD Foundation, Inc.
 * All rights reserved.
 *
 * This code is derived from software contributed to The NetBSD Foundation
 * by Dieter Baron and Thomas Klausner.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 * 1. Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE NETBSD FOUNDATION, INC. AND CONTRIBUTORS
 * ``AS IS'' AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED
 * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE FOUNDATION OR CONTRIBUTORS
 * BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */



```

`IntelPTDriver/packages/librdkafka.redist.1.8.2/README.md`:

```md
librdkafka - the Apache Kafka C/C++ client library
==================================================

Copyright (c) 2012-2020, [Magnus Edenhill](http://www.edenhill.se/).

[https://github.com/edenhill/librdkafka](https://github.com/edenhill/librdkafka)

**librdkafka** is a C library implementation of the
[Apache Kafka](https://kafka.apache.org/) protocol, providing Producer, Consumer
and Admin clients. It was designed with message delivery reliability
and high performance in mind, current figures exceed 1 million msgs/second for
the producer and 3 million msgs/second for the consumer.

**librdkafka** is licensed under the 2-clause BSD license.

KAFKA is a registered trademark of The Apache Software Foundation and
has been licensed for use by librdkafka. librdkafka has no
affiliation with and is not endorsed by The Apache Software Foundation.


# Features #
  * Full Exactly-Once-Semantics (EOS) support
  * High-level producer, including Idempotent and Transactional producers
  * High-level balanced KafkaConsumer (requires broker >= 0.9)
  * Simple (legacy) consumer
  * Admin client
  * Compression: snappy, gzip, lz4, zstd
  * [SSL](https://github.com/edenhill/librdkafka/wiki/Using-SSL-with-librdkafka) support
  * [SASL](https://github.com/edenhill/librdkafka/wiki/Using-SASL-with-librdkafka) (GSSAPI/Kerberos/SSPI, PLAIN, SCRAM, OAUTHBEARER) support
  * Full list of [supported KIPs](INTRODUCTION.md#supported-kips)
  * Broker version support: >=0.8 (see [Broker version compatibility](INTRODUCTION.md#broker-version-compatibility))
  * Guaranteed API stability for C & C++ APIs (ABI safety guaranteed for C)
  * [Statistics](STATISTICS.md) metrics
  * Debian package: librdkafka1 and librdkafka-dev in Debian and Ubuntu
  * RPM package: librdkafka and librdkafka-devel
  * Gentoo package: dev-libs/librdkafka
  * Portable: runs on Linux, MacOS X, Windows, Solaris, FreeBSD, AIX, ...

# Documentation

 * Public API in [C header](src/rdkafka.h) and [C++ header](src-cpp/rdkafkacpp.h).
 * Introduction and manual in [INTRODUCTION.md](https://github.com/edenhill/librdkafka/blob/master/INTRODUCTION.md).
 * Configuration properties in
[CONFIGURATION.md](https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md).
 * Statistics metrics in [STATISTICS.md](https://github.com/edenhill/librdkafka/blob/master/STATISTICS.md).
 * [Frequently asked questions](https://github.com/edenhill/librdkafka/wiki).

**NOTE**: The `master` branch is actively developed, use latest [release](https://github.com/edenhill/librdkafka/releases) for production use.


# Installation

## Installing prebuilt packages

On Mac OSX, install librdkafka with homebrew:

```bash
$ brew install librdkafka
```

On Debian and Ubuntu, install librdkafka from the Confluent APT repositories,
see instructions [here](https://docs.confluent.io/current/installation/installing_cp/deb-ubuntu.html#get-the-software) and then install librdkafka:

 ```bash
 $ apt install librdkafka-dev
 ```

On RedHat, CentOS, Fedora, install librdkafka from the Confluent YUM repositories,
instructions [here](https://docs.confluent.io/current/installation/installing_cp/rhel-centos.html#get-the-software) and then install librdkafka:

```bash
$ yum install librdkafka-devel
```

On Windows, reference [librdkafka.redist](https://www.nuget.org/packages/librdkafka.redist/) NuGet package in your Visual Studio project.


For other platforms, follow the source building instructions below.


## Installing librdkafka using vcpkg

You can download and install librdkafka using the [vcpkg](https://github.com/Microsoft/vcpkg) dependency manager:

```bash
# Install vcpkg if not already installed
$ git clone https://github.com/Microsoft/vcpkg.git
$ cd vcpkg
$ ./bootstrap-vcpkg.sh
$ ./vcpkg integrate install

# Install librdkafka
$ vcpkg install librdkafka
```

The librdkafka package in vcpkg is kept up to date by Microsoft team members and community contributors.
If the version is out of date, please [create an issue or pull request](https://github.com/Microsoft/vcpkg) on the vcpkg repository.


## Build from source

### Requirements
	The GNU toolchain
	GNU make
   	pthreads
	zlib-dev (optional, for gzip compression support)
	libssl-dev (optional, for SSL and SASL SCRAM support)
	libsasl2-dev (optional, for SASL GSSAPI support)
	libzstd-dev (optional, for ZStd compression support)

**NOTE**: Static linking of ZStd (requires zstd >= 1.2.1) in the producer
          enables encoding the original size in the compression frame header,
          which will speed up the consumer.
          Use `STATIC_LIB_libzstd=/path/to/libzstd.a ./configure --enable-static`
          to enable static ZStd linking.
          MacOSX example:
          `STATIC_LIB_libzstd=$(brew ls -v zstd | grep libzstd.a$) ./configure --enable-static`


### Building

      ./configure
      # Or, to automatically install dependencies using the system's package manager:
      # ./configure --install-deps
      # Or, build dependencies from source:
      # ./configure --install-deps --source-deps-only

      make
      sudo make install


**NOTE**: See [README.win32](README.win32) for instructions how to build
          on Windows with Microsoft Visual Studio.

**NOTE**: See [CMake instructions](packaging/cmake/README.md) for experimental
          CMake build (unsupported).


## Usage in code

1. Refer to the [examples directory](examples/) for code using:

* Producers: basic producers, idempotent producers, transactional producers.
* Consumers: basic consumers, reading batches of messages.
* Performance and latency testing tools.

2. Refer to the [examples GitHub repo](https://github.com/confluentinc/examples/tree/master/clients/cloud/c) for code connecting to a cloud streaming data service based on Apache Kafka

3. Link your program with `-lrdkafka` (C) or `-lrdkafka++` (C++).


## Commercial support

Commercial support is available from [Confluent Inc](https://www.confluent.io/)


## Community support

**Only the [last official release](https://github.com/edenhill/librdkafka/releases) is supported for community members.**

File bug reports, feature requests and questions using
[GitHub Issues](https://github.com/edenhill/librdkafka/issues)

Questions and discussions are also welcome on the [Confluent Community slack](https://launchpass.com/confluentcommunity) #clients channel.


# Language bindings #

  * C#/.NET: [confluent-kafka-dotnet](https://github.com/confluentinc/confluent-kafka-dotnet) (based on [rdkafka-dotnet](https://github.com/ah-/rdkafka-dotnet))
  * C++: [cppkafka](https://github.com/mfontanini/cppkafka)
  * C++: [modern-cpp-kafka](https://github.com/Morgan-Stanley/modern-cpp-kafka)
  * Common Lisp: [cl-rdkafka](https://github.com/SahilKang/cl-rdkafka)
  * D (C-like): [librdkafka](https://github.com/DlangApache/librdkafka/)
  * D (C++-like): [librdkafkad](https://github.com/tamediadigital/librdkafka-d)
  * Erlang: [erlkaf](https://github.com/silviucpp/erlkaf)
  * Go: [confluent-kafka-go](https://github.com/confluentinc/confluent-kafka-go)
  * Haskell (kafka, conduit, avro, schema registry): [hw-kafka](https://github.com/haskell-works/hw-kafka)
  * Lua: [luardkafka](https://github.com/mistsv/luardkafka)
  * Node.js: [node-rdkafka](https://github.com/Blizzard/node-rdkafka)
  * OCaml: [ocaml-kafka](https://github.com/didier-wenzek/ocaml-kafka)
  * Perl: [Net::Kafka](https://github.com/bookingcom/perl-Net-Kafka)
  * PHP: [php-rdkafka](https://github.com/arnaud-lb/php-rdkafka)
  * PHP: [php-simple-kafka-client](https://github.com/php-kafka/php-simple-kafka-client)
  * Python: [confluent-kafka-python](https://github.com/confluentinc/confluent-kafka-python)
  * Python: [PyKafka](https://github.com/Parsely/pykafka)
  * Ruby: [Hermann](https://github.com/reiseburo/hermann)
  * Ruby: [rdkafka-ruby](https://github.com/appsignal/rdkafka-ruby)
  * Rust: [rust-rdkafka](https://github.com/fede1024/rust-rdkafka)
  * Tcl: [KafkaTcl](https://github.com/flightaware/kafkatcl)
  * Shell: [kafkacat](https://github.com/edenhill/kafkacat) - Apache Kafka command line tool
  * Swift: [Perfect-Kafka](https://github.com/PerfectlySoft/Perfect-Kafka)


See [Powered by librdkafka](https://github.com/edenhill/librdkafka/wiki/Powered-by-librdkafka) for an incomplete list of librdkafka users.

```

`IntelPTDriver/packages/librdkafka.redist.1.8.2/build/librdkafka.redist.props`:

```props
<?xml version="1.0" encoding="utf-8" ?>
<Project ToolsVersion="12.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <ItemGroup>
    <Content Include="$(MSBuildThisFileDirectory)..\runtimes\win-x86\native\*">
      <Link>librdkafka\x86\%(Filename)%(Extension)</Link>
      <CopyToOutputDirectory>PreserveNewest</CopyToOutputDirectory>
    </Content>
    <Content Include="$(MSBuildThisFileDirectory)..\runtimes\win-x64\native\*">
      <Link>librdkafka\x64\%(Filename)%(Extension)</Link>
      <CopyToOutputDirectory>PreserveNewest</CopyToOutputDirectory>
    </Content>
  </ItemGroup>
  <ItemDefinitionGroup>
    <ClCompile>
      <AdditionalIncludeDirectories>$(MSBuildThisFileDirectory)include;%(AdditionalIncludeDirectories)</AdditionalIncludeDirectories>
    </ClCompile>
  </ItemDefinitionGroup>
</Project>

```

`IntelPTDriver/packages/librdkafka.redist.1.8.2/build/native/include/librdkafka/rdkafka.h`:

```h
/*
 * librdkafka - Apache Kafka C library
 *
 * Copyright (c) 2012-2020 Magnus Edenhill
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice,
 *    this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 *    this list of conditions and the following disclaimer in the documentation
 *    and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/**
 * @file rdkafka.h
 * @brief Apache Kafka C/C++ consumer and producer client library.
 *
 * rdkafka.h contains the public API for librdkafka.
 * The API is documented in this file as comments prefixing the function, type,
 * enum, define, etc.
 *
 * @sa For the C++ interface see rdkafkacpp.h
 *
 * @tableofcontents
 */


/* @cond NO_DOC */
#ifndef _RDKAFKA_H_
#define _RDKAFKA_H_

#include <stdio.h>
#include <inttypes.h>
#include <sys/types.h>

#ifdef __cplusplus
extern "C" {
#if 0
} /* Restore indent */
#endif
#endif

#ifdef _WIN32
#include <basetsd.h>
#ifndef WIN32_MEAN_AND_LEAN
#define WIN32_MEAN_AND_LEAN
#endif
#include <winsock2.h>  /* for sockaddr, .. */
#ifndef _SSIZE_T_DEFINED
#define _SSIZE_T_DEFINED
typedef SSIZE_T ssize_t;
#endif
#define RD_UNUSED
#define RD_INLINE __inline
#define RD_DEPRECATED __declspec(deprecated)
#define RD_FORMAT(...)
#undef RD_EXPORT
#ifdef LIBRDKAFKA_STATICLIB
#define RD_EXPORT
#else
#ifdef LIBRDKAFKA_EXPORTS
#define RD_EXPORT __declspec(dllexport)
#else
#define RD_EXPORT __declspec(dllimport)
#endif
#ifndef LIBRDKAFKA_TYPECHECKS
#define LIBRDKAFKA_TYPECHECKS 0
#endif
#endif

#else
#include <sys/socket.h> /* for sockaddr, .. */

#define RD_UNUSED __attribute__((unused))
#define RD_INLINE inline
#define RD_EXPORT
#define RD_DEPRECATED __attribute__((deprecated))

#if defined(__clang__) || defined(__GNUC__) || defined(__GNUG__)
#define RD_FORMAT(...) __attribute__((format (__VA_ARGS__)))
#else
#define RD_FORMAT(...)
#endif

#ifndef LIBRDKAFKA_TYPECHECKS
#define LIBRDKAFKA_TYPECHECKS 1
#endif
#endif


/**
 * @brief Type-checking macros
 * Compile-time checking that \p ARG is of type \p TYPE.
 * @returns \p RET
 */
#if LIBRDKAFKA_TYPECHECKS
#define _LRK_TYPECHECK(RET,TYPE,ARG)                    \
        ({ if (0) { TYPE __t RD_UNUSED = (ARG); } RET; })

#define _LRK_TYPECHECK2(RET,TYPE,ARG,TYPE2,ARG2)        \
        ({                                              \
                if (0) {                                \
                        TYPE __t RD_UNUSED = (ARG);     \
                        TYPE2 __t2 RD_UNUSED = (ARG2);  \
                }                                       \
                RET; })

#define _LRK_TYPECHECK3(RET,TYPE,ARG,TYPE2,ARG2,TYPE3,ARG3) \
        ({                                              \
                if (0) {                                \
                        TYPE __t RD_UNUSED = (ARG);     \
                        TYPE2 __t2 RD_UNUSED = (ARG2);  \
                        TYPE3 __t3 RD_UNUSED = (ARG3);  \
                }                                       \
                RET; })
#else
#define _LRK_TYPECHECK(RET,TYPE,ARG)  (RET)
#define _LRK_TYPECHECK2(RET,TYPE,ARG,TYPE2,ARG2) (RET)
#define _LRK_TYPECHECK3(RET,TYPE,ARG,TYPE2,ARG2,TYPE3,ARG3) (RET)
#endif

/* @endcond */


/**
 * @name librdkafka version
 * @{
 *
 *
 */

/**
 * @brief librdkafka version
 *
 * Interpreted as hex \c MM.mm.rr.xx:
 *  - MM = Major
 *  - mm = minor
 *  - rr = revision
 *  - xx = pre-release id (0xff is the final release)
 *
 * E.g.: \c 0x000801ff = 0.8.1
 *
 * @remark This value should only be used during compile time,
 *         for runtime checks of version use rd_kafka_version()
 */
#define RD_KAFKA_VERSION  0x010802ff

/**
 * @brief Returns the librdkafka version as integer.
 *
 * @returns Version integer.
 *
 * @sa See RD_KAFKA_VERSION for how to parse the integer format.
 * @sa Use rd_kafka_version_str() to retreive the version as a string.
 */
RD_EXPORT
int rd_kafka_version(void);

/**
 * @brief Returns the librdkafka version as string.
 *
 * @returns Version string
 */
RD_EXPORT
const char *rd_kafka_version_str (void);

/**@}*/


/**
 * @name Constants, errors, types
 * @{
 *
 *
 */


/**
 * @enum rd_kafka_type_t
 *
 * @brief rd_kafka_t handle type.
 *
 * @sa rd_kafka_new()
 */
typedef enum rd_kafka_type_t {
	RD_KAFKA_PRODUCER, /**< Producer client */
	RD_KAFKA_CONSUMER  /**< Consumer client */
} rd_kafka_type_t;


/*!
 * Timestamp types
 *
 * @sa rd_kafka_message_timestamp()
 */
typedef enum rd_kafka_timestamp_type_t {
	RD_KAFKA_TIMESTAMP_NOT_AVAILABLE,   /**< Timestamp not available */
	RD_KAFKA_TIMESTAMP_CREATE_TIME,     /**< Message creation time */
	RD_KAFKA_TIMESTAMP_LOG_APPEND_TIME  /**< Log append time */
} rd_kafka_timestamp_type_t;



/**
 * @brief Retrieve supported debug contexts for use with the \c \"debug\"
 *        configuration property. (runtime)
 *
 * @returns Comma-separated list of available debugging contexts.
 */
RD_EXPORT
const char *rd_kafka_get_debug_contexts(void);

/**
 * @brief Supported debug contexts. (compile time)
 *
 * @deprecated This compile time value may be outdated at runtime due to
 *             linking another version of the library.
 *             Use rd_kafka_get_debug_contexts() instead.
 */
#define RD_KAFKA_DEBUG_CONTEXTS \
        "all,generic,broker,topic,metadata,feature,queue,msg,protocol,cgrp,security,fetch,interceptor,plugin,consumer,admin,eos,mock,assignor,conf"


/* @cond NO_DOC */
/* Private types to provide ABI compatibility */
typedef struct rd_kafka_s rd_kafka_t;
typedef struct rd_kafka_topic_s rd_kafka_topic_t;
typedef struct rd_kafka_conf_s rd_kafka_conf_t;
typedef struct rd_kafka_topic_conf_s rd_kafka_topic_conf_t;
typedef struct rd_kafka_queue_s rd_kafka_queue_t;
typedef struct rd_kafka_op_s rd_kafka_event_t;
typedef struct rd_kafka_topic_result_s rd_kafka_topic_result_t;
typedef struct rd_kafka_consumer_group_metadata_s
rd_kafka_consumer_group_metadata_t;
typedef struct rd_kafka_error_s rd_kafka_error_t;
typedef struct rd_kafka_headers_s rd_kafka_headers_t;
typedef struct rd_kafka_group_result_s rd_kafka_group_result_t;
/* @endcond */


/**
 * @enum rd_kafka_resp_err_t
 * @brief Error codes.
 *
 * The negative error codes delimited by two underscores
 * (\c RD_KAFKA_RESP_ERR__..) denotes errors internal to librdkafka and are
 * displayed as \c \"Local: \<error string..\>\", while the error codes
 * delimited by a single underscore (\c RD_KAFKA_RESP_ERR_..) denote broker
 * errors and are displayed as \c \"Broker: \<error string..\>\".
 *
 * @sa Use rd_kafka_err2str() to translate an error code a human readable string
 */
typedef enum {
	/* Internal errors to rdkafka: */
	/** Begin internal error codes */
	RD_KAFKA_RESP_ERR__BEGIN = -200,
	/** Received message is incorrect */
	RD_KAFKA_RESP_ERR__BAD_MSG = -199,
	/** Bad/unknown compression */
	RD_KAFKA_RESP_ERR__BAD_COMPRESSION = -198,
	/** Broker is going away */
	RD_KAFKA_RESP_ERR__DESTROY = -197,
	/** Generic failure */
	RD_KAFKA_RESP_ERR__FAIL = -196,
	/** Broker transport failure */
	RD_KAFKA_RESP_ERR__TRANSPORT = -195,
	/** Critical system resource */
	RD_KAFKA_RESP_ERR__CRIT_SYS_RESOURCE = -194,
	/** Failed to resolve broker */
	RD_KAFKA_RESP_ERR__RESOLVE = -193,
	/** Produced message timed out*/
	RD_KAFKA_RESP_ERR__MSG_TIMED_OUT = -192,
	/** Reached the end of the topic+partition queue on
	 * the broker. Not really an error.
	 * This event is disabled by default,
	 * see the `enable.partition.eof` configuration property. */
	RD_KAFKA_RESP_ERR__PARTITION_EOF = -191,
	/** Permanent: Partition does not exist in cluster. */
	RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION = -190,
	/** File or filesystem error */
	RD_KAFKA_RESP_ERR__FS = -189,
	 /** Permanent: Topic does not exist in cluster. */
	RD_KAFKA_RESP_ERR__UNKNOWN_TOPIC = -188,
	/** All broker connections are down. */
	RD_KAFKA_RESP_ERR__ALL_BROKERS_DOWN = -187,
	/** Invalid argument, or invalid configuration */
	RD_KAFKA_RESP_ERR__INVALID_ARG = -186,
	/** Operation timed out */
	RD_KAFKA_RESP_ERR__TIMED_OUT = -185,
	/** Queue is full */
	RD_KAFKA_RESP_ERR__QUEUE_FULL = -184,
	/** ISR count < required.acks */
        RD_KAFKA_RESP_ERR__ISR_INSUFF = -183,
	/** Broker node update */
        RD_KAFKA_RESP_ERR__NODE_UPDATE = -182,
	/** SSL error */
	RD_KAFKA_RESP_ERR__SSL = -181,
	/** Waiting for coordinator to become available. */
        RD_KAFKA_RESP_ERR__WAIT_COORD = -180,
	/** Unknown client group */
        RD_KAFKA_RESP_ERR__UNKNOWN_GROUP = -179,
	/** Operation in progress */
        RD_KAFKA_RESP_ERR__IN_PROGRESS = -178,
	 /** Previous operation in progress, wait for it to finish. */
        RD_KAFKA_RESP_ERR__PREV_IN_PROGRESS = -177,
	 /** This operation would interfere with an existing subscription */
        RD_KAFKA_RESP_ERR__EXISTING_SUBSCRIPTION = -176,
	/** Assigned partitions (rebalance_cb) */
        RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS = -175,
	/** Revoked partitions (rebalance_cb) */
        RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS = -174,
	/** Conflicting use */
        RD_KAFKA_RESP_ERR__CONFLICT = -173,
	/** Wrong state */
        RD_KAFKA_RESP_ERR__STATE = -172,
	/** Unknown protocol */
        RD_KAFKA_RESP_ERR__UNKNOWN_PROTOCOL = -171,
	/** Not implemented */
        RD_KAFKA_RESP_ERR__NOT_IMPLEMENTED = -170,
	/** Authentication failure*/
	RD_KAFKA_RESP_ERR__AUTHENTICATION = -169,
	/** No stored offset */
	RD_KAFKA_RESP_ERR__NO_OFFSET = -168,
	/** Outdated */
	RD_KAFKA_RESP_ERR__OUTDATED = -167,
	/** Timed out in queue */
	RD_KAFKA_RESP_ERR__TIMED_OUT_QUEUE = -166,
        /** Feature not supported by broker */
        RD_KAFKA_RESP_ERR__UNSUPPORTED_FEATURE = -165,
        /** Awaiting cache update */
        RD_KAFKA_RESP_ERR__WAIT_CACHE = -164,
        /** Operation interrupted (e.g., due to yield)) */
        RD_KAFKA_RESP_ERR__INTR = -163,
        /** Key serialization error */
        RD_KAFKA_RESP_ERR__KEY_SERIALIZATION = -162,
        /** Value serialization error */
        RD_KAFKA_RESP_ERR__VALUE_SERIALIZATION = -161,
        /** Key deserialization error */
        RD_KAFKA_RESP_ERR__KEY_DESERIALIZATION = -160,
        /** Value deserialization error */
        RD_KAFKA_RESP_ERR__VALUE_DESERIALIZATION = -159,
        /** Partial response */
        RD_KAFKA_RESP_ERR__PARTIAL = -158,
        /** Modification attempted on read-only object */
        RD_KAFKA_RESP_ERR__READ_ONLY = -157,
        /** No such entry / item not found */
        RD_KAFKA_RESP_ERR__NOENT = -156,
        /** Read underflow */
        RD_KAFKA_RESP_ERR__UNDERFLOW = -155,
        /** Invalid type */
        RD_KAFKA_RESP_ERR__INVALID_TYPE = -154,
        /** Retry operation */
        RD_KAFKA_RESP_ERR__RETRY = -153,
        /** Purged in queue */
        RD_KAFKA_RESP_ERR__PURGE_QUEUE = -152,
        /** Purged in flight */
        RD_KAFKA_RESP_ERR__PURGE_INFLIGHT = -151,
        /** Fatal error: see rd_kafka_fatal_error() */
        RD_KAFKA_RESP_ERR__FATAL = -150,
        /** Inconsistent state */
        RD_KAFKA_RESP_ERR__INCONSISTENT = -149,
        /** Gap-less ordering would not be guaranteed if proceeding */
        RD_KAFKA_RESP_ERR__GAPLESS_GUARANTEE = -148,
        /** Maximum poll interval exceeded */
        RD_KAFKA_RESP_ERR__MAX_POLL_EXCEEDED = -147,
        /** Unknown broker */
        RD_KAFKA_RESP_ERR__UNKNOWN_BROKER = -146,
        /** Functionality not configured */
        RD_KAFKA_RESP_ERR__NOT_CONFIGURED = -145,
        /** Instance has been fenced */
        RD_KAFKA_RESP_ERR__FENCED = -144,
        /** Application generated error */
        RD_KAFKA_RESP_ERR__APPLICATION = -143,
        /** Assignment lost */
        RD_KAFKA_RESP_ERR__ASSIGNMENT_LOST = -142,
        /** No operation performed */
        RD_KAFKA_RESP_ERR__NOOP = -141,
        /** No offset to automatically reset to */
        RD_KAFKA_RESP_ERR__AUTO_OFFSET_RESET = -140,

	/** End internal error codes */
	RD_KAFKA_RESP_ERR__END = -100,

	/* Kafka broker errors: */
	/** Unknown broker error */
	RD_KAFKA_RESP_ERR_UNKNOWN = -1,
	/** Success */
	RD_KAFKA_RESP_ERR_NO_ERROR = 0,
	/** Offset out of range */
	RD_KAFKA_RESP_ERR_OFFSET_OUT_OF_RANGE = 1,
	/** Invalid message */
	RD_KAFKA_RESP_ERR_INVALID_MSG = 2,
	/** Unknown topic or partition */
	RD_KAFKA_RESP_ERR_UNKNOWN_TOPIC_OR_PART = 3,
	/** Invalid message size */
	RD_KAFKA_RESP_ERR_INVALID_MSG_SIZE = 4,
	/** Leader not available */
	RD_KAFKA_RESP_ERR_LEADER_NOT_AVAILABLE = 5,
	/** Not leader for partition */
	RD_KAFKA_RESP_ERR_NOT_LEADER_FOR_PARTITION = 6,
	/** Request timed out */
	RD_KAFKA_RESP_ERR_REQUEST_TIMED_OUT = 7,
	/** Broker not available */
	RD_KAFKA_RESP_ERR_BROKER_NOT_AVAILABLE = 8,
	/** Replica not available */
	RD_KAFKA_RESP_ERR_REPLICA_NOT_AVAILABLE = 9,
	/** Message size too large */
	RD_KAFKA_RESP_ERR_MSG_SIZE_TOO_LARGE = 10,
	/** StaleControllerEpochCode */
	RD_KAFKA_RESP_ERR_STALE_CTRL_EPOCH = 11,
	/** Offset metadata string too large */
	RD_KAFKA_RESP_ERR_OFFSET_METADATA_TOO_LARGE = 12,
	/** Broker disconnected before response received */
	RD_KAFKA_RESP_ERR_NETWORK_EXCEPTION = 13,
        /** Coordinator load in progress */
        RD_KAFKA_RESP_ERR_COORDINATOR_LOAD_IN_PROGRESS = 14,
        /** Group coordinator load in progress */
#define RD_KAFKA_RESP_ERR_GROUP_LOAD_IN_PROGRESS        \
        RD_KAFKA_RESP_ERR_COORDINATOR_LOAD_IN_PROGRESS
        /** Coordinator not available */
        RD_KAFKA_RESP_ERR_COORDINATOR_NOT_AVAILABLE = 15,
        /** Group coordinator not available */
#define RD_KAFKA_RESP_ERR_GROUP_COORDINATOR_NOT_AVAILABLE       \
        RD_KAFKA_RESP_ERR_COORDINATOR_NOT_AVAILABLE
        /** Not coordinator */
        RD_KAFKA_RESP_ERR_NOT_COORDINATOR = 16,
        /** Not coordinator for group */
#define RD_KAFKA_RESP_ERR_NOT_COORDINATOR_FOR_GROUP     \
        RD_KAFKA_RESP_ERR_NOT_COORDINATOR
	/** Invalid topic */
        RD_KAFKA_RESP_ERR_TOPIC_EXCEPTION = 17,
	/** Message batch larger than configured server segment size */
        RD_KAFKA_RESP_ERR_RECORD_LIST_TOO_LARGE = 18,
	/** Not enough in-sync replicas */
        RD_KAFKA_RESP_ERR_NOT_ENOUGH_REPLICAS = 19,
	/** Message(s) written to insufficient number of in-sync replicas */
        RD_KAFKA_RESP_ERR_NOT_ENOUGH_REPLICAS_AFTER_APPEND = 20,
	/** Invalid required acks value */
        RD_KAFKA_RESP_ERR_INVALID_REQUIRED_ACKS = 21,
	/** Specified group generation id is not valid */
        RD_KAFKA_RESP_ERR_ILLEGAL_GENERATION = 22,
	/** Inconsistent group protocol */
        RD_KAFKA_RESP_ERR_INCONSISTENT_GROUP_PROTOCOL = 23,
	/** Invalid group.id */
	RD_KAFKA_RESP_ERR_INVALID_GROUP_ID = 24,
	/** Unknown member */
        RD_KAFKA_RESP_ERR_UNKNOWN_MEMBER_ID = 25,
	/** Invalid session timeout */
        RD_KAFKA_RESP_ERR_INVALID_SESSION_TIMEOUT = 26,
	/** Group rebalance in progress */
	RD_KAFKA_RESP_ERR_REBALANCE_IN_PROGRESS = 27,
	/** Commit offset data size is not valid */
        RD_KAFKA_RESP_ERR_INVALID_COMMIT_OFFSET_SIZE = 28,
	/** Topic authorization failed */
        RD_KAFKA_RESP_ERR_TOPIC_AUTHORIZATION_FAILED = 29,
	/** Group authorization failed */
	RD_KAFKA_RESP_ERR_GROUP_AUTHORIZATION_FAILED = 30,
	/** Cluster authorization failed */
	RD_KAFKA_RESP_ERR_CLUSTER_AUTHORIZATION_FAILED = 31,
	/** Invalid timestamp */
	RD_KAFKA_RESP_ERR_INVALID_TIMESTAMP = 32,
	/** Unsupported SASL mechanism */
	RD_KAFKA_RESP_ERR_UNSUPPORTED_SASL_MECHANISM = 33,
	/** Illegal SASL state */
	RD_KAFKA_RESP_ERR_ILLEGAL_SASL_STATE = 34,
	/** Unuspported version */
	RD_KAFKA_RESP_ERR_UNSUPPORTED_VERSION = 35,
	/** Topic already exists */
	RD_KAFKA_RESP_ERR_TOPIC_ALREADY_EXISTS = 36,
	/** Invalid number of partitions */
	RD_KAFKA_RESP_ERR_INVALID_PARTITIONS = 37,
	/** Invalid replication factor */
	RD_KAFKA_RESP_ERR_INVALID_REPLICATION_FACTOR = 38,
	/** Invalid replica assignment */
	RD_KAFKA_RESP_ERR_INVALID_REPLICA_ASSIGNMENT = 39,
	/** Invalid config */
	RD_KAFKA_RESP_ERR_INVALID_CONFIG = 40,
	/** Not controller for cluster */
	RD_KAFKA_RESP_ERR_NOT_CONTROLLER = 41,
	/** Invalid request */
	RD_KAFKA_RESP_ERR_INVALID_REQUEST = 42,
	/** Message format on broker does not support request */
	RD_KAFKA_RESP_ERR_UNSUPPORTED_FOR_MESSAGE_FORMAT = 43,
        /** Policy violation */
        RD_KAFKA_RESP_ERR_POLICY_VIOLATION = 44,
        /** Broker received an out of order sequence number */
        RD_KAFKA_RESP_ERR_OUT_OF_ORDER_SEQUENCE_NUMBER = 45,
        /** Broker received a duplicate sequence number */
        RD_KAFKA_RESP_ERR_DUPLICATE_SEQUENCE_NUMBER = 46,
        /** Producer attempted an operation with an old epoch */
        RD_KAFKA_RESP_ERR_INVALID_PRODUCER_EPOCH = 47,
        /** Producer attempted a transactional operation in an invalid state */
        RD_KAFKA_RESP_ERR_INVALID_TXN_STATE = 48,
        /** Producer attempted to use a producer id which is not
         *  currently assigned to its transactional id */
        RD_KAFKA_RESP_ERR_INVALID_PRODUCER_ID_MAPPING = 49,
        /** Transaction timeout is larger than the maximum
         *  value allowed by the broker's max.transaction.timeout.ms */
        RD_KAFKA_RESP_ERR_INVALID_TRANSACTION_TIMEOUT = 50,
        /** Producer attempted to update a transaction while another
         *  concurrent operation on the same transaction was ongoing */
        RD_KAFKA_RESP_ERR_CONCURRENT_TRANSACTIONS = 51,
        /** Indicates that the transaction coordinator sending a
         *  WriteTxnMarker is no longer the current coordinator for a
         *  given producer */
        RD_KAFKA_RESP_ERR_TRANSACTION_COORDINATOR_FENCED = 52,
        /** Transactional Id authorization failed */
        RD_KAFKA_RESP_ERR_TRANSACTIONAL_ID_AUTHORIZATION_FAILED = 53,
        /** Security features are disabled */
        RD_KAFKA_RESP_ERR_SECURITY_DISABLED = 54,
        /** Operation not attempted */
        RD_KAFKA_RESP_ERR_OPERATION_NOT_ATTEMPTED = 55,
        /** Disk error when trying to access log file on the disk */
        RD_KAFKA_RESP_ERR_KAFKA_STORAGE_ERROR = 56,
        /** The user-specified log directory is not found in the broker config */
        RD_KAFKA_RESP_ERR_LOG_DIR_NOT_FOUND = 57,
        /** SASL Authentication failed */
        RD_KAFKA_RESP_ERR_SASL_AUTHENTICATION_FAILED = 58,
        /** Unknown Producer Id */
        RD_KAFKA_RESP_ERR_UNKNOWN_PRODUCER_ID = 59,
        /** Partition reassignment is in progress */
        RD_KAFKA_RESP_ERR_REASSIGNMENT_IN_PROGRESS = 60,
        /** Delegation Token feature is not enabled */
        RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_AUTH_DISABLED = 61,
        /** Delegation Token is not found on server */
        RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_NOT_FOUND = 62,
        /** Specified Principal is not valid Owner/Renewer */
        RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_OWNER_MISMATCH = 63,
        /** Delegation Token requests are not allowed on this connection */
        RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_REQUEST_NOT_ALLOWED = 64,
        /** Delegation Token authorization failed */
        RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_AUTHORIZATION_FAILED = 65,
        /** Delegation Token is expired */
        RD_KAFKA_RESP_ERR_DELEGATION_TOKEN_EXPIRED = 66,
        /** Supplied principalType is not supported */
        RD_KAFKA_RESP_ERR_INVALID_PRINCIPAL_TYPE = 67,
        /** The group is not empty */
        RD_KAFKA_RESP_ERR_NON_EMPTY_GROUP = 68,
        /** The group id does not exist */
        RD_KAFKA_RESP_ERR_GROUP_ID_NOT_FOUND = 69,
        /** The fetch session ID was not found */
        RD_KAFKA_RESP_ERR_FETCH_SESSION_ID_NOT_FOUND = 70,
        /** The fetch session epoch is invalid */
        RD_KAFKA_RESP_ERR_INVALID_FETCH_SESSION_EPOCH = 71,
        /** No matching listener */
        RD_KAFKA_RESP_ERR_LISTENER_NOT_FOUND = 72,
        /** Topic deletion is disabled */
        RD_KAFKA_RESP_ERR_TOPIC_DELETION_DISABLED = 73,
        /** Leader epoch is older than broker epoch */
        RD_KAFKA_RESP_ERR_FENCED_LEADER_EPOCH = 74,
        /** Leader epoch is newer than broker epoch */
        RD_KAFKA_RESP_ERR_UNKNOWN_LEADER_EPOCH = 75,
        /** Unsupported compression type */
        RD_KAFKA_RESP_ERR_UNSUPPORTED_COMPRESSION_TYPE = 76,
        /** Broker epoch has changed */
        RD_KAFKA_RESP_ERR_STALE_BROKER_EPOCH = 77,
        /** Leader high watermark is not caught up */
        RD_KAFKA_RESP_ERR_OFFSET_NOT_AVAILABLE = 78,
        /** Group member needs a valid member ID */
        RD_KAFKA_RESP_ERR_MEMBER_ID_REQUIRED = 79,
        /** Preferred leader was not available */
        RD_KAFKA_RESP_ERR_PREFERRED_LEADER_NOT_AVAILABLE = 80,
        /** Consumer group has reached maximum size */
        RD_KAFKA_RESP_ERR_GROUP_MAX_SIZE_REACHED = 81,
        /** Static consumer fenced by other consumer with same
         *  group.instance.id. */
        RD_KAFKA_RESP_ERR_FENCED_INSTANCE_ID = 82,
        /** Eligible partition leaders are not available */
        RD_KAFKA_RESP_ERR_ELIGIBLE_LEADERS_NOT_AVAILABLE = 83,
        /** Leader election not needed for topic partition */
        RD_KAFKA_RESP_ERR_ELECTION_NOT_NEEDED = 84,
        /** No partition reassignment is in progress */
        RD_KAFKA_RESP_ERR_NO_REASSIGNMENT_IN_PROGRESS = 85,
        /** Deleting offsets of a topic while the consumer group is
         *  subscribed to it */
        RD_KAFKA_RESP_ERR_GROUP_SUBSCRIBED_TO_TOPIC = 86,
        /** Broker failed to validate record */
        RD_KAFKA_RESP_ERR_INVALID_RECORD = 87,
        /** There are unstable offsets that need to be cleared */
        RD_KAFKA_RESP_ERR_UNSTABLE_OFFSET_COMMIT = 88,
        /** Throttling quota has been exceeded */
        RD_KAFKA_RESP_ERR_THROTTLING_QUOTA_EXCEEDED = 89,
        /** There is a newer producer with the same transactionalId
         *  which fences the current one */
        RD_KAFKA_RESP_ERR_PRODUCER_FENCED = 90,
        /** Request illegally referred to resource that does not exist */
        RD_KAFKA_RESP_ERR_RESOURCE_NOT_FOUND = 91,
        /** Request illegally referred to the same resource twice */
        RD_KAFKA_RESP_ERR_DUPLICATE_RESOURCE = 92,
        /** Requested credential would not meet criteria for acceptability */
        RD_KAFKA_RESP_ERR_UNACCEPTABLE_CREDENTIAL = 93,
        /** Indicates that the either the sender or recipient of a
         *  voter-only request is not one of the expected voters */
        RD_KAFKA_RESP_ERR_INCONSISTENT_VOTER_SET = 94,
        /** Invalid update version */
        RD_KAFKA_RESP_ERR_INVALID_UPDATE_VERSION = 95,
        /** Unable to update finalized features due to server error */
        RD_KAFKA_RESP_ERR_FEATURE_UPDATE_FAILED = 96,
        /** Request principal deserialization failed during forwarding */
        RD_KAFKA_RESP_ERR_PRINCIPAL_DESERIALIZATION_FAILURE = 97,

        RD_KAFKA_RESP_ERR_END_ALL,
} rd_kafka_resp_err_t;


/**
 * @brief Error code value, name and description.
 *        Typically for use with language bindings to automatically expose
 *        the full set of librdkafka error codes.
 */
struct rd_kafka_err_desc {
	rd_kafka_resp_err_t code;/**< Error code */
	const char *name;      /**< Error name, same as code enum sans prefix */
	const char *desc;      /**< Human readable error description. */
};


/**
 * @brief Returns the full list of error codes.
 */
RD_EXPORT
void rd_kafka_get_err_descs (const struct rd_kafka_err_desc **errdescs,
			     size_t *cntp);




/**
 * @brief Returns a human readable representation of a kafka error.
 *
 * @param err Error code to translate
 */
RD_EXPORT
const char *rd_kafka_err2str (rd_kafka_resp_err_t err);



/**
 * @brief Returns the error code name (enum name).
 *
 * @param err Error code to translate
 */
RD_EXPORT
const char *rd_kafka_err2name (rd_kafka_resp_err_t err);


/**
 * @brief Returns the last error code generated by a legacy API call
 *        in the current thread.
 *
 * The legacy APIs are the ones using errno to propagate error value, namely:
 *  - rd_kafka_topic_new()
 *  - rd_kafka_consume_start()
 *  - rd_kafka_consume_stop()
 *  - rd_kafka_consume()
 *  - rd_kafka_consume_batch()
 *  - rd_kafka_consume_callback()
 *  - rd_kafka_consume_queue()
 *  - rd_kafka_produce()
 *
 * The main use for this function is to avoid converting system \p errno
 * values to rd_kafka_resp_err_t codes for legacy APIs.
 *
 * @remark The last error is stored per-thread, if multiple rd_kafka_t handles
 *         are used in the same application thread the developer needs to
 *         make sure rd_kafka_last_error() is called immediately after
 *         a failed API call.
 *
 * @remark errno propagation from librdkafka is not safe on Windows
 *         and should not be used, use rd_kafka_last_error() instead.
 */
RD_EXPORT
rd_kafka_resp_err_t rd_kafka_last_error (void);


/**
 * @brief Converts the system errno value \p errnox to a rd_kafka_resp_err_t
 *        error code upon failure from the following functions:
 *  - rd_kafka_topic_new()
 *  - rd_kafka_consume_start()
 *  - rd_kafka_consume_stop()
 *  - rd_kafka_consume()
 *  - rd_kafka_consume_batch()
 *  - rd_kafka_consume_callback()
 *  - rd_kafka_consume_queue()
 *  - rd_kafka_produce()
 *
 * @param errnox  System errno value to convert
 *
 * @returns Appropriate error code for \p errnox
 *
 * @remark A better alternative is to call rd_kafka_last_error() immediately
 *         after any of the above functions return -1 or NULL.
 *
 * @deprecated Use rd_kafka_last_error() to retrieve the last error code
 *             set by the legacy librdkafka APIs.
 *
 * @sa rd_kafka_last_error()
 */
RD_EXPORT RD_DEPRECATED
rd_kafka_resp_err_t rd_kafka_errno2err(int errnox);


/**
 * @brief Returns the thread-local system errno
 *
 * On most platforms this is the same as \p errno but in case of different
 * runtimes between library and application (e.g., Windows static DLLs)
 * this provides a means for exposing the errno librdkafka uses.
 *
 * @remark The value is local to the current calling thread.
 *
 * @deprecated Use rd_kafka_last_error() to retrieve the last error code
 *             set by the legacy librdkafka APIs.
 */
RD_EXPORT RD_DEPRECATED
int rd_kafka_errno (void);




/**
 * @brief Returns the first fatal error set on this client instance,
 *        or RD_KAFKA_RESP_ERR_NO_ERROR if no fatal error has occurred.
 *
 * This function is to be used with the Idempotent Producer and \c error_cb
 * to detect fatal errors.
 *
 * Generally all errors raised by \c error_cb are to be considered
 * informational and temporary, the client will try to recover from all
 * errors in a graceful fashion (by retrying, etc).
 *
 * However, some errors should logically be considered fatal to retain
 * consistency; in particular a set of errors that may occur when using the
 * Idempotent Producer and the in-order or exactly-once producer guarantees
 * can't be satisfied.
 *
 * @param rk Client instance.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written to if there is a fatal error.
 * @param errstr_size Writable size in \p errstr.
 *
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR if no fatal error has been raised, else
 *          any other error code.
 */
RD_EXPORT
rd_kafka_resp_err_t rd_kafka_fatal_error (rd_kafka_t *rk,
                                          char *errstr, size_t errstr_size);


/**
 * @brief Trigger a fatal error for testing purposes.
 *
 * Since there is no practical way to trigger real fatal errors in the
 * idempotent producer, this method allows an application to trigger
 * fabricated fatal errors in tests to check its error handling code.
 *
 * @param rk Client instance.
 * @param err The underlying error code.
 * @param reason A human readable error reason.
 *               Will be prefixed with "test_fatal_error: " to differentiate
 *               from real fatal errors.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR if a fatal error was triggered, or
 *          RD_KAFKA_RESP_ERR__PREV_IN_PROGRESS if a previous fatal error
 *          has already been triggered.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_test_fatal_error (rd_kafka_t *rk, rd_kafka_resp_err_t err,
                           const char *reason);


/**
 * @returns the error code for \p error or RD_KAFKA_RESP_ERR_NO_ERROR if
 *          \p error is NULL.
 */
RD_EXPORT
rd_kafka_resp_err_t rd_kafka_error_code (const rd_kafka_error_t *error);

/**
 * @returns the error code name for \p error, e.g, "ERR_UNKNOWN_MEMBER_ID",
 *          or an empty string if \p error is NULL.
 *
 * @remark The lifetime of the returned pointer is the same as the error object.
 *
 * @sa rd_kafka_err2name()
 */
RD_EXPORT
const char *rd_kafka_error_name (const rd_kafka_error_t *error);

/**
 * @returns a human readable error string for \p error,
 *          or an empty string if \p error is NULL.
 *
 * @remark The lifetime of the returned pointer is the same as the error object.
 */
RD_EXPORT
const char *rd_kafka_error_string (const rd_kafka_error_t *error);


/**
 * @returns 1 if the error is a fatal error, indicating that the client
 *          instance is no longer usable, else 0 (also if \p error is NULL).
 */
RD_EXPORT
int rd_kafka_error_is_fatal (const rd_kafka_error_t *error);


/**
 * @returns 1 if the operation may be retried,
 *          else 0 (also if \p error is NULL).
 */
RD_EXPORT
int rd_kafka_error_is_retriable (const rd_kafka_error_t *error);


/**
 * @returns 1 if the error is an abortable transaction error in which case
 *          the application must call rd_kafka_abort_transaction() and
 *          start a new transaction with rd_kafka_begin_transaction() if it
 *          wishes to proceed with transactions.
 *          Else returns 0 (also if \p error is NULL).
 *
 * @remark The return value of this method is only valid for errors returned
 *         by the transactional API.
 */
RD_EXPORT
int rd_kafka_error_txn_requires_abort (const rd_kafka_error_t *error);

/**
 * @brief Free and destroy an error object.
 *
 * @remark As a conveniance it is permitted to pass a NULL \p error.
 */
RD_EXPORT
void rd_kafka_error_destroy (rd_kafka_error_t *error);


/**
 * @brief Create a new error object with error \p code and optional
 *        human readable error string in \p fmt.
 *
 * This method is mainly to be used for mocking errors in application test code.
 *
 * The returned object must be destroyed with rd_kafka_error_destroy().
 */
RD_EXPORT
rd_kafka_error_t *rd_kafka_error_new (rd_kafka_resp_err_t code,
                                      const char *fmt, ...)
        RD_FORMAT(printf, 2, 3);


/**
 * @brief Topic+Partition place holder
 *
 * Generic place holder for a Topic+Partition and its related information
 * used for multiple purposes:
 *   - consumer offset (see rd_kafka_commit(), et.al.)
 *   - group rebalancing callback (rd_kafka_conf_set_rebalance_cb())
 *   - offset commit result callback (rd_kafka_conf_set_offset_commit_cb())
 */

/**
 * @brief Generic place holder for a specific Topic+Partition.
 *
 * @sa rd_kafka_topic_partition_list_new()
 */
typedef struct rd_kafka_topic_partition_s {
        char        *topic;             /**< Topic name */
        int32_t      partition;         /**< Partition */
	int64_t      offset;            /**< Offset */
        void        *metadata;          /**< Metadata */
        size_t       metadata_size;     /**< Metadata size */
        void        *opaque;            /**< Opaque value for application use */
        rd_kafka_resp_err_t err;        /**< Error code, depending on use. */
        void       *_private;           /**< INTERNAL USE ONLY,
                                         *   INITIALIZE TO ZERO, DO NOT TOUCH */
} rd_kafka_topic_partition_t;


/**
 * @brief Destroy a rd_kafka_topic_partition_t.
 * @remark This must not be called for elements in a topic partition list.
 */
RD_EXPORT
void rd_kafka_topic_partition_destroy (rd_kafka_topic_partition_t *rktpar);


/**
 * @brief A growable list of Topic+Partitions.
 *
 */
typedef struct rd_kafka_topic_partition_list_s {
        int cnt;               /**< Current number of elements */
        int size;              /**< Current allocated size */
        rd_kafka_topic_partition_t *elems; /**< Element array[] */
} rd_kafka_topic_partition_list_t;


/**
 * @brief Create a new list/vector Topic+Partition container.
 *
 * @param size  Initial allocated size used when the expected number of
 *              elements is known or can be estimated.
 *              Avoids reallocation and possibly relocation of the
 *              elems array.
 *
 * @returns A newly allocated Topic+Partition list.
 *
 * @remark Use rd_kafka_topic_partition_list_destroy() to free all resources
 *         in use by a list and the list itself.
 * @sa     rd_kafka_topic_partition_list_add()
 */
RD_EXPORT
rd_kafka_topic_partition_list_t *rd_kafka_topic_partition_list_new (int size);


/**
 * @brief Free all resources used by the list and the list itself.
 */
RD_EXPORT
void
rd_kafka_topic_partition_list_destroy (rd_kafka_topic_partition_list_t *rkparlist);

/**
 * @brief Add topic+partition to list
 *
 * @param rktparlist List to extend
 * @param topic      Topic name (copied)
 * @param partition  Partition id
 *
 * @returns The object which can be used to fill in additionals fields.
 */
RD_EXPORT
rd_kafka_topic_partition_t *
rd_kafka_topic_partition_list_add (rd_kafka_topic_partition_list_t *rktparlist,
                                   const char *topic, int32_t partition);


/**
 * @brief Add range of partitions from \p start to \p stop inclusive.
 *
 * @param rktparlist List to extend
 * @param topic      Topic name (copied)
 * @param start      Start partition of range
 * @param stop       Last partition of range (inclusive)
 */
RD_EXPORT
void
rd_kafka_topic_partition_list_add_range (rd_kafka_topic_partition_list_t
                                         *rktparlist,
                                         const char *topic,
                                         int32_t start, int32_t stop);



/**
 * @brief Delete partition from list.
 *
 * @param rktparlist List to modify
 * @param topic      Topic name to match
 * @param partition  Partition to match
 *
 * @returns 1 if partition was found (and removed), else 0.
 *
 * @remark Any held indices to elems[] are unusable after this call returns 1.
 */
RD_EXPORT
int
rd_kafka_topic_partition_list_del (rd_kafka_topic_partition_list_t *rktparlist,
				   const char *topic, int32_t partition);


/**
 * @brief Delete partition from list by elems[] index.
 *
 * @returns 1 if partition was found (and removed), else 0.
 *
 * @sa rd_kafka_topic_partition_list_del()
 */
RD_EXPORT
int
rd_kafka_topic_partition_list_del_by_idx (
	rd_kafka_topic_partition_list_t *rktparlist,
	int idx);


/**
 * @brief Make a copy of an existing list.
 *
 * @param src   The existing list to copy.
 *
 * @returns A new list fully populated to be identical to \p src
 */
RD_EXPORT
rd_kafka_topic_partition_list_t *
rd_kafka_topic_partition_list_copy (const rd_kafka_topic_partition_list_t *src);




/**
 * @brief Set offset to \p offset for \p topic and \p partition
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or
 *          RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION if \p partition was not found
 *          in the list.
 */
RD_EXPORT
rd_kafka_resp_err_t rd_kafka_topic_partition_list_set_offset (
	rd_kafka_topic_partition_list_t *rktparlist,
	const char *topic, int32_t partition, int64_t offset);



/**
 * @brief Find element by \p topic and \p partition.
 *
 * @returns a pointer to the first matching element, or NULL if not found.
 */
RD_EXPORT
rd_kafka_topic_partition_t *
rd_kafka_topic_partition_list_find (
        const rd_kafka_topic_partition_list_t *rktparlist,
        const char *topic, int32_t partition);


/**
 * @brief Sort list using comparator \p cmp.
 *
 * If \p cmp is NULL the default comparator will be used that
 * sorts by ascending topic name and partition.
 *
 * \p cmp_opaque is provided as the \p cmp_opaque argument to \p cmp.
 *
 */
RD_EXPORT void
rd_kafka_topic_partition_list_sort (rd_kafka_topic_partition_list_t *rktparlist,
                                    int (*cmp) (const void *a, const void *b,
                                                void *cmp_opaque),
                                    void *cmp_opaque);


/**@}*/



/**
 * @name Var-arg tag types
 * @{
 *
 */

/**
 * @enum rd_kafka_vtype_t
 *
 * @brief Var-arg tag types
 *
 * @sa rd_kafka_producev()
 */
typedef enum rd_kafka_vtype_t {
        RD_KAFKA_VTYPE_END,       /**< va-arg sentinel */
        RD_KAFKA_VTYPE_TOPIC,     /**< (const char *) Topic name */
        RD_KAFKA_VTYPE_RKT,       /**< (rd_kafka_topic_t *) Topic handle */
        RD_KAFKA_VTYPE_PARTITION, /**< (int32_t) Partition */
        RD_KAFKA_VTYPE_VALUE,     /**< (void *, size_t) Message value (payload)*/
        RD_KAFKA_VTYPE_KEY,       /**< (void *, size_t) Message key */
        RD_KAFKA_VTYPE_OPAQUE,    /**< (void *) Per-message application opaque
                                   *            value. This is the same as
                                   *            the _private field in
                                   *            rd_kafka_message_t, also known
                                   *            as the msg_opaque. */
        RD_KAFKA_VTYPE_MSGFLAGS,  /**< (int) RD_KAFKA_MSG_F_.. flags */
        RD_KAFKA_VTYPE_TIMESTAMP, /**< (int64_t) Milliseconds since epoch UTC */
        RD_KAFKA_VTYPE_HEADER,    /**< (const char *, const void *, ssize_t)
                                   *   Message Header */
        RD_KAFKA_VTYPE_HEADERS,   /**< (rd_kafka_headers_t *) Headers list */
} rd_kafka_vtype_t;


/**
 * @brief VTYPE + argument container for use with rd_kafka_produce_va()
 *
 * See RD_KAFKA_V_..() macros below for which union field corresponds
 * to which RD_KAFKA_VTYPE_...
 */
typedef struct rd_kafka_vu_s {
        rd_kafka_vtype_t vtype;           /**< RD_KAFKA_VTYPE_.. */
        /** Value union, see RD_KAFKA_V_.. macros for which field to use. */
        union {
                const char *cstr;
                rd_kafka_topic_t *rkt;
                int i;
                int32_t i32;
                int64_t i64;
                struct {
                        void *ptr;
                        size_t size;
                } mem;
                struct {
                        const char *name;
                        const void *val;
                        ssize_t size;
                } header;
                rd_kafka_headers_t *headers;
                void *ptr;
                char _pad[64];  /**< Padding size for future-proofness */
        } u;
} rd_kafka_vu_t;

/**
 * @brief Convenience macros for rd_kafka_vtype_t that takes the
 *        correct arguments for each vtype.
 */

/*!
 * va-arg end sentinel used to terminate the variable argument list
 */
#define RD_KAFKA_V_END RD_KAFKA_VTYPE_END

/*!
 * Topic name (const char *)
 *
 * rd_kafka_vu_t field: u.cstr
 */
#define RD_KAFKA_V_TOPIC(topic)                                         \
        _LRK_TYPECHECK(RD_KAFKA_VTYPE_TOPIC, const char *, topic),      \
        (const char *)topic
/*!
 * Topic object (rd_kafka_topic_t *)
 *
 * rd_kafka_vu_t field: u.rkt
 */
#define RD_KAFKA_V_RKT(rkt)                                             \
        _LRK_TYPECHECK(RD_KAFKA_VTYPE_RKT, rd_kafka_topic_t *, rkt),    \
        (rd_kafka_topic_t *)rkt
/*!
 * Partition (int32_t)
 *
 * rd_kafka_vu_t field: u.i32
 */
#define RD_KAFKA_V_PARTITION(partition)                                 \
        _LRK_TYPECHECK(RD_KAFKA_VTYPE_PARTITION, int32_t, partition),   \
        (int32_t)partition
/*!
 * Message value/payload pointer and length (void *, size_t)
 *
 * rd_kafka_vu_t fields: u.mem.ptr, u.mem.size
 */
#define RD_KAFKA_V_VALUE(VALUE,LEN)                                     \
        _LRK_TYPECHECK2(RD_KAFKA_VTYPE_VALUE, void *, VALUE, size_t, LEN), \
        (void *)VALUE, (size_t)LEN
/*!
 * Message key pointer and length (const void *, size_t)
 *
 * rd_kafka_vu_t field: u.mem.ptr, rd_kafka_vu.t.u.mem.size
 */
#define RD_KAFKA_V_KEY(KEY,LEN)                                         \
        _LRK_TYPECHECK2(RD_KAFKA_VTYPE_KEY, const void *, KEY, size_t, LEN), \
        (void *)KEY, (size_t)LEN
/*!
 * Message opaque pointer (void *)
 * Same as \c msg_opaque, \c produce(.., msg_opaque),
 * and \c rkmessage->_private .
 *
 * rd_kafka_vu_t field: u.ptr
 */
#define RD_KAFKA_V_OPAQUE(msg_opaque)                                   \
        _LRK_TYPECHECK(RD_KAFKA_VTYPE_OPAQUE, void *, msg_opaque),      \
        (void *)msg_opaque
/*!
 * Message flags (int)
 * @sa RD_KAFKA_MSG_F_COPY, et.al.
 *
 * rd_kafka_vu_t field: u.i
 */
#define RD_KAFKA_V_MSGFLAGS(msgflags)                                 \
        _LRK_TYPECHECK(RD_KAFKA_VTYPE_MSGFLAGS, int, msgflags),       \
        (int)msgflags
/*!
 * Timestamp in milliseconds since epoch UTC (int64_t).
 * A value of 0 will use the current wall-clock time.
 *
 * rd_kafka_vu_t field: u.i64
 */
#define RD_KAFKA_V_TIMESTAMP(timestamp)                                 \
        _LRK_TYPECHECK(RD_KAFKA_VTYPE_TIMESTAMP, int64_t, timestamp),   \
        (int64_t)timestamp
/*!
 * Add Message Header (const char *NAME, const void *VALUE, ssize_t LEN).
 * @sa rd_kafka_header_add()
 * @remark RD_KAFKA_V_HEADER() and RD_KAFKA_V_HEADERS() MUST NOT be mixed
 *         in the same call to producev().
 *
 * rd_kafka_vu_t fields: u.header.name, u.header.val, u.header.size
 */
#define RD_KAFKA_V_HEADER(NAME,VALUE,LEN)                               \
        _LRK_TYPECHECK3(RD_KAFKA_VTYPE_HEADER, const char *, NAME,      \
                        const void *, VALUE, ssize_t, LEN),             \
                (const char *)NAME, (const void *)VALUE, (ssize_t)LEN

/*!
 * Message Headers list (rd_kafka_headers_t *).
 * The message object will assume ownership of the headers (unless producev()
 * fails).
 * Any existing headers will be replaced.
 * @sa rd_kafka_message_set_headers()
 * @remark RD_KAFKA_V_HEADER() and RD_KAFKA_V_HEADERS() MUST NOT be mixed
 *         in the same call to producev().
 *
 * rd_kafka_vu_t fields: u.headers
 */
#define RD_KAFKA_V_HEADERS(HDRS)                                        \
        _LRK_TYPECHECK(RD_KAFKA_VTYPE_HEADERS, rd_kafka_headers_t *, HDRS), \
                (rd_kafka_headers_t *)HDRS


/**@}*/


/**
 * @name Message headers
 * @{
 *
 * @brief Message headers consist of a list of (string key, binary value) pairs.
 *        Duplicate keys are supported and the order in which keys were
 *        added are retained.
 *
 *        Header values are considered binary and may have three types of
 *        value:
 *          - proper value with size > 0 and a valid pointer
 *          - empty value with size = 0 and any non-NULL pointer
 *          - null value with size = 0 and a NULL pointer
 *
 *        Headers require Apache Kafka broker version v0.11.0.0 or later.
 *
 *        Header operations are O(n).
 */


/**
 * @brief Create a new headers list.
 *
 * @param initial_count Preallocate space for this number of headers.
 *                      Any number of headers may be added, updated and
 *                      removed regardless of the initial count.
 */
RD_EXPORT rd_kafka_headers_t *rd_kafka_headers_new (size_t initial_count);

/**
 * @brief Destroy the headers list. The object and any returned value pointers
 *        are not usable after this call.
 */
RD_EXPORT void rd_kafka_headers_destroy (rd_kafka_headers_t *hdrs);

/**
 * @brief Make a copy of headers list \p src.
 */
RD_EXPORT rd_kafka_headers_t *
rd_kafka_headers_copy (const rd_kafka_headers_t *src);

/**
 * @brief Add header with name \p name and value \p val (copied) of size
 *        \p size (not including null-terminator).
 *
 * @param hdrs       Headers list.
 * @param name       Header name.
 * @param name_size  Header name size (not including the null-terminator).
 *                   If -1 the \p name length is automatically acquired using
 *                   strlen().
 * @param value      Pointer to header value, or NULL (set size to 0 or -1).
 * @param value_size Size of header value. If -1 the \p value is assumed to be a
 *                   null-terminated string and the length is automatically
 *                   acquired using strlen().
 *
 * @returns RD_KAFKA_RESP_ERR__READ_ONLY if the headers are read-only,
 *          else RD_KAFKA_RESP_ERR_NO_ERROR.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_header_add (rd_kafka_headers_t *hdrs,
                     const char *name, ssize_t name_size,
                     const void *value, ssize_t value_size);

/**
 * @brief Remove all headers for the given key (if any).
 *
 * @returns RD_KAFKA_RESP_ERR__READ_ONLY if the headers are read-only,
 *          RD_KAFKA_RESP_ERR__NOENT if no matching headers were found,
 *          else RD_KAFKA_RESP_ERR_NO_ERROR if headers were removed.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_header_remove (rd_kafka_headers_t *hdrs, const char *name);


/**
 * @brief Find last header in list \p hdrs matching \p name.
 *
 * @param hdrs   Headers list.
 * @param name   Header to find (last match).
 * @param valuep (out) Set to a (null-terminated) const pointer to the value
 *               (may be NULL).
 * @param sizep  (out) Set to the value's size (not including null-terminator).
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR if an entry was found, else
 *          RD_KAFKA_RESP_ERR__NOENT.
 *
 * @remark The returned pointer in \p valuep includes a trailing null-terminator
 *         that is not accounted for in \p sizep.
 * @remark The returned pointer is only valid as long as the headers list and
 *         the header item is valid.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_header_get_last (const rd_kafka_headers_t *hdrs,
                          const char *name, const void **valuep, size_t *sizep);

/**
 * @brief Iterator for headers matching \p name.
 *
 *        Same semantics as rd_kafka_header_get_last()
 *
 * @param hdrs   Headers to iterate.
 * @param idx    Iterator index, start at 0 and increment by one for each call
 *               as long as RD_KAFKA_RESP_ERR_NO_ERROR is returned.
 * @param name   Header name to match.
 * @param valuep (out) Set to a (null-terminated) const pointer to the value
 *               (may be NULL).
 * @param sizep  (out) Set to the value's size (not including null-terminator).
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_header_get (const rd_kafka_headers_t *hdrs, size_t idx,
                     const char *name, const void **valuep, size_t *sizep);


/**
 * @brief Iterator for all headers.
 *
 *        Same semantics as rd_kafka_header_get()
 *
 * @sa rd_kafka_header_get()
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_header_get_all (const rd_kafka_headers_t *hdrs, size_t idx,
                         const char **namep,
                         const void **valuep, size_t *sizep);



/**@}*/



/**
 * @name Kafka messages
 * @{
 *
 */



// FIXME: This doesn't show up in docs for some reason
// "Compound rd_kafka_message_t is not documented."

/**
 * @brief A Kafka message as returned by the \c rd_kafka_consume*() family
 *        of functions as well as provided to the Producer \c dr_msg_cb().
 *
 * For the consumer this object has two purposes:
 *  - provide the application with a consumed message. (\c err == 0)
 *  - report per-topic+partition consumer errors (\c err != 0)
 *
 * The application must check \c err to decide what action to take.
 *
 * When the application is finished with a message it must call
 * rd_kafka_message_destroy() unless otherwise noted.
 */
typedef struct rd_kafka_message_s {
	rd_kafka_resp_err_t err;   /**< Non-zero for error signaling. */
	rd_kafka_topic_t *rkt;     /**< Topic */
	int32_t partition;         /**< Partition */
	void   *payload;           /**< Producer: original message payload.
				    * Consumer: Depends on the value of \c err :
				    * - \c err==0: Message payload.
				    * - \c err!=0: Error string */
	size_t  len;               /**< Depends on the value of \c err :
				    * - \c err==0: Message payload length
				    * - \c err!=0: Error string length */
	void   *key;               /**< Depends on the value of \c err :
				    * - \c err==0: Optional message key */
	size_t  key_len;           /**< Depends on the value of \c err :
				    * - \c err==0: Optional message key length*/
	int64_t offset;            /**< Consumer:
                                    * - Message offset (or offset for error
				    *   if \c err!=0 if applicable).
                                    *   Producer, dr_msg_cb:
                                    *   Message offset assigned by broker.
                                    *   May be RD_KAFKA_OFFSET_INVALID
                                    *   for retried messages when
                                    *   idempotence is enabled. */
        void  *_private;           /**< Consumer:
                                    *  - rdkafka private pointer: DO NOT MODIFY
                                    *  Producer:
                                    *  - dr_msg_cb:
                                    *    msg_opaque from produce() call or
                                    *    RD_KAFKA_V_OPAQUE from producev(). */
} rd_kafka_message_t;


/**
 * @brief Frees resources for \p rkmessage and hands ownership back to rdkafka.
 */
RD_EXPORT
void rd_kafka_message_destroy(rd_kafka_message_t *rkmessage);




/**
 * @brief Returns the error string for an errored rd_kafka_message_t or NULL if
 *        there was no error.
 *
 * @remark This function MUST NOT be used with the producer.
 */
RD_EXPORT
const char *rd_kafka_message_errstr (const rd_kafka_message_t *rkmessage);


/**
 * @brief Returns the message timestamp for a consumed message.
 *
 * The timestamp is the number of milliseconds since the epoch (UTC).
 *
 * \p tstype (if not NULL) is updated to indicate the type of timestamp.
 *
 * @returns message timestamp, or -1 if not available.
 *
 * @remark Message timestamps require broker version 0.10.0 or later.
 */
RD_EXPORT
int64_t rd_kafka_message_timestamp (const rd_kafka_message_t *rkmessage,
				    rd_kafka_timestamp_type_t *tstype);



/**
 * @brief Returns the latency for a produced message measured from
 *        the produce() call.
 *
 * @returns the latency in microseconds, or -1 if not available.
 */
RD_EXPORT
int64_t rd_kafka_message_latency (const rd_kafka_message_t *rkmessage);


/**
 * @brief Returns the broker id of the broker the message was produced to
 *        or fetched from.
 *
 * @returns a broker id if known, else -1.
 */
RD_EXPORT
int32_t rd_kafka_message_broker_id (const rd_kafka_message_t *rkmessage);


/**
 * @brief Get the message header list.
 *
 * The returned pointer in \p *hdrsp is associated with the \p rkmessage and
 * must not be used after destruction of the message object or the header
 * list is replaced with rd_kafka_message_set_headers().
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR if headers were returned,
 *          RD_KAFKA_RESP_ERR__NOENT if the message has no headers,
 *          or another error code if the headers could not be parsed.
 *
 * @remark Headers require broker version 0.11.0.0 or later.
 *
 * @remark As an optimization the raw protocol headers are parsed on
 *         the first call to this function.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_message_headers (const rd_kafka_message_t *rkmessage,
                          rd_kafka_headers_t **hdrsp);

/**
 * @brief Get the message header list and detach the list from the message
 *        making the application the owner of the headers.
 *        The application must eventually destroy the headers using
 *        rd_kafka_headers_destroy().
 *        The message's headers will be set to NULL.
 *
 *        Otherwise same semantics as rd_kafka_message_headers()
 *
 * @sa rd_kafka_message_headers
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_message_detach_headers (rd_kafka_message_t *rkmessage,
                                 rd_kafka_headers_t **hdrsp);


/**
 * @brief Replace the message's current headers with a new list.
 *
 * @param rkmessage The message to set headers.
 * @param hdrs New header list. The message object assumes ownership of
 *             the list, the list will be destroyed automatically with
 *             the message object.
 *             The new headers list may be updated until the message object
 *             is passed or returned to librdkafka.
 *
 * @remark The existing headers object, if any, will be destroyed.
 */
RD_EXPORT
void rd_kafka_message_set_headers (rd_kafka_message_t *rkmessage,
                                   rd_kafka_headers_t *hdrs);


/**
 * @brief Returns the number of header key/value pairs
 *
 * @param hdrs   Headers to count
 */
RD_EXPORT size_t rd_kafka_header_cnt (const rd_kafka_headers_t *hdrs);


/**
 * @enum rd_kafka_msg_status_t
 * @brief Message persistence status can be used by the application to
 *        find out if a produced message was persisted in the topic log.
 */
typedef enum {
        /** Message was never transmitted to the broker, or failed with
         *  an error indicating it was not written to the log.
         *  Application retry risks ordering, but not duplication. */
        RD_KAFKA_MSG_STATUS_NOT_PERSISTED = 0,

        /** Message was transmitted to broker, but no acknowledgement was
         *  received.
         *  Application retry risks ordering and duplication. */
        RD_KAFKA_MSG_STATUS_POSSIBLY_PERSISTED = 1,

        /** Message was written to the log and acknowledged by the broker.
         *  No reason for application to retry.
         *  Note: this value should only be trusted with \c acks=all. */
        RD_KAFKA_MSG_STATUS_PERSISTED =  2
} rd_kafka_msg_status_t;


/**
 * @brief Returns the message's persistence status in the topic log.
 *
 * @remark The message status is not available in on_acknowledgement
 *         interceptors.
 */
RD_EXPORT rd_kafka_msg_status_t
rd_kafka_message_status (const rd_kafka_message_t *rkmessage);

/**@}*/


/**
 * @name Configuration interface
 * @{
 *
 * @brief Main/global configuration property interface
 *
 */

/**
 * @enum rd_kafka_conf_res_t
 * @brief Configuration result type
 */
typedef enum {
	RD_KAFKA_CONF_UNKNOWN = -2, /**< Unknown configuration name. */
	RD_KAFKA_CONF_INVALID = -1, /**< Invalid configuration value or
                                     *   property or value not supported in
                                     *   this build. */
	RD_KAFKA_CONF_OK = 0        /**< Configuration okay */
} rd_kafka_conf_res_t;


/**
 * @brief Create configuration object.
 *
 * When providing your own configuration to the \c rd_kafka_*_new_*() calls
 * the rd_kafka_conf_t objects needs to be created with this function
 * which will set up the defaults.
 * I.e.:
 * @code
 *   rd_kafka_conf_t *myconf;
 *   rd_kafka_conf_res_t res;
 *
 *   myconf = rd_kafka_conf_new();
 *   res = rd_kafka_conf_set(myconf, "socket.timeout.ms", "600",
 *                           errstr, sizeof(errstr));
 *   if (res != RD_KAFKA_CONF_OK)
 *      die("%s\n", errstr);
 *
 *   rk = rd_kafka_new(..., myconf);
 * @endcode
 *
 * Please see CONFIGURATION.md for the default settings or use
 * rd_kafka_conf_properties_show() to provide the information at runtime.
 *
 * The properties are identical to the Apache Kafka configuration properties
 * whenever possible.
 *
 * @remark A successful call to rd_kafka_new() will assume ownership of
 * the conf object and rd_kafka_conf_destroy() must not be called.
 *
 * @returns A new rd_kafka_conf_t object with defaults set.
 *
 * @sa rd_kafka_new(), rd_kafka_conf_set(), rd_kafka_conf_destroy()
 */
RD_EXPORT
rd_kafka_conf_t *rd_kafka_conf_new(void);


/**
 * @brief Destroys a conf object.
 */
RD_EXPORT
void rd_kafka_conf_destroy(rd_kafka_conf_t *conf);


/**
 * @brief Creates a copy/duplicate of configuration object \p conf
 *
 * @remark Interceptors are NOT copied to the new configuration object.
 * @sa rd_kafka_interceptor_f_on_conf_dup
 */
RD_EXPORT
rd_kafka_conf_t *rd_kafka_conf_dup(const rd_kafka_conf_t *conf);


/**
 * @brief Same as rd_kafka_conf_dup() but with an array of property name
 *        prefixes to filter out (ignore) when copying.
 */
RD_EXPORT
rd_kafka_conf_t *rd_kafka_conf_dup_filter (const rd_kafka_conf_t *conf,
                                           size_t filter_cnt,
                                           const char **filter);



/**
 * @returns the configuration object used by an rd_kafka_t instance.
 *          For use with rd_kafka_conf_get(), et.al., to extract configuration
 *          properties from a running client.
 *
 * @remark the returned object is read-only and its lifetime is the same
 *         as the rd_kafka_t object.
 */
RD_EXPORT
const rd_kafka_conf_t *rd_kafka_conf (rd_kafka_t *rk);


/**
 * @brief Sets a configuration property.
 *
 * \p conf must have been previously created with rd_kafka_conf_new().
 *
 * Fallthrough:
 * Topic-level configuration properties may be set using this interface
 * in which case they are applied on the \c default_topic_conf.
 * If no \c default_topic_conf has been set one will be created.
 * Any sub-sequent rd_kafka_conf_set_default_topic_conf() calls will
 * replace the current default topic configuration.
 *
 * @returns \c rd_kafka_conf_res_t to indicate success or failure.
 * In case of failure \p errstr is updated to contain a human readable
 * error string.
 *
 * @remark Setting properties or values that were disabled at build time due to
 *         missing dependencies will return RD_KAFKA_CONF_INVALID.
 */
RD_EXPORT
rd_kafka_conf_res_t rd_kafka_conf_set(rd_kafka_conf_t *conf,
				       const char *name,
				       const char *value,
				       char *errstr, size_t errstr_size);


/**
 * @brief Enable event sourcing.
 * \p events is a bitmask of \c RD_KAFKA_EVENT_* of events to enable
 * for consumption by `rd_kafka_queue_poll()`.
 */
RD_EXPORT
void rd_kafka_conf_set_events(rd_kafka_conf_t *conf, int events);


/**
 * @brief Generic event callback to be used with the event API to trigger
 *        callbacks for \c rd_kafka_event_t objects from a background
 *        thread serving the background queue.
 *
 * How to use:
 *  1. First set the event callback on the configuration object with this
 *     function, followed by creating an rd_kafka_t instance
 *     with rd_kafka_new().
 *  2. Get the instance's background queue with rd_kafka_queue_get_background()
 *     and pass it as the reply/response queue to an API that takes an
 *     event queue, such as rd_kafka_CreateTopics().
 *  3. As the response event is ready and enqueued on the background queue the
 *     event callback will be triggered from the background thread.
 *  4. Prior to destroying the client instance, loose your reference to the
 *     background queue by calling rd_kafka_queue_destroy().
 *
 * The application must destroy the \c rkev passed to \p event cb using
 * rd_kafka_event_destroy().
 *
 * The \p event_cb \c opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * @remark This callback is a specialized alternative to the poll-based
 *         event API described in the Event interface section.
 *
 * @remark The \p event_cb will be called spontaneously from a background
 *         thread completely managed by librdkafka.
 *         Take care to perform proper locking of application objects.
 *
 * @warning The application MUST NOT call rd_kafka_destroy() from the
 *          event callback.
 *
 * @sa rd_kafka_queue_get_background
 */
RD_EXPORT void
rd_kafka_conf_set_background_event_cb (rd_kafka_conf_t *conf,
                                       void (*event_cb) (rd_kafka_t *rk,
                                                         rd_kafka_event_t *rkev,
                                                         void *opaque));


/**
 * @deprecated See rd_kafka_conf_set_dr_msg_cb()
 */
RD_EXPORT
void rd_kafka_conf_set_dr_cb(rd_kafka_conf_t *conf,
			      void (*dr_cb) (rd_kafka_t *rk,
					     void *payload, size_t len,
					     rd_kafka_resp_err_t err,
					     void *opaque, void *msg_opaque));

/**
 * @brief \b Producer: Set delivery report callback in provided \p conf object.
 *
 * The delivery report callback will be called once for each message
 * accepted by rd_kafka_produce() (et.al) with \p err set to indicate
 * the result of the produce request.
 *
 * The callback is called when a message is succesfully produced or
 * if librdkafka encountered a permanent failure.
 * Delivery errors occur when the retry count is exceeded, when the
 * message.timeout.ms timeout is exceeded or there is a permanent error
 * like RD_KAFKA_RESP_ERR_UNKNOWN_TOPIC_OR_PART.
 *
 * An application must call rd_kafka_poll() at regular intervals to
 * serve queued delivery report callbacks.
 *
 * The broker-assigned offset can be retrieved with \c rkmessage->offset
 * and the timestamp can be retrieved using rd_kafka_message_timestamp().
 *
 * The \p dr_msg_cb \c opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 * The per-message msg_opaque value is available in
 * \c rd_kafka_message_t._private.
 *
 * @remark The Idempotent Producer may return invalid timestamp
 *         (RD_KAFKA_TIMESTAMP_NOT_AVAILABLE), and
 *         and offset (RD_KAFKA_OFFSET_INVALID) for retried messages
 *         that were previously successfully delivered but not properly
 *         acknowledged.
 */
RD_EXPORT
void rd_kafka_conf_set_dr_msg_cb(rd_kafka_conf_t *conf,
                                  void (*dr_msg_cb) (rd_kafka_t *rk,
                                                     const rd_kafka_message_t *
                                                     rkmessage,
                                                     void *opaque));


/**
 * @brief \b Consumer: Set consume callback for use with
 *        rd_kafka_consumer_poll()
 *
 * The \p consume_cb \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 */
RD_EXPORT
void rd_kafka_conf_set_consume_cb (rd_kafka_conf_t *conf,
                                   void (*consume_cb) (rd_kafka_message_t *
                                                       rkmessage,
                                                       void *opaque));

/**
 * @brief \b Consumer: Set rebalance callback for use with
 *                     coordinated consumer group balancing.
 *
 * The \p err field is set to either RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS
 * or RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS and 'partitions'
 * contains the full partition set that was either assigned or revoked.
 *
 * Registering a \p rebalance_cb turns off librdkafka's automatic
 * partition assignment/revocation and instead delegates that responsibility
 * to the application's \p rebalance_cb.
 *
 * The rebalance callback is responsible for updating librdkafka's
 * assignment set based on the two events: RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS
 * and RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS but should also be able to handle
 * arbitrary rebalancing failures where \p err is neither of those.
 * @remark In this latter case (arbitrary error), the application must
 *         call rd_kafka_assign(rk, NULL) to synchronize state.
 *
 * For eager/non-cooperative `partition.assignment.strategy` assignors,
 * such as `range` and `roundrobin`, the application must use
 * rd_kafka_assign() to set or clear the entire assignment.
 * For the cooperative assignors, such as `cooperative-sticky`, the application
 * must use rd_kafka_incremental_assign() for
 * RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS and rd_kafka_incremental_unassign()
 * for RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS.
 *
 * Without a rebalance callback this is done automatically by librdkafka
 * but registering a rebalance callback gives the application flexibility
 * in performing other operations along with the assigning/revocation,
 * such as fetching offsets from an alternate location (on assign)
 * or manually committing offsets (on revoke).
 *
 * rebalance_cb is always triggered exactly once when a rebalance completes
 * with a new assignment, even if that assignment is empty. If an
 * eager/non-cooperative assignor is configured, there will eventually be
 * exactly one corresponding call to rebalance_cb to revoke these partitions
 * (even if empty), whether this is due to a group rebalance or lost
 * partitions. In the cooperative case, rebalance_cb will never be called if
 * the set of partitions being revoked is empty (whether or not lost).
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * @remark The \p partitions list is destroyed by librdkafka on return
 *         return from the rebalance_cb and must not be freed or
 *         saved by the application.
 *
 * @remark Be careful when modifying the \p partitions list.
 *         Changing this list should only be done to change the initial
 *         offsets for each partition.
 *         But a function like `rd_kafka_position()` might have unexpected
 *         effects for instance when a consumer gets assigned a partition
 *         it used to consume at an earlier rebalance. In this case, the
 *         list of partitions will be updated with the old offset for that
 *         partition. In this case, it is generally better to pass a copy
 *         of the list (see `rd_kafka_topic_partition_list_copy()`).
 *         The result of `rd_kafka_position()` is typically outdated in
 *         RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS.
 *
 * @sa rd_kafka_assign()
 * @sa rd_kafka_incremental_assign()
 * @sa rd_kafka_incremental_unassign()
 * @sa rd_kafka_assignment_lost()
 * @sa rd_kafka_rebalance_protocol()
 *
 * The following example shows the application's responsibilities:
 * @code
 *    static void rebalance_cb (rd_kafka_t *rk, rd_kafka_resp_err_t err,
 *                              rd_kafka_topic_partition_list_t *partitions,
 *                              void *opaque) {
 *
 *        switch (err)
 *        {
 *          case RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS:
 *             // application may load offets from arbitrary external
 *             // storage here and update \p partitions
 *             if (!strcmp(rd_kafka_rebalance_protocol(rk), "COOPERATIVE"))
 *                     rd_kafka_incremental_assign(rk, partitions);
 *             else // EAGER
 *                     rd_kafka_assign(rk, partitions);
 *             break;
 *
 *          case RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS:
 *             if (manual_commits) // Optional explicit manual commit
 *                 rd_kafka_commit(rk, partitions, 0); // sync commit
 *
 *             if (!strcmp(rd_kafka_rebalance_protocol(rk), "COOPERATIVE"))
 *                     rd_kafka_incremental_unassign(rk, partitions);
 *             else // EAGER
 *                     rd_kafka_assign(rk, NULL);
 *             break;
 *
 *          default:
 *             handle_unlikely_error(err);
 *             rd_kafka_assign(rk, NULL); // sync state
 *             break;
 *         }
 *    }
 * @endcode
 *
 * @remark The above example lacks error handling for assign calls, see
 *         the examples/ directory.
 */
RD_EXPORT
void rd_kafka_conf_set_rebalance_cb (
        rd_kafka_conf_t *conf,
        void (*rebalance_cb) (rd_kafka_t *rk,
                              rd_kafka_resp_err_t err,
                              rd_kafka_topic_partition_list_t *partitions,
                              void *opaque));



/**
 * @brief \b Consumer: Set offset commit callback for use with consumer groups.
 *
 * The results of automatic or manual offset commits will be scheduled
 * for this callback and is served by rd_kafka_consumer_poll().
 *
 * If no partitions had valid offsets to commit this callback will be called
 * with \p err == RD_KAFKA_RESP_ERR__NO_OFFSET which is not to be considered
 * an error.
 *
 * The \p offsets list contains per-partition information:
 *   - \c offset: committed offset (attempted)
 *   - \c err:    commit error
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 */
RD_EXPORT
void rd_kafka_conf_set_offset_commit_cb (
        rd_kafka_conf_t *conf,
        void (*offset_commit_cb) (rd_kafka_t *rk,
                                  rd_kafka_resp_err_t err,
                                  rd_kafka_topic_partition_list_t *offsets,
                                  void *opaque));


/**
 * @brief Set error callback in provided conf object.
 *
 * The error callback is used by librdkafka to signal warnings and errors
 * back to the application.
 *
 * These errors should generally be considered informational and non-permanent,
 * the client will try to recover automatically from all type of errors.
 * Given that the client and cluster configuration is correct the
 * application should treat these as temporary errors.
 *
 * \p error_cb will be triggered with \c err set to RD_KAFKA_RESP_ERR__FATAL
 * if a fatal error has been raised; in this case use rd_kafka_fatal_error() to
 * retrieve the fatal error code and error string, and then begin terminating
 * the client instance.
 *
 * If no \p error_cb is registered, or RD_KAFKA_EVENT_ERROR has not been set
 * with rd_kafka_conf_set_events, then the errors will be logged instead.
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 */
RD_EXPORT
void rd_kafka_conf_set_error_cb(rd_kafka_conf_t *conf,
				 void  (*error_cb) (rd_kafka_t *rk, int err,
						    const char *reason,
						    void *opaque));

/**
 * @brief Set throttle callback.
 *
 * The throttle callback is used to forward broker throttle times to the
 * application for Produce and Fetch (consume) requests.
 *
 * Callbacks are triggered whenever a non-zero throttle time is returned by
 * the broker, or when the throttle time drops back to zero.
 *
 * An application must call rd_kafka_poll() or rd_kafka_consumer_poll() at
 * regular intervals to serve queued callbacks.
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * @remark Requires broker version 0.9.0 or later.
 */
RD_EXPORT
void rd_kafka_conf_set_throttle_cb (rd_kafka_conf_t *conf,
				    void (*throttle_cb) (
					    rd_kafka_t *rk,
					    const char *broker_name,
					    int32_t broker_id,
					    int throttle_time_ms,
					    void *opaque));


/**
 * @brief Set logger callback.
 *
 * The default is to print to stderr, but a syslog logger is also available,
 * see rd_kafka_log_print and rd_kafka_log_syslog for the builtin alternatives.
 * Alternatively the application may provide its own logger callback.
 * Or pass \p func as NULL to disable logging.
 *
 * This is the configuration alternative to the deprecated rd_kafka_set_logger()
 *
 * @remark The log_cb will be called spontaneously from librdkafka's internal
 *         threads unless logs have been forwarded to a poll queue through
 *         \c rd_kafka_set_log_queue().
 *         An application MUST NOT call any librdkafka APIs or do any prolonged
 *         work in a non-forwarded \c log_cb.
 */
RD_EXPORT
void rd_kafka_conf_set_log_cb(rd_kafka_conf_t *conf,
			  void (*log_cb) (const rd_kafka_t *rk, int level,
                                          const char *fac, const char *buf));


/**
 * @brief Set statistics callback in provided conf object.
 *
 * The statistics callback is triggered from rd_kafka_poll() every
 * \c statistics.interval.ms (needs to be configured separately).
 * Function arguments:
 *   - \p rk - Kafka handle
 *   - \p json - String containing the statistics data in JSON format
 *   - \p json_len - Length of \p json string.
 *   - \p opaque - Application-provided opaque as set by
 *                 rd_kafka_conf_set_opaque().
 *
 * For more information on the format of \p json, see
 * https://github.com/edenhill/librdkafka/wiki/Statistics
 *
 * If the application wishes to hold on to the \p json pointer and free
 * it at a later time it must return 1 from the \p stats_cb.
 * If the application returns 0 from the \p stats_cb then librdkafka
 * will immediately free the \p json pointer.
 *
 * See STATISTICS.md for a full definition of the JSON object.
 */
RD_EXPORT
void rd_kafka_conf_set_stats_cb(rd_kafka_conf_t *conf,
				 int (*stats_cb) (rd_kafka_t *rk,
						  char *json,
						  size_t json_len,
						  void *opaque));

/**
 * @brief Set SASL/OAUTHBEARER token refresh callback in provided conf object.
 *
 * @param conf the configuration to mutate.
 * @param oauthbearer_token_refresh_cb the callback to set; callback function
 *  arguments:<br>
 *   \p rk - Kafka handle<br>
 *   \p oauthbearer_config - Value of configuration property
 *                           sasl.oauthbearer.config.
 *   \p opaque - Application-provided opaque set via
 *               rd_kafka_conf_set_opaque()
 *
 * The SASL/OAUTHBEARER token refresh callback is triggered via rd_kafka_poll()
 * whenever OAUTHBEARER is the SASL mechanism and a token needs to be retrieved,
 * typically based on the configuration defined in \c sasl.oauthbearer.config.
 *
 * The callback should invoke rd_kafka_oauthbearer_set_token()
 * or rd_kafka_oauthbearer_set_token_failure() to indicate success
 * or failure, respectively.
 *
 * The refresh operation is eventable and may be received via
 * rd_kafka_queue_poll() with an event type of
 * \c RD_KAFKA_EVENT_OAUTHBEARER_TOKEN_REFRESH.
 *
 * Note that before any SASL/OAUTHBEARER broker connection can succeed the
 * application must call rd_kafka_oauthbearer_set_token() once -- either
 * directly or, more typically, by invoking either rd_kafka_poll() or
 * rd_kafka_queue_poll() -- in order to cause retrieval of an initial token to
 * occur.
 *
 * An unsecured JWT refresh handler is provided by librdkafka for development
 * and testing purposes, it is enabled by setting
 * the \c enable.sasl.oauthbearer.unsecure.jwt property to true and is
 * mutually exclusive to using a refresh callback.
 */
RD_EXPORT
void rd_kafka_conf_set_oauthbearer_token_refresh_cb (
        rd_kafka_conf_t *conf,
        void (*oauthbearer_token_refresh_cb) (rd_kafka_t *rk,
                                              const char *oauthbearer_config,
                                              void *opaque));

/**
 * @brief Set socket callback.
 *
 * The socket callback is responsible for opening a socket
 * according to the supplied \p domain, \p type and \p protocol.
 * The socket shall be created with \c CLOEXEC set in a racefree fashion, if
 * possible.
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * Default:
 *  - on linux: racefree CLOEXEC
 *  - others  : non-racefree CLOEXEC
 *
 * @remark The callback will be called from an internal librdkafka thread.
 */
RD_EXPORT
void rd_kafka_conf_set_socket_cb(rd_kafka_conf_t *conf,
                                  int (*socket_cb) (int domain, int type,
                                                    int protocol,
                                                    void *opaque));



/**
 * @brief Set connect callback.
 *
 * The connect callback is responsible for connecting socket \p sockfd
 * to peer address \p addr.
 * The \p id field contains the broker identifier.
 *
 * \p connect_cb shall return 0 on success (socket connected) or an error
 * number (errno) on error.
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * @remark The callback will be called from an internal librdkafka thread.
 */
RD_EXPORT void
rd_kafka_conf_set_connect_cb (rd_kafka_conf_t *conf,
                              int (*connect_cb) (int sockfd,
                                                 const struct sockaddr *addr,
                                                 int addrlen,
                                                 const char *id,
                                                 void *opaque));

/**
 * @brief Set close socket callback.
 *
 * Close a socket (optionally opened with socket_cb()).
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * @remark The callback will be called from an internal librdkafka thread.
 */
RD_EXPORT void
rd_kafka_conf_set_closesocket_cb (rd_kafka_conf_t *conf,
                                  int (*closesocket_cb) (int sockfd,
                                                         void *opaque));



#ifndef _WIN32
/**
 * @brief Set open callback.
 *
 * The open callback is responsible for opening the file specified by
 * pathname, flags and mode.
 * The file shall be opened with \c CLOEXEC set in a racefree fashion, if
 * possible.
 *
 * Default:
 *  - on linux: racefree CLOEXEC
 *  - others  : non-racefree CLOEXEC
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * @remark The callback will be called from an internal librdkafka thread.
 */
RD_EXPORT
void rd_kafka_conf_set_open_cb (rd_kafka_conf_t *conf,
                                int (*open_cb) (const char *pathname,
                                                int flags, mode_t mode,
                                                void *opaque));
#endif


/**
 * @brief Sets the verification callback of the broker certificate
 *
 * The verification callback is triggered from internal librdkafka threads
 * upon connecting to a broker. On each connection attempt the callback
 * will be called for each certificate in the broker's certificate chain,
 * starting at the root certification, as long as the application callback
 * returns 1 (valid certificate).
 * \c broker_name and \c broker_id correspond to the broker the connection
 * is being made to.
 * The \c x509_error argument indicates if OpenSSL's verification of
 * the certificate succeed (0) or failed (an OpenSSL error code).
 * The application may set the SSL context error code by returning 0
 * from the verify callback and providing a non-zero SSL context error code
 * in \c x509_error.
 * If the verify callback sets \c x509_error to 0, returns 1, and the
 * original \c x509_error was non-zero, the error on the SSL context will
 * be cleared.
 * \c x509_error is always a valid pointer to an int.
 *
 * \c depth is the depth of the current certificate in the chain, starting
 * at the root certificate.
 *
 * The certificate itself is passed in binary DER format in \c buf of
 * size \c size.
 *
 * The callback must return 1 if verification succeeds, or
 * 0 if verification fails and then write a human-readable error message
 * to \c errstr (limited to \c errstr_size bytes, including nul-term).
 *
 * The callback's \p opaque argument is the opaque set with
 * rd_kafka_conf_set_opaque().
 *
 * @returns RD_KAFKA_CONF_OK if SSL is supported in this build, else
 *          RD_KAFKA_CONF_INVALID.
 *
 * @warning This callback will be called from internal librdkafka threads.
 *
 * @remark See <openssl/x509_vfy.h> in the OpenSSL source distribution
 *         for a list of \p x509_error codes.
 */
RD_EXPORT
rd_kafka_conf_res_t rd_kafka_conf_set_ssl_cert_verify_cb (
        rd_kafka_conf_t *conf,
        int (*ssl_cert_verify_cb) (rd_kafka_t *rk,
                                   const char *broker_name,
                                   int32_t broker_id,
                                   int *x509_error,
                                   int depth,
                                   const char *buf, size_t size,
                                   char *errstr, size_t errstr_size,
                                   void *opaque));


/**
 * @enum rd_kafka_cert_type_t
 *
 * @brief SSL certificate type
 *
 * @sa rd_kafka_conf_set_ssl_cert
 */
typedef enum rd_kafka_cert_type_t {
        RD_KAFKA_CERT_PUBLIC_KEY,  /**< Client's public key */
        RD_KAFKA_CERT_PRIVATE_KEY, /**< Client's private key */
        RD_KAFKA_CERT_CA,          /**< CA certificate */
        RD_KAFKA_CERT__CNT,
} rd_kafka_cert_type_t;

/**
 * @enum rd_kafka_cert_enc_t
 *
 * @brief SSL certificate encoding
 *
 * @sa rd_kafka_conf_set_ssl_cert
 */
typedef enum rd_kafka_cert_enc_t {
        RD_KAFKA_CERT_ENC_PKCS12,  /**< PKCS#12 */
        RD_KAFKA_CERT_ENC_DER,     /**< DER / binary X.509 ASN1 */
        RD_KAFKA_CERT_ENC_PEM,     /**< PEM */
        RD_KAFKA_CERT_ENC__CNT,
} rd_kafka_cert_enc_t;


/**
 * @brief Set certificate/key \p cert_type from the \p cert_enc encoded
 *        memory at \p buffer of \p size bytes.
 *
 * @param conf Configuration object.
 * @param cert_type Certificate or key type to configure.
 * @param cert_enc  Buffer \p encoding type.
 * @param buffer Memory pointer to encoded certificate or key.
 *               The memory is not referenced after this function returns.
 * @param size Size of memory at \p buffer.
 * @param errstr Memory were a human-readable error string will be written
 *               on failure.
 * @param errstr_size Size of \p errstr, including space for nul-terminator.
 *
 * @returns RD_KAFKA_CONF_OK on success or RD_KAFKA_CONF_INVALID if the
 *          memory in \p buffer is of incorrect encoding, or if librdkafka
 *          was not built with SSL support.
 *
 * @remark Calling this method multiple times with the same \p cert_type
 *         will replace the previous value.
 *
 * @remark Calling this method with \p buffer set to NULL will clear the
 *         configuration for \p cert_type.
 *
 * @remark The private key may require a password, which must be specified
 *         with the `ssl.key.password` configuration property prior to
 *         calling this function.
 *
 * @remark Private and public keys in PEM format may also be set with the
 *         `ssl.key.pem` and `ssl.certificate.pem` configuration properties.
 *
 * @remark CA certificate in PEM format may also be set with the
 *         `ssl.ca.pem` configuration property.
 */
RD_EXPORT rd_kafka_conf_res_t
rd_kafka_conf_set_ssl_cert (rd_kafka_conf_t *conf,
                            rd_kafka_cert_type_t cert_type,
                            rd_kafka_cert_enc_t cert_enc,
                            const void *buffer, size_t size,
                            char *errstr, size_t errstr_size);


/**
 * @brief Set callback_data for OpenSSL engine.
 *
 * @param conf Configuration object.
 * @param callback_data passed to engine callbacks,
 *                      e.g. \c ENGINE_load_ssl_client_cert.
 *
 * @remark The \c ssl.engine.location configuration must be set for this
 *         to have affect.
 *
 * @remark The memory pointed to by \p value must remain valid for the
 *         lifetime of the configuration object and any Kafka clients that
 *         use it.
 */
RD_EXPORT
void rd_kafka_conf_set_engine_callback_data (rd_kafka_conf_t *conf,
                                             void *callback_data);


/**
 * @brief Sets the application's opaque pointer that will be passed to callbacks
 *
 * @sa rd_kafka_opaque()
 */
RD_EXPORT
void rd_kafka_conf_set_opaque(rd_kafka_conf_t *conf, void *opaque);

/**
 * @brief Retrieves the opaque pointer previously set
 *        with rd_kafka_conf_set_opaque()
 */
RD_EXPORT
void *rd_kafka_opaque(const rd_kafka_t *rk);



/**
 * @brief Sets the default topic configuration to use for automatically
 *        subscribed topics (e.g., through pattern-matched topics).
 *        The topic config object is not usable after this call.
 *
 * @warning Any topic configuration settings that have been set on the
 *          global rd_kafka_conf_t object will be overwritten by this call
 *          since the implicitly created default topic config object is
 *          replaced by the user-supplied one.
 *
 * @deprecated Set default topic level configuration on the
 *             global rd_kafka_conf_t object instead.
 */
RD_EXPORT
void rd_kafka_conf_set_default_topic_conf (rd_kafka_conf_t *conf,
                                           rd_kafka_topic_conf_t *tconf);

/**
 * @brief Gets the default topic configuration as previously set with
 *        rd_kafka_conf_set_default_topic_conf() or that was implicitly created
 *        by configuring a topic-level property on the global \p conf object.
 *
 * @returns the \p conf's default topic configuration (if any), or NULL.
 *
 * @warning The returned topic configuration object is owned by the \p conf
 *          object. It may be modified but not destroyed and its lifetime is
 *          the same as the \p conf object or the next call to
 *          rd_kafka_conf_set_default_topic_conf().
 */
RD_EXPORT rd_kafka_topic_conf_t *
rd_kafka_conf_get_default_topic_conf (rd_kafka_conf_t *conf);


/**
 * @brief Retrieve configuration value for property \p name.
 *
 * If \p dest is non-NULL the value will be written to \p dest with at
 * most \p dest_size.
 *
 * \p *dest_size is updated to the full length of the value, thus if
 * \p *dest_size initially is smaller than the full length the application
 * may reallocate \p dest to fit the returned \p *dest_size and try again.
 *
 * If \p dest is NULL only the full length of the value is returned.
 *
 * Fallthrough:
 * Topic-level configuration properties from the \c default_topic_conf
 * may be retrieved using this interface.
 *
 * @returns \p RD_KAFKA_CONF_OK if the property name matched, else
 * \p RD_KAFKA_CONF_UNKNOWN.
 */
RD_EXPORT
rd_kafka_conf_res_t rd_kafka_conf_get (const rd_kafka_conf_t *conf,
                                       const char *name,
                                       char *dest, size_t *dest_size);


/**
 * @brief Retrieve topic configuration value for property \p name.
 *
 * @sa rd_kafka_conf_get()
 */
RD_EXPORT
rd_kafka_conf_res_t rd_kafka_topic_conf_get (const rd_kafka_topic_conf_t *conf,
                                             const char *name,
                                             char *dest, size_t *dest_size);


/**
 * @brief Dump the configuration properties and values of \p conf to an array
 *        with \"key\", \"value\" pairs.
 *
 * The number of entries in the array is returned in \p *cntp.
 *
 * The dump must be freed with `rd_kafka_conf_dump_free()`.
 */
RD_EXPORT
const char **rd_kafka_conf_dump(rd_kafka_conf_t *conf, size_t *cntp);


/**
 * @brief Dump the topic configuration properties and values of \p conf
 *        to an array with \"key\", \"value\" pairs.
 *
 * The number of entries in the array is returned in \p *cntp.
 *
 * The dump must be freed with `rd_kafka_conf_dump_free()`.
 */
RD_EXPORT
const char **rd_kafka_topic_conf_dump(rd_kafka_topic_conf_t *conf,
				       size_t *cntp);

/**
 * @brief Frees a configuration dump returned from `rd_kafka_conf_dump()` or
 *        `rd_kafka_topic_conf_dump().
 */
RD_EXPORT
void rd_kafka_conf_dump_free(const char **arr, size_t cnt);

/**
 * @brief Prints a table to \p fp of all supported configuration properties,
 *        their default values as well as a description.
 *
 * @remark All properties and properties and values are shown, even those
 *         that have been disabled at build time due to missing dependencies.
 */
RD_EXPORT
void rd_kafka_conf_properties_show(FILE *fp);

/**@}*/


/**
 * @name Topic configuration
 * @{
 *
 * @brief Topic configuration property interface
 *
 */


/**
 * @brief Create topic configuration object
 *
 * @sa Same semantics as for rd_kafka_conf_new().
 */
RD_EXPORT
rd_kafka_topic_conf_t *rd_kafka_topic_conf_new(void);


/**
 * @brief Creates a copy/duplicate of topic configuration object \p conf.
 */
RD_EXPORT
rd_kafka_topic_conf_t *rd_kafka_topic_conf_dup(const rd_kafka_topic_conf_t
						*conf);

/**
 * @brief Creates a copy/duplicate of \p rk 's default topic configuration
 *        object.
 */
RD_EXPORT
rd_kafka_topic_conf_t *rd_kafka_default_topic_conf_dup (rd_kafka_t *rk);


/**
 * @brief Destroys a topic conf object.
 */
RD_EXPORT
void rd_kafka_topic_conf_destroy(rd_kafka_topic_conf_t *topic_conf);


/**
 * @brief Sets a single rd_kafka_topic_conf_t value by property name.
 *
 * \p topic_conf should have been previously set up
 * with `rd_kafka_topic_conf_new()`.
 *
 * @returns rd_kafka_conf_res_t to indicate success or failure.
 */
RD_EXPORT
rd_kafka_conf_res_t rd_kafka_topic_conf_set(rd_kafka_topic_conf_t *conf,
					     const char *name,
					     const char *value,
					     char *errstr, size_t errstr_size);

/**
 * @brief Sets the application's opaque pointer that will be passed to all topic
 * callbacks as the \c rkt_opaque argument.
 *
 * @sa rd_kafka_topic_opaque()
 */
RD_EXPORT
void rd_kafka_topic_conf_set_opaque(rd_kafka_topic_conf_t *conf,
                                    void *rkt_opaque);


/**
 * @brief \b Producer: Set partitioner callback in provided topic conf object.
 *
 * The partitioner may be called in any thread at any time,
 * it may be called multiple times for the same message/key.
 *
 * The callback's \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The callback's \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * Partitioner function constraints:
 *   - MUST NOT call any rd_kafka_*() functions except:
 *       rd_kafka_topic_partition_available()
 *   - MUST NOT block or execute for prolonged periods of time.
 *   - MUST return a value between 0 and partition_cnt-1, or the
 *     special \c RD_KAFKA_PARTITION_UA value if partitioning
 *     could not be performed.
 */
RD_EXPORT
void
rd_kafka_topic_conf_set_partitioner_cb (rd_kafka_topic_conf_t *topic_conf,
					int32_t (*partitioner) (
						const rd_kafka_topic_t *rkt,
						const void *keydata,
						size_t keylen,
						int32_t partition_cnt,
						void *rkt_opaque,
						void *msg_opaque));


/**
 * @brief \b Producer: Set message queueing order comparator callback.
 *
 * The callback may be called in any thread at any time,
 * it may be called multiple times for the same message.
 *
 * Ordering comparator function constraints:
 *   - MUST be stable sort (same input gives same output).
 *   - MUST NOT call any rd_kafka_*() functions.
 *   - MUST NOT block or execute for prolonged periods of time.
 *
 * The comparator shall compare the two messages and return:
 *  - < 0 if message \p a should be inserted before message \p b.
 *  - >=0 if message \p a should be inserted after message \p b.
 *
 * @remark Insert sorting will be used to enqueue the message in the
 *         correct queue position, this comes at a cost of O(n).
 *
 * @remark If `queuing.strategy=fifo` new messages are enqueued to the
 *         tail of the queue regardless of msg_order_cmp, but retried messages
 *         are still affected by msg_order_cmp.
 *
 * @warning THIS IS AN EXPERIMENTAL API, SUBJECT TO CHANGE OR REMOVAL,
 *          DO NOT USE IN PRODUCTION.
 */
RD_EXPORT void
rd_kafka_topic_conf_set_msg_order_cmp (rd_kafka_topic_conf_t *topic_conf,
                                       int (*msg_order_cmp) (
                                               const rd_kafka_message_t *a,
                                               const rd_kafka_message_t *b));


/**
 * @brief Check if partition is available (has a leader broker).
 *
 * @returns 1 if the partition is available, else 0.
 *
 * @warning This function must only be called from inside a partitioner function
 */
RD_EXPORT
int rd_kafka_topic_partition_available(const rd_kafka_topic_t *rkt,
					int32_t partition);


/*******************************************************************
 *								   *
 * Partitioners provided by rdkafka                                *
 *								   *
 *******************************************************************/

/**
 * @brief Random partitioner.
 *
 * Will try not to return unavailable partitions.
 *
 * The \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * @returns a random partition between 0 and \p partition_cnt - 1.
 *
 */
RD_EXPORT
int32_t rd_kafka_msg_partitioner_random(const rd_kafka_topic_t *rkt,
					 const void *key, size_t keylen,
					 int32_t partition_cnt,
					 void *rkt_opaque, void *msg_opaque);

/**
 * @brief Consistent partitioner.
 *
 * Uses consistent hashing to map identical keys onto identical partitions.
 *
 * The \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * @returns a \"random\" partition between 0 and \p partition_cnt - 1 based on
 *          the CRC value of the key
 */
RD_EXPORT
int32_t rd_kafka_msg_partitioner_consistent (const rd_kafka_topic_t *rkt,
					 const void *key, size_t keylen,
					 int32_t partition_cnt,
					 void *rkt_opaque, void *msg_opaque);

/**
 * @brief Consistent-Random partitioner.
 *
 * This is the default partitioner.
 * Uses consistent hashing to map identical keys onto identical partitions, and
 * messages without keys will be assigned via the random partitioner.
 *
 * The \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * @returns a \"random\" partition between 0 and \p partition_cnt - 1 based on
 *          the CRC value of the key (if provided)
 */
RD_EXPORT
int32_t rd_kafka_msg_partitioner_consistent_random (const rd_kafka_topic_t *rkt,
           const void *key, size_t keylen,
           int32_t partition_cnt,
           void *rkt_opaque, void *msg_opaque);


/**
 * @brief Murmur2 partitioner (Java compatible).
 *
 * Uses consistent hashing to map identical keys onto identical partitions
 * using Java-compatible Murmur2 hashing.
 *
 * The \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * @returns a partition between 0 and \p partition_cnt - 1.
 */
RD_EXPORT
int32_t rd_kafka_msg_partitioner_murmur2 (const rd_kafka_topic_t *rkt,
                                          const void *key, size_t keylen,
                                          int32_t partition_cnt,
                                          void *rkt_opaque,
                                          void *msg_opaque);

/**
 * @brief Consistent-Random Murmur2 partitioner (Java compatible).
 *
 * Uses consistent hashing to map identical keys onto identical partitions
 * using Java-compatible Murmur2 hashing.
 * Messages without keys will be assigned via the random partitioner.
 *
 * The \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * @returns a partition between 0 and \p partition_cnt - 1.
 */
RD_EXPORT
int32_t rd_kafka_msg_partitioner_murmur2_random (const rd_kafka_topic_t *rkt,
                                                 const void *key, size_t keylen,
                                                 int32_t partition_cnt,
                                                 void *rkt_opaque,
                                                 void *msg_opaque);


/**
 * @brief FNV-1a partitioner.
 *
 * Uses consistent hashing to map identical keys onto identical partitions
 * using FNV-1a hashing.
 *
 * The \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * @returns a partition between 0 and \p partition_cnt - 1.
 */
RD_EXPORT
int32_t rd_kafka_msg_partitioner_fnv1a (const rd_kafka_topic_t *rkt,
                                        const void *key, size_t keylen,
                                        int32_t partition_cnt,
                                        void *rkt_opaque,
                                        void *msg_opaque);


/**
 * @brief Consistent-Random FNV-1a partitioner.
 *
 * Uses consistent hashing to map identical keys onto identical partitions
 * using FNV-1a hashing.
 * Messages without keys will be assigned via the random partitioner.
 *
 * The \p rkt_opaque argument is the opaque set by
 * rd_kafka_topic_conf_set_opaque().
 * The \p msg_opaque argument is the per-message opaque
 * passed to produce().
 *
 * @returns a partition between 0 and \p partition_cnt - 1.
 */
RD_EXPORT
int32_t rd_kafka_msg_partitioner_fnv1a_random (const rd_kafka_topic_t *rkt,
                                               const void *key, size_t keylen,
                                               int32_t partition_cnt,
                                               void *rkt_opaque,
                                               void *msg_opaque);


/**@}*/



/**
 * @name Main Kafka and Topic object handles
 * @{
 *
 *
 */




/**
 * @brief Creates a new Kafka handle and starts its operation according to the
 *        specified \p type (\p RD_KAFKA_CONSUMER or \p RD_KAFKA_PRODUCER).
 *
 * \p conf is an optional struct created with `rd_kafka_conf_new()` that will
 * be used instead of the default configuration.
 * The \p conf object is freed by this function on success and must not be used
 * or destroyed by the application sub-sequently.
 * See `rd_kafka_conf_set()` et.al for more information.
 *
 * \p errstr must be a pointer to memory of at least size \p errstr_size where
 * `rd_kafka_new()` may write a human readable error message in case the
 * creation of a new handle fails. In which case the function returns NULL.
 *
 * @remark \b RD_KAFKA_CONSUMER: When a new \p RD_KAFKA_CONSUMER
 *           rd_kafka_t handle is created it may either operate in the
 *           legacy simple consumer mode using the rd_kafka_consume_start()
 *           interface, or the High-level KafkaConsumer API.
 * @remark An application must only use one of these groups of APIs on a given
 *         rd_kafka_t RD_KAFKA_CONSUMER handle.

 *
 * @returns The Kafka handle on success or NULL on error (see \p errstr)
 *
 * @sa To destroy the Kafka handle, use rd_kafka_destroy().
 */
RD_EXPORT
rd_kafka_t *rd_kafka_new(rd_kafka_type_t type, rd_kafka_conf_t *conf,
			  char *errstr, size_t errstr_size);


/**
 * @brief Destroy Kafka handle.
 *
 * @remark This is a blocking operation.
 * @remark rd_kafka_consumer_close() will be called from this function
 *         if the instance type is RD_KAFKA_CONSUMER, a \c group.id was
 *         configured, and the rd_kafka_consumer_close() was not
 *         explicitly called by the application. This in turn may
 *         trigger consumer callbacks, such as rebalance_cb.
 *         Use rd_kafka_destroy_flags() with
 *         RD_KAFKA_DESTROY_F_NO_CONSUMER_CLOSE to avoid this behaviour.
 *
 * @sa rd_kafka_destroy_flags()
 */
RD_EXPORT
void        rd_kafka_destroy(rd_kafka_t *rk);


/**
 * @brief Destroy Kafka handle according to specified destroy flags
 *
 */
RD_EXPORT
void rd_kafka_destroy_flags (rd_kafka_t *rk, int flags);

/**
 * @brief Flags for rd_kafka_destroy_flags()
 */

/*!
 * Don't call consumer_close() to leave group and commit final offsets.
 *
 * This also disables consumer callbacks to be called from rd_kafka_destroy*(),
 * such as rebalance_cb.
 *
 * The consumer group handler is still closed internally, but from an
 * application perspective none of the functionality from consumer_close()
 * is performed.
 */
#define RD_KAFKA_DESTROY_F_NO_CONSUMER_CLOSE 0x8



/**
 * @brief Returns Kafka handle name.
 */
RD_EXPORT
const char *rd_kafka_name(const rd_kafka_t *rk);


/**
 * @brief Returns Kafka handle type.
 */
RD_EXPORT
rd_kafka_type_t rd_kafka_type(const rd_kafka_t *rk);


/**
 * @brief Returns this client's broker-assigned group member id.
 *
 * @remark This currently requires the high-level KafkaConsumer
 *
 * @returns An allocated string containing the current broker-assigned group
 *          member id, or NULL if not available.
 *          The application must free the string with \p free() or
 *          rd_kafka_mem_free()
 */
RD_EXPORT
char *rd_kafka_memberid (const rd_kafka_t *rk);



/**
 * @brief Returns the ClusterId as reported in broker metadata.
 *
 * @param rk         Client instance.
 * @param timeout_ms If there is no cached value from metadata retrieval
 *                   then this specifies the maximum amount of time
 *                   (in milliseconds) the call will block waiting
 *                   for metadata to be retrieved.
 *                   Use 0 for non-blocking calls.

 * @remark Requires broker version >=0.10.0 and api.version.request=true.
 *
 * @remark The application must free the returned pointer
 *         using rd_kafka_mem_free().
 *
 * @returns a newly allocated string containing the ClusterId, or NULL
 *          if no ClusterId could be retrieved in the allotted timespan.
 */
RD_EXPORT
char *rd_kafka_clusterid (rd_kafka_t *rk, int timeout_ms);


/**
 * @brief Returns the current ControllerId as reported in broker metadata.
 *
 * @param rk         Client instance.
 * @param timeout_ms If there is no cached value from metadata retrieval
 *                   then this specifies the maximum amount of time
 *                   (in milliseconds) the call will block waiting
 *                   for metadata to be retrieved.
 *                   Use 0 for non-blocking calls.

 * @remark Requires broker version >=0.10.0 and api.version.request=true.
 *
 * @returns the controller broker id (>= 0), or -1 if no ControllerId could be
 *          retrieved in the allotted timespan.
 */
RD_EXPORT
int32_t rd_kafka_controllerid (rd_kafka_t *rk, int timeout_ms);


/**
 * @brief Creates a new topic handle for topic named \p topic.
 *
 * \p conf is an optional configuration for the topic created with
 * `rd_kafka_topic_conf_new()` that will be used instead of the default
 * topic configuration.
 * The \p conf object is freed by this function and must not be used or
 * destroyed by the application sub-sequently.
 * See `rd_kafka_topic_conf_set()` et.al for more information.
 *
 * Topic handles are refcounted internally and calling rd_kafka_topic_new()
 * again with the same topic name will return the previous topic handle
 * without updating the original handle's configuration.
 * Applications must eventually call rd_kafka_topic_destroy() for each
 * succesfull call to rd_kafka_topic_new() to clear up resources.
 *
 * @returns the new topic handle or NULL on error (use rd_kafka_errno2err()
 *          to convert system \p errno to an rd_kafka_resp_err_t error code.
 *
 * @sa rd_kafka_topic_destroy()
 */
RD_EXPORT
rd_kafka_topic_t *rd_kafka_topic_new(rd_kafka_t *rk, const char *topic,
				      rd_kafka_topic_conf_t *conf);



/**
 * @brief Loose application's topic handle refcount as previously created
 *        with `rd_kafka_topic_new()`.
 *
 * @remark Since topic objects are refcounted (both internally and for the app)
 *         the topic object might not actually be destroyed by this call,
 *         but the application must consider the object destroyed.
 */
RD_EXPORT
void rd_kafka_topic_destroy(rd_kafka_topic_t *rkt);


/**
 * @brief Returns the topic name.
 */
RD_EXPORT
const char *rd_kafka_topic_name(const rd_kafka_topic_t *rkt);


/**
 * @brief Get the \p rkt_opaque pointer that was set in the topic configuration
 *        with rd_kafka_topic_conf_set_opaque().
 */
RD_EXPORT
void *rd_kafka_topic_opaque (const rd_kafka_topic_t *rkt);


/**
 * @brief Unassigned partition.
 *
 * The unassigned partition is used by the producer API for messages
 * that should be partitioned using the configured or default partitioner.
 */
#define RD_KAFKA_PARTITION_UA  ((int32_t)-1)


/**
 * @brief Polls the provided kafka handle for events.
 *
 * Events will cause application provided callbacks to be called.
 *
 * The \p timeout_ms argument specifies the maximum amount of time
 * (in milliseconds) that the call will block waiting for events.
 * For non-blocking calls, provide 0 as \p timeout_ms.
 * To wait indefinately for an event, provide -1.
 *
 * @remark  An application should make sure to call poll() at regular
 *          intervals to serve any queued callbacks waiting to be called.
 * @remark  If your producer doesn't have any callback set (in particular
 *          via rd_kafka_conf_set_dr_msg_cb or rd_kafka_conf_set_error_cb)
 *          you might chose not to call poll(), though this is not
 *          recommended.
 *
 * Events:
 *   - delivery report callbacks  (if dr_cb/dr_msg_cb is configured) [producer]
 *   - error callbacks (rd_kafka_conf_set_error_cb()) [all]
 *   - stats callbacks (rd_kafka_conf_set_stats_cb()) [all]
 *   - throttle callbacks (rd_kafka_conf_set_throttle_cb()) [all]
 *   - OAUTHBEARER token refresh callbacks (rd_kafka_conf_set_oauthbearer_token_refresh_cb()) [all]
 *
 * @returns the number of events served.
 */
RD_EXPORT
int rd_kafka_poll(rd_kafka_t *rk, int timeout_ms);


/**
 * @brief Cancels the current callback dispatcher (rd_kafka_poll(),
 *        rd_kafka_consume_callback(), etc).
 *
 * A callback may use this to force an immediate return to the calling
 * code (caller of e.g. rd_kafka_poll()) without processing any further
 * events.
 *
 * @remark This function MUST ONLY be called from within a librdkafka callback.
 */
RD_EXPORT
void rd_kafka_yield (rd_kafka_t *rk);




/**
 * @brief Pause producing or consumption for the provided list of partitions.
 *
 * Success or error is returned per-partition \p err in the \p partitions list.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_pause_partitions (rd_kafka_t *rk,
			   rd_kafka_topic_partition_list_t *partitions);



/**
 * @brief Resume producing consumption for the provided list of partitions.
 *
 * Success or error is returned per-partition \p err in the \p partitions list.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_resume_partitions (rd_kafka_t *rk,
			    rd_kafka_topic_partition_list_t *partitions);




/**
 * @brief Query broker for low (oldest/beginning) and high (newest/end) offsets
 *        for partition.
 *
 * Offsets are returned in \p *low and \p *high respectively.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an error code on failure.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_query_watermark_offsets (rd_kafka_t *rk,
		      const char *topic, int32_t partition,
		      int64_t *low, int64_t *high, int timeout_ms);


/**
 * @brief Get last known low (oldest/beginning) and high (newest/end) offsets
 *        for partition.
 *
 * The low offset is updated periodically (if statistics.interval.ms is set)
 * while the high offset is updated on each fetched message set from the broker.
 *
 * If there is no cached offset (either low or high, or both) then
 * RD_KAFKA_OFFSET_INVALID will be returned for the respective offset.
 *
 * Offsets are returned in \p *low and \p *high respectively.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an error code on failure.
 *
 * @remark Shall only be used with an active consumer instance.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_get_watermark_offsets (rd_kafka_t *rk,
				const char *topic, int32_t partition,
				int64_t *low, int64_t *high);



/**
 * @brief Look up the offsets for the given partitions by timestamp.
 *
 * The returned offset for each partition is the earliest offset whose
 * timestamp is greater than or equal to the given timestamp in the
 * corresponding partition.
 *
 * The timestamps to query are represented as \c offset in \p offsets
 * on input, and \c offset will contain the offset on output.
 *
 * The function will block for at most \p timeout_ms milliseconds.
 *
 * @remark Duplicate Topic+Partitions are not supported.
 * @remark Per-partition errors may be returned in \c rd_kafka_topic_partition_t.err
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR if offsets were be queried (do note
 *          that per-partition errors might be set),
 *          RD_KAFKA_RESP_ERR__TIMED_OUT if not all offsets could be fetched
 *          within \p timeout_ms,
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if the \p offsets list is empty,
 *          RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION if all partitions are unknown,
 *          RD_KAFKA_RESP_ERR_LEADER_NOT_AVAILABLE if unable to query leaders
 *          for the given partitions.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_offsets_for_times (rd_kafka_t *rk,
                            rd_kafka_topic_partition_list_t *offsets,
                            int timeout_ms);



/**
 * @brief Allocate and zero memory using the same allocator librdkafka uses.
 *
 * This is typically an abstraction for the calloc(3) call and makes sure
 * the application can use the same memory allocator as librdkafka for
 * allocating pointers that are used by librdkafka.
 *
 * \p rk can be set to return memory allocated by a specific \c rk instance
 * otherwise pass NULL for \p rk.
 *
 * @remark Memory allocated by rd_kafka_mem_calloc() must be freed using
 *         rd_kafka_mem_free()
 */
RD_EXPORT
void *rd_kafka_mem_calloc (rd_kafka_t *rk, size_t num, size_t size);



/**
 * @brief Allocate memory using the same allocator librdkafka uses.
 *
 * This is typically an abstraction for the malloc(3) call and makes sure
 * the application can use the same memory allocator as librdkafka for
 * allocating pointers that are used by librdkafka.
 *
 * \p rk can be set to return memory allocated by a specific \c rk instance
 * otherwise pass NULL for \p rk.
 *
 * @remark Memory allocated by rd_kafka_mem_malloc() must be freed using
 *         rd_kafka_mem_free()
 */
RD_EXPORT
void *rd_kafka_mem_malloc (rd_kafka_t *rk, size_t size);



/**
 * @brief Free pointer returned by librdkafka
 *
 * This is typically an abstraction for the free(3) call and makes sure
 * the application can use the same memory allocator as librdkafka for
 * freeing pointers returned by librdkafka.
 *
 * In standard setups it is usually not necessary to use this interface
 * rather than the free(3) functione.
 *
 * \p rk must be set for memory returned by APIs that take an \c rk argument,
 * for other APIs pass NULL for \p rk.
 *
 * @remark rd_kafka_mem_free() must only be used for pointers returned by APIs
 *         that explicitly mention using this function for freeing.
 */
RD_EXPORT
void rd_kafka_mem_free (rd_kafka_t *rk, void *ptr);


/**@}*/





/**
 * @name Queue API
 * @{
 *
 * Message queues allows the application to re-route consumed messages
 * from multiple topic+partitions into one single queue point.
 * This queue point containing messages from a number of topic+partitions
 * may then be served by a single rd_kafka_consume*_queue() call,
 * rather than one call per topic+partition combination.
 */


/**
 * @brief Create a new message queue.
 *
 * See rd_kafka_consume_start_queue(), rd_kafka_consume_queue(), et.al.
 */
RD_EXPORT
rd_kafka_queue_t *rd_kafka_queue_new(rd_kafka_t *rk);

/**
 * Destroy a queue, purging all of its enqueued messages.
 */
RD_EXPORT
void rd_kafka_queue_destroy(rd_kafka_queue_t *rkqu);


/**
 * @returns a reference to the main librdkafka event queue.
 * This is the queue served by rd_kafka_poll().
 *
 * Use rd_kafka_queue_destroy() to loose the reference.
 */
RD_EXPORT
rd_kafka_queue_t *rd_kafka_queue_get_main (rd_kafka_t *rk);


/**
 * @returns a reference to the librdkafka consumer queue.
 * This is the queue served by rd_kafka_consumer_poll().
 *
 * Use rd_kafka_queue_destroy() to loose the reference.
 *
 * @remark rd_kafka_queue_destroy() MUST be called on this queue
 *         prior to calling rd_kafka_consumer_close().
 */
RD_EXPORT
rd_kafka_queue_t *rd_kafka_queue_get_consumer (rd_kafka_t *rk);

/**
 * @returns a reference to the partition's queue, or NULL if
 *          partition is invalid.
 *
 * Use rd_kafka_queue_destroy() to loose the reference.
 *
 * @remark rd_kafka_queue_destroy() MUST be called on this queue
 *
 * @remark This function only works on consumers.
 */
RD_EXPORT
rd_kafka_queue_t *rd_kafka_queue_get_partition (rd_kafka_t *rk,
                                                const char *topic,
                                                int32_t partition);

/**
 * @returns a reference to the background thread queue, or NULL if the
 *          background queue is not enabled.
 *
 * To enable the background thread queue set a generic event handler callback
 * with rd_kafka_conf_set_background_event_cb() on the client instance
 * configuration object (rd_kafka_conf_t).
 *
 * The background queue is polled and served by librdkafka and MUST NOT be
 * polled, forwarded, or otherwise managed by the application, it may only
 * be used as the destination queue passed to queue-enabled APIs, such as
 * the Admin API.
 *
 * The background thread queue provides the application with an automatically
 * polled queue that triggers the event callback in a background thread,
 * this background thread is completely managed by librdkafka.
 *
 * Use rd_kafka_queue_destroy() to loose the reference.
 *
 * @warning The background queue MUST NOT be read from (polled, consumed, etc),
 *          or forwarded from.
 */
RD_EXPORT
rd_kafka_queue_t *rd_kafka_queue_get_background (rd_kafka_t *rk);


/**
 * @brief Forward/re-route queue \p src to \p dst.
 * If \p dst is \c NULL the forwarding is removed.
 *
 * The internal refcounts for both queues are increased.
 *
 * @remark Regardless of whether \p dst is NULL or not, after calling this
 *         function, \p src will not forward it's fetch queue to the consumer
 *         queue.
 */
RD_EXPORT
void rd_kafka_queue_forward (rd_kafka_queue_t *src, rd_kafka_queue_t *dst);

/**
 * @brief Forward librdkafka logs (and debug) to the specified queue
 *        for serving with one of the ..poll() calls.
 *
 *        This allows an application to serve log callbacks (\c log_cb)
 *        in its thread of choice.
 *
 * @param rk   Client instance.
 * @param rkqu Queue to forward logs to. If the value is NULL the logs
 *        are forwarded to the main queue.
 *
 * @remark The configuration property \c log.queue MUST also be set to true.
 *
 * @remark librdkafka maintains its own reference to the provided queue.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an error code on error.
 */
RD_EXPORT
rd_kafka_resp_err_t rd_kafka_set_log_queue (rd_kafka_t *rk,
                                            rd_kafka_queue_t *rkqu);


/**
 * @returns the current number of elements in queue.
 */
RD_EXPORT
size_t rd_kafka_queue_length (rd_kafka_queue_t *rkqu);


/**
 * @brief Enable IO event triggering for queue.
 *
 * To ease integration with IO based polling loops this API
 * allows an application to create a separate file-descriptor
 * that librdkafka will write \p payload (of size \p size) to
 * whenever a new element is enqueued on a previously empty queue.
 *
 * To remove event triggering call with \p fd = -1.
 *
 * librdkafka will maintain a copy of the \p payload.
 *
 * @remark IO and callback event triggering are mutually exclusive.
 * @remark When using forwarded queues the IO event must only be enabled
 *         on the final forwarded-to (destination) queue.
 * @remark The file-descriptor/socket must be set to non-blocking.
 */
RD_EXPORT
void rd_kafka_queue_io_event_enable (rd_kafka_queue_t *rkqu, int fd,
                                     const void *payload, size_t size);

/**
 * @brief Enable callback event triggering for queue.
 *
 * The callback will be called from an internal librdkafka thread
 * when a new element is enqueued on a previously empty queue.
 *
 * To remove event triggering call with \p event_cb = NULL.
 *
 * The \p qev_opaque is passed to the callback's \p qev_opaque argument.
 *
 * @remark IO and callback event triggering are mutually exclusive.
 * @remark Since the callback may be triggered from internal librdkafka
 *         threads, the application must not perform any pro-longed work in
 *         the callback, or call any librdkafka APIs (for the same rd_kafka_t
 *         handle).
 */
RD_EXPORT
void rd_kafka_queue_cb_event_enable (rd_kafka_queue_t *rkqu,
                                     void (*event_cb) (rd_kafka_t *rk,
                                                       void *qev_opaque),
                                     void *qev_opaque);


/**
 * @brief Cancels the current rd_kafka_queue_poll() on \p rkqu.
 *
 * An application may use this from another thread to force
 * an immediate return to the calling code (caller of rd_kafka_queue_poll()).
 * Must not be used from signal handlers since that may cause deadlocks.
 */
RD_EXPORT
void rd_kafka_queue_yield (rd_kafka_queue_t *rkqu);


/**@}*/

/**
 *
 * @name Simple Consumer API (legacy)
 * @{
 *
 */


#define RD_KAFKA_OFFSET_BEGINNING -2  /**< Start consuming from beginning of
				       *   kafka partition queue: oldest msg */
#define RD_KAFKA_OFFSET_END       -1  /**< Start consuming from end of kafka
				       *   partition queue: next msg */
#define RD_KAFKA_OFFSET_STORED -1000  /**< Start consuming from offset retrieved
				       *   from offset store */
#define RD_KAFKA_OFFSET_INVALID -1001 /**< Invalid offset */


/** @cond NO_DOC */
#define RD_KAFKA_OFFSET_TAIL_BASE -2000 /* internal: do not use */
/** @endcond */

/**
 * @brief Start consuming \p CNT messages from topic's current end offset.
 *
 * That is, if current end offset is 12345 and \p CNT is 200, it will start
 * consuming from offset \c 12345-200 = \c 12145. */
#define RD_KAFKA_OFFSET_TAIL(CNT)  (RD_KAFKA_OFFSET_TAIL_BASE - (CNT))

/**
 * @brief Start consuming messages for topic \p rkt and \p partition
 * at offset \p offset which may either be an absolute \c (0..N)
 * or one of the logical offsets:
 *  - RD_KAFKA_OFFSET_BEGINNING
 *  - RD_KAFKA_OFFSET_END
 *  - RD_KAFKA_OFFSET_STORED
 *  - RD_KAFKA_OFFSET_TAIL
 *
 * rdkafka will attempt to keep \c queued.min.messages (config property)
 * messages in the local queue by repeatedly fetching batches of messages
 * from the broker until the threshold is reached.
 *
 * The application shall use one of the `rd_kafka_consume*()` functions
 * to consume messages from the local queue, each kafka message being
 * represented as a `rd_kafka_message_t *` object.
 *
 * `rd_kafka_consume_start()` must not be called multiple times for the same
 * topic and partition without stopping consumption first with
 * `rd_kafka_consume_stop()`.
 *
 * @returns 0 on success or -1 on error in which case errno is set accordingly:
 *  - EBUSY    - Conflicts with an existing or previous subscription
 *               (RD_KAFKA_RESP_ERR__CONFLICT)
 *  - EINVAL   - Invalid offset, or incomplete configuration (lacking group.id)
 *               (RD_KAFKA_RESP_ERR__INVALID_ARG)
 *  - ESRCH    - requested \p partition is invalid.
 *               (RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION)
 *  - ENOENT   - topic is unknown in the Kafka cluster.
 *               (RD_KAFKA_RESP_ERR__UNKNOWN_TOPIC)
 *
 * Use `rd_kafka_errno2err()` to convert sytem \c errno to `rd_kafka_resp_err_t`
 */
RD_EXPORT
int rd_kafka_consume_start(rd_kafka_topic_t *rkt, int32_t partition,
			    int64_t offset);

/**
 * @brief Same as rd_kafka_consume_start() but re-routes incoming messages to
 * the provided queue \p rkqu (which must have been previously allocated
 * with `rd_kafka_queue_new()`.
 *
 * The application must use one of the `rd_kafka_consume_*_queue()` functions
 * to receive fetched messages.
 *
 * `rd_kafka_consume_start_queue()` must not be called multiple times for the
 * same topic and partition without stopping consumption first with
 * `rd_kafka_consume_stop()`.
 * `rd_kafka_consume_start()` and `rd_kafka_consume_start_queue()` must not
 * be combined for the same topic and partition.
 */
RD_EXPORT
int rd_kafka_consume_start_queue(rd_kafka_topic_t *rkt, int32_t partition,
				  int64_t offset, rd_kafka_queue_t *rkqu);

/**
 * @brief Stop consuming messages for topic \p rkt and \p partition, purging
 * all messages currently in the local queue.
 *
 * NOTE: To enforce synchronisation this call will block until the internal
 *       fetcher has terminated and offsets are committed to configured
 *       storage method.
 *
 * The application needs to be stop all consumers before calling
 * `rd_kafka_destroy()` on the main object handle.
 *
 * @returns 0 on success or -1 on error (see `errno`).
 */
RD_EXPORT
int rd_kafka_consume_stop(rd_kafka_topic_t *rkt, int32_t partition);



/**
 * @brief Seek consumer for topic+partition to \p offset which is either an
 *        absolute or logical offset.
 *
 * If \p timeout_ms is not 0 the call will wait this long for the
 * seek to be performed. If the timeout is reached the internal state
 * will be unknown and this function returns `RD_KAFKA_RESP_ERR__TIMED_OUT`.
 * If \p timeout_ms is 0 it will initiate the seek but return
 * immediately without any error reporting (e.g., async).
 *
 * This call will purge all pre-fetched messages for the given partition, which
 * may be up to \c queued.max.message.kbytes in size. Repeated use of seek
 * may thus lead to increased network usage as messages are re-fetched from
 * the broker.
 *
 * @remark Seek must only be performed for already assigned/consumed partitions,
 *         use rd_kafka_assign() (et.al) to set the initial starting offset
 *         for a new assignmenmt.
 *
 * @returns `RD_KAFKA_RESP_ERR__NO_ERROR` on success else an error code.
 *
 * @deprecated Use rd_kafka_seek_partitions().
 */
RD_EXPORT
rd_kafka_resp_err_t rd_kafka_seek (rd_kafka_topic_t *rkt,
                                   int32_t partition,
                                   int64_t offset,
                                   int timeout_ms);



/**
 * @brief Seek consumer for partitions in \p partitions to the per-partition
 *        offset in the \c .offset field of \p partitions.
 *
 * The offset may be either absolute (>= 0) or a logical offset.
 *
 * If \p timeout_ms is not 0 the call will wait this long for the
 * seeks to be performed. If the timeout is reached the internal state
 * will be unknown for the remaining partitions to seek and this function
 * will return an error with the error code set to
 * `RD_KAFKA_RESP_ERR__TIMED_OUT`.
 *
 * If \p timeout_ms is 0 it will initiate the seek but return
 * immediately without any error reporting (e.g., async).
 *
 * This call will purge all pre-fetched messages for the given partition, which
 * may be up to \c queued.max.message.kbytes in size. Repeated use of seek
 * may thus lead to increased network usage as messages are re-fetched from
 * the broker.
 *
 * Individual partition errors are reported in the per-partition \c .err field
 * of \p partitions.
 *
 * @remark Seek must only be performed for already assigned/consumed partitions,
 *         use rd_kafka_assign() (et.al) to set the initial starting offset
 *         for a new assignmenmt.
 *
 * @returns NULL on success or an error object on failure.
 */
RD_EXPORT rd_kafka_error_t *
rd_kafka_seek_partitions (rd_kafka_t *rk,
                          rd_kafka_topic_partition_list_t *partitions,
                          int timeout_ms);


/**
 * @brief Consume a single message from topic \p rkt and \p partition
 *
 * \p timeout_ms is maximum amount of time to wait for a message to be received.
 * Consumer must have been previously started with `rd_kafka_consume_start()`.
 *
 * @returns a message object on success or \c NULL on error.
 * The message object must be destroyed with `rd_kafka_message_destroy()`
 * when the application is done with it.
 *
 * Errors (when returning NULL):
 *  - ETIMEDOUT - \p timeout_ms was reached with no new messages fetched.
 *  - ENOENT    - \p rkt + \p partition is unknown.
 *                 (no prior `rd_kafka_consume_start()` call)
 *
 * NOTE: The returned message's \c ..->err must be checked for errors.
 * NOTE: \c ..->err \c == \c RD_KAFKA_RESP_ERR__PARTITION_EOF signals that the
 *       end of the partition has been reached, which should typically not be
 *       considered an error. The application should handle this case
 *       (e.g., ignore).
 *
 * @remark on_consume() interceptors may be called from this function prior to
 *         passing message to application.
 */
RD_EXPORT
rd_kafka_message_t *rd_kafka_consume(rd_kafka_topic_t *rkt, int32_t partition,
				      int timeout_ms);



/**
 * @brief Consume up to \p rkmessages_size from topic \p rkt and \p partition
 *        putting a pointer to each message in the application provided
 *        array \p rkmessages (of size \p rkmessages_size entries).
 *
 * `rd_kafka_consume_batch()` provides higher throughput performance
 * than `rd_kafka_consume()`.
 *
 * \p timeout_ms is the maximum amount of time to wait for all of
 * \p rkmessages_size messages to be put into \p rkmessages.
 * If no messages were available within the timeout period this function
 * returns 0 and \p rkmessages remains untouched.
 * This differs somewhat from `rd_kafka_consume()`.
 *
 * The message objects must be destroyed with `rd_kafka_message_destroy()`
 * when the application is done with it.
 *
 * @returns the number of rkmessages added in \p rkmessages,
 * or -1 on error (same error codes as for `rd_kafka_consume()`.
 *
 * @sa rd_kafka_consume()
 *
 * @remark on_consume() interceptors may be called from this function prior to
 *         passing message to application.
 */
RD_EXPORT
ssize_t rd_kafka_consume_batch(rd_kafka_topic_t *rkt, int32_t partition,
				int timeout_ms,
				rd_kafka_message_t **rkmessages,
				size_t rkmessages_size);



/**
 * @brief Consumes messages from topic \p rkt and \p partition, calling
 * the provided callback for each consumed messsage.
 *
 * `rd_kafka_consume_callback()` provides higher throughput performance
 * than both `rd_kafka_consume()` and `rd_kafka_consume_batch()`.
 *
 * \p timeout_ms is the maximum amount of time to wait for one or more messages
 * to arrive.
 *
 * The provided \p consume_cb function is called for each message,
 * the application \b MUST \b NOT call `rd_kafka_message_destroy()` on the
 * provided \p rkmessage.
 *
 * The \p commit_opaque argument is passed to the \p consume_cb
 * as \p commit_opaque.
 *
 * @returns the number of messages processed or -1 on error.
 *
 * @sa rd_kafka_consume()
 *
 * @remark on_consume() interceptors may be called from this function prior to
 *         passing message to application.
 *
 * @remark This function will return early if a transaction control message is
 *         received, these messages are not exposed to the application but
 *         still enqueued on the consumer queue to make sure their
 *         offsets are stored.
 *
 * @deprecated This API is deprecated and subject for future removal.
 *             There is no new callback-based consume interface, use the
 *             poll/queue based alternatives.
 */
RD_EXPORT
int rd_kafka_consume_callback(rd_kafka_topic_t *rkt, int32_t partition,
                              int timeout_ms,
                              void (*consume_cb) (rd_kafka_message_t
                                                  *rkmessage,
                                                  void *commit_opaque),
                              void *commit_opaque);


/**
 * @name Simple Consumer API (legacy): Queue consumers
 * @{
 *
 * The following `..._queue()` functions are analogue to the functions above
 * but reads messages from the provided queue \p rkqu instead.
 * \p rkqu must have been previously created with `rd_kafka_queue_new()`
 * and the topic consumer must have been started with
 * `rd_kafka_consume_start_queue()` utilising the the same queue.
 */

/**
 * @brief Consume from queue
 *
 * @sa rd_kafka_consume()
 */
RD_EXPORT
rd_kafka_message_t *rd_kafka_consume_queue(rd_kafka_queue_t *rkqu,
                                           int timeout_ms);

/**
 * @brief Consume batch of messages from queue
 *
 * @sa rd_kafka_consume_batch()
 */
RD_EXPORT
ssize_t rd_kafka_consume_batch_queue(rd_kafka_queue_t *rkqu,
				      int timeout_ms,
				      rd_kafka_message_t **rkmessages,
				      size_t rkmessages_size);

/**
 * @brief Consume multiple messages from queue with callback
 *
 * @sa rd_kafka_consume_callback()
 *
 * @deprecated This API is deprecated and subject for future removal.
 *             There is no new callback-based consume interface, use the
 *             poll/queue based alternatives.
 */
RD_EXPORT
int rd_kafka_consume_callback_queue (rd_kafka_queue_t *rkqu,
                                     int timeout_ms,
                                     void (*consume_cb) (rd_kafka_message_t
                                                         *rkmessage,
                                                         void *commit_opaque),
                                     void *commit_opaque);


/**@}*/




/**
 * @name Simple Consumer API (legacy): Topic+partition offset store.
 * @{
 *
 * If \c auto.commit.enable is true the offset is stored automatically prior to
 * returning of the message(s) in each of the rd_kafka_consume*() functions
 * above.
 */


/**
 * @brief Store offset \p offset + 1 for topic \p rkt partition \p partition.
 *
 * The \c offset + 1 will be committed (written) to broker (or file) according
 * to \c `auto.commit.interval.ms` or manual offset-less commit()
 *
 * @remark \c `enable.auto.offset.store` must be set to "false" when using
 *         this API.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an error code on error.
 */
RD_EXPORT
rd_kafka_resp_err_t rd_kafka_offset_store (rd_kafka_topic_t *rkt,
					   int32_t partition, int64_t offset);


/**
 * @brief Store offsets for next auto-commit for one or more partitions.
 *
 * The offset will be committed (written) to the offset store according
 * to \c `auto.commit.interval.ms` or manual offset-less commit().
 *
 * Per-partition success/error status propagated through each partition's
 * \c .err field.
 *
 * @remark The \c .offset field is stored as is, it will NOT be + 1.
 *
 * @remark \c `enable.auto.offset.store` must be set to "false" when using
 *         this API.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success, or
 *          RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION if none of the
 *          offsets could be stored, or
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if \c enable.auto.offset.store
 *          is true.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_offsets_store (rd_kafka_t *rk,
                        rd_kafka_topic_partition_list_t *offsets);
/**@}*/




/**
 * @name KafkaConsumer (C)
 * @{
 * @brief High-level KafkaConsumer C API
 *
 *
 *
 */

/**
 * @brief Subscribe to topic set using balanced consumer groups.
 *
 * Wildcard (regex) topics are supported:
 * any topic name in the \p topics list that is prefixed with \c \"^\" will
 * be regex-matched to the full list of topics in the cluster and matching
 * topics will be added to the subscription list.
 *
 * The full topic list is retrieved every \c topic.metadata.refresh.interval.ms
 * to pick up new or delete topics that match the subscription.
 * If there is any change to the matched topics the consumer will
 * immediately rejoin the group with the updated set of subscribed topics.
 *
 * Regex and full topic names can be mixed in \p topics.
 *
 * @remark Only the \c .topic field is used in the supplied \p topics list,
 *         all other fields are ignored.
 *
 * @remark subscribe() is an asynchronous method which returns immediately:
 *         background threads will (re)join the group, wait for group rebalance,
 *         issue any registered rebalance_cb, assign() the assigned partitions,
 *         and then start fetching messages. This cycle may take up to
 *         \c session.timeout.ms * 2 or more to complete.
 *
 * @remark A consumer error will be raised for each unavailable topic in the
 *         \p topics. The error will be RD_KAFKA_RESP_ERR_UNKNOWN_TOPIC_OR_PART
 *         for non-existent topics, and
 *         RD_KAFKA_RESP_ERR_TOPIC_AUTHORIZATION_FAILED for unauthorized topics.
 *         The consumer error will be raised through rd_kafka_consumer_poll()
 *         (et.al.) with the \c rd_kafka_message_t.err field set to one of the
 *         error codes mentioned above.
 *         The subscribe function itself is asynchronous and will not return
 *         an error on unavailable topics.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if list is empty, contains invalid
 *          topics or regexes or duplicate entries,
 *          RD_KAFKA_RESP_ERR__FATAL if the consumer has raised a fatal error.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_subscribe (rd_kafka_t *rk,
                    const rd_kafka_topic_partition_list_t *topics);


/**
 * @brief Unsubscribe from the current subscription set.
 */
RD_EXPORT
rd_kafka_resp_err_t rd_kafka_unsubscribe (rd_kafka_t *rk);


/**
 * @brief Returns the current topic subscription
 *
 * @returns An error code on failure, otherwise \p topic is updated
 *          to point to a newly allocated topic list (possibly empty).
 *
 * @remark The application is responsible for calling
 *         rd_kafka_topic_partition_list_destroy on the returned list.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_subscription (rd_kafka_t *rk,
                       rd_kafka_topic_partition_list_t **topics);



/**
 * @brief Poll the consumer for messages or events.
 *
 * Will block for at most \p timeout_ms milliseconds.
 *
 * @remark  An application should make sure to call consumer_poll() at regular
 *          intervals, even if no messages are expected, to serve any
 *          queued callbacks waiting to be called. This is especially
 *          important when a rebalance_cb has been registered as it needs
 *          to be called and handled properly to synchronize internal
 *          consumer state.
 *
 * @returns A message object which is a proper message if \p ->err is
 *          RD_KAFKA_RESP_ERR_NO_ERROR, or an event or error for any other
 *          value.
 *
 * @remark on_consume() interceptors may be called from this function prior to
 *         passing message to application.
 *
 * @remark When subscribing to topics the application must call poll at
 *         least every \c max.poll.interval.ms to remain a member of the
 *         consumer group.
 *
 * Noteworthy errors returned in \c ->err:
 * - RD_KAFKA_RESP_ERR__MAX_POLL_EXCEEDED - application failed to call
 *   poll within `max.poll.interval.ms`.
 *
 * @sa rd_kafka_message_t
 */
RD_EXPORT
rd_kafka_message_t *rd_kafka_consumer_poll (rd_kafka_t *rk, int timeout_ms);

/**
 * @brief Close down the KafkaConsumer.
 *
 * @remark This call will block until the consumer has revoked its assignment,
 *         calling the \c rebalance_cb if it is configured, committed offsets
 *         to broker, and left the consumer group.
 *         The maximum blocking time is roughly limited to session.timeout.ms.
 *
 * @returns An error code indicating if the consumer close was succesful
 *          or not.
 *          RD_KAFKA_RESP_ERR__FATAL is returned if the consumer has raised
 *          a fatal error.
 *
 * @remark The application still needs to call rd_kafka_destroy() after
 *         this call finishes to clean up the underlying handle resources.
 *
 */
RD_EXPORT
rd_kafka_resp_err_t rd_kafka_consumer_close (rd_kafka_t *rk);


/**
 * @brief Incrementally add \p partitions to the current assignment.
 *
 * If a COOPERATIVE assignor (i.e. incremental rebalancing) is being used,
 * this method should be used in a rebalance callback to adjust the current
 * assignment appropriately in the case where the rebalance type is
 * RD_KAFKA_RESP_ERR__ASSIGN_PARTITIONS. The application must pass the
 * partition list passed to the callback (or a copy of it), even if the
 * list is empty. \p partitions must not be NULL. This method may also be
 * used outside the context of a rebalance callback.
 *
 * @returns NULL on success, or an error object if the operation was
 *          unsuccessful.
 *
 * @remark The returned error object (if not NULL) must be destroyed with
 *         rd_kafka_error_destroy().
 */
RD_EXPORT rd_kafka_error_t *
rd_kafka_incremental_assign (rd_kafka_t *rk,
                             const rd_kafka_topic_partition_list_t
                             *partitions);


/**
 * @brief Incrementally remove \p partitions from the current assignment.
 *
 * If a COOPERATIVE assignor (i.e. incremental rebalancing) is being used,
 * this method should be used in a rebalance callback to adjust the current
 * assignment appropriately in the case where the rebalance type is
 * RD_KAFKA_RESP_ERR__REVOKE_PARTITIONS. The application must pass the
 * partition list passed to the callback (or a copy of it), even if the
 * list is empty. \p partitions must not be NULL. This method may also be
 * used outside the context of a rebalance callback.
 *
 * @returns NULL on success, or an error object if the operation was
 *          unsuccessful.
 *
 * @remark The returned error object (if not NULL) must be destroyed with
 *         rd_kafka_error_destroy().
 */
RD_EXPORT rd_kafka_error_t *
rd_kafka_incremental_unassign (rd_kafka_t *rk,
                               const rd_kafka_topic_partition_list_t
                               *partitions);


/**
 * @brief The rebalance protocol currently in use. This will be
 *        "NONE" if the consumer has not (yet) joined a group, else it will
 *        match the rebalance protocol ("EAGER", "COOPERATIVE") of the
 *        configured and selected assignor(s). All configured
 *        assignors must have the same protocol type, meaning
 *        online migration of a consumer group from using one
 *        protocol to another (in particular upgading from EAGER
 *        to COOPERATIVE) without a restart is not currently
 *        supported.
 *
 * @returns NULL on error, or one of "NONE", "EAGER", "COOPERATIVE" on success.
 */
RD_EXPORT
const char *rd_kafka_rebalance_protocol (rd_kafka_t *rk);


/**
 * @brief Atomic assignment of partitions to consume.
 *
 * The new \p partitions will replace the existing assignment.
 *
 * A zero-length \p partitions will treat the partitions as a valid,
 * albeit empty assignment, and maintain internal state, while a \c NULL
 * value for \p partitions will reset and clear the internal state.
 *
 * When used from a rebalance callback, the application should pass the
 * partition list passed to the callback (or a copy of it) even if the list
 * is empty (i.e. should not pass NULL in this case) so as to maintain
 * internal join state. This is not strictly required - the application
 * may adjust the assignment provided by the group. However, this is rarely
 * useful in practice.
 *
 * @returns An error code indicating if the new assignment was applied or not.
 *          RD_KAFKA_RESP_ERR__FATAL is returned if the consumer has raised
 *          a fatal error.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_assign (rd_kafka_t *rk,
                 const rd_kafka_topic_partition_list_t *partitions);

/**
 * @brief Returns the current partition assignment as set by rd_kafka_assign()
 *        or rd_kafka_incremental_assign().
 *
 * @returns An error code on failure, otherwise \p partitions is updated
 *          to point to a newly allocated partition list (possibly empty).
 *
 * @remark The application is responsible for calling
 *         rd_kafka_topic_partition_list_destroy on the returned list.
 *
 * @remark This assignment represents the partitions assigned through the
 *         assign functions and not the partitions assigned to this consumer
 *         instance by the consumer group leader.
 *         They are usually the same following a rebalance but not necessarily
 *         since an application is free to assign any partitions.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_assignment (rd_kafka_t *rk,
                     rd_kafka_topic_partition_list_t **partitions);


/**
 * @brief Check whether the consumer considers the current assignment to
 *        have been lost involuntarily. This method is only applicable for
 *        use with a high level subscribing consumer. Assignments are revoked
 *        immediately when determined to have been lost, so this method
 *        is only useful when reacting to a RD_KAFKA_EVENT_REBALANCE event
 *        or from within a rebalance_cb. Partitions that have been lost may
 *        already be owned by other members in the group and therefore
 *        commiting offsets, for example, may fail.
 *
 * @remark Calling rd_kafka_assign(), rd_kafka_incremental_assign() or
 *         rd_kafka_incremental_unassign() resets this flag.
 *
 * @returns Returns 1 if the current partition assignment is considered
 *          lost, 0 otherwise.
 */
RD_EXPORT int
rd_kafka_assignment_lost (rd_kafka_t *rk);


/**
 * @brief Commit offsets on broker for the provided list of partitions.
 *
 * \p offsets should contain \c topic, \c partition, \c offset and possibly
 * \c metadata. The \c offset should be the offset where consumption will
 * resume, i.e., the last processed offset + 1.
 * If \p offsets is NULL the current partition assignment will be used instead.
 *
 * If \p async is false this operation will block until the broker offset commit
 * is done, returning the resulting success or error code.
 *
 * If a rd_kafka_conf_set_offset_commit_cb() offset commit callback has been
 * configured the callback will be enqueued for a future call to
 * rd_kafka_poll(), rd_kafka_consumer_poll() or similar.
 *
 * @returns An error code indiciating if the commit was successful,
 *          or successfully scheduled if asynchronous, or failed.
 *          RD_KAFKA_RESP_ERR__FATAL is returned if the consumer has raised
 *          a fatal error.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_commit (rd_kafka_t *rk, const rd_kafka_topic_partition_list_t *offsets,
                 int async);


/**
 * @brief Commit message's offset on broker for the message's partition.
 *        The committed offset is the message's offset + 1.
 *
 * @sa rd_kafka_commit
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_commit_message (rd_kafka_t *rk, const rd_kafka_message_t *rkmessage,
                         int async);


/**
 * @brief Commit offsets on broker for the provided list of partitions.
 *
 * See rd_kafka_commit for \p offsets semantics.
 *
 * The result of the offset commit will be posted on the provided \p rkqu queue.
 *
 * If the application uses one of the poll APIs (rd_kafka_poll(),
 * rd_kafka_consumer_poll(), rd_kafka_queue_poll(), ..) to serve the queue
 * the \p cb callback is required.
 *
 * The \p commit_opaque argument is passed to the callback as \p commit_opaque,
 * or if using the event API the callback is ignored and the offset commit
 * result will be returned as an RD_KAFKA_EVENT_COMMIT event and the
 * \p commit_opaque value will be available with rd_kafka_event_opaque().
 *
 * If \p rkqu is NULL a temporary queue will be created and the callback will
 * be served by this call.
 *
 * @sa rd_kafka_commit()
 * @sa rd_kafka_conf_set_offset_commit_cb()
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_commit_queue (rd_kafka_t *rk,
		       const rd_kafka_topic_partition_list_t *offsets,
		       rd_kafka_queue_t *rkqu,
		       void (*cb) (rd_kafka_t *rk,
				   rd_kafka_resp_err_t err,
				   rd_kafka_topic_partition_list_t *offsets,
                                   void *commit_opaque),
		       void *commit_opaque);


/**
 * @brief Retrieve committed offsets for topics+partitions.
 *
 * The \p offset field of each requested partition will either be set to
 * stored offset or to RD_KAFKA_OFFSET_INVALID in case there was no stored
 * offset for that partition.
 *
 * Committed offsets will be returned according to the `isolation.level`
 * configuration property, if set to `read_committed` (default) then only
 * stable offsets for fully committed transactions will be returned, while
 * `read_uncommitted` may return offsets for not yet committed transactions.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success in which case the
 *          \p offset or \p err field of each \p partitions' element is filled
 *          in with the stored offset, or a partition specific error.
 *          Else returns an error code.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_committed (rd_kafka_t *rk,
		    rd_kafka_topic_partition_list_t *partitions,
		    int timeout_ms);



/**
 * @brief Retrieve current positions (offsets) for topics+partitions.
 *
 * The \p offset field of each requested partition will be set to the offset
 * of the last consumed message + 1, or RD_KAFKA_OFFSET_INVALID in case there was
 * no previous message.
 *
 * @remark  In this context the last consumed message is the offset consumed
 *          by the current librdkafka instance and, in case of rebalancing, not
 *          necessarily the last message fetched from the partition.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success in which case the
 *          \p offset or \p err field of each \p partitions' element is filled
 *          in with the stored offset, or a partition specific error.
 *          Else returns an error code.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_position (rd_kafka_t *rk,
		   rd_kafka_topic_partition_list_t *partitions);




/**
 * @returns the current consumer group metadata associated with this consumer,
 *          or NULL if \p rk is not a consumer configured with a \c group.id.
 *          This metadata object should be passed to the transactional
 *          producer's rd_kafka_send_offsets_to_transaction() API.
 *
 * @remark The returned pointer must be freed by the application using
 *         rd_kafka_consumer_group_metadata_destroy().
 *
 * @sa rd_kafka_send_offsets_to_transaction()
 */
RD_EXPORT rd_kafka_consumer_group_metadata_t *
rd_kafka_consumer_group_metadata (rd_kafka_t *rk);


/**
 * @brief Create a new consumer group metadata object.
 *        This is typically only used for writing tests.
 *
 * @param group_id The group id.
 *
 * @remark The returned pointer must be freed by the application using
 *         rd_kafka_consumer_group_metadata_destroy().
 */
RD_EXPORT rd_kafka_consumer_group_metadata_t *
rd_kafka_consumer_group_metadata_new (const char *group_id);


/**
 * @brief Create a new consumer group metadata object.
 *        This is typically only used for writing tests.
 *
 * @param group_id The group id.
 * @param generation_id The group generation id.
 * @param member_id The group member id.
 * @param group_instance_id The group instance id (may be NULL).
 *
 * @remark The returned pointer must be freed by the application using
 *         rd_kafka_consumer_group_metadata_destroy().
 */
RD_EXPORT rd_kafka_consumer_group_metadata_t *
rd_kafka_consumer_group_metadata_new_with_genid (const char *group_id,
                                                 int32_t generation_id,
                                                 const char *member_id,
                                                 const char
                                                 *group_instance_id);


/**
 * @brief Frees the consumer group metadata object as returned by
 *        rd_kafka_consumer_group_metadata().
 */
RD_EXPORT void
rd_kafka_consumer_group_metadata_destroy (rd_kafka_consumer_group_metadata_t *);


/**
 * @brief Serialize the consumer group metadata to a binary format.
 *        This is mainly for client binding use and not for application use.
 *
 * @remark The serialized metadata format is private and is not compatible
 *         across different versions or even builds of librdkafka.
 *         It should only be used in the same process runtime and must only
 *         be passed to rd_kafka_consumer_group_metadata_read().
 *
 * @param cgmd Metadata to be serialized.
 * @param bufferp On success this pointer will be updated to point to na
 *                allocated buffer containing the serialized metadata.
 *                The buffer must be freed with rd_kafka_mem_free().
 * @param sizep The pointed to size will be updated with the size of
 *              the serialized buffer.
 *
 * @returns NULL on success or an error object on failure.
 *
 * @sa rd_kafka_consumer_group_metadata_read()
 */
RD_EXPORT rd_kafka_error_t *
rd_kafka_consumer_group_metadata_write (
        const rd_kafka_consumer_group_metadata_t *cgmd,
        void **bufferp, size_t *sizep);

/**
 * @brief Reads serialized consumer group metadata and returns a
 *        consumer group metadata object.
 *        This is mainly for client binding use and not for application use.
 *
 * @remark The serialized metadata format is private and is not compatible
 *         across different versions or even builds of librdkafka.
 *         It should only be used in the same process runtime and must only
 *         be passed to rd_kafka_consumer_group_metadata_read().
 *
 * @param cgmdp On success this pointer will be updated to point to a new
 *              consumer group metadata object which must be freed with
 *              rd_kafka_consumer_group_metadata_destroy().
 * @param buffer Pointer to the serialized data.
 * @param size Size of the serialized data.
 *
 * @returns NULL on success or an error object on failure.
 *
 * @sa rd_kafka_consumer_group_metadata_write()
 */
RD_EXPORT rd_kafka_error_t *
rd_kafka_consumer_group_metadata_read (
        rd_kafka_consumer_group_metadata_t **cgmdp,
        const void *buffer, size_t size);

/**@}*/



/**
 * @name Producer API
 * @{
 *
 *
 */


/**
 * @brief Producer message flags
 */
#define RD_KAFKA_MSG_F_FREE  0x1 /**< Delegate freeing of payload to rdkafka. */
#define RD_KAFKA_MSG_F_COPY  0x2 /**< rdkafka will make a copy of the payload. */
#define RD_KAFKA_MSG_F_BLOCK 0x4 /**< Block produce*() on message queue full.
				  *   WARNING: If a delivery report callback
				  *            is used the application MUST
				  *            call rd_kafka_poll() (or equiv.)
				  *            to make sure delivered messages
				  *            are drained from the internal
				  *            delivery report queue.
				  *            Failure to do so will result
				  *            in indefinately blocking on
				  *            the produce() call when the
				  *            message queue is full. */
#define RD_KAFKA_MSG_F_PARTITION 0x8 /**< produce_batch() will honor
                                      * per-message partition. */



/**
 * @brief Produce and send a single message to broker.
 *
 * \p rkt is the target topic which must have been previously created with
 * `rd_kafka_topic_new()`.
 *
 * `rd_kafka_produce()` is an asynch non-blocking API.
 * See `rd_kafka_conf_set_dr_msg_cb` on how to setup a callback to be called
 * once the delivery status (success or failure) is known. The delivery report
 * is trigged by the application calling `rd_kafka_poll()` (at regular
 * intervals) or `rd_kafka_flush()` (at termination).
 *
 * Since producing is asynchronous, you should call `rd_kafka_flush()` before
 * you destroy the producer. Otherwise, any outstanding messages will be
 * silently discarded.
 *
 * When temporary errors occur, librdkafka automatically retries to produce the
 * messages. Retries are triggered after retry.backoff.ms and when the
 * leader broker for the given partition is available. Otherwise, librdkafka
 * falls back to polling the topic metadata to monitor when a new leader is
 * elected (see the topic.metadata.refresh.fast.interval.ms and
 * topic.metadata.refresh.interval.ms configurations) and then performs a
 * retry. A delivery error will occur if the message could not be produced
 * within message.timeout.ms.
 *
 * See the "Message reliability" chapter in INTRODUCTION.md for more
 * information.
 *
 * \p partition is the target partition, either:
 *   - RD_KAFKA_PARTITION_UA (unassigned) for
 *     automatic partitioning using the topic's partitioner function, or
 *   - a fixed partition (0..N)
 *
 * \p msgflags is zero or more of the following flags OR:ed together:
 *    RD_KAFKA_MSG_F_BLOCK - block \p produce*() call if
 *                           \p queue.buffering.max.messages or
 *                           \p queue.buffering.max.kbytes are exceeded.
 *                           Messages are considered in-queue from the point they
 *                           are accepted by produce() until their corresponding
 *                           delivery report callback/event returns.
 *                           It is thus a requirement to call
 *                           rd_kafka_poll() (or equiv.) from a separate
 *                           thread when F_BLOCK is used.
 *                           See WARNING on \c RD_KAFKA_MSG_F_BLOCK above.
 *
 *    RD_KAFKA_MSG_F_FREE - rdkafka will free(3) \p payload when it is done
 *                          with it.
 *    RD_KAFKA_MSG_F_COPY - the \p payload data will be copied and the
 *                          \p payload pointer will not be used by rdkafka
 *                          after the call returns.
 *    RD_KAFKA_MSG_F_PARTITION - produce_batch() will honour per-message
 *                               partition, either set manually or by the
 *                               configured partitioner.
 *
 *    .._F_FREE and .._F_COPY are mutually exclusive. If neither of these are
 *    set, the caller must ensure that the memory backing \p payload remains
 *    valid and is not modified or reused until the delivery callback is
 *    invoked. Other buffers passed to `rd_kafka_produce()` don't have this
 *    restriction on reuse, i.e. the memory backing the key or the topic name
 *    may be reused as soon as `rd_kafka_produce()` returns.
 *
 *    If the function returns -1 and RD_KAFKA_MSG_F_FREE was specified, then
 *    the memory associated with the payload is still the caller's
 *    responsibility.
 *
 * \p payload is the message payload of size \p len bytes.
 *
 * \p key is an optional message key of size \p keylen bytes, if non-NULL it
 * will be passed to the topic partitioner as well as be sent with the
 * message to the broker and passed on to the consumer.
 *
 * \p msg_opaque is an optional application-provided per-message opaque
 * pointer that will provided in the message's delivery report callback
 * (\c dr_msg_cb or \c dr_cb) and the \c rd_kafka_message_t \c _private field.
 *
 * @remark on_send() and on_acknowledgement() interceptors may be called
 *         from this function. on_acknowledgement() will only be called if the
 *         message fails partitioning.
 *
 * @remark If the producer is transactional (\c transactional.id is configured)
 *         producing is only allowed during an on-going transaction, namely
 *         after rd_kafka_begin_transaction() has been called.
 *
 * @returns 0 on success or -1 on error in which case errno is set accordingly:
 *  - ENOBUFS  - maximum number of outstanding messages has been reached:
 *               "queue.buffering.max.messages"
 *               (RD_KAFKA_RESP_ERR__QUEUE_FULL)
 *  - EMSGSIZE - message is larger than configured max size:
 *               "messages.max.bytes".
 *               (RD_KAFKA_RESP_ERR_MSG_SIZE_TOO_LARGE)
 *  - ESRCH    - requested \p partition is unknown in the Kafka cluster.
 *               (RD_KAFKA_RESP_ERR__UNKNOWN_PARTITION)
 *  - ENOENT   - topic is unknown in the Kafka cluster.
 *               (RD_KAFKA_RESP_ERR__UNKNOWN_TOPIC)
 *  - ECANCELED - fatal error has been raised on producer, see
 *                rd_kafka_fatal_error(),
 *               (RD_KAFKA_RESP_ERR__FATAL).
 *  - ENOEXEC  - transactional state forbids producing
 *               (RD_KAFKA_RESP_ERR__STATE)
 *
 * @sa Use rd_kafka_errno2err() to convert `errno` to rdkafka error code.
 */
RD_EXPORT
int rd_kafka_produce(rd_kafka_topic_t *rkt, int32_t partition,
		      int msgflags,
		      void *payload, size_t len,
		      const void *key, size_t keylen,
		      void *msg_opaque);


/**
 * @brief Produce and send a single message to broker.
 *
 * The message is defined by a va-arg list using \c rd_kafka_vtype_t
 * tag tuples which must be terminated with a single \c RD_KAFKA_V_END.
 *
 * @returns \c RD_KAFKA_RESP_ERR_NO_ERROR on success, else an error code as
 *          described in rd_kafka_produce().
 *          \c RD_KAFKA_RESP_ERR__CONFLICT is returned if _V_HEADER and
 *          _V_HEADERS are mixed.
 *
 * @sa rd_kafka_produce, rd_kafka_produceva, RD_KAFKA_V_END
 */
RD_EXPORT
rd_kafka_resp_err_t rd_kafka_producev (rd_kafka_t *rk, ...);


/**
 * @brief Produce and send a single message to broker.
 *
 * The message is defined by an array of \c rd_kafka_vu_t of
 * count \p cnt.
 *
 * @returns an error object on failure or NULL on success.
 *          See rd_kafka_producev() for specific error codes.
 *
 * @sa rd_kafka_produce, rd_kafka_producev, RD_KAFKA_V_END
 */
RD_EXPORT
rd_kafka_error_t *rd_kafka_produceva (rd_kafka_t *rk,
                                      const rd_kafka_vu_t *vus,
                                      size_t cnt);


/**
 * @brief Produce multiple messages.
 *
 * If partition is RD_KAFKA_PARTITION_UA the configured partitioner will
 * be run for each message (slower), otherwise the messages will be enqueued
 * to the specified partition directly (faster).
 *
 * The messages are provided in the array \p rkmessages of count \p message_cnt
 * elements.
 * The \p partition and \p msgflags are used for all provided messages.
 *
 * Honoured \p rkmessages[] fields are:
 *  - payload,len    Message payload and length
 *  - key,key_len    Optional message key
 *  - _private       Message opaque pointer (msg_opaque)
 *  - err            Will be set according to success or failure, see
 *                   rd_kafka_produce() for possible error codes.
 *                   Application only needs to check for errors if
 *                   return value != \p message_cnt.
 *
 * @remark If \c RD_KAFKA_MSG_F_PARTITION is set in \p msgflags, the
 *         \c .partition field of the \p rkmessages is used instead of
 *         \p partition.
 *
 * @returns the number of messages succesfully enqueued for producing.
 *
 * @remark This interface does NOT support setting message headers on
 *         the provided \p rkmessages.
 */
RD_EXPORT
int rd_kafka_produce_batch(rd_kafka_topic_t *rkt, int32_t partition,
                            int msgflags,
                            rd_kafka_message_t *rkmessages, int message_cnt);




/**
 * @brief Wait until all outstanding produce requests, et.al, are completed.
 *        This should typically be done prior to destroying a producer instance
 *        to make sure all queued and in-flight produce requests are completed
 *        before terminating.
 *
 * @remark This function will call rd_kafka_poll() and thus trigger callbacks.
 *
 * @remark The \c linger.ms time will be ignored for the duration of the call,
 *         queued messages will be sent to the broker as soon as possible.
 *
 * @remark If RD_KAFKA_EVENT_DR has been enabled
 *         (through rd_kafka_conf_set_events()) this function will not call
 *         rd_kafka_poll() but instead wait for the librdkafka-handled
 *         message count to reach zero. This requires the application to
 *         serve the event queue in a separate thread.
 *         In this mode only messages are counted, not other types of
 *         queued events.
 *
 * @returns RD_KAFKA_RESP_ERR__TIMED_OUT if \p timeout_ms was reached before all
 *          outstanding requests were completed, else RD_KAFKA_RESP_ERR_NO_ERROR
 *
 * @sa rd_kafka_outq_len()
 */
RD_EXPORT
rd_kafka_resp_err_t rd_kafka_flush (rd_kafka_t *rk, int timeout_ms);



/**
 * @brief Purge messages currently handled by the producer instance.
 *
 * @param rk          Client instance.
 * @param purge_flags Tells which messages to purge and how.
 *
 * The application will need to call rd_kafka_poll() or rd_kafka_flush()
 * afterwards to serve the delivery report callbacks of the purged messages.
 *
 * Messages purged from internal queues fail with the delivery report
 * error code set to RD_KAFKA_RESP_ERR__PURGE_QUEUE, while purged messages that
 * are in-flight to or from the broker will fail with the error code set to
 * RD_KAFKA_RESP_ERR__PURGE_INFLIGHT.
 *
 * @warning Purging messages that are in-flight to or from the broker
 *          will ignore any sub-sequent acknowledgement for these messages
 *          received from the broker, effectively making it impossible
 *          for the application to know if the messages were successfully
 *          produced or not. This may result in duplicate messages if the
 *          application retries these messages at a later time.
 *
 * @remark This call may block for a short time while background thread
 *         queues are purged.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success,
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if the \p purge flags are invalid
 *          or unknown,
 *          RD_KAFKA_RESP_ERR__NOT_IMPLEMENTED if called on a non-producer
 *          client instance.
 */
RD_EXPORT
rd_kafka_resp_err_t rd_kafka_purge (rd_kafka_t *rk, int purge_flags);


/**
 * @brief Flags for rd_kafka_purge()
 */

/*!
 * Purge messages in internal queues.
 */
#define RD_KAFKA_PURGE_F_QUEUE 0x1

/*!
 * Purge messages in-flight to or from the broker.
 * Purging these messages will void any future acknowledgements from the
 * broker, making it impossible for the application to know if these
 * messages were successfully delivered or not.
 * Retrying these messages may lead to duplicates.
 */
#define RD_KAFKA_PURGE_F_INFLIGHT 0x2


/*!
 * Don't wait for background thread queue purging to finish.
 */
#define RD_KAFKA_PURGE_F_NON_BLOCKING 0x4


/**@}*/


/**
* @name Metadata API
* @{
*
*
*/


/**
 * @brief Broker information
 */
typedef struct rd_kafka_metadata_broker {
        int32_t     id;             /**< Broker Id */
        char       *host;           /**< Broker hostname */
        int         port;           /**< Broker listening port */
} rd_kafka_metadata_broker_t;

/**
 * @brief Partition information
 */
typedef struct rd_kafka_metadata_partition {
        int32_t     id;             /**< Partition Id */
        rd_kafka_resp_err_t err;    /**< Partition error reported by broker */
        int32_t     leader;         /**< Leader broker */
        int         replica_cnt;    /**< Number of brokers in \p replicas */
        int32_t    *replicas;       /**< Replica brokers */
        int         isr_cnt;        /**< Number of ISR brokers in \p isrs */
        int32_t    *isrs;           /**< In-Sync-Replica brokers */
} rd_kafka_metadata_partition_t;

/**
 * @brief Topic information
 */
typedef struct rd_kafka_metadata_topic {
        char       *topic;          /**< Topic name */
        int         partition_cnt;  /**< Number of partitions in \p partitions*/
        struct rd_kafka_metadata_partition *partitions; /**< Partitions */
        rd_kafka_resp_err_t err;    /**< Topic error reported by broker */
} rd_kafka_metadata_topic_t;


/**
 * @brief Metadata container
 */
typedef struct rd_kafka_metadata {
        int         broker_cnt;     /**< Number of brokers in \p brokers */
        struct rd_kafka_metadata_broker *brokers;  /**< Brokers */

        int         topic_cnt;      /**< Number of topics in \p topics */
        struct rd_kafka_metadata_topic *topics;    /**< Topics */

        int32_t     orig_broker_id;   /**< Broker originating this metadata */
        char       *orig_broker_name; /**< Name of originating broker */
} rd_kafka_metadata_t;


/**
 * @brief Request Metadata from broker.
 *
 * Parameters:
 *  - \p all_topics  if non-zero: request info about all topics in cluster,
 *                   if zero: only request info about locally known topics.
 *  - \p only_rkt    only request info about this topic
 *  - \p metadatap   pointer to hold metadata result.
 *                   The \p *metadatap pointer must be released
 *                   with rd_kafka_metadata_destroy().
 *  - \p timeout_ms  maximum response time before failing.
 *
 * @remark Consumer: If \p all_topics is non-zero the Metadata response
 *         information may trigger a re-join if any subscribed topics
 *         have changed partition count or existence state.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success (in which case *metadatap)
 *          will be set, else RD_KAFKA_RESP_ERR__TIMED_OUT on timeout or
 *          other error code on error.
 */
RD_EXPORT
rd_kafka_resp_err_t
rd_kafka_metadata (rd_kafka_t *rk, int all_topics,
                   rd_kafka_topic_t *only_rkt,
                   const struct rd_kafka_metadata **metadatap,
                   int timeout_ms);

/**
 * @brief Release metadata memory.
 */
RD_EXPORT
void rd_kafka_metadata_destroy(const struct rd_kafka_metadata *metadata);


/**@}*/



/**
* @name Client group information
* @{
*
*
*/


/**
 * @brief Group member information
 *
 * For more information on \p member_metadata format, see
 * https://cwiki.apache.org/confluence/display/KAFKA/A+Guide+To+The+Kafka+Protocol#AGuideToTheKafkaProtocol-GroupMembershipAPI
 *
 */
struct rd_kafka_group_member_info {
        char *member_id;            /**< Member id (generated by broker) */
        char *client_id;            /**< Client's \p client.id */
        char *client_host;          /**< Client's hostname */
        void *member_metadata;      /**< Member metadata (binary),
                                     *   format depends on \p protocol_type. */
        int   member_metadata_size; /**< Member metadata size in bytes */
        void *member_assignment;    /**< Member assignment (binary),
                                     *    format depends on \p protocol_type. */
        int   member_assignment_size; /**< Member assignment size in bytes */
};

/**
 * @brief Group information
 */
struct rd_kafka_group_info {
        struct rd_kafka_metadata_broker broker; /**< Originating broker info */
        char *group;                            /**< Group name */
        rd_kafka_resp_err_t err;                /**< Broker-originated error */
        char *state;                            /**< Group state */
        char *protocol_type;                    /**< Group protocol type */
        char *protocol;                         /**< Group protocol */
        struct rd_kafka_group_member_info *members; /**< Group members */
        int   member_cnt;                       /**< Group member count */
};

/**
 * @brief List of groups
 *
 * @sa rd_kafka_group_list_destroy() to release list memory.
 */
struct rd_kafka_group_list {
        struct rd_kafka_group_info *groups;   /**< Groups */
        int    group_cnt;                     /**< Group count */
};


/**
 * @brief List and describe client groups in cluster.
 *
 * \p group is an optional group name to describe, otherwise (\p NULL) all
 * groups are returned.
 *
 * \p timeout_ms is the (approximate) maximum time to wait for response
 * from brokers and must be a positive value.
 *
 * @returns \c RD_KAFKA_RESP_ERR__NO_ERROR on success and \p grplistp is
 *           updated to point to a newly allocated list of groups.
 *           \c RD_KAFKA_RESP_ERR__PARTIAL if not all brokers responded
 *           in time but at least one group is returned in  \p grplistlp.
 *           \c RD_KAFKA_RESP_ERR__TIMED_OUT if no groups were returned in the
 *           given timeframe but not all brokers have yet responded, or
 *           if the list of brokers in the cluster could not be obtained within
 *           the given timeframe.
 *           \c RD_KAFKA_RESP_ERR__TRANSPORT if no brokers were found.
 *           Other error codes may also be returned from the request layer.
 *
 *           The \p grplistp remains untouched if any error code is returned,
 *           with the exception of RD_KAFKA_RESP_ERR__PARTIAL which behaves
 *           as RD_KAFKA_RESP_ERR__NO_ERROR (success) but with an incomplete
 *           group list.
 *
 * @sa Use rd_kafka_group_list_destroy() to release list memory.
 */
RD_EXPORT
rd_kafka_resp_err_t
rd_kafka_list_groups (rd_kafka_t *rk, const char *group,
                      const struct rd_kafka_group_list **grplistp,
                      int timeout_ms);

/**
 * @brief Release list memory
 */
RD_EXPORT
void rd_kafka_group_list_destroy (const struct rd_kafka_group_list *grplist);


/**@}*/



/**
 * @name Miscellaneous APIs
 * @{
 *
 */


/**
 * @brief Adds one or more brokers to the kafka handle's list of initial
 *        bootstrap brokers.
 *
 * Additional brokers will be discovered automatically as soon as rdkafka
 * connects to a broker by querying the broker metadata.
 *
 * If a broker name resolves to multiple addresses (and possibly
 * address families) all will be used for connection attempts in
 * round-robin fashion.
 *
 * \p brokerlist is a ,-separated list of brokers in the format:
 *   \c \<broker1\>,\<broker2\>,..
 * Where each broker is in either the host or URL based format:
 *   \c \<host\>[:\<port\>]
 *   \c \<proto\>://\<host\>[:port]
 * \c \<proto\> is either \c PLAINTEXT, \c SSL, \c SASL, \c SASL_PLAINTEXT
 * The two formats can be mixed but ultimately the value of the
 * `security.protocol` config property decides what brokers are allowed.
 *
 * Example:
 *    brokerlist = "broker1:10000,broker2"
 *    brokerlist = "SSL://broker3:9000,ssl://broker2"
 *
 * @returns the number of brokers successfully added.
 *
 * @remark Brokers may also be defined with the \c metadata.broker.list or
 *         \c bootstrap.servers configuration property (preferred method).
 *
 * @deprecated Set bootstrap servers with the \c bootstrap.servers
 *             configuration property.
 */
RD_EXPORT
int rd_kafka_brokers_add(rd_kafka_t *rk, const char *brokerlist);




/**
 * @brief Set logger function.
 *
 * The default is to print to stderr, but a syslog logger is also available,
 * see rd_kafka_log_(print|syslog) for the builtin alternatives.
 * Alternatively the application may provide its own logger callback.
 * Or pass 'func' as NULL to disable logging.
 *
 * @deprecated Use rd_kafka_conf_set_log_cb()
 *
 * @remark \p rk may be passed as NULL in the callback.
 */
RD_EXPORT RD_DEPRECATED
void rd_kafka_set_logger(rd_kafka_t *rk,
			  void (*func) (const rd_kafka_t *rk, int level,
					const char *fac, const char *buf));


/**
 * @brief Specifies the maximum logging level emitted by
 *        internal kafka logging and debugging.
 *
 * @deprecated Set the \c "log_level" configuration property instead.
 *
 * @remark If the \p \"debug\" configuration property is set the log level is
 *         automatically adjusted to \c LOG_DEBUG (7).
 */
RD_EXPORT
void rd_kafka_set_log_level(rd_kafka_t *rk, int level);


/**
 * @brief Builtin (default) log sink: print to stderr
 */
RD_EXPORT
void rd_kafka_log_print(const rd_kafka_t *rk, int level,
			 const char *fac, const char *buf);


/**
 * @brief Builtin log sink: print to syslog.
 * @remark This logger is only available if librdkafka was built
 *         with syslog support.
 */
RD_EXPORT
void rd_kafka_log_syslog(const rd_kafka_t *rk, int level,
			  const char *fac, const char *buf);


/**
 * @brief Returns the current out queue length.
 *
 * The out queue length is the sum of:
 *  - number of messages waiting to be sent to, or acknowledged by,
 *    the broker.
 *  - number of delivery reports (e.g., dr_msg_cb) waiting to be served
 *    by rd_kafka_poll() or rd_kafka_flush().
 *  - number of callbacks (e.g., error_cb, stats_cb, etc) waiting to be
 *    served by rd_kafka_poll(), rd_kafka_consumer_poll() or rd_kafka_flush().
 *  - number of events waiting to be served by background_event_cb() in
 *    the background queue (see rd_kafka_conf_set_background_event_cb).
 *
 * An application should wait for the return value of this function to reach
 * zero before terminating to make sure outstanding messages,
 * requests (such as offset commits), callbacks and events are fully processed.
 * See rd_kafka_flush().
 *
 * @returns number of messages and events waiting in queues.
 *
 * @sa rd_kafka_flush()
 */
RD_EXPORT
int         rd_kafka_outq_len(rd_kafka_t *rk);



/**
 * @brief Dumps rdkafka's internal state for handle \p rk to stream \p fp
 *
 * This is only useful for debugging rdkafka, showing state and statistics
 * for brokers, topics, partitions, etc.
 */
RD_EXPORT
void rd_kafka_dump(FILE *fp, rd_kafka_t *rk);



/**
 * @brief Retrieve the current number of threads in use by librdkafka.
 *
 * Used by regression tests.
 */
RD_EXPORT
int rd_kafka_thread_cnt(void);


/**
 * @enum rd_kafka_thread_type_t
 *
 * @brief librdkafka internal thread type.
 *
 * @sa rd_kafka_interceptor_add_on_thread_start()
 */
typedef enum rd_kafka_thread_type_t {
        RD_KAFKA_THREAD_MAIN,       /**< librdkafka's internal main thread */
        RD_KAFKA_THREAD_BACKGROUND, /**< Background thread (if enabled) */
        RD_KAFKA_THREAD_BROKER      /**< Per-broker thread */
} rd_kafka_thread_type_t;


/**
 * @brief Wait for all rd_kafka_t objects to be destroyed.
 *
 * Returns 0 if all kafka objects are now destroyed, or -1 if the
 * timeout was reached.
 *
 * @remark This function is deprecated.
 */
RD_EXPORT
int rd_kafka_wait_destroyed(int timeout_ms);


/**
 * @brief Run librdkafka's built-in unit-tests.
 *
 * @returns the number of failures, or 0 if all tests passed.
 */
RD_EXPORT
int rd_kafka_unittest (void);


/**@}*/




/**
 * @name Experimental APIs
 * @{
 */

/**
 * @brief Redirect the main (rd_kafka_poll()) queue to the KafkaConsumer's
 *        queue (rd_kafka_consumer_poll()).
 *
 * @warning It is not permitted to call rd_kafka_poll() after directing the
 *          main queue with rd_kafka_poll_set_consumer().
 */
RD_EXPORT
rd_kafka_resp_err_t rd_kafka_poll_set_consumer (rd_kafka_t *rk);


/**@}*/

/**
 * @name Event interface
 *
 * @brief The event API provides an alternative pollable non-callback interface
 *        to librdkafka's message and event queues.
 *
 * @{
 */


/**
 * @brief Event types
 */
typedef int rd_kafka_event_type_t;
#define RD_KAFKA_EVENT_NONE          0x0  /**< Unset value */
#define RD_KAFKA_EVENT_DR            0x1  /**< Producer Delivery report batch */
#define RD_KAFKA_EVENT_FETCH         0x2  /**< Fetched message (consumer) */
#define RD_KAFKA_EVENT_LOG           0x4  /**< Log message */
#define RD_KAFKA_EVENT_ERROR         0x8  /**< Error */
#define RD_KAFKA_EVENT_REBALANCE     0x10 /**< Group rebalance (consumer) */
#define RD_KAFKA_EVENT_OFFSET_COMMIT 0x20 /**< Offset commit result */
#define RD_KAFKA_EVENT_STATS         0x40 /**< Stats */
#define RD_KAFKA_EVENT_CREATETOPICS_RESULT 100 /**< CreateTopics_result_t */
#define RD_KAFKA_EVENT_DELETETOPICS_RESULT 101 /**< DeleteTopics_result_t */
#define RD_KAFKA_EVENT_CREATEPARTITIONS_RESULT 102 /**< CreatePartitions_result_t */
#define RD_KAFKA_EVENT_ALTERCONFIGS_RESULT 103 /**< AlterConfigs_result_t */
#define RD_KAFKA_EVENT_DESCRIBECONFIGS_RESULT 104 /**< DescribeConfigs_result_t */
#define RD_KAFKA_EVENT_DELETERECORDS_RESULT 105 /**< DeleteRecords_result_t */
#define RD_KAFKA_EVENT_DELETEGROUPS_RESULT 106 /**< DeleteGroups_result_t */
/** DeleteConsumerGroupOffsets_result_t */
#define RD_KAFKA_EVENT_DELETECONSUMERGROUPOFFSETS_RESULT 107
#define RD_KAFKA_EVENT_OAUTHBEARER_TOKEN_REFRESH 0x100 /**< SASL/OAUTHBEARER
                                                             token needs to be
                                                             refreshed */


/**
 * @returns the event type for the given event.
 *
 * @remark As a convenience it is okay to pass \p rkev as NULL in which case
 *         RD_KAFKA_EVENT_NONE is returned.
 */
RD_EXPORT
rd_kafka_event_type_t rd_kafka_event_type (const rd_kafka_event_t *rkev);

/**
 * @returns the event type's name for the given event.
 *
 * @remark As a convenience it is okay to pass \p rkev as NULL in which case
 *         the name for RD_KAFKA_EVENT_NONE is returned.
 */
RD_EXPORT
const char *rd_kafka_event_name (const rd_kafka_event_t *rkev);


/**
 * @brief Destroy an event.
 *
 * @remark Any references to this event, such as extracted messages,
 *         will not be usable after this call.
 *
 * @remark As a convenience it is okay to pass \p rkev as NULL in which case
 *         no action is performed.
 */
RD_EXPORT
void rd_kafka_event_destroy (rd_kafka_event_t *rkev);


/**
 * @returns the next message from an event.
 *
 * Call repeatedly until it returns NULL.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_FETCH  (1 message)
 *  - RD_KAFKA_EVENT_DR     (>=1 message(s))
 *
 * @remark The returned message(s) MUST NOT be
 *         freed with rd_kafka_message_destroy().
 *
 * @remark on_consume() interceptor may be called
 *         from this function prior to passing message to application.
 */
RD_EXPORT
const rd_kafka_message_t *rd_kafka_event_message_next (rd_kafka_event_t *rkev);


/**
 * @brief Extacts \p size message(s) from the event into the
 *        pre-allocated array \p rkmessages.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_FETCH  (1 message)
 *  - RD_KAFKA_EVENT_DR     (>=1 message(s))
 *
 * @returns the number of messages extracted.
 *
 * @remark on_consume() interceptor may be called
 *         from this function prior to passing message to application.
 */
RD_EXPORT
size_t rd_kafka_event_message_array (rd_kafka_event_t *rkev,
				     const rd_kafka_message_t **rkmessages,
				     size_t size);


/**
 * @returns the number of remaining messages in the event.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_FETCH  (1 message)
 *  - RD_KAFKA_EVENT_DR     (>=1 message(s))
 */
RD_EXPORT
size_t rd_kafka_event_message_count (rd_kafka_event_t *rkev);


/**
 * @returns the associated configuration string for the event, or NULL
 *          if the configuration property is not set or if
 *          not applicable for the given event type.
 *
 * The returned memory is read-only and its lifetime is the same as the
 * event object.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_OAUTHBEARER_TOKEN_REFRESH: value of sasl.oauthbearer.config
 */
RD_EXPORT
const char *rd_kafka_event_config_string (rd_kafka_event_t *rkev);


/**
 * @returns the error code for the event.
 *
 * Use rd_kafka_event_error_is_fatal() to detect if this is a fatal error.
 *
 * Event types:
 *  - all
 */
RD_EXPORT
rd_kafka_resp_err_t rd_kafka_event_error (rd_kafka_event_t *rkev);


/**
 * @returns the error string (if any).
 *          An application should check that rd_kafka_event_error() returns
 *          non-zero before calling this function.
 *
 * Event types:
 *  - all
 */
RD_EXPORT
const char *rd_kafka_event_error_string (rd_kafka_event_t *rkev);


/**
 * @returns 1 if the error is a fatal error, else 0.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_ERROR
 *
 * @sa rd_kafka_fatal_error()
 */
RD_EXPORT
int rd_kafka_event_error_is_fatal (rd_kafka_event_t *rkev);


/**
 * @returns the event opaque (if any) as passed to rd_kafka_commit() (et.al) or
 *          rd_kafka_AdminOptions_set_opaque(), depending on event type.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_OFFSET_COMMIT
 *  - RD_KAFKA_EVENT_CREATETOPICS_RESULT
 *  - RD_KAFKA_EVENT_DELETETOPICS_RESULT
 *  - RD_KAFKA_EVENT_CREATEPARTITIONS_RESULT
 *  - RD_KAFKA_EVENT_ALTERCONFIGS_RESULT
 *  - RD_KAFKA_EVENT_DESCRIBECONFIGS_RESULT
 *  - RD_KAFKA_EVENT_DELETEGROUPS_RESULT
 *  - RD_KAFKA_EVENT_DELETECONSUMERGROUPOFFSETS_RESULT
 *  - RD_KAFKA_EVENT_DELETERECORDS_RESULT
 */
RD_EXPORT
void *rd_kafka_event_opaque (rd_kafka_event_t *rkev);


/**
 * @brief Extract log message from the event.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_LOG
 *
 * @returns 0 on success or -1 if unsupported event type.
 */
RD_EXPORT
int rd_kafka_event_log (rd_kafka_event_t *rkev,
			const char **fac, const char **str, int *level);


/**
 * @brief Extract log debug context from event.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_LOG
 *
 *  @param rkev the event to extract data from.
 *  @param dst destination string for comma separated list.
 *  @param dstsize size of provided dst buffer.
 *  @returns 0 on success or -1 if unsupported event type.
 */
RD_EXPORT
int rd_kafka_event_debug_contexts (rd_kafka_event_t *rkev,
            char *dst, size_t dstsize);


/**
 * @brief Extract stats from the event.
 *
 * Event types:
 *  - RD_KAFKA_EVENT_STATS
 *
 * @returns stats json string.
 *
 * @remark the returned string will be freed automatically along with the event object
 *
 */
RD_EXPORT
const char *rd_kafka_event_stats (rd_kafka_event_t *rkev);


/**
 * @returns the topic partition list from the event.
 *
 * @remark The list MUST NOT be freed with rd_kafka_topic_partition_list_destroy()
 *
 * Event types:
 *  - RD_KAFKA_EVENT_REBALANCE
 *  - RD_KAFKA_EVENT_OFFSET_COMMIT
 */
RD_EXPORT rd_kafka_topic_partition_list_t *
rd_kafka_event_topic_partition_list (rd_kafka_event_t *rkev);


/**
 * @returns a newly allocated topic_partition container, if applicable for the event type,
 *          else NULL.
 *
 * @remark The returned pointer MUST be freed with rd_kafka_topic_partition_destroy().
 *
 * Event types:
 *   RD_KAFKA_EVENT_ERROR  (for partition level errors)
 */
RD_EXPORT rd_kafka_topic_partition_t *
rd_kafka_event_topic_partition (rd_kafka_event_t *rkev);


/*! CreateTopics result type */
typedef rd_kafka_event_t rd_kafka_CreateTopics_result_t;
/*! DeleteTopics result type */
typedef rd_kafka_event_t rd_kafka_DeleteTopics_result_t;
/*! CreatePartitions result type */
typedef rd_kafka_event_t rd_kafka_CreatePartitions_result_t;
/*! AlterConfigs result type */
typedef rd_kafka_event_t rd_kafka_AlterConfigs_result_t;
/*! CreateTopics result type */
typedef rd_kafka_event_t rd_kafka_DescribeConfigs_result_t;
/*! DeleteRecords result type */
typedef rd_kafka_event_t rd_kafka_DeleteRecords_result_t;
/*! DeleteGroups result type */
typedef rd_kafka_event_t rd_kafka_DeleteGroups_result_t;
/*! DeleteConsumerGroupOffsets result type */
typedef rd_kafka_event_t rd_kafka_DeleteConsumerGroupOffsets_result_t;

/**
 * @brief Get CreateTopics result.
 *
 * @returns the result of a CreateTopics request, or NULL if event is of
 *          different type.
 *
 * Event types:
 *   RD_KAFKA_EVENT_CREATETOPICS_RESULT
 */
RD_EXPORT const rd_kafka_CreateTopics_result_t *
rd_kafka_event_CreateTopics_result (rd_kafka_event_t *rkev);

/**
 * @brief Get DeleteTopics result.
 *
 * @returns the result of a DeleteTopics request, or NULL if event is of
 *          different type.
 *
 * Event types:
 *   RD_KAFKA_EVENT_DELETETOPICS_RESULT
 */
RD_EXPORT const rd_kafka_DeleteTopics_result_t *
rd_kafka_event_DeleteTopics_result (rd_kafka_event_t *rkev);

/**
 * @brief Get CreatePartitions result.
 *
 * @returns the result of a CreatePartitions request, or NULL if event is of
 *          different type.
 *
 * Event types:
 *   RD_KAFKA_EVENT_CREATEPARTITIONS_RESULT
 */
RD_EXPORT const rd_kafka_CreatePartitions_result_t *
rd_kafka_event_CreatePartitions_result (rd_kafka_event_t *rkev);

/**
 * @brief Get AlterConfigs result.
 *
 * @returns the result of a AlterConfigs request, or NULL if event is of
 *          different type.
 *
 * Event types:
 *   RD_KAFKA_EVENT_ALTERCONFIGS_RESULT
 */
RD_EXPORT const rd_kafka_AlterConfigs_result_t *
rd_kafka_event_AlterConfigs_result (rd_kafka_event_t *rkev);

/**
 * @brief Get DescribeConfigs result.
 *
 * @returns the result of a DescribeConfigs request, or NULL if event is of
 *          different type.
 *
 * Event types:
 *   RD_KAFKA_EVENT_DESCRIBECONFIGS_RESULT
 */
RD_EXPORT const rd_kafka_DescribeConfigs_result_t *
rd_kafka_event_DescribeConfigs_result (rd_kafka_event_t *rkev);

/**
 * @returns the result of a DeleteRecords request, or NULL if event is of
 *          different type.
 *
 * Event types:
 *   RD_KAFKA_EVENT_DELETERECORDS_RESULT
 */
RD_EXPORT const rd_kafka_DeleteRecords_result_t *
rd_kafka_event_DeleteRecords_result (rd_kafka_event_t *rkev);

/**
 * @brief Get DeleteGroups result.
 *
 * @returns the result of a DeleteGroups request, or NULL if event is of
 *          different type.
 *
 * Event types:
 *   RD_KAFKA_EVENT_DELETEGROUPS_RESULT
 */
RD_EXPORT const rd_kafka_DeleteGroups_result_t *
rd_kafka_event_DeleteGroups_result (rd_kafka_event_t *rkev);

/**
 * @brief Get DeleteConsumerGroupOffsets result.
 *
 * @returns the result of a DeleteConsumerGroupOffsets request, or NULL if
 *          event is of different type.
 *
 * Event types:
 *   RD_KAFKA_EVENT_DELETECONSUMERGROUPOFFSETS_RESULT
 */
RD_EXPORT const rd_kafka_DeleteConsumerGroupOffsets_result_t *
rd_kafka_event_DeleteConsumerGroupOffsets_result (rd_kafka_event_t *rkev);

/**
 * @brief Poll a queue for an event for max \p timeout_ms.
 *
 * @returns an event, or NULL.
 *
 * @remark Use rd_kafka_event_destroy() to free the event.
 *
 * @sa rd_kafka_conf_set_background_event_cb()
 */
RD_EXPORT
rd_kafka_event_t *rd_kafka_queue_poll (rd_kafka_queue_t *rkqu, int timeout_ms);

/**
* @brief Poll a queue for events served through callbacks for max \p timeout_ms.
*
* @returns the number of events served.
*
* @remark This API must only be used for queues with callbacks registered
*         for all expected event types. E.g., not a message queue.
*
* @remark Also see rd_kafka_conf_set_background_event_cb() for triggering
*         event callbacks from a librdkafka-managed background thread.
*
* @sa rd_kafka_conf_set_background_event_cb()
*/
RD_EXPORT
int rd_kafka_queue_poll_callback (rd_kafka_queue_t *rkqu, int timeout_ms);


/**@}*/


/**
 * @name Plugin interface
 *
 * @brief A plugin interface that allows external runtime-loaded libraries
 *        to integrate with a client instance without modifications to
 *        the application code.
 *
 *        Plugins are loaded when referenced through the `plugin.library.paths`
 *        configuration property and operates on the \c rd_kafka_conf_t
 *        object prior \c rd_kafka_t instance creation.
 *
 * @warning Plugins require the application to link librdkafka dynamically
 *          and not statically. Failure to do so will lead to missing symbols
 *          or finding symbols in another librdkafka library than the
 *          application was linked with.
 */


/**
 * @brief Plugin's configuration initializer method called each time the
 *        library is referenced from configuration (even if previously loaded by
 *        another client instance).
 *
 * @remark This method MUST be implemented by plugins and have the symbol name
 *         \c conf_init
 *
 * @param conf Configuration set up to this point.
 * @param plug_opaquep Plugin can set this pointer to a per-configuration
 *                     opaque pointer.
 * @param errstr String buffer of size \p errstr_size where plugin must write
 *               a human readable error string in the case the initializer
 *               fails (returns non-zero).
 * @param errstr_size Maximum space (including \0) in \p errstr.
 *
 * @remark A plugin may add an on_conf_destroy() interceptor to clean up
 *         plugin-specific resources created in the plugin's conf_init() method.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an error code on error.
 */
typedef rd_kafka_resp_err_t
(rd_kafka_plugin_f_conf_init_t) (rd_kafka_conf_t *conf,
                                 void **plug_opaquep,
                                 char *errstr, size_t errstr_size);

/**@}*/



/**
 * @name Interceptors
 *
 * @{
 *
 * @brief A callback interface that allows message interception for both
 *        producer and consumer data pipelines.
 *
 * Except for the on_new(), on_conf_set(), on_conf_dup() and on_conf_destroy()
 * interceptors, interceptors are added to the
 * newly created rd_kafka_t client instance. These interceptors MUST only
 * be added from on_new() and MUST NOT be added after rd_kafka_new() returns.
 *
 * The on_new(), on_conf_set(), on_conf_dup() and on_conf_destroy() interceptors
 * are added to the configuration object which is later passed to
 * rd_kafka_new() where on_new() is called to allow addition of
 * other interceptors.
 *
 * Each interceptor reference consists of a display name (ic_name),
 * a callback function, and an application-specified opaque value that is
 * passed as-is to the callback.
 * The ic_name must be unique for the interceptor implementation and is used
 * to reject duplicate interceptor methods.
 *
 * Any number of interceptors can be added and they are called in the order
 * they were added, unless otherwise noted.
 * The list of registered interceptor methods are referred to as
 * interceptor chains.
 *
 * @remark Contrary to the Java client the librdkafka interceptor interface
 *         does not support message key and value modification.
 *         Message mutability is discouraged in the Java client and the
 *         combination of serializers and headers cover most use-cases.
 *
 * @remark Interceptors are NOT copied to the new configuration on
 *         rd_kafka_conf_dup() since it would be hard for interceptors to
 *         track usage of the interceptor's opaque value.
 *         An interceptor should rely on the plugin, which will be copied
 *         in rd_kafka_conf_conf_dup(), to set up the initial interceptors.
 *         An interceptor should implement the on_conf_dup() method
 *         to manually set up its internal configuration on the newly created
 *         configuration object that is being copied-to based on the
 *         interceptor-specific configuration properties.
 *         conf_dup() should thus be treated the same as conf_init().
 *
 * @remark Interceptors are keyed by the interceptor type (on_..()), the
 *         interceptor name (ic_name) and the interceptor method function.
 *         Duplicates are not allowed and the .._add_on_..() method will
 *         return RD_KAFKA_RESP_ERR__CONFLICT if attempting to add a duplicate
 *         method.
 *         The only exception is on_conf_destroy() which may be added multiple
 *         times by the same interceptor to allow proper cleanup of
 *         interceptor configuration state.
 */


/**
 * @brief on_conf_set() is called from rd_kafka_*_conf_set() in the order
 *        the interceptors were added.
 *
 * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
 * @param name The configuration property to set.
 * @param val The configuration value to set, or NULL for reverting to default
 *            in which case the previous value should be freed.
 * @param errstr A human readable error string in case the interceptor fails.
 * @param errstr_size Maximum space (including \0) in \p errstr.
 *
 * @returns RD_KAFKA_CONF_OK if the property was known and successfully
 *          handled by the interceptor, RD_KAFKA_CONF_INVALID if the
 *          property was handled by the interceptor but the value was invalid,
 *          or RD_KAFKA_CONF_UNKNOWN if the interceptor did not handle
 *          this property, in which case the property is passed on on the
 *          interceptor in the chain, finally ending up at the built-in
 *          configuration handler.
 */
typedef rd_kafka_conf_res_t
(rd_kafka_interceptor_f_on_conf_set_t) (rd_kafka_conf_t *conf,
                                        const char *name, const char *val,
                                        char *errstr, size_t errstr_size,
                                        void *ic_opaque);


/**
 * @brief on_conf_dup() is called from rd_kafka_conf_dup() in the
 *        order the interceptors were added and is used to let
 *        an interceptor re-register its conf interecptors with a new
 *        opaque value.
 *        The on_conf_dup() method is called prior to the configuration from
 *        \p old_conf being copied to \p new_conf.
 *
 * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an error code
 *          on failure (which is logged but otherwise ignored).
 *
 * @remark No on_conf_* interceptors are copied to the new configuration
 *         object on rd_kafka_conf_dup().
 */
typedef rd_kafka_resp_err_t
(rd_kafka_interceptor_f_on_conf_dup_t) (rd_kafka_conf_t *new_conf,
                                        const rd_kafka_conf_t *old_conf,
                                        size_t filter_cnt,
                                        const char **filter,
                                        void *ic_opaque);


/**
 * @brief on_conf_destroy() is called from rd_kafka_*_conf_destroy() in the
 *        order the interceptors were added.
 *
 * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
 */
typedef rd_kafka_resp_err_t
(rd_kafka_interceptor_f_on_conf_destroy_t) (void *ic_opaque);


/**
 * @brief on_new() is called from rd_kafka_new() prior toreturning
 *        the newly created client instance to the application.
 *
 * @param rk The client instance.
 * @param conf The client instance's final configuration.
 * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
 * @param errstr A human readable error string in case the interceptor fails.
 * @param errstr_size Maximum space (including \0) in \p errstr.
 *
 * @returns an error code on failure, the error is logged but otherwise ignored.
 *
 * @warning The \p rk client instance will not be fully set up when this
 *          interceptor is called and the interceptor MUST NOT call any
 *          other rk-specific APIs than rd_kafka_interceptor_add..().
 *
 */
typedef rd_kafka_resp_err_t
(rd_kafka_interceptor_f_on_new_t) (rd_kafka_t *rk, const rd_kafka_conf_t *conf,
                                   void *ic_opaque,
                                   char *errstr, size_t errstr_size);


/**
 * @brief on_destroy() is called from rd_kafka_destroy() or (rd_kafka_new()
 *        if rd_kafka_new() fails during initialization).
 *
 * @param rk The client instance.
 * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
 */
typedef rd_kafka_resp_err_t
(rd_kafka_interceptor_f_on_destroy_t) (rd_kafka_t *rk, void *ic_opaque);




/**
 * @brief on_send() is called from rd_kafka_produce*() (et.al) prior to
 *        the partitioner being called.
 *
 * @param rk The client instance.
 * @param rkmessage The message being produced. Immutable.
 * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
 *
 * @remark This interceptor is only used by producer instances.
 *
 * @remark The \p rkmessage object is NOT mutable and MUST NOT be modified
 *         by the interceptor.
 *
 * @remark If the partitioner fails or an unknown partition was specified,
 *         the on_acknowledgement() interceptor chain will be called from
 *         within the rd_kafka_produce*() call to maintain send-acknowledgement
 *         symmetry.
 *
 * @returns an error code on failure, the error is logged but otherwise ignored.
 */
typedef rd_kafka_resp_err_t
(rd_kafka_interceptor_f_on_send_t) (rd_kafka_t *rk,
                                    rd_kafka_message_t *rkmessage,
                                    void *ic_opaque);

/**
 * @brief on_acknowledgement() is called to inform interceptors that a message
 *        was succesfully delivered or permanently failed delivery.
 *        The interceptor chain is called from internal librdkafka background
 *        threads, or rd_kafka_produce*() if the partitioner failed.
 *
 * @param rk The client instance.
 * @param rkmessage The message being produced. Immutable.
 * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
 *
 * @remark This interceptor is only used by producer instances.
 *
 * @remark The \p rkmessage object is NOT mutable and MUST NOT be modified
 *         by the interceptor.
 *
 * @warning The on_acknowledgement() method may be called from internal
 *         librdkafka threads. An on_acknowledgement() interceptor MUST NOT
 *         call any librdkafka API's associated with the \p rk, or perform
 *         any blocking or prolonged work.
 *
 * @returns an error code on failure, the error is logged but otherwise ignored.
 */
typedef rd_kafka_resp_err_t
(rd_kafka_interceptor_f_on_acknowledgement_t) (rd_kafka_t *rk,
                                               rd_kafka_message_t *rkmessage,
                                               void *ic_opaque);


/**
 * @brief on_consume() is called just prior to passing the message to the
 *        application in rd_kafka_consumer_poll(), rd_kafka_consume*(),
 *        the event interface, etc.
 *
 * @param rk The client instance.
 * @param rkmessage The message being consumed. Immutable.
 * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
 *
 * @remark This interceptor is only used by consumer instances.
 *
 * @remark The \p rkmessage object is NOT mutable and MUST NOT be modified
 *         by the interceptor.
 *
 * @returns an error code on failure, the error is logged but otherwise ignored.
 */
typedef rd_kafka_resp_err_t
(rd_kafka_interceptor_f_on_consume_t) (rd_kafka_t *rk,
                                       rd_kafka_message_t *rkmessage,
                                       void *ic_opaque);

/**
 * @brief on_commit() is called on completed or failed offset commit.
 *        It is called from internal librdkafka threads.
 *
 * @param rk The client instance.
 * @param offsets List of topic+partition+offset+error that were committed.
 *                The error message of each partition should be checked for
 *                error.
 * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
 *
 * @remark This interceptor is only used by consumer instances.
 *
 * @warning The on_commit() interceptor is called from internal
 *          librdkafka threads. An on_commit() interceptor MUST NOT
 *          call any librdkafka API's associated with the \p rk, or perform
 *          any blocking or prolonged work.
 *
 *
 * @returns an error code on failure, the error is logged but otherwise ignored.
 */
typedef rd_kafka_resp_err_t
(rd_kafka_interceptor_f_on_commit_t) (
        rd_kafka_t *rk,
        const rd_kafka_topic_partition_list_t *offsets,
        rd_kafka_resp_err_t err, void *ic_opaque);


/**
 * @brief on_request_sent() is called when a request has been fully written
 *        to a broker TCP connections socket.
 *
 * @param rk The client instance.
 * @param sockfd Socket file descriptor.
 * @param brokername Broker request is being sent to.
 * @param brokerid Broker request is being sent to.
 * @param ApiKey Kafka protocol request type.
 * @param ApiVersion Kafka protocol request type version.
 * @param Corrid Kafka protocol request correlation id.
 * @param size Size of request.
 * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
 *
 * @warning The on_request_sent() interceptor is called from internal
 *          librdkafka broker threads. An on_request_sent() interceptor MUST NOT
 *          call any librdkafka API's associated with the \p rk, or perform
 *          any blocking or prolonged work.
 *
 * @returns an error code on failure, the error is logged but otherwise ignored.
 */
typedef rd_kafka_resp_err_t
(rd_kafka_interceptor_f_on_request_sent_t) (
        rd_kafka_t *rk,
        int sockfd,
        const char *brokername,
        int32_t brokerid,
        int16_t ApiKey,
        int16_t ApiVersion,
        int32_t CorrId,
        size_t  size,
        void *ic_opaque);


/**
 * @brief on_response_received() is called when a protocol response has been
 *        fully received from a broker TCP connection socket but before the
 *        response payload is parsed.
 *
 * @param rk The client instance.
 * @param sockfd Socket file descriptor (always -1).
 * @param brokername Broker response was received from, possibly empty string
 *                   on error.
 * @param brokerid Broker response was received from.
 * @param ApiKey Kafka protocol request type or -1 on error.
 * @param ApiVersion Kafka protocol request type version or -1 on error.
 * @param Corrid Kafka protocol request correlation id, possibly -1 on error.
 * @param size Size of response, possibly 0 on error.
 * @param rtt Request round-trip-time in microseconds, possibly -1 on error.
 * @param err Receive error.
 * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
 *
 * @warning The on_response_received() interceptor is called from internal
 *          librdkafka broker threads. An on_response_received() interceptor
 *          MUST NOT call any librdkafka API's associated with the \p rk, or
 *          perform any blocking or prolonged work.
 *
 * @returns an error code on failure, the error is logged but otherwise ignored.
 */
typedef rd_kafka_resp_err_t
(rd_kafka_interceptor_f_on_response_received_t) (
        rd_kafka_t *rk,
        int sockfd,
        const char *brokername,
        int32_t brokerid,
        int16_t ApiKey,
        int16_t ApiVersion,
        int32_t CorrId,
        size_t  size,
        int64_t rtt,
        rd_kafka_resp_err_t err,
        void *ic_opaque);


/**
 * @brief on_thread_start() is called from a newly created librdkafka-managed
 *        thread.

 * @param rk The client instance.
 * @param thread_type Thread type.
 * @param thread_name Human-readable thread name, may not be unique.
 * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
 *
 * @warning The on_thread_start() interceptor is called from internal
 *          librdkafka threads. An on_thread_start() interceptor MUST NOT
 *          call any librdkafka API's associated with the \p rk, or perform
 *          any blocking or prolonged work.
 *
 * @returns an error code on failure, the error is logged but otherwise ignored.
 */
typedef rd_kafka_resp_err_t
(rd_kafka_interceptor_f_on_thread_start_t) (
        rd_kafka_t *rk,
        rd_kafka_thread_type_t thread_type,
        const char *thread_name,
        void *ic_opaque);


/**
 * @brief on_thread_exit() is called just prior to a librdkafka-managed
 *        thread exiting from the exiting thread itself.
 *
 * @param rk The client instance.
 * @param thread_type Thread type.n
 * @param thread_name Human-readable thread name, may not be unique.
 * @param ic_opaque The interceptor's opaque pointer specified in ..add..().
 *
 * @remark Depending on the thread type, librdkafka may execute additional
 *         code on the thread after on_thread_exit() returns.
 *
 * @warning The on_thread_exit() interceptor is called from internal
 *          librdkafka threads. An on_thread_exit() interceptor MUST NOT
 *          call any librdkafka API's associated with the \p rk, or perform
 *          any blocking or prolonged work.
 *
 * @returns an error code on failure, the error is logged but otherwise ignored.
 */
typedef rd_kafka_resp_err_t
(rd_kafka_interceptor_f_on_thread_exit_t) (
        rd_kafka_t *rk,
        rd_kafka_thread_type_t thread_type,
        const char *thread_name,
        void *ic_opaque);



/**
 * @brief Append an on_conf_set() interceptor.
 *
 * @param conf Configuration object.
 * @param ic_name Interceptor name, used in logging.
 * @param on_conf_set Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_conf_interceptor_add_on_conf_set (
        rd_kafka_conf_t *conf, const char *ic_name,
        rd_kafka_interceptor_f_on_conf_set_t *on_conf_set,
        void *ic_opaque);


/**
 * @brief Append an on_conf_dup() interceptor.
 *
 * @param conf Configuration object.
 * @param ic_name Interceptor name, used in logging.
 * @param on_conf_dup Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_conf_interceptor_add_on_conf_dup (
        rd_kafka_conf_t *conf, const char *ic_name,
        rd_kafka_interceptor_f_on_conf_dup_t *on_conf_dup,
        void *ic_opaque);

/**
 * @brief Append an on_conf_destroy() interceptor.
 *
 * @param conf Configuration object.
 * @param ic_name Interceptor name, used in logging.
 * @param on_conf_destroy Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR
 *
 * @remark Multiple on_conf_destroy() interceptors are allowed to be added
 *         to the same configuration object.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_conf_interceptor_add_on_conf_destroy (
        rd_kafka_conf_t *conf, const char *ic_name,
        rd_kafka_interceptor_f_on_conf_destroy_t *on_conf_destroy,
        void *ic_opaque);


/**
 * @brief Append an on_new() interceptor.
 *
 * @param conf Configuration object.
 * @param ic_name Interceptor name, used in logging.
 * @param on_new Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
  *
 * @remark Since the on_new() interceptor is added to the configuration object
 *         it may be copied by rd_kafka_conf_dup().
 *         An interceptor implementation must thus be able to handle
 *         the same interceptor,ic_opaque tuple to be used by multiple
 *         client instances.
 *
 * @remark An interceptor plugin should check the return value to make sure it
 *         has not already been added.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_conf_interceptor_add_on_new (
        rd_kafka_conf_t *conf, const char *ic_name,
        rd_kafka_interceptor_f_on_new_t *on_new,
        void *ic_opaque);



/**
 * @brief Append an on_destroy() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_destroy Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_interceptor_add_on_destroy (
        rd_kafka_t *rk, const char *ic_name,
        rd_kafka_interceptor_f_on_destroy_t *on_destroy,
        void *ic_opaque);


/**
 * @brief Append an on_send() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_send Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_interceptor_add_on_send (
        rd_kafka_t *rk, const char *ic_name,
        rd_kafka_interceptor_f_on_send_t *on_send,
        void *ic_opaque);

/**
 * @brief Append an on_acknowledgement() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_acknowledgement Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_interceptor_add_on_acknowledgement (
        rd_kafka_t *rk, const char *ic_name,
        rd_kafka_interceptor_f_on_acknowledgement_t *on_acknowledgement,
        void *ic_opaque);


/**
 * @brief Append an on_consume() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_consume Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_interceptor_add_on_consume (
        rd_kafka_t *rk, const char *ic_name,
        rd_kafka_interceptor_f_on_consume_t *on_consume,
        void *ic_opaque);


/**
 * @brief Append an on_commit() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_commit() Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_interceptor_add_on_commit (
        rd_kafka_t *rk, const char *ic_name,
        rd_kafka_interceptor_f_on_commit_t *on_commit,
        void *ic_opaque);


/**
 * @brief Append an on_request_sent() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_request_sent() Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_interceptor_add_on_request_sent (
        rd_kafka_t *rk, const char *ic_name,
        rd_kafka_interceptor_f_on_request_sent_t *on_request_sent,
        void *ic_opaque);


/**
 * @brief Append an on_response_received() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_response_received() Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_interceptor_add_on_response_received (
        rd_kafka_t *rk, const char *ic_name,
        rd_kafka_interceptor_f_on_response_received_t *on_response_received,
        void *ic_opaque);


/**
 * @brief Append an on_thread_start() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_thread_start() Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_interceptor_add_on_thread_start (
        rd_kafka_t *rk, const char *ic_name,
        rd_kafka_interceptor_f_on_thread_start_t *on_thread_start,
        void *ic_opaque);


/**
 * @brief Append an on_thread_exit() interceptor.
 *
 * @param rk Client instance.
 * @param ic_name Interceptor name, used in logging.
 * @param on_thread_exit() Function pointer.
 * @param ic_opaque Opaque value that will be passed to the function.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or RD_KAFKA_RESP_ERR__CONFLICT
 *          if an existing intercepted with the same \p ic_name and function
 *          has already been added to \p conf.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_interceptor_add_on_thread_exit (
        rd_kafka_t *rk, const char *ic_name,
        rd_kafka_interceptor_f_on_thread_exit_t *on_thread_exit,
        void *ic_opaque);



/**@}*/



/**
 * @name Auxiliary types
 *
 * @{
 */



/**
 * @brief Topic result provides per-topic operation result information.
 *
 */

/**
 * @returns the error code for the given topic result.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_topic_result_error (const rd_kafka_topic_result_t *topicres);

/**
 * @returns the human readable error string for the given topic result,
 *          or NULL if there was no error.
 *
 * @remark lifetime of the returned string is the same as the \p topicres.
 */
RD_EXPORT const char *
rd_kafka_topic_result_error_string (const rd_kafka_topic_result_t *topicres);

/**
 * @returns the name of the topic for the given topic result.
 * @remark lifetime of the returned string is the same as the \p topicres.
 *
 */
RD_EXPORT const char *
rd_kafka_topic_result_name (const rd_kafka_topic_result_t *topicres);

/**
 * @brief Group result provides per-group operation result information.
 *
 */

/**
 * @returns the error for the given group result, or NULL on success.
 * @remark lifetime of the returned error is the same as the \p groupres.
 */
RD_EXPORT const rd_kafka_error_t *
rd_kafka_group_result_error (const rd_kafka_group_result_t *groupres);

/**
 * @returns the name of the group for the given group result.
 * @remark lifetime of the returned string is the same as the \p groupres.
 *
 */
RD_EXPORT const char *
rd_kafka_group_result_name (const rd_kafka_group_result_t *groupres);

/**
 * @returns the partitions/offsets for the given group result, if applicable
 *          to the request type, else NULL.
 * @remark lifetime of the returned list is the same as the \p groupres.
 */
RD_EXPORT const rd_kafka_topic_partition_list_t *
rd_kafka_group_result_partitions (const rd_kafka_group_result_t *groupres);


/**@}*/


/**
 * @name Admin API
 * @{
 *
 * @brief The Admin API enables applications to perform administrative
 *        Apache Kafka tasks, such as creating and deleting topics,
 *        altering and reading broker configuration, etc.
 *
 * The Admin API is asynchronous and makes use of librdkafka's standard
 * \c rd_kafka_queue_t queues to propagate the result of an admin operation
 * back to the application.
 * The supplied queue may be any queue, such as a temporary single-call queue,
 * a shared queue used for multiple requests, or even the main queue or
 * consumer queues.
 *
 * Use \c rd_kafka_queue_poll() to collect the result of an admin operation
 * from the queue of your choice, then extract the admin API-specific result
 * type by using the corresponding \c rd_kafka_event_CreateTopics_result,
 * \c rd_kafka_event_DescribeConfigs_result, etc, methods.
 * Use the getter methods on the \c .._result_t type to extract response
 * information and finally destroy the result and event by calling
 * \c rd_kafka_event_destroy().
 *
 * Use rd_kafka_event_error() and rd_kafka_event_error_string() to acquire
 * the request-level error/success for an Admin API request.
 * Even if the returned value is \c RD_KAFKA_RESP_ERR_NO_ERROR there
 * may be individual objects (topics, resources, etc) that have failed.
 * Extract per-object error information with the corresponding
 * \c rd_kafka_..._result_topics|resources|..() to check per-object errors.
 *
 * Locally triggered errors:
 *  - \c RD_KAFKA_RESP_ERR__TIMED_OUT - (Controller) broker connection did not
 *    become available in the time allowed by AdminOption_set_request_timeout.
  */


/**
 * @enum rd_kafka_admin_op_t
 *
 * @brief Admin operation enum name for use with rd_kafka_AdminOptions_new()
 *
 * @sa rd_kafka_AdminOptions_new()
 */
typedef enum rd_kafka_admin_op_t {
        RD_KAFKA_ADMIN_OP_ANY = 0,          /**< Default value */
        RD_KAFKA_ADMIN_OP_CREATETOPICS,     /**< CreateTopics */
        RD_KAFKA_ADMIN_OP_DELETETOPICS,     /**< DeleteTopics */
        RD_KAFKA_ADMIN_OP_CREATEPARTITIONS, /**< CreatePartitions */
        RD_KAFKA_ADMIN_OP_ALTERCONFIGS,     /**< AlterConfigs */
        RD_KAFKA_ADMIN_OP_DESCRIBECONFIGS,  /**< DescribeConfigs */
        RD_KAFKA_ADMIN_OP_DELETERECORDS,    /**< DeleteRecords */
        RD_KAFKA_ADMIN_OP_DELETEGROUPS,     /**< DeleteGroups */
        /** DeleteConsumerGroupOffsets */
        RD_KAFKA_ADMIN_OP_DELETECONSUMERGROUPOFFSETS,
        RD_KAFKA_ADMIN_OP__CNT              /**< Number of ops defined */
} rd_kafka_admin_op_t;

/**
 * @brief AdminOptions provides a generic mechanism for setting optional
 *        parameters for the Admin API requests.
 *
 * @remark Since AdminOptions is decoupled from the actual request type
 *         there is no enforcement to prevent setting unrelated properties,
 *         e.g. setting validate_only on a DescribeConfigs request is allowed
 *         but is silently ignored by DescribeConfigs.
 *         Future versions may introduce such enforcement.
 */


typedef struct rd_kafka_AdminOptions_s rd_kafka_AdminOptions_t;

/**
 * @brief Create a new AdminOptions object.
 *
 *        The options object is not modified by the Admin API request APIs,
 *        (e.g. CreateTopics) and may be reused for multiple calls.
 *
 * @param rk Client instance.
 * @param for_api Specifies what Admin API this AdminOptions object will be used
 *                for, which will enforce what AdminOptions_set_..() calls may
 *                be used based on the API, causing unsupported set..() calls
 *                to fail.
 *                Specifying RD_KAFKA_ADMIN_OP_ANY disables the enforcement
 *                allowing any option to be set, even if the option
 *                is not used in a future call to an Admin API method.
 *
 * @returns a new AdminOptions object (which must be freed with
 *          rd_kafka_AdminOptions_destroy()), or NULL if \p for_api was set to
 *          an unknown API op type.
 */
RD_EXPORT rd_kafka_AdminOptions_t *
rd_kafka_AdminOptions_new (rd_kafka_t *rk, rd_kafka_admin_op_t for_api);


/**
 * @brief Destroy a AdminOptions object.
 */
RD_EXPORT void rd_kafka_AdminOptions_destroy (rd_kafka_AdminOptions_t *options);


/**
 * @brief Sets the overall request timeout, including broker lookup,
 *        request transmission, operation time on broker, and response.
 *
 * @param options Admin options.
 * @param timeout_ms Timeout in milliseconds, use -1 for indefinite timeout.
 *                   Defaults to `socket.timeout.ms`.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written in case of error.
 * @param errstr_size Writable size in \p errstr.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success, or
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if timeout was out of range in which
 *          case an error string will be written \p errstr.
 *
 * @remark This option is valid for all Admin API requests.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_AdminOptions_set_request_timeout (rd_kafka_AdminOptions_t *options,
                                           int timeout_ms,
                                           char *errstr, size_t errstr_size);


/**
 * @brief Sets the broker's operation timeout, such as the timeout for
 *        CreateTopics to complete the creation of topics on the controller
 *        before returning a result to the application.
 *
 * CreateTopics: values <= 0 will return immediately after triggering topic
 * creation, while > 0 will wait this long for topic creation to propagate
 * in cluster. Default: 60 seconds.
 *
 * DeleteTopics: same semantics as CreateTopics.
 * CreatePartitions: same semantics as CreateTopics.
 *
 * @param options Admin options.
 * @param timeout_ms Timeout in milliseconds.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written in case of error.
 * @param errstr_size Writable size in \p errstr.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success, or
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if timeout was out of range in which
 *          case an error string will be written \p errstr.
 *
 * @remark This option is valid for CreateTopics, DeleteTopics,
 *         CreatePartitions, and DeleteRecords.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_AdminOptions_set_operation_timeout (rd_kafka_AdminOptions_t *options,
                                             int timeout_ms,
                                             char *errstr, size_t errstr_size);


/**
 * @brief Tell broker to only validate the request, without performing
 *        the requested operation (create topics, etc).
 *
 * @param options Admin options.
 * @param true_or_false Defaults to false.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written in case of error.
 * @param errstr_size Writable size in \p errstr.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an
 *          error code on failure in which case an error string will
 *          be written \p errstr.
 *
 * @remark This option is valid for CreateTopics,
 *         CreatePartitions, AlterConfigs.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_AdminOptions_set_validate_only (rd_kafka_AdminOptions_t *options,
                                        int true_or_false,
                                        char *errstr, size_t errstr_size);


/**
 * @brief Override what broker the Admin request will be sent to.
 *
 * By default, Admin requests are sent to the controller broker, with
 * the following exceptions:
 *   - AlterConfigs with a BROKER resource are sent to the broker id set
 *     as the resource name.
 *   - DescribeConfigs with a BROKER resource are sent to the broker id set
 *     as the resource name.
 *
 * @param options Admin Options.
 * @param broker_id The broker to send the request to.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written in case of error.
 * @param errstr_size Writable size in \p errstr.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success or an
 *          error code on failure in which case an error string will
 *          be written \p errstr.
 *
 * @remark This API should typically not be used, but serves as a workaround
 *         if new resource types are to the broker that the client
 *         does not know where to send.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_AdminOptions_set_broker (rd_kafka_AdminOptions_t *options,
                                  int32_t broker_id,
                                  char *errstr, size_t errstr_size);



/**
 * @brief Set application opaque value that can be extracted from the
 *        result event using rd_kafka_event_opaque()
 */
RD_EXPORT void
rd_kafka_AdminOptions_set_opaque (rd_kafka_AdminOptions_t *options,
                                  void *ev_opaque);






/*
 * CreateTopics - create topics in cluster.
 *
 */


/*! Defines a new topic to be created. */
typedef struct rd_kafka_NewTopic_s rd_kafka_NewTopic_t;

/**
 * @brief Create a new NewTopic object. This object is later passed to
 *        rd_kafka_CreateTopics().
 *
 * @param topic Topic name to create.
 * @param num_partitions Number of partitions in topic, or -1 to use the
 *                       broker's default partition count (>= 2.4.0).
 * @param replication_factor Default replication factor for the topic's
 *                           partitions, or -1 to use the broker's default
 *                           replication factor (>= 2.4.0) or if
 *                           set_replica_assignment() will be used.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written in case of error.
 * @param errstr_size Writable size in \p errstr.
 *
 *
 * @returns a new allocated NewTopic object, or NULL if the input parameters
 *          are invalid.
 *          Use rd_kafka_NewTopic_destroy() to free object when done.
 */
RD_EXPORT rd_kafka_NewTopic_t *
rd_kafka_NewTopic_new (const char *topic, int num_partitions,
                       int replication_factor,
                       char *errstr, size_t errstr_size);

/**
 * @brief Destroy and free a NewTopic object previously created with
 *        rd_kafka_NewTopic_new()
 */
RD_EXPORT void
rd_kafka_NewTopic_destroy (rd_kafka_NewTopic_t *new_topic);


/**
 * @brief Helper function to destroy all NewTopic objects in the \p new_topics
 *        array (of \p new_topic_cnt elements).
 *        The array itself is not freed.
 */
RD_EXPORT void
rd_kafka_NewTopic_destroy_array (rd_kafka_NewTopic_t **new_topics,
                                 size_t new_topic_cnt);


/**
 * @brief Set the replica (broker) assignment for \p partition to the
 *        replica set in \p broker_ids (of \p broker_id_cnt elements).
 *
 * @remark When this method is used, rd_kafka_NewTopic_new() must have
 *         been called with a \c replication_factor of -1.
 *
 * @remark An application must either set the replica assignment for
 *         all new partitions, or none.
 *
 * @remark If called, this function must be called consecutively for each
 *         partition, starting at 0.
 *
 * @remark Use rd_kafka_metadata() to retrieve the list of brokers
 *         in the cluster.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success, or an error code
 *          if the arguments were invalid.
 *
 * @sa rd_kafka_AdminOptions_set_validate_only()
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_NewTopic_set_replica_assignment (rd_kafka_NewTopic_t *new_topic,
                                          int32_t partition,
                                          int32_t *broker_ids,
                                          size_t broker_id_cnt,
                                          char *errstr, size_t errstr_size);

/**
 * @brief Set (broker-side) topic configuration name/value pair.
 *
 * @remark The name and value are not validated by the client, the validation
 *         takes place on the broker.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success, or an error code
 *          if the arguments were invalid.
 *
 * @sa rd_kafka_AdminOptions_set_validate_only()
 * @sa http://kafka.apache.org/documentation.html#topicconfigs
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_NewTopic_set_config (rd_kafka_NewTopic_t *new_topic,
                              const char *name, const char *value);


/**
 * @brief Create topics in cluster as specified by the \p new_topics
 *        array of size \p new_topic_cnt elements.
 *
 * @param rk Client instance.
 * @param new_topics Array of new topics to create.
 * @param new_topic_cnt Number of elements in \p new_topics array.
 * @param options Optional admin options, or NULL for defaults.
 * @param rkqu Queue to emit result on.
 *
 * Supported admin options:
 *  - rd_kafka_AdminOptions_set_validate_only() - default false
 *  - rd_kafka_AdminOptions_set_operation_timeout() - default 60 seconds
 *  - rd_kafka_AdminOptions_set_request_timeout() - default socket.timeout.ms
 *
 * @remark The result event type emitted on the supplied queue is of type
 *         \c RD_KAFKA_EVENT_CREATETOPICS_RESULT
 */
RD_EXPORT void
rd_kafka_CreateTopics (rd_kafka_t *rk,
                       rd_kafka_NewTopic_t **new_topics,
                       size_t new_topic_cnt,
                       const rd_kafka_AdminOptions_t *options,
                       rd_kafka_queue_t *rkqu);


/*
 * CreateTopics result type and methods
 */

/**
 * @brief Get an array of topic results from a CreateTopics result.
 *
 * The returned \p topics life-time is the same as the \p result object.
 *
 * @param result Result to get topics from.
 * @param cntp Updated to the number of elements in the array.
 */
RD_EXPORT const rd_kafka_topic_result_t **
rd_kafka_CreateTopics_result_topics (
        const rd_kafka_CreateTopics_result_t *result,
        size_t *cntp);





/*
 * DeleteTopics - delete topics from cluster
 *
 */

/*! Represents a topic to be deleted. */
typedef struct rd_kafka_DeleteTopic_s rd_kafka_DeleteTopic_t;

/**
 * @brief Create a new DeleteTopic object. This object is later passed to
 *        rd_kafka_DeleteTopics().
 *
 * @param topic Topic name to delete.
 *
 * @returns a new allocated DeleteTopic object.
 *          Use rd_kafka_DeleteTopic_destroy() to free object when done.
 */
RD_EXPORT rd_kafka_DeleteTopic_t *
rd_kafka_DeleteTopic_new (const char *topic);

/**
 * @brief Destroy and free a DeleteTopic object previously created with
 *        rd_kafka_DeleteTopic_new()
 */
RD_EXPORT void
rd_kafka_DeleteTopic_destroy (rd_kafka_DeleteTopic_t *del_topic);

/**
 * @brief Helper function to destroy all DeleteTopic objects in
 *        the \p del_topics array (of \p del_topic_cnt elements).
 *        The array itself is not freed.
 */
RD_EXPORT void
rd_kafka_DeleteTopic_destroy_array (rd_kafka_DeleteTopic_t **del_topics,
                                    size_t del_topic_cnt);

/**
 * @brief Delete topics from cluster as specified by the \p topics
 *        array of size \p topic_cnt elements.
 *
 * @param rk Client instance.
 * @param del_topics Array of topics to delete.
 * @param del_topic_cnt Number of elements in \p topics array.
 * @param options Optional admin options, or NULL for defaults.
 * @param rkqu Queue to emit result on.
 *
 * @remark The result event type emitted on the supplied queue is of type
 *         \c RD_KAFKA_EVENT_DELETETOPICS_RESULT
 */
RD_EXPORT
void rd_kafka_DeleteTopics (rd_kafka_t *rk,
                            rd_kafka_DeleteTopic_t **del_topics,
                            size_t del_topic_cnt,
                            const rd_kafka_AdminOptions_t *options,
                            rd_kafka_queue_t *rkqu);



/*
 * DeleteTopics result type and methods
 */

/**
 * @brief Get an array of topic results from a DeleteTopics result.
 *
 * The returned \p topics life-time is the same as the \p result object.
 *
 * @param result Result to get topic results from.
 * @param cntp is updated to the number of elements in the array.
 */
RD_EXPORT const rd_kafka_topic_result_t **
rd_kafka_DeleteTopics_result_topics (
        const rd_kafka_DeleteTopics_result_t *result,
        size_t *cntp);






/*
 * CreatePartitions - add partitions to topic.
 *
 */

/*! Defines a new partition to be created. */
typedef struct rd_kafka_NewPartitions_s rd_kafka_NewPartitions_t;

/**
 * @brief Create a new NewPartitions. This object is later passed to
 *        rd_kafka_CreatePartitions() to increase the number of partitions
 *        to \p new_total_cnt for an existing topic.
 *
 * @param topic Topic name to create more partitions for.
 * @param new_total_cnt Increase the topic's partition count to this value.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written in case of error.
 * @param errstr_size Writable size in \p errstr.
 *
 * @returns a new allocated NewPartitions object, or NULL if the
 *          input parameters are invalid.
 *          Use rd_kafka_NewPartitions_destroy() to free object when done.
 */
RD_EXPORT rd_kafka_NewPartitions_t *
rd_kafka_NewPartitions_new (const char *topic, size_t new_total_cnt,
                            char *errstr, size_t errstr_size);

/**
 * @brief Destroy and free a NewPartitions object previously created with
 *        rd_kafka_NewPartitions_new()
 */
RD_EXPORT void
rd_kafka_NewPartitions_destroy (rd_kafka_NewPartitions_t *new_parts);

/**
 * @brief Helper function to destroy all NewPartitions objects in the
 *        \p new_parts array (of \p new_parts_cnt elements).
 *        The array itself is not freed.
 */
RD_EXPORT void
rd_kafka_NewPartitions_destroy_array (rd_kafka_NewPartitions_t **new_parts,
                                      size_t new_parts_cnt);

/**
 * @brief Set the replica (broker id) assignment for \p new_partition_idx to the
 *        replica set in \p broker_ids (of \p broker_id_cnt elements).
 *
 * @remark An application must either set the replica assignment for
 *         all new partitions, or none.
 *
 * @remark If called, this function must be called consecutively for each
 *         new partition being created,
 *         where \p new_partition_idx 0 is the first new partition,
 *         1 is the second, and so on.
 *
 * @remark \p broker_id_cnt should match the topic's replication factor.
 *
 * @remark Use rd_kafka_metadata() to retrieve the list of brokers
 *         in the cluster.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR on success, or an error code
 *          if the arguments were invalid.
 *
 * @sa rd_kafka_AdminOptions_set_validate_only()
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_NewPartitions_set_replica_assignment (rd_kafka_NewPartitions_t *new_parts,
                                               int32_t new_partition_idx,
                                               int32_t *broker_ids,
                                               size_t broker_id_cnt,
                                               char *errstr,
                                               size_t errstr_size);


/**
 * @brief Create additional partitions for the given topics, as specified
 *        by the \p new_parts array of size \p new_parts_cnt elements.
 *
 * @param rk Client instance.
 * @param new_parts Array of topics for which new partitions are to be created.
 * @param new_parts_cnt Number of elements in \p new_parts array.
 * @param options Optional admin options, or NULL for defaults.
 * @param rkqu Queue to emit result on.
 *
 * Supported admin options:
 *  - rd_kafka_AdminOptions_set_validate_only() - default false
 *  - rd_kafka_AdminOptions_set_operation_timeout() - default 60 seconds
 *  - rd_kafka_AdminOptions_set_request_timeout() - default socket.timeout.ms
 *
 * @remark The result event type emitted on the supplied queue is of type
 *         \c RD_KAFKA_EVENT_CREATEPARTITIONS_RESULT
 */
RD_EXPORT void
rd_kafka_CreatePartitions (rd_kafka_t *rk,
                           rd_kafka_NewPartitions_t **new_parts,
                           size_t new_parts_cnt,
                           const rd_kafka_AdminOptions_t *options,
                           rd_kafka_queue_t *rkqu);



/*
 * CreatePartitions result type and methods
 */

/**
 * @brief Get an array of topic results from a CreatePartitions result.
 *
 * The returned \p topics life-time is the same as the \p result object.
 *
 * @param result Result o get topic results from.
 * @param cntp is updated to the number of elements in the array.
 */
RD_EXPORT const rd_kafka_topic_result_t **
rd_kafka_CreatePartitions_result_topics (
        const rd_kafka_CreatePartitions_result_t *result,
        size_t *cntp);





/*
 * Cluster, broker, topic configuration entries, sources, etc.
 *
 */

/**
 * @enum rd_kafka_ConfigSource_t
 *
 * @brief Apache Kafka config sources.
 *
 * @remark These entities relate to the cluster, not the local client.
 *
 * @sa rd_kafka_conf_set(), et.al. for local client configuration.
 */
typedef enum rd_kafka_ConfigSource_t {
        /** Source unknown, e.g., in the ConfigEntry used for alter requests
         *  where source is not set */
        RD_KAFKA_CONFIG_SOURCE_UNKNOWN_CONFIG = 0,
        /** Dynamic topic config that is configured for a specific topic */
        RD_KAFKA_CONFIG_SOURCE_DYNAMIC_TOPIC_CONFIG = 1,
        /** Dynamic broker config that is configured for a specific broker */
        RD_KAFKA_CONFIG_SOURCE_DYNAMIC_BROKER_CONFIG = 2,
        /** Dynamic broker config that is configured as default for all
         *  brokers in the cluster */
        RD_KAFKA_CONFIG_SOURCE_DYNAMIC_DEFAULT_BROKER_CONFIG = 3,
        /** Static broker config provided as broker properties at startup
         *  (e.g. from server.properties file) */
        RD_KAFKA_CONFIG_SOURCE_STATIC_BROKER_CONFIG = 4,
        /** Built-in default configuration for configs that have a
         *  default value */
        RD_KAFKA_CONFIG_SOURCE_DEFAULT_CONFIG = 5,

        /** Number of source types defined */
        RD_KAFKA_CONFIG_SOURCE__CNT,
} rd_kafka_ConfigSource_t;


/**
 * @returns a string representation of the \p confsource.
 */
RD_EXPORT const char *
rd_kafka_ConfigSource_name (rd_kafka_ConfigSource_t confsource);


/*! Apache Kafka configuration entry. */
typedef struct rd_kafka_ConfigEntry_s rd_kafka_ConfigEntry_t;

/**
 * @returns the configuration property name
 */
RD_EXPORT const char *
rd_kafka_ConfigEntry_name (const rd_kafka_ConfigEntry_t *entry);

/**
 * @returns the configuration value, may be NULL for sensitive or unset
 *          properties.
 */
RD_EXPORT const char *
rd_kafka_ConfigEntry_value (const rd_kafka_ConfigEntry_t *entry);

/**
 * @returns the config source.
 */
RD_EXPORT rd_kafka_ConfigSource_t
rd_kafka_ConfigEntry_source (const rd_kafka_ConfigEntry_t *entry);

/**
 * @returns 1 if the config property is read-only on the broker, else 0.
 * @remark Shall only be used on a DescribeConfigs result, otherwise returns -1.
 */
RD_EXPORT int
rd_kafka_ConfigEntry_is_read_only (const rd_kafka_ConfigEntry_t *entry);

/**
 * @returns 1 if the config property is set to its default value on the broker,
 *          else 0.
 * @remark Shall only be used on a DescribeConfigs result, otherwise returns -1.
 */
RD_EXPORT int
rd_kafka_ConfigEntry_is_default (const rd_kafka_ConfigEntry_t *entry);

/**
 * @returns 1 if the config property contains sensitive information (such as
 *          security configuration), else 0.
 * @remark An application should take care not to include the value of
 *         sensitive configuration entries in its output.
 * @remark Shall only be used on a DescribeConfigs result, otherwise returns -1.
 */
RD_EXPORT int
rd_kafka_ConfigEntry_is_sensitive (const rd_kafka_ConfigEntry_t *entry);

/**
 * @returns 1 if this entry is a synonym, else 0.
 */
RD_EXPORT int
rd_kafka_ConfigEntry_is_synonym (const rd_kafka_ConfigEntry_t *entry);


/**
 * @returns the synonym config entry array.
 *
 * @param entry Entry to get synonyms for.
 * @param cntp is updated to the number of elements in the array.
 *
 * @remark The lifetime of the returned entry is the same as \p conf .
 * @remark Shall only be used on a DescribeConfigs result,
 *         otherwise returns NULL.
 */
RD_EXPORT const rd_kafka_ConfigEntry_t **
rd_kafka_ConfigEntry_synonyms (const rd_kafka_ConfigEntry_t *entry,
                               size_t *cntp);




/*! Apache Kafka resource types */
typedef enum rd_kafka_ResourceType_t {
        RD_KAFKA_RESOURCE_UNKNOWN = 0, /**< Unknown */
        RD_KAFKA_RESOURCE_ANY = 1,     /**< Any (used for lookups) */
        RD_KAFKA_RESOURCE_TOPIC = 2,   /**< Topic */
        RD_KAFKA_RESOURCE_GROUP = 3,   /**< Group */
        RD_KAFKA_RESOURCE_BROKER = 4,  /**< Broker */
        RD_KAFKA_RESOURCE__CNT,        /**< Number of resource types defined */
} rd_kafka_ResourceType_t;

/**
 * @returns a string representation of the \p restype
 */
RD_EXPORT const char *
rd_kafka_ResourceType_name (rd_kafka_ResourceType_t restype);

/*! Apache Kafka configuration resource. */
typedef struct rd_kafka_ConfigResource_s rd_kafka_ConfigResource_t;


/**
 * @brief Create new ConfigResource object.
 *
 * @param restype The resource type (e.g., RD_KAFKA_RESOURCE_TOPIC)
 * @param resname The resource name (e.g., the topic name)
 *
 * @returns a newly allocated object
 */
RD_EXPORT rd_kafka_ConfigResource_t *
rd_kafka_ConfigResource_new (rd_kafka_ResourceType_t restype,
                             const char *resname);

/**
 * @brief Destroy and free a ConfigResource object previously created with
 *        rd_kafka_ConfigResource_new()
 */
RD_EXPORT void
rd_kafka_ConfigResource_destroy (rd_kafka_ConfigResource_t *config);


/**
 * @brief Helper function to destroy all ConfigResource objects in
 *        the \p configs array (of \p config_cnt elements).
 *        The array itself is not freed.
 */
RD_EXPORT void
rd_kafka_ConfigResource_destroy_array (rd_kafka_ConfigResource_t **config,
                                       size_t config_cnt);


/**
 * @brief Set configuration name value pair.
 *
 * @param config ConfigResource to set config property on.
 * @param name Configuration name, depends on resource type.
 * @param value Configuration value, depends on resource type and \p name.
 *              Set to \c NULL to revert configuration value to default.
 *
 * This will overwrite the current value.
 *
 * @returns RD_KAFKA_RESP_ERR_NO_ERROR if config was added to resource,
 *          or RD_KAFKA_RESP_ERR__INVALID_ARG on invalid input.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_ConfigResource_set_config (rd_kafka_ConfigResource_t *config,
                                    const char *name, const char *value);


/**
 * @brief Get an array of config entries from a ConfigResource object.
 *
 * The returned object life-times are the same as the \p config object.
 *
 * @param config ConfigResource to get configs from.
 * @param cntp is updated to the number of elements in the array.
 */
RD_EXPORT const rd_kafka_ConfigEntry_t **
rd_kafka_ConfigResource_configs (const rd_kafka_ConfigResource_t *config,
                                 size_t *cntp);



/**
 * @returns the ResourceType for \p config
 */
RD_EXPORT rd_kafka_ResourceType_t
rd_kafka_ConfigResource_type (const rd_kafka_ConfigResource_t *config);

/**
 * @returns the name for \p config
 */
RD_EXPORT const char *
rd_kafka_ConfigResource_name (const rd_kafka_ConfigResource_t *config);

/**
 * @returns the error for this resource from an AlterConfigs request
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_ConfigResource_error (const rd_kafka_ConfigResource_t *config);

/**
 * @returns the error string for this resource from an AlterConfigs
 *          request, or NULL if no error.
 */
RD_EXPORT const char *
rd_kafka_ConfigResource_error_string (const rd_kafka_ConfigResource_t *config);


/*
 * AlterConfigs - alter cluster configuration.
 *
 */


/**
 * @brief Update the configuration for the specified resources.
 *        Updates are not transactional so they may succeed for a subset
 *        of the provided resources while the others fail.
 *        The configuration for a particular resource is updated atomically,
 *        replacing values using the provided ConfigEntrys and reverting
 *        unspecified ConfigEntrys to their default values.
 *
 * @remark Requires broker version >=0.11.0.0
 *
 * @warning AlterConfigs will replace all existing configuration for
 *          the provided resources with the new configuration given,
 *          reverting all other configuration to their default values.
 *
 * @remark Multiple resources and resource types may be set, but at most one
 *         resource of type \c RD_KAFKA_RESOURCE_BROKER is allowed per call
 *         since these resource requests must be sent to the broker specified
 *         in the resource.
 *
 */
RD_EXPORT
void rd_kafka_AlterConfigs (rd_kafka_t *rk,
                            rd_kafka_ConfigResource_t **configs,
                            size_t config_cnt,
                            const rd_kafka_AdminOptions_t *options,
                            rd_kafka_queue_t *rkqu);


/*
 * AlterConfigs result type and methods
 */

/**
 * @brief Get an array of resource results from a AlterConfigs result.
 *
 * Use \c rd_kafka_ConfigResource_error() and
 * \c rd_kafka_ConfigResource_error_string() to extract per-resource error
 * results on the returned array elements.
 *
 * The returned object life-times are the same as the \p result object.
 *
 * @param result Result object to get resource results from.
 * @param cntp is updated to the number of elements in the array.
 *
 * @returns an array of ConfigResource elements, or NULL if not available.
 */
RD_EXPORT const rd_kafka_ConfigResource_t **
rd_kafka_AlterConfigs_result_resources (
        const rd_kafka_AlterConfigs_result_t *result,
        size_t *cntp);






/*
 * DescribeConfigs - retrieve cluster configuration.
 *
 */


/**
 * @brief Get configuration for the specified resources in \p configs.
 *
 * The returned configuration includes default values and the
 * rd_kafka_ConfigEntry_is_default() or rd_kafka_ConfigEntry_source()
 * methods may be used to distinguish them from user supplied values.
 *
 * The value of config entries where rd_kafka_ConfigEntry_is_sensitive()
 * is true will always be NULL to avoid disclosing sensitive
 * information, such as security settings.
 *
 * Configuration entries where rd_kafka_ConfigEntry_is_read_only()
 * is true can't be updated (with rd_kafka_AlterConfigs()).
 *
 * Synonym configuration entries are returned if the broker supports
 * it (broker version >= 1.1.0). See rd_kafka_ConfigEntry_synonyms().
 *
 * @remark Requires broker version >=0.11.0.0
 *
 * @remark Multiple resources and resource types may be requested, but at most
 *         one resource of type \c RD_KAFKA_RESOURCE_BROKER is allowed per call
 *         since these resource requests must be sent to the broker specified
 *         in the resource.
 */
RD_EXPORT
void rd_kafka_DescribeConfigs (rd_kafka_t *rk,
                               rd_kafka_ConfigResource_t **configs,
                               size_t config_cnt,
                               const rd_kafka_AdminOptions_t *options,
                               rd_kafka_queue_t *rkqu);




/*
 * DescribeConfigs result type and methods
 */

/**
 * @brief Get an array of resource results from a DescribeConfigs result.
 *
 * The returned \p resources life-time is the same as the \p result object.
 *
 * @param result Result object to get resource results from.
 * @param cntp is updated to the number of elements in the array.
 */
RD_EXPORT const rd_kafka_ConfigResource_t **
rd_kafka_DescribeConfigs_result_resources (
        const rd_kafka_DescribeConfigs_result_t *result,
        size_t *cntp);


/*
 * DeleteRecords - delete records (messages) from partitions
 *
 *
 */

/**! Represents records to be deleted */
typedef struct rd_kafka_DeleteRecords_s rd_kafka_DeleteRecords_t;

/**
 * @brief Create a new DeleteRecords object. This object is later passed to
 *        rd_kafka_DeleteRecords().
 *
 * \p before_offsets must contain \c topic, \c partition, and
 * \c offset is the offset before which the messages will
 * be deleted (exclusive).
 * Set \c offset to RD_KAFKA_OFFSET_END (high-watermark) in order to
 * delete all data in the partition.
 *
 * @param before_offsets For each partition delete all messages up to but not
 *                       including the specified offset.
 *
 * @returns a new allocated DeleteRecords object.
 *          Use rd_kafka_DeleteRecords_destroy() to free object when done.
 */
RD_EXPORT rd_kafka_DeleteRecords_t *
rd_kafka_DeleteRecords_new (const rd_kafka_topic_partition_list_t *
                            before_offsets);

/**
 * @brief Destroy and free a DeleteRecords object previously created with
 *        rd_kafka_DeleteRecords_new()
 */
RD_EXPORT void
rd_kafka_DeleteRecords_destroy (rd_kafka_DeleteRecords_t *del_records);

/**
 * @brief Helper function to destroy all DeleteRecords objects in
 *        the \p del_groups array (of \p del_group_cnt elements).
 *        The array itself is not freed.
 */
RD_EXPORT void
rd_kafka_DeleteRecords_destroy_array (rd_kafka_DeleteRecords_t **del_records,
                                      size_t del_record_cnt);

/**
 * @brief Delete records (messages) in topic partitions older than the
 *        offsets provided.
 *
 * @param rk Client instance.
 * @param del_records The offsets to delete (up to).
 *                    Currently only one DeleteRecords_t (but containing
 *                    multiple offsets) is supported.
 * @param del_record_cnt The number of elements in del_records, must be 1.
 * @param options Optional admin options, or NULL for defaults.
 * @param rkqu Queue to emit result on.
 *
 * Supported admin options:
 *  - rd_kafka_AdminOptions_set_operation_timeout() - default 60 seconds.
 *    Controls how long the brokers will wait for records to be deleted.
 *  - rd_kafka_AdminOptions_set_request_timeout() - default socket.timeout.ms.
 *    Controls how long \c rdkafka will wait for the request to complete.
 *
 * @remark The result event type emitted on the supplied queue is of type
 *         \c RD_KAFKA_EVENT_DELETERECORDS_RESULT
 */
RD_EXPORT void
rd_kafka_DeleteRecords (rd_kafka_t *rk,
                        rd_kafka_DeleteRecords_t **del_records,
                        size_t del_record_cnt,
                        const rd_kafka_AdminOptions_t *options,
                        rd_kafka_queue_t *rkqu);


/*
 * DeleteRecords result type and methods
 */

/**
 * @brief Get a list of topic and partition results from a DeleteRecords result.
 *        The returned objects will contain \c topic, \c partition, \c offset
 *        and \c err. \c offset will be set to the post-deletion low-watermark
 *        (smallest available offset of all live replicas). \c err will be set
 *        per-partition if deletion failed.
 *
 * The returned object's life-time is the same as the \p result object.
 */
RD_EXPORT const rd_kafka_topic_partition_list_t *
rd_kafka_DeleteRecords_result_offsets (
    const rd_kafka_DeleteRecords_result_t *result);

/*
 * DeleteGroups - delete groups from cluster
 *
 *
 */

/*! Represents a group to be deleted. */
typedef struct rd_kafka_DeleteGroup_s rd_kafka_DeleteGroup_t;

/**
 * @brief Create a new DeleteGroup object. This object is later passed to
 *        rd_kafka_DeleteGroups().
 *
 * @param group Name of group to delete.
 *
 * @returns a new allocated DeleteGroup object.
 *          Use rd_kafka_DeleteGroup_destroy() to free object when done.
 */
RD_EXPORT rd_kafka_DeleteGroup_t *
rd_kafka_DeleteGroup_new (const char *group);

/**
 * @brief Destroy and free a DeleteGroup object previously created with
 *        rd_kafka_DeleteGroup_new()
 */
RD_EXPORT void
rd_kafka_DeleteGroup_destroy (rd_kafka_DeleteGroup_t *del_group);

/**
 * @brief Helper function to destroy all DeleteGroup objects in
 *        the \p del_groups array (of \p del_group_cnt elements).
 *        The array itself is not freed.
 */
RD_EXPORT void
rd_kafka_DeleteGroup_destroy_array (rd_kafka_DeleteGroup_t **del_groups,
                                    size_t del_group_cnt);

/**
 * @brief Delete groups from cluster as specified by the \p del_groups
 *        array of size \p del_group_cnt elements.
 *
 * @param rk Client instance.
 * @param del_groups Array of groups to delete.
 * @param del_group_cnt Number of elements in \p del_groups array.
 * @param options Optional admin options, or NULL for defaults.
 * @param rkqu Queue to emit result on.
 *
 * @remark The result event type emitted on the supplied queue is of type
 *         \c RD_KAFKA_EVENT_DELETEGROUPS_RESULT
 */
RD_EXPORT
void rd_kafka_DeleteGroups (rd_kafka_t *rk,
                            rd_kafka_DeleteGroup_t **del_groups,
                            size_t del_group_cnt,
                            const rd_kafka_AdminOptions_t *options,
                            rd_kafka_queue_t *rkqu);



/*
 * DeleteGroups result type and methods
 */

/**
 * @brief Get an array of group results from a DeleteGroups result.
 *
 * The returned groups life-time is the same as the \p result object.
 *
 * @param result Result to get group results from.
 * @param cntp is updated to the number of elements in the array.
 */
RD_EXPORT const rd_kafka_group_result_t **
rd_kafka_DeleteGroups_result_groups (
        const rd_kafka_DeleteGroups_result_t *result,
        size_t *cntp);


/*
 * DeleteConsumerGroupOffsets - delete groups from cluster
 *
 *
 */

/*! Represents consumer group committed offsets to be deleted. */
typedef struct rd_kafka_DeleteConsumerGroupOffsets_s
rd_kafka_DeleteConsumerGroupOffsets_t;

/**
 * @brief Create a new DeleteConsumerGroupOffsets object.
 *        This object is later passed to rd_kafka_DeleteConsumerGroupOffsets().
 *
 * @param group Consumer group id.
 * @param partitions Partitions to delete committed offsets for.
 *                   Only the topic and partition fields are used.
 *
 * @returns a new allocated DeleteConsumerGroupOffsets object.
 *          Use rd_kafka_DeleteConsumerGroupOffsets_destroy() to free
 *          object when done.
 */
RD_EXPORT rd_kafka_DeleteConsumerGroupOffsets_t *
rd_kafka_DeleteConsumerGroupOffsets_new (const char *group,
                                         const rd_kafka_topic_partition_list_t
                                         *partitions);

/**
 * @brief Destroy and free a DeleteConsumerGroupOffsets object previously
 *        created with rd_kafka_DeleteConsumerGroupOffsets_new()
 */
RD_EXPORT void
rd_kafka_DeleteConsumerGroupOffsets_destroy (
        rd_kafka_DeleteConsumerGroupOffsets_t *del_grpoffsets);

/**
 * @brief Helper function to destroy all DeleteConsumerGroupOffsets objects in
 *        the \p del_grpoffsets array (of \p del_grpoffsets_cnt elements).
 *        The array itself is not freed.
 */
RD_EXPORT void
rd_kafka_DeleteConsumerGroupOffsets_destroy_array (
        rd_kafka_DeleteConsumerGroupOffsets_t **del_grpoffsets,
        size_t del_grpoffset_cnt);

/**
 * @brief Delete committed offsets for a set of partitions in a conusmer
 *        group. This will succeed at the partition level only if the group
 *        is not actively subscribed to the corresponding topic.
 *
 * @param rk Client instance.
 * @param del_grpoffsets Array of group committed offsets to delete.
 *                       MUST only be one single element.
 * @param del_grpoffsets_cnt Number of elements in \p del_grpoffsets array.
 *                           MUST always be 1.
 * @param options Optional admin options, or NULL for defaults.
 * @param rkqu Queue to emit result on.
 *
 * @remark The result event type emitted on the supplied queue is of type
 *         \c RD_KAFKA_EVENT_DELETECONSUMERGROUPOFFSETS_RESULT
 *
 * @remark The current implementation only supports one group per invocation.
 */
RD_EXPORT
void rd_kafka_DeleteConsumerGroupOffsets (
        rd_kafka_t *rk,
        rd_kafka_DeleteConsumerGroupOffsets_t **del_grpoffsets,
        size_t del_grpoffsets_cnt,
        const rd_kafka_AdminOptions_t *options,
        rd_kafka_queue_t *rkqu);



/*
 * DeleteConsumerGroupOffsets result type and methods
 */

/**
 * @brief Get an array of results from a DeleteConsumerGroupOffsets result.
 *
 * The returned groups life-time is the same as the \p result object.
 *
 * @param result Result to get group results from.
 * @param cntp is updated to the number of elements in the array.
 */
RD_EXPORT const rd_kafka_group_result_t **
rd_kafka_DeleteConsumerGroupOffsets_result_groups (
        const rd_kafka_DeleteConsumerGroupOffsets_result_t *result,
        size_t *cntp);


/**@}*/


/**
 * @name Security APIs
 * @{
 *
 */

/**
 * @brief Set SASL/OAUTHBEARER token and metadata
 *
 * @param rk Client instance.
 * @param token_value the mandatory token value to set, often (but not
 *  necessarily) a JWS compact serialization as per
 *  https://tools.ietf.org/html/rfc7515#section-3.1.
 * @param md_lifetime_ms when the token expires, in terms of the number of
 *  milliseconds since the epoch.
 * @param md_principal_name the mandatory Kafka principal name associated
 *  with the token.
 * @param extensions optional SASL extensions key-value array with
 *  \p extensions_size elements (number of keys * 2), where [i] is the key and
 *  [i+1] is the key's value, to be communicated to the broker
 *  as additional key-value pairs during the initial client response as per
 *  https://tools.ietf.org/html/rfc7628#section-3.1. The key-value pairs are
 *  copied.
 * @param extension_size the number of SASL extension keys plus values,
 *  which must be a non-negative multiple of 2.
 * @param errstr A human readable error string (nul-terminated) is written to
 *               this location that must be of at least \p errstr_size bytes.
 *               The \p errstr is only written in case of error.
 * @param errstr_size Writable size in \p errstr.
 *
 * The SASL/OAUTHBEARER token refresh callback or event handler should invoke
 * this method upon success. The extension keys must not include the reserved
 * key "`auth`", and all extension keys and values must conform to the required
 * format as per https://tools.ietf.org/html/rfc7628#section-3.1:
 *
 *     key            = 1*(ALPHA)
 *     value          = *(VCHAR / SP / HTAB / CR / LF )
 *
 * @returns \c RD_KAFKA_RESP_ERR_NO_ERROR on success, otherwise \p errstr set
 *              and:<br>
 *          \c RD_KAFKA_RESP_ERR__INVALID_ARG if any of the arguments are
 *              invalid;<br>
 *          \c RD_KAFKA_RESP_ERR__NOT_IMPLEMENTED if SASL/OAUTHBEARER is not
 *              supported by this build;<br>
 *          \c RD_KAFKA_RESP_ERR__STATE if SASL/OAUTHBEARER is supported but is
 *              not configured as the client's authentication mechanism.<br>
 *
 * @sa rd_kafka_oauthbearer_set_token_failure
 * @sa rd_kafka_conf_set_oauthbearer_token_refresh_cb
 */
RD_EXPORT
rd_kafka_resp_err_t
rd_kafka_oauthbearer_set_token (rd_kafka_t *rk,
                                const char *token_value,
                                int64_t md_lifetime_ms,
                                const char *md_principal_name,
                                const char **extensions, size_t extension_size,
                                char *errstr, size_t errstr_size);

/**
 * @brief SASL/OAUTHBEARER token refresh failure indicator.
 *
 * @param rk Client instance.
 * @param errstr mandatory human readable error reason for failing to acquire
 *  a token.
 *
 * The SASL/OAUTHBEARER token refresh callback or event handler should invoke
 * this method upon failure.
 *
 * @returns \c RD_KAFKA_RESP_ERR_NO_ERROR on success, otherwise:<br>
 *          \c RD_KAFKA_RESP_ERR__NOT_IMPLEMENTED if SASL/OAUTHBEARER is not
 *              supported by this build;<br>
 *          \c RD_KAFKA_RESP_ERR__STATE if SASL/OAUTHBEARER is supported but is
 *              not configured as the client's authentication mechanism,<br>
 *          \c RD_KAFKA_RESP_ERR__INVALID_ARG if no error string is supplied.
 *
 * @sa rd_kafka_oauthbearer_set_token
 * @sa rd_kafka_conf_set_oauthbearer_token_refresh_cb
 */
RD_EXPORT
rd_kafka_resp_err_t
rd_kafka_oauthbearer_set_token_failure (rd_kafka_t *rk, const char *errstr);

/**@}*/


/**
 * @name Transactional producer API
 *
 * The transactional producer operates on top of the idempotent producer,
 * and provides full exactly-once semantics (EOS) for Apache Kafka when used
 * with the transaction aware consumer (\c isolation.level=read_committed).
 *
 * A producer instance is configured for transactions by setting the
 * \c transactional.id to an identifier unique for the application. This
 * id will be used to fence stale transactions from previous instances of
 * the application, typically following an outage or crash.
 *
 * After creating the transactional producer instance using rd_kafka_new()
 * the transactional state must be initialized by calling
 * rd_kafka_init_transactions(). This is a blocking call that will
 * acquire a runtime producer id from the transaction coordinator broker
 * as well as abort any stale transactions and fence any still running producer
 * instances with the same \c transactional.id.
 *
 * Once transactions are initialized the application may begin a new
 * transaction by calling rd_kafka_begin_transaction().
 * A producer instance may only have one single on-going transaction.
 *
 * Any messages produced after the transaction has been started will
 * belong to the ongoing transaction and will be committed or aborted
 * atomically.
 * It is not permitted to produce messages outside a transaction
 * boundary, e.g., before rd_kafka_begin_transaction() or after
 * rd_kafka_commit_transaction(), rd_kafka_abort_transaction(), or after
 * the current transaction has failed.
 *
 * If consumed messages are used as input to the transaction, the consumer
 * instance must be configured with \c enable.auto.commit set to \c false.
 * To commit the consumed offsets along with the transaction pass the
 * list of consumed partitions and the last offset processed + 1 to
 * rd_kafka_send_offsets_to_transaction() prior to committing the transaction.
 * This allows an aborted transaction to be restarted using the previously
 * committed offsets.
 *
 * To commit the produced messages, and any consumed offsets, to the
 * current transaction, call rd_kafka_commit_transaction().
 * This call will block until the transaction has been fully committed or
 * failed (typically due to fencing by a newer producer instance).
 *
 * Alternatively, if processing fails, or an abortable transaction error is
 * raised, the transaction needs to be aborted by calling
 * rd_kafka_abort_transaction() which marks any produced messages and
 * offset commits as aborted.
 *
 * After the current transaction has been committed or aborted a new
 * transaction may be started by calling rd_kafka_begin_transaction() again.
 *
 * @par Retriable errors
 * Some error cases allow the attempted operation to be retried, this is
 * indicated by the error object having the retriable flag set which can
 * be detected by calling rd_kafka_error_is_retriable().
 * When this flag is set the application may retry the operation immediately
 * or preferably after a shorter grace period (to avoid busy-looping).
 * Retriable errors include timeouts, broker transport failures, etc.
 *
 * @par Abortable errors
 * An ongoing transaction may fail permanently due to various errors,
 * such as transaction coordinator becoming unavailable, write failures to the
 * Apache Kafka log, under-replicated partitions, etc.
 * At this point the producer application must abort the current transaction
 * using rd_kafka_abort_transaction() and optionally start a new transaction
 * by calling rd_kafka_begin_transaction().
 * Whether an error is abortable or not is detected by calling
 * rd_kafka_error_txn_requires_abort() on the returned error object.
 *
 * @par Fatal errors
 * While the underlying idempotent producer will typically only raise
 * fatal errors for unrecoverable cluster errors where the idempotency
 * guarantees can't be maintained, most of these are treated as abortable by
 * the transactional producer since transactions may be aborted and retried
 * in their entirety;
 * The transactional producer on the other hand introduces a set of additional
 * fatal errors which the application needs to handle by shutting down the
 * producer and terminate. There is no way for a producer instance to recover
 * from fatal errors.
 * Whether an error is fatal or not is detected by calling
 * rd_kafka_error_is_fatal() on the returned error object or by checking
 * the global rd_kafka_fatal_error() code.
 * Fatal errors are raised by triggering the \c error_cb (see the
 * Fatal error chapter in INTRODUCTION.md for more information), and any
 * sub-sequent transactional API calls will return RD_KAFKA_RESP_ERR__FATAL
 * or have the fatal flag set (see rd_kafka_error_is_fatal()).
 * The originating fatal error code can be retrieved by calling
 * rd_kafka_fatal_error().
 *
 * @par Handling of other errors
 * For errors that have neither retriable, abortable or the fatal flag set
 * it is not always obvious how to handle them. While some of these errors
 * may be indicative of bugs in the application code, such as when
 * an invalid parameter is passed to a method, other errors might originate
 * from the broker and be passed thru as-is to the application.
 * The general recommendation is to treat these errors, that have
 * neither the retriable or abortable flags set, as fatal.
 *
 * @par Error handling example
 * @code
 *     retry:
 *        rd_kafka_error_t *error;
 *
 *        error = rd_kafka_commit_transaction(producer, 10*1000);
 *        if (!error)
 *            return success;
 *        else if (rd_kafka_error_txn_requires_abort(error)) {
 *            do_abort_transaction_and_reset_inputs();
 *        } else if (rd_kafka_error_is_retriable(error)) {
 *            rd_kafka_error_destroy(error);
 *            goto retry;
 *        } else { // treat all other errors as fatal errors
 *            fatal_error(rd_kafka_error_string(error));
 *        }
 *        rd_kafka_error_destroy(error);
 * @endcode
 *
 *
 * @{
 */


/**
 * @brief Initialize transactions for the producer instance.
 *
 * This function ensures any transactions initiated by previous instances
 * of the producer with the same \c transactional.id are completed.
 * If the previous instance failed with a transaction in progress the
 * previous transaction will be aborted.
 * This function needs to be called before any other transactional or
 * produce functions are called when the \c transactional.id is configured.
 *
 * If the last transaction had begun completion (following transaction commit)
 * but not yet finished, this function will await the previous transaction's
 * completion.
 *
 * When any previous transactions have been fenced this function
 * will acquire the internal producer id and epoch, used in all future
 * transactional messages issued by this producer instance.
 *
 * @param rk Producer instance.
 * @param timeout_ms The maximum time to block. On timeout the operation
 *                   may continue in the background, depending on state,
 *                   and it is okay to call init_transactions() again.
 *
 * @remark This function may block up to \p timeout_ms milliseconds.
 *
 * @returns NULL on success or an error object on failure.
 *          Check whether the returned error object permits retrying
 *          by calling rd_kafka_error_is_retriable(), or whether a fatal
 *          error has been raised by calling rd_kafka_error_is_fatal().
 *          Error codes:
 *          RD_KAFKA_RESP_ERR__TIMED_OUT if the transaction coordinator
 *          could be not be contacted within \p timeout_ms (retriable),
 *          RD_KAFKA_RESP_ERR_COORDINATOR_NOT_AVAILABLE if the transaction
 *          coordinator is not available (retriable),
 *          RD_KAFKA_RESP_ERR_CONCURRENT_TRANSACTIONS if a previous transaction
 *          would not complete within \p timeout_ms (retriable),
 *          RD_KAFKA_RESP_ERR__STATE if transactions have already been started
 *          or upon fatal error,
 *          RD_KAFKA_RESP_ERR__UNSUPPORTED_FEATURE if the broker(s) do not
 *          support transactions (<Apache Kafka 0.11), this also raises a
 *          fatal error,
 *          RD_KAFKA_RESP_ERR_INVALID_TRANSACTION_TIMEOUT if the configured
 *          \c transaction.timeout.ms is outside the broker-configured range,
 *          this also raises a fatal error,
 *          RD_KAFKA_RESP_ERR__NOT_CONFIGURED if transactions have not been
 *          configured for the producer instance,
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if \p rk is not a producer instance,
 *          or \p timeout_ms is out of range.
 *          Other error codes not listed here may be returned, depending on
 *          broker version.
 *
 * @remark The returned error object (if not NULL) must be destroyed with
 *         rd_kafka_error_destroy().
 */
RD_EXPORT
rd_kafka_error_t *
rd_kafka_init_transactions (rd_kafka_t *rk, int timeout_ms);



/**
 * @brief Begin a new transaction.
 *
 * rd_kafka_init_transactions() must have been called successfully (once)
 * before this function is called.
 *
 * Upon successful return from this function the application has to perform at
 * least one of the following operations within \c transaction.timeout.ms to
 * avoid timing out the transaction on the broker:
 *   * rd_kafka_produce() (et.al)
 *   * rd_kafka_send_offsets_to_transaction()
 *   * rd_kafka_commit_transaction()
 *   * rd_kafka_abort_transaction()
 *
 * Any messages produced, offsets sent (rd_kafka_send_offsets_to_transaction()),
 * etc, after the successful return of this function will be part of
 * the transaction and committed or aborted atomatically.
 *
 * Finish the transaction by calling rd_kafka_commit_transaction() or
 * abort the transaction by calling rd_kafka_abort_transaction().
 *
 * @param rk Producer instance.
 *
 * @returns NULL on success or an error object on failure.
 *          Check whether a fatal error has been raised by
 *          calling rd_kafka_error_is_fatal().
 *          Error codes:
 *          RD_KAFKA_RESP_ERR__STATE if a transaction is already in progress
 *          or upon fatal error,
 *          RD_KAFKA_RESP_ERR__NOT_CONFIGURED if transactions have not been
 *          configured for the producer instance,
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if \p rk is not a producer instance.
 *          Other error codes not listed here may be returned, depending on
 *          broker version.
 *
 * @remark With the transactional producer, rd_kafka_produce(),
 *         rd_kafka_producev(), et.al, are only allowed during an on-going
 *         transaction, as started with this function.
 *         Any produce call outside an on-going transaction, or for a failed
 *         transaction, will fail.
 *
 * @remark The returned error object (if not NULL) must be destroyed with
 *         rd_kafka_error_destroy().
 */
RD_EXPORT
rd_kafka_error_t *rd_kafka_begin_transaction (rd_kafka_t *rk);


/**
 * @brief Sends a list of topic partition offsets to the consumer group
 *        coordinator for \p cgmetadata, and marks the offsets as part
 *        part of the current transaction.
 *        These offsets will be considered committed only if the transaction is
 *        committed successfully.
 *
 *        The offsets should be the next message your application will consume,
 *        i.e., the last processed message's offset + 1 for each partition.
 *        Either track the offsets manually during processing or use
 *        rd_kafka_position() (on the consumer) to get the current offsets for
 *        the partitions assigned to the consumer.
 *
 *        Use this method at the end of a consume-transform-produce loop prior
 *        to committing the transaction with rd_kafka_commit_transaction().
 *
 * @param rk Producer instance.
 * @param offsets List of offsets to commit to the consumer group upon
 *                successful commit of the transaction. Offsets should be
 *                the next message to consume, e.g., last processed message + 1.
 * @param cgmetadata The current consumer group metadata as returned by
 *                   rd_kafka_consumer_group_metadata() on the consumer
 *                   instance the provided offsets were consumed from.
 * @param timeout_ms Maximum time allowed to register the offsets on the broker.
 *
 * @remark This function must be called on the transactional producer instance,
 *         not the consumer.
 *
 * @remark The consumer must disable auto commits
 *         (set \c enable.auto.commit to false on the consumer).
 *
 * @remark Logical and invalid offsets (such as RD_KAFKA_OFFSET_INVALID) in
 *         \p offsets will be ignored, if there are no valid offsets in
 *         \p offsets the function will return RD_KAFKA_RESP_ERR_NO_ERROR
 *         and no action will be taken.
 *
 * @returns NULL on success or an error object on failure.
 *          Check whether the returned error object permits retrying
 *          by calling rd_kafka_error_is_retriable(), or whether an abortable
 *          or fatal error has been raised by calling
 *          rd_kafka_error_txn_requires_abort() or rd_kafka_error_is_fatal()
 *          respectively.
 *          Error codes:
 *          RD_KAFKA_RESP_ERR__STATE if not currently in a transaction,
 *          RD_KAFKA_RESP_ERR_INVALID_PRODUCER_EPOCH if the current producer
 *          transaction has been fenced by a newer producer instance,
 *          RD_KAFKA_RESP_ERR_TRANSACTIONAL_ID_AUTHORIZATION_FAILED if the
 *          producer is no longer authorized to perform transactional
 *          operations,
 *          RD_KAFKA_RESP_ERR_GROUP_AUTHORIZATION_FAILED if the producer is
 *          not authorized to write the consumer offsets to the group
 *          coordinator,
 *          RD_KAFKA_RESP_ERR__NOT_CONFIGURED if transactions have not been
 *          configured for the producer instance,
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if \p rk is not a producer instance,
 *          or if the \p consumer_group_id or \p offsets are empty,
 *          RD_KAFKA_RESP_ERR__PREV_IN_PROGRESS if a previous
 *          rd_kafka_send_offsets_to_transaction() call is still in progress.
 *          Other error codes not listed here may be returned, depending on
 *          broker version.
 *
 * @remark The returned error object (if not NULL) must be destroyed with
 *         rd_kafka_error_destroy().
 */
RD_EXPORT
rd_kafka_error_t *
rd_kafka_send_offsets_to_transaction (
        rd_kafka_t *rk,
        const rd_kafka_topic_partition_list_t *offsets,
        const rd_kafka_consumer_group_metadata_t *cgmetadata,
        int timeout_ms);


/**
 * @brief Commit the current transaction (as started with
 *        rd_kafka_begin_transaction()).
 *
 *        Any outstanding messages will be flushed (delivered) before actually
 *        committing the transaction.
 *
 *        If any of the outstanding messages fail permanently the current
 *        transaction will enter the abortable error state and this
 *        function will return an abortable error, in this case the application
 *        must call rd_kafka_abort_transaction() before attempting a new
 *        transaction with rd_kafka_begin_transaction().
 *
 * @param rk Producer instance.
 * @param timeout_ms The maximum time to block. On timeout the operation
 *                   may continue in the background, depending on state,
 *                   and it is okay to call this function again.
 *                   Pass -1 to use the remaining transaction timeout,
 *                   this is the recommended use.
 *
 * @remark It is strongly recommended to always pass -1 (remaining transaction
 *         time) as the \p timeout_ms. Using other values risk internal
 *         state desynchronization in case any of the underlying protocol
 *         requests fail.
 *
 * @remark This function will block until all outstanding messages are
 *         delivered and the transaction commit request has been successfully
 *         handled by the transaction coordinator, or until \p timeout_ms
 *         expires, which ever comes first. On timeout the application may
 *         call the function again.
 *
 * @remark Will automatically call rd_kafka_flush() to ensure all queued
 *         messages are delivered before attempting to commit the
 *         transaction.
 *         If the application has enabled RD_KAFKA_EVENT_DR it must
 *         serve the event queue in a separate thread since rd_kafka_flush()
 *         will not serve delivery reports in this mode.
 *
 * @returns NULL on success or an error object on failure.
 *          Check whether the returned error object permits retrying
 *          by calling rd_kafka_error_is_retriable(), or whether an abortable
 *          or fatal error has been raised by calling
 *          rd_kafka_error_txn_requires_abort() or rd_kafka_error_is_fatal()
 *          respectively.
 *          Error codes:
 *          RD_KAFKA_RESP_ERR__STATE if not currently in a transaction,
 *          RD_KAFKA_RESP_ERR__TIMED_OUT if the transaction could not be
 *          complete commmitted within \p timeout_ms, this is a retriable
 *          error as the commit continues in the background,
 *          RD_KAFKA_RESP_ERR_INVALID_PRODUCER_EPOCH if the current producer
 *          transaction has been fenced by a newer producer instance,
 *          RD_KAFKA_RESP_ERR_TRANSACTIONAL_ID_AUTHORIZATION_FAILED if the
 *          producer is no longer authorized to perform transactional
 *          operations,
 *          RD_KAFKA_RESP_ERR__NOT_CONFIGURED if transactions have not been
 *          configured for the producer instance,
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if \p rk is not a producer instance,
 *          Other error codes not listed here may be returned, depending on
 *          broker version.
 *
 * @remark The returned error object (if not NULL) must be destroyed with
 *         rd_kafka_error_destroy().
 */
RD_EXPORT
rd_kafka_error_t *
rd_kafka_commit_transaction (rd_kafka_t *rk, int timeout_ms);


/**
 * @brief Aborts the ongoing transaction.
 *
 *        This function should also be used to recover from non-fatal abortable
 *        transaction errors.
 *
 *        Any outstanding messages will be purged and fail with
 *        RD_KAFKA_RESP_ERR__PURGE_INFLIGHT or RD_KAFKA_RESP_ERR__PURGE_QUEUE.
 *        See rd_kafka_purge() for details.
 *
 * @param rk Producer instance.
 * @param timeout_ms The maximum time to block. On timeout the operation
 *                   may continue in the background, depending on state,
 *                   and it is okay to call this function again.
 *                   Pass -1 to use the remaining transaction timeout,
 *                   this is the recommended use.
 *
 * @remark It is strongly recommended to always pass -1 (remaining transaction
 *         time) as the \p timeout_ms. Using other values risk internal
 *         state desynchronization in case any of the underlying protocol
 *         requests fail.
 *
 * @remark This function will block until all outstanding messages are purged
 *         and the transaction abort request has been successfully
 *         handled by the transaction coordinator, or until \p timeout_ms
 *         expires, which ever comes first. On timeout the application may
 *         call the function again.
 *         If the application has enabled RD_KAFKA_EVENT_DR it must
 *         serve the event queue in a separate thread since rd_kafka_flush()
 *         will not serve delivery reports in this mode.

 *
 * @returns NULL on success or an error object on failure.
 *          Check whether the returned error object permits retrying
 *          by calling rd_kafka_error_is_retriable(), or whether a fatal error
 *          has been raised by calling rd_kafka_error_is_fatal().
 *          Error codes:
 *          RD_KAFKA_RESP_ERR__STATE if not currently in a transaction,
 *          RD_KAFKA_RESP_ERR__TIMED_OUT if the transaction could not be
 *          complete commmitted within \p timeout_ms, this is a retriable
 *          error as the commit continues in the background,
 *          RD_KAFKA_RESP_ERR_INVALID_PRODUCER_EPOCH if the current producer
 *          transaction has been fenced by a newer producer instance,
 *          RD_KAFKA_RESP_ERR_TRANSACTIONAL_ID_AUTHORIZATION_FAILED if the
 *          producer is no longer authorized to perform transactional
 *          operations,
 *          RD_KAFKA_RESP_ERR__NOT_CONFIGURED if transactions have not been
 *          configured for the producer instance,
 *          RD_KAFKA_RESP_ERR__INVALID_ARG if \p rk is not a producer instance,
 *          Other error codes not listed here may be returned, depending on
 *          broker version.
 *
 * @remark The returned error object (if not NULL) must be destroyed with
 *         rd_kafka_error_destroy().
 */
RD_EXPORT
rd_kafka_error_t *
rd_kafka_abort_transaction (rd_kafka_t *rk, int timeout_ms);


/**@}*/

/* @cond NO_DOC */
#ifdef __cplusplus
}
#endif
#endif /* _RDKAFKA_H_ */
/* @endcond NO_DOC */

```

`IntelPTDriver/packages/librdkafka.redist.1.8.2/build/native/include/librdkafka/rdkafka_mock.h`:

```h
/*
 * librdkafka - Apache Kafka C library
 *
 * Copyright (c) 2019 Magnus Edenhill
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice,
 *    this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 *    this list of conditions and the following disclaimer in the documentation
 *    and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

#ifndef _RDKAFKA_MOCK_H_
#define _RDKAFKA_MOCK_H_

#ifndef _RDKAFKA_H_
#error "rdkafka_mock.h must be included after rdkafka.h"
#endif

#ifdef __cplusplus
extern "C" {
#if 0
} /* Restore indent */
#endif
#endif


/**
 * @name Mock cluster
 *
 * Provides a mock Kafka cluster with a configurable number of brokers
 * that support a reasonable subset of Kafka protocol operations,
 * error injection, etc.
 *
 * There are two ways to use the mock clusters, the most simple approach
 * is to configure `test.mock.num.brokers` (to e.g. 3) on the rd_kafka_t
 * in an existing application, which will replace the configured
 * `bootstrap.servers` with the mock cluster brokers.
 * This approach is convenient to easily test existing applications.
 *
 * The second approach is to explicitly create a mock cluster on an
 * rd_kafka_t instance by using rd_kafka_mock_cluster_new().
 *
 * Mock clusters provide localhost listeners that can be used as the bootstrap
 * servers by multiple rd_kafka_t instances.
 *
 * Currently supported functionality:
 *  - Producer
 *  - Idempotent Producer
 *  - Transactional Producer
 *  - Low-level consumer
 *  - High-level balanced consumer groups with offset commits
 *  - Topic Metadata and auto creation
 *
 * @remark High-level consumers making use of the balanced consumer groups
 *         are not supported.
 *
 * @remark This is an experimental public API that is NOT covered by the
 *         librdkafka API or ABI stability guarantees.
 *
 *
 * @warning THIS IS AN EXPERIMENTAL API, SUBJECT TO CHANGE OR REMOVAL.
 *
 * @{
 */

typedef struct rd_kafka_mock_cluster_s rd_kafka_mock_cluster_t;


/**
 * @brief Create new mock cluster with \p broker_cnt brokers.
 *
 * The broker ids will start at 1 up to and including \p broker_cnt.
 *
 * The \p rk instance is required for internal book keeping but continues
 * to operate as usual.
 */
RD_EXPORT
rd_kafka_mock_cluster_t *rd_kafka_mock_cluster_new (rd_kafka_t *rk,
                                                    int broker_cnt);


/**
 * @brief Destroy mock cluster.
 */
RD_EXPORT
void rd_kafka_mock_cluster_destroy (rd_kafka_mock_cluster_t *mcluster);



/**
 * @returns the rd_kafka_t instance for a cluster as passed to
 *          rd_kafka_mock_cluster_new().
 */
RD_EXPORT rd_kafka_t *
rd_kafka_mock_cluster_handle (const rd_kafka_mock_cluster_t *mcluster);


/**
 * @returns the rd_kafka_mock_cluster_t instance as created by
 *          setting the `test.mock.num.brokers` configuration property,
 *          or NULL if no such instance.
 */
RD_EXPORT rd_kafka_mock_cluster_t *
rd_kafka_handle_mock_cluster (const rd_kafka_t *rk);



/**
 * @returns the mock cluster's bootstrap.servers list
 */
RD_EXPORT const char *
rd_kafka_mock_cluster_bootstraps (const rd_kafka_mock_cluster_t *mcluster);


/**
 * @brief Clear the cluster's error state for the given \p ApiKey.
 */
RD_EXPORT
void rd_kafka_mock_clear_request_errors (rd_kafka_mock_cluster_t *mcluster,
                                         int16_t ApiKey);


/**
 * @brief Push \p cnt errors in the \p ... va-arg list onto the cluster's
 *        error stack for the given \p ApiKey.
 *
 * \p ApiKey is the Kafka protocol request type, e.g., ProduceRequest (0).
 *
 * The following \p cnt protocol requests matching \p ApiKey will fail with the
 * provided error code and removed from the stack, starting with
 * the first error code, then the second, etc.
 *
 * Passing \c RD_KAFKA_RESP_ERR__TRANSPORT will make the mock broker
 * disconnect the client which can be useful to trigger a disconnect on certain
 * requests.
 */
RD_EXPORT
void rd_kafka_mock_push_request_errors (rd_kafka_mock_cluster_t *mcluster,
                                        int16_t ApiKey, size_t cnt, ...);


/**
 * @brief Same as rd_kafka_mock_push_request_errors() but takes
 *        an array of errors.
 */
RD_EXPORT void
rd_kafka_mock_push_request_errors_array (rd_kafka_mock_cluster_t *mcluster,
                                         int16_t ApiKey,
                                         size_t cnt,
                                         const rd_kafka_resp_err_t *errors);


/**
 * @brief Push \p cnt errors and RTT tuples in the \p ... va-arg list onto
 *        the broker's error stack for the given \p ApiKey.
 *
 * \p ApiKey is the Kafka protocol request type, e.g., ProduceRequest (0).
 *
 * Each entry is a tuple of:
 *   rd_kafka_resp_err_t err - error to return (or 0)
 *   int rtt_ms              - response RTT/delay in milliseconds (or 0)
 *
 * The following \p cnt protocol requests matching \p ApiKey will fail with the
 * provided error code and removed from the stack, starting with
 * the first error code, then the second, etc.
 *
 * @remark The broker errors take precedence over the cluster errors.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_mock_broker_push_request_error_rtts (rd_kafka_mock_cluster_t *mcluster,
                                              int32_t broker_id,
                                              int16_t ApiKey, size_t cnt, ...);


/**
 * @brief Set the topic error to return in protocol requests.
 *
 * Currently only used for TopicMetadataRequest and AddPartitionsToTxnRequest.
 */
RD_EXPORT
void rd_kafka_mock_topic_set_error (rd_kafka_mock_cluster_t *mcluster,
                                    const char *topic,
                                    rd_kafka_resp_err_t err);


/**
 * @brief Creates a topic.
 *
 * This is an alternative to automatic topic creation as performed by
 * the client itself.
 *
 * @remark The Topic Admin API (CreateTopics) is not supported by the
 *         mock broker.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_mock_topic_create (rd_kafka_mock_cluster_t *mcluster,
                            const char *topic, int partition_cnt,
                            int replication_factor);


/**
 * @brief Sets the partition leader.
 *
 * The topic will be created if it does not exist.
 *
 * \p broker_id needs to be an existing broker, or -1 to make the
 * partition leader-less.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_mock_partition_set_leader (rd_kafka_mock_cluster_t *mcluster,
                                    const char *topic, int32_t partition,
                                    int32_t broker_id);

/**
 * @brief Sets the partition's preferred replica / follower.
 *
 * The topic will be created if it does not exist.
 *
 * \p broker_id does not need to point to an existing broker.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_mock_partition_set_follower (rd_kafka_mock_cluster_t *mcluster,
                                      const char *topic, int32_t partition,
                                      int32_t broker_id);

/**
 * @brief Sets the partition's preferred replica / follower low and high
 *        watermarks.
 *
 * The topic will be created if it does not exist.
 *
 * Setting an offset to -1 will revert back to the leader's corresponding
 * watermark.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_mock_partition_set_follower_wmarks (rd_kafka_mock_cluster_t *mcluster,
                                             const char *topic,
                                             int32_t partition,
                                             int64_t lo, int64_t hi);


/**
 * @brief Disconnects the broker and disallows any new connections.
 *        This does NOT trigger leader change.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_mock_broker_set_down (rd_kafka_mock_cluster_t *mcluster,
                               int32_t broker_id);

/**
 * @brief Makes the broker accept connections again.
 *        This does NOT trigger leader change.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_mock_broker_set_up (rd_kafka_mock_cluster_t *mcluster,
                             int32_t broker_id);


/**
 * @brief Set broker round-trip-time delay in milliseconds.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_mock_broker_set_rtt (rd_kafka_mock_cluster_t *mcluster,
                              int32_t broker_id, int rtt_ms);

/**
 * @brief Sets the broker's rack as reported in Metadata to the client.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_mock_broker_set_rack (rd_kafka_mock_cluster_t *mcluster,
                               int32_t broker_id, const char *rack);



/**
 * @brief Explicitly sets the coordinator. If this API is not a standard
 *        hashing scheme will be used.
 *
 * @param key_type  "transaction" or "group"
 * @param key       The transactional.id or group.id
 * @param broker_id The new coordinator, does not have to be a valid broker.
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_mock_coordinator_set (rd_kafka_mock_cluster_t *mcluster,
                               const char *key_type, const char *key,
                               int32_t broker_id);



/**
 * @brief Set the allowed ApiVersion range for \p ApiKey.
 *
 *        Set \p MinVersion and \p MaxVersion to -1 to disable the API
 *        completely.
 *
 *        \p MaxVersion MUST not exceed the maximum implemented value,
 *        see rdkafka_mock_handlers.c.
 *
 * @param ApiKey Protocol request type/key
 * @param MinVersion Minimum version supported (or -1 to disable).
 * @param MinVersion Maximum version supported (or -1 to disable).
 */
RD_EXPORT rd_kafka_resp_err_t
rd_kafka_mock_set_apiversion (rd_kafka_mock_cluster_t *mcluster,
                              int16_t ApiKey,
                              int16_t MinVersion, int16_t MaxVersion);


/**@}*/

#ifdef __cplusplus
}
#endif
#endif /* _RDKAFKA_MOCK_H_ */

```

`IntelPTDriver/packages/librdkafka.redist.1.8.2/build/native/include/librdkafka/rdkafkacpp.h`:

```h
/*
 * librdkafka - Apache Kafka C/C++ library
 *
 * Copyright (c) 2014 Magnus Edenhill
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice,
 *    this list of conditions and the following disclaimer.
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 *    this list of conditions and the following disclaimer in the documentation
 *    and/or other materials provided with the distribution.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

#ifndef _RDKAFKACPP_H_
#define _RDKAFKACPP_H_

/**
 * @file rdkafkacpp.h
 * @brief Apache Kafka C/C++ consumer and producer client library.
 *
 * rdkafkacpp.h contains the public C++ API for librdkafka.
 * The API is documented in this file as comments prefixing the class,
 * function, type, enum, define, etc.
 * For more information, see the C interface in rdkafka.h and read the
 * manual in INTRODUCTION.md.
 * The C++ interface is STD C++ '03 compliant and adheres to the
 * Google C++ Style Guide.

 * @sa For the C interface see rdkafka.h
 *
 * @tableofcontents
 */

/**@cond NO_DOC*/
#include <string>
#include <list>
#include <vector>
#include <cstdlib>
#include <cstring>
#include <stdint.h>
#include <sys/types.h>

#ifdef _WIN32
#ifndef ssize_t
#ifndef _BASETSD_H_
#include <basetsd.h>
#endif
#ifndef _SSIZE_T_DEFINED
#define _SSIZE_T_DEFINED
typedef SSIZE_T ssize_t;
#endif
#endif
#undef RD_EXPORT
#ifdef LIBRDKAFKA_STATICLIB
#define RD_EXPORT
#else
#ifdef LIBRDKAFKACPP_EXPORTS
#define RD_EXPORT __declspec(dllexport)
#else
#define RD_EXPORT __declspec(dllimport)
#endif
#endif
#else
#define RD_EXPORT
#endif

/**@endcond*/

extern "C" {
        /* Forward declarations */
        struct rd_kafka_s;
        struct rd_kafka_topic_s;
        struct rd_kafka_message_s;
        struct rd_kafka_conf_s;
        struct rd_kafka_topic_conf_s;
}

namespace RdKafka {

/**
 * @name Miscellaneous APIs
 * @{
 */

/**
 * @brief librdkafka version
 *
 * Interpreted as hex \c MM.mm.rr.xx:
 *  - MM = Major
 *  - mm = minor
 *  - rr = revision
 *  - xx = pre-release id (0xff is the final release)
 *
 * E.g.: \c 0x000801ff = 0.8.1
 *
 * @remark This value should only be used during compile time,
 *         for runtime checks of version use RdKafka::version()
 */
#define RD_KAFKA_VERSION  0x010802ff

/**
 * @brief Returns the librdkafka version as integer.
 *
 * @sa See RD_KAFKA_VERSION for how to parse the integer format.
 */
RD_EXPORT
int          version ();

/**
 * @brief Returns the librdkafka version as string.
 */
RD_EXPORT
std::string  version_str();

/**
 * @brief Returns a CSV list of the supported debug contexts
 *        for use with Conf::Set("debug", ..).
 */
RD_EXPORT
std::string get_debug_contexts();

/**
 * @brief Wait for all rd_kafka_t objects to be destroyed.
 *
 * @returns 0 if all kafka objects are now destroyed, or -1 if the
 * timeout was reached.
 * Since RdKafka handle deletion is an asynch operation the
 * \p wait_destroyed() function can be used for applications where
 * a clean shutdown is required.
 */
RD_EXPORT
int          wait_destroyed(int timeout_ms);

/**
 * @brief Allocate memory using the same allocator librdkafka uses.
 *
 * This is typically an abstraction for the malloc(3) call and makes sure
 * the application can use the same memory allocator as librdkafka for
 * allocating pointers that are used by librdkafka.
 *
 * @remark Memory allocated by mem_malloc() must be freed using
 *         mem_free().
 */
RD_EXPORT
void *mem_malloc (size_t size);

/**
 * @brief Free pointer returned by librdkafka
 *
 * This is typically an abstraction for the free(3) call and makes sure
 * the application can use the same memory allocator as librdkafka for
 * freeing pointers returned by librdkafka.
 *
 * In standard setups it is usually not necessary to use this interface
 * rather than the free(3) function.
 *
 * @remark mem_free() must only be used for pointers returned by APIs
 *         that explicitly mention using this function for freeing.
 */
RD_EXPORT
void mem_free (void *ptr);

/**@}*/



/**
 * @name Constants, errors, types
 * @{
 *
 *
 */

/**
 * @brief Error codes.
 *
 * The negative error codes delimited by two underscores
 * (\c _ERR__..) denotes errors internal to librdkafka and are
 * displayed as \c \"Local: \<error string..\>\", while the error codes
 * delimited by a single underscore (\c ERR_..) denote broker
 * errors and are displayed as \c \"Broker: \<error string..\>\".
 *
 * @sa Use RdKafka::err2str() to translate an error code a human readable string
 */
enum ErrorCode {
	/* Internal errors to rdkafka: */
	/** Begin internal error codes */
	ERR__BEGIN = -200,
	/** Received message is incorrect */
	ERR__BAD_MSG = -199,
	/** Bad/unknown compression */
	ERR__BAD_COMPRESSION = -198,
	/** Broker is going away */
	ERR__DESTROY = -197,
	/** Generic failure */
	ERR__FAIL = -196,
	/** Broker transport failure */
	ERR__TRANSPORT = -195,
	/** Critical system resource */
	ERR__CRIT_SYS_RESOURCE = -194,
	/** Failed to resolve broker */
	ERR__RESOLVE = -193,
	/** Produced message timed out*/
	ERR__MSG_TIMED_OUT = -192,
	/** Reached the end of the topic+partition queue on
	 *  the broker. Not really an error.
	 *  This event is disabled by default,
	 *  see the `enable.partition.eof` configuration property. */
	ERR__PARTITION_EOF = -191,
	/** Permanent: Partition does not exist in cluster. */
	ERR__UNKNOWN_PARTITION = -190,
	/** File or filesystem error */
	ERR__FS = -189,
	 /** Permanent: Topic does not exist in cluster. */
	ERR__UNKNOWN_TOPIC = -188,
	/** All broker connections are down. */
	ERR__ALL_BROKERS_DOWN = -187,
	/** Invalid argument, or invalid configuration */
	ERR__INVALID_ARG = -186,
	/** Operation timed out */
	ERR__TIMED_OUT = -185,
	/** Queue is full */
	ERR__QUEUE_FULL = -184,
	/** ISR count < required.acks */
        ERR__ISR_INSUFF = -183,
	/** Broker node update */
        ERR__NODE_UPDATE = -182,
	/** SSL error */
	ERR__SSL = -181,
	/** Waiting for coordinator to become available. */
        ERR__WAIT_COORD = -180,
	/** Unknown client group */
        ERR__UNKNOWN_GROUP = -179,
	/** Operation in progress */
        ERR__IN_PROGRESS = -178,
	 /** Previous operation in progress, wait for it to finish. */
        ERR__PREV_IN_PROGRESS = -177,
	 /** This operation would interfere with an existing subscription */
        ERR__EXISTING_SUBSCRIPTION = -176,
	/** Assigned partitions (rebalance_cb) */
        ERR__ASSIGN_PARTITIONS = -175,
	/** Revoked partitions (rebalance_cb) */
        ERR__REVOKE_PARTITIONS = -174,
	/** Conflicting use */
        ERR__CONFLICT = -173,
	/** Wrong state */
        ERR__STATE = -172,
	/** Unknown protocol */
        ERR__UNKNOWN_PROTOCOL = -171,
	/** Not implemented */
        ERR__NOT_IMPLEMENTED = -170,
	/** Authentication failure*/
	ERR__AUTHENTICATION = -169,
	/** No stored offset */
	ERR__NO_OFFSET = -168,
	/** Outdated */
	ERR__OUTDATED = -167,
	/** Timed out in queue */
	ERR__TIMED_OUT_QUEUE = -166,
        /** Feature not supported by broker */
        ERR__UNSUPPORTED_FEATURE = -165,
        /** Awaiting cache update */
        ERR__WAIT_CACHE = -164,
        /** Operation interrupted */
        ERR__INTR = -163,
        /** Key serialization error */
        ERR__KEY_SERIALIZATION = -162,
        /** Value serialization error */
        ERR__VALUE_SERIALIZATION = -161,
        /** Key deserialization error */
        ERR__KEY_DESERIALIZATION = -160,
        /** Value deserialization error */
        ERR__VALUE_DESERIALIZATION = -159,
        /** Partial response */
        ERR__PARTIAL = -158,
        /** Modification attempted on read-only object */
        ERR__READ_ONLY = -157,
        /** No such entry / item not found */
        ERR__NOENT = -156,
        /** Read underflow */
        ERR__UNDERFLOW = -155,
        /** Invalid type */
        ERR__INVALID_TYPE = -154,
        /** Retry operation */
        ERR__RETRY = -153,
        /** Purged in queue */
        ERR__PURGE_QUEUE = -152,
        /** Purged in flight */
        ERR__PURGE_INFLIGHT = -151,
        /** Fatal error: see RdKafka::Handle::fatal_error() */
        ERR__FATAL = -150,
        /** Inconsistent state */
        ERR__INCONSISTENT = -149,
        /** Gap-less ordering would not be guaranteed if proceeding */
        ERR__GAPLESS_GUARANTEE = -148,
        /** Maximum poll interval exceeded */
        ERR__MAX_POLL_EXCEEDED = -147,
        /** Unknown broker */
        ERR__UNKNOWN_BROKER = -146,
        /** Functionality not configured */
        ERR__NOT_CONFIGURED = -145,
        /** Instance has been fenced */
        ERR__FENCED = -144,
        /** Application generated error */
        ERR__APPLICATION = -143,
        /** Assignment lost */
        ERR__ASSIGNMENT_LOST = -142,
        /** No operation performed */
        ERR__NOOP = -141,
        /** No offset to automatically reset to */
        ERR__AUTO_OFFSET_RESET = -140,

        /** End internal error codes */
	ERR__END = -100,

	/* Kafka broker errors: */
	/** Unknown broker error */
	ERR_UNKNOWN = -1,
	/** Success */
	ERR_NO_ERROR = 0,
	/** Offset out of range */
	ERR_OFFSET_OUT_OF_RANGE = 1,
	/** Invalid message */
	ERR_INVALID_MSG = 2,
	/** Unknown topic or partition */
	ERR_UNKNOWN_TOPIC_OR_PART = 3,
	/** Invalid message size */
	ERR_INVALID_MSG_SIZE = 4,
	/** Leader not available */
	ERR_LEADER_NOT_AVAILABLE = 5,
	/** Not leader for partition */
	ERR_NOT_LEADER_FOR_PARTITION = 6,
	/** Request timed out */
	ERR_REQUEST_TIMED_OUT = 7,
	/** Broker not available */
	ERR_BROKER_NOT_AVAILABLE = 8,
	/** Replica not available */
	ERR_REPLICA_NOT_AVAILABLE = 9,
	/** Message size too large */
	ERR_MSG_SIZE_TOO_LARGE = 10,
	/** StaleControllerEpochCode */
	ERR_STALE_CTRL_EPOCH = 11,
	/** Offset metadata string too large */
	ERR_OFFSET_METADATA_TOO_LARGE = 12,
	/** Broker disconnected before response received */
	ERR_NETWORK_EXCEPTION = 13,
        /** Coordinator load in progress */
        ERR_COORDINATOR_LOAD_IN_PROGRESS = 14,
        /** Group coordinator load in progress */
#define ERR_GROUP_LOAD_IN_PROGRESS           ERR_COORDINATOR_LOAD_IN_PROGRESS
        /** Coordinator not available */
        ERR_COORDINATOR_NOT_AVAILABLE = 15,
        /** Group coordinator not available */
#define ERR_GROUP_COORDINATOR_NOT_AVAILABLE  ERR_COORDINATOR_NOT_AVAILABLE
        /** Not coordinator */
        ERR_NOT_COORDINATOR = 16,
        /** Not coordinator for group */
#define ERR_NOT_COORDINATOR_FOR_GROUP        ERR_NOT_COORDINATOR
	/** Invalid topic */
        ERR_TOPIC_EXCEPTION = 17,
	/** Message batch larger than configured server segment size */
        ERR_RECORD_LIST_TOO_LARGE = 18,
	/** Not enough in-sync replicas */
        ERR_NOT_ENOUGH_REPLICAS = 19,
	/** Message(s) written to insufficient number of in-sync replicas */
        ERR_NOT_ENOUGH_REPLICAS_AFTER_APPEND = 20,
	/** Invalid required acks value */
        ERR_INVALID_REQUIRED_ACKS = 21,
	/** Specified group generation id is not valid */
        ERR_ILLEGAL_GENERATION = 22,
	/** Inconsistent group protocol */
        ERR_INCONSISTENT_GROUP_PROTOCOL = 23,
	/** Invalid group.id */
	ERR_INVALID_GROUP_ID = 24,
	/** Unknown member */
        ERR_UNKNOWN_MEMBER_ID = 25,
	/** Invalid session timeout */
        ERR_INVALID_SESSION_TIMEOUT = 26,
	/** Group rebalance in progress */
	ERR_REBALANCE_IN_PROGRESS = 27,
	/** Commit offset data size is not valid */
        ERR_INVALID_COMMIT_OFFSET_SIZE = 28,
	/** Topic authorization failed */
        ERR_TOPIC_AUTHORIZATION_FAILED = 29,
	/** Group authorization failed */
	ERR_GROUP_AUTHORIZATION_FAILED = 30,
	/** Cluster authorization failed */
	ERR_CLUSTER_AUTHORIZATION_FAILED = 31,
        /** Invalid timestamp */
        ERR_INVALID_TIMESTAMP = 32,
        /** Unsupported SASL mechanism */
        ERR_UNSUPPORTED_SASL_MECHANISM = 33,
        /** Illegal SASL state */
        ERR_ILLEGAL_SASL_STATE = 34,
        /** Unuspported version */
        ERR_UNSUPPORTED_VERSION = 35,
        /** Topic already exists */
        ERR_TOPIC_ALREADY_EXISTS = 36,
        /** Invalid number of partitions */
        ERR_INVALID_PARTITIONS = 37,
        /** Invalid replication factor */
        ERR_INVALID_REPLICATION_FACTOR = 38,
        /** Invalid replica assignment */
        ERR_INVALID_REPLICA_ASSIGNMENT = 39,
        /** Invalid config */
        ERR_INVALID_CONFIG = 40,
        /** Not controller for cluster */
        ERR_NOT_CONTROLLER = 41,
        /** Invalid request */
        ERR_INVALID_REQUEST = 42,
        /** Message format on broker does not support request */
        ERR_UNSUPPORTED_FOR_MESSAGE_FORMAT = 43,
        /** Policy violation */
        ERR_POLICY_VIOLATION = 44,
        /** Broker received an out of order sequence number */
        ERR_OUT_OF_ORDER_SEQUENCE_NUMBER = 45,
        /** Broker received a duplicate sequence number */
        ERR_DUPLICATE_SEQUENCE_NUMBER = 46,
        /** Producer attempted an operation with an old epoch */
        ERR_INVALID_PRODUCER_EPOCH = 47,
        /** Producer attempted a transactional operation in an invalid state */
        ERR_INVALID_TXN_STATE = 48,
        /** Producer attempted to use a producer id which is not
         *  currently assigned to its transactional id */
        ERR_INVALID_PRODUCER_ID_MAPPING = 49,
        /** Transaction timeout is larger than the maximum
         *  value allowed by the broker's max.transaction.timeout.ms */
        ERR_INVALID_TRANSACTION_TIMEOUT = 50,
        /** Producer attempted to update a transaction while another
         *  concurrent operation on the same transaction was ongoing */
        ERR_CONCURRENT_TRANSACTIONS = 51,
        /** Indicates that the transaction coordinator sending a
         *  WriteTxnMarker is no longer the current coordinator for a
         *  given producer */
        ERR_TRANSACTION_COORDINATOR_FENCED = 52,
        /** Transactional Id authorization failed */
        ERR_TRANSACTIONAL_ID_AUTHORIZATION_FAILED = 53,
        /** Security features are disabled */
        ERR_SECURITY_DISABLED = 54,
        /** Operation not attempted */
        ERR_OPERATION_NOT_ATTEMPTED = 55,
        /** Disk error when trying to access log file on the disk */
        ERR_KAFKA_STORAGE_ERROR = 56,
        /** The user-specified log directory is not found in the broker config */
        ERR_LOG_DIR_NOT_FOUND = 57,
        /** SASL Authentication failed */
        ERR_SASL_AUTHENTICATION_FAILED = 58,
        /** Unknown Producer Id */
        ERR_UNKNOWN_PRODUCER_ID = 59,
        /** Partition reassignment is in progress */
        ERR_REASSIGNMENT_IN_PROGRESS = 60,
        /** Delegation Token feature is not enabled */
        ERR_DELEGATION_TOKEN_AUTH_DISABLED = 61,
        /** Delegation Token is not found on server */
        ERR_DELEGATION_TOKEN_NOT_FOUND = 62,
        /** Specified Principal is not valid Owner/Renewer */
        ERR_DELEGATION_TOKEN_OWNER_MISMATCH = 63,
        /** Delegation Token requests are not allowed on this connection */
        ERR_DELEGATION_TOKEN_REQUEST_NOT_ALLOWED = 64,
        /** Delegation Token authorization failed */
        ERR_DELEGATION_TOKEN_AUTHORIZATION_FAILED = 65,
        /** Delegation Token is expired */
        ERR_DELEGATION_TOKEN_EXPIRED = 66,
        /** Supplied principalType is not supported */
        ERR_INVALID_PRINCIPAL_TYPE = 67,
        /** The group is not empty */
        ERR_NON_EMPTY_GROUP = 68,
        /** The group id does not exist */
        ERR_GROUP_ID_NOT_FOUND = 69,
        /** The fetch session ID was not found */
        ERR_FETCH_SESSION_ID_NOT_FOUND = 70,
        /** The fetch session epoch is invalid */
        ERR_INVALID_FETCH_SESSION_EPOCH = 71,
        /** No matching listener */
        ERR_LISTENER_NOT_FOUND = 72,
        /** Topic deletion is disabled */
        ERR_TOPIC_DELETION_DISABLED = 73,
        /** Leader epoch is older than broker epoch */
        ERR_FENCED_LEADER_EPOCH = 74,
        /** Leader epoch is newer than broker epoch */
        ERR_UNKNOWN_LEADER_EPOCH = 75,
        /** Unsupported compression type */
        ERR_UNSUPPORTED_COMPRESSION_TYPE = 76,
        /** Broker epoch has changed */
        ERR_STALE_BROKER_EPOCH = 77,
        /** Leader high watermark is not caught up */
        ERR_OFFSET_NOT_AVAILABLE = 78,
        /** Group member needs a valid member ID */
        ERR_MEMBER_ID_REQUIRED = 79,
        /** Preferred leader was not available */
        ERR_PREFERRED_LEADER_NOT_AVAILABLE = 80,
        /** Consumer group has reached maximum size */
        ERR_GROUP_MAX_SIZE_REACHED = 81,
        /** Static consumer fenced by other consumer with same
         * group.instance.id. */
        ERR_FENCED_INSTANCE_ID = 82,
        /** Eligible partition leaders are not available */
        ERR_ELIGIBLE_LEADERS_NOT_AVAILABLE = 83,
        /** Leader election not needed for topic partition */
        ERR_ELECTION_NOT_NEEDED = 84,
        /** No partition reassignment is in progress */
        ERR_NO_REASSIGNMENT_IN_PROGRESS = 85,
        /** Deleting offsets of a topic while the consumer group is
         *  subscribed to it */
        ERR_GROUP_SUBSCRIBED_TO_TOPIC = 86,
        /** Broker failed to validate record */
        ERR_INVALID_RECORD = 87,
        /** There are unstable offsets that need to be cleared */
        ERR_UNSTABLE_OFFSET_COMMIT = 88,
        /** Throttling quota has been exceeded */
        ERR_THROTTLING_QUOTA_EXCEEDED = 89,
        /** There is a newer producer with the same transactionalId
         *  which fences the current one */
        ERR_PRODUCER_FENCED = 90,
        /** Request illegally referred to resource that does not exist */
        ERR_RESOURCE_NOT_FOUND = 91,
        /** Request illegally referred to the same resource twice */
        ERR_DUPLICATE_RESOURCE = 92,
        /** Requested credential would not meet criteria for acceptability */
        ERR_UNACCEPTABLE_CREDENTIAL = 93,
        /** Indicates that the either the sender or recipient of a
         *  voter-only request is not one of the expected voters */
        ERR_INCONSISTENT_VOTER_SET = 94,
        /** Invalid update version */
        ERR_INVALID_UPDATE_VERSION = 95,
        /** Unable to update finalized features due to server error */
        ERR_FEATURE_UPDATE_FAILED = 96,
        /** Request principal deserialization failed during forwarding */
        ERR_PRINCIPAL_DESERIALIZATION_FAILURE = 97
};


/**
 * @brief Returns a human readable representation of a kafka error.
 */
RD_EXPORT
std::string  err2str(RdKafka::ErrorCode err);



/**
 * @enum CertificateType
 * @brief SSL certificate types
 */
enum CertificateType {
  CERT_PUBLIC_KEY,   /**< Client's public key */
  CERT_PRIVATE_KEY,  /**< Client's private key */
  CERT_CA,           /**< CA certificate */
  CERT__CNT
};

/**
 * @enum CertificateEncoding
 * @brief SSL certificate encoding
 */
enum CertificateEncoding {
  CERT_ENC_PKCS12,  /**< PKCS#12 */
  CERT_ENC_DER,     /**< DER / binary X.509 ASN1 */
  CERT_ENC_PEM,     /**< PEM */
  CERT_ENC__CNT
};

/**@} */



/**@cond NO_DOC*/
/* Forward declarations */
class Handle;
class Producer;
class Message;
class Headers;
class Queue;
class Event;
class Topic;
class TopicPartition;
class Metadata;
class KafkaConsumer;
/**@endcond*/


/**
 * @name Error class
 * @{
 *
 */

/**
 * @brief The Error class is used as a return value from APIs to propagate
 *        an error. The error consists of an error code which is to be used
 *        programatically, an error string for showing to the user,
 *        and various error flags that can be used programmatically to decide
 *        how to handle the error; e.g., should the operation be retried,
 *        was it a fatal error, etc.
 *
 * Error objects must be deleted explicitly to free its resources.
 */
class RD_EXPORT Error {
 public:

 /**
  * @brief Create error object.
  */
 static Error *create (ErrorCode code, const std::string *errstr);

 virtual ~Error () { }

 /*
  * Error accessor methods
  */

 /**
  * @returns the error code, e.g., RdKafka::ERR_UNKNOWN_MEMBER_ID.
  */
 virtual ErrorCode code () const = 0;

 /**
  * @returns the error code name, e.g, "ERR_UNKNOWN_MEMBER_ID".
  */
 virtual std::string name () const = 0;

  /**
   * @returns a human readable error string.
   */
 virtual std::string str () const = 0;

 /**
  * @returns true if the error is a fatal error, indicating that the client
  *          instance is no longer usable, else false.
  */
 virtual bool is_fatal () const = 0;

 /**
  * @returns true if the operation may be retried, else false.
  */
 virtual bool is_retriable () const = 0;

 /**
  * @returns true if the error is an abortable transaction error in which case
  *          the application must call RdKafka::Producer::abort_transaction()
  *          and start a new transaction with
  *          RdKafka::Producer::begin_transaction() if it wishes to proceed
  *          with transactions.
  *          Else returns false.
  *
  * @remark The return value of this method is only valid for errors returned
  *         by the transactional API.
  */
 virtual bool txn_requires_abort () const = 0;
};

/**@}*/


/**
 * @name Callback classes
 * @{
 *
 *
 * librdkafka uses (optional) callbacks to propagate information and
 * delegate decisions to the application logic.
 *
 * An application must call RdKafka::poll() at regular intervals to
 * serve queued callbacks.
 */


/**
 * @brief Delivery Report callback class
 *
 * The delivery report callback will be called once for each message
 * accepted by RdKafka::Producer::produce() (et.al) with
 * RdKafka::Message::err() set to indicate the result of the produce request.
 *
 * The callback is called when a message is succesfully produced or
 * if librdkafka encountered a permanent failure, or the retry counter for
 * temporary errors has been exhausted.
 *
 * An application must call RdKafka::poll() at regular intervals to
 * serve queued delivery report callbacks.

 */
class RD_EXPORT DeliveryReportCb {
 public:
  /**
   * @brief Delivery report callback.
   */
  virtual void dr_cb (Message &message) = 0;

  virtual ~DeliveryReportCb() { }
};


/**
 * @brief SASL/OAUTHBEARER token refresh callback class
 *
 * The SASL/OAUTHBEARER token refresh callback is triggered via RdKafka::poll()
 * whenever OAUTHBEARER is the SASL mechanism and a token needs to be retrieved,
 * typically based on the configuration defined in \c sasl.oauthbearer.config.
 *
 * The \c oauthbearer_config argument is the value of the
 * \c sasl.oauthbearer.config configuration property.
 *
 * The callback should invoke RdKafka::Handle::oauthbearer_set_token() or
 * RdKafka::Handle::oauthbearer_set_token_failure() to indicate success or
 * failure, respectively.
 * 
 * The refresh operation is eventable and may be received when an event
 * callback handler is set with an event type of
 * \c RdKafka::Event::EVENT_OAUTHBEARER_TOKEN_REFRESH.
 *
 * Note that before any SASL/OAUTHBEARER broker connection can succeed the
 * application must call RdKafka::Handle::oauthbearer_set_token() once -- either
 * directly or, more typically, by invoking RdKafka::poll() -- in order to
 * cause retrieval of an initial token to occur.
 *
 * An application must call RdKafka::poll() at regular intervals to
 * serve queued SASL/OAUTHBEARER token refresh callbacks (when
 * OAUTHBEARER is the SASL mechanism).
 */
class RD_EXPORT OAuthBearerTokenRefreshCb {
 public:
  /**
   * @brief SASL/OAUTHBEARER token refresh callback class.
   *
   * @param handle The RdKafka::Handle which requires a refreshed token.
   * @param oauthbearer_config The value of the
   * \p sasl.oauthbearer.config configuration property for \p handle.
   */
  virtual void oauthbearer_token_refresh_cb (RdKafka::Handle* handle,
                                             const std::string &oauthbearer_config) = 0;

  virtual ~OAuthBearerTokenRefreshCb() { }
};


/**
 * @brief Partitioner callback class
 *
 * Generic partitioner callback class for implementing custom partitioners.
 *
 * @sa RdKafka::Conf::set() \c "partitioner_cb"
 */
class RD_EXPORT PartitionerCb {
 public:
  /**
   * @brief Partitioner callback
   *
   * Return the partition to use for \p key in \p topic.
   *
   * The \p msg_opaque is the same \p msg_opaque provided in the
   * RdKafka::Producer::produce() call.
   *
   * @remark \p key may be NULL or the empty.
   *
   * @returns Must return a value between 0 and \p partition_cnt (non-inclusive).
   *          May return RD_KAFKA_PARTITION_UA (-1) if partitioning failed.
   *
   * @sa The callback may use RdKafka::Topic::partition_available() to check
   *     if a partition has an active leader broker.
   */
  virtual int32_t partitioner_cb (const Topic *topic,
                                  const std::string *key,
                                  int32_t partition_cnt,
                                  void *msg_opaque) = 0;

  virtual ~PartitionerCb() { }
};

/**
 * @brief  Variant partitioner with key pointer
 *
 */
class PartitionerKeyPointerCb {
 public:
  /**
   * @brief Variant partitioner callback that gets \p key as pointer and length
   *        instead of as a const std::string *.
   *
   * @remark \p key may be NULL or have \p key_len 0.
   *
   * @sa See RdKafka::PartitionerCb::partitioner_cb() for exact semantics
   */
  virtual int32_t partitioner_cb (const Topic *topic,
                                  const void *key,
                                  size_t key_len,
                                  int32_t partition_cnt,
                                  void *msg_opaque) = 0;

  virtual ~PartitionerKeyPointerCb() { }
};



/**
 * @brief Event callback class
 *
 * Events are a generic interface for propagating errors, statistics, logs, etc
 * from librdkafka to the application.
 *
 * @sa RdKafka::Event
 */
class RD_EXPORT EventCb {
 public:
  /**
   * @brief Event callback
   *
   * @sa RdKafka::Event
   */
  virtual void event_cb (Event &event) = 0;

  virtual ~EventCb() { }
};


/**
 * @brief Event object class as passed to the EventCb callback.
 */
class RD_EXPORT Event {
 public:
  /** @brief Event type */
  enum Type {
    EVENT_ERROR,     /**< Event is an error condition */
    EVENT_STATS,     /**< Event is a statistics JSON document */
    EVENT_LOG,       /**< Event is a log message */
    EVENT_THROTTLE   /**< Event is a throttle level signaling from the broker */
  };

  /** @brief EVENT_LOG severities (conforms to syslog(3) severities) */
  enum Severity {
    EVENT_SEVERITY_EMERG = 0,
    EVENT_SEVERITY_ALERT = 1,
    EVENT_SEVERITY_CRITICAL = 2,
    EVENT_SEVERITY_ERROR = 3,
    EVENT_SEVERITY_WARNING = 4,
    EVENT_SEVERITY_NOTICE = 5,
    EVENT_SEVERITY_INFO = 6,
    EVENT_SEVERITY_DEBUG = 7
  };

  virtual ~Event () { }

  /*
   * Event Accessor methods
   */

  /**
   * @returns The event type
   * @remark Applies to all event types
   */
  virtual Type        type () const = 0;

  /**
   * @returns Event error, if any.
   * @remark Applies to all event types except THROTTLE
   */
  virtual ErrorCode   err () const = 0;

  /**
   * @returns Log severity level.
   * @remark Applies to LOG event type.
   */
  virtual Severity    severity () const = 0;

  /**
   * @returns Log facility string.
   * @remark Applies to LOG event type.
   */
  virtual std::string fac () const = 0;

  /**
   * @returns Log message string.
   *
   * \c EVENT_LOG: Log message string.
   * \c EVENT_STATS: JSON object (as string).
   *
   * @remark Applies to LOG event type.
   */
  virtual std::string str () const = 0;

  /**
   * @returns Throttle time in milliseconds.
   * @remark Applies to THROTTLE event type.
   */
  virtual int         throttle_time () const = 0;

  /**
   * @returns Throttling broker's name.
   * @remark Applies to THROTTLE event type.
   */
  virtual std::string broker_name () const = 0;

  /**
   * @returns Throttling broker's id.
   * @remark Applies to THROTTLE event type.
   */
  virtual int         broker_id () const = 0;


  /**
   * @returns true if this is a fatal error.
   * @remark Applies to ERROR event type.
   * @sa RdKafka::Handle::fatal_error()
   */
  virtual bool        fatal () const = 0;
};



/**
 * @brief Consume callback class
 */
class RD_EXPORT ConsumeCb {
 public:
  /**
   * @brief The consume callback is used with
   *        RdKafka::Consumer::consume_callback()
   *        methods and will be called for each consumed \p message.
   *
   * The callback interface is optional but provides increased performance.
   */
  virtual void consume_cb (Message &message, void *opaque) = 0;

  virtual ~ConsumeCb() { }
};


/**
 * @brief \b KafkaConsumer: Rebalance callback class
 */
class RD_EXPORT RebalanceCb {
public:
  /**
   * @brief Group rebalance callback for use with RdKafka::KafkaConsumer
   *
   * Registering a \p rebalance_cb turns off librdkafka's automatic
   * partition assignment/revocation and instead delegates that responsibility
   * to the application's \p rebalance_cb.
   *
   * The rebalance callback is responsible for updating librdkafka's
   * assignment set based on the two events: RdKafka::ERR__ASSIGN_PARTITIONS
   * and RdKafka::ERR__REVOKE_PARTITIONS but should also be able to handle
   * arbitrary rebalancing failures where \p err is neither of those.
   * @remark In this latter case (arbitrary error), the application must
   *         call unassign() to synchronize state.
   *
   * For eager/non-cooperative `partition.assignment.strategy` assignors,
   * such as `range` and `roundrobin`, the application must use
   * assign assign() to set and unassign() to clear the entire assignment.
   * For the cooperative assignors, such as `cooperative-sticky`, the
   * application must use incremental_assign() for ERR__ASSIGN_PARTITIONS and
   * incremental_unassign() for ERR__REVOKE_PARTITIONS.
   *
   * Without a rebalance callback this is done automatically by librdkafka
   * but registering a rebalance callback gives the application flexibility
   * in performing other operations along with the assinging/revocation,
   * such as fetching offsets from an alternate location (on assign)
   * or manually committing offsets (on revoke).
   *
   * @sa RdKafka::KafkaConsumer::assign()
   * @sa RdKafka::KafkaConsumer::incremental_assign()
   * @sa RdKafka::KafkaConsumer::incremental_unassign()
   * @sa RdKafka::KafkaConsumer::assignment_lost()
   * @sa RdKafka::KafkaConsumer::rebalance_protocol()
   *
   * The following example show's the application's responsibilities:
   * @code
   *    class MyRebalanceCb : public RdKafka::RebalanceCb {
   *     public:
   *      void rebalance_cb (RdKafka::KafkaConsumer *consumer,
   *                    RdKafka::ErrorCode err,
   *                    std::vector<RdKafka::TopicPartition*> &partitions) {
   *         if (err == RdKafka::ERR__ASSIGN_PARTITIONS) {
   *           // application may load offets from arbitrary external
   *           // storage here and update \p partitions
   *           if (consumer->rebalance_protocol() == "COOPERATIVE")
   *             consumer->incremental_assign(partitions);
   *           else
   *             consumer->assign(partitions);
   *
   *         } else if (err == RdKafka::ERR__REVOKE_PARTITIONS) {
   *           // Application may commit offsets manually here
   *           // if auto.commit.enable=false
   *           if (consumer->rebalance_protocol() == "COOPERATIVE")
   *             consumer->incremental_unassign(partitions);
   *           else
   *             consumer->unassign();
   *
   *         } else {
   *           std::cerr << "Rebalancing error: " <<
   *                        RdKafka::err2str(err) << std::endl;
   *           consumer->unassign();
   *         }
   *     }
   *  }
   * @endcode
   *
   * @remark The above example lacks error handling for assign calls, see
   *         the examples/ directory.
   */
 virtual void rebalance_cb (RdKafka::KafkaConsumer *consumer,
                            RdKafka::ErrorCode err,
                            std::vector<TopicPartition*>&partitions) = 0;

 virtual ~RebalanceCb() { }
};


/**
 * @brief Offset Commit callback class
 */
class RD_EXPORT OffsetCommitCb {
public:
  /**
   * @brief Set offset commit callback for use with consumer groups
   *
   * The results of automatic or manual offset commits will be scheduled
   * for this callback and is served by RdKafka::KafkaConsumer::consume().
   *
   * If no partitions had valid offsets to commit this callback will be called
   * with \p err == ERR__NO_OFFSET which is not to be considered an error.
   *
   * The \p offsets list contains per-partition information:
   *   - \c topic      The topic committed
   *   - \c partition  The partition committed
   *   - \c offset:    Committed offset (attempted)
   *   - \c err:       Commit error
   */
  virtual void offset_commit_cb(RdKafka::ErrorCode err,
                                std::vector<TopicPartition*>&offsets) = 0;

  virtual ~OffsetCommitCb() { }
};



/**
 * @brief SSL broker certificate verification class.
 *
 * @remark Class instance must outlive the RdKafka client instance.
 */
class RD_EXPORT SslCertificateVerifyCb {
public:
  /**
   * @brief SSL broker certificate verification callback.
   *
   * The verification callback is triggered from internal librdkafka threads
   * upon connecting to a broker. On each connection attempt the callback
   * will be called for each certificate in the broker's certificate chain,
   * starting at the root certification, as long as the application callback
   * returns 1 (valid certificate).
   *
   * \p broker_name and \p broker_id correspond to the broker the connection
   * is being made to.
   * The \c x509_error argument indicates if OpenSSL's verification of
   * the certificate succeed (0) or failed (an OpenSSL error code).
   * The application may set the SSL context error code by returning 0
   * from the verify callback and providing a non-zero SSL context error code
   * in \p x509_error.
   * If the verify callback sets \p x509_error to 0, returns 1, and the
   * original \p x509_error was non-zero, the error on the SSL context will
   * be cleared.
   * \p x509_error is always a valid pointer to an int.
   *
   * \p depth is the depth of the current certificate in the chain, starting
   * at the root certificate.
   *
   * The certificate itself is passed in binary DER format in \p buf of
   * size \p size.
   *
   * The callback must 1 if verification succeeds, or 0 if verification fails
   * and write a human-readable error message
   * to \p errstr.
   *
   * @warning This callback will be called from internal librdkafka threads.
   *
   * @remark See <openssl/x509_vfy.h> in the OpenSSL source distribution
   *         for a list of \p x509_error codes.
   */
  virtual bool ssl_cert_verify_cb (const std::string &broker_name,
                                   int32_t broker_id,
                                   int *x509_error,
                                   int depth,
                                   const char *buf, size_t size,
                                   std::string &errstr) = 0;

  virtual ~SslCertificateVerifyCb() {}
};


/**
 * @brief \b Portability: SocketCb callback class
 *
 */
class RD_EXPORT SocketCb {
 public:
  /**
   * @brief Socket callback
   *
   * The socket callback is responsible for opening a socket
   * according to the supplied \p domain, \p type and \p protocol.
   * The socket shall be created with \c CLOEXEC set in a racefree fashion, if
   * possible.
   *
   * It is typically not required to register an alternative socket
   * implementation
   *
   * @returns The socket file descriptor or -1 on error (\c errno must be set)
   */
  virtual int socket_cb (int domain, int type, int protocol) = 0;

  virtual ~SocketCb() { }
};


/**
 * @brief \b Portability: OpenCb callback class
 *
 */
class RD_EXPORT OpenCb {
 public:
  /**
   * @brief Open callback
   * The open callback is responsible for opening the file specified by
   * \p pathname, using \p flags and \p mode.
   * The file shall be opened with \c CLOEXEC set in a racefree fashion, if
   * possible.
   *
   * It is typically not required to register an alternative open implementation
   *
   * @remark Not currently available on native Win32
   */
  virtual int open_cb (const std::string &path, int flags, int mode) = 0;

  virtual ~OpenCb() { }
};


/**@}*/




/**
 * @name Configuration interface
 * @{
 *
 */

/**
 * @brief Configuration interface
 *
 * Holds either global or topic configuration that are passed to
 * RdKafka::Consumer::create(), RdKafka::Producer::create(),
 * RdKafka::KafkaConsumer::create(), etc.
 *
 * @sa CONFIGURATION.md for the full list of supported properties.
 */
class RD_EXPORT Conf {
 public:
  /**
   * @brief Configuration object type
   */
  enum ConfType {
    CONF_GLOBAL, /**< Global configuration */
    CONF_TOPIC   /**< Topic specific configuration */
  };

  /**
   * @brief RdKafka::Conf::Set() result code
   */
  enum ConfResult {
    CONF_UNKNOWN = -2,  /**< Unknown configuration property */
    CONF_INVALID = -1,  /**< Invalid configuration value */
    CONF_OK = 0         /**< Configuration property was succesfully set */
  };


  /**
   * @brief Create configuration object
   */
  static Conf *create (ConfType type);

  virtual ~Conf () { }

  /**
   * @brief Set configuration property \p name to value \p value.
   *
   * Fallthrough:
   * Topic-level configuration properties may be set using this interface
   * in which case they are applied on the \c default_topic_conf.
   * If no \c default_topic_conf has been set one will be created.
   * Any sub-sequent set("default_topic_conf", ..) calls will
   * replace the current default topic configuration.

   * @returns CONF_OK on success, else writes a human readable error
   *          description to \p errstr on error.
   */
  virtual Conf::ConfResult set (const std::string &name,
                                const std::string &value,
                                std::string &errstr) = 0;

  /** @brief Use with \p name = \c \"dr_cb\" */
  virtual Conf::ConfResult set (const std::string &name,
                                DeliveryReportCb *dr_cb,
                                std::string &errstr) = 0;

  /** @brief Use with \p name = \c \"oauthbearer_token_refresh_cb\" */
  virtual Conf::ConfResult set (const std::string &name,
                        OAuthBearerTokenRefreshCb *oauthbearer_token_refresh_cb,
                        std::string &errstr) = 0;

  /** @brief Use with \p name = \c \"event_cb\" */
  virtual Conf::ConfResult set (const std::string &name,
                                EventCb *event_cb,
                                std::string &errstr) = 0;

  /** @brief Use with \p name = \c \"default_topic_conf\"
   *
   * Sets the default topic configuration to use for for automatically
   * subscribed topics.
   *
   * @sa RdKafka::KafkaConsumer::subscribe()
   */
  virtual Conf::ConfResult set (const std::string &name,
                                const Conf *topic_conf,
                                std::string &errstr) = 0;

  /** @brief Use with \p name = \c \"partitioner_cb\" */
  virtual Conf::ConfResult set (const std::string &name,
                                PartitionerCb *partitioner_cb,
                                std::string &errstr) = 0;

  /** @brief Use with \p name = \c \"partitioner_key_pointer_cb\" */
  virtual Conf::ConfResult set (const std::string &name,
                                PartitionerKeyPointerCb *partitioner_kp_cb,
                                std::string &errstr) = 0;

  /** @brief Use with \p name = \c \"socket_cb\" */
  virtual Conf::ConfResult set (const std::string &name, SocketCb *socket_cb,
                                std::string &errstr) = 0;

  /** @brief Use with \p name = \c \"open_cb\" */
  virtual Conf::ConfResult set (const std::string &name, OpenCb *open_cb,
                                std::string &errstr) = 0;

  /** @brief Use with \p name = \c \"rebalance_cb\" */
  virtual Conf::ConfResult set (const std::string &name,
                                RebalanceCb *rebalance_cb,
                                std::string &errstr) = 0;

  /** @brief Use with \p name = \c \"offset_commit_cb\" */
  virtual Conf::ConfResult set (const std::string &name,
                                OffsetCommitCb *offset_commit_cb,
                                std::string &errstr) = 0;

  /** @brief Use with \p name = \c \"ssl_cert_verify_cb\".
   *  @returns CONF_OK on success or CONF_INVALID if SSL is
   *           not supported in this build.
  */
  virtual Conf::ConfResult set(const std::string &name,
                               SslCertificateVerifyCb *ssl_cert_verify_cb,
                               std::string &errstr) = 0;

  /**
   * @brief Set certificate/key \p cert_type from the \p cert_enc encoded
   *        memory at \p buffer of \p size bytes.
   *
   * @param cert_type Certificate or key type to configure.
   * @param cert_enc  Buffer \p encoding type.
   * @param buffer Memory pointer to encoded certificate or key.
   *               The memory is not referenced after this function returns.
   * @param size Size of memory at \p buffer.
   * @param errstr A human-readable error string will be written to this string
   *               on failure.
   *
   * @returns CONF_OK on success or CONF_INVALID if the memory in
   *          \p buffer is of incorrect encoding, or if librdkafka
   *          was not built with SSL support.
   *
   * @remark Calling this method multiple times with the same \p cert_type
   *         will replace the previous value.
   *
   * @remark Calling this method with \p buffer set to NULL will clear the
   *         configuration for \p cert_type.
   *
   * @remark The private key may require a password, which must be specified
   *         with the `ssl.key.password` configuration property prior to
   *         calling this function.
   *
   * @remark Private and public keys in PEM format may also be set with the
   *         `ssl.key.pem` and `ssl.certificate.pem` configuration properties.
   *
   * @remark CA certificate in PEM format may also be set with the
   *         `ssl.ca.pem` configuration property.
   */
  virtual Conf::ConfResult set_ssl_cert (RdKafka::CertificateType cert_type,
                                         RdKafka::CertificateEncoding cert_enc,
                                         const void *buffer, size_t size,
                                         std::string &errstr) = 0;

  /** @brief Query single configuration value
   *
   * Do not use this method to get callbacks registered by the configuration file.
   * Instead use the specific get() methods with the specific callback parameter in the signature.
   *
   * Fallthrough:
   * Topic-level configuration properties from the \c default_topic_conf
   * may be retrieved using this interface.
   *
   *  @returns CONF_OK if the property was set previously set and
   *           returns the value in \p value. */
  virtual Conf::ConfResult get(const std::string &name,
	  std::string &value) const = 0;

  /** @brief Query single configuration value
   *  @returns CONF_OK if the property was set previously set and
   *           returns the value in \p dr_cb. */
  virtual Conf::ConfResult get(DeliveryReportCb *&dr_cb) const = 0;

  /** @brief Query single configuration value
   *  @returns CONF_OK if the property was set previously set and
   *           returns the value in \p oauthbearer_token_refresh_cb. */
  virtual Conf::ConfResult get(
          OAuthBearerTokenRefreshCb *&oauthbearer_token_refresh_cb) const = 0;

  /** @brief Query single configuration value
   *  @returns CONF_OK if the property was set previously set and
   *           returns the value in \p event_cb. */
  virtual Conf::ConfResult get(EventCb *&event_cb) const = 0;

  /** @brief Query single configuration value
   *  @returns CONF_OK if the property was set previously set and
   *           returns the value in \p partitioner_cb. */
  virtual Conf::ConfResult get(PartitionerCb *&partitioner_cb) const = 0;

  /** @brief Query single configuration value
   *  @returns CONF_OK if the property was set previously set and
   *           returns the value in \p partitioner_kp_cb. */
  virtual Conf::ConfResult get(PartitionerKeyPointerCb *&partitioner_kp_cb) const = 0;

  /** @brief Query single configuration value
   *  @returns CONF_OK if the property was set previously set and
   *           returns the value in \p socket_cb. */
  virtual Conf::ConfResult get(SocketCb *&socket_cb) const = 0;

  /** @brief Query single configuration value
   *  @returns CONF_OK if the property was set previously set and
   *           returns the value in \p open_cb. */
  virtual Conf::ConfResult get(OpenCb *&open_cb) const = 0;

  /** @brief Query single configuration value
   *  @returns CONF_OK if the property was set previously set and
   *           returns the value in \p rebalance_cb. */
  virtual Conf::ConfResult get(RebalanceCb *&rebalance_cb) const = 0;

  /** @brief Query single configuration value
   *  @returns CONF_OK if the property was set previously set and
   *           returns the value in \p offset_commit_cb. */
  virtual Conf::ConfResult get(OffsetCommitCb *&offset_commit_cb) const = 0;

  /** @brief Use with \p name = \c \"ssl_cert_verify_cb\" */
  virtual Conf::ConfResult get(SslCertificateVerifyCb *&ssl_cert_verify_cb) const = 0;

  /** @brief Dump configuration names and values to list containing
   *         name,value tuples */
  virtual std::list<std::string> *dump () = 0;

  /** @brief Use with \p name = \c \"consume_cb\" */
  virtual Conf::ConfResult set (const std::string &name, ConsumeCb *consume_cb,
                                std::string &errstr) = 0;

  /**
   * @brief Returns the underlying librdkafka C rd_kafka_conf_t handle.
   *
   * @warning Calling the C API on this handle is not recommended and there
   *          is no official support for it, but for cases where the C++
   *          does not provide the proper functionality this C handle can be
   *          used to interact directly with the core librdkafka API.
   *
   * @remark The lifetime of the returned pointer is the same as the Conf
   *         object this method is called on.
   *
   * @remark Include <rdkafka/rdkafka.h> prior to including
   *         <rdkafka/rdkafkacpp.h>
   *
   * @returns \c rd_kafka_conf_t* if this is a CONF_GLOBAL object, else NULL.
   */
  virtual struct rd_kafka_conf_s *c_ptr_global () = 0;

  /**
   * @brief Returns the underlying librdkafka C rd_kafka_topic_conf_t handle.
   *
   * @warning Calling the C API on this handle is not recommended and there
   *          is no official support for it, but for cases where the C++
   *          does not provide the proper functionality this C handle can be
   *          used to interact directly with the core librdkafka API.
   *
   * @remark The lifetime of the returned pointer is the same as the Conf
   *         object this method is called on.
   *
   * @remark Include <rdkafka/rdkafka.h> prior to including
   *         <rdkafka/rdkafkacpp.h>
   *
   * @returns \c rd_kafka_topic_conf_t* if this is a CONF_TOPIC object,
   *          else NULL.
   */
  virtual struct rd_kafka_topic_conf_s *c_ptr_topic () = 0;

  /**
   * @brief Set callback_data for ssl engine.
   *
   * @remark The \c ssl.engine.location configuration must be set for this
   *         to have affect.
   *
   * @remark The memory pointed to by \p value must remain valid for the
   *         lifetime of the configuration object and any Kafka clients that
   *         use it.
   *
   * @returns CONF_OK on success, else CONF_INVALID.
   */
  virtual Conf::ConfResult set_engine_callback_data (void *value,
                                                     std::string &errstr) = 0;
};

/**@}*/


/**
 * @name Kafka base client handle
 * @{
 *
 */

/**
 * @brief Base handle, super class for specific clients.
 */
class RD_EXPORT Handle {
 public:
  virtual ~Handle() { }

  /** @returns the name of the handle */
  virtual const std::string name () const = 0;

  /**
   * @brief Returns the client's broker-assigned group member id
   *
   * @remark This currently requires the high-level KafkaConsumer
   *
   * @returns Last assigned member id, or empty string if not currently
   *          a group member.
   */
  virtual const std::string memberid () const = 0;


  /**
   * @brief Polls the provided kafka handle for events.
   *
   * Events will trigger application provided callbacks to be called.
   *
   * The \p timeout_ms argument specifies the maximum amount of time
   * (in milliseconds) that the call will block waiting for events.
   * For non-blocking calls, provide 0 as \p timeout_ms.
   * To wait indefinately for events, provide -1.
   *
   * Events:
   *   - delivery report callbacks (if an RdKafka::DeliveryCb is configured) [producer]
   *   - event callbacks (if an RdKafka::EventCb is configured) [producer & consumer]
   *
   * @remark  An application should make sure to call poll() at regular
   *          intervals to serve any queued callbacks waiting to be called.
   *
   * @warning This method MUST NOT be used with the RdKafka::KafkaConsumer,
   *          use its RdKafka::KafkaConsumer::consume() instead.
   *
   * @returns the number of events served.
   */
  virtual int poll (int timeout_ms) = 0;

  /**
   * @brief  Returns the current out queue length
   *
   * The out queue contains messages and requests waiting to be sent to,
   * or acknowledged by, the broker.
   */
  virtual int outq_len () = 0;

  /**
   * @brief Request Metadata from broker.
   *
   * Parameters:
   *  \p all_topics  - if non-zero: request info about all topics in cluster,
   *                   if zero: only request info about locally known topics.
   *  \p only_rkt    - only request info about this topic
   *  \p metadatap   - pointer to hold metadata result.
   *                   The \p *metadatap pointer must be released with \c delete.
   *  \p timeout_ms  - maximum response time before failing.
   *
   * @returns RdKafka::ERR_NO_ERROR on success (in which case \p *metadatap
   * will be set), else RdKafka::ERR__TIMED_OUT on timeout or
   * other error code on error.
   */
  virtual ErrorCode metadata (bool all_topics, const Topic *only_rkt,
                              Metadata **metadatap, int timeout_ms) = 0;


  /**
   * @brief Pause producing or consumption for the provided list of partitions.
   *
   * Success or error is returned per-partition in the \p partitions list.
   *
   * @returns ErrorCode::NO_ERROR
   *
   * @sa resume()
   */
  virtual ErrorCode pause (std::vector<TopicPartition*> &partitions) = 0;


  /**
   * @brief Resume producing or consumption for the provided list of partitions.
   *
   * Success or error is returned per-partition in the \p partitions list.
   *
   * @returns ErrorCode::NO_ERROR
   *
   * @sa pause()
   */
  virtual ErrorCode resume (std::vector<TopicPartition*> &partitions) = 0;


  /**
   * @brief Query broker for low (oldest/beginning)
   *        and high (newest/end) offsets for partition.
   *
   * Offsets are returned in \p *low and \p *high respectively.
   *
   * @returns RdKafka::ERR_NO_ERROR on success or an error code on failure.
   */
  virtual ErrorCode query_watermark_offsets (const std::string &topic,
					     int32_t partition,
					     int64_t *low, int64_t *high,
					     int timeout_ms) = 0;

  /**
   * @brief Get last known low (oldest/beginning)
   *        and high (newest/end) offsets for partition.
   *
   * The low offset is updated periodically (if statistics.interval.ms is set)
   * while the high offset is updated on each fetched message set from the
   * broker.
   *
   * If there is no cached offset (either low or high, or both) then
   * OFFSET_INVALID will be returned for the respective offset.
   *
   * Offsets are returned in \p *low and \p *high respectively.
   *
   * @returns RdKafka::ERR_NO_ERROR on success or an error code on failure.
   *
   * @remark Shall only be used with an active consumer instance.
   */
  virtual ErrorCode get_watermark_offsets (const std::string &topic,
					   int32_t partition,
					   int64_t *low, int64_t *high) = 0;


  /**
   * @brief Look up the offsets for the given partitions by timestamp.
   *
   * The returned offset for each partition is the earliest offset whose
   * timestamp is greater than or equal to the given timestamp in the
   * corresponding partition.
   *
   * The timestamps to query are represented as \c offset in \p offsets
   * on input, and \c offset() will return the closest earlier offset
   * for the timestamp on output.
   *
   * Timestamps are expressed as milliseconds since epoch (UTC).
   *
   * The function will block for at most \p timeout_ms milliseconds.
   *
   * @remark Duplicate Topic+Partitions are not supported.
   * @remark Errors are also returned per TopicPartition, see \c err()
   *
   * @returns an error code for general errors, else RdKafka::ERR_NO_ERROR
   *          in which case per-partition errors might be set.
   */
  virtual ErrorCode offsetsForTimes (std::vector<TopicPartition*> &offsets,
                                     int timeout_ms) = 0;


  /**
   * @brief Retrieve queue for a given partition.
   *
   * @returns The fetch queue for the given partition if successful. Else,
   *          NULL is returned.
   *
   * @remark This function only works on consumers.
   */
  virtual Queue *get_partition_queue (const TopicPartition *partition) = 0;

  /**
   * @brief Forward librdkafka logs (and debug) to the specified queue
   *        for serving with one of the ..poll() calls.
   *
   *        This allows an application to serve log callbacks (\c log_cb)
   *        in its thread of choice.
   *
   * @param queue Queue to forward logs to. If the value is NULL the logs
   *        are forwarded to the main queue.
   *
   * @remark The configuration property \c log.queue MUST also be set to true.
   *
   * @remark librdkafka maintains its own reference to the provided queue.
   *
   * @returns ERR_NO_ERROR on success or an error code on error.
   */
  virtual ErrorCode set_log_queue (Queue *queue) = 0;

  /**
   * @brief Cancels the current callback dispatcher (Handle::poll(),
   *        KafkaConsumer::consume(), etc).
   *
   * A callback may use this to force an immediate return to the calling
   * code (caller of e.g. Handle::poll()) without processing any further
   * events.
   *
   * @remark This function MUST ONLY be called from within a
   *         librdkafka callback.
   */
  virtual void yield () = 0;

  /**
   * @brief Returns the ClusterId as reported in broker metadata.
   *
   * @param timeout_ms If there is no cached value from metadata retrieval
   *                   then this specifies the maximum amount of time
   *                   (in milliseconds) the call will block waiting
   *                   for metadata to be retrieved.
   *                   Use 0 for non-blocking calls.
   *
   * @remark Requires broker version >=0.10.0 and api.version.request=true.
   *
   * @returns Last cached ClusterId, or empty string if no ClusterId could be
   *          retrieved in the allotted timespan.
   */
  virtual const std::string clusterid (int timeout_ms) = 0;

  /**
   * @brief Returns the underlying librdkafka C rd_kafka_t handle.
   *
   * @warning Calling the C API on this handle is not recommended and there
   *          is no official support for it, but for cases where the C++
   *          does not provide the proper functionality this C handle can be
   *          used to interact directly with the core librdkafka API.
   *
   * @remark The lifetime of the returned pointer is the same as the Topic
   *         object this method is called on.
   *
   * @remark Include <rdkafka/rdkafka.h> prior to including
   *         <rdkafka/rdkafkacpp.h>
   *
   * @returns \c rd_kafka_t*
   */
  virtual struct rd_kafka_s *c_ptr () = 0;

  /**
   * @brief Returns the current ControllerId (controller broker id)
   *        as reported in broker metadata.
   *
   * @param timeout_ms If there is no cached value from metadata retrieval
   *                   then this specifies the maximum amount of time
   *                   (in milliseconds) the call will block waiting
   *                   for metadata to be retrieved.
   *                   Use 0 for non-blocking calls.
   *
   * @remark Requires broker version >=0.10.0 and api.version.request=true.
   *
   * @returns Last cached ControllerId, or -1 if no ControllerId could be
   *          retrieved in the allotted timespan.
   */
  virtual int32_t controllerid (int timeout_ms) = 0;


  /**
   * @brief Returns the first fatal error set on this client instance,
   *        or ERR_NO_ERROR if no fatal error has occurred.
   *
   * This function is to be used with the Idempotent Producer and
   * the Event class for \c EVENT_ERROR events to detect fatal errors.
   *
   * Generally all errors raised by the error event are to be considered
   * informational and temporary, the client will try to recover from all
   * errors in a graceful fashion (by retrying, etc).
   *
   * However, some errors should logically be considered fatal to retain
   * consistency; in particular a set of errors that may occur when using the
   * Idempotent Producer and the in-order or exactly-once producer guarantees
   * can't be satisfied.
   *
   * @param errstr A human readable error string if a fatal error was set.
   *
   * @returns ERR_NO_ERROR if no fatal error has been raised, else
   *          any other error code.
   */
  virtual ErrorCode fatal_error (std::string &errstr) const = 0;

  /**
   * @brief Set SASL/OAUTHBEARER token and metadata
   *
   * @param token_value the mandatory token value to set, often (but not
   *  necessarily) a JWS compact serialization as per
   *  https://tools.ietf.org/html/rfc7515#section-3.1.
   * @param md_lifetime_ms when the token expires, in terms of the number of
   *  milliseconds since the epoch.
   * @param md_principal_name the Kafka principal name associated with the
   *  token.
   * @param extensions potentially empty SASL extension keys and values where
   *  element [i] is the key and [i+1] is the key's value, to be communicated
   *  to the broker as additional key-value pairs during the initial client
   *  response as per https://tools.ietf.org/html/rfc7628#section-3.1.  The
   *  number of SASL extension keys plus values must be a non-negative multiple
   *  of 2. Any provided keys and values are copied.
   * @param errstr A human readable error string is written here, only if
   *  there is an error.
   *
   * The SASL/OAUTHBEARER token refresh callback should invoke
   * this method upon success. The extension keys must not include the reserved
   * key "`auth`", and all extension keys and values must conform to the
   * required format as per https://tools.ietf.org/html/rfc7628#section-3.1:
   *
   *     key            = 1*(ALPHA)
   *     value          = *(VCHAR / SP / HTAB / CR / LF )
   *
   * @returns \c RdKafka::ERR_NO_ERROR on success, otherwise \p errstr set
   *              and:<br>
   *          \c RdKafka::ERR__INVALID_ARG if any of the arguments are
   *              invalid;<br>
   *          \c RdKafka::ERR__NOT_IMPLEMENTED if SASL/OAUTHBEARER is not
   *              supported by this build;<br>
   *          \c RdKafka::ERR__STATE if SASL/OAUTHBEARER is supported but is
   *              not configured as the client's authentication mechanism.<br>
   *
   * @sa RdKafka::oauthbearer_set_token_failure
   * @sa RdKafka::Conf::set() \c "oauthbearer_token_refresh_cb"
   */
  virtual ErrorCode oauthbearer_set_token (const std::string &token_value,
                                           int64_t md_lifetime_ms,
                                           const std::string &md_principal_name,
                                           const std::list<std::string> &extensions,
                                           std::string &errstr) = 0;

    /**
     * @brief SASL/OAUTHBEARER token refresh failure indicator.
     *
     * @param errstr human readable error reason for failing to acquire a token.
     *
     * The SASL/OAUTHBEARER token refresh callback should
     * invoke this method upon failure to refresh the token.
     *
     * @returns \c RdKafka::ERR_NO_ERROR on success, otherwise:<br>
     *          \c RdKafka::ERR__NOT_IMPLEMENTED if SASL/OAUTHBEARER is not
     *              supported by this build;<br>
     *          \c RdKafka::ERR__STATE if SASL/OAUTHBEARER is supported but is
     *              not configured as the client's authentication mechanism.
     *
     * @sa RdKafka::oauthbearer_set_token
     * @sa RdKafka::Conf::set() \c "oauthbearer_token_refresh_cb"
     */
    virtual ErrorCode oauthbearer_set_token_failure (const std::string &errstr) = 0;

   /**
     * @brief Allocate memory using the same allocator librdkafka uses.
     *
     * This is typically an abstraction for the malloc(3) call and makes sure
     * the application can use the same memory allocator as librdkafka for
     * allocating pointers that are used by librdkafka.
     *
     * @remark Memory allocated by mem_malloc() must be freed using
     *         mem_free().
     */
    virtual void *mem_malloc (size_t size) = 0;

   /**
     * @brief Free pointer returned by librdkafka
     *
     * This is typically an abstraction for the free(3) call and makes sure
     * the application can use the same memory allocator as librdkafka for
     * freeing pointers returned by librdkafka.
     *
     * In standard setups it is usually not necessary to use this interface
     * rather than the free(3) function.
     *
     * @remark mem_free() must only be used for pointers returned by APIs
     *         that explicitly mention using this function for freeing.
     */
    virtual void mem_free (void *ptr) = 0;
};


/**@}*/


/**
 * @name Topic and partition objects
 * @{
 *
 */

/**
 * @brief Topic+Partition
 *
 * This is a generic type to hold a single partition and various
 * information about it.
 *
 * Is typically used with std::vector<RdKafka::TopicPartition*> to provide
 * a list of partitions for different operations.
 */
class RD_EXPORT TopicPartition {
public:
  /**
   * @brief Create topic+partition object for \p topic and \p partition.
   *
   * Use \c delete to deconstruct.
   */
  static TopicPartition *create (const std::string &topic, int partition);

  /**
   * @brief Create topic+partition object for \p topic and \p partition
   *        with offset \p offset.
   *
   * Use \c delete to deconstruct.
   */
  static TopicPartition *create (const std::string &topic, int partition,
                                 int64_t offset);

  virtual ~TopicPartition() = 0;

  /**
   * @brief Destroy/delete the TopicPartitions in \p partitions
   *        and clear the vector.
   */
  static void destroy (std::vector<TopicPartition*> &partitions);

  /** @returns topic name */
  virtual const std::string &topic () const = 0;

  /** @returns partition id */
  virtual int partition () const = 0;

  /** @returns offset (if applicable) */
  virtual int64_t offset () const = 0;

  /** @brief Set offset */
  virtual void set_offset (int64_t offset) = 0;

  /** @returns error code (if applicable) */
  virtual ErrorCode err () const = 0;
};



/**
 * @brief Topic handle
 *
 */
class RD_EXPORT Topic {
 public:
  /**
   * @brief Unassigned partition.
   *
   * The unassigned partition is used by the producer API for messages
   * that should be partitioned using the configured or default partitioner.
   */
  static const int32_t PARTITION_UA;

  /** @brief Special offsets */
  static const int64_t OFFSET_BEGINNING; /**< Consume from beginning */
  static const int64_t OFFSET_END; /**< Consume from end */
  static const int64_t OFFSET_STORED; /**< Use offset storage */
  static const int64_t OFFSET_INVALID; /**< Invalid offset */


  /**
   * @brief Creates a new topic handle for topic named \p topic_str
   *
   * \p conf is an optional configuration for the topic  that will be used
   * instead of the default topic configuration.
   * The \p conf object is reusable after this call.
   *
   * @returns the new topic handle or NULL on error (see \p errstr).
   */
  static Topic *create (Handle *base, const std::string &topic_str,
                        const Conf *conf, std::string &errstr);

  virtual ~Topic () = 0;


  /** @returns the topic name */
  virtual const std::string name () const = 0;

  /**
   * @returns true if \p partition is available for the topic (has leader).
   * @warning \b MUST \b ONLY be called from within a
   *          RdKafka::PartitionerCb callback.
   */
  virtual bool partition_available (int32_t partition) const = 0;

  /**
   * @brief Store offset \p offset + 1 for topic partition \p partition.
   * The offset will be committed (written) to the broker (or file) according
   * to \p auto.commit.interval.ms or next manual offset-less commit call.
   *
   * @remark \c enable.auto.offset.store must be set to \c false when using
   *         this API.
   *
   * @returns RdKafka::ERR_NO_ERROR on success or an error code if none of the
   *          offsets could be stored.
   */
  virtual ErrorCode offset_store (int32_t partition, int64_t offset) = 0;

  /**
   * @brief Returns the underlying librdkafka C rd_kafka_topic_t handle.
   *
   * @warning Calling the C API on this handle is not recommended and there
   *          is no official support for it, but for cases where the C++ API
   *          does not provide the underlying functionality this C handle can be
   *          used to interact directly with the core librdkafka API.
   *
   * @remark The lifetime of the returned pointer is the same as the Topic
   *         object this method is called on.
   *
   * @remark Include <rdkafka/rdkafka.h> prior to including
   *         <rdkafka/rdkafkacpp.h>
   *
   * @returns \c rd_kafka_topic_t*
   */
  virtual struct rd_kafka_topic_s *c_ptr () = 0;
};


/**@}*/


/**
 * @name Message object
 * @{
 *
 */


/**
 * @brief Message timestamp object
 *
 * Represents the number of milliseconds since the epoch (UTC).
 *
 * The MessageTimestampType dictates the timestamp type or origin.
 *
 * @remark Requires Apache Kafka broker version >= 0.10.0
 *
 */

class RD_EXPORT MessageTimestamp {
public:
  /*! Message timestamp type */
  enum MessageTimestampType {
    MSG_TIMESTAMP_NOT_AVAILABLE,   /**< Timestamp not available */
    MSG_TIMESTAMP_CREATE_TIME,     /**< Message creation time (source) */
    MSG_TIMESTAMP_LOG_APPEND_TIME  /**< Message log append time (broker) */
  };

  MessageTimestampType type;       /**< Timestamp type */
  int64_t timestamp;               /**< Milliseconds since epoch (UTC). */
};


/**
 * @brief Headers object
 *
 * Represents message headers.
 *
 * https://cwiki.apache.org/confluence/display/KAFKA/KIP-82+-+Add+Record+Headers
 *
 * @remark Requires Apache Kafka >= 0.11.0 brokers
 */
class RD_EXPORT Headers {
public:
  virtual ~Headers() = 0;

  /**
   * @brief Header object
   *
   * This object represents a single Header with a key value pair
   * and an ErrorCode
   *
   * @remark dynamic allocation of this object is not supported.
   */
  class Header {
   public:
    /**
     * @brief Header object to encapsulate a single Header
     *
     * @param key the string value for the header key
     * @param value the bytes of the header value, or NULL
     * @param value_size the length in bytes of the header value
     *
     * @remark key and value are copied.
     *
     */
    Header(const std::string &key,
           const void *value,
           size_t value_size):
    key_(key), err_(ERR_NO_ERROR), value_size_(value_size) {
      value_ = copy_value(value, value_size);
    }

    /**
     * @brief Header object to encapsulate a single Header
     *
     * @param key the string value for the header key
     * @param value the bytes of the header value
     * @param value_size the length in bytes of the header value
     * @param err the error code if one returned
     *
     * @remark The error code is used for when the Header is constructed
     *         internally by using RdKafka::Headers::get_last which constructs
     *         a Header encapsulating the ErrorCode in the process.
     *         If err is set, the value and value_size fields will be undefined.
     */
    Header(const std::string &key,
           const void *value,
           size_t value_size,
           const RdKafka::ErrorCode err):
    key_(key), err_(err), value_(NULL), value_size_(value_size) {
      if (err == ERR_NO_ERROR)
        value_ = copy_value(value, value_size);
    }

    /**
     * @brief Copy constructor
     *
     * @param other Header to make a copy of.
     */
    Header(const Header &other):
    key_(other.key_), err_(other.err_), value_size_(other.value_size_) {
      value_ = copy_value(other.value_, value_size_);
    }

    /**
     * @brief Assignment operator
     *
     * @param other Header to make a copy of.
     */
    Header& operator=(const Header &other)
    {
      if (&other == this) {
        return *this;
      }

      key_ = other.key_;
      err_ = other.err_;
      value_size_ = other.value_size_;

      if (value_ != NULL)
        mem_free(value_);

      value_ = copy_value(other.value_, value_size_);

      return *this;
    }

    ~Header() {
      if (value_ != NULL)
        mem_free(value_);
    }

    /** @returns the key/name associated with this Header */
    std::string key() const {
      return key_;
    }

     /** @returns returns the binary value, or NULL */
    const void *value() const {
      return value_;
    }

    /** @returns returns the value casted to a nul-terminated C string,
     *           or NULL. */
    const char *value_string() const {
      return static_cast<const char *>(value_);
    }

    /** @returns Value Size the length of the Value in bytes */
    size_t value_size() const {
      return value_size_;
    }

    /** @returns the error code of this Header (usually ERR_NO_ERROR) */
    RdKafka::ErrorCode err() const {
      return err_;
    }

 private:
    char *copy_value(const void *value, size_t value_size) {
      if (!value)
        return NULL;

      char *dest = (char *)mem_malloc(value_size + 1);
      memcpy(dest, (const char *)value, value_size);
      dest[value_size] = '\0';

      return dest;
    }

    std::string key_;
    RdKafka::ErrorCode err_;
    char *value_;
    size_t value_size_;
    void *operator new(size_t); /* Prevent dynamic allocation */
  };

  /**
   * @brief Create a new instance of the Headers object
   *
   * @returns an empty Headers list
   */
  static Headers *create();

  /**
   * @brief Create a new instance of the Headers object from a std::vector
   *
   * @param headers std::vector of RdKafka::Headers::Header objects.
   *                The headers are copied, not referenced.
   *
   * @returns a Headers list from std::vector set to the size of the std::vector
   */
  static Headers *create(const std::vector<Header> &headers);

  /**
   * @brief Adds a Header to the end of the list.
   *
   * @param key header key/name
   * @param value binary value, or NULL
   * @param value_size size of the value
   *
   * @returns an ErrorCode signalling success or failure to add the header.
   */
  virtual ErrorCode add(const std::string &key, const void *value,
                        size_t value_size) = 0;

  /**
   * @brief Adds a Header to the end of the list.
   *
   * Convenience method for adding a std::string as a value for the header.
   *
   * @param key header key/name
   * @param value value string
   *
   * @returns an ErrorCode signalling success or failure to add the header.
   */
  virtual ErrorCode add(const std::string &key, const std::string &value) = 0;

  /**
   * @brief Adds a Header to the end of the list.
   *
   * This method makes a copy of the passed header.
   *
   * @param header Existing header to copy
   *
   * @returns an ErrorCode signalling success or failure to add the header.
   */
  virtual ErrorCode add(const Header &header) = 0;

  /**
   * @brief Removes all the Headers of a given key
   *
   * @param key header key/name to remove
   *
   * @returns An ErrorCode signalling a success or failure to remove the Header.
   */
  virtual ErrorCode remove(const std::string &key) = 0;

  /**
   * @brief Gets all of the Headers of a given key
   *
   * @param key header key/name
   *
   * @remark If duplicate keys exist this will return them all as a std::vector
   *
   * @returns a std::vector containing all the Headers of the given key.
   */
  virtual std::vector<Header> get(const std::string &key) const = 0;

  /**
   * @brief Gets the last occurrence of a Header of a given key
   *
   * @param key header key/name
   *
   * @remark This will only return the most recently added header
   *
   * @returns the Header if found, otherwise a Header with an err set to
   *          ERR__NOENT.
   */
  virtual Header get_last(const std::string &key) const = 0;

  /**
   * @brief Returns all Headers
   *
   * @returns a std::vector containing all of the Headers
   */
  virtual std::vector<Header> get_all() const = 0;

  /**
   * @returns the number of headers.
   */
  virtual size_t size() const = 0;
};


/**
 * @brief Message object
 *
 * This object represents either a single consumed or produced message,
 * or an event (\p err() is set).
 *
 * An application must check RdKafka::Message::err() to see if the
 * object is a proper message (error is RdKafka::ERR_NO_ERROR) or a
 * an error event.
 *
 */
class RD_EXPORT Message {
 public:
  /** @brief Message persistence status can be used by the application to
   *         find out if a produced message was persisted in the topic log. */
  enum Status {
    /** Message was never transmitted to the broker, or failed with
     *  an error indicating it was not written to the log.
     *  Application retry risks ordering, but not duplication. */
    MSG_STATUS_NOT_PERSISTED = 0,

    /** Message was transmitted to broker, but no acknowledgement was
     *  received.
     *  Application retry risks ordering and duplication. */
    MSG_STATUS_POSSIBLY_PERSISTED = 1,

    /** Message was written to the log and fully acknowledged.
     *  No reason for application to retry.
     *  Note: this value should only be trusted with \c acks=all. */
    MSG_STATUS_PERSISTED = 2,
  };

  /**
   * @brief Accessor functions*
   * @remark Not all fields are present in all types of callbacks.
   */

  /** @returns The error string if object represent an error event,
   *           else an empty string. */
  virtual std::string         errstr() const = 0;

  /** @returns The error code if object represents an error event, else 0. */
  virtual ErrorCode           err () const = 0;

  /** @returns the RdKafka::Topic object for a message (if applicable),
   *            or NULL if a corresponding RdKafka::Topic object has not been
   *            explicitly created with RdKafka::Topic::create().
   *            In this case use topic_name() instead. */
  virtual Topic              *topic () const = 0;

  /** @returns Topic name (if applicable, else empty string) */
  virtual std::string         topic_name () const = 0;

  /** @returns Partition (if applicable) */
  virtual int32_t             partition () const = 0;

  /** @returns Message payload (if applicable) */
  virtual void               *payload () const = 0 ;

  /** @returns Message payload length (if applicable) */
  virtual size_t              len () const = 0;

  /** @returns Message key as string (if applicable) */
  virtual const std::string  *key () const = 0;

  /** @returns Message key as void pointer  (if applicable) */
  virtual const void         *key_pointer () const = 0 ;

  /** @returns Message key's binary length (if applicable) */
  virtual size_t              key_len () const = 0;

  /** @returns Message or error offset (if applicable) */
  virtual int64_t             offset () const = 0;

  /** @returns Message timestamp (if applicable) */
  virtual MessageTimestamp    timestamp () const = 0;

  /** @returns The \p msg_opaque as provided to RdKafka::Producer::produce() */
  virtual void               *msg_opaque () const = 0;

  virtual ~Message () = 0;

  /** @returns the latency in microseconds for a produced message measured
   *           from the produce() call, or -1 if latency is not available. */
  virtual int64_t             latency () const = 0;

  /**
   * @brief Returns the underlying librdkafka C rd_kafka_message_t handle.
   *
   * @warning Calling the C API on this handle is not recommended and there
   *          is no official support for it, but for cases where the C++ API
   *          does not provide the underlying functionality this C handle can be
   *          used to interact directly with the core librdkafka API.
   *
   * @remark The lifetime of the returned pointer is the same as the Message
   *         object this method is called on.
   *
   * @remark Include <rdkafka/rdkafka.h> prior to including
   *         <rdkafka/rdkafkacpp.h>
   *
   * @returns \c rd_kafka_message_t*
   */
  virtual struct rd_kafka_message_s *c_ptr () = 0;

  /**
   * @brief Returns the message's persistence status in the topic log.
   */
  virtual Status status () const = 0;

  /** @returns the Headers instance for this Message, or NULL if there
   *  are no headers.
   *
   * @remark The lifetime of the Headers are the same as the Message. */
  virtual RdKafka::Headers   *headers () = 0;

  /** @returns the Headers instance for this Message (if applicable).
   *  If NULL is returned the reason is given in \p err, which
   *  is either ERR__NOENT if there were no headers, or another
   *  error code if header parsing failed.
   *
   * @remark The lifetime of the Headers are the same as the Message. */
  virtual RdKafka::Headers   *headers (RdKafka::ErrorCode *err) = 0;

  /** @returns the broker id of the broker the message was produced to or
   *           fetched from, or -1 if not known/applicable. */
  virtual int32_t broker_id () const = 0;
};

/**@}*/


/**
 * @name Queue interface
 * @{
 *
 */


/**
 * @brief Queue interface
 *
 * Create a new message queue.  Message queues allows the application
 * to re-route consumed messages from multiple topic+partitions into
 * one single queue point.  This queue point, containing messages from
 * a number of topic+partitions, may then be served by a single
 * consume() method, rather than one per topic+partition combination.
 *
 * See the RdKafka::Consumer::start(), RdKafka::Consumer::consume(), and
 * RdKafka::Consumer::consume_callback() methods that take a queue as the first
 * parameter for more information.
 */
class RD_EXPORT Queue {
 public:
  /**
   * @brief Create Queue object
   */
  static Queue *create (Handle *handle);

  /**
   * @brief Forward/re-route queue to \p dst.
   * If \p dst is \c NULL, the forwarding is removed.
   *
   * The internal refcounts for both queues are increased.
   *
   * @remark Regardless of whether \p dst is NULL or not, after calling this
   *         function, \p src will not forward it's fetch queue to the consumer
   *         queue.
   */
  virtual ErrorCode forward (Queue *dst) = 0;


  /**
   * @brief Consume message or get error event from the queue.
   *
   * @remark Use \c delete to free the message.
   *
   * @returns One of:
   *  - proper message (RdKafka::Message::err() is ERR_NO_ERROR)
   *  - error event (RdKafka::Message::err() is != ERR_NO_ERROR)
   *  - timeout due to no message or event in \p timeout_ms
   *    (RdKafka::Message::err() is ERR__TIMED_OUT)
   */
  virtual Message *consume (int timeout_ms) = 0;

  /**
   * @brief Poll queue, serving any enqueued callbacks.
   *
   * @remark Must NOT be used for queues containing messages.
   *
   * @returns the number of events served or 0 on timeout.
   */
  virtual int poll (int timeout_ms) = 0;

  virtual ~Queue () = 0;

  /**
   * @brief Enable IO event triggering for queue.
   *
   * To ease integration with IO based polling loops this API
   * allows an application to create a separate file-descriptor
   * that librdkafka will write \p payload (of size \p size) to
   * whenever a new element is enqueued on a previously empty queue.
   *
   * To remove event triggering call with \p fd = -1.
   *
   * librdkafka will maintain a copy of the \p payload.
   *
   * @remark When using forwarded queues the IO event must only be enabled
   *         on the final forwarded-to (destination) queue.
   */
  virtual void io_event_enable (int fd, const void *payload, size_t size) = 0;
};

/**@}*/

/**
 * @name ConsumerGroupMetadata
 * @{
 *
 */
/**
 * @brief ConsumerGroupMetadata holds a consumer instance's group
 *        metadata state.
 *
 * This class currently does not have any public methods.
 */
class RD_EXPORT ConsumerGroupMetadata {
public:
  virtual ~ConsumerGroupMetadata () = 0;
};

/**@}*/

/**
 * @name KafkaConsumer
 * @{
 *
 */


/**
 * @brief High-level KafkaConsumer (for brokers 0.9 and later)
 *
 * @remark Requires Apache Kafka >= 0.9.0 brokers
 *
 * Currently supports the \c range and \c roundrobin partition assignment
 * strategies (see \c partition.assignment.strategy)
 */
class RD_EXPORT KafkaConsumer : public virtual Handle {
public:
  /**
   * @brief Creates a KafkaConsumer.
   *
   * The \p conf object must have \c group.id set to the consumer group to join.
   *
   * Use RdKafka::KafkaConsumer::close() to shut down the consumer.
   *
   * @sa RdKafka::RebalanceCb
   * @sa CONFIGURATION.md for \c group.id, \c session.timeout.ms,
   *     \c partition.assignment.strategy, etc.
   */
  static KafkaConsumer *create (const Conf *conf, std::string &errstr);

  virtual ~KafkaConsumer () = 0;


  /** @brief Returns the current partition assignment as set by
   *         RdKafka::KafkaConsumer::assign() */
  virtual ErrorCode assignment (std::vector<RdKafka::TopicPartition*> &partitions) = 0;

  /** @brief Returns the current subscription as set by
   *         RdKafka::KafkaConsumer::subscribe() */
  virtual ErrorCode subscription (std::vector<std::string> &topics) = 0;

  /**
   * @brief Update the subscription set to \p topics.
   *
   * Any previous subscription will be unassigned and  unsubscribed first.
   *
   * The subscription set denotes the desired topics to consume and this
   * set is provided to the partition assignor (one of the elected group
   * members) for all clients which then uses the configured
   * \c partition.assignment.strategy to assign the subscription sets's
   * topics's partitions to the consumers, depending on their subscription.
   *
   * The result of such an assignment is a rebalancing which is either
   * handled automatically in librdkafka or can be overridden by the application
   * by providing a RdKafka::RebalanceCb.
   *
   * The rebalancing passes the assigned partition set to
   * RdKafka::KafkaConsumer::assign() to update what partitions are actually
   * being fetched by the KafkaConsumer.
   *
   * Regex pattern matching automatically performed for topics prefixed
   * with \c \"^\" (e.g. \c \"^myPfx[0-9]_.*\"
   *
   * @remark A consumer error will be raised for each unavailable topic in the
   *  \p topics. The error will be ERR_UNKNOWN_TOPIC_OR_PART
   *  for non-existent topics, and
   *  ERR_TOPIC_AUTHORIZATION_FAILED for unauthorized topics.
   *  The consumer error will be raised through consume() (et.al.)
   *  with the \c RdKafka::Message::err() returning one of the
   *  error codes mentioned above.
   *  The subscribe function itself is asynchronous and will not return
   *  an error on unavailable topics.
   *
   * @returns an error if the provided list of topics is invalid.
   */
  virtual ErrorCode subscribe (const std::vector<std::string> &topics) = 0;

  /** @brief Unsubscribe from the current subscription set. */
  virtual ErrorCode unsubscribe () = 0;

  /**
   *  @brief Update the assignment set to \p partitions.
   *
   * The assignment set is the set of partitions actually being consumed
   * by the KafkaConsumer.
   */
  virtual ErrorCode assign (const std::vector<TopicPartition*> &partitions) = 0;

  /**
   * @brief Stop consumption and remove the current assignment.
   */
  virtual ErrorCode unassign () = 0;

  /**
   * @brief Consume message or get error event, triggers callbacks.
   *
   * Will automatically call registered callbacks for any such queued events,
   * including RdKafka::RebalanceCb, RdKafka::EventCb, RdKafka::OffsetCommitCb,
   * etc.
   *
   * @remark Use \c delete to free the message.
   *
   * @remark  An application should make sure to call consume() at regular
   *          intervals, even if no messages are expected, to serve any
   *          queued callbacks waiting to be called. This is especially
   *          important when a RebalanceCb has been registered as it needs
   *          to be called and handled properly to synchronize internal
   *          consumer state.
   *
   * @remark Application MUST NOT call \p poll() on KafkaConsumer objects.
   *
   * @returns One of:
   *  - proper message (RdKafka::Message::err() is ERR_NO_ERROR)
   *  - error event (RdKafka::Message::err() is != ERR_NO_ERROR)
   *  - timeout due to no message or event in \p timeout_ms
   *    (RdKafka::Message::err() is ERR__TIMED_OUT)
   */
  virtual Message *consume (int timeout_ms) = 0;

  /**
   * @brief Commit offsets for the current assignment.
   *
   * @remark This is the synchronous variant that blocks until offsets
   *         are committed or the commit fails (see return value).
   *
   * @remark If a RdKafka::OffsetCommitCb callback is registered it will
   *         be called with commit details on a future call to
   *         RdKafka::KafkaConsumer::consume()

   *
   * @returns ERR_NO_ERROR or error code.
   */
  virtual ErrorCode commitSync () = 0;

  /**
   * @brief Asynchronous version of RdKafka::KafkaConsumer::CommitSync()
   *
   * @sa RdKafka::KafkaConsumer::commitSync()
   */
  virtual ErrorCode commitAsync () = 0;

  /**
   * @brief Commit offset for a single topic+partition based on \p message
   *
   * @remark The offset committed will be the message's offset + 1.
   *
   * @remark This is the synchronous variant.
   *
   * @sa RdKafka::KafkaConsumer::commitSync()
   */
  virtual ErrorCode commitSync (Message *message) = 0;

  /**
   * @brief Commit offset for a single topic+partition based on \p message
   *
   * @remark The offset committed will be the message's offset + 1.
   *
   * @remark This is the asynchronous variant.
   *
   * @sa RdKafka::KafkaConsumer::commitSync()
   */
  virtual ErrorCode commitAsync (Message *message) = 0;

  /**
   * @brief Commit offsets for the provided list of partitions.
   *
   * @remark The \c .offset of the partitions in \p offsets should be the
   *         offset where consumption will resume, i.e., the last
   *         processed offset + 1.
   *
   * @remark This is the synchronous variant.
   */
  virtual ErrorCode commitSync (std::vector<TopicPartition*> &offsets) = 0;

  /**
   * @brief Commit offset for the provided list of partitions.
   *
   * @remark The \c .offset of the partitions in \p offsets should be the
   *         offset where consumption will resume, i.e., the last
   *         processed offset + 1.
   *
   * @remark This is the asynchronous variant.
   */
  virtual ErrorCode commitAsync (const std::vector<TopicPartition*> &offsets) = 0;

  /**
   * @brief Commit offsets for the current assignment.
   *
   * @remark This is the synchronous variant that blocks until offsets
   *         are committed or the commit fails (see return value).
   *
   * @remark The provided callback will be called from this function.
   *
   * @returns ERR_NO_ERROR or error code.
   */
  virtual ErrorCode commitSync (OffsetCommitCb *offset_commit_cb) = 0;

  /**
   * @brief Commit offsets for the provided list of partitions.
   *
   * @remark This is the synchronous variant that blocks until offsets
   *         are committed or the commit fails (see return value).
   *
   * @remark The provided callback will be called from this function.
   *
   * @returns ERR_NO_ERROR or error code.
   */
  virtual ErrorCode commitSync (std::vector<TopicPartition*> &offsets,
                                OffsetCommitCb *offset_commit_cb) = 0;




  /**
   * @brief Retrieve committed offsets for topics+partitions.
   *
   * @returns ERR_NO_ERROR on success in which case the
   *          \p offset or \p err field of each \p partitions' element is filled
   *          in with the stored offset, or a partition specific error.
   *          Else returns an error code.
   */
  virtual ErrorCode committed (std::vector<TopicPartition*> &partitions,
			       int timeout_ms) = 0;

  /**
   * @brief Retrieve current positions (offsets) for topics+partitions.
   *
   * @returns ERR_NO_ERROR on success in which case the
   *          \p offset or \p err field of each \p partitions' element is filled
   *          in with the stored offset, or a partition specific error.
   *          Else returns an error code.
   */
  virtual ErrorCode position (std::vector<TopicPartition*> &partitions) = 0;


  /**
   * For pausing and resuming consumption, see
   * @sa RdKafka::Handle::pause() and RdKafka::Handle::resume()
   */


  /**
   * @brief Close and shut down the proper.
   *
   * This call will block until the following operations are finished:
   *  - Trigger a local rebalance to void the current assignment
   *  - Stop consumption for current assignment
   *  - Commit offsets
   *  - Leave group
   *
   * The maximum blocking time is roughly limited to session.timeout.ms.
   *
   * @remark Callbacks, such as RdKafka::RebalanceCb and
   *         RdKafka::OffsetCommitCb, etc, may be called.
   *
   * @remark The consumer object must later be freed with \c delete
   */
  virtual ErrorCode close () = 0;


  /**
   * @brief Seek consumer for topic+partition to offset which is either an
   *        absolute or logical offset.
   *
   * If \p timeout_ms is not 0 the call will wait this long for the
   * seek to be performed. If the timeout is reached the internal state
   * will be unknown and this function returns `ERR__TIMED_OUT`.
   * If \p timeout_ms is 0 it will initiate the seek but return
   * immediately without any error reporting (e.g., async).
   *
   * This call triggers a fetch queue barrier flush.
   *
   * @remark Consumtion for the given partition must have started for the
   *         seek to work. Use assign() to set the starting offset.
   *
   * @returns an ErrorCode to indicate success or failure.
   */
  virtual ErrorCode seek (const TopicPartition &partition, int timeout_ms) = 0;


  /**
   * @brief Store offset \p offset for topic partition \p partition.
   * The offset will be committed (written) to the offset store according
   * to \p auto.commit.interval.ms or the next manual offset-less commit*()
   *
   * Per-partition success/error status propagated through TopicPartition.err()
   *
   * @remark The \c .offset field is stored as is, it will NOT be + 1.
   *
   * @remark \c enable.auto.offset.store must be set to \c false when using
   *         this API.
   *
   * @returns RdKafka::ERR_NO_ERROR on success, or
   *          RdKafka::ERR___UNKNOWN_PARTITION if none of the offsets could
   *          be stored, or
   *          RdKafka::ERR___INVALID_ARG if \c enable.auto.offset.store is true.
   */
  virtual ErrorCode offsets_store (std::vector<TopicPartition*> &offsets) = 0;


  /**
   * @returns the current consumer group metadata associated with this consumer,
   *          or NULL if the consumer is configured with a \c group.id.
   *          This metadata object should be passed to the transactional
   *          producer's RdKafka::Producer::send_offsets_to_transaction() API.
   *
   * @remark The returned object must be deleted by the application.
   *
   * @sa RdKafka::Producer::send_offsets_to_transaction()
   */
  virtual ConsumerGroupMetadata *groupMetadata () = 0;


  /** @brief Check whether the consumer considers the current assignment to
   *         have been lost involuntarily. This method is only applicable for
   *         use with a subscribing consumer. Assignments are revoked
   *         immediately when determined to have been lost, so this method is
   *         only useful within a rebalance callback. Partitions that have
   *         been lost may already be owned by other members in the group and
   *         therefore commiting offsets, for example, may fail.
   *
   * @remark Calling assign(), incremental_assign() or incremental_unassign()
   *         resets this flag.
   *
   * @returns Returns true if the current partition assignment is considered
   *          lost, false otherwise.
   */
  virtual bool assignment_lost () = 0;

  /**
   * @brief The rebalance protocol currently in use. This will be
   *        "NONE" if the consumer has not (yet) joined a group, else it will
   *        match the rebalance protocol ("EAGER", "COOPERATIVE") of the
   *        configured and selected assignor(s). All configured
   *        assignors must have the same protocol type, meaning
   *        online migration of a consumer group from using one
   *        protocol to another (in particular upgading from EAGER
   *        to COOPERATIVE) without a restart is not currently
   *        supported.
   *
   * @returns an empty string on error, or one of
   *          "NONE", "EAGER", "COOPERATIVE" on success.
   */

  virtual std::string rebalance_protocol () = 0;


  /**
   * @brief Incrementally add \p partitions to the current assignment.
   *
   * If a COOPERATIVE assignor (i.e. incremental rebalancing) is being used,
   * this method should be used in a rebalance callback to adjust the current
   * assignment appropriately in the case where the rebalance type is
   * ERR__ASSIGN_PARTITIONS. The application must pass the partition list
   * passed to the callback (or a copy of it), even if the list is empty.
   * This method may also be used outside the context of a rebalance callback.
   *
   * @returns NULL on success, or an error object if the operation was
   *          unsuccessful.
   *
   * @remark The returned object must be deleted by the application.
   */
  virtual Error *incremental_assign (const std::vector<TopicPartition*> &partitions) = 0;


  /**
   * @brief Incrementally remove \p partitions from the current assignment.
   *
   * If a COOPERATIVE assignor (i.e. incremental rebalancing) is being used,
   * this method should be used in a rebalance callback to adjust the current
   * assignment appropriately in the case where the rebalance type is
   * ERR__REVOKE_PARTITIONS. The application must pass the partition list
   * passed to the callback (or a copy of it), even if the list is empty.
   * This method may also be used outside the context of a rebalance callback.
   *
   * @returns NULL on success, or an error object if the operation was
   *          unsuccessful.
   *
   * @remark The returned object must be deleted by the application.
   */
  virtual Error *incremental_unassign (const std::vector<TopicPartition*> &partitions) = 0;

};


/**@}*/


/**
 * @name Simple Consumer (legacy)
 * @{
 *
 */

/**
 * @brief Simple Consumer (legacy)
 *
 * A simple non-balanced, non-group-aware, consumer.
 */
class RD_EXPORT Consumer : public virtual Handle {
 public:
  /**
   * @brief Creates a new Kafka consumer handle.
   *
   * \p conf is an optional object that will be used instead of the default
   * configuration.
   * The \p conf object is reusable after this call.
   *
   * @returns the new handle on success or NULL on error in which case
   * \p errstr is set to a human readable error message.
   */
  static Consumer *create (const Conf *conf, std::string &errstr);

  virtual ~Consumer () = 0;


  /**
   * @brief Start consuming messages for topic and \p partition
   * at offset \p offset which may either be a proper offset (0..N)
   * or one of the the special offsets: \p OFFSET_BEGINNING or \p OFFSET_END.
   *
   * rdkafka will attempt to keep \p queued.min.messages (config property)
   * messages in the local queue by repeatedly fetching batches of messages
   * from the broker until the threshold is reached.
   *
   * The application shall use one of the \p ..->consume*() functions
   * to consume messages from the local queue, each kafka message being
   * represented as a `RdKafka::Message *` object.
   *
   * \p ..->start() must not be called multiple times for the same
   * topic and partition without stopping consumption first with
   * \p ..->stop().
   *
   * @returns an ErrorCode to indicate success or failure.
   */
  virtual ErrorCode start (Topic *topic, int32_t partition, int64_t offset) = 0;

  /**
   * @brief Start consuming messages for topic and \p partition on
   *        queue \p queue.
   *
   * @sa RdKafka::Consumer::start()
   */
  virtual ErrorCode start (Topic *topic, int32_t partition, int64_t offset,
                           Queue *queue) = 0;

  /**
   * @brief Stop consuming messages for topic and \p partition, purging
   *        all messages currently in the local queue.
   *
   * The application needs to be stop all consumers before destroying
   * the Consumer handle.
   *
   * @returns an ErrorCode to indicate success or failure.
   */
  virtual ErrorCode stop (Topic *topic, int32_t partition) = 0;

  /**
   * @brief Seek consumer for topic+partition to \p offset which is either an
   *        absolute or logical offset.
   *
   * If \p timeout_ms is not 0 the call will wait this long for the
   * seek to be performed. If the timeout is reached the internal state
   * will be unknown and this function returns `ERR__TIMED_OUT`.
   * If \p timeout_ms is 0 it will initiate the seek but return
   * immediately without any error reporting (e.g., async).
   *
   * This call triggers a fetch queue barrier flush.
   *
   * @returns an ErrorCode to indicate success or failure.
   */
  virtual ErrorCode seek (Topic *topic, int32_t partition, int64_t offset,
			  int timeout_ms) = 0;

  /**
   * @brief Consume a single message from \p topic and \p partition.
   *
   * \p timeout_ms is maximum amount of time to wait for a message to be
   * received.
   * Consumer must have been previously started with \p ..->start().
   *
   * @returns a Message object, the application needs to check if message
   * is an error or a proper message RdKafka::Message::err() and checking for
   * \p ERR_NO_ERROR.
   *
   * The message object must be destroyed when the application is done with it.
   *
   * Errors (in RdKafka::Message::err()):
   *  - ERR__TIMED_OUT - \p timeout_ms was reached with no new messages fetched.
   *  - ERR__PARTITION_EOF - End of partition reached, not an error.
   */
  virtual Message *consume (Topic *topic, int32_t partition,
                            int timeout_ms) = 0;

  /**
   * @brief Consume a single message from the specified queue.
   *
   * \p timeout_ms is maximum amount of time to wait for a message to be
   * received.
   * Consumer must have been previously started on the queue with
   * \p ..->start().
   *
   * @returns a Message object, the application needs to check if message
   * is an error or a proper message \p Message->err() and checking for
   * \p ERR_NO_ERROR.
   *
   * The message object must be destroyed when the application is done with it.
   *
   * Errors (in RdKafka::Message::err()):
   *   - ERR__TIMED_OUT - \p timeout_ms was reached with no new messages fetched
   *
   * Note that Message->topic() may be nullptr after certain kinds of
   * errors, so applications should check that it isn't null before
   * dereferencing it.
   */
  virtual Message *consume (Queue *queue, int timeout_ms) = 0;

  /**
   * @brief Consumes messages from \p topic and \p partition, calling
   *        the provided callback for each consumed messsage.
   *
   * \p consume_callback() provides higher throughput performance
   * than \p consume().
   *
   * \p timeout_ms is the maximum amount of time to wait for one or
   * more messages to arrive.
   *
   * The provided \p consume_cb instance has its \p consume_cb function
   * called for every message received.
   *
   * The \p opaque argument is passed to the \p consume_cb as \p opaque.
   *
   * @returns the number of messages processed or -1 on error.
   *
   * @sa RdKafka::Consumer::consume()
   */
  virtual int consume_callback (Topic *topic, int32_t partition,
                                int timeout_ms,
                                ConsumeCb *consume_cb,
                                void *opaque) = 0;

  /**
   * @brief Consumes messages from \p queue, calling the provided callback for
   *        each consumed messsage.
   *
   * @sa RdKafka::Consumer::consume_callback()
   */
  virtual int consume_callback (Queue *queue, int timeout_ms,
                                RdKafka::ConsumeCb *consume_cb,
                                void *opaque) = 0;

  /**
   * @brief Converts an offset into the logical offset from the tail of a topic.
   *
   * \p offset is the (positive) number of items from the end.
   *
   * @returns the logical offset for message \p offset from the tail, this value
   *          may be passed to Consumer::start, et.al.
   * @remark The returned logical offset is specific to librdkafka.
   */
  static int64_t OffsetTail(int64_t offset);
};

/**@}*/


/**
 * @name Producer
 * @{
 *
 */


/**
 * @brief Producer
 */
class RD_EXPORT Producer : public virtual Handle {
 public:
  /**
   * @brief Creates a new Kafka producer handle.
   *
   * \p conf is an optional object that will be used instead of the default
   * configuration.
   * The \p conf object is reusable after this call.
   *
   * @returns the new handle on success or NULL on error in which case
   *          \p errstr is set to a human readable error message.
   */
  static Producer *create (const Conf *conf, std::string &errstr);


  virtual ~Producer () = 0;

  /**
   * @brief RdKafka::Producer::produce() \p msgflags
   *
   * These flags are optional.
   */
  enum {
    RK_MSG_FREE = 0x1, /**< rdkafka will free(3) \p payload
                         * when it is done with it.
                         * Mutually exclusive with RK_MSG_COPY. */
    RK_MSG_COPY = 0x2, /**< the \p payload data will be copied
                        * and the \p payload pointer will not
                        * be used by rdkafka after the
                        * call returns.
                        * Mutually exclusive with RK_MSG_FREE. */
    RK_MSG_BLOCK = 0x4  /**< Block produce*() on message queue
                         *   full.
                         *   WARNING:
                         *   If a delivery report callback
                         *   is used the application MUST
                         *   call rd_kafka_poll() (or equiv.)
                         *   to make sure delivered messages
                         *   are drained from the internal
                         *   delivery report queue.
                         *   Failure to do so will result
                         *   in indefinately blocking on
                         *   the produce() call when the
                         *   message queue is full.
                         */


  /**@cond NO_DOC*/
  /* For backwards compatibility: */
#ifndef MSG_COPY /* defined in sys/msg.h */
    , /** this comma must exist betwen
       *  RK_MSG_BLOCK and MSG_FREE
       */
    MSG_FREE = RK_MSG_FREE,
    MSG_COPY = RK_MSG_COPY
#endif
  /**@endcond*/
  };

  /**
   * @brief Produce and send a single message to broker.
   *
   * This is an asynch non-blocking API.
   *
   * \p partition is the target partition, either:
   *   - RdKafka::Topic::PARTITION_UA (unassigned) for
   *     automatic partitioning using the topic's partitioner function, or
   *   - a fixed partition (0..N)
   *
   * \p msgflags is zero or more of the following flags OR:ed together:
   *    RK_MSG_BLOCK - block \p produce*() call if
   *                   \p queue.buffering.max.messages or
   *                   \p queue.buffering.max.kbytes are exceeded.
   *                   Messages are considered in-queue from the point they
   *                   are accepted by produce() until their corresponding
   *                   delivery report callback/event returns.
   *                   It is thus a requirement to call
   *                   poll() (or equiv.) from a separate
   *                   thread when RK_MSG_BLOCK is used.
   *                   See WARNING on \c RK_MSG_BLOCK above.
   *    RK_MSG_FREE - rdkafka will free(3) \p payload when it is done with it.
   *    RK_MSG_COPY - the \p payload data will be copied and the \p payload
   *               pointer will not be used by rdkafka after the
   *               call returns.
   *
   *  NOTE: RK_MSG_FREE and RK_MSG_COPY are mutually exclusive.
   *
   *  If the function returns an error code and RK_MSG_FREE was specified, then
   *  the memory associated with the payload is still the caller's
   *  responsibility.
   *
   * \p payload is the message payload of size \p len bytes.
   *
   * \p key is an optional message key, if non-NULL it
   * will be passed to the topic partitioner as well as be sent with the
   * message to the broker and passed on to the consumer.
   *
   * \p msg_opaque is an optional application-provided per-message opaque
   * pointer that will provided in the delivery report callback (\p dr_cb) for
   * referencing this message.
   *
   * @returns an ErrorCode to indicate success or failure:
   *  - ERR_NO_ERROR           - message successfully enqueued for transmission.
   *
   *  - ERR__QUEUE_FULL        - maximum number of outstanding messages has been
   *                             reached: \c queue.buffering.max.message
   *
   *  - ERR_MSG_SIZE_TOO_LARGE - message is larger than configured max size:
   *                            \c messages.max.bytes
   *
   *  - ERR__UNKNOWN_PARTITION - requested \p partition is unknown in the
   *                           Kafka cluster.
   *
   *  - ERR__UNKNOWN_TOPIC     - topic is unknown in the Kafka cluster.
   */
  virtual ErrorCode produce (Topic *topic, int32_t partition,
                             int msgflags,
                             void *payload, size_t len,
                             const std::string *key,
                             void *msg_opaque) = 0;

  /**
   * @brief Variant produce() that passes the key as a pointer and length
   *        instead of as a const std::string *.
   */
  virtual ErrorCode produce (Topic *topic, int32_t partition,
                             int msgflags,
                             void *payload, size_t len,
                             const void *key, size_t key_len,
                             void *msg_opaque) = 0;

  /**
   * @brief produce() variant that takes topic as a string (no need for
   *        creating a Topic object), and also allows providing the
   *        message timestamp (milliseconds since beginning of epoch, UTC).
   *        Otherwise identical to produce() above.
   */
  virtual ErrorCode produce (const std::string topic_name, int32_t partition,
                             int msgflags,
                             void *payload, size_t len,
                             const void *key, size_t key_len,
                             int64_t timestamp, void *msg_opaque) = 0;

  /**
   * @brief produce() variant that that allows for Header support on produce
   *        Otherwise identical to produce() above.
   *
   * @warning The \p headers will be freed/deleted if the produce() call
   *          succeeds, or left untouched if produce() fails.
   */
  virtual ErrorCode produce (const std::string topic_name, int32_t partition,
                             int msgflags,
                             void *payload, size_t len,
                             const void *key, size_t key_len,
                             int64_t timestamp,
                             RdKafka::Headers *headers,
                             void *msg_opaque) = 0;


  /**
   * @brief Variant produce() that accepts vectors for key and payload.
   *        The vector data will be copied.
   */
  virtual ErrorCode produce (Topic *topic, int32_t partition,
                             const std::vector<char> *payload,
                             const std::vector<char> *key,
                             void *msg_opaque) = 0;


  /**
   * @brief Wait until all outstanding produce requests, et.al, are completed.
   *        This should typically be done prior to destroying a producer instance
   *        to make sure all queued and in-flight produce requests are completed
   *        before terminating.
   *
   * @remark The \c linger.ms time will be ignored for the duration of the call,
   *         queued messages will be sent to the broker as soon as possible.
   *
   * @remark This function will call Producer::poll() and thus
   *         trigger callbacks.
   *
   * @returns ERR__TIMED_OUT if \p timeout_ms was reached before all
   *          outstanding requests were completed, else ERR_NO_ERROR
   */
  virtual ErrorCode flush (int timeout_ms) = 0;


  /**
   * @brief Purge messages currently handled by the producer instance.
   *
   * @param purge_flags tells which messages should be purged and how.
   *
   * The application will need to call Handle::poll() or Producer::flush()
   * afterwards to serve the delivery report callbacks of the purged messages.
   *
   * Messages purged from internal queues fail with the delivery report
   * error code set to ERR__PURGE_QUEUE, while purged messages that
   * are in-flight to or from the broker will fail with the error code set to
   * ERR__PURGE_INFLIGHT.
   *
   * @warning Purging messages that are in-flight to or from the broker
   *          will ignore any sub-sequent acknowledgement for these messages
   *          received from the broker, effectively making it impossible
   *          for the application to know if the messages were successfully
   *          produced or not. This may result in duplicate messages if the
   *          application retries these messages at a later time.
   *
   * @remark This call may block for a short time while background thread
   *         queues are purged.
   *
   * @returns ERR_NO_ERROR on success,
   *          ERR__INVALID_ARG if the \p purge flags are invalid or unknown,
   *          ERR__NOT_IMPLEMENTED if called on a non-producer client instance.
   */
  virtual ErrorCode purge (int purge_flags) = 0;

  /**
   * @brief RdKafka::Handle::purge() \p purge_flags
   */
  enum {
    PURGE_QUEUE = 0x1, /**< Purge messages in internal queues */

    PURGE_INFLIGHT = 0x2, /*! Purge messages in-flight to or from the broker.
                           *  Purging these messages will void any future
                           *  acknowledgements from the broker, making it
                           *  impossible for the application to know if these
                           *  messages were successfully delivered or not.
                           *  Retrying these messages may lead to duplicates. */

    PURGE_NON_BLOCKING = 0x4 /* Don't wait for background queue
                              * purging to finish. */
  };

  /**
   * @name Transactional API
   * @{
   *
   * Requires Kafka broker version v0.11.0 or later
   *
   * See the Transactional API documentation in rdkafka.h for more information.
   */

  /**
   * @brief Initialize transactions for the producer instance.
   *
   * @param timeout_ms The maximum time to block. On timeout the operation
   *                   may continue in the background, depending on state,
   *                   and it is okay to call init_transactions() again.
   *
   * @returns an RdKafka::Error object on error, or NULL on success.
   *          Check whether the returned error object permits retrying
   *          by calling RdKafka::Error::is_retriable(), or whether a fatal
   *          error has been raised by calling RdKafka::Error::is_fatal().
   *
   * @remark The returned error object (if not NULL) must be deleted.
   *
   * See rd_kafka_init_transactions() in rdkafka.h for more information.
   *
   */
  virtual Error *init_transactions (int timeout_ms) = 0;


  /**
   * @brief init_transactions() must have been called successfully
   *        (once) before this function is called.
   *
   * @returns an RdKafka::Error object on error, or NULL on success.
   *          Check whether a fatal error has been raised by calling
   *          RdKafka::Error::is_fatal_error().
   *
   * @remark The returned error object (if not NULL) must be deleted.
   *
   * See rd_kafka_begin_transaction() in rdkafka.h for more information.
   */
  virtual Error *begin_transaction () = 0;

  /**
   * @brief Sends a list of topic partition offsets to the consumer group
   *        coordinator for \p group_metadata, and marks the offsets as part
   *        part of the current transaction.
   *        These offsets will be considered committed only if the transaction
   *        is committed successfully.
   *
   *        The offsets should be the next message your application will
   *        consume,
   *        i.e., the last processed message's offset + 1 for each partition.
   *        Either track the offsets manually during processing or use
   *        RdKafka::KafkaConsumer::position() (on the consumer) to get the
   *        current offsets for
   *        the partitions assigned to the consumer.
   *
   *        Use this method at the end of a consume-transform-produce loop prior
   *        to committing the transaction with commit_transaction().
   *
   * @param offsets List of offsets to commit to the consumer group upon
   *                successful commit of the transaction. Offsets should be
   *                the next message to consume,
   *                e.g., last processed message + 1.
   * @param group_metadata The current consumer group metadata as returned by
   *                   RdKafka::KafkaConsumer::groupMetadata() on the consumer
   *                   instance the provided offsets were consumed from.
   * @param timeout_ms Maximum time allowed to register the
   *                   offsets on the broker.
   *
   * @remark This function must be called on the transactional producer
   *         instance, not the consumer.
   *
   * @remark The consumer must disable auto commits
   *         (set \c enable.auto.commit to false on the consumer).
   *
   * @returns an RdKafka::Error object on error, or NULL on success.
   *          Check whether the returned error object permits retrying
   *          by calling RdKafka::Error::is_retriable(), or whether an abortable
   *          or fatal error has been raised by calling
   *          RdKafka::Error::txn_requires_abort() or RdKafka::Error::is_fatal()
   *          respectively.
   *
   * @remark The returned error object (if not NULL) must be deleted.
   *
   * See rd_kafka_send_offsets_to_transaction() in rdkafka.h for
   * more information.
   */
  virtual Error *send_offsets_to_transaction (
          const std::vector<TopicPartition*> &offsets,
          const ConsumerGroupMetadata *group_metadata,
          int timeout_ms) = 0;

  /**
   * @brief Commit the current transaction as started with begin_transaction().
   *
   *        Any outstanding messages will be flushed (delivered) before actually
   *        committing the transaction.
   *
   * @param timeout_ms The maximum time to block. On timeout the operation
   *                   may continue in the background, depending on state,
   *                   and it is okay to call this function again.
   *                   Pass -1 to use the remaining transaction timeout,
   *                   this is the recommended use.
   *
   * @remark It is strongly recommended to always pass -1 (remaining transaction
   *         time) as the \p timeout_ms. Using other values risk internal
   *         state desynchronization in case any of the underlying protocol
   *         requests fail.
   *
   * @returns an RdKafka::Error object on error, or NULL on success.
   *          Check whether the returned error object permits retrying
   *          by calling RdKafka::Error::is_retriable(), or whether an abortable
   *          or fatal error has been raised by calling
   *          RdKafka::Error::txn_requires_abort() or RdKafka::Error::is_fatal()
   *          respectively.
   *
   * @remark The returned error object (if not NULL) must be deleted.
   *
   * See rd_kafka_commit_transaction() in rdkafka.h for more information.
   */
  virtual Error *commit_transaction (int timeout_ms) = 0;

  /**
   * @brief Aborts the ongoing transaction.
   *
   *        This function should also be used to recover from non-fatal abortable
   *        transaction errors.
   *
   *        Any outstanding messages will be purged and fail with
   *        RdKafka::ERR__PURGE_INFLIGHT or RdKafka::ERR__PURGE_QUEUE.
   *        See RdKafka::Producer::purge() for details.
   *
   * @param timeout_ms The maximum time to block. On timeout the operation
   *                   may continue in the background, depending on state,
   *                   and it is okay to call this function again.
   *                   Pass -1 to use the remaining transaction timeout,
   *                   this is the recommended use.
   *
   * @remark It is strongly recommended to always pass -1 (remaining transaction
   *         time) as the \p timeout_ms. Using other values risk internal
   *         state desynchronization in case any of the underlying protocol
   *         requests fail.
   *
   * @returns an RdKafka::Error object on error, or NULL on success.
   *          Check whether the returned error object permits retrying
   *          by calling RdKafka::Error::is_retriable(), or whether a
   *          fatal error has been raised by calling RdKafka::Error::is_fatal().
   *
   * @remark The returned error object (if not NULL) must be deleted.
   *
   * See rd_kafka_abort_transaction() in rdkafka.h for more information.
   */
  virtual Error *abort_transaction (int timeout_ms) = 0;

  /**@}*/
};

/**@}*/


/**
 * @name Metadata interface
 * @{
 *
 */


/**
 * @brief Metadata: Broker information
 */
class BrokerMetadata {
 public:
  /** @returns Broker id */
  virtual int32_t id() const = 0;

  /** @returns Broker hostname */
  virtual const std::string host() const = 0;

  /** @returns Broker listening port */
  virtual int port() const = 0;

  virtual ~BrokerMetadata() = 0;
};



/**
 * @brief Metadata: Partition information
 */
class PartitionMetadata {
 public:
  /** @brief Replicas */
  typedef std::vector<int32_t> ReplicasVector;
  /** @brief ISRs (In-Sync-Replicas) */
  typedef std::vector<int32_t> ISRSVector;

  /** @brief Replicas iterator */
  typedef ReplicasVector::const_iterator ReplicasIterator;
  /** @brief ISRs iterator */
  typedef ISRSVector::const_iterator     ISRSIterator;


  /** @returns Partition id */
  virtual int32_t id() const = 0;

  /** @returns Partition error reported by broker */
  virtual ErrorCode err() const = 0;

  /** @returns Leader broker (id) for partition */
  virtual int32_t leader() const = 0;

  /** @returns Replica brokers */
  virtual const std::vector<int32_t> *replicas() const = 0;

  /** @returns In-Sync-Replica brokers
   *  @warning The broker may return a cached/outdated list of ISRs.
   */
  virtual const std::vector<int32_t> *isrs() const = 0;

  virtual ~PartitionMetadata() = 0;
};



/**
 * @brief Metadata: Topic information
 */
class TopicMetadata {
 public:
  /** @brief Partitions */
  typedef std::vector<const PartitionMetadata*> PartitionMetadataVector;
  /** @brief Partitions iterator */
  typedef PartitionMetadataVector::const_iterator PartitionMetadataIterator;

  /** @returns Topic name */
  virtual const std::string topic() const = 0;

  /** @returns Partition list */
  virtual const PartitionMetadataVector *partitions() const = 0;

  /** @returns Topic error reported by broker */
  virtual ErrorCode err() const = 0;

  virtual ~TopicMetadata() = 0;
};


/**
 * @brief Metadata container
 */
class Metadata {
 public:
  /** @brief Brokers */
  typedef std::vector<const BrokerMetadata*> BrokerMetadataVector;
  /** @brief Topics */
  typedef std::vector<const TopicMetadata*>  TopicMetadataVector;

  /** @brief Brokers iterator */
  typedef BrokerMetadataVector::const_iterator BrokerMetadataIterator;
  /** @brief Topics iterator */
  typedef TopicMetadataVector::const_iterator  TopicMetadataIterator;


  /**
   * @brief Broker list
   * @remark Ownership of the returned pointer is retained by the instance of
   * Metadata that is called.
   */
  virtual const BrokerMetadataVector *brokers() const = 0;

  /**
   * @brief Topic list
   * @remark Ownership of the returned pointer is retained by the instance of
   * Metadata that is called.
   */
  virtual const TopicMetadataVector  *topics() const = 0;

  /** @brief Broker (id) originating this metadata */
  virtual int32_t orig_broker_id() const = 0;

  /** @brief Broker (name) originating this metadata */
  virtual const std::string orig_broker_name() const = 0;

  virtual ~Metadata() = 0;
};

/**@}*/

}


#endif /* _RDKAFKACPP_H_ */

```

`IntelPTDriver/packages/librdkafka.redist.1.8.2/build/native/librdkafka.redist.targets`:

```targets
<Project ToolsVersion="4.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
  <ItemDefinitionGroup>
    <Link>
      <AdditionalDependencies Condition="'$(Platform)' == 'x64'">$(MSBuildThisFileDirectory)lib\win\x64\win-x64-Release\v140\librdkafka.lib;%(AdditionalDependencies)</AdditionalDependencies>
      <AdditionalDependencies Condition="'$(Platform)' != 'x64'">$(MSBuildThisFileDirectory)lib\win\x86\win-x86-Release\v140\librdkafka.lib;%(AdditionalDependencies)</AdditionalDependencies>
      <AdditionalLibraryDirectories Condition="'$(Platform)' == 'x64'">$(MSBuildThisFileDirectory)lib\win\x64\win-x64-Release\v140;%(AdditionalLibraryDirectories)</AdditionalLibraryDirectories>
      <AdditionalLibraryDirectories Condition="'$(Platform)' != 'x64'">$(MSBuildThisFileDirectory)lib\win\x86\win-x86-Release\v140;%(AdditionalLibraryDirectories)</AdditionalLibraryDirectories>
    </Link>
    <ClCompile>
      <AdditionalIncludeDirectories>$(MSBuildThisFileDirectory)include;%(AdditionalIncludeDirectories)</AdditionalIncludeDirectories>
    </ClCompile>
  </ItemDefinitionGroup>
  <ItemGroup Condition="'$(Platform)' == 'x64'">
    <ReferenceCopyLocalPaths Include="$(MSBuildThisFileDirectory)..\..\runtimes\win-x64\native\*.dll" />
  </ItemGroup>
  <ItemGroup Condition="'$(Platform)' != 'x64'">
    <ReferenceCopyLocalPaths Include="$(MSBuildThisFileDirectory)..\..\runtimes\win-x86\native\*.dll" />
  </ItemGroup>
</Project>

```

`README.md`:

```md
# ingsoc
```