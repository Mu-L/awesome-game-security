Project Path: arc_cellebrite-labs_LabSync_ehs38moy

Source Tree:

```txt
arc_cellebrite-labs_LabSync_ehs38moy
├── LICENSE.txt
├── README.md
├── example
│   ├── conflict.gif
│   ├── f396862963d4f7ec5f518b09bee77115.yaml
│   └── gzip-O3-moutline
├── labsync.cfg
├── labsync.py
├── requirements.txt
├── ruff.toml
└── test_labsync.py

```

`LICENSE.txt`:

```txt
                    GNU GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <http://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU General Public License is a free, copyleft license for
software and other kinds of works.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
the GNU General Public License is intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.  We, the Free Software Foundation, use the
GNU General Public License for most of our software; it applies also to
any other work released this way by its authors.  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  To protect your rights, we need to prevent others from denying you
these rights or asking you to surrender the rights.  Therefore, you have
certain responsibilities if you distribute copies of the software, or if
you modify it: responsibilities to respect the freedom of others.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must pass on to the recipients the same
freedoms that you received.  You must make sure that they, too, receive
or can get the source code.  And you must show them these terms so they
know their rights.

  Developers that use the GNU GPL protect your rights with two steps:
(1) assert copyright on the software, and (2) offer you this License
giving you legal permission to copy, distribute and/or modify it.

  For the developers' and authors' protection, the GPL clearly explains
that there is no warranty for this free software.  For both users' and
authors' sake, the GPL requires that modified versions be marked as
changed, so that their problems will not be attributed erroneously to
authors of previous versions.

  Some devices are designed to deny users access to install or run
modified versions of the software inside them, although the manufacturer
can do so.  This is fundamentally incompatible with the aim of
protecting users' freedom to change the software.  The systematic
pattern of such abuse occurs in the area of products for individuals to
use, which is precisely where it is most unacceptable.  Therefore, we
have designed this version of the GPL to prohibit the practice for those
products.  If such problems arise substantially in other domains, we
stand ready to extend this provision to those domains in future versions
of the GPL, as needed to protect the freedom of users.

  Finally, every program is threatened constantly by software patents.
States should not allow patents to restrict development and use of
software on general-purpose computers, but in those that do, we wish to
avoid the special danger that patents applied to a free program could
make it effectively proprietary.  To prevent this, the GPL assures that
patents cannot be used to render the program non-free.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Use with the GNU Affero General Public License.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU Affero General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the special requirements of the GNU Affero General Public License,
section 13, concerning interaction through a network will apply to the
combination as such.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If the program does terminal interaction, make it output a short
notice like this when it starts in an interactive mode:

    <program>  Copyright (C) <year>  <name of author>
    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, your program's commands
might be different; for a GUI interface, you would use an "about box".

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU GPL, see
<http://www.gnu.org/licenses/>.

  The GNU General Public License does not permit incorporating your program
into proprietary programs.  If your program is a subroutine library, you
may consider it more useful to permit linking proprietary applications with
the library.  If this is what you want to do, use the GNU Lesser General
Public License instead of this License.  But first, please read
<http://www.gnu.org/philosophy/why-not-lgpl.html>.

```

`README.md`:

```md
# LabSync

LabSync is an IDA plugin that can be used to partially synchronize IDBs between different users
working on reversing the same binaries.

LabSync is intended to be non-intrusive, lightweight, and easy to use for very frequent syncs
(think as frequently and easily as you saved your IDB before Undo was a thing).

The leading use case is multiple people reversing the same binary at the same time, and especially
for binaries that don't start from a good "baseline" IDB (i.e. no typing information, non-standard
formats or architectures), and whose structure keeps changing during the reversing process.

## How it works

When enabled, whenever the IDB is saved, LabSync will synchronize some of its data with other
reverse engineers using a shared git repo.

LabSync generates a YAML from some of the data stored in your IDB (e.g. names, types, inlined
functions, etc.), and when you save the IDB this YAML is also committed to the shared git repo. The
repo is then pulled and pushed to sync with others.

In case there were any remote changes since the last time you saved your IDB, they will be fetched
during the git pull operation and merged into your local changes.

In case of any merge conflicts, git mergetool will automatically be started for you to resolve them
in a convenient textual format, and the sync process will finish afterwards.

The YAML for each IDB is saved under the MD5 of the input file, so LabSync is able to sync it with
other reverse engineers no matter their local IDB filename, as long as it originated from the same
binary.

## Example

We generated an example YAML for the `gzip-O3-moutline` example from the FunctionInliner repo
([source](https://github.com/cellebrite-labs/FunctionInliner/tree/master/test) /
[binary](example/gzip-O3-moutline)) after inlining all outlined
functions, and you can take a look at it [here](example/f396862963d4f7ec5f518b09bee77115.yaml).

An example of conflict resolution can be seen in the following animation (we change some function
name and sync, while it was also changed in upstream since our last change):

![](example/conflict.gif)

## Motivation

In Cellebrite Labs we often have multiple researchers working at the same time on reversing huge
binaries with no symbols or typing information. The structure of some of these also undergos
drastic changes during the reversing process (e.g. when
[FunctionInliner](https://github.com/cellebrite-labs/FunctionInliner) is used).

After reviewing past and existing IDA synchronization solutions, we failed to find a solution that
satisfied the following requirements:

1. Syncing should be very fast and non-intrusive -- either live or as quick and easy as a keypress.
2. In case of non-live synchronization or live-synchronization supporting offline work
   (i.e. where conflicts can occur), we want conflict resolution to be clear, intuitive and
   non-intrusive.
   1. Require the least amount of user interaction
   2. Be "easy" for a user unfamiliar with IDA/the solution internals
   3. Conflict resolution should not be a "must" when you just want to open the IDB to check
      something
3. [FunctionInliner](https://github.com/cellebrite-labs/FunctionInliner) should be a "first class
   citizen"
   1. IDA's [built-in outlined function support](https://hex-rays.com/blog/igors-tip-of-the-week-106-outlined-functions)
      (which is trivial to sync) is not sufficient for "deep" reversing, for example because xrefs
      from outlined chunks to functions/data are not propagated the "parent" functions, and because
      it makes static analysis of disassembly practically impossible.
   2. Non-live syncing of FunctionInliner's effects on the IDB are practically impossible without
      the solution being "FunctionInliner-aware" (i.e. because if two users inline different
      functions before syncing, the "clone" segments will most likely collide on the same unused
      EAs).

We tried to develop a solution based on the Pareto principle -- support the minimum amount of
features that will give the maximum amount of assistance to shared reversing work. We also
chose to dismiss syncing of features that, even if very helpful, would require a lot of
maintenance, or would require going down deep rabbit holes of edge cases. For example, we chose to
*not* sync decompiler comments, because we suspected that it will require exact-same decompilation
for all users which is impossible in the general case unless perfect synchronization is achieved
for a lot of other features).

We chose to use YAML files to store the data because they're easily editable by humans that will
need to understand what's going on and resolve potential conflicts.

We chose to use git as a backend because it's proven to work well for storing text files,
handling merges and conflicts and everyone already knows how to use it (or should :) ).

## Installing

1. Install the dependencies listed in `requirements.txt` where IDA can import them. For example
   using `/path/to/python3/used/by/ida -m pip install -r requirements.txt`.
2. Optionally install [FunctionInliner](https://github.com/cellebrite-labs/FunctionInliner).
3. Clone this repository and symlink `~/.idapro/plugins/labsync.py` to `labsync.py` in the cloned
   repo.
4. Create a new (empty) git repository that will be used for the synchronization data. This repo
   should be cloned by all of the users that will share their work.
5. Follow the next section on how to configure LabSync.

## Configuring

1. LabSync expects to find its configuration file under `~/.idapro/cfg/labsync.cfg`
2. Copy the example `labsync.cfg` from this repository and change `repo_path` to point to the path
   where your local clone of the *data* repository is (i.e. the one from step 4 above).
3. Make sure that you have `merge.tool` configured in your git configuration (either globally or
   locally for the *data* repo).
   1. You can check what it's globally configured to with `git config --global merge.tool`
   2. In case the above is empty, you should configure it e.g. with
      `git config --global merge.tool opendiff`
   3. You can test your configured `merge.tool` using
      [this repo](https://github.com/redguardtoo/test-git-mergetool).

## Usage

To start synchronizing the current IDB to LabSync, use `Edit > Plugins > LabSync > Enable`.

Afterwards -- just save your IDB regularly in order to synchronize to the repository.

NOTE: when resolving merge conflicts for names/prototypes, make sure that you verify the EAs of the
conflicting chunks you're comparing. Git's merge strategy compares the files line-by-line and
isn't aware of YAML's syntax, so it may create a conflict when two different adjacent keys have
been added to a dictionary such as `names` or `prototypes`.

## What does it sync?

We currently only sync:

1. Names given to EAs (e.g. functions, globals)
2. EAs of functions that have been inlined with functioninliner
3. Local types (i.e. structures, unions, enums)
4. Function prototypes

## Advanced features

### Branching out

You can checkout a different branch on the data repository and push it to the remote data
repository, and then your changes will sync only with people that are using the same branch,
and won't affect everyone else.

Unless you intend to merge back into the main branch eventually, it's recommended to keep a copy
of your IDB before you do the above so it'll be easy to revert once done.

### Local type tracking

LabSync generates a UUID for each local type and documents it in a comment above the local type
definition written in the YAML. This is used to track renames of local types, so that we don't
delete and re-create a local type in case it was renamed remotely (since then all references to it
will be destroyed).

When first enabling sync on an IDB whose binary has already been synced in the past, LabSync will
try to "adopt" the UUIDs saved in the repo for local types that have the same names as those saved
in the repo.

Note that this is a best-effort heuristical approach to reduce the amount of "duplicate" local
types that will then have to be replaced manually. However, this heuristic might not be complete,
and/or may also lead to false positives (e.g. a logically-different local type with the same name
as one saved in the repo will be assigned its UUID).

Please take the above into consideration when reviewing the initial commit merge, and replace
false-positive UUIDs with new random ones.

### Resetting synchronization

In order to reset the synchronization state of an IDB, you should first disable the synchronization
using `Edit > Plugins > LabSync > Disable` and then use `Edit > Plugins > LabSync > Reset`.

If LabSync will be enabled afterwards, it'll treat the IDB as "new" when syncing it with the repo,
and in case it won't be deleted from git, it'll be merged with the existing repo data.

### Mapping segments to a different data file

In some cases it's useful to reverse a binary together with a software library that it uses in the
same IDB. In order to support syncing features related to the library between the IDBs of different
binaries that use it, you can ask LabSync to sync some segments to separate files.

Note that all of the local types will be synced to all of the associated files, so in case of a
conflict involving local types, you may have to resolve it more than once during conflict
resolution.

This functionality is mostly intended to be used by IDA loaders, so we don't expose it using UI,
but rather using the following API:

```python
from labsync import LabSyncPlugin
LabSyncPlugin.map_segments_to_idb_id(seg_prefix, idb_id)
```

Where `seg_prefix` is e.g. `libwhatever.` and `idb_id` can technically be any string, but is
expected to be the MD5 hash of the `libwhatever` binary. This will cause LabSync to sync features
(e.g. names, prototypes) related to EAs whose segment name starts with `<seg_prefix>` to
`<idb_id>.yaml` instead of the main YAML file.

Because the library may be loaded to different EAs in different IDBs, the EAs in the library
YAML will be relative to the start EA of the first matching segment. If needed, you can override
the base EA by providing a `base_ea` argument to `map_segments_to_idb_id`.

## Known issues

1. LabSync does not currently do things as creating functions or changing their properties, so this
   may lead to issues e.g. when syncing inlined functions that don't have the same function end.

   Therefore we recommend to start using LabSync on a certain IDB only after it has passed the
   basic preprocessing it has to go through, in case there is any (e.g. inlining all outlined
   functions, on applicable IDBs).

2. When testing LabSync with IDBs created from binaries that had some type information, we found
   that there are multiple kinds of issues with local types and prototypes autogenerated by IDA,
   where if you just try to reapply the same local type or prototype that are already there it will
   already fail.

   We tried to solve some of them, e.g. by adding forward declarations of pointed types that were
   used in function prototypes, but some were more problematic to solve and you will have to solve
   them manually.

   Some kinds of the issues we don't currently resolve:
   1. IDA sometimes has issues reapplying prototypes which use namespaced types (e.g. try repplying
      the type for the `_copy` function in the `gzip-O3-moutline` example that we used [above](
      #example)). In such cases it's best it's best to tick `Don't display this message again` when
      the `Bad declaration. See the message window for details.` popup shows and ignore it.
   2. Syncing of templated types, e.g. from stl (currently we skip dumping them to the YAML and
      spit a warning instead).
   3. In some cases IDA generates
      [VFT structs](https://docs.hex-rays.com/user-guide/user-interface/menu-bar/view/c++-type-details)
      with the `_vtbl_layout` suffix instead of `_vtbl`.
      On reimport, IDA will yell that constructors/destructors must have the name of the class
      (even though it allows a `_vtbl` suffix). You can rename these structs and change the
      `_vtbl_layout` suffix to `_vtbl` in order to work around the issue.
   4. We encountered cases where IDA would complain about `Type 'id' is already defined` where
      some struct name is `id` under namespaces. You can rename it to work around the issue.
   5. We encountered an issue where IDA misinterprets nested namespaces, e.g. for:
      ```c
      struct B::fields {
        int b;
      };

      struct A {
        B::fields b;
      };
      ```
      It complains that `B::A::fields` does not exist. This is problematic with the naming scheme
      used by `ida_kernelcache`. We're currently investigating a workaround for this issue.

## Troubleshooting

- Q: I finished resolving a conflict with the mergetool, but IDA still hangs!
- A: In case you're using macOS, you have to actually quit the mergetool application for git/IDA
     to see that it's "done".

<!-- -->

- Q: Something went wrong during conflict resolution and I just want to go back!
- A: Go to your data repository and do `git merge --abort`

<!-- -->

- Q: I have a merge conflict on the UUID comment that LabSync added. What should I do?
- A: First read the part about *Local type tracking* above to understand what these UUIDs mean.
     Essentially, what happened is that both you and upstream have added a type with the same
     name. If these types are logically different, you can just rename one of them during conflict
     resolution. If these types are logically same, you should do the same (rename one of them) and
     then replace all of its usages to the other, delete it, and resync. On older IDA versions
     there was an [automated way to do it](https://hex-rays.com/blog/igors-tip-of-the-week-142-mapping-local-types),
     but at least on IDA 8.4 it seems to be gone.

## Future work

1. Auto-resolving of YAML conflicts -- a lot of git conflicts can be resolved by a YAML-aware merger
   (e.g. when two adjacent functions are renamed, git will conflict but there is no logical
   conflict)
2. Sync "data" types -- currently we only sync types for function EAs (i.e. prototypes) and not for
   "data" EAs
3. Merging names of functions and their prototypes into one in the YAML (currently a logical
   conflict in a function name change will result in two git conflicts)
4. "git blame" tooltip -- see who last named/typed a function to ask them about it
5. YAML metadata -- add a title/description of the IDB to the YAML

## Meta

Authored by Tomer Harpaz of Cellebrite Labs
Developed and tested for IDA 8.4 on macOS with Python 3.9.19

```

`example/f396862963d4f7ec5f518b09bee77115.yaml`:

```yaml
inlined_funcs:
- 0x10000dbf8
- 0x10000dc00
- 0x10000dc10
- 0x10000dc1c
- 0x10000dc34
- 0x10000dc44
- 0x10000dc5c
- 0x10000dc64
- 0x10000dc88
- 0x10000dc98
- 0x10000dcc0
- 0x10000dcc8
- 0x10000dcdc
- 0x10000dcf0
- 0x10000dcfc
- 0x10000dd10
- 0x10000dd1c
- 0x10000dd28
- 0x10000dd34
- 0x10000dd4c
- 0x10000dd64
- 0x10000dd7c
- 0x10000dd98
- 0x10000ddb0
- 0x10000ddb8
- 0x10000ddc4
- 0x10000ddd0
- 0x10000dddc
- 0x10000ddf8
- 0x10000de14
- 0x10000de24
- 0x10000de34
- 0x10000de44
- 0x10000de54
- 0x10000de68
- 0x10000de78
- 0x10000de88
- 0x10000dea0
- 0x10000debc
- 0x10000decc
- 0x10000dee8
- 0x10000def4
- 0x10000df08
- 0x10000df14
- 0x10000df20
- 0x10000df34
- 0x10000df4c
- 0x10000df5c
- 0x10000df74
- 0x10000df8c
- 0x10000dfa4
- 0x10000dfac
- 0x10000dfbc
- 0x10000dfdc
- 0x10000dfec
- 0x10000dffc
- 0x10000e008
- 0x10000e010
- 0x10000e018
- 0x10000e028
- 0x10000e038
- 0x10000e044
- 0x10000e05c
- 0x10000e070
- 0x10000e088
- 0x10000e098
- 0x10000e0b8
- 0x10000e0d4
- 0x10000e0e4
- 0x10000e0f8
- 0x10000e10c
- 0x10000e120
- 0x10000e12c
- 0x10000e140
- 0x10000e154
- 0x10000e160
- 0x10000e174
- 0x10000e17c
- 0x10000e18c
- 0x10000e1a0
- 0x10000e1b4
- 0x10000e1d0
- 0x10000e1dc
- 0x10000e1e8
- 0x10000e1f0
- 0x10000e204
- 0x10000e210
- 0x10000e21c
- 0x10000e230
- 0x10000e238
- 0x10000e240
- 0x10000e254
- 0x10000e268
- 0x10000e27c
- 0x10000e290
- 0x10000e29c
- 0x10000e2a8
- 0x10000e2b4
- 0x10000e2c0
- 0x10000e2cc
- 0x10000e2d4
local_types: |-
  /* >> LABSYNC DO NOT TOUCH: 5acbdd09-fb3c-4ea8-bf8a-0c8bee6459ce << */
  struct _RuneCharClass
  {
    char __name[14];
    __uint32_t __mask;
  };

  /* >> LABSYNC DO NOT TOUCH: 2d818a33-0259-4061-82db-e1a6d943d47e << */
  struct _RuneEntry
  {
    __darwin_rune_t __min;
    __darwin_rune_t __max;
    __darwin_rune_t __map;
    __uint32_t *__types;
  };

  /* >> LABSYNC DO NOT TOUCH: 4e2dc9c5-6cb8-4d6f-a984-bcbb69eda153 << */
  struct _RuneLocale
  {
    char __magic[8];
    char __encoding[32];
    __darwin_rune_t (__cdecl *__sgetrune)(const char *, __darwin_size_t, const char **);
    int (__cdecl *__sputrune)(__darwin_rune_t, char *, __darwin_size_t, char **);
    __darwin_rune_t __invalid_rune;
    __uint32_t __runetype[256];
    __darwin_rune_t __maplower[256];
    __darwin_rune_t __mapupper[256];
    _RuneRange __runetype_ext;
    _RuneRange __maplower_ext;
    _RuneRange __mapupper_ext;
    void *__variable;
    int __variable_len;
    int __ncharclasses;
    _RuneCharClass *__charclasses;
  };

  /* >> LABSYNC DO NOT TOUCH: 3f221dd1-ba46-40f4-9674-3ba0e8c83150 << */
  struct _RuneRange
  {
    int __nranges;
    _RuneEntry *__ranges;
  };

  /* >> LABSYNC DO NOT TOUCH: 23ad52d3-b27c-45b6-b4ef-2e6b29f41102 << */
  typedef __int64_t __darwin_blkcnt_t;

  /* >> LABSYNC DO NOT TOUCH: 5830a09c-64e3-49cf-adb1-7c95da42764d << */
  typedef __int32_t __darwin_blksize_t;

  /* >> LABSYNC DO NOT TOUCH: 961eb519-612d-4165-80ee-b4f4d4931d06 << */
  typedef __int32_t __darwin_dev_t;

  /* >> LABSYNC DO NOT TOUCH: 1fcd322b-9900-4931-b66a-e48e50a95901 << */
  typedef __uint32_t __darwin_gid_t;

  /* >> LABSYNC DO NOT TOUCH: ba544ace-be19-420b-b925-7fe250df7d72 << */
  typedef __uint64_t __darwin_ino64_t;

  /* >> LABSYNC DO NOT TOUCH: b50fc9d0-ae4f-4684-a90b-0daa8cac609d << */
  typedef __uint16_t __darwin_mode_t;

  /* >> LABSYNC DO NOT TOUCH: 2a88c553-411f-44b5-9945-ab07e185c072 << */
  typedef __int64_t __darwin_off_t;

  /* >> LABSYNC DO NOT TOUCH: 5ec1d659-94a0-4c9d-a7d6-1f3877c7618c << */
  typedef __darwin_wchar_t __darwin_rune_t;

  /* >> LABSYNC DO NOT TOUCH: d60b92ae-3e37-4bfa-be13-bbb6b80ac3da << */
  typedef unsigned __int64 __darwin_size_t;

  /* >> LABSYNC DO NOT TOUCH: 4114422b-3e48-48da-9497-c7c32a1197a7 << */
  typedef __int64 __darwin_time_t;

  /* >> LABSYNC DO NOT TOUCH: 37da2e80-af94-44f3-9e19-7e3b11de903f << */
  typedef __uint32_t __darwin_uid_t;

  /* >> LABSYNC DO NOT TOUCH: 57cbd9ef-c228-43cc-954a-7802db709b84 << */
  typedef int __darwin_wchar_t;

  /* >> LABSYNC DO NOT TOUCH: 2169feda-a5d4-4f46-8961-c62c6bd0d67f << */
  typedef int __int32_t;

  /* >> LABSYNC DO NOT TOUCH: 756f423f-8d5d-4cde-884b-05f74481284f << */
  typedef __int64 __int64_t;

  /* >> LABSYNC DO NOT TOUCH: d3fd903e-9fd4-4a71-a2c6-ecbb5a2c2981 << */
  union __attribute__((aligned(8))) __n128
  {
    unsigned __int64 n128_u64[2];
    unsigned __int32 n128_u32[4];
    unsigned __int16 n128_u16[8];
    unsigned __int8 n128_u8[16];
    __int64 n128_i64[2];
    __int32 n128_i32[4];
    __int16 n128_i16[8];
    __int8 n128_i8[16];
    float n128_f32[4];
    double n128_f64[2];
  };

  /* >> LABSYNC DO NOT TOUCH: 866ba5ac-0770-4ec3-b749-6672d21fbd3a << */
  union __attribute__((aligned(8))) __n64
  {
    unsigned __int64 n64_u64[1];
    unsigned __int32 n64_u32[2];
    unsigned __int16 n64_u16[4];
    unsigned __int8 n64_u8[8];
    __int64 n64_i64[1];
    __int32 n64_i32[2];
    __int16 n64_i16[4];
    __int8 n64_i8[8];
    float n64_f32[2];
    double n64_f64[1];
  };

  /* >> LABSYNC DO NOT TOUCH: a36a6c9d-afa6-4fb1-a307-b31d39c222e7 << */
  typedef unsigned __int16 __uint16_t;

  /* >> LABSYNC DO NOT TOUCH: 985a4a22-2144-4f77-be7e-aed577891491 << */
  typedef unsigned int __uint32_t;

  /* >> LABSYNC DO NOT TOUCH: c9a8b4f6-de58-46a2-9fe7-1e2f501abe29 << */
  typedef unsigned __int64 __uint64_t;

  /* >> LABSYNC DO NOT TOUCH: e512452e-e829-4fc2-9925-026bbe3f23f2 << */
  typedef __darwin_blkcnt_t blkcnt_t;

  /* >> LABSYNC DO NOT TOUCH: 046a1035-675c-4c3e-8350-ca5df1e1987b << */
  typedef __darwin_blksize_t blksize_t;

  /* >> LABSYNC DO NOT TOUCH: 66bd002a-10f2-4108-9563-af0d17f7a3af << */
  struct build_version_command
  {
    uint32_t cmd;
    uint32_t cmdsize;
    uint32_t platform;
    uint32_t minos;
    uint32_t sdk;
    uint32_t ntools;
  };

  /* >> LABSYNC DO NOT TOUCH: c8fc54b8-2193-4465-aea7-9dc8f3e75335 << */
  typedef __darwin_dev_t dev_t;

  /* >> LABSYNC DO NOT TOUCH: 95bff06c-412e-42fc-b43f-ad3078c2b977 << */
  struct dyld_info_command
  {
    uint32_t cmd;
    uint32_t cmdsize;
    uint32_t rebase_off;
    uint32_t rebase_size;
    uint32_t bind_off;
    uint32_t bind_size;
    uint32_t weak_bind_off;
    uint32_t weak_bind_size;
    uint32_t lazy_bind_off;
    uint32_t lazy_bind_size;
    uint32_t export_off;
    uint32_t export_size;
  };

  /* >> LABSYNC DO NOT TOUCH: c29daf04-4027-4039-a4ae-60500fd4d765 << */
  struct dylib
  {
    lc_str name;
    uint32_t timestamp;
    uint32_t current_version;
    uint32_t compatibility_version;
  };

  /* >> LABSYNC DO NOT TOUCH: 824cfa9a-83f5-484b-aab8-7ab4e6b007ea << */
  struct dylib_command
  {
    uint32_t cmd;
    uint32_t cmdsize;
    dylib dylib;
  };

  /* >> LABSYNC DO NOT TOUCH: 20ac51a8-fe70-4fe0-b5ce-6d6eec53ffd7 << */
  struct dylinker_command
  {
    uint32_t cmd;
    uint32_t cmdsize;
    lc_str name;
  };

  /* >> LABSYNC DO NOT TOUCH: 3ba4b053-70ef-41da-bc09-77dc73aa85c7 << */
  struct dysymtab_command
  {
    uint32_t cmd;
    uint32_t cmdsize;
    uint32_t ilocalsym;
    uint32_t nlocalsym;
    uint32_t iextdefsym;
    uint32_t nextdefsym;
    uint32_t iundefsym;
    uint32_t nundefsym;
    uint32_t tocoff;
    uint32_t ntoc;
    uint32_t modtaboff;
    uint32_t nmodtab;
    uint32_t extrefsymoff;
    uint32_t nextrefsyms;
    uint32_t indirectsymoff;
    uint32_t nindirectsyms;
    uint32_t extreloff;
    uint32_t nextrel;
    uint32_t locreloff;
    uint32_t nlocrel;
  };

  /* >> LABSYNC DO NOT TOUCH: 974e690c-0e27-431e-beb4-775ab098c46e << */
  struct entry_point_command
  {
    uint32_t cmd;
    uint32_t cmdsize;
    uint64_t entryoff;
    uint64_t stacksize;
  };

  /* >> LABSYNC DO NOT TOUCH: 54768367-3987-4ff3-af0e-a4842935100e << */
  typedef float16x4_t float16x2_t;

  /* >> LABSYNC DO NOT TOUCH: 44f80af5-79a5-4185-b0e4-71b2a6a60a50 << */
  typedef __darwin_gid_t gid_t;

  /* >> LABSYNC DO NOT TOUCH: b2705078-e71f-4465-af74-08b0db811216 << */
  typedef int16x4_t int16x2_t;

  /* >> LABSYNC DO NOT TOUCH: af9fc1f4-d91c-4ee4-a4e2-1f2f3d218372 << */
  typedef int int32_t;

  /* >> LABSYNC DO NOT TOUCH: 1739cc51-aa3a-4f93-bf45-7537e362e901 << */
  union lc_str
  {
    uint32_t offset;
  };

  /* >> LABSYNC DO NOT TOUCH: 71073399-a183-43e0-9dc6-ae252c641bb7 << */
  struct linkedit_data_command
  {
    uint32_t cmd;
    uint32_t cmdsize;
    uint32_t dataoff;
    uint32_t datasize;
  };

  /* >> LABSYNC DO NOT TOUCH: ce4306db-1003-42ae-946e-263e6eacd58d << */
  struct mach_header_64
  {
    uint32_t magic;
    int32_t cputype;
    int32_t cpusubtype;
    uint32_t filetype;
    uint32_t ncmds;
    uint32_t sizeofcmds;
    uint32_t flags;
    uint32_t reserved;
  };

  /* >> LABSYNC DO NOT TOUCH: 9d734e39-277e-40f1-8c19-cc0590603b20 << */
  typedef __darwin_mode_t mode_t;

  /* >> LABSYNC DO NOT TOUCH: 0012d8a6-a6ef-41c9-b92f-408e7159c9f6 << */
  typedef __uint16_t nlink_t;

  /* >> LABSYNC DO NOT TOUCH: f6ecd5f9-54d7-45db-9bad-46fd0b338d09 << */
  typedef __darwin_off_t off_t;

  /* >> LABSYNC DO NOT TOUCH: 6dd54d24-31aa-4866-8cc5-953e3212cec8 << */
  struct section_64
  {
    char sectname[16] __strlit(C,"UTF-8");
    char segname[16] __strlit(C,"UTF-8");
    uint64_t addr;
    uint64_t size;
    uint32_t offset;
    uint32_t align;
    uint32_t reloff;
    uint32_t nreloc;
    uint32_t flags;
    uint32_t reserved1;
    uint32_t reserved2;
    uint32_t reserved3;
  };

  /* >> LABSYNC DO NOT TOUCH: fbc3b44e-8448-4241-9955-214c91b8edaf << */
  struct segment_command_64
  {
    uint32_t cmd;
    uint32_t cmdsize;
    char segname[16] __strlit(C,"UTF-8");
    uint64_t vmaddr;
    uint64_t vmsize;
    uint64_t fileoff;
    uint64_t filesize;
    int32_t maxprot;
    int32_t initprot;
    uint32_t nsects;
    uint32_t flags;
  };

  /* >> LABSYNC DO NOT TOUCH: 51daed76-64f3-4c64-9392-cd82cfd1c1f1 << */
  struct source_version_command
  {
    uint32_t cmd;
    uint32_t cmdsize;
    uint64_t version;
  };

  /* >> LABSYNC DO NOT TOUCH: de37c4b5-0ec2-4cfb-95bd-7e85361e27f5 << */
  struct stat
  {
    dev_t st_dev;
    mode_t st_mode;
    nlink_t st_nlink;
    __darwin_ino64_t st_ino;
    uid_t st_uid;
    gid_t st_gid;
    dev_t st_rdev;
    timespec st_atimespec;
    timespec st_mtimespec;
    timespec st_ctimespec;
    timespec st_birthtimespec;
    off_t st_size;
    blkcnt_t st_blocks;
    blksize_t st_blksize;
    __uint32_t st_flags;
    __uint32_t st_gen;
    __int32_t st_lspare;
    __int64_t st_qspare[2];
  };

  /* >> LABSYNC DO NOT TOUCH: 1aa588d7-4d71-4ba7-8e85-9ac75753f253 << */
  struct symtab_command
  {
    uint32_t cmd;
    uint32_t cmdsize;
    uint32_t symoff;
    uint32_t nsyms;
    uint32_t stroff;
    uint32_t strsize;
  };

  /* >> LABSYNC DO NOT TOUCH: 8b27fdb5-a146-440d-ba0a-15faef0f91e2 << */
  typedef __darwin_time_t time_t;

  /* >> LABSYNC DO NOT TOUCH: 7f9f9eaf-a2f2-4627-b738-976aca72cc37 << */
  struct timespec
  {
    __darwin_time_t tv_sec;
    __int64 tv_nsec;
  };

  /* >> LABSYNC DO NOT TOUCH: d1d70767-818a-4d0c-bab1-545df4b16694 << */
  typedef unsigned int u_int32_t;

  /* >> LABSYNC DO NOT TOUCH: 9dc51d65-2de3-4771-9020-70bcc497a13c << */
  typedef unsigned __int64 u_int64_t;

  /* >> LABSYNC DO NOT TOUCH: d4948a53-1864-4107-8484-912bf5b67b36 << */
  typedef unsigned __int8 u_int8_t;

  /* >> LABSYNC DO NOT TOUCH: c8b2e277-690e-4259-94e8-07b8f4f63841 << */
  typedef __darwin_uid_t uid_t;

  /* >> LABSYNC DO NOT TOUCH: 7237edb2-e0ab-40f9-95e4-08232922e0cd << */
  typedef uint16x4_t uint16x2_t;

  /* >> LABSYNC DO NOT TOUCH: 9b913b91-db29-40c3-a47a-85745d939b7c << */
  typedef u_int32_t uint32_t;

  /* >> LABSYNC DO NOT TOUCH: 0ecbf17f-b3db-4cf5-a4ac-76456e96ee85 << */
  typedef u_int64_t uint64_t;

  /* >> LABSYNC DO NOT TOUCH: 9758bf7e-c5a0-4888-ae4b-a5f81671454a << */
  typedef u_int8_t uint8_t;

  /* >> LABSYNC DO NOT TOUCH: 0fc9f00a-b75f-4564-bca1-b45f4ef139a2 << */
  struct utimbuf
  {
    time_t actime;
    time_t modtime;
  };

  /* >> LABSYNC DO NOT TOUCH: d9880975-e8a9-4cfa-bb72-5927bdbcde62 << */
  struct uuid_command
  {
    uint32_t cmd;
    uint32_t cmdsize;
    uint8_t uuid[16];
  };
names:
  0x100000000: __mh_execute_header
  0x1000005ec: aUsrLibDyld
  0x100000678: aUsrLibLibsyste
  0x10000173c: _bi_init
  0x100001768: _file_read
  0x100001864: _send_bits
  0x1000019bc: _flush_outbuf
  0x1000019fc: _bi_reverse
  0x100001a28: _bi_windup
  0x100001b8c: _copy_block
  0x100001d9c: _lm_init
  0x10000201c: _error
  0x10000205c: _longest_match
  0x1000022ec: _deflate
  0x100002ed4: _ct_tally
  0x100003208: _flush_block
  0x100003898: __getopt_internal
  0x100004480: _getopt
  0x10000448c: _zip
  0x100004954: _main
  0x1000051c4: def_100004B84
  0x100005208: _base_name
  0x100005230: _add_envopt
  0x100005414: _abort_gzip_signal
  0x100005424: _getopt_long
  0x100005428: _lzw
  0x100005494: _treat_file
  0x100005cfc: _treat_stdin
  0x1000060d4: _do_list
  0x100006738: _abort_gzip
  0x1000067a4: _huft_build
  0x100006f64: _huft_free
  0x100006f8c: _inflate_codes
  0x10000757c: _fill_inbuf
  0x100007718: _flush_window
  0x100007798: _inflate_stored
  0x100007a74: _inflate_fixed
  0x100007bc8: _inflate_dynamic
  0x100008220: _inflate_block
  0x10000836c: _inflate
  0x100008460: _ct_init
  0x10000882c: _gen_codes
  0x1000089c4: _init_block
  0x100008adc: _build_tree_1
  0x100009138: _compress_block
  0x1000092ac: _unlzh
  0x100009bb0: _write_buf
  0x100009c14: _unlzw
  0x10000a3ac: _read_error
  0x10000a418: _unpack
  0x10000ab2c: _check_zipfile
  0x10000ac3c: _unzip
  0x10000b4f8: _updcrc
  0x10000b538: _copy
  0x10000b648: _clear_bufs
  0x10000b674: _write_error
  0x10000b6c0: _strlwr
  0x10000b73c: _xunlink
  0x10000b740: _make_simple_name
  0x10000b78c: _xmalloc
  0x10000b7e0: _warning
  0x10000b84c: _display_ratio
  0x10000b894: _fprint_off
  0x10000b9a8: _yesno
  0x10000ba6c: _rpmatch
  0x10000baa0: _getopt_long_only
  0x10000baa8: _get_method
  0x10000c41c: _reset_times
  0x10000c4e0: _make_ofname
  0x10000c730: _create_outfile
  0x10000ccc4: _copy_stat
  0x10000ce94: _get_suffix
  0x10000d01c: _shorten_name
  0x10000d24c: _do_remove
  0x10000d280: _send_tree
  0x10000d590: _fillbuf
  0x10000d6bc: _read_pt_len
  0x10000d8a4: _make_table
  0x10000dbf0: _OUTLINED_FUNCTION_0
  0x10000dcb0: _OUTLINED_FUNCTION_11
  0x10000dcb8: _OUTLINED_FUNCTION_12
  0x10000dd08: _OUTLINED_FUNCTION_18
  0x10000dda8: _OUTLINED_FUNCTION_27
  0x10000de98: _OUTLINED_FUNCTION_42
  0x10000dec4: _OUTLINED_FUNCTION_45
  0x10000df2c: _OUTLINED_FUNCTION_52
  0x10000dfd4: _OUTLINED_FUNCTION_61
  0x10000e068: _OUTLINED_FUNCTION_72
  0x10000e0b0: _OUTLINED_FUNCTION_76
  0x10000e184: _OUTLINED_FUNCTION_89
  0x10000e1c8: _OUTLINED_FUNCTION_93
  0x10000e2e0: _progerror
  0x10000e330: _treat_file.cold.1
  0x10000e380: _treat_file.cold.2
  0x10000e404: _treat_file.cold.3
  0x10000e454: _treat_stdin.cold.1
  0x10000e4e8: _write_buf.cold.1
  0x10000e534: _unlzw.cold.1
  0x10000e580: _create_outfile.cold.1
  0x10000e5ec: _create_outfile.cold.2
  0x10000e66c: _create_outfile.cold.3
  0x10000e6c0: ___error
  0x10000e6cc: ___maskrune
  0x10000e6d8: ___strcat_chk
  0x10000e6e4: ___strcpy_chk
  0x10000e6f0: ___tolower
  0x10000e6fc: __exit
  0x10000e708: _atoi
  0x10000e714: _bzero
  0x10000e720: _calloc
  0x10000e72c: _close
  0x10000e738: _closedir
  0x10000e744: _ctime
  0x10000e750: _exit
  0x10000e75c: _fchmod
  0x10000e768: _fchown
  0x10000e774: _fflush
  0x10000e780: _fileno
  0x10000e78c: _fprintf
  0x10000e798: _fputc
  0x10000e7a4: _free
  0x10000e7b0: _fstat
  0x10000e7bc: _fwrite
  0x10000e7c8: _getchar
  0x10000e7d4: _getenv
  0x10000e7e0: _isatty
  0x10000e7ec: _lseek
  0x10000e7f8: _lstat
  0x10000e804: _malloc
  0x10000e810: _memcpy
  0x10000e81c: _memset
  0x10000e828: _memset_pattern16
  0x10000e834: _open
  0x10000e840: _opendir
  0x10000e84c: _perror
  0x10000e858: _printf
  0x10000e864: _putc
  0x10000e870: _putchar
  0x10000e87c: _puts
  0x10000e888: _read
  0x10000e894: _readdir
  0x10000e8a0: _signal
  0x10000e8ac: _stat
  0x10000e8b8: _strchr
  0x10000e8c4: _strcmp
  0x10000e8d0: _strcpy
  0x10000e8dc: _strcspn
  0x10000e8e8: _strlen
  0x10000e8f4: _strncmp
  0x10000e900: _strrchr
  0x10000e90c: _strspn
  0x10000e918: _unlink
  0x10000e924: _utime
  0x10000e930: _write
  0x10000ebd0: jpt_100004B84
  0x10000ecc0: _configuration_table
  0x10000ed10: _border
  0x10000ed5c: _bl_order
  0x10000edb0: aBadPackLevel
  0x10000edc2: aSOptionSIsAmbi
  0x10000ede0: aSOptionSDoesnT
  0x10000ee0d: aSOptionCSDoesn
  0x10000ee3a: aSOptionSRequir
  0x10000ee60: aSUnrecognizedO
  0x10000ee80: aSUnrecognizedO_0
  0x10000eea1: aSIllegalOption
  0x10000eebb: aSInvalidOption
  0x10000eed5: aSOptionRequire
  0x10000eefc: aSOptionWSIsAmb
  0x10000ef1d: aSOptionWSDoesn
  0x10000ef4b: aAscii
  0x10000ef51: aToStdout
  0x10000ef5b: aStdout
  0x10000ef62: aDecompress
  0x10000ef6d: aUncompress
  0x10000ef78: aForce
  0x10000ef7e: aHelp
  0x10000ef83: aList
  0x10000ef88: aLicense
  0x10000ef90: aNoName
  0x10000ef98: aName
  0x10000ef9d: aQuiet
  0x10000efa3: aSilent
  0x10000efaa: aRecursive
  0x10000efb4: aSuffix
  0x10000efbb: aTest
  0x10000efc0: aNoTime
  0x10000efc8: aVerbose
  0x10000efd0: aVersion
  0x10000efd8: aFast
  0x10000efdd: aBest
  0x10000efe2: aLzw
  0x10000efe6: aBits
  0x10000efeb: aRsyncable
  0x10000eff5: aExe
  0x10000effa: aGzip
  0x10000efff: aUn
  0x10000f002: aGun
  0x10000f006: aCat
  0x10000f00a: aGzcat
  0x10000f010: aGz
  0x10000f014: aAbCdfhhLlmmnnq
  0x10000f035: aSBOperandIsNot
  0x10000f057: aSZNotSupported
  0x10000f07d: aSOptionAsciiIg
  0x10000f0a8: aSIncorrectSuff
  0x10000f0c3: aIncompleteLite
  0x10000f0dd: aIncompleteDist
  0x10000f0f8: aOutputInCompre
  0x10000f124: aSSWarningUnkno
  0x10000f14a: aSSCompressedWi
  0x10000f185: aCorruptInput
  0x10000f194: aCorruptInputUs
  0x10000f1c2: aInvalidCompres
  0x10000f1e8: aSSNotAValidZip
  0x10000f207: aSSFirstEntryNo
  0x10000f241: aSSEncryptedFil
  0x10000f267: aOutOfMemory
  0x10000f275: aInvalidCompres_0
  0x10000f29e: aLenLdSizLd
  0x10000f2b0: aInvalidCompres_1
  0x10000f2d9: aInternalErrorI
  0x10000f2f8: aSSInvalidCompr
  0x10000f325: aSSInvalidCompr_0
  0x10000f355: aSSHasMoreThanO
  0x10000f383: aSSHasMoreThanO_0
  0x10000f3b3: aArgc0
  0x10000f3bb: aSSS
  0x10000f3c8: aSSWarningS
  0x10000f3dd: aS
  0x10000f3e3: aSUnexpectedEnd
  0x10000f3ff: a51f
  0x10000f40a: aPosixlyCorrect
  0x10000f41a: aUsageSScdfhlln
  0x10000f451: aR
  0x10000f453: aCStdoutWriteOn
  0x10000f49d: aDDecompressDec
  0x10000f4ba: aFForceForceOve
  0x10000f4fe: aHHelpGiveThisH
  0x10000f51f: aLListListCompr
  0x10000f54f: aLLicenseDispla
  0x10000f57a: aNNoNameDoNotSa
  0x10000f5c4: aNNameSaveOrRes
  0x10000f607: aQQuietSuppress
  0x10000f62f: aRRecursiveOper
  0x10000f664: aSSufSuffixSufU
  0x10000f6a4: aTTestTestCompr
  0x10000f6d5: aVVerboseVerbos
  0x10000f6f4: aVVersionDispla
  0x10000f71d: a1FastCompressF
  0x10000f73f: a9BestCompressB
  0x10000f761: aRsyncableMakeR
  0x10000f78f: aFileFilesToDeC
  0x10000f7db: aReportBugsToBu
  0x10000f7fe: aSSS_0
  0x10000f80a: a135
  0x10000f810: a20020930
  0x10000f81b: aCopyright2002F
  0x10000f843: aCopyright19921
  0x10000f868: aThisProgramCom
  0x10000f898: aYouMayRedistri
  0x10000f8c4: aUnderTheTermsO
  0x10000f8f7: aForMoreInforma
  0x10000f93d: aCompilationOpt
  0x10000f959: aDirent
  0x10000f960: aUtime
  0x10000f966: aStdcHeaders
  0x10000f974: aHaveUnistdH
  0x10000f983: aHaveMemoryH
  0x10000f992: aHaveStringH
  0x10000f9a1: aHaveLstat
  0x10000f9ad: aSCompressedDat
  0x10000f9f4: aReadFrom
  0x10000f9fe: aWrittenTo
  0x10000fa09: aDe
  0x10000fa0c: aForHelpTypeSH
  0x10000fa23: aStdin
  0x10000fa29: aStandardInput
  0x10000fa38: aOk
  0x10000fa3d: aS_0
  0x10000fa45: aSSUnknownMetho
  0x10000fa71: aSSIsEncryptedN
  0x10000fa97: aSSIsAAMultiPar
  0x10000facc: aSSHasFlags0xXN
  0x10000faf4: aSSPartNumberU
  0x10000fb0c: aSSExtraFieldOf
  0x10000fb35: aCorruptedInput
  0x10000fb5c: aPk
  0x10000fb6a: aSSNotInGzipFor
  0x10000fb87: aSSDecompressio
  0x10000fbbf: aSSDecompressio_0
  0x10000fbf6: aSSIsADirectory
  0x10000fc18: aSSIsNotADirect
  0x10000fc4f: aSSHasLuOtherLi
  0x10000fc79: aSSCompressedTo
  0x10000fc92: aS_1
  0x10000fc97: aOk_0
  0x10000fc9b: aReplacedWithS
  0x10000fcb0: aZ
  0x10000fcb3: aZ_0
  0x10000fcb6: aZ_1
  0x10000fcb9: aSSFileNameTooL
  0x10000fcd5: aTaz
  0x10000fcda: aTgz
  0x10000fcdf: aGz_0
  0x10000fce3: aZ_2
  0x10000fce6: aZ_3
  0x10000fced: aSSSPathnameToo
  0x10000fd0b: aSSUnknownSuffi
  0x10000fd2e: aTar
  0x10000fd33: aSSAlreadyHasSS
  0x10000fd5e: aSSWarningNameT
  0x10000fd7f: aSSCannotScompr
  0x10000fda6: aSSAndSAreTheSa
  0x10000fdc7: aSSAlreadyExist
  0x10000fdde: aDoYouWishToOve
  0x10000fe03: aNotOverwritten
  0x10000fe15: aNameTooShort
  0x10000fe24: aCanTRecoverSuf
  0x10000fe3a: aInternalErrorI_0
  0x10000fe59: aSTimeStampRest
  0x10000fe72: aStore
  0x10000fe78: aCompr
  0x10000fe7e: aPack
  0x10000fe84: aLzh
  0x10000fe8a: aDefla
  0x10000fe90: aMethodCrcDateT
  0x10000fead: aSSRatioUncompr
  0x10000fed3: aCompressed
  0x10000fede: aUncompressed
  0x10000ff08: a5s08lx11s
  0x10000ff18: aS_2
  0x10000ff1d: aBadTable
  0x10000ff28: aInvalidCompres_2
  0x10000ff5a: aTooManyLeavesI
  0x10000ff7a: aWrittenByJeanL
  0x10000ff97: aTotals
  0x100010000: __DefaultRuneLocale_ptr
  0x100010008: ___stderrp_ptr
  0x100010010: ___stdinp_ptr
  0x100010018: ___stdoutp_ptr
  0x100010020: dyld_stub_binder_ptr
  0x100010028: _do_list.methods
  0x100014000: ___error_ptr
  0x100014008: ___maskrune_ptr
  0x100014010: ___strcat_chk_ptr
  0x100014018: ___strcpy_chk_ptr
  0x100014020: ___tolower_ptr
  0x100014028: __exit_ptr
  0x100014030: _atoi_ptr
  0x100014038: _bzero_ptr
  0x100014040: _calloc_ptr
  0x100014048: _close_ptr
  0x100014050: _closedir_ptr
  0x100014058: _ctime_ptr
  0x100014060: _exit_ptr
  0x100014068: _fchmod_ptr
  0x100014070: _fchown_ptr
  0x100014078: _fflush_ptr
  0x100014080: _fileno_ptr
  0x100014088: _fprintf_ptr
  0x100014090: _fputc_ptr
  0x100014098: _free_ptr
  0x1000140a0: _fstat_ptr
  0x1000140a8: _fwrite_ptr
  0x1000140b0: _getchar_ptr
  0x1000140b8: _getenv_ptr
  0x1000140c0: _isatty_ptr
  0x1000140c8: _lseek_ptr
  0x1000140d0: _lstat_ptr
  0x1000140d8: _malloc_ptr
  0x1000140e0: _memcpy_ptr
  0x1000140e8: _memset_ptr
  0x1000140f0: _memset_pattern16_ptr
  0x1000140f8: _open_ptr
  0x100014100: _opendir_ptr
  0x100014108: _perror_ptr
  0x100014110: _printf_ptr
  0x100014118: _putc_ptr
  0x100014120: _putchar_ptr
  0x100014128: _puts_ptr
  0x100014130: _read_ptr
  0x100014138: _readdir_ptr
  0x100014140: _signal_ptr
  0x100014148: _stat_ptr
  0x100014150: _strchr_ptr
  0x100014158: _strcmp_ptr
  0x100014160: _strcpy_ptr
  0x100014168: _strcspn_ptr
  0x100014170: _strlen_ptr
  0x100014178: _strncmp_ptr
  0x100014180: _strrchr_ptr
  0x100014188: _strspn_ptr
  0x100014190: _unlink_ptr
  0x100014198: _utime_ptr
  0x1000141a0: _write_ptr
  0x1000141a8: __dyld_private
  0x1000141b0: _window_size
  0x1000141b8: _optind
  0x1000141bc: _opterr
  0x1000141c0: _optopt
  0x1000141c4: _no_name
  0x1000141c8: _no_time
  0x1000141cc: _maxbits
  0x1000141d0: _method
  0x1000141d4: _level
  0x1000141d8: _longopts
  0x1000144f8: _work
  0x100014500: _mask_bits
  0x100014524: _lbits
  0x100014528: _dbits
  0x10001452c: _block_mode
  0x100014530: _updcrc.crc
  0x100014538: _crc_32_tab
  0x100014d38: _get_istat.suffixes
  0x100014d68: _get_suffix.known_suffixes
  0x100014db0: _extra_blbits
  0x100014e00: __MergedGlobals
  0x100015058: _read_buf
  0x100015060: _outcnt
  0x100015064: _good_match
  0x100015068: _nice_match
  0x10001506c: _max_chain_length
  0x100015070: _strstart
  0x100015078: _block_start
  0x100015080: _prev_length
  0x100015084: _match_start
  0x100015088: _rsync
  0x100015090: _optarg
  0x100015098: ___getopt_initialized
  0x10001509c: _ascii
  0x1000150a0: _to_stdout
  0x1000150a4: _decompress
  0x1000150a8: _force
  0x1000150ac: _recursive
  0x1000150b0: _list
  0x1000150b4: _verbose
  0x1000150b8: _quiet
  0x1000150bc: _do_lzw
  0x1000150c0: _test
  0x1000150c4: _exit_code
  0x1000150c8: _args
  0x1000150d0: _remove_ofname
  0x1000150d8: _progname
  0x1000150e0: _env
  0x1000150e8: _foreground
  0x1000150f0: _z_suffix
  0x1000150f8: _z_len
  0x100015100: _hufts
  0x100015108: _bb
  0x100015110: _bk
  0x100015114: _inptr
  0x100015118: _insize
  0x10001511c: _inbuf
  0x10001d160: _file_type
  0x10001d168: _file_method
  0x10001d170: _d_buf
  0x10002d170: _ifd
  0x10002d174: _ofd
  0x10002d178: _ifname
  0x10002d578: _bytes_in
  0x10002d580: _bytes_out
  0x10002d588: _pkzip
  0x10002d58c: _ext_header
  0x10002d590: _decrypt
  0x10002d594: _ofname
  0x10002d994: _save_orig_name
  0x10002d998: _time_stamp
  0x10002d9a0: _header_bytes
  0x10002d9a8: _outbuf
  0x1000321a8: _window
  0x1000421a8: _prev
  0x1000621a8: _last_member
  0x1000621ac: _part_nb
  0x1000621b0: _ifile_size
  0x1000621b8: _total_in
  0x1000621c0: _total_out
  0x1000621c8: _istat
  0x100062258: _key
  0x100062260: _msg_done
  0x100062261: _flag_buf
  0x100063264: _do_list.first_time
  0x100063268: _do_exit.in_exit
  0x10006326c: _heap
  0x100063b60: __MergedGlobals.212
  0x1000648ae: __MergedGlobals.213
  0x100065630: __DefaultRuneLocale
  0x100065638: ___stderrp
  0x100065640: ___stdinp
  0x100065648: ___stdoutp
  0x100065650: dyld_stub_binder
  0x100065658: __imp____error
  0x100065660: __imp____maskrune
  0x100065668: __imp____strcat_chk
  0x100065670: __imp____strcpy_chk
  0x100065678: __imp____tolower
  0x100065680: __imp___exit
  0x100065688: __imp__atoi
  0x100065690: __imp__bzero
  0x100065698: __imp__calloc
  0x1000656a0: __imp__close
  0x1000656a8: __imp__closedir
  0x1000656b0: __imp__ctime
  0x1000656b8: __imp__exit
  0x1000656c0: __imp__fchmod
  0x1000656c8: __imp__fchown
  0x1000656d0: __imp__fflush
  0x1000656d8: __imp__fileno
  0x1000656e0: __imp__fprintf
  0x1000656e8: __imp__fputc
  0x1000656f0: __imp__free
  0x1000656f8: __imp__fstat
  0x100065700: __imp__fwrite
  0x100065708: __imp__getchar
  0x100065710: __imp__getenv
  0x100065718: __imp__isatty
  0x100065720: __imp__lseek
  0x100065728: __imp__lstat
  0x100065730: __imp__malloc
  0x100065738: __imp__memcpy
  0x100065740: __imp__memset
  0x100065748: __imp__memset_pattern16
  0x100065750: __imp__open
  0x100065758: __imp__opendir
  0x100065760: __imp__perror
  0x100065768: __imp__printf
  0x100065770: __imp__putc
  0x100065778: __imp__putchar
  0x100065780: __imp__puts
  0x100065788: __imp__read
  0x100065790: __imp__readdir
  0x100065798: __imp__signal
  0x1000657a0: __imp__stat
  0x1000657a8: __imp__strchr
  0x1000657b0: __imp__strcmp
  0x1000657b8: __imp__strcpy
  0x1000657c0: __imp__strcspn
  0x1000657c8: __imp__strlen
  0x1000657d0: __imp__strncmp
  0x1000657d8: __imp__strrchr
  0x1000657e0: __imp__strspn
  0x1000657e8: __imp__unlink
  0x1000657f0: __imp__utime
  0x1000657f8: __imp__write
prototypes:
  0x100001768: __int64 __fastcall _file_read(void *, size_t)
  0x1000022ec: int __cdecl _deflate(z_streamp strm, int flush)
  0x100003898: __int64 __fastcall __getopt_internal(int, int, char *__s)
  0x100004480: int __cdecl _getopt(int, char *const [], const char *)
  0x100004954: int __fastcall _main(int argc, const char **argv, const char **envp)
  0x100005230: __int64 __fastcall _add_envopt(int, int, char *)
  0x100005424: int __cdecl _getopt_long(int, char *const *, const char *, const option
    *, int *)
  0x100005494: __int64 __fastcall _treat_file(_QWORD)
  0x100005cfc: __int64 __fastcall _treat_stdin(_QWORD)
  0x1000060d4: __int64 __fastcall _do_list(int)
  0x10000836c: int __cdecl _inflate(z_streamp strm, int flush)
  0x100008460: __int64 __fastcall _ct_init(__int64, __int64)
  0x10000882c: __int64 _gen_codes(void)
  0x1000089c4: __int64 __fastcall _init_block(_QWORD)
  0x100009bb0: __int64 __fastcall _write_buf(int, int, size_t __nbyte)
  0x100009c14: __int64 __fastcall _unlzw(int, int)
  0x10000b538: void __cdecl _copy(const std::__fs::filesystem::path *__from, const
    std::__fs::filesystem::path *__to, std::__fs::filesystem::copy_options __opt,
    std::error_code *__ec)
  0x10000b73c: int __cdecl _xunlink(const char *)
  0x10000b78c: __int64 __fastcall _xmalloc(size_t __size)
  0x10000ba6c: int __cdecl _rpmatch(const char *)
  0x10000baa0: int __cdecl _getopt_long_only(int, char *const *, const char *, const
    option *, int *)
  0x10000de98: __int64 __fastcall _OUTLINED_FUNCTION_42(_QWORD)
  0x10000dfd4: void __noreturn _OUTLINED_FUNCTION_61(void)
  0x10000e6c0: int *___error(void)
  0x10000e6f0: int __cdecl ___tolower(int)
  0x10000e6fc: void __cdecl __noreturn __exit(int)
  0x10000e708: int __cdecl _atoi(const char *)
  0x10000e714: void __cdecl _bzero(void *, size_t)
  0x10000e720: void *__cdecl _calloc(size_t __count, size_t __size)
  0x10000e72c: int __cdecl _close(int)
  0x10000e738: int __cdecl _closedir(DIR *)
  0x10000e744: char *__cdecl _ctime(const time_t *)
  0x10000e750: void __cdecl __noreturn _exit(int)
  0x10000e75c: int __cdecl _fchmod(int, mode_t)
  0x10000e768: int __cdecl _fchown(int, uid_t, gid_t)
  0x10000e774: int __cdecl _fflush(FILE *)
  0x10000e780: int __cdecl _fileno(FILE *)
  0x10000e78c: int _fprintf(FILE *, const char *, ...)
  0x10000e798: int __cdecl _fputc(int, FILE *)
  0x10000e7a4: void __cdecl _free(void *)
  0x10000e7b0: int __cdecl _fstat(int, stat *)
  0x10000e7bc: size_t __cdecl _fwrite(const void *__ptr, size_t __size, size_t __nitems,
    FILE *__stream)
  0x10000e7c8: int _getchar(void)
  0x10000e7d4: char *__cdecl _getenv(const char *)
  0x10000e7e0: int __cdecl _isatty(int)
  0x10000e7ec: off_t __cdecl _lseek(int, off_t, int)
  0x10000e7f8: int __cdecl _lstat(const char *, stat *)
  0x10000e804: void *__cdecl _malloc(size_t __size)
  0x10000e810: void *__cdecl _memcpy(void *__dst, const void *__src, size_t __n)
  0x10000e81c: void *__cdecl _memset(void *__b, int __c, size_t __len)
  0x10000e828: void __cdecl _memset_pattern16(void *__b, const void *__pattern16,
    size_t __len)
  0x10000e834: int _open(const char *, int, ...)
  0x10000e840: DIR *__cdecl _opendir(const char *)
  0x10000e84c: void __cdecl _perror(const char *)
  0x10000e858: int _printf(const char *, ...)
  0x10000e864: int __cdecl _putc(int, FILE *)
  0x10000e870: int __cdecl _putchar(int)
  0x10000e87c: int __cdecl _puts(const char *)
  0x10000e888: ssize_t __cdecl _read(int, void *, size_t)
  0x10000e894: dirent *__cdecl _readdir(DIR *)
  0x10000e8a0: void (__cdecl *__cdecl _signal(int, void (__cdecl *)(int)))(int)
  0x10000e8ac: int __cdecl _stat(const char *, stat *)
  0x10000e8b8: char *__cdecl _strchr(char *__s, int __c)
  0x10000e8c4: int __cdecl _strcmp(const char *__s1, const char *__s2)
  0x10000e8d0: char *__cdecl _strcpy(char *__dst, const char *__src)
  0x10000e8dc: size_t __cdecl _strcspn(const char *__s, const char *__charset)
  0x10000e8e8: size_t __cdecl _strlen(const char *__s)
  0x10000e8f4: int __cdecl _strncmp(const char *__s1, const char *__s2, size_t __n)
  0x10000e900: char *__cdecl _strrchr(char *__s, int __c)
  0x10000e90c: size_t __cdecl _strspn(const char *__s, const char *__charset)
  0x10000e918: int __cdecl _unlink(const char *)
  0x10000e924: int __cdecl _utime(const char *, const utimbuf *)
  0x10000e930: ssize_t __cdecl _write(int __fd, const void *__buf, size_t __nbyte)
version: 0x4

```

`labsync.cfg`:

```cfg
repo_path = /path/to/labsync/data/repo
log = INFO

# advanced
lock_timeout_sec = 10

```

`labsync.py`:

```py
from __future__ import annotations

import configparser
import contextlib
import dataclasses
import functools
import heapq
import io
import logging
import os
import pathlib
import re
import string
import tempfile
import time
import uuid
from collections.abc import Generator
from typing import IO, Any, Optional

import git
import parse
import yaml

try:
    import functioninliner
except ModuleNotFoundError:
    # define a mock that will allow us to work without functioninliner installed
    class functioninliner:  # noqa: N801
        class ClonesStorage(dict):
            def update_from_storage(self) -> None:
                pass

try:
    import ida_diskio
    import ida_idaapi
    import ida_idp
    import ida_kernwin
    import ida_loader
    import ida_nalt
    import ida_name
    import ida_segment
    import ida_typeinf
    import ida_xref
    import idautils
    import netnode
    import sark
except ModuleNotFoundError:
    # define mocks to support importing outside of IDA for testing
    class ida_kernwin:  # noqa: N801
        class action_handler_t:  # noqa: N801
            pass

        class UI_Hooks:  # noqa: N801
            pass

    class ida_idaapi:  # noqa: N801
        PLUGIN_MOD = 0
        PLUGIN_HIDE = 0
        BADADDR = 0

        class plugin_t:  # noqa: N801
            pass

    class ida_idp:  # noqa: N801
        IDP_INTERFACE_VERSION = 0

    class ida_typeinf:  # noqa: N801
        class text_sink_t:  # noqa: N801
            pass

        class tinfo_t:  # noqa: N801
            pass

    class netnode:  # noqa: N801
        class Netnode:
            pass


# CONFIGURATION


# we decided not to normalize prototypes because it makes it much harder to resolve conflicts since
# you don't know which function you're looking at
#
# the downside is that a conflict on a function name change will result in two conflicts (one on
# the name and one on the prototype)
NORMALIZE_PROTOTYPES = False

# we decided not to remove names and prototypes that are missing in the YAML since some times
# exporting/importing them remotely can be an issue and so they will be removed locally as well
REMOVE_MISSING_NAMES_AND_PROTOTYPES = False

LOCAL_TYPES_COMMENT_FMT = "/* >> LABSYNC DO NOT TOUCH: {} << */"

LOCKFILE = "labsync.lock"
DEFAULT_LOCK_TIMEOUT = 60  # sec


# LOGGING


class LoggerWithTrace(logging.getLoggerClass()):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        logging.TRACE = 5
        logging.addLevelName(logging.TRACE, "TRACE")

    def trace(self, msg: str, *args, **kwargs) -> None:
        self.log(logging.TRACE, msg, *args, **kwargs)


logger = LoggerWithTrace("LabSync")


# EXCEPTIONS


class LabSyncError(Exception):
    pass


class LabSyncLockError(LabSyncError):
    pass


class LabSyncBinaryMatchingError(LabSyncError):
    pass


# HELPERS


class LabSyncYAMLDumper(yaml.CDumper):
    @staticmethod
    def _hex_representer(dumper: yaml.Dumper, data: int) -> str:
        return dumper.represent_scalar("tag:yaml.org,2002:int", hex(data))

    @staticmethod
    def _str_representer(dumper: yaml.dumper, data: str) -> str:
        if "\n" in data:
            return dumper.represent_scalar("tag:yaml.org,2002:str", data, style="|")
        else:
            return dumper.represent_scalar("tag:yaml.org,2002:str", data)


LabSyncYAMLDumper.add_representer(int, LabSyncYAMLDumper._hex_representer)  # noqa: SLF001
LabSyncYAMLDumper.add_representer(str, LabSyncYAMLDumper._str_representer)  # noqa: SLF001


@contextlib.contextmanager
def wait_box(msg: str, *, hide_cancel: bool = False) -> None:
    prefix = "HIDECANCEL\n" if hide_cancel else ""
    ida_kernwin.show_wait_box(prefix + msg)
    try:
        yield None
    finally:
        ida_kernwin.hide_wait_box()


class StringIOTextSink(ida_typeinf.text_sink_t):
    def __init__(self):
        super().__init__()
        self.sio = io.StringIO()

    def _print(self, thing: str) -> int:
        self.sio.write(thing)
        return 0


def local_types() -> Generator[str]:
    name = ida_typeinf.first_named_type(None, ida_typeinf.NTF_TYPE)
    while name:
        yield name
        name = ida_typeinf.next_named_type(None, name, ida_typeinf.NTF_TYPE)


# EXPORT LOGIC


@dataclasses.dataclass(eq=True, order=True)
class SyncedBinary:
    idb_id: str = dataclasses.field(compare=False)
    start_ea: int = 0  # must be the first field, because we rely on it when sorting
    end_ea: int = ida_idaapi.BADADDR  # exclusive
    base_ea: int = 0
    seg_prefix: Optional[str] = None

    def contains(self, ea: int) -> bool:
        return self.start_ea <= ea < self.end_ea

    def ea2dump(self, ea: int) -> int:
        return ea - self.base_ea

    def dump2ea(self, dea: int) -> int:
        return dea + self.base_ea


def dump_names(binary: SyncedBinary, storage: functioninliner.ClonesStorage) -> dict[int, str]:
    d = {}
    for ea, name in idautils.Names():
        if not binary.contains(ea):
            continue

        seg = ida_segment.getseg(ea)
        seg_name = ida_segment.get_segm_name(seg)

        # skip names that are in inlined chunks
        if seg_name.startswith("inlined_"):
            continue

        # skip names for inlined functions
        if ea in storage:
            continue

        dea = binary.ea2dump(ea)
        d[dea] = name

    return d


def dump_inlined_funcs(binary: SyncedBinary, storage: functioninliner.ClonesStorage) -> list[int]:
    funcs = (binary.ea2dump(ea) for ea in storage if binary.contains(ea))
    return list(sorted(funcs))  # noqa: C413


def stable_topological_sort(graph: dict[Any, set[Any]]) -> Generator[Any]:
    heap = []
    next_v = None

    while heap or graph:
        for v, edges in list(graph.items()):
            if next_v is not None:
                edges.discard(next_v)
            if not edges:
                heapq.heappush(heap, v)
                del graph[v]

        if not heap:
            msg = "graph contains unsolvable dependencies"
            raise ValueError(msg)

        next_v = heapq.heappop(heap)
        yield next_v


def dump_local_types(types: netnode.Netnode) -> str:
    # we emulate print_decls() ourselves because it internally uses PRTYPE_NOREGEX and this
    # removes namespaces which starts with double underscore (e.g. std::__1::__libcpp_refstring)

    local_types_by_ordinal = {}  # ordinal: (name, decl, dependencies)
    local_types_by_name = {}  # name: (ordinal, decl, dependencies)

    tinfo = ida_typeinf.tinfo_t()
    for ordinal in range(1, ida_typeinf.get_ordinal_qty(None)):
        if not tinfo.get_numbered_type(None, ordinal):
            continue  # deleted ordinal

        name = tinfo.get_type_name()

        flags = (
            ida_typeinf.PRTYPE_MULTI |  # multiline
            ida_typeinf.PRTYPE_TYPE |  # required to have it named
            ida_typeinf.PRTYPE_PRAGMA |  # include alignment pragmas
            ida_typeinf.PRTYPE_SEMI |  # end with semicolon
            ida_typeinf.PRTYPE_CPP |  # unsure if this is needed, but to be on the safe side...
            ida_typeinf.PRTYPE_DEF |  # required to have a full definition
            ida_typeinf.PRTYPE_NOREGEX  # required to keep the name as-is
        )

        decl = ida_typeinf.print_tinfo(None, 2, 0, flags, tinfo, name, None)
        decl = decl.strip()
        # also strip trailing spaces since they arn't block-encodable in YAML
        decl = "\n".join(line.rstrip() for line in decl.splitlines())

        # TODO @TH: there is an IDA bug where comment-only changes to local types are not updated
        #           when using parse_decls() so for now we just strip all comments until it'll be
        #           fixed
        decl = strip_comments(decl)

        # IDA apparently can't handle templates in parse_decls(), so we we don't bother syncing
        # them at all. hopefully no sane reverser actually uses them and these are only imported
        # from debug symbols and never touched
        if "<" in decl:
            logger.warning(
                f"skipping syncing of local type {name!r} because templates are unsupported"  # noqa: COM812
            )
            continue

        # do a sanity for the extract of the name, since it'll be used when updating
        parsed_name, _ = decl_to_name_and_type(decl)
        assert name == parsed_name

        # see if this type is dependent on others
        dependencies = set()

        udt = ida_typeinf.udt_type_data_t()
        if tinfo.get_udt_details(udt):
            for i in range(udt.size()):
                udm = udt[i]
                udm_ordinal = udm.type.get_ordinal()
                if udm_ordinal:
                    dependencies.add(udm_ordinal)

        # keep it
        local_types_by_ordinal[ordinal] = (name, decl, dependencies)
        assert name not in local_types_by_name
        local_types_by_name[name] = (ordinal, decl, dependencies)

    dep_graph = {}
    for name, _, deps in local_types_by_ordinal.values():
        dep_names = {local_types_by_ordinal[d][0] for d in deps if d in local_types_by_ordinal}
        if len(dep_names) != len(deps):
            logger.warning(
                f"skipping syncing of local type {name!r} because it depends on other non-synced "
                "local types"  # noqa: COM812
            )
            continue

        dep_graph[name] = dep_names

    # generate a normalized header file that is sorted by dependecy order and lexigraphically,
    # to make YAML diffs sane
    nhdr = io.StringIO()

    for name in stable_topological_sort(dep_graph):
        _, decl, _ = local_types_by_name[name]

        # add uuid
        tid = ida_typeinf.get_named_type_tid(name)
        if tid == ida_idaapi.BADADDR:
            msg = f"failed to resolve tid of local type {name!r}:\n{decl}"
            raise LabSyncError(msg)
        decl_uuid = types.get(tid)
        if not decl_uuid:
            types[tid] = decl_uuid = str(uuid.uuid4())

        nhdr.write(LOCAL_TYPES_COMMENT_FMT.format(decl_uuid))
        nhdr.write("\n")
        nhdr.write(decl)
        nhdr.write("\n\n")

    return nhdr.getvalue().strip()


def strip_comments(decl: str) -> str:
    stripped_lines = []

    for line in decl.splitlines():
        try:
            line = line[:line.index("//")].rstrip()
            if not line:
                continue
        except ValueError:
            pass
        stripped_lines.append(line)

    return "\n".join(stripped_lines)


@functools.cache
def decl_to_type_name_pat() -> re.Pattern:
    """this generates a pattern that tries to match the first line of a (stripped) decl to the
    its type (e.g. struct/union/typedef) and name

    the regex matches 2N groups where group 2i is "type" and group 2i+1 is name, for different
    possible subregexes. Only 2 groups (for some i) should be matched

    this regex is a bit more "allowing" than how IDA formats decls, since we also use it to
    match decls from YAMLs in which the user might've changed some whitespacing while manually
    resolving a merge conflict
    """

    # from ida.cfg:TypeNameChars
    name_chars = r"_:$()`'{}" + string.digits + string.ascii_letters
    # from blackbox testing what's allowed from name_chars as the first character
    name_first_chars = r"_$`" + string.ascii_letters

    name_pat = r"([" + re.escape(name_first_chars) + "][" + re.escape(name_chars) + "]*?)"

    type_pat = (
        r"(?!typedef)(\S+)(?=\s)"  # only match non-typedefs
        r".*?\s" +  # everything up to the name (i.e. type + attributes)
        name_pat +
        r"(?:\s*(?<!:):(?!:).*)?"  # optional inheritance or IDA syntax for data types of enums
        r"\s*;?"  # optional semicolon in case of forward declarations
    )

    fptr_typedef_pat = (
        r"(typedef)(?=\s)"  # only match typdefs
        r".*?"  # everything up to the name
        r"\*\s*" +  # the star before the name
        name_pat +
        r"\s*\)\(.*"  # match the first function def in the line, in case there is also a fptr arg
        r".*;"  # everything else
    )

    norm_typedef_pat = (
        r"(typedef)(?=\s)"  # only match typdefs
        r".*?"  # everything up to the name
        r"\**" +  # optional stars before the name
        name_pat +
        r"(?:\s*\[\s*\d*\s*\])?"  # optional array part
        r"\s*;"  # end of the typedef
    )

    pat = (
        r"^(?:"  # start of line
            r"(?:" + type_pat + ")"
        r"|"
            r"(?:" + fptr_typedef_pat + ")"
        r"|"
            r"(?:" + norm_typedef_pat + ")"
        r")$"  # end of line
    )

    return re.compile(pat)


def decl_to_name_and_type(decl: str) -> tuple[str, str]:
    # skip pragma/comment lines
    for first_line in decl.splitlines():
        if not any(first_line.lstrip().startswith(x) for x in ("#", "//")):
            break
    else:
        msg = f"empty local type:\n{decl}"
        raise LabSyncError(msg)

    # strip the first line
    first_line = first_line.strip()

    # extract the name and decl type
    pat = decl_to_type_name_pat()
    m = pat.match(first_line)

    if not m:
        msg = f"failed to parse local type:\n{decl}"
        raise LabSyncError(msg)

    decl_type = m.group(m.lastindex - 1)
    name = m.group(m.lastindex)
        # should never happen according to our regex
    assert decl_type
    assert name

    return name, decl_type


def fix_non_present_arguments(name: str, tinfo: ida_typeinf.tinfo_t, *, add: bool = True) \
        -> tuple[ida_typeinf.tinfo_t, bool]:

    def type_exists(tinfo: ida_typeinf.tinfo_t) -> bool:
        if tinfo.present():
            return True

        # originally we used just tinfo.present(), but for some reason it keeps returning False
        # even after we saved the type (as a forward declaration)
        #
        # then we used tinfo.get_ordinal() > 0 as a test, but on huge IDBs with >10k types, for
        # some reason it kept returning 0 even after we saved the type
        #
        # therefore we moved to checking if we can get the tid for the type name. you have to watch
        # out, however, since for deleted types tinfo.get_type_name() raises UnicodeDecodeError
        try:
            tname = tinfo.get_type_name()
        except UnicodeDecodeError:
            return False

        tid = ida_typeinf.get_named_type_tid(tname)
        return tid != ida_idaapi.BADADDR

    def fix_non_present(tinfo: ida_typeinf.tinfo_t) -> tuple[ida_typeinf.tinfo_t, bool]:
        tinfo_orig = tinfo.copy()

        # deref pointer/array until we reach the actual type
        depth = 0
        while depth < 128:
            if not tinfo.remove_ptr_or_array():
                break
            depth += 1
        else:
            msg = "max pointer depth reached"
            raise LabSyncError(msg)

        # if we're allowed to and this type is missing, add its base to local types
        if not type_exists(tinfo) and add:
            # add the type to local types
            if tinfo.save_type() == 0:
                tname = tinfo.get_type_name()

                logger.warning(
                    f"the prototype for {name} used type {tname} that was not present in the TIL. "
                    "we silently added it to allow syncing"  # noqa: COM812
                )

                return tinfo_orig, False
            else:
                # we can't use tname for deleted types (it raises UnicodeDecodeError)
                tinfo_clean = tinfo.copy()
                tinfo_clean.set_modifiers(0)
                tname = tinfo_clean.dstr()

                logger.warning(
                    f"the prototype for {name} used type {tname} that was not present in the TIL. "
                    "we failed to silently add it to allow syncing (perhaps a deleted type?)"  # noqa: COM812
                )

        # TODO @TH: IDA has a bug where they can't parse _BOOL8 args, so we replace them.
        #           remove this flow after they fix it
        bool8_realtype = ida_typeinf.BT_BOOL | ida_typeinf.BTMT_BOOL8
        if tinfo.get_realtype() == bool8_realtype:
            pass
        elif type_exists(tinfo):
            # if it's real present type, we're good
            return tinfo_orig, False

        # replace the type with an unknown type
        tinfo_generic = ida_typeinf.tinfo_t()
        assert tinfo_generic.create_simple_type(ida_typeinf.BT_UNKNOWN)

            # set the original modifiers
        tinfo_generic.set_modifiers(tinfo.get_modifiers())

            # recrate the pointer depth on top of tinfo_generic
        for _ in range(depth):
            assert tinfo_generic.create_ptr(tinfo_generic)

        return tinfo_generic, True

    ftype = ida_typeinf.func_type_data_t()
    assert tinfo.get_func_details(ftype, ida_typeinf.GTD_NO_ARGLOCS)

    ftype.rettype, fixed = fix_non_present(ftype.rettype)
    for i, argtype in enumerate(ftype):
        ftype[i].type, arg_fixed = fix_non_present(argtype.type)
        fixed |= arg_fixed

    tinfo_new = ida_typeinf.tinfo_t()
    assert tinfo_new.create_func(ftype)

    return tinfo_new, fixed


def prototype(ea: int) -> str:
    tinfo = ida_typeinf.tinfo_t()
    if not ida_nalt.get_tinfo(tinfo, ea):
        return None

    name = ida_name.get_ea_name(ea, ida_name.GN_VISIBLE)

    # replace non-present arguments in the prototype if relevant
    tinfo_new, fixed = fix_non_present_arguments(name, tinfo)

    if fixed:
        ptype = ida_typeinf.print_tinfo(None, 0, 0, ida_typeinf.PRTYPE_1LINE, tinfo, None, None)
        new_ptype = ida_typeinf.print_tinfo(None, 0, 0, ida_typeinf.PRTYPE_1LINE, tinfo_new, None,
                                            None)
        logger.warning(
            f"replacing prototype for {name} because it uses types that are not present in the "
            f"TIL from:\n\t{ptype!r}\nto:\n\t{new_ptype!r}"  # noqa: COM812
        )

        if not ida_nalt.set_tinfo(ea, tinfo_new):
            logger.warning(f"failed setting new prototype for {name}! skipping it")
            return None

        tinfo = tinfo_new

    # generate the prototype to dump
    if NORMALIZE_PROTOTYPES:
        name = "FUNCTION"

    # we have to remove special characters from the name, otherwise we'll have an issue applying
    # the prototype afterwards (e.g. `__Foo.cxx_destruct_`)
    allowed = r"_$" + string.digits + string.ascii_letters
    pname = "".join(c if c in allowed else "_" for c in name)
    return ida_typeinf.print_tinfo(None, 0, 0, ida_typeinf.PRTYPE_1LINE, tinfo, pname, None)


def dump_prototypes(
    binary: SyncedBinary, storage: functioninliner.ClonesStorage) -> dict[int, str]:

    d = {}
    for ea in idautils.Functions():
        if not binary.contains(ea):
            continue

        seg = ida_segment.getseg(ea)
        seg_name = ida_segment.get_segm_name(seg)

        # skip funcs that are in inlined chunks somehow (shouldn't happen)
        if seg_name.startswith("inlined_"):
            continue

        # skip funcs that have been inlined
        if ea in storage:
            continue

        ptype = prototype(ea)
        if ptype:
            dea = binary.ea2dump(ea)
            d[dea] = ptype

    return d


def dump(binary: SyncedBinary, types: netnode.Netnode) -> str:
    storage = functioninliner.ClonesStorage()
    storage.update_from_storage()

    d = {
        "version": 4,
        "names": dump_names(binary, storage),
        "inlined_funcs": dump_inlined_funcs(binary, storage),
        # we have to dump prototypes before we dump local types because this may add new types to
        # the TIL
        "prototypes": dump_prototypes(binary, storage),
        "local_types": dump_local_types(types),
    }

    return yaml.dump(
        d, Dumper=LabSyncYAMLDumper, default_flow_style=False, sort_keys=True,
    )


# IMPORT LOGIC


def update_names(
    binary: SyncedBinary, storage: functioninliner.ClonesStorage, names: dict[int, str]) -> None:

    # delete names if required
    if REMOVE_MISSING_NAMES_AND_PROTOTYPES:
        for dea in dump_names(binary, storage):
            # delete name if unnamed in the new dict
            if dea not in names:
                ea = binary.dump2ea(dea)

                msg = f"removing name from {ea:#x}"
                logger.debug(msg)

                success = ida_name.set_name(ea, "", ida_name.SN_NOWARN)
                if not success:
                    if logger.getEffectiveLevel() > logging.DEBUG:
                        logger.warning("failed " + msg)
                    else:
                        logger.warning("removal failed!")

    # update names
    for dea, name in names.items():
        ea = binary.dump2ea(dea)
        cur_name = ida_name.get_name(ea)

        if cur_name != name:
            msg = f"renaming {ea:#x} from {cur_name!r} to {name!r}"
            logger.debug(msg)

            # check if the new name already exists in the database
            cur_ea = ida_name.get_name_ea(ida_idaapi.BADADDR, name)
            name_changed = False
            try:
                # if the new name is already in use in the IDB --
                if cur_ea != ida_idaapi.BADADDR:
                    # verify that the repo also has a different name for the EA currently holding
                    # the new name
                    #
                    # perhaps we can even assert that this never happens
                    cur_dea = binary.ea2dump(cur_ea)
                    if cur_dea not in names:
                        logger.warning(
                            f"cannot rename {ea:#x} to {name!r} as this name already "
                            f"exists in the IDB for {cur_ea:#x}, and that EA doesn't "
                            "have a different name in the repo"  # noqa: COM812
                        )
                        continue

                    # temporarily rename it to something else
                    msg2 = f"temporarily renaming {cur_ea:#x} away from {name!r}"
                    logger.debug("\t" + msg2)
                    success = ida_name.set_name(
                        cur_ea,
                        name + "_labsync_temp",
                        ida_name.SN_NOWARN | ida_name.SN_FORCE,
                    )

                    # handle temporary rename failure
                    if not success:
                        if logger.getEffectiveLevel() > logging.DEBUG:
                            logger.warning("failed " + msg2)
                        else:
                            logger.warning("\ttemporary rename failed!")
                        continue

                # now do the actual rename
                success = ida_name.set_name(ea, name, ida_name.SN_NOWARN)

                # handle rename failure
                if not success:
                    if logger.getEffectiveLevel() > logging.DEBUG:
                        logger.warning("failed " + msg)
                    else:
                        logger.warning("rename failed!")
                    continue

                name_changed = True
            finally:
                # if we failed, undo the temporary rename if we did any
                if cur_ea != ida_idaapi.BADADDR and not name_changed:
                    msg2 = f"undoing the temporarily rename of {cur_ea:#x}"
                    logger.debug("\t" + msg2)
                    success = ida_name.set_name(cur_ea, name, ida_name.SN_NOWARN)

                    # handle temporary rename undoing failure
                    if not success:
                        if logger.getEffectiveLevel() > logging.DEBUG:
                            logger.warning("failed " + msg2)
                        else:
                            logger.warning("\ttemporary rename undoing failed!")


def update_inlined_funcs(
    binary: SyncedBinary, storage: functioninliner.ClonesStorage, funcs: list[int]) -> None:

    cur = {ea for ea in storage if binary.contains(ea)}
    new = set(map(binary.dump2ea, funcs))

    undo = cur - new
    do = new - cur

    for ea in undo:
        func = sark.Function(ea)
        if func.ea != ea:
            logger.warning(f"\tcannot undo inlining of {ea:#x} since it's not a function start!")
            continue
        msg = f"undoing inlining of {func.name}"
        logger.debug(msg)
        functioninliner.undo_inline_function(func)

    for ea in do:
        func = sark.Function(ea)
        if func.ea != ea:
            logger.warning(
                f"not inlining function @ {ea:#x} since it's not a function start"  # noqa: COM812
            )
        logger.debug(f"inlining {func.name}")
        functioninliner.inline_function(func)


def _rename_local_type(tid: int, name: str) -> tuple[int, str]:
    """note: if name is in use by another local type, that local type will be removed"""

    ordinal = ida_typeinf.get_tid_ordinal(tid)
    assert ordinal

    tinfo = ida_typeinf.tinfo_t()
    assert tinfo.get_numbered_type(None, ordinal)

    # TODO  @TH: perhaps tinfo.rename_type can be used instead? found out about it later
    err = tinfo.set_numbered_type(None, ordinal, ida_typeinf.NTF_REPLACE, name)
    errstr = ida_typeinf.tinfo_errstr(err)
    return err, errstr


def rename_local_type(
    cur_name: str, name: str, types: netnode.Netnode, uuids: dict[int, str],
) -> None:

    msg = f"renaming local type {cur_name!r} to {name!r}"
    logger.debug(msg)

    # resolve the type we're changing
    tid = ida_typeinf.get_named_type_tid(cur_name)
    assert tid != ida_idaapi.BADADDR

    # check if the new name is in use
    cur_tid = ida_typeinf.get_named_type_tid(name)
    name_changed = False
    try:
        # if the new name is already in use in the IDB --
        if cur_tid != ida_idaapi.BADADDR:
            # assert that the repo also has a different name for the type currently holding the new
            # name
            #
            # this is an assertion because if it's missing from the repo we should've already
            # deleted it
            #
            # also, we don't actually verify that the name since we already verified beforehand
            # that there are no duplicate names
            cur_uuid = types[cur_tid]
            assert cur_uuid in uuids

            # temporarily rename it to something else
            msg2 = f"temporarily renaming local type {name!r}"
            logger.debug("\t" + msg2)
            err, errstr = _rename_local_type(cur_tid, name + "_labsync_temp")

            # handle temporary rename failure
            if err:
                if logger.getEffectiveLevel() > logging.DEBUG:
                    logger.warning("failed " + msg2 + f": {errstr}")
                else:
                    logger.warning(f"\ttemporary rename failed: {errstr}")
                return False

        # now do the actual rename
        err, errstr = _rename_local_type(tid, name)

        # handle rename failure
        if err:
            if logger.getEffectiveLevel() > logging.DEBUG:
                logger.warning("failed " + msg + f": {errstr}")
            else:
                logger.warning(f"rename failed: {errstr}")
            return False

        name_changed = True
    finally:
        # if we failed, undo the temporary rename if we did any
        if cur_tid != ida_idaapi.BADADDR and not name_changed:
            msg2 = f"undoing the temporarily rename of local type {name!r}"
            logger.debug("\t" + msg2)
            err, errstr = _rename_local_type(cur_tid, name)

            # handle temporary rename undoing failure
            if err:
                if logger.getEffectiveLevel() > logging.DEBUG:
                    logger.warning("failed " + msg2 + f": {errstr}")
                else:
                    logger.warning(f"\ttemporary rename undoing failed: {errstr}")

    return bool(err)


def parse_local_types(nhdr: str) -> Generator[tuple[str, str, str, str]]:
    # split according to empty lines
    decls = re.split(r"\n\n", nhdr)

    for decl in decls:
        decl = decl.strip()

        # extract the uuid of the type
        uuid_line, decl = decl.split("\n", maxsplit=1)
        r = parse.parse(LOCAL_TYPES_COMMENT_FMT, uuid_line)
        if not r:
            msg = f"failed to extract uuid from local type uuid line:\n{uuid_line}"
            raise LabSyncError(msg)
        decl_uuid = r.fixed[0]

        # extract the name of the type and generate a forward declaration for it
        name, decl_type = decl_to_name_and_type(decl)

        yield name, decl_uuid, decl, decl_type


def update_local_types(nhdr: str, types: netnode.Netnode) -> None:
    # parse type declaration names
    name2decl = {}
    uuids = {}
    decls = []
    fdecls = []
    typedefs = []
    for name, decl_uuid, decl, decl_type in parse_local_types(nhdr):
        # make sure it's unique, mostly for sanity purposes
        if name in name2decl:
            if (name2decl[name] == decl and
                uuids.get(decl_uuid) == name):
                # duplicate local type with same decl and UUID. probably accidentally copied from
                # both sides during conflict resolution. we'll skip the redundant copy
                continue

            msg = f"found two local type declarations with the same name: {name}"
            raise LabSyncError(msg)
        name2decl[name] = decl

        # remember the name for each uuid
        uuids[decl_uuid] = name

        # accumulate
        if decl_type in {"struct", "union", "enum", "class"}:
            decls.append(decl)

            fdecl = f"{decl_type} {name};"
            fdecls.append(fdecl)
        elif decl_type == "typedef":
            typedefs.append(decl)
        else:
            msg = f"found unexpected kind of local type:\n{decl}"
            raise LabSyncError(msg)

    # remove local types if required
    for tid, decl_uuid in list(types.items()):
        if decl_uuid not in uuids:
            name = ida_typeinf.get_tid_name(tid)
            logger.debug(f"removing local type {name!r}")
            ida_typeinf.del_named_type(None, name, ida_typeinf.NTF_TYPE)
            del types[tid]

    # rename local types if required
        # create a mapping from uuid to type name in our IDB
    cur_uuid_to_name = {u: ida_typeinf.get_tid_name(t) for t, u in types.items()}

    for decl_uuid, name in uuids.items():
        # skip if uuid doesn't exist (this is a new type) or name didn't change
        cur_name = cur_uuid_to_name.get(decl_uuid)
        if not cur_name or cur_name == name:
            continue

        # do the renaming
        rename_local_type(cur_name, name, types, uuids)

    # create a reordered header that that should be parsable with regards to forward declarations
    hdr = "\n".join(fdecls) + "\n\n" + "\n".join(typedefs) + "\n\n" + "\n\n".join(decls)

    # iteratively try to load the header as long as dependencies get resolved
    #
    # TODO @TH: originally we didn't move the typedefs to before the decls, so honestly I think
    #           that now there should always be just one iteration here. we should probably verify
    #           that and remove the loop here afterwards
    last_n_errors = float("inf")
    iters = 1
    while iters < len(name2decl) + 1:
        # parse some more local types
        n_errors = ida_typeinf.parse_decls(None, hdr, None, ida_typeinf.HTI_DCL)
        logger.debug(f"loaded local types with {n_errors} errors")

        # add new uuids. we do this on every iteration in case we will eventually bail out --
        # we don't want to have added new types without keeping their uuids
        for decl_uuid, name in uuids.items():
            tid = ida_typeinf.get_named_type_tid(name)
            if tid != ida_idaapi.BADADDR:
                if tid not in types:
                    types[tid] = decl_uuid
                else:
                    assert types[tid] == decl_uuid

        # stop if there are no more errors or if we didn't add anything on this iteration
        if n_errors == 0 or n_errors >= last_n_errors:
            break

        last_n_errors = n_errors
        iters += 1
    else:
        msg = "local type loading took more than it makes sense. aborting"
        raise LabSyncError(msg)

    logger.debug(
        f"finished loading local types with {n_errors} errors after {iters} iterations "
        f"({len(name2decl)} types)"  # noqa: COM812
    )

    if n_errors:
        # IDA SDK doesn't properly export an interface for printer_t so we can't get the actual
        # errors :/
        fd, hdr_path = tempfile.mkstemp(suffix=".h", text=True)
        os.write(fd, hdr.encode("latin1"))
        os.close(fd)

        logger.error(
            "run the following in IDC shell to see the local types parsing errors:\n"
            f'\tparse_decls("{hdr_path}", PT_FILE)'  # noqa: COM812
        )

        msg = f"failed to parse local types ({n_errors} errors)"
        raise LabSyncError(msg)

    # in case loading the local types resulted in a new type being created, we might've encountered
    # a bug where IDA recreates an anonymous local type for an unnamed embedded subtype
    #
    # in that case, look for and delete dangling anonymous local types that should've been left
    new_names = set(local_types())
    if set(name2decl.keys()) != new_names:
        for name in new_names:
            tinfo = ida_typeinf.tinfo_t()
            assert tinfo.get_named_type(None, name)

            # check if anonymous
            if not tinfo.is_anonymous_udt():
                continue

            # check if it has any xrefs
            tid = tinfo.get_tid()
            if (ida_xref.get_first_cref_to(tid) != ida_idaapi.BADADDR or
                ida_xref.get_first_dref_to(tid) != ida_idaapi.BADADDR):
                continue

            # check if it's referenced by any typedef
            used_by_typedef = False

            any_tinfo = ida_typeinf.tinfo_t()
            for any_name in local_types():
                assert any_tinfo.get_named_type(None, any_name)

                if not any_tinfo.is_typedef():
                    continue

                if any_tinfo.get_next_type_name() != name:
                    continue

                used_by_typedef = True
                break

            if used_by_typedef:
                continue

            # remove it
            logger.debug(f"removing dangling local type {name!r}")
            ida_typeinf.del_named_type(None, name, ida_typeinf.NTF_TYPE)
            del types[tid]


def update_prototypes(
    binary: SyncedBinary, storage: functioninliner.ClonesStorage, d: dict[int, str]) -> None:

    # delete prototypes if required
    if REMOVE_MISSING_NAMES_AND_PROTOTYPES:
        # dump_prototypes() is a bit heavy, so we duplicate some code here instead of calling it
        # because we don't need the actual prototypes
        for ea in idautils.Functions():
            seg = ida_segment.getseg(ea)
            seg_name = ida_segment.get_segm_name(seg)

            # skip funcs that are in inlined chunks somehow (shouldn't happen)
            if seg_name.startswith("inlined_"):
                continue

            # skip funcs that have been inlined
            if ea in storage:
                continue

            # delete type if untyped in the new dict
            dea = binary.ea2dump(ea)
            if dea not in d:
                logger.debug(f"removing prototype from {ea:#x}")
                ida_nalt.del_tinfo(ea)

    # update prototypes
    for dea, ptype in d.items():
        ea = binary.dump2ea(dea)
        cur_ptype = prototype(ea)

        # TODO @TH: because we had numerous issues with prototypes syncing, we want to make sure
        #           that we always apply all prototypes so that issues will be raised early.
        #           we can probably undo it after we feel more confident
        # if cur_ptype != ptype:
        if True:
            msg = f"changing {ea:#x} prototype from:\n\t{cur_ptype!r}\nto\n\t{ptype!r}"
            if cur_ptype != ptype:
                logger.debug(msg)

            success = ida_typeinf.apply_cdecl(
                None, ea, ptype + ";", ida_typeinf.TINFO_DEFINITE,
            )

            if not success:
                if logger.getEffectiveLevel() > logging.DEBUG or cur_ptype == ptype:
                    logger.warning("failed " + msg)
                else:
                    logger.warning("prototype change failed!")


def migrate(d: dict, types: netnode.Netnode) -> dict:  # noqa: ARG001
    ver = d["version"]
    if ver not in {1, 2, 3, 4}:
        msg = f"data file is of unexpected version: {ver}"
        raise LabSyncError(msg)

    # version 4 is latest
    if ver == 4:
        return d

    # version 3 was missing local type uuids
        # we dropped the logic here since it was non-trivial to update per newer changes in code
        # and this version is very old anyway
    msg = ("migrating from YAML version 3 is not supported. use an older version of LabSync to do "
           "so")
    raise NotImplementedError(msg)

    if ver == 3:
        return d

    # version 2 was missing prototypes
    d["prototypes"] = dump_prototypes()

    if ver == 2:
        return d

    # version 1 was missing local types
    d["local_types"] = dump_local_types()

    return d


def parse_data(data: str, types: netnode.Netnode) -> dict:
    d = yaml.load(data, Loader=yaml.CLoader)  # noqa: S506
    return migrate(d, types)


def update(binary: SyncedBinary, data: str, types: netnode.Netnode) -> None:
    storage = functioninliner.ClonesStorage()
    storage.update_from_storage()

    d = parse_data(data, types)

    update_names(binary, storage, d["names"])
    update_inlined_funcs(binary, storage, d["inlined_funcs"])
    update_local_types(d["local_types"], types)
    update_prototypes(binary, storage, d["prototypes"])


def adopt_uuids(data: str, types: netnode.Netnode) -> None:
    # extract local types normalized header
    d = parse_data(data, types)
    nhdr = d["local_types"]

    # parse uuids
    name_to_uuid = {}
    for name, decl_uuid, _, _ in parse_local_types(nhdr):
        if name in name_to_uuid:
            msg = (
                f"found two local type declarations with the same name: {name} (uuids "
                f"{name_to_uuid[name]} and {decl_uuid})"
            )
            raise LabSyncError(msg)

        name_to_uuid[name] = decl_uuid

    # adopt uuids for local types we don't already have uuids for, if their names match
    for name in local_types():
        tid = ida_typeinf.get_named_type_tid(name)
        if tid == ida_idaapi.BADADDR:
            msg = f"failed to resolve tid of local type {name!r}"
            raise LabSyncError(msg)

        # skip local types that already have a uuid
        if tid in types:
            continue

        # adopt the uuid from the repo if it has a local type with the same name (best effort
        # heuristic)
        if name in name_to_uuid:
            decl_uuid = name_to_uuid[name]
            logger.debug(f"adopting repo uuid for local type {name!r}: {decl_uuid}")
            types[tid] = decl_uuid


# REPO


class LabSyncRepo:
    _path: str
    _repo: git.Repo

    def __init__(self, path: str):
        # TODO @TH: we should add a mechanism to lock the repo while we're doing git operations on
        #           it, in case multiple IDBs are using it at the same time

        self._path = path
        self._repo = git.Repo(path)

        with self._repo.config_reader() as cr:
            try:
                mergetool = cr.get("merge", "tool")
                logger.info(f"using merge.tool = {mergetool}")
            except (configparser.NoOptionError, configparser.NoSectionError):
                msg = "merge.tool must be set in git configuration to use this plugin"
                raise LabSyncError(msg)  # noqa: B904

        if self._repo.active_branch.name != "master":
            logger.warning("data repo is not on branch 'master'")

        with self._repo.config_writer() as cw:
            cw.set_value("mergetool", "keepbackup", "false")
            cw.set_value("mergetool", "writetotemp", "true")
            cw.set_value("mergetool", "hideresolved", "true")

    def _is_clean(self) -> bool:
        return not (self._repo.is_dirty() or self._repo.untracked_files)

    def _ensure_clean(self) -> None:
        if self._repo.is_dirty():
            msg = "data repo is dirty. please fix this externally for now"
            raise LabSyncError(msg)

        if self._repo.untracked_files:
            msg = "data repo has untracked files. please fix this externally for now"
            raise LabSyncError(msg)

    @property
    def path(self) -> str:
        return self._path

    def _pull(self) -> None:
        assert self._is_clean()

        # resolve remote tracking branch
        current = self._repo.active_branch
        tracking_branch = current.tracking_branch()
        if not tracking_branch:
            logger.warning("no remote tracking branch. skipping pull")
            return

        # pull
        try:
            self._repo.git.pull(rebase="false", allow_unrelated_histories=True)
        except git.GitCommandError as e:
            if "fix conflicts and then commit" not in e.stdout:
                raise

            logger.warning(f"pull failed with: {e.stdout}")
        else:
            return

        # resolve conflicts
        try:
            self._repo.git.mergetool("--no-prompt", "--gui")
        except git.GitCommandError as e:
            msg = (
                "mergetool returned failure. please fix the conflict and conclude the merge "
                "manually"
            )
            raise LabSyncError(msg) from e

        # conclude the merge
        self._repo.git.commit("--no-edit")

        # make sure that we're done
        if self._repo.is_dirty():
            msg = (
                "data repo is still dirty after supposedly committing the fixed merge. please "
                "fix the repo manually"
            )
            raise LabSyncError(msg)

    def _id_path(self, _id: str) -> pathlib.Path:
        return pathlib.Path(self.path) / f"{_id}.yaml"

    @contextlib.contextmanager
    def _open(self, _id: str, mode: str) -> IO:
        path = self._id_path(_id)
        with path.open(mode) as fp:
            yield fp

    def _commit(self, _id: str) -> None:
        index = self._repo.index
        index.add([_id + ".yaml"])
        # this is broken for some reason: index.commit(f"updated {_id}")
        self._repo.git.commit("-m", f"updated {_id}")

    def _commit_dangling(self, _id: str) -> None:
        index = self._repo.index
        index.add([_id + ".yaml"])
        index.commit(f"initial commit for {_id}", [])

    def _push(self) -> None:
        assert self._is_clean()

        # resolve remote tracking branch
        current = self._repo.active_branch
        tracking_branch = current.tracking_branch()
        if not tracking_branch:
            logger.warning("no remote tracking branch. skipping push")
            return

        # push
        self._repo.git.push()

    def get(self, _id: str) -> tuple[Optional[str], str]:
        self._ensure_clean()

        self._pull()

        commit = self._repo.commit().hexsha

        if self._id_path(_id).exists():
            with self._open(_id, "rt") as fp:
                data = fp.read()
        else:
            data = None

        return data, commit

    def put(self, _id: str, content: str, *, base: Optional[str]) -> tuple[str, bool]:
        self._ensure_clean()

        if base and self._repo.head.commit.hexsha != base:
            # we need to fetch before resetting, because if the IDB was copied from somewhere,
            # it could be that we don't locally have the base commit
            logger.debug("fetching repo")
            self._repo.git.fetch()

            logger.debug(f"resetting HEAD to {base}")
            self._repo.head.reset(base, working_tree=True)

        with self._open(_id, "wt") as fp:
            fp.write(content)

        if self._is_clean():
            logger.debug("no changes")

            changed = False
        else:
            if base:
                self._commit(_id)
            else:
                logger.debug("committing to a danging commit")
                self._commit_dangling(_id)

            changed = True

        return self._repo.commit().hexsha, changed

    def sync(self) -> str:
        self._pull()
        self._push()

        return self._repo.commit().hexsha

    def ping(self) -> bool:
        try:
            self._repo.git.ls_remote(heads=True)
        except git.GitCommandError:
            return False
        else:
            return True


# PLUGIN STUFF


class LabSyncActionBase(ida_kernwin.action_handler_t):
    plugin: ida_idaapi.plugin_t

    def __init__(self, plugin: ida_idaapi.plugin_t):
        super().__init__()
        self.plugin = plugin

    @property
    def name(self) -> str:
        return f"{self.plugin.wanted_name}:{self.__class__.__name__}"

    @property
    def label(self) -> str:
        raise NotImplementedError

    @property
    def shortcut(self) -> Optional[str]:
        return None

    @property
    def tooltip(self) -> Optional[str]:
        return None

    @property
    def icon(self) -> int:
        return 0

    @property
    def flags(self) -> int:
        return 0

    @property
    def path(self) -> str:
        return f"Edit/Plugins/{self.plugin.wanted_name}/"

    def register(self) -> None:
        desc = ida_kernwin.action_desc_t(
            self.name,
            self.label,
            self,
            self.shortcut,
            self.tooltip,
            self.icon,
        )
        ida_kernwin.register_action(desc)

    def unregister(self) -> None:
        ida_kernwin.unregister_action(self.name)

    def activate(self, ctx: Any) -> None:
        raise NotImplementedError

    def update(self, ctx: Any) -> None:
        raise NotImplementedError


class LabSyncEnableAction(LabSyncActionBase):
    @property
    def label(self) -> str:
        return "Enable"

    def activate(self, ctx: Any) -> None:
        self.plugin.enable()

    def update(self, ctx: Any) -> None:
        if self.plugin.enabled:
            return ida_kernwin.AST_DISABLE
        else:
            return ida_kernwin.AST_ENABLE


class LabSyncDisableAction(LabSyncActionBase):
    @property
    def label(self) -> str:
        return "Disable"

    def activate(self, ctx: Any) -> None:
        self.plugin.disable()

    def update(self, ctx: Any) -> None:
        if self.plugin.enabled:
            return ida_kernwin.AST_ENABLE
        else:
            return ida_kernwin.AST_DISABLE


class LabSyncResetAction(LabSyncActionBase):
    @property
    def label(self) -> str:
        return "Reset"

    def activate(self, ctx: Any) -> None:
        self.plugin.reset()

    def update(self, ctx: Any) -> None:
        if self.plugin.enabled:
            return ida_kernwin.AST_DISABLE
        else:
            return ida_kernwin.AST_ENABLE


class LabSyncHooks(ida_kernwin.UI_Hooks):
    plugin: ida_idaapi.plugin_t
    menu_actions: list[LabSyncActionBase]

    def __init__(self, plugin: ida_idaapi.plugin_t, menu_actions: list[LabSyncActionBase]):
        super().__init__()

        self.plugin = plugin
        self.menu_actions = menu_actions

    def ready_to_run(self) -> None:
        for action in self.menu_actions:
            ida_kernwin.attach_action_to_menu(
                action.path, action.name, ida_kernwin.SETMENU_APP,
            )

    def saved(self) -> None:
        if not self.plugin.enabled:
            return

        if self.plugin.sync_in_progress:
            return

        # TODO @TH: we need to figure out a way not to have this run on "Save as...".
        #           or if we do want it to run -- get_path(PATH_TYPE_IDB) here will still hold the
        #           "old" path so reinvoke the 2nd save differently
        changed = self.plugin.sync()
        if changed:
            logger.debug("resaving the idb with synced changes")
            idb_name = ida_loader.get_path(ida_loader.PATH_TYPE_IDB)
            # TODO @TH: can we somehow find the flags the original save was invoked with and reuse
            #           them?
            ida_loader.save_database(idb_name, 0)


class LabSyncPlugin(ida_idaapi.plugin_t):
    version: int = ida_idp.IDP_INTERFACE_VERSION
    flags: int = ida_idaapi.PLUGIN_MOD | ida_idaapi.PLUGIN_HIDE

    comment: str = "helps to partially synchronize IDBs over git"
    help: str = ""
    wanted_name: str = "LabSync"
    wanted_hotkey: str = ""

    menu_actions_types: list[type[LabSyncActionBase], ...] = \
        (LabSyncEnableAction, LabSyncDisableAction, LabSyncResetAction)

    CFGFILE: str = "labsync.cfg"
    NETNODE: str = "$ labsync.plugin"
    TYPES_NETNODE: str = "$ labsync.types"
    BINARIES_NETNODE: str = "$ labsync.binaries"

    COMMIT_KEY_PREFIX = "commit."

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.menu_actions = []
        self.hooks = None

        self._sync_in_progress = False

    @classmethod
    @property
    @functools.cache
    def netnode(cls) -> netnode.Netnode:
        """this netnode holds:
            commit.<idb_id>: Optional[str]  # the git commit this binary is synced to
            enabled: bool  # whether LabSync is enabled
            custom_idb_id: Optional[str]  # override of the IDB id to use
        """
        return netnode.Netnode(LabSyncPlugin.NETNODE)

    @classmethod
    @functools.cache
    def types(cls, idb_id: str) -> netnode.Netnode:
        """this netnode matches:
                tid [int]  # of a local type
            to
                uuid [str]  # of it in the YAML
        """
        name = f"{LabSyncPlugin.TYPES_NETNODE}.{idb_id}"
        return netnode.Netnode(name)

    @classmethod
    @property
    @functools.cache
    def _binaries_netnode(cls) -> netnode.Netnode:
        """this netnode matches:
                seg_prefix [str]  # prefix of segment names
            to
                tuple[str, int]  # of (<idb id>, <base ea>) to use for mapping segments to separate
                                 # YAMLs
        """
        return netnode.Netnode(LabSyncPlugin.BINARIES_NETNODE)

    @classmethod
    @property
    @functools.cache
    def cfg(cls) -> dict:
        path = (
            pathlib.Path(ida_diskio.get_user_idadir()) / ida_diskio.CFG_SUBDIR /
            LabSyncPlugin.CFGFILE
        )
        try:
            cfg_raw = path.open("rt").read()
        except OSError as e:
            logger.error(f"failed to read configuration file from {path!r}: {e}")  # noqa: TRY400
            return None

        cfg_parser = configparser.ConfigParser()
        try:
            cfg_parser.read_string("[section]\n" + cfg_raw)
        except Exception:
            logger.exception("failed to parse configuration file")
            return None

        return dict(cfg_parser["section"].items())

    @classmethod
    @property
    @functools.cache
    def repo(cls) -> LabSyncRepo:
        # validate/apply basic configuration
        if "repo_path" not in cls.cfg:
            logger.error("repo_path not found in configuration file")
            return None

        try:
            repo = LabSyncRepo(cls.cfg["repo_path"])
        except Exception:
            logger.exception("configured repo_path does not point to a valid git repo")
            return None

        logger.info(f"using repo: {repo.path}")

        return repo

    @classmethod
    @property
    def idb_id(cls) -> str:
        default_id = ida_nalt.retrieve_input_file_md5().hex()
        return cls.netnode.get("custom_idb_id", default_id)

    @classmethod
    def commit_key(cls, idb_id: str) -> str:
        return f"{cls.COMMIT_KEY_PREFIX}{idb_id}"

    def _register(self) -> None:
        for t in LabSyncPlugin.menu_actions_types:
            a = t(self)
            a.register()
            self.menu_actions.append(a)

        self.hooks = LabSyncHooks(self, self.menu_actions)
        self.hooks.hook()

    def _deregister(self) -> None:
        if self.hooks:
            self.hooks.unhook()

        for a in self.menu_actions:
            a.unregister()

    @staticmethod
    def is_compatible() -> bool:
        info = ida_idaapi.get_inf_structure()
        return info.procname == "ARM" and info.is_64bit()

    @staticmethod
    def _init_logging() -> None:
        logger_formatter = logging.Formatter(
            fmt="{name}.{levelname:<5s}: {message}", style="{",
        )

        logger_hdlr = logging.StreamHandler()
        logger_hdlr.setFormatter(logger_formatter)

        logger.addHandler(logger_hdlr)

        # may be overridden with the 'log' config entry
        logger.setLevel(logging.INFO)

    def migrate(self) -> None:
        # migrate types to be per binary
        old_types = netnode.Netnode(LabSyncPlugin.TYPES_NETNODE)
        new_types = self.types(self.idb_id)
        for tid, _uuid in list(old_types.items()):
            new_types[tid] = _uuid
            del old_types[tid]

        # migrate commit to be per binary
        old_commit = self.netnode.get("commit")
        if old_commit is not None:
            self.netnode[self.commit_key(self.idb_id)] = old_commit
            del self.netnode["commit"]

    def init(self) -> int:
        LabSyncPlugin._init_logging()

        if not self.cfg or not self.repo:
            return ida_idaapi.PLUGIN_SKIP

        for binary in self.binaries:
            if binary.seg_prefix is None:
                name = "main"
            else:
                name = f"{binary.seg_prefix}*"
            logger.info(f"{name} idb id: {binary.idb_id}")

        if self.enabled:
            logger.info("syncing enabled")
        else:
            logger.info("syncing disabled")

        self.migrate()

        for binary in self.binaries:
            commit = self.netnode.get(self.commit_key(binary.idb_id))
            if self.enabled or commit:
                logger.info(
                    f"{binary.idb_id} last synced to commit: {commit}",
                )

        log_level = self.cfg.get("log")
        if log_level:
            logger.setLevel(log_level)

        self._register()

        logger.info("initialized successfully")

        return ida_idaapi.PLUGIN_KEEP

    def term(self) -> None:
        self._deregister()

    def run(self, arg: int = 0) -> None:
        pass

    @classmethod
    @property
    def _lockfile_path(cls) -> pathlib.Path:
        return pathlib.Path(__file__).parent / LOCKFILE

    @contextlib.contextmanager
    def _lock(
        self, wait_timeout: Optional[float] = None,
    ) -> contextlib.AbstractContextManager[None]:
        """it makes more sense to logically lock the LabSyncRepo and not LabSyncPlugin, but it's
        problematic because we want to use a lockfile which orignally wasn't in the .gitignore of
        that repo (and so syncing older IDBs will fail because we'll create the lockfile and that
        will make the repo dirty)
        """

        deadline = time.monotonic() + wait_timeout
        while True:  # we want to try once even if the timeout is 0
            logger.debug("taking lock on the repo")
            try:
                fd = os.open(self._lockfile_path, os.O_CREAT | os.O_EXCL | os.O_WRONLY)
                break
            except FileExistsError:
                pass

            if time.monotonic() >= deadline:
                logger.error(
                    "failed to get lock on the data repository within the configured timeout. "
                    "either another IDA instance is still syncing, or something bad happened "
                    "during a previous sync. if you're sure it's safe, you can delete the "
                    f"lockfile manually: {self._lockfile_path}"  # noqa: COM812
                )
                msg = "failed to get repo lock"
                raise LabSyncLockError(msg)

        try:
            yield
        finally:
            logger.debug("releasing lock on the repo")
            os.close(fd)
            self._lockfile_path.unlink()

    @property
    def sync_in_progress(self) -> bool:
        return self._sync_in_progress

    @classmethod
    @property
    def binaries(cls) -> tuple[SyncedBinary, ...]:
        rules = dict(cls._binaries_netnode.items())
        return cls._binaries_from_mapping_rules(rules)

    @classmethod
    def _binaries_from_mapping_rules(
        cls, rules: dict[str, tuple[str, Optional[int]]]) -> tuple[SyncedBinary, ...]:

        if not rules:
            return (SyncedBinary(idb_id=cls.idb_id),)

        # generate a synced binary per every segment matching rule and the default
        seg2bin = {}
        for seg_prefix, (idb_id, base_ea) in rules.items():
            # start with start_ea/end_ea pointing at max/min, and we'll have update them in the
            # next loop
            seg2bin[seg_prefix] = SyncedBinary(
                idb_id=idb_id, start_ea=ida_idaapi.BADADDR, end_ea=0, base_ea=base_ea,
                seg_prefix=seg_prefix,
            )
        default_bin = SyncedBinary(
            idb_id=cls.idb_id, start_ea=ida_idaapi.BADADDR, end_ea=0, seg_prefix=None,
        )

        # find the start/end EAs for each synced binary
        for i in range(ida_segment.get_segm_qty()):
            seg = ida_segment.getnseg(i)
            name = ida_segment.get_segm_name(seg)

            # skip inlined chunks segments
            if name.startswith("inlined_"):
                continue

            matched = False
            for seg_prefix, binary in seg2bin.items():
                if name.startswith(seg_prefix):
                    binary.start_ea = min(binary.start_ea, seg.start_ea)
                    binary.end_ea = max(binary.end_ea, seg.end_ea)
                    matched = True

            if not matched:
                default_bin.start_ea = min(default_bin.start_ea, seg.start_ea)
                default_bin.end_ea = max(default_bin.end_ea, seg.end_ea)

        # remove synced binaries that didn't match any segment
        for seg, _bin in list(seg2bin.items()):
            if _bin.start_ea == ida_idaapi.BADADDR:
                del seg2bin[seg]
        if default_bin.start_ea == ida_idaapi.BADADDR:
            assert default_bin.end_ea == 0
            default_bin.start_ea = 0  # make it empty

        # make sure that none of the synced binaries overlap
        last_bin = None
        last_seg = None
        for _bin, seg in sorted(
                [(_bin, seg) for (seg, _bin) in seg2bin.items()] + [(default_bin, "<default>")],
            ):

            assert _bin.start_ea <= _bin.end_ea
            if last_bin:
                assert last_bin.start_ea <= _bin.start_ea
                if last_bin.end_ea > _bin.start_ea:
                    msg = f"Found overlapping segments for {last_seg!r} and {seg!r}"
                    raise LabSyncBinaryMatchingError(msg)

            last_bin = _bin
            last_seg = seg

        # validate/set base_ea-s
        for seg, _bin in seg2bin.items():
            if _bin.base_ea is None:
                _bin.base_ea = _bin.start_ea
            elif _bin.base_ea > _bin.start_ea:
                msg = f"Base EA for {seg!r} is greater than its start EA"
                raise LabSyncBinaryMatchingError(msg)

        binaries = (default_bin, *seg2bin.values())

        # make sure the we don't have overlapping IDB ids
        assert len(binaries) == len({b.idb_id for b in binaries})

        return binaries

    @classmethod
    def map_segments_to_idb_id(
        cls, seg_prefix: str, idb_id: str, *, base_ea: Optional[int] = None) -> None:

        rules = dict(cls._binaries_netnode.items())

        new_rules = rules.copy()
        seg_rule = (idb_id, base_ea)
        new_rules[seg_prefix] = seg_rule

        # check that the new rules are valid
        try:
            cls._binaries_from_mapping_rules(new_rules)
        except LabSyncBinaryMatchingError:
            logger.exception(f"failed mapping segment prefix {seg_prefix!r} to idb id {idb_id}")
            return

        # add the rule to netnode
        cls._binaries_netnode[seg_prefix] = seg_rule

        # invalidate previous types if we had them (since they might be outdated)
        types = cls.types(idb_id)
        for tid in list(types.keys()):
            del types[tid]

    def sync(self) -> None:
        assert not self.sync_in_progress

        try:
            self._sync_in_progress = True

            logger.debug("checking that the remote repository is reachable")
            if not self.repo.ping():
                logger.warning("skipping sync because the remote repository is unreachable")
                return False

            timeout = float(self.cfg.get("lock_timeout_sec", DEFAULT_LOCK_TIMEOUT))
            with self._lock(timeout):
                binaries = self.binaries
                for binary in self.binaries:
                    if len(binaries) > 1:
                        logger.debug(f"syncing {binary.idb_id}")

                    # in case this is the initial commit, try to adopt local types from the repo
                    # (in case this IDB was already synced)
                    base = self.netnode.get(self.commit_key(binary.idb_id))
                    if not base:
                        latest_data, latest_commit = self.repo.get(binary.idb_id)
                        if latest_data:
                            logger.debug(f"adopting local type uuids from commit: {latest_commit}")
                            adopt_uuids(latest_data, self.types(binary.idb_id))

                    # sync IDB -> repo
                    logger.debug("saving changes to repo")

                    data = dump(binary, self.types(binary.idb_id))
                    commit, changed = self.repo.put(binary.idb_id, data, base=base)

                    if changed:
                        # save the IDB with the updated types and commit, in case something will
                        # break later during the merge
                        #
                        # TODO @TH: this path will be wrong if we're under "Save as..." flow.
                        #           see comments under LabSyncHooks.saved
                        self.netnode[self.commit_key(binary.idb_id)] = commit

                        idb_name = ida_loader.get_path(ida_loader.PATH_TYPE_IDB)
                        ida_loader.save_database(idb_name, 0)

                    # sync repo with upstream
                    self.repo.sync()

                    # sync repo -> IDB
                    new_data, new_commit = self.repo.get(binary.idb_id)

                        # if there's a new commit with new data for our IDB -- update
                    if new_data != data:
                        logger.debug("updating with changes from repo")
                        with wait_box("updating with changes from repo...", hide_cancel=True):
                            update(binary, new_data, self.types(binary.idb_id))
                            changed = True

                        # save the commit we're synced to in netnode (even if just the commit
                        # changed)
                    if new_commit != commit:
                        self.netnode[self.commit_key(binary.idb_id)] = new_commit
                        changed = True

                    p = "" if len(binaries) == 1 else f"{binary.idb_id} "
                    s = "" if changed else " (no changes)"
                    logger.info(f"{p}synced to commit: {new_commit}{s}")

                return changed
        except BaseException as exc:  # noqa: BLE001
            ida_kernwin.warning("LabSync synchronization failed. See output window for more "
                                "details")
            if not isinstance(exc, LabSyncLockError):
                raise
        finally:
            self._sync_in_progress = False

    @classmethod
    def enable(cls) -> None:
        cls.netnode["enabled"] = True
        logger.info("syncing enabled")

        for binary in cls.binaries:
            commit = cls.netnode.get(cls.commit_key(binary.idb_id))
            logger.info(
                f"{binary.idb_id} last synced to commit: {commit}",
            )

    @classmethod
    def disable(cls) -> None:
        cls.netnode["enabled"] = False
        logger.info("syncing disabled")

    @classmethod
    def reset(cls) -> None:
        # make sure that we're already disabled, to make sure that no one resets by mistake
        assert not cls.enabled

        idb_ids = {}

        # delete the commit states
        for k in cls.netnode.keys():  # noqa: SIM118
            if k.startswith(cls.COMMIT_KEY_PREFIX):
                idb_ids.add(k[len(cls.COMMIT_KEY_PREFIX):])
                del cls.netnode[k]

        for binary in cls.binaries:
            idb_ids.add(binary.idb_id)

        # delete the tid->uuid mapping for all of the IDB ids we noted
        for idb_id in idb_ids:
            types = cls.types(idb_id)
            for tid in list(types.keys()):
                del types[tid]

        logger.info("sync data reset")

    @classmethod
    @property
    def enabled(cls) -> bool:
        return cls.netnode.get("enabled", False)


def PLUGIN_ENTRY() -> ida_idaapi.plugin_t:  # noqa: N802
    return LabSyncPlugin()

```

`requirements.txt`:

```txt
PyYAML
GitPython
sark
ida-netnode
parse
# functioninliner

```

`ruff.toml`:

```toml
target-version = "py39"
line-length = 99

[lint]
preview = true
select = ["ALL"]
ignore = [
  "D", "S101", "TD003", "FIX002", "PLR0912", "PLR0913", "RET505", "RET506", "TRY003",
  "PLR2004", "G004", "C901", "PLR0915", "S311", "SIM108", "PLW2901", "G003", "TCH003",
  "UP007", "ARG002", "ANN101", "ANN204", "ANN002", "ANN003", "ANN401", "ANN102", "CPY",
  "E116", "PLR1702", "PLR0914",
]

```

`test_labsync.py`:

```py
import random
import string

import pytest

from labsync import decl_to_name_and_type, stable_topological_sort


class MockTypes:
    def __getitem__(self, key: str) -> str:
        return "00000000-0000-0000-0000-000000000000"

    @staticmethod
    def get(key: str) -> str:  # noqa: ARG004
        return "00000000-0000-0000-0000-000000000000"


def test_decl_to_name_pat() -> None:
    tests = (
        (  # normal local type
            "struct __CFString",
            "__CFString",
        ),
        (  # typedef
            "typedef const __CFString *CFStringRef;",
            "CFStringRef",
        ),
        (  # fptr typedef
            "typedef CFStringRef (__cdecl *CFArrayCopyDescriptionCallBack)(const void *);",
            "CFArrayCopyDescriptionCallBack",
        ),
        (  # forward declaration
            "struct x;",
            "x",
        ),
    )

    for t, n in tests:
        assert decl_to_name_and_type(t)[0] == n


def test_stable_topological_sort() -> None:
    tests = (
        (
            {
                "a": {"b"},
                "b": {"c"},
                "c": set(),
            },
            ("c", "b", "a"),
        ),
        (
            {
                "a": {"b"},
                "b": set(),
                "c": set(),
            },
            ("b", "a", "c"),
        ),
    )

    for graph, expected_result in tests:
        result = tuple(stable_topological_sort(graph))
        assert result == expected_result

    # test bad graphs
    with pytest.raises(ValueError):  # noqa: PT011
        tuple(stable_topological_sort({"a": "b", "b": "a"}))
    with pytest.raises(ValueError):  # noqa: PT011
        tuple(stable_topological_sort({"a": "a"}))
    with pytest.raises(ValueError):  # noqa: PT011
        tuple(stable_topological_sort({"a": "b"}))

    # test lexical ordering
    for _ in range(10):
        vertices = [random.choice(string.ascii_letters) for _ in range(10)]
        graph = {x: set() for x in vertices}
        result = tuple(stable_topological_sort(graph))
        expected_result = tuple(sorted(set(vertices)))
        assert result == expected_result

```