Project Path: arc_gmh5225_lc0_ffv_q2l6

Source Tree:

```txt
arc_gmh5225_lc0_ffv_q2l6
├── CONTRIBUTING.md
├── COPYING
├── FLAGS.md
├── README.md
├── appveyor.yml
├── build.cmd
├── build.sh
├── changelog.txt
├── cross-files
│   ├── aarch64-linux-android
│   └── armv7a-linux-android
├── dist
│   └── README-cuda.txt
├── install_openSUSE_lc0.sh
├── libs
│   └── lczero-common
├── meson.build
├── meson_options.txt
├── openSUSE_install.md
├── scripts
│   ├── appveyor_android_build.cmd
│   ├── appveyor_android_package.cmd
│   ├── appveyor_win_build.cmd
│   ├── appveyor_win_package.cmd
│   ├── bumpversion.py
│   ├── check_dx.bat
│   ├── check_opencl.bat
│   ├── checkdir.py
│   ├── compile_proto.py
│   ├── gen_py_bindings.py
│   └── pybind
│       ├── __init__.py
│       ├── core.py
│       ├── exceptions.py
│       ├── functions.py
│       ├── parameters.py
│       ├── retval.py
│       └── writer.py
├── src
│   ├── benchmark
│   │   ├── backendbench.cc
│   │   ├── backendbench.h
│   │   ├── benchmark.cc
│   │   └── benchmark.h
│   ├── chess
│   │   ├── bitboard.cc
│   │   ├── bitboard.h
│   │   ├── board.cc
│   │   ├── board.h
│   │   ├── board_test.cc
│   │   ├── callbacks.h
│   │   ├── pgn.h
│   │   ├── position.cc
│   │   ├── position.h
│   │   ├── position_test.cc
│   │   ├── uciloop.cc
│   │   └── uciloop.h
│   ├── engine.cc
│   ├── engine.h
│   ├── lc0ctl
│   │   ├── describenet.cc
│   │   ├── describenet.h
│   │   ├── leela2onnx.cc
│   │   ├── leela2onnx.h
│   │   ├── onnx2leela.cc
│   │   └── onnx2leela.h
│   ├── main.cc
│   ├── mcts
│   │   ├── node.cc
│   │   ├── node.h
│   │   ├── params.cc
│   │   ├── params.h
│   │   ├── search.cc
│   │   ├── search.h
│   │   └── stoppers
│   │       ├── alphazero.cc
│   │       ├── alphazero.h
│   │       ├── common.cc
│   │       ├── common.h
│   │       ├── factory.cc
│   │       ├── factory.h
│   │       ├── legacy.cc
│   │       ├── legacy.h
│   │       ├── smooth.cc
│   │       ├── smooth.h
│   │       ├── stoppers.cc
│   │       ├── stoppers.h
│   │       ├── timemgr.cc
│   │       └── timemgr.h
│   ├── neural
│   │   ├── blas
│   │   │   ├── README.md
│   │   │   ├── blas.h
│   │   │   ├── convolution1.cc
│   │   │   ├── convolution1.h
│   │   │   ├── fully_connected_layer.cc
│   │   │   ├── fully_connected_layer.h
│   │   │   ├── network_blas.cc
│   │   │   ├── se_unit.cc
│   │   │   ├── se_unit.h
│   │   │   ├── winograd_convolution3.cc
│   │   │   ├── winograd_convolution3.h
│   │   │   └── winograd_transform.ispc
│   │   ├── cache.cc
│   │   ├── cache.h
│   │   ├── cuda
│   │   │   ├── common_kernels.cu
│   │   │   ├── cuda_common.h
│   │   │   ├── fp16_kernels.cu
│   │   │   ├── inputs_outputs.h
│   │   │   ├── kernels.h
│   │   │   ├── layers.cc
│   │   │   ├── layers.h
│   │   │   ├── network_cuda.cc
│   │   │   ├── network_cudnn.cc
│   │   │   ├── readme.txt
│   │   │   └── winograd_helper.inc
│   │   ├── decoder.cc
│   │   ├── decoder.h
│   │   ├── dx
│   │   │   ├── MetaCommand.h
│   │   │   ├── dx_common.h
│   │   │   ├── layers_dx.cc
│   │   │   ├── layers_dx.h
│   │   │   ├── network_dx.cc
│   │   │   ├── network_dx.h
│   │   │   ├── shader_wrapper.cc
│   │   │   ├── shader_wrapper.h
│   │   │   └── shaders
│   │   │       ├── AddVectors.hlsl
│   │   │       ├── Conv1x1.hlsl
│   │   │       ├── ExpandPlanes.hlsl
│   │   │       ├── Gemm.hlsl
│   │   │       ├── PolicyMap.hlsl
│   │   │       ├── SE.hlsl
│   │   │       ├── WinogradCommon.h
│   │   │       ├── WinogradTransform.hlsl
│   │   │       ├── WinogradTransformSE.hlsl
│   │   │       ├── dxc_helper.py
│   │   │       ├── meson.build
│   │   │       ├── shader_shared.h
│   │   │       └── shaders.h
│   │   ├── encoder.cc
│   │   ├── encoder.h
│   │   ├── encoder_test.cc
│   │   ├── factory.cc
│   │   ├── factory.h
│   │   ├── loader.cc
│   │   ├── loader.h
│   │   ├── network.h
│   │   ├── network_check.cc
│   │   ├── network_demux.cc
│   │   ├── network_legacy.cc
│   │   ├── network_legacy.h
│   │   ├── network_mux.cc
│   │   ├── network_random.cc
│   │   ├── network_record.cc
│   │   ├── network_rr.cc
│   │   ├── network_tf_cc.cc
│   │   ├── network_trivial.cc
│   │   ├── onednn
│   │   │   ├── layers.cc
│   │   │   ├── layers.h
│   │   │   └── network_onednn.cc
│   │   ├── onnx
│   │   │   ├── adapters.cc
│   │   │   ├── adapters.h
│   │   │   ├── builder.cc
│   │   │   ├── builder.h
│   │   │   ├── converter.cc
│   │   │   ├── converter.h
│   │   │   ├── network_onnx.cc
│   │   │   └── onnx.proto
│   │   ├── opencl
│   │   │   ├── OpenCL.cc
│   │   │   ├── OpenCL.h
│   │   │   ├── OpenCLBuffers.cc
│   │   │   ├── OpenCLBuffers.h
│   │   │   ├── OpenCLParams.h
│   │   │   ├── OpenCLTuner.cc
│   │   │   ├── OpenCLTuner.h
│   │   │   ├── README.md
│   │   │   ├── clblast_level3
│   │   │   │   ├── common.opencl
│   │   │   │   ├── xgemm_batched.opencl
│   │   │   │   ├── xgemm_part1.opencl
│   │   │   │   ├── xgemm_part2.opencl
│   │   │   │   ├── xgemm_part3.opencl
│   │   │   │   └── xgemv.opencl
│   │   │   ├── clsource
│   │   │   │   ├── config.opencl
│   │   │   │   ├── convolve1.opencl
│   │   │   │   ├── convolve3.opencl
│   │   │   │   ├── policymap.opencl
│   │   │   │   └── se.opencl
│   │   │   └── network_opencl.cc
│   │   └── shared
│   │       ├── activation.cc
│   │       ├── activation.h
│   │       ├── activation.ispc
│   │       ├── attention_policy_map.h
│   │       ├── policy_map.h
│   │       ├── winograd_filter.cc
│   │       └── winograd_filter.h
│   ├── python
│   │   └── weights.h
│   ├── selfplay
│   │   ├── game.cc
│   │   ├── game.h
│   │   ├── loop.cc
│   │   ├── loop.h
│   │   ├── tournament.cc
│   │   └── tournament.h
│   ├── syzygy
│   │   ├── syzygy.cc
│   │   ├── syzygy.h
│   │   └── syzygy_test.cc
│   ├── trainingdata
│   │   ├── reader.cc
│   │   ├── reader.h
│   │   ├── trainingdata.cc
│   │   ├── trainingdata.h
│   │   ├── writer.cc
│   │   └── writer.h
│   ├── utils
│   │   ├── bititer.h
│   │   ├── cache-old.h
│   │   ├── cache.h
│   │   ├── commandline.cc
│   │   ├── commandline.h
│   │   ├── configfile.cc
│   │   ├── configfile.h
│   │   ├── cppattributes.h
│   │   ├── esc_codes.cc
│   │   ├── esc_codes.h
│   │   ├── exception.h
│   │   ├── fastmath.h
│   │   ├── files.cc
│   │   ├── files.h
│   │   ├── filesystem.h
│   │   ├── filesystem.posix.cc
│   │   ├── filesystem.win32.cc
│   │   ├── fp16_utils.cc
│   │   ├── fp16_utils.h
│   │   ├── hashcat.h
│   │   ├── hashcat_test.cc
│   │   ├── histogram.cc
│   │   ├── histogram.h
│   │   ├── logging.cc
│   │   ├── logging.h
│   │   ├── mutex.h
│   │   ├── numa.cc
│   │   ├── numa.h
│   │   ├── optionsdict.cc
│   │   ├── optionsdict.h
│   │   ├── optionsparser.cc
│   │   ├── optionsparser.h
│   │   ├── optionsparser_test.cc
│   │   ├── protomessage.cc
│   │   ├── protomessage.h
│   │   ├── random.cc
│   │   ├── random.h
│   │   ├── smallarray.h
│   │   ├── string.cc
│   │   ├── string.h
│   │   ├── transpose.h
│   │   ├── weights_adapter.cc
│   │   └── weights_adapter.h
│   ├── version.cc
│   ├── version.h
│   └── version.inc
├── subprojects
├── tensorflow.md
├── third_party
│   ├── cl2.hpp
│   └── d3dx12.h
└── windows_build.md

```

`CONTRIBUTING.md`:

```md
# Contributing to lc0

These are the guidelines and standards followed by this codebase.

The language is C++, specifically C++17. As such, manual `new` and `delete` memory mangement is strongly discouraged; use the standard library tools for managing memory (such as `unique_ptr`, `shared_ptr` etc.).

This codebase uses semantic versioning. A release is the final commit for that version number, and all subsequent commits are development for the next version. `master` is the default branch, and the active development branch (as such, all Pull Requests go here); it always targets a minor (or major) version which succeeds the current relase. `release` is always equivalent to the latest tag.


### Style

Style is of course the first guideline on every new contributor's mind :)

This codebase largely complies with the [Google C++ style guide](https://google.github.io/styleguide/cppguide.html). The maintainers recommend the use of [Clang's auto formatter](https://clang.llvm.org/docs/ClangFormatStyleOptions.html).

Notable exceptions:
 1. C++ exceptions are allowed (in fact, only `lczero::Exception`, defined in `utils/exception.h`, is allowed)
 2. We use `#pragma once` instead of header guards.
 3. Default function parameters are sometimes allowed.
 4. Rvalue reference function params are sometimes allowed, not only for constructors and assignment operators.

For items (3) and (4), usage of those are discouraged, only use them if they benefit readability or have significant performance gain. It's possible that those exceptions (3) and (4) will be disallowed in future.

The most important rule to follow is consistency: look at the surrounding code when doing changes and follow similar style.

These are the most important parts of the codebase style (as a sort of tl;dr):

 * Comments must be full sentences, i.e. capitalized and ending in a period. (Sentences with elided subjects are fine.) Only `//` style comments are allowed, `/* */` style comments aren't.

 * Braces are a variant of K&R style, as can be gleaned from existing code. All `if` statements must use braces, with the possible exception of single statement `if`s, which *may* omit if the braces *if* the conditional and following statement are on the same line. Again, see surrounding code for examples.

 * Indentation is two spaces; \t characters are disallowed.

 * Code line length is strictly capped at 80 characters.

 * Using non-`const` references as function parameters is disallowed; use pointers instead. (Using `const` references as parameters is fine.)

 * Identifier style:
   - `kLikeThis` for constants and enum values
   - `like_this` for variables
   - `like_this_` for member variables
   - `LikeThis` for function and class names

 * All code should be inside `namespace lczero`

The internal code dependency structure looks like this:

 * Code in `src/utils` is not allowed to depend on any other code.

 * Code in `src/chess` only depends on `src/utils`

 * Code in `src/neural` only depends on `src/utils` and `src/chess`

 * Code in `src/mcts` only depends on `src/utils`, `src/chess` and `src/neural`


### Git history

Pull Requests are squashed when merged. This means all commits in the branch will be squashed into one commit applied onto master, so branches and their PRs should stick to *one* topic only. If you think changes deserve separate commits, make separate PRs for each commit.

This also means it's not possible to reuse one branch for multiple PRs; new PRs must either use entirely new branches, or else you could use `git reset --hard` on the current branch.


### Allowed features

Lc0 is still in early stages of development, and has not yet reached the point where we are ready to add small tweaks to add few points of a rating. Large code changes still happen, and having lots of small optimizations adds overhead to larger changes, slowing development.

Therefore, as a rule, search algorithm tweaks that give a gain of less than ~20 Elo points are discouraged at this point. (This limit will gradually be lowered as Lc0 code matures, eventually to 0.0 Elo).


#### Adding new command line flags/UCI parameters

Only add new parameters if users can significantly (>20 Elo) benefit by tweaking it. We don't want to make every single constant configurable (or rather, users don't want to see hundreds of parameters which don't do anything).

Try to minimize number of parameters that your feature introduces. If your feature introduces several parameters, every individual parameter should be significant (i.e. tweaking it with other fixes will give >20 Elo).


#### Adding features for testing

It is fine to temporarily commit a feature of unknown Elo gain so that people may test it. It's also fine to expose many parameters for the feature initially so that people can tune them. However, if the tweak doesn't prove to be significant, it should be removed after a few weeks.


```

`COPYING`:

```
                    GNU GENERAL PUBLIC LICENSE
                       Version 3, 29 June 2007

 Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>
 Everyone is permitted to copy and distribute verbatim copies
 of this license document, but changing it is not allowed.

                            Preamble

  The GNU General Public License is a free, copyleft license for
software and other kinds of works.

  The licenses for most software and other practical works are designed
to take away your freedom to share and change the works.  By contrast,
the GNU General Public License is intended to guarantee your freedom to
share and change all versions of a program--to make sure it remains free
software for all its users.  We, the Free Software Foundation, use the
GNU General Public License for most of our software; it applies also to
any other work released this way by its authors.  You can apply it to
your programs, too.

  When we speak of free software, we are referring to freedom, not
price.  Our General Public Licenses are designed to make sure that you
have the freedom to distribute copies of free software (and charge for
them if you wish), that you receive source code or can get it if you
want it, that you can change the software or use pieces of it in new
free programs, and that you know you can do these things.

  To protect your rights, we need to prevent others from denying you
these rights or asking you to surrender the rights.  Therefore, you have
certain responsibilities if you distribute copies of the software, or if
you modify it: responsibilities to respect the freedom of others.

  For example, if you distribute copies of such a program, whether
gratis or for a fee, you must pass on to the recipients the same
freedoms that you received.  You must make sure that they, too, receive
or can get the source code.  And you must show them these terms so they
know their rights.

  Developers that use the GNU GPL protect your rights with two steps:
(1) assert copyright on the software, and (2) offer you this License
giving you legal permission to copy, distribute and/or modify it.

  For the developers' and authors' protection, the GPL clearly explains
that there is no warranty for this free software.  For both users' and
authors' sake, the GPL requires that modified versions be marked as
changed, so that their problems will not be attributed erroneously to
authors of previous versions.

  Some devices are designed to deny users access to install or run
modified versions of the software inside them, although the manufacturer
can do so.  This is fundamentally incompatible with the aim of
protecting users' freedom to change the software.  The systematic
pattern of such abuse occurs in the area of products for individuals to
use, which is precisely where it is most unacceptable.  Therefore, we
have designed this version of the GPL to prohibit the practice for those
products.  If such problems arise substantially in other domains, we
stand ready to extend this provision to those domains in future versions
of the GPL, as needed to protect the freedom of users.

  Finally, every program is threatened constantly by software patents.
States should not allow patents to restrict development and use of
software on general-purpose computers, but in those that do, we wish to
avoid the special danger that patents applied to a free program could
make it effectively proprietary.  To prevent this, the GPL assures that
patents cannot be used to render the program non-free.

  The precise terms and conditions for copying, distribution and
modification follow.

                       TERMS AND CONDITIONS

  0. Definitions.

  "This License" refers to version 3 of the GNU General Public License.

  "Copyright" also means copyright-like laws that apply to other kinds of
works, such as semiconductor masks.

  "The Program" refers to any copyrightable work licensed under this
License.  Each licensee is addressed as "you".  "Licensees" and
"recipients" may be individuals or organizations.

  To "modify" a work means to copy from or adapt all or part of the work
in a fashion requiring copyright permission, other than the making of an
exact copy.  The resulting work is called a "modified version" of the
earlier work or a work "based on" the earlier work.

  A "covered work" means either the unmodified Program or a work based
on the Program.

  To "propagate" a work means to do anything with it that, without
permission, would make you directly or secondarily liable for
infringement under applicable copyright law, except executing it on a
computer or modifying a private copy.  Propagation includes copying,
distribution (with or without modification), making available to the
public, and in some countries other activities as well.

  To "convey" a work means any kind of propagation that enables other
parties to make or receive copies.  Mere interaction with a user through
a computer network, with no transfer of a copy, is not conveying.

  An interactive user interface displays "Appropriate Legal Notices"
to the extent that it includes a convenient and prominently visible
feature that (1) displays an appropriate copyright notice, and (2)
tells the user that there is no warranty for the work (except to the
extent that warranties are provided), that licensees may convey the
work under this License, and how to view a copy of this License.  If
the interface presents a list of user commands or options, such as a
menu, a prominent item in the list meets this criterion.

  1. Source Code.

  The "source code" for a work means the preferred form of the work
for making modifications to it.  "Object code" means any non-source
form of a work.

  A "Standard Interface" means an interface that either is an official
standard defined by a recognized standards body, or, in the case of
interfaces specified for a particular programming language, one that
is widely used among developers working in that language.

  The "System Libraries" of an executable work include anything, other
than the work as a whole, that (a) is included in the normal form of
packaging a Major Component, but which is not part of that Major
Component, and (b) serves only to enable use of the work with that
Major Component, or to implement a Standard Interface for which an
implementation is available to the public in source code form.  A
"Major Component", in this context, means a major essential component
(kernel, window system, and so on) of the specific operating system
(if any) on which the executable work runs, or a compiler used to
produce the work, or an object code interpreter used to run it.

  The "Corresponding Source" for a work in object code form means all
the source code needed to generate, install, and (for an executable
work) run the object code and to modify the work, including scripts to
control those activities.  However, it does not include the work's
System Libraries, or general-purpose tools or generally available free
programs which are used unmodified in performing those activities but
which are not part of the work.  For example, Corresponding Source
includes interface definition files associated with source files for
the work, and the source code for shared libraries and dynamically
linked subprograms that the work is specifically designed to require,
such as by intimate data communication or control flow between those
subprograms and other parts of the work.

  The Corresponding Source need not include anything that users
can regenerate automatically from other parts of the Corresponding
Source.

  The Corresponding Source for a work in source code form is that
same work.

  2. Basic Permissions.

  All rights granted under this License are granted for the term of
copyright on the Program, and are irrevocable provided the stated
conditions are met.  This License explicitly affirms your unlimited
permission to run the unmodified Program.  The output from running a
covered work is covered by this License only if the output, given its
content, constitutes a covered work.  This License acknowledges your
rights of fair use or other equivalent, as provided by copyright law.

  You may make, run and propagate covered works that you do not
convey, without conditions so long as your license otherwise remains
in force.  You may convey covered works to others for the sole purpose
of having them make modifications exclusively for you, or provide you
with facilities for running those works, provided that you comply with
the terms of this License in conveying all material for which you do
not control copyright.  Those thus making or running the covered works
for you must do so exclusively on your behalf, under your direction
and control, on terms that prohibit them from making any copies of
your copyrighted material outside their relationship with you.

  Conveying under any other circumstances is permitted solely under
the conditions stated below.  Sublicensing is not allowed; section 10
makes it unnecessary.

  3. Protecting Users' Legal Rights From Anti-Circumvention Law.

  No covered work shall be deemed part of an effective technological
measure under any applicable law fulfilling obligations under article
11 of the WIPO copyright treaty adopted on 20 December 1996, or
similar laws prohibiting or restricting circumvention of such
measures.

  When you convey a covered work, you waive any legal power to forbid
circumvention of technological measures to the extent such circumvention
is effected by exercising rights under this License with respect to
the covered work, and you disclaim any intention to limit operation or
modification of the work as a means of enforcing, against the work's
users, your or third parties' legal rights to forbid circumvention of
technological measures.

  4. Conveying Verbatim Copies.

  You may convey verbatim copies of the Program's source code as you
receive it, in any medium, provided that you conspicuously and
appropriately publish on each copy an appropriate copyright notice;
keep intact all notices stating that this License and any
non-permissive terms added in accord with section 7 apply to the code;
keep intact all notices of the absence of any warranty; and give all
recipients a copy of this License along with the Program.

  You may charge any price or no price for each copy that you convey,
and you may offer support or warranty protection for a fee.

  5. Conveying Modified Source Versions.

  You may convey a work based on the Program, or the modifications to
produce it from the Program, in the form of source code under the
terms of section 4, provided that you also meet all of these conditions:

    a) The work must carry prominent notices stating that you modified
    it, and giving a relevant date.

    b) The work must carry prominent notices stating that it is
    released under this License and any conditions added under section
    7.  This requirement modifies the requirement in section 4 to
    "keep intact all notices".

    c) You must license the entire work, as a whole, under this
    License to anyone who comes into possession of a copy.  This
    License will therefore apply, along with any applicable section 7
    additional terms, to the whole of the work, and all its parts,
    regardless of how they are packaged.  This License gives no
    permission to license the work in any other way, but it does not
    invalidate such permission if you have separately received it.

    d) If the work has interactive user interfaces, each must display
    Appropriate Legal Notices; however, if the Program has interactive
    interfaces that do not display Appropriate Legal Notices, your
    work need not make them do so.

  A compilation of a covered work with other separate and independent
works, which are not by their nature extensions of the covered work,
and which are not combined with it such as to form a larger program,
in or on a volume of a storage or distribution medium, is called an
"aggregate" if the compilation and its resulting copyright are not
used to limit the access or legal rights of the compilation's users
beyond what the individual works permit.  Inclusion of a covered work
in an aggregate does not cause this License to apply to the other
parts of the aggregate.

  6. Conveying Non-Source Forms.

  You may convey a covered work in object code form under the terms
of sections 4 and 5, provided that you also convey the
machine-readable Corresponding Source under the terms of this License,
in one of these ways:

    a) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by the
    Corresponding Source fixed on a durable physical medium
    customarily used for software interchange.

    b) Convey the object code in, or embodied in, a physical product
    (including a physical distribution medium), accompanied by a
    written offer, valid for at least three years and valid for as
    long as you offer spare parts or customer support for that product
    model, to give anyone who possesses the object code either (1) a
    copy of the Corresponding Source for all the software in the
    product that is covered by this License, on a durable physical
    medium customarily used for software interchange, for a price no
    more than your reasonable cost of physically performing this
    conveying of source, or (2) access to copy the
    Corresponding Source from a network server at no charge.

    c) Convey individual copies of the object code with a copy of the
    written offer to provide the Corresponding Source.  This
    alternative is allowed only occasionally and noncommercially, and
    only if you received the object code with such an offer, in accord
    with subsection 6b.

    d) Convey the object code by offering access from a designated
    place (gratis or for a charge), and offer equivalent access to the
    Corresponding Source in the same way through the same place at no
    further charge.  You need not require recipients to copy the
    Corresponding Source along with the object code.  If the place to
    copy the object code is a network server, the Corresponding Source
    may be on a different server (operated by you or a third party)
    that supports equivalent copying facilities, provided you maintain
    clear directions next to the object code saying where to find the
    Corresponding Source.  Regardless of what server hosts the
    Corresponding Source, you remain obligated to ensure that it is
    available for as long as needed to satisfy these requirements.

    e) Convey the object code using peer-to-peer transmission, provided
    you inform other peers where the object code and Corresponding
    Source of the work are being offered to the general public at no
    charge under subsection 6d.

  A separable portion of the object code, whose source code is excluded
from the Corresponding Source as a System Library, need not be
included in conveying the object code work.

  A "User Product" is either (1) a "consumer product", which means any
tangible personal property which is normally used for personal, family,
or household purposes, or (2) anything designed or sold for incorporation
into a dwelling.  In determining whether a product is a consumer product,
doubtful cases shall be resolved in favor of coverage.  For a particular
product received by a particular user, "normally used" refers to a
typical or common use of that class of product, regardless of the status
of the particular user or of the way in which the particular user
actually uses, or expects or is expected to use, the product.  A product
is a consumer product regardless of whether the product has substantial
commercial, industrial or non-consumer uses, unless such uses represent
the only significant mode of use of the product.

  "Installation Information" for a User Product means any methods,
procedures, authorization keys, or other information required to install
and execute modified versions of a covered work in that User Product from
a modified version of its Corresponding Source.  The information must
suffice to ensure that the continued functioning of the modified object
code is in no case prevented or interfered with solely because
modification has been made.

  If you convey an object code work under this section in, or with, or
specifically for use in, a User Product, and the conveying occurs as
part of a transaction in which the right of possession and use of the
User Product is transferred to the recipient in perpetuity or for a
fixed term (regardless of how the transaction is characterized), the
Corresponding Source conveyed under this section must be accompanied
by the Installation Information.  But this requirement does not apply
if neither you nor any third party retains the ability to install
modified object code on the User Product (for example, the work has
been installed in ROM).

  The requirement to provide Installation Information does not include a
requirement to continue to provide support service, warranty, or updates
for a work that has been modified or installed by the recipient, or for
the User Product in which it has been modified or installed.  Access to a
network may be denied when the modification itself materially and
adversely affects the operation of the network or violates the rules and
protocols for communication across the network.

  Corresponding Source conveyed, and Installation Information provided,
in accord with this section must be in a format that is publicly
documented (and with an implementation available to the public in
source code form), and must require no special password or key for
unpacking, reading or copying.

  7. Additional Terms.

  "Additional permissions" are terms that supplement the terms of this
License by making exceptions from one or more of its conditions.
Additional permissions that are applicable to the entire Program shall
be treated as though they were included in this License, to the extent
that they are valid under applicable law.  If additional permissions
apply only to part of the Program, that part may be used separately
under those permissions, but the entire Program remains governed by
this License without regard to the additional permissions.

  When you convey a copy of a covered work, you may at your option
remove any additional permissions from that copy, or from any part of
it.  (Additional permissions may be written to require their own
removal in certain cases when you modify the work.)  You may place
additional permissions on material, added by you to a covered work,
for which you have or can give appropriate copyright permission.

  Notwithstanding any other provision of this License, for material you
add to a covered work, you may (if authorized by the copyright holders of
that material) supplement the terms of this License with terms:

    a) Disclaiming warranty or limiting liability differently from the
    terms of sections 15 and 16 of this License; or

    b) Requiring preservation of specified reasonable legal notices or
    author attributions in that material or in the Appropriate Legal
    Notices displayed by works containing it; or

    c) Prohibiting misrepresentation of the origin of that material, or
    requiring that modified versions of such material be marked in
    reasonable ways as different from the original version; or

    d) Limiting the use for publicity purposes of names of licensors or
    authors of the material; or

    e) Declining to grant rights under trademark law for use of some
    trade names, trademarks, or service marks; or

    f) Requiring indemnification of licensors and authors of that
    material by anyone who conveys the material (or modified versions of
    it) with contractual assumptions of liability to the recipient, for
    any liability that these contractual assumptions directly impose on
    those licensors and authors.

  All other non-permissive additional terms are considered "further
restrictions" within the meaning of section 10.  If the Program as you
received it, or any part of it, contains a notice stating that it is
governed by this License along with a term that is a further
restriction, you may remove that term.  If a license document contains
a further restriction but permits relicensing or conveying under this
License, you may add to a covered work material governed by the terms
of that license document, provided that the further restriction does
not survive such relicensing or conveying.

  If you add terms to a covered work in accord with this section, you
must place, in the relevant source files, a statement of the
additional terms that apply to those files, or a notice indicating
where to find the applicable terms.

  Additional terms, permissive or non-permissive, may be stated in the
form of a separately written license, or stated as exceptions;
the above requirements apply either way.

  8. Termination.

  You may not propagate or modify a covered work except as expressly
provided under this License.  Any attempt otherwise to propagate or
modify it is void, and will automatically terminate your rights under
this License (including any patent licenses granted under the third
paragraph of section 11).

  However, if you cease all violation of this License, then your
license from a particular copyright holder is reinstated (a)
provisionally, unless and until the copyright holder explicitly and
finally terminates your license, and (b) permanently, if the copyright
holder fails to notify you of the violation by some reasonable means
prior to 60 days after the cessation.

  Moreover, your license from a particular copyright holder is
reinstated permanently if the copyright holder notifies you of the
violation by some reasonable means, this is the first time you have
received notice of violation of this License (for any work) from that
copyright holder, and you cure the violation prior to 30 days after
your receipt of the notice.

  Termination of your rights under this section does not terminate the
licenses of parties who have received copies or rights from you under
this License.  If your rights have been terminated and not permanently
reinstated, you do not qualify to receive new licenses for the same
material under section 10.

  9. Acceptance Not Required for Having Copies.

  You are not required to accept this License in order to receive or
run a copy of the Program.  Ancillary propagation of a covered work
occurring solely as a consequence of using peer-to-peer transmission
to receive a copy likewise does not require acceptance.  However,
nothing other than this License grants you permission to propagate or
modify any covered work.  These actions infringe copyright if you do
not accept this License.  Therefore, by modifying or propagating a
covered work, you indicate your acceptance of this License to do so.

  10. Automatic Licensing of Downstream Recipients.

  Each time you convey a covered work, the recipient automatically
receives a license from the original licensors, to run, modify and
propagate that work, subject to this License.  You are not responsible
for enforcing compliance by third parties with this License.

  An "entity transaction" is a transaction transferring control of an
organization, or substantially all assets of one, or subdividing an
organization, or merging organizations.  If propagation of a covered
work results from an entity transaction, each party to that
transaction who receives a copy of the work also receives whatever
licenses to the work the party's predecessor in interest had or could
give under the previous paragraph, plus a right to possession of the
Corresponding Source of the work from the predecessor in interest, if
the predecessor has it or can get it with reasonable efforts.

  You may not impose any further restrictions on the exercise of the
rights granted or affirmed under this License.  For example, you may
not impose a license fee, royalty, or other charge for exercise of
rights granted under this License, and you may not initiate litigation
(including a cross-claim or counterclaim in a lawsuit) alleging that
any patent claim is infringed by making, using, selling, offering for
sale, or importing the Program or any portion of it.

  11. Patents.

  A "contributor" is a copyright holder who authorizes use under this
License of the Program or a work on which the Program is based.  The
work thus licensed is called the contributor's "contributor version".

  A contributor's "essential patent claims" are all patent claims
owned or controlled by the contributor, whether already acquired or
hereafter acquired, that would be infringed by some manner, permitted
by this License, of making, using, or selling its contributor version,
but do not include claims that would be infringed only as a
consequence of further modification of the contributor version.  For
purposes of this definition, "control" includes the right to grant
patent sublicenses in a manner consistent with the requirements of
this License.

  Each contributor grants you a non-exclusive, worldwide, royalty-free
patent license under the contributor's essential patent claims, to
make, use, sell, offer for sale, import and otherwise run, modify and
propagate the contents of its contributor version.

  In the following three paragraphs, a "patent license" is any express
agreement or commitment, however denominated, not to enforce a patent
(such as an express permission to practice a patent or covenant not to
sue for patent infringement).  To "grant" such a patent license to a
party means to make such an agreement or commitment not to enforce a
patent against the party.

  If you convey a covered work, knowingly relying on a patent license,
and the Corresponding Source of the work is not available for anyone
to copy, free of charge and under the terms of this License, through a
publicly available network server or other readily accessible means,
then you must either (1) cause the Corresponding Source to be so
available, or (2) arrange to deprive yourself of the benefit of the
patent license for this particular work, or (3) arrange, in a manner
consistent with the requirements of this License, to extend the patent
license to downstream recipients.  "Knowingly relying" means you have
actual knowledge that, but for the patent license, your conveying the
covered work in a country, or your recipient's use of the covered work
in a country, would infringe one or more identifiable patents in that
country that you have reason to believe are valid.

  If, pursuant to or in connection with a single transaction or
arrangement, you convey, or propagate by procuring conveyance of, a
covered work, and grant a patent license to some of the parties
receiving the covered work authorizing them to use, propagate, modify
or convey a specific copy of the covered work, then the patent license
you grant is automatically extended to all recipients of the covered
work and works based on it.

  A patent license is "discriminatory" if it does not include within
the scope of its coverage, prohibits the exercise of, or is
conditioned on the non-exercise of one or more of the rights that are
specifically granted under this License.  You may not convey a covered
work if you are a party to an arrangement with a third party that is
in the business of distributing software, under which you make payment
to the third party based on the extent of your activity of conveying
the work, and under which the third party grants, to any of the
parties who would receive the covered work from you, a discriminatory
patent license (a) in connection with copies of the covered work
conveyed by you (or copies made from those copies), or (b) primarily
for and in connection with specific products or compilations that
contain the covered work, unless you entered into that arrangement,
or that patent license was granted, prior to 28 March 2007.

  Nothing in this License shall be construed as excluding or limiting
any implied license or other defenses to infringement that may
otherwise be available to you under applicable patent law.

  12. No Surrender of Others' Freedom.

  If conditions are imposed on you (whether by court order, agreement or
otherwise) that contradict the conditions of this License, they do not
excuse you from the conditions of this License.  If you cannot convey a
covered work so as to satisfy simultaneously your obligations under this
License and any other pertinent obligations, then as a consequence you may
not convey it at all.  For example, if you agree to terms that obligate you
to collect a royalty for further conveying from those to whom you convey
the Program, the only way you could satisfy both those terms and this
License would be to refrain entirely from conveying the Program.

  13. Use with the GNU Affero General Public License.

  Notwithstanding any other provision of this License, you have
permission to link or combine any covered work with a work licensed
under version 3 of the GNU Affero General Public License into a single
combined work, and to convey the resulting work.  The terms of this
License will continue to apply to the part which is the covered work,
but the special requirements of the GNU Affero General Public License,
section 13, concerning interaction through a network will apply to the
combination as such.

  14. Revised Versions of this License.

  The Free Software Foundation may publish revised and/or new versions of
the GNU General Public License from time to time.  Such new versions will
be similar in spirit to the present version, but may differ in detail to
address new problems or concerns.

  Each version is given a distinguishing version number.  If the
Program specifies that a certain numbered version of the GNU General
Public License "or any later version" applies to it, you have the
option of following the terms and conditions either of that numbered
version or of any later version published by the Free Software
Foundation.  If the Program does not specify a version number of the
GNU General Public License, you may choose any version ever published
by the Free Software Foundation.

  If the Program specifies that a proxy can decide which future
versions of the GNU General Public License can be used, that proxy's
public statement of acceptance of a version permanently authorizes you
to choose that version for the Program.

  Later license versions may give you additional or different
permissions.  However, no additional obligations are imposed on any
author or copyright holder as a result of your choosing to follow a
later version.

  15. Disclaimer of Warranty.

  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY
APPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT
HOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM "AS IS" WITHOUT WARRANTY
OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,
THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
PURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM
IS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF
ALL NECESSARY SERVICING, REPAIR OR CORRECTION.

  16. Limitation of Liability.

  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING
WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS
THE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY
GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE
USE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF
DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD
PARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),
EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF
SUCH DAMAGES.

  17. Interpretation of Sections 15 and 16.

  If the disclaimer of warranty and limitation of liability provided
above cannot be given local legal effect according to their terms,
reviewing courts shall apply local law that most closely approximates
an absolute waiver of all civil liability in connection with the
Program, unless a warranty or assumption of liability accompanies a
copy of the Program in return for a fee.

                     END OF TERMS AND CONDITIONS

            How to Apply These Terms to Your New Programs

  If you develop a new program, and you want it to be of the greatest
possible use to the public, the best way to achieve this is to make it
free software which everyone can redistribute and change under these terms.

  To do so, attach the following notices to the program.  It is safest
to attach them to the start of each source file to most effectively
state the exclusion of warranty; and each file should have at least
the "copyright" line and a pointer to where the full notice is found.

    <one line to give the program's name and a brief idea of what it does.>
    Copyright (C) <year>  <name of author>

    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <https://www.gnu.org/licenses/>.

Also add information on how to contact you by electronic and paper mail.

  If the program does terminal interaction, make it output a short
notice like this when it starts in an interactive mode:

    <program>  Copyright (C) <year>  <name of author>
    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.
    This is free software, and you are welcome to redistribute it
    under certain conditions; type `show c' for details.

The hypothetical commands `show w' and `show c' should show the appropriate
parts of the General Public License.  Of course, your program's commands
might be different; for a GUI interface, you would use an "about box".

  You should also get your employer (if you work as a programmer) or school,
if any, to sign a "copyright disclaimer" for the program, if necessary.
For more information on this, and how to apply and follow the GNU GPL, see
<https://www.gnu.org/licenses/>.

  The GNU General Public License does not permit incorporating your program
into proprietary programs.  If your program is a subroutine library, you
may consider it more useful to permit linking proprietary applications with
the library.  If this is what you want to do, use the GNU Lesser General
Public License instead of this License.  But first, please read
<https://www.gnu.org/licenses/why-not-lgpl.html>.
```

`FLAGS.md`:

```md
# Modes

`lc0` supports several operating modes, each of which has a different set of 
command line flags (although there are common ones).

Currently, `lc0` has the following modes:

| Mode |   Description |
|------|---------------|
| uci (*default*) | Acts as a UCI chess engine |
| selfplay | Plays one or multiple games with itself and optionally generates training data |
| debug | Generates debug data for a position |

To run `lc0` in any of those modes, specify a mode name as the first argument (`uci` may be omitted).
For example:

```bash
$ ./lc0 selfplay ...    # For selfplay mode
$ ./lc0 ...             # For UCI mode
```

In any of those modes, it's possible to get help using the `--help` command line argument:

```bash
$ ./lc0 --help            # Help for UCI mode
$ ./lc0 selfplay --help   # Help for selfplay mode
```

## UCI Mode

In UCI engine mode, all of command line parameters can also be changed using a UCI parameter.

List of command line flags:

| Flag | UCI Parameter | Description |
|------|---------------|-------------|
| -w PATH,<br>--weights=PATH | Network weights file path | Path to load network weights from.<br>Default is `<autodiscover>`, which makes it search for the latest (by file date) file in ./ and ./weights/ subdirectories which look like weights. |
| -t NUM,<br>--threads=NUM | Number of worker threads | Number of (CPU) threads to use.<br> Default is `2`. There's currently no use of making it more than 3 as it's limited by mutex contention which is yet to be optimized. |
| --nncache=SIZE | NNCache size | Number of positions to store in cache.<br>Default: `2000000` |
| <nobr>--backend=BACKEND</nobr><br><nobr>--backend-opts=OPTS</nobr> | NN backend to use<br>NN backend parameters | Configuration of backend parameters. Described in details [here](#backendconfiguration).<br>Default depends on particular build type (cuDNN, tensorflow, etc.). |
| --slowmover=NUM | Scale thinking time | Parameter value `X` means that the whole remaining time is split in such a way that the current move gets `X × Y` seconds, and next moves will get `1 × Y` seconds. However, due to smart pruning, the engine usually doesn't use all allocated time.<br>Default: `2.2`|
| <nobr>--move-overhead=NUM</nobr> | Move time overhead in milliseconds | How much overhead should the engine allocate for every move (to counteract things like slow connection, interprocess communication, etc.).<br>Default: `100` ms. |
| <nobr>--minibatch-size=NUM</nobr> | Minibatch size for NN inference | How many positions the engine tries to batch together for computation. Theoretically larger batches may reduce strengths a bit, especially on a small number of playouts.<br>Default is `256`. Every backend/hardware has different optimal value (e.g., `1` if batching is not supported). |
| <nobr>--max-prefetch=NUM</nobr> | Maximum prefetch nodes per NN call | When the engine can't gather a large enough batch for immediate use, try to prefetch up to `X` positions, which are likely to be useful soon, and put them in the cache.<br>Default: `32`. |
| <nobr>--cpuct=NUM</nobr> | Cpuct MCTS option | C_puct constant from Upper Confidence Tree search algorithm. Higher values promote more exploration/wider search, lower values promote more confidence/deeper search.<br>Default: `1.2`. |
| <nobr>--temperature=NUM</nobr> | Initial temperature | Tau value from softmax formula. If equal to 0, the engine also picks the best move to make. Larger values increase randomness while making the move.<br>Default: `0` |
| <nobr>--tempdecay-moves=NUM</nobr> | Moves with temperature decay | Reduce temperature for every move linearly from initial temperature to `0`, during this number of moves since the game started. `0` disables temperature decay.<br>Default: `0` |
| -n,<br>--[no-]noise | Add Dirichlet noise to root node | Add noise to root node prior probabilities. This allows the engine to explore moves which are known to be very bad, and this is useful to discover new ideas during training.<br>Default: `false` |
| <nobr>--[no-]verbose-move-stats | Display verbose move stats | Display `Q`, `V`, `N`, `U` and `P` values of every move candidate after each move.<br>Default: `false` |
| --[no-]smart-pruning  | Enable smart pruning | Default: `true` |
| --virtual-loss-bug=NUM | Virtual loss bug | Default: `0` |
| --fpu-reduction=NUM | First Play Urgency reduction | Default: `0.2` |
| --cache-history-length=NUM | The length of history to include in the cache | Default: `7` |
| --extra-virtual-loss=NUM | Extra virtual loss | Default: `0` |
| -l,<br>--logfile=FILENAME | Do debug logging into a file | Default is off (empty string) |


## Configuration Files
`lc0` supports using a configuration file instead of passing flags on the command line.  The default configuration file is `lc0.config`, but it can be changed with the `--config` command line flag.  `lc0` configuration files only support the long flags that begin with `--`, and there must only be 1 flag per line.  For example:
```
# Lines beginning with a # is a comment
--threads=1
--minibatch-size=32
--sticky-checkmate
# The -- is optional.  The following flags will work as well:
weights=10445.txt.gz
syzygy-paths=syzygy
logfile=lc0.log
```
You can tell `lc0` to ignore the default configuration file by passing `--config=` on the command line.  Command line arguments will override any arguments that also exist in the configuration file.


## Backend Configuration

To be explained. That's the most interesting and undocumented!


## Selfplay Mode

To be explained.


## Debug Mode

To be explained.

```

`README.md`:

```md
[![CircleCI](https://circleci.com/gh/LeelaChessZero/lc0.svg?style=shield)](https://circleci.com/gh/LeelaChessZero/lc0)
[![AppVeyor](https://ci.appveyor.com/api/projects/status/3245b83otdee7oj7?svg=true)](https://ci.appveyor.com/project/leelachesszero/lc0)

# Lc0

Lc0 is a UCI-compliant chess engine designed to play chess via neural network, specifically those of the [LeelaChessZero project](https://lczero.org).

## Downloading source

Lc0 can be acquired either via a git clone or an archive download from GitHub. Be aware that there is a required submodule which isn't included in source archives.

For essentially all purposes, including selfplay game generation and match play, we highly recommend using the latest `release/version` branch (for example `release/0.28`), which is equivalent to using the latest version tag.

Versioning follows the Semantic Versioning guidelines, with major, minor and patch sections. The training server enforces game quality using the versions output by the client and engine.


Download using git:

```
git clone -b release/0.28 --recurse-submodules https://github.com/LeelaChessZero/lc0.git
```

If you have cloned already an old version, fetch, view and checkout a new branch:
```
git fetch --all
git branch --all
git checkout -t remotes/origin/release/0.28
```


If you prefer to download an archive, you need to also download and place the submodule:
 * Download the [.zip](https://api.github.com/repos/LeelaChessZero/lc0/zipball/release/0.28) file ([.tar.gz](https://api.github.com/repos/LeelaChessZero/lc0/tarball/release/0.28) archive is also available)
 * Extract
 * Download https://github.com/LeelaChessZero/lczero-common/archive/master.zip (also available as [.tar.gz](https://github.com/LeelaChessZero/lczero-common/archive/master.tar.gz))
 * Move the second archive into the first archive's `libs/lczero-common/` folder and extract
 * The final form should look like `<TOP>/libs/lczero-common/proto/`

Having successfully acquired Lc0 via either of these methods, proceed to the build section below and follow the instructions for your OS.


## Building and running Lc0

Building should be easier now than it was in the past. Please report any problems you have.

Aside from the git submodule, lc0 requires the Meson build system and at least one backend library for evaluating the neural network, as well as the required `zlib`. (`gtest` is optionally used for the test suite.) If your system already has this library installed, they will be used; otherwise Meson will generate its own copy of the two (a "subproject"), which in turn requires that git is installed (yes, separately from cloning the actual lc0 repository). Meson also requires python and Ninja.

Backend support includes (in theory) any CBLAS-compatible library for CPU usage, such as OpenBLAS or Intel's DNNL or MKL. For GPUs, OpenCL and CUDA+cudnn are supported, while DX-12 can be used in Windows 10 with latest drivers.

Finally, lc0 requires a compiler supporting C++17. Minimal versions seem to be g++ v8.0, clang v5.0 (with C++17 stdlib) or Visual Studio 2017.

*Note* that cuda checks the compiler version and stops even with newer compilers, and to work around this we have added the `nvcc_ccbin` build option. This is more of an issue with new Linux versions, where we recommend to install `g++-7` and add `-Dnvcc_ccbin=g++-7` to the `build.sh` command.

Given those basics, the OS and backend specific instructions are below.

### Linux

#### Generic

1. Install backend:
    - If you want to use NVidia graphics cards Install [CUDA](https://developer.nvidia.com/cuda-zone) and [cuDNN](https://developer.nvidia.com/cudnn).
    - If you want to use AMD graphics cards install OpenCL.
    - if you want OpenBLAS version Install OpenBLAS (`libopenblas-dev`).
2. Install ninja build (`ninja-build`), meson, and (optionally) gtest (`libgtest-dev`).
3. Go to `lc0/`
4. Run `./build.sh`
5. `lc0` will be in `lc0/build/release/` directory
6. Unzip a [neural network](https://lczero.org/play/networks/bestnets/) in the same directory as the binary.

If you want to build with a different compiler, pass the `CC` and `CXX` environment variables:

    CC=clang-6.0 CXX=clang++-6.0 ./build.sh

#### Note on installing CUDA on Ubuntu

Nvidia provides .deb packages. CUDA will be installed in `/usr/local/cuda-10.0` and requires 3GB of diskspace.
If your `/usr/local` partition doesn't have that much space left you can create a symbolic link before
doing the install; for example: `sudo ln -s /opt/cuda-10.0 /usr/local/cuda-10.0`

The instructions given on the nvidia website tell you to finish with `apt install cuda`. However, this
might not work (missing dependencies). In that case use `apt install cuda-10-0`. Afterwards you can
install the meta package `cuda` which will cause an automatic upgrade to a newer version when that
comes available (assuming you use `Installer Type deb (network)`, if you'd want that (just cuda-10-0 will
stay at version 10). If you don't know what to do, only install cuda-10-0.

cuDNN exists of two packages, the Runtime Library and the Developer Library (both a .deb package).

Before you can download the latter you need to create a (free) "developer" account with nvidia for
which at least a legit email address is required (their website says: The e-mail address is not made public
and will only be used if you wish to receive a new password or wish to receive certain news or notifications
by e-mail.). Further they ask for a name, date of birth (not visible later on), country, organisation ("LeelaZero"
if you have none), primary industry segment ("Other"/none) and which development areas you are interested
in ("Deep Learning").

#### Ubuntu 18.04

For Ubuntu 18.04 you need the latest version of meson, libstdc++-8-dev, and clang-6.0 before performing the steps above:

    sudo apt-get install libstdc++-8-dev clang-6.0 ninja-build pkg-config
    pip3 install meson --user
    CC=clang-6.0 CXX=clang++-6.0 INSTALL_PREFIX=~/.local ./build.sh

Make sure that `~/.local/bin` is in your `PATH` environment variable. You can now type `lc0 --help` and start.

#### Ubuntu 16.04

For Ubuntu 16.04 you need the latest version of meson, ninja, clang-6.0, and libstdc++-8:

    wget -O - https://apt.llvm.org/llvm-snapshot.gpg.key | sudo apt-key add -
    sudo apt-add-repository 'deb http://apt.llvm.org/xenial/ llvm-toolchain-xenial-6.0 main'
    sudo add-apt-repository ppa:ubuntu-toolchain-r/test
    sudo apt-get update
    sudo apt-get install clang-6.0 libstdc++-8-dev
    pip3 install meson ninja --user
    CC=clang-6.0 CXX=clang++-6.0 INSTALL_PREFIX=~/.local ./build.sh

Make sure that `~/.local/bin` is in your `PATH` environment variable. You can now type `lc0 --help` and start.

#### openSUSE (all versions)

Instructions, packages and tools for building on openSUSE are at [openSUSE_install.md](openSUSE_install.md)

#### Docker

Use https://github.com/vochicong/lc0-docker
to run latest releases of lc0 and the client inside a Docker container.


### Windows

Here are the brief instructions for CUDA/CuDNN, for details and other options see `windows-build.md`.

0. Install Microsoft Visual Studio (2017 or later)
1. Install [CUDA](https://developer.nvidia.com/cuda-zone)
2. Install [cuDNN](https://developer.nvidia.com/cudnn).
3. Install Python3
4. Install Meson: `pip3 install --upgrade meson`
5. Edit `build.cmd`:

* Set `CUDA_PATH` with your CUDA directory
* Set `CUDNN_PATH` with your cuDNN directory (may be the same with CUDA_PATH)

6. Run `build.cmd`. It will ask permission to delete the build directory, then generate MSVS project and pause.

Then either:

7. Hit `Enter` to build it.
8. Resulting binary will be `build/lc0.exe`

Or.

7. Open generated solution `build/lc0.sln` in Visual Studio and build yourself.

### Mac

First you need to install some required packages through Terminal:
1. Install brew as per the instructions at https://brew.sh/
2. Install python3: `brew install python3`
3. Install meson: `brew install meson`
4. Install ninja: `brew install ninja`
5. (For Mac OS 10.14 Mojave, or if the other step 5 fails):
 * Install developer tools: ``xcode-select --install``
 * When using Mojave install SDK headers: `installer -pkg /Library/Developer/CommandLineTools/Packages/macOS_SDK_headers_for_macOS_10.14.pkg -target /` (if this doesn't work, use `sudo installer` instead of just `installer`.)

Or.

5. (For MacOS 10.15 Catalina, or if the other step 5 fails): 
 * Install Xcode command-line tools: ``xcode-select --install``
 * Install "XCode Developer Tools" through the app store. (First one on the list of Apps if searched.)
 * Associate the SDK headers in XCode with a command: export CPATH=\`xcrun --show-sdk-path\`/usr/include
 
Now download the lc0 source, if you haven't already done so, following the instructions earlier in the page.

6. Go to the lc0 directory.
7. Run `./build.sh -Dgtest=false` (needs step 5)

### Raspberry Pi

You'll need to be running the latest Raspberry Pi OS "buster".

1. Install OpenBLAS

```
git clone https://github.com/xianyi/OpenBLAS.git
cd OpenBLAS/
make
sudo make PREFIX=/usr install
cd ..
```

2. Install Meson

```
pip3 install meson
pip3 install ninja
```

3. Install compiler and standard libraries

```
sudo apt install clang-6.0 libstdc++-8-dev
```

4. Clone lc0 and compile

```
git clone https://github.com/LeelaChessZero/lc0.git
cd lc0
git submodule update --init --recursive
CC=clang-6.0 CXX=clang++-6.0 ./build.sh -Ddefault_library=static
```

5. The resulting binary will be in build/release

## License

Leela Chess is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Leela Chess is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

### Additional permission under GNU GPL version 3 section 7

_The source files of Lc0 with the exception of the BLAS and OpenCL
backends (all files in the `blas` and `opencl` sub-directories) have
the following additional permission, as allowed under GNU GPL version 3
section 7:_

If you modify this Program, or any covered work, by linking or
combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
modified version of those libraries), containing parts covered by the
terms of the respective license agreement, the licensors of this
Program grant you additional permission to convey the resulting work.


```

`appveyor.yml`:

```yml
version: '{build}'
configuration: Release
platform: x64
image:
- Visual Studio 2017
environment:
  matrix:
  - NAME: gpu-nvidia-cudnn
  - NAME: gpu-nvidia-cuda
  - NAME: gpu-dx12
  - NAME: gpu-opencl
  - NAME: cpu-dnnl
  - NAME: cpu-openblas
  - NAME: onednn
  - NAME: android
for:
-
  matrix:
    only:
    - NAME: gpu-opencl
    - NAME: cpu-dnnl
  skip_non_tags: true
clone_folder: c:\projects\lc0
install:
- cmd: set CUDA=false
- cmd: set CUDNN=false
- cmd: set DX=false
- cmd: set OPENCL=false
- cmd: set BLAS=false
- cmd: set ONEDNN=false
- cmd: set GTEST=false
- cmd: set ANDROID=false
- cmd: IF %NAME%==android set ANDROID=true
- cmd: IF %NAME%==gpu-nvidia-cudnn set CUDNN=true
- cmd: IF %NAME%==gpu-nvidia-cudnn set CUDA=true
- cmd: IF %NAME%==gpu-nvidia-cuda set CUDA=true
- cmd: IF %NAME%==gpu-dx12 set DX=true
- cmd: IF %NAME%==gpu-opencl set OPENCL=true
- cmd: IF %NAME%==cpu-dnnl set BLAS=true
- cmd: IF %NAME%==cpu-openblas set BLAS=true
- cmd: IF %NAME%==cpu-openblas set GTEST=true
- cmd: IF %NAME%==onednn set ONEDNN=true
- cmd: set NET=744204
- cmd: set NET_HASH=0f2f738e314bf618384045d4320a55333375d273d093adb805a4268ee53b519c
- cmd: IF NOT %BLAS%==true IF NOT %ANDROID%==true set NET=753723
- cmd: IF NOT %BLAS%==true IF NOT %ANDROID%==true set NET_HASH=3e3444370b9fe413244fdc79671a490e19b93d3cca1669710ffeac890493d198
- cmd: call "C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Auxiliary\Build\vcvarsall.bat" amd64
- cmd: set DNNL_NAME=dnnl_win_1.5.0_cpu_vcomp
- cmd: IF %NAME%==cpu-dnnl IF NOT EXIST C:\cache\%DNNL_NAME% appveyor DownloadFile https://github.com/oneapi-src/oneDNN/releases/download/v1.5/dnnl_win_1.5.0_cpu_vcomp.zip
- cmd: IF %NAME%==cpu-dnnl IF NOT EXIST C:\cache\%DNNL_NAME% 7z x dnnl_win_1.5.0_cpu_vcomp.zip -oC:\cache
- cmd: IF %NAME%==onednn set DNNL_NAME=dnnl_win_2.6.0_cpu_vcomp_gpu_vcomp
- cmd: IF %NAME%==onednn IF NOT EXIST C:\cache\%DNNL_NAME% appveyor DownloadFile https://github.com/borg323/oneDNN/releases/download/v2.6/dnnl_win_2.6.0_cpu_vcomp_gpu_vcomp.zip
- cmd: IF %NAME%==onednn IF NOT EXIST C:\cache\%DNNL_NAME% 7z x dnnl_win_2.6.0_cpu_vcomp_gpu_vcomp.zip -oC:\cache
- cmd: IF %NAME%==cpu-openblas IF NOT EXIST C:\cache\OpenBLAS appveyor DownloadFile https://sjeng.org/ftp/OpenBLAS-0.3.3-win-oldthread.zip
- cmd: IF %NAME%==cpu-openblas IF NOT EXIST C:\cache\OpenBLAS 7z x OpenBLAS-0.3.3-win-oldthread.zip -oC:\cache\OpenBLAS
- cmd: IF %OPENCL%==true nuget install opencl-nug -Version 0.777.77 -OutputDirectory C:\cache
- cmd: set ISPC=%BLAS%
- cmd: IF %NAME%==android set ISPC=true
- cmd: IF %ISPC%==true IF NOT EXIST C:\cache\ispc-v1.13.0-windows appveyor DownloadFile https://github.com/ispc/ispc/releases/download/v1.13.0/ispc-v1.13.0-windows.zip
- cmd: IF %ISPC%==true IF NOT EXIST C:\cache\ispc-v1.13.0-windows 7z x ispc-v1.13.0-windows.zip -oC:\cache\ispc-v1.13.0-windows
- cmd: IF %ISPC%==true set PATH=C:\cache\ispc-v1.13.0-windows\bin;%PATH%
- cmd: set "CUDA_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0"
- cmd: IF %CUDNN%==true IF NOT EXIST "%CUDA_PATH%\cuda" set CUDNN_INSTALL=1
- cmd: IF DEFINED CUDNN_INSTALL appveyor DownloadFile https://developer.nvidia.com/compute/cuda/10.0/Prod/network_installers/cuda_10.0.130_win10_network
- cmd: IF DEFINED CUDNN_INSTALL cuda_10.0.130_win10_network -s nvcc_10.0 cublas_dev_10.0 cublas_10.0 cudart_10.0
- cmd: IF DEFINED CUDNN_INSTALL appveyor DownloadFile http://developer.download.nvidia.com/compute/redist/cudnn/v7.4.2/cudnn-10.0-windows10-x64-v7.4.2.24.zip
- cmd: IF DEFINED CUDNN_INSTALL 7z x cudnn-10.0-windows10-x64-v7.4.2.24.zip -o"%CUDA_PATH%"
- cmd: IF %CUDNN%==false set "CUDA_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1"
- cmd: IF %CUDA%==true IF NOT EXIST "%CUDA_PATH%" set CUDA_INSTALL=1
- cmd: IF DEFINED CUDA_INSTALL appveyor DownloadFile https://developer.download.nvidia.com/compute/cuda/11.1.0/network_installers/cuda_11.1.0_win10_network.exe
- cmd: IF DEFINED CUDA_INSTALL cuda_11.1.0_win10_network.exe -s nvcc_11.1 cublas_dev_11.1 cublas_11.1 cudart_11.1 documentation_11.1
- cmd: IF %CUDA%==true set PATH=%CUDA_PATH%\bin;%PATH%
- cmd: set PATH=C:\Python36;C:\Python36\scripts;%PATH%
- cmd: pip3 install --upgrade meson==0.55.3
- cmd: set MIMALLOC_PATH=C:\cache\mimalloc-1.7.1
- cmd: IF %ANDROID%==false IF NOT EXIST "%MIMALLOC_PATH%" appveyor DownloadFile https://github.com/microsoft/mimalloc/archive/refs/tags/v1.7.1.zip
- cmd: IF %ANDROID%==false IF NOT EXIST "%MIMALLOC_PATH%" 7z x v1.7.1.zip -oC:\cache\
- cmd: IF %ANDROID%==false IF NOT EXIST "%MIMALLOC_PATH%"\out msbuild "%MIMALLOC_PATH%"\ide\vs2017\mimalloc-override.vcxproj /p:Configuration=Release /m
- cmd: IF %NAME%==android IF NOT EXIST C:\ndk\android-ndk-r19c\toolchains\llvm\prebuilt\windows-x86_64 appveyor DownloadFile https://dl.google.com/android/repository/android-ndk-r19c-windows-x86_64.zip
- cmd: IF %NAME%==android IF NOT EXIST C:\ndk\android-ndk-r19c\toolchains\llvm\prebuilt\windows-x86_64 7z x android-ndk-r19c-windows-x86_64.zip -oC:\ndk
- cmd: IF %NAME%==android set PATH=C:\ndk\android-ndk-r19c\toolchains\llvm\prebuilt\windows-x86_64\bin;%PATH%
- cmd: IF %NAME%==android sed "s/clang+*/&.cmd/" cross-files/aarch64-linux-android >crossfile-aarch64
- cmd: IF %NAME%==android IF NOT EXIST C:\cache\OpenBLAS\android-aarch64 appveyor DownloadFile https://github.com/borg323/OpenBLAS/releases/download/android-0.3.8-2/openblas-android-aarch64.zip
- cmd: IF %NAME%==android IF NOT EXIST C:\cache\OpenBLAS\android-aarch64 7z x openblas-android-aarch64.zip -oC:\cache\OpenBLAS
- cmd: IF %NAME%==android sed "s/clang+*/&.cmd/" cross-files/armv7a-linux-android >crossfile-armv7a
- cmd: IF %NAME%==android IF NOT EXIST C:\cache\OpenBLAS\android-armv7a appveyor DownloadFile https://github.com/borg323/OpenBLAS/releases/download/android-0.3.8-2/openblas-android-armv7a.zip
- cmd: IF %NAME%==android IF NOT EXIST C:\cache\OpenBLAS\android-armv7a 7z x openblas-android-armv7a.zip -oC:\cache\OpenBLAS
- cmd: set PKG_FOLDER="C:\cache"
- cmd: IF NOT EXIST c:\cache mkdir c:\cache
- cmd: IF NOT EXIST c:\cache\%NET%.pb.gz appveyor DownloadFile http://training.lczero.org/get_network?sha=%NET_HASH% -Filename c:\cache\%NET%.pb.gz
- cmd: touch -t 201801010000.00 c:\cache\%NET%.pb.gz
- cmd: IF %GTEST%==true IF NOT EXIST C:\cache\syzygy mkdir C:\cache\syzygy
- cmd: IF %GTEST%==true cd C:\cache\syzygy
- cmd: IF %GTEST%==true IF NOT EXIST KQvK.rtbz curl --remote-name-all https://tablebase.lichess.ovh/tables/standard/3-4-5/K{P,N,R,B,Q}vK.rtb{w,z}
- cmd: IF %GTEST%==true IF NOT EXIST KQQvK.rtbz curl --remote-name-all https://tablebase.lichess.ovh/tables/standard/3-4-5/K{P,N,R,B,Q}{P,N,R,B,Q}vK.rtb{w,z}
- cmd: IF %GTEST%==true IF NOT EXIST KQvKQ.rtbz curl --remote-name-all https://tablebase.lichess.ovh/tables/standard/3-4-5/K{P,N,R,B,Q}vK{P,N,R,B,Q}.rtb{w,z}
- cmd: cd C:\projects\lc0
cache:
  - C:\cache
  - 'C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0'
  - 'C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1'
  - C:\projects\lc0\subprojects\packagecache
  - C:\ndk\android-ndk-r19c\toolchains\llvm\prebuilt\windows-x86_64
before_build:
- cmd: git submodule update --init --recursive
- cmd: IF %BLAS%==true (echo.#define DEFAULT_MINIBATCH_SIZE 7 & echo.#define DEFAULT_MAX_PREFETCH 0 & echo.#define DEFAULT_TASK_WORKERS 0) > params_override.h
- cmd: IF %ANDROID%==true (echo.#define DEFAULT_MINIBATCH_SIZE 7 & echo.#define DEFAULT_MAX_PREFETCH 0 & echo.#define DEFAULT_TASK_WORKERS 0) > params_override.h
- cmd: SET BUILD_BLAS=%BLAS%
- cmd: IF %OPENCL%==true SET BUILD_BLAS=true
- cmd: IF %DX%==true SET BUILD_BLAS=true
- cmd: SET EMBED=false
- cmd: IF %APPVEYOR_REPO_TAG%==true IF %ANDROID%==true SET EMBED=true
- cmd: SET EXTRA=
- cmd: IF %ANDROID%==false SET EXTRA=-Db_vscrt=md
- cmd: IF %ANDROID%==false meson build --backend vs2017 --buildtype release -Dgtest=%GTEST% -Dopencl=%OPENCL% -Dblas=%BUILD_BLAS% -Ddnnl=true -Ddx=%DX% -Dcudnn=%CUDNN% -Donednn=%ONEDNN% -Dispc_native_only=false -Dpopcnt=false -Dcudnn_include="%CUDA_PATH%\include","%CUDA_PATH%\cuda\include" -Dcudnn_libdirs="%CUDA_PATH%\lib\x64","%CUDA_PATH%\cuda\lib\x64" -Dopenblas_include="%PKG_FOLDER%\OpenBLAS\dist64\include" -Dopenblas_libdirs="%PKG_FOLDER%\OpenBLAS\dist64\lib" -Ddnnl_dir="%PKG_FOLDER%\%DNNL_NAME%" -Dopencl_include="%PKG_FOLDER%\opencl-nug.0.777.77\build\native\include" -Dopencl_libdirs="%PKG_FOLDER%\opencl-nug.0.777.77\build\native\lib\x64" -Ddefault_library=static -Dmalloc=mimalloc -Dmimalloc_libdir="%MIMALLOC_PATH%"\out\msvc-x64\Release %EXTRA%
- cmd: IF %ANDROID%==true meson arm64-v8a --buildtype release -Dgtest=false -Dopenblas_include="%PKG_FOLDER%\OpenBLAS\android-aarch64\include" -Dopenblas_libdirs="%PKG_FOLDER%\OpenBLAS\android-aarch64\lib" -Dembed=%EMBED% -Ddefault_library=static --cross-file crossfile-aarch64
- cmd: IF %ANDROID%==true meson armeabi-v7a --buildtype release -Dgtest=false -Dopenblas_include="%PKG_FOLDER%\OpenBLAS\android-armv7a\include" -Dopenblas_libdirs="%PKG_FOLDER%\OpenBLAS\android-armv7a\lib" -Dembed=%EMBED% -Ddefault_library=static --cross-file crossfile-armv7a -Dispc=false -Dneon=false
build_script:
- cmd: IF %ANDROID%==false call scripts\appveyor_win_build.cmd
- cmd: IF %ANDROID%==true call scripts\appveyor_android_build.cmd
- cmd: cd C:\projects\lc0
after_build:
- cmd: IF %APPVEYOR_REPO_TAG%==true IF %ANDROID%==false call scripts\appveyor_win_package.cmd
- cmd: IF %APPVEYOR_REPO_TAG%==true IF %ANDROID%==true call scripts\appveyor_android_package.cmd
- cmd: cd C:\projects\lc0
artifacts:
  - path: build/lc0.exe
    name: lc0-$(NAME)
  - path: arm64-v8a/lc0
    name: lc0-android-arm64-v8a
  - path: armeabi-v7a/lc0
    name: lc0-android-armeabi-v7a
  - path: /lc0*.zip/
    name: lc0-$(APPVEYOR_REPO_TAG_NAME)-windows-$(NAME)-zip
  - path: build/lc0.pdb
    name: lc0-debug-symbols
  - path: /lc0*.apk/
    name: lc0-$(APPVEYOR_REPO_TAG_NAME)-android-apk
  - path: dnnl.dll
    name: dnnl-dll
deploy:
  - provider: GitHub
    artifact: /.*\.zip/
    auth_token:
      secure: USFAdwQKTXqOXQjCYQfzWvzRpUhvqJLBkN4hbOg+j876vDxGZHt9bMYayb5evePp
    on:
      appveyor_repo_tag: true
  - provider: GitHub
    artifact: /.*\.apk/
    auth_token:
      secure: USFAdwQKTXqOXQjCYQfzWvzRpUhvqJLBkN4hbOg+j876vDxGZHt9bMYayb5evePp
    on:
      appveyor_repo_tag: true
test_script:
- cmd: IF %GTEST%==true cd build
- cmd: IF %GTEST%==true xcopy /s /i C:\cache\syzygy syzygy
- cmd: IF %GTEST%==true meson test --print-errorlogs
- cmd: cd C:\projects\lc0
on_finish:
- cmd: IF %GTEST%==true cd C:\projects\lc0\build
- cmd: IF %GTEST%==true for %%a in (*.xml) do curl -F file=@%%a https://ci.appveyor.com/api/testresults/junit/%APPVEYOR_JOB_ID%
- cmd: cd C:\projects\lc0

```

`build.cmd`:

```cmd
@echo off
setlocal

rem 1. Set the following for the options you want to build.
set CUDNN=true
set CUDA=true
set DX12=false
set OPENCL=false
set MKL=false
set DNNL=false
set OPENBLAS=false
set EIGEN=false
set TEST=false

rem 2. Edit the paths for the build dependencies.
set CUDA_PATH=C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v10.0
set CUDNN_PATH=%CUDA_PATH%
set OPENBLAS_PATH=C:\OpenBLAS
set MKL_PATH=C:\Program Files (x86)\IntelSWTools\compilers_and_libraries\windows\mkl
set DNNL_PATH=C:\dnnl_win_1.1.1_cpu_vcomp
set OPENCL_LIB_PATH=%CUDA_PATH%\lib\x64
set OPENCL_INCLUDE_PATH=%CUDA_PATH%\include

rem 3. In most cases you won't need to change anything further down.
echo Deleting build directory:
rd /s build

set CC=cl
set CXX=cl
set CC_LD=link
set CXX_LD=link

if exist "C:\Program Files (x86)\Microsoft Visual Studio\2019" (
  where /q cl
  if errorlevel 1 call "C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC\Auxiliary\Build\vcvarsall.bat" amd64
  set backend=vs2019
) else (
  where /q cl
  if errorlevel 1 call "C:\Program Files (x86)\Microsoft Visual Studio\2017\Community\VC\Auxiliary\Build\vcvarsall.bat" amd64
  set backend=vs2017
)

set BLAS=true
if %MKL%==false if %DNNL%==false if %OPENBLAS%==false if %EIGEN%==false set BLAS=false

if "%CUDA_PATH%"=="%CUDNN_PATH%" (
  set CUDNN_LIB_PATH=%CUDNN_PATH%\lib\x64
  set CUDNN_INCLUDE_PATH=%CUDNN_PATH%\include
) else (
  set CUDNN_LIB_PATH=%CUDA_PATH%\lib\x64,%CUDNN_PATH%\lib\x64
  set CUDNN_INCLUDE_PATH=%CUDA_PATH%\include,%CUDNN_PATH%\include
)

if %CUDNN%==true set PATH=%CUDA_PATH%\bin;%PATH%

meson build --backend %backend% --buildtype release -Ddx=%DX12% -Dcudnn=%CUDNN% -Dplain_cuda=%CUDA% ^
-Dopencl=%OPENCL% -Dblas=%BLAS% -Dmkl=%MKL% -Dopenblas=%OPENBLAS% -Ddnnl=%DNNL% -Dgtest=%TEST% ^
-Dcudnn_include="%CUDNN_INCLUDE_PATH%" -Dcudnn_libdirs="%CUDNN_LIB_PATH%" ^
-Dmkl_include="%MKL_PATH%\include" -Dmkl_libdirs="%MKL_PATH%\lib\intel64" -Ddnnl_dir="%DNNL_PATH%" ^
-Dopencl_libdirs="%OPENCL_LIB_PATH%" -Dopencl_include="%OPENCL_INCLUDE_PATH%" ^
-Dopenblas_include="%OPENBLAS_PATH%\include" -Dopenblas_libdirs="%OPENBLAS_PATH%\lib" ^
-Ddefault_library=static

if errorlevel 1 exit /b

pause

cd build

msbuild /m /p:Configuration=Release /p:Platform=x64 /p:WholeProgramOptimization=true ^
/p:PreferredToolArchitecture=x64 lc0.sln /filelogger

```

`build.sh`:

```sh
#!/usr/bin/env bash

pushd "$(dirname "$0")"

set -e

case $1 in
  plain|debug|debugoptimized|release|minsize)
    BUILDTYPE=$1
    shift
    ;;
  *)
    BUILDTYPE=release
    ;;
esac

BUILDDIR=build/${BUILDTYPE}

if ! hash meson 2>/dev/null && [ -x ${HOME}/.local/bin/meson ]
then
  export PATH=${PATH}:${HOME}/.local/bin
fi

if [ -f ${BUILDDIR}/build.ninja ]
then
  meson configure ${BUILDDIR} -Dbuildtype=${BUILDTYPE} -Dprefix=${INSTALL_PREFIX:-/usr/local} "$@"
else
  meson ${BUILDDIR} --buildtype ${BUILDTYPE} --prefix ${INSTALL_PREFIX:-/usr/local} "$@"
fi

cd ${BUILDDIR}

NINJA=$(awk '/ninja/ {ninja=$4} END {print ninja}' meson-logs/meson-log.txt)

if [ -n "${INSTALL_PREFIX}" ]
then
  ${NINJA} install
else
  ${NINJA}
fi

popd

```

`changelog.txt`:

```txt
v0.29.0-rc0 (2022-04-03)
~~~~~~~
* Initial support for attention policy, only cuda backend and partially in
  blas/dnnl/eigen (good enough for T79).
* Non multigather (legacy) search code and `--multigather` option are removed.
* 15b default net is now 753723.
* The onnx backend now allows selecting gpu to use.
* Improved error messages for unsupported network files.
* Some assorted fixes.

v0.28.2 (2021-12-13)
~~~~~~~
* No changes from v0.28.1-rc1 as the v0.28.1 release was botched.

v0.28.1 (2021-12-12)
~~~~~~~
* No changes from rc1.

v0.28.1-rc1 (2021-12-05)
~~~~~~~
* Improved cuda performance for 512 filter networks on Amprere GPUs.
* Several fixes for the onnx backend.
* Command line options for network file conversion to/from onnx.
* Documentation updates.
* Correctness fixes for rescorer support functions.

v0.28.0 (2021-08-25)
~~~~~~~
* Fixed an issue with small third-party nets on the cuda/cudnn backends.
* Minor tweak to the default task-workers for the cpu packages.

v0.28.0-rc2 (2021-08-20)
~~~~~~~
* The cuda backend option multi_stream is now off by default. You should
  consider setting it to on if you have a recent gpu with a lot of vram.
* The default time manager is back to "legacy".
* Updated default parameters.
* Newer and stronger nets are included in the release packages.
* Added support for onnx network files and runtime with the "onnx" backend.
* Several bug and stability fixes.

v0.28.0-rc1 (2021-06-16)
~~~~~~~
* Multigather is now made the default (and also improved). Some search settings
  have changed meaning, so if you have modified values please discard them.
  Specifically, `max-collision-events`, `max-collision-visits` and
 `max-out-of-order-evals-factor` have changed default values, but other options
  also affect the search. Similarly, check that your gui is not caching the old
  values.
* Performance improvements for the cuda/cudnn backends.
* Support for policy focus during training.
* Larger/stronger 15b default net for all packages except android, blas and dnnl
  that get a new 10b network.
* The distributed binaries come with the mimalloc memory allocator for better
  performance when a large tree has to be destroyed (e.g. after an unexpected
  move).
* The `legacy` time manager will use more time for the first move after a long
  book line.
* The `--preload` command line flag will initialize the backend and load the
  network during startup.
* A 'fen' command was added as a UCI extension to print the current position.
* Experimental onednn backend for recent intel cpus and gpus.

v0.27.0 (2021-02-21)
~~~~~~~
* A better value for the backendbench Clippy threshold.

v0.27.0-rc2 (2021-02-18)
~~~~~~~
* Fix additional cases where 'invalid move' could be incorrectly reported.
* Replace WDL softmax in cudnn backend with same implementation as cuda
  backend. This fixes some inaccuracy issues that were causing training
  data to be rejected at a fairly low frequency.
* Ensure that training data Q/D pairs form valid WDL targets even if there
  is accumulated drift in calculation.
* Fix for the calculation of the 'best q is proven' bit in training data.
* Multiple fixes for timelosses and infinite instamoving in smooth time
  manager. Smooth time manager now made default after these fixes.

v0.27.0-rc1 (2021-02-06)
~~~~~~~
* Fix a bug which meant `position ... moves ...` didn't work if the moves 
  went off the end of the existing tree. 

v0.27.0-rc0 (2021-02-06)
~~~~~~~
* Multigather search inspired by Ceres.
* V6 training format with additional info for training experiments.
* Updated default search parameters.
* A better algorithm for the backendbench assistant.
* Terminate search early if only 1 move isn't a proven loss.
* Various build system changes.

v0.26.3 (2020-10-10)
~~~~~~~
* Increased maximum value of TempDecayMoves.

v0.26.3-rc2 (2020-10-03)
~~~~~~~
* Fix for uninitialized variable that led to crashes with the cudnn backend.
* Correct windows support for systems with more than 64 threads.
* A new package is built for the `cuda` backend with cuda 11.1. The old cuda
  package is renamed to `cudnn`.

v0.26.3-rc1 (2020-09-28)
~~~~~~~
* Residual block fusion optimization for cudnn backend, that depends on
  `custom_winograd=true`. Enabled by default only for networks with up to 384
  filters in fp16 mode and never in fp32 mode. Default can be overridden with
  `--backend-opts=res_block_fusing=false` to disable (or `=true` to enable).
* New experimental cuda backend without cudnn dependency (`cuda-auto`, `cuda`
  and `cuda-fp16` are available).

v0.26.2 (2020-08-31)
~~~~~~~
* No changes from rc1.

v0.26.2-rc1 (2020-08-28)
~~~~~~~~~~~
* Repetitions in the search tree are marked as draws, to explore more promising
  lines. Enabled by default (except in selfplay mode) use
  `--two-fold-draws=false` to disable.
* Syzygy tablebase files can now be used in selfplay. Still need to add
  adjudication support before we can consider using this for training.
* Default net updated to 703810.
* Fix for book with CR/LF line endings.
* Updated Eigen wrap to use new download link.

v0.26.1 (2020-07-15)
~~~~~~~
* Fix a bug where invalid openings-pgn settings would result in the book
  being ignored rather than used.
* Add support for compressed book files.

v0.26.0 (2020-07-03)
~~~~~~~
* No changes from rc1.

v0.26.0-rc1 (2020-06-29)
~~~~~~~~~~~

* Verbose move stats now includes a line for the root node itself.
* Added optional `alphazero` time manager type for fixed fraction of 
  remaining time per move. 
* The WL score is now tracked with double pecision to improve accuracy
  during very long search.
* Fix for a performance bug when playing from tablebase position with
  tablebases enabled and the PV move was changing frequently.
* Illegal searchmove restrictions will now be ignored rather than crash.
* Policy is cleared for terminal losses to encourage better quality MLH
  estimates by reducing how many visits a move that will not be selected
  (unless all other options are equally bad) receives.
* Smart pruning will now cause leela to play immediately once mate score has
  been declared.
* Fix an issue where sometimes the pv reported wouldn't match the move that
  would be selected at that moment.
* Improvement for logic for when to disable custom_winograd optimization to
  avoid running out of video ram.
* `--show-hidden` can now be specified after `--help` and still work.
* Performance tuning for populating the policy into nodes after nn eval
  completes.
* Enable custom optimized SE paths for nets with 384 filters when using the
  custom_winograd=false path.
* Updates to zlib/gtest/eigen when included via meson wrap.
* Added build option to build python bindings to the lc0 engine.
* Only show the git hash in uci name if not a release tag build.
* Add `--nps-limit` option to artificially reduce nps to make for easier
  opponent or whatever other reason you want.
* Fixed a bug where search tree shape could be affected even when the
  `--smart-pruning-factor` setting was 0.
* Changed the search logic to find the lc0.config file if left on the default
  value.
* Changed the search logic to find network files in autodiscover mode.
* Changed the logic to determine the default location for training games
  generated by selfplay in training mode.
* Changed the logic to decide where to look for the opencl backend tuning
  settings file.
* Android binaries published by appveyor are now stripped.
* Build can now use system installed eigen if available.
* When nodes in the tree get proven terminal, parents are updated as if they
  had always been terminal. This allows for faster convergence on more
  accurate MLH estimates amongst other details.
* Removed shortsightedness and logit-q options that have not found a reliable
  use case.
* Fixed a bug where m_effect calculated as part of S in verbose move stats was
  not consistent with the value used in search itself.
* Added 'pro' mode as an alternative to `--show-hidden` for UCI hosts that do
  not support command line arguments. Simply rename the lc0 binary to include
  'pro' in order to enable.
* `backendbench` now has a `--clippy` option to try and auto suggest which
  batch size is a good idea.
* The demux backend now splits the batch into equal sizes based on the number
  of threads that demux is using rather than number of backends. By default
  this is no change as usually there is 1 thread per backend. But it allows
  to more easily use demux against a blas backend sending one chunk per core.
* Added support for new training input variants canonical_hectoplies and 
  canonical_hectoplies_armageddon.
* Fixed a bug where if the network search paths for autodiscover contain files
  which lc0 cannot open it would error out rather than continuing on to other
  files.
* Blas backends no longer have a `blas_cores` option, as it never seemed useful
  compared to running more threads at a higher level.
* `--help-md` option removed as it was deemed not very useful.
* Updated to the latest version of dnnl for the dnnl build.
* Selfplay mode now supports per color settings in addition to per player
  settings. Per player settings have higher priority if there is a conflict.
  This will be used as part of armageddon training.
* Added a new experimental backend type: `recordreplay`. This allows to
  record the output of a backend under a particular search and then replay it
  back again later. Theoretically this lets you simulate a CPU bottlenecked
  environment but still use a search tree that is a match for what might be a
  GPU bottlenecked environment. In practice there are a lot of corner cases
  where replay is not reliable yet.  At a minimum you must disable prefetch.
* During search the node tree is occasionally compacted to reduce cache misses
  during the search tree walk. New option `--solid-tree-threshold` can be used
  to adjust how aggressive this optimization is.  Note that very small values
  can cause very large growth in ram usage and are not a good idea. The default
  value is a little conservative, if you have plenty of spare ram it can be
  good to decrease it a bit.
* Small performance optimization for windows build with MLH enabled.
* Meson configuration changed to build with LTO by default. Note that meson
  does not always configure visual studio project files to apply this
  correctly on windows.
* The included net in appveyor builds is now 703350. This network supports MLH
  although the default MLH parameters are still threshold 1.0 which means it
  will not trigger without parameter adjustment.
* New backend option to explicitly override the net details and force MLH
  disabled. If you weren't going to use MLH anyway, this may give a tiny nps
  increase.
* New flag `--show-movesleft` (or `UCI_ShowMovesLeft` for UCI hosts that
  support it) will cause movesleft (in moves) to be reported in the uci info
  messages. Only works with networks that have MLH enabled.
* More sensible default values for MLH are in. Note that threshold is still
  1.0 by default, so that will still need to be configured to enable it.
* The `smooth-experimental` time manager has been renamed `smooth` and support
  added to increase search time whenever the best N does not correspond with
  the move with best utility estimate. `legacy` remains the default for now
  as `smooth` has only been tuned for short time controls and evidence suggests
  it doesn't scale with these defaults.
* Selfplay mode now supports a logfile parameter just like normal mode.
* Reinstated the 4 billion visit limit on search to avoid overflowing counters
  and causing very strange behavior to occur.
* Performance optimization to make tree walk faster by ensuring that node
  edges are always sorted by policy. This has some very small side effects to
  do with tiebreaks in search no longer always being dominated by movegen
  order.
* Appveyor built blas and Android binaries now default to minibatch size 1
  and prefetch 0, which should be much better than the normal GPU optimized
  defaults. Note this *only* affects Appveyor built binaries.
* The included client in Windows Appveyor releases is now v27 and is named
  `lc0-training-client.exe` instead of `client.exe`.

v0.25.1 (2020-04-30)
~~~~~~~

* Fixed some issues with cudnn backend on the 16xx GTX models and also for 
  low memory devices with large network files where the new optimizations
  could result in out of memory errors.
* Added a workaround for a cutechess issue where reporting depth 0 during
  instamoves causes it to ignore our info message.

v0.25.0 (2020-04-28)
~~~~~~~

* Relax strictness for complete standard fens in uci and opening books. Fen
  must still be standard, but default values will be substituted for sections
  that are missing.
* Restore some backwards compatibility in cudnn backends that was lost with
  the addition of the new convolution implementation. It is also on by default
  for more scenarios, although still off for fp16 on RTX gpus.
* Small logic fix for nps smoothing in the new optional experimental time 
  manager.

v0.25.0-rc2 (2020-04-23)
~~~~~~~~~~~

* Increased upper limit for maximum collision events.
* Allow negative values for some of the extended moves left head parameters.
* Fix a critical bug in training data generation for input type 3.
* Fix for switching between positions in uci mode that only differ by 50 move
  rule in initial fen.
* Some refinements of certainty propagation.
* Better support for c++17 implementations that are missing charconv.
* Option to more accurately apply time management for uci hosts using 
  cuteseal or similar timing techniques.
* Fix for selfplay mode to allow exactly book length total games.
* Fix for selfplay opening books with castling moves starting from chess960 fens.
* Add build option to override nvcc compiler.
* Improved validity checking for some uci input parameters.
* Updated the Q to CP conversion formula to better fit recent T60 net outputs to
  expectations.
* Add a new experimental time manager.
* Bug fix for the Q+U in verbose move stats. It is now called S: and contains
  the total score, including any moves left based effect if applicable.
* New temperature decay option to allow to delay the start of decay.
* All temperature options have been hidden by default.
* New optional cuda backend convolution implementation. Off by default for 
  cudnn-fp16 until an issue with cublas performance on some gpus is resolved.

v0.25.0-rc1 (2020-04-09)
~~~~~~~~~~~

* Now requires a c++17 supporting compilation environment to build.
* Support for Moves Left Head based networks. Includes options to adjust search
  to favour shorter/longer wins/losses based on the moves left head output.
* Mate score reporting is now possible, and move selection will prefer shorter
  mates over longer ones when they are proven.
* Training now outputs v5 format data. This passes the moves left information
  back to training. This also includes support for multiple sub formats, 
  including the existing standard, a new variant which can encode FRC960
  castling, and also a further extension of that which tries to make training
  data cannonical, so there aren't multiple positions that are trivially
  equivalent with different network inputs.
* Benchmark now includes a suite of 34 positions to test by default instead of
  just start position.
* Tensorflow backend works once more, almost just as hard to compile as it used
  to be though.
* `--noise` flag is gone, use `--noise-epsilon=0.25` to get the old behavior.
* Some bug fixes related to drawscore.
* Selfplay mode now defaults to the same value as match play for 
  `--root-has-own-cpuct-params` (true).
* Some advanced time management parameters are now accessed via the new 
  `--time-manager` parameter instead of individual parameters.
* Windows build script has been modernized.
* Separate Eigen backend option for CPU.
* Random backend no longer requires a network.
* Random backend supports producing training data of any input format sub type.
* Integer parameters now give better error messages when given invalid values.

v0.24.1 (2020-03-15)
~~~~~~~

* Fix issues where logitq was being passed as drawscore and logitq wasn't 
  passed to some GetQ calls. Causing major performance issues when either 
  setting was non-default.

v0.24.0 (2020-03-11)
~~~~~~~

* New parameter `--max-out-of-order-evals-factor` replaces 
  `--max-out-of-order-evals` that was introduced in v0.24.0-rc3 and provides
  the factor to multiply the maximum batch size to set maximum number
  out-of-order evals per batch. The default value of 1.0 keeps the behavior
  of previous releases.
* Bug fix for hangs with very early stop command from non-conforming UCI hosts.

v0.24.0-rc3 (2020-03-08)
~~~~~~~~~~~

* New parameter `--max-out-of-order-evals` to set maximum number out-of-order
  evals per batch (was equal to the batch size before).
* It's now possible to embed networks into the binary. It allows easier builds
  of .apk for Android.
* New parameter `--smart-pruning-minimum-batches` to only allow smart pruning
  to stop after at least k batches, preventing insta-moves on slow backends.

v0.24.0-rc2 (2020-03-01)
~~~~~~~~~~~

* All releases are now bundled with network id591226 (and the file date is old 
  enough so it has a lower priority than networks that you already may have
  in your directory).
* Added a 'backendbench' mode to benchmark NN evaluation performance without
  search.
* Android builds are added to the official releases.

v0.24.0-rc1 (2020-02-23)
~~~~~~~~~~~

* Introduced DirectX12 backend.
* Optimized Cpuct/FPU parameters are now default.
* There is now a separate set of CPuct parameters for the root node.
* Support of running selfplay games from an opening book.
* It's possible to adjust draw score from 0 to something else.
* There is a new --max-concurrent-seachers parameter (default is 1) which
  helps with thread congestion at the beginning of the search.
* Cache fullness is not reported in UCI info line by default anymore.
* Removed libproto dependency.

v0.23.3 (2020-02-18)
~~~~~~~

* Fix a bug in time management which sometimes led to insta-moves in long time
  control.

v0.23.2 (2019-12-31)
~~~~~~~

* Fixed a bug where odd length openings had reversed training data results in
  selfplay.
* Fixed a bug where zero length training games could be generated due to
  discard pile containing positions that were already considered end of game.
* Add cudnn-auto backend.

v0.23.1 (2019-12-03)
~~~~~~~

* Fixed a bug with Lc0 crashing sometimes during match phase of training game
  generation.
* Release packages now include CUDNN version without DLLs bundled.

v0.23.0 (2019-12-01)
~~~~~~~

* Fixed the order of BLAS options so that Eigen is lower priority, to match
  assumption in check_opencl patch introduced in v0.23.0-rc2.

v0.23.0-rc2 (2019-11-27)
~~~~~~~~~~~

* Fixes in nps and time reporting during search.
* Introduced DNNL BLAS build for modern CPUs in addition to OpenBLAS.
* Build fixes on MacOS without OpenCL.
* Fixed smart pruning and KLDGain trying to stop search in `go infinite` mode.
* OpenCL package now has check_opencl tool to find computation behaves sanely.
* Fixed a bug in interoperation of shortsighteness and certainty propagation.

v0.23.0-rc1 (2019-11-21)
~~~~~~~~~~~

* Support for Fischer Random Chess (`UCI_Chess960` option to enable FRC-style
  castling). Also added support for FRC-compatible weight files, but no training
  code yet.
* New option `--logit-q` (UCI: `LogitQ`). Changes subtree selection algorithm a
  bit, possibly making it stronger (experimental, default off).
* Lc0 now reports WDL score. To enable it, use `--show-wdl` command-line
  argument or `UCI_ShowWdl` UCI option.
* Added "Badgame split" mode during the training. After the engine makes
  inferior move due to temperature, the game is branched and later the game is
  replayed from the position of the branch.
* Added experimental `--short-sightedness` (UCI: `ShortSightedness`) parameter.
  Treats longer variations as more "drawish".
* Lc0 can now open Fat Fritz weight files.
* Time management code refactoring. No functional changes, but will make time
  management changes easier.
* Lc0 logo is now printed in red! \o/
* Command line argument `-v` is now short for `--verbose-move-stats`.
* Errors in `--backend-opts` parameter syntax are now reported.
* The most basic version of "certainty propagation" feature (actually without
  "propagation"). If the engine sees checkmate, it plays it!
  (before it could play other good move).
* Benchmark mode no longer supports smart pruning.
* Various small changes: hidden options to control Dirichlet noise, floating
  point optimizations, Better error reporting if there is exception in worker
  thread, better error messages in CUDA backend.

v0.22.0 (2019-08-05)
~~~~~~~

(no changes)

v0.22.0-rc1 (2019-08-03)
~~~~~~~~~~~

* Remove softmax calculation from backends and apply it after filtering for
  illegal moves to ensure spurious outputs on illegal moves don't reduce (or
  entirely remove) the quality of the policy values on the legal moves.
* Fix for blas backend allocation bug with small network sizes.
* The blas backend can be built with eigen - the result is reasonably optimized
  for the build machine.
* Other small tweaks piled up in master branch.


v0.21.4 (2019-07-28)
~~~~~~~~~~~~~~~~~~~~

* A fix for crashes that can occur during use of sticky-endgames.
* Change the false positive value reported when in wdl style resign and display
  average nodes per move as part of tournament stats in selfplay mode.

v0.21.3 (2019-07-21)
~~~~~~~

* Fix for potential memory corruption/crash in using small networks or using the
  wdl head with cuda backends. (#892)
* Fix for building with newer versions of meson. (#904)

v0.21.2 (2019-06-09)
~~~~~~~

* Divide by a slightly smaller divisor to truncate to +/-12800. (#880)

v0.21.2-rc3 (2019-06-08)
~~~~~~~~~~~

* Centipawn conversion (#860)

v0.21.2-rc2 (2019-05-22)
~~~~~~~~~~~

* Add 320 and 352 channel support for fused SE layer (#855)
* SE layer fix when not using fused kernel (#852)
* Fp16 nchw for cudnn-fp16 backend (support GTX 16xx GPUs) (#849)

v0.21.2-rc1 (2019-05-05)
~~~~~~~~~~~

* Make --sticky-endgames on by default (still off in training) (#844)
* update download links in README (#842)
* Recalibrate centipawn formula (#841)
* Also make parents Terminal if any move is a win or all moves are loss or draw. (#822)
* Use parent Q as a default score instead of 0 for unvisited pv. (#828)
* Add stop command to selfplay interactive mode to allow for graceful exit. (#810)
* Increased hard limit on batch size in opencl backend to 32 (#807)

v0.21.0-rc2 (2019-03-06)
~~~~~~~~~~~

* Add support for cudnn7.0 (#717)
* Informative Tournament Stats (#698)
* Memory leak fix cuda backend (#747)
* cudnn-fp16 fallback path for unusual se-ratios. (#739)
* Cudnn 7.4.2 in packaged binary and warning for using old cudnn with new gpu (#741)
* Move mode specific options to end of help. (#745)
* LogLiveStats hidden option (#754)
* Optional markdown support for help output (#769)
* Improved folding of batch norm into weights and biases - fixes negative gamma bug. (#779)

v0.21.0-rc1 (2019-02-16)
~~~~~~~~~~~

* Check Syzygy tablebase file sizes for corruption (#690)
* search for nvcc on the path first (#709)
* AZ-style policy head support (#712) 
* Implement V4TrainingData (#722)
* WDL value head support (#635)
* Add option for doing kldgain thresholding rather than absolute visit
  limiting (#721)
* Easily run latest releases of lc0 and client using NVIDIA docker (#621)
* Add WDL style resign option. (#724)
* Add a uniform output option for random backend to support a0 seed data
  style (#725)
* Fix c hw switching in cudnn-fp16 mode with convolution policy head.
  (#729)
* misc (non-functional) changes to cudnn backend (#731)
* handle 64 filter SE networks (#624)

v0.20.2 (2019-02-01)
~~~~~~~~~~~

* Favor winning moves that minimize DTZ to reduce shuffling by assuming
  repeated position by default (#708)
* Print cuda and gpu info, warn if mismatches are noticed (#711)

v0.20.2-rc1 (2019-01-27)
~~~~~~~~~~~

* no terminal multivisits (#683)
* better fix for issue 651 (#693)
* Changed output of --help flag to stdout rather than stderr (#687)
* Movegen speedup via magic bitboards (#640)
* modify default benchmark setting to run for 10 seconds (#681)
* Fix incorrect index in OpenCL Winograd output transform (#676)
* Update OpenCL (#655)

v0.20.1 (2019-01-07)
~~~~~~~~~~~

* Change to atomic for cache capacity. (#665)

v0.20.1-rc3 (2019-01-07)
~~~~~~~~~~~

* Remove ffast-math from the default flags (#661)

v0.20.1-rc2 (2019-01-05)
~~~~~~~~~~~

* Don't use Winograd for 1x1 conv. (#659)
* Fix issues with pondering and search limits. (#658)
* Check for zero capacity in cache (#648)
* fix undefined behavior in DiscoverWeightsFile() (#650)
* fix fastmath.h undefined behavior and clean it up (#643)

v0.20.1-rc1 (2019-01-01)
~~~~~~~~~~~

* Simplify movestogo approximator to use median residual time. (#634)
* Replace time curve logic with movestogo approximator. (#271)
* Cache best edge to improve PickNodeToExtend performance. (#619)
* fix building with tensorflow 1.12 (#626)
* Minor changes to `src/chess` (#606)
* make uci search parameters the defaults ones (#609)
* Preallocate nodes in advance of their need to avoid the allocation being
  behind a mutex. (#613)
* imrpove meson error when no backends enabled (#614)
* allow building with the mklml library as an mkl alternative (#612)
* Only build the history up if we are actually going to extend the position.
  (#607)
* fix warning (#604)

v0.20.0 (2019-01-01)
~~~~~~~~~~~

* no lto builds by default (#625)

v0.20.0-rc2 (2018-12-24)
~~~~~~~~~~~

* Fix for demux backend to match cuda expected threading model for 
  computations. (#605)

v0.20.0-rc1 (2018-12-22)
~~~~~~~~~~~

* Squeeze-and-Excitation Networks are now supported! (lc0.org/se)
* Older text network files are no longer supported.
* Various performance fixes (most major being having fast approximate math
  functions).
* For systems with multiple GPUs, in addition to "multiplexing" backend
  we now also have "demux" backend and "roundrobin" backend.
* Compiler settings tweaks (use VS2017 for windows builds, always have LTO
  enabled, windows releases have PGO enabled).
* Benchmark mode has more options now (e.g. movetime) and saner defaults.
* Added an option to prevent engine to resign too early (used in training).
* Fixed a bug when number of visits could be too high in collision nodes.
  The fix is pretty hacky, there will be better fix later.
* 32-bit version compiles again.

v0.19.1 (2018-12-10)
~~~~~~~

(no changes relative to v0.19.1-rc2)

v0.19.1-rc2 (2018-12-07)
~~~~~~~~~~~

* Temperature and FPU related params. (#568)
* Rework Cpuct related params. (#567)

v0.19.1-rc1 (2018-12-06)
~~~~~~~~~~~

* Updated cpuct formula from alphazero paper. (#563)
* remove UpdateFromUciOptions() from EnsureReady() (#558)
* revert IsSearchActive() and better fix for one of #500 crashes (#555)

v0.19.0 (2018-11-19)
~~~~~~~

* remove Wait() from EngineController::Stop() (#522)

v0.19.0-rc5 (2018-11-17)
~~~~~~~~~~~

* OpenCL: replace thread_local with a resource pool. (#516)
* optional wtime and btime (#515)
* Make convolve1 work with workgroup size of 128 (#514)
* adjust average depth calculation for multivisits (#510)

v0.19.0-rc4 (2018-11-12)
~~~~~~~~~~~

* Microseconds have 6 digits, not 3! (#505)
* use bestmove_is_sent_ for Search::IsSearchActive() (#502)

v0.19.0-rc3 (2018-11-07)
~~~~~~~~~~~

* Fix OpenCL tuner always loading the first saved tuning (#491)
* Do not show warning when ComputeBlocking() takes too much time. (#494)
* Output microseconds in log rather than milliseconds. (#495)
* Add benchmark features (#483)
* Fix EncodePositionForNN test failure (#490)

v0.19.0-rc2 (2018-11-03)
~~~~~~~~~~~

* Version v0.19.0-rc1 reported it's version as v0.19.0-dev
  Therefore v0.19.0-rc2 is released with this issue fixed.

v0.19.0-rc1 (2018-11-03)
~~~~~~~~~~~

* Search algorithm changes

  When visiting terminal nodes and collisions, instead of counting that as one
  visit, estimate how many subsequent visits will also go to the same node, and
  do a batch update.

  That should slightly improve nps near terminal nodes and in multithread
  configurations. Command line parameters that control that:

  --max-collision-events – number of collision events allowed per batch.
    Default is 32. This parameter is roughly equivalent to
    --allowed-node-collisions in v0.18.
  
  --max-collision-visits – total number of estimated collisions per NN batch.
    Default is 9999.

* Time management

  Multiple changes have been done to make Leela track used time more precisely
  (particularly, the moment when to start timer is now much closer to the moment
  GUIs start timer).

  For smart pruning, Leela's timer only starts when the first batch comes from
  NN eval. That should help against instamoves, especially on non-even GPUs.

  Also Leela stops the search quicker now when it sees that time is up (it could
  continue the search for hundreds of milliseconds after that, which caused time
  trouble if opponent moves very fast).

  Those changes should help a lot in ultra-bullet configurations.

* Better logging

  Much more information is outputted now to the log file. That will allow us to
  easier diagnose problems if they occur. To have debug file written, add a
  command line option:

  --logfile=/path/to/logfile

  (or short option "-l /path/to/logfile", or corresponding UCI option "LogFile")

  It's recommended to always have logging on, to make it easier to report bugs
  when it happens.

* Configuration parameters change

  Large part of parameter handling has been reworked. As the result:

  All UCI parameters have been changed to have more "classical" look.
    E.g. was "Network weights file path", became "WeightsFile".

  Much more detailed help is shown than before when you run
    ./lc0 --help

  Some flags have been renamed, e.g.
    --futile-move-aversion
    is renamed back to
    --smart-pruning-factor.

  After setting a parameter (using command line parameter or uci setoption
    command), uci command "uci" shows updated result. That way you can check the
    current option values.

  Some command-line and UCI options are hidden now. Use --show-hidden command
    line parameter to unhide them. E.g.
    ./lc0 --show-hidden --help

  Also, in selfplay mode the per player configuration format has been changed
  (although probably noone knew that anyway):
    Was: ./lc0 selfplay player1: --movetime=14
    Became: ./lc0 selfplay --player1.movetime=14

* Other

  "go depth X" uci command now causes search to stop when depth information in
  uci info line reaches X. Not that it makes much sense for it to work this way,
  but at least it's better than noting.

  Network file size can now be larger than 64MB.

  There is now an experimental flag --ramlimit-mb. The engine tries to estimate
  how much memory it uses and stops search when tree size (plus cache size)
  reaches RAM limit. The estimation is very rough. We'll see how it performs and
  improve estimation later.  
  In situations when search cannot be stopped (`go infinite` or ponder),
  `bestmove` is not automatically outputted. Instead, search stops progress and
  outputs warning.
  
  Benchmark mode has been implemented. Run run, use the following command line:
    ./lc0 benchmark
  This feature is pretty basic in the current version, but will be expanded later.

  As Leela plays much weaker in positions without history, it now is able to
  synthesize it and do not blunder in custom FEN positions. There is a
  --history-fill flag for it. Setting it to "no" disables the feature, setting
  to "fen_only" (default) enables it for all positions except chess start
  position, and setting it to "always" enables it even for startpos.

  Instead of output current win estimation as centipawn score approximation,
  Leela can how show it's raw score. A flag that controls that is --score-type.
  Possible values:
    - centipawn (default) – approximate the win rate in centipawns, like Leela
      always did.
    - win_percentage – value from 0 to 100.0 which represents expected score in
      percents.
    - Q – the same, but scales from -100.0 to 100.0 rather than from 0 to 100.0

v0.18.1 (2018-10-02)
~~~~~~~

* Fix for falling into threefold repetition in a winning endgame tablebase position.


v0.18.0 (2018-09-30)
~~~~~~~

* No changes from rc2 except the version.


v0.18.0-rc2 (2018-09-26)
~~~~~~~~~~~

* Severe bug fixed: Race condition when out-of-order-eval was enabled (and it
  was enabled by default)

* Windows 32-bit builds are now possible (CPU only for now)


v0.18.0-rc1 (2018-09-24)
~~~~~~~~~~~

KNOWN BUG!

* We have credible reports that in some rare cases Lc0 crashes!
  However, we were not able to reproduce it reliably. If you see the crash,
  please report to devs! What seems to increase crash probability:
  - Very short move time (milliseconds)
  - Proximity to a checkmate (happens 1-3 moves before the checkmate)


New features:

* Endgame tablebases support! Both WDL and DTZ now.

* Added MultiPv support.


Time management changes:

* Introduced --immediate-time-use flag. Yes, yet another time management
  flag. Posible values are between 0.0 and 1.0. Setting it closer to 
  1.0 makes Leela use time saved from futile search aversion earlier.

* Some time management parameters were changed:
  - Slowmover is 1.0 now (was 2.4)
  - Immediate-time-use is 0.6 now (didn't exist before, so was 0.0)

* Fixed a bug, because of which futile search aversion tolerance was incorrectly
  applied, which resulted in instamoves.

* Now search stops immediately when it runs out of budgeted time.
  Should help against timeouts, especially on slow backends (e.g. BLAS).

* Move overhead now is a fixed time, doesn't depend on number of remaining
  moves.


Other:

* Out of order eval is on by default. That brings slight nps improvement.

* Default FPU reduction is 1.2 now (was 0.9)

* Cudnn backend now has max_batch parameter.
  (can be set for example like this --backend-opts=max_batch=100).
  This is needed for lower end GPUs that didn't have enough VRAM for a buffer
  of size 1024. Make sure that this setting is not lower than --minibatch-size.

* Small memory usage optimizations.

* Engine name in UCI response is shorter now. Fritz chess UI should be able
  to work with Leela now

* Added flag --temp-visit-offset, will allow to offset temperature during
  training.

* Command line and UCI parameter values are now checked for validity.

* You can now build for older processors that don't support the popcnt
  instruction by passing -Dpopcnt=false to meson when building.

* 32-bit build is possible now. CPU only and we were only able to build it 
  in Linux for now, including Raspberry Pi.

* Threading issue which caused crash in heavily multithreaded environment
  with slow backends was fixed.


v0.17.0 (2018-08-27)
~~~~~~~

No changes from rc2 except the version.


v0.17.0-rc2 (2018-08-21)
~~~~~~~~~~~

* Fixed a bug, that rule50 value was located in wrong place in a training data.
* OpenCL uses much less VRAM now.
* Default OpenCL batch size is 16 now (was 1).
* Default time management related configuration was tweaked:
  --futile-move-aversion is 1.33 now (was 1.47)
  --slowmover is 2.4 now (was 2.6)


v0.17.0-rc1 (2018-08-19)
~~~~~~~~~~~

New visible features:
* Implemented ponder support.
* Tablebases are supported now (only WDL probe for now).
  Command line parameter is
  --syzygy-paths=/path/to/syzygy/
* Old smart pruning flag is gone. Instead there is
  --futile-search-aversion flag.
  --futile-search-aversion=0 is equivalent to old --no-smart-pruning.
  --futile-search-aversion=1 is equivalent to old --smart-pruning.
  Now default is 1.47, which means that engine will sometimes decide to
  stop search earlier even when there is theoretical chance (but not very
  probable) that best move decision could be changed if allowed to think more.
* Lc0 now supports configuration files. Options can be listed there instead of
  command line flags / uci params.
  Config should be named lc0.config and located in the same directory as lc0.
  Should list one command line option per line, with '--' in the beginning
  being optional, for example:

     syzygy-paths=/path/to/syzygy/

* In uci info, "depth" is now average depth rather than full depth
  (which was 4 all the time).
  Also, depth values do not include reused tree, only nodes visited during the
  current search session.
* --sticky-checkmates experimental flag (default off), supposed to find shorter
  checkmate sequences.
* More features in backend "check".


Performance optimizations:
* Release windows executables are built with "whole program optimization".
* Added --out-of-order-eval flag (default is off).
  Switching it on makes cached/terminal nodes higher priority, which increases
  nps.
* OpenCL backend now supports batches (up to 5x speedup!)
* Performance optimizations for BLAS backend.
* Total visited policy (for FPU reduction) is now cached.
* Values of priors (P) are stored now as 16-bit float rather than 32-bit float,
  that saves considerable amount of RAM.


Bugfixes:
* Fixed en passant detection bug which caused the position after pawn moving by
  two squares not counted towards threefold repetition even if en passant was
  not possible.
* Fixed the bug which caused --cache-history-length for values 2..7 work the
  same as --cache-history-length=1.
  This is fixed, but default is temporarily changed to --cache-history-length=1
  during play. (For training games, it's 7)


Removed features:
* Backpropagation beta / backpropagation gamma parameters have been removed.


Other changes:
* Release lc0-windows-cuda.zip package now contains NVdia CUDA and cuDNN .dlls.


v0.16.0 (2018-07-20)
~~~~~~~

* Fully switched to official releases! No more https://crem.xyz/lc0/
* Fixed a bug when pv display and smart pruning didn't sometimes work properly
  after tree reuse.
* Format of protobuf network files was changed.
* Autodiscovery of protobuf based network files works now.


lc0-win-20180715-cuda92-cudnn714
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Support of new format of network files (needed for lc0 launch on main
  training server)
* Fixed hang/poor performance in the beginning of search when there are many
  threads. (Happened on linux only though).
* Memory footprint is reduced a bit. (~-60 bytes per node)

lc0-win-20180711-cuda92-cudnn714
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Edge-node separation introduced a bug that smart pruning didn't work. That's
  fixed.
* Changed options parsing so that --backend-opts=cudnn-fp16 is now possible.
* Performance fixes (mostly for slowness introduced by edge-node separation).

lc0-win-20180708-cuda92-cudnn714
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Mutex contention has been reduced (but locking mutex more rarely).
  Helps a lot with many threads running. Especially recommended to check with
  multi-GPU configuration.
* Memory usage reduced at least 2x (probably more).
* cudnn backend crashed on large batches (>800) that's fixed.
  There is still a limit of batch size 1024 though.
* (not in cudnn build, but for completeness)
  Fixed NN computation with BLAS backend, it had up to 5% error before that.
* Default time budgeting params have been changed again! (not by mach this time)
  --slowmover=1.95
  --time-curve-peak=26.2
  --time-curve-left-width=82
  --time-curve-right-width=74

lc0-win-20180701-cuda92-cudnn714
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* fp16-based computation for very modern NVidia GPUs!
  May reduce precision a bit, but should be compensated by nps boost.
  Enable with --backend=cudnn-fp16 flag
* V is now not stored in nodes (a bit less RAM used while thinking)
* (not in cudnn build, but listing for completeness) blas batching support.

lc0-win-20180629-cuda92-cudnn714
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Default time budgeting parameters have been changed (again!):
  --slowmover=1.93
  --time-curve-peak=26
  --time-curve-left-width=67
  --time-curve-right-width=76
* When generating training games, the engine could confuse client by sending
  corrupted output. That's fixed.

lc0-win-20180624-cuda92-cudnn714
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Default time budgeting parameters have been changed:
  --slowmover=2.13  (was 1.8)
  --time-curve-peak=22.0  (was 41.0)
  --time-curve-left-width=450.0  (was 1000.0)
  --time-curve-right-width=30.0  (was 39.5)
* During training game generation, the engine is able to send resign statistics.


lc0-win-20180622-cuda92-cudnn714
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Time budged allocation has been changed, it allocates more time to early
  stages of the game.
  Graphs are here: https://github.com/LeelaChessZero/lc0/pull/59
  Slowmover value has so be recalibrated, and default value was changed from 2.2 to 1.8.
* Fixed a race condition in cache prefetch code. Realistically it hardly every
  occured before though.

lc0-win-20180619-cuda92-cudnn714-00
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* Fix a bug instroduced in version 20180609 which caused the engine to miss checkmates sometimes.

lc0-win-20180614-cuda92-cudnn714
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* "go searchmoves" uci command is now supported
* It's possible now to disable tree reuse in training games
* Few improvements for random backend
* Lc0 now shows version in uci response
* Analyzer mode has been removed
* extra-virtual-loss has been removed
* Implemented resign (for training games)

lc0-win-20180609-cuda92-cudnn714-01
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

* In addition to --backpropagate-gamma, there is also --backpropagate-beta!
  Default is 1.0.

lc0-win-20180609-cuda92-cudnn714-00
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Visible changes:
* Experimental changes from 20180604 are now default.
* Memory footprint is reduced by 8 bytes per visible node (+ ~240 bytes in
  invisible nodes per visible)
* Introduced --backpropagate-gamma flag.
  Default is 1.0. There are rumours that reducing it to 0.75 improves play.
* Extra-virtual-loss parameter has been removed.
* Quotes in backend-opts parameter were not parsed properly, that's fixed.


lc0-win-20180604-cuda92-cudnn714-experimental
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Visible changes:
* Experimental default settings:
  cPUCT: 3.4
  FPU reduction: 0.9
  policy Softmax: 2.2

* Fix memory leak when GUI doesn't ever issue `isready` uci command.


lc0-win-20180602-cuda92-cudnn714-00
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Visible changes:
* cPUCT is now 3.1 by default instead of 1.2 (or what it was before)
* Fixed Batch normalization epsilon in tensorflow backend (but noone uses tensorflow anyway)
* Periodically (every 5 seconds) output "uci info" even if bestmove/depth doesn't change.
* Memory management is redone so that node release happens after "bestmove" and "isready", rather than after "position" uci command.
  That garbage collection could take tens of milliseconds and chess GUI already started timer at that point.
  Memory management is always fragile, so fresh crashes and memory leaks are possible.

Invisible changes:
* Store castlings again as e1g1 and not e1h1. Fixes a bug that tree was not reused after castling.

```

`cross-files/aarch64-linux-android`:

```

# Tested with Android NDK r19c, default toolchain
# Targeting API level 21

# Set the toolchain path on your environment
# export PATH="$HOME/.local/share/android-sdk/ndk-bundle/toolchains/llvm/prebuilt/linux-x86_64/bin:$PATH"

[host_machine]
system = 'android'
cpu_family = 'arm'
cpu = 'aarch64'
endian = 'little'

[properties]
cpp_link_args = ['-llog', '-static-libstdc++']

[binaries]
c = 'aarch64-linux-android21-clang'
cpp = 'aarch64-linux-android21-clang++'
ar = 'aarch64-linux-android-ar'
strip = 'aarch64-linux-android-strip'
ld = 'aarch64-linux-android-ld'
ranlib = 'aarch64-linux-android-ranlib'
as = 'aarch64-linux-android-as'

```

`cross-files/armv7a-linux-android`:

```

# Tested with Android NDK r19c, default toolchain
# Targeting API level 21

# When targeting API levels < 24 the build fails unless _FILE_OFFSET_BITS is unset.
# Meson passes _FILE_OFFSET_BITS=64 but recent NDK toolchains have issues building
# for 32-bit ABIs when such macro it set. Relevant links:
#  https://android.googlesource.com/platform/bionic/+/master/docs/32-bit-abi.md
#  https://github.com/mesonbuild/meson/pull/2996#issuecomment-384045808

# Set the toolchain path on your environment
# export PATH="$HOME/.local/share/android-sdk/ndk-bundle/toolchains/llvm/prebuilt/linux-x86_64/bin:$PATH"

[host_machine]
system = 'android'
cpu_family = 'arm'
cpu = 'armv7a'
endian = 'little'

[properties]
cpp_args = ['-U_FILE_OFFSET_BITS']
cpp_link_args = ['-llog', '-static-libstdc++']

[binaries]
c = 'armv7a-linux-androideabi21-clang'
cpp = 'armv7a-linux-androideabi21-clang++'
ar = 'arm-linux-androideabi-ar'
strip = 'arm-linux-androideabi-strip'
ld = 'arm-linux-androideabi-ld'
ranlib = 'arm-linux-androideabi-ranlib'
as = 'arm-linux-androideabi-as'

```

`dist/README-cuda.txt`:

```txt
Lc0

Lc0 is a UCI-compliant chess engine designed to play chess via
neural network, specifically those of the LeelaChessZero project
(https://lczero.org).

This binary uses CUDA and cuDNN dynamic link libraries copyrighted
by Nvidia corporation (http://www.nvidia.com), and redistributed as
permitted by the respective license file (see CUDA.txt section 2.2
and CUDNN.txt section "CUDNN DISTRIBUTION" for details). You are
authorized to redistribute these libraries together with this
package as a whole but not individually.


License

Leela Chess is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Leela Chess is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

Additional permission under GNU GPL version 3 section 7

If you modify this Program, or any covered work, by linking or
combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
modified version of those libraries), containing parts covered by the
terms of the respective license agreement, the licensors of this
Program grant you additional permission to convey the resulting work.


```

`install_openSUSE_lc0.sh`:

```sh
#!/bin/bash


# This script can be run from any location, but must be run in a root console.

# Start OS detection and setting variables
ID=$(grep -w ID= /etc/os-release | sed -e 's/ID=//')
VERSION_ID=$(grep -w VERSION_ID= /etc/os-release | sed -e 's/VERSION_ID=//' | sed -e 's/"//g')

#If Tumbleweed is detected, the Tumbleweed install
if [ "$ID" = '"opensuse-tumbleweed"' ]; then

# Update system, always a good idea before a major install
zypper dup -y

echo "No repositories will be added"
# Tumbleweed repositories are currently disabled because of a Tumbleweed bug, but may not be significant due to the bleeding edge and rolling release nature of Tumbleweed. Currently, lc0 builds fine without these repos	
# zypper -n ar -f https://download.opensuse.org/repositories/devel:/gcc/openSUSE_Factory/ Tumbleweed:devel:gcc 
# zypper -n ar -f	https://download.opensuse.org/repositories/devel:/languages:/python:/Factory/openSUSE_Tumbleweed/ Tumbleweed:devel:languages:python
# zypper -n ar -f	http://download.opensuse.org/repositories/science/openSUSE_Tumbleweed/ openSUSE_Tumbleweed:science 

#Enable to access Tensorflow packages
# zypper -n ar -f https://download.opensuse.org/repositories/science:/machinelearning/openSUSE_Leap_$VERSION_ID/ Tumbleweed::science:machinelearning


else
#If LEAP is detected, then the LEAP install for that specific version of LEAP
if [ "$ID" = '"opensuse-leap"' ]; then

# Update system, always a good idea before a major install
zypper up -y

zypper -n ar -f https://download.opensuse.org/repositories/devel:/gcc/openSUSE_Leap_"$VERSION_ID"/ LEAP:devel:gcc 
zypper -n ar -f	https://download.opensuse.org/repositories/devel:/languages:/python:/backports/openSUSE_Leap_"$VERSION_ID"/ LEAP:devel:languages:python:backports
zypper -n ar -f	http://download.opensuse.org/repositories/science/openSUSE_Leap_"$VERSION_ID"/ LEAP:science 

#Enable to access Tensorflow packages
#zypper -n ar -f https://download.opensuse.org/repositories/science:/machinelearning/openSUSE_Leap_$VERSION_ID/ Tumbleweed::science:machinelearning

fi
# Package repositories are activated
zypper --gpg-auto-import-keys ref
fi


# install dependencies
# If any build dependencies aren't met, enable the following which installs the complete C/C++ Development environment
# zypper in -y -t pattern devel_C_C++ 

# The following dependencies are sometimes found, sometimes not found by the lc0 meson build, so are not installed by default
# When the following are not found already on the system, the lc0 build towards the end of this script will do its own install
# zypper in -y python3-protobuf libprotoc17 libprotobuf17 

zypper in -y --allow-vendor-change --allow-downgrade git gcc-c++ gcc7-c++ meson ninja python3-abseil openblas_pthreads-devel-static libz1



# Clone lc0 github repo
# The following clones the development branch.
# If you want to instead clone a stable release branch, visit the following URL in a web browser, select and copy the release you want, replacing the URL that follows "git clone" 5 lines below, but leaving the "/opt/lc0" at the end of the line untouched(In other words, everything between "https" and "git" inclusively)
# https://github.com/LeelaChessZero/lc0

# Note following installs into /opt. Modify location as desired
mkdir /opt/lc0
git clone https://github.com/LeelaChessZero/lc0.git /opt/lc0
cd /opt/lc0/ || exit


# Execute
./build.sh

# Final Message
echo "Completed. If you see no errors above, you can attach your chessboard to the lc0 binary at /opt/lc0/build/release/lc0"
echo "Or, follow the instructions for your specific chessboard frontend at"
echo "https://github.com/LeelaChessZero/lc0/wiki/Running-Leela-Chess-Zero-in-a-Chess-GUI"



```

`meson.build`:

```build
# This file is part of Leela Chess Zero.
# Copyright (C) 2018-2022 The LCZero Authors
#
# Leela Chess is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Leela Chess is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

project('lc0', 'cpp',
        default_options : ['cpp_std=c++17', 'b_ndebug=if-release', 'warning_level=3', 'b_lto=true', 'b_vscrt=mt'],
        meson_version: '>=0.52')

cc = meson.get_compiler('cpp')

if not cc.has_header('optional') or not cc.has_header('string_view')
    error('Lc0 requires a compiler supporting C++17, for example g++ v8.0, ' +
          'clang v5.0 or later (with C++17 stdlib) and Visual Studio 2017 or ' +
          'later.')
endif

if not cc.has_header('charconv')
    warning('Your compiler or library does not have full C++17 support. ' +
            'See the README for compilers that are known to be working. ' +
            'This will become an error in the future.')
endif

if cc.get_id() == 'clang'
  # Thread safety annotation
  add_project_arguments('-Wthread-safety', language : 'cpp')
endif
if cc.get_id() == 'clang' or cc.get_id() == 'gcc'
  if get_option('buildtype') == 'release'
    add_project_arguments(cc.get_supported_arguments(['-march=native']), language : 'cpp')
  endif
endif
if cc.get_id() == 'msvc'
  # Silence some zlib warnings.
  add_global_arguments('/wd4131', '/wd4267', '/wd4127', '/wd4244', '/wd4245', language : 'c')
endif
if host_machine.system() == 'windows'
  add_project_arguments('-DNOMINMAX', language : 'cpp')
endif
if host_machine.cpu_family() == 'arm'
  if get_option('neon')
    add_project_arguments(cc.get_supported_arguments(['-mfpu=neon']), language : 'cpp')
    add_project_link_arguments(cc.get_supported_arguments(['-mfpu=neon']), language : 'cpp')
  endif
endif

# Files to compile.
deps = []
files = []
includes = []
has_backends = false

# Third party files.
includes += include_directories('third_party', is_system: true)

# Compiling protobufs.
compile_proto = find_program('scripts/compile_proto.py')
gen = generator(compile_proto, output: ['@BASENAME@.pb.h'],
  arguments : [
    '--proto_path=@CURRENT_SOURCE_DIR@/libs/lczero-common',
    '--cpp_out=@BUILD_DIR@',
    '@INPUT@'])

# Handle submodules.
git = find_program('git', required: false)
if run_command('scripts/checkdir.py', 'libs/lczero-common/proto').returncode() != 0
  if git.found()
    if run_command(git, 'status').returncode() == 0
      message('updating git submodule libs/lczero-common')
      run_command(git, 'submodule', 'update', '--init', '--recursive')
    else
      message('cloning lczero-common.git into libs/lczero-common')
      run_command(git, 'clone', '--depth=1',
                  'https://github.com/LeelaChessZero/lczero-common.git',
                  'libs/lczero-common/')
    endif
  else
    error('Please install git to automatically fetch submodules or download the archives manually from GitHub.')
  endif
endif

pb_files = [
  'src/utils/protomessage.cc',
  gen.process('libs/lczero-common/proto/net.proto',
    preserve_path_from : meson.current_source_dir() + '/libs/lczero-common/')
]
files += pb_files

# Extract git short revision.
short_rev = 'unknown'
if git.found()
  r = run_command(git, 'rev-parse', '--short', 'HEAD')
  if r.returncode() == 0
    # Now let's check if the working directory is clean.
    if run_command(git, 'diff-index', '--quiet', 'HEAD').returncode() == 0
      short_rev = r.stdout().strip()
      if run_command(git, 'describe', '--exact-match', '--tags').returncode() == 0
        short_rev = ''
      endif
    else
      short_rev = 'dirty'
      warning('Cannot extract valid git short revision from dirty working directory.')
    endif
  else
    warning('Failed to parse short revision. Use git clone instead of downloading the archive from GitHub.')
  endif
endif

# Construct build identifier.
build_identifier = ''
if short_rev != ''
  build_identifier = 'git.' + short_rev
  message('Using build identifier "' + build_identifier + '".')
endif

conf_data = configuration_data()
conf_data.set_quoted('BUILD_IDENTIFIER', build_identifier)
configure_file(output: 'build_id.h', configuration: conf_data)

# Some malloc libraries require to be linked first.
if get_option('malloc') == 'mimalloc' and cc.get_id() == 'msvc'
  if get_option('b_vscrt') != 'md' and get_option('b_vscrt') != 'mdd'
    error('You need -Db_vscrt=md (or mdd)')
  endif
  add_project_link_arguments('/INCLUDE:mi_version', language : 'cpp')
  deps += cc.find_library('mimalloc-override', dirs: get_option('mimalloc_libdir'), required: true)
elif get_option('malloc') != ''
  deps += cc.find_library(get_option('malloc'), required: true)
endif

# ONNX protobufs.
gen_proto_src = generator(compile_proto, output: ['@BASENAME@.pb.h'],
  arguments : [
    '--proto_path=@CURRENT_SOURCE_DIR@/src',
    '--cpp_out=@BUILD_DIR@',
    '@INPUT@'])

files += gen_proto_src.process('src/neural/onnx/onnx.proto',
  preserve_path_from : meson.current_source_dir() + '/src/')

#############################################################################
## Main files
#############################################################################
files += [
  'src/benchmark/backendbench.cc',
  'src/benchmark/benchmark.cc',
  'src/chess/bitboard.cc',
  'src/chess/board.cc',
  'src/chess/position.cc',
  'src/chess/uciloop.cc',
  'src/engine.cc',
  'src/lc0ctl/describenet.cc',
  'src/lc0ctl/leela2onnx.cc',
  'src/lc0ctl/onnx2leela.cc',  
  'src/mcts/node.cc',
  'src/mcts/params.cc',
  'src/mcts/search.cc',
  'src/mcts/stoppers/alphazero.cc',
  'src/mcts/stoppers/common.cc',
  'src/mcts/stoppers/factory.cc',
  'src/mcts/stoppers/legacy.cc',
  'src/mcts/stoppers/smooth.cc',
  'src/mcts/stoppers/stoppers.cc',
  'src/mcts/stoppers/timemgr.cc',
  'src/neural/cache.cc',
  'src/neural/decoder.cc',
  'src/neural/encoder.cc',
  'src/neural/factory.cc',
  'src/neural/loader.cc',
  'src/neural/network_check.cc',
  'src/neural/network_demux.cc',
  'src/neural/network_legacy.cc',
  'src/neural/network_mux.cc',
  'src/neural/network_random.cc',
  'src/neural/network_record.cc',
  'src/neural/network_rr.cc',
  'src/neural/network_trivial.cc',
  'src/neural/onnx/adapters.cc',
  'src/neural/onnx/builder.cc',
  'src/neural/onnx/converter.cc',
  'src/selfplay/game.cc',
  'src/selfplay/loop.cc',
  'src/selfplay/tournament.cc',
  'src/syzygy/syzygy.cc',
  'src/trainingdata/reader.cc',
  'src/trainingdata/trainingdata.cc',
  'src/trainingdata/writer.cc',
  'src/utils/commandline.cc',
  'src/utils/configfile.cc',
  'src/utils/esc_codes.cc',
  'src/utils/files.cc',
  'src/utils/histogram.cc',
  'src/utils/logging.cc',
  'src/utils/numa.cc',
  'src/utils/optionsdict.cc',
  'src/utils/optionsparser.cc',
  'src/utils/random.cc',
  'src/utils/string.cc',
  'src/utils/weights_adapter.cc',
  'src/utils/fp16_utils.cc',
  'src/version.cc',
]
includes += include_directories('src')

deps += dependency('threads')

#############################################################################
## Platform specific files
############################################################################
if host_machine.system() == 'windows'
  files += 'src/utils/filesystem.win32.cc'
else
  files += 'src/utils/filesystem.posix.cc'
endif

#############################################################################
## BACKENDS
#############################################################################

if get_option('build_backends')
  ## ~~~~~~~~~~
  ## Tensorflow
  ## ~~~~~~~~~~
  tf_dl_lib = cc.find_library('dl', required: false)
  # We had `is_system: true` to reduce warnings, but meson > 0.56.0 breaks.
  tf_tensorflow_cc_lib = dependency('tensorflow_cc', required: false)
  if get_option('tensorflow') and tf_dl_lib.found() and tf_tensorflow_cc_lib.found()
    deps += [tf_dl_lib, tf_tensorflow_cc_lib]
    files += 'src/neural/network_tf_cc.cc'
    has_backends = true
  endif

  ## ~~~~~
  ## Blas
  ## ~~~~~

  shared_files = []

  accelerate_lib = dependency('Accelerate', required: false)

  mkl_libdirs = get_option('mkl_libdirs')
  mkl_lib = cc.find_library('mkl_rt', dirs: mkl_libdirs, required: false)
  if not mkl_lib.found()
    mkl_lib = cc.find_library('mklml', dirs: mkl_libdirs, required: false)
  endif

  dnnl_libdirs = [get_option('dnnl_dir') + '/lib64', get_option('dnnl_dir') + '/lib']
  dnnl_lib = cc.find_library('dnnl', dirs: dnnl_libdirs, required: false)

  openblas_libdirs = get_option('openblas_libdirs')
  openblas_lib = cc.find_library('openblas.dll', dirs: openblas_libdirs, required: false)
  if not openblas_lib.found()
    openblas_lib = cc.find_library('openblas', dirs: openblas_libdirs, required: false)
  endif

  if get_option('blas')
    if get_option('mkl') and mkl_lib.found()
      add_project_arguments(['-DUSE_MKL', '-DUSE_BLAS'], language : 'cpp')
      mkl_inc = get_option('mkl_include')
      if run_command('scripts/checkdir.py', mkl_inc).returncode() == 0
        includes += include_directories(mkl_inc)
      endif
      deps += [ mkl_lib ]

    elif get_option('dnnl') and dnnl_lib.found()
      add_project_arguments(['-DUSE_DNNL', '-DUSE_BLAS'], language : 'cpp')
      includes += include_directories(get_option('dnnl_dir') + '/include')
      deps += [ dnnl_lib, dependency('openmp', required:true) ]

    elif get_option('accelerate') and accelerate_lib.found()
      deps += [ accelerate_lib ]
      add_project_arguments('-DUSE_BLAS', language : 'cpp')

    elif get_option('openblas') and openblas_lib.found()
      add_project_arguments(['-DUSE_OPENBLAS', '-DUSE_BLAS'], language : 'cpp')

      required_openblas_header = 'openblas_config.h'
      if not cc.has_header(required_openblas_header)
        openblas_headers_found = false

        # add the first valid include directory
        foreach d : get_option('openblas_include')
          if not openblas_headers_found and cc.has_header(required_openblas_header, args: '-I' + d)
            includes += include_directories(d)
            openblas_headers_found = true
          endif
        endforeach

        if not openblas_headers_found
          error('Failed to detect OpenBLAS headers. Did you install libopenblas-dev?')
        endif
      endif

      deps += [ openblas_lib ]

    endif

    deps += dependency('eigen3', fallback: ['eigen', 'eigen_dep']).as_system()

    ispc = find_program('ispc', required: false)
    ispc_arch = 'x86-64'
    ispc_extra_args = []
    if get_option('ispc') and ispc.found()
      ispc_native_only = get_option('ispc_native_only')
      if host_machine.system() == 'windows'
        outputnames = [ '@BASENAME@.obj']
        if not ispc_native_only
          outputnames += ['@BASENAME@_sse2.obj', '@BASENAME@_sse4.obj',
                          '@BASENAME@_avx.obj', '@BASENAME@_avx2.obj',
                          '@BASENAME@_avx512knl.obj', '@BASENAME@_avx512skx.obj' ]
        endif
      else
        ispc_extra_args += ['--pic']
        outputnames = [ '@BASENAME@.o']
        if not ispc_native_only
          outputnames += ['@BASENAME@_sse2.o', '@BASENAME@_sse4.o',
                          '@BASENAME@_avx.o', '@BASENAME@_avx2.o',
                          '@BASENAME@_avx512knl.o', '@BASENAME@_avx512skx.o' ]
        endif
      endif
      if ispc_native_only
        ispc_target = 'host'
      else
        ispc_target = 'sse2-i32x8,sse4-i32x8,avx1-i32x8,avx2-i32x8,avx512knl-i32x16,avx512skx-i32x16'
      endif

      if host_machine.system() == 'android'
        ispc_extra_args += ['--target-os=android']
        if host_machine.cpu_family() == 'arm'
          outputnames = [ '@BASENAME@.o']
          if host_machine.cpu() == 'aarch64'
            ispc_target = 'neon-i32x8'
            ispc_arch = 'aarch64'
          else
            ispc_target = 'neon-i32x4'
            ispc_arch = 'arm'
          endif
        endif
      endif

      iscp_gen = generator(ispc,
        output: [ '@BASENAME@_ispc.h', outputnames ],
        arguments: [ '-O2', '--wno-perf', '--arch=' + ispc_arch,
                     '--target=' + ispc_target,
                     '@INPUT@', '-o', '@OUTPUT1@' ,'-h', '@OUTPUT0@' ]
                     + ispc_extra_args
      )
    endif

    blas_files = [
    'src/neural/blas/convolution1.cc',
    'src/neural/blas/fully_connected_layer.cc',
    'src/neural/blas/se_unit.cc',
    'src/neural/blas/network_blas.cc',
    'src/neural/blas/winograd_convolution3.cc'
    ]

    shared_files = [
    'src/neural/shared/activation.cc',
    'src/neural/shared/winograd_filter.cc',
    ]

    files += blas_files
    has_backends = true

    if get_option('ispc') and ispc.found()
      files += iscp_gen.process('src/neural/blas/winograd_transform.ispc')
      files += iscp_gen.process('src/neural/shared/activation.ispc')
      add_project_arguments('-DUSE_ISPC', language : 'cpp')
    endif

  endif


  ## ~~~~~
  ## OpenCL
  ## ~~~~~

  has_opencl = false

  opencl_libdirs = get_option('opencl_libdirs')
  opencl_lib=cc.find_library('OpenCL', dirs: opencl_libdirs, required: false)

  opencl_framework=dependency('OpenCL', method: 'extraframework', required: false)
  if opencl_framework.found()
      opencl_dep = [ opencl_framework ]
      has_opencl = true

  elif opencl_lib.found() and cc.has_header('CL/opencl.h', args: '-I' + get_option('opencl_include'))
      opencl_dep = [ opencl_lib ]
      has_opencl = true

  endif

  if get_option('opencl') and has_opencl

    opencl_files = [
      'src/neural/opencl/network_opencl.cc',
      'src/neural/opencl/OpenCL.cc',
      'src/neural/opencl/OpenCLTuner.cc',
      'src/neural/opencl/OpenCLBuffers.cc',
    ]

    shared_files = [
    'src/neural/shared/activation.cc',
    'src/neural/shared/winograd_filter.cc',
    ]

    if not opencl_framework.found()
      includes += include_directories(get_option('opencl_include'))
    endif
    deps += opencl_dep
    files += opencl_files
    has_backends = true

  endif

  files += shared_files

  ## ~~~~~
  ## cuDNN
  ## ~~~~~
  cudnn_libdirs = get_option('cudnn_libdirs')
  cu_blas = cc.find_library('cublas', dirs: cudnn_libdirs, required: false)
  cu_dnn = cc.find_library('cudnn', dirs: cudnn_libdirs, required: false)
  cu_dart = cc.find_library('cudart', dirs: cudnn_libdirs, required: false)
  nvcc = find_program('nvcc', '/usr/local/cuda/bin/nvcc', '/opt/cuda/bin/nvcc',
                      required: false)

  if (get_option('cudnn') or get_option('plain_cuda')) and cu_blas.found() and cu_dart.found() and nvcc.found()
    deps += [cu_blas, cu_dart]
    cuda_files = ['src/neural/cuda/layers.cc']
    if get_option('cudnn') and cu_dnn.found()
      deps += cu_dnn
      cuda_files += 'src/neural/cuda/network_cudnn.cc'
      add_project_arguments('-DUSE_CUDNN', language : 'cpp')
    endif
    if get_option('plain_cuda')
      cuda_files += 'src/neural/cuda/network_cuda.cc'
    endif
    foreach d : get_option('cudnn_include')
      if run_command('scripts/checkdir.py', d).returncode() == 0
        includes += include_directories(d)
      endif
    endforeach
    includes += include_directories('src/neural/cuda/')

    cuda_arguments = ['-c', '@INPUT@', '-o', '@OUTPUT@',
                      '-I', meson.current_source_dir() + '/src']
    if host_machine.system() == 'windows'
      if get_option('b_vscrt') == 'mt'
        cuda_arguments += ['-Xcompiler', '-MT']
      elif get_option('b_vscrt') == 'mtd'
        cuda_arguments += ['-Xcompiler', '-MTd']
      elif get_option('b_vscrt') == 'mdd' or (get_option('b_vscrt') == 'from_buildtype' and get_option('buildtype') == 'debug')
        cuda_arguments += ['-Xcompiler', '-MDd']
      elif get_option('b_vscrt') != 'none'
        cuda_arguments += ['-Xcompiler', '-MD']
      endif
    else
      cuda_arguments += ['--std=c++14', '-Xcompiler', '-fPIC']
    endif
    if get_option('nvcc_ccbin') != ''
      cuda_arguments += ['-ccbin=' + get_option('nvcc_ccbin')]
    endif
    cuda_cc = get_option('cc_cuda') # Unfortunately option cuda_cc is reserved.
    nvcc_extra_args = []
    if cuda_cc != ''
      nvcc_extra_args = ['-arch=compute_' + cuda_cc, '-code=sm_' + cuda_cc]
    endif
    foreach x : get_option('cudnn_include')
      cuda_arguments += ['-I', x]
    endforeach
    if host_machine.system() == 'windows'
      outputname = '@BASENAME@.obj'
    else
      outputname = '@BASENAME@.o'
    endif
	 files += cuda_files
    files += custom_target('cuda fp32 code',
      input : 'src/neural/cuda/common_kernels.cu',
      output : outputname,
      depend_files: 'src/neural/cuda/winograd_helper.inc',
      command : [nvcc, nvcc_extra_args, cuda_arguments]
    )

    # Handling of fp16 cuda code.
    nvcc_arch = '-arch=compute_70'
    nvcc_sm_list = ['sm_80', 'sm_75', 'sm_86', 'sm_70']
    if host_machine.system() != 'windows'
      nvcc_arch = '-arch=compute_60'
      nvcc_sm_list += ['sm_60']
      if ['arm', 'aarch64'].contains(host_machine.cpu_family())
        # Add Jetson versions to the list.
        message('Jetson support enabled.')
        nvcc_arch = '-arch=compute_53'
        nvcc_sm_list += ['sm_72', 'sm_62', 'sm_53']
      endif
    endif
    # Ignore the given CC for fp16 when it is not in the supported list.
    if cuda_cc == '' or not nvcc_sm_list.contains('sm_' + cuda_cc)
      nvcc_extra_args = [nvcc_arch]
      nvcc_help = run_command(nvcc, '-h').stdout()
      foreach x : nvcc_sm_list
        if nvcc_help.contains(x)
          nvcc_extra_args += '-code=' + x
        endif
      endforeach
    endif
    files += custom_target('cuda fp16 code',
      input : 'src/neural/cuda/fp16_kernels.cu',
      output : outputname,
      depend_files: 'src/neural/cuda/winograd_helper.inc',
      command : [nvcc, nvcc_extra_args, cuda_arguments]
    )
    has_backends = true
  endif

  ## ~~~~~~~~
  ## DirectX
  ## ~~~~~~~~

  # we should always be able to build DirectX12 backend on windows platform
  if host_machine.system() == 'windows' and get_option('dx')
    dx_d3d12 = cc.find_library('d3d12')
    dx_dxgi = cc.find_library('dxgi')

    dx_files = [
      'src/neural/dx/network_dx.cc',
      'src/neural/dx/shader_wrapper.cc',
      'src/neural/dx/layers_dx.cc',
    ]
    files += dx_files
    deps += [dx_d3d12, dx_dxgi]

    subdir('src/neural/dx/shaders')

    has_backends = true
  endif

  if get_option('onednn') and dnnl_lib.found()
    includes += include_directories(get_option('dnnl_dir') + '/include')
    deps += [ dnnl_lib, dependency('openmp', required:true) ]
    files += [
      'src/neural/onednn/network_onednn.cc',
      'src/neural/onednn/layers.cc',
    ]
    has_backends = true
  endif

  ## ~~~~~~~~~~
  ## ONNX
  ## ~~~~~~~~~~
  if get_option('onnx_libdir') != '' and get_option('onnx_include') != ''
    onnx_lib = cc.find_library('onnxruntime',
                               dirs: get_option('onnx_libdir'),
                               required: true)
    includes += include_directories(get_option('onnx_include'), is_system: true)
    cc.has_header('onnxruntime_cxx_api.h',
                  required: true,
                  args: '-I' + get_option('onnx_include'))
    deps += [onnx_lib]

    files += [
      'src/neural/onnx/network_onnx.cc',
    ]

    has_backends = true
  endif

endif # if get_option('build_backends')

if not has_backends and get_option('build_backends')
  error('''

        No usable computation backends (cudnn/opencl/blas/etc) enabled.
        If you want to build with the random backend only, add
        -Dbuild_backends=false to the build command line.''')
endif


#############################################################################
## Dependencies
#############################################################################
  ## ~~~~
  ## zlib
  ## ~~~~
  # Pick latest from https://wrapdb.mesonbuild.com/zlib and put into
  # subprojects/zlib.wrap
  if host_machine.system() == 'windows'
    # In several cases where a zlib dependency was detected on windows, it
    # caused trouble (crashes or failed builds). Better safe than sorry.
    deps += subproject('zlib').get_variable('zlib_dep')
  else
    deps += dependency('zlib', fallback: ['zlib', 'zlib_dep'])
  endif

  ## ~~~~~~~~
  ## Profiler
  ## ~~~~~~~~
  if get_option('buildtype') != 'release'
    deps += cc.find_library('libprofiler',
      dirs: ['/usr/local/lib'], required: false)
  endif

  deps += cc.find_library('libatomic', required: false)

#############################################################################
## Main Executable
#############################################################################

if not get_option('popcnt')
  add_project_arguments('-DNO_POPCNT', language : 'cpp')
endif

if not get_option('pext')
  add_project_arguments('-DNO_PEXT', language : 'cpp')
endif

if get_option('embed')
  add_project_arguments('-DEMBED', language : 'cpp')
endif

executable('lc0', 'src/main.cc',
       files, include_directories: includes, dependencies: deps, install: true)

#############################################################################
## Tests
#############################################################################

if get_option('gtest')
  gtest = dependency('gtest', fallback: ['gtest', 'gtest_dep'])
  lc0_lib = library('lc0_lib', files, include_directories: includes, dependencies: deps)

  test('ChessBoard',
    executable('chessboard_test', 'src/chess/board_test.cc',
    include_directories: includes, link_with: lc0_lib, dependencies: gtest
  ), args: '--gtest_output=xml:chessboard.xml', timeout: 90)

  test('HashCat',
    executable('hashcat_test', 'src/utils/hashcat_test.cc',
    include_directories: includes, link_with: lc0_lib, dependencies: gtest
  ), args: '--gtest_output=xml:hashcat.xml', timeout: 90)

  test('PositionTest',
    executable('position_test', 'src/chess/position_test.cc',
    include_directories: includes, link_with: lc0_lib, dependencies: gtest
  ), args: '--gtest_output=xml:position.xml', timeout: 90)

  test('OptionsParserTest',
    executable('optionsparser_test', 'src/utils/optionsparser_test.cc',
    include_directories: includes, link_with: lc0_lib, dependencies: gtest
  ), args: '--gtest_output=xml:optionsparser.xml', timeout: 90)

  test('SyzygyTest',
    executable('syzygy_test', 'src/syzygy/syzygy_test.cc',
    include_directories: includes, link_with: lc0_lib, dependencies: gtest
  ), args: '--gtest_output=xml:syzygy.xml', timeout: 90)

  test('EncodePositionForNN',
    executable('encoder_test', 'src/neural/encoder_test.cc', pb_files,
    include_directories: includes, link_with: lc0_lib,
    dependencies: [gtest]
  ), args: '--gtest_output=xml:encoder.xml', timeout: 90)
endif


#############################################################################
## Python bindings
#############################################################################

if get_option('python_bindings')
  pymod = import('python')
  python = pymod.find_installation('python3')
  if python.language_version() < '3.7'
    error('You need python 3.7 or newer')
  endif
  py_bindings_generator = find_program('scripts/gen_py_bindings.py')

  gen_py_bindings = custom_target('backends', input:[], output:['backends.cc'],
    command : [py_bindings_generator, '@OUTPUT0@'])

  py_files = [ gen_py_bindings ]

  cpython = dependency('python3')
  python.extension_module('backends',
    [py_files + files],
    include_directories: [includes],
    dependencies: [cpython] + deps)
endif

```

`meson_options.txt`:

```txt
option('openblas_include',
       type: 'array',
       value: ['/usr/include/openblas/'],
       description: 'Paths to openblas include directories')

option('opencl_include',
       type: 'string',
       value: '/usr/include/',
       description: 'Path to OpenCL include directory')

option('openblas_libdirs',
       type: 'array',
       value: ['/usr/lib/'],
       description: 'Paths to OpenBLAS libraries')

option('opencl_libdirs',
       type: 'array',
       value: ['/opt/cuda/lib64/', '/usr/local/cuda/lib64/'],
       description: 'Paths to OpenCL libraries')

option('cudnn_libdirs',
       type: 'array',
       value: ['/opt/cuda/lib64/', '/usr/local/cuda/lib64/', '/usr/lib/cuda/lib64/'],
       description: 'Paths to Cuda/cudnn libraries')

option('mkl_libdirs',
       type: 'array',
       value: ['/opt/intel/lib/intel64', '/opt/intel/mkl/lib/intel64', '/opt/intel/mkl/lib'],
       description: 'Paths to MKL libraries')

option('mkl_include',
       type: 'array',
       value: ['/opt/intel/mkl/include'],
       description: 'Paths to MKL libraries')

option('dnnl_dir',
       type: 'string',
       value: '/usr',
       description: 'Paths to DNNL install directory')

option('cudnn_include',
       type: 'array',
       value: ['/opt/cuda/include/', '/usr/local/cuda/include/', '/usr/lib/cuda/include/'],
       description: 'Paths to cudnn include directory')

option('build_backends',
       type: 'boolean',
       value: true,
       description: 'Build backends for NN computation')

option('blas',
       type: 'boolean',
       value: true,
       description: 'Enable BLAS backend')

option('ispc',
       type: 'boolean',
       value: true,
       description: 'use ispc')

option('ispc_native_only',
       type: 'boolean',
       value: true,
       description: 'use ispc and enable native arch only')

option('cudnn',
       type: 'boolean',
       value: true,
       description: 'Enable cuDNN backend')

option('plain_cuda',
       type: 'boolean',
       value: true,
       description: 'Enable CUDA backend')

option('opencl',
       type: 'boolean',
       value: true,
       description: 'Enable OpenCL backend')

option('dx',
       type: 'boolean',
       value: true,
       description: 'Enable DirectX12 backend')

option('tensorflow',
       type: 'boolean',
       value: false,
       description: 'Enable TensorFlow backend')

option('onednn',
       type: 'boolean',
       value: false,
       description: 'Enable oneDNN backend')

option('openblas',
       type: 'boolean',
       value: true,
       description: 'Enable OpenBLAS support')

option('mkl',
       type: 'boolean',
       value: true,
       description: 'Enable MKL BLAS support')

option('dnnl',
       type: 'boolean',
       value: false,
       description: 'Enable DNNL BLAS support')

option('accelerate',
       type: 'boolean',
       value: true,
       description: 'Enable Accelerate BLAS support')

option('malloc',
       type : 'string',
       value: '',
       description: 'Use alternative memory allocator, e.g. tcmalloc/jemalloc')

option('mimalloc_libdir',
       type : 'string',
       value: '',
       description: 'Library directory for malloc=mimalloc')

option('popcnt',
       type: 'boolean',
       value: true,
       description: 'Use the popcnt instruction')

option('pext',
       type: 'boolean',
       value: false,
       description: 'Use the pext instruction')

option('neon',
       type: 'boolean',
       value: true,
       description: 'Use neon instructions on arm processors')

option('gtest',
       type: 'boolean',
       value: true,
       description: 'Build gtest tests')

option('embed',
       type: 'boolean',
       value: false,
       description: 'Use embedded net by default')

option('nvcc_ccbin',
       type: 'string',
       value: '',
       description: 'Override C++ compiler used by cuda nvcc')

option('python_bindings',
       type: 'boolean',
       value: false,
       description: 'Build Python bindings for the python to bind.')

option('cc_cuda',
       type: 'string',
       value: '',
       description: 'Build for a specific cuda CC, e.g. -Dcc_cuda=35 for CC 3.5')

option('onnx_libdir',
       type: 'string',
       value: '',
       description: 'Paths to ONNX runtime libraries')

option('onnx_include',
       type: 'string',
       value: '',
       description: 'Paths to ONNX runtime includes')

```

`openSUSE_install.md`:

```md
![openSUSE Logo](https://github.com/openSUSE/artwork/blob/master/logos/official/logo-color.png?style=centerme)
# lc0 on openSUSE 

openSUSE is a popular RPM based Linux distro, the install sources can be downloaded from [https://software.opensuse.org](https://software.opensuse.org) .

The steps described here are minimal, enough to install and run lc0 on openSUSE. The reader is encouraged to skim the Supplementary Information after completing the Install where additional information can be found to download an icon to beautify and enter complete information about the lc0 engine into Arena. I doubt anyone would want to run on a 32-bit machine, but for these kinds of oddball installs, modifying the script should be next to trivial.

If the User finds anything in this guide unclear, the original documentation is linked under "Supplemental Information" in the last section. And, don't forget to recommend(as an Issue) or submit a change (as a Pull).

## RPM packages vs Building from Source

An extremely versatile Build script that makes building from source simple and trivial is provided below which should run on practically any version of openSUSE but supports only the openBLAS backend, which means it can be run on any openSUSE without any hardware dependencies, and supports any version of openSUSE LEAP(ie 42.x, 15.1, etc, possibly ARM) or Tumbleweed. Most of the procedure after starting the script is just waiting to finish, no technical knowledge is required.

For those who instead prefer to not install the many files to build lc0, experimental packages (as of this writing) are available for LEAP 15 and Tumbleweed only (as of this writing). Currently the RPM supports OpenCL(AMD GPU only) and BLAS (which can be installed on any hardware). For Users who wish to use a pre-built binary from a package, skip down to ["RPM packages"](https://github.com/putztzu/lc0/blob/master/openSUSE_install.md#rpm-packages)

If someone wishes to do the work to investigate the procedures to install other backends (As of this writing, Tensorflow and CUDA are possible) findings and updates to this guide are welcomed.

## Supported openSUSE versions

The following until the "RPM Packages" section describes running the build script which would be the more flexible and versativle option today, which creates a binary for more than just the LEAP 15.0 and Tumbleweed x64 platforms

These build script instructions have been tested on current versions of LEAP 15.0 (The stable release) and Tumbleweed (The Development Rolling Release) on 64-bit machines but these instructions and the build script should work with all versions of LEAP and Tumbleweed both past and into the forseeable future provided the repositories and packages are available..

## Supported lc0 Backends

Although lc0 supports many backends including GPU computing, this guide currently describes using only the openBLAS backend for its lack of hardware dependencies. This means that this guide can be used to install on any openSUSE regardless of CPU or GPU, on physical or virtual machines. Anyone who wishes to build CUDA, OpenCL or Tensorflow backends are invited to contribute and modify this page.

## For Players who don't want to know the details, Just set up and Go!

Not all Players are Tech-heads, they just want something that will work ASAP without knowing the details. An Install Script is provided for you, which should have you playing in a very short period of time.

Summary of steps

* Compile the lc0 Engine
* Download the Networks file and place in the same folder as the compiled lc0 binary
* Move or Copy the folder containing the lc0 binary to a convenient location for setting up the graphical chessboard
* Configure the graphical chessboard pointing to the lc0 binary

#### Compile the lc0 Engine

Download and save the following file to your machine by clicking on the following link

[install_openSUSE_lc0.sh](install_openSUSE_lc0.sh)

The file you just downloaded can be run from any location, but must be executed in a root console.

Open a root console by first opening whatever terminal or console was installed in your Desktop, examples might be Xterm, Konsole, Qterminal, Gnome Terminal.
Then in your console type

`
su
`

For the following two commands and elsewhere below, the User can either copy the text into an open console or type the commands by hand.<br>

Change directory to where your downloaded script is (most likely your Downloads folder) and execute.
If necessary, modify the execute permission of the script with "chmod+x install_openSUSE_lc0.sh"

```

cd ~/Downloads
./install_openSUSE_lc0.sh
```


## RPM Packages

If you ran the script that builds from source, you can ignore this section and skip to the next section which describes installing a networks file. 

Otherwise, if you skipped most of what is described above because you want to use pre-built packages, this is where you should start!
1. Using an openSUSE provided Web browser (recommend Firefox), find the package for your version of openSUSE and download and/or install the package. The actual location of packages may change depending on project status, so you may also want to use the web package search at https://software.opensuse.org/package/lc0 and click on the "One-click install for your version of openSUSE.
2. Once the RPM file has been downloaded, you can install using YaST, zypper or RPM by simply pointing the install command to the file.
3. If successfully installed, you will find the lc0 binary at the following location and you can now proceed to the next section.
```

/usr/bin/lco
```



## The One Thing you must do Manually

#### Download and install the Networks file

The Install script automates practically everything needed for the lc0 Engine to run when hooked up to a graphical chessboard. The one thing missing that you, the User has to do on your own is to select a "Networks" file which contains the game data for its thinking. New data is generated continuously and players may want to try different files so this can't be automated. You need to select a file from the following page (usually under 50MBytes) and drop the file into the same folder as your lc0 binary.

[https://lczero.org/play/networks/bestnets/](https://lczero.org/play/networks/bestnets/)

## An example setup with the Arena graphical chessboard

The following applies if you compiled your own lc0 binary or if you are using the pre-built lc0 binary on a machine with an AMD processor. Special instructions to use Arena with the RPM is provided below, otherwise if you built from source Arena should "just work." An alternative is to use another graphical chessboard, [Cute Chess](https://cutechess.com/) has been tested and verified to work. Setting up Cute Chess is generally similar to setting up on Arena, with fewer options but generally the same major steps. If Users are unable to figure out how to set up with Cute Chess, a section will be added later.

Download the Arena Linux app from [Download Arena for Linux](http://www.playwitharena.com/?Download:Arena_for_Linux)

If your web browser offers to extract the archive, you should decline. Perform the following which installs in your User's Home Directory

```

mkdir ~/Arena
mv arenalinux_64bit_1.1.tar.gz ~/Arena/
cd ~/Arena/
tar -xf arenalinux_64bit_1.1.tar.gz
```
Now, a special command to reset permissions on ~/.configure/ so that a regular User account can write to this directory. After you run the following command as root, you will be able to run Arena as a regular User.


```

chown -R $USER:$(id -gn $USER) ~/.config
```



Now you can execute Arena as a normal, non-root User with the following command.

```

./Arena_x86_64_linux
```

#### Copying the lc0 binary to a location for Arena access

The above completes the installation of Arena, but does not hook it up to any Chess Engines. You can set up the Chess Engines that come with Arena, but the following describes how to set up lc0.
Assuming that your command console is still open and at the root of the Arena application, the next steps set up lc0 to connect to Arena

The "network file" in the following is the file you should have downloaded in the above section "The One thing you must do Manually"

If you built using the "install_openSUSE_lc0.sh" script, run the following
```

mkdir Engines/lc0
cp -r /opt/lc0/build/release/* Engines/lc0/
cp "network file" Engines/lc0/
```
If you installed using a RPM, run the following
```

mkdir Engines/lc0
cp /usr/bin/lc0 Engines/lc0/
cp "network file" Engines/lc0/
```



#### Configure Arena to point to the lc0 binary

Your files are now pre-positioned and ready for Arena.

Launch the Arena Engine Install Wizard with the F11 key. Or, if that doesn't work, then from the Arena menubar, 
Engines > Manage > Details tab > Installation Wizard button

Enter the information as required, pointing to your lc0 binary at

```

[Arena_root]/Engines/lc0/lc0
```

__RPM Package installs only__:

* In Arena, "Load" the lc0 engine
* In Arena, Press Ctl-1 or 
   Engines > Engine 1 > Configure
   Find the "backend" setting and select your preference, currently should be either opencl or blas
* While still in "Engine 1" start the engine

You should now be able to play!




## Removal / Uninstall

If RPMs were used to install lc0, you can remove using ordinary package management commands, eg the YaST Software Manager or the following command line
...


zypper rm lc0
...

The following describes removing files manually, either to verify total removal or if installed from source

The following commands remove parts of the install, the User can decide which to implement

The script installs lc0 files in /opt/lc0
So the following removes these files

```

rm -r /opt/lc0/
```

The script adds the following repositories. If you don't have something else using these repositories, they can be removed with the following command

```

zypper rr LEAP:devel:gcc LEAP:devel:languages:python:backports LEAP:science
```

The following packages were installed by the script and can be uninstalled. Note that uninstalling packages doesn't remove any dependencies of these packages, and libzl is excluded from the list because it's commonly used for other things.

```

zypper rm git gcc-c++ gcc7-c++ meson ninja python3-abseil openblas_pthreads-devel-static
```

## Re-install

Unless the Install script is interrupted, it's unlikely that the script has to be run again. If the script is run on a system more than once, non-critical errors (might look ugly, but will not harm a system) will be seen. Depending on the reason to run the Install script more than once, the User should consider either removing what has been installed (see Uninstall section) or commenting the lines in the Install script that would cause errors (primarily the Add Repo lines).

## Supplementary Information

Full Instructions connecting lc0 to graphical chessboards including Arena

[Running Leela Chess Zero in a Chess GUI"](https://github.com/LeelaChessZero/lc0/wiki/Running-Leela-Chess-Zero-in-a-Chess-GUI)

Original instructions for setting up lc0 on all platforms

[Getting Started](https://github.com/LeelaChessZero/lc0/wiki/Getting-Started)

Original compilation Instructions

[Build Instructions](https://github.com/LeelaChessZero/lc0)

```

`scripts/appveyor_android_build.cmd`:

```cmd
cd arm64-v8a
ninja
aarch64-linux-android-strip lc0
cd C:\projects\lc0
cd armeabi-v7a
ninja
arm-linux-androideabi-strip lc0

```

`scripts/appveyor_android_package.cmd`:

```cmd
git clone https://github.com/LeelaChessZero/chessenginesupport-androidlib.git --branch lc0 --single-branch oex
cd oex
git checkout 0c02a8893b9c57ec57b40569ba60625912c6d32f
cd ..
perl -e "printf '%%sLc0!', pack('V', -s 'c:/cache/%NET%.pb.gz')" >tail.bin
copy /y /b arm64-v8a\lc0+c:\cache\%NET%.pb.gz+tail.bin oex\LeelaChessEngine\leelaChessEngine\src\main\jniLibs\arm64-v8a\liblc0.so
copy /y /b armeabi-v7a\lc0+c:\cache\%NET%.pb.gz+tail.bin oex\LeelaChessEngine\leelaChessEngine\src\main\jniLibs\armeabi-v7a\liblc0.so
set ANDROID_HOME=C:\android-sdk-windows
appveyor DownloadFile https://dl.google.com/android/repository/sdk-tools-windows-3859397.zip
7z x sdk-tools-windows-3859397.zip -oC:\android-sdk-windows > nul
yes | C:\android-sdk-windows\tools\bin\sdkmanager.bat --licenses
cd oex\LeelaChessEngine
sed -i "s/591226/%NET%/" leelaChessEngine/src/main/res/values/strings.xml
sed -i "/versionCode/ s/1/%APPVEYOR_BUILD_NUMBER%/" leelaChessEngine/src/main/AndroidManifest.xml
sed -i "s/0.25dev/%APPVEYOR_REPO_TAG_NAME%/" leelaChessEngine/src/main/AndroidManifest.xml
call gradlew.bat assemble
copy leelaChessEngine\build\outputs\apk\debug\leelaChessEngine-debug.apk ..\..\lc0-%APPVEYOR_REPO_TAG_NAME%-android.apk

```

`scripts/appveyor_win_build.cmd`:

```cmd
SET PGO=false
IF %APPVEYOR_REPO_TAG%==true IF %DX%==false SET PGO=true
IF %PGO%==false msbuild "C:\projects\lc0\build\lc0.sln" /m /p:WholeProgramOptimization=true /logger:"C:\Program Files\AppVeyor\BuildAgent\Appveyor.MSBuildLogger.dll"
IF EXIST build\lc0.pdb del build\lc0.pdb
IF %PGO%==true msbuild "C:\projects\lc0\build\lc0.sln" /m /p:WholeProgramOptimization=PGInstrument /logger:"C:\Program Files\AppVeyor\BuildAgent\Appveyor.MSBuildLogger.dll"
IF ERRORLEVEL 1 EXIT
cd build
IF %NAME%==cpu-openblas copy C:\cache\OpenBLAS\dist64\bin\libopenblas.dll
IF %NAME%==cpu-dnnl copy C:\cache\%DNNL_NAME%\bin\dnnl.dll
IF %NAME%==onednn copy C:\cache\%DNNL_NAME%\bin\dnnl.dll
IF %NAME%==onednn copy dnnl.dll ..
copy "%MIMALLOC_PATH%"\out\msvc-x64\Release\mimalloc-override.dll
copy "%MIMALLOC_PATH%"\out\msvc-x64\Release\mimalloc-redirect.dll
IF %PGO%==true (
  IF %OPENCL%==true copy C:\cache\opencl-nug.0.777.77\build\native\bin\OpenCL.dll
  IF %CUDA%==true copy "%CUDA_PATH%"\bin\*.dll
  IF %CUDNN%==true copy "%CUDA_PATH%"\cuda\bin\cudnn64_7.dll
  lc0 benchmark --num-positions=1 --weights=c:\cache\%NET%.pb.gz --backend=random --movetime=10000
  lc0 benchmark --num-positions=1 --weights=c:\cache\%NET%.pb.gz --backend=random --movetime=10000 --multi-gather=false
)
cd ..
IF %PGO%==true msbuild "C:\projects\lc0\build\lc0.sln" /m /p:WholeProgramOptimization=PGOptimize /p:DebugInformationFormat=ProgramDatabase /logger:"C:\Program Files\AppVeyor\BuildAgent\Appveyor.MSBuildLogger.dll"

```

`scripts/appveyor_win_package.cmd`:

```cmd
7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip %APPVEYOR_BUILD_FOLDER%\build\lc0.exe
appveyor DownloadFile "https://ci.appveyor.com/api/projects/LeelaChessZero/lczero-client/artifacts/lc0-training-client.exe?branch=release&pr=false&job=Environment%%3A%%20NAME%%3D.exe%%2C%%20GOOS%%3Dwindows"
7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip lc0-training-client.exe
type COPYING |more /P > dist\COPYING
7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip .\dist\COPYING
7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip c:\cache\%NET%.pb.gz
type "%MIMALLOC_PATH%"\readme.md |more /P > dist\mimalloc-readme.md
type "%MIMALLOC_PATH%"\LICENSE |more /P > dist\mimalloc-LICENSE
7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip "%MIMALLOC_PATH%"\out\msvc-x64\Release\mimalloc-override.dll
7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip "%MIMALLOC_PATH%"\out\msvc-x64\Release\mimalloc-redirect.dll
7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip .\dist\mimalloc-readme.md
7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip .\dist\mimalloc-LICENSE
IF %CUDA%==true copy lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%-nodll.zip
IF %NAME%==cpu-openblas 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip C:\cache\OpenBLAS\dist64\bin\libopenblas.dll
IF %NAME%==cpu-dnnl 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip C:\cache\%DNNL_NAME%\bin\dnnl.dll
IF %NAME%==onednn 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip C:\cache\%DNNL_NAME%\bin\dnnl.dll
IF %OPENCL%==true 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip C:\cache\opencl-nug.0.777.77\build\native\bin\OpenCL.dll
IF %CUDNN%==true 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip "%CUDA_PATH%\bin\cudart64_100.dll" "%CUDA_PATH%\bin\cublas64_100.dll"
IF %CUDNN%==true 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip "%CUDA_PATH%\cuda\bin\cudnn64_7.dll"
IF %CUDA%==true 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip "%CUDA_PATH%\bin\cudart64_110.dll" "%CUDA_PATH%\bin\cublas64_11.dll" "%CUDA_PATH%\bin\cublasLt64_11.dll"
IF %NAME%==cpu-dnnl copy "%PKG_FOLDER%\%DNNL_NAME%\LICENSE" dist\DNNL-LICENSE
IF %NAME%==cpu-dnnl copy "%PKG_FOLDER%\%DNNL_NAME%\THIRD-PARTY-PROGRAMS" dist\DNNL-THIRD-PARTY-PROGRAMS
IF %NAME%==cpu-dnnl 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip .\dist\DNNL-LICENSE
IF %NAME%==cpu-dnnl 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip .\dist\DNNL-THIRD-PARTY-PROGRAMS
IF %NAME%==onednn copy "%PKG_FOLDER%\%DNNL_NAME%\LICENSE" dist\DNNL-LICENSE
IF %NAME%==onednn copy "%PKG_FOLDER%\%DNNL_NAME%\THIRD-PARTY-PROGRAMS" dist\DNNL-THIRD-PARTY-PROGRAMS
IF %NAME%==onednn 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip .\dist\DNNL-LICENSE
IF %NAME%==onednn 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip .\dist\DNNL-THIRD-PARTY-PROGRAMS
IF %OPENCL%==true type scripts\check_opencl.bat |more /P > dist\check_opencl.bat
IF %OPENCL%==true 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip .\dist\check_opencl.bat
IF %DX%==true type scripts\check_dx.bat |more /P > dist\check_dx.bat
IF %DX%==true 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip .\dist\check_dx.bat
IF %CUDA%==true copy "%CUDA_PATH%\EULA.txt" dist\CUDA.txt
IF %CUDA%==true type dist\README-cuda.txt |more /P > dist\README.txt
IF %CUDA%==true 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip .\dist\README.txt .\dist\CUDA.txt
IF %CUDNN%==true copy "%CUDA_PATH%\cuda\NVIDIA_SLA_cuDNN_Support.txt" dist\CUDNN.txt
IF %CUDNN%==true 7z a lc0-%APPVEYOR_REPO_TAG_NAME%-windows-%NAME%.zip .\dist\CUDNN.txt
IF EXIST lc0-%APPVEYOR_REPO_TAG_NAME%-windows-gpu-dx12.zip ren lc0-%APPVEYOR_REPO_TAG_NAME%-windows-gpu-dx12.zip lc0-%APPVEYOR_REPO_TAG_NAME%-windows10-gpu-dx12.zip

```

`scripts/bumpversion.py`:

```py
#!/usr/bin/env python3

import argparse
import os


VERSION_FILE = os.path.join(os.path.dirname(os.path.realpath(__file__)), "../src/version.inc")
VERSION_CONTENT = """
#define LC0_VERSION_MAJOR {}
#define LC0_VERSION_MINOR {}
#define LC0_VERSION_PATCH {}
#define LC0_VERSION_POSTFIX "{}"
"""
VERSION_CONTENT = VERSION_CONTENT.strip()


def get_version():
    with open(VERSION_FILE, 'r') as f:
        major = int(f.readline().split()[2])
        minor = int(f.readline().split()[2])
        patch = int(f.readline().split()[2])
        postfix = f.readline().split()[2]

    postfix = postfix.replace('"', '')
    return major, minor, patch, postfix


def set_version(major, minor, patch, postfix=""):
    version_inc = VERSION_CONTENT.format(major, minor, patch, postfix)

    with open(VERSION_FILE, 'w') as f:
        f.write(version_inc)


def update(major, minor, patch, postfix=""):
    set_version(major, minor, patch, postfix)


def main(argv):
    major, minor, patch, postfix = get_version()

    if argv.major:
        major += 1
        minor = 0
        patch = 0
        postfix = ""
        update(major, minor, patch)
    if argv.minor:
        minor += 1
        patch = 0
        postfix = ""
        update(major, minor, patch)
    if argv.patch:
        patch += 1
        postfix = ""
        update(major, minor, patch)
    if argv.postfix and len(argv.postfix) > 0:
        postfix = argv.postfix
        update(major, minor, patch, postfix)

    if len(postfix) == 0:
        print('v{}.{}.{}'.format(major, minor, patch))
    else:
        print('v{}.{}.{}-{}'.format(major, minor, patch, postfix))


if __name__ == "__main__":
    argparser = argparse.ArgumentParser(description=\
            'Set or read current version.')
    argparser.add_argument('--major', action='store_true',
            help='bumps major version')
    argparser.add_argument('--minor', action='store_true',
            help='bumps minor version')
    argparser.add_argument('--patch', action='store_true',
            help='bumps patch')
    argparser.add_argument('--postfix', type=str,
            help='set postfix')
    argv = argparser.parse_args()
    main(argv)


```

`scripts/check_dx.bat`:

```bat
@ECHO OFF
ECHO Sanity checking the dx12 driver.
lc0 benchmark --num-positions=1 --backend=check --backend-opts=mode=check,freq=1.0,atol=5e-1,dx12 %*
PAUSE


```

`scripts/check_opencl.bat`:

```bat
@ECHO OFF
ECHO Sanity checking the opencl driver.
lc0 benchmark --num-positions=1 --backend=check --backend-opts=mode=check,freq=1.0,opencl %*
PAUSE


```

`scripts/checkdir.py`:

```py
#!/usr/bin/env python3

import sys
import os
if len(sys.argv) > 1 and os.path.isdir(sys.argv[1]):
  sys.exit(0)
sys.exit(1)

```

`scripts/compile_proto.py`:

```py
#!/usr/bin/env python3

#  This file is part of Leela Chess Zero.
#  Copyright (C) 2020 The LCZero Authors
#
#  Leela Chess is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  Leela Chess is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
#
#  Additional permission under GNU GPL version 3 section 7
#
#  If you modify this Program, or any covered work, by linking or
#  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
#  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
#  modified version of those libraries), containing parts covered by the
#  terms of the respective license agreement, the licensors of this
#  Program grant you additional permission to convey the resulting work.

import argparse
import os
import re
import sys

VARINT_TYPES = {
    'int32': 'std::int32_t',
    'int64': 'std::int64_t',
    'uint32': 'std::uint32_t',
    'uint64': 'std::uint64_t',
    'sint32': 'std::uint32_t',
    'sint64': 'std::uint64_t',
    'bool': 'bool',
}

FIXED64_TYPES = {
    'fixed64': 'std::uint64_t',
    'sfixed64': 'std::int64_t',
    'double': 'double',
}
FIXED32_TYPES = {
    'fixed32': 'std::uint32_t',
    'sfixed32': 'std::int32_t',
    'float': 'float',
}
BYTES_TYPES = {
    'string': 'std::string_view',
    'bytes': 'std::string_view',
}
ZIGZAG_TYPES = set(['sint32', 'sint64'])
FLOAT_TYPES = set(['float', 'double'])

TYPES = {**VARINT_TYPES, **FIXED32_TYPES, **FIXED64_TYPES, **BYTES_TYPES}

RESERVED_WORDS = [
    'syntax',
    'package',
    'message',
    'optional',
    'repeated',
    'enum',
] + list(TYPES.keys())

GRAMMAR = ([(r'%s\b' % x, x)
            for x in RESERVED_WORDS] + [('\\' + x, x) for x in '=;{}.'] + [
                (r'/\*.*?\*/', None),  # /* Comment */
                (r'//.*?$', None),  # // Comment
                (r'\s+', None),  # Whitespace
                (r'$', 'EOF'),
                (r'"((?:[^"\\]|\\.)*)"', 'string'),
                (r'\d+', 'number'),
                (r'\w+', 'identifier'),
            ])


class Lexer:
    def __init__(self, text):
        self.text = text
        self.grammar = [(re.compile(x, re.S + re.M), y) for x, y in GRAMMAR]
        self.cur_token = None
        self.cur_offset = 0

    def Pick(self):
        '''Picks the last token in queue. Doesn't advance the queue.'''
        if self.cur_token is None:
            self.cur_token = self.NextToken()
        return self.cur_token

    def Consume(self, expected_token, value=None, group=0):
        '''Gets the token from the queue and advances the queue.

        If @expected_token if of wrong type, or @value is not equal to regexes
        @group, throws an error.
        '''
        token, match = self.Pick()
        if expected_token != token:
            self.Error('Expected token type [%s]' % expected_token)
        if value is not None and value != match.group(group):
            self.Error('Expected value [%s]' % value)
        self.cur_offset = match.span()[1]
        self.cur_token = None
        return match

    def NextToken(self):
        '''Reads the stream and returns the next token.

        (which is not whitespace or comment)
        '''
        while True:
            token, match = self.NextTokenOrWhitespace()
            if token is None:
                self.cur_offset = match.span()[1]
            else:
                return token, match

    def NextTokenOrWhitespace(self):
        '''Reads the stream and returns the next token (possibly whitespace).'''
        for r, token in self.grammar:
            m = r.match(self.text, self.cur_offset)
            if m:
                return (token, m)
        self.Error('Unexpected token')

    def Error(self, text):
        '''Throws an error with context in the file read.'''
        line_start = self.text.rfind('\n', 0, self.cur_offset) + 1
        line_end = self.text.find('\n', line_start)
        sys.stderr.write('%s:\n' % text)
        sys.stderr.write(self.text[line_start:line_end] + '\n')
        sys.stderr.write(' ' * (self.cur_offset - line_start) + '^^^\n')
        raise ValueError("Parse error: %s at offset %d." %
                         (text, self.cur_offset))


def ReadIdentifierPath(lexer):
    '''Reads qualified identifier a.b.d into ['a', 'b', 'd'] list'''
    path = []
    while True:
        path.append(lexer.Consume('identifier').group(0))
        if lexer.Pick()[0] != '.':
            return path
        lexer.Consume('.')


def LookupType(name, stack):
    '''Looks up the (possibly qualified) from the innermost scope first.'''
    for y in stack:
        for x in y:
            if x.GetName() == name[0]:
                if len(name) == 1:
                    return x.GetType()
                else:
                    return LookupType(name[1:], [x.GetTypes()])
    raise ValueError("Cannot find type: %s." % '.'.join(name))


# All *Parser classes have the following semantics:
# * They are called with lexer as input to parse grammar from .proto file.
# * The Generate() function writes relevant portion of .pb.h file.


class ProtoTypeParser:
    def __init__(self, lexer, object_stack):
        token, match = lexer.Pick()
        if token in TYPES:
            self.typetype = 'basic'
            self.name = token
            lexer.Consume(token)
        elif token == 'identifier':
            self.name = ReadIdentifierPath(lexer)
            self.typetype = LookupType(self.name, object_stack)
        else:
            lexer.Error('Type expected')

    def IsZigzag(self):
        if self.typetype == 'basic':
            return self.name in ZIGZAG_TYPES
        return False

    def GetCppType(self):
        if self.typetype == 'basic':
            return TYPES[self.name]
        else:
            return '::'.join(self.name)

    def GetVariableCppType(self):
        if self.IsBytesType():
            return 'std::string'
        else:
            return self.GetCppType()

    def IsVarintType(self):
        return self.typetype == 'enum' or (self.typetype == 'basic'
                                           and self.name in VARINT_TYPES)

    def IsFixedType(self):
        return self.typetype == 'basic' and (self.name in FIXED64_TYPES
                                             or self.name in FIXED32_TYPES)

    def IsBytesType(self):
        return self.typetype == 'basic' and self.name in BYTES_TYPES

    def IsFloatType(self):
        return self.typetype == 'basic' and self.name in FLOAT_TYPES

    def GetWireType(self):
        if self.typetype == 'basic':
            if self.name in VARINT_TYPES:
                return 0
            if self.name in FIXED64_TYPES:
                return 1
            if self.name in BYTES_TYPES:
                return 2
            if self.name in FIXED32_TYPES:
                return 5
            raise ValueError('Unknown type %s' % self.name)
        elif self.typetype == 'enum':
            return 0
        elif self.typetype == 'message':
            return 2
        else:
            raise ValueError('Unknown typetype %s' % self.typetype)

    def IsMessage(self):
        return self.typetype == 'message'

    def IsIntegralType(self):
        if self.typetype == 'basic':
            if self.name == 'double':
                return False
            if self.name == 'float':
                return False
            if self.name in BYTES_TYPES:
                return False
            if self.name in TYPES:
                return True
            raise ValueError('Unknown type %s' % self.name)
        elif self.typetype == 'enum':
            return True
        elif self.typetype == 'message':
            return False
        else:
            raise ValueError('Unknown typetype %s' % self.typetype)


class ProtoFieldParser:
    def __init__(self, lexer, object_stack):
        token, match = lexer.Pick()
        if token not in ['repeated', 'optional', 'required']:
            lexer.Error('repeated, optional or required expected')
        self.category = token
        lexer.Consume(token)
        self.type = ProtoTypeParser(lexer, object_stack)
        self.name = lexer.Consume('identifier')
        lexer.Consume('=')
        self.number = int(lexer.Consume('number').group(0))
        lexer.Consume(';')

    def IsType(self):
        return False

    def GetParser(self):
        name = self.name.group(0)
        if self.type.IsMessage():
            if self.category == 'repeated':
                return 'add_%s()->MergeFromString(val)' % name
            else:
                return 'mutable_%s()->MergeFromString(val)' % name

        cpp_type = self.type.GetCppType()
        val = 'NOT IMPLEMENTED!'
        if self.type.IsVarintType():
            val_val = 'UnZigZag(val)' if self.type.IsZigzag() else 'val'
            val = 'static_cast<%s>(%s)' % (cpp_type, val_val)
        elif self.type.IsFixedType():
            if self.type.IsFloatType():
                val = 'bit_cast<%s>(val)' % cpp_type
            else:
                val = 'static_cast<%s>(val)' % cpp_type
        elif self.type.IsBytesType():
            val = 'val'

        if self.category == 'repeated':
            return '%s_.emplace_back(%s)' % (name, val)
        else:
            return 'set_%s(%s)' % (name, val)

    def GenerateCaseClause(self, w):
        w.Write('case %d: %s; break;' % (self.number, self.GetParser()))

    def GenerateClear(self, w):
        name = self.name.group(0)
        if self.category == 'repeated':
            w.Write('%s_.clear();' % name)
        else:
            w.Write('has_%s_ = false;' % name)
            w.Write('%s_ = {};' % name)

    def GenerateOutput(self, w):
        fname = {
            0: 'AppendVarInt',
            1: 'AppendInt64',
            2: 'AppendString',
            5: 'AppendInt32'
        }
        tname = {
            0: 'std::uint64_t',
            1: 'std::uint64_t',
            2: 'std::string_view',
            5: 'std::uint32_t'
        }
        wire_id = self.type.GetWireType()
        if self.category == 'repeated':
            prefix = 'for (const auto& x : %s)' % (self.name.group(0) + '_')
            name = 'x'
        else:
            name = self.name.group(0) + '_'
            prefix = 'if (has_%s)' % (name)
        if self.type.IsMessage():
            name += '.OutputAsString()'
        elif self.type.IsFloatType():
            name = 'bit_cast<%s>(%s)' % (tname[wire_id], name)

        w.Write('%s %s(%d, %s, &out);' %
                (prefix, fname[wire_id], self.number, name))

    def GenerateFunctions(self, w):
        name = self.name.group(0)
        cpp_type = self.type.GetCppType()
        var_cpp_type = self.type.GetVariableCppType()
        if self.category == 'repeated':
            if self.type.IsMessage():
                w.Write("%s* add_%s() { return &%s_.emplace_back(); }" %
                        (cpp_type, name, name))
            else:
                w.Write("void add_%s(%s val) { %s_.emplace_back(val); }" %
                        (name, cpp_type, name))
            w.Write("const std::vector<%s>& %s() const { return %s_; }" %
                    (var_cpp_type, name, name))
            if self.type.IsMessage():
                w.Write("const %s& %s(size_t idx) const { return %s_[idx]; }" %
                        (cpp_type, name, name))
            else:
                w.Write("%s %s(size_t idx) const { return %s_[idx]; }" %
                        (cpp_type, name, name))
            w.Write("size_t %s_size() const { return %s_.size(); }" %
                    (name, name))
        else:
            w.Write("bool has_%s() const { return has_%s_; }" % (name, name))
            if self.type.IsMessage():
                w.Write("const %s& %s() const { return %s_; }" %
                        (cpp_type, name, name))
                w.Write("%s* mutable_%s() {" % (cpp_type, name))
                w.Indent()
                w.Write('has_%s_ = true;' % (name))
                w.Write('return &%s_;' % name)
                w.Unindent()
                w.Write("}")
            else:
                w.Write("%s %s() const { return %s_; }" %
                        (cpp_type, name, name))
                w.Write("void set_%s(%s val) {" % (name, cpp_type))
                w.Indent()
                w.Write("has_%s_ = true;" % name)
                w.Write("%s_ = val;" % name)
                w.Unindent()
                w.Write("}")

    def GenerateVariable(self, w):
        name = self.name.group(0)
        cpp_type = self.type.GetVariableCppType()
        if self.category == 'repeated':
            w.Write("std::vector<%s> %s_;" % (cpp_type, name))
        else:
            w.Write("bool has_%s_{};" % (name))
            w.Write("%s %s_{};" % (cpp_type, name))
        return


class ProtoEnumParser:
    def __init__(self, lexer):
        lexer.Consume('enum')
        self.name = lexer.Consume('identifier').group(0)
        self.values = []
        lexer.Consume('{')
        while True:
            token, match = lexer.Pick()
            if token == '}':
                break
            key = lexer.Consume('identifier').group(0)
            lexer.Consume('=')
            value = int(lexer.Consume('number').group(0))
            lexer.Consume(';')
            self.values.append((key, value))
        lexer.Consume('}')

    def GetName(self):
        return self.name

    def GetType(self):
        return 'enum'

    def IsType(self):
        return True

    def Generate(self, w):
        # Protobuf enum is mapped directly to C++ enum.
        w.Write('enum %s {' % self.name)
        w.Indent()
        for key, value in self.values:
            w.Write('%s = %d,' % (key, value))
        w.Unindent()
        w.Write('};')
        # Static array of all possible enum values.
        w.Write('static constexpr std::array<%s,%d> %s_AllValues = {' %
                (self.name, len(self.values), self.name))
        w.Indent()
        for key, _ in self.values:
            w.Write('%s,' % key)
        w.Unindent()
        w.Write('};')
        # Static function to convert an enum value to its name.
        w.Write('static std::string %s_Name(%s val) {' %
                (self.name, self.name))
        w.Indent()
        w.Write('switch (val) {')
        w.Indent()
        for key, _ in self.values:
            w.Write('case %s:' % key)
            w.Write('  return "%s";' % key)
        w.Unindent()
        w.Write('};')
        w.Write('return "%s(" + std::to_string(val) + ")";' % self.name)
        w.Unindent()
        w.Write('}')


class ProtoMessageParser:
    def __init__(self, lexer, type_stack):
        self.types = []
        self.fields = []
        lexer.Consume('message')
        self.name = lexer.Consume('identifier').group(0)
        lexer.Consume('{')
        while True:
            token, match = lexer.Pick()
            if token == '}':
                break
            elif token == 'message':
                self.types.append(
                    ProtoMessageParser(lexer, [self.types, *type_stack]))
            elif token == 'enum':
                self.types.append(ProtoEnumParser(lexer))
            elif token in ['repeated', 'optional', 'required']:
                self.fields.append(
                    ProtoFieldParser(lexer, [self.types, *type_stack]))
            else:
                lexer.Error('Expected field or type')
        lexer.Consume('}')

    def GetName(self):
        return self.name

    def GetType(self):
        return 'message'

    def IsType(self):
        return True

    def GetTypes(self):
        return self.types

    def GetFieldsGruppedByWireType(self):
        type_to_fields = {}
        for x in self.fields:
            type_to_fields.setdefault(x.type.GetWireType(), []).append(x)
        return type_to_fields

    def WriteFieldParser(self, w, wire_id, fields):
        fname = {0: 'SetVarInt', 1: 'SetInt64', 2: 'SetString', 5: 'SetInt32'}
        tname = {
            0: 'std::uint64_t',
            1: 'std::uint64_t',
            2: 'std::string_view',
            5: 'std::uint32_t'
        }
        w.Write('void %s(int field_id, %s val) override {' %
                (fname[wire_id], tname[wire_id]))
        w.Indent()
        w.Write('switch (field_id) {')
        w.Indent()
        for field in fields:
            field.GenerateCaseClause(w)
        w.Unindent()
        w.Write('}')
        w.Unindent()
        w.Write('}')

    def Generate(self, w):
        # Protobuf message is a C++ class.
        w.Write('class %s : public lczero::ProtoMessage {' % self.name)
        w.Write(' public:')
        w.Indent()
        # Writing submessages and enums.
        for x in self.types:
            x.Generate(w)
        for x in self.fields:
            w.Write('')
            x.GenerateFunctions(w)
        w.Write('')
        w.Write('std::string OutputAsString() const override {')
        w.Indent()
        w.Write('std::string out;')
        for x in sorted(self.fields, key=lambda x: x.number):
            x.GenerateOutput(w)
        w.Write('return out;')
        w.Unindent()
        w.Write('}')
        w.Write('')
        w.Write('void Clear() override {')
        w.Indent()
        for x in self.fields:
            x.GenerateClear(w)
        w.Unindent()
        w.Write('}')
        w.Unindent()
        w.Write('')
        w.Write(' private:')
        w.Indent()
        for k, v in self.GetFieldsGruppedByWireType().items():
            self.WriteFieldParser(w, k, v)
        w.Write('')
        for x in self.fields:
            x.GenerateVariable(w)
        w.Unindent()
        w.Write('};')


class ProtoFileParser:
    '''Root grammar of .proto file'''
    def __init__(self, lexer):
        self.package = None
        self.objects = []
        while True:
            token, match = lexer.Pick()
            if token == 'EOF':
                return
            elif token == 'syntax':
                self.ParseSyntax(lexer)
            elif token == 'package':
                self.ParsePackage(lexer)
            elif token == 'message':
                self.ParseMessage(lexer)
            else:
                lexer.Error('Expected message or something similar')

    def ParseSyntax(self, lexer):
        lexer.Consume('syntax')
        lexer.Consume('=')
        lexer.Consume('string', 'proto2', 1)
        lexer.Consume(';')

    def ParsePackage(self, lexer):
        lexer.Consume('package')
        if self.package is not None:
            lexer.Error('Package was already defined')
        self.package = ReadIdentifierPath(lexer)
        lexer.Consume(';')

    def ParseMessage(self, lexer):
        self.objects.append(ProtoMessageParser(lexer, [self.objects]))

    def Generate(self, w):
        w.Write('// This file is AUTOGENERATED, do not edit.')
        w.Write('#pragma once')
        w.Write('#include "utils/protomessage.h"')
        for x in self.package:
            w.Write('namespace %s {' % x)
        w.Indent()
        for object in self.objects:
            object.Generate(w)
        w.Unindent()
        for x in reversed(self.package):
            w.Write('}  // namespace %s' % x)


class Writer:
    '''A helper class for writing file line by line with indent.'''
    def __init__(self, fo):
        self.fo = fo
        self.indent = 0

    def Indent(self):
        self.indent += 2

    def Unindent(self):
        self.indent -= 2

    def Write(self, text):
        if text:
            self.fo.write(' ' * self.indent + text + '\n')
        else:
            self.fo.write('\n')


if __name__ == "__main__":
    # Have the same flags as protoc has.
    parser = argparse.ArgumentParser(description="Compile protobuf files.")
    parser.add_argument('input', type=str)
    parser.add_argument('--proto_path', type=str)
    parser.add_argument('--cpp_out', type=str)
    args = parser.parse_args()

    rel_path = os.path.relpath(args.input, args.proto_path)
    dest_name = os.path.splitext(rel_path)[0] + '.pb.h'
    dest_path = os.path.join(args.cpp_out, dest_name)
    dest_dir = os.path.dirname(dest_path)
    os.makedirs(dest_dir, exist_ok=True)

    with open(args.input, 'r') as input, open(dest_path, 'w') as output:
        proto_file = ProtoFileParser(Lexer(input.read()))
        writer = Writer(output)
        proto_file.Generate(writer)

```

`scripts/gen_py_bindings.py`:

```py
#!/usr/bin/env python3

#  This file is part of Leela Chess Zero.
#  Copyright (C) 2020 The LCZero Authors
#
#  Leela Chess is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  Leela Chess is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
#
#  Additional permission under GNU GPL version 3 section 7
#
#  If you modify this Program, or any covered work, by linking or
#  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
#  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
#  modified version of those libraries), containing parts covered by the
#  terms of the respective license agreement, the licensors of this
#  Program grant you additional permission to convey the resulting work.

import sys
from pybind.writer import Writer
from pybind import Module, Class
from pybind.parameters import (StringParameter, ClassParameter,
                               NumericParameter, ArgvObjects, IntegralArgv,
                               ListOfStringsParameter)
from pybind.retval import (StringViewRetVal, StringRetVal, ListOfStringsRetVal,
                           NumericRetVal, ObjCopyRetval, ObjOwnerRetval,
                           ObjTupleRetVal, IntegralTupleRetVal)
from pybind.exceptions import CppException

# Module
mod = Module('backends')
mod.AddInclude('python/weights.h')
mod.AddInitialization('lczero::InitializeMagicBitboards();')
ex = mod.AddException(
    CppException('LczeroException', cpp_name='lczero::Exception'))

# Weights class
weights = mod.AddClass(Class('Weights', cpp_name='lczero::python::Weights'))
weights.constructor.AddParameter(
    StringParameter('filename', optional=True, can_be_none=True)).AddEx(ex)
weights.AddMethod('filename').AddRetVal(StringViewRetVal())
weights.AddMethod('license').AddRetVal(StringViewRetVal())
weights.AddMethod('min_version').AddRetVal(StringRetVal())
weights.AddMethod('input_format').AddRetVal(NumericRetVal('i'))
weights.AddMethod('policy_format').AddRetVal(NumericRetVal('i'))
weights.AddMethod('value_format').AddRetVal(NumericRetVal('i'))
weights.AddMethod('moves_left_format').AddRetVal(NumericRetVal('i'))
weights.AddMethod('blocks').AddRetVal(NumericRetVal('i'))
weights.AddMethod('filters').AddRetVal(NumericRetVal('i'))

# Input class
input = mod.AddClass(Class('Input', cpp_name='lczero::python::Input'))
input.AddMethod('set_mask').AddParameter(NumericParameter('plane'),
                                         NumericParameter(
                                             'mask', type='u64')).AddEx(ex)
input.AddMethod('set_val').AddParameter(NumericParameter('plane'),
                                        NumericParameter('value',
                                                         type='f32')).AddEx(ex)
input.AddMethod('mask').AddParameter(NumericParameter('plane')).AddRetVal(
    NumericRetVal('u64')).AddEx(ex)
input.AddMethod('val').AddParameter(NumericParameter('plane')).AddRetVal(
    NumericRetVal('f32')).AddEx(ex)
input.AddMethod('clone').AddRetVal(ObjOwnerRetval(input))

# Output class
output = mod.AddClass(
    Class('Output',
          cpp_name='lczero::python::Output',
          disable_constructor=True))
output.AddMethod('q').AddRetVal(NumericRetVal('f32'))
output.AddMethod('d').AddRetVal(NumericRetVal('f32'))
output.AddMethod('m').AddRetVal(NumericRetVal('f32'))
output.AddMethod('p_raw').AddParameter(IntegralArgv('samples', 'i')).AddRetVal(
    IntegralTupleRetVal('f32')).AddEx(ex)
output.AddMethod('p_softmax').AddParameter(IntegralArgv(
    'samples', 'i')).AddRetVal(IntegralTupleRetVal('f32')).AddEx(ex)

# Backend capabilities class
backend_caps = mod.AddClass(
    Class('BackendCapabilities',
          cpp_name='lczero::python::BackendCapabilities',
          disable_constructor=True))
backend_caps.AddMethod('input_format').AddRetVal(NumericRetVal('i'))
backend_caps.AddMethod('moves_left_format').AddRetVal(NumericRetVal('i'))

# Backend class
backend = mod.AddClass(Class('Backend', cpp_name='lczero::python::Backend'))
backend.AddStaticMethod('available_backends').AddRetVal(ListOfStringsRetVal())
backend.constructor.AddParameter(
    ClassParameter(weights, 'weights', optional=True),
    StringParameter('backend', optional=True, can_be_none=True),
    StringParameter('options', optional=True, can_be_none=True)).AddEx(ex)
backend.AddMethod('evaluate').AddParameter(ArgvObjects(
    'inputs', input)).AddRetVal(ObjTupleRetVal(output)).AddEx(ex)
backend.AddMethod('capabilities').AddRetVal(ObjCopyRetval(backend_caps))

# PositionHistory class
game_state = mod.AddClass(
    Class('GameState', cpp_name='lczero::python::GameState'))
game_state.constructor.AddParameter(
    StringParameter('fen', optional=True, can_be_none=True),
    ListOfStringsParameter('moves', optional=True),
).AddEx(ex)
game_state.AddMethod('as_input').AddParameter(
    ClassParameter(backend, 'backend',
                   optional=False)).AddRetVal(ObjOwnerRetval(input)).AddEx(ex)
game_state.AddMethod('moves').AddRetVal(ListOfStringsRetVal())
game_state.AddMethod('policy_indices').AddRetVal(IntegralTupleRetVal('i'))
game_state.AddMethod('as_string').AddRetVal(StringRetVal())

with open(sys.argv[1], 'wt') as f:
    writer = Writer(f)
    mod.Generate(writer)
```

`scripts/pybind/__init__.py`:

```py
#  This file is part of Leela Chess Zero.
#  Copyright (C) 2020 The LCZero Authors
#
#  Leela Chess is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  Leela Chess is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
#
#  Additional permission under GNU GPL version 3 section 7
#
#  If you modify this Program, or any covered work, by linking or
#  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
#  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
#  modified version of those libraries), containing parts covered by the
#  terms of the respective license agreement, the licensors of this
#  Program grant you additional permission to convey the resulting work.

from .core import PyObject
from .functions import (Constructor, MemberFunction, StaticFunction,
                        DisabledConstructor)
from .exceptions import (CppException)


class FunctionContainer(PyObject):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.functions = []

    def _generate_functions(self, w):
        for func in self.functions:
            func.Generate(w)
        self._generate_function_list(w)

    def _generate_function_list(self, w):
        w.Open(f'PyMethodDef {self.function_list_name()}[] = {{')
        for f in self.functions:
            w.Write(f'{{"{f.name}", '
                    f'reinterpret_cast<PyCFunction>(&{f.gen_function_name}), '
                    f'{f.function_meth_flags()}, {f.BuildDocString()}}},')
        w.Write('{nullptr, nullptr, 0, nullptr}')
        w.Close('};\n')


class Class(FunctionContainer):
    def __init__(self,
                 name,
                 *argv,
                 cpp_name=None,
                 disable_constructor=False,
                 **kwargs):
        self.cpp_name = cpp_name or name
        super().__init__(name, *argv, **kwargs)
        if disable_constructor:
            self.constructor = DisabledConstructor(
                gen_function_name=self.constructor_name())
        else:
            self.constructor = Constructor(
                cpp_type_name=self.cpp_name,
                gen_function_name=self.constructor_name(),
                self_type=self.object_struct_name())

    def GenerateForwardDeclarations(self, w):
        w.Write(f'struct {self.object_struct_name()};')
        w.Write(f'extern PyTypeObject {self.type_object_name()};')

    def Generate(self, w, module):
        # Object type.
        w.Open(f'struct {self.object_struct_name()} {{')
        w.Write('PyObject_HEAD')
        w.Write(f'{self.cpp_name} *value;')
        w.Close('};\n')

        # Functions
        self._generate_functions(w)

        # Constructor and destructor.
        self.constructor.Generate(w)
        self._generate_destructor(w)

        # Type object.
        self._generate_class_struct(w, module)

    def GenerateRegister(self, w):
        w.Write(f'if (PyType_Ready(&{self.type_object_name()}) '
                '!= 0) return nullptr;')
        w.Write(f'PyModule_AddObject(module, "{self.name}", '
                f'&{self.type_object_name()}.ob_base.ob_base);')

    def AddMethod(self, name, cpp_name=None, *argv, **kwargs):
        method = MemberFunction(name,
                                cpp_name=cpp_name,
                                self_type=self.object_struct_name(),
                                gen_function_name=f'F{self.name}Method{name}',
                                *argv,
                                **kwargs)
        self.functions.append(method)
        return method

    def AddStaticMethod(self, name, cpp_name=None, *argv, **kwargs):
        method = StaticFunction(
            name,
            cpp_type_name=self.cpp_name,
            cpp_name=cpp_name,
            gen_function_name=f'F{self.name}StaticMethod{name}',
            *argv,
            **kwargs)
        self.functions.append(method)
        return method

    def object_struct_name(self):
        return f'T{self.name}ClassType'

    def type_object_name(self):
        return f'obj{self.name}ClassType'

    def constructor_name(self):
        return f'F{self.name}Constructor'

    def destructor_name(self):
        return f'F{self.name}Destructor'

    def function_list_name(self):
        return f'rg{self.name}ClassFunctions'

    def _generate_class_struct(self, w, module):
        w.Open(f'PyTypeObject {self.type_object_name()} = {{')
        w.Write('.ob_base = PyVarObject_HEAD_INIT(NULL, 0)')
        w.Write(f'.tp_name = "{module.name}.{self.name}",')
        w.Write(f'.tp_basicsize = sizeof({self.object_struct_name()}),')
        w.Write('.tp_dealloc = reinterpret_cast<destructor>'
                f'({self.destructor_name()}),')
        w.Write('.tp_flags = Py_TPFLAGS_DEFAULT,')
        w.Write(f'.tp_doc = {self.BuildDocString()},')
        w.Write(f'.tp_methods = {self.function_list_name()},')
        w.Write('.tp_init = reinterpret_cast<initproc>'
                f'({self.constructor_name()}),')
        w.Write('.tp_alloc = PyType_GenericAlloc,')
        w.Write('.tp_new = PyType_GenericNew,')
        w.Close('};')

    def _generate_destructor(self, w):
        w.Open(f'void {self.destructor_name()}('
               f'{self.object_struct_name()}* self) {{')
        w.Write('delete self->value;')
        w.Write('Py_TYPE(self)->tp_free(&self->ob_base);')
        w.Close('}\n')


class Module(FunctionContainer):
    def __init__(self, *argv, **kwargs):
        super().__init__(*argv, **kwargs)
        self.includes = []
        self.classes = []
        self.exceptions = []
        self.initialization = []

    def AddInclude(self, s):
        assert isinstance(s, str)
        self.includes.append(s)

    def AddClass(self, cls):
        assert isinstance(cls, Class)
        self.classes.append(cls)
        return cls

    def AddException(self, ex):
        assert isinstance(ex, CppException)
        self.exceptions.append(ex)
        return ex

    def AddInitialization(self, s):
        assert isinstance(s, str)
        self.initialization.append(s)

    def Generate(self, w):
        w.Write('// This file is AUTOGENERATED, do not edit.')
        w.Write('#define PY_SSIZE_T_CLEAN')
        w.Write('#include <Python.h>')
        for x in self.includes:
            w.Write(f'#include "{x}"')

        w.Write('\nnamespace {')
        for cls in self.exceptions:
            cls.Generate(w)
        for cls in self.classes:
            cls.GenerateForwardDeclarations(w)
        for cls in self.classes:
            cls.Generate(w, self)
        self._generate_functions(w)
        self._generate_module_stuct(w)
        w.Write('}  // anonymous namespace\n')

        self._generate_main_func(w)

    def struct_name(self):
        return f'T{self.name}Module'

    def function_list_name(self):
        return f'rg{self.name}ModuleFunctions'

    def _generate_module_stuct(self, w):
        w.Open(f'PyModuleDef {self.struct_name()} = {{')
        w.Write('PyModuleDef_HEAD_INIT,')
        w.Write(f'"{self.name}",')
        w.Write(f'{self.BuildDocString()},')
        w.Write('-1,')
        w.Write(f'{self.function_list_name()},')
        w.Write('nullptr, ' * 4)
        w.Close('};')

    def _generate_main_func(self, w):
        w.Open(f'PyMODINIT_FUNC PyInit_{self.name}() {{')
        for x in self.initialization:
            w.Write('%s\n' % x)
        w.Write(f'PyObject* module = PyModule_Create(&{self.struct_name()});')
        w.Write('if (module == nullptr) return nullptr;')
        for x in self.exceptions:
            x.GenerateRegister(w, self)
        for x in self.classes:
            x.GenerateRegister(w)
        w.Write('return module;')
        w.Close('}')

```

`scripts/pybind/core.py`:

```py
#  This file is part of Leela Chess Zero.
#  Copyright (C) 2020 The LCZero Authors
#
#  Leela Chess is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  Leela Chess is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
#
#  Additional permission under GNU GPL version 3 section 7
#
#  If you modify this Program, or any covered work, by linking or
#  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
#  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
#  modified version of those libraries), containing parts covered by the
#  terms of the respective license agreement, the licensors of this
#  Program grant you additional permission to convey the resulting work.


class PyObject:
    def __init__(self, name):
        self.name = name

    def BuildDocString(self):
        return "nullptr"

```

`scripts/pybind/exceptions.py`:

```py
#  This file is part of Leela Chess Zero.
#  Copyright (C) 2020 The LCZero Authors
#
#  Leela Chess is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  Leela Chess is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
#
#  Additional permission under GNU GPL version 3 section 7
#
#  If you modify this Program, or any covered work, by linking or
#  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
#  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
#  modified version of those libraries), containing parts covered by the
#  terms of the respective license agreement, the licensors of this
#  Program grant you additional permission to convey the resulting work.

from .core import PyObject


class CppException(PyObject):
    def __init__(self, name, cpp_name, *args, **kwargs):
        self.cpp_name = cpp_name
        super().__init__(name, *args, **kwargs)

    def type_struct_name(self):
        return f'T{self.name}ExceptionType'

    def Generate(self, w):
        w.Write(f'PyObject *{self.type_struct_name()};')

    def GenerateRegister(self, w, module):
        w.Write(f'{self.type_struct_name()} = PyErr_NewException('
                f'"{module.name}.{self.name}", nullptr, nullptr);')
        w.Write(f'if ({self.type_struct_name()} == nullptr) return nullptr;')
        w.Write(f'Py_INCREF({self.type_struct_name()});')
        w.Write(f'PyModule_AddObject(module, "{self.name}", '
                f'{self.type_struct_name()});')

    def GenerateHandle(self, w, func):
        w.Unindent()
        w.Open(f'}} catch (const {self.cpp_name} &ex) {{')
        w.Write(f'PyErr_SetString({self.type_struct_name()}, ex.what());')
        w.Write(f'return {func._failure()};')

```

`scripts/pybind/functions.py`:

```py
#  This file is part of Leela Chess Zero.
#  Copyright (C) 2020 The LCZero Authors
#
#  Leela Chess is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  Leela Chess is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
#
#  Additional permission under GNU GPL version 3 section 7
#
#  If you modify this Program, or any covered work, by linking or
#  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
#  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
#  modified version of those libraries), containing parts covered by the
#  terms of the respective license agreement, the licensors of this
#  Program grant you additional permission to convey the resulting work.

from enum import Enum
from .core import PyObject
from .parameters import Parameter
from .retval import RetVal, NoneRetVal
from .exceptions import CppException


class FunctionType(Enum):
    METH_VARARGS = 1
    METH_KEYWORDS = 2
    METH_FASTCALL = 3
    METH_FASTCALL_KEYWORDS = 4
    METH_NOARGS = 5
    METH_O = 6


class Function(PyObject):
    def __init__(self,
                 name,
                 gen_function_name,
                 self_type=None,
                 param_type=None):
        super().__init__(name)
        self.parameters = []
        self.exceptions = []
        self.gen_function_name = gen_function_name
        self.self_type = self_type
        self.param_typ = param_type
        self.retval = NoneRetVal()

    def AddParameter(self, *params):
        for param in params:
            assert isinstance(param, Parameter)
            self.parameters.append(param)
        return self

    def AddRetVal(self, val):
        assert isinstance(self.retval, NoneRetVal)
        assert isinstance(val, RetVal)
        self.retval = val
        return self

    def AddEx(self, ex):
        assert isinstance(ex, CppException)
        self.exceptions.append(ex)
        return self

    def Generate(self, w):
        w.Open(f'{self._return_cpp_type()} '
               f'{self.gen_function_name}({self._generate_params()}) {{')

        self._generate_parse_params(w)
        self.retval.GenerateDeclaration(w)

        if self.exceptions:
            w.Open('try {')

        self._generate_call(w)
        self.retval.GenerateConversion(w)

        if self.exceptions:
            self._handle_exceptions(w)
            w.Close('}')

        w.Write(f'return {self._success()};')
        w.Close('}\n')

    def param_type(self):
        return self.param_typ or Function.TypeFromParameters(self.parameters)

    def function_meth_flags(self):
        return {
            FunctionType.METH_KEYWORDS: 'METH_VARARGS | METH_KEYWORDS',
            FunctionType.METH_NOARGS: 'METH_NOARGS',
            FunctionType.METH_FASTCALL: 'METH_FASTCALL',
        }[self.param_type()]

    def _return_cpp_type(self):
        return 'PyObject*'

    def _generate_parse_params(self, w):
        if self.param_type() == FunctionType.METH_KEYWORDS:
            for param in self.parameters:
                param.GenerateParseTupleSinkDeclaration(w)
            w.Write('const char* keywords[] = {%s};' %
                    ', '.join([f'"{x.name}"'
                               for x in self.parameters] + ['nullptr']))

            def GatherFormatString():
                res = ''
                is_optional = False
                is_keyword_only = False
                for param in self.parameters:
                    if param.optional and not is_optional:
                        is_optional = True
                        res = res + '|'
                    if param.only_keyword and not param.optional:
                        raise ValueError('Non-optional only_keyword.')
                    if param.only_keyword and not is_keyword_only:
                        is_keyword_only = True
                        res = res + '$'
                    if not param.optional and is_optional:
                        raise ValueError('Non-optional after optional.')
                    res += param.parse_tuple_format()
                return res

            params_list = ',\n    '.join([
                'args',
                'kwargs',
                f'"{GatherFormatString()}"',
                'const_cast<char**>(keywords)',
            ] + [
                x for xs in self.parameters
                for x in xs.parse_tuple_sink_list()
            ])
            w.Open(f'if (!PyArg_ParseTupleAndKeywords({params_list})) {{')
            w.Write(f'return {self._failure()};')
            w.Close('}')
            for param in self.parameters:
                param.GenerateCppParamInitialization(w, self)
            return

        if self.param_type() == FunctionType.METH_NOARGS:
            return

        if self.param_type() == FunctionType.METH_FASTCALL:
            assert len(self.parameters) == 1
            param = self.parameters[0]
            assert param.needs_entire_argv()
            param.GenerateCppParamInitialization(w, self)
            return

        raise NotImplementedError('Not implemented function type %d' %
                                  self.param_type())

    def _generate_params(self):
        params = []
        if self.self_type:
            params.append(f'{self.self_type}* self')
        else:
            params.append('void* /* not used */')
        param_type = self.param_type()
        if param_type == FunctionType.METH_VARARGS:
            params.append('PyObject *args')
        elif param_type == FunctionType.METH_KEYWORDS:
            params.append('PyObject *args')
            params.append('PyObject *kwargs')
        elif param_type == FunctionType.METH_FASTCALL:
            params.append('PyObject **args')
            params.append('int num_args')
        elif param_type == FunctionType.METH_FASTCALL_KEYWORDS:
            params.append('PyObject **args')
            params.append('int num_args')
            params.append('PyObject *kwargs')
        elif param_type == FunctionType.METH_NOARGS:
            params.append('PyObject* /* not used */')
        elif param_type == FunctionType.METH_O:
            params.append('PyObject **arg')
        else:
            raise NotImplementedError('Unknown parameter type')
        return ', '.join(params)

    def _list_caller_params(self):
        return ', '.join([x.name_at_caller() for x in self.parameters])

    def _success(self):
        return self.retval.ret_val()

    def _failure(self):
        return 'nullptr'

    def _handle_exceptions(self, w):
        for ex in self.exceptions:
            ex.GenerateHandle(w, self)

    @staticmethod
    def TypeFromParameters(parameters):
        if len(parameters) == 0:
            return FunctionType.METH_NOARGS
        if len(parameters) == 1 and parameters[0].needs_entire_argv():
            return FunctionType.METH_FASTCALL
        return FunctionType.METH_KEYWORDS


class MemberFunction(Function):
    def __init__(self, name, *args, cpp_name=None, **kwargs):
        self.cpp_name = cpp_name or name
        super().__init__(name, *args, **kwargs)

    def _generate_call(self, w):
        if isinstance(self.retval, NoneRetVal):
            w.Write(
                f'self->value->{self.cpp_name}({self._list_caller_params()});')
        else:
            w.Write(f'{self.retval.cpp_type()} '
                    f'{self.retval.cpp_val()} = self->value->'
                    f'{self.cpp_name}({self._list_caller_params()});')


class StaticFunction(Function):
    def __init__(self, name, cpp_type_name, *args, cpp_name=None, **kwargs):
        self.cpp_name = cpp_name or name
        self.cpp_type_name = cpp_type_name
        super().__init__(name, *args, **kwargs)

    def function_meth_flags(self):
        return super().function_meth_flags() + '| METH_STATIC'

    def _generate_call(self, w):
        if isinstance(self.retval, NoneRetVal):
            w.Write(f'{self.cpp_type_name}::{self.cpp_name}'
                    f'({self._list_caller_params()});')
        else:
            w.Write(f'{self.retval.cpp_type()} '
                    f'{self.retval.cpp_val()} = '
                    f'{self.cpp_type_name}::'
                    f'{self.cpp_name}({self._list_caller_params()});')


class Constructor(Function):
    def __init__(self, cpp_type_name, gen_function_name, *args, **kwargs):
        self.cpp_type_name = cpp_type_name
        super().__init__('__init__',
                         gen_function_name=gen_function_name,
                         param_type=FunctionType.METH_KEYWORDS,
                         *args,
                         **kwargs)

    def _generate_call(self, w):
        w.Write(f'self->value = new '
                f'{self.cpp_type_name}({self._list_caller_params()});')

    def _return_cpp_type(self):
        return 'int'

    def _success(self):
        return '0'

    def _failure(self):
        return '-1'


class DisabledConstructor(Function):
    def __init__(self, gen_function_name):
        super().__init__('__init__',
                         gen_function_name=gen_function_name,
                         param_type=FunctionType.METH_KEYWORDS)

    def Generate(self, w):
        w.Open(f'{self._return_cpp_type()} '
               f'{self.gen_function_name}({self._generate_params()}) {{')
        w.Write('PyErr_SetString(PyExc_TypeError, '
                '"Not possible to create instances of this type.");')
        w.Write(f'return {self._failure()};')
        w.Close('}\n')

    def _return_cpp_type(self):
        return 'int'

    def _success(self):
        return '0'

    def _failure(self):
        return '-1'
```

`scripts/pybind/parameters.py`:

```py
#  This file is part of Leela Chess Zero.
#  Copyright (C) 2020 The LCZero Authors
#
#  Leela Chess is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  Leela Chess is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
#
#  Additional permission under GNU GPL version 3 section 7
#
#  If you modify this Program, or any covered work, by linking or
#  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
#  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
#  modified version of those libraries), containing parts covered by the
#  terms of the respective license agreement, the licensors of this
#  Program grant you additional permission to convey the resulting work.


class Parameter:
    def __init__(self,
                 name,
                 optional=False,
                 cpp_name=None,
                 only_keyword=False):
        self.name = name
        self.cpp_name = cpp_name or name
        self.optional = optional
        self.only_keyword = only_keyword

    def name_at_caller(self):
        return self.cpp_name

    def needs_entire_argv(self):
        return False


class StringParameter(Parameter):
    def __init__(self, *argv, can_be_none=False, **kwargs):
        super().__init__(*argv, **kwargs)
        self.can_be_none = can_be_none

    def GenerateParseTupleSinkDeclaration(self, w):
        w.Write(f'const char* {self.name} = nullptr;')
        w.Write(f'Py_ssize_t {self.name}_len = 0;')

    def GenerateCppParamInitialization(self, w, func):
        if self.optional or self.can_be_none:
            w.Write(f'std::optional<std::string> {self.name_at_caller()};')
            w.Write(f'if ({self.name} != nullptr) '
                    f'{self.name_at_caller()}.emplace('
                    f'{self.name}, {self.name}_len);')
        else:
            raise NotImplementedError()

    def parse_tuple_sink_list(self):
        return [f'&{self.name}', f'&{self.name}_len']

    def parse_tuple_format(self):
        return 'z#' if self.can_be_none else 's#'

    def name_at_caller(self):
        return f'{self.cpp_name}_cpp'


class ClassParameter(Parameter):
    def __init__(self, cls, *args, **kwargs):
        self.cls = cls
        super().__init__(*args, **kwargs)

    def GenerateParseTupleSinkDeclaration(self, w):
        w.Write(f'{self.cls.object_struct_name()}* {self.name} = nullptr;')

    def parse_tuple_sink_list(self):
        return [f'&{self.cls.type_object_name()}', f'&{self.name}']

    def parse_tuple_format(self):
        return 'O!'

    def GenerateCppParamInitialization(self, w, func):
        pass

    def name_at_caller(self):
        if self.optional:
            return f'{self.cpp_name} ? {self.cpp_name}->value : nullptr'
        else:
            return f'*{self.cpp_name}->value'


class ListOfStringsParameter(Parameter):
    def GenerateParseTupleSinkDeclaration(self, w):
        w.Write(f'PyObject* {self.name} = nullptr;')

    def parse_tuple_format(self):
        return 'O!'

    def parse_tuple_sink_list(self):
        return ['&PyList_Type', f'&{self.name}']

    def GenerateCppParamInitialization(self, w, func):
        w.Write(f'std::vector<std::string> {self.name_at_caller()};')
        w.Open(f'if ({self.name} != nullptr) {{')
        w.Write(f'{self.name_at_caller()}.reserve(PyList_Size({self.name}));')
        w.Open(f'for (Py_ssize_t i = 0; i < PyList_Size({self.name}); ++i) {{')
        w.Write(f'PyObject* tmp = PyList_GetItem({self.name}, i);')
        w.Open('if (!PyUnicode_Check(tmp)) {')
        w.Write('PyErr_SetString(PyExc_TypeError, "String type expected.");')
        w.Write(f'return {func._failure()};')
        w.Close('}')
        w.Write('Py_ssize_t size;')
        w.Write('const char* str = PyUnicode_AsUTF8AndSize(tmp, &size);')
        w.Write(f'{self.name_at_caller()}.emplace_back(str, size);')
        w.Close('}')
        w.Close('}')

    def name_at_caller(self):
        return f'{self.cpp_name}_cpp'


class NumericParameter(Parameter):
    def __init__(self, *args, type='i', **kwargs):
        self.type = type
        super().__init__(*args, **kwargs)

    def GenerateParseTupleSinkDeclaration(self, w):
        w.Write(f'{self.cpp_type()} {self.name} = 0;')

    def cpp_type(self):
        return {
            'i': 'int',
            'u64': 'uint64_t',
            'f32': 'float',
        }[self.type]

    def parse_tuple_sink_list(self):
        return [f'&{self.name}']

    def parse_tuple_format(self):
        return {
            'i': 'i',
            'u64': 'k',
            'f32': 'f',
        }[self.type]

    def GenerateCppParamInitialization(self, w, func):
        pass


class ArgvParameter(Parameter):
    def __init__(self, name, type, *argv, **kwargs):
        self.type = type
        super().__init__(name, *argv, **kwargs)

    def needs_entire_argv(self):
        return True

    def cpp_type(self):
        return f'std::vector<{self.type.cpp_name}*>'


class ArgvObjects(ArgvParameter):
    def cpp_type(self):
        return f'std::vector<{self.type.cpp_name}*>'

    def GenerateCppParamInitialization(self, w, func):
        w.Write(f'{self.cpp_type()} {self.name}(num_args);')
        w.Open('for (int i = 0; i < num_args; ++i) {')
        w.Write(f'{self.name}[i] = reinterpret_cast<'
                f'{self.type.object_struct_name()}*>(args[i])->value;')
        w.Close('}')


class IntegralArgv(ArgvParameter):
    def cpp_type(self):
        return f'std::vector<{self.item_cpp_type()}>'

    def item_cpp_type(self):
        return {
            'i': 'int',
            'u64': 'uint64_t',
            'f32': 'float',
        }[self.type]

    def item_fetch_function(self):
        return {
            'i': 'PyLong_AsLongLong',
        }[self.type]

    def item_fetch_err_value(self):
        return {
            'i': '-1',
        }[self.type]

    def GenerateCppParamInitialization(self, w, func):
        w.Write(f'{self.cpp_type()} {self.name}(num_args);')
        w.Open('for (int i = 0; i < num_args; ++i) {')
        w.Write(f'auto tmp = {self.item_fetch_function()}(args[i]);')
        w.Write(f'if (tmp == {self.item_fetch_err_value()} '
                f'&& PyErr_Occurred() != nullptr) return {func._failure()};')
        w.Write(f'{self.name}[i] = tmp;')
        w.Close('}')

```

`scripts/pybind/retval.py`:

```py
#  This file is part of Leela Chess Zero.
#  Copyright (C) 2020 The LCZero Authors
#
#  Leela Chess is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  Leela Chess is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
#
#  Additional permission under GNU GPL version 3 section 7
#
#  If you modify this Program, or any covered work, by linking or
#  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
#  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
#  modified version of those libraries), containing parts covered by the
#  terms of the respective license agreement, the licensors of this
#  Program grant you additional permission to convey the resulting work.


class RetVal:
    def cpp_val(self):
        return 'retval_cpp'

    def py_val(self):
        return 'retval'

    def ret_val(self):
        return self.py_val()

    def GenerateDeclaration(self, w):
        w.Write(f'PyObject *{self.py_val()};')

    def GenerateConversion(self, w):
        raise NotImplementedError()


class NoneRetVal(RetVal):
    def GenerateDeclaration(self, w):
        pass

    def GenerateConversion(self, w):
        pass

    def py_val(self):
        return 'Py_BuildValue("")'


class GenericStringRetVal(RetVal):
    def GenerateConversion(self, w):
        w.Write(f'{self.py_val()} = Py_BuildValue("s#", '
                f'{self.cpp_val()}.data(), {self.cpp_val()}.size());')


class StringViewRetVal(GenericStringRetVal):
    def cpp_type(self):
        return 'std::string_view'

    def GenerateConversion(self, w):
        w.Write(f'{self.py_val()} = Py_BuildValue("s#", '
                f'{self.cpp_val()}.data(), {self.cpp_val()}.size());')


class StringRetVal(GenericStringRetVal):
    def cpp_type(self):
        return 'const std::string&'

    def GenerateConversion(self, w):
        w.Write(f'{self.py_val()} = Py_BuildValue("s#", '
                f'{self.cpp_val()}.data(), {self.cpp_val()}.size());')


class ListOfStringsRetVal(RetVal):
    def cpp_type(self):
        return 'std::vector<std::string>'

    def GenerateConversion(self, w):
        w.Write(f'{self.py_val()} = PyList_New({self.cpp_val()}.size());')
        w.Open(f'for (size_t i = 0; i < {self.cpp_val()}.size(); ++i) {{')
        w.Write(f'const std::string& s = {self.cpp_val()}[i];')
        w.Write(f'PyList_SetItem({self.py_val()}, '
                'i, Py_BuildValue("s#", s.data(), s.size()));')
        w.Close('}')


class NumericRetVal(RetVal):
    def __init__(self, type):
        self.type = type

    def cpp_type(self):
        return {
            'i': 'int',
            'u64': 'uint64_t',
            'f32': 'float',
        }[self.type]

    def parse_tuple_format(self):
        return {
            'i': 'i',
            'u64': 'k',
            'f32': 'f',
        }[self.type]

    def GenerateConversion(self, w):
        w.Write(f'{self.py_val()} = Py_BuildValue('
                f'"{self.parse_tuple_format()}", {self.cpp_val()});')


class ObjCopyRetval(RetVal):
    def __init__(self, type):
        self.type = type

    def cpp_type(self):
        return f'const {self.type.cpp_name}&'

    def ret_val(self):
        return f'&{self.py_val()}->ob_base'

    def GenerateDeclaration(self, w):
        w.Write(f'{self.type.object_struct_name()} *{self.py_val()};')

    def GenerateConversion(self, w):
        w.Write(f'{self.py_val()} = PyObject_New('
                f'{self.type.object_struct_name()}, '
                f'&{self.type.type_object_name()});')
        w.Write(f'{self.py_val()}->value = '
                f'new {self.type.cpp_name}({self.cpp_val()});')


class ObjOwnerRetval(RetVal):
    def __init__(self, type):
        self.type = type

    def cpp_type(self):
        return f'std::unique_ptr<{self.type.cpp_name}>'

    def ret_val(self):
        return f'&{self.py_val()}->ob_base'

    def GenerateDeclaration(self, w):
        w.Write(f'{self.type.object_struct_name()} *{self.py_val()};')

    def GenerateConversion(self, w):
        w.Write(f'{self.py_val()} = PyObject_New('
                f'{self.type.object_struct_name()}, '
                f'&{self.type.type_object_name()});')
        w.Write(f'{self.py_val()}->value = ' f'{self.cpp_val()}.release();')


class ObjTupleRetVal(RetVal):
    def __init__(self, type):
        self.type = type

    def cpp_type(self):
        return f'std::vector<std::unique_ptr<{self.type.cpp_name}>>'

    def GenerateConversion(self, w):
        w.Write(f'{self.py_val()} = PyTuple_New({self.cpp_val()}.size());')
        w.Open(f'for (size_t i = 0; i < {self.cpp_val()}.size(); ++i) {{')
        w.Write(f'{self.type.object_struct_name()}* tmp = PyObject_New('
                f'{self.type.object_struct_name()}, '
                f'&{self.type.type_object_name()});')
        w.Write(f'tmp->value = {self.cpp_val()}[i].release();')
        w.Write(f'PyTuple_SetItem({self.py_val()}, i, &tmp->ob_base);')
        w.Close('}')


class IntegralTupleRetVal(RetVal):
    def __init__(self, type):
        self.type = type

    def cpp_item_type(self):
        return {
            'i': 'int',
            'u64': 'uint64_t',
            'f32': 'float',
        }[self.type]

    def parse_tuple_format(self):
        return {
            'i': 'i',
            'u64': 'k',
            'f32': 'f',
        }[self.type]

    def cpp_type(self):
        return f'std::vector<{self.cpp_item_type()}>'

    def GenerateConversion(self, w):
        w.Write(f'{self.py_val()} = PyTuple_New({self.cpp_val()}.size());')
        w.Open(f'for (size_t i = 0; i < {self.cpp_val()}.size(); ++i) {{')
        w.Write(f'PyTuple_SetItem({self.py_val()}, i, Py_BuildValue('
                f'"{self.parse_tuple_format()}", {self.cpp_val()}[i]));')
        w.Close('}')

```

`scripts/pybind/writer.py`:

```py
#  This file is part of Leela Chess Zero.
#  Copyright (C) 2020 The LCZero Authors
#
#  Leela Chess is free software: you can redistribute it and/or modify
#  it under the terms of the GNU General Public License as published by
#  the Free Software Foundation, either version 3 of the License, or
#  (at your option) any later version.
#
#  Leela Chess is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
#
#  Additional permission under GNU GPL version 3 section 7
#
#  If you modify this Program, or any covered work, by linking or
#  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
#  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
#  modified version of those libraries), containing parts covered by the
#  terms of the respective license agreement, the licensors of this
#  Program grant you additional permission to convey the resulting work.


class Writer:
    '''A helper class for writing file line by line with indent.'''
    def __init__(self, fo):
        self.fo = fo
        self.indent = 0

    def Indent(self):
        self.indent += 2

    def Unindent(self):
        self.indent -= 2

    def Write(self, text):
        for line in text.split('\n'):
            if line:
                self.fo.write(' ' * self.indent + line + '\n')
            else:
                self.fo.write('\n')

    def Open(self, text='{'):
        self.Write(text)
        self.Indent()

    def Close(self, text='}'):
        self.Unindent()
        self.Write(text)

```

`src/benchmark/backendbench.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020-2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "benchmark/backendbench.h"

#include "chess/board.h"
#include "mcts/node.h"
#include "neural/factory.h"
#include "utils/optionsparser.h"

namespace lczero {
namespace {
const int kDefaultThreads = 1;

const OptionId kThreadsOptionId{"threads", "Threads",
                                "Number of (CPU) worker threads to use.", 't'};
const OptionId kBatchesId{"batches", "",
                          "Number of batches to run as a benchmark."};
const OptionId kStartBatchSizeId{"start-batch-size", "",
                                 "Start benchmark from this batch size."};
const OptionId kMaxBatchSizeId{"max-batch-size", "",
                               "Maximum batch size to benchmark."};
const OptionId kBatchStepId{"batch-step", "",
                            "Step of batch size in benchmark."};
const OptionId kFenId{"fen", "", "Benchmark initial position FEN."};

const OptionId kClippyId{"clippy", "", "Enable helpful assistant."};

const OptionId kClippyThresholdId{"clippy-threshold", "",
                                  "Ratio of nps improvement necessary for each "
                                  "doubling of batchsize to be considered "
                                  "best."};

void Clippy(std::string msg) {
  std::cout << "  __" << std::endl;
  std::cout << " /  \\" << std::endl;
  std::cout << " |  |" << std::endl;
  std::cout << " +  +    " << std::string(msg.length() + 2, '_') << std::endl;
  std::cout << "(@)(@) _|" << std::string(msg.length() + 2, ' ') << '|'
            << std::endl;
  std::cout << " |  |  \\  " << msg << " |" << std::endl;
  std::cout << " || |/  |" << std::string(msg.length() + 2, '_') << '|'
            << std::endl;
  std::cout << " || ||" << std::endl;
  std::cout << " |\\_/|" << std::endl;
  std::cout << " \\___/" << std::endl;
}
}  // namespace

void BackendBenchmark::Run() {
  OptionsParser options;
  NetworkFactory::PopulateOptions(&options);
  options.Add<IntOption>(kThreadsOptionId, 1, 128) = kDefaultThreads;

  options.Add<IntOption>(kBatchesId, 1, 999999999) = 100;
  options.Add<IntOption>(kStartBatchSizeId, 1, 1024) = 1;
  options.Add<IntOption>(kMaxBatchSizeId, 1, 1024) = 256;
  options.Add<IntOption>(kBatchStepId, 1, 256) = 1;
  options.Add<StringOption>(kFenId) = ChessBoard::kStartposFen;
  options.Add<BoolOption>(kClippyId) = false;
  options.Add<FloatOption>(kClippyThresholdId, 0.0f, 1.0f) = 0.15f;

  if (!options.ProcessAllFlags()) return;

  try {
    auto option_dict = options.GetOptionsDict();

    auto network = NetworkFactory::LoadNetwork(option_dict);

    NodeTree tree;
    tree.ResetToPosition(option_dict.Get<std::string>(kFenId), {});

    // Do any backend initialization outside the loop.
    auto warmup = network->NewComputation();
    warmup->AddInput(EncodePositionForNN(
        network->GetCapabilities().input_format, tree.GetPositionHistory(), 8,
        FillEmptyHistory::ALWAYS, nullptr));
    warmup->ComputeBlocking();

    const int batches = option_dict.Get<int>(kBatchesId);

    int best = 1;
    float best_nps = 0.0f;
    std::optional<std::chrono::time_point<std::chrono::steady_clock>> pending;

    for (int i = option_dict.Get<int>(kStartBatchSizeId);
         i <= option_dict.Get<int>(kMaxBatchSizeId);
         i += option_dict.Get<int>(kBatchStepId)) {
      const auto start = std::chrono::steady_clock::now();
      // TODO: support threads not equal to 1 to be able to more sensibly test
      // multiplexing backend.
      for (int j = 0; j < batches; j++) {
        // Put i copies of tree root node into computation and compute.
        auto computation = network->NewComputation();
        for (int k = 0; k < i; k++) {
          computation->AddInput(EncodePositionForNN(
              network->GetCapabilities().input_format,
              tree.GetPositionHistory(), 8, FillEmptyHistory::ALWAYS, nullptr));
        }
        computation->ComputeBlocking();
      }

      const auto end = std::chrono::steady_clock::now();
      std::chrono::duration<double> time = end - start;
      const auto nps = i * batches / time.count();
      std::cout << "Benchmark batch size " << i
                << " with inference average time "
                << time.count() / batches * 1000 << "ms - throughput " << nps
                << " nps." << std::endl;

      if (option_dict.Get<bool>(kClippyId)) {
        const float threshold = option_dict.Get<float>(kClippyThresholdId);

        if (nps > best_nps &&
            threshold * (i - best) * best_nps < (nps - best_nps) * best) {
          best_nps = nps;
          best = i;
          if (!pending) {
            pending = std::chrono::steady_clock::now();
          }
        }
        if (pending) {
          time = std::chrono::steady_clock::now() - *pending;
          if (time.count() > 10) {
            Clippy(
                std::to_string(best) +
                " looks like the best minibatch-size for this net (so far).");
            pending.reset();
          }
        }
      }
    }
    if (option_dict.Get<bool>(kClippyId)) {
      Clippy(std::to_string(best) +
             " looks like the best minibatch-size for this net.");
    }
  } catch (Exception& ex) {
    std::cerr << ex.what() << std::endl;
  }
}
}  // namespace lczero

```

`src/benchmark/backendbench.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

namespace lczero {

class BackendBenchmark {
 public:
  BackendBenchmark() = default;

  void Run();
};

}  // namespace lczero

```

`src/benchmark/benchmark.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "benchmark/benchmark.h"

#include <numeric>

#include "mcts/search.h"
#include "mcts/stoppers/factory.h"
#include "mcts/stoppers/stoppers.h"

namespace lczero {
namespace {
const int kDefaultThreads = 2;

const OptionId kThreadsOptionId{"threads", "Threads",
                                "Number of (CPU) worker threads to use.", 't'};
const OptionId kNodesId{"nodes", "", "Number of nodes to run as a benchmark."};
const OptionId kMovetimeId{"movetime", "",
                           "Benchmark time allocation, in milliseconds."};
const OptionId kFenId{"fen", "", "Benchmark position FEN."};
const OptionId kNumPositionsId{"num-positions", "",
                               "The number of benchmark positions to test."};
}  // namespace

void Benchmark::Run() {
  OptionsParser options;
  NetworkFactory::PopulateOptions(&options);
  options.Add<IntOption>(kThreadsOptionId, 1, 128) = kDefaultThreads;
  options.Add<IntOption>(kNNCacheSizeId, 0, 999999999) = 200000;
  SearchParams::Populate(&options);

  options.Add<IntOption>(kNodesId, -1, 999999999) = -1;
  options.Add<IntOption>(kMovetimeId, -1, 999999999) = 10000;
  options.Add<StringOption>(kFenId) = "";
  options.Add<IntOption>(kNumPositionsId, 1, 34) = 34;

  if (!options.ProcessAllFlags()) return;

  try {
    auto option_dict = options.GetOptionsDict();

    auto network = NetworkFactory::LoadNetwork(option_dict);

    const int visits = option_dict.Get<int>(kNodesId);
    const int movetime = option_dict.Get<int>(kMovetimeId);
    const std::string fen = option_dict.Get<std::string>(kFenId);
    int num_positions = option_dict.Get<int>(kNumPositionsId);

    std::vector<std::double_t> times;
    std::vector<std::int64_t> playouts;
    std::uint64_t cnt = 1;

    if (fen.length() > 0) {
      positions = {fen};
      num_positions = 1;
    }
    std::vector<std::string> testing_positions(
        positions.cbegin(), positions.cbegin() + num_positions);

    for (std::string position : testing_positions) {
      std::cout << "\nPosition: " << cnt++ << "/" << testing_positions.size()
                << " " << position << std::endl;

      auto stopper = std::make_unique<ChainedSearchStopper>();
      if (movetime > -1) {
        stopper->AddStopper(std::make_unique<TimeLimitStopper>(movetime));
      }
      if (visits > -1) {
        stopper->AddStopper(std::make_unique<VisitsStopper>(visits, false));
      }

      NNCache cache;
      cache.SetCapacity(option_dict.Get<int>(kNNCacheSizeId));

      NodeTree tree;
      tree.ResetToPosition(position, {});

      const auto start = std::chrono::steady_clock::now();
      auto search = std::make_unique<Search>(
          tree, network.get(),
          std::make_unique<CallbackUciResponder>(
              std::bind(&Benchmark::OnBestMove, this, std::placeholders::_1),
              std::bind(&Benchmark::OnInfo, this, std::placeholders::_1)),
          MoveList(), start, std::move(stopper), false, option_dict, &cache,
          nullptr);
      search->StartThreads(option_dict.Get<int>(kThreadsOptionId));
      search->Wait();
      const auto end = std::chrono::steady_clock::now();

      const auto time =
          std::chrono::duration_cast<std::chrono::milliseconds>(end - start);
      times.push_back(time.count());
      playouts.push_back(search->GetTotalPlayouts());
    }

    const auto total_playouts =
        std::accumulate(playouts.begin(), playouts.end(), 0);
    const auto total_time = std::accumulate(times.begin(), times.end(), 0);
    std::cout << "\n==========================="
              << "\nTotal time (ms) : " << total_time
              << "\nNodes searched  : " << total_playouts
              << "\nNodes/second    : "
              << std::lround(1000.0 * total_playouts / (total_time + 1))
              << std::endl;
  } catch (Exception& ex) {
    std::cerr << ex.what() << std::endl;
  }
}

void Benchmark::OnBestMove(const BestMoveInfo& move) {
  std::cout << "bestmove " << move.bestmove.as_string() << std::endl;
}

void Benchmark::OnInfo(const std::vector<ThinkingInfo>& infos) {
  std::string line = "Benchmark time " + std::to_string(infos[0].time);
  line += " ms, " + std::to_string(infos[0].nodes) + " nodes, ";
  line += std::to_string(infos[0].nps) + " nps";
  if (!infos[0].pv.empty()) line += ", move " + infos[0].pv[0].as_string();
  std::cout << line << std::endl;
}

}  // namespace lczero

```

`src/benchmark/benchmark.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include "mcts/search.h"
#include "neural/cache.h"
#include "neural/factory.h"
#include "utils/optionsparser.h"

namespace lczero {

class Benchmark{
 public:
  Benchmark() = default;

  // Same positions as Stockfish uses.
  std::vector<std::string> positions = {
      "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1",
      "r3k2r/p1ppqpb1/bn2pnp1/3PN3/1p2P3/2N2Q1p/PPPBBPPP/R3K2R w KQkq - 0 10",
      "8/2p5/3p4/KP5r/1R3p1k/8/4P1P1/8 w - - 0 11",
      "4rrk1/pp1n3p/3q2pQ/2p1pb2/2PP4/2P3N1/P2B2PP/4RRK1 b - - 7 19",
      "rq3rk1/ppp2ppp/1bnpb3/3N2B1/3NP3/7P/PPPQ1PP1/2KR3R w - - 7 14 moves "
      "d4e6",
      "r1bq1r1k/1pp1n1pp/1p1p4/4p2Q/4Pp2/1BNP4/PPP2PPP/3R1RK1 w - - 2 14 moves "
      "g2g4",
      "r3r1k1/2p2ppp/p1p1bn2/8/1q2P3/2NPQN2/PPP3PP/R4RK1 b - - 2 15",
      "r1bbk1nr/pp3p1p/2n5/1N4p1/2Np1B2/8/PPP2PPP/2KR1B1R w kq - 0 13",
      "r1bq1rk1/ppp1nppp/4n3/3p3Q/3P4/1BP1B3/PP1N2PP/R4RK1 w - - 1 16",
      "4r1k1/r1q2ppp/ppp2n2/4P3/5Rb1/1N1BQ3/PPP3PP/R5K1 w - - 1 17",
      "2rqkb1r/ppp2p2/2npb1p1/1N1Nn2p/2P1PP2/8/PP2B1PP/R1BQK2R b KQ - 0 11",
      "r1bq1r1k/b1p1npp1/p2p3p/1p6/3PP3/1B2NN2/PP3PPP/R2Q1RK1 w - - 1 16",
      "3r1rk1/p5pp/bpp1pp2/8/q1PP1P2/b3P3/P2NQRPP/1R2B1K1 b - - 6 22",
      "r1q2rk1/2p1bppp/2Pp4/p6b/Q1PNp3/4B3/PP1R1PPP/2K4R w - - 2 18",
      "4k2r/1pb2ppp/1p2p3/1R1p4/3P4/2r1PN2/P4PPP/1R4K1 b - - 3 22",
      "3q2k1/pb3p1p/4pbp1/2r5/PpN2N2/1P2P2P/5PP1/Q2R2K1 b - - 4 26",
      "6k1/6p1/6Pp/ppp5/3pn2P/1P3K2/1PP2P2/3N4 b - - 0 1",
      "3b4/5kp1/1p1p1p1p/pP1PpP1P/P1P1P3/3KN3/8/8 w - - 0 1",
      "2K5/p7/7P/5pR1/8/5k2/r7/8 w - - 0 1 moves g5g6 f3e3 g6g5 e3f3",
      "8/6pk/1p6/8/PP3p1p/5P2/4KP1q/3Q4 w - - 0 1",
      "7k/3p2pp/4q3/8/4Q3/5Kp1/P6b/8 w - - 0 1",
      "8/2p5/8/2kPKp1p/2p4P/2P5/3P4/8 w - - 0 1",
      "8/1p3pp1/7p/5P1P/2k3P1/8/2K2P2/8 w - - 0 1",
      "8/pp2r1k1/2p1p3/3pP2p/1P1P1P1P/P5KR/8/8 w - - 0 1",
      "8/3p4/p1bk3p/Pp6/1Kp1PpPp/2P2P1P/2P5/5B2 b - - 0 1",
      "5k2/7R/4P2p/5K2/p1r2P1p/8/8/8 b - - 0 1",
      "6k1/6p1/P6p/r1N5/5p2/7P/1b3PP1/4R1K1 w - - 0 1",
      "1r3k2/4q3/2Pp3b/3Bp3/2Q2p2/1p1P2P1/1P2KP2/3N4 w - - 0 1",
      "6k1/4pp1p/3p2p1/P1pPb3/R7/1r2P1PP/3B1P2/6K1 w - - 0 1",
      "8/3p3B/5p2/5P2/p7/PP5b/k7/6K1 w - - 0 1",
      "5rk1/q6p/2p3bR/1pPp1rP1/1P1Pp3/P3B1Q1/1K3P2/R7 w - - 93 90",
      "4rrk1/1p1nq3/p7/2p1P1pp/3P2bp/3Q1Bn1/PPPB4/1K2R1NR w - - 40 21",
      "r3k2r/3nnpbp/q2pp1p1/p7/Pp1PPPP1/4BNN1/1P5P/R2Q1RK1 w kq - 0 16",
      "3Qb1k1/1r2ppb1/pN1n2q1/Pp1Pp1Pr/4P2p/4BP2/4B1R1/1R5K b - - 11 40"
  };

  void Run();
  void OnBestMove(const BestMoveInfo& move);
  void OnInfo(const std::vector<ThinkingInfo>& infos);
};

}  // namespace lczero

```

`src/chess/bitboard.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "chess/bitboard.h"

#include "utils/exception.h"

namespace lczero {

namespace {

const Move kIdxToMove[] = {
    "a1b1",  "a1c1",  "a1d1",  "a1e1",  "a1f1",  "a1g1",  "a1h1",  "a1a2",
    "a1b2",  "a1c2",  "a1a3",  "a1b3",  "a1c3",  "a1a4",  "a1d4",  "a1a5",
    "a1e5",  "a1a6",  "a1f6",  "a1a7",  "a1g7",  "a1a8",  "a1h8",  "b1a1",
    "b1c1",  "b1d1",  "b1e1",  "b1f1",  "b1g1",  "b1h1",  "b1a2",  "b1b2",
    "b1c2",  "b1d2",  "b1a3",  "b1b3",  "b1c3",  "b1d3",  "b1b4",  "b1e4",
    "b1b5",  "b1f5",  "b1b6",  "b1g6",  "b1b7",  "b1h7",  "b1b8",  "c1a1",
    "c1b1",  "c1d1",  "c1e1",  "c1f1",  "c1g1",  "c1h1",  "c1a2",  "c1b2",
    "c1c2",  "c1d2",  "c1e2",  "c1a3",  "c1b3",  "c1c3",  "c1d3",  "c1e3",
    "c1c4",  "c1f4",  "c1c5",  "c1g5",  "c1c6",  "c1h6",  "c1c7",  "c1c8",
    "d1a1",  "d1b1",  "d1c1",  "d1e1",  "d1f1",  "d1g1",  "d1h1",  "d1b2",
    "d1c2",  "d1d2",  "d1e2",  "d1f2",  "d1b3",  "d1c3",  "d1d3",  "d1e3",
    "d1f3",  "d1a4",  "d1d4",  "d1g4",  "d1d5",  "d1h5",  "d1d6",  "d1d7",
    "d1d8",  "e1a1",  "e1b1",  "e1c1",  "e1d1",  "e1f1",  "e1g1",  "e1h1",
    "e1c2",  "e1d2",  "e1e2",  "e1f2",  "e1g2",  "e1c3",  "e1d3",  "e1e3",
    "e1f3",  "e1g3",  "e1b4",  "e1e4",  "e1h4",  "e1a5",  "e1e5",  "e1e6",
    "e1e7",  "e1e8",  "f1a1",  "f1b1",  "f1c1",  "f1d1",  "f1e1",  "f1g1",
    "f1h1",  "f1d2",  "f1e2",  "f1f2",  "f1g2",  "f1h2",  "f1d3",  "f1e3",
    "f1f3",  "f1g3",  "f1h3",  "f1c4",  "f1f4",  "f1b5",  "f1f5",  "f1a6",
    "f1f6",  "f1f7",  "f1f8",  "g1a1",  "g1b1",  "g1c1",  "g1d1",  "g1e1",
    "g1f1",  "g1h1",  "g1e2",  "g1f2",  "g1g2",  "g1h2",  "g1e3",  "g1f3",
    "g1g3",  "g1h3",  "g1d4",  "g1g4",  "g1c5",  "g1g5",  "g1b6",  "g1g6",
    "g1a7",  "g1g7",  "g1g8",  "h1a1",  "h1b1",  "h1c1",  "h1d1",  "h1e1",
    "h1f1",  "h1g1",  "h1f2",  "h1g2",  "h1h2",  "h1f3",  "h1g3",  "h1h3",
    "h1e4",  "h1h4",  "h1d5",  "h1h5",  "h1c6",  "h1h6",  "h1b7",  "h1h7",
    "h1a8",  "h1h8",  "a2a1",  "a2b1",  "a2c1",  "a2b2",  "a2c2",  "a2d2",
    "a2e2",  "a2f2",  "a2g2",  "a2h2",  "a2a3",  "a2b3",  "a2c3",  "a2a4",
    "a2b4",  "a2c4",  "a2a5",  "a2d5",  "a2a6",  "a2e6",  "a2a7",  "a2f7",
    "a2a8",  "a2g8",  "b2a1",  "b2b1",  "b2c1",  "b2d1",  "b2a2",  "b2c2",
    "b2d2",  "b2e2",  "b2f2",  "b2g2",  "b2h2",  "b2a3",  "b2b3",  "b2c3",
    "b2d3",  "b2a4",  "b2b4",  "b2c4",  "b2d4",  "b2b5",  "b2e5",  "b2b6",
    "b2f6",  "b2b7",  "b2g7",  "b2b8",  "b2h8",  "c2a1",  "c2b1",  "c2c1",
    "c2d1",  "c2e1",  "c2a2",  "c2b2",  "c2d2",  "c2e2",  "c2f2",  "c2g2",
    "c2h2",  "c2a3",  "c2b3",  "c2c3",  "c2d3",  "c2e3",  "c2a4",  "c2b4",
    "c2c4",  "c2d4",  "c2e4",  "c2c5",  "c2f5",  "c2c6",  "c2g6",  "c2c7",
    "c2h7",  "c2c8",  "d2b1",  "d2c1",  "d2d1",  "d2e1",  "d2f1",  "d2a2",
    "d2b2",  "d2c2",  "d2e2",  "d2f2",  "d2g2",  "d2h2",  "d2b3",  "d2c3",
    "d2d3",  "d2e3",  "d2f3",  "d2b4",  "d2c4",  "d2d4",  "d2e4",  "d2f4",
    "d2a5",  "d2d5",  "d2g5",  "d2d6",  "d2h6",  "d2d7",  "d2d8",  "e2c1",
    "e2d1",  "e2e1",  "e2f1",  "e2g1",  "e2a2",  "e2b2",  "e2c2",  "e2d2",
    "e2f2",  "e2g2",  "e2h2",  "e2c3",  "e2d3",  "e2e3",  "e2f3",  "e2g3",
    "e2c4",  "e2d4",  "e2e4",  "e2f4",  "e2g4",  "e2b5",  "e2e5",  "e2h5",
    "e2a6",  "e2e6",  "e2e7",  "e2e8",  "f2d1",  "f2e1",  "f2f1",  "f2g1",
    "f2h1",  "f2a2",  "f2b2",  "f2c2",  "f2d2",  "f2e2",  "f2g2",  "f2h2",
    "f2d3",  "f2e3",  "f2f3",  "f2g3",  "f2h3",  "f2d4",  "f2e4",  "f2f4",
    "f2g4",  "f2h4",  "f2c5",  "f2f5",  "f2b6",  "f2f6",  "f2a7",  "f2f7",
    "f2f8",  "g2e1",  "g2f1",  "g2g1",  "g2h1",  "g2a2",  "g2b2",  "g2c2",
    "g2d2",  "g2e2",  "g2f2",  "g2h2",  "g2e3",  "g2f3",  "g2g3",  "g2h3",
    "g2e4",  "g2f4",  "g2g4",  "g2h4",  "g2d5",  "g2g5",  "g2c6",  "g2g6",
    "g2b7",  "g2g7",  "g2a8",  "g2g8",  "h2f1",  "h2g1",  "h2h1",  "h2a2",
    "h2b2",  "h2c2",  "h2d2",  "h2e2",  "h2f2",  "h2g2",  "h2f3",  "h2g3",
    "h2h3",  "h2f4",  "h2g4",  "h2h4",  "h2e5",  "h2h5",  "h2d6",  "h2h6",
    "h2c7",  "h2h7",  "h2b8",  "h2h8",  "a3a1",  "a3b1",  "a3c1",  "a3a2",
    "a3b2",  "a3c2",  "a3b3",  "a3c3",  "a3d3",  "a3e3",  "a3f3",  "a3g3",
    "a3h3",  "a3a4",  "a3b4",  "a3c4",  "a3a5",  "a3b5",  "a3c5",  "a3a6",
    "a3d6",  "a3a7",  "a3e7",  "a3a8",  "a3f8",  "b3a1",  "b3b1",  "b3c1",
    "b3d1",  "b3a2",  "b3b2",  "b3c2",  "b3d2",  "b3a3",  "b3c3",  "b3d3",
    "b3e3",  "b3f3",  "b3g3",  "b3h3",  "b3a4",  "b3b4",  "b3c4",  "b3d4",
    "b3a5",  "b3b5",  "b3c5",  "b3d5",  "b3b6",  "b3e6",  "b3b7",  "b3f7",
    "b3b8",  "b3g8",  "c3a1",  "c3b1",  "c3c1",  "c3d1",  "c3e1",  "c3a2",
    "c3b2",  "c3c2",  "c3d2",  "c3e2",  "c3a3",  "c3b3",  "c3d3",  "c3e3",
    "c3f3",  "c3g3",  "c3h3",  "c3a4",  "c3b4",  "c3c4",  "c3d4",  "c3e4",
    "c3a5",  "c3b5",  "c3c5",  "c3d5",  "c3e5",  "c3c6",  "c3f6",  "c3c7",
    "c3g7",  "c3c8",  "c3h8",  "d3b1",  "d3c1",  "d3d1",  "d3e1",  "d3f1",
    "d3b2",  "d3c2",  "d3d2",  "d3e2",  "d3f2",  "d3a3",  "d3b3",  "d3c3",
    "d3e3",  "d3f3",  "d3g3",  "d3h3",  "d3b4",  "d3c4",  "d3d4",  "d3e4",
    "d3f4",  "d3b5",  "d3c5",  "d3d5",  "d3e5",  "d3f5",  "d3a6",  "d3d6",
    "d3g6",  "d3d7",  "d3h7",  "d3d8",  "e3c1",  "e3d1",  "e3e1",  "e3f1",
    "e3g1",  "e3c2",  "e3d2",  "e3e2",  "e3f2",  "e3g2",  "e3a3",  "e3b3",
    "e3c3",  "e3d3",  "e3f3",  "e3g3",  "e3h3",  "e3c4",  "e3d4",  "e3e4",
    "e3f4",  "e3g4",  "e3c5",  "e3d5",  "e3e5",  "e3f5",  "e3g5",  "e3b6",
    "e3e6",  "e3h6",  "e3a7",  "e3e7",  "e3e8",  "f3d1",  "f3e1",  "f3f1",
    "f3g1",  "f3h1",  "f3d2",  "f3e2",  "f3f2",  "f3g2",  "f3h2",  "f3a3",
    "f3b3",  "f3c3",  "f3d3",  "f3e3",  "f3g3",  "f3h3",  "f3d4",  "f3e4",
    "f3f4",  "f3g4",  "f3h4",  "f3d5",  "f3e5",  "f3f5",  "f3g5",  "f3h5",
    "f3c6",  "f3f6",  "f3b7",  "f3f7",  "f3a8",  "f3f8",  "g3e1",  "g3f1",
    "g3g1",  "g3h1",  "g3e2",  "g3f2",  "g3g2",  "g3h2",  "g3a3",  "g3b3",
    "g3c3",  "g3d3",  "g3e3",  "g3f3",  "g3h3",  "g3e4",  "g3f4",  "g3g4",
    "g3h4",  "g3e5",  "g3f5",  "g3g5",  "g3h5",  "g3d6",  "g3g6",  "g3c7",
    "g3g7",  "g3b8",  "g3g8",  "h3f1",  "h3g1",  "h3h1",  "h3f2",  "h3g2",
    "h3h2",  "h3a3",  "h3b3",  "h3c3",  "h3d3",  "h3e3",  "h3f3",  "h3g3",
    "h3f4",  "h3g4",  "h3h4",  "h3f5",  "h3g5",  "h3h5",  "h3e6",  "h3h6",
    "h3d7",  "h3h7",  "h3c8",  "h3h8",  "a4a1",  "a4d1",  "a4a2",  "a4b2",
    "a4c2",  "a4a3",  "a4b3",  "a4c3",  "a4b4",  "a4c4",  "a4d4",  "a4e4",
    "a4f4",  "a4g4",  "a4h4",  "a4a5",  "a4b5",  "a4c5",  "a4a6",  "a4b6",
    "a4c6",  "a4a7",  "a4d7",  "a4a8",  "a4e8",  "b4b1",  "b4e1",  "b4a2",
    "b4b2",  "b4c2",  "b4d2",  "b4a3",  "b4b3",  "b4c3",  "b4d3",  "b4a4",
    "b4c4",  "b4d4",  "b4e4",  "b4f4",  "b4g4",  "b4h4",  "b4a5",  "b4b5",
    "b4c5",  "b4d5",  "b4a6",  "b4b6",  "b4c6",  "b4d6",  "b4b7",  "b4e7",
    "b4b8",  "b4f8",  "c4c1",  "c4f1",  "c4a2",  "c4b2",  "c4c2",  "c4d2",
    "c4e2",  "c4a3",  "c4b3",  "c4c3",  "c4d3",  "c4e3",  "c4a4",  "c4b4",
    "c4d4",  "c4e4",  "c4f4",  "c4g4",  "c4h4",  "c4a5",  "c4b5",  "c4c5",
    "c4d5",  "c4e5",  "c4a6",  "c4b6",  "c4c6",  "c4d6",  "c4e6",  "c4c7",
    "c4f7",  "c4c8",  "c4g8",  "d4a1",  "d4d1",  "d4g1",  "d4b2",  "d4c2",
    "d4d2",  "d4e2",  "d4f2",  "d4b3",  "d4c3",  "d4d3",  "d4e3",  "d4f3",
    "d4a4",  "d4b4",  "d4c4",  "d4e4",  "d4f4",  "d4g4",  "d4h4",  "d4b5",
    "d4c5",  "d4d5",  "d4e5",  "d4f5",  "d4b6",  "d4c6",  "d4d6",  "d4e6",
    "d4f6",  "d4a7",  "d4d7",  "d4g7",  "d4d8",  "d4h8",  "e4b1",  "e4e1",
    "e4h1",  "e4c2",  "e4d2",  "e4e2",  "e4f2",  "e4g2",  "e4c3",  "e4d3",
    "e4e3",  "e4f3",  "e4g3",  "e4a4",  "e4b4",  "e4c4",  "e4d4",  "e4f4",
    "e4g4",  "e4h4",  "e4c5",  "e4d5",  "e4e5",  "e4f5",  "e4g5",  "e4c6",
    "e4d6",  "e4e6",  "e4f6",  "e4g6",  "e4b7",  "e4e7",  "e4h7",  "e4a8",
    "e4e8",  "f4c1",  "f4f1",  "f4d2",  "f4e2",  "f4f2",  "f4g2",  "f4h2",
    "f4d3",  "f4e3",  "f4f3",  "f4g3",  "f4h3",  "f4a4",  "f4b4",  "f4c4",
    "f4d4",  "f4e4",  "f4g4",  "f4h4",  "f4d5",  "f4e5",  "f4f5",  "f4g5",
    "f4h5",  "f4d6",  "f4e6",  "f4f6",  "f4g6",  "f4h6",  "f4c7",  "f4f7",
    "f4b8",  "f4f8",  "g4d1",  "g4g1",  "g4e2",  "g4f2",  "g4g2",  "g4h2",
    "g4e3",  "g4f3",  "g4g3",  "g4h3",  "g4a4",  "g4b4",  "g4c4",  "g4d4",
    "g4e4",  "g4f4",  "g4h4",  "g4e5",  "g4f5",  "g4g5",  "g4h5",  "g4e6",
    "g4f6",  "g4g6",  "g4h6",  "g4d7",  "g4g7",  "g4c8",  "g4g8",  "h4e1",
    "h4h1",  "h4f2",  "h4g2",  "h4h2",  "h4f3",  "h4g3",  "h4h3",  "h4a4",
    "h4b4",  "h4c4",  "h4d4",  "h4e4",  "h4f4",  "h4g4",  "h4f5",  "h4g5",
    "h4h5",  "h4f6",  "h4g6",  "h4h6",  "h4e7",  "h4h7",  "h4d8",  "h4h8",
    "a5a1",  "a5e1",  "a5a2",  "a5d2",  "a5a3",  "a5b3",  "a5c3",  "a5a4",
    "a5b4",  "a5c4",  "a5b5",  "a5c5",  "a5d5",  "a5e5",  "a5f5",  "a5g5",
    "a5h5",  "a5a6",  "a5b6",  "a5c6",  "a5a7",  "a5b7",  "a5c7",  "a5a8",
    "a5d8",  "b5b1",  "b5f1",  "b5b2",  "b5e2",  "b5a3",  "b5b3",  "b5c3",
    "b5d3",  "b5a4",  "b5b4",  "b5c4",  "b5d4",  "b5a5",  "b5c5",  "b5d5",
    "b5e5",  "b5f5",  "b5g5",  "b5h5",  "b5a6",  "b5b6",  "b5c6",  "b5d6",
    "b5a7",  "b5b7",  "b5c7",  "b5d7",  "b5b8",  "b5e8",  "c5c1",  "c5g1",
    "c5c2",  "c5f2",  "c5a3",  "c5b3",  "c5c3",  "c5d3",  "c5e3",  "c5a4",
    "c5b4",  "c5c4",  "c5d4",  "c5e4",  "c5a5",  "c5b5",  "c5d5",  "c5e5",
    "c5f5",  "c5g5",  "c5h5",  "c5a6",  "c5b6",  "c5c6",  "c5d6",  "c5e6",
    "c5a7",  "c5b7",  "c5c7",  "c5d7",  "c5e7",  "c5c8",  "c5f8",  "d5d1",
    "d5h1",  "d5a2",  "d5d2",  "d5g2",  "d5b3",  "d5c3",  "d5d3",  "d5e3",
    "d5f3",  "d5b4",  "d5c4",  "d5d4",  "d5e4",  "d5f4",  "d5a5",  "d5b5",
    "d5c5",  "d5e5",  "d5f5",  "d5g5",  "d5h5",  "d5b6",  "d5c6",  "d5d6",
    "d5e6",  "d5f6",  "d5b7",  "d5c7",  "d5d7",  "d5e7",  "d5f7",  "d5a8",
    "d5d8",  "d5g8",  "e5a1",  "e5e1",  "e5b2",  "e5e2",  "e5h2",  "e5c3",
    "e5d3",  "e5e3",  "e5f3",  "e5g3",  "e5c4",  "e5d4",  "e5e4",  "e5f4",
    "e5g4",  "e5a5",  "e5b5",  "e5c5",  "e5d5",  "e5f5",  "e5g5",  "e5h5",
    "e5c6",  "e5d6",  "e5e6",  "e5f6",  "e5g6",  "e5c7",  "e5d7",  "e5e7",
    "e5f7",  "e5g7",  "e5b8",  "e5e8",  "e5h8",  "f5b1",  "f5f1",  "f5c2",
    "f5f2",  "f5d3",  "f5e3",  "f5f3",  "f5g3",  "f5h3",  "f5d4",  "f5e4",
    "f5f4",  "f5g4",  "f5h4",  "f5a5",  "f5b5",  "f5c5",  "f5d5",  "f5e5",
    "f5g5",  "f5h5",  "f5d6",  "f5e6",  "f5f6",  "f5g6",  "f5h6",  "f5d7",
    "f5e7",  "f5f7",  "f5g7",  "f5h7",  "f5c8",  "f5f8",  "g5c1",  "g5g1",
    "g5d2",  "g5g2",  "g5e3",  "g5f3",  "g5g3",  "g5h3",  "g5e4",  "g5f4",
    "g5g4",  "g5h4",  "g5a5",  "g5b5",  "g5c5",  "g5d5",  "g5e5",  "g5f5",
    "g5h5",  "g5e6",  "g5f6",  "g5g6",  "g5h6",  "g5e7",  "g5f7",  "g5g7",
    "g5h7",  "g5d8",  "g5g8",  "h5d1",  "h5h1",  "h5e2",  "h5h2",  "h5f3",
    "h5g3",  "h5h3",  "h5f4",  "h5g4",  "h5h4",  "h5a5",  "h5b5",  "h5c5",
    "h5d5",  "h5e5",  "h5f5",  "h5g5",  "h5f6",  "h5g6",  "h5h6",  "h5f7",
    "h5g7",  "h5h7",  "h5e8",  "h5h8",  "a6a1",  "a6f1",  "a6a2",  "a6e2",
    "a6a3",  "a6d3",  "a6a4",  "a6b4",  "a6c4",  "a6a5",  "a6b5",  "a6c5",
    "a6b6",  "a6c6",  "a6d6",  "a6e6",  "a6f6",  "a6g6",  "a6h6",  "a6a7",
    "a6b7",  "a6c7",  "a6a8",  "a6b8",  "a6c8",  "b6b1",  "b6g1",  "b6b2",
    "b6f2",  "b6b3",  "b6e3",  "b6a4",  "b6b4",  "b6c4",  "b6d4",  "b6a5",
    "b6b5",  "b6c5",  "b6d5",  "b6a6",  "b6c6",  "b6d6",  "b6e6",  "b6f6",
    "b6g6",  "b6h6",  "b6a7",  "b6b7",  "b6c7",  "b6d7",  "b6a8",  "b6b8",
    "b6c8",  "b6d8",  "c6c1",  "c6h1",  "c6c2",  "c6g2",  "c6c3",  "c6f3",
    "c6a4",  "c6b4",  "c6c4",  "c6d4",  "c6e4",  "c6a5",  "c6b5",  "c6c5",
    "c6d5",  "c6e5",  "c6a6",  "c6b6",  "c6d6",  "c6e6",  "c6f6",  "c6g6",
    "c6h6",  "c6a7",  "c6b7",  "c6c7",  "c6d7",  "c6e7",  "c6a8",  "c6b8",
    "c6c8",  "c6d8",  "c6e8",  "d6d1",  "d6d2",  "d6h2",  "d6a3",  "d6d3",
    "d6g3",  "d6b4",  "d6c4",  "d6d4",  "d6e4",  "d6f4",  "d6b5",  "d6c5",
    "d6d5",  "d6e5",  "d6f5",  "d6a6",  "d6b6",  "d6c6",  "d6e6",  "d6f6",
    "d6g6",  "d6h6",  "d6b7",  "d6c7",  "d6d7",  "d6e7",  "d6f7",  "d6b8",
    "d6c8",  "d6d8",  "d6e8",  "d6f8",  "e6e1",  "e6a2",  "e6e2",  "e6b3",
    "e6e3",  "e6h3",  "e6c4",  "e6d4",  "e6e4",  "e6f4",  "e6g4",  "e6c5",
    "e6d5",  "e6e5",  "e6f5",  "e6g5",  "e6a6",  "e6b6",  "e6c6",  "e6d6",
    "e6f6",  "e6g6",  "e6h6",  "e6c7",  "e6d7",  "e6e7",  "e6f7",  "e6g7",
    "e6c8",  "e6d8",  "e6e8",  "e6f8",  "e6g8",  "f6a1",  "f6f1",  "f6b2",
    "f6f2",  "f6c3",  "f6f3",  "f6d4",  "f6e4",  "f6f4",  "f6g4",  "f6h4",
    "f6d5",  "f6e5",  "f6f5",  "f6g5",  "f6h5",  "f6a6",  "f6b6",  "f6c6",
    "f6d6",  "f6e6",  "f6g6",  "f6h6",  "f6d7",  "f6e7",  "f6f7",  "f6g7",
    "f6h7",  "f6d8",  "f6e8",  "f6f8",  "f6g8",  "f6h8",  "g6b1",  "g6g1",
    "g6c2",  "g6g2",  "g6d3",  "g6g3",  "g6e4",  "g6f4",  "g6g4",  "g6h4",
    "g6e5",  "g6f5",  "g6g5",  "g6h5",  "g6a6",  "g6b6",  "g6c6",  "g6d6",
    "g6e6",  "g6f6",  "g6h6",  "g6e7",  "g6f7",  "g6g7",  "g6h7",  "g6e8",
    "g6f8",  "g6g8",  "g6h8",  "h6c1",  "h6h1",  "h6d2",  "h6h2",  "h6e3",
    "h6h3",  "h6f4",  "h6g4",  "h6h4",  "h6f5",  "h6g5",  "h6h5",  "h6a6",
    "h6b6",  "h6c6",  "h6d6",  "h6e6",  "h6f6",  "h6g6",  "h6f7",  "h6g7",
    "h6h7",  "h6f8",  "h6g8",  "h6h8",  "a7a1",  "a7g1",  "a7a2",  "a7f2",
    "a7a3",  "a7e3",  "a7a4",  "a7d4",  "a7a5",  "a7b5",  "a7c5",  "a7a6",
    "a7b6",  "a7c6",  "a7b7",  "a7c7",  "a7d7",  "a7e7",  "a7f7",  "a7g7",
    "a7h7",  "a7a8",  "a7b8",  "a7c8",  "b7b1",  "b7h1",  "b7b2",  "b7g2",
    "b7b3",  "b7f3",  "b7b4",  "b7e4",  "b7a5",  "b7b5",  "b7c5",  "b7d5",
    "b7a6",  "b7b6",  "b7c6",  "b7d6",  "b7a7",  "b7c7",  "b7d7",  "b7e7",
    "b7f7",  "b7g7",  "b7h7",  "b7a8",  "b7b8",  "b7c8",  "b7d8",  "c7c1",
    "c7c2",  "c7h2",  "c7c3",  "c7g3",  "c7c4",  "c7f4",  "c7a5",  "c7b5",
    "c7c5",  "c7d5",  "c7e5",  "c7a6",  "c7b6",  "c7c6",  "c7d6",  "c7e6",
    "c7a7",  "c7b7",  "c7d7",  "c7e7",  "c7f7",  "c7g7",  "c7h7",  "c7a8",
    "c7b8",  "c7c8",  "c7d8",  "c7e8",  "d7d1",  "d7d2",  "d7d3",  "d7h3",
    "d7a4",  "d7d4",  "d7g4",  "d7b5",  "d7c5",  "d7d5",  "d7e5",  "d7f5",
    "d7b6",  "d7c6",  "d7d6",  "d7e6",  "d7f6",  "d7a7",  "d7b7",  "d7c7",
    "d7e7",  "d7f7",  "d7g7",  "d7h7",  "d7b8",  "d7c8",  "d7d8",  "d7e8",
    "d7f8",  "e7e1",  "e7e2",  "e7a3",  "e7e3",  "e7b4",  "e7e4",  "e7h4",
    "e7c5",  "e7d5",  "e7e5",  "e7f5",  "e7g5",  "e7c6",  "e7d6",  "e7e6",
    "e7f6",  "e7g6",  "e7a7",  "e7b7",  "e7c7",  "e7d7",  "e7f7",  "e7g7",
    "e7h7",  "e7c8",  "e7d8",  "e7e8",  "e7f8",  "e7g8",  "f7f1",  "f7a2",
    "f7f2",  "f7b3",  "f7f3",  "f7c4",  "f7f4",  "f7d5",  "f7e5",  "f7f5",
    "f7g5",  "f7h5",  "f7d6",  "f7e6",  "f7f6",  "f7g6",  "f7h6",  "f7a7",
    "f7b7",  "f7c7",  "f7d7",  "f7e7",  "f7g7",  "f7h7",  "f7d8",  "f7e8",
    "f7f8",  "f7g8",  "f7h8",  "g7a1",  "g7g1",  "g7b2",  "g7g2",  "g7c3",
    "g7g3",  "g7d4",  "g7g4",  "g7e5",  "g7f5",  "g7g5",  "g7h5",  "g7e6",
    "g7f6",  "g7g6",  "g7h6",  "g7a7",  "g7b7",  "g7c7",  "g7d7",  "g7e7",
    "g7f7",  "g7h7",  "g7e8",  "g7f8",  "g7g8",  "g7h8",  "h7b1",  "h7h1",
    "h7c2",  "h7h2",  "h7d3",  "h7h3",  "h7e4",  "h7h4",  "h7f5",  "h7g5",
    "h7h5",  "h7f6",  "h7g6",  "h7h6",  "h7a7",  "h7b7",  "h7c7",  "h7d7",
    "h7e7",  "h7f7",  "h7g7",  "h7f8",  "h7g8",  "h7h8",  "a8a1",  "a8h1",
    "a8a2",  "a8g2",  "a8a3",  "a8f3",  "a8a4",  "a8e4",  "a8a5",  "a8d5",
    "a8a6",  "a8b6",  "a8c6",  "a8a7",  "a8b7",  "a8c7",  "a8b8",  "a8c8",
    "a8d8",  "a8e8",  "a8f8",  "a8g8",  "a8h8",  "b8b1",  "b8b2",  "b8h2",
    "b8b3",  "b8g3",  "b8b4",  "b8f4",  "b8b5",  "b8e5",  "b8a6",  "b8b6",
    "b8c6",  "b8d6",  "b8a7",  "b8b7",  "b8c7",  "b8d7",  "b8a8",  "b8c8",
    "b8d8",  "b8e8",  "b8f8",  "b8g8",  "b8h8",  "c8c1",  "c8c2",  "c8c3",
    "c8h3",  "c8c4",  "c8g4",  "c8c5",  "c8f5",  "c8a6",  "c8b6",  "c8c6",
    "c8d6",  "c8e6",  "c8a7",  "c8b7",  "c8c7",  "c8d7",  "c8e7",  "c8a8",
    "c8b8",  "c8d8",  "c8e8",  "c8f8",  "c8g8",  "c8h8",  "d8d1",  "d8d2",
    "d8d3",  "d8d4",  "d8h4",  "d8a5",  "d8d5",  "d8g5",  "d8b6",  "d8c6",
    "d8d6",  "d8e6",  "d8f6",  "d8b7",  "d8c7",  "d8d7",  "d8e7",  "d8f7",
    "d8a8",  "d8b8",  "d8c8",  "d8e8",  "d8f8",  "d8g8",  "d8h8",  "e8e1",
    "e8e2",  "e8e3",  "e8a4",  "e8e4",  "e8b5",  "e8e5",  "e8h5",  "e8c6",
    "e8d6",  "e8e6",  "e8f6",  "e8g6",  "e8c7",  "e8d7",  "e8e7",  "e8f7",
    "e8g7",  "e8a8",  "e8b8",  "e8c8",  "e8d8",  "e8f8",  "e8g8",  "e8h8",
    "f8f1",  "f8f2",  "f8a3",  "f8f3",  "f8b4",  "f8f4",  "f8c5",  "f8f5",
    "f8d6",  "f8e6",  "f8f6",  "f8g6",  "f8h6",  "f8d7",  "f8e7",  "f8f7",
    "f8g7",  "f8h7",  "f8a8",  "f8b8",  "f8c8",  "f8d8",  "f8e8",  "f8g8",
    "f8h8",  "g8g1",  "g8a2",  "g8g2",  "g8b3",  "g8g3",  "g8c4",  "g8g4",
    "g8d5",  "g8g5",  "g8e6",  "g8f6",  "g8g6",  "g8h6",  "g8e7",  "g8f7",
    "g8g7",  "g8h7",  "g8a8",  "g8b8",  "g8c8",  "g8d8",  "g8e8",  "g8f8",
    "g8h8",  "h8a1",  "h8h1",  "h8b2",  "h8h2",  "h8c3",  "h8h3",  "h8d4",
    "h8h4",  "h8e5",  "h8h5",  "h8f6",  "h8g6",  "h8h6",  "h8f7",  "h8g7",
    "h8h7",  "h8a8",  "h8b8",  "h8c8",  "h8d8",  "h8e8",  "h8f8",  "h8g8",
    "a7a8q", "a7a8r", "a7a8b", "a7b8q", "a7b8r", "a7b8b", "b7a8q", "b7a8r",
    "b7a8b", "b7b8q", "b7b8r", "b7b8b", "b7c8q", "b7c8r", "b7c8b", "c7b8q",
    "c7b8r", "c7b8b", "c7c8q", "c7c8r", "c7c8b", "c7d8q", "c7d8r", "c7d8b",
    "d7c8q", "d7c8r", "d7c8b", "d7d8q", "d7d8r", "d7d8b", "d7e8q", "d7e8r",
    "d7e8b", "e7d8q", "e7d8r", "e7d8b", "e7e8q", "e7e8r", "e7e8b", "e7f8q",
    "e7f8r", "e7f8b", "f7e8q", "f7e8r", "f7e8b", "f7f8q", "f7f8r", "f7f8b",
    "f7g8q", "f7g8r", "f7g8b", "g7f8q", "g7f8r", "g7f8b", "g7g8q", "g7g8r",
    "g7g8b", "g7h8q", "g7h8r", "g7h8b", "h7g8q", "h7g8r", "h7g8b", "h7h8q",
    "h7h8r", "h7h8b"};

std::vector<unsigned short> BuildMoveIndices() {
  std::vector<unsigned short> res(4 * 64 * 64);
  for (size_t i = 0; i < sizeof(kIdxToMove) / sizeof(kIdxToMove[0]); ++i) {
    res[kIdxToMove[i].as_packed_int()] = i;
  }
  return res;
}

const std::vector<unsigned short> kMoveToIdx = BuildMoveIndices();
const int kKingCastleIndex =
    kMoveToIdx[BoardSquare("e1").as_int() * 64 + BoardSquare("h1").as_int()];
const int kQueenCastleIndex =
    kMoveToIdx[BoardSquare("e1").as_int() * 64 + BoardSquare("a1").as_int()];

BoardSquare Transform(BoardSquare sq, int transform) {
  if ((transform & FlipTransform) != 0) {
    sq.set(sq.row(), 7 - sq.col());
  }
  if ((transform & MirrorTransform) != 0) {
    sq.set(7 - sq.row(), sq.col());
  }
  if ((transform & TransposeTransform) != 0) {
    sq.set(7 - sq.col(), 7 - sq.row());
  }
  return sq;
}
}  // namespace

Move::Move(const std::string& str, bool black) {
  if (str.size() < 4) throw Exception("Bad move: " + str);
  SetFrom(BoardSquare(str.substr(0, 2), black));
  SetTo(BoardSquare(str.substr(2, 2), black));
  if (str.size() != 4) {
    if (str.size() != 5) throw Exception("Bad move: " + str);
    switch (str[4]) {
      case 'q':
      case 'Q':
        SetPromotion(Promotion::Queen);
        break;
      case 'r':
      case 'R':
        SetPromotion(Promotion::Rook);
        break;
      case 'b':
      case 'B':
        SetPromotion(Promotion::Bishop);
        break;
      case 'n':
      case 'N':
        SetPromotion(Promotion::Knight);
        break;
      default:
        throw Exception("Bad move: " + str);
    }
  }
}

uint16_t Move::as_packed_int() const {
  if (promotion() == Promotion::Knight) {
    return from().as_int() * 64 + to().as_int();
  } else {
    return static_cast<int>(promotion()) * 64 * 64 + from().as_int() * 64 +
           to().as_int();
  }
}

uint16_t Move::as_nn_index(int transform) const {
  if (transform == 0) {
    return kMoveToIdx[as_packed_int()];
  }
  Move transformed = *this;
  transformed.SetTo(Transform(to(), transform));
  transformed.SetFrom(Transform(from(), transform));
  return transformed.as_nn_index(0);
}

}  // namespace lczero

```

`src/chess/bitboard.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <cassert>
#include <cstdint>
#include <string>
#include <vector>

#include "utils/bititer.h"

namespace lczero {

// Stores a coordinates of a single square.
class BoardSquare {
 public:
  constexpr BoardSquare() {}
  // As a single number, 0 to 63, bottom to top, left to right.
  // 0 is a1, 8 is a2, 63 is h8.
  constexpr BoardSquare(std::uint8_t num) : square_(num) {}
  // From row(bottom to top), and col(left to right), 0-based.
  constexpr BoardSquare(int row, int col) : BoardSquare(row * 8 + col) {}
  // From Square name, e.g e4. Only lowercase.
  BoardSquare(const std::string& str, bool black = false)
      : BoardSquare(black ? '8' - str[1] : str[1] - '1', str[0] - 'a') {}
  constexpr std::uint8_t as_int() const { return square_; }
  constexpr std::uint64_t as_board() const { return 1ULL << square_; }
  void set(int row, int col) { square_ = row * 8 + col; }

  // 0-based, bottom to top.
  int row() const { return square_ / 8; }
  // 0-based, left to right.
  int col() const { return square_ % 8; }

  // Row := 7 - row.  Col remains the same.
  void Mirror() { square_ = square_ ^ 0b111000; }

  // Checks whether coordinate is within 0..7.
  static bool IsValidCoord(int x) { return x >= 0 && x < 8; }

  // Checks whether coordinates are within 0..7.
  static bool IsValid(int row, int col) {
    return IsValidCoord(row) && IsValidCoord(col);
  }

  constexpr bool operator==(const BoardSquare& other) const {
    return square_ == other.square_;
  }

  constexpr bool operator!=(const BoardSquare& other) const {
    return square_ != other.square_;
  }

  // Returns the square in algebraic notation (e.g. "e4").
  std::string as_string() const {
    return std::string(1, 'a' + col()) + std::string(1, '1' + row());
  }

 private:
  std::uint8_t square_ = 0;  // Only lower six bits should be set.
};

// Represents a board as an array of 64 bits.
// Bit enumeration goes from bottom to top, from left to right:
// Square a1 is bit 0, square a8 is bit 7, square b1 is bit 8.
class BitBoard {
 public:
  constexpr BitBoard(std::uint64_t board) : board_(board) {}
  BitBoard() = default;
  BitBoard(const BitBoard&) = default;
  BitBoard& operator=(const BitBoard&) = default;

  std::uint64_t as_int() const { return board_; }
  void clear() { board_ = 0; }

  // Counts the number of set bits in the BitBoard.
  int count() const {
#if defined(NO_POPCNT)
    std::uint64_t x = board_;
    x -= (x >> 1) & 0x5555555555555555;
    x = (x & 0x3333333333333333) + ((x >> 2) & 0x3333333333333333);
    x = (x + (x >> 4)) & 0x0F0F0F0F0F0F0F0F;
    return (x * 0x0101010101010101) >> 56;
#elif defined(_MSC_VER) && defined(_WIN64)
    return _mm_popcnt_u64(board_);
#elif defined(_MSC_VER)
    return __popcnt(board_) + __popcnt(board_ >> 32);
#else
    return __builtin_popcountll(board_);
#endif
  }

  // Like count() but using algorithm faster on a very sparse BitBoard.
  // May be slower for more than 4 set bits, but still correct.
  // Useful when counting bits in a Q, R, N or B BitBoard.
  int count_few() const {
#if defined(NO_POPCNT)
    std::uint64_t x = board_;
    int count;
    for (count = 0; x != 0; ++count) {
      // Clear the rightmost set bit.
      x &= x - 1;
    }
    return count;
#else
    return count();
#endif
  }

  // Sets the value for given square to 1 if cond is true.
  // Otherwise does nothing (doesn't reset!).
  void set_if(BoardSquare square, bool cond) { set_if(square.as_int(), cond); }
  void set_if(std::uint8_t pos, bool cond) {
    board_ |= (std::uint64_t(cond) << pos);
  }
  void set_if(int row, int col, bool cond) {
    set_if(BoardSquare(row, col), cond);
  }

  // Sets value of given square to 1.
  void set(BoardSquare square) { set(square.as_int()); }
  void set(std::uint8_t pos) { board_ |= (std::uint64_t(1) << pos); }
  void set(int row, int col) { set(BoardSquare(row, col)); }

  // Sets value of given square to 0.
  void reset(BoardSquare square) { reset(square.as_int()); }
  void reset(std::uint8_t pos) { board_ &= ~(std::uint64_t(1) << pos); }
  void reset(int row, int col) { reset(BoardSquare(row, col)); }

  // Gets value of a square.
  bool get(BoardSquare square) const { return get(square.as_int()); }
  bool get(std::uint8_t pos) const {
    return board_ & (std::uint64_t(1) << pos);
  }
  bool get(int row, int col) const { return get(BoardSquare(row, col)); }

  // Returns whether all bits of a board are set to 0.
  bool empty() const { return board_ == 0; }

  // Checks whether two bitboards have common bits set.
  bool intersects(const BitBoard& other) const { return board_ & other.board_; }

  // Flips black and white side of a board.
  void Mirror() { board_ = ReverseBytesInBytes(board_); }

  bool operator==(const BitBoard& other) const {
    return board_ == other.board_;
  }

  bool operator!=(const BitBoard& other) const {
    return board_ != other.board_;
  }

  BitIterator<BoardSquare> begin() const { return board_; }
  BitIterator<BoardSquare> end() const { return 0; }

  std::string DebugString() const {
    std::string res;
    for (int i = 7; i >= 0; --i) {
      for (int j = 0; j < 8; ++j) {
        if (get(i, j))
          res += '#';
        else
          res += '.';
      }
      res += '\n';
    }
    return res;
  }

  // Applies a mask to the bitboard (intersects).
  BitBoard& operator&=(const BitBoard& a) {
    board_ &= a.board_;
    return *this;
  }

  friend void swap(BitBoard& a, BitBoard& b) {
    using std::swap;
    swap(a.board_, b.board_);
  }

  // Returns union (bitwise OR) of two boards.
  friend BitBoard operator|(const BitBoard& a, const BitBoard& b) {
    return {a.board_ | b.board_};
  }

  // Returns intersection (bitwise AND) of two boards.
  friend BitBoard operator&(const BitBoard& a, const BitBoard& b) {
    return {a.board_ & b.board_};
  }

  // Returns bitboard with one bit reset.
  friend BitBoard operator-(const BitBoard& a, const BoardSquare& b) {
    return {a.board_ & ~b.as_board()};
  }

  // Returns difference (bitwise AND-NOT) of two boards.
  friend BitBoard operator-(const BitBoard& a, const BitBoard& b) {
    return {a.board_ & ~b.board_};
  }

 private:
  std::uint64_t board_ = 0;
};

class Move {
 public:
  enum class Promotion : std::uint8_t { None, Queen, Rook, Bishop, Knight };
  Move() = default;
  constexpr Move(BoardSquare from, BoardSquare to)
      : data_(to.as_int() + (from.as_int() << 6)) {}
  constexpr Move(BoardSquare from, BoardSquare to, Promotion promotion)
      : data_(to.as_int() + (from.as_int() << 6) +
              (static_cast<uint8_t>(promotion) << 12)) {}
  Move(const std::string& str, bool black = false);
  Move(const char* str, bool black = false) : Move(std::string(str), black) {}

  BoardSquare to() const { return BoardSquare(data_ & kToMask); }
  BoardSquare from() const { return BoardSquare((data_ & kFromMask) >> 6); }
  Promotion promotion() const { return Promotion((data_ & kPromoMask) >> 12); }

  void SetTo(BoardSquare to) { data_ = (data_ & ~kToMask) | to.as_int(); }
  void SetFrom(BoardSquare from) {
    data_ = (data_ & ~kFromMask) | (from.as_int() << 6);
  }
  void SetPromotion(Promotion promotion) {
    data_ = (data_ & ~kPromoMask) | (static_cast<uint8_t>(promotion) << 12);
  }
  // 0 .. 16384, knight promotion and no promotion is the same.
  uint16_t as_packed_int() const;

  // 0 .. 1857, to use in neural networks.
  // Transform is a bit field which describes a transform to be applied to the
  // the move before converting it to an index.
  uint16_t as_nn_index(int transform) const;

  explicit operator bool() const { return data_ != 0; }
  bool operator==(const Move& other) const { return data_ == other.data_; }

  void Mirror() { data_ ^= 0b111000111000; }

  std::string as_string() const {
    std::string res = from().as_string() + to().as_string();
    switch (promotion()) {
      case Promotion::None:
        return res;
      case Promotion::Queen:
        return res + 'q';
      case Promotion::Rook:
        return res + 'r';
      case Promotion::Bishop:
        return res + 'b';
      case Promotion::Knight:
        return res + 'n';
    }
    assert(false);
    return "Error!";
  }

 private:
  uint16_t data_ = 0;
  // Move, using the following encoding:
  // bits 0..5 "to"-square
  // bits 6..11 "from"-square
  // bits 12..14 promotion value

  enum Masks : uint16_t {
    kToMask = 0b0000000000111111,
    kFromMask = 0b0000111111000000,
    kPromoMask = 0b0111000000000000,
  };
};

using MoveList = std::vector<Move>;

}  // namespace lczero

```

`src/chess/board.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "chess/board.h"

#include <algorithm>
#include <cctype>
#include <cstdlib>
#include <cstring>
#include <sstream>

#include "utils/exception.h"

#if not defined(NO_PEXT)
// Include header for pext instruction.
#include <immintrin.h>
#endif

namespace lczero {

const char* ChessBoard::kStartposFen =
    "rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1";

const ChessBoard ChessBoard::kStartposBoard(ChessBoard::kStartposFen);

const BitBoard ChessBoard::kPawnMask = 0x00FFFFFFFFFFFF00ULL;

void ChessBoard::Clear() {
  std::memset(reinterpret_cast<void*>(this), 0, sizeof(ChessBoard));
}

void ChessBoard::Mirror() {
  our_pieces_.Mirror();
  their_pieces_.Mirror();
  std::swap(our_pieces_, their_pieces_);
  rooks_.Mirror();
  bishops_.Mirror();
  pawns_.Mirror();
  our_king_.Mirror();
  their_king_.Mirror();
  std::swap(our_king_, their_king_);
  castlings_.Mirror();
  flipped_ = !flipped_;
}

namespace {
static const std::pair<int, int> kKingMoves[] = {
    {-1, -1}, {-1, 0}, {-1, 1}, {0, -1}, {0, 1}, {1, -1}, {1, 0}, {1, 1}};

static const std::pair<int, int> kRookDirections[] = {
    {1, 0}, {-1, 0}, {0, 1}, {0, -1}};

static const std::pair<int, int> kBishopDirections[] = {
    {1, 1}, {-1, 1}, {1, -1}, {-1, -1}};

// Which squares can rook attack from every of squares.
static const BitBoard kRookAttacks[] = {
    0x01010101010101FEULL, 0x02020202020202FDULL, 0x04040404040404FBULL,
    0x08080808080808F7ULL, 0x10101010101010EFULL, 0x20202020202020DFULL,
    0x40404040404040BFULL, 0x808080808080807FULL, 0x010101010101FE01ULL,
    0x020202020202FD02ULL, 0x040404040404FB04ULL, 0x080808080808F708ULL,
    0x101010101010EF10ULL, 0x202020202020DF20ULL, 0x404040404040BF40ULL,
    0x8080808080807F80ULL, 0x0101010101FE0101ULL, 0x0202020202FD0202ULL,
    0x0404040404FB0404ULL, 0x0808080808F70808ULL, 0x1010101010EF1010ULL,
    0x2020202020DF2020ULL, 0x4040404040BF4040ULL, 0x80808080807F8080ULL,
    0x01010101FE010101ULL, 0x02020202FD020202ULL, 0x04040404FB040404ULL,
    0x08080808F7080808ULL, 0x10101010EF101010ULL, 0x20202020DF202020ULL,
    0x40404040BF404040ULL, 0x808080807F808080ULL, 0x010101FE01010101ULL,
    0x020202FD02020202ULL, 0x040404FB04040404ULL, 0x080808F708080808ULL,
    0x101010EF10101010ULL, 0x202020DF20202020ULL, 0x404040BF40404040ULL,
    0x8080807F80808080ULL, 0x0101FE0101010101ULL, 0x0202FD0202020202ULL,
    0x0404FB0404040404ULL, 0x0808F70808080808ULL, 0x1010EF1010101010ULL,
    0x2020DF2020202020ULL, 0x4040BF4040404040ULL, 0x80807F8080808080ULL,
    0x01FE010101010101ULL, 0x02FD020202020202ULL, 0x04FB040404040404ULL,
    0x08F7080808080808ULL, 0x10EF101010101010ULL, 0x20DF202020202020ULL,
    0x40BF404040404040ULL, 0x807F808080808080ULL, 0xFE01010101010101ULL,
    0xFD02020202020202ULL, 0xFB04040404040404ULL, 0xF708080808080808ULL,
    0xEF10101010101010ULL, 0xDF20202020202020ULL, 0xBF40404040404040ULL,
    0x7F80808080808080ULL};
// Which squares can bishop attack.
static const BitBoard kBishopAttacks[] = {
    0x8040201008040200ULL, 0x0080402010080500ULL, 0x0000804020110A00ULL,
    0x0000008041221400ULL, 0x0000000182442800ULL, 0x0000010204885000ULL,
    0x000102040810A000ULL, 0x0102040810204000ULL, 0x4020100804020002ULL,
    0x8040201008050005ULL, 0x00804020110A000AULL, 0x0000804122140014ULL,
    0x0000018244280028ULL, 0x0001020488500050ULL, 0x0102040810A000A0ULL,
    0x0204081020400040ULL, 0x2010080402000204ULL, 0x4020100805000508ULL,
    0x804020110A000A11ULL, 0x0080412214001422ULL, 0x0001824428002844ULL,
    0x0102048850005088ULL, 0x02040810A000A010ULL, 0x0408102040004020ULL,
    0x1008040200020408ULL, 0x2010080500050810ULL, 0x4020110A000A1120ULL,
    0x8041221400142241ULL, 0x0182442800284482ULL, 0x0204885000508804ULL,
    0x040810A000A01008ULL, 0x0810204000402010ULL, 0x0804020002040810ULL,
    0x1008050005081020ULL, 0x20110A000A112040ULL, 0x4122140014224180ULL,
    0x8244280028448201ULL, 0x0488500050880402ULL, 0x0810A000A0100804ULL,
    0x1020400040201008ULL, 0x0402000204081020ULL, 0x0805000508102040ULL,
    0x110A000A11204080ULL, 0x2214001422418000ULL, 0x4428002844820100ULL,
    0x8850005088040201ULL, 0x10A000A010080402ULL, 0x2040004020100804ULL,
    0x0200020408102040ULL, 0x0500050810204080ULL, 0x0A000A1120408000ULL,
    0x1400142241800000ULL, 0x2800284482010000ULL, 0x5000508804020100ULL,
    0xA000A01008040201ULL, 0x4000402010080402ULL, 0x0002040810204080ULL,
    0x0005081020408000ULL, 0x000A112040800000ULL, 0x0014224180000000ULL,
    0x0028448201000000ULL, 0x0050880402010000ULL, 0x00A0100804020100ULL,
    0x0040201008040201ULL};
// Which squares can knight attack.
static const BitBoard kKnightAttacks[] = {
    0x0000000000020400ULL, 0x0000000000050800ULL, 0x00000000000A1100ULL,
    0x0000000000142200ULL, 0x0000000000284400ULL, 0x0000000000508800ULL,
    0x0000000000A01000ULL, 0x0000000000402000ULL, 0x0000000002040004ULL,
    0x0000000005080008ULL, 0x000000000A110011ULL, 0x0000000014220022ULL,
    0x0000000028440044ULL, 0x0000000050880088ULL, 0x00000000A0100010ULL,
    0x0000000040200020ULL, 0x0000000204000402ULL, 0x0000000508000805ULL,
    0x0000000A1100110AULL, 0x0000001422002214ULL, 0x0000002844004428ULL,
    0x0000005088008850ULL, 0x000000A0100010A0ULL, 0x0000004020002040ULL,
    0x0000020400040200ULL, 0x0000050800080500ULL, 0x00000A1100110A00ULL,
    0x0000142200221400ULL, 0x0000284400442800ULL, 0x0000508800885000ULL,
    0x0000A0100010A000ULL, 0x0000402000204000ULL, 0x0002040004020000ULL,
    0x0005080008050000ULL, 0x000A1100110A0000ULL, 0x0014220022140000ULL,
    0x0028440044280000ULL, 0x0050880088500000ULL, 0x00A0100010A00000ULL,
    0x0040200020400000ULL, 0x0204000402000000ULL, 0x0508000805000000ULL,
    0x0A1100110A000000ULL, 0x1422002214000000ULL, 0x2844004428000000ULL,
    0x5088008850000000ULL, 0xA0100010A0000000ULL, 0x4020002040000000ULL,
    0x0400040200000000ULL, 0x0800080500000000ULL, 0x1100110A00000000ULL,
    0x2200221400000000ULL, 0x4400442800000000ULL, 0x8800885000000000ULL,
    0x100010A000000000ULL, 0x2000204000000000ULL, 0x0004020000000000ULL,
    0x0008050000000000ULL, 0x00110A0000000000ULL, 0x0022140000000000ULL,
    0x0044280000000000ULL, 0x0088500000000000ULL, 0x0010A00000000000ULL,
    0x0020400000000000ULL};
// Opponent pawn attacks
static const BitBoard kPawnAttacks[] = {
    0x0000000000000200ULL, 0x0000000000000500ULL, 0x0000000000000A00ULL,
    0x0000000000001400ULL, 0x0000000000002800ULL, 0x0000000000005000ULL,
    0x000000000000A000ULL, 0x0000000000004000ULL, 0x0000000000020000ULL,
    0x0000000000050000ULL, 0x00000000000A0000ULL, 0x0000000000140000ULL,
    0x0000000000280000ULL, 0x0000000000500000ULL, 0x0000000000A00000ULL,
    0x0000000000400000ULL, 0x0000000002000000ULL, 0x0000000005000000ULL,
    0x000000000A000000ULL, 0x0000000014000000ULL, 0x0000000028000000ULL,
    0x0000000050000000ULL, 0x00000000A0000000ULL, 0x0000000040000000ULL,
    0x0000000200000000ULL, 0x0000000500000000ULL, 0x0000000A00000000ULL,
    0x0000001400000000ULL, 0x0000002800000000ULL, 0x0000005000000000ULL,
    0x000000A000000000ULL, 0x0000004000000000ULL, 0x0000020000000000ULL,
    0x0000050000000000ULL, 0x00000A0000000000ULL, 0x0000140000000000ULL,
    0x0000280000000000ULL, 0x0000500000000000ULL, 0x0000A00000000000ULL,
    0x0000400000000000ULL, 0x0002000000000000ULL, 0x0005000000000000ULL,
    0x000A000000000000ULL, 0x0014000000000000ULL, 0x0028000000000000ULL,
    0x0050000000000000ULL, 0x00A0000000000000ULL, 0x0040000000000000ULL,
    0x0000000000000000ULL, 0x0000000000000000ULL, 0x0000000000000000ULL,
    0x0000000000000000ULL, 0x0000000000000000ULL, 0x0000000000000000ULL,
    0x0000000000000000ULL, 0x0000000000000000ULL, 0x0000000000000000ULL,
    0x0000000000000000ULL, 0x0000000000000000ULL, 0x0000000000000000ULL,
    0x0000000000000000ULL, 0x0000000000000000ULL, 0x0000000000000000ULL,
    0x0000000000000000ULL};

static const Move::Promotion kPromotions[] = {
    Move::Promotion::Queen,
    Move::Promotion::Rook,
    Move::Promotion::Bishop,
    Move::Promotion::Knight,
};

// Magic bitboard routines and structures.
// We use so-called "fancy" magic bitboards.

// Structure holding all relevant magic parameters per square.
struct MagicParams {
  // Relevant occupancy mask.
  uint64_t mask_;
  // Pointer to lookup table.
  BitBoard* attacks_table_;
#if defined(NO_PEXT)
  // Magic number.
  uint64_t magic_number_;
  // Number of bits to shift.
  uint8_t shift_bits_;
#endif
};

#if defined(NO_PEXT)
// Magic numbers determined via trial and error with random number generator
// such that the number of relevant occupancy bits suffice to index the attacks
// tables with only constructive collisions.
static const BitBoard kRookMagicNumbers[] = {
    0x088000102088C001ULL, 0x10C0200040001000ULL, 0x83001041000B2000ULL,
    0x0680280080041000ULL, 0x488004000A080080ULL, 0x0100180400010002ULL,
    0x040001C401021008ULL, 0x02000C04A980C302ULL, 0x0000800040082084ULL,
    0x5020C00820025000ULL, 0x0001002001044012ULL, 0x0402001020400A00ULL,
    0x00C0800800040080ULL, 0x4028800200040080ULL, 0x00A0804200802500ULL,
    0x8004800040802100ULL, 0x0080004000200040ULL, 0x1082810020400100ULL,
    0x0020004010080040ULL, 0x2004818010042800ULL, 0x0601010008005004ULL,
    0x4600808002001400ULL, 0x0010040009180210ULL, 0x020412000406C091ULL,
    0x040084228000C000ULL, 0x8000810100204000ULL, 0x0084110100402000ULL,
    0x0046001A00204210ULL, 0x2001040080080081ULL, 0x0144020080800400ULL,
    0x0840108400080229ULL, 0x0480308A0000410CULL, 0x0460324002800081ULL,
    0x620080A001804000ULL, 0x2800802000801006ULL, 0x0002809000800800ULL,
    0x4C09040080802800ULL, 0x4808800C00800200ULL, 0x0200311004001802ULL,
    0x0400008402002141ULL, 0x0410800140008020ULL, 0x000080C001050020ULL,
    0x004080204A020010ULL, 0x0224201001010038ULL, 0x0109001108010004ULL,
    0x0282004844020010ULL, 0x8228180110040082ULL, 0x0001000080C10002ULL,
    0x024000C120801080ULL, 0x0001406481060200ULL, 0x0101243200418600ULL,
    0x0108800800100080ULL, 0x4022080100100D00ULL, 0x0000843040600801ULL,
    0x8301000200CC0500ULL, 0x1000004500840200ULL, 0x1100104100800069ULL,
    0x2001008440001021ULL, 0x2002008830204082ULL, 0x0010145000082101ULL,
    0x01A2001004200842ULL, 0x1007000608040041ULL, 0x000A08100203028CULL,
    0x02D4048040290402ULL};
static const BitBoard kBishopMagicNumbers[] = {
    0x0008201802242020ULL, 0x0021040424806220ULL, 0x4006360602013080ULL,
    0x0004410020408002ULL, 0x2102021009001140ULL, 0x08C2021004000001ULL,
    0x6001031120200820ULL, 0x1018310402201410ULL, 0x401CE00210820484ULL,
    0x001029D001004100ULL, 0x2C00101080810032ULL, 0x0000082581000010ULL,
    0x10000A0210110020ULL, 0x200002016C202000ULL, 0x0201018821901000ULL,
    0x006A0300420A2100ULL, 0x0010014005450400ULL, 0x1008C12008028280ULL,
    0x00010010004A0040ULL, 0x3000820802044020ULL, 0x0000800405A02820ULL,
    0x8042004300420240ULL, 0x10060801210D2000ULL, 0x0210840500511061ULL,
    0x0008142118509020ULL, 0x0021109460040104ULL, 0x00A1480090019030ULL,
    0x0102008808008020ULL, 0x884084000880E001ULL, 0x040041020A030100ULL,
    0x3000810104110805ULL, 0x04040A2006808440ULL, 0x0044040404C01100ULL,
    0x4122B80800245004ULL, 0x0044020502380046ULL, 0x0100400888020200ULL,
    0x01C0002060020080ULL, 0x4008811100021001ULL, 0x8208450441040609ULL,
    0x0408004900008088ULL, 0x0294212051220882ULL, 0x000041080810E062ULL,
    0x10480A018E005000ULL, 0x80400A0204201600ULL, 0x2800200204100682ULL,
    0x0020200400204441ULL, 0x0A500600A5002400ULL, 0x801602004A010100ULL,
    0x0801841008040880ULL, 0x10010880C4200028ULL, 0x0400004424040000ULL,
    0x0401000142022100ULL, 0x00A00010020A0002ULL, 0x1010400204010810ULL,
    0x0829910400840000ULL, 0x0004235204010080ULL, 0x1002008143082000ULL,
    0x11840044440C2080ULL, 0x2802A02104030440ULL, 0x6100000900840401ULL,
    0x1C20A15A90420200ULL, 0x0088414004480280ULL, 0x0000204242881100ULL,
    0x0240080802809010ULL};
#endif

// Magic parameters for rooks/bishops.
static MagicParams rook_magic_params[64];
static MagicParams bishop_magic_params[64];

// Precomputed attacks bitboard tables.
static BitBoard rook_attacks_table[102400];
static BitBoard bishop_attacks_table[5248];

// Builds rook or bishop attacks table.
static void BuildAttacksTable(MagicParams* magic_params,
                              BitBoard* attacks_table,
                              const std::pair<int, int>* directions) {
  // Offset into lookup table.
  uint32_t table_offset = 0;

  // Initialize for all board squares.
  for (unsigned square = 0; square < 64; square++) {
    const BoardSquare b_sq(square);

    // Calculate relevant occupancy masks.
    BitBoard mask = {0};

    for (int j = 0; j < 4; j++) {
      auto direction = directions[j];
      auto dst_row = b_sq.row();
      auto dst_col = b_sq.col();
      while (true) {
        dst_row += direction.first;
        dst_col += direction.second;
        // If the next square in this direction is invalid, the current square
        // is at the board's edge and should not be added.
        if (!BoardSquare::IsValid(dst_row + direction.first,
                                  dst_col + direction.second))
          break;
        const BoardSquare destination(dst_row, dst_col);
        mask.set(destination);
      }
    }

    // Set mask.
    magic_params[square].mask_ = mask.as_int();

    // Cache relevant occupancy board squares.
    std::vector<BoardSquare> occupancy_squares;

    for (auto occ_sq : BitBoard(magic_params[square].mask_)) {
      occupancy_squares.emplace_back(occ_sq);
    }

#if defined(NO_PEXT)
    // Set number of shifted bits. The magic numbers have been chosen such that
    // the number of relevant occupancy bits suffice to index the attacks table.
    magic_params[square].shift_bits_ = 64 - occupancy_squares.size();
#endif

    // Set pointer to lookup table.
    magic_params[square].attacks_table_ = &attacks_table[table_offset];

    // Clear attacks table (used for sanity check later on).
    for (int i = 0; i < (1 << occupancy_squares.size()); i++) {
      attacks_table[table_offset + i] = 0;
    }

    // Build square attacks table for every possible relevant occupancy
    // bitboard.
    for (int i = 0; i < (1 << occupancy_squares.size()); i++) {
      BitBoard occupancy(0);

      for (size_t bit = 0; bit < occupancy_squares.size(); bit++) {
        occupancy.set_if(occupancy_squares[bit], (1 << bit) & i);
      }

      // Calculate attacks bitboard corresponding to this occupancy bitboard.
      BitBoard attacks(0);

      for (int j = 0; j < 4; j++) {
        auto direction = directions[j];
        auto dst_row = b_sq.row();
        auto dst_col = b_sq.col();
        while (true) {
          dst_row += direction.first;
          dst_col += direction.second;
          if (!BoardSquare::IsValid(dst_row, dst_col)) break;
          const BoardSquare destination(dst_row, dst_col);
          attacks.set(destination);
          if (occupancy.get(destination)) break;
        }
      }

#if defined(NO_PEXT)
      // Calculate magic index.
      uint64_t index = occupancy.as_int();
      index *= magic_params[square].magic_number_;
      index >>= magic_params[square].shift_bits_;

      // Sanity check. The magic numbers have been chosen such that
      // the number of relevant occupancy bits suffice to index the attacks
      // table. If the table already contains an attacks bitboard, possible
      // collisions should be constructive.
      if (attacks_table[table_offset + index] != 0 &&
          attacks_table[table_offset + index] != attacks) {
        throw Exception("Invalid magic number!");
      }
#else
      uint64_t index =
          _pext_u64(occupancy.as_int(), magic_params[square].mask_);
#endif

      // Update table.
      attacks_table[table_offset + index] = attacks;
    }

    // Update table offset.
    table_offset += (1 << occupancy_squares.size());
  }
}

// Returns the rook attacks bitboard for the given rook board square and the
// given occupied piece bitboard.
static inline BitBoard GetRookAttacks(const BoardSquare rook_square,
                                      const BitBoard pieces) {
  // Calculate magic index.
  const uint8_t square = rook_square.as_int();

#if defined(NO_PEXT)
  uint64_t index = pieces.as_int() & rook_magic_params[square].mask_;
  index *= rook_magic_params[square].magic_number_;
  index >>= rook_magic_params[square].shift_bits_;
#else
  uint64_t index = _pext_u64(pieces.as_int(), rook_magic_params[square].mask_);
#endif

  // Return attacks bitboard.
  return rook_magic_params[square].attacks_table_[index];
}

// Returns the bishop attacks bitboard for the given bishop board square and
// the given occupied piece bitboard.
static inline BitBoard GetBishopAttacks(const BoardSquare bishop_square,
                                        const BitBoard pieces) {
  // Calculate magic index.
  const uint8_t square = bishop_square.as_int();

#if defined(NO_PEXT)
  uint64_t index = pieces.as_int() & bishop_magic_params[square].mask_;
  index *= bishop_magic_params[square].magic_number_;
  index >>= bishop_magic_params[square].shift_bits_;
#else
  uint64_t index =
      _pext_u64(pieces.as_int(), bishop_magic_params[square].mask_);
#endif

  // Return attacks bitboard.
  return bishop_magic_params[square].attacks_table_[index];
}

}  // namespace

void InitializeMagicBitboards() {
#if defined(NO_PEXT)
  // Set magic numbers for all board squares.
  for (unsigned square = 0; square < 64; square++) {
    rook_magic_params[square].magic_number_ =
        kRookMagicNumbers[square].as_int();
    bishop_magic_params[square].magic_number_ =
        kBishopMagicNumbers[square].as_int();
  }
#endif

  // Build attacks tables.
  BuildAttacksTable(rook_magic_params, rook_attacks_table, kRookDirections);
  BuildAttacksTable(bishop_magic_params, bishop_attacks_table,
                    kBishopDirections);
}

MoveList ChessBoard::GeneratePseudolegalMoves() const {
  MoveList result;
  result.reserve(60);
  for (auto source : our_pieces_) {
    // King
    if (source == our_king_) {
      for (const auto& delta : kKingMoves) {
        const auto dst_row = source.row() + delta.first;
        const auto dst_col = source.col() + delta.second;
        if (!BoardSquare::IsValid(dst_row, dst_col)) continue;
        const BoardSquare destination(dst_row, dst_col);
        if (our_pieces_.get(destination)) continue;
        if (IsUnderAttack(destination)) continue;
        result.emplace_back(source, destination);
      }
      // Castlings.
      auto walk_free = [this](int from, int to, int rook, int king) {
        for (int i = from; i <= to; ++i) {
          if (i == rook || i == king) continue;
          if (our_pieces_.get(i) || their_pieces_.get(i)) return false;
        }
        return true;
      };
      // @From may be less or greater than @to. @To is not included in check
      // unless it is the same with @from.
      auto range_attacked = [this](int from, int to) {
        if (from == to) return IsUnderAttack(from);
        const int increment = from < to ? 1 : -1;
        while (from != to) {
          if (IsUnderAttack(from)) return true;
          from += increment;
        }
        return false;
      };
      const uint8_t king = source.col();
      // For castlings we don't check destination king square for checks, it
      // will be done in legal move check phase.
      if (castlings_.we_can_000()) {
        const uint8_t qrook = castlings_.queenside_rook();
        if (walk_free(std::min(static_cast<uint8_t>(C1), qrook),
                      std::max(static_cast<uint8_t>(D1), king), qrook, king) &&
            !range_attacked(king, C1)) {
          result.emplace_back(source,
                              BoardSquare(RANK_1, castlings_.queenside_rook()));
        }
      }
      if (castlings_.we_can_00()) {
        const uint8_t krook = castlings_.kingside_rook();
        if (walk_free(std::min(static_cast<uint8_t>(F1), king),
                      std::max(static_cast<uint8_t>(G1), krook), krook, king) &&
            !range_attacked(king, G1)) {
          result.emplace_back(source,
                              BoardSquare(RANK_1, castlings_.kingside_rook()));
        }
      }
      continue;
    }
    bool processed_piece = false;
    // Rook (and queen)
    if (rooks_.get(source)) {
      processed_piece = true;
      BitBoard attacked =
          GetRookAttacks(source, our_pieces_ | their_pieces_) - our_pieces_;

      for (const auto& destination : attacked) {
        result.emplace_back(source, destination);
      }
    }
    // Bishop (and queen)
    if (bishops_.get(source)) {
      processed_piece = true;
      BitBoard attacked =
          GetBishopAttacks(source, our_pieces_ | their_pieces_) - our_pieces_;

      for (const auto& destination : attacked) {
        result.emplace_back(source, destination);
      }
    }
    if (processed_piece) continue;
    // Pawns.
    if ((pawns_ & kPawnMask).get(source)) {
      // Moves forward.
      {
        const auto dst_row = source.row() + 1;
        const auto dst_col = source.col();
        const BoardSquare destination(dst_row, dst_col);

        if (!our_pieces_.get(destination) && !their_pieces_.get(destination)) {
          if (dst_row != RANK_8) {
            result.emplace_back(source, destination);
            if (dst_row == RANK_3) {
              // Maybe it'll be possible to move two squares.
              if (!our_pieces_.get(RANK_4, dst_col) &&
                  !their_pieces_.get(RANK_4, dst_col)) {
                result.emplace_back(source, BoardSquare(RANK_4, dst_col));
              }
            }
          } else {
            // Promotions
            for (auto promotion : kPromotions) {
              result.emplace_back(source, destination, promotion);
            }
          }
        }
      }
      // Captures.
      {
        for (auto direction : {-1, 1}) {
          const auto dst_row = source.row() + 1;
          const auto dst_col = source.col() + direction;
          if (dst_col < 0 || dst_col >= 8) continue;
          const BoardSquare destination(dst_row, dst_col);
          if (their_pieces_.get(destination)) {
            if (dst_row == RANK_8) {
              // Promotion.
              for (auto promotion : kPromotions) {
                result.emplace_back(source, destination, promotion);
              }
            } else {
              // Ordinary capture.
              result.emplace_back(source, destination);
            }
          } else if (dst_row == RANK_6 && pawns_.get(RANK_8, dst_col)) {
            // En passant.
            // "Pawn" on opponent's file 8 means that en passant is possible.
            // Those fake pawns are reset in ApplyMove.
            result.emplace_back(source, destination);
          }
        }
      }
      continue;
    }
    // Knight.
    {
      for (const auto destination :
           kKnightAttacks[source.as_int()] - our_pieces_) {
        result.emplace_back(source, destination);
      }
    }
  }
  return result;
}  // namespace lczero

bool ChessBoard::ApplyMove(Move move) {
  const auto& from = move.from();
  const auto& to = move.to();
  const auto from_row = from.row();
  const auto from_col = from.col();
  const auto to_row = to.row();
  const auto to_col = to.col();

  // Castlings.
  if (from == our_king_) {
    castlings_.reset_we_can_00();
    castlings_.reset_we_can_000();
    auto do_castling = [this](int king_dst, int rook_src, int rook_dst) {
      // Remove en passant flags.
      pawns_ &= kPawnMask;
      our_pieces_.reset(our_king_);
      our_pieces_.reset(rook_src);
      rooks_.reset(rook_src);
      our_pieces_.set(king_dst);
      our_pieces_.set(rook_dst);
      rooks_.set(rook_dst);
      our_king_ = king_dst;
    };
    if (from_row == RANK_1 && to_row == RANK_1) {
      const auto our_rooks = rooks() & our_pieces_;
      if (our_rooks.get(to)) {
        // Castling.
        if (to_col > from_col) {
          // Kingside.
          do_castling(G1, to.as_int(), F1);
        } else {
          // Queenside.
          do_castling(C1, to.as_int(), D1);
        }
        return false;
      } else if (from_col == FILE_E && to_col == FILE_G) {
        // Non FRC-style e1g1 castling (as opposed to e1h1).
        do_castling(G1, H1, F1);
        return false;
      } else if (from_col == FILE_E && to_col == FILE_C) {
        // Non FRC-style e1c1 castling (as opposed to e1a1).
        do_castling(C1, A1, D1);
        return false;
      }
    }
  }

  // Move in our pieces.
  our_pieces_.reset(from);
  our_pieces_.set(to);

  // Remove captured piece.
  bool reset_50_moves = their_pieces_.get(to);
  their_pieces_.reset(to);
  rooks_.reset(to);
  bishops_.reset(to);
  pawns_.reset(to);
  if (to.as_int() == 56 + castlings_.kingside_rook()) {
    castlings_.reset_they_can_00();
  }
  if (to.as_int() == 56 + castlings_.queenside_rook()) {
    castlings_.reset_they_can_000();
  }

  // En passant.
  if (from_row == RANK_5 && pawns_.get(from) && from_col != to_col &&
      pawns_.get(RANK_8, to_col)) {
    pawns_.reset(RANK_5, to_col);
    their_pieces_.reset(RANK_5, to_col);
  }

  // Remove en passant flags.
  pawns_ &= kPawnMask;

  // If pawn was moved, reset 50 move draw counter.
  reset_50_moves |= pawns_.get(from);

  // King, non-castling move
  if (from == our_king_) {
    our_king_ = to;
    return reset_50_moves;
  }

  // Promotion.
  if (to_row == RANK_8 && pawns_.get(from)) {
    switch (move.promotion()) {
      case Move::Promotion::Rook:
        rooks_.set(to);
        break;
      case Move::Promotion::Bishop:
        bishops_.set(to);
        break;
      case Move::Promotion::Queen:
        rooks_.set(to);
        bishops_.set(to);
        break;
      default:;
    }
    pawns_.reset(from);
    return true;
  }

  // Reset castling rights.
  if (from_row == RANK_1 && rooks_.get(from)) {
    if (from_col == castlings_.queenside_rook()) castlings_.reset_we_can_000();
    if (from_col == castlings_.kingside_rook()) castlings_.reset_we_can_00();
  }

  // Ordinary move.
  rooks_.set_if(to, rooks_.get(from));
  bishops_.set_if(to, bishops_.get(from));
  pawns_.set_if(to, pawns_.get(from));
  rooks_.reset(from);
  bishops_.reset(from);
  pawns_.reset(from);

  // Set en passant flag.
  if (to_row - from_row == 2 && pawns_.get(to)) {
    BoardSquare ep_sq(to_row - 1, to_col);
    if (kPawnAttacks[ep_sq.as_int()].intersects(their_pieces_ & pawns_)) {
      pawns_.set(0, to_col);
    }
  }
  return reset_50_moves;
}

bool ChessBoard::IsUnderAttack(BoardSquare square) const {
  const int row = square.row();
  const int col = square.col();
  // Check king.
  {
    const int krow = their_king_.row();
    const int kcol = their_king_.col();
    if (std::abs(krow - row) <= 1 && std::abs(kcol - col) <= 1) return true;
  }
  // Check rooks (and queens).
  if (GetRookAttacks(square, our_pieces_ | their_pieces_)
          .intersects(their_pieces_ & rooks_)) {
    return true;
  }
  // Check bishops.
  if (GetBishopAttacks(square, our_pieces_ | their_pieces_)
          .intersects(their_pieces_ & bishops_)) {
    return true;
  }
  // Check pawns.
  if (kPawnAttacks[square.as_int()].intersects(their_pieces_ & pawns_)) {
    return true;
  }
  // Check knights.
  {
    if (kKnightAttacks[square.as_int()].intersects(their_pieces_ - their_king_ -
                                                   rooks_ - bishops_ -
                                                   (pawns_ & kPawnMask))) {
      return true;
    }
  }
  return false;
}

bool ChessBoard::IsSameMove(Move move1, Move move2) const {
  // If moves are equal, it's the same move.
  if (move1 == move2) return true;
  // Explicitly check all legacy castling moves. Need to check for king, for
  // e.g. rook e1a1 and e1c1 are different moves.
  if (move1.from() != move2.from() || move1.from() != E1 ||
      our_king_ != move1.from()) {
    return false;
  }
  if (move1.to() == A1 && move2.to() == C1) return true;
  if (move1.to() == C1 && move2.to() == A1) return true;
  if (move1.to() == G1 && move2.to() == H1) return true;
  if (move1.to() == H1 && move2.to() == G1) return true;
  return false;
}

Move ChessBoard::GetLegacyMove(Move move) const {
  if (our_king_ != move.from() || !our_pieces_.get(move.to())) {
    return move;
  }
  if (move == Move(E1, H1)) return Move(E1, G1);
  if (move == Move(E1, A1)) return Move(E1, C1);
  return move;
}

Move ChessBoard::GetModernMove(Move move) const {
  if (our_king_ != E1 || move.from() != E1) return move;
  if (move == Move(E1, G1) && !our_pieces_.get(G1)) return Move(E1, H1);
  if (move == Move(E1, C1) && !our_pieces_.get(C1)) return Move(E1, A1);
  return move;
}

KingAttackInfo ChessBoard::GenerateKingAttackInfo() const {
  KingAttackInfo king_attack_info;

  // Number of attackers that give check (used for double check detection).
  unsigned num_king_attackers = 0;

  const int row = our_king_.row();
  const int col = our_king_.col();
  // King checks are unnecessary, as kings cannot give check.
  // Check rooks (and queens).
  if (kRookAttacks[our_king_.as_int()].intersects(their_pieces_ & rooks_)) {
    for (const auto& direction : kRookDirections) {
      auto dst_row = row;
      auto dst_col = col;
      BitBoard attack_line(0);
      bool possible_pinned_piece_found = false;
      BoardSquare possible_pinned_piece;
      while (true) {
        dst_row += direction.first;
        dst_col += direction.second;
        if (!BoardSquare::IsValid(dst_row, dst_col)) break;
        const BoardSquare destination(dst_row, dst_col);
        if (our_pieces_.get(destination)) {
          if (possible_pinned_piece_found) {
            // No pieces pinned.
            break;
          } else {
            // This is a possible pinned piece.
            possible_pinned_piece_found = true;
            possible_pinned_piece = destination;
          }
        }
        if (!possible_pinned_piece_found) {
          attack_line.set(destination);
        }
        if (their_pieces_.get(destination)) {
          if (rooks_.get(destination)) {
            if (possible_pinned_piece_found) {
              // Store the pinned piece.
              king_attack_info.pinned_pieces_.set(possible_pinned_piece);
            } else {
              // Update attack lines.
              king_attack_info.attack_lines_ =
                  king_attack_info.attack_lines_ | attack_line;
              num_king_attackers++;
            }
          }
          break;
        }
      }
    }
  }
  // Check bishops.
  if (kBishopAttacks[our_king_.as_int()].intersects(their_pieces_ & bishops_)) {
    for (const auto& direction : kBishopDirections) {
      auto dst_row = row;
      auto dst_col = col;
      BitBoard attack_line(0);
      bool possible_pinned_piece_found = false;
      BoardSquare possible_pinned_piece;
      while (true) {
        dst_row += direction.first;
        dst_col += direction.second;
        if (!BoardSquare::IsValid(dst_row, dst_col)) break;
        const BoardSquare destination(dst_row, dst_col);
        if (our_pieces_.get(destination)) {
          if (possible_pinned_piece_found) {
            // No pieces pinned.
            break;
          } else {
            // This is a possible pinned piece.
            possible_pinned_piece_found = true;
            possible_pinned_piece = destination;
          }
        }
        if (!possible_pinned_piece_found) {
          attack_line.set(destination);
        }
        if (their_pieces_.get(destination)) {
          if (bishops_.get(destination)) {
            if (possible_pinned_piece_found) {
              // Store the pinned piece.
              king_attack_info.pinned_pieces_.set(possible_pinned_piece);
            } else {
              // Update attack lines.
              king_attack_info.attack_lines_ =
                  king_attack_info.attack_lines_ | attack_line;
              num_king_attackers++;
            }
          }
          break;
        }
      }
    }
  }
  // Check pawns.
  const BitBoard attacking_pawns =
      kPawnAttacks[our_king_.as_int()] & their_pieces_ & pawns_;
  king_attack_info.attack_lines_ =
      king_attack_info.attack_lines_ | attacking_pawns;

  if (attacking_pawns.as_int()) {
    // No more than one pawn can give check.
    num_king_attackers++;
  }

  // Check knights.
  const BitBoard attacking_knights =
      kKnightAttacks[our_king_.as_int()] &
      (their_pieces_ - their_king_ - rooks_ - bishops_ - (pawns_ & kPawnMask));
  king_attack_info.attack_lines_ =
      king_attack_info.attack_lines_ | attacking_knights;

  if (attacking_knights.as_int()) {
    // No more than one knight can give check.
    num_king_attackers++;
  }

  assert(num_king_attackers <= 2);
  king_attack_info.double_check_ = (num_king_attackers == 2);

  return king_attack_info;
}

bool ChessBoard::IsLegalMove(Move move,
                             const KingAttackInfo& king_attack_info) const {
  const auto& from = move.from();
  const auto& to = move.to();

  // En passant. Complex but rare. Just apply
  // and check that we are not under check.
  if (from.row() == 4 && pawns_.get(from) && from.col() != to.col() &&
      pawns_.get(7, to.col())) {
    ChessBoard board(*this);
    board.ApplyMove(move);
    return !board.IsUnderCheck();
  }

  // Check if we are already under check.
  if (king_attack_info.in_check()) {
    // King move.
    if (from == our_king_) {
      // Just apply and check that we are not under check.
      ChessBoard board(*this);
      board.ApplyMove(move);
      return !board.IsUnderCheck();
    }

    // Pinned pieces can never resolve a check.
    if (king_attack_info.is_pinned(from)) {
      return false;
    }

    // The piece to move is no king and is not pinned.
    if (king_attack_info.in_double_check()) {
      // Only a king move can resolve the double check.
      return false;
    } else {
      // Only one attacking piece gives check.
      // Our piece is free to move (not pinned). Check if the attacker is
      // captured or interposed after the piece has moved to its destination
      // square.
      return king_attack_info.is_on_attack_line(to);
    }
  }

  // King moves.
  if (from == our_king_) {
    if (from.row() != 0 || to.row() != 0 ||
        (abs(from.col() - to.col()) == 1 && !our_pieces_.get(to))) {
      // Non-castling move. Already checked during movegen.
      return true;
    }
    // Checking whether king is under check after castling.
    ChessBoard board(*this);
    board.ApplyMove(move);
    return !board.IsUnderCheck();
  }

  // If we get here, we are not under check.
  // If the piece is not pinned, it is free to move anywhere.
  if (!king_attack_info.is_pinned(from)) return true;

  // The piece is pinned. Now check that it stays on the same line w.r.t. the
  // king.
  const int dx_from = from.col() - our_king_.col();
  const int dy_from = from.row() - our_king_.row();
  const int dx_to = to.col() - our_king_.col();
  const int dy_to = to.row() - our_king_.row();

  if (dx_from == 0 || dx_to == 0) {
    return (dx_from == dx_to);
  } else {
    return (dx_from * dy_to == dx_to * dy_from);
  }
}

MoveList ChessBoard::GenerateLegalMoves() const {
  const KingAttackInfo king_attack_info = GenerateKingAttackInfo();
  MoveList result = GeneratePseudolegalMoves();
  result.erase(
      std::remove_if(result.begin(), result.end(),
                     [&](Move m) { return !IsLegalMove(m, king_attack_info); }),
      result.end());
  return result;
}

void ChessBoard::SetFromFen(std::string fen, int* rule50_ply, int* moves) {
  Clear();
  int row = 7;
  int col = 0;

  // Remove any trailing whitespaces to detect eof after the last field.
  fen.erase(std::find_if(fen.rbegin(), fen.rend(),
                         [](char c) { return !std::isspace(c); })
                .base(),
            fen.end());

  std::istringstream fen_str(fen);
  std::string board;
  fen_str >> board;
  std::string who_to_move = "w";
  if (!fen_str.eof()) fen_str >> who_to_move;
  // Assume no castling rights. Other engines, e.g., Stockfish, assume kings and
  // rooks on their initial rows can each castle with the outer-most rook.  Our
  // implementation currently supports 960 castling where white and black rooks
  // have matching columns, so it's unclear which rights to assume.
  std::string castlings = "-";
  if (!fen_str.eof()) fen_str >> castlings;
  std::string en_passant = "-";
  if (!fen_str.eof()) fen_str >> en_passant;
  int rule50_halfmoves = 0;
  if (!fen_str.eof()) fen_str >> rule50_halfmoves;
  int total_moves = 1;
  if (!fen_str.eof()) fen_str >> total_moves;
  if (!fen_str) throw Exception("Bad fen string: " + fen);

  for (char c : board) {
    if (c == '/') {
      --row;
      if (row < 0) throw Exception("Bad fen string (too many rows): " + fen);
      col = 0;
      continue;
    }
    if (std::isdigit(c)) {
      col += c - '0';
      continue;
    }
    if (col >= 8) throw Exception("Bad fen string (too many columns): " + fen);

    if (std::isupper(c)) {
      // White piece.
      our_pieces_.set(row, col);
    } else {
      // Black piece.
      their_pieces_.set(row, col);
    }

    if (c == 'K') {
      our_king_.set(row, col);
    } else if (c == 'k') {
      their_king_.set(row, col);
    } else if (c == 'R' || c == 'r') {
      rooks_.set(row, col);
    } else if (c == 'B' || c == 'b') {
      bishops_.set(row, col);
    } else if (c == 'Q' || c == 'q') {
      rooks_.set(row, col);
      bishops_.set(row, col);
    } else if (c == 'P' || c == 'p') {
      if (row == 7 || row == 0) {
        throw Exception("Bad fen string (pawn in first/last row): " + fen);
      }
      pawns_.set(row, col);
    } else if (c == 'N' || c == 'n') {
      // Do nothing
    } else {
      throw Exception("Bad fen string: " + fen);
    }
    ++col;
  }

  if (castlings != "-") {
    uint8_t left_rook = FILE_A;
    uint8_t right_rook = FILE_H;
    for (char c : castlings) {
      const bool is_black = std::islower(c);
      const int king_col = (is_black ? their_king_ : our_king_).col();
      if (!is_black) c = std::tolower(c);
      const auto rooks =
          (is_black ? their_pieces_ : our_pieces_) & ChessBoard::rooks();
      if (c == 'k') {
        // Finding rightmost rook.
        for (right_rook = FILE_H; right_rook > king_col; --right_rook) {
          if (rooks.get(is_black ? RANK_8 : RANK_1, right_rook)) break;
        }
        if (right_rook == king_col) {
          throw Exception("Bad fen string (no kingside rook): " + fen);
        }
        if (is_black) {
          castlings_.set_they_can_00();
        } else {
          castlings_.set_we_can_00();
        }
      } else if (c == 'q') {
        // Finding leftmost rook.
        for (left_rook = FILE_A; left_rook < king_col; ++left_rook) {
          if (rooks.get(is_black ? RANK_8 : RANK_1, left_rook)) break;
        }
        if (left_rook == king_col) {
          throw Exception("Bad fen string (no queenside rook): " + fen);
        }
        if (is_black) {
          castlings_.set_they_can_000();
        } else {
          castlings_.set_we_can_000();
        }
      } else if (c >= 'a' && c <= 'h') {
        int rook_col = c - 'a';
        if (rook_col < king_col) {
          left_rook = rook_col;
          if (is_black) {
            castlings_.set_they_can_000();
          } else {
            castlings_.set_we_can_000();
          }
        } else {
          right_rook = rook_col;
          if (is_black) {
            castlings_.set_they_can_00();
          } else {
            castlings_.set_we_can_00();
          }
        }
      } else {
        throw Exception("Bad fen string (unexpected casting symbol): " + fen);
      }
    }
    castlings_.SetRookPositions(left_rook, right_rook);
  }

  if (en_passant != "-") {
    auto square = BoardSquare(en_passant);
    if (square.row() != RANK_3 && square.row() != RANK_6)
      throw Exception("Bad fen string: " + fen + " wrong en passant rank");
    pawns_.set((square.row() == RANK_3) ? RANK_1 : RANK_8, square.col());
  }

  if (who_to_move == "b" || who_to_move == "B") {
    Mirror();
  } else if (who_to_move != "w" && who_to_move != "W") {
    throw Exception("Bad fen string (side to move): " + fen);
  }
  if (rule50_ply) *rule50_ply = rule50_halfmoves;
  if (moves) *moves = total_moves;
}

bool ChessBoard::HasMatingMaterial() const {
  if (!rooks_.empty() || !pawns_.empty()) {
    return true;
  }

  if ((our_pieces_ | their_pieces_).count() < 4) {
    // K v K, K+B v K, K+N v K.
    return false;
  }
  if (!(knights().empty())) {
    return true;
  }

  // Only kings and bishops remain.

  constexpr BitBoard kLightSquares(0x55AA55AA55AA55AAULL);
  constexpr BitBoard kDarkSquares(0xAA55AA55AA55AA55ULL);

  const bool light_bishop = bishops_.intersects(kLightSquares);
  const bool dark_bishop = bishops_.intersects(kDarkSquares);
  return light_bishop && dark_bishop;
}

std::string ChessBoard::DebugString() const {
  std::string result;
  for (int i = 7; i >= 0; --i) {
    for (int j = 0; j < 8; ++j) {
      if (!our_pieces_.get(i, j) && !their_pieces_.get(i, j)) {
        if (i == 2 && pawns_.get(0, j))
          result += '*';
        else if (i == 5 && pawns_.get(7, j))
          result += '*';
        else
          result += '.';
        continue;
      }
      if (our_king_ == i * 8 + j) {
        result += 'K';
        continue;
      }
      if (their_king_ == i * 8 + j) {
        result += 'k';
        continue;
      }
      char c = '?';
      if ((pawns_ & kPawnMask).get(i, j)) {
        c = 'p';
      } else if (bishops_.get(i, j)) {
        if (rooks_.get(i, j))
          c = 'q';
        else
          c = 'b';
      } else if (rooks_.get(i, j)) {
        c = 'r';
      } else {
        c = 'n';
      }
      if (our_pieces_.get(i, j)) c = std::toupper(c);
      result += c;
    }
    if (i == 0) {
      result += " " + castlings_.DebugString();
      result += flipped_ ? " (from black's eyes)" : " (from white's eyes)";
      result += " Hash: " + std::to_string(Hash());
    }
    result += '\n';
  }
  return result;
}

}  // namespace lczero

```

`src/chess/board.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <cassert>
#include <string>

#include "chess/bitboard.h"
#include "utils/hashcat.h"

namespace lczero {

// Initializes internal magic bitboard structures.
void InitializeMagicBitboards();

// Represents king attack info used during legal move detection.
class KingAttackInfo {
 public:
  bool in_check() const { return attack_lines_.as_int(); }
  bool in_double_check() const { return double_check_; }
  bool is_pinned(const BoardSquare square) const {
    return pinned_pieces_.get(square);
  }
  bool is_on_attack_line(const BoardSquare square) const {
    return attack_lines_.get(square);
  }

  bool double_check_ = 0;
  BitBoard pinned_pieces_ = {0};
  BitBoard attack_lines_ = {0};
};

// Represents a board position.
// Unlike most chess engines, the board is mirrored for black.
class ChessBoard {
 public:
  ChessBoard() = default;
  ChessBoard(const std::string& fen) { SetFromFen(fen); }

  static const char* kStartposFen;
  static const ChessBoard kStartposBoard;
  static const BitBoard kPawnMask;

  // Sets position from FEN string.
  // If @rule50_ply and @moves are not nullptr, they are filled with number
  // of moves without capture and number of full moves since the beginning of
  // the game.
  void SetFromFen(std::string fen, int* rule50_ply = nullptr,
                  int* moves = nullptr);
  // Nullifies the whole structure.
  void Clear();
  // Swaps black and white pieces and mirrors them relative to the
  // middle of the board. (what was on rank 1 appears on rank 8, what was
  // on file b remains on file b).
  void Mirror();

  // Generates list of possible moves for "ours" (white), but may leave king
  // under check.
  MoveList GeneratePseudolegalMoves() const;
  // Applies the move. (Only for "ours" (white)). Returns true if 50 moves
  // counter should be removed.
  bool ApplyMove(Move move);
  // Checks if the square is under attack from "theirs" (black).
  bool IsUnderAttack(BoardSquare square) const;
  // Generates the king attack info used for legal move detection.
  KingAttackInfo GenerateKingAttackInfo() const;
  // Checks if "our" (white) king is under check.
  bool IsUnderCheck() const { return IsUnderAttack(our_king_); }

  // Checks whether at least one of the sides has mating material.
  bool HasMatingMaterial() const;
  // Generates legal moves.
  MoveList GenerateLegalMoves() const;
  // Check whether pseudolegal move is legal.
  bool IsLegalMove(Move move, const KingAttackInfo& king_attack_info) const;
  // Returns whether two moves are actually the same move in the position.
  bool IsSameMove(Move move1, Move move2) const;
  // Returns the same move but with castling encoded in legacy way.
  Move GetLegacyMove(Move move) const;
  // Returns the same move but with castling encoded in modern way.
  Move GetModernMove(Move move) const;

  uint64_t Hash() const {
    return HashCat({our_pieces_.as_int(), their_pieces_.as_int(),
                    rooks_.as_int(), bishops_.as_int(), pawns_.as_int(),
                    (static_cast<uint32_t>(our_king_.as_int()) << 24) |
                        (static_cast<uint32_t>(their_king_.as_int()) << 16) |
                        (static_cast<uint32_t>(castlings_.as_int()) << 8) |
                        static_cast<uint32_t>(flipped_)});
  }

  class Castlings {
   public:
    Castlings() : queenside_rook_(0), kingside_rook_(7) {}

    void set_we_can_00() { data_ |= 1; }
    void set_we_can_000() { data_ |= 2; }
    void set_they_can_00() { data_ |= 4; }
    void set_they_can_000() { data_ |= 8; }

    void reset_we_can_00() { data_ &= ~1; }
    void reset_we_can_000() { data_ &= ~2; }
    void reset_they_can_00() { data_ &= ~4; }
    void reset_they_can_000() { data_ &= ~8; }

    bool we_can_00() const { return data_ & 1; }
    bool we_can_000() const { return data_ & 2; }
    bool they_can_00() const { return data_ & 4; }
    bool they_can_000() const { return data_ & 8; }
    bool no_legal_castle() const { return data_ == 0; }

    void Mirror() { data_ = ((data_ & 0b11) << 2) + ((data_ & 0b1100) >> 2); }

    // Note: this is not a strict xfen compatible output. Without access to the
    // board its not possible to know whether there is ambiguity so all cases
    // with any non-standard rook positions are encoded in the x-fen format
    std::string as_string() const {
      if (data_ == 0) return "-";
      std::string result;
      if (queenside_rook() == FILE_A && kingside_rook() == FILE_H) {
        if (we_can_00()) result += 'K';
        if (we_can_000()) result += 'Q';
        if (they_can_00()) result += 'k';
        if (they_can_000()) result += 'q';
      } else {
        if (we_can_00()) result += 'A' + kingside_rook();
        if (we_can_000()) result += 'A' + queenside_rook();
        if (they_can_00()) result += 'a' + kingside_rook();
        if (they_can_000()) result += 'a' + queenside_rook();
      }
      return result;
    }

    std::string DebugString() const {
      std::string result;
      if (data_ == 0) result = "-";
      if (we_can_00()) result += 'K';
      if (we_can_000()) result += 'Q';
      if (they_can_00()) result += 'k';
      if (they_can_000()) result += 'q';
      result += '[';
      result += 'a' + queenside_rook();
      result += 'a' + kingside_rook();
      result += ']';
      return result;
    }

    uint8_t as_int() const { return data_; }

    bool operator==(const Castlings& other) const {
      assert(queenside_rook_ == other.queenside_rook_ &&
             kingside_rook_ == other.kingside_rook_);
      return data_ == other.data_;
    }

    uint8_t queenside_rook() const { return queenside_rook_; }
    uint8_t kingside_rook() const { return kingside_rook_; }
    void SetRookPositions(std::uint8_t left, std::uint8_t right) {
      queenside_rook_ = left;
      kingside_rook_ = right;
    }

   private:
    // Position of "left" (queenside) rook in starting game position.
    std::uint8_t queenside_rook_ : 3;
    // Position of "right" (kingside) rook in starting position.
    std::uint8_t kingside_rook_ : 3;

    // - Bit 0 -- "our" side's kingside castle.
    // - Bit 1 -- "our" side's queenside castle.
    // - Bit 2 -- opponent's side's kingside castle.
    // - Bit 3 -- opponent's side's queenside castle.
    std::uint8_t data_ = 0;
  };

  std::string DebugString() const;

  BitBoard ours() const { return our_pieces_; }
  BitBoard theirs() const { return their_pieces_; }
  BitBoard pawns() const { return pawns_ & kPawnMask; }
  BitBoard en_passant() const { return pawns_ - kPawnMask; }
  BitBoard bishops() const { return bishops_ - rooks_; }
  BitBoard rooks() const { return rooks_ - bishops_; }
  BitBoard queens() const { return rooks_ & bishops_; }
  BitBoard knights() const {
    return (our_pieces_ | their_pieces_) - pawns() - our_king_ - their_king_ -
           rooks_ - bishops_;
  }
  BitBoard kings() const {
    return our_king_.as_board() | their_king_.as_board();
  }
  const Castlings& castlings() const { return castlings_; }
  bool flipped() const { return flipped_; }

  bool operator==(const ChessBoard& other) const {
    return (our_pieces_ == other.our_pieces_) &&
           (their_pieces_ == other.their_pieces_) && (rooks_ == other.rooks_) &&
           (bishops_ == other.bishops_) && (pawns_ == other.pawns_) &&
           (our_king_ == other.our_king_) &&
           (their_king_ == other.their_king_) &&
           (castlings_ == other.castlings_) && (flipped_ == other.flipped_);
  }

  bool operator!=(const ChessBoard& other) const { return !operator==(other); }

  enum Square : uint8_t {
    // clang-format off
    A1 = 0, B1, C1, D1, E1, F1, G1, H1,
    A2, B2, C2, D2, E2, F2, G2, H2,
    A3, B3, C3, D3, E3, F3, G3, H3,
    A4, B4, C4, D4, E4, F4, G4, H4,
    A5, B5, C5, D5, E5, F5, G5, H5,
    A6, B6, C6, D6, E6, F6, G6, H6,
    A7, B7, C7, D7, E7, F7, G7, H7,
    A8, B8, C8, D8, E8, F8, G8, H8,
    // clang-format on
  };

  enum File : uint8_t {
    // clang-format off
    FILE_A = 0, FILE_B, FILE_C, FILE_D, FILE_E, FILE_F, FILE_G, FILE_H
    // clang-format on
  };

  enum Rank : uint8_t {
    // clang-format off
    RANK_1 = 0, RANK_2, RANK_3, RANK_4, RANK_5, RANK_6, RANK_7, RANK_8
    // clang-format on
  };

 private:
  // All white pieces.
  BitBoard our_pieces_;
  // All black pieces.
  BitBoard their_pieces_;
  // Rooks and queens.
  BitBoard rooks_;
  // Bishops and queens;
  BitBoard bishops_;
  // Pawns.
  // Ranks 1 and 8 have special meaning. Pawn at rank 1 means that
  // corresponding white pawn on rank 4 can be taken en passant. Rank 8 is the
  // same for black pawns. Those "fake" pawns are not present in our_pieces_ and
  // their_pieces_ bitboards.
  BitBoard pawns_;
  BoardSquare our_king_;
  BoardSquare their_king_;
  Castlings castlings_;
  bool flipped_ = false;  // aka "Black to move".
};

}  // namespace lczero

```

`src/chess/board_test.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
*/

#include "chess/board.h"

#include <gtest/gtest.h>

#include <iostream>

#include "chess/bitboard.h"

#include "utils/exception.h"

namespace lczero {

TEST(BoardSquare, BoardSquare) {
  {
    auto x = BoardSquare(ChessBoard::C2);
    EXPECT_EQ(x.row(), 1);
    EXPECT_EQ(x.col(), 2);
  }

  {
    auto x = BoardSquare("c2");
    EXPECT_EQ(x.row(), 1);
    EXPECT_EQ(x.col(), 2);
  }

  {
    auto x = BoardSquare(1, 2);
    EXPECT_EQ(x.row(), 1);
    EXPECT_EQ(x.col(), 2);
  }

  {
    auto x = BoardSquare(1, 2);
    x.Mirror();
    EXPECT_EQ(x.row(), 6);
    EXPECT_EQ(x.col(), 2);
  }
}

TEST(ChessBoard, IllegalFirstRankPawns) {
  ChessBoard board;
  EXPECT_THROW(board.SetFromFen("nqrbkrnr/bnnbnbnn/8/8/8/8/NNNBPNBN/QNRPKPQQ w - - 0 1");,
               Exception);
}

TEST(ChessBoard, PseudolegalMovesStartingPos) {
  ChessBoard board;
  board.SetFromFen(ChessBoard::kStartposFen);
  board.Mirror();
  auto moves = board.GeneratePseudolegalMoves();

  EXPECT_EQ(moves.size(), 20);
}

TEST(ChessBoard, PartialFen) {
  ChessBoard board;
  int rule50ply;
  int gameply;
  board.SetFromFen("k/1R//K", &rule50ply, &gameply);
  auto moves = board.GeneratePseudolegalMoves();

  EXPECT_EQ(moves.size(), 19);
  EXPECT_EQ(rule50ply, 0);
  EXPECT_EQ(gameply, 1);
}

TEST(ChessBoard, PartialFenWithSpaces) {
  ChessBoard board;
  int rule50ply;
  int gameply;
  board.SetFromFen("   k/1R//K   w   ", &rule50ply, &gameply);
  auto moves = board.GeneratePseudolegalMoves();

  EXPECT_EQ(moves.size(), 19);
  EXPECT_EQ(rule50ply, 0);
  EXPECT_EQ(gameply, 1);
}

namespace {
int Perft(const ChessBoard& board, int max_depth, bool dump = false,
          int depth = 0) {
  if (depth == max_depth) return 1;
  int total_count = 0;
  auto moves = board.GeneratePseudolegalMoves();

  auto legal_moves = board.GenerateLegalMoves();
  auto iter = legal_moves.begin();

  for (const auto& move : moves) {
    auto new_board = board;
    new_board.ApplyMove(move);
    if (new_board.IsUnderCheck()) {
      if (iter != legal_moves.end()) {
        EXPECT_NE(iter->as_packed_int(), move.as_packed_int())
            << board.DebugString() << "legal:[" << iter->as_string()
            << "]==pseudo:(" << move.as_string() << ") Under check:\n"
            << new_board.DebugString();
      }
      continue;
    }

    EXPECT_EQ(iter->as_packed_int(), move.as_packed_int())
        << board.DebugString() << "legal:[" << iter->as_string() << "]pseudo:("
        << move.as_string() << ") after:\n"
        << new_board.DebugString();

    new_board.Mirror();
    ++iter;
    int count = Perft(new_board, max_depth, dump, depth + 1);
    if (dump && depth == 0) {
      Move m = move;
      if (board.flipped()) m.Mirror();
      std::cerr << m.as_string() << ": " << count << '\n';
    }
    total_count += count;
  }

  EXPECT_EQ(iter, legal_moves.end());
  return total_count;
}
}  // namespace

/* TEST(ChessBoard, MoveGenStartingPos) {
  ChessBoard board;
  board.SetFromFen(ChessBoard::kStartposFen);

  EXPECT_EQ(Perft(board, 0), 1);
  EXPECT_EQ(Perft(board, 1), 20);
  EXPECT_EQ(Perft(board, 2), 400);
  EXPECT_EQ(Perft(board, 3), 8902);
  EXPECT_EQ(Perft(board, 4), 197281);
  EXPECT_EQ(Perft(board, 5), 4865609);
  EXPECT_EQ(Perft(board, 6), 119060324);
} */

TEST(ChessBoard, MoveGenKiwipete) {
  ChessBoard board;
  board.SetFromFen(
      "r3k2r/p1ppqpb1/bn2pnp1/3PN3/1p2P3/2N2Q1p/PPPBBPPP/R3K2R w KQkq - 1 1");

  EXPECT_EQ(Perft(board, 1), 48);
  EXPECT_EQ(Perft(board, 2), 2039);
  EXPECT_EQ(Perft(board, 3), 97862);
  EXPECT_EQ(Perft(board, 4), 4085603);
  //  EXPECT_EQ(Perft(board, 5), 193690690);
}

TEST(ChessBoard, MoveGenPosition3) {
  ChessBoard board;
  board.SetFromFen("8/2p5/3p4/KP5r/1R3p1k/8/4P1P1/8 w - - 1 1");

  EXPECT_EQ(Perft(board, 1), 14);
  EXPECT_EQ(Perft(board, 2), 191);
  EXPECT_EQ(Perft(board, 3), 2812);
  EXPECT_EQ(Perft(board, 4), 43238);
  EXPECT_EQ(Perft(board, 5), 674624);
  EXPECT_EQ(Perft(board, 6), 11030083);
}

TEST(ChessBoard, MoveGenPosition4) {
  ChessBoard board;
  board.SetFromFen(
      "r2q1rk1/pP1p2pp/Q4n2/bbp1p3/Np6/1B3NBn/pPPP1PPP/R3K2R b KQ - 0 1");

  EXPECT_EQ(Perft(board, 1), 6);
  EXPECT_EQ(Perft(board, 2), 264);
  EXPECT_EQ(Perft(board, 3), 9467);
  EXPECT_EQ(Perft(board, 4), 422333);
  EXPECT_EQ(Perft(board, 5), 15833292);
}

TEST(ChessBoard, MoveGenPosition5) {
  ChessBoard board;
  board.SetFromFen("rnbq1k1r/pp1Pbppp/2p5/8/2B5/8/PPP1NnPP/RNBQK2R w KQ - 1 8");

  EXPECT_EQ(Perft(board, 1), 44);
  EXPECT_EQ(Perft(board, 2), 1486);
  EXPECT_EQ(Perft(board, 3), 62379);
  EXPECT_EQ(Perft(board, 4), 2103487);
  //  EXPECT_EQ(Perft(board, 5), 89941194);
}

TEST(ChessBoard, MoveGenPosition6) {
  ChessBoard board;
  board.SetFromFen(
      "r4rk1/1pp1qppp/p1np1n2/2b1p1B1/2B1P1b1/P1NP1N2/1PP1QPPP/R4RK1 w - - 0 "
      "10");

  EXPECT_EQ(Perft(board, 1), 46);
  EXPECT_EQ(Perft(board, 2), 2079);
  EXPECT_EQ(Perft(board, 3), 89890);
  EXPECT_EQ(Perft(board, 4), 3894594);
}

namespace {
const struct {
  const char* const fen;
  const uint32_t perft[6];
} kChess960Positions[] = {
    // {"bqnb1rkr/pp3ppp/3ppn2/2p5/5P2/P2P4/NPP1P1PP/BQ1BNRKR w HFhf - 2 9",
    // {21, 528, 12189, 326672, 8146062, 227689589}},  // 1
    // {"2nnrbkr/p1qppppp/8/1ppb4/6PP/3PP3/PPP2P2/BQNNRBKR w HEhe - 1 9", {21,
    // 807, 18002, 667366, 16253601, 590751109}},  // 2
    // {"b1q1rrkb/pppppppp/3nn3/8/P7/1PPP4/4PPPP/BQNNRKRB w GE - 1 9", {20, 479,
    // 10471, 273318, 6417013, 177654692}},  // 3
    // {"qbbnnrkr/2pp2pp/p7/1p2pp2/8/P3PP2/1PPP1KPP/QBBNNR1R w hf - 0 9", {22,
    // 593, 13440, 382958, 9183776, 274103539}},  // 4
    // {"1nbbnrkr/p1p1ppp1/3p4/1p3P1p/3Pq2P/8/PPP1P1P1/QNBBNRKR w HFhf - 0 9",
    // {28, 1120, 31058, 1171749, 34030312, 1250970898}},  // 5
    // {"qnbnr1kr/ppp1b1pp/4p3/3p1p2/8/2NPP3/PPP1BPPP/QNB1R1KR w HEhe - 1 9",
    // {29, 899, 26578, 824055, 24851983, 775718317}},  // 6
    // {"q1bnrkr1/ppppp2p/2n2p2/4b1p1/2NP4/8/PPP1PPPP/QNB1RRKB w ge - 1 9", {30,
    // 860, 24566, 732757, 21093346, 649209803}},  // 7
    // {"qbn1brkr/ppp1p1p1/2n4p/3p1p2/P7/6PP/QPPPPP2/1BNNBRKR w HFhf - 0 9",
    // {25, 635, 17054, 465806, 13203304, 377184252}},  // 8
    // {"qnnbbrkr/1p2ppp1/2pp3p/p7/1P5P/2NP4/P1P1PPP1/Q1NBBRKR w HFhf - 0 9",
    // {24, 572, 15243, 384260, 11110203, 293989890}},  // 9
    // {"qn1rbbkr/ppp2p1p/1n1pp1p1/8/3P4/P6P/1PP1PPPK/QNNRBB1R w hd - 2 9", {28,
    // 811, 23175, 679699, 19836606, 594527992}},  // 10
    // {"qnr1bkrb/pppp2pp/3np3/5p2/8/P2P2P1/NPP1PP1P/QN1RBKRB w GDg - 3 9", {33,
    // 823, 26895, 713420, 23114629, 646390782}},  // 11
    // {"qb1nrkbr/1pppp1p1/1n3p2/p1B4p/8/3P1P1P/PPP1P1P1/QBNNRK1R w HEhe - 0 9",
    // {31, 855, 25620, 735703, 21796206, 651054626}},  // 12
    // {"qnnbrk1r/1p1ppbpp/2p5/p4p2/2NP3P/8/PPP1PPP1/Q1NBRKBR w HEhe - 0 9",
    // {26, 790, 21238, 642367, 17819770, 544866674}},  // 13
    // {"1qnrkbbr/1pppppp1/p1n4p/8/P7/1P1N1P2/2PPP1PP/QN1RKBBR w HDhd - 0 9",
    // {37, 883, 32187, 815535, 29370838, 783201510}},  // 14
    {"qn1rkrbb/pp1p1ppp/2p1p3/3n4/4P2P/2NP4/PPP2PP1/Q1NRKRBB w FDfd - 1 9",
     {24, 585, 14769, 356950, 9482310, 233468620}},  // 15
    // {"bb1qnrkr/pp1p1pp1/1np1p3/4N2p/8/1P4P1/P1PPPP1P/BBNQ1RKR w HFhf - 0 9",
    // {29, 864, 25747, 799727, 24219627, 776836316}},  // 16
    // {"bnqbnr1r/p1p1ppkp/3p4/1p4p1/P7/3NP2P/1PPP1PP1/BNQB1RKR w HF - 0 9",
    // {26, 889, 24353, 832956, 23701014, 809194268}},  // 17
    // {"bnqnrbkr/1pp2pp1/p7/3pP2p/4P1P1/8/PPPP3P/BNQNRBKR w HEhe d6 0 9", {31,
    // 984, 28677, 962591, 29032175, 1008880643}},  // 18
    // {"b1qnrrkb/ppp1pp1p/n2p1Pp1/8/8/P7/1PPPP1PP/BNQNRKRB w GE - 0 9", {20,
    // 484, 10532, 281606, 6718715, 193594729}},  // 19
    // {"n1bqnrkr/pp1ppp1p/2p5/6p1/2P2b2/PN6/1PNPPPPP/1BBQ1RKR w HFhf - 2 9",
    // {23, 732, 17746, 558191, 14481581, 457140569}},  // 20
    // {"n1bb1rkr/qpnppppp/2p5/p7/P1P5/5P2/1P1PPRPP/NQBBN1KR w Hhf - 1 9", {27,
    // 697, 18724, 505089, 14226907, 400942568}},  // 21
    // {"nqb1rbkr/pppppp1p/4n3/6p1/4P3/1NP4P/PP1P1PP1/1QBNRBKR w HEhe - 1 9",
    // {28, 641, 18811, 456916, 13780398, 354122358}},  // 22
    // {"n1bnrrkb/pp1pp2p/2p2p2/6p1/5B2/3P4/PPP1PPPP/NQ1NRKRB w GE - 2 9", {28,
    // 606, 16883, 381646, 10815324, 254026570}},  // 23
    // {"nbqnbrkr/2ppp1p1/pp3p1p/8/4N2P/1N6/PPPPPPP1/1BQ1BRKR w HFhf - 0 9",
    // {26, 626, 17268, 437525, 12719546, 339132046}},  // 24
    // {"nq1bbrkr/pp2nppp/2pp4/4p3/1PP1P3/1B6/P2P1PPP/NQN1BRKR w HFhf - 2 9",
    // {21, 504, 11812, 302230, 7697880, 207028745}},  // 25
    // {"nqnrb1kr/2pp1ppp/1p1bp3/p1B5/5P2/3N4/PPPPP1PP/NQ1R1BKR w HDhd - 0 9",
    // {30, 672, 19307, 465317, 13454573, 345445468}},  // 26
    // {"nqn2krb/p1prpppp/1pbp4/7P/5P2/8/PPPPPKP1/NQNRB1RB w g - 3 9", {21, 461,
    // 10608, 248069, 6194124, 152861936}},  // 27
    // {"nb1n1kbr/ppp1rppp/3pq3/P3p3/8/4P3/1PPPRPPP/NBQN1KBR w Hh - 1 9", {19,
    // 566, 11786, 358337, 8047916, 249171636}},  // 28
    // {"nqnbrkbr/1ppppp1p/p7/6p1/6P1/P6P/1PPPPP2/NQNBRKBR w HEhe - 1 9", {20,
    // 382, 8694, 187263, 4708975, 112278808}},  // 29
    // {"nq1rkb1r/pp1pp1pp/1n2bp1B/2p5/8/5P1P/PPPPP1P1/NQNRKB1R w HDhd - 2 9",
    // {24, 809, 20090, 673811, 17647882, 593457788}},  // 30
    // {"nqnrkrb1/pppppp2/7p/4b1p1/8/PN1NP3/1PPP1PPP/1Q1RKRBB w FDfd - 1 9",
    // {26, 683, 18102, 473911, 13055173, 352398011}},  // 31
    // {"bb1nqrkr/1pp1ppp1/pn5p/3p4/8/P2NNP2/1PPPP1PP/BB2QRKR w HFhf - 0 9",
    // {29, 695, 21193, 552634, 17454857, 483785639}},  // 32
    // {"bnn1qrkr/pp1ppp1p/2p5/b3Q1p1/8/5P1P/PPPPP1P1/BNNB1RKR w HFhf - 2 9",
    // {44, 920, 35830, 795317, 29742670, 702867204}},  // 33
    // {"bnnqrbkr/pp1p2p1/2p1p2p/5p2/1P5P/1R6/P1PPPPP1/BNNQRBK1 w Ehe - 0 9",
    // {33, 1022, 32724, 1024721, 32898113, 1047360456}},  // 34
    // {"b1nqrkrb/2pppppp/p7/1P6/1n6/P4P2/1P1PP1PP/BNNQRKRB w GEge - 0 9", {23,
    // 638, 15744, 446539, 11735969, 344211589}},  // 35
    // {"n1bnqrkr/3ppppp/1p6/pNp1b3/2P3P1/8/PP1PPP1P/NBB1QRKR w HFhf - 1 9",
    // {29, 728, 20768, 532084, 15621236, 415766465}},  // 36
    // {"n2bqrkr/p1p1pppp/1pn5/3p1b2/P6P/1NP5/1P1PPPP1/1NBBQRKR w HFhf - 3 9",
    // {20, 533, 12152, 325059, 8088751, 223068417}},  // 37
    // {"nnbqrbkr/1pp1p1p1/p2p4/5p1p/2P1P3/N7/PPQP1PPP/N1B1RBKR w HEhe - 0 9",
    // {27, 619, 18098, 444421, 13755384, 357222394}},  // 38
    // {"nnbqrkr1/pp1pp2p/2p2b2/5pp1/1P5P/4P1P1/P1PP1P2/NNBQRKRB w GEge - 1 9",
    // {32, 1046, 33721, 1111186, 36218182, 1202830851}},  // 39
    // {"nb1qbrkr/p1pppp2/1p1n2pp/8/1P6/2PN3P/P2PPPP1/NB1QBRKR w HFhf - 0 9",
    // {25, 521, 14021, 306427, 8697700, 201455191}},  // 40
    // {"nnq1brkr/pp1pppp1/8/2p4P/8/5K2/PPPbPP1P/NNQBBR1R w hf - 0 9", {23, 724,
    // 18263, 571072, 15338230, 484638597}},  // 41
    // {"nnqrbb1r/pppppk2/5pp1/7p/1P6/3P2PP/P1P1PP2/NNQRBBKR w HD - 0 9", {30,
    // 717, 21945, 547145, 17166700, 450069742}},  // 42
    // {"nnqr1krb/p1p1pppp/2bp4/8/1p1P4/4P3/PPP2PPP/NNQRBKRB w GDgd - 0 9", {25,
    // 873, 20796, 728628, 18162741, 641708630}},  // 43
    // {"nbnqrkbr/p2ppp2/1p4p1/2p4p/3P3P/3N4/PPP1PPPR/NB1QRKB1 w Ehe - 0 9",
    // {24, 589, 15190, 382317, 10630667, 279474189}},  // 44
    // {"n1qbrkbr/p1ppp2p/2n2pp1/1p6/1P6/2P3P1/P2PPP1P/NNQBRKBR w HEhe - 0 9",
    // {22, 592, 14269, 401976, 10356818, 301583306}},  // 45
    // {"2qrkbbr/ppn1pppp/n1p5/3p4/5P2/P1PP4/1P2P1PP/NNQRKBBR w HDhd - 1 9",
    // {27, 750, 20584, 605458, 16819085, 516796736}},  // 46
    // {"1nqr1rbb/pppkp1pp/1n3p2/3p4/1P6/5P1P/P1PPPKP1/NNQR1RBB w - - 1 9", {24,
    // 623, 15921, 429446, 11594634, 322745925}},  // 47
    // {"bbn1rqkr/pp1pp2p/4npp1/2p5/1P6/2BPP3/P1P2PPP/1BNNRQKR w HEhe - 0 9",
    // {23, 730, 17743, 565340, 14496370, 468608864}},  // 48
    // {"bn1brqkr/pppp2p1/3npp2/7p/PPP5/8/3PPPPP/BNNBRQKR w HEhe - 0 9", {25,
    // 673, 17835, 513696, 14284338, 434008567}},  // 49
    // {"bn1rqbkr/ppp1ppp1/1n6/2p4p/7P/3P4/PPP1PPP1/BN1RQBKR w HDhd - 0 9", {25,
    // 776, 20562, 660217, 18486027, 616653869}},  // 50
    // {"bnnr1krb/ppp2ppp/3p4/3Bp3/q1P3PP/8/PP1PPP2/BNNRQKR1 w GDgd - 0 9", {29,
    // 1040, 30772, 1053113, 31801525, 1075147725}},  // 51
    // {"1bbnrqkr/pp1ppppp/8/2p5/n7/3PNPP1/PPP1P2P/NBB1RQKR w HEhe - 1 9", {24,
    // 598, 15673, 409766, 11394778, 310589129}},  // 52
    // {"nnbbrqkr/p2ppp1p/1pp5/8/6p1/N1P5/PPBPPPPP/N1B1RQKR w HEhe - 0 9", {26,
    // 530, 14031, 326312, 8846766, 229270702}},  // 53
    // {"nnbrqbkr/2p1p1pp/p4p2/1p1p4/8/NP6/P1PPPPPP/N1BRQBKR w HDhd - 0 9", {17,
    // 496, 10220, 303310, 7103549, 217108001}},  // 54
    // {"nnbrqk1b/pp2pprp/2pp2p1/8/3PP1P1/8/PPP2P1P/NNBRQRKB w d - 1 9", {33,
    // 820, 27856, 706784, 24714401, 645835197}},  // 55
    // {"1bnrbqkr/ppnpp1p1/2p2p1p/8/1P6/4PPP1/P1PP3P/NBNRBQKR w HDhd - 0 9",
    // {27, 705, 19760, 548680, 15964771, 464662032}},  // 56
    // {"n1rbbqkr/pp1pppp1/7p/P1p5/1n6/2PP4/1P2PPPP/NNRBBQKR w HChc - 0 9", {22,
    // 631, 14978, 431801, 10911545, 320838556}},  // 57
    // {"n1rqb1kr/p1pppp1p/1pn4b/3P2p1/P7/1P6/2P1PPPP/NNRQBBKR w HChc - 0 9",
    // {24, 477, 12506, 263189, 7419372, 165945904}},  // 58
    // {"nnrqbkrb/pppp1pp1/7p/4p3/6P1/2N2B2/PPPPPP1P/NR1QBKR1 w Ggc - 2 9", {29,
    // 658, 19364, 476620, 14233587, 373744834}},  // 59
    // {"n1nrqkbr/ppb2ppp/3pp3/2p5/2P3P1/5P2/PP1PPB1P/NBNRQK1R w HDhd - 1 9",
    // {32, 801, 25861, 681428, 22318948, 619857455}},  // 60
    // {"2rbqkbr/p1pppppp/1nn5/1p6/7P/P4P2/1PPPP1PB/NNRBQK1R w HChc - 2 9", {27,
    // 647, 18030, 458057, 13189156, 354689323}},  // 61
    // {"nn1qkbbr/pp2ppp1/2rp4/2p4p/P2P4/1N5P/1PP1PPP1/1NRQKBBR w HCh - 1 9",
    // {24, 738, 18916, 586009, 16420659, 519075930}},  // 62
    // {"nnrqk1bb/p1ppp2p/5rp1/1p3p2/1P4P1/5P1P/P1PPP3/NNRQKRBB w FCc - 1 9",
    // {25, 795, 20510, 648945, 17342527, 556144017}},  // 63
    // {"bb1nrkqr/ppppn2p/4ppp1/8/1P4P1/4P3/P1PPKP1P/BBNNR1QR w he - 0 9", {29,
    // 664, 20024, 498376, 15373803, 406016364}},  // 64
    // {"bnnbrkqr/1p1ppp2/8/p1p3pp/1P6/N4P2/PBPPP1PP/2NBRKQR w HEhe - 0 9", {31,
    // 770, 24850, 677212, 22562080, 662029574}},  // 65
    // {"1nnrkbqr/p1pp1ppp/4p3/1p6/1Pb1P3/6PB/P1PP1P1P/BNNRK1QR w HDhd - 0 9",
    // {27, 776, 22133, 641002, 19153245, 562738257}},  // 66
    // {"bnr1kqrb/pppp1pp1/1n5p/4p3/P3P3/3P2P1/1PP2P1P/BNNRKQRB w GDg - 0 9",
    // {26, 624, 16411, 435426, 11906515, 338092952}},  // 67
    // {"nbbnrkqr/p1ppp1pp/1p3p2/8/2P5/4P3/PP1P1PPP/NBBNRKQR w HEhe - 1 9", {25,
    // 624, 15561, 419635, 10817378, 311138112}},  // 68
    // {"nn1brkqr/pp1bpppp/8/2pp4/P4P2/1PN5/2PPP1PP/N1BBRKQR w HEhe - 1 9", {23,
    // 659, 16958, 476567, 13242252, 373557073}},  // 69
    // {"n1brkbqr/ppp1pp1p/6pB/3p4/2Pn4/8/PP2PPPP/NN1RKBQR w HDhd - 0 9", {32,
    // 1026, 30360, 978278, 29436320, 957904151}},  // 70
    // {"nnbrkqrb/p2ppp2/Q5pp/1pp5/4PP2/2N5/PPPP2PP/N1BRK1RB w GDgd - 0 9", {36,
    // 843, 29017, 715537, 24321197, 630396940}},  // 71
    // {"nbnrbk1r/pppppppq/8/7p/8/1N2QPP1/PPPPP2P/NB1RBK1R w HDhd - 2 9", {36,
    // 973, 35403, 1018054, 37143354, 1124883780}},  // 72
    // {"nnrbbkqr/2pppp1p/p7/6p1/1p2P3/4QPP1/PPPP3P/NNRBBK1R w HChc - 0 9", {36,
    // 649, 22524, 489526, 16836636, 416139320}},  // 73
    // {"nnrkbbqr/1p2pppp/p2p4/2p5/8/1N2P1P1/PPPP1P1P/1NKRBBQR w hc - 0 9", {26,
    // 672, 18136, 477801, 13342771, 363074681}},  // 74
    // {"n1rkbqrb/pp1ppp2/2n3p1/2p4p/P5PP/1P6/2PPPP2/NNRKBQRB w GCgc - 0 9",
    // {24, 804, 20712, 684001, 18761475, 617932151}},  // 75
    // {"nbkr1qbr/1pp1pppp/pn1p4/8/3P2P1/5R2/PPP1PP1P/NBN1KQBR w H - 2 9", {30,
    // 627, 18669, 423329, 12815016, 312798696}},  // 76
    // {"nnr1kqbr/pp1pp1p1/2p5/b4p1p/P7/1PNP4/2P1PPPP/N1RBKQBR w HChc - 1 9",
    // {12, 421, 6530, 227044, 4266410, 149176979}},  // 77
    // {"n1rkqbbr/p1pp1pp1/np2p2p/8/8/N4PP1/PPPPP1BP/N1RKQ1BR w HChc - 0 9",
    // {27, 670, 19119, 494690, 14708490, 397268628}},  // 78
    // {"nnr1qrbb/p2kpppp/1p1p4/2p5/6P1/PP1P4/2P1PP1P/NNRKQRBB w FC - 0 9", {27,
    // 604, 17043, 409665, 11993332, 308518181}},  // 79
    // {"bbnnrkrq/ppp1pp2/6p1/3p4/7p/7P/PPPPPPP1/BBNNRRKQ w ge - 0 9", {20, 559,
    // 12242, 355326, 8427161, 252274233}},  // 80
    // {"bnnbrkr1/ppp2p1p/5q2/3pp1p1/4P3/1N4P1/PPPPRP1P/BN1B1KRQ w Gge - 0 9",
    // {26, 1036, 27228, 1028084, 28286576, 1042120495}},  // 81
    // {"bn1rkbrq/1pppppp1/p6p/1n6/3P4/6PP/PPPRPP2/BNN1KBRQ w Ggd - 2 9", {29,
    // 633, 19278, 455476, 14333034, 361900466}},  // 82
    // {"b1nrkrqb/1p1npppp/p2p4/2p5/5P2/4P2P/PPPP1RP1/BNNRK1QB w Dfd - 1 9",
    // {25, 475, 12603, 270909, 7545536, 179579818}},  // 83
    // {"1bbnrkrq/ppppppp1/8/7p/1n4P1/1PN5/P1PPPP1P/NBBR1KRQ w Gge - 0 9", {30,
    // 803, 25473, 709716, 23443854, 686365049}},  // 84
    // {"nnbbrkrq/2pp1pp1/1p5p/pP2p3/7P/N7/P1PPPPP1/N1BBRKRQ w GEge - 0 9", {18,
    // 432, 9638, 242350, 6131124, 160393505}},  // 85
    // {"nnbrkbrq/1pppp1p1/p7/7p/1P2Pp2/BN6/P1PP1PPP/1N1RKBRQ w GDgd - 0 9",
    // {27, 482, 13441, 282259, 8084701, 193484216}},  // 86
    // {"n1brkrqb/pppp3p/n3pp2/6p1/3P1P2/N1P5/PP2P1PP/N1BRKRQB w FDfd - 0 9",
    // {28, 642, 19005, 471729, 14529434, 384837696}},  // 87
    {"nbnrbk2/p1pppp1p/1p3qr1/6p1/1B1P4/1N6/PPP1PPPP/1BNR1RKQ w d - 2 9",
     {30, 796, 22780, 687302, 20120565, 641832725}},  // 88
    // {"nnrbbrkq/1pp2ppp/3p4/p3p3/3P1P2/1P2P3/P1P3PP/NNRBBKRQ w GC - 1 9", {31,
    // 827, 24538, 663082, 19979594, 549437308}},  // 89
    // {"nnrkbbrq/1pp2p1p/p2pp1p1/2P5/8/8/PP1PPPPP/NNRKBBRQ w Ggc - 0 9", {24,
    // 762, 19283, 624598, 16838099, 555230555}},  // 90
    // {"nnr1brqb/1ppkp1pp/8/p2p1p2/1P1P4/N1P5/P3PPPP/N1RKBRQB w FC - 1 9", {23,
    // 640, 15471, 444905, 11343507, 334123513}},  // 91
    // {"nbnrkrbq/2ppp2p/p4p2/1P4p1/4PP2/8/1PPP2PP/NBNRKRBQ w FDfd - 0 9", {31,
    // 826, 26137, 732175, 23555139, 686250413}},  // 92
    // {"1nrbkr1q/1pppp1pp/1n6/p4p2/N1b4P/8/PPPPPPPB/N1RBKR1Q w FCfc - 2 9",
    // {27, 862, 24141, 755171, 22027695, 696353497}},  // 93
    // {"nnrkrbbq/pppp2pp/8/4pp2/4P3/P7/1PPPBPPP/NNKRR1BQ w c - 0 9", {25, 792,
    // 19883, 636041, 16473376, 532214177}},  // 94
    // {"n1rk1qbb/pppprpp1/2n4p/4p3/2PP3P/8/PP2PPP1/NNRKRQBB w ECc - 1 9", {25,
    // 622, 16031, 425247, 11420973, 321855685}},  // 95
    // {"bbq1rnkr/pnp1pp1p/1p1p4/6p1/2P5/2Q1P2P/PP1P1PP1/BB1NRNKR w HEhe - 2 9",
    // {36, 870, 30516, 811047, 28127620, 799738334}},  // 96
    // {"bq1brnkr/1p1ppp1p/1np5/p5p1/8/1N5P/PPPPPPP1/BQ1BRNKR w HEhe - 0 9",
    // {22, 588, 13524, 380068, 9359618, 273795898}},  // 97
    // {"bq1rn1kr/1pppppbp/Nn4p1/8/8/P7/1PPPPPPP/BQ1RNBKR w HDhd - 1 9", {24,
    // 711, 18197, 542570, 14692779, 445827351}},  // 98
    // {"bqnr1kr1/pppppp1p/6p1/5n2/4B3/3N2PP/PbPPPP2/BQNR1KR1 w GDgd - 2 9",
    // {31, 1132, 36559, 1261476, 43256823, 1456721391}},  // 99
    // {"qbb1rnkr/ppp3pp/4n3/3ppp2/1P3PP1/8/P1PPPN1P/QBB1RNKR w HEhe - 0 9",
    // {28, 696, 20502, 541886, 16492398, 456983120}},  // 100
    // {"qnbbr1kr/pp1ppp1p/4n3/6p1/2p3P1/2PP1P2/PP2P2P/QNBBRNKR w HEhe - 0 9",
    // {25, 655, 16520, 450189, 11767038, 335414976}},  // 101
    // {"1nbrnbkr/p1ppp1pp/1p6/5p2/4q1PP/3P4/PPP1PP2/QNBRNBKR w HDhd - 1 9",
    // {30, 1162, 33199, 1217278, 36048727, 1290346802}},  // 102
    // {"q1brnkrb/p1pppppp/n7/1p6/P7/3P1P2/QPP1P1PP/1NBRNKRB w GDgd - 0 9", {32,
    // 827, 26106, 718243, 23143989, 673147648}},  // 103
    // {"qbnrb1kr/ppp1pp1p/3p4/2n3p1/1P6/6N1/P1PPPPPP/QBNRB1KR w HDhd - 2 9",
    // {29, 751, 23132, 610397, 19555214, 530475036}},  // 104
    // {"q1rbbnkr/pppp1p2/2n3pp/2P1p3/3P4/8/PP1NPPPP/Q1RBBNKR w HChc - 2 9",
    // {29, 806, 24540, 687251, 21694330, 619907316}},  // 105
    // {"q1r1bbkr/pnpp1ppp/2n1p3/1p6/2P2P2/2N1N3/PP1PP1PP/Q1R1BBKR w HChc - 2
    // 9", {32, 1017, 32098, 986028, 31204371, 958455898}},  // 106
    // {"2rnbkrb/pqppppp1/1pn5/7p/2P5/P1R5/QP1PPPPP/1N1NBKRB w Ggc - 4 9", {26,
    // 625, 16506, 434635, 11856964, 336672890}},  // 107
    // {"qbnr1kbr/p2ppppp/2p5/1p6/4n2P/P4N2/1PPP1PP1/QBNR1KBR w HDhd - 0 9",
    // {27, 885, 23828, 767273, 21855658, 706272554}},  // 108
    // {"qnrbnk1r/pp1pp2p/5p2/2pbP1p1/3P4/1P6/P1P2PPP/QNRBNKBR w HChc - 0 9",
    // {26, 954, 24832, 892456, 24415089, 866744329}},  // 109
    // {"qnrnk1br/p1p2ppp/8/1pbpp3/8/PP2N3/1QPPPPPP/1NR1KBBR w HChc - 0 9", {26,
    // 783, 20828, 634267, 17477825, 539674275}},  // 110
    // {"qnrnkrbb/Bpppp2p/6p1/5p2/5P2/3PP3/PPP3PP/QNRNKR1B w FCfc - 1 9", {28,
    // 908, 25730, 861240, 25251641, 869525254}},  // 111
    // {"bbnqrn1r/ppppp2k/5p2/6pp/7P/1QP5/PP1PPPP1/B1N1RNKR w HE - 0 9", {33,
    // 643, 21790, 487109, 16693640, 410115900}},  // 112
    // {"b1qbrnkr/ppp1pp2/2np4/6pp/4P3/2N4P/PPPP1PP1/BQ1BRNKR w HEhe - 0 9",
    // {28, 837, 24253, 745617, 22197063, 696399065}},  // 113
    // {"bnqr1bkr/pp1ppppp/2p5/4N3/5P2/P7/1PPPPnPP/BNQR1BKR w HDhd - 3 9", {25,
    // 579, 13909, 341444, 8601011, 225530258}},  // 114
    // {"b1qr1krb/pp1ppppp/n2n4/8/2p5/2P3P1/PP1PPP1P/BNQRNKRB w GDgd - 0 9",
    // {28, 707, 19721, 549506, 15583376, 468399900}},  // 115
    // {"nbbqr1kr/1pppp1pp/8/p1n2p2/4P3/PN6/1PPPQPPP/1BB1RNKR w HEhe - 0 9",
    // {30, 745, 23416, 597858, 19478789, 515473678}},  // 116
    // {"nqbbrn1r/p1pppp1k/1p4p1/7p/4P3/1R3B2/PPPP1PPP/NQB2NKR w H - 0 9", {24,
    // 504, 13512, 317355, 9002073, 228726497}},  // 117
    // {"nqbr1bkr/p1p1ppp1/1p1n4/3pN2p/1P6/8/P1PPPPPP/NQBR1BKR w HDhd - 0 9",
    // {29, 898, 26532, 809605, 24703467, 757166494}},  // 118
    // {"nqbrn1rb/pppp1kp1/5p1p/4p3/P4B2/3P2P1/1PP1PP1P/NQ1RNKRB w GD - 0 9",
    // {34, 671, 22332, 473110, 15556806, 353235120}},  // 119
    // {"nb1r1nkr/ppp1ppp1/2bp4/7p/3P2qP/P6R/1PP1PPP1/NBQRBNK1 w Dhd - 1 9",
    // {38, 1691, 60060, 2526992, 88557078, 3589649998}},  // 120
    // {"n1rbbnkr/1p1pp1pp/p7/2p1qp2/1B3P2/3P4/PPP1P1PP/NQRB1NKR w HChc - 0 9",
    // {24, 913, 21595, 807544, 19866918, 737239330}},  // 121
    // {"nqrnbbkr/p2p1p1p/1pp5/1B2p1p1/1P3P2/4P3/P1PP2PP/NQRNB1KR w HChc - 0 9",
    // {33, 913, 30159, 843874, 28053260, 804687975}},  // 122
    // {"nqr1bkrb/ppp1pp2/2np2p1/P6p/8/2P4P/1P1PPPP1/NQRNBKRB w GCgc - 0 9",
    // {24, 623, 16569, 442531, 12681936, 351623879}},  // 123
    // {"nb1rnkbr/pqppppp1/1p5p/8/1PP4P/8/P2PPPP1/NBQRNKBR w HDhd - 1 9", {31,
    // 798, 24862, 694386, 22616076, 666227466}},  // 124
    // {"nqrbnkbr/2p1p1pp/3p4/pp3p2/6PP/3P1N2/PPP1PP2/NQRB1KBR w HChc - 0 9",
    // {24, 590, 14409, 383690, 9698432, 274064911}},  // 125
    // {"nqrnkbbr/pp1p1p1p/4p1p1/1p6/8/5P1P/P1PPP1P1/NQRNKBBR w HChc - 0 9",
    // {30, 1032, 31481, 1098116, 34914919, 1233362066}},  // 126
    // {"nqrnkrbb/p2ppppp/1p6/2p5/2P3P1/5P2/PP1PPN1P/NQR1KRBB w FCfc - 1 9",
    // {30, 775, 23958, 668000, 21141738, 621142773}},  // 127
    // {"bbnrqrk1/pp2pppp/4n3/2pp4/P7/1N5P/BPPPPPP1/B2RQNKR w HD - 2 9", {23,
    // 708, 17164, 554089, 14343443, 481405144}},  // 128
    // {"bnr1qnkr/p1pp1p1p/1p4p1/4p1b1/2P1P3/1P6/PB1P1PPP/1NRBQNKR w HChc - 1
    // 9", {30, 931, 29249, 921746, 30026687, 968109774}},  // 129
    // {"b1rqnbkr/ppp1ppp1/3p3p/2n5/P3P3/2NP4/1PP2PPP/B1RQNBKR w HChc - 0 9",
    // {24, 596, 15533, 396123, 11099382, 294180723}},  // 130
    // {"bnrqnr1b/pp1pkppp/2p1p3/P7/2P5/7P/1P1PPPP1/BNRQNKRB w GC - 0 9", {24,
    // 572, 15293, 390903, 11208688, 302955778}},  // 131
    // {"n1brq1kr/bppppppp/p7/8/4P1Pn/8/PPPP1P2/NBBRQNKR w HDhd - 0 9", {20,
    // 570, 13139, 371247, 9919113, 284592289}},  // 132
    // {"1rbbqnkr/ppn1ppp1/3p3p/2p5/3P4/1N4P1/PPPBPP1P/1R1BQNKR w HBhb - 0 9",
    // {29, 1009, 29547, 1040816, 31059587, 1111986835}},  // 133
    // {"nrbq2kr/ppppppb1/5n1p/5Pp1/8/P5P1/1PPPP2P/NRBQNBKR w HBhb - 1 9", {20,
    // 520, 11745, 316332, 7809837, 216997152}},  // 134
    // {"nrb1nkrb/pp3ppp/1qBpp3/2p5/8/P5P1/1PPPPP1P/NRBQNKR1 w GBgb - 2 9", {32,
    // 850, 25642, 734088, 21981567, 664886187}},  // 135
    // {"1br1bnkr/ppqppp1p/1np3p1/8/1PP4P/4N3/P2PPPP1/NBRQB1KR w HChc - 1 9",
    // {32, 798, 24765, 691488, 22076141, 670296871}},  // 136
    // {"nrqbb1kr/1p1pp1pp/2p3n1/p4p2/3PP3/P5N1/1PP2PPP/NRQBB1KR w HBhb - 0 9",
    // {32, 791, 26213, 684890, 23239122, 634260266}},  // 137
    // {"nrqn1bkr/ppppp1pp/4b3/8/4P1p1/5P2/PPPP3P/NRQNBBKR w HBhb - 0 9", {29,
    // 687, 20223, 506088, 15236287, 398759980}},  // 138
    // {"nrqnbrkb/pppp1p2/4p2p/3B2p1/8/1P4P1/PQPPPP1P/NR1NBKR1 w GB - 0 9", {37,
    // 764, 27073, 610950, 21284835, 514864869}},  // 139
    // {"nbrq1kbr/Bp3ppp/2pnp3/3p4/5P2/2P4P/PP1PP1P1/NBRQNK1R w HChc - 0 9",
    // {40, 1271, 48022, 1547741, 56588117, 1850696281}},  // 140
    // {"nrqbnkbr/1p2ppp1/p1p4p/3p4/1P6/8/PQPPPPPP/1RNBNKBR w HBhb - 0 9", {28,
    // 757, 23135, 668025, 21427496, 650939962}},  // 141
    // {"nrqn1bbr/2ppkppp/4p3/pB6/8/2P1P3/PP1P1PPP/NRQNK1BR w HB - 1 9", {27,
    // 642, 17096, 442653, 11872805, 327545120}},  // 142
    // {"nrqnkrb1/p1ppp2p/1p4p1/4bp2/4PP1P/4N3/PPPP2P1/NRQ1KRBB w FBfb - 1 9",
    // {27, 958, 27397, 960350, 28520172, 995356563}},  // 143
    // {"1bnrnqkr/pbpp2pp/8/1p2pp2/P6P/3P1N2/1PP1PPP1/BBNR1QKR w HDhd - 0 9",
    // {27, 859, 23475, 773232, 21581178, 732696327}},  // 144
    // {"b1rbnqkr/1pp1ppp1/2n4p/p2p4/5P2/1PBP4/P1P1P1PP/1NRBNQKR w HChc - 0 9",
    // {26, 545, 14817, 336470, 9537260, 233549184}},  // 145
    // {"1nrnqbkr/p1pppppp/1p6/8/2b2P2/P1N5/1PP1P1PP/BNR1QBKR w HChc - 2 9",
    // {24, 668, 17716, 494866, 14216070, 406225409}},  // 146
    // {"1nrnqkrb/2ppp1pp/p7/1p3p2/5P2/N5K1/PPPPP2P/B1RNQ1RB w gc - 0 9", {33,
    // 725, 23572, 559823, 18547476, 471443091}},  // 147
    // {"nbbr1qkr/p1pppppp/8/1p1n4/3P4/1N3PP1/PPP1P2P/1BBRNQKR w HDhd - 1 9",
    // {28, 698, 20527, 539625, 16555068, 458045505}},  // 148
    // {"1rbbnqkr/1pnppp1p/p5p1/2p5/2P4P/5P2/PP1PP1PR/NRBBNQK1 w Bhb - 1 9",
    // {24, 554, 14221, 362516, 9863080, 269284081}},  // 149
    // {"nrb1qbkr/2pppppp/2n5/p7/2p5/4P3/PPNP1PPP/1RBNQBKR w HBhb - 0 9", {23,
    // 618, 15572, 443718, 12044358, 360311412}},  // 150
    // {"nrb1qkrb/2ppppp1/p3n3/1p1B3p/2P5/6P1/PP1PPPRP/NRBNQK2 w Bgb - 2 9",
    // {27, 593, 16770, 401967, 11806808, 303338935}},  // 151
    // {"nbrn1qkr/ppp1pp2/3p2p1/3Q3P/b7/8/PPPPPP1P/NBRNB1KR w HChc - 2 9", {39,
    // 1056, 40157, 1133446, 42201531, 1239888683}},  // 152
    // {"nr1bbqkr/pp1pp2p/1n3pp1/2p5/8/1P4P1/P1PPPPQP/NRNBBK1R w hb - 0 9", {25,
    // 585, 15719, 406544, 11582539, 320997679}},  // 153
    // {"nr2bbkr/ppp1pppp/1n1p4/8/6PP/1NP4q/PP1PPP2/1RNQBBKR w HBhb - 1 9", {22,
    // 742, 15984, 545231, 13287051, 457010195}},  // 154
    // {"1rnqbkrb/ppp1p1p1/1n3p2/3p3p/P6P/4P3/1PPP1PP1/NRNQBRKB w gb - 0 9",
    // {22, 574, 14044, 379648, 9968830, 281344367}},  // 155
    // {"nb1rqkbr/1pppp1pp/4n3/p4p2/6PP/5P2/PPPPPN2/NBR1QKBR w HCh - 0 9", {25,
    // 621, 16789, 462600, 13378840, 396575613}},  // 156
    // {"nrnbqkbr/2pp2pp/4pp2/pp6/8/1P3P2/P1PPPBPP/NRNBQ1KR w hb - 0 9", {25,
    // 656, 16951, 466493, 12525939, 358763789}},  // 157
    // {"nrnqkbbr/ppppp1p1/7p/5p2/8/P4PP1/NPPPP2P/NR1QKBBR w HBhb - 0 9", {28,
    // 723, 20621, 547522, 15952533, 439046803}},  // 158
    // {"1rnqkr1b/ppppp2p/1n3pp1/8/2P3P1/Pb1N4/1P1PPP1P/NR1QKRBB w FBfb - 0 9",
    // {26, 713, 19671, 548875, 15865528, 454532806}},  // 159
    // {"bbnrnkqr/1pppp1pp/5p2/p7/7P/1P6/PBPPPPPR/1BNRNKQ1 w D - 2 9", {26, 649,
    // 17834, 502279, 14375839, 435585252}},  // 160
    // {"bnrbk1qr/1ppp1ppp/p2np3/8/P7/2N2P2/1PPPP1PP/B1RBNKQR w HC - 0 9", {26,
    // 621, 17569, 451452, 13514201, 364421088}},  // 161
    // {"br1nkbqr/ppppppp1/8/n6p/8/N1P2PP1/PP1PP2P/B1RNKBQR w HCh - 1 9", {29,
    // 664, 20182, 512316, 16125924, 442508159}},  // 162
    // {"bnr1kqrb/pp1pppp1/2n5/2p5/1P4Pp/4N3/P1PPPP1P/BNKR1QRB w gc - 0 9", {36,
    // 888, 31630, 789863, 27792175, 719015345}},  // 163
    // {"1bbrnkqr/pp1p1ppp/2p1p3/1n6/5P2/3Q4/PPPPP1PP/NBBRNK1R w HDhd - 2 9",
    // {36, 891, 31075, 781792, 26998966, 702903862}},  // 164
    {"nrbbnk1r/pp2pppq/8/2pp3p/3P2P1/1N6/PPP1PP1P/1RBBNKQR w HBhb - 0 9",
     {29, 1036, 31344, 1139166, 35627310, 1310683359}},  // 165
    // {"nr1nkbqr/ppp3pp/5p2/3pp3/6b1/3PP3/PPP2PPP/NRBNKBQR w hb - 0 9", {18,
    // 664, 13306, 483892, 10658989, 386307449}},  // 166
    // {"nrbnk1rb/ppp1pq1p/3p4/5pp1/2P1P3/1N6/PP1PKPPP/1RBN1QRB w gb - 2 9",
    // {25, 966, 24026, 920345, 23957242, 913710194}},  // 167
    // {"1brnbkqr/pppppp2/6p1/7p/1Pn5/P1NP4/2P1PPPP/NBR1BKQR w HChc - 0 9", {22,
    // 627, 13760, 395829, 9627826, 285900573}},  // 168
    // {"nrnbbk1r/p1pppppq/8/7p/1p6/P5PP/1PPPPPQ1/NRNBBK1R w HBhb - 2 9", {29,
    // 888, 26742, 874270, 27229468, 930799376}},  // 169
    // {"n1nkb1qr/prppppbp/6p1/1p6/2P2P2/P7/1P1PP1PP/NRNKBBQR w HBh - 1 9", {29,
    // 804, 24701, 688520, 21952444, 623156747}},  // 170
    // {"nr2bqrb/ppkpp1pp/1np5/5p1P/5P2/2P5/PP1PP1P1/NRNKBQRB w GB - 0 9", {22,
    // 530, 13055, 347657, 9244693, 264088392}},  // 171
    // {"nbr1kqbr/p3pppp/2ppn3/1p4P1/4P3/1P6/P1PP1P1P/NBRNKQBR w HChc - 1 9",
    // {23, 555, 14291, 350917, 9692630, 247479180}},  // 172
    // {"nr1bkqbr/1p1pp1pp/pnp2p2/8/6P1/P1PP4/1P2PP1P/NRNBKQBR w HBhb - 0 9",
    // {22, 565, 13343, 365663, 9305533, 268612479}},  // 173
    // {"nr1kqbbr/np2pppp/p1p5/1B1p1P2/8/4P3/PPPP2PP/NRNKQ1BR w HBhb - 0 9",
    // {32, 730, 23391, 556995, 18103280, 454569900}},  // 174
    // {"nrnk1rbb/p1p2ppp/3pq3/Qp2p3/1P1P4/8/P1P1PPPP/NRN1KRBB w fb - 2 9", {28,
    // 873, 25683, 791823, 23868737, 747991356}},  // 175
    // {"bbnrnkrq/pp1ppp1p/6p1/2p5/6P1/P5RP/1PPPPP2/BBNRNK1Q w Dgd - 3 9", {37,
    // 1260, 45060, 1542086, 54843403, 1898432768}},  // 176
    // {"bnrb1rkq/ppnpppp1/3Q4/2p4p/7P/N7/PPPPPPP1/B1RBNKR1 w GC - 2 9", {38,
    // 878, 31944, 800440, 28784300, 784569826}},  // 177
    // {"bnrnkbrq/p1ppppp1/1p5p/8/P2PP3/5P2/1PP3PP/BNRNKBRQ w GCgc - 1 9", {26,
    // 617, 16992, 419099, 11965544, 311309576}},  // 178
    // {"bnrnkrqb/pp2p2p/2pp1pp1/8/P7/2PP1P2/1P2P1PP/BNRNKRQB w FCfc - 0 9",
    // {26, 721, 19726, 560824, 15966934, 467132503}},  // 179
    // {"nbbrnkr1/1pppp1p1/p6q/P4p1p/8/5P2/1PPPP1PP/NBBRNRKQ w gd - 2 9", {18,
    // 556, 10484, 316634, 6629293, 202528241}},  // 180
    // {"nrb1nkrq/2pp1ppp/p4b2/1p2p3/P4B2/3P4/1PP1PPPP/NR1BNRKQ w gb - 0 9",
    // {24, 562, 14017, 355433, 9227883, 247634489}},  // 181
    // {"nrbnkbrq/p3p1pp/1p6/2pp1P2/8/3PP3/PPP2P1P/NRBNKBRQ w GBgb - 0 9", {31,
    // 746, 24819, 608523, 21019301, 542954168}},  // 182
    // {"nrbnkrqb/pppp1p1p/4p1p1/8/7P/2P1P3/PPNP1PP1/1RBNKRQB w FBfb - 0 9",
    // {20, 459, 9998, 242762, 5760165, 146614723}},  // 183
    // {"nbrn1krq/ppp1p2p/6b1/3p1pp1/8/4N1PP/PPPPPP2/NBR1BRKQ w gc - 1 9", {27,
    // 835, 23632, 766397, 22667987, 760795567}},  // 184
    // {"nrnbbkrq/p1pp2pp/5p2/1p6/2P1pP1B/1P6/P2PP1PP/NRNB1KRQ w GBgb - 0 9",
    // {24, 646, 16102, 444472, 11489727, 324948755}},  // 185
    // {"nrn1bbrq/1ppkppp1/p2p3p/8/1P3N2/4P3/P1PP1PPP/NR1KBBRQ w GB - 2 9", {32,
    // 591, 18722, 381683, 12069159, 269922838}},  // 186
    // {"n1krbrqb/1ppppppp/p7/8/4n3/P4P1P/1PPPPQP1/NRNKBR1B w FB - 2 9", {26,
    // 639, 16988, 417190, 12167153, 312633873}},  // 187
    // {"n1rnkrbq/1p1ppp1p/8/p1p1b1p1/3PQ1P1/4N3/PPP1PP1P/NBR1KRB1 w FCfc - 0
    // 9", {35, 1027, 35731, 1040417, 35738410, 1060661628}},  // 188
    // {"nrnbkrbq/2pp1pp1/pp6/4p2p/P7/5PPP/1PPPP3/NRNBKRBQ w FBfb - 0 9", {26,
    // 628, 16731, 436075, 11920087, 331498921}},  // 189
    // {"1rnkrbbq/pp1p2pp/1n3p2/1Bp1p3/1P6/1N2P3/P1PP1PPP/1RNKR1BQ w EBeb - 0
    // 9", {33, 992, 32244, 983481, 31703749, 980306735}},  // 190
    // {"nr1krqbb/p1ppppp1/8/1p5p/1Pn5/5P2/P1PPP1PP/NRNKRQBB w EBeb - 0 9", {24,
    // 670, 15985, 445492, 11371067, 325556465}},  // 191
    // {"bbq1rkr1/1ppppppp/p1n2n2/8/2P2P2/1P6/PQ1PP1PP/BB1NRKNR w HEe - 3 9",
    // {32, 794, 26846, 689334, 24085223, 645633370}},  // 192
    // {"b1nbrknr/1qppp1pp/p4p2/1p6/6P1/P2NP3/1PPP1P1P/BQ1BRKNR w HEhe - 1 9",
    // {25, 663, 17138, 482994, 13157826, 389603029}},  // 193
    // {"bqnrk1nr/pp2ppbp/6p1/2pp4/2P5/5P2/PPQPP1PP/B1NRKBNR w HDhd - 0 9", {26,
    // 850, 22876, 759768, 21341087, 719712622}},  // 194
    // {"bqnrknrb/1ppp1p1p/p7/6p1/1P2p3/P1PN4/3PPPPP/BQ1RKNRB w GDgd - 0 9",
    // {25, 721, 19290, 581913, 16391601, 511725087}},  // 195
    // {"q1b1rknr/pp1pppp1/4n2p/2p1b3/1PP5/4P3/PQ1P1PPP/1BBNRKNR w HEhe - 1 9",
    // {32, 975, 32566, 955493, 32649943, 962536105}},  // 196
    // {"qnbbrknr/1p1ppppp/8/p1p5/5P2/PP1P4/2P1P1PP/QNBBRKNR w HEhe - 0 9", {27,
    // 573, 16331, 391656, 11562434, 301166330}},  // 197
    // {"q1brkb1r/p1pppppp/np3B2/8/6n1/1P5N/P1PPPPPP/QN1RKB1R w HDhd - 0 9",
    // {32, 984, 31549, 1007217, 32597704, 1075429389}},  // 198
    // {"qn1rk1rb/p1pppppp/1p2n3/8/2b5/4NPP1/PPPPP1RP/QNBRK2B w Dgd - 4 9", {22,
    // 802, 19156, 697722, 17761431, 650603534}},  // 199
    // {"qbnrbknr/ppp2p1p/8/3pp1p1/1PP1B3/5N2/P2PPPPP/Q1NRBK1R w HDhd - 0 9",
    // {34, 943, 32506, 930619, 32523099, 955802240}},  // 200
    // {"qnrbb1nr/pp1p1ppp/2p2k2/4p3/4P3/5PPP/PPPP4/QNRBBKNR w HC - 0 9", {20,
    // 460, 10287, 241640, 5846781, 140714047}},  // 201
    // {"qnr1bbnr/ppk1p1pp/3p4/2p2p2/8/2P5/PP1PPPPP/QNKRBBNR w - - 1 9", {19,
    // 572, 11834, 357340, 7994547, 243724815}},  // 202
    // {"qnrkbnrb/1p1p1ppp/2p5/4p3/p7/N1BP4/PPP1PPPP/Q1R1KNRB w gc - 0 9", {27,
    // 579, 16233, 375168, 10845146, 268229097}},  // 203
    // {"qbnrkn1r/1pppp1p1/p3bp2/2BN3p/8/5P2/PPPPP1PP/QBNRK2R w HDhd - 0 9",
    // {40, 1027, 38728, 1059229, 38511307, 1104094381}},  // 204
    // {"qnrbknbr/1pp2ppp/4p3/p6N/2p5/8/PPPPPPPP/Q1RBK1BR w HChc - 0 9", {22,
    // 510, 11844, 300180, 7403327, 200581103}},  // 205
    // {"1qkrnbbr/p1pppppp/2n5/1p6/8/5NP1/PPPPPP1P/QNRK1BBR w HC - 4 9", {24,
    // 549, 13987, 352037, 9396521, 255676649}},  // 206
    // {"q1rknr1b/1ppppppb/2n5/p2B3p/8/1PN3P1/P1PPPP1P/Q1RKNRB1 w FCfc - 3 9",
    // {31, 924, 28520, 861944, 27463479, 847726572}},  // 207
    // {"bbnqrk1r/pp1pppp1/2p4p/8/6n1/1N1P1P2/PPP1P1PP/BBQ1RKNR w HEhe - 4 9",
    // {24, 804, 20147, 666341, 18024195, 595947631}},  // 208
    // {"bn1brknr/ppp1p1pp/5p2/3p4/6qQ/3P3P/PPP1PPP1/BN1BRKNR w HEhe - 4 9",
    // {25, 854, 22991, 704173, 20290974, 600195008}},  // 209
    // {"1nqrkbnr/2pp1ppp/pp2p3/3b4/2P5/N7/PP1PPPPP/B1QRKBNR w HDhd - 0 9", {22,
    // 651, 16173, 479152, 13133439, 390886040}},  // 210
    // {"bnqrk1rb/1pp1pppp/p2p4/4n3/2PPP3/8/PP3PPP/BNQRKNRB w GDgd - 1 9", {30,
    // 950, 28169, 889687, 27610213, 880739164}},  // 211
    // {"nbb1rknr/1ppq1ppp/3p4/p3p3/4P3/1N2R3/PPPP1PPP/1BBQ1KNR w Hhe - 2 9",
    // {33, 988, 31293, 967575, 30894863, 985384035}},  // 212
    // {"nqbbrknr/2ppp2p/pp4p1/5p2/7P/3P1P2/PPPBP1P1/NQ1BRKNR w HEhe - 0 9",
    // {27, 492, 13266, 276569, 7583292, 175376176}},  // 213
    // {"1qbrkb1r/pppppppp/8/3n4/4P1n1/PN6/1PPP1P1P/1QBRKBNR w HDhd - 3 9", {28,
    // 800, 21982, 630374, 17313279, 507140861}},  // 214
    // {"1qbrknrb/1p1ppppp/1np5/8/p4P1P/4P1N1/PPPP2P1/NQBRK1RB w GDgd - 0 9",
    // {21, 482, 10581, 267935, 6218644, 168704845}},  // 215
    // {"nbqrbkr1/ppp1pppp/8/3p4/6n1/2P2PPN/PP1PP2P/NBQRBK1R w HDd - 1 9", {29,
    // 921, 25748, 840262, 24138518, 806554650}},  // 216
    // {"nqrb1knr/1ppbpp1p/p7/3p2p1/2P3P1/5P1P/PP1PP3/NQRBBKNR w HChc - 1 9",
    // {31, 803, 25857, 665799, 21998733, 583349773}},  // 217
    // {"1qrkbbr1/pppp1ppp/1n3n2/4p3/5P2/1N6/PPPPP1PP/1QRKBBNR w HCc - 0 9",
    // {25, 715, 19118, 556325, 15514933, 459533767}},  // 218
    // {"nqrkb1rb/pp2pppp/2p1n3/3p4/3PP1N1/8/PPP2PPP/NQRKB1RB w GCgc - 0 9",
    // {26, 795, 21752, 679387, 19185851, 616508881}},  // 219
    // {"nb1rknbr/pp2ppp1/8/2Bp3p/6P1/2P2P1q/PP1PP2P/NBQRKN1R w HDhd - 0 9",
    // {35, 1391, 43025, 1726888, 53033675, 2139267832}},  // 220
    // {"nqrbkn1r/pp1pp1pp/8/2p2p2/5P2/P3B2P/1PbPP1P1/NQRBKN1R w HChc - 0 9",
    // {23, 758, 19439, 653854, 18296195, 628403401}},  // 221
    // {"nqrknbbr/pp1pppp1/7p/2p5/7P/1P1N4/P1PPPPPB/NQRK1B1R w HChc - 2 9", {29,
    // 824, 23137, 683686, 19429491, 595493802}},  // 222
    // {"1qrknrbb/B1p1pppp/8/1p1p4/2n2P2/1P6/P1PPP1PP/NQRKNR1B w FCfc - 0 9",
    // {28, 771, 20237, 581721, 16065378, 483037840}},  // 223
    // {"bbnrqk1r/1ppppppp/8/7n/1p6/P6P/1BPPPPP1/1BNRQKNR w HDhd - 0 9", {25,
    // 601, 15471, 396661, 10697065, 289472497}},  // 224
    // {"bnrbqknr/ppp3p1/3ppp1Q/7p/3P4/1P6/P1P1PPPP/BNRB1KNR w HChc - 0 9", {32,
    // 845, 26876, 742888, 23717883, 682154649}},  // 225
    // {"bn1qkb1r/pprppppp/8/2p5/2PPP1n1/8/PPR2PPP/BN1QKBNR w Hh - 1 9", {32,
    // 856, 27829, 768595, 25245957, 727424329}},  // 226
    // {"1nrqknrb/p1pp1ppp/1p2p3/3N4/5P1P/5b2/PPPPP3/B1RQKNRB w GCgc - 2 9",
    // {33, 873, 27685, 779473, 25128076, 745401024}},  // 227
    // {"nbbrqrk1/pppppppp/8/2N1n3/P7/6P1/1PPPPP1P/1BBRQKNR w HD - 3 9", {25,
    // 555, 14339, 342296, 9153089, 234841945}},  // 228
    // {"1rbbqknr/1ppp1pp1/1n2p3/p6p/4P1P1/P6N/1PPP1P1P/NRBBQK1R w HBhb - 0 9",
    // {25, 693, 18652, 528070, 15133381, 439344945}},  // 229
    // {"nrq1kbnr/p1pbpppp/3p4/1p6/6P1/1N3N2/PPPPPP1P/1RBQKB1R w HBhb - 4 9",
    // {24, 648, 16640, 471192, 12871967, 380436777}},  // 230
    // {"nr1qknr1/p1pppp1p/b5p1/1p6/8/P4PP1/1bPPP1RP/NRBQKN1B w Bgb - 0 9", {18,
    // 533, 11215, 331243, 7777833, 234905172}},  // 231
    // {"nbrqbknr/1ppp2pp/8/4pp2/p2PP1P1/7N/PPP2P1P/NBRQBK1R w HChc - 0 9", {29,
    // 803, 24416, 706648, 22305910, 672322762}},  // 232
    // {"nr1b1k1r/ppp1pppp/2bp1n2/6P1/2P3q1/5P2/PP1PP2P/NRQBBKNR w HBhb - 1 9",
    // {27, 1199, 30908, 1296241, 35121759, 1418677099}},  // 233
    // {"nrqkbbnr/2pppp1p/p7/1p6/2P1Pp2/8/PPNP2PP/1RQKBBNR w HBhb - 0 9", {28,
    // 613, 17874, 432750, 13097064, 345294379}},  // 234
    // {"1rqkbnrb/pp1ppp1p/1n4p1/B1p5/3PP3/4N3/PPP2PPP/NRQK2RB w GBgb - 0 9",
    // {33, 723, 23991, 590970, 19715083, 535650233}},  // 235
    // {"nbrqkn1r/1pppp2p/5pp1/p2b4/5P2/P2PN3/1PP1P1PP/NBRQK1BR w HChc - 2 9",
    // {23, 607, 15482, 400970, 11026383, 290708878}},  // 236
    // {"nrqbknbr/pp1pppp1/8/2p4p/P3PP2/8/1PPP2PP/NRQBKNBR w HBhb - 1 9", {26,
    // 700, 19371, 556026, 16058815, 485460242}},  // 237
    // {"nrqknbbr/p2pppp1/1pp5/6Qp/3P4/1P3P2/P1P1P1PP/NR1KNBBR w HBhb - 0 9",
    // {40, 905, 32932, 829746, 29263502, 791963709}},  // 238
    // {"nrqknrbb/1p3ppp/p2p4/2p1p3/1P6/3PP1P1/P1P2P1P/NRQKNRBB w FBfb - 0 9",
    // {29, 780, 22643, 654495, 19532077, 593181101}},  // 239
    // {"1bnrkqnr/p1pppp2/7p/1p4p1/4b3/7N/PPPP1PPP/BBNRKQ1R w HDhd - 0 9", {25,
    // 725, 19808, 565006, 16661676, 487354613}},  // 240
    // {"bnrbkq1r/pp2p1pp/5n2/2pp1p2/P7/N1PP4/1P2PPPP/B1RBKQNR w HChc - 1 9",
    // {24, 745, 18494, 584015, 15079602, 488924040}},  // 241
    {"2rkqbnr/p1pppppp/2b5/1pn5/1P3P1Q/2B5/P1PPP1PP/1NRK1BNR w HChc - 3 9",
     {33, 904, 30111, 840025, 28194726, 801757709}},  // 242
    // {"bnrkqnrb/2pppp2/8/pp4pp/1P5P/6P1/P1PPPPB1/BNRKQNR1 w GCgc - 0 9", {34,
    // 1059, 34090, 1054311, 33195397, 1036498304}},  // 243
    // {"1bbrkq1r/pppp2pp/1n2pp1n/8/2PP4/1N4P1/PP2PP1P/1BBRKQNR w HDhd - 1 9",
    // {33, 891, 28907, 814247, 26970098, 788040469}},  // 244
    // {"nrbbkqnr/1p2pp1p/p1p3p1/3p4/8/1PP5/P2PPPPP/NRBBKQNR w HBhb - 0 9", {21,
    // 567, 13212, 376487, 9539687, 284426039}},  // 245
    // {"1rbkqbr1/ppp1pppp/1n5n/3p4/3P4/1PP3P1/P3PP1P/NRBKQBNR w HBb - 1 9",
    // {27, 752, 20686, 606783, 16986290, 521817800}},  // 246
    // {"nrbkq1rb/1ppp1pp1/4p1n1/p6p/2PP4/5P2/PPK1P1PP/NRB1QNRB w gb - 0 9",
    // {35, 697, 23678, 505836, 16906409, 390324794}},  // 247
    // {"nbrkbqnr/p2pp1p1/5p2/1pp4p/7P/3P2P1/PPP1PP2/NBKRBQNR w hc - 0 9", {25,
    // 679, 17223, 484921, 12879258, 376652259}},  // 248
    // {"nrkb1qnr/ppppp1p1/6bp/5p2/1PP1P1P1/8/P2P1P1P/NRKBBQNR w HBhb - 1 9",
    // {32, 761, 24586, 632916, 20671433, 568524724}},  // 249
    // {"nrk1bbnr/p1q1pppp/1ppp4/8/3P3P/4K3/PPP1PPP1/NR1QBBNR w hb - 0 9", {30,
    // 719, 21683, 541389, 16278120, 423649784}},  // 250
    // {"nrkqbr1b/1pppp1pp/5pn1/p6N/1P3P2/8/P1PPP1PP/NRKQB1RB w GBb - 0 9", {26,
    // 494, 13815, 296170, 8763742, 206993496}},  // 251
    // {"nbrkq2r/pppp1bpp/4p1n1/5p2/7P/2P3N1/PP1PPPP1/NBKRQ1BR w hc - 0 9", {27,
    // 701, 19536, 535052, 15394667, 443506342}},  // 252
    // {"nrkbqnbr/2ppp2p/pp6/5pp1/P1P5/8/1P1PPPPP/NRKBQNBR w HBhb - 0 9", {21,
    // 487, 11341, 285387, 7218486, 193586674}},  // 253
    // {"nr1qnbbr/pk1pppp1/1pp4p/8/3P4/5P1P/PPP1P1P1/NRKQNBBR w HB - 0 9", {22,
    // 546, 13615, 352855, 9587439, 259830255}},  // 254
    // {"nrkq1rbb/pp1ppp1p/2pn4/8/PP3Pp1/7P/2PPP1P1/NRKQNRBB w FBfb - 0 9", {26,
    // 839, 22075, 723845, 19867117, 658535326}},  // 255
    // {"b2rknqr/pp1ppppp/8/2P5/n7/P7/1PPNPPPb/BBNRK1QR w HDhd - 2 9", {24, 699,
    // 19523, 575172, 17734818, 535094237}},  // 256
    // {"bnrbknqr/pp2p2p/2p3p1/3p1p2/8/3P4/PPPNPPPP/B1RBKNQR w HChc - 0 9", {23,
    // 580, 14320, 385917, 10133092, 288041554}},  // 257
    // {"bnrknb1r/pppp2pp/8/4pp2/6P1/3P3P/qPP1PPQ1/BNRKNB1R w HChc - 0 9", {28,
    // 1100, 31813, 1217514, 36142423, 1361341249}},  // 258
    // {"b1rknqrb/ppp1p1p1/2np1p1p/8/4N3/6PQ/PPPPPP1P/B1RKN1RB w GCgc - 0 9",
    // {36, 629, 23082, 453064, 16897544, 367503974}},  // 259
    // {"nb1rknqr/pbppp2p/6p1/1p3p2/5P2/3KP3/PPPP2PP/NBBR1NQR w hd - 2 9", {18,
    // 557, 9779, 300744, 5822387, 180936551}},  // 260
    // {"nr1bknqr/1ppb1ppp/p7/3pp3/B7/2P3NP/PP1PPPP1/NRB1K1QR w HBhb - 2 9",
    // {28, 688, 19541, 519785, 15153092, 425149249}},  // 261
    // {"nrbkn2r/pppp1pqp/4p1p1/8/3P2P1/P3B3/P1P1PP1P/NR1KNBQR w HBhb - 1 9",
    // {32, 808, 25578, 676525, 22094260, 609377239}},  // 262
    // {"nrbknqrb/2p1ppp1/1p6/p2p2Bp/1P6/3P1P2/P1P1P1PP/NR1KNQRB w GBgb - 0 9",
    // {30, 625, 18288, 418895, 12225742, 301834282}},  // 263
    // {"nbr1knqr/1pp1p1pp/3p1pb1/8/7P/5P2/PPPPPQP1/NBRKBN1R w HC - 2 9", {29,
    // 863, 25767, 800239, 24965592, 799182442}},  // 264
    // {"n1kbbnqr/prp2ppp/1p1p4/4p3/1P2P3/3P1B2/P1P2PPP/NRK1BNQR w HBh - 2 9",
    // {26, 653, 17020, 449719, 12187583, 336872952}},  // 265
    // {"nrknbbqr/pp3p1p/B3p1p1/2pp4/4P3/2N3P1/PPPP1P1P/NRK1B1QR w HBhb - 0 9",
    // {29, 683, 19755, 501807, 14684565, 394951291}},  // 266
    // {"n1knbqrb/pr1p1ppp/Qp6/2p1p3/4P3/6P1/PPPP1P1P/NRKNB1RB w GBg - 2 9",
    // {31, 552, 17197, 371343, 11663330, 283583340}},  // 267
    // {"nbrknqbr/p3p1pp/1p1p1p2/2p5/2Q1PP2/8/PPPP2PP/NBRKN1BR w HChc - 0 9",
    // {37, 913, 32470, 825748, 28899548, 759875563}},  // 268
    // {"nrkb1qbr/pp1pppp1/5n2/7p/2p5/1N1NPP2/PPPP2PP/1RKB1QBR w HBhb - 0 9",
    // {25, 712, 18813, 543870, 15045589, 445074372}},  // 269
    // {"nrk2bbr/pppqpppp/3p4/8/1P3nP1/3P4/P1P1PP1P/NRKNQBBR w HBhb - 1 9", {24,
    // 814, 19954, 670162, 17603960, 592121050}},  // 270
    // {"nrknqrbb/1p2ppp1/2pp4/Q6p/P2P3P/8/1PP1PPP1/NRKN1RBB w FBfb - 0 9", {34,
    // 513, 16111, 303908, 9569590, 206509331}},  // 271
    // {"bbnrk1rq/pp2p1pp/2ppn3/5p2/8/3NNP1P/PPPPP1P1/BB1RK1RQ w GDgd - 1 9",
    // {28, 697, 20141, 517917, 15301879, 410843713}},  // 272
    // {"bnrbknrq/ppppp2p/6p1/5p2/4QPP1/8/PPPPP2P/BNRBKNR1 w GCgc - 0 9", {37,
    // 901, 32612, 877372, 31385912, 903831981}},  // 273
    // {"bnkrnbrq/ppppp1p1/B6p/5p2/8/4P3/PPPP1PPP/BNKRN1RQ w - - 0 9", {26, 417,
    // 11124, 217095, 5980981, 133080499}},  // 274
    // {"bnrk1rqb/2pppp1p/3n4/pp4p1/3Q1P2/2N3P1/PPPPP2P/B1RKNR1B w FCfc - 0 9",
    // {49, 1655, 74590, 2512003, 107234294, 3651608327}},  // 275
    // {"nbbrk1rq/pp2pppp/2pp4/8/2P2n2/6N1/PP1PP1PP/NBBRKR1Q w Dgd - 0 9", {28,
    // 960, 26841, 884237, 26083252, 846682836}},  // 276
    // {"nrbb2rq/pppk1ppp/4p1n1/3p4/6P1/1BP5/PP1PPPQP/NRB1KNR1 w GB - 0 9", {28,
    // 735, 22048, 593839, 18588316, 512048946}},  // 277
    // {"nrbk1brq/p1ppppp1/7p/1p6/4P1nP/P7/1PPP1PP1/NRBKNBRQ w GBgb - 0 9", {22,
    // 572, 12739, 351494, 8525056, 247615348}},  // 278
    // {"nrbk1rqb/1pp2ppp/5n2/p2pp3/5B2/1N1P2P1/PPP1PP1P/1R1KNRQB w FBfb - 0 9",
    // {35, 927, 31559, 849932, 28465693, 783048748}},  // 279
    // {"nbrkb1rq/p1pp1ppp/4n3/4p3/Pp6/6N1/1PPPPPPP/NBRKBRQ1 w Cgc - 0 9", {20,
    // 456, 10271, 247733, 6124625, 154766108}},  // 280
    // {"nrkb1nrq/p2pp1pp/1pp2p2/7b/6PP/5P2/PPPPP2N/NRKBB1RQ w GBgb - 0 9", {21,
    // 479, 11152, 264493, 6696458, 165253524}},  // 281
    // {"nr1nbbr1/pppkpp1p/6p1/3p4/P6P/1P6/1RPPPPP1/N1KNBBRQ w G - 1 9", {20,
    // 498, 11304, 288813, 7197322, 188021682}},  // 282
    // {"nrknbrqb/3p1ppp/ppN1p3/8/6P1/8/PPPPPP1P/1RKNBRQB w FBfb - 0 9", {32,
    // 526, 17267, 319836, 10755190, 220058991}},  // 283
    // {"nbrkn1bq/p1pppr1p/1p6/5pp1/8/1N2PP2/PPPP2PP/1BKRNRBQ w c - 1 9", {19,
    // 491, 10090, 277313, 6230616, 180748649}},  // 284
    // {"nrkbnrbq/ppppppp1/8/8/7p/PP3P2/2PPPRPP/NRKBN1BQ w Bfb - 0 9", {16, 353,
    // 6189, 156002, 3008668, 82706705}},  // 285
    // {"nrknrbbq/p4ppp/2p1p3/1p1p4/1P2P3/2P5/P1NP1PPP/1RKNRBBQ w EBeb - 0 9",
    // {29, 728, 21915, 587668, 18231199, 511686397}},  // 286
    // {"nrknr1bb/pppp1p2/7p/2qPp1p1/8/1P5P/P1P1PPP1/NRKNRQBB w EBeb - 0 9",
    // {20, 714, 14336, 500458, 11132758, 386064577}},  // 287
    // {"bbqnrrkn/ppp2p1p/3pp1p1/8/1PP5/2Q5/P1BPPPPP/B2NRKRN w GE - 0 9", {39,
    // 593, 23446, 424799, 16764576, 346185058}},  // 288
    // {"bqn1rkrn/p1p2ppp/1p1p4/4p3/3PP2b/8/PPP2PPP/BQNBRKRN w GEge - 2 9", {25,
    // 773, 20042, 616817, 16632403, 515838333}},  // 289
    // {"bqnrkb1n/p1p1pprp/3p4/1p2P1p1/2PP4/8/PP3PPP/BQNRKBRN w GDd - 1 9", {31,
    // 860, 28102, 810379, 27233018, 813751250}},  // 290
    // {"bqr1krnb/ppppppp1/7p/3n4/1P4P1/P4N2/2PPPP1P/BQNRKR1B w FDf - 3 9", {31,
    // 709, 22936, 559830, 18608857, 480498340}},  // 291
    // {"qbbn1krn/pp3ppp/4r3/2ppp3/P1P4P/8/1P1PPPP1/QBBNRKRN w GEg - 1 9", {26,
    // 775, 21100, 649673, 18476807, 582542257}},  // 292
    // {"qnbbrkrn/1p1pp2p/p7/2p2pp1/8/4P2P/PPPP1PPK/QNBBRR1N w ge - 0 9", {25,
    // 599, 15139, 389104, 10260500, 279222412}},  // 293
    // {"qnbrkbrn/1ppp2p1/p3p2p/5p2/P4P2/1P6/2PPP1PP/QNBRKBRN w GDgd - 0 9",
    // {27, 588, 16735, 394829, 11640416, 293541380}},  // 294
    // {"1nbrkrnb/p1pppp1p/1pq3p1/8/4P3/P1P4N/1P1P1PPP/QNBRKR1B w FDfd - 1 9",
    // {18, 609, 11789, 406831, 8604788, 299491047}},  // 295
    // {"qb1r1krn/pppp2pp/1n2ppb1/4P3/7P/8/PPPP1PP1/QBNRBKRN w GDgd - 0 9", {20,
    // 578, 12205, 349453, 7939483, 229142178}},  // 296
    // {"qnr1bkrn/p3pppp/1bpp4/1p6/2P2PP1/8/PP1PPN1P/QNRBBKR1 w GCgc - 0 9",
    // {30, 865, 26617, 771705, 24475596, 719842237}},  // 297
    // {"1nkrbbrn/qppppppp/8/8/p2P4/1P5P/P1P1PPP1/QNKRBBRN w - - 0 9", {27, 672,
    // 18371, 505278, 14065717, 410130412}},  // 298
    // {"1qrkbrnb/ppp1p1pp/n2p4/5p2/4N3/8/PPPPPPPP/Q1RKBRNB w Ffc - 2 9", {25,
    // 718, 18573, 536771, 14404324, 424279467}},  // 299
    // {"q1nrkrbn/pp1pppp1/2p4p/8/P7/5Pb1/BPPPPNPP/Q1NRKRB1 w FDfd - 0 9", {22,
    // 558, 12911, 336042, 8516966, 228074630}},  // 300
    // {"qnrbkrbn/1p1p1pp1/p1p5/4p2p/8/3P1P2/PPP1P1PP/QNRBKRBN w FCfc - 0 9",
    // {28, 669, 17713, 440930, 12055174, 313276304}},  // 301
    // {"qnrkr1bn/p1pp1ppp/8/1p2p3/3P1P2/bP4P1/P1P1P2P/QNRKRBBN w ECec - 1 9",
    // {23, 845, 20973, 759778, 19939053, 718075943}},  // 302
    // {"q1krrnbb/p1p1pppp/2np4/1pB5/5P2/8/PPPPP1PP/QNRKRN1B w EC - 0 9", {29,
    // 776, 21966, 631941, 18110831, 549019739}},  // 303
    // {"bbn1rkrn/pp1p1ppp/8/2p1p1q1/6P1/P7/BPPPPP1P/B1NQRKRN w GEge - 0 9",
    // {26, 936, 25177, 906801, 24984621, 901444251}},  // 304
    // {"bn1brkrn/pp1qpp1p/2p3p1/3p4/1PPP4/P7/4PPPP/BNQBRKRN w GEge - 1 9", {29,
    // 755, 22858, 645963, 20128587, 600207069}},  // 305
    // {"b2rkbrn/p1pppppp/qp6/8/1n6/2B2P2/P1PPP1PP/1NQRKBRN w GDgd - 0 9", {24,
    // 878, 21440, 791007, 20840078, 775795187}},  // 306
    // {"b2rkrnb/pqp1pppp/n7/1p1p4/P7/N1P2N2/1P1PPPPP/B1QRKR1B w FDfd - 4 9",
    // {26, 724, 19558, 571891, 16109522, 492933398}},  // 307
    // {"1bbqrkrn/ppppp1p1/8/5p1p/P1n3P1/3P4/1PP1PP1P/NBBQRRKN w ge - 1 9", {25,
    // 678, 17351, 461211, 12173245, 329661421}},  // 308
    // {"nqb1rrkn/ppp1bppp/3pp3/8/3P4/1P6/PQP1PPPP/N1BBRRKN w - - 1 9", {23,
    // 503, 12465, 290341, 7626054, 188215608}},  // 309
    // {"nqbrkbr1/p1pppppp/1p6/2N2n2/2P5/5P2/PP1PP1PP/1QBRKBRN w GDgd - 1 9",
    // {29, 688, 20289, 506302, 15167248, 399015237}},  // 310
    // {"nqbrkrn1/1ppppp2/6pp/p7/1P6/2Q5/P1PPPPPP/N1BRKRNB w FDfd - 0 9", {36,
    // 602, 20985, 397340, 13706856, 291708797}},  // 311
    // {"nbqrbrkn/pp1p1pp1/2p5/4p2p/2P3P1/1P3P2/P2PP2P/NBQRBKRN w GD - 0 9",
    // {34, 655, 22581, 474396, 16613630, 379344541}},  // 312
    // {"nqrbbrkn/1p1pppp1/8/p1p4p/4P2P/1N4P1/PPPP1P2/1QRBBKRN w GC - 0 9", {23,
    // 597, 14468, 400357, 10096863, 294900903}},  // 313
    // {"nqrkbbrn/2p1p1pp/pp1p1p2/8/P2N4/2P5/1P1PPPPP/1QRKBBRN w GCgc - 0 9",
    // {32, 744, 23310, 550728, 17597164, 428786656}},  // 314
    // {"n1krbrnb/q1pppppp/p7/1p6/3Q4/2P2P2/PP1PP1PP/N1RKBRNB w FC - 1 9", {43,
    // 1038, 41327, 1074450, 40918952, 1126603824}},  // 315
    // {"nb1rkrbn/p1pp1p1p/qp6/4p1p1/5PP1/P7/1PPPPB1P/NBQRKR1N w FDfd - 2 9",
    // {26, 645, 16463, 445464, 11911314, 342563372}},  // 316
    // {"nqr1krbn/pppp1ppp/8/8/3pP3/5P2/PPPb1NPP/NQRBKRB1 w FCfc - 3 9", {2, 51,
    // 1047, 27743, 612305, 17040200}},  // 317
    // {"n1rkrbbn/pqppppp1/7p/1p6/8/1NPP4/PP1KPPPP/1QR1RBBN w ec - 0 9", {25,
    // 674, 17553, 505337, 13421727, 403551903}},  // 318
    {"1qrkrnbb/1p1p1ppp/pnp1p3/8/3PP3/P6P/1PP2PP1/NQRKRNBB w ECec - 0 9",
     {24, 688, 17342, 511444, 13322502, 403441498}},  // 319
    // {"1bnrqkrn/2ppppp1/p7/1p1b3p/3PP1P1/8/PPPQ1P1P/BBNR1KRN w GDgd - 1 9",
    // {35, 925, 32238, 857060, 30458921, 824344087}},  // 320
    // {"bnrbqkr1/ppp2pp1/6n1/3pp2p/1P6/2N3N1/P1PPPPPP/B1RBQRK1 w gc - 0 9",
    // {23, 704, 17345, 539587, 14154852, 450893738}},  // 321
    // {"1nrqkbrn/p1pppppp/8/1p1b4/P6P/5P2/1PPPP1P1/BNRQKBRN w GCgc - 1 9", {19,
    // 505, 10619, 281422, 6450025, 175593967}},  // 322
    // {"b1rqkrnb/ppppppp1/8/6p1/3n4/NP6/P1PPPP1P/B1RQKRNB w FCfc - 0 9", {25,
    // 614, 15578, 377660, 10391021, 259629603}},  // 323
    // {"nbbrqkrn/ppp3p1/3pp3/5p1p/1P2P3/P7/2PPQPPP/NBBR1KRN w GDgd - 0 9", {30,
    // 833, 25719, 717713, 22873901, 649556666}},  // 324
    // {"nr1bqrk1/ppp1pppp/6n1/3pP3/8/5PQb/PPPP2PP/NRBB1KRN w GB - 3 9", {26,
    // 734, 20161, 582591, 17199594, 512134836}},  // 325
    // {"1rbqkbr1/ppppp1pp/1n6/4np2/3P1P2/6P1/PPPQP2P/NRB1KBRN w GBgb - 1 9",
    // {27, 662, 17897, 447464, 13038519, 338365642}},  // 326
    // {"nr1qkr1b/ppp1pp1p/4bn2/3p2p1/4P3/1Q6/PPPP1PPP/NRB1KRNB w FBfb - 4 9",
    // {33, 939, 30923, 942138, 30995969, 991509814}},  // 327
    // {"nb1qbkrn/pprp1pp1/7p/2p1pB2/Q1PP4/8/PP2PPPP/N1R1BKRN w GCg - 2 9", {47,
    // 1128, 50723, 1306753, 56747878, 1560584212}},  // 328
    // {"nrqb1rkn/pp2pppp/2bp4/2p5/6P1/2P3N1/PP1PPP1P/NRQBBRK1 w - - 3 9", {24,
    // 828, 21148, 723705, 19506135, 668969549}},  // 329
    // {"nrq1bbrn/ppkpp2p/2p3p1/P4p2/8/4P1N1/1PPP1PPP/NRQKBBR1 w GB - 0 9", {25,
    // 525, 13533, 309994, 8250997, 201795680}},  // 330
    // {"Br1kbrn1/pqpppp2/8/6pp/3b2P1/1N6/PPPPPP1P/1RQKBRN1 w FBfb - 3 9", {20,
    // 790, 18175, 695905, 17735648, 669854148}},  // 331
    // {"nbrqkrbn/2p1p1pp/p7/1p1p1p2/4P1P1/5P2/PPPP3P/NBRQKRBN w FCfc - 0 9",
    // {29, 771, 22489, 647106, 19192982, 591335970}},  // 332
    // {"1rqbkrbn/1ppppp1p/1n6/p1N3p1/8/2P4P/PP1PPPP1/1RQBKRBN w FBfb - 0 9",
    // {29, 502, 14569, 287739, 8652810, 191762235}},  // 333
    // {"1rqkrbbn/ppnpp1pp/8/2p5/6p1/3P4/PPP1PPPP/NRK1RBBN w eb - 0 9", {19,
    // 531, 10812, 300384, 6506674, 184309316}},  // 334
    // {"nrqkrnbb/p1pp2pp/5p2/4P3/2p5/4N3/PP1PP1PP/NRQKR1BB w EBeb - 0 9", {26,
    // 800, 23256, 756695, 23952941, 809841274}},  // 335
    // {"bbnrkqrn/pp3pp1/4p2p/2pp4/4P1P1/1PB5/P1PP1P1P/1BNRKQRN w GDgd - 0 9",
    // {33, 915, 30536, 878648, 29602610, 881898159}},  // 336
    // {"bnrbkqr1/1p2pppp/6n1/p1pp4/7P/P3P3/1PPPKPP1/BNRB1QRN w gc - 0 9", {19,
    // 457, 9332, 238944, 5356253, 144653627}},  // 337
    // {"b1rkqbrn/pp1p2pp/2n1p3/2p2p2/3P2PP/8/PPP1PP2/BNKRQBRN w gc - 0 9", {30,
    // 985, 30831, 1011700, 32684185, 1080607773}},  // 338
    // {"b1rkqrnb/2ppppp1/np6/p6p/1P6/P2P3P/2P1PPP1/BNRKQRNB w FCfc - 0 9", {26,
    // 692, 18732, 517703, 14561181, 413226841}},  // 339
    // {"nbbrkqrn/1ppp1p2/p6p/4p1p1/5P2/1P5P/P1PPPNP1/NBBRKQR1 w GDgd - 0 9",
    // {22, 561, 13222, 367487, 9307003, 273928315}},  // 340
    // {"nrbbkqrn/p1pppppp/8/1p6/4P3/7Q/PPPP1PPP/NRBBK1RN w GBgb - 0 9", {38,
    // 769, 28418, 632310, 23091070, 560139600}},  // 341
    // {"nrbkqbrn/1pppp2p/8/p4pp1/P4PQ1/8/1PPPP1PP/NRBK1BRN w GBgb - 0 9", {23,
    // 507, 13067, 321423, 8887567, 237475184}},  // 342
    // {"nr1kqr1b/pp2pppp/5n2/2pp4/P5b1/5P2/1PPPPRPP/NRBK1QNB w Bfb - 2 9", {18,
    // 626, 12386, 434138, 9465555, 335004239}},  // 343
    // {"nbkrbqrn/1pppppp1/8/4P2p/pP6/P7/2PP1PPP/NBRKBQRN w GC - 0 9", {22, 329,
    // 8475, 148351, 4160034, 82875306}},  // 344
    // {"nrkb1qrn/pp1pp1pp/8/5p1b/P1p4P/6N1/1PPPPPP1/NRKBBQR1 w GBgb - 2 9",
    // {16, 479, 9037, 275354, 5862341, 184959796}},  // 345
    // {"1rkq1brn/ppppp1pp/1n6/3b1p2/3N3P/5P2/PPPPP1P1/1RKQBBRN w GBgb - 3 9",
    // {23, 614, 15324, 418395, 11090645, 313526088}},  // 346
    // {"nrk1brnb/pp1ppppp/2p5/3q4/5P2/PP6/1KPPP1PP/NR1QBRNB w fb - 1 9", {25,
    // 942, 21765, 792179, 19318837, 685549171}},  // 347
    // {"nbrkqr1n/1pppp2p/p4pp1/2Bb4/5P2/6P1/PPPPP2P/NBRKQ1RN w Cfc - 2 9", {30,
    // 841, 24775, 677876, 20145765, 557578726}},  // 348
    // {"n1kbqrbn/2p1pppp/1r6/pp1p4/P7/3P4/1PP1PPPP/NRKBQRBN w FBf - 2 9", {21,
    // 591, 14101, 394289, 10295086, 292131422}},  // 349
    // {"nrkqrbb1/ppp1pppp/3p4/8/4P3/2Pn1P2/PP4PP/NRKQRBBN w EBeb - 0 9", {4,
    // 88, 3090, 73414, 2640555, 66958031}},  // 350
    // {"nrkqrnbb/ppppp1p1/7p/1P3p2/3P4/2P5/P3PPPP/NRKQRNBB w EBeb - 0 9", {29,
    // 689, 21091, 508789, 16226660, 408570219}},  // 351
    // {"bbnr1rqn/pp2pkpp/2pp1p2/8/4P1P1/8/PPPP1P1P/BBNRKRQN w FD - 0 9", {21,
    // 463, 11135, 256244, 6826249, 165025370}},  // 352
    // {"bnrbk1qn/1pppprpp/8/p4p1P/6P1/3P4/PPP1PP2/BNRBKRQN w FCc - 0 9", {22,
    // 459, 11447, 268157, 7371098, 190583454}},  // 353
    // {"1nrkrbqn/p1pp1ppp/4p3/1p6/1PP5/6PB/P2PPPbP/BNRKR1QN w ECec - 0 9", {30,
    // 931, 29012, 887414, 28412902, 869228014}},  // 354
    // {"b1rkr1nb/pppppqp1/n4B2/7p/8/1P4P1/P1PPPP1P/1NKRRQNB w ec - 1 9", {36,
    // 934, 31790, 930926, 30392925, 952871799}},  // 355
    // {"nbbrkrqn/p1ppp1p1/8/1p3p1p/2P3PP/8/PP1PPPQ1/NBBRKR1N w FDfd - 0 9",
    // {34, 938, 31848, 921716, 31185844, 944483246}},  // 356
    // {"1rbbkrqn/ppp1pp2/1n1p2p1/7p/P3P1P1/3P4/1PP2P1P/NRBBKRQN w FBfb - 0 9",
    // {26, 646, 18083, 472744, 14006203, 384101783}},  // 357
    // {"nrbkrbq1/Qpppp1pp/2n5/5p2/P4P2/6N1/1PPPP1PP/NRBKRB2 w EBeb - 1 9", {27,
    // 619, 16713, 421845, 11718463, 313794027}},  // 358
    // {"1rbkr1nb/pppp1qpp/1n6/4pp2/1PP1P3/8/PB1P1PPP/NR1KRQNB w EBeb - 1 9",
    // {32, 1029, 32970, 1080977, 35483796, 1181835398}},  // 359
    // {"nbrk1rqn/p1ppp2p/1p6/5ppb/8/1N2P2P/PPPP1PP1/1BKRBRQN w fc - 0 9", {18,
    // 594, 12350, 408544, 9329122, 315021712}},  // 360
    // {"nrkbbrqn/3pppp1/7p/ppp5/P7/1N5P/1PPPPPP1/1RKBBRQN w FBfb - 0 9", {19,
    // 417, 9026, 218513, 5236331, 137024458}},  // 361
    // {"nrkr1bqn/ppp1pppp/3p4/1b6/7P/P7/1PPPPPP1/NRKRBBQN w DBdb - 1 9", {17,
    // 457, 9083, 243872, 5503579, 150091997}},  // 362
    // {"nrkrbqnb/p4ppp/1p2p3/2pp4/6P1/2P2N2/PPNPPP1P/1RKRBQ1B w DBdb - 0 9",
    // {27, 755, 21012, 620093, 17883987, 547233320}},  // 363
    // {"nbkrr1bn/ppB2ppp/4p3/2qp4/4P3/5P2/PPPP2PP/NBRKRQ1N w EC - 1 9", {37,
    // 1473, 51939, 1956521, 68070015, 2490912491}},  // 364
    // {"n1kbrqbn/p1pp1pp1/4p2p/2B5/1r3P2/8/PPPPP1PP/NRKBRQ1N w EBe - 2 9", {30,
    // 1029, 30874, 1053163, 32318550, 1106487743}},  // 365
    // {"nrkrqbbn/2pppp1p/8/pp6/1P1P2p1/P5P1/2P1PP1P/NRKRQBBN w DBdb - 0 9",
    // {22, 421, 10034, 221927, 5754555, 141245633}},  // 366
    // {"nrkr1nbb/1ppp2pp/p3q3/4pp2/2P5/P3P3/1PKP1PPP/NR1RQNBB w db - 0 9", {22,
    // 619, 13953, 411392, 9905109, 301403003}},  // 367
    // {"bbnrkrnq/1pp1p2p/6p1/p2p1p2/8/1P2P3/P1PP1PPP/BBNRKRNQ w FDfd - 0 9",
    // {27, 805, 21915, 688224, 19133881, 620749189}},  // 368
    // {"bnrbkrn1/pp1ppp2/2p3pp/8/2Pq4/P4PP1/1P1PP2P/BNRBKRNQ w FCfc - 1 9",
    // {20, 770, 16593, 577980, 13581691, 456736500}},  // 369
    // {"b1rkrbnq/1pp1pppp/2np4/p5N1/8/1P2P3/P1PP1PPP/BNRKRB1Q w ECec - 0 9",
    // {37, 740, 27073, 581744, 21156664, 485803600}},  // 370
    // {"b1krrnqb/pp1ppp1p/n1p3p1/2N5/6P1/8/PPPPPP1P/B1RKRNQB w EC - 0 9", {34,
    // 850, 28494, 752350, 25360295, 698159474}},  // 371
    // {"1bbr1rnq/ppppkppp/8/3np3/4P3/3P4/PPP1KPPP/NBBRR1NQ w - - 1 9", {27,
    // 704, 18290, 480474, 12817011, 341026662}},  // 372
    // {"nrbbk1nq/p1p1prpp/1p6/N2p1p2/P7/8/1PPPPPPP/R1BBKRNQ w Fb - 2 9", {23,
    // 552, 13710, 348593, 9236564, 248469879}},  // 373
    // {"1rbkrb1q/1pppp1pp/1n5n/p4p2/P3P3/1P6/2PPNPPP/NRBKRB1Q w EBeb - 1 9",
    // {22, 415, 10198, 217224, 5735644, 135295774}},  // 374
    // {"nrbkr1qb/1pp1pppp/6n1/p2p4/2P1P3/1N4N1/PP1P1PPP/1RBKR1QB w EBeb - 0 9",
    // {27, 709, 19126, 506214, 14192779, 380516508}},  // 375
    // {"nbrkbrnq/p3p1pp/1pp2p2/3p4/1PP5/4P3/P1KP1PPP/NBR1BRNQ w fc - 0 9", {24,
    // 715, 18009, 535054, 14322279, 427269976}},  // 376
    // {"nrk1brnq/pp1p1pp1/7p/b1p1p3/1P6/6P1/P1PPPPQP/NRKBBRN1 w FBfb - 2 9",
    // {29, 675, 20352, 492124, 15316285, 389051744}},  // 377
    // {"nrkr1bnq/1p2pppp/p2p4/1bp5/PP6/1R5N/2PPPPPP/N1KRBB1Q w Ddb - 2 9", {27,
    // 744, 20494, 571209, 16188945, 458900901}},  // 378
    // {"nrk1b1qb/pppn1ppp/3rp3/3p4/2P3P1/3P4/PPN1PP1P/1RKRBNQB w DBb - 3 9",
    // {35, 941, 33203, 935791, 33150360, 968024386}},  // 379
    // {"nb1rrnbq/ppkp1ppp/8/2p1p3/P7/1N2P3/1PPP1PPP/1BKRRNBQ w - - 1 9", {19,
    // 451, 9655, 235472, 5506897, 139436165}},  // 380
    // {"nrkbrnbq/4pppp/1ppp4/p7/2P1P3/3P2N1/PP3PPP/NRKBR1BQ w EBeb - 0 9", {29,
    // 591, 17132, 384358, 11245508, 270967202}},  // 381
    // {"nrkrnbbq/3p1ppp/1p6/p1p1p3/3P2P1/P4Q2/1PP1PP1P/NRKRNBB1 w DBdb - 0 9",
    // {38, 792, 28597, 640961, 22654797, 540864616}},  // 382
    // {"nr1rnqbb/ppp1pp1p/3k2p1/3p4/1P5P/3P1N2/P1P1PPP1/NRKR1QBB w DB - 1 9",
    // {25, 758, 18547, 543643, 13890077, 402109399}},  // 383
    // {"bbqrnnkr/1ppp1p1p/5p2/p5p1/P7/1P4P1/2PPPP1P/1BQRNNKR w HDhd - 0 9",
    // {20, 322, 7224, 145818, 3588435, 82754650}},  // 384
    // {"bqrb2k1/pppppppr/5nnp/8/3P1P2/4P1N1/PPP3PP/BQRBN1KR w HCc - 1 9", {25,
    // 597, 15872, 397970, 11162476, 295682250}},  // 385
    // {"bqrnn1kr/1pppbppp/8/4p3/1p6/2P1N2P/P2PPPP1/BQR1NBKR w HChc - 1 9", {34,
    // 921, 31695, 864023, 30126510, 850296236}},  // 386
    // {"bqr1nkr1/pppppp2/2n3p1/7p/1P1b1P2/8/PQP1P1PP/B1RNNKRB w GCgc - 0 9",
    // {23, 788, 21539, 686795, 20849374, 645694580}},  // 387
    // {"qbbrnn1r/1pppp1pk/p7/5p1p/P2P3P/3N4/1PP1PPP1/QBBR1NKR w HD - 0 9", {34,
    // 713, 24475, 562189, 19494094, 482645160}},  // 388
    // {"qrbb2kr/p1pppppp/1p1n4/8/1P3n2/P7/Q1PPP1PP/1RBBNNKR w HBhb - 0 9", {28,
    // 977, 26955, 949925, 27802999, 992109168}},  // 389
    // {"qrb2bkr/1pp1pppp/2np1n2/pN6/3P4/4B3/PPP1PPPP/QR2NBKR w HBhb - 0 9",
    // {27, 730, 20534, 585091, 17005916, 507008968}},  // 390
    // {"qrbnnkrb/pp2pp1p/8/2pp2p1/7P/P1P5/QP1PPPP1/1RBNNKRB w GBgb - 0 9", {24,
    // 813, 21142, 707925, 19615756, 655850285}},  // 391
    // {"1brnb1kr/p1pppppp/1p6/8/4q2n/1P2P1P1/PNPP1P1P/QBR1BNKR w HChc - 3 9",
    // {17, 734, 13462, 530809, 11032633, 416356876}},  // 392
    // {"1rnbbnkr/1pp1pppp/1q1p4/p7/4P3/5PN1/PPPP1BPP/QRNB2KR w HBhb - 1 9",
    // {26, 809, 21764, 706677, 20292750, 675408811}},  // 393
    // {"qrnnbb1Q/ppp1pk1p/3p2p1/5p2/PP6/5P2/2PPP1PP/1RNNBBKR w HB - 0 9", {37,
    // 751, 27902, 603931, 22443036, 515122176}},  // 394
    // {"qrnnbkrb/p3p1pp/3p1p2/1pp5/PP2P3/8/2PP1PPP/QRNNBRKB w gb - 0 9", {30,
    // 906, 27955, 872526, 27658191, 890966633}},  // 395
    {"qbrnnkbr/1p2pp1p/p1p3p1/3p4/6P1/P1N4P/1PPPPP2/QBR1NKBR w HChc - 0 9",
     {26, 701, 18930, 521377, 14733245, 416881799}},  // 396
    // {"qr1b1kbr/1p1ppppp/1n1n4/p1p5/4P3/5NPP/PPPP1P2/QRNB1KBR w HBhb - 1 9",
    // {26, 649, 17235, 451997, 12367604, 342165821}},  // 397
    // {"qrnnkb1r/1pppppp1/7p/p4b2/4P3/5P1P/PPPP2PR/QRNNKBB1 w Bhb - 1 9", {34,
    // 941, 31720, 901240, 30307554, 888709821}},  // 398
    // {"qr1nkrbb/p2ppppp/1pp5/8/3Pn3/1NP3P1/PP2PP1P/QR1NKRBB w FBfb - 1 9",
    // {19, 505, 11107, 294251, 7046501, 190414579}},  // 399
    // {"bbrqn1kr/1pppp1pp/4n3/5p2/p5P1/3P4/PPP1PPKP/BBRQNN1R w hc - 0 9", {24,
    // 573, 12963, 335845, 8191054, 227555387}},  // 400
    // {"brqb1nkr/pppppp1p/8/4N1pn/5P2/6P1/PPPPP2P/BRQB1NKR w HBhb - 0 9", {26,
    // 550, 14338, 331666, 8903754, 223437427}},  // 401
    // {"brqnn1kr/pp3ppp/2pbp3/3p4/8/2NPP3/PPP1BPPP/BRQ1N1KR w HBhb - 0 9", {27,
    // 780, 20760, 589328, 16243731, 463883447}},  // 402
    // {"brq1nkrb/ppp2ppp/8/n2pp2P/P7/4P3/1PPP1PP1/BRQNNKRB w GBgb - 1 9", {17,
    // 426, 8295, 235162, 5048497, 153986034}},  // 403
    // {"rbbqn1kr/pp2p1pp/6n1/2pp1p2/2P4P/P7/BP1PPPP1/R1BQNNKR w HAha - 0 9",
    // {27, 916, 25798, 890435, 26302461, 924181432}},  // 404
    // {"1qbbn1kr/1ppppppp/r3n3/8/p1P5/P7/1P1PPPPP/RQBBNNKR w HAh - 1 9", {29,
    // 817, 24530, 720277, 22147642, 670707652}},  // 405
    // {"rqbnnbkr/ppp1ppp1/7p/3p4/PP6/7P/1NPPPPP1/RQB1NBKR w HAa - 1 9", {23,
    // 572, 14509, 381474, 10416981, 288064942}},  // 406
    // {"r1bnnkrb/q1ppp1pp/p7/1p3pB1/2P1P3/3P4/PP3PPP/RQ1NNKRB w GAga - 2 9",
    // {31, 925, 27776, 860969, 26316355, 843078864}},  // 407
    // {"rbqnb1kr/ppppp1pp/5p2/5N2/7P/1n3P2/PPPPP1P1/RBQNB1KR w HAha - 1 9",
    // {32, 864, 27633, 766551, 24738875, 707188107}},  // 408
    // {"rqnbbn1r/ppppppp1/6k1/8/6Pp/2PN4/PP1PPPKP/RQ1BBN1R w - - 0 9", {27,
    // 566, 15367, 347059, 9714509, 234622128}},  // 409
    // {"rqnnbbkr/p1p2pp1/1p1p3p/4p3/4NP2/6P1/PPPPP2P/RQN1BBKR w HAha - 0 9",
    // {27, 631, 17923, 452734, 13307890, 356279813}},  // 410
    // {"1qnnbrkb/rppp1ppp/p3p3/8/4P3/2PP1P2/PP4PP/RQNNBKRB w GA - 1 9", {24,
    // 479, 12135, 271469, 7204345, 175460841}},  // 411
    // {"rbqnn1br/p1pppk1p/1p4p1/5p2/8/P1P2P2/1PBPP1PP/R1QNNKBR w HA - 0 9",
    // {31, 756, 23877, 625194, 20036784, 554292502}},  // 412
    // {"rqnbnkbr/1ppppp2/p5p1/8/1P4p1/4PP2/P1PP3P/RQNBNKBR w HAha - 0 9", {24,
    // 715, 18536, 575589, 16013189, 515078271}},  // 413
    // {"rq1nkbbr/1p2pppp/p2n4/2pp4/1P4P1/P2N4/2PPPP1P/RQ1NKBBR w HAha - 1 9",
    // {27, 694, 19840, 552904, 16685687, 494574415}},  // 414
    // {"r1nnkrbb/pp1pppp1/2p3q1/7p/8/1PPP3P/P3PPP1/RQNNKRBB w FAfa - 1 9", {18,
    // 520, 10808, 329085, 7508201, 235103697}},  // 415
    // {"bbrnqk1r/pppp3p/6p1/4pp2/3P2P1/8/PPP1PP1P/BBRN1NKR w HC - 0 9", {22,
    // 566, 12965, 362624, 8721079, 259069471}},  // 416
    // {"brnb1nkr/pppqpp2/3p2pp/8/3PP3/1P6/PBP2PPP/1RNBQNKR w HBhb - 0 9", {32,
    // 859, 28517, 817464, 27734108, 829785474}},  // 417
    // {"brnq1b1r/ppp1ppkp/3p1np1/8/8/5P1P/PPPPPKPR/BRNQNB2 w - - 0 9", {21,
    // 511, 10951, 273756, 6372681, 167139732}},  // 418
    // {"brnq1rkb/1pppppp1/3n3p/p7/8/P4NP1/1PPPPPRP/BRNQ1K1B w B - 0 9", {25,
    // 548, 14049, 341208, 9015901, 235249649}},  // 419
    // {"rbb1qnkr/p1ppp1pp/1p3p2/6n1/8/1PN1P2P/P1PP1PP1/RBB1QNKR w HAha - 0 9",
    // {25, 673, 16412, 467660, 12099119, 361714466}},  // 420
    // {"rnbb1nkr/1ppp1ppp/4p3/p5q1/6P1/1PP5/PB1PPP1P/RN1BQNKR w HAha - 1 9",
    // {19, 663, 14149, 489653, 11491355, 399135495}},  // 421
    // {"rnbqnbkr/1pp1p2p/3p1p2/p5p1/5PP1/2P5/PPNPP2P/RNBQ1BKR w HAha - 0 9",
    // {24, 647, 16679, 461931, 12649636, 361157611}},  // 422
    // {"rnb2krb/pppqppnp/8/3p2p1/1P4P1/7P/P1PPPPB1/RNBQNKR1 w GAga - 1 9", {24,
    // 722, 18749, 605229, 16609220, 563558512}},  // 423
    // {"rbnqb1kr/pppn1pp1/3p3p/4p3/1P6/P7/R1PPPPPP/1BNQBNKR w Hha - 1 9", {20,
    // 538, 12277, 345704, 8687621, 255304141}},  // 424
    // {"rnqb1nkr/p1pbp1pp/8/1pPp1p2/P2P4/8/1P2PPPP/RNQBBNKR w HAha - 1 9", {35,
    // 764, 26952, 632796, 22592380, 564255328}},  // 425
    // {"rnq1bbkr/1p1ppp1p/4n3/p1p3p1/P1PP4/8/RP2PPPP/1NQNBBKR w Hha - 0 9",
    // {29, 709, 21296, 570580, 17597398, 506140370}},  // 426
    // {"1nqnbkrb/1pppp2p/r7/p4pp1/3P4/8/PPPBPPPP/RNQNK1RB w g - 0 9", {27,
    // 1028, 28534, 1050834, 30251988, 1096869832}},  // 427
    // {"rbnqnkbr/p1pp1p1p/8/1p2p3/3P2pP/2P5/PP2PPP1/RBNQNKBR w HAha - 0 9",
    // {32, 832, 27120, 750336, 24945574, 724171581}},  // 428
    // {"rnq1nkbr/1p1p1ppp/2p1pb2/p7/7P/2P5/PPNPPPPB/RNQB1K1R w HAha - 2 9",
    // {31, 779, 24010, 638640, 19919434, 551494771}},  // 429
    // {"rnqnk1br/p1ppp1bp/1p3p2/6p1/4N3/P5P1/1PPPPP1P/R1QNKBBR w HAha - 2 9",
    // {25, 717, 19396, 576577, 16525239, 507175842}},  // 430
    // {"rnq1krbb/p1p1pppp/8/1p1p4/1n5B/2N2P2/PPPPP1PP/RNQ1KR1B w FAfa - 0 9",
    // {28, 867, 24029, 735686, 21112751, 654808184}},  // 431
    // {"bbrnnqkr/1pp1pppp/3p4/p7/P3P3/7P/1PPP1PP1/BBRNNQKR w HChc - 0 9", {24,
    // 405, 11025, 210557, 6196438, 131401224}},  // 432
    // {"brnbnqkr/p1ppp3/1p5p/5Pp1/5P2/3N4/PPPPP2P/BRNB1QKR w HBhb g6 0 9", {25,
    // 785, 21402, 698331, 20687969, 695850727}},  // 433
    // {"br1nqbkr/1ppppp2/pn6/6pp/2PP4/1N4P1/PP2PP1P/BR1NQBKR w HBhb - 0 9",
    // {25, 596, 16220, 421882, 12185361, 337805606}},  // 434
    // {"1rnnqkrb/p2ppp1p/1pp5/2N3p1/8/1P6/P1PPPPKP/BR1NQ1RB w gb - 0 9", {38,
    // 960, 34831, 913665, 32490040, 880403591}},  // 435
    // {"rbbnnqkr/pp3pp1/2p1p3/3p3p/3P3P/1PP5/P3PPP1/RBBNNQKR w HAha - 0 9",
    // {30, 785, 23079, 656618, 19885037, 599219582}},  // 436
    // {"rn1bnqkr/p1ppppp1/8/1p5p/P4P1P/3N4/1PPPP1b1/RNBB1QKR w HAha - 0 9",
    // {27, 752, 21735, 613194, 18862234, 547415271}},  // 437
    // {"1nbnqbkr/1p1p1ppp/r3p3/p1p5/P3P3/3Q4/1PPP1PPP/RNBN1BKR w HAh - 2 9",
    // {33, 721, 24278, 572535, 19648535, 496023732}},  // 438
    // {"rnbnqkrb/2pppppp/1p6/p7/1PP5/4N2P/P2PPPP1/RNB1QKRB w GAg - 0 9", {23,
    // 570, 14225, 374196, 10022614, 279545007}},  // 439
    // {"rbnnbq1r/ppppppkp/6p1/N7/4P3/P7/1PPP1PPP/RB1NBQKR w HA - 5 9", {27,
    // 620, 18371, 440594, 13909432, 349478320}},  // 440
    // {"r1nbbqkr/pppppp1p/8/8/1n3Pp1/3N1QP1/PPPPP2P/RN1BB1KR w HAha - 0 9",
    // {31, 791, 25431, 682579, 22408813, 636779732}},  // 441
    // {"rnq1bbkr/pp1p1ppp/2pnp3/8/7P/1QP5/PP1PPPPR/RNN1BBK1 w Aha - 2 9", {28,
    // 559, 16838, 390887, 12242780, 315431511}},  // 442
    // {"rnnqbrkb/2ppppp1/1p1N4/p6p/4P3/8/PPPP1PPP/R1NQBKRB w GA - 0 9", {32,
    // 638, 20591, 438792, 14395828, 331782223}},  // 443
    // {"rbnnq1br/pppp1kp1/4pp2/7p/PP6/2PP4/4PPPP/RBNNQKBR w HA - 0 9", {21,
    // 521, 12201, 320429, 8239159, 227346638}},  // 444
    // {"rnnbqkbr/p2ppp2/7p/1pp3p1/2P2N2/8/PP1PPPPP/RN1BQKBR w HAha - 0 9", {25,
    // 528, 13896, 326094, 9079829, 232750602}},  // 445
    // {"rnn1kbbr/ppppqp2/6p1/2N1p2p/P7/2P5/1P1PPPPP/RN1QKBBR w HAha - 2 9",
    // {27, 801, 22088, 707078, 20334071, 682580976}},  // 446
    // {"rnnqkrbb/p1p1p1pp/1p3p2/8/3p2Q1/P1P1P3/1P1P1PPP/RNN1KRBB w FAfa - 0 9",
    // {37, 1014, 34735, 998999, 32921537, 988770109}},  // 447
    // {"bbrnk1qr/1pppppp1/p4n1p/8/P2P2N1/8/1PP1PPPP/BBR1NKQR w HC - 1 9", {21,
    // 481, 11213, 279993, 7015419, 187564853}},  // 448
    // {"brnbnkqr/1pp1p1p1/p2p1p2/7p/1P4PP/8/PBPPPP2/1RNBNKQR w HBhb - 0 9",
    // {31, 743, 24260, 660177, 22391185, 653721389}},  // 449
    // {"br2kbqr/ppppp1pp/3n1p2/3P4/3n3P/3N4/PPP1PPP1/BR1NKBQR w HBhb - 3 9",
    // {25, 872, 22039, 748726, 20281962, 685749952}},  // 450
    // {"br1nkqrb/ppppppp1/8/7p/4P3/n1P2PP1/PP1P3P/BRNNKQRB w GBgb - 0 9", {28,
    // 607, 16934, 396483, 11607818, 294181806}},  // 451
    // {"rbbn1kqr/pp1pp1p1/2pn3p/5p2/5P2/1P1N4/PNPPP1PP/RBB2KQR w HAha - 1 9",
    // {27, 725, 21543, 616082, 19239812, 581716972}},  // 452
    // {"rnbbnk1r/pp1ppp1p/6q1/2p5/PP4p1/4P3/2PP1PPP/RNBBNKQR w HAha - 1 9",
    // {25, 1072, 26898, 1088978, 28469879, 1122703887}},  // 453
    // {"rnbnkbqr/1pp3pp/3p4/p3pp2/3P2P1/2N1N3/PPP1PP1P/R1B1KBQR w HAha - 0 9",
    // {31, 1028, 32907, 1095472, 36025223, 1211187800}},  // 454
    // {"r1bnkqrb/1ppppppp/p3n3/8/6P1/4N3/PPPPPPRP/RNB1KQ1B w Aga - 1 9", {23,
    // 457, 11416, 250551, 6666787, 159759052}},  // 455
    // {"rbn1bkqr/p1pp1pp1/1pn5/4p2p/7P/1PBP4/P1P1PPP1/RBNN1KQR w HAha - 0 9",
    // {23, 470, 11649, 264274, 6963287, 172833738}},  // 456
    // {"rnnbbkqr/3ppppp/p7/1pp5/P6P/6P1/1PPPPP2/RNNBBKQR w HAha - 0 9", {26,
    // 569, 15733, 375556, 11008114, 284485303}},  // 457
    // {"r1nk1bqr/1pppp1pp/2n5/p4p1b/5P2/1N4B1/PPPPP1PP/RN1K1BQR w HAha - 2 9",
    // {25, 824, 21983, 738366, 20904119, 716170771}},  // 458
    // {"r1nkbqrb/p2pppp1/npp4p/8/4PP2/2N4P/PPPP2P1/R1NKBQRB w GAga - 0 9", {31,
    // 548, 17480, 349633, 11469548, 255067638}},  // 459
    // {"rbnnkqbr/ppppp2p/5p2/6p1/2P1B3/P6P/1P1PPPP1/R1NNKQBR w HAha - 1 9",
    // {31, 809, 24956, 680747, 21247414, 606221516}},  // 460
    // {"1r1bkqbr/pppp1ppp/2nnp3/8/2P5/N4P2/PP1PP1PP/1RNBKQBR w Hh - 0 9", {28,
    // 810, 22844, 694599, 20188622, 636748147}},  // 461
    // {"rn1kqbbr/p1pppp1p/1p4p1/1n6/1P2P3/4Q2P/P1PP1PP1/RNNK1BBR w HAha - 1 9",
    // {39, 848, 30100, 724426, 25594662, 659615710}},  // 462
    // {"rn1kqrbb/pppppppp/8/8/2nP2P1/1P2P3/P1P2P1P/RNNKQRBB w FAfa - 1 9", {29,
    // 766, 21701, 567971, 16944425, 456898648}},  // 463
    // {"b1rnnkrq/bpppppp1/7p/8/1p6/2B5/PNPPPPPP/1BR1NKRQ w GCgc - 2 9", {25,
    // 667, 17253, 472678, 12865247, 365621294}},  // 464
    // {"brnb1krq/pppppppp/8/5P2/2P1n2P/8/PP1PP1P1/BRNBNKRQ w GBgb - 1 9", {23,
    // 620, 14882, 402561, 10776855, 300125003}},  // 465
    // {"b1nnkbrq/pr1pppp1/1p5p/2p5/P2N1P2/8/1PPPP1PP/BR1NKBRQ w GBg - 0 9",
    // {24, 472, 12181, 267398, 7370758, 178605165}},  // 466
    // {"br1nkrqb/p1p1p1pp/3n4/1p1p1p2/5N1P/4P3/PPPP1PP1/BR1NKRQB w FBfb - 0 9",
    // {24, 775, 19398, 624309, 16429837, 539767605}},  // 467
    // {"rbbnnkrq/p2pp1pp/2p5/5p2/1pPP1B2/P7/1P2PPPP/RB1NNKRQ w GAga - 0 9",
    // {34, 921, 30474, 849933, 28095833, 806446436}},  // 468
    // {"rnbbnkr1/1p1ppp1p/2p3p1/p7/2Pq4/1P1P4/P2BPPPP/RN1BNKRQ w GAga - 2 9",
    // {26, 1139, 29847, 1204863, 32825932, 1281760240}},  // 469
    // {"1rbnkbrq/pppppp2/n5pp/2P5/P7/4N3/1P1PPPPP/RNB1KBRQ w GAg - 2 9", {23,
    // 574, 14146, 391413, 10203438, 301874034}},  // 470
    // {"1nbnkr1b/rppppppq/p7/7p/1P5P/3P2P1/P1P1PP2/RNBNKRQB w FAf - 1 9", {33,
    // 823, 26696, 724828, 23266182, 672294132}},  // 471
    // {"rbn1bkrq/ppppp3/4n2p/5pp1/1PN5/2P5/P2PPPPP/RBN1BKRQ w GAga - 0 9", {27,
    // 859, 24090, 796482, 23075785, 789152120}},  // 472
    {"r1nbbkrq/1ppp2pp/2n2p2/p3p3/5P2/1N4BP/PPPPP1P1/RN1B1KRQ w GAga - 0 9",
     {25, 774, 20141, 618805, 16718577, 515864053}},  // 473
    // {"rnnkbbrq/1pppp1p1/5p2/7p/p6P/3N1P2/PPPPP1PQ/RN1KBBR1 w GAga - 0 9",
    // {29, 673, 20098, 504715, 15545590, 416359581}},  // 474
    // {"r1nkbrqb/pppp1p2/n3p1p1/7p/2P2P2/1P6/P2PPQPP/RNNKBR1B w FAfa - 0 9",
    // {27, 722, 21397, 593762, 18742426, 537750982}},  // 475
    // {"rbnnkr1q/1ppp2pp/p4p2/P2bp3/4P2P/8/1PPP1PP1/RBNNKRBQ w FAfa - 1 9",
    // {26, 848, 23387, 741674, 21591790, 675163653}},  // 476
    // {"rn1bkrb1/1ppppp1p/pn4p1/8/P2q3P/3P4/NPP1PPP1/RN1BKRBQ w FAfa - 1 9",
    // {22, 803, 18322, 632920, 15847763, 536419559}},  // 477
    // {"rn1krbbq/pppp1npp/4pp2/8/4P2P/3P2P1/PPP2P2/RNNKRBBQ w EAea - 1 9", {29,
    // 810, 23968, 670500, 20361517, 575069358}},  // 478
    // {"rnn1rqbb/ppkp1pp1/2p1p2p/2P5/8/3P1P2/PP2P1PP/RNNKRQBB w EA - 0 9", {22,
    // 506, 11973, 292344, 7287368, 189865944}},  // 479
    // {"bbqr1knr/pppppp1p/8/4n1p1/2P1P3/6P1/PPQP1P1P/BB1RNKNR w HDhd - 0 9",
    // {26, 650, 18253, 481200, 14301029, 394943978}},  // 480
    // {"bq1bnknr/pprppp1p/8/2p3p1/4PPP1/8/PPPP3P/BQRBNKNR w HCh - 0 9", {24,
    // 548, 14021, 347611, 9374021, 250988458}},  // 481
    // {"bqrnkb1r/1p2pppp/p1pp3n/5Q2/2P4P/5N2/PP1PPPP1/B1RNKB1R w HChc - 0 9",
    // {46, 823, 33347, 673905, 26130444, 582880996}},  // 482
    // {"bq1rknrb/pppppp1p/4n3/6p1/4P1P1/3P1P2/PPP4P/BQRNKNRB w GCg - 0 9", {23,
    // 618, 14815, 419474, 10606831, 315124518}},  // 483
    // {"q1brnknr/pp1pp1p1/8/2p2p1p/5b2/P4N2/1PPPP1PP/QBBRK1NR w hd - 0 9", {22,
    // 675, 15778, 473994, 12077228, 368479752}},  // 484
    // {"qrbbnknr/1p1ppp1p/p1p5/8/1P2P1p1/3P1B2/P1P2PPP/QRB1NKNR w HBhb - 0 9",
    // {32, 722, 24049, 569905, 19584539, 484814878}},  // 485
    // {"qrb1kbnr/p3pppp/2n5/1ppp4/7P/3P1P2/PPP1P1PR/QRBNKBN1 w Bhb - 0 9", {26,
    // 831, 22606, 724505, 20500804, 662608969}},  // 486
    // {"qrbnknrb/ppp1pp2/6p1/7p/PPNp4/8/2PPPPPP/QRB1KNRB w GBgb - 0 9", {31,
    // 840, 26762, 742772, 24422614, 701363800}},  // 487
    // {"qbrnbknr/pp1pp1pp/8/2p2p2/3Q4/PP6/2PPPPPP/1BRNBKNR w HChc - 0 9", {38,
    // 1121, 39472, 1198438, 41108769, 1285503872}},  // 488
    // {"qr1bbk1r/pppppp1p/1n6/5np1/4B3/1PP5/P2PPPPP/QRN1BKNR w HBhb - 0 9",
    // {25, 694, 16938, 472950, 12164609, 345122090}},  // 489
    // {"qrnkbbnr/1p1pp2p/p7/2p1Npp1/6P1/7P/PPPPPP2/QR1KBBNR w HBhb - 0 9", {27,
    // 586, 16348, 393391, 11409633, 298054792}},  // 490
    // {"qrnkbnrb/pp1p1p2/2p1p1pp/4N3/P4P2/8/1PPPP1PP/QR1KBNRB w GBgb - 0 9",
    // {32, 645, 20737, 460319, 15037464, 358531599}},  // 491
    // {"qbrnknbr/1pppppp1/p6p/8/1P6/3PP3/PQP2PPP/1BRNKNBR w HChc - 3 9", {26,
    // 595, 16755, 415022, 12214768, 323518628}},  // 492
    // {"qrnbk1br/1ppppp1p/p5p1/8/4Pn2/4K1P1/PPPP1P1P/QRNB1NBR w hb - 0 9", {24,
    // 609, 13776, 359415, 8538539, 230364479}},  // 493
    // {"qrnk1bbr/1pnp1ppp/p1p1p3/8/3Q4/1P1N3P/P1PPPPP1/1RNK1BBR w HBhb - 0 9",
    // {43, 1106, 42898, 1123080, 41695761, 1113836402}},  // 494
    // {"qrnknrb1/pppppp2/8/6pp/4P2P/3P1P2/PbP3P1/QRNKNRBB w FBfb - 0 9", {24,
    // 658, 17965, 488373, 14457245, 400971226}},  // 495
    // {"bbrqnrk1/ppp2ppp/7n/3pp3/8/P4N1N/1PPPPPPP/BBRQ1RK1 w - - 1 9", {22,
    // 503, 12078, 310760, 8080951, 224960353}},  // 496
    // {"brqbnk1r/1ppp1ppp/8/p3pn2/8/2PP1P2/PP2PKPP/BRQBN1NR w hb - 1 9", {25,
    // 745, 19387, 570459, 15520298, 460840861}},  // 497
    // {"brqnkbnr/pp2pp1p/3p4/2p5/5p2/3P3P/PPP1PPP1/B1RNKBNR w Hhb - 0 9", {19,
    // 516, 10755, 312996, 6995034, 214340699}},  // 498
    // {"brq1kn1b/1ppppprp/2n3p1/p7/P1N5/6P1/1PPPPP1P/BRQNK1RB w GBb - 2 9",
    // {29, 557, 16739, 352277, 10840256, 249999654}},  // 499
    // {"rbbq1k1r/ppp1pppp/7n/1n1p4/5P2/P2P4/1PPBP1PP/RB1QNKNR w HAha - 1 9",
    // {25, 769, 20110, 638340, 17438715, 570893953}},  // 500
    // {"r1bbnk1r/qpp1pppp/p6n/3p4/1P6/5N1P/P1PPPPP1/RQBBK1NR w ha - 0 9", {23,
    // 728, 18209, 587364, 16053564, 529082811}},  // 501
    // {"rqbnkbnr/1pp2p1p/3p4/p3p1p1/8/2P2P2/PP1PPNPP/RQBNKB1R w HAha - 0 9",
    // {26, 772, 21903, 653704, 19571559, 593915677}},  // 502
    // {"r1bnknrb/pqppp1p1/1p5p/5p2/7P/3P2N1/PPP1PPP1/RQBNK1RB w GAga - 2 9",
    // {27, 748, 20291, 597105, 16324542, 506453626}},  // 503
    // {"rbqnbknr/pp1pppp1/8/2p5/3P3p/5N1P/PPP1PPPR/RBQNBK2 w Aha - 0 9", {30,
    // 859, 26785, 819631, 26363334, 842796987}},  // 504
    // {"rqnbbrk1/ppppppp1/8/5n1p/3P3P/2B3P1/PPP1PP2/RQNB1KNR w HA - 0 9", {22,
    // 505, 11452, 283464, 7055215, 186760784}},  // 505
    // {"rqnkbbnr/pp2p1p1/8/2pp1p1p/3PPP2/8/PPP1N1PP/RQNKBB1R w HAha - 0 9",
    // {28, 832, 23142, 722857, 20429246, 663183060}},  // 506
    // {"rqnkbnr1/pppp2bp/6p1/4pp2/1P2P3/3NN3/P1PP1PPP/RQ1KB1RB w GAga - 0 9",
    // {28, 641, 18835, 459993, 14038570, 364210162}},  // 507
    // {"rbq2kbr/pppppppp/2n5/P7/3P1n2/2P5/1P2PPPP/RBQNKNBR w HA - 1 9", {31,
    // 889, 27028, 766181, 24299415, 692180754}},  // 508
    // {"rq1bkn1r/ppppp2p/3n4/5pp1/2b3P1/1N1P1P2/PPP1P2P/RQ1BKNBR w HAha - 1 9",
    // {28, 810, 22667, 657520, 18719949, 556282676}},  // 509
    // {"r1nknbbr/p2ppp1p/1pp3p1/8/1P6/4P3/P1PPNPPq/R1QKNBBR w HAha - 0 9", {24,
    // 797, 22144, 719069, 21862776, 716521139}},  // 510
    // {"rqnknrbb/ppp1p3/5ppp/2Np4/2P5/4P3/PP1P1PPP/RQNK1RBB w FAfa - 0 9", {34,
    // 686, 23277, 515541, 17664543, 423574794}},  // 511
    // {"1brnqknr/2p1pppp/p2p4/1P6/6P1/4Nb2/PP1PPP1P/BBR1QKNR w HChc - 1 9",
    // {34, 1019, 32982, 1003103, 33322477, 1043293394}},  // 512
    // {"brn1qknr/1p1pppp1/pb5p/Q1p5/3P3P/8/PPP1PPPR/BRNB1KN1 w Bhb - 2 9", {32,
    // 642, 20952, 464895, 15454749, 371861782}},  // 513
    // {"brnqkbnr/pppppp2/8/6pp/6P1/P2P1P2/1PP1P2P/BRNQKBNR w HBhb - 0 9", {20,
    // 441, 9782, 240220, 5770284, 153051835}},  // 514
    // {"2nqknrb/1rpppppp/5B2/pp6/1PP1b3/3P4/P3PPPP/1RNQKNRB w GBg - 1 9", {35,
    // 1042, 36238, 1101159, 38505058, 1202668717}},  // 515
    // {"rb1nqknr/1pp1pppp/8/3p4/p2P4/6PN/PPPQPP1P/RBBN1K1R w HAha - 0 9", {29,
    // 692, 21237, 555018, 17820605, 497251206}},  // 516
    // {"rnbbqknr/pppp4/5p2/4p1pp/P7/2N2PP1/1PPPP2P/R1BBQKNR w HAha - 0 9", {23,
    // 595, 14651, 415772, 10881112, 329010121}},  // 517
    // {"rn1qkbnr/p1p1pp1p/bp4p1/3p4/1P6/4P3/P1PP1PPP/RNBQKBNR w HAha - 0 9",
    // {30, 794, 24319, 690811, 21657601, 647745807}},  // 518
    // {"r1bqk1rb/pppnpppp/5n2/3p4/2P3PP/2N5/PP1PPP2/R1BQKNRB w GAga - 1 9",
    // {32, 821, 27121, 733155, 24923473, 710765657}},  // 519
    // {"rbnqbknr/1p1ppp1p/6p1/p1p5/7P/3P4/PPP1PPP1/RBNQBKNR w HAha - 0 9", {24,
    // 720, 18842, 575027, 15992882, 501093456}},  // 520
    // {"r1qbbk1r/pp1ppppp/n1p5/5n2/B1P3P1/8/PP1PPP1P/RNQ1BKNR w HAha - 0 9",
    // {27, 831, 22293, 698986, 19948650, 637973209}},  // 521
    // {"rnqkbb1r/p1pppppp/8/8/1p4n1/PP4PP/2PPPP2/RNQKBBNR w HAha - 0 9", {18,
    // 463, 9519, 256152, 6065231, 172734380}},  // 522
    // {"rnqk1nrb/pppbpp2/7p/3p2p1/4B3/2N1N1P1/PPPPPP1P/R1QKB1R1 w GAga - 0 9",
    // {34, 1171, 38128, 1318217, 42109356, 1465473753}},  // 523
    // {"rbnqknbr/1pp1ppp1/3p4/7p/p2P2PP/2P5/PP2PP2/RBNQKNBR w HAha - 0 9", {32,
    // 867, 28342, 798722, 26632459, 781067145}},  // 524
    // {"rn1bknbr/pq2pppp/1p6/2pp4/P7/1P1P4/2PNPPPP/RNQBK1BR w HAha - 0 9", {24,
    // 627, 16652, 462942, 13200921, 385193532}},  // 525
    // {"r1qk1bbr/ppp1pp1p/2np1n2/6p1/2PP4/3BP3/PP3PPP/RNQKN1BR w HAha - 2 9",
    // {31, 992, 30213, 986631, 30397368, 1011631987}},  // 526
    // {"r1qknrbb/pppp1p2/2n3p1/4p2p/8/QPP5/P1NPPPPP/RN1K1RBB w FAfa - 2 9",
    // {30, 702, 21563, 532939, 16813114, 438096194}},  // 527
    // {"bbkr1qnr/2pppppp/2n5/pp6/8/PPN5/1BPPPPPP/1BR1KQNR w HC - 2 9", {25,
    // 573, 15183, 380910, 10554668, 283975400}},  // 528
    // {"1rnbkqnr/1bpppppp/1p6/7P/p2P4/5P2/PPP1P1P1/BRNBKQNR w HBhb - 0 9", {21,
    // 503, 11790, 301084, 7679979, 207799378}},  // 529
    // {"brnkqbnr/2p1pppp/1p6/3p4/1pP5/P6P/3PPPP1/BRNKQBNR w HBhb - 0 9", {28,
    // 743, 21054, 587192, 17354516, 507176753}},  // 530
    // {"br1kqnrb/npp1pppp/8/3p4/p4N2/PP6/2PPPPPP/BR1KQNRB w GBgb - 0 9", {31,
    // 808, 25585, 698475, 22376575, 640362920}},  // 531
    // {"rbbnkq1r/pppppp1p/7n/6p1/P5P1/2P2N2/1P1PPP1P/RBBNKQ1R w HAha - 1 9",
    // {29, 580, 17585, 404831, 12730970, 325226128}},  // 532
    // {"rnbbk1nr/pp2qppp/2ppp3/8/3P4/P1N4N/1PP1PPPP/R1BBKQ1R w HAha - 0 9",
    // {29, 838, 24197, 721884, 21100580, 646624429}},  // 533
    // {"rnbk1b1r/ppppn1pp/4pp2/7q/7P/P5PB/1PPPPP2/RNBKQ1NR w HAha - 3 9", {20,
    // 729, 16633, 576199, 14507076, 498621813}},  // 534
    // {"r2kqnrb/pbppppp1/np5p/8/4Q1P1/3P4/PPP1PP1P/RNBK1NRB w GAga - 2 9", {47,
    // 1219, 55009, 1486353, 65239153, 1834391369}},  // 535
    // {"rbnkbq1r/p1p2ppp/1p2pn2/3p4/P3P3/3P4/1PP1KPPP/RBN1BQNR w ha - 2 9",
    // {29, 923, 27179, 883866, 26202752, 868565895}},  // 536
    // {"rk1bb1nr/ppppqppp/n7/1N2p3/6P1/7N/PPPPPP1P/R1KBBQ1R w HA - 6 9", {27,
    // 703, 19478, 559525, 16049807, 492966455}},  // 537
    // {"rnkqbbnr/p1ppp2p/1p4p1/8/1B3p1P/2NP4/PPP1PPP1/R1KQ1BNR w HAha - 0 9",
    // {29, 610, 18855, 438277, 14020041, 355083962}},  // 538
    // {"rnkqb1rb/pp1p1ppp/4p3/2P3n1/8/1PP5/P3PPPP/RNKQBNRB w GAga - 0 9", {29,
    // 675, 20699, 535821, 17000613, 476598337}},  // 539
    // {"rb1kqnbr/pp1pp1p1/1np2p2/7p/P1P3PP/8/1P1PPP2/RBNKQNBR w HAha - 0 9",
    // {31, 1077, 33661, 1183381, 37415304, 1328374620}},  // 540
    // {"rnkbq1br/ppp2ppp/3p4/Q3p1n1/5P2/3P2P1/PPP1P2P/RNKB1NBR w HAha - 0 9",
    // {41, 1201, 46472, 1420367, 52991625, 1675608008}},  // 541
    // {"rn1qnbbr/pp2pppp/2ppk3/8/2PP4/3Q1N2/PP2PPPP/RNK2BBR w HA - 1 9", {34,
    // 666, 22474, 472299, 15860369, 353831792}},  // 542
    // {"rnkqnr1b/ppppp1pp/5p2/8/Q1P2P2/8/PP1P2PP/RbK1NRBB w FAfa - 0 9", {36,
    // 876, 31987, 788580, 29022529, 736717252}},  // 543
    // {"bbrn1nqr/ppp1k1pp/5p2/3pp3/7P/3PN3/PPP1PPP1/BBRK1NQR w - - 1 9", {24,
    // 583, 15063, 383532, 10522064, 280707118}},  // 544
    // {"brnbkn1r/1pppp1p1/4q3/p4p1p/7P/1N3P2/PPPPP1PQ/BR1BKN1R w HBhb - 2 9",
    // {27, 935, 26120, 885699, 26000648, 873063158}},  // 545
    // {"br1knbqr/pp2p1pp/1n6/2pp1p2/6P1/2P4B/PP1PPPQP/BRNKN2R w HBhb - 0 9",
    // {27, 681, 19202, 510687, 14954779, 415624943}},  // 546
    // {"brnk1qrb/p1ppppp1/1p5p/8/P3n3/1N4P1/1PPPPPRP/BR1KNQ1B w Bgb - 0 9",
    // {22, 638, 13991, 412346, 9760752, 293499724}},  // 547
    // {"rbbnknqr/pppp3p/5pp1/8/1P1pP3/7P/P1P2PP1/RBBNKNQR w HAha - 0 9", {29,
    // 756, 21616, 614074, 17602252, 528140595}},  // 548
    // {"1nbbknqr/rpp1ppp1/1Q1p3p/p7/2P2PP1/8/PP1PP2P/RNBBKN1R w HAh - 2 9",
    // {37, 977, 34977, 944867, 33695089, 940198007}},  // 549
    {"rnb2bqr/ppkpppp1/3n3p/2p5/6PP/2N2P2/PPPPP3/R1BKNBQR w HA - 2 9",
     {30, 647, 20365, 467780, 15115531, 369257622}},  // 550
    // {"rn1k1qrb/p1pppppp/bp6/8/4n3/P4BPP/1PPPPP2/RNBKNQR1 w GAga - 2 9", {22,
    // 670, 14998, 451517, 11199653, 339919682}},  // 551
    // {"rb2bnqr/nppkpppp/3p4/p7/1P6/P2N2P1/2PPPP1P/RB1KBNQR w HA - 3 9", {22,
    // 479, 11475, 264739, 6831555, 167329117}},  // 552
    // {"r1kbb1qr/2pppppp/np2n3/p7/2P3P1/8/PP1PPPQP/RNKBBN1R w HAha - 1 9", {32,
    // 723, 23953, 581832, 19472074, 504622114}},  // 553
    // {"rnknbb1r/p1ppp1pp/8/1p1P1p1q/8/P1P5/1P2PPPP/RNKNBBQR w HAha - 1 9",
    // {19, 607, 12733, 417451, 9753617, 325177085}},  // 554
    // {"rnkn1qrb/pp1bp1pp/2p5/1N1p1p2/8/2P5/PPKPPPPP/R2NBQRB w ga - 2 9", {27,
    // 533, 14549, 330747, 9206957, 232664675}},  // 555
    // {"r1nknqbr/pp2p1pp/2p2p2/3p4/6P1/PP1P4/2P1PP1b/RBNKNQBR w HAha - 0 9",
    // {20, 582, 13777, 409166, 10708639, 326565393}},  // 556
    // {"rnkb1qbr/p1pp1p1p/1p2pn2/1Q4p1/4P3/N4P2/PPPP2PP/R1KBN1BR w HAha - 0 9",
    // {40, 1038, 39356, 1051441, 39145902, 1079612614}},  // 557
    // {"rn2qbbr/1pkppp1p/p3n1p1/8/8/2P2P2/PP1PP1PP/RNKN1BBR w HA - 0 9", {24,
    // 605, 14888, 385964, 9687507, 260874068}},  // 558
    // {"rn1nqrbb/p1kppp1p/8/1pp3p1/1P6/2N1P3/P1PP1PPP/RK1NQRBB w - - 0 9", {21,
    // 540, 12489, 337997, 8436136, 237525904}},  // 559
    // {"bbrnknrq/1pp3pp/p2p1p2/4p3/P7/1P2N3/2PPPPPP/BBRN1RKQ w gc - 0 9", {24,
    // 527, 13900, 326175, 9139962, 226253685}},  // 560
    // {"brnb1nrq/pppp1kpp/4p3/8/5p1P/P1P3P1/1P1PPP2/BRNBKNRQ w GB - 1 9", {29,
    // 773, 23904, 638768, 20503775, 560338709}},  // 561
    // {"br1k1brq/ppppp2p/1n1n1pp1/8/P1P5/3P2P1/1P2PP1P/BRNKNBRQ w GBgb - 0 9",
    // {28, 811, 23550, 664880, 19913758, 565143976}},  // 562
    // {"1r1knrqb/n1pppppp/p1b5/1p6/8/3N1P2/PPPPP1PP/BRNK1RQB w fb - 3 9", {29,
    // 753, 23210, 620019, 20044474, 558383603}},  // 563
    // {"rbbnk1rq/pppppppp/8/3Pn3/8/4P1P1/PPP2P1P/RBBNKNRQ w GAga - 1 9", {22,
    // 551, 12619, 324608, 8204171, 217689974}},  // 564
    // {"rnbbk1rq/2pppp1p/p3n1p1/1p6/P3N3/8/1PPPPPPP/RNBB1KRQ w ga - 0 9", {26,
    // 742, 20061, 599527, 16787080, 525678162}},  // 565
    // {"rnbkn1rq/ppppppb1/6p1/7p/2B2P2/1P2P3/P1PP2PP/RNBKN1RQ w GAga - 1 9",
    // {28, 799, 23210, 689436, 20755098, 639632905}},  // 566
    // {"rn1knrqb/p2pppp1/b1p5/1p5p/2P2P2/1P6/P2PP1PP/RNBKNRQB w FAfa - 1 9",
    // {30, 579, 18481, 397545, 13257198, 311282465}},  // 567
    // {"rbnkbnrq/pp2p1Np/2p2p2/8/3p4/8/PPPPPPPP/RBNKBR1Q w Aga - 0 9", {23,
    // 670, 16435, 501883, 13012378, 411860744}},  // 568
    // {"rk1bbnrq/ppp1pppp/n7/3p4/5P2/3P2NP/PPP1P1P1/RNKBB1RQ w GA - 0 9", {26,
    // 597, 16238, 402506, 11269462, 296701249}},  // 569
    // {"r1knbbrq/pppp2p1/2n1p2p/5p2/4P3/P1PP4/1P3PPP/RNKNBBRQ w GAga - 1 9",
    // {20, 596, 13091, 399069, 9416862, 293659781}},  // 570
    // {"rnknbrqb/p1p1pp1p/3p4/1p1N2p1/8/N7/PPPPPPPP/1RK1BRQB w Ffa - 0 9", {26,
    // 724, 18942, 552040, 15257204, 461293885}},  // 571
    // {"rbnknrb1/1p1ppp1p/p1p3p1/8/1P3P2/1R6/PqPPP1PP/RBNKN1BQ w Afa - 0 9",
    // {31, 1183, 34723, 1289502, 38722152, 1421492227}},  // 572
    // {"rnkbnrbq/2p1ppp1/p7/1p1p3p/3P4/1P4P1/P1P1PP1P/RNKBNRBQ w FAfa - 0 9",
    // {24, 506, 12748, 301464, 8086100, 207129256}},  // 573
    // {"r1knrbbq/pp1ppppp/2p1n3/8/2P3P1/P7/1PKPPP1P/RN1NRBBQ w ea - 0 9", {28,
    // 570, 16037, 352471, 10278695, 242592363}},  // 574
    // {"rnknrq1b/ppp1p1p1/4b3/3p1p1p/6P1/P4P2/1PPPPQ1P/RNKNR1BB w EAea - 2 9",
    // {30, 739, 23124, 594962, 19252739, 521629794}},  // 575
    // {"bbqr1krn/pppp1p1p/5n2/4p1p1/3P4/P3QP2/1PP1P1PP/BB1RNKRN w GDgd - 0 9",
    // {31, 799, 25627, 674913, 22172123, 609277274}},  // 576
    // {"bq1b1krn/pp1ppppp/3n4/2r5/3p3N/6N1/PPP1PPPP/BQRB1KR1 w GCg - 2 9", {21,
    // 798, 18571, 688429, 17546069, 647165916}},  // 577
    // {"bqrnkbrn/2pp1pp1/p7/1p2p2p/1P6/4N3/P1PPPPPP/BQR1KBRN w GCgc - 0 9",
    // {27, 783, 22327, 670798, 20059741, 624462073}},  // 578
    // {"bqr1krnb/1np1pppp/8/pp1p4/8/2P2N2/PP1PPPPP/BQRNKR1B w FCfc - 0 9", {28,
    // 636, 18874, 461104, 14237097, 372181570}},  // 579
    // {"qbb1rkrn/1ppppppp/p7/7n/8/P2P4/1PP1PPPP/QBBRNKRN w Gg - 0 9", {25, 547,
    // 13837, 332918, 8849383, 229112926}},  // 580
    // {"1rbbnkrn/p1p1pp1p/2q5/1p1p2p1/8/2P3P1/PP1PPP1P/QRBBNKRN w GBgb - 2 9",
    // {24, 1010, 24370, 983770, 24328258, 961371180}},  // 581
    // {"qrb1kbrn/ppp1p2p/4npp1/3p4/8/1PP4P/PR1PPPP1/Q1BNKBRN w Ggb - 1 9", {18,
    // 451, 9291, 247310, 5568106, 155744022}},  // 582
    // {"qr2krnb/p1p1pppp/b1np4/1p6/3NP3/7P/PPPP1PP1/QRBNKR1B w FBfb - 2 9",
    // {25, 667, 17081, 476030, 12458875, 361495148}},  // 583
    // {"qbrnbkrn/ppp3pp/3p4/5p2/2P1pP2/6PP/PP1PP3/QBRNBKRN w GCgc - 0 9", {24,
    // 650, 16835, 445263, 12187382, 326834539}},  // 584
    // {"qrnb1krn/ppp1p1pp/5p2/2Np4/b2P4/2P5/PP2PPPP/QR1BBKRN w GBgb - 0 9",
    // {27, 641, 17490, 432041, 12103076, 310695797}},  // 585
    // {"qrnkbbrn/pp2pp2/8/2pp2pp/6PP/3P4/PPPKPP2/QRN1BBRN w gb - 0 9", {22,
    // 554, 13116, 357404, 9014737, 258925091}},  // 586
    // {"qrnkbrnb/p1p1ppp1/1p6/3p4/3P3p/5N1P/PPP1PPP1/QRNKBR1B w FBfb - 0 9",
    // {24, 529, 13205, 318722, 8295874, 213856651}},  // 587
    // {"qbr1krbn/1pppp1pp/p7/5pn1/2PP4/8/PPB1PPPP/Q1RNKRBN w FCfc - 0 9", {26,
    // 831, 21651, 696830, 18961456, 621884383}},  // 588
    // {"1rnbkrbn/1qp1pppp/3p4/pp6/4P3/1NP4P/PP1P1PP1/QR1BKRBN w FBfb - 0 9",
    // {24, 597, 15089, 404761, 10832084, 307793179}},  // 589
    // {"q1rkrbbn/ppp1pppp/8/3p4/1PnP4/P7/1RP1PPPP/Q1NKRBBN w Ee - 1 9", {20,
    // 520, 10769, 278067, 6452205, 170268300}},  // 590
    // {"qrnkrn1b/ppppp1pp/4b3/7P/6p1/P7/1PPPPP2/QRNKRNBB w EBeb - 0 9", {26,
    // 566, 15623, 381312, 10940750, 287987207}},  // 591
    // {"bbr1nkrn/ppp1pppp/3q4/3p4/8/P7/1PPPPPPP/BBRQNRKN w gc - 5 9", {19, 661,
    // 13895, 460396, 10870247, 356399665}},  // 592
    // {"brqbnkrn/pp1pp2p/5pp1/2p5/4P3/P2P1N2/1PP2PPP/BRQB1KRN w GBgb - 0 9",
    // {27, 679, 19916, 527306, 16391730, 455940859}},  // 593
    // {"2qnkbrn/p1pppppp/8/1r6/1p2bP2/7N/PPPPP1PP/BR1QKBRN w GBg - 4 9", {18,
    // 774, 15713, 635461, 14371755, 559579332}},  // 594
    // {"r1qnkr1b/p1pppppp/7n/1p6/8/1P3b1N/PRPPPPPP/B1QNK1RB w f - 5 9", {21,
    // 677, 15437, 501520, 12463801, 410795298}},  // 595
    // {"rbbqn1rn/pppp1pp1/3k4/4p2Q/2PPP3/8/PP3PPP/RBB1NKRN w GA - 1 9", {40,
    // 742, 28757, 579833, 21852196, 471452088}},  // 596
    // {"rqbbnkrn/3pppp1/p1p4p/1p6/5P2/P2N4/1PPPP1PP/RQBBK1RN w ga - 0 9", {23,
    // 665, 16400, 492544, 12794736, 396640086}},  // 597
    // {"r2nkbrn/pp2pppp/8/2ppqb2/2P3P1/5P2/PP1PPN1P/RQB1KBRN w GAga - 3 9",
    // {28, 1108, 31164, 1194581, 34780853, 1292405738}},  // 598
    // {"rqbnk1nb/p1pppr1p/5p2/1p4p1/1PP1P3/8/P2P1PPP/RQBNKRNB w FAa - 1 9",
    // {26, 650, 18208, 491403, 14565370, 416833400}},  // 599
    // {"rbqnb1rn/p1pp1kpp/1p2pp2/8/4P2P/P5P1/1PPP1P2/RBQNBKRN w GA - 0 9", {20,
    // 437, 9423, 222154, 5282124, 132309824}},  // 600
    // {"rqnbbkrn/p1p1pppp/3p4/1p5B/8/1P1NP3/P1PP1PPP/RQ2BKRN w GAga - 0 9",
    // {30, 606, 18382, 422491, 12989786, 326601372}},  // 601
    // {"rqnkbbr1/ppppp1pp/5p2/7n/8/2PNP2P/PP1P1PP1/RQ1KBBRN w GAga - 1 9", {23,
    // 482, 12506, 297869, 8430874, 217797292}},  // 602
    // {"r1nkbrnb/2ppppp1/1q6/pp5p/1P6/P3P3/2PPKPPP/RQN1BRNB w fa - 2 9", {25,
    // 827, 21518, 701071, 19290675, 632892337}},  // 603
    // {"rbqnkrbn/p1ppppp1/7p/1p6/7P/2N1P3/PPPP1PPB/RBQ1KR1N w FAfa - 1 9", {30,
    // 627, 18566, 440217, 12976682, 337377291}},  // 604
    // {"r1nbkrbn/p1qp1ppp/8/1pp1p3/2P1P3/6P1/PP1PBP1P/RQN1KRBN w FAfa - 2 9",
    // {22, 616, 14503, 431199, 10850952, 335943324}},  // 605
    // {"rqnkr1bn/ppp1ppb1/3p2pp/8/P7/2P2P2/1PKPP1PP/RQN1RBBN w ea - 1 9", {31,
    // 679, 21365, 493500, 15661072, 379844460}},  // 606
    // {"r2krnbb/qppp1ppp/1n6/p3p3/PP6/4N3/N1PPPPPP/RQ1KR1BB w EAea - 4 9", {24,
    // 645, 17054, 487028, 13837270, 416239106}},  // 607
    // {"bbr1qk1n/1ppppp1p/2n5/p7/P7/1P2P3/2PP1PrP/1BRNQKRN w GCc - 0 9", {18,
    // 520, 10680, 304462, 7215306, 207612575}},  // 608
    // {"brnbq1rn/2ppppkp/p5p1/1p6/8/1BP3P1/PP1PPP1P/BRN1QRKN w - - 0 9", {21,
    // 625, 13989, 419667, 9929336, 300902534}},  // 609
    // {"brn1kbrn/pp2p1pp/3p4/q1p2p2/2P4P/6P1/PP1PPP2/BRNQKBRN w GBgb - 1 9",
    // {18, 477, 10205, 273925, 6720181, 187205941}},  // 610
    // {"brn1krnb/p3pppp/1qpp4/1p6/2P3P1/1P6/P2PPP1P/BRNQKRNB w FBfb - 1 9",
    // {30, 835, 24761, 716151, 21806428, 654487872}},  // 611
    // {"r1b1qkrn/1p1ppppp/p1p1n3/8/4P3/1PN5/P1PPQPPb/RBB2KRN w GAga - 0 9",
    // {28, 825, 24536, 716585, 22079005, 647939781}},  // 612
    // {"r1bbqk1n/p1pppprp/n7/1p4p1/5P2/2N3N1/PPPPP1PP/1RBBQKR1 w Ga - 4 9",
    // {25, 545, 14657, 358854, 10271111, 273864588}},  // 613
    // {"rnbqkbrn/p1pp1pp1/4p3/7p/2p4P/2P5/PP1PPPP1/R1BQKBRN w GAga - 0 9", {17,
    // 445, 9076, 255098, 5918310, 174733195}},  // 614
    // {"rnbqkrnb/1p1pp1p1/2p4p/p4p2/3P2P1/7N/PPPBPP1P/RN1QKR1B w FAfa - 0 9",
    // {34, 746, 25319, 623133, 21285553, 569141201}},  // 615
    // {"rbnqbkr1/1ppppp2/p5n1/6pp/4P3/1N6/PPPP1PPP/RBQ1BRKN w ga - 2 9", {18,
    // 466, 9683, 260864, 6051500, 170135726}},  // 616
    // {"rnqb1krn/ppppp1p1/7p/7b/P1P2pPP/8/1P1PPP2/RNQBBKRN w GAga - 0 9", {24,
    // 575, 15400, 385825, 11039042, 291243811}},  // 617
    // {"rnqkbbr1/p1pp1ppp/4p3/1p6/P3P2n/5P2/1PPP1NPP/RNQKBBR1 w GAga - 2 9",
    // {27, 803, 22883, 694449, 20666099, 638696065}},  // 618
    // {"rn1kbrnb/1qppp1pp/1p6/p4p2/1B1P4/1P5N/P1P1PPPP/RNQK1R1B w FAfa - 0 9",
    // {37, 1209, 43015, 1425600, 49748034, 1671593862}},  // 619
    // {"rbnqkrbn/Bppp1p2/p5pp/4p3/5P2/6PP/PPPPP3/RBNQKR1N w FAfa - 0 9", {29,
    // 720, 20434, 534148, 15384362, 421343249}},  // 620
    // {"rnqbkr1n/1p1ppbpp/3p1p2/p7/8/1P6/P1PPPPPP/R1QBKRBN w FAfa - 0 9", {20,
    // 657, 14424, 492678, 11843134, 413965054}},  // 621
    // {"rnqkrb1n/ppppp3/6p1/5p1p/2b2P2/P1N5/1PPPP1PP/RQ1KRBBN w EAea - 1 9",
    // {28, 749, 20684, 543151, 15379233, 417191461}},  // 622
    // {"rnqk1nbb/1pp2ppp/3pr3/p3p3/3P1P2/2N3N1/PPP1P1PP/R1QKR1BB w EAa - 1 9",
    // {29, 883, 26412, 815098, 25144295, 789705382}},  // 623
    // {"bbr1kqrn/p1p1ppp1/1p2n2p/3p4/1P1P4/2N5/P1P1PPPP/BBR1KQRN w GCgc - 0 9",
    // {22, 485, 11475, 271271, 6825123, 171793012}},  // 624
    // {"brnbkq1n/ppp1ppr1/7p/3p2p1/2P3PP/8/PPBPPP2/BRN1KQRN w GBb - 2 9", {30,
    // 634, 19017, 442537, 13674310, 345386924}},  // 625
    // {"brnkqbr1/1pppp1pp/5p2/p7/P1P1P2n/8/1P1P1PP1/BRNKQBRN w GBgb - 0 9",
    // {21, 504, 11672, 305184, 7778289, 217596497}},  // 626
    {"b1rkqrnb/p1ppp1pp/1p1n4/5p2/5P2/PN5P/1PPPP1P1/BR1KQRNB w FBf - 0 9",
     {23, 688, 17259, 531592, 14228372, 451842354}},  // 627
    // {"1bbnkqrn/rppppp2/p5p1/7p/7P/P1P1P3/1P1P1PP1/RBBNKQRN w GAg - 1 9", {25,
    // 450, 12391, 263946, 7752404, 185393913}},  // 628
    // {"rnbbkqr1/1pppppp1/7p/p3n3/PP5P/8/1BPPPPP1/RN1BKQRN w GAga - 0 9", {23,
    // 543, 12224, 305812, 7549008, 199883770}},  // 629
    // {"r1bkqbrn/ppppp1pp/8/5p2/3nPP2/1P4N1/P1PP2PP/RNBKQBR1 w GAga - 1 9",
    // {27, 751, 21158, 600417, 17989920, 527273615}},  // 630
    // {"rnbkqr1b/1p1pp1pp/p4p1n/2p5/1P5P/N4P2/P1PPP1P1/R1BKQRNB w FAfa - 0 9",
    // {21, 498, 11738, 302278, 7808375, 216224115}},  // 631
    // {"rbnkbqrn/p1p3pp/1p1p4/B3pp2/3P2P1/6N1/PPP1PP1P/RBNK1QR1 w GAga - 0 9",
    // {34, 977, 33464, 961128, 33318567, 978991050}},  // 632
    // {"r1kbbqrn/ppp3pp/2np1p2/1P2p3/3P1P2/8/P1P1P1PP/RNKBBQRN w GAga - 0 9",
    // {32, 920, 28916, 844881, 26763259, 797524786}},  // 633
    // {"rk1qbbrn/p2npppp/1p6/2p4Q/8/4P3/PPPP1PPP/RNK1B1RN w GA - 2 9", {35,
    // 657, 22359, 495406, 16662477, 419496845}},  // 634
    // {"rnk1brnb/pp1p1pp1/8/q1p1p2p/5P2/NP6/P1PPP1PP/R1KQBRNB w FAfa - 1 9",
    // {26, 774, 20215, 610661, 16987110, 523437649}},  // 635
    // {"rb1kqrbn/npp1ppp1/p7/3P3p/2PP4/8/PP3PPP/RBNKQRBN w FAfa - 0 9", {35,
    // 775, 27395, 661118, 23983464, 625669222}},  // 636
    // {"rnkb1rbn/pp1p2pp/8/2p1pp1q/P6P/1PN5/2PPPPP1/R1KBQRBN w FAfa - 1 9",
    // {22, 899, 21188, 850597, 21518343, 857951339}},  // 637
    // {"rnkqrbbn/1pppp1p1/8/p2N1p1p/2P4P/8/PP1PPPP1/R1KQRBBN w EAea - 0 9",
    // {29, 585, 17571, 393221, 12238776, 299752383}},  // 638
    // {"rnk1r1bb/pp1ppppp/1q4n1/2p5/5P1P/3PP3/PPP3P1/RNKQRNBB w EAea - 1 9",
    // {27, 884, 24613, 811915, 23698701, 790239502}},  // 639
    // {"bbrnkrqn/1ppp1p2/6pp/p3p3/5PP1/2PB4/PP1PP2P/B1RNKRQN w FCfc - 0 9",
    // {37, 693, 25425, 550527, 20138432, 481498664}},  // 640
    // {"b1rbkrqn/ppp2ppp/1n2p3/3p4/6P1/2PP4/PP2PP1P/BRNBKRQN w FBf - 1 9", {21,
    // 463, 10610, 253204, 6307276, 159025909}},  // 641
    // {"brnkrb1n/1pp1p1pp/3p4/p1Nq1p2/2P5/8/PP1PPPPP/BRK1RBQN w eb - 2 9", {27,
    // 725, 17842, 496072, 12604078, 362747791}},  // 642
    // {"brn1r1nb/ppppkppp/4p3/8/2PP1P2/8/PP1KP1PP/BRN1RQNB w - - 1 9", {25,
    // 623, 16874, 426659, 12290985, 317097424}},  // 643
    // {"rbb1krqn/1pp1pp1p/p3n1p1/3pP3/8/1PN5/P1PP1PPP/RBB1KRQN w FAfa d6 0 9",
    // {23, 529, 12641, 310277, 7861413, 202594556}},  // 644
    // {"r1bbkrqn/p1pppppp/8/4n3/1p5P/P2P2P1/1PP1PP2/RNBBKRQN w FAfa - 0 9",
    // {23, 571, 13133, 346793, 8699448, 243460643}},  // 645
    // {"rnbkrbqn/p1pp1ppp/4p3/1p6/8/BPN3P1/P1PPPP1P/R2KRBQN w EAea - 2 9", {29,
    // 692, 20014, 500375, 14904192, 386694739}},  // 646
    // {"rnbkrqn1/pppppp2/8/1Q2b1pp/P3P3/5P2/1PPP2PP/RNBKR1NB w EAea - 0 9",
    // {37, 1001, 36440, 987842, 35626426, 993747544}},  // 647
    // {"rbnkbrqn/p1pppp2/7p/1p4pP/3P1P2/8/PPP1P1P1/RBNKBRQN w FAfa - 0 9", {30,
    // 564, 17143, 381364, 11859538, 293703269}},  // 648
    // {"1nkbbrqn/3ppppp/r1p5/pp6/8/4PP2/PPPPN1PP/RNKBBRQ1 w FAf - 2 9", {26,
    // 546, 14641, 344592, 9556962, 245137199}},  // 649
    // {"rnkrbbq1/pppppnp1/7p/8/1B1Q1p2/3P1P2/PPP1P1PP/RNKR1B1N w DAda - 2 9",
    // {43, 887, 36240, 846858, 33185346, 851927292}},  // 650
    // {"1rkrbqnb/pppppp2/2n3p1/7p/3P3P/P4N2/1PP1PPP1/RNKRBQ1B w DAd - 0 9",
    // {26, 622, 16049, 403921, 10786140, 285233838}},  // 651
    // {"rbnkr1bn/pp1pqp1p/2p1p3/6p1/3P4/7P/PPP1PPP1/RBNKRQBN w EAea - 0 9",
    // {19, 566, 12257, 381197, 9107175, 293397389}},  // 652
    // {"r1kbrqb1/pppp2pp/2n1p1n1/5p1B/4PP2/P7/1PPP2PP/RNK1RQBN w EAea - 2 9",
    // {39, 1359, 53626, 1876028, 73871486, 2633945690}},  // 653
    // {"rnkrqbbn/p1p3pp/1p1ppp2/8/1P6/3P2P1/PKP1PP1P/RN1RQBBN w da - 0 9", {26,
    // 776, 20735, 611907, 16884013, 503561996}},  // 654
    // {"rnkrqnbb/ppp2p1p/3p4/4p1p1/3P3P/N1Q5/PPP1PPP1/R1KR1NBB w DAda - 0 9",
    // {40, 1175, 45637, 1375884, 52620163, 1633655838}},  // 655
    // {"bbrnkrn1/p1pppp2/1p6/6pp/3q4/1P3QP1/P1PPPP1P/BBRNKRN1 w FCfc - 0 9",
    // {34, 1398, 45749, 1712950, 57268492, 2059942014}},  // 656
    // {"br1bkrnq/1p2pppp/pnp5/3p4/P1P5/5P2/1P1PPKPP/BRNB1RNQ w fb - 2 9", {24,
    // 501, 12237, 284936, 7049659, 177940764}},  // 657
    // {"brnkrbn1/pppppp1q/B6p/6p1/8/1P2PP2/P1PP2PP/BRNKR1NQ w EBeb - 0 9", {34,
    // 815, 25868, 700970, 22006883, 639803952}},  // 658
    // {"br1krnqb/pppppp1p/1n4p1/8/8/P2NN3/2PPPPPP/BR1K1RQB w Beb - 2 9", {37,
    // 1029, 36748, 1025712, 36214583, 1026195877}},  // 659
    // {"rbbnkr1q/p1p2ppp/1p1ppn2/8/1PP4P/8/P2PPPP1/RBBNKRNQ w FAfa - 0 9", {28,
    // 755, 22623, 605106, 18972778, 513486101}},  // 660
    // {"r1b1krnq/pp2pppp/1bn5/2pp4/4N3/5P2/PPPPPRPP/R1BBK1NQ w Afa - 0 9", {24,
    // 705, 17427, 532521, 13532966, 426443376}},  // 661
    // {"1nbkrbn1/rpppppqp/p7/6p1/4P3/3P2P1/PPP1KP1P/RNB1RBNQ w e - 1 9", {31,
    // 800, 24748, 693366, 21193292, 625757852}},  // 662
    // {"r1bkrnqb/pp3ppp/n1ppp3/8/1P5P/P7/R1PPPPP1/1NBKRNQB w Eea - 0 9", {21,
    // 482, 11417, 275339, 7112890, 180378139}},  // 663
    // {"rbnkbrnq/ppp1p2p/5p2/3p2p1/1B1P4/1N4P1/PPP1PP1P/RB1K1RNQ w FAfa - 0 9",
    // {33, 780, 25532, 628945, 20756770, 535497008}},  // 664
    // {"rnk1brnq/pp1ppppp/2p5/b7/8/1P2P2P/P1PP1PPQ/RNKBBRN1 w FAfa - 3 9", {29,
    // 648, 19043, 449637, 13722785, 341389148}},  // 665
    // {"rnkrbbnq/p1p3pp/5p2/1p1pp3/P7/1PN2P2/2PPP1PP/R1KRBBNQ w DAda - 0 9",
    // {26, 827, 21865, 683167, 18916370, 589161126}},  // 666
    // {"r1krbnqb/p1pp1ppp/2n1p3/8/1p4P1/PPP5/3PPP1P/RNKRBNQB w DAda - 1 9",
    // {25, 540, 14709, 331332, 9491817, 225389422}},  // 667
    // {"rbnkrnbq/ppp1pp2/3p2p1/2N5/P6p/2P5/1P1PPPPP/RB1KRNBQ w EAea - 0 9",
    // {32, 790, 25107, 661207, 20906017, 578332225}},  // 668
    // {"rnkbrn1q/1ppppppb/8/p4N1p/8/P1N5/1PPPPPPP/R1KBR1BQ w EAea - 0 9", {31,
    // 691, 20813, 510665, 15308408, 404129987}},  // 669
    // {"rnkrnbbq/p1p2ppp/3pp3/1p6/6P1/4PQ1B/PPPP1P1P/RNKRN1B1 w DAda - 0 9",
    // {29, 558, 16800, 352887, 10825379, 246965507}},  // 670
    // {"rnkrnqbb/pp2p1p1/3p3p/2p2p2/5P2/1P1N4/P1PPPQPP/RNKR2BB w DAda - 0 9",
    // {29, 762, 23210, 644936, 20522675, 596067005}},  // 671
    // {"bb1rknnr/ppqppppp/8/2p5/3P1N2/1P6/P1P1PPPP/BBQRKN1R w HDhd - 1 9", {33,
    // 963, 32279, 1000890, 34552118, 1124738493}},  // 672
    // {"bqrbknnr/ppp1p2p/8/3p1p2/5p2/P3N2P/1PPPP1P1/BQRBK1NR w HChc - 0 9",
    // {20, 398, 9009, 194859, 4834319, 113660536}},  // 673
    // {"b1rk1bnr/qpp1pppp/p4n2/3p4/3PPP2/7N/PPP3PP/BQRKNB1R w HChc - 1 9", {25,
    // 648, 16587, 455720, 12200870, 351766307}},  // 674
    // {"bqkrnnrb/pppp2p1/4pp2/4P2p/6P1/7P/PPPP1P2/BQRKNNRB w GC - 1 9", {30,
    // 493, 15118, 280726, 8786998, 181492621}},  // 675
    // {"q1brknnr/1p1ppppp/p7/2p5/8/1PPP4/P2RPPPP/QBB1KNNR w Hhd - 0 9", {25,
    // 501, 13206, 290463, 7982978, 192717198}},  // 676
    // {"qrb1k1nr/ppppb1pp/6n1/4ppN1/3P4/4N3/PPP1PPPP/QRBBK2R w HBhb - 2 9",
    // {31, 872, 26191, 739276, 22493014, 646855304}},  // 677
    // {"1rbknbnr/1ppp1pp1/q6p/p3p3/5P2/2PPB3/PP2P1PP/QR1KNBNR w HBhb - 0 9",
    // {28, 1020, 28147, 984000, 27484692, 947786800}},  // 678
    // {"qrbk2rb/1ppp1ppp/5nn1/p3p3/1N6/P7/1PPPPPPP/QRB1KNRB w gb - 0 9", {23,
    // 592, 14398, 395716, 10098215, 293988585}},  // 679
    // {"qbrk1nnr/1pp1pppp/2b5/p2p4/P2P2P1/8/1PP1PP1P/QBKRBNNR w hc - 1 9", {26,
    // 654, 18103, 471653, 13740891, 373081138}},  // 680
    // {"qrkbbnnr/ppp2p1p/4p3/3p2p1/P7/2PP4/1P2PPPP/QRKBBNNR w HBhb - 0 9", {25,
    // 626, 16616, 431634, 12079406, 324006164}},  // 681
    // {"qr1kbbnr/ppp1pp1p/4n1p1/2Pp4/6P1/4N3/PP1PPP1P/QRK1BBNR w HB d6 0 9",
    // {26, 699, 18068, 497152, 13353359, 375702908}},  // 682
    // {"qrk1b1rb/p1pppppp/3nnQ2/1p6/1P3P2/3P4/P1P1P1PP/1RKNBNRB w GBgb - 3 9",
    // {43, 1369, 55463, 1831200, 71514365, 2427477375}},  // 683
    // {"qbrk1nbr/pppp3p/5n2/4ppp1/3P1P2/4N3/PPP1P1PP/QBKRN1BR w hc - 0 9", {25,
    // 752, 20165, 615263, 17493373, 543180234}},  // 684
    // {"qrkb1nbr/1pppppQp/3n4/p7/5p2/1P1N4/P1PPP1PP/1RKB1NBR w HBhb - 0 9",
    // {45, 946, 40100, 966903, 39736157, 1051910977}},  // 685
    // {"qrk1nbbr/ppp1p1p1/4n2p/3p1p2/1P5P/3P2P1/P1P1PP2/QRKNNBBR w HBhb - 1 9",
    // {32, 770, 25367, 646977, 21717615, 577979364}},  // 686
    // {"qrkn1rbb/pp2pppp/2p5/3p4/P2Qn1P1/1P6/2PPPP1P/1RKNNRBB w FBfb - 0 9",
    // {38, 943, 35335, 868165, 31909835, 798405123}},  // 687
    // {"bbrqknnr/ppp4p/3pp3/5pp1/4PP2/5Q2/PPPP2PP/BBR1KNNR w HChc - 0 9", {36,
    // 843, 29974, 758528, 26828059, 723306114}},  // 688
    // {"1rqbkn1r/p1p1pppp/1p5n/P2p4/3Pb1P1/8/1PP1PP1P/BRQBKNNR w HBhb - 0 9",
    // {23, 778, 19482, 649789, 17337683, 579112676}},  // 689
    // {"br1knbnr/1qp1pppp/pp1p4/8/8/PP6/2PPPPPP/BRQKNBNR w HBhb - 2 9", {26,
    // 697, 18835, 546622, 15280079, 473071890}},  // 690
    // {"brqk2rb/ppppp1pp/4np2/8/2n5/3P1Q2/PP2PPPP/BR1KNNRB w GBgb - 0 9", {32,
    // 948, 30434, 885713, 29821322, 874251866}},  // 691
    // {"r1bqknnr/pp1pp1p1/5p1p/2p1b2N/2P5/8/PPQPPPPP/RBB1K1NR w HAha - 0 9",
    // {31, 785, 25549, 659952, 22244193, 592797491}},  // 692
    // {"rqbbknnr/ppppp2p/5pp1/8/8/1P3PP1/PQPPP2P/R1BBKNNR w HAha - 0 9", {23,
    // 391, 10163, 198450, 5576671, 121267576}},  // 693
    // {"rqbknbnr/1pp1p2p/p7/3p1pp1/7N/1PP5/P2PPPPP/RQBK1BNR w HAha - 0 9", {27,
    // 676, 19606, 522428, 15955388, 448477218}},  // 694
    // {"rqb1nnrb/2ppkppp/1p2p3/p7/2PPP3/1P6/P4PPP/RQBKNNRB w GA - 1 9", {31,
    // 727, 22895, 570647, 18361051, 483248153}},  // 695
    // {"rb1kbn1r/p1ppppp1/qp5n/7p/P7/RPP5/3PPPPP/1BQKBNNR w Hha - 2 9", {29,
    // 837, 23815, 730083, 21279560, 682863811}},  // 696
    // {"rqkbb1nr/p1p2ppp/1p1p2n1/3Np3/4P3/5N2/PPPP1PPP/RQKBB2R w HAha - 0 9",
    // {28, 717, 20663, 550987, 16347343, 453153783}},  // 697
    // {"rqknbbr1/p1pppp1p/1p3np1/8/4P3/2P2P1P/PP1P2P1/RQKNBBNR w HAa - 0 9",
    // {27, 650, 18231, 475303, 13847463, 383256006}},  // 698
    // {"r1k1bnrb/1qpppppp/1p2n3/p7/1P5P/6P1/P1PPPP2/RQKNBNR1 w GAga - 1 9",
    // {24, 806, 20693, 713220, 19382263, 686009788}},  // 699
    // {"rb1knnbr/1pp1ppp1/p2p3p/5q2/3B2P1/3P1P2/PPP1P2P/RBQKNN1R w HAha - 0 9",
    // {34, 1360, 44096, 1605706, 51973672, 1837704407}},  // 700
    // {"rqkb1nbr/p1p1ppp1/1p3n1p/2Qp4/8/2P5/PP1PPPPP/R1KBNNBR w HAha - 2 9",
    // {39, 983, 38218, 940989, 36347815, 918801645}},  // 701
    // {"rqknnbbr/2pppp2/pp5p/6p1/1P1P4/4PP2/P1P3PP/RQKNNBBR w HAha - 0 9", {26,
    // 628, 17638, 464924, 13787303, 386125234}},  // 702
    // {"rqkn1rbb/1pp1pppp/p7/3p4/3Pn3/2P1PP2/PP4PP/RQKNNRBB w FAfa - 1 9", {20,
    // 527, 12216, 321533, 8082183, 219311659}},  // 703
    {"bbrkqn1r/1pppppp1/5n2/p7/1PP2P1p/7N/P2PP1PP/BBRKQN1R w HChc - 1 9",
     {36, 963, 35291, 973839, 35907489, 1034223364}},  // 704
    // {"brkbqn1r/p2ppppp/7n/1p6/P1p3PP/8/1PPPPP1N/BRKBQ1NR w HBhb - 0 9", {18,
    // 583, 11790, 394603, 8858385, 304339862}},  // 705
    // {"brkq1bnr/pp1ppp1p/8/2p2np1/P7/8/1PPPPPPP/BRKQNBNR w HBhb - 0 9", {19,
    // 552, 11811, 354260, 8432183, 262293169}},  // 706
    // {"brkqnnrb/1ppppppp/8/8/p3P3/5N2/PPPP1PPP/BRKQ1NRB w GBgb - 3 9", {21,
    // 397, 9653, 204350, 5489836, 128389738}},  // 707
    // {"rbbkq1nr/1p2pppp/p1p3nB/3p4/1Q1P4/6N1/PPP1PPPP/RB1K2NR w HAha - 0 9",
    // {40, 1132, 43404, 1260470, 47425783, 1415578783}},  // 708
    // {"rkbbq1nr/1pppp1p1/4np2/p6p/8/PP3P2/1KPPP1PP/R1BBQNNR w ha - 0 9", {24,
    // 596, 15220, 402121, 10822049, 302056813}},  // 709
    // {"r1bqn1nr/pkpppp1p/1p4pb/8/PN6/R7/1PPPPPPP/1KBQ1BNR w H - 2 9", {33,
    // 794, 25450, 649150, 20919309, 561073410}},  // 710
    // {"rkb1nnrb/1pppq1pp/p4p2/4p3/5P2/1P1PB3/P1P1P1PP/RK1QNNRB w GAga - 0 9",
    // {26, 625, 17050, 442036, 12515042, 342967558}},  // 711
    // {"rbkqbn1r/pppp1p1p/2n1p1p1/8/8/1P1PP1N1/P1P2PPP/RBKQB1NR w HAha - 1 9",
    // {30, 660, 20308, 492714, 15348335, 403323883}},  // 712
    // {"rkqbb1n1/pppppppr/8/6np/5P2/8/PPPPP1PP/RKQBBNNR w HAa - 6 9", {23, 500,
    // 12154, 292936, 7519117, 196524441}},  // 713
    // {"rkqnbbnr/ppppppp1/8/7p/3N4/6PP/PPPPPP2/RKQNBB1R w HAa - 0 9", {24, 484,
    // 12495, 284570, 7775173, 193947530}},  // 714
    // {"rkqnb1rb/p1p1pppp/1p1p4/2n5/3P4/2P1N1N1/PP2PPPP/RKQ1B1RB w GAga - 0 9",
    // {28, 1020, 29124, 1027904, 30515456, 1073711823}},  // 715
    // {"rbk1nnbr/1ppq1ppp/p2p4/4p3/P3B2P/2P5/1P1PPPP1/R1KQNNBR w HAha - 2 9",
    // {38, 998, 37265, 1047592, 38552638, 1139322479}},  // 716
    // {"r1qbn1br/k1pppppp/6n1/pp6/5P1P/P7/1PPPP1PB/RKQBNN1R w HA - 1 9", {22,
    // 549, 12867, 348574, 8725809, 251613569}},  // 717
    // {"rkqnn1br/pppp3p/4p1pb/5p2/P2P4/7P/1PP1PPPB/RKQNNB1R w HAha - 1 9", {32,
    // 659, 21249, 469701, 15434721, 365761521}},  // 718
    // {"rk1nnrbb/p1p1pppp/1p6/3p1q2/P3P3/2NN4/1PPP1PPP/RKQ2RBB w FAfa - 3 9",
    // {29, 989, 29087, 980477, 29643404, 998848556}},  // 719
    // {"bbrk1q1r/ppppppp1/3n4/7p/3Pn3/6PN/PPP1PPNP/BBRK1Q1R w HChc - 2 9", {23,
    // 712, 16551, 516177, 12995202, 411077508}},  // 720
    // {"brkbnq1r/p1ppp2p/5ppn/1p6/5P2/1P1P2P1/P1P1P2P/BRKBNQNR w HBhb - 0 9",
    // {28, 856, 24984, 780503, 23529352, 754501112}},  // 721
    // {"br1k1bnr/ppppp1pp/4np2/1B2P2q/3P4/8/PPP2PPP/BRKNQ1NR w HB - 3 9", {36,
    // 1214, 40615, 1328331, 45096834, 1470987023}},  // 722
    // {"brk1qnrb/pnppp1p1/1p6/5p1p/8/5PPP/PPPPP1R1/BRKNQN1B w Bgb - 0 9", {22,
    // 551, 13111, 353317, 9040545, 259643605}},  // 723
    // {"rbbkn1nr/1ppp2pp/p3p3/2q2p2/3P4/6P1/PPPBPP1P/RB1KNQNR w HAha - 0 9",
    // {31, 1060, 31332, 1015099, 30314172, 976268967}},  // 724
    // {"rkbbn1nr/ppppp1pp/8/6N1/5p2/1q6/P1PPPPPP/RKBBN1QR w HAha - 0 9", {3,
    // 72, 1919, 50827, 1400832, 39654253}},  // 725
    // {"rkb2bnr/pp2pppp/2p1n3/3p4/q2P4/5NP1/PPP1PP1P/RKBNQBR1 w Aha - 0 9",
    // {29, 861, 24504, 763454, 22763215, 731511256}},  // 726
    // {"rkbq1nrb/ppppppp1/7p/8/1P1n4/P4P1P/2PPP1P1/RKBNQNRB w GAga - 0 9", {25,
    // 672, 17631, 473864, 12954224, 361237536}},  // 727
    // {"rbknb1nr/ppp1qp1p/6p1/3pp3/3P3P/2B1P3/PPP2PP1/RBKN1QNR w HAha - 1 9",
    // {27, 857, 24688, 792538, 23790033, 768247869}},  // 728
    // {"rknbbq1r/p1pppppp/1p2N3/8/3n4/2P5/PP1PPPPP/RK1BBQNR w HAha - 4 9", {29,
    // 763, 22138, 574054, 16926075, 447896703}},  // 729
    // {"r1nqbbnr/1pppp1pp/1k6/p4p2/8/4P3/PPPP1PPP/RKN1BBNR w HA - 0 9", {26,
    // 658, 17302, 464039, 12380488, 349047256}},  // 730
    // {"rkn2qrb/ppp1pppp/6n1/1b1p4/1P6/4PPB1/P1PP2PP/RKNQ1NRB w GAga - 3 9",
    // {23, 574, 14070, 370324, 9501401, 263870337}},  // 731
    // {"rbkn2br/ppppp1p1/4np1p/1P5q/8/2P1N3/P2PPPPP/RBK1QNBR w HAha - 1 9",
    // {29, 992, 29506, 999564, 30148787, 1045942540}},  // 732
    // {"1knbqnbr/1ppppp1p/r5p1/p7/7P/2PN2P1/PP1PPP2/RK1BQNBR w HAh - 2 9", {26,
    // 698, 19395, 512023, 14848229, 402599313}},  // 733
    // {"rk1qnbbr/pnpppp1p/6p1/1p6/3P4/1P6/P1P1PPPP/RKNQNBBR w HAha - 1 9", {20,
    // 480, 11159, 287539, 7425917, 203194521}},  // 734
    // {"rknqnrbb/pp1p2p1/5p1p/2p1p3/2P1P3/P2P4/1P3PPP/RKNQNRBB w FAfa - 0 9",
    // {26, 679, 18116, 494953, 13790137, 392629571}},  // 735
    // {"bbrk2qr/pp1p1ppp/3n2n1/2p1p3/3P1P2/6N1/PPP1P1PP/BBRKN1QR w HChc - 0 9",
    // {26, 790, 21521, 673269, 19259490, 617563700}},  // 736
    // {"b1krnnqr/1p1ppppp/p1p5/b6B/P7/4P1N1/1PPP1PPP/BRK1N1QR w HB - 2 9", {26,
    // 625, 16451, 415452, 11490615, 304805107}},  // 737
    // {"1rknnbqr/3ppppp/p7/1pp5/4b2P/P4P2/1PPPP1PR/BRKNNBQ1 w Bhb - 1 9", {24,
    // 757, 19746, 618777, 17275100, 544309489}},  // 738
    // {"br1nn1rb/pppkpqpp/3p1p2/8/PP6/4N3/1KPPPPPP/BR2NQRB w - - 3 9", {24,
    // 682, 17129, 482711, 13057308, 375033550}},  // 739
    // {"rbbkn1qr/pppp2p1/6np/4pp2/7N/7P/PPPPPPPR/RBBK1NQ1 w Aha - 0 9", {22,
    // 586, 14158, 409891, 10607781, 324452612}},  // 740
    // {"rk1bn1qr/pppbpppp/4n3/4p3/4P3/5P2/PPPP2PP/RKBB1NQR w HAha - 1 9", {22,
    // 530, 13440, 348004, 9514787, 259898748}},  // 741
    // {"rkbnnbqr/1ppp1ppp/p7/4p3/8/QP3P2/P1PPP1PP/RKBNNB1R w HAha - 0 9", {29,
    // 705, 21511, 551042, 17524731, 472356665}},  // 742
    // {"1kbnnqrb/1pp1p1pp/r4p2/p2p4/N4P2/3P4/PPP1P1PP/RKB1NQRB w GAg - 2 9",
    // {21, 623, 14979, 437554, 11601134, 343214006}},  // 743
    // {"rbknbn1r/pppp1p1p/4p1q1/8/P1P3Pp/8/1P1PPP2/RBKNBNQR w HAha - 0 9", {30,
    // 813, 24959, 708454, 23379040, 692576573}},  // 744
    // {"rk1bb1qr/2pppppp/p2nn3/1p4P1/6QP/8/PPPPPP2/RKNBBN1R w HAha - 2 9", {36,
    // 857, 30124, 757524, 26485812, 696999449}},  // 745
    // {"rkn1bbqr/p2ppppp/2p1n3/1p6/4PP2/6PP/PPPP4/RKNNBBQR w HAha - 0 9", {33,
    // 687, 22744, 511018, 17101732, 412778368}},  // 746
    // {"rkn1bqrb/pnp1pppp/3p4/8/Pp6/1N2NP2/1PPPP1PP/RK2BQRB w GAga - 0 9", {28,
    // 591, 17174, 406025, 12182448, 312575205}},  // 747
    // {"rbk1n1br/ppp1ppqp/2n5/2Np2p1/8/2P5/PPBPPPPP/R1KN1QBR w HAha - 4 9",
    // {35, 930, 30663, 844433, 27160490, 780616047}},  // 748
    // {"rknbn1br/1ppp1ppp/p3p3/8/1q6/2P2N1P/P2PPPP1/RKNB1QBR w HAha - 0 9", {4,
    // 157, 3697, 138102, 3454704, 125373395}},  // 749
    // {"rkn1qbbr/pp3ppp/4n3/2ppp3/4P1P1/P2P4/1PP2P1P/RKNNQBBR w HAha - 0 9",
    // {28, 840, 24437, 771328, 23200961, 756489357}},  // 750
    // {"rkn1qrbb/pp1ppp2/2p1n1p1/7p/2P2P1P/6P1/PP1PP3/RKNNQRBB w FAfa - 1 9",
    // {32, 867, 27595, 757836, 24485663, 688115847}},  // 751
    // {"b1rknnrq/bpppp1p1/p6p/5p1P/6P1/4N3/PPPPPP2/BBRKN1RQ w GCgc - 1 9", {33,
    // 851, 28888, 763967, 26686205, 731944177}},  // 752
    // {"brkb1nr1/pppppp2/3n2pp/3B4/1P6/4P3/PqPP1PPP/BRK1NNRQ w GBgb - 2 9", {4,
    // 98, 2965, 76143, 2352530, 64251468}},  // 753
    // {"brk1nbrq/1ppppn1p/6p1/p4p2/P5P1/5R2/1PPPPP1P/BRKNNB1Q w Bgb - 0 9",
    // {29, 922, 27709, 879527, 27463717, 888881062}},  // 754
    // {"brkn1rqb/1p1ppppp/3n4/p1p5/1P3P2/8/PNPPP1PP/BR1KNRQB w fb - 1 9", {29,
    // 633, 19399, 469818, 15076198, 396737074}},  // 755
    // {"rb1k1nrq/pbp1pppp/1p1p1n2/8/5P2/4NN1P/PPPPP1P1/RBBK2RQ w GAga - 2 9",
    // {28, 841, 24056, 710751, 20772996, 613798447}},  // 756
    // {"rkbbnnrq/p1pp3p/4p1p1/1p3p2/P6P/1P6/1BPPPPP1/RK1BNNRQ w GAga - 0 9",
    // {33, 957, 30668, 907217, 29735654, 903933626}},  // 757
    // {"rk2nbrq/p1ppppp1/bpn5/7p/6P1/2N2P2/PPPPP1QP/RKB1NBR1 w GAga - 2 9",
    // {24, 687, 18206, 544627, 15518417, 484217179}},  // 758
    // {"rkbn1r1b/pp1pppnp/6q1/2p3p1/5P1P/4N3/PPPPP1P1/RKB1NRQB w FAfa - 1 9",
    // {23, 831, 21254, 754622, 21126103, 744755212}},  // 759
    // {"rbknb1rq/ppp1p1p1/3pnp1p/8/6PP/2PP4/PP2PP2/RBKNBNRQ w GAga - 0 9", {31,
    // 838, 26800, 736910, 24008129, 677776408}},  // 760
    // {"rknbb1rq/p1pn1ppp/4p3/1p1p4/2P5/1P2N1P1/P2PPP1P/RKNBB1RQ w GAga - 1 9",
    // {29, 830, 24798, 721630, 22243832, 660040360}},  // 761
    // {"rk1nbbrq/pp1p1ppp/3n4/P3p3/2p4P/8/1PPPPPP1/RKNNBBRQ w GAga - 1 9", {24,
    // 484, 12776, 297419, 8379748, 214004367}},  // 762
    // {"rknnbr1b/ppp2pqp/3p4/4p1p1/7P/3P1P2/PPP1P1P1/RKNNBRQB w FAfa - 0 9",
    // {32, 838, 26408, 740701, 23472124, 699211365}},  // 763
    // {"rb1k1rbq/ppppN1pp/2nn4/5p2/7P/8/PPPPPPP1/RBK1NRBQ w FA - 1 9", {27,
    // 800, 22785, 701742, 20804424, 660917073}},  // 764
    // {"r1nbnrbq/kppppp1p/6p1/8/p1PP1P2/4P3/PP4PP/RKNBNRBQ w FA - 1 9", {28,
    // 757, 21198, 602699, 17180857, 507618340}},  // 765
    // {"rkn1rbbq/p1pppppp/2n5/1pP5/8/1N2P3/PP1P1PPP/RK1NRBBQ w EAea - 1 9",
    // {22, 483, 11890, 283679, 7497674, 191130942}},  // 766
    // {"rknnrqbb/2pppppp/8/p7/Np3P2/3P4/PPP1P1PP/RKN1RQBB w EAea - 0 9", {25,
    // 536, 14456, 339180, 9694947, 245669668}},  // 767
    // {"bb1rknrn/1qppppp1/1p4B1/p6N/8/2P5/PP1PPPPP/B1QRK1RN w GDgd - 1 9", {32,
    // 715, 22421, 575008, 17860156, 502410909}},  // 768
    // {"b1rbknrn/qpp1ppp1/p6p/3p4/2P5/1P1P1P2/P3P1PP/BQRBKNRN w GCgc - 0 9",
    // {30, 818, 24421, 688711, 20981488, 611986786}},  // 769
    // {"bqkrnbrn/1pp1pp1p/p7/1B1p2p1/4P3/7P/PPPP1PP1/BQKRN1RN w - - 0 9", {28,
    // 676, 18366, 478054, 13126287, 363765666}},  // 770
    // {"bqrknrnb/1p2ppp1/p1pp3p/8/3P1P2/1PP5/P3P1PP/BQRKNRNB w FCfc - 0 9",
    // {31, 646, 20686, 455607, 14984618, 349082278}},  // 771
    // {"qbbrkn1r/pppppp1p/8/6p1/2P1Pn1P/6N1/PP1P1PP1/QBBRKNR1 w GDd - 3 9",
    // {20, 532, 11581, 303586, 7512432, 202967948}},  // 772
    // {"1rbbknr1/p1ppp1pp/1pq2pn1/8/3P4/P3P3/QPP2PPP/1RBBKNRN w GBgb - 3 9",
    // {31, 1002, 30581, 999607, 30642468, 1009228283}},  // 773
    // {"qrbkn1rn/pppp1ppp/8/6b1/P1P1Pp2/8/1P1P2PP/QRBKNBRN w GBgb - 0 9", {22,
    // 505, 12447, 304863, 8192621, 214730959}},  // 774
    // {"qrbk1rnb/p2ppp1p/5n2/1pp3p1/8/7P/PPPPPPPN/QRBKR1NB w Bfb - 0 9", {20,
    // 619, 13448, 449630, 10571176, 369603424}},  // 775
    // {"qbrkb1r1/ppp2ppp/3pn1n1/P3p3/4P3/3P4/1PP2PPP/QBRKBNRN w GCgc - 1 9",
    // {26, 755, 20596, 604483, 17164382, 510878835}},  // 776
    // {"qrkbb1r1/ppp1pnpp/3p2n1/5p2/1P3P2/2Q3N1/P1PPP1PP/1RKBB1RN w GBgb - 0
    // 9", {35, 918, 32244, 870888, 30933394, 867833733}},  // 777
    // {"qrknbbrn/ppp1ppp1/8/7p/2Bp4/4PPP1/PPPP3P/QRKNB1RN w GBgb - 0 9", {27,
    // 593, 16168, 376808, 10422676, 258348640}},  // 778
    // {"qrk1brnb/ppppp3/4n2p/5pp1/2PP4/2N4P/PP2PPP1/QRK1BRNB w FBfb - 2 9",
    // {24, 672, 17447, 506189, 13765777, 414930519}},  // 779
    // {"qbrknrb1/p2ppppp/2p3n1/8/p4P2/6PP/1PPPP3/QBRKNRBN w FCfc - 0 9", {29,
    // 759, 23235, 634493, 20416668, 584870558}},  // 780
    {"1rkb1rbn/p1pp1ppp/3np3/1p6/4qP2/3NB3/PPPPPRPP/QRKB3N w Bfb - 0 9",
     {22, 923, 22585, 914106, 24049880, 957218571}},  // 781
    // {"1rknrbbn/p1pp1p1p/8/1p2p1p1/4qPP1/2P5/PP1PP1BP/QRKNR1BN w EBeb - 0 9",
    // {28, 1309, 36355, 1568968, 44576409, 1846382333}},  // 782
    // {"qrk1rn1b/ppppp2p/4n3/3b1pp1/4P2P/5BP1/PPPP1P2/QRKNRNB1 w EBeb - 3 9",
    // {26, 839, 22189, 726354, 19978260, 661207281}},  // 783
    // {"bbrqk1rn/pp1ppppp/8/2p5/2P1P3/5n1P/PPBP1PP1/B1RQKNRN w GCgc - 1 9", {3,
    // 95, 2690, 85038, 2518864, 80775549}},  // 784
    // {"brqbk2n/pppppprp/8/6p1/1P3n2/5P2/P1PPP1PP/R1QBKNRN w Gb - 2 9", {22,
    // 593, 13255, 362760, 8922397, 253271592}},  // 785
    // {"brqknbr1/pp3ppp/3p2n1/2p1p3/2P5/5P2/PPKPP1PP/BRQ1NBRN w gb - 0 9", {21,
    // 590, 13190, 397355, 9581695, 304103516}},  // 786
    // {"1rqknrnb/2pp1ppp/p3p3/1p6/P2P4/5bP1/1PP1PP1P/BRQKNRNB w FBfb - 0 9",
    // {24, 737, 20052, 598439, 17948681, 536330341}},  // 787
    // {"rbb1k1rn/p1pqpppp/6n1/1p1p4/5P2/3PP3/PPP1K1PP/RBBQ1NRN w ga - 3 9",
    // {24, 694, 16773, 513782, 13094823, 419402704}},  // 788
    // {"rqbbknr1/1ppp2pp/p5n1/4pp2/P7/1PP5/1Q1PPPPP/R1BBKNRN w GAga - 0 9",
    // {24, 600, 15347, 408207, 11029596, 308553169}},  // 789
    // {"rqbknbrn/2pppppp/6Q1/pp6/8/2P5/PP1PPPPP/R1BKNBRN w GAga - 2 9", {40,
    // 949, 34100, 889887, 31296485, 881529007}},  // 790
    // {"rqbknr1b/pp1ppp2/2p2n1p/6p1/8/3P1PPP/PPP1P3/RQBKNRNB w FAfa - 0 9",
    // {20, 560, 12275, 373921, 8687544, 277906201}},  // 791
    // {"rbqkbnrn/p3pppp/1p6/3p4/P1p3P1/1P6/1QPPPP1P/RB1KBNRN w GAga - 0 9",
    // {30, 1155, 35865, 1351455, 43092716, 1614019629}},  // 792
    // {"rqkbb1rn/p1p1pppn/1p1p4/7p/4PP2/7P/PPPPB1P1/RQK1BNRN w GAga - 1 9",
    // {30, 701, 20804, 515942, 15450970, 401499189}},  // 793
    // {"rqknbbrn/1p2pp1p/3p2p1/p1p5/P2P4/1P6/1KP1PPPP/RQ1NBBRN w ga - 0 9",
    // {28, 756, 21655, 610320, 17989811, 525585996}},  // 794
    // {"rqknbrnb/1pp3pp/5p2/p2pp3/P7/3PPN2/1PP2PPP/RQKNBR1B w FAfa - 0 9", {26,
    // 731, 19509, 550395, 15209404, 439767476}},  // 795
    // {"rbqkr1bn/p1pppp1p/1p1n4/6p1/7P/3P1PP1/PPP1P3/RBQKNRBN w FAa - 0 9",
    // {27, 586, 16282, 381604, 10905865, 274364342}},  // 796
    // {"rqk1nrb1/ppbp1ppp/4p1n1/2p5/7P/1PP5/P2PPPP1/RQKBNRBN w FAfa - 1 9",
    // {27, 749, 21480, 602318, 18084787, 520547029}},  // 797
    // {"rqknrbbn/pp1p1ppp/4p3/2p5/3P2P1/7P/PPP1PP2/RQKNRBBN w EAa - 0 9", {20,
    // 533, 11829, 336248, 8230417, 245871540}},  // 798
    // {"rqknrnbb/pp1ppp1p/2p3p1/8/8/1P2P1NP/P1PP1PP1/RQKNR1BB w EAea - 0 9",
    // {22, 633, 14480, 441877, 10827868, 343525739}},  // 799
    // {"1brkq1rn/2pppppp/1p2n3/p2bN3/8/7P/PPPPPPP1/BBRKQ1RN w GCgc - 2 9", {27,
    // 748, 20134, 580054, 16010135, 475206624}},  // 800
    // {"brkbqnrn/2pp1ppp/8/1p2p3/Pp2N3/8/2PPPPPP/BRKBQNR1 w GBgb - 0 9", {30,
    // 827, 25308, 757837, 23746165, 751690068}},  // 801
    // {"brk1nbrn/pp1ppppp/2p5/7P/5P2/q2P4/PPP1P1P1/BRKQNBRN w GBgb - 1 9", {15,
    // 471, 8716, 276424, 5960901, 190316951}},  // 802
    // {"brkqnrnb/1p1pp1p1/p4p2/2p4p/8/P2PP3/1PP1QPPP/BRK1NRNB w FBfb - 0 9",
    // {24, 479, 12584, 280081, 7830230, 190419716}},  // 803
    // {"rbbkqnrn/2ppp2p/pp3p2/6p1/P6P/8/RPPPPPP1/1BBKQNRN w Gga - 0 9", {21,
    // 523, 12125, 328733, 8322614, 242240658}},  // 804
    // {"rkbbqr1n/1ppppppn/7p/p7/4P3/2P2P2/PP1PB1PP/RKB1QNRN w GAa - 3 9", {27,
    // 563, 16026, 372148, 11105151, 283211800}},  // 805
    // {"rkbqnbrn/ppppp3/8/5ppp/2P3P1/7P/PPQPPP2/RKB1NBRN w GAga - 0 9", {28,
    // 639, 19250, 469250, 14872172, 384663405}},  // 806
    // {"rkb1nrnb/pppp1pp1/5q1p/8/P3p3/4R1P1/1PPPPP1P/1KBQNRNB w Ffa - 0 9",
    // {28, 873, 23690, 720814, 20209424, 625281937}},  // 807
    // {"rbkqb1rn/1p1ppppp/4n3/p1p5/8/3PBP2/PPP1P1PP/RBKQ1NRN w GAga - 0 9",
    // {26, 798, 21416, 667496, 18475618, 591681956}},  // 808
    // {"rk1qbnrn/1p1ppppp/1b6/p1p5/P7/2P3NP/1P1PPPP1/RKQBB1RN w GAga - 0 9",
    // {22, 506, 12313, 301029, 7891676, 205739580}},  // 809
    // {"rk1nbbrn/ppp1ppp1/8/3p3p/1P1P2q1/5PB1/P1P1P1PP/RKQN1BRN w GAga - 1 9",
    // {31, 956, 29219, 903799, 27827461, 876341492}},  // 810
    // {"rkqnbr1b/pp1pppp1/7p/2p2n2/P2P4/7N/RPP1PPPP/1KQNBR1B w Ffa - 0 9", {31,
    // 750, 24267, 646252, 21639104, 617064197}},  // 811
    // {"rbkq1rbn/2p1pppp/pp3n2/3p4/5P2/3N2N1/PPPPP1PP/RBKQR1B1 w Afa - 2 9",
    // {26, 647, 18027, 465119, 13643783, 369702807}},  // 812
    // {"rkqbr1bn/p2ppppp/1pp2n2/8/5P2/3P1N2/PPP1PRPP/RKQB2BN w Aa - 3 9", {24,
    // 574, 14593, 371597, 10066892, 271121237}},  // 813
    // {"rk1qrbbn/p1ppp1pp/1p2n3/5p2/1P6/K3N3/P1PPPPPP/R1Q1RBBN w ea - 0 9",
    // {25, 548, 14069, 340734, 9043111, 235545764}},  // 814
    // {"rkqnrnbb/pp1pp3/2p5/5ppp/8/PP4NP/2PPPPP1/RKQNR1BB w EAea - 0 9", {23,
    // 727, 18228, 566572, 15078056, 471296844}},  // 815
    // {"bbrknq1r/ppppppp1/8/7p/5n2/3P4/PPP1PNPP/BBKRNQR1 w c - 0 9", {21, 610,
    // 13300, 394705, 9605845, 293532398}},  // 816
    // {"brkbnqr1/2pppnpp/pp3p2/8/4PPPP/8/PPPP4/BRKBNQRN w GBgb - 1 9", {30,
    // 757, 23908, 621332, 20360394, 548380577}},  // 817
    // {"brk1qb1n/ppppppr1/2n3pp/8/2P3P1/2N5/PP1PPP1P/BR1KQBRN w b - 1 9", {26,
    // 570, 15537, 352883, 10081351, 242864559}},  // 818
    // {"brknq1nb/pp2prpp/8/2pP1p2/6P1/2N5/PPPP1P1P/BRK1QRNB w FBb - 1 9", {33,
    // 830, 27897, 764915, 26262884, 765831403}},  // 819
    // {"rbbk1qrn/ppp1p1pp/5p2/3p1n2/7N/P7/1PPPPPPP/RBB1KQRN w ga - 0 9", {21,
    // 562, 13060, 378883, 9520963, 290579255}},  // 820
    // {"rk1b1qrn/ppp1pppp/5n2/3pN3/P6P/7b/1PPPPPP1/RKBB1QRN w GAga - 4 9", {28,
    // 677, 19235, 488740, 14354779, 383207197}},  // 821
    // {"rkbnqbrn/pp1ppp1p/2p5/6p1/P7/4P3/KPPPQPPP/R1BN1BRN w - - 3 9", {28,
    // 585, 17443, 401483, 12574541, 310495538}},  // 822
    // {"rk1nqrnb/pbpppp2/1p4p1/7p/P7/5NP1/1PPPPPBP/RKBNQR2 w FAfa - 2 9", {26,
    // 774, 21626, 645200, 19093408, 576325868}},  // 823
    // {"rbknb1rn/p1pp2pp/1p6/4pp2/1q3P1B/2N5/PPPPPNPP/RBK2QR1 w GAga - 2 9",
    // {31, 1206, 36940, 1374158, 42849564, 1555711209}},  // 824
    // {"rk1bbqrn/pp1pp1pp/3n4/5p2/3p4/1PP5/PK2PPPP/R1NBBQRN w ga - 0 9", {21,
    // 629, 14059, 429667, 10587910, 332632033}},  // 825
    // {"rknqbbr1/p1pp1pp1/1p4n1/4p2p/4P1P1/6RB/PPPP1P1P/RKNQB2N w Aga - 0 9",
    // {27, 753, 20918, 593155, 17318772, 507563675}},  // 826
    // {"rknqbr1b/pppp1ppp/4p2n/8/1P3P2/4P3/P1PPN1PP/RKNQBR1B w FAfa - 2 9",
    // {26, 623, 17177, 460663, 13389799, 383508368}},  // 827
    // {"r2kqrbn/bppppppp/2n5/p4B2/5P2/2P5/PP1PP1PP/1RKNQRBN w F - 2 9", {39,
    // 1026, 37800, 1011922, 35946987, 992756232}},  // 828
    // {"rk1bqrb1/ppppppp1/1n6/7p/2P2P1n/4P1Q1/PP1P2PP/RKNB1RBN w FAfa - 0 9",
    // {35, 760, 25817, 610557, 21014787, 536852043}},  // 829
    // {"rkq1rb1n/ppppp1pp/1n6/5p2/PPb2P2/8/1KPPP1PP/R1NQRBBN w ea - 1 9", {27,
    // 754, 21009, 568788, 16461795, 448313956}},  // 830
    // {"rknqr2b/pppnp1pp/3p4/3b1p2/8/1N1P2N1/PPP1PPPP/RKQ1R1BB w EAea - 1 9",
    // {27, 803, 23708, 700453, 21875031, 654754840}},  // 831
    // {"bbrknrqn/ppppp1pB/8/2P2p1p/8/5N2/PP1PPPPP/B1RK1RQN w FCfc - 0 9", {30,
    // 799, 23923, 671112, 20532790, 603059376}},  // 832
    // {"brkbnrq1/1pppp1p1/6np/p4p2/4P3/1PP5/P1KP1PPP/BR1BNRQN w fb - 1 9", {27,
    // 726, 19329, 555622, 15156662, 457601127}},  // 833
    // {"brknrbq1/1p1p1ppp/p3p1n1/2p5/8/1P1BPP2/P1PP2PP/BRKNR1QN w EBeb - 0 9",
    // {36, 786, 27868, 655019, 22852433, 577223409}},  // 834
    // {"brknrqnb/p2ppp1p/2p5/1p6/3P2p1/P1P1N3/1P2PPPP/BRK1RQNB w EBeb - 0 9",
    // {23, 649, 15169, 440504, 10687843, 320881984}},  // 835
    // {"rbbk1rqn/1ppppppp/3n4/p7/2P5/3N4/PP1PPPPP/RBB1KRQN w fa - 1 9", {20,
    // 478, 11094, 275250, 7094988, 185488058}},  // 836
    // {"rkbbnrqn/p2p1ppp/1p2p3/8/P1p1P3/1BP5/1P1P1PPP/RKB1NRQN w FAfa - 0 9",
    // {22, 570, 13295, 346811, 8671852, 229898448}},  // 837
    // {"rkb1rb1n/ppppppqp/8/2n3p1/2P1P1P1/8/PP1P1P1P/RKBNRBQN w EAea - 1 9",
    // {23, 663, 16212, 490748, 12900485, 404944553}},  // 838
    // {"rkb1rqnb/pppp3p/2n3p1/4pp2/P2P3P/2P5/1P2PPP1/RKBNRQNB w EAea - 0 9",
    // {25, 845, 22188, 741972, 20276176, 683290790}},  // 839
    // {"rbk1brqn/ppp1pppp/8/3p4/7P/1P4P1/2PPPP2/RBKNBRQN w FAfa - 0 9", {24,
    // 526, 13862, 322175, 9054028, 222704171}},  // 840
    // {"rknbbrqn/pp3pp1/4p3/2pp3p/2P5/8/PPBPPPPP/RKN1BRQN w FAfa - 0 9", {26,
    // 756, 19280, 559186, 14697705, 433719427}},  // 841
    // {"1knrbbqn/rp1p1ppp/p3p3/2p5/8/5P1P/PPPPP1P1/RKNRBBQN w DAd - 0 9", {26,
    // 539, 15194, 345070, 10223443, 248715580}},  // 842
    // {"rknr1qnb/ppp1p1pp/3p2b1/8/4p3/1P3P1P/P1PP2P1/RKNRBQNB w DAda - 0 9",
    // {25, 701, 18969, 561369, 16047041, 496340789}},  // 843
    // {"rbk1r1bn/ppppp1pp/4n3/5p2/1P3P2/4N2P/PqPPP1P1/RBK1RQBN w EAea - 1 9",
    // {2, 60, 1319, 41765, 1017864, 33183408}},  // 844
    // {"r1nbrqbn/k1ppp1pp/1p6/p4p2/2P5/6PQ/PP1PPP1P/RKNBR1BN w EA - 0 9", {27,
    // 699, 20436, 561765, 17192121, 499247248}},  // 845
    // {"rknrqbbn/1pp1pp2/p5p1/3p3p/6P1/PN5P/1PPPPP2/RK1RQBBN w DAda - 0 9",
    // {23, 611, 15515, 435927, 11917036, 352885930}},  // 846
    // {"rknrqn1b/p1pp1ppb/8/1p2p1Qp/3P4/3N4/PPP1PPPP/RK1R1NBB w DAda - 0 9",
    // {45, 1170, 48283, 1320341, 52213677, 1500007485}},  // 847
    // {"bbkrnrnq/p2p1ppp/2p1p3/1p6/1P2Q3/6P1/P1PPPP1P/BBKRNRN1 w - - 0 9", {41,
    // 1035, 39895, 1035610, 38555608, 1037686769}},  // 848
    // {"brkbnr2/1ppppp1p/7n/p5N1/P2q4/8/1PPPPPPP/BRKBNRQ1 w FBfb - 1 9", {22,
    // 869, 19234, 679754, 16453359, 567287944}},  // 849
    // {"brknrbnq/p1ppppp1/1p6/7p/2PP4/5P2/PPK1P1PP/BR1NRBNQ w eb - 1 9", {23,
    // 641, 14748, 422240, 10192718, 302864305}},  // 850
    // {"brk1r1qb/pp1ppnpp/2p2pn1/8/6N1/2N3P1/PPPPPP1P/BRK1R1QB w EBeb - 3 9",
    // {32, 863, 28379, 773191, 25848794, 720443112}},  // 851
    // {"rbbk1rnq/pppp1pp1/4p2p/8/3P2n1/4BN1P/PPP1PPP1/RB1K1RNQ w FAfa - 3 9",
    // {26, 628, 16151, 411995, 11237919, 300314373}},  // 852
    // {"rkbbnr1q/p1pppppp/5n2/1p5B/PP6/4P3/2PP1PPP/RKB1NRNQ w FAfa - 0 9", {30,
    // 692, 21036, 519283, 16025428, 420887328}},  // 853
    // {"rkb1rbnq/1pppp1pp/5p2/p7/5n1P/1PN3P1/P1PPPP2/RKB1RBNQ w EAea - 0 9",
    // {32, 825, 27130, 697251, 23593363, 622249676}},  // 854
    // {"rkbnrnqb/1ppp1p1p/p5p1/4p3/4P3/2N2P2/PPPP2PP/RKBR1NQB w Aea - 0 9",
    // {24, 487, 13300, 301989, 8782713, 215787079}},  // 855
    // {"rbknbr1q/pppp2pp/4p3/5p1n/1P2P2N/8/P1PP1PPP/RBKNBR1Q w FAfa - 0 9",
    // {23, 571, 13799, 365272, 9224232, 257288920}},  // 856
    // {"rknbb1nq/pppppr2/5pp1/7p/8/1N4P1/PPPPPP1P/RK1BBRNQ w FAa - 2 9", {26,
    // 548, 15618, 350173, 10587626, 253006082}},  // 857
    {"rknr1bnq/p2pp1pp/1p3p2/2p4b/6PP/2P2N2/PP1PPP2/RKNRBB1Q w DAda - 1 9",
     {25, 502, 13150, 279098, 7824941, 175766730}},  // 858
    // {"rknrb1qb/ppp1pppp/3p4/8/4P1nP/2P5/PPKP1PP1/R1NRBNQB w da - 1 9", {23,
    // 643, 14849, 426616, 10507328, 312096061}},  // 859
    // {"rbk1rnbq/pppp1npp/4p3/5p2/4P1P1/7P/PPPP1P1N/RBKNR1BQ w EAea - 1 9",
    // {24, 591, 15178, 376988, 10251465, 263574861}},  // 860
    // {"rknbrnb1/p1pppp1p/1p6/3N2p1/P3q1P1/8/1PPPPP1P/RKNBR1BQ w EAea - 1 9",
    // {28, 948, 27343, 864588, 26241141, 812343987}},  // 861
    // {"rknrn1b1/ppppppqp/8/6p1/2P5/2P1BP2/PP2P1PP/RKNRNB1Q w DAda - 1 9", {31,
    // 807, 24360, 672973, 20455205, 588518645}},  // 862
    // {"1k1rnqbb/npppppp1/r7/p2B3p/5P2/1N4P1/PPPPP2P/RK1RNQB1 w DAd - 0 9",
    // {40, 1122, 44297, 1249989, 48711073, 1412437357}},  // 863
    // {"bbqr1rkn/pp1ppppp/8/2p5/1P2P1n1/7N/P1PP1P1P/BBQRKR1N w FD - 0 9", {26,
    // 841, 22986, 746711, 21328001, 705170410}},  // 864
    // {"bqkr1rnn/1ppp1ppp/p4b2/4p3/P7/3PP2N/1PP2PPP/BQRBKR1N w FC - 3 9", {24,
    // 500, 12802, 293824, 7928916, 197806842}},  // 865
    // {"bqrkrbnn/1pp1ppp1/8/p6p/3p4/P3P2P/QPPP1PP1/B1RKRBNN w ECec - 0 9", {31,
    // 592, 18585, 396423, 12607528, 298629240}},  // 866
    // {"bqkrrnnb/2p1pppp/p7/1P1p4/8/2R3P1/PP1PPP1P/BQ1KRNNB w E - 0 9", {42,
    // 1124, 45187, 1276664, 50052573, 1483524894}},  // 867
    // {"qbbrkrn1/p1pppn1p/8/1p3Pp1/2P5/8/PP1PPP1P/QBBRKRNN w FDfd - 0 9", {21,
    // 577, 13244, 392131, 9683808, 300294295}},  // 868
    // {"qrbbkrnn/pp1p2pp/4p3/5p2/2p2P1P/2P5/PP1PP1P1/QRBBKRNN w FBfb - 0 9",
    // {21, 571, 12736, 345681, 8239872, 228837930}},  // 869
    // {"qrbkrbn1/1pp1pppp/p2p4/8/5PPn/2P5/PP1PP3/QRBKRBNN w EBeb - 0 9", {18,
    // 466, 9443, 257776, 5679073, 162883949}},  // 870
    // {"qrb1rnnb/pp1p1ppp/2pk4/4p3/1P2P3/1R6/P1PP1PPP/Q1BKRNNB w E - 4 9", {37,
    // 760, 26863, 562201, 19486022, 421740856}},  // 871
    // {"qbrkbrn1/p1pppp1p/6n1/1p4p1/1P6/5P2/P1PPPBPP/QBRK1RNN w FCfc - 1 9",
    // {33, 824, 27385, 750924, 25176664, 734656217}},  // 872
    // {"qrkbbr2/2pppppp/5nn1/pp1Q4/P7/3P4/1PP1PPPP/1RKBBRNN w FBfb - 0 9", {42,
    // 1147, 44012, 1311247, 48216013, 1522548864}},  // 873
    // {"qrkrbbnn/pp2pp2/2pp2pp/1B6/P7/4P3/1PPP1PPP/QRKRB1NN w DBdb - 0 9", {26,
    // 464, 12653, 242892, 6928220, 142507795}},  // 874
    // {"qrkrbnnb/p1pp1pp1/1p5p/4p3/1P6/6PN/PKPPPP1P/QR1RBN1B w db - 0 9", {29,
    // 705, 20000, 529810, 15055365, 419552571}},  // 875
    // {"qbrkr1bn/p1p1pp1p/1p1p2n1/6p1/3P1P2/4P3/PPP3PP/QBKRRNBN w ec - 2 9",
    // {23, 613, 14835, 426484, 10747407, 323905533}},  // 876
    // {"qrk1rnb1/p1pp1ppp/1p2Bbn1/8/4P3/6P1/PPPP1P1P/QRK1RNBN w EBeb - 1 9",
    // {28, 927, 24887, 846839, 23063284, 807913585}},  // 877
    // {"1qkrnbbn/1rpppppp/pp6/5N2/P4P2/8/1PPPP1PP/QRKRNBB1 w DBd - 3 9", {30,
    // 542, 16646, 345172, 10976745, 251694423}},  // 878
    // {"qrkr2bb/pppppppp/8/1n2n3/1N5P/1P6/P1PPPPP1/QRKR1NBB w DBdb - 1 9", {28,
    // 719, 21048, 562015, 17351761, 479400272}},  // 879
    // {"bbrqkrnn/3ppppp/8/ppp5/6P1/4P2N/PPPPKP1P/BBRQ1R1N w fc - 0 9", {21,
    // 704, 16119, 546215, 13676371, 470796854}},  // 880
    // {"brqbkrnn/1pp2p1p/3pp1p1/p5N1/8/1P6/P1PPPPPP/BRQBK1RN w Bfb - 0 9", {34,
    // 688, 22827, 505618, 16639723, 402140795}},  // 881
    // {"br1krb1n/2qppppp/pp3n2/8/1P4P1/8/P1PPPP1P/1RQKRBNN w EBeb - 0 9", {24,
    // 945, 23943, 926427, 25019636, 959651619}},  // 882
    // {"brqkr1nb/2ppp1pp/1p2np2/p7/2P1PN2/8/PP1P1PPP/BRQKRN1B w EBeb - 0 9",
    // {28, 675, 19728, 504128, 15516491, 417396563}},  // 883
    // {"rbbqkrnn/3pppp1/p7/1pp4p/2P1P2P/8/PP1P1PP1/RBBQKRNN w FAfa - 0 9", {26,
    // 671, 18164, 496806, 14072641, 404960259}},  // 884
    // {"rqbbkr1n/pp1p1p1p/4pn2/2p3p1/4P1P1/3P3P/PPP2P2/RQBBKRNN w FAfa - 0 9",
    // {22, 633, 14629, 441809, 10776416, 335689685}},  // 885
    // {"rqbkrbnn/p1ppp3/1p3pp1/7p/3P4/P1P5/1PQ1PPPP/R1BKRBNN w EAea - 0 9",
    // {32, 607, 20339, 454319, 15586203, 383515709}},  // 886
    // {"rqbkrnn1/pp2ppbp/3p4/2p3p1/2P5/1P3N1P/P2PPPP1/RQBKRN1B w EAea - 1 9",
    // {29, 943, 28732, 908740, 28761841, 907579129}},  // 887
    // {"rbqkb1nn/1ppppr1p/p5p1/5p2/1P6/2P4P/P1KPPPP1/RBQ1BRNN w a - 1 9", {22,
    // 441, 10403, 231273, 5784206, 140934555}},  // 888
    // {"rqkb1rnn/1pp1pp1p/p5p1/1b1p4/3P4/P5P1/RPP1PP1P/1QKBBRNN w Ffa - 1 9",
    // {21, 505, 11592, 290897, 7147063, 188559137}},  // 889
    // {"rq1rbbnn/pkp1ppp1/3p3p/1p2N1P1/8/8/PPPPPP1P/RQKRBB1N w DA - 0 9", {27,
    // 608, 16419, 387751, 10808908, 268393274}},  // 890
    // {"rqkrb2b/p2ppppp/2p3nn/1p6/5P2/PP1P4/2P1P1PP/RQKRBNNB w DAda - 1 9",
    // {30, 749, 21563, 581531, 16916813, 485406712}},  // 891
    // {"rbqkr1bn/pp1ppp2/2p1n2p/6p1/8/4BPNP/PPPPP1P1/RBQKRN2 w EAea - 0 9",
    // {23, 600, 15082, 410057, 11041820, 314327867}},  // 892
    // {"rqkbrnb1/2ppp1pp/pp3pn1/8/5P2/B2P4/PPP1P1PP/RQKBRN1N w EAea - 2 9",
    // {22, 569, 13541, 371471, 9395816, 269460607}},  // 893
    // {"rqkrnbb1/p1p1pppp/1p4n1/3p4/7P/P3P3/1PPPBPP1/RQKRN1BN w DAda - 0 9",
    // {27, 579, 15565, 373079, 10238486, 266047417}},  // 894
    // {"rqkrn1bb/p1ppp1pp/4n3/1p6/6p1/4N3/PPPPPPPP/RQKR2BB w DAda - 0 9", {20,
    // 462, 10234, 274162, 6563859, 193376359}},  // 895
    // {"bbrkqr2/pppp1ppp/6nn/8/2P1p3/3PP2N/PP3PPP/BBRKQR1N w FCfc - 0 9", {28,
    // 724, 21688, 619064, 19318355, 593204629}},  // 896
    // {"brk1qrnn/1pppbppp/4p3/8/1p6/P1P4P/3PPPP1/BRKBQRNN w FBfb - 1 9", {24,
    // 662, 16920, 468215, 12610387, 355969349}},  // 897
    // {"1r1qrbnn/p1pkpppp/1p1p4/8/3P1PP1/P4b2/1PP1P2P/BRKQRBNN w EB - 1 9",
    // {22, 696, 17021, 510247, 13697382, 401903030}},  // 898
    // {"1rkqrnnb/p1p1p1pp/1p1p4/3b1p1N/4P3/5N2/PPPP1PPP/BRKQR2B w EBeb - 1 9",
    // {29, 887, 27035, 816176, 26051242, 791718847}},  // 899
    // {"rbbkq1rn/pppppppp/7n/8/P7/3P3P/1PPKPPP1/RBB1QRNN w a - 3 9", {22, 417,
    // 9900, 216855, 5505063, 134818483}},  // 900
    // {"rkbbqr1n/1p1pppp1/2p2n2/p4NBp/8/3P4/PPP1PPPP/RK1BQRN1 w FAfa - 0 9",
    // {37, 832, 30533, 728154, 26676373, 673756141}},  // 901
    // {"rkbqrb1n/3pBppp/ppp2n2/8/8/P2P4/1PP1PPPP/RK1QRBNN w EAea - 0 9", {28,
    // 685, 19718, 543069, 16033316, 482288814}},  // 902
    // {"rkb1rn1b/ppppqppp/4p3/8/1P2n1P1/5Q2/P1PP1P1P/RKB1RNNB w EAea - 2 9",
    // {37, 1158, 40114, 1234768, 44672979, 1389312729}},  // 903
    // {"r1kqbrnn/pp1pp1p1/7p/2P2p2/5b2/3P4/P1P1P1PP/RBKQBRNN w FAfa - 0 9", {5,
    // 161, 4745, 154885, 4734999, 157499039}},  // 904
    // {"rkqbbr1n/ppp1ppp1/8/Q2p3p/4n3/3P1P2/PPP1P1PP/RK1BBRNN w FAfa - 2 9",
    // {38, 1144, 40433, 1236877, 43832975, 1366087771}},  // 905
    // {"rkqrbbn1/p1ppppp1/Bp5p/8/P6n/2P1P3/1P1P1PPP/RKQRB1NN w DAda - 0 9",
    // {28, 551, 15488, 350861, 9944107, 251179183}},  // 906
    // {"rkqrb1nb/1ppp1ppp/p7/4p3/5n2/3P2N1/PPPQPPPP/RK1RB1NB w DAda - 0 9",
    // {26, 690, 19877, 513628, 15965907, 418191735}},  // 907
    // {"rbkqrnbn/pppp1p2/4p1p1/7p/7P/P2P4/BPP1PPP1/R1KQRNBN w EAea - 0 9", {27,
    // 515, 13992, 309727, 8792550, 218658292}},  // 908
    // {"rkqbrnbn/pp1ppp2/8/2p3p1/P1P4p/5P2/1PKPP1PP/R1QBRNBN w ea - 0 9", {27,
    // 627, 16843, 431101, 11978698, 328434174}},  // 909
    // {"rkqrnbbn/1p2pp1p/3p2p1/p1p5/P5PP/3N4/1PPPPP2/RKQR1BBN w DAda - 0 9",
    // {23, 624, 15512, 451860, 11960861, 367311176}},  // 910
    // {"rk2rnbb/ppqppppp/2pn4/8/1P3P2/6P1/P1PPP1NP/RKQR1NBB w DAa - 1 9", {27,
    // 727, 20206, 581003, 16633696, 505212747}},  // 911
    // {"b1krrqnn/pp1ppp1p/2p3p1/8/P3Pb1P/1P6/2PP1PP1/BBRKRQNN w EC - 0 9", {32,
    // 943, 30759, 865229, 28672582, 800922511}},  // 912
    // {"1rkbrqnn/p1pp1ppp/1p6/8/P2Pp3/8/1PPKPPQP/BR1BR1NN w eb - 0 9", {28,
    // 916, 24892, 817624, 22840279, 759318058}},  // 913
    // {"brkrqb1n/1pppp1pp/p7/3n1p2/P5P1/3PP3/1PP2P1P/BRKRQBNN w DBdb - 0 9",
    // {27, 669, 18682, 484259, 13956472, 380267099}},  // 914
    // {"brkrqnnb/3pppp1/1p6/p1p4p/2P3P1/6N1/PP1PPP1P/BRKRQ1NB w DBdb - 0 9",
    // {29, 699, 20042, 512639, 15093909, 406594531}},  // 915
    // {"r1bkrq1n/pp2pppp/3b1n2/2pp2B1/6P1/3P1P2/PPP1P2P/RB1KRQNN w EAea - 2 9",
    // {27, 835, 22848, 713550, 19867800, 631209313}},  // 916
    // {"rk1brq1n/p1p1pppp/3p1n2/1p3b2/4P3/2NQ4/PPPP1PPP/RKBBR2N w EAea - 4 9",
    // {36, 1004, 35774, 979608, 35143142, 966310885}},  // 917
    // {"rkbrqbnn/1p2ppp1/B1p5/p2p3p/4P2P/8/PPPP1PP1/RKBRQ1NN w DAda - 0 9",
    // {27, 748, 21005, 597819, 17597073, 515304215}},  // 918
    // {"rkbrqn1b/pp1pp1pp/2p2p2/5n2/8/2P2P2/PP1PP1PP/RKBRQ1NB w DAda - 0 9",
    // {20, 479, 10485, 266446, 6253775, 167767913}},  // 919
    // {"rbkrbnn1/ppppp1pp/5q2/5p2/5P2/P3P2N/1PPP2PP/RBKRBQ1N w DAda - 3 9",
    // {28, 947, 26900, 876068, 26007841, 838704143}},  // 920
    // {"rkr1bqnn/1ppp1p1p/p5p1/4p3/3PP2b/2P2P2/PP4PP/RKRBBQNN w CAca - 0 9",
    // {31, 1004, 32006, 1006830, 32688124, 1024529879}},  // 921
    // {"rkrqbbnn/pppp3p/8/4ppp1/1PP4P/8/P2PPPP1/RKRQBBNN w CAca - 0 9", {24,
    // 717, 18834, 564137, 15844525, 484884485}},  // 922
    // {"rkrqbn1b/pppp2pp/8/4pp2/1P1P2n1/5N2/P1P1PP1P/RKRQBN1B w CAca - 0 9",
    // {25, 718, 19654, 587666, 17257753, 537354146}},  // 923
    // {"rbkrqnbn/p1p1ppp1/1p1p4/8/3PP2p/2PB4/PP3PPP/R1KRQNBN w DAda - 0 9",
    // {30, 754, 23298, 611322, 19338246, 532603566}},  // 924
    // {"1krbqnbn/1p2pppp/r1pp4/p7/8/1P1P2PP/P1P1PP2/RKRBQNBN w CAc - 0 9", {21,
    // 566, 13519, 375128, 9700847, 279864836}},  // 925
    // {"rkrq1b2/pppppppb/3n2np/2N5/4P3/7P/PPPP1PP1/RKRQ1BBN w CAca - 1 9", {33,
    // 654, 21708, 479678, 15990307, 382218272}},  // 926
    // {"rkr1nnbb/ppp2p1p/3p1qp1/4p3/P5P1/3PN3/1PP1PP1P/RKRQN1BB w CAca - 1 9",
    // {28, 715, 20361, 555328, 16303092, 468666425}},  // 927
    // {"bbrkrnqn/1p1ppppp/8/8/p2pP3/PP6/2P2PPP/BBRKRNQN w ECec - 0 9", {24,
    // 757, 19067, 603231, 15957628, 509307623}},  // 928
    // {"brkbrnqn/ppp2p2/4p3/P2p2pp/6P1/5P2/1PPPP2P/BRKBRNQN w EBeb - 0 9", {25,
    // 548, 14563, 348259, 9688526, 247750144}},  // 929
    // {"brkr1bqn/1pppppp1/3n3p/1p6/P7/4P1P1/1PPP1P1P/BRKRN1QN w DBdb - 0 9",
    // {19, 359, 7430, 157099, 3521652, 81787718}},  // 930
    // {"brkr1qnb/pppp2pp/2B1p3/5p2/2n5/6PP/PPPPPPN1/BRKR1QN1 w DBdb - 1 9",
    // {27, 854, 23303, 741626, 20558538, 667089231}},  // 931
    // {"rbbkrnqn/p1p1p1pp/8/1p1p4/1P1Pp3/6N1/P1P2PPP/RBBKRNQ1 w EAea - 0 9",
    // {28, 723, 19844, 514440, 14621108, 397454100}},  // 932
    // {"rkbbrn1n/pppppp2/5q1p/6p1/3P3P/4P3/PPP2PP1/RKBBRNQN w EAea - 1 9", {25,
    // 741, 19224, 585198, 15605840, 485037906}},  // 933
    // {"rkbr1bq1/ppnppppp/6n1/2p5/2P1N2P/8/PP1PPPP1/RKBRNBQ1 w DAda - 3 9",
    // {24, 547, 14359, 339497, 9410221, 234041078}},  // 934
    {"1kbrnqnb/r1ppppp1/8/pp5p/8/1P1NP3/P1PP1PPP/RKB1RQNB w Ad - 2 9",
     {26, 618, 17305, 442643, 13112297, 357030697}},  // 935
    // {"rbkrb1qn/1pp1ppp1/3pn2p/pP6/8/4N1P1/P1PPPP1P/RBKRB1QN w DAda - 0 9",
    // {21, 544, 12492, 338832, 8381483, 236013157}},  // 936
    // {"rkrbbnqn/ppppp3/5p2/6pp/5PBP/4P3/PPPP2P1/RKR1BNQN w CAca - 0 9", {30,
    // 891, 25435, 764356, 21894752, 669256602}},  // 937
    // {"rkr1bb1n/ppppp1pp/5p2/4n3/3QP3/5P2/RPPP2PP/1KRNBB1N w Cca - 1 9", {45,
    // 1172, 51766, 1332060, 57856784, 1501852662}},  // 938
    // {"rkr1bqnb/pp1ppppp/8/2pN4/1P6/5N2/P1PPnPPP/RKR1BQ1B w CAca - 0 9", {28,
    // 730, 20511, 559167, 16323242, 463032124}},  // 939
    // {"rbkrnqb1/2ppppp1/p5np/1p6/8/3N4/PPPPPPPP/RBKRQNB1 w DAda - 2 9", {20,
    // 417, 9159, 217390, 5180716, 133936564}},  // 940
    // {"rkrbnqb1/p1pppnpp/5p2/1p6/2P5/1P1P1N2/P3PPPP/RKRB1QBN w CAca - 0 9",
    // {25, 546, 14039, 330316, 8813781, 222026485}},  // 941
    // {"rkr1qbbn/ppppppp1/4n3/7p/8/P7/KPPPPPPP/R1RNQBBN w ca - 0 9", {22, 484,
    // 11458, 267495, 6633319, 163291279}},  // 942
    // {"rkrnqnb1/1ppppp2/p5p1/7p/8/P1bPP3/1PP1QPPP/RKRN1NBB w CAca - 0 9", {22,
    // 636, 15526, 441001, 11614241, 331083405}},  // 943
    // {"b2krn1q/p1rppppp/1Q3n2/2p1b3/1P4P1/8/P1PPPP1P/BBRKRNN1 w ECe - 3 9",
    // {36, 1192, 42945, 1406795, 50382104, 1650202838}},  // 944
    // {"brkbrnn1/pp1pppp1/7q/2p5/6Pp/4P1NP/PPPP1P2/BRKBR1NQ w EBeb - 2 9", {30,
    // 978, 29593, 942398, 29205057, 936568065}},  // 945
    // {"brkrnb1q/pp1p1ppp/2p1p3/5n2/1P6/5N1N/P1PPPPPP/BRKR1B1Q w DBdb - 1 9",
    // {31, 897, 27830, 810187, 25423729, 755334868}},  // 946
    // {"brkr1nqb/pp1p1pp1/2pn3p/P3p3/4P3/6P1/1PPP1P1P/BRKRNNQB w DBdb - 0 9",
    // {19, 382, 8052, 182292, 4232274, 103537333}},  // 947
    // {"r1bkrn1q/ppbppppp/5n2/2p5/3P4/P6N/1PP1PPPP/RBBKRNQ1 w EAea - 3 9", {27,
    // 822, 22551, 678880, 19115128, 578210135}},  // 948
    // {"rkbbrnnq/pp2pppp/8/2pp4/P1P5/1P3P2/3PP1PP/RKBBRNNQ w EAea - 1 9", {23,
    // 643, 15410, 442070, 11170489, 329615708}},  // 949
    // {"rkbr1b1q/p1pppppp/1p1n4/7n/5QP1/3N4/PPPPPP1P/RKBR1BN1 w DAda - 4 9",
    // {37, 943, 34382, 880474, 31568111, 842265141}},  // 950
    // {"rkbr1nqb/pppp2np/8/4ppp1/1P6/6N1/P1PPPPPP/RKBRN1QB w DAda - 1 9", {23,
    // 574, 13260, 362306, 9020291, 261247606}},  // 951
    // {"rbkr1nnq/p1p1pp1p/1p4p1/3p4/b3P3/4N3/PPPPNPPP/RBKRB1Q1 w DAda - 0 9",
    // {26, 900, 23414, 805006, 21653203, 745802405}},  // 952
    // {"rkrbb1nq/p2pppp1/1p4n1/2p4p/3N4/4P1P1/PPPP1P1P/RKRBBN1Q w CAca - 0 9",
    // {32, 697, 22231, 531121, 17150175, 441578567}},  // 953
    // {"rkrnbb1q/pp2pp1p/6pn/2pp4/2B1P2P/8/PPPP1PP1/RKRNB1NQ w CAca - 0 9",
    // {28, 854, 23853, 755990, 21823412, 712787248}},  // 954
    // {"rk2bnqb/pprpppp1/4n2p/2p5/P7/3P2NP/1PP1PPP1/RKRNB1QB w CAa - 1 9", {26,
    // 596, 16251, 414862, 11758184, 323043654}},  // 955
    // {"r1krnnbq/pp1ppp1p/6p1/2p5/2P5/P3P3/Rb1P1PPP/1BKRNNBQ w Dda - 0 9", {2,
    // 61, 1312, 40072, 937188, 28753562}},  // 956
    // {"1krbnnbq/1pp1p1pp/r7/p2p1p2/3PP3/2P3P1/PP3P1P/RKRBNNBQ w CAc - 0 9",
    // {30, 953, 28033, 860530, 25531358, 787205262}},  // 957
    // {"rkr1nbbq/2ppp1pp/1pn5/p4p2/P6P/3P4/1PP1PPPB/RKRNNB1Q w CAca - 1 9",
    // {24, 645, 15689, 446423, 11484012, 341262639}},  // 958
    // {"rkrnnqbb/p1ppp2p/Qp6/4Pp2/5p2/8/PPPP2PP/RKRNN1BB w CAca - 0 9", {35,
    // 929, 32020, 896130, 31272517, 915268405}},  // 959
    // {"bbq1nr1r/pppppk1p/2n2p2/6p1/P4P2/4P1P1/1PPP3P/BBQNNRKR w HF - 1 9",
    // {23, 589, 14744, 387556, 10316716, 280056112}},  // 960
    {"bqrkrbnn/ppp1pppp/8/8/8/8/PPP1PPPP/BQRKRBNN w CKeq - 0 1",
     {19, 342, 6987, 142308, 3294156, 75460468}},  // 961 castling
    {"r1bkrn1q/ppbppppp/5n2/2p5/3P4/P6N/1PP1PPPP/RBBKRNQ1 w KQkq - 3 9",
     {27, 822, 22551, 678880, 19115128, 578210135}},  // 962
    {"qn1rkrbb/pp1p1ppp/2p1p3/3n4/4P2P/2NP4/PPP2PP1/Q1NRKRBB w KQkq - 1 9",
     {24, 585, 14769, 356950, 9482310, 233468620}},  // 963
    {"qnr1bkrb/pppp2pp/3np3/5p2/8/P2P2P1/NPP1PP1P/QN1RBKRB w KQk - 3 9",
     {33, 823, 26895, 713420, 23114629, 646390782}},  // 964
    {"bnrbk1qn/1pppprpp/8/p4p1P/6P1/3P4/PPP1PP2/BNRBKRQN w KQq - 0 9",
     {22, 459, 11447, 268157, 7371098, 190583454}},  // 965
    {"1rbnkbrq/pppppp2/n5pp/2P5/P7/4N3/1P1PPPPP/RNB1KBRQ w KQk - 2 9",
     {23, 574, 14146, 391413, 10203438, 301874034}},  // 966
    {"rbbk1rnq/pppp1pp1/4p2p/8/3P2n1/4BN1P/PPP1PPP1/RB1K1RNQ w KQkq - 3 9",
     {26, 628, 16151, 411995, 11237919, 300314373}},  // 967
};
}  // namespace

TEST(ChessBoard, FRC) {
  // Up to 6 is possible, but too long, so keeping at 4.
  for (int i = 0; i < 4; ++i) {
    for (const auto& x : kChess960Positions) {
      ChessBoard board;
      board.SetFromFen(x.fen);
      EXPECT_EQ(Perft(board, i + 1), x.perft[i])
          << "Position: [" << x.fen << "] Depth: " << i + 1 << "\n"
          << board.DebugString();
    }
  }
}

TEST(ChessBoard, HasMatingMaterialStartPosition) {
  ChessBoard board;
  board.SetFromFen(ChessBoard::kStartposFen);
  EXPECT_TRUE(board.HasMatingMaterial());
}

TEST(ChessBoard, HasMatingMaterialBareKings) {
  ChessBoard board;
  board.SetFromFen("8/8/8/4k3/8/8/2K5/8 w - - 0 1");
  EXPECT_FALSE(board.HasMatingMaterial());
}

TEST(ChessBoard, HasMatingMaterialSingleMinorPiece) {
  ChessBoard board;
  board.SetFromFen("8/8/8/4k3/1N6/8/2K5/8 w - - 0 1");
  EXPECT_FALSE(board.HasMatingMaterial());
  board.SetFromFen("8/8/8/4k3/7b/8/2K5/8 w - - 0 1");
  EXPECT_FALSE(board.HasMatingMaterial());
}

TEST(ChessBoard, HasMatingMaterialSingleMajorPieceOrPawn) {
  ChessBoard board;
  board.SetFromFen("8/8/8/4k3/8/5R2/2K5/8 w - - 0 1");
  EXPECT_TRUE(board.HasMatingMaterial());
  board.SetFromFen("8/8/8/4k3/8/5q2/2K5/8 w - - 0 1");
  EXPECT_TRUE(board.HasMatingMaterial());
  board.SetFromFen("8/8/8/4k3/8/5P2/2K5/8 w - - 0 1");
  EXPECT_TRUE(board.HasMatingMaterial());
}

TEST(ChessBoard, HasMatingMaterialTwoKnights) {
  ChessBoard board;
  board.SetFromFen("8/8/8/3nk3/8/5N2/2K5/8 w - - 0 1");
  EXPECT_TRUE(board.HasMatingMaterial());
  board.SetFromFen("8/8/8/3nk3/8/5n2/2K5/8 w - - 0 1");
  EXPECT_TRUE(board.HasMatingMaterial());
}

TEST(ChessBoard, HasMatingMaterialBishopAndKnight) {
  ChessBoard board;
  board.SetFromFen("8/8/8/3bk3/8/5N2/2K5/8 w - - 0 1");
  EXPECT_TRUE(board.HasMatingMaterial());
  board.SetFromFen("8/8/8/3Bk3/8/5N2/2K5/8 w - - 0 1");
  EXPECT_TRUE(board.HasMatingMaterial());
}

TEST(ChessBoard, HasMatingMaterialMultipleBishopsSameColor) {
  ChessBoard board;
  board.SetFromFen("8/8/8/3Bk3/8/5B2/2K5/8 w - - 0 1");
  EXPECT_FALSE(board.HasMatingMaterial());
  board.SetFromFen("8/8/8/4kb2/8/2K2B2/8/8 w - - 0 1");
  EXPECT_FALSE(board.HasMatingMaterial());
  board.SetFromFen("B7/1B3b2/2B3b1/4k2b/8/8/2K5/8 w - - 0 1");
  EXPECT_FALSE(board.HasMatingMaterial());
  board.SetFromFen("B7/1B6/2B5/4k3/8/8/2K5/8 w - - 0 1");
  EXPECT_FALSE(board.HasMatingMaterial());
}

TEST(ChessBoard, HasMatingMaterialMultipleBishopsNotSameColor) {
  ChessBoard board;
  board.SetFromFen("8/8/8/4k3/8/2K1bb2/8/8 w - - 0 1");
  EXPECT_TRUE(board.HasMatingMaterial());
  board.SetFromFen("8/8/8/4k3/8/2K1Bb2/8/8 w - - 0 1");
  EXPECT_TRUE(board.HasMatingMaterial());
  board.SetFromFen("8/8/8/4k3/8/2K2b2/5B2/8 w - - 0 1");
  EXPECT_TRUE(board.HasMatingMaterial());
  board.SetFromFen("B7/1B3b2/2B3b1/4k2b/7B/8/2K5/8 w - - 0 1");
  EXPECT_TRUE(board.HasMatingMaterial());
}

TEST(ChessBoard, CastlingIsSameMove) {
  ChessBoard board;
  board.SetFromFen(
      "r3k2r/ppp1bppp/2np1n2/4p1B1/4P1b1/2NP1N2/PPP1BPPP/R3K2R w KQkq - 0 1");
  EXPECT_TRUE(board.IsSameMove("e1c1", "e1c1"));
  EXPECT_TRUE(board.IsSameMove("e1a1", "e1a1"));
  EXPECT_TRUE(board.IsSameMove("e1c1", "e1a1"));
  EXPECT_FALSE(board.IsSameMove("e1c1", "e1b1"));
  EXPECT_FALSE(board.IsSameMove("e1b1", "e1a1"));
  EXPECT_FALSE(board.IsSameMove("e1c1", "e1g1"));
  EXPECT_FALSE(board.IsSameMove("e1a1", "e1h1"));
  EXPECT_FALSE(board.IsSameMove("e1c1", "e1h1"));
  EXPECT_FALSE(board.IsSameMove("e1a1", "e1g1"));
  EXPECT_FALSE(board.IsSameMove("e1f1", "e1g1"));
  EXPECT_FALSE(board.IsSameMove("e1f1", "e1h1"));
  EXPECT_TRUE(board.IsSameMove("e2c2", "e2c2"));
  EXPECT_TRUE(board.IsSameMove("e2a2", "e2a2"));
  EXPECT_FALSE(board.IsSameMove("e2c2", "e2a2"));
}

namespace {
void TestInvalid(std::string fen) {
  ChessBoard board;
  try {
    board.SetFromFen(fen);
    FAIL() << "Invalid Fen accepted: " + fen + "\n";
  } catch (...) {
    SUCCEED();
  }
}
}  // namespace


TEST(ChessBoard, InvalidFEN) {
  TestInvalid("rnbqkbnr/ppppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1");
  TestInvalid("rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR/8 w KQkq - 0 1");
  TestInvalid("rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR g KQkq - 0 1");
  TestInvalid("rnbqkbnr/ppp2ppp/4p3/3pP3/8/8/PPPP1PPP/RNBQKBNR w KQkq i6 0 3");
  TestInvalid("rnbqkbnr/ppp2ppp/4p3/3pP3/8/8/PPPP1PPP/RNBQKBNR w KQkq A6 0 3");
}

// Default promotion to knight was leaving an en-passant flag set.
TEST(ChessBoard, InvalidEnPassantFromKnightPromotion) {
  ChessBoard board;
  board.SetFromFen("Q3b3/2P2pnk/3R3p/p7/1pp1p3/PnP1P2P/2B2PP1/5RK1 w - - 1 31");
  board.ApplyMove(Move("c7c8"));
  EXPECT_TRUE(board.en_passant().empty());
}

}  // namespace lczero

int main(int argc, char** argv) {
  ::testing::InitGoogleTest(&argc, argv);
  lczero::InitializeMagicBitboards();
  return RUN_ALL_TESTS();
}

```

`src/chess/callbacks.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <functional>
#include <memory>
#include <optional>
#include <string>
#include <vector>

#include "chess/bitboard.h"
#include "chess/position.h"

namespace lczero {

// Is sent when search decides on the best move.
struct BestMoveInfo {
  BestMoveInfo(Move bestmove, Move ponder = Move{})
      : bestmove(bestmove), ponder(ponder) {}
  Move bestmove;
  Move ponder;
  // Those are extensions and not really UCI protocol.
  // 1 if it's "player1", 2 if it's "player2"
  int player = -1;
  // Index of the game in the tournament (0-based).
  int game_id = -1;
  // The color of the player, if known.
  std::optional<bool> is_black;
};

// Is sent during the search.
struct ThinkingInfo {
  // Full depth.
  int depth = -1;
  // Maximum depth.
  int seldepth = -1;
  // Time since start of thinking.
  int64_t time = -1;
  // Nodes visited.
  int64_t nodes = -1;
  // Nodes per second.
  int nps = -1;
  // Hash fullness * 1000
  int hashfull = -1;
  // Moves to mate.
  std::optional<int> mate;
  // Win in centipawns.
  std::optional<int> score;
  // Win/Draw/Lose probability * 1000.
  struct WDL {
    int w;
    int d;
    int l;
  };
  std::optional<WDL> wdl;
  // Number of successful TB probes (not the same as playouts ending in TB hit).
  int tb_hits = -1;
  // Best line found. Moves are from perspective of white player.
  std::vector<Move> pv;
  // Multipv index.
  int multipv = -1;
  // Freeform comment.
  std::string comment;

  // Those are extensions and not really UCI protocol.
  // 1 if it's "player1", 2 if it's "player2"
  int player = -1;
  // Index of the game in the tournament (0-based).
  int game_id = -1;
  // The color of the player, if known.
  std::optional<bool> is_black;
  // Moves left
  std::optional<int> moves_left;
};

// Is sent when a single game is finished.
struct GameInfo {
  // Game result.
  GameResult game_result = GameResult::UNDECIDED;
  // Name of the file with training data.
  std::string training_filename;
  // Initial fen of the game.
  std::string initial_fen;
  // Game moves.
  std::vector<Move> moves;
  // Ply within moves that the game actually started.
  int play_start_ply;
  // Index of the game in the tournament (0-based).
  int game_id = -1;
  // The color of the player1, if known.
  std::optional<bool> is_black;
  // Minimum resign threshold which would have resulted in a false positive
  // if resign had of been enabled.
  // Only provided if the game wasn't played with resign enabled.
  std::optional<float> min_false_positive_threshold;

  using Callback = std::function<void(const GameInfo&)>;
};

// Is sent in the end of tournament and also during the tournament.
struct TournamentInfo {
  // Did tournament finish, so those results are final.
  bool finished = false;

  // Player1's [win/draw/lose] as [white/black].
  // e.g. results[2][1] is how many times player 1 lost as black.
  int results[3][2] = {{0, 0}, {0, 0}, {0, 0}};
  int move_count_ = 0;
  uint64_t nodes_total_ = 0;

  using Callback = std::function<void(const TournamentInfo&)>;
};

// A class which knows how to output UCI responses.
class UciResponder {
 public:
  virtual ~UciResponder() = default;
  virtual void OutputBestMove(BestMoveInfo* info) = 0;
  virtual void OutputThinkingInfo(std::vector<ThinkingInfo>* infos) = 0;
};

// The responder which calls callbacks. Used for easier transition from old
// code.
class CallbackUciResponder : public UciResponder {
 public:
  using ThinkingCallback =
      std::function<void(const std::vector<ThinkingInfo>&)>;
  using BestMoveCallback = std::function<void(const BestMoveInfo&)>;

  CallbackUciResponder(BestMoveCallback bestmove, ThinkingCallback info)
      : bestmove_callback_(bestmove), info_callback_(info) {}

  void OutputBestMove(BestMoveInfo* info) { bestmove_callback_(*info); }
  void OutputThinkingInfo(std::vector<ThinkingInfo>* infos) {
    info_callback_(*infos);
  }

 private:
  const BestMoveCallback bestmove_callback_;
  const ThinkingCallback info_callback_;
};

// The responnder which doesn't own the parent. Used to transition from old code
// where we need to create a copy.
class NonOwningUciRespondForwarder : public UciResponder {
 public:
  NonOwningUciRespondForwarder(UciResponder* parent) : parent_(parent) {}
  virtual void OutputBestMove(BestMoveInfo* info) {
    parent_->OutputBestMove(info);
  }
  virtual void OutputThinkingInfo(std::vector<ThinkingInfo>* infos) {
    parent_->OutputThinkingInfo(infos);
  }

 private:
  UciResponder* const parent_;
};

// Base class for uci response transformations.
class TransformingUciResponder : public UciResponder {
 public:
  TransformingUciResponder(std::unique_ptr<UciResponder> parent)
      : parent_(std::move(parent)) {}

  virtual void TransformBestMove(BestMoveInfo*) {}
  virtual void TransformThinkingInfo(std::vector<ThinkingInfo>*) {}

 private:
  virtual void OutputBestMove(BestMoveInfo* info) {
    TransformBestMove(info);
    parent_->OutputBestMove(info);
  }
  virtual void OutputThinkingInfo(std::vector<ThinkingInfo>* infos) {
    TransformThinkingInfo(infos);
    parent_->OutputThinkingInfo(infos);
  }
  std::unique_ptr<UciResponder> parent_;
};

class WDLResponseFilter : public TransformingUciResponder {
  using TransformingUciResponder::TransformingUciResponder;
  void TransformThinkingInfo(std::vector<ThinkingInfo>* infos) override {
    for (auto& info : *infos) info.wdl.reset();
  }
};

class MovesLeftResponseFilter : public TransformingUciResponder {
  using TransformingUciResponder::TransformingUciResponder;
  void TransformThinkingInfo(std::vector<ThinkingInfo>* infos) override {
    for (auto& info : *infos) info.moves_left.reset();
  }
};

// Remaps FRC castling to legacy castling.
class Chess960Transformer : public TransformingUciResponder {
 public:
  Chess960Transformer(std::unique_ptr<UciResponder> parent,
                      ChessBoard head_board)
      : TransformingUciResponder(std::move(parent)), head_board_(head_board) {}

 private:
  void TransformBestMove(BestMoveInfo* best_move) override {
    std::vector<Move> moves({best_move->bestmove, best_move->ponder});
    ConvertToLegacyCastling(head_board_, &moves);
    best_move->bestmove = moves[0];
    best_move->ponder = moves[1];
  }
  void TransformThinkingInfo(std::vector<ThinkingInfo>* infos) override {
    for (auto& x : *infos) ConvertToLegacyCastling(head_board_, &x.pv);
  }
  static void ConvertToLegacyCastling(ChessBoard pos,
                                      std::vector<Move>* moves) {
    for (auto& move : *moves) {
      if (pos.flipped()) move.Mirror();
      move = pos.GetLegacyMove(move);
      pos.ApplyMove(move);
      if (pos.flipped()) move.Mirror();
      pos.Mirror();
    }
  }

  const ChessBoard head_board_;
};

}  // namespace lczero

```

`src/chess/pgn.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <zlib.h>

#include <algorithm>
#include <cctype>
#include <cerrno>
#include <fstream>

#include "chess/bitboard.h"
#include "chess/board.h"
#include "utils/exception.h"
#include "utils/logging.h"

namespace lczero {

struct Opening {
  std::string start_fen = ChessBoard::kStartposFen;
  MoveList moves;
};

inline bool GzGetLine(gzFile file, std::string& line) {
  bool flag = false;
  char s[2000];
  line.clear();
  while (gzgets(file, s, sizeof(s))) {
    flag = true;
    line += s;
    auto r = line.find_last_of('\n');
    if (r != std::string::npos) {
      line.erase(r);
      break;
    }
  }
  return flag;
}

class PgnReader {
 public:
  void AddPgnFile(const std::string& filepath) {
    const gzFile file = gzopen(filepath.c_str(), "r");
    if (!file) {
      throw Exception(errno == ENOENT ? "Opening book file not found."
                                      : "Error opening opening book file.");
    }
    std::string line;
    bool in_comment = false;
    bool started = false;
    while (GzGetLine(file, line)) {
      if (!line.empty() && line.back() == '\r') line.pop_back();
      // TODO: support line breaks in tags to ensure they are properly ignored.
      if (line.empty() || line[0] == '[') {
        if (started) {
          Flush();
          started = false;
        }
        auto uc_line = line;
        std::transform(
            uc_line.begin(), uc_line.end(), uc_line.begin(),
            [](unsigned char c) { return std::toupper(c); }  // correct
        );
        if (uc_line.find("[FEN \"", 0) == 0) {
          auto start_trimmed = line.substr(6);
          cur_startpos_ = start_trimmed.substr(0, start_trimmed.find('"'));
          cur_board_.SetFromFen(cur_startpos_);
        }
        continue;
      }
      // Must have at least one non-tag non-empty line in order to be considered
      // a game.
      started = true;
      // Handle braced comments.
      int cur_offset = 0;
      while ((in_comment && line.find('}', cur_offset) != std::string::npos) ||
             (!in_comment && line.find('{', cur_offset) != std::string::npos)) {
        if (in_comment && line.find('}', cur_offset) != std::string::npos) {
          line = line.substr(0, cur_offset) +
                 line.substr(line.find('}', cur_offset) + 1);
          in_comment = false;
        } else {
          cur_offset = line.find('{', cur_offset);
          in_comment = true;
        }
      }
      if (in_comment) {
        line = line.substr(0, cur_offset);
      }
      // Trim trailing comment.
      if (line.find(';') != std::string::npos) {
        line = line.substr(0, line.find(';'));
      }
      if (line.empty()) continue;
      std::istringstream iss(line);
      std::string word;
      while (!iss.eof()) {
        word.clear();
        iss >> word;
        if (word.size() < 2) continue;
        // Trim move numbers from front.
        const auto idx = word.find('.');
        if (idx != std::string::npos) {
          bool all_nums = true;
          for (size_t i = 0; i < idx; i++) {
            if (word[i] < '0' || word[i] > '9') {
              all_nums = false;
              break;
            }
          }
          if (all_nums) {
            word = word.substr(idx + 1);
          }
        }
        // Pure move numbers can be skipped.
        if (word.size() < 2) continue;
        // Ignore score line.
        if (word == "1/2-1/2" || word == "1-0" || word == "0-1" || word == "*")
          continue;
        cur_game_.push_back(SanToMove(word, cur_board_));
        cur_board_.ApplyMove(cur_game_.back());
        // Board ApplyMove wants mirrored for black, but outside code wants
        // normal, so mirror it back again.
        // Check equal to 0 since we've already added the position.
        if ((cur_game_.size() % 2) == 0) {
          cur_game_.back().Mirror();
        }
        cur_board_.Mirror();
      }
    }
    if (started) {
      Flush();
    }
    gzclose(file);
  }
  std::vector<Opening> GetGames() const { return games_; }
  std::vector<Opening>&& ReleaseGames() { return std::move(games_); }

 private:
  void Flush() {
    games_.push_back({cur_startpos_, cur_game_});
    cur_game_.clear();
    cur_board_.SetFromFen(ChessBoard::kStartposFen);
    cur_startpos_ = ChessBoard::kStartposFen;
  }

  Move::Promotion PieceToPromotion(int p) {
    switch (p) {
      case -1:
        return Move::Promotion::None;
      case 2:
        return Move::Promotion::Queen;
      case 3:
        return Move::Promotion::Bishop;
      case 4:
        return Move::Promotion::Knight;
      case 5:
        return Move::Promotion::Rook;
      default:
        // 0 and 1 are pawn and king, which are not legal promotions, other
        // numbers don't correspond to a known piece type.
        CERR << "Unexpected promotion!!";
        throw Exception("Trying to create a move with illegal promotion.");
    }
  }

  Move SanToMove(const std::string& san, const ChessBoard& board) {
    int p = 0;
    size_t idx = 0;
    if (san[0] == 'K') {
      p = 1;
    } else if (san[0] == 'Q') {
      p = 2;
    } else if (san[0] == 'B') {
      p = 3;
    } else if (san[0] == 'N') {
      p = 4;
    } else if (san[0] == 'R') {
      p = 5;
    } else if (san[0] == 'O' && san.size() > 2 && san[1] == '-' &&
               san[2] == 'O') {
      Move m;
      auto king_board = board.kings() & board.ours();
      BoardSquare king_sq(GetLowestBit(king_board.as_int()));
      if (san.size() > 4 && san[3] == '-' && san[4] == 'O') {
        m = Move(BoardSquare(0, king_sq.col()),
                 BoardSquare(0, board.castlings().queenside_rook()));
      } else {
        m = Move(BoardSquare(0, king_sq.col()),
                 BoardSquare(0, board.castlings().kingside_rook()));
      }
      return m;
    }
    if (p != 0) idx++;
    // Formats e4 1e5 de5 d1e5 - with optional x's - followed by =Q for
    // promotions, and even more characters after that also optional.
    int r1 = -1;
    int c1 = -1;
    int r2 = -1;
    int c2 = -1;
    int p2 = -1;
    bool pPending = false;
    for (; idx < san.size(); idx++) {
      if (san[idx] == 'x') continue;
      if (san[idx] == '=') {
        pPending = true;
        continue;
      }
      if (san[idx] >= '1' && san[idx] <= '8') {
        r1 = r2;
        r2 = san[idx] - '1';
        continue;
      }
      if (san[idx] >= 'a' && san[idx] <= 'h') {
        c1 = c2;
        c2 = san[idx] - 'a';
        continue;
      }
      if (pPending) {
        if (san[idx] == 'Q') {
          p2 = 2;
        } else if (san[idx] == 'B') {
          p2 = 3;
        } else if (san[idx] == 'N') {
          p2 = 4;
        } else if (san[idx] == 'R') {
          p2 = 5;
        }
        pPending = false;
        break;
      }
      break;
    }
    if (r1 == -1 || c1 == -1) {
      // Need to find the from cell based on piece.
      int sr1 = r1;
      int sr2 = r2;
      if (board.flipped()) {
        if (sr1 != -1) sr1 = 7 - sr1;
        sr2 = 7 - sr2;
      }
      BitBoard searchBits;
      if (p == 0) {
        searchBits = (board.pawns() & board.ours());
      } else if (p == 1) {
        searchBits = (board.kings() & board.ours());
      } else if (p == 2) {
        searchBits = (board.queens() & board.ours());
      } else if (p == 3) {
        searchBits = (board.bishops() & board.ours());
      } else if (p == 4) {
        searchBits = (board.knights() & board.ours());
      } else if (p == 5) {
        searchBits = (board.rooks() & board.ours());
      }
      auto plm = board.GenerateLegalMoves();
      int pr1 = -1;
      int pc1 = -1;
      for (BoardSquare sq : searchBits) {
        if (sr1 != -1 && sq.row() != sr1) continue;
        if (c1 != -1 && sq.col() != c1) continue;
        if (std::find(plm.begin(), plm.end(),
                      Move(sq, BoardSquare(sr2, c2), PieceToPromotion(p2))) ==
            plm.end()) {
          continue;
        }
        if (pc1 != -1) {
          CERR << "Ambiguous!!";
          throw Exception("Opening book move seems ambiguous.");
        }
        pr1 = sq.row();
        pc1 = sq.col();
      }
      if (pc1 == -1) {
        CERR << "No Match!!";
        throw Exception("Opening book move seems illegal.");
      }
      r1 = pr1;
      c1 = pc1;
      if (board.flipped()) {
        r1 = 7 - r1;
      }
    }
    Move m(BoardSquare(r1, c1), BoardSquare(r2, c2), PieceToPromotion(p2));
    if (board.flipped()) m.Mirror();
    return m;
  }

  ChessBoard cur_board_{ChessBoard::kStartposFen};
  MoveList cur_game_;
  std::string cur_startpos_ = ChessBoard::kStartposFen;
  std::vector<Opening> games_;
};

}  // namespace lczero

```

`src/chess/position.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "chess/position.h"

#include <cassert>
#include <cctype>
#include <cstdlib>
#include <cstring>

namespace {
// GetPieceAt returns the piece found at row, col on board or the null-char '\0'
// in case no piece there.
char GetPieceAt(const lczero::ChessBoard& board, int row, int col) {
  char c = '\0';
  if (board.ours().get(row, col) || board.theirs().get(row, col)) {
    if (board.pawns().get(row, col)) {
      c = 'P';
    } else if (board.kings().get(row, col)) {
      c = 'K';
    } else if (board.bishops().get(row, col)) {
      c = 'B';
    } else if (board.queens().get(row, col)) {
      c = 'Q';
    } else if (board.rooks().get(row, col)) {
      c = 'R';
    } else {
      c = 'N';
    }
    if (board.theirs().get(row, col)) {
      c = std::tolower(c);  // Capitals are for white.
    }
  }
  return c;
}

}  // namespace
namespace lczero {

Position::Position(const Position& parent, Move m)
    : rule50_ply_(parent.rule50_ply_ + 1), ply_count_(parent.ply_count_ + 1) {
  them_board_ = parent.us_board_;
  const bool is_zeroing = them_board_.ApplyMove(m);
  us_board_ = them_board_;
  us_board_.Mirror();
  if (is_zeroing) rule50_ply_ = 0;
}

Position::Position(const ChessBoard& board, int rule50_ply, int game_ply)
    : rule50_ply_(rule50_ply), repetitions_(0), ply_count_(game_ply) {
  us_board_ = board;
  them_board_ = board;
  them_board_.Mirror();
}

uint64_t Position::Hash() const {
  return HashCat({us_board_.Hash(), static_cast<unsigned long>(repetitions_)});
}

std::string Position::DebugString() const { return us_board_.DebugString(); }

GameResult operator-(const GameResult& res) {
  return res == GameResult::BLACK_WON   ? GameResult::WHITE_WON
         : res == GameResult::WHITE_WON ? GameResult::BLACK_WON
                                        : res;
}

GameResult PositionHistory::ComputeGameResult() const {
  const auto& board = Last().GetBoard();
  auto legal_moves = board.GenerateLegalMoves();
  if (legal_moves.empty()) {
    if (board.IsUnderCheck()) {
      // Checkmate.
      return IsBlackToMove() ? GameResult::WHITE_WON : GameResult::BLACK_WON;
    }
    // Stalemate.
    return GameResult::DRAW;
  }

  if (!board.HasMatingMaterial()) return GameResult::DRAW;
  if (Last().GetRule50Ply() >= 100) return GameResult::DRAW;
  if (Last().GetRepetitions() >= 2) return GameResult::DRAW;

  return GameResult::UNDECIDED;
}

void PositionHistory::Reset(const ChessBoard& board, int rule50_ply,
                            int game_ply) {
  positions_.clear();
  positions_.emplace_back(board, rule50_ply, game_ply);
}

void PositionHistory::Append(Move m) {
  // TODO(mooskagh) That should be emplace_back(Last(), m), but MSVS STL
  //                has a bug in implementation of emplace_back, when
  //                reallocation happens. (it also reallocates Last())
  positions_.push_back(Position(Last(), m));
  int cycle_length;
  int repetitions = ComputeLastMoveRepetitions(&cycle_length);
  positions_.back().SetRepetitions(repetitions, cycle_length);
}

int PositionHistory::ComputeLastMoveRepetitions(int* cycle_length) const {
  *cycle_length = 0;
  const auto& last = positions_.back();
  // TODO(crem) implement hash/cache based solution.
  if (last.GetRule50Ply() < 4) return 0;

  for (int idx = positions_.size() - 3; idx >= 0; idx -= 2) {
    const auto& pos = positions_[idx];
    if (pos.GetBoard() == last.GetBoard()) {
      *cycle_length = positions_.size() - 1 - idx;
      return 1 + pos.GetRepetitions();
    }
    if (pos.GetRule50Ply() < 2) return 0;
  }
  return 0;
}

bool PositionHistory::DidRepeatSinceLastZeroingMove() const {
  for (auto iter = positions_.rbegin(), end = positions_.rend(); iter != end;
       ++iter) {
    if (iter->GetRepetitions() > 0) return true;
    if (iter->GetRule50Ply() == 0) return false;
  }
  return false;
}

uint64_t PositionHistory::HashLast(int positions) const {
  uint64_t hash = positions;
  for (auto iter = positions_.rbegin(), end = positions_.rend(); iter != end;
       ++iter) {
    if (!positions--) break;
    hash = HashCat(hash, iter->Hash());
  }
  return HashCat(hash, Last().GetRule50Ply());
}

std::string GetFen(const Position& pos) {
  std::string result;
  const ChessBoard& board = pos.GetWhiteBoard();
  for (int row = 7; row >= 0; --row) {
    int emptycounter = 0;
    for (int col = 0; col < 8; ++col) {
      char piece = GetPieceAt(board, row, col);
      if (emptycounter > 0 && piece) {
        result += std::to_string(emptycounter);
        emptycounter = 0;
      }
      if (piece) {
        result += piece;
      } else {
        emptycounter++;
      }
    }
    if (emptycounter > 0) result += std::to_string(emptycounter);
    if (row > 0) result += "/";
  }
  std::string enpassant = "-";
  if (!board.en_passant().empty()) {
    auto sq = *board.en_passant().begin();
    enpassant = BoardSquare(pos.IsBlackToMove() ? 2 : 5, sq.col()).as_string();
  }
  result += pos.IsBlackToMove() ? " b" : " w";
  result += " " + board.castlings().as_string();
  result += " " + enpassant;
  result += " " + std::to_string(pos.GetRule50Ply());
  result += " " + std::to_string(
                      (pos.GetGamePly() + (pos.IsBlackToMove() ? 1 : 2)) / 2);
  return result;
}
}  // namespace lczero

```

`src/chess/position.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <string>

#include "chess/board.h"

namespace lczero {

class Position {
 public:
  // From parent position and move.
  Position(const Position& parent, Move m);
  // From particular position.
  Position(const ChessBoard& board, int rule50_ply, int game_ply);

  uint64_t Hash() const;
  bool IsBlackToMove() const { return us_board_.flipped(); }

  // Number of half-moves since beginning of the game.
  int GetGamePly() const { return ply_count_; }

  // How many time the same position appeared in the game before.
  int GetRepetitions() const { return repetitions_; }

  // How many half-moves since the same position appeared in the game before.
  int GetPliesSincePrevRepetition() const { return cycle_length_; }

  // Someone outside that class knows better about repetitions, so they can
  // set it.
  void SetRepetitions(int repetitions, int cycle_length) {
    repetitions_ = repetitions;
    cycle_length_ = cycle_length;
  }

  // Number of ply with no captures and pawn moves.
  int GetRule50Ply() const { return rule50_ply_; }

  // Gets board from the point of view of player to move.
  const ChessBoard& GetBoard() const { return us_board_; }
  // Gets board from the point of view of opponent.
  const ChessBoard& GetThemBoard() const { return them_board_; }
  // Gets board from the point of view of the white player.
  const ChessBoard& GetWhiteBoard() const {
    return us_board_.flipped() ? them_board_ : us_board_;
  };

  std::string DebugString() const;

 private:
  // The board from the point of view of the player to move.
  ChessBoard us_board_;
  // The board from the point of view of opponent.
  ChessBoard them_board_;

  // How many half-moves without capture or pawn move was there.
  int rule50_ply_ = 0;
  // How many repetitions this position had before. For new positions it's 0.
  int repetitions_;
  // How many half-moves since the position was repeated or 0.
  int cycle_length_;
  // number of half-moves since beginning of the game.
  int ply_count_ = 0;
};

// GetFen returns a FEN notation for the position.
std::string GetFen(const Position& pos);

// These are ordered so max() prefers the best result.
enum class GameResult : uint8_t { UNDECIDED, BLACK_WON, DRAW, WHITE_WON };
GameResult operator-(const GameResult& res);

class PositionHistory {
 public:
  PositionHistory() = default;
  PositionHistory(const PositionHistory& other) = default;
  PositionHistory(PositionHistory&& other) = default;

  PositionHistory& operator=(const PositionHistory& other) = default;
  PositionHistory& operator=(PositionHistory&& other) = default;  

  // Returns first position of the game (or fen from which it was initialized).
  const Position& Starting() const { return positions_.front(); }

  // Returns the latest position of the game.
  const Position& Last() const { return positions_.back(); }

  // N-th position of the game, 0-based.
  const Position& GetPositionAt(int idx) const { return positions_[idx]; }

  // Trims position to a given size.
  void Trim(int size) {
    positions_.erase(positions_.begin() + size, positions_.end());
  }

  // Can be used to reduce allocation cost while performing a sequence of moves
  // in succession.
  void Reserve(int size) { positions_.reserve(size); }

  // Number of positions in history.
  int GetLength() const { return positions_.size(); }

  // Resets the position to a given state.
  void Reset(const ChessBoard& board, int rule50_ply, int game_ply);

  // Appends a position to history.
  void Append(Move m);

  // Pops last move from history.
  void Pop() { positions_.pop_back(); }

  // Finds the endgame state (win/lose/draw/nothing) for the last position.
  GameResult ComputeGameResult() const;

  // Returns whether next move is history should be black's.
  bool IsBlackToMove() const { return Last().IsBlackToMove(); }

  // Builds a hash from last X positions.
  uint64_t HashLast(int positions) const;

  // Checks for any repetitions since the last time 50 move rule was reset.
  bool DidRepeatSinceLastZeroingMove() const;

 private:
  int ComputeLastMoveRepetitions(int* cycle_length) const;

  std::vector<Position> positions_;
};

}  // namespace lczero

```

`src/chess/position_test.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
*/

#include "chess/position.h"

#include <gtest/gtest.h>

#include <iostream>

#include "utils/string.h"

namespace lczero {

TEST(Position, SetFenGetFen) {
  std::vector<Position> positions;
  ChessBoard board;
  std::vector<std::string> source_fens = {
      "r3k2r/p1ppqpb1/bn2pnp1/3PN3/1p2P3/2N2Q1p/PPPBBPPP/R3K2R w KQkq - 1 1",
      // has en_passant space e3 - black to move
      "r3k2r/p1ppqpb1/bn2pnp1/3PN3/1p2P3/2N2Q1p/PPPBBPPP/R3K2R b KQkq e3 1 1",
      // has en_passant space c6 - white to move
      "rnbqkbnr/pp1ppppp/8/2p5/4P3/8/PPPP1PPP/RNBQKBNR w KQkq c6 0 2",
      "8/2p5/3p4/KP5r/1R3p1k/8/4P1P1/8 w - - 1 1",
      "r2q1rk1/pP1p2pp/Q4n2/bbp1p3/Np6/1B3NBn/pPPP1PPP/R3K2R b KQ - 0 1",
      "3b4/rp1r1k2/8/1RP2p1p/p1KP4/P3P2P/5P2/1R2B3 b - - 2 30",
      "rnbq1k1r/pp1Pbppp/2p5/8/2B5/8/PPP1NnPP/RNBQK2R w KQ - 1 8",
      "r4rk1/1pp1qppp/p1np1n2/2b1p1B1/2B1P1b1/P1NP1N2/1PP1QPPP/R4RK1 w - - 0 "
      "10",
      "8/8/8/4k3/8/8/2K5/8 w - - 0 1", "8/8/8/4k3/1N6/8/2K5/8 w - - 0 1"};
  for (size_t i = 0; i < source_fens.size(); i++) {
    board.Clear();
    PositionHistory history;
    int no_capture_ply;
    int game_move;
    board.SetFromFen(source_fens[i], &no_capture_ply, &game_move);
    history.Reset(board, no_capture_ply,
                  2 * game_move - (board.flipped() ? 1 : 2));
    Position pos = history.Last();
    std::string target_fen = GetFen(pos);
    EXPECT_EQ(source_fens[i], target_fen);
  }
}

// https://github.com/LeelaChessZero/lc0/issues/209
TEST(PositionHistory, ComputeLastMoveRepetitionsWithoutLegalEnPassant) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen("3b4/rp1r1k2/8/1RP2p1p/p1KP4/P3P2P/5P2/1R2B3 b - - 2 30");
  history.Reset(board, 2, 30);
  history.Append(Move("f7f8", true));
  history.Append(Move("f2f4", false));
  history.Append(Move("d7h7", true));
  history.Append(Move("c4d3", false));
  history.Append(Move("h7d7", true));
  history.Append(Move("d3c4", false));
  int history_idx = history.GetLength() - 1;
  const Position& repeated_position = history.GetPositionAt(history_idx);
  EXPECT_EQ(repeated_position.GetRepetitions(), 1);
}

TEST(PositionHistory, ComputeLastMoveRepetitionsWithLegalEnPassant) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen("3b4/rp1r1k2/8/1RP2p1p/p1KP2p1/P3P2P/5P2/1R2B3 b - - 2 30");
  history.Reset(board, 2, 30);
  history.Append(Move("f7f8", true));
  history.Append(Move("f2f4", false));
  history.Append(Move("d7h7", true));
  history.Append(Move("c4d3", false));
  history.Append(Move("h7d7", true));
  history.Append(Move("d3c4", false));
  int history_idx = history.GetLength() - 1;
  const Position& repeated_position = history.GetPositionAt(history_idx);
  EXPECT_EQ(repeated_position.GetRepetitions(), 0);
}

TEST(PositionHistory, DidRepeatSinceLastZeroingMoveCurent) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen("3b4/rp1r1k2/8/1RP2p1p/p1KP4/P3P2P/5P2/1R2B3 b - - 2 30");
  history.Reset(board, 2, 30);
  history.Append(Move("f7f8", true));
  history.Append(Move("f2f4", false));
  history.Append(Move("d7h7", true));
  history.Append(Move("c4d3", false));
  history.Append(Move("h7d7", true));
  history.Append(Move("d3c4", false));
  EXPECT_TRUE(history.DidRepeatSinceLastZeroingMove());
}

TEST(PositionHistory, DidRepeatSinceLastZeroingMoveBefore) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen("3b4/rp1r1k2/8/1RP2p1p/p1KP4/P3P2P/5P2/1R2B3 b - - 2 30");
  history.Reset(board, 2, 30);
  history.Append(Move("f7f8", true));
  history.Append(Move("f2f4", false));
  history.Append(Move("d7h7", true));
  history.Append(Move("c4d3", false));
  history.Append(Move("h7d7", true));
  history.Append(Move("d3c4", false));
  history.Append(Move("d7e7", true));
  EXPECT_TRUE(history.DidRepeatSinceLastZeroingMove());
}

TEST(PositionHistory, DidRepeatSinceLastZeroingMoveOlder) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen("3b4/rp1r1k2/8/1RP2p1p/p1KP4/P3P2P/5P2/1R2B3 b - - 2 30");
  history.Reset(board, 2, 30);
  history.Append(Move("f7f8", true));
  history.Append(Move("f2f4", false));
  history.Append(Move("d7h7", true));
  history.Append(Move("c4d3", false));
  history.Append(Move("h7d7", true));
  history.Append(Move("d3c4", false));
  history.Append(Move("d7e7", true));
  history.Append(Move("c4b4", false));
  EXPECT_TRUE(history.DidRepeatSinceLastZeroingMove());
}

TEST(PositionHistory, DidRepeatSinceLastZeroingMoveBeforeZero) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen("3b4/rp1r1k2/8/1RP2p1p/p1KP4/P3P2P/5P2/1R2B3 b - - 2 30");
  history.Reset(board, 2, 30);
  history.Append(Move("f7f8", true));
  history.Append(Move("f2f4", false));
  history.Append(Move("d7h7", true));
  history.Append(Move("c4d3", false));
  history.Append(Move("h7d7", true));
  history.Append(Move("d3c4", false));
  history.Append(Move("d7e7", true));
  history.Append(Move("c4b4", false));
  history.Append(Move("h5h4", true));
  EXPECT_FALSE(history.DidRepeatSinceLastZeroingMove());
}

TEST(PositionHistory, DidRepeatSinceLastZeroingMoveNeverRepeated) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen("3b4/rp1r1k2/8/1RP2p1p/p1KP4/P3P2P/5P2/1R2B3 b - - 2 30");
  history.Reset(board, 2, 30);
  history.Append(Move("f7f8", true));
  history.Append(Move("f2f4", false));
  EXPECT_FALSE(history.DidRepeatSinceLastZeroingMove());
}

}  // namespace lczero

int main(int argc, char** argv) {
  ::testing::InitGoogleTest(&argc, argv);
  lczero::InitializeMagicBitboards();
  return RUN_ALL_TESTS();
}

```

`src/chess/uciloop.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "uciloop.h"

#include <algorithm>
#include <iomanip>
#include <iostream>
#include <mutex>
#include <sstream>
#include <string>
#include <unordered_map>
#include <unordered_set>
#include <utility>

#include "utils/exception.h"
#include "utils/logging.h"
#include "utils/string.h"
#include "version.h"

namespace lczero {

namespace {
const std::unordered_map<std::string, std::unordered_set<std::string>>
    kKnownCommands = {
        {{"uci"}, {}},
        {{"isready"}, {}},
        {{"setoption"}, {"context", "name", "value"}},
        {{"ucinewgame"}, {}},
        {{"position"}, {"fen", "startpos", "moves"}},
        {{"go"},
         {"infinite", "wtime", "btime", "winc", "binc", "movestogo", "depth",
          "nodes", "movetime", "searchmoves", "ponder"}},
        {{"start"}, {}},
        {{"stop"}, {}},
        {{"ponderhit"}, {}},
        {{"quit"}, {}},
        {{"xyzzy"}, {}},
        {{"fen"}, {}},
};

std::pair<std::string, std::unordered_map<std::string, std::string>>
ParseCommand(const std::string& line) {
  std::unordered_map<std::string, std::string> params;
  std::string* value = nullptr;

  std::istringstream iss(line);
  std::string token;
  iss >> token >> std::ws;

  // If empty line, return empty command.
  if (token.empty()) return {};

  const auto command = kKnownCommands.find(token);
  if (command == kKnownCommands.end()) {
    throw Exception("Unknown command: " + line);
  }

  std::string whitespace;
  while (iss >> token) {
    auto iter = command->second.find(token);
    if (iter == command->second.end()) {
      if (!value) throw Exception("Unexpected token: " + token);
      *value += whitespace + token;
      whitespace = " ";
    } else {
      value = &params[token];
      iss >> std::ws;
      whitespace = "";
    }
  }
  return {command->first, params};
}

std::string GetOrEmpty(
    const std::unordered_map<std::string, std::string>& params,
    const std::string& key) {
  const auto iter = params.find(key);
  if (iter == params.end()) return {};
  return iter->second;
}

int GetNumeric(const std::unordered_map<std::string, std::string>& params,
               const std::string& key) {
  const auto iter = params.find(key);
  if (iter == params.end()) {
    throw Exception("Unexpected error");
  }
  const std::string& str = iter->second;
  try {
    if (str.empty()) {
      throw Exception("expected value after " + key);
    }
    return std::stoi(str);
  } catch (std::invalid_argument&) {
    throw Exception("invalid value " + str);
  } catch (const std::out_of_range&) {
    throw Exception("out of range value " + str);
  }
}

bool ContainsKey(const std::unordered_map<std::string, std::string>& params,
                 const std::string& key) {
  return params.find(key) != params.end();
}
}  // namespace

void UciLoop::RunLoop() {
  std::cout.setf(std::ios::unitbuf);
  std::string line;
  while (std::getline(std::cin, line)) {
    LOGFILE << ">> " << line;
    try {
      auto command = ParseCommand(line);
      // Ignore empty line.
      if (command.first.empty()) continue;
      if (!DispatchCommand(command.first, command.second)) break;
    } catch (Exception& ex) {
      SendResponse(std::string("error ") + ex.what());
    }
  }
}

bool UciLoop::DispatchCommand(
    const std::string& command,
    const std::unordered_map<std::string, std::string>& params) {
  if (command == "uci") {
    CmdUci();
  } else if (command == "isready") {
    CmdIsReady();
  } else if (command == "setoption") {
    CmdSetOption(GetOrEmpty(params, "name"), GetOrEmpty(params, "value"),
                 GetOrEmpty(params, "context"));
  } else if (command == "ucinewgame") {
    CmdUciNewGame();
  } else if (command == "position") {
    if (ContainsKey(params, "fen") == ContainsKey(params, "startpos")) {
      throw Exception("Position requires either fen or startpos");
    }
    const std::vector<std::string> moves =
        StrSplitAtWhitespace(GetOrEmpty(params, "moves"));
    CmdPosition(GetOrEmpty(params, "fen"), moves);
  } else if (command == "go") {
    GoParams go_params;
    if (ContainsKey(params, "infinite")) {
      if (!GetOrEmpty(params, "infinite").empty()) {
        throw Exception("Unexpected token " + GetOrEmpty(params, "infinite"));
      }
      go_params.infinite = true;
    }
    if (ContainsKey(params, "searchmoves")) {
      go_params.searchmoves =
          StrSplitAtWhitespace(GetOrEmpty(params, "searchmoves"));
    }
    if (ContainsKey(params, "ponder")) {
      if (!GetOrEmpty(params, "ponder").empty()) {
        throw Exception("Unexpected token " + GetOrEmpty(params, "ponder"));
      }
      go_params.ponder = true;
    }
#define UCIGOOPTION(x)                    \
  if (ContainsKey(params, #x)) {          \
    go_params.x = GetNumeric(params, #x); \
  }
    UCIGOOPTION(wtime);
    UCIGOOPTION(btime);
    UCIGOOPTION(winc);
    UCIGOOPTION(binc);
    UCIGOOPTION(movestogo);
    UCIGOOPTION(depth);
    UCIGOOPTION(nodes);
    UCIGOOPTION(movetime);
#undef UCIGOOPTION
    CmdGo(go_params);
  } else if (command == "stop") {
    CmdStop();
  } else if (command == "ponderhit") {
    CmdPonderHit();
  } else if (command == "start") {
    CmdStart();
  } else if (command == "fen") {
    CmdFen();
  } else if (command == "xyzzy") {
    SendResponse("Nothing happens.");
  } else if (command == "quit") {
    return false;
  } else {
    throw Exception("Unknown command: " + command);
  }
  return true;
}

void UciLoop::SendResponse(const std::string& response) {
  SendResponses({response});
}

void UciLoop::SendResponses(const std::vector<std::string>& responses) {
  static std::mutex output_mutex;
  std::lock_guard<std::mutex> lock(output_mutex);
  for (auto& response : responses) {
    LOGFILE << "<< " << response;
    std::cout << response << std::endl;
  }
}

void UciLoop::SendId() {
  SendResponse("id name Lc0 v" + GetVersionStr());
  SendResponse("id author The LCZero Authors.");
}

void UciLoop::SendBestMove(const BestMoveInfo& move) {
  std::string res = "bestmove " + move.bestmove.as_string();
  if (move.ponder) res += " ponder " + move.ponder.as_string();
  if (move.player != -1) res += " player " + std::to_string(move.player);
  if (move.game_id != -1) res += " gameid " + std::to_string(move.game_id);
  if (move.is_black)
    res += " side " + std::string(*move.is_black ? "black" : "white");
  SendResponse(res);
}

void UciLoop::SendInfo(const std::vector<ThinkingInfo>& infos) {
  std::vector<std::string> reses;
  for (const auto& info : infos) {
    std::string res = "info";
    if (info.player != -1) res += " player " + std::to_string(info.player);
    if (info.game_id != -1) res += " gameid " + std::to_string(info.game_id);
    if (info.is_black)
      res += " side " + std::string(*info.is_black ? "black" : "white");
    if (info.depth >= 0)
      res += " depth " + std::to_string(std::max(info.depth, 1));
    if (info.seldepth >= 0) res += " seldepth " + std::to_string(info.seldepth);
    if (info.time >= 0) res += " time " + std::to_string(info.time);
    if (info.nodes >= 0) res += " nodes " + std::to_string(info.nodes);
    if (info.mate) res += " score mate " + std::to_string(*info.mate);
    if (info.score) res += " score cp " + std::to_string(*info.score);
    if (info.wdl) {
      res += " wdl " + std::to_string(info.wdl->w) + " " +
             std::to_string(info.wdl->d) + " " + std::to_string(info.wdl->l);
    }
    if (info.moves_left) {
      res += " movesleft " + std::to_string(*info.moves_left);
    }
    if (info.hashfull >= 0) res += " hashfull " + std::to_string(info.hashfull);
    if (info.nps >= 0) res += " nps " + std::to_string(info.nps);
    if (info.tb_hits >= 0) res += " tbhits " + std::to_string(info.tb_hits);
    if (info.multipv >= 0) res += " multipv " + std::to_string(info.multipv);

    if (!info.pv.empty()) {
      res += " pv";
      for (const auto& move : info.pv) res += " " + move.as_string();
    }
    if (!info.comment.empty()) res += " string " + info.comment;
    reses.push_back(std::move(res));
  }
  SendResponses(reses);
}

}  // namespace lczero

```

`src/chess/uciloop.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <fstream>
#include <optional>
#include <string>
#include <unordered_map>
#include <vector>

#include "chess/callbacks.h"
#include "utils/exception.h"

namespace lczero {

struct GoParams {
  std::optional<std::int64_t> wtime;
  std::optional<std::int64_t> btime;
  std::optional<std::int64_t> winc;
  std::optional<std::int64_t> binc;
  std::optional<int> movestogo;
  std::optional<int> depth;
  std::optional<int> nodes;
  std::optional<std::int64_t> movetime;
  bool infinite = false;
  std::vector<std::string> searchmoves;
  bool ponder = false;
};

class UciLoop {
 public:
  virtual ~UciLoop() {}
  virtual void RunLoop();

  // Sends response to host.
  void SendResponse(const std::string& response);
  // Sends responses to host ensuring they are received as a block.
  virtual void SendResponses(const std::vector<std::string>& responses);
  void SendBestMove(const BestMoveInfo& move);
  void SendInfo(const std::vector<ThinkingInfo>& infos);
  void SendId();

  // Command handlers.
  virtual void CmdUci() { throw Exception("Not supported"); }
  virtual void CmdIsReady() { throw Exception("Not supported"); }
  virtual void CmdSetOption(const std::string& /*name*/,
                            const std::string& /*value*/,
                            const std::string& /*context*/) {
    throw Exception("Not supported");
  }
  virtual void CmdUciNewGame() { throw Exception("Not supported"); }
  virtual void CmdPosition(const std::string& /*position*/,
                           const std::vector<std::string>& /*moves*/) {
    throw Exception("Not supported");
  }
  virtual void CmdFen() { throw Exception("Not supported"); }
  virtual void CmdGo(const GoParams& /*params*/) {
    throw Exception("Not supported");
  }
  virtual void CmdStop() { throw Exception("Not supported"); }
  virtual void CmdPonderHit() { throw Exception("Not supported"); }
  virtual void CmdStart() { throw Exception("Not supported"); }

 private:
  bool DispatchCommand(
      const std::string& command,
      const std::unordered_map<std::string, std::string>& params);
};

}  // namespace lczero

```

`src/engine.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "engine.h"

#include <algorithm>
#include <cmath>
#include <functional>

#include "mcts/search.h"
#include "mcts/stoppers/factory.h"
#include "utils/configfile.h"
#include "utils/logging.h"

namespace lczero {
namespace {
const int kDefaultThreads = 2;

const OptionId kThreadsOptionId{"threads", "Threads",
                                "Number of (CPU) worker threads to use.", 't'};
const OptionId kLogFileId{"logfile", "LogFile",
                          "Write log to that file. Special value <stderr> to "
                          "output the log to the console.",
                          'l'};
const OptionId kSyzygyTablebaseId{
    "syzygy-paths", "SyzygyPath",
    "List of Syzygy tablebase directories, list entries separated by system "
    "separator (\";\" for Windows, \":\" for Linux).",
    's'};
const OptionId kPonderId{"ponder", "Ponder",
                         "This option is ignored. Here to please chess GUIs."};
const OptionId kUciChess960{
    "chess960", "UCI_Chess960",
    "Castling moves are encoded as \"king takes rook\"."};
const OptionId kShowWDL{"show-wdl", "UCI_ShowWDL",
                        "Show win, draw and lose probability."};
const OptionId kShowMovesleft{"show-movesleft", "UCI_ShowMovesLeft",
                              "Show estimated moves left."};
const OptionId kStrictUciTiming{"strict-uci-timing", "StrictTiming",
                                "The UCI host compensates for lag, waits for "
                                "the 'readyok' reply before sending 'go' and "
                                "only then starts timing."};
const OptionId kPreload{"preload", "",
                        "Initialize backend and load net on engine startup."};

MoveList StringsToMovelist(const std::vector<std::string>& moves,
                           const ChessBoard& board) {
  MoveList result;
  if (moves.size()) {
    result.reserve(moves.size());
    const auto legal_moves = board.GenerateLegalMoves();
    const auto end = legal_moves.end();
    for (const auto& move : moves) {
      const auto m = board.GetModernMove({move, board.flipped()});
      if (std::find(legal_moves.begin(), end, m) != end) result.emplace_back(m);
    }
    if (result.empty()) throw Exception("No legal searchmoves.");
  }
  return result;
}

}  // namespace

EngineController::EngineController(std::unique_ptr<UciResponder> uci_responder,
                                   const OptionsDict& options)
    : options_(options),
      uci_responder_(std::move(uci_responder)),
      current_position_{ChessBoard::kStartposFen, {}} {}

void EngineController::PopulateOptions(OptionsParser* options) {
  using namespace std::placeholders;

  NetworkFactory::PopulateOptions(options);
  options->Add<IntOption>(kThreadsOptionId, 1, 128) = kDefaultThreads;
  options->Add<IntOption>(kNNCacheSizeId, 0, 999999999) = 2000000;
  SearchParams::Populate(options);

  options->Add<StringOption>(kSyzygyTablebaseId);
  // Add "Ponder" option to signal to GUIs that we support pondering.
  // This option is currently not used by lc0 in any way.
  options->Add<BoolOption>(kPonderId) = true;
  options->Add<BoolOption>(kUciChess960) = false;
  options->Add<BoolOption>(kShowWDL) = false;
  options->Add<BoolOption>(kShowMovesleft) = false;

  ConfigFile::PopulateOptions(options);
  PopulateTimeManagementOptions(RunType::kUci, options);

  options->Add<BoolOption>(kStrictUciTiming) = false;
  options->HideOption(kStrictUciTiming);

  options->Add<BoolOption>(kPreload) = false;
}

void EngineController::ResetMoveTimer() {
  move_start_time_ = std::chrono::steady_clock::now();
}

// Updates values from Uci options.
void EngineController::UpdateFromUciOptions() {
  SharedLock lock(busy_mutex_);

  // Syzygy tablebases.
  std::string tb_paths = options_.Get<std::string>(kSyzygyTablebaseId);
  if (!tb_paths.empty() && tb_paths != tb_paths_) {
    syzygy_tb_ = std::make_unique<SyzygyTablebase>();
    CERR << "Loading Syzygy tablebases from " << tb_paths;
    if (!syzygy_tb_->init(tb_paths)) {
      CERR << "Failed to load Syzygy tablebases!";
      syzygy_tb_ = nullptr;
    } else {
      tb_paths_ = tb_paths;
    }
  }

  // Network.
  const auto network_configuration =
      NetworkFactory::BackendConfiguration(options_);
  if (network_configuration_ != network_configuration) {
    network_ = NetworkFactory::LoadNetwork(options_);
    network_configuration_ = network_configuration;
  }

  // Cache size.
  cache_.SetCapacity(options_.Get<int>(kNNCacheSizeId));

  // Check whether we can update the move timer in "Go".
  strict_uci_timing_ = options_.Get<bool>(kStrictUciTiming);
}

void EngineController::EnsureReady() {
  std::unique_lock<RpSharedMutex> lock(busy_mutex_);
  // If a UCI host is waiting for our ready response, we can consider the move
  // not started until we're done ensuring ready.
  ResetMoveTimer();
}

void EngineController::NewGame() {
  // In case anything relies upon defaulting to default position and just calls
  // newgame and goes straight into go.
  ResetMoveTimer();
  SharedLock lock(busy_mutex_);
  cache_.Clear();
  search_.reset();
  tree_.reset();
  CreateFreshTimeManager();
  current_position_ = {ChessBoard::kStartposFen, {}};
  UpdateFromUciOptions();
}

void EngineController::SetPosition(const std::string& fen,
                                   const std::vector<std::string>& moves_str) {
  // Some UCI hosts just call position then immediately call go, while starting
  // the clock on calling 'position'.
  ResetMoveTimer();
  SharedLock lock(busy_mutex_);
  current_position_ = CurrentPosition{fen, moves_str};
  search_.reset();
}

Position EngineController::ApplyPositionMoves() {
  ChessBoard board;
  int no_capture_ply;
  int game_move;
  board.SetFromFen(current_position_.fen, &no_capture_ply, &game_move);
  int game_ply = 2 * game_move - (board.flipped() ? 1 : 2);
  Position pos(board, no_capture_ply, game_ply);
  for (std::string move_str: current_position_.moves) {
    Move move(move_str);
    if (pos.IsBlackToMove()) move.Mirror();
    pos = Position(pos, move);
  }
  return pos;
}

void EngineController::SetupPosition(
    const std::string& fen, const std::vector<std::string>& moves_str) {
  SharedLock lock(busy_mutex_);
  search_.reset();

  UpdateFromUciOptions();

  if (!tree_) tree_ = std::make_unique<NodeTree>();

  std::vector<Move> moves;
  for (const auto& move : moves_str) moves.emplace_back(move);
  const bool is_same_game = tree_->ResetToPosition(fen, moves);
  if (!is_same_game) CreateFreshTimeManager();
}

void EngineController::CreateFreshTimeManager() {
  time_manager_ = MakeTimeManager(options_);
}

namespace {

class PonderResponseTransformer : public TransformingUciResponder {
 public:
  PonderResponseTransformer(std::unique_ptr<UciResponder> parent,
                            std::string ponder_move)
      : TransformingUciResponder(std::move(parent)),
        ponder_move_(std::move(ponder_move)) {}

  void TransformThinkingInfo(std::vector<ThinkingInfo>* infos) override {
    // Output all stats from main variation (not necessary the ponder move)
    // but PV only from ponder move.
    ThinkingInfo ponder_info;
    for (const auto& info : *infos) {
      if (info.multipv <= 1) {
        ponder_info = info;
        if (ponder_info.mate) ponder_info.mate = -*ponder_info.mate;
        if (ponder_info.score) ponder_info.score = -*ponder_info.score;
        if (ponder_info.depth > 1) ponder_info.depth--;
        if (ponder_info.seldepth > 1) ponder_info.seldepth--;
        ponder_info.pv.clear();
      }
      if (!info.pv.empty() && info.pv[0].as_string() == ponder_move_) {
        ponder_info.pv.assign(info.pv.begin() + 1, info.pv.end());
      }
    }
    infos->clear();
    infos->push_back(ponder_info);
  }

 private:
  std::string ponder_move_;
};

}  // namespace

void EngineController::Go(const GoParams& params) {
  // TODO: should consecutive calls to go be considered to be a continuation and
  // hence have the same start time like this behaves, or should we check start
  // time hasn't changed since last call to go and capture the new start time
  // now?
  if (strict_uci_timing_ || !move_start_time_) ResetMoveTimer();
  go_params_ = params;

  std::unique_ptr<UciResponder> responder =
      std::make_unique<NonOwningUciRespondForwarder>(uci_responder_.get());

  // Setting up current position, now that it's known whether it's ponder or
  // not.
  if (params.ponder && !current_position_.moves.empty()) {
    std::vector<std::string> moves(current_position_.moves);
    std::string ponder_move = moves.back();
    moves.pop_back();
    SetupPosition(current_position_.fen, moves);
    responder = std::make_unique<PonderResponseTransformer>(
        std::move(responder), ponder_move);
  } else {
    SetupPosition(current_position_.fen, current_position_.moves);
  }

  if (!options_.Get<bool>(kUciChess960)) {
    // Remap FRC castling to legacy castling.
    responder = std::make_unique<Chess960Transformer>(
        std::move(responder), tree_->HeadPosition().GetBoard());
  }

  if (!options_.Get<bool>(kShowWDL)) {
    // Strip WDL information from the response.
    responder = std::make_unique<WDLResponseFilter>(std::move(responder));
  }

  if (!options_.Get<bool>(kShowMovesleft)) {
    // Strip movesleft information from the response.
    responder = std::make_unique<MovesLeftResponseFilter>(std::move(responder));
  }

  auto stopper = time_manager_->GetStopper(params, *tree_.get());
  search_ = std::make_unique<Search>(
      *tree_, network_.get(), std::move(responder),
      StringsToMovelist(params.searchmoves, tree_->HeadPosition().GetBoard()),
      *move_start_time_, std::move(stopper), params.infinite || params.ponder,
      options_, &cache_, syzygy_tb_.get());

  LOGFILE << "Timer started at "
          << FormatTime(SteadyClockToSystemClock(*move_start_time_));
  search_->StartThreads(options_.Get<int>(kThreadsOptionId));
}

void EngineController::PonderHit() {
  ResetMoveTimer();
  go_params_.ponder = false;
  Go(go_params_);
}

void EngineController::Stop() {
  if (search_) search_->Stop();
}

EngineLoop::EngineLoop()
    : engine_(
          std::make_unique<CallbackUciResponder>(
              std::bind(&UciLoop::SendBestMove, this, std::placeholders::_1),
              std::bind(&UciLoop::SendInfo, this, std::placeholders::_1)),
          options_.GetOptionsDict()) {
  engine_.PopulateOptions(&options_);
  options_.Add<StringOption>(kLogFileId);
}

void EngineLoop::RunLoop() {
  if (!ConfigFile::Init() || !options_.ProcessAllFlags()) return;
  const auto options = options_.GetOptionsDict();
  Logging::Get().SetFilename(options.Get<std::string>(kLogFileId));
  if (options.Get<bool>(kPreload)) engine_.NewGame();
  UciLoop::RunLoop();
}

void EngineLoop::CmdUci() {
  SendId();
  for (const auto& option : options_.ListOptionsUci()) {
    SendResponse(option);
  }
  SendResponse("uciok");
}

void EngineLoop::CmdIsReady() {
  engine_.EnsureReady();
  SendResponse("readyok");
}

void EngineLoop::CmdSetOption(const std::string& name, const std::string& value,
                              const std::string& context) {
  options_.SetUciOption(name, value, context);
  // Set the log filename for the case it was set in UCI option.
  Logging::Get().SetFilename(
      options_.GetOptionsDict().Get<std::string>(kLogFileId));
}

void EngineLoop::CmdUciNewGame() { engine_.NewGame(); }

void EngineLoop::CmdPosition(const std::string& position,
                             const std::vector<std::string>& moves) {
  std::string fen = position;
  if (fen.empty()) {
    fen = ChessBoard::kStartposFen;
  }
  engine_.SetPosition(fen, moves);
}

void EngineLoop::CmdFen() {
  std::string fen = GetFen(engine_.ApplyPositionMoves());
  return SendResponse(fen);
}
void EngineLoop::CmdGo(const GoParams& params) { engine_.Go(params); }

void EngineLoop::CmdPonderHit() { engine_.PonderHit(); }

void EngineLoop::CmdStop() { engine_.Stop(); }

}  // namespace lczero

```

`src/engine.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <optional>

#include "chess/uciloop.h"
#include "mcts/search.h"
#include "neural/cache.h"
#include "neural/factory.h"
#include "neural/network.h"
#include "syzygy/syzygy.h"
#include "utils/mutex.h"
#include "utils/optionsparser.h"

namespace lczero {

struct CurrentPosition {
  std::string fen;
  std::vector<std::string> moves;
};

class EngineController {
 public:
  EngineController(std::unique_ptr<UciResponder> uci_responder,
                   const OptionsDict& options);

  ~EngineController() {
    // Make sure search is destructed first, and it still may be running in
    // a separate thread.
    search_.reset();
  }

  void PopulateOptions(OptionsParser* options);

  // Blocks.
  void EnsureReady();

  // Must not block.
  void NewGame();

  // Blocks.
  void SetPosition(const std::string& fen,
                   const std::vector<std::string>& moves);

  // Must not block.
  void Go(const GoParams& params);
  void PonderHit();
  // Must not block.
  void Stop();

  Position ApplyPositionMoves();

 private:
  void UpdateFromUciOptions();

  void SetupPosition(const std::string& fen,
                     const std::vector<std::string>& moves);
  void ResetMoveTimer();
  void CreateFreshTimeManager();

  const OptionsDict& options_;

  std::unique_ptr<UciResponder> uci_responder_;

  // Locked means that there is some work to wait before responding readyok.
  RpSharedMutex busy_mutex_;
  using SharedLock = std::shared_lock<RpSharedMutex>;

  std::unique_ptr<TimeManager> time_manager_;
  std::unique_ptr<Search> search_;
  std::unique_ptr<NodeTree> tree_;
  std::unique_ptr<SyzygyTablebase> syzygy_tb_;
  std::unique_ptr<Network> network_;
  NNCache cache_;

  // Store current TB and network settings to track when they change so that
  // they are reloaded.
  std::string tb_paths_;
  NetworkFactory::BackendConfiguration network_configuration_;

  // The current position as given with SetPosition. For normal (ie. non-ponder)
  // search, the tree is set up with this position, however, during ponder we
  // actually search the position one move earlier.
  CurrentPosition current_position_;
  GoParams go_params_;

  std::optional<std::chrono::steady_clock::time_point> move_start_time_;

  // If true we can reset move_start_time_ in "Go".
  bool strict_uci_timing_;
};

class EngineLoop : public UciLoop {
 public:
  EngineLoop();

  void RunLoop() override;
  void CmdUci() override;
  void CmdIsReady() override;
  void CmdSetOption(const std::string& name, const std::string& value,
                    const std::string& context) override;
  void CmdUciNewGame() override;
  void CmdPosition(const std::string& position,
                   const std::vector<std::string>& moves) override;
  void CmdFen() override;
  void CmdGo(const GoParams& params) override;
  void CmdPonderHit() override;
  void CmdStop() override;

 private:
  OptionsParser options_;
  EngineController engine_;
};

}  // namespace lczero

```

`src/lc0ctl/describenet.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "lc0ctl/describenet.h"

#include "neural/loader.h"
#include "neural/onnx/onnx.pb.h"
#include "utils/optionsparser.h"

namespace lczero {
namespace {

const OptionId kWeightsFilenameId{"weights", "WeightsFile",
                                  "Path of the input Lc0 weights file.", 'w'};

bool ProcessParameters(OptionsParser* options) {
  options->Add<StringOption>(kWeightsFilenameId);
  if (!options->ProcessAllFlags()) return false;
  const OptionsDict& dict = options->GetOptionsDict();
  dict.EnsureExists<std::string>(kWeightsFilenameId);

  return true;
}

std::string Justify(std::string str, size_t length = 30) {
  if (str.size() + 2 < length) {
    str = std::string(length - 2 - str.size(), ' ') + str;
  }
  str += ": ";
  return str;
}

}  // namespace

void ShowNetworkGenericInfo(const pblczero::Net& weights) {
  const auto& version = weights.min_version();
  COUT << "\nGeneral";
  COUT << "~~~~~~~";
  COUT << Justify("Minimal Lc0 version") << "v" << version.major() << '.'
       << version.minor() << '.' << version.patch();
}

void ShowNetworkFormatInfo(const pblczero::Net& weights) {
  const auto& format = weights.format();
  const auto& net_format = format.network_format();

  using pblczero::Format;
  using pblczero::NetworkFormat;
  COUT << "\nFormat";
  COUT << "~~~~~~";
  if (format.has_weights_encoding()) {
    COUT << Justify("Weights encoding")
         << Format::Encoding_Name(format.weights_encoding());
  }
  if (net_format.has_input()) {
    COUT << Justify("Input")
         << NetworkFormat::InputFormat_Name(net_format.input());
  }
  if (net_format.has_network()) {
    COUT << Justify("Network")
         << NetworkFormat::NetworkStructure_Name(net_format.network());
  }
  if (net_format.has_policy()) {
    COUT << Justify("Policy")
         << NetworkFormat::PolicyFormat_Name(net_format.policy());
  }
  if (net_format.has_value()) {
    COUT << Justify("Value")
         << NetworkFormat::ValueFormat_Name(net_format.value());
  }
  if (net_format.has_moves_left()) {
    COUT << Justify("MLH")
         << NetworkFormat::MovesLeftFormat_Name(net_format.moves_left());
  }
}

void ShowNetworkTrainingInfo(const pblczero::Net& weights) {
  if (!weights.has_training_params()) return;
  COUT << "\nTraining Parameters";
  COUT << "~~~~~~~~~~~~~~~~~~~";
  using pblczero::TrainingParams;
  const auto& params = weights.training_params();
  if (params.has_training_steps()) {
    COUT << Justify("Training steps") << params.training_steps();
  }
  if (params.has_learning_rate()) {
    COUT << Justify("Learning rate") << params.learning_rate();
  }
  if (params.has_mse_loss()) {
    COUT << Justify("MSE loss") << params.mse_loss();
  }
  if (params.has_policy_loss()) {
    COUT << Justify("Policy loss") << params.policy_loss();
  }
  if (params.has_accuracy()) {
    COUT << Justify("Accuracy") << params.accuracy();
  }
  if (params.has_lc0_params()) {
    COUT << Justify("Lc0 Params") << params.lc0_params();
  }
}

void ShowNetworkWeightsInfo(const pblczero::Net& weights) {
  if (!weights.has_weights()) return;
  COUT << "\nWeights";
  COUT << "~~~~~~~";
  const auto& w = weights.weights();
  COUT << Justify("Blocks") << w.residual_size();
  int se_count = 0;
  for (size_t i = 0; i < w.residual_size(); i++) {
    if (w.residual(i).has_se()) se_count++;
  }
  if (se_count > 0) {
    COUT << Justify("SE blocks") << se_count;
  }

  COUT << Justify("Filters")
       << w.input().weights().params().size() / 2 / 112 / 9;
  COUT << Justify("Policy") << (w.has_policy1() ? "Convolution" : "Dense");
  if (!w.has_policy1()) {
    int policy_channels = w.policy().biases().params().size() / 2;
    if (policy_channels == 0) {
      policy_channels = w.policy().bn_means().params().size() / 2;
    }
    COUT << Justify("Policy channels") << policy_channels;
  }
  COUT << Justify("Value")
       << (w.ip2_val_w().params().size() / 2 % 3 == 0 ? "WDL" : "Classical");
  COUT << Justify("MLH") << (w.has_moves_left() ? "Present" : "Absent");
}

void ShowNetworkOnnxInfo(const pblczero::Net& weights,
                         bool show_onnx_internals) {
  if (!weights.has_onnx_model()) return;
  const auto& onnx_model = weights.onnx_model();
  COUT << "\nONNX interface";
  COUT << "~~~~~~~~~~~~~~";
  if (onnx_model.has_data_type()) {
    COUT << Justify("Data type")
         << pblczero::OnnxModel::DataType_Name(onnx_model.data_type());
  }
  if (onnx_model.has_input_planes()) {
    COUT << Justify("Input planes") << onnx_model.input_planes();
  }
  if (onnx_model.has_output_value()) {
    COUT << Justify("Output value") << onnx_model.output_value();
  }
  if (onnx_model.has_output_wdl()) {
    COUT << Justify("Output WDL") << onnx_model.output_wdl();
  }
  if (onnx_model.has_output_policy()) {
    COUT << Justify("Output Policy") << onnx_model.output_policy();
  }
  if (onnx_model.has_output_mlh()) {
    COUT << Justify("Output MLH") << onnx_model.output_mlh();
  }

  if (!show_onnx_internals) return;
  if (!onnx_model.has_model()) return;

  pblczero::ModelProto onnx;
  onnx.ParseFromString(onnx_model.model());
  COUT << "\nONNX model";
  COUT << "~~~~~~~~~~";

  if (onnx.has_ir_version()) {
    COUT << Justify("IR version") << onnx.ir_version();
  }
  if (onnx.has_producer_name()) {
    COUT << Justify("Producer Name") << onnx.producer_name();
  }
  if (onnx.has_producer_version()) {
    COUT << Justify("Producer Version") << onnx.producer_version();
  }
  if (onnx.has_domain()) {
    COUT << Justify("Domain") << onnx.domain();
  }
  if (onnx.has_model_version()) {
    COUT << Justify("Model Version") << onnx.model_version();
  }
  if (onnx.has_doc_string()) {
    COUT << Justify("Doc String") << onnx.doc_string();
  }
  for (const auto& input : onnx.graph().input()) {
    std::string name(input.name());
    if (input.has_doc_string()) {
      name += " (" + std::string(input.doc_string()) + ")";
    }
    COUT << Justify("Input") << name;
  }
  for (const auto& output : onnx.graph().output()) {
    std::string name(output.name());
    if (output.has_doc_string()) {
      name += " (" + std::string(output.doc_string()) + ")";
    }
    COUT << Justify("Output") << name;
  }
  for (const auto& opset : onnx.opset_import()) {
    std::string name;
    if (opset.has_domain()) name += std::string(opset.domain()) + " ";
    COUT << Justify("Opset") << name << opset.version();
  }
}

void ShowAllNetworkInfo(const pblczero::Net& weights) {
  ShowNetworkGenericInfo(weights);
  ShowNetworkFormatInfo(weights);
  ShowNetworkTrainingInfo(weights);
  ShowNetworkWeightsInfo(weights);
  ShowNetworkOnnxInfo(weights, true);
}

void DescribeNetworkCmd() {
  OptionsParser options_parser;
  if (!ProcessParameters(&options_parser)) return;

  const OptionsDict& dict = options_parser.GetOptionsDict();
  auto weights_file =
      LoadWeightsFromFile(dict.Get<std::string>(kWeightsFilenameId));
  ShowAllNetworkInfo(weights_file);
}
}  // namespace lczero

```

`src/lc0ctl/describenet.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "proto/net.pb.h"

namespace lczero {

void DescribeNetworkCmd();
void ShowNetworkGenericInfo(const pblczero::Net& weights);
void ShowNetworkFormatInfo(const pblczero::Net& weights);
void ShowNetworkTrainingInfo(const pblczero::Net& weights);
void ShowNetworkWeightsInfo(const pblczero::Net& weights);
void ShowNetworkOnnxInfo(const pblczero::Net& weights,
                         bool show_onnx_internals);
void ShowAllNetworkInfo(const pblczero::Net& weights);

}  // namespace lczero
```

`src/lc0ctl/leela2onnx.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <fstream>

#include "neural/loader.h"
#include "neural/onnx/converter.h"
#include "utils/files.h"
#include "utils/optionsparser.h"
#include "lc0ctl/describenet.h"

namespace lczero {
namespace {

const OptionId kInputFilenameId{"input", "InputFile",
                                "Path of the input Lc0 weights file."};
const OptionId kOutputFilenameId{"output", "OutputFile",
                                 "Path of the output ONNX file."};

const OptionId kInputPlanesName{"input-planes-name", "InputPlanesName",
                                "ONNX name to use for the input planes node."};
const OptionId kOutputPolicyHead{
    "policy-head-name", "PolicyHeadName",
    "ONNX name to use for the policy head output node."};
const OptionId kOutputWdl{"wdl-head-name", "WdlHeadName",
                          "ONNX name to use for the WDL head output node."};
const OptionId kOutputValue{
    "value-head-name", "ValueHeadName",
    "ONNX name to use for value policy head output node."};
const OptionId kOutputMlh{"mlh-head-name", "MlhHeadName",
                          "ONNX name to use for the MLH head output node."};

bool ProcessParameters(OptionsParser* options) {
  options->Add<StringOption>(kInputFilenameId);
  options->Add<StringOption>(kOutputFilenameId);

  options->Add<StringOption>(kInputPlanesName) = "/input/planes";
  options->Add<StringOption>(kOutputPolicyHead) = "/output/policy";
  options->Add<StringOption>(kOutputWdl) = "/output/wdl";
  options->Add<StringOption>(kOutputValue) = "/output/value";
  options->Add<StringOption>(kOutputMlh) = "/output/mlh";
  if (!options->ProcessAllFlags()) return false;

  const OptionsDict& dict = options->GetOptionsDict();
  dict.EnsureExists<std::string>(kInputFilenameId);
  dict.EnsureExists<std::string>(kOutputFilenameId);

  return true;
}

}  // namespace

void ConvertLeelaToOnnx() {
  OptionsParser options_parser;
  if (!ProcessParameters(&options_parser)) return;

  const OptionsDict& dict = options_parser.GetOptionsDict();
  auto weights_file =
      LoadWeightsFromFile(dict.Get<std::string>(kInputFilenameId));

  ShowNetworkFormatInfo(weights_file);
  if (weights_file.has_onnx_model()) {
    COUT << "The leela network already has ONNX network embedded, extracting.";
  } else {
    ShowNetworkWeightsInfo(weights_file);
    COUT << "Converting Leela network to the ONNX.";
    WeightsToOnnxConverterOptions onnx_options;
    onnx_options.input_planes_name = dict.Get<std::string>(kInputPlanesName);
    onnx_options.output_policy_head = dict.Get<std::string>(kOutputPolicyHead);
    onnx_options.output_wdl = dict.Get<std::string>(kOutputWdl);
    onnx_options.output_value = dict.Get<std::string>(kOutputValue);
    onnx_options.output_wdl = dict.Get<std::string>(kOutputWdl);
    weights_file = ConvertWeightsToOnnx(weights_file, onnx_options);
  }

  const auto& onnx = weights_file.onnx_model();
  WriteStringToFile(dict.Get<std::string>(kOutputFilenameId), onnx.model());
  ShowNetworkOnnxInfo(weights_file, false);
  COUT << "Done.";
}

}  // namespace lczero
```

`src/lc0ctl/leela2onnx.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

namespace lczero {

void ConvertLeelaToOnnx();

}  // namespace lczero
```

`src/lc0ctl/onnx2leela.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <zlib.h>

#include <algorithm>
#include <fstream>
#include <set>

#include "lc0ctl/describenet.h"
#include "neural/onnx/onnx.pb.h"
#include "proto/net.pb.h"
#include "utils/files.h"
#include "utils/optionsparser.h"

namespace lczero {
namespace {

template <typename T, size_t N>
std::vector<std::string> GetAllEnumValues(const std::array<T, N>& vals,
                                          std::string (*func)(T)) {
  std::vector<std::string> res;
  std::transform(vals.begin(), vals.end(), std::back_inserter(res), func);
  return res;
}

template <typename T, size_t N>
T GetEnumValueFromString(const std::string& str_value,
                         const std::array<T, N>& vals, std::string (*func)(T)) {
  auto iter = std::find_if(vals.begin(), vals.end(),
                           [&](T val) { return func(val) == str_value; });
  if (iter == vals.end()) {
    throw Exception("Enum value " + str_value + " is unknown.");
  }
  return *iter;
}

const OptionId kInputFilenameId{"input", "InputFile",
                                "Path of the input Lc0 weights file."};
const OptionId kOutputFilenameId{"output", "OutputFile",
                                 "Path of the output ONNX file."};

const OptionId kInputFormatId(
    "input-format", "InputFormat",
    "Format in which the neural network expects input to be.");
const OptionId kPolicyFormatId(
    "policy-format", "PolicyFormat",
    "Format of the policy head output. Currently the search code does not "
    "distinguish between POLICY_CLASSICAL and POLICY_CONVOLUTION, but maybe "
    "one day for new types it will have new values.");
const OptionId kValueFormatId(
    "value-format", "ValueFormat",
    "Format of the value head output. Currently the search code does not "
    "distinguish between VALUE_CLASSICAL and VALUE_WDL, but maybe one day for "
    "new types it will have new values.");
const OptionId kMovesLeftFormatId("moves-left-format", "MovesLeftFormat",
                                  "Format of the moves left head output.");

// ONNX options.
const OptionId kOnnxDataTypeId("onnx-data-type", "OnnxDataType",
                               "Data type to feed into the neural network.");
const OptionId kOnnxInputId{"onnx-input", "OnnxInput",
                            "The name of the input ONNX node."};
const OptionId kOnnxOutputValueId{
    "onnx-output-value", "OnnxOutputValue",
    "The name of the node for a classical value head."};
const OptionId kOnnxOutputWdlId{"onnx-output-wdl", "OnnxOutputWdl",
                                "The name of the node for a wdl value head."};
const OptionId kOnnxOutputPolicyId{"onnx-output-policy", "OnnxOutputPolicy",
                                   "The name of the node for a policy head."};
const OptionId kOnnxOutputMlhId{"onnx-output-mlh", "OnnxOutputMlh",
                                "The name of the node for a moves left head."};

const OptionId kValidateModelId{"validate-weights", "ValidateWeights",
                                "Do a basic check of the provided ONNX file."};

bool ProcessParameters(OptionsParser* options) {
  using pblczero::NetworkFormat;
  using pblczero::OnnxModel;
  options->Add<StringOption>(kInputFilenameId);
  options->Add<StringOption>(kOutputFilenameId);
  // Data format options.
  options->Add<ChoiceOption>(
      kInputFormatId, GetAllEnumValues(NetworkFormat::InputFormat_AllValues,
                                       NetworkFormat::InputFormat_Name)) =
      NetworkFormat::InputFormat_Name(NetworkFormat::INPUT_CLASSICAL_112_PLANE);
  options->Add<ChoiceOption>(
      kPolicyFormatId, GetAllEnumValues(NetworkFormat::PolicyFormat_AllValues,
                                        NetworkFormat::PolicyFormat_Name)) =
      NetworkFormat::PolicyFormat_Name(NetworkFormat::POLICY_CLASSICAL);
  options->Add<ChoiceOption>(
      kValueFormatId, GetAllEnumValues(NetworkFormat::ValueFormat_AllValues,
                                       NetworkFormat::ValueFormat_Name)) =
      NetworkFormat::ValueFormat_Name(NetworkFormat::VALUE_WDL);
  options->Add<ChoiceOption>(
      kMovesLeftFormatId,
      GetAllEnumValues(NetworkFormat::MovesLeftFormat_AllValues,
                       NetworkFormat::MovesLeftFormat_Name)) =
      NetworkFormat::MovesLeftFormat_Name(NetworkFormat::MOVES_LEFT_V1);
  // Onnx options.
  options->Add<ChoiceOption>(kOnnxDataTypeId,
                             GetAllEnumValues(OnnxModel::DataType_AllValues,
                                              OnnxModel::DataType_Name)) =
      OnnxModel::DataType_Name(OnnxModel::FLOAT);
  options->Add<StringOption>(kOnnxInputId);
  options->Add<StringOption>(kOnnxOutputPolicyId);
  options->Add<StringOption>(kOnnxOutputValueId);
  options->Add<StringOption>(kOnnxOutputWdlId);
  options->Add<StringOption>(kOnnxOutputMlhId);

  options->Add<BoolOption>(kValidateModelId) = true;

  if (!options->ProcessAllFlags()) return false;

  const OptionsDict& dict = options->GetOptionsDict();
  dict.EnsureExists<std::string>(kInputFilenameId);
  dict.EnsureExists<std::string>(kOutputFilenameId);
  return true;
}

bool ValidateNetwork(const pblczero::Net& weights) {
  const auto& onnx_model = weights.onnx_model();
  pblczero::ModelProto onnx;
  onnx.ParseFromString(onnx_model.model());

  if (!onnx.has_ir_version()) {
    CERR << "ONNX file doesn't appear to have version specified. Likely not an "
            "ONNX file.";
    return false;
  }
  const auto& onnx_inputs = onnx.graph().input();
  std::set<std::string> inputs;
  std::transform(onnx_inputs.begin(), onnx_inputs.end(),
                 std::inserter(inputs, inputs.end()),
                 [](const auto& x) { return std::string(x.name()); });

  const auto& onnx_outputs = onnx.graph().output();
  std::set<std::string> outputs;
  std::transform(onnx_outputs.begin(), onnx_outputs.end(),
                 std::inserter(outputs, outputs.end()),
                 [](const auto& x) { return std::string(x.name()); });

  auto check_exists = [](std::string_view n, std::set<std::string>* nodes) {
    std::string name(n);
    if (nodes->count(name) == 0) {
      CERR << "Node '" << name << "' doesn't exist in ONNX.";
      return false;
    }
    nodes->erase(name);
    return true;
  };

  if (onnx_model.has_input_planes() &&
      !check_exists(onnx_model.input_planes(), &inputs)) {
    return false;
  }
  if (onnx_model.has_output_value() &&
      !check_exists(onnx_model.output_value(), &outputs)) {
    return false;
  }
  if (onnx_model.has_output_wdl() &&
      !check_exists(onnx_model.output_wdl(), &outputs)) {
    return false;
  }
  if (onnx_model.has_output_policy() &&
      !check_exists(onnx_model.output_policy(), &outputs)) {
    return false;
  }
  if (onnx_model.has_output_mlh() &&
      !check_exists(onnx_model.output_mlh(), &outputs)) {
    return false;
  }
  for (const auto& input : inputs) {
    CERR << "Warning: ONNX input node '" << input << "' not used.";
  }
  for (const auto& output : outputs) {
    CERR << "Warning: ONNX output node '" << output << "' not used.";
  }

  if (!onnx_model.has_input_planes()) {
    CERR << "The --" << kOnnxInputId.long_flag()
         << " must be defined. Typical value for the ONNX networks exported "
            "from Leela is /input/planes.";
    return false;
  }
  if (!onnx_model.has_output_policy()) {
    CERR << "The --" << kOnnxOutputPolicyId.long_flag()
         << " must be defined. Typical value for the ONNX networks exported "
            "from Leela is /output/policy.";
    return false;
  }
  if (!onnx_model.has_output_value() && !onnx_model.has_output_wdl()) {
    CERR << "Either --" << kOnnxOutputValueId.long_flag() << " or --"
         << kOnnxOutputWdlId.long_flag()
         << " must be defined. Typical values for the ONNX networks exported "
            "from Leela are /output/value and /output/wdl.";
    return false;
  }
  if (onnx_model.has_output_value() && onnx_model.has_output_wdl()) {
    CERR << "Both --" << kOnnxOutputValueId.long_flag() << " and --"
         << kOnnxOutputWdlId.long_flag()
         << " are set. Only one of them has to be set.";
    return false;
  }

  return true;
}

}  // namespace

void ConvertOnnxToLeela() {
  using pblczero::NetworkFormat;
  using pblczero::OnnxModel;
  OptionsParser options_parser;
  if (!ProcessParameters(&options_parser)) return;

  const OptionsDict& dict = options_parser.GetOptionsDict();

  pblczero::Net out_weights;
  out_weights.set_magic(0x1c0);
  // ONNX networks appeared in v0.28.
  out_weights.mutable_min_version()->set_major(0);
  out_weights.mutable_min_version()->set_minor(28);

  auto format = out_weights.mutable_format()->mutable_network_format();
  format->set_network(NetworkFormat::NETWORK_ONNX);
  auto onnx = out_weights.mutable_onnx_model();
  onnx->set_data_type(GetEnumValueFromString(
      dict.Get<std::string>(kOnnxDataTypeId), OnnxModel::DataType_AllValues,
      OnnxModel::DataType_Name));

  // Input.
  format->set_input(GetEnumValueFromString(
      dict.Get<std::string>(kInputFormatId),
      NetworkFormat::InputFormat_AllValues, NetworkFormat::InputFormat_Name));
  if (dict.OwnExists<std::string>(kOnnxInputId)) {
    onnx->set_input_planes(dict.Get<std::string>(kOnnxInputId));
  }

  // Policy.
  format->set_policy(GetEnumValueFromString(
      dict.Get<std::string>(kPolicyFormatId),
      NetworkFormat::PolicyFormat_AllValues, NetworkFormat::PolicyFormat_Name));
  if (dict.OwnExists<std::string>(kOnnxOutputPolicyId)) {
    onnx->set_output_policy(dict.Get<std::string>(kOnnxOutputPolicyId));
  }

  // Value.
  format->set_value(GetEnumValueFromString(
      dict.Get<std::string>(kValueFormatId),
      NetworkFormat::ValueFormat_AllValues, NetworkFormat::ValueFormat_Name));
  if (dict.OwnExists<std::string>(kOnnxOutputValueId)) {
    onnx->set_output_value(dict.Get<std::string>(kOnnxOutputValueId));
  }
  if (dict.OwnExists<std::string>(kOnnxOutputWdlId)) {
    onnx->set_output_wdl(dict.Get<std::string>(kOnnxOutputWdlId));
  }

  // Mlh.
  if (dict.OwnExists<std::string>(kOnnxOutputMlhId)) {
    format->set_moves_left(
        GetEnumValueFromString(dict.Get<std::string>(kMovesLeftFormatId),
                               NetworkFormat::MovesLeftFormat_AllValues,
                               NetworkFormat::MovesLeftFormat_Name));
    onnx->set_output_mlh(dict.Get<std::string>(kOnnxOutputMlhId));
  }

  onnx->set_model(ReadFileToString(dict.Get<std::string>(kInputFilenameId)));
  if (dict.Get<bool>(kValidateModelId) && !ValidateNetwork(out_weights)) {
    return;
  }
  WriteStringToGzFile(dict.Get<std::string>(kOutputFilenameId),
                      out_weights.OutputAsString());
  ShowNetworkFormatInfo(out_weights);
  ShowNetworkOnnxInfo(out_weights, dict.Get<bool>(kValidateModelId));
  COUT << "Done.";
}

}  // namespace lczero
```

`src/lc0ctl/onnx2leela.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

namespace lczero {

void ConvertOnnxToLeela();

}  // namespace lczero
```

`src/main.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "benchmark/backendbench.h"
#include "benchmark/benchmark.h"
#include "chess/board.h"
#include "engine.h"
#include "lc0ctl/describenet.h"
#include "lc0ctl/leela2onnx.h"
#include "lc0ctl/onnx2leela.h"
#include "selfplay/loop.h"
#include "utils/commandline.h"
#include "utils/esc_codes.h"
#include "utils/logging.h"
#include "utils/numa.h"
#include "version.h"

int main(int argc, const char** argv) {
  using namespace lczero;
  EscCodes::Init();
  LOGFILE << "Lc0 started.";
  CERR << EscCodes::Bold() << EscCodes::Red() << "       _";
  CERR << "|   _ | |";
  CERR << "|_ |_ |_|" << EscCodes::Reset() << " v" << GetVersionStr()
       << " built " << __DATE__;

  try {
    Numa::Init();
    Numa::BindThread(0);
    InitializeMagicBitboards();

    CommandLine::Init(argc, argv);
    CommandLine::RegisterMode("uci", "(default) Act as UCI engine");
    CommandLine::RegisterMode("selfplay", "Play games with itself");
    CommandLine::RegisterMode("benchmark", "Quick benchmark");
    CommandLine::RegisterMode("backendbench",
                              "Quick benchmark of backend only");
    CommandLine::RegisterMode("leela2onnx", "Convert Leela network to ONNX.");
    CommandLine::RegisterMode("onnx2leela",
                              "Convert ONNX network to Leela net.");
    CommandLine::RegisterMode("describenet",
                              "Shows details about the Leela network.");

    if (CommandLine::ConsumeCommand("selfplay")) {
      // Selfplay mode.
      SelfPlayLoop loop;
      loop.RunLoop();
    } else if (CommandLine::ConsumeCommand("benchmark")) {
      // Benchmark mode.
      Benchmark benchmark;
      benchmark.Run();
    } else if (CommandLine::ConsumeCommand("backendbench")) {
      // Backend Benchmark mode.
      BackendBenchmark benchmark;
      benchmark.Run();
    } else if (CommandLine::ConsumeCommand("leela2onnx")) {
      lczero::ConvertLeelaToOnnx();
    } else if (CommandLine::ConsumeCommand("onnx2leela")) {
      lczero::ConvertOnnxToLeela();
    } else if (CommandLine::ConsumeCommand("describenet")) {
      lczero::DescribeNetworkCmd();
    } else {
      // Consuming optional "uci" mode.
      CommandLine::ConsumeCommand("uci");
      // Ordinary UCI engine.
      EngineLoop loop;
      loop.RunLoop();
    }
  } catch (std::exception& e) {
    std::cerr << "Unhandled exception: " << e.what() << std::endl;
    abort();
  }
}

```

`src/mcts/node.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "mcts/node.h"

#include <algorithm>
#include <cassert>
#include <cmath>
#include <cstring>
#include <iostream>
#include <sstream>
#include <thread>

#include "neural/encoder.h"
#include "neural/network.h"
#include "utils/exception.h"
#include "utils/hashcat.h"
#include "utils/numa.h"

namespace lczero {

/////////////////////////////////////////////////////////////////////////
// Node garbage collector
/////////////////////////////////////////////////////////////////////////

namespace {
// Periodicity of garbage collection, milliseconds.
const int kGCIntervalMs = 100;

// Every kGCIntervalMs milliseconds release nodes in a separate GC thread.
class NodeGarbageCollector {
 public:
  NodeGarbageCollector() : gc_thread_([this]() { Worker(); }) {}

  // Takes ownership of a subtree, to dispose it in a separate thread when
  // it has time.
  void AddToGcQueue(std::unique_ptr<Node> node, size_t solid_size = 0) {
    if (!node) return;
    Mutex::Lock lock(gc_mutex_);
    subtrees_to_gc_.emplace_back(std::move(node));
    subtrees_to_gc_solid_size_.push_back(solid_size);
  }

  ~NodeGarbageCollector() {
    // Flips stop flag and waits for a worker thread to stop.
    stop_.store(true);
    gc_thread_.join();
  }

 private:
  void GarbageCollect() {
    while (!stop_.load()) {
      // Node will be released in destructor when mutex is not locked.
      std::unique_ptr<Node> node_to_gc;
      size_t solid_size = 0;
      {
        // Lock the mutex and move last subtree from subtrees_to_gc_ into
        // node_to_gc.
        Mutex::Lock lock(gc_mutex_);
        if (subtrees_to_gc_.empty()) return;
        node_to_gc = std::move(subtrees_to_gc_.back());
        subtrees_to_gc_.pop_back();
        solid_size = subtrees_to_gc_solid_size_.back();
        subtrees_to_gc_solid_size_.pop_back();
      }
      // Solid is a hack...
      if (solid_size != 0) {
        for (size_t i = 0; i < solid_size; i++) {
          node_to_gc.get()[i].~Node();
        }
        std::allocator<Node> alloc;
        alloc.deallocate(node_to_gc.release(), solid_size);
      }
    }
  }

  void Worker() {
    // Keep garbage collection on same core as where search workers are most
    // likely to be to make any lock conention on gc mutex cheaper.
    Numa::BindThread(0);
    while (!stop_.load()) {
      std::this_thread::sleep_for(std::chrono::milliseconds(kGCIntervalMs));
      GarbageCollect();
    };
  }

  mutable Mutex gc_mutex_;
  std::vector<std::unique_ptr<Node>> subtrees_to_gc_ GUARDED_BY(gc_mutex_);
  std::vector<size_t> subtrees_to_gc_solid_size_ GUARDED_BY(gc_mutex_);

  // When true, Worker() should stop and exit.
  std::atomic<bool> stop_{false};
  std::thread gc_thread_;
};

NodeGarbageCollector gNodeGc;
}  // namespace

/////////////////////////////////////////////////////////////////////////
// Edge
/////////////////////////////////////////////////////////////////////////

Move Edge::GetMove(bool as_opponent) const {
  if (!as_opponent) return move_;
  Move m = move_;
  m.Mirror();
  return m;
}

// Policy priors (P) are stored in a compressed 16-bit format.
//
// Source values are 32-bit floats:
// * bit 31 is sign (zero means positive)
// * bit 30 is sign of exponent (zero means nonpositive)
// * bits 29..23 are value bits of exponent
// * bits 22..0 are significand bits (plus a "virtual" always-on bit: s ∈ [1,2))
// The number is then sign * 2^exponent * significand, usually.
// See https://www.h-schmidt.net/FloatConverter/IEEE754.html for details.
//
// In compressed 16-bit value we store bits 27..12:
// * bit 31 is always off as values are always >= 0
// * bit 30 is always off as values are always < 2
// * bits 29..28 are only off for values < 4.6566e-10, assume they are always on
// * bits 11..0 are for higher precision, they are dropped leaving only 11 bits
//     of precision
//
// When converting to compressed format, bit 11 is added to in order to make it
// a rounding rather than truncation.
//
// Out of 65556 possible values, 2047 are outside of [0,1] interval (they are in
// interval (1,2)). This is fine because the values in [0,1] are skewed towards
// 0, which is also exactly how the components of policy tend to behave (since
// they add up to 1).

// If the two assumed-on exponent bits (3<<28) are in fact off, the input is
// rounded up to the smallest value with them on. We accomplish this by
// subtracting the two bits from the input and checking for a negative result
// (the subtraction works despite crossing from exponent to significand). This
// is combined with the round-to-nearest addition (1<<11) into one op.
void Edge::SetP(float p) {
  assert(0.0f <= p && p <= 1.0f);
  constexpr int32_t roundings = (1 << 11) - (3 << 28);
  int32_t tmp;
  std::memcpy(&tmp, &p, sizeof(float));
  tmp += roundings;
  p_ = (tmp < 0) ? 0 : static_cast<uint16_t>(tmp >> 12);
}

float Edge::GetP() const {
  // Reshift into place and set the assumed-set exponent bits.
  uint32_t tmp = (static_cast<uint32_t>(p_) << 12) | (3 << 28);
  float ret;
  std::memcpy(&ret, &tmp, sizeof(uint32_t));
  return ret;
}

std::string Edge::DebugString() const {
  std::ostringstream oss;
  oss << "Move: " << move_.as_string() << " p_: " << p_ << " GetP: " << GetP();
  return oss.str();
}

std::unique_ptr<Edge[]> Edge::FromMovelist(const MoveList& moves) {
  std::unique_ptr<Edge[]> edges = std::make_unique<Edge[]>(moves.size());
  auto* edge = edges.get();
  for (const auto move : moves) edge++->move_ = move;
  return edges;
}

/////////////////////////////////////////////////////////////////////////
// Node
/////////////////////////////////////////////////////////////////////////

Node* Node::CreateSingleChildNode(Move move) {
  assert(!edges_);
  assert(!child_);
  edges_ = Edge::FromMovelist({move});
  num_edges_ = 1;
  child_ = std::make_unique<Node>(this, 0);
  return child_.get();
}

void Node::CreateEdges(const MoveList& moves) {
  assert(!edges_);
  assert(!child_);
  edges_ = Edge::FromMovelist(moves);
  num_edges_ = moves.size();
}

Node::ConstIterator Node::Edges() const {
  return {*this, !solid_children_ ? &child_ : nullptr};
}
Node::Iterator Node::Edges() {
  return {*this, !solid_children_ ? &child_ : nullptr};
}

float Node::GetVisitedPolicy() const {
  float sum = 0.0f;
  for (auto* node : VisitedNodes()) sum += GetEdgeToNode(node)->GetP();
  return sum;
}

Edge* Node::GetEdgeToNode(const Node* node) const {
  assert(node->parent_ == this);
  assert(node->index_ < num_edges_);
  return &edges_[node->index_];
}

Edge* Node::GetOwnEdge() const { return GetParent()->GetEdgeToNode(this); }

std::string Node::DebugString() const {
  std::ostringstream oss;
  oss << " Term:" << static_cast<int>(terminal_type_) << " This:" << this
      << " Parent:" << parent_ << " Index:" << index_
      << " Child:" << child_.get() << " Sibling:" << sibling_.get()
      << " WL:" << wl_ << " N:" << n_ << " N_:" << n_in_flight_
      << " Edges:" << static_cast<int>(num_edges_)
      << " Bounds:" << static_cast<int>(lower_bound_) - 2 << ","
      << static_cast<int>(upper_bound_) - 2
      << " Solid:" << solid_children_;
  return oss.str();
}

bool Node::MakeSolid() {
  if (solid_children_ || num_edges_ == 0 || IsTerminal()) return false;
  // Can only make solid if no immediate leaf childredn are in flight since we
  // allow the search code to hold references to leaf nodes across locks.
  Node* old_child_to_check = child_.get();
  uint32_t total_in_flight = 0;
  while (old_child_to_check != nullptr) {
    if (old_child_to_check->GetN() <= 1 &&
        old_child_to_check->GetNInFlight() > 0) {
      return false;
    }
    if (old_child_to_check->IsTerminal() &&
        old_child_to_check->GetNInFlight() > 0) {
      return false;
    }
    total_in_flight += old_child_to_check->GetNInFlight();
    old_child_to_check = old_child_to_check->sibling_.get();
  }
  // If the total of children in flight is not the same as self, then there are
  // collisions against immediate children (which don't update the GetNInFlight
  // of the leaf) and its not safe.
  if (total_in_flight != GetNInFlight()) {
    return false;
  }
  std::allocator<Node> alloc;
  auto* new_children = alloc.allocate(num_edges_);
  for (int i = 0; i < num_edges_; i++) {
    new (&(new_children[i])) Node(this, i);
  }
  std::unique_ptr<Node> old_child = std::move(child_);
  while (old_child) {
    int index = old_child->index_;
    new_children[index] = std::move(*old_child.get());
    // This isn't needed, but it helps crash things faster if something has gone wrong.
    old_child->parent_ = nullptr;
    gNodeGc.AddToGcQueue(std::move(old_child));
    new_children[index].UpdateChildrenParents();
    old_child = std::move(new_children[index].sibling_);
  }
  // This is a hack.
  child_ = std::unique_ptr<Node>(new_children);
  solid_children_ = true;
  return true;
}

void Node::SortEdges() {
  assert(edges_);
  assert(!child_);
  // Sorting on raw p_ is the same as sorting on GetP() as a side effect of
  // the encoding, and its noticeably faster.
  std::sort(edges_.get(), (edges_.get() + num_edges_),
            [](const Edge& a, const Edge& b) { return a.p_ > b.p_; });
}

void Node::MakeTerminal(GameResult result, float plies_left, Terminal type) {
  if (type != Terminal::TwoFold) SetBounds(result, result);
  terminal_type_ = type;
  m_ = plies_left;
  if (result == GameResult::DRAW) {
    wl_ = 0.0f;
    d_ = 1.0f;
  } else if (result == GameResult::WHITE_WON) {
    wl_ = 1.0f;
    d_ = 0.0f;
  } else if (result == GameResult::BLACK_WON) {
    wl_ = -1.0f;
    d_ = 0.0f;
    // Terminal losses have no uncertainty and no reason for their U value to be
    // comparable to another non-loss choice. Force this by clearing the policy.
    if (GetParent() != nullptr) GetOwnEdge()->SetP(0.0f);
  }
}

void Node::MakeNotTerminal() {
  terminal_type_ = Terminal::NonTerminal;
  n_ = 0;

  // If we have edges, we've been extended (1 visit), so include children too.
  if (edges_) {
    n_++;
    for (const auto& child : Edges()) {
      const auto n = child.GetN();
      if (n > 0) {
        n_ += n;
        // Flip Q for opponent.
        // Default values don't matter as n is > 0.
        wl_ += -child.GetWL(0.0f) * n;
        d_ += child.GetD(0.0f) * n;
      }
    }

    // Recompute with current eval (instead of network's) and children's eval.
    wl_ /= n_;
    d_ /= n_;
  }
}

void Node::SetBounds(GameResult lower, GameResult upper) {
  lower_bound_ = lower;
  upper_bound_ = upper;
}

bool Node::TryStartScoreUpdate() {
  if (n_ == 0 && n_in_flight_ > 0) return false;
  ++n_in_flight_;
  return true;
}

void Node::CancelScoreUpdate(int multivisit) {
  n_in_flight_ -= multivisit;
}

void Node::FinalizeScoreUpdate(float v, float d, float m, int multivisit) {
  // Recompute Q.
  wl_ += multivisit * (v - wl_) / (n_ + multivisit);
  d_ += multivisit * (d - d_) / (n_ + multivisit);
  m_ += multivisit * (m - m_) / (n_ + multivisit);

  // Increment N.
  n_ += multivisit;
  // Decrement virtual loss.
  n_in_flight_ -= multivisit;
}

void Node::AdjustForTerminal(float v, float d, float m, int multivisit) {
  // Recompute Q.
  wl_ += multivisit * v / n_;
  d_ += multivisit * d / n_;
  m_ += multivisit * m / n_;
}

void Node::RevertTerminalVisits(float v, float d, float m, int multivisit) {
  // Compute new n_ first, as reducing a node to 0 visits is a special case.
  const int n_new = n_ - multivisit;
  if (n_new <= 0) {
    // If n_new == 0, reset all relevant values to 0.
    wl_ = 0.0;
    d_ = 1.0;
    m_ = 0.0;
    n_ = 0;
  } else {
    // Recompute Q and M.
    wl_ -= multivisit * (v - wl_) / n_new;
    d_ -= multivisit * (d - d_) / n_new;
    m_ -= multivisit * (m - m_) / n_new;
    // Decrement N.
    n_ -= multivisit;
  }
}

void Node::UpdateChildrenParents() {
  if (!solid_children_) {
    Node* cur_child = child_.get();
    while (cur_child != nullptr) {
      cur_child->parent_ = this;
      cur_child = cur_child->sibling_.get();
    }
  } else {
    Node* child_array = child_.get();
    for (int i = 0; i < num_edges_; i++) {
      child_array[i].parent_ = this;
    }
  }
}

void Node::ReleaseChildren() {
  gNodeGc.AddToGcQueue(std::move(child_), solid_children_ ? num_edges_ : 0);
}

void Node::ReleaseChildrenExceptOne(Node* node_to_save) {
  if (solid_children_) {
    std::unique_ptr<Node> saved_node;
    if (node_to_save != nullptr) {
      saved_node = std::make_unique<Node>(this, node_to_save->index_);
      *saved_node = std::move(*node_to_save);
    }
    gNodeGc.AddToGcQueue(std::move(child_), num_edges_);
    child_ = std::move(saved_node);
    if (child_) {
      child_->UpdateChildrenParents();
    }
    solid_children_ = false;
  } else {
    // Stores node which will have to survive (or nullptr if it's not found).
    std::unique_ptr<Node> saved_node;
    // Pointer to unique_ptr, so that we could move from it.
    for (std::unique_ptr<Node>* node = &child_; *node;
         node = &(*node)->sibling_) {
      // If current node is the one that we have to save.
      if (node->get() == node_to_save) {
        // Kill all remaining siblings.
        gNodeGc.AddToGcQueue(std::move((*node)->sibling_));
        // Save the node, and take the ownership from the unique_ptr.
        saved_node = std::move(*node);
        break;
      }
    }
    // Make saved node the only child. (kills previous siblings).
    gNodeGc.AddToGcQueue(std::move(child_));
    child_ = std::move(saved_node);
  }
  if (!child_) {
    num_edges_ = 0;
    edges_.reset();  // Clear edges list.
  }
}

/////////////////////////////////////////////////////////////////////////
// EdgeAndNode
/////////////////////////////////////////////////////////////////////////

std::string EdgeAndNode::DebugString() const {
  if (!edge_) return "(no edge)";
  return edge_->DebugString() + " " +
         (node_ ? node_->DebugString() : "(no node)");
}

/////////////////////////////////////////////////////////////////////////
// NodeTree
/////////////////////////////////////////////////////////////////////////

void NodeTree::MakeMove(Move move) {
  if (HeadPosition().IsBlackToMove()) move.Mirror();
  const auto& board = HeadPosition().GetBoard();

  Node* new_head = nullptr;
  for (auto& n : current_head_->Edges()) {
    if (board.IsSameMove(n.GetMove(), move)) {
      new_head = n.GetOrSpawnNode(current_head_);
      // Ensure head is not terminal, so search can extend or visit children of
      // "terminal" positions, e.g., WDL hits, converted terminals, 3-fold draw.
      if (new_head->IsTerminal()) new_head->MakeNotTerminal();
      break;
    }
  }
  move = board.GetModernMove(move);
  current_head_->ReleaseChildrenExceptOne(new_head);
  new_head = current_head_->child_.get();
  current_head_ =
      new_head ? new_head : current_head_->CreateSingleChildNode(move);
  history_.Append(move);
}

void NodeTree::TrimTreeAtHead() {
  // If solid, this will be empty before move and will be moved back empty
  // afterwards which is fine.
  auto tmp = std::move(current_head_->sibling_);
  // Send dependent nodes for GC instead of destroying them immediately.
  current_head_->ReleaseChildren();
  *current_head_ = Node(current_head_->GetParent(), current_head_->index_);
  current_head_->sibling_ = std::move(tmp);
}

bool NodeTree::ResetToPosition(const std::string& starting_fen,
                               const std::vector<Move>& moves) {
  ChessBoard starting_board;
  int no_capture_ply;
  int full_moves;
  starting_board.SetFromFen(starting_fen, &no_capture_ply, &full_moves);
  if (gamebegin_node_ &&
      (history_.Starting().GetBoard() != starting_board ||
       history_.Starting().GetRule50Ply() != no_capture_ply)) {
    // Completely different position.
    DeallocateTree();
  }

  if (!gamebegin_node_) {
    gamebegin_node_ = std::make_unique<Node>(nullptr, 0);
  }

  history_.Reset(starting_board, no_capture_ply,
                 full_moves * 2 - (starting_board.flipped() ? 1 : 2));

  Node* old_head = current_head_;
  current_head_ = gamebegin_node_.get();
  bool seen_old_head = (gamebegin_node_.get() == old_head);
  for (const auto& move : moves) {
    MakeMove(move);
    if (old_head == current_head_) seen_old_head = true;
  }

  // MakeMove guarantees that no siblings exist; but, if we didn't see the old
  // head, it means we might have a position that was an ancestor to a
  // previously searched position, which means that the current_head_ might
  // retain old n_ and q_ (etc) data, even though its old children were
  // previously trimmed; we need to reset current_head_ in that case.
  if (!seen_old_head) TrimTreeAtHead();
  return seen_old_head;
}

void NodeTree::DeallocateTree() {
  // Same as gamebegin_node_.reset(), but actual deallocation will happen in
  // GC thread.
  gNodeGc.AddToGcQueue(std::move(gamebegin_node_));
  gamebegin_node_ = nullptr;
  current_head_ = nullptr;
}

}  // namespace lczero

```

`src/mcts/node.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <algorithm>
#include <cmath>
#include <iostream>
#include <memory>
#include <mutex>

#include "chess/board.h"
#include "chess/callbacks.h"
#include "chess/position.h"
#include "neural/cache.h"
#include "neural/encoder.h"
#include "proto/net.pb.h"
#include "utils/mutex.h"

namespace lczero {

// Children of a node are stored the following way:
// * Edges and Nodes edges point to are stored separately.
// * There may be dangling edges (which don't yet point to any Node object yet)
// * Edges are stored are a simple array on heap.
// * Nodes are stored as a linked list, and contain index_ field which shows
//   which edge of a parent that node points to.
//   Or they are stored a contiguous array of Node objects on the heap if
//   solid_children_ is true. If the children have been 'solidified' their
//   sibling links are unused and left empty. In this state there are no
//   dangling edges, but the nodes may not have ever received any visits.
//
// Example:
//                                Parent Node
//                                    |
//        +-------------+-------------+----------------+--------------+
//        |              |            |                |              |
//   Edge 0(Nf3)    Edge 1(Bc5)     Edge 2(a4)     Edge 3(Qxf7)    Edge 4(a3)
//    (dangling)         |           (dangling)        |           (dangling)
//                   Node, Q=0.5                    Node, Q=-0.2
//
//  Is represented as:
// +--------------+
// | Parent Node  |
// +--------------+                                        +--------+
// | edges_       | -------------------------------------> | Edge[] |
// |              |    +------------+                      +--------+
// | child_       | -> | Node       |                      | Nf3    |
// +--------------+    +------------+                      | Bc5    |
//                     | index_ = 1 |                      | a4     |
//                     | q_ = 0.5   |    +------------+    | Qxf7   |
//                     | sibling_   | -> | Node       |    | a3     |
//                     +------------+    +------------+    +--------+
//                                       | index_ = 3 |
//                                       | q_ = -0.2  |
//                                       | sibling_   | -> nullptr
//                                       +------------+

class Node;
class Edge {
 public:
  // Creates array of edges from the list of moves.
  static std::unique_ptr<Edge[]> FromMovelist(const MoveList& moves);

  // Returns move from the point of view of the player making it (if as_opponent
  // is false) or as opponent (if as_opponent is true).
  Move GetMove(bool as_opponent = false) const;

  // Returns or sets value of Move policy prior returned from the neural net
  // (but can be changed by adding Dirichlet noise). Must be in [0,1].
  float GetP() const;
  void SetP(float val);

  // Debug information about the edge.
  std::string DebugString() const;

 private:
  // Move corresponding to this node. From the point of view of a player,
  // i.e. black's e7e5 is stored as e2e4.
  // Root node contains move a1a1.
  Move move_;

  // Probability that this move will be made, from the policy head of the neural
  // network; compressed to a 16 bit format (5 bits exp, 11 bits significand).
  uint16_t p_ = 0;
  friend class Node;
};

struct Eval {
  float wl;
  float d;
  float ml;
};

class EdgeAndNode;
template <bool is_const>
class Edge_Iterator;

template <bool is_const>
class VisitedNode_Iterator;

class Node {
 public:
  using Iterator = Edge_Iterator<false>;
  using ConstIterator = Edge_Iterator<true>;

  enum class Terminal : uint8_t { NonTerminal, EndOfGame, Tablebase, TwoFold };

  // Takes pointer to a parent node and own index in a parent.
  Node(Node* parent, uint16_t index)
      : parent_(parent),
        index_(index),
        terminal_type_(Terminal::NonTerminal),
        lower_bound_(GameResult::BLACK_WON),
        upper_bound_(GameResult::WHITE_WON),
        solid_children_(false) {}

  // We have a custom destructor, but its behavior does not need to be emulated
  // during move operations so default is fine.
  Node(Node&& move_from) = default;
  Node& operator=(Node&& move_from) = default;

  // Allocates a new edge and a new node. The node has to be no edges before
  // that.
  Node* CreateSingleChildNode(Move m);

  // Creates edges from a movelist. There has to be no edges before that.
  void CreateEdges(const MoveList& moves);

  // Gets parent node.
  Node* GetParent() const { return parent_; }

  // Returns whether a node has children.
  bool HasChildren() const { return static_cast<bool>(edges_); }

  // Returns sum of policy priors which have had at least one playout.
  float GetVisitedPolicy() const;
  uint32_t GetN() const { return n_; }
  uint32_t GetNInFlight() const { return n_in_flight_; }
  uint32_t GetChildrenVisits() const { return n_ > 0 ? n_ - 1 : 0; }
  // Returns n = n_if_flight.
  int GetNStarted() const { return n_ + n_in_flight_; }
  float GetQ(float draw_score) const { return wl_ + draw_score * d_; }
  // Returns node eval, i.e. average subtree V for non-terminal node and -1/0/1
  // for terminal nodes.
  float GetWL() const { return wl_; }
  float GetD() const { return d_; }
  float GetM() const { return m_; }

  // Returns whether the node is known to be draw/lose/win.
  bool IsTerminal() const { return terminal_type_ != Terminal::NonTerminal; }
  bool IsTbTerminal() const { return terminal_type_ == Terminal::Tablebase; }
  bool IsTwoFoldTerminal() const { return terminal_type_ == Terminal::TwoFold; }
  typedef std::pair<GameResult, GameResult> Bounds;
  Bounds GetBounds() const { return {lower_bound_, upper_bound_}; }
  uint8_t GetNumEdges() const { return num_edges_; }

  // Output must point to at least max_needed floats.
  void CopyPolicy(int max_needed, float* output) const {
    if (!edges_) return;
    int loops = std::min(static_cast<int>(num_edges_), max_needed);
    for (int i = 0; i < loops; i++) {
      output[i] = edges_[i].GetP();
    }
  }

  // Makes the node terminal and sets it's score.
  void MakeTerminal(GameResult result, float plies_left = 0.0f,
                    Terminal type = Terminal::EndOfGame);
  // Makes the node not terminal and updates its visits.
  void MakeNotTerminal();
  void SetBounds(GameResult lower, GameResult upper);

  // If this node is not in the process of being expanded by another thread
  // (which can happen only if n==0 and n-in-flight==1), mark the node as
  // "being updated" by incrementing n-in-flight, and return true.
  // Otherwise return false.
  bool TryStartScoreUpdate();
  // Decrements n-in-flight back.
  void CancelScoreUpdate(int multivisit);
  // Updates the node with newly computed value v.
  // Updates:
  // * Q (weighted average of all V in a subtree)
  // * N (+=1)
  // * N-in-flight (-=1)
  void FinalizeScoreUpdate(float v, float d, float m, int multivisit);
  // Like FinalizeScoreUpdate, but it updates n existing visits by delta amount.
  void AdjustForTerminal(float v, float d, float m, int multivisit);
  // Revert visits to a node which ended in a now reverted terminal.
  void RevertTerminalVisits(float v, float d, float m, int multivisit);
  // When search decides to treat one visit as several (in case of collisions
  // or visiting terminal nodes several times), it amplifies the visit by
  // incrementing n_in_flight.
  void IncrementNInFlight(int multivisit) { n_in_flight_ += multivisit; }

  // Updates max depth, if new depth is larger.
  void UpdateMaxDepth(int depth);

  // Calculates the full depth if new depth is larger, updates it, returns
  // in depth parameter, and returns true if it was indeed updated.
  bool UpdateFullDepth(uint16_t* depth);

  // Returns range for iterating over edges.
  ConstIterator Edges() const;
  Iterator Edges();

  // Returns range for iterating over child nodes with N > 0.
  VisitedNode_Iterator<true> VisitedNodes() const;
  VisitedNode_Iterator<false> VisitedNodes();

  // Deletes all children.
  void ReleaseChildren();

  // Deletes all children except one.
  // The node provided may be moved, so should not be relied upon to exist
  // afterwards.
  void ReleaseChildrenExceptOne(Node* node);

  // For a child node, returns corresponding edge.
  Edge* GetEdgeToNode(const Node* node) const;

  // Returns edge to the own node.
  Edge* GetOwnEdge() const;

  // Debug information about the node.
  std::string DebugString() const;

  // Reallocates this nodes children to be in a solid block, if possible and not
  // already done. Returns true if the transformation was performed.
  bool MakeSolid();

  void SortEdges();

  // Index in parent edges - useful for correlated ordering.
  uint16_t Index() const { return index_; }

  ~Node() {
    if (solid_children_ && child_) {
      // As a hack, solid_children is actually storing an array in here, release
      // so we can correctly invoke the array delete.
      for (int i = 0; i < num_edges_; i++) {
        child_.get()[i].~Node();
      }
      std::allocator<Node> alloc;
      alloc.deallocate(child_.release(), num_edges_);
    }
  }

 private:
  // For each child, ensures that its parent pointer is pointing to this.
  void UpdateChildrenParents();

  // To minimize the number of padding bytes and to avoid having unnecessary
  // padding when new fields are added, we arrange the fields by size, largest
  // to smallest.

  // 8 byte fields.
  // Average value (from value head of neural network) of all visited nodes in
  // subtree. For terminal nodes, eval is stored. This is from the perspective
  // of the player who "just" moved to reach this position, rather than from the
  // perspective of the player-to-move for the position.
  // WL stands for "W minus L". Is equal to Q if draw score is 0.
  double wl_ = 0.0f;

  // 8 byte fields on 64-bit platforms, 4 byte on 32-bit.
  // Array of edges.
  std::unique_ptr<Edge[]> edges_;
  // Pointer to a parent node. nullptr for the root.
  Node* parent_ = nullptr;
  // Pointer to a first child. nullptr for a leaf node.
  // As a 'hack' actually a unique_ptr to Node[] if solid_children.
  std::unique_ptr<Node> child_;
  // Pointer to a next sibling. nullptr if there are no further siblings.
  // Also null in the solid case.
  std::unique_ptr<Node> sibling_;

  // 4 byte fields.
  // Averaged draw probability. Works similarly to WL, except that D is not
  // flipped depending on the side to move.
  float d_ = 0.0f;
  // Estimated remaining plies.
  float m_ = 0.0f;
  // How many completed visits this node had.
  uint32_t n_ = 0;
  // (AKA virtual loss.) How many threads currently process this node (started
  // but not finished). This value is added to n during selection which node
  // to pick in MCTS, and also when selecting the best move.
  uint32_t n_in_flight_ = 0;

  // 2 byte fields.
  // Index of this node is parent's edge list.
  uint16_t index_;

  // 1 byte fields.
  // Number of edges in @edges_.
  uint8_t num_edges_ = 0;

  // Bit fields using parts of uint8_t fields initialized in the constructor.
  // Whether or not this node end game (with a winning of either sides or draw).
  Terminal terminal_type_ : 2;
  // Best and worst result for this node.
  GameResult lower_bound_ : 2;
  GameResult upper_bound_ : 2;
  // Whether the child_ is actually an array of equal length to edges.
  bool solid_children_ : 1;

  // TODO(mooskagh) Unfriend NodeTree.
  friend class NodeTree;
  friend class Edge_Iterator<true>;
  friend class Edge_Iterator<false>;
  friend class Edge;
  friend class VisitedNode_Iterator<true>;
  friend class VisitedNode_Iterator<false>;
};

// Define __i386__  or __arm__ also for 32 bit Windows.
#if defined(_M_IX86)
#define __i386__
#endif
#if defined(_M_ARM) && !defined(_M_AMD64)
#define __arm__
#endif

// A basic sanity check. This must be adjusted when Node members are adjusted.
#if defined(__i386__) || (defined(__arm__) && !defined(__aarch64__))
static_assert(sizeof(Node) == 48, "Unexpected size of Node for 32bit compile");
#else
static_assert(sizeof(Node) == 64, "Unexpected size of Node");
#endif

// Contains Edge and Node pair and set of proxy functions to simplify access
// to them.
class EdgeAndNode {
 public:
  EdgeAndNode() = default;
  EdgeAndNode(Edge* edge, Node* node) : edge_(edge), node_(node) {}
  void Reset() { edge_ = nullptr; }
  explicit operator bool() const { return edge_ != nullptr; }
  bool operator==(const EdgeAndNode& other) const {
    return edge_ == other.edge_;
  }
  bool operator!=(const EdgeAndNode& other) const {
    return edge_ != other.edge_;
  }
  bool HasNode() const { return node_ != nullptr; }
  Edge* edge() const { return edge_; }
  Node* node() const { return node_; }

  // Proxy functions for easier access to node/edge.
  float GetQ(float default_q, float draw_score) const {
    return (node_ && node_->GetN() > 0) ? node_->GetQ(draw_score) : default_q;
  }
  float GetWL(float default_wl) const {
    return (node_ && node_->GetN() > 0) ? node_->GetWL() : default_wl;
  }
  float GetD(float default_d) const {
    return (node_ && node_->GetN() > 0) ? node_->GetD() : default_d;
  }
  float GetM(float default_m) const {
    return (node_ && node_->GetN() > 0) ? node_->GetM() : default_m;
  }
  // N-related getters, from Node (if exists).
  uint32_t GetN() const { return node_ ? node_->GetN() : 0; }
  int GetNStarted() const { return node_ ? node_->GetNStarted() : 0; }
  uint32_t GetNInFlight() const { return node_ ? node_->GetNInFlight() : 0; }

  // Whether the node is known to be terminal.
  bool IsTerminal() const { return node_ ? node_->IsTerminal() : false; }
  bool IsTbTerminal() const { return node_ ? node_->IsTbTerminal() : false; }
  Node::Bounds GetBounds() const {
    return node_ ? node_->GetBounds()
                 : Node::Bounds{GameResult::BLACK_WON, GameResult::WHITE_WON};
  }

  // Edge related getters.
  float GetP() const { return edge_->GetP(); }
  Move GetMove(bool flip = false) const {
    return edge_ ? edge_->GetMove(flip) : Move();
  }

  // Returns U = numerator * p / N.
  // Passed numerator is expected to be equal to (cpuct * sqrt(N[parent])).
  float GetU(float numerator) const {
    return numerator * GetP() / (1 + GetNStarted());
  }

  int GetVisitsToReachU(float target_score, float numerator,
                        float score_without_u) const {
    if (score_without_u >= target_score) return std::numeric_limits<int>::max();
    const auto n1 = GetNStarted() + 1;
    return std::max(1.0f,
                    std::min(std::floor(GetP() * numerator /
                                            (target_score - score_without_u) -
                                        n1) +
                                 1,
                             1e9f));
  }

  std::string DebugString() const;

 protected:
  // nullptr means that the whole pair is "null". (E.g. when search for a node
  // didn't find anything, or as end iterator signal).
  Edge* edge_ = nullptr;
  // nullptr means that the edge doesn't yet have node extended.
  Node* node_ = nullptr;
};

// TODO(crem) Replace this with less hacky iterator once we support C++17.
// This class has multiple hypostases within one class:
// * Range (begin() and end() functions)
// * Iterator (operator++() and operator*())
// * Element, pointed by iterator (EdgeAndNode class mainly, but Edge_Iterator
//   is useful too when client wants to call GetOrSpawnNode).
//   It's safe to slice EdgeAndNode off Edge_Iterator.
// It's more customary to have those as three classes, but
// creating zoo of classes and copying them around while iterating seems
// excessive.
//
// All functions are not thread safe (must be externally synchronized), but
// it's fine if GetOrSpawnNode is called between calls to functions of the
// iterator (e.g. advancing the iterator). Other functions that manipulate
// child_ of parent or the sibling chain are not safe to call while iterating.
template <bool is_const>
class Edge_Iterator : public EdgeAndNode {
 public:
  using Ptr = std::conditional_t<is_const, const std::unique_ptr<Node>*,
                                 std::unique_ptr<Node>*>;

  // Creates "end()" iterator.
  Edge_Iterator() {}

  // Creates "begin()" iterator. Also happens to be a range constructor.
  // child_ptr will be nullptr if parent_node is solid children.
  Edge_Iterator(const Node& parent_node, Ptr child_ptr)
      : EdgeAndNode(parent_node.edges_.get(), nullptr),
        node_ptr_(child_ptr),
        total_count_(parent_node.num_edges_) {
    if (edge_ && child_ptr != nullptr) Actualize();
    if (edge_ && child_ptr == nullptr) {
      node_ = parent_node.child_.get();
    }
  }

  // Function to support range interface.
  Edge_Iterator<is_const> begin() { return *this; }
  Edge_Iterator<is_const> end() { return {}; }

  // Functions to support iterator interface.
  // Equality comparison operators are inherited from EdgeAndNode.
  void operator++() {
    // If it was the last edge in array, become end(), otherwise advance.
    if (++current_idx_ == total_count_) {
      edge_ = nullptr;
    } else {
      ++edge_;
      if (node_ptr_ != nullptr) {
        Actualize();
      } else {
        ++node_;
      }
    }
  }
  Edge_Iterator& operator*() { return *this; }

  // If there is node, return it. Otherwise spawn a new one and return it.
  Node* GetOrSpawnNode(Node* parent) {
    if (node_) return node_;  // If there is already a node, return it.
    // Should never reach here in solid mode.
    assert(node_ptr_ != nullptr);
    Actualize();              // But maybe other thread already did that.
    if (node_) return node_;  // If it did, return.
    // Now we are sure we have to create a new node.
    // Suppose there are nodes with idx 3 and 7, and we want to insert one with
    // idx 5. Here is how it looks like:
    //    node_ptr_ -> &Node(idx_.3).sibling_  ->  Node(idx_.7)
    // Here is how we do that:
    // 1. Store pointer to a node idx_.7:
    //    node_ptr_ -> &Node(idx_.3).sibling_  ->  nullptr
    //    tmp -> Node(idx_.7)
    std::unique_ptr<Node> tmp = std::move(*node_ptr_);
    // 2. Create fresh Node(idx_.5):
    //    node_ptr_ -> &Node(idx_.3).sibling_  ->  Node(idx_.5)
    //    tmp -> Node(idx_.7)
    *node_ptr_ = std::make_unique<Node>(parent, current_idx_);
    // 3. Attach stored pointer back to a list:
    //    node_ptr_ ->
    //         &Node(idx_.3).sibling_ -> Node(idx_.5).sibling_ -> Node(idx_.7)
    (*node_ptr_)->sibling_ = std::move(tmp);
    // 4. Actualize:
    //    node_ -> &Node(idx_.5)
    //    node_ptr_ -> &Node(idx_.5).sibling_ -> Node(idx_.7)
    Actualize();
    return node_;
  }

 private:
  void Actualize() {
    // This must never be called in solid mode.
    assert(node_ptr_ != nullptr);
    // If node_ptr_ is behind, advance it.
    // This is needed (and has to be 'while' rather than 'if') as other threads
    // could spawn new nodes between &node_ptr_ and *node_ptr_ while we didn't
    // see.
    while (*node_ptr_ && (*node_ptr_)->index_ < current_idx_) {
      node_ptr_ = &(*node_ptr_)->sibling_;
    }
    // If in the end node_ptr_ points to the node that we need, populate node_
    // and advance node_ptr_.
    if (*node_ptr_ && (*node_ptr_)->index_ == current_idx_) {
      node_ = (*node_ptr_).get();
      node_ptr_ = &node_->sibling_;
    } else {
      node_ = nullptr;
    }
  }

  // Pointer to a pointer to the next node. Has to be a pointer to pointer
  // as we'd like to update it when spawning a new node.
  Ptr node_ptr_;
  uint16_t current_idx_ = 0;
  uint16_t total_count_ = 0;
};

// TODO(crem) Replace this with less hacky iterator once we support C++17.
// This class has multiple hypostases within one class:
// * Range (begin() and end() functions)
// * Iterator (operator++() and operator*())
// It's more customary to have those as two classes, but
// creating zoo of classes and copying them around while iterating seems
// excessive.
//
// All functions are not thread safe (must be externally synchronized).
template <bool is_const>
class VisitedNode_Iterator {
 public:
  // Creates "end()" iterator.
  VisitedNode_Iterator() {}

  // Creates "begin()" iterator. Also happens to be a range constructor.
  // child_ptr will be nullptr if parent_node is solid children.
  VisitedNode_Iterator(const Node& parent_node, Node* child_ptr)
      : node_ptr_(child_ptr),
        total_count_(parent_node.num_edges_),
        solid_(parent_node.solid_children_) {
    if (node_ptr_ != nullptr && node_ptr_->GetN() == 0) {
      operator++();
    }
  }
  // These are technically wrong, but are usable to compare with end().
  bool operator==(const VisitedNode_Iterator<is_const>& other) const {
    return node_ptr_ == other.node_ptr_;
  }
  bool operator!=(const VisitedNode_Iterator<is_const>& other) const {
    return node_ptr_ != other.node_ptr_;
  }

  // Function to support range interface.
  VisitedNode_Iterator<is_const> begin() { return *this; }
  VisitedNode_Iterator<is_const> end() { return {}; }

  // Functions to support iterator interface.
  // Equality comparison operators are inherited from EdgeAndNode.
  void operator++() {
    if (solid_) {
      while (++current_idx_ != total_count_ &&
             node_ptr_[current_idx_].GetN() == 0) {
        if (node_ptr_[current_idx_].GetNInFlight() == 0) {
          // Once there is not even n in flight, we can skip to the end. This is
          // due to policy being in sorted order meaning that additional n in
          // flight are always selected from the front of the section with no n
          // in flight or visited.
          current_idx_ = total_count_;
          break;
        }
      }
      if (current_idx_ == total_count_) {
        node_ptr_ = nullptr;
      }
    } else {
      do {
        node_ptr_ = node_ptr_->sibling_.get();
        // If n started is 0, can jump direct to end due to sorted policy
        // ensuring that each time a new edge becomes best for the first time,
        // it is always the first of the section at the end that has NStarted of
        // 0.
        if (node_ptr_ != nullptr && node_ptr_->GetN() == 0 &&
            node_ptr_->GetNInFlight() == 0) {
          node_ptr_ = nullptr;
          break;
        }
      } while (node_ptr_ != nullptr && node_ptr_->GetN() == 0);
    }
  }
  Node* operator*() {
    if (solid_) {
      return &(node_ptr_[current_idx_]);
    } else {
      return node_ptr_;
    }
  }

 private:
  // Pointer to current node.
  Node* node_ptr_ = nullptr;
  uint16_t current_idx_ = 0;
  uint16_t total_count_ = 0;
  bool solid_ = false;
};

inline VisitedNode_Iterator<true> Node::VisitedNodes() const {
  return {*this, child_.get()};
}
inline VisitedNode_Iterator<false> Node::VisitedNodes() {
  return {*this, child_.get()};
}

class NodeTree {
 public:
  ~NodeTree() { DeallocateTree(); }
  // Adds a move to current_head_.
  void MakeMove(Move move);
  // Resets the current head to ensure it doesn't carry over details from a
  // previous search.
  void TrimTreeAtHead();
  // Sets the position in a tree, trying to reuse the tree.
  // If @auto_garbage_collect, old tree is garbage collected immediately. (may
  // take some milliseconds)
  // Returns whether a new position the same game as old position (with some
  // moves added). Returns false, if the position is completely different,
  // or if it's shorter than before.
  bool ResetToPosition(const std::string& starting_fen,
                       const std::vector<Move>& moves);
  const Position& HeadPosition() const { return history_.Last(); }
  int GetPlyCount() const { return HeadPosition().GetGamePly(); }
  bool IsBlackToMove() const { return HeadPosition().IsBlackToMove(); }
  Node* GetCurrentHead() const { return current_head_; }
  Node* GetGameBeginNode() const { return gamebegin_node_.get(); }
  const PositionHistory& GetPositionHistory() const { return history_; }

 private:
  void DeallocateTree();
  // A node which to start search from.
  Node* current_head_ = nullptr;
  // Root node of a game tree.
  std::unique_ptr<Node> gamebegin_node_;
  PositionHistory history_;
};

}  // namespace lczero

```

`src/mcts/params.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "mcts/params.h"

#include <algorithm>

#include "utils/exception.h"

#if __has_include("params_override.h")
#include "params_override.h"
#endif

#ifndef DEFAULT_MINIBATCH_SIZE
#define DEFAULT_MINIBATCH_SIZE 256
#endif
#ifndef DEFAULT_MAX_PREFETCH
#define DEFAULT_MAX_PREFETCH 32
#endif
#ifndef DEFAULT_TASK_WORKERS
#define DEFAULT_TASK_WORKERS 4
#endif

namespace lczero {

namespace {
FillEmptyHistory EncodeHistoryFill(std::string history_fill) {
  if (history_fill == "fen_only") return FillEmptyHistory::FEN_ONLY;
  if (history_fill == "always") return FillEmptyHistory::ALWAYS;
  assert(history_fill == "no");
  return FillEmptyHistory::NO;
}

}  // namespace

const OptionId SearchParams::kMiniBatchSizeId{
    "minibatch-size", "MinibatchSize",
    "How many positions the engine tries to batch together for parallel NN "
    "computation. Larger batches may reduce strength a bit, especially with a "
    "small number of playouts."};
const OptionId SearchParams::kMaxPrefetchBatchId{
    "max-prefetch", "MaxPrefetch",
    "When the engine cannot gather a large enough batch for immediate use, try "
    "to prefetch up to X positions which are likely to be useful soon, and put "
    "them into cache."};
const OptionId SearchParams::kCpuctId{
    "cpuct", "CPuct",
    "cpuct_init constant from \"UCT search\" algorithm. Higher values promote "
    "more exploration/wider search, lower values promote more "
    "confidence/deeper search."};
const OptionId SearchParams::kCpuctAtRootId{
    "cpuct-at-root", "CPuctAtRoot",
    "cpuct_init constant from \"UCT search\" algorithm, for root node."};
const OptionId SearchParams::kCpuctBaseId{
    "cpuct-base", "CPuctBase",
    "cpuct_base constant from \"UCT search\" algorithm. Lower value means "
    "higher growth of Cpuct as number of node visits grows."};
const OptionId SearchParams::kCpuctBaseAtRootId{
    "cpuct-base-at-root", "CPuctBaseAtRoot",
    "cpuct_base constant from \"UCT search\" algorithm, for root node."};
const OptionId SearchParams::kCpuctFactorId{
    "cpuct-factor", "CPuctFactor", "Multiplier for the cpuct growth formula."};
const OptionId SearchParams::kCpuctFactorAtRootId{
    "cpuct-factor-at-root", "CPuctFactorAtRoot",
    "Multiplier for the cpuct growth formula at root."};
// Remove this option after 0.25 has been made mandatory in training and the
// training server stops sending it.
const OptionId SearchParams::kRootHasOwnCpuctParamsId{
    "root-has-own-cpuct-params", "RootHasOwnCpuctParams",
    "If enabled, cpuct parameters for root node are taken from *AtRoot "
    "parameters. Otherwise, they are the same as for the rest of nodes. "
    "Temporary flag for transition to a new version."};
const OptionId SearchParams::kTwoFoldDrawsId{
    "two-fold-draws", "TwoFoldDraws",
    "Evaluates twofold repetitions in the search tree as draws. Visits to "
    "these positions are reverted when the first occurrence is played "
    "and not in the search tree anymore."};
const OptionId SearchParams::kTemperatureId{
    "temperature", "Temperature",
    "Tau value from softmax formula for the first move. If equal to 0, the "
    "engine picks the best move to make. Larger values increase randomness "
    "while making the move."};
const OptionId SearchParams::kTempDecayMovesId{
    "tempdecay-moves", "TempDecayMoves",
    "Reduce temperature for every move after the first move, decreasing "
    "linearly over this number of moves from initial temperature to 0. "
    "A value of 0 disables tempdecay."};
const OptionId SearchParams::kTempDecayDelayMovesId{
    "tempdecay-delay-moves", "TempDecayDelayMoves",
    "Delay the linear decrease of temperature by this number of moves, "
    "decreasing linearly from initial temperature to 0. A value of 0 starts "
    "tempdecay after the first move."};
const OptionId SearchParams::kTemperatureCutoffMoveId{
    "temp-cutoff-move", "TempCutoffMove",
    "Move number, starting from which endgame temperature is used rather "
    "than initial temperature. Setting it to 0 disables cutoff."};
const OptionId SearchParams::kTemperatureEndgameId{
    "temp-endgame", "TempEndgame",
    "Temperature used during endgame (starting from cutoff move). Endgame "
    "temperature doesn't decay."};
const OptionId SearchParams::kTemperatureWinpctCutoffId{
    "temp-value-cutoff", "TempValueCutoff",
    "When move is selected using temperature, bad moves (with win "
    "probability less than X than the best move) are not considered at all."};
const OptionId SearchParams::kTemperatureVisitOffsetId{
    "temp-visit-offset", "TempVisitOffset",
    "Adjusts visits by this value when picking a move with a temperature. If a "
    "negative offset reduces visits for a particular move below zero, that "
    "move is not picked. If no moves can be picked, no temperature is used."};
const OptionId SearchParams::kNoiseEpsilonId{
    "noise-epsilon", "DirichletNoiseEpsilon",
    "Amount of Dirichlet noise to combine with root priors. This allows the "
    "engine to discover new ideas during training by exploring moves which are "
    "known to be bad. Not normally used during play."};
const OptionId SearchParams::kNoiseAlphaId{
    "noise-alpha", "DirichletNoiseAlpha",
    "Alpha of Dirichlet noise to control the sharpness of move probabilities. "
    "Larger values result in flatter / more evenly distributed values."};
const OptionId SearchParams::kVerboseStatsId{
    "verbose-move-stats", "VerboseMoveStats",
    "Display Q, V, N, U and P values of every move candidate after each move.",
    'v'};
const OptionId SearchParams::kLogLiveStatsId{
    "log-live-stats", "LogLiveStats",
    "Do VerboseMoveStats on every info update."};
const OptionId SearchParams::kFpuStrategyId{
    "fpu-strategy", "FpuStrategy",
    "How is an eval of unvisited node determined. \"First Play Urgency\" "
    "changes search behavior to visit unvisited nodes earlier or later by "
    "using a placeholder eval before checking the network. The value specified "
    "with --fpu-value results in \"reduction\" subtracting that value from the "
    "parent eval while \"absolute\" directly uses that value."};
const OptionId SearchParams::kFpuValueId{
    "fpu-value", "FpuValue",
    "\"First Play Urgency\" value used to adjust unvisited node eval based on "
    "--fpu-strategy."};
const OptionId SearchParams::kFpuStrategyAtRootId{
    "fpu-strategy-at-root", "FpuStrategyAtRoot",
    "How is an eval of unvisited root children determined. Just like "
    "--fpu-strategy except only at the root level and adjusts unvisited root "
    "children eval with --fpu-value-at-root. In addition to matching the "
    "strategies from --fpu-strategy, this can be \"same\" to disable the "
    "special root behavior."};
const OptionId SearchParams::kFpuValueAtRootId{
    "fpu-value-at-root", "FpuValueAtRoot",
    "\"First Play Urgency\" value used to adjust unvisited root children eval "
    "based on --fpu-strategy-at-root. Has no effect if --fpu-strategy-at-root "
    "is \"same\"."};
const OptionId SearchParams::kCacheHistoryLengthId{
    "cache-history-length", "CacheHistoryLength",
    "Length of history, in half-moves, to include into the cache key. When "
    "this value is less than history that NN uses to eval a position, it's "
    "possble that the search will use eval of the same position with different "
    "history taken from cache."};
const OptionId SearchParams::kPolicySoftmaxTempId{
    "policy-softmax-temp", "PolicyTemperature",
    "Policy softmax temperature. Higher values make priors of move candidates "
    "closer to each other, widening the search."};
const OptionId SearchParams::kMaxCollisionVisitsId{
    "max-collision-visits", "MaxCollisionVisits",
    "Total allowed node collision visits, per batch."};
const OptionId SearchParams::kMaxCollisionEventsId{
    "max-collision-events", "MaxCollisionEvents",
    "Allowed node collision events, per batch."};
const OptionId SearchParams::kOutOfOrderEvalId{
    "out-of-order-eval", "OutOfOrderEval",
    "During the gathering of a batch for NN to eval, if position happens to be "
    "in the cache or is terminal, evaluate it right away without sending the "
    "batch to the NN. When off, this may only happen with the very first node "
    "of a batch; when on, this can happen with any node."};
const OptionId SearchParams::kMaxOutOfOrderEvalsId{
    "max-out-of-order-evals-factor", "MaxOutOfOrderEvalsFactor",
    "Maximum number of out of order evals during gathering of a batch is "
    "calculated by multiplying the maximum batch size by this number."};
const OptionId SearchParams::kStickyEndgamesId{
    "sticky-endgames", "StickyEndgames",
    "When an end of game position is found during search, allow the eval of "
    "the previous move's position to stick to something more accurate. For "
    "example, if at least one move results in checkmate, then the position "
    "should stick as checkmated. Similarly, if all moves are drawn or "
    "checkmated, the position should stick as drawn or checkmate."};
const OptionId SearchParams::kSyzygyFastPlayId{
    "syzygy-fast-play", "SyzygyFastPlay",
    "With DTZ tablebase files, only allow the network pick from winning moves "
    "that have shortest DTZ to play faster (but not necessarily optimally)."};
const OptionId SearchParams::kMultiPvId{
    "multipv", "MultiPV",
    "Number of game play lines (principal variations) to show in UCI info "
    "output."};
const OptionId SearchParams::kPerPvCountersId{
    "per-pv-counters", "PerPVCounters",
    "Show node counts per principal variation instead of total nodes in UCI."};
const OptionId SearchParams::kScoreTypeId{
    "score-type", "ScoreType",
    "What to display as score. Either centipawns (the UCI default), win "
    "percentage or Q (the actual internal score) multiplied by 100."};
const OptionId SearchParams::kHistoryFillId{
    "history-fill", "HistoryFill",
    "Neural network uses 7 previous board positions in addition to the current "
    "one. During the first moves of the game such historical positions don't "
    "exist, but they can be synthesized. This parameter defines when to "
    "synthesize them (always, never, or only at non-standard fen position)."};
const OptionId SearchParams::kMovesLeftMaxEffectId{
    "moves-left-max-effect", "MovesLeftMaxEffect",
    "Maximum bonus to add to the score of a node based on how much "
    "shorter/longer it makes the game when winning/losing."};
const OptionId SearchParams::kMovesLeftThresholdId{
    "moves-left-threshold", "MovesLeftThreshold",
    "Absolute value of node Q needs to exceed this value before shorter wins "
    "or longer losses are considered."};
const OptionId SearchParams::kMovesLeftSlopeId{
    "moves-left-slope", "MovesLeftSlope",
    "Controls how the bonus for shorter wins or longer losses is adjusted "
    "based on how many moves the move is estimated to shorten/lengthen the "
    "game. The move difference is multiplied with the slope and capped at "
    "MovesLeftMaxEffect."};
const OptionId SearchParams::kMovesLeftConstantFactorId{
    "moves-left-constant-factor", "MovesLeftConstantFactor",
    "A simple multiplier to the moves left effect, can be set to 0 to only use "
    "an effect scaled by Q."};
const OptionId SearchParams::kMovesLeftScaledFactorId{
    "moves-left-scaled-factor", "MovesLeftScaledFactor",
    "A factor which is multiplied by the absolute Q of parent node and the "
    "base moves left effect."};
const OptionId SearchParams::kMovesLeftQuadraticFactorId{
    "moves-left-quadratic-factor", "MovesLeftQuadraticFactor",
    "A factor which is multiplied by the square of Q of parent node and the "
    "base moves left effect."};
const OptionId SearchParams::kDisplayCacheUsageId{
    "display-cache-usage", "DisplayCacheUsage",
    "Display cache fullness through UCI info `hash` section."};
const OptionId SearchParams::kMaxConcurrentSearchersId{
    "max-concurrent-searchers", "MaxConcurrentSearchers",
    "If not 0, at most this many search workers can be gathering minibatches "
    "at once."};
const OptionId SearchParams::kDrawScoreSidetomoveId{
    "draw-score-sidetomove", "DrawScoreSideToMove",
    "Score of a drawn game, as seen by a player making the move."};
const OptionId SearchParams::kDrawScoreOpponentId{
    "draw-score-opponent", "DrawScoreOpponent",
    "Score of a drawn game, as seen by the opponent."};
const OptionId SearchParams::kDrawScoreWhiteId{
    "draw-score-white", "DrawScoreWhite",
    "Adjustment, added to a draw score of a white player."};
const OptionId SearchParams::kDrawScoreBlackId{
    "draw-score-black", "DrawScoreBlack",
    "Adjustment, added to a draw score of a black player."};
const OptionId SearchParams::kNpsLimitId{
    "nps-limit", "NodesPerSecondLimit",
    "An option to specify an upper limit to the nodes per second searched. The "
    "accuracy depends on the minibatch size used, increasing for lower sizes, "
    "and on the length of the search. Zero to disable."};
const OptionId SearchParams::kSolidTreeThresholdId{
    "solid-tree-threshold", "SolidTreeThreshold",
    "Only nodes with at least this number of visits will be considered for "
    "solidification for improved cache locality."};
const OptionId SearchParams::kTaskWorkersPerSearchWorkerId{
    "task-workers", "TaskWorkers",
    "The number of task workers to use to help the search worker."};
const OptionId SearchParams::kMinimumWorkSizeForProcessingId{
    "minimum-processing-work", "MinimumProcessingWork",
    "This many visits need to be gathered before tasks will be used to "
    "accelerate processing."};
const OptionId SearchParams::kMinimumWorkSizeForPickingId{
    "minimum-picking-work", "MinimumPickingWork",
    "Search branches with more than this many collisions/visits may be split "
    "off to task workers."};
const OptionId SearchParams::kMinimumRemainingWorkSizeForPickingId{
    "minimum-remaining-picking-work", "MinimumRemainingPickingWork",
    "Search branches won't be split off to task workers unless there is at "
    "least this much work left to do afterwards."};
const OptionId SearchParams::kMinimumWorkPerTaskForProcessingId{
    "minimum-per-task-processing", "MinimumPerTaskProcessing",
    "Processing work won't be split into chunks smaller than this (unless its "
    "more than half of MinimumProcessingWork)."};
const OptionId SearchParams::kIdlingMinimumWorkId{
    "idling-minimum-work", "IdlingMinimumWork",
    "Only early exit gathering due to 'idle' backend if more than this many "
    "nodes will be sent to the backend."};
const OptionId SearchParams::kThreadIdlingThresholdId{
    "thread-idling-threshold", "ThreadIdlingThreshold",
    "If there are more than this number of search threads that are not "
    "actively in the process of either sending data to the backend or waiting "
    "for data from the backend, assume that the backend is idle."};
const OptionId SearchParams::kMaxCollisionVisitsScalingStartId{
    "max-collision-visits-scaling-start", "MaxCollisionVisitsScalingStart",
    "Tree size where max collision visits starts scaling up from 1."};
const OptionId SearchParams::kMaxCollisionVisitsScalingEndId{
    "max-collision-visits-scaling-end", "MaxCollisionVisitsScalingEnd",
    "Tree size where max collision visits reaches max. Set to 0 to disable "
    "scaling entirely."};
const OptionId SearchParams::kMaxCollisionVisitsScalingPowerId{
    "max-collision-visits-scaling-power", "MaxCollisionVisitsScalingPower",
    "Power to apply to the interpolation between 1 and max to make it curved."};

void SearchParams::Populate(OptionsParser* options) {
  // Here the uci optimized defaults" are set.
  // Many of them are overridden with training specific values in tournament.cc.
  options->Add<IntOption>(kMiniBatchSizeId, 1, 1024) = DEFAULT_MINIBATCH_SIZE;
  options->Add<IntOption>(kMaxPrefetchBatchId, 0, 1024) = DEFAULT_MAX_PREFETCH;
  options->Add<FloatOption>(kCpuctId, 0.0f, 100.0f) = 1.745f;
  options->Add<FloatOption>(kCpuctAtRootId, 0.0f, 100.0f) = 1.745f;
  options->Add<FloatOption>(kCpuctBaseId, 1.0f, 1000000000.0f) = 38739.0f;
  options->Add<FloatOption>(kCpuctBaseAtRootId, 1.0f, 1000000000.0f) = 38739.0f;
  options->Add<FloatOption>(kCpuctFactorId, 0.0f, 1000.0f) = 3.894f;
  options->Add<FloatOption>(kCpuctFactorAtRootId, 0.0f, 1000.0f) = 3.894f;
  options->Add<BoolOption>(kRootHasOwnCpuctParamsId) = false;
  options->Add<BoolOption>(kTwoFoldDrawsId) = true;
  options->Add<FloatOption>(kTemperatureId, 0.0f, 100.0f) = 0.0f;
  options->Add<IntOption>(kTempDecayMovesId, 0, 640) = 0;
  options->Add<IntOption>(kTempDecayDelayMovesId, 0, 100) = 0;
  options->Add<IntOption>(kTemperatureCutoffMoveId, 0, 1000) = 0;
  options->Add<FloatOption>(kTemperatureEndgameId, 0.0f, 100.0f) = 0.0f;
  options->Add<FloatOption>(kTemperatureWinpctCutoffId, 0.0f, 100.0f) = 100.0f;
  options->Add<FloatOption>(kTemperatureVisitOffsetId, -1000.0f, 1000.0f) =
      0.0f;
  options->Add<FloatOption>(kNoiseEpsilonId, 0.0f, 1.0f) = 0.0f;
  options->Add<FloatOption>(kNoiseAlphaId, 0.0f, 10000000.0f) = 0.3f;
  options->Add<BoolOption>(kVerboseStatsId) = false;
  options->Add<BoolOption>(kLogLiveStatsId) = false;
  std::vector<std::string> fpu_strategy = {"reduction", "absolute"};
  options->Add<ChoiceOption>(kFpuStrategyId, fpu_strategy) = "reduction";
  options->Add<FloatOption>(kFpuValueId, -100.0f, 100.0f) = 0.330f;
  fpu_strategy.push_back("same");
  options->Add<ChoiceOption>(kFpuStrategyAtRootId, fpu_strategy) = "same";
  options->Add<FloatOption>(kFpuValueAtRootId, -100.0f, 100.0f) = 1.0f;
  options->Add<IntOption>(kCacheHistoryLengthId, 0, 7) = 0;
  options->Add<FloatOption>(kPolicySoftmaxTempId, 0.1f, 10.0f) = 1.359f;
  options->Add<IntOption>(kMaxCollisionEventsId, 1, 65536) = 917;
  options->Add<IntOption>(kMaxCollisionVisitsId, 1, 100000000) = 80000;
  options->Add<IntOption>(kMaxCollisionVisitsScalingStartId, 1, 100000) = 28;
  options->Add<IntOption>(kMaxCollisionVisitsScalingEndId, 0, 100000000) =
      145000;
  options->Add<FloatOption>(kMaxCollisionVisitsScalingPowerId, 0.01, 100) =
      1.25;
  options->Add<BoolOption>(kOutOfOrderEvalId) = true;
  options->Add<FloatOption>(kMaxOutOfOrderEvalsId, 0.0f, 100.0f) = 2.4f;
  options->Add<BoolOption>(kStickyEndgamesId) = true;
  options->Add<BoolOption>(kSyzygyFastPlayId) = false;
  options->Add<IntOption>(kMultiPvId, 1, 500) = 1;
  options->Add<BoolOption>(kPerPvCountersId) = false;
  std::vector<std::string> score_type = {"centipawn",
                                         "centipawn_with_drawscore",
                                         "centipawn_2019",
                                         "centipawn_2018",
                                         "win_percentage",
                                         "Q",
                                         "W-L"};
  options->Add<ChoiceOption>(kScoreTypeId, score_type) = "centipawn";
  std::vector<std::string> history_fill_opt{"no", "fen_only", "always"};
  options->Add<ChoiceOption>(kHistoryFillId, history_fill_opt) = "fen_only";
  options->Add<FloatOption>(kMovesLeftMaxEffectId, 0.0f, 1.0f) = 0.0345f;
  options->Add<FloatOption>(kMovesLeftThresholdId, 0.0f, 1.0f) = 0.0f;
  options->Add<FloatOption>(kMovesLeftSlopeId, 0.0f, 1.0f) = 0.0027f;
  options->Add<FloatOption>(kMovesLeftConstantFactorId, -1.0f, 1.0f) = 0.0f;
  options->Add<FloatOption>(kMovesLeftScaledFactorId, -2.0f, 2.0f) = 1.6521f;
  options->Add<FloatOption>(kMovesLeftQuadraticFactorId, -1.0f, 1.0f) =
      -0.6521f;
  options->Add<BoolOption>(kDisplayCacheUsageId) = false;
  options->Add<IntOption>(kMaxConcurrentSearchersId, 0, 128) = 1;
  options->Add<IntOption>(kDrawScoreSidetomoveId, -100, 100) = 0;
  options->Add<IntOption>(kDrawScoreOpponentId, -100, 100) = 0;
  options->Add<IntOption>(kDrawScoreWhiteId, -100, 100) = 0;
  options->Add<IntOption>(kDrawScoreBlackId, -100, 100) = 0;
  options->Add<FloatOption>(kNpsLimitId, 0.0f, 1e6f) = 0.0f;
  options->Add<IntOption>(kSolidTreeThresholdId, 1, 2000000000) = 100;
  options->Add<IntOption>(kTaskWorkersPerSearchWorkerId, 0, 128) =
      DEFAULT_TASK_WORKERS;
  options->Add<IntOption>(kMinimumWorkSizeForProcessingId, 2, 100000) = 20;
  options->Add<IntOption>(kMinimumWorkSizeForPickingId, 1, 100000) = 1;
  options->Add<IntOption>(kMinimumRemainingWorkSizeForPickingId, 0, 100000) =
      20;
  options->Add<IntOption>(kMinimumWorkPerTaskForProcessingId, 1, 100000) = 8;
  options->Add<IntOption>(kIdlingMinimumWorkId, 0, 10000) = 0;
  options->Add<IntOption>(kThreadIdlingThresholdId, 0, 128) = 1;

  options->HideOption(kNoiseEpsilonId);
  options->HideOption(kNoiseAlphaId);
  options->HideOption(kLogLiveStatsId);
  options->HideOption(kDisplayCacheUsageId);
  options->HideOption(kRootHasOwnCpuctParamsId);
  options->HideOption(kCpuctAtRootId);
  options->HideOption(kCpuctBaseAtRootId);
  options->HideOption(kCpuctFactorAtRootId);
  options->HideOption(kFpuStrategyAtRootId);
  options->HideOption(kFpuValueAtRootId);
  options->HideOption(kTemperatureId);
  options->HideOption(kTempDecayMovesId);
  options->HideOption(kTempDecayDelayMovesId);
  options->HideOption(kTemperatureCutoffMoveId);
  options->HideOption(kTemperatureEndgameId);
  options->HideOption(kTemperatureWinpctCutoffId);
  options->HideOption(kTemperatureVisitOffsetId);
}

SearchParams::SearchParams(const OptionsDict& options)
    : options_(options),
      kCpuct(options.Get<float>(kCpuctId)),
      kCpuctAtRoot(options.Get<float>(
          options.Get<bool>(kRootHasOwnCpuctParamsId) ? kCpuctAtRootId
                                                      : kCpuctId)),
      kCpuctBase(options.Get<float>(kCpuctBaseId)),
      kCpuctBaseAtRoot(options.Get<float>(
          options.Get<bool>(kRootHasOwnCpuctParamsId) ? kCpuctBaseAtRootId
                                                      : kCpuctBaseId)),
      kCpuctFactor(options.Get<float>(kCpuctFactorId)),
      kCpuctFactorAtRoot(options.Get<float>(
          options.Get<bool>(kRootHasOwnCpuctParamsId) ? kCpuctFactorAtRootId
                                                      : kCpuctFactorId)),
      kTwoFoldDraws(options.Get<bool>(kTwoFoldDrawsId)),
      kNoiseEpsilon(options.Get<float>(kNoiseEpsilonId)),
      kNoiseAlpha(options.Get<float>(kNoiseAlphaId)),
      kFpuAbsolute(options.Get<std::string>(kFpuStrategyId) == "absolute"),
      kFpuValue(options.Get<float>(kFpuValueId)),
      kFpuAbsoluteAtRoot(
          (options.Get<std::string>(kFpuStrategyAtRootId) == "same" &&
           kFpuAbsolute) ||
          options.Get<std::string>(kFpuStrategyAtRootId) == "absolute"),
      kFpuValueAtRoot(options.Get<std::string>(kFpuStrategyAtRootId) == "same"
                          ? kFpuValue
                          : options.Get<float>(kFpuValueAtRootId)),
      kCacheHistoryLength(options.Get<int>(kCacheHistoryLengthId)),
      kPolicySoftmaxTemp(options.Get<float>(kPolicySoftmaxTempId)),
      kMaxCollisionEvents(options.Get<int>(kMaxCollisionEventsId)),
      kMaxCollisionVisits(options.Get<int>(kMaxCollisionVisitsId)),
      kOutOfOrderEval(options.Get<bool>(kOutOfOrderEvalId)),
      kStickyEndgames(options.Get<bool>(kStickyEndgamesId)),
      kSyzygyFastPlay(options.Get<bool>(kSyzygyFastPlayId)),
      kHistoryFill(EncodeHistoryFill(options.Get<std::string>(kHistoryFillId))),
      kMiniBatchSize(options.Get<int>(kMiniBatchSizeId)),
      kMovesLeftMaxEffect(options.Get<float>(kMovesLeftMaxEffectId)),
      kMovesLeftThreshold(options.Get<float>(kMovesLeftThresholdId)),
      kMovesLeftSlope(options.Get<float>(kMovesLeftSlopeId)),
      kMovesLeftConstantFactor(options.Get<float>(kMovesLeftConstantFactorId)),
      kMovesLeftScaledFactor(options.Get<float>(kMovesLeftScaledFactorId)),
      kMovesLeftQuadraticFactor(
          options.Get<float>(kMovesLeftQuadraticFactorId)),
      kDisplayCacheUsage(options.Get<bool>(kDisplayCacheUsageId)),
      kMaxConcurrentSearchers(options.Get<int>(kMaxConcurrentSearchersId)),
      kDrawScoreSidetomove{options.Get<int>(kDrawScoreSidetomoveId) / 100.0f},
      kDrawScoreOpponent{options.Get<int>(kDrawScoreOpponentId) / 100.0f},
      kDrawScoreWhite{options.Get<int>(kDrawScoreWhiteId) / 100.0f},
      kDrawScoreBlack{options.Get<int>(kDrawScoreBlackId) / 100.0f},
      kMaxOutOfOrderEvals(std::max(
          1, static_cast<int>(options.Get<float>(kMaxOutOfOrderEvalsId) *
                              options.Get<int>(kMiniBatchSizeId)))),
      kNpsLimit(options.Get<float>(kNpsLimitId)),
      kSolidTreeThreshold(options.Get<int>(kSolidTreeThresholdId)),
      kTaskWorkersPerSearchWorker(options.Get<int>(kTaskWorkersPerSearchWorkerId)),
      kMinimumWorkSizeForProcessing(
          options.Get<int>(kMinimumWorkSizeForProcessingId)),
      kMinimumWorkSizeForPicking(
          options.Get<int>(kMinimumWorkSizeForPickingId)),
      kMinimumRemainingWorkSizeForPicking(
          options.Get<int>(kMinimumRemainingWorkSizeForPickingId)),
      kMinimumWorkPerTaskForProcessing(
          options.Get<int>(kMinimumWorkPerTaskForProcessingId)),
      kIdlingMinimumWork(options.Get<int>(kIdlingMinimumWorkId)),
      kThreadIdlingThreshold(options.Get<int>(kThreadIdlingThresholdId)),
      kMaxCollisionVisitsScalingStart(
          options.Get<int>(kMaxCollisionVisitsScalingStartId)),
      kMaxCollisionVisitsScalingEnd(
          options.Get<int>(kMaxCollisionVisitsScalingEndId)),
      kMaxCollisionVisitsScalingPower(
          options.Get<float>(kMaxCollisionVisitsScalingPowerId)) {
  if (std::max(std::abs(kDrawScoreSidetomove), std::abs(kDrawScoreOpponent)) +
          std::max(std::abs(kDrawScoreWhite), std::abs(kDrawScoreBlack)) >
      1.0f) {
    throw Exception(
        "max{|sidetomove|+|opponent|} + max{|white|+|black|} draw score must "
        "be <= 100");
  }
}

}  // namespace lczero

```

`src/mcts/params.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include "neural/encoder.h"
#include "utils/optionsdict.h"
#include "utils/optionsparser.h"

namespace lczero {

class SearchParams {
 public:
  SearchParams(const OptionsDict& options);
  SearchParams(const SearchParams&) = delete;

  // Populates UciOptions with search parameters.
  static void Populate(OptionsParser* options);

  // Parameter getters.
  int GetMiniBatchSize() const { return kMiniBatchSize; }
  int GetMaxPrefetchBatch() const {
    return options_.Get<int>(kMaxPrefetchBatchId);
  }
  float GetCpuct(bool at_root) const { return at_root ? kCpuctAtRoot : kCpuct; }
  float GetCpuctBase(bool at_root) const {
    return at_root ? kCpuctBaseAtRoot : kCpuctBase;
  }
  float GetCpuctFactor(bool at_root) const {
    return at_root ? kCpuctFactorAtRoot : kCpuctFactor;
  }
  bool GetTwoFoldDraws() const { return kTwoFoldDraws; }
  float GetTemperature() const { return options_.Get<float>(kTemperatureId); }
  float GetTemperatureVisitOffset() const {
    return options_.Get<float>(kTemperatureVisitOffsetId);
  }
  int GetTempDecayMoves() const { return options_.Get<int>(kTempDecayMovesId); }
  int GetTempDecayDelayMoves() const {
    return options_.Get<int>(kTempDecayDelayMovesId);
  }
  int GetTemperatureCutoffMove() const {
    return options_.Get<int>(kTemperatureCutoffMoveId);
  }
  float GetTemperatureEndgame() const {
    return options_.Get<float>(kTemperatureEndgameId);
  }
  float GetTemperatureWinpctCutoff() const {
    return options_.Get<float>(kTemperatureWinpctCutoffId);
  }
  float GetNoiseEpsilon() const { return kNoiseEpsilon; }
  float GetNoiseAlpha() const { return kNoiseAlpha; }
  bool GetVerboseStats() const { return options_.Get<bool>(kVerboseStatsId); }
  bool GetLogLiveStats() const { return options_.Get<bool>(kLogLiveStatsId); }
  bool GetFpuAbsolute(bool at_root) const {
    return at_root ? kFpuAbsoluteAtRoot : kFpuAbsolute;
  }
  float GetFpuValue(bool at_root) const {
    return at_root ? kFpuValueAtRoot : kFpuValue;
  }
  int GetCacheHistoryLength() const { return kCacheHistoryLength; }
  float GetPolicySoftmaxTemp() const { return kPolicySoftmaxTemp; }
  int GetMaxCollisionEvents() const { return kMaxCollisionEvents; }
  int GetMaxCollisionVisits() const { return kMaxCollisionVisits; }
  bool GetOutOfOrderEval() const { return kOutOfOrderEval; }
  bool GetStickyEndgames() const { return kStickyEndgames; }
  bool GetSyzygyFastPlay() const { return kSyzygyFastPlay; }
  int GetMultiPv() const { return options_.Get<int>(kMultiPvId); }
  bool GetPerPvCounters() const { return options_.Get<bool>(kPerPvCountersId); }
  std::string GetScoreType() const {
    return options_.Get<std::string>(kScoreTypeId);
  }
  FillEmptyHistory GetHistoryFill() const { return kHistoryFill; }
  float GetMovesLeftMaxEffect() const { return kMovesLeftMaxEffect; }
  float GetMovesLeftThreshold() const { return kMovesLeftThreshold; }
  float GetMovesLeftSlope() const { return kMovesLeftSlope; }
  float GetMovesLeftConstantFactor() const { return kMovesLeftConstantFactor; }
  float GetMovesLeftScaledFactor() const { return kMovesLeftScaledFactor; }
  float GetMovesLeftQuadraticFactor() const {
    return kMovesLeftQuadraticFactor;
  }
  bool GetDisplayCacheUsage() const { return kDisplayCacheUsage; }
  int GetMaxConcurrentSearchers() const { return kMaxConcurrentSearchers; }
  float GetSidetomoveDrawScore() const { return kDrawScoreSidetomove; }
  float GetOpponentDrawScore() const { return kDrawScoreOpponent; }
  float GetWhiteDrawDelta() const { return kDrawScoreWhite; }
  float GetBlackDrawDelta() const { return kDrawScoreBlack; }
  int GetMaxOutOfOrderEvals() const { return kMaxOutOfOrderEvals; }
  float GetNpsLimit() const { return kNpsLimit; }
  int GetSolidTreeThreshold() const { return kSolidTreeThreshold; }

  int GetTaskWorkersPerSearchWorker() const {
    return kTaskWorkersPerSearchWorker;
  }
  int GetMinimumWorkSizeForProcessing() const {
    return kMinimumWorkSizeForProcessing;
  }
  int GetMinimumWorkSizeForPicking() const {
    return kMinimumWorkSizeForPicking;
  }
  int GetMinimumRemainingWorkSizeForPicking() const {
    return kMinimumRemainingWorkSizeForPicking;
  }
  int GetMinimumWorkPerTaskForProcessing() const {
    return kMinimumWorkPerTaskForProcessing;
  }
  int GetIdlingMinimumWork() const { return kIdlingMinimumWork; }
  int GetThreadIdlingThreshold() const { return kThreadIdlingThreshold; }
  int GetMaxCollisionVisitsScalingStart() const {
    return kMaxCollisionVisitsScalingStart;
  }
  int GetMaxCollisionVisitsScalingEnd() const {
    return kMaxCollisionVisitsScalingEnd;
  }
  float GetMaxCollisionVisitsScalingPower() const {
    return kMaxCollisionVisitsScalingPower;
  }

  // Search parameter IDs.
  static const OptionId kMiniBatchSizeId;
  static const OptionId kMaxPrefetchBatchId;
  static const OptionId kCpuctId;
  static const OptionId kCpuctAtRootId;
  static const OptionId kCpuctBaseId;
  static const OptionId kCpuctBaseAtRootId;
  static const OptionId kCpuctFactorId;
  static const OptionId kCpuctFactorAtRootId;
  static const OptionId kRootHasOwnCpuctParamsId;
  static const OptionId kTwoFoldDrawsId;
  static const OptionId kTemperatureId;
  static const OptionId kTempDecayMovesId;
  static const OptionId kTempDecayDelayMovesId;
  static const OptionId kTemperatureCutoffMoveId;
  static const OptionId kTemperatureEndgameId;
  static const OptionId kTemperatureWinpctCutoffId;
  static const OptionId kTemperatureVisitOffsetId;
  static const OptionId kNoiseEpsilonId;
  static const OptionId kNoiseAlphaId;
  static const OptionId kVerboseStatsId;
  static const OptionId kLogLiveStatsId;
  static const OptionId kFpuStrategyId;
  static const OptionId kFpuValueId;
  static const OptionId kFpuStrategyAtRootId;
  static const OptionId kFpuValueAtRootId;
  static const OptionId kCacheHistoryLengthId;
  static const OptionId kPolicySoftmaxTempId;
  static const OptionId kMaxCollisionEventsId;
  static const OptionId kMaxCollisionVisitsId;
  static const OptionId kOutOfOrderEvalId;
  static const OptionId kStickyEndgamesId;
  static const OptionId kSyzygyFastPlayId;
  static const OptionId kMultiPvId;
  static const OptionId kPerPvCountersId;
  static const OptionId kScoreTypeId;
  static const OptionId kHistoryFillId;
  static const OptionId kMovesLeftMaxEffectId;
  static const OptionId kMovesLeftThresholdId;
  static const OptionId kMovesLeftConstantFactorId;
  static const OptionId kMovesLeftScaledFactorId;
  static const OptionId kMovesLeftQuadraticFactorId;
  static const OptionId kMovesLeftSlopeId;
  static const OptionId kDisplayCacheUsageId;
  static const OptionId kMaxConcurrentSearchersId;
  static const OptionId kDrawScoreSidetomoveId;
  static const OptionId kDrawScoreOpponentId;
  static const OptionId kDrawScoreWhiteId;
  static const OptionId kDrawScoreBlackId;
  static const OptionId kMaxOutOfOrderEvalsId;
  static const OptionId kNpsLimitId;
  static const OptionId kSolidTreeThresholdId;
  static const OptionId kTaskWorkersPerSearchWorkerId;
  static const OptionId kMinimumWorkSizeForProcessingId;
  static const OptionId kMinimumWorkSizeForPickingId;
  static const OptionId kMinimumRemainingWorkSizeForPickingId;
  static const OptionId kMinimumWorkPerTaskForProcessingId;
  static const OptionId kIdlingMinimumWorkId;
  static const OptionId kThreadIdlingThresholdId;
  static const OptionId kMaxCollisionVisitsScalingStartId;
  static const OptionId kMaxCollisionVisitsScalingEndId;
  static const OptionId kMaxCollisionVisitsScalingPowerId;

 private:
  const OptionsDict& options_;
  // Cached parameter values. Values have to be cached if either:
  // 1. Parameter is accessed often and has to be cached for performance
  // reasons.
  // 2. Parameter has to stay the say during the search.
  // TODO(crem) Some of those parameters can be converted to be dynamic after
  //            trivial search optimizations.
  const float kCpuct;
  const float kCpuctAtRoot;
  const float kCpuctBase;
  const float kCpuctBaseAtRoot;
  const float kCpuctFactor;
  const float kCpuctFactorAtRoot;
  const bool kTwoFoldDraws;
  const float kNoiseEpsilon;
  const float kNoiseAlpha;
  const bool kFpuAbsolute;
  const float kFpuValue;
  const bool kFpuAbsoluteAtRoot;
  const float kFpuValueAtRoot;
  const int kCacheHistoryLength;
  const float kPolicySoftmaxTemp;
  const int kMaxCollisionEvents;
  const int kMaxCollisionVisits;
  const bool kOutOfOrderEval;
  const bool kStickyEndgames;
  const bool kSyzygyFastPlay;
  const FillEmptyHistory kHistoryFill;
  const int kMiniBatchSize;
  const float kMovesLeftMaxEffect;
  const float kMovesLeftThreshold;
  const float kMovesLeftSlope;
  const float kMovesLeftConstantFactor;
  const float kMovesLeftScaledFactor;
  const float kMovesLeftQuadraticFactor;
  const bool kDisplayCacheUsage;
  const int kMaxConcurrentSearchers;
  const float kDrawScoreSidetomove;
  const float kDrawScoreOpponent;
  const float kDrawScoreWhite;
  const float kDrawScoreBlack;
  const int kMaxOutOfOrderEvals;
  const float kNpsLimit;
  const int kSolidTreeThreshold;
  const int kTaskWorkersPerSearchWorker;
  const int kMinimumWorkSizeForProcessing;
  const int kMinimumWorkSizeForPicking;
  const int kMinimumRemainingWorkSizeForPicking;
  const int kMinimumWorkPerTaskForProcessing;
  const int kIdlingMinimumWork;
  const int kThreadIdlingThreshold;
  const int kMaxCollisionVisitsScalingStart;
  const int kMaxCollisionVisitsScalingEnd;
  const float kMaxCollisionVisitsScalingPower;
};

}  // namespace lczero

```

`src/mcts/search.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "mcts/search.h"

#include <algorithm>
#include <array>
#include <chrono>
#include <cmath>
#include <iomanip>
#include <iostream>
#include <iterator>
#include <sstream>
#include <thread>

#include "mcts/node.h"
#include "neural/cache.h"
#include "neural/encoder.h"
#include "utils/fastmath.h"
#include "utils/random.h"

namespace lczero {

namespace {
// Maximum delay between outputting "uci info" when nothing interesting happens.
const int kUciInfoMinimumFrequencyMs = 5000;

MoveList MakeRootMoveFilter(const MoveList& searchmoves,
                            SyzygyTablebase* syzygy_tb,
                            const PositionHistory& history, bool fast_play,
                            std::atomic<int>* tb_hits, bool* dtz_success) {
  assert(tb_hits);
  assert(dtz_success);
  // Search moves overrides tablebase.
  if (!searchmoves.empty()) return searchmoves;
  const auto& board = history.Last().GetBoard();
  MoveList root_moves;
  if (!syzygy_tb || !board.castlings().no_legal_castle() ||
      (board.ours() | board.theirs()).count() > syzygy_tb->max_cardinality()) {
    return root_moves;
  }
  if (syzygy_tb->root_probe(
          history.Last(), fast_play || history.DidRepeatSinceLastZeroingMove(),
          &root_moves)) {
    *dtz_success = true;
    tb_hits->fetch_add(1, std::memory_order_acq_rel);
  } else if (syzygy_tb->root_probe_wdl(history.Last(), &root_moves)) {
    tb_hits->fetch_add(1, std::memory_order_acq_rel);
  }
  return root_moves;
}

class MEvaluator {
 public:
  MEvaluator()
      : enabled_{false},
        m_slope_{0.0f},
        m_cap_{0.0f},
        a_constant_{0.0f},
        a_linear_{0.0f},
        a_square_{0.0f},
        q_threshold_{0.0f},
        parent_m_{0.0f} {}

  MEvaluator(const SearchParams& params, const Node* parent = nullptr)
      : enabled_{true},
        m_slope_{params.GetMovesLeftSlope()},
        m_cap_{params.GetMovesLeftMaxEffect()},
        a_constant_{params.GetMovesLeftConstantFactor()},
        a_linear_{params.GetMovesLeftScaledFactor()},
        a_square_{params.GetMovesLeftQuadraticFactor()},
        q_threshold_{params.GetMovesLeftThreshold()},
        parent_m_{parent ? parent->GetM() : 0.0f},
        parent_within_threshold_{parent ? WithinThreshold(parent, q_threshold_)
                                        : false} {}

  void SetParent(const Node* parent) {
    assert(parent);
    if (enabled_) {
      parent_m_ = parent->GetM();
      parent_within_threshold_ = WithinThreshold(parent, q_threshold_);
    }
  }

  float GetM(const EdgeAndNode& child, float q) const {
    if (!enabled_ || !parent_within_threshold_) return 0.0f;
    const float child_m = child.GetM(parent_m_);
    float m = std::clamp(m_slope_ * (child_m - parent_m_), -m_cap_, m_cap_);
    m *= FastSign(-q);
    m *= a_constant_ + a_linear_ * std::abs(q) + a_square_ * q * q;
    return m;
  }

  float GetM(Node* child, float q) const {
    if (!enabled_ || !parent_within_threshold_) return 0.0f;
    const float child_m = child->GetM();
    float m = std::clamp(m_slope_ * (child_m - parent_m_), -m_cap_, m_cap_);
    m *= FastSign(-q);
    m *= a_constant_ + a_linear_ * std::abs(q) + a_square_ * q * q;
    return m;
  }

  // The M utility to use for unvisited nodes.
  float GetDefaultM() const { return 0.0f; }

 private:
  static bool WithinThreshold(const Node* parent, float q_threshold) {
    return std::abs(parent->GetQ(0.0f)) > q_threshold;
  }

  const bool enabled_;
  const float m_slope_;
  const float m_cap_;
  const float a_constant_;
  const float a_linear_;
  const float a_square_;
  const float q_threshold_;
  float parent_m_ = 0.0f;
  bool parent_within_threshold_ = false;
};

}  // namespace

Search::Search(const NodeTree& tree, Network* network,
               std::unique_ptr<UciResponder> uci_responder,
               const MoveList& searchmoves,
               std::chrono::steady_clock::time_point start_time,
               std::unique_ptr<SearchStopper> stopper, bool infinite,
               const OptionsDict& options, NNCache* cache,
               SyzygyTablebase* syzygy_tb)
    : ok_to_respond_bestmove_(!infinite),
      stopper_(std::move(stopper)),
      root_node_(tree.GetCurrentHead()),
      cache_(cache),
      syzygy_tb_(syzygy_tb),
      played_history_(tree.GetPositionHistory()),
      network_(network),
      params_(options),
      searchmoves_(searchmoves),
      start_time_(start_time),
      initial_visits_(root_node_->GetN()),
      root_move_filter_(MakeRootMoveFilter(
          searchmoves_, syzygy_tb_, played_history_,
          params_.GetSyzygyFastPlay(), &tb_hits_, &root_is_in_dtz_)),
      uci_responder_(std::move(uci_responder)) {
  if (params_.GetMaxConcurrentSearchers() != 0) {
    pending_searchers_.store(params_.GetMaxConcurrentSearchers(),
                             std::memory_order_release);
  }
}

namespace {
void ApplyDirichletNoise(Node* node, float eps, double alpha) {
  float total = 0;
  std::vector<float> noise;

  for (int i = 0; i < node->GetNumEdges(); ++i) {
    float eta = Random::Get().GetGamma(alpha, 1.0);
    noise.emplace_back(eta);
    total += eta;
  }

  if (total < std::numeric_limits<float>::min()) return;

  int noise_idx = 0;
  for (const auto& child : node->Edges()) {
    auto* edge = child.edge();
    edge->SetP(edge->GetP() * (1 - eps) + eps * noise[noise_idx++] / total);
  }
}
}  // namespace

void Search::SendUciInfo() REQUIRES(nodes_mutex_) REQUIRES(counters_mutex_) {
  const auto max_pv = params_.GetMultiPv();
  const auto edges = GetBestChildrenNoTemperature(root_node_, max_pv, 0);
  const auto score_type = params_.GetScoreType();
  const auto per_pv_counters = params_.GetPerPvCounters();
  const auto display_cache_usage = params_.GetDisplayCacheUsage();
  const auto draw_score = GetDrawScore(false);

  std::vector<ThinkingInfo> uci_infos;

  // Info common for all multipv variants.
  ThinkingInfo common_info;
  common_info.depth = cum_depth_ / (total_playouts_ ? total_playouts_ : 1);
  common_info.seldepth = max_depth_;
  common_info.time = GetTimeSinceStart();
  if (!per_pv_counters) {
    common_info.nodes = total_playouts_ + initial_visits_;
  }
  if (display_cache_usage) {
    common_info.hashfull =
        cache_->GetSize() * 1000LL / std::max(cache_->GetCapacity(), 1);
  }
  if (nps_start_time_) {
    const auto time_since_first_batch_ms =
        std::chrono::duration_cast<std::chrono::milliseconds>(
            std::chrono::steady_clock::now() - *nps_start_time_)
            .count();
    if (time_since_first_batch_ms > 0) {
      common_info.nps = total_playouts_ * 1000 / time_since_first_batch_ms;
    }
  }
  common_info.tb_hits = tb_hits_.load(std::memory_order_acquire);

  int multipv = 0;
  const auto default_q = -root_node_->GetQ(-draw_score);
  const auto default_wl = -root_node_->GetWL();
  const auto default_d = root_node_->GetD();
  for (const auto& edge : edges) {
    ++multipv;
    uci_infos.emplace_back(common_info);
    auto& uci_info = uci_infos.back();
    const auto wl = edge.GetWL(default_wl);
    const auto floatD = edge.GetD(default_d);
    const auto q = edge.GetQ(default_q, draw_score);
    if (edge.IsTerminal() && wl != 0.0f) {
      uci_info.mate = std::copysign(
          std::round(edge.GetM(0.0f)) / 2 + (edge.IsTbTerminal() ? 101 : 1),
          wl);
    } else if (score_type == "centipawn_with_drawscore") {
      uci_info.score = 90 * tan(1.5637541897 * q);
    } else if (score_type == "centipawn") {
      uci_info.score = 90 * tan(1.5637541897 * wl);
    } else if (score_type == "centipawn_2019") {
      uci_info.score = 295 * wl / (1 - 0.976953126 * std::pow(wl, 14));
    } else if (score_type == "centipawn_2018") {
      uci_info.score = 290.680623072 * tan(1.548090806 * wl);
    } else if (score_type == "win_percentage") {
      uci_info.score = wl * 5000 + 5000;
    } else if (score_type == "Q") {
      uci_info.score = q * 10000;
    } else if (score_type == "W-L") {
      uci_info.score = wl * 10000;
    }

    auto w =
        std::max(0, static_cast<int>(std::round(500.0 * (1.0 + wl - floatD))));
    auto l =
        std::max(0, static_cast<int>(std::round(500.0 * (1.0 - wl - floatD))));
    // Using 1000-w-l so that W+D+L add up to 1000.0.
    auto d = 1000 - w - l;
    if (d < 0) {
      w = std::min(1000, std::max(0, w + d / 2));
      l = 1000 - w;
      d = 0;
    }
    uci_info.wdl = ThinkingInfo::WDL{w, d, l};
    if (network_->GetCapabilities().has_mlh()) {
      uci_info.moves_left = static_cast<int>(
          (1.0f + edge.GetM(1.0f + root_node_->GetM())) / 2.0f);
    }
    if (max_pv > 1) uci_info.multipv = multipv;
    if (per_pv_counters) uci_info.nodes = edge.GetN();
    bool flip = played_history_.IsBlackToMove();
    int depth = 0;
    for (auto iter = edge; iter;
         iter = GetBestChildNoTemperature(iter.node(), depth), flip = !flip) {
      uci_info.pv.push_back(iter.GetMove(flip));
      if (!iter.node()) break;  // Last edge was dangling, cannot continue.
      depth += 1;
    }
  }

  if (!uci_infos.empty()) last_outputted_uci_info_ = uci_infos.front();
  if (current_best_edge_ && !edges.empty()) {
    last_outputted_info_edge_ = current_best_edge_.edge();
  }

  uci_responder_->OutputThinkingInfo(&uci_infos);
}

// Decides whether anything important changed in stats and new info should be
// shown to a user.
void Search::MaybeOutputInfo() {
  SharedMutex::Lock lock(nodes_mutex_);
  Mutex::Lock counters_lock(counters_mutex_);
  if (!bestmove_is_sent_ && current_best_edge_ &&
      (current_best_edge_.edge() != last_outputted_info_edge_ ||
       last_outputted_uci_info_.depth !=
           static_cast<int>(cum_depth_ /
                            (total_playouts_ ? total_playouts_ : 1)) ||
       last_outputted_uci_info_.seldepth != max_depth_ ||
       last_outputted_uci_info_.time + kUciInfoMinimumFrequencyMs <
           GetTimeSinceStart())) {
    SendUciInfo();
    if (params_.GetLogLiveStats()) {
      SendMovesStats();
    }
    if (stop_.load(std::memory_order_acquire) && !ok_to_respond_bestmove_) {
      std::vector<ThinkingInfo> info(1);
      info.back().comment =
          "WARNING: Search has reached limit and does not make any progress.";
      uci_responder_->OutputThinkingInfo(&info);
    }
  }
}

int64_t Search::GetTimeSinceStart() const {
  return std::chrono::duration_cast<std::chrono::milliseconds>(
             std::chrono::steady_clock::now() - start_time_)
      .count();
}

int64_t Search::GetTimeSinceFirstBatch() const REQUIRES(counters_mutex_) {
  if (!nps_start_time_) return 0;
  return std::chrono::duration_cast<std::chrono::milliseconds>(
             std::chrono::steady_clock::now() - *nps_start_time_)
      .count();
}

// Root is depth 0, i.e. even depth.
float Search::GetDrawScore(bool is_odd_depth) const {
  return (is_odd_depth ? params_.GetOpponentDrawScore()
                       : params_.GetSidetomoveDrawScore()) +
         (is_odd_depth == played_history_.IsBlackToMove()
              ? params_.GetWhiteDrawDelta()
              : params_.GetBlackDrawDelta());
}

namespace {
inline float GetFpu(const SearchParams& params, Node* node, bool is_root_node,
                    float draw_score) {
  const auto value = params.GetFpuValue(is_root_node);
  return params.GetFpuAbsolute(is_root_node)
             ? value
             : -node->GetQ(-draw_score) -
                   value * std::sqrt(node->GetVisitedPolicy());
}

// Faster version for if visited_policy is readily available already.
inline float GetFpu(const SearchParams& params, Node* node, bool is_root_node,
                    float draw_score, float visited_pol) {
  const auto value = params.GetFpuValue(is_root_node);
  return params.GetFpuAbsolute(is_root_node)
             ? value
             : -node->GetQ(-draw_score) - value * std::sqrt(visited_pol);
}

inline float ComputeCpuct(const SearchParams& params, uint32_t N,
                          bool is_root_node) {
  const float init = params.GetCpuct(is_root_node);
  const float k = params.GetCpuctFactor(is_root_node);
  const float base = params.GetCpuctBase(is_root_node);
  return init + (k ? k * FastLog((N + base) / base) : 0.0f);
}
}  // namespace

std::vector<std::string> Search::GetVerboseStats(Node* node) const {
  assert(node == root_node_ || node->GetParent() == root_node_);
  const bool is_root = (node == root_node_);
  const bool is_odd_depth = !is_root;
  const bool is_black_to_move = (played_history_.IsBlackToMove() == is_root);
  const float draw_score = GetDrawScore(is_odd_depth);
  const float fpu = GetFpu(params_, node, is_root, draw_score);
  const float cpuct = ComputeCpuct(params_, node->GetN(), is_root);
  const float U_coeff =
      cpuct * std::sqrt(std::max(node->GetChildrenVisits(), 1u));
  std::vector<EdgeAndNode> edges;
  for (const auto& edge : node->Edges()) edges.push_back(edge);

  std::sort(edges.begin(), edges.end(),
            [&fpu, &U_coeff, &draw_score](EdgeAndNode a, EdgeAndNode b) {
              return std::forward_as_tuple(
                         a.GetN(), a.GetQ(fpu, draw_score) + a.GetU(U_coeff)) <
                     std::forward_as_tuple(
                         b.GetN(), b.GetQ(fpu, draw_score) + b.GetU(U_coeff));
            });

  auto print = [](auto* oss, auto pre, auto v, auto post, auto w, int p = 0) {
    *oss << pre << std::setw(w) << std::setprecision(p) << v << post;
  };
  auto print_head = [&](auto* oss, auto label, int i, auto n, auto f, auto p) {
    *oss << std::fixed;
    print(oss, "", label, " ", 5);
    print(oss, "(", i, ") ", 4);
    *oss << std::right;
    print(oss, "N: ", n, " ", 7);
    print(oss, "(+", f, ") ", 2);
    print(oss, "(P: ", p * 100, "%) ", 5, p >= 0.99995f ? 1 : 2);
  };
  auto print_stats = [&](auto* oss, const auto* n) {
    const auto sign = n == node ? -1 : 1;
    if (n) {
      print(oss, "(WL: ", sign * n->GetWL(), ") ", 8, 5);
      print(oss, "(D: ", n->GetD(), ") ", 5, 3);
      print(oss, "(M: ", n->GetM(), ") ", 4, 1);
    } else {
      *oss << "(WL:  -.-----) (D: -.---) (M:  -.-) ";
    }
    print(oss, "(Q: ", n ? sign * n->GetQ(sign * draw_score) : fpu, ") ", 8, 5);
  };
  auto print_tail = [&](auto* oss, const auto* n) {
    const auto sign = n == node ? -1 : 1;
    std::optional<float> v;
    if (n && n->IsTerminal()) {
      v = n->GetQ(sign * draw_score);
    } else {
      NNCacheLock nneval = GetCachedNNEval(n);
      if (nneval) v = -nneval->q;
    }
    if (v) {
      print(oss, "(V: ", sign * *v, ") ", 7, 4);
    } else {
      *oss << "(V:  -.----) ";
    }

    if (n) {
      auto [lo, up] = n->GetBounds();
      if (sign == -1) {
        lo = -lo;
        up = -up;
        std::swap(lo, up);
      }
      *oss << (lo == up                                                ? "(T) "
               : lo == GameResult::DRAW && up == GameResult::WHITE_WON ? "(W) "
               : lo == GameResult::BLACK_WON && up == GameResult::DRAW ? "(L) "
                                                                       : "");
    }
  };

  std::vector<std::string> infos;
  const auto m_evaluator = network_->GetCapabilities().has_mlh()
                               ? MEvaluator(params_, node)
                               : MEvaluator();
  for (const auto& edge : edges) {
    float Q = edge.GetQ(fpu, draw_score);
    float M = m_evaluator.GetM(edge, Q);
    std::ostringstream oss;
    oss << std::left;
    // TODO: should this be displaying transformed index?
    print_head(&oss, edge.GetMove(is_black_to_move).as_string(),
               edge.GetMove().as_nn_index(0), edge.GetN(), edge.GetNInFlight(),
               edge.GetP());
    print_stats(&oss, edge.node());
    print(&oss, "(U: ", edge.GetU(U_coeff), ") ", 6, 5);
    print(&oss, "(S: ", Q + edge.GetU(U_coeff) + M, ") ", 8, 5);
    print_tail(&oss, edge.node());
    infos.emplace_back(oss.str());
  }

  // Include stats about the node in similar format to its children above.
  std::ostringstream oss;
  print_head(&oss, "node ", node->GetNumEdges(), node->GetN(),
             node->GetNInFlight(), node->GetVisitedPolicy());
  print_stats(&oss, node);
  print_tail(&oss, node);
  infos.emplace_back(oss.str());
  return infos;
}

void Search::SendMovesStats() const REQUIRES(counters_mutex_) {
  auto move_stats = GetVerboseStats(root_node_);

  if (params_.GetVerboseStats()) {
    std::vector<ThinkingInfo> infos;
    std::transform(move_stats.begin(), move_stats.end(),
                   std::back_inserter(infos), [](const std::string& line) {
                     ThinkingInfo info;
                     info.comment = line;
                     return info;
                   });
    uci_responder_->OutputThinkingInfo(&infos);
  } else {
    LOGFILE << "=== Move stats:";
    for (const auto& line : move_stats) LOGFILE << line;
  }
  for (auto& edge : root_node_->Edges()) {
    if (!(edge.GetMove(played_history_.IsBlackToMove()) == final_bestmove_)) {
      continue;
    }
    if (edge.HasNode()) {
      LOGFILE << "--- Opponent moves after: " << final_bestmove_.as_string();
      for (const auto& line : GetVerboseStats(edge.node())) {
        LOGFILE << line;
      }
    }
  }
}

NNCacheLock Search::GetCachedNNEval(const Node* node) const {
  if (!node) return {};

  std::vector<Move> moves;
  for (; node != root_node_; node = node->GetParent()) {
    moves.push_back(node->GetOwnEdge()->GetMove());
  }
  PositionHistory history(played_history_);
  for (auto iter = moves.rbegin(), end = moves.rend(); iter != end; ++iter) {
    history.Append(*iter);
  }
  const auto hash = history.HashLast(params_.GetCacheHistoryLength() + 1);
  NNCacheLock nneval(cache_, hash);
  return nneval;
}

void Search::MaybeTriggerStop(const IterationStats& stats,
                              StoppersHints* hints) {
  hints->Reset();
  SharedMutex::Lock nodes_lock(nodes_mutex_);
  Mutex::Lock lock(counters_mutex_);
  // Already responded bestmove, nothing to do here.
  if (bestmove_is_sent_) return;
  // Don't stop when the root node is not yet expanded.
  if (total_playouts_ + initial_visits_ == 0) return;

  if (!stop_.load(std::memory_order_acquire)) {
    if (stopper_->ShouldStop(stats, hints)) FireStopInternal();
  }

  // If we are the first to see that stop is needed.
  if (stop_.load(std::memory_order_acquire) && ok_to_respond_bestmove_ &&
      !bestmove_is_sent_) {
    SendUciInfo();
    EnsureBestMoveKnown();
    SendMovesStats();
    BestMoveInfo info(final_bestmove_, final_pondermove_);
    uci_responder_->OutputBestMove(&info);
    stopper_->OnSearchDone(stats);
    bestmove_is_sent_ = true;
    current_best_edge_ = EdgeAndNode();
  }

  // Use a 0 visit cancel score update to clear out any cached best edge, as
  // at the next iteration remaining playouts may be different.
  // TODO(crem) Is it really needed?
  root_node_->CancelScoreUpdate(0);
}

// Return the evaluation of the actual best child, regardless of temperature
// settings. This differs from GetBestMove, which does obey any temperature
// settings. So, somethimes, they may return results of different moves.
Eval Search::GetBestEval(Move* move, bool* is_terminal) const {
  SharedMutex::SharedLock lock(nodes_mutex_);
  Mutex::Lock counters_lock(counters_mutex_);
  float parent_wl = -root_node_->GetWL();
  float parent_d = root_node_->GetD();
  float parent_m = root_node_->GetM();
  if (!root_node_->HasChildren()) return {parent_wl, parent_d, parent_m};
  EdgeAndNode best_edge = GetBestChildNoTemperature(root_node_, 0);
  if (move) *move = best_edge.GetMove(played_history_.IsBlackToMove());
  if (is_terminal) *is_terminal = best_edge.IsTerminal();
  return {best_edge.GetWL(parent_wl), best_edge.GetD(parent_d),
          best_edge.GetM(parent_m - 1) + 1};
}

std::pair<Move, Move> Search::GetBestMove() {
  SharedMutex::Lock lock(nodes_mutex_);
  Mutex::Lock counters_lock(counters_mutex_);
  EnsureBestMoveKnown();
  return {final_bestmove_, final_pondermove_};
}

std::int64_t Search::GetTotalPlayouts() const {
  SharedMutex::SharedLock lock(nodes_mutex_);
  return total_playouts_;
}

void Search::ResetBestMove() {
  SharedMutex::Lock nodes_lock(nodes_mutex_);
  Mutex::Lock lock(counters_mutex_);
  bool old_sent = bestmove_is_sent_;
  bestmove_is_sent_ = false;
  EnsureBestMoveKnown();
  bestmove_is_sent_ = old_sent;
}

// Computes the best move, maybe with temperature (according to the settings).
void Search::EnsureBestMoveKnown() REQUIRES(nodes_mutex_)
    REQUIRES(counters_mutex_) {
  if (bestmove_is_sent_) return;
  if (root_node_->GetN() == 0) return;
  if (!root_node_->HasChildren()) return;

  float temperature = params_.GetTemperature();
  const int cutoff_move = params_.GetTemperatureCutoffMove();
  const int decay_delay_moves = params_.GetTempDecayDelayMoves();
  const int decay_moves = params_.GetTempDecayMoves();
  const int moves = played_history_.Last().GetGamePly() / 2;

  if (cutoff_move && (moves + 1) >= cutoff_move) {
    temperature = params_.GetTemperatureEndgame();
  } else if (temperature && decay_moves) {
    if (moves >= decay_delay_moves + decay_moves) {
      temperature = 0.0;
    } else if (moves >= decay_delay_moves) {
      temperature *=
          static_cast<float>(decay_delay_moves + decay_moves - moves) /
          decay_moves;
    }
    // don't allow temperature to decay below endgame temperature
    if (temperature < params_.GetTemperatureEndgame()) {
      temperature = params_.GetTemperatureEndgame();
    }
  }

  auto bestmove_edge = temperature
                           ? GetBestRootChildWithTemperature(temperature)
                           : GetBestChildNoTemperature(root_node_, 0);
  final_bestmove_ = bestmove_edge.GetMove(played_history_.IsBlackToMove());

  if (bestmove_edge.GetN() > 0 && bestmove_edge.node()->HasChildren()) {
    final_pondermove_ = GetBestChildNoTemperature(bestmove_edge.node(), 1)
                            .GetMove(!played_history_.IsBlackToMove());
  }
}

// Returns @count children with most visits.
std::vector<EdgeAndNode> Search::GetBestChildrenNoTemperature(Node* parent,
                                                              int count,
                                                              int depth) const {
  // Even if Edges is populated at this point, its a race condition to access
  // the node, so exit quickly.
  if (parent->GetN() == 0) return {};
  const bool is_odd_depth = (depth % 2) == 1;
  const float draw_score = GetDrawScore(is_odd_depth);
  // Best child is selected using the following criteria:
  // * Prefer shorter terminal wins / avoid shorter terminal losses.
  // * Largest number of playouts.
  // * If two nodes have equal number:
  //   * If that number is 0, the one with larger prior wins.
  //   * If that number is larger than 0, the one with larger eval wins.
  std::vector<EdgeAndNode> edges;
  for (auto& edge : parent->Edges()) {
    if (parent == root_node_ && !root_move_filter_.empty() &&
        std::find(root_move_filter_.begin(), root_move_filter_.end(),
                  edge.GetMove()) == root_move_filter_.end()) {
      continue;
    }
    edges.push_back(edge);
  }
  const auto middle = (static_cast<int>(edges.size()) > count)
                          ? edges.begin() + count
                          : edges.end();
  std::partial_sort(
      edges.begin(), middle, edges.end(),
      [draw_score](const auto& a, const auto& b) {
        // The function returns "true" when a is preferred to b.

        // Lists edge types from less desirable to more desirable.
        enum EdgeRank {
          kTerminalLoss,
          kTablebaseLoss,
          kNonTerminal,  // Non terminal or terminal draw.
          kTablebaseWin,
          kTerminalWin,
        };

        auto GetEdgeRank = [](const EdgeAndNode& edge) {
          // This default isn't used as wl only checked for case edge is
          // terminal.
          const auto wl = edge.GetWL(0.0f);
          // Not safe to access IsTerminal if GetN is 0.
          if (edge.GetN() == 0 || !edge.IsTerminal() || !wl) {
            return kNonTerminal;
          }
          if (edge.IsTbTerminal()) {
            return wl < 0.0 ? kTablebaseLoss : kTablebaseWin;
          }
          return wl < 0.0 ? kTerminalLoss : kTerminalWin;
        };

        // If moves have different outcomes, prefer better outcome.
        const auto a_rank = GetEdgeRank(a);
        const auto b_rank = GetEdgeRank(b);
        if (a_rank != b_rank) return a_rank > b_rank;

        // If both are terminal draws, try to make it shorter.
        // Not safe to access IsTerminal if GetN is 0.
        if (a_rank == kNonTerminal && a.GetN() != 0 && b.GetN() != 0 &&
            a.IsTerminal() && b.IsTerminal()) {
          if (a.IsTbTerminal() != b.IsTbTerminal()) {
            // Prefer non-tablebase draws.
            return a.IsTbTerminal() < b.IsTbTerminal();
          }
          // Prefer shorter draws.
          return a.GetM(0.0f) < b.GetM(0.0f);
        }

        // Neither is terminal, use standard rule.
        if (a_rank == kNonTerminal) {
          // Prefer largest playouts then eval then prior.
          if (a.GetN() != b.GetN()) return a.GetN() > b.GetN();
          // Default doesn't matter here so long as they are the same as either
          // both are N==0 (thus we're comparing equal defaults) or N!=0 and
          // default isn't used.
          if (a.GetQ(0.0f, draw_score) != b.GetQ(0.0f, draw_score)) {
            return a.GetQ(0.0f, draw_score) > b.GetQ(0.0f, draw_score);
          }
          return a.GetP() > b.GetP();
        }

        // Both variants are winning, prefer shortest win.
        if (a_rank > kNonTerminal) {
          return a.GetM(0.0f) < b.GetM(0.0f);
        }

        // Both variants are losing, prefer longest losses.
        return a.GetM(0.0f) > b.GetM(0.0f);
      });

  if (count < static_cast<int>(edges.size())) {
    edges.resize(count);
  }
  return edges;
}

// Returns a child with most visits.
EdgeAndNode Search::GetBestChildNoTemperature(Node* parent, int depth) const {
  auto res = GetBestChildrenNoTemperature(parent, 1, depth);
  return res.empty() ? EdgeAndNode() : res.front();
}

// Returns a child of a root chosen according to weighted-by-temperature visit
// count.
EdgeAndNode Search::GetBestRootChildWithTemperature(float temperature) const {
  // Root is at even depth.
  const float draw_score = GetDrawScore(/* is_odd_depth= */ false);

  std::vector<float> cumulative_sums;
  float sum = 0.0;
  float max_n = 0.0;
  const float offset = params_.GetTemperatureVisitOffset();
  float max_eval = -1.0f;
  const float fpu =
      GetFpu(params_, root_node_, /* is_root= */ true, draw_score);

  for (auto& edge : root_node_->Edges()) {
    if (!root_move_filter_.empty() &&
        std::find(root_move_filter_.begin(), root_move_filter_.end(),
                  edge.GetMove()) == root_move_filter_.end()) {
      continue;
    }
    if (edge.GetN() + offset > max_n) {
      max_n = edge.GetN() + offset;
      max_eval = edge.GetQ(fpu, draw_score);
    }
  }

  // TODO(crem) Simplify this code when samplers.h is merged.
  const float min_eval =
      max_eval - params_.GetTemperatureWinpctCutoff() / 50.0f;
  for (auto& edge : root_node_->Edges()) {
    if (!root_move_filter_.empty() &&
        std::find(root_move_filter_.begin(), root_move_filter_.end(),
                  edge.GetMove()) == root_move_filter_.end()) {
      continue;
    }
    if (edge.GetQ(fpu, draw_score) < min_eval) continue;
    sum += std::pow(
        std::max(0.0f,
                 (max_n <= 0.0f
                      ? edge.GetP()
                      : ((static_cast<float>(edge.GetN()) + offset) / max_n))),
        1 / temperature);
    cumulative_sums.push_back(sum);
  }
  assert(sum);

  const float toss = Random::Get().GetFloat(cumulative_sums.back());
  int idx =
      std::lower_bound(cumulative_sums.begin(), cumulative_sums.end(), toss) -
      cumulative_sums.begin();

  for (auto& edge : root_node_->Edges()) {
    if (!root_move_filter_.empty() &&
        std::find(root_move_filter_.begin(), root_move_filter_.end(),
                  edge.GetMove()) == root_move_filter_.end()) {
      continue;
    }
    if (edge.GetQ(fpu, draw_score) < min_eval) continue;
    if (idx-- == 0) return edge;
  }
  assert(false);
  return {};
}

void Search::StartThreads(size_t how_many) {
  thread_count_.store(how_many, std::memory_order_release);
  Mutex::Lock lock(threads_mutex_);
  // First thread is a watchdog thread.
  if (threads_.size() == 0) {
    threads_.emplace_back([this]() { WatchdogThread(); });
  }
  // Start working threads.
  for (size_t i = 0; i < how_many; i++) {
    threads_.emplace_back([this, i]() {
      SearchWorker worker(this, params_, i);
      worker.RunBlocking();
    });
  }
  LOGFILE << "Search started. "
          << std::chrono::duration_cast<std::chrono::milliseconds>(
                 std::chrono::steady_clock::now() - start_time_)
                 .count()
          << "ms already passed.";
}

void Search::RunBlocking(size_t threads) {
  StartThreads(threads);
  Wait();
}

bool Search::IsSearchActive() const {
  return !stop_.load(std::memory_order_acquire);
}

void Search::PopulateCommonIterationStats(IterationStats* stats) {
  stats->time_since_movestart = GetTimeSinceStart();

  SharedMutex::SharedLock nodes_lock(nodes_mutex_);
  {
    Mutex::Lock counters_lock(counters_mutex_);
    stats->time_since_first_batch = GetTimeSinceFirstBatch();
    if (!nps_start_time_ && total_playouts_ > 0) {
      nps_start_time_ = std::chrono::steady_clock::now();
    }
  }
  stats->total_nodes = total_playouts_ + initial_visits_;
  stats->nodes_since_movestart = total_playouts_;
  stats->batches_since_movestart = total_batches_;
  stats->average_depth = cum_depth_ / (total_playouts_ ? total_playouts_ : 1);
  stats->edge_n.clear();
  stats->win_found = false;
  stats->num_losing_edges = 0;
  stats->time_usage_hint_ = IterationStats::TimeUsageHint::kNormal;

  // If root node hasn't finished first visit, none of this code is safe.
  if (root_node_->GetN() > 0) {
    const auto draw_score = GetDrawScore(true);
    const float fpu =
        GetFpu(params_, root_node_, /* is_root_node */ true, draw_score);
    float max_q_plus_m = -1000;
    uint64_t max_n = 0;
    bool max_n_has_max_q_plus_m = true;
    const auto m_evaluator = network_->GetCapabilities().has_mlh()
                                 ? MEvaluator(params_, root_node_)
                                 : MEvaluator();
    for (const auto& edge : root_node_->Edges()) {
      const auto n = edge.GetN();
      const auto q = edge.GetQ(fpu, draw_score);
      const auto m = m_evaluator.GetM(edge, q);
      const auto q_plus_m = q + m;
      stats->edge_n.push_back(n);
      if (n > 0 && edge.IsTerminal() && edge.GetWL(0.0f) > 0.0f) {
        stats->win_found = true;
      }
      if (n > 0 && edge.IsTerminal() && edge.GetWL(0.0f) < 0.0f) {
        stats->num_losing_edges += 1;
      }
      if (max_n < n) {
        max_n = n;
        max_n_has_max_q_plus_m = false;
      }
      if (max_q_plus_m <= q_plus_m) {
        max_n_has_max_q_plus_m = (max_n == n);
        max_q_plus_m = q_plus_m;
      }
    }
    if (!max_n_has_max_q_plus_m) {
      stats->time_usage_hint_ = IterationStats::TimeUsageHint::kNeedMoreTime;
    }
  }
}

void Search::WatchdogThread() {
  Numa::BindThread(0);
  LOGFILE << "Start a watchdog thread.";
  StoppersHints hints;
  IterationStats stats;
  while (true) {
    hints.Reset();
    PopulateCommonIterationStats(&stats);
    MaybeTriggerStop(stats, &hints);
    MaybeOutputInfo();

    constexpr auto kMaxWaitTimeMs = 100;
    constexpr auto kMinWaitTimeMs = 1;

    Mutex::Lock lock(counters_mutex_);
    // Only exit when bestmove is responded. It may happen that search threads
    // already all exited, and we need at least one thread that can do that.
    if (bestmove_is_sent_) break;

    auto remaining_time = hints.GetEstimatedRemainingTimeMs();
    if (remaining_time > kMaxWaitTimeMs) remaining_time = kMaxWaitTimeMs;
    if (remaining_time < kMinWaitTimeMs) remaining_time = kMinWaitTimeMs;
    // There is no real need to have max wait time, and sometimes it's fine
    // to wait without timeout at all (e.g. in `go nodes` mode), but we
    // still limit wait time for exotic cases like when pc goes to sleep
    // mode during thinking.
    // Minimum wait time is there to prevent busy wait and other threads
    // starvation.
    watchdog_cv_.wait_for(
        lock.get_raw(), std::chrono::milliseconds(remaining_time),
        [this]() { return stop_.load(std::memory_order_acquire); });
  }
  LOGFILE << "End a watchdog thread.";
}

void Search::FireStopInternal() {
  stop_.store(true, std::memory_order_release);
  watchdog_cv_.notify_all();
}

void Search::Stop() {
  Mutex::Lock lock(counters_mutex_);
  ok_to_respond_bestmove_ = true;
  FireStopInternal();
  LOGFILE << "Stopping search due to `stop` uci command.";
}

void Search::Abort() {
  Mutex::Lock lock(counters_mutex_);
  if (!stop_.load(std::memory_order_acquire) ||
      (!bestmove_is_sent_ && !ok_to_respond_bestmove_)) {
    bestmove_is_sent_ = true;
    FireStopInternal();
  }
  LOGFILE << "Aborting search, if it is still active.";
}

void Search::Wait() {
  Mutex::Lock lock(threads_mutex_);
  while (!threads_.empty()) {
    threads_.back().join();
    threads_.pop_back();
  }
}

void Search::CancelSharedCollisions() REQUIRES(nodes_mutex_) {
  for (auto& entry : shared_collisions_) {
    Node* node = entry.first;
    for (node = node->GetParent(); node != root_node_->GetParent();
         node = node->GetParent()) {
      node->CancelScoreUpdate(entry.second);
    }
  }
  shared_collisions_.clear();
}

Search::~Search() {
  Abort();
  Wait();
  {
    SharedMutex::Lock lock(nodes_mutex_);
    CancelSharedCollisions();
  }
  LOGFILE << "Search destroyed.";
}

//////////////////////////////////////////////////////////////////////////////
// SearchWorker
//////////////////////////////////////////////////////////////////////////////

void SearchWorker::RunTasks(int tid) {
  while (true) {
    PickTask* task = nullptr;
    int id = 0;
    {
      int spins = 0;
      while (true) {
        int nta = tasks_taken_.load(std::memory_order_acquire);
        int tc = task_count_.load(std::memory_order_acquire);
        if (nta < tc) {
          int val = 0;
          if (task_taking_started_.compare_exchange_weak(
                  val, 1, std::memory_order_acq_rel,
                  std::memory_order_relaxed)) {
            nta = tasks_taken_.load(std::memory_order_acquire);
            tc = task_count_.load(std::memory_order_acquire);
            // We got the spin lock, double check we're still in the clear.
            if (nta < tc) {
              id = tasks_taken_.fetch_add(1, std::memory_order_acq_rel);
              task = &picking_tasks_[id];
              task_taking_started_.store(0, std::memory_order_release);
              break;
            }
            task_taking_started_.store(0, std::memory_order_release);
          }
          SpinloopPause();
          spins = 0;
          continue;
        } else if (tc != -1) {
          spins++;
          if (spins >= 512) {
            std::this_thread::yield();
            spins = 0;
          } else {
            SpinloopPause();
          }
          continue;
        }
        spins = 0;
        // Looks like sleep time.
        Mutex::Lock lock(picking_tasks_mutex_);
        // Refresh them now we have the lock.
        nta = tasks_taken_.load(std::memory_order_acquire);
        tc = task_count_.load(std::memory_order_acquire);
        if (tc != -1) continue;
        if (nta >= tc && exiting_) return;
        task_added_.wait(lock.get_raw());
        // And refresh again now we're awake.
        nta = tasks_taken_.load(std::memory_order_acquire);
        tc = task_count_.load(std::memory_order_acquire);
        if (nta >= tc && exiting_) return;
      }
    }
    if (task != nullptr) {
      switch (task->task_type) {
        case PickTask::kGathering: {
          PickNodesToExtendTask(task->start, task->base_depth,
                                task->collision_limit, task->moves_to_base,
                                &(task->results), &(task_workspaces_[tid]));
          break;
        }
        case PickTask::kProcessing: {
          ProcessPickedTask(task->start_idx, task->end_idx,
                            &(task_workspaces_[tid]));
          break;
        }
      }
      picking_tasks_[id].complete = true;
      completed_tasks_.fetch_add(1, std::memory_order_acq_rel);
    }
  }
}

void SearchWorker::ExecuteOneIteration() {
  // 1. Initialize internal structures.
  InitializeIteration(search_->network_->NewComputation());

  if (params_.GetMaxConcurrentSearchers() != 0) {
    while (true) {
      // If search is stop, we've not gathered or done anything and we don't
      // want to, so we can safely skip all below. But make sure we have done
      // at least one iteration.
      if (search_->stop_.load(std::memory_order_acquire) &&
          search_->GetTotalPlayouts() + search_->initial_visits_ > 0) {
        return;
      }
      int available =
          search_->pending_searchers_.load(std::memory_order_acquire);
      if (available > 0 &&
          search_->pending_searchers_.compare_exchange_weak(
              available, available - 1, std::memory_order_acq_rel)) {
        break;
      }
      // This is a hard spin lock to reduce latency but at the expense of busy
      // wait cpu usage. If search worker count is large, this is probably a bad
      // idea.
    }
  }

  // 2. Gather minibatch.
  GatherMinibatch2();
  task_count_.store(-1, std::memory_order_release);
  search_->backend_waiting_counter_.fetch_add(1, std::memory_order_relaxed);

  // 2b. Collect collisions.
  CollectCollisions();

  // 3. Prefetch into cache.
  MaybePrefetchIntoCache();

  if (params_.GetMaxConcurrentSearchers() != 0) {
    search_->pending_searchers_.fetch_add(1, std::memory_order_acq_rel);
  }

  // 4. Run NN computation.
  RunNNComputation();
  search_->backend_waiting_counter_.fetch_add(-1, std::memory_order_relaxed);

  // 5. Retrieve NN computations (and terminal values) into nodes.
  FetchMinibatchResults();

  // 6. Propagate the new nodes' information to all their parents in the tree.
  DoBackupUpdate();

  // 7. Update the Search's status and progress information.
  UpdateCounters();

  // If required, waste time to limit nps.
  if (params_.GetNpsLimit() > 0) {
    while (search_->IsSearchActive()) {
      int64_t time_since_first_batch_ms = 0;
      {
        Mutex::Lock lock(search_->counters_mutex_);
        time_since_first_batch_ms = search_->GetTimeSinceFirstBatch();
      }
      if (time_since_first_batch_ms <= 0) {
        time_since_first_batch_ms = search_->GetTimeSinceStart();
      }
      auto nps = search_->GetTotalPlayouts() * 1e3f / time_since_first_batch_ms;
      if (nps > params_.GetNpsLimit()) {
        std::this_thread::sleep_for(std::chrono::milliseconds(1));
      } else {
        break;
      }
    }
  }
}

// 1. Initialize internal structures.
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
void SearchWorker::InitializeIteration(
    std::unique_ptr<NetworkComputation> computation) {
  computation_ = std::make_unique<CachingComputation>(std::move(computation),
                                                      search_->cache_);
  computation_->Reserve(params_.GetMiniBatchSize());
  minibatch_.clear();
  minibatch_.reserve(2 * params_.GetMiniBatchSize());
}

// 2. Gather minibatch.
// ~~~~~~~~~~~~~~~~~~~~
namespace {
int Mix(int high, int low, float ratio) {
  return static_cast<int>(std::round(static_cast<float>(low) +
                                     static_cast<float>(high - low) * ratio));
}

int CalculateCollisionsLeft(int64_t nodes, const SearchParams& params) {
  // End checked first
  if (nodes >= params.GetMaxCollisionVisitsScalingEnd()) {
    return params.GetMaxCollisionVisits();
  }
  if (nodes <= params.GetMaxCollisionVisitsScalingStart()) {
    return 1;
  }
  return Mix(params.GetMaxCollisionVisits(), 1,
             std::pow((static_cast<float>(nodes) -
                       params.GetMaxCollisionVisitsScalingStart()) /
                          (params.GetMaxCollisionVisitsScalingEnd() -
                           params.GetMaxCollisionVisitsScalingStart()),
                      params.GetMaxCollisionVisitsScalingPower()));
}
}  // namespace

void SearchWorker::GatherMinibatch2() {
  // Total number of nodes to process.
  int minibatch_size = 0;
  int cur_n = 0;
  {
    SharedMutex::Lock lock(search_->nodes_mutex_);
    cur_n = search_->root_node_->GetN();
  }
  // TODO: GetEstimatedRemainingPlayouts has already had smart pruning factor
  // applied, which doesn't clearly make sense to include here...
  int64_t remaining_n =
      latest_time_manager_hints_.GetEstimatedRemainingPlayouts();
  int collisions_left = CalculateCollisionsLeft(
      std::min(static_cast<int64_t>(cur_n), remaining_n), params_);

  // Number of nodes processed out of order.
  number_out_of_order_ = 0;

  int thread_count = search_->thread_count_.load(std::memory_order_acquire);

  // Gather nodes to process in the current batch.
  // If we had too many nodes out of order, also interrupt the iteration so
  // that search can exit.
  while (minibatch_size < params_.GetMiniBatchSize() &&
         number_out_of_order_ < params_.GetMaxOutOfOrderEvals()) {
    // If there's something to process without touching slow neural net, do it.
    if (minibatch_size > 0 && computation_->GetCacheMisses() == 0) return;

    // If there is backend work to be done, and the backend is idle - exit
    // immediately.
    // Only do this fancy work if there are multiple threads as otherwise we
    // early exit from every batch since there is never another search thread to
    // be keeping the backend busy. Which would mean that threads=1 has a
    // massive nps drop.
    if (thread_count > 1 && minibatch_size > 0 &&
        computation_->GetCacheMisses() > params_.GetIdlingMinimumWork() &&
        thread_count - search_->backend_waiting_counter_.load(
                           std::memory_order_relaxed) >
            params_.GetThreadIdlingThreshold()) {
      return;
    }

    int new_start = static_cast<int>(minibatch_.size());

    PickNodesToExtend(
        std::min({collisions_left, params_.GetMiniBatchSize() - minibatch_size,
                  params_.GetMaxOutOfOrderEvals() - number_out_of_order_}));

    // Count the non-collisions.
    int non_collisions = 0;
    for (int i = new_start; i < static_cast<int>(minibatch_.size()); i++) {
      auto& picked_node = minibatch_[i];
      if (picked_node.IsCollision()) {
        continue;
      }
      ++non_collisions;
      ++minibatch_size;
    }

    bool needs_wait = false;
    int ppt_start = new_start;
    if (params_.GetTaskWorkersPerSearchWorker() > 0 &&
        non_collisions >= params_.GetMinimumWorkSizeForProcessing()) {
      const int num_tasks = std::clamp(
          non_collisions / params_.GetMinimumWorkPerTaskForProcessing(), 2,
          params_.GetTaskWorkersPerSearchWorker() + 1);
      // Round down, left overs can go to main thread so it waits less.
      int per_worker = non_collisions / num_tasks;
      needs_wait = true;
      ResetTasks();
      int found = 0;
      for (int i = new_start; i < static_cast<int>(minibatch_.size()); i++) {
        auto& picked_node = minibatch_[i];
        if (picked_node.IsCollision()) {
          continue;
        }
        ++found;
        if (found == per_worker) {
          picking_tasks_.emplace_back(ppt_start, i + 1);
          task_count_.fetch_add(1, std::memory_order_acq_rel);
          ppt_start = i + 1;
          found = 0;
          if (picking_tasks_.size() == static_cast<size_t>(num_tasks - 1)) {
            break;
          }
        }
      }
    }
    ProcessPickedTask(ppt_start, static_cast<int>(minibatch_.size()),
                      &main_workspace_);
    if (needs_wait) {
      WaitForTasks();
    }
    bool some_ooo = false;
    for (int i = static_cast<int>(minibatch_.size()) - 1; i >= new_start; i--) {
      if (minibatch_[i].ooo_completed) {
        some_ooo = true;
        break;
      }
    }
    if (some_ooo) {
      SharedMutex::Lock lock(search_->nodes_mutex_);
      for (int i = static_cast<int>(minibatch_.size()) - 1; i >= new_start;
           i--) {
        // If there was any OOO, revert 'all' new collisions - it isn't possible
        // to identify exactly which ones are afterwards and only prune those.
        // This may remove too many items, but hopefully most of the time they
        // will just be added back in the same in the next gather.
        if (minibatch_[i].IsCollision()) {
          Node* node = minibatch_[i].node;
          for (node = node->GetParent();
               node != search_->root_node_->GetParent();
               node = node->GetParent()) {
            node->CancelScoreUpdate(minibatch_[i].multivisit);
          }
          minibatch_.erase(minibatch_.begin() + i);
        } else if (minibatch_[i].ooo_completed) {
          DoBackupUpdateSingleNode(minibatch_[i]);
          minibatch_.erase(minibatch_.begin() + i);
          --minibatch_size;
          ++number_out_of_order_;
        }
      }
    }
    for (size_t i = new_start; i < minibatch_.size(); i++) {
      // If there was no OOO, there can stil be collisions.
      // There are no OOO though.
      // Also terminals when OOO is disabled.
      if (!minibatch_[i].nn_queried) continue;
      if (minibatch_[i].is_cache_hit) {
        // Since minibatch_[i] holds cache lock, this is guaranteed to succeed.
        computation_->AddInputByHash(minibatch_[i].hash,
                                     std::move(minibatch_[i].lock));
      } else {
        computation_->AddInput(minibatch_[i].hash,
                               std::move(minibatch_[i].input_planes),
                               std::move(minibatch_[i].probabilities_to_cache));
      }
    }

    // Check for stop at the end so we have at least one node.
    for (size_t i = new_start; i < minibatch_.size(); i++) {
      auto& picked_node = minibatch_[i];

      if (picked_node.IsCollision()) {
        // Check to see if we can upsize the collision to exit sooner.
        if (picked_node.maxvisit > 0 &&
            collisions_left > picked_node.multivisit) {
          SharedMutex::Lock lock(search_->nodes_mutex_);
          int extra = std::min(picked_node.maxvisit, collisions_left) -
                      picked_node.multivisit;
          picked_node.multivisit += extra;
          Node* node = picked_node.node;
          for (node = node->GetParent();
               node != search_->root_node_->GetParent();
               node = node->GetParent()) {
            node->IncrementNInFlight(extra);
          }
        }
        if ((collisions_left -= picked_node.multivisit) <= 0) return;
        if (search_->stop_.load(std::memory_order_acquire)) return;
      }
    }
  }
}

void SearchWorker::ProcessPickedTask(int start_idx, int end_idx,
                                     TaskWorkspace* workspace) {
  auto& history = workspace->history;
  history = search_->played_history_;

  for (int i = start_idx; i < end_idx; i++) {
    auto& picked_node = minibatch_[i];
    if (picked_node.IsCollision()) continue;
    auto* node = picked_node.node;

    // If node is already known as terminal (win/loss/draw according to rules
    // of the game), it means that we already visited this node before.
    if (picked_node.IsExtendable()) {
      // Node was never visited, extend it.
      ExtendNode(node, picked_node.depth, picked_node.moves_to_visit, &history);
      if (!node->IsTerminal()) {
        picked_node.nn_queried = true;
        const auto hash = history.HashLast(params_.GetCacheHistoryLength() + 1);
        picked_node.hash = hash;
        picked_node.lock = NNCacheLock(search_->cache_, hash);
        picked_node.is_cache_hit = picked_node.lock;
        if (!picked_node.is_cache_hit) {
          int transform;
          picked_node.input_planes = EncodePositionForNN(
              search_->network_->GetCapabilities().input_format, history, 8,
              params_.GetHistoryFill(), &transform);
          picked_node.probability_transform = transform;

          std::vector<uint16_t>& moves = picked_node.probabilities_to_cache;
          // Legal moves are known, use them.
          moves.reserve(node->GetNumEdges());
          for (const auto& edge : node->Edges()) {
            moves.emplace_back(edge.GetMove().as_nn_index(transform));
          }
        } else {
          picked_node.probability_transform = TransformForPosition(
              search_->network_->GetCapabilities().input_format, history);
        }
      }
    }
    if (params_.GetOutOfOrderEval() && picked_node.CanEvalOutOfOrder()) {
      // Perform out of order eval for the last entry in minibatch_.
      FetchSingleNodeResult(&picked_node, picked_node, 0);
      picked_node.ooo_completed = true;
    }
  }
}

#define MAX_TASKS 100

void SearchWorker::ResetTasks() {
  task_count_.store(0, std::memory_order_release);
  tasks_taken_.store(0, std::memory_order_release);
  completed_tasks_.store(0, std::memory_order_release);
  picking_tasks_.clear();
  // Reserve because resizing breaks pointers held by the task threads.
  picking_tasks_.reserve(MAX_TASKS);
}

int SearchWorker::WaitForTasks() {
  // Spin lock, other tasks should be done soon.
  while (true) {
    int completed = completed_tasks_.load(std::memory_order_acquire);
    int todo = task_count_.load(std::memory_order_acquire);
    if (todo == completed) return completed;
    SpinloopPause();
  }
}

void SearchWorker::PickNodesToExtend(int collision_limit) {
  ResetTasks();
  {
    // While nothing is ready yet - wake the task runners so they are ready to
    // receive quickly.
    Mutex::Lock lock(picking_tasks_mutex_);
    task_added_.notify_all();
  }
  std::vector<Move> empty_movelist;
  // This lock must be held until after the task_completed_ wait succeeds below.
  // Since the tasks perform work which assumes they have the lock, even though
  // actually this thread does.
  SharedMutex::Lock lock(search_->nodes_mutex_);
  PickNodesToExtendTask(search_->root_node_, 0, collision_limit, empty_movelist,
                        &minibatch_, &main_workspace_);

  WaitForTasks();
  for (int i = 0; i < static_cast<int>(picking_tasks_.size()); i++) {
    for (int j = 0; j < static_cast<int>(picking_tasks_[i].results.size());
         j++) {
      minibatch_.emplace_back(std::move(picking_tasks_[i].results[j]));
    }
  }
}

void SearchWorker::EnsureNodeTwoFoldCorrectForDepth(Node* child_node,
                                                    int depth) {
  // Check whether first repetition was before root. If yes, remove
  // terminal status of node and revert all visits in the tree.
  // Length of repetition was stored in m_. This code will only do
  // something when tree is reused and twofold visits need to be
  // reverted.
  if (child_node->IsTwoFoldTerminal() && depth < child_node->GetM()) {
    // Take a mutex - any SearchWorker specific mutex... since this is
    // not safe to do concurrently between multiple tasks.
    Mutex::Lock lock(picking_tasks_mutex_);
    int depth_counter = 0;
    // Cache node's values as we reset them in the process. We could
    // manually set wl and d, but if we want to reuse this for reverting
    // other terminal nodes this is the way to go.
    const auto wl = child_node->GetWL();
    const auto d = child_node->GetD();
    const auto m = child_node->GetM();
    const auto terminal_visits = child_node->GetN();
    for (Node* node_to_revert = child_node; node_to_revert != nullptr;
         node_to_revert = node_to_revert->GetParent()) {
      // Revert all visits on twofold draw when making it non terminal.
      node_to_revert->RevertTerminalVisits(wl, d, m + (float)depth_counter,
                                           terminal_visits);
      depth_counter++;
      // Even if original tree still exists, we don't want to revert
      // more than until new root.
      if (depth_counter > depth) break;
      // If wl != 0, we would have to switch signs at each depth.
    }
    // Mark the prior twofold draw as non terminal to extend it again.
    child_node->MakeNotTerminal();
    // When reverting the visits, we also need to revert the initial
    // visits, as we reused fewer nodes than anticipated.
    search_->initial_visits_ -= terminal_visits;
    // Max depth doesn't change when reverting the visits, and
    // cum_depth_ only counts the average depth of new nodes, not reused
    // ones.
  }
}

void SearchWorker::PickNodesToExtendTask(
    Node* node, int base_depth, int collision_limit,
    const std::vector<Move>& moves_to_base,
    std::vector<NodeToProcess>* receiver,
    TaskWorkspace* workspace) NO_THREAD_SAFETY_ANALYSIS {
  // TODO: Bring back pre-cached nodes created outside locks in a way that works
  // with tasks.
  // TODO: pre-reserve visits_to_perform for expected depth and likely maximum
  // width. Maybe even do so outside of lock scope.
  auto& vtp_buffer = workspace->vtp_buffer;
  auto& visits_to_perform = workspace->visits_to_perform;
  visits_to_perform.clear();
  auto& vtp_last_filled = workspace->vtp_last_filled;
  vtp_last_filled.clear();
  auto& current_path = workspace->current_path;
  current_path.clear();
  auto& moves_to_path = workspace->moves_to_path;
  moves_to_path = moves_to_base;
  // Sometimes receiver is reused, othertimes not, so only jump start if small.
  if (receiver->capacity() < 30) {
    receiver->reserve(receiver->size() + 30);
  }

  // These 2 are 'filled pre-emptively'.
  std::array<float, 256> current_pol;
  std::array<float, 256> current_util;

  // These 3 are 'filled on demand'.
  std::array<float, 256> current_score;
  std::array<int, 256> current_nstarted;
  auto& cur_iters = workspace->cur_iters;

  Node::Iterator best_edge;
  Node::Iterator second_best_edge;
  // Fetch the current best root node visits for possible smart pruning.
  const int64_t best_node_n = search_->current_best_edge_.GetN();

  int passed_off = 0;
  int completed_visits = 0;

  bool is_root_node = node == search_->root_node_;
  const float even_draw_score = search_->GetDrawScore(false);
  const float odd_draw_score = search_->GetDrawScore(true);
  const auto& root_move_filter = search_->root_move_filter_;
  auto m_evaluator = moves_left_support_ ? MEvaluator(params_) : MEvaluator();

  int max_limit = std::numeric_limits<int>::max();

  current_path.push_back(-1);
  while (current_path.size() > 0) {
    // First prepare visits_to_perform.
    if (current_path.back() == -1) {
      // Need to do n visits, where n is either collision_limit, or comes from
      // visits_to_perform for the current path.
      int cur_limit = collision_limit;
      if (current_path.size() > 1) {
        cur_limit =
            (*visits_to_perform.back())[current_path[current_path.size() - 2]];
      }
      // First check if node is terminal or not-expanded.  If either than create
      // a collision of appropriate size and pop current_path.
      if (node->GetN() == 0 || node->IsTerminal()) {
        if (is_root_node) {
          // Root node is special - since its not reached from anywhere else, so
          // it needs its own logic. Still need to create the collision to
          // ensure the outer gather loop gives up.
          if (node->TryStartScoreUpdate()) {
            cur_limit -= 1;
            minibatch_.push_back(NodeToProcess::Visit(
                node, static_cast<uint16_t>(current_path.size() + base_depth)));
            completed_visits++;
          }
        }
        // Visits are created elsewhere, just need the collisions here.
        if (cur_limit > 0) {
          int max_count = 0;
          if (cur_limit == collision_limit && base_depth == 0 &&
              max_limit > cur_limit) {
            max_count = max_limit;
          }
          receiver->push_back(NodeToProcess::Collision(
              node, static_cast<uint16_t>(current_path.size() + base_depth),
              cur_limit, max_count));
          completed_visits += cur_limit;
        }
        node = node->GetParent();
        current_path.pop_back();
        continue;
      }
      if (is_root_node) {
        // Root node is again special - needs its n in flight updated separately
        // as its not handled on the path to it, since there isn't one.
        node->IncrementNInFlight(cur_limit);
      }

      // Create visits_to_perform new back entry for this level.
      if (vtp_buffer.size() > 0) {
        visits_to_perform.push_back(std::move(vtp_buffer.back()));
        vtp_buffer.pop_back();
      } else {
        visits_to_perform.push_back(std::make_unique<std::array<int, 256>>());
      }
      vtp_last_filled.push_back(-1);

      // Cache all constant UCT parameters.
      // When we're near the leaves we can copy less of the policy, since there
      // is no way iteration will ever reach it.
      // TODO: This is a very conservative formula. It assumes every visit we're
      // aiming to add is going to trigger a new child, and that any visits
      // we've already had have also done so and then a couple extra since we go
      // to 2 unvisited to get second best in worst case.
      // Unclear we can do better without having already walked the children.
      // Which we are putting off until after policy is copied so we can create
      // visited policy without having to cache it in the node (allowing the
      // node to stay at 64 bytes).
      int max_needed = node->GetNumEdges();
      if (!is_root_node || root_move_filter.empty()) {
        max_needed = std::min(max_needed, node->GetNStarted() + cur_limit + 2);
      }
      node->CopyPolicy(max_needed, current_pol.data());
      for (int i = 0; i < max_needed; i++) {
        current_util[i] = std::numeric_limits<float>::lowest();
      }
      // Root depth is 1 here, while for GetDrawScore() it's 0-based, that's why
      // the weirdness.
      const float draw_score = ((current_path.size() + base_depth) % 2 == 0)
                                   ? odd_draw_score
                                   : even_draw_score;
      m_evaluator.SetParent(node);
      float visited_pol = 0.0f;
      for (Node* child : node->VisitedNodes()) {
        int index = child->Index();
        visited_pol += current_pol[index];
        float q = child->GetQ(draw_score);
        current_util[index] = q + m_evaluator.GetM(child, q);
      }
      const float fpu =
          GetFpu(params_, node, is_root_node, draw_score, visited_pol);
      for (int i = 0; i < max_needed; i++) {
        if (current_util[i] == std::numeric_limits<float>::lowest()) {
          current_util[i] = fpu + m_evaluator.GetDefaultM();
        }
      }

      const float cpuct = ComputeCpuct(params_, node->GetN(), is_root_node);
      const float puct_mult =
          cpuct * std::sqrt(std::max(node->GetChildrenVisits(), 1u));
      int cache_filled_idx = -1;
      while (cur_limit > 0) {
        // Perform UCT for current node.
        float best = std::numeric_limits<float>::lowest();
        int best_idx = -1;
        float best_without_u = std::numeric_limits<float>::lowest();
        float second_best = std::numeric_limits<float>::lowest();
        bool can_exit = false;
        best_edge.Reset();
        for (int idx = 0; idx < max_needed; ++idx) {
          if (idx > cache_filled_idx) {
            if (idx == 0) {
              cur_iters[idx] = node->Edges();
            } else {
              cur_iters[idx] = cur_iters[idx - 1];
              ++cur_iters[idx];
            }
            current_nstarted[idx] = cur_iters[idx].GetNStarted();
          }
          int nstarted = current_nstarted[idx];
          const float util = current_util[idx];
          if (idx > cache_filled_idx) {
            current_score[idx] =
                current_pol[idx] * puct_mult / (1 + nstarted) + util;
            cache_filled_idx++;
          }
          if (is_root_node) {
            // If there's no chance to catch up to the current best node with
            // remaining playouts, don't consider it.
            // best_move_node_ could have changed since best_node_n was
            // retrieved. To ensure we have at least one node to expand, always
            // include current best node.
            if (cur_iters[idx] != search_->current_best_edge_ &&
                latest_time_manager_hints_.GetEstimatedRemainingPlayouts() <
                    best_node_n - cur_iters[idx].GetN()) {
              continue;
            }
            // If root move filter exists, make sure move is in the list.
            if (!root_move_filter.empty() &&
                std::find(root_move_filter.begin(), root_move_filter.end(),
                          cur_iters[idx].GetMove()) == root_move_filter.end()) {
              continue;
            }
          }

          float score = current_score[idx];
          if (score > best) {
            second_best = best;
            second_best_edge = best_edge;
            best = score;
            best_idx = idx;
            best_without_u = util;
            best_edge = cur_iters[idx];
          } else if (score > second_best) {
            second_best = score;
            second_best_edge = cur_iters[idx];
          }
          if (can_exit) break;
          if (nstarted == 0) {
            // One more loop will get 2 unvisited nodes, which is sufficient to
            // ensure second best is correct. This relies upon the fact that
            // edges are sorted in policy decreasing order.
            can_exit = true;
          }
        }
        int new_visits = 0;
        if (second_best_edge) {
          int estimated_visits_to_change_best = std::numeric_limits<int>::max();
          if (best_without_u < second_best) {
            const auto n1 = current_nstarted[best_idx] + 1;
            estimated_visits_to_change_best = static_cast<int>(
                std::max(1.0f, std::min(current_pol[best_idx] * puct_mult /
                                                (second_best - best_without_u) -
                                            n1 + 1,
                                        1e9f)));
          }
          second_best_edge.Reset();
          max_limit = std::min(max_limit, estimated_visits_to_change_best);
          new_visits = std::min(cur_limit, estimated_visits_to_change_best);
        } else {
          // No second best - only one edge, so everything goes in here.
          new_visits = cur_limit;
        }
        if (best_idx >= vtp_last_filled.back()) {
          auto* vtp_array = visits_to_perform.back().get()->data();
          std::fill(vtp_array + (vtp_last_filled.back() + 1),
                    vtp_array + best_idx + 1, 0);
        }
        (*visits_to_perform.back())[best_idx] += new_visits;
        cur_limit -= new_visits;
        Node* child_node = best_edge.GetOrSpawnNode(/* parent */ node);

        // Probably best place to check for two-fold draws consistently.
        // Depth starts with 1 at root, so real depth is depth - 1.
        EnsureNodeTwoFoldCorrectForDepth(
            child_node, current_path.size() + base_depth + 1 - 1);

        bool decremented = false;
        if (child_node->TryStartScoreUpdate()) {
          current_nstarted[best_idx]++;
          new_visits -= 1;
          decremented = true;
          if (child_node->GetN() > 0 && !child_node->IsTerminal()) {
            child_node->IncrementNInFlight(new_visits);
            current_nstarted[best_idx] += new_visits;
          }
          current_score[best_idx] = current_pol[best_idx] * puct_mult /
                                        (1 + current_nstarted[best_idx]) +
                                    current_util[best_idx];
        }
        if ((decremented &&
             (child_node->GetN() == 0 || child_node->IsTerminal()))) {
          // Reduce 1 for the visits_to_perform to ensure the collision created
          // doesn't include this visit.
          (*visits_to_perform.back())[best_idx] -= 1;
          receiver->push_back(NodeToProcess::Visit(
              child_node,
              static_cast<uint16_t>(current_path.size() + 1 + base_depth)));
          completed_visits++;
          receiver->back().moves_to_visit.reserve(moves_to_path.size() + 1);
          receiver->back().moves_to_visit = moves_to_path;
          receiver->back().moves_to_visit.push_back(best_edge.GetMove());
        }
        if (best_idx > vtp_last_filled.back() &&
            (*visits_to_perform.back())[best_idx] > 0) {
          vtp_last_filled.back() = best_idx;
        }
      }
      is_root_node = false;
      // Actively do any splits now rather than waiting for potentially long
      // tree walk to get there.
      for (int i = 0; i <= vtp_last_filled.back(); i++) {
        int child_limit = (*visits_to_perform.back())[i];
        if (params_.GetTaskWorkersPerSearchWorker() > 0 &&
            child_limit > params_.GetMinimumWorkSizeForPicking() &&
            child_limit <
                ((collision_limit - passed_off - completed_visits) * 2 / 3) &&
            child_limit + passed_off + completed_visits <
                collision_limit -
                    params_.GetMinimumRemainingWorkSizeForPicking()) {
          Node* child_node = cur_iters[i].GetOrSpawnNode(/* parent */ node);
          // Don't split if not expanded or terminal.
          if (child_node->GetN() == 0 || child_node->IsTerminal()) continue;

          bool passed = false;
          {
            // Multiple writers, so need mutex here.
            Mutex::Lock lock(picking_tasks_mutex_);
            // Ensure not to exceed size of reservation.
            if (picking_tasks_.size() < MAX_TASKS) {
              moves_to_path.push_back(cur_iters[i].GetMove());
              picking_tasks_.emplace_back(
                  child_node, current_path.size() - 1 + base_depth + 1,
                  moves_to_path, child_limit);
              moves_to_path.pop_back();
              task_count_.fetch_add(1, std::memory_order_acq_rel);
              task_added_.notify_all();
              passed = true;
              passed_off += child_limit;
            }
          }
          if (passed) {
            (*visits_to_perform.back())[i] = 0;
          }
        }
      }
      // Fall through to select the first child.
    }
    int min_idx = current_path.back();
    bool found_child = false;
    if (vtp_last_filled.back() > min_idx) {
      int idx = -1;
      for (auto& child : node->Edges()) {
        idx++;
        if (idx > min_idx && (*visits_to_perform.back())[idx] > 0) {
          if (moves_to_path.size() != current_path.size() + base_depth) {
            moves_to_path.push_back(child.GetMove());
          } else {
            moves_to_path.back() = child.GetMove();
          }
          current_path.back() = idx;
          current_path.push_back(-1);
          node = child.GetOrSpawnNode(/* parent */ node);
          found_child = true;
          break;
        }
        if (idx >= vtp_last_filled.back()) break;
      }
    }
    if (!found_child) {
      node = node->GetParent();
      if (!moves_to_path.empty()) moves_to_path.pop_back();
      current_path.pop_back();
      vtp_buffer.push_back(std::move(visits_to_perform.back()));
      visits_to_perform.pop_back();
      vtp_last_filled.pop_back();
    }
  }
}

void SearchWorker::ExtendNode(Node* node, int depth,
                              const std::vector<Move>& moves_to_node,
                              PositionHistory* history) {
  // Initialize position sequence with pre-move position.
  history->Trim(search_->played_history_.GetLength());
  for (size_t i = 0; i < moves_to_node.size(); i++) {
    history->Append(moves_to_node[i]);
  }

  // We don't need the mutex because other threads will see that N=0 and
  // N-in-flight=1 and will not touch this node.
  const auto& board = history->Last().GetBoard();
  auto legal_moves = board.GenerateLegalMoves();

  // Check whether it's a draw/lose by position. Importantly, we must check
  // these before doing the by-rule checks below.
  if (legal_moves.empty()) {
    // Could be a checkmate or a stalemate
    if (board.IsUnderCheck()) {
      node->MakeTerminal(GameResult::WHITE_WON);
    } else {
      node->MakeTerminal(GameResult::DRAW);
    }
    return;
  }

  // We can shortcircuit these draws-by-rule only if they aren't root;
  // if they are root, then thinking about them is the point.
  if (node != search_->root_node_) {
    if (!board.HasMatingMaterial()) {
      node->MakeTerminal(GameResult::DRAW);
      return;
    }

    if (history->Last().GetRule50Ply() >= 100) {
      node->MakeTerminal(GameResult::DRAW);
      return;
    }

    const auto repetitions = history->Last().GetRepetitions();
    // Mark two-fold repetitions as draws according to settings.
    // Depth starts with 1 at root, so number of plies in PV is depth - 1.
    if (repetitions >= 2) {
      node->MakeTerminal(GameResult::DRAW);
      return;
    } else if (repetitions == 1 && depth - 1 >= 4 &&
               params_.GetTwoFoldDraws() &&
               depth - 1 >= history->Last().GetPliesSincePrevRepetition()) {
      const auto cycle_length = history->Last().GetPliesSincePrevRepetition();
      // use plies since first repetition as moves left; exact if forced draw.
      node->MakeTerminal(GameResult::DRAW, (float)cycle_length,
                         Node::Terminal::TwoFold);
      return;
    }

    // Neither by-position or by-rule termination, but maybe it's a TB position.
    if (search_->syzygy_tb_ && !search_->root_is_in_dtz_ &&
        board.castlings().no_legal_castle() &&
        history->Last().GetRule50Ply() == 0 &&
        (board.ours() | board.theirs()).count() <=
            search_->syzygy_tb_->max_cardinality()) {
      ProbeState state;
      const WDLScore wdl =
          search_->syzygy_tb_->probe_wdl(history->Last(), &state);
      // Only fail state means the WDL is wrong, probe_wdl may produce correct
      // result with a stat other than OK.
      if (state != FAIL) {
        // TB nodes don't have NN evaluation, assign M from parent node.
        float m = 0.0f;
        // Need a lock to access parent, in case MakeSolid is in progress.
        {
          SharedMutex::SharedLock lock(search_->nodes_mutex_);
          auto parent = node->GetParent();
          if (parent) {
            m = std::max(0.0f, parent->GetM() - 1.0f);
          }
        }
        // If the colors seem backwards, check the checkmate check above.
        if (wdl == WDL_WIN) {
          node->MakeTerminal(GameResult::BLACK_WON, m,
                             Node::Terminal::Tablebase);
        } else if (wdl == WDL_LOSS) {
          node->MakeTerminal(GameResult::WHITE_WON, m,
                             Node::Terminal::Tablebase);
        } else {  // Cursed wins and blessed losses count as draws.
          node->MakeTerminal(GameResult::DRAW, m, Node::Terminal::Tablebase);
        }
        search_->tb_hits_.fetch_add(1, std::memory_order_acq_rel);
        return;
      }
    }
  }

  // Add legal moves as edges of this node.
  node->CreateEdges(legal_moves);
}

// Returns whether node was already in cache.
bool SearchWorker::AddNodeToComputation(Node* node) {
  const auto hash = history_.HashLast(params_.GetCacheHistoryLength() + 1);
  if (search_->cache_->ContainsKey(hash)) {
    return true;
  }
  int transform;
  auto planes =
      EncodePositionForNN(search_->network_->GetCapabilities().input_format,
                          history_, 8, params_.GetHistoryFill(), &transform);

  std::vector<uint16_t> moves;

  if (node && node->HasChildren()) {
    // Legal moves are known, use them.
    moves.reserve(node->GetNumEdges());
    for (const auto& edge : node->Edges()) {
      moves.emplace_back(edge.GetMove().as_nn_index(transform));
    }
  } else {
    // Cache pseudolegal moves. A bit of a waste, but faster.
    const auto& pseudolegal_moves =
        history_.Last().GetBoard().GeneratePseudolegalMoves();
    moves.reserve(pseudolegal_moves.size());
    for (auto iter = pseudolegal_moves.begin(), end = pseudolegal_moves.end();
         iter != end; ++iter) {
      moves.emplace_back(iter->as_nn_index(transform));
    }
  }

  computation_->AddInput(hash, std::move(planes), std::move(moves));
  return false;
}

// 2b. Copy collisions into shared collisions.
void SearchWorker::CollectCollisions() {
  SharedMutex::Lock lock(search_->nodes_mutex_);

  for (const NodeToProcess& node_to_process : minibatch_) {
    if (node_to_process.IsCollision()) {
      search_->shared_collisions_.emplace_back(node_to_process.node,
                                               node_to_process.multivisit);
    }
  }
}

// 3. Prefetch into cache.
// ~~~~~~~~~~~~~~~~~~~~~~~
void SearchWorker::MaybePrefetchIntoCache() {
  // TODO(mooskagh) Remove prefetch into cache if node collisions work well.
  // If there are requests to NN, but the batch is not full, try to prefetch
  // nodes which are likely useful in future.
  if (search_->stop_.load(std::memory_order_acquire)) return;
  if (computation_->GetCacheMisses() > 0 &&
      computation_->GetCacheMisses() < params_.GetMaxPrefetchBatch()) {
    history_.Trim(search_->played_history_.GetLength());
    SharedMutex::SharedLock lock(search_->nodes_mutex_);
    PrefetchIntoCache(
        search_->root_node_,
        params_.GetMaxPrefetchBatch() - computation_->GetCacheMisses(), false);
  }
}

// Prefetches up to @budget nodes into cache. Returns number of nodes
// prefetched.
int SearchWorker::PrefetchIntoCache(Node* node, int budget, bool is_odd_depth) {
  const float draw_score = search_->GetDrawScore(is_odd_depth);
  if (budget <= 0) return 0;

  // We are in a leaf, which is not yet being processed.
  if (!node || node->GetNStarted() == 0) {
    if (AddNodeToComputation(node)) {
      // Make it return 0 to make it not use the slot, so that the function
      // tries hard to find something to cache even among unpopular moves.
      // In practice that slows things down a lot though, as it's not always
      // easy to find what to cache.
      return 1;
    }
    return 1;
  }

  assert(node);
  // n = 0 and n_in_flight_ > 0, that means the node is being extended.
  if (node->GetN() == 0) return 0;
  // The node is terminal; don't prefetch it.
  if (node->IsTerminal()) return 0;

  // Populate all subnodes and their scores.
  typedef std::pair<float, EdgeAndNode> ScoredEdge;
  std::vector<ScoredEdge> scores;
  const float cpuct =
      ComputeCpuct(params_, node->GetN(), node == search_->root_node_);
  const float puct_mult =
      cpuct * std::sqrt(std::max(node->GetChildrenVisits(), 1u));
  const float fpu =
      GetFpu(params_, node, node == search_->root_node_, draw_score);
  for (auto& edge : node->Edges()) {
    if (edge.GetP() == 0.0f) continue;
    // Flip the sign of a score to be able to easily sort.
    // TODO: should this use logit_q if set??
    scores.emplace_back(-edge.GetU(puct_mult) - edge.GetQ(fpu, draw_score),
                        edge);
  }

  size_t first_unsorted_index = 0;
  int total_budget_spent = 0;
  int budget_to_spend = budget;  // Initialize for the case where there's only
                                 // one child.
  for (size_t i = 0; i < scores.size(); ++i) {
    if (search_->stop_.load(std::memory_order_acquire)) break;
    if (budget <= 0) break;

    // Sort next chunk of a vector. 3 at a time. Most of the time it's fine.
    if (first_unsorted_index != scores.size() &&
        i + 2 >= first_unsorted_index) {
      const int new_unsorted_index =
          std::min(scores.size(), budget < 2 ? first_unsorted_index + 2
                                             : first_unsorted_index + 3);
      std::partial_sort(scores.begin() + first_unsorted_index,
                        scores.begin() + new_unsorted_index, scores.end(),
                        [](const ScoredEdge& a, const ScoredEdge& b) {
                          return a.first < b.first;
                        });
      first_unsorted_index = new_unsorted_index;
    }

    auto edge = scores[i].second;
    // Last node gets the same budget as prev-to-last node.
    if (i != scores.size() - 1) {
      // Sign of the score was flipped for sorting, so flip it back.
      const float next_score = -scores[i + 1].first;
      // TODO: As above - should this use logit_q if set?
      const float q = edge.GetQ(-fpu, draw_score);
      if (next_score > q) {
        budget_to_spend =
            std::min(budget, int(edge.GetP() * puct_mult / (next_score - q) -
                                 edge.GetNStarted()) +
                                 1);
      } else {
        budget_to_spend = budget;
      }
    }
    history_.Append(edge.GetMove());
    const int budget_spent =
        PrefetchIntoCache(edge.node(), budget_to_spend, !is_odd_depth);
    history_.Pop();
    budget -= budget_spent;
    total_budget_spent += budget_spent;
  }
  return total_budget_spent;
}

// 4. Run NN computation.
// ~~~~~~~~~~~~~~~~~~~~~~
void SearchWorker::RunNNComputation() { computation_->ComputeBlocking(); }

// 5. Retrieve NN computations (and terminal values) into nodes.
// ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
void SearchWorker::FetchMinibatchResults() {
  // Populate NN/cached results, or terminal results, into nodes.
  int idx_in_computation = 0;
  for (auto& node_to_process : minibatch_) {
    FetchSingleNodeResult(&node_to_process, *computation_, idx_in_computation);
    if (node_to_process.nn_queried) ++idx_in_computation;
  }
}

template <typename Computation>
void SearchWorker::FetchSingleNodeResult(NodeToProcess* node_to_process,
                                         const Computation& computation,
                                         int idx_in_computation) {
  if (node_to_process->IsCollision()) return;
  Node* node = node_to_process->node;
  if (!node_to_process->nn_queried) {
    // Terminal nodes don't involve the neural NetworkComputation, nor do
    // they require any further processing after value retrieval.
    node_to_process->v = node->GetWL();
    node_to_process->d = node->GetD();
    node_to_process->m = node->GetM();
    return;
  }
  // For NN results, we need to populate policy as well as value.
  // First the value...
  node_to_process->v = -computation.GetQVal(idx_in_computation);
  node_to_process->d = computation.GetDVal(idx_in_computation);
  node_to_process->m = computation.GetMVal(idx_in_computation);
  // ...and secondly, the policy data.
  // Calculate maximum first.
  float max_p = -std::numeric_limits<float>::infinity();
  // Intermediate array to store values when processing policy.
  // There are never more than 256 valid legal moves in any legal position.
  std::array<float, 256> intermediate;
  int counter = 0;
  for (auto& edge : node->Edges()) {
    float p = computation.GetPVal(
        idx_in_computation,
        edge.GetMove().as_nn_index(node_to_process->probability_transform));
    intermediate[counter++] = p;
    max_p = std::max(max_p, p);
  }
  float total = 0.0;
  for (int i = 0; i < counter; i++) {
    // Perform softmax and take into account policy softmax temperature T.
    // Note that we want to calculate (exp(p-max_p))^(1/T) = exp((p-max_p)/T).
    float p =
        FastExp((intermediate[i] - max_p) / params_.GetPolicySoftmaxTemp());
    intermediate[i] = p;
    total += p;
  }
  counter = 0;
  // Normalize P values to add up to 1.0.
  const float scale = total > 0.0f ? 1.0f / total : 1.0f;
  for (auto& edge : node->Edges()) {
    edge.edge()->SetP(intermediate[counter++] * scale);
  }
  // Add Dirichlet noise if enabled and at root.
  if (params_.GetNoiseEpsilon() && node == search_->root_node_) {
    ApplyDirichletNoise(node, params_.GetNoiseEpsilon(),
                        params_.GetNoiseAlpha());
  }
  node->SortEdges();
}

// 6. Propagate the new nodes' information to all their parents in the tree.
// ~~~~~~~~~~~~~~
void SearchWorker::DoBackupUpdate() {
  // Nodes mutex for doing node updates.
  SharedMutex::Lock lock(search_->nodes_mutex_);

  bool work_done = number_out_of_order_ > 0;
  for (const NodeToProcess& node_to_process : minibatch_) {
    DoBackupUpdateSingleNode(node_to_process);
    if (!node_to_process.IsCollision()) {
      work_done = true;
    }
  }
  if (!work_done) return;
  search_->CancelSharedCollisions();
  search_->total_batches_ += 1;
}

void SearchWorker::DoBackupUpdateSingleNode(
    const NodeToProcess& node_to_process) REQUIRES(search_->nodes_mutex_) {
  Node* node = node_to_process.node;
  if (node_to_process.IsCollision()) {
    // Collisions are handled via shared_collisions instead.
    return;
  }

  // For the first visit to a terminal, maybe update parent bounds too.
  auto update_parent_bounds =
      params_.GetStickyEndgames() && node->IsTerminal() && !node->GetN();

  // Backup V value up to a root. After 1 visit, V = Q.
  float v = node_to_process.v;
  float d = node_to_process.d;
  float m = node_to_process.m;
  int n_to_fix = 0;
  float v_delta = 0.0f;
  float d_delta = 0.0f;
  float m_delta = 0.0f;
  uint32_t solid_threshold =
      static_cast<uint32_t>(params_.GetSolidTreeThreshold());
  for (Node *n = node, *p; n != search_->root_node_->GetParent(); n = p) {
    p = n->GetParent();

    // Current node might have become terminal from some other descendant, so
    // backup the rest of the way with more accurate values.
    if (n->IsTerminal()) {
      v = n->GetWL();
      d = n->GetD();
      m = n->GetM();
    }
    n->FinalizeScoreUpdate(v, d, m, node_to_process.multivisit);
    if (n_to_fix > 0 && !n->IsTerminal()) {
      n->AdjustForTerminal(v_delta, d_delta, m_delta, n_to_fix);
    }
    if (n->GetN() >= solid_threshold) {
      if (n->MakeSolid() && n == search_->root_node_) {
        // If we make the root solid, the current_best_edge_ becomes invalid and
        // we should repopulate it.
        search_->current_best_edge_ =
            search_->GetBestChildNoTemperature(search_->root_node_, 0);
      }
    }

    // Nothing left to do without ancestors to update.
    if (!p) break;

    bool old_update_parent_bounds = update_parent_bounds;
    // If parent already is terminal further adjustment is not required.
    if (p->IsTerminal()) n_to_fix = 0;
    // Try setting parent bounds except the root or those already terminal.
    update_parent_bounds =
        update_parent_bounds && p != search_->root_node_ && !p->IsTerminal() &&
        MaybeSetBounds(p, m, &n_to_fix, &v_delta, &d_delta, &m_delta);

    // Q will be flipped for opponent.
    v = -v;
    v_delta = -v_delta;
    m++;

    // Update the stats.
    // Best move.
    // If update_parent_bounds was set, we just adjusted bounds on the
    // previous loop or there was no previous loop, so if n is a terminal, it
    // just became that way and could be a candidate for changing the current
    // best edge. Otherwise a visit can only change best edge if its to an edge
    // that isn't already the best and the new n is equal or greater to the old
    // n.
    if (p == search_->root_node_ &&
        ((old_update_parent_bounds && n->IsTerminal()) ||
         (n != search_->current_best_edge_.node() &&
          search_->current_best_edge_.GetN() <= n->GetN()))) {
      search_->current_best_edge_ =
          search_->GetBestChildNoTemperature(search_->root_node_, 0);
    }
  }
  search_->total_playouts_ += node_to_process.multivisit;
  search_->cum_depth_ += node_to_process.depth * node_to_process.multivisit;
  search_->max_depth_ = std::max(search_->max_depth_, node_to_process.depth);
}

bool SearchWorker::MaybeSetBounds(Node* p, float m, int* n_to_fix,
                                  float* v_delta, float* d_delta,
                                  float* m_delta) const {
  auto losing_m = 0.0f;
  auto prefer_tb = false;

  // Determine the maximum (lower, upper) bounds across all children.
  // (-1,-1) Loss (initial and lowest bounds)
  // (-1, 0) Can't Win
  // (-1, 1) Regular node
  // ( 0, 0) Draw
  // ( 0, 1) Can't Lose
  // ( 1, 1) Win (highest bounds)
  auto lower = GameResult::BLACK_WON;
  auto upper = GameResult::BLACK_WON;
  for (const auto& edge : p->Edges()) {
    const auto [edge_lower, edge_upper] = edge.GetBounds();
    lower = std::max(edge_lower, lower);
    upper = std::max(edge_upper, upper);

    // Checkmate is the best, so short-circuit.
    const auto is_tb = edge.IsTbTerminal();
    if (edge_lower == GameResult::WHITE_WON && !is_tb) {
      prefer_tb = false;
      break;
    } else if (edge_upper == GameResult::BLACK_WON) {
      // Track the longest loss.
      losing_m = std::max(losing_m, edge.GetM(0.0f));
    }
    prefer_tb = prefer_tb || is_tb;
  }

  // The parent's bounds are flipped from the children (-max(U), -max(L))
  // aggregated as if it was a single child (forced move) of the same bound.
  //       Loss (-1,-1) -> ( 1, 1) Win
  //  Can't Win (-1, 0) -> ( 0, 1) Can't Lose
  //    Regular (-1, 1) -> (-1, 1) Regular
  //       Draw ( 0, 0) -> ( 0, 0) Draw
  // Can't Lose ( 0, 1) -> (-1, 0) Can't Win
  //        Win ( 1, 1) -> (-1,-1) Loss

  // Nothing left to do for ancestors if the parent would be a regular node.
  if (lower == GameResult::BLACK_WON && upper == GameResult::WHITE_WON) {
    return false;
  } else if (lower == upper) {
    // Search can stop at the parent if the bounds can't change anymore, so make
    // it terminal preferring shorter wins and longer losses.
    *n_to_fix = p->GetN();
    assert(*n_to_fix > 0);
    float cur_v = p->GetWL();
    float cur_d = p->GetD();
    float cur_m = p->GetM();
    p->MakeTerminal(
        -upper,
        (upper == GameResult::BLACK_WON ? std::max(losing_m, m) : m) + 1.0f,
        prefer_tb ? Node::Terminal::Tablebase : Node::Terminal::EndOfGame);
    // Negate v_delta because we're calculating for the parent, but immediately
    // afterwards we'll negate v_delta in case it has come from the child.
    *v_delta = -(p->GetWL() - cur_v);
    *d_delta = p->GetD() - cur_d;
    *m_delta = p->GetM() - cur_m;
  } else {
    p->SetBounds(-upper, -lower);
  }

  // Bounds were set, so indicate we should check the parent too.
  return true;
}

// 7. Update the Search's status and progress information.
//~~~~~~~~~~~~~~~~~~~~
void SearchWorker::UpdateCounters() {
  search_->PopulateCommonIterationStats(&iteration_stats_);
  search_->MaybeTriggerStop(iteration_stats_, &latest_time_manager_hints_);
  search_->MaybeOutputInfo();

  // If this thread had no work, not even out of order, then sleep for some
  // milliseconds. Collisions don't count as work, so have to enumerate to find
  // out if there was anything done.
  bool work_done = number_out_of_order_ > 0;
  if (!work_done) {
    for (NodeToProcess& node_to_process : minibatch_) {
      if (!node_to_process.IsCollision()) {
        work_done = true;
        break;
      }
    }
  }
  if (!work_done) {
    std::this_thread::sleep_for(std::chrono::milliseconds(10));
  }
}

}  // namespace lczero

```

`src/mcts/search.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <array>
#include <condition_variable>
#include <functional>
#include <optional>
#include <shared_mutex>
#include <thread>

#include "chess/callbacks.h"
#include "chess/uciloop.h"
#include "mcts/node.h"
#include "mcts/params.h"
#include "mcts/stoppers/timemgr.h"
#include "neural/cache.h"
#include "neural/network.h"
#include "syzygy/syzygy.h"
#include "utils/logging.h"
#include "utils/mutex.h"
#include "utils/numa.h"

namespace lczero {

class Search {
 public:
  Search(const NodeTree& tree, Network* network,
         std::unique_ptr<UciResponder> uci_responder,
         const MoveList& searchmoves,
         std::chrono::steady_clock::time_point start_time,
         std::unique_ptr<SearchStopper> stopper, bool infinite,
         const OptionsDict& options, NNCache* cache,
         SyzygyTablebase* syzygy_tb);

  ~Search();

  // Starts worker threads and returns immediately.
  void StartThreads(size_t how_many);

  // Starts search with k threads and wait until it finishes.
  void RunBlocking(size_t threads);

  // Stops search. At the end bestmove will be returned. The function is not
  // blocking, so it returns before search is actually done.
  void Stop();
  // Stops search, but does not return bestmove. The function is not blocking.
  void Abort();
  // Blocks until all worker thread finish.
  void Wait();
  // Returns whether search is active. Workers check that to see whether another
  // search iteration is needed.
  bool IsSearchActive() const;

  // Returns best move, from the point of view of white player. And also ponder.
  // May or may not use temperature, according to the settings.
  std::pair<Move, Move> GetBestMove();

  // Returns the evaluation of the best move, WITHOUT temperature. This differs
  // from the above function; with temperature enabled, these two functions may
  // return results from different possible moves. If @move and @is_terminal are
  // not nullptr they are set to the best move and whether it leads to a
  // terminal node respectively.
  Eval GetBestEval(Move* move = nullptr, bool* is_terminal = nullptr) const;
  // Returns the total number of playouts in the search.
  std::int64_t GetTotalPlayouts() const;
  // Returns the search parameters.
  const SearchParams& GetParams() const { return params_; }

  // If called after GetBestMove, another call to GetBestMove will have results
  // from temperature having been applied again.
  void ResetBestMove();

  // Returns NN eval for a given node from cache, if that node is cached.
  NNCacheLock GetCachedNNEval(const Node* node) const;

 private:
  // Computes the best move, maybe with temperature (according to the settings).
  void EnsureBestMoveKnown();

  // Returns a child with most visits, with or without temperature.
  // NoTemperature is safe to use on non-extended nodes, while WithTemperature
  // accepts only nodes with at least 1 visited child.
  EdgeAndNode GetBestChildNoTemperature(Node* parent, int depth) const;
  std::vector<EdgeAndNode> GetBestChildrenNoTemperature(Node* parent, int count,
                                                        int depth) const;
  EdgeAndNode GetBestRootChildWithTemperature(float temperature) const;

  int64_t GetTimeSinceStart() const;
  int64_t GetTimeSinceFirstBatch() const;
  void MaybeTriggerStop(const IterationStats& stats, StoppersHints* hints);
  void MaybeOutputInfo();
  void SendUciInfo();  // Requires nodes_mutex_ to be held.
  // Sets stop to true and notifies watchdog thread.
  void FireStopInternal();

  void SendMovesStats() const;
  // Function which runs in a separate thread and watches for time and
  // uci `stop` command;
  void WatchdogThread();

  // Fills IterationStats with global (rather than per-thread) portion of search
  // statistics. Currently all stats there (in IterationStats) are global
  // though.
  void PopulateCommonIterationStats(IterationStats* stats);

  // Returns verbose information about given node, as vector of strings.
  // Node can only be root or ponder (depth 1).
  std::vector<std::string> GetVerboseStats(Node* node) const;

  // Returns the draw score at the root of the search. At odd depth pass true to
  // the value of @is_odd_depth to change the sign of the draw score.
  // Depth of a root node is 0 (even number).
  float GetDrawScore(bool is_odd_depth) const;

  // Ensure that all shared collisions are cancelled and clear them out.
  void CancelSharedCollisions();

  mutable Mutex counters_mutex_ ACQUIRED_AFTER(nodes_mutex_);
  // Tells all threads to stop.
  std::atomic<bool> stop_{false};
  // Condition variable used to watch stop_ variable.
  std::condition_variable watchdog_cv_;
  // Tells whether it's ok to respond bestmove when limits are reached.
  // If false (e.g. during ponder or `go infinite`) the search stops but nothing
  // is responded until `stop` uci command.
  bool ok_to_respond_bestmove_ GUARDED_BY(counters_mutex_) = true;
  // There is already one thread that responded bestmove, other threads
  // should not do that.
  bool bestmove_is_sent_ GUARDED_BY(counters_mutex_) = false;
  // Stored so that in the case of non-zero temperature GetBestMove() returns
  // consistent results.
  Move final_bestmove_ GUARDED_BY(counters_mutex_);
  Move final_pondermove_ GUARDED_BY(counters_mutex_);
  std::unique_ptr<SearchStopper> stopper_ GUARDED_BY(counters_mutex_);

  Mutex threads_mutex_;
  std::vector<std::thread> threads_ GUARDED_BY(threads_mutex_);

  Node* root_node_;
  NNCache* cache_;
  SyzygyTablebase* syzygy_tb_;
  // Fixed positions which happened before the search.
  const PositionHistory& played_history_;

  Network* const network_;
  const SearchParams params_;
  const MoveList searchmoves_;
  const std::chrono::steady_clock::time_point start_time_;
  int64_t initial_visits_;
  // root_is_in_dtz_ must be initialized before root_move_filter_.
  bool root_is_in_dtz_ = false;
  // tb_hits_ must be initialized before root_move_filter_.
  std::atomic<int> tb_hits_{0};
  const MoveList root_move_filter_;

  mutable SharedMutex nodes_mutex_;
  EdgeAndNode current_best_edge_ GUARDED_BY(nodes_mutex_);
  Edge* last_outputted_info_edge_ GUARDED_BY(nodes_mutex_) = nullptr;
  ThinkingInfo last_outputted_uci_info_ GUARDED_BY(nodes_mutex_);
  int64_t total_playouts_ GUARDED_BY(nodes_mutex_) = 0;
  int64_t total_batches_ GUARDED_BY(nodes_mutex_) = 0;
  // Maximum search depth = length of longest path taken in PickNodetoExtend.
  uint16_t max_depth_ GUARDED_BY(nodes_mutex_) = 0;
  // Cumulative depth of all paths taken in PickNodetoExtend.
  uint64_t cum_depth_ GUARDED_BY(nodes_mutex_) = 0;

  std::optional<std::chrono::steady_clock::time_point> nps_start_time_
      GUARDED_BY(counters_mutex_);

  std::atomic<int> pending_searchers_{0};
  std::atomic<int> backend_waiting_counter_{0};
  std::atomic<int> thread_count_{0};

  std::vector<std::pair<Node*, int>> shared_collisions_
      GUARDED_BY(nodes_mutex_);

  std::unique_ptr<UciResponder> uci_responder_;

  friend class SearchWorker;
};

// Single thread worker of the search engine.
// That used to be just a function Search::Worker(), but to parallelize it
// within one thread, have to split into stages.
class SearchWorker {
 public:
  SearchWorker(Search* search, const SearchParams& params, int id)
      : search_(search),
        history_(search_->played_history_),
        params_(params),
        moves_left_support_(search_->network_->GetCapabilities().moves_left !=
                            pblczero::NetworkFormat::MOVES_LEFT_NONE) {
    Numa::BindThread(id);
    for (int i = 0; i < params.GetTaskWorkersPerSearchWorker(); i++) {
      task_workspaces_.emplace_back();
      task_threads_.emplace_back([this, i]() {
        Numa::BindThread(i);
        this->RunTasks(i);
      });
    }
  }

  ~SearchWorker() {
    {
      task_count_.store(-1, std::memory_order_release);
      Mutex::Lock lock(picking_tasks_mutex_);
      exiting_ = true;
      task_added_.notify_all();
    }
    for (size_t i = 0; i < task_threads_.size(); i++) {
      task_threads_[i].join();
    }
  }

  // Runs iterations while needed.
  void RunBlocking() {
    LOGFILE << "Started search thread.";
    try {
      // A very early stop may arrive before this point, so the test is at the
      // end to ensure at least one iteration runs before exiting.
      do {
        ExecuteOneIteration();
      } while (search_->IsSearchActive());
    } catch (std::exception& e) {
      std::cerr << "Unhandled exception in worker thread: " << e.what()
                << std::endl;
      abort();
    }
  }

  // Does one full iteration of MCTS search:
  // 1. Initialize internal structures.
  // 2. Gather minibatch.
  // 3. Prefetch into cache.
  // 4. Run NN computation.
  // 5. Retrieve NN computations (and terminal values) into nodes.
  // 6. Propagate the new nodes' information to all their parents in the tree.
  // 7. Update the Search's status and progress information.
  void ExecuteOneIteration();

  // The same operations one by one:
  // 1. Initialize internal structures.
  // @computation is the computation to use on this iteration.
  void InitializeIteration(std::unique_ptr<NetworkComputation> computation);

  // 2. Gather minibatch.
  void GatherMinibatch();
  // Variant for multigather path.
  void GatherMinibatch2();

  // 2b. Copy collisions into shared_collisions_.
  void CollectCollisions();

  // 3. Prefetch into cache.
  void MaybePrefetchIntoCache();

  // 4. Run NN computation.
  void RunNNComputation();

  // 5. Retrieve NN computations (and terminal values) into nodes.
  void FetchMinibatchResults();

  // 6. Propagate the new nodes' information to all their parents in the tree.
  void DoBackupUpdate();

  // 7. Update the Search's status and progress information.
  void UpdateCounters();

 private:
  struct NodeToProcess {
    bool IsExtendable() const { return !is_collision && !node->IsTerminal(); }
    bool IsCollision() const { return is_collision; }
    bool CanEvalOutOfOrder() const {
      return is_cache_hit || node->IsTerminal();
    }

    // The node to extend.
    Node* node;
    // Value from NN's value head, or -1/0/1 for terminal nodes.
    float v;
    // Draw probability for NN's with WDL value head.
    float d;
    // Estimated remaining plies left.
    float m;
    int multivisit = 0;
    // If greater than multivisit, and other parameters don't imply a lower
    // limit, multivist could be increased to this value without additional
    // change in outcome of next selection.
    int maxvisit = 0;
    uint16_t depth;
    bool nn_queried = false;
    bool is_cache_hit = false;
    bool is_collision = false;
    int probability_transform = 0;

    // Details only populated in the multigather path.

    // Only populated for visits,
    std::vector<Move> moves_to_visit;

    // Details that are filled in as we go.
    uint64_t hash;
    NNCacheLock lock;
    std::vector<uint16_t> probabilities_to_cache;
    InputPlanes input_planes;
    mutable int last_idx = 0;
    bool ooo_completed = false;

    static NodeToProcess Collision(Node* node, uint16_t depth,
                                   int collision_count) {
      return NodeToProcess(node, depth, true, collision_count, 0);
    }
    static NodeToProcess Collision(Node* node, uint16_t depth,
                                   int collision_count, int max_count) {
      return NodeToProcess(node, depth, true, collision_count, max_count);
    }
    static NodeToProcess Visit(Node* node, uint16_t depth) {
      return NodeToProcess(node, depth, false, 1, 0);
    }

    // Methods to allow NodeToProcess to conform as a 'Computation'. Only safe
    // to call if is_cache_hit is true in the multigather path.

    float GetQVal(int) const { return lock->q; }

    float GetDVal(int) const { return lock->d; }

    float GetMVal(int) const { return lock->m; }

    float GetPVal(int, int move_id) const {
      const auto& moves = lock->p;

      int total_count = 0;
      while (total_count < moves.size()) {
        // Optimization: usually moves are stored in the same order as queried.
        const auto& move = moves[last_idx++];
        if (last_idx == moves.size()) last_idx = 0;
        if (move.first == move_id) return move.second;
        ++total_count;
      }
      assert(false);  // Move not found.
      return 0;
    }

   private:
    NodeToProcess(Node* node, uint16_t depth, bool is_collision, int multivisit,
                  int max_count)
        : node(node),
          multivisit(multivisit),
          maxvisit(max_count),
          depth(depth),
          is_collision(is_collision) {}
  };

  // Holds per task worker scratch data
  struct TaskWorkspace {
    std::array<Node::Iterator, 256> cur_iters;
    std::vector<std::unique_ptr<std::array<int, 256>>> vtp_buffer;
    std::vector<std::unique_ptr<std::array<int, 256>>> visits_to_perform;
    std::vector<int> vtp_last_filled;
    std::vector<int> current_path;
    std::vector<Move> moves_to_path;
    PositionHistory history;
    TaskWorkspace() {
      vtp_buffer.reserve(30);
      visits_to_perform.reserve(30);
      vtp_last_filled.reserve(30);
      current_path.reserve(30);
      moves_to_path.reserve(30);
      history.Reserve(30);
    }
  };

  struct PickTask {
    enum PickTaskType { kGathering, kProcessing };
    PickTaskType task_type;

    // For task type gathering.
    Node* start;
    int base_depth;
    int collision_limit;
    std::vector<Move> moves_to_base;
    std::vector<NodeToProcess> results;

    // Task type post gather processing.
    int start_idx;
    int end_idx;

    bool complete = false;

    PickTask(Node* node, uint16_t depth, const std::vector<Move>& base_moves,
             int collision_limit)
        : task_type(kGathering),
          start(node),
          base_depth(depth),
          collision_limit(collision_limit),
          moves_to_base(base_moves) {}
    PickTask(int start_idx, int end_idx)
        : task_type(kProcessing), start_idx(start_idx), end_idx(end_idx) {}
  };

  NodeToProcess PickNodeToExtend(int collision_limit);
  bool AddNodeToComputation(Node* node);
  int PrefetchIntoCache(Node* node, int budget, bool is_odd_depth);
  void DoBackupUpdateSingleNode(const NodeToProcess& node_to_process);
  // Returns whether a node's bounds were set based on its children.
  bool MaybeSetBounds(Node* p, float m, int* n_to_fix, float* v_delta,
                      float* d_delta, float* m_delta) const;
  void PickNodesToExtend(int collision_limit);
  void PickNodesToExtendTask(Node* starting_point, int collision_limit,
                             int base_depth,
                             const std::vector<Move>& moves_to_base,
                             std::vector<NodeToProcess>* receiver,
                             TaskWorkspace* workspace);
  void EnsureNodeTwoFoldCorrectForDepth(Node* node, int depth);
  void ProcessPickedTask(int batch_start, int batch_end,
                         TaskWorkspace* workspace);
  void ExtendNode(Node* node, int depth, const std::vector<Move>& moves_to_add,
                  PositionHistory* history);
  template <typename Computation>
  void FetchSingleNodeResult(NodeToProcess* node_to_process,
                             const Computation& computation,
                             int idx_in_computation);
  void RunTasks(int tid);
  void ResetTasks();
  // Returns how many tasks there were.
  int WaitForTasks();

  Search* const search_;
  // List of nodes to process.
  std::vector<NodeToProcess> minibatch_;
  std::unique_ptr<CachingComputation> computation_;
  // History is reset and extended by PickNodeToExtend().
  PositionHistory history_;
  int number_out_of_order_ = 0;
  const SearchParams& params_;
  std::unique_ptr<Node> precached_node_;
  const bool moves_left_support_;
  IterationStats iteration_stats_;
  StoppersHints latest_time_manager_hints_;

  // Multigather task related fields.

  Mutex picking_tasks_mutex_;
  std::vector<PickTask> picking_tasks_;
  std::atomic<int> task_count_ = -1;
  std::atomic<int> task_taking_started_ = 0;
  std::atomic<int> tasks_taken_ = 0;
  std::atomic<int> completed_tasks_ = 0;
  std::condition_variable task_added_;
  std::vector<std::thread> task_threads_;
  std::vector<TaskWorkspace> task_workspaces_;
  TaskWorkspace main_workspace_;
  bool exiting_ = false;
};

}  // namespace lczero

```

`src/mcts/stoppers/alphazero.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "mcts/stoppers/stoppers.h"

namespace lczero {

namespace {

class AlphazeroTimeManager : public TimeManager {
 public:
  AlphazeroTimeManager(int64_t move_overhead, const OptionsDict& params)
      : move_overhead_(move_overhead),
        alphazerotimepct_(
            params.GetOrDefault<float>("alphazero-time-pct", 12.0f)) {
    if (alphazerotimepct_ < 0.0f || alphazerotimepct_ > 100.0f)
      throw Exception("alphazero-time-pct value to be in range [0.0, 100.0]");
  }
  std::unique_ptr<SearchStopper> GetStopper(const GoParams& params,
                                            const NodeTree& tree) override;

 private:
  const int64_t move_overhead_;
  const float alphazerotimepct_;
};

std::unique_ptr<SearchStopper> AlphazeroTimeManager::GetStopper(
    const GoParams& params, const NodeTree& tree) {
  const Position& position = tree.HeadPosition();
  const bool is_black = position.IsBlackToMove();
  const std::optional<int64_t>& time = (is_black ? params.btime : params.wtime);
  // If no time limit is given, don't stop on this condition.
  if (params.infinite || params.ponder || !time) return nullptr;

  auto total_moves_time = *time - move_overhead_;

  float this_move_time = total_moves_time * (alphazerotimepct_ / 100.0f);

  LOGFILE << "Budgeted time for the move: " << this_move_time << "ms"
          << "Remaining time " << *time << "ms(-" << move_overhead_
          << "ms overhead)";

  return std::make_unique<TimeLimitStopper>(this_move_time);
}

}  // namespace

std::unique_ptr<TimeManager> MakeAlphazeroTimeManager(
    int64_t move_overhead, const OptionsDict& params) {
  return std::make_unique<AlphazeroTimeManager>(move_overhead, params);
}
}  // namespace lczero

```

`src/mcts/stoppers/alphazero.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include "utils/optionsdict.h"

namespace lczero {

std::unique_ptr<TimeManager> MakeAlphazeroTimeManager(
    int64_t move_overhead, const OptionsDict& params);

}  // namespace lczero
```

`src/mcts/stoppers/common.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "src/mcts/stoppers/common.h"

namespace lczero {

const OptionId kNNCacheSizeId{
    "nncache", "NNCacheSize",
    "Number of positions to store in a memory cache. A large cache can speed "
    "up searching, but takes memory."};

namespace {

const OptionId kRamLimitMbId{
    "ramlimit-mb", "RamLimitMb",
    "Maximum memory usage for the engine, in megabytes. The estimation is very "
    "rough, and can be off by a lot. For example, multiple visits to a "
    "terminal node counted several times, and the estimation assumes that all "
    "positions have 30 possible moves. When set to 0, no RAM limit is "
    "enforced."};
const OptionId kMinimumKLDGainPerNodeId{
    "minimum-kldgain-per-node", "MinimumKLDGainPerNode",
    "If greater than 0 search will abort unless the last "
    "KLDGainAverageInterval nodes have an average gain per node of at least "
    "this much."};
const OptionId kKLDGainAverageIntervalId{
    "kldgain-average-interval", "KLDGainAverageInterval",
    "Used to decide how frequently to evaluate the average KLDGainPerNode to "
    "check the MinimumKLDGainPerNode, if specified."};
const OptionId kSmartPruningFactorId{
    "smart-pruning-factor", "SmartPruningFactor",
    "Do not spend time on the moves which cannot become bestmove given the "
    "remaining time to search. When no other move can overtake the current "
    "best, the search stops, saving the time. Values greater than 1 stop less "
    "promising moves from being considered even earlier. Values less than 1 "
    "causes hopeless moves to still have some attention. When set to 0, smart "
    "pruning is deactivated."};
const OptionId kMinimumSmartPruningBatchesId{
    "smart-pruning-minimum-batches", "SmartPruningMinimumBatches",
    "Only allow smart pruning to stop search after at least this many batches "
    "have been evaluated. It may be useful to have this value greater than the "
    "number of search threads in use."};
const OptionId kNodesAsPlayoutsId{
    "nodes-as-playouts", "NodesAsPlayouts",
    "Treat UCI `go nodes` command as referring to playouts instead of visits."};

}  // namespace

void PopulateCommonStopperOptions(RunType for_what, OptionsParser* options) {
  options->Add<IntOption>(kKLDGainAverageIntervalId, 1, 10000000) = 100;
  options->Add<FloatOption>(kMinimumKLDGainPerNodeId, 0.0f, 1.0f) = 0.0f;
  options->Add<FloatOption>(kSmartPruningFactorId, 0.0f, 10.0f) =
      (for_what == RunType::kUci ? 1.33f : 0.00f);
  options->Add<IntOption>(kMinimumSmartPruningBatchesId, 0, 10000) = 0;
  options->Add<BoolOption>(kNodesAsPlayoutsId) = false;

  if (for_what == RunType::kUci) {
    options->Add<IntOption>(kRamLimitMbId, 0, 100000000) = 0;
    options->HideOption(kMinimumKLDGainPerNodeId);
    options->HideOption(kKLDGainAverageIntervalId);
    options->HideOption(kNodesAsPlayoutsId);
  }
}

// Parameters needed for selfplay and uci, but not benchmark nor infinite mode.
void PopulateIntrinsicStoppers(ChainedSearchStopper* stopper,
                               const OptionsDict& options) {
  // KLD gain.
  const auto min_kld_gain = options.Get<float>(kMinimumKLDGainPerNodeId);
  if (min_kld_gain > 0.0f) {
    stopper->AddStopper(std::make_unique<KldGainStopper>(
        min_kld_gain, options.Get<int>(kKLDGainAverageIntervalId)));
  }

  // Should be last in the chain.
  const auto smart_pruning_factor = options.Get<float>(kSmartPruningFactorId);
  if (smart_pruning_factor > 0.0f) {
    stopper->AddStopper(std::make_unique<SmartPruningStopper>(
        smart_pruning_factor, options.Get<int>(kMinimumSmartPruningBatchesId)));
  }
}

namespace {
// Stoppers for uci mode only.
void PopulateCommonUciStoppers(ChainedSearchStopper* stopper,
                               const OptionsDict& options,
                               const GoParams& params, int64_t move_overhead) {
  const bool infinite = params.infinite || params.ponder;

  // RAM limit watching stopper.
  const auto cache_size_mb = options.Get<int>(kNNCacheSizeId);
  const int ram_limit = options.Get<int>(kRamLimitMbId);
  if (ram_limit) {
    stopper->AddStopper(std::make_unique<MemoryWatchingStopper>(
        cache_size_mb, ram_limit,
        options.Get<float>(kSmartPruningFactorId) > 0.0f));
  }

  // "go nodes" stopper.
  int64_t node_limit = 0;
  if (params.nodes) {
    if (options.Get<bool>(kNodesAsPlayoutsId)) {
      stopper->AddStopper(std::make_unique<PlayoutsStopper>(
          *params.nodes, options.Get<float>(kSmartPruningFactorId) > 0.0f));
    } else {
        node_limit = *params.nodes;
    }
  }
  // always limit nodes to avoid exceeding the limit 4000000000. That number is default when node_limit = 0)
  stopper->AddStopper(std::make_unique<VisitsStopper>(
        node_limit, options.Get<float>(kSmartPruningFactorId) > 0.0f));

  // "go movetime" stopper.
  if (params.movetime && !infinite) {
    stopper->AddStopper(
        std::make_unique<TimeLimitStopper>(*params.movetime - move_overhead));
  }

  // "go depth" stopper.
  if (params.depth) {
    stopper->AddStopper(std::make_unique<DepthStopper>(*params.depth));
  }

  // Add internal search tree stoppers when we want to automatically stop.
  if (!infinite) PopulateIntrinsicStoppers(stopper, options);
}

class CommonTimeManager : public TimeManager {
 public:
  CommonTimeManager(std::unique_ptr<TimeManager> child_mgr,
                    const OptionsDict& options, int64_t move_overhead)
      : child_mgr_(std::move(child_mgr)),
        options_(options),
        move_overhead_(move_overhead) {}

 private:
  std::unique_ptr<SearchStopper> GetStopper(const GoParams& params,
                                            const NodeTree& tree) override {
    auto result = std::make_unique<ChainedSearchStopper>();
    if (child_mgr_) result->AddStopper(child_mgr_->GetStopper(params, tree));
    PopulateCommonUciStoppers(result.get(), options_, params, move_overhead_);
    return result;
  }

  const std::unique_ptr<TimeManager> child_mgr_;
  const OptionsDict& options_;
  const int64_t move_overhead_;
};

}  // namespace

std::unique_ptr<TimeManager> MakeCommonTimeManager(
    std::unique_ptr<TimeManager> child_manager, const OptionsDict& options,
    int64_t move_overhead) {
  return std::make_unique<CommonTimeManager>(std::move(child_manager), options,
                                             move_overhead);
}

}  // namespace lczero

```

`src/mcts/stoppers/common.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include "mcts/stoppers/stoppers.h"
#include "utils/optionsdict.h"
#include "utils/optionsparser.h"

namespace lczero {

enum class RunType { kUci, kSelfplay };
void PopulateCommonStopperOptions(RunType for_what, OptionsParser* options);

// Option ID for a cache size. It's used from multiple places and there's no
// really nice place to declare, so let it be here.
extern const OptionId kNNCacheSizeId;

// Populates KLDGain and SmartPruning stoppers.
void PopulateIntrinsicStoppers(ChainedSearchStopper* stopper,
                               const OptionsDict& options);

std::unique_ptr<TimeManager> MakeCommonTimeManager(
    std::unique_ptr<TimeManager> child_manager, const OptionsDict& options,
    int64_t move_overhead);

}  // namespace lczero
```

`src/mcts/stoppers/factory.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "mcts/stoppers/factory.h"

#include <optional>

#include "factory.h"
#include "mcts/stoppers/alphazero.h"
#include "mcts/stoppers/legacy.h"
#include "mcts/stoppers/smooth.h"
#include "mcts/stoppers/stoppers.h"
#include "utils/exception.h"

namespace lczero {
namespace {

const OptionId kMoveOverheadId{
    "move-overhead", "MoveOverheadMs",
    "Amount of time, in milliseconds, that the engine subtracts from it's "
    "total available time (to compensate for slow connection, interprocess "
    "communication, etc)."};
const OptionId kTimeManagerId{
    "time-manager", "TimeManager",
    "Name and config of a time manager. "
    "Possible names are 'legacy' (default), 'smooth' and 'alphazero'."
    "See https://lc0.org/timemgr for configuration details."};
}  // namespace

void PopulateTimeManagementOptions(RunType for_what, OptionsParser* options) {
  PopulateCommonStopperOptions(for_what, options);
  if (for_what == RunType::kUci) {
    options->Add<IntOption>(kMoveOverheadId, 0, 100000000) = 200;
    options->Add<StringOption>(kTimeManagerId) = "legacy";
  }
}

std::unique_ptr<TimeManager> MakeTimeManager(const OptionsDict& options) {
  const int64_t move_overhead = options.Get<int>(kMoveOverheadId);

  OptionsDict tm_options;
  tm_options.AddSubdictFromString(options.Get<std::string>(kTimeManagerId));

  const auto managers = tm_options.ListSubdicts();

  std::unique_ptr<TimeManager> time_manager;
  if (managers.size() != 1) {
    throw Exception("Exactly one time manager should be specified, " +
                    std::to_string(managers.size()) + " specified instead.");
  }

  if (managers[0] == "legacy") {
    time_manager =
        MakeLegacyTimeManager(move_overhead, tm_options.GetSubdict("legacy"));
  } else if (managers[0] == "alphazero") {
    time_manager = MakeAlphazeroTimeManager(move_overhead,
                                            tm_options.GetSubdict("alphazero"));
  } else if (managers[0] == "smooth") {
    time_manager =
        MakeSmoothTimeManager(move_overhead, tm_options.GetSubdict("smooth"));
  }

  if (!time_manager) {
    throw Exception("Unknown time manager: [" + managers[0] + "]");
  }
  tm_options.CheckAllOptionsRead("");

  return MakeCommonTimeManager(std::move(time_manager), options, move_overhead);
}

}  // namespace lczero

```

`src/mcts/stoppers/factory.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2019-2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include "mcts/stoppers/common.h"
#include "mcts/stoppers/timemgr.h"
#include "utils/optionsdict.h"
#include "utils/optionsparser.h"

namespace lczero {

// Populates UCI/command line flags with time management options.
void PopulateTimeManagementOptions(RunType for_what, OptionsParser* options);

// Creates a new time manager for a new search.
std::unique_ptr<TimeManager> MakeTimeManager(const OptionsDict& dict);

}  // namespace lczero

```

`src/mcts/stoppers/legacy.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "mcts/stoppers/legacy.h"

#include "mcts/stoppers/stoppers.h"

namespace lczero {

float ComputeEstimatedMovesToGo(int ply, float midpoint, float steepness) {
  // An analysis of chess games shows that the distribution of game lengths
  // looks like a log-logistic distribution. The mean residual time function
  // calculates how many more moves are expected in the game given that we are
  // at the current ply. Given that this function can be expensive to compute,
  // we calculate the median residual time function instead. This is derived and
  // shown to be similar to the mean residual time in "Some Useful Properties of
  // Log-Logistic Random Variables for Health Care Simulations" (Clark &
  // El-Taha, 2015).
  // midpoint: The median length of games.
  // steepness: How quickly the function drops off from its maximum value,
  // around the midpoint.
  const float move = ply / 2.0f;
  return midpoint * std::pow(1 + 2 * std::pow(move / midpoint, steepness),
                             1 / steepness) -
         move;
}

namespace {

class LegacyStopper : public TimeLimitStopper {
 public:
  LegacyStopper(int64_t deadline_ms, int64_t* time_piggy_bank)
      : TimeLimitStopper(deadline_ms), time_piggy_bank_(time_piggy_bank) {}
  virtual void OnSearchDone(const IterationStats& stats) override {
    *time_piggy_bank_ += GetTimeLimitMs() - stats.time_since_movestart;
  }

 private:
  int64_t* const time_piggy_bank_;
};

class LegacyTimeManager : public TimeManager {
 public:
  LegacyTimeManager(int64_t move_overhead, const OptionsDict& params)
      : move_overhead_(move_overhead),
        slowmover_(params.GetOrDefault<float>("slowmover", 1.0f)),
        time_curve_midpoint_(
            params.GetOrDefault<float>("midpoint-move", 51.5f)),
        time_curve_steepness_(params.GetOrDefault<float>("steepness", 7.0f)),
        spend_saved_time_(params.GetOrDefault<float>("immediate-use", 1.0f)),
        book_ply_bonus_(params.GetOrDefault<float>("book-ply-bonus", 0.25f)) {}
  std::unique_ptr<SearchStopper> GetStopper(const GoParams& params,
                                            const NodeTree& tree) override;

 private:
  const int64_t move_overhead_;
  const float slowmover_;
  const float time_curve_midpoint_;
  const float time_curve_steepness_;
  const float spend_saved_time_;
  // When starting a game from a book, add bonus time per ply of the book.
  const float book_ply_bonus_;
  bool first_move_of_game_ = true;
  // No need to be atomic as only one thread will update it.
  int64_t time_spared_ms_ = 0;
};

std::unique_ptr<SearchStopper> LegacyTimeManager::GetStopper(
    const GoParams& params, const NodeTree& tree) {
  const Position& position = tree.HeadPosition();
  const bool is_black = position.IsBlackToMove();
  const std::optional<int64_t>& time = (is_black ? params.btime : params.wtime);
  // If no time limit is given, don't stop on this condition.
  if (params.infinite || params.ponder || !time) return nullptr;

  const std::optional<int64_t>& inc = is_black ? params.binc : params.winc;
  const int increment = inc ? std::max(int64_t(0), *inc) : 0;

  float movestogo = ComputeEstimatedMovesToGo(
      position.GetGamePly(), time_curve_midpoint_, time_curve_steepness_);

  // If the number of moves remaining until the time control are less than
  // the estimated number of moves left in the game, then use the number of
  // moves until the time control instead.
  if (params.movestogo &&
      *params.movestogo > 0 &&  // Ignore non-standard uci command.
      *params.movestogo < movestogo) {
    movestogo = *params.movestogo;
  }

  // Total time, including increments, until time control.
  auto total_moves_time =
      std::max(0.0f, *time + increment * (movestogo - 1) - move_overhead_);

  // If there is time spared from previous searches, the `time_to_squander` part
  // of it will be used immediately, remove that from planning.
  int time_to_squander = 0;
  if (time_spared_ms_ > 0) {
    total_moves_time = std::max(0.0f, total_moves_time - time_spared_ms_);
    time_to_squander = time_spared_ms_ * spend_saved_time_;
    time_spared_ms_ -= time_to_squander;
  }

  // Evenly split total time between all moves.
  float this_move_time = total_moves_time / movestogo;

  // Add bonus time per ply of the opening book to compensate starting from an
  // uncommon position without a tree to reuse.
  // Limit the bonus to max. 12 plies, which also prevents spending too much
  // time on the first move in resumed games.
  if (first_move_of_game_) {
    this_move_time *= (1.0f + book_ply_bonus_ *
                       std::min(12, position.GetGamePly()));
    first_move_of_game_ = false;
  }

  // Only extend thinking time with slowmover if smart pruning can potentially
  // reduce it.
  constexpr int kSmartPruningToleranceMs = 200;
  if (slowmover_ < 1.0 ||
      this_move_time * slowmover_ > kSmartPruningToleranceMs) {
    // If time is planned to be overused because of slowmover, remove excess
    // of that time from spared time.
    time_spared_ms_ -= this_move_time * (slowmover_ - 1);
    this_move_time *= slowmover_;
  }

  LOGFILE << "Budgeted time for the move: " << this_move_time << "ms(+"
          << time_to_squander << "ms to squander). Remaining time " << *time
          << "ms(-" << move_overhead_ << "ms overhead)";
  // Use `time_to_squander` time immediately.
  this_move_time += time_to_squander;

  // Make sure we don't exceed current time limit with what we calculated.
  auto deadline =
      std::min(static_cast<int64_t>(this_move_time), *time - move_overhead_);
  return std::make_unique<LegacyStopper>(deadline, &time_spared_ms_);
}

}  // namespace

std::unique_ptr<TimeManager> MakeLegacyTimeManager(int64_t move_overhead,
                                                   const OptionsDict& params) {
  return std::make_unique<LegacyTimeManager>(move_overhead, params);
}
}  // namespace lczero

```

`src/mcts/stoppers/legacy.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include "mcts/stoppers/timemgr.h"
#include "utils/optionsdict.h"

namespace lczero {

float ComputeEstimatedMovesToGo(int ply, float midpoint, float steepness);

std::unique_ptr<TimeManager> MakeLegacyTimeManager(int64_t move_overhead,
                                                   const OptionsDict& params);

}  // namespace lczero
```

`src/mcts/stoppers/smooth.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020-2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "mcts/stoppers/smooth.h"

#include <functional>
#include <iomanip>

#include "mcts/stoppers/legacy.h"
#include "mcts/stoppers/stoppers.h"
#include "utils/mutex.h"

namespace lczero {
namespace {

class Params {
 public:
  Params(const OptionsDict& /* params */, int64_t move_overhead);

  using MovesLeftEstimator = std::function<float(const NodeTree&)>;

  // Which fraction of the tree is reuse after a full move. Initial guess.
  float initial_tree_reuse() const { return initial_tree_reuse_; }
  // Do not allow tree reuse expectation to go above this value.
  float max_tree_reuse() const { return max_tree_reuse_; }
  // Number of moves needed to update tree reuse estimation halfway.
  float tree_reuse_halfupdate_moves() const {
    return tree_reuse_halfupdate_moves_;
  }
  // Number of seconds to update nps estimation.
  float nps_update_seconds() const { return nps_update_seconds_; }
  // Fraction of the allocated time the engine uses, initial estimation.
  float initial_smartpruning_timeuse() const {
    return initial_smartpruning_timeuse_;
  }
  // Do not allow timeuse estimation to fall below this.
  float min_smartpruning_timeuse() const { return min_smartpruning_timeuse_; }
  // Number of moves to update timeuse estimation halfway.
  float smartpruning_timeuse_halfupdate_moves() const {
    return smartpruning_timeuse_halfupdate_moves_;
  }
  // Fraction of a total available move time that is allowed to use for a single
  // move.
  float max_single_move_time_fraction() const {
    return max_single_move_time_fraction_;
  }
  // How much of the initial time of the game (without increments) is sent to a
  // piggybank.
  float initial_piggybank_fraction() const {
    return initial_piggybank_fraction_;
  }
  // How much of the move time (for every move) it taxed into a piggybank.
  float per_move_piggybank_fraction() const {
    return per_move_piggybank_fraction_;
  }
  // Maximum time in piggybank to use for a single move.
  float max_piggybank_use() const { return max_piggybank_use_; }

  // Max number of avg move times in piggybank.
  float max_piggybank_moves() const { return max_piggybank_moves_; }

  // Move overhead.
  int64_t move_overhead_ms() const { return move_overhead_ms_; }
  // Returns a function function that estimates remaining moves.
  MovesLeftEstimator moves_left_estimator() const {
    return moves_left_estimator_;
  }

 private:
  const int64_t move_overhead_ms_;
  const float initial_tree_reuse_;
  const float max_tree_reuse_;
  const float tree_reuse_halfupdate_moves_;
  const float nps_update_seconds_;
  const float initial_smartpruning_timeuse_;
  const float min_smartpruning_timeuse_;
  const float smartpruning_timeuse_halfupdate_moves_;
  const float max_single_move_time_fraction_;
  const float initial_piggybank_fraction_;
  const float per_move_piggybank_fraction_;
  const float max_piggybank_use_;
  const float max_piggybank_moves_;
  const MovesLeftEstimator moves_left_estimator_;
};

Params::MovesLeftEstimator CreateMovesLeftEstimator(const OptionsDict& params) {
  // The only estimator we have now is MLE-legacy (Moves left estimator).
  const OptionsDict& mle_dict = params.HasSubdict("mle-legacy")
                                    ? params.GetSubdict("mle-legacy")
                                    : params;
  return [midpoint = mle_dict.GetOrDefault<float>("midpoint", 45.2f),
          steepness = mle_dict.GetOrDefault<float>("steepness", 5.93f)](
             const NodeTree& tree) {
    const auto ply = tree.HeadPosition().GetGamePly();
    return ComputeEstimatedMovesToGo(ply, midpoint, steepness);
  };
}

Params::Params(const OptionsDict& params, int64_t move_overhead)
    : move_overhead_ms_(move_overhead),
      initial_tree_reuse_(params.GetOrDefault<float>("init-tree-reuse", 0.52f)),
      max_tree_reuse_(params.GetOrDefault<float>("max-tree-reuse", 0.73f)),
      tree_reuse_halfupdate_moves_(
          params.GetOrDefault<float>("tree-reuse-update-rate", 3.39f)),
      nps_update_seconds_(
          params.GetOrDefault<float>("nps-update-period", 20.0f)),
      initial_smartpruning_timeuse_(
          params.GetOrDefault<float>("init-timeuse", 0.7f)),
      min_smartpruning_timeuse_(
          params.GetOrDefault<float>("min-timeuse", 0.34f)),
      smartpruning_timeuse_halfupdate_moves_(
          params.GetOrDefault<float>("timeuse-update-rate", 5.51f)),
      max_single_move_time_fraction_(
          params.GetOrDefault<float>("max-move-budget", 0.42f)),
      initial_piggybank_fraction_(
          params.GetOrDefault<float>("init-piggybank", 0.09f)),
      per_move_piggybank_fraction_(
          params.GetOrDefault<float>("per-move-piggybank", 0.12f)),
      max_piggybank_use_(
          params.GetOrDefault<float>("max-piggybank-use", 0.94f)),
      max_piggybank_moves_(
          params.GetOrDefault<float>("max-piggybank-moves", 36.5f)),
      moves_left_estimator_(CreateMovesLeftEstimator(params)) {}

// Returns the updated value of @from, towards @to by the number of halves
// equal to number of @steps in @value. E.g. if value=1*step, returns
// (from+to)/2, if value=2*step, return (1*from + 3*to)/4, if
// value=3*step, return (1*from + 7*to)/7, if value=0, returns from.
float ExponentialDecay(float from, float to, float step, float value) {
  return to - (to - from) * std::pow(0.5f, value / step);
}

float LinearInterpolate(float a, float b, float theta) {
  if (theta <= 0) return a;
  if (theta >= 1) return b;
  return a + (b - a) * theta;
}

// @cur_value -- current "average", returned from the previous call of this
//   function.
// @target_value -- current "correct but noisy" value, that we slowly go to.
// @time_since_reliable -- time since we had reliable nps in target_value. For
//    the case of nps, end of the previous move is considered reliable point).
// @time_since_last_update_ms -- how long ago was this function last called.
// @window_size_ms -- in what time should the output converge to @target_value.
float LinearDecay(float cur_value, float target_value,
                  float time_since_reliable_ms, float time_since_last_update_ms,
                  float window_size_ms) {
  // If target is higher than cur value, just return target without any
  // smoothing. That makes sense for nps, but for other averages we should look
  // a bit closer.
  if (cur_value <= target_value) return target_value;
  // If no time passed since last update, return previous value.
  if (time_since_last_update_ms <= 0) return cur_value;
  // If window is large enough, we consider
  if (time_since_reliable_ms >= window_size_ms) return target_value;
  if (time_since_last_update_ms > time_since_reliable_ms) {
    // Should never happen, but just for the case.
    time_since_last_update_ms = time_since_reliable_ms;
  }
  // time_since_reliable_ms during the previous call of this function.
  const float prev_time_since_reliable_ms =
      time_since_reliable_ms - time_since_last_update_ms;
  // The same, but as fraction of window size.
  const float prev_reliable_frac = prev_time_since_reliable_ms / window_size_ms;
  // What was the value when it was reliable.
  const float reliable_value = (prev_reliable_frac * target_value - cur_value) /
                               (prev_reliable_frac - 1);
  // Now we know window width, nps when it was reliable, and time since that,
  // and time to converge, interpolate.
  return LinearInterpolate(reliable_value, target_value,
                           time_since_reliable_ms / window_size_ms);
}

class SmoothTimeManager;

class SmoothStopper : public SearchStopper {
 public:
  SmoothStopper(int64_t deadline_ms, int64_t allowed_piggybank_use_ms,
                SmoothTimeManager* manager);

 private:
  bool ShouldStop(const IterationStats& stats, StoppersHints* hints) override;
  void OnSearchDone(const IterationStats& stats) override;

  const int64_t deadline_ms_;
  const int64_t allowed_piggybank_use_ms_;

  SmoothTimeManager* const manager_;
  std::atomic_flag used_piggybank_;
};

class SmoothTimeManager : public TimeManager {
 public:
  SmoothTimeManager(int64_t move_overhead, const OptionsDict& params)
      : params_(params, move_overhead) {}

  float UpdateNps(int64_t time_since_movestart_ms,
                  int64_t nodes_since_movestart) {
    Mutex::Lock lock(mutex_);
    if (time_since_movestart_ms <= 0) return nps_;
    if (nps_is_reliable_ && time_since_movestart_ms > last_time_) {
      const float nps =
          1000.0f * nodes_since_movestart / time_since_movestart_ms;
      nps_ = LinearDecay(nps_, nps, time_since_movestart_ms,
                         time_since_movestart_ms - last_time_,
                         params_.nps_update_seconds() * 1000.0f);
    } else {
      nps_ = 1000.0f * nodes_since_movestart / time_since_movestart_ms;
    }
    last_time_ = time_since_movestart_ms;
    return nps_;
  }

  void UpdateEndOfMoveStats(int64_t total_move_time, bool used_piggybank,
                            int64_t time_budget, int64_t total_nodes) {
    Mutex::Lock lock(mutex_);
    // Whatever is in nps_ after the first move, is truth now.
    nps_is_reliable_ = true;
    // How different was this move from an average move
    const float this_move_time_fraction =
        avg_ms_per_move_ <= 0.0f ? 0.0f : total_move_time / avg_ms_per_move_;
    // Update time_use estimation.
    const float this_move_time_use =
        move_allocated_time_ms_ <= 0.0f
            ? 1.0f
            : total_move_time / move_allocated_time_ms_;
    // Recompute expected move time for logging.
    const float expected_move_time = move_allocated_time_ms_ * timeuse_;
    // If piggybank was used, cannot update timeuse_.
    int64_t piggybank_time_used = 0;
    if (used_piggybank) {
      piggybank_time_used = std::max(int64_t(), total_move_time - time_budget);
      piggybank_time_ -= piggybank_time_used;
    } else {
      timeuse_ =
          ExponentialDecay(timeuse_, this_move_time_use,
                           params_.smartpruning_timeuse_halfupdate_moves(),
                           this_move_time_fraction);
      if (timeuse_ < params_.min_smartpruning_timeuse()) {
        timeuse_ = params_.min_smartpruning_timeuse();
      }
    }
    // Remember final number of nodes for tree reuse estimation.
    last_move_final_nodes_ = total_nodes;

    LOGFILE << std::fixed << "TMGR: Updating endmove stats. actual_move_time="
            << total_move_time
            << "ms, allocated_move_time=" << move_allocated_time_ms_
            << "ms (ratio=" << this_move_time_use
            << "), expected_move_time=" << expected_move_time
            << "ms. New time_use=" << timeuse_
            << ", update_rate=" << this_move_time_fraction
            << " (avg_move_time=" << avg_ms_per_move_ << "ms)."
            << " piggybank_used=" << piggybank_time_used << "ms";
  }

 private:
  std::unique_ptr<SearchStopper> GetStopper(const GoParams& params,
                                            const NodeTree& tree) override {
    const Position& position = tree.HeadPosition();
    const bool is_black = position.IsBlackToMove();
    const std::optional<int64_t>& time =
        (is_black ? params.btime : params.wtime);
    // If no time limit is given, don't stop on this condition.
    if (params.infinite || params.ponder || !time) return nullptr;

    Mutex::Lock lock(mutex_);

    if (is_first_move_) {
      piggybank_time_ =
          static_cast<int64_t>(*time * params_.initial_piggybank_fraction());
      is_first_move_ = false;
    }

    const auto current_nodes = tree.GetCurrentHead()->GetN();
    if (last_move_final_nodes_ && last_time_ && avg_ms_per_move_ >= 0.0f) {
      UpdateTreeReuseFactor(current_nodes);
    }

    last_time_ = 0;

    // Get remaining moves estimation.
    float remaining_moves = params_.moves_left_estimator()(tree);

    // If the number of moves remaining until the time control are less than
    // the estimated number of moves left in the game, then use the number of
    // moves until the time control instead.
    if (params.movestogo &&
        *params.movestogo > 0 &&  // Ignore non-standard uci command.
        *params.movestogo < remaining_moves) {
      remaining_moves = *params.movestogo;
    }

    const std::optional<int64_t>& inc = is_black ? params.binc : params.winc;
    const int increment = inc ? std::max(int64_t(0), *inc) : 0;

    // Maximum time allowed in piggybank.
    const float max_piggybank_time =
        params_.max_piggybank_moves() *
        std::max(0.0f, *time + increment * (remaining_moves - 1) -
                           params_.move_overhead_ms()) /
        remaining_moves;

    if (piggybank_time_ > max_piggybank_time) {
      piggybank_time_ = max_piggybank_time;
    }

    // Total time, including increments, until time control.
    const auto total_remaining_ms = std::max(
        0.0f, *time - piggybank_time_ + increment * (remaining_moves - 1) -
                  params_.move_overhead_ms());

    // Total remaining nodes that we'll have chance to compute in a game.
    const float remaining_game_nodes = total_remaining_ms * nps_ / 1000.0f;
    // Total (fresh) nodes, in average, to processed per move.
    const float avg_nodes_per_move = remaining_game_nodes / remaining_moves;
    // Average time that will be spent per move.
    avg_ms_per_move_ = total_remaining_ms / remaining_moves;
    // As some part of a tree is usually reused, we can aim to a larger target.
    const float nodes_per_move_including_reuse =
        avg_nodes_per_move / (1.0f - tree_reuse_);
    // Subtract what we already have, and get what we need to compute.
    const float move_estimate_nodes =
        std::max(0.0f, nodes_per_move_including_reuse - current_nodes);
    // This is what time we think will be really spent thinking.
    const float expected_movetime_ms_brutto =
        move_estimate_nodes / nps_ * 1000.0f;
    // Share of this move that will go to piggybank.
    const float time_to_piggybank_ms = std::min(
        max_piggybank_time - piggybank_time_,
        expected_movetime_ms_brutto * params_.per_move_piggybank_fraction());
    // Some share of this time will go to a piggybank, so a move gets less.
    const float expected_movetime_ms =
        expected_movetime_ms_brutto - time_to_piggybank_ms;
    // When we need to use piggybank, we can use it.
    int64_t allowed_piggybank_time_ms =
        piggybank_time_ * params_.max_piggybank_use();
    // This is what is the actual budget as we hope that the search will be
    // shorter due to smart pruning.
    move_allocated_time_ms_ = expected_movetime_ms / timeuse_;

    if (move_allocated_time_ms_ >
        *time * params_.max_single_move_time_fraction()) {
      move_allocated_time_ms_ = *time * params_.max_single_move_time_fraction();
    }
    if (move_allocated_time_ms_ > *time - params_.move_overhead_ms()) {
      move_allocated_time_ms_ = std::max(static_cast<int64_t>(0LL),
                                         *time - params_.move_overhead_ms());
    }
    if (allowed_piggybank_time_ms >
        *time - params_.move_overhead_ms() - move_allocated_time_ms_) {
      allowed_piggybank_time_ms =
          *time - params_.move_overhead_ms() - move_allocated_time_ms_;
    }
    piggybank_time_ += time_to_piggybank_ms;

    LOGFILE << std::fixed << std::setprecision(1)
            << "TMGR: MOVE TIME: allocated()=" << move_allocated_time_ms_
            << "ms, expected(brutto)=" << expected_movetime_ms_brutto
            << "ms, expected(netto)=" << expected_movetime_ms
            << std::setprecision(5) << "ms, timeuse=" << timeuse_;

    LOGFILE << std::fixed << std::setprecision(1)
            << "TMGR: MOVE NODES: avg_total_with_reuse(brutto)="
            << nodes_per_move_including_reuse
            << ", avg_total_without_reuse(brutto)=" << avg_nodes_per_move
            << ", reused=" << current_nodes
            << ", expected_new(brutto)=" << move_estimate_nodes
            << std::setprecision(5) << ", tree_reuse=" << tree_reuse_;

    LOGFILE << std::fixed << std::setprecision(1)
            << "TMGR: PIGGYBANK: total=" << piggybank_time_
            << "ms, increment=" << time_to_piggybank_ms
            << "ms, longthink=" << allowed_piggybank_time_ms
            << "ms, max=" << max_piggybank_time << "ms";

    LOGFILE << std::fixed << std::setprecision(1)
            << "TMGR: REMAINING GAME: nodes=" << remaining_game_nodes
            << ", moves=" << remaining_moves << ", time=" << total_remaining_ms
            << "ms, nps=" << nps_;

    return std::make_unique<SmoothStopper>(move_allocated_time_ms_,
                                           allowed_piggybank_time_ms, this);
  }

  void UpdateTreeReuseFactor(int64_t new_move_nodes) REQUIRES(mutex_) {
    // How different was this move from an average move
    const float this_move_time_fraction =
        avg_ms_per_move_ <= 0.0f ? 0.0f : last_time_ / avg_ms_per_move_;

    const float this_move_tree_reuse =
        static_cast<float>(new_move_nodes) / last_move_final_nodes_;
    tree_reuse_ = ExponentialDecay(tree_reuse_, this_move_tree_reuse,
                                   params_.tree_reuse_halfupdate_moves(),
                                   this_move_time_fraction);
    if (tree_reuse_ > params_.max_tree_reuse()) {
      tree_reuse_ = params_.max_tree_reuse();
    }
    LOGFILE << std::fixed << "TMGR: Updating tree reuse. last_move_nodes="
            << last_move_final_nodes_ << ", this_move_nodes=" << new_move_nodes
            << " (tree_reuse=" << this_move_tree_reuse
            << "). avg_tree_reuse=" << tree_reuse_
            << ", update_rate=" << this_move_time_fraction
            << " (avg_move_time=" << avg_ms_per_move_
            << "ms, actual_move_time=" << last_time_ << "ms)";
  }

  const Params params_;

  Mutex mutex_;
  // Fraction of a tree which usually survives a full move (and is reused).
  float tree_reuse_ GUARDED_BY(mutex_) = params_.initial_tree_reuse();
  // Current NPS estimation.
  float nps_ GUARDED_BY(mutex_) = 20000.0f;
  // NPS is unreliable until the end of the first move.
  bool nps_is_reliable_ GUARDED_BY(mutex_) = false;
  // Fraction of a allocated time usually used.
  float timeuse_ GUARDED_BY(mutex_) = params_.initial_smartpruning_timeuse();

  // Average amount of time per move. Used to compute ratio for timeuse and
  // tree reuse updates.
  float avg_ms_per_move_ GUARDED_BY(mutex_) = 0.0f;
  // Total amount of time allocated for the current move. Used to update
  // timeuse_ when the move ends.
  float move_allocated_time_ms_ GUARDED_BY(mutex_) = 0.0f;
  // Total amount of nodes in the end of the previous search. Used to compute
  // tree reuse factor when a new search starts.
  int64_t last_move_final_nodes_ GUARDED_BY(mutex_) = 0;
  // Time of the last report, since the beginning of the move.
  int64_t last_time_ GUARDED_BY(mutex_) = 0;
  // Time in piggybank (to use when search requests it).
  int64_t piggybank_time_ GUARDED_BY(mutex_) = 0;
  // Tracks whether it's called for the first time, meaning there's a need to
  // do initialization.
  bool is_first_move_ GUARDED_BY(mutex_) = true;
};

SmoothStopper::SmoothStopper(int64_t deadline_ms,
                             int64_t allowed_piggybank_use_ms,
                             SmoothTimeManager* manager)
    : deadline_ms_(deadline_ms),
      allowed_piggybank_use_ms_(allowed_piggybank_use_ms),
      manager_(manager) {
  used_piggybank_.clear();
}

bool SmoothStopper::ShouldStop(const IterationStats& stats,
                               StoppersHints* hints) {
  const auto nps = manager_->UpdateNps(stats.time_since_first_batch,
                                       stats.nodes_since_movestart);
  if (stats.time_usage_hint_ == IterationStats::TimeUsageHint::kImmediateMove) {
    LOGFILE << "Search requested immediate stop, alrite.";
    return true;
  }

  const bool use_piggybank =
      (stats.time_usage_hint_ == IterationStats::TimeUsageHint::kNeedMoreTime);
  const int64_t time_limit =
      use_piggybank ? (deadline_ms_ + allowed_piggybank_use_ms_) : deadline_ms_;
  hints->UpdateEstimatedNps(nps);
  hints->UpdateEstimatedRemainingTimeMs(time_limit -
                                        stats.time_since_movestart);
  if (use_piggybank && stats.time_since_movestart >= deadline_ms_) {
    // It's not entirely correct as due to extended remaining time smart pruning
    // will trigger later and we spend more time than if use_piggyback was
    // false, even before reaching the deadline.
    used_piggybank_.test_and_set();
  }
  if (stats.time_since_movestart >= time_limit) {
    LOGFILE << "Stopping search: Ran out of time.";
    return true;
  }
  return false;
}

void SmoothStopper::OnSearchDone(const IterationStats& stats) {
  manager_->UpdateEndOfMoveStats(stats.time_since_movestart,
                                 used_piggybank_.test_and_set(), deadline_ms_,
                                 stats.total_nodes);
}

}  // namespace

std::unique_ptr<TimeManager> MakeSmoothTimeManager(int64_t move_overhead,
                                                   const OptionsDict& params) {
  return std::make_unique<SmoothTimeManager>(move_overhead, params);
}

}  // namespace lczero

```

`src/mcts/stoppers/smooth.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include "mcts/stoppers/timemgr.h"
#include "utils/optionsdict.h"

namespace lczero {

std::unique_ptr<TimeManager> MakeSmoothTimeManager(int64_t move_overhead,
                                                   const OptionsDict& params);

}  // namespace lczero
```

`src/mcts/stoppers/stoppers.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "mcts/stoppers/stoppers.h"

#include "mcts/node.h"
#include "neural/cache.h"

namespace lczero {

///////////////////////////
// ChainedSearchStopper
///////////////////////////

bool ChainedSearchStopper::ShouldStop(const IterationStats& stats,
                                      StoppersHints* hints) {
  for (const auto& x : stoppers_) {
    if (x->ShouldStop(stats, hints)) return true;
  }
  return false;
}

void ChainedSearchStopper::AddStopper(std::unique_ptr<SearchStopper> stopper) {
  if (stopper) stoppers_.push_back(std::move(stopper));
}

void ChainedSearchStopper::OnSearchDone(const IterationStats& stats) {
  for (const auto& x : stoppers_) x->OnSearchDone(stats);
}

///////////////////////////
// VisitsStopper
///////////////////////////

bool VisitsStopper::ShouldStop(const IterationStats& stats,
                               StoppersHints* hints) {
  if (populate_remaining_playouts_) {
    hints->UpdateEstimatedRemainingPlayouts(nodes_limit_ - stats.total_nodes);
  }
  if (stats.total_nodes >= nodes_limit_) {
    LOGFILE << "Stopped search: Reached visits limit: " << stats.total_nodes
            << ">=" << nodes_limit_;
    return true;
  }
  return false;
}

///////////////////////////
// PlayoutsStopper
///////////////////////////

bool PlayoutsStopper::ShouldStop(const IterationStats& stats,
                                 StoppersHints* hints) {
  if (populate_remaining_playouts_) {
    hints->UpdateEstimatedRemainingPlayouts(nodes_limit_ -
                                            stats.nodes_since_movestart);
  }
  if (stats.nodes_since_movestart >= nodes_limit_) {
    LOGFILE << "Stopped search: Reached playouts limit: "
            << stats.nodes_since_movestart << ">=" << nodes_limit_;
    return true;
  }
  return false;
}

///////////////////////////
// MemoryWatchingStopper
///////////////////////////

namespace {
const size_t kAvgNodeSize =
    sizeof(Node) + MemoryWatchingStopper::kAvgMovesPerPosition * sizeof(Edge);
const size_t kAvgCacheItemSize =
    NNCache::GetItemStructSize() + sizeof(CachedNNRequest) +
    sizeof(CachedNNRequest::IdxAndProb) *
        MemoryWatchingStopper::kAvgMovesPerPosition;
}  // namespace

MemoryWatchingStopper::MemoryWatchingStopper(int cache_size, int ram_limit_mb,
                                             bool populate_remaining_playouts)
    : VisitsStopper(
          (ram_limit_mb * 1000000LL - cache_size * kAvgCacheItemSize) /
              kAvgNodeSize,
          populate_remaining_playouts) {
  LOGFILE << "RAM limit " << ram_limit_mb << "MB. Cache takes "
          << cache_size * kAvgCacheItemSize / 1000000
          << "MB. Remaining memory is enough for " << GetVisitsLimit()
          << " nodes.";
}

///////////////////////////
// TimelimitStopper
///////////////////////////

TimeLimitStopper::TimeLimitStopper(int64_t time_limit_ms)
    : time_limit_ms_(time_limit_ms) {}

bool TimeLimitStopper::ShouldStop(const IterationStats& stats,
                                  StoppersHints* hints) {
  hints->UpdateEstimatedRemainingTimeMs(time_limit_ms_ -
                                        stats.time_since_movestart);
  if (stats.time_since_movestart >= time_limit_ms_) {
    LOGFILE << "Stopping search: Ran out of time.";
    return true;
  }
  return false;
}

int64_t TimeLimitStopper::GetTimeLimitMs() const { return time_limit_ms_; }

///////////////////////////
// DepthStopper
///////////////////////////
bool DepthStopper::ShouldStop(const IterationStats& stats, StoppersHints*) {
  if (stats.average_depth >= depth_) {
    LOGFILE << "Stopped search: Reached depth.";
    return true;
  }
  return false;
}

///////////////////////////
// KldGainStopper
///////////////////////////

KldGainStopper::KldGainStopper(float min_gain, int average_interval)
    : min_gain_(min_gain), average_interval_(average_interval) {}

bool KldGainStopper::ShouldStop(const IterationStats& stats, StoppersHints*) {
  Mutex::Lock lock(mutex_);
  const auto new_child_nodes = stats.total_nodes - 1.0;
  if (new_child_nodes < prev_child_nodes_ + average_interval_) return false;

  const auto new_visits = stats.edge_n;
  if (!prev_visits_.empty()) {
    double kldgain = 0.0;
    for (decltype(new_visits)::size_type i = 0; i < new_visits.size(); i++) {
      double o_p = prev_visits_[i] / prev_child_nodes_;
      double n_p = new_visits[i] / new_child_nodes;
      if (prev_visits_[i] != 0) kldgain += o_p * log(o_p / n_p);
    }
    if (kldgain / (new_child_nodes - prev_child_nodes_) < min_gain_) {
      LOGFILE << "Stopping search: KLDGain per node too small.";
      return true;
    }
  }
  prev_visits_ = new_visits;
  prev_child_nodes_ = new_child_nodes;
  return false;
}

///////////////////////////
// SmartPruningStopper
///////////////////////////

namespace {
const int kSmartPruningToleranceMs = 200;
const int kSmartPruningToleranceNodes = 300;
}  // namespace

SmartPruningStopper::SmartPruningStopper(float smart_pruning_factor,
                                         int64_t minimum_batches)
    : smart_pruning_factor_(smart_pruning_factor),
      minimum_batches_(minimum_batches) {}

bool SmartPruningStopper::ShouldStop(const IterationStats& stats,
                                     StoppersHints* hints) {
  if (smart_pruning_factor_ <= 0.0) return false;
  Mutex::Lock lock(mutex_);
  if (stats.edge_n.size() == 1) {
    LOGFILE << "Only one possible move. Moving immediately.";
    return true;
  }
  if (stats.edge_n.size() <= static_cast<size_t>(stats.num_losing_edges + 1)) {
    LOGFILE << "At most one non losing move, stopping search.";
    return true;
  }
  if (stats.win_found) {
    LOGFILE << "Terminal win found, stopping search.";
    return true;
  }
  if (stats.nodes_since_movestart > 0 && !first_eval_time_) {
    first_eval_time_ = stats.time_since_movestart;
    return false;
  }
  if (!first_eval_time_) return false;
  if (stats.edge_n.size() == 0) return false;
  if (stats.time_since_movestart <
      *first_eval_time_ + kSmartPruningToleranceMs) {
    return false;
  }

  const auto nodes = stats.nodes_since_movestart + kSmartPruningToleranceNodes;
  const auto time = stats.time_since_movestart - *first_eval_time_;
  // If nps is populated by someone who knows better, use it. Otherwise use the
  // value calculated here.
  const auto nps = hints->GetEstimatedNps().value_or(1000LL * nodes / time + 1);

  const double remaining_time_s = hints->GetEstimatedRemainingTimeMs() / 1000.0;
  const auto remaining_playouts =
      std::min(remaining_time_s * nps / smart_pruning_factor_,
               hints->GetEstimatedRemainingPlayouts() / smart_pruning_factor_);

  // May overflow if (nps/smart_pruning_factor) > 180 000 000, but that's not
  // very realistic.
  hints->UpdateEstimatedRemainingPlayouts(remaining_playouts);
  if (stats.batches_since_movestart < minimum_batches_) return false;

  uint32_t largest_n = 0;
  uint32_t second_largest_n = 0;
  for (auto n : stats.edge_n) {
    if (n > largest_n) {
      second_largest_n = largest_n;
      largest_n = n;
    } else if (n > second_largest_n) {
      second_largest_n = n;
    }
  }

  if (remaining_playouts < (largest_n - second_largest_n)) {
    LOGFILE << remaining_playouts << " playouts remaining. Best move has "
            << largest_n << " visits, second best -- " << second_largest_n
            << ". Difference is " << (largest_n - second_largest_n)
            << ", so stopping the search after "
            << stats.batches_since_movestart << " batches.";

    return true;
  }

  return false;
}

}  // namespace lczero

```

`src/mcts/stoppers/stoppers.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <optional>
#include <vector>

#include "mcts/node.h"
#include "mcts/stoppers/timemgr.h"

namespace lczero {

// Combines multiple stoppers into one.
class ChainedSearchStopper : public SearchStopper {
 public:
  ChainedSearchStopper() = default;
  // Calls stoppers one by one until one of them returns true. If one of
  // stoppers modifies hints, next stoppers in the chain see that.
  bool ShouldStop(const IterationStats&, StoppersHints*) override;
  // Can be nullptr, in that canse stopper is not added.
  void AddStopper(std::unique_ptr<SearchStopper> stopper);
  void OnSearchDone(const IterationStats&) override;

 private:
  std::vector<std::unique_ptr<SearchStopper>> stoppers_;
};

// Watches visits (total tree nodes) and predicts remaining visits.
class VisitsStopper : public SearchStopper {
 public:
  VisitsStopper(int64_t limit, bool populate_remaining_playouts)
    : nodes_limit_(limit ? limit : 4000000000ll),
        populate_remaining_playouts_(populate_remaining_playouts) {}
  int64_t GetVisitsLimit() const { return nodes_limit_; }
  bool ShouldStop(const IterationStats&, StoppersHints*) override;

 private:
  const int64_t nodes_limit_;
  const bool populate_remaining_playouts_;
};

// Watches playouts (new tree nodes) and predicts remaining visits.
class PlayoutsStopper : public SearchStopper {
 public:
  PlayoutsStopper(int64_t limit, bool populate_remaining_playouts)
      : nodes_limit_(limit),
        populate_remaining_playouts_(populate_remaining_playouts) {}
  int64_t GetVisitsLimit() const { return nodes_limit_; }
  bool ShouldStop(const IterationStats&, StoppersHints*) override;

 private:
  const int64_t nodes_limit_;
  const bool populate_remaining_playouts_;
};

// Computes tree size which may fit into the memory and limits by that tree
// size.
class MemoryWatchingStopper : public VisitsStopper {
 public:
  // Must be in sync with description at kRamLimitMbId.
  static constexpr size_t kAvgMovesPerPosition = 30;
  MemoryWatchingStopper(int cache_size, int ram_limit_mb,
                        bool populate_remaining_playouts);
};

// Stops after time budget is gone.
class TimeLimitStopper : public SearchStopper {
 public:
  TimeLimitStopper(int64_t time_limit_ms);
  bool ShouldStop(const IterationStats&, StoppersHints*) override;

 protected:
  int64_t GetTimeLimitMs() const;

 private:
  const int64_t time_limit_ms_;
};

// Stops when certain average depth is reached (who needs that?).
class DepthStopper : public SearchStopper {
 public:
  DepthStopper(int depth) : depth_(depth) {}
  bool ShouldStop(const IterationStats&, StoppersHints*) override;

 private:
  const int depth_;
};

// Stops when search doesn't bring required KLD gain.
class KldGainStopper : public SearchStopper {
 public:
  KldGainStopper(float min_gain, int average_interval);
  bool ShouldStop(const IterationStats&, StoppersHints*) override;

 private:
  const double min_gain_;
  const int average_interval_;
  Mutex mutex_;
  std::vector<uint32_t> prev_visits_ GUARDED_BY(mutex_);
  double prev_child_nodes_ GUARDED_BY(mutex_) = 0.0;
};

// Does many things:
// Computes how many nodes are remaining (from remaining time/nodes, scaled by
// smart pruning factor). When this amount of nodes is not enough for second
// best move to potentially become the best one, stop the search.
class SmartPruningStopper : public SearchStopper {
 public:
  SmartPruningStopper(float smart_pruning_factor, int64_t minimum_batches);
  bool ShouldStop(const IterationStats&, StoppersHints*) override;

 private:
  const double smart_pruning_factor_;
  const int64_t minimum_batches_;
  Mutex mutex_;
  std::optional<int64_t> first_eval_time_ GUARDED_BY(mutex_);
};

}  // namespace lczero

```

`src/mcts/stoppers/timemgr.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "mcts/stoppers/timemgr.h"

#include "mcts/stoppers/stoppers.h"

namespace lczero {

StoppersHints::StoppersHints() { Reset(); }

void StoppersHints::UpdateEstimatedRemainingTimeMs(int64_t v) {
  if (v < remaining_time_ms_) remaining_time_ms_ = v;
}
int64_t StoppersHints::GetEstimatedRemainingTimeMs() const {
  return remaining_time_ms_;
}

void StoppersHints::UpdateEstimatedRemainingPlayouts(int64_t v) {
  if (v < remaining_playouts_) remaining_playouts_ = v;
}
int64_t StoppersHints::GetEstimatedRemainingPlayouts() const {
  // Even if we exceeded limits, don't go crazy by not allowing any playouts.
  return std::max(decltype(remaining_playouts_){1}, remaining_playouts_);
}

void StoppersHints::UpdateEstimatedNps(float v) { estimated_nps_ = v; }

std::optional<float> StoppersHints::GetEstimatedNps() const {
  return estimated_nps_;
}

void StoppersHints::Reset() {
  // Slightly more than 3 years.
  remaining_time_ms_ = 100000000000;
  // Type for N in nodes is currently uint32_t, so set limit in order not to
  // overflow it.
  remaining_playouts_ = 4000000000;
  // NPS is not known.
  estimated_nps_.reset();
}

}  // namespace lczero
```

`src/mcts/stoppers/timemgr.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <chrono>
#include <memory>
#include <optional>
#include <vector>

#include "chess/uciloop.h"
#include "mcts/node.h"
#include "utils/optionsdict.h"

namespace lczero {

// Various statistics that search sends to stoppers for their stopping decision.
// It is expected that this structure will grow.
struct IterationStats {
  int64_t time_since_movestart = 0;
  int64_t time_since_first_batch = 0;
  int64_t total_nodes = 0;
  int64_t nodes_since_movestart = 0;
  int64_t batches_since_movestart = 0;
  int average_depth = 0;
  std::vector<uint32_t> edge_n;

  // TODO: remove this in favor of time_usage_hint_=kImmediateMove when
  // smooth time manager is the default.
  bool win_found = false;
  int num_losing_edges = 0;

  enum class TimeUsageHint { kNormal, kNeedMoreTime, kImmediateMove };
  TimeUsageHint time_usage_hint_ = TimeUsageHint::kNormal;
};

// Hints from stoppers back to the search engine. Currently include:
// 1. EstimatedRemainingTime -- for search watchdog thread to know when to
// expect running out of time.
// 2. EstimatedPlayouts -- for smart pruning at root (not pick root nodes that
// cannot potentially become good).
class StoppersHints {
 public:
  StoppersHints();
  void Reset();
  void UpdateEstimatedRemainingTimeMs(int64_t v);
  int64_t GetEstimatedRemainingTimeMs() const;
  void UpdateEstimatedRemainingPlayouts(int64_t v);
  int64_t GetEstimatedRemainingPlayouts() const;
  void UpdateEstimatedNps(float v);
  std::optional<float> GetEstimatedNps() const;

 private:
  int64_t remaining_time_ms_;
  int64_t remaining_playouts_;
  std::optional<float> estimated_nps_;
};

// Interface for search stopper.
// Note that:
// 1. Stoppers are shared between all search threads, so if stopper has mutable
// varibles, it has to think about concurrency (mutex/atomics)
// (maybe in future it will be changed).
// 2. IterationStats and StoppersHints are per search thread, so access to
// them is fine without synchronization.
// 3. OnSearchDone is guaranteed to be called once (i.e. from only one thread).
class SearchStopper {
 public:
  virtual ~SearchStopper() = default;
  // Question to a stopper whether search should stop.
  // Search statistics is sent via IterationStats, the stopper can optionally
  // send hints to the search through StoppersHints.
  virtual bool ShouldStop(const IterationStats&, StoppersHints*) = 0;
  // Is called when search is done.
  virtual void OnSearchDone(const IterationStats&) {}
};

class TimeManager {
 public:
  virtual ~TimeManager() = default;
  virtual std::unique_ptr<SearchStopper> GetStopper(const GoParams& params,
                                                    const NodeTree& tree) = 0;
};

}  // namespace lczero

```

`src/neural/blas/README.md`:

```md
The files in this directory comprise the BLAS backend of Lc0.

## License

Leela Chess is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Leela Chess is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

**The source files of this directory are not covered by any additional
permission.**



```

`src/neural/blas/blas.h`:

```h
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018-2019 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

// Select the BLAS vendor based on defines

#ifdef USE_MKL
#include <mkl.h>
#else

#ifdef USE_OPENBLAS
#include <cblas.h>

// Specific openblas routines.
extern "C" {
int openblas_get_num_procs(void);
void openblas_set_num_threads(int num_threads);
char* openblas_get_corename(void);
char* openblas_get_config(void);
}

#else

#ifdef __APPLE__
#include <Accelerate/Accelerate.h>
#define USE_ACCELERATE

#else

#ifdef USE_DNNL
#include <dnnl.h>

// Implement the cblas subset needed using dnnl_sgemm().
extern "C" {
#define CblasRowMajor 0
#define CblasColMajor 1
#define CblasNoTrans 'N'
#define CblasTrans 'T'
static inline void cblas_sgemm(char order, char transa, char transb,
                               dnnl_dim_t M, dnnl_dim_t N, dnnl_dim_t K,
                               float alpha, const float *A, dnnl_dim_t lda,
                               const float *B, dnnl_dim_t ldb, float beta,
                               float *C, dnnl_dim_t ldc) {
  // DNNL only has row major sgemm.
  if (order == CblasRowMajor) {
    dnnl_sgemm(transa, transb, M, N, K, alpha, A, lda, B, ldb, beta, C, ldc);
  } else {
    dnnl_sgemm(transb, transa, N, M, K, alpha, B, ldb, A, lda, beta, C, ldc);
  }
}

static inline void cblas_sgemv(char order, char transa, dnnl_dim_t M,
                               dnnl_dim_t N, float alpha, const float *A,
                               dnnl_dim_t lda, const float *x, dnnl_dim_t incx,
                               float beta, float *y, dnnl_dim_t incy) {
  cblas_sgemm(order, transa, 'N', M, 1, N, alpha, A, lda, x, incx, beta, y,
              incy);
}

static inline float cblas_sdot(dnnl_dim_t N, const float *x, dnnl_dim_t incx,
                               const float *y, dnnl_dim_t incy) {
  float r = 0;
  dnnl_sgemm('T', 'N', 1, 1, N, 1.0, x, incx, y, incy, 0.0, &r, 1);
  return r;
}
}

#endif

#endif  // __APPLE__

#endif  // USE_OPENBLAS

#endif  // USE_MKL

```

`src/neural/blas/convolution1.cc`:

```cc
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#include "neural/blas/convolution1.h"
#include "neural/blas/blas.h"

#include <Eigen/Dense>

namespace lczero {
template <typename T>
using EigenMatrixMap =
    Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>;
template <typename T>
using ConstEigenMatrixMap =
    Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>;

#ifdef USE_BLAS
template <>
void Convolution1<false>::Forward(const size_t batch_size,
                                  const size_t input_channels,
                                  const size_t output_channels,
                                  const float* input, const float* weights,
                                  float* output) {
  for (size_t i = 0; i < batch_size; i++) {
    // C←αAB + βC
    // M Number of rows in matrices A and C.
    // N Number of columns in matrices B and C.
    // K Number of columns in matrix A; number of rows in matrix B.
    // lda The size of the first dimension of matrix A; if you are
    // passing a matrix A[m][n], the value should be m.
    //    cblas_sgemm(CblasRowMajor, TransA, TransB, M, N, K, alpha, A, lda, B,
    //                ldb, beta, C, N);

    //             C                          A                     B
    //
    //           outputs       :=          weights        x      input
    //
    //   cols:  kSquares (N)         input_channels (K)        kSquares(N)
    //
    //   rows:  output_channels (M)   output_channels (M)  input_channels (K)

    const float* batch_input = input + i * kSquares * input_channels;
    float* batch_output = output + i * kSquares * output_channels;
    cblas_sgemm(CblasRowMajor,         // Row major formar
                CblasNoTrans,          // A not transposed
                CblasNoTrans,          // B not transposed
                (int)output_channels,  // M
                kSquares,              // N
                (int)input_channels,   // K
                1.0f,                  // Alpha
                weights,               // A
                (int)input_channels,   // lda, leading rank of A
                batch_input,           // B
                kSquares,              // ldb, leading rank of B
                0.0f,                  // beta
                batch_output,          // C
                kSquares);             // ldc, leading rank of B
  }
}
#endif

template <>
void Convolution1<true>::Forward(const size_t batch_size,
                                 const size_t input_channels,
                                 const size_t output_channels,
                                 const float* input, const float* weights,
                                 float* output) {
  for (size_t i = 0; i < batch_size; i++) {
    const float* batch_input = input + i * kSquares * input_channels;
    float* batch_output = output + i * kSquares * output_channels;
    auto C_mat = EigenMatrixMap<float>(batch_output, kSquares, output_channels);
    C_mat.noalias() =
        ConstEigenMatrixMap<float>(batch_input, kSquares, input_channels) *
        ConstEigenMatrixMap<float>(weights, input_channels, output_channels);
  }
}

}  // namespace lczero

```

`src/neural/blas/convolution1.h`:

```h
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include <cstddef>
#include <vector>

namespace lczero {

// Convolution 1x1
template <bool use_eigen>
class Convolution1 {
 public:
  Convolution1() = delete;

  // Batched forward inference.
  static void Forward(const size_t batch_size, const size_t input_channels,
                      const size_t output_channels, const float* input,
                      const float* weights, float* output);

 private:
  static constexpr auto kWidth = 8;
  static constexpr auto kHeight = 8;
  static constexpr auto kSquares = kWidth * kHeight;
};
}  // namespace lczero

```

`src/neural/blas/fully_connected_layer.cc`:

```cc
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018-2022 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#include "neural/blas/fully_connected_layer.h"
#include "neural/blas/blas.h"
#include "neural/shared/activation.h"

#include <algorithm>
#include <cassert>
#include <cmath>

#include <Eigen/Dense>

namespace lczero {
namespace {
void ApplyBias(size_t batch_size, const size_t output_size, const float* biases,
               const ActivationFunction activation, float* outputs) {
  for (size_t i = 0; i < batch_size; i++) {
    float* batch_outputs = outputs + i * output_size;
    Activate(output_size, batch_outputs, biases, batch_outputs, activation);
  }
}
}  // namespace

template <typename T>
using EigenVectorMap = Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, 1>>;
template <typename T>
using ConstEigenVectorMap =
    Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, 1>>;
template <typename T>
using EigenMatrixMap =
    Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>;
template <typename T>
using ConstEigenMatrixMap =
    Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>;

#ifdef USE_BLAS
template <>
void FullyConnectedLayer<false>::Forward1D(
    size_t batch_size, const size_t input_size, const size_t output_size,
    const float* inputs, const float* weights, const float* biases,
    const ActivationFunction activation, float* outputs) {
  if (batch_size == 1) {
    // Just a matrix-vector multiplication
    //
    //             C                A                     B
    //
    //         outputs    :=     weights      x       inputs
    //
    //   cols:   1               input_size            1
    //
    //   rows  output_size      output_size          input_size
    //
    cblas_sgemv(CblasRowMajor, CblasNoTrans,
                // M     K
                (int)output_size, (int)input_size, 1.0f, weights,
                (int)input_size, inputs, 1, 0.0f, outputs, 1);
  } else {
    // more columns, matrix-matrix multiplication
    //
    //             C                     A                         B
    //
    //            outputs      :=       weights        x         inputs
    //
    //   cols:   batch_size (N)       input_size  (K)          batch_size (N)
    //
    //   rows  output_size (M)        output_size (M)         input_size (K)
    //

    // C←αAB + βC
    // M Number of rows in matrices A and C.
    // N Number of columns in matrices B and C.
    // K Number of columns in matrix A; number of rows in matrix B.
    // lda The size of the first dimension of matrix A; if you are
    // passing a matrix A[m][n], the value should be m.
    //    cblas_sgemm(CblasRowMajor, TransA, TransB, M, N, K, alpha, A, lda, B,
    //                ldb, beta, C, N);
    cblas_sgemm(CblasColMajor, CblasTrans, CblasNoTrans,
                (int)output_size,   // M
                (int)batch_size,    // N
                (int)input_size,    // K
                1.0f,               // alpha
                weights,            // A
                (int)input_size,    // lda, leading rank of A
                inputs,             // B
                (int)input_size,    // ldb, leading rank of B
                0.0f,               // beta
                outputs,            // C
                (int)output_size);  // ldc, leading rank of C
  }
  ApplyBias(batch_size, output_size, biases, activation, outputs);
}

template <>
float FullyConnectedLayer<false>::Forward0D(const size_t size, const float* x,
                                            const float* y) {
  // A scalar product, also known as a dot-product.
  // float cblas_sdot(const int N, const float *X, const int incX, const float
  // *Y,
  // const int incY);
  return cblas_sdot((int)size, x, 1, y, 1);
}
#endif

template <>
void FullyConnectedLayer<true>::Forward1D(
    size_t batch_size, const size_t input_size, const size_t output_size,
    const float* inputs, const float* weights, const float* biases,
    const ActivationFunction activation, float* outputs) {
  if (batch_size == 1) {
    EigenVectorMap<float> y(outputs, output_size);
    y.noalias() = ConstEigenMatrixMap<float>(weights, input_size, output_size)
                      .transpose() *
                  ConstEigenVectorMap<float>(inputs, input_size);
  } else {
    auto C_mat = EigenMatrixMap<float>(outputs, output_size, batch_size);
    C_mat.noalias() =
        ConstEigenMatrixMap<float>(weights, input_size, output_size)
            .transpose() *
        ConstEigenMatrixMap<float>(inputs, input_size, batch_size);
  }
  ApplyBias(batch_size, output_size, biases, activation, outputs);
}

template <>
float FullyConnectedLayer<true>::Forward0D(const size_t size, const float* x,
                                           const float* y) {
  return ConstEigenVectorMap<float>(x, size).dot(
      ConstEigenVectorMap<float>(y, size));
}

}  // namespace lczero

```

`src/neural/blas/fully_connected_layer.h`:

```h
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include "neural/shared/activation.h"

#include <cstddef>
#include <vector>

namespace lczero {

template <bool use_eigen>
class FullyConnectedLayer {
 public:
  FullyConnectedLayer() = delete;

  // Forward inference, batched, from input_size to output_size
  static void Forward1D(const size_t batch_size, const size_t input_size,
                        const size_t output_size, const float* input,
                        const float* weights, const float* biases,
                        const ActivationFunction activation, float* output);

  // Forward inference, no batched, from input_size to scalar
  static float Forward0D(const size_t input_size, const float* input,
                         const float* weights);

};

}  // namespace lczero

```

`src/neural/blas/network_blas.cc`:

```cc
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018-2022 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#include <Eigen/Core>
#include <algorithm>
#include <cassert>
#include <cmath>
#include <iostream>

#include "neural/blas/blas.h"
#include "neural/blas/convolution1.h"
#include "neural/blas/fully_connected_layer.h"
#include "neural/blas/se_unit.h"
#include "neural/blas/winograd_convolution3.h"
#include "neural/factory.h"
#include "neural/network.h"
#include "neural/network_legacy.h"
#include "neural/shared/activation.h"
#include "neural/shared/attention_policy_map.h"
#include "neural/shared/policy_map.h"
#include "neural/shared/winograd_filter.h"

#ifdef USE_DNNL
#include <omp.h>
#endif

namespace lczero {
namespace {

template <bool use_eigen>
class BlasComputation : public NetworkComputation {
 public:
  BlasComputation(const LegacyWeights& weights, const size_t max_batch_size,
                  const bool wdl, const bool moves_left, const bool conv_policy,
                  const ActivationFunction default_activation,
                  const bool attn_policy);

  virtual ~BlasComputation() {}

  // Adds a sample to the batch.
  void AddInput(InputPlanes&& input) override { planes_.emplace_back(input); }

  // Do the computation.
  void ComputeBlocking() override;

  // Returns how many times AddInput() was called.
  int GetBatchSize() const override { return static_cast<int>(planes_.size()); }

  // Returns Q value of @sample.
  float GetQVal(int sample) const override {
    if (wdl_) {
      auto w = q_values_[3 * sample + 0];
      auto l = q_values_[3 * sample + 2];
      return w - l;
    } else {
      return q_values_[sample];
    }
  }

  float GetDVal(int sample) const override {
    if (wdl_) {
      auto d = q_values_[3 * sample + 1];
      return d;
    } else {
      return 0.0f;
    }
  }

  float GetMVal(int sample) const override {
    if (moves_left_) {
      return m_values_[sample];
    } else {
      return 0.0f;
    }
  }

  // Returns P value @move_id of @sample.
  float GetPVal(int sample, int move_id) const override {
    return policies_[sample][move_id];
  }

 private:
  void EncodePlanes(const InputPlanes& sample, float* buffer);

  static constexpr auto kWidth = 8;
  static constexpr auto kHeight = 8;
  static constexpr auto kSquares = kWidth * kHeight;
  static constexpr auto kPolicyOutputs = 1858;
  // Number of used planes with convolutional policy.
  // The real number of planes is higher because of padding.
  static constexpr auto kPolicyUsedPlanes = 73;

  const LegacyWeights& weights_;
  size_t max_batch_size_;
  std::vector<InputPlanes> planes_;
  std::vector<std::vector<float>> policies_;
  std::vector<float> q_values_;
  std::vector<float> m_values_;
  bool wdl_;
  bool moves_left_;
  bool conv_policy_;
  ActivationFunction default_activation_;
  bool attn_policy_;
};

template <bool use_eigen>
class BlasNetwork : public Network {
 public:
  BlasNetwork(const WeightsFile& weights, const OptionsDict& options);
  virtual ~BlasNetwork(){};

  std::unique_ptr<NetworkComputation> NewComputation() override {
    return std::make_unique<BlasComputation<use_eigen>>(
        weights_, max_batch_size_, wdl_, moves_left_, conv_policy_,
        default_activation_, attn_policy_);
  }

  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

 private:
  // A cap on the max batch size since it consumes a lot of memory
  static constexpr auto kHardMaxBatchSize = 2048;

  const NetworkCapabilities capabilities_;
  LegacyWeights weights_;
  size_t max_batch_size_;
  bool wdl_;
  bool moves_left_;
  bool conv_policy_;
  ActivationFunction default_activation_;
  bool attn_policy_;
};

template <bool use_eigen>
BlasComputation<use_eigen>::BlasComputation(
    const LegacyWeights& weights, const size_t max_batch_size, const bool wdl,
    const bool moves_left, const bool conv_policy,
    const ActivationFunction default_activation, const bool attn_policy)
    : weights_(weights),
      max_batch_size_(max_batch_size),
      policies_(0),
      q_values_(0),
      wdl_(wdl),
      moves_left_(moves_left),
      conv_policy_(conv_policy),
      default_activation_(default_activation),
      attn_policy_(attn_policy) {
#ifdef USE_DNNL
  omp_set_num_threads(1);
#endif
}

template <typename T>
using EigenMatrixMap =
    Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>;
template <typename T>
using ConstEigenMatrixMap =
    Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>;

template <bool use_eigen>
void BlasComputation<use_eigen>::ComputeBlocking() {
  // Retrieve network key dimensions from the weights structure.
  const auto num_value_channels = weights_.ip1_val_b.size();
  const auto num_moves_channels = weights_.ip1_mov_b.size();
  const auto num_value_input_planes = weights_.value.biases.size();
  const auto num_policy_input_planes = weights_.policy.biases.size();
  const auto num_moves_input_planes = weights_.moves_left.biases.size();
  const auto num_output_policy = static_cast<size_t>(kPolicyOutputs);
  const auto output_channels = weights_.input.biases.size();

  // max_channels is the maximum number of input channels of any
  // convolution.
  // Residual blocks are identical, but the first convolution might be bigger
  // when the network has very few filters
  const auto input_channels = static_cast<size_t>(kInputPlanes);
  const auto max_channels = std::max(output_channels, input_channels);

  // The policy head may increase convolution max output size.
  const auto max_output_channels =
      (conv_policy_ && weights_.policy.biases.size() > output_channels)
          ? weights_.policy.biases.size()
          : output_channels;

  // Determine the largest batch for allocations.
  const auto plane_count = planes_.size();
  const auto largest_batch_size = std::min(max_batch_size_, plane_count);

  /* Typically
   input_channels = 112
   output_channels = 192
   max_channels = 192
   num_value_input_planes = 32
   num_policy_input_planes = 32
   num_value_channels = 128
   num_output_policy = 1858
   */

  // Allocate data for the whole batch.
  size_t max_fc_channels = std::max(
      num_value_channels, std::max(num_output_policy, num_moves_channels));
  std::vector<float> output_fc(largest_batch_size * max_fc_channels);

  std::vector<float> res_buffer1(largest_batch_size * max_channels * kSquares);
  std::vector<float> res_buffer2(largest_batch_size * output_channels *
                                 kSquares);
  std::vector<float> res_buffer3(largest_batch_size * output_channels *
                                 kSquares);

  WinogradConvolution3<use_eigen> convolve3(largest_batch_size, max_channels,
                                            max_output_channels);

  size_t max_head_planes =
      std::max(num_policy_input_planes,
               std::max(num_value_input_planes, num_moves_input_planes));
  if (attn_policy_) {
    max_head_planes = std::max(std::max(max_head_planes, size_t{67}),
                               weights_.ip_pol_b.size());
  }
  std::vector<float> head_buffer(largest_batch_size * max_head_planes *
                                 kSquares);

  // These ones will rotate during the computation.
  float* conv_in = res_buffer1.data();
  float* conv_out = res_buffer2.data();
  float* res = res_buffer3.data();

  for (size_t i = 0; i < plane_count; i += largest_batch_size) {
    const auto batch_size = std::min(plane_count - i, largest_batch_size);
    for (size_t j = 0; j < batch_size; j++) {
      EncodePlanes(planes_[i + j], &conv_in[j * kSquares * kInputPlanes]);
    }

    // Input convolution

    convolve3.Forward(batch_size, kInputPlanes, output_channels, conv_in,
                      weights_.input.weights.data(), conv_out);

    BiasActivate(batch_size, output_channels, conv_out,
                 weights_.input.biases.data(), default_activation_);

    // Residual tower

    for (auto& residual : weights_.residual) {
      const auto& conv1 = residual.conv1;
      const auto& conv2 = residual.conv2;
      const auto& se = residual.se;

      std::swap(conv_out, conv_in);

      convolve3.Forward(batch_size, output_channels, output_channels, conv_in,
                        conv1.weights.data(), conv_out);

      BiasActivate(batch_size, output_channels, &conv_out[0],
                   conv1.biases.data(), default_activation_);

      std::swap(conv_in, res);
      std::swap(conv_out, conv_in);

      convolve3.Forward(batch_size, output_channels, output_channels, conv_in,
                        conv2.weights.data(), conv_out);

      if (residual.has_se) {
        // No relu if followed by SE-unit and residual/bias is added later
        std::swap(conv_out, conv_in);

        auto se_fc_outputs = se.b1.size();
        ApplySEUnit<use_eigen>(batch_size, output_channels, se_fc_outputs,
                               conv_in, conv2.biases.data(), res, se.w1.data(),
                               se.b1.data(), se.w2.data(), se.b2.data(),
                               conv_out, default_activation_);
      } else {
        BiasResidual(batch_size, output_channels, &conv_out[0],
                     conv2.biases.data(), res, default_activation_);
      }
    }

    // Need to preserve conv_out which is used for value and moves left heads.
    if (attn_policy_) {
      // NCHW to NHWC conversion.
      for (auto batch = size_t{0}; batch < batch_size; batch++) {
        for (auto i = 0; i < kSquares; i++) {
          for (size_t j = 0; j < output_channels; j++) {
            res[batch * kSquares * output_channels + i * output_channels + j] =
                conv_out[batch * kSquares * output_channels + j * kSquares + i];
          }
        }
      }
      const size_t embedding_size = weights_.ip_pol_b.size();
      // Embedding.
      FullyConnectedLayer<use_eigen>::Forward1D(
          batch_size * kSquares, output_channels, embedding_size, res,
          weights_.ip_pol_w.data(), weights_.ip_pol_b.data(),
          SELU,  // SELU activation for attention head.
          head_buffer.data());

      for (auto layer : weights_.pol_encoder) {
        // TODO: support encoder heads.
        throw Exception(
            "Eigen/Blas backend doesn't support encoder heads yet.");
      }
      const size_t policy_d_model = weights_.ip2_pol_b.size();
      std::vector<float> head_buffer2(largest_batch_size * policy_d_model *
                                      kSquares);
      std::vector<float> head_buffer3(largest_batch_size * policy_d_model *
                                      kSquares);
      // Q
      FullyConnectedLayer<use_eigen>::Forward1D(
          batch_size * kSquares, embedding_size, policy_d_model,
          head_buffer.data(), weights_.ip2_pol_w.data(),
          weights_.ip2_pol_b.data(), NONE, head_buffer2.data());
      // K
      FullyConnectedLayer<use_eigen>::Forward1D(
          batch_size * kSquares, embedding_size, policy_d_model,
          head_buffer.data(), weights_.ip3_pol_w.data(),
          weights_.ip3_pol_b.data(), NONE, head_buffer3.data());
      const float scaling = 1.0f / sqrtf(policy_d_model);
      for (auto batch = size_t{0}; batch < batch_size; batch++) {
        const float* A = &head_buffer2[batch * 64 * policy_d_model];
        const float* B = &head_buffer3[batch * 64 * policy_d_model];
        float* C = &head_buffer[batch * (64 * 64 + 8 * 24)];
        if (use_eigen) {
          auto C_mat = EigenMatrixMap<float>(C, kSquares, kSquares);
          C_mat.noalias() =
              scaling *
              ConstEigenMatrixMap<float>(B, policy_d_model, kSquares)
                  .transpose() *
              ConstEigenMatrixMap<float>(A, policy_d_model, kSquares);
        } else {
#ifdef USE_BLAS
          cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasTrans, kSquares,
                      kSquares, policy_d_model, scaling, A, policy_d_model, B,
                      policy_d_model, 0.0f, C, 64);
#else
          // Should never get here.
          throw Exception("Blas backend internal error");
#endif
        }
      }
      // Promotion offset calculation.
      for (auto batch = size_t{0}; batch < batch_size; batch++) {
        float promotion_offsets[4][8];
        // This is so small that SGEMM seems slower.
        for (int i = 0; i < 4; i++) {
          for (int j = 0; j < 8; j++) {
            float sum = 0;
            for (size_t k = 0; k < policy_d_model; k++) {
              sum += head_buffer3.data()[batch * kSquares * policy_d_model +
                                         (56 + j) * policy_d_model + k] *
                     weights_.ip4_pol_w.data()[i * policy_d_model + k];
            }
            promotion_offsets[i][j] = sum;
          }
        }
        for (int i = 0; i < 3; i++) {
          for (int j = 0; j < 8; j++) {
            promotion_offsets[i][j] += promotion_offsets[3][j];
          }
        }
        for (int k = 0; k < 8; k++) {      // y in cuda
          for (int j = 0; j < 8; j++) {    // w in cuda
            for (int i = 0; i < 3; i++) {  // c in cuda
              head_buffer.data()[batch * (64 * 64 + 8 * 24) + 64 * 64 + 24 * k +
                                 3 * j + i] =
                  head_buffer.data()[batch * (64 * 64 + 8 * 24) +
                                     (48 + k) * 64 + 56 + j] +
                  promotion_offsets[i][j];
            }
          }
        }
      }
      // Mapping from attention policy to lc0 policy
      for (auto batch = size_t{0}; batch < batch_size; batch++) {
        for (auto i = 0; i < 64 * 64 + 8 * 24; i++) {
          auto j = kAttnPolicyMap[i];
          if (j >= 0) {
            output_fc[batch * num_output_policy + j] =
                head_buffer[batch * (64 * 64 + 8 * 24) + i];
          }
        }
      }
    } else if (conv_policy_) {
      convolve3.Forward(batch_size, output_channels, output_channels, conv_out,
                        weights_.policy1.weights.data(), res);

      BiasActivate(batch_size, output_channels, &res[0],
                   weights_.policy1.biases.data(), default_activation_);

      convolve3.Forward(batch_size, output_channels, num_policy_input_planes,
                        res, weights_.policy.weights.data(),
                        head_buffer.data());

      BiasActivate(batch_size, num_policy_input_planes, &head_buffer.data()[0],
                   weights_.policy.biases.data(), NONE);

      // Mapping from convolutional policy to lc0 policy
      for (auto batch = size_t{0}; batch < batch_size; batch++) {
        for (auto i = 0; i < kPolicyUsedPlanes * kSquares; i++) {
          auto j = kConvPolicyMap[i];
          if (j >= 0) {
            output_fc[batch * num_output_policy + j] =
                head_buffer[batch * num_policy_input_planes * kSquares + i];
          }
        }
      }

    } else {
      Convolution1<use_eigen>::Forward(
          batch_size, output_channels, num_policy_input_planes, conv_out,
          weights_.policy.weights.data(), head_buffer.data());

      BiasActivate(batch_size, num_policy_input_planes, &head_buffer[0],
                   weights_.policy.biases.data(), default_activation_);

      FullyConnectedLayer<use_eigen>::Forward1D(
          batch_size, num_policy_input_planes * kSquares, num_output_policy,
          head_buffer.data(), weights_.ip_pol_w.data(),
          weights_.ip_pol_b.data(),
          NONE,  // Activation Off
          output_fc.data());
    }

    for (size_t j = 0; j < batch_size; j++) {
      std::vector<float> policy(num_output_policy);

      // Get the moves
      policy.assign(output_fc.begin() + j * num_output_policy,
                    output_fc.begin() + (j + 1) * num_output_policy);
      policies_.emplace_back(std::move(policy));
    }

    // Value head
    Convolution1<use_eigen>::Forward(
        batch_size, output_channels, num_value_input_planes, conv_out,
        weights_.value.weights.data(), head_buffer.data());

    BiasActivate(batch_size, num_value_input_planes, &head_buffer[0],
                 weights_.value.biases.data(), default_activation_);

    FullyConnectedLayer<use_eigen>::Forward1D(
        batch_size, num_value_input_planes * kSquares, num_value_channels,
        head_buffer.data(), weights_.ip1_val_w.data(),
        weights_.ip1_val_b.data(),
        default_activation_,  // Activation On
        output_fc.data());

    // Now get the score
    if (wdl_) {
      std::vector<float> wdl(3 * batch_size);
      FullyConnectedLayer<use_eigen>::Forward1D(
          batch_size, num_value_channels, 3, output_fc.data(),
          weights_.ip2_val_w.data(), weights_.ip2_val_b.data(),
          NONE,  // Activation Off
          wdl.data());

      for (size_t j = 0; j < batch_size; j++) {
        std::vector<float> wdl_softmax(3);
        SoftmaxActivation(3, &wdl[j * 3], wdl_softmax.data());

        q_values_.emplace_back(wdl_softmax[0]);
        q_values_.emplace_back(wdl_softmax[1]);
        q_values_.emplace_back(wdl_softmax[2]);
      }
    } else {
      for (size_t j = 0; j < batch_size; j++) {
        double winrate = FullyConnectedLayer<use_eigen>::Forward0D(
                             num_value_channels, weights_.ip2_val_w.data(),
                             &output_fc[j * num_value_channels]) +
                         weights_.ip2_val_b[0];

        q_values_.emplace_back(std::tanh(winrate));
      }
    }
    if (moves_left_) {
      Convolution1<use_eigen>::Forward(
          batch_size, output_channels, num_moves_input_planes, conv_out,
          weights_.moves_left.weights.data(), head_buffer.data());

      BiasActivate(batch_size, num_moves_input_planes, &head_buffer[0],
                   weights_.moves_left.biases.data(), default_activation_);

      FullyConnectedLayer<use_eigen>::Forward1D(
          batch_size, num_moves_input_planes * kSquares, num_moves_channels,
          head_buffer.data(), weights_.ip1_mov_w.data(),
          weights_.ip1_mov_b.data(),
          default_activation_,  // Activation On
          output_fc.data());

      std::vector<float> output_moves_left(batch_size);
      FullyConnectedLayer<use_eigen>::Forward1D(
          batch_size, num_moves_channels, 1, output_fc.data(),
          weights_.ip2_mov_w.data(), weights_.ip2_mov_b.data(),
          RELU,  // Specifically Relu
          output_moves_left.data());

      for (size_t j = 0; j < batch_size; j++) {
        m_values_.emplace_back(output_moves_left[j]);
      }
    }
  }
}

template <bool use_eigen>
void BlasComputation<use_eigen>::EncodePlanes(const InputPlanes& sample,
                                              float* buffer) {
  for (const InputPlane& plane : sample) {
    const float value = plane.value;
    for (auto i = 0; i < kSquares; i++)
      *(buffer++) = (plane.mask & (((uint64_t)1) << i)) != 0 ? value : 0;
  }
}

template <bool use_eigen>
BlasNetwork<use_eigen>::BlasNetwork(const WeightsFile& file,
                                    const OptionsDict& options)
    : capabilities_{file.format().network_format().input(),
                    file.format().network_format().moves_left()},
      weights_(file.weights()) {
  max_batch_size_ =
      static_cast<size_t>(options.GetOrDefault<int>("batch_size", 256));

  wdl_ = file.format().network_format().value() ==
         pblczero::NetworkFormat::VALUE_WDL;

  moves_left_ = (file.format().network_format().moves_left() ==
                 pblczero::NetworkFormat::MOVES_LEFT_V1) &&
                options.GetOrDefault<bool>("mlh", true);

  conv_policy_ = file.format().network_format().policy() ==
                 pblczero::NetworkFormat::POLICY_CONVOLUTION;

  attn_policy_ = file.format().network_format().policy() ==
                 pblczero::NetworkFormat::POLICY_ATTENTION;

  default_activation_ = file.format().network_format().default_activation() ==
                                pblczero::NetworkFormat::DEFAULT_ACTIVATION_MISH
                            ? MISH
                            : RELU;

  if (max_batch_size_ > kHardMaxBatchSize) {
    max_batch_size_ = kHardMaxBatchSize;
  }

  const auto inputChannels = kInputPlanes;
  const auto channels = static_cast<int>(weights_.input.biases.size());
  const auto residual_blocks = weights_.residual.size();

  weights_.input.weights =
      WinogradFilterTransformF(weights_.input.weights, channels, inputChannels);

  // residual blocks
  for (size_t i = 0; i < residual_blocks; i++) {
    auto& residual = weights_.residual[i];
    auto& conv1 = residual.conv1;
    auto& conv2 = residual.conv2;

    conv1.weights = WinogradFilterTransformF(conv1.weights, channels, channels);
    conv2.weights = WinogradFilterTransformF(conv2.weights, channels, channels);
  }

  if (conv_policy_) {
    weights_.policy1.weights =
        WinogradFilterTransformF(weights_.policy1.weights, channels, channels);
    auto pol_channels = weights_.policy.biases.size();
    weights_.policy.weights = WinogradFilterTransformF(weights_.policy.weights,
                                                       pol_channels, channels);
  }

  if (use_eigen) {
    CERR << "Using Eigen version " << EIGEN_WORLD_VERSION << "."
         << EIGEN_MAJOR_VERSION << "." << EIGEN_MINOR_VERSION;
    CERR << "Eigen max batch size is " << max_batch_size_ << ".";
  } else {
#ifdef USE_OPENBLAS
    int num_procs = openblas_get_num_procs();
    openblas_set_num_threads(1);
    const char* core_name = openblas_get_corename();
    const char* config = openblas_get_config();
    CERR << "BLAS vendor: OpenBLAS.";
    CERR << "OpenBLAS [" << config << "].";
    CERR << "OpenBLAS found " << num_procs << " " << core_name << " core(s).";
#endif

#ifdef USE_MKL
    mkl_set_num_threads(1);
    CERR << "BLAS vendor: MKL.";
    constexpr int len = 256;
    char versionbuf[len];
    mkl_get_version_string(versionbuf, len);
    CERR << "MKL " << versionbuf << ".";
    MKLVersion version;
    mkl_get_version(&version);
    CERR << "MKL platform: " << version.Platform
         << ", processor: " << version.Processor << ".";
#endif

#ifdef USE_DNNL
    const dnnl_version_t* ver = dnnl_version();
    CERR << "BLAS functions from DNNL version " << ver->major << "."
         << ver->minor << "." << ver->patch;
#endif

#ifdef USE_ACCELERATE
    CERR << "BLAS vendor: Apple vecLib.";
#endif
    CERR << "BLAS max batch size is " << max_batch_size_ << ".";
  }
}

template <bool use_eigen>
std::unique_ptr<Network> MakeBlasNetwork(const std::optional<WeightsFile>& w,
                                         const OptionsDict& options) {
  if (!w) {
    throw Exception("The " + std::string(use_eigen ? "eigen" : "blas") +
                    " backend requires a network file.");
  }
  const WeightsFile& weights = *w;
  if (weights.format().network_format().network() !=
          pblczero::NetworkFormat::NETWORK_CLASSICAL_WITH_HEADFORMAT &&
      weights.format().network_format().network() !=
          pblczero::NetworkFormat::NETWORK_SE_WITH_HEADFORMAT) {
    throw Exception("Network format " +
                    pblczero::NetworkFormat::NetworkStructure_Name(
                        weights.format().network_format().network()) +
                    " is not supported by BLAS backend.");
  }
  if (weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_CLASSICAL &&
      weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_CONVOLUTION &&
      weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_ATTENTION) {
    throw Exception("Policy format " +
                    pblczero::NetworkFormat::PolicyFormat_Name(
                        weights.format().network_format().policy()) +
                    " is not supported by BLAS backend.");
  }
  if (weights.format().network_format().value() !=
          pblczero::NetworkFormat::VALUE_CLASSICAL &&
      weights.format().network_format().value() !=
          pblczero::NetworkFormat::VALUE_WDL) {
    throw Exception("Value format " +
                    pblczero::NetworkFormat::ValueFormat_Name(
                        weights.format().network_format().value()) +
                    " is not supported by BLAS backend.");
  }
  if (weights.format().network_format().default_activation() !=
          pblczero::NetworkFormat::DEFAULT_ACTIVATION_RELU &&
      weights.format().network_format().default_activation() !=
          pblczero::NetworkFormat::DEFAULT_ACTIVATION_MISH) {
    throw Exception(
        "Default activation " +
        pblczero::NetworkFormat::DefaultActivation_Name(
            weights.format().network_format().default_activation()) +
        " is not supported by BLAS backend.");
  }
  return std::make_unique<BlasNetwork<use_eigen>>(weights, options);
}

#ifdef USE_BLAS
REGISTER_NETWORK("blas", MakeBlasNetwork<false>, 50)
#endif
REGISTER_NETWORK("eigen", MakeBlasNetwork<true>, 49)

}  // namespace
}  // namespace lczero

```

`src/neural/blas/se_unit.cc`:

```cc
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#include "neural/blas/se_unit.h"
#include "neural/blas/fully_connected_layer.h"

#include <cmath>

namespace lczero {
namespace {
constexpr int kWidth = 8;
constexpr int kHeight = 8;
constexpr int kSquares = kWidth * kHeight;
}  // namespace

static void global_avg_pooling(size_t batches, size_t channels,
                               const float* input, const float* bias,
                               float* output) {
  for (auto b = size_t{0}; b < batches; b++) {
    for (auto ch = size_t{0}; ch < channels; ch++) {
      auto c = b * channels + ch;
      auto acc = 0.0f;
      for (auto i = size_t{0}; i < kSquares; i++) {
        acc += input[c * kSquares + i];
      }
      output[c] = acc / kSquares + bias[ch];
    }
  }
}

static void apply_se(const size_t channels, const size_t batch_size,
                     const float* input, const float* bias, const float* res,
                     const float* scale, float* output,
                     const ActivationFunction activation) {
  const auto lambda_sigmoid = [](const auto val) {
    return 1.0f / (1.0f + std::exp(-val));
  };

  for (auto batch = size_t{0}; batch < batch_size; batch++) {
    for (auto ch = size_t{0}; ch < channels; ch++) {
      auto c = channels * batch + ch;
      auto gamma = lambda_sigmoid(scale[c + batch * channels]);
      auto beta = scale[c + batch * channels + channels] + gamma * bias[ch];
      Activate(kSquares, gamma, &input[c * kSquares], &res[c * kSquares], beta,
               &output[c * kSquares], activation);
    }
  }
}

template <bool use_eigen>
void ApplySEUnit(const size_t batch_size, const size_t channels,
                 const size_t se_fc_outputs, const float* input,
                 const float* ch_bias, const float* residual,
                 const float* weights_w1, const float* weights_b1,
                 const float* weights_w2, const float* weights_b2,
                 float* output, const ActivationFunction activation) {
  std::vector<float> pool(2 * channels * batch_size);
  std::vector<float> fc_out1(batch_size * se_fc_outputs);

  global_avg_pooling(batch_size, channels, input, ch_bias, pool.data());

  FullyConnectedLayer<use_eigen>::Forward1D(batch_size, channels, se_fc_outputs,
                                            pool.data(), weights_w1, weights_b1,
                                            activation,  // Activation On
                                            fc_out1.data());

  FullyConnectedLayer<use_eigen>::Forward1D(batch_size, se_fc_outputs,
                                            2 * channels, fc_out1.data(),
                                            weights_w2, weights_b2,
                                            NONE,  // Activation Off
                                            pool.data());

  // Sigmoid, scale and add residual
  apply_se(channels, batch_size, input, ch_bias, residual, pool.data(), output,
           activation);
}

template void ApplySEUnit<true>(const size_t batch_size, const size_t channels,
                                const size_t se_fc_outputs, const float* input,
                                const float* bias, const float* residual,
                                const float* weights_w1,
                                const float* weights_b1,
                                const float* weights_w2,
                                const float* weights_b2, float* output,
                                const ActivationFunction activation);
#ifdef USE_BLAS
template void ApplySEUnit<false>(const size_t batch_size, const size_t channels,
                                 const size_t se_fc_outputs, const float* input,
                                 const float* bias, const float* residual,
                                 const float* weights_w1,
                                 const float* weights_b1,
                                 const float* weights_w2,
                                 const float* weights_b2, float* output,
                                 const ActivationFunction activation);
#endif
}  // namespace lczero

```

`src/neural/blas/se_unit.h`:

```h
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include "neural/shared/activation.h"

#include <cstddef>

namespace lczero {

template <bool use_eigen>
void ApplySEUnit(const size_t batch_size, const size_t channels,
                 const size_t se_fc_outputs, const float* input,
                 const float* bias, const float* residual,
                 const float* weights_w1, const float* weights_b1,
                 const float* weights_w2, const float* weights_b2,
                 float* output, const ActivationFunction activation);

}  // namespace lczero

```

`src/neural/blas/winograd_convolution3.cc`:

```cc
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#include "neural/blas/winograd_convolution3.h"
#include "neural/blas/blas.h"

#include <algorithm>
#include <cassert>
#include <cmath>

#include <array>

#ifdef USE_ISPC
#include "winograd_transform_ispc.h"
#endif

#include <Eigen/Dense>

namespace lczero {
template <typename T>
using EigenMatrixMap =
    Eigen::Map<Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>;
template <typename T>
using ConstEigenMatrixMap =
    Eigen::Map<const Eigen::Matrix<T, Eigen::Dynamic, Eigen::Dynamic>>;

template <bool use_eigen>
WinogradConvolution3<use_eigen>::WinogradConvolution3(
    const size_t max_batch_size, const size_t max_input_layers,
    const size_t max_output_layers)
    : V_(max_batch_size * kWinogradTile * max_input_layers * kTiles),
      M_(max_batch_size * kWinogradTile * max_output_layers * kTiles) {}

template <bool use_eigen>
void WinogradConvolution3<use_eigen>::Forward(const size_t batch_size,
                                              const size_t input_channels,
                                              const size_t output_channels,
                                              const float* input,
                                              const float* weights,
                                              float* output) {
  TransformIn(batch_size, input, input_channels);
  Sgemm(batch_size, weights, input_channels, output_channels);
  TransformOut(batch_size, output, output_channels);
}

template <bool use_eigen>
void WinogradConvolution3<use_eigen>::TransformIn(const size_t batch_size,
                                                  const float* input,
                                                  const size_t channels) {
#ifndef USE_ISPC

  static const size_t kCacheSize = 128;
  float x[kWinogradAlpha][kWinogradAlpha];
  float T1[kWinogradAlpha][kWinogradAlpha];
  float R[16][kCacheSize];
  for (size_t batch_index = 0; batch_index < batch_size; batch_index++) {
    size_t channels_rem = channels;
    const float* input_batch =
        input + batch_index * kWidth * kHeight * channels;
    float* V_batch = &V_[channels * kTiles * batch_index];
    for (size_t channel_long = 0; channel_long < channels;
         channel_long += kCacheSize) {
      const size_t channel_step = std::min<size_t>(kCacheSize, channels_rem);
      channels_rem -= channel_step;
      for (int block_y = 0; block_y < kWtiles; block_y++) {
        for (int block_x = 0; block_x < kWtiles; block_x++) {
          // Tiles overlap by 2
          const int yin = 2 * block_y - 1;
          const int xin = 2 * block_x - 1;

          for (size_t ch = 0; ch < channel_step; ++ch) {
            const size_t channel = channel_long + ch;

            const float* input_channel =
                input_batch + channel * (kWidth * kHeight);
            for (int i = 0; i < kWinogradAlpha; i++) {
              for (int j = 0; j < kWinogradAlpha; j++) {
                if ((yin + i) >= 0 && (xin + j) >= 0 && (yin + i) < kHeight &&
                    (xin + j) < kWidth) {
                  x[i][j] = input_channel[(yin + i) * kWidth + (xin + j)];
                } else {
                  x[i][j] = 0.0f;
                }
              }
            }

            // Calculates transpose(B).x.B
            // B = [[ 1.0,  0.0,  0.0,  0.0],
            //      [ 0.0,  1.0, -1.0,  1.0],
            //      [-1.0,  1.0,  1.0,  0.0],
            //      [ 0.0,  0.0,  0.0, -1.0]]

            T1[0][0] = x[0][0] - x[2][0];
            T1[0][1] = x[0][1] - x[2][1];
            T1[0][2] = x[0][2] - x[2][2];
            T1[0][3] = x[0][3] - x[2][3];
            T1[1][0] = x[1][0] + x[2][0];
            T1[1][1] = x[1][1] + x[2][1];
            T1[1][2] = x[1][2] + x[2][2];
            T1[1][3] = x[1][3] + x[2][3];
            T1[2][0] = x[2][0] - x[1][0];
            T1[2][1] = x[2][1] - x[1][1];
            T1[2][2] = x[2][2] - x[1][2];
            T1[2][3] = x[2][3] - x[1][3];
            T1[3][0] = x[1][0] - x[3][0];
            T1[3][1] = x[1][1] - x[3][1];
            T1[3][2] = x[1][2] - x[3][2];
            T1[3][3] = x[1][3] - x[3][3];

            R[0][ch] = T1[0][0] - T1[0][2];
            R[1][ch] = T1[0][1] + T1[0][2];
            R[2][ch] = T1[0][2] - T1[0][1];
            R[3][ch] = T1[0][1] - T1[0][3];
            R[4][ch] = T1[1][0] - T1[1][2];
            R[5][ch] = T1[1][1] + T1[1][2];
            R[6][ch] = T1[1][2] - T1[1][1];
            R[7][ch] = T1[1][1] - T1[1][3];
            R[8][ch] = T1[2][0] - T1[2][2];
            R[9][ch] = T1[2][1] + T1[2][2];
            R[10][ch] = T1[2][2] - T1[2][1];
            R[11][ch] = T1[2][1] - T1[2][3];
            R[12][ch] = T1[3][0] - T1[3][2];
            R[13][ch] = T1[3][1] + T1[3][2];
            R[14][ch] = T1[3][2] - T1[3][1];
            R[15][ch] = T1[3][1] - T1[3][3];
          }
          float* V_channel = V_batch + channel_long;
          const auto V_incr = channels * kTiles * batch_size;
          float* wTile_V = V_channel + channels * (block_y * kWtiles + block_x);
          for (size_t i = 0; i < 16; ++i) {
            for (size_t ch = 0; ch < channel_step; ++ch) {
              wTile_V[ch] = R[i][ch];
            }
            wTile_V += V_incr;
          }
        }
      }
    }
  }

#else  // USE_ISPC

  ispc::winograd_TransformIn_ispc(batch_size, input, channels, &V_[0]);

#endif  // USE_ISPC
}

#ifdef USE_BLAS
template <>
void WinogradConvolution3<false>::Sgemm(const size_t batch_size,
                                        const float* weights,
                                        const size_t input_channels,
                                        const size_t output_channels) {
#ifdef USE_MKL

  /*
   void cblas_sgemm_batch (const CBLAS_LAYOUT Layout, const CBLAS_TRANSPOSE*
   transa_array, const CBLAS_TRANSPOSE* transb_array, const MKL_INT* m_array,
   const MKL_INT* n_array, const MKL_INT* k_array, const float* alpha_array,
   const float **a_array, const MKL_INT* lda_array, const float **b_array, const
   MKL_INT* ldb_array, const float* beta_array, float **c_array, const MKL_INT*
   ldc_array, const MKL_INT group_count, const MKL_INT* group_size);
   */

  CBLAS_TRANSPOSE transA = CblasNoTrans;
  CBLAS_TRANSPOSE transB = CblasNoTrans;
  MKL_INT m_array = output_channels;
  MKL_INT n_array = batch_size * kTiles;
  MKL_INT k_array = input_channels;
  float alpha_array = 1.0;
  const float* a_array[kWinogradTile];
  MKL_INT lda_array = output_channels;
  const float* b_array[kWinogradTile];
  MKL_INT ldb_array = input_channels;
  float* c_array[kWinogradTile];
  MKL_INT ldc_array = output_channels;
  float beta_array = 0.0;
  MKL_INT groupSize = kWinogradTile;

  for (auto b = 0; b < kWinogradTile; b++) {
    auto offset_u = b * output_channels * input_channels;
    auto offset_v = b * batch_size * input_channels * kTiles;
    auto offset_m = b * batch_size * output_channels * kTiles;

    a_array[b] = &weights[offset_u];
    b_array[b] = &V_[offset_v];
    c_array[b] = &M_[offset_m];
  }

  cblas_sgemm_batch(CblasColMajor, &transA, &transB, &m_array, &n_array,
                    &k_array, &alpha_array, a_array, &lda_array, b_array,
                    &ldb_array, &beta_array, c_array, &ldc_array, 1,
                    &groupSize);

#else

  for (size_t b = 0; b < kWinogradTile; b++) {
    auto offset_u = b * output_channels * input_channels;

    // In col major
    //
    //            M               =         weights(T)        x          V
    //
    // cols      tiles                  input_channels              tiles
    // rows   output_channels          output_channels            input_channels

    auto offset_v = b * batch_size * input_channels * kTiles;
    auto offset_m = b * batch_size * output_channels * kTiles;
    cblas_sgemm(CblasColMajor,               // Row major format
                CblasNoTrans,                // A no trans
                CblasNoTrans,                // B no trans
                (int)output_channels,        // rows W, M
                (int)(batch_size * kTiles),  // cols V, M
                (int)input_channels,         // cols W, rows V
                1.0f,                        // alpha
                &weights[offset_u],          // W
                (int)output_channels,        // ldW
                &V_[offset_v],               // V
                (int)input_channels, 0.0f,   // ldV
                &M_[offset_m],               // M
                (int)output_channels);       // ldM
  }
#endif
}
#endif

template <>
void WinogradConvolution3<true>::Sgemm(const size_t batch_size,
                                       const float* weights,
                                       const size_t input_channels,
                                       const size_t output_channels) {
  for (size_t b = 0; b < kWinogradTile; b++) {
    auto offset_u = b * output_channels * input_channels;
    auto offset_v = b * batch_size * input_channels * kTiles;
    auto offset_m = b * batch_size * output_channels * kTiles;
    auto C_mat = EigenMatrixMap<float>(&M_[offset_m], output_channels,
                                       batch_size * kTiles);
    C_mat.noalias() = ConstEigenMatrixMap<float>(
                          &weights[offset_u], output_channels, input_channels) *
                      ConstEigenMatrixMap<float>(&V_[offset_v], input_channels,
                                                 batch_size * kTiles);
  }
}

template <bool use_eigen>
void WinogradConvolution3<use_eigen>::TransformOut(const size_t batch_size,
                                                   float* output,
                                                   const size_t channels) {
#ifndef USE_ISPC

  float m[kWinogradTile];

  for (size_t batch_index = 0; batch_index < batch_size; batch_index++) {
    const float* M_batch = &M_[channels * kTiles * batch_index];
    float* output_batch = output + batch_index * kWidth * kHeight * channels;

    for (size_t channel = 0; channel < channels; channel++) {
      const float* M_channel = M_batch + channel;
      float* output_channel = output_batch + channel * (kHeight * kWidth);

      for (int block_x = 0; block_x < kWtiles; block_x++) {
        for (int block_y = 0; block_y < kWtiles; block_y++) {
          const auto x = 2 * block_x;
          const auto y = 2 * block_y;

          const auto b = block_y * kWtiles + block_x;
          const float* M_wtile = M_channel + channels * b;
          const auto M_incr = channels * kTiles * batch_size;

          for (int wTile = 0; wTile < kWinogradTile; wTile++) {
            m[wTile] = *M_wtile;
            M_wtile += M_incr;
          }

          // Calculates transpose(A).temp_m.A
          //    A = [1.0,  0.0],
          //        [1.0,  1.0],
          //        [1.0, -1.0],
          //        [0.0, -1.0]]

          auto o11 = m[0 * 4 + 0] + m[0 * 4 + 1] + m[0 * 4 + 2] + m[1 * 4 + 0] +
                     m[1 * 4 + 1] + m[1 * 4 + 2] + m[2 * 4 + 0] + m[2 * 4 + 1] +
                     m[2 * 4 + 2];

          auto o12 = m[0 * 4 + 1] - m[0 * 4 + 2] - m[0 * 4 + 3] + m[1 * 4 + 1] -
                     m[1 * 4 + 2] - m[1 * 4 + 3] + m[2 * 4 + 1] - m[2 * 4 + 2] -
                     m[2 * 4 + 3];

          auto o21 = m[1 * 4 + 0] + m[1 * 4 + 1] + m[1 * 4 + 2] - m[2 * 4 + 0] -
                     m[2 * 4 + 1] - m[2 * 4 + 2] - m[3 * 4 + 0] - m[3 * 4 + 1] -
                     m[3 * 4 + 2];

          auto o22 = m[1 * 4 + 1] - m[1 * 4 + 2] - m[1 * 4 + 3] - m[2 * 4 + 1] +
                     m[2 * 4 + 2] + m[2 * 4 + 3] - m[3 * 4 + 1] + m[3 * 4 + 2] +
                     m[3 * 4 + 3];

          output_channel[(y)*kWidth + (x)] = o11;
          output_channel[(y)*kWidth + (x + 1)] = o12;
          output_channel[(y + 1) * kWidth + (x)] = o21;
          output_channel[(y + 1) * kWidth + (x + 1)] = o22;
        }
      }
    }
  }

#else  // USE_ISPC

  ispc::winograd_TransformOut_ispc(batch_size, &M_[0], channels, output);

#endif  // USE_ISPC
}

template class WinogradConvolution3<true>;
#ifdef USE_BLAS
template class WinogradConvolution3<false>;
#endif
}  // namespace lczero

```

`src/neural/blas/winograd_convolution3.h`:

```h
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include <cstddef>
#include <vector>

namespace lczero {

// Convolution 3x3 on a 8x8 board using the Winograd algorithm.
//
// Ref:
//
// Fast Algorithms for Convolutional Neural Networks
// https://arxiv.org/abs/1509.09308
//
// https://ai.intel.com/winograd/
// https://ai.intel.com/winograd-2/

// Convolution 3x3 using the Winograd algorithm
template <bool use_eigen>
class WinogradConvolution3 {
 public:
  // The instance will allocate memory resources for the
  // largest batch size, and the largest input and output
  // layers.
  WinogradConvolution3(const size_t max_batch_size,
                       const size_t max_input_layers,
                       const size_t max_output_layers);

  // Forward inference, batched.
  void Forward(const size_t batch_size, const size_t input_channels,
               const size_t output_channels, const float* input,
               const float* weights, float* output);

 private:
  void TransformIn(const size_t batch_size, const float* input,
                   const size_t channels);

  void Sgemm(const size_t batch_size, const float* weights,
             const size_t input_channels, const size_t output_channels);

  void TransformOut(const size_t batch_size, float* output,
                    const size_t channels);

  static constexpr auto kWidth = 8;
  static constexpr auto kHeight = 8;
  static constexpr auto kSquares = kWidth * kHeight;

  static constexpr auto kWtiles = (kWidth + 1) / 2;  // 4
  static constexpr auto kTiles = kWtiles * kWtiles;  // 16

  static constexpr auto kWinogradAlpha = 4;
  static constexpr auto kWinogradTile = kWinogradAlpha * kWinogradAlpha;

  std::vector<float> V_;
  std::vector<float> M_;
};
}  // namespace lczero

```

`src/neural/blas/winograd_transform.ispc`:

```ispc
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

// This is ispc version of  WinogradConvolution3::TransformIn.

uniform const size_t kWidth = 8;
uniform const size_t kHeight = 8;
uniform const size_t kSquares = kWidth * kHeight;

uniform const size_t kWtiles = 4;                 //(kWidth + 1) / 2;
uniform const size_t kTiles = kWtiles * kWtiles;  // 16

uniform const size_t kWinogradAlpha = 4;
uniform const size_t kWinogradTile = kWinogradAlpha * kWinogradAlpha;

export void winograd_TransformIn_ispc(uniform size_t batch_size,
                                      const uniform float input[],
                                      uniform size_t channels,
                                      uniform float output[]) {
  float x[kWinogradAlpha][kWinogradAlpha];

  for (uniform size_t batch_index = 0; batch_index < batch_size;
       batch_index++) {
    uniform size_t input_batch = batch_index * kWidth * kHeight * channels;
    uniform size_t V_batch = channels * kTiles * batch_index;

    for (uniform int block_y = 0; block_y < kWtiles; block_y++) {
      for (uniform int block_x = 0; block_x < kWtiles; block_x++) {
        const uniform int yin = 2 * block_y - 1;
        const uniform int xin = 2 * block_x - 1;
        const uniform size_t V_incr = channels * kTiles * batch_size;

        foreach (channel = 0 ... channels) {
          size_t V_channel = V_batch + channel;
          const float* input_channel = input + input_batch + channel * kSquares;

          if (block_y == 0) {
            for (uniform int j = 0; j < kWinogradAlpha; j++) {
              x[0][j] = 0.0f;
            }
            for (uniform int i = 1; i < kWinogradAlpha; i++) {
              for (uniform int j = 0; j < kWinogradAlpha / 2; j++) {
                if ((xin + j) >= 0) {
                  x[i][j] = input_channel[(yin + i) * kWidth + (xin + j)];
                } else {
                  x[i][j] = 0.0f;
                }
              }
              for (uniform int j = kWinogradAlpha / 2; j < kWinogradAlpha;
                   j++) {
                if ((xin + j) < kWidth) {
                  x[i][j] = input_channel[(yin + i) * kWidth + (xin + j)];
                } else {
                  x[i][j] = 0.0f;
                }
              }
            }
          } else if (block_y == kWtiles - 1) {
            for (uniform int i = 0; i < kWinogradAlpha - 1; i++) {
              for (uniform int j = 0; j < kWinogradAlpha / 2; j++) {
                if ((xin + j) >= 0) {
                  x[i][j] = input_channel[(yin + i) * kWidth + (xin + j)];
                } else {
                  x[i][j] = 0.0f;
                }
              }
              for (uniform int j = kWinogradAlpha / 2; j < kWinogradAlpha;
                   j++) {
                if ((xin + j) < kWidth) {
                  x[i][j] = input_channel[(yin + i) * kWidth + (xin + j)];
                } else {
                  x[i][j] = 0.0f;
                }
              }
            }
            for (uniform int j = 0; j < kWinogradAlpha; j++) {
              x[kWinogradAlpha - 1][j] = 0.0f;
            }
          } else {
            for (uniform int i = 0; i < kWinogradAlpha; i++) {
              for (uniform int j = 0; j < kWinogradAlpha / 2; j++) {
                if ((xin + j) >= 0) {
                  x[i][j] = input_channel[(yin + i) * kWidth + (xin + j)];
                } else {
                  x[i][j] = 0.0f;
                }
              }
              for (uniform int j = kWinogradAlpha / 2; j < kWinogradAlpha;
                   j++) {
                if ((xin + j) < kWidth) {
                  x[i][j] = input_channel[(yin + i) * kWidth + (xin + j)];
                } else {
                  x[i][j] = 0.0f;
                }
              }
            }
          }

          const size_t wTile_V =
              V_channel + channels * (block_y * kWtiles + block_x);

          output[wTile_V + V_incr * 0] = x[0][0] - x[2][0] - x[0][2] + x[2][2];
          output[wTile_V + V_incr * 1] = x[0][1] - x[2][1] + x[0][2] - x[2][2];
          output[wTile_V + V_incr * 2] = x[0][2] - x[2][2] - x[0][1] + x[2][1];
          output[wTile_V + V_incr * 3] = x[0][1] - x[2][1] - x[0][3] + x[2][3];
          output[wTile_V + V_incr * 4] = x[1][0] + x[2][0] - x[1][2] - x[2][2];
          output[wTile_V + V_incr * 5] = x[1][1] + x[2][1] + x[1][2] + x[2][2];
          output[wTile_V + V_incr * 6] = x[1][2] + x[2][2] - x[1][1] - x[2][1];
          output[wTile_V + V_incr * 7] = x[1][1] + x[2][1] - x[1][3] - x[2][3];
          output[wTile_V + V_incr * 8] = x[2][0] - x[1][0] - x[2][2] + x[1][2];
          output[wTile_V + V_incr * 9] = x[2][1] - x[1][1] + x[2][2] - x[1][2];
          output[wTile_V + V_incr * 10] = x[2][2] - x[1][2] - x[2][1] + x[1][1];
          output[wTile_V + V_incr * 11] = x[2][1] - x[1][1] - x[2][3] + x[1][3];
          output[wTile_V + V_incr * 12] = x[1][0] - x[3][0] - x[1][2] + x[3][2];
          output[wTile_V + V_incr * 13] = x[1][1] - x[3][1] + x[1][2] - x[3][2];
          output[wTile_V + V_incr * 14] = x[1][2] - x[3][2] - x[1][1] + x[3][1];
          output[wTile_V + V_incr * 15] = x[1][1] - x[3][1] - x[1][3] + x[3][3];
        }
      }
    }
  }
}

export void winograd_TransformOut_ispc(uniform size_t batch_size,
                                       const uniform float input[],
                                       uniform size_t channels,
                                       uniform float output[]) {
  for (uniform size_t batch_index = 0; batch_index < batch_size;
       batch_index++) {
    const uniform size_t M_batch = channels * kTiles * batch_index;
    const uniform size_t output_batch = batch_index * kSquares * channels;

    for (uniform int block_y = 0; block_y < kWtiles; block_y++) {
      for (uniform int block_x = 0; block_x < kWtiles; block_x++) {
        const uniform int x = 2 * block_x;
        const uniform int y = 2 * block_y;
        const uniform int b = block_y * kWtiles + block_x;
        const uniform int M_incr = channels * kTiles * batch_size;

        foreach (channel = 0 ... channels) {
          const size_t M_channel = M_batch + channel;
          const size_t output_channel = output_batch + channel * kSquares;
          const float* M_wtile = input + M_channel + channels * b;

          float o11 = M_wtile[0];
          M_wtile += M_incr;
          o11 += M_wtile[0];
          float o12 = M_wtile[0];
          M_wtile += M_incr;
          o11 += M_wtile[0];
          o12 -= M_wtile[0];
          M_wtile += M_incr;
          o12 -= M_wtile[0];
          M_wtile += M_incr;
          o11 += M_wtile[0];
          float o21 = M_wtile[0];
          M_wtile += M_incr;
          o11 += M_wtile[0];
          o12 += M_wtile[0];
          o21 += M_wtile[0];
          float o22 = M_wtile[0];
          M_wtile += M_incr;
          o11 += M_wtile[0];
          o12 -= M_wtile[0];
          o21 += M_wtile[0];
          o22 -= M_wtile[0];
          M_wtile += M_incr;
          o12 -= M_wtile[0];
          o22 -= M_wtile[0];
          M_wtile += M_incr;
          o11 += M_wtile[0];
          o21 -= M_wtile[0];
          M_wtile += M_incr;
          o11 += M_wtile[0];
          o12 += M_wtile[0];
          o21 -= M_wtile[0];
          o22 -= M_wtile[0];
          M_wtile += M_incr;
          o11 += M_wtile[0];
          o12 -= M_wtile[0];
          o21 -= M_wtile[0];
          o22 += M_wtile[0];
          M_wtile += M_incr;
          o12 -= M_wtile[0];
          o22 += M_wtile[0];
          M_wtile += M_incr;
          o21 -= M_wtile[0];
          M_wtile += M_incr;
          o21 -= M_wtile[0];
          o22 -= M_wtile[0];
          M_wtile += M_incr;
          o21 -= M_wtile[0];
          o22 += M_wtile[0];
          M_wtile += M_incr;
          o22 += M_wtile[0];

          output[output_channel + (y)*kWidth + (x)] = o11;
          output[output_channel + (y)*kWidth + (x + 1)] = o12;
          output[output_channel + (y + 1) * kWidth + (x)] = o21;
          output[output_channel + (y + 1) * kWidth + (x + 1)] = o22;
        }
      }
    }
  }
}

```

`src/neural/cache.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#include "neural/cache.h"
#include <cassert>
#include <iostream>

namespace lczero {
CachingComputation::CachingComputation(
    std::unique_ptr<NetworkComputation> parent, NNCache* cache)
    : parent_(std::move(parent)), cache_(cache) {}

int CachingComputation::GetCacheMisses() const {
  return parent_->GetBatchSize();
}

int CachingComputation::GetBatchSize() const { return batch_.size(); }

bool CachingComputation::AddInputByHash(uint64_t hash) {
  NNCacheLock lock(cache_, hash);
  if (!lock) return false;
  AddInputByHash(hash, std::move(lock));
  return true;
}

void CachingComputation::AddInputByHash(uint64_t hash, NNCacheLock&& lock) {
  assert(lock);
  batch_.emplace_back();
  batch_.back().lock = std::move(lock);
  batch_.back().hash = hash;
}

void CachingComputation::PopCacheHit() {
  assert(!batch_.empty());
  assert(batch_.back().lock);
  assert(batch_.back().idx_in_parent == -1);
  batch_.pop_back();
}

void CachingComputation::AddInput(
    uint64_t hash, InputPlanes&& input,
    std::vector<uint16_t>&& probabilities_to_cache) {
  if (AddInputByHash(hash)) return;
  batch_.emplace_back();
  batch_.back().hash = hash;
  batch_.back().idx_in_parent = parent_->GetBatchSize();
  batch_.back().probabilities_to_cache = probabilities_to_cache;
  parent_->AddInput(std::move(input));
}

void CachingComputation::PopLastInputHit() {
  assert(!batch_.empty());
  assert(batch_.back().idx_in_parent == -1);
  batch_.pop_back();
}

void CachingComputation::ComputeBlocking() {
  if (parent_->GetBatchSize() == 0) return;
  parent_->ComputeBlocking();

  // Fill cache with data from NN.
  for (const auto& item : batch_) {
    if (item.idx_in_parent == -1) continue;
    auto req =
        std::make_unique<CachedNNRequest>(item.probabilities_to_cache.size());
    req->q = parent_->GetQVal(item.idx_in_parent);
    req->d = parent_->GetDVal(item.idx_in_parent);
    req->m = parent_->GetMVal(item.idx_in_parent);
    int idx = 0;
    for (auto x : item.probabilities_to_cache) {
      req->p[idx++] =
          std::make_pair(x, parent_->GetPVal(item.idx_in_parent, x));
    }
    cache_->Insert(item.hash, std::move(req));
  }
}

float CachingComputation::GetQVal(int sample) const {
  const auto& item = batch_[sample];
  if (item.idx_in_parent >= 0) return parent_->GetQVal(item.idx_in_parent);
  return item.lock->q;
}

float CachingComputation::GetDVal(int sample) const {
  const auto& item = batch_[sample];
  if (item.idx_in_parent >= 0) return parent_->GetDVal(item.idx_in_parent);
  return item.lock->d;
}

float CachingComputation::GetMVal(int sample) const {
  const auto& item = batch_[sample];
  if (item.idx_in_parent >= 0) return parent_->GetMVal(item.idx_in_parent);
  return item.lock->m;
}

float CachingComputation::GetPVal(int sample, int move_id) const {
  auto& item = batch_[sample];
  if (item.idx_in_parent >= 0)
    return parent_->GetPVal(item.idx_in_parent, move_id);
  const auto& moves = item.lock->p;

  int total_count = 0;
  while (total_count < moves.size()) {
    // Optimization: usually moves are stored in the same order as queried.
    const auto& move = moves[item.last_idx++];
    if (item.last_idx == moves.size()) item.last_idx = 0;
    if (move.first == move_id) return move.second;
    ++total_count;
  }
  assert(false);  // Move not found.
  return 0;
}

}  // namespace lczero

```

`src/neural/cache.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#pragma once

#include "neural/network.h"
#include "utils/cache.h"
#include "utils/smallarray.h"

namespace lczero {

struct CachedNNRequest {
  CachedNNRequest(size_t size) : p(size) {}
  typedef std::pair<uint16_t, float> IdxAndProb;
  float q;
  float d;
  float m;
  // TODO(mooskagh) Don't really need index if using perfect hash.
  SmallArray<IdxAndProb> p;
};

typedef HashKeyedCache<CachedNNRequest> NNCache;
typedef HashKeyedCacheLock<CachedNNRequest> NNCacheLock;

// Wraps around NetworkComputation and caches result.
// While it mostly repeats NetworkComputation interface, it's not derived
// from it, as AddInput() needs hash and index of probabilities to store.
class CachingComputation {
 public:
  CachingComputation(std::unique_ptr<NetworkComputation> parent,
                     NNCache* cache);

  // How many inputs are not found in cache and will be forwarded to a wrapped
  // computation.
  int GetCacheMisses() const;
  // Total number of times AddInput/AddInputByHash were (successfully) called.
  int GetBatchSize() const;
  // Adds input by hash only. If that hash is not in cache, returns false
  // and does nothing. Otherwise adds.
  bool AddInputByHash(uint64_t hash);
  // Adds input by hash with existing lock. Assumes the given lock holds a real
  // reference.
  void AddInputByHash(uint64_t hash, NNCacheLock&& lock);
  // Adds a sample to the batch.
  // @hash is a hash to store/lookup it in the cache.
  // @probabilities_to_cache is which indices of policy head to store.
  void AddInput(uint64_t hash, InputPlanes&& input,
                std::vector<uint16_t>&& probabilities_to_cache);
  // Undos last AddInput. If it was a cache miss, the it's actually not removed
  // from parent's batch.
  void PopLastInputHit();
  // Do the computation.
  void ComputeBlocking();
  // Returns Q value of @sample.
  float GetQVal(int sample) const;
  // Returns probability of draw if NN has WDL value head.
  float GetDVal(int sample) const;
  // Returns estimated remaining moves.
  float GetMVal(int sample) const;
  // Returns P value @move_id of @sample.
  float GetPVal(int sample, int move_id) const;
  // Pops last input from the computation. Only allowed for inputs which were
  // cached.
  void PopCacheHit();

  // Can be used to avoid repeated reallocations internally while adding itemms.
  void Reserve(int batch_size) { batch_.reserve(batch_size); }

 private:
  struct WorkItem {
    uint64_t hash;
    NNCacheLock lock;
    int idx_in_parent = -1;
    std::vector<uint16_t> probabilities_to_cache;
    mutable int last_idx = 0;
  };

  std::unique_ptr<NetworkComputation> parent_;
  NNCache* cache_;
  std::vector<WorkItem> batch_;
};

}  // namespace lczero

```

`src/neural/cuda/common_kernels.cu`:

```cu
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <cassert>
#include <algorithm>
#include "cuda_common.h"
#include "winograd_helper.inc"

namespace lczero {
namespace cudnn_backend {
namespace {
constexpr int kInputPlanes = 112;
}  // namespace

/////////////////////////////////////////////////////////////////////////////
//          Simple CUDA kernels used by certain layers                     //
/////////////////////////////////////////////////////////////////////////////

template <typename T>
__global__ void addVectors_kernel(T* c, T* a, T* b, int size, int asize,
                                  int bsize, ActivationFunction activation) {
  int i = threadIdx.x + blockDim.x * blockIdx.x;
  if (i < size) {
    float aVal = 0;
    float bVal = 0;
    if (a) aVal = (float)(a[i % asize]);
    if (b) bVal = (float)(b[i % bsize]);

    float cVal = aVal + bVal;

    cVal = activate(cVal, activation);

    c[i] = (T)cVal;
  }
}

// Adds two vectors (possibly of different sizes), also do optional relu
// activation.
template <typename T>
void addVectors(T* c, T* a, T* b, int size, int asize, int bsize,
                ActivationFunction activation, cudaStream_t stream) {
  const int kBlockSize = 256;
  int blocks = DivUp(size, kBlockSize);

  addVectors_kernel<<<blocks, kBlockSize, 0, stream>>>(c, a, b, size, asize,
                                                       bsize, activation);
  ReportCUDAErrors(cudaGetLastError());
}

template <typename T, ActivationFunction act>
__global__ void addBiasBatched_kernel(T* output, const T* input, const T* bias,
                                      int N, int C) {
  int batch = blockIdx.y;
  int n = blockIdx.x * blockDim.y + threadIdx.y;
  if (n >= N) return;
  int c = threadIdx.x * 4;

  int biasIndex = batch * C + c;
  int tensorIndex = batch * N * C + n * C + c;

  float val[4];
  float b[4];

  // Load from memory
  const bool fp16 = std::is_same<half, T>::value;
  if (fp16) {
    half inp[4];
    copyAs<uint2>(&inp[0], &input[tensorIndex]);
#pragma unroll
    for (int i = 0; i < 4; i++) val[i] = (float)inp[i];

    copyAs<uint2>(&inp[0], &bias[biasIndex]);
#pragma unroll
    for (int i = 0; i < 4; i++) b[i] = (float)inp[i];
  } else {
    copyAs<uint4>(&val[0], &input[tensorIndex]);
    copyAs<uint4>(&b[0], &bias[biasIndex]);
  }
  
  // Perform bias add and activation
#pragma unroll
  for (int i = 0; i < 4; i++) {
    float x = val[i] + b[i];
    x = activate(x, act);
    val[i] = x;
  }

  // write to memory
  if (fp16) {
    half op[4];
#pragma unroll
    for (int i = 0; i < 4; i++) op[i] = (half)val[i];
    copyAs<uint2>(&output[tensorIndex], &op[0]);
  } else {
    copyAs<uint4>(&output[tensorIndex], &val[0]);
  }
}

// Input/output tensors are Batch * N * C
// bias tensor is N * C (i.e, different bias for each Batch dimension)
template <typename T>
void addBiasBatched(T* output, const T* input, const T* bias, int Batch, int N,
                    int C, ActivationFunction activation, cudaStream_t stream) {
  // process 4 elements per thread to achieve close to peak memory bandwidth
  if (C % 4 != 0) throw Exception("unsupported filter size");
  if (C > 4096) throw Exception("unsupported filter size");

  dim3 blockDim, gridDim;
  blockDim.x = C / 4;
  blockDim.y = std::min(std::max(512 / blockDim.x, 1u), (unsigned int) N);
  blockDim.z = 1;
  gridDim.x = DivUp(N, blockDim.y);
  gridDim.y = Batch;
  gridDim.z = 1;

  switch (activation) {
    case NONE:
      addBiasBatched_kernel<T, NONE><<<gridDim, blockDim, 0, stream>>>(
          output, input, bias, N, C);
      break;
    case SELU:
      addBiasBatched_kernel<T, SELU><<<gridDim, blockDim, 0, stream>>>(
          output, input, bias, N, C);
      break;
    default:
      throw Exception(
          "unsupported activation in addBiasBatched. Add in switch-case here");
  }

  ReportCUDAErrors(cudaGetLastError());
}

template <typename T>
__global__ void addBias_NCHW_kernel(T* c, T* a, T* b, int N, int C, int H,
                                    int W, ActivationFunction activation) {
  int i = threadIdx.x + blockDim.x * blockIdx.x;
  int size = N * C * H * W;
  if (i < size) {
    float aVal = (float)a[i];

    // All this math can be optimized, but the kernel is memory bound anyway.
    int biasIndex = (i / (H * W)) % C;
    float bVal = (float)b[biasIndex];

    float cVal = aVal + bVal;

    cVal = activate(cVal, activation);

    c[i] = (T)cVal;
  }
}

// Add bias to convolution's output.
template <typename T>
void addBias_NCHW(T* c, T* a, T* b, int N, int C, int H, int W,
                  ActivationFunction activation, cudaStream_t stream) {
  int size = N * C * H * W;
  const int kBlockSize = 256;
  int blocks = DivUp(size, kBlockSize);

  addBias_NCHW_kernel<<<blocks, kBlockSize, 0, stream>>>(c, a, b, N, C, H, W,
                                                         activation);
  ReportCUDAErrors(cudaGetLastError());
}

template <typename dT, typename sT>
__device__ dT readNCHW(const sT* input_tensor, int n, int c, int h, int w,
                       int Nin, int Cin, int H, int W) {
  if (n >= Nin || c >= Cin) return 0;

  int index;
  index = n;
  index *= Cin;
  index += c;
  index *= H;
  index += h;
  index *= W;
  index += w;

  return (dT)(input_tensor[index]);
}

template <typename dT, typename sT>
__global__ void NCHWtoNHWC_kernel(dT* output_tensor, const sT* input_tensor,
                                  int Nin, int Cin, int Nout, int Cout, int H,
                                  int W) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;

  if (tid >= Nout * Cout * H * W) return;

  int index = tid;

  int c = (index % Cout);
  index /= Cout;
  int w = index % W;
  index /= W;
  int h = index % H;
  index /= H;
  int n = index;

  output_tensor[tid] =
      readNCHW<dT, sT>(input_tensor, n, c, h, w, Nin, Cin, H, W);
}

template <typename DstType, typename SrcType>
void convertNCHWtoNHWC(DstType* output_tensor, const SrcType* input_tensor,
                       int Nin, int Cin, int Nout, int Cout, int H, int W) {
  size_t numElements = Nout * Cout * H * W;
  const int blockSize = 256;
  int blocks = DivUp(numElements, blockSize);
  NCHWtoNHWC_kernel<<<blocks, blockSize>>>(output_tensor, input_tensor, Nin,
                                           Cin, Nout, Cout, H, W);
}

template <typename DstType, typename SrcType>
__global__ void copyTypeConverted_kernel(DstType* op, SrcType* ip, int N) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;

  if (tid >= N) return;

  DstType el = (DstType)ip[tid];
  op[tid] = el;
}

template <typename DstType, typename SrcType>
void copyTypeConverted(DstType* op, SrcType* ip, int N, cudaStream_t stream) {
  const int kBlockSize = 256;
  int blocks = DivUp(N, kBlockSize);
  copyTypeConverted_kernel<<<blocks, kBlockSize, 0, stream>>>(op, ip, N);
}

template <typename T>
__global__ void batchNorm_kernel(T* output, const T* input, const T* skipInput,
                                 int N, int C, int H, int W, const float* means,
                                 const float* varMultipliers,
                                 ActivationFunction activation) {
  int index = threadIdx.x + blockDim.x * blockIdx.x;

  int wIndex = 0;
  if (sizeof(T) == sizeof(float))
    wIndex = (index / (H * W)) % C;  // NCHW for fp32.
  else
    wIndex = index % C;  // NHWC for fp16.

  float el = input[index];
  float mean = means[wIndex];
  float varMulti = varMultipliers[wIndex];

  el -= mean;
  el *= varMulti;

  if (skipInput) el += (float)skipInput[index];

  el = activate(el, activation);

  output[index] = (T)el;
}

// Every thread processes single element.
template <typename T>
void batchNorm(T* output, const T* input, const T* skipInput, int N, int C,
               int H, int W, float* means, float* var_multipliers,
               ActivationFunction activation) {
  const int total_elements = N * C * H * W;
  const int kBlockSize = 256;
  int blocks = DivUp(total_elements, kBlockSize);

  batchNorm_kernel<<<blocks, kBlockSize>>>(output, input, skipInput, N, C, H, W,
                                           means, var_multipliers, activation);

  ReportCUDAErrors(cudaGetLastError());
}

__global__ void expandPlanes_kernel_Fp32_NCHW(float* output,
                                              const uint64_t* masks,
                                              const float* values, int n) {
  // Block size of 256, same mask/val for 64 consecutive threads.
  constexpr int kNumShmemElements = 256 / 64;

  __shared__ uint64_t shMasks[kNumShmemElements];
  __shared__ float shVals[kNumShmemElements];

  int index = threadIdx.x + blockDim.x * blockIdx.x;

  int planeIndex = index >> 6;

  if (planeIndex >= n) return;

  // Load inputs to shared memory.
  if (threadIdx.x < kNumShmemElements) {
    shMasks[threadIdx.x] = masks[planeIndex + threadIdx.x];
    shVals[threadIdx.x] = values[planeIndex + threadIdx.x];
  }
  __syncthreads();

  uint64_t mask = shMasks[threadIdx.x >> 6];

  int sqIndex = index & 0x3F;
  float op = 0;

  bool set = !!(mask & (1ull << sqIndex));
  if (set) {
    op = shVals[threadIdx.x >> 6];
  }
  output[index] = op;
}

void expandPlanes_Fp32_NCHW(float* output, const uint64_t* masks,
                            const float* values, int n, cudaStream_t stream) {
  int threads = n * 8 * 8;  // Each thread writes a single element.
  const int blockSize = 256;
  int blocks = DivUp(threads, blockSize);
  expandPlanes_kernel_Fp32_NCHW<<<blocks, blockSize, 0, stream>>>(output, masks,
                                                                  values, n);
  ReportCUDAErrors(cudaGetLastError());
}

// TODO: Can optimize using shared memory if this becomes a bottleneck.
__global__ void expandPlanes_kernel_Fp16_NHWC(half* output,
                                              const uint64_t* masks,
                                              const float* values, int n) {
  const int index = threadIdx.x + blockDim.x * blockIdx.x;
  if (index >= n * 8 * 8) return;

  const int planeIndex = index % kInputPlanes;
  const int boardIndex = index / (kInputPlanes * 8 * 8);
  const int sqIndex = (index / kInputPlanes) & 0x3F;

  uint64_t mask = masks[boardIndex * kInputPlanes + planeIndex];

  half op = 0;
  bool set = !!(mask & (1ull << sqIndex));
  if (set) {
    float val = values[boardIndex * kInputPlanes + planeIndex];
    op = (half)val;
  }
  output[index] = op;
}

void expandPlanes_Fp16_NHWC(half* output, const uint64_t* masks,
                            const float* values, int n, cudaStream_t stream) {
  int threads = n * 8 * 8;  // Each thread writes a single element.
  const int kBlockSize = 256;
  int blocks = DivUp(threads, kBlockSize);
  expandPlanes_kernel_Fp16_NHWC<<<blocks, kBlockSize, 0, stream>>>(
      output, masks, values, n);
  ReportCUDAErrors(cudaGetLastError());
}

__global__ void expandPlanes_kernel_Fp16_NCHW(half* output,
                                              const uint64_t* masks,
                                              const float* values, int n) {
  // block size of 256, same mask/val for 64 consecutive threads
  constexpr int kNumShmemElements = 256 / 64;

  __shared__ uint64_t shMasks[kNumShmemElements];
  __shared__ half shVals[kNumShmemElements];

  int index = threadIdx.x + blockDim.x * blockIdx.x;

  int planeIndex = index >> 6;

  if (planeIndex >= n) return;

  // load inputs to shared memory
  if (threadIdx.x < kNumShmemElements) {
    shMasks[threadIdx.x] = masks[planeIndex + threadIdx.x];
    shVals[threadIdx.x] = values[planeIndex + threadIdx.x];
  }
  __syncthreads();

  uint64_t mask = shMasks[threadIdx.x >> 6];

  int sqIndex = index & 0x3F;
  half op = 0;

  bool set = !!(mask & (1ull << sqIndex));
  if (set) {
    op = (half)shVals[threadIdx.x >> 6];
  }
  output[index] = op;
}

void expandPlanes_Fp16_NCHW(half* output, const uint64_t* masks,
                            const float* values, int n, cudaStream_t stream) {
  int threads = n * 8 * 8;  // each thread writes a single element
  const int blockSize = 256;
  int blocks = DivUp(threads, blockSize);
  expandPlanes_kernel_Fp16_NCHW<<<blocks, blockSize, 0, stream>>>(output, masks,
                                                                  values, n);
  ReportCUDAErrors(cudaGetLastError());
}

template <typename T>
__global__ void globalScale_kernel(T* output, const T* input,
                                   const T* scaleBias, const T* prevLayerBias,
                                   int inputSize, int C,
                                   ActivationFunction activation) {
  const int kPlaneSize = 64;

  int tid = blockIdx.x * blockDim.x + threadIdx.x;

  if (tid > inputSize) return;

  int nc = tid / kPlaneSize;
  int n = nc / C;
  int c = nc % C;

  float val1 = input[tid];   // Output of residual block to be scaled.
  float val2 = output[tid];  // Skip connection to be added directly.

  if (prevLayerBias) {
    val1 += (float)(prevLayerBias[c]);
  }

  int startIdx = n * 2 * C;  // Scale and bias interleaved.

  float s = scaleBias[startIdx + c];
  s = 1.0f / (1.0f + exp(-s));  // Sigmoid on scale.

  float b = scaleBias[startIdx + c + C];

  float op = val1 * s + val2 + b;
  op = activate(op, activation);
  output[tid] = (T)op;
}

__global__ void globalScale_kernel_fp16_nhwc(half* output, const half* input,
                                             const half* scaleBias,
                                             const half* prevLayerBias,
                                             int inputSize, int C, int HWC,
                                             ActivationFunction activation) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;

  if (tid > inputSize) return;

  int c = tid % C;
  int n = tid / (HWC);

  float val1 = (float)input[tid];   // Output of residual block to be scaled.
  float val2 = (float)output[tid];  // Skip connection to be added directly.
  if (prevLayerBias) {
    val1 += (float)prevLayerBias[c];
  }

  int startIdx = n * 2 * C;  // Scale and bias interleaved.

  float s = scaleBias[startIdx + c];
  s = 1.0f / (1.0f + exp(-s));  // Sigmoid on scale.

  float b = scaleBias[startIdx + c + C];

  float op = val1 * s + val2 + b;
  op = activate(op, activation);

  output[tid] = (half)op;
}

// N blocks.
// C threads per block.
// 'HWC' input data processed by thread block.
// Each thread writes a single output.
__global__ void globalAvgPool_kernel_NHWC_fp16(half* output, const half* input,
                                               const half* prevLayerBias,
                                               int inputSize, int outputSize) {
  const int elementsPerThread = 64;  // 8x8 board.

  int blockStart = blockIdx.x * blockDim.x;

  float S = 0;

#pragma unroll
  for (int i = 0; i < elementsPerThread; i++) {
    int localIndex = i * blockDim.x + threadIdx.x;
    int inputIndex = blockStart * elementsPerThread + localIndex;
    if (inputIndex < inputSize) S += (float)(input[inputIndex]);
  }

  float avg = S / elementsPerThread;

  // Add bias from previous layer.
  if (prevLayerBias) avg += (float)(prevLayerBias[threadIdx.x]);

  int opIndex = blockStart + threadIdx.x;
  if (opIndex < outputSize) output[opIndex] = (half)avg;
}

// Each thread reads 2 inputs (8x8/32), and each warp writes a single output.
template <typename T>
__global__ void globalAvgPool_kernel(T* output, const T* input,
                                     const T* prevLayerBias, int inputSize,
                                     int outputSize, int C) {
  const int elementsPerWarp = 64;
  const int elementsPerThread = 2;

  int tid = blockIdx.x * blockDim.x + threadIdx.x;

  int laneId = threadIdx.x & 0x1F;
  int laneStartIndex = (tid - laneId) * elementsPerThread;

  // Compute per-thread sum for elementsPerThread elements.
  float S = 0;

#pragma unroll
  for (int i = 0; i < elementsPerWarp; i += 32) {
    int index = laneStartIndex + laneId + i;
    if (index < inputSize) S += (float)(input[index]);
  }

// Compute warp wide sum (for entire plane - elementsPerWarp elements).
#pragma unroll
  for (int offset = 1; offset < 32; offset *= 2) {
    S += __shfl_down_sync(0xFFFFFFFF, S, offset);
  }

  float avg = S / elementsPerWarp;
  int opIndex = tid >> 5;

  // First thread in warp has the sum, write it in output.
  if (laneId == 0) {
    if (opIndex < outputSize) {
      if (prevLayerBias) avg += (float)prevLayerBias[opIndex % C];
      output[opIndex] = (T)avg;
    }
  }
}

template <typename T>
void globalAvgPool(int N, int C, T* output, const T* input,
                   const T* prevLayerBias, bool nhwc) {
  const int kPlaneSize = 64;

  const bool fp16 = std::is_same<half, T>::value;
  if (nhwc) {
    assert(fp16);
    // For NHWC fp16, simply launch N blocks, each with C threads.
    globalAvgPool_kernel_NHWC_fp16<<<N, C>>>((half*)output, (half*)input,
                                             (half*)prevLayerBias,
                                             N * C * kPlaneSize, N * C);
  } else {
    // For NCHW layout (used with fp32),
    // each warp processes a full plane (64 elements), and writes a single
    // average N*C warps are launched.

    const int kTotalWarps = N * C;
    const int kWarpsPerBlock = 8;
    const int kBlockSize = kWarpsPerBlock * 32;

    int blocks = DivUp(kTotalWarps, kWarpsPerBlock);
    globalAvgPool_kernel<<<blocks, kBlockSize>>>(output, input, prevLayerBias,
                                                 N * C * kPlaneSize, N * C, C);
  }
  ReportCUDAErrors(cudaGetLastError());
}

template <typename T>
void globalScale(int N, int C, T* output, const T* input, const T* scaleBias,
                 const T* prevLayerBias, bool nhwc,
                 ActivationFunction activation) {
  const bool fp16 = std::is_same<half, T>::value;

  // Each thread writes one output.
  const int kBlockSize = 256;
  const int kBlocks = DivUp(N * 8 * 8 * C, kBlockSize);

  if (nhwc) {
    assert(fp16);
    globalScale_kernel_fp16_nhwc<<<kBlocks, kBlockSize>>>(
        (half*)output, (half*)input, (half*)scaleBias, (half*)prevLayerBias,
        N * C * 8 * 8, C, 8 * 8 * C, activation);
  } else {
    globalScale_kernel<<<kBlocks, kBlockSize>>>(
        output, input, scaleBias, prevLayerBias, N * C * 8 * 8, C, activation);
  }
  ReportCUDAErrors(cudaGetLastError());
}

template <typename T>
__global__ void policyMap_kernel(T* output, const T* input,
                                 const short* indices, int N, int inputSize,
                                 int usedSize, int outputSize) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;

  int n = tid / usedSize;
  int i = tid % usedSize;

  if (n >= N) return;

  int j = indices[i];

  if (j >= 0) {
    output[n * outputSize + j] = input[n * inputSize + i];
  }
}

template <typename T>
void PolicyMap(int N, T* output, const T* input, const short* indices,
               int inputSize, int usedSize, int outputSize,
               cudaStream_t stream) {
  // Each thread processes one input element
  // Only some of the threads (with valid mapping) write output
  const int kBlockSize = 256;
  const int kBlocks = DivUp(N * usedSize, kBlockSize);

  policyMap_kernel<T><<<kBlocks, kBlockSize, 0, stream>>>(
      (T*)output, (T*)input, (short*)indices, N, inputSize, usedSize,
      outputSize);
  ReportCUDAErrors(cudaGetLastError());
}

template <typename T = float, bool use_se, ActivationFunction activation,
          bool use_bias, bool use_skip>
void OutputInputTransform(int N, int C, int se_K, T* output, const T* input,
                          const T* skip, const T* bias, const T* w1,
                          const T* b1, const T* w2, const T* b2,
                          cudaStream_t stream) {
  // Each thread processes entire chess board
  if (use_se == false) {
    dim3 grid_dim(DivUp(C, kOpInpTransformBlockSize), N, 1);
    OutputTransform_relu_InputTransform_kernel<float, activation, use_bias, use_skip>
        <<<grid_dim, kOpInpTransformBlockSize, 0, stream>>>(N, C, output, input,
                                                            (float*)skip, bias);
  } else if (C > kMaxResBlockFusingChannels) {
    throw Exception(
        "res block fusing opt not supported for the given data type and no "
        "of filters\n");
  } else {
    OutputTransform_SE_relu_InputTransform_kernel<float, activation,
                                                  use_bias, use_skip>
        <<<N, C, 0, stream>>>(N, C, se_K, output, input, (float*)skip, bias, w1,
                              b1, w2, b2);
  }

  ReportCUDAErrors(cudaGetLastError());
}


// softmax along C dimension which is assumed to be 64
// each thread processes two elements. Each warp computes a sum (over 64
// elements)
template <typename T>
__global__ void softmax_opt_64_kernel(T* output, const T* input, int N) {

  int index = blockDim.x * blockIdx.x + threadIdx.x;
  if (index >= N) return;

  float x[2];
  float ex[2];

  // Load from memory
  const bool fp16 = std::is_same<half, T>::value;
  if (fp16) {
    half inp[2];
    copyAs<int>(&inp[0], &input[index * 2]);
    x[0] = (float)inp[0];
    x[1] = (float)inp[1];
  } else {
    copyAs<uint2>(&x[0], &input[index * 2]);
  }

  ex[0] = exp(x[0]);
  ex[1] = exp(x[1]);

  float threadSum = ex[0] + ex[1];
  float Sum = warpReduce(threadSum);
  Sum = __shfl_sync(0xFFFFFFFF, Sum, 0);

  ex[0] = ex[0] / Sum;
  ex[1] = ex[1] / Sum;

  // Store to memory
  if (fp16) {
    half op[2];
    op[0] = (half)ex[0];
    op[1] = (half)ex[1];
    copyAs<int>(&output[index * 2], &op[0]);
  } else {
    copyAs<uint2>(&output[index * 2], &ex[0]);
  }
}


// N * C Tensors
// performs softmax along the C dimension
// Each thread processes one element
// Sums are computed in shared memory
// C threads per block, N blocks
template <typename T>
__global__ void softmax_kernel(T* output, const T* input) {
  int n = blockIdx.x;
  int c = threadIdx.x;
  int C = blockDim.x;
  int index = n * C + c;

  __shared__ float sum;
  if (c == 0) sum = 0;
  __syncthreads();

  // softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis)

  float x = (float)input[index];
  float ex = exp(x);

  // compute warp wide sums first
  float val = warpReduce(ex);

  // update shared memory sum across C dimension
  if ((c & 0x1F) == 0) atomicAdd(&sum, val);

  __syncthreads();

  float op = ex / sum;

  output[index] = (T)op;
}

template <typename T>
void Softmax(int N, int C, T* output, const T* input, cudaStream_t stream) {
  if (C == 64) {
    int size = N * 32;              // Total no of threads needed
    const int kBlockSize = 256;
    int blocks = DivUp(size, kBlockSize);
    softmax_opt_64_kernel<T><<<blocks, kBlockSize, 0, stream>>>(output, input, size);
  } else {
    softmax_kernel<T><<<N, C, 0, stream>>>(output, input);
  }

  ReportCUDAErrors(cudaGetLastError());
}

__device__ __forceinline__ float shared_sum_for_layer_norm(float x) {
  // compute warp-wide sum
  float s = warpReduce(x);

  // warp-wide sums
  // Max product of the two dimension for the below array is 16 (512/32), but
  // we make each dimension 16 for simplicity. if shared memory capacity is the
  // bottleneck (it's not), we can convert these to single dim array and
  // dynamically index
  __shared__ float sum[16][16];

  // compute sum across C dimension using the warp wide partial sums
  if (threadIdx.x == 0) sum[threadIdx.z][threadIdx.y] = s;
  __syncthreads();

  if (threadIdx.x == 0 && threadIdx.y == 0) {
    float cSum = 0;
    for (int j = 0; j < blockDim.y; j++) cSum += sum[threadIdx.z][j];
    sum[threadIdx.z][0] = cSum;
  }
  __syncthreads();

  // s now contains the sum across C dimension
  return sum[threadIdx.z][0];
}

// Each thread processes 4 elements
// 1. Perform Bias add, and skip add
// 2. Perform layer norm (normalize across C dimension)
template <typename T>
__global__ void layer_norm_kernel(int N, int C, T* output, const T* input, const T* bias,
                                  const T* skip, const T* gammas,
                                  const T* betas, float ep) {
  int n = blockIdx.x * blockDim.z + threadIdx.z;
  if (n >= N) return;
  int c = (threadIdx.y * 32 + threadIdx.x) * 4;
  bool oobThread = c >= C;

  int biasIndex = c;
  int tensorIndex = n * C + c;

  float val[4] = {0, 0, 0, 0};
  float b[4] = {0, 0, 0, 0};
  float sk[4] = {0, 0, 0, 0};
  float bts[4] = {0, 0, 0, 0};
  float gms[4] = {0, 0, 0, 0};

  const bool fp16 = std::is_same<half, T>::value;
  if (!oobThread) {
    // Load from memory (4 elements a time)
    if (fp16) {
      half inp[4];
      copyAs<uint2>(&inp[0], &input[tensorIndex]);
      for (int i = 0; i < 4; i++) val[i] = (float)inp[i];
      copyAs<uint2>(&inp[0], &skip[tensorIndex]);
      for (int i = 0; i < 4; i++) sk[i] = (float)inp[i];
      copyAs<uint2>(&inp[0], &bias[biasIndex]);
      for (int i = 0; i < 4; i++) b[i] = (float)inp[i];
      copyAs<uint2>(&inp[0], &betas[biasIndex]);
      for (int i = 0; i < 4; i++) bts[i] = (float)inp[i];
      copyAs<uint2>(&inp[0], &gammas[biasIndex]);
      for (int i = 0; i < 4; i++) gms[i] = (float)inp[i];
    } else {
      copyAs<uint4>(&val[0], &input[tensorIndex]);
      copyAs<uint4>(&sk[0], &skip[tensorIndex]);
      copyAs<uint4>(&b[0], &bias[biasIndex]);
      copyAs<uint4>(&bts[0], &betas[biasIndex]);
      copyAs<uint4>(&gms[0], &gammas[biasIndex]);
    }
  }

  // 1. Compute mean
  float s = 0;
  if (!oobThread)
    for (int i = 0; i < 4; i++) {
      val[i] += b[i] + sk[i];
      s += val[i];
    }
  
  s = shared_sum_for_layer_norm(s);
  float mean = s / C;

  // 2. Compute varience
  s = 0;
  if (!oobThread)
    for (int i = 0; i < 4; i++) {
      float d = val[i] - mean;
      float d_sq = d * d;
      s += d_sq;
    }
  s = shared_sum_for_layer_norm(s);
  float var = s / C;

  // 3. Normalize
  for (int i = 0; i < 4; i++) {
    float d = val[i] - mean;
    float norm = d / sqrt(var + ep);
    float op = norm * gms[i] + bts[i];
    val[i] = op;
  }

  if (!oobThread) {
    // Write to memory
    if (fp16) {
      half op[4];
      for (int i = 0; i < 4; i++) op[i] = (half)val[i];
      copyAs<uint2>(&output[tensorIndex], &op[0]);
    } else {
      copyAs<uint4>(&output[tensorIndex], &val[0]);
    }
  }
}

// add (optional) skip connection to input, and then perform Layer normalization
// normalization is done across C dimension (i.e, sums and std deviations taken over elements in C dim)
template <typename T>
void LayerNorm(int N, int C, T* output, const T* input, const T* bias,
               const T* skip, const T* gammas, const T* betas, float ep,
               cudaStream_t stream) {
  // process 4 elements per thread to achieve close to peak memory bandwidth
  if (C % 4 != 0) throw Exception("unsupported filter size");
  if (C > 4096) throw Exception("unsupported filter size");

  dim3 blockDim, gridDim;
  blockDim.x = 32;
  blockDim.y = DivUp(C / 4, 32);
  blockDim.z =
      std::min(std::max(512 / (blockDim.x * blockDim.y), 1u), (unsigned int)N);
  gridDim.x = DivUp(N, blockDim.z);
  gridDim.y = 1;
  gridDim.z = 1;

  layer_norm_kernel<T><<<gridDim, blockDim, 0, stream>>>(
      N, C, output, input, bias, skip, gammas, betas, ep);

  ReportCUDAErrors(cudaGetLastError());
}

// Compute promotion logits in a single kernel
// keys matrix is of N * 64 * C (but we use only last 8 from the 'rows'
// dimension, so N * 8 * C)
// ppo matrix is 4 * C (weights for dense layer / matrix multiplication)
// policy_attn_logits matrix is N * 64 * 64, but we use only 8x8 part of it
// from each batch dimension (so, N * 8 * 8)
// output matrix (promotion logits) is of N * 8 * 24 size
template <typename T>
__global__ void promotion_logits_kernel(int C, T* output, const T* keys,
                                        const T* ppo,
                                        const T* policy_attn_logits) {
  constexpr int output_stride = 64 * 64 + 8 * 24;
  int n = blockIdx.x;   // [0..N)
  int y = threadIdx.y;  // [0..8)
  int x = threadIdx.x;  // [0..24)     // Can split into 8 * 3

  int threadInGroup = threadIdx.y * 24 + threadIdx.x;

  // phase 1 : compute promotion_offsets by multiplying keys and ppo matrices
  const T* keys_start =
      keys + n * 64 * C + C * 56;  // we are interested only in last 8 out of 64
                                   // 'rows' of keys matrix
  __shared__ float promotion_offsets[4][8];

  // only 32 threads out of 192 in the group are active in this phase, and each
  // thread computes one element of the promotion_offsets matrix
  // TODO: opt idea1, can use more threads to reduce the length of the loop for
  // the matrix multiply (do parallel reduction of partial sums later)
  //       opt idea2, the below loop for matrix mul has very poor memory access
  //       pattern, can do the loop over 32, and do parallel reductions
  if (threadInGroup < 32) {
    int x = threadInGroup % 4;
    int y = threadInGroup / 4;

    float S = 0;
    for (int i = 0; i < C;
         i++) {  // TODO: modify to loop over 32 instead of C (doing parallel
                 // reductions for the 32 sums)
      float a = (float)keys_start[y * C + i];
      float b =
          (float)ppo[x * C + i];  // weight matrix is transposed (col major)
      S += a * b;
    }

    // write the product (promotion_offsets) in shared memory
    promotion_offsets[x][y] = S;
  }

  __syncthreads();

  // phase 2: add the last "row" to the other 3
  // #knight offset is added to the other three
  // promotion_offsets = promotion_offsets[:, :3, :] + promotion_offsets[:, 3:4,
  // :] 
  // Only 24 threads in the group are active in this phase
  if (threadInGroup < 32) {
    int x = threadInGroup % 4;
    int y = threadInGroup / 4;
    if (x < 3) {
      promotion_offsets[x][y] += promotion_offsets[3][y];
    }
  }

  __syncthreads();

  // phase 3: add 8x8 chunk of policy_attn_logits matrix to promotion offsets
  //          the output is 3x8x8 (written as 8 * 24)
  // All threads are active in this phase and they compute one element each
  int w = x / 3;
  int c = x % 3;

  // n_promo_logits = matmul_qk[:, -16:-8, -8:]  # default traversals from rank
  // 7 to rank 8
  float n_promo_logit =
      (float)policy_attn_logits[n * output_stride + (48 + y) * 64 + (56 + w)];
  float promo_offset = promotion_offsets[c][w];

  float op = n_promo_logit + promo_offset;

  output[n * output_stride + threadInGroup] = (T)op;
}

template <typename T>
void ComputePromotionLogits(int N, int C, T* output, const T* keys,
                            const T* ppo, const T* policy_attn_logits,
                            cudaStream_t stream) {
  // N blocks
  // 8 * 24 threads
  // Each thread computes a single output element
  dim3 blockDim(24, 8, 1);
  promotion_logits_kernel<T>
      <<<N, blockDim, 0, stream>>>(C, output, keys, ppo, policy_attn_logits);
}

// Template instantiation.
template void copyTypeConverted<half, float>(half* op, float* ip, int N,
                                             cudaStream_t stream);
template void copyTypeConverted<float, half>(float* op, half* ip, int N,
                                             cudaStream_t stream);
template void copyTypeConverted<float, float>(float* op, float* ip, int N,
                                              cudaStream_t stream);
template void copyTypeConverted<half, half>(half* op, half* ip, int N,
                                            cudaStream_t stream);

template void batchNorm<float>(float* output, const float* input,
                               const float* skipInput, int N, int C, int H,
                               int W, float* means, float* var_multipliers,
                               ActivationFunction activation);
template void batchNorm<half>(half* output, const half* input,
                              const half* skipInput, int N, int C, int H, int W,
                              float* means, float* var_multipliers,
                              ActivationFunction activation);

template void addVectors<float>(float* c, float* a, float* b, int size,
                                int asize, int bsize, ActivationFunction act,
                                cudaStream_t stream);
template void addVectors<half>(half* c, half* a, half* b, int size, int asize,
                               int bsize, ActivationFunction act,
                               cudaStream_t stream);

template void addBiasBatched<float>(float* output, const float* input,
                                    const float* bias, int Batch, int N, int C,
                                    ActivationFunction activation,
                                    cudaStream_t stream);
template void addBiasBatched<half>(half* output, const half* input,
                                   const half* bias, int Batch, int N, int C,
                                   ActivationFunction activation,
                                   cudaStream_t stream);

template void addBias_NCHW<float>(float* c, float* a, float* b, int N, int C,
                                  int H, int W, ActivationFunction activation,
                                  cudaStream_t stream);

template void addBias_NCHW<half>(half* c, half* a, half* b, int N, int C, int H,
                                 int W, ActivationFunction activation,
                                 cudaStream_t stream);

template void globalAvgPool<float>(int N, int C, float* output,
                                   const float* input,
                                   const float* prevLayerBias, bool nhwc);
template void globalAvgPool<half>(int N, int C, half* output, const half* input,
                                  const half* prevLayerBias, bool nhwc);

template void globalScale<float>(int N, int C, float* output,
                                 const float* input, const float* scaleBias,
                                 const float* prevLayerBias, bool nhwc,
                                 ActivationFunction activation);
template void globalScale<half>(int N, int C, half* output, const half* input,
                                const half* scaleBias,
                                const half* prevLayerBias, bool nhwc,
                                ActivationFunction activation);

template void PolicyMap<float>(int N, float* output, const float* input,
                               const short* indices, int inputSize,
                               int usedSize, int outputSize,
                               cudaStream_t stream);

template void PolicyMap<half>(int N, half* output, const half* input,
                              const short* indices, int inputSize, int usedSize,
                              int outputSize, cudaStream_t stream);

template void FilterTransform<float>(int N, int C, float* transformedFilter,
                                     const float* filter);

template void InputTransform<float, true>(int N, int C,
                                          float* transformed_input,
                                          const float* input,
                                          cudaStream_t stream);

template void InputTransform<float, false>(int N, int C,
                                           float* transformed_input,
                                           const float* input,
                                           cudaStream_t stream);

template void OutputTransform<float, true, RELU, true, true, false, false>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputTransform<float, false, RELU, true, true, false, false>(

    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputTransform<float, true, RELU, true, true, true, false>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputTransform<float, false, RELU, true, true, true, false>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputTransform<float, false, RELU, true, false, false, false>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputTransform<float, false, RELU, true, false, false, true>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputTransform<float, true, RELU, true, true, true, true>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputTransform<float, true, MISH, true, true, false, false>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputTransform<float, false, MISH, true, true, false, false>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputTransform<float, true, MISH, true, true, true, false>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputTransform<float, false, MISH, true, true, true, false>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputTransform<float, false, MISH, true, false, false, false>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputTransform<float, false, MISH, true, false, false, true>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputTransform<float, true, MISH, true, true, true, true>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputTransform<float, false, NONE, true, false, false, false>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputInputTransform<float, true, RELU, true, true>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputInputTransform<float, false, RELU, true, true>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputInputTransform<float, false, RELU, true, false>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputInputTransform<float, true, MISH, true, true>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputInputTransform<float, false, MISH, true, true>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void OutputInputTransform<float, false, MISH, true, false>(
    int N, int C, int se_K, float* output, const float* input,
    const float* skip, const float* bias, const float* w1, const float* b1,
    const float* w2, const float* b2, cudaStream_t stream);

template void Softmax<half>(int N, int C, half* output, const half* input,
                            cudaStream_t stream);
template void Softmax<float>(int N, int C, float* output, const float* input,
                             cudaStream_t stream);

template void LayerNorm<half>(int N, int C, half* output, const half* input,
                              const half* bias, const half* skip,
                              const half* gammas, const half* betas, float ep,
                              cudaStream_t stream);
template void LayerNorm<float>(int N, int C, float* output, const float* input,
                               const float* bias, const float* skip,
                               const float* gammas, const float* betas,
                               float ep, cudaStream_t stream);

template void ComputePromotionLogits<half>(int N, int C, half* output,
                                           const half* keys, const half* ppo,
                                           const half* policy_attn_logits,
                                           cudaStream_t stream);
template void ComputePromotionLogits<float>(int N, int C, float* output,
                                            const float* keys, const float* ppo,
                                            const float* policy_attn_logits,
                                            cudaStream_t stream);

template void convertNCHWtoNHWC<half, float>(half* output_tensor,
                                             const float* input_tensor, int Nin,
                                             int Cin, int Nout, int Cout, int H,
                                             int W);
template void convertNCHWtoNHWC<float, float>(float* output_tensor,
                                              const float* input_tensor,
                                              int Nin, int Cin, int Nout,
                                              int Cout, int H, int W);
template void convertNCHWtoNHWC<half, half>(half* output_tensor,
                                            const half* input_tensor, int Nin,
                                            int Cin, int Nout, int Cout, int H,
                                            int W);
}  // namespace cudnn_backend
}  // namespace lczero

```

`src/neural/cuda/cuda_common.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#pragma once

#include <cublas_v2.h>
#include <cuda_fp16.h>
#include <cuda_runtime.h>

#include "utils/exception.h"

#ifdef USE_CUDNN
#include <cudnn.h>
#else
typedef void* cudnnHandle_t;
#endif

#if CUBLAS_VER_MAJOR < 11
#define CUBLAS_PEDANTIC_MATH CUBLAS_DEFAULT_MATH
#endif

namespace lczero {
namespace cudnn_backend {

static constexpr int kNumOutputPolicy = 1858;

// max supported filter count for fast path
// TODO: extend it to cover bigger networks!
// (We are limited by no of registers per thread)
static constexpr int kMaxResBlockFusingChannels = 384;  // limit on num_filters
static constexpr int kMaxResBlockFusingSeKFp16Ampere =
    512;  // (use a different kernel with reduced register pressure)
static constexpr int kMaxResBlockFusingSeK =
    128;  // limit on (num_filters / se_ratio)
static constexpr int kMaxResBlockFusingSeFp16AmpereSmem =
    72 * kMaxResBlockFusingSeKFp16Ampere *
    sizeof(half);  // shared memory used by the special
                   // kernel

#ifdef USE_CUDNN
void CudnnError(cudnnStatus_t status, const char* file, const int& line);
#endif
void CublasError(cublasStatus_t status, const char* file, const int& line);
void CudaError(cudaError_t status, const char* file, const int& line);

#ifdef USE_CUDNN
#define ReportCUDNNErrors(status) CudnnError(status, __FILE__, __LINE__)
#endif
#define ReportCUBLASErrors(status) CublasError(status, __FILE__, __LINE__)
#define ReportCUDAErrors(status) CudaError(status, __FILE__, __LINE__)

inline int DivUp(int a, int b) { return (a + b - 1) / b; }

enum ActivationFunction { NONE, RELU, TANH, SIGMOID, SELU, MISH };

}  // namespace cudnn_backend
}  // namespace lczero

```

`src/neural/cuda/fp16_kernels.cu`:

```cu
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "cuda_common.h"
#include "winograd_helper.inc"

namespace lczero {
namespace cudnn_backend {

/////////////////////////////////////////////////////////////////////////////
//          fp16-specific kernels used by certain layers                   //
/////////////////////////////////////////////////////////////////////////////

// SE layer implementation using single fused kernel.

// N blocks.
// C threads per block.
// 'HWC' input data processed by thread block.
// Each thread processes 8x8 elements.
// K is the no. of outputs of first fully connected layer (same as no. of inputs
// for second fully connected layer).
// The kernel assumes K <= C.

template <int C, int K>
__global__ void SE_Layer_NHWC(half* output, const half* skip, const half* input,
                              const half* w1, const half* b1, const half* w2,
                              const half* b2, const half* bPrev,
                              ActivationFunction activation) {
  const int elementsPerThread = 64;  // 8x8 board
  const int se_K = K;

  int n = blockIdx.x;
  int c = threadIdx.x;

  __shared__ half sharedData[C];

  half2 localData[elementsPerThread];

  half S = 0;

  half bias = 0;
  if (bPrev) bias = bPrev[c];

// 1. Global avg (1 avg per thread).
#pragma unroll
  for (int i = 0; i < elementsPerThread; i++) {
    int localIndex = i * C + c;
    int inputIndex = n * C * elementsPerThread + localIndex;
    localData[i].x = input[inputIndex] + bias;
    localData[i].y = skip[inputIndex];
    S += localData[i].x;
  }

  half avg = S / (half)elementsPerThread;
  sharedData[c] = avg;

  __syncthreads();

  // 2. First fully connected layer.
  if (c < K) {
    S = 0;

#pragma unroll
    for (int i = 0; i < C; i++) {
      S += sharedData[i] * readw1(i, c);
    }

    S += b1[c];

    S = activate(S, activation);

    sharedData[c] = S;
  }
  __syncthreads();

  // 3. Second fully connected layer.
  S = 0;
  half B = 0;
#pragma unroll
  for (int i = 0; i < K; i++) {
    half val = sharedData[i];
    S += val * readw2(i, c);
    B += val * readw2(i, c + C);
  }
  S += b2[c];
  B += b2[c + C];

  // Sigmoid (only on the scale part).
  S = (half)(1.0f / (1.0f + exp(-(float)(S))));

// 4. Scale, and add skip connection, perform relu, and write to output.
#pragma unroll
  for (int i = 0; i < elementsPerThread; i++) {
    int localIndex = i * C + c;
    int inputIndex = n * C * elementsPerThread + localIndex;
    half val = localData[i].y + localData[i].x * S + B;

    // Relu activation function.
    val = (half)activate((float)val, activation);

    output[inputIndex] = val;
  }
}

bool Se_Fp16_NHWC(int N, int C, int numFc1Out, half* output, const half* skip,
                  const half* input, const half* w1, const half* b1,
                  const half* w2, const half* b2, const half* bPrev,
                  ActivationFunction activation) {
  // TODO: Think of more elegant way to avoid this hardcoding :-/
  if (numFc1Out == 16) {
    if (C == 64) {
      SE_Layer_NHWC<64, 16>
          <<<N, C>>>(output, skip, input, w1, b1, w2, b2, bPrev, activation);
    } else {
      // TODO: support other channel counts.
      throw Exception("channel count unsupported by SE layer");
    }
  } else if (numFc1Out == 32) {
    if (C == 64) {
      SE_Layer_NHWC<64, 32>
          <<<N, C>>>(output, skip, input, w1, b1, w2, b2, bPrev, activation);
    } else if (C == 128) {
      SE_Layer_NHWC<128, 32>
          <<<N, C>>>(output, skip, input, w1, b1, w2, b2, bPrev, activation);
    } else if (C == 192) {
      SE_Layer_NHWC<192, 32>
          <<<N, C>>>(output, skip, input, w1, b1, w2, b2, bPrev, activation);
    } else if (C == 256) {
      SE_Layer_NHWC<256, 32>
          <<<N, C>>>(output, skip, input, w1, b1, w2, b2, bPrev, activation);
    } else if (C == 320) {
      SE_Layer_NHWC<320, 32>
          <<<N, C>>>(output, skip, input, w1, b1, w2, b2, bPrev, activation);
    } else if (C == 352) {
      SE_Layer_NHWC<352, 32>
          <<<N, C>>>(output, skip, input, w1, b1, w2, b2, bPrev, activation);
    } else if (C == 384) {
      SE_Layer_NHWC<384, 32>
          <<<N, C>>>(output, skip, input, w1, b1, w2, b2, bPrev, activation);
    } else {
      // TODO: support other channel counts.
      return false;
    }
  } else if (numFc1Out == 64) {
    if (C == 64) {
      SE_Layer_NHWC<64, 64>
          <<<N, C>>>(output, skip, input, w1, b1, w2, b2, bPrev, activation);
    } else if (C == 128) {
      SE_Layer_NHWC<128, 64>
          <<<N, C>>>(output, skip, input, w1, b1, w2, b2, bPrev, activation);
    } else if (C == 192) {
      SE_Layer_NHWC<192, 64>
          <<<N, C>>>(output, skip, input, w1, b1, w2, b2, bPrev, activation);
    } else if (C == 256) {
      SE_Layer_NHWC<256, 64>
          <<<N, C>>>(output, skip, input, w1, b1, w2, b2, bPrev, activation);
    } else if (C == 320) {
      SE_Layer_NHWC<320, 64>
          <<<N, C>>>(output, skip, input, w1, b1, w2, b2, bPrev, activation);
    } else if (C == 384) {
      SE_Layer_NHWC<384, 64>
          <<<N, C>>>(output, skip, input, w1, b1, w2, b2, bPrev, activation);
    } else {
      // TODO: support other channel counts.
      return false;
    }
  } else {
    // TODO: support other sizes.
    return false;
  }
  ReportCUDAErrors(cudaGetLastError());
  return true;
}

// Get board for this thread from shared memory.
// We are just using shared memory to store local thread data in this kernel to
// help reduce some register pressure and spills to local memory.
#define BOARD(y, x) shboard[(y)*8 + (x)]

// input is in transformed space (HWNC layout) --- output of GEMM
// output is also in transformed space (HWNC layout) --- input to GEMM (for
// next layer)
// 'C' threads per block
// 'N' blocks
// Every thread generates an entire board/plane (8x8 elements).
template <ActivationFunction activation, bool use_bias,
          bool use_skip>
__global__ __launch_bounds__(kMaxResBlockFusingSeKFp16Ampere,1)
void OutputInputTransformKernel_fp16_shmem_board(
        int N, int C, int se_K, half* output, const half* input, half* skip,
        const half* bias, const half* w1, const half* b1, const half* w2,
        const half* b2) {
  int k = threadIdx.x;
  int n = blockIdx.x;

  extern __shared__ half _sboard[];
  half* shboard = &_sboard[k * 72];  // 72 instead of 64 to reduce shared
                                     // memory bank conflicts.
  half b = bias[k];

#pragma unroll
  for (int hStart = 0; hStart < 8; hStart += 4)
#pragma unroll
    for (int wStart = 0; wStart < 8; wStart += 4) {
      //  i) read to per thread registers (for doing output transform)
      int shln = n * 4 + (hStart / 4) * 2 + (wStart / 4);
      half outElTransformed[6][6];
#pragma unroll
      for (int y = 0; y < 6; y++)
#pragma unroll
        for (int x = 0; x < 6; x++)
          outElTransformed[y][x] = input[TEMP_INDEX_HWNC(y, x, shln, k)];

      // ii) transform it
      half outEl[4][4];
      OutputTransform4x4(&outEl[0][0], &outElTransformed[0][0]);

#pragma unroll
      for (int y = 0; y < 4; y++)
        copyAs<uint2>(&BOARD(hStart + y, wStart), &outEl[y][0]);
    }

  // Add bias, and compute the average for SE.
  float S = 0;
  float B = 0;

#pragma unroll
  for (int y = 0; y < 8; y++) {
    half boardRow[8];
    copyAs<uint4>(&boardRow, &BOARD(y, 0));
#pragma unroll
    for (int x = 0; x < 8; x++) {
      if (use_bias) boardRow[x] += b;
      S += (float)boardRow[x];
    }
    if (use_bias) copyAs<uint4>(&BOARD(y, 0), &boardRow);
  }

  __shared__ float shared_data[kMaxResBlockFusingSeKFp16Ampere];
  float avg = S / 64;
  shared_data[k] = avg;

  int lane = k & 0x1F;
  int warp = k >> 5;
  __syncthreads();

  // First fully-connected layer for SE

  // As se_K << C, we want to loop over se_K instead of C
  // even if it means taking the sum across threads

  __shared__ float shared_sums[kMaxResBlockFusingSeKFp16Ampere / 32]
                              [kMaxResBlockFusingSeK];  // per-warp sums

  for (int i = 0; i < se_K; i++) {
    float val = shared_data[k] * float(readw1(k, i));
    val = warpReduce(val);
    if (lane == 0) shared_sums[warp][i] = val;
  }
  __syncthreads();
  if (k < se_K) {
    S = 0;
    for (int i = 0; i < C / 32; i++) S += shared_sums[i][k];

    S += (float)b1[k];
    S = activate(S, activation);
    shared_data[k] = S;
  }

  __syncthreads();

  // Second fully-connected layer for SE
  S = 0;
  for (int i = 0; i < se_K; i++) {
    float val = shared_data[i];
    S += val * float(readw2(i, k));
    B += val * float(readw2(i, k + C));
  }
  S += (float)b2[k];
  B += (float)b2[k + C];

  // Sigmoid (only on the scale part).
  S = 1.0f / (1.0f + exp(-S));

  // Scale/bias, add skip connection, perform activation, and write to output.
  for (int h = 0; h < 8; h++) {
    half boardRow[8];
    copyAs<uint4>(&boardRow[0], &BOARD(h, 0));

#pragma unroll
    for (int w = 0; w < 8; w++) {
      boardRow[w] = (half)(float(boardRow[w]) * S + B);
    }

    // residual add
    if (use_skip) {
      half skipInp[8];
      copyAs<uint4>(&skipInp[0], &skip[INDEX_NHCW(n, k, h, 0)]);
#pragma unroll
      for (int w = 0; w < 8; w++) boardRow[w] += skipInp[w];
    }

    if (activation != NONE) {
#pragma unroll
      for (int w = 0; w < 8; w++)
        boardRow[w] = (half)activate((float)boardRow[w], activation);
    }

    // write un-transformed output to 'skip' if required
    if (use_skip) {
      copyAs<uint4>(&skip[INDEX_NHCW(n, k, h, 0)], &boardRow[0]);
    }

    copyAs<uint4>(&BOARD(h, 0), &boardRow);
  }


  // Perform input transform.

  int c = k;
  // top-left
  {
    half inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i + 1][j + 1] = BOARD(i, j);

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 0, c)] = inEl[y][x];
  }

  // top-right
  {
    half inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i + 1][j] = BOARD(i, j + 3);

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 1, c)] = inEl[y][x];
  }

  // bottom-left
  {
    half inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i][j + 1] = BOARD(i + 3, j);

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 2, c)] = inEl[y][x];
  }

  // bottom-right
  {
    half inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i][j] = BOARD(i + 3, j + 3);

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 3, c)] = inEl[y][x];
  }
}

template <typename T = half, bool use_se, ActivationFunction activation,
          bool use_bias, bool use_skip>
void OutputInputTransform(int N, int C, int se_K, T* output, const T* input,
                          const T* skip, const T* bias, const T* w1,
                          const T* b1, const T* w2, const T* b2,
                          cudaStream_t stream) {
  // Each thread processes entire chess board.
  if (use_se == false) {
    dim3 grid_dim(DivUp(C, kOpInpTransformBlockSize), N, 1);
    OutputTransform_relu_InputTransform_kernel<half, activation, use_bias,
                                               use_skip>
        <<<grid_dim, kOpInpTransformBlockSize, 0, stream>>>(N, C, output, input,
                                                            (half*)skip, bias);
  } else if (C > kMaxResBlockFusingChannels) {
    // Use special kernel with reduced register pressure - only works on Ampere,
    // and only for fp16.
    if (C <= kMaxResBlockFusingSeKFp16Ampere) {
      cudaFuncSetAttribute(
          OutputInputTransformKernel_fp16_shmem_board<activation,
                                                      use_bias, use_skip>,
          cudaFuncAttributeMaxDynamicSharedMemorySize,
          72 * C * sizeof(half));
      OutputInputTransformKernel_fp16_shmem_board<activation, use_bias,
                                                  use_skip>
          <<<N, C, 72 * C * sizeof(half), stream>>>(
              N, C, se_K, (half*)output, (const half*)input, (half*)skip,
              (half*)bias, (half*)w1, (half*)b1, (half*)w2, (half*)b2);
    } else {
      throw Exception(
          "res block fusing opt not supported for the given data type and no "
          "of filters\n");
    }
  } else {
    OutputTransform_SE_relu_InputTransform_kernel<half, activation,
                                                  use_bias, use_skip>
        <<<N, C, 0, stream>>>(N, C, se_K, output, input, (half*)skip, bias, w1,
                              b1, w2, b2);
  }
  ReportCUDAErrors(cudaGetLastError());
}

template void FilterTransform<half>(int N, int C, half* transformedFilter,
                                    const half* filter);

template void InputTransform<half, true>(int N, int C, half* transformed_input,
                                         const half* input,
                                         cudaStream_t stream);
template void InputTransform<half, false>(int N, int C, half* transformed_input,
                                          const half* input,
                                          cudaStream_t stream);

template void OutputTransform<half, true, RELU, true, true, false, false>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputTransform<half, false, RELU, true, true, false, false>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputTransform<half, true, RELU, true, true, true, false>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputTransform<half, false, RELU, true, true, true, false>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputTransform<half, false, RELU, true, false, false, false>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputTransform<half, false, RELU, true, false, false, true>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputTransform<half, true, RELU, true, true, true, true>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputTransform<half, true, MISH, true, true, false, false>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputTransform<half, false, MISH, true, true, false, false>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputTransform<half, true, MISH, true, true, true, false>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputTransform<half, false, MISH, true, true, true, false>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputTransform<half, false, MISH, true, false, false, false>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputTransform<half, false, MISH, true, false, false, true>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputTransform<half, true, MISH, true, true, true, true>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputTransform<half, false, NONE, true, false, false, false>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputInputTransform<half, true, RELU, true, true>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputInputTransform<half, false, RELU, true, true>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputInputTransform<half, false, RELU, true, false>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputInputTransform<half, true, MISH, true, true>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputInputTransform<half, false, MISH, true, true>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

template void OutputInputTransform<half, false, MISH, true, false>(
    int N, int C, int se_K, half* output, const half* input, const half* skip,
    const half* bias, const half* w1, const half* b1, const half* w2,
    const half* b2, cudaStream_t stream);

}  // namespace cudnn_backend
}  // namespace lczero

```

`src/neural/cuda/inputs_outputs.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "neural/network.h"

namespace lczero {
namespace cudnn_backend {

struct InputsOutputs {
  InputsOutputs(int maxBatchSize, bool wdl, bool moves_left,
                size_t tensor_mem_size = 0, size_t scratch_size = 0,
                bool cublasDisableTensorCores = false) {
    ReportCUDAErrors(cudaHostAlloc(
        &input_masks_mem_, maxBatchSize * kInputPlanes * sizeof(uint64_t),
        cudaHostAllocMapped));
    ReportCUDAErrors(
        cudaHostGetDevicePointer(&input_masks_mem_gpu_, input_masks_mem_, 0));

    ReportCUDAErrors(cudaHostAlloc(&input_val_mem_,
                                   maxBatchSize * kInputPlanes * sizeof(float),
                                   cudaHostAllocMapped));
    ReportCUDAErrors(
        cudaHostGetDevicePointer(&input_val_mem_gpu_, input_val_mem_, 0));

    ReportCUDAErrors(cudaHostAlloc(
        &op_policy_mem_, maxBatchSize * kNumOutputPolicy * sizeof(float), 0));

    // Seperate device memory copy for policy output.
    // It's faster to write to device memory and then copy to host memory
    // than having the kernel write directly to it.
    ReportCUDAErrors(cudaMalloc(
        &op_policy_mem_gpu_, maxBatchSize * kNumOutputPolicy * sizeof(float)));

    ReportCUDAErrors(cudaHostAlloc(&op_value_mem_,
                                   maxBatchSize * (wdl ? 3 : 1) * sizeof(float),
                                   cudaHostAllocMapped));
    ReportCUDAErrors(
        cudaHostGetDevicePointer(&op_value_mem_gpu_, op_value_mem_, 0));
    if (moves_left) {
      ReportCUDAErrors(cudaHostAlloc(&op_moves_left_mem_,
                                     maxBatchSize * sizeof(float),
                                     cudaHostAllocMapped));
      ReportCUDAErrors(cudaHostGetDevicePointer(&op_moves_left_mem_gpu_,
                                                op_moves_left_mem_, 0));
    }

    // memory for network execution managed inside this structure
    if (tensor_mem_size) {
      multi_stream_ = true;
      ReportCUDAErrors(cudaStreamCreate(&stream_));
      ReportCUDAErrors(cudaMalloc(&scratch_mem_, scratch_size));
      for (auto& mem : tensor_mem_) {
        ReportCUDAErrors(cudaMalloc(&mem, tensor_mem_size));
        ReportCUDAErrors(cudaMemsetAsync(mem, 0, tensor_mem_size, stream_));
      }
      ReportCUBLASErrors(cublasCreate(&cublas_));
      ReportCUBLASErrors(cublasSetMathMode(
          cublas_, cublasDisableTensorCores ? CUBLAS_PEDANTIC_MATH
                                            : CUBLAS_TENSOR_OP_MATH));
      ReportCUBLASErrors(cublasSetStream(cublas_, stream_));
    } else {
      multi_stream_ = false;
    }
  }
  ~InputsOutputs() {
    ReportCUDAErrors(cudaFreeHost(input_masks_mem_));
    ReportCUDAErrors(cudaFreeHost(input_val_mem_));
    ReportCUDAErrors(cudaFreeHost(op_policy_mem_));
    ReportCUDAErrors(cudaFree(op_policy_mem_gpu_));
    ReportCUDAErrors(cudaFreeHost(op_value_mem_));

    if (multi_stream_) {
      for (auto mem : tensor_mem_) {
        if (mem) ReportCUDAErrors(cudaFree(mem));
      }
      if (scratch_mem_) ReportCUDAErrors(cudaFree(scratch_mem_));

      cudaStreamDestroy(stream_);
      cublasDestroy(cublas_);
    }
  
  }
  uint64_t* input_masks_mem_;
  float* input_val_mem_;
  float* op_policy_mem_;
  float* op_value_mem_;
  float* op_moves_left_mem_;

  // GPU pointers for the above allocations.
  uint64_t* input_masks_mem_gpu_;
  float* input_val_mem_gpu_;
  float* op_value_mem_gpu_;
  float* op_moves_left_mem_gpu_;

  // This is a seperate copy.
  float* op_policy_mem_gpu_;

  // memory needed to run the network owned by InputsOutputs when multi_stream
  // is enabled
  bool multi_stream_;
  void* tensor_mem_[3];
  void* scratch_mem_;

  // cuda stream used to run the network
  cudaStream_t stream_;
  cublasHandle_t cublas_;

  // cublas handle used to run the network

};

}  // namespace cudnn_backend
}  // namespace lczero

```

`src/neural/cuda/kernels.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "cuda_common.h"

namespace lczero {
namespace cudnn_backend {

// Adds two vectors (possibly of different sizes), also do optional
// activation (relu, tanh or sigmoid).
template <typename T>
void addVectors(T* c, T* a, T* b, int size, int asize, int bsize,
                ActivationFunction activation, cudaStream_t stream);

// Optimized kernel to add bias to innermost dimension
// and perform optional activation (to be used with GEMMs/fully connected)
template <typename T>
void addBiasBatched(T* output, const T* input, const T* bias, int Batch, int N,
                    int C, ActivationFunction activation, cudaStream_t stream);

// Add bias to convolution's output.
template <typename T>
void addBias_NCHW(T* c, T* a, T* b, int N, int C, int H, int W,
                  ActivationFunction activation, cudaStream_t stream);

// Conversion from NCHW to NHWC, can also change datatype depending on template
// params, also pad/un-pad elements from Batch or Channel dimensions
template <typename DstType, typename SrcType>
void convertNCHWtoNHWC(DstType* output_tensor, const SrcType* input_tensor,
                       int Nin, int Cin, int Nout, int Cout, int H, int W);

// Plain data-type conversion (no layout conversion).
template <typename DstType, typename SrcType>
void copyTypeConverted(DstType* op, SrcType* ip, int N, cudaStream_t stream);

// Perform batch normilization.
template <typename T>
void batchNorm(T* output, const T* input, const T* skipInput, int N, int C,
               int H, int W, float* means, float* var_multipliers,
               ActivationFunction activation);

// Unpack planes (input to network).
void expandPlanes_Fp32_NCHW(float* output, const uint64_t* masks,
                            const float* values, int n, cudaStream_t stream);

void expandPlanes_Fp16_NHWC(half* output, const uint64_t* masks,
                            const float* values, int n, cudaStream_t stream);

void expandPlanes_Fp16_NCHW(half* output, const uint64_t* masks,
                            const float* values, int n, cudaStream_t stream);

// Perform global avg pool.
template <typename T>
void globalAvgPool(int N, int C, T* output, const T* input,
                   const T* prevLayerBias, bool nhwc);

// Perform global scale.
template <typename T>
void globalScale(int N, int C, T* output, const T* input, const T* scaleBias,
                 const T* prevLayerBias, bool nhwc,
                 ActivationFunction activation);

// Perform Squeeze-and-Excitation (SE) in a single fused kernel.
// Returns false if the fused kernel can't handle the sizes.
bool Se_Fp16_NHWC(int N, int C, int numFc1Out, half* output, const half* skip,
                  const half* input, const half* w1, const half* b1,
                  const half* w2, const half* b2, const half* bPrev,
                  ActivationFunction activation);

template <typename T>
void PolicyMap(int N, T* output, const T* input, const short* indices,
               int inputSize, int usedSize, int outputSize,
               cudaStream_t stream);

// Custom winograd helper functions
template <typename T>
void FilterTransform(int N, int C, T* transformedFilter, const T* filter);

template <typename T, bool nhcw>
void InputTransform(int N, int C, T* transformedInput, const T* input,
                    cudaStream_t stream);

template <typename T, bool use_se, ActivationFunction activation, bool use_bias,
          bool use_skip, bool skipInput_nhcw, bool output_nhcw>
void OutputTransform(int N, int C, int se_K, T* output, const T* input,
                     const T* skip, const T* bias, const T* w1, const T* b1,
                     const T* w2, const T* b2, cudaStream_t stream);

template <typename T, bool use_se, ActivationFunction activation, bool use_bias,
          bool use_skip>
void OutputInputTransform(int N, int C, int se_K, T* output, const T* input,
                          const T* skip, const T* bias, const T* w1,
                          const T* b1, const T* w2, const T* b2,
                          cudaStream_t stream);

template <typename T>
void Softmax(int N, int C, T* output, const T* input, cudaStream_t stream);

template <typename T>
void LayerNorm(int N, int C, T* output, const T* input, const T* bias,
               const T* skip, const T* gammas, const T* betas, float ep,
               cudaStream_t stream);

template <typename T>
void ComputePromotionLogits(int N, int C, T* output, const T* keys,
                            const T* ppo, const T* policy_attn_logits,
                            cudaStream_t stream);

}  // namespace cudnn_backend
}  // namespace lczero

```

`src/neural/cuda/layers.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#include "layers.h"

#include <cassert>
#include <cstring>
#include <vector>

#include "cuda_common.h"
#include "kernels.h"
#include "utils/fp16_utils.h"

namespace lczero {
// void dumpTensor(void* memory, int elements, const char* message, bool fp16 =
// false);

namespace cudnn_backend {

// Use Single kernel for entire SE operation.
// Right now supported only for fp16 with nhwc and it's quite a bit faster
// than using multiple passes. The flag can be set to false for debugging.
static constexpr bool kUseFusedSELayer = true;

template <typename DataType>
BaseLayer<DataType>::BaseLayer(int c, int h, int w, BaseLayer* ip, bool nhwc)
    : input_(ip), C(c), H(h), W(w), nhwc_(nhwc), use_gemm_ex_(false) {}

template <typename DataType>
BaseLayer<DataType>::BaseLayer(int c, int h, int w, BaseLayer* ip, bool nhwc,
                               bool gemm_ex)
    : input_(ip), C(c), H(h), W(w), nhwc_(nhwc), use_gemm_ex_(gemm_ex) {}

template <typename DataType>
BaseLayer<DataType>::BaseLayer(int c, int h, int w, BaseLayer* ip)
    : input_(ip), C(c), H(h), W(w), nhwc_(ip->nhwc_), use_gemm_ex_(false) {}

#ifdef USE_CUDNN
template <typename DataType>
void ConvLayer<DataType>::init() {
  // Allocate memory for weights (filter tensor) and biases.
  const size_t weight_size =
      sizeof(DataType) * c_input_ * C * filter_size_ * filter_size_;
  ReportCUDAErrors(cudaMalloc(&weights, weight_size));

  const size_t bias_size = sizeof(DataType) * C;
  ReportCUDAErrors(cudaMalloc(&biases, bias_size));

  const bool fp16 = std::is_same<half, DataType>::value;
  const cudnnDataType_t dataType =
      std::is_same<half, DataType>::value ? CUDNN_DATA_HALF : CUDNN_DATA_FLOAT;

  const cudnnTensorFormat_t layout =
      nhwc_ ? CUDNN_TENSOR_NHWC : CUDNN_TENSOR_NCHW;

  // Create cudnn objects for various tensors, algorithms, etc.
  cudnnCreateFilterDescriptor(&filter_desc_);
  cudnnCreateConvolutionDescriptor(&conv_desc_);
  cudnnCreateTensorDescriptor(&out_tensor_desc_);
  cudnnCreateTensorDescriptor(&in_tensor_desc_);
  cudnnCreateTensorDescriptor(&bias_desc_);
  cudnnCreateActivationDescriptor(&activation_);

  cudnnSetFilter4dDescriptor(filter_desc_, dataType, layout, GetC(), c_input_,
                             filter_size_, filter_size_);

  ReportCUDNNErrors(
      cudnnSetTensor4dDescriptor(bias_desc_, layout, dataType, 1, C, 1, 1));

  const int padding = filter_size_ / 2;
  const bool crossCorr = 1;

  ReportCUDNNErrors(cudnnSetConvolution2dDescriptor(
      conv_desc_, padding, padding, 1, 1, 1, 1,
      crossCorr ? CUDNN_CROSS_CORRELATION : CUDNN_CONVOLUTION, dataType));

  if (fp16 && nhwc_)
    ReportCUDNNErrors(
        cudnnSetConvolutionMathType(conv_desc_, CUDNN_TENSOR_OP_MATH));

  // TODO: dynamic selection of algorithm!
  if ((C > 32) && (!nhwc_) && (filter_size_ > 1)) {
    conv_algo_ = CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED;
  } else {
    conv_algo_ = CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM;
  }

  if (act_ == RELU) {
    cudnnSetActivationDescriptor(activation_, CUDNN_ACTIVATION_RELU,
                                 CUDNN_NOT_PROPAGATE_NAN, 0.0);
  }
#if CUDNN_MAJOR != 7 || CUDNN_MINOR != 0
  else {
    cudnnSetActivationDescriptor(activation_, CUDNN_ACTIVATION_IDENTITY,
                                 CUDNN_NOT_PROPAGATE_NAN, 0.0);
  }
#endif
}

template <typename DataType>
ConvLayer<DataType>::ConvLayer(BaseLayer<DataType>* ip, int C, int H, int W,
                               int filter, int Cin,
                               ActivationFunction activation, bool bias)
    : BaseLayer<DataType>(C, H, W, ip),
      c_input_(Cin),
      filter_size_(filter),
      act_(activation),
      use_bias_(bias) {
  init();
}

template <typename DataType>
ConvLayer<DataType>::ConvLayer(bool nhwc, int C, int H, int W, int filter,
                               int Cin, ActivationFunction activation,
                               bool bias)
    : BaseLayer<DataType>(C, H, W, nullptr, nhwc),
      c_input_(Cin),
      filter_size_(filter),
      act_(activation),
      use_bias_(bias) {
  init();
}

template <>
void ConvLayer<half>::LoadWeights(float* pfilter, float* pBias, void* scratch) {
  const size_t weight_size =
      sizeof(float) * c_input_ * C * filter_size_ * filter_size_;
  const size_t bias_size = sizeof(float) * C;
  // Also need to convert from fp32 NCHW to fp16 NHWC
  // first copy from CPU memory to scratch space in GPU memory
  // and then do the type / layout conversion using a kernel.
  assert(scratch);
  ReportCUDAErrors(
      cudaMemcpy(scratch, pfilter, weight_size, cudaMemcpyHostToDevice));

  if (nhwc_) {
    convertNCHWtoNHWC((half*)weights, (float*)scratch, C, c_input_, C, c_input_,
                      filter_size_, filter_size_);
  } else {
    copyTypeConverted((half*)weights, (float*)scratch,
                      C * c_input_ * filter_size_ * filter_size_, 0);
  }

  if (pBias) {
    ReportCUDAErrors(
        cudaMemcpy(scratch, pBias, bias_size, cudaMemcpyHostToDevice));

    copyTypeConverted((half*)biases, (float*)scratch, C, 0);
  }
}

template <>
void ConvLayer<float>::LoadWeights(float* pfilter, float* pBias,
                                   void* /*scratch*/) {
  const size_t weight_size =
      sizeof(float) * c_input_ * C * filter_size_ * filter_size_;
  const size_t bias_size = sizeof(float) * C;
  ReportCUDAErrors(
      cudaMemcpy(weights, pfilter, weight_size, cudaMemcpyHostToDevice));

  if (pBias) {
    ReportCUDAErrors(
        cudaMemcpy(biases, pBias, bias_size, cudaMemcpyHostToDevice));
  } else {
    ReportCUDAErrors(cudaMemset(biases, 0, bias_size));
  }
}

template <typename DataType>
void ConvLayer<DataType>::Eval(int N, DataType* output, const DataType* input,
                               const DataType* input2, void* scratch,
                               size_t scratch_size, cudnnHandle_t cudnn,
                               cublasHandle_t /*cublas*/, cudaStream_t stream) {
  const cudnnDataType_t dataType =
      std::is_same<half, DataType>::value ? CUDNN_DATA_HALF : CUDNN_DATA_FLOAT;

  const cudnnTensorFormat_t layout =
      nhwc_ ? CUDNN_TENSOR_NHWC : CUDNN_TENSOR_NCHW;

  ReportCUDNNErrors(cudnnSetTensor4dDescriptor(out_tensor_desc_, layout,
                                               dataType, N, C, H, W));

  ReportCUDNNErrors(cudnnSetTensor4dDescriptor(in_tensor_desc_, layout,
                                               dataType, N, c_input_, H, W));

  float alpha = 1.0f, beta = 0.0f;

  if (!(act_ != NONE || use_bias_ || input2)) {
    ReportCUDNNErrors(cudnnConvolutionForward(
        cudnn, &alpha, in_tensor_desc_, input, filter_desc_, weights,
        conv_desc_, conv_algo_, scratch, scratch_size, &beta, out_tensor_desc_,
        output));
  }
#if CUDNN_MAJOR != 7 || CUDNN_MINOR != 0
  else if (input2 && (act_ == RELU || act_ == NONE) && use_bias_) {
    // fused bias + sum + relu!
    ReportCUDNNErrors(cudnnConvolutionBiasActivationForward(
        cudnn, &alpha, in_tensor_desc_, input, filter_desc_, weights,
        conv_desc_, conv_algo_, scratch, scratch_size, &alpha, out_tensor_desc_,
        input2, bias_desc_, biases, activation_, out_tensor_desc_, output));
  } else {
    // For some reason cudnn doesn't support just Convolution + Bias with nchw
    // (winograd algorithm) it works fine when RELU is also needed which is
    // somewhat strange.
    if ((act_ == RELU || (act_ == NONE && nhwc_)) && !input2 && use_bias_) {
      ReportCUDNNErrors(cudnnConvolutionBiasActivationForward(
          cudnn, &alpha, in_tensor_desc_, input, filter_desc_, weights,
          conv_desc_, conv_algo_, scratch, scratch_size, &beta,
          out_tensor_desc_, output, bias_desc_, biases, activation_,
          out_tensor_desc_, output));
    } else {
      // The no special case path...
      ReportCUDNNErrors(cudnnConvolutionForward(
          cudnn, &alpha, in_tensor_desc_, input, filter_desc_, weights,
          conv_desc_, conv_algo_, scratch, scratch_size, &beta,
          out_tensor_desc_, output));
      bool act_done = false;
      if (input2 && input2 != output) {
        // Merge act with residual add unless there is bias.
        addVectors(output, output, (DataType*)input2, N * C * H * W,
                   N * C * H * W, N * C * H * W, use_bias_ ? NONE : act_,
                   stream);
        act_done = !use_bias_;
      }
      // Merge act with bias.
      if (use_bias_) {
        if (!nhwc_) {
          // add bias
          addBias_NCHW(output, output, biases, N, C, H, W, act_, stream);
        } else {
          addVectors(output, output, biases, N * C * H * W, N * C * H * W, C,
                     act_, stream);
        }
      } else if (!act_done && act_ != NONE) {
        addVectors(output, output, (DataType*)nullptr, N * C * H * W,
                   N * C * H * W, 0, act_, stream);
      }
    }
  }
#else
  else {
    ReportCUDNNErrors(cudnnConvolutionForward(
        cudnn, &alpha, in_tensor_desc_, input, filter_desc_, weights,
        conv_desc_, conv_algo_, scratch, scratch_size,
        (input2 == output) ? &alpha : &beta, out_tensor_desc_, output));
    if (input2 && input2 != output) {
      ReportCUDNNErrors(cudnnAddTensor(cudnn, &alpha, out_tensor_desc_, input2,
                                       &alpha, out_tensor_desc_, output));
    }
    if (use_bias_) {
      ReportCUDNNErrors(cudnnAddTensor(cudnn, &alpha, bias_desc_, biases,
                                       &alpha, out_tensor_desc_, output));
    }
    if (act_ == RELU) {
      ReportCUDNNErrors(cudnnActivationForward(cudnn, activation_, &alpha,
                                               out_tensor_desc_, output, &beta,
                                               out_tensor_desc_, output));
    }
    if (act_ != RELU && act_ != NONE) {
      addVectors(output, output, nullptr, N * C * H * W, N * C * H * W, 0, act_,
                 stream);
      // TODO: check this actually compiles?
    }
  }
#endif
}

template <typename DataType>
ConvLayer<DataType>::~ConvLayer() {
  ReportCUDAErrors(cudaFree(weights));
  ReportCUDAErrors(cudaFree(biases));

  cudnnDestroyFilterDescriptor(filter_desc_);
  cudnnDestroyConvolutionDescriptor(conv_desc_);
  cudnnDestroyTensorDescriptor(bias_desc_);
  cudnnDestroyTensorDescriptor(in_tensor_desc_);
  cudnnDestroyTensorDescriptor(out_tensor_desc_);
  cudnnDestroyActivationDescriptor(activation_);
}
#endif

template <typename DataType>
SELayer<DataType>::SELayer(BaseLayer<DataType>* ip, int fc1Outputs,
                           bool addPrevLayerBias, ActivationFunction activation)
    : BaseLayer<DataType>(ip->GetC(), ip->GetH(), ip->GetW(), ip),
      numFc1Out_(fc1Outputs),
      addPrevLayerBias_(addPrevLayerBias),
      act_(activation) {
  ReportCUDAErrors(cudaMalloc(&w1_, C * numFc1Out_ * sizeof(DataType)));
  ReportCUDAErrors(cudaMalloc(&w2_, 2 * C * numFc1Out_ * sizeof(DataType)));

  if (kUseFusedSELayer && nhwc_) {
    ReportCUDAErrors(cudaMalloc(&w1_t_, C * numFc1Out_ * sizeof(DataType)));
    ReportCUDAErrors(cudaMalloc(&w2_t_, 2 * C * numFc1Out_ * sizeof(DataType)));
  }

  ReportCUDAErrors(cudaMalloc(&b1_, numFc1Out_ * sizeof(DataType)));
  ReportCUDAErrors(cudaMalloc(&b2_, 2 * C * sizeof(DataType)));

  ReportCUDAErrors(cudaMalloc(&bPrev_, C * sizeof(DataType)));
}

template <typename DataType>
SELayer<DataType>::~SELayer() {
  ReportCUDAErrors(cudaFree(w1_));
  ReportCUDAErrors(cudaFree(w2_));
  ReportCUDAErrors(cudaFree(b1_));
  ReportCUDAErrors(cudaFree(b2_));
  ReportCUDAErrors(cudaFree(bPrev_));
}

template <>
void SELayer<float>::LoadWeights(float* w1, float* b1, float* w2, float* b2,
                                 float* prevLayerBias, void* /*scratch*/) {
  const size_t num_weights1 = C * numFc1Out_;
  const size_t weight_size1 = sizeof(float) * num_weights1;

  const size_t weight_size2 = 2 * weight_size1;

  // Weight for the first FC layer.
  ReportCUDAErrors(cudaMemcpy(w1_, w1, weight_size1, cudaMemcpyHostToDevice));

  // Weight for the second FC layer.
  ReportCUDAErrors(cudaMemcpy(w2_, w2, weight_size2, cudaMemcpyHostToDevice));

  // Bias for the first FC layer.
  ReportCUDAErrors(
      cudaMemcpy(b1_, b1, numFc1Out_ * sizeof(float), cudaMemcpyHostToDevice));

  // Bias for the second FC layer.
  ReportCUDAErrors(
      cudaMemcpy(b2_, b2, 2 * C * sizeof(float), cudaMemcpyHostToDevice));

  // Bias for previous layer (Convolution).
  if (prevLayerBias) {
    ReportCUDAErrors(cudaMemcpy(bPrev_, prevLayerBias, C * sizeof(float),
                                cudaMemcpyHostToDevice));
  }
}

void cpuTranspose(float* op, float* ip, int rows, int cols) {
  for (int i = 0; i < rows; i++)
    for (int j = 0; j < cols; j++) op[j * rows + i] = ip[i * cols + j];
}

template <>
void SELayer<half>::LoadWeights(float* w1, float* b1, float* w2, float* b2,
                                float* prevLayerBias, void* scratch) {
  const size_t num_weights1 = C * numFc1Out_;
  size_t weight_size1 = sizeof(float) * num_weights1;

  const size_t num_weights2 = 2 * num_weights1;
  size_t weight_size2 = 2 * weight_size1;

  // Transpose the weight matrices for the fused path.
  std::vector<float> temp(weight_size2);

  // Weight for the first FC layer.
  ReportCUDAErrors(
      cudaMemcpy(scratch, w1, weight_size1, cudaMemcpyHostToDevice));
  copyTypeConverted((half*)w1_, (float*)scratch, (int)num_weights1, 0);
  if (kUseFusedSELayer && nhwc_) {
    // transposed copy for fused SE kernel
    cpuTranspose(temp.data(), w1, numFc1Out_, C);
    ReportCUDAErrors(
        cudaMemcpy(scratch, temp.data(), weight_size1, cudaMemcpyHostToDevice));
    copyTypeConverted((half*)w1_t_, (float*)scratch, (int)num_weights1, 0);
  }

  // Weight for the second FC layer.
  ReportCUDAErrors(
      cudaMemcpy(scratch, w2, weight_size2, cudaMemcpyHostToDevice));
  copyTypeConverted((half*)w2_, (float*)scratch, (int)num_weights2, 0);
  if (kUseFusedSELayer && nhwc_) {
    cpuTranspose(temp.data(), w2, 2 * C, numFc1Out_);
    ReportCUDAErrors(
        cudaMemcpy(scratch, temp.data(), weight_size2, cudaMemcpyHostToDevice));
    copyTypeConverted((half*)w2_t_, (float*)scratch, (int)num_weights2, 0);
  }

  // Bias for the first FC layer.
  ReportCUDAErrors(cudaMemcpy(scratch, b1, numFc1Out_ * sizeof(float),
                              cudaMemcpyHostToDevice));
  copyTypeConverted((half*)b1_, (float*)scratch, numFc1Out_, 0);

  // Bias for the second FC layer.
  ReportCUDAErrors(
      cudaMemcpy(scratch, b2, 2 * C * sizeof(float), cudaMemcpyHostToDevice));
  copyTypeConverted((half*)b2_, (float*)scratch, 2 * C, 0);

  // Bias for previous layer (Convolution).
  if (prevLayerBias) {
    ReportCUDAErrors(cudaMemcpy(scratch, prevLayerBias, C * sizeof(float),
                                cudaMemcpyHostToDevice));
    copyTypeConverted((half*)bPrev_, (float*)scratch, C, 0);
  }
}

template <>
void SELayer<float>::Eval(int N, float* output, const float* input,
                          const float* /*input2*/, void* scratch,
                          size_t scratch_size, cudnnHandle_t /*cudnn*/,
                          cublasHandle_t cublas, cudaStream_t stream) {
  // Ping-pong between 'op1' and 'op2' (parts of scratch memory).
  float* op1 = (float*)scratch;
  float* op2 = (float*)scratch + scratch_size / sizeof(float) / 2;

  // 1. Global avg pooling (also adds previous layer bias before computing
  // averages).
  globalAvgPool(N, C, op2, input, bPrev_, false);

  // 2. First fully connected layer.
  float alpha = 1.0f, beta = 0.0f;
  ReportCUBLASErrors(cublasSgemm(cublas, CUBLAS_OP_T, CUBLAS_OP_N, numFc1Out_,
                                 N, C, &alpha, w1_, C, op2, C, &beta, op1,
                                 numFc1Out_));
  addVectors(op1, b1_, op1, numFc1Out_ * N, numFc1Out_, numFc1Out_ * N, act_,
             stream);

  // 3. Second fully connected layer.
  ReportCUBLASErrors(cublasSgemm(cublas, CUBLAS_OP_T, CUBLAS_OP_N, 2 * C, N,
                                 numFc1Out_, &alpha, w2_, numFc1Out_, op1,
                                 numFc1Out_, &beta, op2, 2 * C));
  addVectors(op2, b2_, op2, 2 * C * N, 2 * C, 2 * C * N, NONE, stream);

  // 4. (Optional prev layer bias add), Global scale, residual add, relu and
  // bias.
  globalScale(N, C, output, input, op2, bPrev_, false, act_);
}

template <>
void SELayer<half>::Eval(int N, half* output, const half* input,
                         const half* input2, void* scratch, size_t scratch_size,
                         cudnnHandle_t /*cudnn*/, cublasHandle_t cublas,
                         cudaStream_t stream) {
  bool se_done = false;
  if (kUseFusedSELayer && nhwc_) {
    se_done = Se_Fp16_NHWC(N, C, numFc1Out_, output, input2, input, w1_t_, b1_,
                           w2_t_, b2_, bPrev_, act_);
  }
  if (!se_done) {
    assert(output == input2);
    // Ping-pong between 'op1' and 'op2' (parts of scratch memory).
    half* op1 = (half*)scratch;
    half* op2 = (half*)scratch + scratch_size / sizeof(half) / 2;

    // 1. Global avg pooling (also adds previous layer bias before computing
    // averages).
    globalAvgPool(N, C, op2, input, bPrev_, nhwc_);

    // 2. First fully connected layer.
    __half_raw one_h{0x3C00};
    __half_raw zero_h{0};
    half alpha = one_h;
    half beta = zero_h;
    ReportCUBLASErrors(cublasHgemm(cublas, CUBLAS_OP_T, CUBLAS_OP_N, numFc1Out_,
                                   N, C, &alpha, w1_, C, op2, C, &beta, op1,
                                   numFc1Out_));
    addVectors(op1, b1_, op1, numFc1Out_ * N, numFc1Out_, numFc1Out_ * N, act_,
               stream);

    // 3. Second fully connected layer.
    ReportCUBLASErrors(cublasHgemm(cublas, CUBLAS_OP_T, CUBLAS_OP_N, 2 * C, N,
                                   numFc1Out_, &alpha, w2_, numFc1Out_, op1,
                                   numFc1Out_, &beta, op2, 2 * C));
    addVectors(op2, b2_, op2, 2 * C * N, 2 * C, 2 * C * N, NONE, stream);

    // 4. (Optional prev layer bias add), Global scale, residual add, relu and
    // bias.
    globalScale(N, C, output, input, op2, bPrev_, nhwc_, act_);
  }
}

template <typename DataType>
FCLayer<DataType>::FCLayer(BaseLayer<DataType>* ip, int C, int H, int W,
                           bool bias, ActivationFunction activation)
    : BaseLayer<DataType>(C, H, W, ip), use_bias_(bias), act_(activation) {
  const size_t weight_size =
      sizeof(DataType) * C * H * W * ip->GetC() * ip->GetH() * ip->GetW();
  const size_t bias_size = sizeof(DataType) * C * H * W;
  ReportCUDAErrors(cudaMalloc(&weights_, weight_size));
  if (use_bias_) {
    ReportCUDAErrors(cudaMalloc(&biases_, bias_size));
  } else {
    biases_ = nullptr;
  }
}

template <>
void FCLayer<half>::LoadWeights(float* cpuWeight, float* cpuBias,
                                void* scratch) {
  const size_t num_weights =
      C * H * W * input_->GetC() * input_->GetH() * input_->GetW();
  const size_t weight_size = sizeof(float) * num_weights;
  const size_t num_biases = C * H * W;
  const size_t bias_size = sizeof(float) * num_biases;

  // also need to convert from fp32 to fp16
  assert(scratch);
  ReportCUDAErrors(
      cudaMemcpy(scratch, cpuWeight, weight_size, cudaMemcpyHostToDevice));

  if (nhwc_) {
    convertNCHWtoNHWC((half*)weights_, (float*)scratch, (int)num_biases,
                      input_->GetC(), (int)num_biases, input_->GetC(),
                      input_->GetH(), input_->GetW());
  } else {
    copyTypeConverted((half*)weights_, (float*)scratch, (int)num_weights, 0);
  }

  if (cpuBias) {
    ReportCUDAErrors(
        cudaMemcpy(scratch, cpuBias, bias_size, cudaMemcpyHostToDevice));
    copyTypeConverted((half*)biases_, (float*)scratch, (int)num_biases, 0);
  }
}

template <>
void FCLayer<float>::LoadWeights(float* cpuWeight, float* cpuBias,
                                 void* /*scratch*/) {
  const size_t num_weights =
      C * H * W * input_->GetC() * input_->GetH() * input_->GetW();
  const size_t weight_size = sizeof(float) * num_weights;
  const size_t num_biases = C * H * W;
  const size_t bias_size = sizeof(float) * num_biases;

  ReportCUDAErrors(
      cudaMemcpy(weights_, cpuWeight, weight_size, cudaMemcpyHostToDevice));
  if (use_bias_) {
    ReportCUDAErrors(
        cudaMemcpy(biases_, cpuBias, bias_size, cudaMemcpyHostToDevice));
  }
}

template <>
void FCLayer<half>::Eval(int N, half* output_tensor, const half* input_tensor,
                         const half* /*input2*/, void* /*scratch*/,
                         size_t /*scratch_size*/, cudnnHandle_t /*cudnn*/,
                         cublasHandle_t cublas, cudaStream_t stream) {
  const int num_outputs = C * H * W;
  const int num_inputs = input_->GetC() * input_->GetH() * input_->GetW();

  // half alpha = float2half_rn(1.0f), beta = float2half_rn(0.0f);
  const __half_raw one_h{0x3C00};
  const __half_raw zero_h{0};
  half alpha = one_h;
  half beta = zero_h;
  ReportCUBLASErrors(cublasHgemm(cublas, CUBLAS_OP_T, CUBLAS_OP_N, num_outputs,
                                 N, num_inputs, &alpha, weights_, num_inputs,
                                 input_tensor, num_inputs, &beta, output_tensor,
                                 num_outputs));

  if (use_bias_ || (act_ != NONE)) {
    addVectors(output_tensor, biases_, output_tensor, num_outputs * N,
               num_outputs, num_outputs * N, act_, stream);
  }
}

template <>
void FCLayer<float>::Eval(int N, float* output_tensor,
                          const float* input_tensor, const float* /*input2*/,
                          void* /*scratch*/, size_t /*scratch_size*/,
                          cudnnHandle_t /*cudnn*/, cublasHandle_t cublas,
                          cudaStream_t stream) {
  const int num_outputs = C * H * W;
  const int num_inputs = input_->GetC() * input_->GetH() * input_->GetW();

  float alpha = 1.0f, beta = 0.0f;
  ReportCUBLASErrors(cublasSgemm(cublas, CUBLAS_OP_T, CUBLAS_OP_N, num_outputs,
                                 N, num_inputs, &alpha, weights_, num_inputs,
                                 input_tensor, num_inputs, &beta, output_tensor,
                                 num_outputs));

  if (use_bias_ || (act_ != NONE)) {
    addVectors(output_tensor, biases_, output_tensor, num_outputs * N,
               num_outputs, num_outputs * N, act_, stream);
  }
}

template <typename DataType>
FCLayer<DataType>::~FCLayer() {
  ReportCUDAErrors(cudaFree(weights_));
  ReportCUDAErrors(cudaFree(biases_));
}

template <typename DataType>
PolicyMapLayer<DataType>::PolicyMapLayer(BaseLayer<DataType>* ip, int C, int H,
                                         int W, int usedSize, bool attention)
    : BaseLayer<DataType>(C, H, W, ip),
      used_size_(usedSize),
      attention_map_(attention) {
  size_t weight_size = sizeof(short) * this->input_->GetC() * 64;
  if (attention) weight_size = sizeof(short) * usedSize;
  ReportCUDAErrors(cudaMalloc(&weights_, weight_size));
}

template <typename DataType>
void PolicyMapLayer<DataType>::LoadWeights(const short* cpuWeight,
                                           void* /*scratch*/) {
  size_t weight_size = sizeof(short) * used_size_;

  if (nhwc_ && !attention_map_) {
    // convert CHW to HWC
    int C = used_size_ / 64;
    int Cin = this->input_->GetC();

    // C is the no. of channels actually used (typically 73).
    // Cin the the no. of channels in previous layer (padded up to 80).
    // Weights of this layer is a mapping to select which output index of the
    // policy vector (1858 elements) maps to every element of input
    // tensor (assuming NCHW layout). Note that there are 73x64 valid inputs
    // (80x64 taking padding), and only 1858 outputs so the mapping isn't
    // one to one. Only few of the indices point to valid index in policy
    // vector. Invalid entries are set to -1.

    // In fp16 mode, the tensor layout is NHWC so the weights need to be
    // adjusted to make them work as intended.

    // This is how the original weights looks like (CHW layout):
    /*
               HW (64)
       ----|-------------|
           |             |
           |             |
    C (73) |             |
           |             |
           |             |
       ------------------|   Cin (80)
           |  padding    |
           |-------------|
    */
    // The padding is not part of the weights provided (used_size_ is 73 x 64).
    //
    // The weights converted to HWC looks like this
    /*
                 C (73)
            |-------------|---|
            |             | P |
            |             | a |
    HW (64) |             | d |
            |             |   |
            |             |   |
            |-----------------|
                     Cin (80)
    */
    // In HWC, because the padding is now part of each row
    // we need to increase the size of weights to account
    // for it.
    // The pad elements point to -1 (invalid output index) and the
    // same kernel works for both HWC and CHW layouts after used_size_
    // is updated to include padding (80x64).

    used_size_ = Cin * 64;
    std::vector<short> convertedWeights(used_size_);

    for (int hw = 0; hw < 64; hw++)
      for (int c = 0; c < Cin; c++) {
        if (c < C)
          convertedWeights[hw * Cin + c] = cpuWeight[c * 64 + hw];
        else
          convertedWeights[hw * Cin + c] = -1;
      }
    ReportCUDAErrors(cudaMemcpy(weights_, convertedWeights.data(),
                                used_size_ * sizeof(short),
                                cudaMemcpyHostToDevice));
  } else {
    ReportCUDAErrors(
        cudaMemcpy(weights_, cpuWeight, weight_size, cudaMemcpyHostToDevice));
  }
}

template <typename DataType>
void PolicyMapLayer<DataType>::Eval(
    int N, DataType* output_tensor, const DataType* input_tensor,
    const DataType* /*input2*/, void* /*scratch*/, size_t /*scratch_size*/,
    cudnnHandle_t /*cudnn*/, cublasHandle_t /*cublas*/, cudaStream_t stream) {
  int inputSize =
      this->input_->GetC() * this->input_->GetH() * this->input_->GetW();
  if (attention_map_) inputSize = used_size_;
  int outputSize = this->C * this->H * this->W;
  PolicyMap(N, output_tensor, input_tensor, weights_, inputSize, used_size_,
            outputSize, stream);
}

template <typename DataType>
PolicyMapLayer<DataType>::~PolicyMapLayer() {
  ReportCUDAErrors(cudaFree(weights_));
}

template <typename DataType>
FusedWinogradConvSELayer<DataType>::FusedWinogradConvSELayer(
    BaseLayer<DataType>* ip, int C, int H, int W, int Cin,
    ActivationFunction activation, bool bias, bool skip_add, bool se, int se_k,
    bool use_gemm_ex, bool op_nhcw)
    : BaseLayer<DataType>(C, H, W, ip, false, use_gemm_ex),
      c_input_(Cin),
      act_(activation),
      use_bias_(bias),
      skip_add_(skip_add),
      has_se_(se),
      se_k_(se_k),
      op_nhcw_(op_nhcw) {
  if (act_ != RELU && act_ != MISH && act_ != NONE) {
    throw Exception("Unsupported activation for fused winograd conv SE layer.");
  }
  // Allocate memory for weights (filter tensor) and biases.
  const size_t weight_size = sizeof(DataType) * c_input_ * C * 3 * 3;

  if (use_bias_) {
    const size_t bias_size = sizeof(DataType) * C;
    ReportCUDAErrors(cudaMalloc(&biases_, bias_size));
  }

  // 6x6 transformed filter size, for 3x3 convolution
  ReportCUDAErrors(cudaMalloc(&transformed_weights_, weight_size * 4));

  if (has_se_) {
    const size_t num_weights1 = C * se_k_;
    const size_t num_weights2 = num_weights1 * 2;
    const size_t num_biases1 = se_k_;
    const size_t num_biases2 = 2 * C;

    const size_t weight_size1 = sizeof(DataType) * num_weights1;
    const size_t weight_size2 = sizeof(DataType) * num_weights2;
    const size_t biases_size1 = sizeof(DataType) * num_biases1;
    const size_t biases_size2 = sizeof(DataType) * num_biases2;

    ReportCUDAErrors(cudaMalloc(&w1_, weight_size1));
    ReportCUDAErrors(cudaMalloc(&w2_, weight_size2));
    ReportCUDAErrors(cudaMalloc(&b1_, biases_size1));
    ReportCUDAErrors(cudaMalloc(&b2_, biases_size2));
  }
}

template <typename DataType>
void FusedWinogradConvSELayer<DataType>::LoadWeights(float* pfilter,
                                                     float* pBias,
                                                     void* scratch) {
  const size_t weight_size = sizeof(float) * c_input_ * C * 3 * 3;
  const size_t bias_size = sizeof(float) * C;

  // Store untransformed weights in scratch.
  const DataType* weights = (DataType*)scratch + weight_size + bias_size;

  // first copy from CPU memory to scratch space in GPU memory
  // and then do the type conversion using a kernel
  assert(scratch);
  ReportCUDAErrors(
      cudaMemcpy(scratch, pfilter, weight_size, cudaMemcpyHostToDevice));
  copyTypeConverted((DataType*)weights, (float*)scratch, C * c_input_ * 3 * 3,
                    0);

  if (pBias) {
    ReportCUDAErrors(
        cudaMemcpy(scratch, pBias, bias_size, cudaMemcpyHostToDevice));
    copyTypeConverted((DataType*)biases_, (float*)scratch, C, 0);
  }

  // run winograd transform kernel for the filter
  FilterTransform(C, c_input_, transformed_weights_, weights);
}

// TODO: Do this on the GPU to improve network load time!
static inline void CpuTranspose(float* op, float* ip, size_t rows,
                                size_t cols) {
  for (size_t i = 0; i < rows; i++)
    for (size_t j = 0; j < cols; j++) op[j * rows + i] = ip[i * cols + j];
}

template <typename DataType>
void FusedWinogradConvSELayer<DataType>::LoadSEWeights(float* w1, float* b1,
                                                       float* w2, float* b2,
                                                       void* scratch) {
  const size_t num_weights1 = C * se_k_;
  const size_t num_weights2 = num_weights1 * 2;
  const size_t num_biases1 = se_k_;
  const size_t num_biases2 = 2 * C;

  // The shader uses transposed weight matrices.
  std::vector<float> temp_transposed(num_weights2);

  CpuTranspose(temp_transposed.data(), w1, se_k_, C);
  ReportCUDAErrors(cudaMemcpy(scratch, temp_transposed.data(),
                              num_weights1 * sizeof(float),
                              cudaMemcpyHostToDevice));
  copyTypeConverted((DataType*)w1_, (float*)scratch, (int)num_weights1, 0);

  CpuTranspose(temp_transposed.data(), w2, 2 * C, se_k_);
  ReportCUDAErrors(cudaMemcpy(scratch, temp_transposed.data(),
                              num_weights2 * sizeof(float),
                              cudaMemcpyHostToDevice));
  copyTypeConverted((DataType*)w2_, (float*)scratch, (int)num_weights2, 0);

  ReportCUDAErrors(cudaMemcpy(scratch, b1, num_biases1 * sizeof(float),
                              cudaMemcpyHostToDevice));
  copyTypeConverted((DataType*)b1_, (float*)scratch, (int)num_biases1, 0);

  ReportCUDAErrors(cudaMemcpy(scratch, b2, num_biases2 * sizeof(float),
                              cudaMemcpyHostToDevice));
  copyTypeConverted((DataType*)b2_, (float*)scratch, (int)num_biases2, 0);
}

template <>
void BaseLayer<half>::cublasRowMajorMatrixMul(const half* A, const half* B,
                                              half* Out, int M, int N, int K,
                                              int batchSize,
                                              cublasHandle_t cublas) {
  // Need to initialize 1.0 and 0.0 as hexadecimal for fp16 because typecasting
  // float to half type doesn't work before CUDA 10.0
  __half_raw one_h{0x3C00};
  __half_raw zero_h{0};
  half halfOne = one_h;
  half halfZero = zero_h;

  // dimensions of matrix A = M x K
  // dimensions of matrix B = K x N
  // dimensions of output   = M x N

  // cublas supports only col major output
  // to multiply row major matrices, use the trick below
  ReportCUBLASErrors(cublasGemmStridedBatchedEx(
      cublas, CUBLAS_OP_N, CUBLAS_OP_N, N, M, K, &halfOne, B, CUDA_R_16F, N,
      N * K, A, CUDA_R_16F, K, K * M, &halfZero, Out, CUDA_R_16F, N, N * M,
      batchSize, CUDA_R_16F, CUBLAS_GEMM_DEFAULT));
}

template <>
void BaseLayer<float>::cublasRowMajorMatrixMul(const float* A, const float* B,
                                               float* Out, int M, int N, int K,
                                               int batchSize,
                                               cublasHandle_t cublas) {
  float floatOne = 1.0f;
  float floatZero = 0.0f;
  if (use_gemm_ex_)
    ReportCUBLASErrors(cublasGemmStridedBatchedEx(
        cublas, CUBLAS_OP_N, CUBLAS_OP_N, N, M, K, &floatOne, B, CUDA_R_32F, N,
        N * K, A, CUDA_R_32F, K, K * M, &floatZero, Out, CUDA_R_32F, N, N * M,
        batchSize, CUDA_R_32F, CUBLAS_GEMM_DEFAULT));
  else
    // Much slower on RTX 2060.. why? Maybe a cublas bug :-/
    ReportCUBLASErrors(cublasSgemmStridedBatched(
        cublas, CUBLAS_OP_N, CUBLAS_OP_N, N, M, K, &floatOne, B, N, N * K, A, K,
        K * M, &floatZero, Out, N, N * M, batchSize));
}

template <typename DataType>
void FusedWinogradConvSELayer<DataType>::Eval(
    int N, DataType* output, const DataType* input, const DataType* input2,
    void* scratch, size_t scratch_size, cudnnHandle_t /*cudnn*/,
    cublasHandle_t cublas, cudaStream_t stream) {
  // Split the scratch space into two parts - use first part for holding
  // transformed input and second part for transformed output.
  DataType* transformed_input = (DataType*)scratch;
  DataType* transformed_output =
      transformed_input + scratch_size / (2 * sizeof(DataType));

  InputTransform<DataType, false>(N, c_input_, transformed_input, input,
                                  stream);
  BaseLayer<DataType>::cublasRowMajorMatrixMul(
      transformed_input, transformed_weights_, transformed_output, N * 4, C,
      c_input_, 36, cublas);

  if (act_ == NONE) {
    if (!has_se_ && use_bias_ && !skip_add_)
      OutputTransform<DataType, false, NONE, true, false, false, false>(
          N, C, 0, output, transformed_output, nullptr, biases_, nullptr,
          nullptr, nullptr, nullptr, stream);
    else
      throw Exception("unsupported network type!");
  } else if (act_ == RELU) {
    if (has_se_ && use_bias_ && skip_add_)
      OutputTransform<DataType, true, RELU, true, true, false, false>(
          N, C, se_k_, output, transformed_output, input2, biases_, w1_, b1_,
          w2_, b2_, stream);
    else if (!has_se_ && use_bias_ && !skip_add_) {
      if (op_nhcw_)
        OutputTransform<DataType, false, RELU, true, false, false, true>(
            N, C, 0, output, transformed_output, nullptr, biases_, nullptr,
            nullptr, nullptr, nullptr, stream);
      else
        OutputTransform<DataType, false, RELU, true, false, false, false>(
            N, C, 0, output, transformed_output, nullptr, biases_, nullptr,
            nullptr, nullptr, nullptr, stream);
    } else if (!has_se_ && use_bias_ && skip_add_)
      OutputTransform<DataType, false, RELU, true, true, false, false>(
          N, C, 0, output, transformed_output, input2, biases_, nullptr,
          nullptr, nullptr, nullptr, stream);
    else
      throw Exception("unsupported network type!");
  } else if (act_ == MISH) {
    if (has_se_ && use_bias_ && skip_add_)
      OutputTransform<DataType, true, MISH, true, true, false, false>(
          N, C, se_k_, output, transformed_output, input2, biases_, w1_, b1_,
          w2_, b2_, stream);
    else if (!has_se_ && use_bias_ && !skip_add_) {
      if (op_nhcw_)
        OutputTransform<DataType, false, MISH, true, false, false, true>(
            N, C, 0, output, transformed_output, nullptr, biases_, nullptr,
            nullptr, nullptr, nullptr, stream);
      else
        OutputTransform<DataType, false, MISH, true, false, false, false>(
            N, C, 0, output, transformed_output, nullptr, biases_, nullptr,
            nullptr, nullptr, nullptr, stream);
    } else if (!has_se_ && use_bias_ && skip_add_)
      OutputTransform<DataType, false, MISH, true, true, false, false>(
          N, C, 0, output, transformed_output, input2, biases_, nullptr,
          nullptr, nullptr, nullptr, stream);
    else
      throw Exception("unsupported network type!");
  } else
    throw Exception("unsupported network type!");
}

template <typename DataType>
FusedWinogradConvSELayer<DataType>::~FusedWinogradConvSELayer() {
  ReportCUDAErrors(cudaFree(transformed_weights_));
  if (use_bias_) ReportCUDAErrors(cudaFree(biases_));
  if (has_se_) {
    ReportCUDAErrors(cudaFree(w1_));
    ReportCUDAErrors(cudaFree(w2_));
    ReportCUDAErrors(cudaFree(b1_));
    ReportCUDAErrors(cudaFree(b2_));
  }
}

template <typename DataType>
Conv1Layer<DataType>::Conv1Layer(BaseLayer<DataType>* ip, int C, int H, int W,
                                 int Cin, ActivationFunction activation,
                                 bool bias, bool use_gemm_ex)
    : BaseLayer<DataType>(C, H, W, ip, false, use_gemm_ex),
      c_input_(Cin),
      act_(activation),
      use_bias_(bias) {
  // Allocate memory for weights (filter tensor) and biases.
  const size_t weight_size = sizeof(DataType) * c_input_ * C * 1 * 1;
  ReportCUDAErrors(cudaMalloc(&weights_, weight_size));

  if (use_bias_) {
    const size_t bias_size = sizeof(DataType) * C;
    ReportCUDAErrors(cudaMalloc(&biases_, bias_size));
  }
}

template <typename DataType>
void Conv1Layer<DataType>::LoadWeights(float* pfilter, float* pBias,
                                       void* scratch) {
  const size_t weight_size = sizeof(float) * c_input_ * C * 1 * 1;
  const size_t bias_size = sizeof(float) * C;

  // first copy from CPU memory to scratch space in GPU memory
  // and then do the type conversion using a kernel
  assert(scratch);
  ReportCUDAErrors(
      cudaMemcpy(scratch, pfilter, weight_size, cudaMemcpyHostToDevice));
  copyTypeConverted((DataType*)weights_, (float*)scratch, C * c_input_ * 1 * 1,
                    0);

  if (pBias) {
    ReportCUDAErrors(
        cudaMemcpy(scratch, pBias, bias_size, cudaMemcpyHostToDevice));
    copyTypeConverted((DataType*)biases_, (float*)scratch, C, 0);
  }
}
template <>
void Conv1Layer<half>::cublasSpecialMatrixMul(const half* A, const half* B,
                                              half* Out, int M, int N, int K,
                                              int batchSize,
                                              cublasHandle_t cublas) {
  // Need to initialize 1.0 and 0.0 as hexadecimal for fp16 because typecasting
  // float to half type doesn't work before CUDA 10.0
  __half_raw one_h{0x3C00};
  __half_raw zero_h{0};
  half halfOne = one_h;
  half halfZero = zero_h;

  // dimensions of matrix A = M x K
  // dimensions of matrix B = K x N
  // dimensions of output   = M x N

  // cublas supports only col major output
  // to multiply row major matrices, use the trick below
  // NOTE strideB set to 0 below!
  ReportCUBLASErrors(cublasGemmStridedBatchedEx(
      cublas, CUBLAS_OP_N, CUBLAS_OP_N, N, M, K, &halfOne, B, CUDA_R_16F, N,
      N * K, A, CUDA_R_16F, K, 0, &halfZero, Out, CUDA_R_16F, N, N * M,
      batchSize, CUDA_R_16F, CUBLAS_GEMM_DEFAULT));
}

template <>
void Conv1Layer<float>::cublasSpecialMatrixMul(const float* A, const float* B,
                                               float* Out, int M, int N, int K,
                                               int batchSize,
                                               cublasHandle_t cublas) {
  float floatOne = 1.0f;
  float floatZero = 0.0f;

  // NOTE strideB set to 0 below!
  if (use_gemm_ex_)
    ReportCUBLASErrors(cublasGemmStridedBatchedEx(
        cublas, CUBLAS_OP_N, CUBLAS_OP_N, N, M, K, &floatOne, B, CUDA_R_32F, N,
        N * K, A, CUDA_R_32F, K, 0, &floatZero, Out, CUDA_R_32F, N, N * M,
        batchSize, CUDA_R_32F, CUBLAS_GEMM_DEFAULT));
  else
    // Much slower on RTX 2060.. why? Maybe a cublas bug :-/
    ReportCUBLASErrors(cublasSgemmStridedBatched(
        cublas, CUBLAS_OP_N, CUBLAS_OP_N, N, M, K, &floatOne, B, N, N * K, A, K,
        0, &floatZero, Out, N, N * M, batchSize));
}

template <typename DataType>
void Conv1Layer<DataType>::Eval(int N, DataType* output, const DataType* input,
                                const DataType* /*input2*/, void* /*scratch*/,
                                size_t /*scratch_size*/,
                                cudnnHandle_t /*cudnn*/, cublasHandle_t cublas,
                                cudaStream_t stream) {
  cublasSpecialMatrixMul(weights_, input, output, C, H * W, c_input_, N,
                         cublas);

  if (use_bias_)
    addBias_NCHW(output, output, biases_, N, C, H, W, act_, stream);
  else if (act_ != NONE)
    addVectors(output, output, (DataType*)nullptr, N * C * H * W, N * C * H * W,
               0, act_, stream);
}

template <typename DataType>
Conv1Layer<DataType>::~Conv1Layer() {
  ReportCUDAErrors(cudaFree(weights_));
  if (use_bias_) ReportCUDAErrors(cudaFree(biases_));
}

template <typename DataType>
ResidualBlock<DataType>::ResidualBlock(BaseLayer<DataType>* ip, int C, bool se,
                                       int se_k, bool use_gemm_ex, bool first,

                                       bool last, ActivationFunction activation,
                                       int shared_mem_size)
    : BaseLayer<DataType>(C, 8, 8, ip, ip->isNHWC(), use_gemm_ex),
      has_se_(se),
      se_k_(se_k),
      c_input_(C),
      first_block_(first),
      last_block_(last),
      shared_mem_size_(shared_mem_size),
      act_(activation) {
  if (act_ != RELU && act_ != MISH) {
    throw Exception("Unsupported activation for residual block.");
  }
  // Allocate memory for weights (filter tensor) and biases.
  const size_t weight_size = sizeof(DataType) * C * C * 3 * 3;

  const size_t bias_size = sizeof(DataType) * C;
  ReportCUDAErrors(cudaMalloc(&biases0_, bias_size));
  ReportCUDAErrors(cudaMalloc(&biases1_, bias_size));

  // 6x6 transformed filter size, for 3x3 convolution
  ReportCUDAErrors(cudaMalloc(&transformed_weights0_, weight_size * 4));
  ReportCUDAErrors(cudaMalloc(&transformed_weights1_, weight_size * 4));

  if (has_se_) {
    const size_t num_weights1 = C * se_k_;
    const size_t num_weights2 = num_weights1 * 2;
    const size_t num_biases1 = se_k_;
    const size_t num_biases2 = 2 * C;

    const size_t weight_size1 = sizeof(DataType) * num_weights1;
    const size_t weight_size2 = sizeof(DataType) * num_weights2;
    const size_t biases_size1 = sizeof(DataType) * num_biases1;
    const size_t biases_size2 = sizeof(DataType) * num_biases2;

    ReportCUDAErrors(cudaMalloc(&w1_, weight_size1));
    ReportCUDAErrors(cudaMalloc(&w2_, weight_size2));
    ReportCUDAErrors(cudaMalloc(&b1_, biases_size1));
    ReportCUDAErrors(cudaMalloc(&b2_, biases_size2));
  }
}

template <typename DataType>
void ResidualBlock<DataType>::LoadWeights0(float* pfilter, float* pBias,
                                           void* scratch) {
  const size_t weight_size = sizeof(float) * c_input_ * C * 3 * 3;
  const size_t bias_size = sizeof(float) * C;

  // Store untransformed weights in scratch.
  const DataType* weights = (DataType*)scratch + weight_size;

  // first copy from CPU memory to scratch space in GPU memory
  // and then do the type conversion using a kernel
  assert(scratch);
  ReportCUDAErrors(
      cudaMemcpy(scratch, pfilter, weight_size, cudaMemcpyHostToDevice));
  copyTypeConverted((DataType*)weights, (float*)scratch, C * c_input_ * 3 * 3,
                    0);

  if (pBias) {
    ReportCUDAErrors(
        cudaMemcpy(scratch, pBias, bias_size, cudaMemcpyHostToDevice));
    copyTypeConverted((DataType*)biases0_, (float*)scratch, C, 0);
  }

  // run winograd transform kernel for the filter
  FilterTransform(C, c_input_, transformed_weights0_, weights);
}

template <typename DataType>
void ResidualBlock<DataType>::LoadWeights1(float* pfilter, float* pBias,
                                           void* scratch) {
  const size_t weight_size = sizeof(float) * C * C * 3 * 3;
  const size_t bias_size = sizeof(float) * C;

  // Store untransformed weights in scratch.
  const DataType* weights = (DataType*)scratch + weight_size;

  // first copy from CPU memory to scratch space in GPU memory
  // and then do the type conversion using a kernel
  assert(scratch);
  ReportCUDAErrors(
      cudaMemcpy(scratch, pfilter, weight_size, cudaMemcpyHostToDevice));
  copyTypeConverted((DataType*)weights, (float*)scratch, C * C * 3 * 3, 0);

  if (pBias) {
    ReportCUDAErrors(
        cudaMemcpy(scratch, pBias, bias_size, cudaMemcpyHostToDevice));
    copyTypeConverted((DataType*)biases1_, (float*)scratch, C, 0);
  }

  // run winograd transform kernel for the filter
  FilterTransform(C, C, transformed_weights1_, weights);
}

template <typename DataType>
void ResidualBlock<DataType>::LoadSEWeights(float* w1, float* b1, float* w2,
                                            float* b2, void* scratch) {
  const size_t num_weights1 = C * se_k_;
  const size_t num_weights2 = num_weights1 * 2;
  const size_t num_biases1 = se_k_;
  const size_t num_biases2 = 2 * C;

  // The shader uses transposed weight matrices.
  std::vector<float> temp_transposed(num_weights2);

  CpuTranspose(temp_transposed.data(), w1, se_k_, C);
  ReportCUDAErrors(cudaMemcpy(scratch, temp_transposed.data(),
                              num_weights1 * sizeof(float),
                              cudaMemcpyHostToDevice));
  copyTypeConverted((DataType*)w1_, (float*)scratch, (int)num_weights1, 0);

  CpuTranspose(temp_transposed.data(), w2, 2 * C, se_k_);
  ReportCUDAErrors(cudaMemcpy(scratch, temp_transposed.data(),
                              num_weights2 * sizeof(float),
                              cudaMemcpyHostToDevice));
  copyTypeConverted((DataType*)w2_, (float*)scratch, (int)num_weights2, 0);

  ReportCUDAErrors(cudaMemcpy(scratch, b1, num_biases1 * sizeof(float),
                              cudaMemcpyHostToDevice));
  copyTypeConverted((DataType*)b1_, (float*)scratch, (int)num_biases1, 0);

  ReportCUDAErrors(cudaMemcpy(scratch, b2, num_biases2 * sizeof(float),
                              cudaMemcpyHostToDevice));
  copyTypeConverted((DataType*)b2_, (float*)scratch, (int)num_biases2, 0);
}

template <typename DataType>
void ResidualBlock<DataType>::Eval(int N, DataType* output,
                                   const DataType* input,
                                   const DataType* /*input2*/, void* scratch,
                                   size_t scratch_size, cudnnHandle_t /*cudnn*/,
                                   cublasHandle_t cublas, cudaStream_t stream) {
  // normally:
  // - "output" initially contains the transformed input,
  //    and after this layer, it contains the transformed input for next layer
  // - "input" contains the original/untransformed input
  // special cases:
  //   - for first_block_, input is real input (untransformed)
  //   - for last_block_, output is the final output of this block
  //   (untransformed)

  // Split the scratch space into two parts - use first part for holding
  // transformed input and second part for transformed output.
  DataType* transformed_input = (DataType*)scratch;
  DataType* transformed_output =
      transformed_input + scratch_size / (2 * sizeof(DataType));

  if (first_block_) {
    InputTransform<DataType, true>(N, c_input_, transformed_input, input,
                                   stream);
    BaseLayer<DataType>::cublasRowMajorMatrixMul(
        transformed_input, transformed_weights0_, transformed_output, N * 4, C,
        c_input_, 36, cublas);
  } else {
    BaseLayer<DataType>::cublasRowMajorMatrixMul(output, transformed_weights0_,
                                                 transformed_output, N * 4, C,
                                                 c_input_, 36, cublas);
  }

  if (act_ == RELU) {
    OutputInputTransform<DataType, false, RELU, true, false>(
        N, C, 0, transformed_input, transformed_output, nullptr, biases0_,
        nullptr, nullptr, nullptr, nullptr, stream);
  } else if (act_ == MISH) {
    OutputInputTransform<DataType, false, MISH, true, false>(
        N, C, 0, transformed_input, transformed_output, nullptr, biases0_,
        nullptr, nullptr, nullptr, nullptr, stream);
  }
  // "transformed_input" tensor now contains transformed input for the next
  // convolution

  BaseLayer<DataType>::cublasRowMajorMatrixMul(
      transformed_input, transformed_weights1_, transformed_output, N * 4, C, C,
      36, cublas);

  const bool fp16 = std::is_same<half, DataType>::value;
  bool allowFusing =
      (C <= kMaxResBlockFusingChannels) ||
      (fp16 && (shared_mem_size_ >= kMaxResBlockFusingSeFp16AmpereSmem) &&
       (C <= kMaxResBlockFusingSeKFp16Ampere));

  if (act_ == RELU) {
    if (last_block_) {
      if (has_se_)
        OutputTransform<DataType, true, RELU, true, true, true, false>(
            N, C, se_k_, output, transformed_output, input, biases1_, w1_, b1_,
            w2_, b2_, stream);
      else
        OutputTransform<DataType, false, RELU, true, true, true, false>(
            N, C, se_k_, output, transformed_output, input, biases1_, w1_, b1_,
            w2_, b2_, stream);
    } else {
      if (has_se_) {
        if (allowFusing) {
          OutputInputTransform<DataType, true, RELU, true, true>(
              N, C, se_k_, output, transformed_output, input, biases1_, w1_,
              b1_, w2_, b2_, stream);
        } else {
          OutputTransform<DataType, true, RELU, true, true, true, true>(
              N, C, se_k_, (DataType*)input, transformed_output, input,
              biases1_, w1_, b1_, w2_, b2_, stream);
          InputTransform<DataType, true>(N, C, output, (DataType*)input,
                                         stream);
        }
      } else
        OutputInputTransform<DataType, false, RELU, true, true>(
            N, C, se_k_, output, transformed_output, input, biases1_, w1_, b1_,
            w2_, b2_, stream);
    }
  } else if (act_ == MISH) {
    if (last_block_) {
      if (has_se_)
        OutputTransform<DataType, true, MISH, true, true, true, false>(
            N, C, se_k_, output, transformed_output, input, biases1_, w1_, b1_,
            w2_, b2_, stream);
      else
        OutputTransform<DataType, false, MISH, true, true, true, false>(
            N, C, se_k_, output, transformed_output, input, biases1_, w1_, b1_,
            w2_, b2_, stream);
    } else {
      if (has_se_) {
        if (allowFusing) {
          OutputInputTransform<DataType, true, MISH, true, true>(
              N, C, se_k_, output, transformed_output, input, biases1_, w1_,
              b1_, w2_, b2_, stream);
        } else {
          OutputTransform<DataType, true, MISH, true, true, true, true>(
              N, C, se_k_, (DataType*)input, transformed_output, input,
              biases1_, w1_, b1_, w2_, b2_, stream);
          InputTransform<DataType, true>(N, C, output, (DataType*)input,
                                         stream);
        }
      } else
        OutputInputTransform<DataType, false, MISH, true, true>(
            N, C, se_k_, output, transformed_output, input, biases1_, w1_, b1_,
            w2_, b2_, stream);
    }
  }
  // "output" tensor now contains transformed input for the next
  // convolution
}

template <typename DataType>
ResidualBlock<DataType>::~ResidualBlock() {
  ReportCUDAErrors(cudaFree(transformed_weights0_));
  ReportCUDAErrors(cudaFree(biases0_));
  ReportCUDAErrors(cudaFree(transformed_weights1_));
  ReportCUDAErrors(cudaFree(biases1_));
  if (has_se_) {
    ReportCUDAErrors(cudaFree(w1_));
    ReportCUDAErrors(cudaFree(w2_));
    ReportCUDAErrors(cudaFree(b1_));
    ReportCUDAErrors(cudaFree(b2_));
  }
}

template <typename DataType>
void allocAndUpload(DataType** gpu_dest, std::vector<float> cpu_src,
                    void* scratch) {
  size_t size = cpu_src.size() * sizeof(DataType);
  if (size == 0) {
    *gpu_dest = nullptr;
    return;
  }
  ReportCUDAErrors(cudaMalloc(gpu_dest, size));
  ReportCUDAErrors(cudaMemcpy(scratch, &cpu_src[0],
                              cpu_src.size() * sizeof(float),
                              cudaMemcpyHostToDevice));
  copyTypeConverted((DataType*)(*gpu_dest), (float*)scratch,
                    (int)cpu_src.size(), 0);
}

template <typename DataType>
AttentionPolicyHead<DataType>::AttentionPolicyHead(BaseLayer<DataType>* ip,
                                                   const LegacyWeights& weights,
                                                   void* scratch)
    : BaseLayer<DataType>(64 * 64 + 24 * 8, 1, 1, ip) {
  embedding_op_size_ = weights.ip_pol_b.size();
  wq_op_size_ = weights.ip2_pol_b.size();
  wk_op_size_ = weights.ip3_pol_b.size();

  encoder_heads_ = weights.pol_encoder_head_count;
  policy_d_model_ = wq_op_size_;

  allocAndUpload<DataType>(&ip_pol_w_, weights.ip_pol_w, scratch);
  allocAndUpload<DataType>(&ip_pol_b_, weights.ip_pol_b, scratch);

  allocAndUpload<DataType>(&ip2_pol_w_, weights.ip2_pol_w, scratch);
  allocAndUpload<DataType>(&ip2_pol_b_, weights.ip2_pol_b, scratch);

  allocAndUpload<DataType>(&ip3_pol_w_, weights.ip3_pol_w, scratch);
  allocAndUpload<DataType>(&ip3_pol_b_, weights.ip3_pol_b, scratch);

  // big allocation to hold wq and wk weights one after the other
  {
    size_t elements = weights.ip2_pol_w.size();
    assert(elements == weights.ip3_pol_w.size());

    size_t size = elements * sizeof(DataType) * 2;
    ReportCUDAErrors(cudaMalloc(&wqk_w_, size));
    ReportCUDAErrors(
        cudaMemcpy(wqk_w_, ip2_pol_w_, size / 2, cudaMemcpyDeviceToDevice));
    ReportCUDAErrors(cudaMemcpy(wqk_w_ + elements, ip3_pol_w_, size / 2,
                                cudaMemcpyDeviceToDevice));

    elements = weights.ip2_pol_b.size();
    size = elements * sizeof(DataType) * 2;
    ReportCUDAErrors(cudaMalloc(&wqk_b_, size));
    ReportCUDAErrors(
        cudaMemcpy(wqk_b_, ip2_pol_b_, size / 2, cudaMemcpyDeviceToDevice));
    ReportCUDAErrors(cudaMemcpy(wqk_b_ + elements, ip3_pol_b_, size / 2,
                                cudaMemcpyDeviceToDevice));
  }

  allocAndUpload<DataType>(&ip4_pol_w_, weights.ip4_pol_w, scratch);

  for (const auto& enc : weights.pol_encoder) {
    EncoderWeights* pW = new EncoderWeights(enc, scratch);
    encoder_weights_.emplace_back(pW);
  }
}

template <typename DataType>
AttentionPolicyHead<DataType>::EncoderWeights::EncoderWeights(
    const LegacyWeights::EncoderLayer& cpu_weights, void* scratch) {
  mha_q_size_ = cpu_weights.mha.q_b.size();
  mha_k_size_ = cpu_weights.mha.k_b.size();
  mha_v_size_ = cpu_weights.mha.v_b.size();
  mha_dense_size_ = cpu_weights.mha.dense_b.size();
  ffn_dense1_size_ = cpu_weights.ffn.dense1_b.size();
  ffn_dense2_size_ = cpu_weights.ffn.dense2_b.size();

  allocAndUpload<DataType>(&mha_q_w, cpu_weights.mha.q_w, scratch);
  allocAndUpload<DataType>(&mha_q_b, cpu_weights.mha.q_b, scratch);

  allocAndUpload<DataType>(&mha_k_w, cpu_weights.mha.k_w, scratch);
  allocAndUpload<DataType>(&mha_k_b, cpu_weights.mha.k_b, scratch);

  allocAndUpload<DataType>(&mha_v_w, cpu_weights.mha.v_w, scratch);
  allocAndUpload<DataType>(&mha_v_b, cpu_weights.mha.v_b, scratch);

  // big allocation to hold qkv weights one after the other
  {
    size_t elements = cpu_weights.mha.q_w.size();
    size_t size = elements * sizeof(DataType) * 3;
    ReportCUDAErrors(cudaMalloc(&mha_qkv_w, size));
    ReportCUDAErrors(
        cudaMemcpy(mha_qkv_w, mha_q_w, size / 3, cudaMemcpyDeviceToDevice));
    ReportCUDAErrors(cudaMemcpy(mha_qkv_w + elements, mha_k_w, size / 3,
                                cudaMemcpyDeviceToDevice));
    ReportCUDAErrors(cudaMemcpy(mha_qkv_w + elements * 2, mha_v_w, size / 3,
                                cudaMemcpyDeviceToDevice));

    elements = cpu_weights.mha.q_b.size();
    size = elements * sizeof(DataType) * 3;
    ReportCUDAErrors(cudaMalloc(&mha_qkv_b, size));
    ReportCUDAErrors(
        cudaMemcpy(mha_qkv_b, mha_q_b, size / 3, cudaMemcpyDeviceToDevice));
    ReportCUDAErrors(cudaMemcpy(mha_qkv_b + elements, mha_k_b, size / 3,
                                cudaMemcpyDeviceToDevice));
    ReportCUDAErrors(cudaMemcpy(mha_qkv_b + elements * 2, mha_v_b, size / 3,
                                cudaMemcpyDeviceToDevice));
  }

  allocAndUpload<DataType>(&mha_dense_w, cpu_weights.mha.dense_w, scratch);
  allocAndUpload<DataType>(&mha_dense_b, cpu_weights.mha.dense_b, scratch);

  allocAndUpload<DataType>(&ln1_gammas, cpu_weights.ln1_gammas, scratch);
  allocAndUpload<DataType>(&ln1_betas, cpu_weights.ln1_betas, scratch);

  allocAndUpload<DataType>(&ffn_dense1_w, cpu_weights.ffn.dense1_w, scratch);
  allocAndUpload<DataType>(&ffn_dense1_b, cpu_weights.ffn.dense1_b, scratch);

  allocAndUpload<DataType>(&ffn_dense2_w, cpu_weights.ffn.dense2_w, scratch);
  allocAndUpload<DataType>(&ffn_dense2_b, cpu_weights.ffn.dense2_b, scratch);

  allocAndUpload<DataType>(&ln2_gammas, cpu_weights.ln2_gammas, scratch);
  allocAndUpload<DataType>(&ln2_betas, cpu_weights.ln2_betas, scratch);
}

template <typename DataType>
static void cublasXgemm(cublasHandle_t handle, cublasOperation_t transa,
                        cublasOperation_t transb, int m, int n, int k,
                        float alpha, const DataType* A, int lda,
                        const DataType* B, int ldb, float beta, DataType* C,
                        int ldc) {
  const bool fp16 = std::is_same<half, DataType>::value;
  if (fp16) {
    unsigned short alpha_h = FP32toFP16(alpha);
    unsigned short beta_h = FP32toFP16(beta);
    ReportCUBLASErrors(cublasHgemm(
        handle, transa, transb, m, n, k, (const half*)&alpha_h, (const half*)A,
        lda, (const half*)B, ldb, (const half*)&beta_h, (half*)C, ldc));
  } else {
    ReportCUBLASErrors(cublasSgemm(handle, transa, transb, m, n, k, &alpha,
                                   (const float*)A, lda, (const float*)B, ldb,
                                   &beta, (float*)C, ldc));
  }
}

template <typename DataType>
static void cublasXGemmStridedBatched(
    cublasHandle_t handle, cublasOperation_t transa, cublasOperation_t transb,
    int m, int n, int k, float alpha, const void* A, int lda,
    long long int strideA, const void* B, int ldb, long long int strideB,
    float beta, void* C, int ldc, long long int strideC, int batchCount) {
  const bool fp16 = std::is_same<half, DataType>::value;
  if (fp16) {
    unsigned short alpha_h = FP32toFP16(alpha);
    unsigned short beta_h = FP32toFP16(beta);
    ReportCUBLASErrors(cublasGemmStridedBatchedEx(
        handle, transa, transb, m, n, k, &alpha_h, A, CUDA_R_16F, lda, strideA,
        B, CUDA_R_16F, ldb, strideB, &beta_h, C, CUDA_R_16F, ldc, strideC,
        batchCount, CUDA_R_16F, CUBLAS_GEMM_DEFAULT));
  } else {
    ReportCUBLASErrors(cublasGemmStridedBatchedEx(
        handle, transa, transb, m, n, k, &alpha, A, CUDA_R_32F, lda, strideA, B,
        CUDA_R_32F, ldb, strideB, &beta, C, CUDA_R_32F, ldc, strideC,
        batchCount, CUDA_R_32F, CUBLAS_GEMM_DEFAULT));
  }
}

template <typename DataType>
void AttentionPolicyHead<DataType>::Eval(
    int N, DataType* output, const DataType* input, const DataType* input2,
    void* scratch, size_t scratch_size, cudnnHandle_t /*cudnn*/,
    cublasHandle_t cublas, cudaStream_t stream) {
  DataType* scratch0 = (DataType*)scratch;
  DataType* scratch1 = (DataType*)input2;
  DataType* scratch2 = output + scratch_size / (2 * sizeof(DataType));
  DataType* scratch3 = scratch1 + scratch_size / (2 * sizeof(DataType));

  int inputC = this->input_->GetC();
  convertNCHWtoNHWC(scratch0, input, N, inputC, N, inputC, 8, 8);

  // 1. Policy embedding (fully connected layer)
  // Input data in NHWC layout N*(64)*C, output is N*(64)*embedding_op_size_
  DataType* pol_embedding = scratch1;
  {
    const int num_outputs = embedding_op_size_;
    const int num_inputs = inputC;
    const int batch = N * 64;
    cublasXgemm<DataType>(cublas, CUBLAS_OP_T, CUBLAS_OP_N, num_outputs, batch,
                          num_inputs, 1.0f, (const DataType*)ip_pol_w_,
                          num_inputs, scratch0, num_inputs, 0.0f, pol_embedding,
                          num_outputs);
    addBiasBatched(pol_embedding, pol_embedding, ip_pol_b_, 1, batch,
                   num_outputs, SELU, stream);
  }

  // 2. Encoder layers
  for (const auto pEnc : encoder_weights_) {
    const auto& enc = *pEnc;
    const int d_model = enc.mha_q_size_;
    const int depth = d_model / encoder_heads_;

    DataType* mha_q;
    DataType* mha_k;
    DataType* mha_v;

    {
      const int num_inputs = embedding_op_size_;
      const int num_outputs = d_model;
      const int batch = N * 64;

      mha_q = scratch0;
      mha_k = mha_q + num_outputs * batch;
      mha_v = mha_k + num_outputs * batch;

      cublasXGemmStridedBatched<DataType>(
          cublas, CUBLAS_OP_T, CUBLAS_OP_N, num_outputs, batch, num_inputs,
          1.0f, enc.mha_qkv_w, num_inputs, num_inputs * num_outputs,
          pol_embedding, num_inputs, 0, 0.0f, mha_q, num_outputs,
          num_outputs * batch, 3);
      addBiasBatched<DataType>(mha_q, mha_q, enc.mha_qkv_b, 3, batch,
                               num_outputs, NONE, stream);
    }

    // Apply split_heads() to q, k and v
    // which basically transposes (batch_size, 64, num_heads, depth)
    // to (batch_size, num_heads, 64, depth)
    // Do we really need to transpose here?
    // (Maybe not, we can play with strides of the gemm and do independent gemms
    // for each encoder head)

    // Apply scaled dot product attention:
    /*
        matmul_qk = tf.matmul(q, k, transpose_b=True)
        dk = tf.cast(tf.shape(k)[-1], self.model_dtype)
        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)
        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)
        output = tf.matmul(attention_weights, v)
    */

    // shape(k)[-1] = depth
    float factor = 1.0f / sqrt((float)depth);

    // matmul_qk = tf.matmul(q, k, transpose_b=True)
    for (int i = 0; i < encoder_heads_; i++) {
      int offset = i * depth;
      // layout of the output: encoder_heads_ * Batch * 64 * 64
      int outOffset = i * N * 64 * 64;
      cublasXGemmStridedBatched<DataType>(
          cublas, CUBLAS_OP_T, CUBLAS_OP_N, 64 /*M*/, 64 /*N*/,
          depth /*K*/,  // A/B, and M/N are swapped for row-major to col-major
                        // transform
          factor,       // to handle "/ tf.math.sqrt(dk)"
          mha_k + offset /*A*/,
          d_model /*LDA*/,  // (d_model = depth * encoder_heads_) to skip over
                            // other "depth" slices / heads
          64 * d_model,     /*strideA*/
          mha_q + offset /*B*/,
          d_model /*LDB*/,  // to skip over other other "depth" slices / heads
          64 * d_model,     /*strideB*/
          0.0f,
          scratch2 + outOffset /*C*/,  // output (matmul_qk) goes to scratch2
          64 /*LDC*/, 64 * 64 /*strideC*/, N);
    }

    // attention_weights = tf.nn.softmax(scaled_attention_logits, axis = -1)
    // attention_weights -> scratch2
    Softmax(encoder_heads_ * N * 64, 64, scratch2, scratch2, stream);

    // output = tf.matmul(attention_weights, v)
    for (int i = 0; i < encoder_heads_; i++) {
      int offset = i * depth;  // for output and "v" matrix
      // layout: encoder_heads_ * Batch*64*64
      int weightsOffset = i * N * 64 * 64;
      cublasXGemmStridedBatched<DataType>(
          cublas, CUBLAS_OP_N, CUBLAS_OP_N, depth /*M*/, 64 /*N*/, 64 /*K*/,
          1.0f, mha_v + offset /*A*/,  // "v" matrix
          d_model /*LDA*/,  // to skip over other "depth" slices / heads
          64 * d_model,     /*strideA*/
          scratch2 + weightsOffset /*B*/, 64 /*LDB*/, 64 * 64, /*strideB*/
          0.0f, scratch3 + offset /*C*/,  // output goes to scratch3
          d_model /*LDC*/, 64 * d_model /*strideC*/, N);
    }

    // #final dense layer (mha_dense), scratch3 -> scratch2
    {
      const int num_inputs = d_model;
      const int num_outputs = embedding_op_size_;
      const int batch = N * 64;
      cublasXgemm(cublas, CUBLAS_OP_T, CUBLAS_OP_N, num_outputs, batch,
                  num_inputs, 1.0f, (const DataType*)enc.mha_dense_w,
                  num_inputs, scratch3, num_inputs, 0.0f, scratch2,
                  num_outputs);
    }

    // LN1: skip connection and layer normalization (also bias add of prev gemm)
    // scratch2/scratch1 -> scratch0
    LayerNorm<DataType>(N * 64, embedding_op_size_, scratch0, scratch2,
                        enc.mha_dense_b, scratch1, enc.ln1_gammas,
                        enc.ln1_betas, 1e-6, stream);

    // #FFN dense 1, scratch0 -> scratch1
    const int encoder_dff = enc.ffn_dense1_size_;
    {
      const int num_inputs = embedding_op_size_;
      const int num_outputs = encoder_dff;
      const int batch = N * 64;
      cublasXgemm(cublas, CUBLAS_OP_T, CUBLAS_OP_N, num_outputs, batch,
                  num_inputs, 1.0f, (const DataType*)enc.ffn_dense1_w,
                  num_inputs, scratch0, num_inputs, 0.0f, scratch1,
                  num_outputs);
      addBiasBatched(scratch1, scratch1, enc.ffn_dense1_b, 1, batch,
                     num_outputs, SELU, stream);
    }

    // #FFN dense 2, scratch1 -> scratch2
    {
      const int num_inputs = encoder_dff;
      const int num_outputs = embedding_op_size_;
      const int batch = N * 64;
      cublasXgemm(cublas, CUBLAS_OP_T, CUBLAS_OP_N, num_outputs, batch,
                  num_inputs, 1.0f, (const DataType*)enc.ffn_dense2_w,
                  num_inputs, scratch1, num_inputs, 0.0f, scratch2,
                  num_outputs);
    }

    // LN2: skip connection and layer normilization (also bias add of prev gemm)
    // scratch2/scratch0 -> scratch1
    LayerNorm<DataType>(N * 64, embedding_op_size_, scratch1, scratch2,
                        enc.ffn_dense2_b, scratch0, enc.ln2_gammas,
                        enc.ln2_betas, 1e-6, stream);

  }  // End of encoder blocks

  DataType* wq;
  DataType* wk;
  {
    const int num_inputs = embedding_op_size_;
    const int num_outputs = policy_d_model_;
    const int batch = N * 64;
    wq = scratch0;
    wk = wq + num_outputs * batch;

    cublasXGemmStridedBatched<DataType>(
        cublas, CUBLAS_OP_T, CUBLAS_OP_N, num_outputs, batch, num_inputs, 1.0f,
        wqk_w_, num_inputs, num_inputs * num_outputs, scratch1, num_inputs, 0,
        0.0f, wq, num_outputs, num_outputs * batch, 2);

    addBiasBatched<DataType>(wq, wq, wqk_b_, 2, batch, num_outputs, NONE,
                             stream);
  }

  // dk = tf.math.sqrt(tf.cast(tf.shape(keys)[-1], self.model_dtype))
  // policy matmul_qk = tf.matmul(queries, keys, transpose_b=True)
  // policy_attn_logits = matmul_qk / dk
  {
    // shape(keys)[-1] = policy_d_model_
    float factor = 1.0f / sqrt((float)policy_d_model_);

    // A/B, and M/N are swapped for row-major to col-major transform
    // leave 8*24 after each batch to interleave promotion_logits (computed
    // later below)
    cublasXGemmStridedBatched<DataType>(
        cublas, CUBLAS_OP_T, CUBLAS_OP_N, 64 /*M*/, 64 /*N*/,
        policy_d_model_ /*K*/,
        factor,  // to handle "/ tf.math.sqrt(dk)"
        wk /*A*/, policy_d_model_ /*LDA*/, 64 * policy_d_model_, /*strideA*/
        wq /*B*/, policy_d_model_ /*LDB*/, 64 * policy_d_model_, /*strideB*/
        0.0f, output /*C*/,  // output (policy_attn_logits)
        64 /*LDC*/, 64 * 64 + 8 * 24 /*strideC*/, N);
  }

  // Compute promotion_logits in a single kernel (and put the result just after
  // policy_attn_logits interleaved to get concat for free)
  DataType* promotion_logits = output + 64 * 64;

  ComputePromotionLogits<DataType>(N, policy_d_model_, promotion_logits, wk,
                                   ip4_pol_w_, output, stream);
}

template <typename DataType>
AttentionPolicyHead<DataType>::~AttentionPolicyHead() {
  ReportCUDAErrors(cudaFree(ip_pol_w_));
  ReportCUDAErrors(cudaFree(ip_pol_b_));
  ReportCUDAErrors(cudaFree(ip2_pol_w_));
  ReportCUDAErrors(cudaFree(ip2_pol_b_));
  ReportCUDAErrors(cudaFree(ip3_pol_w_));
  ReportCUDAErrors(cudaFree(ip3_pol_b_));
  ReportCUDAErrors(cudaFree(ip4_pol_w_));
  ReportCUDAErrors(cudaFree(wqk_w_));
  ReportCUDAErrors(cudaFree(wqk_b_));
  for (const auto pEnc : encoder_weights_) delete pEnc;
}

template <typename DataType>
AttentionPolicyHead<DataType>::EncoderWeights::~EncoderWeights() {
  ReportCUDAErrors(cudaFree(mha_q_w));
  ReportCUDAErrors(cudaFree(mha_q_b));
  ReportCUDAErrors(cudaFree(mha_k_w));
  ReportCUDAErrors(cudaFree(mha_k_b));
  ReportCUDAErrors(cudaFree(mha_v_w));
  ReportCUDAErrors(cudaFree(mha_v_b));
  ReportCUDAErrors(cudaFree(mha_qkv_w));
  ReportCUDAErrors(cudaFree(mha_qkv_b));
  ReportCUDAErrors(cudaFree(mha_dense_w));
  ReportCUDAErrors(cudaFree(mha_dense_b));
  ReportCUDAErrors(cudaFree(ln1_gammas));
  ReportCUDAErrors(cudaFree(ln1_betas));
  ReportCUDAErrors(cudaFree(ffn_dense1_w));
  ReportCUDAErrors(cudaFree(ffn_dense1_b));
  ReportCUDAErrors(cudaFree(ffn_dense2_w));
  ReportCUDAErrors(cudaFree(ffn_dense2_b));
  ReportCUDAErrors(cudaFree(ln2_gammas));
  ReportCUDAErrors(cudaFree(ln2_betas));
}

// Template instantiation.
#ifdef USE_CUDNN
template class ConvLayer<half>;
template class ConvLayer<float>;
#endif

template class FCLayer<half>;
template class FCLayer<float>;

template class SELayer<half>;
template class SELayer<float>;

template class PolicyMapLayer<half>;
template class PolicyMapLayer<float>;

template class FusedWinogradConvSELayer<half>;
template class FusedWinogradConvSELayer<float>;

template class Conv1Layer<half>;
template class Conv1Layer<float>;

template class ResidualBlock<half>;
template class ResidualBlock<float>;

template class AttentionPolicyHead<half>;
template class AttentionPolicyHead<float>;

// Misc error handling stuff.
#ifdef USE_CUDNN
void CudnnError(cudnnStatus_t status, const char* file, const int& line) {
  if (status != CUDNN_STATUS_SUCCESS) {
    char message[128];
    sprintf(message, "CUDNN error: %s (%s:%d) ", cudnnGetErrorString(status),
            file, line);
    throw Exception(message);
  }
}
#endif

const char* CublasGetErrorString(cublasStatus_t status) {
  switch (status) {
    case CUBLAS_STATUS_SUCCESS:
      return "CUBLAS_STATUS_SUCCESS";
    case CUBLAS_STATUS_NOT_INITIALIZED:
      return "CUBLAS_STATUS_NOT_INITIALIZED";
    case CUBLAS_STATUS_ALLOC_FAILED:
      return "CUBLAS_STATUS_ALLOC_FAILED";
    case CUBLAS_STATUS_INVALID_VALUE:
      return "CUBLAS_STATUS_INVALID_VALUE";
    case CUBLAS_STATUS_ARCH_MISMATCH:
      return "CUBLAS_STATUS_ARCH_MISMATCH";
    case CUBLAS_STATUS_MAPPING_ERROR:
      return "CUBLAS_STATUS_MAPPING_ERROR";
    case CUBLAS_STATUS_EXECUTION_FAILED:
      return "CUBLAS_STATUS_EXECUTION_FAILED";
    case CUBLAS_STATUS_INTERNAL_ERROR:
      return "CUBLAS_STATUS_INTERNAL_ERROR";
    case CUBLAS_STATUS_NOT_SUPPORTED:
      return "CUBLAS_STATUS_NOT_SUPPORTED";
    case CUBLAS_STATUS_LICENSE_ERROR:
      return "CUBLAS_STATUS_LICENSE_ERROR";
  }
  return "unknown cublas error";
}

void CublasError(cublasStatus_t status, const char* file, const int& line) {
  if (status != CUBLAS_STATUS_SUCCESS) {
    char message[128];
    sprintf(message, "CUBLAS error: %s (%s:%d) ", CublasGetErrorString(status),
            file, line);
    throw Exception(message);
  }
}

void CudaError(cudaError_t status, const char* file, const int& line) {
  if (status != cudaSuccess) {
    char message[128];
    sprintf(message, "CUDA error: %s (%s:%d) ", cudaGetErrorString(status),
            file, line);
    throw Exception(message);
  }
}

}  // namespace cudnn_backend
}  // namespace lczero

```

`src/neural/cuda/layers.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#pragma once

#include <cublas_v2.h>

#include <cstddef>

#include "cuda_common.h"
#include "neural/network_legacy.h"

#ifdef USE_CUDNN
#include <cudnn.h>
#else
typedef void* cudnnHandle_t;
#endif

namespace lczero {
namespace cudnn_backend {

// The Layer objects only hold memory for weights, biases, etc
// memory for input and output tensors is provided by caller of Eval.

template <typename DataType>
class BaseLayer {
 public:
  int GetC() const { return C; }
  int GetH() const { return H; }
  int GetW() const { return W; }
  bool isNHWC() const { return nhwc_; }

  BaseLayer(int c, int h, int w, BaseLayer* ip);
  BaseLayer(int c, int h, int w, BaseLayer* ip, bool nhwc);
  BaseLayer(int c, int h, int w, BaseLayer* ip, bool nhwc, bool use_gemm_ex);
  virtual ~BaseLayer() = default;
  size_t GetOutputSize(int N) const { return sizeof(DataType) * N * C * H * W; }

  // Input2 is optional (skip connection).
  virtual void Eval(int N, DataType* output, const DataType* input,
                    const DataType* input2, void* scratch, size_t scratch_size,
                    cudnnHandle_t cudnn, cublasHandle_t cublas,
                    cudaStream_t stream) = 0;

 protected:
  BaseLayer* input_;

  int C;  // Output tensor dimensions.
  int H;
  int W;

  bool nhwc_;  // tensor layout
  const bool use_gemm_ex_;

  void cublasRowMajorMatrixMul(const DataType* A, const DataType* B,
                               DataType* Out, int M, int N, int K,
                               int batchSize, cublasHandle_t cublas);
};

#ifdef USE_CUDNN
template <typename DataType>
class ConvLayer : public BaseLayer<DataType> {
  using BaseLayer<DataType>::C;
  using BaseLayer<DataType>::H;
  using BaseLayer<DataType>::W;
  using BaseLayer<DataType>::GetC;
  using BaseLayer<DataType>::GetH;
  using BaseLayer<DataType>::GetW;
  using BaseLayer<DataType>::nhwc_;

 public:
  ConvLayer(BaseLayer<DataType>* ip, int C, int H, int W, int size, int Cin,
            ActivationFunction activation = NONE, bool bias = false);

  ConvLayer(bool nhwc, int C, int H, int W, int size, int Cin,
            ActivationFunction activation = NONE, bool bias = false);

  ~ConvLayer();
  void LoadWeights(float* pfilter, float* pBias, void* scratch);
  void Eval(int N, DataType* output, const DataType* input,
            const DataType* input2, void* scratch, size_t scratch_size,
            cudnnHandle_t cudnn, cublasHandle_t cublas,
            cudaStream_t stream) override;

 private:
  const int c_input_;
  const int filter_size_;
  const ActivationFunction act_;
  const bool use_bias_;

  DataType* biases = nullptr;
  DataType* weights = nullptr;

  cudnnFilterDescriptor_t filter_desc_;
  cudnnConvolutionDescriptor_t conv_desc_;
  cudnnConvolutionFwdAlgo_t conv_algo_;

  cudnnTensorDescriptor_t bias_desc_;
  cudnnTensorDescriptor_t in_tensor_desc_;
  cudnnTensorDescriptor_t out_tensor_desc_;
  cudnnActivationDescriptor_t activation_;

  void init();
};

#endif

template <typename DataType>
class FCLayer : public BaseLayer<DataType> {
  using BaseLayer<DataType>::nhwc_;

 public:
  FCLayer(BaseLayer<DataType>* ip, int C, int H, int W, bool bias,
          ActivationFunction activation);
  ~FCLayer();

  void LoadWeights(float* cpuWeight, float* cpuBias, void* scratch);
  void Eval(int N, DataType* output, const DataType* input,
            const DataType* input2, void* scratch, size_t scratch_size,
            cudnnHandle_t cudnn, cublasHandle_t cublas,
            cudaStream_t stream) override;

 private:
  const bool use_bias_;
  const ActivationFunction act_;
  DataType* weights_ = nullptr;
  DataType* biases_ = nullptr;
};

template <typename DataType>
class PolicyMapLayer : public BaseLayer<DataType> {
  using BaseLayer<DataType>::nhwc_;

 public:
  PolicyMapLayer(BaseLayer<DataType>* ip, int C, int H, int W, int usedSize,
                 bool attention);
  ~PolicyMapLayer();

  void LoadWeights(const short* cpuWeight, void* scratch);
  void Eval(int N, DataType* output, const DataType* input,
            const DataType* input2, void* scratch, size_t scratch_size,
            cudnnHandle_t cudnn, cublasHandle_t cublas,
            cudaStream_t stream) override;

 private:
  int used_size_;  // Size of the input without padding (typically 73x64).
                   // This is over-written to contain size with padding
                   // (typically 80x64) after CHW->HWC conversion for fp16.
  const bool attention_map_;
  short* weights_ = nullptr;
};

// Fused SE layer:
// (optional bias add +) global avg -> FC1 -> FC2 -> global scale -> add skip
// connection -> RELU.
template <typename DataType>
class SELayer : public BaseLayer<DataType> {
  using BaseLayer<DataType>::C;
  using BaseLayer<DataType>::nhwc_;

 public:
  SELayer(BaseLayer<DataType>* ip, int numFc1Out, bool addPrevLayerBias,
          ActivationFunction activation);
  ~SELayer();

  void LoadWeights(float* w1, float* b1, float* w2, float* b2,
                   float* prevLayerBias, void* scratch);

  void Eval(int N, DataType* output, const DataType* input,
            const DataType* input2, void* scratch, size_t scratch_size,
            cudnnHandle_t cudnn, cublasHandle_t cublas,
            cudaStream_t stream) override;

 private:
  DataType* w1_ = nullptr;
  DataType* w1_t_ = nullptr;  // transposed copy used by fused SE kernel
  DataType* b1_ = nullptr;
  DataType* w2_ = nullptr;
  DataType* w2_t_ = nullptr;
  DataType* b2_ = nullptr;
  DataType* bPrev_ = nullptr;
  int numFc1Out_;
  bool addPrevLayerBias_;
  const ActivationFunction act_;
};

// Multi-pass Winograd Conv fused with (optional) SE
template <typename DataType>
class FusedWinogradConvSELayer : public BaseLayer<DataType> {
  using BaseLayer<DataType>::C;
  using BaseLayer<DataType>::H;
  using BaseLayer<DataType>::W;
  using BaseLayer<DataType>::GetC;
  using BaseLayer<DataType>::GetH;
  using BaseLayer<DataType>::GetW;
  using BaseLayer<DataType>::nhwc_;

 public:
  FusedWinogradConvSELayer(BaseLayer<DataType>* ip, int C, int H, int W,
                           int Cin, ActivationFunction activation, bool bias,
                           bool skipAdd, bool se, int se_k, bool use_gemm_ex,
                           bool op_nhcw = false);

  ~FusedWinogradConvSELayer();
  void LoadWeights(float* pfilter, float* pBias, void* scratch);
  void LoadSEWeights(float* w1, float* b1, float* w2, float* b2, void* scratch);
  void Eval(int N, DataType* output, const DataType* input,
            const DataType* input2, void* scratch, size_t scratch_size,
            cudnnHandle_t cudnn, cublasHandle_t cublas,
            cudaStream_t stream) override;

 private:
  const int c_input_;
  const ActivationFunction act_;
  const bool use_bias_;
  const bool skip_add_;
  const bool has_se_;
  const int se_k_;
  const bool op_nhcw_;

  DataType* biases_ = nullptr;
  DataType* transformed_weights_ = nullptr;  // After winograd transform.

  // Weights and Biases for (optional) SE.
  DataType* w1_;
  DataType* w2_;
  DataType* b1_;
  DataType* b2_;
};

template <typename DataType>
class Conv1Layer : public BaseLayer<DataType> {
  using BaseLayer<DataType>::C;
  using BaseLayer<DataType>::H;
  using BaseLayer<DataType>::W;
  using BaseLayer<DataType>::GetC;
  using BaseLayer<DataType>::GetH;
  using BaseLayer<DataType>::GetW;
  using BaseLayer<DataType>::nhwc_;

 public:
  Conv1Layer(BaseLayer<DataType>* ip, int C, int H, int W, int Cin,
             ActivationFunction activation, bool bias, bool use_gemm_ex);

  ~Conv1Layer();
  void LoadWeights(float* pfilter, float* pBias, void* scratch);
  void Eval(int N, DataType* output, const DataType* input,
            const DataType* input2, void* scratch, size_t scratch_size,
            cudnnHandle_t cudnn, cublasHandle_t cublas,
            cudaStream_t stream) override;

 private:
  const int c_input_;
  const ActivationFunction act_;
  const bool use_bias_;

  DataType* biases_ = nullptr;
  DataType* weights_ = nullptr;

  // uses stride of 0 to read a vector as a matrix
  void cublasSpecialMatrixMul(const DataType* A, const DataType* B,
                              DataType* Out, int M, int N, int K, int batchSize,
                              cublasHandle_t cublas);
};

// Multi-pass Winograd Conv fused with (optional) SE
template <typename DataType>
class ResidualBlock : public BaseLayer<DataType> {
  using BaseLayer<DataType>::C;
  using BaseLayer<DataType>::H;
  using BaseLayer<DataType>::W;
  using BaseLayer<DataType>::GetC;
  using BaseLayer<DataType>::GetH;
  using BaseLayer<DataType>::GetW;

 public:
  ResidualBlock(BaseLayer<DataType>* ip, int C, bool se, int se_k,
                bool use_gemm_ex, bool first, bool last,
                ActivationFunction activation, int shared_mem_size);

  ~ResidualBlock();
  void LoadWeights0(float* pfilter, float* pBias, void* scratch);
  void LoadWeights1(float* pfilter, float* pBias, void* scratch);
  void LoadSEWeights(float* w1, float* b1, float* w2, float* b2, void* scratch);

  void Eval(int N, DataType* output, const DataType* input,
            const DataType* input2, void* scratch, size_t scratch_size,
            cudnnHandle_t cudnn, cublasHandle_t cublas,
            cudaStream_t stream) override;

 private:
  const bool has_se_;
  const int se_k_;
  const int c_input_;
  const bool first_block_;
  const bool last_block_;
  const int shared_mem_size_;
  const ActivationFunction act_;

  DataType* biases0_ = nullptr;
  DataType* biases1_ = nullptr;
  DataType* transformed_weights0_ = nullptr;  // After winograd transform.
  DataType* transformed_weights1_ = nullptr;  // After winograd transform.

  // Weights and Biases for (optional) SE.
  DataType* w1_;
  DataType* w2_;
  DataType* b1_;
  DataType* b2_;
};

// The Attention policy head implementation
// Responsible for loading weights into GPU memory, and evaluating the entire
// policy head
template <typename DataType>
class AttentionPolicyHead : public BaseLayer<DataType> {
  using BaseLayer<DataType>::C;
  using BaseLayer<DataType>::H;
  using BaseLayer<DataType>::W;
  using BaseLayer<DataType>::GetC;
  using BaseLayer<DataType>::GetH;
  using BaseLayer<DataType>::GetW;

 public:
  AttentionPolicyHead(BaseLayer<DataType>* ip, const LegacyWeights& weights,
                      void* scratch);
  ~AttentionPolicyHead();
  void Eval(int N, DataType* output, const DataType* input,
            const DataType* input2, void* scratch, size_t scratch_size,
            cudnnHandle_t cudnn, cublasHandle_t cublas,
            cudaStream_t stream) override;

 private:
  struct EncoderWeights {
    EncoderWeights(const LegacyWeights::EncoderLayer& cpu_weights,
                   void* scratch);
    ~EncoderWeights();
    // all GPU side pointers
    DataType *mha_q_w, *mha_q_b;
    DataType *mha_k_w, *mha_k_b;
    DataType *mha_v_w, *mha_v_b;
    DataType *mha_qkv_w, *mha_qkv_b;
    DataType *mha_dense_w, *mha_dense_b;

    DataType *ln1_gammas, *ln1_betas;

    DataType *ffn_dense1_w, *ffn_dense1_b;
    DataType *ffn_dense2_w, *ffn_dense2_b;

    DataType *ln2_gammas, *ln2_betas;

    int mha_q_size_;
    int mha_k_size_;
    int mha_v_size_;
    int mha_dense_size_;

    int ffn_dense1_size_;
    int ffn_dense2_size_;
  };

  // GPU allocations to hold various weights used by the attention policy head
  DataType *ip_pol_w_, *ip_pol_b_;    // "embedding" in policy attention
  DataType *ip2_pol_w_, *ip2_pol_b_;  // "wq" in policy attention
  DataType *ip3_pol_w_, *ip3_pol_b_;  // "wk" in policy attention
  DataType* ip4_pol_w_;               // "ppo" in policy attention

  DataType *wqk_w_, *wqk_b_;          // allocation containing both "wq" and "wq"

  int embedding_op_size_;
  int wq_op_size_;
  int wk_op_size_;

  int encoder_heads_;
  int policy_d_model_;

  std::vector<EncoderWeights*> encoder_weights_;
};

}  // namespace cudnn_backend
}  // namespace lczero

```

`src/neural/cuda/network_cuda.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2022 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#include <algorithm>
#include <cassert>
#include <functional>
#include <list>
#include <memory>
#include <mutex>

#include "cuda_common.h"
#include "inputs_outputs.h"
#include "kernels.h"
#include "layers.h"
#include "neural/factory.h"
#include "neural/network_legacy.h"
#include "neural/shared/attention_policy_map.h"
#include "neural/shared/policy_map.h"
#include "utils/bititer.h"
#include "utils/exception.h"

namespace lczero {
using namespace cudnn_backend;

template <typename DataType>
class CudaNetwork;

static size_t getMaxAttentionHeadSize(const LegacyWeights& weights, int N) {
  const size_t embedding_op_size = weights.ip_pol_b.size();
  const size_t policy_d_model = weights.ip2_pol_b.size();
  assert(policy_d_model == weights.ip3_pol_b.size());

  size_t encoder_d_model = 0;
  size_t encoder_dff = 0;

  if (weights.pol_encoder.size() > 0) {
    encoder_d_model = weights.pol_encoder[0].mha.q_b.size();
    encoder_dff = weights.pol_encoder[0].ffn.dense1_b.size();

    assert(encoder_d_model == weights.pol_encoder[0].mha.k_b.size());
    assert(encoder_d_model == weights.pol_encoder[0].mha.v_b.size());
    assert(embedding_op_size == weights.pol_encoder[0].ffn.dense2_b.size());
  }

  const size_t encoder_heads = weights.pol_encoder_head_count;

  size_t size = N * 64 * std::max(std::max(embedding_op_size, encoder_dff),
                                  policy_d_model);

  // size of matmul_qk matrix = encoder_heads_ * Batch * 64 * 64
  const size_t matmul_qk_size = encoder_heads * N * 64 * 64;
  const size_t output_size = N * (64 * 64 + 8 * 24);
  size = std::max(size, std::max(matmul_qk_size, output_size));

  size_t qkv_size = N * 64 * encoder_d_model;
  // We store qkv in single allocation, and other intermediate tensors are
  // sometimes stored by splitting an allocation into two halves.
  size = std::max(2 * size, 3 * qkv_size);
  return size;
}

template <typename DataType>
class CudaNetworkComputation : public NetworkComputation {
 public:
  CudaNetworkComputation(CudaNetwork<DataType>* network, bool wdl,
                         bool moves_left);
  ~CudaNetworkComputation();

  void AddInput(InputPlanes&& input) override {
    const auto iter_mask =
        &inputs_outputs_->input_masks_mem_[batch_size_ * kInputPlanes];
    const auto iter_val =
        &inputs_outputs_->input_val_mem_[batch_size_ * kInputPlanes];

    int i = 0;
    for (const auto& plane : input) {
      iter_mask[i] = plane.mask;
      iter_val[i] = plane.value;
      i++;
    }

    batch_size_++;
  }

  void ComputeBlocking() override;

  int GetBatchSize() const override { return batch_size_; }

  float GetQVal(int sample) const override {
    if (wdl_) {
      auto w = inputs_outputs_->op_value_mem_[3 * sample + 0];
      auto l = inputs_outputs_->op_value_mem_[3 * sample + 2];
      return w - l;
    } else {
      return inputs_outputs_->op_value_mem_[sample];
    }
  }

  float GetDVal(int sample) const override {
    if (wdl_) {
      auto d = inputs_outputs_->op_value_mem_[3 * sample + 1];
      return d;
    } else {
      return 0.0f;
    }
  }

  float GetPVal(int sample, int move_id) const override {
    return inputs_outputs_->op_policy_mem_[sample * kNumOutputPolicy + move_id];
  }

  float GetMVal(int sample) const override {
    if (moves_left_) {
      return inputs_outputs_->op_moves_left_mem_[sample];
    }
    return 0.0f;
  }

 private:
  // Memory holding inputs, outputs.
  std::unique_ptr<InputsOutputs> inputs_outputs_;
  int batch_size_;
  bool wdl_;
  bool moves_left_;

  CudaNetwork<DataType>* network_;
};

template <typename DataType>
class CudaNetwork : public Network {
 public:
  CudaNetwork(const WeightsFile& file, const OptionsDict& options)
      : capabilities_{file.format().network_format().input(),
                      file.format().network_format().moves_left()} {
    LegacyWeights weights(file.weights());
    gpu_id_ = options.GetOrDefault<int>("gpu", 0);

    conv_policy_ = file.format().network_format().policy() ==
                   pblczero::NetworkFormat::POLICY_CONVOLUTION;

    attn_policy_ = file.format().network_format().policy() ==
                   pblczero::NetworkFormat::POLICY_ATTENTION;

    max_batch_size_ = options.GetOrDefault<int>("max_batch", 1024);

    showInfo();

    int total_gpus;
    ReportCUDAErrors(cudaGetDeviceCount(&total_gpus));

    if (gpu_id_ >= total_gpus)
      throw Exception("Invalid GPU Id: " + std::to_string(gpu_id_));

    cudaDeviceProp deviceProp = {};
    cudaGetDeviceProperties(&deviceProp, gpu_id_);
    showDeviceInfo(deviceProp);

    // Select GPU to run on (for *the current* thread).
    ReportCUDAErrors(cudaSetDevice(gpu_id_));

    multi_stream_ = options.GetOrDefault<bool>("multi_stream", false);

    // layout used by cuda backend is nchw.
    has_tensor_cores_ = false;
    constexpr bool fp16 = std::is_same<half, DataType>::value;

    if (fp16) {
      // Check if the GPU support FP16.

      if ((deviceProp.major == 6 && deviceProp.minor != 1) ||
          (deviceProp.major == 5 && deviceProp.minor == 3)) {
        // FP16 without tensor cores supported on GP100 (SM 6.0) and Jetson
        // (SM 5.3 and 6.2). SM 6.1 GPUs also have FP16, but slower than FP32.
        ;
      } else if (deviceProp.major >= 7) {
        // Some GPUs (GTX 16xx) are SM 7.5 but don't have tensor cores
        // enabling TENSOR_OP_MATH for them works but is very very slow
        // (likely because the system emulates it).
        if (!strstr(deviceProp.name, "GTX 16")) {
          has_tensor_cores_ = true;
        }
      } else {
        throw Exception("Your GPU doesn't support FP16");
      }
    }

    if (!multi_stream_) {
      ReportCUBLASErrors(cublasCreate(&cublas_));
      if (has_tensor_cores_)
        ReportCUBLASErrors(cublasSetMathMode(
            cublas_,
            CUBLAS_TENSOR_OP_MATH));  // Deprecated on CUDA 11.0 and later
      else if (fp16)
        ReportCUBLASErrors(cublasSetMathMode(
            cublas_,
            CUBLAS_PEDANTIC_MATH));  // Explicitly set PEDANTIC_MATH mode to
                                     // avoid cublas bug of making use of tensor
                                     // core math on TU11x GPUs that don't
                                     // support it.
    }

    const int kNumInputPlanes = kInputPlanes;
    const int kNumFilters = (int)weights.input.biases.size();
    numBlocks_ = (int)weights.residual.size();

    // Warn if the memory required for storing transformed weights is
    // going to exceed 40% of total video memory, force custom_winograd off
    // if it's going to exceed 50% of memory.
    size_t residual_single_layer_weight_size =
        3 * 3 * kNumFilters * kNumFilters * sizeof(DataType);
    size_t residual_weight_size =
        residual_single_layer_weight_size * numBlocks_ * 2;
    size_t transformed_residual_weight_size = residual_weight_size * 4;

    if (transformed_residual_weight_size > 0.4 * deviceProp.totalGlobalMem) {
      CERR << "WARNING: Low GPU video memory. You may run into OOM errors. Try "
              "using a smaller network.";
    }

    // Disable res block fusing for fp32 for now (not worth it)
    // TODO: make it work for filters not a multiple of 32.
    // Note that when used with SE, the optimization
    // works only when filter count is <= 384 (pre-Ampere), or less than 512
    // (Ampere)
    // It turns dynamically off based on filter count (see
    // ResidualBlock<DataType>::Eval)
    if (kNumFilters % 32 == 0 && std::is_same<half, DataType>::value) {
      use_res_block_winograd_fuse_opt_ = true;
    } else {
      use_res_block_winograd_fuse_opt_ = false;
    }
    // Override if set in backend-opts.
    if (!options.IsDefault<bool>("res_block_fusing")) {
      use_res_block_winograd_fuse_opt_ = options.Get<bool>("res_block_fusing");
    }

    const bool use_gemm_ex = deviceProp.major >= 5;

    // 0. Check for SE.
    has_se_ = false;
    if (weights.residual[0].has_se) {
      has_se_ = true;
    }

    // Have some minumum as we also use this for transforming weights.
    size_t max_weight_size = 128 * 1024 * 1024;

    // parts from scratch allocation are suballocated to hold various weights
    // and biases when transforming winograd weights (one layer at a time), 128
    // MB is way more than that what we need but make sure it's at least 3x of
    // single layer's weight size to be safe.
    if (max_weight_size < 3 * residual_single_layer_weight_size)
      max_weight_size = 3 * residual_single_layer_weight_size;

    scratch_size_ = max_weight_size;

    // Need additional space for transformed input/outputs which are 36/16
    // times size (4x4 block transformed into 6x6).
    const size_t transformed_tensor_size = (size_t)(
        max_batch_size_ * kNumFilters * 64 * (36.0 / 16.0) * sizeof(DataType));
    scratch_size_ = std::max(scratch_size_, 2 * transformed_tensor_size);

    // Attention policy head may need more memory
    const size_t attentionSize =
        getMaxAttentionHeadSize(weights, max_batch_size_);
    scratch_size_ = std::max(scratch_size_, attentionSize);

    ReportCUDAErrors(cudaMalloc(&scratch_mem_, scratch_size_));

    const bool mish_net = file.format().network_format().default_activation() ==
                          pblczero::NetworkFormat::DEFAULT_ACTIVATION_MISH;

    // 2. Build the network, and copy the weights to GPU memory.

    // Input.
    {
      auto inputConv = std::make_unique<FusedWinogradConvSELayer<DataType>>(
          nullptr, kNumFilters, 8, 8, kNumInputPlanes, mish_net ? MISH : RELU,
          true, false, false, 0, use_gemm_ex, use_res_block_winograd_fuse_opt_);
      inputConv->LoadWeights(&weights.input.weights[0],
                             &weights.input.biases[0], scratch_mem_);
      network_.emplace_back(std::move(inputConv));
    }

    // Residual block.
    for (int block = 0; block < numBlocks_; block++) {
      bool has_se = weights.residual[block].has_se;
      int se_k = (int)weights.residual[block].se.b1.size();

      if (use_res_block_winograd_fuse_opt_) {
        auto layer = std::make_unique<ResidualBlock<DataType>>(
            getLastLayer(), kNumFilters, has_se, se_k, use_gemm_ex, block == 0,
            block == (numBlocks_ - 1), mish_net ? MISH : RELU,
            deviceProp.sharedMemPerBlockOptin);
        layer->LoadWeights0(&weights.residual[block].conv1.weights[0],
                            &weights.residual[block].conv1.biases[0],
                            scratch_mem_);
        layer->LoadWeights1(&weights.residual[block].conv2.weights[0],
                            &weights.residual[block].conv2.biases[0],
                            scratch_mem_);
        if (has_se)
          layer->LoadSEWeights(&weights.residual[block].se.w1[0],
                               &weights.residual[block].se.b1[0],
                               &weights.residual[block].se.w2[0],
                               &weights.residual[block].se.b2[0], scratch_mem_);
        network_.emplace_back(std::move(layer));
      } else {
        auto conv1 = std::make_unique<FusedWinogradConvSELayer<DataType>>(
            getLastLayer(), kNumFilters, 8, 8, kNumFilters,
            mish_net ? MISH : RELU, true, false, false, 0, use_gemm_ex);
        conv1->LoadWeights(&weights.residual[block].conv1.weights[0],
                           &weights.residual[block].conv1.biases[0],
                           scratch_mem_);
        network_.emplace_back(std::move(conv1));

        auto conv2 = std::make_unique<FusedWinogradConvSELayer<DataType>>(
            getLastLayer(), kNumFilters, 8, 8, kNumFilters,
            mish_net ? MISH : RELU, true, true, has_se, se_k, use_gemm_ex);
        conv2->LoadWeights(&weights.residual[block].conv2.weights[0],
                           &weights.residual[block].conv2.biases[0],
                           scratch_mem_);
        if (has_se)
          conv2->LoadSEWeights(&weights.residual[block].se.w1[0],
                               &weights.residual[block].se.b1[0],
                               &weights.residual[block].se.w2[0],
                               &weights.residual[block].se.b2[0], scratch_mem_);
        network_.emplace_back(std::move(conv2));
      }
    }

    resi_last_ = getLastLayer();

    // Policy head.
    if (attn_policy_) {
      auto AttentionPolicy = std::make_unique<AttentionPolicyHead<DataType>>(
          getLastLayer(), weights, scratch_mem_);
      network_.emplace_back(std::move(AttentionPolicy));

      auto policymap = std::make_unique<PolicyMapLayer<DataType>>(
          getLastLayer(), kNumOutputPolicy, 1, 1, 64 * 64 + 8 * 24, true);
      policymap->LoadWeights(kAttnPolicyMap, scratch_mem_);
      network_.emplace_back(std::move(policymap));

    } else if (conv_policy_) {
      auto conv1 = std::make_unique<FusedWinogradConvSELayer<DataType>>(
          resi_last_, kNumFilters, 8, 8, kNumFilters, mish_net ? MISH : RELU,
          true, false, false, 0, use_gemm_ex);
      conv1->LoadWeights(&weights.policy1.weights[0],
                         &weights.policy1.biases[0], scratch_mem_);
      network_.emplace_back(std::move(conv1));

      auto pol_channels = weights.policy.biases.size();

      // No relu
      auto conv2 = std::make_unique<FusedWinogradConvSELayer<DataType>>(
          getLastLayer(), pol_channels, 8, 8, kNumFilters, NONE, true, false,
          false, 0, use_gemm_ex);
      conv2->LoadWeights(&weights.policy.weights[0], &weights.policy.biases[0],
                         scratch_mem_);
      network_.emplace_back(std::move(conv2));

      auto policymap = std::make_unique<PolicyMapLayer<DataType>>(
          getLastLayer(), kNumOutputPolicy, 1, 1, 73 * 8 * 8, false);
      policymap->LoadWeights(kConvPolicyMap, scratch_mem_);

      network_.emplace_back(std::move(policymap));
    } else {
      auto convPol = std::make_unique<Conv1Layer<DataType>>(
          resi_last_, weights.policy.biases.size(), 8, 8, kNumFilters,
          mish_net ? MISH : RELU, true, use_gemm_ex);
      convPol->LoadWeights(&weights.policy.weights[0],
                           &weights.policy.biases[0], scratch_mem_);
      network_.emplace_back(std::move(convPol));

      auto FCPol = std::make_unique<FCLayer<DataType>>(
          getLastLayer(), weights.ip_pol_b.size(), 1, 1, true, NONE);
      FCPol->LoadWeights(&weights.ip_pol_w[0], &weights.ip_pol_b[0],
                         scratch_mem_);
      network_.emplace_back(std::move(FCPol));
    }
    policy_out_ = getLastLayer();

    // Value head.
    {
      auto convVal = std::make_unique<Conv1Layer<DataType>>(
          resi_last_, weights.value.biases.size(), 8, 8, kNumFilters,
          mish_net ? MISH : RELU, true, use_gemm_ex);
      convVal->LoadWeights(&weights.value.weights[0], &weights.value.biases[0],
                           scratch_mem_);
      network_.emplace_back(std::move(convVal));

      auto FCVal1 = std::make_unique<FCLayer<DataType>>(
          getLastLayer(), weights.ip1_val_b.size(), 1, 1, true,
          mish_net ? MISH : RELU);
      FCVal1->LoadWeights(&weights.ip1_val_w[0], &weights.ip1_val_b[0],
                          scratch_mem_);
      network_.emplace_back(std::move(FCVal1));

      wdl_ = file.format().network_format().value() ==
             pblczero::NetworkFormat::VALUE_WDL;
      auto fc2_tanh = !wdl_;

      auto FCVal2 = std::make_unique<FCLayer<DataType>>(
          getLastLayer(), weights.ip2_val_b.size(), 1, 1, true,
          fc2_tanh ? TANH : NONE);
      FCVal2->LoadWeights(&weights.ip2_val_w[0], &weights.ip2_val_b[0],
                          scratch_mem_);
      network_.emplace_back(std::move(FCVal2));
    }
    value_out_ = getLastLayer();

    // Moves left head
    moves_left_ = (file.format().network_format().moves_left() ==
                   pblczero::NetworkFormat::MOVES_LEFT_V1) &&
                  options.GetOrDefault<bool>("mlh", true);
    if (moves_left_) {
      auto convMov = std::make_unique<Conv1Layer<DataType>>(
          resi_last_, weights.moves_left.biases.size(), 8, 8, kNumFilters,
          mish_net ? MISH : RELU, true, use_gemm_ex);
      convMov->LoadWeights(&weights.moves_left.weights[0],
                           &weights.moves_left.biases[0], scratch_mem_);
      network_.emplace_back(std::move(convMov));

      auto FCMov1 = std::make_unique<FCLayer<DataType>>(
          getLastLayer(), weights.ip1_mov_b.size(), 1, 1, true,
          mish_net ? MISH : RELU);
      FCMov1->LoadWeights(&weights.ip1_mov_w[0], &weights.ip1_mov_b[0],
                          scratch_mem_);
      network_.emplace_back(std::move(FCMov1));

      auto FCMov2 = std::make_unique<FCLayer<DataType>>(getLastLayer(), 1, 1, 1,
                                                        true, RELU);
      FCMov2->LoadWeights(&weights.ip2_mov_w[0], &weights.ip2_mov_b[0],
                          scratch_mem_);
      network_.emplace_back(std::move(FCMov2));
    }
    moves_left_out_ = getLastLayer();

    // 3. Allocate GPU memory for running the network:
    //    - three buffers of max size are enough (one to hold input, second to
    //      hold output and third to hold skip connection's input).

    // size of input to the network
    size_t maxSize = max_batch_size_ * kNumInputPlanes * 64 * sizeof(DataType);

    // take max size of all layers
    for (auto& layer : network_) {
      maxSize = std::max(maxSize, layer->GetOutputSize(max_batch_size_));
    }

    if ((attn_policy_ || use_res_block_winograd_fuse_opt_) &&
        (scratch_size_ > maxSize)) {
      maxSize = scratch_size_;
    }

    if (!multi_stream_) {
      for (auto& mem : tensor_mem_) {
        ReportCUDAErrors(cudaMalloc(&mem, maxSize));
        ReportCUDAErrors(cudaMemset(mem, 0, maxSize));
      }
    }

    tensor_mem_size_ = multi_stream_ ? maxSize : 0;

    // pre-allocate one InputsOutputs object
    // The first call to allocate memory, create cublas,
    // strem, etc takes really long (600 ms)
    std::unique_ptr<InputsOutputs> io = GetInputsOutputs();
  }

  void forwardEval(InputsOutputs* io, int batchSize) {
    if (!multi_stream_) lock_.lock();

#ifdef DEBUG_RAW_NPS
    auto t_start = std::chrono::high_resolution_clock::now();
#endif

    // Expand packed planes to full planes.
    uint64_t* ipDataMasks = io->input_masks_mem_gpu_;
    float* ipDataValues = io->input_val_mem_gpu_;

    DataType* tensor_mem[3];
    void* scratch_mem;
    cudaStream_t stream;
    cublasHandle_t cublas;
    if (multi_stream_) {
      // We use tensor and scratch memory from InputOutputs (so that multiple
      // requests can run in parallel)
      for (int i = 0; i < 3; i++) tensor_mem[i] = (DataType*)io->tensor_mem_[i];
      scratch_mem = io->scratch_mem_;
      stream = io->stream_;
      cublas = io->cublas_;
    } else {
      for (int i = 0; i < 3; i++) tensor_mem[i] = tensor_mem_[i];
      scratch_mem = scratch_mem_;
      stream = 0;  // default stream
      cublas = cublas_;
    }

    bool fp16 = std::is_same<half, DataType>::value;
    if (fp16) {
      expandPlanes_Fp16_NCHW((half*)(tensor_mem[0]), ipDataMasks, ipDataValues,
                             batchSize * kInputPlanes, stream);
    } else {
      expandPlanes_Fp32_NCHW((float*)(tensor_mem[0]), ipDataMasks, ipDataValues,
                             batchSize * kInputPlanes, stream);
    }

    float* opPol = io->op_policy_mem_gpu_;
    float* opVal = io->op_value_mem_gpu_;
    float* opMov = io->op_moves_left_mem_gpu_;

    int l = 0;
    // Input.
    network_[l++]->Eval(
        batchSize,
        use_res_block_winograd_fuse_opt_ ? tensor_mem[1] : tensor_mem[2],
        tensor_mem[0], nullptr, scratch_mem, scratch_size_, nullptr, cublas,
        stream);  // input conv

    // Residual block.
    for (int block = 0; block < numBlocks_; block++) {
      if (use_res_block_winograd_fuse_opt_) {
        network_[l++]->Eval(batchSize, tensor_mem[2], tensor_mem[1], nullptr,
                            scratch_mem, scratch_size_, nullptr, cublas,
                            stream);  // block
      } else {
        network_[l++]->Eval(batchSize, tensor_mem[0], tensor_mem[2], nullptr,
                            scratch_mem, scratch_size_, nullptr, cublas,
                            stream);  // conv1

        network_[l++]->Eval(batchSize, tensor_mem[2], tensor_mem[0],
                            tensor_mem[2], scratch_mem, scratch_size_, nullptr,
                            cublas, stream);  // conv2
      }
    }

    // Policy head.
    if (attn_policy_) {
      network_[l++]->Eval(
          batchSize, tensor_mem[0], tensor_mem[2], tensor_mem[1], scratch_mem,
          scratch_size_, nullptr, cublas,
          stream);  // Entire Attention policy head except for the policy map
      if (fp16) {
        network_[l++]->Eval(batchSize, tensor_mem[1], tensor_mem[0], nullptr,
                            scratch_mem, scratch_size_, nullptr, cublas,
                            stream);  // policy map layer
        copyTypeConverted(opPol, (half*)(tensor_mem[1]),
                          batchSize * kNumOutputPolicy,
                          stream);  // POLICY output
      } else {
        network_[l++]->Eval(batchSize, (DataType*)opPol, tensor_mem[0], nullptr,
                            scratch_mem, scratch_size_, nullptr, cublas,
                            stream);  // policy map layer  // POLICY output
      }

    } else if (conv_policy_) {
      network_[l++]->Eval(batchSize, tensor_mem[0], tensor_mem[2], nullptr,
                          scratch_mem, scratch_size_, nullptr, cublas,
                          stream);  // policy conv1

      network_[l++]->Eval(batchSize, tensor_mem[1], tensor_mem[0], nullptr,
                          scratch_mem, scratch_size_, nullptr, cublas,
                          stream);  // policy conv2

      if (fp16) {
        network_[l++]->Eval(batchSize, tensor_mem[0], tensor_mem[1], nullptr,
                            scratch_mem, scratch_size_, nullptr, cublas,
                            stream);  // policy map layer
        copyTypeConverted(opPol, (half*)(tensor_mem[0]),
                          batchSize * kNumOutputPolicy,
                          stream);  // POLICY output
      } else {
        network_[l++]->Eval(batchSize, (DataType*)opPol, tensor_mem[1], nullptr,
                            scratch_mem, scratch_size_, nullptr, cublas,
                            stream);  // policy map layer  // POLICY output
      }
    } else {
      network_[l++]->Eval(batchSize, tensor_mem[0], tensor_mem[2], nullptr,
                          scratch_mem, scratch_size_, nullptr, cublas,
                          stream);  // pol conv

      if (fp16) {
        network_[l++]->Eval(batchSize, tensor_mem[1], tensor_mem[0], nullptr,
                            scratch_mem, scratch_size_, nullptr, cublas,
                            stream);  // pol FC

        copyTypeConverted(opPol, (half*)(tensor_mem[1]),
                          batchSize * kNumOutputPolicy, stream);  // POLICY
      } else {
        network_[l++]->Eval(batchSize, (DataType*)opPol, tensor_mem[0], nullptr,
                            scratch_mem, scratch_size_, nullptr, cublas,
                            stream);  // pol FC  // POLICY
      }
    }

    // Copy policy output from device memory to host memory.
    ReportCUDAErrors(
        cudaMemcpyAsync(io->op_policy_mem_, io->op_policy_mem_gpu_,
                        sizeof(float) * kNumOutputPolicy * batchSize,
                        cudaMemcpyDeviceToHost, stream));

    // value head
    network_[l++]->Eval(batchSize, tensor_mem[0], tensor_mem[2], nullptr,
                        scratch_mem, scratch_size_, nullptr, cublas,
                        stream);  // value conv

    network_[l++]->Eval(batchSize, tensor_mem[1], tensor_mem[0], nullptr,
                        scratch_mem, scratch_size_, nullptr, cublas,
                        stream);  // value FC1

    if (wdl_) {
      if (fp16) {
        network_[l++]->Eval(batchSize, tensor_mem[0], tensor_mem[1], nullptr,
                            scratch_mem, scratch_size_, nullptr, cublas,
                            stream);  // value FC2    // VALUE
        copyTypeConverted(opVal, (half*)(tensor_mem[0]), 3 * batchSize,
                          stream);  // VALUE
      } else {
        network_[l++]->Eval(batchSize, (DataType*)opVal, tensor_mem[1], nullptr,
                            scratch_mem, scratch_size_, nullptr, cublas,
                            stream);  // value FC2    // VALUE
      }
    } else {
      if (fp16) {
        // TODO: consider fusing the bias-add of FC2 with format conversion.
        network_[l++]->Eval(batchSize, tensor_mem[0], tensor_mem[1], nullptr,
                            scratch_mem, scratch_size_, nullptr, cublas,
                            stream);  // value FC2
        copyTypeConverted(opVal, (half*)(tensor_mem[0]), batchSize,
                          stream);  // VALUE
      } else {
        network_[l++]->Eval(batchSize, (DataType*)opVal, tensor_mem[1], nullptr,
                            scratch_mem, scratch_size_, nullptr, cublas,
                            stream);  // value FC2    // VALUE
      }
    }

    if (moves_left_) {
      // Moves left head
      network_[l++]->Eval(batchSize, tensor_mem[0], tensor_mem[2], nullptr,
                          scratch_mem, scratch_size_, nullptr, cublas,
                          stream);  // moves conv

      network_[l++]->Eval(batchSize, tensor_mem[1], tensor_mem[0], nullptr,
                          scratch_mem, scratch_size_, nullptr, cublas,
                          stream);  // moves FC1

      // Moves left FC2
      if (fp16) {
        // TODO: consider fusing the bias-add of FC2 with format conversion.
        network_[l++]->Eval(batchSize, tensor_mem[0], tensor_mem[1], nullptr,
                            scratch_mem, scratch_size_, nullptr, cublas,
                            stream);
        copyTypeConverted(opMov, (half*)(tensor_mem[0]), batchSize, stream);
      } else {
        network_[l++]->Eval(batchSize, (DataType*)opMov, tensor_mem[1], nullptr,
                            scratch_mem, scratch_size_, nullptr, cublas,
                            stream);
      }
    }

    if (multi_stream_) {
      ReportCUDAErrors(cudaStreamSynchronize(stream));
    } else {
      ReportCUDAErrors(cudaDeviceSynchronize());
      // The next thread can start using the GPU now.
      lock_.unlock();
    }

    if (wdl_) {
      // Value softmax done cpu side.
      for (int i = 0; i < batchSize; i++) {
        float w = io->op_value_mem_[3 * i + 0];
        float d = io->op_value_mem_[3 * i + 1];
        float l = io->op_value_mem_[3 * i + 2];
        float m = std::max({w, d, l});
        w = std::exp(w - m);
        d = std::exp(d - m);
        l = std::exp(l - m);
        float sum = w + d + l;
        w /= sum;
        l /= sum;
        d = 1.0f - w - l;
        io->op_value_mem_[3 * i + 0] = w;
        io->op_value_mem_[3 * i + 1] = d;
        io->op_value_mem_[3 * i + 2] = l;
      }
    }
  }

  ~CudaNetwork() {
    if (scratch_mem_) ReportCUDAErrors(cudaFree(scratch_mem_));
    if (!multi_stream_) {
      for (auto mem : tensor_mem_) {
        if (mem) ReportCUDAErrors(cudaFree(mem));
      }
      cublasDestroy(cublas_);
    }
  }

  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

  std::unique_ptr<NetworkComputation> NewComputation() override {
    // Set correct gpu id for this computation (as it might have been called
    // from a different thread).
    ReportCUDAErrors(cudaSetDevice(gpu_id_));
    return std::make_unique<CudaNetworkComputation<DataType>>(this, wdl_,
                                                              moves_left_);
  }

  std::unique_ptr<InputsOutputs> GetInputsOutputs() {
    std::lock_guard<std::mutex> lock(inputs_outputs_lock_);
    if (free_inputs_outputs_.empty()) {
      return std::make_unique<InputsOutputs>(
          max_batch_size_, wdl_, moves_left_, tensor_mem_size_, scratch_size_,
          !has_tensor_cores_ && std::is_same<half, DataType>::value);
    } else {
      std::unique_ptr<InputsOutputs> resource =
          std::move(free_inputs_outputs_.front());
      free_inputs_outputs_.pop_front();
      return resource;
    }
  }

  void ReleaseInputsOutputs(std::unique_ptr<InputsOutputs> resource) {
    std::lock_guard<std::mutex> lock(inputs_outputs_lock_);
    free_inputs_outputs_.push_back(std::move(resource));
  }

  // Apparently nvcc doesn't see constructor invocations through make_unique.
  // This function invokes constructor just to please complier and silence
  // warning. Is never called (but compiler thinks that it could).
  void UglyFunctionToSilenceNvccWarning() { InputsOutputs io(0, false, false); }

 private:
  const NetworkCapabilities capabilities_;
  int gpu_id_;
  int max_batch_size_;
  bool wdl_;
  bool moves_left_;
  bool use_res_block_winograd_fuse_opt_;  // fuse operations inside the residual
                                          // tower
  bool multi_stream_;                     // run multiple parallel network evals

  // Currently only one NN Eval can happen a time (we can fix this if needed
  // by allocating more memory).
  mutable std::mutex lock_;

  int numBlocks_;
  bool has_se_;
  bool conv_policy_;
  bool attn_policy_;
  std::vector<std::unique_ptr<BaseLayer<DataType>>> network_;
  BaseLayer<DataType>* getLastLayer() { return network_.back().get(); }

  BaseLayer<DataType>* resi_last_;
  BaseLayer<DataType>* policy_out_;
  BaseLayer<DataType>* value_out_;
  BaseLayer<DataType>* moves_left_out_;

  size_t tensor_mem_size_;
  size_t scratch_size_;

  // this copy is used only for initialization when multi-stream is enabled
  void* scratch_mem_;

  bool has_tensor_cores_;

  // not used when multi-steam is enabled
  cublasHandle_t cublas_;
  DataType* tensor_mem_[3];

  mutable std::mutex inputs_outputs_lock_;
  std::list<std::unique_ptr<InputsOutputs>> free_inputs_outputs_;

  void showInfo() const {
    int version;
    int ret = cudaRuntimeGetVersion(&version);
    switch (ret) {
      case cudaErrorInitializationError:
        throw Exception("CUDA driver and/or runtime could not be initialized");
      case cudaErrorInsufficientDriver:
        throw Exception("No CUDA driver, or one older than the CUDA library");
      case cudaErrorNoDevice:
        throw Exception("No CUDA-capable devices detected");
    }
    int major = version / 1000;
    int minor = (version - major * 1000) / 10;
    int pl = version - major * 1000 - minor * 10;
    CERR << "CUDA Runtime version: " << major << "." << minor << "." << pl;
    if (version != CUDART_VERSION) {
      major = CUDART_VERSION / 1000;
      minor = (CUDART_VERSION - major * 1000) / 10;
      pl = CUDART_VERSION - major * 1000 - minor * 10;
      CERR << "WARNING: CUDA Runtime version mismatch, was compiled with "
              "version "
           << major << "." << minor << "." << pl;
    }
    cudaDriverGetVersion(&version);
    major = version / 1000;
    minor = (version - major * 1000) / 10;
    pl = version - major * 1000 - minor * 10;
    CERR << "Latest version of CUDA supported by the driver: " << major << "."
         << minor << "." << pl;
    if (version < CUDART_VERSION) {
      CERR << "WARNING: code was compiled with unsupported CUDA version.";
    }
  }

  void showDeviceInfo(const cudaDeviceProp& deviceProp) const {
    CERR << "GPU: " << deviceProp.name;
    CERR << "GPU memory: " << deviceProp.totalGlobalMem / std::pow(2.0f, 30)
         << " Gb";
    CERR << "GPU clock frequency: " << deviceProp.clockRate / 1e3f << " MHz";
    CERR << "GPU compute capability: " << deviceProp.major << "."
         << deviceProp.minor;

    if (std::is_same<float, DataType>::value && deviceProp.major >= 7) {
      CERR << "WARNING: you will probably get better performance from the "
              "cuda-fp16 backend.";
    }
  }
};

template <typename DataType>
CudaNetworkComputation<DataType>::CudaNetworkComputation(
    CudaNetwork<DataType>* network, bool wdl, bool moves_left)
    : wdl_(wdl), moves_left_(moves_left), network_(network) {
  batch_size_ = 0;
  inputs_outputs_ = network_->GetInputsOutputs();
}

template <typename DataType>
CudaNetworkComputation<DataType>::~CudaNetworkComputation() {
  network_->ReleaseInputsOutputs(std::move(inputs_outputs_));
}

template <typename DataType>
void CudaNetworkComputation<DataType>::ComputeBlocking() {
  network_->forwardEval(inputs_outputs_.get(), GetBatchSize());
}

template <typename DataType>
std::unique_ptr<Network> MakeCudaNetwork(const std::optional<WeightsFile>& w,
                                         const OptionsDict& options) {
  if (!w) {
    throw Exception(
        "The cuda" +
        std::string(std::is_same<half, DataType>::value ? "-fp16" : "") +
        " backend requires a network file.");
  }
  const WeightsFile& weights = *w;
  if (weights.format().network_format().network() !=
          pblczero::NetworkFormat::NETWORK_CLASSICAL_WITH_HEADFORMAT &&
      weights.format().network_format().network() !=
          pblczero::NetworkFormat::NETWORK_SE_WITH_HEADFORMAT) {
    throw Exception("Network format " +
                    pblczero::NetworkFormat::NetworkStructure_Name(
                        weights.format().network_format().network()) +
                    " is not supported by the CUDA backend.");
  }
  if (weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_CLASSICAL &&
      weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_CONVOLUTION &&
      weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_ATTENTION) {
    throw Exception("Policy format " +
                    pblczero::NetworkFormat::PolicyFormat_Name(
                        weights.format().network_format().policy()) +
                    " is not supported by the CUDA backend.");
  }
  if (weights.format().network_format().value() !=
          pblczero::NetworkFormat::VALUE_CLASSICAL &&
      weights.format().network_format().value() !=
          pblczero::NetworkFormat::VALUE_WDL) {
    throw Exception("Value format " +
                    pblczero::NetworkFormat::ValueFormat_Name(
                        weights.format().network_format().value()) +
                    " is not supported by the CUDA backend.");
  }
  if (weights.format().network_format().moves_left() !=
          pblczero::NetworkFormat::MOVES_LEFT_NONE &&
      weights.format().network_format().moves_left() !=
          pblczero::NetworkFormat::MOVES_LEFT_V1) {
    throw Exception("Moves left head format " +
                    pblczero::NetworkFormat::MovesLeftFormat_Name(
                        weights.format().network_format().moves_left()) +
                    " is not supported by the CUDA backend.");
  }
  if (weights.format().network_format().default_activation() !=
          pblczero::NetworkFormat::DEFAULT_ACTIVATION_RELU &&
      weights.format().network_format().default_activation() !=
          pblczero::NetworkFormat::DEFAULT_ACTIVATION_MISH) {
    throw Exception(
        "Default activation " +
        pblczero::NetworkFormat::DefaultActivation_Name(
            weights.format().network_format().default_activation()) +
        " is not supported by the CUDA backend.");
  }
  return std::make_unique<CudaNetwork<DataType>>(weights, options);
}

std::unique_ptr<Network> MakeCudaNetworkAuto(
    const std::optional<WeightsFile>& weights, const OptionsDict& options) {
  int gpu_id = options.GetOrDefault<int>("gpu", 0);
  cudaDeviceProp deviceProp = {};
  // No error checking here, this will be repeated later.
  cudaGetDeviceProperties(&deviceProp, gpu_id);

  // Check if the GPU supports FP16.
  if (deviceProp.major >= 7 ||
      (deviceProp.major == 6 && deviceProp.minor != 1) ||
      (deviceProp.major == 5 && deviceProp.minor == 3)) {
    CERR << "Switching to [cuda-fp16]...";
    return MakeCudaNetwork<half>(weights, options);
  }
  CERR << "Switching to [cuda]...";
  return MakeCudaNetwork<float>(weights, options);
}

REGISTER_NETWORK("cuda-auto", MakeCudaNetworkAuto, 104)
REGISTER_NETWORK("cuda", MakeCudaNetwork<float>, 103)
REGISTER_NETWORK("cuda-fp16", MakeCudaNetwork<half>, 102)

}  // namespace lczero

```

`src/neural/cuda/network_cudnn.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2022 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#include <algorithm>
#include <cassert>
#include <functional>
#include <list>
#include <memory>
#include <mutex>

#include "cuda_common.h"
#include "inputs_outputs.h"
#include "kernels.h"
#include "layers.h"
#include "neural/factory.h"
#include "neural/network_legacy.h"
#include "neural/shared/policy_map.h"
#include "utils/bititer.h"
#include "utils/exception.h"

//#define DEBUG_RAW_NPS

namespace lczero {
using namespace cudnn_backend;

#if 0
// debug code to dump allocation in GPU memory
void dumpTensor(void *memory, int elements, const char *message, bool fp16 = false)
{
    printf("\n%s\n", message);
    int elementSize = (int) (fp16 ? sizeof(half) : sizeof(float));
    int bytes = elements * elementSize;
    void *temp = malloc(bytes);
    cudaMemcpy(temp, memory, bytes, cudaMemcpyDeviceToHost);

    for (int i = 0; i < elements; i++)
    {
        float val;
        if (fp16) 
        {
            half *arr = (half*)temp;
            val = (float)arr[i];
        }
        else
        {
            float *arr = (float *)temp;
            val = arr[i];
        }
        printf("%8.4f ", val);
        if ((i % 8) == 7) printf("\n");
    }
    free(temp);
    printf("\n");
}
#endif

template <typename DataType>
class CudnnNetwork;

template <typename DataType>
class CudnnNetworkComputation : public NetworkComputation {
 public:
  CudnnNetworkComputation(CudnnNetwork<DataType>* network, bool wdl,
                          bool moves_left);
  ~CudnnNetworkComputation();

  void AddInput(InputPlanes&& input) override {
    const auto iter_mask =
        &inputs_outputs_->input_masks_mem_[batch_size_ * kInputPlanes];
    const auto iter_val =
        &inputs_outputs_->input_val_mem_[batch_size_ * kInputPlanes];

    int i = 0;
    for (const auto& plane : input) {
      iter_mask[i] = plane.mask;
      iter_val[i] = plane.value;
      i++;
    }

    batch_size_++;
  }

  void ComputeBlocking() override;

  int GetBatchSize() const override { return batch_size_; }

  float GetQVal(int sample) const override {
    if (wdl_) {
      auto w = inputs_outputs_->op_value_mem_[3 * sample + 0];
      auto l = inputs_outputs_->op_value_mem_[3 * sample + 2];
      return w - l;
    } else {
      return inputs_outputs_->op_value_mem_[sample];
    }
  }

  float GetDVal(int sample) const override {
    if (wdl_) {
      auto d = inputs_outputs_->op_value_mem_[3 * sample + 1];
      return d;
    } else {
      return 0.0f;
    }
  }

  float GetPVal(int sample, int move_id) const override {
    return inputs_outputs_->op_policy_mem_[sample * kNumOutputPolicy + move_id];
  }

  float GetMVal(int sample) const override {
    if (moves_left_) {
      return inputs_outputs_->op_moves_left_mem_[sample];
    }
    return 0.0f;
  }

 private:
  // Memory holding inputs, outputs.
  std::unique_ptr<InputsOutputs> inputs_outputs_;
  int batch_size_;
  bool wdl_;
  bool moves_left_;

  CudnnNetwork<DataType>* network_;
};

template <typename DataType>
class CudnnNetwork : public Network {
 public:
  CudnnNetwork(const WeightsFile& file, const OptionsDict& options)
      : capabilities_{file.format().network_format().input(),
                      file.format().network_format().moves_left()} {
    LegacyWeights weights(file.weights());
    gpu_id_ = options.GetOrDefault<int>("gpu", 0);

    conv_policy_ = file.format().network_format().policy() ==
                   pblczero::NetworkFormat::POLICY_CONVOLUTION;

    max_batch_size_ = options.GetOrDefault<int>("max_batch", 1024);

    showInfo();

    int total_gpus;
    ReportCUDAErrors(cudaGetDeviceCount(&total_gpus));

    if (gpu_id_ >= total_gpus)
      throw Exception("Invalid GPU Id: " + std::to_string(gpu_id_));

    cudaDeviceProp deviceProp = {};
    cudaGetDeviceProperties(&deviceProp, gpu_id_);
    showDeviceInfo(deviceProp);

    // Select GPU to run on (for *the current* thread).
    ReportCUDAErrors(cudaSetDevice(gpu_id_));

    ReportCUDNNErrors(cudnnCreate(&cudnn_));
    ReportCUBLASErrors(cublasCreate(&cublas_));

    // Default layout is nchw.
    nhwc_ = false;
    bool hasTensorCores = false;
    constexpr bool fp16 = std::is_same<half, DataType>::value;

    if (fp16) {
      // Check if the GPU support FP16.

      if ((deviceProp.major == 6 && deviceProp.minor != 1) ||
          (deviceProp.major == 5 && deviceProp.minor == 3)) {
        // FP16 without tensor cores supported on GP100 (SM 6.0) and Jetson
        // (SM 5.3 and 6.2). SM 6.1 GPUs also have FP16, but slower than FP32.
        // nhwc_ remains false.
      } else if (deviceProp.major >= 7) {
        // NHWC layout is faster with Tensor Cores when using cudnn's implicit
        // gemm algorithm.
        // Supported on Volta and Turing (and hopefully future GPUs too).

        // Some GPUs (GTX 16xx) are SM 7.5 but don't have tensor cores
        // enabling TENSOR_OP_MATH or nhwc_ layout for them works but is
        // very very slow (likely because the system emulates it).
        if (!strstr(deviceProp.name, "GTX 16")) {
          hasTensorCores = true;
          nhwc_ = true;
        }
      } else {
        throw Exception("Your GPU doesn't support FP16");
      }

      // Override if forced from backend option
      if (!options.IsDefault<bool>("nhwc")) nhwc_ = options.Get<bool>("nhwc");
    }

    if (hasTensorCores)
      ReportCUBLASErrors(cublasSetMathMode(
          cublas_,
          CUBLAS_TENSOR_OP_MATH));  // Deprecated on CUDA 11.0 and later
    else if (fp16)
      ReportCUBLASErrors(cublasSetMathMode(
          cublas_,
          CUBLAS_PEDANTIC_MATH));  // Explicitly set PEDANTIC_MATH mode to
                                   // avoid cublas bug of making use of tensor
                                   // core math on TU11x GPUs that don't
                                   // support it.

    const int kNumInputPlanes = kInputPlanes;
    const int kNumFilters = (int)weights.input.biases.size();
    numBlocks_ = (int)weights.residual.size();

    // Use our custom winograd for residual tower convolutions for most cases:
    //
    //  1. Should be always faster than cudnn's winograd that we use for fp32,
    //  and for fp16 on GPUs without tensor cores
    //
    //  2. Should also be faster than cudnn's implicit GEMM on GPUs with tensor
    //     cores too, but only for networks with 256 or higher no. of filters.
    //
    //  3. Currently a bug in cublas makes it slower on RTX GPUs with fp16 so
    //  it's disabled. TODO: Enable it once the bug has been fixed and it's
    //  tested to be faster. Putting check for cuda 11 for now.

    if (fp16) {
      int cuda_version;
      cudaRuntimeGetVersion(&cuda_version);
      if (!hasTensorCores)
        use_custom_winograd_ = false;
      else if (kNumFilters >= 256 &&
               !(deviceProp.major == 7 && deviceProp.minor == 5 &&
                 cuda_version < 11000))
        use_custom_winograd_ = true;
      else
        use_custom_winograd_ = false;
    } else {
      use_custom_winograd_ = true;
    }

    // Warn if the memory required for storing transformed weights is
    // going to exceed 40% of total video memory, force custom_winograd off
    // if it's going to exceed 50% of memory.
    size_t residual_single_layer_weight_size =
        3 * 3 * kNumFilters * kNumFilters * sizeof(DataType);
    size_t residual_weight_size =
        residual_single_layer_weight_size * numBlocks_ * 2;
    size_t transformed_residual_weight_size = residual_weight_size * 4;

    if (residual_weight_size > 0.6 * deviceProp.totalGlobalMem) {
      CERR << "Low video memory detected. You may run into OOM errors. Please "
              "consider using a smaller network.";
    }

    const bool custom_winograd_override =
        !options.IsDefault<bool>("custom_winograd");

    if (!custom_winograd_override && use_custom_winograd_ &&
        transformed_residual_weight_size > 0.5 * deviceProp.totalGlobalMem) {
      CERR << "WARNING: Low GPU video memory. Turning off custom_winograd "
              "path. You may still run into OOM errors. "
              "Please consider using a smaller network.";
      use_custom_winograd_ = false;
    }

    // Override if set in backend-opts.
    if (custom_winograd_override)
      use_custom_winograd_ = options.Get<bool>("custom_winograd");

    if (use_custom_winograd_ &&
        transformed_residual_weight_size > 0.4 * deviceProp.totalGlobalMem) {
      CERR << "WARNING: Low GPU video memory. You may still run into OOM "
              "errors. Try with backend-opts=custom_winograd=false, or "
              "using a smaller network.";
    }

    // Winograd needs nchw tensor layout.
    if (use_custom_winograd_) nhwc_ = false;

    use_res_block_winograd_fuse_opt_ = false;
    if (use_custom_winograd_) {
      // Disable res block fusing for fp32 for now.
      // TODO: make it work for filters not a multiple of 32.
      if (kNumFilters % 32 == 0 && fp16) {
        use_res_block_winograd_fuse_opt_ = true;
      }
      // Override if set in backend-opts.
      if (!options.IsDefault<bool>("res_block_fusing")) {
        use_res_block_winograd_fuse_opt_ =
            options.Get<bool>("res_block_fusing");
      }
    }

    const bool use_gemm_ex = deviceProp.major >= 5;

    // 0. Check for SE.
    has_se_ = false;
    if (weights.residual[0].has_se) {
      has_se_ = true;
    }

    const bool mish_net = file.format().network_format().default_activation() ==
                          pblczero::NetworkFormat::DEFAULT_ACTIVATION_MISH;

    // 1. Allocate scratch space (used internally by cudnn to run convolutions,
    //     and also for format/layout conversion for weights).
    cudnnFilterDescriptor_t wDesc;
    cudnnConvolutionDescriptor_t convDesc;
    cudnnTensorDescriptor_t xDesc;
    cudnnCreateFilterDescriptor(&wDesc);
    cudnnCreateConvolutionDescriptor(&convDesc);
    cudnnCreateTensorDescriptor(&xDesc);
    cudnnConvolutionFwdAlgo_t conv_algo;

    const int maxChannels = std::max(kInputPlanes, kNumFilters);

    const cudnnDataType_t datatype = fp16 ? CUDNN_DATA_HALF : CUDNN_DATA_FLOAT;
    const cudnnTensorFormat_t layout =
        nhwc_ ? CUDNN_TENSOR_NHWC : CUDNN_TENSOR_NCHW;

    ReportCUDNNErrors(cudnnSetFilter4dDescriptor(
        wDesc, datatype, layout, maxChannels, maxChannels, 3, 3));

    ReportCUDNNErrors(cudnnSetTensor4dDescriptor(
        xDesc, layout, datatype, max_batch_size_, maxChannels, 8, 8));

    ReportCUDNNErrors(cudnnSetConvolution2dDescriptor(
        convDesc, 1, 1, 1, 1, 1, 1, CUDNN_CROSS_CORRELATION, datatype));

    // It will fall back to non-tensor math if not supported.
    ReportCUDNNErrors(
        cudnnSetConvolutionMathType(convDesc, CUDNN_TENSOR_OP_MATH));

    if (nhwc_) {
      conv_algo = CUDNN_CONVOLUTION_FWD_ALGO_IMPLICIT_PRECOMP_GEMM;
    } else {
      conv_algo = CUDNN_CONVOLUTION_FWD_ALGO_WINOGRAD_NONFUSED;
    }

    // Query expected scratch space from cudnn.
    ReportCUDNNErrors(cudnnGetConvolutionForwardWorkspaceSize(
        cudnn_, xDesc, wDesc, convDesc, xDesc, conv_algo, &scratch_size_));

    // Have some minumum as we also use this for transforming weights.
    size_t max_weight_size = 128 * 1024 * 1024;

    // parts from scratch allocation are suballocated to hold various weights
    // and biases when transforming winograd weights (one layer at a time), 128
    // MB is way more than that what we need but make sure it's at least 3x of
    // single layer's weight size to be safe.
    if (max_weight_size < 3 * residual_single_layer_weight_size)
      max_weight_size = 3 * residual_single_layer_weight_size;

    if (scratch_size_ < max_weight_size) scratch_size_ = max_weight_size;

    size_t transformed_tensor_size = 0;
    if (use_custom_winograd_) {
      // Need additional space for transformed input/outputs which are 36/16
      // times size (4x4 block transformed into 6x6).
      transformed_tensor_size = (size_t)(max_batch_size_ * kNumFilters * 64 *
                                         (36.0 / 16.0) * sizeof(DataType));
      scratch_size_ = std::max(scratch_size_, 2 * transformed_tensor_size);
    }

    ReportCUDAErrors(cudaMalloc(&scratch_mem_, scratch_size_));
#ifdef DEBUG_RAW_NPS
    CERR << "allocated " << scratch_size_ << " bytes for scratch memory";
#endif

    // 2. Build the network, and copy the weights to GPU memory.

    // Input.
    if (use_custom_winograd_) {
      auto inputConv = std::make_unique<FusedWinogradConvSELayer<DataType>>(
          nullptr, kNumFilters, 8, 8, kNumInputPlanes, mish_net ? MISH : RELU,
          true, false, false, 0, use_gemm_ex, use_res_block_winograd_fuse_opt_);
      inputConv->LoadWeights(&weights.input.weights[0],
                             &weights.input.biases[0], scratch_mem_);
      network_.emplace_back(std::move(inputConv));
    } else {
      auto inputConv = std::make_unique<ConvLayer<DataType>>(
          nhwc_, kNumFilters, 8, 8, 3, kNumInputPlanes, mish_net ? MISH : RELU,
          true);
      inputConv->LoadWeights(&weights.input.weights[0],
                             &weights.input.biases[0], scratch_mem_);
      network_.emplace_back(std::move(inputConv));
    }

    // Residual block.
    for (int block = 0; block < numBlocks_; block++) {
      if (use_custom_winograd_) {
        bool has_se = weights.residual[block].has_se;
        int se_k = (int)weights.residual[block].se.b1.size();

        if (use_res_block_winograd_fuse_opt_) {
          auto layer = std::make_unique<ResidualBlock<DataType>>(
              getLastLayer(), kNumFilters, has_se, se_k, use_gemm_ex,
              block == 0, block == (numBlocks_ - 1), mish_net ? MISH : RELU,
              deviceProp.sharedMemPerBlockOptin);
          layer->LoadWeights0(&weights.residual[block].conv1.weights[0],
                              &weights.residual[block].conv1.biases[0],
                              scratch_mem_);
          layer->LoadWeights1(&weights.residual[block].conv2.weights[0],
                              &weights.residual[block].conv2.biases[0],
                              scratch_mem_);
          if (has_se)
            layer->LoadSEWeights(&weights.residual[block].se.w1[0],
                                 &weights.residual[block].se.b1[0],
                                 &weights.residual[block].se.w2[0],
                                 &weights.residual[block].se.b2[0],
                                 scratch_mem_);
          network_.emplace_back(std::move(layer));
        } else {
          auto conv1 = std::make_unique<FusedWinogradConvSELayer<DataType>>(
              getLastLayer(), kNumFilters, 8, 8, kNumFilters,
              mish_net ? MISH : RELU, true, false, false, 0, use_gemm_ex);
          conv1->LoadWeights(&weights.residual[block].conv1.weights[0],
                             &weights.residual[block].conv1.biases[0],
                             scratch_mem_);
          network_.emplace_back(std::move(conv1));

          auto conv2 = std::make_unique<FusedWinogradConvSELayer<DataType>>(
              getLastLayer(), kNumFilters, 8, 8, kNumFilters,
              mish_net ? MISH : RELU, true, true, has_se, se_k, use_gemm_ex);
          conv2->LoadWeights(&weights.residual[block].conv2.weights[0],
                             &weights.residual[block].conv2.biases[0],
                             scratch_mem_);
          if (has_se)
            conv2->LoadSEWeights(&weights.residual[block].se.w1[0],
                                 &weights.residual[block].se.b1[0],
                                 &weights.residual[block].se.w2[0],
                                 &weights.residual[block].se.b2[0],
                                 scratch_mem_);
          network_.emplace_back(std::move(conv2));
        }

      } else {
        auto conv1 = std::make_unique<ConvLayer<DataType>>(
            getLastLayer(), kNumFilters, 8, 8, 3, kNumFilters,
            mish_net ? MISH : RELU, true);
        conv1->LoadWeights(&weights.residual[block].conv1.weights[0],
                           &weights.residual[block].conv1.biases[0],
                           scratch_mem_);
        network_.emplace_back(std::move(conv1));

        // Relu and bias of second convolution is handled by SELayer.
        bool useReluAndBias = weights.residual[block].has_se ? false : true;

        auto conv2 = std::make_unique<ConvLayer<DataType>>(
            getLastLayer(), kNumFilters, 8, 8, 3, kNumFilters,
            useReluAndBias ? (mish_net ? MISH : RELU) : NONE, useReluAndBias);
        conv2->LoadWeights(
            &weights.residual[block].conv2.weights[0],
            useReluAndBias ? &weights.residual[block].conv2.biases[0] : nullptr,
            scratch_mem_);
        network_.emplace_back(std::move(conv2));

        if (weights.residual[block].has_se) {
          int numFCOut = (int)weights.residual[block].se.b1.size();
          auto se = std::make_unique<SELayer<DataType>>(
              getLastLayer(), numFCOut, false, mish_net ? MISH : RELU);
          se->LoadWeights(&weights.residual[block].se.w1[0],
                          &weights.residual[block].se.b1[0],
                          &weights.residual[block].se.w2[0],
                          &weights.residual[block].se.b2[0],
                          &weights.residual[block].conv2.biases[0],
                          scratch_mem_);
          network_.emplace_back(std::move(se));
        }
      }
    }

    resi_last_ = getLastLayer();

    // Policy head.
    if (conv_policy_) {
      auto conv1 = std::make_unique<ConvLayer<DataType>>(
          resi_last_, kNumFilters, 8, 8, 3, kNumFilters, mish_net ? MISH : RELU,
          true);
      conv1->LoadWeights(&weights.policy1.weights[0],
                         &weights.policy1.biases[0], scratch_mem_);
      network_.emplace_back(std::move(conv1));

      auto pol_channels = weights.policy.biases.size();

      // No relu
      auto conv2 = std::make_unique<ConvLayer<DataType>>(
          getLastLayer(), pol_channels, 8, 8, 3, kNumFilters, NONE, true);
      conv2->LoadWeights(&weights.policy.weights[0], &weights.policy.biases[0],
                         scratch_mem_);
      network_.emplace_back(std::move(conv2));

      auto policymap = std::make_unique<PolicyMapLayer<DataType>>(
          getLastLayer(), kNumOutputPolicy, 1, 1, 73 * 8 * 8, false);
      policymap->LoadWeights(kConvPolicyMap, scratch_mem_);

      network_.emplace_back(std::move(policymap));
    } else {
      auto convPol = std::make_unique<ConvLayer<DataType>>(
          resi_last_, weights.policy.biases.size(), 8, 8, 1, kNumFilters,
          mish_net ? MISH : RELU, true);
      convPol->LoadWeights(&weights.policy.weights[0],
                           &weights.policy.biases[0], scratch_mem_);
      network_.emplace_back(std::move(convPol));

      auto FCPol = std::make_unique<FCLayer<DataType>>(
          getLastLayer(), weights.ip_pol_b.size(), 1, 1, true, NONE);
      FCPol->LoadWeights(&weights.ip_pol_w[0], &weights.ip_pol_b[0],
                         scratch_mem_);
      network_.emplace_back(std::move(FCPol));
    }
    policy_out_ = getLastLayer();

    // Value head.
    {
      auto convVal = std::make_unique<ConvLayer<DataType>>(
          resi_last_, weights.value.biases.size(), 8, 8, 1, kNumFilters,
          mish_net ? MISH : RELU, true);
      convVal->LoadWeights(&weights.value.weights[0], &weights.value.biases[0],
                           scratch_mem_);
      network_.emplace_back(std::move(convVal));

      auto FCVal1 = std::make_unique<FCLayer<DataType>>(
          getLastLayer(), weights.ip1_val_b.size(), 1, 1, true,
          mish_net ? MISH : RELU);
      FCVal1->LoadWeights(&weights.ip1_val_w[0], &weights.ip1_val_b[0],
                          scratch_mem_);
      network_.emplace_back(std::move(FCVal1));

      wdl_ = file.format().network_format().value() ==
             pblczero::NetworkFormat::VALUE_WDL;
      auto fc2_tanh = !wdl_;

      auto FCVal2 = std::make_unique<FCLayer<DataType>>(
          getLastLayer(), weights.ip2_val_b.size(), 1, 1, true,
          fc2_tanh ? TANH : NONE);
      FCVal2->LoadWeights(&weights.ip2_val_w[0], &weights.ip2_val_b[0],
                          scratch_mem_);
      network_.emplace_back(std::move(FCVal2));
    }
    value_out_ = getLastLayer();

    // Moves left head
    moves_left_ = (file.format().network_format().moves_left() ==
                   pblczero::NetworkFormat::MOVES_LEFT_V1) &&
                  options.GetOrDefault<bool>("mlh", true);
    if (moves_left_) {
      auto convMov = std::make_unique<ConvLayer<DataType>>(
          resi_last_, weights.moves_left.biases.size(), 8, 8, 1, kNumFilters,
          mish_net ? MISH : RELU, true);
      convMov->LoadWeights(&weights.moves_left.weights[0],
                           &weights.moves_left.biases[0], scratch_mem_);
      network_.emplace_back(std::move(convMov));

      auto FCMov1 = std::make_unique<FCLayer<DataType>>(
          getLastLayer(), weights.ip1_mov_b.size(), 1, 1, true,
          mish_net ? MISH : RELU);
      FCMov1->LoadWeights(&weights.ip1_mov_w[0], &weights.ip1_mov_b[0],
                          scratch_mem_);
      network_.emplace_back(std::move(FCMov1));

      auto FCMov2 = std::make_unique<FCLayer<DataType>>(getLastLayer(), 1, 1, 1,
                                                        true, RELU);
      FCMov2->LoadWeights(&weights.ip2_mov_w[0], &weights.ip2_mov_b[0],
                          scratch_mem_);
      network_.emplace_back(std::move(FCMov2));
    }
    moves_left_out_ = getLastLayer();

    // 3. Allocate GPU memory for running the network:
    //    - three buffers of max size are enough (one to hold input, second to
    //      hold output and third to hold skip connection's input).

    // size of input to the network
    size_t maxSize = max_batch_size_ * kNumInputPlanes * 64 * sizeof(DataType);

    // take max size of all layers
    for (auto& layer : network_) {
      maxSize = std::max(maxSize, layer->GetOutputSize(max_batch_size_));
    }

    // when this optimization is enabled, we write transformed outputs to
    // intermediate tensor memory
    if (use_res_block_winograd_fuse_opt_ && transformed_tensor_size > maxSize)
      maxSize = transformed_tensor_size;

    for (auto& mem : tensor_mem_) {
      ReportCUDAErrors(cudaMalloc(&mem, maxSize));
      ReportCUDAErrors(cudaMemset(mem, 0, maxSize));
    }

    cudnnDestroyFilterDescriptor(wDesc);
    cudnnDestroyConvolutionDescriptor(convDesc);
    cudnnDestroyTensorDescriptor(xDesc);

#ifdef DEBUG_RAW_NPS
    CERR << "allocated " << 3 * maxSize
         << " bytes of GPU memory to run the network";
#endif
  }

  void forwardEval(InputsOutputs* io, int batchSize) {
    std::unique_lock<std::mutex> lock(lock_);

#ifdef DEBUG_RAW_NPS
    auto t_start = std::chrono::high_resolution_clock::now();
#endif

    // TODO: consider supporting multi-stream path for cudnn backend too.
    cudaStream_t stream = 0;  // default stream

    // Expand packed planes to full planes.
    uint64_t* ipDataMasks = io->input_masks_mem_gpu_;
    float* ipDataValues = io->input_val_mem_gpu_;

    bool fp16 = std::is_same<half, DataType>::value;
    if (fp16) {
      if (nhwc_)
        expandPlanes_Fp16_NHWC((half*)(tensor_mem_[0]), ipDataMasks,
                               ipDataValues, batchSize * kInputPlanes, stream);
      else
        expandPlanes_Fp16_NCHW((half*)(tensor_mem_[0]), ipDataMasks,
                               ipDataValues, batchSize * kInputPlanes, stream);
    } else {
      expandPlanes_Fp32_NCHW((float*)(tensor_mem_[0]), ipDataMasks,
                             ipDataValues, batchSize * kInputPlanes, stream);
    }

    // debug code example
    // dumpTensor(tensor_mem_[0], 1024, "After expand Planes", fp16);

    float* opPol = io->op_policy_mem_gpu_;
    float* opVal = io->op_value_mem_gpu_;
    float* opMov = io->op_moves_left_mem_gpu_;

    int l = 0;
    // Input.
    network_[l++]->Eval(
        batchSize,
        use_res_block_winograd_fuse_opt_ ? tensor_mem_[1] : tensor_mem_[2],
        tensor_mem_[0], nullptr, scratch_mem_, scratch_size_, cudnn_, cublas_,
        stream);  // input conv

    // Residual block.
    for (int block = 0; block < numBlocks_; block++) {
      if (use_res_block_winograd_fuse_opt_) {
        network_[l++]->Eval(batchSize, tensor_mem_[2], tensor_mem_[1], nullptr,
                            scratch_mem_, scratch_size_, cudnn_, cublas_,
                            stream);  // block
      } else {
        network_[l++]->Eval(batchSize, tensor_mem_[0], tensor_mem_[2], nullptr,
                            scratch_mem_, scratch_size_, cudnn_, cublas_,
                            stream);  // conv1

        if (use_custom_winograd_) {
          network_[l++]->Eval(batchSize, tensor_mem_[2], tensor_mem_[0],
                              tensor_mem_[2], scratch_mem_, scratch_size_,
                              cudnn_, cublas_, stream);  // conv2
        } else {
          // For SE Resnet, skip connection is added after SE (and bias is added
          // as part of SE).
          if (has_se_) {
            network_[l++]->Eval(batchSize, tensor_mem_[1], tensor_mem_[0],
                                nullptr, scratch_mem_, scratch_size_, cudnn_,
                                cublas_, stream);  // conv2
          } else {
            network_[l++]->Eval(batchSize, tensor_mem_[2], tensor_mem_[0],
                                tensor_mem_[2], scratch_mem_, scratch_size_,
                                cudnn_, cublas_, stream);  // conv2
          }

          if (has_se_) {
            network_[l++]->Eval(batchSize, tensor_mem_[2], tensor_mem_[1],
                                tensor_mem_[2], scratch_mem_, scratch_size_,
                                cudnn_, cublas_, stream);  // SE layer
          }
        }
      }
    }

    // Policy head.
    if (conv_policy_) {
      network_[l++]->Eval(batchSize, tensor_mem_[0], tensor_mem_[2], nullptr,
                          scratch_mem_, scratch_size_, cudnn_, cublas_,
                          stream);  // policy conv1

      network_[l++]->Eval(batchSize, tensor_mem_[1], tensor_mem_[0], nullptr,
                          scratch_mem_, scratch_size_, cudnn_, cublas_,
                          stream);  // policy conv2

      if (fp16) {
        network_[l++]->Eval(batchSize, tensor_mem_[0], tensor_mem_[1], nullptr,
                            scratch_mem_, scratch_size_, cudnn_, cublas_,
                            stream);  // policy map layer
        copyTypeConverted(opPol, (half*)(tensor_mem_[0]),
                          batchSize * kNumOutputPolicy,
                          stream);  // POLICY output
      } else {
        network_[l++]->Eval(batchSize, (DataType*)opPol, tensor_mem_[1],
                            nullptr, scratch_mem_, scratch_size_, cudnn_,
                            cublas_,
                            stream);  // policy map layer  // POLICY output
      }
    } else {
      network_[l++]->Eval(batchSize, tensor_mem_[0], tensor_mem_[2], nullptr,
                          scratch_mem_, scratch_size_, cudnn_, cublas_,
                          stream);  // pol conv

      if (fp16) {
        network_[l++]->Eval(batchSize, tensor_mem_[1], tensor_mem_[0], nullptr,
                            scratch_mem_, scratch_size_, cudnn_, cublas_,
                            stream);  // pol FC

        copyTypeConverted(opPol, (half*)(tensor_mem_[1]),
                          batchSize * kNumOutputPolicy, stream);  // POLICY
      } else {
        network_[l++]->Eval(batchSize, (DataType*)opPol, tensor_mem_[0],
                            nullptr, scratch_mem_, scratch_size_, cudnn_,
                            cublas_, stream);  // pol FC  // POLICY
      }
    }

    // Copy policy output from device memory to host memory.
    ReportCUDAErrors(cudaMemcpyAsync(
        io->op_policy_mem_, io->op_policy_mem_gpu_,
        sizeof(float) * kNumOutputPolicy * batchSize, cudaMemcpyDeviceToHost));

    // value head
    network_[l++]->Eval(batchSize, tensor_mem_[0], tensor_mem_[2], nullptr,
                        scratch_mem_, scratch_size_, cudnn_, cublas_,
                        stream);  // value conv

    network_[l++]->Eval(batchSize, tensor_mem_[1], tensor_mem_[0], nullptr,
                        scratch_mem_, scratch_size_, cudnn_, cublas_,
                        stream);  // value FC1

    if (fp16) {
      // TODO: consider fusing the bias-add of FC2 with format conversion.
      network_[l++]->Eval(batchSize, tensor_mem_[0], tensor_mem_[1], nullptr,
                          scratch_mem_, scratch_size_, cudnn_, cublas_,
                          stream);  // value FC2
      copyTypeConverted(opVal, (half*)(tensor_mem_[0]),
                        wdl_ ? 3 * batchSize : batchSize, stream);  // VALUE
    } else {
      network_[l++]->Eval(batchSize, (DataType*)opVal, tensor_mem_[1], nullptr,
                          scratch_mem_, scratch_size_, cudnn_, cublas_,
                          stream);  // value FC2    // VALUE
    }

    if (moves_left_) {
      // Moves left head
      network_[l++]->Eval(batchSize, tensor_mem_[0], tensor_mem_[2], nullptr,
                          scratch_mem_, scratch_size_, cudnn_, cublas_,
                          stream);  // moves conv

      network_[l++]->Eval(batchSize, tensor_mem_[1], tensor_mem_[0], nullptr,
                          scratch_mem_, scratch_size_, cudnn_, cublas_,
                          stream);  // moves FC1

      // Moves left FC2
      if (fp16) {
        // TODO: consider fusing the bias-add of FC2 with format conversion.
        network_[l++]->Eval(batchSize, tensor_mem_[0], tensor_mem_[1], nullptr,
                            scratch_mem_, scratch_size_, cudnn_, cublas_,
                            stream);
        copyTypeConverted(opMov, (half*)(tensor_mem_[0]), batchSize, stream);
      } else {
        network_[l++]->Eval(batchSize, (DataType*)opMov, tensor_mem_[1],
                            nullptr, scratch_mem_, scratch_size_, cudnn_,
                            cublas_, stream);
      }
    }

    ReportCUDAErrors(cudaDeviceSynchronize());
    // The next thread can start using the GPU now.
    lock.unlock();

    if (wdl_) {
      // Value softmax done cpu side.
      for (int i = 0; i < batchSize; i++) {
        float w = io->op_value_mem_[3 * i + 0];
        float d = io->op_value_mem_[3 * i + 1];
        float l = io->op_value_mem_[3 * i + 2];
        float m = std::max({w, d, l});
        w = std::exp(w - m);
        d = std::exp(d - m);
        l = std::exp(l - m);
        float sum = w + d + l;
        w /= sum;
        l /= sum;
        d = 1.0f - w - l;
        io->op_value_mem_[3 * i + 0] = w;
        io->op_value_mem_[3 * i + 1] = d;
        io->op_value_mem_[3 * i + 2] = l;
      }
    }

#ifdef DEBUG_RAW_NPS
    const int reportingCalls = 100;
    static int numCalls = 0;
    static int sumBatchSize = 0;
    static double totalTime = 0;

    sumBatchSize += batchSize;
    numCalls++;

    auto t_end = std::chrono::high_resolution_clock::now();

    double dt = std::chrono::duration<double>(t_end - t_start).count();
    totalTime += dt;
    if (numCalls == reportingCalls) {
      double avgBatchSize = ((double)sumBatchSize) / numCalls;
      double nps = sumBatchSize / totalTime;
      CERR << "Avg batch size: " << avgBatchSize
           << ", NN eval time: " << totalTime << " seconds per " << sumBatchSize
           << " evals. NPS: " << nps;
      sumBatchSize = 0;
      totalTime = 0;
      numCalls = 0;
    }
#endif
  }

  ~CudnnNetwork() {
    for (auto mem : tensor_mem_) {
      if (mem) ReportCUDAErrors(cudaFree(mem));
    }
    if (scratch_mem_) ReportCUDAErrors(cudaFree(scratch_mem_));
    cudnnDestroy(cudnn_);
    cublasDestroy(cublas_);
  }

  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

  std::unique_ptr<NetworkComputation> NewComputation() override {
    // Set correct gpu id for this computation (as it might have been called
    // from a different thread).
    ReportCUDAErrors(cudaSetDevice(gpu_id_));
    return std::make_unique<CudnnNetworkComputation<DataType>>(this, wdl_,
                                                               moves_left_);
  }

  std::unique_ptr<InputsOutputs> GetInputsOutputs() {
    std::lock_guard<std::mutex> lock(inputs_outputs_lock_);
    if (free_inputs_outputs_.empty()) {
      return std::make_unique<InputsOutputs>(max_batch_size_, wdl_,
                                             moves_left_);
    } else {
      std::unique_ptr<InputsOutputs> resource =
          std::move(free_inputs_outputs_.front());
      free_inputs_outputs_.pop_front();
      return resource;
    }
  }

  void ReleaseInputsOutputs(std::unique_ptr<InputsOutputs> resource) {
    std::lock_guard<std::mutex> lock(inputs_outputs_lock_);
    free_inputs_outputs_.push_back(std::move(resource));
  }

  // Apparently nvcc doesn't see constructor invocations through make_unique.
  // This function invokes constructor just to please complier and silence
  // warning. Is never called (but compiler thinks that it could).
  void UglyFunctionToSilenceNvccWarning() { InputsOutputs io(0, false, false); }

 private:
  const NetworkCapabilities capabilities_;
  cudnnHandle_t cudnn_;
  cublasHandle_t cublas_;
  int gpu_id_;
  int max_batch_size_;
  bool wdl_;
  bool moves_left_;

  bool nhwc_;  // do we want to use nhwc layout (fastest with fp16 with tensor
               // cores)

  bool use_custom_winograd_;  // Custom winograd convolution implementation for
                              // convolutions of the residual tower.

  bool use_res_block_winograd_fuse_opt_;  // Fuse operations inside the residual
                                          // tower.

  // Currently only one NN Eval can happen a time (we can fix this if needed
  // by allocating more memory).
  mutable std::mutex lock_;

  int numBlocks_;
  bool has_se_;
  bool conv_policy_;
  std::vector<std::unique_ptr<BaseLayer<DataType>>> network_;
  BaseLayer<DataType>* getLastLayer() { return network_.back().get(); }

  BaseLayer<DataType>* resi_last_;
  BaseLayer<DataType>* policy_out_;
  BaseLayer<DataType>* value_out_;
  BaseLayer<DataType>* moves_left_out_;

  DataType* tensor_mem_[3];
  void* scratch_mem_;
  size_t scratch_size_;

  mutable std::mutex inputs_outputs_lock_;
  std::list<std::unique_ptr<InputsOutputs>> free_inputs_outputs_;

  void showInfo() const {
    int version;
    int ret = cudaRuntimeGetVersion(&version);
    switch (ret) {
      case cudaErrorInitializationError:
        throw Exception("CUDA driver and/or runtime could not be initialized");
      case cudaErrorInsufficientDriver:
        throw Exception("No CUDA driver, or one older than the CUDA library");
      case cudaErrorNoDevice:
        throw Exception("No CUDA-capable devices detected");
    }
    int major = version / 1000;
    int minor = (version - major * 1000) / 10;
    int pl = version - major * 1000 - minor * 10;
    CERR << "CUDA Runtime version: " << major << "." << minor << "." << pl;
    if (version != CUDART_VERSION) {
      major = CUDART_VERSION / 1000;
      minor = (CUDART_VERSION - major * 1000) / 10;
      pl = CUDART_VERSION - major * 1000 - minor * 10;
      CERR << "WARNING: CUDA Runtime version mismatch, was compiled with "
              "version "
           << major << "." << minor << "." << pl;
    }
    version = (int)cudnnGetVersion();
    major = version / 1000;
    minor = (version - major * 1000) / 100;
    pl = version - major * 1000 - minor * 100;
    CERR << "Cudnn version: " << major << "." << minor << "." << pl;
    if (version != CUDNN_VERSION) {
      CERR << "WARNING: CUDNN Runtime version mismatch, was compiled with "
              "version "
           << CUDNN_MAJOR << "." << CUDNN_MINOR << "." << CUDNN_PATCHLEVEL;
    }
    cudaDriverGetVersion(&version);
    major = version / 1000;
    minor = (version - major * 1000) / 10;
    pl = version - major * 1000 - minor * 10;
    CERR << "Latest version of CUDA supported by the driver: " << major << "."
         << minor << "." << pl;
    if (version < CUDART_VERSION) {
      CERR << "WARNING: code was compiled with unsupported CUDA version.";
    }
  }

  void showDeviceInfo(const cudaDeviceProp& deviceProp) const {
    CERR << "GPU: " << deviceProp.name;
    CERR << "GPU memory: " << deviceProp.totalGlobalMem / std::pow(2.0f, 30)
         << " GiB";
    CERR << "GPU clock frequency: " << deviceProp.clockRate / 1e3f << " MHz";
    CERR << "GPU compute capability: " << deviceProp.major << "."
         << deviceProp.minor;

    int version = (int)cudnnGetVersion();
    if (version < 7301 && (deviceProp.major > 7 ||
                           (deviceProp.major == 7 && deviceProp.minor >= 5))) {
      CERR << "WARNING: CUDNN version 7.3.1 or newer is better for this GPU.";
    }
    if (std::is_same<float, DataType>::value && deviceProp.major >= 7) {
      CERR << "WARNING: you will probably get better performance from the "
              "cudnn-fp16 backend.";
    }
  }
};

template <typename DataType>
CudnnNetworkComputation<DataType>::CudnnNetworkComputation(
    CudnnNetwork<DataType>* network, bool wdl, bool moves_left)
    : wdl_(wdl), moves_left_(moves_left), network_(network) {
  batch_size_ = 0;
  inputs_outputs_ = network_->GetInputsOutputs();
}

template <typename DataType>
CudnnNetworkComputation<DataType>::~CudnnNetworkComputation() {
  network_->ReleaseInputsOutputs(std::move(inputs_outputs_));
}

template <typename DataType>
void CudnnNetworkComputation<DataType>::ComputeBlocking() {
  network_->forwardEval(inputs_outputs_.get(), GetBatchSize());
}

template <typename DataType>
std::unique_ptr<Network> MakeCudnnNetwork(const std::optional<WeightsFile>& w,
                                          const OptionsDict& options) {
  if (!w) {
    throw Exception(
        "The cudnn" +
        std::string(std::is_same<half, DataType>::value ? "-fp16" : "") +
        " backend requires a network file.");
  }
  const WeightsFile& weights = *w;
  if (weights.format().network_format().network() !=
          pblczero::NetworkFormat::NETWORK_CLASSICAL_WITH_HEADFORMAT &&
      weights.format().network_format().network() !=
          pblczero::NetworkFormat::NETWORK_SE_WITH_HEADFORMAT) {
    throw Exception("Network format " +
                    pblczero::NetworkFormat::NetworkStructure_Name(
                        weights.format().network_format().network()) +
                    " is not supported by CuDNN backend.");
  }
  if (weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_CLASSICAL &&
      weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_CONVOLUTION) {
    throw Exception("Policy format " +
                    pblczero::NetworkFormat::PolicyFormat_Name(
                        weights.format().network_format().policy()) +
                    " is not supported by CuDNN backend.");
  }
  if (weights.format().network_format().value() !=
          pblczero::NetworkFormat::VALUE_CLASSICAL &&
      weights.format().network_format().value() !=
          pblczero::NetworkFormat::VALUE_WDL) {
    throw Exception("Value format " +
                    pblczero::NetworkFormat::ValueFormat_Name(
                        weights.format().network_format().value()) +
                    " is not supported by CuDNN backend.");
  }
  if (weights.format().network_format().moves_left() !=
          pblczero::NetworkFormat::MOVES_LEFT_NONE &&
      weights.format().network_format().moves_left() !=
          pblczero::NetworkFormat::MOVES_LEFT_V1) {
    throw Exception("Moves left head format " +
                    pblczero::NetworkFormat::MovesLeftFormat_Name(
                        weights.format().network_format().moves_left()) +
                    " is not supported by CuDNN backend.");
  }
  if (weights.format().network_format().default_activation() !=
          pblczero::NetworkFormat::DEFAULT_ACTIVATION_RELU &&
      weights.format().network_format().default_activation() !=
          pblczero::NetworkFormat::DEFAULT_ACTIVATION_MISH) {
    throw Exception(
        "Default activation " +
        pblczero::NetworkFormat::DefaultActivation_Name(
            weights.format().network_format().default_activation()) +
        " is not supported by CuDNN backend.");
  }
  return std::make_unique<CudnnNetwork<DataType>>(weights, options);
}

std::unique_ptr<Network> MakeCudnnNetworkAuto(
    const std::optional<WeightsFile>& weights, const OptionsDict& options) {
  int gpu_id = options.GetOrDefault<int>("gpu", 0);
  cudaDeviceProp deviceProp = {};
  // No error checking here, this will be repeated later.
  cudaGetDeviceProperties(&deviceProp, gpu_id);

  // Check if the GPU supports FP16.
  if (deviceProp.major >= 7 ||
      (deviceProp.major == 6 && deviceProp.minor != 1) ||
      (deviceProp.major == 5 && deviceProp.minor == 3)) {
    CERR << "Switching to [cudnn-fp16]...";
    return MakeCudnnNetwork<half>(weights, options);
  }
  CERR << "Switching to [cudnn]...";
  return MakeCudnnNetwork<float>(weights, options);
}

REGISTER_NETWORK("cudnn-auto", MakeCudnnNetworkAuto, 120)
REGISTER_NETWORK("cudnn", MakeCudnnNetwork<float>, 110)
REGISTER_NETWORK("cudnn-fp16", MakeCudnnNetwork<half>, 105)

}  // namespace lczero

```

`src/neural/cuda/readme.txt`:

```txt
cuda/cudnn backend for lc0. Here is a brief description of various files:

1. network_cudnn.cc -> cpp file containing network, computation, etc stuff related to lc0
2. layers.cc -> cpp files containing layer classes
3. layers.h -> header file for layer classes.
4. kernels.h -> header file for cuda kernels
5. common_kernels.cu -> common kernels (fp32, and fp16 that can work with old GPUs)
6. fp16_kernels.cu -> fp16 specific kernels (not used on other GPUs)
7. cuda_common.h -> header for common cuda stuff like ReportCUDAErrors, etc.
8. readme.txt -> this file

High level overview: network is built of layer objects, layers are either implemented using cudnn/cublas libraries, or custom cuda kernels.

lc0 search -> network_cudnn -> layers -> kernels
```

`src/neural/cuda/winograd_helper.inc`:

```inc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

namespace lczero {
namespace cudnn_backend {

__device__ __forceinline__ float mishActivate(float el) {
  auto e = __expf(el);
  auto n = e * e + 2.0f * e;
  auto d = __fdividef(el, n + 2.0f);
  if (el <= -0.6f) {
    return n * d;
  } else {
    return el - 2.0f * d;
  }
}
__device__ __forceinline__ float activate(float cVal,
                                          ActivationFunction activation) {
  switch (activation) {
    case RELU:
      if (cVal < 0) cVal = 0;
      break;
    case TANH:
      cVal = tanh(cVal);
      break;
    case SIGMOID:
      cVal = 1.0f / (1.0f + __expf(-cVal));
      break;
    case SELU: {
      float alpha = 1.67326324f, scale = 1.05070098f;
      if (cVal > 0)
        cVal = scale * cVal;
      else
        cVal = scale * alpha * (__expf(cVal) - 1.0f);
      break;
    }
    case MISH:
      cVal = mishActivate(cVal);
      break;
  }
  return cVal;
}

template <typename T, int M, int N, int K>
__device__ __forceinline__ void matrixMul_gpu_serial(T* c, const T* a,
                                                     const T* b) {
#pragma unroll
  for (int i = 0; i < M; ++i)
#pragma unroll
    for (int j = 0; j < N; ++j) {
      T S = 0;
#pragma unroll
      for (int k = 0; k < K; ++k) S += a[i * K + k] * b[k * N + j];
      c[i * N + j] = S;
    }
}

template <typename T>
__device__ __forceinline__ void FilterTransform4x4(T* transformed_filter,
                                                   const T* filter) {
  // transform applied to filter (of size 3x3)
  T G[6 * 3] = {1.0f / 4,  0,         0,         -1.0f / 6,  -1.0f / 6,
                -1.0f / 6, -1.0f / 6, 1.0f / 6,  -1.0f / 6,  1.0f / 24,
                1.0f / 12, 1.0f / 6,  1.0f / 24, -1.0f / 12, 1.0f / 6,
                0,         0,         1};

  T Gt[3 * 6] = {1.0f / 4, -1.0f / 6, -1.0f / 6, 1.0f / 24, 1.0f / 24,  0,
                 0,        -1.0f / 6, 1.0f / 6,  1.0f / 12, -1.0f / 12, 0,
                 0,        -1.0f / 6, -1.0f / 6, 1.0f / 6,  1.0f / 6,   1};

  T temp_filter[6 * 3];
  matrixMul_gpu_serial<T, 6, 3, 3>(temp_filter, G, filter);
  matrixMul_gpu_serial<T, 6, 6, 3>(transformed_filter, temp_filter, Gt);
}

template <typename T>
__device__ __forceinline__ void InputTransform4x4(T* transformedInput,
                                                  const T* input) {
  // transform applied to input tile (of size 4x4)
  const T Bt[6 * 6] = {4, 0, -5, 0,  1, 0, 0, -4, -4, 1,  1, 0,
                       0, 4, -4, -1, 1, 0, 0, -2, -1, 2,  1, 0,
                       0, 2, -1, -2, 1, 0, 0, 4,  0,  -5, 0, 1};

  const T B[6 * 6] = {4,  0,  0,  0,  0,  0, 0, -4, 4,  -2, 2,  4,
                      -5, -4, -4, -1, -1, 0, 0, 1,  -1, 2,  -2, -5,
                      1,  1,  1,  1,  1,  0, 0, 0,  0,  0,  0,  1};

  T tempIp1[6 * 6];
  matrixMul_gpu_serial<T, 6, 6, 6>(tempIp1, Bt, input);
  matrixMul_gpu_serial<T, 6, 6, 6>(transformedInput, tempIp1, B);
}

template <typename T>
__device__ __forceinline__ void OutputTransform4x4(T* output,
                                                   const T* transformedOutput) {
  // transform applied to result
  const T At[4 * 6] = {1, 1, 1, 1, 1, 0, 0, 1, -1, 2, -2, 0,
                       0, 1, 1, 4, 4, 0, 0, 1, -1, 8, -8, 1};

  const T A[6 * 4] = {1, 0, 0, 0, 1, 1,  1, 1,  1, -1, 1, -1,
                      1, 2, 4, 8, 1, -2, 4, -8, 0, 0,  0, 1};

  T tempOp[4 * 6];
  matrixMul_gpu_serial<T, 4, 6, 6>(tempOp, At, transformedOutput);
  matrixMul_gpu_serial<T, 4, 4, 6>(output, tempOp, A);
}

#define FILTER_IDX_NCHW(k, c, h, w) ((k)*C * S * R + (c)*S * R + (h)*R + w)
template <typename T>
__global__ void filterTransform_kernel(int K, int C, int elements,
                                       T* transformed_filter, const T* filter) {
  int tid = blockIdx.x * blockDim.x + threadIdx.x;
  if (tid >= elements) return;

  constexpr int S = 3;
  constexpr int R = 3;

  int c = tid % C;
  int k = tid / C;

  T filter_tile[3][3];
  T transformed_tile[6][6];

  // read input from memory
  for (int s = 0; s < S; s++)
    for (int r = 0; r < R; r++) {
      filter_tile[s][r] = filter[FILTER_IDX_NCHW(k, c, s, r)];
    }

  // transform it
  FilterTransform4x4(&(transformed_tile[0][0]), &(filter_tile[0][0]));

  // write to output (output is in HWCK layout)
  for (int i = 0; i < 6; i++)
    for (int j = 0; j < 6; j++) {
      transformed_filter[i * 6 * C * K + j * C * K + c * K + k] =
          transformed_tile[i][j];
    }
}

#define INDEX_NCHW(n, c, h, w) ((n)*C * 8 * 8 + (c)*8 * 8 + (h)*8 + w)
#define INDEX_NHCW(n, c, h, w) ((n)*C * 8 * 8 + (h)*C * 8 + (c)*8 + w)

// index in intermediate/temp tensor
// W, H == 6 here! (6x6 transformed blocks)
// N also includes part of dimension (2x2)
#define GemmN (N * 4)
#define TEMP_INDEX_HWNC(h, w, n, c) \
  ((h)*6 * GemmN * C + (w)*GemmN * C + (n)*C + c)

// 'C' threads per block
// 'N' blocks
// every thread transforms an entire board/plane (8x8 elements)
// - producing 4 x 6x6 elements
template <typename T, bool nhcw>
__global__ void InputTransform_kernel(int N, int C, const T* input, T* output) {
  int c = threadIdx.x;
  int n = blockIdx.x;

  T board[8][8];

  const bool fp16 = std::is_same<half, T>::value;

// read the board (a row at a time for fp16)
#pragma unroll
  for (int y = 0; y < 8; y++) {
    if (nhcw) {
      *((uint4*)(&board[y][0])) = *((uint4*)(&input[INDEX_NHCW(n, c, y, 0)]));
      if (!fp16)
        *((uint4*)(&board[y][4])) = *((uint4*)(&input[INDEX_NHCW(n, c, y, 4)]));
    } else {
      *((uint4*)(&board[y][0])) = *((uint4*)(&input[INDEX_NCHW(n, c, y, 0)]));
      if (!fp16)
        *((uint4*)(&board[y][4])) = *((uint4*)(&input[INDEX_NCHW(n, c, y, 4)]));
    }
  }

  // top-left
  {
    T inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i + 1][j + 1] = board[i][j];

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 0, c)] = inEl[y][x];
  }

  // top-right
  {
    T inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i + 1][j] = board[i][j + 3];

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 1, c)] = inEl[y][x];
  }

  // bottom-left
  {
    T inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i][j + 1] = board[i + 3][j];

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 2, c)] = inEl[y][x];
  }

  // bottom-right
  {
    T inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i][j] = board[i + 3][j + 3];

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 3, c)] = inEl[y][x];
  }
}

#define readw1(row, col) (w1[(row)*se_K + (col)])
#define readw2(row, col) (w2[(row)*2 * C + (col)])

// input is in transformed space (HWNC layout)
// output is NCHW
// 'C' threads per block
// 'N' blocks
// every thread generates an entire board/plane (8x8 elements)
template <typename T, bool use_se, ActivationFunction activation, bool use_bias,
          bool use_skip, bool skipInput_nhcw, bool output_nhcw>
__global__ void OutputTransform_kernel(int N, int C, int se_K, T* output,
                                       const T* input, const T* skip,
                                       const T* bias, const T* w1, const T* b1,
                                       const T* w2, const T* b2) {
  const bool fp16 = std::is_same<half, T>::value;

  int k = threadIdx.x;
  int n = blockIdx.x;

  T board[8][8];
  T b = bias[k];

#pragma unroll
  for (int hStart = 0; hStart < 8; hStart += 4)
#pragma unroll
    for (int wStart = 0; wStart < 8; wStart += 4) {
      //  i) read to per thread registers (for doing output transform)
      int shln = n * 4 + (hStart / 4) * 2 + (wStart / 4);
      T outElTransformed[6][6];
#pragma unroll
      for (int y = 0; y < 6; y++)
#pragma unroll
        for (int x = 0; x < 6; x++)
          outElTransformed[y][x] = input[TEMP_INDEX_HWNC(y, x, shln, k)];

      // ii) transform it
      T outEl[4][4];
      OutputTransform4x4(&outEl[0][0], &outElTransformed[0][0]);

#pragma unroll
      for (int y = 0; y < 4; y++)
#pragma unroll
        for (int x = 0; x < 4; x++) board[hStart + y][wStart + x] = outEl[y][x];
    }

  // Add bias, and compute the average for SE.
  float S = 0;
  float B = 0;

#pragma unroll
  for (int y = 0; y < 8; y++)
#pragma unroll
    for (int x = 0; x < 8; x++) {
      if (use_bias) board[y][x] += b;
      if (use_se) S += (float)board[y][x];
    }

  if (use_se) {
    __shared__ float shared_data[1024];
    float avg = S / 64;
    shared_data[k] = avg;
    __syncthreads();

    // First fully-connected layer for SE
    if (k < se_K) {
      S = 0;
      for (int i = 0; i < C; i++) {
        S += shared_data[i] * float(readw1(i, k));
      }
      S += (float)b1[k];
      S = activate(S, activation);
    }
    __syncthreads();
    if (k < se_K) {
      shared_data[k] = S;
    }
    __syncthreads();

    // Second fully-connected layer for SE
    S = 0;
    for (int i = 0; i < se_K; i++) {
      float val = shared_data[i];
      S += val * float(readw2(i, k));
      B += val * float(readw2(i, k + C));
    }
    S += (float)b2[k];
    B += (float)b2[k + C];

    // Sigmoid (only on the scale part).
    S = 1.0f / (1.0f + exp(-S));
  }

  // Scale/bias, add skip connection, perform relu, and write to output.
  for (int h = 0; h < 8; h++) {
    if (use_se)
#pragma unroll
      for (int w = 0; w < 8; w++) board[h][w] = (T)(float(board[h][w]) * S + B);

    // residual add
    if (use_skip) {
      T skipInp[8];
      if (skipInput_nhcw) {
        *((uint4*)(&skipInp[0])) = *((uint4*)(&skip[INDEX_NHCW(n, k, h, 0)]));
        if (!fp16)
          *((uint4*)(&skipInp[4])) = *((uint4*)(&skip[INDEX_NHCW(n, k, h, 4)]));
      } else {
        *((uint4*)(&skipInp[0])) = *((uint4*)(&skip[INDEX_NCHW(n, k, h, 0)]));
        if (!fp16)
          *((uint4*)(&skipInp[4])) = *((uint4*)(&skip[INDEX_NCHW(n, k, h, 4)]));
      }
#pragma unroll
      for (int w = 0; w < 8; w++) board[h][w] += skipInp[w];
    }

    // relu
    if (activation != NONE) {
#pragma unroll
      for (int w = 0; w < 8; w++)
        board[h][w] = (T)activate((float)board[h][w], activation);
    }

    // Write to output (use 128 bit writes to store one row a time)
    if (output_nhcw) {
      *((uint4*)(&output[INDEX_NHCW(n, k, h, 0)])) = *((uint4*)&board[h][0]);
      if (!fp16)
        *((uint4*)(&output[INDEX_NHCW(n, k, h, 4)])) = *((uint4*)&board[h][4]);
    } else {
      *((uint4*)(&output[INDEX_NCHW(n, k, h, 0)])) = *((uint4*)&board[h][0]);
      if (!fp16)
        *((uint4*)(&output[INDEX_NCHW(n, k, h, 4)])) = *((uint4*)&board[h][4]);
    }
  }
}

// fast reduction for the warp
__device__ __forceinline__ float warpReduce(float x) {
#pragma unroll
  for (int mask = 16; mask > 0; mask >>= 1)
    x += __shfl_xor_sync(0xFFFFFFFF, x, mask);

  return x;
}

// Helper fuction to do vector loads/stores
template <typename T>
__device__ __forceinline__ void copyAs(void* dst, const void* src) {
  *((T*)(dst)) = *((const T*)(src));
}

// input is in transformed space (HWNC layout) --- output of GEMM
// output is also in transformed space (HWNC layout) --- input to GEMM (for next
// layer)
// 'C' threads per block
// 'N' blocks
// every thread generates an entire board/plane (8x8 elements)
template <typename T, ActivationFunction activation, bool use_bias,
          bool use_skip>
__global__ __launch_bounds__(kMaxResBlockFusingChannels, 1)
void OutputTransform_SE_relu_InputTransform_kernel(
        int N, int C, int se_K, T* output, const T* input, T* skip,
        const T* bias, const T* w1, const T* b1, const T* w2, const T* b2) {
  const bool fp16 = std::is_same<half, T>::value;

  int k = threadIdx.x;
  int n = blockIdx.x;

  T board[8][8];
  T b = bias[k];

#pragma unroll
  for (int hStart = 0; hStart < 8; hStart += 4)
#pragma unroll
    for (int wStart = 0; wStart < 8; wStart += 4) {
      //  i) read to per thread registers (for doing output transform)
      int shln = n * 4 + (hStart / 4) * 2 + (wStart / 4);
      T outElTransformed[6][6];
#pragma unroll
      for (int y = 0; y < 6; y++)
#pragma unroll
        for (int x = 0; x < 6; x++)
          outElTransformed[y][x] = input[TEMP_INDEX_HWNC(y, x, shln, k)];

      // ii) transform it
      T outEl[4][4];
      OutputTransform4x4(&outEl[0][0], &outElTransformed[0][0]);

#pragma unroll
      for (int y = 0; y < 4; y++)
#pragma unroll
        for (int x = 0; x < 4; x++) board[hStart + y][wStart + x] = outEl[y][x];
    }

  // Add bias, and compute the average for SE.
  float S = 0;
  float B = 0;

#pragma unroll
  for (int y = 0; y < 8; y++)
#pragma unroll
    for (int x = 0; x < 8; x++) {
      if (use_bias) board[y][x] += b;
      S += (float)board[y][x];
    }

  {
    __shared__ float shared_data[kMaxResBlockFusingChannels];
    float avg = S / 64;
    shared_data[k] = avg;

    int lane = k & 0x1F;
    int warp = k >> 5;
    __syncthreads();

    // First fully-connected layer for SE

    // As se_K << C, we want to loop over se_K instead of C
    // even if it means taking the sum across threads

    __shared__ float shared_sums[kMaxResBlockFusingChannels / 32]
                                [kMaxResBlockFusingSeK];  // per-warp sums

    for (int i = 0; i < se_K; i++) {
      float val = shared_data[k] * float(readw1(k, i));
      val = warpReduce(val);
      if (lane == 0) shared_sums[warp][i] = val;
    }
    __syncthreads();
    if (k < se_K) {
      S = 0;
      for (int i = 0; i < C / 32; i++) S += shared_sums[i][k];

      S += (float)b1[k];
      S = activate(S, activation);
      shared_data[k] = S;
    }

    __syncthreads();

    // Second fully-connected layer for SE
    S = 0;
    for (int i = 0; i < se_K; i++) {
      float val = shared_data[i];
      S += val * float(readw2(i, k));
      B += val * float(readw2(i, k + C));
    }
    S += (float)b2[k];
    B += (float)b2[k + C];

    // Sigmoid (only on the scale part).
    S = 1.0f / (1.0f + exp(-S));
  }

  // Scale/bias, add skip connection, perform relu, and write to output.
  for (int h = 0; h < 8; h++) {
#pragma unroll
    for (int w = 0; w < 8; w++) board[h][w] = (T)(float(board[h][w]) * S + B);

    // residual add
    if (use_skip) {
      T skipInp[8];
      copyAs<uint4>(&skipInp[0], &skip[INDEX_NHCW(n, k, h, 0)]);
      if (!fp16) copyAs<uint4>(&skipInp[4], &skip[INDEX_NHCW(n, k, h, 4)]);
#pragma unroll
      for (int w = 0; w < 8; w++) board[h][w] += skipInp[w];
    }

    // relu
    if (activation != NONE) {
#pragma unroll
      for (int w = 0; w < 8; w++)
        board[h][w] = (T)activate((float)board[h][w], activation);
    }

    // write un-transformed output to 'skip' if required
    if (use_skip) {
      // Write to skip (use 128 bit writes to store one row a time)
      copyAs<uint4>(&skip[INDEX_NHCW(n, k, h, 0)], &board[h][0]);
      if (!fp16) copyAs<uint4>(&skip[INDEX_NHCW(n, k, h, 4)], &board[h][4]);
    }
  }

  // perform input transform

  int c = k;
  // top-left
  {
    T inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i + 1][j + 1] = board[i][j];

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 0, c)] = inEl[y][x];
  }

  // top-right
  {
    T inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i + 1][j] = board[i][j + 3];

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 1, c)] = inEl[y][x];
  }

  // bottom-left
  {
    T inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i][j + 1] = board[i + 3][j];

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 2, c)] = inEl[y][x];
  }

  // bottom-right
  {
    T inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i][j] = board[i + 3][j + 3];

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 3, c)] = inEl[y][x];
  }
}


constexpr int kOpInpTransformBlockSize = 64;
template <typename T, ActivationFunction activation, bool use_bias, bool use_skip>
__global__ __launch_bounds__(kOpInpTransformBlockSize, 4)
void OutputTransform_relu_InputTransform_kernel(int N, int C,
                                                T* output, const T* input,
                                                T* skip, const T* bias) {
  const bool fp16 = std::is_same<half, T>::value;

  int k = threadIdx.x + blockIdx.x * kOpInpTransformBlockSize;
  if (k >= C) return;   // wasted threads (for non-multiple of 64 channel counts)
  int n = blockIdx.y;

  T board[8][8];
  T b = bias[k];

  T skipInp[8][8];
#pragma unroll
  for (int h = 0; h < 8; h++) {
    copyAs<uint4>(&skipInp[h][0], &skip[INDEX_NHCW(n, k, h, 0)]);
    if (!fp16) copyAs<uint4>(&skipInp[h][4], &skip[INDEX_NHCW(n, k, h, 4)]);
  }

#pragma unroll
  for (int hStart = 0; hStart < 8; hStart += 4)
#pragma unroll
    for (int wStart = 0; wStart < 8; wStart += 4) {
      //  i) read to per thread registers (for doing output transform)
      int shln = n * 4 + (hStart / 4) * 2 + (wStart / 4);
      T outElTransformed[6][6];
#pragma unroll
      for (int y = 0; y < 6; y++)
#pragma unroll
        for (int x = 0; x < 6; x++)
          outElTransformed[y][x] = input[TEMP_INDEX_HWNC(y, x, shln, k)];

      // ii) transform it
      T outEl[4][4];
      OutputTransform4x4(&outEl[0][0], &outElTransformed[0][0]);

#pragma unroll
      for (int y = 0; y < 4; y++)
#pragma unroll
        for (int x = 0; x < 4; x++) board[hStart + y][wStart + x] = outEl[y][x];
    }

    // Add bias
#pragma unroll
  for (int y = 0; y < 8; y++)
#pragma unroll
    for (int x = 0; x < 8; x++)
      if (use_bias) board[y][x] += b;


  // Add skip connection, perform relu, and write to output.
  for (int h = 0; h < 8; h++) {
    // residual add
    if (use_skip) {
#pragma unroll
      for (int w = 0; w < 8; w++) board[h][w] += skipInp[h][w];
    }

    // activation
    if (activation != NONE) {
#pragma unroll
      for (int w = 0; w < 8; w++)
        board[h][w] = (T) activate((float)board[h][w], activation);
    }

    // write un-transformed output to 'skip' if required
    if (use_skip) {
      // Write to skip (use 128 bit writes to store one row a time)
      copyAs<uint4>(&skip[INDEX_NHCW(n, k, h, 0)], &board[h][0]);
      if (!fp16) copyAs<uint4>(&skip[INDEX_NHCW(n, k, h, 4)], &board[h][4]);
    }
  }

  // perform input transform

  int c = k;
  // top-left
  {
    T inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i + 1][j + 1] = board[i][j];

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 0, c)] = inEl[y][x];
  }

  // top-right
  {
    T inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i + 1][j] = board[i][j + 3];

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 1, c)] = inEl[y][x];
  }

  // bottom-left
  {
    T inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i][j + 1] = board[i + 3][j];

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 2, c)] = inEl[y][x];
  }

  // bottom-right
  {
    T inEl[6][6] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};

#pragma unroll
    for (int i = 0; i < 5; i++)
#pragma unroll
      for (int j = 0; j < 5; j++) inEl[i][j] = board[i + 3][j + 3];

    InputTransform4x4(&inEl[0][0], &inEl[0][0]);

#pragma unroll
    for (int y = 0; y < 6; y++)
#pragma unroll
      for (int x = 0; x < 6; x++)
        output[TEMP_INDEX_HWNC(y, x, n * 4 + 3, c)] = inEl[y][x];
  }
}


template <typename T>
void FilterTransform(int N, int C, T* transformedFilter, const T* filter) {
  // Each thread processes entire filter block (input 3x3 elements -> output 6x6
  // elements)
  const int kBlockSize = 64;
  const int kBlocks = DivUp(N * C, kBlockSize);

  filterTransform_kernel<<<kBlocks, kBlockSize>>>(N, C, N * C,
                                                  transformedFilter, filter);

  ReportCUDAErrors(cudaGetLastError());
}

template <typename T, bool nhcw>
void InputTransform(int N, int C, T* transformed_input, const T* input,
                    cudaStream_t stream) {
  // Each thread processes entire chess board (input 8x8 elements -> outputs
  // 2x2, 6x6 elements)
  InputTransform_kernel<T, nhcw>
      <<<N, C, 0, stream>>>(N, C, input, transformed_input);

  ReportCUDAErrors(cudaGetLastError());
}

template <typename T, bool use_se, ActivationFunction activation, bool use_bias,
          bool use_skip, bool skipInput_nhcw, bool output_nhcw>
void OutputTransform(int N, int C, int se_K, T* output, const T* input,
                     const T* skip, const T* bias, const T* w1, const T* b1,
                     const T* w2, const T* b2, cudaStream_t stream) {
  // Each thread processes entire chess board
  OutputTransform_kernel<T, use_se, activation, use_bias, use_skip,
                         skipInput_nhcw, output_nhcw><<<N, C, 0, stream>>>(
      N, C, se_K, output, input, skip, bias, w1, b1, w2, b2);
  ReportCUDAErrors(cudaGetLastError());
}

}  // namespace cudnn_backend
}  // namespace lczero

```

`src/neural/decoder.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "neural/decoder.h"

#include "neural/encoder.h"

namespace lczero {

namespace {

BoardSquare SingleSquare(BitBoard input) {
  for (auto sq : input) {
    return sq;
  }
  assert(false);
  return BoardSquare();
}

BitBoard MaskDiffWithMirror(const InputPlane& cur, const InputPlane& prev) {
  auto to_mirror = BitBoard(prev.mask);
  to_mirror.Mirror();
  return BitBoard(cur.mask ^ to_mirror.as_int());
}

BoardSquare OldPosition(const InputPlane& prev, BitBoard mask_diff) {
  auto to_mirror = BitBoard(prev.mask);
  to_mirror.Mirror();
  return SingleSquare(to_mirror & mask_diff);
}

}  // namespace

void PopulateBoard(pblczero::NetworkFormat::InputFormat input_format,
                   InputPlanes planes, ChessBoard* board, int* rule50,
                   int* gameply) {
  auto pawnsOurs = BitBoard(planes[0].mask);
  auto knightsOurs = BitBoard(planes[1].mask);
  auto bishopOurs = BitBoard(planes[2].mask);
  auto rookOurs = BitBoard(planes[3].mask);
  auto queenOurs = BitBoard(planes[4].mask);
  auto kingOurs = BitBoard(planes[5].mask);
  auto pawnsTheirs = BitBoard(planes[6].mask);
  auto knightsTheirs = BitBoard(planes[7].mask);
  auto bishopTheirs = BitBoard(planes[8].mask);
  auto rookTheirs = BitBoard(planes[9].mask);
  auto queenTheirs = BitBoard(planes[10].mask);
  auto kingTheirs = BitBoard(planes[11].mask);
  ChessBoard::Castlings castlings;
  switch (input_format) {
    case pblczero::NetworkFormat::InputFormat::INPUT_CLASSICAL_112_PLANE: {
      if (planes[kAuxPlaneBase + 0].mask != 0) {
        castlings.set_we_can_000();
      }
      if (planes[kAuxPlaneBase + 1].mask != 0) {
        castlings.set_we_can_00();
      }
      if (planes[kAuxPlaneBase + 2].mask != 0) {
        castlings.set_they_can_000();
      }
      if (planes[kAuxPlaneBase + 3].mask != 0) {
        castlings.set_they_can_00();
      }
      break;
    }
    case pblczero::NetworkFormat::INPUT_112_WITH_CASTLING_PLANE:
    case pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION:
    case pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION_HECTOPLIES:
    case pblczero::NetworkFormat::
        INPUT_112_WITH_CANONICALIZATION_HECTOPLIES_ARMAGEDDON:
    case pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION_V2:
    case pblczero::NetworkFormat::
        INPUT_112_WITH_CANONICALIZATION_V2_ARMAGEDDON: {
      auto queenside = 0;
      auto kingside = 7;
      if (planes[kAuxPlaneBase + 0].mask != 0) {
        auto mask = planes[kAuxPlaneBase + 0].mask;
        queenside = GetLowestBit((mask >> 56) | mask);
        if ((mask & 0xFFLL) != 0) {
          castlings.set_we_can_000();
        }
        if (mask >> 56 != 0) {
          castlings.set_they_can_000();
        }
      }
      if (planes[kAuxPlaneBase + 1].mask != 0) {
        auto mask = planes[kAuxPlaneBase + 1].mask;
        kingside = GetLowestBit((mask >> 56) | mask);
        if ((mask & 0xFFLL) != 0) {
          castlings.set_we_can_00();
        }
        if (mask >> 56 != 0) {
          castlings.set_they_can_00();
        }
      }
      castlings.SetRookPositions(queenside, kingside);
      break;
    }

    default:
      throw Exception("Unsupported input plane encoding " +
                      std::to_string(input_format));
  }
  std::string fen;
  // Canonical input has no sense of side to move, so we should simply assume
  // the starting position is always white.
  bool black_to_move =
      !IsCanonicalFormat(input_format) && planes[kAuxPlaneBase + 4].mask != 0;
  if (black_to_move) {
    // Flip to white perspective rather than side to move perspective.
    std::swap(pawnsOurs, pawnsTheirs);
    std::swap(knightsOurs, knightsTheirs);
    std::swap(bishopOurs, bishopTheirs);
    std::swap(rookOurs, rookTheirs);
    std::swap(queenOurs, queenTheirs);
    std::swap(kingOurs, kingTheirs);
    pawnsOurs.Mirror();
    pawnsTheirs.Mirror();
    knightsOurs.Mirror();
    knightsTheirs.Mirror();
    bishopOurs.Mirror();
    bishopTheirs.Mirror();
    rookOurs.Mirror();
    rookTheirs.Mirror();
    queenOurs.Mirror();
    queenTheirs.Mirror();
    kingOurs.Mirror();
    kingTheirs.Mirror();
    castlings.Mirror();
  }
  for (int row = 7; row >= 0; --row) {
    int emptycounter = 0;
    for (int col = 0; col < 8; ++col) {
      char piece = '\0';
      if (pawnsOurs.get(row, col)) {
        piece = 'P';
      } else if (pawnsTheirs.get(row, col)) {
        piece = 'p';
      } else if (knightsOurs.get(row, col)) {
        piece = 'N';
      } else if (knightsTheirs.get(row, col)) {
        piece = 'n';
      } else if (bishopOurs.get(row, col)) {
        piece = 'B';
      } else if (bishopTheirs.get(row, col)) {
        piece = 'b';
      } else if (rookOurs.get(row, col)) {
        piece = 'R';
      } else if (rookTheirs.get(row, col)) {
        piece = 'r';
      } else if (queenOurs.get(row, col)) {
        piece = 'Q';
      } else if (queenTheirs.get(row, col)) {
        piece = 'q';
      } else if (kingOurs.get(row, col)) {
        piece = 'K';
      } else if (kingTheirs.get(row, col)) {
        piece = 'k';
      }
      if (emptycounter > 0 && piece) {
        fen += std::to_string(emptycounter);
        emptycounter = 0;
      }
      if (piece) {
        fen += piece;
      } else {
        emptycounter++;
      }
    }
    if (emptycounter > 0) fen += std::to_string(emptycounter);
    if (row > 0) fen += "/";
  }
  fen += " ";
  fen += black_to_move ? "b" : "w";
  fen += " ";
  fen += castlings.as_string();
  fen += " ";
  if (IsCanonicalFormat(input_format)) {
    // Canonical format helpfully has the en passant details ready for us.
    if (planes[kAuxPlaneBase + 4].mask == 0) {
      fen += "-";
    } else {
      int col = GetLowestBit(planes[kAuxPlaneBase + 4].mask >> 56);
      fen += BoardSquare(5, col).as_string();
    }
  } else {
    auto pawndiff = BitBoard(planes[6].mask ^ planes[kPlanesPerBoard + 6].mask);
    // If no pawns then 2 pawns, history isn't filled properly and we shouldn't
    // try and infer enpassant.
    if (pawndiff.count() == 2 && planes[kPlanesPerBoard + 6].mask != 0) {
      auto from =
          SingleSquare(planes[kPlanesPerBoard + 6].mask & pawndiff.as_int());
      auto to = SingleSquare(planes[6].mask & pawndiff.as_int());
      if (from.col() != to.col() || std::abs(from.row() - to.row()) != 2) {
        fen += "-";
      } else {
        // TODO: Ensure enpassant is legal rather than setting it blindly?
        // Doesn't matter for rescoring use case as only legal moves will be
        // performed afterwards.
        fen +=
            BoardSquare((planes[kAuxPlaneBase + 4].mask != 0) ? 2 : 5, to.col())
                .as_string();
      }
    } else {
      fen += "-";
    }
  }
  fen += " ";
  int rule50plane = (int)planes[kAuxPlaneBase + 5].value;
  if (IsHectopliesFormat(input_format)) {
    rule50plane = (int)(100.0f * planes[kAuxPlaneBase + 5].value);
  }
  fen += std::to_string(rule50plane);
  // Reuse the 50 move rule as gameply since we don't know better.
  fen += " ";
  fen += std::to_string(rule50plane);
  board->SetFromFen(fen, rule50, gameply);
}

Move DecodeMoveFromInput(const InputPlanes& planes, const InputPlanes& prior) {
  auto pawndiff = MaskDiffWithMirror(planes[6], prior[0]);
  auto knightdiff = MaskDiffWithMirror(planes[7], prior[1]);
  auto bishopdiff = MaskDiffWithMirror(planes[8], prior[2]);
  auto rookdiff = MaskDiffWithMirror(planes[9], prior[3]);
  auto queendiff = MaskDiffWithMirror(planes[10], prior[4]);
  // Handle Promotion.
  if (pawndiff.count() == 1) {
    auto from = SingleSquare(pawndiff);
    if (knightdiff.count() == 1) {
      auto to = SingleSquare(knightdiff);
      return Move(from, to, Move::Promotion::Knight);
    }
    if (bishopdiff.count() == 1) {
      auto to = SingleSquare(bishopdiff);
      return Move(from, to, Move::Promotion::Bishop);
    }
    if (rookdiff.count() == 1) {
      auto to = SingleSquare(rookdiff);
      return Move(from, to, Move::Promotion::Rook);
    }
    if (queendiff.count() == 1) {
      auto to = SingleSquare(queendiff);
      return Move(from, to, Move::Promotion::Queen);
    }
    assert(false);
    return Move();
  }
  // check king first as castling moves both king and rook.
  auto kingdiff = MaskDiffWithMirror(planes[11], prior[5]);
  if (kingdiff.count() == 2) {
    if (rookdiff.count() == 2) {
      auto from = OldPosition(prior[5], kingdiff);
      auto to = OldPosition(prior[3], rookdiff);
      return Move(from, to);
    }
    auto from = OldPosition(prior[5], kingdiff);
    auto to = SingleSquare(planes[11].mask & kingdiff.as_int());
    if (std::abs(from.col() - to.col()) > 1) {
      // Chess 960 castling can leave the rook in place, but the king has moved
      // from one side of the rook to the other - thus has gone at least 2
      // squares, which is impossible for a normal king move. Can't work out the
      // rook location from rookdiff since its empty, but it is known given the
      // direction of the king movement and the knowledge that the rook hasn't
      // moved.
      if (from.col() > to.col()) {
        to = BoardSquare(from.row(), to.col() + 1);
      } else {
        to = BoardSquare(from.row(), to.col() - 1);
      }
    }
    return Move(from, to);
  }
  if (queendiff.count() == 2) {
    auto from = OldPosition(prior[4], queendiff);
    auto to = SingleSquare(planes[10].mask & queendiff.as_int());
    return Move(from, to);
  }
  if (rookdiff.count() == 2) {
    auto from = OldPosition(prior[3], rookdiff);
    auto to = SingleSquare(planes[9].mask & rookdiff.as_int());
    // Only one king, so we can simply grab its current location directly.
    auto kingpos = SingleSquare(planes[11].mask);
    if (from.row() == kingpos.row() && to.row() == kingpos.row() &&
        ((from.col() < kingpos.col() && to.col() > kingpos.col()) ||
         (from.col() > kingpos.col() && to.col() < kingpos.col()))) {
      // If the king hasn't moved, this could still be a chess 960 castling move
      // if the rook has passed through the king.
      // Destination of the castling move is where the rook started.
      to = from;
      // And since the king didn't move it forms the start position.
      from = kingpos;
    }
    return Move(from, to);
  }
  if (bishopdiff.count() == 2) {
    auto from = OldPosition(prior[2], bishopdiff);
    auto to = SingleSquare(planes[8].mask & bishopdiff.as_int());
    return Move(from, to);
  }
  if (knightdiff.count() == 2) {
    auto from = OldPosition(prior[1], knightdiff);
    auto to = SingleSquare(planes[7].mask & knightdiff.as_int());
    return Move(from, to);
  }
  if (pawndiff.count() == 2) {
    auto from = OldPosition(prior[0], pawndiff);
    auto to = SingleSquare(planes[6].mask & pawndiff.as_int());
    return Move(from, to);
  }
  assert(false);
  return Move();
}

}  // namespace lczero

```

`src/neural/decoder.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include "chess/position.h"
#include "neural/network.h"
#include "proto/net.pb.h"

namespace lczero {

// Decodes the move that led to current position using the current and previous
// input planes. Move is from the perspective of the current position to move
// player, so will need flipping if it is to be applied to the prior position.
//
// NOTE: Assumes InputPlanes are not transformed. Any canonical transforms must
// have already been reverted.
Move DecodeMoveFromInput(const InputPlanes& planes, const InputPlanes& prev);

// Decodes the current position into a board, rule50 and gameply.
//
// NOTE: Assumes InputPlanes are not transformed, regardless of input_format.
void PopulateBoard(pblczero::NetworkFormat::InputFormat input_format,
                   InputPlanes planes, ChessBoard* board, int* rule50,
                   int* gameply);

}  // namespace lczero
```

`src/neural/dx/MetaCommand.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

namespace lczero {
// Obtained by using EnumerateMetaCommands and EnumerateMetaCommandParameters
// calls. Simplified a bit.

struct TensorDesc {
  uint64_t DataType;
  uint64_t Flags;
  uint64_t DimensionCount;
  uint64_t Size[5];
  uint64_t Stride[5];
  uint64_t StrideAlignment[5];
  uint64_t BaseAlignmentInBytes;
  uint64_t PhysicalSizeInElements;
};

//----------------------------------------------------------------------------------//
// GEMM (Matrix multiply)
//----------------------------------------------------------------------------------//

constexpr GUID GemmGuid = {0x1e52ebab,
                           0x25ba,
                           0x463a,
                           {0xa2, 0x85, 0x0a, 0x78, 0x8e, 0xef, 0x5d, 0x01}};

struct GemmCreateDesc {
  TensorDesc DescA;
  TensorDesc DescB;
  TensorDesc DescC;
  uint64_t cMatrixNull;
  TensorDesc DescOut;

  uint64_t Precision;
  uint64_t TransA;
  uint64_t TransB;
  float Alpha;
  float Beta;

  uint64_t ActivationFunction;
  float ActivationParam1, ActivationParam2;
  uint64_t ActivationIsNull;
  uint64_t BindFlags;
};

struct GemmInitDesc {
  D3D12_GPU_DESCRIPTOR_HANDLE AResource;
  D3D12_GPU_DESCRIPTOR_HANDLE BResource;
  D3D12_GPU_DESCRIPTOR_HANDLE CResource;
  D3D12_GPU_DESCRIPTOR_HANDLE PersistentResource;
  D3D12_GPU_DESCRIPTOR_HANDLE TemporaryResource;
};

struct GemmExecuteDesc {
  D3D12_GPU_DESCRIPTOR_HANDLE AResource;
  D3D12_GPU_DESCRIPTOR_HANDLE BResource;
  D3D12_GPU_DESCRIPTOR_HANDLE CResource;
  D3D12_GPU_DESCRIPTOR_HANDLE OutputResource;

  D3D12_GPU_DESCRIPTOR_HANDLE PersistentResource;
  D3D12_GPU_DESCRIPTOR_HANDLE TemporaryResource;
};

//----------------------------------------------------------------------------------//
// Convolution
//----------------------------------------------------------------------------------//

constexpr GUID ConvGuid = {0x17804d6b,
                           0xebfe,
                           0x426f,
                           {0x88, 0xfc, 0xfe, 0xa7, 0x2e, 0x3f, 0x33, 0x56}};

struct ConvCreateDesc {
  TensorDesc InputDesc;
  TensorDesc FilterDesc;
  TensorDesc BiasDesc;
  uint64_t BiasNull;
  TensorDesc OutputDesc;

  uint64_t Mode;
  uint64_t Direction;
  uint64_t Precision;
  uint64_t Stride[3];
  uint64_t Dilation[3];
  uint64_t StartPadding[3];
  uint64_t EndPadding[3];
  uint64_t DimensionCount;
  uint64_t OutputPadding[5];
  uint64_t GroupCount;
  uint64_t ActivationFunction;
  float ActivationParam1, ActivationParam2;
  uint64_t ActivationIsNull;
  uint64_t BindFlags;
};

struct InitConvDesc {
  D3D12_GPU_DESCRIPTOR_HANDLE InputResource;
  D3D12_GPU_DESCRIPTOR_HANDLE FilterResource;
  D3D12_GPU_DESCRIPTOR_HANDLE BiasResource;
  D3D12_GPU_DESCRIPTOR_HANDLE PersistentResource;
  D3D12_GPU_DESCRIPTOR_HANDLE TemporaryResource;
};

struct ExecuteConvDesc {
  D3D12_GPU_DESCRIPTOR_HANDLE InputResource;
  D3D12_GPU_DESCRIPTOR_HANDLE FilterResource;
  D3D12_GPU_DESCRIPTOR_HANDLE BiasResource;
  D3D12_GPU_DESCRIPTOR_HANDLE OutputResource;
  D3D12_GPU_DESCRIPTOR_HANDLE PersistentResource;
  D3D12_GPU_DESCRIPTOR_HANDLE TemporaryResource;
};

};  // namespace lczero
```

`src/neural/dx/dx_common.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#pragma once

#include <d3d12.h>
#include <dxgi1_6.h>
#include <stdint.h>

#include <cstdint>

#include "d3dx12.h"
#include "utils/fp16_utils.h"

#define DEFAULT_FP16 true

// To debug in case some GPUs can't read from sysmem directly in shader.
//#define COPY_BEFORE_SHADER_READ
// Dump per-layer debug data to find where things go wrong.
//#define DEBUG_DUMP_PER_LAYER_DATA

namespace lczero {

namespace dx_backend {

void DxError(HRESULT status, const char* file, const int& line);
#define ReportDxErrors(status) DxError(status, __FILE__, __LINE__)

struct DXAlloc {
  ID3D12Resource* resource;
  uint32_t offset;
  // Various ways of binding an allocation to shader:
  // 1. RAW/Structured buffer bound as root UAV, use gpu_va directly.
  // 2. Typed buffer UAV bound as 4-component typed format (e.g:
  // R16G16B16A16_FLOAT)
  // 3. Typed buffer UAV bound as single component scalar typed format (e.g:
  // R16_FLOAT)

  uint64_t gpu_va;

  // Handle of UAV created as 4-component vector type.
  D3D12_GPU_DESCRIPTOR_HANDLE desc_handle_vector;

  // Handle of UAV created as scalar type.
  D3D12_GPU_DESCRIPTOR_HANDLE desc_handle_scalar;
};

typedef uint16_t dx_half;

inline int DivUp(int a, int b) { return (a + b - 1) / b; }

}  // namespace dx_backend
}  // namespace lczero

```

`src/neural/dx/layers_dx.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "layers_dx.h"
#include <cassert>
#include <cstring>
#include <vector>
#include "comdef.h"
#include "utils/exception.h"

#include "MetaCommand.h"
#include "network_dx.h"

#include <comdef.h>

namespace lczero {
namespace dx_backend {

// Utility functions used only in this file.
namespace {

static void CopyFloatToHalf(dx_half* out, const float* in, size_t elements) {
  for (int i = 0; i < elements; i++) {
    out[i] = FP32toFP16(in[i]);
  }
}

static void CpuTranspose(float* op, float* ip, size_t rows, size_t cols) {
  for (size_t i = 0; i < rows; i++)
    for (size_t j = 0; j < cols; j++) op[j * rows + i] = ip[i * cols + j];
}


template <int M, int N, int K>
static void MatrixMulCPU(float* c, const float* a, const float* b) {
  for (int i = 0; i < M; ++i)
    for (int j = 0; j < N; ++j) {
      float S = 0;
      for (int k = 0; k < K; ++k)
        S += a[i * K + k] * b[k * N + j];
      c[i * N + j] = S;
    }
}

static void FilterTransform4x4(float* transformed_filter, const float* filter) {
  // transform applied to filter (of size 3x3)
  float G[6 * 3] = { 1.0f / 4,   0,           0,       -1.0f / 6, -1.0f / 6, -1.0f / 6,
                    -1.0f / 6,   1.0f / 6,   -1.0f / 6, 1.0f / 24, 1.0f / 12, 1.0f / 6,
                     1.0f / 24, -1.0f / 12,   1.0f / 6, 0,         0,         1};

  float Gt[3 * 6] = {1.0f / 4, -1.0f / 6, -1.0f / 6,  1.0f / 24,  1.0f / 24,  0,
                     0,        -1.0f / 6,  1.0f / 6,  1.0f / 12, -1.0f / 12,  0,
                     0,        -1.0f / 6, -1.0f / 6,  1.0f / 6,   1.0f / 6,   1};

  float temp_filter[6 * 3];
  MatrixMulCPU<6, 3, 3>(temp_filter, G, filter);
  MatrixMulCPU<6, 6, 3>(transformed_filter, temp_filter, Gt);
}

#define FILTER_IDX_NCHW(k, c, h, w) ((k)*C * S * R + (c)*S * R + (h)*R + w)

// Transform filter for winograd.
// (e.g: for K C H W - 256x256x3x3, filter output is 6x6x256x256 - H W K C)
static void TransformFilterTensor_Winograd4x4(int K, int C, float* transformed_filter,
                                              const float* weight) {
  constexpr int S = 3;
  constexpr int R = 3;

  for (int k = 0; k < K; k++) {
    for (int c = 0; c < C; c++) {
      // 1. read single filter from memory
      float filter_tile[3][3];
      for (int s = 0; s < S; s++)
        for (int r = 0; r < R; r++) {
          filter_tile[s][r] = weight[FILTER_IDX_NCHW(k, c, s, r)];
        }

      // 2. transform it
      float transformed_tile[6][6];
      FilterTransform4x4(&(transformed_tile[0][0]), &(filter_tile[0][0]));

      // 3. write it back to memory (in HWCK layout)
      for (int i = 0; i < 6; i++)
        for (int j = 0; j < 6; j++) {
          transformed_filter[i * 6 * C * K + j * C * K + c * K + k] =
              transformed_tile[i][j];
        }
    }
  }
}


static void GetGemmTensorDesc(TensorDesc* out_desc, int batch_size, int rows,
                              int cols, bool fp16) {
  memset(out_desc, 0, sizeof(TensorDesc));
  out_desc->DimensionCount = 4;
  out_desc->DataType = fp16 ? 1 : 0;

  out_desc->Size[0] = batch_size;
  out_desc->Size[1] = 1;
  out_desc->Size[2] = rows;
  out_desc->Size[3] = cols;

  // row-major by default
  out_desc->Stride[3] = 1;
  out_desc->Stride[2] = cols;
  out_desc->Stride[1] = rows * cols;
  out_desc->Stride[0] = rows * cols;

  for (int i = 0; i < 4; i++) out_desc->StrideAlignment[i] = 1;

  out_desc->BaseAlignmentInBytes = 4096;  // arbitary
  out_desc->PhysicalSizeInElements = batch_size * rows * cols;
}

static void GetConvTensorDesc(TensorDesc* out_desc, int N, int C,
                              int H, int W, bool fp16) {
  memset(out_desc, 0, sizeof(TensorDesc));
  out_desc->DimensionCount = 4;
  out_desc->DataType = fp16 ? 1 : 0;

  out_desc->Size[0] = N;
  out_desc->Size[1] = C;
  out_desc->Size[2] = H;
  out_desc->Size[3] = W;

  // NCHW layout
  out_desc->Stride[3] = 1;
  out_desc->Stride[2] = W;
  out_desc->Stride[1] = H * W;
  out_desc->Stride[0] = C * H * W;

  for (int i = 0; i < 4; i++) out_desc->StrideAlignment[i] = 1;

  out_desc->BaseAlignmentInBytes = 4096;  // arbitary
  out_desc->PhysicalSizeInElements = N * C * H * W;
}

};  // namespace

GemmMetaCommand::GemmMetaCommand(DxContext* dx_context, int rows, int cols, int K,
                                 int gemm_batch, bool fp16, bool a_transpose,
                                 bool b_transpose) {
  memset(scratch_data_persistent_, 0, sizeof(scratch_data_persistent_));
  memset(scratch_data_temporary_, 0, sizeof(scratch_data_temporary_));
  memset(meta_commands_, 0, sizeof(meta_commands_));

  // Note: the way GEMM is used, the 'rows'/M - dimension is a function of
  // batch size. gemm_batch is different and unrelated (either 36 for Winograd,
  // or 1 for other FC layers)
  int num_meta_commands = 1;
  if (rows == 0) {
    // Create metacommands for each 'rows' that is multiple of 8.
    num_meta_commands = kMaxMetacommands;
    rows_known_ = false;
  } else {
    rows_known_ = true;
  }

  for (int i = 0; i < num_meta_commands; i++) {
    int num_rows = rows ? rows : (i + 1) * kMetacommandGranulity;

    GemmCreateDesc createDesc = {};
    GetGemmTensorDesc(&createDesc.DescOut, gemm_batch, num_rows, cols, fp16);
    GetGemmTensorDesc(&createDesc.DescA, gemm_batch, a_transpose ? K : num_rows,
                  a_transpose ? num_rows : K, fp16);
    GetGemmTensorDesc(&createDesc.DescB, gemm_batch, b_transpose ? cols : K,
                  b_transpose ? K : cols, fp16);
    createDesc.cMatrixNull = 1;
    createDesc.ActivationIsNull = 1;
    createDesc.Alpha = 1.0;
    createDesc.Beta = 0.0;
    createDesc.Precision = fp16 ? 1 : 0;  // 0 - fp32, 1 - fp16
    createDesc.TransA = a_transpose;
    createDesc.TransB = b_transpose;

    ID3D12MetaCommand* pMetacommand = nullptr;
    HRESULT hr = dx_context->getDevice()->CreateMetaCommand(
        GemmGuid, 1, &createDesc, sizeof(createDesc),
        IID_PPV_ARGS(&pMetacommand));

    if (hr != S_OK) {
#ifdef DEBUG_DUMP_PER_LAYER_DATA
      printf(
          "\nCan't create GEMM Metacommand for "
          "rows: %d, cols: %d, K: %d, batch: "
          "%d\n",
          num_rows, cols, K, gemm_batch);
#endif
      create_succeeded_ = false;
      return;
    }

    meta_commands_[i] = pMetacommand;

    size_t persistent_size = pMetacommand->GetRequiredParameterResourceSize(
        D3D12_META_COMMAND_PARAMETER_STAGE_EXECUTION, 4);
    size_t temp_size = pMetacommand->GetRequiredParameterResourceSize(
        D3D12_META_COMMAND_PARAMETER_STAGE_EXECUTION, 5);

    if (persistent_size) {
      dx_context->CreateAlloc(persistent_size, D3D12_HEAP_TYPE_DEFAULT,
                            scratch_data_persistent_[i], fp16);
    }

    if (temp_size) {
      dx_context->CreateAlloc(temp_size, D3D12_HEAP_TYPE_DEFAULT,
                            scratch_data_temporary_[i], fp16);
    }

    GemmInitDesc initDesc = {};
    initDesc.PersistentResource = scratch_data_persistent_[i].desc_handle_scalar;
    initDesc.TemporaryResource = scratch_data_temporary_[i].desc_handle_scalar;

    dx_context->getCommandList()->InitializeMetaCommand(
        meta_commands_[i], &initDesc, sizeof(initDesc));
  }

  create_succeeded_ = true;
}

void GemmMetaCommand::PerformGemm(int rows, DXAlloc A, DXAlloc B,
                                  DXAlloc output,
                                  ID3D12GraphicsCommandList4* command_list) {
  if (!create_succeeded_)
    throw Exception("Metacommand not created");

  int index = 0;
  if (!rows_known_) {
    index = DivUp(rows, 8) - 1;
  }

  ID3D12MetaCommand* meta_command = meta_commands_[index];
  DXAlloc& scratch_persistent = scratch_data_persistent_[index];
  DXAlloc& scratch_temporary = scratch_data_temporary_[index];

  GemmExecuteDesc exec_desc = {};
  exec_desc.AResource = A.desc_handle_scalar;
  exec_desc.BResource = B.desc_handle_scalar;
  exec_desc.OutputResource = output.desc_handle_scalar;
  exec_desc.PersistentResource = scratch_persistent.desc_handle_scalar;
  exec_desc.TemporaryResource = scratch_temporary.desc_handle_scalar;

  command_list->ExecuteMetaCommand(meta_command, &exec_desc, sizeof(exec_desc));
}

GemmMetaCommand::~GemmMetaCommand() {
  for (int i = 0; i < kMaxMetacommands; i++) {
    if (scratch_data_temporary_[i].resource)
      scratch_data_temporary_[i].resource->Release();
    if (scratch_data_persistent_[i].resource)
      scratch_data_persistent_[i].resource->Release();
    if (meta_commands_[i]) meta_commands_[i]->Release();
  }
}

ConvMetaCommand::ConvMetaCommand(DxContext* dx_context, int C, int K, int H,
                                 int W, int F, bool relu, bool bias,
                                 bool fp16) {
  memset(scratch_data_persistent_, 0, sizeof(scratch_data_persistent_));
  memset(scratch_data_temporary_, 0, sizeof(scratch_data_temporary_));
  memset(meta_commands_, 0, sizeof(meta_commands_));

  for (int i = 0; i < kMaxMetacommands; i++) {
    int n = (i + 1) * kMetacommandGranulity;

    ConvCreateDesc createDesc = {};
    GetConvTensorDesc(&createDesc.InputDesc, n, C, H, W, fp16);
    GetConvTensorDesc(&createDesc.OutputDesc, n, K, H, W, fp16);
    GetConvTensorDesc(&createDesc.FilterDesc, K, C, F, F, fp16);
    GetConvTensorDesc(&createDesc.BiasDesc, K, 1, 1, 1, fp16);
    createDesc.BiasNull = bias ? 0 : 1;
    createDesc.Mode = 1;  // 1 is for cross-correlation (0 - conv)

    createDesc.Direction = 0;  // forward
    createDesc.DimensionCount = 2;  // 2D conv
    createDesc.Stride[0] = 1;
    createDesc.Stride[1] = 1;
    createDesc.Dilation[0] = 1;
    createDesc.Dilation[1] = 1;

    int pad = (F - 1) / 2;
    createDesc.StartPadding[0] = pad;
    createDesc.StartPadding[1] = pad;
    createDesc.EndPadding[0] = pad;
    createDesc.EndPadding[1] = pad;
    createDesc.GroupCount = 1;
    if (relu) {
      createDesc.ActivationFunction = 9;  // relu (guess?)
      createDesc.ActivationIsNull = 0;
    } else {
      createDesc.ActivationIsNull = 1;
    }
    createDesc.Precision = fp16 ? 1 : 0;  // 0 - fp32, 1 - fp16

    ID3D12MetaCommand* pMetacommand = nullptr;
    HRESULT hr = dx_context->getDevice()->CreateMetaCommand(
        ConvGuid, 1, &createDesc, sizeof(createDesc),
        IID_PPV_ARGS(&pMetacommand));

    if (hr != S_OK) {
#ifdef DEBUG_DUMP_PER_LAYER_DATA
      printf(
          "\nCan't create Conv Metacommand for "
          "N, C, K, H, W, f: %d %d %d %d %d %d\n",
          n, C, K, H, W, F);
#endif
      create_succeeded_ = false;
      return;
    }

    meta_commands_[i] = pMetacommand;

    size_t persistent_size = pMetacommand->GetRequiredParameterResourceSize(
        D3D12_META_COMMAND_PARAMETER_STAGE_EXECUTION, 4);
    size_t temp_size = pMetacommand->GetRequiredParameterResourceSize(
        D3D12_META_COMMAND_PARAMETER_STAGE_EXECUTION, 5);

    if (persistent_size) {
      dx_context->CreateAlloc(persistent_size, D3D12_HEAP_TYPE_DEFAULT,
                            scratch_data_persistent_[i], fp16);
    }

    if (temp_size) {
      dx_context->CreateAlloc(temp_size, D3D12_HEAP_TYPE_DEFAULT,
                            scratch_data_temporary_[i], fp16);
    }

    InitConvDesc initDesc = {};
    initDesc.PersistentResource = scratch_data_persistent_[i].desc_handle_scalar;
    initDesc.TemporaryResource = scratch_data_temporary_[i].desc_handle_scalar;

    dx_context->getCommandList()->InitializeMetaCommand(
        meta_commands_[i], &initDesc, sizeof(initDesc));
  }
  use_bias_ = bias;
  create_succeeded_ = true;
}

void ConvMetaCommand::PerformConv(int batch, DXAlloc input, DXAlloc filter,
                                  DXAlloc bias, DXAlloc output,
                                  ID3D12GraphicsCommandList4* command_list) {
  if (!create_succeeded_) throw Exception("Metacommand not created");

  int index = DivUp(batch, 8) - 1;

  ID3D12MetaCommand* meta_command = meta_commands_[index];
  DXAlloc& scratch_persistent = scratch_data_persistent_[index];
  DXAlloc& scratch_temporary = scratch_data_temporary_[index];

  ExecuteConvDesc exec_desc = {};
  exec_desc.InputResource = input.desc_handle_scalar;
  exec_desc.FilterResource = filter.desc_handle_scalar;
  if (use_bias_)
    exec_desc.BiasResource = bias.desc_handle_scalar;
  exec_desc.OutputResource = output.desc_handle_scalar;
  exec_desc.PersistentResource = scratch_persistent.desc_handle_scalar;
  exec_desc.TemporaryResource = scratch_temporary.desc_handle_scalar;

  command_list->ExecuteMetaCommand(meta_command, &exec_desc, sizeof(exec_desc));
}

ConvMetaCommand::~ConvMetaCommand() {
  for (int i = 0; i < kMaxMetacommands; i++) {
    if (scratch_data_temporary_[i].resource)
      scratch_data_temporary_[i].resource->Release();
    if (scratch_data_persistent_[i].resource)
      scratch_data_persistent_[i].resource->Release();
    if (meta_commands_[i]) meta_commands_[i]->Release();
  }
}


BaseLayer::BaseLayer(int c, int h, int w, BaseLayer* ip, DxContext* dx_context,
                     bool fp16)
    : input_(ip), C(c), H(h), W(w), dx_context_(dx_context), fp16_(fp16) {}

ConvLayer::ConvLayer(bool fp16, GemmMetaCommand* pMetaCommandGemm,
                     ConvMetaCommand* pMetaCommandConv,
                     DxContext* dx_context, BaseLayer* ip, int C, int H, int W,
                     int filter, int Cin, bool bias, bool relu, bool skipAdd,
                     bool se, int se_k)
    : BaseLayer(C, H, W, ip, dx_context, fp16),
      meta_command_gemm_(pMetaCommandGemm),
      meta_command_conv_(pMetaCommandConv),
      c_input_(Cin),
      filter_size_(filter),
      use_relu_(relu),
      use_bias_(bias),
      skip_add_(skipAdd),
      has_se_(se),
      se_k_(se_k),
      weights_(),
      transformed_weights_(),
      biases_(),
      w1_(),
      w2_(),
      b1_(),
      b2_() {
  size_t element_size = fp16 ? sizeof(dx_half) : sizeof(float);
  size_t weight_size = element_size * C * Cin * filter * filter;
  size_t blas_size = element_size * C;

  dx_context->CreateAlloc(weight_size, D3D12_HEAP_TYPE_DEFAULT, weights_, fp16);

  if (filter == 3) {
    // 6x6 transformed filter size, for 3x3 convolution
    dx_context->CreateAlloc(weight_size * 4, D3D12_HEAP_TYPE_DEFAULT,
                          transformed_weights_, fp16);
  }

  if (use_bias_) {
    dx_context->CreateAlloc(blas_size, D3D12_HEAP_TYPE_DEFAULT, biases_, fp16);
  }

  if (has_se_)
  {
    const size_t num_weights1 = C * se_k_;
    const size_t num_weights2 = num_weights1 * 2;
    const size_t num_biases1 = se_k_;
    const size_t num_biases2 = 2 * C;

    const size_t weight_size1 = element_size * num_weights1;
    const size_t weight_size2 = element_size * num_weights2;
    const size_t biases_size1 = element_size * num_biases1;
    const size_t biases_size2 = element_size * num_biases2;

    dx_context->CreateAlloc(weight_size1, D3D12_HEAP_TYPE_DEFAULT, w1_, fp16);
    dx_context->CreateAlloc(weight_size2, D3D12_HEAP_TYPE_DEFAULT, w2_, fp16);
    dx_context->CreateAlloc(biases_size1, D3D12_HEAP_TYPE_DEFAULT, b1_, fp16);
    dx_context->CreateAlloc(biases_size2, D3D12_HEAP_TYPE_DEFAULT, b2_, fp16);
  }

  shader_wrapper_ = dx_context->getShaderWrapper();
}

void ConvLayer::LoadWeights(float* cpu_filter, float* cpu_bias, DxContext* dx_context) {
  int num_weights = c_input_ * C * filter_size_ * filter_size_;
  size_t element_size = fp16_ ? sizeof(dx_half) : sizeof(float);
  size_t weight_size = element_size * num_weights;
  size_t bias_size = element_size * C;

  std::vector<dx_half> temp(num_weights);
  if (fp16_) {
    CopyFloatToHalf(temp.data(), cpu_filter, num_weights);
    dx_context->ScheduleUpload(weights_, temp.data(), weight_size);
  } else {
    dx_context->ScheduleUpload(weights_, cpu_filter, weight_size);
  }

  if (cpu_bias) {
    if (fp16_) {
      CopyFloatToHalf(temp.data(), cpu_bias, C);
      dx_context->ScheduleUpload(biases_, temp.data(), bias_size);
    } else {
      dx_context->ScheduleUpload(biases_, cpu_bias, bias_size);
    }
  }

  if (filter_size_ == 3) {
    std::vector<float> temp_transformed(num_weights * 4);
    TransformFilterTensor_Winograd4x4(C, c_input_, temp_transformed.data(),
                                      cpu_filter);
    if (fp16_) {
      std::vector<dx_half> temp_transformed_half(num_weights * 4);
      CopyFloatToHalf(temp_transformed_half.data(), temp_transformed.data(),
                      num_weights * 4);
      dx_context->ScheduleUpload(transformed_weights_,
                               temp_transformed_half.data(), weight_size * 4);
    } else {
      dx_context->ScheduleUpload(transformed_weights_, temp_transformed.data(),
                               weight_size * 4);
    }
  }
}

void ConvLayer::LoadSEWeights(float* w1, float* b1, float* w2, float* b2) {
  size_t element_size = fp16_ ? sizeof(dx_half) : sizeof(float);
  const size_t num_weights1 = C * se_k_;
  const size_t num_weights2 = num_weights1 * 2;
  const size_t weight_size1 = element_size * num_weights1;
  const size_t weight_size2 = element_size * num_weights2;

  const size_t num_biases1 = se_k_;
  const size_t biases_size1 = element_size * num_biases1;
  const size_t num_biases2 = 2*C;
  const size_t biases_size2 = element_size * num_biases2;

  // The shader uses transposed weight matrices.

  std::vector<float> temp_transposed(num_weights2);
  std::vector<dx_half> temp_half(num_weights2);

  CpuTranspose(temp_transposed.data(), w1, se_k_, C);
  if (fp16_) {
    CopyFloatToHalf(temp_half.data(), temp_transposed.data(), num_weights1);
    dx_context_->ScheduleUpload(w1_, temp_half.data(), weight_size1);
  } else {
    dx_context_->ScheduleUpload(w1_, temp_transposed.data(), weight_size1);
  }

  CpuTranspose(temp_transposed.data(), w2, 2*C, se_k_);
  if (fp16_) {
    CopyFloatToHalf(temp_half.data(), temp_transposed.data(), num_weights2);
    dx_context_->ScheduleUpload(w2_, temp_half.data(), weight_size2);
  } else {
    dx_context_->ScheduleUpload(w2_, temp_transposed.data(), weight_size2);
  }

  if (fp16_) {
    CopyFloatToHalf(temp_half.data(), b1, num_biases1);
    dx_context_->ScheduleUpload(b1_, temp_half.data(), biases_size1);
  } else {
    dx_context_->ScheduleUpload(b1_, b1, biases_size1);
  }

  if (fp16_) {
    CopyFloatToHalf(temp_half.data(), b2, num_biases2);
    dx_context_->ScheduleUpload(b2_, temp_half.data(), biases_size2);
  } else {
    dx_context_->ScheduleUpload(b2_, b2, biases_size2);
  }
}

void ConvLayer::Eval(int N, DXAlloc output, DXAlloc input, DXAlloc input2,
                     DXAlloc scratch, DXAlloc scratch2,
                     ID3D12GraphicsCommandList4* command_list) {

  // Use winograd for filter size of 3, when GEMM metacommand is available,
  // Or when GEMM metacommand isn't available but Convolution metacommand is
  // also not available (compute shader matrix multiply path).
  bool useWinograd =
      (filter_size_ == 3) && 
      ((meta_command_gemm_ && meta_command_gemm_->IsAvailable()) ||
       !meta_command_conv_ || !meta_command_conv_->IsAvailable());

  if (useWinograd) {
    // Need to pad up the input to gemm too (i.e, the transformed Input tensor)!
    // It's in HWNC layout, and 'N'/GemmN needs to be padded up (HW = 6x6)
    // to make it simple, just pad up N to multiple of 2 here (so that gemmN is
    // multiple of 8).
    // TODO: figure out why padding up by 4 is needed (instead of 2!)
    //N = ((N + 1) / 2) * 2;
    N = ((N + 3) / 4) * 4;

    // 1. Input transform (input->scratch)
    shader_wrapper_->InputTransform(command_list, scratch, input, N, c_input_,
                                    fp16_);

    dx_context_->UavBarrier(command_list);

    // 2. Gemm (scratch -> scratch2)
    if (meta_command_gemm_ && meta_command_gemm_->IsAvailable())
      meta_command_gemm_->PerformGemm(N * 4, scratch, transformed_weights_, scratch2,
                                 command_list);
    else
      shader_wrapper_->MatrixMultiply(command_list, scratch2, scratch,
                                      transformed_weights_, N * 4, C, c_input_,
                                      36, fp16_);

    dx_context_->UavBarrier(command_list);

    // 3. Output transform (scratch2 -> output)
    shader_wrapper_->OutputTransform(
        command_list, output, scratch2, input2, biases_, w1_, b1_, w2_, b2_, N,
        C, use_relu_, use_bias_, skip_add_, has_se_, se_k_, fp16_);

  } 
  else if (meta_command_conv_ && meta_command_conv_->IsAvailable()) {
    if (skip_add_ || has_se_)
      meta_command_conv_->PerformConv(N, input, weights_, biases_, scratch,
                                      command_list);
    else
      meta_command_conv_->PerformConv(N, input, weights_, biases_, output,
                                      command_list);
    if (has_se_) {
      dx_context_->UavBarrier(command_list);
      shader_wrapper_->Se(command_list, output, scratch, input2, biases_, w1_,
                          b1_, w2_, b2_, N, C, use_relu_, false, skip_add_,
                          se_k_, fp16_);
    } else if (skip_add_) {
      // Need seperate pass for skip connection addition as Metacommand API 
      // doesn't allow it to be fused with convolution.
      dx_context_->UavBarrier(command_list);
      shader_wrapper_->AddVectors(command_list, output, scratch, input2,
                                  N * C * H * W, N * C * H * W, N * C * H * W,
                                  use_relu_, false, fp16_); 
    }
  }
  else if (filter_size_ == 1) {
    shader_wrapper_->Conv1x1(command_list, output, input, weights_, biases_, N,
                             c_input_, C, use_relu_, use_bias_, fp16_);
  } else {
    throw Exception("Unsupported filter shape for convolution! ");
  }
}

ConvLayer::~ConvLayer() {
  if (weights_.resource) weights_.resource->Release();
  if (biases_.resource) biases_.resource->Release();
  if (transformed_weights_.resource) transformed_weights_.resource->Release();

  if (w1_.resource) w1_.resource->Release();
  if (w2_.resource) w2_.resource->Release();
  if (b1_.resource) b1_.resource->Release();
  if (b2_.resource) b2_.resource->Release();
}

FCLayer::FCLayer(bool fp16, DxContext* dx_context, BaseLayer* ip, int C, int H,
                 int W, bool bias, bool relu, bool tanh)
    : BaseLayer(C, H, W, ip, dx_context, fp16),
      use_bias_(bias),
      use_relu_(relu),
      meta_command_(),
      use_tanh_(tanh) {
  size_t element_size = fp16_ ? sizeof(dx_half) : sizeof(float);
  size_t weight_size =
      element_size * C * H * W * ip->GetC() * ip->GetH() * ip->GetW();
  size_t blas_size = element_size * C * H * W;

  dx_context->CreateAlloc(weight_size, D3D12_HEAP_TYPE_DEFAULT, weights_, fp16);
  if (use_bias_)
    dx_context->CreateAlloc(blas_size, D3D12_HEAP_TYPE_DEFAULT, biases_, fp16);

  shader_wrapper_ = dx_context->getShaderWrapper();

  // Create metacommand object
  int rows = 0;  // batch size
  int cols = C * H * W; // cols of the output matrix
  int K = ip->GetC() * ip->GetH() * ip->GetW();  // cols of input matrix
  // We do Out = A * weight.
  // The weight matrix need to be transpsoed before it can be multiplied.
  // The transpose is done on CPU when loading weights
  meta_command_ = std::make_unique<GemmMetaCommand>(dx_context, rows, cols, K, 1,
                                                    fp16, false, false);
}

void FCLayer::LoadWeights(float* cpuWeight, float* cpuBias,
                          DxContext* dx_context) {
  size_t rows = C * H * W;
  size_t cols = input_->GetC() * input_->GetH() * input_->GetW();
  size_t num_weights =
      rows * cols;

  size_t element_size = fp16_ ? sizeof(dx_half) : sizeof(float);
  size_t weight_size = element_size * num_weights;
  size_t num_biases = C * H * W;
  size_t bias_size = element_size * num_biases;

  std::vector<float> temp_transposed(num_weights);
  CpuTranspose(temp_transposed.data(), cpuWeight, rows, cols);
  std::vector<dx_half> temp(num_weights);
  if (fp16_) {
    CopyFloatToHalf(temp.data(), temp_transposed.data(), num_weights);
    dx_context->ScheduleUpload(weights_, temp.data(), weight_size);
  } else {
    dx_context->ScheduleUpload(weights_, temp_transposed.data(), weight_size);
  }

  if (cpuBias) {
    if (fp16_) {
      CopyFloatToHalf(temp.data(), cpuBias, C);
      dx_context->ScheduleUpload(biases_, temp.data(), bias_size);
    } else {
      dx_context->ScheduleUpload(biases_, cpuBias, bias_size);
    }
  }
}

void FCLayer::Eval(int N, DXAlloc output, DXAlloc input, DXAlloc /*input2*/,
                   DXAlloc /*scratch*/, DXAlloc /*scratch2*/,
                   ID3D12GraphicsCommandList4* command_list) {
  int num_outputs = C * H * W;
  int num_inputs = input_->GetC() * input_->GetH() * input_->GetW();

  if (meta_command_->IsAvailable())
    meta_command_->PerformGemm(N, input, weights_, output, command_list);
  else
    shader_wrapper_->MatrixMultiply(command_list, output, input, weights_,
                                    DivUp(N, 8) * 8, num_outputs, num_inputs, 1,
                                    fp16_);

  if (use_bias_ || use_relu_ || use_tanh_) {
    dx_context_->UavBarrier(command_list);
    shader_wrapper_->AddVectors(command_list, output, output, biases_,
                                N * num_outputs, N * num_outputs, num_outputs, 
                                use_relu_, use_tanh_, fp16_);
  }
}

FCLayer::~FCLayer() {
  if (weights_.resource) weights_.resource->Release();
  if (biases_.resource) biases_.resource->Release();
}


PolicyMapLayer::PolicyMapLayer(bool fp16, DxContext* dx_context, BaseLayer* ip,
                               int C, int H, int W, int usedSize)
    : BaseLayer(C, H, W, ip, dx_context, fp16),
      used_size_(usedSize) {
  size_t weight_size = sizeof(int) * used_size_;
  dx_context->CreateAlloc(weight_size, D3D12_HEAP_TYPE_DEFAULT, weights_, fp16);
}

void PolicyMapLayer::LoadWeights(const short* cpuWeights) {
  // convert from short to int (as HLSL might have trouble reading short)
  std::vector<int> temp(used_size_);
  for (int i = 0; i < used_size_; i++) temp[i] = (int)cpuWeights[i];
  dx_context_->ScheduleUpload(weights_, temp.data(), sizeof(int) * used_size_);
}

void PolicyMapLayer::Eval(int N, DXAlloc output, DXAlloc input, DXAlloc /*input2*/,
                          DXAlloc /*scratch*/, DXAlloc /*scratch2*/,
                          ID3D12GraphicsCommandList4* command_list) {
  int inputSize =
      this->input_->GetC() * this->input_->GetH() * this->input_->GetW();
  int outputSize = this->C * this->H * this->W;
  dx_context_->getShaderWrapper()->PolicyMap(command_list, output, input,
                                             weights_, N, inputSize, outputSize,
                                             used_size_, fp16_);
}

PolicyMapLayer::~PolicyMapLayer() {
  if (weights_.resource) weights_.resource->Release();
}


void DxError(HRESULT status, const char* file, const int& line) {
  if (FAILED(status)) {
    assert(0);
    char message[512];
    _com_error err(status);
    LPCTSTR errMsg = err.ErrorMessage();
    sprintf_s(message, "Dx error: %s (%s:%d) ", errMsg, file, line);
    throw Exception(message);
  }
}

}  // namespace dx_backend
}  // namespace lczero

```

`src/neural/dx/layers_dx.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#pragma once

#include <dxgi.h>

#include <memory>

#include "dx_common.h"
#include "shader_wrapper.h"

namespace lczero {
namespace dx_backend {

class DxContext;
constexpr int kMaxSupportedBatchSize = 256;

// The Layer objects only hold memory for weights, biases, etc
// memory for input and output tensors is provided by caller of Eval.

class BaseLayer {
 public:
  int GetC() const { return C; }
  int GetH() const { return H; }
  int GetW() const { return W; }

  BaseLayer(int c, int h, int w, BaseLayer* ip, DxContext* dx_context,
            bool fp16);
  virtual ~BaseLayer() = default;
  size_t GetOutputSize(int N) const {
    return (fp16_ ? sizeof(dx_half) : sizeof(float)) * N * C * H * W;
  }

  // input2 is optional (skip connection).
  virtual void Eval(int N, DXAlloc output, DXAlloc input, DXAlloc input2,
                    DXAlloc scratch, DXAlloc scratch2,
                    ID3D12GraphicsCommandList4* command_list) = 0;

 protected:
  BaseLayer* input_;
  DxContext* dx_context_;

  bool fp16_;

  // Output tensor dimensions.
  int C;
  int H;
  int W;
};

// Holds Metacommand objects and their scratch space for all allowed batch
// sizes.
class GemmMetaCommand {
 private:
  // Need to create a Metacommand object for each batch size unfortunately!
  // Some hw vendors don't support arbitary sizes anyway, so we create only
  // multiples of 8 in no. of rows (when M is 0).

  static constexpr int kMetacommandGranulity = 8;
  static constexpr int kMaxMetacommands =
      (kMaxSupportedBatchSize * 4) / kMetacommandGranulity;
  ID3D12MetaCommand* meta_commands_[kMaxMetacommands];

  DXAlloc scratch_data_persistent_[kMaxMetacommands];
  DXAlloc scratch_data_temporary_[kMaxMetacommands];

  bool rows_known_;
  bool create_succeeded_;

 public:
  GemmMetaCommand(DxContext* dx_context, int M, int N, int K, int gemm_batch,
                  bool fp16, bool a_transpose, bool b_transpose);
  ~GemmMetaCommand();

  void PerformGemm(int rows, DXAlloc A, DXAlloc B, DXAlloc Output,
                   ID3D12GraphicsCommandList4* command_list);

  bool IsAvailable() { return create_succeeded_; }
};

class ConvMetaCommand {
 private:
  // Metacommand objects for each multiple of 8 batch size
  static constexpr int kMetacommandGranulity = 8;
  static constexpr int kMaxMetacommands =
      kMaxSupportedBatchSize / kMetacommandGranulity;
  ID3D12MetaCommand* meta_commands_[kMaxMetacommands];

  DXAlloc scratch_data_persistent_[kMaxMetacommands];
  DXAlloc scratch_data_temporary_[kMaxMetacommands];
  bool create_succeeded_;
  bool use_bias_;

 public:
  ConvMetaCommand(DxContext* dx_context, int C, int K, int H, int W, int F,
                  bool relu, bool bias, bool fp16);
  ~ConvMetaCommand();

  void PerformConv(int batch, DXAlloc input, DXAlloc filter, DXAlloc bias,
                   DXAlloc output, ID3D12GraphicsCommandList4* command_list);

  bool IsAvailable() { return create_succeeded_; }
};

class ConvLayer : public BaseLayer {
  using BaseLayer::C;
  using BaseLayer::GetC;
  using BaseLayer::GetH;
  using BaseLayer::GetW;
  using BaseLayer::H;
  using BaseLayer::W;

 public:
  ConvLayer(bool fp16, GemmMetaCommand* meta_command_gemm,
            ConvMetaCommand* meta_command_conv, DxContext* dx_context,
            BaseLayer* ip, int C, int H, int W, int size, int Cin, bool bias,
            bool relu, bool skipAdd = false, bool se = false, int se_k = 0);
  ~ConvLayer();

  // returns space in uploadBuffer used for loading weights
  void LoadWeights(float* filter, float* bias, DxContext* dx_context);
  void LoadSEWeights(float* w1, float* b1, float* w2, float* b2);
  void Eval(int N, DXAlloc output, DXAlloc input, DXAlloc input2,
            DXAlloc scratch, DXAlloc scratch2,
            ID3D12GraphicsCommandList4* command_list) override;

 private:
  const int c_input_;
  const int filter_size_;
  const bool use_relu_;
  const bool use_bias_;
  const bool skip_add_;
  const bool has_se_;
  const int se_k_;

  DXAlloc biases_;
  DXAlloc weights_;
  DXAlloc transformed_weights_;  // After winograd transform.

  // Weights and Biases for (optional) SE.
  DXAlloc w1_;
  DXAlloc w2_;
  DXAlloc b1_;
  DXAlloc b2_;

  ShaderWrapper* shader_wrapper_;
  GemmMetaCommand* meta_command_gemm_;
  ConvMetaCommand* meta_command_conv_;
};

class FCLayer : public BaseLayer {
 public:
  FCLayer(bool fp16, DxContext* dx_context, BaseLayer* ip, int C, int H, int W,
          bool bias, bool relu, bool tanh);
  ~FCLayer();

  // returns space in uploadBuffer used for loading weights
  void LoadWeights(float* cpu_weight, float* cpu_bias, DxContext* dx_context);
  void Eval(int N, DXAlloc output, DXAlloc input, DXAlloc input2,
            DXAlloc scratch, DXAlloc scratch2,
            ID3D12GraphicsCommandList4* command_list) override;

 private:
  const bool use_bias_;

  // Only one of the below 2 activation functions should be enabled.
  const bool use_relu_;
  const bool use_tanh_;

  DXAlloc biases_;
  DXAlloc weights_;
  ShaderWrapper* shader_wrapper_;
  std::unique_ptr<GemmMetaCommand> meta_command_;
};

class PolicyMapLayer : public BaseLayer {
 public:
  PolicyMapLayer(bool fp16, DxContext* dx_context, BaseLayer* ip, int C, int H,
                 int W, int used_size);
  ~PolicyMapLayer();
  void LoadWeights(const short* cpu_weights);
  void Eval(int N, DXAlloc output, DXAlloc input, DXAlloc input2,
            DXAlloc scratch, DXAlloc scratch2,
            ID3D12GraphicsCommandList4* command_list) override;

 private:
  const int used_size_;
  DXAlloc weights_;
};

}  // namespace dx_backend
}  // namespace lczero

```

`src/neural/dx/network_dx.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020-2022 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#include "network_dx.h"

#include <algorithm>
#include <cassert>
#include <functional>
#include <list>
#include <memory>
#include <mutex>
#include <vector>

#include "layers_dx.h"
#include "neural/shared/policy_map.h"
#include "shader_wrapper.h"
#include "utils/bititer.h"
#include "utils/exception.h"

namespace lczero {
namespace dx_backend {

uint64_t DxContext::FlushCL(ID3D12GraphicsCommandList4* cl) {
  if (!cl) cl = command_list_;
  cl->Close();
  command_queue_->ExecuteCommandLists(1, (ID3D12CommandList**)&cl);
  command_queue_->Signal(fence_, ++fence_val_);
  return fence_val_;
}

void DxContext::WaitForGpu(uint64_t fence_val) {
  if (!fence_val) fence_val = fence_val_;
  // Wait for commands to finish on GPU.
  // (spinloop has lowest latency, we can try event based signal if CPU
  // overhead becomes a bottleneck).
  while (fence_->GetCompletedValue() < fence_val)
    ;
  upload_scratch_mem_.offset = 0;
}

void DxContext::ResetCL(ID3D12GraphicsCommandList4* cl,
                        ID3D12CommandAllocator* ca, bool reset) {
  if (!cl) cl = command_list_;
  if (!ca) ca = command_allocator_;
  if (reset) {
    ca->Reset();
    cl->Reset(ca, NULL);
  }
  cl->SetDescriptorHeaps(1, &desc_heap_);
}

void DxContext::FlushAndWait() {
  FlushCL();
  WaitForGpu();
  ResetCL();
}

void DxContext::UavBarrier(ID3D12GraphicsCommandList4* command_list) {
  if (!command_list) command_list = command_list_;
  CD3DX12_RESOURCE_BARRIER uav_barrier = CD3DX12_RESOURCE_BARRIER::UAV(nullptr);
  command_list->ResourceBarrier(1, &uav_barrier);
}

void DxContext::DumpFp32(float* buf, int elements) {
  printf("\n");
  for (int i = 0; i < elements; i++) {
    printf("%8.4f ", buf[i]);
    if ((i % 8) == 7) printf("\n");
  }
  printf("\n");
}

void DxContext::CopyTensor(DXAlloc dst, DXAlloc src, int bytes) {
  CD3DX12_RESOURCE_BARRIER barrier;

  barrier = CD3DX12_RESOURCE_BARRIER::Transition(
      src.resource, D3D12_RESOURCE_STATE_UNORDERED_ACCESS,
      D3D12_RESOURCE_STATE_COPY_SOURCE);
  command_list_->ResourceBarrier(1, &barrier);

  barrier = CD3DX12_RESOURCE_BARRIER::Transition(
      dst.resource, D3D12_RESOURCE_STATE_UNORDERED_ACCESS,
      D3D12_RESOURCE_STATE_COPY_DEST);
  command_list_->ResourceBarrier(1, &barrier);

  command_list_->CopyBufferRegion(dst.resource, dst.offset, src.resource,
                                  src.offset, bytes);

  barrier = CD3DX12_RESOURCE_BARRIER::Transition(
      src.resource, D3D12_RESOURCE_STATE_COPY_SOURCE,
      D3D12_RESOURCE_STATE_UNORDERED_ACCESS);
  command_list_->ResourceBarrier(1, &barrier);

  barrier = CD3DX12_RESOURCE_BARRIER::Transition(
      dst.resource, D3D12_RESOURCE_STATE_COPY_DEST,
      D3D12_RESOURCE_STATE_UNORDERED_ACCESS);
  command_list_->ResourceBarrier(1, &barrier);
}

void DxContext::DumpCpuTensor(void* data, int size, bool fp16,
                              bool allnewline) {
  printf("\n");
  float* fp32arr = (float*)data;
  uint16_t* arr = (uint16_t*)data;

  for (int i = 0; i < size; i++) {
    printf("%8.4f ", fp16 ? FP16toFP32(arr[i]) : fp32arr[i]);
    if (allnewline || ((i % 8) == 7)) printf("\n");
  }
  printf("\n");
}

#ifdef DEBUG_DUMP_PER_LAYER_DATA
void DxContext::DumpTensor(const char* message, DXAlloc alloc, int size,
                           bool fp16, bool allnewline) {
  printf("\n%s", message);
  int bytes = size * (fp16 ? sizeof(dx_half) : sizeof(float));
  CD3DX12_RESOURCE_BARRIER barrier;
  barrier = CD3DX12_RESOURCE_BARRIER::Transition(
      alloc.resource, D3D12_RESOURCE_STATE_UNORDERED_ACCESS,
      D3D12_RESOURCE_STATE_COPY_SOURCE);
  command_list_->ResourceBarrier(1, &barrier);

  command_list_->CopyBufferRegion(readback_scratch_mem_.resource,
                                  readback_scratch_mem_.offset, alloc.resource,
                                  alloc.offset, bytes);

  barrier = CD3DX12_RESOURCE_BARRIER::Transition(
      alloc.resource, D3D12_RESOURCE_STATE_COPY_SOURCE,
      D3D12_RESOURCE_STATE_UNORDERED_ACCESS);
  command_list_->ResourceBarrier(1, &barrier);

  FlushAndWait();
  void* cpuPtr;
  readback_scratch_mem_.resource->Map(0, nullptr, &cpuPtr);
  DumpCpuTensor(cpuPtr, size, fp16, allnewline);
  readback_scratch_mem_.resource->Unmap(0, nullptr);
}
#else
void DxContext::DumpTensor(const char*, DXAlloc, int, bool, bool) {}
#endif

DxContext::DxContext(const OptionsDict& options) {
  gpu_id_ = options.GetOrDefault<int>("gpu", 0);

  IDXGIFactory4* pFactory = nullptr;
  IDXGIAdapter* pAdapter = nullptr;
  ReportDxErrors(CreateDXGIFactory2(0, IID_PPV_ARGS(&pFactory)));
  ReportDxErrors(pFactory->EnumAdapters(gpu_id_, &pAdapter));
  pFactory->Release();

  if (!pAdapter) throw Exception("Invalid GPU Id: " + std::to_string(gpu_id_));

  ReportDxErrors(D3D12CreateDevice(pAdapter, D3D_FEATURE_LEVEL_11_0,
                                   IID_PPV_ARGS(&device_)));
  pAdapter->Release();

  D3D12_COMMAND_QUEUE_DESC commandqueueDesc;
  commandqueueDesc.Flags = D3D12_COMMAND_QUEUE_FLAG_NONE;
  commandqueueDesc.NodeMask = 0;
  commandqueueDesc.Priority = 0;
  commandqueueDesc.Type = D3D12_COMMAND_LIST_TYPE_DIRECT;
  ReportDxErrors(device_->CreateCommandQueue(&commandqueueDesc,
                                             IID_PPV_ARGS(&command_queue_)));

  ReportDxErrors(device_->CreateCommandAllocator(
      D3D12_COMMAND_LIST_TYPE_DIRECT, IID_PPV_ARGS(&command_allocator_)));

  ReportDxErrors(device_->CreateCommandList(1, D3D12_COMMAND_LIST_TYPE_DIRECT,
                                            command_allocator_, NULL,
                                            IID_PPV_ARGS(&command_list_)));

  D3D12_DESCRIPTOR_HEAP_DESC heapDesc = {};
  heapDesc.Type = D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV;
  heapDesc.Flags = D3D12_DESCRIPTOR_HEAP_FLAG_SHADER_VISIBLE;
  heapDesc.NumDescriptors = kNumDescHeapSlots;
  ReportDxErrors(
      device_->CreateDescriptorHeap(&heapDesc, IID_PPV_ARGS(&desc_heap_)));

  command_list_->SetDescriptorHeaps(1, &desc_heap_);

  next_slot_in_desc_heap_ = 0;

  fence_val_ = 0ull;
  ReportDxErrors(device_->CreateFence(fence_val_, D3D12_FENCE_FLAG_NONE,
                                      IID_PPV_ARGS(&fence_)));

  shader_wrapper_.Init(device_);

  // Allocate scratch space for uploads and read-back.
  CreateAlloc(kUploadDownloadScratchSize, D3D12_HEAP_TYPE_UPLOAD,
              upload_scratch_mem_, false);
  CreateAlloc(kUploadDownloadScratchSize, D3D12_HEAP_TYPE_READBACK,
              readback_scratch_mem_, false);
}

DxContext::~DxContext() {
  // Make sure nothing is in flight
  FlushAndWait();

  upload_scratch_mem_.resource->Release();
  readback_scratch_mem_.resource->Release();

  shader_wrapper_.Destroy();
  command_list_->Release();
  command_allocator_->Release();
  command_queue_->Release();
  fence_->Release();
  desc_heap_->Release();
  device_->Release();
}

void DxContext::CreateAlloc(size_t size, D3D12_HEAP_TYPE type, DXAlloc& alloc,
                            bool fp16) {
  // some alignment
  int factor = DivUp((int)size, 4);
  size = factor * 4;

  D3D12_HEAP_PROPERTIES heapDesc = {};
  heapDesc.CPUPageProperty = D3D12_CPU_PAGE_PROPERTY_UNKNOWN;
  heapDesc.MemoryPoolPreference = D3D12_MEMORY_POOL_UNKNOWN;
  heapDesc.CreationNodeMask = 1;
  heapDesc.VisibleNodeMask = 1;

  if (type == D3D12_HEAP_TYPE_CUSTOM) {
    // Use custom heap type to allow GPU writing to system memory directly
    heapDesc.MemoryPoolPreference = D3D12_MEMORY_POOL_L0;
    heapDesc.CPUPageProperty = D3D12_CPU_PAGE_PROPERTY_WRITE_BACK;
  }

  heapDesc.Type = type;

  D3D12_RESOURCE_DESC bufferDesc = {};
  bufferDesc.MipLevels = 1;
  bufferDesc.Format = DXGI_FORMAT_UNKNOWN;
  bufferDesc.Height = 1;
  if (type == D3D12_HEAP_TYPE_DEFAULT || type == D3D12_HEAP_TYPE_CUSTOM)
    bufferDesc.Flags = D3D12_RESOURCE_FLAG_ALLOW_UNORDERED_ACCESS;
  bufferDesc.DepthOrArraySize = 1;
  bufferDesc.SampleDesc.Count = 1;
  bufferDesc.SampleDesc.Quality = 0;
  bufferDesc.Dimension = D3D12_RESOURCE_DIMENSION_BUFFER;
  bufferDesc.Layout = D3D12_TEXTURE_LAYOUT_ROW_MAJOR;

  D3D12_RESOURCE_STATES resourceState = D3D12_RESOURCE_STATE_UNORDERED_ACCESS;
  if (type == D3D12_HEAP_TYPE_UPLOAD)
    resourceState = D3D12_RESOURCE_STATE_GENERIC_READ;
  else if (type == D3D12_HEAP_TYPE_READBACK)
    resourceState = D3D12_RESOURCE_STATE_COPY_DEST;

  bufferDesc.Width = size;
  ReportDxErrors(device_->CreateCommittedResource(
      &heapDesc, D3D12_HEAP_FLAG_NONE, &bufferDesc, resourceState, nullptr,
      IID_PPV_ARGS(&alloc.resource)));

  alloc.offset = 0;
  alloc.gpu_va = alloc.resource->GetGPUVirtualAddress();

  // Create desc heap entries for UAV resources.
  if (resourceState == D3D12_RESOURCE_STATE_UNORDERED_ACCESS) {
    int handleIncrementSize = device_->GetDescriptorHandleIncrementSize(
        D3D12_DESCRIPTOR_HEAP_TYPE_CBV_SRV_UAV);

    size_t element_size = fp16 ? sizeof(dx_half) : sizeof(float);

    // Scalar UAV.
    {
      int slot = next_slot_in_desc_heap_++;

      CD3DX12_CPU_DESCRIPTOR_HANDLE cpuDescHandle(
          desc_heap_->GetCPUDescriptorHandleForHeapStart(), slot,
          handleIncrementSize);

      CD3DX12_GPU_DESCRIPTOR_HANDLE gpuDescHandle(
          desc_heap_->GetGPUDescriptorHandleForHeapStart(), slot,
          handleIncrementSize);

      D3D12_UNORDERED_ACCESS_VIEW_DESC uavDesc = {};
      uavDesc.ViewDimension = D3D12_UAV_DIMENSION_BUFFER;
      uavDesc.Format = fp16 ? DXGI_FORMAT_R16_FLOAT : DXGI_FORMAT_R32_FLOAT;
      uavDesc.Buffer.FirstElement = 0;
      uavDesc.Buffer.NumElements = (UINT)(size / element_size);

      device_->CreateUnorderedAccessView(alloc.resource, nullptr, &uavDesc,
                                         cpuDescHandle);

      alloc.desc_handle_scalar = gpuDescHandle;
    }

    // 4-component vector UAV.
    {
      int slot = next_slot_in_desc_heap_++;

      CD3DX12_CPU_DESCRIPTOR_HANDLE cpuDescHandle(
          desc_heap_->GetCPUDescriptorHandleForHeapStart(), slot,
          handleIncrementSize);

      CD3DX12_GPU_DESCRIPTOR_HANDLE gpuDescHandle(
          desc_heap_->GetGPUDescriptorHandleForHeapStart(), slot,
          handleIncrementSize);

      D3D12_UNORDERED_ACCESS_VIEW_DESC uavDesc = {};
      uavDesc.ViewDimension = D3D12_UAV_DIMENSION_BUFFER;
      uavDesc.Format = fp16 ? DXGI_FORMAT_R16G16B16A16_FLOAT
                            : DXGI_FORMAT_R32G32B32A32_FLOAT;
      uavDesc.Buffer.FirstElement = 0;
      uavDesc.Buffer.NumElements = (UINT)(size / (4 * element_size));

      device_->CreateUnorderedAccessView(alloc.resource, nullptr, &uavDesc,
                                         cpuDescHandle);

      alloc.desc_handle_vector = gpuDescHandle;
    }
  }
}

void DxContext::ScheduleUpload(DXAlloc alloc, const void* data, size_t size) {
  // Make sure enough space is available in the upload scratch buffer
  assert(size <= kUploadDownloadScratchSize);
  if (upload_scratch_mem_.offset + size > kUploadDownloadScratchSize)
    FlushAndWait();

  uint8_t* temp;
  upload_scratch_mem_.resource->Map(0, nullptr, (void**)&temp);

  dx_half* cpuPtr = (dx_half*)(temp + upload_scratch_mem_.offset);
  memcpy(cpuPtr, data, size);

  CD3DX12_RESOURCE_BARRIER barrier;

  barrier = CD3DX12_RESOURCE_BARRIER::Transition(
      alloc.resource, D3D12_RESOURCE_STATE_UNORDERED_ACCESS,
      D3D12_RESOURCE_STATE_COPY_DEST);
  command_list_->ResourceBarrier(1, &barrier);

  command_list_->CopyBufferRegion(alloc.resource, alloc.offset,
                                  upload_scratch_mem_.resource,
                                  upload_scratch_mem_.offset, size);

  barrier = CD3DX12_RESOURCE_BARRIER::Transition(
      alloc.resource, D3D12_RESOURCE_STATE_COPY_DEST,
      D3D12_RESOURCE_STATE_UNORDERED_ACCESS);
  command_list_->ResourceBarrier(1, &barrier);

  upload_scratch_mem_.resource->Unmap(0, nullptr);

  // reset at flush and wait
  upload_scratch_mem_.offset += (uint32_t)size;
}

DxNetwork::DxNetwork(const WeightsFile& file, const OptionsDict& options)
    : dx_context_(options),
      capabilities_{file.format().network_format().input(),
                    file.format().network_format().moves_left()} {
  LegacyWeights weights(file.weights());

  has_conv_policy_ = file.format().network_format().policy() ==
                     pblczero::NetworkFormat::POLICY_CONVOLUTION;
  max_batch_size_ =
      options.GetOrDefault<int>("max_batch", kMaxSupportedBatchSize);

  // Default is fp16, to use fp32: --backend-opts=fp16=false.
  fp16_ = options.GetOrDefault<bool>("fp16", DEFAULT_FP16);

  // Default is to attempt using Winograd algorithm for Convolutions using GEMM
  // Metacommand first, if not available - attempt using Convolution Metacommand
  // directly (whatever algorithm HW vendor is providing), and if neither is
  // available use winograd algorithm with our own GEMM compute shader.
  // The below backend options can be used to override this for testing.
  bool enable_gemm_metacommand =
      options.GetOrDefault<bool>("enable-gemm-metacommand", true);
  bool enable_conv_metacommand =
      options.GetOrDefault<bool>("enable-conv-metacommand", true);

  const int kNumFilters = (int)weights.input.biases.size();

  num_blocks_ = (int)weights.residual.size();
  has_se_ = weights.residual[0].has_se;
  int pol_channels = (int)weights.policy.biases.size();

  // Build the network, and copy the weights to GPU memory.

  // Unique GEMMs for winograd required by the network.
  if (enable_gemm_metacommand) {
    input_conv_gemm_metacommand_ = std::make_unique<GemmMetaCommand>(
        &dx_context_, 0, kNumFilters, kInputPlanes, 36, fp16_, false, false);

    residual_block_gemm_metacommand_ = std::make_unique<GemmMetaCommand>(
        &dx_context_, 0, kNumFilters, kNumFilters, 36, fp16_, false, false);

    if (has_conv_policy_) {
      policy_conv_gemm_metacommand_ = std::make_unique<GemmMetaCommand>(
          &dx_context_, 0, pol_channels, kNumFilters, 36, fp16_, false, false);
    }
  }

  // Unique Conv metacommands required by the network.
  if (enable_conv_metacommand) {
    // Create only if we were not able to create GEMM metacommands for some
    // reason 3x3, 112 channels -> kNumFilters channels, relu, bias.
    if (!input_conv_gemm_metacommand_ ||
        !input_conv_gemm_metacommand_->IsAvailable())
      input_conv_metacommand_ = std::make_unique<ConvMetaCommand>(
          &dx_context_, kInputPlanes, kNumFilters, 8, 8, 3, true, true, fp16_);

    if (!residual_block_gemm_metacommand_ ||
        !residual_block_gemm_metacommand_->IsAvailable()) {
      // 3x3, kNumFilters channels -> kNumFilters channels, relu, bias.
      resi_block_conv_1_metacommand_ = std::make_unique<ConvMetaCommand>(
          &dx_context_, kNumFilters, kNumFilters, 8, 8, 3, true, true, fp16_);

      // 3x3, kNumFilters channels -> kNumFilters channels, no relu
      // relu needs to be done after SE and/or skip connection add.
      resi_block_conv_2_metacommand_ = std::make_unique<ConvMetaCommand>(
          &dx_context_, kNumFilters, kNumFilters, 8, 8, 3, false, true, fp16_);
    }

    if (has_conv_policy_ && (!policy_conv_gemm_metacommand_ ||
                             !policy_conv_gemm_metacommand_->IsAvailable()))
      policy_conv_metacommand_ = std::make_unique<ConvMetaCommand>(
          &dx_context_, kNumFilters, pol_channels, 8, 8, 3, false, true, fp16_);
  }

  // input
  {
    auto inputConv = std::make_unique<ConvLayer>(
        fp16_, input_conv_gemm_metacommand_.get(),
        input_conv_metacommand_.get(), &dx_context_, nullptr, kNumFilters, 8, 8,
        3, kInputPlanes, true, true);

    inputConv->LoadWeights(&weights.input.weights[0], &weights.input.biases[0],
                           &dx_context_);

    network_.emplace_back(std::move(inputConv));
  }

  // residual block
  for (size_t block = 0; block < weights.residual.size(); block++) {
    auto conv1 = std::make_unique<ConvLayer>(
        fp16_, residual_block_gemm_metacommand_.get(),
        resi_block_conv_1_metacommand_.get(), &dx_context_, getLastLayer(),
        kNumFilters, 8, 8, 3, kNumFilters, true, true);

    conv1->LoadWeights(&weights.residual[block].conv1.weights[0],
                       &weights.residual[block].conv1.biases[0], &dx_context_);

    network_.emplace_back(std::move(conv1));

    int se_k = 0;
    if (has_se_) se_k = (int)weights.residual[block].se.b1.size();

    auto conv2 = std::make_unique<ConvLayer>(
        fp16_, residual_block_gemm_metacommand_.get(),
        resi_block_conv_2_metacommand_.get(), &dx_context_, getLastLayer(),
        kNumFilters, 8, 8, 3, kNumFilters, true, true, true, has_se_, se_k);

    conv2->LoadWeights(&weights.residual[block].conv2.weights[0],
                       &weights.residual[block].conv2.biases[0], &dx_context_);

    if (has_se_) {
      conv2->LoadSEWeights(
          &weights.residual[block].se.w1[0], &weights.residual[block].se.b1[0],
          &weights.residual[block].se.w2[0], &weights.residual[block].se.b2[0]);
    }
    network_.emplace_back(std::move(conv2));
  }

  BaseLayer* resi_last = getLastLayer();

  // policy head
  if (has_conv_policy_) {
    // conv1 is same as residual block convolution.
    auto conv1 = std::make_unique<ConvLayer>(
        fp16_, residual_block_gemm_metacommand_.get(),
        resi_block_conv_1_metacommand_.get(), &dx_context_, getLastLayer(),
        kNumFilters, 8, 8, 3, kNumFilters, true, true);
    conv1->LoadWeights(&weights.policy1.weights[0], &weights.policy1.biases[0],
                       &dx_context_);
    network_.emplace_back(std::move(conv1));

    // conv2 has different no. of output filters (pol_channels). No relu.
    auto conv2 = std::make_unique<ConvLayer>(
        fp16_, policy_conv_gemm_metacommand_.get(),
        policy_conv_metacommand_.get(), &dx_context_, getLastLayer(),
        pol_channels, 8, 8, 3, kNumFilters, true, false);

    conv2->LoadWeights(&weights.policy.weights[0], &weights.policy.biases[0],
                       &dx_context_);
    network_.emplace_back(std::move(conv2));

    // Policy map layer
    auto policyMap =
        std::make_unique<PolicyMapLayer>(fp16_, &dx_context_, getLastLayer(),
                                         kNumOutputPolicy, 1, 1, 73 * 8 * 8);
    policyMap->LoadWeights(kConvPolicyMap);
    network_.emplace_back(std::move(policyMap));

  } else {
    // 1x1 convolution, pol_channels output filters
    auto convPol = std::make_unique<ConvLayer>(
        fp16_, nullptr, nullptr, &dx_context_, getLastLayer(), pol_channels, 8,
        8, 1, kNumFilters, true, true);
    convPol->LoadWeights(&weights.policy.weights[0], &weights.policy.biases[0],
                         &dx_context_);
    network_.emplace_back(std::move(convPol));

    // FC with bias, no activation
    // pad up kNumOutputPolicy to be a multiple of 8
    assert(weights.ip_pol_b.size() == kNumOutputPolicy);
    auto FCPol = std::make_unique<FCLayer>(fp16_, &dx_context_, getLastLayer(),
                                           kNumOutputPolicyPadded8, 1, 1, true,
                                           false, false);
    // Copy weights to temp space which is padded in size.
    std::vector<float> tempBias(kNumOutputPolicyPadded8);
    std::vector<float> tempWeight(kNumOutputPolicyPadded8 *
                                  weights.ip_pol_w.size() / kNumOutputPolicy);
    memcpy(tempBias.data(), weights.ip_pol_b.data(),
           weights.ip_pol_b.size() * sizeof(float));
    memcpy(tempWeight.data(), weights.ip_pol_w.data(),
           weights.ip_pol_w.size() * sizeof(float));

    FCPol->LoadWeights(tempWeight.data(), tempBias.data(), &dx_context_);
    network_.emplace_back(std::move(FCPol));
  }

  // value head
  {
    int val_channels = (int)weights.value.biases.size();

    // 1x1 convolution, val_channels output filters
    auto convVal = std::make_unique<ConvLayer>(
        fp16_, nullptr, nullptr, &dx_context_, resi_last, val_channels, 8, 8, 1,
        kNumFilters, true, true);
    convVal->LoadWeights(&weights.value.weights[0], &weights.value.biases[0],
                         &dx_context_);
    network_.emplace_back(std::move(convVal));

    // Bias and relu activation.
    auto FCVal1 = std::make_unique<FCLayer>(fp16_, &dx_context_, getLastLayer(),
                                            (int)weights.ip1_val_b.size(), 1, 1,
                                            true, true, false);
    FCVal1->LoadWeights(&weights.ip1_val_w[0], &weights.ip1_val_b[0],
                        &dx_context_);
    network_.emplace_back(std::move(FCVal1));

    has_wdl_ = file.format().network_format().value() ==
               pblczero::NetworkFormat::VALUE_WDL;

    // Fully connected layer with Bias.
    // tanh activation for non wdl nets, no activation for wdl.
    auto fc2_tanh = !has_wdl_;
    auto FCVal2 = std::make_unique<FCLayer>(fp16_, &dx_context_, getLastLayer(),
                                            kNumOutputValuePadded8, 1, 1, true,
                                            false, fc2_tanh);
    // Pad up the weights
    std::vector<float> tempBias(kNumOutputValuePadded8);
    std::vector<float> tempWeight(kNumOutputValuePadded8 *
                                  weights.ip2_val_w.size() /
                                  weights.ip2_val_b.size());
    memcpy(tempBias.data(), weights.ip2_val_b.data(),
           weights.ip2_val_b.size() * sizeof(float));
    memcpy(tempWeight.data(), weights.ip2_val_w.data(),
           weights.ip2_val_w.size() * sizeof(float));
    FCVal2->LoadWeights(tempWeight.data(), tempBias.data(), &dx_context_);
    network_.emplace_back(std::move(FCVal2));
  }

  // Moves left head
  moves_left_ = (file.format().network_format().moves_left() ==
                 pblczero::NetworkFormat::MOVES_LEFT_V1) &&
                options.GetOrDefault<bool>("mlh", true);
  if (moves_left_) {
    // 1x1 convolution, moves_left biases output filters
    auto convMov = std::make_unique<ConvLayer>(
        fp16_, nullptr, nullptr, &dx_context_, resi_last,
        weights.moves_left.biases.size(), 8, 8, 1, kNumFilters, true, true);
    convMov->LoadWeights(&weights.moves_left.weights[0],
                         &weights.moves_left.biases[0], &dx_context_);
    network_.emplace_back(std::move(convMov));

    // Bias and relu activation.
    auto FCMov1 = std::make_unique<FCLayer>(fp16_, &dx_context_, getLastLayer(),
                                            (int)weights.ip1_mov_b.size(), 1, 1,
                                            true, true, false);
    FCMov1->LoadWeights(&weights.ip1_mov_w[0], &weights.ip1_mov_b[0],
                        &dx_context_);
    network_.emplace_back(std::move(FCMov1));

    // Fully connected layer with Bias and relu.
    auto FCMov2 = std::make_unique<FCLayer>(fp16_, &dx_context_, getLastLayer(),
                                            kNumOutputMovesLeftPadded8, 1, 1,
                                            true, true, false);
    // Pad up the weights
    std::vector<float> tempBias(kNumOutputMovesLeftPadded8);
    std::vector<float> tempWeight(kNumOutputMovesLeftPadded8 *
                                  weights.ip2_mov_w.size() /
                                  weights.ip2_mov_b.size());
    memcpy(tempBias.data(), weights.ip2_mov_b.data(),
           weights.ip2_mov_b.size() * sizeof(float));
    memcpy(tempWeight.data(), weights.ip2_mov_w.data(),
           weights.ip2_mov_w.size() * sizeof(float));
    FCMov2->LoadWeights(tempWeight.data(), tempBias.data(), &dx_context_);
    network_.emplace_back(std::move(FCMov2));
  }

  dx_context_.FlushAndWait();

  // Allocate GPU memory for running the network
  // 4 buffers of max size are enough:
  //   * one to hold input,
  //   * second to hold output
  //   * third to hold skip connection's input
  //   * and fourth to act as scratch space needed by some layers.
  size_t max_size = resi_last->GetOutputSize(max_batch_size_);

  // Winograd transformed inputs/outputs need more space.
  // Every 4x4 block of input/output is transfored to 6x6 block.
  max_size *= (size_t)ceil(36.0 / 16.0);

  for (auto& mem : tensor_mem_) {
    dx_context_.CreateAlloc(max_size, D3D12_HEAP_TYPE_DEFAULT, mem, fp16_);
  }
}

void DxNetwork::Eval(InputsOutputsDx* io, int batch_size) {
  if (batch_size > kMaxSupportedBatchSize)
    throw Exception("Unsupported batch size: " + std::to_string(batch_size));

#ifdef DEBUG_DUMP_PER_LAYER_DATA
  lock_.lock();
  ID3D12GraphicsCommandList4* cl = dx_context_.getCommandList();
#else
  ID3D12GraphicsCommandList4* cl = io->command_list_;
  dx_context_.ResetCL(cl, io->command_allocator_, io->needs_reset_);
#endif

// Expand packed board representation into full planes.

#ifdef COPY_BEFORE_SHADER_READ
  // First copy from upload heap to scratch mem
  CD3DX12_RESOURCE_BARRIER barrier;

  barrier = CD3DX12_RESOURCE_BARRIER::Transition(
      tensor_mem_[1].resource, D3D12_RESOURCE_STATE_UNORDERED_ACCESS,
      D3D12_RESOURCE_STATE_COPY_DEST);
  cl->ResourceBarrier(1, &barrier);

  barrier = CD3DX12_RESOURCE_BARRIER::Transition(
      tensor_mem_[2].resource, D3D12_RESOURCE_STATE_UNORDERED_ACCESS,
      D3D12_RESOURCE_STATE_COPY_DEST);
  cl->ResourceBarrier(1, &barrier);

  cl->CopyBufferRegion(tensor_mem_[1].resource, 0,
                       io->input_masks_mem_gpu_.resource, 0,
                       sizeof(uint64_t) * batch_size * kInputPlanes);
  cl->CopyBufferRegion(tensor_mem_[2].resource, 0,
                       io->input_val_mem_gpu_.resource, 0,
                       sizeof(float) * batch_size * kInputPlanes);

  barrier = CD3DX12_RESOURCE_BARRIER::Transition(
      tensor_mem_[1].resource, D3D12_RESOURCE_STATE_COPY_DEST,
      D3D12_RESOURCE_STATE_UNORDERED_ACCESS);
  cl->ResourceBarrier(1, &barrier);

  barrier = CD3DX12_RESOURCE_BARRIER::Transition(
      tensor_mem_[2].resource, D3D12_RESOURCE_STATE_COPY_DEST,
      D3D12_RESOURCE_STATE_UNORDERED_ACCESS);
  cl->ResourceBarrier(1, &barrier);

  dx_context_.UavBarrier(cl);

  dx_context_.getShaderWrapper()->ExpandPlanes(
      cl, tensor_mem_[0], tensor_mem_[1], tensor_mem_[2], batch_size, fp16_);

#else
  dx_context_.getShaderWrapper()->ExpandPlanes(
      cl, tensor_mem_[0], io->input_masks_mem_gpu_, io->input_val_mem_gpu_,
      batch_size, fp16_);
#endif

  dx_context_.UavBarrier(cl);

  // Debug logging (not compiled by default)
  dx_context_.DumpTensor("After expand planes", tensor_mem_[0], 1024, fp16_);

  int l = 0;

  //-----------------------------------///---------------------------------------
  // Input Conv
  network_[l++]->Eval(batch_size, tensor_mem_[2], tensor_mem_[0], DXAlloc(),
                      tensor_mem_[1], tensor_mem_[3], cl);
  dx_context_.UavBarrier(cl);

  dx_context_.DumpTensor("After input conv", tensor_mem_[2], 1024, fp16_);

  //-----------------------------------///---------------------------------------

  // Residual tower.
  for (int block = 0; block < num_blocks_; block++) {
    // conv1
    network_[l++]->Eval(batch_size, tensor_mem_[0], tensor_mem_[2], DXAlloc(),
                        tensor_mem_[1], tensor_mem_[3], cl);
    dx_context_.UavBarrier(cl);

    // conv2
    network_[l++]->Eval(batch_size, tensor_mem_[2], tensor_mem_[0],
                        tensor_mem_[2], tensor_mem_[1], tensor_mem_[3], cl);
    dx_context_.UavBarrier(cl);
  }

  dx_context_.DumpTensor("After Residual tower", tensor_mem_[2], 1024, fp16_);

  //-----------------------------------///---------------------------------------

  // Policy head.
  if (has_conv_policy_) {
    // Policy conv1.
    network_[l++]->Eval(batch_size, tensor_mem_[0], tensor_mem_[2], DXAlloc(),
                        tensor_mem_[1], tensor_mem_[3], cl);
    dx_context_.UavBarrier(cl);

    dx_context_.DumpTensor("After policy conv1", tensor_mem_[0], 1024, fp16_);

    // Policy conv2
    network_[l++]->Eval(batch_size, tensor_mem_[1], tensor_mem_[0], DXAlloc(),
                        tensor_mem_[1], tensor_mem_[3], cl);

    dx_context_.UavBarrier(cl);

    dx_context_.DumpTensor("After policy conv2", tensor_mem_[1], 1024, fp16_);

    // Policy Map layer  (writes directly to system memory).
    network_[l++]->Eval(batch_size, io->op_policy_mem_gpu_, tensor_mem_[1],
                        DXAlloc(), DXAlloc(), DXAlloc(), cl);

    // Output of policy map layer is always FP32.
    dx_context_.DumpTensor("After policy map", io->op_policy_mem_gpu_, 1024,
                           false);

  } else {
    // Policy conv.
    network_[l++]->Eval(batch_size, tensor_mem_[0], tensor_mem_[2], DXAlloc(),
                        tensor_mem_[1], tensor_mem_[3], cl);
    dx_context_.UavBarrier(cl);

    // Policy FC (writes directly to system memory).
    network_[l++]->Eval(batch_size, io->op_policy_mem_gpu_, tensor_mem_[0],
                        DXAlloc(), tensor_mem_[1], tensor_mem_[3], cl);
  }

  //-----------------------------------///---------------------------------------

  // Value head.

  // Value conv.
  network_[l++]->Eval(batch_size, tensor_mem_[0], tensor_mem_[2], DXAlloc(),
                      tensor_mem_[1], tensor_mem_[3], cl);
  dx_context_.UavBarrier(cl);

  dx_context_.DumpTensor("After value conv", tensor_mem_[0], 1024, fp16_);

  // value FC1.
  network_[l++]->Eval(batch_size, tensor_mem_[1], tensor_mem_[0], DXAlloc(),
                      DXAlloc(), DXAlloc(), cl);
  dx_context_.UavBarrier(cl);

  dx_context_.DumpTensor("After value fc1", tensor_mem_[1], 128, fp16_);

  // value FC2.
  network_[l++]->Eval(batch_size, io->op_value_mem_gpu_, tensor_mem_[1],
                      DXAlloc(), DXAlloc(), DXAlloc(), cl);

  dx_context_.DumpTensor("After value fc2", io->op_value_mem_gpu_, 8, fp16_);

  //-----------------------------------///---------------------------------------

  // Moves left head.
  if (moves_left_) {
    // Moves left conv.
    network_[l++]->Eval(batch_size, tensor_mem_[0], tensor_mem_[2], DXAlloc(),
                        tensor_mem_[1], tensor_mem_[3], cl);
    dx_context_.UavBarrier(cl);

    dx_context_.DumpTensor("After moves left conv", tensor_mem_[0], 1024,
                           fp16_);

    // Moves left FC1.
    network_[l++]->Eval(batch_size, tensor_mem_[1], tensor_mem_[0], DXAlloc(),
                        DXAlloc(), DXAlloc(), cl);
    dx_context_.UavBarrier(cl);

    dx_context_.DumpTensor("After moves left fc1", tensor_mem_[1], 512, fp16_);

    // Moves left FC2.
    network_[l++]->Eval(batch_size, io->op_moves_left_mem_gpu_, tensor_mem_[1],
                        DXAlloc(), DXAlloc(), DXAlloc(), cl);

    dx_context_.DumpTensor("After moves left fc2", io->op_moves_left_mem_gpu_,
                           8, fp16_);
  }

//-----------------------------------///---------------------------------------
#ifdef DEBUG_DUMP_PER_LAYER_DATA
  dx_context_.FlushAndWait();
  lock_.unlock();
#else
  // TODO: Get rid of this lock once we move the Command Queue also to
  // InputsOutputs structure This isn't a bottleneck anyway (for CPU side perf).
  // The hope is that we will get some GPU side parallelism with multiple async
  // compute queues.
  lock_.lock();
  uint64_t fence = dx_context_.FlushCL(cl);
  lock_.unlock();

  dx_context_.WaitForGpu(fence);
  io->needs_reset_ = true;
#endif

  // Do some simple post-processing operations on CPU:
  // - un-padding of policy and value heads.
  // - value head softmax (for wdl enabled nets)
  // We do them outside the lock to get some more parallelism.
  int val_vector_size = has_wdl_ ? 3 : 1;
  if (fp16_) {
    // Policy:
    // Un-pad policy output, and convert to fp32.
    if (!has_conv_policy_) {
      dx_half* padded_pol_fp16 = (dx_half*)io->op_policy_mem_;
      for (int n = 0; n < batch_size; n++)
        for (int i = 0; i < kNumOutputPolicy; i++)
          io->op_policy_mem_final_[n * kNumOutputPolicy + i] =
              FP16toFP32(padded_pol_fp16[n * kNumOutputPolicyPadded8 + i]);
    }
    // Value:
    // Un-pad value output, converting it to fp32.
    dx_half* padded_val_fp16 = (dx_half*)io->op_value_mem_;
    for (int n = 0; n < batch_size; n++)
      for (int i = 0; i < val_vector_size; i++)
        io->op_value_mem_final_[n * val_vector_size + i] =
            FP16toFP32(padded_val_fp16[n * kNumOutputValuePadded8 + i]);
    if (moves_left_) {
      // Moves left:
      // Un-pad moves left output, converting it to fp32.
      dx_half* padded_moves_left_fp16 = (dx_half*)io->op_moves_left_mem_;
      for (int n = 0; n < batch_size; n++)
        io->op_moves_left_mem_final_[n] =
            FP16toFP32(padded_moves_left_fp16[n * kNumOutputMovesLeftPadded8]);
    }
  } else {
    // Policy:
    // Un-pad policy output.
    if (!has_conv_policy_) {
      for (int i = 0; i < batch_size; i++)
        memcpy(io->op_policy_mem_final_ + kNumOutputPolicy * i,
               io->op_policy_mem_ + kNumOutputPolicyPadded8 * i,
               kNumOutputPolicy * sizeof(float));
    }

    // Value:
    // Un-pad value output.
    for (int i = 0; i < batch_size; i++)
      memcpy(io->op_value_mem_final_ + val_vector_size * i,
             io->op_value_mem_ + kNumOutputValuePadded8 * i,
             val_vector_size * sizeof(float));
    if (moves_left_) {
      // Moves left:
      // Un-pad moves left output.
      for (int i = 0; i < batch_size; i++)
        memcpy(io->op_moves_left_mem_final_ + i,
               io->op_moves_left_mem_ + kNumOutputMovesLeftPadded8 * i,
               sizeof(float));
    }
  }

  // Softmax on value head for wdl enabled networks.
  if (has_wdl_) {
    for (int i = 0; i < batch_size; i++) {
      float w_val = io->op_value_mem_final_[i * 3 + 0];
      float d_val = io->op_value_mem_final_[i * 3 + 1];
      float l_val = io->op_value_mem_final_[i * 3 + 2];
      float max_val = std::max({w_val, d_val, l_val});
      w_val = exp(w_val - max_val);
      d_val = exp(d_val - max_val);
      l_val = exp(l_val - max_val);
      float S = w_val + d_val + l_val;
      w_val /= S;
      d_val /= S;
      l_val /= S;

      io->op_value_mem_final_[i * 3 + 0] = w_val;
      io->op_value_mem_final_[i * 3 + 1] = d_val;
      io->op_value_mem_final_[i * 3 + 2] = l_val;
    }
  }
}

DxNetwork::~DxNetwork() {
  dx_context_.FlushAndWait();
  // Free memory and destroy all dx objects.
  for (auto mem : tensor_mem_) {
    mem.resource->Release();
  }
}

std::unique_ptr<NetworkComputation> DxNetwork::NewComputation() {
  return std::make_unique<DxNetworkComputation>(this, has_wdl_, moves_left_);
}

std::unique_ptr<InputsOutputsDx> DxNetwork::GetInputsOutputs() {
  std::lock_guard<std::mutex> lock(inputs_outputs_lock_);
  if (free_inputs_outputs_.empty()) {
    return std::make_unique<InputsOutputsDx>(max_batch_size_, &dx_context_,
                                             has_wdl_, moves_left_,
                                             has_conv_policy_, fp16_);
  } else {
    std::unique_ptr<InputsOutputsDx> resource =
        std::move(free_inputs_outputs_.front());
    free_inputs_outputs_.pop_front();
    return resource;
  }
}

void DxNetwork::ReleaseInputsOutputs(
    std::unique_ptr<InputsOutputsDx> resource) {
  std::lock_guard<std::mutex> lock(inputs_outputs_lock_);
  free_inputs_outputs_.push_back(std::move(resource));
}

DxNetworkComputation::DxNetworkComputation(DxNetwork* network, bool wdl,
                                           bool moves_left)
    : network_(network), wdl_(wdl), moves_left_(moves_left) {
  batch_size_ = 0;
  inputs_outputs_ = network_->GetInputsOutputs();
}

DxNetworkComputation::~DxNetworkComputation() {
  network_->ReleaseInputsOutputs(std::move(inputs_outputs_));
}

void DxNetworkComputation::AddInput(InputPlanes&& input) {
  auto iter_mask =
      &inputs_outputs_->input_masks_mem_[batch_size_ * kInputPlanes];
  auto iter_val = &inputs_outputs_->input_val_mem_[batch_size_ * kInputPlanes];

  int i = 0;
  for (const auto& plane : input) {
    iter_mask[i] = plane.mask;
    iter_val[i] = plane.value;
    i++;
  }

  batch_size_++;
}

void DxNetworkComputation::ComputeBlocking() {
  network_->Eval(inputs_outputs_.get(), GetBatchSize());
}

InputsOutputsDx::InputsOutputsDx(int maxBatchSize, DxContext* dx_context,
                                 bool wdl, bool moves_left, bool policy_map,
                                 bool fp16)
    : uses_policy_map_(policy_map),
      needs_reset_(false),
      moves_left_(moves_left) {
  // CPU accesses on Default heap doesn't work.
  // GPU accesses on Upload heap works.
  dx_context->CreateAlloc(maxBatchSize * kInputPlanes * sizeof(uint64_t),
                          D3D12_HEAP_TYPE_UPLOAD /*D3D12_HEAP_TYPE_DEFAULT*/,
                          input_masks_mem_gpu_, fp16);

  dx_context->CreateAlloc(maxBatchSize * kInputPlanes * sizeof(float),
                          D3D12_HEAP_TYPE_UPLOAD /*D3D12_HEAP_TYPE_DEFAULT*/,
                          input_val_mem_gpu_, fp16);

  // CUSTOM heap created to have GPU directly write to system memory
  dx_context->CreateAlloc(
      maxBatchSize * kNumOutputPolicyPadded8 * sizeof(float),
      D3D12_HEAP_TYPE_CUSTOM, op_policy_mem_gpu_, fp16);

  dx_context->CreateAlloc(maxBatchSize * kNumOutputValuePadded8 * sizeof(float),
                          D3D12_HEAP_TYPE_CUSTOM, op_value_mem_gpu_, fp16);

  if (moves_left) {
    dx_context->CreateAlloc(
        maxBatchSize * kNumOutputMovesLeftPadded8 * sizeof(float),
        D3D12_HEAP_TYPE_CUSTOM, op_moves_left_mem_gpu_, fp16);
  }

  ReportDxErrors(input_masks_mem_gpu_.resource->Map(0, nullptr,
                                                    (void**)&input_masks_mem_));

  ReportDxErrors(
      input_val_mem_gpu_.resource->Map(0, nullptr, (void**)&input_val_mem_));

  ReportDxErrors(
      op_policy_mem_gpu_.resource->Map(0, nullptr, (void**)&op_policy_mem_));

  ReportDxErrors(
      op_value_mem_gpu_.resource->Map(0, nullptr, (void**)&op_value_mem_));

  if (moves_left) {
    ReportDxErrors(op_moves_left_mem_gpu_.resource->Map(
        0, nullptr, (void**)&op_moves_left_mem_));
  }

  // When policy map is enabled, GPU writes directly to the final policy output.
  if (uses_policy_map_)
    op_policy_mem_final_ = op_policy_mem_;
  else
    op_policy_mem_final_ = new float[maxBatchSize * kNumOutputPolicy];
  op_value_mem_final_ = new float[maxBatchSize * (wdl ? 3 : 1)];
  if (moves_left) op_moves_left_mem_final_ = new float[maxBatchSize];

  ReportDxErrors(dx_context->getDevice()->CreateCommandAllocator(
      D3D12_COMMAND_LIST_TYPE_DIRECT, IID_PPV_ARGS(&command_allocator_)));

  ReportDxErrors(dx_context->getDevice()->CreateCommandList(
      1, D3D12_COMMAND_LIST_TYPE_DIRECT, command_allocator_, NULL,
      IID_PPV_ARGS(&command_list_)));
}

InputsOutputsDx::~InputsOutputsDx() {
  input_masks_mem_gpu_.resource->Unmap(0, nullptr);
  input_val_mem_gpu_.resource->Unmap(0, nullptr);
  op_policy_mem_gpu_.resource->Unmap(0, nullptr);
  op_value_mem_gpu_.resource->Unmap(0, nullptr);
  if (moves_left_) {
    op_moves_left_mem_gpu_.resource->Unmap(0, nullptr);
  }

  input_masks_mem_gpu_.resource->Release();
  input_val_mem_gpu_.resource->Release();
  op_policy_mem_gpu_.resource->Release();
  op_value_mem_gpu_.resource->Release();
  if (moves_left_) {
    op_moves_left_mem_gpu_.resource->Release();
  }

  command_allocator_->Release();
  command_list_->Release();

  if (!uses_policy_map_) delete[] op_policy_mem_final_;
  delete[] op_value_mem_final_;
  if (moves_left_) delete[] op_moves_left_mem_final_;
}

std::unique_ptr<Network> MakeDxNetwork(const std::optional<WeightsFile>& w,
                                       const OptionsDict& options) {
  if (!w) {
    throw Exception("The dx12 backend requires a network file.");
  }
  const WeightsFile& weights = *w;
  if (weights.format().network_format().network() !=
          pblczero::NetworkFormat::NETWORK_CLASSICAL_WITH_HEADFORMAT &&
      weights.format().network_format().network() !=
          pblczero::NetworkFormat::NETWORK_SE_WITH_HEADFORMAT) {
    throw Exception("Network format " +
                    pblczero::NetworkFormat::NetworkStructure_Name(
                        weights.format().network_format().network()) +
                    " is not supported by the DX12 backend.");
  }
  if (weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_CLASSICAL &&
      weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_CONVOLUTION) {
    throw Exception("Policy format " +
                    pblczero::NetworkFormat::PolicyFormat_Name(
                        weights.format().network_format().policy()) +
                    " is not supported by the DX12 backend.");
  }
  if (weights.format().network_format().value() !=
          pblczero::NetworkFormat::VALUE_CLASSICAL &&
      weights.format().network_format().value() !=
          pblczero::NetworkFormat::VALUE_WDL) {
    throw Exception("Value format " +
                    pblczero::NetworkFormat::ValueFormat_Name(
                        weights.format().network_format().value()) +
                    " is not supported by the DX12 backend.");
  }
  if (weights.format().network_format().default_activation() !=
      pblczero::NetworkFormat::DEFAULT_ACTIVATION_RELU) {
    throw Exception(
        "Default activation " +
        pblczero::NetworkFormat::DefaultActivation_Name(
            weights.format().network_format().default_activation()) +
        " is not supported by the DX12 backend.");
  }
  return std::make_unique<DxNetwork>(weights, options);
}

REGISTER_NETWORK("dx12", MakeDxNetwork, 120)

}  // namespace dx_backend
}  // namespace lczero

```

`src/neural/dx/network_dx.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#pragma once

#include "dx_common.h"
#include "layers_dx.h"
#include "neural/factory.h"
#include "neural/network_legacy.h"

// TODO: Consider refactoring common part of this backend's code and cudnn
// backend into some base class(es).

namespace lczero {
namespace dx_backend {

class DxNetwork;

static constexpr int kNumOutputPolicy = 1858;

// Padding needed because on some HW (e.g: NV) fp16 requires gemm matrix
// dimensions to be multiples of 8
static constexpr int kNumOutputPolicyPadded8 =
    ((kNumOutputPolicy - 1) / 8 + 1) * 8;

// Normally 3 when using wdl, and 1 without
static constexpr int kNumOutputValuePadded8 = 8;

static constexpr int kNumOutputMovesLeftPadded8 = 8;

struct InputsOutputsDx {
  InputsOutputsDx(int maxBatchSize, DxContext* dx_context, bool wdl,
                  bool moves_left, bool conv_policy, bool fp16);
  ~InputsOutputsDx();

  // Wanted to put these in default heap (video memory, mapped to support CPU
  // writes too).
  //  - but this isn't supported by DX12 API!
  // So right now we have it in upload ueap (system memory mapped for both CPU
  // and GPU).
  DXAlloc input_masks_mem_gpu_;
  DXAlloc input_val_mem_gpu_;

  // In readback heap (system memory mapped for both CPU and GPU).
  DXAlloc op_policy_mem_gpu_;
  DXAlloc op_value_mem_gpu_;
  DXAlloc op_moves_left_mem_gpu_;

  // CPU pointers of the above allocations.
  uint64_t* input_masks_mem_;
  float* input_val_mem_;
  float* op_policy_mem_;
  float* op_value_mem_;
  float* op_moves_left_mem_;

  // Separate copy, un-padded and always in fp32
  float* op_policy_mem_final_;
  float* op_value_mem_final_;
  float* op_moves_left_mem_final_;

  // For recording GPU commands.
  ID3D12GraphicsCommandList4* command_list_;
  ID3D12CommandAllocator* command_allocator_;

  // Always need to reset command list / allocator after first time.
  bool needs_reset_;

  const bool uses_policy_map_;
  const bool moves_left_;
};

class DxNetworkComputation : public NetworkComputation {
 public:
  DxNetworkComputation(DxNetwork* network, bool wdl, bool moves_left);
  ~DxNetworkComputation();

  void AddInput(InputPlanes&& input) override;

  void ComputeBlocking() override;

  int GetBatchSize() const override { return batch_size_; }

  float GetQVal(int sample) const override {
    if (wdl_) {
      auto w = inputs_outputs_->op_value_mem_final_[3 * sample + 0];
      auto l = inputs_outputs_->op_value_mem_final_[3 * sample + 2];
      return w - l;
    } else {
      return inputs_outputs_->op_value_mem_final_[sample];
    }
  }

  float GetDVal(int sample) const override {
    if (wdl_) {
      auto d = inputs_outputs_->op_value_mem_final_[3 * sample + 1];
      return d;
    } else {
      return 0.0f;
    }
  }

  float GetPVal(int sample, int move_id) const override {
    return inputs_outputs_
        ->op_policy_mem_final_[sample * kNumOutputPolicy + move_id];
  }

  float GetMVal(int sample) const override {
    if (moves_left_) {
      return inputs_outputs_->op_moves_left_mem_final_[sample];
    }
    return 0.0f;
  }

 private:
  // Memory holding inputs, outputs.
  std::unique_ptr<InputsOutputsDx> inputs_outputs_;
  int batch_size_;
  bool wdl_;
  bool moves_left_;

  DxNetwork* network_;
};

// D3D12 related stuff.
class DxContext {
 private:
  // Should be enough to hold descriptors for all resources.
  static constexpr int kNumDescHeapSlots = 65536;

  ID3D12Device5* device_;
  ID3D12CommandQueue* command_queue_;
  ID3D12GraphicsCommandList4* command_list_;
  ID3D12CommandAllocator* command_allocator_;
  ID3D12DescriptorHeap* desc_heap_;
  ID3D12Fence* fence_;
  uint64_t fence_val_;
  ShaderWrapper shader_wrapper_;

  std::atomic<unsigned int> next_slot_in_desc_heap_;

  // Scratch space in system memory (used to copy to/from CPU data).
  // 256 MB should be enough for uploading weights, etc.
  constexpr static size_t kUploadDownloadScratchSize = 256 * 1024 * 1024;
  DXAlloc upload_scratch_mem_;
  DXAlloc readback_scratch_mem_;

  int gpu_id_;

 public:
  DxContext(const OptionsDict& options);
  ~DxContext();

  ID3D12Device5* getDevice() { return device_; }
  ID3D12GraphicsCommandList4* getCommandList() { return command_list_; }
  ShaderWrapper* getShaderWrapper() { return &shader_wrapper_; }

  // util functions
  void CreateAlloc(size_t size, D3D12_HEAP_TYPE type, DXAlloc& alloc,
                   bool fp16);
  void UavBarrier(ID3D12GraphicsCommandList4* cl = nullptr);
  uint64_t FlushCL(ID3D12GraphicsCommandList4* cl = nullptr);
  void WaitForGpu(uint64_t fence_val = 0);
  void ResetCL(ID3D12GraphicsCommandList4* cl = nullptr,
               ID3D12CommandAllocator* ca = nullptr, bool reset = true);

  void FlushAndWait();
  void ScheduleUpload(DXAlloc alloc, const void* data, size_t size);
  void DumpFp32(float* buf, int elements);
  void CopyTensor(DXAlloc dst, DXAlloc src, int bytes);
  void DumpTensor(const char* message, DXAlloc alloc, int size,
                  bool fp16 = true, bool allnewline = false);
  void DumpCpuTensor(void* data, int size, bool fp16 = true,
                     bool allnewline = false);
};

class DxNetwork : public Network {
  friend struct InputsOutputsDx;

 public:
  DxNetwork(const WeightsFile& file, const OptionsDict& options);
  ~DxNetwork();

  void Eval(InputsOutputsDx* io, int batchSize);
  std::unique_ptr<NetworkComputation> NewComputation() override;
  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

  std::unique_ptr<InputsOutputsDx> GetInputsOutputs();
  void ReleaseInputsOutputs(std::unique_ptr<InputsOutputsDx> resource);

 private:
  const NetworkCapabilities capabilities_;
  DxContext dx_context_;
  int max_batch_size_;

  // Currently only one NN Eval can happen a time (we can fix this if needed
  // by allocating more memory).
  mutable std::mutex lock_;

  // Network Properties.
  int num_blocks_;
  bool has_se_;
  bool has_wdl_;
  bool has_conv_policy_;
  bool fp16_;
  bool moves_left_;

  std::vector<std::unique_ptr<BaseLayer>> network_;
  BaseLayer* getLastLayer() { return network_.back().get(); }

  // Unique Metacommands used multiple times in the network.

  // GEMM metacommands needed by winograd algorithm.
  std::unique_ptr<GemmMetaCommand> input_conv_gemm_metacommand_;
  std::unique_ptr<GemmMetaCommand> residual_block_gemm_metacommand_;
  std::unique_ptr<GemmMetaCommand> policy_conv_gemm_metacommand_;

  // Convolution metacommands to directly perform convolution.
  //  - used only when GEMM metacommand isn't supported.
  std::unique_ptr<ConvMetaCommand> input_conv_metacommand_;
  std::unique_ptr<ConvMetaCommand> resi_block_conv_1_metacommand_;
  std::unique_ptr<ConvMetaCommand> resi_block_conv_2_metacommand_;
  std::unique_ptr<ConvMetaCommand> policy_conv_metacommand_;

  // In device memory.
  DXAlloc tensor_mem_[4];

  mutable std::mutex inputs_outputs_lock_;
  std::list<std::unique_ptr<InputsOutputsDx>> free_inputs_outputs_;
};

}  // namespace dx_backend
}  // namespace lczero

```

`src/neural/dx/shader_wrapper.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#include "shader_wrapper.h"
#include <cassert>
#include <cstring>
#include "comdef.h"
#include "neural/network.h"
#include "shaders/shader_shared.h"
#include "shaders/shaders.h"

#define ARR_ELEMENT_COUNT(x) (sizeof(x) / sizeof(x[0]))
namespace lczero {
namespace dx_backend {

// Helper macros to reduce copy-paste.
#define CREATE_WINOGRAD_SE_PSO(channels)                                  \
  state_desc.CS = {g_output_transform_shader_fp32_se_##channels,          \
                   sizeof(g_output_transform_shader_fp32_se_##channels)}; \
  ReportDxErrors(device->CreateComputePipelineState(                      \
      &state_desc,                                                        \
      IID_PPV_ARGS(&winograd_output_transform_se_##channels##_)));

#define CREATE_SE_PSO(channels)                               \
  state_desc.CS = {g_se_##channels, sizeof(g_se_##channels)}; \
  ReportDxErrors(device->CreateComputePipelineState(          \
      &state_desc, IID_PPV_ARGS(&se_##channels##_)));


#define SET_WINOGRAD_SE_PSO(channels)                         \
  command_list->SetPipelineState(                             \
      winograd_output_transform_se_##channels##_);

void ShaderWrapper::Init(ID3D12Device* device) {
  // Create root signature - common for all shaders.

  // 8+1+8 slots
  // slot 0 to 7  -> root UAV slots 0 to 7 (all in space 0)
  // slot 8       -> root constants (16 constants - should be enough)
  // slot 9 to 16 -> descriptor UAVs of same allocations as slots 0-7, bound
  //                 at shader slots 8-15

  D3D12_ROOT_PARAMETER root_parameter[kUavSlots + 1 + kUavSlots];
  for (int i = 0; i < kUavSlots; i++) {
    root_parameter[i].ParameterType = D3D12_ROOT_PARAMETER_TYPE_UAV;
    root_parameter[i].Descriptor.RegisterSpace = 0;
    root_parameter[i].Descriptor.ShaderRegister = i;
    root_parameter[i].ShaderVisibility = D3D12_SHADER_VISIBILITY_ALL;
  }

  root_parameter[kUavSlots].ParameterType =
      D3D12_ROOT_PARAMETER_TYPE_32BIT_CONSTANTS;
  root_parameter[kUavSlots].Constants.RegisterSpace = 0;
  root_parameter[kUavSlots].Constants.ShaderRegister = 0;
  root_parameter[kUavSlots].Constants.Num32BitValues = 16;
  root_parameter[kUavSlots].ShaderVisibility = D3D12_SHADER_VISIBILITY_ALL;

  D3D12_DESCRIPTOR_RANGE descRange[kUavSlots] = {};
  for (int i = 0; i < kUavSlots; i++) {
    descRange[i].BaseShaderRegister = i + kUavSlots;
    descRange[i].NumDescriptors = 1;
    descRange[i].OffsetInDescriptorsFromTableStart = 0;
    descRange[i].RangeType = D3D12_DESCRIPTOR_RANGE_TYPE_UAV;
    descRange[i].RegisterSpace = 0;

    root_parameter[i + kUavSlots + 1].ParameterType =
        D3D12_ROOT_PARAMETER_TYPE_DESCRIPTOR_TABLE;
    root_parameter[i + kUavSlots + 1].DescriptorTable.NumDescriptorRanges = 1;
    root_parameter[i + kUavSlots + 1].DescriptorTable.pDescriptorRanges =
        &descRange[i];
    root_parameter[i + kUavSlots + 1].ShaderVisibility =
        D3D12_SHADER_VISIBILITY_ALL;
  }

  D3D12_ROOT_SIGNATURE_DESC root_sig_desc = {kUavSlots + 1 + kUavSlots,
                                             root_parameter, 0, NULL,
                                             D3D12_ROOT_SIGNATURE_FLAG_NONE};

  ID3DBlob* serialized_layout = NULL;
  D3D12SerializeRootSignature(&root_sig_desc, D3D_ROOT_SIGNATURE_VERSION_1_0,
                              &serialized_layout, NULL);

  ReportDxErrors(device->CreateRootSignature(
      1, serialized_layout->GetBufferPointer(),
      serialized_layout->GetBufferSize(), IID_PPV_ARGS(&root_sign_)));

  serialized_layout->Release();

  // Create PSO objects for each shader.
  // PSO basically holds the compiled shader object.

  // Expand planes shaders.
  D3D12_COMPUTE_PIPELINE_STATE_DESC state_desc = {};
  state_desc.pRootSignature = root_sign_;
  
  state_desc.CS = {g_ExpandPlanes_shader_fp16,
                   sizeof(g_ExpandPlanes_shader_fp16)};
  ReportDxErrors(device->CreateComputePipelineState(
      &state_desc, IID_PPV_ARGS(&expand_planes_fp16_)));

  state_desc.CS = {g_ExpandPlanes_shader_fp32,
                   sizeof(g_ExpandPlanes_shader_fp32)};
  ReportDxErrors(device->CreateComputePipelineState(
      &state_desc, IID_PPV_ARGS(&expand_planes_fp32_)));

  // Winograd Input Transform shader.
  state_desc.CS = {g_input_transform_shader_fp32,
                   sizeof(g_input_transform_shader_fp32)};
  ReportDxErrors(device->CreateComputePipelineState(
      &state_desc, IID_PPV_ARGS(&winograd_input_transform_)));

  // Winograd Output Transform shader.
  state_desc.CS = {g_output_transform_shader_fp32,
                   sizeof(g_output_transform_shader_fp32)};
  ReportDxErrors(device->CreateComputePipelineState(
      &state_desc, IID_PPV_ARGS(&winograd_output_transform_)));

  // 1x1 convolution shader.
  state_desc.CS = {g_conv_1x1_shader_fp32, sizeof(g_conv_1x1_shader_fp32)};
  ReportDxErrors(device->CreateComputePipelineState(
      &state_desc, IID_PPV_ARGS(&conv_1x1_)));

  // policy map shader.
  state_desc.CS = {g_policy_map_shader_fp32, sizeof(g_policy_map_shader_fp32)};
  ReportDxErrors(device->CreateComputePipelineState(
      &state_desc, IID_PPV_ARGS(&policy_map_)));

  state_desc.CS = {g_MatrixMul_Fp32, sizeof(g_MatrixMul_Fp32)};
  ReportDxErrors(device->CreateComputePipelineState(&state_desc,
                                                    IID_PPV_ARGS(&gemm_)));

  // Add vectors shader.
  state_desc.CS = {g_add_vectors_shader, sizeof(g_add_vectors_shader)};
  ReportDxErrors(device->CreateComputePipelineState(
      &state_desc, IID_PPV_ARGS(&add_vectors_)));

  // Various SE shaders.
  CREATE_SE_PSO(128);
  CREATE_SE_PSO(256);
  CREATE_SE_PSO(320);
  CREATE_SE_PSO(384);
  CREATE_SE_PSO(512);
  CREATE_SE_PSO(640);
  CREATE_SE_PSO(768);
  CREATE_SE_PSO(1024);

  // Various output-transform fused with SE shaders.
  CREATE_WINOGRAD_SE_PSO(128)
  CREATE_WINOGRAD_SE_PSO(256)
  CREATE_WINOGRAD_SE_PSO(320)
  CREATE_WINOGRAD_SE_PSO(384)
  CREATE_WINOGRAD_SE_PSO(512)
  CREATE_WINOGRAD_SE_PSO(640)
  CREATE_WINOGRAD_SE_PSO(768)
  CREATE_WINOGRAD_SE_PSO(1024)
}

void ShaderWrapper::Destroy() {
  expand_planes_fp16_->Release();
  expand_planes_fp32_->Release();

  winograd_input_transform_->Release();
  winograd_output_transform_->Release();
  conv_1x1_->Release();
  policy_map_->Release();
  gemm_->Release();
  add_vectors_->Release();

  se_128_->Release();
  se_256_->Release();
  se_320_->Release();
  se_384_->Release();
  se_512_->Release();
  se_640_->Release();
  se_768_->Release();
  se_1024_->Release();

  winograd_output_transform_se_128_->Release();
  winograd_output_transform_se_256_->Release();
  winograd_output_transform_se_320_->Release();
  winograd_output_transform_se_384_->Release();
  winograd_output_transform_se_512_->Release();
  winograd_output_transform_se_640_->Release();
  winograd_output_transform_se_768_->Release();
  winograd_output_transform_se_1024_->Release();

  root_sign_->Release();
}

void ShaderWrapper::ExpandPlanes(ID3D12GraphicsCommandList4* command_list,
                                 DXAlloc output_tensor, DXAlloc masks,
                                 DXAlloc values, int batchSize, bool fp16) {
  const int N = batchSize * kInputPlanes;
  int consts[] = {N, kInputPlanes};
  command_list->SetComputeRootSignature(root_sign_);
  command_list->SetPipelineState(fp16 ? expand_planes_fp16_
                                      : expand_planes_fp32_);
  command_list->SetComputeRootUnorderedAccessView(0, output_tensor.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(1, masks.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(2, values.gpu_va);
  command_list->SetComputeRoot32BitConstants(kUavSlots, 2, &consts, 0);

  int elements = batchSize * kInputPlanes * 8 * 8;
  int blocks = DivUp(elements, kExpandPlanesElementsPerBlock);
  command_list->Dispatch(blocks, 1, 1);
}

void ShaderWrapper::InputTransform(ID3D12GraphicsCommandList4* command_list,
                                   DXAlloc transformed_input, DXAlloc input,
                                   int N, int C, bool /*fp16*/) {
  int consts[] = {N, C};
  command_list->SetComputeRootSignature(root_sign_);
  command_list->SetPipelineState(winograd_input_transform_);
  command_list->SetComputeRootUnorderedAccessView(0, input.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(1, transformed_input.gpu_va);
  command_list->SetComputeRoot32BitConstants(
      kUavSlots, ARR_ELEMENT_COUNT(consts), &consts, 0);
  command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 0,
                                              input.desc_handle_vector);
  command_list->SetComputeRootDescriptorTable(
      kUavSlots + 1 + 1, transformed_input.desc_handle_scalar);

  int blocks = DivUp(N * C, kWinogradTransformShaderBlockSize);
  command_list->Dispatch(blocks, 1, 1);
}

void ShaderWrapper::Se(ID3D12GraphicsCommandList4* command_list, DXAlloc output,
    DXAlloc input, DXAlloc skip_connection, DXAlloc bias,
    DXAlloc se_w1, DXAlloc se_b1, DXAlloc se_w2,
    DXAlloc se_b2, int N, int K, bool relu, bool bias_add,
    bool skip_add, int se_k, bool /*fp16*/) {
  int consts[] = {N, K, relu, bias_add, skip_add, se_k};
  command_list->SetComputeRootSignature(root_sign_);
  command_list->SetComputeRootUnorderedAccessView(0, input.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(1, output.gpu_va);
  if (bias_add) command_list->SetComputeRootUnorderedAccessView(2, bias.gpu_va);
  if (skip_add)
    command_list->SetComputeRootUnorderedAccessView(3, skip_connection.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(4, se_w1.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(5, se_b1.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(6, se_w2.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(7, se_b2.gpu_va);

  command_list->SetComputeRoot32BitConstants(
      kUavSlots, ARR_ELEMENT_COUNT(consts), &consts, 0);

  command_list->SetComputeRootDescriptorTable(
      kUavSlots + 1 + 0, input.desc_handle_vector);
  command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 1,
                                              output.desc_handle_vector);
  if (bias_add)
    command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 2,
                                                bias.desc_handle_scalar);
  if (skip_add)
    command_list->SetComputeRootDescriptorTable(
        kUavSlots + 1 + 3, skip_connection.desc_handle_vector);

  command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 4,
                                              se_w1.desc_handle_scalar);
  command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 5,
                                              se_b1.desc_handle_scalar);
  command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 6,
                                              se_w2.desc_handle_scalar);
  command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 7,
                                              se_b2.desc_handle_scalar);

  int blocks = N;
  if (K <= 128)
    command_list->SetPipelineState(se_128_);
  else if (K <= 256)
    command_list->SetPipelineState(se_256_);
  else if (K <= 320)
    command_list->SetPipelineState(se_320_);
  else if (K <= 384)
    command_list->SetPipelineState(se_384_);
  else if (K <= 512)
    command_list->SetPipelineState(se_512_);
  else if (K <= 640)
    command_list->SetPipelineState(se_640_);
  else if (K <= 768)
    command_list->SetPipelineState(se_768_);
  else if (K <= 1024)
    command_list->SetPipelineState(se_1024_);
  else
    throw Exception("Unsupported channel count for SE");

  command_list->Dispatch(blocks, 1, 1);
}

void ShaderWrapper::OutputTransform(ID3D12GraphicsCommandList4* command_list,
                                    DXAlloc output, DXAlloc transformed_output,
                                    DXAlloc skip_connection, DXAlloc bias,
                                    DXAlloc se_w1, DXAlloc se_b1, DXAlloc se_w2,
                                    DXAlloc se_b2, int N, int K, bool relu,
                                    bool bias_add, bool skip_add, bool fused_se,
                                    int se_k, bool /*fp16*/) {
  int consts[] = {N, K, relu, bias_add, skip_add, fused_se, se_k};
  command_list->SetComputeRootSignature(root_sign_);
  command_list->SetComputeRootUnorderedAccessView(0, transformed_output.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(1, output.gpu_va);
  if (bias_add) command_list->SetComputeRootUnorderedAccessView(2, bias.gpu_va);
  if (skip_add)
    command_list->SetComputeRootUnorderedAccessView(3, skip_connection.gpu_va);
  if (fused_se) {
    command_list->SetComputeRootUnorderedAccessView(4, se_w1.gpu_va);
    command_list->SetComputeRootUnorderedAccessView(5, se_b1.gpu_va);
    command_list->SetComputeRootUnorderedAccessView(6, se_w2.gpu_va);
    command_list->SetComputeRootUnorderedAccessView(7, se_b2.gpu_va);
  }
  command_list->SetComputeRoot32BitConstants(
      kUavSlots, ARR_ELEMENT_COUNT(consts), &consts, 0);

  command_list->SetComputeRootDescriptorTable(
      kUavSlots + 1 + 0, transformed_output.desc_handle_scalar);
  command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 1,
                                              output.desc_handle_vector);
  if (bias_add)
    command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 2,
                                                bias.desc_handle_scalar);
  if (skip_add)
    command_list->SetComputeRootDescriptorTable(
        kUavSlots + 1 + 3, skip_connection.desc_handle_vector);
  if (fused_se) {
    command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 4,
                                                se_w1.desc_handle_scalar);
    command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 5,
                                                se_b1.desc_handle_scalar);
    command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 6,
                                                se_w2.desc_handle_scalar);
    command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 7,
                                                se_b2.desc_handle_scalar);
  }

  int blocks = 0;
  if (fused_se) {
    blocks = N;
    if (K <= 128)
      SET_WINOGRAD_SE_PSO(128)
    else if (K <= 256)
      SET_WINOGRAD_SE_PSO(256)
    else if (K <= 320)
      SET_WINOGRAD_SE_PSO(320)
    else if (K <= 384)
      SET_WINOGRAD_SE_PSO(384)
    else if (K <= 512)
      SET_WINOGRAD_SE_PSO(512)
    else if (K <= 640)
      SET_WINOGRAD_SE_PSO(640)
    else if (K <= 768)
      SET_WINOGRAD_SE_PSO(768)
    else if (K <= 1024)
      SET_WINOGRAD_SE_PSO(1024)
    else
      throw Exception("Unsupported channel count for SE");

  } else {
    blocks = DivUp(N * K, kWinogradTransformShaderBlockSize);
    command_list->SetPipelineState(winograd_output_transform_);
  }

  command_list->Dispatch(blocks, 1, 1);
}

void ShaderWrapper::Conv1x1(ID3D12GraphicsCommandList4* command_list,
                            DXAlloc output, DXAlloc input, DXAlloc weight,
                            DXAlloc bias, int N, int C, int K, bool relu,
                            bool useBias, bool /*fp16*/) {
  int consts[] = {N, K, C, useBias, relu};
  command_list->SetComputeRootSignature(root_sign_);
  command_list->SetPipelineState(conv_1x1_);
  command_list->SetComputeRootUnorderedAccessView(0, output.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(1, input.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(2, weight.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(3, bias.gpu_va);
  command_list->SetComputeRoot32BitConstants(
      kUavSlots, ARR_ELEMENT_COUNT(consts), &consts, 0);

  command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 0,
                                              output.desc_handle_scalar);
  command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 1,
                                              input.desc_handle_scalar);
  command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 2,
                                              weight.desc_handle_scalar);
  command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 3,
                                              bias.desc_handle_scalar);

  command_list->Dispatch(K, N, 1);
}

void ShaderWrapper::AddVectors(ID3D12GraphicsCommandList4* command_list,
                               DXAlloc C, DXAlloc A, DXAlloc B, int c_size,
                               int a_size, int b_size, bool relu, bool tanh,
                               bool fp16) {
  if (fp16) {
    // Shader handles 2 elements per thread in fp16 mode.
    assert(a_size % 2 == 0);
    assert(b_size % 2 == 0);
    assert(c_size % 2 == 0);
    a_size /= 2;
    b_size /= 2;
    c_size /= 2;
  }
  int consts[] = {a_size, b_size, c_size, relu, tanh, fp16};
  command_list->SetComputeRootSignature(root_sign_);
  command_list->SetPipelineState(add_vectors_);
  command_list->SetComputeRootUnorderedAccessView(0, A.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(1, B.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(2, C.gpu_va);
  command_list->SetComputeRoot32BitConstants(
      kUavSlots, ARR_ELEMENT_COUNT(consts), &consts, 0);

  int blocks = DivUp(c_size, kAddVectorsBlockSize);
  command_list->Dispatch(blocks, 1, 1);
}

void ShaderWrapper::PolicyMap(ID3D12GraphicsCommandList4* command_list,
                              DXAlloc output, DXAlloc input, DXAlloc weights,
                              int N, int input_size, int output_size,
                              int used_size, bool /*fp16*/) {
  int consts[] = {N, input_size, used_size, output_size};
  command_list->SetComputeRootSignature(root_sign_);
  command_list->SetPipelineState(policy_map_);
  command_list->SetComputeRootUnorderedAccessView(0, input.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(1, output.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(2, weights.gpu_va);
  command_list->SetComputeRoot32BitConstants(kUavSlots, ARR_ELEMENT_COUNT(consts), &consts, 0);
  command_list->SetComputeRootDescriptorTable(kUavSlots+1, input.desc_handle_scalar);

  int blocks = DivUp(N * used_size, kPolicyMapBlockSize);
  command_list->Dispatch(blocks, 1, 1);
}

void ShaderWrapper::MatrixMultiply(ID3D12GraphicsCommandList4* command_list,
                                   DXAlloc output, DXAlloc A, DXAlloc B, int M,
                                   int N, int K, int batch, bool /*fp16*/) {
  int Consts[] = {M, N, K, batch};
  command_list->SetComputeRootSignature(root_sign_);

  command_list->SetPipelineState(gemm_);

  command_list->SetComputeRootUnorderedAccessView(0, A.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(1, B.gpu_va);
  command_list->SetComputeRootUnorderedAccessView(2, output.gpu_va);
  command_list->SetComputeRoot32BitConstants(
      kUavSlots, ARR_ELEMENT_COUNT(Consts), &Consts, 0);
  command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 0,
                                              A.desc_handle_vector);
  command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 1,
                                              B.desc_handle_vector);
  command_list->SetComputeRootDescriptorTable(kUavSlots + 1 + 2,
                                              output.desc_handle_vector);


  int blocksX = DivUp(N, kGemmElPerBlockX);
  int blocksY = DivUp(M, kGemmElPerBlockY);
  int blocksZ = batch;

  command_list->Dispatch(blocksX, blocksY, blocksZ);
}

}  // namespace dx_backend
}  // namespace lczero

```

`src/neural/dx/shader_wrapper.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once
#include "dx_common.h"

namespace lczero {
namespace dx_backend {

class ShaderWrapper {
 private:
  ID3D12RootSignature* root_sign_;
  static constexpr int kUavSlots = 8;

  // Various shaders used by the backend:
  //
  // 1. Expand planes: Used to convert packed bit-board representation to
  //                   'planes' that is input to NN
  // 2. Winograd Input transform.
  // 3. Winograd Output transform.
  //      - Fused with bias add, skip connection add, and relu.
  //      - Fused with SE, bias add, skip connection add and relu.
  // 4. Policy Map layer. (can also be done on CPU side)
  // 5. 1x1 convolution custom kernel (used by policy and value heads).
  //      - TODO: Try replacing this with conv metacommand when available.
  //
  // For best performance it would seem that we would need two copies of all the
  // shaders - fp16 and fp32 versions. However 2 copies not needed for now, as:
  //   i) We should use typed UAVs for resource access as they seem to be
  //   faster. With Typed UAVs, the shader automatically recieves datatype converted
  //   values (e.g: in fp32 even when the allocation was in fp16)
  //   ii) Most of these operations are memory bound - except for the GEMM, but:
  //  iii) Due to driver/compiler bugs or lack of optimizations fp16 path seems
  //  slower on both Nvidia and AMD for most of the shaders - even GEMM on AMD
  //  is way slower with fp16 math than with fp32.

  // Only expand planes has different shaders for different datatypes.
  //  - Mostly a meaningless 'early' optimization as this shouldn't be the bottleneck.
  ID3D12PipelineState* expand_planes_fp16_;
  ID3D12PipelineState* expand_planes_fp32_;

  ID3D12PipelineState* winograd_input_transform_;
  ID3D12PipelineState* winograd_output_transform_;
  ID3D12PipelineState* conv_1x1_;
  ID3D12PipelineState* policy_map_;

  // Gemm shaders (used when gemm Metacommand isn't supported by the HW vendor)
  ID3D12PipelineState* gemm_;
 
  // Another simple shader to add bias, apply relu/tanh, etc.
  ID3D12PipelineState* add_vectors_;

  // Fused SE shaders for various standard channel counts.
  ID3D12PipelineState* se_128_;
  ID3D12PipelineState* se_256_;
  ID3D12PipelineState* se_320_;
  ID3D12PipelineState* se_384_;
  ID3D12PipelineState* se_512_;
  ID3D12PipelineState* se_640_;
  ID3D12PipelineState* se_768_;
  ID3D12PipelineState* se_1024_;

  // Winograd output transform fused with SE for various standard channel
  // counts.
  ID3D12PipelineState* winograd_output_transform_se_128_;
  ID3D12PipelineState* winograd_output_transform_se_256_;
  ID3D12PipelineState* winograd_output_transform_se_320_;
  ID3D12PipelineState* winograd_output_transform_se_384_;
  ID3D12PipelineState* winograd_output_transform_se_512_;
  ID3D12PipelineState* winograd_output_transform_se_640_;
  ID3D12PipelineState* winograd_output_transform_se_768_;
  ID3D12PipelineState* winograd_output_transform_se_1024_;

 public:
  void Init(ID3D12Device* pDevice);
  void Destroy();

  void ExpandPlanes(ID3D12GraphicsCommandList4* command_list,
                    DXAlloc output_tensor, DXAlloc masks, DXAlloc values,
                    int batchSize, bool fp16);

  void InputTransform(ID3D12GraphicsCommandList4* command_list,
                      DXAlloc transformed_input, DXAlloc input, int N, int C,
                      bool fp16);

  void OutputTransform(ID3D12GraphicsCommandList4* command_list, DXAlloc output,
                       DXAlloc transformed_output, DXAlloc skip_connection,
                       DXAlloc bias, DXAlloc se_w1, DXAlloc se_b1,
                       DXAlloc se_w2, DXAlloc se_b2, int N, int K, bool relu,
                       bool bias_add, bool skip_add, bool fused_se, int se_k,
                       bool fp16);

  void Se(ID3D12GraphicsCommandList4* command_list, DXAlloc output,
          DXAlloc input, DXAlloc skip_connection, DXAlloc bias, DXAlloc se_w1,
          DXAlloc se_b1, DXAlloc se_w2, DXAlloc se_b2, int N, int K, bool relu,
          bool bias_add, bool skip_add, int se_k, bool fp16);

  void Conv1x1(ID3D12GraphicsCommandList4* command_list, DXAlloc output,
               DXAlloc input, DXAlloc weight, DXAlloc bias, int N, int C, int K,
               bool relu, bool useBias, bool fp16);

  void AddVectors(ID3D12GraphicsCommandList4* command_list, DXAlloc C,
                  DXAlloc A, DXAlloc B, int c_size, int b_size, int a_size,
                  bool relu, bool tanh, bool fp16);

  void PolicyMap(ID3D12GraphicsCommandList4* command_list, DXAlloc output,
                 DXAlloc input, DXAlloc weights, int N, int input_size,
                 int output_size, int used_size, bool fp16);

  void MatrixMultiply(ID3D12GraphicsCommandList4* command_list, DXAlloc output,
                      DXAlloc A, DXAlloc B, int M, int N, int K, int batch,
                      bool fp16);
};

}  // namespace dx_backend
}  // namespace lczero

```

`src/neural/dx/shaders/AddVectors.hlsl`:

```hlsl
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "shader_shared.h"

// ------------------- Add Vectors kernel -----------------------------//
//
// C = act(A + B)
// A and B can have different lengths, mod size is used to pick the required
// element.
// fp16 version processes 2 elements at a time.

RWStructuredBuffer<uint> A : register(u0);
RWStructuredBuffer<uint> B : register(u1);
RWStructuredBuffer<uint> C : register(u2);

cbuffer AddVectorConsts : register(b0) {
  // sizes are /2 for fp16
  uint a_size;
  uint b_size;
  uint c_size;
  uint relu;
  uint act_tanh;
  uint fp16;
};

float2 extractElements(uint packedVal) {
  return float2(f16tof32(packedVal & 0xFFFF),
                f16tof32((packedVal >> 16) & 0xFFFF));
}

[numthreads(kAddVectorsBlockSize, 1, 1)] 
void add_vectors_shader
(
  uint3 globalThreadIdx : SV_DispatchThreadID
) 
{
  int index = globalThreadIdx.x;
  if (index >= c_size) return;
  uint a = A[index % a_size];
  uint b = B[index % b_size];
  uint opVal;
  if (fp16) {
    float2 f2a = extractElements(a);
    float2 f2b = extractElements(b);
    float2 f2c = f2a + f2b;
    if (relu) {
      if (f2c.x < 0) f2c.x = 0;
      if (f2c.y < 0) f2c.y = 0;
    }
    if (act_tanh) {
      f2c = tanh(f2c);
    }
    uint2 opu = f32tof16(f2c);
    opVal = opu.x | (opu.y << 16);
  } else {
    float c = asfloat(a) + asfloat(b);
    if (relu && c < 0) c = 0;
    if (act_tanh) c = tanh(c);
    opVal = asuint(c);
  }
  C[index] = opVal;
}

```

`src/neural/dx/shaders/Conv1x1.hlsl`:

```hlsl
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "shader_shared.h"

// ------------------- Simple 1x1 convolution shader -----------------------------//
RWBuffer<float> output  : register(u8);
RWBuffer<float> input   : register(u9);
RWBuffer<float> filter  : register(u10);
RWBuffer<float> bias    : register(u11);

cbuffer ConvConsts : register(b0) {
  uint N, K, C;       
  uint useBias;
  uint relu;
};


#define MAX_FILTERS 1024
groupshared float sh_filter[MAX_FILTERS];
groupshared float sh_bias;


// N*K thread blocks launched (groupIdx.y, and groupIdx.x resp.)
// Each block has 64 (8x8) thread.
// Each thread writes single output element.
[numthreads(kConv1x1BlockSize, 1, 1)] 
#if FP16_IO == 1
void conv_1x1_shader_fp16
#else
void conv_1x1_shader_fp32
#endif
(
    uint3 gtid  : SV_DispatchThreadID, 
    uint3 tid   : SV_GroupThreadID,
    uint3 gid   : SV_GroupID
) 
{
  int k = gid.x;
  int n = gid.y;

  // load bias into shared memory
  if (tid.x == 0) 
      sh_bias = useBias ? bias[k] : 0;

  // load filter into shared memory
  const int iterations = (C - 1) / kConv1x1BlockSize + 1;
  for (int i = 0; i < iterations; i++)
  {
    int c = i * kConv1x1BlockSize + tid.x;
    if (c < C) 
        sh_filter[c] = filter[k * C + c];
  }

  GroupMemoryBarrierWithGroupSync();

  float op = sh_bias;
  for (int c = 0; c < C; c++)
  {
    float ip = input[n * C * 64 + c * 64 + tid.x];
    float filter = sh_filter[c];
    op += ip * filter;
  }

  if (relu && op < 0) op = 0;

  output[n * K * 64 + k * 64 + tid.x] = op;
}


```

`src/neural/dx/shaders/ExpandPlanes.hlsl`:

```hlsl
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "shader_shared.h"

// ------------------- Expand Planes Shader -----------------------------//


RWStructuredBuffer<float>    output_fp32 : register(u0);
RWStructuredBuffer<uint>     output_fp16 : register(u0);
RWStructuredBuffer<uint64_t> masks       : register(u1);
RWStructuredBuffer<float>    values      : register(u2);

cbuffer ExpandPlanesConsts : register(b0) {
  uint N;             // total no of planes to process
  uint kInputPlanes;  // no of planes per position
};


// Block size of 256, same mask/val for 64 consecutive threads.
#define kNumShmemElements (kExpandPlanesElementsPerBlock / 64)
groupshared uint64_t sh_masks[kNumShmemElements];
groupshared float    sh_vals[kNumShmemElements];

[numthreads(kExpandPlanesFp32BlockSize, 1, 1)] 
void ExpandPlanes_shader_fp32
(
    uint3 globalThreadIdx  : SV_DispatchThreadID, 
    uint3 threadIdxInGroup : SV_GroupThreadID
) 
{

  int global_index = globalThreadIdx.x;
  int local_index  = threadIdxInGroup.x;

  int plane_index = global_index >> 6;

  if (plane_index >= N) return;

  // Load inputs to shared memory.
  if (local_index < kNumShmemElements) {
    sh_masks[local_index] = masks[plane_index + local_index];
    sh_vals[local_index] = values[plane_index + local_index];
  }

  GroupMemoryBarrierWithGroupSync();

  uint64_t mask = sh_masks[local_index >> 6];

  int sq_index = global_index & 0x3F;
  float op = 0;

  bool set = !!(mask & (1ull << sq_index));
  if (set) {
    op = sh_vals[local_index >> 6];
  }
  output_fp32[global_index] = op;
}


// every thread writes two consecutive elements
// NCHW means that the consecutive elements are in W dimension
[numthreads(kExpandPlanesFp16BlockSize, 1, 1)] 
void ExpandPlanes_shader_fp16
(
    uint3 globalThreadIdx  : SV_DispatchThreadID,
    uint3 threadIdxInGroup : SV_GroupThreadID
) 
{
  int global_index = globalThreadIdx.x * 2;
  int local_index = threadIdxInGroup.x * 2;

  int plane_index = global_index >> 6;

  if (plane_index >= N) return;

  // Load inputs to shared memory.
  if (threadIdxInGroup.x < kNumShmemElements) {
    sh_masks[threadIdxInGroup.x] = masks[plane_index + threadIdxInGroup.x];
    sh_vals[threadIdxInGroup.x] = values[plane_index + threadIdxInGroup.x];
  }

  GroupMemoryBarrierWithGroupSync();

  uint64_t mask = sh_masks[local_index >> 6];

  int sq_index0 = global_index & 0x3F;
  int sq_index1 = sq_index0 + 1;

  bool set0 = !!(mask & (1ull << sq_index0));
  bool set1 = !!(mask & (1ull << sq_index1));

  float2 opf = 0;

  if (set0) {
    opf.x = sh_vals[local_index >> 6];
  }

  if (set1) {
    opf.y = sh_vals[local_index >> 6];
  }

  uint2 opu = f32tof16(opf);
  uint opVal = opu.x | (opu.y << 16);
  output_fp16[globalThreadIdx.x] = opVal;
}

```

`src/neural/dx/shaders/Gemm.hlsl`:

```hlsl
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "shader_shared.h"

// ------------------- Matrix Multiply Shader -------------------------//

#define MAT_A_INDEX(b, y, x) ((b)*M*K + (y)*K + (x))
#define MAT_B_INDEX(b, y, x) ((b)*K*N + (y)*N + (x))
#define MAT_C_INDEX(b, y, x) ((b)*M*N + (y)*N + (x))

#if USE_FP16_MATH == 1
RWBuffer<float16_t4>    matrixA    : register(u8);
RWBuffer<float16_t4>    matrixB    : register(u9);
RWBuffer<float16_t4>    matrixC    : register(u10);
#else
RWBuffer<float4>        matrixA    : register(u8);
RWBuffer<float4>        matrixB    : register(u9);
RWBuffer<float4>        matrixC    : register(u10);
#endif

cbuffer consts : register(b0) {
    uint M, N, K;
    uint batch;
};

// All matrices are row-major.


// Use shared memory to load inputs.
// Also multiple elements per thread.

// Double-buffered shared memory buffers
// (so that the GPU can overlap loads into shared memory with the math)
#if USE_FP16_MATH==1
groupshared float16_t sharedA[2][kGemmElPerBlockY][kGemmShMemKChunk];
groupshared float16_t sharedB[2][kGemmShMemKChunk][kGemmElPerBlockX];
#else
groupshared float sharedA[2][kGemmElPerBlockY][kGemmShMemKChunk];
groupshared float sharedB[2][kGemmShMemKChunk][kGemmElPerBlockX];
#endif

#define divUp(a, b) (((a)-1)/(b) + 1)

void loadShmemBuffers(int batch, int hs, int ws, int ks, int tidy, int tidx, int bufIndex)
{
#if USE_FP16_MATH==1
    float16_t4 temp;
#else
    float4 temp;
#endif

    const int iterationsA = divUp((kGemmElPerBlockY * kGemmShMemKChunk),
                                  (kGemmBlockWidth * kGemmBlockHeight * 4));
    int i;
    [unroll]
    for (i = 0; i < iterationsA; i++)
    {
      int index = (kGemmBlockWidth * kGemmBlockHeight) * i +
                  kGemmBlockHeight * tidy + tidx;
        index *= 4;
        int lx = index % kGemmShMemKChunk;
        int ly = index / kGemmShMemKChunk;
        if ((hs + ly < M) && (ks + lx < K))
        {
            temp = matrixA[MAT_A_INDEX(batch, hs + ly, ks + lx) / 4];
        }
        else
        {
            temp = 0;
        }
        sharedA[bufIndex][ly][lx + 0] = temp.x;
        sharedA[bufIndex][ly][lx + 1] = temp.y;
        sharedA[bufIndex][ly][lx + 2] = temp.z;
        sharedA[bufIndex][ly][lx + 3] = temp.w;

    }

    const int iterationsB = divUp((kGemmShMemKChunk * kGemmElPerBlockX),
                                  (kGemmBlockWidth * kGemmBlockHeight * 4));
    [unroll]
    for (i = 0; i < iterationsB; i++)
    {
      int index = (kGemmBlockWidth * kGemmBlockHeight) * i +
                  kGemmBlockHeight * tidy + tidx;
        index *= 4;
        int lx = index % kGemmElPerBlockX;
        int ly = index / kGemmElPerBlockX;
        if ((ks + ly < K) && (ws + lx < N))
        {
            temp = matrixB[MAT_B_INDEX(batch, ks + ly, ws + lx) / 4];
        }
        else
        {
            temp = 0;
        }
        sharedB[bufIndex][ly][lx + 0] = temp.x;
        sharedB[bufIndex][ly][lx + 1] = temp.y;
        sharedB[bufIndex][ly][lx + 2] = temp.z;
        sharedB[bufIndex][ly][lx + 3] = temp.w;
    }
}


[numthreads(kGemmBlockWidth, kGemmBlockHeight, 1)] 
void MatrixMul(
    uint3 g_tid : SV_DispatchThreadID,
    uint3 gid : SV_GroupID,
    uint3 l_tid : SV_GroupThreadID
)
{
    int x, y;

#if USE_FP16_MATH==1
    float16_t S[kGemmElPerThreadY][kGemmElPerThreadX];
#else
    float S[kGemmElPerThreadY][kGemmElPerThreadX];
#endif

    [unroll] for (y = 0; y < kGemmElPerThreadY; y++)
      [unroll] for (x = 0; x < kGemmElPerThreadX; x++)
        S[y][x] = 0;

    int wStartThread = g_tid.x * kGemmElPerThreadX;
    int hStartThread = g_tid.y * kGemmElPerThreadY;

    int wStartBlock = gid.x * kGemmBlockWidth * kGemmElPerThreadX;
    int hStartBlock = gid.y * kGemmBlockHeight * kGemmElPerThreadY;

    for (int ks = 0, index = 0; ks < K; ks += kGemmShMemKChunk, index++)
    {
        int shIndex = index & 1;
        // Load chunks of matrices A and B into shared memory.
        loadShmemBuffers(gid.z, hStartBlock, wStartBlock, ks, l_tid.y, l_tid.x, shIndex);

        GroupMemoryBarrierWithGroupSync();

        // Do the Multiplication for the Tile.
        // Removing this unroll improves performance on Nvidia Turing but makes it slightly slower on AMD Vega 7.
        [unroll]
        for (int k = 0; k < kGemmShMemKChunk; k++)
            [unroll]
            for (y = 0; y < kGemmElPerThreadY; y++)
                [unroll]
                for (x = 0; x < kGemmElPerThreadX; x++)
                {
                    int shy = y + l_tid.y * kGemmElPerThreadY;
                    int shx = x + l_tid.x * kGemmElPerThreadX;
                    S[y][x] += sharedA[shIndex][shy][k] * sharedB[shIndex][k][shx];
                }
    }

    // Write results to output.
#if USE_FP16_MATH==1
    float16_t4 temp;
#else
    float4 temp;
#endif

    [unroll]
    for (y = 0; y < kGemmElPerThreadY; y++)
    {
        int w = wStartThread;
        int h = hStartThread + y;

        if (h < M && w < N)
        {
          temp.x = S[y][0];
          temp.y = S[y][1];
          temp.z = S[y][2];
          temp.w = S[y][3];
          matrixC[MAT_C_INDEX(gid.z, h, w + 0) / 4] = temp;

          temp.x = S[y][4];
          temp.y = S[y][5];
          temp.z = S[y][6];
          temp.w = S[y][7];
          matrixC[MAT_C_INDEX(gid.z, h, w + 4) / 4] = temp;
        }
    }
}


```

`src/neural/dx/shaders/PolicyMap.hlsl`:

```hlsl
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "shader_shared.h"

// ------------------- Policy Map Shader -----------------------------//
RWBuffer<float> input : register(u8);

// Output is always fp32.
RWStructuredBuffer<float> output  : register(u1);

// Weights are always int32.
RWStructuredBuffer<int> indices : register(u2);

cbuffer PolicyMapConsts : register(b0) {
  uint N;
  uint inputSize;
  uint usedSize;
  uint outputSize;
};

[numthreads(kPolicyMapBlockSize, 1, 1)] 
void PolicyMapShader
(
  uint3 globalThreadIdx : SV_DispatchThreadID
) 
{
  int tid = globalThreadIdx.x;
  int n = tid / usedSize;
  int i = tid % usedSize;

  if (n >= N) return;

  int j = indices[i];

  if (j >= 0) {
    output[n * outputSize + j] = input[n * inputSize + i];
  }
}

```

`src/neural/dx/shaders/SE.hlsl`:

```hlsl
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

cbuffer consts : register(b0) {
  uint N, C;

  // Additional fused ops.
  // Used only by output transform shader.
  uint relu;
  uint useBias;
  uint skipAdd;

  // The channel count after SE (C / se-ratio)
  uint se_K;
};


RWBuffer<float4> input              : register(u8);
RWBuffer<float4> output             : register(u9);
RWBuffer<float>  bias               : register(u10);
RWBuffer<float4> skipConnection     : register(u11);
RWBuffer<float>  se_w1              : register(u12);
RWBuffer<float>  se_b1              : register(u13);
RWBuffer<float>  se_w2              : register(u14);
RWBuffer<float>  se_b2              : register(u15);

// index in input/output tensors
#define INDEX_NCHW(n, c, h, w) ((n)*C * H * W + (c)*H * W + (h)*W + w)

// Fused SE layer implementation
// Each thread block processes entire 'C' dimension worth of data.
// N thread blocks launched.
// As DX Compute block size needs to be known in advance, we compile multiple
// versions of this same HLSL shader with various standard channel counts (128,
// 256, 320, 384, 512, 640, 768 and 1024) and use the next bigger channel count
// version to handle channel counts not in the list of standard counts.

// Note that the weight matrices are transposed.
#define readw1(row, col) (se_w1[(row)*se_K + (col)])
#define readw2(row, col) (se_w2[(row)*2 * C + (col)])

groupshared float sharedData[BLOCK_SIZE];

[numthreads(BLOCK_SIZE, 1, 1)]
void SE
(
    uint3 gid  : SV_GroupID,
    uint3 tid  : SV_GroupThreadID 
)
{
    const int H = 8, W = 8;

    int n = gid.x;
    int k = tid.x;

    // C is set to K in the constant buffer
    if (k >= C) return;

    // TODO: Try float4 board_r1[8], board_r2[8];
    //       Also try reading skip connection tensor early to get more
    //       math-memory access overlap.
    float board[8][8];
    float b = useBias ? bias[k] : 0;

    // Read input tensor.
    int h;
    [unroll] 
    for (h = 0; h < 8; h++) {
      int index = INDEX_NCHW(n, k, h, 0) / 4;
      float4 r1 = input[index];
      float4 r2 = input[index + 1];
      board[h][0] = r1.x;
      board[h][1] = r1.y;
      board[h][2] = r1.z;
      board[h][3] = r1.w;
      board[h][4] = r2.x;
      board[h][5] = r2.y;
      board[h][6] = r2.z;
      board[h][7] = r2.w;
    }

    // Add bias, and compute the average for SE.
    float S = 0;
    [unroll]
    for (int y = 0; y < 8; y++)
        [unroll]
        for (int x = 0; x < 8; x++)
        {
            board[y][x] += b;
            S += board[y][x];
        }
    float avg = S / 64;
    sharedData[k] = avg;

    GroupMemoryBarrierWithGroupSync();

    // First fully-connected layer for SE
    if (k < se_K) {
      S = 0;

      for (int i = 0; i < C; i++) {
        S += sharedData[i] * readw1(i, k);
      }

      S += se_b1[k];

      // relu
      if (S < 0) S = 0;

      sharedData[k] = S;
    }

    GroupMemoryBarrierWithGroupSync();

    // Second fully-connected layer for SE
    S = 0;
    float B = 0;
    for (int i = 0; i < se_K; i++) {
      float val = sharedData[i];
      S += val * readw2(i, k);
      B += val * readw2(i, k + C);
    }
    S += se_b2[k];
    B += se_b2[k + C];

    // Sigmoid (only on the scale part).
    S = 1.0 / (1.0 + exp(-S));


    // Scale, add skip connection, perform relu, and write to output.
    [unroll]
    for (h = 0; h < 8; h++)
    {
        int index = INDEX_NCHW(n, k, h, 0) / 4;
        // can possibly use uint4 to write entire row at a time?
        // couldn't find half2 to uint re-interpret functions :(
        // same issue for reads.
        float4 r1;
        float4 r2;
        r1.x = board[h][0];
        r1.y = board[h][1];
        r1.z = board[h][2];
        r1.w = board[h][3];
        r2.x = board[h][4];
        r2.y = board[h][5];
        r2.z = board[h][6];
        r2.w = board[h][7];

        // SE scale and bias
        r1 = r1*S + B;
        r2 = r2*S + B;

        // residual add
        if (skipAdd) {
            r1 += skipConnection[index];
            r2 += skipConnection[index+1];
        }

        // relu
        if (relu) {
            float4 zeros = float4(0, 0, 0, 0);
            r1 = max(r1, zeros);
            r2 = max(r2, zeros);
        }

        output[index]     = r1;
        output[index + 1] = r2;
    }
}

```

`src/neural/dx/shaders/WinogradCommon.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "shader_shared.h"

#if USE_FP16_MATH == 1

RWBuffer<float16_t4>  input               : register(u8);
RWBuffer<float16_t>   transformedInput    : register(u9);

RWBuffer<float16_t>  transformedOutput    : register(u8);
RWBuffer<float16_t4> output               : register(u9);
RWBuffer<float16_t>  bias                 : register(u10);
RWBuffer<float16_t4> skipConnection       : register(u11);
RWBuffer<float16_t>  se_w1                : register(u12);
RWBuffer<float16_t>  se_b1                : register(u13);
RWBuffer<float16_t>  se_w2                : register(u14);
RWBuffer<float16_t>  se_b2                : register(u15);

#else

RWBuffer<float4>  input               : register(u8);
RWBuffer<float>   transformedInput    : register(u9);

RWBuffer<float>  transformedOutput    : register(u8);
RWBuffer<float4> output               : register(u9);
RWBuffer<float>  bias                 : register(u10);
RWBuffer<float4> skipConnection       : register(u11);
RWBuffer<float>  se_w1                : register(u12);
RWBuffer<float>  se_b1                : register(u13);
RWBuffer<float>  se_w2                : register(u14);
RWBuffer<float>  se_b2                : register(u15);
#endif


cbuffer consts : register(b0) {
    uint N, C;

    // Additional fused ops.
    // Used only by output transform shader.
    uint relu;
    uint useBias;
    uint skipAdd;
    uint fusedSe;

    // The channel count after SE (C / se-ratio)
    uint se_K;
};


// index in input/output tensors
#define INDEX_NCHW(n,c,h,w) ((n)*C*H*W + (c)*H*W + (h)*W + w)

// index in intermediate/temp tensor
// W, H == 6 here! (6x6 transformed blocks)
// N also includes part of dimension (2x2)
#define GemmN (N * 4)
#define TEMP_INDEX_HWNC(h,w,n,c) ((h)*6*GemmN*C + (w)*GemmN*C + (n)*C + c)

//----------------------------- Utility functions for Winograd transform ------------------------------//

// fp16/half math seems a bit slow! - on both Nvidia Turing and AMD Vega 7 (Bugs? Lack of optimizations?)
// These are memory bandwidth bound shaders anyway.
#if USE_FP16_MATH == 1

void matrixMul_gpu_serial_6x6x6(out float16_t c[6][6], in float16_t a[6][6], in float16_t b[6][6])
{
    [unroll]
    for (int i = 0; i < 6; ++i)
        [unroll]
        for (int j = 0; j < 6; ++j)
        {
            float16_t S = 0;
            [unroll]
            for (int k = 0; k < 6; ++k)
                S += a[i][k] * b[k][j];
            c[i][j] = S;
        }
}

void matrixMul_gpu_serial_4x6x6(out float16_t c[4][6], in float16_t a[4][6], in float16_t b[6][6])
{
    [unroll]
    for (int i = 0; i < 4; ++i)
        [unroll]
        for (int j = 0; j < 6; ++j)
        {
            float16_t S = 0;
            [unroll]
            for (int k = 0; k < 6; ++k)
                S += a[i][k] * b[k][j];
            c[i][j] = S;
        }
}

void matrixMul_gpu_serial_4x4x6(out float16_t c[4][4], in float16_t a[4][6], in float16_t b[6][4])
{
    [unroll]
    for (int i = 0; i < 4; ++i)
        [unroll]
        for (int j = 0; j < 4; ++j)
        {
            float16_t S = 0;
            [unroll]
            for (int k = 0; k < 6; ++k)
                S += a[i][k] * b[k][j];
            c[i][j] = S;
        }
}

void inputTransform4x4_gpu(out float16_t op[6][6], in const float16_t ip[6][6])
{
    // transform applied to input tile (of size 4x4 - padded up to 6x6)
    const float16_t Bt[6][6] = 
    {
        4,  0, -5,  0, 1, 0,
        0, -4, -4,  1, 1, 0,
        0,  4, -4, -1, 1, 0,
        0, -2, -1,  2, 1, 0,
        0,  2, -1, -2, 1, 0,
        0,  4,  0, -5, 0, 1
    };

    const float16_t B[6][6] =
    {
        4,  0,  0,  0,  0,  0,
        0, -4,  4, -2,  2,  4,
       -5, -4, -4, -1, -1,  0,
        0,  1, -1,  2, -2, -5,
        1,  1,  1,  1,  1,  0,
        0,  0,  0,  0,  0,  1
    };

    float16_t tempIp1[6][6];
    matrixMul_gpu_serial_6x6x6(tempIp1, Bt, ip);
    matrixMul_gpu_serial_6x6x6(op, tempIp1, B);
}

void outputTransform4x4_gpu(out float16_t output[4][4], in const float16_t transformedOutput[6][6])
{
    // transform applied to result
    const float16_t At[4][6] = {
        1, 1, 1, 1, 1, 0,
        0, 1,-1, 2,-2, 0,
        0, 1, 1, 4, 4, 0,
        0, 1,-1, 8,-8, 1
    };

    const float16_t A[6][4] = {
        1, 0, 0, 0,
        1, 1, 1, 1,
        1,-1, 1,-1,
        1, 2, 4, 8,
        1,-2, 4,-8,
        0, 0, 0, 1
    };

    float16_t tempOp[4][6];
    matrixMul_gpu_serial_4x6x6(tempOp, At, transformedOutput);
    matrixMul_gpu_serial_4x4x6(output, tempOp, A);
}

#else


//----------------------------- FP32 versions of the same code above ------------------------------//

void matrixMul_gpu_serial_6x6x6(out float c[6][6], in float a[6][6], in float b[6][6])
{
    [unroll]
    for (int i = 0; i < 6; ++i)
        [unroll]
        for (int j = 0; j < 6; ++j)
        {
            float S = 0;
            [unroll]
            for (int k = 0; k < 6; ++k)
                S += a[i][k] * b[k][j];
            c[i][j] = S;
        }
}

void matrixMul_gpu_serial_4x6x6(out float c[4][6], in float a[4][6], in float b[6][6])
{
    [unroll]
    for (int i = 0; i < 4; ++i)
        [unroll]
        for (int j = 0; j < 6; ++j)
        {
            float S = 0;
            [unroll]
            for (int k = 0; k < 6; ++k)
                S += a[i][k] * b[k][j];
            c[i][j] = S;
        }
}

void matrixMul_gpu_serial_4x4x6(out float c[4][4], in float a[4][6], in float b[6][4])
{
    [unroll]
    for (int i = 0; i < 4; ++i)
        [unroll]
        for (int j = 0; j < 4; ++j)
        {
            float S = 0;
            [unroll]
            for (int k = 0; k < 6; ++k)
                S += a[i][k] * b[k][j];
            c[i][j] = S;
        }
}


void inputTransform4x4_gpu(out float op[6][6], in const float ip[6][6])
{
    // transform applied to input tile (of size 4x4 - padded up to 6x6)
    const float Bt[6][6] =
    {
        4,  0, -5,  0, 1, 0,
        0, -4, -4,  1, 1, 0,
        0,  4, -4, -1, 1, 0,
        0, -2, -1,  2, 1, 0,
        0,  2, -1, -2, 1, 0,
        0,  4,  0, -5, 0, 1
    };

    const float B[6][6] =
    {
        4,  0,  0,  0,  0,  0,
        0, -4,  4, -2,  2,  4,
       -5, -4, -4, -1, -1,  0,
        0,  1, -1,  2, -2, -5,
        1,  1,  1,  1,  1,  0,
        0,  0,  0,  0,  0,  1
    };

    float tempIp1[6][6];
    matrixMul_gpu_serial_6x6x6(tempIp1, Bt, ip);
    matrixMul_gpu_serial_6x6x6(op, tempIp1, B);
}

void outputTransform4x4_gpu(out float output[4][4], in const float transformedOutput[6][6])
{
    // transform applied to result
    const float At[4][6] = {
        1, 1, 1, 1, 1, 0,
        0, 1,-1, 2,-2, 0,
        0, 1, 1, 4, 4, 0,
        0, 1,-1, 8,-8, 1
    };

    const float A[6][4] = {
        1, 0, 0, 0,
        1, 1, 1, 1,
        1,-1, 1,-1,
        1, 2, 4, 8,
        1,-2, 4,-8,
        0, 0, 0, 1
    };

    float tempOp[4][6];
    matrixMul_gpu_serial_4x6x6(tempOp, At, transformedOutput);
    matrixMul_gpu_serial_4x4x6(output, tempOp, A);
}

#endif
```

`src/neural/dx/shaders/WinogradTransform.hlsl`:

```hlsl
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "WinogradCommon.h"

// ------------------- Winograd Transform shaders -------------------------//

// fp16/half math seems a bit slow! - on both nvidia Turing and AMD Vega 7
#if USE_FP16_MATH == 1

[numthreads(kWinogradTransformShaderBlockSize, 1, 1)] 
void input_transform_shader_fp16
(    
    uint3 tid : SV_DispatchThreadID
)
{
    const int H = 8, W = 8;
    int c = tid.x % C;
    int n = tid.x / C;
    if (n > N) return;

    float16_t board[8][8];
    
    // read the board (a row at a time)
    [unroll]
    for (int y = 0; y < 8; y++)
    {
        int index = INDEX_NCHW(n, c, y, 0) / 4;
        float16_t4 r1 = input[index];
        float16_t4 r2 = input[index + 1];
        board[y][0] = r1.x;
        board[y][1] = r1.y;
        board[y][2] = r1.z;
        board[y][3] = r1.w;
        board[y][4] = r2.x;
        board[y][5] = r2.y;
        board[y][6] = r2.z;
        board[y][7] = r2.w;
    }

    // top-left
    {
        float16_t inEl[6][6] = {0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0};

        [unroll]
        for (int i = 0; i < 5; i++)
            [unroll]
            for (int j = 0; j < 5; j++)
                inEl[i + 1][j + 1] = board[i][j];

        // ii) transform it
        inputTransform4x4_gpu(inEl, inEl);

        // iii) write to output
        [unroll]
        for (int y = 0; y < 6; y++)
            [unroll]
            for (int x = 0; x < 6; x++)
                transformedInput[TEMP_INDEX_HWNC(y, x, n * 4 + 0, c)] = inEl[y][x];
    }

    // top-right
    {
        float16_t inEl[6][6] = { 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0 };

        [unroll]
        for (int i = 0; i < 5; i++)
            [unroll]
            for (int j = 0; j < 5; j++)
                inEl[i + 1][j] = board[i][j+3];


        // ii) transform it
        inputTransform4x4_gpu(inEl, inEl);

        // iii) write to output
        [unroll]
        for (int y = 0; y < 6; y++)
            [unroll]
            for (int x = 0; x < 6; x++)
                transformedInput[TEMP_INDEX_HWNC(y, x, n * 4 + 1, c)] = inEl[y][x];
    }


    // bottom-left
    {
        float16_t inEl[6][6] = { 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0 };

        [unroll]
        for (int i = 0; i < 5; i++)
            [unroll]
            for (int j = 0; j < 5; j++)
                inEl[i][j + 1] = board[i+3][j];

        // ii) transform it
        inputTransform4x4_gpu(inEl, inEl);
        
        // iii) write to output
        [unroll]
        for (int y = 0; y < 6; y++)
            [unroll]
            for (int x = 0; x < 6; x++)
                transformedInput[TEMP_INDEX_HWNC(y, x, n * 4 + 2, c)] = inEl[y][x];
    }

    // bottom-right
    {
        float16_t inEl[6][6] = { 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0 };

        [unroll]
        for (int i = 0; i < 5; i++)
            [unroll]
            for (int j = 0; j < 5; j++)
                inEl[i][j] = board[i+3][j+3];

        // ii) transform it
        inputTransform4x4_gpu(inEl, inEl);

        // iii) write to output
        [unroll]
        for (int y = 0; y < 6; y++)
            [unroll]
            for (int x = 0; x < 6; x++)
                transformedInput[TEMP_INDEX_HWNC(y, x, n * 4 + 3, c)] = inEl[y][x];
    }
}

[numthreads(kWinogradTransformShaderBlockSize, 1, 1)]
void output_transform_shader_fp16
(
    uint3 tid : SV_DispatchThreadID
)
{
    const int H = 8, W = 8;

    int k = tid.x % C;      // C is set to K in the constant buffer
    int n = tid.x / C;
    if (n > N) return;

    float16_t board[8][8];
    float16_t b = useBias ? bias[k] : 0;

    [unroll]
    for (int hStart = 0; hStart < 8; hStart += 4)
        [unroll]
        for (int wStart = 0; wStart < 8; wStart += 4)
        {
            //  i) read to per thread registers (for doing output transform)
            int shln = n * 4 + (hStart / 4) * 2 + (wStart / 4);
            float16_t outElTransformed[6][6];
            [unroll]
            for (int y = 0; y < 6; y++)
                [unroll]
                for (int x = 0; x < 6; x++)
                    outElTransformed[y][x] = transformedOutput[TEMP_INDEX_HWNC(y, x, shln, k)];

            // ii) transform it
            float16_t outEl[4][4];
            outputTransform4x4_gpu(outEl, outElTransformed);

            {
                [unroll]
                for (int y = 0; y < 4; y++)
                    [unroll]
                    for (int x = 0; x < 4; x++)
                        board[hStart + y][wStart + x] = outEl[y][x];
            }
        }

    // iii) write to output
    {
        [unroll]
        for (int y = 0; y < 8; y++)
        {
            int index = INDEX_NCHW(n, k, y, 0) / 4;
            // can possibly use uint4 to write entire row at a time?
            // couldn't find half2 to uint re-interpret functions :(
            // same issue for reads.
            float16_t4 r1;
            float16_t4 r2;
            r1.x = board[y][0];
            r1.y = board[y][1];
            r1.z = board[y][2];
            r1.w = board[y][3];
            r2.x = board[y][4];
            r2.y = board[y][5];
            r2.z = board[y][6];
            r2.w = board[y][7];

            // bias
            r1 += b;
            r2 += b;

            // residual add
            if (skipAdd) {
                r1 += skipConnection[index];
                r2 += skipConnection[index + 1];
            }

            // relu
            if (relu) {
              float16_t4 zeros = float16_t4(0, 0, 0, 0);
              r1 = max(r1, zeros);
              r2 = max(r2, zeros);
            }
            output[index]     = r1;
            output[index + 1] = r2;
        }
    }
}

#else

//----------------------------- FP32 versions of the same shaders above ------------------------------//

[numthreads(kWinogradTransformShaderBlockSize, 1, 1)]
#if USE_FP16_MATH == 1
void input_transform_shader_fp16
#else
void input_transform_shader_fp32
#endif
(
    uint3 tid : SV_DispatchThreadID
)
{
    const int H = 8, W = 8;
    int c = tid.x % C;
    int n = tid.x / C;
    if (n > N) return;

    float board[8][8];

    // read the board (a row at a time)
    [unroll]
    for (int y = 0; y < 8; y++)
    {
        int index = INDEX_NCHW(n, c, y, 0) / 4;
        float4 r1 = input[index];
        float4 r2 = input[index + 1];
        board[y][0] = r1.x;
        board[y][1] = r1.y;
        board[y][2] = r1.z;
        board[y][3] = r1.w;
        board[y][4] = r2.x;
        board[y][5] = r2.y;
        board[y][6] = r2.z;
        board[y][7] = r2.w;
    }

    // top-left
    {
        float inEl[6][6] = { 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0 };

        [unroll]
        for (int i = 0; i < 5; i++)
            [unroll]
            for (int j = 0; j < 5; j++)
                inEl[i + 1][j + 1] = board[i][j];

        // ii) transform it
        inputTransform4x4_gpu(inEl, inEl);

        // iii) write to output
        [unroll]
        for (int y = 0; y < 6; y++)
            [unroll]
            for (int x = 0; x < 6; x++)
                transformedInput[TEMP_INDEX_HWNC(y, x, n * 4 + 0, c)] = inEl[y][x];
    }

    // top-right
    {
        float inEl[6][6] = { 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0 };

        [unroll]
        for (int i = 0; i < 5; i++)
            [unroll]
            for (int j = 0; j < 5; j++)
                inEl[i + 1][j] = board[i][j + 3];


        // ii) transform it
        inputTransform4x4_gpu(inEl, inEl);

        // iii) write to output
        [unroll]
        for (int y = 0; y < 6; y++)
            [unroll]
            for (int x = 0; x < 6; x++)
                transformedInput[TEMP_INDEX_HWNC(y, x, n * 4 + 1, c)] = inEl[y][x];
    }


    // bottom-left
    {
        float inEl[6][6] = { 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0 };

        [unroll]
        for (int i = 0; i < 5; i++)
            [unroll]
            for (int j = 0; j < 5; j++)
                inEl[i][j + 1] = board[i + 3][j];

        // ii) transform it
        inputTransform4x4_gpu(inEl, inEl);

        // iii) write to output
        [unroll]
        for (int y = 0; y < 6; y++)
            [unroll]
        for (int x = 0; x < 6; x++)
            transformedInput[TEMP_INDEX_HWNC(y, x, n * 4 + 2, c)] = inEl[y][x];
    }

    // bottom-right
    {
        float inEl[6][6] = { 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0, 0,0,0,0,0,0 };

        [unroll]
        for (int i = 0; i < 5; i++)
            [unroll]
        for (int j = 0; j < 5; j++)
            inEl[i][j] = board[i + 3][j + 3];

        // ii) transform it
        inputTransform4x4_gpu(inEl, inEl);

        // iii) write to output
        [unroll]
        for (int y = 0; y < 6; y++)
            [unroll]
        for (int x = 0; x < 6; x++)
            transformedInput[TEMP_INDEX_HWNC(y, x, n * 4 + 3, c)] = inEl[y][x];
    }
}


[numthreads(kWinogradTransformShaderBlockSize, 1, 1)]
#if USE_FP16_MATH == 1
void output_transform_shader_fp16
#else
void output_transform_shader_fp32
#endif
(
    uint3 tid : SV_DispatchThreadID
)
{
    const int H = 8, W = 8;

    int k = tid.x % C;      // C is set to K in the constant buffer
    int n = tid.x / C;
    if (n > N) return;

    float board[8][8];
    float b = useBias ? bias[k] : 0;

    [unroll]
    for (int hStart = 0; hStart < 8; hStart += 4)
        [unroll]
        for (int wStart = 0; wStart < 8; wStart += 4)
        {
            //  i) read to per thread registers (for doing output transform)
            int shln = n * 4 + (hStart / 4) * 2 + (wStart / 4);
            float outElTransformed[6][6];
            [unroll]
            for (int y = 0; y < 6; y++)
                [unroll]
                for (int x = 0; x < 6; x++)
                    outElTransformed[y][x] = transformedOutput[TEMP_INDEX_HWNC(y, x, shln, k)];

            // ii) transform it
            float outEl[4][4];
            outputTransform4x4_gpu(outEl, outElTransformed);

            {
                [unroll]
                for (int y = 0; y < 4; y++)
                    [unroll]
                    for (int x = 0; x < 4; x++)
                        board[hStart + y][wStart + x] = outEl[y][x];
            }
        }

    // iii) write to output
    {
        [unroll]
        for (int y = 0; y < 8; y++)
        {
            int index = INDEX_NCHW(n, k, y, 0) / 4;
            // can possibly use uint4 to write entire row at a time?
            // couldn't find half2 to uint re-interpret functions :(
            // same issue for reads.
            float4 r1;
            float4 r2;
            r1.x = board[y][0];
            r1.y = board[y][1];
            r1.z = board[y][2];
            r1.w = board[y][3];
            r2.x = board[y][4];
            r2.y = board[y][5];
            r2.z = board[y][6];
            r2.w = board[y][7];

            // bias
            r1 += b;
            r2 += b;

            // residual add
            if (skipAdd) {
                r1 += skipConnection[index];
                r2 += skipConnection[index+1];
            }

            // relu
            if (relu) {
              float4 zeros = float4(0, 0, 0, 0);
              r1 = max(r1, zeros);
              r2 = max(r2, zeros);
            }

            output[index]     = r1;
            output[index + 1] = r2;
        }
    }
}

#endif
```

`src/neural/dx/shaders/WinogradTransformSE.hlsl`:

```hlsl
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "WinogradCommon.h"

// Output transform shader - fused with SE.
// Each thread block processes entire 'C' dimension worth of data.
// N thread blocks launched.
// As DX Compute block size needs to be known in advance, we compile multiple
// versions of this same HLSL shader with various standard channel counts (128,
// 256, 320, 384, 512, 640, 768 and 1024) and use the next bigger channel count
// version to handle channel counts not in the list of standard counts.

// Note that the weight matrices are transposed.
#define readw1(row, col) (se_w1[(row)*se_K + (col)])
#define readw2(row, col) (se_w2[(row)*2 * C + (col)])

#if USE_FP16_MATH == 1
groupshared half sharedData[BLOCK_SIZE];

[numthreads(BLOCK_SIZE, 1, 1)]
void OutputTransformSE
(
    uint3 gid  : SV_GroupID,
    uint3 tid  : SV_GroupThreadID 
)
{
    const int H = 8, W = 8;

    int n = gid.x;
    int k = tid.x;

    // C is set to K in the constant buffer
    if (k >= C) return;


    half board[8][8];
    half b = useBias ? bias[k] : 0;

    // Winograd output-transform
    [unroll]
    for (int hStart = 0; hStart < 8; hStart += 4)
        [unroll]
        for (int wStart = 0; wStart < 8; wStart += 4)
        {
            //  i) read to per thread registers (for doing output transform)
            int shln = n * 4 + (hStart / 4) * 2 + (wStart / 4);
            half outElTransformed[6][6];
            [unroll]
            for (int y = 0; y < 6; y++)
                [unroll]
                for (int x = 0; x < 6; x++)
                    outElTransformed[y][x] = transformedOutput[TEMP_INDEX_HWNC(y, x, shln, k)];

            // ii) transform it
            half outEl[4][4];
            outputTransform4x4_gpu(outEl, outElTransformed);

            {
                [unroll]
                for (int y = 0; y < 4; y++)
                    [unroll]
                    for (int x = 0; x < 4; x++)
                        board[hStart + y][wStart + x] = outEl[y][x];
            }
        }

    // Add bias, and compute the average for SE.
    half S = 0;
    [unroll]
    for (int y = 0; y < 8; y++)
        [unroll]
        for (int x = 0; x < 8; x++)
        {
            board[y][x] += b;
            S += board[y][x];
        }
    half avg = S / 64;
    sharedData[k] = avg;

    GroupMemoryBarrierWithGroupSync();

    // First fully-connected layer for SE
    if (k < se_K) {
      S = 0;

      for (int i = 0; i < C; i++) {
        S += sharedData[i] * readw1(i, k);
      }

      S += se_b1[k];

      // relu
      if (S < 0) S = 0;

      sharedData[k] = S;
    }

    GroupMemoryBarrierWithGroupSync();

    // Second fully-connected layer for SE
    S = 0;
    half B = 0;
    for (int i = 0; i < se_K; i++) {
      half val = sharedData[i];
      S += val * readw2(i, k);
      B += val * readw2(i, k + C);
    }
    S += se_b2[k];
    B += se_b2[k + C];

    // Sigmoid (only on the scale part).
    S = 1.0 / (1.0 + exp(-S));


    // Scale, add skip connection, perform relu, and write to output.
    [unroll]
    for (int h = 0; h < 8; h++)
    {
        int index = INDEX_NCHW(n, k, h, 0) / 4;
        // can possibly use uint4 to write entire row at a time?
        // couldn't find half2 to uint re-interpret functions :(
        // same issue for reads.
        half4 r1;
        half4 r2;
        r1.x = board[h][0];
        r1.y = board[h][1];
        r1.z = board[h][2];
        r1.w = board[h][3];
        r2.x = board[h][4];
        r2.y = board[h][5];
        r2.z = board[h][6];
        r2.w = board[h][7];

        // SE scale and bias
        r1 = r1*S + B;
        r2 = r2*S + B;

        // residual add
        if (skipAdd) {
            r1 += skipConnection[index];
            r2 += skipConnection[index+1];
        }

        // relu
        if (relu) {
            half4 zeros = half4(0, 0, 0, 0);
            r1 = max(r1, zeros);
            r2 = max(r2, zeros);
        }

        output[index]     = r1;
        output[index + 1] = r2;
    }
}

#else

groupshared float sharedData[BLOCK_SIZE];

[numthreads(BLOCK_SIZE, 1, 1)]
void OutputTransformSE
(
    uint3 gid  : SV_GroupID,
    uint3 tid  : SV_GroupThreadID 
)
{
    const int H = 8, W = 8;

    int n = gid.x;
    int k = tid.x;

    // C is set to K in the constant buffer
    if (k >= C) return;


    float board[8][8];
    float b = useBias ? bias[k] : 0;

    // Winograd output-transform
    [unroll]
    for (int hStart = 0; hStart < 8; hStart += 4)
        [unroll]
        for (int wStart = 0; wStart < 8; wStart += 4)
        {
            //  i) read to per thread registers (for doing output transform)
            int shln = n * 4 + (hStart / 4) * 2 + (wStart / 4);
            float outElTransformed[6][6];
            [unroll]
            for (int y = 0; y < 6; y++)
                [unroll]
                for (int x = 0; x < 6; x++)
                    outElTransformed[y][x] = transformedOutput[TEMP_INDEX_HWNC(y, x, shln, k)];

            // ii) transform it
            float outEl[4][4];
            outputTransform4x4_gpu(outEl, outElTransformed);

            {
                [unroll]
                for (int y = 0; y < 4; y++)
                    [unroll]
                    for (int x = 0; x < 4; x++)
                        board[hStart + y][wStart + x] = outEl[y][x];
            }
        }

    // Add bias, and compute the average for SE.
    float S = 0;
    [unroll]
    for (int y = 0; y < 8; y++)
        [unroll]
        for (int x = 0; x < 8; x++)
        {
            board[y][x] += b;
            S += board[y][x];
        }
    float avg = S / 64;
    sharedData[k] = avg;

    GroupMemoryBarrierWithGroupSync();

    // First fully-connected layer for SE
    if (k < se_K) {
      S = 0;

      for (int i = 0; i < C; i++) {
        S += sharedData[i] * readw1(i, k);
      }

      S += se_b1[k];

      // relu
      if (S < 0) S = 0;

      sharedData[k] = S;
    }

    GroupMemoryBarrierWithGroupSync();

    // Second fully-connected layer for SE
    S = 0;
    float B = 0;
    for (int i = 0; i < se_K; i++) {
      float val = sharedData[i];
      S += val * readw2(i, k);
      B += val * readw2(i, k + C);
    }
    S += se_b2[k];
    B += se_b2[k + C];

    // Sigmoid (only on the scale part).
    S = 1.0 / (1.0 + exp(-S));


    // Scale, add skip connection, perform relu, and write to output.
    [unroll]
    for (int h = 0; h < 8; h++)
    {
        int index = INDEX_NCHW(n, k, h, 0) / 4;
        // can possibly use uint4 to write entire row at a time?
        // couldn't find half2 to uint re-interpret functions :(
        // same issue for reads.
        float4 r1;
        float4 r2;
        r1.x = board[h][0];
        r1.y = board[h][1];
        r1.z = board[h][2];
        r1.w = board[h][3];
        r2.x = board[h][4];
        r2.y = board[h][5];
        r2.z = board[h][6];
        r2.w = board[h][7];

        // SE scale and bias
        r1 = r1*S + B;
        r2 = r2*S + B;

        // residual add
        if (skipAdd) {
            r1 += skipConnection[index];
            r2 += skipConnection[index+1];
        }

        // relu
        if (relu) {
            float4 zeros = float4(0, 0, 0, 0);
            r1 = max(r1, zeros);
            r2 = max(r2, zeros);
        }

        output[index]     = r1;
        output[index + 1] = r2;
    }
}
#endif
```

`src/neural/dx/shaders/dxc_helper.py`:

```py
#!/usr/bin/env python
# Call dxc with absolute paths where '/' are converted to '\' (in windows).
import sys
import os
import re

x = sys.argv
x.pop(0)
for i in range(len(x)):
  if re.match(r"[a-zA-Z]:/", x[i]):
    x[i] = os.path.normpath(x[i])
# We asssume dxc is already on the path.
os.system('dxc ' + '"{}"'.format('" "'.join(x)))


```

`src/neural/dx/shaders/meson.build`:

```build
# This file is part of Leela Chess Zero.
# Copyright (C) 2020 The LCZero Authors
#
# Leela Chess is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Leela Chess is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

# Check for 'dxc' as 'dxc_helper.py' assumes it is on the path.
dxc = find_program('dxc', required: true)

# Used to work around a 'dxc' limitation with '/' in absolute paths.
dxc_helper = find_program('dxc_helper.py', required: true)

dxc_common = ['/Tcs_6_2', '/Fh', '@OUTPUT@', '@INPUT@']

files += custom_target('MatrixMul_Fp32',
    output : 'MatrixMul_Fp32.h',
    input : 'Gemm.hlsl',
    command : [dxc_helper, '/EMatrixMul', '/Vn', 'g_MatrixMul_Fp32'] + dxc_common
)

files += custom_target('MatrixMul_Fp16',
    output : 'MatrixMul_Fp16.h',
    input : 'Gemm.hlsl',
    command : [dxc_helper, '/EMatrixMul', '/Vn', 'g_MatrixMul_Fp16', '/DUSE_FP16_MATH=1',
               '-enable-16bit-types'] + dxc_common
)

files += custom_target('ExpandPlanes_shader_fp32',
    output : 'ExpandPlanes_shader_fp32.h',
    input : 'ExpandPlanes.hlsl',
    command : [dxc_helper, '/EExpandPlanes_shader_fp32'] + dxc_common
)

files += custom_target('ExpandPlanes_shader_fp16',
    output : 'ExpandPlanes_shader_fp16.h',
    input : 'ExpandPlanes.hlsl',
    command : [dxc_helper, '/EExpandPlanes_shader_fp16'] + dxc_common
)

files += custom_target('input_transform_shader_fp32',
    output : 'input_transform_shader_fp32.h',
    input : 'WinogradTransform.hlsl',
    command : [dxc_helper, '/Einput_transform_shader_fp32'] + dxc_common
)

files += custom_target('output_transform_shader_fp32',
    output : 'output_transform_shader_fp32.h',
    input : 'WinogradTransform.hlsl',
    command : [dxc_helper, '/Eoutput_transform_shader_fp32'] + dxc_common
)

files += custom_target('conv_1x1_shader_fp32',
    output : 'conv_1x1_shader_fp32.h',
    input : 'Conv1x1.hlsl',
    command : [dxc_helper, '/Econv_1x1_shader_fp32'] + dxc_common
)

files += custom_target('add_vectors_shader',
    output : 'add_vectors_shader.h',
    input : 'AddVectors.hlsl',
    command : [dxc_helper, '/Eadd_vectors_shader'] + dxc_common
)

files += custom_target('policy_map_shader_fp32',
    output : 'policy_map_shader_fp32.h',
    input : 'PolicyMap.hlsl',
    command : [dxc_helper, '/EPolicyMapShader', '/Vn', 'g_policy_map_shader_fp32'] + dxc_common
)

files += custom_target('output_transform_shader_fp32_se_128',
    output : 'output_transform_shader_fp32_se_128.h',
    input : 'WinogradTransformSE.hlsl',
    command : [dxc_helper, '/EOutputTransformSE', '/Vn', 'g_output_transform_shader_fp32_se_128',
               '/DBLOCK_SIZE=128'] + dxc_common
)

files += custom_target('output_transform_shader_fp32_se_256',
    output : 'output_transform_shader_fp32_se_256.h',
    input : 'WinogradTransformSE.hlsl',
    command : [dxc_helper, '/EOutputTransformSE', '/Vn', 'g_output_transform_shader_fp32_se_256',
               '/DBLOCK_SIZE=256'] + dxc_common
)

files += custom_target('output_transform_shader_fp32_se_320',
    output : 'output_transform_shader_fp32_se_320.h',
    input : 'WinogradTransformSE.hlsl',
    command : [dxc_helper, '/EOutputTransformSE', '/Vn', 'g_output_transform_shader_fp32_se_320',
               '/DBLOCK_SIZE=320'] + dxc_common
)

files += custom_target('output_transform_shader_fp32_se_384',
    output : 'output_transform_shader_fp32_se_384.h',
    input : 'WinogradTransformSE.hlsl',
    command : [dxc_helper, '/EOutputTransformSE', '/Vn', 'g_output_transform_shader_fp32_se_384',
               '/DBLOCK_SIZE=384'] + dxc_common
)

files += custom_target('output_transform_shader_fp32_se_512',
    output : 'output_transform_shader_fp32_se_512.h',
    input : 'WinogradTransformSE.hlsl',
    command : [dxc_helper, '/EOutputTransformSE', '/Vn', 'g_output_transform_shader_fp32_se_512',
               '/DBLOCK_SIZE=512'] + dxc_common
)

files += custom_target('output_transform_shader_fp32_se_640',
    output : 'output_transform_shader_fp32_se_640.h',
    input : 'WinogradTransformSE.hlsl',
    command : [dxc_helper, '/EOutputTransformSE', '/Vn', 'g_output_transform_shader_fp32_se_640',
               '/DBLOCK_SIZE=640'] + dxc_common
)

files += custom_target('output_transform_shader_fp32_se_768',
    output : 'output_transform_shader_fp32_se_768.h',
    input : 'WinogradTransformSE.hlsl',
    command : [dxc_helper, '/EOutputTransformSE', '/Vn', 'g_output_transform_shader_fp32_se_768',
               '/DBLOCK_SIZE=768'] + dxc_common
)

files += custom_target('output_transform_shader_fp32_se_1024',
    output : 'output_transform_shader_fp32_se_1024.h',
    input : 'WinogradTransformSE.hlsl',
    command : [dxc_helper, '/EOutputTransformSE', '/Vn', 'g_output_transform_shader_fp32_se_1024',
               '/DBLOCK_SIZE=1024'] + dxc_common
)

files += custom_target('se_128',
    output : 'se_128.h',
    input : 'SE.hlsl',
    command : [dxc_helper, '/ESE', '/Vn', 'g_se_128', '/DBLOCK_SIZE=128'] + dxc_common
)

files += custom_target('se_256',
    output : 'se_256.h',
    input : 'SE.hlsl',
    command : [dxc_helper, '/ESE', '/Vn', 'g_se_256', '/DBLOCK_SIZE=256'] + dxc_common
)

files += custom_target('se_320',
    output : 'se_320.h',
    input : 'SE.hlsl',
    command : [dxc_helper, '/ESE', '/Vn', 'g_se_320', '/DBLOCK_SIZE=320'] + dxc_common
)

files += custom_target('se_384',
    output : 'se_384.h',
    input : 'SE.hlsl',
    command : [dxc_helper, '/ESE', '/Vn', 'g_se_384', '/DBLOCK_SIZE=384'] + dxc_common
)

files += custom_target('se_512',
    output : 'se_512.h',
    input : 'SE.hlsl',
    command : [dxc_helper, '/ESE', '/Vn', 'g_se_512', '/DBLOCK_SIZE=512'] + dxc_common
)

files += custom_target('se_640',
    output : 'se_640.h',
    input : 'SE.hlsl',
    command : [dxc_helper, '/ESE', '/Vn', 'g_se_640', '/DBLOCK_SIZE=640'] + dxc_common
)

files += custom_target('se_768',
    output : 'se_768.h',
    input : 'SE.hlsl',
    command : [dxc_helper, '/ESE', '/Vn', 'g_se_768', '/DBLOCK_SIZE=768'] + dxc_common
)

files += custom_target('se_1024',
    output : 'se_1024.h',
    input : 'SE.hlsl',
    command : [dxc_helper, '/ESE', '/Vn', 'g_se_1024', '/DBLOCK_SIZE=1024'] + dxc_common
)


```

`src/neural/dx/shaders/shader_shared.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#define kExpandPlanesElementsPerBlock 256
#define kExpandPlanesFp32BlockSize kExpandPlanesElementsPerBlock
#define kExpandPlanesFp16BlockSize (kExpandPlanesElementsPerBlock / 2)

// for both input transform and output transform shaders
#define kWinogradTransformShaderBlockSize 64

#define kConv1x1BlockSize 64

#define kAddVectorsBlockSize 512

#define kPolicyMapBlockSize 256


// Constants for GEMM shader.
#define kGemmBlockWidth 16
#define kGemmBlockHeight 16

#define kGemmElPerThreadX 8
#define kGemmElPerThreadY 8

#define kGemmElPerBlockX (kGemmElPerThreadX * kGemmBlockWidth)
#define kGemmElPerBlockY (kGemmElPerThreadY * kGemmBlockHeight)

#define kGemmShMemKChunk 16
```

`src/neural/dx/shaders/shaders.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include "ExpandPlanes_shader_fp32.h"
#include "ExpandPlanes_shader_fp16.h"
#include "input_transform_shader_fp32.h"
#include "output_transform_shader_fp32.h"
#include "conv_1x1_shader_fp32.h"
#include "add_vectors_shader.h"
#include "policy_map_shader_fp32.h"

#include "output_transform_shader_fp32_se_128.h"
#include "output_transform_shader_fp32_se_256.h"
#include "output_transform_shader_fp32_se_320.h"
#include "output_transform_shader_fp32_se_384.h"
#include "output_transform_shader_fp32_se_512.h"
#include "output_transform_shader_fp32_se_640.h"
#include "output_transform_shader_fp32_se_768.h"
#include "output_transform_shader_fp32_se_1024.h"

#include "se_128.h"
#include "se_256.h"
#include "se_320.h"
#include "se_384.h"
#include "se_512.h"
#include "se_640.h"
#include "se_768.h"
#include "se_1024.h"

#include "MatrixMul_Fp32.h"
#include "MatrixMul_Fp16.h"

```

`src/neural/encoder.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "neural/encoder.h"

#include <algorithm>

namespace lczero {

namespace {

int CompareTransposing(BitBoard board, int initial_transform) {
  uint64_t value = board.as_int();
  if ((initial_transform & FlipTransform) != 0) {
    value = ReverseBitsInBytes(value);
  }
  if ((initial_transform & MirrorTransform) != 0) {
    value = ReverseBytesInBytes(value);
  }
  auto alternative = TransposeBitsInBytes(value);
  if (value < alternative) return -1;
  if (value > alternative) return 1;
  return 0;
}

int ChooseTransform(const ChessBoard& board) {
  // If there are any castling options no transform is valid.
  // Even using FRC rules, king and queen side castle moves are not symmetrical.
  if (!board.castlings().no_legal_castle()) {
    return 0;
  }
  auto our_king = (board.kings() & board.ours()).as_int();
  int transform = NoTransform;
  if ((our_king & 0x0F0F0F0F0F0F0F0FULL) != 0) {
    transform |= FlipTransform;
    our_king = ReverseBitsInBytes(our_king);
  }
  // If there are any pawns only horizontal flip is valid.
  if (board.pawns().as_int() != 0) {
    return transform;
  }
  if ((our_king & 0xFFFFFFFF00000000ULL) != 0) {
    transform |= MirrorTransform;
    our_king = ReverseBytesInBytes(our_king);
  }
  // Our king is now always in bottom right quadrant.
  // Transpose for king in top right triangle, or if on diagonal whichever has
  // the smaller integer value for each test scenario.
  if ((our_king & 0xE0C08000ULL) != 0) {
    transform |= TransposeTransform;
  } else if ((our_king & 0x10204080ULL) != 0) {
    auto outcome = CompareTransposing(board.ours() | board.theirs(), transform);
    if (outcome == -1) return transform;
    if (outcome == 1) return transform | TransposeTransform;
    outcome = CompareTransposing(board.ours(), transform);
    if (outcome == -1) return transform;
    if (outcome == 1) return transform | TransposeTransform;
    outcome = CompareTransposing(board.kings(), transform);
    if (outcome == -1) return transform;
    if (outcome == 1) return transform | TransposeTransform;
    outcome = CompareTransposing(board.queens(), transform);
    if (outcome == -1) return transform;
    if (outcome == 1) return transform | TransposeTransform;
    outcome = CompareTransposing(board.rooks(), transform);
    if (outcome == -1) return transform;
    if (outcome == 1) return transform | TransposeTransform;
    outcome = CompareTransposing(board.knights(), transform);
    if (outcome == -1) return transform;
    if (outcome == 1) return transform | TransposeTransform;
    outcome = CompareTransposing(board.bishops(), transform);
    if (outcome == -1) return transform;
    if (outcome == 1) return transform | TransposeTransform;
    // If all piece types are symmetrical and ours is symmetrical and
    // ours+theirs is symmetrical, everything is symmetrical, so transpose is a
    // no-op.
  }
  return transform;
}
}  // namespace

bool IsCanonicalFormat(pblczero::NetworkFormat::InputFormat input_format) {
  return input_format >=
         pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION;
}
bool IsCanonicalArmageddonFormat(
    pblczero::NetworkFormat::InputFormat input_format) {
  return input_format ==
             pblczero::NetworkFormat::
                 INPUT_112_WITH_CANONICALIZATION_HECTOPLIES_ARMAGEDDON ||
         input_format == pblczero::NetworkFormat::
                             INPUT_112_WITH_CANONICALIZATION_V2_ARMAGEDDON;
}
bool IsHectopliesFormat(pblczero::NetworkFormat::InputFormat input_format) {
  return input_format >=
         pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION_HECTOPLIES;
}
bool Is960CastlingFormat(pblczero::NetworkFormat::InputFormat input_format) {
  return input_format >= pblczero::NetworkFormat::INPUT_112_WITH_CASTLING_PLANE;
}

int TransformForPosition(pblczero::NetworkFormat::InputFormat input_format,
                         const PositionHistory& history) {
  if (!IsCanonicalFormat(input_format)) {
    return 0;
  }
  const ChessBoard& board = history.Last().GetBoard();
  return ChooseTransform(board);
}

InputPlanes EncodePositionForNN(
    pblczero::NetworkFormat::InputFormat input_format,
    const PositionHistory& history, int history_planes,
    FillEmptyHistory fill_empty_history, int* transform_out) {
  InputPlanes result(kAuxPlaneBase + 8);

  int transform = 0;
  // Canonicalization format needs to stop early to avoid applying transform in
  // history across incompatible transitions.  It is also more canonical since
  // history before these points is not relevant to the final result.
  bool stop_early = IsCanonicalFormat(input_format);
  // When stopping early, we want to know if castlings has changed, so capture
  // it for the first board.
  ChessBoard::Castlings castlings;
  {
    const ChessBoard& board = history.Last().GetBoard();
    const bool we_are_black = board.flipped();
    if (IsCanonicalFormat(input_format)) {
      transform = ChooseTransform(board);
    }
    switch (input_format) {
      case pblczero::NetworkFormat::INPUT_CLASSICAL_112_PLANE: {
        // "Legacy" input planes with:
        // - Plane 104 (0-based) filled with 1 if we can castle queenside.
        // - Plane 105 filled with ones if we can castle kingside.
        // - Plane 106 filled with ones if they can castle queenside.
        // - Plane 107 filled with ones if they can castle kingside.
        if (board.castlings().we_can_000()) result[kAuxPlaneBase + 0].SetAll();
        if (board.castlings().we_can_00()) result[kAuxPlaneBase + 1].SetAll();
        if (board.castlings().they_can_000()) {
          result[kAuxPlaneBase + 2].SetAll();
        }
        if (board.castlings().they_can_00()) result[kAuxPlaneBase + 3].SetAll();
        break;
      }

      case pblczero::NetworkFormat::INPUT_112_WITH_CASTLING_PLANE:
      case pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION:
      case pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION_HECTOPLIES:
      case pblczero::NetworkFormat::
          INPUT_112_WITH_CANONICALIZATION_HECTOPLIES_ARMAGEDDON:
      case pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION_V2:
      case pblczero::NetworkFormat::
          INPUT_112_WITH_CANONICALIZATION_V2_ARMAGEDDON: {
        // - Plane 104 for positions of rooks (both white and black) which
        // have
        // a-side (queenside) castling right.
        // - Plane 105 for positions of rooks (both white and black) which have
        // h-side (kingside) castling right.
        const auto& cast = board.castlings();
        result[kAuxPlaneBase + 0].mask =
            ((cast.we_can_000() ? BoardSquare(ChessBoard::A1).as_board() : 0) |
             (cast.they_can_000() ? BoardSquare(ChessBoard::A8).as_board() : 0))
            << cast.queenside_rook();
        result[kAuxPlaneBase + 1].mask =
            ((cast.we_can_00() ? BoardSquare(ChessBoard::A1).as_board() : 0) |
             (cast.they_can_00() ? BoardSquare(ChessBoard::A8).as_board() : 0))
            << cast.kingside_rook();
        break;
      }
      default:
        throw Exception("Unsupported input plane encoding " +
                        std::to_string(input_format));
    };
    if (IsCanonicalFormat(input_format)) {
      result[kAuxPlaneBase + 4].mask = board.en_passant().as_int();
    } else {
      if (we_are_black) result[kAuxPlaneBase + 4].SetAll();
    }
    if (IsHectopliesFormat(input_format)) {
      result[kAuxPlaneBase + 5].Fill(history.Last().GetRule50Ply() / 100.0f);
    } else {
      result[kAuxPlaneBase + 5].Fill(history.Last().GetRule50Ply());
    }
    // Plane kAuxPlaneBase + 6 used to be movecount plane, now it's all zeros
    // unless we need it for canonical armageddon side to move.
    if (IsCanonicalArmageddonFormat(input_format)) {
      if (we_are_black) result[kAuxPlaneBase + 6].SetAll();
    }
    // Plane kAuxPlaneBase + 7 is all ones to help NN find board edges.
    result[kAuxPlaneBase + 7].SetAll();
    if (stop_early) {
      castlings = board.castlings();
    }
  }
  bool skip_non_repeats =
      input_format ==
          pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION_V2 ||
      input_format == pblczero::NetworkFormat::
                          INPUT_112_WITH_CANONICALIZATION_V2_ARMAGEDDON;
  bool flip = false;
  int history_idx = history.GetLength() - 1;
  for (int i = 0; i < std::min(history_planes, kMoveHistory);
       ++i, --history_idx) {
    const Position& position =
        history.GetPositionAt(history_idx < 0 ? 0 : history_idx);
    const ChessBoard& board =
        flip ? position.GetThemBoard() : position.GetBoard();
    // Castling changes can't be repeated, so we can stop early.
    if (stop_early && board.castlings().as_int() != castlings.as_int()) break;
    // Enpassants can't be repeated, but we do need to always send the current
    // position.
    if (stop_early && history_idx != history.GetLength() - 1 &&
        !board.en_passant().empty()) {
      break;
    }
    if (history_idx < 0 && fill_empty_history == FillEmptyHistory::NO) break;
    // Board may be flipped so compare with position.GetBoard().
    if (history_idx < 0 && fill_empty_history == FillEmptyHistory::FEN_ONLY &&
        position.GetBoard() == ChessBoard::kStartposBoard) {
      break;
    }
    const int repetitions = position.GetRepetitions();
    // Canonical v2 only writes an item if it is a repeat, unless its the most
    // recent position.
    if (skip_non_repeats && repetitions == 0 && i > 0) {
      if (history_idx > 0) flip = !flip;
      // If no capture no pawn is 0, the previous was start of game, capture or
      // pawn push, so there can't be any more repeats that are worth
      // considering.
      if (position.GetRule50Ply() == 0) break;
      // Decrement i so it remains the same as the history_idx decrements.
      --i;
      continue;
    }

    const int base = i * kPlanesPerBoard;
    result[base + 0].mask = (board.ours() & board.pawns()).as_int();
    result[base + 1].mask = (board.ours() & board.knights()).as_int();
    result[base + 2].mask = (board.ours() & board.bishops()).as_int();
    result[base + 3].mask = (board.ours() & board.rooks()).as_int();
    result[base + 4].mask = (board.ours() & board.queens()).as_int();
    result[base + 5].mask = (board.ours() & board.kings()).as_int();

    result[base + 6].mask = (board.theirs() & board.pawns()).as_int();
    result[base + 7].mask = (board.theirs() & board.knights()).as_int();
    result[base + 8].mask = (board.theirs() & board.bishops()).as_int();
    result[base + 9].mask = (board.theirs() & board.rooks()).as_int();
    result[base + 10].mask = (board.theirs() & board.queens()).as_int();
    result[base + 11].mask = (board.theirs() & board.kings()).as_int();

    if (repetitions >= 1) result[base + 12].SetAll();

    // If en passant flag is set, undo last pawn move by removing the pawn from
    // the new square and putting into pre-move square.
    if (history_idx < 0 && !board.en_passant().empty()) {
      const auto idx = GetLowestBit(board.en_passant().as_int());
      if (idx < 8) {  // "Us" board
        result[base + 0].mask +=
            ((0x0000000000000100ULL - 0x0000000001000000ULL) << idx);
      } else {
        result[base + 6].mask +=
            ((0x0001000000000000ULL - 0x0000000100000000ULL) << (idx - 56));
      }
    }
    if (history_idx > 0) flip = !flip;
    // If no capture no pawn is 0, the previous was start of game, capture or
    // pawn push, so no need to go back further if stopping early.
    if (stop_early && position.GetRule50Ply() == 0) break;
  }
  if (transform != NoTransform) {
    // Transform all masks.
    for (int i = 0; i <= kAuxPlaneBase + 4; i++) {
      auto v = result[i].mask;
      if (v == 0 || v == ~0ULL) continue;
      if ((transform & FlipTransform) != 0) {
        v = ReverseBitsInBytes(v);
      }
      if ((transform & MirrorTransform) != 0) {
        v = ReverseBytesInBytes(v);
      }
      if ((transform & TransposeTransform) != 0) {
        v = TransposeBitsInBytes(v);
      }
      result[i].mask = v;
    }
  }
  if (transform_out) *transform_out = transform;
  return result;
}

}  // namespace lczero

```

`src/neural/encoder.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include "chess/position.h"
#include "neural/network.h"
#include "proto/net.pb.h"

namespace lczero {

constexpr int kMoveHistory = 8;
constexpr int kPlanesPerBoard = 13;
constexpr int kAuxPlaneBase = kPlanesPerBoard * kMoveHistory;

enum class FillEmptyHistory { NO, FEN_ONLY, ALWAYS };

// Returns the transform that would be used in EncodePositionForNN.
int TransformForPosition(pblczero::NetworkFormat::InputFormat input_format,
                         const PositionHistory& history);

// Encodes the last position in history for the neural network request.
InputPlanes EncodePositionForNN(
    pblczero::NetworkFormat::InputFormat input_format,
    const PositionHistory& history, int history_planes,
    FillEmptyHistory fill_empty_history, int* transform_out);

bool IsCanonicalFormat(pblczero::NetworkFormat::InputFormat input_format);
bool IsCanonicalArmageddonFormat(
    pblczero::NetworkFormat::InputFormat input_format);
bool IsHectopliesFormat(pblczero::NetworkFormat::InputFormat input_format);
bool Is960CastlingFormat(pblczero::NetworkFormat::InputFormat input_format);

}  // namespace lczero

```

`src/neural/encoder_test.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
*/

#include "src/neural/encoder.h"

#include <gtest/gtest.h>

namespace lczero {

auto kAllSquaresMask = std::numeric_limits<std::uint64_t>::max();

TEST(EncodePositionForNN, EncodeStartPosition) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen(ChessBoard::kStartposFen);
  history.Reset(board, 0, 1);

  InputPlanes encoded_planes =
      EncodePositionForNN(pblczero::NetworkFormat::INPUT_CLASSICAL_112_PLANE,
                          history, 8, FillEmptyHistory::NO, nullptr);

  InputPlane our_pawns_plane = encoded_planes[0];
  auto our_pawns_mask = 0ull;
  for (auto i = 0; i < 8; i++) {
    // First pawn is at square a2 (position 8)
    // Last pawn is at square h2 (position 8 + 7 = 15)
    our_pawns_mask |= 1ull << (8 + i);
  }
  EXPECT_EQ(our_pawns_plane.mask, our_pawns_mask);
  EXPECT_EQ(our_pawns_plane.value, 1.0f);

  InputPlane our_knights_plane = encoded_planes[1];
  EXPECT_EQ(our_knights_plane.mask, (1ull << 1) | (1ull << 6));
  EXPECT_EQ(our_knights_plane.value, 1.0f);

  InputPlane our_bishops_plane = encoded_planes[2];
  EXPECT_EQ(our_bishops_plane.mask, (1ull << 2) | (1ull << 5));
  EXPECT_EQ(our_bishops_plane.value, 1.0f);

  InputPlane our_rooks_plane = encoded_planes[3];
  EXPECT_EQ(our_rooks_plane.mask, 1ull | (1ull << 7));
  EXPECT_EQ(our_rooks_plane.value, 1.0f);

  InputPlane our_queens_plane = encoded_planes[4];
  EXPECT_EQ(our_queens_plane.mask, 1ull << 3);
  EXPECT_EQ(our_queens_plane.value, 1.0f);

  InputPlane our_king_plane = encoded_planes[5];
  EXPECT_EQ(our_king_plane.mask, 1ull << 4);
  EXPECT_EQ(our_king_plane.value, 1.0f);

  // Sanity check opponent's pieces
  InputPlane their_king_plane = encoded_planes[11];
  auto their_king_row = 7;
  auto their_king_col = 4;
  EXPECT_EQ(their_king_plane.mask,
            1ull << (8 * their_king_row + their_king_col));
  EXPECT_EQ(their_king_plane.value, 1.0f);

  // Start of game, no history.
  for (int i = 0; i < 7; i++) {
    for (int j = 0; j < 13; j++) {
      InputPlane zeroed_history = encoded_planes[13 + i * 13 + j];
      EXPECT_EQ(zeroed_history.mask, 0ull);
    }
  }

  // Auxiliary planes

  // It's the start of the game, so all castlings should be allowed.
  for (auto i = 0; i < 4; i++) {
    InputPlane can_castle_plane = encoded_planes[13 * 8 + i];
    EXPECT_EQ(can_castle_plane.mask, kAllSquaresMask);
    EXPECT_EQ(can_castle_plane.value, 1.0f);
  }

  InputPlane we_are_black_plane = encoded_planes[13 * 8 + 4];
  EXPECT_EQ(we_are_black_plane.mask, 0ull);

  InputPlane fifty_move_counter_plane = encoded_planes[13 * 8 + 5];
  EXPECT_EQ(fifty_move_counter_plane.mask, kAllSquaresMask);
  EXPECT_EQ(fifty_move_counter_plane.value, 0.0f);

  // We no longer encode the move count, so that plane should be all zeros
  InputPlane zeroed_move_count_plane = encoded_planes[13 * 8 + 6];
  EXPECT_EQ(zeroed_move_count_plane.mask, 0ull);

  InputPlane all_ones_plane = encoded_planes[13 * 8 + 7];
  EXPECT_EQ(all_ones_plane.mask, kAllSquaresMask);
  EXPECT_EQ(all_ones_plane.value, 1.0f);
}

TEST(EncodePositionForNN, EncodeStartPositionFormat2) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen(ChessBoard::kStartposFen);
  history.Reset(board, 0, 1);

  InputPlanes encoded_planes = EncodePositionForNN(
      pblczero::NetworkFormat::INPUT_112_WITH_CASTLING_PLANE, history, 8,
      FillEmptyHistory::NO, nullptr);

  InputPlane our_pawns_plane = encoded_planes[0];
  auto our_pawns_mask = 0ull;
  for (auto i = 0; i < 8; i++) {
    // First pawn is at square a2 (position 8)
    // Last pawn is at square h2 (position 8 + 7 = 15)
    our_pawns_mask |= 1ull << (8 + i);
  }
  EXPECT_EQ(our_pawns_plane.mask, our_pawns_mask);
  EXPECT_EQ(our_pawns_plane.value, 1.0f);

  InputPlane our_knights_plane = encoded_planes[1];
  EXPECT_EQ(our_knights_plane.mask, (1ull << 1) | (1ull << 6));
  EXPECT_EQ(our_knights_plane.value, 1.0f);

  InputPlane our_bishops_plane = encoded_planes[2];
  EXPECT_EQ(our_bishops_plane.mask, (1ull << 2) | (1ull << 5));
  EXPECT_EQ(our_bishops_plane.value, 1.0f);

  InputPlane our_rooks_plane = encoded_planes[3];
  EXPECT_EQ(our_rooks_plane.mask, 1ull | (1ull << 7));
  EXPECT_EQ(our_rooks_plane.value, 1.0f);

  InputPlane our_queens_plane = encoded_planes[4];
  EXPECT_EQ(our_queens_plane.mask, 1ull << 3);
  EXPECT_EQ(our_queens_plane.value, 1.0f);

  InputPlane our_king_plane = encoded_planes[5];
  EXPECT_EQ(our_king_plane.mask, 1ull << 4);
  EXPECT_EQ(our_king_plane.value, 1.0f);

  // Sanity check opponent's pieces
  InputPlane their_king_plane = encoded_planes[11];
  auto their_king_row = 7;
  auto their_king_col = 4;
  EXPECT_EQ(their_king_plane.mask,
            1ull << (8 * their_king_row + their_king_col));
  EXPECT_EQ(their_king_plane.value, 1.0f);

  // Start of game, no history.
  for (int i = 0; i < 7; i++) {
    for (int j = 0; j < 13; j++) {
      InputPlane zeroed_history = encoded_planes[13 + i * 13 + j];
      EXPECT_EQ(zeroed_history.mask, 0ull);
    }
  }

  // Auxiliary planes

  // Queen side castling at game start.
  InputPlane can_castle_plane = encoded_planes[13 * 8 + 0];
  EXPECT_EQ(can_castle_plane.mask, 1ull | (1ull << 56));
  EXPECT_EQ(can_castle_plane.value, 1.0f);
  // king side castling at game start.
  can_castle_plane = encoded_planes[13 * 8 + 1];
  EXPECT_EQ(can_castle_plane.mask, 1ull << 7 | (1ull << 63));
  EXPECT_EQ(can_castle_plane.value, 1.0f);

  // Zeroed castling planes.
  InputPlane zeroed_castling_plane = encoded_planes[13 * 8 + 2];
  EXPECT_EQ(zeroed_castling_plane.mask, 0ull);
  zeroed_castling_plane = encoded_planes[13 * 8 + 3];
  EXPECT_EQ(zeroed_castling_plane.mask, 0ull);

  InputPlane we_are_black_plane = encoded_planes[13 * 8 + 4];
  EXPECT_EQ(we_are_black_plane.mask, 0ull);

  InputPlane fifty_move_counter_plane = encoded_planes[13 * 8 + 5];
  EXPECT_EQ(fifty_move_counter_plane.mask, kAllSquaresMask);
  EXPECT_EQ(fifty_move_counter_plane.value, 0.0f);

  // We no longer encode the move count, so that plane should be all zeros
  InputPlane zeroed_move_count_plane = encoded_planes[13 * 8 + 6];
  EXPECT_EQ(zeroed_move_count_plane.mask, 0ull);

  InputPlane all_ones_plane = encoded_planes[13 * 8 + 7];
  EXPECT_EQ(all_ones_plane.mask, kAllSquaresMask);
  EXPECT_EQ(all_ones_plane.value, 1.0f);
}

TEST(EncodePositionForNN, EncodeStartPositionFormat3) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen(ChessBoard::kStartposFen);
  history.Reset(board, 0, 1);

  InputPlanes encoded_planes = EncodePositionForNN(
      pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION, history, 8,
      FillEmptyHistory::NO, nullptr);

  InputPlane our_pawns_plane = encoded_planes[0];
  auto our_pawns_mask = 0ull;
  for (auto i = 0; i < 8; i++) {
    // First pawn is at square a2 (position 8)
    // Last pawn is at square h2 (position 8 + 7 = 15)
    our_pawns_mask |= 1ull << (8 + i);
  }
  EXPECT_EQ(our_pawns_plane.mask, our_pawns_mask);
  EXPECT_EQ(our_pawns_plane.value, 1.0f);

  InputPlane our_knights_plane = encoded_planes[1];
  EXPECT_EQ(our_knights_plane.mask, (1ull << 1) | (1ull << 6));
  EXPECT_EQ(our_knights_plane.value, 1.0f);

  InputPlane our_bishops_plane = encoded_planes[2];
  EXPECT_EQ(our_bishops_plane.mask, (1ull << 2) | (1ull << 5));
  EXPECT_EQ(our_bishops_plane.value, 1.0f);

  InputPlane our_rooks_plane = encoded_planes[3];
  EXPECT_EQ(our_rooks_plane.mask, 1ull | (1ull << 7));
  EXPECT_EQ(our_rooks_plane.value, 1.0f);

  InputPlane our_queens_plane = encoded_planes[4];
  EXPECT_EQ(our_queens_plane.mask, 1ull << 3);
  EXPECT_EQ(our_queens_plane.value, 1.0f);

  InputPlane our_king_plane = encoded_planes[5];
  EXPECT_EQ(our_king_plane.mask, 1ull << 4);
  EXPECT_EQ(our_king_plane.value, 1.0f);

  // Sanity check opponent's pieces
  InputPlane their_king_plane = encoded_planes[11];
  auto their_king_row = 7;
  auto their_king_col = 4;
  EXPECT_EQ(their_king_plane.mask,
            1ull << (8 * their_king_row + their_king_col));
  EXPECT_EQ(their_king_plane.value, 1.0f);

  // Start of game, no history.
  for (int i = 0; i < 7; i++) {
    for (int j = 0; j < 13; j++) {
      InputPlane zeroed_history = encoded_planes[13 + i * 13 + j];
      EXPECT_EQ(zeroed_history.mask, 0ull);
    }
  }

  // Auxiliary planes

  // Queen side castling at game start.
  InputPlane can_castle_plane = encoded_planes[13 * 8 + 0];
  EXPECT_EQ(can_castle_plane.mask, 1ull | (1ull << 56));
  EXPECT_EQ(can_castle_plane.value, 1.0f);
  // king side castling at game start.
  can_castle_plane = encoded_planes[13 * 8 + 1];
  EXPECT_EQ(can_castle_plane.mask, 1ull << 7 | (1ull << 63));
  EXPECT_EQ(can_castle_plane.value, 1.0f);

  // Zeroed castling planes.
  InputPlane zeroed_castling_plane = encoded_planes[13 * 8 + 2];
  EXPECT_EQ(zeroed_castling_plane.mask, 0ull);
  zeroed_castling_plane = encoded_planes[13 * 8 + 3];
  EXPECT_EQ(zeroed_castling_plane.mask, 0ull);

  InputPlane enpassant_plane = encoded_planes[13 * 8 + 4];
  EXPECT_EQ(enpassant_plane.mask, 0ull);

  InputPlane fifty_move_counter_plane = encoded_planes[13 * 8 + 5];
  EXPECT_EQ(fifty_move_counter_plane.mask, kAllSquaresMask);
  EXPECT_EQ(fifty_move_counter_plane.value, 0.0f);

  // We no longer encode the move count, so that plane should be all zeros
  InputPlane zeroed_move_count_plane = encoded_planes[13 * 8 + 6];
  EXPECT_EQ(zeroed_move_count_plane.mask, 0ull);

  InputPlane all_ones_plane = encoded_planes[13 * 8 + 7];
  EXPECT_EQ(all_ones_plane.mask, kAllSquaresMask);
  EXPECT_EQ(all_ones_plane.value, 1.0f);
}

TEST(EncodePositionForNN, EncodeFiftyMoveCounter) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen(ChessBoard::kStartposFen);
  history.Reset(board, 0, 1);

  // 1. Nf3
  history.Append(Move("g1f3", false));

  InputPlanes encoded_planes =
      EncodePositionForNN(pblczero::NetworkFormat::INPUT_CLASSICAL_112_PLANE,
                          history, 8, FillEmptyHistory::NO, nullptr);

  InputPlane we_are_black_plane = encoded_planes[13 * 8 + 4];
  EXPECT_EQ(we_are_black_plane.mask, kAllSquaresMask);
  EXPECT_EQ(we_are_black_plane.value, 1.0f);

  InputPlane fifty_move_counter_plane = encoded_planes[13 * 8 + 5];
  EXPECT_EQ(fifty_move_counter_plane.mask, kAllSquaresMask);
  EXPECT_EQ(fifty_move_counter_plane.value, 1.0f);

  // 1. Nf3 Nf6
  history.Append(Move("g8f6", true));

  encoded_planes =
      EncodePositionForNN(pblczero::NetworkFormat::INPUT_CLASSICAL_112_PLANE,
                          history, 8, FillEmptyHistory::NO, nullptr);

  we_are_black_plane = encoded_planes[13 * 8 + 4];
  EXPECT_EQ(we_are_black_plane.mask, 0ull);

  fifty_move_counter_plane = encoded_planes[13 * 8 + 5];
  EXPECT_EQ(fifty_move_counter_plane.mask, kAllSquaresMask);
  EXPECT_EQ(fifty_move_counter_plane.value, 2.0f);
}

TEST(EncodePositionForNN, EncodeFiftyMoveCounterFormat3) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen(ChessBoard::kStartposFen);
  history.Reset(board, 0, 1);

  // 1. Nf3
  history.Append(Move("g1f3", false));

  InputPlanes encoded_planes = EncodePositionForNN(
      pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION, history, 8,
      FillEmptyHistory::NO, nullptr);

  InputPlane enpassant_plane = encoded_planes[13 * 8 + 4];
  EXPECT_EQ(enpassant_plane.mask, 0ull);

  InputPlane fifty_move_counter_plane = encoded_planes[13 * 8 + 5];
  EXPECT_EQ(fifty_move_counter_plane.mask, kAllSquaresMask);
  EXPECT_EQ(fifty_move_counter_plane.value, 1.0f);

  // 1. Nf3 Nf6
  history.Append(Move("g8f6", true));

  encoded_planes = EncodePositionForNN(
      pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION, history, 8,
      FillEmptyHistory::NO, nullptr);

  enpassant_plane = encoded_planes[13 * 8 + 4];
  EXPECT_EQ(enpassant_plane.mask, 0ull);

  fifty_move_counter_plane = encoded_planes[13 * 8 + 5];
  EXPECT_EQ(fifty_move_counter_plane.mask, kAllSquaresMask);
  EXPECT_EQ(fifty_move_counter_plane.value, 2.0f);
}

TEST(EncodePositionForNN, EncodeEndGameFormat1) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen("3r4/4k3/8/1K6/8/8/8/8 w - - 0 1");
  history.Reset(board, 0, 1);

  int transform;
  InputPlanes encoded_planes =
      EncodePositionForNN(pblczero::NetworkFormat::INPUT_CLASSICAL_112_PLANE,
                          history, 8, FillEmptyHistory::NO, &transform);

  EXPECT_EQ(transform, NoTransform);

  InputPlane our_king_plane = encoded_planes[5];
  EXPECT_EQ(our_king_plane.mask, 1ull << 33);
  EXPECT_EQ(our_king_plane.value, 1.0f);
  InputPlane their_king_plane = encoded_planes[11];
  EXPECT_EQ(their_king_plane.mask, 1ull << 52);
  EXPECT_EQ(their_king_plane.value, 1.0f);
}

TEST(EncodePositionForNN, EncodeEndGameFormat3) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen("3r4/4k3/8/1K6/8/8/8/8 w - - 0 1");
  history.Reset(board, 0, 1);

  int transform;
  InputPlanes encoded_planes = EncodePositionForNN(
      pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION, history, 8,
      FillEmptyHistory::NO, &transform);

  EXPECT_EQ(transform, FlipTransform | MirrorTransform | TransposeTransform);

  InputPlane our_king_plane = encoded_planes[5];
  EXPECT_EQ(our_king_plane.mask, 1ull << 12);
  EXPECT_EQ(our_king_plane.value, 1.0f);
  InputPlane their_king_plane = encoded_planes[11];
  EXPECT_EQ(their_king_plane.mask, 1ull << 38);
  EXPECT_EQ(their_king_plane.value, 1.0f);
}

TEST(EncodePositionForNN, EncodeEndGameKingOnDiagonalFormat3) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen("3r4/4k3/2K5/8/8/8/8/8 w - - 0 1");
  history.Reset(board, 0, 1);

  int transform;
  InputPlanes encoded_planes = EncodePositionForNN(
      pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION, history, 8,
      FillEmptyHistory::NO, &transform);

  // After mirroring transforms, our king is on diagonal and other pieces are
  // all below the diagonal, so transposing will increase the value of ours |
  // theirs.
  EXPECT_EQ(transform, FlipTransform | MirrorTransform);

  InputPlane our_king_plane = encoded_planes[5];
  EXPECT_EQ(our_king_plane.mask, 1ull << 21);
  EXPECT_EQ(our_king_plane.value, 1.0f);
  InputPlane their_king_plane = encoded_planes[11];
  EXPECT_EQ(their_king_plane.mask, 1ull << 11);
  EXPECT_EQ(their_king_plane.value, 1.0f);
}

TEST(EncodePositionForNN, EncodeEnpassantFormat3) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen(ChessBoard::kStartposFen);
  history.Reset(board, 0, 1);
  // Move to en passant.
  history.Append(Move("e2e4", false));
  history.Append(Move("g2g3", false));
  history.Append(Move("e4e5", false));
  history.Append(Move("f2f4", false));

  InputPlanes encoded_planes = EncodePositionForNN(
      pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION, history, 8,
      FillEmptyHistory::NO, nullptr);

  InputPlane enpassant_plane = encoded_planes[13 * 8 + 4];
  EXPECT_EQ(enpassant_plane.mask, 1ull << 61);

  // Pawn move, no history.
  for (int i = 0; i < 7; i++) {
    for (int j = 0; j < 13; j++) {
      InputPlane zeroed_history = encoded_planes[13 + i * 13 + j];
      EXPECT_EQ(zeroed_history.mask, 0ull);
    }
  }

  // Boring move.
  history.Append(Move("g1f3", false));

  encoded_planes = EncodePositionForNN(
      pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION, history, 8,
      FillEmptyHistory::NO, nullptr);

  // No more en passant bit.
  enpassant_plane = encoded_planes[13 * 8 + 4];
  EXPECT_EQ(enpassant_plane.mask, 0ull);

  // Previous was en passant, no history.
  for (int i = 0; i < 7; i++) {
    for (int j = 0; j < 13; j++) {
      InputPlane zeroed_history = encoded_planes[13 + i * 13 + j];
      EXPECT_EQ(zeroed_history.mask, 0ull);
    }
  }

  // Another boring move.
  history.Append(Move("g1f3", false));

  encoded_planes = EncodePositionForNN(
      pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION, history, 8,
      FillEmptyHistory::NO, nullptr);

  // Should be one plane of history.
  for (int i = 0; i < 7; i++) {
    for (int j = 0; j < 13; j++) {
      InputPlane zeroed_history = encoded_planes[13 + i * 13 + j];
      // 13th plane of first layer is repeats and there are none, so it should
      // be empty.
      if (i == 0 && j < 12) {
        EXPECT_NE(zeroed_history.mask, 0ull);
      } else {
        EXPECT_EQ(zeroed_history.mask, 0ull);
      }
    }
  }
}

TEST(EncodePositionForNN, EncodeEarlyGameFlipFormat3) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen(ChessBoard::kStartposFen);
  history.Reset(board, 0, 1);
  // Move to break castling and king offside.
  history.Append(Move("e2e4", false));
  history.Append(Move("e2e4", false));
  history.Append(Move("e1e2", false));
  history.Append(Move("e1e2", false));
  history.Append(Move("e2d3", false));
  // Their king offside, but not ours.

  int transform;
  InputPlanes encoded_planes = EncodePositionForNN(
      pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION, history, 8,
      FillEmptyHistory::NO, &transform);

  EXPECT_EQ(transform, NoTransform);

  InputPlane our_king_plane = encoded_planes[5];
  EXPECT_EQ(our_king_plane.mask, 1ull << 12);
  EXPECT_EQ(our_king_plane.value, 1.0f);
  InputPlane their_king_plane = encoded_planes[11];
  EXPECT_EQ(their_king_plane.mask, 1ull << 43);
  EXPECT_EQ(their_king_plane.value, 1.0f);

  history.Append(Move("e2e3", false));

  // Our king offside, but theirs is not.
  encoded_planes = EncodePositionForNN(
      pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION, history, 8,
      FillEmptyHistory::NO, &transform);

  EXPECT_EQ(transform, FlipTransform);

  our_king_plane = encoded_planes[5];
  EXPECT_EQ(our_king_plane.mask, 1ull << 20);
  EXPECT_EQ(our_king_plane.value, 1.0f);
  their_king_plane = encoded_planes[11];
  EXPECT_EQ(their_king_plane.mask, 1ull << 43);
  EXPECT_EQ(their_king_plane.value, 1.0f);
}

}  // namespace lczero

int main(int argc, char** argv) {
  ::testing::InitGoogleTest(&argc, argv);
  lczero::InitializeMagicBitboards();
  return RUN_ALL_TESTS();
}

```

`src/neural/factory.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "neural/factory.h"

#include <algorithm>

#include "neural/loader.h"
#include "utils/commandline.h"
#include "utils/logging.h"

namespace lczero {

const OptionId NetworkFactory::kWeightsId{
    "weights", "WeightsFile",
    "Path from which to load network weights.\nSetting it to <autodiscover> "
    "makes it search in ./ and ./weights/ subdirectories for the latest (by "
    "file date) file which looks like weights.",
    'w'};
const OptionId NetworkFactory::kBackendId{
    "backend", "Backend", "Neural network computational backend to use.", 'b'};
const OptionId NetworkFactory::kBackendOptionsId{
    "backend-opts", "BackendOptions",
    "Parameters of neural network backend. "
    "Exact parameters differ per backend.",
    'o'};
const char* kAutoDiscover = "<autodiscover>";
const char* kEmbed = "<built in>";

NetworkFactory* NetworkFactory::Get() {
  static NetworkFactory factory;
  return &factory;
}

NetworkFactory::Register::Register(const std::string& name, FactoryFunc factory,
                                   int priority) {
  NetworkFactory::Get()->RegisterNetwork(name, factory, priority);
}

void NetworkFactory::PopulateOptions(OptionsParser* options) {
#if defined(EMBED)
  options->Add<StringOption>(NetworkFactory::kWeightsId) = kEmbed;
#else
  options->Add<StringOption>(NetworkFactory::kWeightsId) = kAutoDiscover;
#endif
  const auto backends = NetworkFactory::Get()->GetBackendsList();
  options->Add<ChoiceOption>(NetworkFactory::kBackendId, backends) =
      backends.empty() ? "<none>" : backends[0];
  options->Add<StringOption>(NetworkFactory::kBackendOptionsId);
}

void NetworkFactory::RegisterNetwork(const std::string& name,
                                     FactoryFunc factory, int priority) {
  factories_.emplace_back(name, factory, priority);
  std::sort(factories_.begin(), factories_.end());
}

std::vector<std::string> NetworkFactory::GetBackendsList() const {
  std::vector<std::string> result;
  for (const auto& x : factories_) result.emplace_back(x.name);
  return result;
}

std::unique_ptr<Network> NetworkFactory::Create(
    const std::string& network, const std::optional<WeightsFile>& weights,
    const OptionsDict& options) {
  CERR << "Creating backend [" << network << "]...";
  for (const auto& factory : factories_) {
    if (factory.name == network) {
      return factory.factory(weights, options);
    }
  }
  throw Exception("Unknown backend: " + network);
}

NetworkFactory::BackendConfiguration::BackendConfiguration(
    const OptionsDict& options)
    : weights_path(options.Get<std::string>(kWeightsId)),
      backend(options.Get<std::string>(kBackendId)),
      backend_options(options.Get<std::string>(kBackendOptionsId)) {}

bool NetworkFactory::BackendConfiguration::operator==(
    const BackendConfiguration& other) const {
  return (weights_path == other.weights_path && backend == other.backend &&
          backend_options == other.backend_options);
}

std::unique_ptr<Network> NetworkFactory::LoadNetwork(
    const OptionsDict& options) {
  std::string net_path = options.Get<std::string>(kWeightsId);
  const std::string backend = options.Get<std::string>(kBackendId);
  const std::string backend_options =
      options.Get<std::string>(kBackendOptionsId);

  if (net_path == kAutoDiscover) {
    net_path = DiscoverWeightsFile();
  } else if (net_path == kEmbed) {
    net_path = CommandLine::BinaryName();
  } else {
    CERR << "Loading weights file from: " << net_path;
  }
  std::optional<WeightsFile> weights;
  if (!net_path.empty()) {
    weights = LoadWeightsFromFile(net_path);
  }

  OptionsDict network_options(&options);
  network_options.AddSubdictFromString(backend_options);

  auto ptr = NetworkFactory::Get()->Create(backend, weights, network_options);
  network_options.CheckAllOptionsRead(backend);
  return ptr;
}

}  // namespace lczero

```

`src/neural/factory.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <functional>
#include <optional>
#include <string>

#include "neural/loader.h"
#include "neural/network.h"
#include "utils/optionsdict.h"
#include "utils/optionsparser.h"

namespace lczero {

class NetworkFactory {
 public:
  using FactoryFunc = std::function<std::unique_ptr<Network>(
      const std::optional<WeightsFile>&, const OptionsDict&)>;

  static NetworkFactory* Get();

  // Registers network so it can be created by name.
  // @name -- name
  // @options -- options to pass to the network
  // @priority -- how high should be the network in the list. The network with
  //              the highest priority is the default.
  class Register {
   public:
    Register(const std::string& name, FactoryFunc factory, int priority = 0);
  };

  // Add the network/backend parameters to the options dictionary.
  static void PopulateOptions(OptionsParser* options);

  // Returns list of backend names, sorted by priority (higher priority first).
  std::vector<std::string> GetBackendsList() const;

  // Creates a backend given name and config.
  std::unique_ptr<Network> Create(const std::string& network,
                                  const std::optional<WeightsFile>&,
                                  const OptionsDict& options);

  // Helper function to load the network from the options. Returns nullptr
  // if no network options changed since the previous call.
  static std::unique_ptr<Network> LoadNetwork(const OptionsDict& options);

  // Parameter IDs.
  static const OptionId kWeightsId;
  static const OptionId kBackendId;
  static const OptionId kBackendOptionsId;

  struct BackendConfiguration {
    BackendConfiguration() = default;
    BackendConfiguration(const OptionsDict& options);
    std::string weights_path;
    std::string backend;
    std::string backend_options;
    bool operator==(const BackendConfiguration& other) const;
    bool operator!=(const BackendConfiguration& other) const {
      return !operator==(other);
    }
    bool operator<(const BackendConfiguration& other) const {
      return std::tie(weights_path, backend, backend_options) <
             std::tie(other.weights_path, other.backend, other.backend_options);
    }
  };

 private:
  void RegisterNetwork(const std::string& name, FactoryFunc factory,
                       int priority);

  NetworkFactory() {}

  struct Factory {
    Factory(const std::string& name, FactoryFunc factory, int priority)
        : name(name), factory(factory), priority(priority) {}

    bool operator<(const Factory& other) const {
      if (priority != other.priority) return priority > other.priority;
      return name < other.name;
    }

    std::string name;
    FactoryFunc factory;
    int priority;
  };

  std::vector<Factory> factories_;
  friend class Register;
};

#define REGISTER_NETWORK_WITH_COUNTER2(name, func, priority, counter) \
  namespace {                                                         \
  static NetworkFactory::Register regH38fhs##counter(                 \
      name,                                                           \
      [](const std::optional<WeightsFile>& w, const OptionsDict& o) { \
        return func(w, o);                                            \
      },                                                              \
      priority);                                                      \
  }
#define REGISTER_NETWORK_WITH_COUNTER(name, func, priority, counter) \
  REGISTER_NETWORK_WITH_COUNTER2(name, func, priority, counter)

// Registers a Network.
// Constructor of a network class must have parameters:
// (const Weights& w, const OptionsDict& o)
// @name -- name under which the backend will be known in configs.
// @func -- Factory function for a backend.
//          std::unique_ptr<Network>(const WeightsFile&, const OptionsDict&)
// @priority -- numeric priority of a backend. Higher is higher, highest number
// is the default backend.
#define REGISTER_NETWORK(name, func, priority) \
  REGISTER_NETWORK_WITH_COUNTER(name, func, priority, __LINE__)
}  // namespace lczero

```

`src/neural/loader.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "neural/loader.h"

#include <zlib.h>

#include <algorithm>
#include <cassert>
#include <cctype>
#include <cstdio>
#include <fstream>
#include <sstream>
#include <string>

#include "proto/net.pb.h"
#include "utils/commandline.h"
#include "utils/exception.h"
#include "utils/filesystem.h"
#include "utils/logging.h"
#include "version.h"

#ifdef _WIN32
#include <io.h>
#else
#include <unistd.h>
#endif

namespace lczero {

namespace {
const std::uint32_t kWeightMagic = 0x1c0;

std::string DecompressGzip(const std::string& filename) {
  const int kStartingSize = 8 * 1024 * 1024;  // 8M
  std::string buffer;
  buffer.resize(kStartingSize);
  int bytes_read = 0;

  // Read whole file into a buffer.
  FILE* fp = fopen(filename.c_str(), "rb");
  if (!fp) {
    throw Exception("Cannot read weights from " + filename);
  }
  if (filename == CommandLine::BinaryName()) {
    // The network file should be appended at the end of the lc0 executable,
    // followed by the network file size and a "Lc0!" (0x2130634c) magic.
    int32_t size, magic;
    if (fseek(fp, -8, SEEK_END) || fread(&size, 4, 1, fp) != 1 ||
        fread(&magic, 4, 1, fp) != 1 || magic != 0x2130634c) {
      fclose(fp);
      throw Exception("No embedded file detected.");
    }
    fseek(fp, -size - 8, SEEK_END);
  }
  fflush(fp);
  gzFile file = gzdopen(dup(fileno(fp)), "rb");
  fclose(fp);
  if (!file) {
    throw Exception("Cannot process file " + filename);
  }
  while (true) {
    const int sz =
        gzread(file, &buffer[bytes_read], buffer.size() - bytes_read);
    if (sz < 0) {
      int errnum;
      throw Exception(gzerror(file, &errnum));
    }
    if (sz == static_cast<int>(buffer.size()) - bytes_read) {
      bytes_read = buffer.size();
      buffer.resize(buffer.size() * 2);
    } else {
      bytes_read += sz;
      buffer.resize(bytes_read);
      break;
    }
  }
  gzclose(file);

  return buffer;
}

void FixOlderWeightsFile(WeightsFile* file) {
  using nf = pblczero::NetworkFormat;
  auto network_format = file->format().network_format().network();
  const auto has_network_format = file->format().has_network_format();
  if (has_network_format && network_format != nf::NETWORK_CLASSICAL &&
      network_format != nf::NETWORK_SE) {
    // Already in a new format, return unchanged.
    return;
  }

  auto* net = file->mutable_format()->mutable_network_format();
  if (!has_network_format) {
    // Older protobufs don't have format definition.
    net->set_input(nf::INPUT_CLASSICAL_112_PLANE);
    net->set_output(nf::OUTPUT_CLASSICAL);
    net->set_network(nf::NETWORK_CLASSICAL_WITH_HEADFORMAT);
    net->set_value(nf::VALUE_CLASSICAL);
    net->set_policy(nf::POLICY_CLASSICAL);
  } else if (network_format == pblczero::NetworkFormat::NETWORK_CLASSICAL) {
    // Populate policyFormat and valueFormat fields in old protobufs
    // without these fields.
    net->set_network(nf::NETWORK_CLASSICAL_WITH_HEADFORMAT);
    net->set_value(nf::VALUE_CLASSICAL);
    net->set_policy(nf::POLICY_CLASSICAL);
  } else if (network_format == pblczero::NetworkFormat::NETWORK_SE) {
    net->set_network(nf::NETWORK_SE_WITH_HEADFORMAT);
    net->set_value(nf::VALUE_CLASSICAL);
    net->set_policy(nf::POLICY_CLASSICAL);
  }
}

WeightsFile ParseWeightsProto(const std::string& buffer) {
  WeightsFile net;
  net.ParseFromString(buffer);

  if (net.magic() != kWeightMagic) {
    throw Exception("Invalid weight file: bad header.");
  }

  const auto min_version =
      GetVersionStr(net.min_version().major(), net.min_version().minor(),
                    net.min_version().patch(), "", "");
  const auto lc0_ver = GetVersionInt();
  const auto net_ver =
      GetVersionInt(net.min_version().major(), net.min_version().minor(),
                    net.min_version().patch());

  FixOlderWeightsFile(&net);

  // Weights files with this signature are also compatible.
  if (net_ver != 0x5c99973 && net_ver > lc0_ver) {
    throw Exception("Invalid weight file: lc0 version >= " + min_version +
                    " required.");
  }

  if (net.has_weights() &&
      net.format().weights_encoding() != pblczero::Format::LINEAR16) {
    throw Exception("Invalid weight file: unsupported encoding.");
  }

  return net;
}

}  // namespace

WeightsFile LoadWeightsFromFile(const std::string& filename) {
  FloatVectors vecs;
  auto buffer = DecompressGzip(filename);

  if (buffer.size() < 2) {
    throw Exception("Invalid weight file: too small.");
  }
  if (buffer[0] == '1' && buffer[1] == '\n') {
    throw Exception("Invalid weight file: no longer supported.");
  }
  if (buffer[0] == '2' && buffer[1] == '\n') {
    throw Exception(
        "Text format weights files are no longer supported. Use a command line "
        "tool to convert it to the new format.");
  }

  return ParseWeightsProto(buffer);
}

std::string DiscoverWeightsFile() {
  const int kMinFileSize = 500000;  // 500 KB

  std::vector<std::string> data_dirs = {CommandLine::BinaryDirectory()};
  const std::string user_data_path = GetUserDataDirectory();
  if (!user_data_path.empty()) {
    data_dirs.emplace_back(user_data_path + "lc0");
  }
  for (const auto& dir : GetSystemDataDirectoryList()) {
    data_dirs.emplace_back(dir + (dir.back() == '/' ? "" : "/") + "lc0");
  }

  for (const auto& dir : data_dirs) {
    // Open all files in <dir> amd <dir>/networks,
    // ones which are >= kMinFileSize are candidates.
    std::vector<std::pair<time_t, std::string> > time_and_filename;
    for (const auto& path : {"", "/networks"}) {
      for (const auto& file : GetFileList(dir + path)) {
        const std::string filename = dir + path + "/" + file;
        if (GetFileSize(filename) < kMinFileSize) continue;
        time_and_filename.emplace_back(GetFileTime(filename), filename);
      }
    }

    std::sort(time_and_filename.rbegin(), time_and_filename.rend());

    // Open all candidates, from newest to oldest, possibly gzipped, and try to
    // read version for it. If version is 2 or if the file is our protobuf,
    // return it.
    for (const auto& candidate : time_and_filename) {
      const gzFile file = gzopen(candidate.second.c_str(), "rb");

      if (!file) continue;
      unsigned char buf[256];
      int sz = gzread(file, buf, 256);
      gzclose(file);
      if (sz < 0) continue;

      std::string str(buf, buf + sz);
      std::istringstream data(str);
      int val = 0;
      data >> val;
      if (!data.fail() && val == 2) {
        CERR << "Found txt network file: " << candidate.second;
        return candidate.second;
      }

      // First byte of the protobuf stream is 0x0d for fixed32, so we ignore it
      // as our own magic should suffice.
      const auto magic = buf[1] | (static_cast<uint32_t>(buf[2]) << 8) |
                         (static_cast<uint32_t>(buf[3]) << 16) |
                         (static_cast<uint32_t>(buf[4]) << 24);
      if (magic == kWeightMagic) {
        CERR << "Found pb network file: " << candidate.second;
        return candidate.second;
      }
    }
  }
  LOGFILE << "Network weights file not found.";
  return {};
}

}  // namespace lczero

```

`src/neural/loader.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <string>
#include <vector>

#include "neural/network.h"
#include "proto/net.pb.h"

namespace lczero {

using FloatVector = std::vector<float>;
using FloatVectors = std::vector<FloatVector>;

using WeightsFile = pblczero::Net;

// Read weights file and fill the weights structure.
WeightsFile LoadWeightsFromFile(const std::string& filename);

// Tries to find a file which looks like a weights file, and located in
// directory of binary_name or one of subdirectories. If there are several such
// files, returns one which has the latest modification date.
std::string DiscoverWeightsFile();

}  // namespace lczero

```

`src/neural/network.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <memory>
#include <vector>

#include "proto/net.pb.h"
#include "utils/exception.h"

namespace lczero {

const int kInputPlanes = 112;

// All input planes are 64 value vectors, every element of which is either
// 0 or some value, unique for the plane. Therefore, input is defined as
// a bitmask showing where to set the value, and the value itself.
struct InputPlane {
  InputPlane() = default;
  void SetAll() { mask = ~0ull; }
  void Fill(float val) {
    SetAll();
    value = val;
  }
  std::uint64_t mask = 0ull;
  float value = 1.0f;
};
using InputPlanes = std::vector<InputPlane>;

// An interface to implement by computing backends.
class NetworkComputation {
 public:
  // Adds a sample to the batch.
  virtual void AddInput(InputPlanes&& input) = 0;
  // Do the computation.
  virtual void ComputeBlocking() = 0;
  // Returns how many times AddInput() was called.
  virtual int GetBatchSize() const = 0;
  // Returns Q value of @sample.
  virtual float GetQVal(int sample) const = 0;
  virtual float GetDVal(int sample) const = 0;
  // Returns P value @move_id of @sample.
  virtual float GetPVal(int sample, int move_id) const = 0;
  virtual float GetMVal(int sample) const = 0;
  virtual ~NetworkComputation() = default;
};

// The plan:
// 1. Search must not look directly into any fields of NetworkFormat anymore.
// 2. Backends populate NetworkCapabilities that show search how to use NN, both
//    for input and output.
// 3. Input part of NetworkCapabilities is just copy of InputFormat for now, and
//    is likely to stay so (because search not knowing how to use NN is not very
//    useful), but it's fine if it will change.
// 4. On the other hand, output part of NetworkCapabilities is set of
//    independent parameters (like WDL, moves left head etc), because search can
//    look what's set and act accordingly. Backends may derive it from
//    output head format fields or other places.

struct NetworkCapabilities {
  pblczero::NetworkFormat::InputFormat input_format;
  pblczero::NetworkFormat::MovesLeftFormat moves_left;
  // TODO expose information of whether GetDVal() is usable or always zero.

  // Combines capabilities by setting the most restrictive ones. May throw
  // exception.
  void Merge(const NetworkCapabilities& other) {
    if (input_format != other.input_format) {
      throw Exception("Incompatible input formats, " +
                      std::to_string(input_format) + " vs " +
                      std::to_string(other.input_format));
    }
  }

  bool has_mlh() const {
    return moves_left !=
           pblczero::NetworkFormat::MovesLeftFormat::MOVES_LEFT_NONE;
  }
};

class Network {
 public:
  virtual const NetworkCapabilities& GetCapabilities() const = 0;
  virtual std::unique_ptr<NetworkComputation> NewComputation() = 0;
  virtual ~Network() = default;
};

}  // namespace lczero

```

`src/neural/network_check.cc`:

```cc
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018-2020 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
 */

#include <algorithm>
#include <cmath>
#include <iomanip>

#include "neural/decoder.h"
#include "neural/factory.h"
#include "neural/network.h"
#include "utils/histogram.h"
#include "utils/logging.h"
#include "utils/random.h"

namespace lczero {

namespace {

class CheckNetwork;

enum CheckMode {
  kCheckOnly,
  kErrorDisplay,
  kHistogram,
};

struct CheckParams {
  CheckMode mode;
  double absolute_tolerance;
  double relative_tolerance;
  pblczero::NetworkFormat::InputFormat input_format;
};

class CheckComputation : public NetworkComputation {
 public:
  CheckComputation(const CheckParams& params,
                   std::unique_ptr<NetworkComputation> work_comp,
                   std::unique_ptr<NetworkComputation> check_comp)
      : params_(params),
        work_comp_(std::move(work_comp)),
        check_comp_(std::move(check_comp)) {}

  void AddInput(InputPlanes&& input) override {
    InputPlanes x = input;
    InputPlanes y = input;
    work_comp_->AddInput(std::move(x));
    check_comp_->AddInput(std::move(y));

    ChessBoard board;
    int rule50;
    int gameply;
    PopulateBoard(params_.input_format, input, &board, &rule50, &gameply);
    moves_.emplace_back(board.GenerateLegalMoves());
  }

  void ComputeBlocking() override {
    work_comp_->ComputeBlocking();
    check_comp_->ComputeBlocking();
    switch (params_.mode) {
      case kCheckOnly:
        CheckOnly();
        break;
      case kErrorDisplay:
        DisplayError();
        break;
      case kHistogram:
        DisplayHistogram();
        break;
    }
  }

  int GetBatchSize() const override {
    return static_cast<int>(work_comp_->GetBatchSize());
  }

  float GetQVal(int sample) const override {
    return work_comp_->GetQVal(sample);
  }

  float GetDVal(int sample) const override {
    return work_comp_->GetDVal(sample);
  }

  float GetMVal(int sample) const override {
    return work_comp_->GetMVal(sample);
  }

  float GetPVal(int sample, int move_id) const override {
    return work_comp_->GetPVal(sample, move_id);
  }

 private:
  static constexpr int kNumOutputPolicies = 1858;
  const CheckParams& params_;
  std::vector<MoveList> moves_;

  std::vector<float> PolicySoftMax(const NetworkComputation* comp, int sample,
                                   const std::vector<Move>& moves) const {
    float max_p = -std::numeric_limits<float>::infinity();
    std::vector<float> policy;
    policy.reserve(moves.size());
    for (const auto move : moves) {
      policy.emplace_back(comp->GetPVal(sample, move.as_nn_index(0)));
      max_p = std::max(max_p, policy.back());
    }
    float total = 0;
    for (auto& p : policy) {
      p = std::exp(p - max_p);
      total += p;
    }
    if (total > 0) {
      for (auto& p : policy) {
        p /= total;
      }
    }
    return policy;
  }

  void CheckOnly() const {
    bool valueAlmostEqual = true;
    const int size = GetBatchSize();
    for (int i = 0; i < size && valueAlmostEqual; i++) {
      const float v1 = work_comp_->GetQVal(i);
      const float v2 = check_comp_->GetQVal(i);
      valueAlmostEqual &= IsAlmostEqual(v1, v2);
    }

    bool policyAlmostEqual = true;
    for (int i = 0; i < size && policyAlmostEqual; i++) {
      const auto work = PolicySoftMax(work_comp_.get(), i, moves_[i]);
      const auto check = PolicySoftMax(check_comp_.get(), i, moves_[i]);
      for (size_t j = 0; j < work.size(); j++) {
        policyAlmostEqual &= IsAlmostEqual(work[j], check[j]);
      }
    }

    if (valueAlmostEqual && policyAlmostEqual) {
      CERR << "Check passed for a batch of " << size << ".";
      return;
    }

    if (!valueAlmostEqual && !policyAlmostEqual) {
      CERR << "*** ERROR check failed for a batch of " << size
           << " both value and policy incorrect.";
      return;
    }

    if (!valueAlmostEqual) {
      CERR << "*** ERROR check failed for a batch of " << size
           << " value incorrect (but policy ok).";
      return;
    }

    CERR << "*** ERROR check failed for a batch of " << size
         << " policy incorrect (but value ok).";
  }

  bool IsAlmostEqual(double a, double b) const {
    return std::abs(a - b) <= std::max(params_.relative_tolerance *
                                           std::max(std::abs(a), std::abs(b)),
                                       params_.absolute_tolerance);
  }

  void DisplayHistogram() {
    Histogram histogram(-15, 1, 5);

    const int size = GetBatchSize();
    for (int i = 0; i < size; i++) {
      const float qv1 = work_comp_->GetQVal(i);
      const float qv2 = check_comp_->GetQVal(i);
      histogram.Add(qv2 - qv1);
      const auto work = PolicySoftMax(work_comp_.get(), i, moves_[i]);
      const auto check = PolicySoftMax(check_comp_.get(), i, moves_[i]);
      for (size_t j = 0; j < work.size(); j++) {
        histogram.Add(check[j] - work[j]);
      }
    }
    CERR << "Absolute error histogram for a batch of " << size;
    histogram.Dump();
  }

  // Compute maximum absolute/relative errors.
  struct MaximumError {
    double max_absolute_error = 0;
    double max_relative_error = 0;

    void Add(double a, double b) {
      const double absolute_error = GetAbsoluteError(a, b);
      if (absolute_error > max_absolute_error) {
        max_absolute_error = absolute_error;
      }
      const double relative_error = GetRelativeError(a, b);
      if (relative_error > max_relative_error) {
        max_relative_error = relative_error;
      }
    }

    void Dump(const char* name) {
      CERR << std::scientific << std::setprecision(1) << name
           << ": absolute: " << max_absolute_error
           << ", relative: " << max_relative_error << ".";
    }

    static double GetRelativeError(double a, double b) {
      const double max = std::max(std::abs(a), std::abs(b));
      return max == 0 ? 0 : std::abs(a - b) / max;
    }

    static double GetAbsoluteError(double a, double b) {
      return std::abs(a - b);
    }
  };

  void DisplayError() {
    MaximumError value_error;
    const int size = GetBatchSize();
    for (int i = 0; i < size; i++) {
      const float v1 = work_comp_->GetQVal(i);
      const float v2 = check_comp_->GetQVal(i);
      value_error.Add(v1, v2);
    }

    MaximumError policy_error;
    for (int i = 0; i < size; i++) {
      const auto work = PolicySoftMax(work_comp_.get(), i, moves_[i]);
      const auto check = PolicySoftMax(check_comp_.get(), i, moves_[i]);
      for (size_t j = 0; j < work.size(); j++) {
        policy_error.Add(work[j], check[j]);
      }
    }

    CERR << "maximum error for a batch of " << size << ":";

    value_error.Dump("  value");
    policy_error.Dump("  policy");
  }

  std::unique_ptr<NetworkComputation> work_comp_;
  std::unique_ptr<NetworkComputation> check_comp_;
};

class CheckNetwork : public Network {
 public:
  static constexpr CheckMode kDefaultMode = kCheckOnly;
  static constexpr double kDefaultCheckFrequency = 0.2;
  static constexpr double kDefaultAbsoluteTolerance = 1e-5;
  static constexpr double kDefaultRelativeTolerance = 1e-4;

  CheckNetwork(const std::optional<WeightsFile>& weights,
               const OptionsDict& options) {
    params_.mode = kDefaultMode;
    params_.absolute_tolerance = kDefaultAbsoluteTolerance;
    params_.relative_tolerance = kDefaultRelativeTolerance;
    check_frequency_ = kDefaultCheckFrequency;

    OptionsDict dict1;
    std::string backendName1 = "opencl";
    OptionsDict& backend1_dict = dict1;

    OptionsDict dict2;
    std::string backendName2 = "eigen";
    OptionsDict& backend2_dict = dict2;

    const std::string mode = options.GetOrDefault<std::string>("mode", "check");
    if (mode == "check") {
      params_.mode = kCheckOnly;
    } else if (mode == "histo") {
      params_.mode = kHistogram;
    } else if (mode == "display") {
      params_.mode = kErrorDisplay;
    }

    params_.absolute_tolerance =
        options.GetOrDefault<float>("atol", kDefaultAbsoluteTolerance);
    params_.relative_tolerance =
        options.GetOrDefault<float>("rtol", kDefaultRelativeTolerance);

    const auto parents = options.ListSubdicts();
    if (parents.size() > 0) {
      backendName1 = parents[0];
      backend1_dict = options.GetSubdict(backendName1);
      backendName1 =
          backend1_dict.GetOrDefault<std::string>("backend", backendName1);
    }
    if (parents.size() > 1) {
      backendName2 = parents[1];
      backend2_dict = options.GetSubdict(backendName2);
      backendName2 =
          backend2_dict.GetOrDefault<std::string>("backend", backendName2);
    }
    if (parents.size() > 2) {
      CERR << "Warning, cannot check more than two backends";
    }

    CERR << "Working backend set to " << backendName1 << ".";
    CERR << "Reference backend set to " << backendName2 << ".";

    work_net_ =
        NetworkFactory::Get()->Create(backendName1, weights, backend1_dict);
    check_net_ =
        NetworkFactory::Get()->Create(backendName2, weights, backend2_dict);

    capabilities_ = work_net_->GetCapabilities();
    capabilities_.Merge(check_net_->GetCapabilities());

    params_.input_format = capabilities_.input_format;

    check_frequency_ =
        options.GetOrDefault<float>("freq", kDefaultCheckFrequency);
    switch (params_.mode) {
      case kCheckOnly:
        CERR << std::scientific << std::setprecision(1)
             << "Check mode: check only with relative tolerance "
             << params_.relative_tolerance << ", absolute tolerance "
             << params_.absolute_tolerance << ".";
        break;
      case kErrorDisplay:
        CERR << "Check mode: error display.";
        break;
      case kHistogram:
        CERR << "Check mode: histogram.";
        break;
    }
    CERR << "Check rate: " << std::fixed << std::setprecision(0)
         << 100 * check_frequency_ << "%.";
  }

  std::unique_ptr<NetworkComputation> NewComputation() override {
    const double draw = Random::Get().GetDouble(1.0);
    const bool check = draw < check_frequency_;
    if (check) {
      std::unique_ptr<NetworkComputation> work_comp =
          work_net_->NewComputation();
      std::unique_ptr<NetworkComputation> check_comp =
          check_net_->NewComputation();
      return std::make_unique<CheckComputation>(params_, std::move(work_comp),
                                                std::move(check_comp));
    }
    return work_net_->NewComputation();
  }

  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

 private:
  CheckParams params_;

  // How frequently an iteration is checked (0: never, 1: always).
  double check_frequency_;
  std::unique_ptr<Network> work_net_;
  std::unique_ptr<Network> check_net_;
  NetworkCapabilities capabilities_;
};

std::unique_ptr<Network> MakeCheckNetwork(
    const std::optional<WeightsFile>& weights, const OptionsDict& options) {
  return std::make_unique<CheckNetwork>(weights, options);
}

REGISTER_NETWORK("check", MakeCheckNetwork, -800)

}  // namespace
}  // namespace lczero

```

`src/neural/network_demux.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <condition_variable>
#include <queue>
#include <thread>

#include "neural/factory.h"
#include "utils/exception.h"
#include "utils/numa.h"

namespace lczero {
namespace {

class DemuxingNetwork;
class DemuxingComputation : public NetworkComputation {
 public:
  DemuxingComputation(DemuxingNetwork* network) : network_(network) {}

  void AddInput(InputPlanes&& input) override { planes_.emplace_back(input); }

  void ComputeBlocking() override;

  int GetBatchSize() const override { return planes_.size(); }

  float GetQVal(int sample) const override {
    const int idx = sample / partial_size_;
    const int offset = sample % partial_size_;
    return parents_[idx]->GetQVal(offset);
  }

  float GetDVal(int sample) const override {
    int idx = sample / partial_size_;
    int offset = sample % partial_size_;
    return parents_[idx]->GetDVal(offset);
  }

  float GetMVal(int sample) const override {
    int idx = sample / partial_size_;
    int offset = sample % partial_size_;
    return parents_[idx]->GetMVal(offset);
  }

  float GetPVal(int sample, int move_id) const override {
    const int idx = sample / partial_size_;
    const int offset = sample % partial_size_;
    return parents_[idx]->GetPVal(offset, move_id);
  }

  void NotifyComplete() {
    std::unique_lock<std::mutex> lock(mutex_);
    dataready_--;
    if (dataready_ == 0) {
      dataready_cv_.notify_one();
    }
  }

  NetworkComputation* AddParentFromNetwork(Network* network) {
    std::unique_lock<std::mutex> lock(mutex_);
    parents_.emplace_back(network->NewComputation());
    const int cur_idx = (parents_.size() - 1) * partial_size_;
    for (int i = cur_idx; i < std::min(GetBatchSize(), cur_idx + partial_size_);
         i++) {
      parents_.back()->AddInput(std::move(planes_[i]));
    }
    return parents_.back().get();
  }

 private:
  std::vector<InputPlanes> planes_;
  DemuxingNetwork* network_;
  std::vector<std::unique_ptr<NetworkComputation>> parents_;

  std::mutex mutex_;
  std::condition_variable dataready_cv_;
  int dataready_ = 0;
  int partial_size_ = 0;
};

class DemuxingNetwork : public Network {
 public:
  DemuxingNetwork(const std::optional<WeightsFile>& weights,
                  const OptionsDict& options) {
    minimum_split_size_ = options.GetOrDefault<int>("minimum-split-size", 0);
    const auto parents = options.ListSubdicts();
    if (parents.empty()) {
      // If options are empty, or multiplexer configured in root object,
      // initialize on root object and default backend.
      auto backends = NetworkFactory::Get()->GetBackendsList();
      AddBackend(backends[0], weights, options);
    }

    for (const auto& name : parents) {
      AddBackend(name, weights, options.GetSubdict(name));
    }
  }

  void AddBackend(const std::string& name,
                  const std::optional<WeightsFile>& weights,
                  const OptionsDict& opts) {
    const int nn_threads = opts.GetOrDefault<int>("threads", 1);
    const std::string backend = opts.GetOrDefault<std::string>("backend", name);

    networks_.emplace_back(
        NetworkFactory::Get()->Create(backend, weights, opts));

    if (networks_.size() == 1) {
      capabilities_ = networks_.back()->GetCapabilities();
    } else {
      capabilities_.Merge(networks_.back()->GetCapabilities());
    }

    for (int i = 0; i < nn_threads; ++i) {
      threads_.emplace_back([this, i]() { Worker(i); });
    }
  }

  std::unique_ptr<NetworkComputation> NewComputation() override {
    return std::make_unique<DemuxingComputation>(this);
  }

  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

  void Enqueue(DemuxingComputation* computation) {
    std::lock_guard<std::mutex> lock(mutex_);
    queue_.push(computation);
    cv_.notify_one();
  }

  ~DemuxingNetwork() {
    Abort();
    Wait();
    // Unstuck waiting computations.
    while (!queue_.empty()) {
      queue_.front()->NotifyComplete();
      queue_.pop();
    }
  }

  void Worker(int id) {
    // Add one to the id in order to leave space for an active search thread.
    Numa::BindThread(id + 1);
    // While Abort() is not called (and it can only be called from destructor).
    while (!abort_) {
      {
        {
          std::unique_lock<std::mutex> lock(mutex_);
          // Wait until there's come work to compute.
          cv_.wait(lock, [&] { return abort_ || !queue_.empty(); });
          if (abort_) break;
        }

        // While there is a work in queue, process it.
        while (true) {
          DemuxingComputation* to_notify;
          {
            std::unique_lock<std::mutex> lock(mutex_);
            if (queue_.empty()) break;
            to_notify = queue_.front();
            queue_.pop();
          }
          long long net_idx = ++(counter_) % networks_.size();
          NetworkComputation* to_compute =
              to_notify->AddParentFromNetwork(networks_[net_idx].get());
          to_compute->ComputeBlocking();
          to_notify->NotifyComplete();
        }
      }
    }
  }

  void Abort() {
    {
      std::lock_guard<std::mutex> lock(mutex_);
      abort_ = true;
    }
    cv_.notify_all();
  }

  void Wait() {
    while (!threads_.empty()) {
      threads_.back().join();
      threads_.pop_back();
    }
  }

  std::vector<std::unique_ptr<Network>> networks_;
  NetworkCapabilities capabilities_;
  std::queue<DemuxingComputation*> queue_;
  int minimum_split_size_ = 0;
  std::atomic<long long> counter_;
  bool abort_ = false;

  std::mutex mutex_;
  std::condition_variable cv_;

  std::vector<std::thread> threads_;
};

void DemuxingComputation::ComputeBlocking() {
  if (GetBatchSize() == 0) return;
  partial_size_ = (GetBatchSize() + network_->threads_.size() - 1) /
                  network_->threads_.size();
  if (partial_size_ < network_->minimum_split_size_) {
    partial_size_ = std::min(GetBatchSize(), network_->minimum_split_size_);
  }
  const int splits = (GetBatchSize() + partial_size_ - 1) / partial_size_;

  std::unique_lock<std::mutex> lock(mutex_);
  dataready_ = splits;
  for (int j = 0; j < splits; j++) {
    network_->Enqueue(this);
  }
  dataready_cv_.wait(lock, [this]() { return dataready_ == 0; });
}

std::unique_ptr<Network> MakeDemuxingNetwork(
    const std::optional<WeightsFile>& weights, const OptionsDict& options) {
  return std::make_unique<DemuxingNetwork>(weights, options);
}

REGISTER_NETWORK("demux", MakeDemuxingNetwork, -1001)

}  // namespace
}  // namespace lczero

```

`src/neural/network_legacy.cc`:

```cc
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018-2019 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#include "neural/network_legacy.h"

#include <algorithm>
#include <cmath>

#include "utils/weights_adapter.h"

namespace lczero {
namespace {
static constexpr float kEpsilon = 1e-5f;
}  // namespace

LegacyWeights::LegacyWeights(const pblczero::Weights& weights)
    : input(weights.input()),
      policy1(weights.policy1()),
      policy(weights.policy()),
      ip_pol_w(LayerAdapter(weights.ip_pol_w()).as_vector()),
      ip_pol_b(LayerAdapter(weights.ip_pol_b()).as_vector()),
      ip2_pol_w(LayerAdapter(weights.ip2_pol_w()).as_vector()),
      ip2_pol_b(LayerAdapter(weights.ip2_pol_b()).as_vector()),
      ip3_pol_w(LayerAdapter(weights.ip3_pol_w()).as_vector()),
      ip3_pol_b(LayerAdapter(weights.ip3_pol_b()).as_vector()),
      ip4_pol_w(LayerAdapter(weights.ip4_pol_w()).as_vector()),
      value(weights.value()),
      ip1_val_w(LayerAdapter(weights.ip1_val_w()).as_vector()),
      ip1_val_b(LayerAdapter(weights.ip1_val_b()).as_vector()),
      ip2_val_w(LayerAdapter(weights.ip2_val_w()).as_vector()),
      ip2_val_b(LayerAdapter(weights.ip2_val_b()).as_vector()),
      moves_left(weights.moves_left()),
      ip1_mov_w(LayerAdapter(weights.ip1_mov_w()).as_vector()),
      ip1_mov_b(LayerAdapter(weights.ip1_mov_b()).as_vector()),
      ip2_mov_w(LayerAdapter(weights.ip2_mov_w()).as_vector()),
      ip2_mov_b(LayerAdapter(weights.ip2_mov_b()).as_vector()) {
  for (const auto& res : weights.residual()) {
    residual.emplace_back(res);
  }
  pol_encoder_head_count = weights.pol_headcount();
  for (const auto& enc : weights.pol_encoder()) {
    pol_encoder.emplace_back(enc);
  }
}

LegacyWeights::SEunit::SEunit(const pblczero::Weights::SEunit& se)
    : w1(LayerAdapter(se.w1()).as_vector()),
      b1(LayerAdapter(se.b1()).as_vector()),
      w2(LayerAdapter(se.w2()).as_vector()),
      b2(LayerAdapter(se.b2()).as_vector()) {}

LegacyWeights::Residual::Residual(const pblczero::Weights::Residual& residual)
    : conv1(residual.conv1()),
      conv2(residual.conv2()),
      se(residual.se()),
      has_se(residual.has_se()) {}

LegacyWeights::ConvBlock::ConvBlock(const pblczero::Weights::ConvBlock& block)
    : weights(LayerAdapter(block.weights()).as_vector()),
      biases(LayerAdapter(block.biases()).as_vector()),
      bn_gammas(LayerAdapter(block.bn_gammas()).as_vector()),
      bn_betas(LayerAdapter(block.bn_betas()).as_vector()),
      bn_means(LayerAdapter(block.bn_means()).as_vector()),
      bn_stddivs(LayerAdapter(block.bn_stddivs()).as_vector()) {
  if (weights.size() == 0) {
    // Empty ConvBlock.
    return;
  }

  if (bn_betas.size() == 0) {
    // Old net without gamma and beta.
    for (auto i = size_t{0}; i < bn_means.size(); i++) {
      bn_betas.emplace_back(0.0f);
      bn_gammas.emplace_back(1.0f);
    }
  }
  if (biases.size() == 0) {
    for (auto i = size_t{0}; i < bn_means.size(); i++) {
      biases.emplace_back(0.0f);
    }
  }

  if (bn_means.size() == 0) {
    // No batch norm.
    return;
  }

  // Fold batch norm into weights and biases.
  // Variance to gamma.
  for (auto i = size_t{0}; i < bn_stddivs.size(); i++) {
    bn_gammas[i] *= 1.0f / std::sqrt(bn_stddivs[i] + kEpsilon);
    bn_means[i] -= biases[i];
  }

  auto outputs = biases.size();

  // We can treat the [inputs, filter_size, filter_size] dimensions as one.
  auto inputs = weights.size() / outputs;

  for (auto o = size_t{0}; o < outputs; o++) {
    for (auto c = size_t{0}; c < inputs; c++) {
      weights[o * inputs + c] *= bn_gammas[o];
    }

    biases[o] = -bn_gammas[o] * bn_means[o] + bn_betas[o];
  }

  // Batch norm weights are not needed anymore.
  bn_stddivs.clear();
  bn_means.clear();
  bn_betas.clear();
  bn_gammas.clear();
}

LegacyWeights::MHA::MHA(const pblczero::Weights::MHA& mha)
    : q_w(LayerAdapter(mha.q_w()).as_vector()),
      q_b(LayerAdapter(mha.q_b()).as_vector()),
      k_w(LayerAdapter(mha.k_w()).as_vector()),
      k_b(LayerAdapter(mha.k_b()).as_vector()),
      v_w(LayerAdapter(mha.v_w()).as_vector()),
      v_b(LayerAdapter(mha.v_b()).as_vector()),
      dense_w(LayerAdapter(mha.dense_w()).as_vector()),
      dense_b(LayerAdapter(mha.dense_b()).as_vector()) {}

LegacyWeights::FFN::FFN(const pblczero::Weights::FFN& ffn)
    : dense1_w(LayerAdapter(ffn.dense1_w()).as_vector()),
      dense1_b(LayerAdapter(ffn.dense1_b()).as_vector()),
      dense2_w(LayerAdapter(ffn.dense2_w()).as_vector()),
      dense2_b(LayerAdapter(ffn.dense2_b()).as_vector()) {}

LegacyWeights::EncoderLayer::EncoderLayer(
    const pblczero::Weights::EncoderLayer& encoder)
    : mha(MHA(encoder.mha())),
      ln1_gammas(LayerAdapter(encoder.ln1_gammas()).as_vector()),
      ln1_betas(LayerAdapter(encoder.ln1_betas()).as_vector()),
      ffn(FFN(encoder.ffn())),
      ln2_gammas(LayerAdapter(encoder.ln2_gammas()).as_vector()),
      ln2_betas(LayerAdapter(encoder.ln2_betas()).as_vector()) {}

}  // namespace lczero

```

`src/neural/network_legacy.h`:

```h
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018-2019 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include <vector>

#include "proto/net.pb.h"

namespace lczero {

struct LegacyWeights {
  explicit LegacyWeights(const pblczero::Weights& weights);

  using Vec = std::vector<float>;
  struct ConvBlock {
    explicit ConvBlock(const pblczero::Weights::ConvBlock& block);

    Vec weights;
    Vec biases;
    Vec bn_gammas;
    Vec bn_betas;
    Vec bn_means;
    Vec bn_stddivs;
  };

  struct SEunit {
    explicit SEunit(const pblczero::Weights::SEunit& se);
    Vec w1;
    Vec b1;
    Vec w2;
    Vec b2;
  };

  struct Residual {
    explicit Residual(const pblczero::Weights::Residual& residual);
    ConvBlock conv1;
    ConvBlock conv2;
    SEunit se;
    bool has_se;
  };

  struct MHA {
    explicit MHA(const pblczero::Weights::MHA& mha);
    Vec q_w;
    Vec q_b;
    Vec k_w;
    Vec k_b;
    Vec v_w;
    Vec v_b;
    Vec dense_w;
    Vec dense_b;
  };

  struct FFN {
    explicit FFN(const pblczero::Weights::FFN& mha);
    Vec dense1_w;
    Vec dense1_b;
    Vec dense2_w;
    Vec dense2_b;
  };

  struct EncoderLayer {
    explicit EncoderLayer(const pblczero::Weights::EncoderLayer& encoder);
    MHA mha;
    Vec ln1_gammas;
    Vec ln1_betas;
    FFN ffn;
    Vec ln2_gammas;
    Vec ln2_betas;
  };

  // Input convnet.
  ConvBlock input;

  // Residual tower.
  std::vector<Residual> residual;

  // Policy head
  // Extra convolution for AZ-style policy head
  ConvBlock policy1;
  ConvBlock policy;
  Vec ip_pol_w;
  Vec ip_pol_b;
  // Extra params for attention policy head
  Vec ip2_pol_w;
  Vec ip2_pol_b;
  Vec ip3_pol_w;
  Vec ip3_pol_b;
  Vec ip4_pol_w;
  int pol_encoder_head_count;
  std::vector<EncoderLayer> pol_encoder;


  // Value head
  ConvBlock value;
  Vec ip1_val_w;
  Vec ip1_val_b;
  Vec ip2_val_w;
  Vec ip2_val_b;

  // Moves left head
  ConvBlock moves_left;
  Vec ip1_mov_w;
  Vec ip1_mov_b;
  Vec ip2_mov_w;
  Vec ip2_mov_b;
};

}  // namespace lczero

```

`src/neural/network_mux.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <condition_variable>
#include <queue>
#include <thread>

#include "neural/factory.h"
#include "utils/exception.h"
#include "utils/numa.h"

namespace lczero {
namespace {

class MuxingNetwork;
class MuxingComputation : public NetworkComputation {
 public:
  MuxingComputation(MuxingNetwork* network) : network_(network) {}

  void AddInput(InputPlanes&& input) override { planes_.emplace_back(input); }

  void ComputeBlocking() override;

  int GetBatchSize() const override { return planes_.size(); }

  float GetQVal(int sample) const override {
    return parent_->GetQVal(sample + idx_in_parent_);
  }

  float GetDVal(int sample) const override {
    return parent_->GetDVal(sample + idx_in_parent_);
  }

  float GetMVal(int sample) const override {
    return parent_->GetMVal(sample + idx_in_parent_);
  }

  float GetPVal(int sample, int move_id) const override {
    return parent_->GetPVal(sample + idx_in_parent_, move_id);
  }

  void PopulateToParent(std::shared_ptr<NetworkComputation> parent) {
    // Populate our batch into batch of batches.
    parent_ = parent;
    idx_in_parent_ = parent->GetBatchSize();
    for (auto& x : planes_) parent_->AddInput(std::move(x));
  }

  void NotifyReady() {
    std::unique_lock<std::mutex> lock(mutex_);
    dataready_ = true;
    dataready_cv_.notify_one();
  }

 private:
  std::vector<InputPlanes> planes_;
  MuxingNetwork* network_;
  std::shared_ptr<NetworkComputation> parent_;
  int idx_in_parent_ = 0;

  std::mutex mutex_;
  std::condition_variable dataready_cv_;
  bool dataready_ = false;
};

class MuxingNetwork : public Network {
 public:
  MuxingNetwork(const std::optional<WeightsFile>& weights,
                const OptionsDict& options) {
    // int threads, int max_batch)
    //: network_(std::move(network)), max_batch_(max_batch) {

    const auto parents = options.ListSubdicts();
    if (parents.empty()) {
      // If options are empty, or multiplexer configured in root object,
      // initialize on root object and default backend.
      auto backends = NetworkFactory::Get()->GetBackendsList();
      AddBackend(backends[0], weights, options);
    }

    for (const auto& name : parents) {
      AddBackend(name, weights, options.GetSubdict(name));
    }
  }

  void AddBackend(const std::string& name,
                  const std::optional<WeightsFile>& weights,
                  const OptionsDict& opts) {
    const int nn_threads = opts.GetOrDefault<int>("threads", 1);
    const int max_batch = opts.GetOrDefault<int>("max_batch", 256);
    const std::string backend = opts.GetOrDefault<std::string>("backend", name);

    networks_.emplace_back(
        NetworkFactory::Get()->Create(backend, weights, opts));
    Network* net = networks_.back().get();

    if (networks_.size() == 1) {
      capabilities_ = net->GetCapabilities();
    } else {
      capabilities_.Merge(net->GetCapabilities());
    }

    for (int i = 0; i < nn_threads; ++i) {
      threads_.emplace_back(
          [this, net, max_batch, i]() { Worker(net, max_batch, i); });
    }
  }

  std::unique_ptr<NetworkComputation> NewComputation() override {
    return std::make_unique<MuxingComputation>(this);
  }

  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

  void Enqueue(MuxingComputation* computation) {
    std::lock_guard<std::mutex> lock(mutex_);
    queue_.push(computation);
    cv_.notify_one();
  }

  ~MuxingNetwork() {
    Abort();
    Wait();
    // Unstuck waiting computations.
    while (!queue_.empty()) {
      queue_.front()->NotifyReady();
      queue_.pop();
    }
  }

  void Worker(Network* network, const int max_batch, int id) {
    // Add one to the id in order to leave space for an active search thread.
    Numa::BindThread(id + 1);
    // While Abort() is not called (and it can only be called from destructor).
    while (!abort_) {
      std::vector<MuxingComputation*> children;
      // Create new computation in "upstream" network, to gather batch into
      // there.
      std::shared_ptr<NetworkComputation> parent(network->NewComputation());
      {
        std::unique_lock<std::mutex> lock(mutex_);
        // Wait until there's come work to compute.
        cv_.wait(lock, [&] { return abort_ || !queue_.empty(); });
        if (abort_) break;

        // While there is a work in queue, add it.
        while (!queue_.empty()) {
          // If we are reaching batch size limit, stop adding.
          // However, if a single input batch is larger than output batch limit,
          // we still have to add it.
          if (parent->GetBatchSize() != 0 &&
              parent->GetBatchSize() + queue_.front()->GetBatchSize() >
                  max_batch) {
            break;
          }
          // Remember which of "input" computations we serve.
          children.push_back(queue_.front());
          queue_.pop();
          // Make "input" computation populate data into output batch.
          children.back()->PopulateToParent(parent);
        }
      }

      // Compute.
      parent->ComputeBlocking();
      // Notify children that data is ready!
      for (auto child : children) child->NotifyReady();
    }
  }

  void Abort() {
    {
      std::lock_guard<std::mutex> lock(mutex_);
      abort_ = true;
    }
    cv_.notify_all();
  }

  void Wait() {
    while (!threads_.empty()) {
      threads_.back().join();
      threads_.pop_back();
    }
  }

 private:
  std::vector<std::unique_ptr<Network>> networks_;
  std::queue<MuxingComputation*> queue_;
  bool abort_ = false;
  NetworkCapabilities capabilities_;

  std::mutex mutex_;
  std::condition_variable cv_;

  std::vector<std::thread> threads_;
};

void MuxingComputation::ComputeBlocking() {
  network_->Enqueue(this);
  std::unique_lock<std::mutex> lock(mutex_);
  dataready_cv_.wait(lock, [this]() { return dataready_; });
}

std::unique_ptr<Network> MakeMuxingNetwork(
    const std::optional<WeightsFile>& weights, const OptionsDict& options) {
  return std::make_unique<MuxingNetwork>(weights, options);
}

REGISTER_NETWORK("multiplexing", MakeMuxingNetwork, -1000)

}  // namespace
}  // namespace lczero

```

`src/neural/network_random.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <chrono>
#include <cmath>
#include <cstring>
#include <functional>
#include <memory>
#include <thread>

#include "neural/factory.h"
#include "utils/hashcat.h"

namespace lczero {
namespace {

class RandomNetworkComputation : public NetworkComputation {
 public:
  RandomNetworkComputation(int delay, int seed, bool uniform_mode)
      : delay_ms_(delay), seed_(seed), uniform_mode_(uniform_mode) {}

  void AddInput(InputPlanes&& input) override {
    std::uint64_t hash = seed_;
    for (const auto& plane : input) {
      hash = HashCat({hash, plane.mask});
      std::uint32_t tmp;
      std::memcpy(&tmp, &plane.value, sizeof(float));
      const std::uint64_t value_hash = tmp;
      hash = HashCat({hash, value_hash});
    }
    inputs_.push_back(hash);
  }

  void ComputeBlocking() override {
    if (delay_ms_) {
      std::this_thread::sleep_for(std::chrono::milliseconds(delay_ms_));
    }
  }

  int GetBatchSize() const override { return inputs_.size(); }

  float GetQVal(int sample) const override {
    if (uniform_mode_) return 0.0f;
    return (int(inputs_[sample] % 200000) - 100000) / 100000.0;
  }

  float GetDVal(int sample) const override {
    if (uniform_mode_) return 0.0f;
    // Maximum D value is 1 - abs(Q) for W, D, L to be in range [0.0, 1.0].
    float q = GetQVal(sample);
    float max_d = 1.0f - std::fabs(q);
    // Hash in arbitrary constant to make D return different value from Q.
    float d = max_d * (HashCat({inputs_[sample], 1234}) % 10000) / 10000.0;
    return d;
  }

  float GetMVal(int /* sample */) const override { return 0.0f; }

  float GetPVal(int sample, int move_id) const override {
    if (uniform_mode_) return 1.0f;

    // Note that this function returns the policy value *before* softmax.
    // We choose a uniform distribution over [0, a], implying that the
    // proportion between the smallest and largest policy value *after* softmax
    // exponentiation (but before normalization) is equal to S = exp(-a).
    // Choosing a = 3.0 leads to S = 0.05.
    const float a = 3.0f;
    return (HashCat({inputs_[sample], static_cast<unsigned long>(move_id)}) %
            10000) *
           (a / 10000.0f);
  }

 private:
  std::vector<std::uint64_t> inputs_;
  int delay_ms_ = 0;
  int seed_ = 0;
  bool uniform_mode_ = false;
};

class RandomNetwork : public Network {
 public:
  RandomNetwork(const OptionsDict& options)
      : delay_ms_(options.GetOrDefault<int>("delay", 0)),
        seed_(options.GetOrDefault<int>("seed", 0)),
        uniform_mode_(options.GetOrDefault<bool>("uniform", false)),
        capabilities_{
            static_cast<pblczero::NetworkFormat::InputFormat>(
                options.GetOrDefault<int>(
                    "input_mode",
                    pblczero::NetworkFormat::INPUT_CLASSICAL_112_PLANE)),
            pblczero::NetworkFormat::MOVES_LEFT_NONE} {}
  std::unique_ptr<NetworkComputation> NewComputation() override {
    return std::make_unique<RandomNetworkComputation>(delay_ms_, seed_,
                                                      uniform_mode_);
  }
  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

 private:
  int delay_ms_ = 0;
  int seed_ = 0;
  bool uniform_mode_ = false;
  NetworkCapabilities capabilities_{
      pblczero::NetworkFormat::INPUT_CLASSICAL_112_PLANE,
      pblczero::NetworkFormat::MOVES_LEFT_NONE};
};
}  // namespace

std::unique_ptr<Network> MakeRandomNetwork(
    const std::optional<WeightsFile>& /*weights*/, const OptionsDict& options) {
  return std::make_unique<RandomNetwork>(options);
}

REGISTER_NETWORK("random", MakeRandomNetwork, 0)

}  // namespace lczero

```

`src/neural/network_record.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <iostream>

#include "neural/factory.h"
#include "utils/hashcat.h"

namespace lczero {
namespace {

class RecordComputation : public NetworkComputation {
 public:
  RecordComputation(std::unique_ptr<NetworkComputation>&& inner,
                    const std::string& record_file)
      : inner_(std::move(inner)), record_file_(record_file) {}
  static uint64_t make_hash(const InputPlanes& input) {
    std::uint64_t hash = 0x2134435D4534LL;
    for (const auto& plane : input) {
      hash = HashCat({hash, plane.mask});
      std::uint32_t tmp;
      std::memcpy(&tmp, &plane.value, sizeof(float));
      const std::uint64_t value_hash = tmp;
      hash = HashCat({hash, value_hash});
    }
    return hash;
  }
  // Adds a sample to the batch.
  void AddInput(InputPlanes&& input) override {
    hashes_.push_back(make_hash(input));
    requests_.emplace_back();
    q_count_.push_back(0);
    inner_->AddInput(std::move(input));
  }
  // Do the computation.
  void ComputeBlocking() override { inner_->ComputeBlocking(); }
  // Returns how many times AddInput() was called.
  int GetBatchSize() const override { return inner_->GetBatchSize(); }
  float Capture(float value, int index) const {
    // Only capture until we see Q again - the rest can be infered from that
    // set.
    if (q_count_[index] > 1) return value;
    requests_[index].push_back(value);
    return value;
  }
  // Returns Q value of @sample.
  float GetQVal(int sample) const override {
    q_count_[sample]++;
    return Capture(inner_->GetQVal(sample), sample);
  }
  float GetDVal(int sample) const override {
    return Capture(inner_->GetDVal(sample), sample);
  }
  // Returns P value @move_id of @sample.
  float GetPVal(int sample, int move_id) const override {
    return Capture(inner_->GetPVal(sample, move_id), sample);
  }
  float GetMVal(int sample) const override {
    return Capture(inner_->GetMVal(sample), sample);
  }
  virtual ~RecordComputation() {
    Mutex::Lock lock(mutex_);
    std::fstream output(record_file_, std::ios::app | std::ios_base::binary);
    for (size_t i = 0; i < hashes_.size(); i++) {
      uint64_t value = hashes_[i];
      output.write(reinterpret_cast<const char*>(&value), sizeof(value));
      int32_t length = static_cast<int32_t>(requests_[i].size());
      output.write(reinterpret_cast<const char*>(&length), sizeof(length));
      for (int j = 0; j < length; j++) {
        float recorded = requests_[i][j];
        output.write(reinterpret_cast<const char*>(&recorded),
                     sizeof(recorded));
      }
    }
  }
  std::unique_ptr<NetworkComputation> inner_;
  std::string record_file_;
  std::vector<uint64_t> hashes_;
  mutable std::vector<int> q_count_;
  mutable std::vector<std::vector<float>> requests_;
  static Mutex mutex_;
};

Mutex RecordComputation::mutex_;

class ReplayComputation : public NetworkComputation {
 public:
  ReplayComputation(std::unordered_map<uint64_t, std::vector<float>>* lookup)
      : lookup_(lookup) {}
  // Adds a sample to the batch.
  void AddInput(InputPlanes&& input) override {
    hashes_.push_back(RecordComputation::make_hash(input));
    replay_counter_.push_back(0);
  }
  // Do the computation.
  void ComputeBlocking() override {}
  // Returns how many times AddInput() was called.
  int GetBatchSize() const override { return static_cast<int>(hashes_.size()); }
  float Replay(int index) const {
    const auto& entry_ptr = lookup_->find(hashes_[index]);
    if (entry_ptr == lookup_->end()) {
      return 0.0f;
    }
    const auto& entry = entry_ptr->second;
    size_t counter = replay_counter_[index];
    if (counter >= entry.size()) {
      // Second pass reads the same things in the same order as first.
      counter = counter - entry.size();
      if (counter >= entry.size()) {
        // Third pass skips the first 3, then reads the rest in the same order.
        counter = counter - entry.size() + 3;
        if (counter >= entry.size()) {
          return 0.0f;
        }
      }
    }
    replay_counter_[index]++;
    return entry[counter];
  }
  // Returns Q value of @sample.
  float GetQVal(int sample) const override { return Replay(sample); }
  float GetDVal(int sample) const override { return Replay(sample); }
  // Returns P value @move_id of @sample.
  float GetPVal(int sample, int) const override { return Replay(sample); }
  float GetMVal(int sample) const override { return Replay(sample); }
  virtual ~ReplayComputation() {}

  std::unique_ptr<NetworkComputation> inner_;
  std::vector<uint64_t> hashes_;
  mutable std::vector<size_t> replay_counter_;
  std::unordered_map<uint64_t, std::vector<float>>* lookup_;
};

class RecordReplayNetwork : public Network {
 public:
  RecordReplayNetwork(const std::optional<WeightsFile>& weights,
                      const OptionsDict& options) {
    const auto parents = options.ListSubdicts();
    if (parents.empty()) {
      // If options are empty, or multiplexer configured in root object,
      // initialize on root object and default backend.
      auto backends = NetworkFactory::Get()->GetBackendsList();
      AddBackend(backends[0], weights, options);
    }

    for (const auto& name : parents) {
      AddBackend(name, weights, options.GetSubdict(name));
    }
    replay_file_ = options.GetOrDefault<std::string>("replay_file", "");
    record_file_ = options.GetOrDefault<std::string>("record_file", "");
    if (replay_file_.size() > 0) {
      lookup_ =
          std::make_unique<std::unordered_map<uint64_t, std::vector<float>>>();
      std::ifstream input(replay_file_, std::ios_base::binary);
      input.seekg(0, input.end);
      auto file_length = input.tellg();
      input.seekg(0, input.beg);
      while (input.tellg() < file_length) {
        uint64_t value = 0;
        input.read(reinterpret_cast<char*>(&value), sizeof(value));
        int32_t length = 0;
        input.read(reinterpret_cast<char*>(&length), sizeof(length));
        auto& entry = (*lookup_)[value];
        // Only use the first recorded value for any hash collisions.
        bool fill = entry.size() == 0;
        for (int j = 0; j < length; j++) {
          float recorded = 0.0f;
          input.read(reinterpret_cast<char*>(&recorded), sizeof(recorded));
          if (fill) {
            entry.push_back(recorded);
          }
        }
      }
    }
  }

  void AddBackend(const std::string& name,
                  const std::optional<WeightsFile>& weights,
                  const OptionsDict& opts) {
    const std::string backend = opts.GetOrDefault<std::string>("backend", name);

    networks_.emplace_back(
        NetworkFactory::Get()->Create(backend, weights, opts));

    if (networks_.size() == 1) {
      capabilities_ = networks_.back()->GetCapabilities();
    } else {
      capabilities_.Merge(networks_.back()->GetCapabilities());
    }
  }

  std::unique_ptr<NetworkComputation> NewComputation() override {
    if (!lookup_) {
      const long long val = ++counter_;
      return std::make_unique<RecordComputation>(
          networks_[val % networks_.size()]->NewComputation(), record_file_);
    }
    return std::make_unique<ReplayComputation>(lookup_.get());
  }

  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

  ~RecordReplayNetwork() {}

 private:
  std::vector<std::unique_ptr<Network>> networks_;
  std::atomic<long long> counter_;
  NetworkCapabilities capabilities_;
  std::string replay_file_;
  std::string record_file_;
  std::unique_ptr<std::unordered_map<uint64_t, std::vector<float>>> lookup_;
};

std::unique_ptr<Network> MakeRecordReplayNetwork(
    const std::optional<WeightsFile>& weights, const OptionsDict& options) {
  return std::make_unique<RecordReplayNetwork>(weights, options);
}

REGISTER_NETWORK("recordreplay", MakeRecordReplayNetwork, -999)

}  // namespace
}  // namespace lczero

```

`src/neural/network_rr.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <condition_variable>
#include <queue>
#include <thread>

#include "neural/factory.h"
#include "utils/exception.h"

namespace lczero {
namespace {

class RoundRobinNetwork : public Network {
 public:
  RoundRobinNetwork(const std::optional<WeightsFile>& weights,
                    const OptionsDict& options) {
    const auto parents = options.ListSubdicts();
    if (parents.empty()) {
      // If options are empty, or multiplexer configured in root object,
      // initialize on root object and default backend.
      auto backends = NetworkFactory::Get()->GetBackendsList();
      AddBackend(backends[0], weights, options);
    }

    for (const auto& name : parents) {
      AddBackend(name, weights, options.GetSubdict(name));
    }
  }

  void AddBackend(const std::string& name,
                  const std::optional<WeightsFile>& weights,
                  const OptionsDict& opts) {
    const std::string backend = opts.GetOrDefault<std::string>("backend", name);

    networks_.emplace_back(
        NetworkFactory::Get()->Create(backend, weights, opts));

    if (networks_.size() == 1) {
      capabilities_ = networks_.back()->GetCapabilities();
    } else {
      capabilities_.Merge(networks_.back()->GetCapabilities());
    }
  }

  std::unique_ptr<NetworkComputation> NewComputation() override {
    const long long val = ++counter_;
    return networks_[val % networks_.size()]->NewComputation();
  }

  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

  ~RoundRobinNetwork() {}

 private:
  std::vector<std::unique_ptr<Network>> networks_;
  std::atomic<long long> counter_;
  NetworkCapabilities capabilities_;
};

std::unique_ptr<Network> MakeRoundRobinNetwork(
    const std::optional<WeightsFile>& weights, const OptionsDict& options) {
  return std::make_unique<RoundRobinNetwork>(weights, options);
}

REGISTER_NETWORK("roundrobin", MakeRoundRobinNetwork, -999)

}  // namespace
}  // namespace lczero

```

`src/neural/network_tf_cc.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2022 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

// Hack around c++ version incompatibility.
#include <absl/base/config.h>

#include <cstddef>
#include <string>
#undef ABSL_HAVE_STD_STRING_VIEW

#include <tensorflow/cc/client/client_session.h>
#include <tensorflow/cc/ops/standard_ops.h>
#include <tensorflow/core/framework/tensor.h>

#include "neural/factory.h"
#include "neural/network_legacy.h"
#include "neural/shared/policy_map.h"
#include "utils/bititer.h"
#include "utils/optionsdict.h"
#include "utils/transpose.h"

namespace lczero {

using namespace tensorflow;
using namespace tensorflow::ops;

namespace {

Output MakeConst(const Scope& scope, TensorShape shape,
                 const std::vector<float>& values,
                 const std::vector<int>& order = {}) {
  auto tensor = Tensor(DataType::DT_FLOAT, shape);
  CHECK_EQ(tensor.NumElements(), static_cast<int>(values.size()))
      << shape.DebugString();

  std::vector<int> dims;
  for (const auto& x : shape) {
    dims.push_back(x.size);
  }
  TransposeTensor(dims, order, values, tensor.flat<float>().data());

  return Const(scope, tensor);
}

Output MakeIntConst(const Scope& scope, TensorShape shape,
                    const std::vector<int32_t>& values) {
  auto tensor = Tensor(DataType::DT_INT32, shape);
  CHECK_EQ(tensor.NumElements(), static_cast<int>(values.size()))
      << shape.DebugString();
  memcpy(tensor.flat<int32_t>().data(), values.data(),
         values.size() * sizeof(values[0]));
  return Const(scope, tensor);
}

template <bool CPU>
Output SqueezeAndExcite(const Scope& scope, Input input, int channels,
                        const LegacyWeights::SEunit& weights,
                        const std::string& basename) {
  const int se_channels = weights.b1.size();
  // NCHW ("NHWC" for CPU case) format reduced to NC.
  auto pooled = Mean(scope, input, CPU ? Input({1, 2}) : Input({2, 3}));
  auto w1 = MakeConst(scope, {channels, se_channels}, weights.w1);
  auto b1 = MakeConst(scope, {se_channels}, weights.b1);
  auto dense1_mul = MatMul(scope, pooled, w1);
  auto dense1_add = Add(scope, dense1_mul, b1);
  auto relu = Relu(scope, dense1_add);
  auto w2 = MakeConst(scope, {se_channels, 2 * channels}, weights.w2);
  auto b2 = MakeConst(scope, {2 * channels}, weights.b2);
  auto dense2_mul = MatMul(scope, relu, w2);
  auto dense2_add = Add(scope, dense2_mul, b2);
  auto reshape =
      Reshape(scope, dense2_add, CPU ? Input({-1, 1, 1, 2 * channels})
                                     : Input({-1, 2 * channels, 1, 1}));
  auto outputs = Split(scope, CPU ? 3 : 1, reshape, 2);
  auto sigmoid = Sigmoid(scope, outputs[0]);
  auto out_mul = Mul(scope, sigmoid, input);
  auto out_add = Add(scope, out_mul, outputs[1]);

  pooled.node()->set_name(basename + "/pooled");
  w1.node()->set_name(basename + "/w1");
  b1.node()->set_name(basename + "/b1");
  dense1_mul.node()->set_name(basename + "/dense1_mul");
  dense1_add.node()->set_name(basename + "/dense1_add");
  relu.node()->set_name(basename + "/relu");
  w2.node()->set_name(basename + "/w2");
  b2.node()->set_name(basename + "/b2");
  dense2_mul.node()->set_name(basename + "/dense2_mul");
  dense2_add.node()->set_name(basename + "/dense2_add");
  reshape.node()->set_name(basename + "/reshape");
  outputs[0].node()->set_name(basename + "/outputs/1");
  outputs[0].node()->set_name(basename + "/outputs/2");
  sigmoid.node()->set_name(basename + "/sigmoid");
  out_mul.node()->set_name(basename + "/out_mul");
  out_add.node()->set_name(basename + "/out_add");

  return out_add;
}

template <bool CPU>
Output MakeConvBlock(const Scope& scope, Input input, int channels,
                     int input_channels, int output_channels,
                     const LegacyWeights::ConvBlock& weights,
                     const std::string& basename,
                     const LegacyWeights::SEunit* const seunit = nullptr,
                     Input* mixin = nullptr, bool relu = true) {
  // CPU only supports "NHWC", while for GPU "NCHW" is better.
  const char* const kDataFormat = CPU ? "NHWC" : "NCHW";
  auto w_conv =
      MakeConst(scope, {channels, channels, input_channels, output_channels},
                weights.weights, {3, 2, 0, 1});
  w_conv.node()->set_name(basename + "/weights");
  auto conv2d = Conv2D(scope, input, w_conv, {1, 1, 1, 1}, "SAME",
                       Conv2D::DataFormat(kDataFormat).Dilations({1, 1, 1, 1}));
  conv2d.node()->set_name(basename + "/conv");
  auto b_conv = MakeConst(scope, {output_channels}, weights.biases);
  b_conv.node()->set_name(basename + "/weights_biases");
  Output conv_b =
      BiasAdd(scope, conv2d, b_conv, BiasAdd::DataFormat(kDataFormat));
  conv_b.node()->set_name(basename + "/bias_add");
  if (seunit) {
    conv_b = SqueezeAndExcite<CPU>(scope, conv_b, output_channels, *seunit,
                                   basename + "/se");
  }
  if (mixin) {
    conv_b = Add(scope, conv_b, *mixin);
    conv_b.node()->set_name(basename + "/mixin");
  }
  if (relu) {
    auto out = Relu(scope, conv_b);
    out.node()->set_name(basename + "/relu");
    return out;
  } else {
    return conv_b;
  }
}

template <bool CPU>
Output MakeResidualBlock(const Scope& scope, Input input, int channels,
                         const LegacyWeights::Residual& weights,
                         const std::string& basename) {
  auto block1 = MakeConvBlock<CPU>(scope, input, 3, channels, channels,
                                   weights.conv1, basename + "/conv1");
  auto block2 = MakeConvBlock<CPU>(
      scope, block1, 3, channels, channels, weights.conv2, basename + "/conv2",
      weights.has_se ? &weights.se : nullptr, &input);
  return block2;
}

template <bool CPU>
std::tuple<Output, Output, Output> MakeNetwork(const Scope& scope, Input input,
                                               const LegacyWeights& weights,
                                               bool wdl, bool moves_left) {
  const int filters = weights.input.weights.size() / kInputPlanes / 9;

  // Input convolution.
  auto flow = MakeConvBlock<CPU>(scope, input, 3, kInputPlanes, filters,
                                 weights.input, "input/conv");

  // Residual tower
  for (size_t i = 0; i < weights.residual.size(); ++i) {
    const auto& block = weights.residual[i];
    flow = MakeResidualBlock<CPU>(scope, flow, filters, block,
                                  "block_" + std::to_string(i));
  }

  // Policy head
  Output policy_head;
  if (!weights.policy1.weights.empty()) {
    // Conv policy head.
    auto conv_pol1 = MakeConvBlock<CPU>(scope, flow, 3, filters, filters,
                                        weights.policy1, "policy/conv1");
    auto conv_pol =
        MakeConvBlock<CPU>(scope, conv_pol1, 3, filters, 80, weights.policy,
                           "policy/conv2", nullptr, nullptr, /* relu= */ false);

    // [1858 -> HWC or CHW]
    std::vector<int> policy_map(1858);
    for (const auto& mapping : kConvPolicyMap) {
      if (mapping == -1) continue;
      const auto index = &mapping - kConvPolicyMap;
      const auto displacement = index / 64;
      const auto square = index % 64;
      const auto row = square / 8;
      const auto col = square % 8;
      if (CPU) {
        policy_map[mapping] = ((row * 8) + col) * 80 + displacement;
      } else {
        policy_map[mapping] = ((displacement * 8) + row) * 8 + col;
      }
    }
    auto mapping = MakeIntConst(scope, {1858}, policy_map);
    auto flattened_conv =
        Reshape(scope, conv_pol, Const(scope, {-1, 80 * 8 * 8}));
    policy_head = GatherV2(scope, flattened_conv, mapping, 1);

    mapping.node()->set_name("policy/mapping_table");
    flattened_conv.node()->set_name("policy/flatten");
  } else {
    const int policy_conv_size = weights.policy.biases.size();
    auto conv_pol =
        MakeConvBlock<CPU>(scope, flow, 1, filters, policy_conv_size,
                           weights.policy, "policy/conv");
    conv_pol =
        Reshape(scope, conv_pol, Const(scope, {-1, policy_conv_size * 8 * 8}));
    auto ip_pol_w = CPU ? MakeConst(scope, {8, 8, policy_conv_size, 1858},
                                    weights.ip_pol_w, {3, 2, 0, 1})
                        : MakeConst(scope, {policy_conv_size, 8, 8, 1858},
                                    weights.ip_pol_w, {3, 0, 1, 2});
    ip_pol_w = Reshape(scope, ip_pol_w,
                       Const(scope, {policy_conv_size * 8 * 8, 1858}));
    auto ip_pol_b = MakeConst(scope, {1858}, weights.ip_pol_b);
    policy_head = Add(scope, MatMul(scope, conv_pol, ip_pol_w), ip_pol_b);
  }

  // Value head
  auto conv_val = MakeConvBlock<CPU>(scope, flow, 1, filters, 32, weights.value,
                                     "value/conv");
  conv_val = Reshape(scope, conv_val, Const(scope, {-1, 32 * 8 * 8}));

  auto ip1_val_w =
      CPU ? MakeConst(scope, {8, 8, 32, 128}, weights.ip1_val_w, {3, 2, 0, 1})
          : MakeConst(scope, {32, 8, 8, 128}, weights.ip1_val_w, {3, 0, 1, 2});
  ip1_val_w = Reshape(scope, ip1_val_w, Const(scope, {32 * 8 * 8, 128}));
  auto ip1_val_b = MakeConst(scope, {128}, weights.ip1_val_b);
  auto value_flow =
      Relu(scope, Add(scope, MatMul(scope, conv_val, ip1_val_w), ip1_val_b));
  Output value_head;
  if (wdl) {
    auto ip2_val_w = MakeConst(scope, {128, 3}, weights.ip2_val_w);
    auto ip2_val_b = MakeConst(scope, {3}, weights.ip2_val_b);
    ip2_val_w.node()->set_name("value/w2");
    ip2_val_b.node()->set_name("value/b2");
    auto ip_fc = Add(scope, MatMul(scope, value_flow, ip2_val_w), ip2_val_b);
    value_head = Softmax(scope, ip_fc);
  } else {
    auto ip2_val_w = MakeConst(scope, {128, 1}, weights.ip2_val_w);
    auto ip2_val_b = MakeConst(scope, {1}, weights.ip2_val_b);
    ip2_val_w.node()->set_name("value/w2");
    ip2_val_b.node()->set_name("value/b2");
    auto ip_fc = Add(scope, MatMul(scope, value_flow, ip2_val_w), ip2_val_b);
    value_head = Tanh(scope, ip_fc);
  }
  ip1_val_w.node()->set_name("value/w1");
  ip1_val_b.node()->set_name("value/b1");
  value_flow.node()->set_name("value/relu");

  // Moves left head
  Output moves_left_head;
  if (moves_left) {
    const int mlh_channels = weights.moves_left.biases.size();
    auto conv_mov = MakeConvBlock<CPU>(scope, flow, 1, filters, mlh_channels,
                                       weights.moves_left, "mlh/conv");
    conv_mov =
        Reshape(scope, conv_mov, Const(scope, {-1, mlh_channels * 8 * 8}));

    const int mlh_fc1_outputs = weights.ip1_mov_b.size();
    auto ip1_mov_w =
        CPU ? MakeConst(scope, {8, 8, mlh_channels, mlh_fc1_outputs},
                        weights.ip1_mov_w, {3, 2, 0, 1})
            : MakeConst(scope, {mlh_channels, 8, 8, mlh_fc1_outputs},
                        weights.ip1_mov_w, {3, 0, 1, 2});
    ip1_mov_w = Reshape(scope, ip1_mov_w,
                        Const(scope, {mlh_channels * 8 * 8, mlh_fc1_outputs}));
    auto ip1_mov_b = MakeConst(scope, {mlh_fc1_outputs}, weights.ip1_mov_b);
    auto mov_flow =
        Relu(scope, Add(scope, MatMul(scope, conv_mov, ip1_mov_w), ip1_mov_b));
    auto ip2_mov_w = MakeConst(scope, {mlh_fc1_outputs, 1}, weights.ip2_mov_w);
    auto ip2_mov_b = MakeConst(scope, {1}, weights.ip2_mov_b);
    auto ip_fc = Add(scope, MatMul(scope, mov_flow, ip2_mov_w), ip2_mov_b);
    moves_left_head = Relu(scope, ip_fc);
  }

  policy_head.node()->set_name("policy/out");
  value_head.node()->set_name("value/out");
  moves_left_head.node()->set_name("mlh/out");
  return {policy_head, value_head, moves_left_head};
}

template <bool CPU>
class TFNetworkComputation;

template <bool CPU>
class TFNetwork : public Network {
 public:
  TFNetwork(const WeightsFile& file, const OptionsDict& options, bool wdl);

  std::unique_ptr<NetworkComputation> NewComputation() override;

  tensorflow::Status Compute(tensorflow::Tensor& input,
                             std::vector<tensorflow::Tensor>* outputs) const;

  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

  bool IsWdl() const { return wdl_; }

  bool IsMlh() const {
    return capabilities_.moves_left == pblczero::NetworkFormat::MOVES_LEFT_V1;
  }

 private:
  tensorflow::Scope scope_;
  std::unique_ptr<tensorflow::ClientSession> session_;

  std::unique_ptr<tensorflow::ops::Placeholder> input_;
  std::unique_ptr<tensorflow::Output> policy_head_;
  std::unique_ptr<tensorflow::Output> value_head_;
  std::unique_ptr<tensorflow::Output> moves_left_head_;
  const NetworkCapabilities capabilities_;
  const bool wdl_;
};

template <bool CPU>
class TFNetworkComputation : public NetworkComputation {
 public:
  TFNetworkComputation(const TFNetwork<CPU>* network) : network_(network) {}
  void AddInput(InputPlanes&& input) override {
    raw_input_.emplace_back(input);
  }
  void ComputeBlocking() override {
    PrepareInput();
    status_ = network_->Compute(input_, &output_);
    CHECK(status_.ok()) << status_.ToString();
  }

  int GetBatchSize() const override { return raw_input_.size(); }
  float GetQVal(int sample) const override {
    if (network_->IsWdl()) {
      const auto w = output_[0].template matrix<float>()(sample, 0);
      const auto l = output_[0].template matrix<float>()(sample, 2);
      return w - l;
    } else {
      return output_[0].template matrix<float>()(sample, 0);
    }
  }
  float GetDVal(int sample) const override {
    if (network_->IsWdl()) {
      const auto d = output_[0].template matrix<float>()(sample, 1);
      return d;
    } else {
      return 0.0f;
    }
  }
  float GetPVal(int sample, int move_id) const override {
    return output_[1].template matrix<float>()(sample, move_id);
  }
  float GetMVal(int sample) const override {
    if (network_->IsMlh()) {
      return output_[2].template matrix<float>()(sample, 0);
    } else {
      return 0.0f;
    }
  }

 private:
  void PrepareInput();

  const TFNetwork<CPU>* network_;
  std::vector<InputPlanes> raw_input_;

  tensorflow::Tensor input_;
  std::vector<tensorflow::Tensor> output_;
  tensorflow::Status status_;
};

// Version for GPU.
template <>
void TFNetworkComputation<false>::PrepareInput() {
  input_ = tensorflow::Tensor(
      tensorflow::DataType::DT_FLOAT,
      {static_cast<int>(raw_input_.size()), kInputPlanes, 8, 8});

  auto flat = input_.flat<float>();
  memset(flat.data(), 0, flat.size() * sizeof(*flat.data()));
  auto iter = flat.data();
  for (const auto& sample : raw_input_) {
    CHECK_EQ(sample.size(), kInputPlanes);
    for (const auto& plane : sample) {
      for (auto bit : IterateBits(plane.mask)) {
        *(iter + bit) = plane.value;
      }
      iter += 64;
    }
  }
}

// Version for CPU.
template <>
void TFNetworkComputation<true>::PrepareInput() {
  input_ = tensorflow::Tensor(
      tensorflow::DataType::DT_FLOAT,
      {static_cast<int>(raw_input_.size()), 8, 8, kInputPlanes});

  auto flat = input_.flat<float>();
  memset(flat.data(), 0, flat.size() * sizeof(*flat.data()));
  auto* data = flat.data();
  for (size_t input_idx = 0; input_idx < raw_input_.size(); ++input_idx) {
    const auto& sample = raw_input_[input_idx];
    int base = kInputPlanes * 8 * 8 * input_idx;

    CHECK_EQ(sample.size(), kInputPlanes);
    for (int plane_idx = 0; plane_idx < kInputPlanes; ++plane_idx) {
      const auto& plane = sample[plane_idx];
      for (auto bit : IterateBits(plane.mask)) {
        data[base + bit * kInputPlanes + plane_idx] = plane.value;
      }
    }
  }
}  // namespace

template <bool CPU>
TFNetwork<CPU>::TFNetwork(const WeightsFile& file, const OptionsDict& options,
                          bool wdl)
    : scope_(Scope::NewRootScope()),
      capabilities_{file.format().network_format().input(),
                    file.format().network_format().moves_left()},
      wdl_(wdl) {
  const LegacyWeights weights(file.weights());
  tensorflow::SessionOptions session_options;
  if (CPU) (*session_options.config.mutable_device_count())["GPU"] = 0;
  session_ =
      std::make_unique<tensorflow::ClientSession>(scope_, session_options);

  if (CPU) {
    input_ = std::make_unique<Placeholder>(
        scope_, DataType::DT_FLOAT,
        Placeholder::Shape({-1, 8, 8, kInputPlanes}));
  } else {
    input_ = std::make_unique<Placeholder>(
        scope_, DataType::DT_FLOAT,
        Placeholder::Shape({-1, kInputPlanes, 8, 8}));
  }
  input_->node()->set_name("input_planes");

  auto output = MakeNetwork<CPU>(scope_, *input_, weights, wdl, IsMlh());
  CHECK(scope_.ok()) << scope_.status().ToString();
  policy_head_ = std::make_unique<Output>(std::get<0>(output));
  value_head_ = std::make_unique<Output>(std::get<1>(output));
  moves_left_head_ = std::make_unique<Output>(std::get<2>(output));

  if (options.Exists<std::string>("dump-graphdef") ||
      options.Exists<std::string>("dump-graphdef-txt")) {
    GraphDef gdef;
    CHECK(scope_.ToGraphDef(&gdef).ok());
    if (options.Exists<std::string>("dump-graphdef")) {
      std::ofstream f(options.Get<std::string>("dump-graphdef").c_str());
      f.exceptions(std::ifstream::failbit);
      f << gdef.SerializeAsString();
    }
    if (options.Exists<std::string>("dump-graphdef-txt")) {
      std::ofstream f(options.Get<std::string>("dump-graphdef-txt").c_str());
      f.exceptions(std::ifstream::failbit);
      f << gdef.DebugString();
    }
  }

  // First request to tensorflow is slow (0.6s), so doing an empty request for
  // preheating.
  auto fake_request = NewComputation();
  fake_request->AddInput(InputPlanes(kInputPlanes));
  fake_request->ComputeBlocking();
}

template <bool CPU>
tensorflow::Status TFNetwork<CPU>::Compute(tensorflow::Tensor& input,
                                           std::vector<Tensor>* outputs) const {
  std::vector<Output> fetch_outputs = {*value_head_, *policy_head_};
  if (IsMlh()) fetch_outputs.push_back(*moves_left_head_);
  return session_->Run({{*input_, input}}, fetch_outputs, outputs);
}

template <bool CPU>
std::unique_ptr<NetworkComputation> TFNetwork<CPU>::NewComputation() {
  return std::make_unique<TFNetworkComputation<CPU>>(this);
}

template <bool CPU>
std::unique_ptr<Network> MakeTFNetwork(const std::optional<WeightsFile>& w,
                                       const OptionsDict& options) {
  if (!w) {
    throw Exception("The " +
                    std::string(CPU ? "tensorflow-cc-cpu" : "tensorflow-cc") +
                    " backend requires a network file.");
  }
  const WeightsFile& weights = *w;
  if (weights.format().network_format().network() !=
          pblczero::NetworkFormat::NETWORK_CLASSICAL_WITH_HEADFORMAT &&
      weights.format().network_format().network() !=
          pblczero::NetworkFormat::NETWORK_SE_WITH_HEADFORMAT) {
    throw Exception("Network format " +
                    pblczero::NetworkFormat::NetworkStructure_Name(
                        weights.format().network_format().network()) +
                    " is not supported by Tensorflow C++ backend.");
  }
  if (weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_CLASSICAL &&
      weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_CONVOLUTION) {
    throw Exception("Policy format " +
                    pblczero::NetworkFormat::PolicyFormat_Name(
                        weights.format().network_format().policy()) +
                    " is not supported by Tensorflow C++ backend.");
  }
  if (weights.format().network_format().value() !=
          pblczero::NetworkFormat::VALUE_CLASSICAL &&
      weights.format().network_format().value() !=
          pblczero::NetworkFormat::VALUE_WDL) {
    throw Exception("Value format " +
                    pblczero::NetworkFormat::ValueFormat_Name(
                        weights.format().network_format().value()) +
                    " is not supported by Tensorflow C++ backend.");
  }
  if (weights.format().network_format().default_activation() !=
      pblczero::NetworkFormat::DEFAULT_ACTIVATION_RELU) {
    throw Exception(
        "Default activation " +
        pblczero::NetworkFormat::DefaultActivation_Name(
            weights.format().network_format().default_activation()) +
        " is not supported by Tensorflow C++ backend.");
  }
  return std::make_unique<TFNetwork<CPU>>(
      weights, options, weights.format().network_format().value() ==
                            pblczero::NetworkFormat::VALUE_WDL);
}

REGISTER_NETWORK("tensorflow-cc-cpu", MakeTFNetwork<true>, 90)
REGISTER_NETWORK("tensorflow-cc", MakeTFNetwork<false>, 80)

}  // namespace
}  // namespace lczero

```

`src/neural/network_trivial.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

// This is the trivial backend, which.
// Uses idea from
// https://www.chessprogramming.org/Simplified_Evaluation_Function
// for Q (but coefficients are "trained" from 1000 arbitrary test60 games).
// Returns the same P vector always ("trained" from 1 hour of test60 games).

#include <algorithm>
#include <array>
#include <cmath>
#include <cstdint>
#include <iterator>
#include <memory>

#include "neural/factory.h"
#include "utils/bititer.h"
#include "utils/logging.h"

namespace lczero {
namespace {

constexpr std::array<float, 1858> kLogPolicy = {
    -3.27805f, -2.55371f, -2.46718f, -2.59232f, -2.74631f, -2.59647f, -2.47084f,
    -3.65601f, -2.09820f, -1.43034f, -3.51708f, -1.26485f, -2.36647f, -2.94045f,
    -2.40305f, -2.70842f, -2.52492f, -2.57050f, -2.48690f, -2.21723f, -2.35995f,
    -1.97193f, -1.93535f, -2.93369f, -2.62881f, -2.61207f, -2.72703f, -2.71673f,
    -2.52759f, -2.49393f, -2.22701f, -2.63556f, -2.12130f, -1.82262f, -3.50585f,
    -2.87152f, -1.57311f, -2.45245f, -2.88140f, -2.58226f, -2.62983f, -2.85953f,
    -2.63080f, -3.08031f, -2.13966f, -2.85110f, -2.12827f, -3.24672f, -2.85159f,
    -2.67270f, -2.98899f, -3.06657f, -2.71692f, -2.68217f, -2.30527f, -1.55012f,
    -2.70463f, -2.65240f, -1.78385f, -2.58179f, -1.94060f, -2.98342f, -1.57773f,
    -2.49979f, -2.94035f, -2.23586f, -2.80895f, -2.83289f, -2.76390f, -4.54929f,
    -2.25179f, -2.14827f, -3.16414f, -3.54998f, -3.06761f, -3.11185f, -3.34247f,
    -3.05679f, -2.83999f, -2.23640f, -2.66808f, -3.11225f, -2.74733f, -1.83896f,
    -2.95630f, -1.93498f, -3.40767f, -1.60610f, -2.95001f, -3.24019f, -2.87622f,
    -3.88308f, -3.14287f, -3.82922f, -2.98526f, -2.76060f, -2.11013f, -2.55618f,
    -3.16483f, -2.89776f, -3.00680f, -3.40576f, -2.92824f, -1.08351f, -2.18712f,
    -3.40693f, -3.31301f, -2.08158f, -2.03667f, -2.36173f, -1.76878f, -3.00632f,
    -2.05124f, -2.29763f, -2.66771f, -3.03798f, -2.47170f, -2.67618f, -2.86332f,
    -3.16487f, -2.63969f, -2.14089f, -2.70953f, -2.94945f, -2.52531f, -2.31311f,
    -2.39041f, -2.38423f, -2.79781f, -2.21194f, -1.83517f, -2.31660f, -1.19398f,
    -2.70912f, -2.19063f, -1.67444f, -2.92405f, -1.89225f, -3.19352f, -2.53407f,
    -2.78346f, -2.33611f, -2.86697f, -4.23423f, -2.88961f, -2.62569f, -1.95808f,
    -2.29278f, -2.36494f, -2.33724f, -2.40053f, -2.56087f, -2.83420f, -3.53338f,
    -2.18170f, -2.13917f, -2.04813f, -2.66216f, -2.12418f, -0.69842f, -2.65481f,
    -3.79830f, -2.08032f, -2.71945f, -2.10865f, -2.59930f, -2.09607f, -2.59304f,
    -1.96199f, -2.21306f, -2.00576f, -2.20327f, -2.30800f, -2.18593f, -2.14309f,
    -2.43553f, -3.97334f, -3.46230f, -1.39132f, -1.53451f, -2.93402f, -2.16152f,
    -1.20217f, -3.35988f, -2.23190f, -2.86953f, -2.25928f, -2.66984f, -2.10801f,
    -2.55898f, -1.92820f, -2.37356f, -1.55857f, -2.03181f, -2.28908f, -2.17008f,
    -1.79786f, -2.11051f, -2.25579f, -2.25968f, -2.21662f, -1.93364f, -1.75094f,
    -1.66550f, -2.72936f, -1.64063f, -1.61075f, -2.86833f, -1.78174f, -2.57420f,
    -2.09685f, -2.58283f, -1.92704f, -2.48349f, -1.69953f, -2.59193f, -1.76127f,
    -2.01200f, -2.94340f, -2.38107f, -3.08673f, -1.99493f, -2.33547f, -2.10375f,
    -2.35984f, -2.29878f, -1.96549f, -1.78870f, -1.65615f, -2.77078f, -2.80018f,
    -2.27486f, -1.53731f, -2.04792f, -3.32063f, -1.71385f, -2.84901f, -2.34161f,
    -2.99670f, -2.23259f, -3.02214f, -1.93832f, -2.77222f, -1.81773f, -2.68912f,
    -3.35246f, -2.98197f, -2.77642f, -2.82059f, -2.61695f, -2.79896f, -2.40170f,
    -2.51546f, -2.62934f, -2.31413f, -2.02635f, -1.92560f, -2.99387f, -2.57964f,
    -2.24623f, -2.52376f, -1.91827f, -3.34948f, -2.56071f, -1.65398f, -2.16174f,
    -3.18252f, -2.82216f, -3.51838f, -2.64710f, -3.88911f, -2.21975f, -3.90789f,
    -2.14583f, -3.95923f, -2.95078f, -2.62058f, -2.57345f, -3.03102f, -2.59558f,
    -2.61131f, -2.31295f, -2.28283f, -2.43661f, -2.33765f, -2.13112f, -2.71698f,
    -2.17856f, -2.11073f, -2.33474f, -2.28725f, -3.00949f, -2.64762f, -1.04994f,
    -2.69146f, -2.90213f, -3.23718f, -2.76631f, -3.25924f, -2.51251f, -3.72328f,
    -2.33565f, -2.08714f, -3.21947f, -2.85503f, -2.63804f, -2.84485f, -3.52504f,
    -2.55603f, -2.60966f, -2.76627f, -2.33495f, -2.05168f, -2.27313f, -2.22733f,
    -2.38077f, -2.58201f, -1.68292f, -2.27004f, -2.34945f, -3.15454f, -2.29580f,
    -2.00001f, -2.28900f, -3.42943f, -3.26178f, -2.78328f, -3.65967f, -3.76857f,
    -2.82601f, -2.31787f, -2.18220f, -2.66768f, -2.23966f, -2.30392f, -2.29266f,
    -3.16170f, -2.21344f, -2.27064f, -2.44117f, -2.36266f, -1.94422f, -1.93731f,
    -2.12489f, -2.01889f, -1.51826f, -2.97232f, -1.76259f, -2.73162f, -2.50737f,
    -2.17278f, -3.13102f, -2.52260f, -2.77223f, -2.61413f, -2.56946f, -2.58251f,
    -2.45132f, -2.52211f, -2.31542f, -1.88442f, -2.20100f, -2.68182f, -2.46038f,
    -3.37642f, -1.84869f, -1.97638f, -2.04854f, -2.16895f, -2.11686f, -1.75997f,
    -2.36928f, -1.71232f, -2.30947f, -2.62440f, -2.89605f, -3.20371f, -1.87585f,
    -3.72998f, -2.35252f, -3.22560f, -2.32038f, -2.96892f, -2.29096f, -2.57950f,
    -2.06247f, -2.79851f, -1.75405f, -1.94448f, -2.01396f, -2.33409f, -1.49118f,
    -1.63272f, -1.72012f, -1.75873f, -1.82517f, -1.88277f, -1.51172f, -1.95535f,
    -1.63343f, -2.66199f, -2.63170f, -2.11238f, -3.26804f, -2.48582f, -1.96174f,
    -2.38943f, -1.84035f, -2.43613f, -1.79448f, -2.44656f, -1.70873f, -2.23079f,
    -3.07955f, -2.69291f, -2.12766f, -2.08189f, -2.07082f, -2.11996f, -2.29495f,
    -2.34251f, -2.08395f, -1.99731f, -1.86997f, -1.72110f, -2.69204f, -1.74916f,
    -2.03069f, -2.18267f, -2.32030f, -2.60985f, -2.09827f, -2.55477f, -1.83743f,
    -2.57651f, -1.75630f, -2.52175f, -3.80455f, -2.50695f, -3.12213f, -2.78579f,
    -2.55680f, -2.16556f, -2.33857f, -2.61161f, -2.53169f, -2.26087f, -2.60362f,
    -2.39008f, -2.21694f, -2.07617f, -1.99662f, -2.42189f, -2.87815f, -2.15145f,
    -2.29009f, -3.10180f, -2.87811f, -2.45276f, -3.09524f, -2.79113f, -3.00962f,
    -2.39040f, -3.07481f, -1.97233f, -2.56667f, -2.98239f, -4.48054f, -2.51603f,
    -3.68408f, -2.80978f, -4.02947f, -2.30009f, -2.18421f, -2.31835f, -2.93369f,
    -2.46494f, -2.11828f, -2.10616f, -2.28425f, -2.31544f, -2.12953f, -2.09662f,
    -3.37759f, -2.30327f, -2.55404f, -1.79951f, -2.90035f, -3.11283f, -3.39870f,
    -2.44633f, -2.98232f, -2.80023f, -2.48821f, -2.88594f, -2.01654f, -2.82274f,
    -2.03482f, -2.78492f, -3.26634f, -3.24195f, -2.72334f, -2.99127f, -3.19865f,
    -3.04587f, -2.51103f, -2.31969f, -2.55876f, -2.55748f, -2.58753f, -2.47201f,
    -2.15586f, -2.12523f, -2.40960f, -2.37746f, -2.45204f, -2.89473f, -2.60158f,
    -2.42614f, -2.46064f, -2.34556f, -3.24733f, -2.44070f, -2.59262f, -2.49762f,
    -3.37558f, -3.75832f, -2.48748f, -3.80084f, -2.19600f, -3.79062f, -2.04098f,
    -3.40081f, -3.18389f, -2.68514f, -3.22397f, -2.92402f, -2.90829f, -2.52897f,
    -2.13440f, -2.08583f, -2.84989f, -2.35185f, -2.54278f, -2.46315f, -2.01244f,
    -1.96864f, -2.41030f, -2.47213f, -2.47850f, -1.83279f, -2.75409f, -2.39579f,
    -2.91434f, -2.95888f, -2.45982f, -2.54589f, -2.72006f, -3.42412f, -3.10157f,
    -2.77292f, -3.85904f, -3.18310f, -2.29223f, -2.19258f, -2.87856f, -3.52170f,
    -2.38993f, -4.64160f, -3.25123f, -3.15322f, -2.15700f, -1.91129f, -2.05896f,
    -3.87007f, -2.31107f, -2.38624f, -2.48424f, -2.48776f, -1.92920f, -2.00539f,
    -2.58454f, -2.72011f, -2.08597f, -2.33988f, -2.38105f, -3.78748f, -2.94846f,
    -2.81264f, -3.03018f, -3.92556f, -3.45293f, -2.77133f, -3.05216f, -2.55676f,
    -2.84859f, -2.82938f, -2.03113f, -2.42253f, -2.85646f, -2.17388f, -3.81336f,
    -2.56566f, -1.72219f, -1.79065f, -2.17955f, -2.09137f, -2.20016f, -2.20045f,
    -2.41179f, -2.33142f, -1.68436f, -2.00165f, -2.40213f, -1.78619f, -2.97991f,
    -2.03122f, -2.81125f, -2.70508f, -2.75108f, -2.93190f, -2.82492f, -2.93223f,
    -2.89579f, -2.66369f, -3.09179f, -1.96086f, -2.33415f, -2.70489f, -2.09119f,
    -1.74156f, -1.50930f, -1.71216f, -1.62041f, -1.75809f, -1.80507f, -2.00153f,
    -2.09758f, -2.10626f, -1.47714f, -1.75447f, -1.44928f, -2.73195f, -2.70826f,
    -2.44281f, -2.19051f, -2.65505f, -2.11028f, -2.54282f, -1.93472f, -2.45693f,
    -1.74596f, -2.18828f, -2.47454f, -2.35589f, -2.49861f, -2.31872f, -2.37263f,
    -2.17848f, -2.16253f, -2.49976f, -2.37605f, -2.41639f, -2.31335f, -2.25894f,
    -2.49608f, -2.27300f, -2.19441f, -1.77622f, -2.08721f, -2.51806f, -2.67657f,
    -2.87500f, -2.30523f, -2.55959f, -1.86961f, -2.64929f, -2.16811f, -2.43415f,
    -2.77701f, -2.21105f, -2.18239f, -2.38285f, -2.10211f, -2.01523f, -1.90136f,
    -1.96068f, -1.95664f, -1.99073f, -2.10522f, -2.22135f, -2.06296f, -2.18416f,
    -2.01866f, -1.93989f, -2.16718f, -1.68751f, -2.05182f, -2.70005f, -2.14289f,
    -2.19256f, -2.50235f, -1.96130f, -2.59285f, -1.86936f, -2.70564f, -2.43371f,
    -2.98850f, -2.71071f, -3.27384f, -2.33446f, -2.88198f, -2.54575f, -3.56300f,
    -2.35963f, -2.14711f, -2.40189f, -2.36698f, -2.25897f, -2.24260f, -2.02243f,
    -2.33412f, -2.38856f, -2.52751f, -2.29562f, -3.00430f, -2.42419f, -2.58575f,
    -1.73016f, -2.44198f, -3.69007f, -3.07589f, -2.42086f, -2.78920f, -3.15726f,
    -2.19145f, -3.34312f, -2.14677f, -2.80956f, -2.92510f, -2.54229f, -2.64030f,
    -2.71488f, -3.23822f, -2.48303f, -2.98548f, -2.42741f, -2.76244f, -2.19121f,
    -2.15651f, -2.01607f, -2.65838f, -2.36130f, -2.53446f, -2.14098f, -2.16233f,
    -2.40205f, -2.78516f, -2.52937f, -2.88440f, -1.83036f, -2.54164f, -1.68723f,
    -3.03116f, -2.99693f, -2.60829f, -2.41607f, -3.37414f, -2.98597f, -3.04663f,
    -2.39045f, -2.91719f, -2.14359f, -2.83079f, -2.96203f, -2.45407f, -2.96953f,
    -2.58018f, -2.92212f, -2.37740f, -2.69961f, -2.49857f, -2.69541f, -2.05724f,
    -2.00530f, -1.99569f, -2.69320f, -2.15570f, -2.45105f, -2.28308f, -1.95776f,
    -2.01425f, -2.57625f, -2.58345f, -2.62041f, -1.66176f, -2.22696f, -2.04774f,
    -2.95035f, -2.73967f, -2.66807f, -2.64775f, -2.73152f, -3.26277f, -2.57792f,
    -2.34761f, -3.10747f, -2.71430f, -2.19213f, -3.38385f, -2.18705f, -2.92125f,
    -2.70237f, -2.15332f, -2.88140f, -3.03359f, -2.45556f, -2.30681f, -1.91017f,
    -2.36607f, -3.24726f, -2.05841f, -2.25753f, -2.29119f, -2.23472f, -1.88084f,
    -2.05394f, -2.36612f, -2.49803f, -2.19139f, -2.28553f, -2.56190f, -2.83538f,
    -3.06285f, -2.97343f, -2.69375f, -3.27634f, -3.95293f, -3.38088f, -2.66496f,
    -3.65575f, -2.03979f, -2.48842f, -1.95172f, -2.38707f, -2.00054f, -2.07675f,
    -2.65607f, -1.79267f, -1.73012f, -1.78330f, -1.99767f, -1.89127f, -2.07869f,
    -2.11741f, -2.21555f, -2.20841f, -1.72231f, -1.86764f, -2.05040f, -1.61327f,
    -2.07715f, -1.72813f, -2.77681f, -2.37597f, -2.60204f, -2.52153f, -2.52887f,
    -2.56209f, -2.45590f, -2.00872f, -2.15372f, -2.04436f, -2.10929f, -2.16001f,
    -2.18408f, -1.82629f, -1.59137f, -1.79041f, -1.74602f, -1.95427f, -1.93570f,
    -2.06620f, -2.30465f, -2.22643f, -1.74341f, -2.34048f, -1.47648f, -2.00384f,
    -2.61990f, -2.79520f, -2.33649f, -2.32341f, -2.28627f, -2.44100f, -1.89457f,
    -2.04758f, -2.09793f, -2.17350f, -2.03410f, -2.20825f, -2.02451f, -2.03058f,
    -1.95847f, -1.89330f, -1.97147f, -2.03824f, -2.17988f, -2.34249f, -2.15968f,
    -2.23169f, -2.19646f, -2.08405f, -1.77245f, -1.60113f, -2.11544f, -1.84474f,
    -2.43358f, -2.35725f, -1.90083f, -2.44999f, -2.10366f, -2.83689f, -2.19778f,
    -2.35656f, -3.09341f, -2.25288f, -2.18496f, -2.49246f, -2.48306f, -2.10760f,
    -2.43646f, -2.28027f, -2.05602f, -2.09228f, -2.31467f, -2.35102f, -2.36965f,
    -2.28385f, -2.23430f, -2.80955f, -2.01406f, -2.05944f, -2.66423f, -3.36453f,
    -1.96616f, -2.97724f, -2.56376f, -1.98829f, -2.87213f, -2.25546f, -2.23133f,
    -2.34622f, -2.06627f, -2.45802f, -2.81215f, -2.35850f, -2.36959f, -2.10324f,
    -3.05414f, -2.08282f, -2.12720f, -1.84930f, -2.49966f, -2.12045f, -2.09591f,
    -2.12239f, -2.34968f, -2.50831f, -2.44692f, -2.36269f, -2.98886f, -1.93362f,
    -2.09213f, -1.91741f, -2.79879f, -2.67868f, -2.87130f, -2.05039f, -2.80665f,
    -2.62980f, -2.20870f, -2.70801f, -2.29572f, -2.79538f, -2.59612f, -2.37224f,
    -2.40995f, -2.43122f, -2.66385f, -2.36472f, -2.38147f, -2.33425f, -3.11712f,
    -2.10967f, -2.03662f, -2.03923f, -2.63280f, -2.25739f, -2.22740f, -2.07232f,
    -2.02925f, -2.49691f, -2.61640f, -2.45189f, -3.06772f, -2.01314f, -2.09911f,
    -2.10208f, -2.60957f, -2.65398f, -3.01437f, -2.23397f, -2.79582f, -2.93628f,
    -2.76973f, -2.25740f, -2.77257f, -2.69414f, -2.22957f, -2.51010f, -2.35899f,
    -2.71161f, -2.36220f, -2.59277f, -2.30781f, -2.76481f, -2.40330f, -2.95625f,
    -1.98644f, -2.05878f, -2.09858f, -3.31837f, -2.09829f, -2.32696f, -2.26766f,
    -1.99927f, -2.11274f, -2.48006f, -2.56481f, -2.72063f, -2.04023f, -2.19388f,
    -1.94067f, -3.56758f, -2.81211f, -2.85422f, -2.23250f, -3.82901f, -2.88714f,
    -2.95403f, -2.30589f, -2.81081f, -2.50157f, -2.01331f, -2.31846f, -2.08757f,
    -2.24877f, -2.20436f, -2.19524f, -2.44630f, -2.53404f, -2.41221f, -1.78045f,
    -1.86100f, -1.95456f, -2.91479f, -1.90417f, -2.04239f, -2.17665f, -2.14768f,
    -1.80827f, -1.87976f, -2.09031f, -2.42031f, -1.81085f, -1.79782f, -1.86047f,
    -2.76310f, -2.57748f, -2.66853f, -2.33193f, -3.13747f, -2.80059f, -2.46372f,
    -2.06273f, -3.13628f, -1.86735f, -2.68757f, -2.06358f, -2.42925f, -2.06488f,
    -2.17551f, -2.91779f, -2.19911f, -2.16090f, -1.85600f, -2.09475f, -1.81766f,
    -1.90589f, -2.00586f, -2.22196f, -2.14874f, -1.72591f, -1.71773f, -2.65898f,
    -1.85318f, -1.77443f, -2.45163f, -2.54152f, -3.17261f, -2.24711f, -3.59329f,
    -2.71727f, -2.04273f, -2.44538f, -1.96606f, -2.19677f, -2.10418f, -2.19514f,
    -1.91936f, -2.34775f, -1.77923f, -1.95359f, -1.94258f, -1.77794f, -1.87704f,
    -1.99098f, -2.17642f, -2.29225f, -2.37089f, -1.87244f, -2.12701f, -1.75107f,
    -1.67364f, -2.81174f, -2.69765f, -2.23031f, -2.44985f, -2.00851f, -2.11672f,
    -2.03517f, -2.14776f, -1.96397f, -2.28422f, -1.96835f, -2.19551f, -1.42431f,
    -2.16588f, -1.91686f, -1.82988f, -1.40790f, -2.10013f, -2.27586f, -2.36663f,
    -2.47849f, -2.38244f, -2.31331f, -1.97177f, -1.54729f, -1.92420f, -1.74783f,
    -2.03165f, -2.27844f, -2.52088f, -2.14359f, -2.03773f, -2.19033f, -1.97465f,
    -2.29625f, -1.98357f, -2.30993f, -2.22969f, -1.83153f, -2.06495f, -1.95874f,
    -1.94909f, -1.85345f, -2.04891f, -1.95348f, -2.03582f, -2.27380f, -2.58700f,
    -2.39653f, -2.33205f, -2.09450f, -2.16998f, -1.61820f, -1.95539f, -2.26379f,
    -3.03213f, -2.07728f, -2.51776f, -2.47519f, -2.29791f, -2.34987f, -2.24506f,
    -2.10877f, -2.35250f, -2.04200f, -2.34712f, -2.54766f, -2.33497f, -2.03394f,
    -2.21075f, -2.75461f, -1.96144f, -2.07166f, -1.89114f, -2.23968f, -2.04990f,
    -2.05186f, -2.11099f, -2.53494f, -2.53646f, -2.40433f, -2.15578f, -2.96947f,
    -2.03372f, -1.74629f, -2.09146f, -2.44340f, -2.63727f, -3.22198f, -2.24365f,
    -2.80178f, -2.54007f, -2.27926f, -2.29653f, -2.42813f, -2.36442f, -2.41354f,
    -2.23481f, -2.37359f, -2.35228f, -2.30055f, -2.27875f, -2.29771f, -2.71068f,
    -1.91342f, -2.06465f, -1.89668f, -2.51739f, -2.12320f, -2.14759f, -2.01413f,
    -2.15683f, -2.53451f, -2.55102f, -2.19632f, -2.72338f, -2.03579f, -1.81866f,
    -2.08212f, -2.96693f, -2.76705f, -2.86854f, -2.27029f, -2.85617f, -2.81300f,
    -2.09222f, -2.28959f, -2.10792f, -2.19547f, -2.19424f, -2.24168f, -2.13982f,
    -2.07873f, -2.19986f, -2.01142f, -2.34383f, -2.17709f, -1.74786f, -1.86743f,
    -1.78310f, -2.36384f, -1.96474f, -2.15577f, -2.20774f, -1.83719f, -1.91188f,
    -2.25855f, -2.15191f, -2.42236f, -1.97486f, -1.68240f, -1.84782f, -2.60395f,
    -2.46040f, -2.64373f, -2.16081f, -2.56212f, -2.65208f, -2.39016f, -1.96899f,
    -2.37672f, -1.98351f, -2.11495f, -2.16868f, -2.13437f, -1.98334f, -2.19722f,
    -2.22096f, -2.40551f, -2.06782f, -1.76509f, -1.87992f, -1.82811f, -2.27810f,
    -1.97049f, -2.12135f, -2.23576f, -2.21375f, -1.87601f, -1.84173f, -1.99004f,
    -2.26579f, -1.92204f, -1.53696f, -1.94886f, -2.44900f, -2.42089f, -2.35997f,
    -2.14107f, -2.86418f, -2.62388f, -2.29501f, -1.82484f, -2.15865f, -1.95074f,
    -2.07764f, -2.09247f, -2.14312f, -1.78663f, -2.28377f, -2.24162f, -1.87493f,
    -1.73979f, -1.76333f, -1.81149f, -1.85047f, -1.96739f, -2.04565f, -2.17483f,
    -2.11464f, -1.74922f, -1.71497f, -2.12079f, -1.84692f, -1.47210f, -2.05774f,
    -2.39308f, -2.39104f, -2.02159f, -2.80460f, -2.61564f, -1.90047f, -2.33373f,
    -2.00211f, -2.09685f, -2.16511f, -2.26695f, -1.58998f, -2.13473f, -1.70681f,
    -1.85100f, -1.77046f, -1.68764f, -1.81053f, -1.92529f, -2.04866f, -2.17788f,
    -2.15314f, -1.87239f, -1.95897f, -2.00624f, -1.45804f, -2.81546f, -2.43162f,
    -2.00752f, -2.21081f, -1.69523f, -2.27725f, -1.66769f, -2.35110f, -1.66064f,
    -2.28703f, -1.80589f, -2.22687f, -1.33695f, -2.02242f, -1.90671f, -1.77027f,
    -1.59172f, -2.10972f, -2.31081f, -2.43459f, -2.47780f, -2.53299f, -2.40122f,
    -2.10230f, -2.04187f, -1.85619f, -1.97726f, -2.29052f, -2.21670f, -2.38972f,
    -1.89230f, -2.39340f, -1.84567f, -2.54204f, -2.07184f, -1.91261f, -2.36088f,
    -1.44757f, -2.22705f, -1.93230f, -2.04882f, -1.85666f, -1.72142f, -2.04852f,
    -2.18148f, -2.47233f, -2.57103f, -2.66228f, -2.36146f, -2.10443f, -2.17253f,
    -2.14498f, -2.13797f, -2.20457f, -2.37645f, -2.42756f, -2.22670f, -2.44680f,
    -2.06698f, -2.49345f, -2.06786f, -2.23341f, -1.95126f, -2.39432f, -1.68125f,
    -2.15135f, -2.40909f, -1.85258f, -2.04602f, -1.86844f, -2.05475f, -2.06725f,
    -2.01961f, -2.19149f, -2.48149f, -2.65855f, -2.45480f, -2.19928f, -2.50001f,
    -2.09398f, -2.13474f, -2.05691f, -2.40477f, -2.35070f, -2.38167f, -2.37739f,
    -2.11717f, -2.05558f, -2.42946f, -2.07644f, -2.05003f, -1.76712f, -2.37327f,
    -1.71331f, -2.13889f, -2.12451f, -1.72115f, -1.99994f, -1.78942f, -2.02653f,
    -2.01901f, -2.08600f, -2.01737f, -2.15539f, -2.50478f, -2.43162f, -2.14439f,
    -2.68323f, -1.82002f, -2.02668f, -1.80627f, -2.23072f, -2.21415f, -2.24463f,
    -2.15904f, -2.24000f, -2.15755f, -2.37529f, -2.03020f, -2.18104f, -1.83134f,
    -2.28310f, -1.80883f, -2.13150f, -1.90072f, -1.74498f, -1.93361f, -1.76177f,
    -2.26838f, -2.01060f, -2.07943f, -2.16495f, -1.98597f, -2.04789f, -2.32455f,
    -2.22731f, -2.35536f, -1.86481f, -1.95994f, -1.94230f, -2.89104f, -1.96322f,
    -2.08665f, -2.06367f, -2.05313f, -2.15806f, -1.96075f, -2.20531f, -2.07987f,
    -1.55964f, -2.29668f, -1.76706f, -2.08963f, -1.79572f, -1.70844f, -1.77588f,
    -1.68404f, -2.03263f, -1.90899f, -2.07456f, -2.22453f, -2.23994f, -1.87902f,
    -1.84461f, -2.06319f, -2.12513f, -1.90459f, -1.87878f, -2.01137f, -2.64095f,
    -1.95214f, -1.95016f, -1.92596f, -2.03448f, -1.81453f, -2.15834f, -1.80823f,
    -2.28257f, -1.95188f, -1.40515f, -2.21123f, -1.68100f, -1.78388f, -1.56169f,
    -1.68899f, -1.58597f, -1.82104f, -1.98177f, -2.11101f, -2.21825f, -2.16944f,
    -1.79568f, -1.70052f, -2.12545f, -1.76546f, -1.88302f, -1.88185f, -1.81223f,
    -1.97519f, -1.80843f, -2.01822f, -1.72436f, -2.16737f, -1.79040f, -2.17144f,
    -1.97642f, -1.08338f, -2.09722f, -1.55166f, -1.58894f, -1.67195f, -1.79258f,
    -1.91590f, -2.07872f, -2.17675f, -2.22306f, -2.34160f, -1.85982f, -1.92394f,
    -1.67892f, -1.81541f, -1.97390f, -1.62706f, -2.00251f, -1.44317f, -2.06995f,
    -1.47269f, -2.00043f, -1.57277f, -1.98441f, -1.65323f, -1.94968f, -1.32563f,
    -1.76913f, -1.54770f, -1.45107f, -1.55195f, -1.94615f, -2.22312f, -2.17876f,
    -2.18266f, -2.22343f, -2.20101f, -2.17880f, -1.82719f, -1.91612f, -1.71975f,
    -2.01348f, -1.67986f, -2.00790f, -1.66986f, -2.02330f, -1.73576f, -1.26400f,
    -1.92733f, -1.09162f, -1.92195f, -1.52879f, -1.53502f, -1.49707f, -1.27697f,
    -1.89492f, -1.85442f, -2.12911f, -2.16070f, -2.14918f, -2.17024f, -2.14222f,
    -1.98419f, -1.95113f, -2.03801f, -1.68961f, -2.05959f, -1.87573f, -2.10187f,
    -1.75431f, -1.99309f, -1.43739f, -2.07272f, -1.32327f, -2.06925f, -1.72851f,
    -1.46024f, -1.58201f, -1.53596f, -1.51227f, -2.09855f, -1.84421f, -1.86674f,
    -2.12931f, -2.21361f, -2.19483f, -2.15773f, -2.01178f, -2.02010f, -2.02189f,
    -2.00588f, -1.78924f, -1.92400f, -2.19067f, -1.89411f, -1.97522f, -1.36829f,
    -2.15518f, -1.42762f, -2.11017f, -1.71658f, -1.56530f, -1.66244f, -1.64796f,
    -1.65544f, -2.08416f, -2.15704f, -1.86680f, -1.85526f, -2.18876f, -2.25134f,
    -2.19230f, -1.91536f, -1.92085f, -1.93780f, -1.89535f, -1.95840f, -1.81512f,
    -2.05763f, -1.73630f, -1.93331f, -1.37147f, -2.23705f, -1.34394f, -1.88755f,
    -1.63986f, -1.61512f, -1.73315f, -1.64673f, -1.87312f, -2.06245f, -2.13445f,
    -2.11927f, -1.79248f, -1.81999f, -2.19646f, -2.14435f, -1.60343f, -1.66710f,
    -1.72212f, -1.75136f, -1.70293f, -1.82036f, -1.65613f, -1.91504f, -1.79989f,
    -1.34902f, -2.02918f, -1.33355f, -1.85791f, -1.47304f, -1.39559f, -1.51913f,
    -1.52185f, -1.86153f, -1.90207f, -1.93224f, -1.95073f, -1.90642f, -1.60040f,
    -1.60577f, -1.98155f, -1.60383f, -1.71976f, -1.72190f, -1.69195f, -1.80138f,
    -1.56773f, -1.93684f, -1.55984f, -1.98119f, -1.75877f, -1.03839f, -1.99725f,
    -1.13111f, -1.28862f, -1.39230f, -1.59500f, -1.42287f, -1.83753f, -1.92297f,
    -1.94480f, -1.87559f, -1.89891f, -1.51436f, -1.51757f, -1.61669f, -1.77426f,
    -1.58805f, -1.80870f, -1.50397f, -1.82487f, -1.47130f, -1.86784f, -1.50498f,
    -1.93723f, -1.76847f, -1.10059f, -2.00065f, -1.24889f, -1.42306f, -1.55937f,
    -1.85574f, -1.93160f, -1.96405f, -1.91510f, -1.99475f, -2.03417f, -1.63940f,
    -1.46124f, -2.51354f, -3.05328f, -0.23796f, -1.09853f, -1.69816f, -0.88060f,
    -2.59322f, -2.72630f, -1.39634f, -2.47534f, -2.93509f, -0.40550f, -1.30283f,
    -2.31982f, -0.32459f, -1.27121f, -2.06339f, -1.59923f, -2.71933f, -3.21009f,
    -0.09426f, -1.33884f, -1.94104f, -0.13521f, -1.30427f, -2.12129f, -1.50089f,
    -2.55270f, -2.85786f, -0.05611f, -0.82796f, -2.33907f, -0.21596f, -1.13357f,
    -1.38835f, -1.29421f, -2.32614f, -2.77775f, -0.70767f, -1.27214f, -2.37027f,
    -0.26273f, -0.95609f, -1.41015f, -1.10089f, -2.11966f, -2.40177f, -0.35224f,
    -1.20165f, -1.40952f, -0.0f,     -0.65666f, -1.98171f, -1.05692f, -2.01931f,
    -2.43228f, -0.0f,     -2.29416f, -3.49818f, -0.46989f, -1.00615f, -1.49424f,
    -1.23419f, -2.09375f, -2.43350f};

constexpr std::array<float, 64> kPawns = {
    -0.00000f, -0.00000f, -0.00000f, -0.00000f, 0.00000f,  -0.00000f, 0.00000f,
    -0.00000f, 0.06662f,  0.09583f,  0.06643f,  0.05536f,  0.02236f,  0.04939f,
    0.09071f,  0.09352f,  0.08847f,  0.08068f,  0.07738f,  0.05534f,  0.06063f,
    0.06393f,  0.08791f,  0.09560f,  0.07608f,  0.08692f,  0.06337f,  0.07179f,
    0.07750f,  0.07100f,  0.08159f,  0.08283f,  0.14966f,  0.09968f,  0.10335f,
    0.10362f,  0.08502f,  0.11313f,  0.08158f,  0.10561f,  0.13946f,  0.12644f,
    0.12511f,  0.11497f,  0.13403f,  0.09999f,  0.12569f,  0.14075f,  0.16155f,
    0.12969f,  0.16869f,  0.17744f,  0.17258f,  0.19959f,  0.14792f,  0.15976f,
    0.00000f,  -0.00000f, 0.00000f,  0.00000f,  -0.00000f, -0.00000f, -0.00000f,
    -0.00000f};
constexpr std::array<float, 64> kKnights = {
    0.12549f, 0.05358f, 0.06001f, 0.08798f, 0.09084f, 0.07007f, 0.05983f,
    0.07110f, 0.03532f, 0.08703f, 0.13308f, 0.07691f, 0.11283f, 0.06292f,
    0.08848f, 0.05982f, 0.07493f, 0.10743f, 0.10747f, 0.12312f, 0.11972f,
    0.10002f, 0.07905f, 0.06539f, 0.12239f, 0.14532f, 0.08843f, 0.12103f,
    0.10833f, 0.13367f, 0.09003f, 0.07247f, 0.12163f, 0.16514f, 0.15197f,
    0.11901f, 0.14494f, 0.13974f, 0.12081f, 0.10957f, 0.10746f, 0.11014f,
    0.15582f, 0.22478f, 0.15931f, 0.15341f, 0.11198f, 0.07076f, 0.16764f,
    0.14658f, 0.20785f, 0.12558f, 0.10667f, 0.19004f, 0.07353f, 0.14162f,
    0.03220f, 0.06356f, 0.13316f, 0.12845f, 0.14233f, 0.19931f, 0.07425f,
    0.20774f,
};
constexpr std::array<float, 64> kBishops = {
    0.08299f, 0.11050f, 0.11387f, 0.12347f, 0.13993f, 0.10414f, 0.18594f,
    0.06085f, 0.10235f, 0.15733f, 0.13970f, 0.13631f, 0.10189f, 0.17399f,
    0.14002f, 0.10948f, 0.14439f, 0.13286f, 0.15316f, 0.13379f, 0.13762f,
    0.13907f, 0.11989f, 0.12127f, 0.15578f, 0.13964f, 0.16643f, 0.14614f,
    0.12861f, 0.15553f, 0.16397f, 0.09271f, 0.18553f, 0.14091f, 0.18698f,
    0.15018f, 0.15590f, 0.12655f, 0.14573f, 0.10276f, 0.19904f, 0.15973f,
    0.13077f, 0.14071f, 0.15390f, 0.11180f, 0.10273f, 0.19621f, 0.14963f,
    0.14949f, 0.12911f, 0.11972f, 0.17507f, 0.14455f, 0.10058f, 0.11797f,
    0.15988f, 0.14084f, 0.15436f, 0.24262f, 0.12838f, 0.15251f, 0.10853f,
    0.14240f,
};
constexpr std::array<float, 64> kRooks = {
    0.19343f, 0.22010f, 0.19814f, 0.20439f, 0.20660f, 0.20584f, 0.19275f,
    0.20042f, 0.18159f, 0.19006f, 0.19286f, 0.19677f, 0.22751f, 0.22487f,
    0.19256f, 0.16757f, 0.19102f, 0.23716f, 0.21167f, 0.19747f, 0.23355f,
    0.21321f, 0.17478f, 0.17279f, 0.15728f, 0.16795f, 0.26422f, 0.22453f,
    0.24422f, 0.21715f, 0.19039f, 0.24305f, 0.18434f, 0.25995f, 0.25855f,
    0.24373f, 0.25450f, 0.23517f, 0.20909f, 0.22781f, 0.19181f, 0.26571f,
    0.26481f, 0.21262f, 0.25547f, 0.22559f, 0.22430f, 0.23066f, 0.28646f,
    0.25282f, 0.25758f, 0.21276f, 0.25720f, 0.26076f, 0.25661f, 0.24443f,
    0.23237f, 0.21318f, 0.23230f, 0.19967f, 0.21947f, 0.22544f, 0.23956f,
    0.21579f,
};
constexpr std::array<float, 64> kQueens = {
    0.23063f, 0.23157f, 0.25371f, 0.27579f, 0.27878f, 0.23600f, 0.29552f,
    0.29963f, 0.27729f, 0.29837f, 0.29026f, 0.25105f, 0.27772f, 0.28502f,
    0.29344f, 0.24009f, 0.29366f, 0.28859f, 0.27538f, 0.27713f, 0.26159f,
    0.28383f, 0.28749f, 0.24996f, 0.35152f, 0.26595f, 0.26428f, 0.30264f,
    0.29376f, 0.29841f, 0.27352f, 0.29242f, 0.30948f, 0.30742f, 0.30822f,
    0.31232f, 0.31701f, 0.28862f, 0.28218f, 0.29562f, 0.30423f, 0.33840f,
    0.29070f, 0.29734f, 0.25349f, 0.27276f, 0.24977f, 0.27319f, 0.27835f,
    0.35061f, 0.33633f, 0.29402f, 0.32144f, 0.33461f, 0.29777f, 0.28501f,
    0.31223f, 0.33044f, 0.33788f, 0.26788f, 0.29851f, 0.28789f, 0.31030f,
    0.28824f,
};
constexpr std::array<float, 64> kKings = {
    0.02852f,  0.00453f,  -0.05309f, -0.02416f, -0.14581f, -0.01472f, -0.02206f,
    0.02207f,  0.03712f,  0.02324f,  -0.02501f, -0.06653f, -0.07605f, -0.01135f,
    0.03666f,  -0.02999f, 0.00700f,  -0.02668f, -0.06998f, -0.02305f, -0.03816f,
    -0.00129f, -0.08264f, 0.01139f,  -0.08866f, -0.01720f, -0.03161f, -0.03092f,
    -0.01507f, 0.00172f,  -0.03457f, 0.02657f,  -0.00569f, 0.00000f,  0.00341f,
    -0.00108f, -0.01445f, -0.02948f, -0.00883f, 0.00954f,  0.01116f,  -0.01762f,
    0.01088f,  -0.00005f, -0.00275f, 0.00038f,  0.00219f,  0.01970f,  0.00000f,
    -0.00024f, -0.01798f, -0.00339f, -0.00226f, 0.00842f,  0.03543f,  -0.00122f,
    -0.00000f, 0.00532f,  -0.00000f, 0.00002f,  0.00138f,  0.00571f,  -0.00078f,
    0.00145f,
};
constexpr std::array<float, 64> kKingsEndgame = {
    -0.03908f, -0.02837f, -0.02194f, -0.03649f, -0.04754f, -0.03390f, -0.03172f,
    0.02852f,  -0.02071f, -0.01429f, -0.02296f, -0.01087f, -0.02774f, -0.01505f,
    -0.00469f, -0.03894f, -0.03979f, 0.02244f,  -0.00705f, -0.01847f, -0.00316f,
    -0.04952f, 0.01103f,  -0.00487f, 0.01769f,  0.01299f,  0.03068f,  -0.01422f,
    -0.00579f, -0.01817f, 0.01946f,  -0.01716f, 0.04860f,  0.01099f,  0.05517f,
    0.06880f,  0.00036f,  0.03165f,  0.07524f,  -0.00409f, 0.01840f,  0.04146f,
    0.04435f,  0.09172f,  0.08918f,  0.03642f,  0.04095f,  0.00081f,  0.07254f,
    0.03901f,  0.07186f,  0.06869f,  0.05215f,  0.00621f,  0.02240f,  -0.00463f,
    -0.01242f, -0.02068f, 0.01925f,  0.03103f,  0.02797f,  -0.02299f, -0.01944f,
    -0.04503f};

float DotProduct(uint64_t plane, const std::array<float, 64>& weights) {
  float result = 0.0f;
  for (auto idx : IterateBits(plane)) result += weights[idx];
  return result;
}

int NumBits(uint64_t x) {
  return std::distance(BitIterator<int>(x), BitIterator<int>(0));
}

class TrivialNetworkComputation : public NetworkComputation {
 public:
  void AddInput(InputPlanes&& input) override {
    float q = 0.0f;
    q += DotProduct(input[0].mask, kPawns);
    q -= DotProduct(ReverseBytesInBytes(input[6].mask), kPawns);
    q += DotProduct(input[1].mask, kKnights);
    q -= DotProduct(ReverseBytesInBytes(input[7].mask), kKnights);
    q += DotProduct(input[2].mask, kBishops);
    q -= DotProduct(ReverseBytesInBytes(input[8].mask), kBishops);
    q += DotProduct(input[3].mask, kRooks);
    q -= DotProduct(ReverseBytesInBytes(input[9].mask), kRooks);
    q += DotProduct(input[4].mask, kQueens);
    q -= DotProduct(ReverseBytesInBytes(input[10].mask), kQueens);
    const bool endgame =
        (input[4].mask == 0 ||
         (input[3].mask == 0 && NumBits(input[1].mask | input[2].mask) <= 1)) &&
        (input[10].mask == 0 ||
         (input[9].mask == 0 && NumBits(input[7].mask | input[8].mask) <= 1));
    if (endgame) {
      q += DotProduct(input[5].mask, kKingsEndgame);
      q -= DotProduct(ReverseBytesInBytes(input[11].mask), kKingsEndgame);
    } else {
      q += DotProduct(input[5].mask, kKings);
      q -= DotProduct(ReverseBytesInBytes(input[11].mask), kKings);
    }
    // Multiply Q by 10, otherwise evals too low. :-/
    q_.push_back(2.0f / (1.0f + std::exp(q * -10.0f)) - 1.0f);
  }

  void ComputeBlocking() override {}

  int GetBatchSize() const override { return q_.size(); }

  float GetQVal(int sample) const override { return q_[sample]; }

  float GetDVal(int) const override { return 0.0f; }

  float GetMVal(int /* sample */) const override { return 0.0f; }

  float GetPVal(int /* sample */, int move_id) const override {
    return kLogPolicy[move_id];
  }

 private:
  std::vector<float> q_;
};

class TrivialNetwork : public Network {
 public:
  TrivialNetwork(const OptionsDict& options)
      : capabilities_{
            static_cast<pblczero::NetworkFormat::InputFormat>(
                options.GetOrDefault<int>(
                    "input_mode",
                    pblczero::NetworkFormat::INPUT_CLASSICAL_112_PLANE)),
            pblczero::NetworkFormat::MOVES_LEFT_NONE} {}
  std::unique_ptr<NetworkComputation> NewComputation() override {
    return std::make_unique<TrivialNetworkComputation>();
  }
  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

 private:
  NetworkCapabilities capabilities_{
      pblczero::NetworkFormat::INPUT_CLASSICAL_112_PLANE,
      pblczero::NetworkFormat::MOVES_LEFT_NONE};
};
}  // namespace

std::unique_ptr<Network> MakeTrivialNetwork(
    const std::optional<WeightsFile>& /*weights*/, const OptionsDict& options) {
  return std::make_unique<TrivialNetwork>(options);
}

REGISTER_NETWORK("trivial", MakeTrivialNetwork, 4)

}  // namespace lczero
```

`src/neural/onednn/layers.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021-2022 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "layers.h"
#include <cassert>
#include <cmath>
#include <cstring>
#include <vector>

namespace lczero {

namespace onednn_backend {

BaseLayer::BaseLayer(int c, int h, int w, BaseLayer* ip)
    : input_(ip), C(c), H(h), W(w) {
  if (ip) {
    data_type_ = ip->data_type_;
    convolution_type_ = ip->convolution_type_;
  } else {
    data_type_ = dnnl::memory::data_type::undef;
    convolution_type_ = dnnl::algorithm::convolution_auto;
  }
}

ConvLayer::ConvLayer(BaseLayer* ip, int C, int H, int W, int filter, int Cin,
                     ActivationFunction activation, bool skip)
    : BaseLayer(C, H, W, ip),
      c_input_(Cin),
      filter_size_(filter),
      activation_(activation),
      use_skip_(skip) {}

void ConvLayer::LoadWeights(dnnl::memory& w1, dnnl::memory& b1,
                            dnnl::engine& eng, dnnl::stream& stream) {
  auto filter_md =
      dnnl::memory::desc({C, c_input_, filter_size_, filter_size_}, data_type_,
                         dnnl::memory::format_tag::oihw);
  filter_mem = dnnl::memory(filter_md, eng);
  dnnl::reorder(w1, filter_mem).execute(stream, w1, filter_mem);

  auto bias_md =
      dnnl::memory::desc({C}, data_type_, dnnl::memory::format_tag::a);
  bias_mem = dnnl::memory(bias_md, eng);
  dnnl::reorder(b1, bias_mem).execute(stream, b1, bias_mem);
}

void ConvLayer::Eval(int N, dnnl::memory& output, dnnl::memory& input,
                     dnnl::engine& eng, dnnl::stream& stream) {
  std::lock_guard<std::mutex> lock(lock_);
  if (last_batch_ != N) {
    auto t_in_md = dnnl::memory::desc({N, c_input_, H, W}, data_type_,
                                      dnnl::memory::format_tag::any);

    auto t_filter_md =
        dnnl::memory::desc({C, c_input_, filter_size_, filter_size_},
                           data_type_, dnnl::memory::format_tag::any);

    auto t_out_md = dnnl::memory::desc({N, C, H, W}, data_type_,
                                       dnnl::memory::format_tag::any);

    const int padding = filter_size_ / 2;
    auto conv_d = dnnl::convolution_forward::desc(
        dnnl::prop_kind::forward_inference,
        filter_size_ == 3 ? convolution_type_
                          : dnnl::algorithm::convolution_auto,
        t_in_md, t_filter_md, bias_mem.get_desc(), t_out_md, {1, 1},
        {padding, padding}, {padding, padding});
    dnnl::post_ops conv_ops;
    if (use_skip_) {
      conv_ops.append_sum();
    }
    if (activation_ == RELU) {
      conv_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_relu, 0.0f, 0.0f);
    } else if (activation_ == TANH) {
      conv_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_tanh, 0.0f, 0.0f);
    }
    dnnl::primitive_attr conv_attr;
    conv_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
    conv_attr.set_post_ops(conv_ops);
    auto conv_pd =
        dnnl::convolution_forward::primitive_desc(conv_d, conv_attr, eng);
    auto scratchpad_md = conv_pd.scratchpad_desc();
    conv_ = dnnl::convolution_forward(conv_pd);

    in_md = conv_pd.src_desc();
    out_md = conv_pd.dst_desc();

    // Apparently convolution doesn't go well with mish post op.
    if (activation_ == MISH) {
      auto mish_d = dnnl::eltwise_forward::desc(
          dnnl::prop_kind::forward_inference, dnnl::algorithm::eltwise_mish,
          out_md, 0.f, 0.f);
      dnnl::primitive_attr mish_attr;
      mish_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
      auto mish_pd =
          dnnl::eltwise_forward::primitive_desc(mish_d, mish_attr, eng);
      mish_ = dnnl::eltwise_forward(mish_pd);
      if (scratchpad_md.get_size() < mish_pd.scratchpad_desc().get_size()) {
        scratchpad_md = mish_pd.scratchpad_desc();
      }
    }

    if (!conv_filter_mem ||
        conv_pd.weights_desc() != conv_filter_mem.get_desc()) {
      // This may be a transformation for Winograd convolution, so keep the
      // original weights.
      conv_filter_mem = dnnl::memory(conv_pd.weights_desc(), eng);
      dnnl::reorder(filter_mem, conv_filter_mem)
          .execute(stream, filter_mem, conv_filter_mem);
    }

    dnnl::primitive_attr reorder_attr;
    reorder_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
    auto in_reorder_pd = dnnl::reorder::primitive_desc(
        eng, input.get_desc(), eng, in_md, reorder_attr);
    in_reorder_ = dnnl::reorder(in_reorder_pd);
    if (scratchpad_md.get_size() < in_reorder_pd.scratchpad_desc().get_size()) {
      scratchpad_md = in_reorder_pd.scratchpad_desc();
    }

    if (use_skip_) {
      auto skip_reorder_pd = dnnl::reorder::primitive_desc(
          eng, output.get_desc(), eng, out_md, reorder_attr);
      skip_reorder_ = dnnl::reorder(skip_reorder_pd);
      if (scratchpad_md.get_size() <
          skip_reorder_pd.scratchpad_desc().get_size()) {
        scratchpad_md = skip_reorder_pd.scratchpad_desc();
      }
    }

    scratchpad_mem = dnnl::memory(scratchpad_md, eng);

    last_batch_ = N;
  }

  if (in_md != input.get_desc()) {
    auto tmp = dnnl::memory(in_md, eng);
    in_reorder_.execute(stream, {{DNNL_ARG_SRC, input},
                                 {DNNL_ARG_DST, tmp},
                                 {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});
    input = tmp;
  }

  if (!output || out_md != output.get_desc()) {
    if (use_skip_) {
      auto tmp = dnnl::memory(out_md, eng);
      skip_reorder_.execute(stream, {{DNNL_ARG_SRC, output},
                                     {DNNL_ARG_DST, tmp},
                                     {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});
      output = tmp;
    } else {
      output = dnnl::memory(out_md, eng);
    }
  }

  conv_.execute(stream, {{DNNL_ARG_SRC, input},
                         {DNNL_ARG_WEIGHTS, conv_filter_mem},
                         {DNNL_ARG_BIAS, bias_mem},
                         {DNNL_ARG_DST, output},
                         {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});

  if (activation_ == MISH) {
    mish_.execute(stream, {{DNNL_ARG_SRC, output},
                           {DNNL_ARG_DST, output},
                           {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});
  }
}

SELayer::SELayer(BaseLayer* ip, int fc1Outputs, ActivationFunction activation)
    : BaseLayer(ip->GetC(), ip->GetH(), ip->GetW(), ip),
      numFc1Out_(fc1Outputs),
      activation_(activation) {}

void SELayer::LoadWeights(dnnl::memory& w1, dnnl::memory& b1, dnnl::memory& w2,
                          dnnl::memory& b2, dnnl::engine& eng,
                          dnnl::stream& stream) {
  auto filter_md = dnnl::memory::desc({numFc1Out_, C}, data_type_,
                                      dnnl::memory::format_tag::ab);
  filter_mem = dnnl::memory(filter_md, eng);
  dnnl::reorder(w1, filter_mem).execute(stream, w1, filter_mem);

  auto bias_md =
      dnnl::memory::desc({numFc1Out_}, data_type_, dnnl::memory::format_tag::a);
  bias_mem = dnnl::memory(bias_md, eng);
  dnnl::reorder(b1, bias_mem).execute(stream, b1, bias_mem);

  auto filter2_md = dnnl::memory::desc({2 * C, numFc1Out_}, data_type_,
                                       dnnl::memory::format_tag::ab);

  filter2_mem = dnnl::memory(filter2_md, eng);
  dnnl::reorder(w2, filter2_mem).execute(stream, w2, filter2_mem);

  auto bias2_md =
      dnnl::memory::desc({2 * C}, data_type_, dnnl::memory::format_tag::a);

  bias2_mem = dnnl::memory(bias2_md, eng);
  dnnl::reorder(b2, bias2_mem).execute(stream, b2, bias2_mem);
}

void SELayer::Eval(int N, dnnl::memory& output, dnnl::memory& input,
                   dnnl::engine& eng, dnnl::stream& stream) {
  std::lock_guard<std::mutex> lock(lock_);
  if (last_batch_ != N) {
    // Also the broadcast input memory format for the binary primitives.
    auto t_pool_out_md = dnnl::memory::desc({N, C, 1, 1}, data_type_,
                                            dnnl::memory::format_tag::any);

    // Also the output memory format for the fc2 inner products.
    auto t_fc1_in_md =
        dnnl::memory::desc({N, C}, data_type_, dnnl::memory::format_tag::any);

    auto t_fc1_out_md = dnnl::memory::desc({N, numFc1Out_}, data_type_,
                                           dnnl::memory::format_tag::any);

    auto t_filter_md = dnnl::memory::desc({numFc1Out_, C}, data_type_,
                                          dnnl::memory::format_tag::any);

    auto t_filter2_md = dnnl::memory::desc({2 * C, numFc1Out_}, data_type_,
                                           dnnl::memory::format_tag::any);

    auto t_fc2_out_md = dnnl::memory::desc({N, 2 * C}, data_type_,
                                           dnnl::memory::format_tag::any);

    auto pooling_d = dnnl::pooling_forward::desc(
        dnnl::prop_kind::forward_inference, dnnl::algorithm::pooling_avg,
        input.get_desc(), t_pool_out_md, {1, 1}, {H, W}, {0, 0}, {0, 0});
    dnnl::primitive_attr pooling_attr;
    pooling_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
    auto pooling_pd =
        dnnl::pooling_forward::primitive_desc(pooling_d, pooling_attr, eng);
    pooling_ = dnnl::pooling_forward(pooling_pd);
    auto scratchpad_md = pooling_pd.scratchpad_desc();

    // This is also the optimized memory format descriptor for the binary
    // primitives.
    pool_out_md = pooling_pd.dst_desc();

    auto fc_d = dnnl::inner_product_forward::desc(
        dnnl::prop_kind::forward_inference, t_fc1_in_md, t_filter_md,
        bias_mem.get_desc(), t_fc1_out_md);
    dnnl::post_ops fc_ops;
    if (activation_ == RELU) {
      fc_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_relu, 0.0f, 0.0f);
    } else if (activation_ == MISH) {
      fc_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_mish, 0.0f, 0.0f);
    } else if (activation_ == TANH) {
      fc_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_tanh, 0.0f, 0.0f);
    }
    dnnl::primitive_attr fc_attr;
    fc_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
    fc_attr.set_post_ops(fc_ops);
    auto fc_pd =
        dnnl::inner_product_forward::primitive_desc(fc_d, fc_attr, eng);
    fc_ = dnnl::inner_product_forward(fc_pd);
    if (scratchpad_md.get_size() < fc_pd.scratchpad_desc().get_size()) {
      scratchpad_md = fc_pd.scratchpad_desc();
    }

    fc1_in_md = fc_pd.src_desc().reshape({N, C, 1, 1});
    fc1_out_md = fc_pd.dst_desc();
    if (fc_pd.weights_desc() != filter_mem.get_desc()) {
      auto tmp = dnnl::memory(fc_pd.weights_desc(), eng);
      dnnl::reorder(filter_mem, tmp).execute(stream, filter_mem, tmp);
      filter_mem = tmp;
    }

    auto fc2_d = dnnl::inner_product_forward::desc(
        dnnl::prop_kind::forward_inference, fc1_out_md, t_filter2_md,
        bias2_mem.get_desc(), t_fc2_out_md);
    dnnl::primitive_attr fc2_attr;
    fc2_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
    auto fc2_pd =
        dnnl::inner_product_forward::primitive_desc(fc2_d, fc2_attr, eng);
    fc2_ = dnnl::inner_product_forward(fc2_pd);
    if (scratchpad_md.get_size() < fc2_pd.scratchpad_desc().get_size()) {
      scratchpad_md = fc2_pd.scratchpad_desc();
    }

    if (fc2_pd.weights_desc() != filter2_mem.get_desc()) {
      auto tmp = dnnl::memory(fc2_pd.weights_desc(), eng);
      dnnl::reorder(filter2_mem, tmp).execute(stream, filter2_mem, tmp);
      filter2_mem = tmp;
    }

    fc2_out_md = fc2_pd.dst_desc();

    auto sigmoid_d = dnnl::eltwise_forward::desc(
        dnnl::prop_kind::forward_inference, dnnl::algorithm::eltwise_logistic,
        pool_out_md, 0.f, 0.f);
    dnnl::primitive_attr sigmoid_attr;
    sigmoid_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
    auto sigmoid_pd =
        dnnl::eltwise_forward::primitive_desc(sigmoid_d, sigmoid_attr, eng);
    sigmoid_ = dnnl::eltwise_forward(sigmoid_pd);
    if (scratchpad_md.get_size() < sigmoid_pd.scratchpad_desc().get_size()) {
      scratchpad_md = sigmoid_pd.scratchpad_desc();
    }

    auto mul_d =
        dnnl::binary::desc(dnnl::algorithm::binary_mul, input.get_desc(),
                           pool_out_md, output.get_desc());
    dnnl::post_ops mul_ops;
    mul_ops.append_sum();
    if (eng.get_kind() == dnnl::engine::kind::gpu) {
      // Using binary post-ops is a gain on gpu but a huge loss on cpu.
      mul_ops.append_binary(dnnl::algorithm::binary_add, pool_out_md);
      if (activation_ == RELU) {
        mul_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_relu, 0.0f, 0.0f);
      } else if (activation_ == MISH) {
        mul_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_mish, 0.0f, 0.0f);
      } else if (activation_ == TANH) {
        mul_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_tanh, 0.0f, 0.0f);
      }
    }
    dnnl::primitive_attr mul_attr;
    mul_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
    mul_attr.set_post_ops(mul_ops);
    auto mul_pd = dnnl::binary::primitive_desc(mul_d, mul_attr, eng);
    mul_ = dnnl::binary(mul_pd);
    if (scratchpad_md.get_size() < mul_pd.scratchpad_desc().get_size()) {
      scratchpad_md = mul_pd.scratchpad_desc();
    }

    if (eng.get_kind() != dnnl::engine::kind::gpu) {
      auto add_d =
          dnnl::binary::desc(dnnl::algorithm::binary_add, output.get_desc(),
                             pool_out_md, output.get_desc());
      dnnl::post_ops add_ops;
      if (activation_ == RELU) {
        add_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_relu, 0.0f, 0.0f);
      } else if (activation_ == MISH) {
        add_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_mish, 0.0f, 0.0f);
      } else if (activation_ == TANH) {
        add_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_tanh, 0.0f, 0.0f);
      }
      dnnl::primitive_attr add_attr;
      add_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
      add_attr.set_post_ops(add_ops);
      auto add_pd = dnnl::binary::primitive_desc(add_d, add_attr, eng);
      add_ = dnnl::binary(add_pd);
      if (scratchpad_md.get_size() < add_pd.scratchpad_desc().get_size()) {
        scratchpad_md = add_pd.scratchpad_desc();
      }
    }

    dnnl::primitive_attr reorder_attr;
    reorder_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
    auto fc1_reorder_pd = dnnl::reorder::primitive_desc(
        eng, pool_out_md, eng, fc1_in_md, reorder_attr);
    fc1_reorder_ = dnnl::reorder(fc1_reorder_pd);
    if (scratchpad_md.get_size() <
        fc1_reorder_pd.scratchpad_desc().get_size()) {
      scratchpad_md = fc1_reorder_pd.scratchpad_desc();
    }

    auto mul_reorder_pd = dnnl::reorder::primitive_desc(
        eng, fc2_out_md.submemory_desc({N, C}, {0, 0}).reshape({N, C, 1, 1}),
        eng, pool_out_md, reorder_attr);
    mul_reorder_ = dnnl::reorder(mul_reorder_pd);
    if (scratchpad_md.get_size() <
        mul_reorder_pd.scratchpad_desc().get_size()) {
      scratchpad_md = mul_reorder_pd.scratchpad_desc();
    }

    auto add_reorder_pd = dnnl::reorder::primitive_desc(
        eng, fc2_out_md.submemory_desc({N, C}, {0, C}).reshape({N, C, 1, 1}),
        eng, pool_out_md, reorder_attr);
    add_reorder_ = dnnl::reorder(add_reorder_pd);
    if (scratchpad_md.get_size() <
        add_reorder_pd.scratchpad_desc().get_size()) {
      scratchpad_md = add_reorder_pd.scratchpad_desc();
    }

    scratchpad_mem = dnnl::memory(scratchpad_md, eng);

    last_batch_ = N;
  }

  auto pool_out_mem = dnnl::memory(pool_out_md, eng);
  auto fc1_out_mem = dnnl::memory(fc1_out_md, eng);
  auto fc2_out_mem = dnnl::memory(fc2_out_md, eng);

  pooling_.execute(stream, {{DNNL_ARG_SRC, input},
                            {DNNL_ARG_DST, pool_out_mem},
                            {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});

  dnnl::memory fc1_in_mem;
  if (fc1_in_md != pool_out_md) {
    fc1_in_mem = dnnl::memory(fc1_in_md, eng);
    fc1_reorder_.execute(stream, {{DNNL_ARG_SRC, pool_out_mem},
                                  {DNNL_ARG_DST, fc1_in_mem},
                                  {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});
  } else {
    fc1_in_mem = pool_out_mem;
  }

  fc_.execute(stream, {{DNNL_ARG_SRC, fc1_in_mem},
                       {DNNL_ARG_WEIGHTS, filter_mem},
                       {DNNL_ARG_BIAS, bias_mem},
                       {DNNL_ARG_DST, fc1_out_mem},
                       {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});

  fc2_.execute(stream, {{DNNL_ARG_SRC, fc1_out_mem},
                        {DNNL_ARG_WEIGHTS, filter2_mem},
                        {DNNL_ARG_BIAS, bias2_mem},
                        {DNNL_ARG_DST, fc2_out_mem},
                        {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});

  dnnl::memory mul_in_mem;
  mul_in_mem = dnnl::memory(pool_out_md, eng);
  mul_reorder_.execute(stream, {{DNNL_ARG_SRC, fc2_out_mem},
                                {DNNL_ARG_DST, mul_in_mem},
                                {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});

  sigmoid_.execute(stream, {{DNNL_ARG_SRC, mul_in_mem},
                            {DNNL_ARG_DST, mul_in_mem},
                            {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});

  dnnl::memory add_in_mem;
  add_in_mem = dnnl::memory(pool_out_md, eng);
  add_reorder_.execute(stream, {{DNNL_ARG_SRC, fc2_out_mem},
                                {DNNL_ARG_DST, add_in_mem},
                                {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});

  if (eng.get_kind() == dnnl::engine::kind::gpu) {
    mul_.execute(stream, {{DNNL_ARG_SRC_0, input},
                          {DNNL_ARG_SRC_1, mul_in_mem},
                          {DNNL_ARG_ATTR_MULTIPLE_POST_OP(1) | DNNL_ARG_SRC_1,
                           add_in_mem},
                          {DNNL_ARG_DST, output},
                          {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});
  } else {
    mul_.execute(stream, {{DNNL_ARG_SRC_0, input},
                          {DNNL_ARG_SRC_1, mul_in_mem},
                          {DNNL_ARG_DST, output},
                          {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});

    add_.execute(stream, {{DNNL_ARG_SRC_0, output},
                          {DNNL_ARG_SRC_1, add_in_mem},
                          {DNNL_ARG_DST, output},
                          {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});
  }
}

FCLayer::FCLayer(BaseLayer* ip, int C, int H, int W,
                 ActivationFunction activation)
    : BaseLayer(C, H, W, ip), activation_(activation) {}

void FCLayer::LoadWeights(dnnl::memory& w1, dnnl::memory& b1, dnnl::engine& eng,
                          dnnl::stream& stream) {
  const int num_outputs = C * H * W;

  auto filter_md = dnnl::memory::desc(
      {num_outputs, input_->GetC(), input_->GetH(), input_->GetW()}, data_type_,
      dnnl::memory::format_tag::abcd);
  filter_mem = dnnl::memory(filter_md, eng);
  dnnl::reorder(w1, filter_mem).execute(stream, w1, filter_mem);

  auto bias_md = dnnl::memory::desc({num_outputs}, data_type_,
                                    dnnl::memory::format_tag::a);
  bias_mem = dnnl::memory(bias_md, eng);
  dnnl::reorder(b1, bias_mem).execute(stream, b1, bias_mem);
}

void FCLayer::Eval(int N, dnnl::memory& output, dnnl::memory& input,
                   dnnl::engine& eng, dnnl::stream& stream) {
  std::lock_guard<std::mutex> lock(lock_);
  if (last_batch_ != N) {
    const int num_outputs = C * H * W;

    auto t_in_md =
        dnnl::memory::desc({N, input_->GetC(), input_->GetH(), input_->GetW()},
                           data_type_, dnnl::memory::format_tag::any);

    auto t_filter_md = dnnl::memory::desc(
        {num_outputs, input_->GetC(), input_->GetH(), input_->GetW()},
        data_type_, dnnl::memory::format_tag::any);

    auto t_out_md = dnnl::memory::desc({N, C, H, W}, data_type_,
                                       dnnl::memory::format_tag::any);

    auto fc_d = dnnl::inner_product_forward::desc(
        dnnl::prop_kind::forward_inference, t_in_md, t_filter_md,
        bias_mem.get_desc(), t_out_md.reshape({N, num_outputs}));
    dnnl::post_ops fc_ops;
    if (activation_ == RELU) {
      fc_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_relu, 0.0f, 0.0f);
    } else if (activation_ == MISH) {
      fc_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_mish, 0.0f, 0.0f);
    } else if (activation_ == TANH) {
      fc_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_tanh, 0.0f, 0.0f);
    }
    dnnl::primitive_attr fc_attr;
    fc_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
    fc_attr.set_post_ops(fc_ops);
    auto fc_pd =
        dnnl::inner_product_forward::primitive_desc(fc_d, fc_attr, eng);
    fc_ = dnnl::inner_product_forward(fc_pd);
    auto scratchpad_md = fc_pd.scratchpad_desc();

    in_md = fc_pd.src_desc();
    out_md = fc_pd.dst_desc().reshape({N, C, H, W});
    if (fc_pd.weights_desc() != filter_mem.get_desc()) {
      auto tmp = dnnl::memory(fc_pd.weights_desc(), eng);
      dnnl::reorder(filter_mem, tmp).execute(stream, filter_mem, tmp);
      filter_mem = tmp;
    }

    dnnl::primitive_attr reorder_attr;
    reorder_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
    auto in_reorder_pd = dnnl::reorder::primitive_desc(
        eng, input.get_desc(), eng, in_md, reorder_attr);
    in_reorder_ = dnnl::reorder(in_reorder_pd);
    if (scratchpad_md.get_size() < in_reorder_pd.scratchpad_desc().get_size()) {
      scratchpad_md = in_reorder_pd.scratchpad_desc();
    }

    scratchpad_mem = dnnl::memory(fc_pd.scratchpad_desc(), eng);

    last_batch_ = N;
  }

  if (in_md != input.get_desc()) {
    auto tmp = dnnl::memory(in_md, eng);
    in_reorder_.execute(stream, {{DNNL_ARG_SRC, input},
                                 {DNNL_ARG_DST, tmp},
                                 {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});
    input = tmp;
  }

  if (!output || out_md != output.get_desc()) {
    output = dnnl::memory(out_md, eng);
  }

  fc_.execute(stream, {{DNNL_ARG_SRC, input},
                       {DNNL_ARG_WEIGHTS, filter_mem},
                       {DNNL_ARG_BIAS, bias_mem},
                       {DNNL_ARG_DST, output},
                       {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});
}

void AttentionPolicyHead::LoadWeights(dnnl::memory& w1, dnnl::memory& b1,
                                      dnnl::memory& w2, dnnl::memory& b2,
                                      dnnl::memory& w3, dnnl::memory& b3,
                                      dnnl::memory& w4, dnnl::engine& eng,
                                      dnnl::stream& stream) {
  auto fc_filter_md = dnnl::memory::desc({C, embedding_size_}, data_type_,
                                         dnnl::memory::format_tag::ab);
  fc_filter_mem = dnnl::memory(fc_filter_md, eng);
  dnnl::reorder(w1, fc_filter_mem).execute(stream, w1, fc_filter_mem);
  auto fc_bias_md = dnnl::memory::desc({embedding_size_}, data_type_,
                                       dnnl::memory::format_tag::a);
  fc_bias_mem = dnnl::memory(fc_bias_md, eng);
  dnnl::reorder(b1, fc_bias_mem).execute(stream, b1, fc_bias_mem);

  auto fcQK_filter_md =
      dnnl::memory::desc({embedding_size_, policy_d_model_}, data_type_,
                         dnnl::memory::format_tag::ab);

  fcQ_filter_mem = dnnl::memory(fcQK_filter_md, eng);
  dnnl::reorder(w2, fcQ_filter_mem).execute(stream, w2, fcQ_filter_mem);
  auto fcQK_bias_md = dnnl::memory::desc({policy_d_model_}, data_type_,
                                         dnnl::memory::format_tag::a);
  fcQ_bias_mem = dnnl::memory(fcQK_bias_md, eng);
  dnnl::reorder(b2, fcQ_bias_mem).execute(stream, b2, fcQ_bias_mem);

  fcK_filter_mem = dnnl::memory(fcQK_filter_md, eng);
  dnnl::reorder(w3, fcK_filter_mem).execute(stream, w3, fcK_filter_mem);
  fcK_bias_mem = dnnl::memory(fcQK_bias_md, eng);
  dnnl::reorder(b3, fcK_bias_mem).execute(stream, b3, fcK_bias_mem);

  auto pmul_md = dnnl::memory::desc({1, 4, policy_d_model_}, data_type_,
                                    dnnl::memory::format_tag::abc);
  pmul_mem = dnnl::memory(pmul_md, eng);
  dnnl::reorder(w4, pmul_mem).execute(stream, w4, pmul_mem);
}

void AttentionPolicyHead::Eval(int N, dnnl::memory& output, dnnl::memory& input,
                               dnnl::engine& eng, dnnl::stream& stream) {
  std::lock_guard<std::mutex> lock(lock_);
  if (last_batch_ != N) {
    in_md = dnnl::memory::desc({N, C, H, W}, data_type_,
                               dnnl::memory::format_tag::nhwc);
    out_md = dnnl::memory::desc({N, 67, 8, 8}, data_type_,
                                dnnl::memory::format_tag::nchw);

    auto fc_out_md = dnnl::memory::desc({N * 64, embedding_size_}, data_type_,
                                        dnnl::memory::format_tag::ab);
    fc_out_mem = dnnl::memory(fc_out_md, eng);
    auto foo_md = dnnl::memory::desc({N, H, W, C}, data_type_,
                                     dnnl::memory::format_tag::nchw);
    auto fc_d = dnnl::inner_product_forward::desc(
        dnnl::prop_kind::forward_inference, foo_md.reshape({N * H * W, C}),
        fc_filter_mem.get_desc(), fc_bias_mem.get_desc(), fc_out_md);
    dnnl::post_ops fc_ops;
    // SELU activation.
    fc_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_elu, 1.67326324f,
                          0.0f);
    fc_ops.append_eltwise(1.0f, dnnl::algorithm::eltwise_linear, 1.05070098f,
                          0.0f);
    dnnl::primitive_attr fc_attr;
    fc_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
    fc_attr.set_post_ops(fc_ops);
    auto fc_pd =
        dnnl::inner_product_forward::primitive_desc(fc_d, fc_attr, eng);
    fc_ = dnnl::inner_product_forward(fc_pd);
    auto scratchpad_md = fc_pd.scratchpad_desc();

    // Q
    auto fcQK_out_md = dnnl::memory::desc({N * 64, policy_d_model_}, data_type_,
                                          dnnl::memory::format_tag::ab);
    fcQ_out_mem = dnnl::memory(fcQK_out_md, eng);

    auto fcQK_d = dnnl::inner_product_forward::desc(
        dnnl::prop_kind::forward_inference, fc_out_md,
        fcQ_filter_mem.get_desc(), fcQ_bias_mem.get_desc(), fcQK_out_md);
    dnnl::primitive_attr common_attr;
    common_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
    auto fcQK_pd =
        dnnl::inner_product_forward::primitive_desc(fcQK_d, common_attr, eng);
    fcQK_ = dnnl::inner_product_forward(fcQK_pd);
    if (scratchpad_md.get_size() < fcQK_pd.scratchpad_desc().get_size()) {
      scratchpad_md = fcQK_pd.scratchpad_desc();
    }

    // K
    fcK_out_mem = dnnl::memory(fcQK_out_md, eng);
    const float scaling = sqrtf(policy_d_model_);
    auto mul_A_md = dnnl::memory::desc({N, 64, policy_d_model_}, data_type_,
                                       dnnl::memory::format_tag::abc);
    auto mul_B_md = dnnl::memory::desc({N, policy_d_model_, 64}, data_type_,
                                       dnnl::memory::format_tag::acb);
    auto mul_C_md =
        dnnl::memory::desc({N, 64, 64}, data_type_, {64 * 67, 64, 1});
    auto mul_d = dnnl::matmul::desc(mul_A_md, mul_B_md, mul_C_md);
    dnnl::primitive_attr mul_attr;
    mul_attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
    mul_attr.set_output_scales(0, {1.0f / scaling});
    auto mul_pd = dnnl::matmul::primitive_desc(mul_d, mul_attr, eng);
    mul_ = dnnl::matmul(mul_pd);
    if (scratchpad_md.get_size() < mul_pd.scratchpad_desc().get_size()) {
      scratchpad_md = mul_pd.scratchpad_desc();
    }

    // The promotion offsets are stored in the output array for now.
    auto promo_md =
        out_md.submemory_desc({N, 1, 4, 8}, {0, 64, 0, 0}).reshape({N, 4, 8});

    if (eng.get_kind() == dnnl::engine::kind::gpu) {
      // The gpu matmul primitive ignores memory offsets, so a copy is needed.
      auto reorder_pd = dnnl::reorder::primitive_desc(
          eng, mul_B_md.submemory_desc({N, policy_d_model_, 8}, {0, 0, 56}),
          eng, mul_B_md.submemory_desc({N, policy_d_model_, 8}, {0, 0, 0}),
          common_attr);
      hack_reorder_ = dnnl::reorder(reorder_pd);
      if (scratchpad_md.get_size() < reorder_pd.scratchpad_desc().get_size()) {
        scratchpad_md = reorder_pd.scratchpad_desc();
      }

      auto pmul_d = dnnl::matmul::desc(
          pmul_mem.get_desc(),
          mul_B_md.submemory_desc({N, policy_d_model_, 8}, {0, 0, 0}),
          mul_A_md.submemory_desc({N, 4, 8}, {0, 0, 0}));
      auto pmul_pd = dnnl::matmul::primitive_desc(pmul_d, common_attr, eng);
      pmul_ = dnnl::matmul(pmul_pd);
      if (scratchpad_md.get_size() < pmul_pd.scratchpad_desc().get_size()) {
        scratchpad_md = pmul_pd.scratchpad_desc();
      }

      reorder_pd = dnnl::reorder::primitive_desc(
          eng, mul_A_md.submemory_desc({N, 4, 8}, {0, 0, 0}), eng, promo_md,
          common_attr);
      hack_reorder_2_ = dnnl::reorder(reorder_pd);
      if (scratchpad_md.get_size() < reorder_pd.scratchpad_desc().get_size()) {
        scratchpad_md = reorder_pd.scratchpad_desc();
      }

    } else {
      auto pmul_d = dnnl::matmul::desc(
          pmul_mem.get_desc(),
          mul_B_md.submemory_desc({N, policy_d_model_, 8}, {0, 0, 56}),
          promo_md);
      auto pmul_pd = dnnl::matmul::primitive_desc(pmul_d, common_attr, eng);
      pmul_ = dnnl::matmul(pmul_pd);
      if (scratchpad_md.get_size() < pmul_pd.scratchpad_desc().get_size()) {
        scratchpad_md = pmul_pd.scratchpad_desc();
      }
    }

    auto in_reorder_pd = dnnl::reorder::primitive_desc(eng, input.get_desc(),
                                                       eng, in_md, common_attr);
    in_reorder_ = dnnl::reorder(in_reorder_pd);
    if (scratchpad_md.get_size() < in_reorder_pd.scratchpad_desc().get_size()) {
      scratchpad_md = in_reorder_pd.scratchpad_desc();
    }

    scratchpad_mem = dnnl::memory(scratchpad_md, eng);

    last_batch_ = N;
  }

  // Convert to NHWC.
  if (in_md != input.get_desc()) {
    auto tmp = dnnl::memory(in_md, eng);
    in_reorder_.execute(stream, {{DNNL_ARG_SRC, input},
                                 {DNNL_ARG_DST, tmp},
                                 {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});
    input = tmp;
  }

  if (!output || out_md != output.get_desc()) {
    output = dnnl::memory(out_md, eng);
  }

  fc_.execute(stream, {{DNNL_ARG_SRC, input},
                       {DNNL_ARG_WEIGHTS, fc_filter_mem},
                       {DNNL_ARG_BIAS, fc_bias_mem},
                       {DNNL_ARG_DST, fc_out_mem},
                       {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});

  fcQK_.execute(stream, {{DNNL_ARG_SRC, fc_out_mem},
                         {DNNL_ARG_WEIGHTS, fcQ_filter_mem},
                         {DNNL_ARG_BIAS, fcQ_bias_mem},
                         {DNNL_ARG_DST, fcQ_out_mem},
                         {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});

  fcQK_.execute(stream, {{DNNL_ARG_SRC, fc_out_mem},
                         {DNNL_ARG_WEIGHTS, fcK_filter_mem},
                         {DNNL_ARG_BIAS, fcK_bias_mem},
                         {DNNL_ARG_DST, fcK_out_mem},
                         {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});

  mul_.execute(stream, {{DNNL_ARG_SRC, fcQ_out_mem},
                        {DNNL_ARG_WEIGHTS, fcK_out_mem},
                        {DNNL_ARG_DST, output},
                        {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});

  if (eng.get_kind() == dnnl::engine::kind::gpu) {
    hack_reorder_.execute(stream, {{DNNL_ARG_SRC, fcK_out_mem},
                                   {DNNL_ARG_DST, fcQ_out_mem},
                                   {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});
    pmul_.execute(stream, {{DNNL_ARG_SRC, pmul_mem},
                           {DNNL_ARG_WEIGHTS, fcQ_out_mem},
                           {DNNL_ARG_DST, fcK_out_mem},
                           {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});
    hack_reorder_2_.execute(stream, {{DNNL_ARG_SRC, fcK_out_mem},
                                     {DNNL_ARG_DST, output},
                                     {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});

  } else {
    pmul_.execute(stream, {{DNNL_ARG_SRC, pmul_mem},
                           {DNNL_ARG_WEIGHTS, fcK_out_mem},
                           {DNNL_ARG_DST, output},
                           {DNNL_ARG_SCRATCHPAD, scratchpad_mem}});
  }
}

}  // namespace onednn_backend
}  // namespace lczero

```

`src/neural/onednn/layers.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021-2022 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#pragma once

#include "neural/shared/activation.h"
#include "utils/exception.h"

#include "dnnl.hpp"

namespace lczero {
namespace onednn_backend {

// The Layer objects only hold memory for weights, biases, etc
// memory for input and output tensors is provided by caller of Eval.

class BaseLayer {
 public:
  int GetC() const { return C; }
  int GetH() const { return H; }
  int GetW() const { return W; }

  BaseLayer(int c, int h, int w, BaseLayer* ip);
  virtual ~BaseLayer() = default;
  size_t GetOutputSize(int N) const { return sizeof(float) * N * C * H * W; }
  void SetDataType(dnnl::memory::data_type type) { data_type_ = type; }
  void SetConvolutionType(dnnl::algorithm type) { convolution_type_ = type; }
  virtual void Eval(int N, dnnl::memory& output, dnnl::memory& input,
                    dnnl::engine& eng, dnnl::stream& stream) = 0;

 protected:
  BaseLayer* input_;

  int C;  // Output tensor dimensions.
  int H;
  int W;
  dnnl::memory::data_type data_type_;
  dnnl::algorithm convolution_type_;
  std::mutex lock_;
};

class ConvLayer : public BaseLayer {
 public:
  ConvLayer(BaseLayer* ip, int C, int H, int W, int size, int Cin,
            ActivationFunction activation = NONE, bool skip = false);

  void LoadWeights(dnnl::memory& w1, dnnl::memory& b1, dnnl::engine& eng,
                   dnnl::stream& stream);

  // If there is a skip connection the output doubles as an input.
  void Eval(int N, dnnl::memory& output, dnnl::memory& input, dnnl::engine& eng,
            dnnl::stream& stream) override;

 private:
  const int c_input_;
  const int filter_size_;
  const ActivationFunction activation_;
  const bool use_skip_;

  dnnl::memory filter_mem;       // The original weights.
  dnnl::memory conv_filter_mem;  // Transformed weights (maybe for Winograd).
  dnnl::memory bias_mem;

  // Cache previous convolution primitive in case the batch size is the same.
  int last_batch_ = 0;
  dnnl::convolution_forward conv_;
  dnnl::eltwise_forward mish_;
  dnnl::reorder in_reorder_;
  dnnl::reorder skip_reorder_;
  dnnl::memory scratchpad_mem;
  // Cached values to change in/out tensors for best performance.
  dnnl::memory::desc in_md;
  dnnl::memory::desc out_md;
};

class FCLayer : public BaseLayer {
 public:
  FCLayer(BaseLayer* ip, int C, int H, int W, ActivationFunction activation);

  void LoadWeights(dnnl::memory& w1, dnnl::memory& b1, dnnl::engine& eng,
                   dnnl::stream& stream);
  void Eval(int N, dnnl::memory& output, dnnl::memory& input, dnnl::engine& eng,
            dnnl::stream& stream) override;

 private:
  ActivationFunction activation_;

  dnnl::memory filter_mem;
  dnnl::memory bias_mem;

  // Cache previous primitive in case the batch size is the same.
  int last_batch_ = 0;
  dnnl::inner_product_forward fc_;
  dnnl::reorder in_reorder_;
  dnnl::memory scratchpad_mem;
  // Cached values to change in/out tensors for best performance.
  dnnl::memory::desc in_md;
  dnnl::memory::desc out_md;
};

// Fused SE layer:
// global avg -> FC1 -> FC2 -> global scale -> add skip connection ->
// activation.
class SELayer : public BaseLayer {
 public:
  SELayer(BaseLayer* ip, int numFc1Out, ActivationFunction activation);

  void LoadWeights(dnnl::memory& w1, dnnl::memory& b1, dnnl::memory& w2,
                   dnnl::memory& b2, dnnl::engine& eng, dnnl::stream& stream);

  // Initially output holds the skip connection. Both input and output are
  // assumed to be the same memory format.
  void Eval(int N, dnnl::memory& output, dnnl::memory& input, dnnl::engine& eng,
            dnnl::stream& stream) override;

 private:
  dnnl::memory filter_mem;
  dnnl::memory bias_mem;
  dnnl::memory filter2_mem;
  dnnl::memory bias2_mem;

  int numFc1Out_;

  ActivationFunction activation_;

  // Cache previous primitives in case the batch size is the same.
  int last_batch_ = 0;
  dnnl::pooling_forward pooling_;
  dnnl::inner_product_forward fc_;
  dnnl::inner_product_forward fc2_;
  dnnl::eltwise_forward sigmoid_;
  dnnl::binary mul_;
  dnnl::binary add_;
  dnnl::reorder fc1_reorder_;
  dnnl::reorder mul_reorder_;
  dnnl::reorder add_reorder_;
  dnnl::memory scratchpad_mem;

  // Cached values to change tensors for best performance.
  dnnl::memory::desc pool_out_md;
  dnnl::memory::desc fc1_in_md;
  dnnl::memory::desc fc1_out_md;
  dnnl::memory::desc fc2_out_md;
};

class AttentionPolicyHead : public BaseLayer {
 public:
  AttentionPolicyHead(BaseLayer* ip, const int embedding_size,
                      const int policy_d_model)
      : BaseLayer(ip->GetC(), ip->GetH(), ip->GetW(), ip),
        embedding_size_(embedding_size),
        policy_d_model_(policy_d_model) {}
  void LoadWeights(dnnl::memory& w1, dnnl::memory& b1, dnnl::memory& w2,
                   dnnl::memory& b2, dnnl::memory& w3, dnnl::memory& b3,
                   dnnl::memory& w4, dnnl::engine& eng, dnnl::stream& stream);
  void Eval(int N, dnnl::memory& output, dnnl::memory& input, dnnl::engine& eng,
            dnnl::stream& stream) override;

 private:
  const int embedding_size_;
  const int policy_d_model_;

  dnnl::memory fc_filter_mem;
  dnnl::memory fc_bias_mem;
  dnnl::memory fcQ_filter_mem;
  dnnl::memory fcQ_bias_mem;
  dnnl::memory fcK_filter_mem;
  dnnl::memory fcK_bias_mem;
  dnnl::memory pmul_mem;

  // Cache previous primitives in case the batch size is the same.
  int last_batch_ = 0;

  dnnl::memory::desc in_md;
  dnnl::memory::desc out_md;

  dnnl::memory fc_out_mem;
  dnnl::memory fcQ_out_mem;
  dnnl::memory fcK_out_mem;
  dnnl::memory promo_mem;

  dnnl::memory scratchpad_mem;

  dnnl::reorder in_reorder_;
  dnnl::inner_product_forward fc_;
  dnnl::inner_product_forward fcQK_;
  dnnl::matmul mul_;
  dnnl::matmul pmul_;
  dnnl::binary add_;
  dnnl::binary add2_;
  // For gpu bug workaround.
  dnnl::reorder hack_reorder_;
  dnnl::reorder hack_reorder_2_;
};

}  // namespace onednn_backend
}  // namespace lczero

```

`src/neural/onednn/network_onednn.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021-2022 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#include <algorithm>
#include <cassert>
#include <cmath>
#include <functional>
#include <list>
#include <memory>
#include <mutex>

#include "layers.h"
#include "neural/factory.h"
#include "neural/network_legacy.h"
#include "neural/shared/attention_policy_map.h"
#include "neural/shared/policy_map.h"
#include "utils/bititer.h"
#include "utils/exception.h"

#include <omp.h>

namespace lczero {
using namespace onednn_backend;

static constexpr int kNumOutputPolicy = 1858;

struct InputsOutputs {
  InputsOutputs(int maxBatchSize, bool wdl, bool moves_left) {
    input_masks_mem_ =
        (uint64_t*)malloc(maxBatchSize * kInputPlanes * sizeof(uint64_t));

    input_val_mem_ =
        (float*)malloc(maxBatchSize * kInputPlanes * sizeof(float));

    op_policy_mem_ =
        (float*)malloc(maxBatchSize * kNumOutputPolicy * sizeof(float));

    op_value_mem_ =
        (float*)malloc(maxBatchSize * (wdl ? 3 : 1) * sizeof(float));

    if (moves_left) {
      op_moves_left_mem_ = (float*)malloc(maxBatchSize * sizeof(float));
    } else
      op_moves_left_mem_ = nullptr;
  }
  ~InputsOutputs() {
    free(input_masks_mem_);
    free(input_val_mem_);
    free(op_policy_mem_);
    free(op_value_mem_);
    if (op_moves_left_mem_) {
      free(op_moves_left_mem_);
    }
  }
  uint64_t* input_masks_mem_;
  float* input_val_mem_;
  float* op_policy_mem_;
  float* op_value_mem_;
  float* op_moves_left_mem_;
};

class OnednnNetwork;

class OnednnNetworkComputation : public NetworkComputation {
 public:
  OnednnNetworkComputation(OnednnNetwork* network, bool wdl, bool moves_left);
  ~OnednnNetworkComputation();

  void AddInput(InputPlanes&& input) override {
    const auto iter_mask =
        &inputs_outputs_->input_masks_mem_[batch_size_ * kInputPlanes];
    const auto iter_val =
        &inputs_outputs_->input_val_mem_[batch_size_ * kInputPlanes];

    int i = 0;
    for (const auto& plane : input) {
      iter_mask[i] = plane.mask;
      iter_val[i] = plane.value;
      i++;
    }

    batch_size_++;
  }

  void ComputeBlocking() override;

  int GetBatchSize() const override { return batch_size_; }

  float GetQVal(int sample) const override {
    if (wdl_) {
      auto w = inputs_outputs_->op_value_mem_[3 * sample + 0];
      auto l = inputs_outputs_->op_value_mem_[3 * sample + 2];
      return w - l;
    } else {
      return inputs_outputs_->op_value_mem_[sample];
    }
  }

  float GetDVal(int sample) const override {
    if (wdl_) {
      auto d = inputs_outputs_->op_value_mem_[3 * sample + 1];
      return d;
    } else {
      return 0.0f;
    }
  }

  float GetPVal(int sample, int move_id) const override {
    return inputs_outputs_->op_policy_mem_[sample * kNumOutputPolicy + move_id];
  }

  float GetMVal(int sample) const override {
    if (moves_left_) {
      return inputs_outputs_->op_moves_left_mem_[sample];
    }
    return 0.0f;
  }

 private:
  // Memory holding inputs, outputs.
  std::unique_ptr<InputsOutputs> inputs_outputs_;
  int batch_size_;
  bool wdl_;
  bool moves_left_;

  OnednnNetwork* network_;
};

class OnednnNetwork : public Network {
 public:
  OnednnNetwork(const WeightsFile& file, const OptionsDict& options)
      : capabilities_{file.format().network_format().input(),
                      file.format().network_format().moves_left()} {
    LegacyWeights weights(file.weights());

    conv_policy_ = file.format().network_format().policy() ==
                   pblczero::NetworkFormat::POLICY_CONVOLUTION;

    attn_policy_ = file.format().network_format().policy() ==
                   pblczero::NetworkFormat::POLICY_ATTENTION;

    default_activation_ =
        file.format().network_format().default_activation() ==
                pblczero::NetworkFormat::DEFAULT_ACTIVATION_MISH
            ? MISH
            : RELU;

#if DNNL_VERSION_MAJOR * 100 + DNNL_VERSION_MINOR >= 105
    dnnl::set_primitive_cache_capacity(
        options.GetOrDefault<int>("jit_cache", 1024));
#endif

    if (!options.IsDefault<int>("threads")) {
      omp_set_num_threads(options.Get<int>("threads"));
    }

    cpu_eng_ = dnnl::engine(dnnl::engine::kind::cpu, 0);

    if (!options.IsDefault<int>("gpu")) {
      eng_ = dnnl::engine(dnnl::engine::kind::gpu, options.Get<int>("gpu"));
    } else {
      eng_ = cpu_eng_;
    }
    eng_stream_ = dnnl::stream(eng_);

    auto data_type = dnnl::memory::data_type::f32;
    if (options.GetOrDefault<bool>(
            "fp16", eng_.get_kind() == dnnl::engine::kind::gpu)) {
      if (eng_.get_kind() == dnnl::engine::kind::cpu) {
        data_type = dnnl::memory::data_type::bf16;
      } else {
        data_type = dnnl::memory::data_type::f16;
      }
    }

    // Unfortunately current oneDNN versions get this wrong, selecting Winograd
    // on gpu and not on cpu (last tested with version 2.6.0). So for the time
    // being this will be overriden in every case.
    auto convolution_type = dnnl::algorithm::convolution_auto;
    if (!options.IsDefault<bool>("winograd")) {
      if (options.Get<bool>("winograd")) {
        convolution_type = dnnl::algorithm::convolution_winograd;
      } else {
        convolution_type = dnnl::algorithm::convolution_direct;
      }
    } else {
      // Heuristic: only use Winograd convolution on cpu newer than avx2.
      if (eng_.get_kind() == dnnl::engine::kind::cpu &&
          dnnl::get_effective_cpu_isa() > dnnl::cpu_isa::avx2) {
        convolution_type = dnnl::algorithm::convolution_winograd;
      } else {
        convolution_type = dnnl::algorithm::convolution_direct;
      }
    }

    max_batch_size_ = options.GetOrDefault<int>("max_batch", 1024);

    batch_size_ = options.GetOrDefault<int>(
        "batch", data_type == dnnl::memory::data_type::f32 ? 32 : 64);

    steps_ = options.GetOrDefault<int>("steps", 2);
    if (batch_size_ <= 0) {
      steps_ = 1;
    } else if (steps_ > max_batch_size_ / batch_size_) {
      steps_ = max_batch_size_ / batch_size_;
    }

    // Default layout is nchw.
    numFilters_ = (int)weights.input.biases.size();
    numBlocks_ = (int)weights.residual.size();

    pol_channels_ = weights.policy.biases.size();

    // 1. Check for SE.
    has_se_ = false;
    if (weights.residual[0].has_se) {
      has_se_ = true;
    }

    // 2. Build the network, and copy the weights to GPU memory.

    layers_.resize(steps_);
    for (int idx = 0; idx < steps_; idx++) {
      // Input.
      {
        auto inputConv = std::make_unique<ConvLayer>(
            nullptr, numFilters_, 8, 8, 3, kInputPlanes, default_activation_);
        // Set the data type first, the following layers will pick it up.
        inputConv->SetDataType(data_type);
        inputConv->SetConvolutionType(convolution_type);
        auto w_md = dnnl::memory::desc({numFilters_, kInputPlanes, 3, 3},
                                       dnnl::memory::data_type::f32,
                                       dnnl::memory::format_tag::oihw);
        auto w_mem = dnnl::memory(w_md, cpu_eng_, &weights.input.weights[0]);
        auto b_md =
            dnnl::memory::desc({numFilters_}, dnnl::memory::data_type::f32,
                               dnnl::memory::format_tag::a);
        auto b_mem = dnnl::memory(b_md, cpu_eng_, &weights.input.biases[0]);
        inputConv->LoadWeights(w_mem, b_mem, eng_, eng_stream_);
        layers_[idx].emplace_back(std::move(inputConv));
      }

      // Residual block.
      for (size_t block = 0; block < weights.residual.size(); block++) {
        auto conv1 =
            std::make_unique<ConvLayer>(getLastLayer(idx), numFilters_, 8, 8, 3,
                                        numFilters_, default_activation_);
        auto w_md = dnnl::memory::desc({numFilters_, numFilters_, 3, 3},
                                       dnnl::memory::data_type::f32,
                                       dnnl::memory::format_tag::oihw);
        auto w_mem = dnnl::memory(w_md, cpu_eng_,
                                  &weights.residual[block].conv1.weights[0]);
        auto b_md =
            dnnl::memory::desc({numFilters_}, dnnl::memory::data_type::f32,
                               dnnl::memory::format_tag::a);
        auto b_mem = dnnl::memory(b_md, cpu_eng_,
                                  &weights.residual[block].conv1.biases[0]);
        conv1->LoadWeights(w_mem, b_mem, eng_, eng_stream_);
        layers_[idx].emplace_back(std::move(conv1));

        // Activation of second convolution and skip connection is handled by
        // SELayer.
        bool has_se = weights.residual[block].has_se;

        auto conv2 = std::make_unique<ConvLayer>(
            getLastLayer(idx), numFilters_, 8, 8, 3, numFilters_,
            has_se ? NONE : default_activation_, !has_se);
        w_mem = dnnl::memory(w_md, cpu_eng_,
                             &weights.residual[block].conv2.weights[0]);
        b_mem = dnnl::memory(b_md, cpu_eng_,
                             &weights.residual[block].conv2.biases[0]);
        conv2->LoadWeights(w_mem, b_mem, eng_, eng_stream_);
        layers_[idx].emplace_back(std::move(conv2));

        if (has_se) {
          int numFCOut = (int)weights.residual[block].se.b1.size();

          auto se = std::make_unique<SELayer>(getLastLayer(idx), numFCOut,
                                              default_activation_);
          w_md = dnnl::memory::desc({numFCOut, numFilters_},
                                    dnnl::memory::data_type::f32,
                                    dnnl::memory::format_tag::ab);
          w_mem =
              dnnl::memory(w_md, cpu_eng_, &weights.residual[block].se.w1[0]);
          b_md = dnnl::memory::desc({numFCOut}, dnnl::memory::data_type::f32,
                                    dnnl::memory::format_tag::a);
          b_mem =
              dnnl::memory(b_md, cpu_eng_, &weights.residual[block].se.b1[0]);
          auto w2_md = dnnl::memory::desc({2 * numFilters_, numFCOut},
                                          dnnl::memory::data_type::f32,
                                          dnnl::memory::format_tag::ab);
          auto w2_mem =
              dnnl::memory(w2_md, cpu_eng_, &weights.residual[block].se.w2[0]);
          auto b2_md = dnnl::memory::desc({2 * numFilters_},
                                          dnnl::memory::data_type::f32,
                                          dnnl::memory::format_tag::a);
          auto b2_mem =
              dnnl::memory(b2_md, cpu_eng_, &weights.residual[block].se.b2[0]);
          se->LoadWeights(w_mem, b_mem, w2_mem, b2_mem, eng_, eng_stream_);
          layers_[idx].emplace_back(std::move(se));
        }
      }

      BaseLayer* resi_last = getLastLayer(idx);

      // Policy head.
      if (attn_policy_) {
        for (auto layer : weights.pol_encoder) {
          // TODO: support encoder heads.
          throw Exception(
              "Encoder heads are not yet supported by the oneDNN backend.");
        }
        const int embedding_size = weights.ip_pol_b.size();
        const int policy_d_model = weights.ip2_pol_b.size();

        auto attn = std::make_unique<AttentionPolicyHead>(
            resi_last, embedding_size, policy_d_model);
        auto ip_w_md = dnnl::memory::desc({numFilters_, embedding_size},
                                          dnnl::memory::data_type::f32,
                                          dnnl::memory::format_tag::ab);
        auto ip_w_mem =
            dnnl::memory(ip_w_md, cpu_eng_, weights.ip_pol_w.data());
        auto ip_b_md =
            dnnl::memory::desc({embedding_size}, dnnl::memory::data_type::f32,
                               dnnl::memory::format_tag::a);
        auto ip_b_mem =
            dnnl::memory(ip_b_md, cpu_eng_, weights.ip_pol_b.data());
        auto ip23_w_md = dnnl::memory::desc({embedding_size, policy_d_model},
                                            dnnl::memory::data_type::f32,
                                            dnnl::memory::format_tag::ab);
        auto ip2_w_mem =
            dnnl::memory(ip23_w_md, cpu_eng_, weights.ip2_pol_w.data());
        auto ip23_b_md =
            dnnl::memory::desc({policy_d_model}, dnnl::memory::data_type::f32,
                               dnnl::memory::format_tag::a);
        auto ip2_b_mem =
            dnnl::memory(ip23_b_md, cpu_eng_, weights.ip2_pol_b.data());
        auto ip3_w_mem =
            dnnl::memory(ip23_w_md, cpu_eng_, weights.ip3_pol_w.data());
        auto ip3_b_mem =
            dnnl::memory(ip23_b_md, cpu_eng_, weights.ip3_pol_b.data());
        auto ip4_w_md = dnnl::memory::desc({1, 4, policy_d_model},
                                           dnnl::memory::data_type::f32,
                                           dnnl::memory::format_tag::abc);
        auto ip4_w_mem =
            dnnl::memory(ip4_w_md, cpu_eng_, weights.ip4_pol_w.data());
        attn->LoadWeights(ip_w_mem, ip_b_mem, ip2_w_mem, ip2_b_mem, ip3_w_mem,
                          ip3_b_mem, ip4_w_mem, eng_, eng_stream_);
        layers_[idx].emplace_back(std::move(attn));
      } else if (conv_policy_) {
        auto conv1 = std::make_unique<ConvLayer>(
            resi_last, numFilters_, 8, 8, 3, numFilters_, default_activation_);
        auto w_md = dnnl::memory::desc({numFilters_, numFilters_, 3, 3},
                                       dnnl::memory::data_type::f32,
                                       dnnl::memory::format_tag::oihw);
        auto w_mem = dnnl::memory(w_md, cpu_eng_, &weights.policy1.weights[0]);
        auto b_md =
            dnnl::memory::desc({numFilters_}, dnnl::memory::data_type::f32,
                               dnnl::memory::format_tag::a);
        auto b_mem = dnnl::memory(b_md, cpu_eng_, &weights.policy1.biases[0]);
        conv1->LoadWeights(w_mem, b_mem, eng_, eng_stream_);
        layers_[idx].emplace_back(std::move(conv1));

        // No Activation
        auto conv2 = std::make_unique<ConvLayer>(
            getLastLayer(idx), pol_channels_, 8, 8, 3, numFilters_, NONE);
        w_md = dnnl::memory::desc({pol_channels_, numFilters_, 3, 3},
                                  dnnl::memory::data_type::f32,
                                  dnnl::memory::format_tag::oihw);
        w_mem = dnnl::memory(w_md, cpu_eng_, &weights.policy.weights[0]);
        b_md = dnnl::memory::desc({pol_channels_}, dnnl::memory::data_type::f32,
                                  dnnl::memory::format_tag::a);
        b_mem = dnnl::memory(b_md, cpu_eng_, &weights.policy.biases[0]);
        conv2->LoadWeights(w_mem, b_mem, eng_, eng_stream_);
        layers_[idx].emplace_back(std::move(conv2));
      } else {
        auto convPol =
            std::make_unique<ConvLayer>(resi_last, pol_channels_, 8, 8, 1,
                                        numFilters_, default_activation_);
        auto w_md = dnnl::memory::desc({pol_channels_, numFilters_, 1, 1},
                                       dnnl::memory::data_type::f32,
                                       dnnl::memory::format_tag::oihw);
        auto w_mem = dnnl::memory(w_md, cpu_eng_, &weights.policy.weights[0]);
        auto b_md =
            dnnl::memory::desc({pol_channels_}, dnnl::memory::data_type::f32,
                               dnnl::memory::format_tag::a);
        auto b_mem = dnnl::memory(b_md, cpu_eng_, &weights.policy.biases[0]);
        convPol->LoadWeights(w_mem, b_mem, eng_, eng_stream_);
        layers_[idx].emplace_back(std::move(convPol));

        auto FCPol = std::make_unique<FCLayer>(getLastLayer(idx),
                                               kNumOutputPolicy, 1, 1, NONE);
        w_md = dnnl::memory::desc({kNumOutputPolicy, pol_channels_, 8, 8},
                                  dnnl::memory::data_type::f32,
                                  dnnl::memory::format_tag::abcd);
        w_mem = dnnl::memory(w_md, cpu_eng_, &weights.ip_pol_w[0]);
        b_md =
            dnnl::memory::desc({kNumOutputPolicy}, dnnl::memory::data_type::f32,
                               dnnl::memory::format_tag::a);
        b_mem = dnnl::memory(b_md, cpu_eng_, &weights.ip_pol_b[0]);
        FCPol->LoadWeights(w_mem, b_mem, eng_, eng_stream_);
        layers_[idx].emplace_back(std::move(FCPol));
      }

      // Value head.
      {
        value_channels_ = weights.ip1_val_b.size();
        value_input_planes_ = weights.value.biases.size();

        auto convVal =
            std::make_unique<ConvLayer>(resi_last, value_input_planes_, 8, 8, 1,
                                        numFilters_, default_activation_);
        auto w_md = dnnl::memory::desc({value_input_planes_, numFilters_, 1, 1},
                                       dnnl::memory::data_type::f32,
                                       dnnl::memory::format_tag::oihw);
        auto w_mem = dnnl::memory(w_md, cpu_eng_, &weights.value.weights[0]);
        auto b_md = dnnl::memory::desc({value_input_planes_},
                                       dnnl::memory::data_type::f32,
                                       dnnl::memory::format_tag::a);
        auto b_mem = dnnl::memory(b_md, cpu_eng_, &weights.value.biases[0]);
        convVal->LoadWeights(w_mem, b_mem, eng_, eng_stream_);
        layers_[idx].emplace_back(std::move(convVal));

        auto FCVal1 = std::make_unique<FCLayer>(
            getLastLayer(idx), value_channels_, 1, 1, default_activation_);
        w_md = dnnl::memory::desc({value_channels_, value_input_planes_, 8, 8},
                                  dnnl::memory::data_type::f32,
                                  dnnl::memory::format_tag::abcd);
        w_mem = dnnl::memory(w_md, cpu_eng_, &weights.ip1_val_w[0]);
        b_md =
            dnnl::memory::desc({value_channels_}, dnnl::memory::data_type::f32,
                               dnnl::memory::format_tag::a);
        b_mem = dnnl::memory(b_md, cpu_eng_, &weights.ip1_val_b[0]);
        FCVal1->LoadWeights(w_mem, b_mem, eng_, eng_stream_);
        layers_[idx].emplace_back(std::move(FCVal1));

        wdl_ = file.format().network_format().value() ==
               pblczero::NetworkFormat::VALUE_WDL;
        auto fc2_tanh = !wdl_;

        auto FCVal2 = std::make_unique<FCLayer>(getLastLayer(idx), wdl_ ? 3 : 1,
                                                1, 1, fc2_tanh ? TANH : NONE);
        w_md = dnnl::memory::desc({wdl_ ? 3 : 1, value_channels_, 1, 1},
                                  dnnl::memory::data_type::f32,
                                  dnnl::memory::format_tag::abcd);
        w_mem = dnnl::memory(w_md, cpu_eng_, &weights.ip2_val_w[0]);
        b_md = dnnl::memory::desc({wdl_ ? 3 : 1}, dnnl::memory::data_type::f32,
                                  dnnl::memory::format_tag::a);
        b_mem = dnnl::memory(b_md, cpu_eng_, &weights.ip2_val_b[0]);
        FCVal2->LoadWeights(w_mem, b_mem, eng_, eng_stream_);
        layers_[idx].emplace_back(std::move(FCVal2));
      }

      // Moves left head
      moves_left_ = (file.format().network_format().moves_left() ==
                     pblczero::NetworkFormat::MOVES_LEFT_V1) &&
                    options.GetOrDefault<bool>("mlh", true);
      if (moves_left_) {
        moves_channels_ = weights.ip1_mov_b.size();
        moves_input_planes_ = weights.moves_left.biases.size();

        auto convMov =
            std::make_unique<ConvLayer>(resi_last, moves_input_planes_, 8, 8, 1,
                                        numFilters_, default_activation_);
        auto w_md = dnnl::memory::desc({moves_input_planes_, numFilters_, 1, 1},
                                       dnnl::memory::data_type::f32,
                                       dnnl::memory::format_tag::oihw);
        auto w_mem =
            dnnl::memory(w_md, cpu_eng_, &weights.moves_left.weights[0]);

        auto b_md = dnnl::memory::desc({moves_input_planes_},
                                       dnnl::memory::data_type::f32,
                                       dnnl::memory::format_tag::a);
        auto b_mem =
            dnnl::memory(b_md, cpu_eng_, &weights.moves_left.biases[0]);
        convMov->LoadWeights(w_mem, b_mem, eng_, eng_stream_);
        layers_[idx].emplace_back(std::move(convMov));

        auto FCMov1 = std::make_unique<FCLayer>(
            getLastLayer(idx), moves_channels_, 1, 1, default_activation_);
        w_md = dnnl::memory::desc({moves_channels_, moves_input_planes_, 8, 8},
                                  dnnl::memory::data_type::f32,
                                  dnnl::memory::format_tag::abcd);
        w_mem = dnnl::memory(w_md, cpu_eng_, &weights.ip1_mov_w[0]);
        b_md =
            dnnl::memory::desc({moves_channels_}, dnnl::memory::data_type::f32,
                               dnnl::memory::format_tag::a);
        b_mem = dnnl::memory(b_md, cpu_eng_, &weights.ip1_mov_b[0]);
        FCMov1->LoadWeights(w_mem, b_mem, eng_, eng_stream_);
        layers_[idx].emplace_back(std::move(FCMov1));

        auto FCMov2 = std::make_unique<FCLayer>(getLastLayer(idx), 1, 1, 1,
                                                default_activation_);
        w_md = dnnl::memory::desc({1, moves_channels_, 1, 1},
                                  dnnl::memory::data_type::f32,
                                  dnnl::memory::format_tag::abcd);
        w_mem = dnnl::memory(w_md, cpu_eng_, &weights.ip2_mov_w[0]);
        b_md = dnnl::memory::desc({1}, dnnl::memory::data_type::f32,
                                  dnnl::memory::format_tag::a);
        b_mem = dnnl::memory(b_md, cpu_eng_, &weights.ip2_mov_b[0]);
        FCMov2->LoadWeights(w_mem, b_mem, eng_, eng_stream_);
        layers_[idx].emplace_back(std::move(FCMov2));
      }

      // Initialize layers if batch size fixed.
      if (options.GetOrDefault<bool>("init", true) && batch_size_ > 0) {
        int batchSize = (idx + 1) * batch_size_;
        InputsOutputs io(batchSize, wdl_, moves_left_);
        memset(io.input_masks_mem_, 0,
               batchSize * kInputPlanes * sizeof(uint64_t));
        memset(io.input_val_mem_, 0, batchSize * kInputPlanes * sizeof(float));
        forwardEval(&io, batchSize);
      }
    }
  }

  void forwardEval(InputsOutputs* io, int inputBatchSize) {
    // Expand packed planes to full planes.
    uint64_t* ipDataMasks = io->input_masks_mem_;
    float* ipDataValues = io->input_val_mem_;

    int batchSize = steps_ * batch_size_;
    if (batchSize <= 0) {
      // Use just one batch of variable size.
      batchSize = inputBatchSize;
    }

    // Break input batch in smaller batches.
    for (int start = 0; start < inputBatchSize; start += batchSize) {
      int idx = steps_ - 1;

      int currentBatchSize = inputBatchSize - start;
      if (currentBatchSize > batchSize) {
        currentBatchSize = batchSize;
      } else {
        idx = (currentBatchSize - 1) / batch_size_;
        batchSize = (idx + 1) * batch_size_;
      }

      auto input_desc = dnnl::memory::desc({batchSize, kInputPlanes, 8, 8},
                                           dnnl::memory::data_type::f32,
                                           dnnl::memory::format_tag::nchw);
      dnnl::memory input_mem = dnnl::memory(input_desc, cpu_eng_);

      float* buffer = (float*)input_mem.get_data_handle();
      for (int j = 0; j < currentBatchSize * kInputPlanes; j++) {
        const float value = ipDataValues[j + start * kInputPlanes];
        const uint64_t mask = ipDataMasks[j + start * kInputPlanes];
        for (auto i = 0; i < 64; i++)
          *(buffer++) = (mask & (((uint64_t)1) << i)) != 0 ? value : 0;
      }
      // Clear remaining buffer (if any).
      memset(buffer, 0, (batchSize - currentBatchSize) * kInputPlanes * 64 *
                            sizeof(float));

      // Move input to the gpu.
      if (eng_.get_kind() != dnnl::engine::kind::cpu) {
        auto tmp = dnnl::memory(input_desc, eng_);
        dnnl::reorder in_reorder = dnnl::reorder(input_mem, tmp);
        in_reorder.execute(eng_stream_, input_mem, tmp);
        input_mem = tmp;
      }

      // Output descriptors.
      dnnl::memory::desc opPol_desc;
      if (attn_policy_) {
        opPol_desc = dnnl::memory::desc({batchSize, 67, 8, 8},
                                        dnnl::memory::data_type::f32,
                                        dnnl::memory::format_tag::nchw);
      } else if (conv_policy_) {
        opPol_desc = dnnl::memory::desc({batchSize, pol_channels_, 8, 8},
                                        dnnl::memory::data_type::f32,
                                        dnnl::memory::format_tag::nchw);
      } else {
        opPol_desc = dnnl::memory::desc({batchSize, kNumOutputPolicy, 1, 1},
                                        dnnl::memory::data_type::f32,
                                        dnnl::memory::format_tag::nchw);
      }
      auto opVal_desc = dnnl::memory::desc({batchSize, wdl_ ? 3 : 1, 1, 1},
                                           dnnl::memory::data_type::f32,
                                           dnnl::memory::format_tag::nchw);
      auto opMov_desc =
          dnnl::memory::desc({batchSize, 1, 1, 1}, dnnl::memory::data_type::f32,
                             dnnl::memory::format_tag::nchw);
      // Output memory.
      dnnl::memory opPol_mem;
      dnnl::memory opVal_mem;
      dnnl::memory opMov_mem;

      // Intermediate tensors.
      dnnl::memory tensor_mem[3];

      int l = 0;

      // Input.
      layers_[idx][l++]->Eval(batchSize, tensor_mem[2], input_mem, eng_,
                              eng_stream_);  // input conv

      // Residual block.
      for (int block = 0; block < numBlocks_; block++) {
        layers_[idx][l++]->Eval(batchSize, tensor_mem[0], tensor_mem[2], eng_,
                                eng_stream_);  // conv1

        // For SE Resnet, skip connection is added after SE.
        if (has_se_) {
          layers_[idx][l++]->Eval(batchSize, tensor_mem[1], tensor_mem[0], eng_,
                                  eng_stream_);  // conv2
        } else {
          layers_[idx][l++]->Eval(batchSize, tensor_mem[2], tensor_mem[0], eng_,
                                  eng_stream_);  // conv2
        }

        if (has_se_) {
          layers_[idx][l++]->Eval(batchSize, tensor_mem[2], tensor_mem[1], eng_,
                                  eng_stream_);  // SE layer
        }
      }

      // Policy head.
      if (attn_policy_) {
        layers_[idx][l++]->Eval(batchSize, opPol_mem, tensor_mem[2], eng_,
                                eng_stream_);  // attention head
      } else if (conv_policy_) {
        layers_[idx][l++]->Eval(batchSize, tensor_mem[0], tensor_mem[2], eng_,
                                eng_stream_);  // policy conv1

        layers_[idx][l++]->Eval(batchSize, opPol_mem, tensor_mem[0], eng_,
                                eng_stream_);  // policy conv2
      } else {
        dnnl::memory policy_mem;
        layers_[idx][l++]->Eval(batchSize, policy_mem, tensor_mem[2], eng_,
                                eng_stream_);  // pol conv

        layers_[idx][l++]->Eval(batchSize, opPol_mem, policy_mem, eng_,
                                eng_stream_);  // pol FC  // POLICY
      }

      // value head
      {
        dnnl::memory tmp1_mem;
        dnnl::memory tmp2_mem;
        layers_[idx][l++]->Eval(batchSize, tmp1_mem, tensor_mem[2], eng_,
                                eng_stream_);  // value conv

        layers_[idx][l++]->Eval(batchSize, tmp2_mem, tmp1_mem, eng_,
                                eng_stream_);  // value FC1

        layers_[idx][l++]->Eval(batchSize, opVal_mem, tmp2_mem, eng_,
                                eng_stream_);  // value FC2    // VALUE
      }

      if (moves_left_) {
        // Moves left head
        dnnl::memory tmp1_mem;
        dnnl::memory tmp2_mem;
        layers_[idx][l++]->Eval(batchSize, tmp1_mem, tensor_mem[2], eng_,
                                eng_stream_);  // moves conv

        layers_[idx][l++]->Eval(batchSize, tmp2_mem, tmp1_mem, eng_,
                                eng_stream_);  // moves FC1

        // Moves left FC2
        layers_[idx][l++]->Eval(batchSize, opMov_mem, tmp2_mem, eng_,
                                eng_stream_);
      }

      // Convert output data to nchw and if on gpu move them to the cpu.
      if (opPol_desc != opPol_mem.get_desc() ||
          eng_.get_kind() != dnnl::engine::kind::cpu) {
        auto tmp = dnnl::memory(opPol_desc, cpu_eng_);
        dnnl::reorder pol_reorder = dnnl::reorder(opPol_mem, tmp);
        pol_reorder.execute(eng_stream_, opPol_mem, tmp);
        opPol_mem = tmp;
      }

      if (opVal_desc != opVal_mem.get_desc() ||
          eng_.get_kind() != dnnl::engine::kind::cpu) {
        auto tmp = dnnl::memory(opVal_desc, cpu_eng_);
        dnnl::reorder val_reorder_ = dnnl::reorder(opVal_mem, tmp);
        val_reorder_.execute(eng_stream_, opVal_mem, tmp);
        opVal_mem = tmp;
      }

      if (moves_left_ && (opMov_desc != opMov_mem.get_desc() ||
                          eng_.get_kind() != dnnl::engine::kind::cpu)) {
        auto tmp = dnnl::memory(opMov_desc, cpu_eng_);
        dnnl::reorder mov_reorder_ = dnnl::reorder(opMov_mem, tmp);
        mov_reorder_.execute(eng_stream_, opMov_mem, tmp);
        opMov_mem = tmp;
      }

      eng_stream_.wait();

      // Copy memory to output buffers and do final transformations.
      if (wdl_) {
        // Value softmax done cpu side.
        float* opVal = (float*)opVal_mem.get_data_handle();
        for (int i = 0; i < currentBatchSize; i++) {
          float w = opVal[3 * i + 0];
          float d = opVal[3 * i + 1];
          float l = opVal[3 * i + 2];
          float m = std::max({w, d, l});
          w = std::exp(w - m);
          d = std::exp(d - m);
          l = std::exp(l - m);
          float sum = w + d + l;
          w /= sum;
          l /= sum;
          d = 1.0f - w - l;
          io->op_value_mem_[3 * (i + start) + 0] = w;
          io->op_value_mem_[3 * (i + start) + 1] = d;
          io->op_value_mem_[3 * (i + start) + 2] = l;
        }
      } else {
        memcpy(io->op_value_mem_ + start, opVal_mem.get_data_handle(),
               currentBatchSize * sizeof(float));
      }
      if (attn_policy_) {
        float* opPol = (float*)opPol_mem.get_data_handle();
        // The promotion offsets are extracted from the output tensor.
        float promotion_offsets[3][8];
        for (int batch = 0; batch < currentBatchSize; batch++) {
          for (int i = 0; i < 3; i++) {
            for (int j = 0; j < 8; j++) {
              promotion_offsets[i][j] =
                  opPol[batch * (64 * 64 + 8 * 24) + 64 * 64 + i * 8 + j] +
                  opPol[batch * (64 * 64 + 8 * 24) + 64 * 64 + 24 + j];
            }
          }
          for (int x = 0; x < 64 * 64; x++) {
            auto y = kAttnPolicyMap[x];
            if (y >= 0) {
              io->op_policy_mem_[(batch + start) * kNumOutputPolicy + y] =
                  opPol[batch * (64 * 64 + 8 * 24) + x];
            }
          }
          for (int k = 0; k < 8; k++) {
            for (int j = 0; j < 8; j++) {
              for (int i = 0; i < 3; i++) {
                auto y = kAttnPolicyMap[64 * 64 + 24 * k + 3 * j + i];
                if (y >= 0) {
                  io->op_policy_mem_[(batch + start) * kNumOutputPolicy + y] =
                      opPol[batch * (64 * 64 + 8 * 24) + (48 + k) * 64 + 56 +
                            j] +
                      promotion_offsets[i][j];
                }
              }
            }
          }
        }
      } else if (conv_policy_) {
        float* opPol = (float*)opPol_mem.get_data_handle();
        for (int batch = 0; batch < currentBatchSize; batch++) {
          for (int i = 0; i < 73 * 8 * 8; i++) {
            auto j = kConvPolicyMap[i];
            if (j >= 0) {
              io->op_policy_mem_[(batch + start) * kNumOutputPolicy + j] =
                  opPol[batch * pol_channels_ * 64 + i];
            }
          }
        }
      } else {
        memcpy(io->op_policy_mem_ + start * kNumOutputPolicy,
               opPol_mem.get_data_handle(),
               currentBatchSize * kNumOutputPolicy * sizeof(float));
      }

      if (moves_left_) {
        memcpy(io->op_moves_left_mem_ + start, opMov_mem.get_data_handle(),
               currentBatchSize * sizeof(float));
      }
    }
  }

  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

  std::unique_ptr<NetworkComputation> NewComputation() override {
    return std::make_unique<OnednnNetworkComputation>(this, wdl_, moves_left_);
  }

  std::unique_ptr<InputsOutputs> GetInputsOutputs() {
    std::lock_guard<std::mutex> lock(inputs_outputs_lock_);
    if (free_inputs_outputs_.empty()) {
      return std::make_unique<InputsOutputs>(max_batch_size_, wdl_,
                                             moves_left_);
    } else {
      std::unique_ptr<InputsOutputs> resource =
          std::move(free_inputs_outputs_.front());
      free_inputs_outputs_.pop_front();
      return resource;
    }
  }

  void ReleaseInputsOutputs(std::unique_ptr<InputsOutputs> resource) {
    std::lock_guard<std::mutex> lock(inputs_outputs_lock_);
    free_inputs_outputs_.push_back(std::move(resource));
  }

 private:
  const NetworkCapabilities capabilities_;
  dnnl::engine cpu_eng_;
  dnnl::engine eng_;
  dnnl::stream eng_stream_;
  int max_batch_size_;
  int batch_size_;
  int steps_;
  bool wdl_;
  bool moves_left_;

  std::mutex lock_;

  int numBlocks_;
  int numFilters_;
  int pol_channels_;
  int value_channels_;
  int value_input_planes_;
  int moves_channels_;
  int moves_input_planes_;

  bool has_se_;
  bool conv_policy_;
  bool attn_policy_;
  ActivationFunction default_activation_;

  std::vector<std::vector<std::unique_ptr<BaseLayer>>> layers_;
  BaseLayer* getLastLayer(int idx) { return layers_[idx].back().get(); }

  std::mutex inputs_outputs_lock_;
  std::list<std::unique_ptr<InputsOutputs>> free_inputs_outputs_;
};

OnednnNetworkComputation::OnednnNetworkComputation(OnednnNetwork* network,
                                                   bool wdl, bool moves_left)
    : wdl_(wdl), moves_left_(moves_left), network_(network) {
  batch_size_ = 0;
  inputs_outputs_ = network_->GetInputsOutputs();
}

OnednnNetworkComputation::~OnednnNetworkComputation() {
  network_->ReleaseInputsOutputs(std::move(inputs_outputs_));
}

void OnednnNetworkComputation::ComputeBlocking() {
  network_->forwardEval(inputs_outputs_.get(), GetBatchSize());
}

std::unique_ptr<Network> MakeOnednnNetwork(const std::optional<WeightsFile>& w,
                                           const OptionsDict& options) {
  if (!w) {
    throw Exception("The oneDNN backend requires a network file.");
  }
  const WeightsFile& weights = *w;
  if (weights.format().network_format().network() !=
          pblczero::NetworkFormat::NETWORK_CLASSICAL_WITH_HEADFORMAT &&
      weights.format().network_format().network() !=
          pblczero::NetworkFormat::NETWORK_SE_WITH_HEADFORMAT) {
    throw Exception("Network format " +
                    pblczero::NetworkFormat::NetworkStructure_Name(
                        weights.format().network_format().network()) +
                    " is not supported by the oneDNN backend.");
  }
  if (weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_CLASSICAL &&
      weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_CONVOLUTION &&
      weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_ATTENTION) {
    throw Exception("Policy format " +
                    pblczero::NetworkFormat::PolicyFormat_Name(
                        weights.format().network_format().policy()) +
                    " is not supported by the oneDNN backend.");
  }
  if (weights.format().network_format().value() !=
          pblczero::NetworkFormat::VALUE_CLASSICAL &&
      weights.format().network_format().value() !=
          pblczero::NetworkFormat::VALUE_WDL) {
    throw Exception("Value format " +
                    pblczero::NetworkFormat::ValueFormat_Name(
                        weights.format().network_format().value()) +
                    " is not supported by the oneDNN backend.");
  }
  if (weights.format().network_format().moves_left() !=
          pblczero::NetworkFormat::MOVES_LEFT_NONE &&
      weights.format().network_format().moves_left() !=
          pblczero::NetworkFormat::MOVES_LEFT_V1) {
    throw Exception("Moves left head format " +
                    pblczero::NetworkFormat::MovesLeftFormat_Name(
                        weights.format().network_format().moves_left()) +
                    " is not supported by the oneDNN backend.");
  }
  if (weights.format().network_format().default_activation() !=
          pblczero::NetworkFormat::DEFAULT_ACTIVATION_RELU &&
      weights.format().network_format().default_activation() !=
          pblczero::NetworkFormat::DEFAULT_ACTIVATION_MISH) {
    throw Exception(
        "Default activation " +
        pblczero::NetworkFormat::DefaultActivation_Name(
            weights.format().network_format().default_activation()) +
        " is not supported by the oneDNN backend.");
  }
  return std::make_unique<OnednnNetwork>(weights, options);
}

REGISTER_NETWORK("onednn", MakeOnednnNetwork, 110)

}  // namespace lczero

```

`src/neural/onnx/adapters.cc`:

```cc
/*
This file is part of Leela Chess Zero.
Copyright (C) 2021 The LCZero Authors

Leela Chess is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Leela Chess is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

Additional permission under GNU GPL version 3 section 7

If you modify this Program, or any covered work, by linking or
combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
modified version of those libraries), containing parts covered by the
terms of the respective license agreement, the licensors of this
Program grant you additional permission to convey the resulting work.
*/

#include "neural/onnx/adapters.h"

#include "utils/transpose.h"

namespace lczero {

FloatOnnxWeightsAdapter::FloatOnnxWeightsAdapter(
    const std::vector<float>& weights, std::initializer_list<int> dims,
    std::initializer_list<int> order)
    : weights_(weights), dims_(dims), order_(order) {}

pblczero::TensorProto::DataType FloatOnnxWeightsAdapter::GetDataType() const {
  return pblczero::TensorProto::FLOAT;
}

std::vector<int> FloatOnnxWeightsAdapter::GetDimensions() const {
  // TODO factor out to a separate class as soon as there will be something else
  // than FloatOnnxWeightsAdapter.
  return dims_;
}
std::string FloatOnnxWeightsAdapter::GetRawData() const {
  if (order_.empty()) {
    return {reinterpret_cast<const char*>(weights_.data()),
            reinterpret_cast<const char*>(weights_.data() + weights_.size())};
  } else {
    std::vector<float> dst(weights_.size());
    TransposeTensor(dims_, order_, weights_, &dst[0]);
    return {reinterpret_cast<const char*>(dst.data()),
            reinterpret_cast<const char*>(dst.data() + dst.size())};
  }
}

}  // namespace lczero
```

`src/neural/onnx/adapters.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <initializer_list>

#include "neural/onnx/builder.h"
#include "neural/onnx/onnx.pb.h"
#include "proto/net.pb.h"
#include "utils/weights_adapter.h"

namespace lczero {

// FloatOnnxWeightsAdapter takes weights as a vector of floats and converts them
// as FLOAT initializer for OnnxBuilder. Optionally can transpose the tensor.
class FloatOnnxWeightsAdapter : public OnnxConst {
 public:
  FloatOnnxWeightsAdapter(const std::vector<float>& weights,
                          std::initializer_list<int> dims,
                          std::initializer_list<int> order = {});

 private:
  pblczero::TensorProto::DataType GetDataType() const override;
  std::vector<int> GetDimensions() const override;
  std::string GetRawData() const override;

  const std::vector<float>& weights_;
  std::vector<int> dims_;
  std::vector<int> order_;
};

// GenericOnnxConst takes inline constant (usually short and known at compile
// time) and converts to initializer for OnnxBuilder.
template <typename T>
class GenericOnnxConst : public OnnxConst {
 public:
  GenericOnnxConst(const std::vector<T> data, std::initializer_list<int> dims)
      : data_(data), dims_(dims) {}

 private:
  std::vector<int> GetDimensions() const override { return dims_; }
  std::string GetRawData() const override {
    return {reinterpret_cast<const char*>(data_.data()),
            reinterpret_cast<const char*>(data_.data() + data_.size())};
  }

  std::vector<T> data_;
  std::vector<int> dims_;
};

// GenericOnnxConst for int32 values.
class Int32OnnxConst : public GenericOnnxConst<int32_t> {
 public:
  using GenericOnnxConst<int32_t>::GenericOnnxConst;

 private:
  pblczero::TensorProto::DataType GetDataType() const override {
    return pblczero::TensorProto::INT32;
  }
};

// GenericOnnxConst for int64 values.
class Int64OnnxConst : public GenericOnnxConst<int64_t> {
 public:
  using GenericOnnxConst<int64_t>::GenericOnnxConst;

 private:
  pblczero::TensorProto::DataType GetDataType() const override {
    return pblczero::TensorProto::INT64;
  }
};

}  // namespace lczero
```

`src/neural/onnx/builder.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "neural/onnx/builder.h"

#include <initializer_list>

#include "neural/onnx/onnx.pb.h"
#include "utils/random.h"
#include "version.h"

namespace lczero {

OnnxBuilder::OnnxBuilder() {
  model_.set_ir_version(4);
  model_.set_domain("org.lczero.models.*");
  model_.set_producer_name("Lc0");
  model_.set_producer_version(GetVersionStr());
  model_.add_opset_import()->set_version(9);
  model_.mutable_graph()->set_name("org.lczero/converted/" +
                                   Random::Get().GetString(16));
}

namespace {
void FillValueInfo(pblczero::ValueInfoProto* vip, const std::string& name,
                   std::initializer_list<int> dims,
                   pblczero::TensorProto::DataType datatype) {
  vip->set_name(name);
  auto* type = vip->mutable_type()->mutable_tensor_type();
  type->set_elem_type(datatype);
  auto* shape = type->mutable_shape();
  for (const auto d : dims) {
    auto* dim = shape->add_dim();
    if (d < 0) {
      dim->set_dim_param("batch");
    } else {
      dim->set_dim_value(d);
    }
  }
}

void AddIntAttribute(pblczero::NodeProto* node, const std::string& name,
                     int val) {
  auto* attr = node->add_attribute();
  attr->set_name(name);
  attr->set_type(pblczero::AttributeProto::INT);
  attr->set_i(val);
}

void AddIntsAttribute(pblczero::NodeProto* node, const std::string& name,
                      std::initializer_list<int> vals) {
  auto* attr = node->add_attribute();
  attr->set_name(name);
  attr->set_type(pblczero::AttributeProto::INTS);
  for (const int x : vals) attr->add_ints(x);
}

}  // namespace

void OnnxBuilder::AddInput(const std::string& name,
                           std::initializer_list<int> dims,
                           pblczero::TensorProto::DataType datatype) {
  FillValueInfo(model_.mutable_graph()->add_input(), name, dims, datatype);
}

void OnnxBuilder::AddOutput(const std::string& name,
                            std::initializer_list<int> dims,
                            pblczero::TensorProto::DataType datatype) {
  FillValueInfo(model_.mutable_graph()->add_output(), name, dims, datatype);
}

std::string OnnxBuilder::AddInitializer(const std::string& name,
                                        const OnnxConst& weights) {
  auto* init = model_.mutable_graph()->add_initializer();
  init->set_name(name);
  init->set_data_type(weights.GetDataType());
  for (const int dim : weights.GetDimensions()) init->add_dims(dim);
  init->set_raw_data(weights.GetRawData());

  return name;
}

namespace {

std::string PopulateStdNodeFields(pblczero::NodeProto* node,
                                  const std::string& name,
                                  const std::string& input,
                                  const std::string& type) {
  node->set_name(name);
  node->set_op_type(type);
  node->add_input(input);
  node->add_output(name);
  return name;
}

}  // namespace

std::string OnnxBuilder::Conv(const std::string& name,
                              const std::string& input_name,
                              const OnnxConst& kernel_weights,
                              const OnnxConst& bias_weights, int pads) {
  auto* node = model_.mutable_graph()->add_node();
  auto out = PopulateStdNodeFields(node, name, input_name, "Conv");
  node->add_input(AddInitializer(name + "/w/kernel", kernel_weights));
  node->add_input(AddInitializer(name + "/w/bias", bias_weights));
  AddIntsAttribute(node, "pads", {pads, pads, pads, pads});
  return out;
}

std::string OnnxBuilder::Add(const std::string& name, const std::string& input1,
                             const std::string& input2) {
  auto* node = model_.mutable_graph()->add_node();
  auto out = PopulateStdNodeFields(node, name, input1, "Add");
  node->add_input(input2);
  return out;
}

std::string OnnxBuilder::Add(const std::string& name, const std::string& input1,
                             const OnnxConst& input2) {
  auto* node = model_.mutable_graph()->add_node();
  auto out = PopulateStdNodeFields(node, name, input1, "Add");
  node->add_input(AddInitializer(name + "/w", input2));
  return out;
}

std::string OnnxBuilder::GlobalAveragePool(const std::string& name,
                                           const std::string& input) {
  auto* node = model_.mutable_graph()->add_node();
  return PopulateStdNodeFields(node, name, input, "GlobalAveragePool");
}

std::string OnnxBuilder::Squeeze(const std::string& name,
                                 const std::string& input) {
  auto* node = model_.mutable_graph()->add_node();
  auto out = PopulateStdNodeFields(node, name, input, "Squeeze");
  AddIntsAttribute(node, "axes", {2, 3});
  return out;
}

std::string OnnxBuilder::Mul(const std::string& name, const std::string& input1,
                             const std::string& input2) {
  auto* node = model_.mutable_graph()->add_node();
  auto out = PopulateStdNodeFields(node, name, input1, "Mul");
  node->add_input(input2);
  return out;
}

std::string OnnxBuilder::MatMul(const std::string& name,
                                const std::string& input1,
                                const OnnxConst& input2) {
  auto* node = model_.mutable_graph()->add_node();
  auto out = PopulateStdNodeFields(node, name, input1, "MatMul");
  node->add_input(AddInitializer(name + "/w", input2));
  return out;
}

std::string OnnxBuilder::Relu(const std::string& name,
                              const std::string& input) {
  auto* node = model_.mutable_graph()->add_node();
  return PopulateStdNodeFields(node, name, input, "Relu");
}

std::string OnnxBuilder::Tanh(const std::string& name,
                              const std::string& input) {
  auto* node = model_.mutable_graph()->add_node();
  return PopulateStdNodeFields(node, name, input, "Tanh");
}

std::string OnnxBuilder::Softmax(const std::string& name,
                                 const std::string& input) {
  auto* node = model_.mutable_graph()->add_node();
  return PopulateStdNodeFields(node, name, input, "Softmax");
}

std::string OnnxBuilder::Reshape(const std::string& name,
                                 const std::string& input,
                                 const std::string& shape) {
  auto* node = model_.mutable_graph()->add_node();
  auto out = PopulateStdNodeFields(node, name, input, "Reshape");
  node->add_input(shape);
  return out;
}

std::string OnnxBuilder::Gather(const std::string& name,
                                const std::string& input1,
                                const std::string& input2, int axis) {
  auto* node = model_.mutable_graph()->add_node();
  auto out = PopulateStdNodeFields(node, name, input1, "Gather");
  node->add_input(input2);
  AddIntAttribute(node, "axis", axis);
  return out;
}

std::pair<std::string, std::string> OnnxBuilder::Split(const std::string& name,
                                                       const std::string& input,
                                                       int axis) {
  auto* node = model_.mutable_graph()->add_node();
  node->set_name(name);
  node->set_op_type("Split");
  node->add_input(input);
  node->add_output(name + "/out1");
  node->add_output(name + "/out2");
  AddIntAttribute(node, "axis", axis);
  return {name + "/out1", name + "/out2"};
}

std::string OnnxBuilder::Sigmoid(const std::string& name,
                                 const std::string& input) {
  auto* node = model_.mutable_graph()->add_node();
  return PopulateStdNodeFields(node, name, input, "Sigmoid");
}

}  // namespace lczero
```

`src/neural/onnx/builder.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <initializer_list>
#include <string>

#include "neural/onnx/onnx.pb.h"

namespace lczero {

class OnnxConst {
 public:
  virtual ~OnnxConst() = default;
  virtual pblczero::TensorProto::DataType GetDataType() const = 0;
  virtual std::vector<int> GetDimensions() const = 0;
  virtual std::string GetRawData() const = 0;
};

// Builds Onnx::ModelProto.
class OnnxBuilder {
 public:
  OnnxBuilder();
  void AddInput(const std::string& name, std::initializer_list<int> dims,
                pblczero::TensorProto::DataType datatype);
  void AddOutput(const std::string& name, std::initializer_list<int> dims,
                 pblczero::TensorProto::DataType datatype);

  // Functions to add operators.
  // Every function appends one node to the graph.
  // @name parameter is used to name both the added node, and the output edge.
  // @input{,1,2} input tensor.
  //     - if std::string, contains the name of input tensor.
  //     - if OnnxConst, adds it as initializer, and then uses.
  // Return the name of the output edge, which is in most cases the same as the
  // node name.
  std::string Conv(const std::string& name, const std::string& input_name,
                   const OnnxConst& kernel_weights,
                   const OnnxConst& bias_weights, int pads = 1);
  std::string Add(const std::string& name, const std::string& input1,
                  const std::string& input2);
  std::string Add(const std::string& name, const std::string& input1,
                  const OnnxConst&);
  std::string GlobalAveragePool(const std::string& name,
                                const std::string& input);
  std::string Squeeze(const std::string& name, const std::string& input);
  std::string MatMul(const std::string& name, const std::string& input1,
                     const OnnxConst& input2);
  std::string Mul(const std::string& name, const std::string& input1,
                  const std::string& input2);
  std::string Relu(const std::string& name, const std::string& input);
  std::string Tanh(const std::string& name, const std::string& input);
  std::string Softmax(const std::string& name, const std::string& input);
  std::string AddInitializer(const std::string& name, const OnnxConst& weights);
  std::string Reshape(const std::string& name, const std::string& input,
                      const std::string& shape);
  std::pair<std::string, std::string> Split(const std::string& name,
                                            const std::string& input, int axis);
  std::string Sigmoid(const std::string& name, const std::string& input);
  std::string Gather(const std::string& name, const std::string& input1,
                     const std::string& input2, int axis);

  // Returns ONNX model as protobuf.
  const pblczero::ModelProto& as_proto() const { return model_; }
  // Returns serialized model.
  std::string OutputAsString() const { return model_.OutputAsString(); }

 private:
  pblczero::ModelProto model_;
};

}  // namespace lczero
```

`src/neural/onnx/converter.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "neural/onnx/converter.h"

#include <cstddef>
#include <initializer_list>
#include <memory>

#include "neural/loader.h"
#include "neural/network.h"
#include "neural/network_legacy.h"
#include "neural/onnx/adapters.h"
#include "neural/onnx/builder.h"
#include "neural/shared/policy_map.h"
#include "proto/net.pb.h"
#include "utils/exception.h"

namespace lczero {
namespace {

class Converter {
 public:
  Converter(const pblczero::Net& net,
            const WeightsToOnnxConverterOptions& options)
      : src_(net), options_(options) {}

  void Convert(pblczero::Net* dst);

 private:
  int NumFilters() const {
    return LayerAdapter(src_.weights().input().weights()).size() /
           kInputPlanes / 9;
  }
  size_t NumBlocks() const { return src_.weights().residual_size(); }
  void CopyGenericFields(pblczero::Net* dst);
  void GenerateOnnx(pblczero::OnnxModel* onnx);
  void FillValueInfo(pblczero::ValueInfoProto* vip, const std::string& name,
                     std::initializer_list<int> dims);

  std::string MakeConvBlock(OnnxBuilder* builder,
                            const LegacyWeights::ConvBlock&, int input_channels,
                            int output_channels, const std::string& input,
                            const std::string& name,
                            const LegacyWeights::SEunit* se_unit = nullptr,
                            const std::string& mixin = "", bool relu = true,
                            int filters = 3);

  std::string MakeResidualBlock(OnnxBuilder* builder,
                                const LegacyWeights::Residual&,
                                const std::string& input,
                                const std::string& name);

  std::string MakeSqueezeAndExcite(OnnxBuilder* builder,
                                   const LegacyWeights::SEunit& se_unit,
                                   const std::string& input,
                                   const std::string& name);

  void MakePolicyHead(pblczero::OnnxModel* onnx, OnnxBuilder* builder,
                      const std::string& input, const LegacyWeights& weights);

  void MakeValueHead(pblczero::OnnxModel* onnx, OnnxBuilder* builder,
                     const std::string& input, const LegacyWeights& weights);

  void MakeMovesLeftHead(pblczero::OnnxModel* onnx, OnnxBuilder* builder,
                         const std::string& input,
                         const LegacyWeights& weights);

  void AddStdInitializers(OnnxBuilder* builder);

  pblczero::TensorProto::DataType GetDataType() const;
  std::unique_ptr<OnnxConst> GetWeghtsConverter(
      const std::vector<float>&, std::initializer_list<int> dims,
      std::initializer_list<int> order = {});

  const pblczero::Net& src_;
  const WeightsToOnnxConverterOptions& options_;
};

pblczero::TensorProto::DataType Converter::GetDataType() const {
  switch (options_.data_type_) {
    case WeightsToOnnxConverterOptions::DataType::kFloat32:
      return pblczero::TensorProto::FLOAT;
    default:
      return pblczero::TensorProto::UNDEFINED;
  }
}

std::unique_ptr<OnnxConst> Converter::GetWeghtsConverter(
    const std::vector<float>& weights, std::initializer_list<int> dims,
    std::initializer_list<int> order) {
  switch (options_.data_type_) {
    case WeightsToOnnxConverterOptions::DataType::kFloat32:
      return std::make_unique<FloatOnnxWeightsAdapter>(weights, dims, order);
  }
  throw Exception("Data type " +
                  std::to_string(static_cast<int>(options_.data_type_)) +
                  " is not supported in weights converter");
}

std::string Converter::MakeSqueezeAndExcite(
    OnnxBuilder* builder, const LegacyWeights::SEunit& se_unit,
    const std::string& input, const std::string& name) {
  const int se_filters = se_unit.b1.size();

  auto flow = builder->GlobalAveragePool(name + "/pooled", input);
  flow = builder->Squeeze(name + "/squeeze", flow);
  flow = builder->MatMul(
      name + "/matmul1", flow,
      *GetWeghtsConverter(se_unit.w1, {NumFilters(), se_filters}, {1, 0}));
  flow = builder->Add(name + "/add1", flow,
                      *GetWeghtsConverter(se_unit.b1, {se_filters}));
  flow = builder->Relu(name + "/relu", flow);
  flow = builder->MatMul(
      name + "/matmul2", flow,
      *GetWeghtsConverter(se_unit.w2, {se_filters, 2 * NumFilters()}, {1, 0}));
  flow = builder->Add(name + "/add2", flow,
                      *GetWeghtsConverter(se_unit.b2, {2 * NumFilters()}));
  flow = builder->Reshape(name + "/reshape", flow, "/const/se_reshape");

  auto splits = builder->Split(name + "/split", flow, 1);

  flow = builder->Sigmoid(name + "/sigmoid", splits.first);
  flow = builder->Mul(name + "/matmul3", flow, input);
  return builder->Add(name + "/add3", flow, splits.second);
}

std::string Converter::MakeConvBlock(
    OnnxBuilder* builder, const LegacyWeights::ConvBlock& weights,
    int input_channels, int output_channels, const std::string& input,
    const std::string& name, const LegacyWeights::SEunit* seunit,
    const std::string& mixin, bool relu, int filters) {
  auto flow = builder->Conv(
      name, input,
      *GetWeghtsConverter(weights.weights,
                          {output_channels, input_channels, filters, filters}),
      *GetWeghtsConverter(weights.biases, {output_channels}),
      (filters - 1) / 2);

  if (seunit) flow = MakeSqueezeAndExcite(builder, *seunit, flow, name + "/se");
  if (!mixin.empty()) flow = builder->Add(name + "/mixin", flow, mixin);
  if (relu) flow = builder->Relu(name + "/relu", flow);
  return flow;
}

std::string Converter::MakeResidualBlock(OnnxBuilder* builder,
                                         const LegacyWeights::Residual& res,
                                         const std::string& input,
                                         const std::string& name) {
  auto block1 = MakeConvBlock(builder, res.conv1, NumFilters(), NumFilters(),
                              input, name + "/conv1");
  return MakeConvBlock(builder, res.conv2, NumFilters(), NumFilters(), block1,
                       name + "/conv2", res.has_se ? &res.se : nullptr, input);
}

void Converter::AddStdInitializers(OnnxBuilder* builder) {
  builder->AddInitializer("/const/se_reshape",
                          Int64OnnxConst({-1, NumFilters() * 2, 1, 1}, {4}));
}

namespace {
std::vector<int> MakePolicyMap() {
  std::vector<int> policy_map(1858);
  int idx = 0;
  for (const auto& mapping : kConvPolicyMap) {
    if (mapping > -1) policy_map[mapping] = idx;
    idx++;
  }
  return policy_map;
}
}  // namespace

void Converter::MakePolicyHead(pblczero::OnnxModel* onnx, OnnxBuilder* builder,
                               const std::string& input,
                               const LegacyWeights& weights) {
  if (!weights.policy1.weights.empty()) {
    // Conv policy head.
    auto flow = MakeConvBlock(builder, weights.policy1, NumFilters(),
                              NumFilters(), input, "/policy/conv1");
    flow = MakeConvBlock(builder, weights.policy, NumFilters(), 80, flow,
                         "/policy/conv2", nullptr, "", false);
    flow = builder->Reshape(
        "/policy/flatten", flow,
        builder->AddInitializer("/const/policy_shape",
                                Int64OnnxConst({-1, 80 * 8 * 8}, {2})));
    auto output = builder->Gather(
        options_.output_policy_head, flow,
        builder->AddInitializer("/const/mapping_table",
                                Int32OnnxConst(MakePolicyMap(), {1858})),
        1);
    builder->AddOutput(output, {-1, 1858}, GetDataType());
    onnx->set_output_policy(output);
  } else {
    // Dense policy head.
    const int pol_channels = weights.policy.biases.size();
    auto flow =
        MakeConvBlock(builder, weights.policy, NumFilters(), pol_channels,
                      input, "/policy/conv", nullptr, "", true, 1);
    flow =
        builder->Reshape("/policy/reshape", flow,
                         builder->AddInitializer(
                             "/const/policy_shape",
                             Int64OnnxConst({-1, pol_channels * 8 * 8}, {2})));
    flow = builder->MatMul(
        "/policy/dense/matmul", flow,
        *GetWeghtsConverter(weights.ip_pol_w, {pol_channels * 8 * 8, 1858},
                            {1, 0}));
    auto output = builder->Add(options_.output_policy_head, flow,
                               *GetWeghtsConverter(weights.ip_pol_b, {1858}));
    builder->AddOutput(output, {-1, 1858}, GetDataType());
    onnx->set_output_policy(output);
  }
}

void Converter::MakeValueHead(pblczero::OnnxModel* onnx, OnnxBuilder* builder,
                              const std::string& input,
                              const LegacyWeights& weights) {
  auto flow = MakeConvBlock(builder, weights.value, NumFilters(), 32, input,
                            "/value/conv", nullptr, "", true, 1);
  flow = builder->Reshape(
      "/value/reshape", flow,
      builder->AddInitializer("/const/value_shape",
                              Int64OnnxConst({-1, 32 * 8 * 8}, {2})));
  flow = builder->MatMul(
      "/value/dense1/matmul", flow,
      *GetWeghtsConverter(weights.ip1_val_w, {32 * 8 * 8, 128}, {1, 0}));
  flow = builder->Add("/value/dense1/add", flow,
                      *GetWeghtsConverter(weights.ip1_val_b, {128}));
  flow = builder->Relu("/value/dense1/relu", flow);

  const bool wdl = src_.format().network_format().value() ==
                   pblczero::NetworkFormat::VALUE_WDL;
  if (wdl) {
    flow = builder->MatMul(
        "/value/dense2/matmul", flow,
        *GetWeghtsConverter(weights.ip2_val_w, {128, 3}, {1, 0}));
    flow = builder->Add("/value/dense2/add", flow,
                        *GetWeghtsConverter(weights.ip2_val_b, {3}));
    auto output = builder->Softmax(options_.output_wdl, flow);
    builder->AddOutput(output, {-1, 3}, GetDataType());
    onnx->set_output_wdl(output);
  } else {
    flow = builder->MatMul(
        "/value/dense2/matmul", flow,
        *GetWeghtsConverter(weights.ip2_val_w, {128, 1}, {1, 0}));
    flow = builder->Add("/value/dense2/add", flow,
                        *GetWeghtsConverter(weights.ip2_val_b, {1}));
    auto output = builder->Tanh(options_.output_value, flow);
    builder->AddOutput(output, {-1, 1}, GetDataType());
    onnx->set_output_value(output);
  }
}

void Converter::MakeMovesLeftHead(pblczero::OnnxModel* onnx,
                                  OnnxBuilder* builder,
                                  const std::string& input,
                                  const LegacyWeights& weights) {
  if (src_.format().network_format().moves_left() !=
      pblczero::NetworkFormat::MOVES_LEFT_V1) {
    return;
  }
  const int mlh_channels = weights.moves_left.biases.size();
  const int mlh_fc1_outputs = weights.ip1_mov_b.size();
  auto flow =
      MakeConvBlock(builder, weights.moves_left, NumFilters(), mlh_channels,
                    input, "/mlh/conv", nullptr, "", true, 1);
  flow = builder->Reshape(
      "/mlh/reshape", flow,
      builder->AddInitializer("/const/mlh_shape",
                              Int64OnnxConst({-1, mlh_channels * 8 * 8}, {2})));
  flow = builder->MatMul(
      "/mlh/dense1/matmul", flow,
      *GetWeghtsConverter(weights.ip1_mov_w,
                          {mlh_channels * 8 * 8, mlh_fc1_outputs}, {1, 0}));
  flow =
      builder->Add("/mlh/dense1/add", flow,
                   *GetWeghtsConverter(weights.ip1_mov_b, {mlh_fc1_outputs}));
  flow = builder->Relu("/mlh/dense1/relu", flow);
  flow = builder->MatMul(
      "/mlh/dense2/matmul", flow,
      *GetWeghtsConverter(weights.ip2_mov_w, {mlh_fc1_outputs, 1}, {1, 0}));
  flow = builder->Add("/mlh/dense2/add", flow,
                      *GetWeghtsConverter(weights.ip2_mov_b, {1}));
  auto output = builder->Relu(options_.output_mlh, flow);
  builder->AddOutput(output, {-1, 1}, GetDataType());
  onnx->set_output_mlh(output);
}

void Converter::GenerateOnnx(pblczero::OnnxModel* onnx) {
  LegacyWeights weights(src_.weights());
  OnnxBuilder builder;

  AddStdInitializers(&builder);

  onnx->set_input_planes(options_.input_planes_name);
  builder.AddInput(options_.input_planes_name, {-1, 112, 8, 8}, GetDataType());

  // Input convolution.
  auto flow = MakeConvBlock(&builder, weights.input, kInputPlanes, NumFilters(),
                            options_.input_planes_name, "/inputconv");

  // Residual tower.
  for (size_t i = 0; i < NumBlocks(); ++i) {
    flow = MakeResidualBlock(&builder, weights.residual[i], flow,
                             "/block" + std::to_string(i));
  }

  // Policy head.
  MakePolicyHead(onnx, &builder, flow, weights);
  // Value head.
  MakeValueHead(onnx, &builder, flow, weights);
  // Moves left head.
  MakeMovesLeftHead(onnx, &builder, flow, weights);

  onnx->set_model(builder.OutputAsString());
}

void Converter::CopyGenericFields(pblczero::Net* dst) {
  dst->set_license(src_.license());
  dst->set_magic(src_.magic());
  auto* min_version = dst->mutable_min_version();
  min_version->set_minor(28);
  auto* network_format = dst->mutable_format()->mutable_network_format();
  network_format->set_input(src_.format().network_format().input());
  network_format->set_output(src_.format().network_format().output());
  network_format->set_network(pblczero::NetworkFormat::NETWORK_ONNX);
  // We add convolution-to-classical layer to ONNX layers anyway, so from
  // outside they are all POLICY_CLASSICAL.
  network_format->set_policy(pblczero::NetworkFormat::POLICY_CLASSICAL);
  network_format->set_value(src_.format().network_format().value());
  network_format->set_moves_left(src_.format().network_format().moves_left());

  *dst->mutable_training_params() = src_.training_params();
}

void Converter::Convert(pblczero::Net* dst) {
  if (src_.has_onnx_model() && src_.format().network_format().network() ==
                                   pblczero::NetworkFormat::NETWORK_ONNX) {
    *dst = src_;
    return;
  }
  if (!src_.has_weights()) {
    throw Exception("The network doesn't have weights.");
  }
  if (src_.has_onnx_model()) {
    throw Exception("The network already has ONNX section.");
  }
  CopyGenericFields(dst);
  GenerateOnnx(dst->mutable_onnx_model());
}

}  // namespace

pblczero::Net ConvertWeightsToOnnx(
    const pblczero::Net& net, const WeightsToOnnxConverterOptions& options) {
  Converter converter(net, options);
  pblczero::Net dst;
  converter.Convert(&dst);
  return dst;
}

}  // namespace lczero

```

`src/neural/onnx/converter.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include "neural/onnx/onnx.pb.h"
#include "proto/net.pb.h"

namespace lczero {

// Options to use when converting "old" weights to ONNX weights format.
struct WeightsToOnnxConverterOptions {
  enum class DataType { kFloat32 };
  DataType data_type_ = DataType::kFloat32;
  std::string input_planes_name = "/input/planes";
  std::string output_policy_head = "/output/policy";
  std::string output_wdl = "/output/wdl";
  std::string output_value = "/output/value";
  std::string output_mlh = "/output/mlh";
};

// Converts "classical" weights file to weights file with embedded ONNX model.
pblczero::Net ConvertWeightsToOnnx(const pblczero::Net&,
                                   const WeightsToOnnxConverterOptions&);

}  // namespace lczero
```

`src/neural/onnx/network_onnx.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <algorithm>
#include <cassert>
#include <fstream>
#include <iterator>
#include <memory>
#include <string>
#include <vector>

#include "cpu_provider_factory.h"
#include "neural/factory.h"
#include "neural/loader.h"
#include "neural/network.h"
#include "neural/onnx/converter.h"
#include "onnxruntime_cxx_api.h"
#include "utils/bititer.h"
#include "utils/exception.h"
#include "utils/logging.h"

namespace lczero {
namespace {

enum class OnnxProvider { CPU, CUDA };

class OnnxNetwork;

class OnnxComputation : public NetworkComputation {
 public:
  OnnxComputation(OnnxNetwork* network) : network_(network) {}
  void AddInput(InputPlanes&& input) override {
    raw_input_.emplace_back(input);
  }
  int GetBatchSize() const override { return raw_input_.size(); }
  void ComputeBlocking() override;
  float GetQVal(int sample) const override;
  float GetDVal(int sample) const override;
  float GetPVal(int sample, int move_id) const override;
  float GetMVal(int sample) const override;

 private:
  Ort::Value PrepareInput();

  OnnxNetwork* network_;
  std::vector<InputPlanes> raw_input_;
  std::vector<float> input_tensor_data_;
  std::vector<Ort::Value> output_tensors_;
};

class OnnxNetwork : public Network {
 public:
  OnnxNetwork(const WeightsFile& file, const OptionsDict& options,
              OnnxProvider provider);
  std::unique_ptr<NetworkComputation> NewComputation() override {
    return std::make_unique<OnnxComputation>(this);
  }
  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

  Ort::Env onnx_env_;
  Ort::SessionOptions session_options_;
  Ort::Session session_;
  std::vector<std::string> inputs_;
  // Points to strings in inputs_.
  std::vector<const char*> inputs_cstr_;
  std::vector<std::string> outputs_;
  // Points to strings in outputs_.
  std::vector<const char*> outputs_cstr_;
  // Indices in output_cstr_ vector.
  int policy_head_ = -1;
  int wdl_head_ = -1;
  int value_head_ = -1;
  int mlh_head_ = -1;
  NetworkCapabilities capabilities_;
};

float OnnxComputation::GetQVal(int sample) const {
  if (network_->wdl_head_ != -1) {
    const auto& data =
        output_tensors_[network_->wdl_head_].GetTensorData<float>();
    return data[sample * 3 + 0] - data[sample * 3 + 2];
  } else {
    const auto& data =
        output_tensors_[network_->value_head_].GetTensorData<float>();
    return data[sample];
  }
}
float OnnxComputation::GetDVal(int sample) const {
  if (network_->wdl_head_ == -1) return 0.0f;
  const auto& data =
      output_tensors_[network_->wdl_head_].GetTensorData<float>();
  return data[sample * 3 + 1];
}
float OnnxComputation::GetPVal(int sample, int move_id) const {
  const auto& data =
      output_tensors_[network_->policy_head_].GetTensorData<float>();
  return data[sample * 1858 + move_id];
}
float OnnxComputation::GetMVal(int sample) const {
  if (network_->mlh_head_ == -1) return 0.0f;
  const auto& data =
      output_tensors_[network_->mlh_head_].GetTensorData<float>();
  return data[sample];
}

Ort::Value OnnxComputation::PrepareInput() {
  input_tensor_data_.clear();
  input_tensor_data_.resize(raw_input_.size() * kInputPlanes * 8 * 8);
  auto iter = input_tensor_data_.data();
  for (const auto& sample : raw_input_) {
    assert(sample.size() == kInputPlanes);
    for (const auto& plane : sample) {
      for (auto bit : IterateBits(plane.mask)) {
        *(iter + bit) = plane.value;
      }
      iter += 64;
    }
  }
  int64_t dims[] = {static_cast<int64_t>(raw_input_.size()), kInputPlanes, 8,
                    8};
  auto memory_info =
      Ort::MemoryInfo::CreateCpu(OrtArenaAllocator, OrtMemTypeDefault);
  // Hopefully having dims in a temporary variable is fine.
  return Ort::Value::CreateTensor<float>(memory_info, input_tensor_data_.data(),
                                         input_tensor_data_.size(), dims, 4);
}

void OnnxComputation::ComputeBlocking() {
  auto input_tensor = PrepareInput();
  output_tensors_ = network_->session_.Run(
      {}, network_->inputs_cstr_.data(), &input_tensor, 1,
      network_->outputs_cstr_.data(), network_->outputs_cstr_.size());
}

Ort::SessionOptions GetOptions(OnnxProvider provider, const OptionsDict& dict) {
  Ort::SessionOptions options;
  OrtCUDAProviderOptions cuda_options;
  // options.SetIntraOpNumThreads(1);
  options.SetGraphOptimizationLevel(GraphOptimizationLevel::ORT_ENABLE_ALL);
  switch (provider) {
    case OnnxProvider::CUDA:
      cuda_options.device_id = dict.GetOrDefault<int>("gpu", 0);
      options.AppendExecutionProvider_CUDA(cuda_options);
      break;
    case OnnxProvider::CPU:
      // Doesn't really work. :-( There are two execution providers (CUDA and
      // CPU) already added, don't know how to force it to use CPU.
      auto status = OrtSessionOptionsAppendExecutionProvider_CPU(options, 0);
      if (status) {
        std::string error_message = Ort::GetApi().GetErrorMessage(status);
        OrtErrorCode error_code = Ort::GetApi().GetErrorCode(status);
        Ort::GetApi().ReleaseStatus(status);
        throw Exception("ONNX CPU error " + std::to_string(error_code) + ": " +
                        error_message);
      }
      break;
  }
  return options;
}

OnnxNetwork::OnnxNetwork(const WeightsFile& file, const OptionsDict& dict,
                         OnnxProvider provider)
    : onnx_env_(ORT_LOGGING_LEVEL_WARNING, "lc0"),
      session_(onnx_env_, file.onnx_model().model().data(),
               file.onnx_model().model().size(), GetOptions(provider, dict)),
      capabilities_{file.format().network_format().input(),
                    file.format().network_format().moves_left()} {
  const auto& md = file.onnx_model();
  if (!md.has_input_planes()) {
    throw Exception("NN doesn't have input planes defined.");
  }
  inputs_.emplace_back(md.input_planes());
  if (!md.has_output_policy()) {
    throw Exception("NN doesn't have policy head defined.");
  }
  policy_head_ = outputs_.size();
  outputs_.emplace_back(md.output_policy());
  if (md.has_output_wdl()) {
    wdl_head_ = outputs_.size();
    outputs_.emplace_back(md.output_wdl());
  } else if (md.has_output_value()) {
    value_head_ = outputs_.size();
    outputs_.emplace_back(md.output_value());
  } else {
    throw Exception("NN doesn't have value head.");
  }
  if (md.has_output_mlh()) {
    mlh_head_ = outputs_.size();
    outputs_.emplace_back(md.output_mlh());
  }
  std::transform(inputs_.begin(), inputs_.end(),
                 std::back_inserter(inputs_cstr_),
                 [](const auto& x) { return x.c_str(); });
  std::transform(outputs_.begin(), outputs_.end(),
                 std::back_inserter(outputs_cstr_),
                 [](const auto& x) { return x.c_str(); });
}

template <OnnxProvider kProvider>
std::unique_ptr<Network> MakeOnnxNetwork(const std::optional<WeightsFile>& w,
                                         const OptionsDict& opts) {
  if (!w) throw Exception("The ONNX backend requires a network file.");

  if (w->has_onnx_model()) {
    return std::make_unique<OnnxNetwork>(*w, opts, kProvider);
  } else {
    if (w->format().network_format().network() !=
            pblczero::NetworkFormat::NETWORK_CLASSICAL_WITH_HEADFORMAT &&
        w->format().network_format().network() !=
            pblczero::NetworkFormat::NETWORK_SE_WITH_HEADFORMAT) {
      throw Exception("Network format " +
                      pblczero::NetworkFormat::NetworkStructure_Name(
                          w->format().network_format().network()) +
                      " is not supported by the ONNX backend.");
    }
    if (w->format().network_format().policy() !=
            pblczero::NetworkFormat::POLICY_CLASSICAL &&
        w->format().network_format().policy() !=
            pblczero::NetworkFormat::POLICY_CONVOLUTION) {
      throw Exception("Policy format " +
                      pblczero::NetworkFormat::PolicyFormat_Name(
                          w->format().network_format().policy()) +
                      " is not supported by the ONNX backend.");
    }
    if (w->format().network_format().value() !=
            pblczero::NetworkFormat::VALUE_CLASSICAL &&
        w->format().network_format().value() !=
            pblczero::NetworkFormat::VALUE_WDL) {
      throw Exception("Value format " +
                      pblczero::NetworkFormat::ValueFormat_Name(
                          w->format().network_format().value()) +
                      " is not supported by the ONNX backend.");
    }
    if (w->format().network_format().default_activation() !=
        pblczero::NetworkFormat::DEFAULT_ACTIVATION_RELU) {
      throw Exception("Default activation " +
                      pblczero::NetworkFormat::DefaultActivation_Name(
                          w->format().network_format().default_activation()) +
                      " is not supported by the ONNX backend.");
    }
    auto converted = ConvertWeightsToOnnx(*w, {});
    return std::make_unique<OnnxNetwork>(converted, opts, kProvider);
  }
}

REGISTER_NETWORK("onnx-cuda", MakeOnnxNetwork<OnnxProvider::CUDA>, 61)
REGISTER_NETWORK("onnx-cpu", MakeOnnxNetwork<OnnxProvider::CPU>, 62)

}  // namespace
}  // namespace lczero

```

`src/neural/onnx/onnx.proto`:

```proto
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

syntax = "proto2";

package pblczero;

message TensorProto {
  enum DataType {
    UNDEFINED = 0;
    // Basic types.
    FLOAT = 1;   // float
    UINT8 = 2;   // uint8_t
    INT8 = 3;    // int8_t
    UINT16 = 4;  // uint16_t
    INT16 = 5;   // int16_t
    INT32 = 6;   // int32_t
    INT64 = 7;   // int64_t
    STRING = 8;  // string
    BOOL = 9;    // bool
    FLOAT16 = 10;
    DOUBLE = 11;
    UINT32 = 12;
    UINT64 = 13;
    COMPLEX64 = 14;
    COMPLEX128 = 15;
    BFLOAT16 = 16;
  }
  repeated int64 dims = 1;
  optional DataType data_type = 2;
  optional string name = 8;
  optional bytes raw_data = 9;
  optional string doc_string = 12;
}

message AttributeProto {
  enum AttributeType {
    UNDEFINED = 0;
    FLOAT = 1;
    INT = 2;
    STRING = 3;
    TENSOR = 4;
    // GRAPH = 5;

    FLOATS = 6;
    INTS = 7;
    STRINGS = 8;
    TENSORS = 9;
    // GRAPHS = 10;
  }  
  optional string name = 1;
  optional string doc_string = 13;

  // Exactly ONE of the following fields must be present
  optional float f = 2;               // float
  optional int64 i = 3;               // int
  optional bytes s = 4;               // UTF-8 string
  optional TensorProto t = 5;         // tensor value
  // optional GraphProto g = 6;          // graph

  repeated float floats = 7;          // list of floats
  repeated int64 ints = 8;            // list of ints
  repeated bytes strings = 9;         // list of UTF-8 strings
  repeated TensorProto tensors = 10;  // list of tensors
  // repeated GraphProto graphs = 11;    // list of graph

  optional AttributeType type = 20;
}

message NodeProto {
  repeated string input = 1;  
  repeated string output = 2; 
  optional string name = 3;   
  optional string op_type = 4;  
  repeated AttributeProto attribute = 5;
  optional string doc_string = 6;
  optional string domain = 7;
}

message TensorShapeProto {
  message Dimension {
    optional int64 dim_value = 1;
    optional string dim_param = 2;
  }
  repeated Dimension dim = 1;
}

message TypeProto {
  message Tensor {
    optional TensorProto.DataType elem_type = 1;
    optional TensorShapeProto shape = 2;
  }
  optional Tensor tensor_type = 1;
}

message ValueInfoProto {
  optional string name = 1;
  optional TypeProto type = 2;
  optional string doc_string = 3;
}

message GraphProto {
  repeated NodeProto node = 1;
  optional string name = 2;
  repeated TensorProto initializer = 5;
  optional string doc_string = 10;
  repeated ValueInfoProto input = 11;
  repeated ValueInfoProto output = 12;
}

message OperatorSetIdProto {
  optional string domain = 1;
  optional int64 version = 2;
}

// The subset of Onnx protos that are used in lczero.
message ModelProto {  
  optional int64 ir_version = 1;
  optional string producer_name = 2;
  optional string producer_version = 3;
  optional string domain = 4; // 'org.lczero.models.*'
  optional int64 model_version = 5;
  optional string doc_string = 6;
  optional GraphProto graph = 7;
  repeated OperatorSetIdProto opset_import = 8;
}
```

`src/neural/opencl/OpenCL.cc`:

```cc
/*
  Originally from the Leela Zero project.
  Copyright (C) 2017 Gian-Carlo Pascutto

  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
*/

#include <algorithm>
#include <array>
#include <cassert>
#include <cmath>
#include <cstdio>
#include <cstdlib>
#include <fstream>
#include <iomanip>
#include <memory>
#include <sstream>
#include <string>
#include <thread>

#include "neural/opencl/OpenCL.h"
#include "neural/opencl/OpenCLParams.h"
#include "neural/opencl/OpenCLTuner.h"
#include "utils/logging.h"

static std::string cl_args =
    "-cl-mad-enable -cl-fast-relaxed-math -cl-no-signed-zeros "
    "-cl-denorms-are-zero";

const std::string sourceCode_config =
#include "clsource/config.opencl"
    ;

const std::string sourceCode_convolve1 =
#include "clsource/convolve1.opencl"
    ;

const std::string sourceCode_convolve3 =
#include "clsource/convolve3.opencl"
    ;

const std::string sourceCode_se =
#include "clsource/se.opencl"
    ;

const std::string sourceCode_policymap =
#include "clsource/policymap.opencl"
    ;

const std::string sourceCode_blast_level3_common =
#include "clblast_level3/common.opencl"
    ;

const std::string sourceCode_blast_level3_xgemm_part1 =
#include "clblast_level3/xgemm_part1.opencl"
    ;

const std::string sourceCode_blast_level3_xgemm_part2 =
#include "clblast_level3/xgemm_part2.opencl"
    ;

const std::string sourceCode_blast_level3_xgemm_part3 =
#include "clblast_level3/xgemm_part3.opencl"
    ;

const std::string sourceCode_blast_level3_xgemm_batched =
#include "clblast_level3/xgemm_batched.opencl"
    ;

// Important: Keep the following order (common/part1/part2/part3/batched).
const std::string sourceCode_sgemm =
    sourceCode_blast_level3_common + sourceCode_blast_level3_xgemm_part1 +
    sourceCode_blast_level3_xgemm_part2 + sourceCode_blast_level3_xgemm_part3 +
    sourceCode_blast_level3_xgemm_batched;

const std::string sourceCode_sgemv =
#include "clblast_level3/xgemv.opencl"
    ;

void OpenCL_Network::add_weights(size_t layer, size_t size,
                                 const float* weights) {
  if (layer >= m_layers.size()) {
    m_layers.push_back(Layer());
  }

  auto converted_weights = std::vector<net_t>();
  for (auto i = size_t{0}; i < size; i++) {
    converted_weights.emplace_back(weights[i]);
  }

  auto weightSize = size * sizeof(decltype(converted_weights)::value_type);
  m_layers.back().weights.emplace_back(
      m_opencl.m_context, CL_MEM_COPY_HOST_PTR | CL_MEM_READ_ONLY, weightSize,
      const_cast<net_t*>(converted_weights.data()));
}

void OpenCL_Network::add_weights_short(size_t layer, size_t size,
                                       const short* weights) {
  if (layer >= m_layers.size()) {
    m_layers.push_back(Layer());
  }

  auto weightSize = size * sizeof(short);
  m_layers.back().weights.emplace_back(m_opencl.m_context,
                                       CL_MEM_COPY_HOST_PTR | CL_MEM_READ_ONLY,
                                       weightSize, (void*)weights);
}

template <class T>
static std::string opencl_dev_type_to_string(T type) {
  if (type == CL_DEVICE_TYPE_CPU) {
    return "CPU";
  } else if (type == CL_DEVICE_TYPE_GPU) {
    return "GPU";
  } else if (type == CL_DEVICE_TYPE_ACCELERATOR) {
    return "Accelerator";
  } else {
    return "Unknown";
  }
}

static const char* trim_left(const char* trim_me) {
  while (isspace(*trim_me)) trim_me++;
  return trim_me;
}

void OpenCL::process_tuners(std::string tuners) {
  std::string buf;
  std::stringstream ss(tuners);
  std::size_t found;

  auto mwg = false;
  auto nwg = false;
  auto kwg = false;
  auto ndimc = false;
  auto mdimc = false;
  auto vwm = false;
  auto vwn = false;
  while (ss >> buf) {
    found = buf.find("=");
    if (found == std::string::npos) {
      CERR << "Invalid tuner string: " << tuners << std::endl;
      std::exit(-1);
    }
    std::string name = buf.substr(0, found);
    auto value = std::stoi(buf.substr(found + 1, std::string::npos));
    if (name == "-DMWG") {
      m_sgemm_tuners.mwg = value;
      mwg = true;
    }
    if (name == "-DNWG") {
      m_sgemm_tuners.nwg = value;
      nwg = true;
    }
    if (name == "-DKWG") {
      m_sgemm_tuners.kwg = value;
      kwg = true;
    }
    if (name == "-DMDIMC") {
      m_sgemm_tuners.mdimc = value;
      mdimc = true;
    }
    if (name == "-DNDIMC") {
      m_sgemm_tuners.ndimc = value;
      ndimc = true;
    }
    if (name == "-DVWM") {
      m_sgemm_tuners.vwm = value;
      vwm = true;
    }
    if (name == "-DVWN") {
      m_sgemm_tuners.vwn = value;
      vwn = true;
    }
  }
  if (!mwg || !nwg || !kwg || !mdimc || !ndimc || !vwm || !vwn) {
    CERR << "Missing tuner parameters";
    if (!mwg) {
      CERR << " MWG";
    }
    if (!nwg) {
      CERR << " NWG";
    }
    if (!kwg) {
      CERR << " KWG";
    }
    if (!mdimc) {
      CERR << " MDIMC";
    }
    if (!ndimc) {
      CERR << " NDIMC";
    }
    if (!vwm) {
      CERR << " VWM";
    }
    if (!vwn) {
      CERR << " VWN";
    }
    CERR << std::endl;
    std::exit(-1);
  }
}

std::vector<size_t> OpenCL::get_sgemm_tuners(void) {
  std::vector<size_t> tuners;

  tuners.emplace_back(m_sgemm_tuners.mwg);
  tuners.emplace_back(m_sgemm_tuners.nwg);
  tuners.emplace_back(m_sgemm_tuners.kwg);
  tuners.emplace_back(m_sgemm_tuners.vwm);
  tuners.emplace_back(m_sgemm_tuners.vwn);
  tuners.emplace_back(m_sgemm_tuners.mdimc);
  tuners.emplace_back(m_sgemm_tuners.ndimc);

  return tuners;
}

void OpenCL::initialize(const int channels, const OpenCLParams& params) {
  CERR << "Initializing OpenCL.";
  std::vector<cl::Platform> platforms;
  try {
    cl::Platform::get(&platforms);
  } catch (const cl::Error& e) {
    CERR << "OpenCL: " << e.what();
    throw;
  }

  auto best_version = 0.0f;
  cl::Platform best_platform;
  cl::Device best_device;
  std::string best_vendor;
  auto best_score = 0;
  auto found_device = false;
  auto id = 0;

  CERR << "Detected " << platforms.size() << " OpenCL platforms.";

  for (const auto& p : platforms) {
    std::string platvers = p.getInfo<CL_PLATFORM_VERSION>();
    std::string platprof = p.getInfo<CL_PLATFORM_PROFILE>();
    std::string platname = p.getInfo<CL_PLATFORM_NAME>();
    std::string platvend = p.getInfo<CL_PLATFORM_VENDOR>();
    CERR << "Platform version: " << platvers;
    CERR << "Platform profile: " << platprof;
    CERR << "Platform name:    " << platname;
    CERR << "Platform vendor:  " << platvend;

    std::istringstream versstream(platvers);
    std::string tmp;
    float opencl_version;
    versstream >> tmp >> opencl_version;

    std::vector<cl::Device> devices;
    try {
      p.getDevices(CL_DEVICE_TYPE_ALL, &devices);
    } catch (const cl::Error& e) {
      CERR << "Error getting device(s): " << e.what() << ": " << e.err()
           << std::endl;
      devices.clear();
    }
    for (auto& d : devices) {
      CERR << "Device ID:      " << id;
      CERR << "Device name:    "
           << trim_left(d.getInfo<CL_DEVICE_NAME>().c_str());
      CERR << "Device type:    "
           << opencl_dev_type_to_string(d.getInfo<CL_DEVICE_TYPE>());
      CERR << "Device vendor:  " << d.getInfo<CL_DEVICE_VENDOR>();
      CERR << "Device driver:  " << d.getInfo<CL_DRIVER_VERSION>();
      CERR << "Device speed:   " << d.getInfo<CL_DEVICE_MAX_CLOCK_FREQUENCY>()
           << " MHZ";
      CERR << "Device cores:   " << d.getInfo<CL_DEVICE_MAX_COMPUTE_UNITS>()
           << " CU";

      // assign score, try to find best device
      int this_score = 0;
      std::string this_vendor = d.getInfo<CL_DEVICE_VENDOR>();
      std::transform(this_vendor.begin(), this_vendor.end(),
                     this_vendor.begin(), ::tolower);
      this_score += 1000 * (this_vendor.find("advanced micro devices") !=
                            std::string::npos);
      this_score += 1000 * (this_vendor.find("amd") != std::string::npos);
      this_score += 1000 * (this_vendor.find("nvidia") != std::string::npos);
      this_score += 500 * (this_vendor.find("intel") != std::string::npos);
      this_score += 100 * (d.getInfo<CL_DEVICE_TYPE>() == CL_DEVICE_TYPE_GPU);
      this_score += opencl_version * 10;
      CERR << "Device score:   " << this_score;

      bool preferred = params.gpuId == id;

      if ((this_score > best_score) || preferred) {
        best_version = opencl_version;
        best_platform = p;
        best_device = d;
        best_vendor = this_vendor;
        if (preferred) {
          best_score = std::numeric_limits<decltype(best_score)>::max();
        } else {
          best_score = this_score;
        }
        found_device = true;
      }
      id++;
    }
  }

  if (!found_device) {
    throw std::runtime_error("No suitable OpenCL device found.");
  }

  CERR << "Selected platform: " << best_platform.getInfo<CL_PLATFORM_NAME>();
  CERR << "Selected device: "
       << trim_left(best_device.getInfo<CL_DEVICE_NAME>().c_str());
  CERR << "with OpenCL " << std::fixed << std::setprecision(1) << best_version
       << " capability.";
  cl::Context context;
  try {
    context = cl::Context(best_device);
  } catch (const cl::Error& e) {
    CERR << "Error creating OpenCL context: " << e.what() << ": " << e.err();
    throw std::runtime_error("Error creating OpenCL context.");
  }
  m_context = context;
  m_device = best_device;

  // Make program of the source code in the context.
  try {
    m_program = cl::Program(
        m_context, sourceCode_config + sourceCode_convolve1 +
                       sourceCode_convolve3 + sourceCode_se + sourceCode_sgemm +
                       sourceCode_sgemv + sourceCode_policymap);
  } catch (const cl::Error& e) {
    CERR << "Error getting kernels: " << e.what() << ": " << e.err();
    throw std::runtime_error("Error getting OpenCL kernels.");
  }

  m_cl_args = cl_args;

  auto t = Tuner(*this, params, m_context, m_device);
  auto sgemm_tuners = t.load_sgemm_tuners(
      channels, params.tune_batch_size * WINOGRAD_P, channels, WINOGRAD_TILE);

  // Build program for these specific devices.
  try {
    std::string args = cl_args;
    args += sgemm_tuners;
    m_program.build(args.c_str());
  } catch (const cl::Error&) {
    CERR << "Error building kernels: "
         << m_program.getBuildInfo<CL_PROGRAM_BUILD_LOG>(m_device) << ".";
    throw std::runtime_error("Error building OpenCL kernels.");
  }

  process_tuners(sgemm_tuners);

  auto sgemm_kernel = cl::Kernel(m_program, "XgemmBatched");

  m_wavefront_size =
      sgemm_kernel
          .getWorkGroupInfo<CL_KERNEL_PREFERRED_WORK_GROUP_SIZE_MULTIPLE>(
              best_device);
  CERR << "Wavefront/Warp size: " << m_wavefront_size << std::endl;

  m_max_workgroup_size = best_device.getInfo<CL_DEVICE_MAX_WORK_GROUP_SIZE>();
  m_max_workgroup_dims = best_device.getInfo<CL_DEVICE_MAX_WORK_ITEM_SIZES>();

  CERR << "Max workgroup size: " << m_max_workgroup_size;
  std::ostringstream ss;
  for (auto d : m_max_workgroup_dims) ss << ' ' << d;
  CERR << "Max workgroup dimensions:" << ss.str();
  m_init_ok = true;
}

std::unique_ptr<OpenCLBuffers> OpenCL_Network::acquire_buffers() const {
  std::lock_guard<std::mutex> lock(m_pool_mutex);
  if (m_buffers_pool.empty()) return std::make_unique<OpenCLBuffers>(*this);
  auto result = std::move(m_buffers_pool.back());
  m_buffers_pool.pop_back();
  return result;
}

void OpenCL_Network::release_buffers(
    std::unique_ptr<OpenCLBuffers> buffers) const {
  std::lock_guard<std::mutex> lock(m_pool_mutex);
  m_buffers_pool.push_back(std::move(buffers));
}

std::string OpenCL::get_device_name() {
  std::stringstream ss;

  ss << "OpenCL: ";
  ss << m_device.getInfo<CL_DEVICE_VENDOR>() << " ";
  ss << m_device.getInfo<CL_DEVICE_NAME>() << " @ ";
  ss << m_device.getInfo<CL_DEVICE_MAX_CLOCK_FREQUENCY>() << "MHz";

  return ss.str();
}

```

`src/neural/opencl/OpenCL.h`:

```h
/*
  Originally from the Leela Zero project.
  Copyright (C) 2017 Gian-Carlo Pascutto

  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
*/

#pragma once

using net_t = float;

#define CL_HPP_MINIMUM_OPENCL_VERSION 110
#if defined(_WIN32) && !defined(_WIN64)
#define CL_HPP_TARGET_OPENCL_VERSION 110
#else
#define CL_HPP_TARGET_OPENCL_VERSION 120
#endif
#define CL_HPP_ENABLE_EXCEPTIONS
#include <cstddef>
#include <memory>
#include <mutex>
#include <string>
#include <vector>

#include "cl2.hpp"
#include "neural/opencl/OpenCLBuffers.h"
#include "neural/opencl/OpenCLParams.h"

inline size_t ceilMultiple(size_t a, size_t b) {
  if (a % b == 0) return a;
  return a + (b - a % b);
}

static constexpr auto WINOGRAD_P = 8 * 8 / 4;
static constexpr auto WINOGRAD_TILE = 4 * 4;

class OpenCL;
class OpenCLBuffers;

class Layer {
  friend class OpenCL_Network;
  friend class OpenCLBuffers;

 private:
  unsigned int channels{0};
  unsigned int outputs{0};
  unsigned int se_fc_outputs{0};
  unsigned int filter_size{0};
  unsigned int ip_in_size{0};
  unsigned int ip_out_size{0};
  bool is_input_convolution{false};
  bool is_residual_block{false};
  bool is_se_unit{false};
  bool is_policy{false};
  bool is_conv_policy{false};
  bool is_value{false};
  bool is_moves_left{false};
  std::vector<cl::Buffer> weights;
};

class OpenCL_Network {
  friend class OpenCLBuffers;

 public:
  OpenCL_Network(OpenCL& opencl) : m_opencl(opencl), m_max_batch_size(1) {}

  std::unique_ptr<OpenCLBuffers> acquire_buffers() const;
  void release_buffers(std::unique_ptr<OpenCLBuffers>) const;

  OpenCL& getOpenCL() const { return m_opencl; }

  size_t getMaxMatchSize() const { return m_max_batch_size; }

  void setMaxMatchSize(size_t new_value) { m_max_batch_size = new_value; }

  void push_input_convolution(unsigned int filter_size, unsigned int channels,
                              unsigned int outputs,
                              const std::vector<float>& weights,
                              const std::vector<float>& biases) {
    size_t layer = get_layer_count();
    push_weights(layer, weights);
    push_weights(layer, biases);
    m_layers[layer].is_input_convolution = true;
    m_layers[layer].outputs = outputs;
    m_layers[layer].filter_size = filter_size;
    m_layers[layer].channels = channels;
  }

  void push_residual(unsigned int filter_size, unsigned int channels,
                     unsigned int outputs, const std::vector<float>& weights_1,
                     const std::vector<float>& biases_1,
                     const std::vector<float>& weights_2,
                     const std::vector<float>& biases_2) {
    size_t layer = get_layer_count();
    push_weights(layer, weights_1);
    push_weights(layer, biases_1);
    push_weights(layer, weights_2);
    push_weights(layer, biases_2);
    m_layers[layer].is_residual_block = true;
    m_layers[layer].outputs = outputs;
    m_layers[layer].filter_size = filter_size;
    m_layers[layer].channels = channels;
  }

  void push_se(unsigned int channels, unsigned int se_fc_outputs,
               const std::vector<float>& weights_1,
               const std::vector<float>& biases_1,
               const std::vector<float>& weights_2,
               const std::vector<float>& biases_2) {
    size_t layer = get_layer_count();
    push_weights(layer, weights_1);
    push_weights(layer, biases_1);
    push_weights(layer, weights_2);
    push_weights(layer, biases_2);
    m_layers[layer].is_se_unit = true;
    m_layers[layer].channels = channels;
    m_layers[layer].se_fc_outputs = se_fc_outputs;
    m_layers[layer].outputs = channels;
  }

  void push_policy(unsigned int channels, unsigned int outputs,
                   unsigned int ip_in, unsigned int ip_out,
                   const std::vector<float>& weights,
                   const std::vector<float>& biases,
                   const std::vector<float>& fc_w,
                   const std::vector<float>& fc_b) {
    size_t layer = get_layer_count();
    push_weights(layer, weights);
    push_weights(layer, biases);
    push_weights(layer, fc_w);
    push_weights(layer, fc_b);
    m_layers[layer].is_policy = true;
    m_layers[layer].outputs = outputs;
    m_layers[layer].channels = channels;
    m_layers[layer].ip_in_size = ip_in;
    m_layers[layer].ip_out_size = ip_out;
  }

  void push_conv_policy(unsigned int channels, unsigned int outputs,
                        unsigned int ip_in, unsigned int ip_out,
                        const std::vector<float>& weights_1,
                        const std::vector<float>& biases_1,
                        const std::vector<float>& weights_2,
                        const std::vector<float>& biases_2,
                        const std::vector<short>& indices) {
    size_t layer = get_layer_count();
    push_weights(layer, weights_1);
    push_weights(layer, biases_1);
    push_weights(layer, weights_2);
    push_weights(layer, biases_2);
    push_weights_short(layer, indices);
    m_layers[layer].is_conv_policy = true;
    m_layers[layer].outputs = outputs;
    m_layers[layer].channels = channels;
    m_layers[layer].ip_in_size = ip_in;
    m_layers[layer].ip_out_size = ip_out;
  }

  void push_value(unsigned int channels, unsigned int outputs,
                  unsigned int ip_in, unsigned int ip_out,
                  const std::vector<float>& weights,
                  const std::vector<float>& biases,
                  const std::vector<float>& fc_w,
                  const std::vector<float>& fc_b) {
    size_t layer = get_layer_count();
    push_weights(layer, weights);
    push_weights(layer, biases);
    push_weights(layer, fc_w);
    push_weights(layer, fc_b);
    m_layers[layer].is_value = true;
    m_layers[layer].outputs = outputs;
    m_layers[layer].channels = channels;
    m_layers[layer].ip_in_size = ip_in;
    m_layers[layer].ip_out_size = ip_out;
  }

  void push_moves_left(unsigned int channels, unsigned int outputs,
                       unsigned int ip_in, unsigned int ip_out,
                       const std::vector<float>& weights,
                       const std::vector<float>& biases,
                       const std::vector<float>& fc_w,
                       const std::vector<float>& fc_b) {
    size_t layer = get_layer_count();
    push_weights(layer, weights);
    push_weights(layer, biases);
    push_weights(layer, fc_w);
    push_weights(layer, fc_b);
    m_layers[layer].is_moves_left = true;
    m_layers[layer].outputs = outputs;
    m_layers[layer].channels = channels;
    m_layers[layer].ip_in_size = ip_in;
    m_layers[layer].ip_out_size = ip_out;
  }

  size_t get_layer_count() const { return m_layers.size(); }

 private:
  void push_weights(size_t layer, const std::vector<float>& weights) {
    add_weights(layer, weights.size(), weights.data());
  }
  void add_weights(size_t layer, size_t size, const float* weights);

  void push_weights_short(size_t layer, const std::vector<short>& weights) {
    add_weights_short(layer, weights.size(), weights.data());
  }
  void add_weights_short(size_t layer, size_t size, const short* weights);

  OpenCL& m_opencl;
  size_t m_max_batch_size;

  std::vector<Layer> m_layers;

  mutable std::mutex m_pool_mutex;
  mutable std::vector<std::unique_ptr<OpenCLBuffers>> m_buffers_pool;
};

class OpenCL {
  friend class OpenCL_Network;
  friend class OpenCLBuffers;
  friend class Tuner;

 public:
  void initialize(const int channels, const OpenCLParams& params);
  std::string get_device_name();

  std::vector<size_t> get_sgemm_tuners(void);

  cl::Device m_device;
  cl::Context m_context;

 private:
  void tune_sgemm(void);
  void process_tuners(std::string tuners);

  cl::Program m_program;
  std::string m_cl_args;

  struct sgemm_tuners {
    size_t mwg, nwg, kwg;
    size_t vwm, vwn;
    size_t mdimc, ndimc;
  };
  sgemm_tuners m_sgemm_tuners;
  size_t m_wavefront_size{0};
  size_t m_max_workgroup_size{0};
  std::vector<size_t> m_max_workgroup_dims;
  bool m_init_ok{false};
};

extern const std::string sourceCode_sgemm;

```

`src/neural/opencl/OpenCLBuffers.cc`:

```cc
/*
  Originally from the Leela Zero project.
  Copyright (C) 2017 Gian-Carlo Pascutto

  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
*/

#include "neural/opencl/OpenCLBuffers.h"

OpenCLBuffers::OpenCLBuffers(const OpenCL_Network& opencl_net)
    : m_opencl_net(opencl_net), m_opencl(opencl_net.getOpenCL()) {
  auto& program = m_opencl.m_program;
  auto& context = m_opencl.m_context;
  auto& device = m_opencl.m_device;

  m_convolve1_kernel = cl::Kernel(program, "convolve1");
  m_merge_kernel = cl::Kernel(program, "merge_bn");
  m_in_transform_kernel = cl::Kernel(program, "in_transform");
  m_sgemm_kernel = cl::Kernel(program, "XgemmBatched");
  m_out_transform_bn_kernel = cl::Kernel(program, "out_transform_fused_bn");
  m_out_transform_bn_in_kernel =
      cl::Kernel(program, "out_transform_fused_bn_in");
  m_global_avg_pooling_kernel = cl::Kernel(program, "global_avg_pooling");
  m_apply_se_kernel = cl::Kernel(program, "apply_se");
  m_policymap_kernel = cl::Kernel(program, "policymap");
  m_sgemv_kernel = cl::Kernel(program, "Xgemv");
  m_commandqueue = cl::CommandQueue(context, device);

  auto& layers = m_opencl_net.m_layers;

  constexpr auto tiles = WINOGRAD_P;
  constexpr auto width = 8;
  constexpr auto height = 8;

  m_finalSize_pol = 0;
  m_finalSize_val = 0;
  m_finalSize_mov = 0;

  auto max_channels = unsigned{0};
  for (const auto& layer : layers) {
    max_channels =
        std::max(max_channels, std::max(layer.channels, layer.outputs));
    if (layer.is_policy || layer.is_conv_policy) {
      m_finalSize_pol = layer.ip_out_size * sizeof(net_t);
    }
    if (layer.is_value) {
      m_finalSize_val = layer.ip_out_size * sizeof(net_t);
    }
    if (layer.is_moves_left) {
      m_finalSize_mov = layer.ip_out_size * sizeof(net_t);
    }
  }

  const auto mwg = m_opencl.m_sgemm_tuners.mwg;
  const auto nwg = m_opencl.m_sgemm_tuners.nwg;
  const auto vwm = m_opencl.m_sgemm_tuners.vwm;
  const auto vwn = m_opencl.m_sgemm_tuners.vwn;

  const auto m_ceil = ceilMultiple(ceilMultiple(max_channels, mwg), vwm);
  const auto n_ceil = ceilMultiple(ceilMultiple(tiles, nwg), vwn);

  const auto max_batch_size = m_opencl_net.getMaxMatchSize();
  const auto alloc_inSize =
      max_batch_size * width * height * max_channels * sizeof(net_t);
  const auto alloc_vm_size =
      max_batch_size * WINOGRAD_TILE * m_ceil * n_ceil * sizeof(net_t);
  const auto alloc_pool_size =
      max_batch_size * 2 * max_channels * sizeof(net_t);

  auto v_zeros = std::vector<float>(alloc_vm_size);

  m_inBuffer = cl::Buffer(m_opencl.m_context, CL_MEM_READ_WRITE, alloc_inSize);
  m_inBuffer2 = cl::Buffer(m_opencl.m_context, CL_MEM_READ_WRITE, alloc_inSize);
  m_VBuffer = cl::Buffer(
      m_opencl.m_context,
      CL_MEM_READ_WRITE | CL_MEM_HOST_NO_ACCESS | CL_MEM_COPY_HOST_PTR,
      alloc_vm_size, v_zeros.data(), nullptr);
  m_MBuffer =
      cl::Buffer(m_opencl.m_context, CL_MEM_READ_WRITE | CL_MEM_HOST_NO_ACCESS,
                 alloc_vm_size);

  try {
    m_pinnedOutBuffer_pol = cl::Buffer(
        m_opencl.m_context, CL_MEM_WRITE_ONLY | CL_MEM_ALLOC_HOST_PTR,
        max_batch_size * m_finalSize_pol);
  } catch (const cl::Error& e) {
    CERR << "Error in m_pinnedOutBuffer_pol: " << e.what() << ": " << e.err()
         << std::endl;
    throw;
  }

  try {
    m_pinnedOutBuffer_val = cl::Buffer(
        m_opencl.m_context, CL_MEM_WRITE_ONLY | CL_MEM_ALLOC_HOST_PTR,
        max_batch_size * m_finalSize_val);
  } catch (const cl::Error& e) {
    CERR << "Error in m_pinnedOutBuffer_val: " << e.what() << ": " << e.err()
         << std::endl;
    throw;
  }

  if (m_finalSize_mov > 0) {
    try {
      m_pinnedOutBuffer_mov = cl::Buffer(
          m_opencl.m_context, CL_MEM_WRITE_ONLY | CL_MEM_ALLOC_HOST_PTR,
          max_batch_size * m_finalSize_mov);
    } catch (const cl::Error& e) {
      CERR << "Error in m_pinnedOutBuffer_mov: " << e.what() << ": " << e.err()
           << std::endl;
      throw;
    }
  }

  m_pool_buffer =
      cl::Buffer(m_opencl.m_context, CL_MEM_READ_WRITE | CL_MEM_HOST_NO_ACCESS,
                 alloc_pool_size);
}

void OpenCLBuffers::forward(const std::vector<net_t>& input,
                            std::vector<net_t>& output_pol,
                            std::vector<net_t>& output_val,
                            std::vector<net_t>& output_mov,
                            const int batch_size) {
  auto& layers = m_opencl_net.m_layers;

  const auto inSize = sizeof(net_t) * input.size();
  m_commandqueue.enqueueWriteBuffer(m_inBuffer, CL_FALSE, 0, inSize,
                                    input.data());

  auto skip_in_trans = false;
  for (auto iter = cbegin(layers); iter != cend(layers); iter++) {
    const auto& layer = *iter;
    const auto niter = std::next(iter);

    if (layer.is_input_convolution) {
      assert(niter != cend(layers));
      auto conv_weights = begin(layer.weights);
      auto conv_biases = begin(layer.weights) + 1;
      auto skip_next_in_trans = false;
      if (niter->is_residual_block) {
        skip_next_in_trans = true;
      }
      convolve3(layer.channels, layer.outputs, m_inBuffer, m_inBuffer,
                m_VBuffer, m_MBuffer, conv_weights, nullptr, conv_biases,
                skip_in_trans, skip_next_in_trans, true, true, batch_size);
      skip_in_trans = skip_next_in_trans;
    } else if (layer.is_residual_block) {
      assert(layer.channels == layer.outputs);
      assert(niter != cend(layers));
      auto conv1_weights = begin(layer.weights);
      auto conv1_biases = begin(layer.weights) + 1;
      auto conv2_weights = begin(layer.weights) + 2;
      auto conv2_biases = begin(layer.weights) + 3;

      convolve3(layer.channels,  // channels
                layer.outputs,   // outputs
                m_inBuffer,      // bufferIn
                m_inBuffer2,     // bufferOut
                m_VBuffer,       // bufferV
                m_MBuffer,       // bufferM
                conv1_weights,   // weights
                nullptr,         // bufferResidual
                conv1_biases,    // biases
                skip_in_trans,   // skip_in_transform
                true,            // fuse_in_transform
                false,           // store_inout
                true,            // relu
                batch_size);     // batch_size

      auto skip_next_in_trans = false;
      if (niter->is_residual_block) {
        skip_next_in_trans = true;
      }
      auto relu = true;
      auto residual = &m_inBuffer;
      auto out_buffer = m_inBuffer;
      auto store_inout = true;
      if (niter->is_se_unit) {
        // SE unit does relu
        relu = false;
        residual = nullptr;
        out_buffer = m_inBuffer2;
        store_inout = false;
      }
      convolve3(layer.channels,      // channels
                layer.outputs,       // outputs
                m_inBuffer2,         // bufferIn
                out_buffer,          // bufferOut
                m_VBuffer,           // bufferV
                m_MBuffer,           // bufferM
                conv2_weights,       // weights
                residual,            // bufferResidual
                conv2_biases,        // biases
                true,                // skip_in_transform
                skip_next_in_trans,  // fuse_in_transform
                store_inout,         // store_inout
                relu,                // relu
                batch_size);         // batch_size
      skip_in_trans = skip_next_in_trans;
    } else if (layer.is_se_unit) {
      // inBuffer: residual connection from start of the residual block
      // inBuffer2: Last block output
      // Output will be written in inBuffer
      assert(niter != cend(layers));
      auto se_weights = begin(layer.weights);
      squeeze_excitation(layer.outputs,        // channels
                         layer.se_fc_outputs,  // fc_outputs
                         m_inBuffer2,          // bufferIn
                         m_pool_buffer,        // bufferTemp1
                         m_MBuffer,            // bufferTemp2
                         se_weights,           // weights
                         m_inBuffer,           // residual
                         batch_size);          // batch_size
    } else if (layer.is_conv_policy) {
      assert(niter != cend(layers));
      auto conv1_weights = begin(layer.weights);
      auto conv1_biases = begin(layer.weights) + 1;
      auto conv2_weights = begin(layer.weights) + 2;
      auto conv2_biases = begin(layer.weights) + 3;
      auto indices = begin(layer.weights) + 4;

      convolve3(layer.channels,  // channels
                layer.channels,  // outputs
                m_inBuffer,      // bufferIn
                m_inBuffer2,     // bufferOut
                m_VBuffer,       // bufferV
                m_MBuffer,       // bufferM
                conv1_weights,   // weights
                nullptr,         // bufferResidual
                conv1_biases,    // biases
                skip_in_trans,   // skip_in_transform
                true,            // fuse_in_transform
                false,           // store_inout
                true,            // relu
                batch_size);     // batch_size

      // m_inBuffer needs to be preserved for value head
      convolve3(layer.channels,  // channels
                layer.outputs,   // outputs
                m_inBuffer2,     // bufferIn
                m_inBuffer2,     // bufferOut
                m_VBuffer,       // bufferV
                m_MBuffer,       // bufferM
                conv2_weights,   // weights
                nullptr,         // bufferResidual
                conv2_biases,    // biases
                true,            // skip_in_transform
                false,           // fuse_in_transform
                false,           // store_inout
                false,           // relu
                batch_size);     // batch_size

      policymap(batch_size, m_inBuffer2, m_pinnedOutBuffer_pol, indices[0],
                layer.outputs * 8 * 8, layer.ip_in_size, layer.ip_out_size);

    } else {
      assert(layer.is_value || layer.is_policy || layer.is_moves_left);

      cl::Buffer out_buffer;
      if (layer.is_policy) {
        out_buffer = m_pinnedOutBuffer_pol;
      } else if (layer.is_value) {
        out_buffer = m_pinnedOutBuffer_val;
      } else {
        out_buffer = m_pinnedOutBuffer_mov;
      }

      auto conv_weights = begin(layer.weights);
      auto conv_biases = begin(layer.weights) + 1;
      auto ip_w = begin(layer.weights) + 2;
      auto ip_b = begin(layer.weights) + 3;

      convolve1(layer.channels, layer.outputs, m_inBuffer, m_inBuffer2,
                m_VBuffer, conv_weights, conv_biases, batch_size);

      innerproduct(m_inBuffer2, ip_w, ip_b, out_buffer, layer.ip_in_size,
                   layer.ip_out_size, layer.is_value, batch_size);
    }
  }

  auto pinnedOutBufferHost_pol = m_commandqueue.enqueueMapBuffer(
      m_pinnedOutBuffer_pol, CL_FALSE, CL_MAP_READ, 0,
      batch_size * m_finalSize_pol);
  auto pinnedOutBufferHost_val = m_commandqueue.enqueueMapBuffer(
      m_pinnedOutBuffer_val, CL_FALSE, CL_MAP_READ, 0,
      batch_size * m_finalSize_val);
  void* pinnedOutBufferHost_mov;
  if (m_finalSize_mov > 0) {
    pinnedOutBufferHost_mov = m_commandqueue.enqueueMapBuffer(
        m_pinnedOutBuffer_mov, CL_FALSE, CL_MAP_READ, 0,
        batch_size * m_finalSize_mov);
  }

  m_commandqueue.finish();

  std::memcpy(output_pol.data(), pinnedOutBufferHost_pol,
              batch_size * m_finalSize_pol);
  std::memcpy(output_val.data(), pinnedOutBufferHost_val,
              batch_size * m_finalSize_val);
  if (m_finalSize_mov > 0) {
    std::memcpy(output_mov.data(), pinnedOutBufferHost_mov,
                batch_size * m_finalSize_mov);
  }

  m_commandqueue.enqueueUnmapMemObject(m_pinnedOutBuffer_pol,
                                       pinnedOutBufferHost_pol);
  m_commandqueue.enqueueUnmapMemObject(m_pinnedOutBuffer_val,
                                       pinnedOutBufferHost_val);
  if (m_finalSize_mov > 0) {
    m_commandqueue.enqueueUnmapMemObject(m_pinnedOutBuffer_mov,
                                         pinnedOutBufferHost_mov);
  }
}

void OpenCLBuffers::convolve3(int channels, int outputs, cl::Buffer& bufferIn,
                              cl::Buffer& bufferOut, cl::Buffer& bufferV,
                              cl::Buffer& bufferM, weight_slice_t weights,
                              cl::Buffer* bufferResidual, weight_slice_t biases,
                              bool skip_in_transform, bool fuse_in_transform,
                              bool store_inout, bool relu, int batch_size) {
  auto mwg = m_opencl.m_sgemm_tuners.mwg;
  auto nwg = m_opencl.m_sgemm_tuners.nwg;
  auto kwg = m_opencl.m_sgemm_tuners.kwg;
  auto vwm = m_opencl.m_sgemm_tuners.vwm;
  auto vwn = m_opencl.m_sgemm_tuners.vwn;
  auto mdimc = m_opencl.m_sgemm_tuners.mdimc;
  auto ndimc = m_opencl.m_sgemm_tuners.ndimc;
  auto wavefront_size = m_opencl.m_wavefront_size;

  assert(mwg != 0);
  assert(nwg != 0);
  assert(kwg != 0);
  assert(mdimc != 0);
  assert(ndimc != 0);
  assert(vwm != 0);
  assert(vwn != 0);
  assert(wavefront_size != 0);

  constexpr auto tiles = WINOGRAD_P;
  constexpr auto width = 8;
  constexpr auto height = 8;

  auto wgs = ceilMultiple(tiles, wavefront_size);
  auto m_ceil = int(ceilMultiple(ceilMultiple(outputs, mwg), vwm));
  auto n_ceil = int(ceilMultiple(ceilMultiple(batch_size * tiles, nwg), vwn));
  auto k_ceil = int(ceilMultiple(ceilMultiple(channels, kwg), vwm));

  if (!skip_in_transform) {
    try {
      m_in_transform_kernel.setArg(0, bufferIn);
      m_in_transform_kernel.setArg(1, bufferV);
      m_in_transform_kernel.setArg(2, channels);
      m_in_transform_kernel.setArg(3, k_ceil);
      m_in_transform_kernel.setArg(4, n_ceil);

      m_commandqueue.enqueueNDRangeKernel(
          m_in_transform_kernel, cl::NullRange,
          cl::NDRange(wgs, channels, batch_size));
    } catch (const cl::Error& e) {
      CERR << "Error in convolve3/in: " << e.what() << ": " << e.err()
           << std::endl;
      throw;
    }
  }

  try {
    m_sgemm_kernel.setArg(0, m_ceil);
    m_sgemm_kernel.setArg(1, n_ceil);
    m_sgemm_kernel.setArg(2, k_ceil);
    m_sgemm_kernel.setArg(3, weights[0]);
    m_sgemm_kernel.setArg(4, bufferV);
    m_sgemm_kernel.setArg(5, bufferM);

    cl::NDRange local_sgemm = {mdimc, ndimc, 1};

    cl::NDRange size_sgemm = {(m_ceil * mdimc) / mwg, (n_ceil * ndimc) / nwg,
                              (cl::size_type)WINOGRAD_TILE};

    m_commandqueue.enqueueNDRangeKernel(m_sgemm_kernel, cl::NullRange,
                                        size_sgemm, local_sgemm);
  } catch (const cl::Error& e) {
    CERR << "Error in convolve3/sgemm: " << e.what() << ": " << e.err()
         << std::endl;
    throw;
  }

  try {
    if (fuse_in_transform) {
      assert(relu);  // No relu not supported

      // TODO : Eventually this might also be something tuneable?
      constexpr auto dim_size = 2;
      m_out_transform_bn_in_kernel.setArg(0, bufferM);
      if (store_inout) {
        m_out_transform_bn_in_kernel.setArg(1, bufferOut);
      } else {
        m_out_transform_bn_in_kernel.setArg(1, nullptr);
      }
      m_out_transform_bn_in_kernel.setArg(2, bufferV);
      m_out_transform_bn_in_kernel.setArg(3, outputs);
      m_out_transform_bn_in_kernel.setArg(4, m_ceil);
      m_out_transform_bn_in_kernel.setArg(5, n_ceil);
      // k_ceil of the next convolution
      auto k_ceil2 = int(ceilMultiple(ceilMultiple(outputs, kwg), vwm));
      m_out_transform_bn_in_kernel.setArg(6, k_ceil2);
      if (bufferResidual) {
        m_out_transform_bn_in_kernel.setArg(7, *bufferResidual);
      } else {
        m_out_transform_bn_in_kernel.setArg(7, nullptr);
      }
      m_out_transform_bn_in_kernel.setArg(8, biases[0]);
      m_out_transform_bn_in_kernel.setArg(
          9, cl::Local(dim_size * width * height * sizeof(float)));

      m_commandqueue.enqueueNDRangeKernel(
          m_out_transform_bn_in_kernel, cl::NullRange,
          cl::NDRange(outputs, wgs, batch_size), cl::NDRange(dim_size, wgs, 1));
    } else {
      m_out_transform_bn_kernel.setArg(0, bufferM);
      m_out_transform_bn_kernel.setArg(1, bufferOut);
      m_out_transform_bn_kernel.setArg(2, outputs);
      m_out_transform_bn_kernel.setArg(3, m_ceil);
      m_out_transform_bn_kernel.setArg(4, n_ceil);
      m_out_transform_bn_kernel.setArg(5, static_cast<int>(relu));
      if (bufferResidual) {
        m_out_transform_bn_kernel.setArg(6, *bufferResidual);
      } else {
        m_out_transform_bn_kernel.setArg(6, nullptr);
      }
      m_out_transform_bn_kernel.setArg(7, biases[0]);

      m_commandqueue.enqueueNDRangeKernel(
          m_out_transform_bn_kernel, cl::NullRange,
          cl::NDRange(outputs, wgs, batch_size));
    }
  } catch (const cl::Error& e) {
    CERR << "Error in convolve3/out: " << e.what() << ": " << e.err()
         << std::endl;
    throw;
  }
}

void OpenCLBuffers::squeeze_excitation(
    int channels, int fc_outputs, cl::Buffer& bufferIn, cl::Buffer& bufferTemp1,
    cl::Buffer& bufferTemp2, weight_slice_t weights, cl::Buffer& bufferResidual,
    int batch_size) {
  constexpr int width = 8;

  try {
    m_global_avg_pooling_kernel.setArg(0, batch_size * channels);
    m_global_avg_pooling_kernel.setArg(1, bufferIn);
    m_global_avg_pooling_kernel.setArg(2, bufferTemp1);

    m_commandqueue.enqueueNDRangeKernel(
        m_global_avg_pooling_kernel, cl::NullRange,
        cl::NDRange(width, batch_size * channels), cl::NDRange(width, 1));
  } catch (const cl::Error& e) {
    CERR << "Error in squeeze_excitation/pooling: " << e.what() << ": "
         << e.err() << std::endl;
    throw;
  }

  innerproduct(bufferTemp1, weights, weights + 1, bufferTemp2, channels,
               fc_outputs, true, batch_size);

  innerproduct(bufferTemp2, weights + 2, weights + 3, bufferTemp1, fc_outputs,
               2 * channels, false, batch_size);

  try {
    m_apply_se_kernel.setArg(0, channels);
    m_apply_se_kernel.setArg(1, batch_size);
    m_apply_se_kernel.setArg(2, bufferIn);
    m_apply_se_kernel.setArg(3, bufferResidual);
    m_apply_se_kernel.setArg(4, bufferTemp1);

    m_commandqueue.enqueueNDRangeKernel(
        m_apply_se_kernel, cl::NullRange,
        cl::NDRange(width, batch_size * channels));
  } catch (const cl::Error& e) {
    CERR << "Error in squeeze_excitation/apply_se: " << e.what() << ": "
         << e.err() << std::endl;
    throw;
  }
}

void OpenCLBuffers::convolve1(int channels, int outputs,
                              cl::Buffer& bufferInput, cl::Buffer& bufferOutput,
                              cl::Buffer& bufferMerge,
                              weight_slice_t conv_weights,
                              weight_slice_t conv_biases, int batch_size) {
  // fixed for 8x8.
  constexpr int width = 8;
  constexpr int height = 8;
  constexpr int boardsize = width * height;
  constexpr int rowTiles = 8;

  // Input channel grouping in multiples of 8.
  constexpr int channelGroup = 8;
  constexpr int channelShift = 3;
  constexpr int rowGroup = 1;
  // Assumes that if outputs > 16, then outputs is divisible by 16.
  size_t outputGroup = std::min(outputs, 16);

#ifndef NDEBUG
  // Total output size after reducing.
  size_t outSize = width * height * outputs * sizeof(net_t);

  // Produce channel * output planes and merge them at the end.
  size_t mergeSize = (channels >> channelShift) * outSize;
  assert(mergeSize <= bufferMerge.getInfo<CL_MEM_SIZE>());
#endif

  // Copy the rows locally.
  size_t stripSize = width * sizeof(float);

  int rowBuffer = std::min<int>(channelGroup, 7);
  size_t rowSize = channelGroup * outputGroup * rowBuffer * sizeof(float);

  try {
    m_convolve1_kernel.setArg(0, bufferInput);
    m_convolve1_kernel.setArg(1, bufferMerge);
    m_convolve1_kernel.setArg(2, conv_weights[0]);
    m_convolve1_kernel.setArg(3,
                              cl::Local(stripSize * channelGroup * rowGroup));
    m_convolve1_kernel.setArg(4, cl::Local(rowSize));

    m_commandqueue.enqueueNDRangeKernel(
        m_convolve1_kernel, cl::NullRange,
        cl::NDRange(channels, outputs, batch_size * rowTiles),
        cl::NDRange(channelGroup, outputGroup, rowGroup));
  } catch (const cl::Error& e) {
    CERR << "Error in convolve1: " << e.what() << ": " << e.err() << std::endl;
    throw;
  }

  assert(channels % (1 << channelShift) == 0);

  try {
    m_merge_kernel.setArg(0, bufferMerge);
    m_merge_kernel.setArg(1, bufferOutput);
    m_merge_kernel.setArg(2, channels >> channelShift);
    m_merge_kernel.setArg(3, conv_biases[0]);

    m_commandqueue.enqueueNDRangeKernel(
        m_merge_kernel, cl::NullRange,
        cl::NDRange(outputs, boardsize, batch_size),
        cl::NDRange(std::min(8, outputs), 8, 1));
  } catch (const cl::Error& e) {
    CERR << "Error in merge: " << e.what() << ": " << e.err() << std::endl;
    throw;
  }
}

void OpenCLBuffers::innerproduct(cl::Buffer& input, weight_slice_t weights,
                                 weight_slice_t biases, cl::Buffer& output,
                                 const int inputs, const int outputs,
                                 const int relu, int batch_size) {
  // TODO: Tune these.
  size_t wgs1 = 64;
  size_t wpt1 = 1;

  auto m_ceil = int(ceilMultiple(outputs, wgs1 * wpt1));
  auto global_size = m_ceil / wpt1;
  auto local_size = wgs1;

  try {
    // Sets the kernel arguments.
    m_sgemv_kernel.setArg(0, static_cast<int>(outputs));
    m_sgemv_kernel.setArg(1, static_cast<int>(inputs));
    m_sgemv_kernel.setArg(2, weights[0]);
    m_sgemv_kernel.setArg(3, static_cast<int>(0));
    m_sgemv_kernel.setArg(4, static_cast<int>(inputs));
    m_sgemv_kernel.setArg(5, input);
    m_sgemv_kernel.setArg(6, static_cast<int>(0));
    m_sgemv_kernel.setArg(7, output);
    m_sgemv_kernel.setArg(8, static_cast<int>(0));
    m_sgemv_kernel.setArg(9, biases[0]);
    m_sgemv_kernel.setArg(10, static_cast<int>(relu));

    m_commandqueue.enqueueNDRangeKernel(m_sgemv_kernel, cl::NullRange,
                                        cl::NDRange(global_size, batch_size),
                                        cl::NDRange(local_size, 1));
  } catch (const cl::Error& e) {
    CERR << "Error in innerproduct: " << e.what() << ": " << e.err()
         << std::endl;
    throw;
  }
}

void OpenCLBuffers::policymap(int N, const cl::Buffer& input,
                              cl::Buffer& output, const cl::Buffer& indices,
                              int inputSize, int usedSize, int outputSize) {
  try {
    m_policymap_kernel.setArg(0, input);
    m_policymap_kernel.setArg(1, output);
    m_policymap_kernel.setArg(2, indices);
    m_policymap_kernel.setArg(3, N);
    m_policymap_kernel.setArg(4, inputSize);
    m_policymap_kernel.setArg(5, usedSize);
    m_policymap_kernel.setArg(6, outputSize);

    m_commandqueue.enqueueNDRangeKernel(m_policymap_kernel, cl::NullRange,
                                        cl::NDRange(N * usedSize));
  } catch (const cl::Error& e) {
    CERR << "Error in policymap: " << e.what() << ": " << e.err() << std::endl;
    throw;
  }
}

```

`src/neural/opencl/OpenCLBuffers.h`:

```h
/*
  Originally from the Leela Zero project.
  Copyright (C) 2017 Gian-Carlo Pascutto

  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
*/

#pragma once

#include <algorithm>
#include <array>
#include <cassert>
#include <cmath>
#include <cstdio>
#include <cstdlib>
#include <fstream>
#include <iomanip>
#include <memory>
#include <sstream>
#include <string>
#include <thread>

#include "neural/opencl/OpenCL.h"
#include "neural/opencl/OpenCLParams.h"
#include "neural/opencl/OpenCLTuner.h"
#include "utils/logging.h"

class OpenCL_Network;
class OpenCL;

class OpenCLBuffers {
  friend class OpenCL;
  friend class OpenCL_Network;

 public:
  OpenCLBuffers(const OpenCL_Network& opencl_net);

  void forward(const std::vector<net_t>& input, std::vector<net_t>& output_pol,
               std::vector<net_t>& output_val, std::vector<net_t>& output_mov,
               const int batch_size);

 private:
  using weight_slice_t = std::vector<cl::Buffer>::const_iterator;

  void convolve3(int channels, int outputs, cl::Buffer& bufferIn,
                 cl::Buffer& bufferOut, cl::Buffer& bufferV,
                 cl::Buffer& bufferM, weight_slice_t weights,
                 cl::Buffer* bufferResidual, weight_slice_t biases,
                 bool skip_in_transform, bool fuse_in_transform,
                 bool store_inout, bool relu, int batch_size);

  void convolve1(int channels, int outputs, cl::Buffer& bufferInput,
                 cl::Buffer& bufferOutput, cl::Buffer& bufferMerge,
                 weight_slice_t weights, weight_slice_t biases, int batch_size);

  void innerproduct(cl::Buffer& input, weight_slice_t weights,
                    weight_slice_t biases, cl::Buffer& output, const int inputs,
                    const int outputs, const int relu, int batch_size);

  void squeeze_excitation(int channels, int fc_outputs, cl::Buffer& bufferIn,
                          cl::Buffer& bufferTemp1, cl::Buffer& bufferTemp2,
                          weight_slice_t weights, cl::Buffer& bufferResidual,
                          int batch_size);

  void policymap(int N, const cl::Buffer& input, cl::Buffer& output,
                 const cl::Buffer& indices, int inputSize, int usedSize,
                 int outputSize);

  const OpenCL_Network& m_opencl_net;
  const OpenCL& m_opencl;

  cl::CommandQueue m_commandqueue;
  cl::Kernel m_convolve1_kernel;
  cl::Kernel m_merge_kernel;
  cl::Kernel m_in_transform_kernel;
  cl::Kernel m_sgemm_kernel;
  cl::Kernel m_sgemv_kernel;
  cl::Kernel m_out_transform_bn_kernel;
  cl::Kernel m_out_transform_bn_in_kernel;
  cl::Kernel m_global_avg_pooling_kernel;
  cl::Kernel m_apply_se_kernel;
  cl::Kernel m_policymap_kernel;
  cl::Buffer m_inBuffer;
  cl::Buffer m_inBuffer2;
  cl::Buffer m_VBuffer;
  cl::Buffer m_MBuffer;
  cl::Buffer m_pool_buffer;
  cl::Buffer m_pinnedOutBuffer_pol;
  cl::Buffer m_pinnedOutBuffer_val;
  cl::Buffer m_pinnedOutBuffer_mov;
  size_t m_finalSize_pol;
  size_t m_finalSize_val;
  size_t m_finalSize_mov;
};

```

`src/neural/opencl/OpenCLParams.h`:

```h
/*
  Originally from the Leela Zero project.
  Copyright (C) 2017 Gian-Carlo Pascutto

  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
*/

#pragma once

struct OpenCLParams {
  int gpuId = -1;

  bool tune_only = false;
  bool force_tune = false;
  bool tune_exhaustive = false;
  int tune_batch_size = 1;
  std::string tuner_file;
};

```

`src/neural/opencl/OpenCLTuner.cc`:

```cc
/*
  Originally from the Leela Zero project.
  Copyright (C) 2017 Gian-Carlo Pascutto

  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
*/

#include "neural/opencl/OpenCLTuner.h"

#include <array>
#include <cassert>
#include <cmath>
#include <fstream>
#include <iomanip>
#include <map>
#include <random>
#include <sstream>
#include <string>

#include "neural/opencl/OpenCL.h"
#include "neural/opencl/OpenCLParams.h"
#include "utils/logging.h"

constexpr auto MAX_ERROR = 1e-4f;

static void sgemmBatched_ref(const std::vector<float>& a,
                             const std::vector<float>& b, std::vector<float>& c,
                             const int m, const int n, const int k,
                             const int batch_size) {
  for (auto batch = 0; batch < batch_size; batch++) {
    auto offset_u = batch * m * k;
    auto offset_v = batch * n * k;
    auto offset_m = batch * m * n;

    // Calculates C = transpose(tranpose(A) * B) in row major, or
    // C = A * transpose(B) in column major.
    for (auto i = 0; i < m; i++) {
      for (auto j = 0; j < n; j++) {
        auto acc = 0.0f;
        for (auto l = 0; l < k; l++) {
          acc += a[l * m + i + offset_u] * b[l * n + j + offset_v];
        }
        c[j * m + i + offset_m] = acc;
      }
    }
  }
}

static bool IsMultiple(const size_t a, const size_t b) { return (a % b == 0); }

bool Tuner::valid_config_sgemm(TuneParameters p, bool exhaustive) {
  if (!IsMultiple(p["MWG"], p["MDIMC"] * p["VWM"])) {
    return false;
  }
  if (!IsMultiple(p["NWG"], p["NDIMC"] * p["VWN"])) {
    return false;
  }
  if (!IsMultiple(p["MWG"], p["MDIMA"] * p["VWM"])) {
    return false;
  }
  if (!IsMultiple(p["NWG"], p["NDIMB"] * p["VWN"])) {
    return false;
  }
  if (!IsMultiple(p["KWG"], p["MDIMC"] * p["NDIMC"] / p["MDIMA"])) {
    return false;
  }
  if (!IsMultiple(p["KWG"], p["MDIMC"] * p["NDIMC"] / p["NDIMB"])) {
    return false;
  }
  // Extra restrictions for a fast tuning run
  if (!exhaustive) {
    if (p["MDIMC"] != p["MDIMA"]) {
      return false;
    }
    if (p["NDIMC"] != p["NDIMB"]) {
      return false;
    }
    if (p["SA"] != p["SB"]) {
      return false;
    }
  }
  return true;
}

TuneParameters Tuner::get_parameters_by_int(
    const std::vector<Configurations>& opts, const int n) {
  TuneParameters param;
  std::vector<size_t> choices(opts.size());

  for (auto c = size_t{0}; c < opts.size(); c++) {
    choices[c] = opts[c].second.size();
  }
  auto j = n;

  for (auto c = size_t{0}; c < opts.size(); c++) {
    auto o = opts[c];
    auto s = o.first;
    auto v = o.second[j % choices[c]];
    j /= choices[c];
    param[s] = v;
  }

  return param;
}

std::string Tuner::parameters_to_defines(const TuneParameters& p) {
  std::string s;
  for (auto const& x : p) {
    s += " -D" + x.first + "=" + std::to_string(x.second);
  }
  return s;
}

std::string Tuner::parameters_to_string(const TuneParameters& p) {
  std::string s;
  for (auto const& x : p) {
    s += x.first + "=" + std::to_string(x.second) + " ";
  }
  if (s.size() > 0) {
    s.resize(s.size() - 1);
  }
  return s;
}

static size_t next_power_of_two(const size_t x) {
  return 2 << (size_t)(std::ceil(std::log2(x)) - 1);
}

static void sgemm_generate_data(std::vector<float>& x, const int m, const int n,
                                const int batch_size, const int m_ceil,
                                const int n_ceil) {
  for (auto batch = 0; batch < batch_size; batch++) {
    for (auto i = 0; i < n_ceil; i++) {
      if (i < n) {
        for (auto j = 0; j < m; j++) {
          x[batch * n_ceil * m_ceil + i * m_ceil + j] =
              0.01f * (((i ^ j) + batch - 50) % 100);
        }
        for (auto j = m; j < m_ceil; j++) {
          x[batch * n_ceil * m_ceil + i * m_ceil + j] = 0.0f;
        }
      } else {
        for (auto j = 0; j < m_ceil; j++) {
          x[batch * n_ceil * m_ceil + i * m_ceil + j] = 0.0f;
        }
      }
    }
  }
}

static float compare_ref(std::vector<float>& x, std::vector<float>& ref,
                         const int m, const int n, const int batch_size,
                         const int m_ceil, const int n_ceil) {
  auto sum = 0.0f;
  for (auto batch = 0; batch < batch_size; batch++) {
    for (auto j = 0; j < m; j++) {
      for (auto i = 0; i < n; i++) {
        auto r = ref[batch * n * m + j * n + i];
        auto y = x[batch * n_ceil * m_ceil + j * n_ceil + i];

        sum += (r - y) * (r - y);
      }
    }
  }
  return sum / (m * n * batch_size);
}

std::string Tuner::tune_sgemm(const int m, const int n, const int k,
                              const int batch_size, const int runs) {
  auto opts = std::vector<Configurations>();
  if (m_params.tune_exhaustive) {
    opts = {
        {"MWG", {16, 32, 64}},  {"NWG", {16, 32, 64}},  {"KWG", {16, 32}},
        {"MDIMC", {8, 16, 32}}, {"NDIMC", {8, 16, 32}}, {"MDIMA", {8, 16, 32}},
        {"NDIMB", {8, 16, 32}}, {"KWI", {2, 8}},        {"VWM", {1, 2, 4, 8}},
        {"VWN", {1, 2, 4, 8}},  {"STRM", {0, 1}},       {"STRN", {0, 1}},
        {"SA", {0, 1}},         {"SB", {0, 1}},
    };
  } else {
    opts = {
        {"MWG", {16, 32, 64}},  {"NWG", {16, 32, 64}},  {"KWG", {32}},
        {"MDIMC", {8, 16, 32}}, {"NDIMC", {8, 16, 32}}, {"MDIMA", {8, 16, 32}},
        {"NDIMB", {8, 16, 32}}, {"KWI", {2}},           {"VWM", {1, 2, 4}},
        {"VWN", {1, 2, 4}},     {"STRM", {0}},          {"STRN", {0}},
        {"SA", {0, 1}},         {"SB", {0, 1}},
    };
  }

  // This needs to be at minimum the maximum (MNK/WG) values above.
  auto m_max = std::max(64, m);
  auto n_max = std::max(64, n);
  auto k_max = std::max(32, k);

  auto at_size =
      batch_size * next_power_of_two(k_max) * next_power_of_two(m_max);
  auto b_size =
      batch_size * next_power_of_two(k_max) * next_power_of_two(n_max);
  auto c_size =
      batch_size * next_power_of_two(m_max) * next_power_of_two(n_max);

  auto total_flops = batch_size * 2.0 * m * n * k;

  auto at = std::vector<float>(at_size);
  auto b = std::vector<float>(b_size);
  auto c = std::vector<float>(c_size);
  auto c_ref = std::vector<float>(c_size);

  sgemm_generate_data(at, k, m, batch_size, k, m);
  sgemm_generate_data(b, n, k, batch_size, n, k);

  sgemmBatched_ref(at, b, c_ref, m, n, k, batch_size);

  auto aBuffer = cl::Buffer(m_context, CL_MEM_READ_WRITE,
                            sizeof(float) * at_size, nullptr, nullptr);
  auto bBuffer = cl::Buffer(m_context, CL_MEM_READ_WRITE,
                            sizeof(float) * b_size, nullptr, nullptr);
  auto cBuffer = cl::Buffer(m_context, CL_MEM_READ_WRITE,
                            sizeof(float) * c_size, nullptr, nullptr);

  CERR << "Started OpenCL SGEMM tuner with batch size " << n / WINOGRAD_P
       << ".";

  auto valid_params = std::vector<int>{};
  auto cfgs = 1;
  for (auto c = size_t{0}; c < opts.size(); c++) {
    cfgs *= opts[c].second.size();
  }

  for (auto i = 0; i < cfgs; i++) {
    TuneParameters param = get_parameters_by_int(opts, i);
    if (valid_config_sgemm(param, m_params.tune_exhaustive)) {
      if (m_params.tune_exhaustive) {
        //                if (rng.RandInt<std::uint16_t>(16) != 0) {
        //                    continue;
        //               }
      }
      valid_params.emplace_back(i);
    }
  }

  CERR << "Will try " << valid_params.size() << " valid configurations.";

  std::string best_params;
  auto best_time = unsigned{0};

  auto queue = cl::CommandQueue(m_context, m_device, CL_QUEUE_PROFILING_ENABLE);
  auto event = cl::Event();
  auto program = cl::Program(m_context, sourceCode_sgemm);

  auto m_ceil_prev = 0;
  auto n_ceil_prev = 0;
  auto k_ceil_prev = 0;
  auto param_counter = size_t{0};

  for (const auto& i : valid_params) {
    param_counter++;

    auto p = get_parameters_by_int(opts, i);
    auto defines = parameters_to_defines(p);

    try {
      auto args = m_opencl.m_cl_args + " " + defines;
      program.build(args.c_str());
    } catch (const cl::Error&) {
      // Failed to compile, get next parameter.
      continue;
    }

    auto sgemm_kernel = cl::Kernel(program, "XgemmBatched");

    auto m_ceil = (int)ceilMultiple(ceilMultiple(m, p["MWG"]), p["VWM"]);
    auto n_ceil = (int)ceilMultiple(ceilMultiple(n, p["NWG"]), p["VWN"]);
    auto k_ceil = (int)ceilMultiple(ceilMultiple(k, p["KWG"]), p["VWM"]);

    if (m_ceil != m_ceil_prev || n_ceil != n_ceil_prev ||
        k_ceil != k_ceil_prev) {
      m_ceil_prev = m_ceil;
      n_ceil_prev = n_ceil;
      k_ceil_prev = k_ceil;

      sgemm_generate_data(at, k, m, batch_size, k_ceil, m_ceil);
      sgemm_generate_data(b, n, k, batch_size, n_ceil, k_ceil);

      queue.enqueueWriteBuffer(aBuffer, CL_FALSE, 0, at_size * sizeof(float),
                               at.data());
      queue.enqueueWriteBuffer(bBuffer, CL_FALSE, 0, b_size * sizeof(float),
                               b.data());
      queue.finish();
    }

    sgemm_kernel.setArg(0, m_ceil);
    sgemm_kernel.setArg(1, n_ceil);
    sgemm_kernel.setArg(2, k_ceil);
    sgemm_kernel.setArg(3, aBuffer);
    sgemm_kernel.setArg(4, bBuffer);
    sgemm_kernel.setArg(5, cBuffer);

    cl::NDRange local_sgemm = {p["MDIMC"], p["NDIMC"], 1};

    cl::NDRange size_sgemm = {(m_ceil * p["MDIMC"]) / p["MWG"],
                              (n_ceil * p["NDIMC"]) / p["NWG"],
                              (size_t)batch_size};

    auto sum = 0.0f;
    auto max_error = 0.0f;
    for (auto r = 0; r < runs; r++) {
      try {
        queue.enqueueNDRangeKernel(sgemm_kernel, cl::NullRange, size_sgemm,
                                   local_sgemm, nullptr, &event);
        queue.finish();
        event.wait();

        queue.enqueueReadBuffer(cBuffer, CL_FALSE, 0, c_size * sizeof(float),
                                c.data());
        queue.finish();

        auto this_error =
            compare_ref(c, c_ref, n, m, batch_size, n_ceil, m_ceil);
        max_error = std::max(max_error, this_error);

        auto elapsed = event.getProfilingInfo<CL_PROFILING_COMMAND_END>() -
                       event.getProfilingInfo<CL_PROFILING_COMMAND_START>();

        sum += elapsed;
      } catch (const cl::Error&) {
        // Failed to enqueue kernel. Set error to max.
        max_error = MAX_ERROR;
        break;
      }
    }
    if (max_error < MAX_ERROR && (best_time == 0 || sum < best_time)) {
      auto param_str = parameters_to_string(p);
      auto kernel_us = 1e-3f * (sum / runs);
      // Timing is in nanoseconds (10^-9), Giga = 10^9, so this works out.
      auto kernel_gflops = total_flops / (sum / runs);
      CERR << std::fixed << std::setprecision(1) << "(" << param_counter << "/"
           << valid_params.size() << ") " << param_str << " " << kernel_us
           << " us (" << kernel_gflops << " GFLOPS)";

      best_time = sum;
      best_params = defines;
    }
  }
  if (best_time == 0) {
    CERR << "Failed to find a working configuration." << std::endl
         << "Check your OpenCL drivers.";
    throw std::runtime_error("Tuner failed to find working configuration.");
  }
  return best_params;
}

void Tuner::store_sgemm_tuners(const int m, const int n, const int k,
                               const int batch_size, std::string tuners) {
  auto file_contents = std::vector<std::string>();
  {
    // Read the previous contents to string.
    auto file = std::ifstream{m_params.tuner_file};
    if (file.good()) {
      auto line = std::string{};
      while (std::getline(file, line)) {
        file_contents.emplace_back(line);
      }
    }
  }
  auto file = std::ofstream{m_params.tuner_file};

  auto device_name = m_opencl.get_device_name();
  auto tuning_params = std::stringstream{};
  tuning_params << m << ";" << n << ";" << k << ";" << batch_size;

  auto tuning_line_prefix = std::to_string(TUNER_VERSION) + ";XgemmBatched;" +
                            tuning_params.str() + ";";
  auto tuning_line = tuning_line_prefix + tuners + ";" + device_name;

  // Write back previous data as long as it's not the device and
  // tuning we just tuned.
  for (const auto& line : file_contents) {
    if (line.find(tuning_line_prefix) == std::string::npos ||
        line.find(device_name) == std::string::npos) {
      file << line << std::endl;
    }
  }

  // Write new tuning.
  file << tuning_line << std::endl;

  if (file.fail()) {
    CERR << "Could not save the tuning result.";
    CERR << "Do I have write permissions on " << m_params.tuner_file << "?";
  }
}

std::string Tuner::sgemm_tuners_from_line(std::string line, const int m,
                                          const int n, const int k,
                                          const int batch_size) {
  auto s = std::vector<std::string>{};
  auto ss = std::stringstream{line};
  auto item = std::string{};

  while (std::getline(ss, item, ';')) {
    s.emplace_back(item);
  }

  if (s.size() != 8) {
    return "";
  }

  if (s[0] != std::to_string(TUNER_VERSION)) {
    return "";
  }

  if (s[1] != "XgemmBatched") {
    return "";
  }

  if (s[2] != std::to_string(m)) {
    return "";
  }

  if (s[3] != std::to_string(n)) {
    return "";
  }

  if (s[4] != std::to_string(k)) {
    return "";
  }

  if (s[5] != std::to_string(batch_size)) {
    return "";
  }

  if (s[7] != m_opencl.get_device_name()) {
    return "";
  }

  return s[6];
}

std::string Tuner::load_sgemm_tuners(const int m, const int n, const int k,
                                     const int batch_size) {
  if (!m_params.force_tune) {
    auto file = std::ifstream{m_params.tuner_file};
    if (file.good()) {
      auto line = std::string{};
      while (std::getline(file, line)) {
        auto tuners = sgemm_tuners_from_line(line, m, n, k, batch_size);
        if (tuners.size() != 0) {
          // batch_size argument is the number of batched sgemm calls, which
          // equals the number of elements in one tile.
          // Convolution batch size affects the "n" dimension of
          // the matrix multiplication (n = WINOGRAD_P * batch_size).
          CERR << "Loaded existing SGEMM tuning for batch size "
               << n / WINOGRAD_P << ".";
          return tuners;
        }
      }
    }
  }

  auto tuners = tune_sgemm(m, n, k, batch_size);
  store_sgemm_tuners(m, n, k, batch_size, tuners);

  // Exit immediately after tuning. Some NVIDIA drivers are buggy,
  // and will fail to compile the rest of the kernels after a tuning,
  // run. See #729.
  if (m_params.tune_only) {
    exit(EXIT_SUCCESS);
  }
  return tuners;
}

```

`src/neural/opencl/OpenCLTuner.h`:

```h
/*
  Originally from the Leela Zero project.
  Copyright (C) 2017 Gian-Carlo Pascutto

  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
*/

#pragma once

#include <map>
#include <string>
#include <vector>

#include "OpenCLParams.h"
#include "neural/opencl/OpenCL.h"
#include "neural/opencl/OpenCLParams.h"

using Configurations = std::pair<std::string, std::vector<size_t>>;
using TuneParameters = std::map<std::string, size_t>;

class OpenCL;

class Tuner {
  OpenCL& m_opencl;
  const OpenCLParams& m_params;
  cl::Context m_context;
  cl::Device m_device;

 public:
  std::string tune_sgemm(const int m, const int n, const int k,
                         const int batch_size, const int runs = 4);
  std::string load_sgemm_tuners(const int m, const int n, const int k,
                                const int batch_size);

  static constexpr auto TUNER_VERSION = 0;
  Tuner(OpenCL& opencl, const OpenCLParams& params, cl::Context context,
        cl::Device device)
      : m_opencl(opencl),
        m_params(params),
        m_context(context),
        m_device(device) {}

 private:
  void store_sgemm_tuners(const int m, const int n, const int k,
                          const int batch_size, std::string tuners);
  bool valid_config_sgemm(TuneParameters p, bool exhaustive);
  std::string parameters_to_defines(const TuneParameters& p);
  std::string parameters_to_string(const TuneParameters& p);
  TuneParameters get_parameters_by_int(const std::vector<Configurations>& opts,
                                       const int n);
  std::string sgemm_tuners_from_line(std::string line, const int m, const int n,
                                     const int k, const int batch_size);
};

```

`src/neural/opencl/README.md`:

```md
The files in this directory comprise the OpenCL backend of Lc0.

## License

Leela Chess is free software: you can redistribute it and/or modify
it under the terms of the GNU General Public License as published by
the Free Software Foundation, either version 3 of the License, or
(at your option) any later version.

Leela Chess is distributed in the hope that it will be useful,
but WITHOUT ANY WARRANTY; without even the implied warranty of
MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
GNU General Public License for more details.

You should have received a copy of the GNU General Public License
along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

**The source files of this directory are not covered by any additional
permission.**



```

`src/neural/opencl/clblast_level3/common.opencl`:

```opencl

// =================================================================================================
// This file is part of the CLBlast project. The project is licensed under Apache Version 2.0. This
// project loosely follows the Google C++ styleguide and uses a tab-size of two spaces and a max-
// width of 100 characters per line.
//
// Author(s):
//   Cedric Nugteren <www.cedricnugteren.nl>
//
// This file contains the common defines and type-defs for the CLBlast OpenCL kernels.
//
// =================================================================================================

// Enables loading of this file using the C++ pre-processor's #include (C++11 standard raw string
// literal). Comment-out this line for syntax-highlighting when developing.
R"(
// =================================================================================================

#define ROUTINE_GEMMBATCHED

// Parameters set by the tuner or by the database. Here they are given a basic default value in case
// this file is used outside of the CLBlast library.
#ifndef PRECISION
  #define PRECISION 32      // Data-types: half, single or double precision, complex or regular
#endif

// =================================================================================================
#ifndef CUDA
  // Enable support for double-precision
  #if PRECISION == 16
    #pragma OPENCL EXTENSION cl_khr_fp16: enable
  #endif
#endif

// Half-precision
#if PRECISION == 16
  typedef half real;
  typedef half2 real2;
  typedef half4 real4;
  typedef half8 real8;
  typedef half16 real16;
  #define ZERO 0
  #define ONE 1
  #define SMALLEST -1.0e14

// Single-precision
#elif PRECISION == 32
  typedef float real;
  typedef float2 real2;
  typedef float4 real4;
  typedef float8 real8;
  typedef float16 real16;
  #define ZERO 0.0f
  #define ONE 1.0f
  #define SMALLEST -1.0e37f
#endif

// Single-element version of a complex number
  typedef real singlereal;

// Converts a 'real argument' value to a 'real' value as passed to the kernel. Normally there is no
// conversion, but half-precision is not supported as kernel argument so it is converted from float.
#if PRECISION == 16
  typedef float real_arg;
  #define GetRealArg(x) (half)x
#else
  typedef real real_arg;
  #define GetRealArg(x) x
#endif

// Pointers to local memory objects (using a define because CUDA doesn't need them)
#ifndef LOCAL_PTR
  #define LOCAL_PTR __local
#endif

// =================================================================================================

// Don't use the non-IEEE754 compliant OpenCL built-in mad() instruction per default. For specific
// devices, this is enabled (see src/routine.cpp).
#ifndef USE_CL_MAD
  #define USE_CL_MAD 0
#endif

// Sets a variable to zero
#define SetToZero(a) a = ZERO

// Sets a variable to zero (only the imaginary part)
#define ImagToZero(a)

// Sets a variable to one
#define SetToOne(a) a = ONE

// Determines whether a variable is zero
#define IsZero(a) (a == ZERO)

// The absolute value (component-wise)
#define AbsoluteValue(value) value = fabs(value)

// Negation (component-wise)
#define Negate(value) value = -(value)

// Adds two complex variables
#define Add(c,a,b) c = a + b

// Subtracts two complex variables
#define Subtract(c,a,b) c = a - b

// The scalar multiply function
#define Multiply(c,a,b) c = a * b

// The scalar multiply-add function
#if USE_CL_MAD == 1
  #define MultiplyAdd(c,a,b) c = mad(a, b, c)
#else
  #define MultiplyAdd(c,a,b) c += a * b
#endif

// The scalar multiply-subtract function
#define MultiplySubtract(c,a,b) c -= a * b

// The scalar division function: full division
#define DivideFull(c,a,b) c = a / b

// The scalar AXPBY function
#define AXPBY(e,a,b,c,d) e = a*b + c*d

// The complex conjugate operation for complex transforms
#define COMPLEX_CONJUGATE(value)

// =================================================================================================

// Force inlining functions or not: some compilers don't support the inline keyword
#ifdef USE_INLINE_KEYWORD
  #define INLINE_FUNC inline
#else
  #define INLINE_FUNC
#endif

// =================================================================================================

// Shuffled workgroup indices to avoid partition camping, see below. For specific devices, this is
// enabled (see src/routine.cc).
#ifndef USE_STAGGERED_INDICES
  #define USE_STAGGERED_INDICES 0
#endif

// Staggered/shuffled group indices to avoid partition camping (AMD GPUs). Formula's are taken from:
// http://docs.nvidia.com/cuda/samples/6_Advanced/transpose/doc/MatrixTranspose.pdf
// More details: https://github.com/CNugteren/CLBlast/issues/53
#if USE_STAGGERED_INDICES == 1
  INLINE_FUNC int GetGroupIDFlat() {
    return get_group_id(0) + get_num_groups(0) * get_group_id(1);
  }
  INLINE_FUNC int GetGroupID1() {
    return (GetGroupIDFlat()) % get_num_groups(1);
  }
  INLINE_FUNC int GetGroupID0() {
    return ((GetGroupIDFlat() / get_num_groups(1)) + GetGroupID1()) % get_num_groups(0);
  }
#else
  INLINE_FUNC int GetGroupID1() { return get_group_id(1); }
  INLINE_FUNC int GetGroupID0() { return get_group_id(0); }
#endif

// =================================================================================================

// End of the C++11 raw string literal
)"

// =================================================================================================

```

`src/neural/opencl/clblast_level3/xgemm_batched.opencl`:

```opencl

// =================================================================================================
// This file is part of the CLBlast project. The project is licensed under Apache Version 2.0. This
// project loosely follows the Google C++ styleguide and uses a tab-size of two spaces and a max-
// width of 100 characters per line.
//
// Author(s):
//   Cedric Nugteren <www.cedricnugteren.nl>
//
// This file contains the batched version of the non-direct GEMM kernel. See part 1 for information
// about the non-batched version of the kernel.
//
// =================================================================================================

// Enables loading of this file using the C++ pre-processor's #include (C++11 standard raw string
// literal). Comment-out this line for syntax-highlighting when developing.
R"(

// =================================================================================================

// Main entry point of the kernel. This is the regular full version.
__kernel __attribute__((reqd_work_group_size(MDIMC, NDIMC, 1)))
void XgemmBatched(const int kSizeM, const int kSizeN, const int kSizeK,
                  const __global realM* restrict agm,
                  const __global realN* restrict bgm,
                  __global realM* restrict cgm) {
  const int batch = get_group_id(2);

  // Sets the offsets
  const int a_offset = kSizeM*kSizeK*batch;
  const int b_offset = kSizeK*kSizeN*batch;
  const int c_offset = kSizeM*kSizeN*batch;
  const __global realM* restrict agm_ = &agm[a_offset / VWM];
  const __global realN* restrict bgm_ = &bgm[b_offset / VWN];
  __global realM* restrict cgm_ = &cgm[c_offset / VWM];

  // Allocates workgroup-private memory (local memory)
  #if SA == 1
    __local realM alm[KWG * MWG/VWM];
  #endif
  #if SB == 1
    __local realN blm[KWG * NWG/VWN];
  #endif

  // Computes the matrix-multiplication and stores the result in global memory
  #if SA == 1 && SB == 1
    XgemmBody(kSizeM, kSizeN, kSizeK, agm_, bgm_, cgm_, alm, blm);
  #elif SA == 1
    XgemmBody(kSizeM, kSizeN, kSizeK, agm_, bgm_, cgm_, alm);
  #elif SB == 1
    XgemmBody(kSizeM, kSizeN, kSizeK, agm_, bgm_, cgm_, blm);
  #else
    XgemmBody(kSizeM, kSizeN, kSizeK, agm_, bgm_, cgm_);
  #endif
}

// =================================================================================================

// End of the C++11 raw string literal
)"

// =================================================================================================

```

`src/neural/opencl/clblast_level3/xgemm_part1.opencl`:

```opencl

// =================================================================================================
// This file is part of the CLBlast project. The project is licensed under Apache Version 2.0. This
// project loosely follows the Google C++ styleguide and uses a tab-size of two spaces and a max-
// width of 100 characters per line.
//
// Author(s):
//   Cedric Nugteren <www.cedricnugteren.nl>
//
// This file contains an optimized matrix-multiplication kernel inspired by the paper by Matsumoto
// et al. and the tutorial on http://www.cedricnugteren.nl/tutorial.php. It is fully configurable
// (and tunable!) using more or less the same parameters/naming conventions as in the paper. It
// supports different data-types (SGEMM/DGEMM/CGEMM/ZGEMM/HGEMM) through a pre-processor define.
//
// Matrices are accessed as follows:
// A: [k*M + m], with 'k' ranging from 0:K and 'm' from 0:M (m,k,m)
// B: [k*N + n], with 'k' ranging from 0:K and 'n' from 0:N (n,k,n)
// C: [n*M + m], with 'n' ranging from 0:N and 'm' from 0:M (m,n,m)
//
// Or as an image (assuming column-major)
//       K                      
//    o-------o                 
//    |       |                 
//  N | [B^T] |                 
//    |       |                 
//    o-------o                 
//        K               N     
//    o-------o        o-----o  
//  M |  [A]  |      M | [C] |  
//    |       |        |     |  
//    o-------o        o-----o  
//                              
//
// This kernel is separated into three files. This is part 1 out of 4.
//
// =================================================================================================

// Enables loading of this file using the C++ pre-processor's #include (C++11 standard raw string
// literal). Comment-out this line for syntax-highlighting when developing.
R"(

// =================================================================================================

// Parameters set by the tuner or by the database. Here they are given a basic default value in case
// this kernel file is used outside of the CLBlast library.
#ifndef MWG
  #define MWG 8      // Tile-size in dimension M (e.g. 64, 128)
#endif
#ifndef NWG
  #define NWG 8      // Tile-size in dimension N (e.g. 64, 128)
#endif
#ifndef KWG
  #define KWG 8      // Tile-size in dimension K (e.g. 8, 16)
#endif
#ifndef MDIMC
  #define MDIMC 8    // Threads per workgroup in M-dimension (e.g. 8, 16, 32)
#endif
#ifndef NDIMC
  #define NDIMC 8    // Threads per workgroup in N-dimension (e.g. 8, 16, 32)
#endif
#ifndef MDIMA
  #define MDIMA 8    // Re-shaped tile dimension of matrix A: KDIMA * MDIMA
#endif
#ifndef NDIMB
  #define NDIMB 8    // Re-shaped tile dimension of matrix B: KDIMB * NDIMB
#endif
#ifndef KWI
  #define KWI 1      // Unroll factor of the KWG loop (smaller or equal than KWG)
#endif
#ifndef VWM
  #define VWM 1      // Vector width of matrices A and C
#endif
#ifndef VWN
  #define VWN 1      // Vector width of matrix B
#endif
#ifndef STRM
  #define STRM 0     // Use strided access within a thread in the M-dimension (1) or not (0)
#endif
#ifndef STRN
  #define STRN 0     // Use strided access within a thread in the N-dimension (1) or not (0)
#endif
#ifndef SA
  #define SA 0       // Use local/shared memory to cache matrix A (1) or not (0)
#endif
#ifndef SB
  #define SB 0       // Use local/shared memory to cache matrix B (1) or not (0)
#endif

// Helper parameters based on the above tuning parameters
#define MWI (MWG/MDIMC)               // Work per work-item (M-dimension)
#define NWI (NWG/NDIMC)               // Work per work-item (N-dimension)
#define KDIMA ((MDIMC*NDIMC)/(MDIMA)) // Re-shaped tile dimension of matrix A: KDIMA * MDIMA
#define KDIMB ((MDIMC*NDIMC)/(NDIMB)) // Re-shaped tile dimension of matrix B: KDIMB * NDIMB
#define MWA (MWG/MDIMA)               // Amount of loads-per-thread for matrix A (M-dimension)
#define KWA (KWG/KDIMA)               // Amount of loads-per-thread for matrix A (K-dimension)
#define KWB (KWG/KDIMB)               // Amount of loads-per-thread for matrix B (K-dimension)
#define NWB (NWG/NDIMB)               // Amount of loads-per-thread for matrix B (N-dimension)

// Settings
#ifndef USE_VECTOR_MAD
  #define USE_VECTOR_MAD 0      // Unroll (0) or don't (1) unroll the vector MAD manually
#endif
#ifndef GLOBAL_MEM_FENCE
  #define GLOBAL_MEM_FENCE 0    // Global synchronisation barrier for potential better performance
#endif

// =================================================================================================

// Data-widths in dimension M
#if VWM == 1
    typedef real realM;
#elif VWM == 2
    typedef real2 realM;
#elif VWM == 4
    typedef real4 realM;
#elif VWM == 8
    typedef real8 realM;
#elif VWM == 16
    typedef real16 realM;
#endif

// Data-widths in dimension N
#if VWN == 1
    typedef real realN;
#elif VWN == 2
    typedef real2 realN;
#elif VWN == 4
    typedef real4 realN;
#elif VWN == 8
    typedef real8 realN;
#elif VWN == 16
    typedef real16 realN;
#endif

// =================================================================================================

// Initializes the accumulation registers to zero
INLINE_FUNC realM InitAccRegisters() {
  realM result;
  #if VWM == 1
    SetToZero(result);
  #elif VWM == 2
    SetToZero(result.x);
    SetToZero(result.y);
  #elif VWM == 4
    SetToZero(result.x);
    SetToZero(result.y);
    SetToZero(result.z);
    SetToZero(result.w);
  #elif VWM == 8
    SetToZero(result.s0);
    SetToZero(result.s1);
    SetToZero(result.s2);
    SetToZero(result.s3);
    SetToZero(result.s4);
    SetToZero(result.s5);
    SetToZero(result.s6);
    SetToZero(result.s7);
  #elif VWM == 16
    SetToZero(result.s0);
    SetToZero(result.s1);
    SetToZero(result.s2);
    SetToZero(result.s3);
    SetToZero(result.s4);
    SetToZero(result.s5);
    SetToZero(result.s6);
    SetToZero(result.s7);
    SetToZero(result.s8);
    SetToZero(result.s9);
    SetToZero(result.sA);
    SetToZero(result.sB);
    SetToZero(result.sC);
    SetToZero(result.sD);
    SetToZero(result.sE);
    SetToZero(result.sF);
  #endif
  return result;
}

// =================================================================================================

// Caches global off-chip memory into local (shared) memory on-chip. This function is specific for
// caching the A input matrix.
#if SA == 1
INLINE_FUNC void GlobalToLocalA(const __global realM* restrict agm, LOCAL_PTR realM* alm,
                                const int kSizeM, const int tid, const int kwg) {
  const int la0 = tid % MDIMA;
  const int la1 = tid / MDIMA;
  #pragma unroll
  for (int _mia = 0; _mia < MWA/VWM; _mia += 1) {
    #pragma unroll
    for (int _kia = 0; _kia < KWA; _kia += 1) {

      // Computes the indices based on strided/non-strided access
      #if STRM == 0
        int mg = _mia + la0*(MWA/VWM);
      #elif STRM == 1
        int mg = la0 + _mia*MDIMA;
      #endif

      // Computes the indices for the global memory
      int kg = _kia + la1*KWA;
      int idm = mg + GetGroupID0() * (MWG/VWM);
      int idk = kg + kwg;

      // Loads the data from global memory (not transposed) into the local memory
      alm[kg*(MWG/VWM) + mg] = agm[idk*(kSizeM/VWM) + idm];
    }
  }
}
#endif

// Same as above, but now for the B input matrix
#if SB == 1
INLINE_FUNC void GlobalToLocalB(const __global realN* restrict bgm, LOCAL_PTR realN* blm,
                                const int kSizeN, const int tid, const int kwg) {
  const int lb0 = tid % NDIMB;
  const int lb1 = tid / NDIMB;
  #pragma unroll
  for (int _kib = 0; _kib < KWB; _kib += 1) {
    #pragma unroll
    for (int _nib = 0; _nib < NWB/VWN; _nib += 1) {

      // Computes the indices based on strided/non-strided access
      #if STRN == 0
        int ng = _nib + lb0*(NWB/VWN);
      #elif STRN == 1
        int ng = lb0 + _nib*NDIMB;
      #endif

      // Computes the indices for the global memory
      int kg = _kib + lb1*KWB;
      int idn = ng + GetGroupID1() * (NWG/VWN);
      int idk = kg + kwg;

      // Loads the data from global memory (transposed) into the local memory
      blm[kg*(NWG/VWN) + ng] = bgm[idk*(kSizeN/VWN) + idn];
    }
  }
}
#endif

// =================================================================================================

// Caches global off-chip memory directly into per-thread private memory (registers). This function
// is specific for caching the A input matrix.
#if SA == 0
INLINE_FUNC realM GlobalToPrivateA(const __global realM* restrict agm, const int _mi,
                                   const int kSizeM, const int idk, const int kwg) {
  // Computes the indices based on strided/non-strided access
  #if STRM == 0
    int mg = _mi + get_local_id(0)*(MWI/VWM);
  #elif STRM == 1
    int mg = get_local_id(0) + _mi*MDIMC;
  #endif

  // Computes the indices for the global memory
  int idm = mg + GetGroupID0() * (MWG/VWM);

  // Loads the data from global memory (not transposed) and stores into registers
  return agm[idk*(kSizeM/VWM) + idm];
}
#endif

// Same as above, but now for the B input matrix
#if SB == 0
INLINE_FUNC realN GlobalToPrivateB(const __global realN* restrict bgm, const int _ni,
                                   const int kSizeN, const int idk) {
  // Computes the indices based on strided/non-strided access
  #if STRN == 0
    int ng = _ni + get_local_id(1)*(NWI/VWN);
  #elif STRN == 1
    int ng = get_local_id(1) + _ni*NDIMC;
  #endif

  // Computes the indices for the global memory
  int idn = ng + GetGroupID1() * (NWG/VWN);

  // Loads the data from global memory (transposed) and stores into registers
  return bgm[idk*(kSizeN/VWN) + idn];
}
#endif

// =================================================================================================

// Caches on-chip local memory into per-thread private memory (registers). This function is specific
// for caching the A input matrix.
#if SA == 1
INLINE_FUNC realM LocalToPrivateA(LOCAL_PTR realM* alm, const int _mi, const int kg) {
  #if STRM == 0
    int mg = _mi + get_local_id(0)*(MWI/VWM);
  #elif STRM == 1
    int mg = get_local_id(0) + _mi*MDIMC;
  #endif
  return alm[kg*(MWG/VWM) + mg];
}
#endif

// Same as above, but now for the B input matrix
#if SB == 1
INLINE_FUNC realN LocalToPrivateB(LOCAL_PTR realN* blm, const int _ni, const int kg) {
  #if STRN == 0
    int ng = _ni + get_local_id(1)*(NWI/VWN);
  #elif STRN == 1
    int ng = get_local_id(1) + _ni*NDIMC;
  #endif
  return blm[kg*(NWG/VWN) + ng];
}
#endif

// =================================================================================================

// End of the C++11 raw string literal
)"

// =================================================================================================

```

`src/neural/opencl/clblast_level3/xgemm_part2.opencl`:

```opencl

// =================================================================================================
// This file is part of the CLBlast project. The project is licensed under Apache Version 2.0. This
// project loosely follows the Google C++ styleguide and uses a tab-size of two spaces and a max-
// width of 100 characters per line.
//
// Author(s):
//   Cedric Nugteren <www.cedricnugteren.nl>
//
// This is part 2 of 4 of the GEMM kernel. See part 1 for more information.
//
// =================================================================================================

// Enables loading of this file using the C++ pre-processor's #include (C++11 standard raw string
// literal). Comment-out this line for syntax-highlighting when developing.
R"(

// =================================================================================================

// The vectorised multiply-add function
INLINE_FUNC realM MultiplyAddVector(realM cvec, const realM avec, const real bval) {
  #if USE_VECTOR_MAD == 1
    cvec += avec * bval;
  #else
    #if VWM == 1
      MultiplyAdd(cvec,    avec,    bval);
    #elif VWM == 2
      MultiplyAdd(cvec.x , avec.x,  bval);
      MultiplyAdd(cvec.y , avec.y,  bval);
    #elif VWM == 4
      MultiplyAdd(cvec.x , avec.x,  bval);
      MultiplyAdd(cvec.y , avec.y,  bval);
      MultiplyAdd(cvec.z , avec.z,  bval);
      MultiplyAdd(cvec.w , avec.w,  bval);
    #elif VWM == 8
      MultiplyAdd(cvec.s0, avec.s0, bval);
      MultiplyAdd(cvec.s1, avec.s1, bval);
      MultiplyAdd(cvec.s2, avec.s2, bval);
      MultiplyAdd(cvec.s3, avec.s3, bval);
      MultiplyAdd(cvec.s4, avec.s4, bval);
      MultiplyAdd(cvec.s5, avec.s5, bval);
      MultiplyAdd(cvec.s6, avec.s6, bval);
      MultiplyAdd(cvec.s7, avec.s7, bval);
    #elif VWM == 16
      MultiplyAdd(cvec.s0, avec.s0, bval);
      MultiplyAdd(cvec.s1, avec.s1, bval);
      MultiplyAdd(cvec.s2, avec.s2, bval);
      MultiplyAdd(cvec.s3, avec.s3, bval);
      MultiplyAdd(cvec.s4, avec.s4, bval);
      MultiplyAdd(cvec.s5, avec.s5, bval);
      MultiplyAdd(cvec.s6, avec.s6, bval);
      MultiplyAdd(cvec.s7, avec.s7, bval);
      MultiplyAdd(cvec.s8, avec.s8, bval);
      MultiplyAdd(cvec.s9, avec.s9, bval);
      MultiplyAdd(cvec.sA, avec.sA, bval);
      MultiplyAdd(cvec.sB, avec.sB, bval);
      MultiplyAdd(cvec.sC, avec.sC, bval);
      MultiplyAdd(cvec.sD, avec.sD, bval);
      MultiplyAdd(cvec.sE, avec.sE, bval);
      MultiplyAdd(cvec.sF, avec.sF, bval);
    #endif
  #endif
  return cvec;
}

// =================================================================================================

// Merges the results in Cpm with the global array in Cgm.
INLINE_FUNC void StoreResults(__global realM* cgm, realM cpm[NWI*MWI/VWM], const int kSizeM) {
  #pragma unroll
  for (int _ni = 0; _ni < NWI; _ni += 1) {
    #pragma unroll
    for (int _mi = 0; _mi < MWI/VWM; _mi += 1) {
      #if STRM == 0
        int mg = _mi + get_local_id(0)*(MWI/VWM);
      #elif STRM == 1
        int mg = get_local_id(0) + _mi*MDIMC;
      #endif
      #if STRN == 0
        int ng = _ni + get_local_id(1)*NWI;
      #elif STRN == 1
        int ng = _ni%VWN + get_local_id(1)*VWN + (_ni/VWN)*VWN*NDIMC;
      #endif
      int idm = mg + GetGroupID0() * (MWG/VWM);
      int idn = ng + GetGroupID1() * NWG;
      int index = idn*(kSizeM/VWM) + idm;

      cgm[index] = cpm[_ni * (MWI/VWM) + _mi];

    }
  }
}

// =================================================================================================

// End of the C++11 raw string literal
)"

// =================================================================================================

```

`src/neural/opencl/clblast_level3/xgemm_part3.opencl`:

```opencl

// =================================================================================================
// This file is part of the CLBlast project. The project is licensed under Apache Version 2.0. This
// project loosely follows the Google C++ styleguide and uses a tab-size of two spaces and a max-
// width of 100 characters per line.
//
// Author(s):
//   Cedric Nugteren <www.cedricnugteren.nl>
//
// This is part 3 of 4 of the GEMM kernel. See part 1 for more information.
//
// =================================================================================================

// Enables loading of this file using the C++ pre-processor's #include (C++11 standard raw string
// literal). Comment-out this line for syntax-highlighting when developing.
R"(

// =================================================================================================

// Main body of the matrix-multiplication algorithm. It calls various (inlined) functions.
INLINE_FUNC void XgemmBody(const int kSizeM, const int kSizeN, const int kSizeK,
                           const __global realM* restrict agm, const __global realN* restrict bgm,
                           __global realM* cgm
                           #if SA == 1 && SB == 1
                             , LOCAL_PTR realM* alm, LOCAL_PTR realN* blm
                           #elif SA == 1
                             , LOCAL_PTR realM* alm
                           #elif SB == 1
                             , LOCAL_PTR realN* blm
                           #endif
                           ) {

  // Allocates workitem-private memory (registers)
  #pragma promote_to_registers
  realM apm[MWI/VWM];
  #pragma promote_to_registers
  realN bpm[NWI/VWN];
  #pragma promote_to_registers
  realM cpm[NWI*(MWI/VWM)];

  // Combined thread identifier (volatile to disable caching)
  #if SA == 1 || SB == 1
    volatile int tid = get_local_id(0) + MDIMC*get_local_id(1);
  #endif

  // Initializes the accumulation registers
  #pragma unroll
  for (int _mi = 0; _mi < MWI/VWM; _mi += 1) {
    #pragma unroll
    for (int _ni = 0; _ni < NWI; _ni += 1) {
      cpm[_ni * (MWI/VWM) + _mi] = InitAccRegisters();
    }
  }


  // Loops over all workgroup tiles
  for (int kwg = 0; kwg < kSizeK; kwg += KWG) {

    // Loads data: off-chip --> local (matrix A)
    #if SA == 1
      GlobalToLocalA(agm, alm, kSizeM, tid, kwg);
    #endif
    // Loads data: off-chip --> local (matrix B)
    #if SB == 1
      GlobalToLocalB(bgm, blm, kSizeN, tid, kwg);
    #endif
    #if SA == 1 || SB == 1
      barrier(CLK_LOCAL_MEM_FENCE);
    #endif

    // Loops over all workitem tiles, unrolled by a factor KWI
    for (int pwi = 0; pwi < KWG; pwi += KWI) {
      #pragma unroll
      for (int _pit = 0; _pit < KWI; _pit += 1) {
        #if SA == 0 || SB == 0
          int idk = kwg + pwi + _pit;
        #endif
        #if SA == 1 || SB == 1
          int kg = pwi + _pit;
        #endif

        #pragma unroll
        for (int _mi = 0; _mi < MWI/VWM; _mi += 1) {
          // Loads data: local --> private (matrix A)
          #if SA == 1
            apm[_mi] = LocalToPrivateA(alm, _mi, kg);
          // Loads data: off-chip --> private (matrix A)
          #else
            apm[_mi] = GlobalToPrivateA(agm, _mi, kSizeM, idk, kwg);
          #endif
        }

        // Loads data: local --> private (matrix B)
        #pragma unroll
        for (int _ni = 0; _ni < NWI/VWN; _ni += 1) {
          #if SB == 1
            bpm[_ni] = LocalToPrivateB(blm, _ni, kg);
          // Loads data: off-chip --> private (matrix B)
          #else
            bpm[_ni] = GlobalToPrivateB(bgm, _ni, kSizeN, idk);
          #endif
        }

        // Performs the accumulation (Cpm += Apm * Bpm)
        #pragma unroll
        for (int _ni = 0; _ni < NWI/VWN; _ni += 1) {
          #pragma unroll
          for (int _mi = 0; _mi < MWI/VWM; _mi += 1) {
            const realM aval = apm[_mi];
            #if VWN == 1
              cpm[(_ni*VWN + 0)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 0)*(MWI/VWM) + _mi], aval, bpm[_ni]);
            #elif VWN == 2
              cpm[(_ni*VWN + 0)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 0)*(MWI/VWM) + _mi], aval, bpm[_ni].x);
              cpm[(_ni*VWN + 1)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 1)*(MWI/VWM) + _mi], aval, bpm[_ni].y);
            #elif VWN == 4
              cpm[(_ni*VWN + 0)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 0)*(MWI/VWM) + _mi], aval, bpm[_ni].x);
              cpm[(_ni*VWN + 1)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 1)*(MWI/VWM) + _mi], aval, bpm[_ni].y);
              cpm[(_ni*VWN + 2)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 2)*(MWI/VWM) + _mi], aval, bpm[_ni].z);
              cpm[(_ni*VWN + 3)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 3)*(MWI/VWM) + _mi], aval, bpm[_ni].w);
            #elif VWN == 8
              cpm[(_ni*VWN + 0)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 0)*(MWI/VWM) + _mi], aval, bpm[_ni].s0);
              cpm[(_ni*VWN + 1)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 1)*(MWI/VWM) + _mi], aval, bpm[_ni].s1);
              cpm[(_ni*VWN + 2)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 2)*(MWI/VWM) + _mi], aval, bpm[_ni].s2);
              cpm[(_ni*VWN + 3)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 3)*(MWI/VWM) + _mi], aval, bpm[_ni].s3);
              cpm[(_ni*VWN + 4)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 4)*(MWI/VWM) + _mi], aval, bpm[_ni].s4);
              cpm[(_ni*VWN + 5)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 5)*(MWI/VWM) + _mi], aval, bpm[_ni].s5);
              cpm[(_ni*VWN + 6)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 6)*(MWI/VWM) + _mi], aval, bpm[_ni].s6);
              cpm[(_ni*VWN + 7)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 7)*(MWI/VWM) + _mi], aval, bpm[_ni].s7);
            #elif VWN == 16
              cpm[(_ni*VWN + 0 )*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 0 )*(MWI/VWM) + _mi], aval, bpm[_ni].s0);
              cpm[(_ni*VWN + 1 )*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 1 )*(MWI/VWM) + _mi], aval, bpm[_ni].s1);
              cpm[(_ni*VWN + 2 )*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 2 )*(MWI/VWM) + _mi], aval, bpm[_ni].s2);
              cpm[(_ni*VWN + 3 )*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 3 )*(MWI/VWM) + _mi], aval, bpm[_ni].s3);
              cpm[(_ni*VWN + 4 )*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 4 )*(MWI/VWM) + _mi], aval, bpm[_ni].s4);
              cpm[(_ni*VWN + 5 )*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 5 )*(MWI/VWM) + _mi], aval, bpm[_ni].s5);
              cpm[(_ni*VWN + 6 )*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 6 )*(MWI/VWM) + _mi], aval, bpm[_ni].s6);
              cpm[(_ni*VWN + 7 )*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 7 )*(MWI/VWM) + _mi], aval, bpm[_ni].s7);
              cpm[(_ni*VWN + 8 )*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 8 )*(MWI/VWM) + _mi], aval, bpm[_ni].s8);
              cpm[(_ni*VWN + 9 )*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 9 )*(MWI/VWM) + _mi], aval, bpm[_ni].s9);
              cpm[(_ni*VWN + 10)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 10)*(MWI/VWM) + _mi], aval, bpm[_ni].sA);
              cpm[(_ni*VWN + 11)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 11)*(MWI/VWM) + _mi], aval, bpm[_ni].sB);
              cpm[(_ni*VWN + 12)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 12)*(MWI/VWM) + _mi], aval, bpm[_ni].sC);
              cpm[(_ni*VWN + 13)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 13)*(MWI/VWM) + _mi], aval, bpm[_ni].sD);
              cpm[(_ni*VWN + 14)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 14)*(MWI/VWM) + _mi], aval, bpm[_ni].sE);
              cpm[(_ni*VWN + 15)*(MWI/VWM) + _mi] = MultiplyAddVector(cpm[(_ni*VWN + 15)*(MWI/VWM) + _mi], aval, bpm[_ni].sF);
            #endif
          }
        }

      }
    }
    #if SA == 1 || SB == 1
      barrier(CLK_LOCAL_MEM_FENCE);
    #endif
  }
  #if GLOBAL_MEM_FENCE == 1
    barrier(CLK_GLOBAL_MEM_FENCE);
  #endif

  // Stores an MWG * NWG tile of results
  StoreResults(cgm, cpm, kSizeM);
}

// =================================================================================================

// End of the C++11 raw string literal
)"

// =================================================================================================

```

`src/neural/opencl/clblast_level3/xgemv.opencl`:

```opencl

// =================================================================================================
// This file is part of the CLBlast project. The project is licensed under Apache Version 2.0. This
// project loosely follows the Google C++ styleguide and uses a tab-size of two spaces and a max-
// width of 100 characters per line.
//
// Author(s):
//   Cedric Nugteren <www.cedricnugteren.nl>
//
// This file contains the Xgemv kernel (generic version) for matrix-vector multiplication.
//
// =================================================================================================

// Enables loading of this file using the C++ pre-processor's #include (C++11 standard raw string
// literal). Comment-out this line for syntax-highlighting when developing.
R"(

// =================================================================================================

// Parameters set by the tuner or by the database. Here they are given a basic default value in case
// this kernel file is used outside of the CLBlast library.

// 1: For the full version of the kernel
#ifndef WGS1
  #define WGS1 64     // The local work-group size
#endif
#ifndef WPT1
  #define WPT1 1      // The amount of work-per-thread
#endif
#ifndef UNROLL1
  #define UNROLL1 32  // Unroll factor (must be a divider of WGS1)
#endif

// 2 and 3: For the fast versions, see 'xgemv_fast.opencl'

// =================================================================================================

// Defines how to load the input matrix in the non-vectorized case
INLINE_FUNC real LoadMatrixA(const __global real* restrict agm, const int x, const int y,
                             const int a_ld, const int a_offset) {

  return agm[a_ld*y + x + a_offset];
}

// =================================================================================================

// Full version of the kernel
__kernel __attribute__((reqd_work_group_size(WGS1, 1, 1)))
void Xgemv(const int m, const int n,
                    const __global real* restrict agm, const int a_offset, const int a_ld,
                    const __global real* restrict x, const int x_offset,
                    __global real* y, const int y_offset,
                    __global real* bias, const int relu) {

  const int batch = get_global_id(1);
  __global real* xgm=x + batch*n;
  __global real* ygm=y + batch*m;

  // Local memory for the vector X
  __local real xlm[WGS1];

  // Initializes the accumulation register
  #pragma promote_to_registers
  real acc1[WPT1];
  #pragma unroll
  for (int _w = 0; _w < WPT1; _w += 1) {
    SetToZero(acc1[_w]);
  }

  // Divides the work in a main and tail section
  const int n_tail = n % WGS1;
  const int n_floor = n - n_tail;

  // Loops over work-group sized portions of the work
  for (int kwg=0; kwg<n_floor; kwg+=WGS1) {

    // Loads the vector X into local memory
    const int lid = get_local_id(0);
    xlm[lid] = xgm[(kwg + lid) + x_offset];

    // Synchronizes all threads in a workgroup
    barrier(CLK_LOCAL_MEM_FENCE);

    // Loops over the work per thread, and checks whether in bounds
    #pragma unroll
    for (int _w = 0; _w < WPT1; _w += 1) {
      const int gid = _w*get_global_size(0) + get_global_id(0);
      if (gid < m) {

        // The multiply-add function for the main part (divisable by WGS1)
	    for (int kloop=0; kloop<WGS1; kloop+=UNROLL1) {
		  #pragma unroll
		  for (int _kunroll = 0; _kunroll < UNROLL1; _kunroll += 1) {
		    const int k = kwg + kloop + _kunroll;
		    real value = LoadMatrixA(agm, k, gid, a_ld, a_offset);
		    MultiplyAdd(acc1[_w], xlm[kloop + _kunroll], value);
		  }
	    }
      }
    }

    // Synchronizes all threads in a workgroup
    barrier(CLK_LOCAL_MEM_FENCE);
  }

  // Loops over the work per thread, and checks whether in bounds
  #pragma unroll
  for (int _w = 0; _w < WPT1; _w += 1) {
    const int gid = _w*get_global_size(0) + get_global_id(0);
    if (gid < m) {

      // The multiply-add function for the remainder part (not divisable by WGS1)
      for (int k=n_floor; k<n; ++k) {
        real value = LoadMatrixA(agm, k, gid, a_ld, a_offset);
        MultiplyAdd(acc1[_w], xgm[k + x_offset], value);
      }

      // Stores the final result
	  real out = acc1[_w] + bias[gid];
	  if (relu) {
	    out = out > 0.0f ? out : 0.0f;
	  }
      ygm[gid + y_offset] = out;
    }
  }
}

// =================================================================================================

// End of the C++11 raw string literal
)"

// =================================================================================================

```

`src/neural/opencl/clsource/config.opencl`:

```opencl
// Enables loading of this file using the C++ pre-processor's #include (C++11 standard raw string
// literal). Comment-out this line for syntax-highlighting when developing.
R"(

typedef float net_t;
#define vload_net_t(offset,p) ((p)[(offset)])
#define vstore_net_t(data,offset,p) (((p)[(offset)])=(data))

#define BOARD_SIZE 8
#define BOARD_SQUARES (BOARD_SIZE*BOARD_SIZE)

// End of the C++11 raw string literal
)"

```

`src/neural/opencl/clsource/convolve1.opencl`:

```opencl
// Enables loading of this file using the C++ pre-processor's #include (C++11 standard raw string
// literal). Comment-out this line for syntax-highlighting when developing.
R"(

__kernel
__attribute__((work_group_size_hint(8, 16, 1)))
void convolve1(
               __global const net_t * restrict in,
               __global net_t * restrict merge,
               __global const net_t * restrict weights,
               __local float * channel_buff,
               __local float * row_buff) {
  // cl::NDRange global(channels, outputs, row);
  const int c   = get_global_id(0);  // channel
  const int o   = get_global_id(1);  // output
  const int row_batch = get_global_id(2);  // row * batch_size

  const int width = 8;
  const int height = 8;
  const int boardsize = width * height;

  const int row = row_batch % 8;
  const int batch = row_batch / 8;

  const int channels = get_global_size(0);
  const int outputs  = get_global_size(1);

  const int input_offset = batch * boardsize * channels;
  const int merge_offset = batch * boardsize * (channels >> 3) * outputs;

  // cl::NDRange local(2, (1->32), 1);
  const int lx = get_local_id(0);
  const int ly = get_local_id(1);
  const int chan_buff_size = 8;
  const int out_buff_size  = get_local_size(1);
  const int row_buff_size  = 7;
  const int chan_shift     = 3;
  // input = channels * height * width
  // output = outputs * height * width
  // weights = output * channels * filter
  // merge = channels * outputs * height * width
  const int strip_size = width;
  // Copy the input channels (strips) locally
  if (out_buff_size < 8 && ly == 0) {
    // strip-row
    for (int w = 0; w < width; w++) {
      channel_buff[lx * width + w] =
      vload_net_t((c * height + row) * width + w + input_offset, in);
    }
  } else if (out_buff_size >= 8 && ly < 8) {
    // Every thread copies a column
    channel_buff[lx * width + ly] = vload_net_t((c * height + row) * width +
	    ly + input_offset, in);
  }
  // Copy the filter we are applying locally
  __private float filter_buff = vload_net_t((o * channels + c), weights);
  barrier(CLK_LOCAL_MEM_FENCE);
  int out_lane = 0;
  int out_cw   = 0;
#pragma unroll
  for (int cw = 0; cw < width; cw++) {
    int fid = lx * strip_size;
    float out  = channel_buff[fid + cw] * filter_buff;
    row_buff[(ly * chan_buff_size + lx) * row_buff_size + out_lane] = out;
    out_lane++;
    // Row buffer full or last lane?
    if (out_lane == row_buff_size || (cw == width - 1)) {
      barrier(CLK_LOCAL_MEM_FENCE);
      if (lx < out_lane) {
        float val;
        val  = row_buff[(ly * chan_buff_size + 0) * row_buff_size + lx];
        val += row_buff[(ly * chan_buff_size + 1) * row_buff_size + lx];
        val += row_buff[(ly * chan_buff_size + 2) * row_buff_size + lx];
        val += row_buff[(ly * chan_buff_size + 3) * row_buff_size + lx];
        val += row_buff[(ly * chan_buff_size + 4) * row_buff_size + lx];
        val += row_buff[(ly * chan_buff_size + 5) * row_buff_size + lx];
        val += row_buff[(ly * chan_buff_size + 6) * row_buff_size + lx];
        val += row_buff[(ly * chan_buff_size + 7) * row_buff_size + lx];
        vstore_net_t(val, (((c >> chan_shift) * height + row) * width +
		    out_cw + lx) * outputs + o + merge_offset, merge);
      }
      out_cw  += row_buff_size;
      out_lane = 0;
    }
  }
}

__kernel void merge_bn(
                       __global const net_t * restrict in,
                       __global net_t * restrict out,
                       __private const int channels,
                       __constant const net_t * restrict biases) {
  // cl::NDRange global(outputs, 8*8);
  const int gx = get_global_id(0);
  const int gy = get_global_id(1);
  const int batch = get_global_id(2);
  const int output = gx;
  const int b = gy;
  const int outputs = get_global_size(0);
  const int width = 8;
  const int height = 8;
  const int boardsize = width * height;
  const int o = output;
  float sum = 0;
  for (int c = 0; c < channels; c++) {
    sum += vload_net_t(batch * channels * boardsize * outputs +
	    (c * boardsize + b) * outputs + o, in);
  }
  if (biases) {
    const float bias = vload_net_t(o, biases);

    sum = sum + bias;
    sum = sum > 0 ? sum : 0.0f;
  }
  vstore_net_t(sum, batch * outputs * boardsize + o * boardsize + b, out);
}

// End of the C++11 raw string literal
)"

```

`src/neural/opencl/clsource/convolve3.opencl`:

```opencl
// Enables loading of this file using the C++ pre-processor's #include (C++11 standard raw string
// literal). Comment-out this line for syntax-highlighting when developing.
R"(

void __in_transform_eq(float x[4][4], __global float * restrict V, int offset, int CPpad) {
  float T1[4][4];
  
  T1[0][0] = x[0][0] - x[2][0];
  T1[0][1] = x[0][1] - x[2][1];
  T1[0][2] = x[0][2] - x[2][2];
  T1[0][3] = x[0][3] - x[2][3];
  T1[1][0] = x[1][0] + x[2][0];
  T1[1][1] = x[1][1] + x[2][1];
  T1[1][2] = x[1][2] + x[2][2];
  T1[1][3] = x[1][3] + x[2][3];
  T1[2][0] = x[2][0] - x[1][0];
  T1[2][1] = x[2][1] - x[1][1];
  T1[2][2] = x[2][2] - x[1][2];
  T1[2][3] = x[2][3] - x[1][3];
  T1[3][0] = x[1][0] - x[3][0];
  T1[3][1] = x[1][1] - x[3][1];
  T1[3][2] = x[1][2] - x[3][2];
  T1[3][3] = x[1][3] - x[3][3];

  // Scatter each sub element in tile to separate matrices
  
  V[(0*4 + 0)*CPpad + offset] = T1[0][0] - T1[0][2];
  V[(0*4 + 1)*CPpad + offset] = T1[0][1] + T1[0][2];
  V[(0*4 + 2)*CPpad + offset] = T1[0][2] - T1[0][1];
  V[(0*4 + 3)*CPpad + offset] = T1[0][1] - T1[0][3];
  V[(1*4 + 0)*CPpad + offset] = T1[1][0] - T1[1][2];
  V[(1*4 + 1)*CPpad + offset] = T1[1][1] + T1[1][2];
  V[(1*4 + 2)*CPpad + offset] = T1[1][2] - T1[1][1];
  V[(1*4 + 3)*CPpad + offset] = T1[1][1] - T1[1][3];
  V[(2*4 + 0)*CPpad + offset] = T1[2][0] - T1[2][2];
  V[(2*4 + 1)*CPpad + offset] = T1[2][1] + T1[2][2];
  V[(2*4 + 2)*CPpad + offset] = T1[2][2] - T1[2][1];
  V[(2*4 + 3)*CPpad + offset] = T1[2][1] - T1[2][3];
  V[(3*4 + 0)*CPpad + offset] = T1[3][0] - T1[3][2];
  V[(3*4 + 1)*CPpad + offset] = T1[3][1] + T1[3][2];
  V[(3*4 + 2)*CPpad + offset] = T1[3][2] - T1[3][1];
  V[(3*4 + 3)*CPpad + offset] = T1[3][1] - T1[3][3];
}

__kernel void in_transform(__global net_t * restrict in, __global float * restrict V,
                           const int C, const int Cpad,
                           const int Ppad) {
  const int width = 8;
  const int height = 8;
  const int boardsize = width * height;
  const int WTILES = (width + 1) / 2;
  const int P = WTILES*WTILES;
  const int CPpad = Ppad * Cpad;
  
  const int block = get_global_id(0);
  const int ch = get_global_id(1);
  const int batch = get_global_id(2);
  
  const int block_x = block % WTILES;
  const int block_y = block / WTILES;
  
  // Tiles overlap by 2
  const int yin = 2 * block_y - 1;
  const int xin = 2 * block_x - 1;
  
  if (block < P && ch < C) {
    // Cache input tile and handle zero padding
    float x[4][4];
    for (int i = 0; i < 4; i++) {
      for (int j = 0; j < 4; j++) {
        int a = xin + j;
        int b = yin + i;
        if (b >= 0 && a >= 0 && b < height && a < width) {
          x[i][j] = vload_net_t(batch * C * boardsize +
		      ch * boardsize + b * width + a, in);
        } else {
          x[i][j] = 0.0f;
        }
      }
    }
  
    // V dimensions are [16, input_channels, batch_size * tiles].
	// Padded with zeros as necessary for SGEMM
	// = [16, Cpad, Ppad]

    const int offset = ch * Ppad + P * batch + block;
    __in_transform_eq(x, V, offset, CPpad);
  }
}

void __out_transform_eq(__global const float * restrict M, float o[4],
                        int Kpad, int Ppad, int block, int batch)
{
  const int W = 8;
  const int H = 8;
  const int WTILES = (W + 1) / 2;
  const int P = WTILES * WTILES;
  const int b = P * batch + block;
  const int k = get_global_id(0);
  float temp_m[16];
  // M dimensions are [16, outputs, batch_size * tiles].
  // Plus zero padding from SGEMM.

  const int offset = b * Kpad + k;
  for (int xn = 0; xn < 16; xn++) {
      temp_m[xn] = vload_net_t(xn * Kpad * Ppad + offset, M);
  }
  
  o[0] = temp_m[0*4 + 0] + temp_m[0*4 + 1] + temp_m[0*4 + 2] +
  temp_m[1*4 + 0] + temp_m[1*4 + 1] + temp_m[1*4 + 2] +
  temp_m[2*4 + 0] + temp_m[2*4 + 1] + temp_m[2*4 + 2];
  
  o[1] = temp_m[0*4 + 1] - temp_m[0*4 + 2] - temp_m[0*4 + 3] +
  temp_m[1*4 + 1] - temp_m[1*4 + 2] - temp_m[1*4 + 3] +
  temp_m[2*4 + 1] - temp_m[2*4 + 2] - temp_m[2*4 + 3];
  
  o[2] = temp_m[1*4 + 0] + temp_m[1*4 + 1] + temp_m[1*4 + 2] -
  temp_m[2*4 + 0] - temp_m[2*4 + 1] - temp_m[2*4 + 2] -
  temp_m[3*4 + 0] - temp_m[3*4 + 1] - temp_m[3*4 + 2];
  
  o[3] = temp_m[1*4 + 1] - temp_m[1*4 + 2] - temp_m[1*4 + 3] -
  temp_m[2*4 + 1] + temp_m[2*4 + 2] + temp_m[2*4 + 3] -
  temp_m[3*4 + 1] + temp_m[3*4 + 2] + temp_m[3*4 + 3];
}

__kernel void out_transform_fused_bn(__global const float * restrict M,
                                     __global net_t * restrict Y,
                                     const int K,
                                     const int Kpad, const int Ppad,
                                     const int relu,
                                     __global const net_t * restrict residual,
                                     __constant const net_t * restrict biases) {
  const int W = 8;
  const int H = 8;
  const int WTILES = (W + 1) / 2;
  const int P = WTILES * WTILES;
  
  const int k = get_global_id(0);
  const int block = get_global_id(1);
  const int batch = get_global_id(2);

  const int block_x = block % WTILES;
  const int block_y = block / WTILES;
  
  int x = 2*block_x;
  int y = 2*block_y;
  int a_ind = y * W + x;
  if (k < K && block < P) {
    const int kHW = batch * K * BOARD_SQUARES + k * BOARD_SQUARES;
    float o[4];
    __out_transform_eq(M, o, Kpad, Ppad, block, batch);
    
    const float bias = vload_net_t(k, biases);
    
    const bool pred[4] = { 1, x+1 < W, y+1 < H, x+1 < W & y+1 < H};
    
    const int a[4] = {a_ind, a_ind+1, a_ind+W, a_ind+W+1};
    
    for (int i = 0; i < 4; i++) {
      if (pred[i]) {
        o[i] = o[i] + bias;
        if (residual) {
          o[i] += vload_net_t(kHW + a[i], residual);
        }
        if (relu) {
          o[i] = o[i] > 0 ? o[i] : 0.0f;
        }
        vstore_net_t(o[i], kHW + a[i], Y);
      }
    }
  }
}

__kernel void out_transform_fused_bn_in(
                                        __global const float * restrict M,
                                        __global net_t * restrict Y,
                                        __global net_t * restrict V,
                                        const int K,
                                        const int Kpad, const int Ppad, const int Cpad,
                                        __global const net_t * restrict residual,
                                        __constant const net_t * restrict biases,
                                        __local float * ybuf) {
  const int W = 8;
  const int H = 8;
  const int WTILES = (W + 1) / 2;
  const int P = WTILES * WTILES;
  
  const int k = get_global_id(0);
  const int kg = get_local_id(0);
  const int block = get_global_id(1);
  const int batch = get_global_id(2);
  
  const int block_x = block % WTILES;
  const int block_y = block / WTILES;
  
  const int yin = 2 * block_y - 1;
  const int xin = 2 * block_x - 1;
  
  
  const int x = 2*block_x;
  const int y = 2*block_y;
  int a_ind = y * W + x;
  
  
  if (k < K && block < P) {
    const int a[4] = {a_ind, a_ind+1, a_ind+W, a_ind+W+1};
    const bool pred[4] = { 1, x+1 < W, y+1 < H, x+1 < W & y+1 < H};
    const int kHW = batch * K * BOARD_SQUARES + k * BOARD_SQUARES;
    
    float o[4];
    __out_transform_eq(M, o, Kpad, Ppad, block, batch);
    
    const float bias = vload_net_t(k, biases);
    
    for (int i = 0; i < 4; i++) {
      if (pred[i]) {
        o[i] = o[i] + bias;
        if (residual) {
          o[i] += vload_net_t(kHW + a[i], residual);
        }
        o[i] = o[i] > 0 ? o[i] : 0.0f;
        ybuf[kg * BOARD_SQUARES + a[i]] = o[i];
        if (Y) {
          vstore_net_t(o[i], kHW + a[i], Y);
        }
      }
    }
  }
  
  barrier(CLK_LOCAL_MEM_FENCE);
  
  if (block < P && k < K) {
    const int CPpad = Ppad * Cpad;
    // Cache input tile and handle zero padding
    float xx[4][4];
    for (int i = 0; i < 4; i++) {
      int b = yin + i;
      for (int j = 0; j < 4; j++) {
        int a = xin + j;
        if (b >= 0 && a >= 0 && b < H && a < W) {
          xx[i][j] = ybuf[kg * BOARD_SQUARES + b * W + a];
        } else {
          xx[i][j] = 0.0f;
        }
      }
    }
    
    const int offset = k * Ppad + P * batch + block;
    __in_transform_eq(xx, V, offset, CPpad);
  }
}

// End of the C++11 raw string literal
)"

```

`src/neural/opencl/clsource/policymap.opencl`:

```opencl
// Enables loading of this file using the C++ pre-processor's #include (C++11 standard raw string
// literal). Comment-out this line for syntax-highlighting when developing.

R"(
__kernel void policymap(
              __global const net_t * restrict input,
              __global net_t * restrict output,
              __global short* restrict indices,
              const int N,
              const int inputSize,
              const int usedSize,
              const int outputSize) {

  int tid = get_global_id(0);

  int n = tid / usedSize;
  int i = tid % usedSize;

  if (n >= N) return;

  int j = indices[i];

  if (j >= 0) {
    output[n * outputSize + j] = input[n * inputSize + i];
  }
}
// End of the C++11 raw string literal
)"

```

`src/neural/opencl/clsource/se.opencl`:

```opencl
// Enables loading of this file using the C++ pre-processor's #include (C++11 standard raw string
// literal). Comment-out this line for syntax-highlighting when developing.

R"(
    __kernel void global_avg_pooling(
                   const int channels,
                   __global const net_t * restrict in,
                   __global net_t * restrict out) {

        const int col = get_global_id(0);  // column
        const int c = get_global_id(1);  // channel

        const int lid = get_local_id(0);

        __local net_t row_acc[BOARD_SIZE];

        if (c < channels && col < BOARD_SIZE) {

            net_t acc = 0.0f;

            for ( int i = 0; i < BOARD_SIZE; i++) {
                acc += vload_net_t(c * BOARD_SQUARES + i * BOARD_SIZE + col, in);
            }
            row_acc[lid] = acc;
        }

        barrier(CLK_LOCAL_MEM_FENCE);

        if (lid == 0) {
            net_t acc = 0.0f;
            for ( int i = 0; i < BOARD_SIZE; i++) {
                acc += row_acc[i];
            }
            acc = acc/BOARD_SQUARES;
            vstore_net_t(acc, c, out);
        }
    }

    __kernel void apply_se(
                  const int channels,
                  const int batch_size,
                  __global const net_t * restrict input,
                  __global net_t * restrict residual,
                  __global const net_t * restrict fc_out) {

        const int col = get_global_id(0);  // column
        const int c = get_global_id(1);  // channel

        const int batch = c / channels;

        if (c < batch_size * channels && col < BOARD_SIZE) {
            net_t gamma = vload_net_t(c + batch * channels, fc_out);
            gamma = 1.0f/(1.0f + exp(-gamma)); // Sigmoid
            net_t beta = vload_net_t(c + batch * channels + channels, fc_out);

            for ( int i = 0; i < BOARD_SIZE; i++) {
                const int idx = c * BOARD_SQUARES + i * BOARD_SIZE + col;
                const net_t in = vload_net_t(idx, input);
                const net_t res = vload_net_t(idx, residual);

                net_t val = gamma * in + res + beta;

                val = val > 0.0f ? val : 0.0f;

                vstore_net_t(val, idx, residual);
            }
        }
    }
// End of the C++11 raw string literal
)"

```

`src/neural/opencl/network_opencl.cc`:

```cc
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018-2022 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#include <algorithm>
#include <cassert>
#include <cmath>
#include <condition_variable>
#include <thread>

#include "neural/factory.h"
#include "neural/network.h"
#include "neural/network_legacy.h"
#include "neural/opencl/OpenCL.h"
#include "neural/opencl/OpenCLParams.h"
#include "neural/shared/activation.h"
#include "neural/shared/policy_map.h"
#include "neural/shared/winograd_filter.h"
#include "utils/bititer.h"
#include "utils/exception.h"
#include "utils/filesystem.h"
#include "utils/logging.h"
#include "utils/weights_adapter.h"

namespace lczero {

namespace {

class OpenCLNetwork;

// Copy the vectors we need after weights is deallocated.
struct OpenCLWeights {
  const std::vector<float> ip2_val_w;
  const std::vector<float> ip2_val_b;
  const std::vector<float> ip2_mov_w;
  const std::vector<float> ip2_mov_b;
  const size_t num_output_policies = 1858;
  const size_t num_value_channels;
  const size_t num_moves_channels;

  OpenCLWeights(const WeightsFile& file)
      : ip2_val_w(LayerAdapter(file.weights().ip2_val_w()).as_vector()),
        ip2_val_b(LayerAdapter(file.weights().ip2_val_b()).as_vector()),
        ip2_mov_w(LayerAdapter(file.weights().ip2_mov_w()).as_vector()),
        ip2_mov_b(LayerAdapter(file.weights().ip2_mov_b()).as_vector()),
        num_value_channels(LayerAdapter(file.weights().ip1_val_b()).size()),
        num_moves_channels(LayerAdapter(file.weights().ip1_mov_b()).size()) {}
};

class OpenCLComputation : public NetworkComputation {
 public:
  OpenCLComputation(const OpenCL_Network& opencl_net,
                    const OpenCLWeights& weights, const bool wdl,
                    const bool moves_left)
      : opencl_net_(opencl_net),
        weights_(weights),
        policies_(),
        q_values_(),
        m_values_(),
        wdl_(wdl),
        moves_left_(moves_left) {
    buffers_ = opencl_net.acquire_buffers();
  }

  virtual ~OpenCLComputation() {
    opencl_net_.release_buffers(std::move(buffers_));
  }

  // Adds a sample to the batch.
  void AddInput(InputPlanes&& input) override { planes_.emplace_back(input); }

  // Do the computation.
  void ComputeBlocking() override {
    // Determine the largest batch for allocations.
    const auto plane_count = planes_.size();
    const auto max_batch_size = opencl_net_.getMaxMatchSize();
    const auto largest_batch_size = std::min(max_batch_size, plane_count);

    const auto num_output_policies = weights_.num_output_policies;
    const auto num_value_channels = weights_.num_value_channels;
    const auto num_moves_channels = weights_.num_moves_channels;

    // Typically
    // input_channels = 112
    // num_value_channels = 128
    // num_output_policy = 1858

    std::vector<float> output_pol(largest_batch_size * num_output_policies);
    std::vector<float> output_val(largest_batch_size * num_value_channels);
    std::vector<float> output_mov(largest_batch_size * num_moves_channels);
    std::vector<float> input_data(largest_batch_size * kInputPlanes * kSquares);

    for (size_t i = 0; i < plane_count; i += largest_batch_size) {
      const auto batch_size = std::min(plane_count - i, largest_batch_size);
      for (size_t j = 0; j < batch_size; j++) {
        EncodePlanes(planes_[i + j], &input_data[j * kSquares * kInputPlanes]);
      }

      buffers_->forward(input_data, output_pol, output_val, output_mov,
                        batch_size);

      for (size_t j = 0; j < batch_size; j++) {
        std::vector<float> policy(num_output_policies);

        // Get the moves.
        policy.assign(output_pol.begin() + j * num_output_policies,
                      output_pol.begin() + (j + 1) * num_output_policies);
        policies_.emplace_back(std::move(policy));

        // Now get the score.
        if (wdl_) {
          std::vector<float> wdl(weights_.ip2_val_b);
          auto ptr_weights = weights_.ip2_val_w.data();
          auto ptr_outputs = &output_val[j * num_value_channels];
          for (size_t q = 0; q < 3; q++) {
            for (size_t i = 0; i < num_value_channels; i++) {
              wdl[q] +=
                  ptr_weights[i + q * num_value_channels] * ptr_outputs[i];
            }
          }

          std::vector<float> wdl_softmax(3);
          SoftmaxActivation(3, wdl.data(), wdl_softmax.data());

          q_values_.emplace_back(wdl_softmax[0]);
          q_values_.emplace_back(wdl_softmax[1]);
          q_values_.emplace_back(wdl_softmax[2]);
        } else {
          auto winrate = weights_.ip2_val_b[0];
          auto ptr_weights = weights_.ip2_val_w.data();
          auto ptr_outputs = &output_val[j * num_value_channels];
          for (size_t i = 0; i < num_value_channels; i++)
            winrate += ptr_weights[i] * ptr_outputs[i];

          q_values_.emplace_back(std::tanh(winrate));
        }

        if (moves_left_) {
          auto m = weights_.ip2_mov_b[0];
          auto ptr_weights = weights_.ip2_mov_w.data();
          auto ptr_outputs = &output_mov[j * num_moves_channels];
          for (size_t i = 0; i < num_moves_channels; i++)
            m += ptr_weights[i] * std::max(0.0f, ptr_outputs[i]);

          m_values_.emplace_back(std::max(0.0f, m));
        }
      }
    }
  }

  // Returns how many times AddInput() was called.
  int GetBatchSize() const override { return static_cast<int>(planes_.size()); }

  // Returns Q value of @sample.
  float GetQVal(int sample) const override {
    if (wdl_) {
      auto w = q_values_[3 * sample + 0];
      auto l = q_values_[3 * sample + 2];
      return w - l;
    } else {
      return q_values_[sample];
    }
  }

  float GetDVal(int sample) const override {
    if (wdl_) {
      auto d = q_values_[3 * sample + 1];
      return d;
    } else {
      return 0.0f;
    }
  }

  float GetMVal(int sample) const override {
    if (moves_left_) {
      auto d = m_values_[sample];
      return d;
    } else {
      return 0.0f;
    }
  }

  // Returns P value @move_id of @sample.
  float GetPVal(int sample, int move_id) const override {
    return policies_[sample][move_id];
  }

 private:
  static constexpr auto kWidth = 8;
  static constexpr auto kHeight = 8;
  static constexpr auto kSquares = kWidth * kHeight;

  void EncodePlanes(const InputPlanes& sample, float* buffer);

  const OpenCL_Network& opencl_net_;
  const OpenCLWeights& weights_;

  std::vector<InputPlanes> planes_;

  std::vector<std::vector<float>> policies_;
  std::vector<float> q_values_;
  std::vector<float> m_values_;

  std::unique_ptr<OpenCLBuffers> buffers_;
  bool wdl_;
  bool moves_left_;
};

void OpenCLComputation::EncodePlanes(const InputPlanes& sample, float* buffer) {
  for (const InputPlane& plane : sample) {
    const float value = plane.value;
    for (auto i = 0; i < kSquares; i++) {
      *(buffer++) = (plane.mask & (((uint64_t)1) << i)) != 0 ? value : 0;
    }
  }
}

class OpenCLNetwork : public Network {
 public:
  virtual ~OpenCLNetwork(){};

  OpenCLNetwork(const WeightsFile& file, const OptionsDict& options)
      : capabilities_{file.format().network_format().input(),
                      file.format().network_format().moves_left()},
        weights_(file),
        params_(),
        opencl_(),
        opencl_net_(opencl_) {
    LegacyWeights weights(file.weights());
    params_.gpuId = options.GetOrDefault<int>("gpu", -1);
    params_.force_tune = options.GetOrDefault<bool>("force_tune", false);
    params_.tune_only = options.GetOrDefault<bool>("tune_only", false);
    params_.tune_exhaustive =
        options.GetOrDefault<bool>("tune_exhaustive", false);
    if (options.IsDefault<std::string>("tuner_file")) {
      std::string user_cache_path = GetUserCacheDirectory();
      if (!user_cache_path.empty()) {
        user_cache_path += "lc0/";
        CreateDirectory(user_cache_path);
      }
      params_.tuner_file = user_cache_path + "leelaz_opencl_tuning";
    } else {
      params_.tuner_file = options.Get<std::string>("tuner_file");
    }

    wdl_ = file.format().network_format().output() ==
           pblczero::NetworkFormat::OUTPUT_WDL;

    moves_left_ = (file.format().network_format().moves_left() ==
                   pblczero::NetworkFormat::MOVES_LEFT_V1) &&
                  options.GetOrDefault<bool>("mlh", true);

    auto max_batch_size_ =
        static_cast<size_t>(options.GetOrDefault<int>("batch_size", 16));
    if (max_batch_size_ > kHardMaxBatchSize) {
      max_batch_size_ = kHardMaxBatchSize;
    }
    CERR << "OpenCL, maximum batch size set to " << max_batch_size_ << ".";

    // By default, the max batch size used for tuning is the max batch size
    // used for computations.
    // It may not be the optimal value, then use tune_batch_size for fine
    // tune.
    params_.tune_batch_size =
        options.GetOrDefault<int>("tune_batch_size", max_batch_size_);

    const auto inputChannels = static_cast<size_t>(kInputPlanes);
    const auto channels = weights.input.biases.size();
    const auto residual_blocks = weights.residual.size();

    const auto num_value_input_planes = weights.value.biases.size();
    const auto num_moves_input_planes = weights.moves_left.biases.size();
    const auto num_policy_input_planes = weights.policy.biases.size();
    const auto num_output_policy = kPolicyOutputs;
    const auto num_value_channels = weights.ip1_val_b.size();
    const auto num_moves_channels = weights.ip1_mov_b.size();

    // Typically
    // input_channels = 112
    // output_channels = 192
    // num_value_input_planes = 32
    // num_policy_input_planes = 32
    // num_value_channels = 128
    // num_output_policy = 1858

    static constexpr auto kWinogradAlpha = 4;

    opencl_.initialize(channels, params_);

    auto tuners = opencl_.get_sgemm_tuners();

    auto mwg = tuners[0];
    auto kwg = tuners[2];
    auto vwm = tuners[3];

    size_t m_ceil = ceilMultiple(ceilMultiple(channels, mwg), vwm);
    size_t k_ceil = ceilMultiple(ceilMultiple(inputChannels, kwg), vwm);

    std::vector<float> input_conv_weights = WinogradFilterTransformF(
        weights.input.weights, channels, inputChannels);

    auto Upad = WinogradFilterZeropadU(input_conv_weights, channels,
                                       inputChannels, m_ceil, k_ceil);

    // Winograd filter transformation changes filter size to 4x4.
    opencl_net_.push_input_convolution(kWinogradAlpha, inputChannels, channels,
                                       Upad, weights.input.biases);

    auto conv_policy = file.format().network_format().policy() ==
                       pblczero::NetworkFormat::POLICY_CONVOLUTION;

    // Residual blocks.
    for (auto i = size_t{0}; i < residual_blocks; i++) {
      auto& residual = weights.residual[i];
      auto& conv1 = residual.conv1;
      auto& conv2 = residual.conv2;
      auto& se = residual.se;

      std::vector<float> conv_weights_1 =
          WinogradFilterTransformF(conv1.weights, channels, channels);
      std::vector<float> conv_weights_2 =
          WinogradFilterTransformF(conv2.weights, channels, channels);

      auto Upad1 = WinogradFilterZeropadU(conv_weights_1, channels, channels,
                                          m_ceil, m_ceil);
      auto Upad2 = WinogradFilterZeropadU(conv_weights_2, channels, channels,
                                          m_ceil, m_ceil);

      opencl_net_.push_residual(kWinogradAlpha, channels, channels, Upad1,
                                conv1.biases, Upad2, conv2.biases);
      if (residual.has_se) {
        auto se_fc_outputs = se.w1.size() / channels;
        if (se.b2.size() != 2 * channels) {
          throw Exception("SE-unit output bias is not right size.");
        }
        opencl_net_.push_se(channels, se_fc_outputs, se.w1, se.b1, se.w2,
                            se.b2);
      }
    }

    constexpr unsigned int width = 8;
    constexpr unsigned int height = 8;

    if (conv_policy) {
      auto& policy1 = weights.policy1;
      auto& policy = weights.policy;
      auto pol_channels = policy.biases.size();

      std::vector<float> conv_weights_1 =
          WinogradFilterTransformF(policy1.weights, channels, channels);
      auto W1 = WinogradFilterZeropadU(conv_weights_1, channels, channels,
                                       m_ceil, m_ceil);

      size_t m_ceil_pol = ceilMultiple(ceilMultiple(pol_channels, mwg), vwm);
      size_t k_ceil_pol = ceilMultiple(ceilMultiple(channels, kwg), vwm);
      std::vector<float> conv_weights_2 =
          WinogradFilterTransformF(policy.weights, pol_channels, channels);
      auto W2 = WinogradFilterZeropadU(conv_weights_2, pol_channels, channels,
                                       m_ceil_pol, k_ceil_pol);

      std::vector<short> indices;
      for (auto i = size_t{0}; i < kPolicyUsedPlanes * 8 * 8; i++) {
        indices.emplace_back(kConvPolicyMap[i]);
      }

      opencl_net_.push_conv_policy(
          channels, pol_channels, kPolicyUsedPlanes * width * height,
          num_output_policy, W1, weights.policy1.biases, W2,
          weights.policy.biases, indices);
    } else {
      opencl_net_.push_policy(channels, num_policy_input_planes,
                              num_policy_input_planes * width * height,
                              num_output_policy, weights.policy.weights,
                              weights.policy.biases, weights.ip_pol_w,
                              weights.ip_pol_b);
    }
    opencl_net_.push_value(channels, num_value_input_planes,
                           num_value_input_planes * width * height,
                           num_value_channels, weights.value.weights,
                           weights.value.biases, weights.ip1_val_w,
                           weights.ip1_val_b);

    if (moves_left_) {
      opencl_net_.push_moves_left(
          channels, num_moves_input_planes,
          num_moves_input_planes * width * height, num_moves_channels,
          weights.moves_left.weights, weights.moves_left.biases,
          weights.ip1_mov_w, weights.ip1_mov_b);
    }

    opencl_net_.setMaxMatchSize(max_batch_size_);
  }

  std::unique_ptr<NetworkComputation> NewComputation() override {
    return std::make_unique<OpenCLComputation>(opencl_net_, weights_, wdl_,
                                               moves_left_);
  }

  const NetworkCapabilities& GetCapabilities() const override {
    return capabilities_;
  }

 private:
  static constexpr auto kHardMaxBatchSize = 32;
  static constexpr auto kPolicyUsedPlanes = 73;
  static constexpr auto kPolicyOutputs = 1858;

  const NetworkCapabilities capabilities_;
  OpenCLWeights weights_;
  OpenCLParams params_;
  OpenCL opencl_;
  OpenCL_Network opencl_net_;
  bool wdl_;
  bool moves_left_;
};

std::unique_ptr<Network> MakeOpenCLNetwork(const std::optional<WeightsFile>& w,
                                           const OptionsDict& options) {
  if (!w) {
    throw Exception("The opencl backend requires a network file.");
  }
  const WeightsFile& weights = *w;
  if (weights.format().network_format().network() !=
          pblczero::NetworkFormat::NETWORK_CLASSICAL_WITH_HEADFORMAT &&
      weights.format().network_format().network() !=
          pblczero::NetworkFormat::NETWORK_SE_WITH_HEADFORMAT) {
    throw Exception("Network format " +
                    pblczero::NetworkFormat::NetworkStructure_Name(
                        weights.format().network_format().network()) +
                    " is not supported by OpenCL backend.");
  }
  if (weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_CLASSICAL &&
      weights.format().network_format().policy() !=
          pblczero::NetworkFormat::POLICY_CONVOLUTION) {
    throw Exception("Policy format " +
                    pblczero::NetworkFormat::PolicyFormat_Name(
                        weights.format().network_format().policy()) +
                    " is not supported by OpenCL backend.");
  }
  if (weights.format().network_format().value() !=
          pblczero::NetworkFormat::VALUE_CLASSICAL &&
      weights.format().network_format().value() !=
          pblczero::NetworkFormat::VALUE_WDL) {
    throw Exception("Value format " +
                    pblczero::NetworkFormat::ValueFormat_Name(
                        weights.format().network_format().value()) +
                    " is not supported by OpenCL backend.");
  }
  if (weights.format().network_format().default_activation() !=
      pblczero::NetworkFormat::DEFAULT_ACTIVATION_RELU) {
    throw Exception(
        "Default activation " +
        pblczero::NetworkFormat::DefaultActivation_Name(
            weights.format().network_format().default_activation()) +
        " is not supported by BLAS backend.");
  }
  return std::make_unique<OpenCLNetwork>(weights, options);
}

REGISTER_NETWORK("opencl", MakeOpenCLNetwork, 100)

}  // namespace
}  // namespace lczero

```

`src/neural/shared/activation.cc`:

```cc
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018-2022 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#include "neural/shared/activation.h"

#include <algorithm>
#include <cmath>

#ifdef USE_ISPC
#include "activation_ispc.h"
#endif

namespace lczero {
namespace {
constexpr int kWidth = 8;
constexpr int kHeight = 8;
constexpr int kSquares = kWidth * kHeight;
}  // namespace

void SoftmaxActivation(const size_t size, const float* input, float* output) {
  auto alpha = *std::max_element(input, input + size);

  auto denom = 0.0f;
  for (size_t i = 0; i < size; i++) {
    auto val = std::exp(input[i] - alpha);
    output[i] = val;
    denom += val;
  }
  for (size_t i = 0; i < size; i++) {
    output[i] = output[i] / denom;
  }
}

static inline float mish(float val) {
  auto e = expf(val);
  auto n = e * e + 2.0f * e;
  auto d = val / (n + 2.0f);
  if (val <= -0.125f) {
    return n * d;
  } else {
    return val - 2.0f * d;
  }
}

static inline float selu(float val) {
  float alpha = 1.67326324f, scale = 1.05070098f;
  if (val > 0) {
    return scale * val;
  } else {
    return scale * alpha * (expf(val) - 1.0f);
  }
}

float Activate(const float val, const ActivationFunction activation) {
  switch (activation) {
    case RELU:
      return val > 0 ? val : 0;
    case MISH:
      return mish(val);
    case TANH:
      return tanhf(val);
    case SIGMOID:
      return 1.0f / (1.0f + expf(-val));
    case SELU:
      return selu(val);
    case NONE:
      // Nothing to do.
      break;
  }
  return val;
}

void Activate(const size_t len, const float* data, const float* bias,
              float* output, const ActivationFunction activation) {
  if (activation == NONE) {
    for (size_t b = 0; b < len; b++) {
      output[b] = data[b] + bias[b];
    }
  } else if (activation == RELU) {
    for (size_t b = 0; b < len; b++) {
      float val = data[b] + bias[b];
      output[b] = val > 0 ? val : 0;
    }
  } else if (activation == MISH) {
#ifndef USE_ISPC
    for (size_t b = 0; b < len; b++) {
      float val = data[b] + bias[b];
      output[b] = mish(val);
    }
#else
    ispc::ActivateMish(len, 1.0f, data, bias, 0.0f, output);
#endif
  } else {
    for (size_t b = 0; b < len; b++) {
      float val = data[b] + bias[b];
      output[b] = Activate(val, activation);
    }
  }
}

void Activate(const size_t len, float gamma, const float* data,
              const float* bias, float beta, float* output,
              const ActivationFunction activation) {
  if (activation == NONE) {
    for (size_t b = 0; b < len; b++) {
      float val = gamma * data[b] + bias[b] + beta;
      output[b] = val;
    }
  } else if (activation == RELU) {
    for (size_t b = 0; b < len; b++) {
      float val = gamma * data[b] + bias[b] + beta;
      output[b] = val > 0 ? val : 0;
    }
  } else if (activation == MISH) {
#ifndef USE_ISPC
    for (size_t b = 0; b < len; b++) {
      float val = gamma * data[b] + bias[b] + beta;
      output[b] = mish(val);
    }
#else
    ispc::ActivateMish(len, gamma, data, bias, beta, output);
#endif
  } else {
    for (size_t b = 0; b < len; b++) {
      float val = gamma * data[b] + bias[b] + beta;
      output[b] = Activate(val, activation);
    }
  }
}

void BiasResidual(const size_t batch_size, const size_t channels, float* data,
                  const float* biases, const float* eltwise,
                  const ActivationFunction activation) {
  for (size_t i = 0; i < batch_size; i++) {
    for (size_t c = 0; c < channels; ++c) {
      auto bias = biases[c];
      auto arr = &data[c * kSquares];
      auto res = &eltwise[c * kSquares];
      if (activation == NONE) {
        for (size_t b = 0; b < kSquares; b++) {
          float val = res[b] + arr[b] + bias;
          arr[b] = val;
        }
      } else if (activation == RELU) {
        for (size_t b = 0; b < kSquares; b++) {
          float val = res[b] + arr[b] + bias;
          arr[b] = val > 0 ? val : 0;
        }
      } else if (activation == MISH) {
#ifndef USE_ISPC
        for (size_t b = 0; b < kSquares; b++) {
          float val = res[b] + arr[b] + bias;
          arr[b] = mish(val);
        }
#else
        ispc::ActivateMish(kSquares, 1.0f, res, arr, bias, arr);
#endif
      } else {
        for (size_t b = 0; b < kSquares; b++) {
          float val = res[b] + arr[b] + bias;
          arr[b] = Activate(val, activation);
        }
      }
    }
    data += channels * kSquares;
    eltwise += channels * kSquares;
  }
}

void BiasActivate(const size_t batch_size, const size_t channels, float* data,
                  const float* biases, const ActivationFunction activation) {
  for (size_t i = 0; i < batch_size; i++) {
    for (size_t c = 0; c < channels; ++c) {
      auto bias = biases[c];
      auto arr = &data[c * kSquares];
      if (activation == NONE) {
        for (size_t b = 0; b < kSquares; b++) {
          float val = arr[b] + bias;
          arr[b] = val;
        }
      } else if (activation == RELU) {
        for (size_t b = 0; b < kSquares; b++) {
          float val = arr[b] + bias;
          arr[b] = val > 0 ? val : 0;
        }
      } else if (activation == MISH) {
#ifndef USE_ISPC
        for (size_t b = 0; b < kSquares; b++) {
          float val = arr[b] + bias;
          arr[b] = mish(val);
        }
#else
        ispc::ActivateMish(kSquares, 0.0f, arr, arr, bias, arr);
#endif
      } else {
        for (size_t b = 0; b < kSquares; b++) {
          float val = arr[b] + bias;
          arr[b] = Activate(val, activation);
        }
      }
    }
    data += channels * kSquares;
  }
}

}  // namespace lczero

```

`src/neural/shared/activation.h`:

```h
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018-2022 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include <cstddef>
#include <vector>

namespace lczero {
enum ActivationFunction { NONE, RELU, TANH, SIGMOID, SELU, MISH };

// Softmax activation
void SoftmaxActivation(const size_t size, const float* input, float* output);

void BiasResidual(const size_t batch_size, const size_t channels, float * data,
                  const float* biases, const float* eltwise,
                  const ActivationFunction activation = RELU);

void BiasActivate(const size_t batch_size, const size_t channels, float * data,
                  const float* biases,
                  const ActivationFunction activation = RELU);

float Activate(const float val, const ActivationFunction activation);

void Activate(const size_t len, const float* data, const float* bias,
              float* output, const ActivationFunction activation);

void Activate(const size_t len, float gamma, const float* data,
              const float* bias, float beta, float* out,
              const ActivationFunction activation);

}  // namespace lczero

```

`src/neural/shared/activation.ispc`:

```ispc
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2022 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

static inline float mish(float val) {
  float e = exp(val);
  float n = e * e + 2.0f * e;
  float d = val / (n + 2.0f);
  if (val <= -0.5f) {
    return n * d;
  } else {
    return val - 2.0f * d;
  }
}

export void ActivateMish(uniform const size_t len, uniform float gamma,
                         const uniform float data[], const uniform float bias[],
                         uniform float beta, uniform float output[]) {
  foreach (b = 0 ... len) {
    float val = gamma * data[b] + bias[b] + beta;
    output[b] = mish(val);
  }
}

```

`src/neural/shared/attention_policy_map.h`:

```h
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2019 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

namespace lczero {

// 64*64 + 8x24
const short kAttnPolicyMap[] = {
    -1,   0,    1,    2,    3,    4,    5,    6,    7,    8,    9,    -1,
    -1,   -1,   -1,   -1,   10,   11,   12,   -1,   -1,   -1,   -1,   -1,
    13,   -1,   -1,   14,   -1,   -1,   -1,   -1,   15,   -1,   -1,   -1,
    16,   -1,   -1,   -1,   17,   -1,   -1,   -1,   -1,   18,   -1,   -1,
    19,   -1,   -1,   -1,   -1,   -1,   20,   -1,   21,   -1,   -1,   -1,
    -1,   -1,   -1,   22,   23,   -1,   24,   25,   26,   27,   28,   29,
    30,   31,   32,   33,   -1,   -1,   -1,   -1,   34,   35,   36,   37,
    -1,   -1,   -1,   -1,   -1,   38,   -1,   -1,   39,   -1,   -1,   -1,
    -1,   40,   -1,   -1,   -1,   41,   -1,   -1,   -1,   42,   -1,   -1,
    -1,   -1,   43,   -1,   -1,   44,   -1,   -1,   -1,   -1,   -1,   45,
    -1,   46,   -1,   -1,   -1,   -1,   -1,   -1,   47,   48,   -1,   49,
    50,   51,   52,   53,   54,   55,   56,   57,   58,   -1,   -1,   -1,
    59,   60,   61,   62,   63,   -1,   -1,   -1,   -1,   -1,   64,   -1,
    -1,   65,   -1,   -1,   -1,   -1,   66,   -1,   -1,   -1,   67,   -1,
    -1,   -1,   68,   -1,   -1,   -1,   -1,   69,   -1,   -1,   70,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   71,   -1,   -1,   -1,   -1,   -1,
    72,   73,   74,   -1,   75,   76,   77,   78,   -1,   79,   80,   81,
    82,   83,   -1,   -1,   -1,   84,   85,   86,   87,   88,   -1,   -1,
    89,   -1,   -1,   90,   -1,   -1,   91,   -1,   -1,   -1,   -1,   92,
    -1,   -1,   -1,   93,   -1,   -1,   -1,   94,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   95,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   96,
    -1,   -1,   -1,   -1,   97,   98,   99,   100,  -1,   101,  102,  103,
    -1,   -1,   104,  105,  106,  107,  108,  -1,   -1,   -1,   109,  110,
    111,  112,  113,  -1,   -1,   114,  -1,   -1,   115,  -1,   -1,   116,
    117,  -1,   -1,   -1,   118,  -1,   -1,   -1,   -1,   -1,   -1,   -1,
    119,  -1,   -1,   -1,   -1,   -1,   -1,   -1,   120,  -1,   -1,   -1,
    -1,   -1,   -1,   -1,   121,  -1,   -1,   -1,   122,  123,  124,  125,
    126,  -1,   127,  128,  -1,   -1,   -1,   129,  130,  131,  132,  133,
    -1,   -1,   -1,   134,  135,  136,  137,  138,  -1,   -1,   139,  -1,
    -1,   140,  -1,   -1,   -1,   141,  -1,   -1,   -1,   142,  -1,   -1,
    143,  -1,   -1,   -1,   -1,   144,  -1,   -1,   -1,   -1,   -1,   -1,
    -1,   145,  -1,   -1,   -1,   -1,   -1,   -1,   -1,   146,  -1,   -1,
    147,  148,  149,  150,  151,  152,  -1,   153,  -1,   -1,   -1,   -1,
    154,  155,  156,  157,  -1,   -1,   -1,   -1,   158,  159,  160,  161,
    -1,   -1,   -1,   162,  -1,   -1,   163,  -1,   -1,   -1,   164,  -1,
    -1,   -1,   165,  -1,   -1,   166,  -1,   -1,   -1,   -1,   167,  -1,
    168,  -1,   -1,   -1,   -1,   -1,   169,  -1,   -1,   -1,   -1,   -1,
    -1,   -1,   170,  -1,   171,  172,  173,  174,  175,  176,  177,  -1,
    -1,   -1,   -1,   -1,   -1,   178,  179,  180,  -1,   -1,   -1,   -1,
    -1,   181,  182,  183,  -1,   -1,   -1,   -1,   184,  -1,   -1,   185,
    -1,   -1,   -1,   186,  -1,   -1,   -1,   187,  -1,   -1,   188,  -1,
    -1,   -1,   -1,   189,  -1,   190,  -1,   -1,   -1,   -1,   -1,   191,
    192,  -1,   -1,   -1,   -1,   -1,   -1,   193,  194,  195,  196,  -1,
    -1,   -1,   -1,   -1,   -1,   197,  198,  199,  200,  201,  202,  203,
    204,  205,  206,  -1,   -1,   -1,   -1,   -1,   207,  208,  209,  -1,
    -1,   -1,   -1,   -1,   210,  -1,   -1,   211,  -1,   -1,   -1,   -1,
    212,  -1,   -1,   -1,   213,  -1,   -1,   -1,   214,  -1,   -1,   -1,
    -1,   215,  -1,   -1,   216,  -1,   -1,   -1,   -1,   -1,   217,  -1,
    218,  219,  220,  221,  -1,   -1,   -1,   -1,   222,  -1,   223,  224,
    225,  226,  227,  228,  229,  230,  231,  232,  -1,   -1,   -1,   -1,
    233,  234,  235,  236,  -1,   -1,   -1,   -1,   -1,   237,  -1,   -1,
    238,  -1,   -1,   -1,   -1,   239,  -1,   -1,   -1,   240,  -1,   -1,
    -1,   241,  -1,   -1,   -1,   -1,   242,  -1,   -1,   243,  -1,   -1,
    -1,   -1,   -1,   244,  245,  246,  247,  248,  249,  -1,   -1,   -1,
    250,  251,  -1,   252,  253,  254,  255,  256,  257,  258,  259,  260,
    261,  -1,   -1,   -1,   262,  263,  264,  265,  266,  -1,   -1,   -1,
    -1,   -1,   267,  -1,   -1,   268,  -1,   -1,   -1,   -1,   269,  -1,
    -1,   -1,   270,  -1,   -1,   -1,   271,  -1,   -1,   -1,   -1,   272,
    -1,   -1,   273,  -1,   -1,   -1,   -1,   -1,   -1,   274,  275,  276,
    277,  278,  -1,   -1,   279,  280,  281,  -1,   282,  283,  284,  285,
    -1,   286,  287,  288,  289,  290,  -1,   -1,   -1,   291,  292,  293,
    294,  295,  -1,   -1,   296,  -1,   -1,   297,  -1,   -1,   298,  -1,
    -1,   -1,   -1,   299,  -1,   -1,   -1,   300,  -1,   -1,   -1,   301,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   302,  -1,   -1,   -1,   -1,
    -1,   -1,   303,  304,  305,  306,  307,  -1,   308,  309,  310,  311,
    -1,   312,  313,  314,  -1,   -1,   315,  316,  317,  318,  319,  -1,
    -1,   -1,   320,  321,  322,  323,  324,  -1,   -1,   325,  -1,   -1,
    326,  -1,   -1,   327,  328,  -1,   -1,   -1,   329,  -1,   -1,   -1,
    -1,   -1,   -1,   -1,   330,  -1,   -1,   -1,   -1,   -1,   -1,   -1,
    331,  -1,   -1,   -1,   -1,   -1,   -1,   332,  333,  334,  335,  336,
    337,  338,  339,  340,  341,  -1,   342,  343,  -1,   -1,   -1,   344,
    345,  346,  347,  348,  -1,   -1,   -1,   349,  350,  351,  352,  353,
    -1,   -1,   354,  -1,   -1,   355,  -1,   -1,   -1,   356,  -1,   -1,
    -1,   357,  -1,   -1,   358,  -1,   -1,   -1,   -1,   359,  -1,   -1,
    -1,   -1,   -1,   -1,   -1,   360,  -1,   -1,   -1,   -1,   -1,   -1,
    361,  362,  363,  364,  365,  366,  367,  368,  369,  370,  -1,   371,
    -1,   -1,   -1,   -1,   372,  373,  374,  375,  -1,   -1,   -1,   -1,
    376,  377,  378,  379,  -1,   -1,   -1,   380,  -1,   -1,   381,  -1,
    -1,   -1,   382,  -1,   -1,   -1,   383,  -1,   -1,   384,  -1,   -1,
    -1,   -1,   385,  -1,   386,  -1,   -1,   -1,   -1,   -1,   387,  -1,
    -1,   -1,   -1,   -1,   -1,   388,  389,  390,  391,  392,  393,  394,
    395,  396,  397,  -1,   -1,   -1,   -1,   -1,   -1,   398,  399,  400,
    -1,   -1,   -1,   -1,   -1,   401,  402,  403,  -1,   -1,   -1,   -1,
    404,  -1,   -1,   405,  -1,   -1,   -1,   406,  -1,   -1,   -1,   407,
    -1,   -1,   408,  -1,   -1,   -1,   -1,   409,  -1,   410,  -1,   -1,
    -1,   -1,   -1,   411,  412,  413,  414,  -1,   -1,   -1,   -1,   -1,
    415,  416,  417,  -1,   -1,   -1,   -1,   -1,   -1,   418,  419,  420,
    421,  422,  423,  424,  425,  426,  427,  -1,   -1,   -1,   -1,   -1,
    428,  429,  430,  -1,   -1,   -1,   -1,   -1,   431,  -1,   -1,   432,
    -1,   -1,   -1,   -1,   433,  -1,   -1,   -1,   434,  -1,   -1,   -1,
    435,  -1,   -1,   -1,   -1,   436,  -1,   -1,   437,  438,  439,  440,
    -1,   -1,   -1,   -1,   441,  442,  443,  444,  -1,   -1,   -1,   -1,
    445,  -1,   446,  447,  448,  449,  450,  451,  452,  453,  454,  455,
    -1,   -1,   -1,   -1,   456,  457,  458,  459,  -1,   -1,   -1,   -1,
    -1,   460,  -1,   -1,   461,  -1,   -1,   -1,   -1,   462,  -1,   -1,
    -1,   463,  -1,   -1,   -1,   464,  -1,   -1,   -1,   -1,   465,  -1,
    466,  467,  468,  469,  470,  -1,   -1,   -1,   471,  472,  473,  474,
    475,  -1,   -1,   -1,   476,  477,  -1,   478,  479,  480,  481,  482,
    483,  484,  485,  486,  487,  -1,   -1,   -1,   488,  489,  490,  491,
    492,  -1,   -1,   -1,   -1,   -1,   493,  -1,   -1,   494,  -1,   -1,
    -1,   -1,   495,  -1,   -1,   -1,   496,  -1,   -1,   -1,   497,  -1,
    -1,   -1,   -1,   498,  -1,   499,  500,  501,  502,  503,  -1,   -1,
    -1,   504,  505,  506,  507,  508,  -1,   -1,   509,  510,  511,  -1,
    512,  513,  514,  515,  -1,   516,  517,  518,  519,  520,  -1,   -1,
    -1,   521,  522,  523,  524,  525,  -1,   -1,   526,  -1,   -1,   527,
    -1,   -1,   528,  -1,   -1,   -1,   -1,   529,  -1,   -1,   -1,   530,
    -1,   -1,   -1,   531,  -1,   -1,   -1,   -1,   -1,   -1,   532,  533,
    534,  535,  536,  -1,   -1,   -1,   537,  538,  539,  540,  541,  -1,
    542,  543,  544,  545,  -1,   546,  547,  548,  -1,   -1,   549,  550,
    551,  552,  553,  -1,   -1,   -1,   554,  555,  556,  557,  558,  -1,
    -1,   559,  -1,   -1,   560,  -1,   -1,   561,  562,  -1,   -1,   -1,
    563,  -1,   -1,   -1,   -1,   -1,   -1,   -1,   564,  -1,   -1,   -1,
    -1,   -1,   -1,   565,  566,  567,  568,  569,  -1,   -1,   -1,   570,
    571,  572,  573,  574,  575,  576,  577,  578,  579,  -1,   580,  581,
    -1,   -1,   -1,   582,  583,  584,  585,  586,  -1,   -1,   -1,   587,
    588,  589,  590,  591,  -1,   -1,   592,  -1,   -1,   593,  -1,   -1,
    -1,   594,  -1,   -1,   -1,   595,  -1,   -1,   596,  -1,   -1,   -1,
    -1,   597,  -1,   -1,   -1,   -1,   -1,   -1,   598,  599,  600,  601,
    -1,   -1,   -1,   -1,   602,  603,  604,  605,  606,  607,  608,  609,
    610,  611,  -1,   612,  -1,   -1,   -1,   -1,   613,  614,  615,  616,
    -1,   -1,   -1,   -1,   617,  618,  619,  620,  -1,   -1,   -1,   621,
    -1,   -1,   622,  -1,   -1,   -1,   623,  -1,   -1,   -1,   624,  -1,
    -1,   625,  -1,   -1,   -1,   -1,   626,  -1,   -1,   -1,   -1,   -1,
    -1,   627,  628,  629,  -1,   -1,   -1,   -1,   -1,   630,  631,  632,
    633,  634,  635,  636,  637,  638,  639,  -1,   -1,   -1,   -1,   -1,
    -1,   640,  641,  642,  -1,   -1,   -1,   -1,   -1,   643,  644,  645,
    -1,   -1,   -1,   -1,   646,  -1,   -1,   647,  -1,   -1,   -1,   648,
    -1,   -1,   -1,   649,  -1,   -1,   650,  -1,   -1,   -1,   -1,   651,
    652,  -1,   -1,   653,  -1,   -1,   -1,   -1,   654,  655,  656,  -1,
    -1,   -1,   -1,   -1,   657,  658,  659,  -1,   -1,   -1,   -1,   -1,
    -1,   660,  661,  662,  663,  664,  665,  666,  667,  668,  669,  -1,
    -1,   -1,   -1,   -1,   670,  671,  672,  -1,   -1,   -1,   -1,   -1,
    673,  -1,   -1,   674,  -1,   -1,   -1,   -1,   675,  -1,   -1,   -1,
    676,  -1,   -1,   -1,   -1,   677,  -1,   -1,   678,  -1,   -1,   -1,
    679,  680,  681,  682,  -1,   -1,   -1,   -1,   683,  684,  685,  686,
    -1,   -1,   -1,   -1,   687,  -1,   688,  689,  690,  691,  692,  693,
    694,  695,  696,  697,  -1,   -1,   -1,   -1,   698,  699,  700,  701,
    -1,   -1,   -1,   -1,   -1,   702,  -1,   -1,   703,  -1,   -1,   -1,
    -1,   704,  -1,   -1,   -1,   705,  -1,   -1,   -1,   -1,   706,  -1,
    -1,   707,  -1,   -1,   708,  709,  710,  711,  712,  -1,   -1,   -1,
    713,  714,  715,  716,  717,  -1,   -1,   -1,   718,  719,  -1,   720,
    721,  722,  723,  724,  725,  726,  727,  728,  729,  -1,   -1,   -1,
    730,  731,  732,  733,  734,  -1,   -1,   -1,   -1,   -1,   735,  -1,
    -1,   736,  -1,   -1,   -1,   -1,   737,  -1,   -1,   -1,   738,  -1,
    739,  -1,   -1,   740,  -1,   -1,   741,  -1,   -1,   742,  743,  744,
    745,  746,  -1,   -1,   -1,   747,  748,  749,  750,  751,  -1,   -1,
    752,  753,  754,  -1,   755,  756,  757,  758,  -1,   759,  760,  761,
    762,  763,  -1,   -1,   -1,   764,  765,  766,  767,  768,  -1,   -1,
    769,  -1,   -1,   770,  -1,   -1,   771,  -1,   -1,   -1,   -1,   772,
    -1,   -1,   -1,   773,  -1,   774,  -1,   -1,   775,  -1,   -1,   776,
    -1,   -1,   777,  778,  779,  780,  781,  -1,   -1,   -1,   782,  783,
    784,  785,  786,  -1,   787,  788,  789,  790,  -1,   791,  792,  793,
    -1,   -1,   794,  795,  796,  797,  798,  -1,   -1,   -1,   799,  800,
    801,  802,  803,  -1,   -1,   804,  -1,   -1,   805,  -1,   -1,   806,
    807,  -1,   -1,   -1,   808,  -1,   -1,   -1,   -1,   -1,   809,  -1,
    -1,   810,  -1,   -1,   -1,   -1,   -1,   811,  812,  813,  814,  815,
    -1,   -1,   -1,   816,  817,  818,  819,  820,  821,  822,  823,  824,
    825,  -1,   826,  827,  -1,   -1,   -1,   828,  829,  830,  831,  832,
    -1,   -1,   -1,   833,  834,  835,  836,  837,  -1,   -1,   838,  -1,
    -1,   839,  -1,   -1,   -1,   840,  -1,   -1,   -1,   841,  -1,   -1,
    -1,   -1,   -1,   842,  -1,   -1,   843,  -1,   -1,   -1,   -1,   -1,
    844,  845,  846,  847,  -1,   -1,   -1,   -1,   848,  849,  850,  851,
    852,  853,  854,  855,  856,  857,  -1,   858,  -1,   -1,   -1,   -1,
    859,  860,  861,  862,  -1,   -1,   -1,   -1,   863,  864,  865,  866,
    -1,   -1,   -1,   867,  -1,   -1,   868,  -1,   -1,   -1,   869,  -1,
    -1,   -1,   870,  -1,   -1,   -1,   -1,   -1,   871,  -1,   -1,   872,
    -1,   -1,   -1,   -1,   -1,   873,  874,  875,  -1,   -1,   -1,   -1,
    -1,   876,  877,  878,  879,  880,  881,  882,  883,  884,  885,  -1,
    -1,   -1,   -1,   -1,   -1,   886,  887,  888,  -1,   -1,   -1,   -1,
    -1,   889,  890,  891,  -1,   -1,   -1,   -1,   892,  -1,   -1,   893,
    -1,   -1,   -1,   894,  -1,   -1,   -1,   895,  896,  -1,   -1,   -1,
    897,  -1,   -1,   -1,   898,  -1,   -1,   899,  -1,   -1,   -1,   -1,
    900,  901,  902,  -1,   -1,   -1,   -1,   -1,   903,  904,  905,  -1,
    -1,   -1,   -1,   -1,   -1,   906,  907,  908,  909,  910,  911,  912,
    913,  914,  915,  -1,   -1,   -1,   -1,   -1,   916,  917,  918,  -1,
    -1,   -1,   -1,   -1,   919,  -1,   -1,   920,  -1,   -1,   -1,   -1,
    -1,   921,  -1,   -1,   -1,   922,  -1,   -1,   -1,   923,  -1,   -1,
    924,  -1,   -1,   -1,   925,  926,  927,  928,  -1,   -1,   -1,   -1,
    929,  930,  931,  932,  -1,   -1,   -1,   -1,   933,  -1,   934,  935,
    936,  937,  938,  939,  940,  941,  942,  943,  -1,   -1,   -1,   -1,
    944,  945,  946,  947,  -1,   -1,   -1,   -1,   -1,   948,  -1,   -1,
    949,  -1,   -1,   -1,   -1,   -1,   950,  -1,   -1,   -1,   951,  -1,
    -1,   -1,   952,  -1,   -1,   953,  -1,   -1,   954,  955,  956,  957,
    958,  -1,   -1,   -1,   959,  960,  961,  962,  963,  -1,   -1,   -1,
    964,  965,  -1,   966,  967,  968,  969,  970,  971,  972,  973,  974,
    975,  -1,   -1,   -1,   976,  977,  978,  979,  980,  -1,   -1,   -1,
    -1,   -1,   981,  -1,   -1,   982,  -1,   -1,   -1,   -1,   -1,   983,
    -1,   -1,   -1,   984,  985,  -1,   -1,   986,  -1,   -1,   987,  -1,
    -1,   988,  989,  990,  991,  992,  -1,   -1,   -1,   993,  994,  995,
    996,  997,  -1,   -1,   998,  999,  1000, -1,   1001, 1002, 1003, 1004,
    -1,   1005, 1006, 1007, 1008, 1009, -1,   -1,   -1,   1010, 1011, 1012,
    1013, 1014, -1,   -1,   1015, -1,   -1,   1016, -1,   -1,   1017, -1,
    1018, -1,   -1,   -1,   1019, -1,   -1,   -1,   -1,   1020, -1,   -1,
    1021, -1,   -1,   1022, -1,   -1,   1023, 1024, 1025, 1026, 1027, -1,
    -1,   -1,   1028, 1029, 1030, 1031, 1032, -1,   1033, 1034, 1035, 1036,
    -1,   1037, 1038, 1039, -1,   -1,   1040, 1041, 1042, 1043, 1044, -1,
    -1,   -1,   1045, 1046, 1047, 1048, 1049, -1,   -1,   1050, -1,   -1,
    1051, -1,   -1,   1052, -1,   1053, -1,   -1,   -1,   1054, -1,   -1,
    -1,   -1,   1055, -1,   -1,   1056, -1,   -1,   -1,   -1,   -1,   1057,
    1058, 1059, 1060, 1061, -1,   -1,   -1,   1062, 1063, 1064, 1065, 1066,
    1067, 1068, 1069, 1070, 1071, -1,   1072, 1073, -1,   -1,   -1,   1074,
    1075, 1076, 1077, 1078, -1,   -1,   -1,   1079, 1080, 1081, 1082, 1083,
    -1,   -1,   1084, -1,   -1,   1085, -1,   -1,   -1,   -1,   1086, -1,
    -1,   -1,   1087, -1,   -1,   -1,   -1,   1088, -1,   -1,   1089, -1,
    -1,   -1,   -1,   -1,   1090, 1091, 1092, 1093, -1,   -1,   -1,   -1,
    1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, -1,   1104,
    -1,   -1,   -1,   -1,   1105, 1106, 1107, 1108, -1,   -1,   -1,   -1,
    1109, 1110, 1111, 1112, -1,   -1,   -1,   1113, -1,   -1,   1114, -1,
    -1,   -1,   -1,   1115, -1,   -1,   -1,   1116, -1,   -1,   -1,   -1,
    1117, -1,   -1,   1118, -1,   -1,   -1,   -1,   -1,   1119, 1120, 1121,
    -1,   -1,   -1,   -1,   -1,   1122, 1123, 1124, 1125, 1126, 1127, 1128,
    1129, 1130, 1131, -1,   -1,   -1,   -1,   -1,   -1,   1132, 1133, 1134,
    -1,   -1,   -1,   -1,   -1,   1135, 1136, 1137, -1,   -1,   -1,   -1,
    1138, -1,   -1,   1139, 1140, -1,   -1,   -1,   -1,   1141, -1,   -1,
    1142, -1,   -1,   -1,   1143, -1,   -1,   -1,   1144, -1,   -1,   1145,
    -1,   -1,   -1,   -1,   1146, 1147, 1148, -1,   -1,   -1,   -1,   -1,
    1149, 1150, 1151, -1,   -1,   -1,   -1,   -1,   -1,   1152, 1153, 1154,
    1155, 1156, 1157, 1158, 1159, 1160, 1161, -1,   -1,   -1,   -1,   -1,
    1162, 1163, 1164, -1,   -1,   -1,   -1,   -1,   -1,   1165, -1,   -1,
    -1,   -1,   1166, -1,   -1,   1167, -1,   -1,   -1,   1168, -1,   -1,
    -1,   1169, -1,   -1,   1170, -1,   -1,   -1,   1171, 1172, 1173, 1174,
    -1,   -1,   -1,   -1,   1175, 1176, 1177, 1178, -1,   -1,   -1,   -1,
    1179, -1,   1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189,
    -1,   -1,   -1,   -1,   1190, 1191, 1192, 1193, -1,   -1,   -1,   -1,
    -1,   -1,   1194, -1,   -1,   -1,   -1,   1195, -1,   -1,   1196, -1,
    -1,   -1,   1197, -1,   -1,   -1,   1198, -1,   -1,   1199, -1,   -1,
    1200, 1201, 1202, 1203, 1204, -1,   -1,   -1,   1205, 1206, 1207, 1208,
    1209, -1,   -1,   -1,   1210, 1211, -1,   1212, 1213, 1214, 1215, 1216,
    1217, 1218, 1219, 1220, 1221, -1,   -1,   -1,   1222, 1223, 1224, 1225,
    1226, -1,   -1,   -1,   -1,   -1,   -1,   1227, -1,   -1,   -1,   -1,
    -1,   -1,   -1,   1228, -1,   -1,   -1,   1229, 1230, -1,   -1,   1231,
    -1,   -1,   1232, -1,   -1,   1233, 1234, 1235, 1236, 1237, -1,   -1,
    -1,   1238, 1239, 1240, 1241, 1242, -1,   -1,   1243, 1244, 1245, -1,
    1246, 1247, 1248, 1249, -1,   1250, 1251, 1252, 1253, 1254, -1,   -1,
    -1,   1255, 1256, 1257, 1258, 1259, -1,   -1,   -1,   -1,   -1,   -1,
    1260, -1,   -1,   -1,   1261, -1,   -1,   -1,   1262, -1,   -1,   -1,
    -1,   1263, -1,   -1,   1264, -1,   -1,   1265, -1,   -1,   1266, 1267,
    1268, 1269, 1270, -1,   -1,   -1,   1271, 1272, 1273, 1274, 1275, -1,
    1276, 1277, 1278, 1279, -1,   1280, 1281, 1282, -1,   -1,   1283, 1284,
    1285, 1286, 1287, -1,   -1,   -1,   1288, 1289, 1290, 1291, 1292, -1,
    1293, -1,   -1,   -1,   -1,   1294, -1,   -1,   -1,   1295, -1,   -1,
    -1,   1296, -1,   -1,   -1,   -1,   1297, -1,   -1,   1298, -1,   -1,
    -1,   -1,   -1,   1299, 1300, 1301, 1302, 1303, -1,   -1,   -1,   1304,
    1305, 1306, 1307, 1308, 1309, 1310, 1311, 1312, 1313, -1,   1314, 1315,
    -1,   -1,   -1,   1316, 1317, 1318, 1319, 1320, -1,   -1,   -1,   1321,
    1322, 1323, 1324, 1325, -1,   1326, -1,   -1,   -1,   -1,   1327, -1,
    -1,   -1,   1328, -1,   -1,   -1,   1329, -1,   -1,   -1,   -1,   1330,
    -1,   -1,   1331, -1,   -1,   -1,   -1,   -1,   1332, 1333, 1334, 1335,
    -1,   -1,   -1,   -1,   1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343,
    1344, 1345, -1,   1346, -1,   -1,   -1,   -1,   1347, 1348, 1349, 1350,
    -1,   -1,   -1,   -1,   1351, 1352, 1353, 1354, -1,   -1,   1355, -1,
    -1,   -1,   -1,   1356, -1,   -1,   -1,   1357, -1,   -1,   -1,   1358,
    -1,   -1,   -1,   -1,   1359, -1,   -1,   1360, -1,   -1,   -1,   -1,
    -1,   1361, 1362, 1363, -1,   -1,   -1,   -1,   -1,   1364, 1365, 1366,
    1367, 1368, 1369, 1370, 1371, 1372, 1373, -1,   -1,   -1,   -1,   -1,
    -1,   1374, 1375, 1376, -1,   -1,   -1,   -1,   -1,   1377, 1378, 1379,
    1380, -1,   -1,   -1,   -1,   -1,   1381, -1,   1382, -1,   -1,   -1,
    -1,   1383, -1,   -1,   1384, -1,   -1,   -1,   1385, -1,   -1,   -1,
    1386, -1,   -1,   1387, -1,   -1,   -1,   -1,   1388, 1389, 1390, -1,
    -1,   -1,   -1,   -1,   1391, 1392, 1393, -1,   -1,   -1,   -1,   -1,
    -1,   1394, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, -1,
    -1,   -1,   -1,   -1,   -1,   1404, -1,   -1,   -1,   -1,   -1,   1405,
    -1,   1406, -1,   -1,   -1,   -1,   1407, -1,   -1,   1408, -1,   -1,
    -1,   1409, -1,   -1,   -1,   1410, -1,   -1,   1411, -1,   -1,   -1,
    1412, 1413, 1414, 1415, -1,   -1,   -1,   -1,   1416, 1417, 1418, 1419,
    -1,   -1,   -1,   -1,   1420, -1,   1421, 1422, 1423, 1424, 1425, 1426,
    1427, 1428, 1429, 1430, -1,   -1,   -1,   -1,   -1,   -1,   1431, -1,
    -1,   -1,   -1,   -1,   -1,   -1,   1432, -1,   -1,   -1,   -1,   1433,
    -1,   -1,   1434, -1,   -1,   -1,   1435, -1,   -1,   -1,   1436, -1,
    -1,   1437, -1,   -1,   1438, 1439, 1440, 1441, 1442, -1,   -1,   -1,
    1443, 1444, 1445, 1446, 1447, -1,   -1,   -1,   1448, 1449, -1,   1450,
    1451, 1452, 1453, 1454, 1455, 1456, 1457, 1458, 1459, -1,   -1,   -1,
    -1,   -1,   -1,   1460, -1,   -1,   -1,   -1,   -1,   -1,   -1,   1461,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   1462, -1,   -1,   -1,   1463,
    1464, -1,   -1,   1465, -1,   -1,   1466, -1,   -1,   1467, 1468, 1469,
    1470, 1471, -1,   -1,   -1,   1472, 1473, 1474, 1475, 1476, -1,   -1,
    1477, 1478, 1479, -1,   1480, 1481, 1482, 1483, -1,   1484, 1485, 1486,
    1487, 1488, -1,   -1,   -1,   -1,   -1,   -1,   1489, -1,   -1,   -1,
    -1,   -1,   -1,   -1,   1490, -1,   -1,   -1,   1491, -1,   -1,   -1,
    1492, -1,   -1,   -1,   -1,   1493, -1,   -1,   1494, -1,   -1,   1495,
    -1,   -1,   1496, 1497, 1498, 1499, 1500, -1,   -1,   -1,   1501, 1502,
    1503, 1504, 1505, -1,   1506, 1507, 1508, 1509, -1,   1510, 1511, 1512,
    -1,   -1,   1513, 1514, 1515, 1516, 1517, -1,   -1,   -1,   -1,   -1,
    -1,   1518, -1,   -1,   1519, -1,   -1,   -1,   -1,   1520, -1,   -1,
    -1,   1521, -1,   -1,   -1,   1522, -1,   -1,   -1,   -1,   1523, -1,
    -1,   1524, -1,   -1,   -1,   -1,   -1,   1525, 1526, 1527, 1528, 1529,
    -1,   -1,   -1,   1530, 1531, 1532, 1533, 1534, 1535, 1536, 1537, 1538,
    1539, -1,   1540, 1541, -1,   -1,   -1,   1542, 1543, 1544, 1545, 1546,
    1547, -1,   -1,   -1,   -1,   -1,   1548, -1,   -1,   1549, -1,   -1,
    -1,   -1,   1550, -1,   -1,   -1,   1551, -1,   -1,   -1,   1552, -1,
    -1,   -1,   -1,   1553, -1,   -1,   1554, -1,   -1,   -1,   -1,   -1,
    1555, 1556, 1557, 1558, -1,   -1,   -1,   -1,   1559, 1560, 1561, 1562,
    1563, 1564, 1565, 1566, 1567, 1568, -1,   1569, -1,   -1,   -1,   -1,
    1570, 1571, 1572, 1573, -1,   1574, -1,   -1,   -1,   -1,   -1,   1575,
    -1,   -1,   1576, -1,   -1,   -1,   -1,   1577, -1,   -1,   -1,   1578,
    -1,   -1,   -1,   1579, -1,   -1,   -1,   -1,   1580, -1,   -1,   1581,
    -1,   -1,   -1,   -1,   -1,   1582, 1583, 1584, -1,   -1,   -1,   -1,
    -1,   1585, 1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594, -1,
    -1,   -1,   -1,   -1,   -1,   1595, 1596, 1597, 1598, -1,   -1,   -1,
    -1,   -1,   -1,   1599, 1600, -1,   -1,   -1,   -1,   -1,   1601, -1,
    1602, -1,   -1,   -1,   -1,   1603, -1,   -1,   1604, -1,   -1,   -1,
    1605, -1,   -1,   -1,   1606, -1,   -1,   1607, -1,   -1,   -1,   -1,
    1608, 1609, 1610, -1,   -1,   -1,   -1,   -1,   1611, 1612, 1613, -1,
    -1,   -1,   -1,   -1,   -1,   1614, 1615, 1616, 1617, 1618, 1619, 1620,
    -1,   1621, -1,   -1,   -1,   -1,   -1,   -1,   -1,   1622, -1,   -1,
    -1,   -1,   -1,   1623, -1,   1624, -1,   -1,   -1,   -1,   1625, -1,
    -1,   1626, -1,   -1,   -1,   1627, -1,   -1,   -1,   1628, -1,   -1,
    1629, -1,   -1,   -1,   1630, 1631, 1632, 1633, -1,   -1,   -1,   -1,
    1634, 1635, 1636, 1637, -1,   -1,   -1,   -1,   1638, -1,   1639, 1640,
    1641, 1642, 1643, 1644, -1,   -1,   1645, -1,   -1,   -1,   -1,   -1,
    -1,   -1,   1646, -1,   -1,   -1,   -1,   -1,   -1,   -1,   1647, -1,
    -1,   -1,   -1,   1648, -1,   -1,   1649, -1,   -1,   -1,   1650, -1,
    -1,   -1,   1651, -1,   -1,   1652, -1,   -1,   1653, 1654, 1655, 1656,
    1657, -1,   -1,   -1,   1658, 1659, 1660, 1661, 1662, -1,   -1,   -1,
    1663, 1664, -1,   1665, 1666, 1667, 1668, 1669, -1,   -1,   -1,   1670,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   1671, -1,   -1,   -1,   -1,
    -1,   -1,   -1,   1672, -1,   -1,   -1,   -1,   -1,   -1,   -1,   1673,
    -1,   -1,   -1,   1674, 1675, -1,   -1,   1676, -1,   -1,   1677, -1,
    -1,   1678, 1679, 1680, 1681, 1682, -1,   -1,   -1,   1683, 1684, 1685,
    1686, 1687, -1,   -1,   1688, 1689, 1690, -1,   1691, 1692, 1693, 1694,
    -1,   -1,   -1,   -1,   1695, -1,   -1,   -1,   -1,   -1,   -1,   -1,
    1696, -1,   -1,   -1,   -1,   -1,   -1,   -1,   1697, -1,   -1,   -1,
    1698, -1,   -1,   -1,   1699, -1,   -1,   -1,   -1,   1700, -1,   -1,
    1701, -1,   -1,   1702, -1,   -1,   1703, 1704, 1705, 1706, 1707, -1,
    -1,   -1,   1708, 1709, 1710, 1711, 1712, -1,   1713, 1714, 1715, 1716,
    -1,   1717, 1718, 1719, -1,   -1,   -1,   -1,   -1,   1720, -1,   -1,
    -1,   -1,   -1,   -1,   -1,   1721, -1,   -1,   1722, -1,   -1,   -1,
    -1,   1723, -1,   -1,   -1,   1724, -1,   -1,   -1,   1725, -1,   -1,
    -1,   -1,   1726, -1,   -1,   1727, -1,   -1,   -1,   -1,   -1,   1728,
    1729, 1730, 1731, 1732, -1,   -1,   -1,   1733, 1734, 1735, 1736, 1737,
    1738, 1739, 1740, 1741, 1742, -1,   1743, 1744, -1,   -1,   -1,   -1,
    -1,   -1,   1745, -1,   1746, -1,   -1,   -1,   -1,   -1,   1747, -1,
    -1,   1748, -1,   -1,   -1,   -1,   1749, -1,   -1,   -1,   1750, -1,
    -1,   -1,   1751, -1,   -1,   -1,   -1,   1752, -1,   -1,   1753, -1,
    -1,   -1,   -1,   -1,   1754, 1755, 1756, 1757, -1,   -1,   -1,   -1,
    1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, -1,   1768,
    1769, -1,   -1,   -1,   -1,   -1,   -1,   1770, -1,   1771, -1,   -1,
    -1,   -1,   -1,   1772, -1,   -1,   1773, -1,   -1,   -1,   -1,   1774,
    -1,   -1,   -1,   1775, -1,   -1,   -1,   1776, -1,   -1,   -1,   -1,
    1777, -1,   -1,   1778, -1,   -1,   -1,   -1,   -1,   1779, 1780, 1781,
    -1,   -1,   -1,   -1,   -1,   1782, 1783, 1784, 1785, 1786, 1787, 1788,
    1789, 1790, 1791, -1,   1792, 1793, 1794, 1795, 1796, 1797, -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   1798, 1799, 1800, 1801, 1802, 1803, 1804, 1805,
    1806, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   1807, 1808, 1809, 1810, 1811,
    1812, 1813, 1814, 1815, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   1816, 1817,
    1818, 1819, 1820, 1821, 1822, 1823, 1824, -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1833, -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   1834, 1835, 1836, 1837, 1838, 1839, 1840, 1841,
    1842, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   1843, 1844, 1845, 1846, 1847,
    1848, 1849, 1850, 1851, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   1852, 1853,
    1854, 1855, 1856, 1857};

}  // namespace lczero


























```

`src/neural/shared/policy_map.h`:

```h
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2019 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

namespace lczero {

// 73x8x8.
const short kConvPolicyMap[] = {
    7,    31,   56,   81,   106,  131,  156,  180,  204,  230,  259,  288,
    317,  346,  374,  400,  425,  453,  485,  518,  551,  584,  615,  642,
    667,  695,  727,  761,  796,  830,  861,  888,  913,  941,  973,  1007,
    1042, 1076, 1107, 1134, 1159, 1187, 1219, 1252, 1285, 1318, 1349, 1376,
    1401, 1428, 1457, 1486, 1515, 1544, 1572, 1597, -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   10,   35,   61,   86,   111,  136,  160,  183,
    207,  234,  264,  293,  322,  351,  378,  403,  428,  457,  490,  523,
    556,  589,  619,  645,  670,  699,  732,  766,  801,  835,  865,  891,
    916,  945,  978,  1012, 1047, 1081, 1111, 1137, 1162, 1191, 1224, 1257,
    1290, 1323, 1353, 1379, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   13,   38,   64,   90,
    115,  140,  163,  185,  210,  237,  267,  297,  326,  355,  381,  405,
    431,  460,  493,  527,  560,  593,  622,  647,  673,  702,  735,  770,
    805,  839,  868,  893,  919,  948,  981,  1016, 1051, 1085, 1114, 1139,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    15,   40,   66,   92,   118,  142,  165,  187,  212,  239,  269,  299,
    329,  357,  383,  407,  433,  462,  495,  529,  563,  595,  624,  649,
    675,  704,  737,  772,  808,  841,  870,  895,  -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   17,   42,   68,   94,   119,  144,  167,  189,
    214,  241,  271,  301,  330,  359,  385,  409,  435,  464,  497,  531,
    564,  597,  626,  651,  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   19,   44,   70,   95,
    120,  145,  169,  191,  216,  243,  273,  302,  331,  360,  387,  411,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    21,   46,   71,   96,   121,  146,  170,  193,  -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   8,    32,   57,   82,   107,  132,  157,  -1,
    205,  231,  260,  289,  318,  347,  375,  -1,   426,  454,  486,  519,
    552,  585,  616,  -1,   668,  696,  728,  762,  797,  831,  862,  -1,
    914,  942,  974,  1008, 1043, 1077, 1108, -1,   1160, 1188, 1220, 1253,
    1286, 1319, 1350, -1,   1402, 1429, 1458, 1487, 1516, 1545, 1573, -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   12,   37,   63,   88,
    113,  138,  -1,   -1,   209,  236,  266,  295,  324,  353,  -1,   -1,
    430,  459,  492,  525,  558,  591,  -1,   -1,   672,  701,  734,  768,
    803,  837,  -1,   -1,   918,  947,  980,  1014, 1049, 1083, -1,   -1,
    1164, 1193, 1226, 1259, 1292, 1325, -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    14,   39,   65,   91,   116,  -1,   -1,   -1,   211,  238,  268,  298,
    327,  -1,   -1,   -1,   432,  461,  494,  528,  561,  -1,   -1,   -1,
    674,  703,  736,  771,  806,  -1,   -1,   -1,   920,  949,  982,  1017,
    1052, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   16,   41,   67,   93,   -1,   -1,   -1,   -1,
    213,  240,  270,  300,  -1,   -1,   -1,   -1,   434,  463,  496,  530,
    -1,   -1,   -1,   -1,   676,  705,  738,  773,  -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   18,   43,   69,   -1,
    -1,   -1,   -1,   -1,   215,  242,  272,  -1,   -1,   -1,   -1,   -1,
    436,  465,  498,  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    20,   45,   -1,   -1,   -1,   -1,   -1,   -1,   217,  244,  -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   22,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   0,    24,   49,   75,
    101,  127,  153,  -1,   197,  223,  252,  282,  312,  342,  371,  -1,
    418,  446,  478,  512,  546,  580,  612,  -1,   660,  688,  720,  755,
    791,  826,  858,  -1,   906,  934,  966,  1001, 1037, 1072, 1104, -1,
    1152, 1180, 1212, 1246, 1280, 1314, 1346, -1,   1394, 1421, 1450, 1480,
    1510, 1540, 1569, -1,   1614, 1639, 1665, 1691, 1717, 1743, 1768, -1,
    1,    25,   50,   76,   102,  128,  -1,   -1,   198,  224,  253,  283,
    313,  343,  -1,   -1,   419,  447,  479,  513,  547,  581,  -1,   -1,
    661,  689,  721,  756,  792,  827,  -1,   -1,   907,  935,  967,  1002,
    1038, 1073, -1,   -1,   1153, 1181, 1213, 1247, 1281, 1315, -1,   -1,
    1395, 1422, 1451, 1481, 1511, 1541, -1,   -1,   1615, 1640, 1666, 1692,
    1718, 1744, -1,   -1,   2,    26,   51,   77,   103,  -1,   -1,   -1,
    199,  225,  254,  284,  314,  -1,   -1,   -1,   420,  448,  480,  514,
    548,  -1,   -1,   -1,   662,  690,  722,  757,  793,  -1,   -1,   -1,
    908,  936,  968,  1003, 1039, -1,   -1,   -1,   1154, 1182, 1214, 1248,
    1282, -1,   -1,   -1,   1396, 1423, 1452, 1482, 1512, -1,   -1,   -1,
    1616, 1641, 1667, 1693, 1719, -1,   -1,   -1,   3,    27,   52,   78,
    -1,   -1,   -1,   -1,   200,  226,  255,  285,  -1,   -1,   -1,   -1,
    421,  449,  481,  515,  -1,   -1,   -1,   -1,   663,  691,  723,  758,
    -1,   -1,   -1,   -1,   909,  937,  969,  1004, -1,   -1,   -1,   -1,
    1155, 1183, 1215, 1249, -1,   -1,   -1,   -1,   1397, 1424, 1453, 1483,
    -1,   -1,   -1,   -1,   1617, 1642, 1668, 1694, -1,   -1,   -1,   -1,
    4,    28,   53,   -1,   -1,   -1,   -1,   -1,   201,  227,  256,  -1,
    -1,   -1,   -1,   -1,   422,  450,  482,  -1,   -1,   -1,   -1,   -1,
    664,  692,  724,  -1,   -1,   -1,   -1,   -1,   910,  938,  970,  -1,
    -1,   -1,   -1,   -1,   1156, 1184, 1216, -1,   -1,   -1,   -1,   -1,
    1398, 1425, 1454, -1,   -1,   -1,   -1,   -1,   1618, 1643, 1669, -1,
    -1,   -1,   -1,   -1,   5,    29,   -1,   -1,   -1,   -1,   -1,   -1,
    202,  228,  -1,   -1,   -1,   -1,   -1,   -1,   423,  451,  -1,   -1,
    -1,   -1,   -1,   -1,   665,  693,  -1,   -1,   -1,   -1,   -1,   -1,
    911,  939,  -1,   -1,   -1,   -1,   -1,   -1,   1157, 1185, -1,   -1,
    -1,   -1,   -1,   -1,   1399, 1426, -1,   -1,   -1,   -1,   -1,   -1,
    1619, 1644, -1,   -1,   -1,   -1,   -1,   -1,   6,    -1,   -1,   -1,
    -1,   -1,   -1,   -1,   203,  -1,   -1,   -1,   -1,   -1,   -1,   -1,
    424,  -1,   -1,   -1,   -1,   -1,   -1,   -1,   666,  -1,   -1,   -1,
    -1,   -1,   -1,   -1,   912,  -1,   -1,   -1,   -1,   -1,   -1,   -1,
    1158, -1,   -1,   -1,   -1,   -1,   -1,   -1,   1400, -1,   -1,   -1,
    -1,   -1,   -1,   -1,   1620, -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   195,  220,  248,  277,
    306,  335,  364,  -1,   416,  443,  474,  507,  540,  573,  605,  -1,
    658,  685,  716,  750,  785,  819,  851,  -1,   904,  931,  962,  996,
    1031, 1065, 1097, -1,   1150, 1177, 1208, 1241, 1274, 1307, 1339, -1,
    1392, 1418, 1446, 1475, 1504, 1533, 1562, -1,   1612, 1636, 1661, 1686,
    1711, 1736, 1761, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   414,  440,  470,  503,
    536,  569,  -1,   -1,   656,  682,  712,  746,  781,  815,  -1,   -1,
    902,  928,  958,  992,  1027, 1061, -1,   -1,   1148, 1174, 1204, 1237,
    1270, 1303, -1,   -1,   1390, 1415, 1442, 1471, 1500, 1529, -1,   -1,
    1610, 1633, 1657, 1682, 1707, 1732, -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   653,  678,  707,  741,
    776,  -1,   -1,   -1,   899,  924,  953,  987,  1022, -1,   -1,   -1,
    1145, 1170, 1199, 1232, 1265, -1,   -1,   -1,   1387, 1411, 1437, 1466,
    1495, -1,   -1,   -1,   1607, 1629, 1652, 1677, 1702, -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   897,  922,  951,  984,
    -1,   -1,   -1,   -1,   1143, 1168, 1197, 1229, -1,   -1,   -1,   -1,
    1385, 1409, 1435, 1463, -1,   -1,   -1,   -1,   1605, 1627, 1650, 1674,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   1141, 1166, 1195, -1,
    -1,   -1,   -1,   -1,   1383, 1407, 1433, -1,   -1,   -1,   -1,   -1,
    1603, 1625, 1648, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   1381, 1405, -1,   -1,
    -1,   -1,   -1,   -1,   1601, 1623, -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   1599, -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    194,  219,  247,  276,  305,  334,  363,  390,  415,  442,  473,  506,
    539,  572,  604,  632,  657,  684,  715,  749,  784,  818,  850,  878,
    903,  930,  961,  995,  1030, 1064, 1096, 1124, 1149, 1176, 1207, 1240,
    1273, 1306, 1338, 1366, 1391, 1417, 1445, 1474, 1503, 1532, 1561, 1587,
    1611, 1635, 1660, 1685, 1710, 1735, 1760, 1784, -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    412,  438,  468,  501,  534,  567,  600,  629,  654,  680,  710,  744,
    779,  813,  846,  875,  900,  926,  956,  990,  1025, 1059, 1092, 1121,
    1146, 1172, 1202, 1235, 1268, 1301, 1334, 1363, 1388, 1413, 1440, 1469,
    1498, 1527, 1557, 1584, 1608, 1631, 1655, 1680, 1705, 1730, 1756, 1781,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    652,  677,  706,  740,  775,  810,  843,  872,  898,  923,  952,  986,
    1021, 1056, 1089, 1118, 1144, 1169, 1198, 1231, 1264, 1298, 1331, 1360,
    1386, 1410, 1436, 1465, 1494, 1524, 1554, 1581, 1606, 1628, 1651, 1676,
    1701, 1727, 1753, 1778, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    896,  921,  950,  983,  1019, 1054, 1087, 1116, 1142, 1167, 1196, 1228,
    1262, 1296, 1329, 1358, 1384, 1408, 1434, 1462, 1492, 1522, 1552, 1579,
    1604, 1626, 1649, 1673, 1699, 1725, 1751, 1776, -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    1140, 1165, 1194, 1227, 1260, 1294, 1327, 1356, 1382, 1406, 1432, 1461,
    1490, 1520, 1550, 1577, 1602, 1624, 1647, 1672, 1697, 1723, 1749, 1774,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    1380, 1404, 1431, 1460, 1489, 1518, 1548, 1575, 1600, 1622, 1646, 1671,
    1696, 1721, 1747, 1772, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    1598, 1621, 1645, 1670, 1695, 1720, 1745, 1770, -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   218,  246,  275,  304,  333,  362,  389,
    -1,   441,  472,  505,  538,  571,  603,  631,  -1,   683,  714,  748,
    783,  817,  849,  877,  -1,   929,  960,  994,  1029, 1063, 1095, 1123,
    -1,   1175, 1206, 1239, 1272, 1305, 1337, 1365, -1,   1416, 1444, 1473,
    1502, 1531, 1560, 1586, -1,   1634, 1659, 1684, 1709, 1734, 1759, 1783,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   466,  499,  532,  565,  598,  627,
    -1,   -1,   708,  742,  777,  811,  844,  873,  -1,   -1,   954,  988,
    1023, 1057, 1090, 1119, -1,   -1,   1200, 1233, 1266, 1299, 1332, 1361,
    -1,   -1,   1438, 1467, 1496, 1525, 1555, 1582, -1,   -1,   1653, 1678,
    1703, 1728, 1754, 1779, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   739,  774,  809,  842,  871,
    -1,   -1,   -1,   985,  1020, 1055, 1088, 1117, -1,   -1,   -1,   1230,
    1263, 1297, 1330, 1359, -1,   -1,   -1,   1464, 1493, 1523, 1553, 1580,
    -1,   -1,   -1,   1675, 1700, 1726, 1752, 1777, -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   1018, 1053, 1086, 1115,
    -1,   -1,   -1,   -1,   1261, 1295, 1328, 1357, -1,   -1,   -1,   -1,
    1491, 1521, 1551, 1578, -1,   -1,   -1,   -1,   1698, 1724, 1750, 1775,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   1293, 1326, 1355,
    -1,   -1,   -1,   -1,   -1,   1519, 1549, 1576, -1,   -1,   -1,   -1,
    -1,   1722, 1748, 1773, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   1547, 1574,
    -1,   -1,   -1,   -1,   -1,   -1,   1746, 1771, -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   1769,
    -1,   23,   48,   74,   100,  126,  152,  177,  -1,   222,  251,  281,
    311,  341,  370,  397,  -1,   445,  477,  511,  545,  579,  611,  639,
    -1,   687,  719,  754,  790,  825,  857,  885,  -1,   933,  965,  1000,
    1036, 1071, 1103, 1131, -1,   1179, 1211, 1245, 1279, 1313, 1345, 1373,
    -1,   1420, 1449, 1479, 1509, 1539, 1568, 1594, -1,   1638, 1664, 1690,
    1716, 1742, 1767, 1791, -1,   -1,   47,   73,   99,   125,  151,  176,
    -1,   -1,   250,  280,  310,  340,  369,  396,  -1,   -1,   476,  510,
    544,  578,  610,  638,  -1,   -1,   718,  753,  789,  824,  856,  884,
    -1,   -1,   964,  999,  1035, 1070, 1102, 1130, -1,   -1,   1210, 1244,
    1278, 1312, 1344, 1372, -1,   -1,   1448, 1478, 1508, 1538, 1567, 1593,
    -1,   -1,   1663, 1689, 1715, 1741, 1766, 1790, -1,   -1,   -1,   72,
    98,   124,  150,  175,  -1,   -1,   -1,   279,  309,  339,  368,  395,
    -1,   -1,   -1,   509,  543,  577,  609,  637,  -1,   -1,   -1,   752,
    788,  823,  855,  883,  -1,   -1,   -1,   998,  1034, 1069, 1101, 1129,
    -1,   -1,   -1,   1243, 1277, 1311, 1343, 1371, -1,   -1,   -1,   1477,
    1507, 1537, 1566, 1592, -1,   -1,   -1,   1688, 1714, 1740, 1765, 1789,
    -1,   -1,   -1,   -1,   97,   123,  149,  174,  -1,   -1,   -1,   -1,
    308,  338,  367,  394,  -1,   -1,   -1,   -1,   542,  576,  608,  636,
    -1,   -1,   -1,   -1,   787,  822,  854,  882,  -1,   -1,   -1,   -1,
    1033, 1068, 1100, 1128, -1,   -1,   -1,   -1,   1276, 1310, 1342, 1370,
    -1,   -1,   -1,   -1,   1506, 1536, 1565, 1591, -1,   -1,   -1,   -1,
    1713, 1739, 1764, 1788, -1,   -1,   -1,   -1,   -1,   122,  148,  173,
    -1,   -1,   -1,   -1,   -1,   337,  366,  393,  -1,   -1,   -1,   -1,
    -1,   575,  607,  635,  -1,   -1,   -1,   -1,   -1,   821,  853,  881,
    -1,   -1,   -1,   -1,   -1,   1067, 1099, 1127, -1,   -1,   -1,   -1,
    -1,   1309, 1341, 1369, -1,   -1,   -1,   -1,   -1,   1535, 1564, 1590,
    -1,   -1,   -1,   -1,   -1,   1738, 1763, 1787, -1,   -1,   -1,   -1,
    -1,   -1,   147,  172,  -1,   -1,   -1,   -1,   -1,   -1,   365,  392,
    -1,   -1,   -1,   -1,   -1,   -1,   606,  634,  -1,   -1,   -1,   -1,
    -1,   -1,   852,  880,  -1,   -1,   -1,   -1,   -1,   -1,   1098, 1126,
    -1,   -1,   -1,   -1,   -1,   -1,   1340, 1368, -1,   -1,   -1,   -1,
    -1,   -1,   1563, 1589, -1,   -1,   -1,   -1,   -1,   -1,   1762, 1786,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   171,  -1,   -1,   -1,   -1,
    -1,   -1,   -1,   391,  -1,   -1,   -1,   -1,   -1,   -1,   -1,   633,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   879,  -1,   -1,   -1,   -1,
    -1,   -1,   -1,   1125, -1,   -1,   -1,   -1,   -1,   -1,   -1,   1367,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   1588, -1,   -1,   -1,   -1,
    -1,   -1,   -1,   1785, -1,   30,   55,   80,   105,  130,  155,  179,
    -1,   229,  258,  287,  316,  345,  373,  399,  -1,   452,  484,  517,
    550,  583,  614,  641,  -1,   694,  726,  760,  795,  829,  860,  887,
    -1,   940,  972,  1006, 1041, 1075, 1106, 1133, -1,   1186, 1218, 1251,
    1284, 1317, 1348, 1375, -1,   1427, 1456, 1485, 1514, 1543, 1571, 1596,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   59,   84,
    109,  134,  158,  181,  -1,   -1,   262,  291,  320,  349,  376,  401,
    -1,   -1,   488,  521,  554,  587,  617,  643,  -1,   -1,   730,  764,
    799,  833,  863,  889,  -1,   -1,   976,  1010, 1045, 1079, 1109, 1135,
    -1,   -1,   1222, 1255, 1288, 1321, 1351, 1377, -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   89,   114,  139,  162,  184,  -1,   -1,   -1,   296,
    325,  354,  380,  404,  -1,   -1,   -1,   526,  559,  592,  621,  646,
    -1,   -1,   -1,   769,  804,  838,  867,  892,  -1,   -1,   -1,   1015,
    1050, 1084, 1113, 1138, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   117,  141,  164,  186,
    -1,   -1,   -1,   -1,   328,  356,  382,  406,  -1,   -1,   -1,   -1,
    562,  594,  623,  648,  -1,   -1,   -1,   -1,   807,  840,  869,  894,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   143,  166,  188,  -1,   -1,   -1,   -1,   -1,   358,  384,  408,
    -1,   -1,   -1,   -1,   -1,   596,  625,  650,  -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   168,  190,  -1,   -1,   -1,   -1,
    -1,   -1,   386,  410,  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   192,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   11,   36,   62,   87,
    112,  137,  161,  -1,   208,  235,  265,  294,  323,  352,  379,  -1,
    429,  458,  491,  524,  557,  590,  620,  -1,   671,  700,  733,  767,
    802,  836,  866,  -1,   917,  946,  979,  1013, 1048, 1082, 1112, -1,
    1163, 1192, 1225, 1258, 1291, 1324, 1354, -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    9,    33,   58,   83,   108,  133,  -1,   -1,   206,  232,  261,  290,
    319,  348,  -1,   -1,   427,  455,  487,  520,  553,  586,  -1,   -1,
    669,  697,  729,  763,  798,  832,  -1,   -1,   915,  943,  975,  1009,
    1044, 1078, -1,   -1,   1161, 1189, 1221, 1254, 1287, 1320, -1,   -1,
    1403, 1430, 1459, 1488, 1517, 1546, -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    196,  221,  249,  278,  307,  336,  -1,   -1,   417,  444,  475,  508,
    541,  574,  -1,   -1,   659,  686,  717,  751,  786,  820,  -1,   -1,
    905,  932,  963,  997,  1032, 1066, -1,   -1,   1151, 1178, 1209, 1242,
    1275, 1308, -1,   -1,   1393, 1419, 1447, 1476, 1505, 1534, -1,   -1,
    1613, 1637, 1662, 1687, 1712, 1737, -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    413,  439,  469,  502,  535,  568,  601,  -1,   655,  681,  711,  745,
    780,  814,  847,  -1,   901,  927,  957,  991,  1026, 1060, 1093, -1,
    1147, 1173, 1203, 1236, 1269, 1302, 1335, -1,   1389, 1414, 1441, 1470,
    1499, 1528, 1558, -1,   1609, 1632, 1656, 1681, 1706, 1731, 1757, -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   437,  467,  500,  533,  566,  599,  628,
    -1,   679,  709,  743,  778,  812,  845,  874,  -1,   925,  955,  989,
    1024, 1058, 1091, 1120, -1,   1171, 1201, 1234, 1267, 1300, 1333, 1362,
    -1,   1412, 1439, 1468, 1497, 1526, 1556, 1583, -1,   1630, 1654, 1679,
    1704, 1729, 1755, 1780, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   245,  274,  303,  332,  361,  388,  -1,   -1,   471,  504,
    537,  570,  602,  630,  -1,   -1,   713,  747,  782,  816,  848,  876,
    -1,   -1,   959,  993,  1028, 1062, 1094, 1122, -1,   -1,   1205, 1238,
    1271, 1304, 1336, 1364, -1,   -1,   1443, 1472, 1501, 1530, 1559, 1585,
    -1,   -1,   1658, 1683, 1708, 1733, 1758, 1782, -1,   -1,   54,   79,
    104,  129,  154,  178,  -1,   -1,   257,  286,  315,  344,  372,  398,
    -1,   -1,   483,  516,  549,  582,  613,  640,  -1,   -1,   725,  759,
    794,  828,  859,  886,  -1,   -1,   971,  1005, 1040, 1074, 1105, 1132,
    -1,   -1,   1217, 1250, 1283, 1316, 1347, 1374, -1,   -1,   1455, 1484,
    1513, 1542, 1570, 1595, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   34,   60,   85,   110,  135,  159,  182,  -1,   233,  263,  292,
    321,  350,  377,  402,  -1,   456,  489,  522,  555,  588,  618,  644,
    -1,   698,  731,  765,  800,  834,  864,  890,  -1,   944,  977,  1011,
    1046, 1080, 1110, 1136, -1,   1190, 1223, 1256, 1289, 1322, 1352, 1378,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   1799, 1808, 1817, 1826, 1835, 1844, 1853,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   1800, 1809, 1818,
    1827, 1836, 1845, 1854, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   1798, 1807, 1816, 1825, 1834, 1843, 1852, -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   1793, 1802, 1811, 1820, 1829, 1838, 1847, 1856,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   1794, 1803, 1812, 1821,
    1830, 1839, 1848, 1857, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    1792, 1801, 1810, 1819, 1828, 1837, 1846, 1855, -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   1796, 1805, 1814, 1823, 1832, 1841, 1850, -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   1797, 1806, 1815, 1824,
    1833, 1842, 1851, -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,
    1795, 1804, 1813, 1822, 1831, 1840, 1849, -1,   -1,   -1,   -1,   -1,
    -1,   -1,   -1,   -1};

}  // namespace lczero

```

`src/neural/shared/winograd_filter.cc`:

```cc
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#include "neural/shared/winograd_filter.h"

#include <array>

namespace lczero {
namespace {

static constexpr auto kWinogradAlpha = 4;
static constexpr auto kWinogradTile = kWinogradAlpha * kWinogradAlpha;

}  // namespace

std::vector<float> WinogradFilterZeropadU(const std::vector<float>& U,
                                          const size_t outputs,
                                          const size_t channels,
                                          const size_t outputs_pad,
                                          const size_t channels_pad) {
  // Fill with zeroes.
  auto Upad = std::vector<float>(kWinogradTile * outputs_pad * channels_pad);

  for (size_t o = 0; o < outputs; o++) {
    for (size_t c = 0; c < channels; c++) {
      for (size_t xi = 0; xi < kWinogradAlpha; xi++) {
        for (size_t nu = 0; nu < kWinogradAlpha; nu++) {
          Upad[xi * (kWinogradAlpha * outputs_pad * channels_pad) +
               nu * (outputs_pad * channels_pad) + c * outputs_pad + o] =
              U[xi * (kWinogradAlpha * outputs * channels) +
                nu * (outputs * channels) + c * outputs + o];
        }
      }
    }
  }
  return Upad;
}

std::vector<float> WinogradFilterTransformF(const std::vector<float>& f,
                                            const size_t outputs,
                                            const size_t channels) {
  // F(2x2, 3x3) Winograd filter transformation
  // transpose(G.dot(f).dot(G.transpose()))
  // U matrix is transposed for better memory layout in SGEMM
  auto U = std::vector<float>(kWinogradTile * outputs * channels);
  auto G = std::array<float, kWinogradTile>{1.0, 0.0,  0.0, 0.5, 0.5, 0.5,
                                            0.5, -0.5, 0.5, 0.0, 0.0, 1.0};
  auto temp = std::array<float, 12>{};

  for (size_t o = 0; o < outputs; o++) {
    for (size_t c = 0; c < channels; c++) {
      for (size_t i = 0; i < 4; i++) {
        for (size_t j = 0; j < 3; j++) {
          auto acc = 0.0f;
          for (size_t k = 0; k < 3; k++) {
            acc += G[i * 3 + k] * f[o * channels * 9 + c * 9 + k * 3 + j];
          }
          temp[i * 3 + j] = acc;
        }
      }

      for (size_t xi = 0; xi < 4; xi++) {
        for (size_t nu = 0; nu < 4; nu++) {
          auto acc = 0.0f;
          for (size_t k = 0; k < 3; k++) {
            acc += temp[xi * 3 + k] * G[nu * 3 + k];
          }
          U[xi * (4 * outputs * channels) + nu * (outputs * channels) +
            c * outputs + o] = acc;
        }
      }
    }
  }
  return U;
}

}  // namespace lczero

```

`src/neural/shared/winograd_filter.h`:

```h
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include <cstddef>
#include <vector>

namespace lczero {

// Here are BLAS-free methods to setup the filter
// for the 3x3 winograd convolution algorithm.
//
// Ref:
//
// Fast Algorithms for Convolutional Neural Networks
// https://arxiv.org/abs/1509.09308
//
// https://ai.intel.com/winograd/
// https://ai.intel.com/winograd-2/

// Convolution filter for 3x3 Winograd algorithm

// Create the zero-padded U matrix.
std::vector<float> WinogradFilterZeropadU(const std::vector<float>& U,
                                          const size_t outputs,
                                          const size_t channels,
                                          const size_t outputs_pad,
                                          const size_t channels_pad);

// Create the filter transform matrix.
std::vector<float> WinogradFilterTransformF(const std::vector<float>& f,
                                            const size_t outputs,
                                            const size_t channels);

}  // namespace lczero

```

`src/python/weights.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <string>

#include "neural/encoder.h"
#include "neural/factory.h"
#include "neural/loader.h"
#include "utils/fastmath.h"
#include "utils/optionsparser.h"

namespace lczero {
namespace python {

class Weights {
 public:
  using InputFormat = pblczero::NetworkFormat::InputFormat;
  using PolicyFormat = pblczero::NetworkFormat::PolicyFormat;
  using ValueFormat = pblczero::NetworkFormat::ValueFormat;
  using MovesLeftFormat = pblczero::NetworkFormat::MovesLeftFormat;

  // Exported methods.
  Weights(const std::optional<std::string>& filename)
      : filename_(filename ? *filename : DiscoverWeightsFile()),
        weights_(LoadWeightsFromFile(filename_)) {}

  std::string_view filename() const { return filename_; }
  std::string_view license() const { return weights_.license(); }
  std::string min_version() const {
    const auto& ver = weights_.min_version();
    return std::to_string(ver.major()) + '.' + std::to_string(ver.minor()) +
           '.' + std::to_string(ver.patch());
  }
  int input_format() const {
    return weights_.format().network_format().input();
  }
  int policy_format() const {
    return weights_.format().network_format().policy();
  }
  int value_format() const {
    return weights_.format().network_format().value();
  }
  int moves_left_format() const {
    return weights_.format().network_format().moves_left();
  }
  int blocks() const { return weights_.weights().residual_size(); }
  int filters() const {
    return weights_.weights().residual(0).conv1().weights().params().size() /
           2304;
  }

  // Not exported methods.

  const WeightsFile& weights() const { return weights_; }

 private:
  const std::string filename_;
  const WeightsFile weights_;
};

inline std::vector<std::string> GetAvailableBackends() {
  return NetworkFactory::Get()->GetBackendsList();
}

class Input {
 public:
  // Exported functions.
  Input() = default;
  void set_mask(int plane, uint64_t mask) {
    CheckPlaneExists(plane);
    data_[plane].mask = mask;
  }
  void set_val(int plane, float val) {
    CheckPlaneExists(plane);
    data_[plane].value = val;
  }
  uint64_t mask(int plane) const {
    CheckPlaneExists(plane);
    return data_[plane].mask;
  }
  float val(int plane) const {
    CheckPlaneExists(plane);
    return data_[plane].value;
  }
  std::unique_ptr<Input> clone() const {
    return std::make_unique<Input>(data_);
  }

  // Not exported.
  const InputPlanes GetPlanes() const { return data_; }
  Input(const InputPlanes& data) : data_(data) {}

 private:
  void CheckPlaneExists(int plane) const {
    if (plane < 0 || plane >= static_cast<int>(data_.size())) {
      throw Exception("Plane index must be between 0 and " +
                      std::to_string(data_.size()));
    }
  }

  InputPlanes data_{kInputPlanes};
};

class Output {
 public:
  // Not exposed.
  Output(const NetworkComputation& computation, int idx) {
    for (int i = 0; i < 1858; ++i) p_[i] = computation.GetPVal(idx, i);
    q_ = computation.GetQVal(idx);
    d_ = computation.GetDVal(idx);
    m_ = computation.GetMVal(idx);
  }
  float q() const { return q_; }
  float d() const { return d_; }
  float m() const { return m_; }
  std::vector<float> p_raw(const std::vector<int>& indicies) {
    std::vector<float> result(indicies.size());
    for (size_t i = 0; i < indicies.size(); ++i) {
      int idx = indicies[i];
      if (idx < 0 || idx > 1857) {
        throw Exception("Policy index must be between 0 and 1857.");
      }
      result[i] = p_[idx];
    }
    return result;
  }

  std::vector<float> p_softmax(const std::vector<int>& indicies) {
    auto p_vals = p_raw(indicies);
    float max_p = -std::numeric_limits<float>::infinity();
    for (auto x : p_vals) max_p = std::max(max_p, x);

    float total = 0.0;
    for (auto& x : p_vals) {
      x = FastExp(x - max_p);
      total += x;
    }
    // Normalize P values to add up to 1.0.
    if (total > 0.0f) {
      const float scale = 1.0f / total;
      for (auto& x : p_vals) x *= scale;
    }
    return p_vals;
  }

 private:
  float p_[1858];
  float q_;
  float d_;
  float m_;
};

class BackendCapabilities {
 public:
  // Exported.
  int input_format() const { return caps_.input_format; }
  int moves_left_format() const { return caps_.moves_left; }

  // Not exposed.
  BackendCapabilities(NetworkCapabilities caps) : caps_(caps) {}

 private:
  const NetworkCapabilities caps_;
};

class Backend {
 public:
  // Exported methods.
  static inline std::vector<std::string> available_backends() {
    return NetworkFactory::Get()->GetBackendsList();
  }

  Backend(const Weights* weights, const std::optional<std::string>& backend,
          const std::optional<std::string>& options) {
    std::optional<WeightsFile> w;
    if (weights) w = weights->weights();
    const auto& backends = GetAvailableBackends();
    const std::string be =
        backend.value_or(backends.empty() ? "<none>" : backends[0]);
    OptionsDict network_options;
    if (options) network_options.AddSubdictFromString(*options);
    network_ = NetworkFactory::Get()->Create(be, w, network_options);
  }

  BackendCapabilities capabilities() const {
    return BackendCapabilities(network_->GetCapabilities());
  }

  std::vector<std::unique_ptr<Output>> evaluate(
      const std::vector<Input*>& inputs) const {
    if (inputs.empty()) return {};
    auto computation = network_->NewComputation();
    for (const auto* input : inputs) {
      InputPlanes input_copy = input->GetPlanes();
      computation->AddInput(std::move(input_copy));
    }
    computation->ComputeBlocking();
    std::vector<std::unique_ptr<Output>> result;
    for (int i = 0; i < computation->GetBatchSize(); ++i) {
      result.push_back(std::make_unique<Output>(*computation, i));
    }
    return result;
  }

 private:
  std::unique_ptr<::lczero::Network> network_;
};

class GameState {
 public:
  GameState(const std::optional<std::string> startpos,
            const std::vector<std::string>& moves) {
    ChessBoard starting_board;
    int no_capture_ply;
    int full_moves;
    starting_board.SetFromFen(startpos.value_or(ChessBoard::kStartposFen),
                              &no_capture_ply, &full_moves);

    history_.Reset(starting_board, no_capture_ply,
                   full_moves * 2 - (starting_board.flipped() ? 1 : 2));

    for (const auto& m : moves) {
      Move move(m, history_.IsBlackToMove());
      move = history_.Last().GetBoard().GetModernMove(move);
      history_.Append(move);
    }
  }

  std::unique_ptr<Input> as_input(const Backend& backend) const {
    int tmp;
    return std::make_unique<Input>(
        EncodePositionForNN(static_cast<pblczero::NetworkFormat::InputFormat>(
                                backend.capabilities().input_format()),
                            history_, 8, FillEmptyHistory::FEN_ONLY, &tmp));
  }

  std::vector<std::string> moves() const {
    auto ms = history_.Last().GetBoard().GenerateLegalMoves();
    bool is_black = history_.IsBlackToMove();
    std::vector<std::string> result;
    for (auto m : ms) {
      if (is_black) m.Mirror();
      result.push_back(m.as_string());
    }
    return result;
  }

  std::vector<int> policy_indices() const {
    auto ms = history_.Last().GetBoard().GenerateLegalMoves();
    std::vector<int> result;
    for (auto m : ms) {
      result.push_back(m.as_nn_index(/* transform= */ 0));
    }
    return result;
  }

  std::string as_string() const {
    bool is_black = history_.IsBlackToMove();
    return (is_black ? history_.Last().GetThemBoard()
                     : history_.Last().GetBoard())
        .DebugString();
  }

 private:
  PositionHistory history_;
};

}  // namespace python
}  // namespace lczero

```

`src/selfplay/game.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "selfplay/game.h"

#include <algorithm>

#include "mcts/stoppers/common.h"
#include "mcts/stoppers/factory.h"

namespace lczero {

namespace {
const OptionId kReuseTreeId{"reuse-tree", "ReuseTree",
                            "Reuse the search tree between moves."};
const OptionId kResignPercentageId{
    "resign-percentage", "ResignPercentage",
    "Resign when win percentage drops below specified value."};
const OptionId kResignWDLStyleId{
    "resign-wdlstyle", "ResignWDLStyle",
    "If set, resign percentage applies to any output state being above "
    "100% minus the percentage instead of winrate being below."};
const OptionId kResignEarliestMoveId{"resign-earliest-move",
                                     "ResignEarliestMove",
                                     "Earliest move that resign is allowed."};
const OptionId kMinimumAllowedVistsId{
    "minimum-allowed-visits", "MinimumAllowedVisits",
    "Unless the selected move is the best move, temperature based selection "
    "will be retried until visits of selected move is greater than or equal to "
    "this threshold."};
const OptionId kUciChess960{
    "chess960", "UCI_Chess960",
    "Castling moves are encoded as \"king takes rook\"."};
const OptionId kSyzygyTablebaseId{
    "syzygy-paths", "SyzygyPath",
    "List of Syzygy tablebase directories, list entries separated by system "
    "separator (\";\" for Windows, \":\" for Linux).",
    's'};
}  // namespace

void SelfPlayGame::PopulateUciParams(OptionsParser* options) {
  options->Add<BoolOption>(kReuseTreeId) = false;
  options->Add<BoolOption>(kResignWDLStyleId) = false;
  options->Add<FloatOption>(kResignPercentageId, 0.0f, 100.0f) = 0.0f;
  options->Add<IntOption>(kResignEarliestMoveId, 0, 1000) = 0;
  options->Add<IntOption>(kMinimumAllowedVistsId, 0, 1000000) = 0;
  options->Add<BoolOption>(kUciChess960) = false;
  PopulateTimeManagementOptions(RunType::kSelfplay, options);
  options->Add<StringOption>(kSyzygyTablebaseId);
}

SelfPlayGame::SelfPlayGame(PlayerOptions white, PlayerOptions black,
                           bool shared_tree, const Opening& opening)
    : options_{white, black},
      chess960_{white.uci_options->Get<bool>(kUciChess960) ||
                black.uci_options->Get<bool>(kUciChess960)},
      training_data_(SearchParams(*white.uci_options).GetHistoryFill(),
                     SearchParams(*black.uci_options).GetHistoryFill(),
                     white.network->GetCapabilities().input_format) {
  orig_fen_ = opening.start_fen;
  tree_[0] = std::make_shared<NodeTree>();
  tree_[0]->ResetToPosition(orig_fen_, {});

  if (shared_tree) {
    tree_[1] = tree_[0];
  } else {
    tree_[1] = std::make_shared<NodeTree>();
    tree_[1]->ResetToPosition(orig_fen_, {});
  }
  for (Move m : opening.moves) {
    tree_[0]->MakeMove(m);
    if (tree_[0] != tree_[1]) tree_[1]->MakeMove(m);
  }
}

void SelfPlayGame::Play(int white_threads, int black_threads, bool training,
                        SyzygyTablebase* syzygy_tb, bool enable_resign) {
  bool blacks_move = tree_[0]->IsBlackToMove();

  // If we are training, verify that input formats are consistent.
  if (training && options_[0].network->GetCapabilities().input_format !=
      options_[1].network->GetCapabilities().input_format) {
    throw Exception("Can't mix networks with different input format!");
  }
  // Take syzygy tablebases from player1 options.
  std::string tb_paths =
      options_[0].uci_options->Get<std::string>(kSyzygyTablebaseId);
  if (!tb_paths.empty()) {  // && tb_paths != tb_paths_) {
    syzygy_tb_ = std::make_unique<SyzygyTablebase>();
    CERR << "Loading Syzygy tablebases from " << tb_paths;
    if (!syzygy_tb_->init(tb_paths)) {
      CERR << "Failed to load Syzygy tablebases!";
      syzygy_tb_ = nullptr;
    }
  }
  // Do moves while not end of the game. (And while not abort_)
  while (!abort_) {
    game_result_ = tree_[0]->GetPositionHistory().ComputeGameResult();

    // If endgame, stop.
    if (game_result_ != GameResult::UNDECIDED) break;
    if (tree_[0]->GetPositionHistory().Last().GetGamePly() >= 450) {
      adjudicated_ = true;
      break;
    }
    // Initialize search.
    const int idx = blacks_move ? 1 : 0;
    if (!options_[idx].uci_options->Get<bool>(kReuseTreeId)) {
      tree_[idx]->TrimTreeAtHead();
    }
    {
      std::lock_guard<std::mutex> lock(mutex_);
      if (abort_) break;
      auto stoppers = options_[idx].search_limits.MakeSearchStopper();
      PopulateIntrinsicStoppers(stoppers.get(), *options_[idx].uci_options);

      std::unique_ptr<UciResponder> responder =
          std::make_unique<CallbackUciResponder>(
              options_[idx].best_move_callback, options_[idx].info_callback);

      if (!chess960_) {
        // Remap FRC castling to legacy castling.
        responder = std::make_unique<Chess960Transformer>(
            std::move(responder), tree_[idx]->HeadPosition().GetBoard());
      }

      search_ = std::make_unique<Search>(
          *tree_[idx], options_[idx].network, std::move(responder),
          /* searchmoves */ MoveList(), std::chrono::steady_clock::now(),
          std::move(stoppers),
          /* infinite */ false, *options_[idx].uci_options, options_[idx].cache,
          syzygy_tb);
    }

    // Do search.
    search_->RunBlocking(blacks_move ? black_threads : white_threads);
    move_count_++;
    nodes_total_ += search_->GetTotalPlayouts();
    if (abort_) break;

    Move best_move;
    bool best_is_terminal;
    const auto best_eval = search_->GetBestEval(&best_move, &best_is_terminal);
    float eval = best_eval.wl;
    eval = (eval + 1) / 2;
    if (eval < min_eval_[idx]) min_eval_[idx] = eval;
    const int move_number = tree_[0]->GetPositionHistory().GetLength() / 2 + 1;
    auto best_w = (best_eval.wl + 1.0f - best_eval.d) / 2.0f;
    auto best_d = best_eval.d;
    auto best_l = best_w - best_eval.wl;
    max_eval_[0] = std::max(max_eval_[0], blacks_move ? best_l : best_w);
    max_eval_[1] = std::max(max_eval_[1], best_d);
    max_eval_[2] = std::max(max_eval_[2], blacks_move ? best_w : best_l);
    if (enable_resign &&
        move_number >=
            options_[idx].uci_options->Get<int>(kResignEarliestMoveId)) {
      const float resignpct =
          options_[idx].uci_options->Get<float>(kResignPercentageId) / 100;
      if (options_[idx].uci_options->Get<bool>(kResignWDLStyleId)) {
        auto threshold = 1.0f - resignpct;
        if (best_w > threshold) {
          game_result_ =
              blacks_move ? GameResult::BLACK_WON : GameResult::WHITE_WON;
          adjudicated_ = true;
          break;
        }
        if (best_l > threshold) {
          game_result_ =
              blacks_move ? GameResult::WHITE_WON : GameResult::BLACK_WON;
          adjudicated_ = true;
          break;
        }
        if (best_d > threshold) {
          game_result_ = GameResult::DRAW;
          adjudicated_ = true;
          break;
        }
      } else {
        if (eval < resignpct) {  // always false when resignpct == 0
          game_result_ =
              blacks_move ? GameResult::WHITE_WON : GameResult::BLACK_WON;
          adjudicated_ = true;
          break;
        }
      }
    }

    auto node = tree_[idx]->GetCurrentHead();
    Eval played_eval = best_eval;
    Move move;
    while (true) {
      move = search_->GetBestMove().first;
      uint32_t max_n = 0;
      uint32_t cur_n = 0;

      for (auto& edge : node->Edges()) {
        if (edge.GetN() > max_n) {
          max_n = edge.GetN();
        }
        if (edge.GetMove(tree_[idx]->IsBlackToMove()) == move) {
          cur_n = edge.GetN();
          played_eval.wl = edge.GetWL(-node->GetWL());
          played_eval.d = edge.GetD(node->GetD());
          played_eval.ml = edge.GetM(node->GetM() - 1) + 1;
        }
      }
      // If 'best move' is less than allowed visits and not max visits,
      // discard it and try again.
      if (cur_n == max_n ||
          static_cast<int>(cur_n) >=
              options_[idx].uci_options->Get<int>(kMinimumAllowedVistsId)) {
        break;
      }
      PositionHistory history_copy = tree_[idx]->GetPositionHistory();
      Move move_for_history = move;
      if (tree_[idx]->IsBlackToMove()) {
        move_for_history.Mirror();
      }
      history_copy.Append(move_for_history);
      // Ensure not to discard games that are already decided.
      if (history_copy.ComputeGameResult() == GameResult::UNDECIDED) {
        auto move_list_to_discard = GetMoves();
        move_list_to_discard.push_back(move);
        options_[idx].discarded_callback({orig_fen_, move_list_to_discard});
      }
      search_->ResetBestMove();
    }

    if (training) {
      bool best_is_proof = best_is_terminal;  // But check for better moves.
      if (best_is_proof && best_eval.wl < 1) {
        auto best =
            (best_eval.wl == 0) ? GameResult::DRAW : GameResult::BLACK_WON;
        auto upper = best;
        for (const auto& edge : node->Edges()) {
          upper = std::max(edge.GetBounds().second, upper);
        }
        if (best < upper) {
          best_is_proof = false;
        }
      }
      // Append training data. The GameResult is later overwritten.
      NNCacheLock nneval =
          search_->GetCachedNNEval(tree_[idx]->GetCurrentHead());
      training_data_.Add(tree_[idx]->GetCurrentHead(),
                         tree_[idx]->GetPositionHistory(), best_eval,
                         played_eval, best_is_proof, best_move, move, nneval);
    }
    // Must reset the search before mutating the tree.
    search_.reset();

    // Add best move to the tree.
    tree_[0]->MakeMove(move);
    if (tree_[0] != tree_[1]) tree_[1]->MakeMove(move);
    blacks_move = !blacks_move;
  }
}

std::vector<Move> SelfPlayGame::GetMoves() const {
  std::vector<Move> moves;
  for (Node* node = tree_[0]->GetCurrentHead();
       node != tree_[0]->GetGameBeginNode(); node = node->GetParent()) {
    moves.push_back(node->GetParent()->GetEdgeToNode(node)->GetMove());
  }
  std::vector<Move> result;
  Position pos = tree_[0]->GetPositionHistory().Starting();
  while (!moves.empty()) {
    Move move = moves.back();
    moves.pop_back();
    if (!chess960_) move = pos.GetBoard().GetLegacyMove(move);
    pos = Position(pos, move);
    // Position already flipped, therefore flip the move if white to move.
    if (!pos.IsBlackToMove()) move.Mirror();
    result.push_back(move);
  }
  return result;
}

float SelfPlayGame::GetWorstEvalForWinnerOrDraw() const {
  // TODO: This assumes both players have the same resign style.
  // Supporting otherwise involves mixing the meaning of worst.
  if (options_[0].uci_options->Get<bool>(kResignWDLStyleId)) {
    if (game_result_ == GameResult::WHITE_WON) {
      return std::max(max_eval_[1], max_eval_[2]);
    } else if (game_result_ == GameResult::BLACK_WON) {
      return std::max(max_eval_[1], max_eval_[0]);
    } else {
      return std::max(max_eval_[2], max_eval_[0]);
    }
  }
  if (game_result_ == GameResult::WHITE_WON) return min_eval_[0];
  if (game_result_ == GameResult::BLACK_WON) return min_eval_[1];
  return std::min(min_eval_[0], min_eval_[1]);
}

void SelfPlayGame::Abort() {
  std::lock_guard<std::mutex> lock(mutex_);
  abort_ = true;
  if (search_) search_->Abort();
}

void SelfPlayGame::WriteTrainingData(TrainingDataWriter* writer) const {
  training_data_.Write(writer, game_result_, adjudicated_);
}

std::unique_ptr<ChainedSearchStopper> SelfPlayLimits::MakeSearchStopper()
    const {
  auto result = std::make_unique<ChainedSearchStopper>();

  // always set VisitsStopper to avoid exceeding the limit 4000000000, the
  // default value when visits = 0
  result->AddStopper(std::make_unique<VisitsStopper>(visits, false));
  if (playouts >= 0) {
    result->AddStopper(std::make_unique<PlayoutsStopper>(playouts, false));
  }
  if (movetime >= 0) {
    result->AddStopper(std::make_unique<TimeLimitStopper>(movetime));
  }
  return result;
}

}  // namespace lczero

```

`src/selfplay/game.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include "chess/pgn.h"
#include "chess/position.h"
#include "chess/uciloop.h"
#include "mcts/search.h"
#include "mcts/stoppers/stoppers.h"
#include "neural/cache.h"
#include "neural/network.h"
#include "trainingdata/trainingdata.h"
#include "utils/optionsparser.h"

namespace lczero {

struct SelfPlayLimits {
  std::int64_t visits = -1;
  std::int64_t playouts = -1;
  std::int64_t movetime = -1;

  std::unique_ptr<ChainedSearchStopper> MakeSearchStopper() const;
};

struct PlayerOptions {
  using OpeningCallback = std::function<void(const Opening&)>;
  // Network to use by the player.
  Network* network;
  // Callback when player moves.
  CallbackUciResponder::BestMoveCallback best_move_callback;
  // Callback when player outputs info.
  CallbackUciResponder::ThinkingCallback info_callback;
  // Callback when player discards a selected move due to low visits.
  OpeningCallback discarded_callback;
  // NNcache to use.
  NNCache* cache;
  // User options dictionary.
  const OptionsDict* uci_options;
  // Limits to use for every move.
  SelfPlayLimits search_limits;
};

// Plays a single game vs itself.
class SelfPlayGame {
 public:
  // Player options may point to the same network/cache/etc.
  // If shared_tree is true, search tree is reused between players.
  // (useful for training games). Otherwise the tree is separate for black
  // and white (useful i.e. when they use different networks).
  SelfPlayGame(PlayerOptions white, PlayerOptions black, bool shared_tree,
               const Opening& opening);

  // Populate command line options that it uses.
  static void PopulateUciParams(OptionsParser* options);

  // Starts the game and blocks until the game is finished.
  void Play(int white_threads, int black_threads, bool training,
            SyzygyTablebase* syzygy_tb, bool enable_resign = true);
  // Aborts the game currently played, doesn't matter if it's synchronous or
  // not.
  void Abort();

  // Writes training data to a file.
  void WriteTrainingData(TrainingDataWriter* writer) const;

  GameResult GetGameResult() const { return game_result_; }
  std::vector<Move> GetMoves() const;
  // Gets the eval which required the biggest swing up to get the final outcome.
  // Eval is the expected outcome in the range 0<->1.
  float GetWorstEvalForWinnerOrDraw() const;
  int move_count_ = 0;
  uint64_t nodes_total_ = 0;

 private:
  // options_[0] is for white player, [1] for black.
  PlayerOptions options_[2];
  // Node tree for player1 and player2. If the tree is shared between players,
  // tree_[0] == tree_[1].
  std::shared_ptr<NodeTree> tree_[2];
  std::string orig_fen_;

  // Search that is currently in progress. Stored in members so that Abort()
  // can stop it.
  std::unique_ptr<Search> search_;
  bool abort_ = false;
  GameResult game_result_ = GameResult::UNDECIDED;
  bool adjudicated_ = false;
  // Track minimum eval for each player so that GetWorstEvalForWinnerOrDraw()
  // can be calculated after end of game.
  float min_eval_[2] = {1.0f, 1.0f};
  // Track the maximum eval for white win, draw, black win for comparison to
  // actual outcome.
  float max_eval_[3] = {0.0f, 0.0f, 0.0f};
  const bool chess960_;
  std::mutex mutex_;

  // Training data to send.
  V6TrainingDataArray training_data_;

  std::unique_ptr<SyzygyTablebase> syzygy_tb_;
};

}  // namespace lczero

```

`src/selfplay/loop.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "selfplay/loop.h"

#include <optional>

#include "selfplay/tournament.h"
#include "utils/configfile.h"
#include "utils/optionsparser.h"

namespace lczero {

namespace {
const OptionId kInteractiveId{
    "interactive", "", "Run in interactive mode with UCI-like interface."};

const OptionId kLogFileId{"logfile", "LogFile",
  "Write log to that file. Special value <stderr> to "
  "output the log to the console."};
}  // namespace


SelfPlayLoop::SelfPlayLoop() {}

SelfPlayLoop::~SelfPlayLoop() {
  if (tournament_) tournament_->Abort();
  if (thread_) thread_->join();
}

void SelfPlayLoop::RunLoop() {
  SelfPlayTournament::PopulateOptions(&options_);

  options_.Add<BoolOption>(kInteractiveId) = false;
  options_.Add<StringOption>(kLogFileId);

  if (!options_.ProcessAllFlags()) return;
  
  Logging::Get().SetFilename(options_.GetOptionsDict().Get<std::string>(kLogFileId));

  if (options_.GetOptionsDict().Get<bool>(kInteractiveId)) {
    UciLoop::RunLoop();
  } else {
    // Send id before starting tournament to allow wrapping client to know
    // who we are.
    SendId();
    SelfPlayTournament tournament(
        options_.GetOptionsDict(),
        std::bind(&UciLoop::SendBestMove, this, std::placeholders::_1),
        std::bind(&UciLoop::SendInfo, this, std::placeholders::_1),
        std::bind(&SelfPlayLoop::SendGameInfo, this, std::placeholders::_1),
        std::bind(&SelfPlayLoop::SendTournament, this, std::placeholders::_1));
    tournament.RunBlocking();
  }
}

void SelfPlayLoop::CmdUci() {
  SendId();
  for (const auto& option : options_.ListOptionsUci()) {
    SendResponse(option);
  }
  SendResponse("uciok");
}

void SelfPlayLoop::CmdStart() {
  if (tournament_) return;
  tournament_ = std::make_unique<SelfPlayTournament>(
      options_.GetOptionsDict(),
      std::bind(&UciLoop::SendBestMove, this, std::placeholders::_1),
      std::bind(&UciLoop::SendInfo, this, std::placeholders::_1),
      std::bind(&SelfPlayLoop::SendGameInfo, this, std::placeholders::_1),
      std::bind(&SelfPlayLoop::SendTournament, this, std::placeholders::_1));
  thread_ =
      std::make_unique<std::thread>([this]() { tournament_->RunBlocking(); });
}

void SelfPlayLoop::CmdStop() {
  tournament_->Stop();
  tournament_->Wait();
}

void SelfPlayLoop::SendGameInfo(const GameInfo& info) {
  std::vector<std::string> responses;
  // Send separate resign report before gameready as client gameready parsing
  // will easily get confused by adding new parameters as both training file
  // and move list potentially contain spaces.
  if (info.min_false_positive_threshold) {
    std::string resign_res = "resign_report";
    resign_res +=
        " fp_threshold " + std::to_string(*info.min_false_positive_threshold);
    responses.push_back(resign_res);
  }
  std::string res = "gameready";
  if (!info.training_filename.empty())
    res += " trainingfile " + info.training_filename;
  if (info.game_id != -1) res += " gameid " + std::to_string(info.game_id);
  res += " play_start_ply " + std::to_string(info.play_start_ply);
  if (info.is_black)
    res += " player1 " + std::string(*info.is_black ? "black" : "white");
  if (info.game_result != GameResult::UNDECIDED) {
    res += std::string(" result ") +
           ((info.game_result == GameResult::DRAW)
                ? "draw"
                : (info.game_result == GameResult::WHITE_WON) ? "whitewon"
                                                              : "blackwon");
  }
  if (!info.moves.empty()) {
    res += " moves";
    for (const auto& move : info.moves) res += " " + move.as_string();
  }
  if (!info.initial_fen.empty() &&
      info.initial_fen != ChessBoard::kStartposFen) {
    res += " from_fen " + info.initial_fen;
  }
  responses.push_back(res);
  SendResponses(responses);
}

void SelfPlayLoop::CmdSetOption(const std::string& name,
                                const std::string& value,
                                const std::string& context) {
  options_.SetUciOption(name, value, context);
}

void SelfPlayLoop::SendTournament(const TournamentInfo& info) {
  const int winp1 = info.results[0][0] + info.results[0][1];
  const int losep1 = info.results[2][0] + info.results[2][1];
  const int draws = info.results[1][0] + info.results[1][1];

  // Initialize variables.
  float percentage = -1;
  std::optional<float> elo;
  std::optional<float> los;

  // Only caculate percentage if any games at all (avoid divide by 0).
  if ((winp1 + losep1 + draws) > 0) {
    percentage =
        (static_cast<float>(draws) / 2 + winp1) / (winp1 + losep1 + draws);
  }
  // Calculate elo and los if percentage strictly between 0 and 1 (avoids divide
  // by 0 or overflow).
  if ((percentage < 1) && (percentage > 0))
    elo = -400 * log(1 / percentage - 1) / log(10);
  if ((winp1 + losep1) > 0) {
    los = .5f +
          .5f * std::erf((winp1 - losep1) / std::sqrt(2.0 * (winp1 + losep1)));
  }
  std::ostringstream oss;
  oss << "tournamentstatus";
  if (info.finished) oss << " final";
  oss << " P1: +" << winp1 << " -" << losep1 << " =" << draws;

  if (percentage > 0) {
    oss << " Win: " << std::fixed << std::setw(5) << std::setprecision(2)
        << (percentage * 100.0f) << "%";
  }
  if (elo) {
    oss << " Elo: " << std::fixed << std::setw(5) << std::setprecision(2)
        << (*elo);
  }
  if (los) {
    oss << " LOS: " << std::fixed << std::setw(5) << std::setprecision(2)
        << (*los * 100.0f) << "%";
  }

  oss << " P1-W: +" << info.results[0][0] << " -" << info.results[2][0] << " ="
      << info.results[1][0];
  oss << " P1-B: +" << info.results[0][1] << " -" << info.results[2][1] << " ="
      << info.results[1][1];
  oss << " npm " + std::to_string(static_cast<double>(info.nodes_total_) /
                                  info.move_count_);
  oss << " nodes " + std::to_string(info.nodes_total_);
  oss << " moves " + std::to_string(info.move_count_);
  SendResponse(oss.str());
}

}  // namespace lczero

```

`src/selfplay/loop.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <thread>
#include "chess/uciloop.h"
#include "selfplay/tournament.h"
#include "utils/optionsparser.h"

namespace lczero {

class SelfPlayLoop : public UciLoop {
 public:
  SelfPlayLoop();
  ~SelfPlayLoop();

  void RunLoop() override;
  void CmdStart() override;
  void CmdStop() override;
  void CmdUci() override;
  void CmdSetOption(const std::string& name, const std::string& value,
                    const std::string& context) override;

 private:
  void SendGameInfo(const GameInfo& move);
  void SendTournament(const TournamentInfo& info);

  void EnsureOptionsSent();
  OptionsParser options_;

  std::unique_ptr<SelfPlayTournament> tournament_;
  std::unique_ptr<std::thread> thread_;
};

}  // namespace lczero

```

`src/selfplay/tournament.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "selfplay/tournament.h"

#include "chess/pgn.h"
#include "mcts/search.h"
#include "mcts/stoppers/factory.h"
#include "neural/factory.h"
#include "selfplay/game.h"
#include "utils/optionsparser.h"
#include "utils/random.h"

namespace lczero {
namespace {
const OptionId kShareTreesId{"share-trees", "ShareTrees",
                             "When on, game tree is shared for two players; "
                             "when off, each side has a separate tree."};
const OptionId kTotalGamesId{
    "games", "Games",
    "Number of games to play. -1 to play forever, -2 to play equal to book "
    "length, or double book length if mirrored."};
const OptionId kParallelGamesId{"parallelism", "Parallelism",
                                "Number of games to play in parallel."};
const OptionId kThreadsId{
    "threads", "Threads",
    "Number of (CPU) worker threads to use for every game,", 't'};
const OptionId kPlayoutsId{"playouts", "Playouts",
                           "Number of playouts per move to search."};
const OptionId kVisitsId{"visits", "Visits",
                         "Number of visits per move to search."};
const OptionId kTimeMsId{"movetime", "MoveTime",
                         "Time per move, in milliseconds."};
const OptionId kTrainingId{
    "training", "Training",
    "Enables writing training data. The training data is stored into a "
    "temporary subdirectory that the engine creates."};
const OptionId kVerboseThinkingId{"verbose-thinking", "VerboseThinking",
                                  "Show verbose thinking messages."};
const OptionId kMoveThinkingId{"move-thinking", "MoveThinking",
                               "Show all the per-move thinking."};
const OptionId kResignPlaythroughId{
    "resign-playthrough", "ResignPlaythrough",
    "The percentage of games which ignore resign."};
const OptionId kDiscardedStartChanceId{
    "discarded-start-chance", "DiscardedStartChance",
    "The percentage chance each game will attempt to start from a position "
    "discarded due to not getting enough visits."};
const OptionId kOpeningsFileId{
    "openings-pgn", "OpeningsPgnFile",
    "A path name to a pgn file containing openings to use."};
const OptionId kOpeningsMirroredId{
    "mirror-openings", "MirrorOpenings",
    "If true, each opening will be played in pairs. "
    "Not really compatible with openings mode random."};
const OptionId kOpeningsModeId{"openings-mode", "OpeningsMode",
                               "A choice of sequential, shuffled, or random."};
const OptionId kSyzygyTablebaseId{
	"syzygy-paths", "SyzygyPath",
	"List of Syzygy tablebase directories, list entries separated by system "
	"separator (\";\" for Windows, \":\" for Linux).",
	's' };

}  // namespace

void SelfPlayTournament::PopulateOptions(OptionsParser* options) {
  options->AddContext("player1");
  options->AddContext("player2");
  options->AddContext("white");
  options->AddContext("black");
  for (const auto context : {"player1", "player2"}) {
    auto* dict = options->GetMutableOptions(context);
    dict->AddSubdict("white")->AddAliasDict(&options->GetOptionsDict("white"));
    dict->AddSubdict("black")->AddAliasDict(&options->GetOptionsDict("black"));
  }

  NetworkFactory::PopulateOptions(options);
  options->Add<IntOption>(kThreadsId, 1, 8) = 1;
  options->Add<IntOption>(kNNCacheSizeId, 0, 999999999) = 2000000;
  SearchParams::Populate(options);

  options->Add<BoolOption>(kShareTreesId) = true;
  options->Add<IntOption>(kTotalGamesId, -2, 999999) = -1;
  options->Add<IntOption>(kParallelGamesId, 1, 256) = 8;
  options->Add<IntOption>(kPlayoutsId, -1, 999999999) = -1;
  options->Add<IntOption>(kVisitsId, -1, 999999999) = -1;
  options->Add<IntOption>(kTimeMsId, -1, 999999999) = -1;
  options->Add<BoolOption>(kTrainingId) = false;
  options->Add<BoolOption>(kVerboseThinkingId) = false;
  options->Add<BoolOption>(kMoveThinkingId) = false;
  options->Add<FloatOption>(kResignPlaythroughId, 0.0f, 100.0f) = 0.0f;
  options->Add<FloatOption>(kDiscardedStartChanceId, 0.0f, 100.0f) = 0.0f;
  options->Add<StringOption>(kOpeningsFileId) = "";
  options->Add<BoolOption>(kOpeningsMirroredId) = false;
  std::vector<std::string> openings_modes = {"sequential", "shuffled",
                                             "random"};
  options->Add<ChoiceOption>(kOpeningsModeId, openings_modes) = "sequential";

  options->Add<StringOption>(kSyzygyTablebaseId);
  SelfPlayGame::PopulateUciParams(options);

  auto defaults = options->GetMutableDefaultsOptions();
  defaults->Set<int>(SearchParams::kMiniBatchSizeId, 32);
  defaults->Set<float>(SearchParams::kCpuctId, 1.2f);
  defaults->Set<float>(SearchParams::kCpuctFactorId, 0.0f);
  defaults->Set<float>(SearchParams::kPolicySoftmaxTempId, 1.0f);
  defaults->Set<int>(SearchParams::kMaxCollisionVisitsId, 1);
  defaults->Set<int>(SearchParams::kMaxCollisionEventsId, 1);
  defaults->Set<int>(SearchParams::kCacheHistoryLengthId, 7);
  defaults->Set<bool>(SearchParams::kOutOfOrderEvalId, false);
  defaults->Set<float>(SearchParams::kTemperatureId, 1.0f);
  defaults->Set<float>(SearchParams::kNoiseEpsilonId, 0.25f);
  defaults->Set<float>(SearchParams::kFpuValueId, 0.0f);
  defaults->Set<std::string>(SearchParams::kHistoryFillId, "no");
  defaults->Set<std::string>(NetworkFactory::kBackendId, "multiplexing");
  defaults->Set<bool>(SearchParams::kStickyEndgamesId, false);
  defaults->Set<bool>(SearchParams::kTwoFoldDrawsId, false);
  defaults->Set<int>(SearchParams::kTaskWorkersPerSearchWorkerId, 0);
}

SelfPlayTournament::SelfPlayTournament(
    const OptionsDict& options,
    CallbackUciResponder::BestMoveCallback best_move_info,
    CallbackUciResponder::ThinkingCallback thinking_info,
    GameInfo::Callback game_info, TournamentInfo::Callback tournament_info)
    : player_options_{{options.GetSubdict("player1").GetSubdict("white"),
                       options.GetSubdict("player1").GetSubdict("black")},
                      {options.GetSubdict("player2").GetSubdict("white"),
                       options.GetSubdict("player2").GetSubdict("black")}},
      best_move_callback_(best_move_info),
      info_callback_(thinking_info),
      game_callback_(game_info),
      tournament_callback_(tournament_info),
      kTotalGames(options.Get<int>(kTotalGamesId)),
      kShareTree(options.Get<bool>(kShareTreesId)),
      kParallelism(options.Get<int>(kParallelGamesId)),
      kTraining(options.Get<bool>(kTrainingId)),
      kResignPlaythrough(options.Get<float>(kResignPlaythroughId)),
      kDiscardedStartChance(options.Get<float>(kDiscardedStartChanceId)) {
  std::string book = options.Get<std::string>(kOpeningsFileId);
  if (!book.empty()) {
    PgnReader book_reader;
    book_reader.AddPgnFile(book);
    openings_ = book_reader.ReleaseGames();
    if (options.Get<std::string>(kOpeningsModeId) == "shuffled") {
      Random::Get().Shuffle(openings_.begin(), openings_.end());
    }
  }
  // If playing just one game, the player1 is white, otherwise randomize.
  if (kTotalGames != 1) {
    first_game_black_ = Random::Get().GetBool();
  }

  // Initializing networks.
  for (const auto& name : {"player1", "player2"}) {
    for (const auto& color : {"white", "black"}) {
      const auto& opts = options.GetSubdict(name).GetSubdict(color);
      const auto config = NetworkFactory::BackendConfiguration(opts);
      if (networks_.find(config) == networks_.end()) {
        networks_.emplace(config, NetworkFactory::LoadNetwork(opts));
      }
    }
  }

  // Initializing cache.
  cache_[0] = std::make_shared<NNCache>(
      options.GetSubdict("player1").Get<int>(kNNCacheSizeId));
  if (kShareTree) {
    cache_[1] = cache_[0];
  } else {
    cache_[1] = std::make_shared<NNCache>(
        options.GetSubdict("player2").Get<int>(kNNCacheSizeId));
  }

  // SearchLimits.
  static constexpr const char* kPlayerNames[2] = {"player1", "player2"};
  static constexpr const char* kPlayerColors[2] = {"white", "black"};
  for (int name_idx : {0, 1}) {
    for (int color_idx : {0, 1}) {
      auto& limits = search_limits_[name_idx][color_idx];
      const auto& dict = options.GetSubdict(kPlayerNames[name_idx])
                             .GetSubdict(kPlayerColors[color_idx]);
      limits.playouts = dict.Get<int>(kPlayoutsId);
      limits.visits = dict.Get<int>(kVisitsId);
      limits.movetime = dict.Get<int>(kTimeMsId);

      if (limits.playouts == -1 && limits.visits == -1 &&
          limits.movetime == -1) {
        throw Exception(
            "Please define --visits, --playouts or --movetime, otherwise it's "
            "not clear when to stop search.");
      }
    }
  }

  // Take syzygy tablebases from options.
  std::string tb_paths =
	  options.Get<std::string>(kSyzygyTablebaseId);
  if (!tb_paths.empty()) {
	  syzygy_tb_ = std::make_unique<SyzygyTablebase>();
	  CERR << "Loading Syzygy tablebases from " << tb_paths;
	  if (!syzygy_tb_->init(tb_paths)) {
		  CERR << "Failed to load Syzygy tablebases!";
		  syzygy_tb_ = nullptr;
	  }
  }

}

void SelfPlayTournament::PlayOneGame(int game_number) {
  bool player1_black;  // Whether player1 will player as black in this game.
  Opening opening;
  {
    Mutex::Lock lock(mutex_);
    player1_black = ((game_number % 2) == 1) != first_game_black_;
    if (!openings_.empty()) {
      if (player_options_[0][0].Get<bool>(kOpeningsMirroredId)) {
        opening = openings_[(game_number / 2) % openings_.size()];
      } else if (player_options_[0][0].Get<std::string>(kOpeningsModeId) ==
                 "random") {
        opening = openings_[Random::Get().GetInt(0, openings_.size() - 1)];
      } else {
        opening = openings_[game_number % openings_.size()];
      }
    }
    if (discard_pile_.size() > 0 &&
        Random::Get().GetFloat(100.0f) < kDiscardedStartChance) {
      const size_t idx = Random::Get().GetInt(0, discard_pile_.size() - 1);
      if (idx != discard_pile_.size() - 1) {
        std::swap(discard_pile_[idx], discard_pile_.back());
      }
      opening = discard_pile_.back();
      discard_pile_.pop_back();
    }
  }
  const int color_idx[2] = {player1_black ? 1 : 0, player1_black ? 0 : 1};

  PlayerOptions options[2];

  std::vector<ThinkingInfo> last_thinking_info;
  for (int pl_idx : {0, 1}) {
    const int color = color_idx[pl_idx];
    const bool verbose_thinking =
        player_options_[pl_idx][color].Get<bool>(kVerboseThinkingId);
    const bool move_thinking =
        player_options_[pl_idx][color].Get<bool>(kMoveThinkingId);
    // Populate per-player options.
    PlayerOptions& opt = options[color_idx[pl_idx]];
    opt.network = networks_[NetworkFactory::BackendConfiguration(
                                player_options_[pl_idx][color])]
                      .get();
    opt.cache = cache_[pl_idx].get();
    opt.uci_options = &player_options_[pl_idx][color];
    opt.search_limits = search_limits_[pl_idx][color];

    // "bestmove" callback.
    opt.best_move_callback = [this, game_number, pl_idx, player1_black,
                              verbose_thinking, move_thinking,
                              &last_thinking_info](const BestMoveInfo& info) {
      if (!move_thinking) {
        last_thinking_info.clear();
        return;
      }
      // In non-verbose mode, output the last "info" message.
      if (!verbose_thinking && !last_thinking_info.empty()) {
        info_callback_(last_thinking_info);
        last_thinking_info.clear();
      }
      BestMoveInfo rich_info = info;
      rich_info.player = pl_idx + 1;
      rich_info.is_black = player1_black ? pl_idx == 0 : pl_idx != 0;
      rich_info.game_id = game_number;
      best_move_callback_(rich_info);
    };

    opt.info_callback =
        [this, game_number, pl_idx, player1_black, verbose_thinking,
         &last_thinking_info](const std::vector<ThinkingInfo>& infos) {
          std::vector<ThinkingInfo> rich_info = infos;
          for (auto& info : rich_info) {
            info.player = pl_idx + 1;
            info.is_black = player1_black ? pl_idx == 0 : pl_idx != 0;
            info.game_id = game_number;
          }
          if (verbose_thinking) {
            info_callback_(rich_info);
          } else {
            // In non-verbose mode, remember the last "info" messages.
            last_thinking_info = std::move(rich_info);
          }
        };
    opt.discarded_callback = [this](const Opening& moves) {
      // Only track discards if discard start chance is non-zero.
      if (kDiscardedStartChance == 0.0f) return;
      Mutex::Lock lock(mutex_);
      discard_pile_.push_back(moves);
      // 10k seems it should be enough to keep a good mix and avoid running out
      // of ram.
      if (discard_pile_.size() > 10000) {
        // Swap a random element to end and pop it to avoid growing.
        const size_t idx = Random::Get().GetInt(0, discard_pile_.size() - 1);
        if (idx != discard_pile_.size() - 1) {
          std::swap(discard_pile_[idx], discard_pile_.back());
        }
        discard_pile_.pop_back();
      }
    };
  }

  // Iterator to store the game in. Have to keep it so that later we can
  // delete it. Need to expose it in games_ member variable only because
  // of possible Abort() that should stop them all.
  std::list<std::unique_ptr<SelfPlayGame>>::iterator game_iter;
  {
    Mutex::Lock lock(mutex_);
    games_.emplace_front(std::make_unique<SelfPlayGame>(options[0], options[1],
                                                        kShareTree, opening));
    game_iter = games_.begin();
  }
  auto& game = **game_iter;

  // If kResignPlaythrough == 0, then this comparison is unconditionally true
  const bool enable_resign =
      Random::Get().GetFloat(100.0f) >= kResignPlaythrough;

  // PLAY GAME!
  auto player1_threads = player_options_[0][color_idx[0]].Get<int>(kThreadsId);
  auto player2_threads = player_options_[1][color_idx[1]].Get<int>(kThreadsId);
  game.Play(player1_threads, player2_threads, kTraining, syzygy_tb_.get(),
            enable_resign);
  
  // If game was aborted, it's still undecided.
  if (game.GetGameResult() != GameResult::UNDECIDED) {
    // Game callback.
    GameInfo game_info;
    game_info.game_result = game.GetGameResult();
    game_info.is_black = player1_black;
    game_info.game_id = game_number;
    game_info.initial_fen = opening.start_fen;
    game_info.moves = game.GetMoves();
    game_info.play_start_ply = opening.moves.size();
    if (!enable_resign) {
      game_info.min_false_positive_threshold =
          game.GetWorstEvalForWinnerOrDraw();
    }
    if (kTraining) {
      TrainingDataWriter writer(game_number);
      game.WriteTrainingData(&writer);
      writer.Finalize();
      game_info.training_filename = writer.GetFileName();
    }
    game_callback_(game_info);

    // Update tournament stats.
    {
      Mutex::Lock lock(mutex_);
      int result = game.GetGameResult() == GameResult::DRAW
                       ? 1
                       : game.GetGameResult() == GameResult::WHITE_WON ? 0 : 2;
      if (player1_black) result = 2 - result;
      ++tournament_info_.results[result][player1_black ? 1 : 0];
      tournament_info_.move_count_ += game.move_count_;
      tournament_info_.nodes_total_ += game.nodes_total_;
      tournament_callback_(tournament_info_);
    }
  }

  {
    Mutex::Lock lock(mutex_);
    games_.erase(game_iter);
  }
}

void SelfPlayTournament::Worker() {
  // Play games while game limit is not reached (or while not aborted).
  while (true) {
    int game_id;
    {
      Mutex::Lock lock(mutex_);
      if (abort_) break;
      bool mirrored = player_options_[0][0].Get<bool>(kOpeningsMirroredId);
      if ((kTotalGames >= 0 && games_count_ >= kTotalGames) ||
          (kTotalGames == -2 && !openings_.empty() &&
           games_count_ >=
               static_cast<int>(openings_.size()) * (mirrored ? 2 : 1)))
        break;
      game_id = games_count_++;
    }
    PlayOneGame(game_id);
  }
}

void SelfPlayTournament::StartAsync() {
  Mutex::Lock lock(threads_mutex_);
  while (threads_.size() < kParallelism) {
    threads_.emplace_back([&]() { Worker(); });
  }
}

void SelfPlayTournament::RunBlocking() {
  if (kParallelism == 1) {
    // No need for multiple threads if there is one worker.
    Worker();
    Mutex::Lock lock(mutex_);
    if (!abort_) {
      tournament_info_.finished = true;
      tournament_callback_(tournament_info_);
    }
  } else {
    StartAsync();
    Wait();
  }
}

void SelfPlayTournament::Wait() {
  {
    Mutex::Lock lock(threads_mutex_);
    while (!threads_.empty()) {
      threads_.back().join();
      threads_.pop_back();
    }
  }
  {
    Mutex::Lock lock(mutex_);
    if (!abort_) {
      tournament_info_.finished = true;
      tournament_callback_(tournament_info_);
    }
  }
}

void SelfPlayTournament::Abort() {
  Mutex::Lock lock(mutex_);
  abort_ = true;
  for (auto& game : games_)
    if (game) game->Abort();
}

void SelfPlayTournament::Stop() {
  Mutex::Lock lock(mutex_);
  abort_ = true;
}

SelfPlayTournament::~SelfPlayTournament() {
  Abort();
  Wait();
}

}  // namespace lczero

```

`src/selfplay/tournament.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <list>

#include "chess/pgn.h"
#include "neural/factory.h"
#include "selfplay/game.h"
#include "utils/mutex.h"
#include "utils/optionsdict.h"
#include "utils/optionsparser.h"

namespace lczero {

// Runs many selfplay games, possibly in parallel.
class SelfPlayTournament {
 public:
  SelfPlayTournament(const OptionsDict& options,
                     CallbackUciResponder::BestMoveCallback best_move_info,
                     CallbackUciResponder::ThinkingCallback thinking_info,
                     GameInfo::Callback game_info,
                     TournamentInfo::Callback tournament_info);

  // Populate command line options that it uses.
  static void PopulateOptions(OptionsParser* options);

  // Starts worker threads and exists immediately.
  void StartAsync();

  // Starts tournament and waits until it finishes.
  void RunBlocking();

  // Blocks until all worker threads finish.
  void Wait();

  // Tells worker threads to finish ASAP. Does not block.
  void Abort();

  // Stops any more games from starting, in progress games will complete.
  void Stop();

  // If there are ongoing games, aborts and waits.
  ~SelfPlayTournament();

 private:
  void Worker();
  void PlayOneGame(int game_id);

  Mutex mutex_;
  // Whether first game will be black for player1.
  bool first_game_black_ GUARDED_BY(mutex_) = false;
  std::vector<Opening> discard_pile_ GUARDED_BY(mutex_);
  // Number of games which already started.
  int games_count_ GUARDED_BY(mutex_) = 0;
  bool abort_ GUARDED_BY(mutex_) = false;
  std::vector<Opening> openings_ GUARDED_BY(mutex_);
  // Games in progress. Exposed here to be able to abort them in case if
  // Abort(). Stored as list and not vector so that threads can keep iterators
  // to them and not worry that it becomes invalid.
  std::list<std::unique_ptr<SelfPlayGame>> games_ GUARDED_BY(mutex_);
  // Place to store tournament stats.
  TournamentInfo tournament_info_ GUARDED_BY(mutex_);

  Mutex threads_mutex_;
  std::vector<std::thread> threads_ GUARDED_BY(threads_mutex_);

  // Map from the backend configuration to a network.
  std::map<NetworkFactory::BackendConfiguration, std::unique_ptr<Network>>
      networks_;
  std::shared_ptr<NNCache> cache_[2];
  // [player1 or player2][white or black].
  const OptionsDict player_options_[2][2];
  SelfPlayLimits search_limits_[2][2];

  CallbackUciResponder::BestMoveCallback best_move_callback_;
  CallbackUciResponder::ThinkingCallback info_callback_;
  GameInfo::Callback game_callback_;
  TournamentInfo::Callback tournament_callback_;
  const int kTotalGames;
  const bool kShareTree;
  const size_t kParallelism;
  const bool kTraining;
  const float kResignPlaythrough;
  const float kDiscardedStartChance;

  std::unique_ptr<SyzygyTablebase> syzygy_tb_;

};

}  // namespace lczero

```

`src/syzygy/syzygy.cc`:

```cc
/*
  Originally from cfish's tbprobe.c
  Copyright (c) 2013-2018 Ronald de Man
  That file may be redistributed and/or modified without restrictions.

  This modified version is available under the GPL:

  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <atomic>
#include <cstdint>
#include <cstdio>
#include <cstdlib>
#include <cstring>
#include <fstream>
#include <mutex>
#include <random>
#include <sstream>
#include <string>

#include "syzygy/syzygy.h"

#include "utils/exception.h"
#include "utils/logging.h"
#include "utils/mutex.h"

#ifndef _WIN32
#include <fcntl.h>
#include <sys/mman.h>
#include <sys/stat.h>
#include <unistd.h>
#else
#define WIN32_LEAN_AND_MEAN
#define NOMINMAX
#include <windows.h>
#endif

namespace lczero {

namespace {

#define TB_PIECES 7
#define TB_HASHBITS (TB_PIECES < 7 ? 11 : 12)
#define TB_MAX_PIECE (TB_PIECES < 7 ? 254 : 650)
#define TB_MAX_PAWN (TB_PIECES < 7 ? 256 : 861)

#ifdef _WIN32
typedef HANDLE map_t;
#define SEP_CHAR ';'
#else
typedef size_t map_t;
#define SEP_CHAR ':'
#endif

typedef uint64_t Key;

constexpr const char* kSuffix[] = {".rtbw", ".rtbm", ".rtbz"};
constexpr uint32_t kMagic[] = {0x5d23e871, 0x88ac504b, 0xa50c66d7};
enum { WDL, DTM, DTZ };

enum { PIECE_ENC, FILE_ENC, RANK_ENC };

enum PieceType {
  PAWN = 1,
  KNIGHT,
  BISHOP,
  ROOK,
  QUEEN,
  KING,
};
enum Piece {
  W_PAWN = 1,
  W_KNIGHT,
  W_BISHOP,
  W_ROOK,
  W_QUEEN,
  W_KING,
  B_PAWN = 9,
  B_KNIGHT,
  B_BISHOP,
  B_ROOK,
  B_QUEEN,
  B_KING,
};

struct PairsData {
  uint8_t* indexTable;
  uint16_t* sizeTable;
  uint8_t* data;
  uint16_t* offset;
  uint8_t* symLen;
  uint8_t* symPat;
  uint8_t blockSize;
  uint8_t idxBits;
  uint8_t minLen;
  uint8_t constValue[2];
  uint64_t base[1];  // must be base[1] in C++
};

struct EncInfo {
  PairsData* precomp;
  size_t factor[TB_PIECES];
  uint8_t pieces[TB_PIECES];
  uint8_t norm[TB_PIECES];
};

struct BaseEntry {
  Key key;
  uint8_t* data[3];
  map_t mapping[3];
  std::atomic<bool> ready[3];
  uint8_t num;
  bool symmetric;
  bool hasPawns;
  bool hasDtm;
  bool hasDtz;
  union {
    bool kk_enc;
    uint8_t pawns[2];
  };
  bool dtmLossOnly;
};

struct PieceEntry : BaseEntry {
  EncInfo ei[5];  // 2 + 2 + 1
  uint16_t* dtmMap;
  uint16_t dtmMapIdx[2][2];
  void* dtzMap;
  uint16_t dtzMapIdx[4];
  uint8_t dtzFlags;
};

struct PawnEntry : BaseEntry {
  EncInfo ei[24];  // 4 * 2 + 6 * 2 + 4
  uint16_t* dtmMap;
  uint16_t dtmMapIdx[6][2][2];
  void* dtzMap;
  uint16_t dtzMapIdx[4][4];
  uint8_t dtzFlags[4];
  bool dtmSwitched;
};

struct TbHashEntry {
  Key key;
  BaseEntry* ptr;
};

constexpr int kWdlToDtz[] = {-1, -101, 0, 101, 1};

// DTZ tables don't store valid scores for moves that reset the rule50 counter
// like captures and pawn moves but we can easily recover the correct dtz of the
// previous move if we know the position's WDL score.
int dtz_before_zeroing(WDLScore wdl) { return kWdlToDtz[wdl + 2]; }

// Return the sign of a number (-1, 0, 1)
template <typename T>
int sign_of(T val) {
  return (T(0) < val) - (val < T(0));
}

int count_pieces(const ChessBoard& pos, int type, bool theirs) {
  const BitBoard all = theirs ? pos.theirs() : pos.ours();
  switch (type) {
    case KING:
      return 1;
    case QUEEN:
      return (all & pos.queens()).count_few();
    case ROOK:
      return (all & pos.rooks()).count_few();
    case BISHOP:
      return (all & pos.bishops()).count_few();
    case KNIGHT:
      return (all & pos.knights()).count_few();
    case PAWN:
      return (all & pos.pawns()).count_few();
    default:
      assert(false);
  }
  return 0;
}

BitBoard pieces(const ChessBoard& pos, int type, bool theirs) {
  const BitBoard all = theirs ? pos.theirs() : pos.ours();
  switch (type) {
    case KING:
      return all & pos.kings();
    case QUEEN:
      return all & pos.queens();
    case ROOK:
      return all & pos.rooks();
    case BISHOP:
      return all & pos.bishops();
    case KNIGHT:
      return all & pos.knights();
    case PAWN:
      return all & pos.pawns();
    default:
      assert(false);
  }
  return BitBoard();
}

bool is_capture(const ChessBoard& pos, const Move& move) {
  // Simple capture.
  if (pos.theirs().get(move.to())) return true;
  // Enpassant capture. Pawn moves other than straight it must be a capture.
  if (pos.pawns().get(move.from()) && move.from().col() != move.to().col()) {
    return true;
  }
  return false;
}

constexpr char kPieceToChar[] = " PNBRQK  pnbrqk";

// Given a position, produce a text string of the form KQPvKRP, where
// "KQP" represents the white pieces if flip == false and the black pieces
// if flip == true.
void prt_str(const ChessBoard& pos, char* str, bool flip) {
  const bool first_theirs = flip ^ pos.flipped();

  for (int pt = KING; pt >= PAWN; pt--) {
    for (int i = count_pieces(pos, pt, first_theirs); i > 0; i--) {
      *str++ = kPieceToChar[pt];
    }
  }
  *str++ = 'v';
  for (int pt = KING; pt >= PAWN; pt--) {
    for (int i = count_pieces(pos, pt, !first_theirs); i > 0; i--) {
      *str++ = kPieceToChar[pt];
    }
  }
  *str++ = 0;
}

#define pchr(i) kPieceToChar[QUEEN - (i)]
#define Swap(a, b) \
  {                \
    int tmp = a;   \
    a = b;         \
    b = tmp;       \
  }

#define PIECE(x) (static_cast<PieceEntry*>(x))
#define PAWN(x) (static_cast<PawnEntry*>(x))

int num_tables(BaseEntry* be, const int type) {
  return be->hasPawns ? type == DTM ? 6 : 4 : 1;
}

EncInfo* first_ei(BaseEntry* be, const int type) {
  return be->hasPawns ? &PAWN(be)->ei[type == WDL ? 0 : type == DTM ? 8 : 20]
                      : &PIECE(be)->ei[type == WDL ? 0 : type == DTM ? 2 : 4];
}

constexpr int8_t kOffDiag[] = {
    0, -1, -1, -1, -1, -1, -1, -1, 1, 0, -1, -1, -1, -1, -1, -1,
    1, 1,  0,  -1, -1, -1, -1, -1, 1, 1, 1,  0,  -1, -1, -1, -1,
    1, 1,  1,  1,  0,  -1, -1, -1, 1, 1, 1,  1,  1,  0,  -1, -1,
    1, 1,  1,  1,  1,  1,  0,  -1, 1, 1, 1,  1,  1,  1,  1,  0};

constexpr uint8_t kTriangle[] = {
    6, 0, 1, 2, 2, 1, 0, 6, 0, 7, 3, 4, 4, 3, 7, 0, 1, 3, 8, 5, 5, 8,
    3, 1, 2, 4, 5, 9, 9, 5, 4, 2, 2, 4, 5, 9, 9, 5, 4, 2, 1, 3, 8, 5,
    5, 8, 3, 1, 0, 7, 3, 4, 4, 3, 7, 0, 6, 0, 1, 2, 2, 1, 0, 6};

constexpr uint8_t kFlipDiag[] = {
    0, 8,  16, 24, 32, 40, 48, 56, 1, 9,  17, 25, 33, 41, 49, 57,
    2, 10, 18, 26, 34, 42, 50, 58, 3, 11, 19, 27, 35, 43, 51, 59,
    4, 12, 20, 28, 36, 44, 52, 60, 5, 13, 21, 29, 37, 45, 53, 61,
    6, 14, 22, 30, 38, 46, 54, 62, 7, 15, 23, 31, 39, 47, 55, 63};

constexpr uint8_t kLower[] = {
    28, 0,  1,  2,  3,  4,  5,  6,  0, 29, 7,  8,  9,  10, 11, 12,
    1,  7,  30, 13, 14, 15, 16, 17, 2, 8,  13, 31, 18, 19, 20, 21,
    3,  9,  14, 18, 32, 22, 23, 24, 4, 10, 15, 19, 22, 33, 25, 26,
    5,  11, 16, 20, 23, 25, 34, 27, 6, 12, 17, 21, 24, 26, 27, 35};

constexpr uint8_t kDiag[] = {
    0, 0, 0, 0, 0, 0,  0,  8, 0, 1, 0, 0, 0,  0,  9, 0, 0, 0, 2, 0, 0,  10,
    0, 0, 0, 0, 0, 3,  11, 0, 0, 0, 0, 0, 0,  12, 4, 0, 0, 0, 0, 0, 13, 0,
    0, 5, 0, 0, 0, 14, 0,  0, 0, 0, 6, 0, 15, 0,  0, 0, 0, 0, 0, 7};

constexpr uint8_t kFlap[2][64] = {
    {0, 0,  0,  0,  0,  0,  0,  0, 0, 6,  12, 18, 18, 12, 6,  0,
     1, 7,  13, 19, 19, 13, 7,  1, 2, 8,  14, 20, 20, 14, 8,  2,
     3, 9,  15, 21, 21, 15, 9,  3, 4, 10, 16, 22, 22, 16, 10, 4,
     5, 11, 17, 23, 23, 17, 11, 5, 0, 0,  0,  0,  0,  0,  0,  0},
    {0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  2,  3,  3,  2,  1,  0,
     4,  5,  6,  7,  7,  6,  5,  4,  8,  9,  10, 11, 11, 10, 9,  8,
     12, 13, 14, 15, 15, 14, 13, 12, 16, 17, 18, 19, 19, 18, 17, 16,
     20, 21, 22, 23, 23, 22, 21, 20, 0,  0,  0,  0,  0,  0,  0,  0}};

constexpr uint8_t kPawnTwist[2][64] = {
    {0,  0,  0,  0, 0, 0,  0,  0,  47, 35, 23, 11, 10, 22, 34, 46,
     45, 33, 21, 9, 8, 20, 32, 44, 43, 31, 19, 7,  6,  18, 30, 42,
     41, 29, 17, 5, 4, 16, 28, 40, 39, 27, 15, 3,  2,  14, 26, 38,
     37, 25, 13, 1, 0, 12, 24, 36, 0,  0,  0,  0,  0,  0,  0,  0},
    {0,  0,  0,  0,  0,  0,  0,  0,  47, 45, 43, 41, 40, 42, 44, 46,
     39, 37, 35, 33, 32, 34, 36, 38, 31, 29, 27, 25, 24, 26, 28, 30,
     23, 21, 19, 17, 16, 18, 20, 22, 15, 13, 11, 9,  8,  10, 12, 14,
     7,  5,  3,  1,  0,  2,  4,  6,  0,  0,  0,  0,  0,  0,  0,  0}};

constexpr int16_t kKKIdx[10][64] = {
    {-1, -1, -1, 0,  1,  2,  3,  4,  -1, -1, -1, 5,  6,  7,  8,  9,
     10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25,
     26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,
     42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57},
    {58,  -1,  -1,  -1,  59,  60,  61,  62,  63,  -1,  -1,  -1,  64,
     65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,
     78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,
     91,  92,  93,  94,  95,  96,  97,  98,  99,  100, 101, 102, 103,
     104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115},
    {116, 117, -1,  -1,  -1,  118, 119, 120, 121, 122, -1,  -1,  -1,
     123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135,
     136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148,
     149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161,
     162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173},
    {174, -1,  -1,  -1,  175, 176, 177, 178, 179, -1,  -1,  -1,  180,
     181, 182, 183, 184, -1,  -1,  -1,  185, 186, 187, 188, 189, 190,
     191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203,
     204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216,
     217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228},
    {229, 230, -1,  -1,  -1,  231, 232, 233, 234, 235, -1,  -1,  -1,
     236, 237, 238, 239, 240, -1,  -1,  -1,  241, 242, 243, 244, 245,
     246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258,
     259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271,
     272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283},
    {284, 285, 286, 287, 288, 289, 290, 291, 292, 293, -1,  -1,  -1,
     294, 295, 296, 297, 298, -1,  -1,  -1,  299, 300, 301, 302, 303,
     -1,  -1,  -1,  304, 305, 306, 307, 308, 309, 310, 311, 312, 313,
     314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326,
     327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338},
    {-1,  -1,  339, 340, 341, 342, 343, 344, -1,  -1,  345, 346, 347,
     348, 349, 350, -1,  -1,  441, 351, 352, 353, 354, 355, -1,  -1,
     -1,  442, 356, 357, 358, 359, -1,  -1,  -1,  -1,  443, 360, 361,
     362, -1,  -1,  -1,  -1,  -1,  444, 363, 364, -1,  -1,  -1,  -1,
     -1,  -1,  445, 365, -1,  -1,  -1,  -1,  -1,  -1,  -1,  446},
    {-1, -1, -1, 366, 367, 368, 369, 370, -1, -1, -1, 371, 372, 373, 374, 375,
     -1, -1, -1, 376, 377, 378, 379, 380, -1, -1, -1, 447, 381, 382, 383, 384,
     -1, -1, -1, -1,  448, 385, 386, 387, -1, -1, -1, -1,  -1,  449, 388, 389,
     -1, -1, -1, -1,  -1,  -1,  450, 390, -1, -1, -1, -1,  -1,  -1,  -1,  451},
    {452, 391, 392, 393, 394, 395, 396, 397, -1,  -1,  -1,  -1,  398,
     399, 400, 401, -1,  -1,  -1,  -1,  402, 403, 404, 405, -1,  -1,
     -1,  -1,  406, 407, 408, 409, -1,  -1,  -1,  -1,  453, 410, 411,
     412, -1,  -1,  -1,  -1,  -1,  454, 413, 414, -1,  -1,  -1,  -1,
     -1,  -1,  455, 415, -1,  -1,  -1,  -1,  -1,  -1,  -1,  456},
    {457, 416, 417, 418, 419, 420, 421, 422, -1,  458, 423, 424, 425,
     426, 427, 428, -1,  -1,  -1,  -1,  -1,  429, 430, 431, -1,  -1,
     -1,  -1,  -1,  432, 433, 434, -1,  -1,  -1,  -1,  -1,  435, 436,
     437, -1,  -1,  -1,  -1,  -1,  459, 438, 439, -1,  -1,  -1,  -1,
     -1,  -1,  460, 440, -1,  -1,  -1,  -1,  -1,  -1,  -1,  461}};

constexpr uint8_t kFileToFile[] = {0, 1, 2, 3, 3, 2, 1, 0};
constexpr int kWdlToMap[5] = {1, 3, 0, 2, 0};
constexpr uint8_t kPAFlags[5] = {8, 0, 0, 0, 4};

size_t Binomial[7][64];
size_t PawnIdx[2][6][24];
size_t PawnFactorFile[6][4];
size_t PawnFactorRank[6][6];
Key MaterialHash[16][64];

void init_indices() {
  // Binomial[k][n] = Bin(n, k)
  for (int i = 0; i < 7; i++)
    for (int j = 0; j < 64; j++) {
      size_t f = 1;
      size_t l = 1;
      for (int k = 0; k < i; k++) {
        f *= (j - k);
        l *= (k + 1);
      }
      Binomial[i][j] = f / l;
    }

  for (int i = 0; i < 6; i++) {
    size_t s = 0;
    for (int j = 0; j < 24; j++) {
      PawnIdx[0][i][j] = s;
      s += Binomial[i][kPawnTwist[0][(1 + (j % 6)) * 8 + (j / 6)]];
      if ((j + 1) % 6 == 0) {
        PawnFactorFile[i][j / 6] = s;
        s = 0;
      }
    }
  }

  for (int i = 0; i < 6; i++) {
    size_t s = 0;
    for (int j = 0; j < 24; j++) {
      PawnIdx[1][i][j] = s;
      s += Binomial[i][kPawnTwist[1][(1 + (j / 4)) * 8 + (j % 4)]];
      if ((j + 1) % 4 == 0) {
        PawnFactorRank[i][j / 4] = s;
        s = 0;
      }
    }
  }
  // TODO: choose a good seed.
  std::mt19937 gen(123523465);
  std::uniform_int_distribution<Key> dist(std::numeric_limits<Key>::lowest(),
                                          std::numeric_limits<Key>::max());
  for (int i = 0; i < 16; i++) {
    // MaterialHash for 0 instances of a piece is 0 as an optimization so
    // calc_key_from_pieces doesn't have to add in all the missing pieces.
    MaterialHash[i][0] = 0;
    for (int j = 1; j < 64; j++) {
      MaterialHash[i][j] = dist(gen);
    }
  }
}

std::once_flag indicies_flag;

void initonce_indicies() { std::call_once(indicies_flag, init_indices); }

// Produce a 64-bit material key corresponding to the material combination
// defined by pcs[16], where pcs[1], ..., pcs[6] are the number of white
// pawns, ..., kings and pcs[9], ..., pcs[14] are the number of black
// pawns, ..., kings.
Key calc_key_from_pcs(int* pcs, bool flip) {
  Key key = 0;

  const int color = !flip ? 0 : 8;
  for (int i = W_PAWN; i <= B_KING; i++) key += MaterialHash[i][pcs[i ^ color]];

  return key;
}

// Produce a 64-bit material key corresponding to the material combination
// piece[0], ..., piece[num - 1], where each value corresponds to a piece
// (1-6 for white pawn-king, 9-14 for black pawn-king).
Key calc_key_from_pieces(uint8_t* piece, int num) {
  Key key = 0;

  for (int i = 0; i < num; i++) {
    if (piece[i]) key += MaterialHash[piece[i]][1];
  }

  return key;
}

Key calc_key_from_position(const ChessBoard& pos) {
  Key key = 0;
  const bool flipped = pos.flipped();
  for (int i = PAWN; i <= KING; i++) {
    // White pieces - ours if not flipped.
    key += MaterialHash[i][count_pieces(pos, i, flipped)];
    // Black pieces - ours if flipped.
    key += MaterialHash[i + 8][count_pieces(pos, i, !flipped)];
  }
  return key;
}

int leading_pawn(int* p, BaseEntry* be, const int enc) {
  for (int i = 1; i < be->pawns[0]; i++) {
    if (kFlap[enc - 1][p[0]] > kFlap[enc - 1][p[i]]) Swap(p[0], p[i]);
  }
  return enc == FILE_ENC ? kFileToFile[p[0] & 7] : (p[0] - 8) >> 3;
}

size_t encode(int* p, EncInfo* ei, BaseEntry* be, const int enc) {
  const int n = be->num;
  size_t idx;
  int k;

  if (p[0] & 0x04) {
    for (int i = 0; i < n; i++) p[i] ^= 0x07;
  }

  if (enc == PIECE_ENC) {
    if (p[0] & 0x20) {
      for (int i = 0; i < n; i++) p[i] ^= 0x38;
    }

    for (int i = 0; i < n; i++) {
      if (kOffDiag[p[i]]) {
        if (kOffDiag[p[i]] > 0 && i < (be->kk_enc ? 2 : 3)) {
          for (int j = 0; j < n; j++) p[j] = kFlipDiag[p[j]];
        }
        break;
      }
    }

    if (be->kk_enc) {
      idx = kKKIdx[kTriangle[p[0]]][p[1]];
      k = 2;
    } else {
      const int s1 = (p[1] > p[0]);
      const int s2 = (p[2] > p[0]) + (p[2] > p[1]);

      if (kOffDiag[p[0]]) {
        idx = kTriangle[p[0]] * 63 * 62 + (p[1] - s1) * 62 + (p[2] - s2);
      } else if (kOffDiag[p[1]]) {
        idx =
            6 * 63 * 62 + kDiag[p[0]] * 28 * 62 + kLower[p[1]] * 62 + p[2] - s2;
      } else if (kOffDiag[p[2]]) {
        idx = 6 * 63 * 62 + 4 * 28 * 62 + kDiag[p[0]] * 7 * 28 +
              (kDiag[p[1]] - s1) * 28 + kLower[p[2]];
      } else {
        idx = 6 * 63 * 62 + 4 * 28 * 62 + 4 * 7 * 28 + kDiag[p[0]] * 7 * 6 +
              (kDiag[p[1]] - s1) * 6 + (kDiag[p[2]] - s2);
      }
      k = 3;
    }
    idx *= ei->factor[0];
  } else {
    for (int i = 1; i < be->pawns[0]; i++) {
      for (int j = i + 1; j < be->pawns[0]; j++) {
        if (kPawnTwist[enc - 1][p[i]] < kPawnTwist[enc - 1][p[j]]) {
          Swap(p[i], p[j]);
        }
      }
    }
    k = be->pawns[0];
    idx = PawnIdx[enc - 1][k - 1][kFlap[enc - 1][p[0]]];
    for (int i = 1; i < k; i++) {
      idx += Binomial[k - i][kPawnTwist[enc - 1][p[i]]];
    }
    idx *= ei->factor[0];

    // Pawns of other color
    if (be->pawns[1]) {
      const int t = k + be->pawns[1];
      for (int i = k; i < t; i++) {
        for (int j = i + 1; j < t; j++) {
          if (p[i] > p[j]) Swap(p[i], p[j]);
        }
      }
      size_t s = 0;
      for (int i = k; i < t; i++) {
        const int sq = p[i];
        int skips = 0;
        for (int j = 0; j < k; j++) skips += (sq > p[j]);
        s += Binomial[i - k + 1][sq - skips - 8];
      }
      idx += s * ei->factor[k];
      k = t;
    }
  }

  for (; k < n;) {
    const int t = k + ei->norm[k];
    for (int i = k; i < t; i++) {
      for (int j = i + 1; j < t; j++) {
        if (p[i] > p[j]) Swap(p[i], p[j]);
      }
    }
    size_t s = 0;
    for (int i = k; i < t; i++) {
      const int sq = p[i];
      int skips = 0;
      for (int j = 0; j < k; j++) skips += (sq > p[j]);
      s += Binomial[i - k + 1][sq - skips];
    }
    idx += s * ei->factor[k];
    k = t;
  }

  return idx;
}

size_t encode_piece(int* p, EncInfo* ei, BaseEntry* be) {
  return encode(p, ei, be, PIECE_ENC);
}

size_t encode_pawn_f(int* p, EncInfo* ei, BaseEntry* be) {
  return encode(p, ei, be, FILE_ENC);
}

size_t encode_pawn_r(int* p, EncInfo* ei, BaseEntry* be) {
  return encode(p, ei, be, RANK_ENC);
}

// Count number of placements of k like pieces on n squares
size_t subfactor(size_t k, size_t n) {
  size_t f = n;
  size_t l = 1;
  for (size_t i = 1; i < k; i++) {
    f *= n - i;
    l *= i + 1;
  }

  return f / l;
}

size_t init_enc_info(EncInfo* ei, BaseEntry* be, uint8_t* tb, int shift, int t,
                     const int enc) {
  const bool more_pawns = enc != PIECE_ENC && be->pawns[1] > 0;

  for (int i = 0; i < be->num; i++) {
    ei->pieces[i] = (tb[i + 1 + more_pawns] >> shift) & 0x0f;
    ei->norm[i] = 0;
  }

  const int order = (tb[0] >> shift) & 0x0f;
  const int order2 = more_pawns ? (tb[1] >> shift) & 0x0f : 0x0f;

  int k = ei->norm[0] = enc != PIECE_ENC ? be->pawns[0] : be->kk_enc ? 2 : 3;

  if (more_pawns) {
    ei->norm[k] = be->pawns[1];
    k += ei->norm[k];
  }

  for (int i = k; i < be->num; i += ei->norm[i]) {
    for (int j = i; j < be->num && ei->pieces[j] == ei->pieces[i]; j++) {
      ei->norm[i]++;
    }
  }

  int n = 64 - k;
  size_t f = 1;

  for (int i = 0; k < be->num || i == order || i == order2; i++) {
    if (i == order) {
      ei->factor[0] = f;
      f *= enc == FILE_ENC
               ? PawnFactorFile[ei->norm[0] - 1][t]
               : enc == RANK_ENC ? PawnFactorRank[ei->norm[0] - 1][t]
                                 : be->kk_enc ? 462 : 31332;
    } else if (i == order2) {
      ei->factor[ei->norm[0]] = f;
      f *= subfactor(ei->norm[ei->norm[0]], 48 - ei->norm[0]);
    } else {
      ei->factor[k] = f;
      f *= subfactor(ei->norm[k], n);
      n -= ei->norm[k];
      k += ei->norm[k];
    }
  }

  return f;
}

void calc_symLen(PairsData* d, uint32_t s, char* tmp) {
  uint8_t* w = d->symPat + 3 * s;
  const uint32_t s2 = (w[2] << 4) | (w[1] >> 4);
  if (s2 == 0x0fff)
    d->symLen[s] = 0;
  else {
    const uint32_t s1 = ((w[1] & 0xf) << 8) | w[0];
    if (!tmp[s1]) calc_symLen(d, s1, tmp);
    if (!tmp[s2]) calc_symLen(d, s2, tmp);
    d->symLen[s] = d->symLen[s1] + d->symLen[s2] + 1;
  }
  tmp[s] = 1;
}

int is_little_endian() {
  union {
    uint32_t i;
    uint8_t byte[4];
  } num_union = {0x01020304};

  return num_union.byte[0] == 4;
}

template <typename T, int Half = sizeof(T) / 2, int End = sizeof(T) - 1>
T swap_endian(T val) {
  static_assert(std::is_unsigned<T>::value,
                "Argument of swap_endian not unsigned");
  T x = val;
  uint8_t tmp, *c = (uint8_t*)&x;
  for (int i = 0; i < Half; ++i) {
    tmp = c[i], c[i] = c[End - i], c[End - i] = tmp;
  }
  return x;
}

uint32_t from_le_u32(uint32_t v) {
  return is_little_endian() ? v : swap_endian(v);
}

uint16_t from_le_u16(uint16_t v) {
  return is_little_endian() ? v : swap_endian(v);
}

uint64_t from_be_u64(uint64_t v) {
  return is_little_endian() ? swap_endian(v) : v;
}

uint32_t from_be_u32(uint32_t v) {
  return is_little_endian() ? swap_endian(v) : v;
}

uint32_t read_le_u32(void* p) {
  return from_le_u32(*static_cast<uint32_t*>(p));
}

uint16_t read_le_u16(void* p) {
  return from_le_u16(*static_cast<uint16_t*>(p));
}

PairsData* setup_pairs(uint8_t** ptr, size_t tb_size, size_t* size,
                       uint8_t* flags, int type) {
  PairsData* d;
  uint8_t* data = *ptr;

  *flags = data[0];
  if (data[0] & 0x80) {
    d = static_cast<PairsData*>(malloc(sizeof(*d)));
    d->idxBits = 0;
    d->constValue[0] = type == WDL ? data[1] : 0;
    d->constValue[1] = 0;
    *ptr = data + 2;
    size[0] = size[1] = size[2] = 0;
    return d;
  }

  const uint8_t block_size = data[1];
  const uint8_t idx_bits = data[2];
  const uint32_t real_num_blocks = read_le_u32(&data[4]);
  const uint32_t num_blocks = real_num_blocks + data[3];
  const int max_len = data[8];
  const int min_len = data[9];
  const int h = max_len - min_len + 1;
  const uint32_t num_syms = read_le_u16(&data[10 + 2 * h]);
  d = static_cast<PairsData*>(
      malloc(sizeof(*d) + h * sizeof(uint64_t) + num_syms));
  d->blockSize = block_size;
  d->idxBits = idx_bits;
  d->offset = reinterpret_cast<uint16_t*>(&data[10]);
  d->symLen = reinterpret_cast<uint8_t*>(d) + sizeof(*d) + h * sizeof(uint64_t);
  d->symPat = &data[12 + 2 * h];
  d->minLen = min_len;
  *ptr = &data[12 + 2 * h + 3 * num_syms + (num_syms & 1)];

  const size_t num_indices = (tb_size + (1ULL << idx_bits) - 1) >> idx_bits;
  size[0] = 6ULL * num_indices;
  size[1] = 2ULL * num_blocks;
  size[2] = static_cast<size_t>(real_num_blocks) << block_size;

  std::vector<char> tmp;
  tmp.resize(num_syms);
  memset(tmp.data(), 0, num_syms);
  for (uint32_t s = 0; s < num_syms; s++) {
    if (!tmp[s]) calc_symLen(d, s, tmp.data());
  }

  d->base[h - 1] = 0;
  for (int i = h - 2; i >= 0; i--) {
    d->base[i] = (d->base[i + 1] +
                  read_le_u16(reinterpret_cast<uint8_t*>(d->offset + i)) -
                  read_le_u16(reinterpret_cast<uint8_t*>(d->offset + i + 1))) /
                 2;
  }
  for (int i = 0; i < h; i++) d->base[i] <<= 64 - (min_len + i);

  d->offset -= d->minLen;

  return d;
}

uint8_t* decompress_pairs(PairsData* d, size_t idx) {
  if (!d->idxBits) return d->constValue;

  const uint32_t main_idx = idx >> d->idxBits;
  int lit_idx = (idx & ((static_cast<size_t>(1) << d->idxBits) - 1)) -
                (static_cast<size_t>(1) << (d->idxBits - 1));
  uint32_t block;
  memcpy(&block, d->indexTable + 6 * main_idx, sizeof(block));
  block = from_le_u32(block);

  const uint16_t idx_offset =
      *reinterpret_cast<uint16_t*>(d->indexTable + 6 * main_idx + 4);
  lit_idx += from_le_u16(idx_offset);

  if (lit_idx < 0) {
    while (lit_idx < 0) lit_idx += d->sizeTable[--block] + 1;
  } else {
    while (lit_idx > d->sizeTable[block]) lit_idx -= d->sizeTable[block++] + 1;
  }

  uint32_t* ptr = reinterpret_cast<uint32_t*>(
      d->data + (static_cast<size_t>(block) << d->blockSize));

  const int m = d->minLen;
  uint16_t* offset = d->offset;
  uint64_t* base = d->base - m;
  uint8_t* sym_len = d->symLen;
  uint32_t sym;
  uint32_t bit_cnt = 0;  // number of "empty bits" in code

  uint64_t code = from_be_u64(*reinterpret_cast<uint64_t*>(ptr));

  ptr += 2;
  for (;;) {
    int l = m;
    while (code < base[l]) l++;
    sym = from_le_u16(offset[l]);
    sym += (code - base[l]) >> (64 - l);
    if (lit_idx < static_cast<int>(sym_len[sym]) + 1) break;
    lit_idx -= static_cast<int>(sym_len[sym]) + 1;
    code <<= l;
    bit_cnt += l;
    if (bit_cnt >= 32) {
      bit_cnt -= 32;
      const uint32_t tmp = from_be_u32(*ptr++);
      code |= static_cast<uint64_t>(tmp) << bit_cnt;
    }
  }

  uint8_t* symPat = d->symPat;
  while (sym_len[sym] != 0) {
    uint8_t* w = symPat + (3 * sym);
    const int s1 = ((w[1] & 0xf) << 8) | w[0];
    if (lit_idx < static_cast<int>(sym_len[s1]) + 1) {
      sym = s1;
    } else {
      lit_idx -= static_cast<int>(sym_len[s1]) + 1;
      sym = (w[2] << 4) | (w[1] >> 4);
    }
  }

  return &symPat[3 * sym];
}

// p[i] is to contain the square 0-63 (A1-H8) for a piece of type
// pc[i] ^ flip, where 1 = white pawn, ..., 14 = black king and pc ^ flip
// flips between white and black if flip == true.
// Pieces of the same type are guaranteed to be consecutive.
int fill_squares(const ChessBoard& pos, uint8_t* pc, bool flip, int mirror,
                 int* p, int i) {
  // if pos.flipped the board is already mirrored, so mirror it again.
  if (pos.flipped()) mirror ^= 0x38;
  BitBoard bb = pieces(pos, pc[i] & 7,
                       static_cast<bool>((pc[i] >> 3)) ^ flip ^ pos.flipped());
  for (auto sq : bb) {
    p[i++] = sq.as_int() ^ mirror;
  }
  return i;
}

}  // namespace

class SyzygyTablebaseImpl {
 public:
  SyzygyTablebaseImpl(const std::string& paths)
      : piece_entries_(TB_MAX_PIECE), pawn_entries_(TB_MAX_PAWN) {
    initonce_indicies();

    if (paths.size() == 0 || paths == "<empty>") return;
    paths_ = paths;

    tb_hash_.resize(1 << TB_HASHBITS);

    char str[33];

    for (int i = 0; i < 5; i++) {
      sprintf(str, "K%cvK", pchr(i));
      init_tb(str);
    }

    for (int i = 0; i < 5; i++) {
      for (int j = i; j < 5; j++) {
        sprintf(str, "K%cvK%c", pchr(i), pchr(j));
        init_tb(str);
      }
    }

    for (int i = 0; i < 5; i++) {
      for (int j = i; j < 5; j++) {
        sprintf(str, "K%c%cvK", pchr(i), pchr(j));
        init_tb(str);
      }
    }

    for (int i = 0; i < 5; i++) {
      for (int j = i; j < 5; j++) {
        for (int k = 0; k < 5; k++) {
          sprintf(str, "K%c%cvK%c", pchr(i), pchr(j), pchr(k));
          init_tb(str);
        }
      }
    }

    for (int i = 0; i < 5; i++) {
      for (int j = i; j < 5; j++) {
        for (int k = j; k < 5; k++) {
          sprintf(str, "K%c%c%cvK", pchr(i), pchr(j), pchr(k));
          init_tb(str);
        }
      }
    }

    // 6- and 7-piece TBs make sense only with a 64-bit address space
    if (sizeof(size_t) < 8 || TB_PIECES < 6) goto finished;

    for (int i = 0; i < 5; i++) {
      for (int j = i; j < 5; j++) {
        for (int k = i; k < 5; k++) {
          for (int l = (i == k) ? j : k; l < 5; l++) {
            sprintf(str, "K%c%cvK%c%c", pchr(i), pchr(j), pchr(k), pchr(l));
            init_tb(str);
          }
        }
      }
    }

    for (int i = 0; i < 5; i++) {
      for (int j = i; j < 5; j++) {
        for (int k = j; k < 5; k++) {
          for (int l = 0; l < 5; l++) {
            sprintf(str, "K%c%c%cvK%c", pchr(i), pchr(j), pchr(k), pchr(l));
            init_tb(str);
          }
        }
      }
    }

    for (int i = 0; i < 5; i++) {
      for (int j = i; j < 5; j++) {
        for (int k = j; k < 5; k++) {
          for (int l = k; l < 5; l++) {
            sprintf(str, "K%c%c%c%cvK", pchr(i), pchr(j), pchr(k), pchr(l));
            init_tb(str);
          }
        }
      }
    }

    if (TB_PIECES < 7) goto finished;

    for (int i = 0; i < 5; i++) {
      for (int j = i; j < 5; j++) {
        for (int k = j; k < 5; k++) {
          for (int l = k; l < 5; l++) {
            for (int m = l; m < 5; m++) {
              sprintf(str, "K%c%c%c%c%cvK", pchr(i), pchr(j), pchr(k), pchr(l),
                      pchr(m));
              init_tb(str);
            }
          }
        }
      }
    }

    for (int i = 0; i < 5; i++) {
      for (int j = i; j < 5; j++) {
        for (int k = j; k < 5; k++) {
          for (int l = k; l < 5; l++) {
            for (int m = 0; m < 5; m++) {
              sprintf(str, "K%c%c%c%cvK%c", pchr(i), pchr(j), pchr(k), pchr(l),
                      pchr(m));
              init_tb(str);
            }
          }
        }
      }
    }

    for (int i = 0; i < 5; i++) {
      for (int j = i; j < 5; j++) {
        for (int k = j; k < 5; k++) {
          for (int l = 0; l < 5; l++) {
            for (int m = l; m < 5; m++) {
              sprintf(str, "K%c%c%cvK%c%c", pchr(i), pchr(j), pchr(k), pchr(l),
                      pchr(m));
              init_tb(str);
            }
          }
        }
      }
    }

  finished:
    CERR << "Found " << num_wdl_ << " WDL, " << num_dtm_ << " DTM and "
         << num_dtz_ << " DTZ tablebase files.";
  }

  ~SyzygyTablebaseImpl() {
    // if pathString was set there may be entries in need of cleaning.
    if (!paths_.empty()) {
      for (int i = 0; i < num_piece_entries_; i++)
        free_tb_entry(&piece_entries_[i]);
      for (int i = 0; i < num_pawn_entries_; i++)
        free_tb_entry(&pawn_entries_[i]);
    }
  }

  int max_cardinality() const { return max_cardinality_; }

  int probe_wdl_table(const ChessBoard& pos, int* success) {
    return probe_table(pos, 0, success, WDL);
  }

  int probe_dtm_table(const ChessBoard& pos, int won, int* success) {
    return probe_table(pos, won, success, DTM);
  }

  int probe_dtz_table(const ChessBoard& pos, int wdl, int* success) {
    return probe_table(pos, wdl, success, DTZ);
  }

 private:
  std::string name_for_tb(const char* str, const char* suffix) {
    std::stringstream path_string_stream(paths_);
    std::string path;
    std::ifstream stream;
    while (std::getline(path_string_stream, path, SEP_CHAR)) {
      std::string fname = path + "/" + str + suffix;
      stream.open(fname);
      if (stream.is_open()) return fname;
    }
    return std::string();
  }

  bool test_tb(const char* str, const char* suffix) {
    return !name_for_tb(str, suffix).empty();
  }

  void* map_tb(const char* name, const char* suffix, map_t* mapping) {
    std::string fname = name_for_tb(name, suffix);
    void* base_address;
#ifndef _WIN32
    struct stat statbuf;
    int fd = ::open(fname.c_str(), O_RDONLY);
    if (fd == -1) return nullptr;
    fstat(fd, &statbuf);
    if (statbuf.st_size % 64 != 16) {
      throw Exception("Corrupt tablebase file " + fname);
    }
    *mapping = statbuf.st_size;
    base_address = mmap(nullptr, statbuf.st_size, PROT_READ, MAP_SHARED, fd, 0);
    ::close(fd);
    if (base_address == MAP_FAILED) {
      throw Exception("Could not mmap() " + fname);
    }
#else
    const HANDLE fd =
        CreateFileA(fname.c_str(), GENERIC_READ, FILE_SHARE_READ, nullptr,
                    OPEN_EXISTING, FILE_ATTRIBUTE_NORMAL, nullptr);
    if (fd == INVALID_HANDLE_VALUE) return nullptr;
    DWORD size_high;
    DWORD size_low = GetFileSize(fd, &size_high);
    if (size_low % 64 != 16) {
      throw Exception("Corrupt tablebase file " + fname);
    }
    HANDLE mmap = CreateFileMapping(fd, nullptr, PAGE_READONLY, size_high,
                                    size_low, nullptr);
    CloseHandle(fd);
    if (!mmap) {
      throw Exception("CreateFileMapping() failed");
    }
    *mapping = mmap;
    base_address = MapViewOfFile(mmap, FILE_MAP_READ, 0, 0, 0);
    if (!base_address) {
      throw Exception("MapViewOfFile() failed, name = " + fname + ", error = " + std::to_string(GetLastError()));
    }
#endif
    return base_address;
  }

  void unmap_file(void* base_address, map_t mapping) {
#ifndef _WIN32
    munmap(base_address, mapping);
#else
    UnmapViewOfFile(base_address);
    CloseHandle(mapping);
#endif
  }

  void add_to_hash(BaseEntry* ptr, Key key) {
    int idx;

    idx = key >> (64 - TB_HASHBITS);

    while (tb_hash_[idx].ptr) idx = (idx + 1) & ((1 << TB_HASHBITS) - 1);

    tb_hash_[idx].key = key;
    tb_hash_[idx].ptr = ptr;
  }

  void init_tb(char* str) {
    if (!test_tb(str, kSuffix[WDL])) return;

    int pcs[16];
    for (int i = 0; i < 16; i++) pcs[i] = 0;
    int color = 0;
    for (char* s = str; *s; s++) {
      if (*s == 'v') {
        color = 8;
      } else {
        for (int i = PAWN; i <= KING; i++) {
          if (*s == kPieceToChar[i]) {
            pcs[i | color]++;
            break;
          }
        }
      }
    }

    const Key key = calc_key_from_pcs(pcs, false);
    const Key key2 = calc_key_from_pcs(pcs, true);

    const bool has_pawns = pcs[W_PAWN] || pcs[B_PAWN];

    BaseEntry* be =
        has_pawns
            ? static_cast<BaseEntry*>(&pawn_entries_[num_pawn_entries_++])
            : static_cast<BaseEntry*>(&piece_entries_[num_piece_entries_++]);
    be->hasPawns = has_pawns;
    be->key = key;
    be->symmetric = key == key2;
    be->num = 0;
    for (int i = 0; i < 16; i++) be->num += pcs[i];

    num_wdl_++;
    num_dtm_ += be->hasDtm = test_tb(str, kSuffix[DTM]);
    num_dtz_ += be->hasDtz = test_tb(str, kSuffix[DTZ]);

    max_cardinality_ = std::max(max_cardinality_, static_cast<int>(be->num));
    if (be->hasDtm)
      max_cardinality_dtm_ =
          std::max(max_cardinality_dtm_, static_cast<int>(be->num));

    for (int type = 0; type < 3; type++) be->ready[type] = 0;

    if (!be->hasPawns) {
      int j = 0;
      for (int i = 0; i < 16; i++) {
        if (pcs[i] == 1) j++;
      }
      be->kk_enc = j == 2;
    } else {
      be->pawns[0] = pcs[W_PAWN];
      be->pawns[1] = pcs[B_PAWN];
      if (pcs[B_PAWN] && (!pcs[W_PAWN] || pcs[W_PAWN] > pcs[B_PAWN])) {
        Swap(be->pawns[0], be->pawns[1]);
      }
    }

    add_to_hash(be, key);
    if (key != key2) add_to_hash(be, key2);
  }

  void free_tb_entry(BaseEntry* be) {
    for (int type = 0; type < 3; type++) {
      if (atomic_load_explicit(&be->ready[type], std::memory_order_relaxed)) {
        unmap_file(be->data[type], be->mapping[type]);
        const int num = num_tables(be, type);
        EncInfo* ei = first_ei(be, type);
        for (int t = 0; t < num; t++) {
          free(ei[t].precomp);
          if (type != DTZ) free(ei[num + t].precomp);
        }
        atomic_store_explicit(&be->ready[type], false,
                              std::memory_order_relaxed);
      }
    }
  }

  bool init_table(BaseEntry* be, const char* str, int type) {
    uint8_t* data =
        static_cast<uint8_t*>(map_tb(str, kSuffix[type], &be->mapping[type]));
    if (!data) return false;

    if (read_le_u32(data) != kMagic[type]) {
      fprintf(stderr, "Corrupted table.\n");
      unmap_file(data, be->mapping[type]);
      return false;
    }

    be->data[type] = data;

    const bool split = type != DTZ && (data[4] & 0x01);
    if (type == DTM) be->dtmLossOnly = data[4] & 0x04;

    data += 5;

    size_t tb_size[6][2];
    const int num = num_tables(be, type);
    EncInfo* ei = first_ei(be, type);
    const int enc = !be->hasPawns ? PIECE_ENC : type != DTM ? FILE_ENC : RANK_ENC;

    for (int t = 0; t < num; t++) {
      tb_size[t][0] = init_enc_info(&ei[t], be, data, 0, t, enc);
      if (split) {
        tb_size[t][1] = init_enc_info(&ei[num + t], be, data, 4, t, enc);
      }
      data += be->num + 1 + (be->hasPawns && be->pawns[1]);
    }
    data += (uintptr_t)data & 1;

    size_t size[6][2][3];
    for (int t = 0; t < num; t++) {
      uint8_t flags;
      ei[t].precomp =
          setup_pairs(&data, tb_size[t][0], size[t][0], &flags, type);
      if (type == DTZ) {
        if (!be->hasPawns) {
          PIECE(be)->dtzFlags = flags;
        } else {
          PAWN(be)->dtzFlags[t] = flags;
        }
      }
      if (split) {
        ei[num + t].precomp =
            setup_pairs(&data, tb_size[t][1], size[t][1], &flags, type);
      } else if (type != DTZ) {
        ei[num + t].precomp = NULL;
      }
    }

    if (type == DTM && !be->dtmLossOnly) {
      uint16_t* map = reinterpret_cast<uint16_t*>(data);
      *(be->hasPawns ? &PAWN(be)->dtmMap : &PIECE(be)->dtmMap) = map;
      uint16_t(*mapIdx)[2][2] =
          be->hasPawns ? &PAWN(be)->dtmMapIdx[0] : &PIECE(be)->dtmMapIdx;
      for (int t = 0; t < num; t++) {
        for (int i = 0; i < 2; i++) {
          mapIdx[t][0][i] = reinterpret_cast<uint16_t*>(data) + 1 - map;
          data += 2 + 2 * read_le_u16(data);
        }
        if (split) {
          for (int i = 0; i < 2; i++) {
            mapIdx[t][1][i] = reinterpret_cast<uint16_t*>(data) + 1 - map;
            data += 2 + 2 * read_le_u16(data);
          }
        }
      }
    }

    if (type == DTZ) {
      void* map = data;
      *(be->hasPawns ? &PAWN(be)->dtzMap : &PIECE(be)->dtzMap) = map;
      uint16_t(*mapIdx)[4] =
          be->hasPawns ? &PAWN(be)->dtzMapIdx[0] : &PIECE(be)->dtzMapIdx;
      uint8_t* flags =
          be->hasPawns ? &PAWN(be)->dtzFlags[0] : &PIECE(be)->dtzFlags;
      for (int t = 0; t < num; t++) {
        if (flags[t] & 2) {
          if (!(flags[t] & 16)) {
            for (int i = 0; i < 4; i++) {
              mapIdx[t][i] = data + 1 - static_cast<uint8_t*>(map);
              data += 1 + data[0];
            }
          } else {
            data += reinterpret_cast<uintptr_t>(data) & 0x01;
            for (int i = 0; i < 4; i++) {
              mapIdx[t][i] = reinterpret_cast<uint16_t*>(data) + 1 -
                             static_cast<uint16_t*>(map);
              data += 2 + 2 * read_le_u16(data);
            }
          }
        }
      }
      data += reinterpret_cast<uintptr_t>(data) & 0x01;
    }

    for (int t = 0; t < num; t++) {
      ei[t].precomp->indexTable = data;
      data += size[t][0][0];
      if (split) {
        ei[num + t].precomp->indexTable = data;
        data += size[t][1][0];
      }
    }

    for (int t = 0; t < num; t++) {
      ei[t].precomp->sizeTable = reinterpret_cast<uint16_t*>(data);
      data += size[t][0][1];
      if (split) {
        ei[num + t].precomp->sizeTable = reinterpret_cast<uint16_t*>(data);
        data += size[t][1][1];
      }
    }

    for (int t = 0; t < num; t++) {
      data = reinterpret_cast<uint8_t*>(
          (reinterpret_cast<uintptr_t>(data) + 0x3f) & ~0x3f);
      ei[t].precomp->data = data;
      data += size[t][0][2];
      if (split) {
        data = reinterpret_cast<uint8_t*>(
            (reinterpret_cast<uintptr_t>(data) + 0x3f) & ~0x3f);
        ei[num + t].precomp->data = data;
        data += size[t][1][2];
      }
    }

    if (type == DTM && be->hasPawns) {
      PAWN(be)->dtmSwitched =
          calc_key_from_pieces(ei[0].pieces, be->num) != be->key;
    }

    return true;
  }

  int probe_table(const ChessBoard& pos, int s, int* success, const int type) {
    // Obtain the position's material-signature key
    const Key key = calc_key_from_position(pos);

    // Test for KvK
    if (type == WDL && (pos.ours() | pos.theirs()) == pos.kings()) {
      return 0;
    }

    int hash_idx = key >> (64 - TB_HASHBITS);
    while (tb_hash_[hash_idx].key && tb_hash_[hash_idx].key != key) {
      hash_idx = (hash_idx + 1) & ((1 << TB_HASHBITS) - 1);
    }
    if (!tb_hash_[hash_idx].ptr) {
      *success = 0;
      return 0;
    }

    BaseEntry* be = tb_hash_[hash_idx].ptr;
    if ((type == DTM && !be->hasDtm) || (type == DTZ && !be->hasDtz)) {
      *success = 0;
      return 0;
    }

    // Use double-checked locking to reduce locking overhead
    if (!atomic_load_explicit(&be->ready[type], std::memory_order_acquire)) {
      Mutex::Lock lock(ready_mutex_);
      if (!atomic_load_explicit(&be->ready[type], std::memory_order_relaxed)) {
        char str[16];
        prt_str(pos, str, be->key != key);
        if (!init_table(be, str, type)) {
          tb_hash_[hash_idx].ptr = nullptr;  // mark as deleted
          *success = 0;
          return 0;
        }
        atomic_store_explicit(&be->ready[type], true,
                              std::memory_order_release);
      }
    }

    bool bside, flip;
    if (!be->symmetric) {
      flip = key != be->key;
      bside = (!pos.flipped()) == flip;
      if (type == DTM && be->hasPawns && PAWN(be)->dtmSwitched) {
        flip = !flip;
        bside = !bside;
      }
    } else {
      flip = pos.flipped();
      bside = false;
    }

    EncInfo* ei = first_ei(be, type);
    int p[TB_PIECES];
    size_t idx;
    int t = 0;
    uint8_t flags = 0;

    if (!be->hasPawns) {
      if (type == DTZ) {
        flags = PIECE(be)->dtzFlags;
        if ((flags & 1) != bside && !be->symmetric) {
          *success = -1;
          return 0;
        }
      }
      ei = type != DTZ ? &ei[bside] : ei;
      for (int i = 0; i < be->num;) {
        i = fill_squares(pos, ei->pieces, flip, 0, p, i);
      }
      idx = encode_piece(p, ei, be);
    } else {
      int i = fill_squares(pos, ei->pieces, flip, flip ? 0x38 : 0, p, 0);
      t = leading_pawn(p, be, type != DTM ? FILE_ENC : RANK_ENC);
      if (type == DTZ) {
        flags = PAWN(be)->dtzFlags[t];
        if ((flags & 1) != bside && !be->symmetric) {
          *success = -1;
          return 0;
        }
      }
      ei = type == WDL ? &ei[t + 4 * bside]
                       : type == DTM ? &ei[t + 6 * bside] : &ei[t];
      while (i < be->num) {
        i = fill_squares(pos, ei->pieces, flip, flip ? 0x38 : 0, p, i);
      }
      idx = type != DTM ? encode_pawn_f(p, ei, be) : encode_pawn_r(p, ei, be);
    }

    uint8_t* w = decompress_pairs(ei->precomp, idx);

    if (type == WDL) return static_cast<int>(w[0]) - 2;

    int v = w[0] + ((w[1] & 0x0f) << 8);

    if (type == DTM) {
      if (!be->dtmLossOnly) {
        v = from_le_u16(
            be->hasPawns
                ? PAWN(be)->dtmMap[PAWN(be)->dtmMapIdx[t][bside][s] + v]
                : PIECE(be)->dtmMap[PIECE(be)->dtmMapIdx[bside][s] + v]);
      }
    } else {
      if (flags & 2) {
        const int m = kWdlToMap[s + 2];
        if (!(flags & 16)) {
          v = be->hasPawns
                  ? static_cast<uint8_t*>(
                        PAWN(be)->dtzMap)[PAWN(be)->dtzMapIdx[t][m] + v]
                  : static_cast<uint8_t*>(
                        PIECE(be)->dtzMap)[PIECE(be)->dtzMapIdx[m] + v];
        } else {
          v = from_le_u16(
              be->hasPawns
                  ? static_cast<uint16_t*>(
                        PAWN(be)->dtzMap)[PAWN(be)->dtzMapIdx[t][m] + v]
                  : static_cast<uint16_t*>(
                        PIECE(be)->dtzMap)[PIECE(be)->dtzMapIdx[m] + v]);
        }
      }
      if (!(flags & kPAFlags[s + 2]) || (s & 1)) v *= 2;
    }

    return v;
  }

  int max_cardinality_ = 0;
  int max_cardinality_dtm_ = 0;

  Mutex ready_mutex_;
  std::string paths_;

  int num_piece_entries_ = 0;
  int num_pawn_entries_ = 0;
  int num_wdl_ = 0;
  int num_dtm_ = 0;
  int num_dtz_ = 0;

  std::vector<PieceEntry> piece_entries_;
  std::vector<PawnEntry> pawn_entries_;
  std::vector<TbHashEntry> tb_hash_;
};

SyzygyTablebase::SyzygyTablebase() : max_cardinality_(0) {}

SyzygyTablebase::~SyzygyTablebase() = default;

bool SyzygyTablebase::init(const std::string& paths) {
  paths_ = paths;
  impl_.reset(new SyzygyTablebaseImpl(paths_));
  max_cardinality_ = impl_->max_cardinality();
  if (max_cardinality_ <= 2) {
    impl_ = nullptr;
    return false;
  }
  return true;
}

// For a position where the side to move has a winning capture it is not
// necessary to store a winning value so the generator treats such positions as
// "don't cares" and tries to assign to it a value that improves the compression
// ratio. Similarly, if the side to move has a drawing capture, then the
// position is at least drawn. If the position is won, then the TB needs to
// store a win value. But if the position is drawn, the TB may store a loss
// value if that is better for compression. All of this means that during
// probing, the engine must look at captures and probe their results and must
// probe the position itself. The "best" result of these probes is the correct
// result for the position. DTZ table don't store values when a following move
// is a zeroing winning move (winning capture or winning pawn move). Also DTZ
// store wrong values for positions where the best move is an ep-move (even if
// losing). So in all these cases set the state to ZEROING_BEST_MOVE.
template <bool CheckZeroingMoves>
WDLScore SyzygyTablebase::search(const Position& pos, ProbeState* result) {
  WDLScore value;
  WDLScore best_value = WDL_LOSS;
  auto move_list = pos.GetBoard().GenerateLegalMoves();
  const size_t total_count = move_list.size();
  size_t move_count = 0;
  for (const Move& move : move_list) {
    if (!is_capture(pos.GetBoard(), move) &&
        (!CheckZeroingMoves || !pos.GetBoard().pawns().get(move.from()))) {
      continue;
    }
    move_count++;
    auto new_pos = Position(pos, move);
    value = static_cast<WDLScore>(-search(new_pos, result));
    if (*result == FAIL) return WDL_DRAW;
    if (value > best_value) {
      best_value = value;
      if (value >= WDL_WIN) {
        *result = ZEROING_BEST_MOVE;  // Winning DTZ-zeroing move
        return value;
      }
    }
  }
  // In case we have already searched all the legal moves we don't have to probe
  // the TB because the stored score could be wrong. For instance TB tables do
  // not contain information on position with ep rights, so in this case the
  // result of probe_wdl_table is wrong. Also in case of only capture moves, for
  // instance here 4K3/4q3/6p1/2k5/6p1/8/8/8 w - - 0 7, we have to return with
  // ZEROING_BEST_MOVE set.
  const bool no_more_moves = (move_count && move_count == total_count);
  if (no_more_moves) {
    value = best_value;
  } else {
    int raw_result = static_cast<int>(ProbeState::OK);
    value = static_cast<WDLScore>(
        impl_->probe_wdl_table(pos.GetBoard(), &raw_result));
    *result = static_cast<ProbeState>(raw_result);
    if (*result == FAIL) return WDL_DRAW;
  }
  // DTZ stores a "don't care" value if bestValue is a win
  if (best_value >= value) {
    *result = (best_value > WDL_DRAW || no_more_moves ? ZEROING_BEST_MOVE : OK);
    return best_value;
  }
  *result = OK;
  return value;
}

// Probe the WDL table for a particular position.
// If *result != FAIL, the probe was successful.
// The return value is from the point of view of the side to move:
// -2 : loss
// -1 : loss, but draw under 50-move rule
//  0 : draw
//  1 : win, but draw under 50-move rule
//  2 : win
WDLScore SyzygyTablebase::probe_wdl(const Position& pos, ProbeState* result) {
  *result = OK;
  return search(pos, result);
}

// Probe the DTZ table for a particular position.
// If *result != FAIL, the probe was successful.
// The return value is from the point of view of the side to move:
//         n < -100 : loss, but draw under 50-move rule
// -100 <= n < -1   : loss in n ply (assuming 50-move counter == 0)
//        -1        : loss, the side to move is mated
//         0        : draw
//     1 < n <= 100 : win in n ply (assuming 50-move counter == 0)
//   100 < n        : win, but draw under 50-move rule
//
// The return value n can be off by 1: a return value -n can mean a loss  in n+1
// ply and a return value +n can mean a win in n+1 ply. This cannot happen for
// tables with positions exactly on the "edge" of the 50-move rule.
//
// This implies that if dtz > 0 is returned, the position is certainly a win if
// dtz + 50-move-counter <= 99. Care must be taken that the engine picks moves
// that preserve dtz + 50-move-counter <= 99.
//
// If n = 100 immediately after a capture or pawn move, then the position is
// also certainly a win, and during the whole phase until the next capture or
// pawn move, the inequality to be preserved is dtz
// + 50-movecounter <= 100.
//
// In short, if a move is available resulting in dtz + 50-move-counter <= 99,
// then do not accept moves leading to dtz + 50-move-counter == 100.
int SyzygyTablebase::probe_dtz(const Position& pos, ProbeState* result) {
  *result = OK;
  const WDLScore wdl = search<true>(pos, result);
  if (*result == FAIL || wdl == WDL_DRAW) {  // DTZ tables don't store draws
    return 0;
  }
  // DTZ stores a 'don't care' value in this case, or even a plain wrong one as
  // in case the best move is a losing ep, so it cannot be probed.
  if (*result == ZEROING_BEST_MOVE) return dtz_before_zeroing(wdl);
  int raw_result = 1;
  int dtz = impl_->probe_dtz_table(pos.GetBoard(), wdl, &raw_result);
  *result = static_cast<ProbeState>(raw_result);
  if (*result == FAIL) return 0;
  if (*result != CHANGE_STM) {
    return (dtz + 1 +
            100 * (wdl == WDL_BLESSED_LOSS || wdl == WDL_CURSED_WIN)) *
           sign_of(wdl);
  }
  // DTZ stores results for the other side, so we need to do a 1-ply search and
  // find the winning move that minimizes DTZ.
  int min_DTZ = 0xFFFF;
  for (const Move& move : pos.GetBoard().GenerateLegalMoves()) {
    Position next_pos = Position(pos, move);
    const bool zeroing = next_pos.GetRule50Ply() == 0;
    // For zeroing moves we want the dtz of the move _before_ doing it,
    // otherwise we will get the dtz of the next move sequence. Search the
    // position after the move to get the score sign (because even in a winning
    // position we could make a losing capture or going for a draw).
    dtz = zeroing ? -dtz_before_zeroing(search(next_pos, result))
                  : -probe_dtz(next_pos, result);
    // If the move mates, force minDTZ to 1
    if (dtz == 1 && next_pos.GetBoard().IsUnderCheck() &&
        next_pos.GetBoard().GenerateLegalMoves().empty()) {
      min_DTZ = 1;
    }
    // Convert result from 1-ply search. Zeroing moves are already accounted by
    // dtz_before_zeroing() that returns the DTZ of the previous move.
    if (!zeroing) dtz += sign_of(dtz);
    // Skip the draws and if we are winning only pick positive dtz
    if (dtz < min_DTZ && sign_of(dtz) == sign_of(wdl)) min_DTZ = dtz;
    if (*result == FAIL) return 0;
  }
  // When there are no legal moves, the position is mate: we return -1
  return min_DTZ == 0xFFFF ? -1 : min_DTZ;
}

// Use the DTZ tables to rank root moves.
//
// A return value false indicates that not all probes were successful.
bool SyzygyTablebase::root_probe(const Position& pos, bool has_repeated,
                                 std::vector<Move>* safe_moves) {
  ProbeState result;
  auto root_moves = pos.GetBoard().GenerateLegalMoves();
  // Obtain 50-move counter for the root position
  const int cnt50 = pos.GetRule50Ply();
  // Check whether a position was repeated since the last zeroing move.
  const bool rep = has_repeated;
  int dtz;
  std::vector<int> ranks;
  ranks.reserve(root_moves.size());
  int best_rank = -1000;
  // Probe and rank each move
  for (auto& m : root_moves) {
    Position next_pos = Position(pos, m);
    // Calculate dtz for the current move counting from the root position
    if (next_pos.GetRule50Ply() == 0) {
      // In case of a zeroing move, dtz is one of -101/-1/0/1/101
      const WDLScore wdl = static_cast<WDLScore>(-probe_wdl(next_pos, &result));
      dtz = dtz_before_zeroing(wdl);
    } else {
      // Otherwise, take dtz for the new position and correct by 1 ply
      dtz = -probe_dtz(next_pos, &result);
      dtz = dtz > 0 ? dtz + 1 : dtz < 0 ? dtz - 1 : dtz;
    }
    // Make sure that a mating move is assigned a dtz value of 1
    if (next_pos.GetBoard().IsUnderCheck() && dtz == 2 &&
        next_pos.GetBoard().GenerateLegalMoves().size() == 0) {
      dtz = 1;
    }
    if (result == FAIL) return false;
    // Better moves are ranked higher. Certain wins are ranked equally.
    // Losing moves are ranked equally unless a 50-move draw is in sight.
    int r = dtz > 0
                ? (dtz + cnt50 <= 99 && !rep ? 1000 : 1000 - (dtz + cnt50))
                : dtz < 0 ? (-dtz * 2 + cnt50 < 100 ? -1000
                                                    : -1000 + (-dtz + cnt50))
                          : 0;
    if (r > best_rank) best_rank = r;
    ranks.push_back(r);
  }
  // Disable all but the equal best moves.
  int counter = 0;
  for (auto& m : root_moves) {
    if (ranks[counter] == best_rank) {
      safe_moves->push_back(m);
    }
    counter++;
  }
  return true;
}

// Use the WDL tables to rank root moves.
// This is a fallback for the case that some or all DTZ tables are missing.
//
// A return value false indicates that not all probes were successful.
bool SyzygyTablebase::root_probe_wdl(const Position& pos,
                                     std::vector<Move>* safe_moves) {
  static const int WDL_to_rank[] = {-1000, -899, 0, 899, 1000};
  auto root_moves = pos.GetBoard().GenerateLegalMoves();
  ProbeState result;
  std::vector<int> ranks;
  ranks.reserve(root_moves.size());
  int best_rank = -1000;
  // Probe and rank each move
  for (auto& m : root_moves) {
    Position nextPos = Position(pos, m);
    const WDLScore wdl = static_cast<WDLScore>(-probe_wdl(nextPos, &result));
    if (result == FAIL) return false;
    ranks.push_back(WDL_to_rank[wdl + 2]);
    if (ranks.back() > best_rank) best_rank = ranks.back();
  }
  // Disable all but the equal best moves.
  int counter = 0;
  for (auto& m : root_moves) {
    if (ranks[counter] == best_rank) {
      safe_moves->push_back(m);
    }
    counter++;
  }
  return true;
}
}  // namespace lczero

```

`src/syzygy/syzygy.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <algorithm>
#include <atomic>
#include <deque>
#include <memory>
#include <tuple>
#include <vector>
#include "chess/position.h"

namespace lczero {

enum WDLScore {
  WDL_LOSS = -2,          // Loss
  WDL_BLESSED_LOSS = -1,  // Loss, but draw under 50-move rule
  WDL_DRAW = 0,           // Draw
  WDL_CURSED_WIN = 1,     // Win, but draw under 50-move rule
  WDL_WIN = 2,            // Win
};

// Possible states after a probing operation
enum ProbeState {
  FAIL = 0,              // Probe failed (missing file table)
  OK = 1,                // Probe succesful
  CHANGE_STM = -1,       // DTZ should check the other side
  ZEROING_BEST_MOVE = 2  // Best move zeroes DTZ (capture or pawn move)
};

class SyzygyTablebaseImpl;

// Provides methods to load and probe syzygy tablebases.
// Thread safe methods are thread safe subject to the non-thread sfaety
// conditions of the init method.
class SyzygyTablebase {
 public:
  SyzygyTablebase();
  virtual ~SyzygyTablebase();
  // Current maximum number of pieces on board that can be probed for. Will
  // be 0 unless initialized with tablebase paths.
  // Thread safe.
  int max_cardinality() { return max_cardinality_; }
  // Allows for the tablebases being used to be changed. This method is not
  // thread safe, there must be no concurrent usage while this method is
  // running. All other thread safe method calls must be strictly ordered with
  // respect to this method.
  bool init(const std::string& paths);
  // Probes WDL tables for the given position to determine a WDLScore.
  // Thread safe.
  // Result is only strictly valid for positions with 0 ply 50 move counter.
  // Probe state will return FAIL if the position is not in the tablebase.
  WDLScore probe_wdl(const Position& pos, ProbeState* result);
  // Probes DTZ tables for the given position to determine the number of ply
  // before a zeroing move under optimal play.
  // Thread safe.
  // Probe state will return FAIL if the position is not in the tablebase.
  int probe_dtz(const Position& pos, ProbeState* result);
  // Probes DTZ tables to determine which moves are on the optimal play path.
  // Assumes the position is one reached such that the side to move has been
  // performing optimal play moves since the last 50 move counter reset.
  // has_repeated should be whether there are any repeats since last 50 move
  // counter reset.
  // Thread safe.
  // Returns false if the position is not in the tablebase.
  // Safe moves are added to the safe_moves output paramater.
  bool root_probe(const Position& pos, bool has_repeated,
                  std::vector<Move>* safe_moves);
  // Probes WDL tables to determine which moves might be on the optimal play
  // path. If 50 move ply counter is non-zero some (or maybe even all) of the
  // returned safe moves in a 'winning' position, may actually be draws.
  // Returns false if the position is not in the tablebase.
  // Safe moves are added to the safe_moves output paramater.
  bool root_probe_wdl(const Position& pos, std::vector<Move>* safe_moves);

 private:
  template <bool CheckZeroingMoves = false>
  WDLScore search(const Position& pos, ProbeState* result);

  std::string paths_;
  // Caches the max_cardinality from the impl, as max_cardinality may be a hot
  // path.
  int max_cardinality_;
  std::unique_ptr<SyzygyTablebaseImpl> impl_;
};

}  // namespace lczero

```

`src/syzygy/syzygy_test.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <gtest/gtest.h>

#include <iostream>
#include "src/syzygy/syzygy.h"

namespace lczero {

// Try to find syzygy relative to current working directory.
constexpr auto kPaths = "syzygy";

void TestValidRootExpectation(SyzygyTablebase* tablebase,
                              const std::string& fen,
                              const MoveList& valid_moves,
                              const MoveList& invalid_moves,
                              const MoveList& invalid_dtz_only = {},
                              bool has_repeated = false) {
  ChessBoard board;
  PositionHistory history;
  int rule50ply;
  int gameply;
  board.SetFromFen(fen, &rule50ply, &gameply);
  history.Reset(board, rule50ply, gameply);
  MoveList allowed_moves_dtz;
  tablebase->root_probe(history.Last(), has_repeated, &allowed_moves_dtz);
  MoveList allowed_moves_wdl;
  tablebase->root_probe_wdl(history.Last(), &allowed_moves_wdl);
  for (auto move : valid_moves) {
    EXPECT_TRUE(std::find(allowed_moves_dtz.begin(), allowed_moves_dtz.end(),
                          move) != allowed_moves_dtz.end());
    EXPECT_TRUE(std::find(allowed_moves_wdl.begin(), allowed_moves_wdl.end(),
                          move) != allowed_moves_wdl.end());
  }
  for (auto move : invalid_moves) {
    EXPECT_FALSE(std::find(allowed_moves_dtz.begin(), allowed_moves_dtz.end(),
                           move) != allowed_moves_dtz.end());
    EXPECT_FALSE(std::find(allowed_moves_wdl.begin(), allowed_moves_wdl.end(),
                           move) != allowed_moves_wdl.end());
  }
  for (auto move : invalid_dtz_only) {
    EXPECT_FALSE(std::find(allowed_moves_dtz.begin(), allowed_moves_dtz.end(),
                           move) != allowed_moves_dtz.end());
    EXPECT_TRUE(std::find(allowed_moves_wdl.begin(), allowed_moves_wdl.end(),
                          move) != allowed_moves_wdl.end());
  }
}

void TestValidExpectation(SyzygyTablebase* tablebase, const std::string& fen,
                          WDLScore expected, int expected_dtz) {
  ChessBoard board;
  PositionHistory history;
  board.SetFromFen(fen);
  history.Reset(board, 0, 1);
  ProbeState result;
  WDLScore score = tablebase->probe_wdl(history.Last(), &result);
  EXPECT_NE(result, FAIL);
  EXPECT_EQ(score, expected);
  int moves = tablebase->probe_dtz(history.Last(), &result);
  EXPECT_NE(result, FAIL);
  EXPECT_EQ(moves, expected_dtz);
}

TEST(Syzygy, Simple3PieceProbes) {
  SyzygyTablebase tablebase;
  tablebase.init(kPaths);
  if (tablebase.max_cardinality() < 3) {
    // These probes require 3 piece tablebase.
    return;
  }
  // Longest 3 piece position.
  TestValidExpectation(&tablebase, "8/8/8/8/8/8/2Rk4/1K6 b - - 0 1", WDL_LOSS,
                       -32);
  // Invert color of above, no change.
  TestValidExpectation(&tablebase, "8/8/8/8/8/8/2rK4/1k6 w - - 0 1", WDL_LOSS,
                       -32);
  // Horizontal mirror.
  TestValidExpectation(&tablebase, "8/8/8/8/8/8/4kR2/6K1 b - - 0 1", WDL_LOSS,
                       -32);
  // Vertical mirror.
  TestValidExpectation(&tablebase, "6K1/4kR2/8/8/8/8/8/8 b - - 0 1", WDL_LOSS,
                       -32);
  // Horizontal mirror again.
  TestValidExpectation(&tablebase, "1K6/2Rk4/8/8/8/8/8/8 b - - 0 1", WDL_LOSS,
                       -32);

  // A draw by capture position, leaving KvK.
  TestValidExpectation(&tablebase, "5Qk1/8/8/8/8/8/8/4K3 b - - 0 1", WDL_DRAW,
                       0);

  // A position with a pawn which is to move and win from there.
  TestValidExpectation(&tablebase, "6k1/8/8/8/8/5p2/8/2K5 b - - 0 1", WDL_WIN,
                       1);

  // A position with a pawn that needs a king move first to win.
  TestValidExpectation(&tablebase, "8/8/8/8/8/k1p5/8/3K4 b - - 0 1", WDL_WIN,
                       3);

  // A position with a pawn that needs a few king moves before its a loss.
  TestValidExpectation(&tablebase, "8/2p5/8/8/8/5k2/8/2K5 w - - 0 1", WDL_LOSS,
                       -8);
}

TEST(Syzygy, Root3PieceProbes) {
  SyzygyTablebase tablebase;
  tablebase.init(kPaths);
  if (tablebase.max_cardinality() < 3) {
    // These probes require 3 piece tablebase.
    return;
  }
  TestValidRootExpectation(&tablebase, "5Qk1/8/8/8/8/8/8/4K3 b - - 0 1",
                           {Move("g8f8", true)}, {Move("g8h7", true)});
  TestValidRootExpectation(&tablebase, "6k1/8/8/8/8/5p2/8/2K5 b - - 0 1",
                           {Move("f3f2", true)}, {Move("g8h7", true)});
  TestValidRootExpectation(&tablebase, "8/8/8/8/8/k1p5/8/3K4 b - - 0 1",
                           {Move("a3b3", true)}, {Move("c3c2", true)});
  // WDL doesn't know that with such a high 50 ply count this position has
  // become a blessed loss (draw) for black.
  TestValidRootExpectation(&tablebase, "8/8/8/8/8/8/2Rk4/1K6 b - - 69 71",
                           {Move("d2d3", true)}, {}, {Move("d2e3", true)});
}

TEST(Syzygy, Simple4PieceProbes) {
  SyzygyTablebase tablebase;
  tablebase.init(kPaths);
  if (tablebase.max_cardinality() < 4) {
    // These probes require 4 piece tablebase.
    return;
  }

  // Longest 4 piece position.
  TestValidExpectation(&tablebase, "8/8/8/6B1/8/8/4k3/1K5N b - - 0 1", WDL_LOSS,
                       -65);

  // Some random checkmate position.
  TestValidExpectation(&tablebase, "8/8/8/8/8/2p5/3q2k1/4K3 w - - 0 1",
                       WDL_LOSS, -1);

  // Enpassant capture victory vs loss without rights.
  TestValidExpectation(&tablebase, "7k/8/8/8/Pp2K3/8/8/8 b - a3 0 1", WDL_WIN,
                       1);
  TestValidExpectation(&tablebase, "7k/8/8/8/Pp2K3/8/8/8 b - - 0 1", WDL_LOSS,
                       -1);
}

TEST(Syzygy, Simple5PieceProbes) {
  SyzygyTablebase tablebase;
  tablebase.init(kPaths);
  if (tablebase.max_cardinality() < 5) {
    // These probes require 5 piece tablebase.
    return;
  }

  // Longest 5 piece position.
  TestValidExpectation(&tablebase, "8/8/8/8/1p2P3/4P3/1k6/3K4 w - - 0 1",
                       WDL_CURSED_WIN, 101);

  // A blessed loss position.
  TestValidExpectation(&tablebase, "8/6B1/8/8/B7/8/K1pk4/8 b - - 0 1",
                       WDL_BLESSED_LOSS, -101);

  // A mate to be played on the board that is a capture.
  TestValidExpectation(&tablebase, "k7/p7/8/8/3Q4/8/5B2/7K w - - 0 1", WDL_WIN,
                       1);

  // Philidor draw position.
  TestValidExpectation(&tablebase, "8/8/8/8/4pk2/R7/7r/4K3 b - - 0 1", WDL_DRAW,
                       0);
  // Double mirrored and color swapped.
  TestValidExpectation(&tablebase, "3k4/R7/7r/2KP4/8/8/8/8 w - - 0 1", WDL_DRAW,
                       0);

  // En passant is a loss, without is draw by stalemate.
  TestValidExpectation(&tablebase, "8/8/8/8/6Pp/7K/5Q2/7k b - g3 0 1", WDL_LOSS,
                       -1);
  TestValidExpectation(&tablebase, "8/8/8/8/6Pp/7K/5Q2/7k b - - 0 1", WDL_DRAW,
                       0);

  // Some suggestions.
  TestValidExpectation(&tablebase, "kqqQK3/8/8/8/8/8/8/8 b - - 0 1", WDL_WIN,
                       1);
  TestValidExpectation(&tablebase, "kqqQK3/8/8/8/8/8/8/8 w - - 0 1", WDL_LOSS,
                       -2);
  TestValidExpectation(&tablebase, "KNNNk3/8/8/8/8/8/8/8 w - - 0 1", WDL_WIN,
                       30);
  TestValidExpectation(&tablebase, "8/1k6/1p1r4/5K2/8/8/8/2R5 w - - 0 1",
                       WDL_DRAW, 0);
  TestValidExpectation(&tablebase, "8/7p/5k2/8/5PK1/7P/8/8 b - - 0 1", WDL_DRAW,
                       0);
  TestValidExpectation(&tablebase, "1k1n4/8/p7/5KP1/8/8/8/8 b - - 0 1", WDL_WIN,
                       5);
  TestValidExpectation(&tablebase, "8/k7/8/2R5/8/4q3/8/4B2K w - - 0 1",
                       WDL_DRAW, 0);
}

TEST(Syzygy, Root5PieceProbes) {
  SyzygyTablebase tablebase;
  tablebase.init(kPaths);
  if (tablebase.max_cardinality() < 5) {
    // These probes require 5 piece tablebase.
    return;
  }
  TestValidRootExpectation(&tablebase, "8/8/8/Q7/8/1k1K4/1r6/8 w - - 79 44",
                           {Move("a5a1", false)}, {}, {Move("a5d5", false)});
  TestValidRootExpectation(&tablebase, "8/8/8/3Q4/k7/3K4/1r6/8 w - - 81 45",
                           {Move("d5a8", false)}, {}, {Move("d3c3", false)});

  // Variant of first test but with plenty of moves left.
  TestValidRootExpectation(&tablebase, "8/8/8/Q7/8/1k1K4/1r6/8 w - - 60 44",
                           {Move("a5a1", false), Move("a5d5", false)}, {}, {});
  // Same, but this time there is a repetition in history, so dtz will enforce
  // choice of equal lowest dtz.
  TestValidRootExpectation(&tablebase, "8/8/8/Q7/8/1k1K4/1r6/8 w - - 60 44",
                           {Move("a5a1", false)}, {}, {Move("a5d5", false)},
                           true);
}

}  // namespace lczero

int main(int argc, char** argv) {
  ::testing::InitGoogleTest(&argc, argv);
  lczero::InitializeMagicBitboards();
  return RUN_ALL_TESTS();
}

```

`src/trainingdata/reader.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "trainingdata/reader.h"

namespace lczero {

InputPlanes PlanesFromTrainingData(const V6TrainingData& data) {
  InputPlanes result;
  for (int i = 0; i < 104; i++) {
    result.emplace_back();
    result.back().mask = ReverseBitsInBytes(data.planes[i]);
  }
  switch (data.input_format) {
    case pblczero::NetworkFormat::InputFormat::INPUT_CLASSICAL_112_PLANE: {
      result.emplace_back();
      result.back().mask = data.castling_us_ooo != 0 ? ~0LL : 0LL;
      result.emplace_back();
      result.back().mask = data.castling_us_oo != 0 ? ~0LL : 0LL;
      result.emplace_back();
      result.back().mask = data.castling_them_ooo != 0 ? ~0LL : 0LL;
      result.emplace_back();
      result.back().mask = data.castling_them_oo != 0 ? ~0LL : 0LL;
      break;
    }
    case pblczero::NetworkFormat::INPUT_112_WITH_CASTLING_PLANE:
    case pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION:
    case pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION_HECTOPLIES:
    case pblczero::NetworkFormat::
        INPUT_112_WITH_CANONICALIZATION_HECTOPLIES_ARMAGEDDON:
    case pblczero::NetworkFormat::INPUT_112_WITH_CANONICALIZATION_V2:
    case pblczero::NetworkFormat::
        INPUT_112_WITH_CANONICALIZATION_V2_ARMAGEDDON: {
      result.emplace_back();
      result.back().mask =
          data.castling_us_ooo |
          (static_cast<uint64_t>(data.castling_them_ooo) << 56);
      result.emplace_back();
      result.back().mask = data.castling_us_oo |
                           (static_cast<uint64_t>(data.castling_them_oo) << 56);
      // 2 empty planes in this format.
      result.emplace_back();
      result.emplace_back();
      break;
    }

    default:
      throw Exception("Unsupported input plane encoding " +
                      std::to_string(data.input_format));
  }
  result.emplace_back();
  auto typed_format =
      static_cast<pblczero::NetworkFormat::InputFormat>(data.input_format);
  if (IsCanonicalFormat(typed_format)) {
    result.back().mask = static_cast<uint64_t>(data.side_to_move_or_enpassant)
                         << 56;
  } else {
    result.back().mask = data.side_to_move_or_enpassant != 0 ? ~0LL : 0LL;
  }
  result.emplace_back();
  if (IsHectopliesFormat(typed_format)) {
    result.back().Fill(data.rule50_count / 100.0f);
  } else {
    result.back().Fill(data.rule50_count);
  }
  result.emplace_back();
  // Empty plane, except for canonical armageddon.
  if (IsCanonicalArmageddonFormat(typed_format) &&
      data.invariance_info >= 128) {
    result.back().SetAll();
  }
  result.emplace_back();
  // All ones plane.
  result.back().SetAll();
  if (IsCanonicalFormat(typed_format) && data.invariance_info != 0) {
    // Undo transformation here as it makes the calling code simpler.
    int transform = data.invariance_info;
    for (size_t i = 0; i <= result.size(); i++) {
      auto v = result[i].mask;
      if (v == 0 || v == ~0ULL) continue;
      if ((transform & TransposeTransform) != 0) {
        v = TransposeBitsInBytes(v);
      }
      if ((transform & MirrorTransform) != 0) {
        v = ReverseBytesInBytes(v);
      }
      if ((transform & FlipTransform) != 0) {
        v = ReverseBitsInBytes(v);
      }
      result[i].mask = v;
    }
  }
  return result;
}

TrainingDataReader::TrainingDataReader(std::string filename)
    : filename_(filename) {
  fin_ = gzopen(filename_.c_str(), "rb");
  if (!fin_) {
    throw Exception("Cannot open gzip file " + filename_);
  }
}

TrainingDataReader::~TrainingDataReader() { gzclose(fin_); }

bool TrainingDataReader::ReadChunk(V6TrainingData* data) {
  if (format_v6) {
    int read_size = gzread(fin_, reinterpret_cast<void*>(data), sizeof(*data));
    if (read_size < 0) throw Exception("Corrupt read.");
    return read_size == sizeof(*data);
  } else {
    int v6_extra = 48;
    int v5_extra = 16;
    int v4_extra = 16;
    int v3_size = sizeof(*data) - v4_extra - v5_extra - v6_extra;
    int read_size = gzread(fin_, reinterpret_cast<void*>(data), v3_size);
    if (read_size < 0) throw Exception("Corrupt read.");
    if (read_size != v3_size) return false;
    auto orig_version = data->version;
    switch (data->version) {
      case 3: {
        data->version = 4;
        // First convert 3 to 4 to reduce code duplication.
        char* v4_extra_start = reinterpret_cast<char*>(data) + v3_size;
        // Write 0 bytes for 16 extra bytes - corresponding to 4 floats of 0.0f.
        for (int i = 0; i < v4_extra; i++) {
          v4_extra_start[i] = 0;
        }
        [[fallthrough]];
      }
      case 4: {
        // If actually 4, we need to read the additional data first.
        if (orig_version == 4) {
          read_size = gzread(
              fin_,
              reinterpret_cast<void*>(reinterpret_cast<char*>(data) + v3_size),
              v4_extra);
          if (read_size < 0) throw Exception("Corrupt read.");
          if (read_size != v4_extra) return false;
        }
        data->version = 5;
        char* data_ptr = reinterpret_cast<char*>(data);
        // Shift data after version back 4 bytes.
        memmove(data_ptr + 2 * sizeof(uint32_t), data_ptr + sizeof(uint32_t),
                v3_size + v4_extra - sizeof(uint32_t));
        data->input_format = pblczero::NetworkFormat::INPUT_CLASSICAL_112_PLANE;
        data->root_m = 0.0f;
        data->best_m = 0.0f;
        data->plies_left = 0.0f;
        [[fallthrough]];
      }
      case 5: {
        // If actually 5, we need to read the additional data first.
        if (orig_version == 5) {
          read_size = gzread(
              fin_,
              reinterpret_cast<void*>(reinterpret_cast<char*>(data) + v3_size),
              v4_extra + v5_extra);
          if (read_size < 0) throw Exception("Corrupt read.");
          if (read_size != v4_extra + v5_extra) return false;
        }
        data->version = 6;
        // Type of dummy was changed from signed to unsigned - which means -1 on
        // disk is read in as 255.
        if (data->dummy > 1 && data->dummy < 255) {
          throw Exception("Invalid result read in v5 data before upgrade.");
        }
        data->result_q =
            data->dummy == 255 ? -1.0f : (data->dummy == 0 ? 0.0f : 1.0f);
        data->result_d = data->dummy == 0 ? 1.0f : 0.0f;
        data->dummy = 0;
        data->played_q = 0.0f;
        data->played_d = 0.0f;
        data->played_m = 0.0f;
        // Mark orig as NaN since scripts further downstream already have to
        // handle that case.
        data->orig_q = std::numeric_limits<float>::quiet_NaN();
        data->orig_d = std::numeric_limits<float>::quiet_NaN();
        data->orig_m = std::numeric_limits<float>::quiet_NaN();
        data->visits = 0;
        data->played_idx = 0;
        data->best_idx = 0;
        data->policy_kld = 0.0f;
        data->reserved = 0;
        return true;
      }
      case 6: {
        format_v6 = true;
        read_size = gzread(
            fin_,
            reinterpret_cast<void*>(reinterpret_cast<char*>(data) + v3_size),
            v4_extra + v5_extra + v6_extra);
        if (read_size < 0) throw Exception("Corrupt read.");
        return read_size == v4_extra + v5_extra + v6_extra;
      }
      default:
        throw Exception("Unknown format.");
    }
  }
}

}  // namespace lczero

```

`src/trainingdata/reader.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include "trainingdata/trainingdata.h"

namespace lczero {

// Constructs InputPlanes from training data.
//
// NOTE: If the training data is a cannonical type, the canonicalization
// transforms are reverted before returning, since it is assumed that the data
// will be used with DecodeMoveFromInput or PopulateBoard which assume the
// InputPlanes are not transformed.
InputPlanes PlanesFromTrainingData(const V6TrainingData& data);

class TrainingDataReader {
 public:
  // Opens the given file to read chunk data from.
  TrainingDataReader(std::string filename);

  ~TrainingDataReader();

  // Reads a chunk. Returns true if a chunk was read.
  bool ReadChunk(V6TrainingData* data);

  // Gets full filename of the file being read.
  std::string GetFileName() const { return filename_; }

 private:
  std::string filename_;
  gzFile fin_;
  bool format_v6 = false;
};

}  // namespace lczero

```

`src/trainingdata/trainingdata.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "trainingdata/trainingdata.h"

namespace lczero {

namespace {
std::tuple<float, float> DriftCorrect(float q, float d) {
  // Training data doesn't have a high number of nodes, so there shouldn't be
  // too much drift. Highest known value not caused by backend bug was 1.5e-7.
  const float allowed_eps = 0.000001f;
  if (q > 1.0f) {
    if (q > 1.0f + allowed_eps) {
      CERR << "Unexpectedly large drift in q " << q;
    }
    q = 1.0f;
  }
  if (q < -1.0f) {
    if (q < -1.0f - allowed_eps) {
      CERR << "Unexpectedly large drift in q " << q;
    }
    q = -1.0f;
  }
  if (d > 1.0f) {
    if (d > 1.0f + allowed_eps) {
      CERR << "Unexpectedly large drift in d " << d;
    }
    d = 1.0f;
  }
  if (d < 0.0f) {
    if (d < 0.0f - allowed_eps) {
      CERR << "Unexpectedly large drift in d " << d;
    }
    d = 0.0f;
  }
  float w = (1.0f - d + q) / 2.0f;
  float l = w - q;
  // Assume q drift is rarer than d drift and apply all correction to d.
  if (w < 0.0f || l < 0.0f) {
    float drift = 2.0f * std::min(w, l);
    if (drift < -allowed_eps) {
      CERR << "Unexpectedly large drift correction for d based on q. " << drift;
    }
    d += drift;
    // Since q is in range -1 to 1 - this correction should never push d outside
    // of range, but precision could be lost in calculations so just in case.
    if (d < 0.0f) {
      d = 0.0f;
    }
  }
  return {q, d};
}
}  // namespace

void V6TrainingDataArray::Write(TrainingDataWriter* writer, GameResult result,
                                bool adjudicated) const {
  if (training_data_.empty()) return;
  // Base estimate off of best_m.  If needed external processing can use a
  // different approach.
  float m_estimate = training_data_.back().best_m + training_data_.size() - 1;
  for (auto chunk : training_data_) {
    bool black_to_move = chunk.side_to_move_or_enpassant;
    if (IsCanonicalFormat(static_cast<pblczero::NetworkFormat::InputFormat>(
            chunk.input_format))) {
      black_to_move = (chunk.invariance_info & (1u << 7)) != 0;
    }
    if (result == GameResult::WHITE_WON) {
      chunk.result_q = black_to_move ? -1 : 1;
      chunk.result_d = 0;
    } else if (result == GameResult::BLACK_WON) {
      chunk.result_q = black_to_move ? 1 : -1;
      chunk.result_d = 0;
    } else {
      chunk.result_q = 0;
      chunk.result_d = 1;
    }
    if (adjudicated) {
      chunk.invariance_info |= 1u << 5;  // Game adjudicated.
    }
    if (adjudicated && result == GameResult::UNDECIDED) {
      chunk.invariance_info |= 1u << 4;  // Max game length exceeded.
    }
    chunk.plies_left = m_estimate;
    m_estimate -= 1.0f;
    writer->WriteChunk(chunk);
  }
}

void V6TrainingDataArray::Add(const Node* node, const PositionHistory& history,
                              Eval best_eval, Eval played_eval,
                              bool best_is_proven, Move best_move,
                              Move played_move, const NNCacheLock& nneval) {
  V6TrainingData result;
  const auto& position = history.Last();

  // Set version.
  result.version = 6;
  result.input_format = input_format_;

  // Populate planes.
  int transform;
  InputPlanes planes = EncodePositionForNN(
      input_format_, history, 8, fill_empty_history_[position.IsBlackToMove()],
      &transform);
  int plane_idx = 0;
  for (auto& plane : result.planes) {
    plane = ReverseBitsInBytes(planes[plane_idx++].mask);
  }

  // Populate probabilities.
  auto total_n = node->GetChildrenVisits();
  // Prevent garbage/invalid training data from being uploaded to server.
  // It's possible to have N=0 when there is only one legal move in position
  // (due to smart pruning).
  if (total_n == 0 && node->GetNumEdges() != 1) {
    throw Exception("Search generated invalid data!");
  }
  // Set illegal moves to have -1 probability.
  std::fill(std::begin(result.probabilities), std::end(result.probabilities),
            -1);
  // Set moves probabilities according to their relative amount of visits.
  // Compute Kullback-Leibler divergence in nats (between policy and visits).
  float kld_sum = 0;
  float max_p = -std::numeric_limits<float>::infinity();
  std::vector<float> intermediate;
  if (nneval) {
    int last_idx = 0;
    for (const auto& child : node->Edges()) {
      auto nn_idx = child.edge()->GetMove().as_nn_index(transform);
      float p = 0;
      for (int i = 0; i < nneval->p.size(); i++) {
        // Optimization: usually moves are stored in the same order as queried.
        const auto& move = nneval->p[last_idx++];
        if (last_idx == nneval->p.size()) last_idx = 0;
        if (move.first == nn_idx) {
          p = move.second;
          break;
        }
      }
      intermediate.emplace_back(p);
      max_p = std::max(max_p, p);
    }
  }
  float total = 0.0;
  auto it = intermediate.begin();
  for (const auto& child : node->Edges()) {
    auto nn_idx = child.edge()->GetMove().as_nn_index(transform);
    float fracv = total_n > 0 ? child.GetN() / static_cast<float>(total_n) : 1;
    if (nneval) {
      float P = std::exp(*it - max_p);
      if (fracv > 0) {
        kld_sum += fracv * std::log(fracv / P);
      }
      total += P;
      it++;
    }
    result.probabilities[nn_idx] = fracv;
  }
  if (nneval) {
    // Add small epsilon for backward compatibility with earlier value of 0.
    auto epsilon = std::numeric_limits<float>::min();
    kld_sum = std::max(kld_sum + std::log(total), 0.0f) + epsilon;
  }
  result.policy_kld = kld_sum;

  const auto& castlings = position.GetBoard().castlings();
  // Populate castlings.
  // For non-frc trained nets, just send 1 like we used to.
  uint8_t queen_side = 1;
  uint8_t king_side = 1;
  // If frc trained, send the bit mask representing rook position.
  if (Is960CastlingFormat(input_format_)) {
    queen_side <<= castlings.queenside_rook();
    king_side <<= castlings.kingside_rook();
  }

  result.castling_us_ooo = castlings.we_can_000() ? queen_side : 0;
  result.castling_us_oo = castlings.we_can_00() ? king_side : 0;
  result.castling_them_ooo = castlings.they_can_000() ? queen_side : 0;
  result.castling_them_oo = castlings.they_can_00() ? king_side : 0;

  // Other params.
  if (IsCanonicalFormat(input_format_)) {
    result.side_to_move_or_enpassant =
        position.GetBoard().en_passant().as_int() >> 56;
    if ((transform & FlipTransform) != 0) {
      result.side_to_move_or_enpassant =
          ReverseBitsInBytes(result.side_to_move_or_enpassant);
    }
    // Send transform in deprecated move count so rescorer can reverse it to
    // calculate the actual move list from the input data.
    result.invariance_info =
        transform | (position.IsBlackToMove() ? (1u << 7) : 0u);
  } else {
    result.side_to_move_or_enpassant = position.IsBlackToMove() ? 1 : 0;
    result.invariance_info = 0;
  }
  if (best_is_proven) {
    result.invariance_info |= 1u << 3;  // Best node is proven best;
  }
  result.dummy = 0;
  result.rule50_count = position.GetRule50Ply();

  // Game result is undecided.
  result.result_q = 0;
  result.result_d = 1;

  Eval orig_eval;
  if (nneval) {
    orig_eval.wl = nneval->q;
    orig_eval.d = nneval->d;
    orig_eval.ml = nneval->m;
  } else {
    orig_eval.wl = std::numeric_limits<float>::quiet_NaN();
    orig_eval.d = std::numeric_limits<float>::quiet_NaN();
    orig_eval.ml = std::numeric_limits<float>::quiet_NaN();
  }

  // Aggregate evaluation WL.
  result.root_q = -node->GetWL();
  result.best_q = best_eval.wl;
  result.played_q = played_eval.wl;
  result.orig_q = orig_eval.wl;

  // Draw probability of WDL head.
  result.root_d = node->GetD();
  result.best_d = best_eval.d;
  result.played_d = played_eval.d;
  result.orig_d = orig_eval.d;

  std::tie(result.best_q, result.best_d) =
      DriftCorrect(result.best_q, result.best_d);
  std::tie(result.root_q, result.root_d) =
      DriftCorrect(result.root_q, result.root_d);
  std::tie(result.played_q, result.played_d) =
      DriftCorrect(result.played_q, result.played_d);

  result.root_m = node->GetM();
  result.best_m = best_eval.ml;
  result.played_m = played_eval.ml;
  result.orig_m = orig_eval.ml;

  result.visits = node->GetN();
  if (position.IsBlackToMove()) {
    best_move.Mirror();
    played_move.Mirror();
  }
  result.best_idx = best_move.as_nn_index(transform);
  result.played_idx = played_move.as_nn_index(transform);
  result.reserved = 0;

  // Unknown here - will be filled in once the full data has been collected.
  result.plies_left = 0;
  training_data_.push_back(result);
}

}  // namespace lczero

```

`src/trainingdata/trainingdata.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include "mcts/node.h"
#include "trainingdata/writer.h"

namespace lczero {

#pragma pack(push, 1)

struct V6TrainingData {
  uint32_t version;
  uint32_t input_format;
  float probabilities[1858];
  uint64_t planes[104];
  uint8_t castling_us_ooo;
  uint8_t castling_us_oo;
  uint8_t castling_them_ooo;
  uint8_t castling_them_oo;
  // For input type 3 contains enpassant column as a mask.
  uint8_t side_to_move_or_enpassant;
  uint8_t rule50_count;
  // Bitfield with the following allocation:
  //  bit 7: side to move (input type 3)
  //  bit 6: position marked for deletion by the rescorer (never set by lc0)
  //  bit 5: game adjudicated (v6)
  //  bit 4: max game length exceeded (v6)
  //  bit 3: best_q is for proven best move (v6)
  //  bit 2: transpose transform (input type 3)
  //  bit 1: mirror transform (input type 3)
  //  bit 0: flip transform (input type 3)
  // In versions prior to v5 this spot contained an unused move count field.
  uint8_t invariance_info;
  // In versions prior to v6 this spot contained thr result as an int8_t.
  uint8_t dummy;
  float root_q;
  float best_q;
  float root_d;
  float best_d;
  float root_m;      // In plies.
  float best_m;      // In plies.
  float plies_left;  // This is the training target for MLH.
  float result_q;
  float result_d;
  float played_q;
  float played_d;
  float played_m;
  // The folowing may be NaN if not found in cache.
  float orig_q;      // For value repair.
  float orig_d;
  float orig_m;
  uint32_t visits;
  // Indices in the probabilities array.
  uint16_t played_idx;
  uint16_t best_idx;
  // Kullback-Leibler divergence between visits and policy (denominator)
  float policy_kld;
  uint32_t reserved;
} PACKED_STRUCT;
static_assert(sizeof(V6TrainingData) == 8356, "Wrong struct size");

#pragma pack(pop)

class V6TrainingDataArray {
 public:
  V6TrainingDataArray(FillEmptyHistory white_fill_empty_history,
                      FillEmptyHistory black_fill_empty_history,
                      pblczero::NetworkFormat::InputFormat input_format)
      : fill_empty_history_{white_fill_empty_history, black_fill_empty_history},
        input_format_(input_format) {}

  // Add a chunk.
  void Add(const Node* node, const PositionHistory& history, Eval best_eval,
           Eval played_eval, bool best_is_proven, Move best_move,
           Move played_move, const NNCacheLock& nneval);

  // Writes training data to a file.
  void Write(TrainingDataWriter* writer, GameResult result,
             bool adjudicated) const;

 private:
  std::vector<V6TrainingData> training_data_;
  FillEmptyHistory fill_empty_history_[2];
  pblczero::NetworkFormat::InputFormat input_format_;
};

}  // namespace lczero

```

`src/trainingdata/writer.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "trainingdata/writer.h"

#include "trainingdata/trainingdata.h"
#include "utils/exception.h"
#include "utils/filesystem.h"
#include "utils/random.h"

namespace lczero {

namespace {
std::string GetLc0CacheDirectory() {
  std::string user_cache_path = GetUserCacheDirectory();
  if (!user_cache_path.empty()) {
    user_cache_path += "lc0/";
    CreateDirectory(user_cache_path);
  }
  return user_cache_path;
}

}  // namespace

TrainingDataWriter::TrainingDataWriter(int game_id) {
  static std::string directory =
      GetLc0CacheDirectory() + "data-" + Random::Get().GetString(12);
  // It's fine if it already exists.
  CreateDirectory(directory.c_str());

  std::ostringstream oss;
  oss << directory << '/' << "game_" << std::setfill('0') << std::setw(6)
      << game_id << ".gz";

  filename_ = oss.str();
  fout_ = gzopen(filename_.c_str(), "wb");
  if (!fout_) throw Exception("Cannot create gzip file " + filename_);
}

void TrainingDataWriter::WriteChunk(const V6TrainingData& data) {
  auto bytes_written =
      gzwrite(fout_, reinterpret_cast<const char*>(&data), sizeof(data));
  if (bytes_written != sizeof(data)) {
    throw Exception("Unable to write into " + filename_);
  }
}

void TrainingDataWriter::Finalize() {
  gzclose(fout_);
  fout_ = nullptr;
}

}  // namespace lczero

```

`src/trainingdata/writer.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <fstream>
#include <zlib.h>

namespace lczero {

struct V6TrainingData;

class TrainingDataWriter {
 public:
  // Creates a new file to write in data directory. It will has @game_id
  // somewhere in the filename.
  TrainingDataWriter(int game_id);

  ~TrainingDataWriter() {
    if (fout_) Finalize();
  }

  // Writes a chunk.
  void WriteChunk(const V6TrainingData& data);

  // Flushes file and closes it.
  void Finalize();

  // Gets full filename of the file written.
  std::string GetFileName() const { return filename_; }

 private:
  std::string filename_;
  gzFile fout_;
};

}  // namespace lczero

```

`src/utils/bititer.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once
#include <cstdint>
#include <iterator>
#ifdef _MSC_VER
#include <intrin.h>
#endif

namespace lczero {

inline unsigned long GetLowestBit(std::uint64_t value) {
#if defined(_MSC_VER) && defined(_WIN64)
  unsigned long result;
  _BitScanForward64(&result, value);
  return result;
#elif defined(_MSC_VER)
  unsigned long result;
  if (value & 0xFFFFFFFF) {
    _BitScanForward(&result, value);
  } else {
    _BitScanForward(&result, value >> 32);
    result += 32;
  }
  return result;
#else
  return __builtin_ctzll(value);
#endif
}

enum BoardTransform {
  NoTransform = 0,
  // Horizontal mirror, ReverseBitsInBytes
  FlipTransform = 1,
  // Vertical mirror, ReverseBytesInBytes
  MirrorTransform = 2,
  // Diagonal transpose A1 to H8, TransposeBitsInBytes.
  TransposeTransform = 4,
};

inline uint64_t ReverseBitsInBytes(uint64_t v) {
  v = ((v >> 1) & 0x5555555555555555ull) | ((v & 0x5555555555555555ull) << 1);
  v = ((v >> 2) & 0x3333333333333333ull) | ((v & 0x3333333333333333ull) << 2);
  v = ((v >> 4) & 0x0F0F0F0F0F0F0F0Full) | ((v & 0x0F0F0F0F0F0F0F0Full) << 4);
  return v;
}

inline uint64_t ReverseBytesInBytes(uint64_t v) {
  v = (v & 0x00000000FFFFFFFF) << 32 | (v & 0xFFFFFFFF00000000) >> 32;
  v = (v & 0x0000FFFF0000FFFF) << 16 | (v & 0xFFFF0000FFFF0000) >> 16;
  v = (v & 0x00FF00FF00FF00FF) << 8 | (v & 0xFF00FF00FF00FF00) >> 8;
  return v;
}

// Transpose across the diagonal connecting bit 7 to bit 56.
inline uint64_t TransposeBitsInBytes(uint64_t v) {
  v = (v & 0xAA00AA00AA00AA00ULL) >> 9 | (v & 0x0055005500550055ULL) << 9 |
      (v & 0x55AA55AA55AA55AAULL);
  v = (v & 0xCCCC0000CCCC0000ULL) >> 18 | (v & 0x0000333300003333ULL) << 18 |
      (v & 0x3333CCCC3333CCCCULL);
  v = (v & 0xF0F0F0F000000000ULL) >> 36 | (v & 0x000000000F0F0F0FULL) << 36 |
      (v & 0x0F0F0F0FF0F0F0F0ULL);
  return v;
}

// Iterates over all set bits of the value, lower to upper. The value of
// dereferenced iterator is bit number (lower to upper, 0 bazed)
template <typename T>
class BitIterator {
 public:
  using iterator_category = std::input_iterator_tag;
  using difference_type = T;
  using value_type = T;
  using pointer = T*;
  using reference = T&;

  BitIterator(std::uint64_t value) : value_(value){};
  bool operator!=(const BitIterator& other) { return value_ != other.value_; }

  void operator++() { value_ &= (value_ - 1); }
  T operator*() const { return GetLowestBit(value_); }

 private:
  std::uint64_t value_;
};

class IterateBits {
 public:
  IterateBits(std::uint64_t value) : value_(value) {}
  BitIterator<int> begin() { return value_; }
  BitIterator<int> end() { return 0; }

 private:
  std::uint64_t value_;
};

}  // namespace lczero

```

`src/utils/cache-old.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <functional>
#include <list>
#include <memory>
#include <mutex>
#include <unordered_map>
#include "utils/hashcat.h"

namespace lczero {

// Generic LRU cache. Thread-safe.
template <class K, class V>
class LruCache {
 public:
  LruCache(int capacity = 128) : capacity_(capacity) {}

  // Inserts the element under key @key with value @val.
  // If the element is pinned, old value is still kept (until fully unpinned),
  // but new lookups will return updated value.
  // If @pinned, pins inserted element, Unpin has to be called to unpin.
  // In any case, puts element to front of the queue (makes it last to evict).
  V* Insert(K key, std::unique_ptr<V> val, bool pinned = false) {
    std::lock_guard<std::mutex> lock(mutex_);
    V* new_val = val.get();
    auto iter = lookup_.find(key);
    if (iter != lookup_.end()) {
      auto list_iter = iter->second;
      MakePending(list_iter);
      list_iter->pins = pinned ? 1 : 0;
      list_iter->value = std::move(val);
      BringToFront(list_iter);
    } else {
      MaybeCleanup(capacity_ - 1);
      lru_.emplace_front(key, std::move(val), pinned ? 1 : 0);
      lookup_[key] = lru_.begin();
    }
    return new_val;
  }

  // Looks up and pins the element by key. Returns nullptr if not found.
  // If found, brings the element to the head of the queue (makes it last to
  // evict).
  V* Lookup(K key) {
    std::lock_guard<std::mutex> lock(mutex_);
    auto iter = lookup_.find(key);
    if (iter == lookup_.end()) return nullptr;
    ++iter->second->pins;
    return iter->second->value.get();
  }

  // Checks whether a key exists. Doesn't lock. Of course the next moment the
  // key may be evicted.
  bool ContainsKey(K key) {
    std::lock_guard<std::mutex> lock(mutex_);
    auto iter = lookup_.find(key);
    return iter != lookup_.end();
  }

  // Unpins the element given key and value.
  void Unpin(K key, V* value) {
    std::lock_guard<std::mutex> lock(mutex_);
    auto pending_iter = pending_pins_.find({key, value});
    if (pending_iter != pending_pins_.end()) {
      if (--pending_iter->second.pins == 0) pending_pins_.erase(pending_iter);
      return;
    }
    auto iter = lookup_.find(key);
    --iter->second->pins;
  }

  // Sets the capacity of the cache. If new capacity is less than current size
  // of the cache, oldest entries are evicted.
  void SetCapacity(int capacity) {
    std::lock_guard<std::mutex> lock(mutex_);
    capacity_ = capacity;
    MaybeCleanup(capacity);
  }

  int GetSize() const { return lru_.size(); }
  int GetCapacity() const { return capacity_; }

 private:
  // Mutex should be locked when calling this function.
  void MaybeCleanup(int size) {
    if (size >= lookup_.size()) return;
    int to_delete = lookup_.size() - size;
    auto iter = std::prev(lru_.end());
    for (int i = 0; i < to_delete; ++i) {
      lookup_.erase(iter->key);
      MakePending(iter);
      iter = lru_.erase(iter);
      --iter;
    }
  }

  struct Item {
    Item(K key, std::unique_ptr<V> value, int pins)
        : key(key), value(std::move(value)), pins(pins) {}
    K key;
    std::unique_ptr<V> value;
    int pins;
  };

  void MakePending(typename std::list<Item>::iterator iter) {
    if (iter->pins > 0) {
      K key = iter->key;
      V* val = iter->value.get();
      int pins = iter->pins;
      pending_pins_.emplace(std::make_pair(key, val),
                            Item{key, std::move(iter->value), pins});
    }
  }

  void BringToFront(typename std::list<Item>::iterator iter) {
    if (iter != lru_.begin())
      lru_.splice(lru_.begin(), lru_, iter, std::next(iter));
  }
  // Fresh in front, stale on back.
  int capacity_;
  std::list<Item> lru_;
  using ListIter = typename std::list<Item>::iterator;
  std::unordered_map<K, ListIter> lookup_;

  struct PairHash {
    std::size_t operator()(const std::pair<K, V*>& p) const {
      return HashCat({std::hash<V*>{}(p.second), std::hash<K>{}(p.first)});
    }
  };
  std::unordered_map<std::pair<K, V*>, Item, PairHash> pending_pins_;
  std::mutex mutex_;
};

template <class K, class V>
class LruCacheLock {
 public:
  // Looks up the value in @cache by @key and pins it if found.
  LruCacheLock(LruCache<K, V>* cache, K key)
      : cache_(cache), key_(key), value_(cache->Lookup(key_)) {}

  // Unpins the cache entry (if holds).
  ~LruCacheLock() {
    if (value_) cache_->Unpin(key_, value_);
  }

  // Returns whether lock holds any value.
  operator bool() const { return value_; }

  // Gets the value.
  V* operator->() const { return value_; }
  V* operator*() const { return value_; }

  LruCacheLock() {}
  LruCacheLock(LruCacheLock&& other)
      : cache_(other.cache_), key_(other.key_), value_(other.value_) {
    other.value_ = nullptr;
  }
  void operator=(LruCacheLock&& other) {
    cache_ = other.cache_;
    key_ = other.key_;
    value_ = other.value_;
    other.value_ = nullptr;
  }

 private:
  LruCache<K, V>* cache_ = nullptr;
  K key_;
  V* value_ = nullptr;
};

}  // namespace lczero

```

`src/utils/cache.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <cassert>
#include <cstring>
#include <memory>
#include <string>

#include "utils/mutex.h"

namespace lczero {

// A hash-keyed cache. Thread-safe. Takes ownership of all values, which are
// deleted upon eviction; thus, using values stored requires pinning them, which
// in turn requires Unpin()ing them after use. The use of HashKeyedCacheLock is
// recommend to automate this element-memory management.
// Unlike LRUCache, doesn't even consider trying to support LRU order.
// Does not support delete.
// Does not support replace! Inserts to existing elements are silently ignored.
// FIFO eviction.
// Assumes that eviction while pinned is rare enough to not need to optimize
// unpin for that case.
template <class V>
class HashKeyedCache {
  static const double constexpr kLoadFactor = 1.9;

 public:
  HashKeyedCache(int capacity = 128)
      : capacity_(capacity),
        hash_(static_cast<size_t>(capacity * kLoadFactor + 1)) {}

  ~HashKeyedCache() {
    EvictToCapacity(0);
    assert(size_ == 0);
    assert(allocated_ == 0);
  }

  // Inserts the element under key @key with value @val. Unless the key is
  // already in the cache.
  void Insert(uint64_t key, std::unique_ptr<V> val) {
    if (capacity_.load(std::memory_order_relaxed) == 0) return;

    SpinMutex::Lock lock(mutex_);

    size_t idx = key % hash_.size();
    while (true) {
      if (!hash_[idx].in_use) break;
      if (hash_[idx].key == key) {
        // Already exists.
        return;
      }
      ++idx;
      if (idx >= hash_.size()) idx -= hash_.size();
    }
    hash_[idx].key = key;
    hash_[idx].value = std::move(val);
    hash_[idx].pins = 0;
    hash_[idx].in_use = true;
    insertion_order_.push_back(key);
    ++size_;
    ++allocated_;

    EvictToCapacity(capacity_);
  }

  // Checks whether a key exists. Doesn't pin. Of course the next moment the
  // key may be evicted.
  bool ContainsKey(uint64_t key) {
    if (capacity_.load(std::memory_order_relaxed) == 0) return false;

    SpinMutex::Lock lock(mutex_);
    size_t idx = key % hash_.size();
    while (true) {
      if (!hash_[idx].in_use) break;
      if (hash_[idx].key == key) {
        return true;
      }
      ++idx;
      if (idx >= hash_.size()) idx -= hash_.size();
    }
    return false;
  }

  // Looks up and pins the element by key. Returns nullptr if not found.
  // If found, a call to Unpin must be made for each such element.
  // Use of HashedKeyCacheLock is recommended to automate this pin management.
  V* LookupAndPin(uint64_t key) {
    if (capacity_.load(std::memory_order_relaxed) == 0) return nullptr;

    SpinMutex::Lock lock(mutex_);

    size_t idx = key % hash_.size();
    while (true) {
      if (!hash_[idx].in_use) break;
      if (hash_[idx].key == key) {
        ++hash_[idx].pins;
        return hash_[idx].value.get();
      }
      ++idx;
      if (idx >= hash_.size()) idx -= hash_.size();
    }
    return nullptr;
  }

  // Unpins the element given key and value. Use of HashedKeyCacheLock is
  // recommended to automate this pin management.
  void Unpin(uint64_t key, V* value) {
    SpinMutex::Lock lock(mutex_);

    // Checking evicted list first.
    for (auto it = evicted_.begin(); it != evicted_.end(); ++it) {
      auto& entry = *it;
      if (key == entry.key && value == entry.value.get()) {
        if (--entry.pins == 0) {
          --allocated_;
          evicted_.erase(it);
          return;
        } else {
          return;
        }
      }
    }
    // Now the main list.
    size_t idx = key % hash_.size();
    while (true) {
      if (!hash_[idx].in_use) break;
      if (hash_[idx].key == key &&
          hash_[idx].value.get() == value) {
        --hash_[idx].pins;
        return;
      }
      ++idx;
      if (idx >= hash_.size()) idx -= hash_.size();
    }
    assert(false);
  }

  // Sets the capacity of the cache. If new capacity is less than current size
  // of the cache, oldest entries are evicted. In any case the hashtable is
  // rehashed.
  void SetCapacity(int capacity) {
    // This is the one operation that can be expected to take a long time, which
    // usually means a SpinMutex is not a great idea. However we should only
    // very rarely have any contention on the lock while this function is
    // running, since its called very rarely and almost always before things
    // start happening.
    SpinMutex::Lock lock(mutex_);

    if (capacity_.load(std::memory_order_relaxed) == capacity) return;
    EvictToCapacity(capacity);
    capacity_.store(capacity);

    std::vector<Entry> new_hash(
        static_cast<size_t>(capacity * kLoadFactor + 1));

    if (size_ != 0) {
      for (Entry& item : hash_) {
        if (!item.in_use) continue;
        size_t idx = item.key % new_hash.size();
        while (true) {
          if (!new_hash[idx].in_use) break;
          ++idx;
          if (idx >= new_hash.size()) idx -= new_hash.size();
        }
        new_hash[idx].key = item.key;
        new_hash[idx].value = std::move(item.value);
        new_hash[idx].pins = item.pins;
        new_hash[idx].in_use = true;
      }
    }
    hash_.swap(new_hash);
  }

  // Clears the cache;
  void Clear() {
    SpinMutex::Lock lock(mutex_);
    EvictToCapacity(0);
  }

  int GetSize() const {
    SpinMutex::Lock lock(mutex_);
    return size_;
  }
  int GetCapacity() const { return capacity_.load(std::memory_order_relaxed); }
  static constexpr size_t GetItemStructSize() { return sizeof(Entry); }

 private:
  struct Entry {
    Entry() {}
    Entry(uint64_t key, std::unique_ptr<V> value)
        : key(key), value(std::move(value)) {}
    uint64_t key;
    std::unique_ptr<V> value;
    int pins = 0;
    bool in_use = false;
  };

  void EvictItem() REQUIRES(mutex_) {
    --size_;
    uint64_t key = insertion_order_.front();
    insertion_order_.pop_front();
    size_t idx = key % hash_.size();
    while (true) {
      if (hash_[idx].in_use && hash_[idx].key == key) {
        break;
      }
      ++idx;
      if (idx >= hash_.size()) idx -= hash_.size();
    }
    if (hash_[idx].pins == 0) {
      --allocated_;
      hash_[idx].value.reset();
      hash_[idx].in_use = false;
    } else {
      evicted_.emplace_back(hash_[idx].key, std::move(hash_[idx].value));
      evicted_.back().pins = hash_[idx].pins;
      hash_[idx].pins = 0;
      hash_[idx].in_use = false;
    }
    size_t next = idx + 1;
    if (next >= hash_.size()) next -= hash_.size();
    while (true) {
      if (!hash_[next].in_use) {
        break;
      }
      size_t target = hash_[next].key % hash_.size();
      if (!InRange(target, idx + 1, next)) {
        std::swap(hash_[next], hash_[idx]);
        idx = next;
      }
      ++next;
      if (next >= hash_.size()) next -= hash_.size();
    }
  }

  bool InRange(size_t target, size_t start, size_t end) {
    if (start <= end) {
      return target >= start && target <= end;
    } else {
      return target >= start || target <= end;
    }
  }

  void EvictToCapacity(int capacity) REQUIRES(mutex_) {
    if (capacity < 0) capacity = 0;
    while (size_ > capacity) {
      EvictItem();
    }
  }

  std::atomic<int> capacity_;
  int size_ GUARDED_BY(mutex_) = 0;
  int allocated_ GUARDED_BY(mutex_) = 0;
  // Fresh in back, stale at front.
  std::deque<uint64_t> GUARDED_BY(mutex_) insertion_order_;
  std::vector<Entry> GUARDED_BY(mutex_) evicted_;
  std::vector<Entry> GUARDED_BY(mutex_) hash_;

  mutable SpinMutex mutex_;
};

// Convenience class for pinning cache items.
template <class V>
class HashKeyedCacheLock {
 public:
  // Looks up the value in @cache by @key and pins it if found.
  HashKeyedCacheLock(HashKeyedCache<V>* cache, uint64_t key)
      : cache_(cache), key_(key), value_(cache->LookupAndPin(key_)) {}

  // Unpins the cache entry (if holds).
  ~HashKeyedCacheLock() {
    if (value_) cache_->Unpin(key_, value_);
  }

  HashKeyedCacheLock(const HashKeyedCacheLock&) = delete;

  // Returns whether lock holds any value.
  operator bool() const { return value_; }

  // Gets the value.
  V* operator->() const { return value_; }
  V* operator*() const { return value_; }

  HashKeyedCacheLock() {}
  HashKeyedCacheLock(HashKeyedCacheLock&& other)
      : cache_(other.cache_), key_(other.key_), value_(other.value_) {
    other.value_ = nullptr;
  }
  void operator=(HashKeyedCacheLock&& other) {
    if (value_) cache_->Unpin(key_, value_);
    cache_ = other.cache_;
    key_ = other.key_;
    value_ = other.value_;
    other.value_ = nullptr;
  }

 private:
  HashKeyedCache<V>* cache_ = nullptr;
  uint64_t key_;
  V* value_ = nullptr;
};

}  // namespace lczero

```

`src/utils/commandline.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "utils/commandline.h"
#include "utils/logging.h"

namespace lczero {

std::string CommandLine::binary_;
std::vector<std::string> CommandLine::arguments_;
std::vector<std::pair<std::string, std::string>> CommandLine::modes_;

void CommandLine::Init(int argc, const char** argv) {
#ifdef _WIN32
  // Under windows argv[0] may not have the extension. Also _get_pgmptr() had
  // issues in some windows 10 versions, so check returned values carefully.
  char* pgmptr = nullptr;
  if (!_get_pgmptr(&pgmptr) && pgmptr != nullptr && *pgmptr) {
    binary_ = pgmptr;
  } else {
    binary_ = argv[0];
  }
#else
  binary_ = argv[0];
#endif
  arguments_.clear();
  std::ostringstream params;
  for (int i = 1; i < argc; ++i) {
    params << ' ' << argv[i];
    arguments_.push_back(argv[i]);
  }
  LOGFILE << "Command line: " << binary_ << params.str();
}

bool CommandLine::ConsumeCommand(const std::string& command) {
  if (arguments_.empty()) return false;
  if (arguments_[0] != command) return false;
  arguments_.erase(arguments_.begin());
  return true;
}

void CommandLine::RegisterMode(const std::string& mode,
                               const std::string& description) {
  modes_.emplace_back(mode, description);
}

std::string CommandLine::BinaryDirectory() {
  std::string path = binary_;
  const auto pos = path.find_last_of("\\/");
  if (pos == std::string::npos) {
    return ".";
  }
  path.resize(pos);
  return path;
}

}  // namespace lczero

```

`src/utils/commandline.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <string>
#include <vector>

namespace lczero {

class CommandLine {
 public:
  CommandLine() = delete;

  // This function must be called before any other.
  static void Init(int argc, const char** argv);

  // Name of the executable filename that was run.
  static const std::string& BinaryName() { return binary_; }

  // Directory where the binary is run. Without trailing slash.
  static std::string BinaryDirectory();

  // If the first command line parameter is @command, remove it and return
  // true. Otherwise return false.
  static bool ConsumeCommand(const std::string& command);

  // Command line arguments.
  static const std::vector<std::string>& Arguments() { return arguments_; }

  static void RegisterMode(const std::string& mode,
                           const std::string& description);

  static const std::vector<std::pair<std::string, std::string>>& GetModes() {
    return modes_;
  }

 private:
  static std::string binary_;
  static std::vector<std::string> arguments_;
  static std::vector<std::pair<std::string, std::string>> modes_;
};

}  // namespace lczero

```

`src/utils/configfile.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "utils/configfile.h"

#include <fstream>
#include <sstream>

#include "utils/commandline.h"
#include "utils/filesystem.h"
#include "utils/logging.h"
#include "utils/optionsparser.h"
#include "utils/string.h"

namespace lczero {
namespace {
const OptionId kConfigFileId{
    "config", "ConfigFile",
    "Path to a configuration file. The format of the file is one command line "
    "parameter per line, e.g.:\n--weights=/path/to/weights",
    'c'};
const char* kDefaultConfigFile = "lc0.config";
const char* kDefaultConfigFileParam = "<default>";
}  // namespace

std::vector<std::string> ConfigFile::arguments_;

void ConfigFile::PopulateOptions(OptionsParser* options) {
  options->Add<StringOption>(kConfigFileId) = kDefaultConfigFile;
}

// This is needed to get the config file from the parameters without calling
// ProcessAllFlags() that should be called only once, and needs the config file.
std::string ConfigFile::ProcessConfigFlag(
    const std::vector<std::string>& args) {
  std::string filename = kDefaultConfigFileParam;
  for (auto iter = args.begin(), end = args.end(); iter != end; ++iter) {
    std::string param = *iter;

    if (param.substr(0, 2) == "--") {
      param = param.substr(2);
      const auto pos = param.find('=');
      if (pos != std::string::npos) {
        if (param.substr(0, pos) == kConfigFileId.long_flag()) {
          filename = param.substr(pos + 1);
        }
      }
    }
    if (param.size() == 2 && param[0] == '-') {
      if (param[1] == kConfigFileId.short_flag() && iter + 1 != end) {
        filename = *(iter + 1);
        ++iter;
      }
    }
  }
  return filename;
}

bool ConfigFile::Init() {
  arguments_.clear();

  // Get the path from the config file parameter.
  std::string filename = ProcessConfigFlag(CommandLine::Arguments());

  // If filename is an empty string then return true. This is to override
  // loading the default configuration file.
  if (filename == "") return true;

  // Parses the file into the arguments_ vector.
  if (!ParseFile(filename)) return false;

  return true;
}

bool ConfigFile::ParseFile(std::string& filename) {
  // Check to see if we are using the default config file or not.
  const bool using_default_config = 
      filename == std::string(kDefaultConfigFileParam);

  std::ifstream input;

  // If no logfile was set on the command line, then the default is
  // to check in the binary directory.
  if (using_default_config) {
    std::vector<std::string> config_dirs = {CommandLine::BinaryDirectory()};
    const std::string user_config_path = GetUserConfigDirectory();
    if (!user_config_path.empty()) {
      config_dirs.emplace_back(user_config_path + "lc0");
    }
    for (const auto& dir : GetSystemConfigDirectoryList()) {
      config_dirs.emplace_back(dir + (dir.back() == '/' ? "" : "/") + "lc0");
    }

    for (const auto& dir : config_dirs) {
      filename = dir + '/' + kDefaultConfigFile;
      input.open(filename);
      if (input.is_open()) break;
    }
  } else {
    input.open(filename);
  }

  if (!input.is_open()) {
    // It is okay if we cannot open the default file since it is normal
    // for it to not exist.
    if (using_default_config) return true;

    CERR << "Could not open configuration file: " << filename;
    return false;
  }

  CERR << "Found configuration file: " << filename;

  for (std::string line; getline(input, line);) {
    // Remove all leading and trailing whitespace.
    line = Trim(line);
    // Ignore comments.
    if (line.substr(0, 1) == "#") continue;
    // Skip blank lines.
    if (line.length() == 0) continue;
    // Allow long form arugments that omit '--'.  If omitted, add here.
    if (line.substr(0, 1) != "-" && line.substr(0, 2) != "--") {
      line = "--" + line;
    }
    // Fail now if the argument does not begin with '--'.
    if (line.substr(0, 2) != "--") {
      CERR << "Only '--' arguments are supported in the "
           << "configuration file: '" << line << "'.";
      return false;
    }
    // Add the line to the arguments list.
    arguments_.emplace_back(line);
  }

  return true;
}

}  // namespace lczero

```

`src/utils/configfile.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <string>
#include <vector>

namespace lczero {

class OptionsParser;

class ConfigFile {
 public:
  ConfigFile() = delete;

  // This function must be called after PopulateOptions.
  static bool Init();

  // Returns the command line arguments from the config file.
  static const std::vector<std::string>& Arguments() { return arguments_; }

  // Add the config file parameter to the options dictionary.
  static void PopulateOptions(OptionsParser* options);

 private:
  // Parses the config file into the arguments_ vector.
  static bool ParseFile(std::string& filename);

  // Returns the absolute path to the config file argument given.
  static std::string ProcessConfigFlag(const std::vector<std::string>& args);

  static std::vector<std::string> arguments_;
};

}  // namespace lczero

```

`src/utils/cppattributes.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

// Enable thread safety attributes only with clang.
// The attributes can be safely erased when compiling with other compilers.
#if defined(__clang__) && (!defined(SWIG))
#define ATTRIBUTE__(x) __attribute__((x))
#else
#define ATTRIBUTE__(x)  // no-op
#endif

#define CAPABILITY(x) ATTRIBUTE__(capability(x))
#define SCOPED_CAPABILITY ATTRIBUTE__(scoped_lockable)
#define GUARDED_BY(x) ATTRIBUTE__(guarded_by(x))
#define PT_GUARDED_BY(x) ATTRIBUTE__(pt_guarded_by(x))
#define ACQUIRED_BEFORE(...) ATTRIBUTE__(acquired_before(__VA_ARGS__))
#define ACQUIRED_AFTER(...) ATTRIBUTE__(acquired_after(__VA_ARGS__))
#define REQUIRES(...) ATTRIBUTE__(requires_capability(__VA_ARGS__))
#define REQUIRES_SHARED(...) \
  ATTRIBUTE__(requires_shared_capability(__VA_ARGS__))
#define ACQUIRE(...) ATTRIBUTE__(acquire_capability(__VA_ARGS__))
#define ACQUIRE_SHARED(...) ATTRIBUTE__(acquire_shared_capability(__VA_ARGS__))
#define RELEASE(...) ATTRIBUTE__(release_capability(__VA_ARGS__))
#define RELEASE_SHARED(...) ATTRIBUTE__(release_shared_capability(__VA_ARGS__))
#define TRY_ACQUIRE(...) ATTRIBUTE__(try_acquire_capability(__VA_ARGS__))
#define TRY_ACQUIRE_SHARED(...) \
  ATTRIBUTE__(try_acquire_shared_capability(__VA_ARGS__))
#define EXCLUDES(...) ATTRIBUTE__(locks_excluded(__VA_ARGS__))
#define ASSERT_CAPABILITY(x) ATTRIBUTE__(assert_capability(x))
#define ASSERT_SHARED_CAPABILITY(x) ATTRIBUTE__(assert_shared_capability(x))
#define RETURN_CAPABILITY(x) ATTRIBUTE__(lock_returned(x))
#define PACKED_STRUCT ATTRIBUTE__(packed)

#define NO_THREAD_SAFETY_ANALYSIS ATTRIBUTE__(no_thread_safety_analysis)

```

`src/utils/esc_codes.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "utils/esc_codes.h"

#ifdef _WIN32
#include <windows.h>
#endif

namespace lczero {

bool EscCodes::enabled_;

void EscCodes::Init() {
#ifdef _WIN32
  HANDLE h = GetStdHandle(STD_OUTPUT_HANDLE);
  DWORD mode;
  GetConsoleMode(h, &mode);
  enabled_ = SetConsoleMode(h, mode | ENABLE_VIRTUAL_TERMINAL_PROCESSING);
#else
  enabled_ = true;
#endif
}

}  // namespace lczero

```

`src/utils/esc_codes.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

namespace lczero {

class EscCodes {
 public:
  EscCodes() = delete;

  // Try to enable ANSI escape sequences for the current terminal.
  static void Init();

  // Supported ANSI escape sequences.
  static const char* Reset() { return enabled_ ? "\033[0m" : ""; }
  static const char* Bold() { return enabled_ ? "\033[1m" : ""; }
  static const char* Underline() { return enabled_ ? "\033[4m" : ""; }
  static const char* Reverse() { return enabled_ ? "\033[7m" : ""; }
  static const char* Normal() { return enabled_ ? "\033[22m" : ""; }
  static const char* NoUnderline() { return enabled_ ? "\033[24m" : ""; }
  static const char* NoReverse() { return enabled_ ? "\033[27m" : ""; }
  static const char* Black() { return enabled_ ? "\033[30m" : ""; }
  static const char* Red() { return enabled_ ? "\033[31m" : ""; }
  static const char* Green() { return enabled_ ? "\033[32m" : ""; }
  static const char* Yellow() { return enabled_ ? "\033[33m" : ""; }
  static const char* Blue() { return enabled_ ? "\033[34m" : ""; }
  static const char* Magenda() { return enabled_ ? "\033[35m" : ""; }
  static const char* Cyan() { return enabled_ ? "\033[36m" : ""; }
  static const char* White() { return enabled_ ? "\033[37m" : ""; }
  static const char* BlackBg() { return enabled_ ? "\033[40m" : ""; }
  static const char* RedBg() { return enabled_ ? "\033[41m" : ""; }
  static const char* GreenBg() { return enabled_ ? "\033[42m" : ""; }
  static const char* YellowBg() { return enabled_ ? "\033[43m" : ""; }
  static const char* BlueBg() { return enabled_ ? "\033[44m" : ""; }
  static const char* MagendaBg() { return enabled_ ? "\033[45m" : ""; }
  static const char* CyanBg() { return enabled_ ? "\033[46m" : ""; }
  static const char* WhiteBg() { return enabled_ ? "\033[47m" : ""; }

 private:
  static bool enabled_;
};

}  // namespace lczero

```

`src/utils/exception.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <stdexcept>
#include "utils/logging.h"

namespace lczero {

// Exception to throw around.
class Exception : public std::runtime_error {
 public:
  Exception(const std::string& what) : std::runtime_error(what) {
    LOGFILE << "Exception: " << what;
  }
};

}  // namespace lczero

```

`src/utils/fastmath.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <cmath>
#include <cstdint>
#include <cstring>

namespace lczero {
// These stunts are performed by trained professionals, do not try this at home.

// Fast approximate log2(x). Does no range checking.
// The approximation used here is log2(2^N*(1+f)) ~ N+f*(1+k-k*f) where N is the
// exponent and f the fraction (mantissa), f>=0. The constant k is used to tune
// the approximation accuracy. In the final version some constants were slightly
// modified for better accuracy with 32 bit floating point math.
inline float FastLog2(const float a) {
  uint32_t tmp;
  std::memcpy(&tmp, &a, sizeof(float));
  uint32_t expb = tmp >> 23;
  tmp = (tmp & 0x7fffff) | (0x7f << 23);
  float out;
  std::memcpy(&out, &tmp, sizeof(float));
  out -= 1.0f;
  // Minimize max absolute error.
  return out * (1.3465552f - 0.34655523f * out) - 127 + expb;
}

// Fast approximate 2^x. Does only limited range checking.
// The approximation used here is 2^(N+f) ~ 2^N*(1+f*(1-k+k*f)) where N is the
// integer and f the fractional part, f>=0. The constant k is used to tune the
// approximation accuracy. In the final version some constants were slightly
// modified for better accuracy with 32 bit floating point math.
inline float FastExp2(const float a) {
  int32_t exp;
  if (a < 0) {
    if (a < -126) return 0.0;
    // Not all compilers optimize floor, so we use (a-1) here to round down.
    // This is obviously off-by-one for integer a, but fortunately the error
    // correction term gives the exact value for 1 (by design, for continuity).
    exp = static_cast<int32_t>(a - 1);
  } else {
    exp = static_cast<int32_t>(a);
  }
  float out = a - exp;
  // Minimize max relative error.
  out = 1.0f + out * (0.6602339f + 0.33976606f * out);
  int32_t tmp;
  std::memcpy(&tmp, &out, sizeof(float));
  tmp += static_cast<int32_t>(static_cast<uint32_t>(exp) << 23);
  std::memcpy(&out, &tmp, sizeof(float));
  return out;
}

// Fast approximate ln(x). Does no range checking.
inline float FastLog(const float a) {
  return 0.6931471805599453f * FastLog2(a);
}

// Fast approximate exp(x). Does only limited range checking.
inline float FastExp(const float a) { return FastExp2(1.442695040f * a); }

inline float FastSign(const float a) {
  // Microsoft compiler does not have a builtin for copysign and emits a
  // library call which is too expensive for hot paths.
#if defined(_MSC_VER)
  // This doesn't treat signed 0's the same way that copysign does, but it
  // should be good enough, for our use case.
  return a < 0 ? -1.0f : 1.0f;
#else
  return std::copysign(1.0f, a);
#endif
}

}  // namespace lczero

```

`src/utils/files.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <zlib.h>

#include <cstdio>

#include "utils/exception.h"

namespace lczero {

std::string ReadFileToString(const std::string& filename) {
  std::string result;

  gzFile f = gzopen(filename.c_str(), "rb");
  if (f == Z_NULL) throw Exception("Cannot open file " + filename);

  std::size_t last_offset = 0;
  std::size_t size = 0x10000;

  while (true) {
    const size_t len = size - last_offset;
    result.resize(size);
    size_t count = gzread(f, result.data() + last_offset, len);
    if (count < len) {
      result.resize(last_offset + count);
      break;
    }
    last_offset += len;
    size *= 2;
  }

  gzclose(f);
  result.shrink_to_fit();
  return result;
}

void WriteStringToFile(const std::string& filename,
                       const std::string_view content) {
  std::FILE* f = std::fopen(filename.c_str(), "wb");
  if (f == nullptr) throw Exception("Cannot open file for write: " + filename);
  if (std::fwrite(content.data(), content.size(), 1, f) != 1) {
    throw Exception("Cannot write to file: " + filename);
  }
  std::fclose(f);
}

void WriteStringToGzFile(const std::string& filename,
                         std::string_view content) {
  gzFile f = gzopen(filename.c_str(), "wb");
  if (f == nullptr)
    throw Exception("Cannot open gzfile for write: " + filename);
  if (gzwrite(f, content.data(), content.size()) !=
      static_cast<int>(content.size())) {
    throw Exception("Cannot write to file: " + filename);
  }
  gzclose(f);
}

}  // namespace lczero
```

`src/utils/files.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2021 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <string>

namespace lczero {

// Reads (possibly gz-compressed) file to string. Throws on error.
std::string ReadFileToString(const std::string& filename);

// Writes string to file, without compression. Throws on error.
void WriteStringToFile(const std::string& filename,
                         std::string_view  content);

// Writes string to gz-compressed file. Throws on error.
void WriteStringToGzFile(const std::string& filename,
                         std::string_view  content);
}  // namespace lczero

```

`src/utils/filesystem.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <time.h>
#include <string>
#include <vector>

namespace lczero {

// Creates directory at a given path. Throws exception if cannot.
// Returns silently if already exists.
void CreateDirectory(const std::string& path);

// Returns list of full paths of regular files in this directory.
// Silently returns empty vector on error.
std::vector<std::string> GetFileList(const std::string& directory);

// Returns size of a file, 0 if file doesn't exist or can't be read.
uint64_t GetFileSize(const std::string& filename);

// Returns modification time of a file, 0 if file doesn't exist or can't be read.
time_t GetFileTime(const std::string& filename);

// Returns the base directory relative to which user specific non-essential data
// files are stored or an empty string if unspecified.
std::string GetUserCacheDirectory();

// Returns the base directory relative to which user specific configuration
// files are stored or an empty string if unspecified.
std::string GetUserConfigDirectory();

// Returns the base directory relative to which user specific data files are
// stored or an empty string if unspecified.
std::string GetUserDataDirectory();

// Returns a vector of base directories to search for configuration files.
std::vector<std::string> GetSystemConfigDirectoryList();

// Returns a vector of base directories to search for data files.
std::vector<std::string> GetSystemDataDirectoryList();

}  // namespace lczero

```

`src/utils/filesystem.posix.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "utils/exception.h"
#include "utils/filesystem.h"
#include "utils/string.h"

#include <dirent.h>
#include <errno.h>
#include <sys/stat.h>

namespace lczero {

void CreateDirectory(const std::string& path) {
  if (mkdir(path.c_str(), 0777) < 0 && errno != EEXIST) {
    throw Exception("Cannot create directory: " + path);
  }
}

std::vector<std::string> GetFileList(const std::string& directory) {
  std::vector<std::string> result;
  DIR* dir = opendir(directory.c_str());
  if (!dir) return result;
  while (auto* entry = readdir(dir)) {
    bool exists = false;
    switch (entry->d_type) {
      case DT_REG:
        exists = true;
        break;
      case DT_LNK:
        // check that the soft link actually points to a regular file.
        const std::string filename = directory + "/" + entry->d_name;
        struct stat s;
        exists =
            stat(filename.c_str(), &s) == 0 && (s.st_mode & S_IFMT) == S_IFREG;
        break;
    }
    if (exists) result.push_back(entry->d_name);
  }
  closedir(dir);
  return result;
}

uint64_t GetFileSize(const std::string& filename) {
  struct stat s;
  if (stat(filename.c_str(), &s) < 0) {
    return 0;
  }
  return s.st_size;
}

time_t GetFileTime(const std::string& filename) {
  struct stat s;
  if (stat(filename.c_str(), &s) < 0) {
    return 0;
  }
#ifdef __APPLE__
  return s.st_mtimespec.tv_sec;
#else
  return s.st_mtim.tv_sec;
#endif
}

namespace {
bool CheckDir(const std::string& dirname) {
  struct stat s;
  return stat(dirname.c_str(), &s) == 0 && S_ISDIR(s.st_mode);
}

}  // namespace

std::string GetUserCacheDirectory() {
#ifdef __APPLE__
  constexpr auto kLocalDir = "Library/Caches/";
#else
  constexpr auto kLocalDir = ".cache/";
  const char *xdg_cache_home = std::getenv("XDG_CACHE_HOME");
  if (xdg_cache_home != NULL && CheckDir(xdg_cache_home)) {
    return std::string(xdg_cache_home) + "/";
  }
#endif
  const char *home = std::getenv("HOME");
  if (home == NULL) return std::string();
  const std::string path = std::string(home) + "/" + kLocalDir;
  if (!CheckDir(path)) return std::string();
  return path;
}

std::string GetUserConfigDirectory() {
#ifdef __APPLE__
  constexpr auto kLocalDir = "Library/Preferences/";
#else
  constexpr auto kLocalDir = ".config/";
  const char *xdg_config_home = std::getenv("XDG_CONFIG_HOME");
  if (xdg_config_home != NULL && CheckDir(xdg_config_home)) {
    return std::string(xdg_config_home) + "/";
  }
#endif
  const char *home = std::getenv("HOME");
  if (home == NULL) return std::string();
  const std::string path = std::string(home) + "/" + kLocalDir;
  if (!CheckDir(path)) return std::string();
  return path;
}

std::string GetUserDataDirectory() {
#ifdef __APPLE__
  constexpr auto kLocalDir = "Library/";
#else
  constexpr auto kLocalDir = ".local/share/";
  const char *xdg_data_home = std::getenv("XDG_DATA_HOME");
  if (xdg_data_home != NULL && CheckDir(xdg_data_home)) {
    return std::string(xdg_data_home) + "/";
  }
#endif
  const char *home = std::getenv("HOME");
  if (home == NULL) return std::string();
  const std::string path = std::string(home) + "/" + kLocalDir;
  if (!CheckDir(path)) return std::string();
  return path;
}

std::vector<std::string> GetSystemConfigDirectoryList() {
#ifdef __APPLE__
  return {};
#else
  std::vector<std::string> result;
  const char *xdg_config_dirs = std::getenv("XDG_CONFIG_DIRS");
  if (xdg_config_dirs == NULL) {
    return {"/etc/xdg/"};
  }
  result = StrSplit(xdg_config_dirs, ":");
  return result;
#endif
}

std::vector<std::string> GetSystemDataDirectoryList() {
#ifdef __APPLE__
  return {};
#else
  std::vector<std::string> result;
  const char *xdg_data_dirs = std::getenv("XDG_DATA_DIRS");
  if (xdg_data_dirs == NULL) {
    return {"/usr/local/share/", "/usr/share/"};
  }
  result = StrSplit(xdg_data_dirs, ":");
  return result;
#endif
}

}  // namespace lczero

```

`src/utils/filesystem.win32.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "utils/exception.h"
#include "utils/filesystem.h"

#include <windows.h>
#undef CreateDirectory

namespace lczero {

void CreateDirectory(const std::string& path) {
  if (CreateDirectoryA(path.c_str(), nullptr)) return;
  if (GetLastError() != ERROR_ALREADY_EXISTS) {
    throw Exception("Cannot create directory: " + path);
  }
}

std::vector<std::string> GetFileList(const std::string& directory) {
  std::vector<std::string> result;
  WIN32_FIND_DATAA dir;
  const auto handle = FindFirstFileA((directory + "\\*").c_str(), &dir);
  if (handle == INVALID_HANDLE_VALUE) return result;
  do {
    if ((dir.dwFileAttributes & FILE_ATTRIBUTE_DIRECTORY) == 0) {
      result.emplace_back(dir.cFileName);
    }
  } while (FindNextFile(handle, &dir) != 0);
  FindClose(handle);
  return result;
}

uint64_t GetFileSize(const std::string& filename) {
  WIN32_FILE_ATTRIBUTE_DATA s;
  if (!GetFileAttributesExA(filename.c_str(), GetFileExInfoStandard, &s)) {
    return 0;
  }
  return (static_cast<uint64_t>(s.nFileSizeHigh) << 32) + s.nFileSizeLow;
}

time_t GetFileTime(const std::string& filename) {
  WIN32_FILE_ATTRIBUTE_DATA s;
  if (!GetFileAttributesExA(filename.c_str(), GetFileExInfoStandard, &s)) {
    return 0;
  }
  return (static_cast<uint64_t>(s.ftLastWriteTime.dwHighDateTime) << 32) +
         s.ftLastWriteTime.dwLowDateTime;
}

std::string GetUserCacheDirectory() {
  return std::string();
}

std::string GetUserConfigDirectory() {
  return std::string();
}

std::string GetUserDataDirectory() {
  return std::string();
}

std::vector<std::string> GetSystemConfigDirectoryList() {
  return {};
}

std::vector<std::string> GetSystemDataDirectoryList() {
  return {};
}


}  // namespace lczero

```

`src/utils/fp16_utils.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <cstdint>
#include <cstring>

// Define NO_F16C to avoid the F16C intrinsics. Also disabled with NO_POPCNT
// since it catches most processors without F16C instructions.

#if defined(_M_IX86) || defined(_M_X64) || defined(__i386__) || \
    defined(__x86_64__)
#include <immintrin.h>
#else
#define NO_F16C
#endif

namespace lczero {

uint16_t FP32toFP16(float f32) {
#if defined(NO_POPCNT) || defined(NO_F16C) || \
    (defined(__GNUC__) && !defined(__F16C__))
  unsigned int x;
  unsigned int sign = 0;
  memcpy(&x, &f32, sizeof(float));
  if (x & 0x80000000) sign = 0x8000;
  x &= 0x7fffffff;
  if (x >= 0x477ff000) {
    if ((x & 0x7f800000) == 0x7f800000 && (x & 0x7fffff)) {
      x = ((x >> 13) - 0x38000) | 0x200;
    } else {
      x = 0x7c00;
    }
  } else if (x <= 0x33000000)
    x = 0;
  else if (x <= 0x387fefff) {
    int shift = 126 - ((x >> 23) & 0xff);
    x = (x & 0x7fffff) | 0x800000;
    if (x & (0x17fffff >> (24 - shift))) x += 0x800000 >> (24 - shift);
    x >>= shift;
  } else {
    // Adjust exponent and round to nearest even.
    if (x & 0x2fff) {
      x -= 0x37fff000;
    } else {
      x -= 0x38000000;
    }
    x >>= 13;
  }
  return x | sign;
#else
  __m128 A = _mm_set_ss(f32);
  __m128i H = _mm_cvtps_ph(A, 0);
  return _mm_extract_epi16(H, 0);
#endif
}

float FP16toFP32(uint16_t f16) {
#if defined(NO_POPCNT) || defined(NO_F16C) || \
    (defined(__GNUC__) && !defined(__F16C__))
  unsigned int x;
  float f;
  x = f16 & 0x7fff;
  if ((x & 0x7c00) == 0) {
    f = 5.9604645e-8f * x;
    memcpy(&x, &f, sizeof(float));
  } else if (x >= 0x7c00) {
    if (x & 0x1ff) x |= 0x200;
    x = (x + 0x38000) << 13;
  } else {
    x = (x + 0x1c000) << 13;
  }
  if (f16 & 0x8000) x |= 0x80000000;
  memcpy(&f, &x, sizeof(float));
  return f;
#else
  __m128i H = _mm_setzero_si128();
  H = _mm_insert_epi16(H, f16, 0);
  __m128 A = _mm_cvtph_ps(H);
  return _mm_cvtss_f32(A);
#endif
}

}  // namespace lczero

```

`src/utils/fp16_utils.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#pragma once
namespace lczero {

uint16_t FP32toFP16(float f32);
float FP16toFP32(uint16_t f16);

};  // namespace lczero

```

`src/utils/hashcat.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include <cstdint>
#include <initializer_list>

#pragma once
namespace lczero {

// Tries to scramble @val.
inline uint64_t Hash(uint64_t val) {
  return 0xfad0d7f2fbb059f1ULL * (val + 0xbaad41cdcb839961ULL) +
         0x7acec0050bf82f43ULL * ((val >> 31) + 0xd571b3a92b1b2755ULL);
}

// Appends value to a hash.
inline uint64_t HashCat(uint64_t hash, uint64_t x) {
  hash ^= 0x299799adf0d95defULL + Hash(x) + (hash << 6) + (hash >> 2);
  return hash;
}

// Combines 64-bit values into concatenated hash.
inline uint64_t HashCat(std::initializer_list<uint64_t> args) {
  uint64_t hash = 0;
  for (uint64_t x : args) hash = HashCat(hash, x);
  return hash;
}

}  // namespace lczero

```

`src/utils/hashcat_test.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "utils/hashcat.h"
#include <gtest/gtest.h>

namespace lczero {

TEST(HashCat, TestCollision) {
  uint64_t hash1 = HashCat({0x8000000010500000, 0x4000080000002000,
                            0x8000000000002000, 0x4000000000000000});
  uint64_t hash2 = HashCat({0x4000000010500000, 0x1000080000002000,
                            0x4000000000002000, 0x1000000000000000});
  EXPECT_NE(hash1, hash2);
}

}  // namespace lczero

int main(int argc, char** argv) {
  ::testing::InitGoogleTest(&argc, argv);
  return RUN_ALL_TESTS();
}

```

`src/utils/histogram.cc`:

```cc
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

 Additional permission under GNU GPL version 3 section 7

 If you modify this Program, or any covered work, by linking or
 combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
 Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
 modified version of those libraries), containing parts covered by the
 terms of the respective license agreement, the licensors of this
 Program grant you additional permission to convey the resulting work.
 */

#include "utils/histogram.h"
#include <stdio.h>
#include <string.h>
#include <algorithm>
#include <cmath>
#include <iomanip>
#include <iostream>

namespace lczero {

namespace {
void Print(const std::string& what) { std::cerr << what; }

void PrintAligned(const std::string& what, int aligned) {
  std::cerr << std::right << std::setw(aligned) << what;
}

std::string Format(const std::string& format, double value) {
  static const int kMaxBufferSize = 32;
  char buffer[kMaxBufferSize];
  const int len = snprintf(buffer, kMaxBufferSize, format.c_str(), value);
  return std::string(buffer, buffer + len);
}
}  // namespace

Histogram::Histogram()
    : Histogram(kDefaultMinExp, kDefaultMaxExp, kDefaultMinorScales) {}

Histogram::Histogram(int min_exp, int max_exp, int minor_scales)
    : min_exp_(min_exp),
      max_exp_(max_exp),
      minor_scales_(minor_scales),
      major_scales_(max_exp_ - min_exp_ + 1),
      total_scales_(major_scales_ * minor_scales_),
      buckets_(total_scales_ + 4) {
  Clear();
}

void Histogram::Clear() {
  std::fill(buckets_.begin(), buckets_.end(), 0);
  total_ = 0;
  max_ = 0;
}

void Histogram::Add(double value) {
  const int index = GetIndex(std::abs(value));
  const int count = ++buckets_[index];
  total_++;
  if (count > max_) max_ = count;
}

void Histogram::Dump() const {
  const double ymax = 0.02 + max_ / (double)total_;
  for (int i = 0; i < 100; i++) {
    const double yscale = 1 - i * 0.01;
    if (yscale > ymax) continue;
    const bool scale = i % 5 == 0;
    if (scale) {
      PrintAligned(Format("%.2g", yscale), 5);
      Print(" +");
    } else {
      Print("      |");
    }
    const double ymin = (99 - i) * 0.01;
    for (size_t j = 0; j < buckets_.size(); j++) {
      const double val = buckets_[j] / (double)total_;
      if (val > ymin) {
        Print("#");
      } else {
        Print(" ");
      }
    }
    if (scale) {
      Print("+");
    } else {
      Print("|");
    }
    Print("\n");
  }
  Print("      +");
  for (int j = 0; j <= major_scales_; j++) {
    const int size = j == 0 ? 5 : minor_scales_;
    for (int k = 0; k < size - 1; k++) Print("-");
    Print("+");
  }
  Print("\n");
  Print("   -inf");
  for (int j = 0; j < major_scales_; j++) {
    const int size = j == 0 ? 5 : minor_scales_;
    Print(" ");
    PrintAligned(Format("%g", min_exp_ + j), size - 1);
  }
  Print("  ");
  PrintAligned("+inf", minor_scales_ - 2);
  Print(" \n");
}

int Histogram::GetIndex(double val) const {
  if (val <= 0) return 0;
  const double log10 = std::log10(val);
  // 2: -15 :    -15.1 ... -14.9          2 ... 3
  // 1:          -15.3 ... -15.1
  // 0:          -15.5 ... -15.3          0 ... 1
  const int index =
      static_cast<int>(std::floor(2.5 + minor_scales_ * (log10 - min_exp_)));
  if (index < 0) return 0;
  if (index >= total_scales_) return total_scales_ + 3;
  return index + 2;
}

}  // namespace lczero

```

`src/utils/histogram.h`:

```h
/*
 This file is part of Leela Chess Zero.
 Copyright (C) 2018 The LCZero Authors

 Leela Chess is free software: you can redistribute it and/or modify
 it under the terms of the GNU General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.

 Leela Chess is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU General Public License for more details.

 You should have received a copy of the GNU General Public License
 along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

 Additional permission under GNU GPL version 3 section 7

 If you modify this Program, or any covered work, by linking or
 combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
 Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
 modified version of those libraries), containing parts covered by the
 terms of the respective license agreement, the licensors of this
 Program grant you additional permission to convey the resulting work.
 */

#pragma once

#include <cstddef>
#include <string>
#include <vector>

namespace lczero {

// Histogram with a logarithmic x-axis.
//
//    0.50   +
//           |
//           |
//           |
//    0.40   |
//
//          ....
//
//           |
//    0.10   +
//           |
//           |#
//           |#         ##                     #|
//           |#  #   # #### #  #     #         #|
//    0.00   +----+----+----+----+---- ... +----+
//
//         -inf  -15  -14  -13  -12        5   inf

class Histogram {
 public:
  // Creates a histogram with default scales.
  Histogram();

  // Creates a histogram from 10^min_exp to 10^max_exp
  // with minor_scales spacing.
  Histogram(int min_exp, int max_exp, int minor_scales);

  void Clear();

  // Adds a sample.
  void Add(double value);

  // Dumps the histogram to stderr.
  void Dump() const;

 private:
  int GetIndex(double val) const;

  static constexpr int kDefaultMinExp = -15;
  static constexpr int kDefaultMaxExp = 5;
  static constexpr int kDefaultMinorScales = 5;

  const int min_exp_;
  const int max_exp_;
  const int minor_scales_;
  const int major_scales_;
  const int total_scales_;
  std::vector<double> buckets_;
  double total_;
  double max_;
};

}  // namespace lczero

```

`src/utils/logging.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "utils/logging.h"

#include <iomanip>
#include <iostream>
#include <thread>

namespace lczero {

namespace {
const size_t kBufferSizeLines = 200;
const char* const kStderrFilename = "<stderr>";
}  // namespace

Logging& Logging::Get() {
  static Logging logging;
  return logging;
}

void Logging::WriteLineRaw(const std::string& line) {
  Mutex::Lock lock_(mutex_);
  if (filename_.empty()) {
    buffer_.push_back(line);
    if (buffer_.size() > kBufferSizeLines) buffer_.pop_front();
  } else {
    auto& file = (filename_ == kStderrFilename) ? std::cerr : file_;
    file << line << std::endl;
  }
}

void Logging::SetFilename(const std::string& filename) {
  Mutex::Lock lock_(mutex_);
  if (filename_ == filename) return;
  filename_ = filename;
  if (filename.empty() || filename == kStderrFilename) {
    file_.close();
  }
  if (filename.empty()) return;
  if (filename != kStderrFilename) file_.open(filename, std::ios_base::app);
  auto& file = (filename == kStderrFilename) ? std::cerr : file_;
  file << "\n\n============= Log started. =============" << std::endl;
  for (const auto& line : buffer_) file << line << std::endl;
  buffer_.clear();
}

LogMessage::LogMessage(const char* file, int line) {
  *this << FormatTime(std::chrono::system_clock::now()) << ' '
        << std::setfill(' ') << std::this_thread::get_id() << std::setfill('0')
        << ' ' << file << ':' << line << "] ";
}

LogMessage::~LogMessage() { Logging::Get().WriteLineRaw(str()); }

StderrLogMessage::StderrLogMessage(const char* file, int line)
    : log_(file, line) {}

StderrLogMessage::~StderrLogMessage() {
  std::cerr << str() << std::endl;
  log_ << str();
}

StdoutLogMessage::StdoutLogMessage(const char* file, int line)
    : log_(file, line) {}

StdoutLogMessage::~StdoutLogMessage() {
  std::cout << str() << std::endl;
  log_ << str();
}

std::chrono::time_point<std::chrono::system_clock> SteadyClockToSystemClock(
    std::chrono::time_point<std::chrono::steady_clock> time) {
  return std::chrono::system_clock::now() +
         std::chrono::duration_cast<std::chrono::system_clock::duration>(
             time - std::chrono::steady_clock::now());
}

std::string FormatTime(
    std::chrono::time_point<std::chrono::system_clock> time) {
  static Mutex mutex;

  std::ostringstream ss;
  using std::chrono::duration_cast;
  using std::chrono::microseconds;
  const auto us =
      duration_cast<microseconds>(time.time_since_epoch()).count() % 1000000;
  auto timer = std::chrono::system_clock::to_time_t(time);
  // std::localtime is not thread safe. Since this is the only place
  // std::localtime is used in the program, guard by mutex.
  // TODO: replace with std::localtime_r or s once they are properly
  // standardised. Or some other more c++ like time component thing, whichever
  // comes first...
  {
    Mutex::Lock lock(mutex);
    ss << std::put_time(std::localtime(&timer), "%m%d %T") << '.'
       << std::setfill('0') << std::setw(6) << us;
  }
  return ss.str();
}

}  // namespace lczero
```

`src/utils/logging.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <deque>
#include <fstream>
#include <iomanip>
#include <sstream>
#include <string>

#include "utils/mutex.h"

namespace lczero {

class Logging {
 public:
  static Logging& Get();

  // Sets the name of the log. Empty name disables logging.
  void SetFilename(const std::string& filename);

 private:
  // Writes line to the log, and appends new line character.
  void WriteLineRaw(const std::string& line);

  Mutex mutex_;
  std::string filename_ GUARDED_BY(mutex_);
  std::ofstream file_ GUARDED_BY(mutex_);
  std::deque<std::string> buffer_ GUARDED_BY(mutex_);

  Logging() = default;
  friend class LogMessage;
};

class LogMessage : public std::ostringstream {
 public:
  LogMessage(const char* file, int line);
  ~LogMessage();
};

class StderrLogMessage : public std::ostringstream {
 public:
  StderrLogMessage(const char* file, int line);
  ~StderrLogMessage();

 private:
  LogMessage log_;
};

class StdoutLogMessage : public std::ostringstream {
 public:
  StdoutLogMessage(const char* file, int line);
  ~StdoutLogMessage();

 private:
  LogMessage log_;
};

std::chrono::time_point<std::chrono::system_clock> SteadyClockToSystemClock(
    std::chrono::time_point<std::chrono::steady_clock> time);

std::string FormatTime(std::chrono::time_point<std::chrono::system_clock> time);
}  // namespace lczero

#define LOGFILE ::lczero::LogMessage(__FILE__, __LINE__)
#define CERR ::lczero::StderrLogMessage(__FILE__, __LINE__)
#define COUT ::lczero::StdoutLogMessage(__FILE__, __LINE__)
```

`src/utils/mutex.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <atomic>
#include <mutex>
#include <shared_mutex>
#include <thread>

#if !defined(__arm__) && !defined(__aarch64__) && !defined(_M_ARM) && \
    !defined(_M_ARM64)
#include <emmintrin.h>
#endif

#include "utils/cppattributes.h"

namespace lczero {

// Implementation of reader-preferenced shared mutex. Based on fair shared
// mutex.
class CAPABILITY("mutex") RpSharedMutex {
 public:
  RpSharedMutex() : waiting_readers_(0) {}

  void lock() ACQUIRE() {
    while (true) {
      mutex_.lock();
      if (waiting_readers_ == 0) return;
      mutex_.unlock();
    }
  }
  void unlock() RELEASE() { mutex_.unlock(); }
  void lock_shared() ACQUIRE_SHARED() {
    ++waiting_readers_;
    mutex_.lock_shared();
  }
  void unlock_shared() RELEASE_SHARED() {
    --waiting_readers_;
    mutex_.unlock_shared();
  }

 private:
  std::shared_timed_mutex mutex_;
  std::atomic<int> waiting_readers_;
};

// std::mutex wrapper for clang thread safety annotation.
class CAPABILITY("mutex") Mutex {
 public:
  // std::unique_lock<std::mutex> wrapper.
  class SCOPED_CAPABILITY Lock {
   public:
    Lock(Mutex& m) ACQUIRE(m) : lock_(m.get_raw()) {}
    ~Lock() RELEASE() {}
    std::unique_lock<std::mutex>& get_raw() { return lock_; }

   private:
    std::unique_lock<std::mutex> lock_;
  };

  void lock() ACQUIRE() { mutex_.lock(); }
  void unlock() RELEASE() { mutex_.unlock(); }
  std::mutex& get_raw() { return mutex_; }

 private:
  std::mutex mutex_;
};

// std::shared_mutex wrapper for clang thread safety annotation.
class CAPABILITY("mutex") SharedMutex {
 public:
  // std::unique_lock<std::shared_mutex> wrapper.
  class SCOPED_CAPABILITY Lock {
   public:
    Lock(SharedMutex& m) ACQUIRE(m) : lock_(m.get_raw()) {}
    ~Lock() RELEASE() {}

   private:
    std::unique_lock<std::shared_timed_mutex> lock_;
  };

  // std::shared_lock<std::shared_mutex> wrapper.
  class SCOPED_CAPABILITY SharedLock {
   public:
    SharedLock(SharedMutex& m) ACQUIRE_SHARED(m) : lock_(m.get_raw()) {}
    ~SharedLock() RELEASE() {}

   private:
    std::shared_lock<std::shared_timed_mutex> lock_;
  };

  void lock() ACQUIRE() { mutex_.lock(); }
  void unlock() RELEASE() { mutex_.unlock(); }
  void lock_shared() ACQUIRE_SHARED() { mutex_.lock_shared(); }
  void unlock_shared() RELEASE_SHARED() { mutex_.unlock_shared(); }

  std::shared_timed_mutex& get_raw() { return mutex_; }

 private:
  std::shared_timed_mutex mutex_;
};

static inline void SpinloopPause() {
#if !defined(__arm__) && !defined(__aarch64__) && !defined(_M_ARM) && \
    !defined(_M_ARM64)
  _mm_pause();
#endif
}

// A very simple spin lock.
class CAPABILITY("mutex") SpinMutex {
 public:
  // std::unique_lock<SpinMutex> wrapper.
  class SCOPED_CAPABILITY Lock {
   public:
    Lock(SpinMutex& m) ACQUIRE(m) : lock_(m) {}
    ~Lock() RELEASE() {}

   private:
    std::unique_lock<SpinMutex> lock_;
  };

  void lock() ACQUIRE() {
    int spins = 0;
    while (true) {
      int val = 0;
      if (mutex_.compare_exchange_weak(val, 1, std::memory_order_acq_rel)) {
        break;
      }
      ++spins;
      // Help avoid complete resource starvation by yielding occasionally if
      // needed.
      if (spins % 512 == 0) {
        std::this_thread::yield();
      } else {
        SpinloopPause();
      }
    }
  }
  void unlock() RELEASE() { mutex_.store(0, std::memory_order_release); }

 private:
  std::atomic<int> mutex_{0};
};

}  // namespace lczero

```

`src/utils/numa.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "utils/numa.h"

#include "chess/bitboard.h"
#include "utils/logging.h"

#ifdef _WIN32
#include <windows.h>
#endif

namespace lczero {

int Numa::threads_per_core_ = 1;

void Numa::Init() {
#if defined(_WIN64) && _WIN32_WINNT >= 0x0601
  SYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX* buffer;
  DWORD len;
  GetLogicalProcessorInformationEx(RelationProcessorCore, NULL, &len);
  buffer = static_cast<SYSTEM_LOGICAL_PROCESSOR_INFORMATION_EX*>(malloc(len));
  GetLogicalProcessorInformationEx(RelationProcessorCore, buffer, &len);
  if (buffer->Processor.Flags & LTP_PC_SMT) {
    threads_per_core_ = BitBoard(buffer->Processor.GroupMask[0].Mask).count();
  }
  free(buffer);

  int group_count = GetActiveProcessorGroupCount();
  int thread_count = GetActiveProcessorCount(ALL_PROCESSOR_GROUPS);
  int core_count = thread_count / threads_per_core_;
  CERR << "Detected " << core_count << " core(s) and " << thread_count
       << " thread(s) in " << group_count << " group(s).";
  for (int group_id = 0; group_id < group_count; group_id++) {
    int group_threads = GetActiveProcessorCount(group_id);
    int group_cores = group_threads / threads_per_core_;
    CERR << "Group " << group_id << " has " << group_cores
         << " core(s) and " << group_threads << " thread(s).";
  }
#endif
}

void Numa::BindThread(int id) {
#if defined(_WIN64) && _WIN32_WINNT >= 0x0601
  int group_count = GetActiveProcessorGroupCount();
  int thread_count = GetActiveProcessorCount(ALL_PROCESSOR_GROUPS);
  int core_count = thread_count / threads_per_core_;
  int core_id = id;
  GROUP_AFFINITY affinity = {};
  for (int group_id = 0; group_id < group_count; group_id++) {
    int group_threads = GetActiveProcessorCount(group_id);
    int group_cores = group_threads / threads_per_core_;
    // Allocate cores of each group in order, and distribute remaining threads
    // to all groups.
    if ((id < core_count && core_id < group_cores) ||
        (id >= core_count && (id - core_count) % group_count == group_id)) {
      affinity.Group = group_id;
      affinity.Mask = ~0ULL >> (64 - group_threads);
      SetThreadGroupAffinity(GetCurrentThread(), &affinity, NULL);
      break;
    }
    core_id -= group_cores;
  }
#else
  // Silence warning.
  (void)id;
#endif
}

}  // namespace lczero

```

`src/utils/numa.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

namespace lczero {

class Numa {
 public:
  Numa() = delete;

  // Initialize and display statistics about processor configuration.
  static void Init();

  // Bind thread to processor group.
  static void BindThread(int id);

 private:
  static int threads_per_core_;
};

}  // namespace lczero

```

`src/utils/optionsdict.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "utils/optionsdict.h"

#include <cassert>
#include <cctype>
#include <sstream>
#include <string>

#include "utils/exception.h"

namespace lczero {

const OptionsDict& OptionsDict::GetSubdict(const std::string& name) const {
  const auto iter = subdicts_.find(name);
  if (iter == subdicts_.end())
    throw Exception("Subdictionary not found: " + name);
  return iter->second;
}

// Returns subdictionary. Throws exception if doesn't exist.
OptionsDict* OptionsDict::GetMutableSubdict(const std::string& name) {
  auto iter = subdicts_.find(name);
  if (iter == subdicts_.end())
    throw Exception("Subdictionary not found: " + name);
  return &iter->second;
}

// Creates subdictionary. Throws exception if already exists.
OptionsDict* OptionsDict::AddSubdict(const std::string& name) {
  const auto iter = subdicts_.find(name);
  if (iter != subdicts_.end())
    throw Exception("Subdictionary already exists: " + name);
  const auto x = &subdicts_.emplace(name, this).first->second;
  return x;
}

void OptionsDict::AddAliasDict(const OptionsDict* dict) {
  aliases_.push_back(dict);
}

// Returns list of subdictionaries.
std::vector<std::string> OptionsDict::ListSubdicts() const {
  std::vector<std::string> result;
  for (const auto& subdict : subdicts_) {
    result.emplace_back(subdict.first);
  }
  return result;
}

bool OptionsDict::HasSubdict(const std::string& name) const {
  return subdicts_.find(name) != subdicts_.end();
}

namespace {

class Lexer {
 public:
  enum TokenType {
    L_INTEGER,
    L_FLOAT,
    L_STRING,
    L_IDENTIFIER,
    L_LEFT_PARENTHESIS,
    L_RIGHT_PARENTHESIS,
    L_COMMA,
    L_EQUAL,
    L_EOF
  };

  Lexer(const std::string& str) : str_(str) { Next(); }

  void Next() {
    // Skip whitespace:
    while (idx_ < str_.size() && std::isspace(str_[idx_])) ++idx_;
    last_offset_ = idx_;

    // If end of line, report end of line.
    if (idx_ == str_.size()) {
      type_ = L_EOF;
      return;
    }

    // Single characters.
    static const std::pair<char, TokenType> kCharacters[] = {
        {',', L_COMMA},
        {'(', L_LEFT_PARENTHESIS},
        {')', L_RIGHT_PARENTHESIS},
        {'=', L_EQUAL}};
    for (const auto& ch : kCharacters) {
      if (str_[idx_] == ch.first) {
        ++idx_;
        type_ = ch.second;
        return;
      }
    }

    // Numbers (integer of float).
    static const std::string kNumberChars = "0123456789-.";
    if (kNumberChars.find(str_[idx_]) != std::string::npos) {
      ReadNumber();
      return;
    }

    // Strings (single or double quoted)
    if (str_[idx_] == '\'' || str_[idx_] == '\"') {
      ReadString();
      return;
    }

    // Identifier
    if (std::isalnum(str_[idx_]) || str_[idx_] == '/') {
      ReadIdentifier();
      return;
    }

    RaiseError("Unable to parse token");
  }

  void RaiseError(const std::string& message) {
    throw Exception("Unable to parse config at offset " +
                    std::to_string(last_offset_) + ": " + str_ + " (" +
                    message + ")");
  }

  TokenType GetToken() const { return type_; }
  const std::string& GetStringVal() const { return string_val_; }
  int GetIntVal() const { return int_val_; }
  float GetFloatVal() const { return float_val_; }

 private:
  void ReadString() {
    last_offset_ = idx_;
    const char quote = str_[idx_++];

    for (; idx_ < str_.size(); ++idx_) {
      if (str_[idx_] == quote) {
        type_ = L_STRING;
        string_val_ = str_.substr(last_offset_ + 1, idx_ - last_offset_ - 1);
        ++idx_;
        return;
      }
    }

    last_offset_ = idx_;
    RaiseError("String is not closed at end of line");
  }

  void ReadIdentifier() {
    string_val_ = "";
    type_ = L_IDENTIFIER;
    static const std::string kAllowedPunctuation = "_-./";
    for (; idx_ < str_.size(); ++idx_) {
      if (!std::isalnum(str_[idx_]) &&
          kAllowedPunctuation.find(str_[idx_]) == std::string::npos) {
        break;
      }
      string_val_ += str_[idx_];
    }
  }

  void ReadNumber() {
    last_offset_ = idx_;
    bool is_float = false;
    static const std::string kFloatChars = ".eE";
    static const std::string kAllowedChars = "+-1234567890.eExX";
    for (; idx_ < str_.size(); ++idx_) {
      if (kAllowedChars.find(str_[idx_]) == std::string::npos) break;
      if (kFloatChars.find(str_[idx_]) != std::string::npos) is_float = true;
    }

    try {
      if (is_float) {
        type_ = L_FLOAT;
        float_val_ = stof(str_.substr(last_offset_, idx_ - last_offset_));

      } else {
        type_ = L_INTEGER;
        int_val_ = stoi(str_.substr(last_offset_, idx_ - last_offset_));
      }

    } catch (...) {
      RaiseError("Unable to parse number");
    }
  }

  float float_val_;
  int int_val_;
  std::string string_val_;
  TokenType type_;
  const std::string str_;
  size_t idx_ = 0;
  int last_offset_ = 0;
};

class Parser {
 public:
  Parser(const std::string& str) : lexer_(str) {}

  void ParseMain(OptionsDict* dict) {
    ParseList(dict);            // Parse list of options
    EnsureToken(Lexer::L_EOF);  // Check that everything is read.
  }

 private:
  // Returns first non-existing subdict with name like "[0]", "[24]", etc.
  static std::string GetFreeSubdictName(OptionsDict* dict) {
    for (int idx = 0;; ++idx) {
      std::string id = "[" + std::to_string(idx) + "]";
      if (!dict->HasSubdict(id)) return id;
    }
    assert(false);
    return "";
  }

  // Parses comma separated list of either:
  // * key=value, or
  // * subdict(comma separated list)
  // Note that in subdict all parts are optional:
  // * (comma separated list) -- name will be synthesized (e.g. "[1]")
  // * subdict() -- empty list
  // * subdict -- the same.
  void ParseList(OptionsDict* dict) {
    while (true) {
      std::string identifier;
      if (lexer_.GetToken() == Lexer::L_LEFT_PARENTHESIS) {
        // List entry starts with "(", that's a special case of subdict without
        // name, we have to come up with the name ourselves.
        identifier = GetFreeSubdictName(dict);
      } else if (lexer_.GetToken() == Lexer::L_IDENTIFIER ||
                 lexer_.GetToken() == Lexer::L_STRING) {
        // Read identifier.
        identifier = lexer_.GetStringVal();
        lexer_.Next();
      } else {
        // Unexpected token, exiting parsing list.
        return;
      }
      // If there is "=" after identifier, that's key=value entry, read value.
      if (lexer_.GetToken() == Lexer::L_EQUAL) {
        lexer_.Next();
        ReadVal(dict, identifier);
      } else {
        // Otherwise it's subdict.
        ReadSubDict(dict, identifier);
      }
      // If next val is not comma, end of the list.
      if (lexer_.GetToken() != Lexer::L_COMMA) return;
      lexer_.Next();
    }
  }

  void EnsureToken(Lexer::TokenType type) {
    if (lexer_.GetToken() != type)
      lexer_.RaiseError("Expected token #" + std::to_string(type));
  }

  void ReadVal(OptionsDict* dict, const std::string& id) {
    if (lexer_.GetToken() == Lexer::L_FLOAT) {
      dict->Set<float>(id, lexer_.GetFloatVal());
    } else if (lexer_.GetToken() == Lexer::L_INTEGER) {
      dict->Set<int>(id, lexer_.GetIntVal());
    } else if (lexer_.GetToken() == Lexer::L_STRING) {
      // Strings may be:
      // * Single quoted: 'asdf'
      // * Double quoted: "asdf"
      // * Without quotes, if only alphanumeric and not "true" or "false".
      dict->Set<std::string>(id, lexer_.GetStringVal());
    } else if (lexer_.GetToken() == Lexer::L_IDENTIFIER) {
      if (lexer_.GetStringVal() == "true") {
        dict->Set<bool>(id, true);
      } else if (lexer_.GetStringVal() == "false") {
        dict->Set<bool>(id, false);
      } else {
        dict->Set<std::string>(id, lexer_.GetStringVal());
      }
    } else {
      lexer_.RaiseError("Expected value");
    }
    lexer_.Next();
  }

  void ReadSubDict(OptionsDict* dict, const std::string& identifier) {
    OptionsDict* new_dict = dict->AddSubdict(identifier);
    // If opening parentheses, read list of a subdict, otherwise list is empty,
    // so return immediately.
    if (lexer_.GetToken() == Lexer::L_LEFT_PARENTHESIS) {
      lexer_.Next();
      ParseList(new_dict);
      EnsureToken(Lexer::L_RIGHT_PARENTHESIS);
      lexer_.Next();
    }
  }

 private:
  Lexer lexer_;
};

}  // namespace

void OptionsDict::AddSubdictFromString(const std::string& str) {
  Parser parser(str);
  parser.ParseMain(this);
}

void OptionsDict::CheckAllOptionsRead(
    const std::string& path_from_parent) const {
  std::string s = path_from_parent.empty() ? "" : path_from_parent + '.';
  TypeDict<bool>::EnsureNoUnusedOptions("boolean", s);
  TypeDict<int>::EnsureNoUnusedOptions("integer", s);
  TypeDict<float>::EnsureNoUnusedOptions("floating point", s);
  TypeDict<std::string>::EnsureNoUnusedOptions("string", s);
  for (auto const& dict : subdicts_) {
    dict.second.CheckAllOptionsRead(s + dict.first);
  }
}

}  // namespace lczero

```

`src/utils/optionsdict.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <map>
#include <optional>
#include <string>
#include <unordered_map>
#include <vector>

#include "utils/exception.h"

namespace lczero {

template <typename T>
class TypeDict {
 protected:
  struct V {
    const T& Get() const {
      is_used_ = true;
      return value_;
    }
    T& Get() {
      is_used_ = true;
      return value_;
    }
    void Set(const T& v) {
      is_used_ = false;
      value_ = v;
    }
    bool IsSet() const { return is_used_; }

   private:
    mutable bool is_used_ = false;
    T value_;
  };
  void EnsureNoUnusedOptions(const std::string& type_name,
                             const std::string& prefix) const {
    for (auto const& option : dict_) {
      if (!option.second.IsSet()) {
        throw Exception("Unknown " + type_name + " option: " + prefix +
                        option.first);
      }
    }
  }

  const std::unordered_map<std::string, V>& dict() const { return dict_; }
  std::unordered_map<std::string, V>* mutable_dict() { return &dict_; }

 private:
  std::unordered_map<std::string, V> dict_;
};

class OptionId {
 public:
  OptionId(const char* long_flag, const char* uci_option, const char* help_text,
           const char short_flag = '\0')
      : long_flag_(long_flag),
        uci_option_(uci_option),
        help_text_(help_text),
        short_flag_(short_flag) {}

  OptionId(const OptionId& other) = delete;
  bool operator==(const OptionId& other) const { return this == &other; }

  const char* long_flag() const { return long_flag_; }
  const char* uci_option() const { return uci_option_; }
  const char* help_text() const { return help_text_; }
  char short_flag() const { return short_flag_; }

 private:
  const char* const long_flag_;
  const char* const uci_option_;
  const char* const help_text_;
  const char short_flag_;
};

class OptionsDict : TypeDict<bool>,
                    TypeDict<int>,
                    TypeDict<std::string>,
                    TypeDict<float> {
 public:
  explicit OptionsDict(const OptionsDict* parent = nullptr)
      : parent_(parent), aliases_{this} {}

  // e.g. dict.Get<int>("threads")
  // Returns value of given type. Throws exception if not found.
  template <typename T>
  T Get(const std::string& key) const;
  template <typename T>
  T Get(const OptionId& option_id) const;

  // Returns the own value of given type (doesn't fall back to querying parent).
  // Returns nullopt if doesn't exist.
  template <typename T>
  std::optional<T> OwnGet(const std::string& key) const;
  template <typename T>
  std::optional<T> OwnGet(const OptionId& option_id) const;

  // Checks whether the given key exists for given type.
  template <typename T>
  bool Exists(const std::string& key) const;
  template <typename T>
  bool Exists(const OptionId& option_id) const;

  // Checks whether the given key exists for given type, and throws an exception
  // if not.
  template <typename T>
  void EnsureExists(const OptionId& option_id) const;

  // Checks whether the given key exists for given type. Does not fall back to
  // check parents.
  template <typename T>
  bool OwnExists(const std::string& key) const;
  template <typename T>
  bool OwnExists(const OptionId& option_id) const;

  // Returns value of given type. Returns default if not found.
  template <typename T>
  T GetOrDefault(const std::string& key, const T& default_val) const;
  template <typename T>
  T GetOrDefault(const OptionId& option_id, const T& default_val) const;

  // Sets value for a given type.
  template <typename T>
  void Set(const std::string& key, const T& value);
  template <typename T>
  void Set(const OptionId& option_id, const T& value);

  // Get reference to assign value to.
  template <typename T>
  T& GetRef(const std::string& key);
  template <typename T>
  T& GetRef(const OptionId& option_id);

  // Returns true when the value is not set anywhere maybe except the root
  // dictionary;
  template <typename T>
  bool IsDefault(const std::string& key) const;
  template <typename T>
  bool IsDefault(const OptionId& option_id) const;

  // Returns subdictionary. Throws exception if doesn't exist.
  const OptionsDict& GetSubdict(const std::string& name) const;

  // Returns subdictionary. Throws exception if doesn't exist.
  OptionsDict* GetMutableSubdict(const std::string& name);

  // Creates subdictionary. Throws exception if already exists.
  OptionsDict* AddSubdict(const std::string& name);

  // Returns list of subdictionaries.
  std::vector<std::string> ListSubdicts() const;

  // Adds alias dictionary.
  void AddAliasDict(const OptionsDict* dict);

  // Creates options dict from string. Example of a string:
  // option1=1, option_two = "string val", subdict(option3=3.14)
  //
  // the sub dictionary is containing a parent pointer refering
  // back to this object. You need to ensure, that this object
  // is still in scope, when the parent pointer is used
  void AddSubdictFromString(const std::string& str);

  // Throws an exception for the first option in the dict that has not been read
  // to find syntax errors in options added using AddSubdictFromString.
  void CheckAllOptionsRead(const std::string& path_from_parent) const;

  bool HasSubdict(const std::string& name) const;

 private:
  static std::string GetOptionId(const OptionId& option_id) {
    return std::to_string(reinterpret_cast<intptr_t>(&option_id));
  }

  const OptionsDict* parent_ = nullptr;
  std::map<std::string, OptionsDict> subdicts_;
  // Dictionaries where to search for "own" parameters. By default contains only
  // this.
  std::vector<const OptionsDict*> aliases_;
};

template <typename T>
T OptionsDict::Get(const std::string& key) const {
  for (const auto* alias : aliases_) {
    const auto value = alias->OwnGet<T>(key);
    if (value) return *value;
  }
  if (parent_) return parent_->Get<T>(key);
  throw Exception("Key [" + key + "] was not set in options.");
}
template <typename T>
T OptionsDict::Get(const OptionId& option_id) const {
  return Get<T>(GetOptionId(option_id));
}
template <typename T>
std::optional<T> OptionsDict::OwnGet(const std::string& key) const {
  const auto& dict = TypeDict<T>::dict();
  auto iter = dict.find(key);
  if (iter != dict.end()) {
    return iter->second.Get();
  }
  return {};
}
template <typename T>
std::optional<T> OptionsDict::OwnGet(const OptionId& option_id) const {
  return OwnGet<T>(GetOptionId(option_id));
}

template <typename T>
bool OptionsDict::Exists(const std::string& key) const {
  for (const auto* alias : aliases_) {
    if (alias->OwnExists<T>(key)) return true;
  }
  return parent_ && parent_->Exists<T>(key);
}
template <typename T>
bool OptionsDict::Exists(const OptionId& option_id) const {
  return Exists<T>(GetOptionId(option_id));
}
template <typename T>
void OptionsDict::EnsureExists(const OptionId& option_id) const {
  if (!OwnExists<T>(option_id)) {
    throw Exception(std::string("The flag --") + option_id.long_flag() +
                    " must be specified.");
  }
}

template <typename T>
bool OptionsDict::OwnExists(const std::string& key) const {
  const auto& dict = TypeDict<T>::dict();
  auto iter = dict.find(key);
  return iter != dict.end();
}
template <typename T>
bool OptionsDict::OwnExists(const OptionId& option_id) const {
  return OwnExists<T>(GetOptionId(option_id));
}

template <typename T>
T OptionsDict::GetOrDefault(const std::string& key,
                            const T& default_val) const {
  for (const auto* alias : aliases_) {
    const auto value = alias->OwnGet<T>(key);
    if (value) return *value;
  }
  if (parent_) return parent_->GetOrDefault<T>(key, default_val);
  return default_val;
}
template <typename T>
T OptionsDict::GetOrDefault(const OptionId& option_id,
                            const T& default_val) const {
  return GetOrDefault<T>(GetOptionId(option_id), default_val);
}

template <typename T>
void OptionsDict::Set(const std::string& key, const T& value) {
  (*TypeDict<T>::mutable_dict())[key].Set(value);
}
template <typename T>
void OptionsDict::Set(const OptionId& option_id, const T& value) {
  Set<T>(GetOptionId(option_id), value);
}

template <typename T>
T& OptionsDict::GetRef(const std::string& key) {
  return (*TypeDict<T>::mutable_dict())[key].Get();
}
template <typename T>
T& OptionsDict::GetRef(const OptionId& option_id) {
  return GetRef<T>(GetOptionId(option_id));
}

template <typename T>
bool OptionsDict::IsDefault(const std::string& key) const {
  if (!parent_) return true;
  for (const auto* alias : aliases_) {
    if (alias->OwnExists<T>(key)) return false;
  }
  return parent_->IsDefault<T>(key);
}
template <typename T>
bool OptionsDict::IsDefault(const OptionId& option_id) const {
  return IsDefault<T>(GetOptionId(option_id));
}

}  // namespace lczero

```

`src/utils/optionsparser.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2019 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "optionsparser.h"

#include <iomanip>
#include <iostream>
#include <sstream>

#include "utils/commandline.h"
#include "utils/configfile.h"
#include "utils/logging.h"
#include "utils/string.h"

#if __has_include(<charconv>)
#include <charconv>
#else
#define NO_CHARCONV
#endif

namespace lczero {
namespace {
const int kHelpIndent = 15;
const int kUciLineIndent = 15;
const int kHelpWidth = 80;
}  // namespace

OptionsParser::Option::Option(const OptionId& id) : id_(id) {}

OptionsParser::OptionsParser() : values_(*defaults_.AddSubdict("values")) {}

std::vector<std::string> OptionsParser::ListOptionsUci() const {
  std::vector<std::string> result;
  for (const auto& iter : options_) {
    if (!iter->GetUciOption().empty() && !iter->hidden_) {
      result.emplace_back("option name " + iter->GetUciOption() + " " +
                          iter->GetOptionString(values_));
    }
  }
  return result;
}

void OptionsParser::SetUciOption(const std::string& name,
                                 const std::string& value,
                                 const std::string& context) {
  auto option = FindOptionByUciName(name);
  if (option) {
    option->SetValue(value, GetMutableOptions(context));
    return;
  }
  throw Exception("Unknown option: " + name);
}

void OptionsParser::HideOption(const OptionId& id) {
  const auto option = FindOptionById(id);
  if (option) option->hidden_ = true;
}

OptionsParser::Option* OptionsParser::FindOptionByLongFlag(
    const std::string& flag) const {
  for (const auto& val : options_) {
    auto longflg = val->GetLongFlag();
    if (flag == longflg || flag == ("no-" + longflg)) return val.get();
  }
  return nullptr;
}

OptionsParser::Option* OptionsParser::FindOptionByUciName(
    const std::string& name) const {
  for (const auto& val : options_) {
    if (StringsEqualIgnoreCase(val->GetUciOption(), name)) return val.get();
  }
  return nullptr;
}

OptionsParser::Option* OptionsParser::FindOptionById(const OptionId& id) const {
  for (const auto& val : options_) {
    if (id == val->GetId()) return val.get();
  }
  return nullptr;
}

OptionsDict* OptionsParser::GetMutableOptions(const std::string& context) {
  if (context == "") return &values_;
  auto* result = &values_;
  for (const auto& x : StrSplit(context, ".")) {
    result = result->GetMutableSubdict(x);
  }
  return result;
}

const OptionsDict& OptionsParser::GetOptionsDict(const std::string& context) {
  if (context == "") return values_;
  const auto* result = &values_;
  for (const auto& x : StrSplit(context, ".")) {
    result = &result->GetSubdict(x);
  }
  return *result;
}

bool OptionsParser::ProcessAllFlags() {
  return ProcessFlags(ConfigFile::Arguments()) &&
         ProcessFlags(CommandLine::Arguments());
}

bool OptionsParser::ProcessFlags(const std::vector<std::string>& args) {
  auto show_help = false;
  if (CommandLine::BinaryName().find("pro") != std::string::npos) {
    ShowHidden();
  }
  for (auto iter = args.begin(), end = args.end(); iter != end; ++iter) {
    std::string param = *iter;
    if (param == "--show-hidden") {
      ShowHidden();
      continue;
    }
    if (param == "-h" || param == "--help") {
      // Set a flag so that --show-hidden after --help works.
      show_help = true;
      continue;
    }

    if (param.substr(0, 2) == "--") {
      std::string context;
      param = param.substr(2);
      std::string value;
      auto pos = param.find('=');
      if (pos != std::string::npos) {
        value = param.substr(pos + 1);
        param = param.substr(0, pos);
      }
      pos = param.rfind('.');
      if (pos != std::string::npos) {
        context = param.substr(0, pos);
        param = param.substr(pos + 1);
      }
      bool processed = false;
      Option* option = FindOptionByLongFlag(param);
      if (option &&
          option->ProcessLongFlag(param, value, GetMutableOptions(context))) {
        processed = true;
      }
      if (!processed) {
        CERR << "Unknown command line flag: " << *iter << ".";
        CERR << "For help run:\n  " << CommandLine::BinaryName() << " --help";
        return false;
      }
      continue;
    }
    if (param.size() == 2 && param[0] == '-') {
      std::string value;
      bool processed = false;
      if (iter + 1 != end) {
        value = *(iter + 1);
      }
      for (auto& option : options_) {
        if (option->ProcessShortFlag(param[1], GetMutableOptions())) {
          processed = true;
          break;
        } else if (option->ProcessShortFlagWithValue(param[1], value,
                                                     GetMutableOptions())) {
          if (!value.empty()) ++iter;
          processed = true;
          break;
        }
      }
      if (!processed) {
        CERR << "Unknown command line flag: " << *iter << ".";
        CERR << "For help run:\n  " << CommandLine::BinaryName() << " --help";
        return false;
      }
      continue;
    }

    CERR << "Unknown command line argument: " << *iter << ".\n";
    CERR << "For help run:\n  " << CommandLine::BinaryName() << " --help";
    return false;
  }
  if (show_help) {
    ShowHelp();
    return false;
  }
  return true;
}

void OptionsParser::AddContext(const std::string& context) {
  values_.AddSubdict(context);
}

namespace {
std ::string FormatFlag(char short_flag, const std::string& long_flag,
                        const std::string& help,
                        const std::string& uci_option = {},
                        const std::string& def = {}) {
  std::ostringstream oss;
  oss << "  ";
  if (short_flag) {
    oss << '-' << short_flag;
  } else {
    oss << "  ";
  }
  if (short_flag && !long_flag.empty()) {
    oss << ",  ";
  } else {
    oss << "   ";
  }
  std::string long_flag_str = "";
  if (!short_flag && long_flag.empty()) {
    long_flag_str = "(uci parameter)";
  } else {
    long_flag_str = long_flag.empty() ? "" : "--" + long_flag;
  }
  oss << long_flag_str;
  auto help_lines = FlowText(help, kHelpWidth);
  bool is_first_line = true;
  for (const auto& line : help_lines) {
    if (is_first_line) {
      is_first_line = false;
      if (long_flag_str.size() < kHelpIndent - 7) {
        oss << std::string(kHelpIndent - 7 - long_flag_str.size(), ' ') << line
            << "\n";
        continue;
      }
      oss << "\n";
    }
    oss << std::string(kHelpIndent, ' ') << line << "\n";
  }
  if (!def.empty() || !uci_option.empty()) {
    oss << std::string(kUciLineIndent, ' ') << '[';
    if (!uci_option.empty()) oss << "UCI: " << uci_option;
    if (!uci_option.empty() && !def.empty()) oss << "  ";
    if (!def.empty()) oss << "DEFAULT: " << def;
    oss << "]\n";
  }
  oss << '\n';
  return oss.str();
}
}  // namespace

void OptionsParser::ShowHelp() const {
  std::cout << "Usage: " << CommandLine::BinaryName() << " [<mode>] [flags...]"
            << std::endl;

  std::cout << "\nAvailable modes. A help for a mode: "
            << CommandLine::BinaryName() << " <mode> --help\n";
  for (const auto& mode : CommandLine::GetModes()) {
    std::cout << "  " << std::setw(10) << std::left << mode.first << " "
              << mode.second << std::endl;
  }

  std::cout << "\nAllowed command line flags for current mode:\n";
  std::cout << FormatFlag('h', "help", "Show help and exit.");
  std::cout << FormatFlag('\0', "show-hidden",
                          "Show hidden options. Use with --help.");
  for (const auto& option : options_) {
    if (!option->hidden_) std::cout << option->GetHelp(defaults_);
  }

  auto contexts = values_.ListSubdicts();
  if (!contexts.empty()) {
    std::cout << "\nFlags can be defined per context (one of: "
              << StrJoin(contexts, ", ") << "), for example:\n";
    std::cout << "       --" << contexts[0] << '.'
              << options_.back()->GetLongFlag() << "=(value)\n";
  }
}

void OptionsParser::ShowHidden() const {
  for (const auto& option : options_) option->hidden_ = false;
}

/////////////////////////////////////////////////////////////////
// StringOption
/////////////////////////////////////////////////////////////////

StringOption::StringOption(const OptionId& id) : Option(id) {}

void StringOption::SetValue(const std::string& value, OptionsDict* dict) {
  SetVal(dict, value);
}

bool StringOption::ProcessLongFlag(const std::string& flag,
                                   const std::string& value,
                                   OptionsDict* dict) {
  if (flag == GetLongFlag()) {
    SetVal(dict, value);
    return true;
  }
  return false;
}

bool StringOption::ProcessShortFlagWithValue(char flag,
                                             const std::string& value,
                                             OptionsDict* dict) {
  if (flag == GetShortFlag()) {
    SetVal(dict, value);
    return true;
  }
  return false;
}

std::string StringOption::GetHelp(const OptionsDict& dict) const {
  return FormatFlag(GetShortFlag(), GetLongFlag() + "=STRING", GetHelpText(),
                    GetUciOption(), GetVal(dict));
}

std::string StringOption::GetOptionString(const OptionsDict& dict) const {
  return "type string default " + GetVal(dict);
}

std::string StringOption::GetVal(const OptionsDict& dict) const {
  return dict.Get<ValueType>(GetId());
}

void StringOption::SetVal(OptionsDict* dict, const ValueType& val) const {
  dict->Set<ValueType>(GetId(), val);
}

/////////////////////////////////////////////////////////////////
// IntOption
/////////////////////////////////////////////////////////////////

IntOption::IntOption(const OptionId& id, int min, int max)
    : Option(id), min_(min), max_(max) {}

void IntOption::SetValue(const std::string& value, OptionsDict* dict) {
  SetVal(dict, ValidateIntString(value));
}

bool IntOption::ProcessLongFlag(const std::string& flag,
                                const std::string& value, OptionsDict* dict) {
  if (flag == GetLongFlag()) {
    SetVal(dict, ValidateIntString(value));
    return true;
  }
  return false;
}

bool IntOption::ProcessShortFlagWithValue(char flag, const std::string& value,
                                          OptionsDict* dict) {
  if (flag == GetShortFlag()) {
    SetVal(dict, ValidateIntString(value));
    return true;
  }
  return false;
}

std::string IntOption::GetHelp(const OptionsDict& dict) const {
  std::string long_flag = GetLongFlag();
  if (!long_flag.empty()) {
    long_flag += "=" + std::to_string(min_) + ".." + std::to_string(max_);
  }
  return FormatFlag(GetShortFlag(), long_flag, GetHelpText(), GetUciOption(),
                    std::to_string(GetVal(dict)) +
                        "  MIN: " + std::to_string(min_) +
                        "  MAX: " + std::to_string(max_));
}

std::string IntOption::GetOptionString(const OptionsDict& dict) const {
  return "type spin default " + std::to_string(GetVal(dict)) + " min " +
         std::to_string(min_) + " max " + std::to_string(max_);
}

IntOption::ValueType IntOption::GetVal(const OptionsDict& dict) const {
  return dict.Get<ValueType>(GetId());
}

void IntOption::SetVal(OptionsDict* dict, const ValueType& val) const {
  if (val < min_ || val > max_) {
    std::ostringstream buf;
    buf << "Flag '--" << GetLongFlag() << "' must be between " << min_
        << " and " << max_ << ".";
    throw Exception(buf.str());
  }
  dict->Set<ValueType>(GetId(), val);
}

#ifndef NO_CHARCONV
int IntOption::ValidateIntString(const std::string& val) const {
  int result;
  const auto end = val.data() + val.size();
  auto [ptr, err] = std::from_chars(val.data(), end, result);
  if (err == std::errc::invalid_argument) {
    throw Exception("Flag '--" + GetLongFlag() + "' has an invalid format.");
  } else if (err == std::errc::result_out_of_range) {
    throw Exception("Flag '--" + GetLongFlag() + "' is out of range.");
  } else if (ptr != end) {
    throw Exception("Flag '--" + GetLongFlag() + "' has trailing characters.");
  } else {
    return result;
  }
}
#else
int IntOption::ValidateIntString(const std::string& val) const {
  char* end;
  errno = 0;
  int result = std::strtol(val.c_str(), &end, 10);
  if (errno == ERANGE) {
    throw Exception("Flag '--" + GetLongFlag() + "' is out of range.");
  } else if (val.length() == 0 || *end != '\0') {
    throw Exception("Flag '--" + GetLongFlag() + "' value is invalid.");
  } else {
    return result;
  }
}
#endif

/////////////////////////////////////////////////////////////////
// FloatOption
/////////////////////////////////////////////////////////////////

FloatOption::FloatOption(const OptionId& id, float min, float max)
    : Option(id), min_(min), max_(max) {}

void FloatOption::SetValue(const std::string& value, OptionsDict* dict) {
  try {
    SetVal(dict, std::stof(value));
  } catch (std::invalid_argument&) {
    throw Exception("invalid value " + value);
  } catch (const std::out_of_range&) {
    throw Exception("out of range value " + value);
  }
}

bool FloatOption::ProcessLongFlag(const std::string& flag,
                                  const std::string& value, OptionsDict* dict) {
  if (flag == GetLongFlag()) {
    try {
      SetVal(dict, std::stof(value));
    } catch (std::invalid_argument&) {
      throw Exception("invalid value " + value);
    } catch (const std::out_of_range&) {
      throw Exception("out of range value " + value);
    }
    return true;
  }
  return false;
}

bool FloatOption::ProcessShortFlagWithValue(char flag, const std::string& value,
                                            OptionsDict* dict) {
  if (flag == GetShortFlag()) {
    try {
      SetVal(dict, std::stof(value));
    } catch (std::invalid_argument&) {
      throw Exception("invalid value " + value);
    } catch (const std::out_of_range&) {
      throw Exception("out of range value " + value);
    }
    return true;
  }
  return false;
}

std::string FloatOption::GetHelp(const OptionsDict& dict) const {
  std::string long_flag = GetLongFlag();
  if (!long_flag.empty()) {
    std::ostringstream oss;
    oss << std::fixed << std::setprecision(2) << min_ << ".." << max_;
    long_flag += "=" + oss.str();
  }
  std::ostringstream oss;
  oss << std::fixed << std::setprecision(2) << GetVal(dict) << "  MIN: " << min_
      << "  MAX: " << max_;
  return FormatFlag(GetShortFlag(), long_flag, GetHelpText(), GetUciOption(),
                    oss.str());
}

std::string FloatOption::GetOptionString(const OptionsDict& dict) const {
  return "type string default " + std::to_string(GetVal(dict));
}

FloatOption::ValueType FloatOption::GetVal(const OptionsDict& dict) const {
  return dict.Get<ValueType>(GetId());
}

void FloatOption::SetVal(OptionsDict* dict, const ValueType& val) const {
  if (val < min_ || val > max_) {
    std::ostringstream buf;
    buf << "Flag '--" << GetLongFlag() << "' must be between " << min_
        << " and " << max_ << ".";
    throw Exception(buf.str());
  }
  dict->Set<ValueType>(GetId(), val);
}

/////////////////////////////////////////////////////////////////
// BoolOption
/////////////////////////////////////////////////////////////////

BoolOption::BoolOption(const OptionId& id) : Option(id) {}

void BoolOption::SetValue(const std::string& value, OptionsDict* dict) {
  ValidateBoolString(value);
  SetVal(dict, value == "true");
}

bool BoolOption::ProcessLongFlag(const std::string& flag,
                                 const std::string& value, OptionsDict* dict) {
  if (flag == "no-" + GetLongFlag()) {
    SetVal(dict, false);
    return true;
  }
  if (flag == GetLongFlag() && value.empty()) {
    SetVal(dict, true);
    return true;
  }

  ValidateBoolString(value);

  if (flag == GetLongFlag()) {
    SetVal(dict, value.empty() || (value != "false"));
    return true;
  }
  return false;
}

bool BoolOption::ProcessShortFlag(char flag, OptionsDict* dict) {
  if (flag == GetShortFlag()) {
    SetVal(dict, !GetVal(*dict));
    return true;
  }
  return false;
}

std::string BoolOption::GetHelp(const OptionsDict& dict) const {
  std::string long_flag = GetLongFlag();
  if (!long_flag.empty()) {
    long_flag = "[no-]" + long_flag;
  }
  return FormatFlag(GetShortFlag(), long_flag, GetHelpText(), GetUciOption(),
                    GetVal(dict) ? "true" : "false");
}

std::string BoolOption::GetOptionString(const OptionsDict& dict) const {
  return "type check default " + std::string(GetVal(dict) ? "true" : "false");
}

BoolOption::ValueType BoolOption::GetVal(const OptionsDict& dict) const {
  return dict.Get<ValueType>(GetId());
}

void BoolOption::SetVal(OptionsDict* dict, const ValueType& val) const {
  dict->Set<ValueType>(GetId(), val);
}

void BoolOption::ValidateBoolString(const std::string& val) {
  if (val != "true" && val != "false") {
    std::ostringstream buf;
    buf << "Flag '--" << GetLongFlag() << "' must be either "
        << "'true' or 'false'.";
    throw Exception(buf.str());
  }
}

/////////////////////////////////////////////////////////////////
// ChoiceOption
/////////////////////////////////////////////////////////////////

ChoiceOption::ChoiceOption(const OptionId& id,
                           const std::vector<std::string>& choices)
    : Option(id), choices_(choices) {}

void ChoiceOption::SetValue(const std::string& value, OptionsDict* dict) {
  SetVal(dict, value);
}

bool ChoiceOption::ProcessLongFlag(const std::string& flag,
                                   const std::string& value,
                                   OptionsDict* dict) {
  if (flag == GetLongFlag()) {
    SetVal(dict, value);
    return true;
  }
  return false;
}

bool ChoiceOption::ProcessShortFlagWithValue(char flag,
                                             const std::string& value,
                                             OptionsDict* dict) {
  if (flag == GetShortFlag()) {
    SetVal(dict, value);
    return true;
  }
  return false;
}

std::string ChoiceOption::GetHelp(const OptionsDict& dict) const {
  std::string values;
  for (const auto& choice : choices_) {
    if (!values.empty()) values += ',';
    values += choice;
  }
  return FormatFlag(GetShortFlag(), GetLongFlag() + "=CHOICE", GetHelpText(),
                    GetUciOption(), GetVal(dict) + "  VALUES: " + values);
}

std::string ChoiceOption::GetOptionString(const OptionsDict& dict) const {
  std::string res = "type combo default " + GetVal(dict);
  for (const auto& choice : choices_) {
    res += " var " + choice;
  }
  return res;
}

std::string ChoiceOption::GetVal(const OptionsDict& dict) const {
  return dict.Get<ValueType>(GetId());
}

void ChoiceOption::SetVal(OptionsDict* dict, const ValueType& val) const {
  bool valid = false;
  std::string choice_string;
  for (const auto& choice : choices_) {
    choice_string += " " + choice;
    if (val == choice) {
      valid = true;
      break;
    }
  }
  if (!valid) {
    std::ostringstream buf;
    buf << "Flag '--" << GetLongFlag() << "' must be one of the "
        << "following values:" << choice_string << ".";
    throw Exception(buf.str());
  }
  dict->Set<ValueType>(GetId(), val);
}

}  // namespace lczero

```

`src/utils/optionsparser.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <map>
#include <memory>
#include <string>
#include <vector>

#include "utils/exception.h"
#include "utils/optionsdict.h"

namespace lczero {

class OptionsParser {
 public:
  OptionsParser();

  class Option {
   public:
    Option(const OptionId& id);
    virtual ~Option(){};
    // Set value from string.
    virtual void SetValue(const std::string& value, OptionsDict* dict) = 0;

   protected:
    const OptionId& GetId() const { return id_; }
    std::string GetUciOption() const { return id_.uci_option(); }
    std::string GetHelpText() const { return id_.help_text(); }
    std::string GetLongFlag() const { return id_.long_flag(); }
    char GetShortFlag() const { return id_.short_flag(); }

   private:
    virtual std::string GetOptionString(const OptionsDict& dict) const = 0;
    virtual bool ProcessLongFlag(const std::string& /*flag*/,
                                 const std::string& /*value*/,
                                 OptionsDict* /*dict*/) {
      return false;
    }
    virtual bool ProcessShortFlag(char /*flag*/, OptionsDict* /*dict*/) {
      return false;
    }
    virtual bool ProcessShortFlagWithValue(char /*flag*/,
                                           const std::string& /*value*/,
                                           OptionsDict* /*dict*/) {
      return false;
    }
    virtual std::string GetHelp(const OptionsDict& dict) const = 0;

    const OptionId& id_;
    bool hidden_ = false;
    friend class OptionsParser;
  };

  // Add an option to the list of available options (from command line flags
  // or UCI params)
  // Usage:
  // options->Add<StringOption>(name, func, long_flag, short_flag) = def_val;
  template <typename Option, typename... Args>
  typename Option::ValueType& Add(Args&&... args) {
    options_.emplace_back(
        std::make_unique<Option>(std::forward<Args>(args)...));
    return defaults_.GetRef<typename Option::ValueType>(
        options_.back()->GetId());
  }

  // Returns list of options in UCI format.
  std::vector<std::string> ListOptionsUci() const;

  // Set the UCI option from string value.
  void SetUciOption(const std::string& name, const std::string& value,
                    const std::string& context = "");
  // Hide this option from help and UCI.
  void HideOption(const OptionId& id);
  // Processes all flags from the command line and an optional
  // configuration file. Returns false if there is an invalid flag.
  bool ProcessAllFlags();
  // Processes either the command line or configuration file flags.
  bool ProcessFlags(const std::vector<std::string>& args);

  // Get the options dict for given context.
  const OptionsDict& GetOptionsDict(const std::string& context = {});
  // Gets the dictionary for given context which caller can modify.
  OptionsDict* GetMutableOptions(const std::string& context = {});
  // Gets the mutable list of default options.
  OptionsDict* GetMutableDefaultsOptions() { return &defaults_; }
  // Adds a subdictionary for a given context.
  void AddContext(const std::string&);
  // Prints help to std::cout.
  void ShowHelp() const;

 private:
  // Make all hidden options visible.
  void ShowHidden() const;
  // Returns an option based on the long flag.
  Option* FindOptionByLongFlag(const std::string& flag) const;
  // Returns an option based by its uci name.
  Option* FindOptionByUciName(const std::string& name) const;
  // Returns an option based by its id.
  Option* FindOptionById(const OptionId& id) const;

  std::vector<std::unique_ptr<Option>> options_;
  OptionsDict defaults_;
  OptionsDict& values_;
};

class StringOption : public OptionsParser::Option {
 public:
  using ValueType = std::string;
  StringOption(const OptionId& id);

  void SetValue(const std::string& value, OptionsDict* dict) override;

 private:
  std::string GetOptionString(const OptionsDict& dict) const override;
  bool ProcessLongFlag(const std::string& flag, const std::string& value,
                       OptionsDict* dict) override;
  std::string GetHelp(const OptionsDict& dict) const override;
  bool ProcessShortFlagWithValue(char flag, const std::string& value,
                                 OptionsDict* dict) override;

  ValueType GetVal(const OptionsDict&) const;
  void SetVal(OptionsDict* dict, const ValueType& val) const;
};

class IntOption : public OptionsParser::Option {
 public:
  using ValueType = int;
  IntOption(const OptionId& id, int min, int max);

  void SetValue(const std::string& value, OptionsDict* dict) override;

 private:
  std::string GetOptionString(const OptionsDict& dict) const override;
  bool ProcessLongFlag(const std::string& flag, const std::string& value,
                       OptionsDict* dict) override;
  std::string GetHelp(const OptionsDict& dict) const override;
  bool ProcessShortFlagWithValue(char flag, const std::string& value,
                                 OptionsDict* dict) override;

  ValueType GetVal(const OptionsDict&) const;
  void SetVal(OptionsDict* dict, const ValueType& val) const;
  int ValidateIntString(const std::string& val) const;

  int min_;
  int max_;
};

class FloatOption : public OptionsParser::Option {
 public:
  using ValueType = float;
  FloatOption(const OptionId& id, float min, float max);

  void SetValue(const std::string& value, OptionsDict* dict) override;

 private:
  std::string GetOptionString(const OptionsDict& dict) const override;
  bool ProcessLongFlag(const std::string& flag, const std::string& value,
                       OptionsDict* dict) override;
  std::string GetHelp(const OptionsDict& dict) const override;
  bool ProcessShortFlagWithValue(char flag, const std::string& value,
                                 OptionsDict* dict) override;

  ValueType GetVal(const OptionsDict&) const;
  void SetVal(OptionsDict* dict, const ValueType& val) const;

  float min_;
  float max_;
};

class BoolOption : public OptionsParser::Option {
 public:
  using ValueType = bool;
  BoolOption(const OptionId& id);

  void SetValue(const std::string& value, OptionsDict* dict) override;

 private:
  std::string GetOptionString(const OptionsDict& dict) const override;
  bool ProcessLongFlag(const std::string& flag, const std::string& value,
                       OptionsDict* dict) override;
  std::string GetHelp(const OptionsDict& dict) const override;
  bool ProcessShortFlag(char flag, OptionsDict* dict) override;

  ValueType GetVal(const OptionsDict&) const;
  void SetVal(OptionsDict* dict, const ValueType& val) const;
  void ValidateBoolString(const std::string& val);
};

class ChoiceOption : public OptionsParser::Option {
 public:
  using ValueType = std::string;
  ChoiceOption(const OptionId& id, const std::vector<std::string>& choices);

  void SetValue(const std::string& value, OptionsDict* dict) override;

 private:
  std::string GetOptionString(const OptionsDict& dict) const override;
  bool ProcessLongFlag(const std::string& flag, const std::string& value,
                       OptionsDict* dict) override;
  std::string GetHelp(const OptionsDict& dict) const override;
  bool ProcessShortFlagWithValue(char flag, const std::string& value,
                                 OptionsDict* dict) override;

  ValueType GetVal(const OptionsDict&) const;
  void SetVal(OptionsDict* dict, const ValueType& val) const;

  std::vector<std::string> choices_;
};

}  // namespace lczero

```

`src/utils/optionsparser_test.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.
*/

#include "utils/optionsparser.h"
#include <gtest/gtest.h>
#include <iostream>

namespace lczero {

TEST(OptionsParser, CheckInvalidOption) {
  OptionsParser options;
  const OptionId id{"this-is-a-valid-option", "this-is-a-valid-option", "help",
                    'a'};
  options.Add<StringOption>(id) = "";
  EXPECT_NO_THROW(
      options.SetUciOption("this-is-a-valid-option", "valid-value"));
  EXPECT_THROW(options.SetUciOption("this-is-an-invalid-option", "0"),
               Exception);
}

TEST(OptionsParser, IntOptionCheckValueConstraints) {
  OptionsParser options;
  const OptionId id{"int-test-a", "int-test-a", "help", 'a'};
  options.Add<IntOption>(id, 25, 75) = 50;

  EXPECT_NO_THROW(options.SetUciOption("int-test-a", "25"));
  EXPECT_NO_THROW(options.SetUciOption("int-test-a", "50"));
  EXPECT_NO_THROW(options.SetUciOption("int-test-a", "75"));
  EXPECT_THROW(options.SetUciOption("int-test-a", "0"), Exception);
  EXPECT_THROW(options.SetUciOption("int-test-a", "100"), Exception);
}

TEST(OptionsParser, FloatOptionCheckValueConstraints) {
  OptionsParser options;
  const OptionId id{"float-test-a", "float-test-a", "help", 'a'};
  options.Add<FloatOption>(id, 25.0f, 75.0f) = 50.0f;

  EXPECT_NO_THROW(options.SetUciOption("float-test-a", "25.0"));
  EXPECT_NO_THROW(options.SetUciOption("float-test-a", "50.0"));
  EXPECT_NO_THROW(options.SetUciOption("float-test-a", "75.0"));
  EXPECT_THROW(options.SetUciOption("float-test-a", "0.0"), Exception);
  EXPECT_THROW(options.SetUciOption("float-test-a", "100.0"), Exception);
}

TEST(OptionsParser, BoolOptionsCheckValueConstraints) {
  OptionsParser options;
  const OptionId id{"bool-test-a", "bool-test-a", "help", 'a'};
  options.Add<BoolOption>(id) = false;

  EXPECT_NO_THROW(options.SetUciOption("bool-test-a", "true"));
  EXPECT_NO_THROW(options.SetUciOption("bool-test-a", "false"));
  EXPECT_THROW(options.SetUciOption("bool-test-a", "leela"), Exception);
}

TEST(OptionsParser, ChoiceOptionCheckValueConstraints) {
  OptionsParser options;
  const OptionId id{"choice-test-a", "choice-test-a", "help", 'a'};
  std::vector<std::string> choices;
  choices.push_back("choice-a");
  choices.push_back("choice-b");
  choices.push_back("choice-c");
  options.Add<ChoiceOption>(id, choices) = "choice-a";

  EXPECT_NO_THROW(options.SetUciOption("choice-test-a", "choice-a"));
  EXPECT_NO_THROW(options.SetUciOption("choice-test-a", "choice-b"));
  EXPECT_NO_THROW(options.SetUciOption("choice-test-a", "choice-c"));
  EXPECT_THROW(options.SetUciOption("choice-test-a", "choice-d"), Exception);
}

}  // namespace lczero

int main(int argc, char** argv) {
  ::testing::InitGoogleTest(&argc, argv);
  return RUN_ALL_TESTS();
}

```

`src/utils/protomessage.cc`:

```cc
#include "utils/protomessage.h"

#include <cstdint>

#include "utils/exception.h"

namespace lczero {

namespace {
uint64_t ReadVarInt(const std::uint8_t** iter, const std::uint8_t* const end) {
  uint64_t res = 0;
  uint64_t multiplier = 1;
  while (*iter < end) {
    std::uint8_t x = **iter;
    ++*iter;
    res += (x & 0x7f) * multiplier;
    if ((x & 0x80) == 0) return res;
    multiplier *= 0x80;
  }
  throw Exception("The file seems truncated.");
}

void CheckOutOfBounds(const std::uint8_t* const iter, size_t size,
                      const std::uint8_t* const end) {
  if (iter + size > end) {
    throw Exception("The file is truncated.");
  }
}

uint64_t ReadFixed(const std::uint8_t** iter, size_t size,
                   const std::uint8_t* const end) {
  CheckOutOfBounds(*iter, size, end);
  uint64_t multiplier = 1;
  uint64_t result = 0;
  for (; size != 0; --size, multiplier *= 256, ++*iter) {
    result += multiplier * **iter;
  }
  return result;
}

void WriteFixed(uint64_t value, size_t size, std::string* out) {
  out->reserve(out->size() + size);
  for (size_t i = 0; i < size; ++i) {
    out->push_back(static_cast<char>(static_cast<uint8_t>(value)));
    value /= 256;
  }
}

// // Kept for serialization part.
std::string EncodeVarInt(std::uint64_t val) {
  std::string res;
  while (true) {
    char c = (val & 0x7f);
    val >>= 7;
    if (val) c |= 0x80;
    res += c;
    if (!val) return res;
  }
}

}  // namespace

void ProtoMessage::ParseFromString(std::string_view str) {
  Clear();
  return MergeFromString(str);
}

void ProtoMessage::MergeFromString(std::string_view str) {
  const std::uint8_t* iter = reinterpret_cast<const std::uint8_t*>(str.data());
  const std::uint8_t* const end = iter + str.size();
  while (iter < end) {
    uint64_t wire_field_id = ReadVarInt(&iter, end);
    uint64_t field_id = wire_field_id >> 3;
    switch (wire_field_id & 0x7) {
      case 0:
        // Varint field, so read one more varint.
        SetVarInt(field_id, ReadVarInt(&iter, end));
        break;
      case 1:
        // Fixed64, read 8 bytes.
        SetInt64(field_id, ReadFixed(&iter, 8, end));
        break;
      case 2: {
        // String/submessage. Varint length and then buffer of that length.
        size_t size = ReadVarInt(&iter, end);
        CheckOutOfBounds(iter, size, end);
        SetString(field_id,
                  std::string_view(reinterpret_cast<const char*>(iter), size));
        iter += size;
        break;
      }
      case 5:
        // Fixed32, read 4 bytes.
        SetInt32(field_id, ReadFixed(&iter, 4, end));
        break;
      default:
        throw Exception("The file seems to be unparseable.");
    }
  }
}

void ProtoMessage::AppendVarInt(int field_id, std::uint64_t value,
                                std::string* out) const {
  *out += EncodeVarInt(field_id << 3);
  *out += EncodeVarInt(value);
}
void ProtoMessage::AppendInt64(int field_id, std::uint64_t value,
                               std::string* out) const {
  *out += EncodeVarInt(1 + (field_id << 3));
  WriteFixed(value, 8, out);
}
void ProtoMessage::AppendInt32(int field_id, std::uint32_t value,
                               std::string* out) const {
  *out += EncodeVarInt(5 + (field_id << 3));
  WriteFixed(value, 4, out);
}

void ProtoMessage::AppendString(int field_id, std::string_view value,
                                std::string* out) const {
  *out += EncodeVarInt(2 + (field_id << 3));
  *out += EncodeVarInt(value.size());
  *out += value;
}

}  // namespace lczero
```

`src/utils/protomessage.h`:

```h
#pragma once

#include <cstdint>
#include <cstring>
#include <functional>
#include <map>
#include <string>
#include <string_view>
#include <array>
#include <vector>

// Undef g++ macros to ged rid of warnings.
#ifdef minor
#undef minor
#endif
#ifdef major
#undef major
#endif

namespace lczero {

class ProtoMessage {
 public:
  virtual ~ProtoMessage() {}
  virtual void Clear() = 0;

  void ParseFromString(std::string_view);
  void MergeFromString(std::string_view);
  virtual std::string OutputAsString() const = 0;

 protected:
  template <class To, class From>
  static To bit_cast(From from) {
    if constexpr (std::is_same_v<From, To>) {
      return from;
    } else {
      To to;
      std::memcpy(&to, &from, sizeof(to));
      return to;
    }
  }

  void AppendVarInt(int field_id, std::uint64_t value, std::string* out) const;
  void AppendInt64(int field_id, std::uint64_t value, std::string* out) const;
  void AppendInt32(int field_id, std::uint32_t value, std::string* out) const;
  void AppendString(int field_id, std::string_view value,
                    std::string* out) const;

 private:
  virtual void SetVarInt(int /* field_id */, uint64_t /* value */) {}
  virtual void SetInt64(int /* field_id */, uint64_t /* value */) {}
  virtual void SetInt32(int /* field_id */, uint32_t /* value */) {}
  virtual void SetString(int /* field_id */, std::string_view /* value */) {}
};

}  // namespace lczero
```

`src/utils/random.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "random.h"
#include <random>

namespace lczero {

Random::Random() : gen_(std::random_device()()) {}

Random& Random::Get() {
  static Random rand;
  return rand;
}

int Random::GetInt(int min, int max) {
  Mutex::Lock lock(mutex_);
  std::uniform_int_distribution<> dist(min, max);
  return dist(gen_);
}

bool Random::GetBool() { return GetInt(0, 1) != 0; }

double Random::GetDouble(double maxval) {
  Mutex::Lock lock(mutex_);
  std::uniform_real_distribution<> dist(0.0, maxval);
  return dist(gen_);
}

float Random::GetFloat(float maxval) {
  Mutex::Lock lock(mutex_);
  std::uniform_real_distribution<> dist(0.0, maxval);
  return dist(gen_);
}

std::string Random::GetString(int length) {
  std::string result;
  for (int i = 0; i < length; ++i) {
    result += 'a' + GetInt(0, 25);
  }
  return result;
}

double Random::GetGamma(double alpha, double beta) {
  Mutex::Lock lock(mutex_);
  std::gamma_distribution<double> dist(alpha, beta);
  return dist(gen_);
}

}  // namespace lczero

```

`src/utils/random.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <algorithm>
#include <random>
#include <string>
#include "utils/mutex.h"

namespace lczero {

class Random {
 public:
  static Random& Get();
  double GetDouble(double max_val);
  float GetFloat(float max_val);
  double GetGamma(double alpha, double beta);
  // Both sides are included.
  int GetInt(int min, int max);
  std::string GetString(int length);
  bool GetBool();
  template <class RandomAccessIterator>
  void Shuffle(RandomAccessIterator s, RandomAccessIterator e);

 private:
  Random();

  Mutex mutex_;
  std::mt19937 gen_ GUARDED_BY(mutex_);
};

template <class RandomAccessIterator>
void Random::Shuffle(RandomAccessIterator s, RandomAccessIterator e) {
  Mutex::Lock lock(mutex_);
  std::shuffle(s, e, gen_);
}

}  // namespace lczero

```

`src/utils/smallarray.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <memory>

namespace lczero {

// Non resizeable array which can contain up to 255 elements.
template <typename T>
class SmallArray {
 public:
  SmallArray() = delete;
  SmallArray(size_t size) : size_(size), data_(std::make_unique<T[]>(size)) {}
  SmallArray(SmallArray&&);  // TODO implement when needed
  T& operator[](int idx) { return data_[idx]; }
  const T& operator[](int idx) const { return data_[idx]; }
  int size() const { return size_; }

 private:
  unsigned char size_;
  std::unique_ptr<T[]> data_;
};

}  // namespace lczero

```

`src/utils/string.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "utils/string.h"

#include <algorithm>
#include <cctype>
#include <sstream>
#include <vector>

namespace lczero {

std::string StrJoin(const std::vector<std::string>& strings,
                    const std::string& delim) {
  std::string res;
  for (const auto& str : strings) {
    if (!res.empty()) res += delim;
    res += str;
  }
  return res;
}

std::vector<std::string> StrSplitAtWhitespace(const std::string& str) {
  std::vector<std::string> result;
  std::istringstream iss(str);
  std::string tmp;
  while (iss >> tmp) result.emplace_back(std::move(tmp));
  return result;
}

std::vector<std::string> StrSplit(const std::string& str,
                                  const std::string& delim) {
  std::vector<std::string> result;
  for (std::string::size_type pos = 0, next = 0; pos != std::string::npos;
       pos = next) {
    next = str.find(delim, pos);
    result.push_back(str.substr(pos, next - pos));
    if (next != std::string::npos) next += delim.size();
  }
  return result;
}

std::vector<int> ParseIntList(const std::string& str) {
  std::vector<int> result;
  for (const auto& x : StrSplit(str, ",")) {
    result.push_back(std::stoi(x));
  }
  return result;
}

std::string LeftTrim(std::string str) {
  const auto it = std::find_if(str.begin(), str.end(),
                         [](int ch) { return !std::isspace(ch); });
  str.erase(str.begin(), it);
  return str;
}

std::string RightTrim(std::string str) {
  auto it = std::find_if(str.rbegin(), str.rend(),
                         [](int ch) { return !std::isspace(ch); });
  str.erase(it.base(), str.end());
  return str;
}

std::string Trim(std::string str) {
  return LeftTrim(RightTrim(std::move(str)));
}

bool StringsEqualIgnoreCase(const std::string& a, const std::string& b) {
  return std::equal(a.begin(), a.end(), b.begin(), b.end(), [](char a, char b) {
    return std::tolower(a) == std::tolower(b);
  });
}

std::vector<std::string> FlowText(const std::string& src, size_t width) {
  std::vector<std::string> result;
  auto paragraphs = StrSplit(src, "\n");
  for (const auto& paragraph : paragraphs) {
    result.emplace_back();
    auto words = StrSplit(paragraph, " ");
    for (const auto& word : words) {
      if (result.back().empty()) {
        // First word in line, always add.
      } else if (result.back().size() + word.size() + 1 > width) {
        // The line doesn't have space for a new word.
        result.emplace_back();
      } else {
        // Appending to the current line.
        result.back() += " ";
      }
      result.back() += word;
    }
  }
  return result;
}

}  // namespace lczero

```

`src/utils/string.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <string>
#include <vector>

namespace lczero {

// Joins strings using @delim as delimiter.
std::string StrJoin(const std::vector<std::string>& strings,
                    const std::string& delim = " ");

// Splits strings at whitespace.
std::vector<std::string> StrSplitAtWhitespace(const std::string& str);

// Split string by delimiter.
std::vector<std::string> StrSplit(const std::string& str,
                                  const std::string& delim);

// Parses comma-separated list of integers.
std::vector<int> ParseIntList(const std::string& str);

// Trims a string of whitespace from the start.
std::string LeftTrim(std::string str);

// Trims a string of whitespace from the end.
std::string RightTrim(std::string str);

// Trims a string of whitespace from both ends.
std::string Trim(std::string str);

// Returns whether strings are equal, ignoring case.
bool StringsEqualIgnoreCase(const std::string& a, const std::string& b);

// Flow text into lines of width up to @width.
std::vector<std::string> FlowText(const std::string& src, size_t width);

}  // namespace lczero

```

`src/utils/transpose.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018-2020 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <cassert>
#include <numeric>
#include <vector>

namespace lczero {

// Transposes flattened tensor from @from into @to. @to must have space for
// from.size() elements.
// @dims -- Dimensions of @from tensor. For example, {120, 60, 3, 3}
// @order -- New-to-old dimension index mapping. For example {3, 2, 0, 1}
template <class T>
void TransposeTensor(const std::vector<int>& dims, std::vector<int> order,
                     const std::vector<T> from, T* to) {
  assert(from.size() == std::accumulate(dims.begin(), dims.end(), 1u,
                                        std::multiplies<size_t>()));
  if (order.empty()) {
    for (size_t i = 0; i < dims.size(); ++i)
      order.push_back(dims.size() - i - 1);
  }
  std::vector<int> cur_idx(dims.size());
  for (size_t _ = 0; _ < from.size(); ++_) {
    size_t from_idx = 0;
    for (int i : order) {
      from_idx *= dims[i];
      from_idx += cur_idx[i];
    }
    *to++ = from[from_idx];
    for (int i = static_cast<int>(dims.size()) - 1; i >= 0; --i) {
      if (++cur_idx[i] == dims[i]) {
        cur_idx[i] = 0;
      } else {
        break;
      }
    }
  }
}

}  // namespace lczero

```

`src/utils/weights_adapter.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#include "src/utils/weights_adapter.h"

namespace lczero {
float LayerAdapter::Iterator::ExtractValue(const uint16_t* ptr,
                                           const LayerAdapter* adapter) {
  return *ptr / static_cast<float>(0xffff) * adapter->range_ + adapter->min_;
}

LayerAdapter::LayerAdapter(const pblczero::Weights::Layer& layer)
    : data_(reinterpret_cast<const uint16_t*>(layer.params().data())),
      size_(layer.params().size() / sizeof(uint16_t)),
      min_(layer.min_val()),
      range_(layer.max_val() - min_) {}

std::vector<float> LayerAdapter::as_vector() const {
  return std::vector<float>(begin(), end());
}
float LayerAdapter::Iterator::operator*() const {
  return ExtractValue(data_, adapter_);
}
float LayerAdapter::Iterator::operator[](size_t idx) const {
  return ExtractValue(data_ + idx, adapter_);
}

}  // namespace lczero
```

`src/utils/weights_adapter.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/

#pragma once

#include <iterator>
#include <vector>

#include "proto/net.pb.h"

namespace lczero {

class LayerAdapter {
 public:
  class Iterator {
   public:
    using iterator_category = std::random_access_iterator_tag;
    using value_type = float;
    using difference_type = std::ptrdiff_t;
    using pointer = float*;
    using reference = float&;

    Iterator() = default;
    Iterator(const Iterator& other) = default;

    float operator*() const;
    float operator[](size_t idx) const;
    bool operator==(const LayerAdapter::Iterator& other) const {
      return data_ == other.data_;
    }
    bool operator!=(const LayerAdapter::Iterator& other) const {
      return data_ != other.data_;
    }
    Iterator& operator++() {
      ++data_;
      return *this;
    }
    Iterator& operator--() {
      --data_;
      return *this;
    }
    ptrdiff_t operator-(const Iterator& other) const {
      return data_ - other.data_;
    }

    // TODO(crem) implement other iterator functions when they are needed.

   private:
    friend class LayerAdapter;
    Iterator(const LayerAdapter* adapter, const uint16_t* ptr)
        : adapter_(adapter), data_(ptr) {}
    static float ExtractValue(const uint16_t* ptr, const LayerAdapter* adapter);

    const LayerAdapter* adapter_ = nullptr;
    const uint16_t* data_ = nullptr;
  };

  LayerAdapter(const pblczero::Weights::Layer& layer);
  std::vector<float> as_vector() const;
  size_t size() const { return size_; }
  float operator[](size_t idx) const { return begin()[idx]; }
  Iterator begin() const { return {this, data_}; }
  Iterator end() const { return {this, data_ + size_}; }

 private:
  const uint16_t* data_ = nullptr;
  const size_t size_ = 0;
  const float min_;
  const float range_;
};

}  // namespace lczero

```

`src/version.cc`:

```cc
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#include "version.h"

std::uint32_t GetVersionInt(int major, int minor, int patch) {
  return major * 1000000 + minor * 1000 + patch;
}

std::string GetVersionStr(int major, int minor, int patch,
                          const std::string& postfix,
                          const std::string& build_id) {
  auto v = std::to_string(major) + "." + std::to_string(minor) + "." +
           std::to_string(patch);
  if (!postfix.empty()) v += "-" + postfix;
  if (!build_id.empty()) v += "+" + build_id;
  return v;
}

```

`src/version.h`:

```h
/*
  This file is part of Leela Chess Zero.
  Copyright (C) 2018 The LCZero Authors

  Leela Chess is free software: you can redistribute it and/or modify
  it under the terms of the GNU General Public License as published by
  the Free Software Foundation, either version 3 of the License, or
  (at your option) any later version.

  Leela Chess is distributed in the hope that it will be useful,
  but WITHOUT ANY WARRANTY; without even the implied warranty of
  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
  GNU General Public License for more details.

  You should have received a copy of the GNU General Public License
  along with Leela Chess.  If not, see <http://www.gnu.org/licenses/>.

  Additional permission under GNU GPL version 3 section 7

  If you modify this Program, or any covered work, by linking or
  combining it with NVIDIA Corporation's libraries from the NVIDIA CUDA
  Toolkit and the NVIDIA CUDA Deep Neural Network library (or a
  modified version of those libraries), containing parts covered by the
  terms of the respective license agreement, the licensors of this
  Program grant you additional permission to convey the resulting work.
*/
#pragma once

// Versioning is performed according to the standard at <https://semver.org/>
// Creating a new version should be performed using scripts/bumpversion.py.

#include <string>
#include "version.inc"
#include "build_id.h"

std::uint32_t GetVersionInt(int major = LC0_VERSION_MAJOR,
                            int minor = LC0_VERSION_MINOR,
                            int patch = LC0_VERSION_PATCH);

std::string GetVersionStr(int major = LC0_VERSION_MAJOR,
                          int minor = LC0_VERSION_MINOR,
                          int patch = LC0_VERSION_PATCH,
                          const std::string& postfix = LC0_VERSION_POSTFIX,
                          const std::string& build_id = BUILD_IDENTIFIER);

```

`src/version.inc`:

```inc
#define LC0_VERSION_MAJOR 0
#define LC0_VERSION_MINOR 30
#define LC0_VERSION_PATCH 0
#define LC0_VERSION_POSTFIX "dev"

```

`tensorflow.md`:

```md
To build with tensorflow under linux you need to install Tensorflow_cc from
<https://github.com/FloopCZ/tensorflow_cc>. Either release v1.9.0, v1.12.0 or
v1.13.0. Tensorflow_cc requires a specific version of protobuf, which constrains
the build. Release v1.9.0 works out of the box, since the default protobuf
subproject (v3.5.1) is compatible and is used instead of a system installed
version. In contrast release v1.12.0 needs protobuf v3.6.0 and release v1.13.0
is built with protobuf 3.6.1 but also works with 3.6.0. For those versions
`-Dprotobuf-3-6-0=true` should be added to the build command line. Note that
this protobuf version has issues with static builds and crashes so is not
recommended for normal use. The crashes look very similar to:
* <https://github.com/protocolbuffers/protobuf/issues/5107>
* <https://github.com/protocolbuffers/protobuf/issues/5353>

```

`third_party/cl2.hpp`:

```hpp
/*******************************************************************************
 * Copyright (c) 2008-2016 The Khronos Group Inc.
 *
 * Permission is hereby granted, free of charge, to any person obtaining a
 * copy of this software and/or associated documentation files (the
 * "Materials"), to deal in the Materials without restriction, including
 * without limitation the rights to use, copy, modify, merge, publish,
 * distribute, sublicense, and/or sell copies of the Materials, and to
 * permit persons to whom the Materials are furnished to do so, subject to
 * the following conditions:
 *
 * The above copyright notice and this permission notice shall be included
 * in all copies or substantial portions of the Materials.
 *
 * MODIFICATIONS TO THIS FILE MAY MEAN IT NO LONGER ACCURATELY REFLECTS
 * KHRONOS STANDARDS. THE UNMODIFIED, NORMATIVE VERSIONS OF KHRONOS
 * SPECIFICATIONS AND HEADER INFORMATION ARE LOCATED AT
 *    https://www.khronos.org/registry/
 *
 * THE MATERIALS ARE PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
 * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
 * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
 * IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
 * CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
 * TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
 * MATERIALS OR THE USE OR OTHER DEALINGS IN THE MATERIALS.
 ******************************************************************************/

/*! \file
 *
 *   \brief C++ bindings for OpenCL 1.0 (rev 48), OpenCL 1.1 (rev 33),
 *       OpenCL 1.2 (rev 15) and OpenCL 2.0 (rev 29)
 *   \author Lee Howes and Bruce Merry
 *
 *   Derived from the OpenCL 1.x C++ bindings written by
 *   Benedict R. Gaster, Laurent Morichetti and Lee Howes
 *   With additions and fixes from:
 *       Brian Cole, March 3rd 2010 and April 2012
 *       Matt Gruenke, April 2012.
 *       Bruce Merry, February 2013.
 *       Tom Deakin and Simon McIntosh-Smith, July 2013
 *       James Price, 2015-
 *
 *   \version 2.0.10
 *   \date 2016-07-20
 *
 *   Optional extension support
 *
 *         cl_ext_device_fission
 *         #define CL_HPP_USE_CL_DEVICE_FISSION
 *         cl_khr_d3d10_sharing
 *         #define CL_HPP_USE_DX_INTEROP
 *         cl_khr_sub_groups
 *         #define CL_HPP_USE_CL_SUB_GROUPS_KHR
 *         cl_khr_image2d_from_buffer
 *         #define CL_HPP_USE_CL_IMAGE2D_FROM_BUFFER_KHR
 *
 *   Doxygen documentation for this header is available here:
 *
 *       http://khronosgroup.github.io/OpenCL-CLHPP/
 *
 *   The latest version of this header can be found on the GitHub releases page:
 *
 *       https://github.com/KhronosGroup/OpenCL-CLHPP/releases
 *
 *   Bugs and patches can be submitted to the GitHub repository:
 *
 *       https://github.com/KhronosGroup/OpenCL-CLHPP
 */

/*! \mainpage
 * \section intro Introduction
 * For many large applications C++ is the language of choice and so it seems
 * reasonable to define C++ bindings for OpenCL.
 *
 * The interface is contained with a single C++ header file \em cl2.hpp and all
 * definitions are contained within the namespace \em cl. There is no additional
 * requirement to include \em cl.h and to use either the C++ or original C
 * bindings; it is enough to simply include \em cl2.hpp.
 *
 * The bindings themselves are lightweight and correspond closely to the
 * underlying C API. Using the C++ bindings introduces no additional execution
 * overhead.
 *
 * There are numerous compatibility, portability and memory management
 * fixes in the new header as well as additional OpenCL 2.0 features.
 * As a result the header is not directly backward compatible and for this
 * reason we release it as cl2.hpp rather than a new version of cl.hpp.
 * 
 *
 * \section compatibility Compatibility
 * Due to the evolution of the underlying OpenCL API the 2.0 C++ bindings
 * include an updated approach to defining supported feature versions
 * and the range of valid underlying OpenCL runtime versions supported.
 *
 * The combination of preprocessor macros CL_HPP_TARGET_OPENCL_VERSION and 
 * CL_HPP_MINIMUM_OPENCL_VERSION control this range. These are three digit
 * decimal values representing OpenCL runime versions. The default for 
 * the target is 200, representing OpenCL 2.0 and the minimum is also 
 * defined as 200. These settings would use 2.0 API calls only.
 * If backward compatibility with a 1.2 runtime is required, the minimum
 * version may be set to 120.
 *
 * Note that this is a compile-time setting, and so affects linking against
 * a particular SDK version rather than the versioning of the loaded runtime.
 *
 * The earlier versions of the header included basic vector and string 
 * classes based loosely on STL versions. These were difficult to 
 * maintain and very rarely used. For the 2.0 header we now assume
 * the presence of the standard library unless requested otherwise.
 * We use std::array, std::vector, std::shared_ptr and std::string 
 * throughout to safely manage memory and reduce the chance of a 
 * recurrance of earlier memory management bugs.
 *
 * These classes are used through typedefs in the cl namespace: 
 * cl::array, cl::vector, cl::pointer and cl::string.
 * In addition cl::allocate_pointer forwards to std::allocate_shared
 * by default.
 * In all cases these standard library classes can be replaced with 
 * custom interface-compatible versions using the CL_HPP_NO_STD_ARRAY, 
 * CL_HPP_NO_STD_VECTOR, CL_HPP_NO_STD_UNIQUE_PTR and 
 * CL_HPP_NO_STD_STRING macros.
 *
 * The OpenCL 1.x versions of the C++ bindings included a size_t wrapper
 * class to interface with kernel enqueue. This caused unpleasant interactions
 * with the standard size_t declaration and led to namespacing bugs.
 * In the 2.0 version we have replaced this with a std::array-based interface.
 * However, the old behaviour can be regained for backward compatibility
 * using the CL_HPP_ENABLE_SIZE_T_COMPATIBILITY macro.
 *
 * Finally, the program construction interface used a clumsy vector-of-pairs
 * design in the earlier versions. We have replaced that with a cleaner 
 * vector-of-vectors and vector-of-strings design. However, for backward 
 * compatibility old behaviour can be regained with the
 * CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY macro.
 * 
 * In OpenCL 2.0 OpenCL C is not entirely backward compatibility with 
 * earlier versions. As a result a flag must be passed to the OpenCL C
 * compiled to request OpenCL 2.0 compilation of kernels with 1.2 as
 * the default in the absence of the flag.
 * In some cases the C++ bindings automatically compile code for ease.
 * For those cases the compilation defaults to OpenCL C 2.0.
 * If this is not wanted, the CL_HPP_CL_1_2_DEFAULT_BUILD macro may
 * be specified to assume 1.2 compilation.
 * If more fine-grained decisions on a per-kernel bases are required
 * then explicit build operations that take the flag should be used.
 *
 *
 * \section parameterization Parameters
 * This header may be parameterized by a set of preprocessor macros.
 *
 * - CL_HPP_TARGET_OPENCL_VERSION
 *
 *   Defines the target OpenCL runtime version to build the header
 *   against. Defaults to 200, representing OpenCL 2.0.
 *
 * - CL_HPP_NO_STD_STRING
 *
 *   Do not use the standard library string class. cl::string is not
 *   defined and may be defined by the user before cl2.hpp is
 *   included.
 *
 * - CL_HPP_NO_STD_VECTOR
 *
 *   Do not use the standard library vector class. cl::vector is not
 *   defined and may be defined by the user before cl2.hpp is
 *   included.
 *
 * - CL_HPP_NO_STD_ARRAY
 *
 *   Do not use the standard library array class. cl::array is not
 *   defined and may be defined by the user before cl2.hpp is
 *   included.
 *
 * - CL_HPP_NO_STD_UNIQUE_PTR
 *
 *   Do not use the standard library unique_ptr class. cl::pointer and
 *   the cl::allocate_pointer functions are not defined and may be
 *   defined by the user before cl2.hpp is included.
 *
 * - CL_HPP_ENABLE_DEVICE_FISSION
 *
 *   Enables device fission for OpenCL 1.2 platforms.
 *
 * - CL_HPP_ENABLE_EXCEPTIONS
 *
 *   Enable exceptions for use in the C++ bindings header. This is the
 *   preferred error handling mechanism but is not required.
 *
 * - CL_HPP_ENABLE_SIZE_T_COMPATIBILITY
 *
 *   Backward compatibility option to support cl.hpp-style size_t
 *   class.  Replaces the updated std::array derived version and
 *   removal of size_t from the namespace. Note that in this case the
 *   new size_t class is placed in the cl::compatibility namespace and
 *   thus requires an additional using declaration for direct backward
 *   compatibility.
 *
 * - CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY
 *
 *   Enable older vector of pairs interface for construction of
 *   programs.
 *
 * - CL_HPP_CL_1_2_DEFAULT_BUILD
 *
 *   Default to OpenCL C 1.2 compilation rather than OpenCL C 2.0
 *   applies to use of cl::Program construction and other program
 *   build variants.
 *
 *
 * \section example Example
 *
 * The following example shows a general use case for the C++
 * bindings, including support for the optional exception feature and
 * also the supplied vector and string classes, see following sections for
 * decriptions of these features.
 *
 * \code
    #define CL_HPP_ENABLE_EXCEPTIONS
    #define CL_HPP_TARGET_OPENCL_VERSION 200

    #include <CL/cl2.hpp>
    #include <iostream>
    #include <vector>
    #include <memory>
    #include <algorithm>

    const int numElements = 32;

    int main(void)
    {
        // Filter for a 2.0 platform and set it as the default
        std::vector<cl::Platform> platforms;
        cl::Platform::get(&platforms);
        cl::Platform plat;
        for (auto &p : platforms) {
            std::string platver = p.getInfo<CL_PLATFORM_VERSION>();
            if (platver.find("OpenCL 2.") != std::string::npos) {
                plat = p;
            }
        }
        if (plat() == 0)  {
            std::cout << "No OpenCL 2.0 platform found.";
            return -1;
        }

        cl::Platform newP = cl::Platform::setDefault(plat);
        if (newP != plat) {
            std::cout << "Error setting default platform.";
            return -1;
        }

        // Use C++11 raw string literals for kernel source code
        std::string kernel1{R"CLC(
            global int globalA;
            kernel void updateGlobal()
            {
              globalA = 75;
            }
        )CLC"};
        std::string kernel2{R"CLC(
            typedef struct { global int *bar; } Foo;
            kernel void vectorAdd(global const Foo* aNum, global const int *inputA, global const int *inputB,
                                  global int *output, int val, write_only pipe int outPipe, queue_t childQueue)
            {
              output[get_global_id(0)] = inputA[get_global_id(0)] + inputB[get_global_id(0)] + val + *(aNum->bar);
              write_pipe(outPipe, &val);
              queue_t default_queue = get_default_queue();
              ndrange_t ndrange = ndrange_1D(get_global_size(0)/2, get_global_size(0)/2);

              // Have a child kernel write into third quarter of output
              enqueue_kernel(default_queue, CLK_ENQUEUE_FLAGS_WAIT_KERNEL, ndrange,
                ^{
                    output[get_global_size(0)*2 + get_global_id(0)] =
                      inputA[get_global_size(0)*2 + get_global_id(0)] + inputB[get_global_size(0)*2 + get_global_id(0)] + globalA;
                });

              // Have a child kernel write into last quarter of output
              enqueue_kernel(childQueue, CLK_ENQUEUE_FLAGS_WAIT_KERNEL, ndrange,
                ^{
                    output[get_global_size(0)*3 + get_global_id(0)] =
                      inputA[get_global_size(0)*3 + get_global_id(0)] + inputB[get_global_size(0)*3 + get_global_id(0)] + globalA + 2;
                });
            }
        )CLC"};

        // New simpler string interface style
        std::vector<std::string> programStrings {kernel1, kernel2};

        cl::Program vectorAddProgram(programStrings);
        try {
            vectorAddProgram.build("-cl-std=CL2.0");
        }
        catch (...) {
            // Print build info for all devices
            cl_int buildErr = CL_SUCCESS;
            auto buildInfo = vectorAddProgram.getBuildInfo<CL_PROGRAM_BUILD_LOG>(&buildErr);
            for (auto &pair : buildInfo) {
                std::cerr << pair.second << std::endl << std::endl;
            }

            return 1;
        }

        typedef struct { int *bar; } Foo;

        // Get and run kernel that initializes the program-scope global
        // A test for kernels that take no arguments
        auto program2Kernel =
            cl::KernelFunctor<>(vectorAddProgram, "updateGlobal");
        program2Kernel(
            cl::EnqueueArgs(
            cl::NDRange(1)));

        //////////////////
        // SVM allocations

        auto anSVMInt = cl::allocate_svm<int, cl::SVMTraitCoarse<>>();
        *anSVMInt = 5;
        cl::SVMAllocator<Foo, cl::SVMTraitCoarse<cl::SVMTraitReadOnly<>>> svmAllocReadOnly;
        auto fooPointer = cl::allocate_pointer<Foo>(svmAllocReadOnly);
        fooPointer->bar = anSVMInt.get();
        cl::SVMAllocator<int, cl::SVMTraitCoarse<>> svmAlloc;
        std::vector<int, cl::SVMAllocator<int, cl::SVMTraitCoarse<>>> inputA(numElements, 1, svmAlloc);
        cl::coarse_svm_vector<int> inputB(numElements, 2, svmAlloc);

        //
        //////////////

        // Traditional cl_mem allocations
        std::vector<int> output(numElements, 0xdeadbeef);
        cl::Buffer outputBuffer(begin(output), end(output), false);
        cl::Pipe aPipe(sizeof(cl_int), numElements / 2);

        // Default command queue, also passed in as a parameter
        cl::DeviceCommandQueue defaultDeviceQueue = cl::DeviceCommandQueue::makeDefault(
            cl::Context::getDefault(), cl::Device::getDefault());

        auto vectorAddKernel =
            cl::KernelFunctor<
                decltype(fooPointer)&,
                int*,
                cl::coarse_svm_vector<int>&,
                cl::Buffer,
                int,
                cl::Pipe&,
                cl::DeviceCommandQueue
                >(vectorAddProgram, "vectorAdd");

        // Ensure that the additional SVM pointer is available to the kernel
        // This one was not passed as a parameter
        vectorAddKernel.setSVMPointers(anSVMInt);

        // Hand control of coarse allocations to runtime
        cl::enqueueUnmapSVM(anSVMInt);
        cl::enqueueUnmapSVM(fooPointer);
        cl::unmapSVM(inputB);
        cl::unmapSVM(output2);

	    cl_int error;
	    vectorAddKernel(
            cl::EnqueueArgs(
                cl::NDRange(numElements/2),
                cl::NDRange(numElements/2)),
            fooPointer,
            inputA.data(),
            inputB,
            outputBuffer,
            3,
            aPipe,
            defaultDeviceQueue,
		    error
            );

        cl::copy(outputBuffer, begin(output), end(output));
        // Grab the SVM output vector using a map
        cl::mapSVM(output2);

        cl::Device d = cl::Device::getDefault();

        std::cout << "Output:\n";
        for (int i = 1; i < numElements; ++i) {
            std::cout << "\t" << output[i] << "\n";
        }
        std::cout << "\n\n";

        return 0;
    }
 *
 * \endcode
 *
 */
#ifndef CL_HPP_
#define CL_HPP_

/* Handle deprecated preprocessor definitions. In each case, we only check for
 * the old name if the new name is not defined, so that user code can define
 * both and hence work with either version of the bindings.
 */
#if !defined(CL_HPP_USE_DX_INTEROP) && defined(USE_DX_INTEROP)
# pragma message("cl2.hpp: USE_DX_INTEROP is deprecated. Define CL_HPP_USE_DX_INTEROP instead")
# define CL_HPP_USE_DX_INTEROP
#endif
#if !defined(CL_HPP_USE_CL_DEVICE_FISSION) && defined(USE_CL_DEVICE_FISSION)
# pragma message("cl2.hpp: USE_CL_DEVICE_FISSION is deprecated. Define CL_HPP_USE_CL_DEVICE_FISSION instead")
# define CL_HPP_USE_CL_DEVICE_FISSION
#endif
#if !defined(CL_HPP_ENABLE_EXCEPTIONS) && defined(__CL_ENABLE_EXCEPTIONS)
# pragma message("cl2.hpp: __CL_ENABLE_EXCEPTIONS is deprecated. Define CL_HPP_ENABLE_EXCEPTIONS instead")
# define CL_HPP_ENABLE_EXCEPTIONS
#endif
#if !defined(CL_HPP_NO_STD_VECTOR) && defined(__NO_STD_VECTOR)
# pragma message("cl2.hpp: __NO_STD_VECTOR is deprecated. Define CL_HPP_NO_STD_VECTOR instead")
# define CL_HPP_NO_STD_VECTOR
#endif
#if !defined(CL_HPP_NO_STD_STRING) && defined(__NO_STD_STRING)
# pragma message("cl2.hpp: __NO_STD_STRING is deprecated. Define CL_HPP_NO_STD_STRING instead")
# define CL_HPP_NO_STD_STRING
#endif
#if defined(VECTOR_CLASS)
# pragma message("cl2.hpp: VECTOR_CLASS is deprecated. Alias cl::vector instead")
#endif
#if defined(STRING_CLASS)
# pragma message("cl2.hpp: STRING_CLASS is deprecated. Alias cl::string instead.")
#endif
#if !defined(CL_HPP_USER_OVERRIDE_ERROR_STRINGS) && defined(__CL_USER_OVERRIDE_ERROR_STRINGS)
# pragma message("cl2.hpp: __CL_USER_OVERRIDE_ERROR_STRINGS is deprecated. Define CL_HPP_USER_OVERRIDE_ERROR_STRINGS instead")
# define CL_HPP_USER_OVERRIDE_ERROR_STRINGS
#endif

/* Warn about features that are no longer supported
 */
#if defined(__USE_DEV_VECTOR)
# pragma message("cl2.hpp: __USE_DEV_VECTOR is no longer supported. Expect compilation errors")
#endif
#if defined(__USE_DEV_STRING)
# pragma message("cl2.hpp: __USE_DEV_STRING is no longer supported. Expect compilation errors")
#endif

/* Detect which version to target */
#if !defined(CL_HPP_TARGET_OPENCL_VERSION)
# pragma message("cl2.hpp: CL_HPP_TARGET_OPENCL_VERSION is not defined. It will default to 200 (OpenCL 2.0)")
# define CL_HPP_TARGET_OPENCL_VERSION 200
#endif
#if CL_HPP_TARGET_OPENCL_VERSION != 100 && CL_HPP_TARGET_OPENCL_VERSION != 110 && CL_HPP_TARGET_OPENCL_VERSION != 120 && CL_HPP_TARGET_OPENCL_VERSION != 200
# pragma message("cl2.hpp: CL_HPP_TARGET_OPENCL_VERSION is not a valid value (100, 110, 120 or 200). It will be set to 200")
# undef CL_HPP_TARGET_OPENCL_VERSION
# define CL_HPP_TARGET_OPENCL_VERSION 200
#endif

#if !defined(CL_HPP_MINIMUM_OPENCL_VERSION)
# define CL_HPP_MINIMUM_OPENCL_VERSION 200
#endif
#if CL_HPP_MINIMUM_OPENCL_VERSION != 100 && CL_HPP_MINIMUM_OPENCL_VERSION != 110 && CL_HPP_MINIMUM_OPENCL_VERSION != 120 && CL_HPP_MINIMUM_OPENCL_VERSION != 200
# pragma message("cl2.hpp: CL_HPP_MINIMUM_OPENCL_VERSION is not a valid value (100, 110, 120 or 200). It will be set to 100")
# undef CL_HPP_MINIMUM_OPENCL_VERSION
# define CL_HPP_MINIMUM_OPENCL_VERSION 100
#endif
#if CL_HPP_MINIMUM_OPENCL_VERSION > CL_HPP_TARGET_OPENCL_VERSION
# error "CL_HPP_MINIMUM_OPENCL_VERSION must not be greater than CL_HPP_TARGET_OPENCL_VERSION"
#endif

#if CL_HPP_MINIMUM_OPENCL_VERSION <= 100 && !defined(CL_USE_DEPRECATED_OPENCL_1_0_APIS)
# define CL_USE_DEPRECATED_OPENCL_1_0_APIS
#endif
#if CL_HPP_MINIMUM_OPENCL_VERSION <= 110 && !defined(CL_USE_DEPRECATED_OPENCL_1_1_APIS)
# define CL_USE_DEPRECATED_OPENCL_1_1_APIS
#endif
#if CL_HPP_MINIMUM_OPENCL_VERSION <= 120 && !defined(CL_USE_DEPRECATED_OPENCL_1_2_APIS)
# define CL_USE_DEPRECATED_OPENCL_1_2_APIS
#endif
#if CL_HPP_MINIMUM_OPENCL_VERSION <= 200 && !defined(CL_USE_DEPRECATED_OPENCL_2_0_APIS)
# define CL_USE_DEPRECATED_OPENCL_2_0_APIS
#endif

#ifdef _WIN32

#include <malloc.h>

#if defined(CL_HPP_USE_DX_INTEROP)
#include <CL/cl_d3d10.h>
#include <CL/cl_dx9_media_sharing.h>
#endif
#endif // _WIN32

#if defined(_MSC_VER)
#include <intrin.h>
#endif // _MSC_VER 
 
 // Check for a valid C++ version

// Need to do both tests here because for some reason __cplusplus is not 
// updated in visual studio
#if (!defined(_MSC_VER) && __cplusplus < 201103L) || (defined(_MSC_VER) && _MSC_VER < 1700)
#error Visual studio 2013 or another C++11-supporting compiler required
#endif

// 
#if defined(CL_HPP_USE_CL_DEVICE_FISSION) || defined(CL_HPP_USE_CL_SUB_GROUPS_KHR)
#include <CL/cl_ext.h>
#endif

#if defined(__APPLE__) || defined(__MACOSX)
#include <OpenCL/opencl.h>
#else
#include <CL/opencl.h>
#endif // !__APPLE__

#if (__cplusplus >= 201103L)
#define CL_HPP_NOEXCEPT_ noexcept
#else
#define CL_HPP_NOEXCEPT_
#endif

#if defined(_MSC_VER)
# define CL_HPP_DEFINE_STATIC_MEMBER_ __declspec(selectany)
#else
# define CL_HPP_DEFINE_STATIC_MEMBER_ __attribute__((weak))
#endif // !_MSC_VER

// Define deprecated prefixes and suffixes to ensure compilation
// in case they are not pre-defined
#if !defined(CL_EXT_PREFIX__VERSION_1_1_DEPRECATED)
#define CL_EXT_PREFIX__VERSION_1_1_DEPRECATED  
#endif // #if !defined(CL_EXT_PREFIX__VERSION_1_1_DEPRECATED)
#if !defined(CL_EXT_SUFFIX__VERSION_1_1_DEPRECATED)
#define CL_EXT_SUFFIX__VERSION_1_1_DEPRECATED
#endif // #if !defined(CL_EXT_PREFIX__VERSION_1_1_DEPRECATED)

#if !defined(CL_EXT_PREFIX__VERSION_1_2_DEPRECATED)
#define CL_EXT_PREFIX__VERSION_1_2_DEPRECATED  
#endif // #if !defined(CL_EXT_PREFIX__VERSION_1_2_DEPRECATED)
#if !defined(CL_EXT_SUFFIX__VERSION_1_2_DEPRECATED)
#define CL_EXT_SUFFIX__VERSION_1_2_DEPRECATED
#endif // #if !defined(CL_EXT_PREFIX__VERSION_1_2_DEPRECATED)

#if !defined(CL_CALLBACK)
#define CL_CALLBACK
#endif //CL_CALLBACK

#include <utility>
#include <limits>
#include <iterator>
#include <mutex>
#include <cstring>
#include <functional>


// Define a size_type to represent a correctly resolved size_t
#if defined(CL_HPP_ENABLE_SIZE_T_COMPATIBILITY)
namespace cl {
    using size_type = ::size_t;
} // namespace cl
#else // #if defined(CL_HPP_ENABLE_SIZE_T_COMPATIBILITY)
namespace cl {
    using size_type = size_t;
} // namespace cl
#endif // #if defined(CL_HPP_ENABLE_SIZE_T_COMPATIBILITY)


#if defined(CL_HPP_ENABLE_EXCEPTIONS)
#include <exception>
#endif // #if defined(CL_HPP_ENABLE_EXCEPTIONS)

#if !defined(CL_HPP_NO_STD_VECTOR)
#include <vector>
namespace cl {
    template < class T, class Alloc = std::allocator<T> >
    using vector = std::vector<T, Alloc>;
} // namespace cl
#endif // #if !defined(CL_HPP_NO_STD_VECTOR)

#if !defined(CL_HPP_NO_STD_STRING)
#include <string>
namespace cl {
    using string = std::string;
} // namespace cl
#endif // #if !defined(CL_HPP_NO_STD_STRING)

#if CL_HPP_TARGET_OPENCL_VERSION >= 200

#if !defined(CL_HPP_NO_STD_UNIQUE_PTR)
#include <memory>
namespace cl {
    // Replace unique_ptr and allocate_pointer for internal use
    // to allow user to replace them
    template<class T, class D>
    using pointer = std::unique_ptr<T, D>;
} // namespace cl
#endif 
#endif // #if CL_HPP_TARGET_OPENCL_VERSION >= 200
#if !defined(CL_HPP_NO_STD_ARRAY)
#include <array>
namespace cl {
    template < class T, size_type N >
    using array = std::array<T, N>;
} // namespace cl
#endif // #if !defined(CL_HPP_NO_STD_ARRAY)

// Define size_type appropriately to allow backward-compatibility
// use of the old size_t interface class
#if defined(CL_HPP_ENABLE_SIZE_T_COMPATIBILITY)
namespace cl {
    namespace compatibility {
        /*! \brief class used to interface between C++ and
        *  OpenCL C calls that require arrays of size_t values, whose
        *  size is known statically.
        */
        template <int N>
        class size_t
        {
        private:
            size_type data_[N];

        public:
            //! \brief Initialize size_t to all 0s
            size_t()
            {
                for (int i = 0; i < N; ++i) {
                    data_[i] = 0;
                }
            }

            size_t(const array<size_type, N> &rhs)
            {
                for (int i = 0; i < N; ++i) {
                    data_[i] = rhs[i];
                }
            }

            size_type& operator[](int index)
            {
                return data_[index];
            }

            const size_type& operator[](int index) const
            {
                return data_[index];
            }

            //! \brief Conversion operator to T*.
            operator size_type* ()             { return data_; }

            //! \brief Conversion operator to const T*.
            operator const size_type* () const { return data_; }

            operator array<size_type, N>() const
            {
                array<size_type, N> ret;

                for (int i = 0; i < N; ++i) {
                    ret[i] = data_[i];
                }
                return ret;
            }
        };
    } // namespace compatibility

    template<int N>
    using size_t = compatibility::size_t<N>;
} // namespace cl
#endif // #if defined(CL_HPP_ENABLE_SIZE_T_COMPATIBILITY)

// Helper alias to avoid confusing the macros
namespace cl {
    namespace detail {
        using size_t_array = array<size_type, 3>;
    } // namespace detail
} // namespace cl


/*! \namespace cl
 *
 * \brief The OpenCL C++ bindings are defined within this namespace.
 *
 */
namespace cl {
    class Memory;

#define CL_HPP_INIT_CL_EXT_FCN_PTR_(name) \
    if (!pfn_##name) {    \
    pfn_##name = (PFN_##name) \
    clGetExtensionFunctionAddress(#name); \
    if (!pfn_##name) {    \
    } \
    }

#define CL_HPP_INIT_CL_EXT_FCN_PTR_PLATFORM_(platform, name) \
    if (!pfn_##name) {    \
    pfn_##name = (PFN_##name) \
    clGetExtensionFunctionAddressForPlatform(platform, #name); \
    if (!pfn_##name) {    \
    } \
    }

    class Program;
    class Device;
    class Context;
    class CommandQueue;
    class DeviceCommandQueue;
    class Memory;
    class Buffer;
    class Pipe;

#if defined(CL_HPP_ENABLE_EXCEPTIONS)
    /*! \brief Exception class 
     * 
     *  This may be thrown by API functions when CL_HPP_ENABLE_EXCEPTIONS is defined.
     */
    class Error : public std::exception
    {
    private:
        cl_int err_;
        const char * errStr_;
    public:
        /*! \brief Create a new CL error exception for a given error code
         *  and corresponding message.
         * 
         *  \param err error code value.
         *
         *  \param errStr a descriptive string that must remain in scope until
         *                handling of the exception has concluded.  If set, it
         *                will be returned by what().
         */
        Error(cl_int err, const char * errStr = NULL) : err_(err), errStr_(errStr)
        {}

        ~Error() throw() {}

        /*! \brief Get error string associated with exception
         *
         * \return A memory pointer to the error message string.
         */
        virtual const char * what() const throw ()
        {
            if (errStr_ == NULL) {
                return "empty";
            }
            else {
                return errStr_;
            }
        }

        /*! \brief Get error code associated with exception
         *
         *  \return The error code.
         */
        cl_int err(void) const { return err_; }
    };
#define CL_HPP_ERR_STR_(x) #x
#else
#define CL_HPP_ERR_STR_(x) NULL
#endif // CL_HPP_ENABLE_EXCEPTIONS


namespace detail
{
#if defined(CL_HPP_ENABLE_EXCEPTIONS)
static inline cl_int errHandler (
    cl_int err,
    const char * errStr = NULL)
{
    if (err != CL_SUCCESS) {
        throw Error(err, errStr);
    }
    return err;
}
#else
static inline cl_int errHandler (cl_int err, const char * errStr = NULL)
{
    (void) errStr; // suppress unused variable warning
    return err;
}
#endif // CL_HPP_ENABLE_EXCEPTIONS
}



//! \cond DOXYGEN_DETAIL
#if !defined(CL_HPP_USER_OVERRIDE_ERROR_STRINGS)
#define __GET_DEVICE_INFO_ERR               CL_HPP_ERR_STR_(clGetDeviceInfo)
#define __GET_PLATFORM_INFO_ERR             CL_HPP_ERR_STR_(clGetPlatformInfo)
#define __GET_DEVICE_IDS_ERR                CL_HPP_ERR_STR_(clGetDeviceIDs)
#define __GET_PLATFORM_IDS_ERR              CL_HPP_ERR_STR_(clGetPlatformIDs)
#define __GET_CONTEXT_INFO_ERR              CL_HPP_ERR_STR_(clGetContextInfo)
#define __GET_EVENT_INFO_ERR                CL_HPP_ERR_STR_(clGetEventInfo)
#define __GET_EVENT_PROFILE_INFO_ERR        CL_HPP_ERR_STR_(clGetEventProfileInfo)
#define __GET_MEM_OBJECT_INFO_ERR           CL_HPP_ERR_STR_(clGetMemObjectInfo)
#define __GET_IMAGE_INFO_ERR                CL_HPP_ERR_STR_(clGetImageInfo)
#define __GET_SAMPLER_INFO_ERR              CL_HPP_ERR_STR_(clGetSamplerInfo)
#define __GET_KERNEL_INFO_ERR               CL_HPP_ERR_STR_(clGetKernelInfo)
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
#define __GET_KERNEL_ARG_INFO_ERR           CL_HPP_ERR_STR_(clGetKernelArgInfo)
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120
#define __GET_KERNEL_WORK_GROUP_INFO_ERR    CL_HPP_ERR_STR_(clGetKernelWorkGroupInfo)
#define __GET_PROGRAM_INFO_ERR              CL_HPP_ERR_STR_(clGetProgramInfo)
#define __GET_PROGRAM_BUILD_INFO_ERR        CL_HPP_ERR_STR_(clGetProgramBuildInfo)
#define __GET_COMMAND_QUEUE_INFO_ERR        CL_HPP_ERR_STR_(clGetCommandQueueInfo)

#define __CREATE_CONTEXT_ERR                CL_HPP_ERR_STR_(clCreateContext)
#define __CREATE_CONTEXT_FROM_TYPE_ERR      CL_HPP_ERR_STR_(clCreateContextFromType)
#define __GET_SUPPORTED_IMAGE_FORMATS_ERR   CL_HPP_ERR_STR_(clGetSupportedImageFormats)

#define __CREATE_BUFFER_ERR                 CL_HPP_ERR_STR_(clCreateBuffer)
#define __COPY_ERR                          CL_HPP_ERR_STR_(cl::copy)
#define __CREATE_SUBBUFFER_ERR              CL_HPP_ERR_STR_(clCreateSubBuffer)
#define __CREATE_GL_BUFFER_ERR              CL_HPP_ERR_STR_(clCreateFromGLBuffer)
#define __CREATE_GL_RENDER_BUFFER_ERR       CL_HPP_ERR_STR_(clCreateFromGLBuffer)
#define __GET_GL_OBJECT_INFO_ERR            CL_HPP_ERR_STR_(clGetGLObjectInfo)
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
#define __CREATE_IMAGE_ERR                  CL_HPP_ERR_STR_(clCreateImage)
#define __CREATE_GL_TEXTURE_ERR             CL_HPP_ERR_STR_(clCreateFromGLTexture)
#define __IMAGE_DIMENSION_ERR               CL_HPP_ERR_STR_(Incorrect image dimensions)
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120
#define __SET_MEM_OBJECT_DESTRUCTOR_CALLBACK_ERR CL_HPP_ERR_STR_(clSetMemObjectDestructorCallback)

#define __CREATE_USER_EVENT_ERR             CL_HPP_ERR_STR_(clCreateUserEvent)
#define __SET_USER_EVENT_STATUS_ERR         CL_HPP_ERR_STR_(clSetUserEventStatus)
#define __SET_EVENT_CALLBACK_ERR            CL_HPP_ERR_STR_(clSetEventCallback)
#define __WAIT_FOR_EVENTS_ERR               CL_HPP_ERR_STR_(clWaitForEvents)

#define __CREATE_KERNEL_ERR                 CL_HPP_ERR_STR_(clCreateKernel)
#define __SET_KERNEL_ARGS_ERR               CL_HPP_ERR_STR_(clSetKernelArg)
#define __CREATE_PROGRAM_WITH_SOURCE_ERR    CL_HPP_ERR_STR_(clCreateProgramWithSource)
#define __CREATE_PROGRAM_WITH_BINARY_ERR    CL_HPP_ERR_STR_(clCreateProgramWithBinary)
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
#define __CREATE_PROGRAM_WITH_BUILT_IN_KERNELS_ERR    CL_HPP_ERR_STR_(clCreateProgramWithBuiltInKernels)
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120
#define __BUILD_PROGRAM_ERR                 CL_HPP_ERR_STR_(clBuildProgram)
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
#define __COMPILE_PROGRAM_ERR               CL_HPP_ERR_STR_(clCompileProgram)
#define __LINK_PROGRAM_ERR                  CL_HPP_ERR_STR_(clLinkProgram)
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120
#define __CREATE_KERNELS_IN_PROGRAM_ERR     CL_HPP_ERR_STR_(clCreateKernelsInProgram)

#if CL_HPP_TARGET_OPENCL_VERSION >= 200
#define __CREATE_COMMAND_QUEUE_WITH_PROPERTIES_ERR          CL_HPP_ERR_STR_(clCreateCommandQueueWithProperties)
#define __CREATE_SAMPLER_WITH_PROPERTIES_ERR                CL_HPP_ERR_STR_(clCreateSamplerWithProperties)
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 200
#define __SET_COMMAND_QUEUE_PROPERTY_ERR    CL_HPP_ERR_STR_(clSetCommandQueueProperty)
#define __ENQUEUE_READ_BUFFER_ERR           CL_HPP_ERR_STR_(clEnqueueReadBuffer)
#define __ENQUEUE_READ_BUFFER_RECT_ERR      CL_HPP_ERR_STR_(clEnqueueReadBufferRect)
#define __ENQUEUE_WRITE_BUFFER_ERR          CL_HPP_ERR_STR_(clEnqueueWriteBuffer)
#define __ENQUEUE_WRITE_BUFFER_RECT_ERR     CL_HPP_ERR_STR_(clEnqueueWriteBufferRect)
#define __ENQEUE_COPY_BUFFER_ERR            CL_HPP_ERR_STR_(clEnqueueCopyBuffer)
#define __ENQEUE_COPY_BUFFER_RECT_ERR       CL_HPP_ERR_STR_(clEnqueueCopyBufferRect)
#define __ENQUEUE_FILL_BUFFER_ERR           CL_HPP_ERR_STR_(clEnqueueFillBuffer)
#define __ENQUEUE_READ_IMAGE_ERR            CL_HPP_ERR_STR_(clEnqueueReadImage)
#define __ENQUEUE_WRITE_IMAGE_ERR           CL_HPP_ERR_STR_(clEnqueueWriteImage)
#define __ENQUEUE_COPY_IMAGE_ERR            CL_HPP_ERR_STR_(clEnqueueCopyImage)
#define __ENQUEUE_FILL_IMAGE_ERR            CL_HPP_ERR_STR_(clEnqueueFillImage)
#define __ENQUEUE_COPY_IMAGE_TO_BUFFER_ERR  CL_HPP_ERR_STR_(clEnqueueCopyImageToBuffer)
#define __ENQUEUE_COPY_BUFFER_TO_IMAGE_ERR  CL_HPP_ERR_STR_(clEnqueueCopyBufferToImage)
#define __ENQUEUE_MAP_BUFFER_ERR            CL_HPP_ERR_STR_(clEnqueueMapBuffer)
#define __ENQUEUE_MAP_IMAGE_ERR             CL_HPP_ERR_STR_(clEnqueueMapImage)
#define __ENQUEUE_UNMAP_MEM_OBJECT_ERR      CL_HPP_ERR_STR_(clEnqueueUnMapMemObject)
#define __ENQUEUE_NDRANGE_KERNEL_ERR        CL_HPP_ERR_STR_(clEnqueueNDRangeKernel)
#define __ENQUEUE_NATIVE_KERNEL             CL_HPP_ERR_STR_(clEnqueueNativeKernel)
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
#define __ENQUEUE_MIGRATE_MEM_OBJECTS_ERR   CL_HPP_ERR_STR_(clEnqueueMigrateMemObjects)
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120

#define __ENQUEUE_ACQUIRE_GL_ERR            CL_HPP_ERR_STR_(clEnqueueAcquireGLObjects)
#define __ENQUEUE_RELEASE_GL_ERR            CL_HPP_ERR_STR_(clEnqueueReleaseGLObjects)

#define __CREATE_PIPE_ERR             CL_HPP_ERR_STR_(clCreatePipe)
#define __GET_PIPE_INFO_ERR           CL_HPP_ERR_STR_(clGetPipeInfo)


#define __RETAIN_ERR                        CL_HPP_ERR_STR_(Retain Object)
#define __RELEASE_ERR                       CL_HPP_ERR_STR_(Release Object)
#define __FLUSH_ERR                         CL_HPP_ERR_STR_(clFlush)
#define __FINISH_ERR                        CL_HPP_ERR_STR_(clFinish)
#define __VECTOR_CAPACITY_ERR               CL_HPP_ERR_STR_(Vector capacity error)

/**
 * CL 1.2 version that uses device fission.
 */
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
#define __CREATE_SUB_DEVICES_ERR            CL_HPP_ERR_STR_(clCreateSubDevices)
#else
#define __CREATE_SUB_DEVICES_ERR            CL_HPP_ERR_STR_(clCreateSubDevicesEXT)
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120

/**
 * Deprecated APIs for 1.2
 */
#if defined(CL_USE_DEPRECATED_OPENCL_1_1_APIS)
#define __ENQUEUE_MARKER_ERR                CL_HPP_ERR_STR_(clEnqueueMarker)
#define __ENQUEUE_WAIT_FOR_EVENTS_ERR       CL_HPP_ERR_STR_(clEnqueueWaitForEvents)
#define __ENQUEUE_BARRIER_ERR               CL_HPP_ERR_STR_(clEnqueueBarrier)
#define __UNLOAD_COMPILER_ERR               CL_HPP_ERR_STR_(clUnloadCompiler)
#define __CREATE_GL_TEXTURE_2D_ERR          CL_HPP_ERR_STR_(clCreateFromGLTexture2D)
#define __CREATE_GL_TEXTURE_3D_ERR          CL_HPP_ERR_STR_(clCreateFromGLTexture3D)
#define __CREATE_IMAGE2D_ERR                CL_HPP_ERR_STR_(clCreateImage2D)
#define __CREATE_IMAGE3D_ERR                CL_HPP_ERR_STR_(clCreateImage3D)
#endif // #if defined(CL_USE_DEPRECATED_OPENCL_1_1_APIS)

/**
 * Deprecated APIs for 2.0
 */
#if defined(CL_USE_DEPRECATED_OPENCL_1_2_APIS)
#define __CREATE_COMMAND_QUEUE_ERR          CL_HPP_ERR_STR_(clCreateCommandQueue)
#define __ENQUEUE_TASK_ERR                  CL_HPP_ERR_STR_(clEnqueueTask)
#define __CREATE_SAMPLER_ERR                CL_HPP_ERR_STR_(clCreateSampler)
#endif // #if defined(CL_USE_DEPRECATED_OPENCL_1_1_APIS)

/**
 * CL 1.2 marker and barrier commands
 */
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
#define __ENQUEUE_MARKER_WAIT_LIST_ERR                CL_HPP_ERR_STR_(clEnqueueMarkerWithWaitList)
#define __ENQUEUE_BARRIER_WAIT_LIST_ERR               CL_HPP_ERR_STR_(clEnqueueBarrierWithWaitList)
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120

#endif // CL_HPP_USER_OVERRIDE_ERROR_STRINGS
//! \endcond


namespace detail {

// Generic getInfoHelper. The final parameter is used to guide overload
// resolution: the actual parameter passed is an int, which makes this
// a worse conversion sequence than a specialization that declares the
// parameter as an int.
template<typename Functor, typename T>
inline cl_int getInfoHelper(Functor f, cl_uint name, T* param, long)
{
    return f(name, sizeof(T), param, NULL);
}

// Specialized for getInfo<CL_PROGRAM_BINARIES>
// Assumes that the output vector was correctly resized on the way in
template <typename Func>
inline cl_int getInfoHelper(Func f, cl_uint name, vector<vector<unsigned char>>* param, int)
{
    if (name != CL_PROGRAM_BINARIES) {
        return CL_INVALID_VALUE;
    }
    if (param) {
        // Create array of pointers, calculate total size and pass pointer array in
        size_type numBinaries = param->size();
        vector<unsigned char*> binariesPointers(numBinaries);

        for (size_type i = 0; i < numBinaries; ++i)
        {
            binariesPointers[i] = (*param)[i].data();
        }

        cl_int err = f(name, numBinaries * sizeof(unsigned char*), binariesPointers.data(), NULL);

        if (err != CL_SUCCESS) {
            return err;
        }
    }


    return CL_SUCCESS;
}

// Specialized getInfoHelper for vector params
template <typename Func, typename T>
inline cl_int getInfoHelper(Func f, cl_uint name, vector<T>* param, long)
{
    size_type required;
    cl_int err = f(name, 0, NULL, &required);
    if (err != CL_SUCCESS) {
        return err;
    }
    const size_type elements = required / sizeof(T);

    // Temporary to avoid changing param on an error
    vector<T> localData(elements);
    err = f(name, required, localData.data(), NULL);
    if (err != CL_SUCCESS) {
        return err;
    }
    if (param) {
        *param = std::move(localData);
    }

    return CL_SUCCESS;
}

/* Specialization for reference-counted types. This depends on the
 * existence of Wrapper<T>::cl_type, and none of the other types having the
 * cl_type member. Note that simplify specifying the parameter as Wrapper<T>
 * does not work, because when using a derived type (e.g. Context) the generic
 * template will provide a better match.
 */
template <typename Func, typename T>
inline cl_int getInfoHelper(
    Func f, cl_uint name, vector<T>* param, int, typename T::cl_type = 0)
{
    size_type required;
    cl_int err = f(name, 0, NULL, &required);
    if (err != CL_SUCCESS) {
        return err;
    }

    const size_type elements = required / sizeof(typename T::cl_type);

    vector<typename T::cl_type> value(elements);
    err = f(name, required, value.data(), NULL);
    if (err != CL_SUCCESS) {
        return err;
    }

    if (param) {
        // Assign to convert CL type to T for each element
        param->resize(elements);

        // Assign to param, constructing with retain behaviour
        // to correctly capture each underlying CL object
        for (size_type i = 0; i < elements; i++) {
            (*param)[i] = T(value[i], true);
        }
    }
    return CL_SUCCESS;
}

// Specialized GetInfoHelper for string params
template <typename Func>
inline cl_int getInfoHelper(Func f, cl_uint name, string* param, long)
{
    size_type required;
    cl_int err = f(name, 0, NULL, &required);
    if (err != CL_SUCCESS) {
        return err;
    }

    // std::string has a constant data member
    // a char vector does not
    if (required > 0) {
        vector<char> value(required);
        err = f(name, required, value.data(), NULL);
        if (err != CL_SUCCESS) {
            return err;
        }
        if (param) {
            param->assign(begin(value), prev(end(value)));
        }
    }
    else if (param) {
        param->assign("");
    }
    return CL_SUCCESS;
}

// Specialized GetInfoHelper for clsize_t params
template <typename Func, size_type N>
inline cl_int getInfoHelper(Func f, cl_uint name, array<size_type, N>* param, long)
{
    size_type required;
    cl_int err = f(name, 0, NULL, &required);
    if (err != CL_SUCCESS) {
        return err;
    }

    size_type elements = required / sizeof(size_type);
    vector<size_type> value(elements, 0);

    err = f(name, required, value.data(), NULL);
    if (err != CL_SUCCESS) {
        return err;
    }
    
    // Bound the copy with N to prevent overruns
    // if passed N > than the amount copied
    if (elements > N) {
        elements = N;
    }
    for (size_type i = 0; i < elements; ++i) {
        (*param)[i] = value[i];
    }

    return CL_SUCCESS;
}

template<typename T> struct ReferenceHandler;

/* Specialization for reference-counted types. This depends on the
 * existence of Wrapper<T>::cl_type, and none of the other types having the
 * cl_type member. Note that simplify specifying the parameter as Wrapper<T>
 * does not work, because when using a derived type (e.g. Context) the generic
 * template will provide a better match.
 */
template<typename Func, typename T>
inline cl_int getInfoHelper(Func f, cl_uint name, T* param, int, typename T::cl_type = 0)
{
    typename T::cl_type value;
    cl_int err = f(name, sizeof(value), &value, NULL);
    if (err != CL_SUCCESS) {
        return err;
    }
    *param = value;
    if (value != NULL)
    {
        err = param->retain();
        if (err != CL_SUCCESS) {
            return err;
        }
    }
    return CL_SUCCESS;
}

#define CL_HPP_PARAM_NAME_INFO_1_0_(F) \
    F(cl_platform_info, CL_PLATFORM_PROFILE, string) \
    F(cl_platform_info, CL_PLATFORM_VERSION, string) \
    F(cl_platform_info, CL_PLATFORM_NAME, string) \
    F(cl_platform_info, CL_PLATFORM_VENDOR, string) \
    F(cl_platform_info, CL_PLATFORM_EXTENSIONS, string) \
    \
    F(cl_device_info, CL_DEVICE_TYPE, cl_device_type) \
    F(cl_device_info, CL_DEVICE_VENDOR_ID, cl_uint) \
    F(cl_device_info, CL_DEVICE_MAX_COMPUTE_UNITS, cl_uint) \
    F(cl_device_info, CL_DEVICE_MAX_WORK_ITEM_DIMENSIONS, cl_uint) \
    F(cl_device_info, CL_DEVICE_MAX_WORK_GROUP_SIZE, size_type) \
    F(cl_device_info, CL_DEVICE_MAX_WORK_ITEM_SIZES, cl::vector<size_type>) \
    F(cl_device_info, CL_DEVICE_PREFERRED_VECTOR_WIDTH_CHAR, cl_uint) \
    F(cl_device_info, CL_DEVICE_PREFERRED_VECTOR_WIDTH_SHORT, cl_uint) \
    F(cl_device_info, CL_DEVICE_PREFERRED_VECTOR_WIDTH_INT, cl_uint) \
    F(cl_device_info, CL_DEVICE_PREFERRED_VECTOR_WIDTH_LONG, cl_uint) \
    F(cl_device_info, CL_DEVICE_PREFERRED_VECTOR_WIDTH_FLOAT, cl_uint) \
    F(cl_device_info, CL_DEVICE_PREFERRED_VECTOR_WIDTH_DOUBLE, cl_uint) \
    F(cl_device_info, CL_DEVICE_MAX_CLOCK_FREQUENCY, cl_uint) \
    F(cl_device_info, CL_DEVICE_ADDRESS_BITS, cl_uint) \
    F(cl_device_info, CL_DEVICE_MAX_READ_IMAGE_ARGS, cl_uint) \
    F(cl_device_info, CL_DEVICE_MAX_WRITE_IMAGE_ARGS, cl_uint) \
    F(cl_device_info, CL_DEVICE_MAX_MEM_ALLOC_SIZE, cl_ulong) \
    F(cl_device_info, CL_DEVICE_IMAGE2D_MAX_WIDTH, size_type) \
    F(cl_device_info, CL_DEVICE_IMAGE2D_MAX_HEIGHT, size_type) \
    F(cl_device_info, CL_DEVICE_IMAGE3D_MAX_WIDTH, size_type) \
    F(cl_device_info, CL_DEVICE_IMAGE3D_MAX_HEIGHT, size_type) \
    F(cl_device_info, CL_DEVICE_IMAGE3D_MAX_DEPTH, size_type) \
    F(cl_device_info, CL_DEVICE_IMAGE_SUPPORT, cl_bool) \
    F(cl_device_info, CL_DEVICE_MAX_PARAMETER_SIZE, size_type) \
    F(cl_device_info, CL_DEVICE_MAX_SAMPLERS, cl_uint) \
    F(cl_device_info, CL_DEVICE_MEM_BASE_ADDR_ALIGN, cl_uint) \
    F(cl_device_info, CL_DEVICE_MIN_DATA_TYPE_ALIGN_SIZE, cl_uint) \
    F(cl_device_info, CL_DEVICE_SINGLE_FP_CONFIG, cl_device_fp_config) \
    F(cl_device_info, CL_DEVICE_DOUBLE_FP_CONFIG, cl_device_fp_config) \
    F(cl_device_info, CL_DEVICE_HALF_FP_CONFIG, cl_device_fp_config) \
    F(cl_device_info, CL_DEVICE_GLOBAL_MEM_CACHE_TYPE, cl_device_mem_cache_type) \
    F(cl_device_info, CL_DEVICE_GLOBAL_MEM_CACHELINE_SIZE, cl_uint)\
    F(cl_device_info, CL_DEVICE_GLOBAL_MEM_CACHE_SIZE, cl_ulong) \
    F(cl_device_info, CL_DEVICE_GLOBAL_MEM_SIZE, cl_ulong) \
    F(cl_device_info, CL_DEVICE_MAX_CONSTANT_BUFFER_SIZE, cl_ulong) \
    F(cl_device_info, CL_DEVICE_MAX_CONSTANT_ARGS, cl_uint) \
    F(cl_device_info, CL_DEVICE_LOCAL_MEM_TYPE, cl_device_local_mem_type) \
    F(cl_device_info, CL_DEVICE_LOCAL_MEM_SIZE, cl_ulong) \
    F(cl_device_info, CL_DEVICE_ERROR_CORRECTION_SUPPORT, cl_bool) \
    F(cl_device_info, CL_DEVICE_PROFILING_TIMER_RESOLUTION, size_type) \
    F(cl_device_info, CL_DEVICE_ENDIAN_LITTLE, cl_bool) \
    F(cl_device_info, CL_DEVICE_AVAILABLE, cl_bool) \
    F(cl_device_info, CL_DEVICE_COMPILER_AVAILABLE, cl_bool) \
    F(cl_device_info, CL_DEVICE_EXECUTION_CAPABILITIES, cl_device_exec_capabilities) \
    F(cl_device_info, CL_DEVICE_PLATFORM, cl_platform_id) \
    F(cl_device_info, CL_DEVICE_NAME, string) \
    F(cl_device_info, CL_DEVICE_VENDOR, string) \
    F(cl_device_info, CL_DRIVER_VERSION, string) \
    F(cl_device_info, CL_DEVICE_PROFILE, string) \
    F(cl_device_info, CL_DEVICE_VERSION, string) \
    F(cl_device_info, CL_DEVICE_EXTENSIONS, string) \
    \
    F(cl_context_info, CL_CONTEXT_REFERENCE_COUNT, cl_uint) \
    F(cl_context_info, CL_CONTEXT_DEVICES, cl::vector<Device>) \
    F(cl_context_info, CL_CONTEXT_PROPERTIES, cl::vector<cl_context_properties>) \
    \
    F(cl_event_info, CL_EVENT_COMMAND_QUEUE, cl::CommandQueue) \
    F(cl_event_info, CL_EVENT_COMMAND_TYPE, cl_command_type) \
    F(cl_event_info, CL_EVENT_REFERENCE_COUNT, cl_uint) \
    F(cl_event_info, CL_EVENT_COMMAND_EXECUTION_STATUS, cl_int) \
    \
    F(cl_profiling_info, CL_PROFILING_COMMAND_QUEUED, cl_ulong) \
    F(cl_profiling_info, CL_PROFILING_COMMAND_SUBMIT, cl_ulong) \
    F(cl_profiling_info, CL_PROFILING_COMMAND_START, cl_ulong) \
    F(cl_profiling_info, CL_PROFILING_COMMAND_END, cl_ulong) \
    \
    F(cl_mem_info, CL_MEM_TYPE, cl_mem_object_type) \
    F(cl_mem_info, CL_MEM_FLAGS, cl_mem_flags) \
    F(cl_mem_info, CL_MEM_SIZE, size_type) \
    F(cl_mem_info, CL_MEM_HOST_PTR, void*) \
    F(cl_mem_info, CL_MEM_MAP_COUNT, cl_uint) \
    F(cl_mem_info, CL_MEM_REFERENCE_COUNT, cl_uint) \
    F(cl_mem_info, CL_MEM_CONTEXT, cl::Context) \
    \
    F(cl_image_info, CL_IMAGE_FORMAT, cl_image_format) \
    F(cl_image_info, CL_IMAGE_ELEMENT_SIZE, size_type) \
    F(cl_image_info, CL_IMAGE_ROW_PITCH, size_type) \
    F(cl_image_info, CL_IMAGE_SLICE_PITCH, size_type) \
    F(cl_image_info, CL_IMAGE_WIDTH, size_type) \
    F(cl_image_info, CL_IMAGE_HEIGHT, size_type) \
    F(cl_image_info, CL_IMAGE_DEPTH, size_type) \
    \
    F(cl_sampler_info, CL_SAMPLER_REFERENCE_COUNT, cl_uint) \
    F(cl_sampler_info, CL_SAMPLER_CONTEXT, cl::Context) \
    F(cl_sampler_info, CL_SAMPLER_NORMALIZED_COORDS, cl_bool) \
    F(cl_sampler_info, CL_SAMPLER_ADDRESSING_MODE, cl_addressing_mode) \
    F(cl_sampler_info, CL_SAMPLER_FILTER_MODE, cl_filter_mode) \
    \
    F(cl_program_info, CL_PROGRAM_REFERENCE_COUNT, cl_uint) \
    F(cl_program_info, CL_PROGRAM_CONTEXT, cl::Context) \
    F(cl_program_info, CL_PROGRAM_NUM_DEVICES, cl_uint) \
    F(cl_program_info, CL_PROGRAM_DEVICES, cl::vector<Device>) \
    F(cl_program_info, CL_PROGRAM_SOURCE, string) \
    F(cl_program_info, CL_PROGRAM_BINARY_SIZES, cl::vector<size_type>) \
    F(cl_program_info, CL_PROGRAM_BINARIES, cl::vector<cl::vector<unsigned char>>) \
    \
    F(cl_program_build_info, CL_PROGRAM_BUILD_STATUS, cl_build_status) \
    F(cl_program_build_info, CL_PROGRAM_BUILD_OPTIONS, string) \
    F(cl_program_build_info, CL_PROGRAM_BUILD_LOG, string) \
    \
    F(cl_kernel_info, CL_KERNEL_FUNCTION_NAME, string) \
    F(cl_kernel_info, CL_KERNEL_NUM_ARGS, cl_uint) \
    F(cl_kernel_info, CL_KERNEL_REFERENCE_COUNT, cl_uint) \
    F(cl_kernel_info, CL_KERNEL_CONTEXT, cl::Context) \
    F(cl_kernel_info, CL_KERNEL_PROGRAM, cl::Program) \
    \
    F(cl_kernel_work_group_info, CL_KERNEL_WORK_GROUP_SIZE, size_type) \
    F(cl_kernel_work_group_info, CL_KERNEL_COMPILE_WORK_GROUP_SIZE, cl::detail::size_t_array) \
    F(cl_kernel_work_group_info, CL_KERNEL_LOCAL_MEM_SIZE, cl_ulong) \
    \
    F(cl_command_queue_info, CL_QUEUE_CONTEXT, cl::Context) \
    F(cl_command_queue_info, CL_QUEUE_DEVICE, cl::Device) \
    F(cl_command_queue_info, CL_QUEUE_REFERENCE_COUNT, cl_uint) \
    F(cl_command_queue_info, CL_QUEUE_PROPERTIES, cl_command_queue_properties)


#define CL_HPP_PARAM_NAME_INFO_1_1_(F) \
    F(cl_context_info, CL_CONTEXT_NUM_DEVICES, cl_uint)\
    F(cl_device_info, CL_DEVICE_PREFERRED_VECTOR_WIDTH_HALF, cl_uint) \
    F(cl_device_info, CL_DEVICE_NATIVE_VECTOR_WIDTH_CHAR, cl_uint) \
    F(cl_device_info, CL_DEVICE_NATIVE_VECTOR_WIDTH_SHORT, cl_uint) \
    F(cl_device_info, CL_DEVICE_NATIVE_VECTOR_WIDTH_INT, cl_uint) \
    F(cl_device_info, CL_DEVICE_NATIVE_VECTOR_WIDTH_LONG, cl_uint) \
    F(cl_device_info, CL_DEVICE_NATIVE_VECTOR_WIDTH_FLOAT, cl_uint) \
    F(cl_device_info, CL_DEVICE_NATIVE_VECTOR_WIDTH_DOUBLE, cl_uint) \
    F(cl_device_info, CL_DEVICE_NATIVE_VECTOR_WIDTH_HALF, cl_uint) \
    F(cl_device_info, CL_DEVICE_OPENCL_C_VERSION, string) \
    \
    F(cl_mem_info, CL_MEM_ASSOCIATED_MEMOBJECT, cl::Memory) \
    F(cl_mem_info, CL_MEM_OFFSET, size_type) \
    \
    F(cl_kernel_work_group_info, CL_KERNEL_PREFERRED_WORK_GROUP_SIZE_MULTIPLE, size_type) \
    F(cl_kernel_work_group_info, CL_KERNEL_PRIVATE_MEM_SIZE, cl_ulong) \
    \
    F(cl_event_info, CL_EVENT_CONTEXT, cl::Context)

#define CL_HPP_PARAM_NAME_INFO_1_2_(F) \
    F(cl_program_info, CL_PROGRAM_NUM_KERNELS, size_type) \
    F(cl_program_info, CL_PROGRAM_KERNEL_NAMES, string) \
    \
    F(cl_program_build_info, CL_PROGRAM_BINARY_TYPE, cl_program_binary_type) \
    \
    F(cl_kernel_info, CL_KERNEL_ATTRIBUTES, string) \
    \
    F(cl_kernel_arg_info, CL_KERNEL_ARG_ADDRESS_QUALIFIER, cl_kernel_arg_address_qualifier) \
    F(cl_kernel_arg_info, CL_KERNEL_ARG_ACCESS_QUALIFIER, cl_kernel_arg_access_qualifier) \
    F(cl_kernel_arg_info, CL_KERNEL_ARG_TYPE_NAME, string) \
    F(cl_kernel_arg_info, CL_KERNEL_ARG_NAME, string) \
    F(cl_kernel_arg_info, CL_KERNEL_ARG_TYPE_QUALIFIER, cl_kernel_arg_type_qualifier) \
    \
    F(cl_device_info, CL_DEVICE_PARENT_DEVICE, cl::Device) \
    F(cl_device_info, CL_DEVICE_PARTITION_PROPERTIES, cl::vector<cl_device_partition_property>) \
    F(cl_device_info, CL_DEVICE_PARTITION_TYPE, cl::vector<cl_device_partition_property>)  \
    F(cl_device_info, CL_DEVICE_REFERENCE_COUNT, cl_uint) \
    F(cl_device_info, CL_DEVICE_PREFERRED_INTEROP_USER_SYNC, size_type) \
    F(cl_device_info, CL_DEVICE_PARTITION_AFFINITY_DOMAIN, cl_device_affinity_domain) \
    F(cl_device_info, CL_DEVICE_BUILT_IN_KERNELS, string) \
    \
    F(cl_image_info, CL_IMAGE_ARRAY_SIZE, size_type) \
    F(cl_image_info, CL_IMAGE_NUM_MIP_LEVELS, cl_uint) \
    F(cl_image_info, CL_IMAGE_NUM_SAMPLES, cl_uint)

#define CL_HPP_PARAM_NAME_INFO_2_0_(F) \
    F(cl_device_info, CL_DEVICE_QUEUE_ON_HOST_PROPERTIES, cl_command_queue_properties) \
    F(cl_device_info, CL_DEVICE_QUEUE_ON_DEVICE_PROPERTIES, cl_command_queue_properties) \
    F(cl_device_info, CL_DEVICE_QUEUE_ON_DEVICE_PREFERRED_SIZE, cl_uint) \
    F(cl_device_info, CL_DEVICE_QUEUE_ON_DEVICE_MAX_SIZE, cl_uint) \
    F(cl_device_info, CL_DEVICE_MAX_ON_DEVICE_QUEUES, cl_uint) \
    F(cl_device_info, CL_DEVICE_MAX_ON_DEVICE_EVENTS, cl_uint) \
    F(cl_device_info, CL_DEVICE_MAX_PIPE_ARGS, cl_uint) \
    F(cl_device_info, CL_DEVICE_PIPE_MAX_ACTIVE_RESERVATIONS, cl_uint) \
    F(cl_device_info, CL_DEVICE_PIPE_MAX_PACKET_SIZE, cl_uint) \
    F(cl_device_info, CL_DEVICE_SVM_CAPABILITIES, cl_device_svm_capabilities) \
    F(cl_device_info, CL_DEVICE_PREFERRED_PLATFORM_ATOMIC_ALIGNMENT, cl_uint) \
    F(cl_device_info, CL_DEVICE_PREFERRED_GLOBAL_ATOMIC_ALIGNMENT, cl_uint) \
    F(cl_device_info, CL_DEVICE_PREFERRED_LOCAL_ATOMIC_ALIGNMENT, cl_uint) \
    F(cl_command_queue_info, CL_QUEUE_SIZE, cl_uint) \
    F(cl_mem_info, CL_MEM_USES_SVM_POINTER, cl_bool) \
    F(cl_program_build_info, CL_PROGRAM_BUILD_GLOBAL_VARIABLE_TOTAL_SIZE, size_type) \
    F(cl_pipe_info, CL_PIPE_PACKET_SIZE, cl_uint) \
    F(cl_pipe_info, CL_PIPE_MAX_PACKETS, cl_uint)

#define CL_HPP_PARAM_NAME_DEVICE_FISSION_(F) \
    F(cl_device_info, CL_DEVICE_PARENT_DEVICE_EXT, cl_device_id) \
    F(cl_device_info, CL_DEVICE_PARTITION_TYPES_EXT, cl::vector<cl_device_partition_property_ext>) \
    F(cl_device_info, CL_DEVICE_AFFINITY_DOMAINS_EXT, cl::vector<cl_device_partition_property_ext>) \
    F(cl_device_info, CL_DEVICE_REFERENCE_COUNT_EXT , cl_uint) \
    F(cl_device_info, CL_DEVICE_PARTITION_STYLE_EXT, cl::vector<cl_device_partition_property_ext>)

template <typename enum_type, cl_int Name>
struct param_traits {};

#define CL_HPP_DECLARE_PARAM_TRAITS_(token, param_name, T) \
struct token;                                        \
template<>                                           \
struct param_traits<detail:: token,param_name>       \
{                                                    \
    enum { value = param_name };                     \
    typedef T param_type;                            \
};

CL_HPP_PARAM_NAME_INFO_1_0_(CL_HPP_DECLARE_PARAM_TRAITS_)
#if CL_HPP_TARGET_OPENCL_VERSION >= 110
CL_HPP_PARAM_NAME_INFO_1_1_(CL_HPP_DECLARE_PARAM_TRAITS_)
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 110
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
CL_HPP_PARAM_NAME_INFO_1_2_(CL_HPP_DECLARE_PARAM_TRAITS_)
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 110
#if CL_HPP_TARGET_OPENCL_VERSION >= 200
CL_HPP_PARAM_NAME_INFO_2_0_(CL_HPP_DECLARE_PARAM_TRAITS_)
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 110


// Flags deprecated in OpenCL 2.0
#define CL_HPP_PARAM_NAME_INFO_1_0_DEPRECATED_IN_2_0_(F) \
    F(cl_device_info, CL_DEVICE_QUEUE_PROPERTIES, cl_command_queue_properties)

#define CL_HPP_PARAM_NAME_INFO_1_1_DEPRECATED_IN_2_0_(F) \
    F(cl_device_info, CL_DEVICE_HOST_UNIFIED_MEMORY, cl_bool)

#define CL_HPP_PARAM_NAME_INFO_1_2_DEPRECATED_IN_2_0_(F) \
    F(cl_image_info, CL_IMAGE_BUFFER, cl::Buffer)

// Include deprecated query flags based on versions
// Only include deprecated 1.0 flags if 2.0 not active as there is an enum clash
#if CL_HPP_TARGET_OPENCL_VERSION > 100 && CL_HPP_MINIMUM_OPENCL_VERSION < 200 && CL_HPP_TARGET_OPENCL_VERSION < 200
CL_HPP_PARAM_NAME_INFO_1_0_DEPRECATED_IN_2_0_(CL_HPP_DECLARE_PARAM_TRAITS_)
#endif // CL_HPP_MINIMUM_OPENCL_VERSION < 110
#if CL_HPP_TARGET_OPENCL_VERSION > 110 && CL_HPP_MINIMUM_OPENCL_VERSION < 200
CL_HPP_PARAM_NAME_INFO_1_1_DEPRECATED_IN_2_0_(CL_HPP_DECLARE_PARAM_TRAITS_)
#endif // CL_HPP_MINIMUM_OPENCL_VERSION < 120
#if CL_HPP_TARGET_OPENCL_VERSION > 120 && CL_HPP_MINIMUM_OPENCL_VERSION < 200
CL_HPP_PARAM_NAME_INFO_1_2_DEPRECATED_IN_2_0_(CL_HPP_DECLARE_PARAM_TRAITS_)
#endif // CL_HPP_MINIMUM_OPENCL_VERSION < 200

#if defined(CL_HPP_USE_CL_DEVICE_FISSION)
CL_HPP_PARAM_NAME_DEVICE_FISSION_(CL_HPP_DECLARE_PARAM_TRAITS_);
#endif // CL_HPP_USE_CL_DEVICE_FISSION

#ifdef CL_PLATFORM_ICD_SUFFIX_KHR
CL_HPP_DECLARE_PARAM_TRAITS_(cl_platform_info, CL_PLATFORM_ICD_SUFFIX_KHR, string)
#endif

#ifdef CL_DEVICE_PROFILING_TIMER_OFFSET_AMD
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_PROFILING_TIMER_OFFSET_AMD, cl_ulong)
#endif

#ifdef CL_DEVICE_GLOBAL_FREE_MEMORY_AMD
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_GLOBAL_FREE_MEMORY_AMD, vector<size_type>)
#endif
#ifdef CL_DEVICE_SIMD_PER_COMPUTE_UNIT_AMD
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_SIMD_PER_COMPUTE_UNIT_AMD, cl_uint)
#endif
#ifdef CL_DEVICE_SIMD_WIDTH_AMD
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_SIMD_WIDTH_AMD, cl_uint)
#endif
#ifdef CL_DEVICE_SIMD_INSTRUCTION_WIDTH_AMD
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_SIMD_INSTRUCTION_WIDTH_AMD, cl_uint)
#endif
#ifdef CL_DEVICE_WAVEFRONT_WIDTH_AMD
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_WAVEFRONT_WIDTH_AMD, cl_uint)
#endif
#ifdef CL_DEVICE_GLOBAL_MEM_CHANNELS_AMD
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_GLOBAL_MEM_CHANNELS_AMD, cl_uint)
#endif
#ifdef CL_DEVICE_GLOBAL_MEM_CHANNEL_BANKS_AMD
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_GLOBAL_MEM_CHANNEL_BANKS_AMD, cl_uint)
#endif
#ifdef CL_DEVICE_GLOBAL_MEM_CHANNEL_BANK_WIDTH_AMD
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_GLOBAL_MEM_CHANNEL_BANK_WIDTH_AMD, cl_uint)
#endif
#ifdef CL_DEVICE_LOCAL_MEM_SIZE_PER_COMPUTE_UNIT_AMD
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_LOCAL_MEM_SIZE_PER_COMPUTE_UNIT_AMD, cl_uint)
#endif
#ifdef CL_DEVICE_LOCAL_MEM_BANKS_AMD
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_LOCAL_MEM_BANKS_AMD, cl_uint)
#endif

#ifdef CL_DEVICE_COMPUTE_CAPABILITY_MAJOR_NV
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_COMPUTE_CAPABILITY_MAJOR_NV, cl_uint)
#endif
#ifdef CL_DEVICE_COMPUTE_CAPABILITY_MINOR_NV
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_COMPUTE_CAPABILITY_MINOR_NV, cl_uint)
#endif
#ifdef CL_DEVICE_REGISTERS_PER_BLOCK_NV
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_REGISTERS_PER_BLOCK_NV, cl_uint)
#endif
#ifdef CL_DEVICE_WARP_SIZE_NV
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_WARP_SIZE_NV, cl_uint)
#endif
#ifdef CL_DEVICE_GPU_OVERLAP_NV
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_GPU_OVERLAP_NV, cl_bool)
#endif
#ifdef CL_DEVICE_KERNEL_EXEC_TIMEOUT_NV
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_KERNEL_EXEC_TIMEOUT_NV, cl_bool)
#endif
#ifdef CL_DEVICE_INTEGRATED_MEMORY_NV
CL_HPP_DECLARE_PARAM_TRAITS_(cl_device_info, CL_DEVICE_INTEGRATED_MEMORY_NV, cl_bool)
#endif

// Convenience functions

template <typename Func, typename T>
inline cl_int
getInfo(Func f, cl_uint name, T* param)
{
    return getInfoHelper(f, name, param, 0);
}

template <typename Func, typename Arg0>
struct GetInfoFunctor0
{
    Func f_; const Arg0& arg0_;
    cl_int operator ()(
        cl_uint param, size_type size, void* value, size_type* size_ret)
    { return f_(arg0_, param, size, value, size_ret); }
};

template <typename Func, typename Arg0, typename Arg1>
struct GetInfoFunctor1
{
    Func f_; const Arg0& arg0_; const Arg1& arg1_;
    cl_int operator ()(
        cl_uint param, size_type size, void* value, size_type* size_ret)
    { return f_(arg0_, arg1_, param, size, value, size_ret); }
};

template <typename Func, typename Arg0, typename T>
inline cl_int
getInfo(Func f, const Arg0& arg0, cl_uint name, T* param)
{
    GetInfoFunctor0<Func, Arg0> f0 = { f, arg0 };
    return getInfoHelper(f0, name, param, 0);
}

template <typename Func, typename Arg0, typename Arg1, typename T>
inline cl_int
getInfo(Func f, const Arg0& arg0, const Arg1& arg1, cl_uint name, T* param)
{
    GetInfoFunctor1<Func, Arg0, Arg1> f0 = { f, arg0, arg1 };
    return getInfoHelper(f0, name, param, 0);
}


template<typename T>
struct ReferenceHandler
{ };

#if CL_HPP_TARGET_OPENCL_VERSION >= 120
/**
 * OpenCL 1.2 devices do have retain/release.
 */
template <>
struct ReferenceHandler<cl_device_id>
{
    /**
     * Retain the device.
     * \param device A valid device created using createSubDevices
     * \return 
     *   CL_SUCCESS if the function executed successfully.
     *   CL_INVALID_DEVICE if device was not a valid subdevice
     *   CL_OUT_OF_RESOURCES
     *   CL_OUT_OF_HOST_MEMORY
     */
    static cl_int retain(cl_device_id device)
    { return ::clRetainDevice(device); }
    /**
     * Retain the device.
     * \param device A valid device created using createSubDevices
     * \return 
     *   CL_SUCCESS if the function executed successfully.
     *   CL_INVALID_DEVICE if device was not a valid subdevice
     *   CL_OUT_OF_RESOURCES
     *   CL_OUT_OF_HOST_MEMORY
     */
    static cl_int release(cl_device_id device)
    { return ::clReleaseDevice(device); }
};
#else // CL_HPP_TARGET_OPENCL_VERSION >= 120
/**
 * OpenCL 1.1 devices do not have retain/release.
 */
template <>
struct ReferenceHandler<cl_device_id>
{
    // cl_device_id does not have retain().
    static cl_int retain(cl_device_id)
    { return CL_SUCCESS; }
    // cl_device_id does not have release().
    static cl_int release(cl_device_id)
    { return CL_SUCCESS; }
};
#endif // ! (CL_HPP_TARGET_OPENCL_VERSION >= 120)

template <>
struct ReferenceHandler<cl_platform_id>
{
    // cl_platform_id does not have retain().
    static cl_int retain(cl_platform_id)
    { return CL_SUCCESS; }
    // cl_platform_id does not have release().
    static cl_int release(cl_platform_id)
    { return CL_SUCCESS; }
};

template <>
struct ReferenceHandler<cl_context>
{
    static cl_int retain(cl_context context)
    { return ::clRetainContext(context); }
    static cl_int release(cl_context context)
    { return ::clReleaseContext(context); }
};

template <>
struct ReferenceHandler<cl_command_queue>
{
    static cl_int retain(cl_command_queue queue)
    { return ::clRetainCommandQueue(queue); }
    static cl_int release(cl_command_queue queue)
    { return ::clReleaseCommandQueue(queue); }
};

template <>
struct ReferenceHandler<cl_mem>
{
    static cl_int retain(cl_mem memory)
    { return ::clRetainMemObject(memory); }
    static cl_int release(cl_mem memory)
    { return ::clReleaseMemObject(memory); }
};

template <>
struct ReferenceHandler<cl_sampler>
{
    static cl_int retain(cl_sampler sampler)
    { return ::clRetainSampler(sampler); }
    static cl_int release(cl_sampler sampler)
    { return ::clReleaseSampler(sampler); }
};

template <>
struct ReferenceHandler<cl_program>
{
    static cl_int retain(cl_program program)
    { return ::clRetainProgram(program); }
    static cl_int release(cl_program program)
    { return ::clReleaseProgram(program); }
};

template <>
struct ReferenceHandler<cl_kernel>
{
    static cl_int retain(cl_kernel kernel)
    { return ::clRetainKernel(kernel); }
    static cl_int release(cl_kernel kernel)
    { return ::clReleaseKernel(kernel); }
};

template <>
struct ReferenceHandler<cl_event>
{
    static cl_int retain(cl_event event)
    { return ::clRetainEvent(event); }
    static cl_int release(cl_event event)
    { return ::clReleaseEvent(event); }
};


#if CL_HPP_TARGET_OPENCL_VERSION >= 120 && CL_HPP_MINIMUM_OPENCL_VERSION < 120
// Extracts version number with major in the upper 16 bits, minor in the lower 16
static cl_uint getVersion(const vector<char> &versionInfo)
{
    int highVersion = 0;
    int lowVersion = 0;
    int index = 7;
    while(versionInfo[index] != '.' ) {
        highVersion *= 10;
        highVersion += versionInfo[index]-'0';
        ++index;
    }
    ++index;
    while(versionInfo[index] != ' ' &&  versionInfo[index] != '\0') {
        lowVersion *= 10;
        lowVersion += versionInfo[index]-'0';
        ++index;
    }
    return (highVersion << 16) | lowVersion;
}

static cl_uint getPlatformVersion(cl_platform_id platform)
{
    size_type size = 0;
    clGetPlatformInfo(platform, CL_PLATFORM_VERSION, 0, NULL, &size);

    vector<char> versionInfo(size);
    clGetPlatformInfo(platform, CL_PLATFORM_VERSION, size, versionInfo.data(), &size);
    return getVersion(versionInfo);
}

static cl_uint getDevicePlatformVersion(cl_device_id device)
{
    cl_platform_id platform;
    clGetDeviceInfo(device, CL_DEVICE_PLATFORM, sizeof(platform), &platform, NULL);
    return getPlatformVersion(platform);
}

static cl_uint getContextPlatformVersion(cl_context context)
{
    // The platform cannot be queried directly, so we first have to grab a
    // device and obtain its context
    size_type size = 0;
    clGetContextInfo(context, CL_CONTEXT_DEVICES, 0, NULL, &size);
    if (size == 0)
        return 0;
    vector<cl_device_id> devices(size/sizeof(cl_device_id));
    clGetContextInfo(context, CL_CONTEXT_DEVICES, size, devices.data(), NULL);
    return getDevicePlatformVersion(devices[0]);
}
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120 && CL_HPP_MINIMUM_OPENCL_VERSION < 120

template <typename T>
class Wrapper
{
public:
    typedef T cl_type;

protected:
    cl_type object_;

public:
    Wrapper() : object_(NULL) { }
    
    Wrapper(const cl_type &obj, bool retainObject) : object_(obj) 
    {
        if (retainObject) { 
            detail::errHandler(retain(), __RETAIN_ERR); 
        }
    }

    ~Wrapper()
    {
        if (object_ != NULL) { release(); }
    }

    Wrapper(const Wrapper<cl_type>& rhs)
    {
        object_ = rhs.object_;
        detail::errHandler(retain(), __RETAIN_ERR);
    }

    Wrapper(Wrapper<cl_type>&& rhs) CL_HPP_NOEXCEPT_
    {
        object_ = rhs.object_;
        rhs.object_ = NULL;
    }

    Wrapper<cl_type>& operator = (const Wrapper<cl_type>& rhs)
    {
        if (this != &rhs) {
            detail::errHandler(release(), __RELEASE_ERR);
            object_ = rhs.object_;
            detail::errHandler(retain(), __RETAIN_ERR);
        }
        return *this;
    }

    Wrapper<cl_type>& operator = (Wrapper<cl_type>&& rhs)
    {
        if (this != &rhs) {
            detail::errHandler(release(), __RELEASE_ERR);
            object_ = rhs.object_;
            rhs.object_ = NULL;
        }
        return *this;
    }

    Wrapper<cl_type>& operator = (const cl_type &rhs)
    {
        detail::errHandler(release(), __RELEASE_ERR);
        object_ = rhs;
        return *this;
    }

    const cl_type& operator ()() const { return object_; }

    cl_type& operator ()() { return object_; }

    const cl_type get() const { return object_; }

    cl_type get() { return object_; }


protected:
    template<typename Func, typename U>
    friend inline cl_int getInfoHelper(Func, cl_uint, U*, int, typename U::cl_type);

    cl_int retain() const
    {
        if (object_ != nullptr) {
            return ReferenceHandler<cl_type>::retain(object_);
        }
        else {
            return CL_SUCCESS;
        }
    }

    cl_int release() const
    {
        if (object_ != nullptr) {
            return ReferenceHandler<cl_type>::release(object_);
        }
        else {
            return CL_SUCCESS;
        }
    }
};

template <>
class Wrapper<cl_device_id>
{
public:
    typedef cl_device_id cl_type;

protected:
    cl_type object_;
    bool referenceCountable_;

    static bool isReferenceCountable(cl_device_id device)
    {
        bool retVal = false;
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
#if CL_HPP_MINIMUM_OPENCL_VERSION < 120
        if (device != NULL) {
            int version = getDevicePlatformVersion(device);
            if(version > ((1 << 16) + 1)) {
                retVal = true;
            }
        }
#else // CL_HPP_MINIMUM_OPENCL_VERSION < 120
        retVal = true;
#endif // CL_HPP_MINIMUM_OPENCL_VERSION < 120
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120
        return retVal;
    }

public:
    Wrapper() : object_(NULL), referenceCountable_(false) 
    { 
    }
    
    Wrapper(const cl_type &obj, bool retainObject) : 
        object_(obj), 
        referenceCountable_(false) 
    {
        referenceCountable_ = isReferenceCountable(obj); 

        if (retainObject) {
            detail::errHandler(retain(), __RETAIN_ERR);
        }
    }

    ~Wrapper()
    {
        release();
    }
    
    Wrapper(const Wrapper<cl_type>& rhs)
    {
        object_ = rhs.object_;
        referenceCountable_ = isReferenceCountable(object_); 
        detail::errHandler(retain(), __RETAIN_ERR);
    }

    Wrapper(Wrapper<cl_type>&& rhs) CL_HPP_NOEXCEPT_
    {
        object_ = rhs.object_;
        referenceCountable_ = rhs.referenceCountable_;
        rhs.object_ = NULL;
        rhs.referenceCountable_ = false;
    }

    Wrapper<cl_type>& operator = (const Wrapper<cl_type>& rhs)
    {
        if (this != &rhs) {
            detail::errHandler(release(), __RELEASE_ERR);
            object_ = rhs.object_;
            referenceCountable_ = rhs.referenceCountable_;
            detail::errHandler(retain(), __RETAIN_ERR);
        }
        return *this;
    }

    Wrapper<cl_type>& operator = (Wrapper<cl_type>&& rhs)
    {
        if (this != &rhs) {
            detail::errHandler(release(), __RELEASE_ERR);
            object_ = rhs.object_;
            referenceCountable_ = rhs.referenceCountable_;
            rhs.object_ = NULL;
            rhs.referenceCountable_ = false;
        }
        return *this;
    }

    Wrapper<cl_type>& operator = (const cl_type &rhs)
    {
        detail::errHandler(release(), __RELEASE_ERR);
        object_ = rhs;
        referenceCountable_ = isReferenceCountable(object_); 
        return *this;
    }

    const cl_type& operator ()() const { return object_; }

    cl_type& operator ()() { return object_; }

    cl_type get() const { return object_; }

protected:
    template<typename Func, typename U>
    friend inline cl_int getInfoHelper(Func, cl_uint, U*, int, typename U::cl_type);

    template<typename Func, typename U>
    friend inline cl_int getInfoHelper(Func, cl_uint, vector<U>*, int, typename U::cl_type);

    cl_int retain() const
    {
        if( object_ != nullptr && referenceCountable_ ) {
            return ReferenceHandler<cl_type>::retain(object_);
        }
        else {
            return CL_SUCCESS;
        }
    }

    cl_int release() const
    {
        if (object_ != nullptr && referenceCountable_) {
            return ReferenceHandler<cl_type>::release(object_);
        }
        else {
            return CL_SUCCESS;
        }
    }
};

template <typename T>
inline bool operator==(const Wrapper<T> &lhs, const Wrapper<T> &rhs)
{
    return lhs() == rhs();
}

template <typename T>
inline bool operator!=(const Wrapper<T> &lhs, const Wrapper<T> &rhs)
{
    return !operator==(lhs, rhs);
}

} // namespace detail
//! \endcond


using BuildLogType = vector<std::pair<cl::Device, typename detail::param_traits<detail::cl_program_build_info, CL_PROGRAM_BUILD_LOG>::param_type>>;
#if defined(CL_HPP_ENABLE_EXCEPTIONS)
/**
* Exception class for build errors to carry build info
*/
class BuildError : public Error
{
private:
    BuildLogType buildLogs;
public:
    BuildError(cl_int err, const char * errStr, const BuildLogType &vec) : Error(err, errStr), buildLogs(vec)
    {
    }

    BuildLogType getBuildLog() const
    {
        return buildLogs;
    }
};
namespace detail {
    static inline cl_int buildErrHandler(
        cl_int err,
        const char * errStr,
        const BuildLogType &buildLogs)
    {
        if (err != CL_SUCCESS) {
            throw BuildError(err, errStr, buildLogs);
        }
        return err;
    }
} // namespace detail

#else
namespace detail {
    static inline cl_int buildErrHandler(
        cl_int err,
        const char * errStr,
        const BuildLogType &buildLogs)
    {
        (void)buildLogs; // suppress unused variable warning
        (void)errStr;
        return err;
    }
} // namespace detail
#endif // #if defined(CL_HPP_ENABLE_EXCEPTIONS)


/*! \stuct ImageFormat
 *  \brief Adds constructors and member functions for cl_image_format.
 *
 *  \see cl_image_format
 */
struct ImageFormat : public cl_image_format
{
    //! \brief Default constructor - performs no initialization.
    ImageFormat(){}

    //! \brief Initializing constructor.
    ImageFormat(cl_channel_order order, cl_channel_type type)
    {
        image_channel_order = order;
        image_channel_data_type = type;
    }

    //! \brief Assignment operator.
    ImageFormat& operator = (const ImageFormat& rhs)
    {
        if (this != &rhs) {
            this->image_channel_data_type = rhs.image_channel_data_type;
            this->image_channel_order     = rhs.image_channel_order;
        }
        return *this;
    }
};

/*! \brief Class interface for cl_device_id.
 *
 *  \note Copies of these objects are inexpensive, since they don't 'own'
 *        any underlying resources or data structures.
 *
 *  \see cl_device_id
 */
class Device : public detail::Wrapper<cl_device_id>
{
private:
    static std::once_flag default_initialized_;
    static Device default_;
    static cl_int default_error_;

    /*! \brief Create the default context.
    *
    * This sets @c default_ and @c default_error_. It does not throw
    * @c cl::Error.
    */
    static void makeDefault();

    /*! \brief Create the default platform from a provided platform.
    *
    * This sets @c default_. It does not throw
    * @c cl::Error.
    */
    static void makeDefaultProvided(const Device &p) {
        default_ = p;
    }

public:
#ifdef CL_HPP_UNIT_TEST_ENABLE
    /*! \brief Reset the default.
    *
    * This sets @c default_ to an empty value to support cleanup in
    * the unit test framework.
    * This function is not thread safe.
    */
    static void unitTestClearDefault() {
        default_ = Device();
    }
#endif // #ifdef CL_HPP_UNIT_TEST_ENABLE

    //! \brief Default constructor - initializes to NULL.
    Device() : detail::Wrapper<cl_type>() { }

    /*! \brief Constructor from cl_device_id.
     * 
     *  This simply copies the device ID value, which is an inexpensive operation.
     */
    explicit Device(const cl_device_id &device, bool retainObject = false) : 
        detail::Wrapper<cl_type>(device, retainObject) { }

    /*! \brief Returns the first device on the default context.
     *
     *  \see Context::getDefault()
     */
    static Device getDefault(
        cl_int *errResult = NULL)
    {
        std::call_once(default_initialized_, makeDefault);
        detail::errHandler(default_error_);
        if (errResult != NULL) {
            *errResult = default_error_;
        }
        return default_;
    }

    /**
    * Modify the default device to be used by
    * subsequent operations.
    * Will only set the default if no default was previously created.
    * @return updated default device.
    *         Should be compared to the passed value to ensure that it was updated.
    */
    static Device setDefault(const Device &default_device)
    {
        std::call_once(default_initialized_, makeDefaultProvided, std::cref(default_device));
        detail::errHandler(default_error_);
        return default_;
    }

    /*! \brief Assignment operator from cl_device_id.
     * 
     *  This simply copies the device ID value, which is an inexpensive operation.
     */
    Device& operator = (const cl_device_id& rhs)
    {
        detail::Wrapper<cl_type>::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
    * Required for MSVC.
    */
    Device(const Device& dev) : detail::Wrapper<cl_type>(dev) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
    * Required for MSVC.
    */
    Device& operator = (const Device &dev)
    {
        detail::Wrapper<cl_type>::operator=(dev);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
    * Required for MSVC.
    */
    Device(Device&& dev) CL_HPP_NOEXCEPT_ : detail::Wrapper<cl_type>(std::move(dev)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
    * Required for MSVC.
    */
    Device& operator = (Device &&dev)
    {
        detail::Wrapper<cl_type>::operator=(std::move(dev));
        return *this;
    }

    //! \brief Wrapper for clGetDeviceInfo().
    template <typename T>
    cl_int getInfo(cl_device_info name, T* param) const
    {
        return detail::errHandler(
            detail::getInfo(&::clGetDeviceInfo, object_, name, param),
            __GET_DEVICE_INFO_ERR);
    }

    //! \brief Wrapper for clGetDeviceInfo() that returns by value.
    template <cl_int name> typename
    detail::param_traits<detail::cl_device_info, name>::param_type
    getInfo(cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_device_info, name>::param_type param;
        cl_int result = getInfo(name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }

    /**
     * CL 1.2 version
     */
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
    //! \brief Wrapper for clCreateSubDevices().
    cl_int createSubDevices(
        const cl_device_partition_property * properties,
        vector<Device>* devices)
    {
        cl_uint n = 0;
        cl_int err = clCreateSubDevices(object_, properties, 0, NULL, &n);
        if (err != CL_SUCCESS) {
            return detail::errHandler(err, __CREATE_SUB_DEVICES_ERR);
        }

        vector<cl_device_id> ids(n);
        err = clCreateSubDevices(object_, properties, n, ids.data(), NULL);
        if (err != CL_SUCCESS) {
            return detail::errHandler(err, __CREATE_SUB_DEVICES_ERR);
        }

        // Cannot trivially assign because we need to capture intermediates 
        // with safe construction
        if (devices) {
            devices->resize(ids.size());

            // Assign to param, constructing with retain behaviour
            // to correctly capture each underlying CL object
            for (size_type i = 0; i < ids.size(); i++) {
                // We do not need to retain because this device is being created 
                // by the runtime
                (*devices)[i] = Device(ids[i], false);
            }
        }

        return CL_SUCCESS;
    }
#elif defined(CL_HPP_USE_CL_DEVICE_FISSION)

/**
 * CL 1.1 version that uses device fission extension.
 */
    cl_int createSubDevices(
        const cl_device_partition_property_ext * properties,
        vector<Device>* devices)
    {
        typedef CL_API_ENTRY cl_int 
            ( CL_API_CALL * PFN_clCreateSubDevicesEXT)(
                cl_device_id /*in_device*/,
                const cl_device_partition_property_ext * /* properties */,
                cl_uint /*num_entries*/,
                cl_device_id * /*out_devices*/,
                cl_uint * /*num_devices*/ ) CL_EXT_SUFFIX__VERSION_1_1;

        static PFN_clCreateSubDevicesEXT pfn_clCreateSubDevicesEXT = NULL;
        CL_HPP_INIT_CL_EXT_FCN_PTR_(clCreateSubDevicesEXT);

        cl_uint n = 0;
        cl_int err = pfn_clCreateSubDevicesEXT(object_, properties, 0, NULL, &n);
        if (err != CL_SUCCESS) {
            return detail::errHandler(err, __CREATE_SUB_DEVICES_ERR);
        }

        vector<cl_device_id> ids(n);
        err = pfn_clCreateSubDevicesEXT(object_, properties, n, ids.data(), NULL);
        if (err != CL_SUCCESS) {
            return detail::errHandler(err, __CREATE_SUB_DEVICES_ERR);
        }
        // Cannot trivially assign because we need to capture intermediates 
        // with safe construction
        if (devices) {
            devices->resize(ids.size());

            // Assign to param, constructing with retain behaviour
            // to correctly capture each underlying CL object
            for (size_type i = 0; i < ids.size(); i++) {
                // We do not need to retain because this device is being created 
                // by the runtime
                (*devices)[i] = Device(ids[i], false);
            }
        }
        return CL_SUCCESS;
    }
#endif // defined(CL_HPP_USE_CL_DEVICE_FISSION)
};

CL_HPP_DEFINE_STATIC_MEMBER_ std::once_flag Device::default_initialized_;
CL_HPP_DEFINE_STATIC_MEMBER_ Device Device::default_;
CL_HPP_DEFINE_STATIC_MEMBER_ cl_int Device::default_error_ = CL_SUCCESS;

/*! \brief Class interface for cl_platform_id.
 *
 *  \note Copies of these objects are inexpensive, since they don't 'own'
 *        any underlying resources or data structures.
 *
 *  \see cl_platform_id
 */
class Platform : public detail::Wrapper<cl_platform_id>
{
private:
    static std::once_flag default_initialized_;
    static Platform default_;
    static cl_int default_error_;

    /*! \brief Create the default context.
    *
    * This sets @c default_ and @c default_error_. It does not throw
    * @c cl::Error.
    */
    static void makeDefault() {
        /* Throwing an exception from a call_once invocation does not do
        * what we wish, so we catch it and save the error.
        */
#if defined(CL_HPP_ENABLE_EXCEPTIONS)
        try
#endif
        {
            // If default wasn't passed ,generate one
            // Otherwise set it
            cl_uint n = 0;

            cl_int err = ::clGetPlatformIDs(0, NULL, &n);
            if (err != CL_SUCCESS) {
                default_error_ = err;
                return;
            }
            if (n == 0) {
                default_error_ = CL_INVALID_PLATFORM;
                return;
            }

            vector<cl_platform_id> ids(n);
            err = ::clGetPlatformIDs(n, ids.data(), NULL);
            if (err != CL_SUCCESS) {
                default_error_ = err;
                return;
            }

            default_ = Platform(ids[0]);
        }
#if defined(CL_HPP_ENABLE_EXCEPTIONS)
        catch (cl::Error &e) {
            default_error_ = e.err();
        }
#endif
    }

    /*! \brief Create the default platform from a provided platform.
     *
     * This sets @c default_. It does not throw
     * @c cl::Error.
     */
    static void makeDefaultProvided(const Platform &p) {
       default_ = p;
    }
    
public:
#ifdef CL_HPP_UNIT_TEST_ENABLE
    /*! \brief Reset the default.
    *
    * This sets @c default_ to an empty value to support cleanup in
    * the unit test framework.
    * This function is not thread safe.
    */
    static void unitTestClearDefault() {
        default_ = Platform();
    }
#endif // #ifdef CL_HPP_UNIT_TEST_ENABLE

    //! \brief Default constructor - initializes to NULL.
    Platform() : detail::Wrapper<cl_type>()  { }

    /*! \brief Constructor from cl_platform_id.
     * 
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  This simply copies the platform ID value, which is an inexpensive operation.
     */
    explicit Platform(const cl_platform_id &platform, bool retainObject = false) : 
        detail::Wrapper<cl_type>(platform, retainObject) { }

    /*! \brief Assignment operator from cl_platform_id.
     * 
     *  This simply copies the platform ID value, which is an inexpensive operation.
     */
    Platform& operator = (const cl_platform_id& rhs)
    {
        detail::Wrapper<cl_type>::operator=(rhs);
        return *this;
    }

    static Platform getDefault(
        cl_int *errResult = NULL)
    {
        std::call_once(default_initialized_, makeDefault);
        detail::errHandler(default_error_);
        if (errResult != NULL) {
            *errResult = default_error_;
        }
        return default_;
    }

    /**
     * Modify the default platform to be used by 
     * subsequent operations.
     * Will only set the default if no default was previously created.
     * @return updated default platform. 
     *         Should be compared to the passed value to ensure that it was updated.
     */
    static Platform setDefault(const Platform &default_platform)
    {
        std::call_once(default_initialized_, makeDefaultProvided, std::cref(default_platform));
        detail::errHandler(default_error_);
        return default_;
    }

    //! \brief Wrapper for clGetPlatformInfo().
    cl_int getInfo(cl_platform_info name, string* param) const
    {
        return detail::errHandler(
            detail::getInfo(&::clGetPlatformInfo, object_, name, param),
            __GET_PLATFORM_INFO_ERR);
    }

    //! \brief Wrapper for clGetPlatformInfo() that returns by value.
    template <cl_int name> typename
    detail::param_traits<detail::cl_platform_info, name>::param_type
    getInfo(cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_platform_info, name>::param_type param;
        cl_int result = getInfo(name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }

    /*! \brief Gets a list of devices for this platform.
     * 
     *  Wraps clGetDeviceIDs().
     */
    cl_int getDevices(
        cl_device_type type,
        vector<Device>* devices) const
    {
        cl_uint n = 0;
        if( devices == NULL ) {
            return detail::errHandler(CL_INVALID_ARG_VALUE, __GET_DEVICE_IDS_ERR);
        }
        cl_int err = ::clGetDeviceIDs(object_, type, 0, NULL, &n);
        if (err != CL_SUCCESS) {
            return detail::errHandler(err, __GET_DEVICE_IDS_ERR);
        }

        vector<cl_device_id> ids(n);
        err = ::clGetDeviceIDs(object_, type, n, ids.data(), NULL);
        if (err != CL_SUCCESS) {
            return detail::errHandler(err, __GET_DEVICE_IDS_ERR);
        }

        // Cannot trivially assign because we need to capture intermediates 
        // with safe construction
        // We must retain things we obtain from the API to avoid releasing
        // API-owned objects.
        if (devices) {
            devices->resize(ids.size());

            // Assign to param, constructing with retain behaviour
            // to correctly capture each underlying CL object
            for (size_type i = 0; i < ids.size(); i++) {
                (*devices)[i] = Device(ids[i], true);
            }
        }
        return CL_SUCCESS;
    }

#if defined(CL_HPP_USE_DX_INTEROP)
   /*! \brief Get the list of available D3D10 devices.
     *
     *  \param d3d_device_source.
     *
     *  \param d3d_object.
     *
     *  \param d3d_device_set.
     *
     *  \param devices returns a vector of OpenCL D3D10 devices found. The cl::Device
     *  values returned in devices can be used to identify a specific OpenCL
     *  device. If \a devices argument is NULL, this argument is ignored.
     *
     *  \return One of the following values:
     *    - CL_SUCCESS if the function is executed successfully.
     *
     *  The application can query specific capabilities of the OpenCL device(s)
     *  returned by cl::getDevices. This can be used by the application to
     *  determine which device(s) to use.
     *
     * \note In the case that exceptions are enabled and a return value
     * other than CL_SUCCESS is generated, then cl::Error exception is
     * generated.
     */
    cl_int getDevices(
        cl_d3d10_device_source_khr d3d_device_source,
        void *                     d3d_object,
        cl_d3d10_device_set_khr    d3d_device_set,
        vector<Device>* devices) const
    {
        typedef CL_API_ENTRY cl_int (CL_API_CALL *PFN_clGetDeviceIDsFromD3D10KHR)(
            cl_platform_id platform, 
            cl_d3d10_device_source_khr d3d_device_source, 
            void * d3d_object,
            cl_d3d10_device_set_khr d3d_device_set,
            cl_uint num_entries,
            cl_device_id * devices,
            cl_uint* num_devices);

        if( devices == NULL ) {
            return detail::errHandler(CL_INVALID_ARG_VALUE, __GET_DEVICE_IDS_ERR);
        }

        static PFN_clGetDeviceIDsFromD3D10KHR pfn_clGetDeviceIDsFromD3D10KHR = NULL;
        CL_HPP_INIT_CL_EXT_FCN_PTR_PLATFORM_(object_, clGetDeviceIDsFromD3D10KHR);

        cl_uint n = 0;
        cl_int err = pfn_clGetDeviceIDsFromD3D10KHR(
            object_, 
            d3d_device_source, 
            d3d_object,
            d3d_device_set, 
            0, 
            NULL, 
            &n);
        if (err != CL_SUCCESS) {
            return detail::errHandler(err, __GET_DEVICE_IDS_ERR);
        }

        vector<cl_device_id> ids(n);
        err = pfn_clGetDeviceIDsFromD3D10KHR(
            object_, 
            d3d_device_source, 
            d3d_object,
            d3d_device_set,
            n, 
            ids.data(), 
            NULL);
        if (err != CL_SUCCESS) {
            return detail::errHandler(err, __GET_DEVICE_IDS_ERR);
        }

        // Cannot trivially assign because we need to capture intermediates 
        // with safe construction
        // We must retain things we obtain from the API to avoid releasing
        // API-owned objects.
        if (devices) {
            devices->resize(ids.size());

            // Assign to param, constructing with retain behaviour
            // to correctly capture each underlying CL object
            for (size_type i = 0; i < ids.size(); i++) {
                (*devices)[i] = Device(ids[i], true);
            }
        }
        return CL_SUCCESS;
    }
#endif

    /*! \brief Gets a list of available platforms.
     * 
     *  Wraps clGetPlatformIDs().
     */
    static cl_int get(
        vector<Platform>* platforms)
    {
        cl_uint n = 0;

        if( platforms == NULL ) {
            return detail::errHandler(CL_INVALID_ARG_VALUE, __GET_PLATFORM_IDS_ERR);
        }

        cl_int err = ::clGetPlatformIDs(0, NULL, &n);
        if (err != CL_SUCCESS) {
            return detail::errHandler(err, __GET_PLATFORM_IDS_ERR);
        }

        vector<cl_platform_id> ids(n);
        err = ::clGetPlatformIDs(n, ids.data(), NULL);
        if (err != CL_SUCCESS) {
            return detail::errHandler(err, __GET_PLATFORM_IDS_ERR);
        }

        if (platforms) {
            platforms->resize(ids.size());

            // Platforms don't reference count
            for (size_type i = 0; i < ids.size(); i++) {
                (*platforms)[i] = Platform(ids[i]);
            }
        }
        return CL_SUCCESS;
    }

    /*! \brief Gets the first available platform.
     * 
     *  Wraps clGetPlatformIDs(), returning the first result.
     */
    static cl_int get(
        Platform * platform)
    {
        cl_int err;
        Platform default_platform = Platform::getDefault(&err);
        if (platform) {
            *platform = default_platform;
        }
        return err;
    }

    /*! \brief Gets the first available platform, returning it by value.
     *
     * \return Returns a valid platform if one is available.
     *         If no platform is available will return a null platform.
     * Throws an exception if no platforms are available
     * or an error condition occurs.
     * Wraps clGetPlatformIDs(), returning the first result.
     */
    static Platform get(
        cl_int * errResult = NULL)
    {
        cl_int err;
        Platform default_platform = Platform::getDefault(&err);
        if (errResult) {
            *errResult = err;
        }
        return default_platform;
    }    
    
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
    //! \brief Wrapper for clUnloadCompiler().
    cl_int
    unloadCompiler()
    {
        return ::clUnloadPlatformCompiler(object_);
    }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120
}; // class Platform

CL_HPP_DEFINE_STATIC_MEMBER_ std::once_flag Platform::default_initialized_;
CL_HPP_DEFINE_STATIC_MEMBER_ Platform Platform::default_;
CL_HPP_DEFINE_STATIC_MEMBER_ cl_int Platform::default_error_ = CL_SUCCESS;


/**
 * Deprecated APIs for 1.2
 */
#if defined(CL_USE_DEPRECATED_OPENCL_1_1_APIS)
/**
 * Unload the OpenCL compiler.
 * \note Deprecated for OpenCL 1.2. Use Platform::unloadCompiler instead.
 */
inline CL_EXT_PREFIX__VERSION_1_1_DEPRECATED cl_int
UnloadCompiler() CL_EXT_SUFFIX__VERSION_1_1_DEPRECATED;
inline cl_int
UnloadCompiler()
{
    return ::clUnloadCompiler();
}
#endif // #if defined(CL_USE_DEPRECATED_OPENCL_1_1_APIS)

/*! \brief Class interface for cl_context.
 *
 *  \note Copies of these objects are shallow, meaning that the copy will refer
 *        to the same underlying cl_context as the original.  For details, see
 *        clRetainContext() and clReleaseContext().
 *
 *  \see cl_context
 */
class Context 
    : public detail::Wrapper<cl_context>
{
private:
    static std::once_flag default_initialized_;
    static Context default_;
    static cl_int default_error_;

    /*! \brief Create the default context from the default device type in the default platform.
     *
     * This sets @c default_ and @c default_error_. It does not throw
     * @c cl::Error.
     */
    static void makeDefault() {
        /* Throwing an exception from a call_once invocation does not do
         * what we wish, so we catch it and save the error.
         */
#if defined(CL_HPP_ENABLE_EXCEPTIONS)
        try
#endif
        {
#if !defined(__APPLE__) && !defined(__MACOS)
            const Platform &p = Platform::getDefault();
            cl_platform_id defaultPlatform = p();
            cl_context_properties properties[3] = {
                CL_CONTEXT_PLATFORM, (cl_context_properties)defaultPlatform, 0
            };
#else // #if !defined(__APPLE__) && !defined(__MACOS)
            cl_context_properties *properties = nullptr;
#endif // #if !defined(__APPLE__) && !defined(__MACOS)

            default_ = Context(
                CL_DEVICE_TYPE_DEFAULT,
                properties,
                NULL,
                NULL,
                &default_error_);
        }
#if defined(CL_HPP_ENABLE_EXCEPTIONS)
        catch (cl::Error &e) {
            default_error_ = e.err();
        }
#endif
    }


    /*! \brief Create the default context from a provided Context.
     *
     * This sets @c default_. It does not throw
     * @c cl::Error.
     */
    static void makeDefaultProvided(const Context &c) {
        default_ = c;
    }
    
public:
#ifdef CL_HPP_UNIT_TEST_ENABLE
    /*! \brief Reset the default.
    *
    * This sets @c default_ to an empty value to support cleanup in
    * the unit test framework.
    * This function is not thread safe.
    */
    static void unitTestClearDefault() {
        default_ = Context();
    }
#endif // #ifdef CL_HPP_UNIT_TEST_ENABLE

    /*! \brief Constructs a context including a list of specified devices.
     *
     *  Wraps clCreateContext().
     */
    Context(
        const vector<Device>& devices,
        cl_context_properties* properties = NULL,
        void (CL_CALLBACK * notifyFptr)(
            const char *,
            const void *,
            size_type,
            void *) = NULL,
        void* data = NULL,
        cl_int* err = NULL)
    {
        cl_int error;

        size_type numDevices = devices.size();
        vector<cl_device_id> deviceIDs(numDevices);

        for( size_type deviceIndex = 0; deviceIndex < numDevices; ++deviceIndex ) {
            deviceIDs[deviceIndex] = (devices[deviceIndex])();
        }

        object_ = ::clCreateContext(
            properties, (cl_uint) numDevices,
            deviceIDs.data(),
            notifyFptr, data, &error);

        detail::errHandler(error, __CREATE_CONTEXT_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    Context(
        const Device& device,
        cl_context_properties* properties = NULL,
        void (CL_CALLBACK * notifyFptr)(
            const char *,
            const void *,
            size_type,
            void *) = NULL,
        void* data = NULL,
        cl_int* err = NULL)
    {
        cl_int error;

        cl_device_id deviceID = device();

        object_ = ::clCreateContext(
            properties, 1,
            &deviceID,
            notifyFptr, data, &error);

        detail::errHandler(error, __CREATE_CONTEXT_ERR);
        if (err != NULL) {
            *err = error;
        }
    }
    
    /*! \brief Constructs a context including all or a subset of devices of a specified type.
     *
     *  Wraps clCreateContextFromType().
     */
    Context(
        cl_device_type type,
        cl_context_properties* properties = NULL,
        void (CL_CALLBACK * notifyFptr)(
            const char *,
            const void *,
            size_type,
            void *) = NULL,
        void* data = NULL,
        cl_int* err = NULL)
    {
        cl_int error;

#if !defined(__APPLE__) && !defined(__MACOS)
        cl_context_properties prop[4] = {CL_CONTEXT_PLATFORM, 0, 0, 0 };

        if (properties == NULL) {
            // Get a valid platform ID as we cannot send in a blank one
            vector<Platform> platforms;
            error = Platform::get(&platforms);
            if (error != CL_SUCCESS) {
                detail::errHandler(error, __CREATE_CONTEXT_FROM_TYPE_ERR);
                if (err != NULL) {
                    *err = error;
                }
                return;
            }

            // Check the platforms we found for a device of our specified type
            cl_context_properties platform_id = 0;
            for (unsigned int i = 0; i < platforms.size(); i++) {

                vector<Device> devices;

#if defined(CL_HPP_ENABLE_EXCEPTIONS)
                try {
#endif

                    error = platforms[i].getDevices(type, &devices);

#if defined(CL_HPP_ENABLE_EXCEPTIONS)
                } catch (cl::Error& e) {
                    error = e.err();
                }
    // Catch if exceptions are enabled as we don't want to exit if first platform has no devices of type
    // We do error checking next anyway, and can throw there if needed
#endif

                // Only squash CL_SUCCESS and CL_DEVICE_NOT_FOUND
                if (error != CL_SUCCESS && error != CL_DEVICE_NOT_FOUND) {
                    detail::errHandler(error, __CREATE_CONTEXT_FROM_TYPE_ERR);
                    if (err != NULL) {
                        *err = error;
                    }
                }

                if (devices.size() > 0) {
                    platform_id = (cl_context_properties)platforms[i]();
                    break;
                }
            }

            if (platform_id == 0) {
                detail::errHandler(CL_DEVICE_NOT_FOUND, __CREATE_CONTEXT_FROM_TYPE_ERR);
                if (err != NULL) {
                    *err = CL_DEVICE_NOT_FOUND;
                }
                return;
            }

            prop[1] = platform_id;
            properties = &prop[0];
        }
#endif
        object_ = ::clCreateContextFromType(
            properties, type, notifyFptr, data, &error);

        detail::errHandler(error, __CREATE_CONTEXT_FROM_TYPE_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Context(const Context& ctx) : detail::Wrapper<cl_type>(ctx) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Context& operator = (const Context &ctx)
    {
        detail::Wrapper<cl_type>::operator=(ctx);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Context(Context&& ctx) CL_HPP_NOEXCEPT_ : detail::Wrapper<cl_type>(std::move(ctx)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Context& operator = (Context &&ctx)
    {
        detail::Wrapper<cl_type>::operator=(std::move(ctx));
        return *this;
    }


    /*! \brief Returns a singleton context including all devices of CL_DEVICE_TYPE_DEFAULT.
     *
     *  \note All calls to this function return the same cl_context as the first.
     */
    static Context getDefault(cl_int * err = NULL) 
    {
        std::call_once(default_initialized_, makeDefault);
        detail::errHandler(default_error_);
        if (err != NULL) {
            *err = default_error_;
        }
        return default_;
    }

    /**
     * Modify the default context to be used by
     * subsequent operations.
     * Will only set the default if no default was previously created.
     * @return updated default context.
     *         Should be compared to the passed value to ensure that it was updated.
     */
    static Context setDefault(const Context &default_context)
    {
        std::call_once(default_initialized_, makeDefaultProvided, std::cref(default_context));
        detail::errHandler(default_error_);
        return default_;
    }

    //! \brief Default constructor - initializes to NULL.
    Context() : detail::Wrapper<cl_type>() { }

    /*! \brief Constructor from cl_context - takes ownership.
     * 
     *  This effectively transfers ownership of a refcount on the cl_context
     *  into the new Context object.
     */
    explicit Context(const cl_context& context, bool retainObject = false) : 
        detail::Wrapper<cl_type>(context, retainObject) { }

    /*! \brief Assignment operator from cl_context - takes ownership.
     * 
     *  This effectively transfers ownership of a refcount on the rhs and calls
     *  clReleaseContext() on the value previously held by this instance.
     */
    Context& operator = (const cl_context& rhs)
    {
        detail::Wrapper<cl_type>::operator=(rhs);
        return *this;
    }

    //! \brief Wrapper for clGetContextInfo().
    template <typename T>
    cl_int getInfo(cl_context_info name, T* param) const
    {
        return detail::errHandler(
            detail::getInfo(&::clGetContextInfo, object_, name, param),
            __GET_CONTEXT_INFO_ERR);
    }

    //! \brief Wrapper for clGetContextInfo() that returns by value.
    template <cl_int name> typename
    detail::param_traits<detail::cl_context_info, name>::param_type
    getInfo(cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_context_info, name>::param_type param;
        cl_int result = getInfo(name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }

    /*! \brief Gets a list of supported image formats.
     *  
     *  Wraps clGetSupportedImageFormats().
     */
    cl_int getSupportedImageFormats(
        cl_mem_flags flags,
        cl_mem_object_type type,
        vector<ImageFormat>* formats) const
    {
        cl_uint numEntries;
        
        if (!formats) {
            return CL_SUCCESS;
        }

        cl_int err = ::clGetSupportedImageFormats(
           object_, 
           flags,
           type, 
           0, 
           NULL, 
           &numEntries);
        if (err != CL_SUCCESS) {
            return detail::errHandler(err, __GET_SUPPORTED_IMAGE_FORMATS_ERR);
        }

        if (numEntries > 0) {
            vector<ImageFormat> value(numEntries);
            err = ::clGetSupportedImageFormats(
                object_,
                flags,
                type,
                numEntries,
                (cl_image_format*)value.data(),
                NULL);
            if (err != CL_SUCCESS) {
                return detail::errHandler(err, __GET_SUPPORTED_IMAGE_FORMATS_ERR);
            }

            formats->assign(begin(value), end(value));
        }
        else {
            // If no values are being returned, ensure an empty vector comes back
            formats->clear();
        }

        return CL_SUCCESS;
    }
};

inline void Device::makeDefault()
{
    /* Throwing an exception from a call_once invocation does not do
    * what we wish, so we catch it and save the error.
    */
#if defined(CL_HPP_ENABLE_EXCEPTIONS)
    try
#endif
    {
        cl_int error = 0;

        Context context = Context::getDefault(&error);
        detail::errHandler(error, __CREATE_CONTEXT_ERR);

        if (error != CL_SUCCESS) {
            default_error_ = error;
        }
        else {
            default_ = context.getInfo<CL_CONTEXT_DEVICES>()[0];
            default_error_ = CL_SUCCESS;
        }
    }
#if defined(CL_HPP_ENABLE_EXCEPTIONS)
    catch (cl::Error &e) {
        default_error_ = e.err();
    }
#endif
}

CL_HPP_DEFINE_STATIC_MEMBER_ std::once_flag Context::default_initialized_;
CL_HPP_DEFINE_STATIC_MEMBER_ Context Context::default_;
CL_HPP_DEFINE_STATIC_MEMBER_ cl_int Context::default_error_ = CL_SUCCESS;

/*! \brief Class interface for cl_event.
 *
 *  \note Copies of these objects are shallow, meaning that the copy will refer
 *        to the same underlying cl_event as the original.  For details, see
 *        clRetainEvent() and clReleaseEvent().
 *
 *  \see cl_event
 */
class Event : public detail::Wrapper<cl_event>
{
public:
    //! \brief Default constructor - initializes to NULL.
    Event() : detail::Wrapper<cl_type>() { }

    /*! \brief Constructor from cl_event - takes ownership.
     * 
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  This effectively transfers ownership of a refcount on the cl_event
     *  into the new Event object.
     */
    explicit Event(const cl_event& event, bool retainObject = false) : 
        detail::Wrapper<cl_type>(event, retainObject) { }

    /*! \brief Assignment operator from cl_event - takes ownership.
     *
     *  This effectively transfers ownership of a refcount on the rhs and calls
     *  clReleaseEvent() on the value previously held by this instance.
     */
    Event& operator = (const cl_event& rhs)
    {
        detail::Wrapper<cl_type>::operator=(rhs);
        return *this;
    }

    //! \brief Wrapper for clGetEventInfo().
    template <typename T>
    cl_int getInfo(cl_event_info name, T* param) const
    {
        return detail::errHandler(
            detail::getInfo(&::clGetEventInfo, object_, name, param),
            __GET_EVENT_INFO_ERR);
    }

    //! \brief Wrapper for clGetEventInfo() that returns by value.
    template <cl_int name> typename
    detail::param_traits<detail::cl_event_info, name>::param_type
    getInfo(cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_event_info, name>::param_type param;
        cl_int result = getInfo(name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }

    //! \brief Wrapper for clGetEventProfilingInfo().
    template <typename T>
    cl_int getProfilingInfo(cl_profiling_info name, T* param) const
    {
        return detail::errHandler(detail::getInfo(
            &::clGetEventProfilingInfo, object_, name, param),
            __GET_EVENT_PROFILE_INFO_ERR);
    }

    //! \brief Wrapper for clGetEventProfilingInfo() that returns by value.
    template <cl_int name> typename
    detail::param_traits<detail::cl_profiling_info, name>::param_type
    getProfilingInfo(cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_profiling_info, name>::param_type param;
        cl_int result = getProfilingInfo(name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }

    /*! \brief Blocks the calling thread until this event completes.
     * 
     *  Wraps clWaitForEvents().
     */
    cl_int wait() const
    {
        return detail::errHandler(
            ::clWaitForEvents(1, &object_),
            __WAIT_FOR_EVENTS_ERR);
    }

#if CL_HPP_TARGET_OPENCL_VERSION >= 110
    /*! \brief Registers a user callback function for a specific command execution status.
     *
     *  Wraps clSetEventCallback().
     */
    cl_int setCallback(
        cl_int type,
        void (CL_CALLBACK * pfn_notify)(cl_event, cl_int, void *),		
        void * user_data = NULL)
    {
        return detail::errHandler(
            ::clSetEventCallback(
                object_,
                type,
                pfn_notify,
                user_data), 
            __SET_EVENT_CALLBACK_ERR);
    }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 110

    /*! \brief Blocks the calling thread until every event specified is complete.
     * 
     *  Wraps clWaitForEvents().
     */
    static cl_int
    waitForEvents(const vector<Event>& events)
    {
        return detail::errHandler(
            ::clWaitForEvents(
                (cl_uint) events.size(), (events.size() > 0) ? (cl_event*)&events.front() : NULL),
            __WAIT_FOR_EVENTS_ERR);
    }
};

#if CL_HPP_TARGET_OPENCL_VERSION >= 110
/*! \brief Class interface for user events (a subset of cl_event's).
 * 
 *  See Event for details about copy semantics, etc.
 */
class UserEvent : public Event
{
public:
    /*! \brief Constructs a user event on a given context.
     *
     *  Wraps clCreateUserEvent().
     */
    UserEvent(
        const Context& context,
        cl_int * err = NULL)
    {
        cl_int error;
        object_ = ::clCreateUserEvent(
            context(),
            &error);

        detail::errHandler(error, __CREATE_USER_EVENT_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    //! \brief Default constructor - initializes to NULL.
    UserEvent() : Event() { }

    /*! \brief Sets the execution status of a user event object.
     *
     *  Wraps clSetUserEventStatus().
     */
    cl_int setStatus(cl_int status)
    {
        return detail::errHandler(
            ::clSetUserEventStatus(object_,status), 
            __SET_USER_EVENT_STATUS_ERR);
    }
};
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 110

/*! \brief Blocks the calling thread until every event specified is complete.
 * 
 *  Wraps clWaitForEvents().
 */
inline static cl_int
WaitForEvents(const vector<Event>& events)
{
    return detail::errHandler(
        ::clWaitForEvents(
            (cl_uint) events.size(), (events.size() > 0) ? (cl_event*)&events.front() : NULL),
        __WAIT_FOR_EVENTS_ERR);
}

/*! \brief Class interface for cl_mem.
 *
 *  \note Copies of these objects are shallow, meaning that the copy will refer
 *        to the same underlying cl_mem as the original.  For details, see
 *        clRetainMemObject() and clReleaseMemObject().
 *
 *  \see cl_mem
 */
class Memory : public detail::Wrapper<cl_mem>
{
public:
    //! \brief Default constructor - initializes to NULL.
    Memory() : detail::Wrapper<cl_type>() { }

    /*! \brief Constructor from cl_mem - takes ownership.
     *
     *  Optionally transfer ownership of a refcount on the cl_mem
     *  into the new Memory object.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *
     *  See Memory for further details.
     */
    explicit Memory(const cl_mem& memory, bool retainObject) :
        detail::Wrapper<cl_type>(memory, retainObject) { }

    /*! \brief Assignment operator from cl_mem - takes ownership.
     *
     *  This effectively transfers ownership of a refcount on the rhs and calls
     *  clReleaseMemObject() on the value previously held by this instance.
     */
    Memory& operator = (const cl_mem& rhs)
    {
        detail::Wrapper<cl_type>::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Memory(const Memory& mem) : detail::Wrapper<cl_type>(mem) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Memory& operator = (const Memory &mem)
    {
        detail::Wrapper<cl_type>::operator=(mem);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Memory(Memory&& mem) CL_HPP_NOEXCEPT_ : detail::Wrapper<cl_type>(std::move(mem)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Memory& operator = (Memory &&mem)
    {
        detail::Wrapper<cl_type>::operator=(std::move(mem));
        return *this;
    }


    //! \brief Wrapper for clGetMemObjectInfo().
    template <typename T>
    cl_int getInfo(cl_mem_info name, T* param) const
    {
        return detail::errHandler(
            detail::getInfo(&::clGetMemObjectInfo, object_, name, param),
            __GET_MEM_OBJECT_INFO_ERR);
    }

    //! \brief Wrapper for clGetMemObjectInfo() that returns by value.
    template <cl_int name> typename
    detail::param_traits<detail::cl_mem_info, name>::param_type
    getInfo(cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_mem_info, name>::param_type param;
        cl_int result = getInfo(name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }

#if CL_HPP_TARGET_OPENCL_VERSION >= 110
    /*! \brief Registers a callback function to be called when the memory object
     *         is no longer needed.
     *
     *  Wraps clSetMemObjectDestructorCallback().
     *
     *  Repeated calls to this function, for a given cl_mem value, will append
     *  to the list of functions called (in reverse order) when memory object's
     *  resources are freed and the memory object is deleted.
     *
     *  \note
     *  The registered callbacks are associated with the underlying cl_mem
     *  value - not the Memory class instance.
     */
    cl_int setDestructorCallback(
        void (CL_CALLBACK * pfn_notify)(cl_mem, void *),		
        void * user_data = NULL)
    {
        return detail::errHandler(
            ::clSetMemObjectDestructorCallback(
                object_,
                pfn_notify,
                user_data), 
            __SET_MEM_OBJECT_DESTRUCTOR_CALLBACK_ERR);
    }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 110

};

// Pre-declare copy functions
class Buffer;
template< typename IteratorType >
cl_int copy( IteratorType startIterator, IteratorType endIterator, cl::Buffer &buffer );
template< typename IteratorType >
cl_int copy( const cl::Buffer &buffer, IteratorType startIterator, IteratorType endIterator );
template< typename IteratorType >
cl_int copy( const CommandQueue &queue, IteratorType startIterator, IteratorType endIterator, cl::Buffer &buffer );
template< typename IteratorType >
cl_int copy( const CommandQueue &queue, const cl::Buffer &buffer, IteratorType startIterator, IteratorType endIterator );


#if CL_HPP_TARGET_OPENCL_VERSION >= 200
namespace detail
{
    class SVMTraitNull
    {
    public:
        static cl_svm_mem_flags getSVMMemFlags()
        {
            return 0;
        }
    };
} // namespace detail

template<class Trait = detail::SVMTraitNull>
class SVMTraitReadWrite
{
public:
    static cl_svm_mem_flags getSVMMemFlags()
    {
        return CL_MEM_READ_WRITE |
            Trait::getSVMMemFlags();
    }
};

template<class Trait = detail::SVMTraitNull>
class SVMTraitReadOnly
{
public:
    static cl_svm_mem_flags getSVMMemFlags()
    {
        return CL_MEM_READ_ONLY |
            Trait::getSVMMemFlags();
    }
};

template<class Trait = detail::SVMTraitNull>
class SVMTraitWriteOnly
{
public:
    static cl_svm_mem_flags getSVMMemFlags()
    {
        return CL_MEM_WRITE_ONLY |
            Trait::getSVMMemFlags();
    }
};

template<class Trait = SVMTraitReadWrite<>>
class SVMTraitCoarse
{
public:
    static cl_svm_mem_flags getSVMMemFlags()
    {
        return Trait::getSVMMemFlags();
    }
};

template<class Trait = SVMTraitReadWrite<>>
class SVMTraitFine
{
public:
    static cl_svm_mem_flags getSVMMemFlags()
    {
        return CL_MEM_SVM_FINE_GRAIN_BUFFER |
            Trait::getSVMMemFlags();
    }
};

template<class Trait = SVMTraitReadWrite<>>
class SVMTraitAtomic
{
public:
    static cl_svm_mem_flags getSVMMemFlags()
    {
        return
            CL_MEM_SVM_FINE_GRAIN_BUFFER |
            CL_MEM_SVM_ATOMICS |
            Trait::getSVMMemFlags();
    }
};

// Pre-declare SVM map function
template<typename T>
inline cl_int enqueueMapSVM(
    T* ptr,
    cl_bool blocking,
    cl_map_flags flags,
    size_type size,
    const vector<Event>* events = NULL,
    Event* event = NULL);

/**
 * STL-like allocator class for managing SVM objects provided for convenience.
 *
 * Note that while this behaves like an allocator for the purposes of constructing vectors and similar objects,
 * care must be taken when using with smart pointers.
 * The allocator should not be used to construct a unique_ptr if we are using coarse-grained SVM mode because
 * the coarse-grained management behaviour would behave incorrectly with respect to reference counting.
 *
 * Instead the allocator embeds a Deleter which may be used with unique_ptr and is used
 * with the allocate_shared and allocate_ptr supplied operations.
 */
template<typename T, class SVMTrait>
class SVMAllocator {
private:
    Context context_;

public:
    typedef T value_type;
    typedef value_type* pointer;
    typedef const value_type* const_pointer;
    typedef value_type& reference;
    typedef const value_type& const_reference;
    typedef std::size_t size_type;
    typedef std::ptrdiff_t difference_type;

    template<typename U>
    struct rebind
    {
        typedef SVMAllocator<U, SVMTrait> other;
    };

    template<typename U, typename V>
    friend class SVMAllocator;

    SVMAllocator() :
        context_(Context::getDefault())
    {
    }

    explicit SVMAllocator(cl::Context context) :
        context_(context)
    {
    }


    SVMAllocator(const SVMAllocator &other) :
        context_(other.context_)
    {
    }

    template<typename U>
    SVMAllocator(const SVMAllocator<U, SVMTrait> &other) :
        context_(other.context_)
    {
    }

    ~SVMAllocator()
    {
    }

    pointer address(reference r) CL_HPP_NOEXCEPT_
    {
        return std::addressof(r);
    }

    const_pointer address(const_reference r) CL_HPP_NOEXCEPT_
    {
        return std::addressof(r);
    }

    /**
     * Allocate an SVM pointer.
     *
     * If the allocator is coarse-grained, this will take ownership to allow
     * containers to correctly construct data in place. 
     */
    pointer allocate(
        size_type size,
        typename cl::SVMAllocator<void, SVMTrait>::const_pointer = 0)
    {
        // Allocate memory with default alignment matching the size of the type
        void* voidPointer =
            clSVMAlloc(
            context_(),
            SVMTrait::getSVMMemFlags(),
            size*sizeof(T),
            0);
        pointer retValue = reinterpret_cast<pointer>(
            voidPointer);
#if defined(CL_HPP_ENABLE_EXCEPTIONS)
        if (!retValue) {
            std::bad_alloc excep;
            throw excep;
        }
#endif // #if defined(CL_HPP_ENABLE_EXCEPTIONS)

        // If allocation was coarse-grained then map it
        if (!(SVMTrait::getSVMMemFlags() & CL_MEM_SVM_FINE_GRAIN_BUFFER)) {
            cl_int err = enqueueMapSVM(retValue, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE, size*sizeof(T));
            if (err != CL_SUCCESS) {
                std::bad_alloc excep;
                throw excep;
            }
        }

        // If exceptions disabled, return null pointer from allocator
        return retValue;
    }

    void deallocate(pointer p, size_type)
    {
        clSVMFree(context_(), p);
    }

    /**
     * Return the maximum possible allocation size.
     * This is the minimum of the maximum sizes of all devices in the context.
     */
    size_type max_size() const CL_HPP_NOEXCEPT_
    {
        size_type maxSize = std::numeric_limits<size_type>::max() / sizeof(T);

        for (const Device &d : context_.getInfo<CL_CONTEXT_DEVICES>()) {
            maxSize = std::min(
                maxSize, 
                static_cast<size_type>(d.getInfo<CL_DEVICE_MAX_MEM_ALLOC_SIZE>()));
        }

        return maxSize;
    }

    template< class U, class... Args >
    void construct(U* p, Args&&... args)
    {
        new(p)T(args...);
    }

    template< class U >
    void destroy(U* p)
    {
        p->~U();
    }

    /**
     * Returns true if the contexts match.
     */
    inline bool operator==(SVMAllocator const& rhs)
    {
        return (context_==rhs.context_);
    }

    inline bool operator!=(SVMAllocator const& a)
    {
        return !operator==(a);
    }
}; // class SVMAllocator        return cl::pointer<T>(tmp, detail::Deleter<T, Alloc>{alloc, copies});


template<class SVMTrait>
class SVMAllocator<void, SVMTrait> {
public:
    typedef void value_type;
    typedef value_type* pointer;
    typedef const value_type* const_pointer;

    template<typename U>
    struct rebind
    {
        typedef SVMAllocator<U, SVMTrait> other;
    };

    template<typename U, typename V>
    friend class SVMAllocator;
};

#if !defined(CL_HPP_NO_STD_UNIQUE_PTR)
namespace detail
{
    template<class Alloc>
    class Deleter {
    private:
        Alloc alloc_;
        size_type copies_;

    public:
        typedef typename std::allocator_traits<Alloc>::pointer pointer;

        Deleter(const Alloc &alloc, size_type copies) : alloc_{ alloc }, copies_{ copies }
        {
        }

        void operator()(pointer ptr) const {
            Alloc tmpAlloc{ alloc_ };
            std::allocator_traits<Alloc>::destroy(tmpAlloc, std::addressof(*ptr));
            std::allocator_traits<Alloc>::deallocate(tmpAlloc, ptr, copies_);
        }
    };
} // namespace detail

/**
 * Allocation operation compatible with std::allocate_ptr.
 * Creates a unique_ptr<T> by default.
 * This requirement is to ensure that the control block is not
 * allocated in memory inaccessible to the host.
 */
template <class T, class Alloc, class... Args>
cl::pointer<T, detail::Deleter<Alloc>> allocate_pointer(const Alloc &alloc_, Args&&... args)
{
    Alloc alloc(alloc_);
    static const size_type copies = 1;

    // Ensure that creation of the management block and the
    // object are dealt with separately such that we only provide a deleter

    T* tmp = std::allocator_traits<Alloc>::allocate(alloc, copies);
    if (!tmp) {
        std::bad_alloc excep;
        throw excep;
    }
    try {
        std::allocator_traits<Alloc>::construct(
            alloc,
            std::addressof(*tmp),
            std::forward<Args>(args)...);

        return cl::pointer<T, detail::Deleter<Alloc>>(tmp, detail::Deleter<Alloc>{alloc, copies});
    }
    catch (std::bad_alloc b)
    {
        std::allocator_traits<Alloc>::deallocate(alloc, tmp, copies);
        throw;
    }
}

template< class T, class SVMTrait, class... Args >
cl::pointer<T, detail::Deleter<SVMAllocator<T, SVMTrait>>> allocate_svm(Args... args)
{
    SVMAllocator<T, SVMTrait> alloc;
    return cl::allocate_pointer<T>(alloc, args...);
}

template< class T, class SVMTrait, class... Args >
cl::pointer<T, detail::Deleter<SVMAllocator<T, SVMTrait>>> allocate_svm(const cl::Context &c, Args... args)
{
    SVMAllocator<T, SVMTrait> alloc(c);
    return cl::allocate_pointer<T>(alloc, args...);
}
#endif // #if !defined(CL_HPP_NO_STD_UNIQUE_PTR)

/*! \brief Vector alias to simplify contruction of coarse-grained SVM containers.
 * 
 */
template < class T >
using coarse_svm_vector = vector<T, cl::SVMAllocator<int, cl::SVMTraitCoarse<>>>;

/*! \brief Vector alias to simplify contruction of fine-grained SVM containers.
*
*/
template < class T >
using fine_svm_vector = vector<T, cl::SVMAllocator<int, cl::SVMTraitFine<>>>;

/*! \brief Vector alias to simplify contruction of fine-grained SVM containers that support platform atomics.
*
*/
template < class T >
using atomic_svm_vector = vector<T, cl::SVMAllocator<int, cl::SVMTraitAtomic<>>>;

#endif // #if CL_HPP_TARGET_OPENCL_VERSION >= 200


/*! \brief Class interface for Buffer Memory Objects.
 * 
 *  See Memory for details about copy semantics, etc.
 *
 *  \see Memory
 */
class Buffer : public Memory
{
public:

    /*! \brief Constructs a Buffer in a specified context.
     *
     *  Wraps clCreateBuffer().
     *
     *  \param host_ptr Storage to be used if the CL_MEM_USE_HOST_PTR flag was
     *                  specified.  Note alignment & exclusivity requirements.
     */
    Buffer(
        const Context& context,
        cl_mem_flags flags,
        size_type size,
        void* host_ptr = NULL,
        cl_int* err = NULL)
    {
        cl_int error;
        object_ = ::clCreateBuffer(context(), flags, size, host_ptr, &error);

        detail::errHandler(error, __CREATE_BUFFER_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    /*! \brief Constructs a Buffer in the default context.
     *
     *  Wraps clCreateBuffer().
     *
     *  \param host_ptr Storage to be used if the CL_MEM_USE_HOST_PTR flag was
     *                  specified.  Note alignment & exclusivity requirements.
     *
     *  \see Context::getDefault()
     */
    Buffer(
         cl_mem_flags flags,
        size_type size,
        void* host_ptr = NULL,
        cl_int* err = NULL)
    {
        cl_int error;

        Context context = Context::getDefault(err);

        object_ = ::clCreateBuffer(context(), flags, size, host_ptr, &error);

        detail::errHandler(error, __CREATE_BUFFER_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    /*!
     * \brief Construct a Buffer from a host container via iterators.
     * IteratorType must be random access.
     * If useHostPtr is specified iterators must represent contiguous data.
     */
    template< typename IteratorType >
    Buffer(
        IteratorType startIterator,
        IteratorType endIterator,
        bool readOnly,
        bool useHostPtr = false,
        cl_int* err = NULL)
    {
        typedef typename std::iterator_traits<IteratorType>::value_type DataType;
        cl_int error;

        cl_mem_flags flags = 0;
        if( readOnly ) {
            flags |= CL_MEM_READ_ONLY;
        }
        else {
            flags |= CL_MEM_READ_WRITE;
        }
        if( useHostPtr ) {
            flags |= CL_MEM_USE_HOST_PTR;
        }
        
        size_type size = sizeof(DataType)*(endIterator - startIterator);

        Context context = Context::getDefault(err);

        if( useHostPtr ) {
            object_ = ::clCreateBuffer(context(), flags, size, static_cast<DataType*>(&*startIterator), &error);
        } else {
            object_ = ::clCreateBuffer(context(), flags, size, 0, &error);
        }

        detail::errHandler(error, __CREATE_BUFFER_ERR);
        if (err != NULL) {
            *err = error;
        }

        if( !useHostPtr ) {
            error = cl::copy(startIterator, endIterator, *this);
            detail::errHandler(error, __CREATE_BUFFER_ERR);
            if (err != NULL) {
                *err = error;
            }
        }
    }

    /*!
     * \brief Construct a Buffer from a host container via iterators using a specified context.
     * IteratorType must be random access.
     * If useHostPtr is specified iterators must represent contiguous data.
     */
    template< typename IteratorType >
    Buffer(const Context &context, IteratorType startIterator, IteratorType endIterator,
        bool readOnly, bool useHostPtr = false, cl_int* err = NULL);
    
    /*!
    * \brief Construct a Buffer from a host container via iterators using a specified queue.
    * If useHostPtr is specified iterators must be random access.
    */
    template< typename IteratorType >
    Buffer(const CommandQueue &queue, IteratorType startIterator, IteratorType endIterator,
        bool readOnly, bool useHostPtr = false, cl_int* err = NULL);

    //! \brief Default constructor - initializes to NULL.
    Buffer() : Memory() { }

    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with earlier versions.
     *
     *  See Memory for further details.
     */
    explicit Buffer(const cl_mem& buffer, bool retainObject = false) :
        Memory(buffer, retainObject) { }

    /*! \brief Assignment from cl_mem - performs shallow copy.
    *
    *  See Memory for further details.
    */
    Buffer& operator = (const cl_mem& rhs)
    {
        Memory::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Buffer(const Buffer& buf) : Memory(buf) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Buffer& operator = (const Buffer &buf)
    {
        Memory::operator=(buf);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Buffer(Buffer&& buf) CL_HPP_NOEXCEPT_ : Memory(std::move(buf)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Buffer& operator = (Buffer &&buf)
    {
        Memory::operator=(std::move(buf));
        return *this;
    }

#if CL_HPP_TARGET_OPENCL_VERSION >= 110
    /*! \brief Creates a new buffer object from this.
     *
     *  Wraps clCreateSubBuffer().
     */
    Buffer createSubBuffer(
        cl_mem_flags flags,
        cl_buffer_create_type buffer_create_type,
        const void * buffer_create_info,
        cl_int * err = NULL)
    {
        Buffer result;
        cl_int error;
        result.object_ = ::clCreateSubBuffer(
            object_, 
            flags, 
            buffer_create_type, 
            buffer_create_info, 
            &error);

        detail::errHandler(error, __CREATE_SUBBUFFER_ERR);
        if (err != NULL) {
            *err = error;
        }

        return result;
    }		
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 110
};

#if defined (CL_HPP_USE_DX_INTEROP)
/*! \brief Class interface for creating OpenCL buffers from ID3D10Buffer's.
 *
 *  This is provided to facilitate interoperability with Direct3D.
 * 
 *  See Memory for details about copy semantics, etc.
 *
 *  \see Memory
 */
class BufferD3D10 : public Buffer
{
public:
   

    /*! \brief Constructs a BufferD3D10, in a specified context, from a
     *         given ID3D10Buffer.
     *
     *  Wraps clCreateFromD3D10BufferKHR().
     */
    BufferD3D10(
        const Context& context,
        cl_mem_flags flags,
        ID3D10Buffer* bufobj,
        cl_int * err = NULL) : pfn_clCreateFromD3D10BufferKHR(nullptr)
    {
        typedef CL_API_ENTRY cl_mem (CL_API_CALL *PFN_clCreateFromD3D10BufferKHR)(
            cl_context context, cl_mem_flags flags, ID3D10Buffer*  buffer,
            cl_int* errcode_ret);
        PFN_clCreateFromD3D10BufferKHR pfn_clCreateFromD3D10BufferKHR;
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
        vector<cl_context_properties> props = context.getInfo<CL_CONTEXT_PROPERTIES>();
        cl_platform platform = -1;
        for( int i = 0; i < props.size(); ++i ) {
            if( props[i] == CL_CONTEXT_PLATFORM ) {
                platform = props[i+1];
            }
        }
        CL_HPP_INIT_CL_EXT_FCN_PTR_PLATFORM_(platform, clCreateFromD3D10BufferKHR);
#elif CL_HPP_TARGET_OPENCL_VERSION >= 110
        CL_HPP_INIT_CL_EXT_FCN_PTR_(clCreateFromD3D10BufferKHR);
#endif

        cl_int error;
        object_ = pfn_clCreateFromD3D10BufferKHR(
            context(),
            flags,
            bufobj,
            &error);

        detail::errHandler(error, __CREATE_GL_BUFFER_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    //! \brief Default constructor - initializes to NULL.
    BufferD3D10() : Buffer() { }

    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with 
     *                     earlier versions.
     *  See Memory for further details.
     */
    explicit BufferD3D10(const cl_mem& buffer, bool retainObject = false) : 
        Buffer(buffer, retainObject) { }

    /*! \brief Assignment from cl_mem - performs shallow copy.
     *
     *  See Memory for further details.
     */
    BufferD3D10& operator = (const cl_mem& rhs)
    {
        Buffer::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    BufferD3D10(const BufferD3D10& buf) : 
        Buffer(buf) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    BufferD3D10& operator = (const BufferD3D10 &buf)
    {
        Buffer::operator=(buf);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    BufferD3D10(BufferD3D10&& buf) CL_HPP_NOEXCEPT_ : Buffer(std::move(buf)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    BufferD3D10& operator = (BufferD3D10 &&buf)
    {
        Buffer::operator=(std::move(buf));
        return *this;
    }
};
#endif

/*! \brief Class interface for GL Buffer Memory Objects.
 *
 *  This is provided to facilitate interoperability with OpenGL.
 * 
 *  See Memory for details about copy semantics, etc.
 * 
 *  \see Memory
 */
class BufferGL : public Buffer
{
public:
    /*! \brief Constructs a BufferGL in a specified context, from a given
     *         GL buffer.
     *
     *  Wraps clCreateFromGLBuffer().
     */
    BufferGL(
        const Context& context,
        cl_mem_flags flags,
        cl_GLuint bufobj,
        cl_int * err = NULL)
    {
        cl_int error;
        object_ = ::clCreateFromGLBuffer(
            context(),
            flags,
            bufobj,
            &error);

        detail::errHandler(error, __CREATE_GL_BUFFER_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    //! \brief Default constructor - initializes to NULL.
    BufferGL() : Buffer() { }

    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  See Memory for further details.
     */
    explicit BufferGL(const cl_mem& buffer, bool retainObject = false) :
        Buffer(buffer, retainObject) { }

    /*! \brief Assignment from cl_mem - performs shallow copy.
     *
     *  See Memory for further details.
     */
    BufferGL& operator = (const cl_mem& rhs)
    {
        Buffer::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    BufferGL(const BufferGL& buf) : Buffer(buf) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    BufferGL& operator = (const BufferGL &buf)
    {
        Buffer::operator=(buf);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    BufferGL(BufferGL&& buf) CL_HPP_NOEXCEPT_ : Buffer(std::move(buf)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    BufferGL& operator = (BufferGL &&buf)
    {
        Buffer::operator=(std::move(buf));
        return *this;
    }

    //! \brief Wrapper for clGetGLObjectInfo().
    cl_int getObjectInfo(
        cl_gl_object_type *type,
        cl_GLuint * gl_object_name)
    {
        return detail::errHandler(
            ::clGetGLObjectInfo(object_,type,gl_object_name),
            __GET_GL_OBJECT_INFO_ERR);
    }
};

/*! \brief Class interface for GL Render Buffer Memory Objects.
 *
 *  This is provided to facilitate interoperability with OpenGL.
 * 
 *  See Memory for details about copy semantics, etc.
 * 
 *  \see Memory
 */
class BufferRenderGL : public Buffer
{
public:
    /*! \brief Constructs a BufferRenderGL in a specified context, from a given
     *         GL Renderbuffer.
     *
     *  Wraps clCreateFromGLRenderbuffer().
     */
    BufferRenderGL(
        const Context& context,
        cl_mem_flags flags,
        cl_GLuint bufobj,
        cl_int * err = NULL)
    {
        cl_int error;
        object_ = ::clCreateFromGLRenderbuffer(
            context(),
            flags,
            bufobj,
            &error);

        detail::errHandler(error, __CREATE_GL_RENDER_BUFFER_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    //! \brief Default constructor - initializes to NULL.
    BufferRenderGL() : Buffer() { }

    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with 
     *                     earlier versions.
     *  See Memory for further details.
     */
    explicit BufferRenderGL(const cl_mem& buffer, bool retainObject = false) :
        Buffer(buffer, retainObject) { }

    /*! \brief Assignment from cl_mem - performs shallow copy.
     *
     *  See Memory for further details.
     */
    BufferRenderGL& operator = (const cl_mem& rhs)
    {
        Buffer::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    BufferRenderGL(const BufferRenderGL& buf) : Buffer(buf) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    BufferRenderGL& operator = (const BufferRenderGL &buf)
    {
        Buffer::operator=(buf);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    BufferRenderGL(BufferRenderGL&& buf) CL_HPP_NOEXCEPT_ : Buffer(std::move(buf)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    BufferRenderGL& operator = (BufferRenderGL &&buf)
    {
        Buffer::operator=(std::move(buf));
        return *this;
    }

    //! \brief Wrapper for clGetGLObjectInfo().
    cl_int getObjectInfo(
        cl_gl_object_type *type,
        cl_GLuint * gl_object_name)
    {
        return detail::errHandler(
            ::clGetGLObjectInfo(object_,type,gl_object_name),
            __GET_GL_OBJECT_INFO_ERR);
    }
};

/*! \brief C++ base class for Image Memory objects.
 *
 *  See Memory for details about copy semantics, etc.
 * 
 *  \see Memory
 */
class Image : public Memory
{
protected:
    //! \brief Default constructor - initializes to NULL.
    Image() : Memory() { }

    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  See Memory for further details.
     */
    explicit Image(const cl_mem& image, bool retainObject = false) :
        Memory(image, retainObject) { }

    /*! \brief Assignment from cl_mem - performs shallow copy.
     *
     *  See Memory for further details.
     */
    Image& operator = (const cl_mem& rhs)
    {
        Memory::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image(const Image& img) : Memory(img) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image& operator = (const Image &img)
    {
        Memory::operator=(img);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image(Image&& img) CL_HPP_NOEXCEPT_ : Memory(std::move(img)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image& operator = (Image &&img)
    {
        Memory::operator=(std::move(img));
        return *this;
    }


public:
    //! \brief Wrapper for clGetImageInfo().
    template <typename T>
    cl_int getImageInfo(cl_image_info name, T* param) const
    {
        return detail::errHandler(
            detail::getInfo(&::clGetImageInfo, object_, name, param),
            __GET_IMAGE_INFO_ERR);
    }
    
    //! \brief Wrapper for clGetImageInfo() that returns by value.
    template <cl_int name> typename
    detail::param_traits<detail::cl_image_info, name>::param_type
    getImageInfo(cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_image_info, name>::param_type param;
        cl_int result = getImageInfo(name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }
};

#if CL_HPP_TARGET_OPENCL_VERSION >= 120
/*! \brief Class interface for 1D Image Memory objects.
 *
 *  See Memory for details about copy semantics, etc.
 * 
 *  \see Memory
 */
class Image1D : public Image
{
public:
    /*! \brief Constructs a 1D Image in a specified context.
     *
     *  Wraps clCreateImage().
     */
    Image1D(
        const Context& context,
        cl_mem_flags flags,
        ImageFormat format,
        size_type width,
        void* host_ptr = NULL,
        cl_int* err = NULL)
    {
        cl_int error;
        cl_image_desc desc =
        {
            CL_MEM_OBJECT_IMAGE1D,
            width,
            0, 0, 0, 0, 0, 0, 0, 0
        };
        object_ = ::clCreateImage(
            context(), 
            flags, 
            &format, 
            &desc, 
            host_ptr, 
            &error);

        detail::errHandler(error, __CREATE_IMAGE_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    //! \brief Default constructor - initializes to NULL.
    Image1D() { }

    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  See Memory for further details.
     */
    explicit Image1D(const cl_mem& image1D, bool retainObject = false) :
        Image(image1D, retainObject) { }

    /*! \brief Assignment from cl_mem - performs shallow copy.
     *
     *  See Memory for further details.
     */
    Image1D& operator = (const cl_mem& rhs)
    {
        Image::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image1D(const Image1D& img) : Image(img) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image1D& operator = (const Image1D &img)
    {
        Image::operator=(img);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image1D(Image1D&& img) CL_HPP_NOEXCEPT_ : Image(std::move(img)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image1D& operator = (Image1D &&img)
    {
        Image::operator=(std::move(img));
        return *this;
    }

};

/*! \class Image1DBuffer
 * \brief Image interface for 1D buffer images.
 */
class Image1DBuffer : public Image
{
public:
    Image1DBuffer(
        const Context& context,
        cl_mem_flags flags,
        ImageFormat format,
        size_type width,
        const Buffer &buffer,
        cl_int* err = NULL)
    {
        cl_int error;
        cl_image_desc desc =
        {
            CL_MEM_OBJECT_IMAGE1D_BUFFER,
            width,
            0, 0, 0, 0, 0, 0, 0,
            buffer()
        };
        object_ = ::clCreateImage(
            context(), 
            flags, 
            &format, 
            &desc, 
            NULL, 
            &error);

        detail::errHandler(error, __CREATE_IMAGE_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    Image1DBuffer() { }

    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  See Memory for further details.
     */
    explicit Image1DBuffer(const cl_mem& image1D, bool retainObject = false) :
        Image(image1D, retainObject) { }

    Image1DBuffer& operator = (const cl_mem& rhs)
    {
        Image::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image1DBuffer(const Image1DBuffer& img) : Image(img) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image1DBuffer& operator = (const Image1DBuffer &img)
    {
        Image::operator=(img);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image1DBuffer(Image1DBuffer&& img) CL_HPP_NOEXCEPT_ : Image(std::move(img)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image1DBuffer& operator = (Image1DBuffer &&img)
    {
        Image::operator=(std::move(img));
        return *this;
    }

};

/*! \class Image1DArray
 * \brief Image interface for arrays of 1D images.
 */
class Image1DArray : public Image
{
public:
    Image1DArray(
        const Context& context,
        cl_mem_flags flags,
        ImageFormat format,
        size_type arraySize,
        size_type width,
        size_type rowPitch,
        void* host_ptr = NULL,
        cl_int* err = NULL)
    {
        cl_int error;
        cl_image_desc desc =
        {
            CL_MEM_OBJECT_IMAGE1D_ARRAY,
            width,
            0, 0,  // height, depth (unused)
            arraySize,
            rowPitch,
            0, 0, 0, 0
        };
        object_ = ::clCreateImage(
            context(), 
            flags, 
            &format, 
            &desc, 
            host_ptr, 
            &error);

        detail::errHandler(error, __CREATE_IMAGE_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    Image1DArray() { }
  
    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  See Memory for further details.
     */
    explicit Image1DArray(const cl_mem& imageArray, bool retainObject = false) :
        Image(imageArray, retainObject) { }


    Image1DArray& operator = (const cl_mem& rhs)
    {
        Image::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image1DArray(const Image1DArray& img) : Image(img) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image1DArray& operator = (const Image1DArray &img)
    {
        Image::operator=(img);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image1DArray(Image1DArray&& img) CL_HPP_NOEXCEPT_ : Image(std::move(img)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image1DArray& operator = (Image1DArray &&img)
    {
        Image::operator=(std::move(img));
        return *this;
    }

};
#endif // #if CL_HPP_TARGET_OPENCL_VERSION >= 120


/*! \brief Class interface for 2D Image Memory objects.
 *
 *  See Memory for details about copy semantics, etc.
 * 
 *  \see Memory
 */
class Image2D : public Image
{
public:
    /*! \brief Constructs a 2D Image in a specified context.
     *
     *  Wraps clCreateImage().
     */
    Image2D(
        const Context& context,
        cl_mem_flags flags,
        ImageFormat format,
        size_type width,
        size_type height,
        size_type row_pitch = 0,
        void* host_ptr = NULL,
        cl_int* err = NULL)
    {
        cl_int error;
        bool useCreateImage;

#if CL_HPP_TARGET_OPENCL_VERSION >= 120 && CL_HPP_MINIMUM_OPENCL_VERSION < 120
        // Run-time decision based on the actual platform
        {
            cl_uint version = detail::getContextPlatformVersion(context());
            useCreateImage = (version >= 0x10002); // OpenCL 1.2 or above
        }
#elif CL_HPP_TARGET_OPENCL_VERSION >= 120
        useCreateImage = true;
#else
        useCreateImage = false;
#endif

#if CL_HPP_TARGET_OPENCL_VERSION >= 120
        if (useCreateImage)
        {
            cl_image_desc desc =
            {
                CL_MEM_OBJECT_IMAGE2D,
                width,
                height,
                0, 0, // depth, array size (unused)
                row_pitch,
                0, 0, 0, 0
            };
            object_ = ::clCreateImage(
                context(),
                flags,
                &format,
                &desc,
                host_ptr,
                &error);

            detail::errHandler(error, __CREATE_IMAGE_ERR);
            if (err != NULL) {
                *err = error;
            }
        }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120
#if CL_HPP_MINIMUM_OPENCL_VERSION < 120
        if (!useCreateImage)
        {
            object_ = ::clCreateImage2D(
                context(), flags,&format, width, height, row_pitch, host_ptr, &error);

            detail::errHandler(error, __CREATE_IMAGE2D_ERR);
            if (err != NULL) {
                *err = error;
            }
        }
#endif // CL_HPP_MINIMUM_OPENCL_VERSION < 120
    }

#if CL_HPP_TARGET_OPENCL_VERSION >= 200 || defined(CL_HPP_USE_CL_IMAGE2D_FROM_BUFFER_KHR)
    /*! \brief Constructs a 2D Image from a buffer.
    * \note This will share storage with the underlying buffer.
    *
    *  Wraps clCreateImage().
    */
    Image2D(
        const Context& context,
        ImageFormat format,
        const Buffer &sourceBuffer,
        size_type width,
        size_type height,
        size_type row_pitch = 0,
        cl_int* err = nullptr)
    {
        cl_int error;

        cl_image_desc desc =
        {
            CL_MEM_OBJECT_IMAGE2D,
            width,
            height,
            0, 0, // depth, array size (unused)
            row_pitch,
            0, 0, 0,
            // Use buffer as input to image
            sourceBuffer()
        };
        object_ = ::clCreateImage(
            context(),
            0, // flags inherited from buffer
            &format,
            &desc,
            nullptr,
            &error);

        detail::errHandler(error, __CREATE_IMAGE_ERR);
        if (err != nullptr) {
            *err = error;
        }
    }
#endif //#if CL_HPP_TARGET_OPENCL_VERSION >= 200 || defined(CL_HPP_USE_CL_IMAGE2D_FROM_BUFFER_KHR)

#if CL_HPP_TARGET_OPENCL_VERSION >= 200
    /*! \brief Constructs a 2D Image from an image.
    * \note This will share storage with the underlying image but may
    *       reinterpret the channel order and type.
    *
    * The image will be created matching with a descriptor matching the source. 
    *
    * \param order is the channel order to reinterpret the image data as.
    *              The channel order may differ as described in the OpenCL 
    *              2.0 API specification.
    *
    * Wraps clCreateImage().
    */
    Image2D(
        const Context& context,
        cl_channel_order order,
        const Image &sourceImage,
        cl_int* err = nullptr)
    {
        cl_int error;

        // Descriptor fields have to match source image
        size_type sourceWidth = 
            sourceImage.getImageInfo<CL_IMAGE_WIDTH>();
        size_type sourceHeight = 
            sourceImage.getImageInfo<CL_IMAGE_HEIGHT>();
        size_type sourceRowPitch =
            sourceImage.getImageInfo<CL_IMAGE_ROW_PITCH>();
        cl_uint sourceNumMIPLevels =
            sourceImage.getImageInfo<CL_IMAGE_NUM_MIP_LEVELS>();
        cl_uint sourceNumSamples =
            sourceImage.getImageInfo<CL_IMAGE_NUM_SAMPLES>();
        cl_image_format sourceFormat =
            sourceImage.getImageInfo<CL_IMAGE_FORMAT>();

        // Update only the channel order. 
        // Channel format inherited from source.
        sourceFormat.image_channel_order = order;
        cl_image_desc desc =
        {
            CL_MEM_OBJECT_IMAGE2D,
            sourceWidth,
            sourceHeight,
            0, 0, // depth (unused), array size (unused)
            sourceRowPitch,
            0, // slice pitch (unused)
            sourceNumMIPLevels,
            sourceNumSamples,
            // Use buffer as input to image
            sourceImage()
        };
        object_ = ::clCreateImage(
            context(),
            0, // flags should be inherited from mem_object
            &sourceFormat,
            &desc,
            nullptr,
            &error);

        detail::errHandler(error, __CREATE_IMAGE_ERR);
        if (err != nullptr) {
            *err = error;
        }
    }
#endif //#if CL_HPP_TARGET_OPENCL_VERSION >= 200

    //! \brief Default constructor - initializes to NULL.
    Image2D() { }

    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  See Memory for further details.
     */
    explicit Image2D(const cl_mem& image2D, bool retainObject = false) :
        Image(image2D, retainObject) { }

    /*! \brief Assignment from cl_mem - performs shallow copy.
     *
     *  See Memory for further details.
     */
    Image2D& operator = (const cl_mem& rhs)
    {
        Image::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image2D(const Image2D& img) : Image(img) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image2D& operator = (const Image2D &img)
    {
        Image::operator=(img);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image2D(Image2D&& img) CL_HPP_NOEXCEPT_ : Image(std::move(img)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image2D& operator = (Image2D &&img)
    {
        Image::operator=(std::move(img));
        return *this;
    }

};


#if defined(CL_USE_DEPRECATED_OPENCL_1_1_APIS)
/*! \brief Class interface for GL 2D Image Memory objects.
 *
 *  This is provided to facilitate interoperability with OpenGL.
 * 
 *  See Memory for details about copy semantics, etc.
 * 
 *  \see Memory
 *  \note Deprecated for OpenCL 1.2. Please use ImageGL instead.
 */
class CL_EXT_PREFIX__VERSION_1_1_DEPRECATED Image2DGL : public Image2D 
{
public:
    /*! \brief Constructs an Image2DGL in a specified context, from a given
     *         GL Texture.
     *
     *  Wraps clCreateFromGLTexture2D().
     */
    Image2DGL(
        const Context& context,
        cl_mem_flags flags,
        cl_GLenum target,
        cl_GLint  miplevel,
        cl_GLuint texobj,
        cl_int * err = NULL)
    {
        cl_int error;
        object_ = ::clCreateFromGLTexture2D(
            context(),
            flags,
            target,
            miplevel,
            texobj,
            &error);

        detail::errHandler(error, __CREATE_GL_TEXTURE_2D_ERR);
        if (err != NULL) {
            *err = error;
        }

    }
    
    //! \brief Default constructor - initializes to NULL.
    Image2DGL() : Image2D() { }

    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  See Memory for further details.
     */
    explicit Image2DGL(const cl_mem& image, bool retainObject = false) : 
        Image2D(image, retainObject) { }

    /*! \brief Assignment from cl_mem - performs shallow copy.
     *c
     *  See Memory for further details.
     */
    Image2DGL& operator = (const cl_mem& rhs)
    {
        Image2D::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image2DGL(const Image2DGL& img) : Image2D(img) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image2DGL& operator = (const Image2DGL &img)
    {
        Image2D::operator=(img);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image2DGL(Image2DGL&& img) CL_HPP_NOEXCEPT_ : Image2D(std::move(img)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image2DGL& operator = (Image2DGL &&img)
    {
        Image2D::operator=(std::move(img));
        return *this;
    }

} CL_EXT_SUFFIX__VERSION_1_1_DEPRECATED;
#endif // CL_USE_DEPRECATED_OPENCL_1_1_APIS

#if CL_HPP_TARGET_OPENCL_VERSION >= 120
/*! \class Image2DArray
 * \brief Image interface for arrays of 2D images.
 */
class Image2DArray : public Image
{
public:
    Image2DArray(
        const Context& context,
        cl_mem_flags flags,
        ImageFormat format,
        size_type arraySize,
        size_type width,
        size_type height,
        size_type rowPitch,
        size_type slicePitch,
        void* host_ptr = NULL,
        cl_int* err = NULL)
    {
        cl_int error;
        cl_image_desc desc =
        {
            CL_MEM_OBJECT_IMAGE2D_ARRAY,
            width,
            height,
            0,       // depth (unused)
            arraySize,
            rowPitch,
            slicePitch,
            0, 0, 0
        };
        object_ = ::clCreateImage(
            context(), 
            flags, 
            &format, 
            &desc, 
            host_ptr, 
            &error);

        detail::errHandler(error, __CREATE_IMAGE_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    Image2DArray() { }
    
    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  See Memory for further details.
     */
    explicit Image2DArray(const cl_mem& imageArray, bool retainObject = false) : Image(imageArray, retainObject) { }

    Image2DArray& operator = (const cl_mem& rhs)
    {
        Image::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image2DArray(const Image2DArray& img) : Image(img) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image2DArray& operator = (const Image2DArray &img)
    {
        Image::operator=(img);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image2DArray(Image2DArray&& img) CL_HPP_NOEXCEPT_ : Image(std::move(img)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image2DArray& operator = (Image2DArray &&img)
    {
        Image::operator=(std::move(img));
        return *this;
    }
};
#endif // #if CL_HPP_TARGET_OPENCL_VERSION >= 120

/*! \brief Class interface for 3D Image Memory objects.
 *
 *  See Memory for details about copy semantics, etc.
 * 
 *  \see Memory
 */
class Image3D : public Image
{
public:
    /*! \brief Constructs a 3D Image in a specified context.
     *
     *  Wraps clCreateImage().
     */
    Image3D(
        const Context& context,
        cl_mem_flags flags,
        ImageFormat format,
        size_type width,
        size_type height,
        size_type depth,
        size_type row_pitch = 0,
        size_type slice_pitch = 0,
        void* host_ptr = NULL,
        cl_int* err = NULL)
    {
        cl_int error;
        bool useCreateImage;

#if CL_HPP_TARGET_OPENCL_VERSION >= 120 && CL_HPP_MINIMUM_OPENCL_VERSION < 120
        // Run-time decision based on the actual platform
        {
            cl_uint version = detail::getContextPlatformVersion(context());
            useCreateImage = (version >= 0x10002); // OpenCL 1.2 or above
        }
#elif CL_HPP_TARGET_OPENCL_VERSION >= 120
        useCreateImage = true;
#else
        useCreateImage = false;
#endif

#if CL_HPP_TARGET_OPENCL_VERSION >= 120
        if (useCreateImage)
        {
            cl_image_desc desc =
            {
                CL_MEM_OBJECT_IMAGE3D,
                width,
                height,
                depth,
                0,      // array size (unused)
                row_pitch,
                slice_pitch,
                0, 0, 0
            };
            object_ = ::clCreateImage(
                context(), 
                flags, 
                &format, 
                &desc, 
                host_ptr, 
                &error);

            detail::errHandler(error, __CREATE_IMAGE_ERR);
            if (err != NULL) {
                *err = error;
            }
        }
#endif  // CL_HPP_TARGET_OPENCL_VERSION >= 120
#if CL_HPP_MINIMUM_OPENCL_VERSION < 120
        if (!useCreateImage)
        {
            object_ = ::clCreateImage3D(
                context(), flags, &format, width, height, depth, row_pitch,
                slice_pitch, host_ptr, &error);

            detail::errHandler(error, __CREATE_IMAGE3D_ERR);
            if (err != NULL) {
                *err = error;
            }
        }
#endif // CL_HPP_MINIMUM_OPENCL_VERSION < 120
    }

    //! \brief Default constructor - initializes to NULL.
    Image3D() : Image() { }

    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  See Memory for further details.
     */
    explicit Image3D(const cl_mem& image3D, bool retainObject = false) : 
        Image(image3D, retainObject) { }

    /*! \brief Assignment from cl_mem - performs shallow copy.
     *
     *  See Memory for further details.
     */
    Image3D& operator = (const cl_mem& rhs)
    {
        Image::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image3D(const Image3D& img) : Image(img) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image3D& operator = (const Image3D &img)
    {
        Image::operator=(img);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image3D(Image3D&& img) CL_HPP_NOEXCEPT_ : Image(std::move(img)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image3D& operator = (Image3D &&img)
    {
        Image::operator=(std::move(img));
        return *this;
    }
};

#if defined(CL_USE_DEPRECATED_OPENCL_1_1_APIS)
/*! \brief Class interface for GL 3D Image Memory objects.
 *
 *  This is provided to facilitate interoperability with OpenGL.
 * 
 *  See Memory for details about copy semantics, etc.
 * 
 *  \see Memory
 */
class Image3DGL : public Image3D
{
public:
    /*! \brief Constructs an Image3DGL in a specified context, from a given
     *         GL Texture.
     *
     *  Wraps clCreateFromGLTexture3D().
     */
    Image3DGL(
        const Context& context,
        cl_mem_flags flags,
        cl_GLenum target,
        cl_GLint  miplevel,
        cl_GLuint texobj,
        cl_int * err = NULL)
    {
        cl_int error;
        object_ = ::clCreateFromGLTexture3D(
            context(),
            flags,
            target,
            miplevel,
            texobj,
            &error);

        detail::errHandler(error, __CREATE_GL_TEXTURE_3D_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    //! \brief Default constructor - initializes to NULL.
    Image3DGL() : Image3D() { }

    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  See Memory for further details.
     */
    explicit Image3DGL(const cl_mem& image, bool retainObject = false) : 
        Image3D(image, retainObject) { }

    /*! \brief Assignment from cl_mem - performs shallow copy.
     *
     *  See Memory for further details.
     */
    Image3DGL& operator = (const cl_mem& rhs)
    {
        Image3D::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image3DGL(const Image3DGL& img) : Image3D(img) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Image3DGL& operator = (const Image3DGL &img)
    {
        Image3D::operator=(img);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image3DGL(Image3DGL&& img) CL_HPP_NOEXCEPT_ : Image3D(std::move(img)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Image3DGL& operator = (Image3DGL &&img)
    {
        Image3D::operator=(std::move(img));
        return *this;
    }
};
#endif // CL_USE_DEPRECATED_OPENCL_1_1_APIS

#if CL_HPP_TARGET_OPENCL_VERSION >= 120
/*! \class ImageGL
 * \brief general image interface for GL interop.
 * We abstract the 2D and 3D GL images into a single instance here
 * that wraps all GL sourced images on the grounds that setup information
 * was performed by OpenCL anyway.
 */
class ImageGL : public Image
{
public:
    ImageGL(
        const Context& context,
        cl_mem_flags flags,
        cl_GLenum target,
        cl_GLint  miplevel,
        cl_GLuint texobj,
        cl_int * err = NULL)
    {
        cl_int error;
        object_ = ::clCreateFromGLTexture(
            context(), 
            flags, 
            target,
            miplevel,
            texobj,
            &error);

        detail::errHandler(error, __CREATE_GL_TEXTURE_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    ImageGL() : Image() { }
    
    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  See Memory for further details.
     */
    explicit ImageGL(const cl_mem& image, bool retainObject = false) : 
        Image(image, retainObject) { }

    ImageGL& operator = (const cl_mem& rhs)
    {
        Image::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    ImageGL(const ImageGL& img) : Image(img) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    ImageGL& operator = (const ImageGL &img)
    {
        Image::operator=(img);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    ImageGL(ImageGL&& img) CL_HPP_NOEXCEPT_ : Image(std::move(img)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    ImageGL& operator = (ImageGL &&img)
    {
        Image::operator=(std::move(img));
        return *this;
    }
};
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120



#if CL_HPP_TARGET_OPENCL_VERSION >= 200
/*! \brief Class interface for Pipe Memory Objects.
*
*  See Memory for details about copy semantics, etc.
*
*  \see Memory
*/
class Pipe : public Memory
{
public:

    /*! \brief Constructs a Pipe in a specified context.
     *
     * Wraps clCreatePipe().
     * @param context Context in which to create the pipe.
     * @param flags Bitfield. Only CL_MEM_READ_WRITE and CL_MEM_HOST_NO_ACCESS are valid.
     * @param packet_size Size in bytes of a single packet of the pipe.
     * @param max_packets Number of packets that may be stored in the pipe.
     *
     */
    Pipe(
        const Context& context,
        cl_uint packet_size,
        cl_uint max_packets,
        cl_int* err = NULL)
    {
        cl_int error;

        cl_mem_flags flags = CL_MEM_READ_WRITE | CL_MEM_HOST_NO_ACCESS;
        object_ = ::clCreatePipe(context(), flags, packet_size, max_packets, nullptr, &error);

        detail::errHandler(error, __CREATE_PIPE_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    /*! \brief Constructs a Pipe in a the default context.
     *
     * Wraps clCreatePipe().
     * @param flags Bitfield. Only CL_MEM_READ_WRITE and CL_MEM_HOST_NO_ACCESS are valid.
     * @param packet_size Size in bytes of a single packet of the pipe.
     * @param max_packets Number of packets that may be stored in the pipe.
     *
     */
    Pipe(
        cl_uint packet_size,
        cl_uint max_packets,
        cl_int* err = NULL)
    {
        cl_int error;

        Context context = Context::getDefault(err);

        cl_mem_flags flags = CL_MEM_READ_WRITE | CL_MEM_HOST_NO_ACCESS;
        object_ = ::clCreatePipe(context(), flags, packet_size, max_packets, nullptr, &error);

        detail::errHandler(error, __CREATE_PIPE_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    //! \brief Default constructor - initializes to NULL.
    Pipe() : Memory() { }

    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with earlier versions.
     *
     *  See Memory for further details.
     */
    explicit Pipe(const cl_mem& pipe, bool retainObject = false) :
        Memory(pipe, retainObject) { }

    /*! \brief Assignment from cl_mem - performs shallow copy.
     *
     *  See Memory for further details.
     */
    Pipe& operator = (const cl_mem& rhs)
    {
        Memory::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Pipe(const Pipe& pipe) : Memory(pipe) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Pipe& operator = (const Pipe &pipe)
    {
        Memory::operator=(pipe);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Pipe(Pipe&& pipe) CL_HPP_NOEXCEPT_ : Memory(std::move(pipe)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Pipe& operator = (Pipe &&pipe)
    {
        Memory::operator=(std::move(pipe));
        return *this;
    }

    //! \brief Wrapper for clGetMemObjectInfo().
    template <typename T>
    cl_int getInfo(cl_pipe_info name, T* param) const
    {
        return detail::errHandler(
            detail::getInfo(&::clGetPipeInfo, object_, name, param),
            __GET_PIPE_INFO_ERR);
    }

    //! \brief Wrapper for clGetMemObjectInfo() that returns by value.
    template <cl_int name> typename
        detail::param_traits<detail::cl_pipe_info, name>::param_type
        getInfo(cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_pipe_info, name>::param_type param;
        cl_int result = getInfo(name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }
}; // class Pipe
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 200


/*! \brief Class interface for cl_sampler.
 *
 *  \note Copies of these objects are shallow, meaning that the copy will refer
 *        to the same underlying cl_sampler as the original.  For details, see
 *        clRetainSampler() and clReleaseSampler().
 *
 *  \see cl_sampler 
 */
class Sampler : public detail::Wrapper<cl_sampler>
{
public:
    //! \brief Default constructor - initializes to NULL.
    Sampler() { }

    /*! \brief Constructs a Sampler in a specified context.
     *
     *  Wraps clCreateSampler().
     */
    Sampler(
        const Context& context,
        cl_bool normalized_coords,
        cl_addressing_mode addressing_mode,
        cl_filter_mode filter_mode,
        cl_int* err = NULL)
    {
        cl_int error;

#if CL_HPP_TARGET_OPENCL_VERSION >= 200
        cl_sampler_properties sampler_properties[] = {
            CL_SAMPLER_NORMALIZED_COORDS, normalized_coords,
            CL_SAMPLER_ADDRESSING_MODE, addressing_mode,
            CL_SAMPLER_FILTER_MODE, filter_mode,
            0 };
        object_ = ::clCreateSamplerWithProperties(
            context(),
            sampler_properties,
            &error);

        detail::errHandler(error, __CREATE_SAMPLER_WITH_PROPERTIES_ERR);
        if (err != NULL) {
            *err = error;
        }
#else
        object_ = ::clCreateSampler(
            context(),
            normalized_coords,
            addressing_mode,
            filter_mode,
            &error);

        detail::errHandler(error, __CREATE_SAMPLER_ERR);
        if (err != NULL) {
            *err = error;
        }
#endif        
    }

    /*! \brief Constructor from cl_sampler - takes ownership.
     * 
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  This effectively transfers ownership of a refcount on the cl_sampler
     *  into the new Sampler object.
     */
    explicit Sampler(const cl_sampler& sampler, bool retainObject = false) : 
        detail::Wrapper<cl_type>(sampler, retainObject) { }

    /*! \brief Assignment operator from cl_sampler - takes ownership.
     *
     *  This effectively transfers ownership of a refcount on the rhs and calls
     *  clReleaseSampler() on the value previously held by this instance.
     */
    Sampler& operator = (const cl_sampler& rhs)
    {
        detail::Wrapper<cl_type>::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Sampler(const Sampler& sam) : detail::Wrapper<cl_type>(sam) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Sampler& operator = (const Sampler &sam)
    {
        detail::Wrapper<cl_type>::operator=(sam);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Sampler(Sampler&& sam) CL_HPP_NOEXCEPT_ : detail::Wrapper<cl_type>(std::move(sam)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Sampler& operator = (Sampler &&sam)
    {
        detail::Wrapper<cl_type>::operator=(std::move(sam));
        return *this;
    }

    //! \brief Wrapper for clGetSamplerInfo().
    template <typename T>
    cl_int getInfo(cl_sampler_info name, T* param) const
    {
        return detail::errHandler(
            detail::getInfo(&::clGetSamplerInfo, object_, name, param),
            __GET_SAMPLER_INFO_ERR);
    }

    //! \brief Wrapper for clGetSamplerInfo() that returns by value.
    template <cl_int name> typename
    detail::param_traits<detail::cl_sampler_info, name>::param_type
    getInfo(cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_sampler_info, name>::param_type param;
        cl_int result = getInfo(name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }
};

class Program;
class CommandQueue;
class DeviceCommandQueue;
class Kernel;

//! \brief Class interface for specifying NDRange values.
class NDRange
{
private:
    size_type sizes_[3];
    cl_uint dimensions_;

public:
    //! \brief Default constructor - resulting range has zero dimensions.
    NDRange()
        : dimensions_(0)
    {
        sizes_[0] = 0;
        sizes_[1] = 0;
        sizes_[2] = 0;
    }

    //! \brief Constructs one-dimensional range.
    NDRange(size_type size0)
        : dimensions_(1)
    {
        sizes_[0] = size0;
        sizes_[1] = 1;
        sizes_[2] = 1;
    }

    //! \brief Constructs two-dimensional range.
    NDRange(size_type size0, size_type size1)
        : dimensions_(2)
    {
        sizes_[0] = size0;
        sizes_[1] = size1;
        sizes_[2] = 1;
    }

    //! \brief Constructs three-dimensional range.
    NDRange(size_type size0, size_type size1, size_type size2)
        : dimensions_(3)
    {
        sizes_[0] = size0;
        sizes_[1] = size1;
        sizes_[2] = size2;
    }

    /*! \brief Conversion operator to const size_type *.
     *  
     *  \returns a pointer to the size of the first dimension.
     */
    operator const size_type*() const { 
        return sizes_; 
    }

    //! \brief Queries the number of dimensions in the range.
    size_type dimensions() const 
    { 
        return dimensions_; 
    }

    //! \brief Returns the size of the object in bytes based on the
    // runtime number of dimensions
    size_type size() const
    {
        return dimensions_*sizeof(size_type);
    }

    size_type* get()
    {
        return sizes_;
    }
    
    const size_type* get() const
    {
        return sizes_;
    }
};

//! \brief A zero-dimensional range.
static const NDRange NullRange;

//! \brief Local address wrapper for use with Kernel::setArg
struct LocalSpaceArg
{
    size_type size_;
};

namespace detail {

template <typename T, class Enable = void>
struct KernelArgumentHandler;

// Enable for objects that are not subclasses of memory
// Pointers, constants etc
template <typename T>
struct KernelArgumentHandler<T, typename std::enable_if<!std::is_base_of<cl::Memory, T>::value>::type>
{
    static size_type size(const T&) { return sizeof(T); }
    static const T* ptr(const T& value) { return &value; }
};

// Enable for subclasses of memory where we want to get a reference to the cl_mem out
// and pass that in for safety
template <typename T>
struct KernelArgumentHandler<T, typename std::enable_if<std::is_base_of<cl::Memory, T>::value>::type>
{
    static size_type size(const T&) { return sizeof(cl_mem); }
    static const cl_mem* ptr(const T& value) { return &(value()); }
};

// Specialization for DeviceCommandQueue defined later

template <>
struct KernelArgumentHandler<LocalSpaceArg, void>
{
    static size_type size(const LocalSpaceArg& value) { return value.size_; }
    static const void* ptr(const LocalSpaceArg&) { return NULL; }
};

} 
//! \endcond

/*! Local
 * \brief Helper function for generating LocalSpaceArg objects.
 */
inline LocalSpaceArg
Local(size_type size)
{
    LocalSpaceArg ret = { size };
    return ret;
}

/*! \brief Class interface for cl_kernel.
 *
 *  \note Copies of these objects are shallow, meaning that the copy will refer
 *        to the same underlying cl_kernel as the original.  For details, see
 *        clRetainKernel() and clReleaseKernel().
 *
 *  \see cl_kernel
 */
class Kernel : public detail::Wrapper<cl_kernel>
{
public:
    inline Kernel(const Program& program, const char* name, cl_int* err = NULL);

    //! \brief Default constructor - initializes to NULL.
    Kernel() { }

    /*! \brief Constructor from cl_kernel - takes ownership.
     * 
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     *  This effectively transfers ownership of a refcount on the cl_kernel
     *  into the new Kernel object.
     */
    explicit Kernel(const cl_kernel& kernel, bool retainObject = false) : 
        detail::Wrapper<cl_type>(kernel, retainObject) { }

    /*! \brief Assignment operator from cl_kernel - takes ownership.
     *
     *  This effectively transfers ownership of a refcount on the rhs and calls
     *  clReleaseKernel() on the value previously held by this instance.
     */
    Kernel& operator = (const cl_kernel& rhs)
    {
        detail::Wrapper<cl_type>::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Kernel(const Kernel& kernel) : detail::Wrapper<cl_type>(kernel) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Kernel& operator = (const Kernel &kernel)
    {
        detail::Wrapper<cl_type>::operator=(kernel);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Kernel(Kernel&& kernel) CL_HPP_NOEXCEPT_ : detail::Wrapper<cl_type>(std::move(kernel)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Kernel& operator = (Kernel &&kernel)
    {
        detail::Wrapper<cl_type>::operator=(std::move(kernel));
        return *this;
    }

    template <typename T>
    cl_int getInfo(cl_kernel_info name, T* param) const
    {
        return detail::errHandler(
            detail::getInfo(&::clGetKernelInfo, object_, name, param),
            __GET_KERNEL_INFO_ERR);
    }

    template <cl_int name> typename
    detail::param_traits<detail::cl_kernel_info, name>::param_type
    getInfo(cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_kernel_info, name>::param_type param;
        cl_int result = getInfo(name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }

#if CL_HPP_TARGET_OPENCL_VERSION >= 120
    template <typename T>
    cl_int getArgInfo(cl_uint argIndex, cl_kernel_arg_info name, T* param) const
    {
        return detail::errHandler(
            detail::getInfo(&::clGetKernelArgInfo, object_, argIndex, name, param),
            __GET_KERNEL_ARG_INFO_ERR);
    }

    template <cl_int name> typename
    detail::param_traits<detail::cl_kernel_arg_info, name>::param_type
    getArgInfo(cl_uint argIndex, cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_kernel_arg_info, name>::param_type param;
        cl_int result = getArgInfo(argIndex, name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120

    template <typename T>
    cl_int getWorkGroupInfo(
        const Device& device, cl_kernel_work_group_info name, T* param) const
    {
        return detail::errHandler(
            detail::getInfo(
                &::clGetKernelWorkGroupInfo, object_, device(), name, param),
                __GET_KERNEL_WORK_GROUP_INFO_ERR);
    }

    template <cl_int name> typename
    detail::param_traits<detail::cl_kernel_work_group_info, name>::param_type
        getWorkGroupInfo(const Device& device, cl_int* err = NULL) const
    {
        typename detail::param_traits<
        detail::cl_kernel_work_group_info, name>::param_type param;
        cl_int result = getWorkGroupInfo(device, name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }
    
#if CL_HPP_TARGET_OPENCL_VERSION >= 200
#if defined(CL_HPP_USE_CL_SUB_GROUPS_KHR)
    cl_int getSubGroupInfo(const cl::Device &dev, cl_kernel_sub_group_info name, const cl::NDRange &range, size_type* param) const
    {
        typedef clGetKernelSubGroupInfoKHR_fn PFN_clGetKernelSubGroupInfoKHR;
        static PFN_clGetKernelSubGroupInfoKHR pfn_clGetKernelSubGroupInfoKHR = NULL;
        CL_HPP_INIT_CL_EXT_FCN_PTR_(clGetKernelSubGroupInfoKHR);

        return detail::errHandler(
            pfn_clGetKernelSubGroupInfoKHR(object_, dev(), name, range.size(), range.get(), sizeof(size_type), param, nullptr),
            __GET_KERNEL_ARG_INFO_ERR);
    }

    template <cl_int name>
        size_type getSubGroupInfo(const cl::Device &dev, const cl::NDRange &range, cl_int* err = NULL) const
    {
        size_type param;
        cl_int result = getSubGroupInfo(dev, name, range, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }
#endif // #if defined(CL_HPP_USE_CL_SUB_GROUPS_KHR)
#endif // #if CL_HPP_TARGET_OPENCL_VERSION >= 200

#if CL_HPP_TARGET_OPENCL_VERSION >= 200
    /*! \brief setArg overload taking a shared_ptr type
     */
    template<typename T, class D>
    cl_int setArg(cl_uint index, const cl::pointer<T, D> &argPtr)
    {
        return detail::errHandler(
            ::clSetKernelArgSVMPointer(object_, index, argPtr.get()),
            __SET_KERNEL_ARGS_ERR);
    }

    /*! \brief setArg overload taking a vector type.
     */
    template<typename T, class Alloc>
    cl_int setArg(cl_uint index, const cl::vector<T, Alloc> &argPtr)
    {
        return detail::errHandler(
            ::clSetKernelArgSVMPointer(object_, index, argPtr.data()),
            __SET_KERNEL_ARGS_ERR);
    }

    /*! \brief setArg overload taking a pointer type
     */
    template<typename T>
    typename std::enable_if<std::is_pointer<T>::value, cl_int>::type
        setArg(cl_uint index, const T argPtr)
    {
        return detail::errHandler(
            ::clSetKernelArgSVMPointer(object_, index, argPtr),
            __SET_KERNEL_ARGS_ERR);
    }
#endif // #if CL_HPP_TARGET_OPENCL_VERSION >= 200

    /*! \brief setArg overload taking a POD type
     */
    template <typename T>
    typename std::enable_if<!std::is_pointer<T>::value, cl_int>::type
        setArg(cl_uint index, const T &value)
    {
        return detail::errHandler(
            ::clSetKernelArg(
                object_,
                index,
                detail::KernelArgumentHandler<T>::size(value),
                detail::KernelArgumentHandler<T>::ptr(value)),
            __SET_KERNEL_ARGS_ERR);
    }

    cl_int setArg(cl_uint index, size_type size, const void* argPtr)
    {
        return detail::errHandler(
            ::clSetKernelArg(object_, index, size, argPtr),
            __SET_KERNEL_ARGS_ERR);
    }

#if CL_HPP_TARGET_OPENCL_VERSION >= 200
    /*!
     * Specify a vector of SVM pointers that the kernel may access in 
     * addition to its arguments.
     */
    cl_int setSVMPointers(const vector<void*> &pointerList)
    {
        return detail::errHandler(
            ::clSetKernelExecInfo(
                object_,
                CL_KERNEL_EXEC_INFO_SVM_PTRS,
                sizeof(void*)*pointerList.size(),
                pointerList.data()));
    }

    /*!
     * Specify a std::array of SVM pointers that the kernel may access in
     * addition to its arguments.
     */
    template<int ArrayLength>
    cl_int setSVMPointers(const std::array<void*, ArrayLength> &pointerList)
    {
        return detail::errHandler(
            ::clSetKernelExecInfo(
                object_,
                CL_KERNEL_EXEC_INFO_SVM_PTRS,
                sizeof(void*)*pointerList.size(),
                pointerList.data()));
    }

    /*! \brief Enable fine-grained system SVM.
     *
     * \note It is only possible to enable fine-grained system SVM if all devices
     *       in the context associated with kernel support it.
     * 
     * \param svmEnabled True if fine-grained system SVM is requested. False otherwise.
     * \return CL_SUCCESS if the function was executed succesfully. CL_INVALID_OPERATION
     *         if no devices in the context support fine-grained system SVM.
     *
     * \see clSetKernelExecInfo
     */
    cl_int enableFineGrainedSystemSVM(bool svmEnabled)
    {
        cl_bool svmEnabled_ = svmEnabled ? CL_TRUE : CL_FALSE;
        return detail::errHandler(
            ::clSetKernelExecInfo(
                object_,
                CL_KERNEL_EXEC_INFO_SVM_FINE_GRAIN_SYSTEM,
                sizeof(cl_bool),
                &svmEnabled_
                )
            );
    }
    
    template<int index, int ArrayLength, class D, typename T0, typename T1, typename... Ts>
    void setSVMPointersHelper(std::array<void*, ArrayLength> &pointerList, const pointer<T0, D> &t0, const pointer<T1, D> &t1, Ts & ... ts)
    {
        pointerList[index] = static_cast<void*>(t0.get());
        setSVMPointersHelper<index + 1, ArrayLength>(pointerList, t1, ts...);
    }

    template<int index, int ArrayLength, typename T0, typename T1, typename... Ts>
    typename std::enable_if<std::is_pointer<T0>::value, void>::type
    setSVMPointersHelper(std::array<void*, ArrayLength> &pointerList, T0 t0, T1 t1, Ts... ts)
    {
        pointerList[index] = static_cast<void*>(t0);
        setSVMPointersHelper<index + 1, ArrayLength>(pointerList, t1, ts...);
    }

    template<int index, int ArrayLength, typename T0, class D>
    void setSVMPointersHelper(std::array<void*, ArrayLength> &pointerList, const pointer<T0, D> &t0)
    {
        pointerList[index] = static_cast<void*>(t0.get());
    }


    template<int index, int ArrayLength, typename T0>
    typename std::enable_if<std::is_pointer<T0>::value, void>::type
    setSVMPointersHelper(std::array<void*, ArrayLength> &pointerList, T0 t0)
    {
        pointerList[index] = static_cast<void*>(t0);
    }

    template<typename T0, typename... Ts>
    cl_int setSVMPointers(const T0 &t0, Ts & ... ts)
    {
        std::array<void*, 1 + sizeof...(Ts)> pointerList;

        setSVMPointersHelper<0, 1 + sizeof...(Ts)>(pointerList, t0, ts...);
        return detail::errHandler(
            ::clSetKernelExecInfo(
            object_,
            CL_KERNEL_EXEC_INFO_SVM_PTRS,
            sizeof(void*)*(1 + sizeof...(Ts)),
            pointerList.data()));
    }
#endif // #if CL_HPP_TARGET_OPENCL_VERSION >= 200
};

/*! \class Program
 * \brief Program interface that implements cl_program.
 */
class Program : public detail::Wrapper<cl_program>
{
public:
#if !defined(CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY)
    typedef vector<vector<unsigned char>> Binaries;
    typedef vector<string> Sources;
#else // #if !defined(CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY)
    typedef vector<std::pair<const void*, size_type> > Binaries;
    typedef vector<std::pair<const char*, size_type> > Sources;
#endif // #if !defined(CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY)
    
    Program(
        const string& source,
        bool build = false,
        cl_int* err = NULL)
    {
        cl_int error;

        const char * strings = source.c_str();
        const size_type length  = source.size();

        Context context = Context::getDefault(err);

        object_ = ::clCreateProgramWithSource(
            context(), (cl_uint)1, &strings, &length, &error);

        detail::errHandler(error, __CREATE_PROGRAM_WITH_SOURCE_ERR);

        if (error == CL_SUCCESS && build) {

            error = ::clBuildProgram(
                object_,
                0,
                NULL,
#if !defined(CL_HPP_CL_1_2_DEFAULT_BUILD)
                "-cl-std=CL2.0",
#else
                "",
#endif // #if !defined(CL_HPP_CL_1_2_DEFAULT_BUILD)
                NULL,
                NULL);

            detail::buildErrHandler(error, __BUILD_PROGRAM_ERR, getBuildInfo<CL_PROGRAM_BUILD_LOG>());
        }

        if (err != NULL) {
            *err = error;
        }
    }

    Program(
        const Context& context,
        const string& source,
        bool build = false,
        cl_int* err = NULL)
    {
        cl_int error;

        const char * strings = source.c_str();
        const size_type length  = source.size();

        object_ = ::clCreateProgramWithSource(
            context(), (cl_uint)1, &strings, &length, &error);

        detail::errHandler(error, __CREATE_PROGRAM_WITH_SOURCE_ERR);

        if (error == CL_SUCCESS && build) {
            error = ::clBuildProgram(
                object_,
                0,
                NULL,
#if !defined(CL_HPP_CL_1_2_DEFAULT_BUILD)
                "-cl-std=CL2.0",
#else
                "",
#endif // #if !defined(CL_HPP_CL_1_2_DEFAULT_BUILD)
                NULL,
                NULL);
            
            detail::buildErrHandler(error, __BUILD_PROGRAM_ERR, getBuildInfo<CL_PROGRAM_BUILD_LOG>());
        }

        if (err != NULL) {
            *err = error;
        }
    }

    /**
     * Create a program from a vector of source strings and the default context.
     * Does not compile or link the program.
     */
    Program(
        const Sources& sources,
        cl_int* err = NULL)
    {
        cl_int error;
        Context context = Context::getDefault(err);

        const size_type n = (size_type)sources.size();

        vector<size_type> lengths(n);
        vector<const char*> strings(n);

        for (size_type i = 0; i < n; ++i) {
#if !defined(CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY)
            strings[i] = sources[(int)i].data();
            lengths[i] = sources[(int)i].length();
#else // #if !defined(CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY)
            strings[i] = sources[(int)i].first;
            lengths[i] = sources[(int)i].second;
#endif // #if !defined(CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY)
        }

        object_ = ::clCreateProgramWithSource(
            context(), (cl_uint)n, strings.data(), lengths.data(), &error);

        detail::errHandler(error, __CREATE_PROGRAM_WITH_SOURCE_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    /**
     * Create a program from a vector of source strings and a provided context.
     * Does not compile or link the program.
     */
    Program(
        const Context& context,
        const Sources& sources,
        cl_int* err = NULL)
    {
        cl_int error;

        const size_type n = (size_type)sources.size();

        vector<size_type> lengths(n);
        vector<const char*> strings(n);

        for (size_type i = 0; i < n; ++i) {
#if !defined(CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY)
            strings[i] = sources[(int)i].data();
            lengths[i] = sources[(int)i].length();
#else // #if !defined(CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY)
            strings[i] = sources[(int)i].first;
            lengths[i] = sources[(int)i].second;
#endif // #if !defined(CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY)
        }

        object_ = ::clCreateProgramWithSource(
            context(), (cl_uint)n, strings.data(), lengths.data(), &error);

        detail::errHandler(error, __CREATE_PROGRAM_WITH_SOURCE_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    /**
     * Construct a program object from a list of devices and a per-device list of binaries.
     * \param context A valid OpenCL context in which to construct the program.
     * \param devices A vector of OpenCL device objects for which the program will be created.
     * \param binaries A vector of pairs of a pointer to a binary object and its length.
     * \param binaryStatus An optional vector that on completion will be resized to
     *   match the size of binaries and filled with values to specify if each binary
     *   was successfully loaded.
     *   Set to CL_SUCCESS if the binary was successfully loaded.
     *   Set to CL_INVALID_VALUE if the length is 0 or the binary pointer is NULL.
     *   Set to CL_INVALID_BINARY if the binary provided is not valid for the matching device.
     * \param err if non-NULL will be set to CL_SUCCESS on successful operation or one of the following errors:
     *   CL_INVALID_CONTEXT if context is not a valid context.
     *   CL_INVALID_VALUE if the length of devices is zero; or if the length of binaries does not match the length of devices; 
     *     or if any entry in binaries is NULL or has length 0.
     *   CL_INVALID_DEVICE if OpenCL devices listed in devices are not in the list of devices associated with context.
     *   CL_INVALID_BINARY if an invalid program binary was encountered for any device. binaryStatus will return specific status for each device.
     *   CL_OUT_OF_HOST_MEMORY if there is a failure to allocate resources required by the OpenCL implementation on the host.
     */
    Program(
        const Context& context,
        const vector<Device>& devices,
        const Binaries& binaries,
        vector<cl_int>* binaryStatus = NULL,
        cl_int* err = NULL)
    {
        cl_int error;
        
        const size_type numDevices = devices.size();
        
        // Catch size mismatch early and return
        if(binaries.size() != numDevices) {
            error = CL_INVALID_VALUE;
            detail::errHandler(error, __CREATE_PROGRAM_WITH_BINARY_ERR);
            if (err != NULL) {
                *err = error;
            }
            return;
        }


        vector<size_type> lengths(numDevices);
        vector<const unsigned char*> images(numDevices);
#if !defined(CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY)
        for (size_type i = 0; i < numDevices; ++i) {
            images[i] = binaries[i].data();
            lengths[i] = binaries[(int)i].size();
        }
#else // #if !defined(CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY)
        for (size_type i = 0; i < numDevices; ++i) {
            images[i] = (const unsigned char*)binaries[i].first;
            lengths[i] = binaries[(int)i].second;
        }
#endif // #if !defined(CL_HPP_ENABLE_PROGRAM_CONSTRUCTION_FROM_ARRAY_COMPATIBILITY)
        
        vector<cl_device_id> deviceIDs(numDevices);
        for( size_type deviceIndex = 0; deviceIndex < numDevices; ++deviceIndex ) {
            deviceIDs[deviceIndex] = (devices[deviceIndex])();
        }

        if(binaryStatus) {
            binaryStatus->resize(numDevices);
        }
        
        object_ = ::clCreateProgramWithBinary(
            context(), (cl_uint) devices.size(),
            deviceIDs.data(),
            lengths.data(), images.data(), (binaryStatus != NULL && numDevices > 0)
               ? &binaryStatus->front()
               : NULL, &error);

        detail::errHandler(error, __CREATE_PROGRAM_WITH_BINARY_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
    /**
     * Create program using builtin kernels.
     * \param kernelNames Semi-colon separated list of builtin kernel names
     */
    Program(
        const Context& context,
        const vector<Device>& devices,
        const string& kernelNames,
        cl_int* err = NULL)
    {
        cl_int error;


        size_type numDevices = devices.size();
        vector<cl_device_id> deviceIDs(numDevices);
        for( size_type deviceIndex = 0; deviceIndex < numDevices; ++deviceIndex ) {
            deviceIDs[deviceIndex] = (devices[deviceIndex])();
        }
        
        object_ = ::clCreateProgramWithBuiltInKernels(
            context(), 
            (cl_uint) devices.size(),
            deviceIDs.data(),
            kernelNames.c_str(), 
            &error);

        detail::errHandler(error, __CREATE_PROGRAM_WITH_BUILT_IN_KERNELS_ERR);
        if (err != NULL) {
            *err = error;
        }
    }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120

    Program() { }
    

    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     */
    explicit Program(const cl_program& program, bool retainObject = false) : 
        detail::Wrapper<cl_type>(program, retainObject) { }

    Program& operator = (const cl_program& rhs)
    {
        detail::Wrapper<cl_type>::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Program(const Program& program) : detail::Wrapper<cl_type>(program) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    Program& operator = (const Program &program)
    {
        detail::Wrapper<cl_type>::operator=(program);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Program(Program&& program) CL_HPP_NOEXCEPT_ : detail::Wrapper<cl_type>(std::move(program)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    Program& operator = (Program &&program)
    {
        detail::Wrapper<cl_type>::operator=(std::move(program));
        return *this;
    }

    cl_int build(
        const vector<Device>& devices,
        const char* options = NULL,
        void (CL_CALLBACK * notifyFptr)(cl_program, void *) = NULL,
        void* data = NULL) const
    {
        size_type numDevices = devices.size();
        vector<cl_device_id> deviceIDs(numDevices);
        
        for( size_type deviceIndex = 0; deviceIndex < numDevices; ++deviceIndex ) {
            deviceIDs[deviceIndex] = (devices[deviceIndex])();
        }

        cl_int buildError = ::clBuildProgram(
            object_,
            (cl_uint)
            devices.size(),
            deviceIDs.data(),
            options,
            notifyFptr,
            data);

        return detail::buildErrHandler(buildError, __BUILD_PROGRAM_ERR, getBuildInfo<CL_PROGRAM_BUILD_LOG>());
    }

    cl_int build(
        const char* options = NULL,
        void (CL_CALLBACK * notifyFptr)(cl_program, void *) = NULL,
        void* data = NULL) const
    {
        cl_int buildError = ::clBuildProgram(
            object_,
            0,
            NULL,
            options,
            notifyFptr,
            data);


        return detail::buildErrHandler(buildError, __BUILD_PROGRAM_ERR, getBuildInfo<CL_PROGRAM_BUILD_LOG>());
    }

#if CL_HPP_TARGET_OPENCL_VERSION >= 120
    cl_int compile(
        const char* options = NULL,
        void (CL_CALLBACK * notifyFptr)(cl_program, void *) = NULL,
        void* data = NULL) const
    {
        cl_int error = ::clCompileProgram(
            object_,
            0,
            NULL,
            options,
            0,
            NULL,
            NULL,
            notifyFptr,
            data);
        return detail::buildErrHandler(error, __COMPILE_PROGRAM_ERR, getBuildInfo<CL_PROGRAM_BUILD_LOG>());
    }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120

    template <typename T>
    cl_int getInfo(cl_program_info name, T* param) const
    {
        return detail::errHandler(
            detail::getInfo(&::clGetProgramInfo, object_, name, param),
            __GET_PROGRAM_INFO_ERR);
    }

    template <cl_int name> typename
    detail::param_traits<detail::cl_program_info, name>::param_type
    getInfo(cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_program_info, name>::param_type param;
        cl_int result = getInfo(name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }

    template <typename T>
    cl_int getBuildInfo(
        const Device& device, cl_program_build_info name, T* param) const
    {
        return detail::errHandler(
            detail::getInfo(
                &::clGetProgramBuildInfo, object_, device(), name, param),
                __GET_PROGRAM_BUILD_INFO_ERR);
    }

    template <cl_int name> typename
    detail::param_traits<detail::cl_program_build_info, name>::param_type
    getBuildInfo(const Device& device, cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_program_build_info, name>::param_type param;
        cl_int result = getBuildInfo(device, name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }
    
    /**
     * Build info function that returns a vector of device/info pairs for the specified 
     * info type and for all devices in the program.
     * On an error reading the info for any device, an empty vector of info will be returned.
     */
    template <cl_int name>
    vector<std::pair<cl::Device, typename detail::param_traits<detail::cl_program_build_info, name>::param_type>>
        getBuildInfo(cl_int *err = NULL) const
    {
        cl_int result = CL_SUCCESS;

        auto devs = getInfo<CL_PROGRAM_DEVICES>(&result);
        vector<std::pair<cl::Device, typename detail::param_traits<detail::cl_program_build_info, name>::param_type>>
            devInfo;

        // If there was an initial error from getInfo return the error
        if (result != CL_SUCCESS) {
            if (err != NULL) {
                *err = result;
            }
            return devInfo;
        }

        for (const cl::Device &d : devs) {
            typename detail::param_traits<
                detail::cl_program_build_info, name>::param_type param;
            result = getBuildInfo(d, name, &param);
            devInfo.push_back(
                std::pair<cl::Device, typename detail::param_traits<detail::cl_program_build_info, name>::param_type>
                (d, param));
            if (result != CL_SUCCESS) {
                // On error, leave the loop and return the error code
                break;
            }
        }
        if (err != NULL) {
            *err = result;
        }
        if (result != CL_SUCCESS) {
            devInfo.clear();
        }
        return devInfo;
    }

    cl_int createKernels(vector<Kernel>* kernels)
    {
        cl_uint numKernels;
        cl_int err = ::clCreateKernelsInProgram(object_, 0, NULL, &numKernels);
        if (err != CL_SUCCESS) {
            return detail::errHandler(err, __CREATE_KERNELS_IN_PROGRAM_ERR);
        }

        vector<cl_kernel> value(numKernels);
        
        err = ::clCreateKernelsInProgram(
            object_, numKernels, value.data(), NULL);
        if (err != CL_SUCCESS) {
            return detail::errHandler(err, __CREATE_KERNELS_IN_PROGRAM_ERR);
        }

        if (kernels) {
            kernels->resize(value.size());

            // Assign to param, constructing with retain behaviour
            // to correctly capture each underlying CL object
            for (size_type i = 0; i < value.size(); i++) {
                // We do not need to retain because this kernel is being created 
                // by the runtime
                (*kernels)[i] = Kernel(value[i], false);
            }
        }
        return CL_SUCCESS;
    }
};

#if CL_HPP_TARGET_OPENCL_VERSION >= 120
inline Program linkProgram(
    Program input1,
    Program input2,
    const char* options = NULL,
    void (CL_CALLBACK * notifyFptr)(cl_program, void *) = NULL,
    void* data = NULL,
    cl_int* err = NULL) 
{
    cl_int error_local = CL_SUCCESS;

    cl_program programs[2] = { input1(), input2() };

    Context ctx = input1.getInfo<CL_PROGRAM_CONTEXT>(&error_local);
    if(error_local!=CL_SUCCESS) {
        detail::errHandler(error_local, __LINK_PROGRAM_ERR);
    }

    cl_program prog = ::clLinkProgram(
        ctx(),
        0,
        NULL,
        options,
        2,
        programs,
        notifyFptr,
        data,
        &error_local);

    detail::errHandler(error_local,__COMPILE_PROGRAM_ERR);
    if (err != NULL) {
        *err = error_local;
    }

    return Program(prog);
}

inline Program linkProgram(
    vector<Program> inputPrograms,
    const char* options = NULL,
    void (CL_CALLBACK * notifyFptr)(cl_program, void *) = NULL,
    void* data = NULL,
    cl_int* err = NULL) 
{
    cl_int error_local = CL_SUCCESS;

    vector<cl_program> programs(inputPrograms.size());

    for (unsigned int i = 0; i < inputPrograms.size(); i++) {
        programs[i] = inputPrograms[i]();
    }
    
    Context ctx;
    if(inputPrograms.size() > 0) {
        ctx = inputPrograms[0].getInfo<CL_PROGRAM_CONTEXT>(&error_local);
        if(error_local!=CL_SUCCESS) {
            detail::errHandler(error_local, __LINK_PROGRAM_ERR);
        }
    }
    cl_program prog = ::clLinkProgram(
        ctx(),
        0,
        NULL,
        options,
        (cl_uint)inputPrograms.size(),
        programs.data(),
        notifyFptr,
        data,
        &error_local);

    detail::errHandler(error_local,__COMPILE_PROGRAM_ERR);
    if (err != NULL) {
        *err = error_local;
    }

    return Program(prog, false);
}
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120

// Template specialization for CL_PROGRAM_BINARIES
template <>
inline cl_int cl::Program::getInfo(cl_program_info name, vector<vector<unsigned char>>* param) const
{
    if (name != CL_PROGRAM_BINARIES) {
        return CL_INVALID_VALUE;
    }
    if (param) {
        // Resize the parameter array appropriately for each allocation
        // and pass down to the helper

        vector<size_type> sizes = getInfo<CL_PROGRAM_BINARY_SIZES>();
        size_type numBinaries = sizes.size();

        // Resize the parameter array and constituent arrays
        param->resize(numBinaries);
        for (size_type i = 0; i < numBinaries; ++i) {
            (*param)[i].resize(sizes[i]);
        }

        return detail::errHandler(
            detail::getInfo(&::clGetProgramInfo, object_, name, param),
            __GET_PROGRAM_INFO_ERR);
    }

    return CL_SUCCESS;
}

template<>
inline vector<vector<unsigned char>> cl::Program::getInfo<CL_PROGRAM_BINARIES>(cl_int* err) const
{
    vector<vector<unsigned char>> binariesVectors;

    cl_int result = getInfo(CL_PROGRAM_BINARIES, &binariesVectors);
    if (err != NULL) {
        *err = result;
    }
    return binariesVectors;
}

inline Kernel::Kernel(const Program& program, const char* name, cl_int* err)
{
    cl_int error;

    object_ = ::clCreateKernel(program(), name, &error);
    detail::errHandler(error, __CREATE_KERNEL_ERR);

    if (err != NULL) {
        *err = error;
    }

}

enum class QueueProperties : cl_command_queue_properties
{
    None = 0,
    Profiling = CL_QUEUE_PROFILING_ENABLE,
    OutOfOrder = CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE,
};

inline QueueProperties operator|(QueueProperties lhs, QueueProperties rhs)
{
    return static_cast<QueueProperties>(static_cast<cl_command_queue_properties>(lhs) | static_cast<cl_command_queue_properties>(rhs));
}

/*! \class CommandQueue
 * \brief CommandQueue interface for cl_command_queue.
 */
class CommandQueue : public detail::Wrapper<cl_command_queue>
{
private:
    static std::once_flag default_initialized_;
    static CommandQueue default_;
    static cl_int default_error_;

    /*! \brief Create the default command queue returned by @ref getDefault.
     *
     * It sets default_error_ to indicate success or failure. It does not throw
     * @c cl::Error.
     */
    static void makeDefault()
    {
        /* We don't want to throw an error from this function, so we have to
         * catch and set the error flag.
         */
#if defined(CL_HPP_ENABLE_EXCEPTIONS)
        try
#endif
        {
            int error;
            Context context = Context::getDefault(&error);

            if (error != CL_SUCCESS) {
                default_error_ = error;
            }
            else {
                Device device = Device::getDefault();
                default_ = CommandQueue(context, device, 0, &default_error_);
            }
        }
#if defined(CL_HPP_ENABLE_EXCEPTIONS)
        catch (cl::Error &e) {
            default_error_ = e.err();
        }
#endif
    }

    /*! \brief Create the default command queue.
     *
     * This sets @c default_. It does not throw
     * @c cl::Error.
     */
    static void makeDefaultProvided(const CommandQueue &c) {
        default_ = c;
    }

public:
#ifdef CL_HPP_UNIT_TEST_ENABLE
    /*! \brief Reset the default.
    *
    * This sets @c default_ to an empty value to support cleanup in
    * the unit test framework.
    * This function is not thread safe.
    */
    static void unitTestClearDefault() {
        default_ = CommandQueue();
    }
#endif // #ifdef CL_HPP_UNIT_TEST_ENABLE
        

    /*!
     * \brief Constructs a CommandQueue based on passed properties.
     * Will return an CL_INVALID_QUEUE_PROPERTIES error if CL_QUEUE_ON_DEVICE is specified.
     */
   CommandQueue(
        cl_command_queue_properties properties,
        cl_int* err = NULL)
    {
        cl_int error;

        Context context = Context::getDefault(&error);
        detail::errHandler(error, __CREATE_CONTEXT_ERR);

        if (error != CL_SUCCESS) {
            if (err != NULL) {
                *err = error;
            }
        }
        else {
            Device device = context.getInfo<CL_CONTEXT_DEVICES>()[0];
            bool useWithProperties;

#if CL_HPP_TARGET_OPENCL_VERSION >= 200 && CL_HPP_MINIMUM_OPENCL_VERSION < 200
            // Run-time decision based on the actual platform
            {
                cl_uint version = detail::getContextPlatformVersion(context());
                useWithProperties = (version >= 0x20000); // OpenCL 2.0 or above
            }
#elif CL_HPP_TARGET_OPENCL_VERSION >= 200
            useWithProperties = true;
#else
            useWithProperties = false;
#endif

#if CL_HPP_TARGET_OPENCL_VERSION >= 200
            if (useWithProperties) {
                cl_queue_properties queue_properties[] = {
                    CL_QUEUE_PROPERTIES, properties, 0 };
                if ((properties & CL_QUEUE_ON_DEVICE) == 0) {
                    object_ = ::clCreateCommandQueueWithProperties(
                        context(), device(), queue_properties, &error);
                }
                else {
                    error = CL_INVALID_QUEUE_PROPERTIES;
                }

                detail::errHandler(error, __CREATE_COMMAND_QUEUE_WITH_PROPERTIES_ERR);
                if (err != NULL) {
                    *err = error;
                }
            }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 200
#if CL_HPP_MINIMUM_OPENCL_VERSION < 200
            if (!useWithProperties) {
                object_ = ::clCreateCommandQueue(
                    context(), device(), properties, &error);

                detail::errHandler(error, __CREATE_COMMAND_QUEUE_ERR);
                if (err != NULL) {
                    *err = error;
                }
            }
#endif // CL_HPP_MINIMUM_OPENCL_VERSION < 200
        }
    }

   /*!
    * \brief Constructs a CommandQueue based on passed properties.
    * Will return an CL_INVALID_QUEUE_PROPERTIES error if CL_QUEUE_ON_DEVICE is specified.
    */
   CommandQueue(
       QueueProperties properties,
       cl_int* err = NULL)
   {
       cl_int error;

       Context context = Context::getDefault(&error);
       detail::errHandler(error, __CREATE_CONTEXT_ERR);

       if (error != CL_SUCCESS) {
           if (err != NULL) {
               *err = error;
           }
       }
       else {
           Device device = context.getInfo<CL_CONTEXT_DEVICES>()[0];
           bool useWithProperties;

#if CL_HPP_TARGET_OPENCL_VERSION >= 200 && CL_HPP_MINIMUM_OPENCL_VERSION < 200
           // Run-time decision based on the actual platform
           {
               cl_uint version = detail::getContextPlatformVersion(context());
               useWithProperties = (version >= 0x20000); // OpenCL 2.0 or above
           }
#elif CL_HPP_TARGET_OPENCL_VERSION >= 200
           useWithProperties = true;
#else
           useWithProperties = false;
#endif

#if CL_HPP_TARGET_OPENCL_VERSION >= 200
           if (useWithProperties) {
               cl_queue_properties queue_properties[] = {
                   CL_QUEUE_PROPERTIES, static_cast<cl_queue_properties>(properties), 0 };

               object_ = ::clCreateCommandQueueWithProperties(
                   context(), device(), queue_properties, &error);

               detail::errHandler(error, __CREATE_COMMAND_QUEUE_WITH_PROPERTIES_ERR);
               if (err != NULL) {
                   *err = error;
               }
           }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 200
#if CL_HPP_MINIMUM_OPENCL_VERSION < 200
           if (!useWithProperties) {
               object_ = ::clCreateCommandQueue(
                   context(), device(), static_cast<cl_command_queue_properties>(properties), &error);

               detail::errHandler(error, __CREATE_COMMAND_QUEUE_ERR);
               if (err != NULL) {
                   *err = error;
               }
           }
#endif // CL_HPP_MINIMUM_OPENCL_VERSION < 200

       }
   }

    /*!
     * \brief Constructs a CommandQueue for an implementation defined device in the given context
     * Will return an CL_INVALID_QUEUE_PROPERTIES error if CL_QUEUE_ON_DEVICE is specified.
     */
    explicit CommandQueue(
        const Context& context,
        cl_command_queue_properties properties = 0,
        cl_int* err = NULL)
    {
        cl_int error;
        bool useWithProperties;
        vector<cl::Device> devices;
        error = context.getInfo(CL_CONTEXT_DEVICES, &devices);

        detail::errHandler(error, __CREATE_CONTEXT_ERR);

        if (error != CL_SUCCESS)
        {
            if (err != NULL) {
                *err = error;
            }
            return;
        }

#if CL_HPP_TARGET_OPENCL_VERSION >= 200 && CL_HPP_MINIMUM_OPENCL_VERSION < 200
        // Run-time decision based on the actual platform
        {
            cl_uint version = detail::getContextPlatformVersion(context());
            useWithProperties = (version >= 0x20000); // OpenCL 2.0 or above
        }
#elif CL_HPP_TARGET_OPENCL_VERSION >= 200
        useWithProperties = true;
#else
        useWithProperties = false;
#endif

#if CL_HPP_TARGET_OPENCL_VERSION >= 200
        if (useWithProperties) {
            cl_queue_properties queue_properties[] = {
                CL_QUEUE_PROPERTIES, properties, 0 };
            if ((properties & CL_QUEUE_ON_DEVICE) == 0) {
                object_ = ::clCreateCommandQueueWithProperties(
                    context(), devices[0](), queue_properties, &error);
            }
            else {
                error = CL_INVALID_QUEUE_PROPERTIES;
            }

            detail::errHandler(error, __CREATE_COMMAND_QUEUE_WITH_PROPERTIES_ERR);
            if (err != NULL) {
                *err = error;
            }
        }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 200
#if CL_HPP_MINIMUM_OPENCL_VERSION < 200
        if (!useWithProperties) {
            object_ = ::clCreateCommandQueue(
                context(), devices[0](), properties, &error);

            detail::errHandler(error, __CREATE_COMMAND_QUEUE_ERR);
            if (err != NULL) {
                *err = error;
            }
        }
#endif // CL_HPP_MINIMUM_OPENCL_VERSION < 200
    }

    /*!
    * \brief Constructs a CommandQueue for an implementation defined device in the given context
    * Will return an CL_INVALID_QUEUE_PROPERTIES error if CL_QUEUE_ON_DEVICE is specified.
    */
    explicit CommandQueue(
        const Context& context,
        QueueProperties properties,
        cl_int* err = NULL)
    {
        cl_int error;
        bool useWithProperties;
        vector<cl::Device> devices;
        error = context.getInfo(CL_CONTEXT_DEVICES, &devices);

        detail::errHandler(error, __CREATE_CONTEXT_ERR);

        if (error != CL_SUCCESS)
        {
            if (err != NULL) {
                *err = error;
            }
            return;
        }

#if CL_HPP_TARGET_OPENCL_VERSION >= 200 && CL_HPP_MINIMUM_OPENCL_VERSION < 200
        // Run-time decision based on the actual platform
        {
            cl_uint version = detail::getContextPlatformVersion(context());
            useWithProperties = (version >= 0x20000); // OpenCL 2.0 or above
        }
#elif CL_HPP_TARGET_OPENCL_VERSION >= 200
        useWithProperties = true;
#else
        useWithProperties = false;
#endif

#if CL_HPP_TARGET_OPENCL_VERSION >= 200
        if (useWithProperties) {
            cl_queue_properties queue_properties[] = {
                CL_QUEUE_PROPERTIES, static_cast<cl_queue_properties>(properties), 0 };
            object_ = ::clCreateCommandQueueWithProperties(
                context(), devices[0](), queue_properties, &error);

            detail::errHandler(error, __CREATE_COMMAND_QUEUE_WITH_PROPERTIES_ERR);
            if (err != NULL) {
                *err = error;
            }
        }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 200
#if CL_HPP_MINIMUM_OPENCL_VERSION < 200
        if (!useWithProperties) {
            object_ = ::clCreateCommandQueue(
                context(), devices[0](), static_cast<cl_command_queue_properties>(properties), &error);

            detail::errHandler(error, __CREATE_COMMAND_QUEUE_ERR);
            if (err != NULL) {
                *err = error;
            }
        }
#endif // CL_HPP_MINIMUM_OPENCL_VERSION < 200
    }

    /*!
     * \brief Constructs a CommandQueue for a passed device and context
     * Will return an CL_INVALID_QUEUE_PROPERTIES error if CL_QUEUE_ON_DEVICE is specified.
     */
    CommandQueue(
        const Context& context,
        const Device& device,
        cl_command_queue_properties properties = 0,
        cl_int* err = NULL)
    {
        cl_int error;
        bool useWithProperties;

#if CL_HPP_TARGET_OPENCL_VERSION >= 200 && CL_HPP_MINIMUM_OPENCL_VERSION < 200
        // Run-time decision based on the actual platform
        {
            cl_uint version = detail::getContextPlatformVersion(context());
            useWithProperties = (version >= 0x20000); // OpenCL 2.0 or above
        }
#elif CL_HPP_TARGET_OPENCL_VERSION >= 200
        useWithProperties = true;
#else
        useWithProperties = false;
#endif

#if CL_HPP_TARGET_OPENCL_VERSION >= 200
        if (useWithProperties) {
            cl_queue_properties queue_properties[] = {
                CL_QUEUE_PROPERTIES, properties, 0 };
            object_ = ::clCreateCommandQueueWithProperties(
                context(), device(), queue_properties, &error);

            detail::errHandler(error, __CREATE_COMMAND_QUEUE_WITH_PROPERTIES_ERR);
            if (err != NULL) {
                *err = error;
            }
        }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 200
#if CL_HPP_MINIMUM_OPENCL_VERSION < 200
        if (!useWithProperties) {
            object_ = ::clCreateCommandQueue(
                context(), device(), properties, &error);

            detail::errHandler(error, __CREATE_COMMAND_QUEUE_ERR);
            if (err != NULL) {
                *err = error;
            }
        }
#endif // CL_HPP_MINIMUM_OPENCL_VERSION < 200
    }

    /*!
     * \brief Constructs a CommandQueue for a passed device and context
     * Will return an CL_INVALID_QUEUE_PROPERTIES error if CL_QUEUE_ON_DEVICE is specified.
     */
    CommandQueue(
        const Context& context,
        const Device& device,
        QueueProperties properties,
        cl_int* err = NULL)
    {
        cl_int error;
        bool useWithProperties;

#if CL_HPP_TARGET_OPENCL_VERSION >= 200 && CL_HPP_MINIMUM_OPENCL_VERSION < 200
        // Run-time decision based on the actual platform
        {
            cl_uint version = detail::getContextPlatformVersion(context());
            useWithProperties = (version >= 0x20000); // OpenCL 2.0 or above
        }
#elif CL_HPP_TARGET_OPENCL_VERSION >= 200
        useWithProperties = true;
#else
        useWithProperties = false;
#endif

#if CL_HPP_TARGET_OPENCL_VERSION >= 200
        if (useWithProperties) {
            cl_queue_properties queue_properties[] = {
                CL_QUEUE_PROPERTIES, static_cast<cl_queue_properties>(properties), 0 };
            object_ = ::clCreateCommandQueueWithProperties(
                context(), device(), queue_properties, &error);

            detail::errHandler(error, __CREATE_COMMAND_QUEUE_WITH_PROPERTIES_ERR);
            if (err != NULL) {
                *err = error;
            }
        }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 200
#if CL_HPP_MINIMUM_OPENCL_VERSION < 200
        if (!useWithProperties) {
            object_ = ::clCreateCommandQueue(
                context(), device(), static_cast<cl_command_queue_properties>(properties), &error);

            detail::errHandler(error, __CREATE_COMMAND_QUEUE_ERR);
            if (err != NULL) {
                *err = error;
            }
        }
#endif // CL_HPP_MINIMUM_OPENCL_VERSION < 200
    }

    static CommandQueue getDefault(cl_int * err = NULL) 
    {
        std::call_once(default_initialized_, makeDefault);
#if CL_HPP_TARGET_OPENCL_VERSION >= 200
        detail::errHandler(default_error_, __CREATE_COMMAND_QUEUE_WITH_PROPERTIES_ERR);
#else // CL_HPP_TARGET_OPENCL_VERSION >= 200
        detail::errHandler(default_error_, __CREATE_COMMAND_QUEUE_ERR);
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 200
        if (err != NULL) {
            *err = default_error_;
        }
        return default_;
    }

    /**
     * Modify the default command queue to be used by
     * subsequent operations.
     * Will only set the default if no default was previously created.
     * @return updated default command queue.
     *         Should be compared to the passed value to ensure that it was updated.
     */
    static CommandQueue setDefault(const CommandQueue &default_queue)
    {
        std::call_once(default_initialized_, makeDefaultProvided, std::cref(default_queue));
        detail::errHandler(default_error_);
        return default_;
    }

    CommandQueue() { }


    /*! \brief Constructor from cl_mem - takes ownership.
     *
     * \param retainObject will cause the constructor to retain its cl object.
     *                     Defaults to false to maintain compatibility with
     *                     earlier versions.
     */
    explicit CommandQueue(const cl_command_queue& commandQueue, bool retainObject = false) : 
        detail::Wrapper<cl_type>(commandQueue, retainObject) { }

    CommandQueue& operator = (const cl_command_queue& rhs)
    {
        detail::Wrapper<cl_type>::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    CommandQueue(const CommandQueue& queue) : detail::Wrapper<cl_type>(queue) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    CommandQueue& operator = (const CommandQueue &queue)
    {
        detail::Wrapper<cl_type>::operator=(queue);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    CommandQueue(CommandQueue&& queue) CL_HPP_NOEXCEPT_ : detail::Wrapper<cl_type>(std::move(queue)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    CommandQueue& operator = (CommandQueue &&queue)
    {
        detail::Wrapper<cl_type>::operator=(std::move(queue));
        return *this;
    }

    template <typename T>
    cl_int getInfo(cl_command_queue_info name, T* param) const
    {
        return detail::errHandler(
            detail::getInfo(
                &::clGetCommandQueueInfo, object_, name, param),
                __GET_COMMAND_QUEUE_INFO_ERR);
    }

    template <cl_int name> typename
    detail::param_traits<detail::cl_command_queue_info, name>::param_type
    getInfo(cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_command_queue_info, name>::param_type param;
        cl_int result = getInfo(name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }

    cl_int enqueueReadBuffer(
        const Buffer& buffer,
        cl_bool blocking,
        size_type offset,
        size_type size,
        void* ptr,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueReadBuffer(
                object_, buffer(), blocking, offset, size,
                ptr,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_READ_BUFFER_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    cl_int enqueueWriteBuffer(
        const Buffer& buffer,
        cl_bool blocking,
        size_type offset,
        size_type size,
        const void* ptr,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueWriteBuffer(
                object_, buffer(), blocking, offset, size,
                ptr,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
                __ENQUEUE_WRITE_BUFFER_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    cl_int enqueueCopyBuffer(
        const Buffer& src,
        const Buffer& dst,
        size_type src_offset,
        size_type dst_offset,
        size_type size,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueCopyBuffer(
                object_, src(), dst(), src_offset, dst_offset, size,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQEUE_COPY_BUFFER_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }
#if CL_HPP_TARGET_OPENCL_VERSION >= 110
    cl_int enqueueReadBufferRect(
        const Buffer& buffer,
        cl_bool blocking,
        const array<size_type, 3>& buffer_offset,
        const array<size_type, 3>& host_offset,
        const array<size_type, 3>& region,
        size_type buffer_row_pitch,
        size_type buffer_slice_pitch,
        size_type host_row_pitch,
        size_type host_slice_pitch,
        void *ptr,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueReadBufferRect(
                object_, 
                buffer(), 
                blocking,
                buffer_offset.data(),
                host_offset.data(),
                region.data(),
                buffer_row_pitch,
                buffer_slice_pitch,
                host_row_pitch,
                host_slice_pitch,
                ptr,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
                __ENQUEUE_READ_BUFFER_RECT_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    cl_int enqueueWriteBufferRect(
        const Buffer& buffer,
        cl_bool blocking,
        const array<size_type, 3>& buffer_offset,
        const array<size_type, 3>& host_offset,
        const array<size_type, 3>& region,
        size_type buffer_row_pitch,
        size_type buffer_slice_pitch,
        size_type host_row_pitch,
        size_type host_slice_pitch,
        const void *ptr,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueWriteBufferRect(
                object_, 
                buffer(), 
                blocking,
                buffer_offset.data(),
                host_offset.data(),
                region.data(),
                buffer_row_pitch,
                buffer_slice_pitch,
                host_row_pitch,
                host_slice_pitch,
                ptr,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
                __ENQUEUE_WRITE_BUFFER_RECT_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    cl_int enqueueCopyBufferRect(
        const Buffer& src,
        const Buffer& dst,
        const array<size_type, 3>& src_origin,
        const array<size_type, 3>& dst_origin,
        const array<size_type, 3>& region,
        size_type src_row_pitch,
        size_type src_slice_pitch,
        size_type dst_row_pitch,
        size_type dst_slice_pitch,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueCopyBufferRect(
                object_, 
                src(), 
                dst(), 
                src_origin.data(),
                dst_origin.data(),
                region.data(),
                src_row_pitch,
                src_slice_pitch,
                dst_row_pitch,
                dst_slice_pitch,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQEUE_COPY_BUFFER_RECT_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 110
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
    /**
     * Enqueue a command to fill a buffer object with a pattern
     * of a given size. The pattern is specified as a vector type.
     * \tparam PatternType The datatype of the pattern field. 
     *     The pattern type must be an accepted OpenCL data type.
     * \tparam offset Is the offset in bytes into the buffer at 
     *     which to start filling. This must be a multiple of 
     *     the pattern size.
     * \tparam size Is the size in bytes of the region to fill.
     *     This must be a multiple of the pattern size.
     */
    template<typename PatternType>
    cl_int enqueueFillBuffer(
        const Buffer& buffer,
        PatternType pattern,
        size_type offset,
        size_type size,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueFillBuffer(
                object_, 
                buffer(),
                static_cast<void*>(&pattern),
                sizeof(PatternType), 
                offset, 
                size,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
                __ENQUEUE_FILL_BUFFER_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120

    cl_int enqueueReadImage(
        const Image& image,
        cl_bool blocking,
        const array<size_type, 3>& origin,
        const array<size_type, 3>& region,
        size_type row_pitch,
        size_type slice_pitch,
        void* ptr,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueReadImage(
                object_, 
                image(), 
                blocking, 
                origin.data(),
                region.data(), 
                row_pitch, 
                slice_pitch, 
                ptr,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_READ_IMAGE_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    cl_int enqueueWriteImage(
        const Image& image,
        cl_bool blocking,
        const array<size_type, 3>& origin,
        const array<size_type, 3>& region,
        size_type row_pitch,
        size_type slice_pitch,
        const void* ptr,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueWriteImage(
                object_, 
                image(), 
                blocking, 
                origin.data(),
                region.data(), 
                row_pitch, 
                slice_pitch, 
                ptr,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_WRITE_IMAGE_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    cl_int enqueueCopyImage(
        const Image& src,
        const Image& dst,
        const array<size_type, 3>& src_origin,
        const array<size_type, 3>& dst_origin,
        const array<size_type, 3>& region,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueCopyImage(
                object_, 
                src(), 
                dst(), 
                src_origin.data(),
                dst_origin.data(), 
                region.data(),
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_COPY_IMAGE_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

#if CL_HPP_TARGET_OPENCL_VERSION >= 120
    /**
     * Enqueue a command to fill an image object with a specified color.
     * \param fillColor is the color to use to fill the image.
     *     This is a four component RGBA floating-point color value if
     *     the image channel data type is not an unnormalized signed or
     *     unsigned data type.
     */
    cl_int enqueueFillImage(
        const Image& image,
        cl_float4 fillColor,
        const array<size_type, 3>& origin,
        const array<size_type, 3>& region,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueFillImage(
                object_, 
                image(),
                static_cast<void*>(&fillColor), 
                origin.data(),
                region.data(),
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
                __ENQUEUE_FILL_IMAGE_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    /**
     * Enqueue a command to fill an image object with a specified color.
     * \param fillColor is the color to use to fill the image.
     *     This is a four component RGBA signed integer color value if
     *     the image channel data type is an unnormalized signed integer
     *     type.
     */
    cl_int enqueueFillImage(
        const Image& image,
        cl_int4 fillColor,
        const array<size_type, 3>& origin,
        const array<size_type, 3>& region,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueFillImage(
                object_, 
                image(),
                static_cast<void*>(&fillColor), 
                origin.data(),
                region.data(),
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
                __ENQUEUE_FILL_IMAGE_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    /**
     * Enqueue a command to fill an image object with a specified color.
     * \param fillColor is the color to use to fill the image.
     *     This is a four component RGBA unsigned integer color value if
     *     the image channel data type is an unnormalized unsigned integer
     *     type.
     */
    cl_int enqueueFillImage(
        const Image& image,
        cl_uint4 fillColor,
        const array<size_type, 3>& origin,
        const array<size_type, 3>& region,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueFillImage(
                object_, 
                image(),
                static_cast<void*>(&fillColor), 
                origin.data(),
                region.data(),
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
                __ENQUEUE_FILL_IMAGE_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120

    cl_int enqueueCopyImageToBuffer(
        const Image& src,
        const Buffer& dst,
        const array<size_type, 3>& src_origin,
        const array<size_type, 3>& region,
        size_type dst_offset,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueCopyImageToBuffer(
                object_, 
                src(), 
                dst(), 
                src_origin.data(),
                region.data(), 
                dst_offset,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_COPY_IMAGE_TO_BUFFER_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    cl_int enqueueCopyBufferToImage(
        const Buffer& src,
        const Image& dst,
        size_type src_offset,
        const array<size_type, 3>& dst_origin,
        const array<size_type, 3>& region,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueCopyBufferToImage(
                object_, 
                src(), 
                dst(), 
                src_offset,
                dst_origin.data(), 
                region.data(),
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_COPY_BUFFER_TO_IMAGE_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    void* enqueueMapBuffer(
        const Buffer& buffer,
        cl_bool blocking,
        cl_map_flags flags,
        size_type offset,
        size_type size,
        const vector<Event>* events = NULL,
        Event* event = NULL,
        cl_int* err = NULL) const
    {
        cl_event tmp;
        cl_int error;
        void * result = ::clEnqueueMapBuffer(
            object_, buffer(), blocking, flags, offset, size,
            (events != NULL) ? (cl_uint) events->size() : 0,
            (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
            (event != NULL) ? &tmp : NULL,
            &error);

        detail::errHandler(error, __ENQUEUE_MAP_BUFFER_ERR);
        if (err != NULL) {
            *err = error;
        }
        if (event != NULL && error == CL_SUCCESS)
            *event = tmp;

        return result;
    }

    void* enqueueMapImage(
        const Image& buffer,
        cl_bool blocking,
        cl_map_flags flags,
        const array<size_type, 3>& origin,
        const array<size_type, 3>& region,
        size_type * row_pitch,
        size_type * slice_pitch,
        const vector<Event>* events = NULL,
        Event* event = NULL,
        cl_int* err = NULL) const
    {
        cl_event tmp;
        cl_int error;
        void * result = ::clEnqueueMapImage(
            object_, buffer(), blocking, flags,
            origin.data(), 
            region.data(),
            row_pitch, slice_pitch,
            (events != NULL) ? (cl_uint) events->size() : 0,
            (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
            (event != NULL) ? &tmp : NULL,
            &error);

        detail::errHandler(error, __ENQUEUE_MAP_IMAGE_ERR);
        if (err != NULL) {
              *err = error;
        }
        if (event != NULL && error == CL_SUCCESS)
            *event = tmp;
        return result;
    }

#if CL_HPP_TARGET_OPENCL_VERSION >= 200
    /**
     * Enqueues a command that will allow the host to update a region of a coarse-grained SVM buffer.
     * This variant takes a raw SVM pointer.
     */
    template<typename T>
    cl_int enqueueMapSVM(
        T* ptr,
        cl_bool blocking,
        cl_map_flags flags,
        size_type size,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(::clEnqueueSVMMap(
            object_, blocking, flags, static_cast<void*>(ptr), size,
            (events != NULL) ? (cl_uint)events->size() : 0,
            (events != NULL && events->size() > 0) ? (cl_event*)&events->front() : NULL,
            (event != NULL) ? &tmp : NULL),
            __ENQUEUE_MAP_BUFFER_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }


    /**
     * Enqueues a command that will allow the host to update a region of a coarse-grained SVM buffer.
     * This variant takes a cl::pointer instance.
     */
    template<typename T, class D>
    cl_int enqueueMapSVM(
        cl::pointer<T, D> &ptr,
        cl_bool blocking,
        cl_map_flags flags,
        size_type size,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(::clEnqueueSVMMap(
            object_, blocking, flags, static_cast<void*>(ptr.get()), size,
            (events != NULL) ? (cl_uint)events->size() : 0,
            (events != NULL && events->size() > 0) ? (cl_event*)&events->front() : NULL,
            (event != NULL) ? &tmp : NULL),
            __ENQUEUE_MAP_BUFFER_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    /**
     * Enqueues a command that will allow the host to update a region of a coarse-grained SVM buffer.
     * This variant takes a cl::vector instance.
     */
    template<typename T, class Alloc>
    cl_int enqueueMapSVM(
        cl::vector<T, Alloc> &container,
        cl_bool blocking,
        cl_map_flags flags,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(::clEnqueueSVMMap(
            object_, blocking, flags, static_cast<void*>(container.data()), container.size(),
            (events != NULL) ? (cl_uint)events->size() : 0,
            (events != NULL && events->size() > 0) ? (cl_event*)&events->front() : NULL,
            (event != NULL) ? &tmp : NULL),
            __ENQUEUE_MAP_BUFFER_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }
#endif // #if CL_HPP_TARGET_OPENCL_VERSION >= 200

    cl_int enqueueUnmapMemObject(
        const Memory& memory,
        void* mapped_ptr,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueUnmapMemObject(
                object_, memory(), mapped_ptr,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_UNMAP_MEM_OBJECT_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }


#if CL_HPP_TARGET_OPENCL_VERSION >= 200
    /**
     * Enqueues a command that will release a coarse-grained SVM buffer back to the OpenCL runtime.
     * This variant takes a raw SVM pointer.
     */
    template<typename T>
    cl_int enqueueUnmapSVM(
        T* ptr,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueSVMUnmap(
            object_, static_cast<void*>(ptr),
            (events != NULL) ? (cl_uint)events->size() : 0,
            (events != NULL && events->size() > 0) ? (cl_event*)&events->front() : NULL,
            (event != NULL) ? &tmp : NULL),
            __ENQUEUE_UNMAP_MEM_OBJECT_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    /**
     * Enqueues a command that will release a coarse-grained SVM buffer back to the OpenCL runtime.
     * This variant takes a cl::pointer instance.
     */
    template<typename T, class D>
    cl_int enqueueUnmapSVM(
        cl::pointer<T, D> &ptr,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueSVMUnmap(
            object_, static_cast<void*>(ptr.get()),
            (events != NULL) ? (cl_uint)events->size() : 0,
            (events != NULL && events->size() > 0) ? (cl_event*)&events->front() : NULL,
            (event != NULL) ? &tmp : NULL),
            __ENQUEUE_UNMAP_MEM_OBJECT_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    /**
     * Enqueues a command that will release a coarse-grained SVM buffer back to the OpenCL runtime.
     * This variant takes a cl::vector instance.
     */
    template<typename T, class Alloc>
    cl_int enqueueUnmapSVM(
        cl::vector<T, Alloc> &container,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueSVMUnmap(
            object_, static_cast<void*>(container.data()),
            (events != NULL) ? (cl_uint)events->size() : 0,
            (events != NULL && events->size() > 0) ? (cl_event*)&events->front() : NULL,
            (event != NULL) ? &tmp : NULL),
            __ENQUEUE_UNMAP_MEM_OBJECT_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }
#endif // #if CL_HPP_TARGET_OPENCL_VERSION >= 200

#if CL_HPP_TARGET_OPENCL_VERSION >= 120
    /**
     * Enqueues a marker command which waits for either a list of events to complete, 
     * or all previously enqueued commands to complete.
     *
     * Enqueues a marker command which waits for either a list of events to complete, 
     * or if the list is empty it waits for all commands previously enqueued in command_queue 
     * to complete before it completes. This command returns an event which can be waited on, 
     * i.e. this event can be waited on to insure that all events either in the event_wait_list 
     * or all previously enqueued commands, queued before this command to command_queue, 
     * have completed.
     */
    cl_int enqueueMarkerWithWaitList(
        const vector<Event> *events = 0,
        Event *event = 0) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueMarkerWithWaitList(
                object_,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_MARKER_WAIT_LIST_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    /**
     * A synchronization point that enqueues a barrier operation.
     *
     * Enqueues a barrier command which waits for either a list of events to complete, 
     * or if the list is empty it waits for all commands previously enqueued in command_queue 
     * to complete before it completes. This command blocks command execution, that is, any 
     * following commands enqueued after it do not execute until it completes. This command 
     * returns an event which can be waited on, i.e. this event can be waited on to insure that 
     * all events either in the event_wait_list or all previously enqueued commands, queued 
     * before this command to command_queue, have completed.
     */
    cl_int enqueueBarrierWithWaitList(
        const vector<Event> *events = 0,
        Event *event = 0) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueBarrierWithWaitList(
                object_,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_BARRIER_WAIT_LIST_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }
    
    /**
     * Enqueues a command to indicate with which device a set of memory objects
     * should be associated.
     */
    cl_int enqueueMigrateMemObjects(
        const vector<Memory> &memObjects,
        cl_mem_migration_flags flags,
        const vector<Event>* events = NULL,
        Event* event = NULL
        ) const
    {
        cl_event tmp;
        
        vector<cl_mem> localMemObjects(memObjects.size());

        for( int i = 0; i < (int)memObjects.size(); ++i ) {
            localMemObjects[i] = memObjects[i]();
        }


        cl_int err = detail::errHandler(
            ::clEnqueueMigrateMemObjects(
                object_, 
                (cl_uint)memObjects.size(), 
                localMemObjects.data(),
                flags,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_UNMAP_MEM_OBJECT_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120

    cl_int enqueueNDRangeKernel(
        const Kernel& kernel,
        const NDRange& offset,
        const NDRange& global,
        const NDRange& local = NullRange,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueNDRangeKernel(
                object_, kernel(), (cl_uint) global.dimensions(),
                offset.dimensions() != 0 ? (const size_type*) offset : NULL,
                (const size_type*) global,
                local.dimensions() != 0 ? (const size_type*) local : NULL,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_NDRANGE_KERNEL_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

#if defined(CL_USE_DEPRECATED_OPENCL_1_2_APIS)
    CL_EXT_PREFIX__VERSION_1_2_DEPRECATED cl_int enqueueTask(
        const Kernel& kernel,
        const vector<Event>* events = NULL,
        Event* event = NULL) const CL_EXT_SUFFIX__VERSION_1_2_DEPRECATED
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueTask(
                object_, kernel(),
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_TASK_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }
#endif // #if defined(CL_USE_DEPRECATED_OPENCL_1_2_APIS)

    cl_int enqueueNativeKernel(
        void (CL_CALLBACK *userFptr)(void *),
        std::pair<void*, size_type> args,
        const vector<Memory>* mem_objects = NULL,
        const vector<const void*>* mem_locs = NULL,
        const vector<Event>* events = NULL,
        Event* event = NULL) const
    {
        size_type elements = 0;
        if (mem_objects != NULL) {
            elements = mem_objects->size();
        }
        vector<cl_mem> mems(elements);
        for (unsigned int i = 0; i < elements; i++) {
            mems[i] = ((*mem_objects)[i])();
        }
        
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueNativeKernel(
                object_, userFptr, args.first, args.second,
                (mem_objects != NULL) ? (cl_uint) mem_objects->size() : 0,
                mems.data(),
                (mem_locs != NULL && mem_locs->size() > 0) ? (const void **) &mem_locs->front() : NULL,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_NATIVE_KERNEL);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

/**
 * Deprecated APIs for 1.2
 */
#if defined(CL_USE_DEPRECATED_OPENCL_1_1_APIS)
    CL_EXT_PREFIX__VERSION_1_1_DEPRECATED 
    cl_int enqueueMarker(Event* event = NULL) const CL_EXT_SUFFIX__VERSION_1_1_DEPRECATED
    {
        cl_event tmp;
        cl_int err = detail::errHandler(
            ::clEnqueueMarker(
                object_, 
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_MARKER_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }

    CL_EXT_PREFIX__VERSION_1_1_DEPRECATED
    cl_int enqueueWaitForEvents(const vector<Event>& events) const CL_EXT_SUFFIX__VERSION_1_1_DEPRECATED
    {
        return detail::errHandler(
            ::clEnqueueWaitForEvents(
                object_,
                (cl_uint) events.size(),
                events.size() > 0 ? (const cl_event*) &events.front() : NULL),
            __ENQUEUE_WAIT_FOR_EVENTS_ERR);
    }
#endif // defined(CL_USE_DEPRECATED_OPENCL_1_1_APIS)

    cl_int enqueueAcquireGLObjects(
         const vector<Memory>* mem_objects = NULL,
         const vector<Event>* events = NULL,
         Event* event = NULL) const
     {
        cl_event tmp;
        cl_int err = detail::errHandler(
             ::clEnqueueAcquireGLObjects(
                 object_,
                 (mem_objects != NULL) ? (cl_uint) mem_objects->size() : 0,
                 (mem_objects != NULL && mem_objects->size() > 0) ? (const cl_mem *) &mem_objects->front(): NULL,
                 (events != NULL) ? (cl_uint) events->size() : 0,
                 (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                 (event != NULL) ? &tmp : NULL),
             __ENQUEUE_ACQUIRE_GL_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
     }

    cl_int enqueueReleaseGLObjects(
         const vector<Memory>* mem_objects = NULL,
         const vector<Event>* events = NULL,
         Event* event = NULL) const
     {
        cl_event tmp;
        cl_int err = detail::errHandler(
             ::clEnqueueReleaseGLObjects(
                 object_,
                 (mem_objects != NULL) ? (cl_uint) mem_objects->size() : 0,
                 (mem_objects != NULL && mem_objects->size() > 0) ? (const cl_mem *) &mem_objects->front(): NULL,
                 (events != NULL) ? (cl_uint) events->size() : 0,
                 (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                 (event != NULL) ? &tmp : NULL),
             __ENQUEUE_RELEASE_GL_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
     }

#if defined (CL_HPP_USE_DX_INTEROP)
typedef CL_API_ENTRY cl_int (CL_API_CALL *PFN_clEnqueueAcquireD3D10ObjectsKHR)(
    cl_command_queue command_queue, cl_uint num_objects,
    const cl_mem* mem_objects, cl_uint num_events_in_wait_list,
    const cl_event* event_wait_list, cl_event* event);
typedef CL_API_ENTRY cl_int (CL_API_CALL *PFN_clEnqueueReleaseD3D10ObjectsKHR)(
    cl_command_queue command_queue, cl_uint num_objects,
    const cl_mem* mem_objects,  cl_uint num_events_in_wait_list,
    const cl_event* event_wait_list, cl_event* event);

    cl_int enqueueAcquireD3D10Objects(
         const vector<Memory>* mem_objects = NULL,
         const vector<Event>* events = NULL,
         Event* event = NULL) const
    {
        static PFN_clEnqueueAcquireD3D10ObjectsKHR pfn_clEnqueueAcquireD3D10ObjectsKHR = NULL;
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
        cl_context context = getInfo<CL_QUEUE_CONTEXT>();
        cl::Device device(getInfo<CL_QUEUE_DEVICE>());
        cl_platform_id platform = device.getInfo<CL_DEVICE_PLATFORM>();
        CL_HPP_INIT_CL_EXT_FCN_PTR_PLATFORM_(platform, clEnqueueAcquireD3D10ObjectsKHR);
#endif
#if CL_HPP_TARGET_OPENCL_VERSION >= 110
        CL_HPP_INIT_CL_EXT_FCN_PTR_(clEnqueueAcquireD3D10ObjectsKHR);
#endif
        
        cl_event tmp;
        cl_int err = detail::errHandler(
             pfn_clEnqueueAcquireD3D10ObjectsKHR(
                 object_,
                 (mem_objects != NULL) ? (cl_uint) mem_objects->size() : 0,
                 (mem_objects != NULL && mem_objects->size() > 0) ? (const cl_mem *) &mem_objects->front(): NULL,
                 (events != NULL) ? (cl_uint) events->size() : 0,
                 (events != NULL) ? (cl_event*) &events->front() : NULL,
                 (event != NULL) ? &tmp : NULL),
             __ENQUEUE_ACQUIRE_GL_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
     }

    cl_int enqueueReleaseD3D10Objects(
         const vector<Memory>* mem_objects = NULL,
         const vector<Event>* events = NULL,
         Event* event = NULL) const
    {
        static PFN_clEnqueueReleaseD3D10ObjectsKHR pfn_clEnqueueReleaseD3D10ObjectsKHR = NULL;
#if CL_HPP_TARGET_OPENCL_VERSION >= 120
        cl_context context = getInfo<CL_QUEUE_CONTEXT>();
        cl::Device device(getInfo<CL_QUEUE_DEVICE>());
        cl_platform_id platform = device.getInfo<CL_DEVICE_PLATFORM>();
        CL_HPP_INIT_CL_EXT_FCN_PTR_PLATFORM_(platform, clEnqueueReleaseD3D10ObjectsKHR);
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 120
#if CL_HPP_TARGET_OPENCL_VERSION >= 110
        CL_HPP_INIT_CL_EXT_FCN_PTR_(clEnqueueReleaseD3D10ObjectsKHR);
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 110

        cl_event tmp;
        cl_int err = detail::errHandler(
            pfn_clEnqueueReleaseD3D10ObjectsKHR(
                object_,
                (mem_objects != NULL) ? (cl_uint) mem_objects->size() : 0,
                (mem_objects != NULL && mem_objects->size() > 0) ? (const cl_mem *) &mem_objects->front(): NULL,
                (events != NULL) ? (cl_uint) events->size() : 0,
                (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
                (event != NULL) ? &tmp : NULL),
            __ENQUEUE_RELEASE_GL_ERR);

        if (event != NULL && err == CL_SUCCESS)
            *event = tmp;

        return err;
    }
#endif

/**
 * Deprecated APIs for 1.2
 */
#if defined(CL_USE_DEPRECATED_OPENCL_1_1_APIS)
    CL_EXT_PREFIX__VERSION_1_1_DEPRECATED
    cl_int enqueueBarrier() const CL_EXT_SUFFIX__VERSION_1_1_DEPRECATED
    {
        return detail::errHandler(
            ::clEnqueueBarrier(object_),
            __ENQUEUE_BARRIER_ERR);
    }
#endif // CL_USE_DEPRECATED_OPENCL_1_1_APIS

    cl_int flush() const
    {
        return detail::errHandler(::clFlush(object_), __FLUSH_ERR);
    }

    cl_int finish() const
    {
        return detail::errHandler(::clFinish(object_), __FINISH_ERR);
    }
}; // CommandQueue

CL_HPP_DEFINE_STATIC_MEMBER_ std::once_flag CommandQueue::default_initialized_;
CL_HPP_DEFINE_STATIC_MEMBER_ CommandQueue CommandQueue::default_;
CL_HPP_DEFINE_STATIC_MEMBER_ cl_int CommandQueue::default_error_ = CL_SUCCESS;


#if CL_HPP_TARGET_OPENCL_VERSION >= 200
enum class DeviceQueueProperties : cl_command_queue_properties
{
    None = 0,
    Profiling = CL_QUEUE_PROFILING_ENABLE,
};

inline DeviceQueueProperties operator|(DeviceQueueProperties lhs, DeviceQueueProperties rhs)
{
    return static_cast<DeviceQueueProperties>(static_cast<cl_command_queue_properties>(lhs) | static_cast<cl_command_queue_properties>(rhs));
}

/*! \class DeviceCommandQueue
 * \brief DeviceCommandQueue interface for device cl_command_queues.
 */
class DeviceCommandQueue : public detail::Wrapper<cl_command_queue>
{
public:

    /*!
     * Trivial empty constructor to create a null queue.
     */
    DeviceCommandQueue() { }

    /*!
     * Default construct device command queue on default context and device
     */
    DeviceCommandQueue(DeviceQueueProperties properties, cl_int* err = NULL)
    {
        cl_int error;
        cl::Context context = cl::Context::getDefault();
        cl::Device device = cl::Device::getDefault();

        cl_command_queue_properties mergedProperties =
            CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE | CL_QUEUE_ON_DEVICE | static_cast<cl_command_queue_properties>(properties);

        cl_queue_properties queue_properties[] = {
            CL_QUEUE_PROPERTIES, mergedProperties, 0 };
        object_ = ::clCreateCommandQueueWithProperties(
            context(), device(), queue_properties, &error);

        detail::errHandler(error, __CREATE_COMMAND_QUEUE_WITH_PROPERTIES_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    /*!
     * Create a device command queue for a specified device in the passed context.
     */
    DeviceCommandQueue(
        const Context& context,
        const Device& device,
        DeviceQueueProperties properties = DeviceQueueProperties::None,
        cl_int* err = NULL)
    {
        cl_int error;

        cl_command_queue_properties mergedProperties =
            CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE | CL_QUEUE_ON_DEVICE | static_cast<cl_command_queue_properties>(properties);
        cl_queue_properties queue_properties[] = {
            CL_QUEUE_PROPERTIES, mergedProperties, 0 };
        object_ = ::clCreateCommandQueueWithProperties(
            context(), device(), queue_properties, &error);

        detail::errHandler(error, __CREATE_COMMAND_QUEUE_WITH_PROPERTIES_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    /*!
     * Create a device command queue for a specified device in the passed context.
     */
    DeviceCommandQueue(
        const Context& context,
        const Device& device,
        cl_uint queueSize,
        DeviceQueueProperties properties = DeviceQueueProperties::None,
        cl_int* err = NULL)
    {
        cl_int error;

        cl_command_queue_properties mergedProperties =
            CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE | CL_QUEUE_ON_DEVICE | static_cast<cl_command_queue_properties>(properties);
        cl_queue_properties queue_properties[] = {
            CL_QUEUE_PROPERTIES, mergedProperties,
            CL_QUEUE_SIZE, queueSize, 
            0 };
        object_ = ::clCreateCommandQueueWithProperties(
            context(), device(), queue_properties, &error);

        detail::errHandler(error, __CREATE_COMMAND_QUEUE_WITH_PROPERTIES_ERR);
        if (err != NULL) {
            *err = error;
        }
    }

    /*! \brief Constructor from cl_command_queue - takes ownership.
    *
    * \param retainObject will cause the constructor to retain its cl object.
    *                     Defaults to false to maintain compatibility with
    *                     earlier versions.
    */
    explicit DeviceCommandQueue(const cl_command_queue& commandQueue, bool retainObject = false) :
        detail::Wrapper<cl_type>(commandQueue, retainObject) { }

    DeviceCommandQueue& operator = (const cl_command_queue& rhs)
    {
        detail::Wrapper<cl_type>::operator=(rhs);
        return *this;
    }

    /*! \brief Copy constructor to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    DeviceCommandQueue(const DeviceCommandQueue& queue) : detail::Wrapper<cl_type>(queue) {}

    /*! \brief Copy assignment to forward copy to the superclass correctly.
     * Required for MSVC.
     */
    DeviceCommandQueue& operator = (const DeviceCommandQueue &queue)
    {
        detail::Wrapper<cl_type>::operator=(queue);
        return *this;
    }

    /*! \brief Move constructor to forward move to the superclass correctly.
     * Required for MSVC.
     */
    DeviceCommandQueue(DeviceCommandQueue&& queue) CL_HPP_NOEXCEPT_ : detail::Wrapper<cl_type>(std::move(queue)) {}

    /*! \brief Move assignment to forward move to the superclass correctly.
     * Required for MSVC.
     */
    DeviceCommandQueue& operator = (DeviceCommandQueue &&queue)
    {
        detail::Wrapper<cl_type>::operator=(std::move(queue));
        return *this;
    }

    template <typename T>
    cl_int getInfo(cl_command_queue_info name, T* param) const
    {
        return detail::errHandler(
            detail::getInfo(
            &::clGetCommandQueueInfo, object_, name, param),
            __GET_COMMAND_QUEUE_INFO_ERR);
    }

    template <cl_int name> typename
        detail::param_traits<detail::cl_command_queue_info, name>::param_type
        getInfo(cl_int* err = NULL) const
    {
        typename detail::param_traits<
            detail::cl_command_queue_info, name>::param_type param;
        cl_int result = getInfo(name, &param);
        if (err != NULL) {
            *err = result;
        }
        return param;
    }

    /*!
    * Create a new default device command queue for the default device,
    * in the default context and of the default size.
    * If there is already a default queue for the specified device this
    * function will return the pre-existing queue.
    */
    static DeviceCommandQueue makeDefault(
        cl_int *err = nullptr)
    {
        cl_int error;
        cl::Context context = cl::Context::getDefault();
        cl::Device device = cl::Device::getDefault();

        cl_command_queue_properties properties =
            CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE | CL_QUEUE_ON_DEVICE | CL_QUEUE_ON_DEVICE_DEFAULT;
        cl_queue_properties queue_properties[] = {
            CL_QUEUE_PROPERTIES, properties,
            0 };
        DeviceCommandQueue deviceQueue(
            ::clCreateCommandQueueWithProperties(
            context(), device(), queue_properties, &error));

        detail::errHandler(error, __CREATE_COMMAND_QUEUE_WITH_PROPERTIES_ERR);
        if (err != NULL) {
            *err = error;
        }

        return deviceQueue;
    }

    /*!
    * Create a new default device command queue for the specified device
    * and of the default size.
    * If there is already a default queue for the specified device this
    * function will return the pre-existing queue.
    */
    static DeviceCommandQueue makeDefault(
        const Context &context, const Device &device, cl_int *err = nullptr)
    {
        cl_int error;

        cl_command_queue_properties properties =
            CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE | CL_QUEUE_ON_DEVICE | CL_QUEUE_ON_DEVICE_DEFAULT;
        cl_queue_properties queue_properties[] = {
            CL_QUEUE_PROPERTIES, properties,
            0 };
        DeviceCommandQueue deviceQueue(
            ::clCreateCommandQueueWithProperties(
            context(), device(), queue_properties, &error));

        detail::errHandler(error, __CREATE_COMMAND_QUEUE_WITH_PROPERTIES_ERR);
        if (err != NULL) {
            *err = error;
        }

        return deviceQueue;
    }

    /*!
     * Create a new default device command queue for the specified device 
     * and of the requested size in bytes.
     * If there is already a default queue for the specified device this
     * function will return the pre-existing queue.
     */
    static DeviceCommandQueue makeDefault(
        const Context &context, const Device &device, cl_uint queueSize, cl_int *err = nullptr)
    {
        cl_int error;

        cl_command_queue_properties properties =
            CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE | CL_QUEUE_ON_DEVICE | CL_QUEUE_ON_DEVICE_DEFAULT;
        cl_queue_properties queue_properties[] = {
            CL_QUEUE_PROPERTIES, properties,
            CL_QUEUE_SIZE, queueSize,
            0 };
        DeviceCommandQueue deviceQueue(
            ::clCreateCommandQueueWithProperties(
                context(), device(), queue_properties, &error));

        detail::errHandler(error, __CREATE_COMMAND_QUEUE_WITH_PROPERTIES_ERR);
        if (err != NULL) {
            *err = error;
        }

        return deviceQueue;
    }
}; // DeviceCommandQueue

namespace detail
{
    // Specialization for device command queue
    template <>
    struct KernelArgumentHandler<cl::DeviceCommandQueue, void>
    {
        static size_type size(const cl::DeviceCommandQueue&) { return sizeof(cl_command_queue); }
        static const cl_command_queue* ptr(const cl::DeviceCommandQueue& value) { return &(value()); }
    };
} // namespace detail

#endif // #if CL_HPP_TARGET_OPENCL_VERSION >= 200


template< typename IteratorType >
Buffer::Buffer(
    const Context &context,
    IteratorType startIterator,
    IteratorType endIterator,
    bool readOnly,
    bool useHostPtr,
    cl_int* err)
{
    typedef typename std::iterator_traits<IteratorType>::value_type DataType;
    cl_int error;

    cl_mem_flags flags = 0;
    if( readOnly ) {
        flags |= CL_MEM_READ_ONLY;
    }
    else {
        flags |= CL_MEM_READ_WRITE;
    }
    if( useHostPtr ) {
        flags |= CL_MEM_USE_HOST_PTR;
    }
    
    size_type size = sizeof(DataType)*(endIterator - startIterator);

    if( useHostPtr ) {
        object_ = ::clCreateBuffer(context(), flags, size, static_cast<DataType*>(&*startIterator), &error);
    } else {
        object_ = ::clCreateBuffer(context(), flags, size, 0, &error);
    }

    detail::errHandler(error, __CREATE_BUFFER_ERR);
    if (err != NULL) {
        *err = error;
    }

    if( !useHostPtr ) {
        CommandQueue queue(context, 0, &error);
        detail::errHandler(error, __CREATE_BUFFER_ERR);
        if (err != NULL) {
            *err = error;
        }

        error = cl::copy(queue, startIterator, endIterator, *this);
        detail::errHandler(error, __CREATE_BUFFER_ERR);
        if (err != NULL) {
            *err = error;
        }
    }
}

template< typename IteratorType >
Buffer::Buffer(
    const CommandQueue &queue,
    IteratorType startIterator,
    IteratorType endIterator,
    bool readOnly,
    bool useHostPtr,
    cl_int* err)
{
    typedef typename std::iterator_traits<IteratorType>::value_type DataType;
    cl_int error;

    cl_mem_flags flags = 0;
    if (readOnly) {
        flags |= CL_MEM_READ_ONLY;
    }
    else {
        flags |= CL_MEM_READ_WRITE;
    }
    if (useHostPtr) {
        flags |= CL_MEM_USE_HOST_PTR;
    }

    size_type size = sizeof(DataType)*(endIterator - startIterator);

    Context context = queue.getInfo<CL_QUEUE_CONTEXT>();

    if (useHostPtr) {
        object_ = ::clCreateBuffer(context(), flags, size, static_cast<DataType*>(&*startIterator), &error);
    }
    else {
        object_ = ::clCreateBuffer(context(), flags, size, 0, &error);
    }

    detail::errHandler(error, __CREATE_BUFFER_ERR);
    if (err != NULL) {
        *err = error;
    }

    if (!useHostPtr) {
        error = cl::copy(queue, startIterator, endIterator, *this);
        detail::errHandler(error, __CREATE_BUFFER_ERR);
        if (err != NULL) {
            *err = error;
        }
    }
}

inline cl_int enqueueReadBuffer(
    const Buffer& buffer,
    cl_bool blocking,
    size_type offset,
    size_type size,
    void* ptr,
    const vector<Event>* events = NULL,
    Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);

    if (error != CL_SUCCESS) {
        return error;
    }

    return queue.enqueueReadBuffer(buffer, blocking, offset, size, ptr, events, event);
}

inline cl_int enqueueWriteBuffer(
        const Buffer& buffer,
        cl_bool blocking,
        size_type offset,
        size_type size,
        const void* ptr,
        const vector<Event>* events = NULL,
        Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);

    if (error != CL_SUCCESS) {
        return error;
    }

    return queue.enqueueWriteBuffer(buffer, blocking, offset, size, ptr, events, event);
}

inline void* enqueueMapBuffer(
        const Buffer& buffer,
        cl_bool blocking,
        cl_map_flags flags,
        size_type offset,
        size_type size,
        const vector<Event>* events = NULL,
        Event* event = NULL,
        cl_int* err = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);
    detail::errHandler(error, __ENQUEUE_MAP_BUFFER_ERR);
    if (err != NULL) {
        *err = error;
    }

    void * result = ::clEnqueueMapBuffer(
            queue(), buffer(), blocking, flags, offset, size,
            (events != NULL) ? (cl_uint) events->size() : 0,
            (events != NULL && events->size() > 0) ? (cl_event*) &events->front() : NULL,
            (cl_event*) event,
            &error);

    detail::errHandler(error, __ENQUEUE_MAP_BUFFER_ERR);
    if (err != NULL) {
        *err = error;
    }
    return result;
}


#if CL_HPP_TARGET_OPENCL_VERSION >= 200
/**
 * Enqueues to the default queue a command that will allow the host to
 * update a region of a coarse-grained SVM buffer.
 * This variant takes a raw SVM pointer.
 */
template<typename T>
inline cl_int enqueueMapSVM(
    T* ptr,
    cl_bool blocking,
    cl_map_flags flags,
    size_type size,
    const vector<Event>* events,
    Event* event)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);
    if (error != CL_SUCCESS) {
        return detail::errHandler(error, __ENQUEUE_MAP_BUFFER_ERR);
    }

    return queue.enqueueMapSVM(
        ptr, blocking, flags, size, events, event);
}

/**
 * Enqueues to the default queue a command that will allow the host to 
 * update a region of a coarse-grained SVM buffer.
 * This variant takes a cl::pointer instance.
 */
template<typename T, class D>
inline cl_int enqueueMapSVM(
    cl::pointer<T, D> ptr,
    cl_bool blocking,
    cl_map_flags flags,
    size_type size,
    const vector<Event>* events = NULL,
    Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);
    if (error != CL_SUCCESS) {
        return detail::errHandler(error, __ENQUEUE_MAP_BUFFER_ERR);
    }

    return queue.enqueueMapSVM(
        ptr, blocking, flags, size, events, event);
}

/**
 * Enqueues to the default queue a command that will allow the host to
 * update a region of a coarse-grained SVM buffer.
 * This variant takes a cl::vector instance.
 */
template<typename T, class Alloc>
inline cl_int enqueueMapSVM(
    cl::vector<T, Alloc> container,
    cl_bool blocking,
    cl_map_flags flags,
    const vector<Event>* events = NULL,
    Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);
    if (error != CL_SUCCESS) {
        return detail::errHandler(error, __ENQUEUE_MAP_BUFFER_ERR);
    }

    return queue.enqueueMapSVM(
        container, blocking, flags, events, event);
}

#endif // #if CL_HPP_TARGET_OPENCL_VERSION >= 200

inline cl_int enqueueUnmapMemObject(
    const Memory& memory,
    void* mapped_ptr,
    const vector<Event>* events = NULL,
    Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);
    detail::errHandler(error, __ENQUEUE_MAP_BUFFER_ERR);
    if (error != CL_SUCCESS) {
        return error;
    }

    cl_event tmp;
    cl_int err = detail::errHandler(
        ::clEnqueueUnmapMemObject(
        queue(), memory(), mapped_ptr,
        (events != NULL) ? (cl_uint)events->size() : 0,
        (events != NULL && events->size() > 0) ? (cl_event*)&events->front() : NULL,
        (event != NULL) ? &tmp : NULL),
        __ENQUEUE_UNMAP_MEM_OBJECT_ERR);

    if (event != NULL && err == CL_SUCCESS)
        *event = tmp;

    return err;
}

#if CL_HPP_TARGET_OPENCL_VERSION >= 200
/**
 * Enqueues to the default queue a command that will release a coarse-grained 
 * SVM buffer back to the OpenCL runtime.
 * This variant takes a raw SVM pointer.
 */
template<typename T>
inline cl_int enqueueUnmapSVM(
    T* ptr,
    const vector<Event>* events = NULL,
    Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);
    if (error != CL_SUCCESS) {
        return detail::errHandler(error, __ENQUEUE_UNMAP_MEM_OBJECT_ERR);
    }

    return detail::errHandler(queue.enqueueUnmapSVM(ptr, events, event), 
        __ENQUEUE_UNMAP_MEM_OBJECT_ERR);

}

/**
 * Enqueues to the default queue a command that will release a coarse-grained 
 * SVM buffer back to the OpenCL runtime.
 * This variant takes a cl::pointer instance.
 */
template<typename T, class D>
inline cl_int enqueueUnmapSVM(
    cl::pointer<T, D> &ptr,
    const vector<Event>* events = NULL,
    Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);
    if (error != CL_SUCCESS) {
        return detail::errHandler(error, __ENQUEUE_UNMAP_MEM_OBJECT_ERR);
    }

    return detail::errHandler(queue.enqueueUnmapSVM(ptr, events, event),
        __ENQUEUE_UNMAP_MEM_OBJECT_ERR);
}

/**
 * Enqueues to the default queue a command that will release a coarse-grained 
 * SVM buffer back to the OpenCL runtime.
 * This variant takes a cl::vector instance.
 */
template<typename T, class Alloc>
inline cl_int enqueueUnmapSVM(
    cl::vector<T, Alloc> &container,
    const vector<Event>* events = NULL,
    Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);
    if (error != CL_SUCCESS) {
        return detail::errHandler(error, __ENQUEUE_UNMAP_MEM_OBJECT_ERR);
    }

    return detail::errHandler(queue.enqueueUnmapSVM(container, events, event),
        __ENQUEUE_UNMAP_MEM_OBJECT_ERR);
}

#endif // #if CL_HPP_TARGET_OPENCL_VERSION >= 200

inline cl_int enqueueCopyBuffer(
        const Buffer& src,
        const Buffer& dst,
        size_type src_offset,
        size_type dst_offset,
        size_type size,
        const vector<Event>* events = NULL,
        Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);

    if (error != CL_SUCCESS) {
        return error;
    }

    return queue.enqueueCopyBuffer(src, dst, src_offset, dst_offset, size, events, event);
}

/**
 * Blocking copy operation between iterators and a buffer.
 * Host to Device.
 * Uses default command queue.
 */
template< typename IteratorType >
inline cl_int copy( IteratorType startIterator, IteratorType endIterator, cl::Buffer &buffer )
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);
    if (error != CL_SUCCESS)
        return error;

    return cl::copy(queue, startIterator, endIterator, buffer);
}

/**
 * Blocking copy operation between iterators and a buffer.
 * Device to Host.
 * Uses default command queue.
 */
template< typename IteratorType >
inline cl_int copy( const cl::Buffer &buffer, IteratorType startIterator, IteratorType endIterator )
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);
    if (error != CL_SUCCESS)
        return error;

    return cl::copy(queue, buffer, startIterator, endIterator);
}

/**
 * Blocking copy operation between iterators and a buffer.
 * Host to Device.
 * Uses specified queue.
 */
template< typename IteratorType >
inline cl_int copy( const CommandQueue &queue, IteratorType startIterator, IteratorType endIterator, cl::Buffer &buffer )
{
    typedef typename std::iterator_traits<IteratorType>::value_type DataType;
    cl_int error;
    
    size_type length = endIterator-startIterator;
    size_type byteLength = length*sizeof(DataType);

    DataType *pointer = 
        static_cast<DataType*>(queue.enqueueMapBuffer(buffer, CL_TRUE, CL_MAP_WRITE, 0, byteLength, 0, 0, &error));
    // if exceptions enabled, enqueueMapBuffer will throw
    if( error != CL_SUCCESS ) {
        return error;
    }
#if defined(_MSC_VER)
    std::copy(
        startIterator, 
        endIterator, 
        stdext::checked_array_iterator<DataType*>(
            pointer, length));
#else
    std::copy(startIterator, endIterator, pointer);
#endif
    Event endEvent;
    error = queue.enqueueUnmapMemObject(buffer, pointer, 0, &endEvent);
    // if exceptions enabled, enqueueUnmapMemObject will throw
    if( error != CL_SUCCESS ) { 
        return error;
    }
    endEvent.wait();
    return CL_SUCCESS;
}

/**
 * Blocking copy operation between iterators and a buffer.
 * Device to Host.
 * Uses specified queue.
 */
template< typename IteratorType >
inline cl_int copy( const CommandQueue &queue, const cl::Buffer &buffer, IteratorType startIterator, IteratorType endIterator )
{
    typedef typename std::iterator_traits<IteratorType>::value_type DataType;
    cl_int error;
        
    size_type length = endIterator-startIterator;
    size_type byteLength = length*sizeof(DataType);

    DataType *pointer = 
        static_cast<DataType*>(queue.enqueueMapBuffer(buffer, CL_TRUE, CL_MAP_READ, 0, byteLength, 0, 0, &error));
    // if exceptions enabled, enqueueMapBuffer will throw
    if( error != CL_SUCCESS ) {
        return error;
    }
    std::copy(pointer, pointer + length, startIterator);
    Event endEvent;
    error = queue.enqueueUnmapMemObject(buffer, pointer, 0, &endEvent);
    // if exceptions enabled, enqueueUnmapMemObject will throw
    if( error != CL_SUCCESS ) { 
        return error;
    }
    endEvent.wait();
    return CL_SUCCESS;
}


#if CL_HPP_TARGET_OPENCL_VERSION >= 200
/**
 * Blocking SVM map operation - performs a blocking map underneath.
 */
template<typename T, class Alloc>
inline cl_int mapSVM(cl::vector<T, Alloc> &container)
{
    return enqueueMapSVM(container, CL_TRUE, CL_MAP_READ | CL_MAP_WRITE);
}

/**
* Blocking SVM map operation - performs a blocking map underneath.
*/
template<typename T, class Alloc>
inline cl_int unmapSVM(cl::vector<T, Alloc> &container)
{
    return enqueueUnmapSVM(container);
}

#endif // #if CL_HPP_TARGET_OPENCL_VERSION >= 200

#if CL_HPP_TARGET_OPENCL_VERSION >= 110
inline cl_int enqueueReadBufferRect(
    const Buffer& buffer,
    cl_bool blocking,
    const array<size_type, 3>& buffer_offset,
    const array<size_type, 3>& host_offset,
    const array<size_type, 3>& region,
    size_type buffer_row_pitch,
    size_type buffer_slice_pitch,
    size_type host_row_pitch,
    size_type host_slice_pitch,
    void *ptr,
    const vector<Event>* events = NULL,
    Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);

    if (error != CL_SUCCESS) {
        return error;
    }

    return queue.enqueueReadBufferRect(
        buffer, 
        blocking, 
        buffer_offset, 
        host_offset,
        region,
        buffer_row_pitch,
        buffer_slice_pitch,
        host_row_pitch,
        host_slice_pitch,
        ptr, 
        events, 
        event);
}

inline cl_int enqueueWriteBufferRect(
    const Buffer& buffer,
    cl_bool blocking,
    const array<size_type, 3>& buffer_offset,
    const array<size_type, 3>& host_offset,
    const array<size_type, 3>& region,
    size_type buffer_row_pitch,
    size_type buffer_slice_pitch,
    size_type host_row_pitch,
    size_type host_slice_pitch,
    const void *ptr,
    const vector<Event>* events = NULL,
    Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);

    if (error != CL_SUCCESS) {
        return error;
    }

    return queue.enqueueWriteBufferRect(
        buffer, 
        blocking, 
        buffer_offset, 
        host_offset,
        region,
        buffer_row_pitch,
        buffer_slice_pitch,
        host_row_pitch,
        host_slice_pitch,
        ptr, 
        events, 
        event);
}

inline cl_int enqueueCopyBufferRect(
    const Buffer& src,
    const Buffer& dst,
    const array<size_type, 3>& src_origin,
    const array<size_type, 3>& dst_origin,
    const array<size_type, 3>& region,
    size_type src_row_pitch,
    size_type src_slice_pitch,
    size_type dst_row_pitch,
    size_type dst_slice_pitch,
    const vector<Event>* events = NULL,
    Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);

    if (error != CL_SUCCESS) {
        return error;
    }

    return queue.enqueueCopyBufferRect(
        src,
        dst,
        src_origin,
        dst_origin,
        region,
        src_row_pitch,
        src_slice_pitch,
        dst_row_pitch,
        dst_slice_pitch,
        events, 
        event);
}
#endif // CL_HPP_TARGET_OPENCL_VERSION >= 110

inline cl_int enqueueReadImage(
    const Image& image,
    cl_bool blocking,
    const array<size_type, 3>& origin,
    const array<size_type, 3>& region,
    size_type row_pitch,
    size_type slice_pitch,
    void* ptr,
    const vector<Event>* events = NULL,
    Event* event = NULL) 
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);

    if (error != CL_SUCCESS) {
        return error;
    }

    return queue.enqueueReadImage(
        image,
        blocking,
        origin,
        region,
        row_pitch,
        slice_pitch,
        ptr,
        events, 
        event);
}

inline cl_int enqueueWriteImage(
    const Image& image,
    cl_bool blocking,
    const array<size_type, 3>& origin,
    const array<size_type, 3>& region,
    size_type row_pitch,
    size_type slice_pitch,
    const void* ptr,
    const vector<Event>* events = NULL,
    Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);

    if (error != CL_SUCCESS) {
        return error;
    }

    return queue.enqueueWriteImage(
        image,
        blocking,
        origin,
        region,
        row_pitch,
        slice_pitch,
        ptr,
        events, 
        event);
}

inline cl_int enqueueCopyImage(
    const Image& src,
    const Image& dst,
    const array<size_type, 3>& src_origin,
    const array<size_type, 3>& dst_origin,
    const array<size_type, 3>& region,
    const vector<Event>* events = NULL,
    Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);

    if (error != CL_SUCCESS) {
        return error;
    }

    return queue.enqueueCopyImage(
        src,
        dst,
        src_origin,
        dst_origin,
        region,
        events,
        event);
}

inline cl_int enqueueCopyImageToBuffer(
    const Image& src,
    const Buffer& dst,
    const array<size_type, 3>& src_origin,
    const array<size_type, 3>& region,
    size_type dst_offset,
    const vector<Event>* events = NULL,
    Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);

    if (error != CL_SUCCESS) {
        return error;
    }

    return queue.enqueueCopyImageToBuffer(
        src,
        dst,
        src_origin,
        region,
        dst_offset,
        events,
        event);
}

inline cl_int enqueueCopyBufferToImage(
    const Buffer& src,
    const Image& dst,
    size_type src_offset,
    const array<size_type, 3>& dst_origin,
    const array<size_type, 3>& region,
    const vector<Event>* events = NULL,
    Event* event = NULL)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);

    if (error != CL_SUCCESS) {
        return error;
    }

    return queue.enqueueCopyBufferToImage(
        src,
        dst,
        src_offset,
        dst_origin,
        region,
        events,
        event);
}


inline cl_int flush(void)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);

    if (error != CL_SUCCESS) {
        return error;
    }

    return queue.flush();
}

inline cl_int finish(void)
{
    cl_int error;
    CommandQueue queue = CommandQueue::getDefault(&error);

    if (error != CL_SUCCESS) {
        return error;
    } 


    return queue.finish();
}

class EnqueueArgs
{
private:
    CommandQueue queue_;
    const NDRange offset_;
    const NDRange global_;
    const NDRange local_;
    vector<Event> events_;

    template<typename... Ts>
    friend class KernelFunctor;

public:
    EnqueueArgs(NDRange global) : 
      queue_(CommandQueue::getDefault()),
      offset_(NullRange), 
      global_(global),
      local_(NullRange)
    {

    }

    EnqueueArgs(NDRange global, NDRange local) : 
      queue_(CommandQueue::getDefault()),
      offset_(NullRange), 
      global_(global),
      local_(local)
    {

    }

    EnqueueArgs(NDRange offset, NDRange global, NDRange local) : 
      queue_(CommandQueue::getDefault()),
      offset_(offset), 
      global_(global),
      local_(local)
    {

    }

    EnqueueArgs(Event e, NDRange global) : 
      queue_(CommandQueue::getDefault()),
      offset_(NullRange), 
      global_(global),
      local_(NullRange)
    {
        events_.push_back(e);
    }

    EnqueueArgs(Event e, NDRange global, NDRange local) : 
      queue_(CommandQueue::getDefault()),
      offset_(NullRange), 
      global_(global),
      local_(local)
    {
        events_.push_back(e);
    }

    EnqueueArgs(Event e, NDRange offset, NDRange global, NDRange local) : 
      queue_(CommandQueue::getDefault()),
      offset_(offset), 
      global_(global),
      local_(local)
    {
        events_.push_back(e);
    }

    EnqueueArgs(const vector<Event> &events, NDRange global) : 
      queue_(CommandQueue::getDefault()),
      offset_(NullRange), 
      global_(global),
      local_(NullRange),
      events_(events)
    {

    }

    EnqueueArgs(const vector<Event> &events, NDRange global, NDRange local) : 
      queue_(CommandQueue::getDefault()),
      offset_(NullRange), 
      global_(global),
      local_(local),
      events_(events)
    {

    }

    EnqueueArgs(const vector<Event> &events, NDRange offset, NDRange global, NDRange local) : 
      queue_(CommandQueue::getDefault()),
      offset_(offset), 
      global_(global),
      local_(local),
      events_(events)
    {

    }

    EnqueueArgs(CommandQueue &queue, NDRange global) : 
      queue_(queue),
      offset_(NullRange), 
      global_(global),
      local_(NullRange)
    {

    }

    EnqueueArgs(CommandQueue &queue, NDRange global, NDRange local) : 
      queue_(queue),
      offset_(NullRange), 
      global_(global),
      local_(local)
    {

    }

    EnqueueArgs(CommandQueue &queue, NDRange offset, NDRange global, NDRange local) : 
      queue_(queue),
      offset_(offset), 
      global_(global),
      local_(local)
    {

    }

    EnqueueArgs(CommandQueue &queue, Event e, NDRange global) : 
      queue_(queue),
      offset_(NullRange), 
      global_(global),
      local_(NullRange)
    {
        events_.push_back(e);
    }

    EnqueueArgs(CommandQueue &queue, Event e, NDRange global, NDRange local) : 
      queue_(queue),
      offset_(NullRange), 
      global_(global),
      local_(local)
    {
        events_.push_back(e);
    }

    EnqueueArgs(CommandQueue &queue, Event e, NDRange offset, NDRange global, NDRange local) : 
      queue_(queue),
      offset_(offset), 
      global_(global),
      local_(local)
    {
        events_.push_back(e);
    }

    EnqueueArgs(CommandQueue &queue, const vector<Event> &events, NDRange global) : 
      queue_(queue),
      offset_(NullRange), 
      global_(global),
      local_(NullRange),
      events_(events)
    {

    }

    EnqueueArgs(CommandQueue &queue, const vector<Event> &events, NDRange global, NDRange local) : 
      queue_(queue),
      offset_(NullRange), 
      global_(global),
      local_(local),
      events_(events)
    {

    }

    EnqueueArgs(CommandQueue &queue, const vector<Event> &events, NDRange offset, NDRange global, NDRange local) : 
      queue_(queue),
      offset_(offset), 
      global_(global),
      local_(local),
      events_(events)
    {

    }
};


//----------------------------------------------------------------------------------------------


/**
 * Type safe kernel functor.
 * 
 */
template<typename... Ts>
class KernelFunctor
{
private:
    Kernel kernel_;

    template<int index, typename T0, typename... T1s>
    void setArgs(T0&& t0, T1s&&... t1s)
    {
        kernel_.setArg(index, t0);
        setArgs<index + 1, T1s...>(std::forward<T1s>(t1s)...);
    }

    template<int index, typename T0>
    void setArgs(T0&& t0)
    {
        kernel_.setArg(index, t0);
    }

    template<int index>
    void setArgs()
    {
    }


public:
    KernelFunctor(Kernel kernel) : kernel_(kernel)
    {}

    KernelFunctor(
        const Program& program,
        const string name,
        cl_int * err = NULL) :
        kernel_(program, name.c_str(), err)
    {}

    //! \brief Return type of the functor
    typedef Event result_type;

    /**
     * Enqueue kernel.
     * @param args Launch parameters of the kernel.
     * @param t0... List of kernel arguments based on the template type of the functor.
     */
    Event operator() (
        const EnqueueArgs& args,
        Ts... ts)
    {
        Event event;
        setArgs<0>(std::forward<Ts>(ts)...);
        
        args.queue_.enqueueNDRangeKernel(
            kernel_,
            args.offset_,
            args.global_,
            args.local_,
            &args.events_,
            &event);

        return event;
    }

    /**
    * Enqueue kernel with support for error code.
    * @param args Launch parameters of the kernel.
    * @param t0... List of kernel arguments based on the template type of the functor.
    * @param error Out parameter returning the error code from the execution.
    */
    Event operator() (
        const EnqueueArgs& args,
        Ts... ts,
        cl_int &error)
    {
        Event event;
        setArgs<0>(std::forward<Ts>(ts)...);

        error = args.queue_.enqueueNDRangeKernel(
            kernel_,
            args.offset_,
            args.global_,
            args.local_,
            &args.events_,
            &event);
        
        return event;
    }

#if CL_HPP_TARGET_OPENCL_VERSION >= 200
    cl_int setSVMPointers(const vector<void*> &pointerList)
    {
        return kernel_.setSVMPointers(pointerList);
    }

    template<typename T0, typename... T1s>
    cl_int setSVMPointers(const T0 &t0, T1s &... ts)
    {
        return kernel_.setSVMPointers(t0, ts...);
    }
#endif // #if CL_HPP_TARGET_OPENCL_VERSION >= 200

    Kernel getKernel()
    {
        return kernel_;
    }
};

namespace compatibility {
    /**
     * Backward compatibility class to ensure that cl.hpp code works with cl2.hpp.
     * Please use KernelFunctor directly.
     */
    template<typename... Ts>
    struct make_kernel
    {
        typedef KernelFunctor<Ts...> FunctorType;

        FunctorType functor_;

        make_kernel(
            const Program& program,
            const string name,
            cl_int * err = NULL) :
            functor_(FunctorType(program, name, err))
        {}

        make_kernel(
            const Kernel kernel) :
            functor_(FunctorType(kernel))
        {}

        //! \brief Return type of the functor
        typedef Event result_type;

        //! \brief Function signature of kernel functor with no event dependency.
        typedef Event type_(
            const EnqueueArgs&,
            Ts...);

        Event operator()(
            const EnqueueArgs& enqueueArgs,
            Ts... args)
        {
            return functor_(
                enqueueArgs, args...);
        }
    };
} // namespace compatibility


//----------------------------------------------------------------------------------------------------------------------

#undef CL_HPP_ERR_STR_
#if !defined(CL_HPP_USER_OVERRIDE_ERROR_STRINGS)
#undef __GET_DEVICE_INFO_ERR
#undef __GET_PLATFORM_INFO_ERR
#undef __GET_DEVICE_IDS_ERR
#undef __GET_CONTEXT_INFO_ERR
#undef __GET_EVENT_INFO_ERR
#undef __GET_EVENT_PROFILE_INFO_ERR
#undef __GET_MEM_OBJECT_INFO_ERR
#undef __GET_IMAGE_INFO_ERR
#undef __GET_SAMPLER_INFO_ERR
#undef __GET_KERNEL_INFO_ERR
#undef __GET_KERNEL_ARG_INFO_ERR
#undef __GET_KERNEL_WORK_GROUP_INFO_ERR
#undef __GET_PROGRAM_INFO_ERR
#undef __GET_PROGRAM_BUILD_INFO_ERR
#undef __GET_COMMAND_QUEUE_INFO_ERR

#undef __CREATE_CONTEXT_ERR
#undef __CREATE_CONTEXT_FROM_TYPE_ERR
#undef __GET_SUPPORTED_IMAGE_FORMATS_ERR

#undef __CREATE_BUFFER_ERR
#undef __CREATE_SUBBUFFER_ERR
#undef __CREATE_IMAGE2D_ERR
#undef __CREATE_IMAGE3D_ERR
#undef __CREATE_SAMPLER_ERR
#undef __SET_MEM_OBJECT_DESTRUCTOR_CALLBACK_ERR

#undef __CREATE_USER_EVENT_ERR
#undef __SET_USER_EVENT_STATUS_ERR
#undef __SET_EVENT_CALLBACK_ERR
#undef __SET_PRINTF_CALLBACK_ERR

#undef __WAIT_FOR_EVENTS_ERR

#undef __CREATE_KERNEL_ERR
#undef __SET_KERNEL_ARGS_ERR
#undef __CREATE_PROGRAM_WITH_SOURCE_ERR
#undef __CREATE_PROGRAM_WITH_BINARY_ERR
#undef __CREATE_PROGRAM_WITH_BUILT_IN_KERNELS_ERR
#undef __BUILD_PROGRAM_ERR
#undef __CREATE_KERNELS_IN_PROGRAM_ERR

#undef __CREATE_COMMAND_QUEUE_ERR
#undef __SET_COMMAND_QUEUE_PROPERTY_ERR
#undef __ENQUEUE_READ_BUFFER_ERR
#undef __ENQUEUE_WRITE_BUFFER_ERR
#undef __ENQUEUE_READ_BUFFER_RECT_ERR
#undef __ENQUEUE_WRITE_BUFFER_RECT_ERR
#undef __ENQEUE_COPY_BUFFER_ERR
#undef __ENQEUE_COPY_BUFFER_RECT_ERR
#undef __ENQUEUE_READ_IMAGE_ERR
#undef __ENQUEUE_WRITE_IMAGE_ERR
#undef __ENQUEUE_COPY_IMAGE_ERR
#undef __ENQUEUE_COPY_IMAGE_TO_BUFFER_ERR
#undef __ENQUEUE_COPY_BUFFER_TO_IMAGE_ERR
#undef __ENQUEUE_MAP_BUFFER_ERR
#undef __ENQUEUE_MAP_IMAGE_ERR
#undef __ENQUEUE_UNMAP_MEM_OBJECT_ERR
#undef __ENQUEUE_NDRANGE_KERNEL_ERR
#undef __ENQUEUE_TASK_ERR
#undef __ENQUEUE_NATIVE_KERNEL

#undef __UNLOAD_COMPILER_ERR
#undef __CREATE_SUB_DEVICES_ERR

#undef __CREATE_PIPE_ERR
#undef __GET_PIPE_INFO_ERR

#endif //CL_HPP_USER_OVERRIDE_ERROR_STRINGS

// Extensions
#undef CL_HPP_INIT_CL_EXT_FCN_PTR_
#undef CL_HPP_INIT_CL_EXT_FCN_PTR_PLATFORM_

#if defined(CL_HPP_USE_CL_DEVICE_FISSION)
#undef CL_HPP_PARAM_NAME_DEVICE_FISSION_
#endif // CL_HPP_USE_CL_DEVICE_FISSION

#undef CL_HPP_NOEXCEPT_
#undef CL_HPP_DEFINE_STATIC_MEMBER_

} // namespace cl

#endif // CL_HPP_

```

`third_party/d3dx12.h`:

```h
//*********************************************************
//
// Copyright (c) Microsoft. All rights reserved.
// This code is licensed under the MIT License (MIT).
// THIS CODE IS PROVIDED *AS IS* WITHOUT WARRANTY OF
// ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING ANY
// IMPLIED WARRANTIES OF FITNESS FOR A PARTICULAR
// PURPOSE, MERCHANTABILITY, OR NON-INFRINGEMENT.
//
//*********************************************************

#ifndef __D3DX12_H__
#define __D3DX12_H__

#include "d3d12.h"

#if defined( __cplusplus )

struct CD3DX12_DEFAULT {};
extern const DECLSPEC_SELECTANY CD3DX12_DEFAULT D3D12_DEFAULT;

//------------------------------------------------------------------------------------------------
inline bool operator==( const D3D12_VIEWPORT& l, const D3D12_VIEWPORT& r )
{
    return l.TopLeftX == r.TopLeftX && l.TopLeftY == r.TopLeftY && l.Width == r.Width &&
        l.Height == r.Height && l.MinDepth == r.MinDepth && l.MaxDepth == r.MaxDepth;
}

//------------------------------------------------------------------------------------------------
inline bool operator!=( const D3D12_VIEWPORT& l, const D3D12_VIEWPORT& r )
{ return !( l == r ); }

//------------------------------------------------------------------------------------------------
struct CD3DX12_RECT : public D3D12_RECT
{
    CD3DX12_RECT()
    {}
    explicit CD3DX12_RECT( const D3D12_RECT& o ) :
        D3D12_RECT( o )
    {}
    explicit CD3DX12_RECT(
        LONG Left,
        LONG Top,
        LONG Right,
        LONG Bottom )
    {
        left = Left;
        top = Top;
        right = Right;
        bottom = Bottom;
    }
    ~CD3DX12_RECT() {}
    operator const D3D12_RECT&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_BOX : public D3D12_BOX
{
    CD3DX12_BOX()
    {}
    explicit CD3DX12_BOX( const D3D12_BOX& o ) :
        D3D12_BOX( o )
    {}
    explicit CD3DX12_BOX(
        LONG Left,
        LONG Right )
    {
        left = Left;
        top = 0;
        front = 0;
        right = Right;
        bottom = 1;
        back = 1;
    }
    explicit CD3DX12_BOX(
        LONG Left,
        LONG Top,
        LONG Right,
        LONG Bottom )
    {
        left = Left;
        top = Top;
        front = 0;
        right = Right;
        bottom = Bottom;
        back = 1;
    }
    explicit CD3DX12_BOX(
        LONG Left,
        LONG Top,
        LONG Front,
        LONG Right,
        LONG Bottom,
        LONG Back )
    {
        left = Left;
        top = Top;
        front = Front;
        right = Right;
        bottom = Bottom;
        back = Back;
    }
    ~CD3DX12_BOX() {}
    operator const D3D12_BOX&() const { return *this; }
};
inline bool operator==( const D3D12_BOX& l, const D3D12_BOX& r )
{
    return l.left == r.left && l.top == r.top && l.front == r.front &&
        l.right == r.right && l.bottom == r.bottom && l.back == r.back;
}
inline bool operator!=( const D3D12_BOX& l, const D3D12_BOX& r )
{ return !( l == r ); }

//------------------------------------------------------------------------------------------------
struct CD3DX12_DEPTH_STENCIL_DESC : public D3D12_DEPTH_STENCIL_DESC
{
    CD3DX12_DEPTH_STENCIL_DESC()
    {}
    explicit CD3DX12_DEPTH_STENCIL_DESC( const D3D12_DEPTH_STENCIL_DESC& o ) :
        D3D12_DEPTH_STENCIL_DESC( o )
    {}
    explicit CD3DX12_DEPTH_STENCIL_DESC( CD3DX12_DEFAULT )
    {
        DepthEnable = TRUE;
        DepthWriteMask = D3D12_DEPTH_WRITE_MASK_ALL;
        DepthFunc = D3D12_COMPARISON_FUNC_LESS;
        StencilEnable = FALSE;
        StencilReadMask = D3D12_DEFAULT_STENCIL_READ_MASK;
        StencilWriteMask = D3D12_DEFAULT_STENCIL_WRITE_MASK;
        const D3D12_DEPTH_STENCILOP_DESC defaultStencilOp =
        { D3D12_STENCIL_OP_KEEP, D3D12_STENCIL_OP_KEEP, D3D12_STENCIL_OP_KEEP, D3D12_COMPARISON_FUNC_ALWAYS };
        FrontFace = defaultStencilOp;
        BackFace = defaultStencilOp;
    }
    explicit CD3DX12_DEPTH_STENCIL_DESC(
        BOOL depthEnable,
        D3D12_DEPTH_WRITE_MASK depthWriteMask,
        D3D12_COMPARISON_FUNC depthFunc,
        BOOL stencilEnable,
        UINT8 stencilReadMask,
        UINT8 stencilWriteMask,
        D3D12_STENCIL_OP frontStencilFailOp,
        D3D12_STENCIL_OP frontStencilDepthFailOp,
        D3D12_STENCIL_OP frontStencilPassOp,
        D3D12_COMPARISON_FUNC frontStencilFunc,
        D3D12_STENCIL_OP backStencilFailOp,
        D3D12_STENCIL_OP backStencilDepthFailOp,
        D3D12_STENCIL_OP backStencilPassOp,
        D3D12_COMPARISON_FUNC backStencilFunc )
    {
        DepthEnable = depthEnable;
        DepthWriteMask = depthWriteMask;
        DepthFunc = depthFunc;
        StencilEnable = stencilEnable;
        StencilReadMask = stencilReadMask;
        StencilWriteMask = stencilWriteMask;
        FrontFace.StencilFailOp = frontStencilFailOp;
        FrontFace.StencilDepthFailOp = frontStencilDepthFailOp;
        FrontFace.StencilPassOp = frontStencilPassOp;
        FrontFace.StencilFunc = frontStencilFunc;
        BackFace.StencilFailOp = backStencilFailOp;
        BackFace.StencilDepthFailOp = backStencilDepthFailOp;
        BackFace.StencilPassOp = backStencilPassOp;
        BackFace.StencilFunc = backStencilFunc;
    }
    ~CD3DX12_DEPTH_STENCIL_DESC() {}
    operator const D3D12_DEPTH_STENCIL_DESC&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_BLEND_DESC : public D3D12_BLEND_DESC
{
    CD3DX12_BLEND_DESC()
    {}
    explicit CD3DX12_BLEND_DESC( const D3D12_BLEND_DESC& o ) :
        D3D12_BLEND_DESC( o )
    {}
    explicit CD3DX12_BLEND_DESC( CD3DX12_DEFAULT )
    {
        AlphaToCoverageEnable = FALSE;
        IndependentBlendEnable = FALSE;
        const D3D12_RENDER_TARGET_BLEND_DESC defaultRenderTargetBlendDesc =
        {
            FALSE,FALSE,
            D3D12_BLEND_ONE, D3D12_BLEND_ZERO, D3D12_BLEND_OP_ADD,
            D3D12_BLEND_ONE, D3D12_BLEND_ZERO, D3D12_BLEND_OP_ADD,
            D3D12_LOGIC_OP_NOOP,
            D3D12_COLOR_WRITE_ENABLE_ALL,
        };
        for (UINT i = 0; i < D3D12_SIMULTANEOUS_RENDER_TARGET_COUNT; ++i)
            RenderTarget[ i ] = defaultRenderTargetBlendDesc;
    }
    ~CD3DX12_BLEND_DESC() {}
    operator const D3D12_BLEND_DESC&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_RASTERIZER_DESC : public D3D12_RASTERIZER_DESC
{
    CD3DX12_RASTERIZER_DESC()
    {}
    explicit CD3DX12_RASTERIZER_DESC( const D3D12_RASTERIZER_DESC& o ) :
        D3D12_RASTERIZER_DESC( o )
    {}
    explicit CD3DX12_RASTERIZER_DESC( CD3DX12_DEFAULT )
    {
        FillMode = D3D12_FILL_MODE_SOLID;
        CullMode = D3D12_CULL_MODE_BACK;
        FrontCounterClockwise = FALSE;
        DepthBias = D3D12_DEFAULT_DEPTH_BIAS;
        DepthBiasClamp = D3D12_DEFAULT_DEPTH_BIAS_CLAMP;
        SlopeScaledDepthBias = D3D12_DEFAULT_SLOPE_SCALED_DEPTH_BIAS;
        DepthClipEnable = TRUE;
        MultisampleEnable = FALSE;
        AntialiasedLineEnable = FALSE;
        ForcedSampleCount = 0;
        ConservativeRaster = D3D12_CONSERVATIVE_RASTERIZATION_MODE_OFF;
    }
    explicit CD3DX12_RASTERIZER_DESC(
        D3D12_FILL_MODE fillMode,
        D3D12_CULL_MODE cullMode,
        BOOL frontCounterClockwise,
        INT depthBias,
        FLOAT depthBiasClamp,
        FLOAT slopeScaledDepthBias,
        BOOL depthClipEnable,
        BOOL multisampleEnable,
        BOOL antialiasedLineEnable, 
        UINT forcedSampleCount, 
        D3D12_CONSERVATIVE_RASTERIZATION_MODE conservativeRaster)
    {
        FillMode = fillMode;
        CullMode = cullMode;
        FrontCounterClockwise = frontCounterClockwise;
        DepthBias = depthBias;
        DepthBiasClamp = depthBiasClamp;
        SlopeScaledDepthBias = slopeScaledDepthBias;
        DepthClipEnable = depthClipEnable;
        MultisampleEnable = multisampleEnable;
        AntialiasedLineEnable = antialiasedLineEnable;
        ForcedSampleCount = forcedSampleCount;
        ConservativeRaster = conservativeRaster;
    }
    ~CD3DX12_RASTERIZER_DESC() {}
    operator const D3D12_RASTERIZER_DESC&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_RESOURCE_ALLOCATION_INFO : public D3D12_RESOURCE_ALLOCATION_INFO
{
    CD3DX12_RESOURCE_ALLOCATION_INFO()
    {}
    explicit CD3DX12_RESOURCE_ALLOCATION_INFO( const D3D12_RESOURCE_ALLOCATION_INFO& o ) :
        D3D12_RESOURCE_ALLOCATION_INFO( o )
    {}
    CD3DX12_RESOURCE_ALLOCATION_INFO(
        UINT64 size,
        UINT64 alignment )
    {
        SizeInBytes = size;
        Alignment = alignment;
    }
    operator const D3D12_RESOURCE_ALLOCATION_INFO&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_HEAP_PROPERTIES : public D3D12_HEAP_PROPERTIES
{
    CD3DX12_HEAP_PROPERTIES()
    {}
    explicit CD3DX12_HEAP_PROPERTIES(const D3D12_HEAP_PROPERTIES &o) :
        D3D12_HEAP_PROPERTIES(o)
    {}
    CD3DX12_HEAP_PROPERTIES( 
        D3D12_CPU_PAGE_PROPERTY cpuPageProperty, 
        D3D12_MEMORY_POOL memoryPoolPreference,
        UINT creationNodeMask = 1, 
        UINT nodeMask = 1 )
    {
        Type = D3D12_HEAP_TYPE_CUSTOM;
        CPUPageProperty = cpuPageProperty;
        MemoryPoolPreference = memoryPoolPreference;
        CreationNodeMask = creationNodeMask;
        VisibleNodeMask = nodeMask;
    }
    explicit CD3DX12_HEAP_PROPERTIES( 
        D3D12_HEAP_TYPE type, 
        UINT creationNodeMask = 1, 
        UINT nodeMask = 1 )
    {
        Type = type;
        CPUPageProperty = D3D12_CPU_PAGE_PROPERTY_UNKNOWN;
        MemoryPoolPreference = D3D12_MEMORY_POOL_UNKNOWN;
        CreationNodeMask = creationNodeMask;
        VisibleNodeMask = nodeMask;
    }
    operator const D3D12_HEAP_PROPERTIES&() const { return *this; }
    bool IsCPUAccessible() const
    {
        return Type == D3D12_HEAP_TYPE_UPLOAD || Type == D3D12_HEAP_TYPE_READBACK || (Type == D3D12_HEAP_TYPE_CUSTOM &&
            (CPUPageProperty == D3D12_CPU_PAGE_PROPERTY_WRITE_COMBINE || CPUPageProperty == D3D12_CPU_PAGE_PROPERTY_WRITE_BACK));
    }
};
inline bool operator==( const D3D12_HEAP_PROPERTIES& l, const D3D12_HEAP_PROPERTIES& r )
{
    return l.Type == r.Type && l.CPUPageProperty == r.CPUPageProperty && 
        l.MemoryPoolPreference == r.MemoryPoolPreference &&
        l.CreationNodeMask == r.CreationNodeMask &&
        l.VisibleNodeMask == r.VisibleNodeMask;
}
inline bool operator!=( const D3D12_HEAP_PROPERTIES& l, const D3D12_HEAP_PROPERTIES& r )
{ return !( l == r ); }

//------------------------------------------------------------------------------------------------
struct CD3DX12_HEAP_DESC : public D3D12_HEAP_DESC
{
    CD3DX12_HEAP_DESC()
    {}
    explicit CD3DX12_HEAP_DESC(const D3D12_HEAP_DESC &o) :
        D3D12_HEAP_DESC(o)
    {}
    CD3DX12_HEAP_DESC( 
        UINT64 size, 
        D3D12_HEAP_PROPERTIES properties, 
        UINT64 alignment = 0, 
        D3D12_HEAP_FLAGS flags = D3D12_HEAP_FLAG_NONE )
    {
        SizeInBytes = size;
        Properties = properties;
        Alignment = alignment;
        Flags = flags;
    }
    CD3DX12_HEAP_DESC( 
        UINT64 size, 
        D3D12_HEAP_TYPE type, 
        UINT64 alignment = 0, 
        D3D12_HEAP_FLAGS flags = D3D12_HEAP_FLAG_NONE )
    {
        SizeInBytes = size;
        Properties = CD3DX12_HEAP_PROPERTIES( type );
        Alignment = alignment;
        Flags = flags;
    }
    CD3DX12_HEAP_DESC( 
        UINT64 size, 
        D3D12_CPU_PAGE_PROPERTY cpuPageProperty, 
        D3D12_MEMORY_POOL memoryPoolPreference, 
        UINT64 alignment = 0, 
        D3D12_HEAP_FLAGS flags = D3D12_HEAP_FLAG_NONE )
    {
        SizeInBytes = size;
        Properties = CD3DX12_HEAP_PROPERTIES( cpuPageProperty, memoryPoolPreference );
        Alignment = alignment;
        Flags = flags;
    }
    CD3DX12_HEAP_DESC( 
        const D3D12_RESOURCE_ALLOCATION_INFO& resAllocInfo,
        D3D12_HEAP_PROPERTIES properties, 
        D3D12_HEAP_FLAGS flags = D3D12_HEAP_FLAG_NONE )
    {
        SizeInBytes = resAllocInfo.SizeInBytes;
        Properties = properties;
        Alignment = resAllocInfo.Alignment;
        Flags = flags;
    }
    CD3DX12_HEAP_DESC( 
        const D3D12_RESOURCE_ALLOCATION_INFO& resAllocInfo,
        D3D12_HEAP_TYPE type, 
        D3D12_HEAP_FLAGS flags = D3D12_HEAP_FLAG_NONE )
    {
        SizeInBytes = resAllocInfo.SizeInBytes;
        Properties = CD3DX12_HEAP_PROPERTIES( type );
        Alignment = resAllocInfo.Alignment;
        Flags = flags;
    }
    CD3DX12_HEAP_DESC( 
        const D3D12_RESOURCE_ALLOCATION_INFO& resAllocInfo,
        D3D12_CPU_PAGE_PROPERTY cpuPageProperty, 
        D3D12_MEMORY_POOL memoryPoolPreference, 
        D3D12_HEAP_FLAGS flags = D3D12_HEAP_FLAG_NONE )
    {
        SizeInBytes = resAllocInfo.SizeInBytes;
        Properties = CD3DX12_HEAP_PROPERTIES( cpuPageProperty, memoryPoolPreference );
        Alignment = resAllocInfo.Alignment;
        Flags = flags;
    }
    operator const D3D12_HEAP_DESC&() const { return *this; }
    bool IsCPUAccessible() const
    { return static_cast< const CD3DX12_HEAP_PROPERTIES* >( &Properties )->IsCPUAccessible(); }
};
inline bool operator==( const D3D12_HEAP_DESC& l, const D3D12_HEAP_DESC& r )
{
    return l.SizeInBytes == r.SizeInBytes &&
        l.Properties == r.Properties && 
        l.Alignment == r.Alignment &&
        l.Flags == r.Flags;
}
inline bool operator!=( const D3D12_HEAP_DESC& l, const D3D12_HEAP_DESC& r )
{ return !( l == r ); }

//------------------------------------------------------------------------------------------------
struct CD3DX12_CLEAR_VALUE : public D3D12_CLEAR_VALUE
{
    CD3DX12_CLEAR_VALUE()
    {}
    explicit CD3DX12_CLEAR_VALUE(const D3D12_CLEAR_VALUE &o) :
        D3D12_CLEAR_VALUE(o)
    {}
    CD3DX12_CLEAR_VALUE( 
        DXGI_FORMAT format, 
        const FLOAT color[4] )
    {
        Format = format;
        memcpy( Color, color, sizeof( Color ) );
    }
    CD3DX12_CLEAR_VALUE( 
        DXGI_FORMAT format, 
        FLOAT depth,
        UINT8 stencil )
    {
        Format = format;
        /* Use memcpy to preserve NAN values */
        memcpy( &DepthStencil.Depth, &depth, sizeof( depth ) );
        DepthStencil.Stencil = stencil;
    }
    operator const D3D12_CLEAR_VALUE&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_RANGE : public D3D12_RANGE
{
    CD3DX12_RANGE()
    {}
    explicit CD3DX12_RANGE(const D3D12_RANGE &o) :
        D3D12_RANGE(o)
    {}
    CD3DX12_RANGE( 
        SIZE_T begin, 
        SIZE_T end )
    {
        Begin = begin;
        End = end;
    }
    operator const D3D12_RANGE&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_SHADER_BYTECODE : public D3D12_SHADER_BYTECODE
{
    CD3DX12_SHADER_BYTECODE()
    {}
    explicit CD3DX12_SHADER_BYTECODE(const D3D12_SHADER_BYTECODE &o) :
        D3D12_SHADER_BYTECODE(o)
    {}
    CD3DX12_SHADER_BYTECODE(
        ID3DBlob* pShaderBlob )
    {
        pShaderBytecode = pShaderBlob->GetBufferPointer();
        BytecodeLength = pShaderBlob->GetBufferSize();
    }
    CD3DX12_SHADER_BYTECODE(
        void* _pShaderBytecode,
        SIZE_T bytecodeLength )
    {
        pShaderBytecode = _pShaderBytecode;
        BytecodeLength = bytecodeLength;
    }
    operator const D3D12_SHADER_BYTECODE&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_TILED_RESOURCE_COORDINATE : public D3D12_TILED_RESOURCE_COORDINATE
{
    CD3DX12_TILED_RESOURCE_COORDINATE()
    {}
    explicit CD3DX12_TILED_RESOURCE_COORDINATE(const D3D12_TILED_RESOURCE_COORDINATE &o) :
        D3D12_TILED_RESOURCE_COORDINATE(o)
    {}
    CD3DX12_TILED_RESOURCE_COORDINATE( 
        UINT x, 
        UINT y, 
        UINT z, 
        UINT subresource ) 
    {
        X = x;
        Y = y;
        Z = z;
        Subresource = subresource;
    }
    operator const D3D12_TILED_RESOURCE_COORDINATE&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_TILE_REGION_SIZE : public D3D12_TILE_REGION_SIZE
{
    CD3DX12_TILE_REGION_SIZE()
    {}
    explicit CD3DX12_TILE_REGION_SIZE(const D3D12_TILE_REGION_SIZE &o) :
        D3D12_TILE_REGION_SIZE(o)
    {}
    CD3DX12_TILE_REGION_SIZE( 
        UINT numTiles, 
        BOOL useBox, 
        UINT width, 
        UINT16 height, 
        UINT16 depth ) 
    {
        NumTiles = numTiles;
        UseBox = useBox;
        Width = width;
        Height = height;
        Depth = depth;
    }
    operator const D3D12_TILE_REGION_SIZE&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_SUBRESOURCE_TILING : public D3D12_SUBRESOURCE_TILING
{
    CD3DX12_SUBRESOURCE_TILING()
    {}
    explicit CD3DX12_SUBRESOURCE_TILING(const D3D12_SUBRESOURCE_TILING &o) :
        D3D12_SUBRESOURCE_TILING(o)
    {}
    CD3DX12_SUBRESOURCE_TILING( 
        UINT widthInTiles, 
        UINT16 heightInTiles, 
        UINT16 depthInTiles, 
        UINT startTileIndexInOverallResource ) 
    {
        WidthInTiles = widthInTiles;
        HeightInTiles = heightInTiles;
        DepthInTiles = depthInTiles;
        StartTileIndexInOverallResource = startTileIndexInOverallResource;
    }
    operator const D3D12_SUBRESOURCE_TILING&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_TILE_SHAPE : public D3D12_TILE_SHAPE
{
    CD3DX12_TILE_SHAPE()
    {}
    explicit CD3DX12_TILE_SHAPE(const D3D12_TILE_SHAPE &o) :
        D3D12_TILE_SHAPE(o)
    {}
    CD3DX12_TILE_SHAPE( 
        UINT widthInTexels, 
        UINT heightInTexels, 
        UINT depthInTexels ) 
    {
        WidthInTexels = widthInTexels;
        HeightInTexels = heightInTexels;
        DepthInTexels = depthInTexels;
    }
    operator const D3D12_TILE_SHAPE&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_RESOURCE_BARRIER : public D3D12_RESOURCE_BARRIER
{
    CD3DX12_RESOURCE_BARRIER()
    {}
    explicit CD3DX12_RESOURCE_BARRIER(const D3D12_RESOURCE_BARRIER &o) :
        D3D12_RESOURCE_BARRIER(o)
    {}
    static inline CD3DX12_RESOURCE_BARRIER Transition(
        _In_ ID3D12Resource* pResource,
        D3D12_RESOURCE_STATES stateBefore,
        D3D12_RESOURCE_STATES stateAfter,
        UINT subresource = D3D12_RESOURCE_BARRIER_ALL_SUBRESOURCES,
        D3D12_RESOURCE_BARRIER_FLAGS flags = D3D12_RESOURCE_BARRIER_FLAG_NONE)
    {
        CD3DX12_RESOURCE_BARRIER result;
        ZeroMemory(&result, sizeof(result));
        D3D12_RESOURCE_BARRIER &barrier = result;
        result.Type = D3D12_RESOURCE_BARRIER_TYPE_TRANSITION;
        result.Flags = flags;
        barrier.Transition.pResource = pResource;
        barrier.Transition.StateBefore = stateBefore;
        barrier.Transition.StateAfter = stateAfter;
        barrier.Transition.Subresource = subresource;
        return result;
    }
    static inline CD3DX12_RESOURCE_BARRIER Aliasing(
        _In_ ID3D12Resource* pResourceBefore,
        _In_ ID3D12Resource* pResourceAfter)
    {
        CD3DX12_RESOURCE_BARRIER result;
        ZeroMemory(&result, sizeof(result));
        D3D12_RESOURCE_BARRIER &barrier = result;
        result.Type = D3D12_RESOURCE_BARRIER_TYPE_ALIASING;
        barrier.Aliasing.pResourceBefore = pResourceBefore;
        barrier.Aliasing.pResourceAfter = pResourceAfter;
        return result;
    }
    static inline CD3DX12_RESOURCE_BARRIER UAV(
        _In_ ID3D12Resource* pResource)
    {
        CD3DX12_RESOURCE_BARRIER result;
        ZeroMemory(&result, sizeof(result));
        D3D12_RESOURCE_BARRIER &barrier = result;
        result.Type = D3D12_RESOURCE_BARRIER_TYPE_UAV;
        barrier.UAV.pResource = pResource;
        return result;
    }
    operator const D3D12_RESOURCE_BARRIER&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_PACKED_MIP_INFO : public D3D12_PACKED_MIP_INFO
{
    CD3DX12_PACKED_MIP_INFO()
    {}
    explicit CD3DX12_PACKED_MIP_INFO(const D3D12_PACKED_MIP_INFO &o) :
        D3D12_PACKED_MIP_INFO(o)
    {}
    CD3DX12_PACKED_MIP_INFO( 
        UINT8 numStandardMips, 
        UINT8 numPackedMips, 
        UINT numTilesForPackedMips, 
        UINT startTileIndexInOverallResource ) 
    {
        NumStandardMips = numStandardMips;
        NumPackedMips = numPackedMips;
        NumTilesForPackedMips = numTilesForPackedMips;
        StartTileIndexInOverallResource = startTileIndexInOverallResource;
    }
    operator const D3D12_PACKED_MIP_INFO&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_SUBRESOURCE_FOOTPRINT : public D3D12_SUBRESOURCE_FOOTPRINT
{
    CD3DX12_SUBRESOURCE_FOOTPRINT()
    {}
    explicit CD3DX12_SUBRESOURCE_FOOTPRINT(const D3D12_SUBRESOURCE_FOOTPRINT &o) :
        D3D12_SUBRESOURCE_FOOTPRINT(o)
    {}
    CD3DX12_SUBRESOURCE_FOOTPRINT( 
        DXGI_FORMAT format, 
        UINT width, 
        UINT height, 
        UINT depth, 
        UINT rowPitch ) 
    {
        Format = format;
        Width = width;
        Height = height;
        Depth = depth;
        RowPitch = rowPitch;
    }
    explicit CD3DX12_SUBRESOURCE_FOOTPRINT( 
        const D3D12_RESOURCE_DESC& resDesc, 
        UINT rowPitch ) 
    {
        Format = resDesc.Format;
        Width = UINT( resDesc.Width );
        Height = resDesc.Height;
        Depth = (resDesc.Dimension == D3D12_RESOURCE_DIMENSION_TEXTURE3D ? resDesc.DepthOrArraySize : 1);
        RowPitch = rowPitch;
    }
    operator const D3D12_SUBRESOURCE_FOOTPRINT&() const { return *this; }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_TEXTURE_COPY_LOCATION : public D3D12_TEXTURE_COPY_LOCATION
{ 
    CD3DX12_TEXTURE_COPY_LOCATION()
    {}
    explicit CD3DX12_TEXTURE_COPY_LOCATION(const D3D12_TEXTURE_COPY_LOCATION &o) :
        D3D12_TEXTURE_COPY_LOCATION(o)
    {}
    CD3DX12_TEXTURE_COPY_LOCATION(ID3D12Resource* pRes) { pResource = pRes; }
    CD3DX12_TEXTURE_COPY_LOCATION(ID3D12Resource* pRes, D3D12_PLACED_SUBRESOURCE_FOOTPRINT const& Footprint)
    {
        pResource = pRes;
        Type = D3D12_TEXTURE_COPY_TYPE_PLACED_FOOTPRINT;
        PlacedFootprint = Footprint;
    }
    CD3DX12_TEXTURE_COPY_LOCATION(ID3D12Resource* pRes, UINT Sub)
    {
        pResource = pRes;
        Type = D3D12_TEXTURE_COPY_TYPE_SUBRESOURCE_INDEX;
        SubresourceIndex = Sub;
    }
}; 

//------------------------------------------------------------------------------------------------
struct CD3DX12_DESCRIPTOR_RANGE : public D3D12_DESCRIPTOR_RANGE
{
    CD3DX12_DESCRIPTOR_RANGE() { }
    explicit CD3DX12_DESCRIPTOR_RANGE(const D3D12_DESCRIPTOR_RANGE &o) :
        D3D12_DESCRIPTOR_RANGE(o)
    {}
    CD3DX12_DESCRIPTOR_RANGE(
        D3D12_DESCRIPTOR_RANGE_TYPE rangeType,
        UINT numDescriptors,
        UINT baseShaderRegister,
        UINT registerSpace = 0,
        UINT offsetInDescriptorsFromTableStart =
        D3D12_DESCRIPTOR_RANGE_OFFSET_APPEND)
    {
        Init(rangeType, numDescriptors, baseShaderRegister, registerSpace, offsetInDescriptorsFromTableStart);
    }
    
    inline void Init(
        D3D12_DESCRIPTOR_RANGE_TYPE rangeType,
        UINT numDescriptors,
        UINT baseShaderRegister,
        UINT registerSpace = 0,
        UINT offsetInDescriptorsFromTableStart =
        D3D12_DESCRIPTOR_RANGE_OFFSET_APPEND)
    {
        Init(*this, rangeType, numDescriptors, baseShaderRegister, registerSpace, offsetInDescriptorsFromTableStart);
    }
    
    static inline void Init(
        _Out_ D3D12_DESCRIPTOR_RANGE &range,
        D3D12_DESCRIPTOR_RANGE_TYPE rangeType,
        UINT numDescriptors,
        UINT baseShaderRegister,
        UINT registerSpace = 0,
        UINT offsetInDescriptorsFromTableStart =
        D3D12_DESCRIPTOR_RANGE_OFFSET_APPEND)
    {
        range.RangeType = rangeType;
        range.NumDescriptors = numDescriptors;
        range.BaseShaderRegister = baseShaderRegister;
        range.RegisterSpace = registerSpace;
        range.OffsetInDescriptorsFromTableStart = offsetInDescriptorsFromTableStart;
    }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_ROOT_DESCRIPTOR_TABLE : public D3D12_ROOT_DESCRIPTOR_TABLE
{
    CD3DX12_ROOT_DESCRIPTOR_TABLE() {}
    explicit CD3DX12_ROOT_DESCRIPTOR_TABLE(const D3D12_ROOT_DESCRIPTOR_TABLE &o) :
        D3D12_ROOT_DESCRIPTOR_TABLE(o)
    {}
    CD3DX12_ROOT_DESCRIPTOR_TABLE(
        UINT numDescriptorRanges,
        _In_reads_opt_(numDescriptorRanges) const D3D12_DESCRIPTOR_RANGE* _pDescriptorRanges)
    {
        Init(numDescriptorRanges, _pDescriptorRanges);
    }
    
    inline void Init(
        UINT numDescriptorRanges,
        _In_reads_opt_(numDescriptorRanges) const D3D12_DESCRIPTOR_RANGE* _pDescriptorRanges)
    {
        Init(*this, numDescriptorRanges, _pDescriptorRanges);
    }
    
    static inline void Init(
        _Out_ D3D12_ROOT_DESCRIPTOR_TABLE &rootDescriptorTable,
        UINT numDescriptorRanges,
        _In_reads_opt_(numDescriptorRanges) const D3D12_DESCRIPTOR_RANGE* _pDescriptorRanges)
    {
        rootDescriptorTable.NumDescriptorRanges = numDescriptorRanges;
        rootDescriptorTable.pDescriptorRanges = _pDescriptorRanges;
    }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_ROOT_CONSTANTS : public D3D12_ROOT_CONSTANTS
{
    CD3DX12_ROOT_CONSTANTS() {}
    explicit CD3DX12_ROOT_CONSTANTS(const D3D12_ROOT_CONSTANTS &o) :
        D3D12_ROOT_CONSTANTS(o)
    {}
    CD3DX12_ROOT_CONSTANTS(
        UINT num32BitValues,
        UINT shaderRegister,
        UINT registerSpace = 0)
    {
        Init(num32BitValues, shaderRegister, registerSpace);
    }
    
    inline void Init(
        UINT num32BitValues,
        UINT shaderRegister,
        UINT registerSpace = 0)
    {
        Init(*this, num32BitValues, shaderRegister, registerSpace);
    }
    
    static inline void Init(
        _Out_ D3D12_ROOT_CONSTANTS &rootConstants,
        UINT num32BitValues,
        UINT shaderRegister,
        UINT registerSpace = 0)
    {
        rootConstants.Num32BitValues = num32BitValues;
        rootConstants.ShaderRegister = shaderRegister;
        rootConstants.RegisterSpace = registerSpace;
    }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_ROOT_DESCRIPTOR : public D3D12_ROOT_DESCRIPTOR
{
    CD3DX12_ROOT_DESCRIPTOR() {}
    explicit CD3DX12_ROOT_DESCRIPTOR(const D3D12_ROOT_DESCRIPTOR &o) :
        D3D12_ROOT_DESCRIPTOR(o)
    {}
    CD3DX12_ROOT_DESCRIPTOR(
        UINT shaderRegister,
        UINT registerSpace = 0)
    {
        Init(shaderRegister, registerSpace);
    }
    
    inline void Init(
        UINT shaderRegister,
        UINT registerSpace = 0)
    {
        Init(*this, shaderRegister, registerSpace);
    }
    
    static inline void Init(_Out_ D3D12_ROOT_DESCRIPTOR &table, UINT shaderRegister, UINT registerSpace = 0)
    {
        table.ShaderRegister = shaderRegister;
        table.RegisterSpace = registerSpace;
    }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_ROOT_PARAMETER : public D3D12_ROOT_PARAMETER
{
    CD3DX12_ROOT_PARAMETER() {}
    explicit CD3DX12_ROOT_PARAMETER(const D3D12_ROOT_PARAMETER &o) :
        D3D12_ROOT_PARAMETER(o)
    {}
    
    static inline void InitAsDescriptorTable(
        _Out_ D3D12_ROOT_PARAMETER &rootParam,
        UINT numDescriptorRanges,
        _In_reads_(numDescriptorRanges) const D3D12_DESCRIPTOR_RANGE* pDescriptorRanges,
        D3D12_SHADER_VISIBILITY visibility = D3D12_SHADER_VISIBILITY_ALL)
    {
        rootParam.ParameterType = D3D12_ROOT_PARAMETER_TYPE_DESCRIPTOR_TABLE;
        rootParam.ShaderVisibility = visibility;
        CD3DX12_ROOT_DESCRIPTOR_TABLE::Init(rootParam.DescriptorTable, numDescriptorRanges, pDescriptorRanges);
    }

    static inline void InitAsConstants(
        _Out_ D3D12_ROOT_PARAMETER &rootParam,
        UINT num32BitValues,
        UINT shaderRegister,
        UINT registerSpace = 0,
        D3D12_SHADER_VISIBILITY visibility = D3D12_SHADER_VISIBILITY_ALL)
    {
        rootParam.ParameterType = D3D12_ROOT_PARAMETER_TYPE_32BIT_CONSTANTS;
        rootParam.ShaderVisibility = visibility;
        CD3DX12_ROOT_CONSTANTS::Init(rootParam.Constants, num32BitValues, shaderRegister, registerSpace);
    }

    static inline void InitAsConstantBufferView(
        _Out_ D3D12_ROOT_PARAMETER &rootParam,
        UINT shaderRegister,
        UINT registerSpace = 0,
        D3D12_SHADER_VISIBILITY visibility = D3D12_SHADER_VISIBILITY_ALL)
    {
        rootParam.ParameterType = D3D12_ROOT_PARAMETER_TYPE_CBV;
        rootParam.ShaderVisibility = visibility;
        CD3DX12_ROOT_DESCRIPTOR::Init(rootParam.Descriptor, shaderRegister, registerSpace);
    }

    static inline void InitAsShaderResourceView(
        _Out_ D3D12_ROOT_PARAMETER &rootParam,
        UINT shaderRegister,
        UINT registerSpace = 0,
        D3D12_SHADER_VISIBILITY visibility = D3D12_SHADER_VISIBILITY_ALL)
    {
        rootParam.ParameterType = D3D12_ROOT_PARAMETER_TYPE_SRV;
        rootParam.ShaderVisibility = visibility;
        CD3DX12_ROOT_DESCRIPTOR::Init(rootParam.Descriptor, shaderRegister, registerSpace);
    }

    static inline void InitAsUnorderedAccessView(
        _Out_ D3D12_ROOT_PARAMETER &rootParam,
        UINT shaderRegister,
        UINT registerSpace = 0,
        D3D12_SHADER_VISIBILITY visibility = D3D12_SHADER_VISIBILITY_ALL)
    {
        rootParam.ParameterType = D3D12_ROOT_PARAMETER_TYPE_UAV;
        rootParam.ShaderVisibility = visibility;
        CD3DX12_ROOT_DESCRIPTOR::Init(rootParam.Descriptor, shaderRegister, registerSpace);
    }
    
    inline void InitAsDescriptorTable(
        UINT numDescriptorRanges,
        _In_reads_(numDescriptorRanges) const D3D12_DESCRIPTOR_RANGE* pDescriptorRanges,
        D3D12_SHADER_VISIBILITY visibility = D3D12_SHADER_VISIBILITY_ALL)
    {
        InitAsDescriptorTable(*this, numDescriptorRanges, pDescriptorRanges, visibility);
    }
    
    inline void InitAsConstants(
        UINT num32BitValues,
        UINT shaderRegister,
        UINT registerSpace = 0,
        D3D12_SHADER_VISIBILITY visibility = D3D12_SHADER_VISIBILITY_ALL)
    {
        InitAsConstants(*this, num32BitValues, shaderRegister, registerSpace, visibility);
    }

    inline void InitAsConstantBufferView(
        UINT shaderRegister,
        UINT registerSpace = 0,
        D3D12_SHADER_VISIBILITY visibility = D3D12_SHADER_VISIBILITY_ALL)
    {
        InitAsConstantBufferView(*this, shaderRegister, registerSpace, visibility);
    }

    inline void InitAsShaderResourceView(
        UINT shaderRegister,
        UINT registerSpace = 0,
        D3D12_SHADER_VISIBILITY visibility = D3D12_SHADER_VISIBILITY_ALL)
    {
        InitAsShaderResourceView(*this, shaderRegister, registerSpace, visibility);
    }

    inline void InitAsUnorderedAccessView(
        UINT shaderRegister,
        UINT registerSpace = 0,
        D3D12_SHADER_VISIBILITY visibility = D3D12_SHADER_VISIBILITY_ALL)
    {
        InitAsUnorderedAccessView(*this, shaderRegister, registerSpace, visibility);
    }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_STATIC_SAMPLER_DESC : public D3D12_STATIC_SAMPLER_DESC
{
    CD3DX12_STATIC_SAMPLER_DESC() {}
    explicit CD3DX12_STATIC_SAMPLER_DESC(const D3D12_STATIC_SAMPLER_DESC &o) :
        D3D12_STATIC_SAMPLER_DESC(o)
    {}
    CD3DX12_STATIC_SAMPLER_DESC(
         UINT shaderRegister,
         D3D12_FILTER filter = D3D12_FILTER_ANISOTROPIC,
         D3D12_TEXTURE_ADDRESS_MODE addressU = D3D12_TEXTURE_ADDRESS_MODE_WRAP,
         D3D12_TEXTURE_ADDRESS_MODE addressV = D3D12_TEXTURE_ADDRESS_MODE_WRAP,
         D3D12_TEXTURE_ADDRESS_MODE addressW = D3D12_TEXTURE_ADDRESS_MODE_WRAP,
         FLOAT mipLODBias = 0,
         UINT maxAnisotropy = 16,
         D3D12_COMPARISON_FUNC comparisonFunc = D3D12_COMPARISON_FUNC_LESS_EQUAL,
         D3D12_STATIC_BORDER_COLOR borderColor = D3D12_STATIC_BORDER_COLOR_OPAQUE_WHITE,
         FLOAT minLOD = 0.f,
         FLOAT maxLOD = D3D12_FLOAT32_MAX,
         D3D12_SHADER_VISIBILITY shaderVisibility = D3D12_SHADER_VISIBILITY_ALL, 
         UINT registerSpace = 0)
    {
        Init(
            shaderRegister,
            filter,
            addressU,
            addressV,
            addressW,
            mipLODBias,
            maxAnisotropy,
            comparisonFunc,
            borderColor,
            minLOD,
            maxLOD,
            shaderVisibility,
            registerSpace);
    }
    
    static inline void Init(
        _Out_ D3D12_STATIC_SAMPLER_DESC &samplerDesc,
         UINT shaderRegister,
         D3D12_FILTER filter = D3D12_FILTER_ANISOTROPIC,
         D3D12_TEXTURE_ADDRESS_MODE addressU = D3D12_TEXTURE_ADDRESS_MODE_WRAP,
         D3D12_TEXTURE_ADDRESS_MODE addressV = D3D12_TEXTURE_ADDRESS_MODE_WRAP,
         D3D12_TEXTURE_ADDRESS_MODE addressW = D3D12_TEXTURE_ADDRESS_MODE_WRAP,
         FLOAT mipLODBias = 0,
         UINT maxAnisotropy = 16,
         D3D12_COMPARISON_FUNC comparisonFunc = D3D12_COMPARISON_FUNC_LESS_EQUAL,
         D3D12_STATIC_BORDER_COLOR borderColor = D3D12_STATIC_BORDER_COLOR_OPAQUE_WHITE,
         FLOAT minLOD = 0.f,
         FLOAT maxLOD = D3D12_FLOAT32_MAX,
         D3D12_SHADER_VISIBILITY shaderVisibility = D3D12_SHADER_VISIBILITY_ALL, 
         UINT registerSpace = 0)
    {
        samplerDesc.ShaderRegister = shaderRegister;
        samplerDesc.Filter = filter;
        samplerDesc.AddressU = addressU;
        samplerDesc.AddressV = addressV;
        samplerDesc.AddressW = addressW;
        samplerDesc.MipLODBias = mipLODBias;
        samplerDesc.MaxAnisotropy = maxAnisotropy;
        samplerDesc.ComparisonFunc = comparisonFunc;
        samplerDesc.BorderColor = borderColor;
        samplerDesc.MinLOD = minLOD;
        samplerDesc.MaxLOD = maxLOD;
        samplerDesc.ShaderVisibility = shaderVisibility;
        samplerDesc.RegisterSpace = registerSpace;
    }
    inline void Init(
         UINT shaderRegister,
         D3D12_FILTER filter = D3D12_FILTER_ANISOTROPIC,
         D3D12_TEXTURE_ADDRESS_MODE addressU = D3D12_TEXTURE_ADDRESS_MODE_WRAP,
         D3D12_TEXTURE_ADDRESS_MODE addressV = D3D12_TEXTURE_ADDRESS_MODE_WRAP,
         D3D12_TEXTURE_ADDRESS_MODE addressW = D3D12_TEXTURE_ADDRESS_MODE_WRAP,
         FLOAT mipLODBias = 0,
         UINT maxAnisotropy = 16,
         D3D12_COMPARISON_FUNC comparisonFunc = D3D12_COMPARISON_FUNC_LESS_EQUAL,
         D3D12_STATIC_BORDER_COLOR borderColor = D3D12_STATIC_BORDER_COLOR_OPAQUE_WHITE,
         FLOAT minLOD = 0.f,
         FLOAT maxLOD = D3D12_FLOAT32_MAX,
         D3D12_SHADER_VISIBILITY shaderVisibility = D3D12_SHADER_VISIBILITY_ALL, 
         UINT registerSpace = 0)
    {
        Init(
            *this,
            shaderRegister,
            filter,
            addressU,
            addressV,
            addressW,
            mipLODBias,
            maxAnisotropy,
            comparisonFunc,
            borderColor,
            minLOD,
            maxLOD,
            shaderVisibility,
            registerSpace);
    }
    
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_ROOT_SIGNATURE_DESC : public D3D12_ROOT_SIGNATURE_DESC
{
    CD3DX12_ROOT_SIGNATURE_DESC() {}
    explicit CD3DX12_ROOT_SIGNATURE_DESC(const D3D12_ROOT_SIGNATURE_DESC &o) :
        D3D12_ROOT_SIGNATURE_DESC(o)
    {}
    CD3DX12_ROOT_SIGNATURE_DESC(
        UINT numParameters,
        _In_reads_opt_(numParameters) const D3D12_ROOT_PARAMETER* _pParameters,
        UINT numStaticSamplers = 0,
        _In_reads_opt_(numStaticSamplers) const D3D12_STATIC_SAMPLER_DESC* _pStaticSamplers = NULL,
        D3D12_ROOT_SIGNATURE_FLAGS flags = D3D12_ROOT_SIGNATURE_FLAG_NONE)
    {
        Init(numParameters, _pParameters, numStaticSamplers, _pStaticSamplers, flags);
    }
    CD3DX12_ROOT_SIGNATURE_DESC(CD3DX12_DEFAULT)
    {
        Init(0, NULL, 0, NULL, D3D12_ROOT_SIGNATURE_FLAG_NONE);
    }
    
    inline void Init(
        UINT numParameters,
        _In_reads_opt_(numParameters) const D3D12_ROOT_PARAMETER* _pParameters,
        UINT numStaticSamplers = 0,
        _In_reads_opt_(numStaticSamplers) const D3D12_STATIC_SAMPLER_DESC* _pStaticSamplers = NULL,
        D3D12_ROOT_SIGNATURE_FLAGS flags = D3D12_ROOT_SIGNATURE_FLAG_NONE)
    {
        Init(*this, numParameters, _pParameters, numStaticSamplers, _pStaticSamplers, flags);
    }

    static inline void Init(
        _Out_ D3D12_ROOT_SIGNATURE_DESC &desc,
        UINT numParameters,
        _In_reads_opt_(numParameters) const D3D12_ROOT_PARAMETER* _pParameters,
        UINT numStaticSamplers = 0,
        _In_reads_opt_(numStaticSamplers) const D3D12_STATIC_SAMPLER_DESC* _pStaticSamplers = NULL,
        D3D12_ROOT_SIGNATURE_FLAGS flags = D3D12_ROOT_SIGNATURE_FLAG_NONE)
    {
        desc.NumParameters = numParameters;
        desc.pParameters = _pParameters;
        desc.NumStaticSamplers = numStaticSamplers;
        desc.pStaticSamplers = _pStaticSamplers;
        desc.Flags = flags;
    }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_CPU_DESCRIPTOR_HANDLE : public D3D12_CPU_DESCRIPTOR_HANDLE
{
    CD3DX12_CPU_DESCRIPTOR_HANDLE() {}
    explicit CD3DX12_CPU_DESCRIPTOR_HANDLE(const D3D12_CPU_DESCRIPTOR_HANDLE &o) :
        D3D12_CPU_DESCRIPTOR_HANDLE(o)
    {}
    CD3DX12_CPU_DESCRIPTOR_HANDLE(CD3DX12_DEFAULT) { ptr = 0; }
    CD3DX12_CPU_DESCRIPTOR_HANDLE(_In_ const D3D12_CPU_DESCRIPTOR_HANDLE &other, INT offsetScaledByIncrementSize)
    {
        InitOffsetted(other, offsetScaledByIncrementSize);
    }
    CD3DX12_CPU_DESCRIPTOR_HANDLE(_In_ const D3D12_CPU_DESCRIPTOR_HANDLE &other, INT offsetInDescriptors, UINT descriptorIncrementSize)
    {
        InitOffsetted(other, offsetInDescriptors, descriptorIncrementSize);
    }
    CD3DX12_CPU_DESCRIPTOR_HANDLE& Offset(INT offsetInDescriptors, UINT descriptorIncrementSize)
    { 
        ptr += offsetInDescriptors * descriptorIncrementSize;
        return *this;
    }
    CD3DX12_CPU_DESCRIPTOR_HANDLE& Offset(INT offsetScaledByIncrementSize) 
    { 
        ptr += offsetScaledByIncrementSize;
        return *this;
    }
    bool operator==(_In_ const D3D12_CPU_DESCRIPTOR_HANDLE& other) const
    {
        return (ptr == other.ptr);
    }
    bool operator!=(_In_ const D3D12_CPU_DESCRIPTOR_HANDLE& other) const
    {
        return (ptr != other.ptr);
    }
    CD3DX12_CPU_DESCRIPTOR_HANDLE &operator=(const D3D12_CPU_DESCRIPTOR_HANDLE &other)
    {
        ptr = other.ptr;
        return *this;
    }
    
    inline void InitOffsetted(_In_ const D3D12_CPU_DESCRIPTOR_HANDLE &base, INT offsetScaledByIncrementSize)
    {
        InitOffsetted(*this, base, offsetScaledByIncrementSize);
    }
    
    inline void InitOffsetted(_In_ const D3D12_CPU_DESCRIPTOR_HANDLE &base, INT offsetInDescriptors, UINT descriptorIncrementSize)
    {
        InitOffsetted(*this, base, offsetInDescriptors, descriptorIncrementSize);
    }
    
    static inline void InitOffsetted(_Out_ D3D12_CPU_DESCRIPTOR_HANDLE &handle, _In_ const D3D12_CPU_DESCRIPTOR_HANDLE &base, INT offsetScaledByIncrementSize)
    {
        handle.ptr = base.ptr + offsetScaledByIncrementSize;
    }
    
    static inline void InitOffsetted(_Out_ D3D12_CPU_DESCRIPTOR_HANDLE &handle, _In_ const D3D12_CPU_DESCRIPTOR_HANDLE &base, INT offsetInDescriptors, UINT descriptorIncrementSize)
    {
        handle.ptr = base.ptr + offsetInDescriptors * descriptorIncrementSize;
    }
};

//------------------------------------------------------------------------------------------------
struct CD3DX12_GPU_DESCRIPTOR_HANDLE : public D3D12_GPU_DESCRIPTOR_HANDLE
{
    CD3DX12_GPU_DESCRIPTOR_HANDLE() {}
    explicit CD3DX12_GPU_DESCRIPTOR_HANDLE(const D3D12_GPU_DESCRIPTOR_HANDLE &o) :
        D3D12_GPU_DESCRIPTOR_HANDLE(o)
    {}
    CD3DX12_GPU_DESCRIPTOR_HANDLE(CD3DX12_DEFAULT) { ptr = 0; }
    CD3DX12_GPU_DESCRIPTOR_HANDLE(_In_ const D3D12_GPU_DESCRIPTOR_HANDLE &other, INT offsetScaledByIncrementSize)
    {
        InitOffsetted(other, offsetScaledByIncrementSize);
    }
    CD3DX12_GPU_DESCRIPTOR_HANDLE(_In_ const D3D12_GPU_DESCRIPTOR_HANDLE &other, INT offsetInDescriptors, UINT descriptorIncrementSize)
    {
        InitOffsetted(other, offsetInDescriptors, descriptorIncrementSize);
    }
    CD3DX12_GPU_DESCRIPTOR_HANDLE& Offset(INT offsetInDescriptors, UINT descriptorIncrementSize)
    { 
        ptr += offsetInDescriptors * descriptorIncrementSize;
        return *this;
    }
    CD3DX12_GPU_DESCRIPTOR_HANDLE& Offset(INT offsetScaledByIncrementSize) 
    { 
        ptr += offsetScaledByIncrementSize;
        return *this;
    }
    inline bool operator==(_In_ const D3D12_GPU_DESCRIPTOR_HANDLE& other) const
    {
        return (ptr == other.ptr);
    }
    inline bool operator!=(_In_ const D3D12_GPU_DESCRIPTOR_HANDLE& other) const
    {
        return (ptr != other.ptr);
    }
    CD3DX12_GPU_DESCRIPTOR_HANDLE &operator=(const D3D12_GPU_DESCRIPTOR_HANDLE &other)
    {
        ptr = other.ptr;
        return *this;
    }
    
    inline void InitOffsetted(_In_ const D3D12_GPU_DESCRIPTOR_HANDLE &base, INT offsetScaledByIncrementSize)
    {
        InitOffsetted(*this, base, offsetScaledByIncrementSize);
    }
    
    inline void InitOffsetted(_In_ const D3D12_GPU_DESCRIPTOR_HANDLE &base, INT offsetInDescriptors, UINT descriptorIncrementSize)
    {
        InitOffsetted(*this, base, offsetInDescriptors, descriptorIncrementSize);
    }
    
    static inline void InitOffsetted(_Out_ D3D12_GPU_DESCRIPTOR_HANDLE &handle, _In_ const D3D12_GPU_DESCRIPTOR_HANDLE &base, INT offsetScaledByIncrementSize)
    {
        handle.ptr = base.ptr + offsetScaledByIncrementSize;
    }
    
    static inline void InitOffsetted(_Out_ D3D12_GPU_DESCRIPTOR_HANDLE &handle, _In_ const D3D12_GPU_DESCRIPTOR_HANDLE &base, INT offsetInDescriptors, UINT descriptorIncrementSize)
    {
        handle.ptr = base.ptr + offsetInDescriptors * descriptorIncrementSize;
    }
};

//------------------------------------------------------------------------------------------------
inline UINT D3D12CalcSubresource( UINT MipSlice, UINT ArraySlice, UINT PlaneSlice, UINT MipLevels, UINT ArraySize )
{ 
    return MipSlice + ArraySlice * MipLevels + PlaneSlice * MipLevels * ArraySize; 
}

//------------------------------------------------------------------------------------------------
template <typename T, typename U, typename V>
inline void D3D12DecomposeSubresource( UINT Subresource, UINT MipLevels, UINT ArraySize, _Out_ T& MipSlice, _Out_ U& ArraySlice, _Out_ V& PlaneSlice )
{
    MipSlice = static_cast<T>(Subresource % MipLevels);
    ArraySlice = static_cast<U>((Subresource / MipLevels) % ArraySize);
    PlaneSlice = static_cast<V>(Subresource / (MipLevels * ArraySize));
}

//------------------------------------------------------------------------------------------------
inline UINT8 D3D12GetFormatPlaneCount(
    _In_ ID3D12Device* pDevice,
    DXGI_FORMAT Format
    )
{
    D3D12_FEATURE_DATA_FORMAT_INFO formatInfo = {Format};
    if (FAILED(pDevice->CheckFeatureSupport(D3D12_FEATURE_FORMAT_INFO, &formatInfo, sizeof(formatInfo))))
    {
        return 0;
    }
    return formatInfo.PlaneCount;
}

//------------------------------------------------------------------------------------------------
struct CD3DX12_RESOURCE_DESC : public D3D12_RESOURCE_DESC
{
    CD3DX12_RESOURCE_DESC()
    {}
    explicit CD3DX12_RESOURCE_DESC( const D3D12_RESOURCE_DESC& o ) :
        D3D12_RESOURCE_DESC( o )
    {}
    CD3DX12_RESOURCE_DESC( 
        D3D12_RESOURCE_DIMENSION dimension,
        UINT64 alignment,
        UINT64 width,
        UINT height,
        UINT16 depthOrArraySize,
        UINT16 mipLevels,
        DXGI_FORMAT format,
        UINT sampleCount,
        UINT sampleQuality,
        D3D12_TEXTURE_LAYOUT layout,
        D3D12_RESOURCE_FLAGS flags )
    {
        Dimension = dimension;
        Alignment = alignment;
        Width = width;
        Height = height;
        DepthOrArraySize = depthOrArraySize;
        MipLevels = mipLevels;
        Format = format;
        SampleDesc.Count = sampleCount;
        SampleDesc.Quality = sampleQuality;
        Layout = layout;
        Flags = flags;
    }
    static inline CD3DX12_RESOURCE_DESC Buffer( 
        const D3D12_RESOURCE_ALLOCATION_INFO& resAllocInfo,
        D3D12_RESOURCE_FLAGS flags = D3D12_RESOURCE_FLAG_NONE )
    {
        return CD3DX12_RESOURCE_DESC( D3D12_RESOURCE_DIMENSION_BUFFER, resAllocInfo.Alignment, resAllocInfo.SizeInBytes, 
            1, 1, 1, DXGI_FORMAT_UNKNOWN, 1, 0, D3D12_TEXTURE_LAYOUT_ROW_MAJOR, flags );
    }
    static inline CD3DX12_RESOURCE_DESC Buffer( 
        UINT64 width,
        D3D12_RESOURCE_FLAGS flags = D3D12_RESOURCE_FLAG_NONE,
        UINT64 alignment = 0 )
    {
        return CD3DX12_RESOURCE_DESC( D3D12_RESOURCE_DIMENSION_BUFFER, alignment, width, 1, 1, 1, 
            DXGI_FORMAT_UNKNOWN, 1, 0, D3D12_TEXTURE_LAYOUT_ROW_MAJOR, flags );
    }
    static inline CD3DX12_RESOURCE_DESC Tex1D( 
        DXGI_FORMAT format,
        UINT64 width,
        UINT16 arraySize = 1,
        UINT16 mipLevels = 0,
        D3D12_RESOURCE_FLAGS flags = D3D12_RESOURCE_FLAG_NONE,
        D3D12_TEXTURE_LAYOUT layout = D3D12_TEXTURE_LAYOUT_UNKNOWN,
        UINT64 alignment = 0 )
    {
        return CD3DX12_RESOURCE_DESC( D3D12_RESOURCE_DIMENSION_TEXTURE1D, alignment, width, 1, arraySize, 
            mipLevels, format, 1, 0, layout, flags );
    }
    static inline CD3DX12_RESOURCE_DESC Tex2D( 
        DXGI_FORMAT format,
        UINT64 width,
        UINT height,
        UINT16 arraySize = 1,
        UINT16 mipLevels = 0,
        UINT sampleCount = 1,
        UINT sampleQuality = 0,
        D3D12_RESOURCE_FLAGS flags = D3D12_RESOURCE_FLAG_NONE,
        D3D12_TEXTURE_LAYOUT layout = D3D12_TEXTURE_LAYOUT_UNKNOWN,
        UINT64 alignment = 0 )
    {
        return CD3DX12_RESOURCE_DESC( D3D12_RESOURCE_DIMENSION_TEXTURE2D, alignment, width, height, arraySize, 
            mipLevels, format, sampleCount, sampleQuality, layout, flags );
    }
    static inline CD3DX12_RESOURCE_DESC Tex3D( 
        DXGI_FORMAT format,
        UINT64 width,
        UINT height,
        UINT16 depth,
        UINT16 mipLevels = 0,
        D3D12_RESOURCE_FLAGS flags = D3D12_RESOURCE_FLAG_NONE,
        D3D12_TEXTURE_LAYOUT layout = D3D12_TEXTURE_LAYOUT_UNKNOWN,
        UINT64 alignment = 0 )
    {
        return CD3DX12_RESOURCE_DESC( D3D12_RESOURCE_DIMENSION_TEXTURE3D, alignment, width, height, depth, 
            mipLevels, format, 1, 0, layout, flags );
    }
    inline UINT16 Depth() const
    { return (Dimension == D3D12_RESOURCE_DIMENSION_TEXTURE3D ? DepthOrArraySize : 1); }
    inline UINT16 ArraySize() const
    { return (Dimension != D3D12_RESOURCE_DIMENSION_TEXTURE3D ? DepthOrArraySize : 1); }
    inline UINT8 PlaneCount(_In_ ID3D12Device* pDevice) const
    { return D3D12GetFormatPlaneCount(pDevice, Format); }
    inline UINT Subresources(_In_ ID3D12Device* pDevice) const
    { return MipLevels * ArraySize() * PlaneCount(pDevice); }
    inline UINT CalcSubresource(UINT MipSlice, UINT ArraySlice, UINT PlaneSlice)
    { return D3D12CalcSubresource(MipSlice, ArraySlice, PlaneSlice, MipLevels, ArraySize()); }
    operator const D3D12_RESOURCE_DESC&() const { return *this; }
};
inline bool operator==( const D3D12_RESOURCE_DESC& l, const D3D12_RESOURCE_DESC& r )
{
    return l.Dimension == r.Dimension &&
        l.Alignment == r.Alignment &&
        l.Width == r.Width &&
        l.Height == r.Height &&
        l.DepthOrArraySize == r.DepthOrArraySize &&
        l.MipLevels == r.MipLevels &&
        l.Format == r.Format &&
        l.SampleDesc.Count == r.SampleDesc.Count &&
        l.SampleDesc.Quality == r.SampleDesc.Quality &&
        l.Layout == r.Layout &&
        l.Flags == r.Flags;
}
inline bool operator!=( const D3D12_RESOURCE_DESC& l, const D3D12_RESOURCE_DESC& r )
{ return !( l == r ); }

//------------------------------------------------------------------------------------------------
// Row-by-row memcpy
inline void MemcpySubresource(
    _In_ const D3D12_MEMCPY_DEST* pDest,
    _In_ const D3D12_SUBRESOURCE_DATA* pSrc,
    SIZE_T RowSizeInBytes,
    UINT NumRows,
    UINT NumSlices)
{
    for (UINT z = 0; z < NumSlices; ++z)
    {
        BYTE* pDestSlice = reinterpret_cast<BYTE*>(pDest->pData) + pDest->SlicePitch * z;
        const BYTE* pSrcSlice = reinterpret_cast<const BYTE*>(pSrc->pData) + pSrc->SlicePitch * z;
        for (UINT y = 0; y < NumRows; ++y)
        {
            memcpy(pDestSlice + pDest->RowPitch * y,
                   pSrcSlice + pSrc->RowPitch * y,
                   RowSizeInBytes);
        }
    }
}

//------------------------------------------------------------------------------------------------
// Returns required size of a buffer to be used for data upload
inline UINT64 GetRequiredIntermediateSize(
    _In_ ID3D12Resource* pDestinationResource,
    _In_range_(0,D3D12_REQ_SUBRESOURCES) UINT FirstSubresource,
    _In_range_(0,D3D12_REQ_SUBRESOURCES-FirstSubresource) UINT NumSubresources)
{
    D3D12_RESOURCE_DESC Desc = pDestinationResource->GetDesc();
    UINT64 RequiredSize = 0;
    
    ID3D12Device* pDevice;
    pDestinationResource->GetDevice(__uuidof(*pDevice), reinterpret_cast<void**>(&pDevice));
    pDevice->GetCopyableFootprints(&Desc, FirstSubresource, NumSubresources, 0, nullptr, nullptr, nullptr, &RequiredSize);
    pDevice->Release();
    
    return RequiredSize;
}

//------------------------------------------------------------------------------------------------
// All arrays must be populated (e.g. by calling GetCopyableFootprints)
inline UINT64 UpdateSubresources(
    _In_ ID3D12GraphicsCommandList* pCmdList,
    _In_ ID3D12Resource* pDestinationResource,
    _In_ ID3D12Resource* pIntermediate,
    _In_range_(0,D3D12_REQ_SUBRESOURCES) UINT FirstSubresource,
    _In_range_(0,D3D12_REQ_SUBRESOURCES-FirstSubresource) UINT NumSubresources,
    UINT64 RequiredSize,
    _In_reads_(NumSubresources) const D3D12_PLACED_SUBRESOURCE_FOOTPRINT* pLayouts,
    _In_reads_(NumSubresources) const UINT* pNumRows,
    _In_reads_(NumSubresources) const UINT64* pRowSizesInBytes,
    _In_reads_(NumSubresources) const D3D12_SUBRESOURCE_DATA* pSrcData)
{
    // Minor validation
    D3D12_RESOURCE_DESC IntermediateDesc = pIntermediate->GetDesc();
    D3D12_RESOURCE_DESC DestinationDesc = pDestinationResource->GetDesc();
    if (IntermediateDesc.Dimension != D3D12_RESOURCE_DIMENSION_BUFFER || 
        IntermediateDesc.Width < RequiredSize + pLayouts[0].Offset || 
        RequiredSize > (SIZE_T)-1 || 
        (DestinationDesc.Dimension == D3D12_RESOURCE_DIMENSION_BUFFER && 
            (FirstSubresource != 0 || NumSubresources != 1)))
    {
        return 0;
    }
    
    BYTE* pData;
    HRESULT hr = pIntermediate->Map(0, NULL, reinterpret_cast<void**>(&pData));
    if (FAILED(hr))
    {
        return 0;
    }
    
    for (UINT i = 0; i < NumSubresources; ++i)
    {
        if (pRowSizesInBytes[i] > (SIZE_T)-1) return 0;
        D3D12_MEMCPY_DEST DestData = { pData + pLayouts[i].Offset, pLayouts[i].Footprint.RowPitch, pLayouts[i].Footprint.RowPitch * pNumRows[i] };
        MemcpySubresource(&DestData, &pSrcData[i], (SIZE_T)pRowSizesInBytes[i], pNumRows[i], pLayouts[i].Footprint.Depth);
    }
    pIntermediate->Unmap(0, NULL);
    
    if (DestinationDesc.Dimension == D3D12_RESOURCE_DIMENSION_BUFFER)
    {
        CD3DX12_BOX SrcBox( UINT( pLayouts[0].Offset ), UINT( pLayouts[0].Offset + pLayouts[0].Footprint.Width ) );
        pCmdList->CopyBufferRegion(
            pDestinationResource, 0, pIntermediate, pLayouts[0].Offset, pLayouts[0].Footprint.Width);
    }
    else
    {
        for (UINT i = 0; i < NumSubresources; ++i)
        {
            CD3DX12_TEXTURE_COPY_LOCATION Dst(pDestinationResource, i + FirstSubresource);
            CD3DX12_TEXTURE_COPY_LOCATION Src(pIntermediate, pLayouts[i]);
            pCmdList->CopyTextureRegion(&Dst, 0, 0, 0, &Src, nullptr);
        }
    }
    return RequiredSize;
}

//------------------------------------------------------------------------------------------------
// Heap-allocating UpdateSubresources implementation
inline UINT64 UpdateSubresources( 
    _In_ ID3D12GraphicsCommandList* pCmdList,
    _In_ ID3D12Resource* pDestinationResource,
    _In_ ID3D12Resource* pIntermediate,
    UINT64 IntermediateOffset,
    _In_range_(0,D3D12_REQ_SUBRESOURCES) UINT FirstSubresource,
    _In_range_(0,D3D12_REQ_SUBRESOURCES-FirstSubresource) UINT NumSubresources,
    _In_reads_(NumSubresources) D3D12_SUBRESOURCE_DATA* pSrcData)
{
    UINT64 RequiredSize = 0;
    UINT64 MemToAlloc = static_cast<UINT64>(sizeof(D3D12_PLACED_SUBRESOURCE_FOOTPRINT) + sizeof(UINT) + sizeof(UINT64)) * NumSubresources;
    if (MemToAlloc > SIZE_MAX)
    {
       return 0;
    }
    void* pMem = HeapAlloc(GetProcessHeap(), 0, static_cast<SIZE_T>(MemToAlloc));
    if (pMem == NULL)
    {
       return 0;
    }
    D3D12_PLACED_SUBRESOURCE_FOOTPRINT* pLayouts = reinterpret_cast<D3D12_PLACED_SUBRESOURCE_FOOTPRINT*>(pMem);
    UINT64* pRowSizesInBytes = reinterpret_cast<UINT64*>(pLayouts + NumSubresources);
    UINT* pNumRows = reinterpret_cast<UINT*>(pRowSizesInBytes + NumSubresources);
    
    D3D12_RESOURCE_DESC Desc = pDestinationResource->GetDesc();
    ID3D12Device* pDevice;
    pDestinationResource->GetDevice(__uuidof(*pDevice), reinterpret_cast<void**>(&pDevice));
    pDevice->GetCopyableFootprints(&Desc, FirstSubresource, NumSubresources, IntermediateOffset, pLayouts, pNumRows, pRowSizesInBytes, &RequiredSize);
    pDevice->Release();
    
    UINT64 Result = UpdateSubresources(pCmdList, pDestinationResource, pIntermediate, FirstSubresource, NumSubresources, RequiredSize, pLayouts, pNumRows, pRowSizesInBytes, pSrcData);
    HeapFree(GetProcessHeap(), 0, pMem);
    return Result;
}

//------------------------------------------------------------------------------------------------
// Stack-allocating UpdateSubresources implementation
template <UINT MaxSubresources>
inline UINT64 UpdateSubresources( 
    _In_ ID3D12GraphicsCommandList* pCmdList,
    _In_ ID3D12Resource* pDestinationResource,
    _In_ ID3D12Resource* pIntermediate,
    UINT64 IntermediateOffset,
    _In_range_(0, MaxSubresources) UINT FirstSubresource,
    _In_range_(1, MaxSubresources - FirstSubresource) UINT NumSubresources,
    _In_reads_(NumSubresources) D3D12_SUBRESOURCE_DATA* pSrcData)
{
    UINT64 RequiredSize = 0;
    D3D12_PLACED_SUBRESOURCE_FOOTPRINT Layouts[MaxSubresources];
    UINT NumRows[MaxSubresources];
    UINT64 RowSizesInBytes[MaxSubresources];
    
    D3D12_RESOURCE_DESC Desc = pDestinationResource->GetDesc();
    ID3D12Device* pDevice;
    pDestinationResource->GetDevice(__uuidof(*pDevice), reinterpret_cast<void**>(&pDevice));
    pDevice->GetCopyableFootprints(&Desc, FirstSubresource, NumSubresources, IntermediateOffset, Layouts, NumRows, RowSizesInBytes, &RequiredSize);
    pDevice->Release();
    
    return UpdateSubresources(pCmdList, pDestinationResource, pIntermediate, FirstSubresource, NumSubresources, RequiredSize, Layouts, NumRows, RowSizesInBytes, pSrcData);
}

//------------------------------------------------------------------------------------------------
inline bool D3D12IsLayoutOpaque( D3D12_TEXTURE_LAYOUT Layout )
{ return Layout == D3D12_TEXTURE_LAYOUT_UNKNOWN || Layout == D3D12_TEXTURE_LAYOUT_64KB_UNDEFINED_SWIZZLE; }

//------------------------------------------------------------------------------------------------
inline ID3D12CommandList * const * CommandListCast(ID3D12GraphicsCommandList * const * pp)
{
    // This cast is useful for passing strongly typed command list pointers into
    // ExecuteCommandLists.
    // This cast is valid as long as the const-ness is respected. D3D12 APIs do
    // respect the const-ness of their arguments.
    return reinterpret_cast<ID3D12CommandList * const *>(pp);
}


#endif // defined( __cplusplus )

#endif //__D3DX12_H__




```

`windows_build.md`:

```md
## Windows build

0. [Install Microsoft Visual Studio](https://visualstudio.microsoft.com/) (2017 or later). Make sure
   the option "Desktop development with C++" is selected (you can add it later if not).

1. [Install git for windows](https://git-scm.com/download/win) - this can be used to get lc0 but is also
   needed for meson. If you haven't downloaded lc0, you can do it now following the instructions in
   the `README`(https://github.com/LeelaChessZero/lc0/blob/master/README.md).

2. GPU users with nVIDIA cards (and "compute capability" 3.0 or higher) can build with CUDA/CuDNN.
*  Install [CUDA](https://developer.nvidia.com/cuda-zone) (v10.0 is fine for Visual Studio 2017, newer is
   needed for Visual Studio 2019) and then
*  install the appropriate [cuDNN](https://developer.nvidia.com/cudnn).

3. GPU users with recent Windows 10 installations can build with DirectX 12, this only requires updated
   SDK headers (that may already be available in Visual Studio).

4. CPU users may want to install a BLAS library. This can be either OpenBLAS, Intel MKL or Intel DNNL.
   This is optional since the Eigen library can be used without installing anything, but probably with
   worse performance.
*  For [OpenBLAS go here](http://www.openblas.net/), you need a binary package with a filename of the
   form `OpenBLAS-version-Win64-int32.zip`, they are not available for all versions, which you just unpack
   at a location of your choise (but not inside the lc0 directory).
*  For [Intel MKL go here](https://software.intel.com/en-us/mkl), where you need to register. After
   installation don't forget to run `mklvars.bat intel64` to set up the paths to the dlls.
*  For [Intel DNNL go here](https://github.com/intel/mkl-dnn/releases). Note that not all releases have
   binaries available, you want `dnnl_win_*_cpu_vcomp.zip`.

5. For OpenCL you also need to install OpenCL developer libraries.
*  For AMD cards the AMD APP SDK 3.0 seems to be the appropriate one, to be installed after the card drivers.
   This is not currently available on the AMD website, but links to a signed installer are available in the
   [AMD community forum](https://community.amd.com/thread/222855).
*  For nVIDIA cards it is included in the [CUDA toolkit](https://developer.nvidia.com/cuda-downloads).

6. [Install Python3](https://www.python.org/) - be sure to check the box to add python to the path.

7. Install Meson: `pip3 install --upgrade meson`

8. Edit `build.cmd`:
*  At the top, set to `true` and `false` the variables for the backends you want to build.
*  Then set the paths for the build dependencies.
    - Note: for `OPENCL_INCLUDE_PATH` you don't want the directory containing `opencl.h`, but one level higher
    (the one containing `CL`).

9. Run `build.cmd`. It will ask permission to delete the build directory, then generate MSVS project and
   pause.

10. Hit `Enter` to build it.

11. Resulting binary will be `build/lc0.exe`

Alternatively you can

10. open generated solution `build/lc0.sln` in Visual Studio and build yourself.


### Troubleshooting

If you get something like

   Downloading zlib patch from https://wrapdb.mesonbuild.com/v1/projects/zlib/1.2.11/4/get_zip
   A fallback URL could be specified using patch_fallback_url key in the wrap file

   meson.build:604:4: ERROR: WrapDB connection failed to https://wrapdb.mesonbuild.com/v1/projects/zlib/1.2.11/4/get_zip with error <urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: certificate has expired (_ssl.c:1108)>

when you run build.cmd, then download the

   https://wrapdb.mesonbuild.com/v1/projects/zlib/1.2.11/4/get_zip

file in your browser, and place the file in the lc0\subprojects\packagecache folder. then Remove the

   lc0\subprojects\zlib-1.2.11

folder, and run build again.

```