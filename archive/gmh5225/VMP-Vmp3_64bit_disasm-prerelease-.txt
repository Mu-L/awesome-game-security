Project Path: arc_gmh5225_VMP-Vmp3_64bit_disasm-prerelease-_lm9eqalr

Source Tree:

```txt
arc_gmh5225_VMP-Vmp3_64bit_disasm-prerelease-_lm9eqalr
├── Cargo.lock
├── Cargo.toml
├── README.md
├── build.rs
├── optimize_llvm.sh
├── src
│   ├── llvm_ir_gen
│   │   ├── VMProtectHelpers_64.ll
│   │   └── mod.rs
│   ├── main.rs
│   ├── match_assembly.rs
│   ├── symbolic.rs
│   ├── transforms.rs
│   ├── util.rs
│   ├── vm_handler.rs
│   └── vm_matchers.rs
├── test.ll
└── vgk.tar.gz

```

`Cargo.lock`:

```lock
# This file is automatically @generated by Cargo.
# It is not intended for manual editing.
version = 3

[[package]]
name = "aho-corasick"
version = "0.7.18"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "1e37cfd5e7657ada45f742d6e99ca5788580b5c529dc78faf11ece6dc702656f"
dependencies = [
 "memchr",
]

[[package]]
name = "atty"
version = "0.2.14"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d9b39be18770d11421cdb1b9947a45dd3f37e93092cbf377614828a319d5fee8"
dependencies = [
 "hermit-abi",
 "libc",
 "winapi 0.3.9",
]

[[package]]
name = "autocfg"
version = "1.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d468802bab17cbc0cc575e9b053f41e72aa36bfa6b7f55e3529ffa43161b97fa"

[[package]]
name = "bitflags"
version = "1.3.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bef38d45163c2f1dde094a7dfd33ccf595c92905c8f8f4fdc18d06fb1037718a"

[[package]]
name = "boolector"
version = "0.4.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5567f99e46c8f1f61624adafd2fdf4424b203d9fc5534368421725af17c5fd21"
dependencies = [
 "boolector-sys",
 "libc",
]

[[package]]
name = "boolector-sys"
version = "0.7.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5b082d1c14d94d734ad51bfb114f451bf6f1e1054dafae6ff00265d2fe5963e5"
dependencies = [
 "cc",
 "cmake",
 "copy_dir",
 "libc",
]

[[package]]
name = "cc"
version = "1.0.73"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2fff2a6927b3bb87f9595d67196a70493f627687a71d87a0d692242c33f58c11"

[[package]]
name = "cfg-if"
version = "0.1.10"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4785bdd1c96b2a846b2bd7cc02e86b6b3dbf14e7e53446c4f54c92a361040822"

[[package]]
name = "cfg-if"
version = "1.0.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "baf1de4339761588bc0619e3cbc0120ee582ebb74b53b4efbf79117bd2da40fd"

[[package]]
name = "clap"
version = "3.2.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9f1fe12880bae935d142c8702d500c63a4e8634b6c3c57ad72bf978fc7b6249a"
dependencies = [
 "atty",
 "bitflags",
 "clap_derive",
 "clap_lex",
 "indexmap",
 "once_cell",
 "strsim",
 "termcolor",
 "textwrap",
]

[[package]]
name = "clap_derive"
version = "3.2.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ed6db9e867166a43a53f7199b5e4d1f522a1e5bd626654be263c999ce59df39a"
dependencies = [
 "heck",
 "proc-macro-error",
 "proc-macro2",
 "quote",
 "syn",
]

[[package]]
name = "clap_lex"
version = "0.2.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "87eba3c8c7f42ef17f6c659fc7416d0f4758cd3e58861ee63c5fa4a4dde649e4"
dependencies = [
 "os_str_bytes",
]

[[package]]
name = "cmake"
version = "0.1.48"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e8ad8cef104ac57b68b89df3208164d228503abbdce70f6880ffa3d970e7443a"
dependencies = [
 "cc",
]

[[package]]
name = "copy_dir"
version = "0.1.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "6e4281031634644843bd2f5aa9c48cf98fc48d6b083bd90bb11becf10deaf8b0"
dependencies = [
 "walkdir",
]

[[package]]
name = "cpp_demangle"
version = "0.2.16"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "13c924384107361ca729c7d46b9134151b9a955ce99a773784f2777498e8552d"
dependencies = [
 "cfg-if 0.1.10",
 "glob",
]

[[package]]
name = "dataview"
version = "0.1.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "47a802a2cad0ff4dfc4f3110da174b7a6928c315cae523e88638cfb72941b4d5"
dependencies = [
 "derive_pod",
]

[[package]]
name = "derive_pod"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2e2d4527fc8a6ad5cdb22622bed719d47f5e47f2e7b53cb821e1c933f62f3eca"

[[package]]
name = "either"
version = "1.6.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e78d4f1cc4ae33bbfc157ed5d5a5ef3bc29227303d595861deb238fcec4e9457"

[[package]]
name = "fixedbitset"
version = "0.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "279fb028e20b3c4c320317955b77c5e0c9701f05a1d309905d6fc702cdc5053e"

[[package]]
name = "glob"
version = "0.3.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9b919933a397b79c37e33b77bb2aa3dc8eb6e165ad809e58ff75bc7db2e34574"

[[package]]
name = "hashbrown"
version = "0.12.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "db0d4cf898abf0081f964436dc980e96670a0f36863e4b83aaacdb65c9d7ccc3"

[[package]]
name = "haybale"
version = "0.7.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4fb3b65f04dd0aa507c6b98a3e7062ceb9a1f9fa67d61cf8fd29408113171418"
dependencies = [
 "boolector",
 "cpp_demangle",
 "either",
 "itertools",
 "llvm-ir",
 "log",
 "reduce",
 "rustc-demangle",
 "rustversion",
]

[[package]]
name = "heck"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2540771e65fc8cb83cd6e8a237f70c319bd5c29f78ed1084ba5d50eeac86f7f9"

[[package]]
name = "hermit-abi"
version = "0.1.19"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "62b467343b94ba476dcb2500d242dadbb39557df889310ac77c5d99100aaac33"
dependencies = [
 "libc",
]

[[package]]
name = "iced-x86"
version = "1.17.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "158f5204401d08f91d19176112146d75e99b3cf745092e268fa7be33e09adcec"
dependencies = [
 "lazy_static",
 "static_assertions",
]

[[package]]
name = "indexmap"
version = "1.9.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "10a35a97730320ffe8e2d410b5d3b69279b98d2c14bdb8b70ea89ecf7888d41e"
dependencies = [
 "autocfg",
 "hashbrown",
]

[[package]]
name = "inkwell"
version = "0.1.0"
source = "git+https://github.com/TheDan64/inkwell?branch=master#25b9fc5870370211504e874e7c81dc53573bca79"
dependencies = [
 "either",
 "inkwell_internals",
 "libc",
 "llvm-sys",
 "once_cell",
 "parking_lot",
]

[[package]]
name = "inkwell_internals"
version = "0.5.0"
source = "git+https://github.com/TheDan64/inkwell?branch=master#25b9fc5870370211504e874e7c81dc53573bca79"
dependencies = [
 "proc-macro2",
 "quote",
 "syn",
]

[[package]]
name = "itertools"
version = "0.10.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a9a9d19fa1e79b6215ff29b9d6880b706147f16e9b1dbb1e4e5947b5b02bc5e3"
dependencies = [
 "either",
]

[[package]]
name = "kernel32-sys"
version = "0.2.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7507624b29483431c0ba2d82aece8ca6cdba9382bff4ddd0f7490560c056098d"
dependencies = [
 "winapi 0.2.8",
 "winapi-build",
]

[[package]]
name = "lazy_static"
version = "1.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e2abad23fbc42b3700f2f279844dc832adb2b2eb069b2df918f455c4e18cc646"

[[package]]
name = "libc"
version = "0.2.126"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "349d5a591cd28b49e1d1037471617a32ddcda5731b99419008085f72d5a53836"

[[package]]
name = "llvm-ir"
version = "0.8.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7f292d36cc01be049372faa8e5fb4aaa261d4875f7d5cdc59a86353434b7ed02"
dependencies = [
 "either",
 "llvm-sys",
 "log",
]

[[package]]
name = "llvm-sys"
version = "130.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bdb6ea20e8a348f6db0b43a7f009fa7d981d22edf4cbe2e0c7b2247dbb25be61"
dependencies = [
 "cc",
 "lazy_static",
 "libc",
 "regex",
 "semver",
]

[[package]]
name = "lock_api"
version = "0.4.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "327fa5b6a6940e4699ec49a9beae1ea4845c6bab9314e4f84ac68742139d8c53"
dependencies = [
 "autocfg",
 "scopeguard",
]

[[package]]
name = "log"
version = "0.4.17"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "abb12e687cfb44aa40f41fc3978ef76448f9b6038cad6aef4259d3c095a2382e"
dependencies = [
 "cfg-if 1.0.0",
]

[[package]]
name = "memchr"
version = "2.5.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2dffe52ecf27772e601905b7522cb4ef790d2cc203488bbd0e2fe85fcb74566d"

[[package]]
name = "no-std-compat"
version = "0.4.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b93853da6d84c2e3c7d730d6473e8817692dd89be387eb01b94d7f108ecb5b8c"

[[package]]
name = "once_cell"
version = "1.12.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7709cef83f0c1f58f666e746a08b21e0085f7440fa6a29cc194d68aac97a4225"

[[package]]
name = "os_str_bytes"
version = "6.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "21326818e99cfe6ce1e524c2a805c189a99b5ae555a35d19f9a284b427d86afa"

[[package]]
name = "parking_lot"
version = "0.12.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3742b2c103b9f06bc9fff0a37ff4912935851bee6d36f3c02bcc755bcfec228f"
dependencies = [
 "lock_api",
 "parking_lot_core",
]

[[package]]
name = "parking_lot_core"
version = "0.9.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "09a279cbf25cb0757810394fbc1e359949b59e348145c643a939a525692e6929"
dependencies = [
 "cfg-if 1.0.0",
 "libc",
 "redox_syscall",
 "smallvec",
 "windows-sys",
]

[[package]]
name = "pelite"
version = "0.9.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9f1510e9ff41f8de8db8e2c763af5f75ad0e45003cff34ed1ba134205db0c9fc"
dependencies = [
 "dataview",
 "libc",
 "no-std-compat",
 "pelite-macros",
 "winapi 0.3.9",
]

[[package]]
name = "pelite-macros"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7a7cf3f8ecebb0f4895f4892a8be0a0dc81b498f9d56735cb769dc31bf00815b"

[[package]]
name = "pest"
version = "2.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "10f4872ae94d7b90ae48754df22fd42ad52ce740b8f370b03da4835417403e53"
dependencies = [
 "ucd-trie",
]

[[package]]
name = "petgraph"
version = "0.6.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e6d5014253a1331579ce62aa67443b4a658c5e7dd03d4bc6d302b94474888143"
dependencies = [
 "fixedbitset",
 "indexmap",
]

[[package]]
name = "proc-macro-error"
version = "1.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "da25490ff9892aab3fcf7c36f08cfb902dd3e71ca0f9f9517bea02a73a5ce38c"
dependencies = [
 "proc-macro-error-attr",
 "proc-macro2",
 "quote",
 "syn",
 "version_check",
]

[[package]]
name = "proc-macro-error-attr"
version = "1.0.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a1be40180e52ecc98ad80b184934baf3d0d29f979574e439af5a55274b35f869"
dependencies = [
 "proc-macro2",
 "quote",
 "version_check",
]

[[package]]
name = "proc-macro2"
version = "1.0.40"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "dd96a1e8ed2596c337f8eae5f24924ec83f5ad5ab21ea8e455d3566c69fbcaf7"
dependencies = [
 "unicode-ident",
]

[[package]]
name = "quote"
version = "1.0.20"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "3bcdf212e9776fbcb2d23ab029360416bb1706b1aea2d1a5ba002727cbcab804"
dependencies = [
 "proc-macro2",
]

[[package]]
name = "redox_syscall"
version = "0.2.13"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "62f25bc4c7e55e0b0b7a1d43fb893f4fa1361d0abe38b9ce4f323c2adfe6ef42"
dependencies = [
 "bitflags",
]

[[package]]
name = "reduce"
version = "0.1.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "16d2dc47b68ac15ea328cd7ebe01d7d512ed29787f7d534ad2a3c341328b35d7"

[[package]]
name = "regex"
version = "1.5.6"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d83f127d94bdbcda4c8cc2e50f6f84f4b611f69c902699ca385a39c3a75f9ff1"
dependencies = [
 "aho-corasick",
 "memchr",
 "regex-syntax",
]

[[package]]
name = "regex-syntax"
version = "0.6.26"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "49b3de9ec5dc0a3417da371aab17d729997c15010e7fd24ff707773a33bddb64"

[[package]]
name = "rustc-demangle"
version = "0.1.21"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "7ef03e0a2b150c7a90d01faf6254c9c48a41e95fb2a8c2ac1c6f0d2b9aefc342"

[[package]]
name = "rustversion"
version = "1.0.7"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a0a5f7c728f5d284929a1cccb5bc19884422bfe6ef4d6c409da2c41838983fcf"

[[package]]
name = "scopeguard"
version = "1.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "d29ab0c6d3fc0ee92fe66e2d99f700eab17a8d57d1c1d3b748380fb20baa78cd"

[[package]]
name = "semver"
version = "0.11.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "f301af10236f6df4160f7c3f04eec6dbc70ace82d23326abad5edee88801c6b6"
dependencies = [
 "semver-parser",
]

[[package]]
name = "semver-parser"
version = "0.10.2"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "00b0bef5b7f9e0df16536d3961cfb6e84331c065b4066afb39768d0e319411f7"
dependencies = [
 "pest",
]

[[package]]
name = "smallvec"
version = "1.8.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "cc88c725d61fc6c3132893370cac4a0200e3fedf5da8331c570664b1987f5ca2"

[[package]]
name = "static_assertions"
version = "1.1.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "a2eb9349b6444b326872e140eb1cf5e7c522154d69e7a0ffb0fb81c06b37543f"

[[package]]
name = "strsim"
version = "0.10.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "73473c0e59e6d5812c5dfe2a064a6444949f089e20eec9a2e5506596494e4623"

[[package]]
name = "syn"
version = "1.0.98"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c50aef8a904de4c23c788f104b7dddc7d6f79c647c7c8ce4cc8f73eb0ca773dd"
dependencies = [
 "proc-macro2",
 "quote",
 "unicode-ident",
]

[[package]]
name = "termcolor"
version = "1.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "bab24d30b911b2376f3a13cc2cd443142f0c81dda04c118693e35b3835757755"
dependencies = [
 "winapi-util",
]

[[package]]
name = "textwrap"
version = "0.15.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "b1141d4d61095b28419e22cb0bbf02755f5e54e0526f97f1e3d1d160e60885fb"

[[package]]
name = "ucd-trie"
version = "0.1.3"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "56dee185309b50d1f11bfedef0fe6d036842e3fb77413abef29f8f8d1c5d4c1c"

[[package]]
name = "unicode-ident"
version = "1.0.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5bd2fe26506023ed7b5e1e315add59d6f584c621d037f9368fea9cfb988f368c"

[[package]]
name = "version_check"
version = "0.9.4"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "49874b5167b65d7193b8aba1567f5c7d93d001cafc34600cee003eda787e483f"

[[package]]
name = "vmp3_disasm"
version = "0.1.0"
dependencies = [
 "clap",
 "haybale",
 "iced-x86",
 "inkwell",
 "pelite",
 "petgraph",
]

[[package]]
name = "walkdir"
version = "0.1.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c66c0b9792f0a765345452775f3adbd28dde9d33f30d13e5dcc5ae17cf6f3780"
dependencies = [
 "kernel32-sys",
 "winapi 0.2.8",
]

[[package]]
name = "winapi"
version = "0.2.8"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "167dc9d6949a9b857f3451275e911c3f44255842c1f7a76f33c55103a909087a"

[[package]]
name = "winapi"
version = "0.3.9"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "5c839a674fcd7a98952e593242ea400abe93992746761e38641405d28b00f419"
dependencies = [
 "winapi-i686-pc-windows-gnu",
 "winapi-x86_64-pc-windows-gnu",
]

[[package]]
name = "winapi-build"
version = "0.1.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "2d315eee3b34aca4797b2da6b13ed88266e6d612562a0c46390af8299fc699bc"

[[package]]
name = "winapi-i686-pc-windows-gnu"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ac3b87c63620426dd9b991e5ce0329eff545bccbbb34f3be09ff6fb6ab51b7b6"

[[package]]
name = "winapi-util"
version = "0.1.5"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "70ec6ce85bb158151cae5e5c87f95a8e97d2c0c4b001223f33a334e3ce5de178"
dependencies = [
 "winapi 0.3.9",
]

[[package]]
name = "winapi-x86_64-pc-windows-gnu"
version = "0.4.0"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "712e227841d057c1ee1cd2fb22fa7e5a5461ae8e48fa2ca79ec42cfc1931183f"

[[package]]
name = "windows-sys"
version = "0.36.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "ea04155a16a59f9eab786fe12a4a450e75cdb175f9e0d80da1e17db09f55b8d2"
dependencies = [
 "windows_aarch64_msvc",
 "windows_i686_gnu",
 "windows_i686_msvc",
 "windows_x86_64_gnu",
 "windows_x86_64_msvc",
]

[[package]]
name = "windows_aarch64_msvc"
version = "0.36.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "9bb8c3fd39ade2d67e9874ac4f3db21f0d710bee00fe7cab16949ec184eeaa47"

[[package]]
name = "windows_i686_gnu"
version = "0.36.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "180e6ccf01daf4c426b846dfc66db1fc518f074baa793aa7d9b9aaeffad6a3b6"

[[package]]
name = "windows_i686_msvc"
version = "0.36.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "e2e7917148b2812d1eeafaeb22a97e4813dfa60a3f8f78ebe204bcc88f12f024"

[[package]]
name = "windows_x86_64_gnu"
version = "0.36.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "4dcd171b8776c41b97521e5da127a2d86ad280114807d0b2ab1e462bc764d9e1"

[[package]]
name = "windows_x86_64_msvc"
version = "0.36.1"
source = "registry+https://github.com/rust-lang/crates.io-index"
checksum = "c811ca4a8c853ef420abd8592ba53ddbbac90410fab6903b3e79972a631f7680"

```

`Cargo.toml`:

```toml
[package]
name = "vmp3_disasm"
version = "0.1.0"
edition = "2021"

# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html

[dependencies]
iced-x86 = "*"
clap = {version = "*", features = ["derive"]}
pelite = "*"
haybale = { version = "*", features = ["llvm-13", "vendor-boolector"] }
inkwell = { git = "https://github.com/TheDan64/inkwell", branch = "master", features = ["llvm13-0"] }
petgraph = "*"

```

`README.md`:

```md
# Vmp3_64bit_disasm-prerelease- WIP WIP WIP
Not all handlers supported yet!
Basic branching supported!
Currently working on loops and branches with more than 2 targets
Comming soon !
## Info

This project was tested on vgk.sys (sha-1 266ddd98fdd9df939993d947b0edb052a347316f)

## Example

### Call into vmp3 with pushed value
![example1](https://user-images.githubusercontent.com/102005914/175548145-8cb85a51-fef4-4a4c-b11b-f8049636b590.png)

### Converting the address to decimal ( Newest commit fixed this use hex addresses now )
![example2](https://user-images.githubusercontent.com/102005914/175548162-5d352eda-c66c-481b-ac7a-1697faa23e09.png)

### Invoking the disassembler
![example3](https://user-images.githubusercontent.com/102005914/175548166-ccc3bde9-fd20-44b7-850e-5b2c07119874.png)

## Example Output
```
0x14039cf4b     | pop64 r19                      | 0x140725d72
0x14039cf50     | pop64 r10                      | 0x14069504f
0x14039cf55     | pop64 r9                       | 0x1406d0453
0x14039cf5a     | pop64 r18                      | 0x14069a8da
0x14039cf5f     | pop64 r11                      | 0x1406de987
0x14039cf64     | pop64 r22                      | 0x140740966
0x14039cf69     | pop64 r21                      | 0x14076a534
0x14039cf6e     | pop64 r8                       | 0x14073cc29
0x14039cf73     | pop64 r17                      | 0x14066b565
0x14039cf78     | pop64 r7                       | 0x1406c5fd7
0x14039cf7d     | pop64 r5                       | 0x1406a0a37
0x14039cf82     | pop64 r0                       | 0x140725d72
0x14039cf87     | pop64 r16                      | 0x14069504f
0x14039cf8c     | pop64 r4                       | 0x1406d0453
0x14039cf91     | pop64 r13                      | 0x14069a8da
0x14039cf96     | pop64 r3                       | 0x1406de987
0x14039cf9b     | pop64 r2                       | 0x140740966
0x14039cfa0     | pop64 r14                      | 0x14076a534
0x14039cfa5     | pop64 r15                      | 0x14073cc29
0x14039cfaa     | push_imm64 0x1400148a2         | 0x1406b58aa
0x14039cfb6     | push64 r19                     | 0x1407233bb
0x14039cfbb     | add64                          | 0x14065c9e4
0x14039cfbf     | pop64 r23                      | 0x14066b565
0x14039cfc4     | pop64 r24                      | 0x1406c5fd7
0x14039cfc9     | pushvsp64                      | 0x1406dd647
0x14039cfcd     | pop64 r25                      | 0x1406a0a37
0x14039cfd2     | push64 r3                      | 0x1407504ba
0x14039cfd7     | pop64 r26                      | 0x140725d72
0x14039cfdc     | push64 r2                      | 0x1406b0c5d
0x14039cfe1     | pop64 r27                      | 0x14069504f
0x14039cfe6     | push64 r13                     | 0x14073ce7e
0x14039cfeb     | pop64 r28                      | 0x1406d0453
0x14039cff0     | push64 r18                     | 0x140776f28
0x14039cff5     | pop64 r29                      | 0x14069a8da
0x14039cffa     | push_imm64 0x1407962a0         | 0x14066ecc1
0x14039d006     | push_imm32 0x3                 | 0x14068f1ea
0x14039d00e     | pop32 r16_dword_0              | 0x14067b681
0x14039d013     | push_imm32 0x0                 | 0x140756802
0x14039d01b     | pop32 r16_dword_1              | 0x14068d4fe
0x14039d020     | push_imm64 0x140074b10         | 0x140720d16
0x14039d02c     | push64 r19                     | 0x140652822
0x14039d031     | add64                          | 0x1406f2971
0x14039d035     | push64 r18                     | 0x140720c6d
0x14039d03a     | pop64 r23                      | 0x1406de987
0x14039d03f     | pop64 r20                      | 0x140740966
0x14039d044     | pop64 r12                      | 0x14076a534
0x14039d049     | push64 r19                     | 0x1407612e8
0x14039d04e     | add64                          | 0x140723579
0x14039d052     | pop64 r15                      | 0x14073cc29
0x14039d057     | push_imm64 0x140069068         | 0x140735929
0x14039d063     | push64 r19                     | 0x1406953ee
0x14039d068     | add64                          | 0x14071e780
0x14039d06c     | pop64 r11                      | 0x14066b565
0x14039d071     | fetch64                        | 0x14071bcc3
0x14039d075     | push64 r2                      | 0x140676a3e
0x14039d07a     | push64 r3                      | 0x140740fae
0x14039d07f     | push64 r13                     | 0x1407233bb
0x14039d084     | push64 r23                     | 0x1407504ba
0x14039d089     | push64 r16                     | 0x1406b0c5d
0x14039d08e     | push64 r0                      | 0x14073ce7e
0x14039d093     | push64 r5                      | 0x140776f28
0x14039d098     | push64 r7                      | 0x140652822
0x14039d09d     | push64 r17                     | 0x140720c6d
0x14039d0a2     | push64 r8                      | 0x1407612e8
0x14039d0a7     | push64 r21                     | 0x1406953ee
0x14039d0ac     | push64 r22                     | 0x140676a3e
0x14039d0b1     | push64 r12                     | 0x140740fae
0x14039d0b6     | push64 r18                     | 0x1407233bb
0x14039d0bb     | push64 r9                      | 0x1407504ba
0x14039d0c0     | push64 r10                     | 0x1406b0c5d
0x14039d0c5     | vm_exit                        | 0x1406f0494
Getting helper stub -> helperstub_14039cf47

define i64 @helperfunction_14039cf47(i64* noalias %rax, i64* noalias %rbx, i64* noalias %rcx, i64* noalias %rdx, i64* noalias %rsi, i64* noalias %rdi, i64* noalias %rbp, i64* noalias %rsp, i64* noalias %r8, i64* noalias %r9, i64* noalias %r10, i64* noalias %r11, i64* noalias %r12, i64* noalias %r13, i64* noalias %r14, i64* noalias %r15, i64* noalias %flags, i64 %KEY_STUB, i64 %RET_ADDR, i64 %REL_ADDR) {
  call void @llvm.experimental.noalias.scope.decl(metadata !10)
  call void @llvm.experimental.noalias.scope.decl(metadata !13)
  call void @llvm.experimental.noalias.scope.decl(metadata !15)
  call void @llvm.experimental.noalias.scope.decl(metadata !17)
  call void @llvm.experimental.noalias.scope.decl(metadata !19)
  %1 = load i64, i64* %rsp, align 8, !tbaa !3, !alias.scope !19, !noalias !21
  %2 = add i64 %1, -8
  %3 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %2
  %4 = bitcast i8* %3 to i64*
  %5 = load i64, i64* %rbx, align 8, !alias.scope !10, !noalias !36
  store i64 5376664224, i64* %4, align 1, !noalias !37
  %6 = load i64, i64* bitcast (i8* getelementptr inbounds ([0 x i8], [0 x i8]* @RAM, i64 0, i64 5369139304) to i64*), align 1, !noalias !37
  store i64 5369187088, i64* %r8, align 8, !tbaa !3, !alias.scope !17, !noalias !38
  store i64 3, i64* %rcx, align 8, !tbaa !3, !alias.scope !13, !noalias !39
  store i64 %5, i64* %rdx, align 8, !tbaa !3, !alias.scope !15, !noalias !40
  store i64 %2, i64* %rsp, align 8, !tbaa !3, !alias.scope !19, !noalias !21
  ret i64 %6
}

```

# Open source software usage
The c++ code that compiles to the llvm helper file was released by FvrMateo and is available here https://github.com/LLVMParty/TicklingVMProtect/tree/master/Helpers


This project uses the following open source rust crates
- iced_x86 https://github.com/icedland/iced
- clap https://github.com/icedland/iced
- pelite https://github.com/CasualX/pelite
- haybale https://github.com/PLSysSec/haybale
- inkwell https://github.com/TheDan64/inkwell
- petgraph https://github.com/petgraph/petgraph

The project also makes heavy use of llvm software


```

`build.rs`:

```rs
fn main() {
    println!("cargo:rustc-link-arg=-lLLVM-13");
}

```

`optimize_llvm.sh`:

```sh
for i in {1..2}
do
    opt devirt.ll --always-inline --cfl-steens-aa --cfl-anders-aa --tbaa --scoped-noalias-aa --simplifycfg --sroa --early-cse-memssa --instcombine --simplifycfg --gvn-hoist --gvn-sink --dse --instcombine --simplifycfg -S -o devirt.ll
done

# @RAM = external dso_local local_unnamed_addr global [0 x i8], align 1
# @RAM = dso_local constant [0 x i8] zeroinitializer, align 1
sed -i "s/RAM = external dso_local local_unnamed_addr global \[0 x i8\], align 1/RAM = dso_local constant [0 x i8] zeroinitializer, align 1/" devirt.ll
sed -i "s/local_unnamed_addr global i64 undef, align 8/constant i64 undef, align 8/" devirt.ll

for i in {1..2}
do
    opt devirt.ll --always-inline --cfl-steens-aa --cfl-anders-aa --tbaa --scoped-noalias-aa --simplifycfg --sroa --early-cse-memssa --instcombine --simplifycfg --gvn-hoist --gvn-sink --dse --instcombine --simplifycfg -S -o devirt.ll
done

#llvm-as devirt.ll
#
#clang devirt.ll -c -O0
#binaryninja devirt.o

```

`src/llvm_ir_gen/VMProtectHelpers_64.ll`:

```ll
; ModuleID = 'VMProtectHelpers.cpp'
source_filename = "VMProtectHelpers.cpp"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-pc-linux-gnu"

%struct.VirtualRegister = type { %union.anon }
%union.anon = type { i64 }
%struct.anon = type { i8, i8, i8, i8, i8, i8, i8, i8 }
%struct.VirtualContext = type { %struct.VirtualRegister, %struct.VirtualRegister, %struct.VirtualRegister, %struct.VirtualRegister, %struct.VirtualRegister, %struct.VirtualRegister, %struct.VirtualRegister, %struct.VirtualRegister, %struct.VirtualRegister, %struct.VirtualRegister, %struct.VirtualRegister, %struct.VirtualRegister, %struct.VirtualRegister, %struct.VirtualRegister, %struct.VirtualRegister, %struct.VirtualRegister }

$_Z8PUSH_IMMImEvRmT_ = comdat any

$_Z8PUSH_IMMIjEvRmT_ = comdat any

$_Z8PUSH_IMMItEvRmT_ = comdat any

$_Z8PUSH_VSPILm64EEvRm = comdat any

$_Z8PUSH_VSPILm32EEvRm = comdat any

$_Z8PUSH_VSPILm16EEvRm = comdat any

$_Z7POP_VSPILm64EEvRm = comdat any

$_Z7POP_VSPILm32EEvRm = comdat any

$_Z7POP_VSPILm16EEvRm = comdat any

$_Z4LOADImEvRm = comdat any

$_Z4LOADIjEvRm = comdat any

$_Z4LOADItEvRm = comdat any

$_Z4LOADIhEvRm = comdat any

$_Z7LOAD_GSImEvRm = comdat any

$_Z7LOAD_GSIjEvRm = comdat any

$_Z7LOAD_GSItEvRm = comdat any

$_Z7LOAD_GSIhEvRm = comdat any

$_Z7LOAD_FSImEvRm = comdat any

$_Z7LOAD_FSIjEvRm = comdat any

$_Z7LOAD_FSItEvRm = comdat any

$_Z7LOAD_FSIhEvRm = comdat any

$_Z5STOREImEvRm = comdat any

$_Z5STOREIjEvRm = comdat any

$_Z5STOREItEvRm = comdat any

$_Z5STOREIhEvRm = comdat any

$_Z10PUSH_VMREGILm8ELm0EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm8ELm1EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm16ELm0EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm16ELm1EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm16ELm2EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm16ELm3EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm32ELm0EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm32ELm1EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm64ELm0EEvRm15VirtualRegister = comdat any

$_Z9POP_VMREGILm8ELm0EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm8ELm1EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm16ELm0EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm16ELm1EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm16ELm2EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm16ELm3EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm32ELm0EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm32ELm1EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm64ELm0EEvRmR15VirtualRegister = comdat any

$_Z3ADDImEvRm = comdat any

$_Z3ADDIjEvRm = comdat any

$_Z3ADDItEvRm = comdat any

$_Z3ADDIhEvRm = comdat any

$_Z3DIVImEvRm = comdat any

$_Z3DIVIjEvRm = comdat any

$_Z3DIVItEvRm = comdat any

$_Z3DIVIhEvRm = comdat any

$_Z4IDIVImEvRm = comdat any

$_Z4IDIVIjEvRm = comdat any

$_Z4IDIVItEvRm = comdat any

$_Z4IDIVIhEvRm = comdat any

$_Z3MULImEvRm = comdat any

$_Z3MULIjEvRm = comdat any

$_Z3MULItEvRm = comdat any

$_Z3MULIhEvRm = comdat any

$_Z4IMULImEvRm = comdat any

$_Z4IMULIjEvRm = comdat any

$_Z4IMULItEvRm = comdat any

$_Z4IMULIhEvRm = comdat any

$_Z3NORImEvRm = comdat any

$_Z3NORIjEvRm = comdat any

$_Z3NORItEvRm = comdat any

$_Z3NORIhEvRm = comdat any

$_Z4NANDImEvRm = comdat any

$_Z4NANDIjEvRm = comdat any

$_Z4NANDItEvRm = comdat any

$_Z4NANDIhEvRm = comdat any

$_Z3SHLImEvRm = comdat any

$_Z3SHLIjEvRm = comdat any

$_Z3SHLItEvRm = comdat any

$_Z3SHLIhEvRm = comdat any

$_Z3SHRImEvRm = comdat any

$_Z3SHRIjEvRm = comdat any

$_Z3SHRItEvRm = comdat any

$_Z3SHRIhEvRm = comdat any

$_Z4SHLDImEvRm = comdat any

$_Z4SHLDIjEvRm = comdat any

$_Z4SHLDItEvRm = comdat any

$_Z4SHLDIhEvRm = comdat any

$_Z4SHRDImEvRm = comdat any

$_Z4SHRDIjEvRm = comdat any

$_Z4SHRDItEvRm = comdat any

$_Z4SHRDIhEvRm = comdat any

@SEM_PUSH_CR0 = dso_local constant void (i64*)* @_Z8PUSH_CR0Rm, align 8
@SEM_PUSH_CR3 = dso_local constant void (i64*)* @_Z8PUSH_CR3Rm, align 8
@SEM_PUSH_IMM_64 = dso_local constant void (i64*, i64)* @_Z8PUSH_IMMImEvRmT_, align 8
@SEM_PUSH_IMM_32 = dso_local constant void (i64*, i32)* @_Z8PUSH_IMMIjEvRmT_, align 8
@SEM_PUSH_IMM_16 = dso_local constant void (i64*, i16)* @_Z8PUSH_IMMItEvRmT_, align 8
@SEM_PUSH_VSP_64 = dso_local constant void (i64*)* @_Z8PUSH_VSPILm64EEvRm, align 8
@SEM_PUSH_VSP_32 = dso_local constant void (i64*)* @_Z8PUSH_VSPILm32EEvRm, align 8
@SEM_PUSH_VSP_16 = dso_local constant void (i64*)* @_Z8PUSH_VSPILm16EEvRm, align 8
@SEM_POP_VSP_64 = dso_local constant void (i64*)* @_Z7POP_VSPILm64EEvRm, align 8
@SEM_POP_VSP_32 = dso_local constant void (i64*)* @_Z7POP_VSPILm32EEvRm, align 8
@SEM_POP_VSP_16 = dso_local constant void (i64*)* @_Z7POP_VSPILm16EEvRm, align 8
@SEM_POP_FLAGS = dso_local constant void (i64*, i64*)* @_Z9POP_FLAGSRmS_, align 8
@SEM_LOAD_SS_64 = dso_local constant void (i64*)* @_Z4LOADImEvRm, align 8
@SEM_LOAD_SS_32 = dso_local constant void (i64*)* @_Z4LOADIjEvRm, align 8
@SEM_LOAD_SS_16 = dso_local constant void (i64*)* @_Z4LOADItEvRm, align 8
@SEM_LOAD_SS_8 = dso_local constant void (i64*)* @_Z4LOADIhEvRm, align 8
@SEM_LOAD_DS_64 = dso_local constant void (i64*)* @_Z4LOADImEvRm, align 8
@SEM_LOAD_DS_32 = dso_local constant void (i64*)* @_Z4LOADIjEvRm, align 8
@SEM_LOAD_DS_16 = dso_local constant void (i64*)* @_Z4LOADItEvRm, align 8
@SEM_LOAD_DS_8 = dso_local constant void (i64*)* @_Z4LOADIhEvRm, align 8
@SEM_LOAD_GS_64 = dso_local constant void (i64*)* @_Z7LOAD_GSImEvRm, align 8
@SEM_LOAD_GS_32 = dso_local constant void (i64*)* @_Z7LOAD_GSIjEvRm, align 8
@SEM_LOAD_GS_16 = dso_local constant void (i64*)* @_Z7LOAD_GSItEvRm, align 8
@SEM_LOAD_GS_8 = dso_local constant void (i64*)* @_Z7LOAD_GSIhEvRm, align 8
@SEM_LOAD_FS_64 = dso_local constant void (i64*)* @_Z7LOAD_FSImEvRm, align 8
@SEM_LOAD_FS_32 = dso_local constant void (i64*)* @_Z7LOAD_FSIjEvRm, align 8
@SEM_LOAD_FS_16 = dso_local constant void (i64*)* @_Z7LOAD_FSItEvRm, align 8
@SEM_LOAD_FS_8 = dso_local constant void (i64*)* @_Z7LOAD_FSIhEvRm, align 8
@SEM_STORE_SS_64 = dso_local constant void (i64*)* @_Z5STOREImEvRm, align 8
@SEM_STORE_SS_32 = dso_local constant void (i64*)* @_Z5STOREIjEvRm, align 8
@SEM_STORE_SS_16 = dso_local constant void (i64*)* @_Z5STOREItEvRm, align 8
@SEM_STORE_SS_8 = dso_local constant void (i64*)* @_Z5STOREIhEvRm, align 8
@SEM_STORE_DS_64 = dso_local constant void (i64*)* @_Z5STOREImEvRm, align 8
@SEM_STORE_DS_32 = dso_local constant void (i64*)* @_Z5STOREIjEvRm, align 8
@SEM_STORE_DS_16 = dso_local constant void (i64*)* @_Z5STOREItEvRm, align 8
@SEM_STORE_DS_8 = dso_local constant void (i64*)* @_Z5STOREIhEvRm, align 8
@SEM_PUSH_VMREG_8_LOW = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm8ELm0EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_8_HIGH = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm8ELm1EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_16_LOWLOW = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm16ELm0EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_16_LOWHIGH = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm16ELm1EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_16_HIGHLOW = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm16ELm2EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_16_HIGHHIGH = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm16ELm3EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_32_LOW = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm32ELm0EEvRm15VirtualRegister, align 8
@SEM_UNDEF_PUSH_VMREG_32 = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm32ELm0EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_32_HIGH = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm32ELm1EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_64 = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm64ELm0EEvRm15VirtualRegister, align 8
@SEM_POP_VMREG_8_LOW = dso_local constant void (i64*, %struct.VirtualRegister*)* @_Z9POP_VMREGILm8ELm0EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_8_HIGH = dso_local constant void (i64*, %struct.VirtualRegister*)* @_Z9POP_VMREGILm8ELm1EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_16_LOWLOW = dso_local constant void (i64*, %struct.VirtualRegister*)* @_Z9POP_VMREGILm16ELm0EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_16_LOWHIGH = dso_local constant void (i64*, %struct.VirtualRegister*)* @_Z9POP_VMREGILm16ELm1EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_16_HIGHLOW = dso_local constant void (i64*, %struct.VirtualRegister*)* @_Z9POP_VMREGILm16ELm2EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_16_HIGHHIGH = dso_local constant void (i64*, %struct.VirtualRegister*)* @_Z9POP_VMREGILm16ELm3EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_32_LOW = dso_local constant void (i64*, %struct.VirtualRegister*)* @_Z9POP_VMREGILm32ELm0EEvRmR15VirtualRegister, align 8
@SEM_UNDEF_POP_VMREG_32 = dso_local constant void (i64*, %struct.VirtualRegister*)* @_Z9POP_VMREGILm32ELm0EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_32_HIGH = dso_local constant void (i64*, %struct.VirtualRegister*)* @_Z9POP_VMREGILm32ELm1EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_64 = dso_local constant void (i64*, %struct.VirtualRegister*)* @_Z9POP_VMREGILm64ELm0EEvRmR15VirtualRegister, align 8
@SEM_PUSH_REG_64 = dso_local constant void (i64*, i64)* @_Z8PUSH_REGRmm, align 8
@SEM_UNDEF_PUSH_REG_32 = dso_local constant void (i64*, i64)* @_Z8PUSH_REGRmm, align 8
@SEM_POP_REG_64 = dso_local constant void (i64*, i64*)* @_Z7POP_REGRmS_, align 8
@SEM_UNDEF_POP_REG_32 = dso_local constant void (i64*, i64*)* @_Z7POP_REGRmS_, align 8
@SEM_POP_VOID_64 = dso_local constant void (i64*)* @_Z8POP_VOIDRm, align 8
@SEM_UNDEF_POP_VOID_32 = dso_local constant void (i64*)* @_Z8POP_VOIDRm, align 8
@SEM_MOVE_VMREG_SLOT = dso_local constant void (%struct.VirtualRegister*, i64*)* @_Z15MOVE_VMREG_SLOTR15VirtualRegisterRm, align 8
@SEM_UNDEF_MOVE_VMREG_SLOT = dso_local constant void (%struct.VirtualRegister*, i64*)* @_Z15MOVE_VMREG_SLOTR15VirtualRegisterRm, align 8
@SEM_POP_SLOT = dso_local constant void (i64*, i64*)* @_Z8POP_SLOTRmS_, align 8
@SEM_UNDEF_POP_SLOT = dso_local constant void (i64*, i64*)* @_Z8POP_SLOTRmS_, align 8
@SEM_CPUID = dso_local constant void (i64*)* @_Z5CPUIDRm, align 8
@SEM_RDTSC = dso_local constant void (i64*)* @_Z5RDTSCRm, align 8
@SEM_ADD_64 = dso_local constant void (i64*)* @_Z3ADDImEvRm, align 8
@SEM_ADD_32 = dso_local constant void (i64*)* @_Z3ADDIjEvRm, align 8
@SEM_ADD_16 = dso_local constant void (i64*)* @_Z3ADDItEvRm, align 8
@SEM_ADD_8 = dso_local constant void (i64*)* @_Z3ADDIhEvRm, align 8
@SEM_DIV_64 = dso_local constant void (i64*)* @_Z3DIVImEvRm, align 8
@SEM_DIV_32 = dso_local constant void (i64*)* @_Z3DIVIjEvRm, align 8
@SEM_DIV_16 = dso_local constant void (i64*)* @_Z3DIVItEvRm, align 8
@SEM_DIV_8 = dso_local constant void (i64*)* @_Z3DIVIhEvRm, align 8
@SEM_IDIV_64 = dso_local constant void (i64*)* @_Z4IDIVImEvRm, align 8
@SEM_IDIV_32 = dso_local constant void (i64*)* @_Z4IDIVIjEvRm, align 8
@SEM_IDIV_16 = dso_local constant void (i64*)* @_Z4IDIVItEvRm, align 8
@SEM_IDIV_8 = dso_local constant void (i64*)* @_Z4IDIVIhEvRm, align 8
@SEM_MUL_64 = dso_local constant void (i64*)* @_Z3MULImEvRm, align 8
@SEM_MUL_32 = dso_local constant void (i64*)* @_Z3MULIjEvRm, align 8
@SEM_MUL_16 = dso_local constant void (i64*)* @_Z3MULItEvRm, align 8
@SEM_MUL_8 = dso_local constant void (i64*)* @_Z3MULIhEvRm, align 8
@SEM_IMUL_64 = dso_local constant void (i64*)* @_Z4IMULImEvRm, align 8
@SEM_IMUL_32 = dso_local constant void (i64*)* @_Z4IMULIjEvRm, align 8
@SEM_IMUL_16 = dso_local constant void (i64*)* @_Z4IMULItEvRm, align 8
@SEM_IMUL_8 = dso_local constant void (i64*)* @_Z4IMULIhEvRm, align 8
@SEM_NOR_64 = dso_local constant void (i64*)* @_Z3NORImEvRm, align 8
@SEM_NOR_32 = dso_local constant void (i64*)* @_Z3NORIjEvRm, align 8
@SEM_NOR_16 = dso_local constant void (i64*)* @_Z3NORItEvRm, align 8
@SEM_NOR_8 = dso_local constant void (i64*)* @_Z3NORIhEvRm, align 8
@SEM_NAND_64 = dso_local constant void (i64*)* @_Z4NANDImEvRm, align 8
@SEM_NAND_32 = dso_local constant void (i64*)* @_Z4NANDIjEvRm, align 8
@SEM_NAND_16 = dso_local constant void (i64*)* @_Z4NANDItEvRm, align 8
@SEM_NAND_8 = dso_local constant void (i64*)* @_Z4NANDIhEvRm, align 8
@SEM_SHL_64 = dso_local constant void (i64*)* @_Z3SHLImEvRm, align 8
@SEM_SHL_32 = dso_local constant void (i64*)* @_Z3SHLIjEvRm, align 8
@SEM_SHL_16 = dso_local constant void (i64*)* @_Z3SHLItEvRm, align 8
@SEM_SHL_8 = dso_local constant void (i64*)* @_Z3SHLIhEvRm, align 8
@SEM_SHR_64 = dso_local constant void (i64*)* @_Z3SHRImEvRm, align 8
@SEM_SHR_32 = dso_local constant void (i64*)* @_Z3SHRIjEvRm, align 8
@SEM_SHR_16 = dso_local constant void (i64*)* @_Z3SHRItEvRm, align 8
@SEM_SHR_8 = dso_local constant void (i64*)* @_Z3SHRIhEvRm, align 8
@SEM_SHLD_64 = dso_local constant void (i64*)* @_Z4SHLDImEvRm, align 8
@SEM_SHLD_32 = dso_local constant void (i64*)* @_Z4SHLDIjEvRm, align 8
@SEM_SHLD_16 = dso_local constant void (i64*)* @_Z4SHLDItEvRm, align 8
@SEM_SHLD_8 = dso_local constant void (i64*)* @_Z4SHLDIhEvRm, align 8
@SEM_SHRD_64 = dso_local constant void (i64*)* @_Z4SHRDImEvRm, align 8
@SEM_SHRD_32 = dso_local constant void (i64*)* @_Z4SHRDIjEvRm, align 8
@SEM_SHRD_16 = dso_local constant void (i64*)* @_Z4SHRDItEvRm, align 8
@SEM_SHRD_8 = dso_local constant void (i64*)* @_Z4SHRDIhEvRm, align 8
@SEM_JUMP_INC = dso_local constant void (i64*, i64*)* @_Z8JUMP_INCRmS_, align 8
@SEM_JUMP_DEC = dso_local constant void (i64*, i64*)* @_Z8JUMP_DECRmS_, align 8
@SEM_JUMP = dso_local constant void (i64*, i64*)* @_Z4JUMPRmS_, align 8
@SEM_EXIT = dso_local constant void (i64*, i64*)* @_Z4JUMPRmS_, align 8
@RAM = external dso_local local_unnamed_addr global [0 x i8], align 1
@GS = external dso_local local_unnamed_addr global [0 x i8], align 1
@FS = external dso_local local_unnamed_addr global [0 x i8], align 1
@__undef = external dso_local local_unnamed_addr global i64, align 8
@llvm.compiler.used = appending global [116 x i8*] [i8* bitcast (void (i64*)** @SEM_ADD_16 to i8*), i8* bitcast (void (i64*)** @SEM_ADD_32 to i8*), i8* bitcast (void (i64*)** @SEM_ADD_64 to i8*), i8* bitcast (void (i64*)** @SEM_ADD_8 to i8*), i8* bitcast (void (i64*)** @SEM_CPUID to i8*), i8* bitcast (void (i64*)** @SEM_DIV_16 to i8*), i8* bitcast (void (i64*)** @SEM_DIV_32 to i8*), i8* bitcast (void (i64*)** @SEM_DIV_64 to i8*), i8* bitcast (void (i64*)** @SEM_DIV_8 to i8*), i8* bitcast (void (i64*, i64*)** @SEM_EXIT to i8*), i8* bitcast (void (i64*)** @SEM_IDIV_16 to i8*), i8* bitcast (void (i64*)** @SEM_IDIV_32 to i8*), i8* bitcast (void (i64*)** @SEM_IDIV_64 to i8*), i8* bitcast (void (i64*)** @SEM_IDIV_8 to i8*), i8* bitcast (void (i64*)** @SEM_IMUL_16 to i8*), i8* bitcast (void (i64*)** @SEM_IMUL_32 to i8*), i8* bitcast (void (i64*)** @SEM_IMUL_64 to i8*), i8* bitcast (void (i64*)** @SEM_IMUL_8 to i8*), i8* bitcast (void (i64*, i64*)** @SEM_JUMP to i8*), i8* bitcast (void (i64*, i64*)** @SEM_JUMP_DEC to i8*), i8* bitcast (void (i64*, i64*)** @SEM_JUMP_INC to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_DS_16 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_DS_32 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_DS_64 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_DS_8 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_FS_16 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_FS_32 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_FS_64 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_FS_8 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_GS_16 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_GS_32 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_GS_64 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_GS_8 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_SS_16 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_SS_32 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_SS_64 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_SS_8 to i8*), i8* bitcast (void (%struct.VirtualRegister*, i64*)** @SEM_MOVE_VMREG_SLOT to i8*), i8* bitcast (void (i64*)** @SEM_MUL_16 to i8*), i8* bitcast (void (i64*)** @SEM_MUL_32 to i8*), i8* bitcast (void (i64*)** @SEM_MUL_64 to i8*), i8* bitcast (void (i64*)** @SEM_MUL_8 to i8*), i8* bitcast (void (i64*)** @SEM_NAND_16 to i8*), i8* bitcast (void (i64*)** @SEM_NAND_32 to i8*), i8* bitcast (void (i64*)** @SEM_NAND_64 to i8*), i8* bitcast (void (i64*)** @SEM_NAND_8 to i8*), i8* bitcast (void (i64*)** @SEM_NOR_16 to i8*), i8* bitcast (void (i64*)** @SEM_NOR_32 to i8*), i8* bitcast (void (i64*)** @SEM_NOR_64 to i8*), i8* bitcast (void (i64*)** @SEM_NOR_8 to i8*), i8* bitcast (void (i64*, i64*)** @SEM_POP_FLAGS to i8*), i8* bitcast (void (i64*, i64*)** @SEM_POP_REG_64 to i8*), i8* bitcast (void (i64*, i64*)** @SEM_POP_SLOT to i8*), i8* bitcast (void (i64*, %struct.VirtualRegister*)** @SEM_POP_VMREG_16_HIGHHIGH to i8*), i8* bitcast (void (i64*, %struct.VirtualRegister*)** @SEM_POP_VMREG_16_HIGHLOW to i8*), i8* bitcast (void (i64*, %struct.VirtualRegister*)** @SEM_POP_VMREG_16_LOWHIGH to i8*), i8* bitcast (void (i64*, %struct.VirtualRegister*)** @SEM_POP_VMREG_16_LOWLOW to i8*), i8* bitcast (void (i64*, %struct.VirtualRegister*)** @SEM_POP_VMREG_32_HIGH to i8*), i8* bitcast (void (i64*, %struct.VirtualRegister*)** @SEM_POP_VMREG_32_LOW to i8*), i8* bitcast (void (i64*, %struct.VirtualRegister*)** @SEM_POP_VMREG_64 to i8*), i8* bitcast (void (i64*, %struct.VirtualRegister*)** @SEM_POP_VMREG_8_HIGH to i8*), i8* bitcast (void (i64*, %struct.VirtualRegister*)** @SEM_POP_VMREG_8_LOW to i8*), i8* bitcast (void (i64*)** @SEM_POP_VOID_64 to i8*), i8* bitcast (void (i64*)** @SEM_POP_VSP_16 to i8*), i8* bitcast (void (i64*)** @SEM_POP_VSP_32 to i8*), i8* bitcast (void (i64*)** @SEM_POP_VSP_64 to i8*), i8* bitcast (void (i64*)** @SEM_PUSH_CR0 to i8*), i8* bitcast (void (i64*)** @SEM_PUSH_CR3 to i8*), i8* bitcast (void (i64*, i16)** @SEM_PUSH_IMM_16 to i8*), i8* bitcast (void (i64*, i32)** @SEM_PUSH_IMM_32 to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_IMM_64 to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_REG_64 to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_16_HIGHHIGH to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_16_HIGHLOW to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_16_LOWHIGH to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_16_LOWLOW to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_32_HIGH to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_32_LOW to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_64 to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_8_HIGH to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_8_LOW to i8*), i8* bitcast (void (i64*)** @SEM_PUSH_VSP_16 to i8*), i8* bitcast (void (i64*)** @SEM_PUSH_VSP_32 to i8*), i8* bitcast (void (i64*)** @SEM_PUSH_VSP_64 to i8*), i8* bitcast (void (i64*)** @SEM_RDTSC to i8*), i8* bitcast (void (i64*)** @SEM_SHLD_16 to i8*), i8* bitcast (void (i64*)** @SEM_SHLD_32 to i8*), i8* bitcast (void (i64*)** @SEM_SHLD_64 to i8*), i8* bitcast (void (i64*)** @SEM_SHLD_8 to i8*), i8* bitcast (void (i64*)** @SEM_SHL_16 to i8*), i8* bitcast (void (i64*)** @SEM_SHL_32 to i8*), i8* bitcast (void (i64*)** @SEM_SHL_64 to i8*), i8* bitcast (void (i64*)** @SEM_SHL_8 to i8*), i8* bitcast (void (i64*)** @SEM_SHRD_16 to i8*), i8* bitcast (void (i64*)** @SEM_SHRD_32 to i8*), i8* bitcast (void (i64*)** @SEM_SHRD_64 to i8*), i8* bitcast (void (i64*)** @SEM_SHRD_8 to i8*), i8* bitcast (void (i64*)** @SEM_SHR_16 to i8*), i8* bitcast (void (i64*)** @SEM_SHR_32 to i8*), i8* bitcast (void (i64*)** @SEM_SHR_64 to i8*), i8* bitcast (void (i64*)** @SEM_SHR_8 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_DS_16 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_DS_32 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_DS_64 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_DS_8 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_SS_16 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_SS_32 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_SS_64 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_SS_8 to i8*), i8* bitcast (void (%struct.VirtualRegister*, i64*)** @SEM_UNDEF_MOVE_VMREG_SLOT to i8*), i8* bitcast (void (i64*, i64*)** @SEM_UNDEF_POP_REG_32 to i8*), i8* bitcast (void (i64*, i64*)** @SEM_UNDEF_POP_SLOT to i8*), i8* bitcast (void (i64*, %struct.VirtualRegister*)** @SEM_UNDEF_POP_VMREG_32 to i8*), i8* bitcast (void (i64*)** @SEM_UNDEF_POP_VOID_32 to i8*), i8* bitcast (void (i64*, i64)** @SEM_UNDEF_PUSH_REG_32 to i8*), i8* bitcast (void (i64*, i64)** @SEM_UNDEF_PUSH_VMREG_32 to i8*)], section "llvm.metadata"

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define dso_local void @_Z8PUSH_CR0Rm(i64* nocapture nonnull align 8 dereferenceable(8) %vsp) #0 {
entry:
  %0 = tail call i64 asm "mov %cr0, $0", "=r,~{dirflag},~{fpsr},~{flags}"() #11, !srcloc !3
  %1 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i = add i64 %1, -8
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  store i64 %0, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: argmemonly mustprogress nofree nosync nounwind willreturn
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: argmemonly mustprogress nofree nosync nounwind willreturn
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #1

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define dso_local void @_Z8PUSH_CR3Rm(i64* nocapture nonnull align 8 dereferenceable(8) %vsp) #0 {
entry:
  %0 = tail call i64 asm "mov %cr3, $0", "=r,~{dirflag},~{fpsr},~{flags}"() #11, !srcloc !8
  %1 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i = add i64 %1, -8
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  store i64 %0, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z8PUSH_IMMImEvRmT_(i64* nonnull align 8 dereferenceable(8) %vsp, i64 %value) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i = add i64 %0, -8
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  store i64 %value, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z8PUSH_IMMIjEvRmT_(i64* nonnull align 8 dereferenceable(8) %vsp, i32 %value) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i = add i64 %0, -4
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  store i32 %value, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z8PUSH_IMMItEvRmT_(i64* nonnull align 8 dereferenceable(8) %vsp, i16 zeroext %value) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i = add i64 %0, -2
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  store i16 %value, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z8PUSH_VSPILm64EEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i = add i64 %0, -8
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  store i64 %0, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z8PUSH_VSPILm32EEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %conv = trunc i64 %0 to i32
  %sub.i = add i64 %0, -4
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  store i32 %conv, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z8PUSH_VSPILm16EEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %conv = trunc i64 %0 to i16
  %sub.i = add i64 %0, -2
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  store i16 %conv, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7POP_VSPILm64EEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  store i64 %value.0.copyload.i, i64* %vsp, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7POP_VSPILm32EEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  %value.0.copyload.i = load i32, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i32
  store i32 %conv.i.i, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 4
  %and = and i64 %add.i, -4294967296
  %conv = zext i32 %value.0.copyload.i to i64
  %or = or i64 %and, %conv
  store i64 %or, i64* %vsp, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7POP_VSPILm16EEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  %and = and i64 %add.i, -65536
  %conv = zext i16 %value.0.copyload.i to i64
  %or = or i64 %and, %conv
  store i64 %or, i64* %vsp, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z9POP_FLAGSRmS_(i64* nocapture nonnull align 8 dereferenceable(8) %vsp, i64* nocapture nonnull align 8 dereferenceable(8) %flags) #2 {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  store i64 %value.0.copyload.i, i64* %flags, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4LOADImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %value.0.copyload.i
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i64*
  %value.0.copyload = load i64, i64* %value.0.arrayidx.sroa_cast, align 1
  store i64 %2, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i7 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %2
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i7 to i64*
  store i64 %value.0.copyload, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4LOADIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %value.0.copyload.i
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i32*
  %value.0.copyload = load i32, i32* %value.0.arrayidx.sroa_cast, align 1
  %sub.i = add i64 %2, 4
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i7 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i7 to i32*
  store i32 %value.0.copyload, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4LOADItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %value.0.copyload.i
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  %value.0.copyload = load i16, i16* %value.0.arrayidx.sroa_cast, align 1
  %sub.i = add i64 %2, 6
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i7 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i7 to i16*
  store i16 %value.0.copyload, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4LOADIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %value.0.copyload.i
  %value.0.copyload = load i8, i8* %arrayidx, align 1
  %conv.i = zext i8 %value.0.copyload to i16
  %sub.i = add i64 %2, 6
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i6 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i6 to i16*
  store i16 %conv.i, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_GSImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @GS, i64 0, i64 %value.0.copyload.i
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i64*
  %value.0.copyload = load i64, i64* %value.0.arrayidx.sroa_cast, align 1
  store i64 %2, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i7 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %2
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i7 to i64*
  store i64 %value.0.copyload, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_GSIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @GS, i64 0, i64 %value.0.copyload.i
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i32*
  %value.0.copyload = load i32, i32* %value.0.arrayidx.sroa_cast, align 1
  %sub.i = add i64 %2, 4
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i7 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i7 to i32*
  store i32 %value.0.copyload, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_GSItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @GS, i64 0, i64 %value.0.copyload.i
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  %value.0.copyload = load i16, i16* %value.0.arrayidx.sroa_cast, align 1
  %sub.i = add i64 %2, 6
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i7 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i7 to i16*
  store i16 %value.0.copyload, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_GSIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @GS, i64 0, i64 %value.0.copyload.i
  %value.0.copyload = load i8, i8* %arrayidx, align 1
  %conv.i = zext i8 %value.0.copyload to i16
  %sub.i = add i64 %2, 6
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i6 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i6 to i16*
  store i16 %conv.i, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_FSImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @FS, i64 0, i64 %value.0.copyload.i
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i64*
  %value.0.copyload = load i64, i64* %value.0.arrayidx.sroa_cast, align 1
  store i64 %2, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i7 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %2
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i7 to i64*
  store i64 %value.0.copyload, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_FSIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @FS, i64 0, i64 %value.0.copyload.i
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i32*
  %value.0.copyload = load i32, i32* %value.0.arrayidx.sroa_cast, align 1
  %sub.i = add i64 %2, 4
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i7 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i7 to i32*
  store i32 %value.0.copyload, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_FSItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @FS, i64 0, i64 %value.0.copyload.i
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  %value.0.copyload = load i16, i16* %value.0.arrayidx.sroa_cast, align 1
  %sub.i = add i64 %2, 6
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i7 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i7 to i16*
  store i16 %value.0.copyload, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_FSIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @FS, i64 0, i64 %value.0.copyload.i
  %value.0.copyload = load i8, i8* %arrayidx, align 1
  %conv.i = zext i8 %value.0.copyload to i16
  %sub.i = add i64 %2, 6
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i6 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i6 to i16*
  store i16 %conv.i, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z5STOREImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i8 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i9 = bitcast i8* %arrayidx.i8 to i64*
  %value.0.copyload.i10 = load i64, i64* %value.0.arrayidx.sroa_cast.i9, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %3, i64* %value.0.arrayidx.sroa_cast.i9, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i11 = add i64 %4, 8
  store i64 %add.i11, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %value.0.copyload.i
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i64*
  store i64 %value.0.copyload.i10, i64* %value.0.arrayidx.sroa_cast, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z5STOREIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i8 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i9 = bitcast i8* %arrayidx.i8 to i32*
  %value.0.copyload.i10 = load i32, i32* %value.0.arrayidx.sroa_cast.i9, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %3 to i32
  store i32 %conv.i.i, i32* %value.0.arrayidx.sroa_cast.i9, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i11 = add i64 %4, 4
  store i64 %add.i11, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %value.0.copyload.i
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i32*
  store i32 %value.0.copyload.i10, i32* %value.0.arrayidx.sroa_cast, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z5STOREItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i8 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i9 = bitcast i8* %arrayidx.i8 to i16*
  %value.0.copyload.i10 = load i16, i16* %value.0.arrayidx.sroa_cast.i9, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %3 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i9, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i11 = add i64 %4, 2
  store i64 %add.i11, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %value.0.copyload.i
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  store i16 %value.0.copyload.i10, i16* %value.0.arrayidx.sroa_cast, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z5STOREIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i7 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i8 = bitcast i8* %arrayidx.i7 to i16*
  %value.0.copyload.i9 = load i16, i16* %value.0.arrayidx.sroa_cast.i8, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %3 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i8, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i10 = add i64 %4, 2
  store i64 %add.i10, i64* %vsp, align 8, !tbaa !4
  %conv.i = trunc i16 %value.0.copyload.i9 to i8
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %value.0.copyload.i
  store i8 %conv.i, i8* %arrayidx, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm8ELm0EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, i64 %vmreg.coerce) #0 comdat {
entry:
  %vmreg.sroa.0.0.extract.trunc = trunc i64 %vmreg.coerce to i16
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub = add i64 %0, -2
  store i64 %sub, i64* %vsp, align 8, !tbaa !4
  %conv.i = and i16 %vmreg.sroa.0.0.extract.trunc, 255
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub
  %byte.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  store i16 %conv.i, i16* %byte.0.arrayidx.sroa_cast, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm8ELm1EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, i64 %vmreg.coerce) #0 comdat {
entry:
  %vmreg.sroa.1.0.extract.shift = lshr i64 %vmreg.coerce, 8
  %vmreg.sroa.1.0.extract.trunc = trunc i64 %vmreg.sroa.1.0.extract.shift to i16
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub = add i64 %0, -2
  store i64 %sub, i64* %vsp, align 8, !tbaa !4
  %conv.i = and i16 %vmreg.sroa.1.0.extract.trunc, 255
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub
  %byte.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  store i16 %conv.i, i16* %byte.0.arrayidx.sroa_cast, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm16ELm0EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, i64 %vmreg.coerce) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub = add i64 %0, -2
  store i64 %sub, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub
  %vmreg.sroa.0.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  %vmreg.sroa.0.0.extract.trunc = trunc i64 %vmreg.coerce to i16
  store i16 %vmreg.sroa.0.0.extract.trunc, i16* %vmreg.sroa.0.0.arrayidx.sroa_cast, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm16ELm1EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, i64 %vmreg.coerce) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub = add i64 %0, -2
  store i64 %sub, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub
  %vmreg.sroa.0.2.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  %vmreg.sroa.0.2.extract.shift = lshr i64 %vmreg.coerce, 16
  %vmreg.sroa.0.2.extract.trunc = trunc i64 %vmreg.sroa.0.2.extract.shift to i16
  store i16 %vmreg.sroa.0.2.extract.trunc, i16* %vmreg.sroa.0.2.arrayidx.sroa_cast, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm16ELm2EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, i64 %vmreg.coerce) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub = add i64 %0, -2
  store i64 %sub, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub
  %vmreg.sroa.0.4.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  %vmreg.sroa.0.4.extract.shift = lshr i64 %vmreg.coerce, 32
  %vmreg.sroa.0.4.extract.trunc = trunc i64 %vmreg.sroa.0.4.extract.shift to i16
  store i16 %vmreg.sroa.0.4.extract.trunc, i16* %vmreg.sroa.0.4.arrayidx.sroa_cast, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm16ELm3EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, i64 %vmreg.coerce) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub = add i64 %0, -2
  store i64 %sub, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub
  %vmreg.sroa.0.6.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  %vmreg.sroa.0.6.extract.shift = lshr i64 %vmreg.coerce, 48
  %vmreg.sroa.0.6.extract.trunc = trunc i64 %vmreg.sroa.0.6.extract.shift to i16
  store i16 %vmreg.sroa.0.6.extract.trunc, i16* %vmreg.sroa.0.6.arrayidx.sroa_cast, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm32ELm0EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, i64 %vmreg.coerce) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub = add i64 %0, -4
  store i64 %sub, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub
  %vmreg.sroa.0.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i32*
  %vmreg.sroa.0.0.extract.trunc = trunc i64 %vmreg.coerce to i32
  store i32 %vmreg.sroa.0.0.extract.trunc, i32* %vmreg.sroa.0.0.arrayidx.sroa_cast, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm32ELm1EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, i64 %vmreg.coerce) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub = add i64 %0, -4
  store i64 %sub, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub
  %vmreg.sroa.0.4.arrayidx.sroa_cast = bitcast i8* %arrayidx to i32*
  %vmreg.sroa.0.4.extract.shift = lshr i64 %vmreg.coerce, 32
  %vmreg.sroa.0.4.extract.trunc = trunc i64 %vmreg.sroa.0.4.extract.shift to i32
  store i32 %vmreg.sroa.0.4.extract.trunc, i32* %vmreg.sroa.0.4.arrayidx.sroa_cast, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm64ELm0EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, i64 %vmreg.coerce) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub = add i64 %0, -8
  store i64 %sub, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub
  %vmreg.sroa.0.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i64*
  store i64 %vmreg.coerce, i64* %vmreg.sroa.0.0.arrayidx.sroa_cast, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm8ELm0EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, %struct.VirtualRegister* nonnull align 1 dereferenceable(8) %vmreg) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %byte.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  %byte.0.copyload = load i16, i16* %byte.0.arrayidx.sroa_cast, align 1
  %conv.i = trunc i16 %byte.0.copyload to i8
  %b0 = bitcast %struct.VirtualRegister* %vmreg to i8*
  store i8 %conv.i, i8* %b0, align 1, !tbaa !9
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i8 = trunc i64 %1 to i16
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx3 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %2
  %undef.0.arrayidx3.sroa_cast = bitcast i8* %arrayidx3 to i16*
  store i16 %conv.i8, i16* %undef.0.arrayidx3.sroa_cast, align 1
  %3 = load i64, i64* %vsp, align 8, !tbaa !4
  %add = add i64 %3, 2
  store i64 %add, i64* %vsp, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm8ELm1EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, %struct.VirtualRegister* nonnull align 1 dereferenceable(8) %vmreg) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %byte.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  %byte.0.copyload = load i16, i16* %byte.0.arrayidx.sroa_cast, align 1
  %conv.i = trunc i16 %byte.0.copyload to i8
  %byte1 = bitcast %struct.VirtualRegister* %vmreg to %struct.anon*
  %b1 = getelementptr inbounds %struct.anon, %struct.anon* %byte1, i64 0, i32 1
  store i8 %conv.i, i8* %b1, align 1, !tbaa !9
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i8 = trunc i64 %1 to i16
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx3 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %2
  %undef.0.arrayidx3.sroa_cast = bitcast i8* %arrayidx3 to i16*
  store i16 %conv.i8, i16* %undef.0.arrayidx3.sroa_cast, align 1
  %3 = load i64, i64* %vsp, align 8, !tbaa !4
  %add = add i64 %3, 2
  store i64 %add, i64* %vsp, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm16ELm0EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, %struct.VirtualRegister* nonnull align 1 dereferenceable(8) %vmreg) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  %value.0.copyload = load i16, i16* %value.0.arrayidx.sroa_cast, align 1
  %qword = getelementptr inbounds %struct.VirtualRegister, %struct.VirtualRegister* %vmreg, i64 0, i32 0, i32 0
  %1 = load i64, i64* %qword, align 1, !tbaa !9
  %and = and i64 %1, -65536
  %conv = zext i16 %value.0.copyload to i64
  %or = or i64 %and, %conv
  store i64 %or, i64* %qword, align 1, !tbaa !9
  %2 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i = trunc i64 %2 to i16
  %3 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx2 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %3
  %undef.0.arrayidx2.sroa_cast = bitcast i8* %arrayidx2 to i16*
  store i16 %conv.i, i16* %undef.0.arrayidx2.sroa_cast, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add = add i64 %4, 2
  store i64 %add, i64* %vsp, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm16ELm1EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, %struct.VirtualRegister* nonnull align 1 dereferenceable(8) %vmreg) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  %value.0.copyload = load i16, i16* %value.0.arrayidx.sroa_cast, align 1
  %qword = getelementptr inbounds %struct.VirtualRegister, %struct.VirtualRegister* %vmreg, i64 0, i32 0, i32 0
  %1 = load i64, i64* %qword, align 1, !tbaa !9
  %and = and i64 %1, -4294901761
  %conv.i = zext i16 %value.0.copyload to i64
  %shl.i = shl nuw nsw i64 %conv.i, 16
  %or = or i64 %and, %shl.i
  store i64 %or, i64* %qword, align 1, !tbaa !9
  %2 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i10 = trunc i64 %2 to i16
  %3 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx4 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %3
  %undef.0.arrayidx4.sroa_cast = bitcast i8* %arrayidx4 to i16*
  store i16 %conv.i10, i16* %undef.0.arrayidx4.sroa_cast, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add = add i64 %4, 2
  store i64 %add, i64* %vsp, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm16ELm2EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, %struct.VirtualRegister* nonnull align 1 dereferenceable(8) %vmreg) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  %value.0.copyload = load i16, i16* %value.0.arrayidx.sroa_cast, align 1
  %qword = getelementptr inbounds %struct.VirtualRegister, %struct.VirtualRegister* %vmreg, i64 0, i32 0, i32 0
  %1 = load i64, i64* %qword, align 1, !tbaa !9
  %and = and i64 %1, -281470681743361
  %conv.i = zext i16 %value.0.copyload to i64
  %shl.i = shl nuw nsw i64 %conv.i, 32
  %or = or i64 %and, %shl.i
  store i64 %or, i64* %qword, align 1, !tbaa !9
  %2 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i10 = trunc i64 %2 to i16
  %3 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx4 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %3
  %undef.0.arrayidx4.sroa_cast = bitcast i8* %arrayidx4 to i16*
  store i16 %conv.i10, i16* %undef.0.arrayidx4.sroa_cast, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add = add i64 %4, 2
  store i64 %add, i64* %vsp, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm16ELm3EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, %struct.VirtualRegister* nonnull align 1 dereferenceable(8) %vmreg) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i16*
  %value.0.copyload = load i16, i16* %value.0.arrayidx.sroa_cast, align 1
  %qword = getelementptr inbounds %struct.VirtualRegister, %struct.VirtualRegister* %vmreg, i64 0, i32 0, i32 0
  %1 = load i64, i64* %qword, align 1, !tbaa !9
  %and = and i64 %1, 281474976710655
  %conv.i = zext i16 %value.0.copyload to i64
  %shl.i = shl nuw i64 %conv.i, 48
  %or = or i64 %and, %shl.i
  store i64 %or, i64* %qword, align 1, !tbaa !9
  %2 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i10 = trunc i64 %2 to i16
  %3 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx4 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %3
  %undef.0.arrayidx4.sroa_cast = bitcast i8* %arrayidx4 to i16*
  store i16 %conv.i10, i16* %undef.0.arrayidx4.sroa_cast, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add = add i64 %4, 2
  store i64 %add, i64* %vsp, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm32ELm0EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, %struct.VirtualRegister* nonnull align 1 dereferenceable(8) %vmreg) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i32*
  %value.0.copyload = load i32, i32* %value.0.arrayidx.sroa_cast, align 1
  %qword = getelementptr inbounds %struct.VirtualRegister, %struct.VirtualRegister* %vmreg, i64 0, i32 0, i32 0
  %1 = load i64, i64* %qword, align 1, !tbaa !9
  %and = and i64 %1, -4294967296
  %conv = zext i32 %value.0.copyload to i64
  %or = or i64 %and, %conv
  store i64 %or, i64* %qword, align 1, !tbaa !9
  %2 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i = trunc i64 %2 to i32
  %3 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx2 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %3
  %undef.0.arrayidx2.sroa_cast = bitcast i8* %arrayidx2 to i32*
  store i32 %conv.i, i32* %undef.0.arrayidx2.sroa_cast, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add = add i64 %4, 4
  store i64 %add, i64* %vsp, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm32ELm1EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, %struct.VirtualRegister* nonnull align 1 dereferenceable(8) %vmreg) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i32*
  %value.0.copyload = load i32, i32* %value.0.arrayidx.sroa_cast, align 1
  %qword = getelementptr inbounds %struct.VirtualRegister, %struct.VirtualRegister* %vmreg, i64 0, i32 0, i32 0
  %1 = load i64, i64* %qword, align 1, !tbaa !9
  %and = and i64 %1, 4294967295
  %conv.i = zext i32 %value.0.copyload to i64
  %shl.i = shl nuw i64 %conv.i, 32
  %or = or i64 %and, %shl.i
  store i64 %or, i64* %qword, align 1, !tbaa !9
  %2 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i10 = trunc i64 %2 to i32
  %3 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx4 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %3
  %undef.0.arrayidx4.sroa_cast = bitcast i8* %arrayidx4 to i32*
  store i32 %conv.i10, i32* %undef.0.arrayidx4.sroa_cast, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add = add i64 %4, 4
  store i64 %add, i64* %vsp, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm64ELm0EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %vsp, %struct.VirtualRegister* nonnull align 1 dereferenceable(8) %vmreg) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast = bitcast i8* %arrayidx to i64*
  %value.0.copyload = load i64, i64* %value.0.arrayidx.sroa_cast, align 1
  %qword = getelementptr inbounds %struct.VirtualRegister, %struct.VirtualRegister* %vmreg, i64 0, i32 0, i32 0
  store i64 %value.0.copyload, i64* %qword, align 1, !tbaa !9
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx1 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %2
  %undef.0.arrayidx1.sroa_cast = bitcast i8* %arrayidx1 to i64*
  store i64 %1, i64* %undef.0.arrayidx1.sroa_cast, align 1
  %3 = load i64, i64* %vsp, align 8, !tbaa !4
  %add = add i64 %3, 8
  store i64 %add, i64* %vsp, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z8PUSH_REGRmm(i64* nocapture nonnull align 8 dereferenceable(8) %vsp, i64 %reg) #2 {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i = add i64 %0, -8
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  store i64 %reg, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z7POP_REGRmS_(i64* nocapture nonnull align 8 dereferenceable(8) %vsp, i64* nocapture nonnull align 8 dereferenceable(8) %reg) #2 {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  store i64 %value.0.copyload.i, i64* %reg, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z8POP_VOIDRm(i64* nocapture nonnull align 8 dereferenceable(8) %vsp) #2 {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z15MOVE_VMREG_SLOTR15VirtualRegisterRm(%struct.VirtualRegister* nocapture nonnull align 1 dereferenceable(8) %vmreg, i64* nocapture nonnull readonly align 8 dereferenceable(8) %slot) #2 {
entry:
  %0 = load i64, i64* %slot, align 8, !tbaa !4
  %qword = getelementptr inbounds %struct.VirtualRegister, %struct.VirtualRegister* %vmreg, i64 0, i32 0, i32 0
  store i64 %0, i64* %qword, align 1, !tbaa !9
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z8POP_SLOTRmS_(i64* nocapture nonnull align 8 dereferenceable(8) %vsp, i64* nocapture nonnull align 8 dereferenceable(8) %slot) #2 {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  store i64 %value.0.copyload.i, i64* %slot, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define dso_local void @_Z5CPUIDRm(i64* nocapture nonnull align 8 dereferenceable(8) %vsp) #0 {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  %value.0.copyload.i = load i32, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i32
  store i32 %conv.i.i, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %3 = tail call { i32, i32, i32, i32 } asm "  xchgq  %rbx,${1:q}\0A  cpuid\0A  xchgq  %rbx,${1:q}", "={ax},=r,={cx},={dx},0,~{dirflag},~{fpsr},~{flags}"(i32 %value.0.copyload.i) #11, !srcloc !10
  %asmresult = extractvalue { i32, i32, i32, i32 } %3, 0
  %asmresult1 = extractvalue { i32, i32, i32, i32 } %3, 1
  %asmresult2 = extractvalue { i32, i32, i32, i32 } %3, 2
  %asmresult3 = extractvalue { i32, i32, i32, i32 } %3, 3
  %arrayidx.i13 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %2
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i13 to i32*
  store i32 %asmresult, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i14 = add i64 %4, -4
  store i64 %sub.i14, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i15 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i14
  %value.addr.0.arrayidx.sroa_cast.i16 = bitcast i8* %arrayidx.i15 to i32*
  store i32 %asmresult1, i32* %value.addr.0.arrayidx.sroa_cast.i16, align 1
  %5 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i17 = add i64 %5, -4
  store i64 %sub.i17, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i18 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i17
  %value.addr.0.arrayidx.sroa_cast.i19 = bitcast i8* %arrayidx.i18 to i32*
  store i32 %asmresult2, i32* %value.addr.0.arrayidx.sroa_cast.i19, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i20 = add i64 %6, -4
  store i64 %sub.i20, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i21 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i20
  %value.addr.0.arrayidx.sroa_cast.i22 = bitcast i8* %arrayidx.i21 to i32*
  store i32 %asmresult3, i32* %value.addr.0.arrayidx.sroa_cast.i22, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define dso_local void @_Z5RDTSCRm(i64* nocapture nonnull align 8 dereferenceable(8) %vsp) #0 {
entry:
  %0 = tail call i64 @llvm.x86.rdtsc()
  %conv = trunc i64 %0 to i32
  %shr.i = lshr i64 %0, 32
  %conv3 = trunc i64 %shr.i to i32
  %1 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i = add i64 %1, -4
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  store i32 %conv, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i11 = add i64 %2, -4
  store i64 %sub.i11, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i12 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i11
  %value.addr.0.arrayidx.sroa_cast.i13 = bitcast i8* %arrayidx.i12 to i32*
  store i32 %conv3, i32* %value.addr.0.arrayidx.sroa_cast.i13, align 1
  ret void
}

; Function Attrs: nounwind
declare i64 @llvm.x86.rdtsc() #3

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z13UPDATE_EFLAGSRmbbbbbb(i64* nocapture nonnull align 8 dereferenceable(8) %flags, i1 zeroext %cf, i1 zeroext %pf, i1 zeroext %af, i1 zeroext %zf, i1 zeroext %sf, i1 zeroext %of) local_unnamed_addr #2 {
entry:
  %0 = load i64, i64* %flags, align 8, !tbaa !4
  %conv = zext i1 %cf to i64
  %or6 = or i64 %0, %conv
  %shl10 = select i1 %pf, i64 4, i64 0
  %shl16 = select i1 %af, i64 16, i64 0
  %and13 = or i64 %shl16, %shl10
  %or17 = or i64 %and13, %or6
  %shl22 = select i1 %zf, i64 64, i64 0
  %shl28 = select i1 %sf, i64 128, i64 0
  %shl34 = select i1 %of, i64 2048, i64 0
  %and25 = or i64 %shl28, %shl22
  %or29 = or i64 %and25, %shl34
  %or30 = or i64 %or29, %or17
  store i64 %or30, i64* %flags, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3ADDImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i28 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i29 = bitcast i8* %arrayidx.i28 to i64*
  %value.0.copyload.i30 = load i64, i64* %value.0.arrayidx.sroa_cast.i29, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %3, i64* %value.0.arrayidx.sroa_cast.i29, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i32 = add i64 %value.0.copyload.i30, %value.0.copyload.i
  %cmp.i.i.i = icmp ult i64 %add.i32, %value.0.copyload.i
  %cmp1.i.i.i = icmp ult i64 %add.i32, %value.0.copyload.i30
  %5 = or i1 %cmp.i.i.i, %cmp1.i.i.i
  %conv.i.i.i = trunc i64 %add.i32 to i8
  %6 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i.i) #3, !range !11
  %xor.i.i.i = xor i64 %value.0.copyload.i30, %value.0.copyload.i
  %xor1.i.i.i = xor i64 %xor.i.i.i, %add.i32
  %and.i.i.i = and i64 %xor1.i.i.i, 16
  %cmp.i.i25.i = icmp eq i64 %add.i32, 0
  %shr.i.i.i = lshr i64 %value.0.copyload.i, 63
  %shr1.i.i.i = lshr i64 %value.0.copyload.i30, 63
  %shr2.i.i.i = lshr i64 %add.i32, 63
  %xor.i.i27.i = xor i64 %shr2.i.i.i, %shr.i.i.i
  %xor3.i.i.i = xor i64 %shr2.i.i.i, %shr1.i.i.i
  %add.i.i.i = add nuw nsw i64 %xor.i.i27.i, %xor3.i.i.i
  %cmp.i.i28.i = icmp eq i64 %add.i.i.i, 2
  %conv.i.i = zext i1 %5 to i64
  %7 = shl nuw nsw i8 %6, 2
  %8 = and i8 %7, 4
  %9 = xor i8 %8, 4
  %10 = zext i8 %9 to i64
  %shl22.i.i = select i1 %cmp.i.i25.i, i64 64, i64 0
  %11 = lshr i64 %add.i32, 56
  %12 = and i64 %11, 128
  %shl34.i.i = select i1 %cmp.i.i28.i, i64 2048, i64 0
  %or6.i.i = or i64 %12, %shl22.i.i
  %and13.i.i = or i64 %or6.i.i, %and.i.i.i
  %or17.i.i = or i64 %and13.i.i, %conv.i.i
  %and25.i.i = or i64 %or17.i.i, %shl34.i.i
  %or29.i.i = or i64 %and25.i.i, %10
  %arrayidx.i33 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i33 to i64*
  store i64 %add.i32, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %13 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i34 = add i64 %13, -8
  store i64 %sub.i34, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i35 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i34
  %value.addr.0.arrayidx.sroa_cast.i36 = bitcast i8* %arrayidx.i35 to i64*
  store i64 %or29.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i36, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3ADDIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  %value.0.copyload.i = load i32, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i32
  store i32 %conv.i.i, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 4
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i28 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i29 = bitcast i8* %arrayidx.i28 to i32*
  %value.0.copyload.i30 = load i32, i32* %value.0.arrayidx.sroa_cast.i29, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i31 = trunc i64 %3 to i32
  store i32 %conv.i.i31, i32* %value.0.arrayidx.sroa_cast.i29, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i33 = add i32 %value.0.copyload.i30, %value.0.copyload.i
  %cmp.i.i.i = icmp ult i32 %add.i33, %value.0.copyload.i
  %cmp1.i.i.i = icmp ult i32 %add.i33, %value.0.copyload.i30
  %5 = or i1 %cmp.i.i.i, %cmp1.i.i.i
  %conv.i.i.i = trunc i32 %add.i33 to i8
  %6 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i.i) #3, !range !11
  %xor.i.i.i = xor i32 %value.0.copyload.i30, %value.0.copyload.i
  %xor1.i.i.i = xor i32 %xor.i.i.i, %add.i33
  %and.i.i.i = and i32 %xor1.i.i.i, 16
  %cmp.i.i25.i = icmp eq i32 %add.i33, 0
  %shr.i.i.i = lshr i32 %value.0.copyload.i, 31
  %shr1.i.i.i = lshr i32 %value.0.copyload.i30, 31
  %shr2.i.i.i = lshr i32 %add.i33, 31
  %xor.i.i27.i = xor i32 %shr2.i.i.i, %shr.i.i.i
  %xor3.i.i.i = xor i32 %shr2.i.i.i, %shr1.i.i.i
  %add.i.i.i = add nuw nsw i32 %xor.i.i27.i, %xor3.i.i.i
  %cmp.i.i28.i = icmp eq i32 %add.i.i.i, 2
  %conv.i.i34 = zext i1 %5 to i64
  %7 = shl nuw nsw i8 %6, 2
  %8 = and i8 %7, 4
  %9 = xor i8 %8, 4
  %10 = zext i8 %9 to i64
  %11 = zext i32 %and.i.i.i to i64
  %shl22.i.i = select i1 %cmp.i.i25.i, i64 64, i64 0
  %12 = lshr i32 %add.i33, 24
  %13 = and i32 %12, 128
  %14 = zext i32 %13 to i64
  %shl34.i.i = select i1 %cmp.i.i28.i, i64 2048, i64 0
  %or6.i.i = or i64 %shl22.i.i, %14
  %and13.i.i = or i64 %or6.i.i, %11
  %or17.i.i = or i64 %and13.i.i, %conv.i.i34
  %and25.i.i = or i64 %or17.i.i, %shl34.i.i
  %or29.i.i = or i64 %and25.i.i, %10
  %arrayidx.i35 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i35 to i32*
  store i32 %add.i33, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %15 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i36 = add i64 %15, -8
  store i64 %sub.i36, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i37 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i36
  %value.addr.0.arrayidx.sroa_cast.i38 = bitcast i8* %arrayidx.i37 to i64*
  store i64 %or29.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i38, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3ADDItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i28 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i29 = bitcast i8* %arrayidx.i28 to i16*
  %value.0.copyload.i30 = load i16, i16* %value.0.arrayidx.sroa_cast.i29, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i31 = trunc i64 %3 to i16
  store i16 %conv.i.i31, i16* %value.0.arrayidx.sroa_cast.i29, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i33 = add i16 %value.0.copyload.i30, %value.0.copyload.i
  %cmp.i.i.i = icmp ult i16 %add.i33, %value.0.copyload.i
  %cmp4.i.i.i = icmp ult i16 %add.i33, %value.0.copyload.i30
  %5 = or i1 %cmp.i.i.i, %cmp4.i.i.i
  %conv.i.i.i = trunc i16 %add.i33 to i8
  %6 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i.i) #3, !range !11
  %xor4.i.i.i = xor i16 %value.0.copyload.i30, %value.0.copyload.i
  %xor35.i.i.i = xor i16 %xor4.i.i.i, %add.i33
  %7 = and i16 %xor35.i.i.i, 16
  %cmp.i.i25.i = icmp eq i16 %add.i33, 0
  %8 = lshr i16 %value.0.copyload.i, 15
  %9 = lshr i16 %value.0.copyload.i30, 15
  %10 = lshr i16 %add.i33, 15
  %xor17.i.i.i = xor i16 %10, %8
  %xor1218.i.i.i = xor i16 %10, %9
  %narrow.i.i.i = add nuw nsw i16 %xor17.i.i.i, %xor1218.i.i.i
  %cmp.i.i27.i = icmp eq i16 %narrow.i.i.i, 2
  %conv.i.i34 = zext i1 %5 to i64
  %11 = shl nuw nsw i8 %6, 2
  %12 = and i8 %11, 4
  %13 = xor i8 %12, 4
  %14 = zext i8 %13 to i64
  %15 = zext i16 %7 to i64
  %shl22.i.i = select i1 %cmp.i.i25.i, i64 64, i64 0
  %16 = lshr i16 %add.i33, 8
  %17 = and i16 %16, 128
  %18 = zext i16 %17 to i64
  %shl34.i.i = select i1 %cmp.i.i27.i, i64 2048, i64 0
  %or6.i.i = or i64 %shl22.i.i, %18
  %and13.i.i = or i64 %or6.i.i, %15
  %or17.i.i = or i64 %and13.i.i, %conv.i.i34
  %and25.i.i = or i64 %or17.i.i, %shl34.i.i
  %or29.i.i = or i64 %and25.i.i, %14
  %arrayidx.i35 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i35 to i16*
  store i16 %add.i33, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %19 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i36 = add i64 %19, -8
  store i64 %sub.i36, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i37 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i36
  %value.addr.0.arrayidx.sroa_cast.i38 = bitcast i8* %arrayidx.i37 to i64*
  store i64 %or29.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i38, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3ADDIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %conv.i = trunc i16 %value.0.copyload.i to i8
  %arrayidx.i23 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i24 = bitcast i8* %arrayidx.i23 to i16*
  %value.0.copyload.i25 = load i16, i16* %value.0.arrayidx.sroa_cast.i24, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i26 = trunc i64 %3 to i16
  store i16 %conv.i.i26, i16* %value.0.arrayidx.sroa_cast.i24, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %conv.i28 = trunc i16 %value.0.copyload.i25 to i8
  %add.i29 = add i8 %conv.i28, %conv.i
  %cmp.i.i.i = icmp ult i8 %add.i29, %conv.i
  %cmp4.i.i.i = icmp ult i8 %add.i29, %conv.i28
  %5 = or i1 %cmp.i.i.i, %cmp4.i.i.i
  %6 = tail call i8 @llvm.ctpop.i8(i8 %add.i29) #3, !range !11
  %xor4.i.i.i38 = xor i16 %value.0.copyload.i25, %value.0.copyload.i
  %xor4.i.i.i = trunc i16 %xor4.i.i.i38 to i8
  %xor35.i.i.i = xor i8 %add.i29, %xor4.i.i.i
  %7 = and i8 %xor35.i.i.i, 16
  %cmp.i.i25.i = icmp eq i8 %add.i29, 0
  %8 = lshr i8 %conv.i, 7
  %9 = lshr i8 %conv.i28, 7
  %10 = lshr i8 %add.i29, 7
  %xor14.i.i.i = xor i8 %10, %8
  %xor1215.i.i.i = xor i8 %10, %9
  %narrow.i.i.i = add nuw nsw i8 %xor14.i.i.i, %xor1215.i.i.i
  %cmp.i.i27.i = icmp eq i8 %narrow.i.i.i, 2
  %conv.i.i30 = zext i1 %5 to i64
  %11 = shl nuw nsw i8 %6, 2
  %12 = and i8 %11, 4
  %13 = or i8 %12, %7
  %and13.i28.i = xor i8 %13, 4
  %and13.i.i = zext i8 %and13.i28.i to i64
  %shl22.i.i = select i1 %cmp.i.i25.i, i64 64, i64 0
  %14 = and i8 %add.i29, -128
  %15 = zext i8 %14 to i64
  %shl34.i.i = select i1 %cmp.i.i27.i, i64 2048, i64 0
  %or6.i.i = or i64 %shl22.i.i, %15
  %or17.i.i = or i64 %or6.i.i, %conv.i.i30
  %and25.i.i = or i64 %or17.i.i, %shl34.i.i
  %or29.i.i = or i64 %and25.i.i, %and13.i.i
  %conv.i31 = zext i8 %add.i29 to i16
  %arrayidx.i32 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i32 to i16*
  store i16 %conv.i31, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %16 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i33 = add i64 %16, -8
  store i64 %sub.i33, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i34 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i33
  %value.addr.0.arrayidx.sroa_cast.i35 = bitcast i8* %arrayidx.i34 to i64*
  store i64 %or29.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i35, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z9DIV_FLAGSRm(i64* nocapture nonnull align 8 dereferenceable(8) %flags) local_unnamed_addr #2 {
entry:
  %0 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i = trunc i64 %0 to i8
  %tobool = icmp ne i8 %conv.i, 0
  %1 = load i64, i64* %flags, align 8, !tbaa !4
  %conv.i27 = zext i1 %tobool to i64
  %or6.i = or i64 %1, %conv.i27
  %and13.i = select i1 %tobool, i64 20, i64 0
  %or17.i = or i64 %or6.i, %and13.i
  %or29.i = select i1 %tobool, i64 2240, i64 0
  %or30.i = or i64 %or17.i, %or29.i
  store i64 %or30.i, i64* %flags, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3DIVImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i64 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i65 = bitcast i8* %arrayidx.i64 to i64*
  %value.0.copyload.i66126127 = load i64, i64* %value.0.arrayidx.sroa_cast.i65, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %3, i64* %value.0.arrayidx.sroa_cast.i65, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i67 = add i64 %4, 8
  store i64 %add.i67, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i68 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i67
  %value.0.arrayidx.sroa_cast.i69 = bitcast i8* %arrayidx.i68 to i64*
  %value.0.copyload.i70 = load i64, i64* %value.0.arrayidx.sroa_cast.i69, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %5, i64* %value.0.arrayidx.sroa_cast.i69, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i71 = add i64 %6, 8
  store i64 %add.i71, i64* %vsp, align 8, !tbaa !4
  %L.sroa.2.0.insert.ext.i91 = zext i64 %value.0.copyload.i66126127 to i128
  %L.sroa.2.0.insert.shift.i92 = shl nuw i128 %L.sroa.2.0.insert.ext.i91, 64
  %L.sroa.0.0.insert.ext.i93 = zext i64 %value.0.copyload.i to i128
  %L.sroa.0.0.insert.insert.i94 = or i128 %L.sroa.2.0.insert.shift.i92, %L.sroa.0.0.insert.ext.i93
  %R.sroa.0.0.insert.ext.i97 = zext i64 %value.0.copyload.i70 to i128
  %L.sroa.0.0.insert.insert.i94.frozen = freeze i128 %L.sroa.0.0.insert.insert.i94
  %R.sroa.0.0.insert.ext.i97.frozen = freeze i128 %R.sroa.0.0.insert.ext.i97
  %div.i = udiv i128 %L.sroa.0.0.insert.insert.i94.frozen, %R.sroa.0.0.insert.ext.i97.frozen
  %retval.sroa.0.0.extract.trunc.i99 = trunc i128 %div.i to i64
  %7 = mul i128 %div.i, %R.sroa.0.0.insert.ext.i97.frozen
  %rem.i.decomposed = sub i128 %L.sroa.0.0.insert.insert.i94.frozen, %7
  %retval.sroa.0.0.extract.trunc.i112 = trunc i128 %rem.i.decomposed to i64
  %8 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %8 to i8
  %tobool.i = icmp ne i8 %conv.i.i, 0
  %conv.i27.i = zext i1 %tobool.i to i64
  %and13.i.i = select i1 %tobool.i, i64 20, i64 0
  %or17.i.i = or i64 %and13.i.i, %conv.i27.i
  %or29.i.i = select i1 %tobool.i, i64 2240, i64 0
  %or30.i.i = or i64 %or17.i.i, %or29.i.i
  store i64 %6, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i117 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %6
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i117 to i64*
  store i64 %retval.sroa.0.0.extract.trunc.i99, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %9 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i118 = add i64 %9, -8
  store i64 %sub.i118, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i119 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i118
  %value.addr.0.arrayidx.sroa_cast.i120 = bitcast i8* %arrayidx.i119 to i64*
  store i64 %retval.sroa.0.0.extract.trunc.i112, i64* %value.addr.0.arrayidx.sroa_cast.i120, align 1
  %10 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i121 = add i64 %10, -8
  store i64 %sub.i121, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i122 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i121
  %value.addr.0.arrayidx.sroa_cast.i123 = bitcast i8* %arrayidx.i122 to i64*
  store i64 %or30.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i123, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3DIVIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  %value.0.copyload.i = load i32, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i32
  store i32 %conv.i.i, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 4
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i47 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i48 = bitcast i8* %arrayidx.i47 to i32*
  %value.0.copyload.i49 = load i32, i32* %value.0.arrayidx.sroa_cast.i48, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i50 = trunc i64 %3 to i32
  store i32 %conv.i.i50, i32* %value.0.arrayidx.sroa_cast.i48, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i51 = add i64 %4, 4
  store i64 %add.i51, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i52 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i51
  %value.0.arrayidx.sroa_cast.i53 = bitcast i8* %arrayidx.i52 to i32*
  %value.0.copyload.i54 = load i32, i32* %value.0.arrayidx.sroa_cast.i53, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i55 = trunc i64 %5 to i32
  store i32 %conv.i.i55, i32* %value.0.arrayidx.sroa_cast.i53, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i56 = add i64 %6, 4
  store i64 %add.i56, i64* %vsp, align 8, !tbaa !4
  %conv.i = zext i32 %value.0.copyload.i to i64
  %conv.i57 = zext i32 %value.0.copyload.i49 to i64
  %conv.i58 = zext i32 %value.0.copyload.i54 to i64
  %shl.i = shl nuw i64 %conv.i57, 32
  %or.i = or i64 %shl.i, %conv.i
  %div.i = udiv i64 %or.i, %conv.i58
  %rem.i = urem i64 %or.i, %conv.i58
  %conv.i59 = trunc i64 %div.i to i32
  %conv.i60 = trunc i64 %rem.i to i32
  %7 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i61 = trunc i64 %7 to i8
  %tobool.i = icmp ne i8 %conv.i.i61, 0
  %conv.i27.i = zext i1 %tobool.i to i64
  %and13.i.i = select i1 %tobool.i, i64 20, i64 0
  %or17.i.i = or i64 %and13.i.i, %conv.i27.i
  %or29.i.i = select i1 %tobool.i, i64 2240, i64 0
  %or30.i.i = or i64 %or17.i.i, %or29.i.i
  store i64 %6, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i62 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %6
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i62 to i32*
  store i32 %conv.i59, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %8 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i63 = add i64 %8, -4
  store i64 %sub.i63, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i64 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i63
  %value.addr.0.arrayidx.sroa_cast.i65 = bitcast i8* %arrayidx.i64 to i32*
  store i32 %conv.i60, i32* %value.addr.0.arrayidx.sroa_cast.i65, align 1
  %9 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i66 = add i64 %9, -8
  store i64 %sub.i66, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i67 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i66
  %value.addr.0.arrayidx.sroa_cast.i68 = bitcast i8* %arrayidx.i67 to i64*
  store i64 %or30.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i68, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3DIVItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i47 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i48 = bitcast i8* %arrayidx.i47 to i16*
  %value.0.copyload.i49 = load i16, i16* %value.0.arrayidx.sroa_cast.i48, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i50 = trunc i64 %3 to i16
  store i16 %conv.i.i50, i16* %value.0.arrayidx.sroa_cast.i48, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i51 = add i64 %4, 2
  store i64 %add.i51, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i52 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i51
  %value.0.arrayidx.sroa_cast.i53 = bitcast i8* %arrayidx.i52 to i16*
  %value.0.copyload.i54 = load i16, i16* %value.0.arrayidx.sroa_cast.i53, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i55 = trunc i64 %5 to i16
  store i16 %conv.i.i55, i16* %value.0.arrayidx.sroa_cast.i53, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i56 = add i64 %6, 2
  store i64 %add.i56, i64* %vsp, align 8, !tbaa !4
  %conv.i = zext i16 %value.0.copyload.i to i32
  %conv.i58 = zext i16 %value.0.copyload.i54 to i32
  %conv.i59 = zext i16 %value.0.copyload.i49 to i32
  %shl.i = shl nuw i32 %conv.i59, 16
  %or3.i = or i32 %shl.i, %conv.i
  %7 = udiv i32 %or3.i, %conv.i58
  %8 = urem i32 %or3.i, %conv.i58
  %conv.i60 = trunc i32 %7 to i16
  %conv.i61 = trunc i32 %8 to i16
  %9 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i62 = trunc i64 %9 to i8
  %tobool.i = icmp ne i8 %conv.i.i62, 0
  %conv.i27.i = zext i1 %tobool.i to i64
  %and13.i.i = select i1 %tobool.i, i64 20, i64 0
  %or17.i.i = or i64 %and13.i.i, %conv.i27.i
  %or29.i.i = select i1 %tobool.i, i64 2240, i64 0
  %or30.i.i = or i64 %or17.i.i, %or29.i.i
  store i64 %6, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i63 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %6
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i63 to i16*
  store i16 %conv.i60, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %10 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i64 = add i64 %10, -2
  store i64 %sub.i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i65 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i64
  %value.addr.0.arrayidx.sroa_cast.i66 = bitcast i8* %arrayidx.i65 to i16*
  store i16 %conv.i61, i16* %value.addr.0.arrayidx.sroa_cast.i66, align 1
  %11 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i67 = add i64 %11, -8
  store i64 %sub.i67, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i68 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i67
  %value.addr.0.arrayidx.sroa_cast.i69 = bitcast i8* %arrayidx.i68 to i64*
  store i64 %or30.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i69, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3DIVIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i38 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i39 = bitcast i8* %arrayidx.i38 to i16*
  %value.0.copyload.i40 = load i16, i16* %value.0.arrayidx.sroa_cast.i39, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i41 = trunc i64 %3 to i16
  store i16 %conv.i.i41, i16* %value.0.arrayidx.sroa_cast.i39, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i42 = add i64 %4, 2
  store i64 %add.i42, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i44 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i42
  %value.0.arrayidx.sroa_cast.i45 = bitcast i8* %arrayidx.i44 to i16*
  %value.0.copyload.i46 = load i16, i16* %value.0.arrayidx.sroa_cast.i45, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i47 = trunc i64 %5 to i16
  store i16 %conv.i.i47, i16* %value.0.arrayidx.sroa_cast.i45, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i48 = add i64 %6, 2
  store i64 %add.i48, i64* %vsp, align 8, !tbaa !4
  %conv.i50 = and i16 %value.0.copyload.i, 255
  %conv.i52 = and i16 %value.0.copyload.i46, 255
  %7 = shl i16 %value.0.copyload.i40, 8
  %or3.i = or i16 %7, %conv.i50
  %8 = udiv i16 %or3.i, %conv.i52
  %9 = urem i16 %or3.i, %conv.i52
  %conv.i54 = trunc i16 %8 to i8
  %conv.i55 = trunc i16 %9 to i8
  %10 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i56 = trunc i64 %10 to i8
  %tobool.i = icmp ne i8 %conv.i.i56, 0
  %conv.i27.i = zext i1 %tobool.i to i64
  %and13.i.i = select i1 %tobool.i, i64 20, i64 0
  %or17.i.i = or i64 %and13.i.i, %conv.i27.i
  %or29.i.i = select i1 %tobool.i, i64 2240, i64 0
  %or30.i.i = or i64 %or17.i.i, %or29.i.i
  %sub.i = add i64 %6, 1
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i57 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  store i8 %conv.i54, i8* %arrayidx.i57, align 1
  %11 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i58 = add i64 %11, -1
  store i64 %sub.i58, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i59 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i58
  store i8 %conv.i55, i8* %arrayidx.i59, align 1
  %12 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i60 = add i64 %12, -8
  store i64 %sub.i60, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i61 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i60
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i61 to i64*
  store i64 %or30.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z10IDIV_FLAGSRm(i64* nocapture nonnull align 8 dereferenceable(8) %flags) local_unnamed_addr #2 {
entry:
  %0 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %0 to i8
  %tobool.i = icmp ne i8 %conv.i.i, 0
  %1 = load i64, i64* %flags, align 8, !tbaa !4
  %conv.i27.i = zext i1 %tobool.i to i64
  %or6.i.i = or i64 %1, %conv.i27.i
  %and13.i.i = select i1 %tobool.i, i64 20, i64 0
  %or17.i.i = or i64 %or6.i.i, %and13.i.i
  %or29.i.i = select i1 %tobool.i, i64 2240, i64 0
  %or30.i.i = or i64 %or17.i.i, %or29.i.i
  store i64 %or30.i.i, i64* %flags, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IDIVImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i69 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i70 = bitcast i8* %arrayidx.i69 to i64*
  %value.0.copyload.i71131132 = load i64, i64* %value.0.arrayidx.sroa_cast.i70, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %3, i64* %value.0.arrayidx.sroa_cast.i70, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i72 = add i64 %4, 8
  store i64 %add.i72, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i73 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i72
  %value.0.arrayidx.sroa_cast.i74 = bitcast i8* %arrayidx.i73 to i64*
  %value.0.copyload.i75 = load i64, i64* %value.0.arrayidx.sroa_cast.i74, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %5, i64* %value.0.arrayidx.sroa_cast.i74, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i76 = add i64 %6, 8
  store i64 %add.i76, i64* %vsp, align 8, !tbaa !4
  %7 = ashr i64 %value.0.copyload.i75, 63
  %L.sroa.2.0.insert.ext.i96 = zext i64 %value.0.copyload.i71131132 to i128
  %L.sroa.2.0.insert.shift.i97 = shl nuw i128 %L.sroa.2.0.insert.ext.i96, 64
  %L.sroa.0.0.insert.ext.i98 = zext i64 %value.0.copyload.i to i128
  %L.sroa.0.0.insert.insert.i99 = or i128 %L.sroa.2.0.insert.shift.i97, %L.sroa.0.0.insert.ext.i98
  %R.sroa.2.0.insert.ext.i100 = zext i64 %7 to i128
  %R.sroa.2.0.insert.shift.i101 = shl nuw i128 %R.sroa.2.0.insert.ext.i100, 64
  %R.sroa.0.0.insert.ext.i102 = zext i64 %value.0.copyload.i75 to i128
  %R.sroa.0.0.insert.insert.i103 = or i128 %R.sroa.2.0.insert.shift.i101, %R.sroa.0.0.insert.ext.i102
  %L.sroa.0.0.insert.insert.i99.frozen = freeze i128 %L.sroa.0.0.insert.insert.i99
  %R.sroa.0.0.insert.insert.i103.frozen = freeze i128 %R.sroa.0.0.insert.insert.i103
  %div.i = sdiv i128 %L.sroa.0.0.insert.insert.i99.frozen, %R.sroa.0.0.insert.insert.i103.frozen
  %retval.sroa.0.0.extract.trunc.i104 = trunc i128 %div.i to i64
  %8 = mul i128 %div.i, %R.sroa.0.0.insert.insert.i103.frozen
  %rem.i.decomposed = sub i128 %L.sroa.0.0.insert.insert.i99.frozen, %8
  %retval.sroa.0.0.extract.trunc.i117 = trunc i128 %rem.i.decomposed to i64
  %9 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i.i = trunc i64 %9 to i8
  %tobool.i.i = icmp ne i8 %conv.i.i.i, 0
  %conv.i27.i.i = zext i1 %tobool.i.i to i64
  %and13.i.i.i = select i1 %tobool.i.i, i64 20, i64 0
  %or17.i.i.i = or i64 %and13.i.i.i, %conv.i27.i.i
  %or29.i.i.i = select i1 %tobool.i.i, i64 2240, i64 0
  %or30.i.i.i = or i64 %or17.i.i.i, %or29.i.i.i
  store i64 %6, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i122 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %6
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i122 to i64*
  store i64 %retval.sroa.0.0.extract.trunc.i104, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %10 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i123 = add i64 %10, -8
  store i64 %sub.i123, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i124 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i123
  %value.addr.0.arrayidx.sroa_cast.i125 = bitcast i8* %arrayidx.i124 to i64*
  store i64 %retval.sroa.0.0.extract.trunc.i117, i64* %value.addr.0.arrayidx.sroa_cast.i125, align 1
  %11 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i126 = add i64 %11, -8
  store i64 %sub.i126, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i127 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i126
  %value.addr.0.arrayidx.sroa_cast.i128 = bitcast i8* %arrayidx.i127 to i64*
  store i64 %or30.i.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i128, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IDIVIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  %value.0.copyload.i = load i32, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i32
  store i32 %conv.i.i, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 4
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i50 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i51 = bitcast i8* %arrayidx.i50 to i32*
  %value.0.copyload.i52 = load i32, i32* %value.0.arrayidx.sroa_cast.i51, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i53 = trunc i64 %3 to i32
  store i32 %conv.i.i53, i32* %value.0.arrayidx.sroa_cast.i51, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i54 = add i64 %4, 4
  store i64 %add.i54, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i55 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i54
  %value.0.arrayidx.sroa_cast.i56 = bitcast i8* %arrayidx.i55 to i32*
  %value.0.copyload.i57 = load i32, i32* %value.0.arrayidx.sroa_cast.i56, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i58 = trunc i64 %5 to i32
  store i32 %conv.i.i58, i32* %value.0.arrayidx.sroa_cast.i56, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i59 = add i64 %6, 4
  store i64 %add.i59, i64* %vsp, align 8, !tbaa !4
  %conv.i = zext i32 %value.0.copyload.i to i64
  %conv.i60 = zext i32 %value.0.copyload.i52 to i64
  %conv.i61 = sext i32 %value.0.copyload.i57 to i64
  %shl.i = shl nuw i64 %conv.i60, 32
  %or.i = or i64 %shl.i, %conv.i
  %div.i = sdiv i64 %or.i, %conv.i61
  %rem.i = srem i64 %or.i, %conv.i61
  %conv.i62 = trunc i64 %div.i to i32
  %conv.i63 = trunc i64 %rem.i to i32
  %7 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i.i = trunc i64 %7 to i8
  %tobool.i.i = icmp ne i8 %conv.i.i.i, 0
  %conv.i27.i.i = zext i1 %tobool.i.i to i64
  %and13.i.i.i = select i1 %tobool.i.i, i64 20, i64 0
  %or17.i.i.i = or i64 %and13.i.i.i, %conv.i27.i.i
  %or29.i.i.i = select i1 %tobool.i.i, i64 2240, i64 0
  %or30.i.i.i = or i64 %or17.i.i.i, %or29.i.i.i
  store i64 %6, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i64 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %6
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i64 to i32*
  store i32 %conv.i62, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %8 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i65 = add i64 %8, -4
  store i64 %sub.i65, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i66 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i65
  %value.addr.0.arrayidx.sroa_cast.i67 = bitcast i8* %arrayidx.i66 to i32*
  store i32 %conv.i63, i32* %value.addr.0.arrayidx.sroa_cast.i67, align 1
  %9 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i68 = add i64 %9, -8
  store i64 %sub.i68, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i69 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i68
  %value.addr.0.arrayidx.sroa_cast.i70 = bitcast i8* %arrayidx.i69 to i64*
  store i64 %or30.i.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i70, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IDIVItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i50 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i51 = bitcast i8* %arrayidx.i50 to i16*
  %value.0.copyload.i52 = load i16, i16* %value.0.arrayidx.sroa_cast.i51, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i53 = trunc i64 %3 to i16
  store i16 %conv.i.i53, i16* %value.0.arrayidx.sroa_cast.i51, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i54 = add i64 %4, 2
  store i64 %add.i54, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i55 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i54
  %value.0.arrayidx.sroa_cast.i56 = bitcast i8* %arrayidx.i55 to i16*
  %value.0.copyload.i57 = load i16, i16* %value.0.arrayidx.sroa_cast.i56, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i58 = trunc i64 %5 to i16
  store i16 %conv.i.i58, i16* %value.0.arrayidx.sroa_cast.i56, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i59 = add i64 %6, 2
  store i64 %add.i59, i64* %vsp, align 8, !tbaa !4
  %conv.i = zext i16 %value.0.copyload.i to i32
  %conv.i62 = zext i16 %value.0.copyload.i52 to i32
  %shl.i = shl nuw i32 %conv.i62, 16
  %or3.i = or i32 %shl.i, %conv.i
  %conv.i63 = sext i32 %or3.i to i64
  %conv1.i = sext i16 %value.0.copyload.i57 to i64
  %div.i = sdiv i64 %conv.i63, %conv1.i
  %conv2.i64 = trunc i64 %div.i to i16
  %rem.i = srem i64 %conv.i63, %conv1.i
  %conv2.i67 = trunc i64 %rem.i to i16
  %7 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i.i = trunc i64 %7 to i8
  %tobool.i.i = icmp ne i8 %conv.i.i.i, 0
  %conv.i27.i.i = zext i1 %tobool.i.i to i64
  %and13.i.i.i = select i1 %tobool.i.i, i64 20, i64 0
  %or17.i.i.i = or i64 %and13.i.i.i, %conv.i27.i.i
  %or29.i.i.i = select i1 %tobool.i.i, i64 2240, i64 0
  %or30.i.i.i = or i64 %or17.i.i.i, %or29.i.i.i
  store i64 %6, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i70 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %6
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i70 to i16*
  store i16 %conv2.i64, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %8 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i71 = add i64 %8, -2
  store i64 %sub.i71, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i72 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i71
  %value.addr.0.arrayidx.sroa_cast.i73 = bitcast i8* %arrayidx.i72 to i16*
  store i16 %conv2.i67, i16* %value.addr.0.arrayidx.sroa_cast.i73, align 1
  %9 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i74 = add i64 %9, -8
  store i64 %sub.i74, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i75 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i74
  %value.addr.0.arrayidx.sroa_cast.i76 = bitcast i8* %arrayidx.i75 to i64*
  store i64 %or30.i.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i76, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IDIVIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i41 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i42 = bitcast i8* %arrayidx.i41 to i16*
  %value.0.copyload.i43 = load i16, i16* %value.0.arrayidx.sroa_cast.i42, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i44 = trunc i64 %3 to i16
  store i16 %conv.i.i44, i16* %value.0.arrayidx.sroa_cast.i42, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i45 = add i64 %4, 2
  store i64 %add.i45, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i47 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i45
  %value.0.arrayidx.sroa_cast.i48 = bitcast i8* %arrayidx.i47 to i16*
  %value.0.copyload.i49 = load i16, i16* %value.0.arrayidx.sroa_cast.i48, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i50 = trunc i64 %5 to i16
  store i16 %conv.i.i50, i16* %value.0.arrayidx.sroa_cast.i48, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i51 = add i64 %6, 2
  store i64 %add.i51, i64* %vsp, align 8, !tbaa !4
  %conv.i53 = and i16 %value.0.copyload.i, 255
  %sext = shl i16 %value.0.copyload.i49, 8
  %conv.i55 = ashr exact i16 %sext, 8
  %7 = shl i16 %value.0.copyload.i43, 8
  %or3.i = or i16 %7, %conv.i53
  %conv.i57 = sext i16 %or3.i to i32
  %conv1.i = sext i16 %conv.i55 to i32
  %div3.i = sdiv i32 %conv.i57, %conv1.i
  %div.sext.i = trunc i32 %div3.i to i8
  %rem3.i = srem i32 %conv.i57, %conv1.i
  %rem.sext.i = trunc i32 %rem3.i to i8
  %8 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i.i = trunc i64 %8 to i8
  %tobool.i.i = icmp ne i8 %conv.i.i.i, 0
  %conv.i27.i.i = zext i1 %tobool.i.i to i64
  %and13.i.i.i = select i1 %tobool.i.i, i64 20, i64 0
  %or17.i.i.i = or i64 %and13.i.i.i, %conv.i27.i.i
  %or29.i.i.i = select i1 %tobool.i.i, i64 2240, i64 0
  %or30.i.i.i = or i64 %or17.i.i.i, %or29.i.i.i
  %sub.i = add i64 %6, 1
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i62 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  store i8 %div.sext.i, i8* %arrayidx.i62, align 1
  %9 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i63 = add i64 %9, -1
  store i64 %sub.i63, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i64 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i63
  store i8 %rem.sext.i, i8* %arrayidx.i64, align 1
  %10 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i65 = add i64 %10, -8
  store i64 %sub.i65, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i66 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i65
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i66 to i64*
  store i64 %or30.i.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3MULImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i50 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i51 = bitcast i8* %arrayidx.i50 to i64*
  %value.0.copyload.i52 = load i64, i64* %value.0.arrayidx.sroa_cast.i51, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %3, i64* %value.0.arrayidx.sroa_cast.i51, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i53 = add i64 %4, 8
  store i64 %add.i53, i64* %vsp, align 8, !tbaa !4
  %L.sroa.0.0.insert.ext.i = zext i64 %value.0.copyload.i to i128
  %R.sroa.0.0.insert.ext.i = zext i64 %value.0.copyload.i52 to i128
  %mul.i = mul nuw i128 %R.sroa.0.0.insert.ext.i, %L.sroa.0.0.insert.ext.i
  %retval.sroa.0.0.extract.trunc.i = trunc i128 %mul.i to i64
  %retval.sroa.2.0.extract.shift.i = lshr i128 %mul.i, 64
  %retval.sroa.2.0.extract.trunc.i = trunc i128 %retval.sroa.2.0.extract.shift.i to i64
  %5 = icmp ugt i128 %mul.i, 18446744073709551615
  %6 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %6 to i8
  %tobool.not.i = icmp eq i8 %conv.i.i, 0
  %conv.i26.i = zext i1 %5 to i64
  %and13.i.i = select i1 %tobool.not.i, i64 0, i64 20
  %shl34.i.i = select i1 %5, i64 2048, i64 0
  %and25.i.i = select i1 %tobool.not.i, i64 0, i64 192
  %or6.i.i = or i64 %shl34.i.i, %conv.i26.i
  %or29.i.i = or i64 %or6.i.i, %and25.i.i
  %or30.i.i = or i64 %or29.i.i, %and13.i.i
  store i64 %4, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i71 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i71 to i64*
  store i64 %retval.sroa.0.0.extract.trunc.i, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %7 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i72 = add i64 %7, -8
  store i64 %sub.i72, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i73 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i72
  %value.addr.0.arrayidx.sroa_cast.i74 = bitcast i8* %arrayidx.i73 to i64*
  store i64 %retval.sroa.2.0.extract.trunc.i, i64* %value.addr.0.arrayidx.sroa_cast.i74, align 1
  %8 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i75 = add i64 %8, -8
  store i64 %sub.i75, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i76 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i75
  %value.addr.0.arrayidx.sroa_cast.i77 = bitcast i8* %arrayidx.i76 to i64*
  store i64 %or30.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i77, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3MULIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  %value.0.copyload.i = load i32, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i32
  store i32 %conv.i.i, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 4
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i39 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i40 = bitcast i8* %arrayidx.i39 to i32*
  %value.0.copyload.i41 = load i32, i32* %value.0.arrayidx.sroa_cast.i40, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i42 = trunc i64 %3 to i32
  store i32 %conv.i.i42, i32* %value.0.arrayidx.sroa_cast.i40, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i43 = add i64 %4, 4
  store i64 %add.i43, i64* %vsp, align 8, !tbaa !4
  %conv.i = zext i32 %value.0.copyload.i to i64
  %conv.i44 = zext i32 %value.0.copyload.i41 to i64
  %mul.i = mul nuw i64 %conv.i44, %conv.i
  %conv.i45 = trunc i64 %mul.i to i32
  %shr.i = lshr i64 %mul.i, 32
  %conv.i46 = trunc i64 %shr.i to i32
  %5 = icmp ugt i64 %mul.i, 4294967295
  %6 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i47 = trunc i64 %6 to i8
  %tobool.not.i = icmp eq i8 %conv.i.i47, 0
  %conv.i24.i = zext i1 %5 to i64
  %and13.i.i = select i1 %tobool.not.i, i64 0, i64 20
  %shl34.i.i = select i1 %5, i64 2048, i64 0
  %and25.i.i = select i1 %tobool.not.i, i64 0, i64 192
  %or6.i.i = or i64 %shl34.i.i, %conv.i24.i
  %or29.i.i = or i64 %or6.i.i, %and25.i.i
  %or30.i.i = or i64 %or29.i.i, %and13.i.i
  store i64 %4, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i48 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i48 to i32*
  store i32 %conv.i45, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %7 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i49 = add i64 %7, -4
  store i64 %sub.i49, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i50 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i49
  %value.addr.0.arrayidx.sroa_cast.i51 = bitcast i8* %arrayidx.i50 to i32*
  store i32 %conv.i46, i32* %value.addr.0.arrayidx.sroa_cast.i51, align 1
  %8 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i52 = add i64 %8, -8
  store i64 %sub.i52, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i53 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i52
  %value.addr.0.arrayidx.sroa_cast.i54 = bitcast i8* %arrayidx.i53 to i64*
  store i64 %or30.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i54, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3MULItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i39 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i40 = bitcast i8* %arrayidx.i39 to i16*
  %value.0.copyload.i41 = load i16, i16* %value.0.arrayidx.sroa_cast.i40, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i42 = trunc i64 %3 to i16
  store i16 %conv.i.i42, i16* %value.0.arrayidx.sroa_cast.i40, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i43 = add i64 %4, 2
  store i64 %add.i43, i64* %vsp, align 8, !tbaa !4
  %conv.i = zext i16 %value.0.copyload.i to i32
  %conv.i44 = zext i16 %value.0.copyload.i41 to i32
  %mul.i = mul nuw i32 %conv.i44, %conv.i
  %conv.i45 = trunc i32 %mul.i to i16
  %5 = lshr i32 %mul.i, 16
  %conv2.i = trunc i32 %5 to i16
  %6 = icmp ugt i32 %mul.i, 65535
  %7 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i48 = trunc i64 %7 to i8
  %tobool.not.i = icmp eq i8 %conv.i.i48, 0
  %conv.i24.i = zext i1 %6 to i64
  %and13.i.i = select i1 %tobool.not.i, i64 0, i64 20
  %shl34.i.i = select i1 %6, i64 2048, i64 0
  %and25.i.i = select i1 %tobool.not.i, i64 0, i64 192
  %or6.i.i = or i64 %shl34.i.i, %conv.i24.i
  %or29.i.i = or i64 %or6.i.i, %and25.i.i
  %or30.i.i = or i64 %or29.i.i, %and13.i.i
  store i64 %4, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i49 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i49 to i16*
  store i16 %conv.i45, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %8 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i50 = add i64 %8, -2
  store i64 %sub.i50, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i51 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i50
  %value.addr.0.arrayidx.sroa_cast.i52 = bitcast i8* %arrayidx.i51 to i16*
  store i16 %conv2.i, i16* %value.addr.0.arrayidx.sroa_cast.i52, align 1
  %9 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i53 = add i64 %9, -8
  store i64 %sub.i53, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i54 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i53
  %value.addr.0.arrayidx.sroa_cast.i55 = bitcast i8* %arrayidx.i54 to i64*
  store i64 %or30.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i55, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3MULIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i32 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i33 = bitcast i8* %arrayidx.i32 to i16*
  %value.0.copyload.i34 = load i16, i16* %value.0.arrayidx.sroa_cast.i33, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i35 = trunc i64 %3 to i16
  store i16 %conv.i.i35, i16* %value.0.arrayidx.sroa_cast.i33, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i36 = add i64 %4, 2
  store i64 %add.i36, i64* %vsp, align 8, !tbaa !4
  %conv.i38 = and i16 %value.0.copyload.i, 255
  %conv.i39 = and i16 %value.0.copyload.i34, 255
  %mul.i = mul nuw i16 %conv.i39, %conv.i38
  %conv.i40 = trunc i16 %mul.i to i8
  %5 = lshr i16 %mul.i, 8
  %conv2.i = trunc i16 %5 to i8
  %6 = icmp ugt i16 %mul.i, 255
  %7 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i43 = trunc i64 %7 to i8
  %tobool.not.i = icmp eq i8 %conv.i.i43, 0
  %conv.i24.i = zext i1 %6 to i64
  %and13.i.i = select i1 %tobool.not.i, i64 0, i64 20
  %shl34.i.i = select i1 %6, i64 2048, i64 0
  %and25.i.i = select i1 %tobool.not.i, i64 0, i64 192
  %or6.i.i = or i64 %shl34.i.i, %conv.i24.i
  %or29.i.i = or i64 %or6.i.i, %and25.i.i
  %or30.i.i = or i64 %or29.i.i, %and13.i.i
  %sub.i = add i64 %4, 1
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i44 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  store i8 %conv.i40, i8* %arrayidx.i44, align 1
  %8 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i45 = add i64 %8, -1
  store i64 %sub.i45, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i46 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i45
  store i8 %conv2.i, i8* %arrayidx.i46, align 1
  %9 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i47 = add i64 %9, -8
  store i64 %sub.i47, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i48 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i47
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i48 to i64*
  store i64 %or30.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IMULImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i53 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i54 = bitcast i8* %arrayidx.i53 to i64*
  %value.0.copyload.i55 = load i64, i64* %value.0.arrayidx.sroa_cast.i54, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %3, i64* %value.0.arrayidx.sroa_cast.i54, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i56 = add i64 %4, 8
  store i64 %add.i56, i64* %vsp, align 8, !tbaa !4
  %5 = ashr i64 %value.0.copyload.i, 63
  %6 = ashr i64 %value.0.copyload.i55, 63
  %L.sroa.2.0.insert.ext.i = zext i64 %5 to i128
  %L.sroa.2.0.insert.shift.i = shl nuw i128 %L.sroa.2.0.insert.ext.i, 64
  %L.sroa.0.0.insert.ext.i = zext i64 %value.0.copyload.i to i128
  %L.sroa.0.0.insert.insert.i = or i128 %L.sroa.2.0.insert.shift.i, %L.sroa.0.0.insert.ext.i
  %R.sroa.2.0.insert.ext.i = zext i64 %6 to i128
  %R.sroa.2.0.insert.shift.i = shl nuw i128 %R.sroa.2.0.insert.ext.i, 64
  %R.sroa.0.0.insert.ext.i = zext i64 %value.0.copyload.i55 to i128
  %R.sroa.0.0.insert.insert.i = or i128 %R.sroa.2.0.insert.shift.i, %R.sroa.0.0.insert.ext.i
  %mul.i = mul nsw i128 %R.sroa.0.0.insert.insert.i, %L.sroa.0.0.insert.insert.i
  %retval.sroa.0.0.extract.trunc.i = trunc i128 %mul.i to i64
  %retval.sroa.2.0.extract.shift.i = lshr i128 %mul.i, 64
  %retval.sroa.2.0.extract.trunc.i = trunc i128 %retval.sroa.2.0.extract.shift.i to i64
  %conv4.i.i.i.i = sext i64 %retval.sroa.0.0.extract.trunc.i to i128
  %cmp.i.i.i.i = icmp ne i128 %mul.i, %conv4.i.i.i.i
  %7 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i.i = trunc i64 %7 to i8
  %tobool.not.i.i = icmp eq i8 %conv.i.i.i, 0
  %conv.i27.i.i = zext i1 %cmp.i.i.i.i to i64
  %and13.i.i.i = select i1 %tobool.not.i.i, i64 0, i64 20
  %shl22.i.i.i = select i1 %tobool.not.i.i, i64 0, i64 64
  %8 = lshr i64 %retval.sroa.0.0.extract.trunc.i, 56
  %9 = and i64 %8, 128
  %shl34.i.i.i = select i1 %cmp.i.i.i.i, i64 2048, i64 0
  %and25.i.i.i = or i64 %and13.i.i.i, %9
  %or29.i.i.i = or i64 %and25.i.i.i, %shl22.i.i.i
  %or17.i.i.i = or i64 %or29.i.i.i, %conv.i27.i.i
  %or30.i.i.i = or i64 %or17.i.i.i, %shl34.i.i.i
  store i64 %4, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i74 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i74 to i64*
  store i64 %retval.sroa.0.0.extract.trunc.i, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %10 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i75 = add i64 %10, -8
  store i64 %sub.i75, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i76 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i75
  %value.addr.0.arrayidx.sroa_cast.i77 = bitcast i8* %arrayidx.i76 to i64*
  store i64 %retval.sroa.2.0.extract.trunc.i, i64* %value.addr.0.arrayidx.sroa_cast.i77, align 1
  %11 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i78 = add i64 %11, -8
  store i64 %sub.i78, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i79 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i78
  %value.addr.0.arrayidx.sroa_cast.i80 = bitcast i8* %arrayidx.i79 to i64*
  store i64 %or30.i.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i80, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IMULIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  %value.0.copyload.i = load i32, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i32
  store i32 %conv.i.i, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 4
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i40 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i41 = bitcast i8* %arrayidx.i40 to i32*
  %value.0.copyload.i42 = load i32, i32* %value.0.arrayidx.sroa_cast.i41, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i43 = trunc i64 %3 to i32
  store i32 %conv.i.i43, i32* %value.0.arrayidx.sroa_cast.i41, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i44 = add i64 %4, 4
  store i64 %add.i44, i64* %vsp, align 8, !tbaa !4
  %conv.i = sext i32 %value.0.copyload.i to i64
  %conv.i45 = sext i32 %value.0.copyload.i42 to i64
  %mul.i = mul nsw i64 %conv.i45, %conv.i
  %conv.i46 = trunc i64 %mul.i to i32
  %shr.i = lshr i64 %mul.i, 32
  %conv.i47 = trunc i64 %shr.i to i32
  %5 = add nsw i64 %mul.i, 2147483648
  %6 = icmp ugt i64 %5, 4294967295
  %7 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i.i = trunc i64 %7 to i8
  %tobool.not.i.i = icmp eq i8 %conv.i.i.i, 0
  %conv.i24.i.i = zext i1 %6 to i64
  %and13.i.i.i = select i1 %tobool.not.i.i, i64 0, i64 20
  %shl22.i.i.i = select i1 %tobool.not.i.i, i64 0, i64 64
  %8 = lshr i64 %mul.i, 24
  %9 = and i64 %8, 128
  %shl34.i.i.i = select i1 %6, i64 2048, i64 0
  %and25.i.i.i = or i64 %9, %conv.i24.i.i
  %or29.i.i.i = or i64 %and25.i.i.i, %shl34.i.i.i
  %or17.i.i.i = or i64 %or29.i.i.i, %and13.i.i.i
  %or30.i.i.i = or i64 %or17.i.i.i, %shl22.i.i.i
  store i64 %4, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i48 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i48 to i32*
  store i32 %conv.i46, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %10 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i49 = add i64 %10, -4
  store i64 %sub.i49, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i50 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i49
  %value.addr.0.arrayidx.sroa_cast.i51 = bitcast i8* %arrayidx.i50 to i32*
  store i32 %conv.i47, i32* %value.addr.0.arrayidx.sroa_cast.i51, align 1
  %11 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i52 = add i64 %11, -8
  store i64 %sub.i52, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i53 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i52
  %value.addr.0.arrayidx.sroa_cast.i54 = bitcast i8* %arrayidx.i53 to i64*
  store i64 %or30.i.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i54, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IMULItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i40 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i41 = bitcast i8* %arrayidx.i40 to i16*
  %value.0.copyload.i42 = load i16, i16* %value.0.arrayidx.sroa_cast.i41, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i43 = trunc i64 %3 to i16
  store i16 %conv.i.i43, i16* %value.0.arrayidx.sroa_cast.i41, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i44 = add i64 %4, 2
  store i64 %add.i44, i64* %vsp, align 8, !tbaa !4
  %conv.i = sext i16 %value.0.copyload.i to i32
  %conv.i45 = sext i16 %value.0.copyload.i42 to i32
  %mul.i = mul nsw i32 %conv.i45, %conv.i
  %conv.i46 = trunc i32 %mul.i to i16
  %5 = lshr i32 %mul.i, 16
  %conv2.i = trunc i32 %5 to i16
  %6 = add nsw i32 %mul.i, 32768
  %7 = icmp ugt i32 %6, 65535
  %8 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i.i = trunc i64 %8 to i8
  %tobool.not.i.i = icmp eq i8 %conv.i.i.i, 0
  %conv.i24.i.i = zext i1 %7 to i64
  %and13.i.i.i = select i1 %tobool.not.i.i, i64 0, i64 20
  %shl22.i.i.i = select i1 %tobool.not.i.i, i64 0, i64 64
  %9 = lshr i16 %conv.i46, 8
  %10 = and i16 %9, 128
  %11 = zext i16 %10 to i64
  %shl34.i.i.i = select i1 %7, i64 2048, i64 0
  %and25.i.i.i = or i64 %shl34.i.i.i, %conv.i24.i.i
  %or29.i.i.i = or i64 %and25.i.i.i, %11
  %or17.i.i.i = or i64 %or29.i.i.i, %and13.i.i.i
  %or30.i.i.i = or i64 %or17.i.i.i, %shl22.i.i.i
  store i64 %4, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i49 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i49 to i16*
  store i16 %conv.i46, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %12 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i50 = add i64 %12, -2
  store i64 %sub.i50, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i51 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i50
  %value.addr.0.arrayidx.sroa_cast.i52 = bitcast i8* %arrayidx.i51 to i16*
  store i16 %conv2.i, i16* %value.addr.0.arrayidx.sroa_cast.i52, align 1
  %13 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i53 = add i64 %13, -8
  store i64 %sub.i53, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i54 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i53
  %value.addr.0.arrayidx.sroa_cast.i55 = bitcast i8* %arrayidx.i54 to i64*
  store i64 %or30.i.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i55, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IMULIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i33 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i34 = bitcast i8* %arrayidx.i33 to i16*
  %value.0.copyload.i35 = load i16, i16* %value.0.arrayidx.sroa_cast.i34, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i36 = trunc i64 %3 to i16
  store i16 %conv.i.i36, i16* %value.0.arrayidx.sroa_cast.i34, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i37 = add i64 %4, 2
  store i64 %add.i37, i64* %vsp, align 8, !tbaa !4
  %sext = shl i16 %value.0.copyload.i, 8
  %conv.i39 = ashr exact i16 %sext, 8
  %sext51 = shl i16 %value.0.copyload.i35, 8
  %conv.i40 = ashr exact i16 %sext51, 8
  %mul.i = mul nsw i16 %conv.i40, %conv.i39
  %conv.i41 = trunc i16 %mul.i to i8
  %5 = lshr i16 %mul.i, 8
  %conv2.i = trunc i16 %5 to i8
  %6 = add nsw i16 %mul.i, 128
  %7 = icmp ugt i16 %6, 255
  %8 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i.i = trunc i64 %8 to i8
  %tobool.not.i.i = icmp eq i8 %conv.i.i.i, 0
  %conv.i24.i.i = zext i1 %7 to i64
  %and13.i.i.i = select i1 %tobool.not.i.i, i64 0, i64 20
  %shl22.i.i.i = select i1 %tobool.not.i.i, i64 0, i64 64
  %9 = and i8 %conv.i41, -128
  %10 = zext i8 %9 to i64
  %shl34.i.i.i = select i1 %7, i64 2048, i64 0
  %and25.i.i.i = or i64 %10, %conv.i24.i.i
  %or29.i.i.i = or i64 %and25.i.i.i, %shl34.i.i.i
  %or17.i.i.i = or i64 %or29.i.i.i, %and13.i.i.i
  %or30.i.i.i = or i64 %or17.i.i.i, %shl22.i.i.i
  %sub.i = add i64 %4, 1
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i44 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  store i8 %conv.i41, i8* %arrayidx.i44, align 1
  %11 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i45 = add i64 %11, -1
  store i64 %sub.i45, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i46 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i45
  store i8 %conv2.i, i8* %arrayidx.i46, align 1
  %12 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i47 = add i64 %12, -8
  store i64 %sub.i47, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i48 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i47
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i48 to i64*
  store i64 %or30.i.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3NORImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i29 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i30 = bitcast i8* %arrayidx.i29 to i64*
  %value.0.copyload.i31 = load i64, i64* %value.0.arrayidx.sroa_cast.i30, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %3, i64* %value.0.arrayidx.sroa_cast.i30, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %or.i = or i64 %value.0.copyload.i31, %value.0.copyload.i
  %neg.i = xor i64 %or.i, -1
  %conv.i.i.i = trunc i64 %neg.i to i8
  %5 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i.i) #3, !range !11
  %cmp.i.i.i = icmp eq i64 %or.i, -1
  %6 = shl nuw nsw i8 %5, 2
  %7 = and i8 %6, 4
  %8 = xor i8 %7, 4
  %9 = zext i8 %8 to i64
  %shl22.i.i = select i1 %cmp.i.i.i, i64 64, i64 0
  %10 = lshr i64 %neg.i, 56
  %11 = and i64 %10, 128
  %or17.i.i = or i64 %11, %shl22.i.i
  %and25.i.i = or i64 %or17.i.i, %9
  %arrayidx.i33 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i33 to i64*
  store i64 %neg.i, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %12 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i34 = add i64 %12, -8
  store i64 %sub.i34, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i35 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i34
  %value.addr.0.arrayidx.sroa_cast.i36 = bitcast i8* %arrayidx.i35 to i64*
  store i64 %and25.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i36, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3NORIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  %value.0.copyload.i = load i32, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i32
  store i32 %conv.i.i, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 4
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i29 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i30 = bitcast i8* %arrayidx.i29 to i32*
  %value.0.copyload.i31 = load i32, i32* %value.0.arrayidx.sroa_cast.i30, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i32 = trunc i64 %3 to i32
  store i32 %conv.i.i32, i32* %value.0.arrayidx.sroa_cast.i30, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %or3.i = or i32 %value.0.copyload.i31, %value.0.copyload.i
  %neg.i = xor i32 %or3.i, -1
  %conv.i.i.i = trunc i32 %neg.i to i8
  %5 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i.i) #3, !range !11
  %cmp.i.i.i = icmp eq i32 %or3.i, -1
  %6 = shl nuw nsw i8 %5, 2
  %7 = and i8 %6, 4
  %8 = xor i8 %7, 4
  %9 = zext i8 %8 to i64
  %shl22.i.i = select i1 %cmp.i.i.i, i64 64, i64 0
  %10 = lshr i32 %neg.i, 24
  %11 = and i32 %10, 128
  %12 = zext i32 %11 to i64
  %or17.i.i = or i64 %shl22.i.i, %12
  %and25.i.i = or i64 %or17.i.i, %9
  %arrayidx.i34 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i34 to i32*
  store i32 %neg.i, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %13 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i35 = add i64 %13, -8
  store i64 %sub.i35, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i36 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i35
  %value.addr.0.arrayidx.sroa_cast.i37 = bitcast i8* %arrayidx.i36 to i64*
  store i64 %and25.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i37, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3NORItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i29 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i30 = bitcast i8* %arrayidx.i29 to i16*
  %value.0.copyload.i31 = load i16, i16* %value.0.arrayidx.sroa_cast.i30, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i32 = trunc i64 %3 to i16
  store i16 %conv.i.i32, i16* %value.0.arrayidx.sroa_cast.i30, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %or3.i = or i16 %value.0.copyload.i31, %value.0.copyload.i
  %neg.i = xor i16 %or3.i, -1
  %conv.i.i.i = trunc i16 %neg.i to i8
  %5 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i.i) #3, !range !11
  %cmp.i.i.i = icmp eq i16 %or3.i, -1
  %6 = shl nuw nsw i8 %5, 2
  %7 = and i8 %6, 4
  %8 = xor i8 %7, 4
  %9 = zext i8 %8 to i64
  %shl22.i.i = select i1 %cmp.i.i.i, i64 64, i64 0
  %10 = lshr i16 %neg.i, 8
  %11 = and i16 %10, 128
  %12 = zext i16 %11 to i64
  %or17.i.i = or i64 %shl22.i.i, %12
  %and25.i.i = or i64 %or17.i.i, %9
  %arrayidx.i34 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i34 to i16*
  store i16 %neg.i, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %13 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i35 = add i64 %13, -8
  store i64 %sub.i35, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i36 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i35
  %value.addr.0.arrayidx.sroa_cast.i37 = bitcast i8* %arrayidx.i36 to i64*
  store i64 %and25.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i37, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3NORIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i24 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i25 = bitcast i8* %arrayidx.i24 to i16*
  %value.0.copyload.i26 = load i16, i16* %value.0.arrayidx.sroa_cast.i25, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i27 = trunc i64 %3 to i16
  store i16 %conv.i.i27, i16* %value.0.arrayidx.sroa_cast.i25, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %or3.i37 = or i16 %value.0.copyload.i26, %value.0.copyload.i
  %or3.i = trunc i16 %or3.i37 to i8
  %neg.i = xor i8 %or3.i, -1
  %5 = tail call i8 @llvm.ctpop.i8(i8 %neg.i) #3, !range !11
  %cmp.i.i.i = icmp eq i8 %or3.i, -1
  %6 = shl nuw nsw i8 %5, 2
  %7 = and i8 %6, 4
  %8 = xor i8 %7, 4
  %9 = zext i8 %8 to i64
  %shl22.i.i = select i1 %cmp.i.i.i, i64 64, i64 0
  %10 = and i8 %neg.i, -128
  %11 = zext i8 %10 to i64
  %or17.i.i = or i64 %shl22.i.i, %11
  %and25.i.i = or i64 %or17.i.i, %9
  %conv.i30 = zext i8 %neg.i to i16
  %arrayidx.i31 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i31 to i16*
  store i16 %conv.i30, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %12 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i32 = add i64 %12, -8
  store i64 %sub.i32, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i33 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i32
  %value.addr.0.arrayidx.sroa_cast.i34 = bitcast i8* %arrayidx.i33 to i64*
  store i64 %and25.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i34, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4NANDImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i29 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i30 = bitcast i8* %arrayidx.i29 to i64*
  %value.0.copyload.i31 = load i64, i64* %value.0.arrayidx.sroa_cast.i30, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %3, i64* %value.0.arrayidx.sroa_cast.i30, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %and.i = and i64 %value.0.copyload.i31, %value.0.copyload.i
  %neg.i = xor i64 %and.i, -1
  %conv.i.i.i.i = trunc i64 %neg.i to i8
  %5 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i.i.i) #3, !range !11
  %cmp.i.i.i.i = icmp eq i64 %and.i, -1
  %6 = shl nuw nsw i8 %5, 2
  %7 = and i8 %6, 4
  %8 = xor i8 %7, 4
  %9 = zext i8 %8 to i64
  %shl22.i.i.i = select i1 %cmp.i.i.i.i, i64 64, i64 0
  %10 = lshr i64 %neg.i, 56
  %11 = and i64 %10, 128
  %or17.i.i.i = or i64 %11, %shl22.i.i.i
  %and25.i.i.i = or i64 %or17.i.i.i, %9
  %arrayidx.i33 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i33 to i64*
  store i64 %neg.i, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %12 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i34 = add i64 %12, -8
  store i64 %sub.i34, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i35 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i34
  %value.addr.0.arrayidx.sroa_cast.i36 = bitcast i8* %arrayidx.i35 to i64*
  store i64 %and25.i.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i36, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4NANDIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  %value.0.copyload.i = load i32, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i32
  store i32 %conv.i.i, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 4
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i29 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i30 = bitcast i8* %arrayidx.i29 to i32*
  %value.0.copyload.i31 = load i32, i32* %value.0.arrayidx.sroa_cast.i30, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i32 = trunc i64 %3 to i32
  store i32 %conv.i.i32, i32* %value.0.arrayidx.sroa_cast.i30, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %and3.i = and i32 %value.0.copyload.i31, %value.0.copyload.i
  %neg.i = xor i32 %and3.i, -1
  %conv.i.i.i.i = trunc i32 %neg.i to i8
  %5 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i.i.i) #3, !range !11
  %cmp.i.i.i.i = icmp eq i32 %and3.i, -1
  %6 = shl nuw nsw i8 %5, 2
  %7 = and i8 %6, 4
  %8 = xor i8 %7, 4
  %9 = zext i8 %8 to i64
  %shl22.i.i.i = select i1 %cmp.i.i.i.i, i64 64, i64 0
  %10 = lshr i32 %neg.i, 24
  %11 = and i32 %10, 128
  %12 = zext i32 %11 to i64
  %or17.i.i.i = or i64 %shl22.i.i.i, %12
  %and25.i.i.i = or i64 %or17.i.i.i, %9
  %arrayidx.i34 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i34 to i32*
  store i32 %neg.i, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %13 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i35 = add i64 %13, -8
  store i64 %sub.i35, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i36 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i35
  %value.addr.0.arrayidx.sroa_cast.i37 = bitcast i8* %arrayidx.i36 to i64*
  store i64 %and25.i.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i37, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4NANDItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i29 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i30 = bitcast i8* %arrayidx.i29 to i16*
  %value.0.copyload.i31 = load i16, i16* %value.0.arrayidx.sroa_cast.i30, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i32 = trunc i64 %3 to i16
  store i16 %conv.i.i32, i16* %value.0.arrayidx.sroa_cast.i30, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %and3.i = and i16 %value.0.copyload.i31, %value.0.copyload.i
  %neg.i = xor i16 %and3.i, -1
  %conv.i.i.i.i = trunc i16 %neg.i to i8
  %5 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i.i.i) #3, !range !11
  %cmp.i.i.i.i = icmp eq i16 %and3.i, -1
  %6 = shl nuw nsw i8 %5, 2
  %7 = and i8 %6, 4
  %8 = xor i8 %7, 4
  %9 = zext i8 %8 to i64
  %shl22.i.i.i = select i1 %cmp.i.i.i.i, i64 64, i64 0
  %10 = lshr i16 %neg.i, 8
  %11 = and i16 %10, 128
  %12 = zext i16 %11 to i64
  %or17.i.i.i = or i64 %shl22.i.i.i, %12
  %and25.i.i.i = or i64 %or17.i.i.i, %9
  %arrayidx.i34 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i34 to i16*
  store i16 %neg.i, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %13 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i35 = add i64 %13, -8
  store i64 %sub.i35, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i36 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i35
  %value.addr.0.arrayidx.sroa_cast.i37 = bitcast i8* %arrayidx.i36 to i64*
  store i64 %and25.i.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i37, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4NANDIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i24 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i25 = bitcast i8* %arrayidx.i24 to i16*
  %value.0.copyload.i26 = load i16, i16* %value.0.arrayidx.sroa_cast.i25, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i27 = trunc i64 %3 to i16
  store i16 %conv.i.i27, i16* %value.0.arrayidx.sroa_cast.i25, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %and3.i37 = and i16 %value.0.copyload.i26, %value.0.copyload.i
  %and3.i = trunc i16 %and3.i37 to i8
  %neg.i = xor i8 %and3.i, -1
  %5 = tail call i8 @llvm.ctpop.i8(i8 %neg.i) #3, !range !11
  %cmp.i.i.i.i = icmp eq i8 %and3.i, -1
  %6 = shl nuw nsw i8 %5, 2
  %7 = and i8 %6, 4
  %8 = xor i8 %7, 4
  %9 = zext i8 %8 to i64
  %shl22.i.i.i = select i1 %cmp.i.i.i.i, i64 64, i64 0
  %10 = and i8 %neg.i, -128
  %11 = zext i8 %10 to i64
  %or17.i.i.i = or i64 %shl22.i.i.i, %11
  %and25.i.i.i = or i64 %or17.i.i.i, %9
  %conv.i30 = zext i8 %neg.i to i16
  %arrayidx.i31 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i31 to i16*
  store i16 %conv.i30, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %12 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i32 = add i64 %12, -8
  store i64 %sub.i32, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i33 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i32
  %value.addr.0.arrayidx.sroa_cast.i34 = bitcast i8* %arrayidx.i33 to i64*
  store i64 %and25.i.i.i, i64* %value.addr.0.arrayidx.sroa_cast.i34, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHLImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i51 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i52 = bitcast i8* %arrayidx.i51 to i16*
  %value.0.copyload.i53 = load i16, i16* %value.0.arrayidx.sroa_cast.i52, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %3 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i52, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i54 = add i64 %4, 2
  store i64 %add.i54, i64* %vsp, align 8, !tbaa !4
  %conv7 = zext i16 %value.0.copyload.i53 to i64
  %shl = shl i64 %value.0.copyload.i, %conv7
  %and.i.i = and i64 %conv7, 63
  %cmp.i.i = icmp eq i64 %and.i.i, 1
  %cmp.i.i.i = icmp slt i64 %value.0.copyload.i, 0
  %cmp.i.i24.i = icmp slt i64 %shl, 0
  %retval.0.i = select i1 %cmp.i.i, i1 %cmp.i.i.i, i1 %cmp.i.i24.i
  %conv.i.i55 = trunc i64 %shl to i8
  %5 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i55) #3, !range !11
  %6 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i = trunc i64 %6 to i8
  %tobool12.not = icmp eq i8 %conv.i, 0
  %cmp.i.i56 = icmp eq i64 %shl, 0
  %7 = xor i64 %shl, %value.0.copyload.i
  %conv.i58 = zext i1 %retval.0.i to i64
  %8 = shl nuw nsw i8 %5, 2
  %9 = and i8 %8, 4
  %10 = xor i8 %9, 4
  %11 = zext i8 %10 to i64
  %shl16.i = select i1 %tobool12.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i56, i64 64, i64 0
  %12 = lshr i64 %shl, 56
  %13 = and i64 %12, 128
  %14 = lshr i64 %7, 52
  %15 = and i64 %14, 2048
  %and13.i = or i64 %13, %shl22.i
  %or17.i = or i64 %and13.i, %15
  %and25.i = or i64 %or17.i, %conv.i58
  %or29.i = or i64 %and25.i, %11
  %or30.i = or i64 %or29.i, %shl16.i
  %sub.i = add i64 %4, -6
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i59 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i59 to i64*
  store i64 %shl, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %16 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i60 = add i64 %16, -8
  store i64 %sub.i60, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i61 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i60
  %value.addr.0.arrayidx.sroa_cast.i62 = bitcast i8* %arrayidx.i61 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i62, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHLIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  %value.0.copyload.i = load i32, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i32
  store i32 %conv.i.i, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 4
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i51 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i52 = bitcast i8* %arrayidx.i51 to i16*
  %value.0.copyload.i53 = load i16, i16* %value.0.arrayidx.sroa_cast.i52, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i54 = trunc i64 %3 to i16
  store i16 %conv.i.i54, i16* %value.0.arrayidx.sroa_cast.i52, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i55 = add i64 %4, 2
  store i64 %add.i55, i64* %vsp, align 8, !tbaa !4
  %conv7 = zext i16 %value.0.copyload.i53 to i32
  %shl = shl i32 %value.0.copyload.i, %conv7
  %and3.i.i = and i32 %conv7, 31
  %cmp.i.i = icmp eq i32 %and3.i.i, 1
  %cmp.i.i.i = icmp slt i32 %value.0.copyload.i, 0
  %cmp.i.i24.i = icmp slt i32 %shl, 0
  %retval.0.i = select i1 %cmp.i.i, i1 %cmp.i.i.i, i1 %cmp.i.i24.i
  %conv.i.i56 = trunc i32 %shl to i8
  %5 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i56) #3, !range !11
  %6 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i = trunc i64 %6 to i8
  %tobool12.not = icmp eq i8 %conv.i, 0
  %cmp.i.i57 = icmp eq i32 %shl, 0
  %7 = xor i32 %shl, %value.0.copyload.i
  %conv.i59 = zext i1 %retval.0.i to i64
  %8 = shl nuw nsw i8 %5, 2
  %9 = and i8 %8, 4
  %10 = xor i8 %9, 4
  %11 = zext i8 %10 to i64
  %shl16.i = select i1 %tobool12.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i57, i64 64, i64 0
  %12 = lshr i32 %shl, 24
  %13 = and i32 %12, 128
  %14 = zext i32 %13 to i64
  %15 = lshr i32 %7, 20
  %16 = and i32 %15, 2048
  %17 = zext i32 %16 to i64
  %and13.i = or i64 %shl22.i, %14
  %or17.i = or i64 %and13.i, %conv.i59
  %and25.i = or i64 %or17.i, %17
  %or29.i = or i64 %and25.i, %11
  %or30.i = or i64 %or29.i, %shl16.i
  %sub.i = add i64 %4, -2
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i60 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i60 to i32*
  store i32 %shl, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %18 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i61 = add i64 %18, -8
  store i64 %sub.i61, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i62 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i61
  %value.addr.0.arrayidx.sroa_cast.i63 = bitcast i8* %arrayidx.i62 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i63, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHLItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i55 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i56 = bitcast i8* %arrayidx.i55 to i16*
  %value.0.copyload.i57 = load i16, i16* %value.0.arrayidx.sroa_cast.i56, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i58 = trunc i64 %3 to i16
  store i16 %conv.i.i58, i16* %value.0.arrayidx.sroa_cast.i56, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i59 = add i64 %4, 2
  store i64 %add.i59, i64* %vsp, align 8, !tbaa !4
  %conv7 = zext i16 %value.0.copyload.i to i32
  %conv8 = zext i16 %value.0.copyload.i57 to i32
  %shl = shl i32 %conv7, %conv8
  %conv9 = trunc i32 %shl to i16
  %and3.i.i = and i16 %value.0.copyload.i57, 31
  %cmp.i.i = icmp eq i16 %and3.i.i, 1
  br i1 %cmp.i.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %entry
  %cmp.i.i.i = icmp slt i16 %value.0.copyload.i, 0
  br label %_Z6CF_SHLItEbT_S0_S0_.exit

if.else.i:                                        ; preds = %entry
  %cmp.i23.i = icmp ult i16 %and3.i.i, 16
  br i1 %cmp.i23.i, label %if.then7.i, label %if.end9.i

if.then7.i:                                       ; preds = %if.else.i
  %cmp.i.i24.i = icmp slt i16 %conv9, 0
  br label %_Z6CF_SHLItEbT_S0_S0_.exit

if.end9.i:                                        ; preds = %if.else.i
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i60 = trunc i64 %5 to i8
  %tobool.i = icmp ne i8 %conv.i.i60, 0
  br label %_Z6CF_SHLItEbT_S0_S0_.exit

_Z6CF_SHLItEbT_S0_S0_.exit:                       ; preds = %if.then.i, %if.then7.i, %if.end9.i
  %retval.0.i = phi i1 [ %cmp.i.i.i, %if.then.i ], [ %cmp.i.i24.i, %if.then7.i ], [ %tobool.i, %if.end9.i ]
  %conv.i.i61 = trunc i32 %shl to i8
  %6 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i61) #3, !range !11
  %7 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i = trunc i64 %7 to i8
  %tobool14.not = icmp eq i8 %conv.i, 0
  %cmp.i.i62 = icmp eq i32 %shl, 0
  %8 = xor i16 %value.0.copyload.i, %conv9
  %conv.i64 = zext i1 %retval.0.i to i64
  %9 = shl nuw nsw i8 %6, 2
  %10 = and i8 %9, 4
  %11 = xor i8 %10, 4
  %12 = zext i8 %11 to i64
  %shl16.i = select i1 %tobool14.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i62, i64 64, i64 0
  %13 = lshr i32 %shl, 24
  %14 = and i32 %13, 128
  %15 = zext i32 %14 to i64
  %16 = lshr i16 %8, 4
  %17 = and i16 %16, 2048
  %18 = zext i16 %17 to i64
  %and13.i = or i64 %shl22.i, %15
  %or17.i = or i64 %and13.i, %18
  %and25.i = or i64 %or17.i, %12
  %or29.i = or i64 %and25.i, %conv.i64
  %or30.i = or i64 %or29.i, %shl16.i
  store i64 %4, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i65 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i65 to i16*
  store i16 %conv9, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %19 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i66 = add i64 %19, -8
  store i64 %sub.i66, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i67 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i66
  %value.addr.0.arrayidx.sroa_cast.i68 = bitcast i8* %arrayidx.i67 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i68, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHLIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %conv.i = trunc i16 %value.0.copyload.i to i8
  %arrayidx.i52 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i53 = bitcast i8* %arrayidx.i52 to i16*
  %value.0.copyload.i54 = load i16, i16* %value.0.arrayidx.sroa_cast.i53, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i55 = trunc i64 %3 to i16
  store i16 %conv.i.i55, i16* %value.0.arrayidx.sroa_cast.i53, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i56 = add i64 %4, 2
  store i64 %add.i56, i64* %vsp, align 8, !tbaa !4
  %conv.i57 = trunc i16 %value.0.copyload.i54 to i8
  %conv.i.mask = and i16 %value.0.copyload.i, 255
  %conv6 = zext i16 %conv.i.mask to i32
  %conv.i57.mask = and i16 %value.0.copyload.i54, 255
  %conv7 = zext i16 %conv.i57.mask to i32
  %shl = shl i32 %conv6, %conv7
  %conv8 = trunc i32 %shl to i8
  %and3.i.i = and i8 %conv.i57, 31
  %cmp.i.i = icmp eq i8 %and3.i.i, 1
  br i1 %cmp.i.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %entry
  %cmp.i.i.i = icmp slt i8 %conv.i, 0
  br label %_Z6CF_SHLIhEbT_S0_S0_.exit

if.else.i:                                        ; preds = %entry
  %cmp.i18.i = icmp ult i8 %and3.i.i, 8
  br i1 %cmp.i18.i, label %if.then7.i, label %if.end9.i

if.then7.i:                                       ; preds = %if.else.i
  %cmp.i.i19.i = icmp slt i8 %conv8, 0
  br label %_Z6CF_SHLIhEbT_S0_S0_.exit

if.end9.i:                                        ; preds = %if.else.i
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i58 = trunc i64 %5 to i8
  %tobool.i = icmp ne i8 %conv.i.i58, 0
  br label %_Z6CF_SHLIhEbT_S0_S0_.exit

_Z6CF_SHLIhEbT_S0_S0_.exit:                       ; preds = %if.then.i, %if.then7.i, %if.end9.i
  %retval.0.i = phi i1 [ %cmp.i.i.i, %if.then.i ], [ %cmp.i.i19.i, %if.then7.i ], [ %tobool.i, %if.end9.i ]
  %6 = tail call i8 @llvm.ctpop.i8(i8 %conv8) #3, !range !11
  %7 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i60 = trunc i64 %7 to i8
  %tobool13.not = icmp eq i8 %conv.i60, 0
  %cmp.i.i61 = icmp eq i32 %shl, 0
  %8 = xor i8 %conv8, %conv.i
  %conv.i63 = zext i1 %retval.0.i to i64
  %9 = shl nuw nsw i8 %6, 2
  %10 = and i8 %9, 4
  %11 = xor i8 %10, 4
  %12 = zext i8 %11 to i64
  %shl16.i = select i1 %tobool13.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i61, i64 64, i64 0
  %13 = lshr i32 %shl, 24
  %14 = and i32 %13, 128
  %15 = zext i32 %14 to i64
  %16 = and i8 %8, -128
  %17 = zext i8 %16 to i64
  %18 = shl nuw nsw i64 %17, 4
  %and25.i = or i64 %shl22.i, %15
  %and13.i = or i64 %18, %and25.i
  %or17.i = or i64 %and13.i, %12
  %or29.i = or i64 %or17.i, %conv.i63
  %or30.i = or i64 %or29.i, %shl16.i
  %conv.i64 = trunc i32 %shl to i16
  store i64 %4, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i65 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i65 to i16*
  store i16 %conv.i64, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %19 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i66 = add i64 %19, -8
  store i64 %sub.i66, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i67 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i66
  %value.addr.0.arrayidx.sroa_cast.i68 = bitcast i8* %arrayidx.i67 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i68, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHRImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i47 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i48 = bitcast i8* %arrayidx.i47 to i16*
  %value.0.copyload.i49 = load i16, i16* %value.0.arrayidx.sroa_cast.i48, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %3 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i48, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i50 = add i64 %4, 2
  store i64 %add.i50, i64* %vsp, align 8, !tbaa !4
  %conv7 = zext i16 %value.0.copyload.i49 to i64
  %shr = lshr i64 %value.0.copyload.i, %conv7
  %and.i.i = and i64 %conv7, 63
  %cmp.i.i = icmp eq i64 %and.i.i, 1
  %and.i25.i = and i64 %value.0.copyload.i, 1
  %cmp.i26.i = icmp ne i64 %and.i25.i, 0
  %and.i28.i = and i64 %shr, 1
  %cmp.i29.i = icmp ne i64 %and.i28.i, 0
  %retval.0.i = select i1 %cmp.i.i, i1 %cmp.i26.i, i1 %cmp.i29.i
  %conv.i.i51 = trunc i64 %shr to i8
  %5 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i51) #3, !range !11
  %6 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i = trunc i64 %6 to i8
  %tobool12.not = icmp eq i8 %conv.i, 0
  %cmp.i.i52 = icmp eq i64 %shr, 0
  %conv.i53 = zext i1 %retval.0.i to i64
  %7 = shl nuw nsw i8 %5, 2
  %8 = and i8 %7, 4
  %9 = xor i8 %8, 4
  %10 = zext i8 %9 to i64
  %shl16.i = select i1 %tobool12.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i52, i64 64, i64 0
  %11 = lshr i64 %value.0.copyload.i, 52
  %12 = and i64 %11, 2048
  %and13.i = or i64 %shl22.i, %12
  %or17.i = or i64 %and13.i, %conv.i53
  %or29.i = or i64 %or17.i, %10
  %or30.i = or i64 %or29.i, %shl16.i
  %sub.i = add i64 %4, -6
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i54 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i54 to i64*
  store i64 %shr, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %13 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i55 = add i64 %13, -8
  store i64 %sub.i55, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i56 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i55
  %value.addr.0.arrayidx.sroa_cast.i57 = bitcast i8* %arrayidx.i56 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i57, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHRIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  %value.0.copyload.i = load i32, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i32
  store i32 %conv.i.i, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 4
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i47 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i48 = bitcast i8* %arrayidx.i47 to i16*
  %value.0.copyload.i49 = load i16, i16* %value.0.arrayidx.sroa_cast.i48, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i50 = trunc i64 %3 to i16
  store i16 %conv.i.i50, i16* %value.0.arrayidx.sroa_cast.i48, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i51 = add i64 %4, 2
  store i64 %add.i51, i64* %vsp, align 8, !tbaa !4
  %conv7 = zext i16 %value.0.copyload.i49 to i32
  %shr = lshr i32 %value.0.copyload.i, %conv7
  %and3.i.i = and i32 %conv7, 31
  %cmp.i.i = icmp eq i32 %and3.i.i, 1
  %and3.i25.i = and i32 %value.0.copyload.i, 1
  %cmp.i26.i = icmp ne i32 %and3.i25.i, 0
  %and3.i28.i = and i32 %shr, 1
  %cmp.i29.i = icmp ne i32 %and3.i28.i, 0
  %retval.0.i = select i1 %cmp.i.i, i1 %cmp.i26.i, i1 %cmp.i29.i
  %conv.i.i52 = trunc i32 %shr to i8
  %5 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i52) #3, !range !11
  %6 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i = trunc i64 %6 to i8
  %tobool12.not = icmp eq i8 %conv.i, 0
  %cmp.i.i53 = icmp eq i32 %shr, 0
  %conv.i54 = zext i1 %retval.0.i to i64
  %7 = shl nuw nsw i8 %5, 2
  %8 = and i8 %7, 4
  %9 = xor i8 %8, 4
  %10 = zext i8 %9 to i64
  %shl16.i = select i1 %tobool12.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i53, i64 64, i64 0
  %11 = lshr i32 %value.0.copyload.i, 20
  %12 = and i32 %11, 2048
  %13 = zext i32 %12 to i64
  %and13.i = or i64 %shl22.i, %13
  %or17.i = or i64 %and13.i, %conv.i54
  %or29.i = or i64 %or17.i, %10
  %or30.i = or i64 %or29.i, %shl16.i
  %sub.i = add i64 %4, -2
  store i64 %sub.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i55 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i55 to i32*
  store i32 %shr, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %14 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i56 = add i64 %14, -8
  store i64 %sub.i56, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i57 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i56
  %value.addr.0.arrayidx.sroa_cast.i58 = bitcast i8* %arrayidx.i57 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i58, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHRItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i50 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i51 = bitcast i8* %arrayidx.i50 to i16*
  %value.0.copyload.i52 = load i16, i16* %value.0.arrayidx.sroa_cast.i51, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i53 = trunc i64 %3 to i16
  store i16 %conv.i.i53, i16* %value.0.arrayidx.sroa_cast.i51, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i54 = add i64 %4, 2
  store i64 %add.i54, i64* %vsp, align 8, !tbaa !4
  %conv7 = zext i16 %value.0.copyload.i to i32
  %conv8 = zext i16 %value.0.copyload.i52 to i32
  %shr = lshr i32 %conv7, %conv8
  %conv9 = trunc i32 %shr to i16
  %and3.i.i = and i16 %value.0.copyload.i52, 31
  %cmp.i.i = icmp eq i16 %and3.i.i, 1
  br i1 %cmp.i.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %entry
  %and3.i25.i = and i16 %value.0.copyload.i, 1
  %cmp.i26.i = icmp ne i16 %and3.i25.i, 0
  br label %_Z6CF_SHRItEbT_S0_S0_.exit

if.else.i:                                        ; preds = %entry
  %cmp.i27.i = icmp ult i16 %and3.i.i, 16
  br i1 %cmp.i27.i, label %if.then8.i, label %if.end11.i

if.then8.i:                                       ; preds = %if.else.i
  %and3.i28.i = and i16 %conv9, 1
  %cmp.i29.i = icmp ne i16 %and3.i28.i, 0
  br label %_Z6CF_SHRItEbT_S0_S0_.exit

if.end11.i:                                       ; preds = %if.else.i
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i55 = trunc i64 %5 to i8
  %tobool.i = icmp ne i8 %conv.i.i55, 0
  br label %_Z6CF_SHRItEbT_S0_S0_.exit

_Z6CF_SHRItEbT_S0_S0_.exit:                       ; preds = %if.then.i, %if.then8.i, %if.end11.i
  %retval.0.i = phi i1 [ %cmp.i26.i, %if.then.i ], [ %cmp.i29.i, %if.then8.i ], [ %tobool.i, %if.end11.i ]
  %conv.i.i56 = trunc i32 %shr to i8
  %6 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i56) #3, !range !11
  %7 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i = trunc i64 %7 to i8
  %tobool14.not = icmp eq i8 %conv.i, 0
  %cmp.i.i57 = icmp eq i32 %shr, 0
  %conv.i58 = zext i1 %retval.0.i to i64
  %8 = shl nuw nsw i8 %6, 2
  %9 = and i8 %8, 4
  %10 = xor i8 %9, 4
  %11 = zext i8 %10 to i64
  %shl16.i = select i1 %tobool14.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i57, i64 64, i64 0
  %12 = lshr i16 %value.0.copyload.i, 4
  %13 = and i16 %12, 2048
  %14 = zext i16 %13 to i64
  %and13.i = or i64 %shl22.i, %14
  %or17.i = or i64 %and13.i, %11
  %or29.i = or i64 %or17.i, %conv.i58
  %or30.i = or i64 %or29.i, %shl16.i
  store i64 %4, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i59 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i59 to i16*
  store i16 %conv9, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %15 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i60 = add i64 %15, -8
  store i64 %sub.i60, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i61 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i60
  %value.addr.0.arrayidx.sroa_cast.i62 = bitcast i8* %arrayidx.i61 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i62, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHRIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i47 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i48 = bitcast i8* %arrayidx.i47 to i16*
  %value.0.copyload.i49 = load i16, i16* %value.0.arrayidx.sroa_cast.i48, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i50 = trunc i64 %3 to i16
  store i16 %conv.i.i50, i16* %value.0.arrayidx.sroa_cast.i48, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i51 = add i64 %4, 2
  store i64 %add.i51, i64* %vsp, align 8, !tbaa !4
  %conv.i52 = trunc i16 %value.0.copyload.i49 to i8
  %conv.i.mask = and i16 %value.0.copyload.i, 255
  %conv6 = zext i16 %conv.i.mask to i32
  %conv.i52.mask = and i16 %value.0.copyload.i49, 255
  %conv7 = zext i16 %conv.i52.mask to i32
  %shr = lshr i32 %conv6, %conv7
  %conv8 = trunc i32 %shr to i8
  %and3.i.i = and i8 %conv.i52, 31
  %cmp.i.i = icmp eq i8 %and3.i.i, 1
  br i1 %cmp.i.i, label %if.then.i, label %if.else.i

if.then.i:                                        ; preds = %entry
  %conv.i = trunc i16 %value.0.copyload.i to i8
  %and3.i20.i = and i8 %conv.i, 1
  br label %_Z6CF_SHRIhEbT_S0_S0_.exit

if.else.i:                                        ; preds = %entry
  %cmp.i22.i = icmp ult i8 %and3.i.i, 8
  br i1 %cmp.i22.i, label %if.then8.i, label %if.end11.i

if.then8.i:                                       ; preds = %if.else.i
  %and3.i23.i = and i8 %conv8, 1
  br label %_Z6CF_SHRIhEbT_S0_S0_.exit

if.end11.i:                                       ; preds = %if.else.i
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i53 = trunc i64 %5 to i8
  br label %_Z6CF_SHRIhEbT_S0_S0_.exit

_Z6CF_SHRIhEbT_S0_S0_.exit:                       ; preds = %if.then.i, %if.then8.i, %if.end11.i
  %retval.0.in.i = phi i8 [ %and3.i20.i, %if.then.i ], [ %and3.i23.i, %if.then8.i ], [ %conv.i.i53, %if.end11.i ]
  %retval.0.i = icmp ne i8 %retval.0.in.i, 0
  %6 = tail call i8 @llvm.ctpop.i8(i8 %conv8) #3, !range !11
  %7 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i55 = trunc i64 %7 to i8
  %tobool13.not = icmp eq i8 %conv.i55, 0
  %cmp.i.i56 = icmp eq i32 %shr, 0
  %conv.i57 = zext i1 %retval.0.i to i64
  %8 = shl nuw nsw i8 %6, 2
  %9 = and i8 %8, 4
  %10 = xor i8 %9, 4
  %11 = zext i8 %10 to i64
  %shl16.i = select i1 %tobool13.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i56, i64 64, i64 0
  %12 = shl i16 %value.0.copyload.i, 4
  %13 = and i16 %12, 2048
  %14 = zext i16 %13 to i64
  %and13.i = or i64 %shl22.i, %14
  %or17.i = or i64 %and13.i, %11
  %or29.i = or i64 %or17.i, %conv.i57
  %or30.i = or i64 %or29.i, %shl16.i
  %conv.i58 = trunc i32 %shr to i16
  store i64 %4, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i59 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %4
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i59 to i16*
  store i16 %conv.i58, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %15 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i60 = add i64 %15, -8
  store i64 %sub.i60, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i61 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i60
  %value.addr.0.arrayidx.sroa_cast.i62 = bitcast i8* %arrayidx.i61 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i62, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHLDImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i53 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i54 = bitcast i8* %arrayidx.i53 to i64*
  %value.0.copyload.i55 = load i64, i64* %value.0.arrayidx.sroa_cast.i54, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %3, i64* %value.0.arrayidx.sroa_cast.i54, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i56 = add i64 %4, 8
  store i64 %add.i56, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i57 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i56
  %value.0.arrayidx.sroa_cast.i58 = bitcast i8* %arrayidx.i57 to i16*
  %value.0.copyload.i59 = load i16, i16* %value.0.arrayidx.sroa_cast.i58, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %5 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i58, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i60 = add i64 %6, 2
  store i64 %add.i60, i64* %vsp, align 8, !tbaa !4
  %7 = and i16 %value.0.copyload.i59, 63
  %and.i = zext i16 %7 to i64
  %shl.i = shl i64 %value.0.copyload.i, %and.i
  %sub.i = sub nuw nsw i64 64, %and.i
  %shr.i = lshr i64 %value.0.copyload.i55, %sub.i
  %or.i = or i64 %shr.i, %shl.i
  %conv.i.i61 = trunc i64 %or.i to i8
  %8 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i61) #3, !range !11
  %9 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i = trunc i64 %9 to i8
  %tobool.not = icmp eq i8 %conv.i, 0
  %cmp.i.i = icmp eq i64 %or.i, 0
  %10 = xor i64 %or.i, %value.0.copyload.i
  %11 = lshr i64 %value.0.copyload.i, %sub.i
  %12 = and i64 %11, 1
  %13 = shl nuw nsw i8 %8, 2
  %14 = and i8 %13, 4
  %15 = xor i8 %14, 4
  %16 = zext i8 %15 to i64
  %shl16.i = select i1 %tobool.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i, i64 64, i64 0
  %17 = lshr i64 %or.i, 56
  %18 = and i64 %17, 128
  %19 = lshr i64 %10, 52
  %20 = and i64 %19, 2048
  %and13.i = or i64 %shl22.i, %12
  %or17.i = or i64 %and13.i, %18
  %and25.i = or i64 %or17.i, %20
  %or29.i = or i64 %and25.i, %shl16.i
  %or30.i = or i64 %or29.i, %16
  %sub.i64 = add i64 %6, -6
  store i64 %sub.i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i65 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i64
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i65 to i64*
  store i64 %or.i, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %21 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i66 = add i64 %21, -8
  store i64 %sub.i66, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i67 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i66
  %value.addr.0.arrayidx.sroa_cast.i68 = bitcast i8* %arrayidx.i67 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i68, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHLDIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  %value.0.copyload.i = load i32, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i32
  store i32 %conv.i.i, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 4
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i53 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i54 = bitcast i8* %arrayidx.i53 to i32*
  %value.0.copyload.i55 = load i32, i32* %value.0.arrayidx.sroa_cast.i54, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i56 = trunc i64 %3 to i32
  store i32 %conv.i.i56, i32* %value.0.arrayidx.sroa_cast.i54, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i57 = add i64 %4, 4
  store i64 %add.i57, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i58 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i57
  %value.0.arrayidx.sroa_cast.i59 = bitcast i8* %arrayidx.i58 to i16*
  %value.0.copyload.i60 = load i16, i16* %value.0.arrayidx.sroa_cast.i59, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i61 = trunc i64 %5 to i16
  store i16 %conv.i.i61, i16* %value.0.arrayidx.sroa_cast.i59, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i62 = add i64 %6, 2
  store i64 %add.i62, i64* %vsp, align 8, !tbaa !4
  %7 = and i16 %value.0.copyload.i60, 31
  %conv.i = zext i32 %value.0.copyload.i to i64
  %conv1.i = zext i16 %7 to i32
  %shl.i = shl i32 %value.0.copyload.i, %conv1.i
  %narrow = sub nuw nsw i16 32, %7
  %conv.i63 = zext i32 %value.0.copyload.i55 to i64
  %conv1.i64 = zext i16 %narrow to i64
  %shr.i = lshr i64 %conv.i63, %conv1.i64
  %conv2.i65 = trunc i64 %shr.i to i32
  %or3.i = or i32 %shl.i, %conv2.i65
  %conv.i.i66 = trunc i32 %or3.i to i8
  %8 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i66) #3, !range !11
  %9 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i67 = trunc i64 %9 to i8
  %tobool.not = icmp eq i8 %conv.i67, 0
  %cmp.i.i = icmp eq i32 %or3.i, 0
  %10 = xor i32 %or3.i, %value.0.copyload.i
  %11 = lshr i64 %conv.i, %conv1.i64
  %12 = and i64 %11, 1
  %13 = shl nuw nsw i8 %8, 2
  %14 = and i8 %13, 4
  %15 = xor i8 %14, 4
  %16 = zext i8 %15 to i64
  %shl16.i = select i1 %tobool.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i, i64 64, i64 0
  %17 = lshr i32 %or3.i, 24
  %18 = and i32 %17, 128
  %19 = zext i32 %18 to i64
  %20 = lshr i32 %10, 20
  %21 = and i32 %20, 2048
  %22 = zext i32 %21 to i64
  %and13.i = or i64 %shl22.i, %12
  %or17.i = or i64 %and13.i, %shl16.i
  %and25.i = or i64 %or17.i, %19
  %or29.i = or i64 %and25.i, %22
  %or30.i = or i64 %or29.i, %16
  %sub.i70 = add i64 %6, -2
  store i64 %sub.i70, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i71 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i70
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i71 to i32*
  store i32 %or3.i, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %23 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i72 = add i64 %23, -8
  store i64 %sub.i72, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i73 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i72
  %value.addr.0.arrayidx.sroa_cast.i74 = bitcast i8* %arrayidx.i73 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i74, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHLDItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i53 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i54 = bitcast i8* %arrayidx.i53 to i16*
  %value.0.copyload.i55 = load i16, i16* %value.0.arrayidx.sroa_cast.i54, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i56 = trunc i64 %3 to i16
  store i16 %conv.i.i56, i16* %value.0.arrayidx.sroa_cast.i54, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i57 = add i64 %4, 2
  store i64 %add.i57, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i58 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i57
  %value.0.arrayidx.sroa_cast.i59 = bitcast i8* %arrayidx.i58 to i16*
  %value.0.copyload.i60 = load i16, i16* %value.0.arrayidx.sroa_cast.i59, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i61 = trunc i64 %5 to i16
  store i16 %conv.i.i61, i16* %value.0.arrayidx.sroa_cast.i59, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i62 = add i64 %6, 2
  store i64 %add.i62, i64* %vsp, align 8, !tbaa !4
  %and3.i = and i16 %value.0.copyload.i60, 31
  %conv.i = zext i16 %value.0.copyload.i to i64
  %conv1.i = zext i16 %and3.i to i64
  %shl.i = shl nuw nsw i64 %conv.i, %conv1.i
  %sub.i = sub nsw i16 16, %and3.i
  %conv.i63 = zext i16 %value.0.copyload.i55 to i64
  %conv1.i64 = zext i16 %sub.i to i64
  %shr.i = lshr i64 %conv.i63, %conv1.i64
  %or3.i77 = or i64 %shr.i, %shl.i
  %or3.i = trunc i64 %or3.i77 to i16
  %conv.i.i66 = trunc i64 %or3.i77 to i8
  %7 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i66) #3, !range !11
  %8 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i67 = trunc i64 %8 to i8
  %tobool.not = icmp eq i8 %conv.i67, 0
  %cmp.i.i = icmp eq i16 %or3.i, 0
  %9 = xor i16 %value.0.copyload.i, %or3.i
  %10 = lshr i64 %conv.i, %conv1.i64
  %11 = and i64 %10, 1
  %12 = shl nuw nsw i8 %7, 2
  %13 = and i8 %12, 4
  %14 = xor i8 %13, 4
  %15 = zext i8 %14 to i64
  %shl16.i = select i1 %tobool.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i, i64 64, i64 0
  %16 = lshr i64 %or3.i77, 8
  %17 = and i64 %16, 128
  %18 = lshr i16 %9, 4
  %19 = and i16 %18, 2048
  %20 = zext i16 %19 to i64
  %and13.i = or i64 %17, %11
  %or17.i = or i64 %and13.i, %shl22.i
  %and25.i = or i64 %or17.i, %shl16.i
  %or29.i = or i64 %and25.i, %20
  %or30.i = or i64 %or29.i, %15
  store i64 %6, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i71 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %6
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i71 to i16*
  store i16 %or3.i, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %21 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i72 = add i64 %21, -8
  store i64 %sub.i72, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i73 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i72
  %value.addr.0.arrayidx.sroa_cast.i74 = bitcast i8* %arrayidx.i73 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i74, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHLDIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.copyload.i = load i8, i8* %arrayidx.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i8
  store i8 %conv.i.i, i8* %arrayidx.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 1
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i42 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.copyload.i43 = load i8, i8* %arrayidx.i42, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i44 = trunc i64 %3 to i8
  store i8 %conv.i.i44, i8* %arrayidx.i42, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i45 = add i64 %4, 1
  store i64 %add.i45, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i46 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i45
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i46 to i16*
  %value.0.copyload.i47 = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i48 = trunc i64 %5 to i16
  store i16 %conv.i.i48, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i49 = add i64 %6, 2
  store i64 %add.i49, i64* %vsp, align 8, !tbaa !4
  %conv = trunc i16 %value.0.copyload.i47 to i8
  %and3.i = and i8 %conv, 31
  %conv.i = zext i8 %value.0.copyload.i to i64
  %conv1.i = zext i8 %and3.i to i64
  %shl.i = shl nuw nsw i64 %conv.i, %conv1.i
  %sub.i = sub nsw i8 8, %and3.i
  %conv.i50 = zext i8 %value.0.copyload.i43 to i64
  %conv1.i51 = zext i8 %sub.i to i64
  %shr.i = lshr i64 %conv.i50, %conv1.i51
  %or3.i62 = or i64 %shr.i, %shl.i
  %or3.i = trunc i64 %or3.i62 to i8
  %7 = tail call i8 @llvm.ctpop.i8(i8 %or3.i) #3, !range !11
  %8 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i53 = trunc i64 %8 to i8
  %tobool.not = icmp eq i8 %conv.i53, 0
  %cmp.i.i = icmp eq i8 %or3.i, 0
  %9 = xor i8 %value.0.copyload.i, %or3.i
  %10 = lshr i64 %conv.i, %conv1.i51
  %11 = and i64 %10, 1
  %12 = shl nuw nsw i8 %7, 2
  %13 = and i8 %12, 4
  %14 = xor i8 %13, 4
  %15 = zext i8 %14 to i64
  %shl16.i = select i1 %tobool.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i, i64 64, i64 0
  %16 = and i64 %or3.i62, 128
  %17 = and i8 %9, -128
  %18 = zext i8 %17 to i64
  %19 = shl nuw nsw i64 %18, 4
  %and25.i = or i64 %16, %shl22.i
  %and13.i = or i64 %shl16.i, %11
  %or17.i = or i64 %and13.i, %and25.i
  %or29.i = or i64 %or17.i, %19
  %or30.i = or i64 %or29.i, %15
  %sub.i56 = add i64 %6, 1
  store i64 %sub.i56, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i57 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i56
  store i8 %or3.i, i8* %arrayidx.i57, align 1
  %20 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i58 = add i64 %20, -8
  store i64 %sub.i58, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i59 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i58
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i59 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHRDImEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i53 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i54 = bitcast i8* %arrayidx.i53 to i64*
  %value.0.copyload.i55 = load i64, i64* %value.0.arrayidx.sroa_cast.i54, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %3, i64* %value.0.arrayidx.sroa_cast.i54, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i56 = add i64 %4, 8
  store i64 %add.i56, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i57 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i56
  %value.0.arrayidx.sroa_cast.i58 = bitcast i8* %arrayidx.i57 to i16*
  %value.0.copyload.i59 = load i16, i16* %value.0.arrayidx.sroa_cast.i58, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %5 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i58, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i60 = add i64 %6, 2
  store i64 %add.i60, i64* %vsp, align 8, !tbaa !4
  %7 = and i16 %value.0.copyload.i59, 63
  %and.i = zext i16 %7 to i64
  %or.i = tail call i64 @llvm.fshr.i64(i64 %value.0.copyload.i55, i64 %value.0.copyload.i, i64 %and.i)
  %sub.i.i.i = add nsw i64 %and.i, -1
  %conv.i.i61 = trunc i64 %or.i to i8
  %8 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i61) #3, !range !11
  %9 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i = trunc i64 %9 to i8
  %tobool.not = icmp eq i8 %conv.i, 0
  %cmp.i.i = icmp eq i64 %or.i, 0
  %10 = xor i64 %or.i, %value.0.copyload.i
  %11 = lshr i64 %value.0.copyload.i, %sub.i.i.i
  %12 = and i64 %11, 1
  %13 = shl nuw nsw i8 %8, 2
  %14 = and i8 %13, 4
  %15 = xor i8 %14, 4
  %16 = zext i8 %15 to i64
  %shl16.i = select i1 %tobool.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i, i64 64, i64 0
  %17 = lshr i64 %or.i, 56
  %18 = and i64 %17, 128
  %19 = lshr i64 %10, 52
  %20 = and i64 %19, 2048
  %and13.i = or i64 %18, %shl22.i
  %or17.i = or i64 %and13.i, %12
  %and25.i = or i64 %or17.i, %20
  %or29.i = or i64 %and25.i, %shl16.i
  %or30.i = or i64 %or29.i, %16
  %sub.i64 = add i64 %6, -6
  store i64 %sub.i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i65 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i64
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i65 to i64*
  store i64 %or.i, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %21 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i66 = add i64 %21, -8
  store i64 %sub.i66, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i67 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i66
  %value.addr.0.arrayidx.sroa_cast.i68 = bitcast i8* %arrayidx.i67 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i68, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHRDIjEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i32*
  %value.0.copyload.i = load i32, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i32
  store i32 %conv.i.i, i32* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 4
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i53 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i54 = bitcast i8* %arrayidx.i53 to i32*
  %value.0.copyload.i55 = load i32, i32* %value.0.arrayidx.sroa_cast.i54, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i56 = trunc i64 %3 to i32
  store i32 %conv.i.i56, i32* %value.0.arrayidx.sroa_cast.i54, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i57 = add i64 %4, 4
  store i64 %add.i57, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i58 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i57
  %value.0.arrayidx.sroa_cast.i59 = bitcast i8* %arrayidx.i58 to i16*
  %value.0.copyload.i60 = load i16, i16* %value.0.arrayidx.sroa_cast.i59, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i61 = trunc i64 %5 to i16
  store i16 %conv.i.i61, i16* %value.0.arrayidx.sroa_cast.i59, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i62 = add i64 %6, 2
  store i64 %add.i62, i64* %vsp, align 8, !tbaa !4
  %7 = and i16 %value.0.copyload.i60, 31
  %and3.i = zext i16 %7 to i32
  %sub.i = sub nuw nsw i32 32, %and3.i
  %conv.i = zext i32 %value.0.copyload.i55 to i64
  %conv1.i = zext i32 %sub.i to i64
  %shl.i = shl nuw i64 %conv.i, %conv1.i
  %conv2.i = trunc i64 %shl.i to i32
  %conv.i63 = zext i32 %value.0.copyload.i to i64
  %shr.i = lshr i32 %value.0.copyload.i, %and3.i
  %or3.i = or i32 %shr.i, %conv2.i
  %sub.i.i.i = add nsw i32 %and3.i, -1
  %conv1.i.i.i = zext i32 %sub.i.i.i to i64
  %conv.i.i66 = trunc i32 %or3.i to i8
  %8 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i66) #3, !range !11
  %9 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i67 = trunc i64 %9 to i8
  %tobool.not = icmp eq i8 %conv.i67, 0
  %cmp.i.i = icmp eq i32 %or3.i, 0
  %10 = xor i32 %or3.i, %value.0.copyload.i
  %11 = lshr i64 %conv.i63, %conv1.i.i.i
  %12 = and i64 %11, 1
  %13 = shl nuw nsw i8 %8, 2
  %14 = and i8 %13, 4
  %15 = xor i8 %14, 4
  %16 = zext i8 %15 to i64
  %shl16.i = select i1 %tobool.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i, i64 64, i64 0
  %17 = lshr i32 %or3.i, 24
  %18 = and i32 %17, 128
  %19 = zext i32 %18 to i64
  %20 = lshr i32 %10, 20
  %21 = and i32 %20, 2048
  %22 = zext i32 %21 to i64
  %and13.i = or i64 %shl16.i, %12
  %or17.i = or i64 %and13.i, %shl22.i
  %and25.i = or i64 %or17.i, %19
  %or29.i = or i64 %and25.i, %22
  %or30.i = or i64 %or29.i, %16
  %sub.i70 = add i64 %6, -2
  store i64 %sub.i70, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i71 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i70
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i71 to i32*
  store i32 %or3.i, i32* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %23 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i72 = add i64 %23, -8
  store i64 %sub.i72, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i73 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i72
  %value.addr.0.arrayidx.sroa_cast.i74 = bitcast i8* %arrayidx.i73 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i74, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHRDItEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i16*
  %value.0.copyload.i = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i16
  store i16 %conv.i.i, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 2
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i53 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.arrayidx.sroa_cast.i54 = bitcast i8* %arrayidx.i53 to i16*
  %value.0.copyload.i55 = load i16, i16* %value.0.arrayidx.sroa_cast.i54, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i56 = trunc i64 %3 to i16
  store i16 %conv.i.i56, i16* %value.0.arrayidx.sroa_cast.i54, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i57 = add i64 %4, 2
  store i64 %add.i57, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i58 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i57
  %value.0.arrayidx.sroa_cast.i59 = bitcast i8* %arrayidx.i58 to i16*
  %value.0.copyload.i60 = load i16, i16* %value.0.arrayidx.sroa_cast.i59, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i61 = trunc i64 %5 to i16
  store i16 %conv.i.i61, i16* %value.0.arrayidx.sroa_cast.i59, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i62 = add i64 %6, 2
  store i64 %add.i62, i64* %vsp, align 8, !tbaa !4
  %and3.i = and i16 %value.0.copyload.i60, 31
  %sub.i = sub nsw i16 16, %and3.i
  %conv.i = zext i16 %value.0.copyload.i55 to i64
  %conv1.i = zext i16 %sub.i to i64
  %shl.i = shl i64 %conv.i, %conv1.i
  %conv.i63 = zext i16 %value.0.copyload.i to i64
  %conv1.i64 = zext i16 %and3.i to i64
  %shr.i = lshr i64 %conv.i63, %conv1.i64
  %or3.i77 = or i64 %shl.i, %shr.i
  %or3.i = trunc i64 %or3.i77 to i16
  %sub.i.i.i = add nsw i16 %and3.i, -1
  %conv1.i.i.i = zext i16 %sub.i.i.i to i64
  %conv.i.i66 = trunc i64 %or3.i77 to i8
  %7 = tail call i8 @llvm.ctpop.i8(i8 %conv.i.i66) #3, !range !11
  %8 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i67 = trunc i64 %8 to i8
  %tobool.not = icmp eq i8 %conv.i67, 0
  %cmp.i.i = icmp eq i16 %or3.i, 0
  %9 = xor i16 %value.0.copyload.i, %or3.i
  %10 = lshr i64 %conv.i63, %conv1.i.i.i
  %11 = and i64 %10, 1
  %12 = shl nuw nsw i8 %7, 2
  %13 = and i8 %12, 4
  %14 = xor i8 %13, 4
  %15 = zext i8 %14 to i64
  %shl16.i = select i1 %tobool.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i, i64 64, i64 0
  %16 = lshr i64 %or3.i77, 8
  %17 = and i64 %16, 128
  %18 = lshr i16 %9, 4
  %19 = and i16 %18, 2048
  %20 = zext i16 %19 to i64
  %and13.i = or i64 %17, %11
  %or17.i = or i64 %and13.i, %shl22.i
  %and25.i = or i64 %or17.i, %shl16.i
  %or29.i = or i64 %and25.i, %20
  %or30.i = or i64 %or29.i, %15
  store i64 %6, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i71 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %6
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i71 to i16*
  store i16 %or3.i, i16* %value.addr.0.arrayidx.sroa_cast.i, align 1
  %21 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i72 = add i64 %21, -8
  store i64 %sub.i72, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i73 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i72
  %value.addr.0.arrayidx.sroa_cast.i74 = bitcast i8* %arrayidx.i73 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i74, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHRDIhEvRm(i64* nonnull align 8 dereferenceable(8) %vsp) #0 comdat {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.copyload.i = load i8, i8* %arrayidx.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i = trunc i64 %1 to i8
  store i8 %conv.i.i, i8* %arrayidx.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 1
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i42 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i
  %value.0.copyload.i43 = load i8, i8* %arrayidx.i42, align 1
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i44 = trunc i64 %3 to i8
  store i8 %conv.i.i44, i8* %arrayidx.i42, align 1
  %4 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i45 = add i64 %4, 1
  store i64 %add.i45, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i46 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %add.i45
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i46 to i16*
  %value.0.copyload.i47 = load i16, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %5 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i.i48 = trunc i64 %5 to i16
  store i16 %conv.i.i48, i16* %value.0.arrayidx.sroa_cast.i, align 1
  %6 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i49 = add i64 %6, 2
  store i64 %add.i49, i64* %vsp, align 8, !tbaa !4
  %conv = trunc i16 %value.0.copyload.i47 to i8
  %and3.i = and i8 %conv, 31
  %sub.i = sub nsw i8 8, %and3.i
  %conv.i = zext i8 %value.0.copyload.i43 to i64
  %conv1.i = zext i8 %sub.i to i64
  %shl.i = shl i64 %conv.i, %conv1.i
  %conv.i50 = zext i8 %value.0.copyload.i to i64
  %conv1.i51 = zext i8 %and3.i to i64
  %shr.i = lshr i64 %conv.i50, %conv1.i51
  %or3.i62 = or i64 %shl.i, %shr.i
  %or3.i = trunc i64 %or3.i62 to i8
  %sub.i.i.i = add nsw i8 %and3.i, -1
  %conv1.i.i.i = zext i8 %sub.i.i.i to i64
  %7 = tail call i8 @llvm.ctpop.i8(i8 %or3.i) #3, !range !11
  %8 = load i64, i64* @__undef, align 8, !tbaa !4
  %conv.i53 = trunc i64 %8 to i8
  %tobool.not = icmp eq i8 %conv.i53, 0
  %cmp.i.i = icmp eq i8 %or3.i, 0
  %9 = xor i8 %value.0.copyload.i, %or3.i
  %10 = lshr i64 %conv.i50, %conv1.i.i.i
  %11 = and i64 %10, 1
  %12 = shl nuw nsw i8 %7, 2
  %13 = and i8 %12, 4
  %14 = xor i8 %13, 4
  %15 = zext i8 %14 to i64
  %shl16.i = select i1 %tobool.not, i64 0, i64 16
  %shl22.i = select i1 %cmp.i.i, i64 64, i64 0
  %16 = and i64 %or3.i62, 128
  %17 = and i8 %9, -128
  %18 = zext i8 %17 to i64
  %19 = shl nuw nsw i64 %18, 4
  %and25.i = or i64 %16, %shl22.i
  %and13.i = or i64 %shl16.i, %11
  %or17.i = or i64 %and13.i, %and25.i
  %or29.i = or i64 %or17.i, %19
  %or30.i = or i64 %or29.i, %15
  %sub.i56 = add i64 %6, 1
  store i64 %sub.i56, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i57 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i56
  store i8 %or3.i, i8* %arrayidx.i57, align 1
  %20 = load i64, i64* %vsp, align 8, !tbaa !4
  %sub.i58 = add i64 %20, -8
  store i64 %sub.i58, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i59 = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %sub.i58
  %value.addr.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i59 to i64*
  store i64 %or30.i, i64* %value.addr.0.arrayidx.sroa_cast.i, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z4JUMPRmS_(i64* nocapture nonnull align 8 dereferenceable(8) %vsp, i64* nocapture nonnull align 8 dereferenceable(8) %vip) #2 {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  store i64 %value.0.copyload.i, i64* %vip, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z8JUMP_DECRmS_(i64* nocapture nonnull align 8 dereferenceable(8) %vsp, i64* nocapture nonnull align 8 dereferenceable(8) %vip) #2 {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %sub = add i64 %value.0.copyload.i, -4
  store i64 %sub, i64* %vip, align 8, !tbaa !4
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z8JUMP_INCRmS_(i64* nocapture nonnull align 8 dereferenceable(8) %vsp, i64* nocapture nonnull align 8 dereferenceable(8) %vip) #2 {
entry:
  %0 = load i64, i64* %vsp, align 8, !tbaa !4
  %arrayidx.i = getelementptr inbounds [0 x i8], [0 x i8]* @RAM, i64 0, i64 %0
  %value.0.arrayidx.sroa_cast.i = bitcast i8* %arrayidx.i to i64*
  %value.0.copyload.i = load i64, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %1 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %1, i64* %value.0.arrayidx.sroa_cast.i, align 1
  %2 = load i64, i64* %vsp, align 8, !tbaa !4
  %add.i = add i64 %2, 8
  store i64 %add.i, i64* %vsp, align 8, !tbaa !4
  %add = add i64 %value.0.copyload.i, 4
  store i64 %add, i64* %vip, align 8, !tbaa !4
  ret void
}

; Function Attrs: mustprogress nounwind uwtable
define dso_local void @HelperUnsupported(i64* noalias nocapture nonnull align 8 dereferenceable(8) %rax, i64* noalias nocapture nonnull align 8 dereferenceable(8) %rbx, i64* noalias nocapture nonnull align 8 dereferenceable(8) %rcx, i64* noalias nocapture nonnull align 8 dereferenceable(8) %rdx, i64* noalias nocapture nonnull align 8 dereferenceable(8) %rsi, i64* noalias nocapture nonnull align 8 dereferenceable(8) %rdi, i64* noalias nocapture nonnull align 8 dereferenceable(8) %rbp, i64* noalias nocapture nonnull align 8 dereferenceable(8) %rsp, i64* noalias nocapture nonnull align 8 dereferenceable(8) %r8, i64* noalias nocapture nonnull align 8 dereferenceable(8) %r9, i64* noalias nocapture nonnull align 8 dereferenceable(8) %r10, i64* noalias nocapture nonnull align 8 dereferenceable(8) %r11, i64* noalias nocapture nonnull align 8 dereferenceable(8) %r12, i64* noalias nocapture nonnull align 8 dereferenceable(8) %r13, i64* noalias nocapture nonnull align 8 dereferenceable(8) %r14, i64* noalias nocapture nonnull align 8 dereferenceable(8) %r15) local_unnamed_addr #4 {
entry:
  %ctx = alloca %struct.VirtualContext, align 8
  %0 = bitcast %struct.VirtualContext* %ctx to i8*
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %0) #3
  %1 = load i64, i64* %rax, align 8, !tbaa !4
  %qword = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 0, i32 0, i32 0
  store i64 %1, i64* %qword, align 8, !tbaa !9
  %2 = load i64, i64* %rbx, align 8, !tbaa !4
  %qword3 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 1, i32 0, i32 0
  store i64 %2, i64* %qword3, align 8, !tbaa !9
  %3 = load i64, i64* %rcx, align 8, !tbaa !4
  %qword5 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 2, i32 0, i32 0
  store i64 %3, i64* %qword5, align 8, !tbaa !9
  %4 = load i64, i64* %rdx, align 8, !tbaa !4
  %qword7 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 3, i32 0, i32 0
  store i64 %4, i64* %qword7, align 8, !tbaa !9
  %5 = load i64, i64* %rsi, align 8, !tbaa !4
  %qword9 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 4, i32 0, i32 0
  store i64 %5, i64* %qword9, align 8, !tbaa !9
  %6 = load i64, i64* %rdi, align 8, !tbaa !4
  %qword11 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 5, i32 0, i32 0
  store i64 %6, i64* %qword11, align 8, !tbaa !9
  %7 = load i64, i64* %rbp, align 8, !tbaa !4
  %qword13 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 6, i32 0, i32 0
  store i64 %7, i64* %qword13, align 8, !tbaa !9
  %8 = load i64, i64* %rsp, align 8, !tbaa !4
  %qword15 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 7, i32 0, i32 0
  store i64 %8, i64* %qword15, align 8, !tbaa !9
  %9 = load i64, i64* %r8, align 8, !tbaa !4
  %qword17 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 8, i32 0, i32 0
  store i64 %9, i64* %qword17, align 8, !tbaa !9
  %10 = load i64, i64* %r9, align 8, !tbaa !4
  %qword19 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 9, i32 0, i32 0
  store i64 %10, i64* %qword19, align 8, !tbaa !9
  %11 = load i64, i64* %r10, align 8, !tbaa !4
  %qword21 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 10, i32 0, i32 0
  store i64 %11, i64* %qword21, align 8, !tbaa !9
  %12 = load i64, i64* %r11, align 8, !tbaa !4
  %qword23 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 11, i32 0, i32 0
  store i64 %12, i64* %qword23, align 8, !tbaa !9
  %13 = load i64, i64* %r12, align 8, !tbaa !4
  %qword25 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 12, i32 0, i32 0
  store i64 %13, i64* %qword25, align 8, !tbaa !9
  %14 = load i64, i64* %r13, align 8, !tbaa !4
  %qword27 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 13, i32 0, i32 0
  store i64 %14, i64* %qword27, align 8, !tbaa !9
  %15 = load i64, i64* %r14, align 8, !tbaa !4
  %qword29 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 14, i32 0, i32 0
  store i64 %15, i64* %qword29, align 8, !tbaa !9
  %16 = load i64, i64* %r15, align 8, !tbaa !4
  %qword31 = getelementptr inbounds %struct.VirtualContext, %struct.VirtualContext* %ctx, i64 0, i32 15, i32 0, i32 0
  store i64 %16, i64* %qword31, align 8, !tbaa !9
  call void @HelperUnsupportedStub(%struct.VirtualContext* nonnull align 1 dereferenceable(128) %ctx) #3
  %17 = load i64, i64* %qword, align 8, !tbaa !9
  store i64 %17, i64* %rax, align 8, !tbaa !4
  %18 = load i64, i64* %qword3, align 8, !tbaa !9
  store i64 %18, i64* %rbx, align 8, !tbaa !4
  %19 = load i64, i64* %qword5, align 8, !tbaa !9
  store i64 %19, i64* %rcx, align 8, !tbaa !4
  %20 = load i64, i64* %qword7, align 8, !tbaa !9
  store i64 %20, i64* %rdx, align 8, !tbaa !4
  %21 = load i64, i64* %qword9, align 8, !tbaa !9
  store i64 %21, i64* %rsi, align 8, !tbaa !4
  %22 = load i64, i64* %qword11, align 8, !tbaa !9
  store i64 %22, i64* %rdi, align 8, !tbaa !4
  %23 = load i64, i64* %qword13, align 8, !tbaa !9
  store i64 %23, i64* %rbp, align 8, !tbaa !4
  %24 = load i64, i64* %qword15, align 8, !tbaa !9
  store i64 %24, i64* %rsp, align 8, !tbaa !4
  %25 = load i64, i64* %qword17, align 8, !tbaa !9
  store i64 %25, i64* %r8, align 8, !tbaa !4
  %26 = load i64, i64* %qword19, align 8, !tbaa !9
  store i64 %26, i64* %r9, align 8, !tbaa !4
  %27 = load i64, i64* %qword21, align 8, !tbaa !9
  store i64 %27, i64* %r10, align 8, !tbaa !4
  %28 = load i64, i64* %qword23, align 8, !tbaa !9
  store i64 %28, i64* %r11, align 8, !tbaa !4
  %29 = load i64, i64* %qword25, align 8, !tbaa !9
  store i64 %29, i64* %r12, align 8, !tbaa !4
  %30 = load i64, i64* %qword27, align 8, !tbaa !9
  store i64 %30, i64* %r13, align 8, !tbaa !4
  %31 = load i64, i64* %qword29, align 8, !tbaa !9
  store i64 %31, i64* %r14, align 8, !tbaa !4
  %32 = load i64, i64* %qword31, align 8, !tbaa !9
  store i64 %32, i64* %r15, align 8, !tbaa !4
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %0) #3
  ret void
}

declare dso_local void @HelperUnsupportedStub(%struct.VirtualContext* nonnull align 1 dereferenceable(128)) local_unnamed_addr #5

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind readonly uwtable willreturn
define dso_local i64 @HelperStubEmpty(i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %rax, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %rbx, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %rcx, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %rdx, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %rsi, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %rdi, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %rbp, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %rsp, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %r8, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %r9, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %r10, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %r11, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %r12, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %r13, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %r14, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %r15, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %flags, i64 %KEY_STUB, i64 %RET_ADDR, i64 %REL_ADDR, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %vsp, i64* noalias nocapture nonnull readonly align 8 dereferenceable(8) %vip, %struct.VirtualRegister* noalias nocapture readnone %vmregs, i64* noalias nocapture readnone %slots) local_unnamed_addr #6 {
entry:
  %0 = load i64, i64* %vip, align 8, !tbaa !4
  ret i64 %0
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define dso_local i64 @HelperFunction(i64* noalias nonnull align 8 dereferenceable(8) %rax, i64* noalias nonnull align 8 dereferenceable(8) %rbx, i64* noalias nonnull align 8 dereferenceable(8) %rcx, i64* noalias nonnull align 8 dereferenceable(8) %rdx, i64* noalias nonnull align 8 dereferenceable(8) %rsi, i64* noalias nonnull align 8 dereferenceable(8) %rdi, i64* noalias nonnull align 8 dereferenceable(8) %rbp, i64* noalias nonnull align 8 dereferenceable(8) %rsp, i64* noalias nonnull align 8 dereferenceable(8) %r8, i64* noalias nonnull align 8 dereferenceable(8) %r9, i64* noalias nonnull align 8 dereferenceable(8) %r10, i64* noalias nonnull align 8 dereferenceable(8) %r11, i64* noalias nonnull align 8 dereferenceable(8) %r12, i64* noalias nonnull align 8 dereferenceable(8) %r13, i64* noalias nonnull align 8 dereferenceable(8) %r14, i64* noalias nonnull align 8 dereferenceable(8) %r15, i64* noalias nonnull align 8 dereferenceable(8) %flags, i64 %KEY_STUB, i64 %RET_ADDR, i64 %REL_ADDR) local_unnamed_addr #0 {
entry:
  %vmregs = alloca [30 x %struct.VirtualRegister], align 16
  %slots = alloca [30 x i64], align 16
  %vip = alloca i64, align 8
  %0 = bitcast [30 x %struct.VirtualRegister]* %vmregs to i8*
  call void @llvm.lifetime.start.p0i8(i64 240, i8* nonnull %0) #3
  call void @llvm.memset.p0i8.i64(i8* noundef nonnull align 16 dereferenceable(240) %0, i8 0, i64 240, i1 false)
  %1 = bitcast [30 x i64]* %slots to i8*
  call void @llvm.lifetime.start.p0i8(i64 240, i8* nonnull %1) #3
  call void @llvm.memset.p0i8.i64(i8* noundef nonnull align 16 dereferenceable(240) %1, i8 0, i64 240, i1 false)
  %2 = bitcast i64* %vip to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2) #3
  store i64 0, i64* %vip, align 8, !tbaa !4
  %arraydecay = getelementptr inbounds [30 x %struct.VirtualRegister], [30 x %struct.VirtualRegister]* %vmregs, i64 0, i64 0
  %arraydecay1 = getelementptr inbounds [30 x i64], [30 x i64]* %slots, i64 0, i64 0
  %call = call i64 @HelperStub(i64* nonnull align 8 dereferenceable(8) %rax, i64* nonnull align 8 dereferenceable(8) %rbx, i64* nonnull align 8 dereferenceable(8) %rcx, i64* nonnull align 8 dereferenceable(8) %rdx, i64* nonnull align 8 dereferenceable(8) %rsi, i64* nonnull align 8 dereferenceable(8) %rdi, i64* nonnull align 8 dereferenceable(8) %rbp, i64* nonnull align 8 dereferenceable(8) %rsp, i64* nonnull align 8 dereferenceable(8) %r8, i64* nonnull align 8 dereferenceable(8) %r9, i64* nonnull align 8 dereferenceable(8) %r10, i64* nonnull align 8 dereferenceable(8) %r11, i64* nonnull align 8 dereferenceable(8) %r12, i64* nonnull align 8 dereferenceable(8) %r13, i64* nonnull align 8 dereferenceable(8) %r14, i64* nonnull align 8 dereferenceable(8) %r15, i64* nonnull align 8 dereferenceable(8) %flags, i64 %KEY_STUB, i64 %RET_ADDR, i64 0, i64* nonnull align 8 dereferenceable(8) %rsp, i64* nonnull align 8 dereferenceable(8) %vip, %struct.VirtualRegister* nonnull %arraydecay, i64* nonnull %arraydecay1) #3
  %3 = load i64, i64* @__undef, align 8, !tbaa !4
  store i64 %3, i64* %flags, align 8, !tbaa !4
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2) #3
  call void @llvm.lifetime.end.p0i8(i64 240, i8* nonnull %1) #3
  call void @llvm.lifetime.end.p0i8(i64 240, i8* nonnull %0) #3
  ret i64 %call
}

; Function Attrs: argmemonly mustprogress nofree nounwind willreturn writeonly
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #7

declare dso_local i64 @HelperStub(i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64, i64, i64, i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), %struct.VirtualRegister*, i64*) local_unnamed_addr #5

; Function Attrs: mustprogress nounwind uwtable
define dso_local i64 @HelperSlicePC(i64 %rax, i64 %rbx, i64 %rcx, i64 %rdx, i64 %rsi, i64 %rdi, i64 %rbp, i64 %rsp, i64 %r8, i64 %r9, i64 %r10, i64 %r11, i64 %r12, i64 %r13, i64 %r14, i64 %r15, i64 %flags, i64 %KEY_STUB, i64 %RET_ADDR, i64 %REL_ADDR) local_unnamed_addr #4 {
entry:
  %rax.addr = alloca i64, align 8
  %rbx.addr = alloca i64, align 8
  %rcx.addr = alloca i64, align 8
  %rdx.addr = alloca i64, align 8
  %rsi.addr = alloca i64, align 8
  %rdi.addr = alloca i64, align 8
  %rbp.addr = alloca i64, align 8
  %rsp.addr = alloca i64, align 8
  %r8.addr = alloca i64, align 8
  %r9.addr = alloca i64, align 8
  %r10.addr = alloca i64, align 8
  %r11.addr = alloca i64, align 8
  %r12.addr = alloca i64, align 8
  %r13.addr = alloca i64, align 8
  %r14.addr = alloca i64, align 8
  %r15.addr = alloca i64, align 8
  %flags.addr = alloca i64, align 8
  %vmregs = alloca [30 x %struct.VirtualRegister], align 16
  %slots = alloca [30 x i64], align 16
  %vsp = alloca i64, align 8
  %vip = alloca i64, align 8
  store i64 %rax, i64* %rax.addr, align 8, !tbaa !4
  store i64 %rbx, i64* %rbx.addr, align 8, !tbaa !4
  store i64 %rcx, i64* %rcx.addr, align 8, !tbaa !4
  store i64 %rdx, i64* %rdx.addr, align 8, !tbaa !4
  store i64 %rsi, i64* %rsi.addr, align 8, !tbaa !4
  store i64 %rdi, i64* %rdi.addr, align 8, !tbaa !4
  store i64 %rbp, i64* %rbp.addr, align 8, !tbaa !4
  store i64 %rsp, i64* %rsp.addr, align 8, !tbaa !4
  store i64 %r8, i64* %r8.addr, align 8, !tbaa !4
  store i64 %r9, i64* %r9.addr, align 8, !tbaa !4
  store i64 %r10, i64* %r10.addr, align 8, !tbaa !4
  store i64 %r11, i64* %r11.addr, align 8, !tbaa !4
  store i64 %r12, i64* %r12.addr, align 8, !tbaa !4
  store i64 %r13, i64* %r13.addr, align 8, !tbaa !4
  store i64 %r14, i64* %r14.addr, align 8, !tbaa !4
  store i64 %r15, i64* %r15.addr, align 8, !tbaa !4
  store i64 %flags, i64* %flags.addr, align 8, !tbaa !4
  %0 = bitcast [30 x %struct.VirtualRegister]* %vmregs to i8*
  call void @llvm.lifetime.start.p0i8(i64 240, i8* nonnull %0) #3
  call void @llvm.memset.p0i8.i64(i8* noundef nonnull align 16 dereferenceable(240) %0, i8 0, i64 240, i1 false)
  %1 = bitcast [30 x i64]* %slots to i8*
  call void @llvm.lifetime.start.p0i8(i64 240, i8* nonnull %1) #3
  call void @llvm.memset.p0i8.i64(i8* noundef nonnull align 16 dereferenceable(240) %1, i8 0, i64 240, i1 false)
  %2 = bitcast i64* %vsp to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %2) #3
  store i64 %rsp, i64* %vsp, align 8, !tbaa !4
  %3 = bitcast i64* %vip to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %3) #3
  store i64 0, i64* %vip, align 8, !tbaa !4
  %arraydecay = getelementptr inbounds [30 x %struct.VirtualRegister], [30 x %struct.VirtualRegister]* %vmregs, i64 0, i64 0
  %arraydecay1 = getelementptr inbounds [30 x i64], [30 x i64]* %slots, i64 0, i64 0
  %call = call i64 @HelperStub(i64* nonnull align 8 dereferenceable(8) %rax.addr, i64* nonnull align 8 dereferenceable(8) %rbx.addr, i64* nonnull align 8 dereferenceable(8) %rcx.addr, i64* nonnull align 8 dereferenceable(8) %rdx.addr, i64* nonnull align 8 dereferenceable(8) %rsi.addr, i64* nonnull align 8 dereferenceable(8) %rdi.addr, i64* nonnull align 8 dereferenceable(8) %rbp.addr, i64* nonnull align 8 dereferenceable(8) %rsp.addr, i64* nonnull align 8 dereferenceable(8) %r8.addr, i64* nonnull align 8 dereferenceable(8) %r9.addr, i64* nonnull align 8 dereferenceable(8) %r10.addr, i64* nonnull align 8 dereferenceable(8) %r11.addr, i64* nonnull align 8 dereferenceable(8) %r12.addr, i64* nonnull align 8 dereferenceable(8) %r13.addr, i64* nonnull align 8 dereferenceable(8) %r14.addr, i64* nonnull align 8 dereferenceable(8) %r15.addr, i64* nonnull align 8 dereferenceable(8) %flags.addr, i64 %KEY_STUB, i64 %RET_ADDR, i64 0, i64* nonnull align 8 dereferenceable(8) %vsp, i64* nonnull align 8 dereferenceable(8) %vip, %struct.VirtualRegister* nonnull %arraydecay, i64* nonnull %arraydecay1) #3
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %3) #3
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %2) #3
  call void @llvm.lifetime.end.p0i8(i64 240, i8* nonnull %1) #3
  call void @llvm.lifetime.end.p0i8(i64 240, i8* nonnull %0) #3
  ret i64 %call
}

; Function Attrs: mustprogress nofree nosync nounwind uwtable willreturn writeonly
define dso_local void @retainPointers() local_unnamed_addr #8 {
entry:
  %call = tail call i64 @HelperKeepPC(i64 0, i64 1) #12
  %conv = trunc i64 %call to i8
  store i8 %conv, i8* getelementptr inbounds ([0 x i8], [0 x i8]* @RAM, i64 0, i64 0), align 1, !tbaa !9
  store i8 0, i8* getelementptr inbounds ([0 x i8], [0 x i8]* @GS, i64 0, i64 0), align 1, !tbaa !9
  store i8 0, i8* getelementptr inbounds ([0 x i8], [0 x i8]* @FS, i64 0, i64 0), align 1, !tbaa !9
  ret void
}

; Function Attrs: mustprogress noduplicate nofree nosync nounwind readnone willreturn
declare dso_local i64 @HelperKeepPC(i64, i64) local_unnamed_addr #9

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare i8 @llvm.ctpop.i8(i8) #10

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare i64 @llvm.fshr.i64(i64, i64, i64) #10

attributes #0 = { alwaysinline mustprogress nounwind uwtable "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+x87,-3dnow,-3dnowa,-aes,-avx,-avx2,-avx512bf16,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vp2intersect,-avx512vpopcntdq,-avxvnni,-f16c,-fma,-fma4,-gfni,-kl,-mmx,-pclmul,-sha,-sse,-sse2,-sse3,-sse4.1,-sse4.2,-sse4a,-ssse3,-vaes,-vpclmulqdq,-widekl,-xop" "tune-cpu"="generic" }
attributes #1 = { argmemonly mustprogress nofree nosync nounwind willreturn }
attributes #2 = { alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+x87,-3dnow,-3dnowa,-aes,-avx,-avx2,-avx512bf16,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vp2intersect,-avx512vpopcntdq,-avxvnni,-f16c,-fma,-fma4,-gfni,-kl,-mmx,-pclmul,-sha,-sse,-sse2,-sse3,-sse4.1,-sse4.2,-sse4a,-ssse3,-vaes,-vpclmulqdq,-widekl,-xop" "tune-cpu"="generic" }
attributes #3 = { nounwind }
attributes #4 = { mustprogress nounwind uwtable "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+x87,-3dnow,-3dnowa,-aes,-avx,-avx2,-avx512bf16,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vp2intersect,-avx512vpopcntdq,-avxvnni,-f16c,-fma,-fma4,-gfni,-kl,-mmx,-pclmul,-sha,-sse,-sse2,-sse3,-sse4.1,-sse4.2,-sse4a,-ssse3,-vaes,-vpclmulqdq,-widekl,-xop" "tune-cpu"="generic" }
attributes #5 = { "frame-pointer"="none" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+x87,-3dnow,-3dnowa,-aes,-avx,-avx2,-avx512bf16,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vp2intersect,-avx512vpopcntdq,-avxvnni,-f16c,-fma,-fma4,-gfni,-kl,-mmx,-pclmul,-sha,-sse,-sse2,-sse3,-sse4.1,-sse4.2,-sse4a,-ssse3,-vaes,-vpclmulqdq,-widekl,-xop" "tune-cpu"="generic" }
attributes #6 = { alwaysinline mustprogress nofree norecurse nosync nounwind readonly uwtable willreturn "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+x87,-3dnow,-3dnowa,-aes,-avx,-avx2,-avx512bf16,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vp2intersect,-avx512vpopcntdq,-avxvnni,-f16c,-fma,-fma4,-gfni,-kl,-mmx,-pclmul,-sha,-sse,-sse2,-sse3,-sse4.1,-sse4.2,-sse4a,-ssse3,-vaes,-vpclmulqdq,-widekl,-xop" "tune-cpu"="generic" }
attributes #7 = { argmemonly mustprogress nofree nounwind willreturn writeonly }
attributes #8 = { mustprogress nofree nosync nounwind uwtable willreturn writeonly "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+x87,-3dnow,-3dnowa,-aes,-avx,-avx2,-avx512bf16,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vp2intersect,-avx512vpopcntdq,-avxvnni,-f16c,-fma,-fma4,-gfni,-kl,-mmx,-pclmul,-sha,-sse,-sse2,-sse3,-sse4.1,-sse4.2,-sse4a,-ssse3,-vaes,-vpclmulqdq,-widekl,-xop" "tune-cpu"="generic" }
attributes #9 = { mustprogress noduplicate nofree nosync nounwind readnone willreturn "frame-pointer"="none" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+x87,-3dnow,-3dnowa,-aes,-avx,-avx2,-avx512bf16,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vp2intersect,-avx512vpopcntdq,-avxvnni,-f16c,-fma,-fma4,-gfni,-kl,-mmx,-pclmul,-sha,-sse,-sse2,-sse3,-sse4.1,-sse4.2,-sse4a,-ssse3,-vaes,-vpclmulqdq,-widekl,-xop" "tune-cpu"="generic" }
attributes #10 = { nofree nosync nounwind readnone speculatable willreturn }
attributes #11 = { nounwind readnone }
attributes #12 = { noduplicate nomerge nounwind readnone willreturn }

!llvm.module.flags = !{!0, !1}
!llvm.ident = !{!2}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"uwtable", i32 1}
!2 = !{!"clang version 13.0.1"}
!3 = !{i64 3772}
!4 = !{!5, !5, i64 0}
!5 = !{!"long", !6, i64 0}
!6 = !{!"omnipotent char", !7, i64 0}
!7 = !{!"Simple C++ TBAA"}
!8 = !{i64 4059}
!9 = !{!6, !6, i64 0}
!10 = !{i64 2152665066, i64 2152665102, i64 2152665126}
!11 = !{i8 0, i8 9}

```

`src/llvm_ir_gen/mod.rs`:

```rs
use inkwell::{
    attributes::{Attribute, AttributeLoc},
    basic_block::BasicBlock,
    builder::Builder,
    context::Context,
    memory_buffer::MemoryBuffer,
    module::Module,
    passes::PassManager,
    values::{
        BasicValue, BasicValueEnum, CallableValue, FunctionValue, InstructionValue, PointerValue,
    }, InlineAsmDialect,
};
use petgraph::{algo::dominators, graphmap::GraphMap, EdgeDirection::Incoming};
use std::{
    cell::RefCell,
    collections::{HashMap, HashSet},
    error::Error,
    path::Path,
};

use crate::{
    symbolic::get_possible_solutions,
    vm_handler::{Registers, VmContext},
    vm_matchers::HandlerVmInstruction,
};



pub struct VmLifter<'ctx> {
    context: &'ctx Context,
    module:  RefCell<Module<'ctx>>,
    builder: Builder<'ctx>,
}





// This leaks memory :(
impl<'ctx> VmLifter<'ctx> {
    pub fn lift_helper_stub(&self,
                            vm_context: &VmContext,
                            handlers: &[(u64, HandlerVmInstruction, u64)]) {
        let helper_stub = self.create_helper_stub(vm_context.initial_vip);
        match helper_stub {
            Some(helper_stub) => {
                self.lift_into_helper_stub(vm_context, handlers, &helper_stub);
            },
            None => {},
        }
    }

    pub fn slice_vip(&self,
                     control_flow_graph: &GraphMap<u64, (), petgraph::Directed>,
                     mut input_vip: u64,
                     root_vip: u64,
                     max_pred_block_count: Option<u64>)
                     -> Result<Vec<u64>, Box<dyn Error>> {
        self.output_module();

        let mut slice_blocks = vec![input_vip];

        println!("Computing dominators");
        let dominator_info = dominators::simple_fast(control_flow_graph, root_vip);
        println!("Done computing dominators");

        let mut loopcount = 0;
        'outer: loop {
            let pred_nodes = control_flow_graph.edges_directed(input_vip, Incoming)
                                               .map(|edge| edge.0)
                                               .collect::<HashSet<u64>>();

            if pred_nodes.is_empty() || pred_nodes.len() > 1 {
                break 'outer;
            }

            let mut temp_slice_blocks = Vec::new();
            for dominator in dominator_info.dominators(input_vip).unwrap() {
                if pred_nodes.contains(&dominator) {
                    temp_slice_blocks.push(dominator);
                    input_vip = dominator;
                    break;
                }
            }

            loopcount += 1;
            println!("Loop iteration -> {}", loopcount);
            slice_blocks.extend_from_slice(&temp_slice_blocks);

            if let Some(max_count) = max_pred_block_count {
                if loopcount == max_count {
                    break 'outer;
                }
            }
        }

        self.create_helper_slicevpc(&slice_blocks);
        self.optimize_module();
        self.output_bitcode();

        let possible_solutions_vip = get_possible_solutions("helperslicevpc")?;

        self.reload_module()?;

        Ok(possible_solutions_vip)
    }

    #[allow(dead_code)]
    pub fn print_module(&self) {
        self.module.borrow().print_to_stderr();
    }

    #[allow(dead_code)]
    pub fn verify_module(&self) {
        self.module.borrow().verify().unwrap();
    }

    pub fn output_module(&self) {
        self.module.borrow().print_to_file("devirt.ll").unwrap();
    }

    pub fn reload_module(&self) -> Result<(), Box<dyn Error>> {
        let path = Path::new("devirt.ll");
        let memory_buffer = MemoryBuffer::create_from_file(path)?;
        let new_module = self.context.create_module_from_ir(memory_buffer)?;

        *self.module.borrow_mut() = new_module;

        Ok(())
    }

    pub fn output_bitcode(&self) {
        self.module
            .borrow()
            .write_bitcode_to_path(std::path::Path::new("devirt.bc"));
    }

    #[allow(dead_code)]
    pub fn print_function(&self,
                          func_name: &str) {
        let function = self.module.borrow().get_function(func_name);

        match function {
            Some(func_val) => func_val.print_to_stderr(),
            None => {
                println!("Function not found {}", func_name)
            },
        }
    }

    pub fn new() -> Self {
        let context = Box::leak(Box::new(inkwell::context::Context::create()));

        let vmprotect_helpers = include_bytes!("VMProtectHelpers_64.ll");

        let memory_buffer =
            MemoryBuffer::create_from_memory_range_copy(vmprotect_helpers.as_slice(),
                                                        "vmp_helpers");
        let module = context.create_module_from_ir(memory_buffer).unwrap();
        let builder = context.create_builder();

        let undef = module.get_global("__undef").unwrap();
        undef.set_initializer(&context.i64_type().get_undef());

        Self { context,
               module: RefCell::new(module),
               builder }
    }

    pub fn optimize_module_no_global_delete(&self) {
        let module_passmanager: PassManager<Module<'_>> = PassManager::create(());

        module_passmanager.add_always_inliner_pass();
        module_passmanager.add_basic_alias_analysis_pass();
        module_passmanager.add_type_based_alias_analysis_pass();
        module_passmanager.add_scoped_no_alias_aa_pass();
        module_passmanager.add_cfg_simplification_pass();
        module_passmanager.add_scalar_repl_aggregates_pass();
        module_passmanager.add_early_cse_mem_ssa_pass();
        module_passmanager.add_instruction_simplify_pass();
        module_passmanager.add_instruction_combining_pass();
        module_passmanager.add_cfg_simplification_pass();
        module_passmanager.add_gvn_pass();
        module_passmanager.add_dead_store_elimination_pass();
        module_passmanager.add_instruction_simplify_pass();
        module_passmanager.add_instruction_combining_pass();
        module_passmanager.add_cfg_simplification_pass();
        module_passmanager.add_strip_symbol_pass();

        for _ in 0 .. 3 {
            module_passmanager.run_on(&self.module.borrow());
        }

        let i64_type = self.context.i64_type();
        let undef = self.module.borrow().get_global("__undef").unwrap();
        undef.set_initializer(&i64_type.get_undef());
        undef.set_constant(true);

        for _ in 0 .. 3 {
            module_passmanager.run_on(&self.module.borrow());
        }
    }

    pub fn optimize_module(&self) {
        let module_passmanager: PassManager<Module<'_>> = PassManager::create(());

        module_passmanager.add_always_inliner_pass();
        module_passmanager.add_basic_alias_analysis_pass();
        module_passmanager.add_type_based_alias_analysis_pass();
        module_passmanager.add_scoped_no_alias_aa_pass();
        module_passmanager.add_cfg_simplification_pass();
        module_passmanager.add_scalar_repl_aggregates_pass();
        module_passmanager.add_early_cse_mem_ssa_pass();
        module_passmanager.add_instruction_simplify_pass();
        module_passmanager.add_instruction_combining_pass();
        module_passmanager.add_cfg_simplification_pass();
        module_passmanager.add_gvn_pass();
        module_passmanager.add_dead_store_elimination_pass();
        module_passmanager.add_instruction_simplify_pass();
        module_passmanager.add_instruction_combining_pass();
        module_passmanager.add_cfg_simplification_pass();
        module_passmanager.add_strip_symbol_pass();

        for _ in 0 .. 3 {
            module_passmanager.run_on(&self.module.borrow());
        }

        let i64_type = self.context.i64_type();
        let undef = self.module.borrow().get_global("__undef").unwrap();
        undef.set_initializer(&i64_type.get_undef());
        undef.set_constant(true);

        for _ in 0 .. 3 {
            module_passmanager.run_on(&self.module.borrow());
        }

        let i8_type = self.context.i8_type();
        let ram = self.module.borrow().get_global("RAM").unwrap();
        ram.set_initializer(&i8_type.array_type(0).const_array(&[]));
        ram.set_constant(true);
    }

    #[allow(dead_code)]
    pub fn fix_arg_names(&self,
                         helper_function_name: &str) {
        let helper_function = self.module
                                  .borrow()
                                  .get_function(helper_function_name)
                                  .unwrap();
        let param_names = ["rax", "rbx", "rcx", "rdx", "rsi", "rdi", "rbp", "rsp", "r8", "r9",
                           "r10", "r11", "r12", "r13", "r14", "r15", "flags", "KEY_STUB",
                           "RET_ADDR", "REL_ADDR"];

        for (i, (&name, param)) in
            std::iter::zip(param_names.iter(), helper_function.get_param_iter()).enumerate()
        {
            param.set_name(name);

            if name != "KEY_STUB" && name != "RET_ADDR" && name != "REL_ADDR" {
                helper_function.add_attribute(AttributeLoc::Param(i as _), self.context.create_enum_attribute(Attribute::get_named_enum_kind_id("noalias"), 0));
            }
        }
    }
    /// If the helper stub already exists return `None`
    fn create_helper_stub(&self,
                          start_vip: u64)
                          -> Option<FunctionValue> {
        let helper_stub_name = format!("helperstub_{:x}", start_vip);
        if self.module.borrow().get_function(&helper_stub_name) != None {
            return None;
        }

        let helper_stub_def = self.module
                                  .borrow()
                                  .get_function("HelperStub")
                                  .expect("Could not find HelperStub in llvm ir file");
        let helper_stub_type = helper_stub_def.get_type();

        let helper_stub = self.module
                              .borrow()
                              .add_function(&helper_stub_name, helper_stub_type, None);
        let entry_bb = self.context.append_basic_block(helper_stub, "");

        let param_names = ["rax", "rbx", "rcx", "rdx", "rsi", "rdi", "rbp", "rsp", "r8", "r9",
                           "r10", "r11", "r12", "r13", "r14", "r15", "flags", "KEY_STUB",
                           "RET_ADDR", "REL_ADDR", "vsp", "vip", "vmregs", "slots"];
        for (i, (&name, param)) in
            std::iter::zip(param_names.iter(), helper_stub.get_param_iter()).enumerate()
        {
            param.set_name(name);

            if name != "KEY_STUB" && name != "RET_ADDR" && name != "REL_ADDR" {
                helper_stub.add_attribute(AttributeLoc::Param(i as _), self.context.create_enum_attribute(Attribute::get_named_enum_kind_id("noalias"), 0));
            }
        }

        self.builder.position_at_end(entry_bb);

        let vip_ptr = get_param_vip(&helper_stub);
        let vip = self.builder.build_load(vip_ptr, "");
        self.builder.build_return(Some(&vip));

        helper_stub.add_attribute(
            inkwell::attributes::AttributeLoc::Function,
            self.context
                .create_enum_attribute(Attribute::get_named_enum_kind_id("alwaysinline"), 0),
        );

        Some(helper_stub)
    }

    pub fn lift_vm_instruction(&self,
                               vm_context: &VmContext,
                               vm_instruction: &HandlerVmInstruction,
                               helper_stub: &FunctionValue) {
        // println!("Lifting -> {}", vm_instruction);
        match vm_instruction {
            HandlerVmInstruction::Pop(size, reg_offset) => {
                self.lift_pop(*size, *reg_offset, helper_stub);
            },
            HandlerVmInstruction::Push(size, reg_offset) => {
                self.lift_push(*size, *reg_offset, helper_stub);
            },
            HandlerVmInstruction::PushImm64(imm) => {
                self.lift_push_imm64(*imm, helper_stub);
            },
            HandlerVmInstruction::PushImm32(imm) => {
                self.lift_push_imm32(*imm, helper_stub);
            },
            HandlerVmInstruction::PushImm16(imm) => {
                self.lift_push_imm16(*imm, helper_stub);
            },
            HandlerVmInstruction::PushImm8(imm) => {
                self.lift_push_imm16(*imm as u16, helper_stub);
            },
            HandlerVmInstruction::PushVsp(size) => {
                self.lift_generic_handler(*size, "PUSH_VSP", helper_stub);
            },
            HandlerVmInstruction::PopVsp(size) => {
                self.lift_generic_handler(*size, "POP_VSP", helper_stub);
            },
            HandlerVmInstruction::Add(size) => {
                self.lift_generic_handler(*size, "ADD", helper_stub);
            },
            HandlerVmInstruction::Mul(size) => {
                self.lift_generic_handler(*size, "MUL", helper_stub);
            },
            HandlerVmInstruction::Imul(size) => {
                self.lift_generic_handler(*size, "IMUL", helper_stub);
            },
            HandlerVmInstruction::Shr(size) => {
                self.lift_generic_handler(*size, "SHR", helper_stub);
            },
            HandlerVmInstruction::Shl(size) => {
                self.lift_generic_handler(*size, "SHL", helper_stub);
            },
            HandlerVmInstruction::Shrd(size) => {
                self.lift_generic_handler(*size, "SHRD", helper_stub);
            },
            HandlerVmInstruction::Shld(size) => {
                self.lift_generic_handler(*size, "SHLD", helper_stub);
            },
            HandlerVmInstruction::Nand(size) => {
                self.lift_generic_handler(*size, "NAND", helper_stub);
            },
            HandlerVmInstruction::Nor(size) => {
                self.lift_generic_handler(*size, "NOR", helper_stub);
            },
            HandlerVmInstruction::Fetch(size) => {
                self.lift_generic_handler(*size, "LOAD_DS", helper_stub);
            },
            HandlerVmInstruction::Store(size) => {
                self.lift_generic_handler(*size, "STORE_DS", helper_stub);
            },
            HandlerVmInstruction::VmExit => {
                self.lift_vm_exit(vm_context, helper_stub);
            },
            HandlerVmInstruction::JumpDec => {
                self.lift_jump_sem(helper_stub, "JUMP_DEC");
            },
            HandlerVmInstruction::JumpInc => {
                self.lift_jump_sem(helper_stub, "JUMP_INC");
            },
            HandlerVmInstruction::JumpDecVspChange => {
                self.lift_jump_sem(helper_stub, "JUMP_DEC");
            },
            HandlerVmInstruction::JumpIncVspChange => {
                self.lift_jump_sem(helper_stub, "JUMP_INC");
            },
            HandlerVmInstruction::JumpDecVspXchng => {
                self.lift_jump_sem(helper_stub, "JUMP_DEC");
            },
            HandlerVmInstruction::JumpIncVspXchng => {
                self.lift_jump_sem(helper_stub, "JUMP_INC");
            },
            HandlerVmInstruction::PushCr0 => {
                self.lift_generic_handler_unsized("PUSH_CR0", helper_stub);
            },
            HandlerVmInstruction::PushCr3 => {
                self.lift_generic_handler_unsized("PUSH_CR3", helper_stub);
            },
            HandlerVmInstruction::Rdtsc => {
                self.lift_generic_handler_unsized("RDTSC", helper_stub);
            },
            HandlerVmInstruction::Nop => {
                // Yep nothing
            },
            HandlerVmInstruction::UnknownByteOperand => todo!("Unkwnown handler"),
            HandlerVmInstruction::UnknownWordOperand => todo!("Unkwnown handler"),
            HandlerVmInstruction::UnknownDwordOperand => todo!("Unkwnown handler"),
            HandlerVmInstruction::UnknownQwordOperand => todo!("Unkwnown handler"),
            HandlerVmInstruction::UnknownNoOperand => todo!("Unkwnown handler"),
            HandlerVmInstruction::UnknownNoVipChange => todo!("Unkwnown handler"),
            HandlerVmInstruction::Unknown => todo!("Unkwnown handler"),
        }
    }

    fn lift_jump_sem(&self,
                     stub_function: &FunctionValue,
                     semantic_name: &str) {
        let vsp = get_param_vsp(stub_function);
        let vip = get_param_vip(stub_function);

        let a_semantic = self.get_semantic(&format!("SEM_{}", semantic_name));
        self.builder
            .build_call(a_semantic, &[vsp.into(), vip.into()], "");
    }

    fn lift_generic_handler(&self,
                            size: usize,
                            sem_base_name: &str,
                            helper_stub: &FunctionValue) {
        let vsp = get_param_vsp(helper_stub);

        let semantic = self.get_semantic(&format!("SEM_{}_{}", sem_base_name, size * 8));

        self.builder.build_call(semantic, &[vsp.into()], "");
    }

    fn lift_generic_handler_unsized(&self,
                                    sem_base_name: &str,
                                    helper_stub: &FunctionValue) {
        let vsp = get_param_vsp(helper_stub);

        let semantic = self.get_semantic(&format!("SEM_{}", sem_base_name));

        self.builder.build_call(semantic, &[vsp.into()], "");
    }

    fn lift_push_imm64(&self,
                       imm64: u64,
                       helper_stub: &FunctionValue) {
        let vsp = get_param_vsp(helper_stub);
        let i64_type = self.context.i64_type();

        let sem_push_imm64 = self.get_semantic("SEM_PUSH_IMM_64");
        let imm64_llvm = i64_type.const_int(imm64, false);

        self.builder
            .build_call(sem_push_imm64, &[vsp.into(), imm64_llvm.into()], "");
    }

    fn lift_push_imm32(&self,
                       imm32: u32,
                       helper_stub: &FunctionValue) {
        let vsp = get_param_vsp(helper_stub);
        let i32_type = self.context.i32_type();

        let sem_push_imm32 = self.get_semantic("SEM_PUSH_IMM_32");
        let imm32_llvm = i32_type.const_int(imm32 as u64, false);

        self.builder
            .build_call(sem_push_imm32, &[vsp.into(), imm32_llvm.into()], "");
    }

    fn lift_push_imm16(&self,
                       imm16: u16,
                       helper_stub: &FunctionValue) {
        let vsp = get_param_vsp(helper_stub);
        let i16_type = self.context.i16_type();

        let sem_push_imm16 = self.get_semantic("SEM_PUSH_IMM_16");
        let imm16_llvm = i16_type.const_int(imm16 as u64, false);

        self.builder
            .build_call(sem_push_imm16, &[vsp.into(), imm16_llvm.into()], "");
    }

    fn lift_push(&self,
                 size: usize,
                 reg_offset: u8,
                 helper_stub: &FunctionValue) {
        let vsp = get_param_vsp(helper_stub);
        let vm_regs = get_param_vm_regs(helper_stub);
        let i64_type = self.context.i64_type();

        let register_number = reg_offset / 8;
        let intra_register_offset = reg_offset % 8;

        let select_vm_reg = unsafe {
            self.builder.build_gep(vm_regs,
                                   &[i64_type.const_int(register_number as _, false)],
                                   "vm_reg")
        };

        let select_vm_reg =
            self.builder.build_pointer_cast(select_vm_reg,
                                            i64_type.ptr_type(inkwell::AddressSpace::Generic),
                                            "");

        let vm_reg = self.builder.build_load(select_vm_reg, "");

        match size {
            8 => {
                let sem_push_vmreg64 = self.get_semantic("SEM_PUSH_VMREG_64");

                self.builder
                    .build_call(sem_push_vmreg64, &[vsp.into(), vm_reg.into()], "");
            },
            4 => match intra_register_offset {
                0 => {
                    let sem_push_vmreg32_low = self.get_semantic("SEM_PUSH_VMREG_32_LOW");

                    self.builder
                        .build_call(sem_push_vmreg32_low, &[vsp.into(), vm_reg.into()], "");
                },
                4 => {
                    let sem_push_vmreg32_high = self.get_semantic("SEM_PUSH_VMREG_32_HIGH");

                    self.builder
                        .build_call(sem_push_vmreg32_high, &[vsp.into(), vm_reg.into()], "");
                },
                _ => unreachable!(),
            },
            2 => match intra_register_offset {
                0 => {
                    let sem_push_vmreg16_lowlow = self.get_semantic("SEM_PUSH_VMREG_16_LOWLOW");

                    self.builder
                        .build_call(sem_push_vmreg16_lowlow, &[vsp.into(), vm_reg.into()], "");
                },
                2 => {
                    let sem_push_vmreg16_lowhigh = self.get_semantic("SEM_PUSH_VMREG_16_LOWHIGH");

                    self.builder
                        .build_call(sem_push_vmreg16_lowhigh, &[vsp.into(), vm_reg.into()], "");
                },
                4 => {
                    let sem_push_vmreg16_highlow = self.get_semantic("SEM_PUSH_VMREG_16_HIGHLOW");

                    self.builder
                        .build_call(sem_push_vmreg16_highlow, &[vsp.into(), vm_reg.into()], "");
                },
                6 => {
                    let sem_push_vmreg16_highhigh = self.get_semantic("SEM_PUSH_VMREG_16_HIGHHIGH");

                    self.builder
                        .build_call(sem_push_vmreg16_highhigh, &[vsp.into(), vm_reg.into()], "");
                },
                _ => unreachable!(),
            },

            1 => {
                assert!(intra_register_offset == 1 || intra_register_offset == 0);
                match intra_register_offset {
                    0 => {
                        let sem_push_vmreg8_low = self.get_semantic("SEM_PUSH_VMREG_8_LOW");

                        self.builder.build_call(sem_push_vmreg8_low,
                                                &[vsp.into(), select_vm_reg.into()],
                                                "");
                    },
                    1 => {
                        let sem_push_vmreg8_high = self.get_semantic("SEM_PUSH_VMREG_8_HIGH");

                        self.builder.build_call(sem_push_vmreg8_high,
                                                &[vsp.into(), select_vm_reg.into()],
                                                "");
                    },
                    _ => unreachable!(),
                }
            },
            _ => todo!(),
        }
    }
    fn lift_pop(&self,
                size: usize,
                reg_offset: u8,
                helper_stub: &FunctionValue) {
        let vsp = get_param_vsp(helper_stub);
        let vm_regs = get_param_vm_regs(helper_stub);
        let i64_type = self.context.i64_type();

        let register_number = reg_offset / 8;
        let intra_register_offset = reg_offset % 8;

        let select_vm_reg = unsafe {
            self.builder.build_gep(vm_regs,
                                   &[i64_type.const_int(register_number as _, false)],
                                   "vm_reg")
        };

        match size {
            8 => {
                assert!(intra_register_offset == 0);

                let sem_pop_vmreg64 = self.get_semantic("SEM_POP_VMREG_64");

                self.builder
                    .build_call(sem_pop_vmreg64, &[vsp.into(), select_vm_reg.into()], "");
            },
            4 => {
                assert!(intra_register_offset == 4 || intra_register_offset == 0);
                match intra_register_offset {
                    0 => {
                        let sem_pop_vmreg32_low = self.get_semantic("SEM_POP_VMREG_32_LOW");

                        self.builder.build_call(sem_pop_vmreg32_low,
                                                &[vsp.into(), select_vm_reg.into()],
                                                "");
                    },
                    4 => {
                        let sem_pop_vmreg32_high = self.get_semantic("SEM_POP_VMREG_32_HIGH");

                        self.builder.build_call(sem_pop_vmreg32_high,
                                                &[vsp.into(), select_vm_reg.into()],
                                                "");
                    },
                    _ => unreachable!(),
                }
            },

            2 => match intra_register_offset {
                0 => {
                    let sem_pop_vmreg16_lowlow = self.get_semantic("SEM_POP_VMREG_16_LOWLOW");

                    self.builder
                        .build_call(sem_pop_vmreg16_lowlow, &[vsp.into(), vm_regs.into()], "");
                },
                2 => {
                    let sem_pop_vmreg16_lowhigh = self.get_semantic("SEM_POP_VMREG_16_LOWHIGH");

                    self.builder
                        .build_call(sem_pop_vmreg16_lowhigh, &[vsp.into(), vm_regs.into()], "");
                },
                4 => {
                    let sem_pop_vmreg16_highlow = self.get_semantic("SEM_POP_VMREG_16_HIGHLOW");

                    self.builder
                        .build_call(sem_pop_vmreg16_highlow, &[vsp.into(), vm_regs.into()], "");
                },
                6 => {
                    let sem_pop_vmreg16_highhigh = self.get_semantic("SEM_POP_VMREG_16_HIGHHIGH");

                    self.builder
                        .build_call(sem_pop_vmreg16_highhigh, &[vsp.into(), vm_regs.into()], "");
                },
                _ => unreachable!(),
            },

            1 => {
                assert!(intra_register_offset == 1 || intra_register_offset == 0);
                match intra_register_offset {
                    0 => {
                        let sem_pop_vmreg8_low = self.get_semantic("SEM_POP_VMREG_8_LOW");

                        self.builder.build_call(sem_pop_vmreg8_low,
                                                &[vsp.into(), select_vm_reg.into()],
                                                "");
                    },
                    1 => {
                        let sem_pop_vmreg8_high = self.get_semantic("SEM_POP_VMREG_8_HIGH");

                        self.builder.build_call(sem_pop_vmreg8_high,
                                                &[vsp.into(), select_vm_reg.into()],
                                                "");
                    },
                    _ => unreachable!(),
                }
            },
            _ => todo!(),
        }
    }
    fn get_semantic(&self,
                    name: &str)
                    -> CallableValue {
        let global_value = self.module
                               .borrow()
                               .get_global(name)
                               .unwrap_or_else(|| panic!("Could not find SEMANTIC {}", name));
        let sem_pointer = self.builder
                              .build_load(global_value.as_pointer_value(), "")
                              .into_pointer_value();

        CallableValue::try_from(sem_pointer).unwrap()
    }

    fn lift_vm_exit(&self,
                    vm_context: &VmContext,
                    stub_function: &FunctionValue) {
        let vsp = get_param_vsp(stub_function);
        let vip = get_param_vip(stub_function);
       
        let vsp_val = self.builder.build_load(vsp, "vsp_val");
        let rsp_ptr = get_param_from_reg(&Registers::Rsp, stub_function);
        self.builder.build_store(rsp_ptr, vsp_val);

        for reg in vm_context.push_order.iter().rev() {
            let sem_push_reg64 = self.get_semantic("SEM_POP_REG_64");
            let reg_ptr = get_param_from_reg(reg, stub_function);
            self.builder
                .build_call(sem_push_reg64, &[vsp.into(), reg_ptr.into()], "");
        }

        let exit_semantic = self.get_semantic("SEM_EXIT");
        self.builder
            .build_call(exit_semantic, &[vsp.into(), vip.into()], "");
    }

    fn lift_vm_entry(&self,
                     vm_context: &VmContext,
                     stub_function: &FunctionValue) {
        let i64_type = self.context.i64_type();
        let vsp = get_param_vsp(stub_function);

        // This type does not implement clone that's the reason for resolving it three
        // times :) Yes this is fucking retarded :)
        let sem_push_imm64 = self.get_semantic("SEM_PUSH_IMM_64");
        let pushed_key = i64_type.const_int(vm_context.pushed_val, false);

        self.builder
            .build_call(sem_push_imm64, &[vsp.into(), pushed_key.into()], "");

        // This type does not implement clone that's the reason for resolving it three
        // times :)
        let sem_push_imm64 = self.get_semantic("SEM_PUSH_IMM_64");

        let ret_addr = i64_type.const_int(vm_context.vm_call_address + 10, false);
        let reloc = i64_type.const_int(vm_context.reloc_value, false);

        self.builder
            .build_call(sem_push_imm64, &[vsp.into(), ret_addr.into()], "");

        for reg in vm_context.push_order.iter() {
            let sem_push_reg64 = self.get_semantic("SEM_PUSH_REG_64");
            let reg_ptr = get_param_from_reg(reg, stub_function);
            let reg_value = self.builder.build_load(reg_ptr, "");
            self.builder
                .build_call(sem_push_reg64, &[vsp.into(), reg_value.into()], "");
        }

        // This type does not implement clone that's the reason for resolving it three
        // times :)
        let sem_push_imm64 = self.get_semantic("SEM_PUSH_IMM_64");
        self.builder
            .build_call(sem_push_imm64, &[vsp.into(), reloc.into()], "");
    }

    fn lift_into_helper_stub(&self,
                             vm_context: &VmContext,
                             handlers: &[(u64, HandlerVmInstruction, u64)],
                             helper_stub: &FunctionValue) {
        let entry_bb = helper_stub.get_basic_blocks()[0];
        let first_instruction = entry_bb.get_first_instruction().unwrap();
        self.builder.position_before(&first_instruction);

        self.lift_vm_entry(vm_context, helper_stub);

        for (_, handler, _) in handlers.iter() {
            self.lift_vm_instruction(vm_context, handler, helper_stub);
        }
    }

    pub fn create_helper_function(&self,
                                  control_flow_graph: &GraphMap<u64, (), petgraph::Directed>,
                                  start_vip: u64,
                                  vm_call_address: u64) -> FunctionValue {




        let helper_function_def = self.module
                                      .borrow()
                                      .get_function("HelperFunction")
                                      .expect("Could not find HelperFunction in llvm ir file");

        let llvm_lifetime_start_p0i8 =
            self.module
                .borrow()
                .get_function("llvm.lifetime.start.p0i8")
                .expect("Could not find llvm.lifetime.start.p0i8 in llvm ir file");

        let llvm_memset_p0i8_i64 =
            self.module
                .borrow()
                .get_function("llvm.memset.p0i8.i64")
                .expect("Could not find llvm.memset.p0i8.i64 in llvm ir file");

        let helper_function_type = helper_function_def.get_type();

        let helper_function = self.module
                                  .borrow()
                                  .add_function(&format!("helperfunction_{:x}", vm_call_address),
                                                helper_function_type,
                                                None);
        let param_names = ["rax", "rbx", "rcx", "rdx", "rsi", "rdi", "rbp", "rsp", "r8", "r9",
                           "r10", "r11", "r12", "r13", "r14", "r15", "flags", "KEY_STUB",
                           "RET_ADDR", "REL_ADDR"];

        for (i, (&name, param)) in
            std::iter::zip(param_names.iter(), helper_function.get_param_iter()).enumerate()
        {
            param.set_name(name);

            if name != "KEY_STUB" && name != "RET_ADDR" && name != "REL_ADDR" {
                helper_function.add_attribute(AttributeLoc::Param(i as _), self.context.create_enum_attribute(Attribute::get_named_enum_kind_id("noalias"), 0));
            }
        }

        // Cloning is all kinds of fucked with the rust lib so it is what it is
        let entry_bb = self.context.append_basic_block(helper_function, "entry");
        self.builder.position_at_end(entry_bb);

        let i64_type = self.context.i64_type();
        let i8_type = self.context.i8_type();
        let bool_type = self.context.bool_type();

        let virtual_register_type = self.module
                                        .borrow()
                                        .get_struct_type("struct.VirtualRegister")
                                        .unwrap();
        let virtual_register_array_type = virtual_register_type.array_type(30);

        let vmregs = self.builder
                         .build_alloca(virtual_register_array_type, "vmregs");

        let i64_array_type = i64_type.array_type(30);
        let slots = self.builder.build_alloca(i64_array_type, "slots");

        let vip = self.builder.build_alloca(i64_type, "vip");

        let t0 = self.builder
                     .build_bitcast(vmregs, i8_type.ptr_type(inkwell::AddressSpace::Generic), "");

        self.builder.build_call(llvm_lifetime_start_p0i8,
                                &[i64_type.const_int(240, false).into(), t0.into()],
                                "");
        self.builder.build_call(llvm_memset_p0i8_i64,
                                &[t0.into(),
                                  i8_type.const_zero().into(),
                                  i64_type.const_int(240, false).into(),
                                  bool_type.const_zero().into()],
                                "");

        let t1 = self.builder
                     .build_bitcast(slots, i8_type.ptr_type(inkwell::AddressSpace::Generic), "");

        self.builder.build_call(llvm_lifetime_start_p0i8,
                                &[i64_type.const_int(240, false).into(), t1.into()],
                                "");
        self.builder.build_call(llvm_memset_p0i8_i64,
                                &[t1.into(),
                                  i8_type.const_zero().into(),
                                  i64_type.const_int(240, false).into(),
                                  bool_type.const_zero().into()],
                                "");

        let t2 = self.builder
                     .build_bitcast(vip, i8_type.ptr_type(inkwell::AddressSpace::Generic), "");

        self.builder.build_call(llvm_lifetime_start_p0i8,
                                &[i64_type.const_int(8, false).into(), t2.into()],
                                "");

        let start_vip_llvm = i64_type.const_int(start_vip, false);
        self.builder.build_store(vip, start_vip_llvm);

        let array_decay = unsafe {
            self.builder.build_gep(vmregs,
                                   &[i64_type.const_zero(), i64_type.const_zero()],
                                   "arraydecay")
        };
        let array_decay1 = unsafe {
            self.builder.build_gep(slots,
                                   &[i64_type.const_zero(), i64_type.const_zero()],
                                   "arraydecay")
        };

        let mut bbs_map = HashMap::new();

        for node in control_flow_graph.nodes() {
            let stub_bb = self.context
                              .append_basic_block(helper_function, &format!("stub_{:x}", node));
            bbs_map.insert(node, stub_bb);
        }

        self.builder.build_unconditional_branch(bbs_map[&start_vip]);

        for node in control_flow_graph.nodes() {
            self.lift_bb_stub(control_flow_graph,
                              &helper_function,
                              &bbs_map,
                              node,
                              vip,
                              array_decay,
                              array_decay1,
                              t0,
                              t1,
                              t2);
        }

        assert!(helper_function.verify(true));
        helper_function
    }

    fn lift_bb_stub(&self,
                    control_flow_graph: &GraphMap<u64, (), petgraph::Directed>,
                    helper_function: &FunctionValue,
                    bbs_map: &HashMap<u64, BasicBlock>,
                    vip: u64,
                    vip_arg: PointerValue,
                    array_decay: PointerValue,
                    array_decay1: PointerValue,
                    t0: BasicValueEnum,
                    t1: BasicValueEnum,
                    t2: BasicValueEnum) {
        let i64_type = self.context.i64_type();

        println!("Getting helper stub -> helperstub_{:x}", vip);
        let helper_stub = self.module
                              .borrow()
                              .get_function(&format!("helperstub_{:x}", vip))
                              .unwrap();

        self.builder.position_at_end(bbs_map[&vip]);

        let call = self.builder.build_call(helper_stub,
                                           &[helper_function.get_nth_param(0).unwrap().into(),
                                             helper_function.get_nth_param(1).unwrap().into(),
                                             helper_function.get_nth_param(2).unwrap().into(),
                                             helper_function.get_nth_param(3).unwrap().into(),
                                             helper_function.get_nth_param(4).unwrap().into(),
                                             helper_function.get_nth_param(5).unwrap().into(),
                                             helper_function.get_nth_param(6).unwrap().into(),
                                             helper_function.get_nth_param(7).unwrap().into(),
                                             helper_function.get_nth_param(8).unwrap().into(),
                                             helper_function.get_nth_param(9).unwrap().into(),
                                             helper_function.get_nth_param(10).unwrap().into(),
                                             helper_function.get_nth_param(11).unwrap().into(),
                                             helper_function.get_nth_param(12).unwrap().into(),
                                             helper_function.get_nth_param(13).unwrap().into(),
                                             helper_function.get_nth_param(14).unwrap().into(),
                                             helper_function.get_nth_param(15).unwrap().into(),
                                             helper_function.get_nth_param(16).unwrap().into(),
                                             helper_function.get_nth_param(17).unwrap().into(),
                                             helper_function.get_nth_param(18).unwrap().into(),
                                             i64_type.const_int(0, false).into(),
                                             helper_function.get_nth_param(7).unwrap().into(),
                                             vip_arg.into(),
                                             array_decay.into(),
                                             array_decay1.into()],
                                           "call");
        let successors = control_flow_graph.edges_directed(vip, petgraph::EdgeDirection::Outgoing)
                                           .map(|(_, target, _)| target)
                                           .collect::<Vec<u64>>();

        match successors.len() {
            0 => {
                let llvm_lifetime_end_p0i8 =
                    self.module
                        .borrow()
                        .get_function("llvm.lifetime.end.p0i8")
                        .expect("Could not find llvm.lifetime.end.p0i8 in llvm ir file");

                let undef = self.module.borrow().get_global("__undef").unwrap();
                let t3 = self.builder.build_load(undef.as_pointer_value(), "");

                self.builder.build_store(helper_function.get_nth_param(16)
                                                        .unwrap()
                                                        .into_pointer_value(),
                                         t3);
                self.builder.build_call(llvm_lifetime_end_p0i8,
                                        &[i64_type.const_int(8, false).into(), t2.into()],
                                        "");
                self.builder.build_call(llvm_lifetime_end_p0i8,
                                        &[i64_type.const_int(240, false).into(), t1.into()],
                                        "");
                self.builder.build_call(llvm_lifetime_end_p0i8,
                                        &[i64_type.const_int(240, false).into(), t0.into()],
                                        "");

                self.builder
                    .build_return(Some(&call.try_as_basic_value().unwrap_left()));
            },
            1 => {
                let destination_vip = successors[0];
                let destination_bb = bbs_map[&destination_vip];
                self.builder.build_unconditional_branch(destination_bb);
            },
            2 => {
                let vip_value = self.builder.build_load(vip_arg, "vip_value");

                let branch_target1 = successors[0];
                let branch_target2 = successors[1];

                let llvm_branch_target1 = i64_type.const_int(branch_target1, false);

                let branch_selector = self.builder.build_int_compare(inkwell::IntPredicate::EQ,
                                                                     vip_value.into_int_value(),
                                                                     llvm_branch_target1,
                                                                     "eq_br1");

                let branch1 = bbs_map[&branch_target1];
                let branch2 = bbs_map[&branch_target2];

                self.builder
                    .build_conditional_branch(branch_selector, branch1, branch2);
            },
            _ => {
                let unreachable_bb = self.context
                                         .append_basic_block(*helper_function, "unreachable");

                let vip_value = self.builder.build_load(vip_arg, "vip_value");

                let cases =
                    successors.iter()
                              .map(|succ| (i64_type.const_int(*succ, false), bbs_map[&succ]))
                              .collect::<Vec<_>>();

                self.builder
                    .build_switch(vip_value.into_int_value(), unreachable_bb, &cases);

                self.builder.position_at_end(unreachable_bb);
                self.builder.build_unreachable();
            },
        }
    }
    

    pub fn create_helper_slicevpc(&self,
                                  vips: &[u64]) {
        let start_vip = vips[0];

        let helper_function_def = self.module
                                      .borrow()
                                      .get_function("HelperFunction")
                                      .expect("Could not find HelperFunction in llvm ir file");

        let llvm_lifetime_start_p0i8 =
            self.module
                .borrow()
                .get_function("llvm.lifetime.start.p0i8")
                .expect("Could not find llvm.lifetime.start.p0i8 in llvm ir file");

        let llvm_lifetime_end_p0i8 =
            self.module
                .borrow()
                .get_function("llvm.lifetime.end.p0i8")
                .expect("Could not find llvm.lifetime.end.p0i8 in llvm ir file");

        let llvm_memset_p0i8_i64 =
            self.module
                .borrow()
                .get_function("llvm.memset.p0i8.i64")
                .expect("Could not find llvm.memset.p0i8.i64 in llvm ir file");

        let helper_function_type = helper_function_def.get_type();

        let helper_slicevpc = self.module
                                  .borrow()
                                  .add_function("helperslicevpc", helper_function_type, None);
        let param_names = ["rax", "rbx", "rcx", "rdx", "rsi", "rdi", "rbp", "rsp", "r8", "r9",
                           "r10", "r11", "r12", "r13", "r14", "r15", "flags", "KEY_STUB",
                           "RET_ADDR", "REL_ADDR"];

        for (i, (&name, param)) in
            std::iter::zip(param_names.iter(), helper_slicevpc.get_param_iter()).enumerate()
        {
            param.set_name(name);

            if name != "KEY_STUB" && name != "RET_ADDR" && name != "REL_ADDR" {
                helper_slicevpc.add_attribute(AttributeLoc::Param(i as _), self.context.create_enum_attribute(Attribute::get_named_enum_kind_id("noalias"), 0));
            }
        }

        // Cloning is all kinds of fucked with the rust lib so it is what it is
        let entry_bb = self.context.append_basic_block(helper_slicevpc, "entry");
        self.builder.position_at_end(entry_bb);

        let i64_type = self.context.i64_type();
        let i8_type = self.context.i8_type();
        let bool_type = self.context.bool_type();

        let virtual_register_type = self.module
                                        .borrow()
                                        .get_struct_type("struct.VirtualRegister")
                                        .unwrap();
        let virtual_register_array_type = virtual_register_type.array_type(30);

        let vmregs = self.builder
                         .build_alloca(virtual_register_array_type, "vmregs");

        let i64_array_type = i64_type.array_type(30);
        let slots = self.builder.build_alloca(i64_array_type, "slots");

        let vip = self.builder.build_alloca(i64_type, "vip");

        let t0 = self.builder
                     .build_bitcast(vmregs, i8_type.ptr_type(inkwell::AddressSpace::Generic), "");

        self.builder.build_call(llvm_lifetime_start_p0i8,
                                &[i64_type.const_int(240, false).into(), t0.into()],
                                "");
        self.builder.build_call(llvm_memset_p0i8_i64,
                                &[t0.into(),
                                  i8_type.const_zero().into(),
                                  i64_type.const_int(240, false).into(),
                                  bool_type.const_zero().into()],
                                "");

        let t1 = self.builder
                     .build_bitcast(slots, i8_type.ptr_type(inkwell::AddressSpace::Generic), "");

        self.builder.build_call(llvm_lifetime_start_p0i8,
                                &[i64_type.const_int(240, false).into(), t1.into()],
                                "");
        self.builder.build_call(llvm_memset_p0i8_i64,
                                &[t1.into(),
                                  i8_type.const_zero().into(),
                                  i64_type.const_int(240, false).into(),
                                  bool_type.const_zero().into()],
                                "");

        let t2 = self.builder
                     .build_bitcast(vip, i8_type.ptr_type(inkwell::AddressSpace::Generic), "");

        self.builder.build_call(llvm_lifetime_start_p0i8,
                                &[i64_type.const_int(8, false).into(), t2.into()],
                                "");

        let start_vip_llvm = i64_type.const_int(start_vip, false);
        self.builder.build_store(vip, start_vip_llvm);

        let array_decay = unsafe {
            self.builder.build_gep(vmregs,
                                   &[i64_type.const_zero(), i64_type.const_zero()],
                                   "arraydecay")
        };
        let array_decay1 = unsafe {
            self.builder.build_gep(slots,
                                   &[i64_type.const_zero(), i64_type.const_zero()],
                                   "arraydecay")
        };

        let mut call = None;
        for stub_vip in vips.iter().rev() {
            // println!("Getting stub -> {:#x}", stub_vip);
            let helper_stub = self.module
                                  .borrow()
                                  .get_function(&format!("helperstub_{:x}", stub_vip))
                                  .unwrap();

            let temp =
                self.builder.build_call(helper_stub,
                                        &[helper_slicevpc.get_nth_param(0).unwrap().into(),
                                          helper_slicevpc.get_nth_param(1).unwrap().into(),
                                          helper_slicevpc.get_nth_param(2).unwrap().into(),
                                          helper_slicevpc.get_nth_param(3).unwrap().into(),
                                          helper_slicevpc.get_nth_param(4).unwrap().into(),
                                          helper_slicevpc.get_nth_param(5).unwrap().into(),
                                          helper_slicevpc.get_nth_param(6).unwrap().into(),
                                          helper_slicevpc.get_nth_param(7).unwrap().into(),
                                          helper_slicevpc.get_nth_param(8).unwrap().into(),
                                          helper_slicevpc.get_nth_param(9).unwrap().into(),
                                          helper_slicevpc.get_nth_param(10).unwrap().into(),
                                          helper_slicevpc.get_nth_param(11).unwrap().into(),
                                          helper_slicevpc.get_nth_param(12).unwrap().into(),
                                          helper_slicevpc.get_nth_param(13).unwrap().into(),
                                          helper_slicevpc.get_nth_param(14).unwrap().into(),
                                          helper_slicevpc.get_nth_param(15).unwrap().into(),
                                          helper_slicevpc.get_nth_param(16).unwrap().into(),
                                          helper_slicevpc.get_nth_param(17).unwrap().into(),
                                          helper_slicevpc.get_nth_param(18).unwrap().into(),
                                          i64_type.const_int(0, false).into(),
                                          helper_slicevpc.get_nth_param(7).unwrap().into(),
                                          vip.into(),
                                          array_decay.into(),
                                          array_decay1.into()],
                                        "call");

            call = Some(temp);
        }

        let undef = self.module.borrow().get_global("__undef").unwrap();
        let t3 = self.builder.build_load(undef.as_pointer_value(), "");

        self.builder.build_store(helper_slicevpc.get_nth_param(16)
                                                .unwrap()
                                                .into_pointer_value(),
                                 t3);
        self.builder.build_call(llvm_lifetime_end_p0i8,
                                &[i64_type.const_int(8, false).into(), t2.into()],
                                "");
        self.builder.build_call(llvm_lifetime_end_p0i8,
                                &[i64_type.const_int(240, false).into(), t1.into()],
                                "");
        self.builder.build_call(llvm_lifetime_end_p0i8,
                                &[i64_type.const_int(240, false).into(), t0.into()],
                                "");

        self.builder
            .build_return(Some(&call.unwrap().try_as_basic_value().unwrap_left()));

        assert!(helper_slicevpc.verify(true));
    }
}

fn get_param_from_reg<'a>(register: &Registers,
                          helper_stub: &FunctionValue<'a>)
                          -> PointerValue<'a> {
    helper_stub.get_nth_param(register.to_arg_index() as _)
               .unwrap()
               .into_pointer_value()
}

#[allow(dead_code)]
fn get_param_reloc<'a>(helper_stub: &FunctionValue<'a>) -> BasicValueEnum<'a> {
    helper_stub.get_nth_param(19).unwrap()
}

fn get_param_vsp<'a>(helper_stub: &FunctionValue<'a>) -> PointerValue<'a> {
    helper_stub.get_nth_param(20).unwrap().into_pointer_value()
}

fn get_param_vip<'a>(helper_stub: &FunctionValue<'a>) -> PointerValue<'a> {
    helper_stub.get_nth_param(21).unwrap().into_pointer_value()
}

fn get_param_vm_regs<'a>(helper_stub: &FunctionValue<'a>) -> PointerValue<'a> {
    helper_stub.get_nth_param(22).unwrap().into_pointer_value()
}

#[allow(dead_code)]
fn get_operand_names(instruction: &InstructionValue) -> Vec<String> {
    let mut return_value = Vec::new();
    let num_operands = instruction.get_num_operands();

    for i in 0 .. num_operands {
        let operand = instruction.get_operand(i).unwrap();
        assert!(operand.is_left());

        let operand = operand.left().unwrap();

        if operand.is_int_value() {
            let temp = operand.into_int_value();
            let name = temp.get_name();
            return_value.push(name.to_str().unwrap().to_owned());
            continue;
        }
        if operand.is_array_value() {
            let temp = operand.into_array_value();
            let name = temp.get_name();
            return_value.push(name.to_str().unwrap().to_owned());
            continue;
        }
        if operand.is_float_value() {
            let temp = operand.into_float_value();
            let name = temp.get_name();
            return_value.push(name.to_str().unwrap().to_owned());
            continue;
        }
        if operand.is_struct_value() {
            let temp = operand.into_struct_value();
            let name = temp.get_name();
            return_value.push(name.to_str().unwrap().to_owned());
            continue;
        }
        if operand.is_pointer_value() {
            let temp = operand.into_pointer_value();
            let name = temp.get_name();
            return_value.push(name.to_str().unwrap().to_owned());
            continue;
        }
        if operand.is_vector_value() {
            let temp = operand.into_vector_value();
            let name = temp.get_name();
            return_value.push(name.to_str().unwrap().to_owned());
            continue;
        }
    }

    return_value
}

```

`src/main.rs`:

```rs
use std::collections::{HashMap, HashSet, VecDeque};
use std::error::Error;

use pelite::pe64::PeFile;
use pelite::FileMap;

use clap::Parser;
use std::io::Write;

use petgraph::dot::Config;
use petgraph::graphmap::GraphMap;
use petgraph::visit::NodeRef;

mod llvm_ir_gen;
mod match_assembly;
mod symbolic;
mod transforms;
mod util;
mod vm_handler;
mod vm_matchers;

use vm_handler::VmContext;

use crate::llvm_ir_gen::VmLifter;
use crate::vm_matchers::HandlerVmInstruction;

fn parse_hex_vm_call(input_str: &str) -> Result<u64, std::num::ParseIntError> {
    let str_trimmed = input_str.trim_start_matches("0x");
    u64::from_str_radix(str_trimmed, 16)
}

#[derive(Parser, Debug)]
struct CommandLineArgs {
    /// Input file
    pub input_file:      String,
    /// Vm call address
    /// Address of the push instruction in
    /// push <const>
    /// call vm_entry
    #[clap(short, long, parse(try_from_str = parse_hex_vm_call))]
    pub vm_call_addresses: Vec<u64>,
    /// Max blocks for slicing
    #[clap(long)]
    pub max_blocks:      Option<u64>,
}

fn main() -> Result<(), Box<dyn Error>> {
    let command_line_args = CommandLineArgs::parse();
    let input_file = &command_line_args.input_file;

    let map = FileMap::open(input_file)?;
    let pe_file = PeFile::from_bytes(&map)?;
    let pe_bytes = std::fs::read(input_file)?;

    // Create the `VmLifter`
    let vm_lifter = VmLifter::new();

    // Lift all the functions specified
    for address in command_line_args.vm_call_addresses.iter().cloned() {
        explore_cfg_and_lift_function(&pe_file, &pe_bytes, address, &vm_lifter, command_line_args.max_blocks)?;
    }

    // We do not want to delete globals in this case as it messes up recompilation
    vm_lifter.optimize_module_no_global_delete();


    for address in command_line_args.vm_call_addresses.iter().cloned() {
        // Fix the argument names since a stripping pass is included
        vm_lifter.fix_arg_names(&format!("helperfunction_{:x}", address));

        // Print the function to stderr
        vm_lifter.print_function(&format!("helperfunction_{:x}", address));
    }


    // Output the module to an ir file devirt.ll
    vm_lifter.output_module();

    Ok(())
}

fn explore_cfg_and_lift_function(pe_file: &PeFile, pe_bytes: &[u8], vm_call_address: u64, vm_lifter: &VmLifter, max_blocks: Option<u64>) -> Result<(), Box<dyn Error>> {

    // Cfg exploration
    let mut explored = HashMap::new();
    let mut worklist = VecDeque::new();
    let mut reprove_list = VecDeque::new();

    let mut control_flow_graph = GraphMap::<u64, (), petgraph::Directed>::new();


    // `VmContext` for the first block
    let mut vm_context =
        VmContext::new_from_vm_entry(pe_file, pe_bytes, vm_call_address);

    let root_vip = vm_context.initial_vip;

    // Disassemble the handlers
    let handlers = vm_context.disassemble_context(pe_file, pe_bytes);
    // Get the last handler
    let last_handler = *handlers.last().unwrap();

    // Lift this first block into a stub
    vm_lifter.lift_helper_stub(&vm_context, &handlers);

    // Add the first block to the cfg
    control_flow_graph.add_node(root_vip);

    // Handle special case of the first block ending in nop
    if last_handler.1 == HandlerVmInstruction::Nop {

        worklist.push_back((vm_context.clone(), last_handler, vm_context.vip_value));
        explored.insert(root_vip, (vm_context, last_handler));
    
    } else if last_handler.1 != HandlerVmInstruction::VmExit {
        let next_vips = vm_lifter.slice_vip(&control_flow_graph,
                                            vm_context.initial_vip,
                                            root_vip,
                                            max_blocks)?;

        for target_vip in next_vips {
            worklist.push_back((vm_context.clone(), last_handler, target_vip));
            explored.insert(root_vip, (vm_context.clone(), last_handler));
        }
    }

    loop {
        // Print the current lists

        println!("Worklist {:#x?}",
                 worklist.iter()
                         .map(|(_, _, target)| target)
                         .collect::<Vec<_>>());
        println!("Explored {:#x?}", explored.keys());
        println!("Reprove {:#x?}",
                 reprove_list.iter()
                             .map(|(_, _, target)| target)
                             .collect::<Vec<_>>());

        // If the worklist is empty extend it with the reprove list, if that is empty
        // we're done
        if worklist.is_empty() {
            if reprove_list.is_empty() {
                break;
            } else {
                while !reprove_list.is_empty() {
                    let reprove = reprove_list.pop_front().unwrap();
                    worklist.push_back(reprove);
                }
            }
        }

        // Should never panic because we explicitly check that the list is not empty
        // first
        // Get an item from the worklist
        // Contains the vm_context of the handler that branched to current_block_vip
        // And the last handler that branched to current_block_vip
        // This iteration we start disassembling from current_block_vip
        let (prev_block_vm_context, last_prev_block_handler, current_block_vip) =
            worklist.pop_front().unwrap();

        // Check if already visited
        if explored.contains_key(&current_block_vip) {
            // Ok nothing to do because we already new about this edge
            if control_flow_graph.contains_edge(prev_block_vm_context.initial_vip,
                                                current_block_vip)
            {
                continue;
            }
            // Ok stuff to do because we did not know about this edge yet but it goes to a
            // block we did know about
            //
            //
            // Get the edges of the cfg that start from current_block_vip
            let outgoing_edges =
                control_flow_graph.edges_directed(current_block_vip,
                                                  petgraph::EdgeDirection::Outgoing);

            // target contains the destination of the edge from current_block_vip
            for (_, target, _) in outgoing_edges {
                // Ok get all the blocks the current_block_vip block could branch to
                let target_block = explored.get(&target);
                
                match target_block {
                    Some(entry) => {let (vm_context, last_handler) = entry;
                    // Add them to the reprove list
                     reprove_list.push_back((vm_context.clone(), *last_handler, target));
                // We remove them from explored
                   explored.remove(&target);},
                None => {
                        // Still in the worklist and not yet explored nothing to do
                    },
            }
            }
        }

        // Add the edge from the previous block to the current block
        println!("ADDING EDGE {:#x} -> {:#x}",
                 prev_block_vm_context.initial_vip, current_block_vip);
        control_flow_graph.add_edge(prev_block_vm_context.initial_vip, current_block_vip, ());

        // Ok we now explore this block so add it to explored
        explored.insert(current_block_vip,
                        (prev_block_vm_context.clone(), last_prev_block_handler));

        // This condition is here for the case of single block stubs
        if last_prev_block_handler.1 == HandlerVmInstruction::VmExit {
            continue;
        }

        // Debug printing
        println!("Previous vm_context start_vip -> {:#x}",
                 prev_block_vm_context.initial_vip);
        println!("Getting vm_context at current_block_vip -> {:#x}",
                 current_block_vip);

        let mut current_block_vm_context;

        // If the last block ended in a nop
        if last_prev_block_handler.1 == HandlerVmInstruction::Nop {
            current_block_vm_context = prev_block_vm_context;
            current_block_vm_context.initial_vip = current_block_vm_context.vip_value;
        } else {
            current_block_vm_context =
                prev_block_vm_context.new_from_jump_handler(&last_prev_block_handler,
                                                            current_block_vip,
                                                            pe_file,
                                                            pe_bytes);
        }

        // Get the new handler of the current_block_vip block
        let current_block_handlers =
            current_block_vm_context.disassemble_context(pe_file, pe_bytes);

        // If this panics shit is fucked anyways
        let last_current_block_handler = *current_block_handlers.last().unwrap();

        // Lift the new handlers into a helper stub
        vm_lifter.lift_helper_stub(&current_block_vm_context, &current_block_handlers);

        // Skip slicing since exit
        if last_current_block_handler.1 == HandlerVmInstruction::VmExit {
            continue;
        }

        // Skip slicing because NOP
        if last_current_block_handler.1 == HandlerVmInstruction::Nop {
            let target_vip = current_block_vm_context.vip_value;
            println!("NOP target_vip -> {:#x}", target_vip);

            worklist.push_back((current_block_vm_context, last_current_block_handler, target_vip));
            continue;
        }

        let next_vips = vm_lifter.slice_vip(&control_flow_graph,
                                            current_block_vm_context.initial_vip,
                                            root_vip,
                                            max_blocks)?;
        for next_vip in next_vips {
            worklist.push_back((current_block_vm_context.clone(),
                                last_current_block_handler,
                                next_vip));
            println!("Next vip -> {:#x}", next_vip);
        }
    }

    // // Write out the control flow graph to a dot file
    // let mut dot_file = std::fs::File::create("cfg.dot")?;
    // writeln!(dot_file,
    //          "{:?}",
    //          petgraph::dot::Dot::with_attr_getters(&control_flow_graph,
    //                                                &[Config::EdgeNoLabel, Config::NodeNoLabel],
    //                                                &|_, _| { "".to_owned() },
    //                                                &|_, node_ref| {
    //                                                    format!("label = \"{:#x}\"",
    //                                                            node_ref.weight())
    //                                                }))?;

    // Create the final lifted function
    let helper_function = vm_lifter.create_helper_function(&control_flow_graph, root_vip, vm_call_address);
    Ok(())
}

```

`src/match_assembly.rs`:

```rs
use iced_x86::{Code, Instruction, OpKind, Register};

use crate::vm_handler::VmRegisterAllocation;

pub fn match_pushfq(instruction: &Instruction) -> bool {
    if instruction.code() != Code::Pushfq {
        return false;
    }
    true
}

pub fn match_popfq(instruction: &Instruction) -> bool {
    if instruction.code() != Code::Popfq {
        return false;
    }
    true
}

pub fn match_ret(instruction: &Instruction) -> bool {
    if instruction.code() != Code::Retnq {
        return false;
    }
    true
}

pub fn match_not_reg(instruction: &Instruction,
                     register: Register)
                     -> bool {
    match instruction.code() {
        Code::Not_rm8 | Code::Not_rm16 | Code::Not_rm32 | Code::Not_rm64
            if instruction.op0_register().full_register() == register =>
        {
            true
        },
        _ => false,
    }
}

pub fn match_mov_reg_source(instruction: &Instruction,
                            register: Register)
                            -> bool {
    match instruction.code() {
        Code::Mov_r64_rm64 |
        Code::Mov_r32_rm32 |
        Code::Mov_r16_rm16 |
        Code::Mov_r8_rm8 |
        Code::Mov_rm64_r64 |
        Code::Mov_rm32_r32 |
        Code::Mov_rm16_r16 |
        Code::Mov_rm8_r8 => {},
        _ => return false,
    }

    if instruction.op0_kind() != OpKind::Register {
        return false;
    }

    if instruction.op1_kind() != OpKind::Register {
        return false;
    }

    if instruction.op1_register() != register {
        return false;
    }

    true
}

#[allow(dead_code)]
pub fn match_mov_reg2_in_reg1(instruction: &Instruction,
                              reg1: Register,
                              reg2: Register)
                              -> Option<usize> {
    let instruction_size = match instruction.code() {
        Code::Mov_rm8_r8 => 1,
        Code::Mov_rm16_r16 => 2,
        Code::Mov_rm32_r32 => 4,
        Code::Mov_rm64_r64 => 8,
        Code::Mov_r8_rm8 => 1,
        Code::Mov_r16_rm16 => 2,
        Code::Mov_r32_rm32 => 4,
        Code::Mov_r64_rm64 => 8,
        _ => return None,
    };

    if instruction.op0_kind() != OpKind::Register {
        return None;
    }

    if instruction.op1_kind() != OpKind::Register {
        return None;
    }

    if instruction.op0_register() != reg1 {
        return None;
    }

    if instruction.op1_register() != reg2 {
        return None;
    }

    Some(instruction_size)
}

/// Return size of the store in bytes
pub fn match_store_reg2_in_reg1(instruction: &Instruction,
                                reg1: Register,
                                reg2: Register)
                                -> Option<usize> {
    let instruction_size = match instruction.code() {
        Code::Mov_rm8_r8 => 1,
        Code::Mov_rm16_r16 => 2,
        Code::Mov_rm32_r32 => 4,
        Code::Mov_rm64_r64 => 8,
        _ => return None,
    };

    if instruction.op0_kind() != OpKind::Memory {
        return None;
    }

    if instruction.op1_kind() != OpKind::Register {
        return None;
    }

    if instruction.memory_base().full_register() != reg1 {
        return None;
    }

    if instruction.op1_register().full_register() != reg2 {
        return None;
    }

    Some(instruction_size)
}

pub fn match_xchng_reg(instruction: &Instruction,
                       reg: Register)
                       -> bool {
    match instruction.code() {
        Code::Xchg_rm64_r64 | Code::Xchg_r64_RAX => {
            if (instruction.op0_register() == reg.full_register() ||
                instruction.op1_register() == reg.full_register()) &&
               instruction.op0_register() != instruction.op1_register()
            {
                true
            } else {
                false
            }
        },
        _ => false,
    }
}

pub fn match_shl_reg_reg(instruction: &Instruction,
                         reg: Register)
                         -> bool {
    match instruction.code() {
        Code::Shl_rm8_CL | Code::Shl_rm16_CL | Code::Shl_rm32_CL | Code::Shl_rm64_CL
            if (instruction.op0_register().full_register() == reg.full_register()) =>
        {
            true
        },

        _ => false,
    }
}

pub fn match_shr_reg_reg(instruction: &Instruction,
                         reg: Register)
                         -> bool {
    match instruction.code() {
        Code::Shr_rm8_CL | Code::Shr_rm16_CL | Code::Shr_rm32_CL | Code::Shr_rm64_CL
            if (instruction.op0_register().full_register() == reg.full_register()) =>
        {
            true
        },

        _ => false,
    }
}

pub fn match_shrd_reg_reg(instruction: &Instruction,
                          reg: Register)
                          -> bool {
    match instruction.code() {
        Code::Shrd_rm16_r16_CL | Code::Shrd_rm32_r32_CL | Code::Shrd_rm64_r64_CL
            if (instruction.op0_register().full_register() == reg.full_register()) =>
        {
            true
        },

        _ => false,
    }
}

pub fn match_shld_reg_reg(instruction: &Instruction,
                          reg: Register)
                          -> bool {
    match instruction.code() {
        Code::Shld_rm16_r16_CL | Code::Shld_rm32_r32_CL | Code::Shld_rm64_r64_CL
            if (instruction.op0_register().full_register() == reg.full_register()) =>
        {
            true
        },

        _ => false,
    }
}

pub fn match_or_reg_reg(instruction: &Instruction,
                        reg1: Register,
                        reg2: Register)
                        -> bool {
    match instruction.code() {
        Code::Or_rm8_r8 |
        Code::Or_rm16_r16 |
        Code::Or_rm32_r32 |
        Code::Or_rm64_r64 |
        Code::Or_r8_rm8 |
        Code::Or_r16_rm16 |
        Code::Or_r32_rm32 |
        Code::Or_r64_rm64
            if (instruction.op0_register().full_register() == reg1 &&
                instruction.op1_register().full_register() == reg2) ||
               (instruction.op0_register().full_register() == reg2 &&
                instruction.op1_register().full_register() == reg1) =>
        {
            true
        },

        _ => false,
    }
}

pub fn match_and_reg_reg(instruction: &Instruction,
                         reg1: Register,
                         reg2: Register)
                         -> bool {
    match instruction.code() {
        Code::And_rm8_r8 |
        Code::And_rm16_r16 |
        Code::And_rm32_r32 |
        Code::And_rm64_r64 |
        Code::And_r8_rm8 |
        Code::And_r16_rm16 |
        Code::And_r32_rm32 |
        Code::And_r64_rm64
            if (instruction.op0_register().full_register() == reg1 &&
                instruction.op1_register().full_register() == reg2) ||
               (instruction.op0_register().full_register() == reg2 &&
                instruction.op1_register().full_register() == reg1) =>
        {
            true
        },

        _ => false,
    }
}

pub fn match_add_reg_reg(instruction: &Instruction,
                         reg1: Register,
                         reg2: Register)
                         -> bool {
    match instruction.code() {
        Code::Add_rm8_r8 |
        Code::Add_rm16_r16 |
        Code::Add_rm32_r32 |
        Code::Add_rm64_r64 |
        Code::Add_r8_rm8 |
        Code::Add_r16_rm16 |
        Code::Add_r32_rm32 |
        Code::Add_r64_rm64
            if (instruction.op0_register().full_register() == reg1 &&
                instruction.op1_register().full_register() == reg2) ||
               (instruction.op0_register().full_register() == reg2 &&
                instruction.op1_register().full_register() == reg1) =>
        {
            true
        },

        _ => false,
    }
}

pub fn match_mul_reg_reg(instruction: &Instruction,
                         reg1: Register,
                         reg2: Register)
                         -> bool {
    match instruction.code() {
        Code::Mul_rm64 | Code::Mul_rm32 | Code::Mul_rm16 | Code::Mul_rm8
            if (instruction.op0_register().full_register() == reg1 ||
                instruction.op0_register().full_register() == reg2) =>
        {
            true
        },

        _ => false,
    }
}

pub fn match_sub_reg_left(instruction: &Instruction,
                          reg1: Register)
                          -> bool {
    match instruction.code() {
        Code::Sub_rm8_r8 |
        Code::Sub_rm16_r16 |
        Code::Sub_rm32_r32 |
        Code::Sub_rm64_r64 |
        Code::Sub_r8_rm8 |
        Code::Sub_r16_rm16 |
        Code::Sub_r32_rm32 |
        Code::Sub_r64_rm64
            if (instruction.op0_register().full_register() == reg1) =>
        {
            true
        },

        _ => false,
    }
}

pub fn match_imul_reg_reg(instruction: &Instruction,
                          reg1: Register,
                          reg2: Register)
                          -> bool {
    match instruction.code() {
        Code::Imul_rm64 | Code::Imul_rm32 | Code::Imul_rm16 | Code::Imul_rm8
            if (instruction.op0_register().full_register() == reg1 ||
                instruction.op0_register().full_register() == reg2) =>
        {
            true
        },

        _ => false,
    }
}

/// Returns the size of the match in bytes if there is one
pub fn match_fetch_reg_any_size(instruction: &Instruction,
                                register: Register)
                                -> Option<usize> {
    let mov_size = match instruction.code() {
        Code::Mov_r64_rm64 if instruction.op1_kind() == OpKind::Memory => Some(8),
        Code::Mov_r32_rm32 if instruction.op1_kind() == OpKind::Memory => Some(4),
        Code::Mov_r16_rm16 if instruction.op1_kind() == OpKind::Memory => Some(2),
        Code::Mov_r8_rm8 if instruction.op1_kind() == OpKind::Memory => Some(1),
        Code::Movzx_r16_rm8 if instruction.op1_kind() == OpKind::Memory => Some(2),
        Code::Movzx_r32_rm8 if instruction.op1_kind() == OpKind::Memory => Some(4),
        Code::Movzx_r64_rm8 if instruction.op1_kind() == OpKind::Memory => Some(8),
        Code::Movzx_r16_rm16 if instruction.op1_kind() == OpKind::Memory => Some(2),
        Code::Movzx_r32_rm16 if instruction.op1_kind() == OpKind::Memory => Some(4),
        Code::Movzx_r64_rm16 if instruction.op1_kind() == OpKind::Memory => Some(8),
        _ => return None,
    };

    if mov_size.is_some() {
        if instruction.memory_base().full_register() == register {
            mov_size
        } else {
            None
        }
    } else {
        None
    }
}

pub fn match_fetch_zx_reg_any_size(instruction: &Instruction,
                                   register: Register)
                                   -> Option<usize> {
    let mov_size = match instruction.code() {
        Code::Movzx_r64_rm8 if instruction.op1_kind() == OpKind::Memory => Some(1),
        Code::Movzx_r64_rm16 if instruction.op1_kind() == OpKind::Memory => Some(2),
        Code::Movzx_r32_rm8 if instruction.op1_kind() == OpKind::Memory => Some(1),
        Code::Movzx_r32_rm16 if instruction.op1_kind() == OpKind::Memory => Some(2),
        Code::Movzx_r16_rm8 if instruction.op1_kind() == OpKind::Memory => Some(1),
        Code::Movzx_r16_rm16 if instruction.op1_kind() == OpKind::Memory => Some(2),
        _ => return None,
    };

    if mov_size.is_some() {
        if instruction.memory_base().full_register() == register {
            mov_size
        } else {
            None
        }
    } else {
        None
    }
}

pub fn match_store_reg_any_size(instruction: &Instruction,
                                register: Register)
                                -> Option<usize> {
    let mov_size = match instruction.code() {
        Code::Mov_rm64_r64 if instruction.op0_kind() == OpKind::Memory => Some(8),
        Code::Mov_rm32_r32 if instruction.op0_kind() == OpKind::Memory => Some(4),
        Code::Mov_rm16_r16 if instruction.op0_kind() == OpKind::Memory => Some(2),
        Code::Mov_rm8_r8 if instruction.op0_kind() == OpKind::Memory => Some(1),
        _ => return None,
    };

    if mov_size.is_some() {
        if instruction.memory_base().full_register() == register {
            mov_size
        } else {
            None
        }
    } else {
        None
    }
}

pub fn match_fetch_vm_reg(instruction: &Instruction,
                          index_reg: Register)
                          -> bool {
    // Check the instruction opcode
    if instruction.code() != Code::Mov_r64_rm64 &&
       instruction.code() != Code::Mov_r32_rm32 &&
       instruction.code() != Code::Mov_r16_rm16 &&
       instruction.code() != Code::Mov_r8_rm8
    {
        return false;
    }
    // Check that the second operand is a memory type operand
    if instruction.op1_kind() != OpKind::Memory {
        return false;
    }

    // Check that the displacement is 0
    if instruction.memory_displacement64() != 0x0 {
        return false;
    }

    // Check that the index register is rsp
    if instruction.memory_base() != Register::RSP {
        return false;
    }

    // Check that the index register is index_reg
    if instruction.memory_index() != index_reg {
        return false;
    }

    true
}

pub fn match_store_vm_reg(instruction: &Instruction,
                          index_reg: Register)
                          -> bool {
    // Check the instruction opcode
    if instruction.code() != Code::Mov_rm64_r64 &&
       instruction.code() != Code::Mov_rm32_r32 &&
       instruction.code() != Code::Mov_rm16_r16 &&
       instruction.code() != Code::Mov_rm8_r8
    {
        return false;
    }
    // Check that the second operand is a memory type operand
    if instruction.op0_kind() != OpKind::Memory {
        return false;
    }

    // Check that the displacement is 0
    if instruction.memory_displacement64() != 0x0 {
        return false;
    }

    // Check that the index register is rsp
    if instruction.memory_base() != Register::RSP {
        return false;
    }

    // Check that the index register is index_reg
    if instruction.memory_index() != index_reg {
        return false;
    }

    true
}

pub fn match_fetch_encrypted_vip(instruction: &Instruction,
                                 vm_register_allocation: &VmRegisterAllocation)
                                 -> bool {
    // Check the instruction opcode
    if instruction.code() != Code::Mov_r64_rm64 {
        return false;
    }

    // Check that the second operand is a memory type operand
    if instruction.op1_kind() != OpKind::Memory {
        return false;
    }

    // Check that the displacement is 90
    if instruction.memory_displacement64() != 0x90 {
        return false;
    }

    // Check that the index register is rsp
    if instruction.memory_base() != Register::RSP {
        return false;
    }

    // Check that the write is to vip
    if instruction.op0_register() != vm_register_allocation.vip.into() {
        return false;
    }

    true
}

pub fn match_fetch_vip(instruction: &Instruction,
                       vm_register_allocation: &VmRegisterAllocation)
                       -> bool {
    if instruction.code() != Code::Mov_r32_rm32 {
        return false;
    }

    if instruction.op1_kind() != OpKind::Memory {
        return false;
    }

    if instruction.memory_base() != vm_register_allocation.vip.into() {
        return false;
    }

    true
}

pub fn match_push_rolling_key(instruction: &Instruction,
                              vm_register_allocation: &VmRegisterAllocation)
                              -> bool {
    if instruction.code() != Code::Push_r64 {
        return false;
    }

    if instruction.op0_kind() != OpKind::Register {
        return false;
    }

    if instruction.op0_register().full_register() != vm_register_allocation.key.into() {
        return false;
    }

    true
}

pub fn match_add_vsp_by_amount(instruction: &Instruction,
                               vm_register_allocation: &VmRegisterAllocation,
                               amount: u32)
                               -> bool {
    if instruction.code() != Code::Add_rm64_imm32 {
        return false;
    }

    if instruction.immediate32() != amount {
        return false;
    }

    if instruction.op0_kind() != OpKind::Register {
        return false;
    }

    if instruction.op0_register().full_register() != vm_register_allocation.vsp.into() {
        return false;
    }

    true
}

pub fn match_sub_vsp_by_amount(instruction: &Instruction,
                               vm_register_allocation: &VmRegisterAllocation,
                               amount: u32)
                               -> bool {
    if instruction.code() != Code::Sub_rm64_imm32 {
        return false;
    }

    if instruction.immediate32() != amount {
        return false;
    }

    if instruction.op0_kind() != OpKind::Register {
        return false;
    }

    if instruction.op0_register().full_register() != vm_register_allocation.vsp.into() {
        return false;
    }

    true
}

pub fn match_sub_reg_by_amount(instruction: &Instruction,
                               reg: Register,
                               amount: u32)
                               -> bool {
    if instruction.code() != Code::Sub_rm64_imm32 {
        return false;
    }

    if instruction.immediate32() != amount {
        return false;
    }

    if instruction.op0_kind() != OpKind::Register {
        return false;
    }

    if instruction.op0_register().full_register() != reg.full_register() {
        return false;
    }

    true
}

pub fn match_add_reg_by_amount(instruction: &Instruction,
                               reg: Register,
                               amount: u32)
                               -> bool {
    if instruction.code() != Code::Add_rm64_imm32 {
        return false;
    }

    if instruction.immediate32() != amount {
        return false;
    }

    if instruction.op0_kind() != OpKind::Register {
        return false;
    }

    if instruction.op0_register().full_register() != reg.full_register() {
        return false;
    }

    true
}

pub fn match_sub_vsp_get_amount(instruction: &Instruction,
                                vm_register_allocation: &VmRegisterAllocation)
                                -> Option<u32> {
    if instruction.code() != Code::Sub_rm64_imm32 {
        return None;
    }

    if instruction.op0_kind() != OpKind::Register {
        return None;
    }

    if instruction.op0_register().full_register() != vm_register_allocation.vsp.into() {
        return None;
    }

    Some(instruction.immediate32())
}

pub fn match_add_vsp_get_amount(instruction: &Instruction,
                                vm_register_allocation: &VmRegisterAllocation)
                                -> Option<u32> {
    if instruction.code() != Code::Add_rm64_imm32 {
        return None;
    }

    if instruction.op0_kind() != OpKind::Register {
        return None;
    }

    if instruction.op0_register().full_register() != vm_register_allocation.vsp.into() {
        return None;
    }

    Some(instruction.immediate32())
}

pub fn match_xor_64_rolling_key_source(instruction: &Instruction,
                                       vm_register_allocation: &VmRegisterAllocation)
                                       -> bool {
    if instruction.code() != Code::Xor_r64_rm64 {
        return false;
    }

    if instruction.op0_kind() != OpKind::Register {
        return false;
    }

    if instruction.op1_kind() != OpKind::Register {
        return false;
    }

    if instruction.op1_register().full_register() != vm_register_allocation.key.into() {
        return false;
    }

    true
}

pub fn match_xor_64_rolling_key_dest(instruction: &Instruction,
                                     vm_register_allocation: &VmRegisterAllocation)
                                     -> bool {
    if instruction.code() != Code::Xor_r64_rm64 {
        return false;
    }

    if instruction.op0_kind() != OpKind::Register {
        return false;
    }

    if instruction.op1_kind() != OpKind::Register {
        return false;
    }

    if instruction.op0_register().full_register() != vm_register_allocation.key.into() {
        return false;
    }

    true
}

pub fn match_xor_16_rolling_key_source(instruction: &Instruction,
                                       vm_register_allocation: &VmRegisterAllocation)
                                       -> bool {
    if instruction.code() != Code::Xor_r16_rm16 {
        return false;
    }

    if instruction.op0_kind() != OpKind::Register {
        return false;
    }

    if instruction.op1_kind() != OpKind::Register {
        return false;
    }

    if instruction.op1_register().full_register() != vm_register_allocation.key.into() {
        return false;
    }

    true
}

pub fn match_xor_16_rolling_key_dest(instruction: &Instruction,
                                     vm_register_allocation: &VmRegisterAllocation)
                                     -> bool {
    if instruction.code() != Code::Xor_r16_rm16 {
        return false;
    }

    if instruction.op0_kind() != OpKind::Register {
        return false;
    }

    if instruction.op1_kind() != OpKind::Register {
        return false;
    }

    if instruction.op0_register().full_register() != vm_register_allocation.key.into() {
        return false;
    }

    true
}

pub fn match_xor_32_rolling_key_source(instruction: &Instruction,
                                       vm_register_allocation: &VmRegisterAllocation)
                                       -> bool {
    if instruction.code() != Code::Xor_r32_rm32 {
        return false;
    }

    if instruction.op0_kind() != OpKind::Register {
        return false;
    }

    if instruction.op1_kind() != OpKind::Register {
        return false;
    }

    if instruction.op1_register().full_register() != vm_register_allocation.key.into() {
        return false;
    }

    true
}
pub fn match_xor_8_rolling_key_source(instruction: &Instruction,
                                      vm_register_allocation: &VmRegisterAllocation)
                                      -> bool {
    if instruction.code() != Code::Xor_r8_rm8 {
        return false;
    }

    if instruction.op0_kind() != OpKind::Register {
        return false;
    }

    if instruction.op1_kind() != OpKind::Register {
        return false;
    }

    if instruction.op1_register().full_register() != vm_register_allocation.key.into() {
        return false;
    }

    true
}

pub fn match_xor_8_rolling_key_dest(instruction: &Instruction,
                                    vm_register_allocation: &VmRegisterAllocation)
                                    -> bool {
    if instruction.code() != Code::Xor_r8_rm8 {
        return false;
    }

    if instruction.op0_kind() != OpKind::Register {
        return false;
    }

    if instruction.op1_kind() != OpKind::Register {
        return false;
    }

    if instruction.op0_register().full_register() != vm_register_allocation.key.into() {
        return false;
    }

    true
}

```

`src/symbolic.rs`:

```rs
use std::error::Error;

use haybale::{
    config::NullPointerChecking, get_possible_return_values_of_func, Config, Project, ReturnValue,
};

pub fn get_possible_solutions(function_name: &str) -> Result<Vec<u64>, Box<dyn Error>> {
    let project = Project::from_bc_path("devirt.bc")?;

    // println!("Pointer size = {}", project.pointer_size_bits());

    let mut config: Config<haybale::backend::DefaultBackend> = Default::default();
    config.loop_bound = 1000;
    config.null_pointer_checking = NullPointerChecking::None;

    config.function_hooks.add("llvm.ctpop.i8", &|state, call| {
                             let _arguments = call.get_arguments();
                             Ok(ReturnValue::Return(state.new_bv_with_name("ctpop".into(),
                                                                           8)
                                                         .unwrap()))
                         });
    config.function_hooks.add("llvm.fshl.i64", &|state, call| {
                             let _arguments = call.get_arguments();
                             Ok(ReturnValue::Return(state.new_bv_with_name("fshl".into(),
                                                                           64)
                                                         .unwrap()))
                         });
    config.function_hooks.add("llvm.fshl.i32", &|state, call| {
                             let _arguments = call.get_arguments();
                             Ok(ReturnValue::Return(state.new_bv_with_name("fshl".into(),
                                                                           32)
                                                         .unwrap()))
                         });
    config.function_hooks.add("llvm.x86.rdtsc", &|state, call| {
                             let _arguments = call.get_arguments();
                             Ok(ReturnValue::Return(state.new_bv_with_name("rdtsc".into(),
                                                                           64)
                                                         .unwrap()))
                         });
    config.function_hooks.add_inline_asm_hook(&|state, call| {
                             let _arguments = call.get_arguments();
                             Ok(ReturnValue::Return(state.new_bv_with_name("cr0".into(),
                                                                           64)
                                                         .unwrap()))
                         });
    config.function_hooks
          .add_default_hook(&|_, _| Ok(ReturnValue::ReturnVoid));

    let possible_solutions =
        get_possible_return_values_of_func(function_name, &project, config, None, None, 10);

    let mut solutions_return = Vec::new();
    match possible_solutions {
        haybale::solver_utils::PossibleSolutions::Exactly(solutions) => {
            for solution in solutions {
                match solution {
                    ReturnValue::Return(value) => {
                        // println!("Branch target -> {:x}", value);
                        solutions_return.push(value);
                    },
                    ReturnValue::ReturnVoid => panic!(),
                    ReturnValue::Throw(_) => panic!(),
                    ReturnValue::Abort => panic!(),
                };
            }
        },
        haybale::solver_utils::PossibleSolutions::AtLeast(_) => {
            panic!("More than 10 options for next vip!")
        },
    }
    Ok(solutions_return)
}

```

`src/transforms.rs`:

```rs
use iced_x86::{Code, Instruction, Register};

use crate::util::check_full_reg_written;

pub fn get_transform_for_instruction(instruction: &Instruction) -> Option<Transform> {
    // Add the transform that represents this instruction to the transforms vec
    match instruction.code() {
        Code::Bswap_r16 => Some(Transform::ByteSwap16),
        Code::Bswap_r32 => Some(Transform::ByteSwap32),
        Code::Bswap_r64 => Some(Transform::ByteSwap64),

        Code::Sub_AL_imm8 => Some(Transform::SubtractConstant8(instruction.immediate8())),
        Code::Sub_rm8_imm8 => Some(Transform::SubtractConstant8(instruction.immediate8())),
        Code::Sub_AX_imm16 => Some(Transform::SubtractConstant16(instruction.immediate16())),
        Code::Sub_rm16_imm16 => Some(Transform::SubtractConstant16(instruction.immediate16())),
        Code::Sub_EAX_imm32 => Some(Transform::SubtractConstant32(instruction.immediate32())),
        Code::Sub_rm32_imm32 => Some(Transform::SubtractConstant32(instruction.immediate32())),
        Code::Sub_RAX_imm32 => Some(Transform::SubtractConstant64(instruction.immediate64())),
        Code::Sub_rm64_imm32 => Some(Transform::SubtractConstant64(instruction.immediate64())),

        Code::Add_AL_imm8 => Some(Transform::AddConstant8(instruction.immediate8())),
        Code::Add_rm8_imm8 => Some(Transform::AddConstant8(instruction.immediate8())),
        Code::Add_AX_imm16 => Some(Transform::AddConstant16(instruction.immediate16())),
        Code::Add_rm16_imm16 => Some(Transform::AddConstant16(instruction.immediate16())),
        Code::Add_EAX_imm32 => Some(Transform::AddConstant32(instruction.immediate32())),
        Code::Add_rm32_imm32 => Some(Transform::AddConstant32(instruction.immediate32())),
        Code::Add_RAX_imm32 => Some(Transform::AddConstant64(instruction.immediate64())),
        Code::Add_rm64_imm32 => Some(Transform::AddConstant64(instruction.immediate64())),

        Code::Neg_rm8 => Some(Transform::Negate8),
        Code::Neg_rm16 => Some(Transform::Negate16),
        Code::Neg_rm32 => Some(Transform::Negate32),
        Code::Neg_rm64 => Some(Transform::Negate64),

        Code::Not_rm8 => Some(Transform::Not8),
        Code::Not_rm16 => Some(Transform::Not16),
        Code::Not_rm32 => Some(Transform::Not32),
        Code::Not_rm64 => Some(Transform::Not64),

        Code::Rol_rm8_imm8 => Some(Transform::RotateLeft8(instruction.immediate8() as u32)),
        Code::Rol_rm16_imm8 => Some(Transform::RotateLeft16(instruction.immediate8() as u32)),
        Code::Rol_rm32_imm8 => Some(Transform::RotateLeft32(instruction.immediate8() as u32)),
        Code::Rol_rm64_imm8 => Some(Transform::RotateLeft64(instruction.immediate8() as u32)),

        Code::Ror_rm8_imm8 => Some(Transform::RotateRight8(instruction.immediate8() as u32)),
        Code::Ror_rm16_imm8 => Some(Transform::RotateRight16(instruction.immediate8() as u32)),
        Code::Ror_rm32_imm8 => Some(Transform::RotateRight32(instruction.immediate8() as u32)),
        Code::Ror_rm64_imm8 => Some(Transform::RotateRight64(instruction.immediate8() as u32)),

        Code::Rol_rm8_1 => Some(Transform::RotateLeft8(1u32)),
        Code::Rol_rm16_1 => Some(Transform::RotateLeft16(1u32)),
        Code::Rol_rm32_1 => Some(Transform::RotateLeft32(1u32)),
        Code::Rol_rm64_1 => Some(Transform::RotateLeft64(1u32)),

        Code::Ror_rm8_1 => Some(Transform::RotateRight8(1u32)),
        Code::Ror_rm16_1 => Some(Transform::RotateRight16(1u32)),
        Code::Ror_rm32_1 => Some(Transform::RotateRight32(1u32)),
        Code::Ror_rm64_1 => Some(Transform::RotateRight64(1u32)),

        Code::Inc_rm8 => Some(Transform::Increment8),
        Code::Inc_rm16 => Some(Transform::Increment16),
        Code::Inc_rm32 => Some(Transform::Increment32),
        Code::Inc_rm64 => Some(Transform::Increment64),

        Code::Dec_rm8 => Some(Transform::Decrement8),
        Code::Dec_rm16 => Some(Transform::Decrement16),
        Code::Dec_rm32 => Some(Transform::Decrement32),
        Code::Dec_rm64 => Some(Transform::Decrement64),

        Code::Xor_AL_imm8 => Some(Transform::XorConstant8(instruction.immediate8())),
        Code::Xor_rm8_imm8 => Some(Transform::XorConstant8(instruction.immediate8())),
        Code::Xor_AX_imm16 => Some(Transform::XorConstant16(instruction.immediate16())),
        Code::Xor_rm16_imm16 => Some(Transform::XorConstant16(instruction.immediate16())),
        Code::Xor_EAX_imm32 => Some(Transform::XorConstant32(instruction.immediate32())),
        Code::Xor_rm32_imm32 => Some(Transform::XorConstant32(instruction.immediate32())),
        Code::Xor_RAX_imm32 => Some(Transform::XorConstant64(instruction.immediate64())),
        Code::Xor_rm64_imm32 => Some(Transform::XorConstant64(instruction.immediate64())),
        _ => None,
    }
}
#[derive(Debug, Clone, Copy)]
pub enum Transform {
    ByteSwap64,
    ByteSwap32,
    ByteSwap16,

    SubtractConstant64(u64),
    SubtractConstant32(u32),
    SubtractConstant16(u16),
    SubtractConstant8(u8),

    AddConstant64(u64),
    AddConstant32(u32),
    AddConstant16(u16),
    AddConstant8(u8),

    Negate64,
    Negate32,
    Negate16,
    Negate8,

    Not64,
    Not32,
    Not16,
    Not8,

    RotateLeft64(u32),
    RotateLeft32(u32),
    RotateLeft16(u32),
    RotateLeft8(u32),

    RotateRight64(u32),
    RotateRight32(u32),
    RotateRight16(u32),
    RotateRight8(u32),

    Increment64,
    Increment32,
    Increment16,
    Increment8,

    Decrement64,
    Decrement32,
    Decrement16,
    Decrement8,

    XorConstant64(u64),
    XorConstant32(u32),
    XorConstant16(u16),
    XorConstant8(u8),
}

pub trait EmulateTransform {
    fn emulate_transform(self,
                         transform: Transform)
                         -> Self;
}

impl EmulateTransform for u8 {
    fn emulate_transform(self,
                         transform: Transform)
                         -> Self {
        emulate_transform8(transform, self)
    }
}

impl EmulateTransform for u16 {
    fn emulate_transform(self,
                         transform: Transform)
                         -> Self {
        emulate_transform16(transform, self)
    }
}

impl EmulateTransform for u32 {
    fn emulate_transform(self,
                         transform: Transform)
                         -> Self {
        emulate_transform32(transform, self)
    }
}

impl EmulateTransform for u64 {
    fn emulate_transform(self,
                         transform: Transform)
                         -> Self {
        emulate_transform64(transform, self)
    }
}

pub trait EmulateEncryption {
    fn emulate_encryption<'a, I>(self,
                                 instruction_iter: I,
                                 rolling_key: &mut u64,
                                 encrypted_reg: Register)
                                 -> Self
        where I: Iterator<Item = &'a Instruction>;
}

impl EmulateEncryption for u64 {
    fn emulate_encryption<'a, I>(mut self,
                                 instruction_iter: I,
                                 rolling_key: &mut u64,
                                 encrypted_reg: Register)
                                 -> Self
        where I: Iterator<Item = &'a Instruction>
    {
        self ^= *rolling_key as u64;

        for instruction in
            instruction_iter.filter(|&insn| check_full_reg_written(insn, encrypted_reg))
        {
            let transform = get_transform_for_instruction(instruction);

            if let Some(transform) = transform {
                self = self.emulate_transform(transform);
            }
        }

        *rolling_key ^= self as u64;

        self
    }
}

impl EmulateEncryption for u32 {
    fn emulate_encryption<'a, I>(mut self,
                                 instruction_iter: I,
                                 rolling_key: &mut u64,
                                 encrypted_reg: Register)
                                 -> Self
        where I: Iterator<Item = &'a Instruction>
    {
        self ^= *rolling_key as u32;

        for instruction in
            instruction_iter.filter(|&insn| check_full_reg_written(insn, encrypted_reg))
        {
            let transform = get_transform_for_instruction(instruction);

            if let Some(transform) = transform {
                self = self.emulate_transform(transform);
            }
        }

        *rolling_key ^= self as u64;

        self
    }
}

impl EmulateEncryption for u16 {
    fn emulate_encryption<'a, I>(mut self,
                                 instruction_iter: I,
                                 rolling_key: &mut u64,
                                 encrypted_reg: Register)
                                 -> Self
        where I: Iterator<Item = &'a Instruction>
    {
        self ^= *rolling_key as u16;

        for instruction in
            instruction_iter.filter(|&insn| check_full_reg_written(insn, encrypted_reg))
        {
            let transform = get_transform_for_instruction(instruction);

            if let Some(transform) = transform {
                self = self.emulate_transform(transform);
            }
        }

        *rolling_key ^= self as u64;

        self
    }
}

impl EmulateEncryption for u8 {
    fn emulate_encryption<'a, I>(mut self,
                                 instruction_iter: I,
                                 rolling_key: &mut u64,
                                 encrypted_reg: Register)
                                 -> Self
        where I: Iterator<Item = &'a Instruction>
    {
        self ^= *rolling_key as u8;

        for instruction in
            instruction_iter.filter(|&insn| check_full_reg_written(insn, encrypted_reg))
        {
            let transform = get_transform_for_instruction(instruction);
            if let Some(transform) = transform {
                self = self.emulate_transform(transform);
            }
        }

        *rolling_key ^= self as u64;

        self
    }
}

fn emulate_transform64(transform: Transform,
                       input: u64)
                       -> u64 {
    match transform {
        Transform::ByteSwap64 => input.swap_bytes(),

        Transform::SubtractConstant64(amount) => input.wrapping_sub(amount),

        Transform::AddConstant64(amount) => input.wrapping_add(amount),

        Transform::XorConstant64(amount) => input ^ amount,

        Transform::Negate64 => (!input).wrapping_add(1),

        Transform::Not64 => !input,

        Transform::RotateLeft64(amount) => input.rotate_left(amount),

        Transform::RotateRight64(amount) => input.rotate_right(amount),

        Transform::Decrement64 => input.wrapping_sub(1),

        Transform::Increment64 => input.wrapping_add(1),
        _ => {
            dbg!(transform);
            unreachable!();
        },
    }
}

fn emulate_transform32(transform: Transform,
                       input: u32)
                       -> u32 {
    match transform {
        Transform::ByteSwap32 => input.swap_bytes(),

        Transform::SubtractConstant32(amount) => input.wrapping_sub(amount),

        Transform::AddConstant32(amount) => input.wrapping_add(amount),

        Transform::XorConstant32(amount) => input ^ amount,

        Transform::Negate32 => (!input).wrapping_add(1),

        Transform::Not32 => !input,

        Transform::RotateLeft32(amount) => input.rotate_left(amount),

        Transform::RotateRight32(amount) => input.rotate_right(amount),

        Transform::Decrement32 => input.wrapping_sub(1),

        Transform::Increment32 => input.wrapping_add(1),
        _ => {
            dbg!(transform);
            unreachable!();
        },
    }
}

fn emulate_transform16(transform: Transform,
                       input: u16)
                       -> u16 {
    match transform {
        Transform::ByteSwap16 => input.swap_bytes(),

        Transform::SubtractConstant16(amount) => input.wrapping_sub(amount),

        Transform::AddConstant16(amount) => input.wrapping_add(amount),

        Transform::XorConstant16(amount) => input ^ amount,

        Transform::Negate16 => (!input).wrapping_add(1),

        Transform::Not16 => !input,

        Transform::RotateLeft16(amount) => input.rotate_left(amount),

        Transform::RotateRight16(amount) => input.rotate_right(amount),

        Transform::Decrement16 => input.wrapping_sub(1),

        Transform::Increment16 => input.wrapping_add(1),
        _ => {
            dbg!(transform);
            unreachable!();
        },
    }
}

fn emulate_transform8(transform: Transform,
                      input: u8)
                      -> u8 {
    match transform {
        Transform::SubtractConstant8(amount) => input.wrapping_sub(amount),

        Transform::AddConstant8(amount) => input.wrapping_add(amount),

        Transform::XorConstant8(amount) => input ^ amount,

        Transform::Negate8 => (!input).wrapping_add(1),

        Transform::Not8 => !input,

        Transform::RotateLeft8(amount) => input.rotate_left(amount),

        Transform::RotateRight8(amount) => input.rotate_right(amount),

        Transform::Decrement8 => input.wrapping_sub(1),

        Transform::Increment8 => input.wrapping_add(1),
        _ => {
            dbg!(transform);
            unreachable!();
        },
    }
}

```

`src/util.rs`:

```rs
use iced_x86::{
    Code, Decoder, DecoderOptions, Instruction, InstructionInfoFactory, InstructionInfoOptions,
    OpAccess, Register,
};
use pelite::pe64::{Pe, PeFile};

/// Read bytes at virtual address
pub fn read_bytes_at_va<'a>(pe_file: &'_ PeFile,
                            pe_bytes: &'a [u8],
                            va: u64,
                            size: usize)
                            -> Result<&'a [u8], pelite::Error> {
    // Relative virtual address
    let rva = pe_file.va_to_rva(va)?;
    let file_offset = pe_file.rva_to_file_offset(rva)?;

    let bytes = &pe_bytes[file_offset .. file_offset + size];
    Ok(bytes)
}

/// Disassemble instruction at virtual address
pub fn disassemble_instruction_at_va(pe_file: &PeFile,
                                     pe_bytes: &[u8],
                                     instruction_address: u64)
                                     -> Instruction {
    let instruction_bytes = read_bytes_at_va(pe_file, pe_bytes, instruction_address, 16).unwrap();

    // Decode the instruction
    let mut decoder = Decoder::with_ip(64,
                                       instruction_bytes,
                                       instruction_address,
                                       DecoderOptions::NONE);

    decoder.decode()
}

/// Handle the calls into vm stub that looks like:
/// push_call_addr ->  push imm
///                    call vm_entry
pub fn handle_vm_call(pe_file: &PeFile,
                      pe_bytes: &[u8],
                      push_call_addr: u64)
                      -> (u64, u64) {
    let push_instruction = disassemble_instruction_at_va(pe_file, pe_bytes, push_call_addr);
    let call_instruction = disassemble_instruction_at_va(pe_file,
                                                         pe_bytes,
                                                         push_call_addr +
                                                         push_instruction.len() as u64);

    // Check if the instructions match the expected opcodes
    if push_instruction.code() != Code::Pushq_imm32 {
        panic!("Vm Entry address is not correctly chosen");
    }

    if call_instruction.code() != Code::Call_rel32_64 {
        panic!("Vm Entry address is not correctly chosen");
    }

    let pushed_val = push_instruction.immediate32to64() as u64;
    let vm_entry_address = call_instruction.near_branch64();

    (pushed_val, vm_entry_address)
}

/// Check wether a given register is written by the given instruction
pub fn check_full_reg_written(instruction: &Instruction,
                              reg: Register)
                              -> bool {
    // Create an instruction factory to get more information about the instruction
    let mut instruction_info_factory = InstructionInfoFactory::new();

    // Get the instruction info from the factory
    let instruction_info =
        instruction_info_factory.info_options(instruction, InstructionInfoOptions::NO_MEMORY_USAGE);

    let used_registers = instruction_info.used_registers();

    for used_register in
        used_registers.iter()
                      .filter(|r| r.register().full_register() == reg.full_register())
    {
        // Check if any type of write happens
        match used_register.access() {
            OpAccess::Write |
            OpAccess::CondWrite |
            OpAccess::ReadWrite |
            OpAccess::ReadCondWrite => {
                return true;
            },
            _ => {
                continue;
            },
        }
    }

    false
}

```

`src/vm_handler.rs`:

```rs
use crate::{
    match_assembly::*,
    transforms::{get_transform_for_instruction, EmulateEncryption, EmulateTransform},
    util::*,
    vm_matchers::{self, HandlerClass, HandlerVmInstruction},
};
use iced_x86::{Code, Instruction, OpKind, Register};
use pelite::pe64::PeFile;

#[derive(Debug, Clone)]
pub struct VmRegisterAllocation {
    pub vip: Registers,
    pub vsp: Registers,
    pub key: Registers,
    pub handler_address: Registers,
}

/// Describes a vm context that is it holds the internal state and the register
/// allocation of the vm, the register allocation changes between jumps, so a
/// new VmContext needs to be made when a jump occurs
#[derive(Debug, Clone)]
pub struct VmContext {
    /// Register allocation of the vm
    pub register_allocation: VmRegisterAllocation,
    /// VmEntry address
    pub vm_entry_address: u64,
    /// Pushed value
    pub pushed_val: u64,
    /// Vip direction
    pub vip_direction_forwards: bool,
    /// Register push order
    pub push_order: Vec<Registers>,
    /// Key value
    pub rolling_key: u64,
    /// Current vip
    pub vip_value: u64,
    /// Initial vip
    pub initial_vip: u64,
    /// Next handler address
    pub handler_address: u64,
    /// Reloc value
    pub reloc_value: u64,
    /// Address of call into the vm
    pub vm_call_address: u64,
}

impl VmContext {
    /// Create a new VmContext from a vm_entry handler
    pub fn new_from_vm_entry(pe_file: &PeFile,
                             pe_bytes: &[u8],
                             vm_call_address: u64)
                             -> Self {
        // Get the pushed value and the vm entry handler address
        let (pushed_val, vm_entry_address) = handle_vm_call(pe_file, pe_bytes, vm_call_address);

        // Collect the instructions of the vm_entry into a VmHandler
        let vm_entry_handler = VmHandler::new(vm_entry_address, pe_file, pe_bytes);

        // Get the native register push order
        let push_order = vm_entry_handler.get_push_order_vm_entry();

        let register_allocation = vm_entry_handler.get_register_allocation_vm_entry();

        let direction_is_forwards = vm_entry_handler.determine_is_forwards(&register_allocation);

        // Get the initial_vip
        let initial_vip =
            vm_entry_handler.get_initial_vip(&register_allocation, pushed_val) + 0x100000000;

        let mut vip = initial_vip;

        // Rolling key is initialized to the initial vip
        let mut rolling_key = initial_vip;

        // Get the handler base address value
        let instruction_iter = vm_entry_handler.instructions.iter();
        let mut instruction_iter = instruction_iter.skip_while(|insn| {
                                                       !(insn.code() == Code::Lea_r64_m &&
                                                         insn.memory_displacement64() != 0 &&
                                                         // This is to fix devirtualizeme
                                                         insn.memory_displacement64() !=
                                                         0x100000000)
                                                   });
        let handler_base_address = instruction_iter.next().unwrap().memory_displacement64();

        let mut instruction_iter =
            instruction_iter.skip_while(|insn| !match_fetch_vip(insn, &register_allocation));

        // Get the reg where the encrypted offset has been loaded into
        let encrypted_offset_reg = instruction_iter.next().unwrap().op0_register();

        let encrypted_offset = fetch_dword_vip(pe_file, pe_bytes, &mut vip, direction_is_forwards);

        let encryption_iter = instruction_iter.take_while(|&insn| {
                                                  !(match_push_rolling_key(insn,
                                                                           &register_allocation))
                                              });

        let unencrypted_offset = encrypted_offset.emulate_encryption(encryption_iter,
                                                                     &mut rolling_key,
                                                                     encrypted_offset_reg);

        // hmmm yes
        // movsxd offset_reg, offset_reg_32
        // add handler_base, offset_reg
        let next_handler_address =
            handler_base_address.wrapping_add(unencrypted_offset as i32 as i64 as u64);

        let vip_value = vip;

        //TODO support different relocs
        let reloc_value = 0;
        Self { register_allocation,
               vm_entry_address,
               pushed_val,
               vip_direction_forwards: direction_is_forwards,
               push_order,
               rolling_key,
               vip_value,
               initial_vip,
               handler_address: next_handler_address,
               reloc_value,
               vm_call_address }
    }

    pub fn new_from_jump_handler(&self,
                                 last_handler: &(u64, HandlerVmInstruction, u64),
                                 jump_target_vip: u64,
                                 pe_file: &PeFile,
                                 pe_bytes: &[u8])
                                 -> Self {
        let jump_handler_address = last_handler.2;
        let (direction_is_forwards, changed_vsp) = match last_handler.1 {
            HandlerVmInstruction::JumpDecVspXchng => (false, true),
            HandlerVmInstruction::JumpIncVspXchng => (true, true),
            HandlerVmInstruction::JumpDecVspChange => (false, true),
            HandlerVmInstruction::JumpIncVspChange => (true, true),
            HandlerVmInstruction::JumpDec => (false, false),
            HandlerVmInstruction::JumpInc => (true, false),
            HandlerVmInstruction::Nop => (self.vip_direction_forwards, false),
            _ => panic!("Not a branch instruction"),
        };

        let mut new_vm_context = self.clone();

        let vm_jump_handler = VmHandler::new(jump_handler_address, pe_file, pe_bytes);

        match last_handler.1 {
            HandlerVmInstruction::JumpDecVspXchng => {
                vm_jump_handler.get_register_allocation_vm_jump_xchng(
                                                        &mut new_vm_context.register_allocation);
            },
            HandlerVmInstruction::JumpIncVspXchng => {
                vm_jump_handler.get_register_allocation_vm_jump_xchng(
                                                        &mut new_vm_context.register_allocation);
            },
            HandlerVmInstruction::JumpDecVspChange => {
                vm_jump_handler.get_register_allocation_vm_jump(changed_vsp,
                                                        &mut new_vm_context.register_allocation);
            },
            HandlerVmInstruction::JumpIncVspChange => {
                vm_jump_handler.get_register_allocation_vm_jump(changed_vsp,
                                                        &mut new_vm_context.register_allocation);
            },
            HandlerVmInstruction::JumpDec => {
                vm_jump_handler.get_register_allocation_vm_jump(changed_vsp,
                                                        &mut new_vm_context.register_allocation);
            },
            HandlerVmInstruction::JumpInc => {
                vm_jump_handler.get_register_allocation_vm_jump(changed_vsp,
                                                        &mut new_vm_context.register_allocation);
            },
            HandlerVmInstruction::Nop => {},
            _ => panic!("Not a branch instruction"),
        };
        println!("{:#x?}", new_vm_context);
        // Rolling key is initialized to the initial vip
        let mut vip = if direction_is_forwards {
            jump_target_vip - 4
        } else {
            jump_target_vip + 4
        };

        let mut rolling_key = vip;

        let instruction_iter = vm_jump_handler.instructions.iter();
        let mut instruction_iter = instruction_iter.skip_while(|insn| {
                                                       !(insn.code() == Code::Lea_r64_m &&
                                                         insn.memory_displacement64() != 0 &&
                                                         insn.memory_displacement64() !=
                                                         0x100000000)
                                                   });

        let handler_base_address = instruction_iter.next().unwrap().memory_displacement64();
        let mut instruction_iter = instruction_iter.skip_while(|insn| {
                                       !match_fetch_vip(insn, &new_vm_context.register_allocation)
                                   });

        // Get the reg where the encrypted offset has been loaded into
        let encrypted_offset_reg = instruction_iter.next().unwrap().op0_register();

        let encrypted_offset = fetch_dword_vip(pe_file, pe_bytes, &mut vip, direction_is_forwards);

        let encryption_iter = instruction_iter.take_while(|&insn| {
                                                  !(match_push_rolling_key(insn,
                                                           &new_vm_context.register_allocation))
                                              });

        let unencrypted_offset = encrypted_offset.emulate_encryption(encryption_iter,
                                                                     &mut rolling_key,
                                                                     encrypted_offset_reg);

        // hmmm yes
        // movsxd offset_reg, offset_reg_32
        // add handler_base, offset_reg
        let next_handler_address =
            handler_base_address.wrapping_add(unencrypted_offset as i32 as i64 as u64);

        let vip_value = vip;

        new_vm_context.vip_direction_forwards = direction_is_forwards;
        new_vm_context.rolling_key = rolling_key;
        new_vm_context.initial_vip = jump_target_vip;
        new_vm_context.vip_value = vip_value;
        new_vm_context.handler_address = next_handler_address;
        new_vm_context
    }

    pub fn disassemble_context(&mut self,
                               pe_file: &PeFile,
                               pe_bytes: &[u8])
                               -> Vec<(u64, HandlerVmInstruction, u64)> {
        let mut handlers = Vec::new();

        println!("{:<15} | {:<30} | Handler address", "VIP", "Disassembly");
        loop {
            let mut halt = false;
            let vm_handler = VmHandler::new(self.handler_address, pe_file, pe_bytes);

            let handler_class = vm_handler.match_handler_class(&self.register_allocation);
            let handler_address = self.handler_address;

            let handler_instruction;

            let vip = self.vip_value;

            match handler_class {
                HandlerClass::UnconditionalBranch => {
                    handler_instruction = vm_handler.match_branch_instructions(self);

                    halt = true;
                },
                HandlerClass::NoVipChange => {
                    handler_instruction = vm_handler.match_no_vip_change_instructions(self);
                    halt = true;
                },
                HandlerClass::ByteOperand => {
                    //println!("Disassembled single byte operand");
                    let byte_operand =
                        self.disassemble_single_byte_operand(&vm_handler, pe_file, pe_bytes);
                    handler_instruction =
                        vm_handler.match_byte_operand_instructions(&self.register_allocation,
                                                                   byte_operand);
                },
                HandlerClass::WordOperand => {
                    //println!("Disassembled single word operand");
                    let word_operand =
                        self.disassemble_single_word_operand(&vm_handler, pe_file, pe_bytes);
                    handler_instruction =
                        vm_handler.match_word_operand_instructions(&self.register_allocation,
                                                                   word_operand);
                },
                HandlerClass::DwordOperand => {
                    //println!("Disassembled single dword operand");
                    let dword_operand =
                        self.disassemble_single_dword_operand(&vm_handler, pe_file, pe_bytes);
                    handler_instruction =
                        vm_handler.match_dword_operand_instructions(&self.register_allocation,
                                                                    dword_operand);
                },
                HandlerClass::QwordOperand => {
                    //println!("Disassembled single qword operand");
                    let qword_operand =
                        self.disassemble_single_qword_operand(&vm_handler, pe_file, pe_bytes);
                    handler_instruction =
                        vm_handler.match_qword_operand_instructions(&self.register_allocation,
                                                                    qword_operand);
                },
                HandlerClass::NoOperand => {
                    //println!("Disassembled no operand");
                    self.disassemble_no_operand(&vm_handler, pe_file, pe_bytes);
                    handler_instruction =
                        vm_handler.match_no_operand_instructions(self, &self.register_allocation);
                    match handler_instruction {
                        HandlerVmInstruction::JumpDecVspXchng |
                        HandlerVmInstruction::JumpIncVspXchng |
                        HandlerVmInstruction::JumpDecVspChange |
                        HandlerVmInstruction::JumpIncVspChange |
                        HandlerVmInstruction::JumpDec |
                        HandlerVmInstruction::JumpInc |
                        HandlerVmInstruction::Nop => {
                            halt = true;
                        },

                        _ => {},
                    }
                },
            }
            handlers.push((vip, handler_instruction, handler_address));
            // This shit don't work if I combine them so fuck it
            println!("{:<15} | {:<30} | {:#x}",
                     format!("{:#x}", vip),
                     format!("{}", handler_instruction),
                     handler_address);

            if halt {
                break;
            }
        }

        handlers
    }
    pub fn disassemble_single_dword_operand(&mut self,
                                            vm_handler: &VmHandler,
                                            pe_file: &PeFile,
                                            pe_bytes: &[u8])
                                            -> u32 {
        let instruction_iter = vm_handler.instructions.iter();
        let mut instruction_iter = instruction_iter.skip_while(|insn| {
                                                       !match_xor_32_rolling_key_source(insn,
                                                                       &self.register_allocation)
                                                   });
        let encrypted_reg = instruction_iter.next().unwrap().op0_register();

        let encryption_iter = instruction_iter.take_while(|insn| {
                                                  !match_push_rolling_key(insn,
                                                                          &self.register_allocation)
                                              });

        let encrypted_dword = fetch_dword_vip(pe_file,
                                              pe_bytes,
                                              &mut self.vip_value,
                                              self.vip_direction_forwards);

        let return_dword = encrypted_dword.emulate_encryption(encryption_iter,
                                                              &mut self.rolling_key,
                                                              encrypted_reg);

        let encrypted_offset = fetch_dword_vip(pe_file,
                                               pe_bytes,
                                               &mut self.vip_value,
                                               self.vip_direction_forwards);

        let instruction_iter = vm_handler.instructions.iter();
        // Skip it once because dword arg
        let mut match_count = 0;
        let instruction_iter =
            instruction_iter.skip_while(|insn| {
                                if match_fetch_vip(insn, &self.register_allocation) {
                                    match_count += 1;
                                }

                                match_count != 2
                            });

        let mut instruction_iter = instruction_iter.skip_while(|insn| {
                                                       !match_xor_32_rolling_key_source(insn,
                                                                       &self.register_allocation)
                                                   });

        let encrypted_reg = instruction_iter.next().unwrap().op0_register();
        let encryption_iter = instruction_iter.take_while(|insn| {
                                                  !match_push_rolling_key(insn,
                                                                          &self.register_allocation)
                                              });

        let unencrypted_offset = encrypted_offset.emulate_encryption(encryption_iter,
                                                                     &mut self.rolling_key,
                                                                     encrypted_reg);

        // hmmm yes
        // movsxd offset_reg, offset_reg_32
        // add handler_base, offset_reg
        let next_handler_address = self.handler_address
                                       .wrapping_add(unencrypted_offset as i32 as i64 as u64);

        self.handler_address = next_handler_address;

        return_dword
    }

    pub fn disassemble_single_qword_operand(&mut self,
                                            vm_handler: &VmHandler,
                                            pe_file: &PeFile,
                                            pe_bytes: &[u8])
                                            -> u64 {
        let instruction_iter = vm_handler.instructions.iter();
        let mut instruction_iter = instruction_iter.skip_while(|insn| {
                                                       !match_xor_64_rolling_key_source(insn,
                                                                       &self.register_allocation)
                                                   });
        let encrypted_reg = instruction_iter.next().unwrap().op0_register();

        let encryption_iter = instruction_iter.take_while(|insn| {
                                  !match_xor_64_rolling_key_dest(insn, &self.register_allocation)
                              });
        let encrypted_qword = fetch_qword_vip(pe_file,
                                              pe_bytes,
                                              &mut self.vip_value,
                                              self.vip_direction_forwards);

        let return_qword = encrypted_qword.emulate_encryption(encryption_iter,
                                                              &mut self.rolling_key,
                                                              encrypted_reg);

        let encrypted_offset = fetch_dword_vip(pe_file,
                                               pe_bytes,
                                               &mut self.vip_value,
                                               self.vip_direction_forwards);

        let instruction_iter = vm_handler.instructions.iter();

        let instruction_iter =
            instruction_iter.skip_while(|insn| !match_fetch_vip(insn, &self.register_allocation));

        let mut instruction_iter = instruction_iter.skip_while(|insn| {
                                                       !match_xor_32_rolling_key_source(insn,
                                                                       &self.register_allocation)
                                                   });
        let encrypted_reg = instruction_iter.next().unwrap().op0_register();
        let encryption_iter = instruction_iter.take_while(|insn| {
                                                  !match_push_rolling_key(insn,
                                                                          &self.register_allocation)
                                              });

        let unencrypted_offset = encrypted_offset.emulate_encryption(encryption_iter,
                                                                     &mut self.rolling_key,
                                                                     encrypted_reg);

        // hmmm yes
        // movsxd offset_reg, offset_reg_32
        // add handler_base, offset_reg
        let next_handler_address = self.handler_address
                                       .wrapping_add(unencrypted_offset as i32 as i64 as u64);

        self.handler_address = next_handler_address;

        return_qword
    }

    pub fn disassemble_single_word_operand(&mut self,
                                           vm_handler: &VmHandler,
                                           pe_file: &PeFile,
                                           pe_bytes: &[u8])
                                           -> u16 {
        let instruction_iter = vm_handler.instructions.iter();
        let mut instruction_iter = instruction_iter.skip_while(|insn| {
                                                       !match_xor_16_rolling_key_source(insn,
                                                                       &self.register_allocation)
                                                   });

        let encrypted_reg = instruction_iter.next().unwrap().op0_register();

        let encryption_iter = instruction_iter.take_while(|insn| {
                                  !match_xor_16_rolling_key_dest(insn, &self.register_allocation)
                              });
        let encrypted_word = fetch_word_vip(pe_file,
                                            pe_bytes,
                                            &mut self.vip_value,
                                            self.vip_direction_forwards);

        let return_word = encrypted_word.emulate_encryption(encryption_iter,
                                                            &mut self.rolling_key,
                                                            encrypted_reg);

        let encrypted_offset = fetch_dword_vip(pe_file,
                                               pe_bytes,
                                               &mut self.vip_value,
                                               self.vip_direction_forwards);

        let instruction_iter = vm_handler.instructions.iter();

        let instruction_iter =
            instruction_iter.skip_while(|insn| !match_fetch_vip(insn, &self.register_allocation));

        let mut instruction_iter = instruction_iter.skip_while(|insn| {
                                                       !match_xor_32_rolling_key_source(insn,
                                                                       &self.register_allocation)
                                                   });
        let encrypted_reg = instruction_iter.next().unwrap().op0_register();
        let encryption_iter = instruction_iter.take_while(|insn| {
                                                  !match_push_rolling_key(insn,
                                                                          &self.register_allocation)
                                              });

        let unencrypted_offset = encrypted_offset.emulate_encryption(encryption_iter,
                                                                     &mut self.rolling_key,
                                                                     encrypted_reg);

        // hmmm yes
        // movsxd offset_reg, offset_reg_32
        // add handler_base, offset_reg
        let next_handler_address = self.handler_address
                                       .wrapping_add(unencrypted_offset as i32 as i64 as u64);

        self.handler_address = next_handler_address;

        return_word
    }

    pub fn disassemble_single_byte_operand(&mut self,
                                           vm_handler: &VmHandler,
                                           pe_file: &PeFile,
                                           pe_bytes: &[u8])
                                           -> u8 {
        let instruction_iter = vm_handler.instructions.iter();
        let mut instruction_iter = instruction_iter.skip_while(|insn| {
                                                       !match_xor_8_rolling_key_source(insn,
                                                                       &self.register_allocation)
                                                   });
        let encrypted_reg = instruction_iter.next().unwrap().op0_register();

        let encryption_iter = instruction_iter.take_while(|insn| {
                                  !match_xor_8_rolling_key_dest(insn, &self.register_allocation)
                              });
        let encrypted_byte = fetch_byte_vip(pe_file,
                                            pe_bytes,
                                            &mut self.vip_value,
                                            self.vip_direction_forwards);

        let return_byte = encrypted_byte.emulate_encryption(encryption_iter,
                                                            &mut self.rolling_key,
                                                            encrypted_reg);

        let encrypted_offset = fetch_dword_vip(pe_file,
                                               pe_bytes,
                                               &mut self.vip_value,
                                               self.vip_direction_forwards);

        let instruction_iter = vm_handler.instructions.iter();

        let instruction_iter =
            instruction_iter.skip_while(|insn| !match_fetch_vip(insn, &self.register_allocation));

        let mut instruction_iter = instruction_iter.skip_while(|insn| {
                                                       !match_xor_32_rolling_key_source(insn,
                                                                       &self.register_allocation)
                                                   });
        let encrypted_reg = instruction_iter.next().unwrap().op0_register();
        let encryption_iter = instruction_iter.take_while(|insn| {
                                                  !match_push_rolling_key(insn,
                                                                          &self.register_allocation)
                                              });

        let unencrypted_offset = encrypted_offset.emulate_encryption(encryption_iter,
                                                                     &mut self.rolling_key,
                                                                     encrypted_reg);

        // hmmm yes
        // movsxd offset_reg, offset_reg_32
        // add handler_base, offset_reg
        let next_handler_address = self.handler_address
                                       .wrapping_add(unencrypted_offset as i32 as i64 as u64);

        self.handler_address = next_handler_address;

        return_byte
    }

    pub fn disassemble_no_operand(&mut self,
                                  vm_handler: &VmHandler,
                                  pe_file: &PeFile,
                                  pe_bytes: &[u8]) {
        let encrypted_offset = fetch_dword_vip(pe_file,
                                               pe_bytes,
                                               &mut self.vip_value,
                                               self.vip_direction_forwards);

        let instruction_iter = vm_handler.instructions.iter();

        let instruction_iter =
            instruction_iter.skip_while(|insn| !match_fetch_vip(insn, &self.register_allocation));

        let mut instruction_iter = instruction_iter.skip_while(|insn| {
                                                       !match_xor_32_rolling_key_source(insn,
                                                                       &self.register_allocation)
                                                   });
        let encrypted_reg = instruction_iter.next().unwrap().op0_register();
        let encryption_iter = instruction_iter.take_while(|insn| {
                                                  !match_push_rolling_key(insn,
                                                                          &self.register_allocation)
                                              });

        let unencrypted_offset = encrypted_offset.emulate_encryption(encryption_iter,
                                                                     &mut self.rolling_key,
                                                                     encrypted_reg);

        // hmmm yes
        // movsxd offset_reg, offset_reg_32
        // add handler_base, offset_reg
        let next_handler_address = self.handler_address
                                       .wrapping_add(unencrypted_offset as i32 as i64 as u64);

        self.handler_address = next_handler_address;
    }
}

pub struct VmHandler {
    pub instructions: Vec<Instruction>,
}

impl VmHandler {
    pub fn new(address: u64,
               pe_file: &PeFile,
               pe_bytes: &[u8])
               -> Self {
        let mut instruction_address = address;
        let mut instructions = Vec::new();

        loop {
            let instruction = disassemble_instruction_at_va(pe_file, pe_bytes, instruction_address);

            match instruction.code() {
                Code::Retnq | Code::Jmp_rm64 => {
                    instructions.push(instruction);
                    break;
                },
                Code::Jmp_rel32_64 => {
                    let jmp_target = instruction.near_branch64();
                    instruction_address = jmp_target;
                },

                _ => {
                    instruction_address += instruction.len() as u64;
                    instructions.push(instruction);
                },
            }
        }

        Self { instructions }
    }

    pub fn get_register_allocation_vm_entry(&self) -> VmRegisterAllocation {
        // Find the handler_address register
        let handler_address_reg = {
            let instruction_last = self.instructions.last().unwrap();
            if instruction_last.code() == Code::Jmp_rm64 {
                instruction_last.op0_register().into()
            } else {
                let instruction = self.instructions
                                      .iter()
                                      .rev()
                                      .find(|&&insn| insn.code() == Code::Push_r64)
                                      .unwrap();
                instruction.op0_register().into()
            }
        };

        // Find key register
        let pop_instruction = self.instructions
                                  .iter()
                                  .rev()
                                  .find(|&&insn| insn.code() == Code::Pop_r64)
                                  .unwrap();

        let key = pop_instruction.op0_register().into();

        // Find vsp register
        let mov_vsp_instruction = self.instructions
                                      .iter()
                                      .find(|&&insn| {
                                          insn.code() == Code::Mov_r64_rm64 &&
                                          insn.op1_register() == iced_x86::Register::RSP
                                      })
                                      .unwrap();
        let vsp = mov_vsp_instruction.op0_register().into();

        // Find vip register
        let mov_vip_instruction = self.instructions
                                      .iter()
                                      .find(|&&insn| {
                                          insn.code() == Code::Mov_r64_rm64 &&
                                          insn.op1_kind() == OpKind::Memory &&
                                          insn.memory_displacement64() == 0x90
                                      })
                                      .unwrap();
        let vip = mov_vip_instruction.op0_register().into();

        VmRegisterAllocation { vip,
                               vsp,
                               key,
                               handler_address: handler_address_reg }
    }

    pub fn get_register_allocation_vm_jump_xchng(&self,
                                                 old_register_allocation: &mut VmRegisterAllocation)
    {
        let mut instruction_iter = self.instructions.iter();
        let mov_to_vip =
            instruction_iter.find(|insn| {
                match_fetch_reg_any_size(insn, old_register_allocation.vsp.into()).is_some()
            });

        let new_vip = mov_to_vip.unwrap().op0_register().full_register();

        let _add_vsp_instruction =
            instruction_iter.find(|insn| {
                                match_add_vsp_get_amount(insn, old_register_allocation).is_some()
                            });

        let xchng = instruction_iter.find(|insn| match_xchng_reg(insn, new_vip));

        let new_vsp_value_reg;
        let new_vip_reg: Register;

        match xchng {
            Some(xchng_insn) => {
                if !(xchng_insn.op0_register() == new_vip &&
                     xchng_insn.op1_register() == old_register_allocation.vsp.into()) &&
                   !(xchng_insn.op1_register() == new_vip &&
                     xchng_insn.op0_register() == old_register_allocation.vsp.into())
                {
                    panic!();
                } else {
                    new_vsp_value_reg = new_vip;
                    new_vip_reg = old_register_allocation.vsp.into();
                }
            },
            None => panic!(),
        }
        let mov_new_vsp =
            instruction_iter.clone()
                            .find(|insn| match_mov_reg_source(insn, new_vsp_value_reg));

        let new_vsp_reg = match mov_new_vsp {
            Some(insn) => insn.op0_register().full_register(),
            None => new_vsp_value_reg,
        };

        let store_key_reg = instruction_iter.find(|insn| match_mov_reg_source(insn, new_vip_reg));

        let new_key_register = store_key_reg.unwrap().op0_register();

        old_register_allocation.vsp = new_vsp_reg.into();
        old_register_allocation.vip = new_vip_reg.into();
        old_register_allocation.key = new_key_register.into();
    }

    pub fn get_register_allocation_vm_jump(&self,
                                           changed_vsp: bool,
                                           old_register_allocation: &mut VmRegisterAllocation) {
        let mut instruction_iter = self.instructions.iter();
        let mov_to_vip =
            instruction_iter.find(|insn| {
                match_fetch_reg_any_size(insn, old_register_allocation.vsp.into()).is_some()
            });

        let new_vip = mov_to_vip.unwrap().op0_register().full_register();

        let _add_vsp_instruction =
            instruction_iter.find(|insn| {
                                match_add_vsp_get_amount(insn, old_register_allocation).is_some()
                            });

        let mut cloned_iter = instruction_iter.clone();
        let mov_vip = cloned_iter.find(|insn| match_mov_reg_source(insn, new_vip));
        let new_vip = match mov_vip {
            Some(mov_vip) => {
                let potential_vip = mov_vip.op0_register().full_register();
                if cloned_iter.any(|insn| match_sub_reg_left(insn, potential_vip)) {
                    new_vip
                } else {
                    potential_vip
                }
            },
            None => new_vip,
        };
        if changed_vsp {
            let mov_vsp =
                instruction_iter.find(|insn| {
                                    match_mov_reg_source(insn, old_register_allocation.vsp.into())
                                });
            let new_vsp = mov_vsp.unwrap().op0_register().full_register();
            old_register_allocation.vsp = new_vsp.into();
        }

        let store_key_reg = instruction_iter.find(|insn| match_mov_reg_source(insn, new_vip));

        let new_key_register = store_key_reg.unwrap().op0_register();

        old_register_allocation.vip = new_vip.into();
        old_register_allocation.key = new_key_register.into();
    }

    pub fn get_push_order_vm_entry(&self) -> Vec<Registers> {
        let mut registers = Vec::new();

        for instruction in self.instructions
                               .iter()
                               .take_while(|&&insn| insn.code() != Code::Mov_r64_imm64)
        {
            match instruction.code() {
                Code::Push_r64 => {
                    let reg = instruction.op0_register();
                    registers.push(reg.into());
                },
                Code::Pushfq => {
                    registers.push(Registers::Flags);
                },
                _ => {},
            }
        }

        registers
    }

    pub fn determine_is_forwards(&self,
                                 reg_allocation: &VmRegisterAllocation)
                                 -> bool {
        for instruction in self.instructions.iter() {
            match instruction.code() {
                Code::Add_rm64_imm32 => {
                    if reg_allocation.vip == instruction.op0_register().into() &&
                       instruction.immediate32() == 0x4
                    {
                        return true;
                    }
                },
                Code::Sub_rm64_imm32 => {
                    if reg_allocation.vip == instruction.op0_register().into() &&
                       instruction.immediate32() == 0x4
                    {
                        return false;
                    }
                },
                _ => continue,
            }
        }
        panic!("Direction of vm_entry not found")
    }

    pub fn get_initial_vip(&self,
                           reg_allocation: &VmRegisterAllocation,
                           pushed_val: u64)
                           -> u64 {
        let mut encrypted_vip = pushed_val as u32;
        for instruction in
            self.instructions
                .iter()
                .skip_while(|&insn| !match_fetch_encrypted_vip(insn, reg_allocation))
                .take_while(|&insn| {
                    !((insn.code() == Code::Lea_r64_m || insn.code() == Code::Add_r64_rm64) &&
                      check_full_reg_written(insn, reg_allocation.vip.into()))
                })
                .filter(|&insn| check_full_reg_written(insn, reg_allocation.vip.into()))
        {
            let transform = get_transform_for_instruction(instruction);

            if let Some(transform) = transform {
                encrypted_vip = encrypted_vip.emulate_transform(transform);
            }
        }

        encrypted_vip as u64
    }
}

#[derive(Clone, Copy, Debug, PartialEq, Eq, Hash)]
pub enum Registers {
    Rax,
    Rbx,
    Rcx,
    Rdx,
    Rsi,
    Rdi,
    Rsp,
    Rbp,
    R8,
    R9,
    R10,
    R11,
    R12,
    R13,
    R14,
    R15,
    Flags,
}

impl Registers {
    pub fn to_arg_index(self) -> u64 {
        match self {
            Registers::Rax => 0,
            Registers::Rbx => 1,
            Registers::Rcx => 2,
            Registers::Rdx => 3,
            Registers::Rsi => 4,
            Registers::Rdi => 5,
            Registers::Rsp => 7,
            Registers::Rbp => 6,
            Registers::R8 => 8,
            Registers::R9 => 9,
            Registers::R10 => 10,
            Registers::R11 => 11,
            Registers::R12 => 12,
            Registers::R13 => 13,
            Registers::R14 => 14,
            Registers::R15 => 15,
            Registers::Flags => 16,
        }
    }
}

impl From<iced_x86::Register> for Registers {
    fn from(reg: iced_x86::Register) -> Self {
        match reg {
            iced_x86::Register::RAX => Registers::Rax,
            iced_x86::Register::RBX => Registers::Rbx,
            iced_x86::Register::RCX => Registers::Rcx,
            iced_x86::Register::RDX => Registers::Rdx,
            iced_x86::Register::RSI => Registers::Rsi,
            iced_x86::Register::RDI => Registers::Rdi,
            iced_x86::Register::RSP => Registers::Rsp,
            iced_x86::Register::RBP => Registers::Rbp,
            iced_x86::Register::R8 => Registers::R8,
            iced_x86::Register::R9 => Registers::R9,
            iced_x86::Register::R10 => Registers::R10,
            iced_x86::Register::R11 => Registers::R11,
            iced_x86::Register::R12 => Registers::R12,
            iced_x86::Register::R13 => Registers::R13,
            iced_x86::Register::R14 => Registers::R14,
            iced_x86::Register::R15 => Registers::R15,

            _ => unimplemented!("Register not implemented"),
        }
    }
}

impl From<Registers> for iced_x86::Register {
    fn from(reg: Registers) -> iced_x86::Register {
        match reg {
            Registers::Rax => iced_x86::Register::RAX,
            Registers::Rbx => iced_x86::Register::RBX,
            Registers::Rcx => iced_x86::Register::RCX,
            Registers::Rdx => iced_x86::Register::RDX,
            Registers::Rsi => iced_x86::Register::RSI,
            Registers::Rdi => iced_x86::Register::RDI,
            Registers::Rsp => iced_x86::Register::RSP,
            Registers::Rbp => iced_x86::Register::RBP,
            Registers::R8 => iced_x86::Register::R8,
            Registers::R9 => iced_x86::Register::R9,
            Registers::R10 => iced_x86::Register::R10,
            Registers::R11 => iced_x86::Register::R11,
            Registers::R12 => iced_x86::Register::R12,
            Registers::R13 => iced_x86::Register::R13,
            Registers::R14 => iced_x86::Register::R14,
            Registers::R15 => iced_x86::Register::R15,
            _ => unreachable!(),
        }
    }
}

pub fn fetch_qword_vip(pe_file: &PeFile,
                       pe_bytes: &[u8],
                       vip: &mut u64,
                       direction_is_forwards: bool)
                       -> u64 {
    let return_value;

    if direction_is_forwards {
        return_value = u64::from_le_bytes(read_bytes_at_va(pe_file, pe_bytes, *vip, 8).unwrap()
                                                                                      .try_into()
                                                                                      .unwrap());
        *vip += 8;
    } else {
        *vip -= 8;
        return_value = u64::from_le_bytes(read_bytes_at_va(pe_file, pe_bytes, *vip, 8).unwrap()
                                                                                      .try_into()
                                                                                      .unwrap());
    }

    return_value
}

pub fn fetch_word_vip(pe_file: &PeFile,
                      pe_bytes: &[u8],
                      vip: &mut u64,
                      direction_is_forwards: bool)
                      -> u16 {
    let return_value;

    if direction_is_forwards {
        return_value = u16::from_le_bytes(read_bytes_at_va(pe_file, pe_bytes, *vip, 2).unwrap()
                                                                                      .try_into()
                                                                                      .unwrap());
        *vip += 2;
    } else {
        *vip -= 2;
        return_value = u16::from_le_bytes(read_bytes_at_va(pe_file, pe_bytes, *vip, 2).unwrap()
                                                                                      .try_into()
                                                                                      .unwrap());
    }

    return_value
}

pub fn fetch_dword_vip(pe_file: &PeFile,
                       pe_bytes: &[u8],
                       vip: &mut u64,
                       direction_is_forwards: bool)
                       -> u32 {
    let return_value;

    if direction_is_forwards {
        return_value = u32::from_le_bytes(read_bytes_at_va(pe_file, pe_bytes, *vip, 4).unwrap()
                                                                                      .try_into()
                                                                                      .unwrap());
        *vip += 4;
    } else {
        *vip -= 4;
        return_value = u32::from_le_bytes(read_bytes_at_va(pe_file, pe_bytes, *vip, 4).unwrap()
                                                                                      .try_into()
                                                                                      .unwrap());
    }

    return_value
}

pub fn fetch_byte_vip(pe_file: &PeFile,
                      pe_bytes: &[u8],
                      vip: &mut u64,
                      direction_is_forwards: bool)
                      -> u8 {
    let return_value;

    if direction_is_forwards {
        return_value = read_bytes_at_va(pe_file, pe_bytes, *vip, 1).unwrap()[0];
        *vip += 1;
    } else {
        *vip -= 1;
        return_value = read_bytes_at_va(pe_file, pe_bytes, *vip, 1).unwrap()[0];
    }

    return_value
}

```

`src/vm_matchers.rs`:

```rs
use std::fmt::Display;

use iced_x86::{Code, Instruction, Register};

use crate::{
    match_assembly::*,
    util::check_full_reg_written,
    vm_handler::{Registers, VmContext, VmHandler, VmRegisterAllocation},
};

#[derive(Clone, Copy, Debug, PartialEq, Eq)]
pub enum HandlerClass {
    ByteOperand,
    WordOperand,
    DwordOperand,
    QwordOperand,
    NoOperand,
    UnconditionalBranch,
    NoVipChange,
}

#[derive(Clone, Copy, Debug, PartialEq, Eq)]
pub enum HandlerVmInstruction {
    /// Size in bytes and reg offset in register file
    Pop(usize, u8),
    Push(usize, u8),
    PushImm64(u64),
    PushImm32(u32),
    PushImm16(u16),
    PushImm8(u8),
    PushVsp(usize),
    PopVsp(usize),
    Add(usize),
    Mul(usize),
    Imul(usize),
    Shr(usize),
    Shl(usize),
    Shrd(usize),
    Shld(usize),
    Nand(usize),
    Nor(usize),
    Fetch(usize),
    Store(usize),
    PushCr0,
    PushCr3,
    Rdtsc,
    Nop,
    VmExit,
    JumpDecVspChange,
    JumpIncVspChange,
    JumpDecVspXchng,
    JumpIncVspXchng,
    JumpDec,
    JumpInc,
    UnknownByteOperand,
    UnknownWordOperand,
    UnknownDwordOperand,
    UnknownQwordOperand,
    UnknownNoOperand,
    UnknownNoVipChange,
    Unknown,
}

impl Display for HandlerVmInstruction {
    fn fmt(&self,
           f: &mut std::fmt::Formatter<'_>)
           -> std::fmt::Result {
        /// Local function for fomatting registers
        fn format_reg_offset(reg_offset: u8,
                             size: usize)
                             -> String {
            let register_number = reg_offset / 8;
            let inner_reg_offset = reg_offset % 8;

            match size {
                8 => {
                    format!("r{}", register_number)
                },
                4 => match inner_reg_offset {
                    0 => format!("r{}_dword_0", register_number),
                    4 => format!("r{}_dword_1", register_number),
                    _ => unimplemented!(),
                },
                2 => match inner_reg_offset {
                    0 => format!("r{}_word_0", register_number),
                    2 => format!("r{}_word_1", register_number),
                    4 => format!("r{}_word_2", register_number),
                    6 => format!("r{}_word_3", register_number),
                    _ => {
                        dbg!(inner_reg_offset);
                        unimplemented!();
                    },
                },
                1 => match inner_reg_offset {
                    0 => format!("r{}_byte_0", register_number),
                    1 => format!("r{}_byte_1", register_number),
                    _ => unimplemented!(),
                },
                _ => unimplemented!("Size -> not implemented yet {}", size),
            }
        }

        match self {
            HandlerVmInstruction::Pop(size, reg_offset) => write!(f,
                                                                  "pop{} {}",
                                                                  size * 8,
                                                                  format_reg_offset(*reg_offset,
                                                                                    *size)),
            HandlerVmInstruction::Push(size, reg_offset) => write!(f,
                                                                   "push{} {}",
                                                                   size * 8,
                                                                   format_reg_offset(*reg_offset,
                                                                                     *size)),
            HandlerVmInstruction::PushImm64(imm64) => write!(f, "push_imm64 {:#x}", imm64),
            HandlerVmInstruction::PushImm32(imm32) => write!(f, "push_imm32 {:#x}", imm32),
            HandlerVmInstruction::PushImm16(imm16) => write!(f, "push_imm16 {:#x}", imm16),
            HandlerVmInstruction::PushImm8(imm8) => write!(f, "push_imm8 {:#x}", imm8),
            HandlerVmInstruction::PushVsp(size) => write!(f, "pushvsp{}", size * 8),
            HandlerVmInstruction::PopVsp(size) => write!(f, "popvsp{}", size * 8),
            HandlerVmInstruction::Add(size) => write!(f, "add{}", size * 8),
            HandlerVmInstruction::Mul(size) => write!(f, "mul{}", size * 8),
            HandlerVmInstruction::Imul(size) => write!(f, "imul{}", size * 8),
            HandlerVmInstruction::Shrd(size) => write!(f, "shrd{}", size * 8),
            HandlerVmInstruction::Shld(size) => write!(f, "shld{}", size * 8),
            HandlerVmInstruction::Shr(size) => write!(f, "shr{}", size * 8),
            HandlerVmInstruction::Shl(size) => write!(f, "shl{}", size * 8),
            HandlerVmInstruction::Nand(size) => write!(f, "nand{}", size * 8),
            HandlerVmInstruction::Nor(size) => write!(f, "nor{}", size * 8),
            HandlerVmInstruction::Fetch(size) => write!(f, "fetch{}", size * 8),
            HandlerVmInstruction::Store(size) => write!(f, "store{}", size * 8),
            HandlerVmInstruction::PushCr0 => write!(f, "push_cr0"),
            HandlerVmInstruction::PushCr3 => write!(f, "push_cr3"),
            HandlerVmInstruction::Rdtsc => write!(f, "rdtsc"),
            HandlerVmInstruction::Nop => write!(f, "nop"),
            HandlerVmInstruction::VmExit => write!(f, "vm_exit"),
            HandlerVmInstruction::JumpDecVspXchng => write!(f, "jump_dec_vsp_xchng"),
            HandlerVmInstruction::JumpIncVspXchng => write!(f, "jump_inc_vsp_xchng"),
            HandlerVmInstruction::JumpDecVspChange => write!(f, "jump_dec_vsp_change"),
            HandlerVmInstruction::JumpIncVspChange => write!(f, "jump_inc_vsp_change"),
            HandlerVmInstruction::JumpDec => write!(f, "jump_dec"),
            HandlerVmInstruction::JumpInc => write!(f, "jump_inc"),
            HandlerVmInstruction::UnknownByteOperand => {
                write!(f, "[unknown byte operand instruction]")
            },
            HandlerVmInstruction::UnknownWordOperand => {
                write!(f, "[unknown word operand instruction]")
            },
            HandlerVmInstruction::UnknownDwordOperand => {
                write!(f, "[unknown dword operand instruction]")
            },
            HandlerVmInstruction::UnknownQwordOperand => {
                write!(f, "[unknown qword operand instruction]")
            },
            HandlerVmInstruction::UnknownNoOperand => write!(f, "[unknown no operand instruction]"),
            HandlerVmInstruction::UnknownNoVipChange => {
                write!(f, "[unknown no vip change instruction]")
            },
            HandlerVmInstruction::Unknown => write!(f, "[unknown instruction]"),
        }
    }
}

impl VmHandler {
    /// Classify the handler based on the class of access to the vip register
    pub fn match_handler_class(&self,
                               reg_allocation: &VmRegisterAllocation)
                               -> HandlerClass {
        // Create an iterator over the instructions of the handler
        let instruction_iter = self.instructions.iter();

        // Collect all the mov accesses to the vip register
        let vip_modification_vec =
            instruction_iter.clone()
                            .filter(|insn| check_full_reg_written(insn, reg_allocation.vip.into()))
                            .filter(|insn| insn.code() == Code::Mov_r64_rm64)
                            .collect::<Vec<_>>();

        // These registers are used in the stack relocation code of some handlers so
        // they are special
        if (reg_allocation.vip != Registers::Rsi &&
            reg_allocation.vip != Registers::Rdi &&
            !vip_modification_vec.is_empty()) ||
           (vip_modification_vec.len() >= 2)
        {
            return HandlerClass::UnconditionalBranch;
        }

        // Collect all instructions that advance the vip and get their immediate values
        let vip_update_vec =
            instruction_iter.filter(|insn| check_full_reg_written(insn, reg_allocation.vip.into()))
                            .filter(|insn| {
                                insn.code() == Code::Add_rm64_imm32 ||
                                insn.code() == Code::Sub_rm64_imm32
                            })
                            .map(|insn| insn.immediate32())
                            .collect::<Vec<_>>();

        // Match the vip offsets to a handler class
        match vip_update_vec.as_slice() {
            &[] => HandlerClass::NoVipChange,
            &[4] => HandlerClass::NoOperand,
            &[8, 4] => HandlerClass::QwordOperand,
            &[4, 4] => HandlerClass::DwordOperand,
            &[2, 4] => HandlerClass::WordOperand,
            &[1, 4] => HandlerClass::ByteOperand,
            slice => {
                panic!("Unimplemented handler class with slice {:?}", slice)
            },
        }
    }

    /// Function for matching handlers of the UnconditionalBranch class
    pub fn match_branch_instructions(&self,
                                     vm_context: &VmContext)
                                     -> HandlerVmInstruction {
        if vm_match_vm_exit(self, &vm_context.register_allocation) {
            return HandlerVmInstruction::VmExit;
        }

        if vm_match_jmp_dec_vsp_change(self, vm_context) {
            return HandlerVmInstruction::JumpDecVspChange;
        }

        if vm_match_jmp_inc_vsp_change(self, vm_context) {
            return HandlerVmInstruction::JumpIncVspChange;
        }

        if vm_match_jmp_dec_vsp_xchng(self, vm_context) {
            return HandlerVmInstruction::JumpDecVspXchng;
        }

        if vm_match_jmp_inc_vsp_xchng(self, vm_context) {
            return HandlerVmInstruction::JumpIncVspXchng;
        }

        if vm_match_jmp_dec(self, vm_context) {
            return HandlerVmInstruction::JumpDec;
        }

        if vm_match_jmp_inc(self, vm_context) {
            return HandlerVmInstruction::JumpInc;
        }

        HandlerVmInstruction::Unknown
    }

    /// Function for mathing the handlers of the NoVipChange class
    pub fn match_no_vip_change_instructions(&self,
                                            vm_context: &VmContext)
                                            -> HandlerVmInstruction {
        if vm_match_vm_exit(self, &vm_context.register_allocation) {
            return HandlerVmInstruction::VmExit;
        }

        if vm_match_jmp_dec_vsp_change(self, vm_context) {
            return HandlerVmInstruction::JumpDecVspChange;
        }

        if vm_match_jmp_inc_vsp_change(self, vm_context) {
            return HandlerVmInstruction::JumpIncVspChange;
        }

        if vm_match_jmp_dec_vsp_xchng(self, vm_context) {
            return HandlerVmInstruction::JumpDecVspXchng;
        }

        if vm_match_jmp_inc_vsp_xchng(self, vm_context) {
            return HandlerVmInstruction::JumpIncVspXchng;
        }

        if vm_match_jmp_dec(self, vm_context) {
            return HandlerVmInstruction::JumpDec;
        }

        if vm_match_jmp_inc(self, vm_context) {
            return HandlerVmInstruction::JumpInc;
        }

        HandlerVmInstruction::UnknownNoVipChange
    }

    /// Function for mathing the handlers of the ByteOperand class
    pub fn match_byte_operand_instructions(&self,
                                           reg_allocation: &VmRegisterAllocation,
                                           byte_operand: u8)
                                           -> HandlerVmInstruction {
        if vm_match_push_imm8(self, reg_allocation) {
            return HandlerVmInstruction::PushImm8(byte_operand);
        }

        if let Some(size) = vm_match_vm_reg_pop(self, reg_allocation) {
            return HandlerVmInstruction::Pop(size, byte_operand);
        }

        if let Some(size) = vm_match_vm_reg_pop_v2(self, reg_allocation) {
            return HandlerVmInstruction::Pop(size, byte_operand);
        }

        if let Some(size) = vm_match_vm_reg_push(self, reg_allocation) {
            return HandlerVmInstruction::Push(size, byte_operand);
        }

        HandlerVmInstruction::UnknownByteOperand
    }

    /// Function for mathing the handlers of the WordOperand class
    pub fn match_word_operand_instructions(&self,
                                           reg_allocation: &VmRegisterAllocation,
                                           word_operand: u16)
                                           -> HandlerVmInstruction {
        if vm_match_push_imm16(self, reg_allocation) {
            return HandlerVmInstruction::PushImm16(word_operand);
        }
        HandlerVmInstruction::UnknownWordOperand
    }

    /// Function for mathing the handlers of the DwordOperand class
    pub fn match_dword_operand_instructions(&self,
                                            reg_allocation: &VmRegisterAllocation,
                                            dword_operand: u32)
                                            -> HandlerVmInstruction {
        if vm_match_push_imm32(self, reg_allocation) {
            return HandlerVmInstruction::PushImm32(dword_operand);
        }
        HandlerVmInstruction::UnknownDwordOperand
    }

    /// Function for mathing the handlers of the QwordOperand class
    pub fn match_qword_operand_instructions(&self,
                                            reg_allocation: &VmRegisterAllocation,
                                            qword_operand: u64)
                                            -> HandlerVmInstruction {
        if vm_match_push_imm64(self, reg_allocation) {
            return HandlerVmInstruction::PushImm64(qword_operand);
        }
        HandlerVmInstruction::UnknownQwordOperand
    }

    /// Function for mathing the handlers of the NoOperand class
    pub fn match_no_operand_instructions(&self,
                                         vm_context: &VmContext,
                                         reg_allocation: &VmRegisterAllocation)
                                         -> HandlerVmInstruction {
        if vm_match_nop(self) {
            return HandlerVmInstruction::Nop;
        }

        if vm_match_push_cr0(self) {
            return HandlerVmInstruction::PushCr0;
        }

        if vm_match_push_cr3(self) {
            return HandlerVmInstruction::PushCr3;
        }

        if vm_match_rdtsc(self) {
            return HandlerVmInstruction::Rdtsc;
        }

        if let Some(size) = vm_match_push_vsp(self, reg_allocation) {
            return HandlerVmInstruction::PushVsp(size);
        }

        if vm_match_jmp_dec_vsp_change(self, vm_context) {
            return HandlerVmInstruction::JumpDecVspChange;
        }

        if vm_match_jmp_inc_vsp_change(self, vm_context) {
            return HandlerVmInstruction::JumpIncVspChange;
        }

        if vm_match_jmp_dec_vsp_xchng(self, vm_context) {
            return HandlerVmInstruction::JumpDecVspXchng;
        }

        if vm_match_jmp_inc_vsp_xchng(self, vm_context) {
            return HandlerVmInstruction::JumpIncVspXchng;
        }

        if vm_match_jmp_dec(self, vm_context) {
            return HandlerVmInstruction::JumpDec;
        }

        if vm_match_jmp_inc(self, vm_context) {
            return HandlerVmInstruction::JumpInc;
        }

        if let Some(size) = vm_match_add(self, reg_allocation) {
            return HandlerVmInstruction::Add(size);
        }

        if let Some(size) = vm_match_add_byte(self, reg_allocation) {
            return HandlerVmInstruction::Add(size);
        }

        if let Some(size) = vm_match_mul(self, reg_allocation) {
            return HandlerVmInstruction::Mul(size);
        }

        if let Some(size) = vm_match_imul(self, reg_allocation) {
            return HandlerVmInstruction::Imul(size);
        }

        if let Some(size) = vm_match_nand(self, reg_allocation) {
            return HandlerVmInstruction::Nand(size);
        }

        if let Some(size) = vm_match_nand_byte(self, reg_allocation) {
            return HandlerVmInstruction::Nand(size);
        }

        if let Some(size) = vm_match_nor(self, reg_allocation) {
            return HandlerVmInstruction::Nor(size);
        }

        if let Some(size) = vm_match_nor_byte(self, reg_allocation) {
            return HandlerVmInstruction::Nor(size);
        }

        if vm_match_pop_vsp_64(self, reg_allocation) {
            return HandlerVmInstruction::PopVsp(8);
        }

        if let Some(size) = vm_match_fetch(self, reg_allocation) {
            return HandlerVmInstruction::Fetch(size);
        }

        if let Some(size) = vm_match_fetch_byte(self, reg_allocation) {
            return HandlerVmInstruction::Fetch(size);
        }

        if let Some(size) = vm_match_store(self, reg_allocation) {
            return HandlerVmInstruction::Store(size);
        }

        if let Some(size) = vm_match_shr(self, reg_allocation) {
            return HandlerVmInstruction::Shr(size);
        }

        if let Some(size) = vm_match_shr_byte(self, reg_allocation) {
            return HandlerVmInstruction::Shr(size);
        }

        if let Some(size) = vm_match_shl(self, reg_allocation) {
            return HandlerVmInstruction::Shl(size);
        }

        if let Some(size) = vm_match_shl_byte(self, reg_allocation) {
            return HandlerVmInstruction::Shl(size);
        }

        if let Some(size) = vm_match_shrd(self, reg_allocation) {
            return HandlerVmInstruction::Shrd(size);
        }

        if let Some(size) = vm_match_shld(self, reg_allocation) {
            return HandlerVmInstruction::Shld(size);
        }

        HandlerVmInstruction::UnknownNoOperand
    }
}

fn vm_match_jmp_dec(vm_handler: &VmHandler,
                    vm_context: &VmContext)
                    -> bool {
    let mut instruction_iter = vm_handler.instructions.iter();
    let mov_to_vip =
        instruction_iter.find(|insn| {
            match_fetch_reg_any_size(insn, vm_context.register_allocation.vsp.into()).is_some()
        });

    if mov_to_vip.is_none() {
        return false;
    }

    let new_vip = mov_to_vip.unwrap().op0_register().full_register();

    let add_vsp_instruction =
        instruction_iter.find(|insn| match_add_vsp_get_amount(insn, &vm_context.register_allocation).is_some());

    if add_vsp_instruction.is_none() {
        return false;
    }

    if add_vsp_instruction.unwrap().immediate32() != 8 {
        return false;
    }

    let mut cloned_iter = instruction_iter.clone();
    let mov_vip = cloned_iter.find(|insn| match_mov_reg_source(insn, new_vip));
    let new_vip = match mov_vip {
        Some(mov_vip) => {
            let potential_vip = mov_vip.op0_register().full_register();
            if cloned_iter.any(|insn| match_sub_reg_left(insn, potential_vip)) {
                new_vip
            } else {
                potential_vip
            }
        },
        None => new_vip,
    };

    let store_key_reg = instruction_iter.find(|insn| match_mov_reg_source(insn, new_vip));

    if store_key_reg.is_none() {
        return false;
    }

    instruction_iter.any(|insn| match_sub_reg_by_amount(insn, new_vip, 4))
}

fn vm_match_jmp_inc(vm_handler: &VmHandler,
                    vm_context: &VmContext)
                    -> bool {
    let mut instruction_iter = vm_handler.instructions.iter();
    let mov_to_vip =
        instruction_iter.find(|insn| {
            match_fetch_reg_any_size(insn, vm_context.register_allocation.vsp.into()).is_some()
        });

    if mov_to_vip.is_none() {
        return false;
    }

    let new_vip = mov_to_vip.unwrap().op0_register().full_register();

    let add_vsp_instruction =
        instruction_iter.find(|insn| match_add_vsp_get_amount(insn, &vm_context.register_allocation).is_some());

    if add_vsp_instruction.is_none() {
        return false;
    }

    if add_vsp_instruction.unwrap().immediate32() != 8 {
        return false;
    }

    let mut cloned_iter = instruction_iter.clone();
    let mov_vip = cloned_iter.find(|insn| match_mov_reg_source(insn, new_vip));
    let new_vip = match mov_vip {
        Some(mov_vip) => {
            let potential_vip = mov_vip.op0_register().full_register();
            if cloned_iter.any(|insn| match_sub_reg_left(insn, potential_vip)) {
                new_vip
            } else {
                potential_vip
            }
        },
        None => new_vip,
    };

    let store_key_reg = instruction_iter.find(|insn| match_mov_reg_source(insn, new_vip));

    if store_key_reg.is_none() {
        return false;
    }

    instruction_iter.any(|insn| match_add_reg_by_amount(insn, new_vip, 4))
}

fn vm_match_jmp_dec_vsp_change(vm_handler: &VmHandler,
                               vm_context: &VmContext)
                               -> bool {
    let mut instruction_iter = vm_handler.instructions.iter();
    let mov_to_vip =
        instruction_iter.find(|insn| {
            match_fetch_reg_any_size(insn, vm_context.register_allocation.vsp.into()).is_some()
        });

    if mov_to_vip.is_none() {
        return false;
    }

    let new_vip = mov_to_vip.unwrap().op0_register().full_register();

    let add_vsp_instruction =
        instruction_iter.find(|insn| match_add_vsp_get_amount(insn, &vm_context.register_allocation).is_some());

    if add_vsp_instruction.is_none() {
        return false;
    }

    if add_vsp_instruction.unwrap().immediate32() != 8 {
        return false;
    }

    let mut cloned_iter = instruction_iter.clone();
    let mov_vip = cloned_iter.find(|insn| match_mov_reg_source(insn, new_vip));
    let new_vip = match mov_vip {
        Some(mov_vip) => {
            let potential_vip = mov_vip.op0_register().full_register();
            if cloned_iter.any(|insn| match_sub_reg_left(insn, potential_vip)) {
                new_vip
            } else {
                potential_vip
            }
        },
        None => new_vip,
    };

    let _mov_vsp =
        instruction_iter.find(|insn| {
                            match_mov_reg_source(insn, vm_context.register_allocation.vsp.into())
                        });

    let store_key_reg = instruction_iter.find(|insn| match_mov_reg_source(insn, new_vip));

    if store_key_reg.is_none() {
        return false;
    }

    instruction_iter.any(|insn| match_sub_reg_by_amount(insn, new_vip, 4))
}

fn vm_match_jmp_inc_vsp_change(vm_handler: &VmHandler,
                               vm_context: &VmContext)
                               -> bool {
    let mut instruction_iter = vm_handler.instructions.iter();
    let mov_to_vip =
        instruction_iter.find(|insn| {
            match_fetch_reg_any_size(insn, vm_context.register_allocation.vsp.into()).is_some()
        });

    if mov_to_vip.is_none() {
        return false;
    }

    let new_vip = mov_to_vip.unwrap().op0_register().full_register();

    let add_vsp_instruction =
        instruction_iter.find(|insn| match_add_vsp_get_amount(insn, &vm_context.register_allocation).is_some());

    if add_vsp_instruction.is_none() {
        return false;
    }

    if add_vsp_instruction.unwrap().immediate32() != 8 {
        return false;
    }

    let mut cloned_iter = instruction_iter.clone();
    let mov_vip = cloned_iter.find(|insn| match_mov_reg_source(insn, new_vip));
    let new_vip = match mov_vip {
        Some(mov_vip) => {
            let potential_vip = mov_vip.op0_register().full_register();
            if cloned_iter.any(|insn| match_sub_reg_left(insn, potential_vip)) {
                new_vip
            } else {
                potential_vip
            }
        },
        None => new_vip,
    };

    let _mov_vsp =
        instruction_iter.find(|insn| {
                            match_mov_reg_source(insn, vm_context.register_allocation.vsp.into())
                        });

    let store_key_reg = instruction_iter.find(|insn| match_mov_reg_source(insn, new_vip));

    if store_key_reg.is_none() {
        return false;
    }

    instruction_iter.any(|insn| match_add_reg_by_amount(insn, new_vip, 4))
}

fn vm_match_jmp_dec_vsp_xchng(vm_handler: &VmHandler,
                              vm_context: &VmContext)
                              -> bool {
    let mut instruction_iter = vm_handler.instructions.iter();
    let mov_to_vip =
        instruction_iter.find(|insn| {
            match_fetch_reg_any_size(insn, vm_context.register_allocation.vsp.into()).is_some()
        });

    if mov_to_vip.is_none() {
        return false;
    }

    let new_vip = mov_to_vip.unwrap().op0_register().full_register();

    let add_vsp_instruction =
        instruction_iter.find(|insn| match_add_vsp_get_amount(insn, &vm_context.register_allocation).is_some());

    if add_vsp_instruction.is_none() {
        return false;
    }

    if add_vsp_instruction.unwrap().immediate32() != 8 {
        return false;
    }

    let xchng = instruction_iter.find(|insn| match_xchng_reg(insn, new_vip));

    let new_vip_reg;
    match xchng {
        Some(xchng_insn) => {
            if !(xchng_insn.op0_register() == new_vip &&
                 xchng_insn.op1_register() == vm_context.register_allocation.vsp.into()) &&
               !(xchng_insn.op1_register() == new_vip &&
                 xchng_insn.op0_register() == vm_context.register_allocation.vsp.into())
            {
                return false;
            } else {
                new_vip_reg = vm_context.register_allocation.vsp.into();
            }
        },
        None => return false,
    }

    let store_key_reg = instruction_iter.find(|insn| match_mov_reg_source(insn, new_vip_reg));

    if store_key_reg.is_none() {
        return false;
    }

    instruction_iter.any(|insn| match_sub_reg_by_amount(insn, new_vip_reg, 4))
}

fn vm_match_jmp_inc_vsp_xchng(vm_handler: &VmHandler,
                              vm_context: &VmContext)
                              -> bool {
    let mut instruction_iter = vm_handler.instructions.iter();
    let mov_to_vip =
        instruction_iter.find(|insn| {
            match_fetch_reg_any_size(insn, vm_context.register_allocation.vsp.into()).is_some()
        });

    if mov_to_vip.is_none() {
        return false;
    }

    let new_vip = mov_to_vip.unwrap().op0_register().full_register();

    let add_vsp_instruction =
        instruction_iter.find(|insn| match_add_vsp_get_amount(insn, &vm_context.register_allocation).is_some());

    if add_vsp_instruction.is_none() {
        return false;
    }

    if add_vsp_instruction.unwrap().immediate32() != 8 {
        return false;
    }

    let xchng = instruction_iter.find(|insn| match_xchng_reg(insn, new_vip));

    let new_vip_reg;
    match xchng {
        Some(xchng_insn) => {
            if !(xchng_insn.op0_register() == new_vip &&
                 xchng_insn.op1_register() == vm_context.register_allocation.vsp.into()) &&
               !(xchng_insn.op1_register() == new_vip &&
                 xchng_insn.op0_register() == vm_context.register_allocation.vsp.into())
            {
                return false;
            } else {
                new_vip_reg = vm_context.register_allocation.vsp.into();
            }
        },
        None => return false,
    }

    let store_key_reg = instruction_iter.find(|insn| match_mov_reg_source(insn, new_vip_reg));

    if store_key_reg.is_none() {
        return false;
    }

    instruction_iter.any(|insn| match_add_reg_by_amount(insn, new_vip_reg, 4))
}
/// Match a pop of a vmregister
fn vm_match_vm_reg_pop(vm_handler: &VmHandler,
                       reg_allocation: &VmRegisterAllocation)
                       -> Option<usize> {
    let mut instruction_iter = vm_handler.instructions.iter();
    instruction_iter.find(|insn| {
                        match_fetch_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                    });
    let _add_vsp_instruction =
        instruction_iter.find(|insn| match_add_vsp_get_amount(insn, reg_allocation).is_some());

    let vip_byte_fetch_instruction =
        instruction_iter.find(|insn| {
                            match_fetch_reg_any_size(insn, reg_allocation.vip.into()).is_some()
                        })?;

    let index_reg = vip_byte_fetch_instruction.op0_register().full_register();

    let store_vm_reg = instruction_iter.find(|insn| match_store_vm_reg(insn, index_reg));
    store_vm_reg.map(|insn| insn.memory_size().size())
}

/// Match a pop of a vmregister (different vmp3 version maybe idk)
fn vm_match_vm_reg_pop_v2(vm_handler: &VmHandler,
                          reg_allocation: &VmRegisterAllocation)
                          -> Option<usize> {
    let mut instruction_iter = vm_handler.instructions.iter();
    let vip_byte_fetch_instruction =
        instruction_iter.find(|insn| {
                            match_fetch_reg_any_size(insn, reg_allocation.vip.into()).is_some()
                        })?;

    let index_reg = vip_byte_fetch_instruction.op0_register().full_register();

    instruction_iter.find(|insn| {
                        match_fetch_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                    });

    let _add_vsp_instruction =
        instruction_iter.find(|insn| match_add_vsp_get_amount(insn, reg_allocation).is_some());

    let store_vm_reg = instruction_iter.find(|insn| match_store_vm_reg(insn, index_reg));
    store_vm_reg.map(|insn| insn.memory_size().size())
}
/// Match a push of a vmregister
fn vm_match_vm_reg_push(vm_handler: &VmHandler,
                        reg_allocation: &VmRegisterAllocation)
                        -> Option<usize> {
    let mut instruction_iter = vm_handler.instructions.iter();

    let vip_byte_fetch_instruction =
        instruction_iter.find(|insn| {
                            match_fetch_reg_any_size(insn, reg_allocation.vip.into()).is_some()
                        })?;

    let index_reg = vip_byte_fetch_instruction.op0_register().full_register();

    let fetch_vm_reg = instruction_iter.find(|insn| match_fetch_vm_reg(insn, index_reg));

    let _sub_vsp_instruction =
        instruction_iter.find(|insn| match_sub_vsp_get_amount(insn, reg_allocation).is_some());

    instruction_iter.find(|insn| {
                        match_store_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                    })?;

    fetch_vm_reg.map(|insn| insn.memory_size().size())
}

/// Match a n byte imm push
fn match_push_imm_n<const N: u32>(vm_handler: &VmHandler,
                                  reg_allocation: &VmRegisterAllocation)
                                  -> bool {
    let mut instruction_iter = vm_handler.instructions.iter();
    instruction_iter.find(|insn| match_sub_vsp_by_amount(insn, reg_allocation, N));
    instruction_iter.any(|insn| match_store_reg_any_size(insn, reg_allocation.vsp.into()).is_some())
}

/// Match 64 bit imm push
fn vm_match_push_imm64(vm_handler: &VmHandler,
                       reg_allocation: &VmRegisterAllocation)
                       -> bool {
    match_push_imm_n::<8>(vm_handler, reg_allocation)
}

/// Match 32 bit imm push
fn vm_match_push_imm32(vm_handler: &VmHandler,
                       reg_allocation: &VmRegisterAllocation)
                       -> bool {
    match_push_imm_n::<4>(vm_handler, reg_allocation)
}

/// Match 16 bit imm push
fn vm_match_push_imm16(vm_handler: &VmHandler,
                       reg_allocation: &VmRegisterAllocation)
                       -> bool {
    match_push_imm_n::<2>(vm_handler, reg_allocation)
}

/// Match 8 bit imm push
fn vm_match_push_imm8(vm_handler: &VmHandler,
                      reg_allocation: &VmRegisterAllocation)
                      -> bool {
    // Push of byte is sign extended
    match_push_imm_n::<2>(vm_handler, reg_allocation)
}

fn vm_match_pop_vsp_64(vm_handler: &VmHandler,
                       reg_allocation: &VmRegisterAllocation)
                       -> bool {
    let mut instruction_iter = vm_handler.instructions.iter();

    let fetch_vsp_instruction_1 =
        instruction_iter.find(|insn| {
                            match_fetch_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                        });
    if fetch_vsp_instruction_1.is_none() {
        return false;
    }

    fetch_vsp_instruction_1.unwrap().op0_register() == reg_allocation.vsp.into()
}

macro_rules! generate_binop_match {
    ($matcher_name:ident, $specific_matcher:expr) => {

        fn $matcher_name(vm_handler: &VmHandler,
                        reg_allocation: &VmRegisterAllocation)
                        -> Option<usize> {
            let mut instruction_iter = vm_handler.instructions.iter();

            let fetch_vsp_instruction_1 =
                instruction_iter.find(|insn| {
                                    match_fetch_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                                })?;
            let reg1 = fetch_vsp_instruction_1.op0_register().full_register();

            let fetch_vsp_instruction_2 =
                instruction_iter.find(|insn| {
                                    match_fetch_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                                })?;
            let reg2 = fetch_vsp_instruction_2.op0_register().full_register();

            let instruction_size = fetch_vsp_instruction_1.memory_size().size();

            $specific_matcher (&mut instruction_iter, reg1, reg2)?;

            instruction_iter.find(|insn| match_pushfq(insn))?;

            Some(instruction_size)
        }
    };
}

macro_rules! generate_binop_match_byte {
    ($matcher_name:ident, $specific_matcher:expr) => {

        fn $matcher_name(vm_handler: &VmHandler,
                        reg_allocation: &VmRegisterAllocation)
                        -> Option<usize> {
            let mut instruction_iter = vm_handler.instructions.iter();

            let fetch_vsp_instruction_1 =
                instruction_iter.find(|insn| {
                                    match_fetch_zx_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                                })?;
            let reg1 = fetch_vsp_instruction_1.op0_register().full_register();

            let fetch_vsp_instruction_2 =
                instruction_iter.find(|insn| {
                                    match_fetch_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                                })?;
            let reg2 = fetch_vsp_instruction_2.op0_register().full_register();

            let instruction_size = fetch_vsp_instruction_1.memory_size().size();

            $specific_matcher (&mut instruction_iter, reg1, reg2)?;

            instruction_iter.find(|insn| match_pushfq(insn))?;

            Some(instruction_size)
        }
    };
}

fn sub_match_add<'a, I>(instruction_iter: &mut I,
                        reg1: Register,
                        reg2: Register)
                        -> Option<&'a Instruction>
    where I: Iterator<Item = &'a Instruction>
{
    instruction_iter.find(|insn| match_add_reg_reg(insn, reg1, reg2))
}
generate_binop_match!(vm_match_add, sub_match_add);
generate_binop_match_byte!(vm_match_add_byte, sub_match_add);

fn sub_match_mul<'a, I>(instruction_iter: &mut I,
                        reg1: Register,
                        reg2: Register)
                        -> Option<&'a Instruction>
    where I: Iterator<Item = &'a Instruction>
{
    instruction_iter.find(|insn| match_mul_reg_reg(insn, reg1, reg2))
}

fn sub_match_imul<'a, I>(instruction_iter: &mut I,
                         reg1: Register,
                         reg2: Register)
                         -> Option<&'a Instruction>
    where I: Iterator<Item = &'a Instruction>
{
    instruction_iter.find(|insn| match_imul_reg_reg(insn, reg1, reg2))
}

generate_binop_match!(vm_match_mul, sub_match_mul);
generate_binop_match!(vm_match_imul, sub_match_imul);

fn sub_match_nand<'a, I>(instruction_iter: &mut I,
                         reg1: Register,
                         reg2: Register)
                         -> Option<&'a Instruction>
    where I: Iterator<Item = &'a Instruction>
{
    instruction_iter.find(|insn| match_not_reg(insn, reg1))?;
    instruction_iter.find(|insn| match_not_reg(insn, reg2))?;

    instruction_iter.find(|insn| match_or_reg_reg(insn, reg1, reg2))
}

generate_binop_match!(vm_match_nand, sub_match_nand);
generate_binop_match_byte!(vm_match_nand_byte, sub_match_nand);

fn sub_match_nor<'a, I>(instruction_iter: &mut I,
                        reg1: Register,
                        reg2: Register)
                        -> Option<&'a Instruction>
    where I: Iterator<Item = &'a Instruction>
{
    instruction_iter.find(|insn| match_not_reg(insn, reg1))?;
    instruction_iter.find(|insn| match_not_reg(insn, reg2))?;

    instruction_iter.find(|insn| match_and_reg_reg(insn, reg1, reg2))
}

generate_binop_match!(vm_match_nor, sub_match_nor);
generate_binop_match_byte!(vm_match_nor_byte, sub_match_nor);

macro_rules! generate_binop_match_single_reg {
    ($matcher_name:ident, $specific_matcher:expr) => {

        fn $matcher_name(vm_handler: &VmHandler,
                        reg_allocation: &VmRegisterAllocation)
                        -> Option<usize> {
            let mut instruction_iter = vm_handler.instructions.iter();

            let fetch_vsp_instruction_1 =
                instruction_iter.find(|insn| {
                                    match_fetch_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                                })?;
            let reg = fetch_vsp_instruction_1.op0_register();

            let _fetch_vsp_instruction_2 =
                instruction_iter.find(|insn| {
                                    match_fetch_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                                })?;

            let instruction_size = fetch_vsp_instruction_1.memory_size().size();

            $specific_matcher (&mut instruction_iter, reg)?;

            instruction_iter.find(|insn| match_pushfq(insn))?;

            Some(instruction_size)
        }
    };
}

macro_rules! generate_binop_match_byte_single_reg {
    ($matcher_name:ident, $specific_matcher:expr) => {

        fn $matcher_name(vm_handler: &VmHandler,
                        reg_allocation: &VmRegisterAllocation)
                        -> Option<usize> {
            let mut instruction_iter = vm_handler.instructions.iter();

            let fetch_vsp_instruction_1 =
                instruction_iter.find(|insn| {
                                    match_fetch_zx_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                                })?;
            let reg = fetch_vsp_instruction_1.op0_register().full_register();

            let _fetch_vsp_instruction_2 =
                instruction_iter.find(|insn| {
                                    match_fetch_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                                })?;

            let instruction_size = fetch_vsp_instruction_1.memory_size().size();

            $specific_matcher (&mut instruction_iter, reg.full_register())?;

            instruction_iter.find(|insn| match_pushfq(insn))?;

            Some(instruction_size)
        }
    };
}

fn sub_match_shr<'a, I>(instruction_iter: &mut I,
                        reg: Register)
                        -> Option<&'a Instruction>
    where I: Iterator<Item = &'a Instruction>
{
    instruction_iter.find(|insn| match_shr_reg_reg(insn, reg))
}

generate_binop_match_single_reg!(vm_match_shr, sub_match_shr);
generate_binop_match_byte_single_reg!(vm_match_shr_byte, sub_match_shr);

fn sub_match_shl<'a, I>(instruction_iter: &mut I,
                        reg: Register)
                        -> Option<&'a Instruction>
    where I: Iterator<Item = &'a Instruction>
{
    instruction_iter.find(|insn| match_shl_reg_reg(insn, reg))
}

generate_binop_match_single_reg!(vm_match_shl, sub_match_shl);
generate_binop_match_byte_single_reg!(vm_match_shl_byte, sub_match_shl);

fn sub_match_shrd<'a, I>(instruction_iter: &mut I,
                         reg: Register)
                         -> Option<&'a Instruction>
    where I: Iterator<Item = &'a Instruction>
{
    instruction_iter.find(|insn| match_shrd_reg_reg(insn, reg))
}

generate_binop_match_single_reg!(vm_match_shrd, sub_match_shrd);

fn sub_match_shld<'a, I>(instruction_iter: &mut I,
                         reg: Register)
                         -> Option<&'a Instruction>
    where I: Iterator<Item = &'a Instruction>
{
    instruction_iter.find(|insn| match_shld_reg_reg(insn, reg))
}

generate_binop_match_single_reg!(vm_match_shld, sub_match_shld);

fn vm_match_fetch(vm_handler: &VmHandler,
                  reg_allocation: &VmRegisterAllocation)
                  -> Option<usize> {
    let mut instruction_iter = vm_handler.instructions.iter();
    let fetch_vsp_instruction =
        instruction_iter.find(|insn| {
                            match_fetch_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                        })?;

    let fetch_register = fetch_vsp_instruction.op0_register();

    let mut fetch_size = 0;
    instruction_iter.find(|insn| {
                        if let Some(size) = match_fetch_reg_any_size(insn, fetch_register) {
                            fetch_size = size;
                            true
                        } else {
                            false
                        }
                    })?;

    Some(fetch_size)
}

fn vm_match_fetch_byte(vm_handler: &VmHandler,
                       reg_allocation: &VmRegisterAllocation)
                       -> Option<usize> {
    let mut instruction_iter = vm_handler.instructions.iter();
    let fetch_vsp_instruction =
        instruction_iter.find(|insn| {
                            match_fetch_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                        })?;

    let fetch_register = fetch_vsp_instruction.op0_register().full_register();

    let mut fetch_size = 0;
    instruction_iter.find(|insn| {
                        if let Some(size) = match_fetch_zx_reg_any_size(insn, fetch_register) {
                            fetch_size = size;
                            true
                        } else {
                            false
                        }
                    })?;

    Some(fetch_size)
}

fn vm_match_store(vm_handler: &VmHandler,
                  reg_allocation: &VmRegisterAllocation)
                  -> Option<usize> {
    let mut instruction_iter = vm_handler.instructions.iter();

    let fetch_vsp_instruction_1 =
        instruction_iter.find(|insn| {
                            match_fetch_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                        })?;

    let reg1 = fetch_vsp_instruction_1.op0_register().full_register();

    let fetch_vsp_instruction_2 =
        instruction_iter.find(|insn| {
                            match_fetch_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                        })?;

    let reg2 = fetch_vsp_instruction_2.op0_register().full_register();

    let mut instruction_size = 0;
    instruction_iter.find(|insn| match match_store_reg2_in_reg1(insn, reg1, reg2) {
                        Some(size) => {
                            instruction_size = size;
                            true
                        },
                        _ => false,
                    })?;
    Some(instruction_size)
}

fn vm_match_push_vsp(vm_handler: &VmHandler,
                     reg_allocation: &VmRegisterAllocation)
                     -> Option<usize> {
    let mut instruction_iter = vm_handler.instructions.iter();
    instruction_iter.find(|insn| match_mov_reg_source(insn, reg_allocation.vsp.into()))?;

    let sub_vsp_instruction =
        instruction_iter.find(|insn| match_sub_vsp_get_amount(insn, reg_allocation).is_some())?;
    let instruction_size = sub_vsp_instruction.immediate32();
    instruction_iter.find(|insn| {
                        match_store_reg_any_size(insn, reg_allocation.vsp.into()).is_some()
                    })?;

    Some(instruction_size as usize)
}

fn vm_match_vm_exit(vm_handler: &VmHandler,
                    reg_allocation: &VmRegisterAllocation)
                    -> bool {
    let instruction_iter = vm_handler.instructions.iter();

    if !instruction_iter.clone().any(match_ret) {
        return false;
    }

    if !instruction_iter.clone().any(match_popfq) {
        return false;
    }

    if !instruction_iter.clone().any(|insn| {
                                    insn.code() == Code::Mov_r64_rm64 &&
                                    insn.op0_register() == Register::RSP &&
                                    insn.op1_register() == reg_allocation.vsp.into()
                                })
    {
        return false;
    }

    if instruction_iter.filter(|insn| insn.code() == Code::Pop_r64)
                       .count() !=
       15
    {
        return false;
    }

    true
}

fn vm_match_push_cr0(vm_handler: &VmHandler) -> bool {
    let first_instruction = vm_handler.instructions[0];
    (first_instruction.code() == Code::Mov_r64_cr) &&
    first_instruction.op1_register() == Register::CR0
}

fn vm_match_push_cr3(vm_handler: &VmHandler) -> bool {
    let first_instruction = vm_handler.instructions[0];
    (first_instruction.code() == Code::Mov_r64_cr) &&
    first_instruction.op1_register() == Register::CR3
}

fn vm_match_rdtsc(vm_handler: &VmHandler) -> bool {
    let first_instruction = vm_handler.instructions[0];
    first_instruction.code() == Code::Rdtsc
}

fn vm_match_nop(vm_handler: &VmHandler) -> bool {
    let first_instruction = vm_handler.instructions[0];
    first_instruction.code() == Code::Lea_r64_m
}

// fn match_lock_xchng(vm_handler: &VmHandler) -> bool {
//     let mut instruction_iter = vm_handler.instructions.iter();
//     instruction_iter.any(|insn| insn.code() == Code::Xchg_rm64_r64 && insn.has_lock_prefix())
// }

```

`test.ll`:

```ll
; ModuleID = 'devirt.bc'
source_filename = "VMProtectHelpers.cpp"
target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"
target triple = "x86_64-pc-linux-gnu"

%0 = type { %1 }
%1 = type { i64 }
%2 = type { i8, i8, i8, i8, i8, i8, i8, i8 }
%3 = type { %0, %0, %0, %0, %0, %0, %0, %0, %0, %0, %0, %0, %0, %0, %0, %0 }

$_Z8PUSH_IMMImEvRmT_ = comdat any

$_Z8PUSH_IMMIjEvRmT_ = comdat any

$_Z8PUSH_IMMItEvRmT_ = comdat any

$_Z8PUSH_VSPILm64EEvRm = comdat any

$_Z8PUSH_VSPILm32EEvRm = comdat any

$_Z8PUSH_VSPILm16EEvRm = comdat any

$_Z7POP_VSPILm64EEvRm = comdat any

$_Z7POP_VSPILm32EEvRm = comdat any

$_Z7POP_VSPILm16EEvRm = comdat any

$_Z4LOADImEvRm = comdat any

$_Z4LOADIjEvRm = comdat any

$_Z4LOADItEvRm = comdat any

$_Z4LOADIhEvRm = comdat any

$_Z7LOAD_GSImEvRm = comdat any

$_Z7LOAD_GSIjEvRm = comdat any

$_Z7LOAD_GSItEvRm = comdat any

$_Z7LOAD_GSIhEvRm = comdat any

$_Z7LOAD_FSImEvRm = comdat any

$_Z7LOAD_FSIjEvRm = comdat any

$_Z7LOAD_FSItEvRm = comdat any

$_Z7LOAD_FSIhEvRm = comdat any

$_Z5STOREImEvRm = comdat any

$_Z5STOREIjEvRm = comdat any

$_Z5STOREItEvRm = comdat any

$_Z5STOREIhEvRm = comdat any

$_Z10PUSH_VMREGILm8ELm0EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm8ELm1EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm16ELm0EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm16ELm1EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm16ELm2EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm16ELm3EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm32ELm0EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm32ELm1EEvRm15VirtualRegister = comdat any

$_Z10PUSH_VMREGILm64ELm0EEvRm15VirtualRegister = comdat any

$_Z9POP_VMREGILm8ELm0EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm8ELm1EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm16ELm0EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm16ELm1EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm16ELm2EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm16ELm3EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm32ELm0EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm32ELm1EEvRmR15VirtualRegister = comdat any

$_Z9POP_VMREGILm64ELm0EEvRmR15VirtualRegister = comdat any

$_Z3ADDImEvRm = comdat any

$_Z3ADDIjEvRm = comdat any

$_Z3ADDItEvRm = comdat any

$_Z3ADDIhEvRm = comdat any

$_Z3DIVImEvRm = comdat any

$_Z3DIVIjEvRm = comdat any

$_Z3DIVItEvRm = comdat any

$_Z3DIVIhEvRm = comdat any

$_Z4IDIVImEvRm = comdat any

$_Z4IDIVIjEvRm = comdat any

$_Z4IDIVItEvRm = comdat any

$_Z4IDIVIhEvRm = comdat any

$_Z3MULImEvRm = comdat any

$_Z3MULIjEvRm = comdat any

$_Z3MULItEvRm = comdat any

$_Z3MULIhEvRm = comdat any

$_Z4IMULImEvRm = comdat any

$_Z4IMULIjEvRm = comdat any

$_Z4IMULItEvRm = comdat any

$_Z4IMULIhEvRm = comdat any

$_Z3NORImEvRm = comdat any

$_Z3NORIjEvRm = comdat any

$_Z3NORItEvRm = comdat any

$_Z3NORIhEvRm = comdat any

$_Z4NANDImEvRm = comdat any

$_Z4NANDIjEvRm = comdat any

$_Z4NANDItEvRm = comdat any

$_Z4NANDIhEvRm = comdat any

$_Z3SHLImEvRm = comdat any

$_Z3SHLIjEvRm = comdat any

$_Z3SHLItEvRm = comdat any

$_Z3SHLIhEvRm = comdat any

$_Z3SHRImEvRm = comdat any

$_Z3SHRIjEvRm = comdat any

$_Z3SHRItEvRm = comdat any

$_Z3SHRIhEvRm = comdat any

$_Z4SHLDImEvRm = comdat any

$_Z4SHLDIjEvRm = comdat any

$_Z4SHLDItEvRm = comdat any

$_Z4SHLDIhEvRm = comdat any

$_Z4SHRDImEvRm = comdat any

$_Z4SHRDIjEvRm = comdat any

$_Z4SHRDItEvRm = comdat any

$_Z4SHRDIhEvRm = comdat any

@SEM_PUSH_IMM_64 = dso_local constant void (i64*, i64)* @_Z8PUSH_IMMImEvRmT_, align 8
@SEM_PUSH_IMM_32 = dso_local constant void (i64*, i32)* @_Z8PUSH_IMMIjEvRmT_, align 8
@SEM_PUSH_IMM_16 = dso_local constant void (i64*, i16)* @_Z8PUSH_IMMItEvRmT_, align 8
@SEM_PUSH_VSP_64 = dso_local constant void (i64*)* @_Z8PUSH_VSPILm64EEvRm, align 8
@SEM_PUSH_VSP_32 = dso_local constant void (i64*)* @_Z8PUSH_VSPILm32EEvRm, align 8
@SEM_PUSH_VSP_16 = dso_local constant void (i64*)* @_Z8PUSH_VSPILm16EEvRm, align 8
@SEM_POP_VSP_64 = dso_local constant void (i64*)* @_Z7POP_VSPILm64EEvRm, align 8
@SEM_POP_VSP_32 = dso_local constant void (i64*)* @_Z7POP_VSPILm32EEvRm, align 8
@SEM_POP_VSP_16 = dso_local constant void (i64*)* @_Z7POP_VSPILm16EEvRm, align 8
@SEM_POP_FLAGS = dso_local constant void (i64*, i64*)* @_Z9POP_FLAGSRmS_, align 8
@SEM_LOAD_SS_64 = dso_local constant void (i64*)* @_Z4LOADImEvRm, align 8
@SEM_LOAD_SS_32 = dso_local constant void (i64*)* @_Z4LOADIjEvRm, align 8
@SEM_LOAD_SS_16 = dso_local constant void (i64*)* @_Z4LOADItEvRm, align 8
@SEM_LOAD_SS_8 = dso_local constant void (i64*)* @_Z4LOADIhEvRm, align 8
@SEM_LOAD_DS_64 = dso_local constant void (i64*)* @_Z4LOADImEvRm, align 8
@SEM_LOAD_DS_32 = dso_local constant void (i64*)* @_Z4LOADIjEvRm, align 8
@SEM_LOAD_DS_16 = dso_local constant void (i64*)* @_Z4LOADItEvRm, align 8
@SEM_LOAD_DS_8 = dso_local constant void (i64*)* @_Z4LOADIhEvRm, align 8
@SEM_LOAD_GS_64 = dso_local constant void (i64*)* @_Z7LOAD_GSImEvRm, align 8
@SEM_LOAD_GS_32 = dso_local constant void (i64*)* @_Z7LOAD_GSIjEvRm, align 8
@SEM_LOAD_GS_16 = dso_local constant void (i64*)* @_Z7LOAD_GSItEvRm, align 8
@SEM_LOAD_GS_8 = dso_local constant void (i64*)* @_Z7LOAD_GSIhEvRm, align 8
@SEM_LOAD_FS_64 = dso_local constant void (i64*)* @_Z7LOAD_FSImEvRm, align 8
@SEM_LOAD_FS_32 = dso_local constant void (i64*)* @_Z7LOAD_FSIjEvRm, align 8
@SEM_LOAD_FS_16 = dso_local constant void (i64*)* @_Z7LOAD_FSItEvRm, align 8
@SEM_LOAD_FS_8 = dso_local constant void (i64*)* @_Z7LOAD_FSIhEvRm, align 8
@SEM_STORE_SS_64 = dso_local constant void (i64*)* @_Z5STOREImEvRm, align 8
@SEM_STORE_SS_32 = dso_local constant void (i64*)* @_Z5STOREIjEvRm, align 8
@SEM_STORE_SS_16 = dso_local constant void (i64*)* @_Z5STOREItEvRm, align 8
@SEM_STORE_SS_8 = dso_local constant void (i64*)* @_Z5STOREIhEvRm, align 8
@SEM_STORE_DS_64 = dso_local constant void (i64*)* @_Z5STOREImEvRm, align 8
@SEM_STORE_DS_32 = dso_local constant void (i64*)* @_Z5STOREIjEvRm, align 8
@SEM_STORE_DS_16 = dso_local constant void (i64*)* @_Z5STOREItEvRm, align 8
@SEM_STORE_DS_8 = dso_local constant void (i64*)* @_Z5STOREIhEvRm, align 8
@SEM_PUSH_VMREG_8_LOW = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm8ELm0EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_8_HIGH = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm8ELm1EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_16_LOWLOW = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm16ELm0EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_16_LOWHIGH = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm16ELm1EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_16_HIGHLOW = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm16ELm2EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_16_HIGHHIGH = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm16ELm3EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_32_LOW = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm32ELm0EEvRm15VirtualRegister, align 8
@SEM_UNDEF_PUSH_VMREG_32 = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm32ELm0EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_32_HIGH = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm32ELm1EEvRm15VirtualRegister, align 8
@SEM_PUSH_VMREG_64 = dso_local constant void (i64*, i64)* @_Z10PUSH_VMREGILm64ELm0EEvRm15VirtualRegister, align 8
@SEM_POP_VMREG_8_LOW = dso_local constant void (i64*, %0*)* @_Z9POP_VMREGILm8ELm0EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_8_HIGH = dso_local constant void (i64*, %0*)* @_Z9POP_VMREGILm8ELm1EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_16_LOWLOW = dso_local constant void (i64*, %0*)* @_Z9POP_VMREGILm16ELm0EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_16_LOWHIGH = dso_local constant void (i64*, %0*)* @_Z9POP_VMREGILm16ELm1EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_16_HIGHLOW = dso_local constant void (i64*, %0*)* @_Z9POP_VMREGILm16ELm2EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_16_HIGHHIGH = dso_local constant void (i64*, %0*)* @_Z9POP_VMREGILm16ELm3EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_32_LOW = dso_local constant void (i64*, %0*)* @_Z9POP_VMREGILm32ELm0EEvRmR15VirtualRegister, align 8
@SEM_UNDEF_POP_VMREG_32 = dso_local constant void (i64*, %0*)* @_Z9POP_VMREGILm32ELm0EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_32_HIGH = dso_local constant void (i64*, %0*)* @_Z9POP_VMREGILm32ELm1EEvRmR15VirtualRegister, align 8
@SEM_POP_VMREG_64 = dso_local constant void (i64*, %0*)* @_Z9POP_VMREGILm64ELm0EEvRmR15VirtualRegister, align 8
@SEM_PUSH_REG_64 = dso_local constant void (i64*, i64)* @_Z8PUSH_REGRmm, align 8
@SEM_UNDEF_PUSH_REG_32 = dso_local constant void (i64*, i64)* @_Z8PUSH_REGRmm, align 8
@SEM_POP_REG_64 = dso_local constant void (i64*, i64*)* @_Z7POP_REGRmS_, align 8
@SEM_UNDEF_POP_REG_32 = dso_local constant void (i64*, i64*)* @_Z7POP_REGRmS_, align 8
@SEM_POP_VOID_64 = dso_local constant void (i64*)* @_Z8POP_VOIDRm, align 8
@SEM_UNDEF_POP_VOID_32 = dso_local constant void (i64*)* @_Z8POP_VOIDRm, align 8
@SEM_MOVE_VMREG_SLOT = dso_local constant void (%0*, i64*)* @_Z15MOVE_VMREG_SLOTR15VirtualRegisterRm, align 8
@SEM_UNDEF_MOVE_VMREG_SLOT = dso_local constant void (%0*, i64*)* @_Z15MOVE_VMREG_SLOTR15VirtualRegisterRm, align 8
@SEM_POP_SLOT = dso_local constant void (i64*, i64*)* @_Z8POP_SLOTRmS_, align 8
@SEM_UNDEF_POP_SLOT = dso_local constant void (i64*, i64*)* @_Z8POP_SLOTRmS_, align 8
@SEM_CPUID = dso_local constant void (i64*)* @_Z5CPUIDRm, align 8
@SEM_RDTSC = dso_local constant void (i64*)* @_Z5RDTSCRm, align 8
@SEM_ADD_64 = dso_local constant void (i64*)* @_Z3ADDImEvRm, align 8
@SEM_ADD_32 = dso_local constant void (i64*)* @_Z3ADDIjEvRm, align 8
@SEM_ADD_16 = dso_local constant void (i64*)* @_Z3ADDItEvRm, align 8
@SEM_ADD_8 = dso_local constant void (i64*)* @_Z3ADDIhEvRm, align 8
@SEM_DIV_64 = dso_local constant void (i64*)* @_Z3DIVImEvRm, align 8
@SEM_DIV_32 = dso_local constant void (i64*)* @_Z3DIVIjEvRm, align 8
@SEM_DIV_16 = dso_local constant void (i64*)* @_Z3DIVItEvRm, align 8
@SEM_DIV_8 = dso_local constant void (i64*)* @_Z3DIVIhEvRm, align 8
@SEM_IDIV_64 = dso_local constant void (i64*)* @_Z4IDIVImEvRm, align 8
@SEM_IDIV_32 = dso_local constant void (i64*)* @_Z4IDIVIjEvRm, align 8
@SEM_IDIV_16 = dso_local constant void (i64*)* @_Z4IDIVItEvRm, align 8
@SEM_IDIV_8 = dso_local constant void (i64*)* @_Z4IDIVIhEvRm, align 8
@SEM_MUL_64 = dso_local constant void (i64*)* @_Z3MULImEvRm, align 8
@SEM_MUL_32 = dso_local constant void (i64*)* @_Z3MULIjEvRm, align 8
@SEM_MUL_16 = dso_local constant void (i64*)* @_Z3MULItEvRm, align 8
@SEM_MUL_8 = dso_local constant void (i64*)* @_Z3MULIhEvRm, align 8
@SEM_IMUL_64 = dso_local constant void (i64*)* @_Z4IMULImEvRm, align 8
@SEM_IMUL_32 = dso_local constant void (i64*)* @_Z4IMULIjEvRm, align 8
@SEM_IMUL_16 = dso_local constant void (i64*)* @_Z4IMULItEvRm, align 8
@SEM_IMUL_8 = dso_local constant void (i64*)* @_Z4IMULIhEvRm, align 8
@SEM_NOR_64 = dso_local constant void (i64*)* @_Z3NORImEvRm, align 8
@SEM_NOR_32 = dso_local constant void (i64*)* @_Z3NORIjEvRm, align 8
@SEM_NOR_16 = dso_local constant void (i64*)* @_Z3NORItEvRm, align 8
@SEM_NOR_8 = dso_local constant void (i64*)* @_Z3NORIhEvRm, align 8
@SEM_NAND_64 = dso_local constant void (i64*)* @_Z4NANDImEvRm, align 8
@SEM_NAND_32 = dso_local constant void (i64*)* @_Z4NANDIjEvRm, align 8
@SEM_NAND_16 = dso_local constant void (i64*)* @_Z4NANDItEvRm, align 8
@SEM_NAND_8 = dso_local constant void (i64*)* @_Z4NANDIhEvRm, align 8
@SEM_SHL_64 = dso_local constant void (i64*)* @_Z3SHLImEvRm, align 8
@SEM_SHL_32 = dso_local constant void (i64*)* @_Z3SHLIjEvRm, align 8
@SEM_SHL_16 = dso_local constant void (i64*)* @_Z3SHLItEvRm, align 8
@SEM_SHL_8 = dso_local constant void (i64*)* @_Z3SHLIhEvRm, align 8
@SEM_SHR_64 = dso_local constant void (i64*)* @_Z3SHRImEvRm, align 8
@SEM_SHR_32 = dso_local constant void (i64*)* @_Z3SHRIjEvRm, align 8
@SEM_SHR_16 = dso_local constant void (i64*)* @_Z3SHRItEvRm, align 8
@SEM_SHR_8 = dso_local constant void (i64*)* @_Z3SHRIhEvRm, align 8
@SEM_SHLD_64 = dso_local constant void (i64*)* @_Z4SHLDImEvRm, align 8
@SEM_SHLD_32 = dso_local constant void (i64*)* @_Z4SHLDIjEvRm, align 8
@SEM_SHLD_16 = dso_local constant void (i64*)* @_Z4SHLDItEvRm, align 8
@SEM_SHLD_8 = dso_local constant void (i64*)* @_Z4SHLDIhEvRm, align 8
@SEM_SHRD_64 = dso_local constant void (i64*)* @_Z4SHRDImEvRm, align 8
@SEM_SHRD_32 = dso_local constant void (i64*)* @_Z4SHRDIjEvRm, align 8
@SEM_SHRD_16 = dso_local constant void (i64*)* @_Z4SHRDItEvRm, align 8
@SEM_SHRD_8 = dso_local constant void (i64*)* @_Z4SHRDIhEvRm, align 8
@SEM_JUMP_INC = dso_local constant void (i64*, i64*)* @_Z8JUMP_INCRmS_, align 8
@SEM_JUMP_DEC = dso_local constant void (i64*, i64*)* @_Z8JUMP_DECRmS_, align 8
@SEM_JUMP = dso_local constant void (i64*, i64*)* @_Z4JUMPRmS_, align 8
@SEM_EXIT = dso_local constant void (i64*, i64*)* @_Z4JUMPRmS_, align 8
@GS = external dso_local local_unnamed_addr global [0 x i8], align 1
@FS = external dso_local local_unnamed_addr global [0 x i8], align 1
@__undef = dso_local local_unnamed_addr constant i64 undef, align 8
@llvm.compiler.used = appending global [114 x i8*] [i8* bitcast (void (i64*)** @SEM_ADD_16 to i8*), i8* bitcast (void (i64*)** @SEM_ADD_32 to i8*), i8* bitcast (void (i64*)** @SEM_ADD_64 to i8*), i8* bitcast (void (i64*)** @SEM_ADD_8 to i8*), i8* bitcast (void (i64*)** @SEM_CPUID to i8*), i8* bitcast (void (i64*)** @SEM_DIV_16 to i8*), i8* bitcast (void (i64*)** @SEM_DIV_32 to i8*), i8* bitcast (void (i64*)** @SEM_DIV_64 to i8*), i8* bitcast (void (i64*)** @SEM_DIV_8 to i8*), i8* bitcast (void (i64*, i64*)** @SEM_EXIT to i8*), i8* bitcast (void (i64*)** @SEM_IDIV_16 to i8*), i8* bitcast (void (i64*)** @SEM_IDIV_32 to i8*), i8* bitcast (void (i64*)** @SEM_IDIV_64 to i8*), i8* bitcast (void (i64*)** @SEM_IDIV_8 to i8*), i8* bitcast (void (i64*)** @SEM_IMUL_16 to i8*), i8* bitcast (void (i64*)** @SEM_IMUL_32 to i8*), i8* bitcast (void (i64*)** @SEM_IMUL_64 to i8*), i8* bitcast (void (i64*)** @SEM_IMUL_8 to i8*), i8* bitcast (void (i64*, i64*)** @SEM_JUMP to i8*), i8* bitcast (void (i64*, i64*)** @SEM_JUMP_DEC to i8*), i8* bitcast (void (i64*, i64*)** @SEM_JUMP_INC to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_DS_16 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_DS_32 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_DS_64 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_DS_8 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_FS_16 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_FS_32 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_FS_64 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_FS_8 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_GS_16 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_GS_32 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_GS_64 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_GS_8 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_SS_16 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_SS_32 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_SS_64 to i8*), i8* bitcast (void (i64*)** @SEM_LOAD_SS_8 to i8*), i8* bitcast (void (%0*, i64*)** @SEM_MOVE_VMREG_SLOT to i8*), i8* bitcast (void (i64*)** @SEM_MUL_16 to i8*), i8* bitcast (void (i64*)** @SEM_MUL_32 to i8*), i8* bitcast (void (i64*)** @SEM_MUL_64 to i8*), i8* bitcast (void (i64*)** @SEM_MUL_8 to i8*), i8* bitcast (void (i64*)** @SEM_NAND_16 to i8*), i8* bitcast (void (i64*)** @SEM_NAND_32 to i8*), i8* bitcast (void (i64*)** @SEM_NAND_64 to i8*), i8* bitcast (void (i64*)** @SEM_NAND_8 to i8*), i8* bitcast (void (i64*)** @SEM_NOR_16 to i8*), i8* bitcast (void (i64*)** @SEM_NOR_32 to i8*), i8* bitcast (void (i64*)** @SEM_NOR_64 to i8*), i8* bitcast (void (i64*)** @SEM_NOR_8 to i8*), i8* bitcast (void (i64*, i64*)** @SEM_POP_FLAGS to i8*), i8* bitcast (void (i64*, i64*)** @SEM_POP_REG_64 to i8*), i8* bitcast (void (i64*, i64*)** @SEM_POP_SLOT to i8*), i8* bitcast (void (i64*, %0*)** @SEM_POP_VMREG_16_HIGHHIGH to i8*), i8* bitcast (void (i64*, %0*)** @SEM_POP_VMREG_16_HIGHLOW to i8*), i8* bitcast (void (i64*, %0*)** @SEM_POP_VMREG_16_LOWHIGH to i8*), i8* bitcast (void (i64*, %0*)** @SEM_POP_VMREG_16_LOWLOW to i8*), i8* bitcast (void (i64*, %0*)** @SEM_POP_VMREG_32_HIGH to i8*), i8* bitcast (void (i64*, %0*)** @SEM_POP_VMREG_32_LOW to i8*), i8* bitcast (void (i64*, %0*)** @SEM_POP_VMREG_64 to i8*), i8* bitcast (void (i64*, %0*)** @SEM_POP_VMREG_8_HIGH to i8*), i8* bitcast (void (i64*, %0*)** @SEM_POP_VMREG_8_LOW to i8*), i8* bitcast (void (i64*)** @SEM_POP_VOID_64 to i8*), i8* bitcast (void (i64*)** @SEM_POP_VSP_16 to i8*), i8* bitcast (void (i64*)** @SEM_POP_VSP_32 to i8*), i8* bitcast (void (i64*)** @SEM_POP_VSP_64 to i8*), i8* bitcast (void (i64*, i16)** @SEM_PUSH_IMM_16 to i8*), i8* bitcast (void (i64*, i32)** @SEM_PUSH_IMM_32 to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_IMM_64 to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_REG_64 to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_16_HIGHHIGH to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_16_HIGHLOW to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_16_LOWHIGH to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_16_LOWLOW to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_32_HIGH to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_32_LOW to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_64 to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_8_HIGH to i8*), i8* bitcast (void (i64*, i64)** @SEM_PUSH_VMREG_8_LOW to i8*), i8* bitcast (void (i64*)** @SEM_PUSH_VSP_16 to i8*), i8* bitcast (void (i64*)** @SEM_PUSH_VSP_32 to i8*), i8* bitcast (void (i64*)** @SEM_PUSH_VSP_64 to i8*), i8* bitcast (void (i64*)** @SEM_RDTSC to i8*), i8* bitcast (void (i64*)** @SEM_SHLD_16 to i8*), i8* bitcast (void (i64*)** @SEM_SHLD_32 to i8*), i8* bitcast (void (i64*)** @SEM_SHLD_64 to i8*), i8* bitcast (void (i64*)** @SEM_SHLD_8 to i8*), i8* bitcast (void (i64*)** @SEM_SHL_16 to i8*), i8* bitcast (void (i64*)** @SEM_SHL_32 to i8*), i8* bitcast (void (i64*)** @SEM_SHL_64 to i8*), i8* bitcast (void (i64*)** @SEM_SHL_8 to i8*), i8* bitcast (void (i64*)** @SEM_SHRD_16 to i8*), i8* bitcast (void (i64*)** @SEM_SHRD_32 to i8*), i8* bitcast (void (i64*)** @SEM_SHRD_64 to i8*), i8* bitcast (void (i64*)** @SEM_SHRD_8 to i8*), i8* bitcast (void (i64*)** @SEM_SHR_16 to i8*), i8* bitcast (void (i64*)** @SEM_SHR_32 to i8*), i8* bitcast (void (i64*)** @SEM_SHR_64 to i8*), i8* bitcast (void (i64*)** @SEM_SHR_8 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_DS_16 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_DS_32 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_DS_64 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_DS_8 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_SS_16 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_SS_32 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_SS_64 to i8*), i8* bitcast (void (i64*)** @SEM_STORE_SS_8 to i8*), i8* bitcast (void (%0*, i64*)** @SEM_UNDEF_MOVE_VMREG_SLOT to i8*), i8* bitcast (void (i64*, i64*)** @SEM_UNDEF_POP_REG_32 to i8*), i8* bitcast (void (i64*, i64*)** @SEM_UNDEF_POP_SLOT to i8*), i8* bitcast (void (i64*, %0*)** @SEM_UNDEF_POP_VMREG_32 to i8*), i8* bitcast (void (i64*)** @SEM_UNDEF_POP_VOID_32 to i8*), i8* bitcast (void (i64*, i64)** @SEM_UNDEF_PUSH_REG_32 to i8*), i8* bitcast (void (i64*, i64)** @SEM_UNDEF_PUSH_VMREG_32 to i8*)], section "llvm.metadata"

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z8PUSH_IMMImEvRmT_(i64* nonnull align 8 dereferenceable(8) %0, i64 %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = add i64 %3, -8
  store i64 %4, i64* %0, align 8, !tbaa !3
  %5 = inttoptr i64 %4 to i64*
  store i64 %1, i64* %5, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z8PUSH_IMMIjEvRmT_(i64* nonnull align 8 dereferenceable(8) %0, i32 %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = add i64 %3, -4
  store i64 %4, i64* %0, align 8, !tbaa !3
  %5 = inttoptr i64 %4 to i32*
  store i32 %1, i32* %5, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z8PUSH_IMMItEvRmT_(i64* nonnull align 8 dereferenceable(8) %0, i16 zeroext %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = add i64 %3, -2
  store i64 %4, i64* %0, align 8, !tbaa !3
  %5 = inttoptr i64 %4 to i16*
  store i16 %1, i16* %5, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z8PUSH_VSPILm64EEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = add i64 %2, -8
  store i64 %3, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i64*
  store i64 %2, i64* %4, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z8PUSH_VSPILm32EEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = trunc i64 %2 to i32
  %4 = add i64 %2, -4
  store i64 %4, i64* %0, align 8, !tbaa !3
  %5 = inttoptr i64 %4 to i32*
  store i32 %3, i32* %5, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z8PUSH_VSPILm16EEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = trunc i64 %2 to i16
  %4 = add i64 %2, -2
  store i64 %4, i64* %0, align 8, !tbaa !3
  %5 = inttoptr i64 %4 to i16*
  store i16 %3, i16* %5, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7POP_VSPILm64EEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  store i64 %4, i64* %0, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7POP_VSPILm32EEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i32*
  %4 = load i32, i32* %3, align 1
  %5 = add i64 %2, 4
  %6 = and i64 %5, -4294967296
  %7 = zext i32 %4 to i64
  %8 = or i64 %6, %7
  store i64 %8, i64* %0, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7POP_VSPILm16EEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  %6 = and i64 %5, -65536
  %7 = zext i16 %4 to i64
  %8 = or i64 %6, %7
  store i64 %8, i64* %0, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z9POP_FLAGSRmS_(i64* nocapture nonnull align 8 dereferenceable(8) %0, i64* nocapture nonnull align 8 dereferenceable(8) %1) #1 {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i64*
  %5 = load i64, i64* %4, align 1
  %6 = add i64 %3, 8
  store i64 %6, i64* %0, align 8, !tbaa !3
  store i64 %5, i64* %1, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4LOADImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %4 to i64*
  %7 = load i64, i64* %6, align 1
  store i64 %2, i64* %0, align 8, !tbaa !3
  store i64 %7, i64* %3, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4LOADIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %4 to i32*
  %7 = load i32, i32* %6, align 1
  %8 = add i64 %2, 4
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i32*
  store i32 %7, i32* %9, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4LOADItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %4 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = add i64 %2, 6
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i16*
  store i16 %7, i16* %9, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4LOADIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %4 to i8*
  %7 = load i8, i8* %6, align 1
  %8 = zext i8 %7 to i16
  %9 = add i64 %2, 6
  store i64 %9, i64* %0, align 8, !tbaa !3
  %10 = inttoptr i64 %9 to i16*
  store i16 %8, i16* %10, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_GSImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = getelementptr inbounds [0 x i8], [0 x i8]* @GS, i64 0, i64 %4
  %7 = bitcast i8* %6 to i64*
  %8 = load i64, i64* %7, align 1
  store i64 %2, i64* %0, align 8, !tbaa !3
  store i64 %8, i64* %3, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_GSIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = getelementptr inbounds [0 x i8], [0 x i8]* @GS, i64 0, i64 %4
  %7 = bitcast i8* %6 to i32*
  %8 = load i32, i32* %7, align 1
  %9 = add i64 %2, 4
  store i64 %9, i64* %0, align 8, !tbaa !3
  %10 = inttoptr i64 %9 to i32*
  store i32 %8, i32* %10, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_GSItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = getelementptr inbounds [0 x i8], [0 x i8]* @GS, i64 0, i64 %4
  %7 = bitcast i8* %6 to i16*
  %8 = load i16, i16* %7, align 1
  %9 = add i64 %2, 6
  store i64 %9, i64* %0, align 8, !tbaa !3
  %10 = inttoptr i64 %9 to i16*
  store i16 %8, i16* %10, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_GSIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = getelementptr inbounds [0 x i8], [0 x i8]* @GS, i64 0, i64 %4
  %7 = load i8, i8* %6, align 1
  %8 = zext i8 %7 to i16
  %9 = add i64 %2, 6
  store i64 %9, i64* %0, align 8, !tbaa !3
  %10 = inttoptr i64 %9 to i16*
  store i16 %8, i16* %10, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_FSImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = getelementptr inbounds [0 x i8], [0 x i8]* @FS, i64 0, i64 %4
  %7 = bitcast i8* %6 to i64*
  %8 = load i64, i64* %7, align 1
  store i64 %2, i64* %0, align 8, !tbaa !3
  store i64 %8, i64* %3, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_FSIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = getelementptr inbounds [0 x i8], [0 x i8]* @FS, i64 0, i64 %4
  %7 = bitcast i8* %6 to i32*
  %8 = load i32, i32* %7, align 1
  %9 = add i64 %2, 4
  store i64 %9, i64* %0, align 8, !tbaa !3
  %10 = inttoptr i64 %9 to i32*
  store i32 %8, i32* %10, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_FSItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = getelementptr inbounds [0 x i8], [0 x i8]* @FS, i64 0, i64 %4
  %7 = bitcast i8* %6 to i16*
  %8 = load i16, i16* %7, align 1
  %9 = add i64 %2, 6
  store i64 %9, i64* %0, align 8, !tbaa !3
  %10 = inttoptr i64 %9 to i16*
  store i16 %8, i16* %10, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z7LOAD_FSIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = getelementptr inbounds [0 x i8], [0 x i8]* @FS, i64 0, i64 %4
  %7 = load i8, i8* %6, align 1
  %8 = zext i8 %7 to i16
  %9 = add i64 %2, 6
  store i64 %9, i64* %0, align 8, !tbaa !3
  %10 = inttoptr i64 %9 to i16*
  store i16 %8, i16* %10, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z5STOREImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64, i64* %6, align 1
  %8 = add i64 %2, 16
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %4 to i64*
  store i64 %7, i64* %9, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z5STOREIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i32*
  %7 = load i32, i32* %6, align 1
  %8 = add i64 %2, 12
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %4 to i32*
  store i32 %7, i32* %9, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z5STOREItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = add i64 %2, 10
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %4 to i16*
  store i16 %7, i16* %9, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z5STOREIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = add i64 %2, 10
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = trunc i16 %7 to i8
  %10 = inttoptr i64 %4 to i8*
  store i8 %9, i8* %10, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm8ELm0EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, i64 %1) #0 comdat {
  %3 = trunc i64 %1 to i16
  %4 = load i64, i64* %0, align 8, !tbaa !3
  %5 = add i64 %4, -2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = and i16 %3, 255
  %7 = inttoptr i64 %5 to i16*
  store i16 %6, i16* %7, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm8ELm1EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, i64 %1) #0 comdat {
  %3 = lshr i64 %1, 8
  %4 = trunc i64 %3 to i16
  %5 = load i64, i64* %0, align 8, !tbaa !3
  %6 = add i64 %5, -2
  store i64 %6, i64* %0, align 8, !tbaa !3
  %7 = and i16 %4, 255
  %8 = inttoptr i64 %6 to i16*
  store i16 %7, i16* %8, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm16ELm0EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, i64 %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = add i64 %3, -2
  store i64 %4, i64* %0, align 8, !tbaa !3
  %5 = inttoptr i64 %4 to i16*
  %6 = trunc i64 %1 to i16
  store i16 %6, i16* %5, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm16ELm1EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, i64 %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = add i64 %3, -2
  store i64 %4, i64* %0, align 8, !tbaa !3
  %5 = inttoptr i64 %4 to i16*
  %6 = lshr i64 %1, 16
  %7 = trunc i64 %6 to i16
  store i16 %7, i16* %5, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm16ELm2EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, i64 %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = add i64 %3, -2
  store i64 %4, i64* %0, align 8, !tbaa !3
  %5 = inttoptr i64 %4 to i16*
  %6 = lshr i64 %1, 32
  %7 = trunc i64 %6 to i16
  store i16 %7, i16* %5, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm16ELm3EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, i64 %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = add i64 %3, -2
  store i64 %4, i64* %0, align 8, !tbaa !3
  %5 = inttoptr i64 %4 to i16*
  %6 = lshr i64 %1, 48
  %7 = trunc i64 %6 to i16
  store i16 %7, i16* %5, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm32ELm0EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, i64 %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = add i64 %3, -4
  store i64 %4, i64* %0, align 8, !tbaa !3
  %5 = inttoptr i64 %4 to i32*
  %6 = trunc i64 %1 to i32
  store i32 %6, i32* %5, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm32ELm1EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, i64 %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = add i64 %3, -4
  store i64 %4, i64* %0, align 8, !tbaa !3
  %5 = inttoptr i64 %4 to i32*
  %6 = lshr i64 %1, 32
  %7 = trunc i64 %6 to i32
  store i32 %7, i32* %5, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z10PUSH_VMREGILm64ELm0EEvRm15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, i64 %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = add i64 %3, -8
  store i64 %4, i64* %0, align 8, !tbaa !3
  %5 = inttoptr i64 %4 to i64*
  store i64 %1, i64* %5, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm8ELm0EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, %0* nonnull align 1 dereferenceable(8) %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i16*
  %5 = load i16, i16* %4, align 1
  %6 = trunc i16 %5 to i8
  %7 = bitcast %0* %1 to i8*
  store i8 %6, i8* %7, align 1, !tbaa !7
  %8 = load i64, i64* %0, align 8, !tbaa !3
  %9 = add i64 %8, 2
  store i64 %9, i64* %0, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm8ELm1EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, %0* nonnull align 1 dereferenceable(8) %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i16*
  %5 = load i16, i16* %4, align 1
  %6 = trunc i16 %5 to i8
  %7 = bitcast %0* %1 to %2*
  %8 = getelementptr inbounds %2, %2* %7, i64 0, i32 1
  store i8 %6, i8* %8, align 1, !tbaa !7
  %9 = load i64, i64* %0, align 8, !tbaa !3
  %10 = add i64 %9, 2
  store i64 %10, i64* %0, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm16ELm0EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, %0* nonnull align 1 dereferenceable(8) %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i16*
  %5 = load i16, i16* %4, align 1
  %6 = getelementptr inbounds %0, %0* %1, i64 0, i32 0, i32 0
  %7 = load i64, i64* %6, align 1, !tbaa !7
  %8 = and i64 %7, -65536
  %9 = zext i16 %5 to i64
  %10 = or i64 %8, %9
  store i64 %10, i64* %6, align 1, !tbaa !7
  %11 = load i64, i64* %0, align 8, !tbaa !3
  %12 = add i64 %11, 2
  store i64 %12, i64* %0, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm16ELm1EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, %0* nonnull align 1 dereferenceable(8) %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i16*
  %5 = load i16, i16* %4, align 1
  %6 = getelementptr inbounds %0, %0* %1, i64 0, i32 0, i32 0
  %7 = load i64, i64* %6, align 1, !tbaa !7
  %8 = and i64 %7, -4294901761
  %9 = zext i16 %5 to i64
  %10 = shl nuw nsw i64 %9, 16
  %11 = or i64 %8, %10
  store i64 %11, i64* %6, align 1, !tbaa !7
  %12 = load i64, i64* %0, align 8, !tbaa !3
  %13 = add i64 %12, 2
  store i64 %13, i64* %0, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm16ELm2EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, %0* nonnull align 1 dereferenceable(8) %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i16*
  %5 = load i16, i16* %4, align 1
  %6 = getelementptr inbounds %0, %0* %1, i64 0, i32 0, i32 0
  %7 = load i64, i64* %6, align 1, !tbaa !7
  %8 = and i64 %7, -281470681743361
  %9 = zext i16 %5 to i64
  %10 = shl nuw nsw i64 %9, 32
  %11 = or i64 %8, %10
  store i64 %11, i64* %6, align 1, !tbaa !7
  %12 = load i64, i64* %0, align 8, !tbaa !3
  %13 = add i64 %12, 2
  store i64 %13, i64* %0, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm16ELm3EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, %0* nonnull align 1 dereferenceable(8) %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i16*
  %5 = load i16, i16* %4, align 1
  %6 = getelementptr inbounds %0, %0* %1, i64 0, i32 0, i32 0
  %7 = load i64, i64* %6, align 1, !tbaa !7
  %8 = and i64 %7, 281474976710655
  %9 = zext i16 %5 to i64
  %10 = shl nuw i64 %9, 48
  %11 = or i64 %8, %10
  store i64 %11, i64* %6, align 1, !tbaa !7
  %12 = load i64, i64* %0, align 8, !tbaa !3
  %13 = add i64 %12, 2
  store i64 %13, i64* %0, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm32ELm0EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, %0* nonnull align 1 dereferenceable(8) %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i32*
  %5 = load i32, i32* %4, align 1
  %6 = getelementptr inbounds %0, %0* %1, i64 0, i32 0, i32 0
  %7 = load i64, i64* %6, align 1, !tbaa !7
  %8 = and i64 %7, -4294967296
  %9 = zext i32 %5 to i64
  %10 = or i64 %8, %9
  store i64 %10, i64* %6, align 1, !tbaa !7
  %11 = load i64, i64* %0, align 8, !tbaa !3
  %12 = add i64 %11, 4
  store i64 %12, i64* %0, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm32ELm1EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, %0* nonnull align 1 dereferenceable(8) %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i32*
  %5 = load i32, i32* %4, align 1
  %6 = getelementptr inbounds %0, %0* %1, i64 0, i32 0, i32 0
  %7 = load i64, i64* %6, align 1, !tbaa !7
  %8 = and i64 %7, 4294967295
  %9 = zext i32 %5 to i64
  %10 = shl nuw i64 %9, 32
  %11 = or i64 %8, %10
  store i64 %11, i64* %6, align 1, !tbaa !7
  %12 = load i64, i64* %0, align 8, !tbaa !3
  %13 = add i64 %12, 4
  store i64 %13, i64* %0, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z9POP_VMREGILm64ELm0EEvRmR15VirtualRegister(i64* nonnull align 8 dereferenceable(8) %0, %0* nonnull align 1 dereferenceable(8) %1) #0 comdat {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i64*
  %5 = load i64, i64* %4, align 1
  %6 = getelementptr inbounds %0, %0* %1, i64 0, i32 0, i32 0
  store i64 %5, i64* %6, align 1, !tbaa !7
  %7 = load i64, i64* %0, align 8, !tbaa !3
  %8 = add i64 %7, 8
  store i64 %8, i64* %0, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z8PUSH_REGRmm(i64* nocapture nonnull align 8 dereferenceable(8) %0, i64 %1) #1 {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = add i64 %3, -8
  store i64 %4, i64* %0, align 8, !tbaa !3
  %5 = inttoptr i64 %4 to i64*
  store i64 %1, i64* %5, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z7POP_REGRmS_(i64* nocapture nonnull align 8 dereferenceable(8) %0, i64* nocapture nonnull align 8 dereferenceable(8) %1) #1 {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i64*
  %5 = load i64, i64* %4, align 1
  %6 = add i64 %3, 8
  store i64 %6, i64* %0, align 8, !tbaa !3
  store i64 %5, i64* %1, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z8POP_VOIDRm(i64* nocapture nonnull align 8 dereferenceable(8) %0) #1 {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = add i64 %2, 8
  store i64 %3, i64* %0, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z15MOVE_VMREG_SLOTR15VirtualRegisterRm(%0* nocapture nonnull align 1 dereferenceable(8) %0, i64* nocapture nonnull readonly align 8 dereferenceable(8) %1) #1 {
  %3 = load i64, i64* %1, align 8, !tbaa !3
  %4 = getelementptr inbounds %0, %0* %0, i64 0, i32 0, i32 0
  store i64 %3, i64* %4, align 1, !tbaa !7
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z8POP_SLOTRmS_(i64* nocapture nonnull align 8 dereferenceable(8) %0, i64* nocapture nonnull align 8 dereferenceable(8) %1) #1 {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i64*
  %5 = load i64, i64* %4, align 1
  %6 = add i64 %3, 8
  store i64 %6, i64* %0, align 8, !tbaa !3
  store i64 %5, i64* %1, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define dso_local void @_Z5CPUIDRm(i64* nocapture nonnull align 8 dereferenceable(8) %0) #0 {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i32*
  %4 = load i32, i32* %3, align 1
  %5 = tail call { i32, i32, i32, i32 } asm "  xchgq  %rbx,${1:q}\0A  cpuid\0A  xchgq  %rbx,${1:q}", "={ax},=r,={cx},={dx},0,~{dirflag},~{fpsr},~{flags}"(i32 %4) #12, !srcloc !8
  %6 = extractvalue { i32, i32, i32, i32 } %5, 0
  %7 = extractvalue { i32, i32, i32, i32 } %5, 1
  %8 = extractvalue { i32, i32, i32, i32 } %5, 2
  %9 = extractvalue { i32, i32, i32, i32 } %5, 3
  store i32 %6, i32* %3, align 1
  %10 = load i64, i64* %0, align 8, !tbaa !3
  %11 = add i64 %10, -4
  store i64 %11, i64* %0, align 8, !tbaa !3
  %12 = inttoptr i64 %11 to i32*
  store i32 %7, i32* %12, align 1
  %13 = load i64, i64* %0, align 8, !tbaa !3
  %14 = add i64 %13, -4
  store i64 %14, i64* %0, align 8, !tbaa !3
  %15 = inttoptr i64 %14 to i32*
  store i32 %8, i32* %15, align 1
  %16 = load i64, i64* %0, align 8, !tbaa !3
  %17 = add i64 %16, -4
  store i64 %17, i64* %0, align 8, !tbaa !3
  %18 = inttoptr i64 %17 to i32*
  store i32 %9, i32* %18, align 1
  ret void
}

; Function Attrs: argmemonly nofree nosync nounwind willreturn
declare void @llvm.lifetime.start.p0i8(i64 immarg, i8* nocapture) #2

; Function Attrs: argmemonly nofree nosync nounwind willreturn
declare void @llvm.lifetime.end.p0i8(i64 immarg, i8* nocapture) #2

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define dso_local void @_Z5RDTSCRm(i64* nocapture nonnull align 8 dereferenceable(8) %0) #0 {
  %2 = tail call i64 @llvm.x86.rdtsc()
  %3 = trunc i64 %2 to i32
  %4 = lshr i64 %2, 32
  %5 = trunc i64 %4 to i32
  %6 = load i64, i64* %0, align 8, !tbaa !3
  %7 = add i64 %6, -4
  store i64 %7, i64* %0, align 8, !tbaa !3
  %8 = inttoptr i64 %7 to i32*
  store i32 %3, i32* %8, align 1
  %9 = load i64, i64* %0, align 8, !tbaa !3
  %10 = add i64 %9, -4
  store i64 %10, i64* %0, align 8, !tbaa !3
  %11 = inttoptr i64 %10 to i32*
  store i32 %5, i32* %11, align 1
  ret void
}

; Function Attrs: nounwind
declare i64 @llvm.x86.rdtsc() #3

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z13UPDATE_EFLAGSRmbbbbbb(i64* nocapture nonnull align 8 dereferenceable(8) %0, i1 zeroext %1, i1 zeroext %2, i1 zeroext %3, i1 zeroext %4, i1 zeroext %5, i1 zeroext %6) local_unnamed_addr #1 {
  %8 = load i64, i64* %0, align 8, !tbaa !3
  %9 = zext i1 %1 to i64
  %10 = or i64 %8, %9
  %11 = select i1 %2, i64 4, i64 0
  %12 = select i1 %3, i64 16, i64 0
  %13 = or i64 %12, %11
  %14 = or i64 %13, %10
  %15 = select i1 %4, i64 64, i64 0
  %16 = select i1 %5, i64 128, i64 0
  %17 = select i1 %6, i64 2048, i64 0
  %18 = or i64 %16, %15
  %19 = or i64 %18, %17
  %20 = or i64 %19, %14
  store i64 %20, i64* %0, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3ADDImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64, i64* %6, align 1
  %8 = add i64 %7, %4
  %9 = icmp ult i64 %8, %4
  %10 = icmp ult i64 %8, %7
  %11 = or i1 %9, %10
  %12 = trunc i64 %8 to i8
  %13 = tail call i8 @llvm.ctpop.i8(i8 %12) #3, !range !9
  %14 = xor i64 %7, %4
  %15 = xor i64 %14, %8
  %16 = and i64 %15, 16
  %17 = icmp eq i64 %8, 0
  %18 = lshr i64 %4, 63
  %19 = lshr i64 %7, 63
  %20 = lshr i64 %8, 63
  %21 = xor i64 %20, %18
  %22 = xor i64 %20, %19
  %23 = add nuw nsw i64 %21, %22
  %24 = icmp eq i64 %23, 2
  %25 = zext i1 %11 to i64
  %26 = shl nuw nsw i8 %13, 2
  %27 = and i8 %26, 4
  %28 = xor i8 %27, 4
  %29 = zext i8 %28 to i64
  %30 = select i1 %17, i64 64, i64 0
  %31 = lshr i64 %8, 56
  %32 = and i64 %31, 128
  %33 = select i1 %24, i64 2048, i64 0
  %34 = or i64 %32, %30
  %35 = or i64 %34, %16
  %36 = or i64 %35, %25
  %37 = or i64 %36, %33
  %38 = or i64 %37, %29
  store i64 %8, i64* %6, align 1
  %39 = load i64, i64* %0, align 8, !tbaa !3
  %40 = add i64 %39, -8
  store i64 %40, i64* %0, align 8, !tbaa !3
  %41 = inttoptr i64 %40 to i64*
  store i64 %38, i64* %41, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3ADDIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i32*
  %4 = load i32, i32* %3, align 1
  %5 = add i64 %2, 4
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i32*
  %7 = load i32, i32* %6, align 1
  %8 = add i32 %7, %4
  %9 = icmp ult i32 %8, %4
  %10 = icmp ult i32 %8, %7
  %11 = or i1 %9, %10
  %12 = trunc i32 %8 to i8
  %13 = tail call i8 @llvm.ctpop.i8(i8 %12) #3, !range !9
  %14 = xor i32 %7, %4
  %15 = xor i32 %14, %8
  %16 = and i32 %15, 16
  %17 = icmp eq i32 %8, 0
  %18 = lshr i32 %4, 31
  %19 = lshr i32 %7, 31
  %20 = lshr i32 %8, 31
  %21 = xor i32 %20, %18
  %22 = xor i32 %20, %19
  %23 = add nuw nsw i32 %21, %22
  %24 = icmp eq i32 %23, 2
  %25 = zext i1 %11 to i64
  %26 = shl nuw nsw i8 %13, 2
  %27 = and i8 %26, 4
  %28 = xor i8 %27, 4
  %29 = zext i8 %28 to i64
  %30 = zext i32 %16 to i64
  %31 = select i1 %17, i64 64, i64 0
  %32 = lshr i32 %8, 24
  %33 = and i32 %32, 128
  %34 = zext i32 %33 to i64
  %35 = select i1 %24, i64 2048, i64 0
  %36 = or i64 %31, %34
  %37 = or i64 %36, %30
  %38 = or i64 %37, %25
  %39 = or i64 %38, %35
  %40 = or i64 %39, %29
  store i32 %8, i32* %6, align 1
  %41 = load i64, i64* %0, align 8, !tbaa !3
  %42 = add i64 %41, -8
  store i64 %42, i64* %0, align 8, !tbaa !3
  %43 = inttoptr i64 %42 to i64*
  store i64 %40, i64* %43, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3ADDItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = add i16 %7, %4
  %9 = icmp ult i16 %8, %4
  %10 = icmp ult i16 %8, %7
  %11 = or i1 %9, %10
  %12 = trunc i16 %8 to i8
  %13 = tail call i8 @llvm.ctpop.i8(i8 %12) #3, !range !9
  %14 = xor i16 %7, %4
  %15 = xor i16 %14, %8
  %16 = and i16 %15, 16
  %17 = icmp eq i16 %8, 0
  %18 = lshr i16 %4, 15
  %19 = lshr i16 %7, 15
  %20 = lshr i16 %8, 15
  %21 = xor i16 %20, %18
  %22 = xor i16 %20, %19
  %23 = add nuw nsw i16 %21, %22
  %24 = icmp eq i16 %23, 2
  %25 = zext i1 %11 to i64
  %26 = shl nuw nsw i8 %13, 2
  %27 = and i8 %26, 4
  %28 = xor i8 %27, 4
  %29 = zext i8 %28 to i64
  %30 = zext i16 %16 to i64
  %31 = select i1 %17, i64 64, i64 0
  %32 = lshr i16 %8, 8
  %33 = and i16 %32, 128
  %34 = zext i16 %33 to i64
  %35 = select i1 %24, i64 2048, i64 0
  %36 = or i64 %31, %34
  %37 = or i64 %36, %30
  %38 = or i64 %37, %25
  %39 = or i64 %38, %35
  %40 = or i64 %39, %29
  store i16 %8, i16* %6, align 1
  %41 = load i64, i64* %0, align 8, !tbaa !3
  %42 = add i64 %41, -8
  store i64 %42, i64* %0, align 8, !tbaa !3
  %43 = inttoptr i64 %42 to i64*
  store i64 %40, i64* %43, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3ADDIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = trunc i16 %4 to i8
  %7 = inttoptr i64 %5 to i16*
  %8 = load i16, i16* %7, align 1
  %9 = trunc i16 %8 to i8
  %10 = add i8 %9, %6
  %11 = icmp ult i8 %10, %6
  %12 = icmp ult i8 %10, %9
  %13 = or i1 %11, %12
  %14 = tail call i8 @llvm.ctpop.i8(i8 %10) #3, !range !9
  %15 = xor i16 %8, %4
  %16 = trunc i16 %15 to i8
  %17 = xor i8 %10, %16
  %18 = and i8 %17, 16
  %19 = icmp eq i8 %10, 0
  %20 = lshr i8 %6, 7
  %21 = lshr i8 %9, 7
  %22 = lshr i8 %10, 7
  %23 = xor i8 %22, %20
  %24 = xor i8 %22, %21
  %25 = add nuw nsw i8 %23, %24
  %26 = icmp eq i8 %25, 2
  %27 = zext i1 %13 to i64
  %28 = shl nuw nsw i8 %14, 2
  %29 = and i8 %28, 4
  %30 = or i8 %29, %18
  %31 = xor i8 %30, 4
  %32 = zext i8 %31 to i64
  %33 = select i1 %19, i64 64, i64 0
  %34 = and i8 %10, -128
  %35 = zext i8 %34 to i64
  %36 = select i1 %26, i64 2048, i64 0
  %37 = or i64 %33, %35
  %38 = or i64 %37, %27
  %39 = or i64 %38, %36
  %40 = or i64 %39, %32
  %41 = zext i8 %10 to i16
  store i16 %41, i16* %7, align 1
  %42 = load i64, i64* %0, align 8, !tbaa !3
  %43 = add i64 %42, -8
  store i64 %43, i64* %0, align 8, !tbaa !3
  %44 = inttoptr i64 %43 to i64*
  store i64 %40, i64* %44, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z9DIV_FLAGSRm(i64* nocapture nonnull align 8 dereferenceable(8) %0) local_unnamed_addr #1 {
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3DIVImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64, i64* %6, align 1
  %8 = add i64 %2, 16
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i64*
  %10 = load i64, i64* %9, align 1
  %11 = zext i64 %7 to i128
  %12 = shl nuw i128 %11, 64
  %13 = zext i64 %4 to i128
  %14 = or i128 %12, %13
  %15 = freeze i64 %10
  %16 = zext i64 %15 to i128
  %17 = freeze i128 %14
  %18 = udiv i128 %17, %16
  %19 = trunc i128 %18 to i64
  %20 = mul i128 %18, %16
  %21 = sub i128 %17, %20
  %22 = trunc i128 %21 to i64
  store i64 %8, i64* %0, align 8, !tbaa !3
  store i64 %19, i64* %9, align 1
  %23 = load i64, i64* %0, align 8, !tbaa !3
  %24 = add i64 %23, -8
  store i64 %24, i64* %0, align 8, !tbaa !3
  %25 = inttoptr i64 %24 to i64*
  store i64 %22, i64* %25, align 1
  %26 = load i64, i64* %0, align 8, !tbaa !3
  %27 = add i64 %26, -8
  store i64 %27, i64* %0, align 8, !tbaa !3
  %28 = inttoptr i64 %27 to i64*
  store i64 0, i64* %28, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3DIVIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i32*
  %4 = load i32, i32* %3, align 1
  %5 = add i64 %2, 4
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i32*
  %7 = load i32, i32* %6, align 1
  %8 = add i64 %2, 8
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i32*
  %10 = load i32, i32* %9, align 1
  %11 = zext i32 %4 to i64
  %12 = zext i32 %7 to i64
  %13 = zext i32 %10 to i64
  %14 = shl nuw i64 %12, 32
  %15 = or i64 %14, %11
  %16 = udiv i64 %15, %13
  %17 = urem i64 %15, %13
  %18 = trunc i64 %16 to i32
  %19 = trunc i64 %17 to i32
  store i64 %8, i64* %0, align 8, !tbaa !3
  store i32 %18, i32* %9, align 1
  %20 = load i64, i64* %0, align 8, !tbaa !3
  %21 = add i64 %20, -4
  store i64 %21, i64* %0, align 8, !tbaa !3
  %22 = inttoptr i64 %21 to i32*
  store i32 %19, i32* %22, align 1
  %23 = load i64, i64* %0, align 8, !tbaa !3
  %24 = add i64 %23, -8
  store i64 %24, i64* %0, align 8, !tbaa !3
  %25 = inttoptr i64 %24 to i64*
  store i64 0, i64* %25, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3DIVItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = add i64 %2, 4
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i16*
  %10 = load i16, i16* %9, align 1
  %11 = zext i16 %4 to i32
  %12 = zext i16 %10 to i32
  %13 = zext i16 %7 to i32
  %14 = shl nuw i32 %13, 16
  %15 = or i32 %14, %11
  %16 = udiv i32 %15, %12
  %17 = urem i32 %15, %12
  %18 = trunc i32 %16 to i16
  %19 = trunc i32 %17 to i16
  store i64 %8, i64* %0, align 8, !tbaa !3
  store i16 %18, i16* %9, align 1
  %20 = load i64, i64* %0, align 8, !tbaa !3
  %21 = add i64 %20, -2
  store i64 %21, i64* %0, align 8, !tbaa !3
  %22 = inttoptr i64 %21 to i16*
  store i16 %19, i16* %22, align 1
  %23 = load i64, i64* %0, align 8, !tbaa !3
  %24 = add i64 %23, -8
  store i64 %24, i64* %0, align 8, !tbaa !3
  %25 = inttoptr i64 %24 to i64*
  store i64 0, i64* %25, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3DIVIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = add i64 %2, 4
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i16*
  %10 = load i16, i16* %9, align 1
  %11 = and i16 %4, 255
  %12 = and i16 %10, 255
  %13 = shl i16 %7, 8
  %14 = or i16 %13, %11
  %15 = udiv i16 %14, %12
  %16 = urem i16 %14, %12
  %17 = trunc i16 %15 to i8
  %18 = trunc i16 %16 to i8
  %19 = add i64 %2, 5
  store i64 %19, i64* %0, align 8, !tbaa !3
  %20 = inttoptr i64 %19 to i8*
  store i8 %17, i8* %20, align 1
  %21 = load i64, i64* %0, align 8, !tbaa !3
  %22 = add i64 %21, -1
  store i64 %22, i64* %0, align 8, !tbaa !3
  %23 = inttoptr i64 %22 to i8*
  store i8 %18, i8* %23, align 1
  %24 = load i64, i64* %0, align 8, !tbaa !3
  %25 = add i64 %24, -8
  store i64 %25, i64* %0, align 8, !tbaa !3
  %26 = inttoptr i64 %25 to i64*
  store i64 0, i64* %26, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z10IDIV_FLAGSRm(i64* nocapture nonnull align 8 dereferenceable(8) %0) local_unnamed_addr #1 {
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IDIVImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64, i64* %6, align 1
  %8 = add i64 %2, 16
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i64*
  %10 = load i64, i64* %9, align 1
  %11 = ashr i64 %10, 63
  %12 = zext i64 %7 to i128
  %13 = shl nuw i128 %12, 64
  %14 = zext i64 %4 to i128
  %15 = or i128 %13, %14
  %16 = zext i64 %11 to i128
  %17 = shl nuw i128 %16, 64
  %18 = zext i64 %10 to i128
  %19 = or i128 %17, %18
  %20 = freeze i128 %15
  %21 = freeze i128 %19
  %22 = sdiv i128 %20, %21
  %23 = trunc i128 %22 to i64
  %24 = mul i128 %22, %21
  %25 = sub i128 %20, %24
  %26 = trunc i128 %25 to i64
  store i64 %8, i64* %0, align 8, !tbaa !3
  store i64 %23, i64* %9, align 1
  %27 = load i64, i64* %0, align 8, !tbaa !3
  %28 = add i64 %27, -8
  store i64 %28, i64* %0, align 8, !tbaa !3
  %29 = inttoptr i64 %28 to i64*
  store i64 %26, i64* %29, align 1
  %30 = load i64, i64* %0, align 8, !tbaa !3
  %31 = add i64 %30, -8
  store i64 %31, i64* %0, align 8, !tbaa !3
  %32 = inttoptr i64 %31 to i64*
  store i64 0, i64* %32, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IDIVIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i32*
  %4 = load i32, i32* %3, align 1
  %5 = add i64 %2, 4
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i32*
  %7 = load i32, i32* %6, align 1
  %8 = add i64 %2, 8
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i32*
  %10 = load i32, i32* %9, align 1
  %11 = zext i32 %4 to i64
  %12 = zext i32 %7 to i64
  %13 = sext i32 %10 to i64
  %14 = shl nuw i64 %12, 32
  %15 = or i64 %14, %11
  %16 = sdiv i64 %15, %13
  %17 = srem i64 %15, %13
  %18 = trunc i64 %16 to i32
  %19 = trunc i64 %17 to i32
  store i64 %8, i64* %0, align 8, !tbaa !3
  store i32 %18, i32* %9, align 1
  %20 = load i64, i64* %0, align 8, !tbaa !3
  %21 = add i64 %20, -4
  store i64 %21, i64* %0, align 8, !tbaa !3
  %22 = inttoptr i64 %21 to i32*
  store i32 %19, i32* %22, align 1
  %23 = load i64, i64* %0, align 8, !tbaa !3
  %24 = add i64 %23, -8
  store i64 %24, i64* %0, align 8, !tbaa !3
  %25 = inttoptr i64 %24 to i64*
  store i64 0, i64* %25, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IDIVItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = add i64 %2, 4
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i16*
  %10 = load i16, i16* %9, align 1
  %11 = zext i16 %4 to i32
  %12 = zext i16 %7 to i32
  %13 = shl nuw i32 %12, 16
  %14 = or i32 %13, %11
  %15 = sext i32 %14 to i64
  %16 = sext i16 %10 to i64
  %17 = sdiv i64 %15, %16
  %18 = trunc i64 %17 to i16
  %19 = srem i64 %15, %16
  %20 = trunc i64 %19 to i16
  store i64 %8, i64* %0, align 8, !tbaa !3
  store i16 %18, i16* %9, align 1
  %21 = load i64, i64* %0, align 8, !tbaa !3
  %22 = add i64 %21, -2
  store i64 %22, i64* %0, align 8, !tbaa !3
  %23 = inttoptr i64 %22 to i16*
  store i16 %20, i16* %23, align 1
  %24 = load i64, i64* %0, align 8, !tbaa !3
  %25 = add i64 %24, -8
  store i64 %25, i64* %0, align 8, !tbaa !3
  %26 = inttoptr i64 %25 to i64*
  store i64 0, i64* %26, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IDIVIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = add i64 %2, 4
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i16*
  %10 = load i16, i16* %9, align 1
  %11 = and i16 %4, 255
  %12 = shl i16 %10, 8
  %13 = ashr exact i16 %12, 8
  %14 = shl i16 %7, 8
  %15 = or i16 %14, %11
  %16 = sext i16 %15 to i32
  %17 = sext i16 %13 to i32
  %18 = sdiv i32 %16, %17
  %19 = trunc i32 %18 to i8
  %20 = srem i32 %16, %17
  %21 = trunc i32 %20 to i8
  %22 = add i64 %2, 5
  store i64 %22, i64* %0, align 8, !tbaa !3
  %23 = inttoptr i64 %22 to i8*
  store i8 %19, i8* %23, align 1
  %24 = load i64, i64* %0, align 8, !tbaa !3
  %25 = add i64 %24, -1
  store i64 %25, i64* %0, align 8, !tbaa !3
  %26 = inttoptr i64 %25 to i8*
  store i8 %21, i8* %26, align 1
  %27 = load i64, i64* %0, align 8, !tbaa !3
  %28 = add i64 %27, -8
  store i64 %28, i64* %0, align 8, !tbaa !3
  %29 = inttoptr i64 %28 to i64*
  store i64 0, i64* %29, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3MULImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64, i64* %6, align 1
  %8 = zext i64 %4 to i128
  %9 = zext i64 %7 to i128
  %10 = mul nuw i128 %9, %8
  %11 = trunc i128 %10 to i64
  %12 = lshr i128 %10, 64
  %13 = trunc i128 %12 to i64
  %14 = icmp ugt i128 %10, 18446744073709551615
  %15 = zext i1 %14 to i64
  %16 = select i1 %14, i64 2048, i64 0
  %17 = or i64 %16, %15
  %18 = or i64 %17, 212
  store i64 %5, i64* %0, align 8, !tbaa !3
  store i64 %11, i64* %6, align 1
  %19 = load i64, i64* %0, align 8, !tbaa !3
  %20 = add i64 %19, -8
  store i64 %20, i64* %0, align 8, !tbaa !3
  %21 = inttoptr i64 %20 to i64*
  store i64 %13, i64* %21, align 1
  %22 = load i64, i64* %0, align 8, !tbaa !3
  %23 = add i64 %22, -8
  store i64 %23, i64* %0, align 8, !tbaa !3
  %24 = inttoptr i64 %23 to i64*
  store i64 %18, i64* %24, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3MULIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i32*
  %4 = load i32, i32* %3, align 1
  %5 = add i64 %2, 4
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i32*
  %7 = load i32, i32* %6, align 1
  %8 = zext i32 %4 to i64
  %9 = zext i32 %7 to i64
  %10 = mul nuw i64 %9, %8
  %11 = trunc i64 %10 to i32
  %12 = lshr i64 %10, 32
  %13 = trunc i64 %12 to i32
  %14 = icmp ugt i64 %10, 4294967295
  %15 = zext i1 %14 to i64
  %16 = select i1 %14, i64 2048, i64 0
  %17 = or i64 %16, %15
  %18 = or i64 %17, 212
  store i64 %5, i64* %0, align 8, !tbaa !3
  store i32 %11, i32* %6, align 1
  %19 = load i64, i64* %0, align 8, !tbaa !3
  %20 = add i64 %19, -4
  store i64 %20, i64* %0, align 8, !tbaa !3
  %21 = inttoptr i64 %20 to i32*
  store i32 %13, i32* %21, align 1
  %22 = load i64, i64* %0, align 8, !tbaa !3
  %23 = add i64 %22, -8
  store i64 %23, i64* %0, align 8, !tbaa !3
  %24 = inttoptr i64 %23 to i64*
  store i64 %18, i64* %24, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3MULItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = zext i16 %4 to i32
  %9 = zext i16 %7 to i32
  %10 = mul nuw i32 %9, %8
  %11 = trunc i32 %10 to i16
  %12 = lshr i32 %10, 16
  %13 = trunc i32 %12 to i16
  %14 = icmp ugt i32 %10, 65535
  %15 = zext i1 %14 to i64
  %16 = select i1 %14, i64 2048, i64 0
  %17 = or i64 %16, %15
  %18 = or i64 %17, 212
  store i64 %5, i64* %0, align 8, !tbaa !3
  store i16 %11, i16* %6, align 1
  %19 = load i64, i64* %0, align 8, !tbaa !3
  %20 = add i64 %19, -2
  store i64 %20, i64* %0, align 8, !tbaa !3
  %21 = inttoptr i64 %20 to i16*
  store i16 %13, i16* %21, align 1
  %22 = load i64, i64* %0, align 8, !tbaa !3
  %23 = add i64 %22, -8
  store i64 %23, i64* %0, align 8, !tbaa !3
  %24 = inttoptr i64 %23 to i64*
  store i64 %18, i64* %24, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3MULIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = and i16 %4, 255
  %9 = and i16 %7, 255
  %10 = mul nuw i16 %9, %8
  %11 = trunc i16 %10 to i8
  %12 = lshr i16 %10, 8
  %13 = trunc i16 %12 to i8
  %14 = icmp ugt i16 %10, 255
  %15 = zext i1 %14 to i64
  %16 = select i1 %14, i64 2048, i64 0
  %17 = or i64 %16, %15
  %18 = or i64 %17, 212
  %19 = add i64 %2, 3
  store i64 %19, i64* %0, align 8, !tbaa !3
  %20 = inttoptr i64 %19 to i8*
  store i8 %11, i8* %20, align 1
  %21 = load i64, i64* %0, align 8, !tbaa !3
  %22 = add i64 %21, -1
  store i64 %22, i64* %0, align 8, !tbaa !3
  %23 = inttoptr i64 %22 to i8*
  store i8 %13, i8* %23, align 1
  %24 = load i64, i64* %0, align 8, !tbaa !3
  %25 = add i64 %24, -8
  store i64 %25, i64* %0, align 8, !tbaa !3
  %26 = inttoptr i64 %25 to i64*
  store i64 %18, i64* %26, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IMULImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64, i64* %6, align 1
  %8 = ashr i64 %4, 63
  %9 = ashr i64 %7, 63
  %10 = zext i64 %8 to i128
  %11 = shl nuw i128 %10, 64
  %12 = zext i64 %4 to i128
  %13 = or i128 %11, %12
  %14 = zext i64 %9 to i128
  %15 = shl nuw i128 %14, 64
  %16 = zext i64 %7 to i128
  %17 = or i128 %15, %16
  %18 = mul nsw i128 %17, %13
  %19 = trunc i128 %18 to i64
  %20 = lshr i128 %18, 64
  %21 = trunc i128 %20 to i64
  %22 = sext i64 %19 to i128
  %23 = icmp ne i128 %18, %22
  %24 = zext i1 %23 to i64
  %25 = lshr i64 %19, 56
  %26 = and i64 %25, 128
  %27 = select i1 %23, i64 2048, i64 0
  %28 = or i64 %26, %24
  %29 = or i64 %28, %27
  %30 = or i64 %29, 84
  store i64 %5, i64* %0, align 8, !tbaa !3
  store i64 %19, i64* %6, align 1
  %31 = load i64, i64* %0, align 8, !tbaa !3
  %32 = add i64 %31, -8
  store i64 %32, i64* %0, align 8, !tbaa !3
  %33 = inttoptr i64 %32 to i64*
  store i64 %21, i64* %33, align 1
  %34 = load i64, i64* %0, align 8, !tbaa !3
  %35 = add i64 %34, -8
  store i64 %35, i64* %0, align 8, !tbaa !3
  %36 = inttoptr i64 %35 to i64*
  store i64 %30, i64* %36, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IMULIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i32*
  %4 = load i32, i32* %3, align 1
  %5 = add i64 %2, 4
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i32*
  %7 = load i32, i32* %6, align 1
  %8 = sext i32 %4 to i64
  %9 = sext i32 %7 to i64
  %10 = mul nsw i64 %9, %8
  %11 = trunc i64 %10 to i32
  %12 = lshr i64 %10, 32
  %13 = trunc i64 %12 to i32
  %14 = add nsw i64 %10, 2147483648
  %15 = icmp ugt i64 %14, 4294967295
  %16 = zext i1 %15 to i64
  %17 = lshr i64 %10, 24
  %18 = and i64 %17, 128
  %19 = select i1 %15, i64 2048, i64 0
  %20 = or i64 %18, %16
  %21 = or i64 %20, %19
  %22 = or i64 %21, 84
  store i64 %5, i64* %0, align 8, !tbaa !3
  store i32 %11, i32* %6, align 1
  %23 = load i64, i64* %0, align 8, !tbaa !3
  %24 = add i64 %23, -4
  store i64 %24, i64* %0, align 8, !tbaa !3
  %25 = inttoptr i64 %24 to i32*
  store i32 %13, i32* %25, align 1
  %26 = load i64, i64* %0, align 8, !tbaa !3
  %27 = add i64 %26, -8
  store i64 %27, i64* %0, align 8, !tbaa !3
  %28 = inttoptr i64 %27 to i64*
  store i64 %22, i64* %28, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IMULItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = sext i16 %4 to i32
  %9 = sext i16 %7 to i32
  %10 = mul nsw i32 %9, %8
  %11 = trunc i32 %10 to i16
  %12 = lshr i32 %10, 16
  %13 = trunc i32 %12 to i16
  %14 = add nsw i32 %10, 32768
  %15 = icmp ugt i32 %14, 65535
  %16 = zext i1 %15 to i64
  %17 = lshr i16 %11, 8
  %18 = and i16 %17, 128
  %19 = zext i16 %18 to i64
  %20 = select i1 %15, i64 2048, i64 0
  %21 = or i64 %20, %16
  %22 = or i64 %21, %19
  %23 = or i64 %22, 84
  store i64 %5, i64* %0, align 8, !tbaa !3
  store i16 %11, i16* %6, align 1
  %24 = load i64, i64* %0, align 8, !tbaa !3
  %25 = add i64 %24, -2
  store i64 %25, i64* %0, align 8, !tbaa !3
  %26 = inttoptr i64 %25 to i16*
  store i16 %13, i16* %26, align 1
  %27 = load i64, i64* %0, align 8, !tbaa !3
  %28 = add i64 %27, -8
  store i64 %28, i64* %0, align 8, !tbaa !3
  %29 = inttoptr i64 %28 to i64*
  store i64 %23, i64* %29, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4IMULIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = shl i16 %4, 8
  %9 = ashr exact i16 %8, 8
  %10 = shl i16 %7, 8
  %11 = ashr exact i16 %10, 8
  %12 = mul nsw i16 %11, %9
  %13 = trunc i16 %12 to i8
  %14 = lshr i16 %12, 8
  %15 = trunc i16 %14 to i8
  %16 = add nsw i16 %12, 128
  %17 = icmp ugt i16 %16, 255
  %18 = zext i1 %17 to i64
  %19 = and i8 %13, -128
  %20 = zext i8 %19 to i64
  %21 = select i1 %17, i64 2048, i64 0
  %22 = or i64 %20, %18
  %23 = or i64 %22, %21
  %24 = or i64 %23, 84
  %25 = add i64 %2, 3
  store i64 %25, i64* %0, align 8, !tbaa !3
  %26 = inttoptr i64 %25 to i8*
  store i8 %13, i8* %26, align 1
  %27 = load i64, i64* %0, align 8, !tbaa !3
  %28 = add i64 %27, -1
  store i64 %28, i64* %0, align 8, !tbaa !3
  %29 = inttoptr i64 %28 to i8*
  store i8 %15, i8* %29, align 1
  %30 = load i64, i64* %0, align 8, !tbaa !3
  %31 = add i64 %30, -8
  store i64 %31, i64* %0, align 8, !tbaa !3
  %32 = inttoptr i64 %31 to i64*
  store i64 %24, i64* %32, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3NORImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64, i64* %6, align 1
  %8 = or i64 %7, %4
  %9 = xor i64 %8, -1
  %10 = trunc i64 %9 to i8
  %11 = tail call i8 @llvm.ctpop.i8(i8 %10) #3, !range !9
  %12 = icmp eq i64 %8, -1
  %13 = shl nuw nsw i8 %11, 2
  %14 = and i8 %13, 4
  %15 = xor i8 %14, 4
  %16 = zext i8 %15 to i64
  %17 = select i1 %12, i64 64, i64 0
  %18 = lshr i64 %9, 56
  %19 = and i64 %18, 128
  %20 = or i64 %19, %17
  %21 = or i64 %20, %16
  store i64 %9, i64* %6, align 1
  %22 = load i64, i64* %0, align 8, !tbaa !3
  %23 = add i64 %22, -8
  store i64 %23, i64* %0, align 8, !tbaa !3
  %24 = inttoptr i64 %23 to i64*
  store i64 %21, i64* %24, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3NORIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i32*
  %4 = load i32, i32* %3, align 1
  %5 = add i64 %2, 4
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i32*
  %7 = load i32, i32* %6, align 1
  %8 = or i32 %7, %4
  %9 = xor i32 %8, -1
  %10 = trunc i32 %9 to i8
  %11 = tail call i8 @llvm.ctpop.i8(i8 %10) #3, !range !9
  %12 = icmp eq i32 %8, -1
  %13 = shl nuw nsw i8 %11, 2
  %14 = and i8 %13, 4
  %15 = xor i8 %14, 4
  %16 = zext i8 %15 to i64
  %17 = select i1 %12, i64 64, i64 0
  %18 = lshr i32 %9, 24
  %19 = and i32 %18, 128
  %20 = zext i32 %19 to i64
  %21 = or i64 %17, %20
  %22 = or i64 %21, %16
  store i32 %9, i32* %6, align 1
  %23 = load i64, i64* %0, align 8, !tbaa !3
  %24 = add i64 %23, -8
  store i64 %24, i64* %0, align 8, !tbaa !3
  %25 = inttoptr i64 %24 to i64*
  store i64 %22, i64* %25, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3NORItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = or i16 %7, %4
  %9 = xor i16 %8, -1
  %10 = trunc i16 %9 to i8
  %11 = tail call i8 @llvm.ctpop.i8(i8 %10) #3, !range !9
  %12 = icmp eq i16 %8, -1
  %13 = shl nuw nsw i8 %11, 2
  %14 = and i8 %13, 4
  %15 = xor i8 %14, 4
  %16 = zext i8 %15 to i64
  %17 = select i1 %12, i64 64, i64 0
  %18 = lshr i16 %9, 8
  %19 = and i16 %18, 128
  %20 = zext i16 %19 to i64
  %21 = or i64 %17, %20
  %22 = or i64 %21, %16
  store i16 %9, i16* %6, align 1
  %23 = load i64, i64* %0, align 8, !tbaa !3
  %24 = add i64 %23, -8
  store i64 %24, i64* %0, align 8, !tbaa !3
  %25 = inttoptr i64 %24 to i64*
  store i64 %22, i64* %25, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3NORIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = or i16 %7, %4
  %9 = trunc i16 %8 to i8
  %10 = xor i8 %9, -1
  %11 = tail call i8 @llvm.ctpop.i8(i8 %10) #3, !range !9
  %12 = icmp eq i8 %9, -1
  %13 = shl nuw nsw i8 %11, 2
  %14 = and i8 %13, 4
  %15 = xor i8 %14, 4
  %16 = zext i8 %15 to i64
  %17 = select i1 %12, i64 64, i64 0
  %18 = and i8 %10, -128
  %19 = zext i8 %18 to i64
  %20 = or i64 %17, %19
  %21 = or i64 %20, %16
  %22 = zext i8 %10 to i16
  store i16 %22, i16* %6, align 1
  %23 = load i64, i64* %0, align 8, !tbaa !3
  %24 = add i64 %23, -8
  store i64 %24, i64* %0, align 8, !tbaa !3
  %25 = inttoptr i64 %24 to i64*
  store i64 %21, i64* %25, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4NANDImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64, i64* %6, align 1
  %8 = and i64 %7, %4
  %9 = xor i64 %8, -1
  %10 = trunc i64 %9 to i8
  %11 = tail call i8 @llvm.ctpop.i8(i8 %10) #3, !range !9
  %12 = icmp eq i64 %8, -1
  %13 = shl nuw nsw i8 %11, 2
  %14 = and i8 %13, 4
  %15 = xor i8 %14, 4
  %16 = zext i8 %15 to i64
  %17 = select i1 %12, i64 64, i64 0
  %18 = lshr i64 %9, 56
  %19 = and i64 %18, 128
  %20 = or i64 %19, %17
  %21 = or i64 %20, %16
  store i64 %9, i64* %6, align 1
  %22 = load i64, i64* %0, align 8, !tbaa !3
  %23 = add i64 %22, -8
  store i64 %23, i64* %0, align 8, !tbaa !3
  %24 = inttoptr i64 %23 to i64*
  store i64 %21, i64* %24, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4NANDIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i32*
  %4 = load i32, i32* %3, align 1
  %5 = add i64 %2, 4
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i32*
  %7 = load i32, i32* %6, align 1
  %8 = and i32 %7, %4
  %9 = xor i32 %8, -1
  %10 = trunc i32 %9 to i8
  %11 = tail call i8 @llvm.ctpop.i8(i8 %10) #3, !range !9
  %12 = icmp eq i32 %8, -1
  %13 = shl nuw nsw i8 %11, 2
  %14 = and i8 %13, 4
  %15 = xor i8 %14, 4
  %16 = zext i8 %15 to i64
  %17 = select i1 %12, i64 64, i64 0
  %18 = lshr i32 %9, 24
  %19 = and i32 %18, 128
  %20 = zext i32 %19 to i64
  %21 = or i64 %17, %20
  %22 = or i64 %21, %16
  store i32 %9, i32* %6, align 1
  %23 = load i64, i64* %0, align 8, !tbaa !3
  %24 = add i64 %23, -8
  store i64 %24, i64* %0, align 8, !tbaa !3
  %25 = inttoptr i64 %24 to i64*
  store i64 %22, i64* %25, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4NANDItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = and i16 %7, %4
  %9 = xor i16 %8, -1
  %10 = trunc i16 %9 to i8
  %11 = tail call i8 @llvm.ctpop.i8(i8 %10) #3, !range !9
  %12 = icmp eq i16 %8, -1
  %13 = shl nuw nsw i8 %11, 2
  %14 = and i8 %13, 4
  %15 = xor i8 %14, 4
  %16 = zext i8 %15 to i64
  %17 = select i1 %12, i64 64, i64 0
  %18 = lshr i16 %9, 8
  %19 = and i16 %18, 128
  %20 = zext i16 %19 to i64
  %21 = or i64 %17, %20
  %22 = or i64 %21, %16
  store i16 %9, i16* %6, align 1
  %23 = load i64, i64* %0, align 8, !tbaa !3
  %24 = add i64 %23, -8
  store i64 %24, i64* %0, align 8, !tbaa !3
  %25 = inttoptr i64 %24 to i64*
  store i64 %22, i64* %25, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4NANDIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = and i16 %7, %4
  %9 = trunc i16 %8 to i8
  %10 = xor i8 %9, -1
  %11 = tail call i8 @llvm.ctpop.i8(i8 %10) #3, !range !9
  %12 = icmp eq i8 %9, -1
  %13 = shl nuw nsw i8 %11, 2
  %14 = and i8 %13, 4
  %15 = xor i8 %14, 4
  %16 = zext i8 %15 to i64
  %17 = select i1 %12, i64 64, i64 0
  %18 = and i8 %10, -128
  %19 = zext i8 %18 to i64
  %20 = or i64 %17, %19
  %21 = or i64 %20, %16
  %22 = zext i8 %10 to i16
  store i16 %22, i16* %6, align 1
  %23 = load i64, i64* %0, align 8, !tbaa !3
  %24 = add i64 %23, -8
  store i64 %24, i64* %0, align 8, !tbaa !3
  %25 = inttoptr i64 %24 to i64*
  store i64 %21, i64* %25, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHLImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = zext i16 %7 to i64
  %9 = shl i64 %4, %8
  %10 = and i64 %8, 63
  %11 = icmp eq i64 %10, 1
  %12 = icmp slt i64 %4, 0
  %13 = icmp slt i64 %9, 0
  %14 = select i1 %11, i1 %12, i1 %13
  %15 = trunc i64 %9 to i8
  %16 = tail call i8 @llvm.ctpop.i8(i8 %15) #3, !range !9
  %17 = icmp eq i64 %9, 0
  %18 = xor i64 %9, %4
  %19 = zext i1 %14 to i64
  %20 = shl nuw nsw i8 %16, 2
  %21 = and i8 %20, 4
  %22 = xor i8 %21, 4
  %23 = zext i8 %22 to i64
  %24 = select i1 %17, i64 64, i64 0
  %25 = lshr i64 %9, 56
  %26 = and i64 %25, 128
  %27 = lshr i64 %18, 52
  %28 = and i64 %27, 2048
  %29 = or i64 %26, %24
  %30 = or i64 %29, %28
  %31 = or i64 %30, %19
  %32 = or i64 %31, %23
  %33 = or i64 %32, 16
  %34 = add i64 %2, 2
  store i64 %34, i64* %0, align 8, !tbaa !3
  %35 = inttoptr i64 %34 to i64*
  store i64 %9, i64* %35, align 1
  %36 = load i64, i64* %0, align 8, !tbaa !3
  %37 = add i64 %36, -8
  store i64 %37, i64* %0, align 8, !tbaa !3
  %38 = inttoptr i64 %37 to i64*
  store i64 %33, i64* %38, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHLIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i32*
  %4 = load i32, i32* %3, align 1
  %5 = add i64 %2, 4
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = zext i16 %7 to i32
  %9 = shl i32 %4, %8
  %10 = and i32 %8, 31
  %11 = icmp eq i32 %10, 1
  %12 = icmp slt i32 %4, 0
  %13 = icmp slt i32 %9, 0
  %14 = select i1 %11, i1 %12, i1 %13
  %15 = trunc i32 %9 to i8
  %16 = tail call i8 @llvm.ctpop.i8(i8 %15) #3, !range !9
  %17 = icmp eq i32 %9, 0
  %18 = xor i32 %9, %4
  %19 = zext i1 %14 to i64
  %20 = shl nuw nsw i8 %16, 2
  %21 = and i8 %20, 4
  %22 = xor i8 %21, 4
  %23 = zext i8 %22 to i64
  %24 = select i1 %17, i64 64, i64 0
  %25 = lshr i32 %9, 24
  %26 = and i32 %25, 128
  %27 = zext i32 %26 to i64
  %28 = lshr i32 %18, 20
  %29 = and i32 %28, 2048
  %30 = zext i32 %29 to i64
  %31 = or i64 %24, %27
  %32 = or i64 %31, %19
  %33 = or i64 %32, %30
  %34 = or i64 %33, %23
  %35 = or i64 %34, 16
  %36 = add i64 %2, 2
  store i64 %36, i64* %0, align 8, !tbaa !3
  %37 = inttoptr i64 %36 to i32*
  store i32 %9, i32* %37, align 1
  %38 = load i64, i64* %0, align 8, !tbaa !3
  %39 = add i64 %38, -8
  store i64 %39, i64* %0, align 8, !tbaa !3
  %40 = inttoptr i64 %39 to i64*
  store i64 %35, i64* %40, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHLItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = zext i16 %4 to i32
  %9 = zext i16 %7 to i32
  %10 = shl i32 %8, %9
  %11 = trunc i32 %10 to i16
  %12 = and i16 %7, 31
  %13 = icmp eq i16 %12, 1
  br i1 %13, label %14, label %16

14:                                               ; preds = %1
  %15 = icmp slt i16 %4, 0
  br label %20

16:                                               ; preds = %1
  %17 = icmp ult i16 %12, 16
  %18 = icmp slt i16 %11, 0
  %19 = select i1 %17, i1 %18, i1 undef
  br label %20

20:                                               ; preds = %16, %14
  %21 = phi i1 [ %15, %14 ], [ %19, %16 ]
  %22 = trunc i32 %10 to i8
  %23 = tail call i8 @llvm.ctpop.i8(i8 %22) #3, !range !9
  %24 = icmp eq i32 %10, 0
  %25 = xor i16 %4, %11
  %26 = zext i1 %21 to i64
  %27 = shl nuw nsw i8 %23, 2
  %28 = and i8 %27, 4
  %29 = xor i8 %28, 4
  %30 = zext i8 %29 to i64
  %31 = select i1 %24, i64 64, i64 0
  %32 = lshr i32 %10, 24
  %33 = and i32 %32, 128
  %34 = zext i32 %33 to i64
  %35 = lshr i16 %25, 4
  %36 = and i16 %35, 2048
  %37 = zext i16 %36 to i64
  %38 = or i64 %31, %34
  %39 = or i64 %38, %37
  %40 = or i64 %39, %30
  %41 = or i64 %40, %26
  %42 = or i64 %41, 16
  store i64 %5, i64* %0, align 8, !tbaa !3
  store i16 %11, i16* %6, align 1
  %43 = load i64, i64* %0, align 8, !tbaa !3
  %44 = add i64 %43, -8
  store i64 %44, i64* %0, align 8, !tbaa !3
  %45 = inttoptr i64 %44 to i64*
  store i64 %42, i64* %45, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHLIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = trunc i16 %4 to i8
  %7 = inttoptr i64 %5 to i16*
  %8 = load i16, i16* %7, align 1
  %9 = trunc i16 %8 to i8
  %10 = and i16 %4, 255
  %11 = zext i16 %10 to i32
  %12 = and i16 %8, 255
  %13 = zext i16 %12 to i32
  %14 = shl i32 %11, %13
  %15 = trunc i32 %14 to i8
  %16 = and i8 %9, 31
  %17 = icmp eq i8 %16, 1
  br i1 %17, label %18, label %20

18:                                               ; preds = %1
  %19 = icmp slt i8 %6, 0
  br label %24

20:                                               ; preds = %1
  %21 = icmp ult i8 %16, 8
  %22 = icmp slt i8 %15, 0
  %23 = select i1 %21, i1 %22, i1 undef
  br label %24

24:                                               ; preds = %20, %18
  %25 = phi i1 [ %19, %18 ], [ %23, %20 ]
  %26 = tail call i8 @llvm.ctpop.i8(i8 %15) #3, !range !9
  %27 = icmp eq i32 %14, 0
  %28 = xor i8 %15, %6
  %29 = zext i1 %25 to i64
  %30 = shl nuw nsw i8 %26, 2
  %31 = and i8 %30, 4
  %32 = xor i8 %31, 4
  %33 = zext i8 %32 to i64
  %34 = select i1 %27, i64 64, i64 0
  %35 = lshr i32 %14, 24
  %36 = and i32 %35, 128
  %37 = zext i32 %36 to i64
  %38 = and i8 %28, -128
  %39 = zext i8 %38 to i64
  %40 = shl nuw nsw i64 %39, 4
  %41 = or i64 %34, %37
  %42 = or i64 %40, %41
  %43 = or i64 %42, %33
  %44 = or i64 %43, %29
  %45 = or i64 %44, 16
  %46 = trunc i32 %14 to i16
  store i64 %5, i64* %0, align 8, !tbaa !3
  store i16 %46, i16* %7, align 1
  %47 = load i64, i64* %0, align 8, !tbaa !3
  %48 = add i64 %47, -8
  store i64 %48, i64* %0, align 8, !tbaa !3
  %49 = inttoptr i64 %48 to i64*
  store i64 %45, i64* %49, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHRImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = zext i16 %7 to i64
  %9 = lshr i64 %4, %8
  %10 = and i64 %8, 63
  %11 = icmp eq i64 %10, 1
  %12 = and i64 %4, 1
  %13 = icmp ne i64 %12, 0
  %14 = and i64 %9, 1
  %15 = icmp ne i64 %14, 0
  %16 = select i1 %11, i1 %13, i1 %15
  %17 = trunc i64 %9 to i8
  %18 = tail call i8 @llvm.ctpop.i8(i8 %17) #3, !range !9
  %19 = icmp eq i64 %9, 0
  %20 = zext i1 %16 to i64
  %21 = shl nuw nsw i8 %18, 2
  %22 = and i8 %21, 4
  %23 = xor i8 %22, 4
  %24 = zext i8 %23 to i64
  %25 = select i1 %19, i64 64, i64 0
  %26 = lshr i64 %4, 52
  %27 = and i64 %26, 2048
  %28 = or i64 %25, %27
  %29 = or i64 %28, %20
  %30 = or i64 %29, %24
  %31 = or i64 %30, 16
  %32 = add i64 %2, 2
  store i64 %32, i64* %0, align 8, !tbaa !3
  %33 = inttoptr i64 %32 to i64*
  store i64 %9, i64* %33, align 1
  %34 = load i64, i64* %0, align 8, !tbaa !3
  %35 = add i64 %34, -8
  store i64 %35, i64* %0, align 8, !tbaa !3
  %36 = inttoptr i64 %35 to i64*
  store i64 %31, i64* %36, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHRIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i32*
  %4 = load i32, i32* %3, align 1
  %5 = add i64 %2, 4
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = zext i16 %7 to i32
  %9 = lshr i32 %4, %8
  %10 = and i32 %8, 31
  %11 = icmp eq i32 %10, 1
  %12 = and i32 %4, 1
  %13 = icmp ne i32 %12, 0
  %14 = and i32 %9, 1
  %15 = icmp ne i32 %14, 0
  %16 = select i1 %11, i1 %13, i1 %15
  %17 = trunc i32 %9 to i8
  %18 = tail call i8 @llvm.ctpop.i8(i8 %17) #3, !range !9
  %19 = icmp eq i32 %9, 0
  %20 = zext i1 %16 to i64
  %21 = shl nuw nsw i8 %18, 2
  %22 = and i8 %21, 4
  %23 = xor i8 %22, 4
  %24 = zext i8 %23 to i64
  %25 = select i1 %19, i64 64, i64 0
  %26 = lshr i32 %4, 20
  %27 = and i32 %26, 2048
  %28 = zext i32 %27 to i64
  %29 = or i64 %25, %28
  %30 = or i64 %29, %20
  %31 = or i64 %30, %24
  %32 = or i64 %31, 16
  %33 = add i64 %2, 2
  store i64 %33, i64* %0, align 8, !tbaa !3
  %34 = inttoptr i64 %33 to i32*
  store i32 %9, i32* %34, align 1
  %35 = load i64, i64* %0, align 8, !tbaa !3
  %36 = add i64 %35, -8
  store i64 %36, i64* %0, align 8, !tbaa !3
  %37 = inttoptr i64 %36 to i64*
  store i64 %32, i64* %37, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHRItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = zext i16 %4 to i32
  %9 = zext i16 %7 to i32
  %10 = lshr i32 %8, %9
  %11 = trunc i32 %10 to i16
  %12 = and i16 %7, 31
  %13 = icmp eq i16 %12, 1
  br i1 %13, label %14, label %17

14:                                               ; preds = %1
  %15 = and i16 %4, 1
  %16 = icmp ne i16 %15, 0
  br label %22

17:                                               ; preds = %1
  %18 = icmp ult i16 %12, 16
  br i1 %18, label %19, label %22

19:                                               ; preds = %17
  %20 = and i16 %11, 1
  %21 = icmp ne i16 %20, 0
  br label %22

22:                                               ; preds = %19, %17, %14
  %23 = phi i1 [ %16, %14 ], [ %21, %19 ], [ undef, %17 ]
  %24 = trunc i32 %10 to i8
  %25 = tail call i8 @llvm.ctpop.i8(i8 %24) #3, !range !9
  %26 = icmp eq i32 %10, 0
  %27 = zext i1 %23 to i64
  %28 = shl nuw nsw i8 %25, 2
  %29 = and i8 %28, 4
  %30 = xor i8 %29, 4
  %31 = zext i8 %30 to i64
  %32 = select i1 %26, i64 64, i64 0
  %33 = lshr i16 %4, 4
  %34 = and i16 %33, 2048
  %35 = zext i16 %34 to i64
  %36 = or i64 %32, %35
  %37 = or i64 %36, %31
  %38 = or i64 %37, %27
  %39 = or i64 %38, 16
  store i64 %5, i64* %0, align 8, !tbaa !3
  store i16 %11, i16* %6, align 1
  %40 = load i64, i64* %0, align 8, !tbaa !3
  %41 = add i64 %40, -8
  store i64 %41, i64* %0, align 8, !tbaa !3
  %42 = inttoptr i64 %41 to i64*
  store i64 %39, i64* %42, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z3SHRIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = trunc i16 %7 to i8
  %9 = and i16 %4, 255
  %10 = zext i16 %9 to i32
  %11 = and i16 %7, 255
  %12 = zext i16 %11 to i32
  %13 = lshr i32 %10, %12
  %14 = trunc i32 %13 to i8
  %15 = and i8 %8, 31
  %16 = icmp eq i8 %15, 1
  %17 = trunc i16 %4 to i8
  %18 = and i8 %17, 1
  %19 = icmp ult i8 %15, 8
  %20 = and i8 %14, 1
  %21 = select i1 %19, i8 %20, i8 undef
  %22 = select i1 %16, i8 %18, i8 %21
  %23 = icmp ne i8 %22, 0
  %24 = tail call i8 @llvm.ctpop.i8(i8 %14) #3, !range !9
  %25 = icmp eq i32 %13, 0
  %26 = zext i1 %23 to i64
  %27 = shl nuw nsw i8 %24, 2
  %28 = and i8 %27, 4
  %29 = xor i8 %28, 4
  %30 = zext i8 %29 to i64
  %31 = select i1 %25, i64 64, i64 0
  %32 = shl i16 %4, 4
  %33 = and i16 %32, 2048
  %34 = zext i16 %33 to i64
  %35 = or i64 %31, %34
  %36 = or i64 %35, %30
  %37 = or i64 %36, %26
  %38 = or i64 %37, 16
  %39 = trunc i32 %13 to i16
  store i64 %5, i64* %0, align 8, !tbaa !3
  store i16 %39, i16* %6, align 1
  %40 = load i64, i64* %0, align 8, !tbaa !3
  %41 = add i64 %40, -8
  store i64 %41, i64* %0, align 8, !tbaa !3
  %42 = inttoptr i64 %41 to i64*
  store i64 %38, i64* %42, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHLDImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64, i64* %6, align 1
  %8 = add i64 %2, 16
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i16*
  %10 = load i16, i16* %9, align 1
  %11 = and i16 %10, 63
  %12 = zext i16 %11 to i64
  %13 = shl i64 %4, %12
  %14 = sub nuw nsw i64 64, %12
  %15 = lshr i64 %7, %14
  %16 = or i64 %15, %13
  %17 = trunc i64 %16 to i8
  %18 = tail call i8 @llvm.ctpop.i8(i8 %17) #3, !range !9
  %19 = icmp eq i64 %16, 0
  %20 = xor i64 %16, %4
  %21 = lshr i64 %4, %14
  %22 = and i64 %21, 1
  %23 = shl nuw nsw i8 %18, 2
  %24 = and i8 %23, 4
  %25 = xor i8 %24, 4
  %26 = zext i8 %25 to i64
  %27 = select i1 %19, i64 64, i64 0
  %28 = lshr i64 %16, 56
  %29 = and i64 %28, 128
  %30 = lshr i64 %20, 52
  %31 = and i64 %30, 2048
  %32 = or i64 %27, %22
  %33 = or i64 %32, %29
  %34 = or i64 %33, %31
  %35 = or i64 %34, %26
  %36 = or i64 %35, 16
  %37 = add i64 %2, 10
  store i64 %37, i64* %0, align 8, !tbaa !3
  %38 = inttoptr i64 %37 to i64*
  store i64 %16, i64* %38, align 1
  %39 = load i64, i64* %0, align 8, !tbaa !3
  %40 = add i64 %39, -8
  store i64 %40, i64* %0, align 8, !tbaa !3
  %41 = inttoptr i64 %40 to i64*
  store i64 %36, i64* %41, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHLDIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i32*
  %4 = load i32, i32* %3, align 1
  %5 = add i64 %2, 4
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i32*
  %7 = load i32, i32* %6, align 1
  %8 = add i64 %2, 8
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i16*
  %10 = load i16, i16* %9, align 1
  %11 = and i16 %10, 31
  %12 = zext i32 %4 to i64
  %13 = zext i16 %11 to i32
  %14 = shl i32 %4, %13
  %15 = sub nuw nsw i16 32, %11
  %16 = zext i32 %7 to i64
  %17 = zext i16 %15 to i64
  %18 = lshr i64 %16, %17
  %19 = trunc i64 %18 to i32
  %20 = or i32 %14, %19
  %21 = trunc i32 %20 to i8
  %22 = tail call i8 @llvm.ctpop.i8(i8 %21) #3, !range !9
  %23 = icmp eq i32 %20, 0
  %24 = xor i32 %20, %4
  %25 = lshr i64 %12, %17
  %26 = and i64 %25, 1
  %27 = shl nuw nsw i8 %22, 2
  %28 = and i8 %27, 4
  %29 = xor i8 %28, 4
  %30 = zext i8 %29 to i64
  %31 = select i1 %23, i64 64, i64 0
  %32 = lshr i32 %20, 24
  %33 = and i32 %32, 128
  %34 = zext i32 %33 to i64
  %35 = lshr i32 %24, 20
  %36 = and i32 %35, 2048
  %37 = zext i32 %36 to i64
  %38 = or i64 %31, %26
  %39 = or i64 %38, %34
  %40 = or i64 %39, %37
  %41 = or i64 %40, %30
  %42 = or i64 %41, 16
  %43 = add i64 %2, 6
  store i64 %43, i64* %0, align 8, !tbaa !3
  %44 = inttoptr i64 %43 to i32*
  store i32 %20, i32* %44, align 1
  %45 = load i64, i64* %0, align 8, !tbaa !3
  %46 = add i64 %45, -8
  store i64 %46, i64* %0, align 8, !tbaa !3
  %47 = inttoptr i64 %46 to i64*
  store i64 %42, i64* %47, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHLDItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = add i64 %2, 4
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i16*
  %10 = load i16, i16* %9, align 1
  %11 = and i16 %10, 31
  %12 = zext i16 %4 to i64
  %13 = zext i16 %11 to i64
  %14 = shl nuw nsw i64 %12, %13
  %15 = sub nsw i16 16, %11
  %16 = zext i16 %7 to i64
  %17 = zext i16 %15 to i64
  %18 = lshr i64 %16, %17
  %19 = or i64 %18, %14
  %20 = trunc i64 %19 to i16
  %21 = trunc i64 %19 to i8
  %22 = tail call i8 @llvm.ctpop.i8(i8 %21) #3, !range !9
  %23 = icmp eq i16 %20, 0
  %24 = xor i16 %4, %20
  %25 = lshr i64 %12, %17
  %26 = and i64 %25, 1
  %27 = shl nuw nsw i8 %22, 2
  %28 = and i8 %27, 4
  %29 = xor i8 %28, 4
  %30 = zext i8 %29 to i64
  %31 = select i1 %23, i64 64, i64 0
  %32 = lshr i64 %19, 8
  %33 = and i64 %32, 128
  %34 = lshr i16 %24, 4
  %35 = and i16 %34, 2048
  %36 = zext i16 %35 to i64
  %37 = or i64 %33, %26
  %38 = or i64 %37, %31
  %39 = or i64 %38, %36
  %40 = or i64 %39, %30
  %41 = or i64 %40, 16
  store i64 %8, i64* %0, align 8, !tbaa !3
  store i16 %20, i16* %9, align 1
  %42 = load i64, i64* %0, align 8, !tbaa !3
  %43 = add i64 %42, -8
  store i64 %43, i64* %0, align 8, !tbaa !3
  %44 = inttoptr i64 %43 to i64*
  store i64 %41, i64* %44, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHLDIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i8*
  %4 = load i8, i8* %3, align 1
  %5 = add i64 %2, 1
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i8*
  %7 = load i8, i8* %6, align 1
  %8 = add i64 %2, 2
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i16*
  %10 = load i16, i16* %9, align 1
  %11 = trunc i16 %10 to i8
  %12 = and i8 %11, 31
  %13 = zext i8 %4 to i64
  %14 = zext i8 %12 to i64
  %15 = shl nuw nsw i64 %13, %14
  %16 = sub nsw i8 8, %12
  %17 = zext i8 %7 to i64
  %18 = zext i8 %16 to i64
  %19 = lshr i64 %17, %18
  %20 = or i64 %19, %15
  %21 = trunc i64 %20 to i8
  %22 = tail call i8 @llvm.ctpop.i8(i8 %21) #3, !range !9
  %23 = icmp eq i8 %21, 0
  %24 = xor i8 %4, %21
  %25 = lshr i64 %13, %18
  %26 = and i64 %25, 1
  %27 = shl nuw nsw i8 %22, 2
  %28 = and i8 %27, 4
  %29 = xor i8 %28, 4
  %30 = zext i8 %29 to i64
  %31 = select i1 %23, i64 64, i64 0
  %32 = and i64 %20, 128
  %33 = and i8 %24, -128
  %34 = zext i8 %33 to i64
  %35 = shl nuw nsw i64 %34, 4
  %36 = or i64 %32, %31
  %37 = or i64 %26, %36
  %38 = or i64 %37, %35
  %39 = or i64 %38, %30
  %40 = or i64 %39, 16
  %41 = add i64 %2, 3
  store i64 %41, i64* %0, align 8, !tbaa !3
  %42 = inttoptr i64 %41 to i8*
  store i8 %21, i8* %42, align 1
  %43 = load i64, i64* %0, align 8, !tbaa !3
  %44 = add i64 %43, -8
  store i64 %44, i64* %0, align 8, !tbaa !3
  %45 = inttoptr i64 %44 to i64*
  store i64 %40, i64* %45, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHRDImEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i64*
  %4 = load i64, i64* %3, align 1
  %5 = add i64 %2, 8
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i64*
  %7 = load i64, i64* %6, align 1
  %8 = add i64 %2, 16
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i16*
  %10 = load i16, i16* %9, align 1
  %11 = and i16 %10, 63
  %12 = zext i16 %11 to i64
  %13 = tail call i64 @llvm.fshr.i64(i64 %7, i64 %4, i64 %12)
  %14 = add nsw i64 %12, -1
  %15 = trunc i64 %13 to i8
  %16 = tail call i8 @llvm.ctpop.i8(i8 %15) #3, !range !9
  %17 = icmp eq i64 %13, 0
  %18 = xor i64 %13, %4
  %19 = lshr i64 %4, %14
  %20 = and i64 %19, 1
  %21 = shl nuw nsw i8 %16, 2
  %22 = and i8 %21, 4
  %23 = xor i8 %22, 4
  %24 = zext i8 %23 to i64
  %25 = select i1 %17, i64 64, i64 0
  %26 = lshr i64 %13, 56
  %27 = and i64 %26, 128
  %28 = lshr i64 %18, 52
  %29 = and i64 %28, 2048
  %30 = or i64 %27, %25
  %31 = or i64 %30, %20
  %32 = or i64 %31, %29
  %33 = or i64 %32, %24
  %34 = or i64 %33, 16
  %35 = add i64 %2, 10
  store i64 %35, i64* %0, align 8, !tbaa !3
  %36 = inttoptr i64 %35 to i64*
  store i64 %13, i64* %36, align 1
  %37 = load i64, i64* %0, align 8, !tbaa !3
  %38 = add i64 %37, -8
  store i64 %38, i64* %0, align 8, !tbaa !3
  %39 = inttoptr i64 %38 to i64*
  store i64 %34, i64* %39, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHRDIjEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i32*
  %4 = load i32, i32* %3, align 1
  %5 = add i64 %2, 4
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i32*
  %7 = load i32, i32* %6, align 1
  %8 = add i64 %2, 8
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i16*
  %10 = load i16, i16* %9, align 1
  %11 = and i16 %10, 31
  %12 = zext i16 %11 to i32
  %13 = sub nuw nsw i32 32, %12
  %14 = zext i32 %7 to i64
  %15 = zext i32 %13 to i64
  %16 = shl nuw i64 %14, %15
  %17 = trunc i64 %16 to i32
  %18 = zext i32 %4 to i64
  %19 = lshr i32 %4, %12
  %20 = or i32 %19, %17
  %21 = add nsw i32 %12, -1
  %22 = zext i32 %21 to i64
  %23 = trunc i32 %20 to i8
  %24 = tail call i8 @llvm.ctpop.i8(i8 %23) #3, !range !9
  %25 = icmp eq i32 %20, 0
  %26 = xor i32 %20, %4
  %27 = lshr i64 %18, %22
  %28 = and i64 %27, 1
  %29 = shl nuw nsw i8 %24, 2
  %30 = and i8 %29, 4
  %31 = xor i8 %30, 4
  %32 = zext i8 %31 to i64
  %33 = select i1 %25, i64 64, i64 0
  %34 = lshr i32 %20, 24
  %35 = and i32 %34, 128
  %36 = zext i32 %35 to i64
  %37 = lshr i32 %26, 20
  %38 = and i32 %37, 2048
  %39 = zext i32 %38 to i64
  %40 = or i64 %28, %33
  %41 = or i64 %40, %36
  %42 = or i64 %41, %39
  %43 = or i64 %42, %32
  %44 = or i64 %43, 16
  %45 = add i64 %2, 6
  store i64 %45, i64* %0, align 8, !tbaa !3
  %46 = inttoptr i64 %45 to i32*
  store i32 %20, i32* %46, align 1
  %47 = load i64, i64* %0, align 8, !tbaa !3
  %48 = add i64 %47, -8
  store i64 %48, i64* %0, align 8, !tbaa !3
  %49 = inttoptr i64 %48 to i64*
  store i64 %44, i64* %49, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHRDItEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i16*
  %4 = load i16, i16* %3, align 1
  %5 = add i64 %2, 2
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i16*
  %7 = load i16, i16* %6, align 1
  %8 = add i64 %2, 4
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i16*
  %10 = load i16, i16* %9, align 1
  %11 = and i16 %10, 31
  %12 = sub nsw i16 16, %11
  %13 = zext i16 %7 to i64
  %14 = zext i16 %12 to i64
  %15 = shl i64 %13, %14
  %16 = zext i16 %4 to i64
  %17 = zext i16 %11 to i64
  %18 = lshr i64 %16, %17
  %19 = or i64 %15, %18
  %20 = trunc i64 %19 to i16
  %21 = add nsw i16 %11, -1
  %22 = zext i16 %21 to i64
  %23 = trunc i64 %19 to i8
  %24 = tail call i8 @llvm.ctpop.i8(i8 %23) #3, !range !9
  %25 = icmp eq i16 %20, 0
  %26 = xor i16 %4, %20
  %27 = lshr i64 %16, %22
  %28 = and i64 %27, 1
  %29 = shl nuw nsw i8 %24, 2
  %30 = and i8 %29, 4
  %31 = xor i8 %30, 4
  %32 = zext i8 %31 to i64
  %33 = select i1 %25, i64 64, i64 0
  %34 = lshr i64 %19, 8
  %35 = and i64 %34, 128
  %36 = lshr i16 %26, 4
  %37 = and i16 %36, 2048
  %38 = zext i16 %37 to i64
  %39 = or i64 %35, %28
  %40 = or i64 %39, %33
  %41 = or i64 %40, %38
  %42 = or i64 %41, %32
  %43 = or i64 %42, 16
  store i64 %8, i64* %0, align 8, !tbaa !3
  store i16 %20, i16* %9, align 1
  %44 = load i64, i64* %0, align 8, !tbaa !3
  %45 = add i64 %44, -8
  store i64 %45, i64* %0, align 8, !tbaa !3
  %46 = inttoptr i64 %45 to i64*
  store i64 %43, i64* %46, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define linkonce_odr dso_local void @_Z4SHRDIhEvRm(i64* nonnull align 8 dereferenceable(8) %0) #0 comdat {
  %2 = load i64, i64* %0, align 8, !tbaa !3
  %3 = inttoptr i64 %2 to i8*
  %4 = load i8, i8* %3, align 1
  %5 = add i64 %2, 1
  store i64 %5, i64* %0, align 8, !tbaa !3
  %6 = inttoptr i64 %5 to i8*
  %7 = load i8, i8* %6, align 1
  %8 = add i64 %2, 2
  store i64 %8, i64* %0, align 8, !tbaa !3
  %9 = inttoptr i64 %8 to i16*
  %10 = load i16, i16* %9, align 1
  %11 = trunc i16 %10 to i8
  %12 = and i8 %11, 31
  %13 = sub nsw i8 8, %12
  %14 = zext i8 %7 to i64
  %15 = zext i8 %13 to i64
  %16 = shl i64 %14, %15
  %17 = zext i8 %4 to i64
  %18 = zext i8 %12 to i64
  %19 = lshr i64 %17, %18
  %20 = or i64 %16, %19
  %21 = trunc i64 %20 to i8
  %22 = add nsw i8 %12, -1
  %23 = zext i8 %22 to i64
  %24 = tail call i8 @llvm.ctpop.i8(i8 %21) #3, !range !9
  %25 = icmp eq i8 %21, 0
  %26 = xor i8 %4, %21
  %27 = lshr i64 %17, %23
  %28 = and i64 %27, 1
  %29 = shl nuw nsw i8 %24, 2
  %30 = and i8 %29, 4
  %31 = xor i8 %30, 4
  %32 = zext i8 %31 to i64
  %33 = select i1 %25, i64 64, i64 0
  %34 = and i64 %20, 128
  %35 = and i8 %26, -128
  %36 = zext i8 %35 to i64
  %37 = shl nuw nsw i64 %36, 4
  %38 = or i64 %34, %33
  %39 = or i64 %28, %38
  %40 = or i64 %39, %37
  %41 = or i64 %40, %32
  %42 = or i64 %41, 16
  %43 = add i64 %2, 3
  store i64 %43, i64* %0, align 8, !tbaa !3
  %44 = inttoptr i64 %43 to i8*
  store i8 %21, i8* %44, align 1
  %45 = load i64, i64* %0, align 8, !tbaa !3
  %46 = add i64 %45, -8
  store i64 %46, i64* %0, align 8, !tbaa !3
  %47 = inttoptr i64 %46 to i64*
  store i64 %42, i64* %47, align 1
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z4JUMPRmS_(i64* nocapture nonnull align 8 dereferenceable(8) %0, i64* nocapture nonnull align 8 dereferenceable(8) %1) #1 {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i64*
  %5 = load i64, i64* %4, align 1
  %6 = add i64 %3, 8
  store i64 %6, i64* %0, align 8, !tbaa !3
  store i64 %5, i64* %1, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z8JUMP_DECRmS_(i64* nocapture nonnull align 8 dereferenceable(8) %0, i64* nocapture nonnull align 8 dereferenceable(8) %1) #1 {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i64*
  %5 = load i64, i64* %4, align 1
  %6 = add i64 %3, 8
  store i64 %6, i64* %0, align 8, !tbaa !3
  %7 = add i64 %5, -4
  store i64 %7, i64* %1, align 8, !tbaa !3
  ret void
}

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn
define dso_local void @_Z8JUMP_INCRmS_(i64* nocapture nonnull align 8 dereferenceable(8) %0, i64* nocapture nonnull align 8 dereferenceable(8) %1) #1 {
  %3 = load i64, i64* %0, align 8, !tbaa !3
  %4 = inttoptr i64 %3 to i64*
  %5 = load i64, i64* %4, align 1
  %6 = add i64 %3, 8
  store i64 %6, i64* %0, align 8, !tbaa !3
  %7 = add i64 %5, 4
  store i64 %7, i64* %1, align 8, !tbaa !3
  ret void
}

; Function Attrs: mustprogress nounwind uwtable
define dso_local void @HelperUnsupported(i64* noalias nocapture nonnull align 8 dereferenceable(8) %0, i64* noalias nocapture nonnull align 8 dereferenceable(8) %1, i64* noalias nocapture nonnull align 8 dereferenceable(8) %2, i64* noalias nocapture nonnull align 8 dereferenceable(8) %3, i64* noalias nocapture nonnull align 8 dereferenceable(8) %4, i64* noalias nocapture nonnull align 8 dereferenceable(8) %5, i64* noalias nocapture nonnull align 8 dereferenceable(8) %6, i64* noalias nocapture nonnull align 8 dereferenceable(8) %7, i64* noalias nocapture nonnull align 8 dereferenceable(8) %8, i64* noalias nocapture nonnull align 8 dereferenceable(8) %9, i64* noalias nocapture nonnull align 8 dereferenceable(8) %10, i64* noalias nocapture nonnull align 8 dereferenceable(8) %11, i64* noalias nocapture nonnull align 8 dereferenceable(8) %12, i64* noalias nocapture nonnull align 8 dereferenceable(8) %13, i64* noalias nocapture nonnull align 8 dereferenceable(8) %14, i64* noalias nocapture nonnull align 8 dereferenceable(8) %15) local_unnamed_addr #4 {
  %17 = alloca %3, align 8
  %18 = bitcast %3* %17 to i8*
  call void @llvm.lifetime.start.p0i8(i64 128, i8* nonnull %18) #3
  %19 = load i64, i64* %0, align 8, !tbaa !3
  %20 = getelementptr inbounds %3, %3* %17, i64 0, i32 0, i32 0, i32 0
  store i64 %19, i64* %20, align 8, !tbaa !7
  %21 = load i64, i64* %1, align 8, !tbaa !3
  %22 = getelementptr inbounds %3, %3* %17, i64 0, i32 1, i32 0, i32 0
  store i64 %21, i64* %22, align 8, !tbaa !7
  %23 = load i64, i64* %2, align 8, !tbaa !3
  %24 = getelementptr inbounds %3, %3* %17, i64 0, i32 2, i32 0, i32 0
  store i64 %23, i64* %24, align 8, !tbaa !7
  %25 = load i64, i64* %3, align 8, !tbaa !3
  %26 = getelementptr inbounds %3, %3* %17, i64 0, i32 3, i32 0, i32 0
  store i64 %25, i64* %26, align 8, !tbaa !7
  %27 = load i64, i64* %4, align 8, !tbaa !3
  %28 = getelementptr inbounds %3, %3* %17, i64 0, i32 4, i32 0, i32 0
  store i64 %27, i64* %28, align 8, !tbaa !7
  %29 = load i64, i64* %5, align 8, !tbaa !3
  %30 = getelementptr inbounds %3, %3* %17, i64 0, i32 5, i32 0, i32 0
  store i64 %29, i64* %30, align 8, !tbaa !7
  %31 = load i64, i64* %6, align 8, !tbaa !3
  %32 = getelementptr inbounds %3, %3* %17, i64 0, i32 6, i32 0, i32 0
  store i64 %31, i64* %32, align 8, !tbaa !7
  %33 = load i64, i64* %7, align 8, !tbaa !3
  %34 = getelementptr inbounds %3, %3* %17, i64 0, i32 7, i32 0, i32 0
  store i64 %33, i64* %34, align 8, !tbaa !7
  %35 = load i64, i64* %8, align 8, !tbaa !3
  %36 = getelementptr inbounds %3, %3* %17, i64 0, i32 8, i32 0, i32 0
  store i64 %35, i64* %36, align 8, !tbaa !7
  %37 = load i64, i64* %9, align 8, !tbaa !3
  %38 = getelementptr inbounds %3, %3* %17, i64 0, i32 9, i32 0, i32 0
  store i64 %37, i64* %38, align 8, !tbaa !7
  %39 = load i64, i64* %10, align 8, !tbaa !3
  %40 = getelementptr inbounds %3, %3* %17, i64 0, i32 10, i32 0, i32 0
  store i64 %39, i64* %40, align 8, !tbaa !7
  %41 = load i64, i64* %11, align 8, !tbaa !3
  %42 = getelementptr inbounds %3, %3* %17, i64 0, i32 11, i32 0, i32 0
  store i64 %41, i64* %42, align 8, !tbaa !7
  %43 = load i64, i64* %12, align 8, !tbaa !3
  %44 = getelementptr inbounds %3, %3* %17, i64 0, i32 12, i32 0, i32 0
  store i64 %43, i64* %44, align 8, !tbaa !7
  %45 = load i64, i64* %13, align 8, !tbaa !3
  %46 = getelementptr inbounds %3, %3* %17, i64 0, i32 13, i32 0, i32 0
  store i64 %45, i64* %46, align 8, !tbaa !7
  %47 = load i64, i64* %14, align 8, !tbaa !3
  %48 = getelementptr inbounds %3, %3* %17, i64 0, i32 14, i32 0, i32 0
  store i64 %47, i64* %48, align 8, !tbaa !7
  %49 = load i64, i64* %15, align 8, !tbaa !3
  %50 = getelementptr inbounds %3, %3* %17, i64 0, i32 15, i32 0, i32 0
  store i64 %49, i64* %50, align 8, !tbaa !7
  call void @HelperUnsupportedStub(%3* nonnull align 1 dereferenceable(128) %17) #3
  %51 = load i64, i64* %20, align 8, !tbaa !7
  store i64 %51, i64* %0, align 8, !tbaa !3
  %52 = load i64, i64* %22, align 8, !tbaa !7
  store i64 %52, i64* %1, align 8, !tbaa !3
  %53 = load i64, i64* %24, align 8, !tbaa !7
  store i64 %53, i64* %2, align 8, !tbaa !3
  %54 = load i64, i64* %26, align 8, !tbaa !7
  store i64 %54, i64* %3, align 8, !tbaa !3
  %55 = load i64, i64* %28, align 8, !tbaa !7
  store i64 %55, i64* %4, align 8, !tbaa !3
  %56 = load i64, i64* %30, align 8, !tbaa !7
  store i64 %56, i64* %5, align 8, !tbaa !3
  %57 = load i64, i64* %32, align 8, !tbaa !7
  store i64 %57, i64* %6, align 8, !tbaa !3
  %58 = load i64, i64* %34, align 8, !tbaa !7
  store i64 %58, i64* %7, align 8, !tbaa !3
  %59 = load i64, i64* %36, align 8, !tbaa !7
  store i64 %59, i64* %8, align 8, !tbaa !3
  %60 = load i64, i64* %38, align 8, !tbaa !7
  store i64 %60, i64* %9, align 8, !tbaa !3
  %61 = load i64, i64* %40, align 8, !tbaa !7
  store i64 %61, i64* %10, align 8, !tbaa !3
  %62 = load i64, i64* %42, align 8, !tbaa !7
  store i64 %62, i64* %11, align 8, !tbaa !3
  %63 = load i64, i64* %44, align 8, !tbaa !7
  store i64 %63, i64* %12, align 8, !tbaa !3
  %64 = load i64, i64* %46, align 8, !tbaa !7
  store i64 %64, i64* %13, align 8, !tbaa !3
  %65 = load i64, i64* %48, align 8, !tbaa !7
  store i64 %65, i64* %14, align 8, !tbaa !3
  %66 = load i64, i64* %50, align 8, !tbaa !7
  store i64 %66, i64* %15, align 8, !tbaa !3
  call void @llvm.lifetime.end.p0i8(i64 128, i8* nonnull %18) #3
  ret void
}

declare dso_local void @HelperUnsupportedStub(%3* nonnull align 1 dereferenceable(128)) local_unnamed_addr #5

; Function Attrs: alwaysinline mustprogress nofree norecurse nosync nounwind readonly uwtable willreturn
define dso_local i64 @HelperStubEmpty(i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %0, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %1, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %2, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %3, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %4, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %5, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %6, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %7, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %8, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %9, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %10, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %11, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %12, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %13, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %14, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %15, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %16, i64 %17, i64 %18, i64 %19, i64* noalias nocapture nonnull readnone align 8 dereferenceable(8) %20, i64* noalias nocapture nonnull readonly align 8 dereferenceable(8) %21, %0* noalias nocapture readnone %22, i64* noalias nocapture readnone %23) local_unnamed_addr #6 {
  %25 = load i64, i64* %21, align 8, !tbaa !3
  ret i64 %25
}

; Function Attrs: alwaysinline mustprogress nounwind uwtable
define dso_local i64 @HelperFunction(i64* noalias nonnull align 8 dereferenceable(8) %0, i64* noalias nonnull align 8 dereferenceable(8) %1, i64* noalias nonnull align 8 dereferenceable(8) %2, i64* noalias nonnull align 8 dereferenceable(8) %3, i64* noalias nonnull align 8 dereferenceable(8) %4, i64* noalias nonnull align 8 dereferenceable(8) %5, i64* noalias nonnull align 8 dereferenceable(8) %6, i64* noalias nonnull align 8 dereferenceable(8) %7, i64* noalias nonnull align 8 dereferenceable(8) %8, i64* noalias nonnull align 8 dereferenceable(8) %9, i64* noalias nonnull align 8 dereferenceable(8) %10, i64* noalias nonnull align 8 dereferenceable(8) %11, i64* noalias nonnull align 8 dereferenceable(8) %12, i64* noalias nonnull align 8 dereferenceable(8) %13, i64* noalias nonnull align 8 dereferenceable(8) %14, i64* noalias nonnull align 8 dereferenceable(8) %15, i64* noalias nonnull align 8 dereferenceable(8) %16, i64 %17, i64 %18, i64 %19) local_unnamed_addr #0 {
  %21 = alloca [30 x %0], align 16
  %22 = alloca [30 x i64], align 16
  %23 = alloca i64, align 8
  %24 = bitcast [30 x %0]* %21 to i8*
  call void @llvm.lifetime.start.p0i8(i64 240, i8* nonnull %24) #3
  call void @llvm.memset.p0i8.i64(i8* noundef nonnull align 16 dereferenceable(240) %24, i8 0, i64 240, i1 false)
  %25 = bitcast [30 x i64]* %22 to i8*
  call void @llvm.lifetime.start.p0i8(i64 240, i8* nonnull %25) #3
  call void @llvm.memset.p0i8.i64(i8* noundef nonnull align 16 dereferenceable(240) %25, i8 0, i64 240, i1 false)
  %26 = bitcast i64* %23 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %26) #3
  store i64 0, i64* %23, align 8, !tbaa !3
  %27 = getelementptr inbounds [30 x %0], [30 x %0]* %21, i64 0, i64 0
  %28 = getelementptr inbounds [30 x i64], [30 x i64]* %22, i64 0, i64 0
  %29 = call i64 @HelperStub(i64* nonnull align 8 dereferenceable(8) %0, i64* nonnull align 8 dereferenceable(8) %1, i64* nonnull align 8 dereferenceable(8) %2, i64* nonnull align 8 dereferenceable(8) %3, i64* nonnull align 8 dereferenceable(8) %4, i64* nonnull align 8 dereferenceable(8) %5, i64* nonnull align 8 dereferenceable(8) %6, i64* nonnull align 8 dereferenceable(8) %7, i64* nonnull align 8 dereferenceable(8) %8, i64* nonnull align 8 dereferenceable(8) %9, i64* nonnull align 8 dereferenceable(8) %10, i64* nonnull align 8 dereferenceable(8) %11, i64* nonnull align 8 dereferenceable(8) %12, i64* nonnull align 8 dereferenceable(8) %13, i64* nonnull align 8 dereferenceable(8) %14, i64* nonnull align 8 dereferenceable(8) %15, i64* nonnull align 8 dereferenceable(8) %16, i64 %17, i64 %18, i64 0, i64* nonnull align 8 dereferenceable(8) %7, i64* nonnull align 8 dereferenceable(8) %23, %0* nonnull %27, i64* nonnull %28) #3
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %26) #3
  call void @llvm.lifetime.end.p0i8(i64 240, i8* nonnull %25) #3
  call void @llvm.lifetime.end.p0i8(i64 240, i8* nonnull %24) #3
  ret i64 %29
}

; Function Attrs: argmemonly nofree nounwind willreturn writeonly
declare void @llvm.memset.p0i8.i64(i8* nocapture writeonly, i8, i64, i1 immarg) #7

declare dso_local i64 @HelperStub(i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), i64, i64, i64, i64* nonnull align 8 dereferenceable(8), i64* nonnull align 8 dereferenceable(8), %0*, i64*) local_unnamed_addr #5

; Function Attrs: mustprogress nounwind uwtable
define dso_local i64 @HelperSlicePC(i64 %0, i64 %1, i64 %2, i64 %3, i64 %4, i64 %5, i64 %6, i64 %7, i64 %8, i64 %9, i64 %10, i64 %11, i64 %12, i64 %13, i64 %14, i64 %15, i64 %16, i64 %17, i64 %18, i64 %19) local_unnamed_addr #4 {
  %21 = alloca i64, align 8
  %22 = alloca i64, align 8
  %23 = alloca i64, align 8
  %24 = alloca i64, align 8
  %25 = alloca i64, align 8
  %26 = alloca i64, align 8
  %27 = alloca i64, align 8
  %28 = alloca i64, align 8
  %29 = alloca i64, align 8
  %30 = alloca i64, align 8
  %31 = alloca i64, align 8
  %32 = alloca i64, align 8
  %33 = alloca i64, align 8
  %34 = alloca i64, align 8
  %35 = alloca i64, align 8
  %36 = alloca i64, align 8
  %37 = alloca i64, align 8
  %38 = alloca [30 x %0], align 16
  %39 = alloca [30 x i64], align 16
  %40 = alloca i64, align 8
  %41 = alloca i64, align 8
  store i64 %0, i64* %21, align 8, !tbaa !3
  store i64 %1, i64* %22, align 8, !tbaa !3
  store i64 %2, i64* %23, align 8, !tbaa !3
  store i64 %3, i64* %24, align 8, !tbaa !3
  store i64 %4, i64* %25, align 8, !tbaa !3
  store i64 %5, i64* %26, align 8, !tbaa !3
  store i64 %6, i64* %27, align 8, !tbaa !3
  store i64 %7, i64* %28, align 8, !tbaa !3
  store i64 %8, i64* %29, align 8, !tbaa !3
  store i64 %9, i64* %30, align 8, !tbaa !3
  store i64 %10, i64* %31, align 8, !tbaa !3
  store i64 %11, i64* %32, align 8, !tbaa !3
  store i64 %12, i64* %33, align 8, !tbaa !3
  store i64 %13, i64* %34, align 8, !tbaa !3
  store i64 %14, i64* %35, align 8, !tbaa !3
  store i64 %15, i64* %36, align 8, !tbaa !3
  store i64 %16, i64* %37, align 8, !tbaa !3
  %42 = bitcast [30 x %0]* %38 to i8*
  call void @llvm.lifetime.start.p0i8(i64 240, i8* nonnull %42) #3
  call void @llvm.memset.p0i8.i64(i8* noundef nonnull align 16 dereferenceable(240) %42, i8 0, i64 240, i1 false)
  %43 = bitcast [30 x i64]* %39 to i8*
  call void @llvm.lifetime.start.p0i8(i64 240, i8* nonnull %43) #3
  call void @llvm.memset.p0i8.i64(i8* noundef nonnull align 16 dereferenceable(240) %43, i8 0, i64 240, i1 false)
  %44 = bitcast i64* %40 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %44) #3
  store i64 %7, i64* %40, align 8, !tbaa !3
  %45 = bitcast i64* %41 to i8*
  call void @llvm.lifetime.start.p0i8(i64 8, i8* nonnull %45) #3
  store i64 0, i64* %41, align 8, !tbaa !3
  %46 = getelementptr inbounds [30 x %0], [30 x %0]* %38, i64 0, i64 0
  %47 = getelementptr inbounds [30 x i64], [30 x i64]* %39, i64 0, i64 0
  %48 = call i64 @HelperStub(i64* nonnull align 8 dereferenceable(8) %21, i64* nonnull align 8 dereferenceable(8) %22, i64* nonnull align 8 dereferenceable(8) %23, i64* nonnull align 8 dereferenceable(8) %24, i64* nonnull align 8 dereferenceable(8) %25, i64* nonnull align 8 dereferenceable(8) %26, i64* nonnull align 8 dereferenceable(8) %27, i64* nonnull align 8 dereferenceable(8) %28, i64* nonnull align 8 dereferenceable(8) %29, i64* nonnull align 8 dereferenceable(8) %30, i64* nonnull align 8 dereferenceable(8) %31, i64* nonnull align 8 dereferenceable(8) %32, i64* nonnull align 8 dereferenceable(8) %33, i64* nonnull align 8 dereferenceable(8) %34, i64* nonnull align 8 dereferenceable(8) %35, i64* nonnull align 8 dereferenceable(8) %36, i64* nonnull align 8 dereferenceable(8) %37, i64 %17, i64 %18, i64 0, i64* nonnull align 8 dereferenceable(8) %40, i64* nonnull align 8 dereferenceable(8) %41, %0* nonnull %46, i64* nonnull %47) #3
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %45) #3
  call void @llvm.lifetime.end.p0i8(i64 8, i8* nonnull %44) #3
  call void @llvm.lifetime.end.p0i8(i64 240, i8* nonnull %43) #3
  call void @llvm.lifetime.end.p0i8(i64 240, i8* nonnull %42) #3
  ret i64 %48
}

; Function Attrs: mustprogress nofree norecurse nosync nounwind uwtable willreturn writeonly
define dso_local void @retainPointers() local_unnamed_addr #8 {
  store i8 0, i8* getelementptr inbounds ([0 x i8], [0 x i8]* @GS, i64 0, i64 0), align 1, !tbaa !7
  store i8 0, i8* getelementptr inbounds ([0 x i8], [0 x i8]* @FS, i64 0, i64 0), align 1, !tbaa !7
  ret void
}

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare i8 @llvm.ctpop.i8(i8) #9

; Function Attrs: nofree nosync nounwind readnone speculatable willreturn
declare i64 @llvm.fshr.i64(i64, i64, i64) #9

; Function Attrs: alwaysinline
define i64 @helperstub_1402454ad(i64* noalias %0, i64* noalias %1, i64* noalias %2, i64* noalias %3, i64* noalias %4, i64* noalias %5, i64* noalias %6, i64* noalias %7, i64* noalias %8, i64* noalias %9, i64* noalias %10, i64* noalias %11, i64* noalias %12, i64* noalias %13, i64* noalias %14, i64* noalias %15, i64* noalias %16, i64 %17, i64 %18, i64 %19, i64* noalias %20, i64* noalias %21, %0* noalias %22, i64* noalias %23) #10 {
  %25 = load i64, i64* %20, align 8, !tbaa !3
  %26 = add i64 %25, -8
  store i64 %26, i64* %20, align 8, !tbaa !3
  %27 = inttoptr i64 %26 to i64*
  store i64 189322307, i64* %27, align 1
  %28 = load i64, i64* %20, align 8, !tbaa !3
  %29 = add i64 %28, -8
  store i64 %29, i64* %20, align 8, !tbaa !3
  %30 = inttoptr i64 %29 to i64*
  store i64 5376664256, i64* %30, align 1
  %31 = load i64, i64* %14, align 8
  %32 = load i64, i64* %20, align 8, !tbaa !3
  %33 = add i64 %32, -8
  store i64 %33, i64* %20, align 8, !tbaa !3
  %34 = inttoptr i64 %33 to i64*
  store i64 %31, i64* %34, align 1
  %35 = load i64, i64* %16, align 8
  %36 = load i64, i64* %20, align 8, !tbaa !3
  %37 = add i64 %36, -8
  store i64 %37, i64* %20, align 8, !tbaa !3
  %38 = inttoptr i64 %37 to i64*
  store i64 %35, i64* %38, align 1
  %39 = load i64, i64* %8, align 8
  %40 = load i64, i64* %20, align 8, !tbaa !3
  %41 = add i64 %40, -8
  store i64 %41, i64* %20, align 8, !tbaa !3
  %42 = inttoptr i64 %41 to i64*
  store i64 %39, i64* %42, align 1
  %43 = load i64, i64* %2, align 8
  %44 = load i64, i64* %20, align 8, !tbaa !3
  %45 = add i64 %44, -8
  store i64 %45, i64* %20, align 8, !tbaa !3
  %46 = inttoptr i64 %45 to i64*
  store i64 %43, i64* %46, align 1
  %47 = load i64, i64* %13, align 8
  %48 = load i64, i64* %20, align 8, !tbaa !3
  %49 = add i64 %48, -8
  store i64 %49, i64* %20, align 8, !tbaa !3
  %50 = inttoptr i64 %49 to i64*
  store i64 %47, i64* %50, align 1
  %51 = load i64, i64* %5, align 8
  %52 = load i64, i64* %20, align 8, !tbaa !3
  %53 = add i64 %52, -8
  store i64 %53, i64* %20, align 8, !tbaa !3
  %54 = inttoptr i64 %53 to i64*
  store i64 %51, i64* %54, align 1
  %55 = load i64, i64* %3, align 8
  %56 = load i64, i64* %20, align 8, !tbaa !3
  %57 = add i64 %56, -8
  store i64 %57, i64* %20, align 8, !tbaa !3
  %58 = inttoptr i64 %57 to i64*
  store i64 %55, i64* %58, align 1
  %59 = load i64, i64* %0, align 8
  %60 = load i64, i64* %20, align 8, !tbaa !3
  %61 = add i64 %60, -8
  store i64 %61, i64* %20, align 8, !tbaa !3
  %62 = inttoptr i64 %61 to i64*
  store i64 %59, i64* %62, align 1
  %63 = load i64, i64* %6, align 8
  %64 = load i64, i64* %20, align 8, !tbaa !3
  %65 = add i64 %64, -8
  store i64 %65, i64* %20, align 8, !tbaa !3
  %66 = inttoptr i64 %65 to i64*
  store i64 %63, i64* %66, align 1
  %67 = load i64, i64* %15, align 8
  %68 = load i64, i64* %20, align 8, !tbaa !3
  %69 = add i64 %68, -8
  store i64 %69, i64* %20, align 8, !tbaa !3
  %70 = inttoptr i64 %69 to i64*
  store i64 %67, i64* %70, align 1
  %71 = load i64, i64* %12, align 8
  %72 = load i64, i64* %20, align 8, !tbaa !3
  %73 = add i64 %72, -8
  store i64 %73, i64* %20, align 8, !tbaa !3
  %74 = inttoptr i64 %73 to i64*
  store i64 %71, i64* %74, align 1
  %75 = load i64, i64* %10, align 8
  %76 = load i64, i64* %20, align 8, !tbaa !3
  %77 = add i64 %76, -8
  store i64 %77, i64* %20, align 8, !tbaa !3
  %78 = inttoptr i64 %77 to i64*
  store i64 %75, i64* %78, align 1
  %79 = load i64, i64* %1, align 8
  %80 = load i64, i64* %20, align 8, !tbaa !3
  %81 = add i64 %80, -8
  store i64 %81, i64* %20, align 8, !tbaa !3
  %82 = inttoptr i64 %81 to i64*
  store i64 %79, i64* %82, align 1
  %83 = load i64, i64* %11, align 8
  %84 = load i64, i64* %20, align 8, !tbaa !3
  %85 = add i64 %84, -8
  store i64 %85, i64* %20, align 8, !tbaa !3
  %86 = inttoptr i64 %85 to i64*
  store i64 %83, i64* %86, align 1
  %87 = load i64, i64* %4, align 8
  %88 = load i64, i64* %20, align 8, !tbaa !3
  %89 = add i64 %88, -8
  store i64 %89, i64* %20, align 8, !tbaa !3
  %90 = inttoptr i64 %89 to i64*
  store i64 %87, i64* %90, align 1
  %91 = load i64, i64* %9, align 8
  %92 = load i64, i64* %20, align 8, !tbaa !3
  %93 = add i64 %92, -8
  store i64 %93, i64* %20, align 8, !tbaa !3
  %94 = inttoptr i64 %93 to i64*
  store i64 %91, i64* %94, align 1
  %95 = load i64, i64* %20, align 8, !tbaa !3
  %96 = add i64 %95, -8
  store i64 %96, i64* %20, align 8, !tbaa !3
  %97 = inttoptr i64 %96 to i64*
  store i64 0, i64* %97, align 1
  %98 = load i64, i64* %20, align 8, !tbaa !3
  %99 = inttoptr i64 %98 to i64*
  %100 = load i64, i64* %99, align 1
  %101 = getelementptr %0, %0* %22, i64 7, i32 0, i32 0
  store i64 %100, i64* %101, align 1, !tbaa !7
  %102 = add i64 %98, 8
  store i64 %102, i64* %20, align 8, !tbaa !3
  %103 = inttoptr i64 %102 to i64*
  %104 = load i64, i64* %103, align 1
  %105 = getelementptr %0, %0* %22, i64 5, i32 0, i32 0
  store i64 %104, i64* %105, align 1, !tbaa !7
  %106 = add i64 %98, 16
  store i64 %106, i64* %20, align 8, !tbaa !3
  %107 = inttoptr i64 %106 to i64*
  %108 = load i64, i64* %107, align 1
  %109 = getelementptr %0, %0* %22, i64 4, i32 0, i32 0
  store i64 %108, i64* %109, align 1, !tbaa !7
  %110 = add i64 %98, 24
  store i64 %110, i64* %20, align 8, !tbaa !3
  %111 = inttoptr i64 %110 to i64*
  %112 = load i64, i64* %111, align 1
  %113 = getelementptr %0, %0* %22, i64 10, i32 0, i32 0
  store i64 %112, i64* %113, align 1, !tbaa !7
  %114 = add i64 %98, 32
  store i64 %114, i64* %20, align 8, !tbaa !3
  %115 = inttoptr i64 %114 to i64*
  %116 = load i64, i64* %115, align 1
  %117 = getelementptr %0, %0* %22, i64 0, i32 0, i32 0
  store i64 %116, i64* %117, align 1, !tbaa !7
  %118 = add i64 %98, 40
  store i64 %118, i64* %20, align 8, !tbaa !3
  %119 = inttoptr i64 %118 to i64*
  %120 = load i64, i64* %119, align 1
  %121 = getelementptr %0, %0* %22, i64 9, i32 0, i32 0
  store i64 %120, i64* %121, align 1, !tbaa !7
  %122 = add i64 %98, 48
  store i64 %122, i64* %20, align 8, !tbaa !3
  %123 = inttoptr i64 %122 to i64*
  %124 = load i64, i64* %123, align 1
  %125 = getelementptr %0, %0* %22, i64 1, i32 0, i32 0
  store i64 %124, i64* %125, align 1, !tbaa !7
  %126 = add i64 %98, 56
  store i64 %126, i64* %20, align 8, !tbaa !3
  %127 = inttoptr i64 %126 to i64*
  %128 = load i64, i64* %127, align 1
  %129 = getelementptr %0, %0* %22, i64 13, i32 0, i32 0
  store i64 %128, i64* %129, align 1, !tbaa !7
  %130 = add i64 %98, 64
  store i64 %130, i64* %20, align 8, !tbaa !3
  %131 = inttoptr i64 %130 to i64*
  %132 = load i64, i64* %131, align 1
  %133 = getelementptr %0, %0* %22, i64 23, i32 0, i32 0
  store i64 %132, i64* %133, align 1, !tbaa !7
  %134 = add i64 %98, 72
  store i64 %134, i64* %20, align 8, !tbaa !3
  %135 = inttoptr i64 %134 to i64*
  %136 = load i64, i64* %135, align 1
  %137 = getelementptr %0, %0* %22, i64 15, i32 0, i32 0
  store i64 %136, i64* %137, align 1, !tbaa !7
  %138 = add i64 %98, 80
  store i64 %138, i64* %20, align 8, !tbaa !3
  %139 = inttoptr i64 %138 to i64*
  %140 = load i64, i64* %139, align 1
  %141 = getelementptr %0, %0* %22, i64 22, i32 0, i32 0
  store i64 %140, i64* %141, align 1, !tbaa !7
  %142 = add i64 %98, 88
  store i64 %142, i64* %20, align 8, !tbaa !3
  %143 = inttoptr i64 %142 to i64*
  %144 = load i64, i64* %143, align 1
  %145 = getelementptr %0, %0* %22, i64 19, i32 0, i32 0
  store i64 %144, i64* %145, align 1, !tbaa !7
  %146 = add i64 %98, 96
  store i64 %146, i64* %20, align 8, !tbaa !3
  %147 = inttoptr i64 %146 to i64*
  %148 = load i64, i64* %147, align 1
  %149 = getelementptr %0, %0* %22, i64 2, i32 0, i32 0
  store i64 %148, i64* %149, align 1, !tbaa !7
  %150 = add i64 %98, 104
  store i64 %150, i64* %20, align 8, !tbaa !3
  %151 = inttoptr i64 %150 to i64*
  %152 = load i64, i64* %151, align 1
  %153 = getelementptr %0, %0* %22, i64 20, i32 0, i32 0
  store i64 %152, i64* %153, align 1, !tbaa !7
  %154 = add i64 %98, 112
  store i64 %154, i64* %20, align 8, !tbaa !3
  %155 = inttoptr i64 %154 to i64*
  %156 = load i64, i64* %155, align 1
  %157 = getelementptr %0, %0* %22, i64 8, i32 0, i32 0
  store i64 %156, i64* %157, align 1, !tbaa !7
  %158 = add i64 %98, 120
  store i64 %158, i64* %20, align 8, !tbaa !3
  %159 = inttoptr i64 %158 to i64*
  %160 = load i64, i64* %159, align 1
  %161 = getelementptr %0, %0* %22, i64 21, i32 0, i32 0
  store i64 %160, i64* %161, align 1, !tbaa !7
  %162 = add i64 %98, 128
  store i64 %162, i64* %20, align 8, !tbaa !3
  %163 = inttoptr i64 %162 to i64*
  %164 = load i64, i64* %163, align 1
  %165 = getelementptr %0, %0* %22, i64 6, i32 0, i32 0
  store i64 %164, i64* %165, align 1, !tbaa !7
  %166 = add i64 %98, 136
  store i64 %166, i64* %20, align 8, !tbaa !3
  %167 = inttoptr i64 %166 to i64*
  %168 = load i64, i64* %167, align 1
  %169 = getelementptr %0, %0* %22, i64 18, i32 0, i32 0
  store i64 %168, i64* %169, align 1, !tbaa !7
  %170 = add i64 %98, 144
  store i64 %170, i64* %20, align 8, !tbaa !3
  %171 = inttoptr i64 %170 to i64*
  %172 = load i64, i64* %171, align 1
  %173 = getelementptr %0, %0* %22, i64 3, i32 0, i32 0
  store i64 %172, i64* %173, align 1, !tbaa !7
  store i64 5368793250, i64* %171, align 1
  %174 = load i64, i64* %101, align 8
  %175 = load i64, i64* %20, align 8, !tbaa !3
  %176 = add i64 %175, -8
  store i64 %176, i64* %20, align 8, !tbaa !3
  %177 = inttoptr i64 %176 to i64*
  store i64 %174, i64* %177, align 1
  %178 = load i64, i64* %20, align 8, !tbaa !3
  %179 = inttoptr i64 %178 to i64*
  %180 = load i64, i64* %179, align 1
  %181 = add i64 %178, 8
  store i64 %181, i64* %20, align 8, !tbaa !3
  %182 = inttoptr i64 %181 to i64*
  %183 = load i64, i64* %182, align 1
  %184 = add i64 %183, %180
  %185 = icmp ult i64 %184, %180
  %186 = icmp ult i64 %184, %183
  %187 = or i1 %185, %186
  %188 = trunc i64 %184 to i8
  %189 = call i8 @llvm.ctpop.i8(i8 %188) #3, !range !9
  %190 = xor i64 %183, %180
  %191 = xor i64 %190, %184
  %192 = and i64 %191, 16
  %193 = icmp eq i64 %184, 0
  %194 = lshr i64 %180, 63
  %195 = lshr i64 %183, 63
  %196 = lshr i64 %184, 63
  %197 = xor i64 %196, %194
  %198 = xor i64 %196, %195
  %199 = add nuw nsw i64 %197, %198
  %200 = icmp eq i64 %199, 2
  %201 = zext i1 %187 to i64
  %202 = shl nuw nsw i8 %189, 2
  %203 = and i8 %202, 4
  %204 = xor i8 %203, 4
  %205 = zext i8 %204 to i64
  %206 = select i1 %193, i64 64, i64 0
  %207 = lshr i64 %184, 56
  %208 = and i64 %207, 128
  %209 = select i1 %200, i64 2048, i64 0
  %210 = or i64 %208, %206
  %211 = or i64 %210, %192
  %212 = or i64 %211, %201
  %213 = or i64 %212, %209
  %214 = or i64 %213, %205
  store i64 %184, i64* %182, align 1
  %215 = load i64, i64* %20, align 8, !tbaa !3
  %216 = add i64 %215, -8
  store i64 %216, i64* %20, align 8, !tbaa !3
  %217 = inttoptr i64 %216 to i64*
  store i64 %214, i64* %217, align 1
  %218 = load i64, i64* %20, align 8, !tbaa !3
  %219 = inttoptr i64 %218 to i64*
  %220 = load i64, i64* %219, align 1
  %221 = getelementptr %0, %0* %22, i64 12, i32 0, i32 0
  store i64 %220, i64* %221, align 1, !tbaa !7
  %222 = add i64 %218, 8
  store i64 %222, i64* %20, align 8, !tbaa !3
  %223 = inttoptr i64 %222 to i64*
  %224 = load i64, i64* %223, align 1
  %225 = getelementptr %0, %0* %22, i64 24, i32 0, i32 0
  store i64 %224, i64* %225, align 1, !tbaa !7
  %226 = add i64 %218, 16
  store i64 %226, i64* %223, align 1
  %227 = load i64, i64* %20, align 8, !tbaa !3
  %228 = inttoptr i64 %227 to i64*
  %229 = load i64, i64* %228, align 1
  %230 = getelementptr %0, %0* %22, i64 25, i32 0, i32 0
  store i64 %229, i64* %230, align 1, !tbaa !7
  %231 = load i64, i64* %133, align 8
  store i64 %231, i64* %228, align 1
  %232 = load i64, i64* %20, align 8, !tbaa !3
  %233 = inttoptr i64 %232 to i64*
  %234 = load i64, i64* %233, align 1
  %235 = getelementptr %0, %0* %22, i64 26, i32 0, i32 0
  store i64 %234, i64* %235, align 1, !tbaa !7
  %236 = load i64, i64* %109, align 8
  store i64 %236, i64* %233, align 1
  %237 = load i64, i64* %20, align 8, !tbaa !3
  %238 = inttoptr i64 %237 to i64*
  %239 = load i64, i64* %238, align 1
  %240 = getelementptr %0, %0* %22, i64 27, i32 0, i32 0
  store i64 %239, i64* %240, align 1, !tbaa !7
  %241 = load i64, i64* %145, align 8
  store i64 %241, i64* %238, align 1
  %242 = load i64, i64* %20, align 8, !tbaa !3
  %243 = inttoptr i64 %242 to i64*
  %244 = load i64, i64* %243, align 1
  %245 = getelementptr %0, %0* %22, i64 28, i32 0, i32 0
  store i64 %244, i64* %245, align 1, !tbaa !7
  %246 = load i64, i64* %117, align 8
  store i64 %246, i64* %243, align 1
  %247 = load i64, i64* %20, align 8, !tbaa !3
  %248 = inttoptr i64 %247 to i64*
  %249 = load i64, i64* %248, align 1
  %250 = getelementptr %0, %0* %22, i64 29, i32 0, i32 0
  store i64 %249, i64* %250, align 1, !tbaa !7
  %251 = load i64, i64* %137, align 8
  %252 = add i64 %247, 4
  store i64 %252, i64* %20, align 8, !tbaa !3
  %253 = inttoptr i64 %252 to i32*
  %254 = trunc i64 %251 to i32
  store i32 %254, i32* %253, align 1
  %255 = load i64, i64* %137, align 8
  %256 = load i64, i64* %20, align 8, !tbaa !3
  %257 = add i64 %256, -4
  store i64 %257, i64* %20, align 8, !tbaa !3
  %258 = inttoptr i64 %257 to i32*
  %259 = trunc i64 %255 to i32
  store i32 %259, i32* %258, align 1
  %260 = load i64, i64* %20, align 8, !tbaa !3
  %261 = inttoptr i64 %260 to i32*
  %262 = load i32, i32* %261, align 1
  %263 = add i64 %260, 4
  store i64 %263, i64* %20, align 8, !tbaa !3
  %264 = inttoptr i64 %263 to i32*
  %265 = load i32, i32* %264, align 1
  %266 = and i32 %265, %262
  %267 = xor i32 %266, -1
  %268 = trunc i32 %267 to i8
  %269 = call i8 @llvm.ctpop.i8(i8 %268) #3, !range !9
  %270 = icmp eq i32 %266, -1
  %271 = shl nuw nsw i8 %269, 2
  %272 = and i8 %271, 4
  %273 = xor i8 %272, 4
  %274 = zext i8 %273 to i64
  %275 = select i1 %270, i64 64, i64 0
  %276 = lshr i32 %267, 24
  %277 = and i32 %276, 128
  %278 = zext i32 %277 to i64
  %279 = or i64 %275, %278
  %280 = or i64 %279, %274
  store i32 %267, i32* %264, align 1
  %281 = load i64, i64* %20, align 8, !tbaa !3
  %282 = add i64 %281, -8
  store i64 %282, i64* %20, align 8, !tbaa !3
  %283 = inttoptr i64 %282 to i64*
  store i64 %280, i64* %283, align 1
  %284 = load i64, i64* %20, align 8, !tbaa !3
  %285 = inttoptr i64 %284 to i64*
  %286 = load i64, i64* %285, align 1
  store i64 %286, i64* %169, align 1, !tbaa !7
  %287 = add i64 %284, 8
  store i64 %287, i64* %285, align 1
  %288 = load i64, i64* %20, align 8, !tbaa !3
  %289 = inttoptr i64 %288 to i64*
  %290 = load i64, i64* %289, align 1
  %291 = add i64 %288, 8
  store i64 %291, i64* %20, align 8, !tbaa !3
  %292 = inttoptr i64 %290 to i32*
  %293 = load i32, i32* %292, align 1
  %294 = add i64 %288, 4
  store i64 %294, i64* %20, align 8, !tbaa !3
  %295 = inttoptr i64 %294 to i32*
  store i32 %293, i32* %295, align 1
  %296 = load i64, i64* %20, align 8, !tbaa !3
  %297 = inttoptr i64 %296 to i32*
  %298 = load i32, i32* %297, align 1
  %299 = add i64 %296, 4
  store i64 %299, i64* %20, align 8, !tbaa !3
  %300 = inttoptr i64 %299 to i32*
  %301 = load i32, i32* %300, align 1
  %302 = and i32 %301, %298
  %303 = xor i32 %302, -1
  %304 = trunc i32 %303 to i8
  %305 = call i8 @llvm.ctpop.i8(i8 %304) #3, !range !9
  %306 = icmp eq i32 %302, -1
  %307 = shl nuw nsw i8 %305, 2
  %308 = and i8 %307, 4
  %309 = xor i8 %308, 4
  %310 = zext i8 %309 to i64
  %311 = select i1 %306, i64 64, i64 0
  %312 = lshr i32 %303, 24
  %313 = and i32 %312, 128
  %314 = zext i32 %313 to i64
  %315 = or i64 %311, %314
  %316 = or i64 %315, %310
  store i32 %303, i32* %300, align 1
  %317 = load i64, i64* %20, align 8, !tbaa !3
  %318 = add i64 %317, -8
  store i64 %318, i64* %20, align 8, !tbaa !3
  %319 = inttoptr i64 %318 to i64*
  store i64 %316, i64* %319, align 1
  %320 = load i64, i64* %20, align 8, !tbaa !3
  %321 = inttoptr i64 %320 to i64*
  %322 = load i64, i64* %321, align 1
  store i64 %322, i64* %169, align 1, !tbaa !7
  %323 = add i64 %320, 8
  store i64 %323, i64* %20, align 8, !tbaa !3
  %324 = inttoptr i64 %323 to i32*
  %325 = load i32, i32* %324, align 1
  %326 = getelementptr %0, %0* %22, i64 14, i32 0, i32 0
  %327 = load i64, i64* %326, align 1, !tbaa !7
  %328 = and i64 %327, -4294967296
  %329 = zext i32 %325 to i64
  %330 = or i64 %328, %329
  store i64 %330, i64* %326, align 1, !tbaa !7
  %331 = add i64 %320, 10
  store i64 %331, i64* %20, align 8, !tbaa !3
  %332 = inttoptr i64 %331 to i16*
  store i16 6, i16* %332, align 1
  %333 = load i64, i64* %169, align 8
  %334 = load i64, i64* %20, align 8, !tbaa !3
  %335 = add i64 %334, -8
  store i64 %335, i64* %20, align 8, !tbaa !3
  %336 = inttoptr i64 %335 to i64*
  store i64 %333, i64* %336, align 1
  %337 = load i64, i64* %169, align 8
  %338 = load i64, i64* %20, align 8, !tbaa !3
  %339 = add i64 %338, -8
  store i64 %339, i64* %20, align 8, !tbaa !3
  %340 = inttoptr i64 %339 to i64*
  store i64 %337, i64* %340, align 1
  %341 = load i64, i64* %20, align 8, !tbaa !3
  %342 = inttoptr i64 %341 to i64*
  %343 = load i64, i64* %342, align 1
  %344 = add i64 %341, 8
  store i64 %344, i64* %20, align 8, !tbaa !3
  %345 = inttoptr i64 %344 to i64*
  %346 = load i64, i64* %345, align 1
  %347 = and i64 %346, %343
  %348 = xor i64 %347, -1
  %349 = trunc i64 %348 to i8
  %350 = call i8 @llvm.ctpop.i8(i8 %349) #3, !range !9
  %351 = icmp eq i64 %347, -1
  %352 = shl nuw nsw i8 %350, 2
  %353 = and i8 %352, 4
  %354 = xor i8 %353, 4
  %355 = zext i8 %354 to i64
  %356 = select i1 %351, i64 64, i64 0
  %357 = lshr i64 %348, 56
  %358 = and i64 %357, 128
  %359 = or i64 %358, %356
  %360 = or i64 %359, %355
  store i64 %348, i64* %345, align 1
  %361 = load i64, i64* %20, align 8, !tbaa !3
  %362 = add i64 %361, -8
  store i64 %362, i64* %20, align 8, !tbaa !3
  %363 = inttoptr i64 %362 to i64*
  store i64 %360, i64* %363, align 1
  %364 = load i64, i64* %20, align 8, !tbaa !3
  %365 = inttoptr i64 %364 to i64*
  %366 = load i64, i64* %365, align 1
  store i64 %366, i64* %161, align 1, !tbaa !7
  store i64 -65, i64* %365, align 1
  %367 = load i64, i64* %20, align 8, !tbaa !3
  %368 = inttoptr i64 %367 to i64*
  %369 = load i64, i64* %368, align 1
  %370 = add i64 %367, 8
  store i64 %370, i64* %20, align 8, !tbaa !3
  %371 = inttoptr i64 %370 to i64*
  %372 = load i64, i64* %371, align 1
  %373 = or i64 %372, %369
  %374 = xor i64 %373, -1
  %375 = trunc i64 %374 to i8
  %376 = call i8 @llvm.ctpop.i8(i8 %375) #3, !range !9
  %377 = icmp eq i64 %373, -1
  %378 = shl nuw nsw i8 %376, 2
  %379 = and i8 %378, 4
  %380 = xor i8 %379, 4
  %381 = zext i8 %380 to i64
  %382 = select i1 %377, i64 64, i64 0
  %383 = lshr i64 %374, 56
  %384 = and i64 %383, 128
  %385 = or i64 %384, %382
  %386 = or i64 %385, %381
  store i64 %374, i64* %371, align 1
  %387 = load i64, i64* %20, align 8, !tbaa !3
  %388 = add i64 %387, -8
  store i64 %388, i64* %20, align 8, !tbaa !3
  %389 = inttoptr i64 %388 to i64*
  store i64 %386, i64* %389, align 1
  %390 = load i64, i64* %20, align 8, !tbaa !3
  %391 = inttoptr i64 %390 to i64*
  %392 = load i64, i64* %391, align 1
  %393 = getelementptr %0, %0* %22, i64 16, i32 0, i32 0
  store i64 %392, i64* %393, align 1, !tbaa !7
  %394 = add i64 %390, 8
  store i64 %394, i64* %20, align 8, !tbaa !3
  %395 = inttoptr i64 %394 to i64*
  %396 = load i64, i64* %395, align 1
  %397 = add i64 %390, 16
  store i64 %397, i64* %20, align 8, !tbaa !3
  %398 = inttoptr i64 %397 to i16*
  %399 = load i16, i16* %398, align 1
  %400 = zext i16 %399 to i64
  %401 = lshr i64 %396, %400
  %402 = and i64 %400, 63
  %403 = icmp eq i64 %402, 1
  %404 = and i64 %396, 1
  %405 = icmp ne i64 %404, 0
  %406 = and i64 %401, 1
  %407 = icmp ne i64 %406, 0
  %408 = select i1 %403, i1 %405, i1 %407
  %409 = trunc i64 %401 to i8
  %410 = call i8 @llvm.ctpop.i8(i8 %409) #3, !range !9
  %411 = icmp eq i64 %401, 0
  %412 = zext i1 %408 to i64
  %413 = shl nuw nsw i8 %410, 2
  %414 = and i8 %413, 4
  %415 = xor i8 %414, 4
  %416 = zext i8 %415 to i64
  %417 = select i1 %411, i64 64, i64 0
  %418 = lshr i64 %396, 52
  %419 = and i64 %418, 2048
  %420 = or i64 %417, %419
  %421 = or i64 %420, %412
  %422 = or i64 %421, %416
  %423 = or i64 %422, 16
  %424 = add i64 %390, 10
  store i64 %424, i64* %20, align 8, !tbaa !3
  %425 = inttoptr i64 %424 to i64*
  store i64 %401, i64* %425, align 1
  %426 = load i64, i64* %20, align 8, !tbaa !3
  %427 = add i64 %426, -8
  store i64 %427, i64* %20, align 8, !tbaa !3
  %428 = inttoptr i64 %427 to i64*
  store i64 %423, i64* %428, align 1
  %429 = load i64, i64* %20, align 8, !tbaa !3
  %430 = inttoptr i64 %429 to i64*
  %431 = load i64, i64* %430, align 1
  store i64 %431, i64* %173, align 1, !tbaa !7
  store i64 -1, i64* %430, align 1
  %432 = load i64, i64* %20, align 8, !tbaa !3
  %433 = inttoptr i64 %432 to i64*
  %434 = load i64, i64* %433, align 1
  %435 = add i64 %432, 8
  store i64 %435, i64* %20, align 8, !tbaa !3
  %436 = inttoptr i64 %435 to i64*
  %437 = load i64, i64* %436, align 1
  %438 = add i64 %437, %434
  %439 = icmp ult i64 %438, %434
  %440 = icmp ult i64 %438, %437
  %441 = or i1 %439, %440
  %442 = trunc i64 %438 to i8
  %443 = call i8 @llvm.ctpop.i8(i8 %442) #3, !range !9
  %444 = xor i64 %437, %434
  %445 = xor i64 %444, %438
  %446 = and i64 %445, 16
  %447 = icmp eq i64 %438, 0
  %448 = lshr i64 %434, 63
  %449 = lshr i64 %437, 63
  %450 = lshr i64 %438, 63
  %451 = xor i64 %450, %448
  %452 = xor i64 %450, %449
  %453 = add nuw nsw i64 %451, %452
  %454 = icmp eq i64 %453, 2
  %455 = zext i1 %441 to i64
  %456 = shl nuw nsw i8 %443, 2
  %457 = and i8 %456, 4
  %458 = xor i8 %457, 4
  %459 = zext i8 %458 to i64
  %460 = select i1 %447, i64 64, i64 0
  %461 = lshr i64 %438, 56
  %462 = and i64 %461, 128
  %463 = select i1 %454, i64 2048, i64 0
  %464 = or i64 %462, %460
  %465 = or i64 %464, %446
  %466 = or i64 %465, %455
  %467 = or i64 %466, %463
  %468 = or i64 %467, %459
  store i64 %438, i64* %436, align 1
  %469 = load i64, i64* %20, align 8, !tbaa !3
  %470 = add i64 %469, -8
  store i64 %470, i64* %20, align 8, !tbaa !3
  %471 = inttoptr i64 %470 to i64*
  store i64 %468, i64* %471, align 1
  %472 = load i64, i64* %20, align 8, !tbaa !3
  %473 = inttoptr i64 %472 to i64*
  %474 = load i64, i64* %473, align 1
  %475 = getelementptr %0, %0* %22, i64 17, i32 0, i32 0
  store i64 %474, i64* %475, align 1, !tbaa !7
  %476 = add i64 %472, 8
  store i64 %476, i64* %20, align 8, !tbaa !3
  %477 = inttoptr i64 %476 to i64*
  %478 = load i64, i64* %477, align 1
  store i64 %478, i64* %161, align 1, !tbaa !7
  store i64 5371089334, i64* %477, align 1
  %479 = load i64, i64* %161, align 8
  %480 = load i64, i64* %20, align 8, !tbaa !3
  %481 = add i64 %480, -8
  store i64 %481, i64* %20, align 8, !tbaa !3
  %482 = inttoptr i64 %481 to i64*
  store i64 %479, i64* %482, align 1
  %483 = load i64, i64* %161, align 8
  %484 = load i64, i64* %20, align 8, !tbaa !3
  %485 = add i64 %484, -8
  store i64 %485, i64* %20, align 8, !tbaa !3
  %486 = inttoptr i64 %485 to i64*
  store i64 %483, i64* %486, align 1
  %487 = load i64, i64* %20, align 8, !tbaa !3
  %488 = inttoptr i64 %487 to i64*
  %489 = load i64, i64* %488, align 1
  %490 = add i64 %487, 8
  store i64 %490, i64* %20, align 8, !tbaa !3
  %491 = inttoptr i64 %490 to i64*
  %492 = load i64, i64* %491, align 1
  %493 = or i64 %492, %489
  %494 = xor i64 %493, -1
  %495 = trunc i64 %494 to i8
  %496 = call i8 @llvm.ctpop.i8(i8 %495) #3, !range !9
  %497 = icmp eq i64 %493, -1
  %498 = shl nuw nsw i8 %496, 2
  %499 = and i8 %498, 4
  %500 = xor i8 %499, 4
  %501 = zext i8 %500 to i64
  %502 = select i1 %497, i64 64, i64 0
  %503 = lshr i64 %494, 56
  %504 = and i64 %503, 128
  %505 = or i64 %504, %502
  %506 = or i64 %505, %501
  store i64 %494, i64* %491, align 1
  %507 = load i64, i64* %20, align 8, !tbaa !3
  %508 = add i64 %507, -8
  store i64 %508, i64* %20, align 8, !tbaa !3
  %509 = inttoptr i64 %508 to i64*
  store i64 %506, i64* %509, align 1
  %510 = load i64, i64* %20, align 8, !tbaa !3
  %511 = inttoptr i64 %510 to i64*
  %512 = load i64, i64* %511, align 1
  store i64 %512, i64* %393, align 1, !tbaa !7
  %513 = add i64 %510, 8
  store i64 %513, i64* %20, align 8, !tbaa !3
  %514 = inttoptr i64 %513 to i64*
  %515 = load i64, i64* %514, align 1
  %516 = add i64 %510, 16
  store i64 %516, i64* %20, align 8, !tbaa !3
  %517 = inttoptr i64 %516 to i64*
  %518 = load i64, i64* %517, align 1
  %519 = and i64 %518, %515
  %520 = xor i64 %519, -1
  %521 = trunc i64 %520 to i8
  %522 = call i8 @llvm.ctpop.i8(i8 %521) #3, !range !9
  %523 = icmp eq i64 %519, -1
  %524 = shl nuw nsw i8 %522, 2
  %525 = and i8 %524, 4
  %526 = xor i8 %525, 4
  %527 = zext i8 %526 to i64
  %528 = select i1 %523, i64 64, i64 0
  %529 = lshr i64 %520, 56
  %530 = and i64 %529, 128
  %531 = or i64 %530, %528
  %532 = or i64 %531, %527
  store i64 %520, i64* %517, align 1
  %533 = load i64, i64* %20, align 8, !tbaa !3
  %534 = add i64 %533, -8
  store i64 %534, i64* %20, align 8, !tbaa !3
  %535 = inttoptr i64 %534 to i64*
  store i64 %532, i64* %535, align 1
  %536 = load i64, i64* %20, align 8, !tbaa !3
  %537 = inttoptr i64 %536 to i64*
  %538 = load i64, i64* %537, align 1
  store i64 %538, i64* %221, align 1, !tbaa !7
  %539 = add i64 %536, 8
  store i64 %539, i64* %537, align 1
  %540 = load i64, i64* %20, align 8, !tbaa !3
  %541 = inttoptr i64 %540 to i64*
  %542 = load i64, i64* %541, align 1
  %543 = add i64 %540, 8
  store i64 %543, i64* %20, align 8, !tbaa !3
  %544 = inttoptr i64 %542 to i64*
  %545 = load i64, i64* %544, align 1
  store i64 %540, i64* %20, align 8, !tbaa !3
  store i64 %545, i64* %541, align 1
  %546 = load i64, i64* %20, align 8, !tbaa !3
  %547 = inttoptr i64 %546 to i64*
  %548 = load i64, i64* %547, align 1
  %549 = add i64 %546, 8
  store i64 %549, i64* %20, align 8, !tbaa !3
  %550 = inttoptr i64 %549 to i64*
  %551 = load i64, i64* %550, align 1
  %552 = or i64 %551, %548
  %553 = xor i64 %552, -1
  %554 = trunc i64 %553 to i8
  %555 = call i8 @llvm.ctpop.i8(i8 %554) #3, !range !9
  %556 = icmp eq i64 %552, -1
  %557 = shl nuw nsw i8 %555, 2
  %558 = and i8 %557, 4
  %559 = xor i8 %558, 4
  %560 = zext i8 %559 to i64
  %561 = select i1 %556, i64 64, i64 0
  %562 = lshr i64 %553, 56
  %563 = and i64 %562, 128
  %564 = or i64 %563, %561
  %565 = or i64 %564, %560
  store i64 %553, i64* %550, align 1
  %566 = load i64, i64* %20, align 8, !tbaa !3
  %567 = add i64 %566, -8
  store i64 %567, i64* %20, align 8, !tbaa !3
  %568 = inttoptr i64 %567 to i64*
  store i64 %565, i64* %568, align 1
  %569 = load i64, i64* %20, align 8, !tbaa !3
  %570 = inttoptr i64 %569 to i64*
  %571 = load i64, i64* %570, align 1
  store i64 %571, i64* %393, align 1, !tbaa !7
  store i64 5371089564, i64* %570, align 1
  %572 = load i64, i64* %161, align 8
  %573 = load i64, i64* %20, align 8, !tbaa !3
  %574 = add i64 %573, -8
  store i64 %574, i64* %20, align 8, !tbaa !3
  %575 = inttoptr i64 %574 to i64*
  store i64 %572, i64* %575, align 1
  %576 = load i64, i64* %20, align 8, !tbaa !3
  %577 = inttoptr i64 %576 to i64*
  %578 = load i64, i64* %577, align 1
  %579 = add i64 %576, 8
  store i64 %579, i64* %20, align 8, !tbaa !3
  %580 = inttoptr i64 %579 to i64*
  %581 = load i64, i64* %580, align 1
  %582 = and i64 %581, %578
  %583 = xor i64 %582, -1
  %584 = trunc i64 %583 to i8
  %585 = call i8 @llvm.ctpop.i8(i8 %584) #3, !range !9
  %586 = icmp eq i64 %582, -1
  %587 = shl nuw nsw i8 %585, 2
  %588 = and i8 %587, 4
  %589 = xor i8 %588, 4
  %590 = zext i8 %589 to i64
  %591 = select i1 %586, i64 64, i64 0
  %592 = lshr i64 %583, 56
  %593 = and i64 %592, 128
  %594 = or i64 %593, %591
  %595 = or i64 %594, %590
  store i64 %583, i64* %580, align 1
  %596 = load i64, i64* %20, align 8, !tbaa !3
  %597 = add i64 %596, -8
  store i64 %597, i64* %20, align 8, !tbaa !3
  %598 = inttoptr i64 %597 to i64*
  store i64 %595, i64* %598, align 1
  %599 = load i64, i64* %20, align 8, !tbaa !3
  %600 = inttoptr i64 %599 to i64*
  %601 = load i64, i64* %600, align 1
  %602 = getelementptr %0, %0* %22, i64 11, i32 0, i32 0
  store i64 %601, i64* %602, align 1, !tbaa !7
  %603 = add i64 %599, 8
  store i64 %603, i64* %600, align 1
  %604 = load i64, i64* %20, align 8, !tbaa !3
  %605 = inttoptr i64 %604 to i64*
  %606 = load i64, i64* %605, align 1
  %607 = add i64 %604, 8
  store i64 %607, i64* %20, align 8, !tbaa !3
  %608 = inttoptr i64 %606 to i64*
  %609 = load i64, i64* %608, align 1
  store i64 %604, i64* %20, align 8, !tbaa !3
  store i64 %609, i64* %605, align 1
  %610 = load i64, i64* %20, align 8, !tbaa !3
  %611 = inttoptr i64 %610 to i64*
  %612 = load i64, i64* %611, align 1
  %613 = add i64 %610, 8
  store i64 %613, i64* %20, align 8, !tbaa !3
  %614 = inttoptr i64 %613 to i64*
  %615 = load i64, i64* %614, align 1
  %616 = or i64 %615, %612
  %617 = xor i64 %616, -1
  %618 = trunc i64 %617 to i8
  %619 = call i8 @llvm.ctpop.i8(i8 %618) #3, !range !9
  %620 = icmp eq i64 %616, -1
  %621 = shl nuw nsw i8 %619, 2
  %622 = and i8 %621, 4
  %623 = xor i8 %622, 4
  %624 = zext i8 %623 to i64
  %625 = select i1 %620, i64 64, i64 0
  %626 = lshr i64 %617, 56
  %627 = and i64 %626, 128
  %628 = or i64 %627, %625
  %629 = or i64 %628, %624
  store i64 %617, i64* %614, align 1
  %630 = load i64, i64* %20, align 8, !tbaa !3
  %631 = add i64 %630, -8
  store i64 %631, i64* %20, align 8, !tbaa !3
  %632 = inttoptr i64 %631 to i64*
  store i64 %629, i64* %632, align 1
  %633 = load i64, i64* %20, align 8, !tbaa !3
  %634 = inttoptr i64 %633 to i64*
  %635 = load i64, i64* %634, align 1
  store i64 %635, i64* %602, align 1, !tbaa !7
  %636 = add i64 %633, 8
  store i64 %636, i64* %20, align 8, !tbaa !3
  %637 = inttoptr i64 %636 to i64*
  %638 = load i64, i64* %637, align 1
  %639 = add i64 %633, 16
  store i64 %639, i64* %20, align 8, !tbaa !3
  %640 = inttoptr i64 %639 to i64*
  %641 = load i64, i64* %640, align 1
  %642 = add i64 %641, %638
  %643 = icmp ult i64 %642, %638
  %644 = icmp ult i64 %642, %641
  %645 = or i1 %643, %644
  %646 = trunc i64 %642 to i8
  %647 = call i8 @llvm.ctpop.i8(i8 %646) #3, !range !9
  %648 = xor i64 %641, %638
  %649 = xor i64 %648, %642
  %650 = and i64 %649, 16
  %651 = icmp eq i64 %642, 0
  %652 = lshr i64 %638, 63
  %653 = lshr i64 %641, 63
  %654 = lshr i64 %642, 63
  %655 = xor i64 %654, %652
  %656 = xor i64 %654, %653
  %657 = add nuw nsw i64 %655, %656
  %658 = icmp eq i64 %657, 2
  %659 = zext i1 %645 to i64
  %660 = shl nuw nsw i8 %647, 2
  %661 = and i8 %660, 4
  %662 = xor i8 %661, 4
  %663 = zext i8 %662 to i64
  %664 = select i1 %651, i64 64, i64 0
  %665 = lshr i64 %642, 56
  %666 = and i64 %665, 128
  %667 = select i1 %658, i64 2048, i64 0
  %668 = or i64 %666, %664
  %669 = or i64 %668, %650
  %670 = or i64 %669, %659
  %671 = or i64 %670, %667
  %672 = or i64 %671, %663
  store i64 %642, i64* %640, align 1
  %673 = load i64, i64* %20, align 8, !tbaa !3
  %674 = add i64 %673, -8
  store i64 %674, i64* %20, align 8, !tbaa !3
  %675 = inttoptr i64 %674 to i64*
  store i64 %672, i64* %675, align 1
  %676 = load i64, i64* %20, align 8, !tbaa !3
  %677 = inttoptr i64 %676 to i64*
  %678 = load i64, i64* %677, align 1
  store i64 %678, i64* %221, align 1, !tbaa !7
  %679 = load i64, i64* %101, align 8
  store i64 %679, i64* %677, align 1
  %680 = load i64, i64* %20, align 8, !tbaa !3
  %681 = inttoptr i64 %680 to i64*
  %682 = load i64, i64* %681, align 1
  %683 = add i64 %680, 8
  store i64 %683, i64* %20, align 8, !tbaa !3
  %684 = inttoptr i64 %683 to i64*
  %685 = load i64, i64* %684, align 1
  %686 = add i64 %685, %682
  %687 = icmp ult i64 %686, %682
  %688 = icmp ult i64 %686, %685
  %689 = or i1 %687, %688
  %690 = trunc i64 %686 to i8
  %691 = call i8 @llvm.ctpop.i8(i8 %690) #3, !range !9
  %692 = xor i64 %685, %682
  %693 = xor i64 %692, %686
  %694 = and i64 %693, 16
  %695 = icmp eq i64 %686, 0
  %696 = lshr i64 %682, 63
  %697 = lshr i64 %685, 63
  %698 = lshr i64 %686, 63
  %699 = xor i64 %698, %696
  %700 = xor i64 %698, %697
  %701 = add nuw nsw i64 %699, %700
  %702 = icmp eq i64 %701, 2
  %703 = zext i1 %689 to i64
  %704 = shl nuw nsw i8 %691, 2
  %705 = and i8 %704, 4
  %706 = xor i8 %705, 4
  %707 = zext i8 %706 to i64
  %708 = select i1 %695, i64 64, i64 0
  %709 = lshr i64 %686, 56
  %710 = and i64 %709, 128
  %711 = select i1 %702, i64 2048, i64 0
  %712 = or i64 %710, %708
  %713 = or i64 %712, %694
  %714 = or i64 %713, %703
  %715 = or i64 %714, %711
  %716 = or i64 %715, %707
  store i64 %686, i64* %684, align 1
  %717 = load i64, i64* %20, align 8, !tbaa !3
  %718 = add i64 %717, -8
  store i64 %718, i64* %20, align 8, !tbaa !3
  %719 = inttoptr i64 %718 to i64*
  store i64 %716, i64* %719, align 1
  %720 = load i64, i64* %20, align 8, !tbaa !3
  %721 = inttoptr i64 %720 to i64*
  %722 = load i64, i64* %721, align 1
  store i64 %722, i64* %221, align 1, !tbaa !7
  %723 = add i64 %720, 8
  store i64 %723, i64* %20, align 8, !tbaa !3
  %724 = inttoptr i64 %723 to i64*
  %725 = load i64, i64* %724, align 1
  store i64 %725, i64* %326, align 1, !tbaa !7
  %726 = load i64, i64* %117, align 8
  store i64 %726, i64* %724, align 1
  %727 = load i64, i64* %105, align 8
  %728 = load i64, i64* %20, align 8, !tbaa !3
  %729 = add i64 %728, -8
  store i64 %729, i64* %20, align 8, !tbaa !3
  %730 = inttoptr i64 %729 to i64*
  store i64 %727, i64* %730, align 1
  %731 = load i64, i64* %157, align 8
  %732 = load i64, i64* %20, align 8, !tbaa !3
  %733 = add i64 %732, -8
  store i64 %733, i64* %20, align 8, !tbaa !3
  %734 = inttoptr i64 %733 to i64*
  store i64 %731, i64* %734, align 1
  %735 = load i64, i64* %105, align 8
  %736 = load i64, i64* %20, align 8, !tbaa !3
  %737 = add i64 %736, -8
  store i64 %737, i64* %20, align 8, !tbaa !3
  %738 = inttoptr i64 %737 to i64*
  store i64 %735, i64* %738, align 1
  %739 = load i64, i64* %129, align 8
  %740 = load i64, i64* %20, align 8, !tbaa !3
  %741 = add i64 %740, -8
  store i64 %741, i64* %20, align 8, !tbaa !3
  %742 = inttoptr i64 %741 to i64*
  store i64 %739, i64* %742, align 1
  %743 = load i64, i64* %169, align 8
  %744 = load i64, i64* %20, align 8, !tbaa !3
  %745 = add i64 %744, -8
  store i64 %745, i64* %20, align 8, !tbaa !3
  %746 = inttoptr i64 %745 to i64*
  store i64 %743, i64* %746, align 1
  %747 = load i64, i64* %145, align 8
  %748 = load i64, i64* %20, align 8, !tbaa !3
  %749 = add i64 %748, -8
  store i64 %749, i64* %20, align 8, !tbaa !3
  %750 = inttoptr i64 %749 to i64*
  store i64 %747, i64* %750, align 1
  %751 = load i64, i64* %133, align 8
  %752 = load i64, i64* %20, align 8, !tbaa !3
  %753 = add i64 %752, -8
  store i64 %753, i64* %20, align 8, !tbaa !3
  %754 = inttoptr i64 %753 to i64*
  store i64 %751, i64* %754, align 1
  %755 = load i64, i64* %113, align 8
  %756 = load i64, i64* %20, align 8, !tbaa !3
  %757 = add i64 %756, -8
  store i64 %757, i64* %20, align 8, !tbaa !3
  %758 = inttoptr i64 %757 to i64*
  store i64 %755, i64* %758, align 1
  %759 = load i64, i64* %121, align 8
  %760 = load i64, i64* %20, align 8, !tbaa !3
  %761 = add i64 %760, -8
  store i64 %761, i64* %20, align 8, !tbaa !3
  %762 = inttoptr i64 %761 to i64*
  store i64 %759, i64* %762, align 1
  %763 = load i64, i64* %117, align 8
  %764 = load i64, i64* %20, align 8, !tbaa !3
  %765 = add i64 %764, -8
  store i64 %765, i64* %20, align 8, !tbaa !3
  %766 = inttoptr i64 %765 to i64*
  store i64 %763, i64* %766, align 1
  %767 = load i64, i64* %149, align 8
  %768 = load i64, i64* %20, align 8, !tbaa !3
  %769 = add i64 %768, -8
  store i64 %769, i64* %20, align 8, !tbaa !3
  %770 = inttoptr i64 %769 to i64*
  store i64 %767, i64* %770, align 1
  %771 = load i64, i64* %137, align 8
  %772 = load i64, i64* %20, align 8, !tbaa !3
  %773 = add i64 %772, -8
  store i64 %773, i64* %20, align 8, !tbaa !3
  %774 = inttoptr i64 %773 to i64*
  store i64 %771, i64* %774, align 1
  %775 = load i64, i64* %165, align 8
  %776 = load i64, i64* %20, align 8, !tbaa !3
  %777 = add i64 %776, -8
  store i64 %777, i64* %20, align 8, !tbaa !3
  %778 = inttoptr i64 %777 to i64*
  store i64 %775, i64* %778, align 1
  %779 = load i64, i64* %125, align 8
  %780 = load i64, i64* %20, align 8, !tbaa !3
  %781 = add i64 %780, -8
  store i64 %781, i64* %20, align 8, !tbaa !3
  %782 = inttoptr i64 %781 to i64*
  store i64 %779, i64* %782, align 1
  %783 = load i64, i64* %109, align 8
  %784 = load i64, i64* %20, align 8, !tbaa !3
  %785 = add i64 %784, -8
  store i64 %785, i64* %20, align 8, !tbaa !3
  %786 = inttoptr i64 %785 to i64*
  store i64 %783, i64* %786, align 1
  %787 = load i64, i64* %153, align 8
  %788 = load i64, i64* %20, align 8, !tbaa !3
  %789 = add i64 %788, -8
  store i64 %789, i64* %20, align 8, !tbaa !3
  %790 = inttoptr i64 %789 to i64*
  store i64 %787, i64* %790, align 1
  %791 = load i64, i64* %141, align 8
  %792 = load i64, i64* %20, align 8, !tbaa !3
  %793 = add i64 %792, -8
  store i64 %793, i64* %20, align 8, !tbaa !3
  %794 = inttoptr i64 %793 to i64*
  store i64 %791, i64* %794, align 1
  %795 = load i64, i64* %101, align 8
  %796 = load i64, i64* %20, align 8, !tbaa !3
  %797 = add i64 %796, -8
  store i64 %797, i64* %20, align 8, !tbaa !3
  %798 = inttoptr i64 %797 to i64*
  store i64 %795, i64* %798, align 1
  %799 = load i64, i64* %326, align 8
  %800 = load i64, i64* %20, align 8, !tbaa !3
  %801 = add i64 %800, -8
  store i64 %801, i64* %20, align 8, !tbaa !3
  %802 = inttoptr i64 %801 to i64*
  store i64 %799, i64* %802, align 1
  %803 = load i64, i64* %20, align 8, !tbaa !3
  %804 = inttoptr i64 %803 to i64*
  %805 = load i64, i64* %804, align 1
  %806 = add i64 %803, 8
  store i64 %806, i64* %20, align 8, !tbaa !3
  %807 = add i64 %805, -4
  store i64 %807, i64* %21, align 8, !tbaa !3
  ret i64 %807
}

define i64 @helperslicevpc(i64* noalias %0, i64* noalias %1, i64* noalias %2, i64* noalias %3, i64* noalias %4, i64* noalias %5, i64* noalias %6, i64* noalias %7, i64* noalias %8, i64* noalias %9, i64* noalias %10, i64* noalias %11, i64* noalias %12, i64* noalias %13, i64* noalias %14, i64* noalias %15, i64* noalias %16, i64 %17, i64 %18, i64 %19) {
  call void @llvm.experimental.noalias.scope.decl(metadata !10)
  call void @llvm.experimental.noalias.scope.decl(metadata !13)
  call void @llvm.experimental.noalias.scope.decl(metadata !15)
  call void @llvm.experimental.noalias.scope.decl(metadata !17)
  call void @llvm.experimental.noalias.scope.decl(metadata !19)
  call void @llvm.experimental.noalias.scope.decl(metadata !21)
  call void @llvm.experimental.noalias.scope.decl(metadata !23)
  call void @llvm.experimental.noalias.scope.decl(metadata !25)
  call void @llvm.experimental.noalias.scope.decl(metadata !27)
  call void @llvm.experimental.noalias.scope.decl(metadata !29)
  call void @llvm.experimental.noalias.scope.decl(metadata !31)
  call void @llvm.experimental.noalias.scope.decl(metadata !33)
  call void @llvm.experimental.noalias.scope.decl(metadata !35)
  call void @llvm.experimental.noalias.scope.decl(metadata !37)
  call void @llvm.experimental.noalias.scope.decl(metadata !39)
  call void @llvm.experimental.noalias.scope.decl(metadata !41)
  call void @llvm.experimental.noalias.scope.decl(metadata !43)
  %21 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %22 = add i64 %21, -8
  store i64 %22, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %23 = inttoptr i64 %22 to i64*
  store i64 189322307, i64* %23, align 1, !noalias !48
  %24 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %25 = add i64 %24, -8
  store i64 %25, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %26 = inttoptr i64 %25 to i64*
  store i64 5376664256, i64* %26, align 1, !noalias !48
  %27 = load i64, i64* %14, align 8, !alias.scope !37, !noalias !49
  %28 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %29 = add i64 %28, -8
  store i64 %29, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %30 = inttoptr i64 %29 to i64*
  store i64 %27, i64* %30, align 1, !noalias !48
  %31 = load i64, i64* %16, align 8, !alias.scope !41, !noalias !50
  %32 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %33 = add i64 %32, -8
  store i64 %33, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %34 = inttoptr i64 %33 to i64*
  store i64 %31, i64* %34, align 1, !noalias !48
  %35 = load i64, i64* %8, align 8, !alias.scope !25, !noalias !51
  %36 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %37 = add i64 %36, -8
  store i64 %37, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %38 = inttoptr i64 %37 to i64*
  store i64 %35, i64* %38, align 1, !noalias !48
  %39 = load i64, i64* %2, align 8, !alias.scope !15, !noalias !52
  %40 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %41 = add i64 %40, -8
  store i64 %41, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %42 = inttoptr i64 %41 to i64*
  store i64 %39, i64* %42, align 1, !noalias !48
  %43 = load i64, i64* %13, align 8, !alias.scope !35, !noalias !53
  %44 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %45 = add i64 %44, -8
  store i64 %45, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %46 = inttoptr i64 %45 to i64*
  store i64 %43, i64* %46, align 1, !noalias !48
  %47 = load i64, i64* %5, align 8, !alias.scope !21, !noalias !54
  %48 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %49 = add i64 %48, -8
  store i64 %49, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %50 = inttoptr i64 %49 to i64*
  store i64 %47, i64* %50, align 1, !noalias !48
  %51 = load i64, i64* %3, align 8, !alias.scope !17, !noalias !55
  %52 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %53 = add i64 %52, -8
  store i64 %53, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %54 = inttoptr i64 %53 to i64*
  store i64 %51, i64* %54, align 1, !noalias !48
  %55 = load i64, i64* %0, align 8, !alias.scope !10, !noalias !56
  %56 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %57 = add i64 %56, -8
  store i64 %57, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %58 = inttoptr i64 %57 to i64*
  store i64 %55, i64* %58, align 1, !noalias !48
  %59 = load i64, i64* %6, align 8, !alias.scope !23, !noalias !57
  %60 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %61 = add i64 %60, -8
  store i64 %61, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %62 = inttoptr i64 %61 to i64*
  store i64 %59, i64* %62, align 1, !noalias !48
  %63 = load i64, i64* %15, align 8, !alias.scope !39, !noalias !58
  %64 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %65 = add i64 %64, -8
  store i64 %65, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %66 = inttoptr i64 %65 to i64*
  store i64 %63, i64* %66, align 1, !noalias !48
  %67 = load i64, i64* %12, align 8, !alias.scope !33, !noalias !59
  %68 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %69 = add i64 %68, -8
  store i64 %69, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %70 = inttoptr i64 %69 to i64*
  store i64 %67, i64* %70, align 1, !noalias !48
  %71 = load i64, i64* %10, align 8, !alias.scope !29, !noalias !60
  %72 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %73 = add i64 %72, -8
  store i64 %73, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %74 = inttoptr i64 %73 to i64*
  store i64 %71, i64* %74, align 1, !noalias !48
  %75 = load i64, i64* %1, align 8, !alias.scope !13, !noalias !61
  %76 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %77 = add i64 %76, -8
  store i64 %77, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %78 = inttoptr i64 %77 to i64*
  store i64 %75, i64* %78, align 1, !noalias !48
  %79 = load i64, i64* %11, align 8, !alias.scope !31, !noalias !62
  %80 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %81 = add i64 %80, -8
  store i64 %81, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %82 = inttoptr i64 %81 to i64*
  store i64 %79, i64* %82, align 1, !noalias !48
  %83 = load i64, i64* %4, align 8, !alias.scope !19, !noalias !63
  %84 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %85 = add i64 %84, -8
  store i64 %85, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %86 = inttoptr i64 %85 to i64*
  store i64 %83, i64* %86, align 1, !noalias !48
  %87 = load i64, i64* %9, align 8, !alias.scope !27, !noalias !64
  %88 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %89 = add i64 %88, -8
  store i64 %89, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %90 = inttoptr i64 %89 to i64*
  store i64 %87, i64* %90, align 1, !noalias !48
  %91 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %92 = add i64 %91, -8
  store i64 %92, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %93 = inttoptr i64 %92 to i64*
  store i64 0, i64* %93, align 1, !noalias !48
  %94 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %95 = inttoptr i64 %94 to i64*
  %96 = load i64, i64* %95, align 1, !noalias !48
  %97 = add i64 %94, 8
  store i64 %97, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %98 = inttoptr i64 %97 to i64*
  %99 = load i64, i64* %98, align 1, !noalias !48
  %100 = add i64 %94, 16
  store i64 %100, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %101 = inttoptr i64 %100 to i64*
  %102 = load i64, i64* %101, align 1, !noalias !48
  %103 = add i64 %94, 24
  store i64 %103, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %104 = inttoptr i64 %103 to i64*
  %105 = load i64, i64* %104, align 1, !noalias !48
  %106 = add i64 %94, 32
  store i64 %106, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %107 = inttoptr i64 %106 to i64*
  %108 = load i64, i64* %107, align 1, !noalias !48
  %109 = add i64 %94, 40
  store i64 %109, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %110 = inttoptr i64 %109 to i64*
  %111 = load i64, i64* %110, align 1, !noalias !48
  %112 = add i64 %94, 48
  store i64 %112, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %113 = inttoptr i64 %112 to i64*
  %114 = load i64, i64* %113, align 1, !noalias !48
  %115 = add i64 %94, 56
  store i64 %115, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %116 = inttoptr i64 %115 to i64*
  %117 = load i64, i64* %116, align 1, !noalias !48
  %118 = add i64 %94, 64
  store i64 %118, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %119 = inttoptr i64 %118 to i64*
  %120 = load i64, i64* %119, align 1, !noalias !48
  %121 = add i64 %94, 72
  store i64 %121, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %122 = inttoptr i64 %121 to i64*
  %123 = load i64, i64* %122, align 1, !noalias !48
  %124 = add i64 %94, 80
  store i64 %124, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %125 = inttoptr i64 %124 to i64*
  %126 = load i64, i64* %125, align 1, !noalias !48
  %127 = add i64 %94, 88
  store i64 %127, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %128 = inttoptr i64 %127 to i64*
  %129 = load i64, i64* %128, align 1, !noalias !48
  %130 = add i64 %94, 96
  store i64 %130, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %131 = inttoptr i64 %130 to i64*
  %132 = load i64, i64* %131, align 1, !noalias !48
  %133 = add i64 %94, 104
  store i64 %133, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %134 = inttoptr i64 %133 to i64*
  %135 = load i64, i64* %134, align 1, !noalias !48
  %136 = add i64 %94, 112
  store i64 %136, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %137 = inttoptr i64 %136 to i64*
  %138 = load i64, i64* %137, align 1, !noalias !48
  %139 = add i64 %94, 128
  store i64 %139, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %140 = inttoptr i64 %139 to i64*
  %141 = load i64, i64* %140, align 1, !noalias !48
  %142 = add i64 %94, 144
  store i64 %142, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %143 = inttoptr i64 %142 to i64*
  store i64 5368793250, i64* %143, align 1, !noalias !48
  %144 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %145 = add i64 %144, -8
  store i64 %145, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %146 = inttoptr i64 %145 to i64*
  store i64 %96, i64* %146, align 1, !noalias !48
  %147 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %148 = inttoptr i64 %147 to i64*
  %149 = load i64, i64* %148, align 1, !noalias !48
  %150 = add i64 %147, 8
  store i64 %150, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %151 = inttoptr i64 %150 to i64*
  %152 = load i64, i64* %151, align 1, !noalias !48
  %153 = add i64 %152, %149
  %154 = icmp ult i64 %153, %149
  %155 = icmp ult i64 %153, %152
  %156 = or i1 %154, %155
  %157 = trunc i64 %153 to i8
  %158 = call i8 @llvm.ctpop.i8(i8 %157) #3, !range !9
  %159 = xor i64 %152, %149
  %160 = xor i64 %159, %153
  %161 = and i64 %160, 16
  %162 = icmp eq i64 %153, 0
  %163 = lshr i64 %149, 63
  %164 = lshr i64 %152, 63
  %165 = lshr i64 %153, 63
  %166 = xor i64 %165, %163
  %167 = xor i64 %165, %164
  %168 = add nuw nsw i64 %166, %167
  %169 = icmp eq i64 %168, 2
  %170 = zext i1 %156 to i64
  %171 = shl nuw nsw i8 %158, 2
  %172 = and i8 %171, 4
  %173 = xor i8 %172, 4
  %174 = zext i8 %173 to i64
  %175 = select i1 %162, i64 64, i64 0
  %176 = lshr i64 %153, 56
  %177 = and i64 %176, 128
  %178 = select i1 %169, i64 2048, i64 0
  %179 = or i64 %177, %175
  %180 = or i64 %179, %161
  %181 = or i64 %180, %170
  %182 = or i64 %181, %178
  %183 = or i64 %182, %174
  store i64 %153, i64* %151, align 1, !noalias !48
  %184 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %185 = add i64 %184, -8
  store i64 %185, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %186 = inttoptr i64 %185 to i64*
  store i64 %183, i64* %186, align 1, !noalias !48
  %187 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %188 = add i64 %187, 8
  store i64 %188, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %189 = add i64 %187, 16
  %190 = inttoptr i64 %188 to i64*
  store i64 %189, i64* %190, align 1, !noalias !48
  %191 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %192 = inttoptr i64 %191 to i64*
  store i64 %120, i64* %192, align 1, !noalias !48
  %193 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %194 = inttoptr i64 %193 to i64*
  store i64 %102, i64* %194, align 1, !noalias !48
  %195 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %196 = inttoptr i64 %195 to i64*
  store i64 %129, i64* %196, align 1, !noalias !48
  %197 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %198 = inttoptr i64 %197 to i64*
  store i64 %108, i64* %198, align 1, !noalias !48
  %199 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %200 = add i64 %199, 4
  store i64 %200, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %201 = inttoptr i64 %200 to i32*
  %202 = trunc i64 %123 to i32
  store i32 %202, i32* %201, align 1, !noalias !48
  %203 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %204 = add i64 %203, -4
  store i64 %204, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %205 = inttoptr i64 %204 to i32*
  store i32 %202, i32* %205, align 1, !noalias !48
  %206 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %207 = inttoptr i64 %206 to i32*
  %208 = load i32, i32* %207, align 1, !noalias !48
  %209 = add i64 %206, 4
  store i64 %209, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %210 = inttoptr i64 %209 to i32*
  %211 = load i32, i32* %210, align 1, !noalias !48
  %212 = and i32 %211, %208
  %213 = xor i32 %212, -1
  %214 = trunc i32 %213 to i8
  %215 = call i8 @llvm.ctpop.i8(i8 %214) #3, !range !9
  %216 = icmp eq i32 %212, -1
  %217 = shl nuw nsw i8 %215, 2
  %218 = and i8 %217, 4
  %219 = xor i8 %218, 4
  %220 = zext i8 %219 to i64
  %221 = select i1 %216, i64 64, i64 0
  %222 = lshr i32 %213, 24
  %223 = and i32 %222, 128
  %224 = zext i32 %223 to i64
  %225 = or i64 %221, %224
  %226 = or i64 %225, %220
  store i32 %213, i32* %210, align 1, !noalias !48
  %227 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %228 = add i64 %227, -8
  store i64 %228, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %229 = inttoptr i64 %228 to i64*
  store i64 %226, i64* %229, align 1, !noalias !48
  %230 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %231 = add i64 %230, 8
  %232 = inttoptr i64 %230 to i64*
  store i64 %231, i64* %232, align 1, !noalias !48
  %233 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %234 = inttoptr i64 %233 to i64*
  %235 = load i64, i64* %234, align 1, !noalias !48
  %236 = add i64 %233, 8
  store i64 %236, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %237 = inttoptr i64 %235 to i32*
  %238 = load i32, i32* %237, align 1, !noalias !48
  %239 = add i64 %233, 4
  store i64 %239, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %240 = inttoptr i64 %239 to i32*
  store i32 %238, i32* %240, align 1, !noalias !48
  %241 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %242 = inttoptr i64 %241 to i32*
  %243 = load i32, i32* %242, align 1, !noalias !48
  %244 = add i64 %241, 4
  store i64 %244, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %245 = inttoptr i64 %244 to i32*
  %246 = load i32, i32* %245, align 1, !noalias !48
  %247 = and i32 %246, %243
  %248 = xor i32 %247, -1
  %249 = trunc i32 %248 to i8
  %250 = call i8 @llvm.ctpop.i8(i8 %249) #3, !range !9
  %251 = icmp eq i32 %247, -1
  %252 = shl nuw nsw i8 %250, 2
  %253 = and i8 %252, 4
  %254 = xor i8 %253, 4
  %255 = zext i8 %254 to i64
  %256 = select i1 %251, i64 64, i64 0
  %257 = lshr i32 %248, 24
  %258 = and i32 %257, 128
  %259 = zext i32 %258 to i64
  %260 = or i64 %256, %259
  %261 = or i64 %260, %255
  store i32 %248, i32* %245, align 1, !noalias !48
  %262 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %263 = add i64 %262, -8
  store i64 %263, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %264 = inttoptr i64 %263 to i64*
  store i64 %261, i64* %264, align 1, !noalias !48
  %265 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %266 = inttoptr i64 %265 to i64*
  %267 = load i64, i64* %266, align 1, !noalias !48
  %268 = add i64 %265, 10
  store i64 %268, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %269 = inttoptr i64 %268 to i16*
  store i16 6, i16* %269, align 1, !noalias !48
  %270 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %271 = add i64 %270, -8
  store i64 %271, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %272 = inttoptr i64 %271 to i64*
  store i64 %267, i64* %272, align 1, !noalias !48
  %273 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %274 = add i64 %273, -8
  store i64 %274, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %275 = inttoptr i64 %274 to i64*
  store i64 %267, i64* %275, align 1, !noalias !48
  %276 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %277 = inttoptr i64 %276 to i64*
  %278 = load i64, i64* %277, align 1, !noalias !48
  %279 = add i64 %276, 8
  store i64 %279, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %280 = inttoptr i64 %279 to i64*
  %281 = load i64, i64* %280, align 1, !noalias !48
  %282 = and i64 %281, %278
  %283 = xor i64 %282, -1
  %284 = trunc i64 %283 to i8
  %285 = call i8 @llvm.ctpop.i8(i8 %284) #3, !range !9
  %286 = icmp eq i64 %282, -1
  %287 = shl nuw nsw i8 %285, 2
  %288 = and i8 %287, 4
  %289 = xor i8 %288, 4
  %290 = zext i8 %289 to i64
  %291 = select i1 %286, i64 64, i64 0
  %292 = lshr i64 %283, 56
  %293 = and i64 %292, 128
  %294 = or i64 %293, %291
  %295 = or i64 %294, %290
  store i64 %283, i64* %280, align 1, !noalias !48
  %296 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %297 = add i64 %296, -8
  store i64 %297, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %298 = inttoptr i64 %297 to i64*
  store i64 %295, i64* %298, align 1, !noalias !48
  %299 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %300 = inttoptr i64 %299 to i64*
  store i64 -65, i64* %300, align 1, !noalias !48
  %301 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %302 = inttoptr i64 %301 to i64*
  %303 = load i64, i64* %302, align 1, !noalias !48
  %304 = add i64 %301, 8
  store i64 %304, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %305 = inttoptr i64 %304 to i64*
  %306 = load i64, i64* %305, align 1, !noalias !48
  %307 = or i64 %306, %303
  %308 = xor i64 %307, -1
  %309 = trunc i64 %308 to i8
  %310 = call i8 @llvm.ctpop.i8(i8 %309) #3, !range !9
  %311 = icmp eq i64 %307, -1
  %312 = shl nuw nsw i8 %310, 2
  %313 = and i8 %312, 4
  %314 = xor i8 %313, 4
  %315 = zext i8 %314 to i64
  %316 = select i1 %311, i64 64, i64 0
  %317 = lshr i64 %308, 56
  %318 = and i64 %317, 128
  %319 = or i64 %318, %316
  %320 = or i64 %319, %315
  store i64 %308, i64* %305, align 1, !noalias !48
  %321 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %322 = add i64 %321, -8
  store i64 %322, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %323 = inttoptr i64 %322 to i64*
  store i64 %320, i64* %323, align 1, !noalias !48
  %324 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %325 = add i64 %324, 8
  store i64 %325, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %326 = inttoptr i64 %325 to i64*
  %327 = load i64, i64* %326, align 1, !noalias !48
  %328 = add i64 %324, 16
  store i64 %328, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %329 = inttoptr i64 %328 to i16*
  %330 = load i16, i16* %329, align 1, !noalias !48
  %331 = zext i16 %330 to i64
  %332 = lshr i64 %327, %331
  %333 = and i64 %331, 63
  %334 = icmp eq i64 %333, 1
  %335 = and i64 %327, 1
  %336 = icmp ne i64 %335, 0
  %337 = and i64 %332, 1
  %338 = icmp ne i64 %337, 0
  %339 = select i1 %334, i1 %336, i1 %338
  %340 = trunc i64 %332 to i8
  %341 = call i8 @llvm.ctpop.i8(i8 %340) #3, !range !9
  %342 = icmp eq i64 %332, 0
  %343 = zext i1 %339 to i64
  %344 = shl nuw nsw i8 %341, 2
  %345 = and i8 %344, 4
  %346 = xor i8 %345, 4
  %347 = zext i8 %346 to i64
  %348 = select i1 %342, i64 64, i64 0
  %349 = lshr i64 %327, 52
  %350 = and i64 %349, 2048
  %351 = or i64 %348, %350
  %352 = or i64 %351, %343
  %353 = or i64 %352, %347
  %354 = or i64 %353, 16
  %355 = add i64 %324, 10
  store i64 %355, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %356 = inttoptr i64 %355 to i64*
  store i64 %332, i64* %356, align 1, !noalias !48
  %357 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %358 = add i64 %357, -8
  store i64 %358, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %359 = inttoptr i64 %358 to i64*
  store i64 %354, i64* %359, align 1, !noalias !48
  %360 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %361 = inttoptr i64 %360 to i64*
  store i64 -1, i64* %361, align 1, !noalias !48
  %362 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %363 = inttoptr i64 %362 to i64*
  %364 = load i64, i64* %363, align 1, !noalias !48
  %365 = add i64 %362, 8
  store i64 %365, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %366 = inttoptr i64 %365 to i64*
  %367 = load i64, i64* %366, align 1, !noalias !48
  %368 = add i64 %367, %364
  %369 = icmp ult i64 %368, %364
  %370 = icmp ult i64 %368, %367
  %371 = or i1 %369, %370
  %372 = trunc i64 %368 to i8
  %373 = call i8 @llvm.ctpop.i8(i8 %372) #3, !range !9
  %374 = xor i64 %367, %364
  %375 = xor i64 %374, %368
  %376 = and i64 %375, 16
  %377 = icmp eq i64 %368, 0
  %378 = lshr i64 %364, 63
  %379 = lshr i64 %367, 63
  %380 = lshr i64 %368, 63
  %381 = xor i64 %380, %378
  %382 = xor i64 %380, %379
  %383 = add nuw nsw i64 %381, %382
  %384 = icmp eq i64 %383, 2
  %385 = zext i1 %371 to i64
  %386 = shl nuw nsw i8 %373, 2
  %387 = and i8 %386, 4
  %388 = xor i8 %387, 4
  %389 = zext i8 %388 to i64
  %390 = select i1 %377, i64 64, i64 0
  %391 = lshr i64 %368, 56
  %392 = and i64 %391, 128
  %393 = select i1 %384, i64 2048, i64 0
  %394 = or i64 %392, %390
  %395 = or i64 %394, %376
  %396 = or i64 %395, %385
  %397 = or i64 %396, %393
  %398 = or i64 %397, %389
  store i64 %368, i64* %366, align 1, !noalias !48
  %399 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %400 = add i64 %399, -8
  store i64 %400, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %401 = inttoptr i64 %400 to i64*
  store i64 %398, i64* %401, align 1, !noalias !48
  %402 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %403 = add i64 %402, 8
  store i64 %403, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %404 = inttoptr i64 %403 to i64*
  %405 = load i64, i64* %404, align 1, !noalias !48
  store i64 5371089334, i64* %404, align 1, !noalias !48
  %406 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %407 = add i64 %406, -8
  store i64 %407, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %408 = inttoptr i64 %407 to i64*
  store i64 %405, i64* %408, align 1, !noalias !48
  %409 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %410 = add i64 %409, -8
  store i64 %410, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %411 = inttoptr i64 %410 to i64*
  store i64 %405, i64* %411, align 1, !noalias !48
  %412 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %413 = inttoptr i64 %412 to i64*
  %414 = load i64, i64* %413, align 1, !noalias !48
  %415 = add i64 %412, 8
  store i64 %415, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %416 = inttoptr i64 %415 to i64*
  %417 = load i64, i64* %416, align 1, !noalias !48
  %418 = or i64 %417, %414
  %419 = xor i64 %418, -1
  %420 = trunc i64 %419 to i8
  %421 = call i8 @llvm.ctpop.i8(i8 %420) #3, !range !9
  %422 = icmp eq i64 %418, -1
  %423 = shl nuw nsw i8 %421, 2
  %424 = and i8 %423, 4
  %425 = xor i8 %424, 4
  %426 = zext i8 %425 to i64
  %427 = select i1 %422, i64 64, i64 0
  %428 = lshr i64 %419, 56
  %429 = and i64 %428, 128
  %430 = or i64 %429, %427
  %431 = or i64 %430, %426
  store i64 %419, i64* %416, align 1, !noalias !48
  %432 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %433 = add i64 %432, -8
  store i64 %433, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %434 = inttoptr i64 %433 to i64*
  store i64 %431, i64* %434, align 1, !noalias !48
  %435 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %436 = add i64 %435, 8
  store i64 %436, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %437 = inttoptr i64 %436 to i64*
  %438 = load i64, i64* %437, align 1, !noalias !48
  %439 = add i64 %435, 16
  store i64 %439, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %440 = inttoptr i64 %439 to i64*
  %441 = load i64, i64* %440, align 1, !noalias !48
  %442 = and i64 %441, %438
  %443 = xor i64 %442, -1
  %444 = trunc i64 %443 to i8
  %445 = call i8 @llvm.ctpop.i8(i8 %444) #3, !range !9
  %446 = icmp eq i64 %442, -1
  %447 = shl nuw nsw i8 %445, 2
  %448 = and i8 %447, 4
  %449 = xor i8 %448, 4
  %450 = zext i8 %449 to i64
  %451 = select i1 %446, i64 64, i64 0
  %452 = lshr i64 %443, 56
  %453 = and i64 %452, 128
  %454 = or i64 %453, %451
  %455 = or i64 %454, %450
  store i64 %443, i64* %440, align 1, !noalias !48
  %456 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %457 = add i64 %456, -8
  store i64 %457, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %458 = inttoptr i64 %457 to i64*
  store i64 %455, i64* %458, align 1, !noalias !48
  %459 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %460 = add i64 %459, 8
  %461 = inttoptr i64 %459 to i64*
  store i64 %460, i64* %461, align 1, !noalias !48
  %462 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %463 = inttoptr i64 %462 to i64*
  %464 = load i64, i64* %463, align 1, !noalias !48
  %465 = add i64 %462, 8
  store i64 %465, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %466 = inttoptr i64 %464 to i64*
  %467 = load i64, i64* %466, align 1, !noalias !48
  store i64 %462, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  store i64 %467, i64* %463, align 1, !noalias !48
  %468 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %469 = inttoptr i64 %468 to i64*
  %470 = load i64, i64* %469, align 1, !noalias !48
  %471 = add i64 %468, 8
  store i64 %471, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %472 = inttoptr i64 %471 to i64*
  %473 = load i64, i64* %472, align 1, !noalias !48
  %474 = or i64 %473, %470
  %475 = xor i64 %474, -1
  %476 = trunc i64 %475 to i8
  %477 = call i8 @llvm.ctpop.i8(i8 %476) #3, !range !9
  %478 = icmp eq i64 %474, -1
  %479 = shl nuw nsw i8 %477, 2
  %480 = and i8 %479, 4
  %481 = xor i8 %480, 4
  %482 = zext i8 %481 to i64
  %483 = select i1 %478, i64 64, i64 0
  %484 = lshr i64 %475, 56
  %485 = and i64 %484, 128
  %486 = or i64 %485, %483
  %487 = or i64 %486, %482
  store i64 %475, i64* %472, align 1, !noalias !48
  %488 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %489 = add i64 %488, -8
  store i64 %489, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %490 = inttoptr i64 %489 to i64*
  store i64 %487, i64* %490, align 1, !noalias !48
  %491 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %492 = inttoptr i64 %491 to i64*
  store i64 5371089564, i64* %492, align 1, !noalias !48
  %493 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %494 = add i64 %493, -8
  store i64 %494, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %495 = inttoptr i64 %494 to i64*
  store i64 %405, i64* %495, align 1, !noalias !48
  %496 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %497 = inttoptr i64 %496 to i64*
  %498 = load i64, i64* %497, align 1, !noalias !48
  %499 = add i64 %496, 8
  store i64 %499, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %500 = inttoptr i64 %499 to i64*
  %501 = load i64, i64* %500, align 1, !noalias !48
  %502 = and i64 %501, %498
  %503 = xor i64 %502, -1
  %504 = trunc i64 %503 to i8
  %505 = call i8 @llvm.ctpop.i8(i8 %504) #3, !range !9
  %506 = icmp eq i64 %502, -1
  %507 = shl nuw nsw i8 %505, 2
  %508 = and i8 %507, 4
  %509 = xor i8 %508, 4
  %510 = zext i8 %509 to i64
  %511 = select i1 %506, i64 64, i64 0
  %512 = lshr i64 %503, 56
  %513 = and i64 %512, 128
  %514 = or i64 %513, %511
  %515 = or i64 %514, %510
  store i64 %503, i64* %500, align 1, !noalias !48
  %516 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %517 = add i64 %516, -8
  store i64 %517, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %518 = inttoptr i64 %517 to i64*
  store i64 %515, i64* %518, align 1, !noalias !48
  %519 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %520 = add i64 %519, 8
  %521 = inttoptr i64 %519 to i64*
  store i64 %520, i64* %521, align 1, !noalias !48
  %522 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %523 = inttoptr i64 %522 to i64*
  %524 = load i64, i64* %523, align 1, !noalias !48
  %525 = add i64 %522, 8
  store i64 %525, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %526 = inttoptr i64 %524 to i64*
  %527 = load i64, i64* %526, align 1, !noalias !48
  store i64 %522, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  store i64 %527, i64* %523, align 1, !noalias !48
  %528 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %529 = inttoptr i64 %528 to i64*
  %530 = load i64, i64* %529, align 1, !noalias !48
  %531 = add i64 %528, 8
  store i64 %531, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %532 = inttoptr i64 %531 to i64*
  %533 = load i64, i64* %532, align 1, !noalias !48
  %534 = or i64 %533, %530
  %535 = xor i64 %534, -1
  %536 = trunc i64 %535 to i8
  %537 = call i8 @llvm.ctpop.i8(i8 %536) #3, !range !9
  %538 = icmp eq i64 %534, -1
  %539 = shl nuw nsw i8 %537, 2
  %540 = and i8 %539, 4
  %541 = xor i8 %540, 4
  %542 = zext i8 %541 to i64
  %543 = select i1 %538, i64 64, i64 0
  %544 = lshr i64 %535, 56
  %545 = and i64 %544, 128
  %546 = or i64 %545, %543
  %547 = or i64 %546, %542
  store i64 %535, i64* %532, align 1, !noalias !48
  %548 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %549 = add i64 %548, -8
  store i64 %549, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %550 = inttoptr i64 %549 to i64*
  store i64 %547, i64* %550, align 1, !noalias !48
  %551 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %552 = add i64 %551, 8
  store i64 %552, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %553 = inttoptr i64 %552 to i64*
  %554 = load i64, i64* %553, align 1, !noalias !48
  %555 = add i64 %551, 16
  store i64 %555, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %556 = inttoptr i64 %555 to i64*
  %557 = load i64, i64* %556, align 1, !noalias !48
  %558 = add i64 %557, %554
  %559 = icmp ult i64 %558, %554
  %560 = icmp ult i64 %558, %557
  %561 = or i1 %559, %560
  %562 = trunc i64 %558 to i8
  %563 = call i8 @llvm.ctpop.i8(i8 %562) #3, !range !9
  %564 = xor i64 %557, %554
  %565 = xor i64 %564, %558
  %566 = and i64 %565, 16
  %567 = icmp eq i64 %558, 0
  %568 = lshr i64 %554, 63
  %569 = lshr i64 %557, 63
  %570 = lshr i64 %558, 63
  %571 = xor i64 %570, %568
  %572 = xor i64 %570, %569
  %573 = add nuw nsw i64 %571, %572
  %574 = icmp eq i64 %573, 2
  %575 = zext i1 %561 to i64
  %576 = shl nuw nsw i8 %563, 2
  %577 = and i8 %576, 4
  %578 = xor i8 %577, 4
  %579 = zext i8 %578 to i64
  %580 = select i1 %567, i64 64, i64 0
  %581 = lshr i64 %558, 56
  %582 = and i64 %581, 128
  %583 = select i1 %574, i64 2048, i64 0
  %584 = or i64 %582, %580
  %585 = or i64 %584, %566
  %586 = or i64 %585, %575
  %587 = or i64 %586, %583
  %588 = or i64 %587, %579
  store i64 %558, i64* %556, align 1, !noalias !48
  %589 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %590 = add i64 %589, -8
  store i64 %590, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %591 = inttoptr i64 %590 to i64*
  store i64 %588, i64* %591, align 1, !noalias !48
  %592 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %593 = inttoptr i64 %592 to i64*
  store i64 %96, i64* %593, align 1, !noalias !48
  %594 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %595 = inttoptr i64 %594 to i64*
  %596 = load i64, i64* %595, align 1, !noalias !48
  %597 = add i64 %594, 8
  store i64 %597, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %598 = inttoptr i64 %597 to i64*
  %599 = load i64, i64* %598, align 1, !noalias !48
  %600 = add i64 %599, %596
  %601 = icmp ult i64 %600, %596
  %602 = icmp ult i64 %600, %599
  %603 = or i1 %601, %602
  %604 = trunc i64 %600 to i8
  %605 = call i8 @llvm.ctpop.i8(i8 %604) #3, !range !9
  %606 = xor i64 %599, %596
  %607 = xor i64 %606, %600
  %608 = and i64 %607, 16
  %609 = icmp eq i64 %600, 0
  %610 = lshr i64 %596, 63
  %611 = lshr i64 %599, 63
  %612 = lshr i64 %600, 63
  %613 = xor i64 %612, %610
  %614 = xor i64 %612, %611
  %615 = add nuw nsw i64 %613, %614
  %616 = icmp eq i64 %615, 2
  %617 = zext i1 %603 to i64
  %618 = shl nuw nsw i8 %605, 2
  %619 = and i8 %618, 4
  %620 = xor i8 %619, 4
  %621 = zext i8 %620 to i64
  %622 = select i1 %609, i64 64, i64 0
  %623 = lshr i64 %600, 56
  %624 = and i64 %623, 128
  %625 = select i1 %616, i64 2048, i64 0
  %626 = or i64 %624, %622
  %627 = or i64 %626, %608
  %628 = or i64 %627, %617
  %629 = or i64 %628, %625
  %630 = or i64 %629, %621
  store i64 %600, i64* %598, align 1, !noalias !48
  %631 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %632 = add i64 %631, -8
  store i64 %632, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %633 = inttoptr i64 %632 to i64*
  store i64 %630, i64* %633, align 1, !noalias !48
  %634 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %635 = add i64 %634, 8
  store i64 %635, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %636 = inttoptr i64 %635 to i64*
  %637 = load i64, i64* %636, align 1, !noalias !48
  store i64 %108, i64* %636, align 1, !noalias !48
  %638 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %639 = add i64 %638, -8
  store i64 %639, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %640 = inttoptr i64 %639 to i64*
  store i64 %99, i64* %640, align 1, !noalias !48
  %641 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %642 = add i64 %641, -8
  store i64 %642, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %643 = inttoptr i64 %642 to i64*
  store i64 %138, i64* %643, align 1, !noalias !48
  %644 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %645 = add i64 %644, -8
  store i64 %645, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %646 = inttoptr i64 %645 to i64*
  store i64 %99, i64* %646, align 1, !noalias !48
  %647 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %648 = add i64 %647, -8
  store i64 %648, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %649 = inttoptr i64 %648 to i64*
  store i64 %117, i64* %649, align 1, !noalias !48
  %650 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %651 = add i64 %650, -8
  store i64 %651, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %652 = inttoptr i64 %651 to i64*
  store i64 %267, i64* %652, align 1, !noalias !48
  %653 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %654 = add i64 %653, -8
  store i64 %654, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %655 = inttoptr i64 %654 to i64*
  store i64 %129, i64* %655, align 1, !noalias !48
  %656 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %657 = add i64 %656, -8
  store i64 %657, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %658 = inttoptr i64 %657 to i64*
  store i64 %120, i64* %658, align 1, !noalias !48
  %659 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %660 = add i64 %659, -8
  store i64 %660, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %661 = inttoptr i64 %660 to i64*
  store i64 %105, i64* %661, align 1, !noalias !48
  %662 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %663 = add i64 %662, -8
  store i64 %663, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %664 = inttoptr i64 %663 to i64*
  store i64 %111, i64* %664, align 1, !noalias !48
  %665 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %666 = add i64 %665, -8
  store i64 %666, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %667 = inttoptr i64 %666 to i64*
  store i64 %108, i64* %667, align 1, !noalias !48
  %668 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %669 = add i64 %668, -8
  store i64 %669, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %670 = inttoptr i64 %669 to i64*
  store i64 %132, i64* %670, align 1, !noalias !48
  %671 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %672 = add i64 %671, -8
  store i64 %672, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %673 = inttoptr i64 %672 to i64*
  store i64 %123, i64* %673, align 1, !noalias !48
  %674 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %675 = add i64 %674, -8
  store i64 %675, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %676 = inttoptr i64 %675 to i64*
  store i64 %141, i64* %676, align 1, !noalias !48
  %677 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %678 = add i64 %677, -8
  store i64 %678, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %679 = inttoptr i64 %678 to i64*
  store i64 %114, i64* %679, align 1, !noalias !48
  %680 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %681 = add i64 %680, -8
  store i64 %681, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %682 = inttoptr i64 %681 to i64*
  store i64 %102, i64* %682, align 1, !noalias !48
  %683 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %684 = add i64 %683, -8
  store i64 %684, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %685 = inttoptr i64 %684 to i64*
  store i64 %135, i64* %685, align 1, !noalias !48
  %686 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %687 = add i64 %686, -8
  store i64 %687, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %688 = inttoptr i64 %687 to i64*
  store i64 %126, i64* %688, align 1, !noalias !48
  %689 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %690 = add i64 %689, -8
  store i64 %690, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %691 = inttoptr i64 %690 to i64*
  store i64 %96, i64* %691, align 1, !noalias !48
  %692 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %693 = add i64 %692, -8
  store i64 %693, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %694 = inttoptr i64 %693 to i64*
  store i64 %637, i64* %694, align 1, !noalias !48
  %695 = load i64, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %696 = inttoptr i64 %695 to i64*
  %697 = load i64, i64* %696, align 1, !noalias !48
  %698 = add i64 %695, 8
  store i64 %698, i64* %7, align 8, !tbaa !3, !alias.scope !43, !noalias !45
  %699 = add i64 %697, -4
  ret i64 %699
}

; Function Attrs: inaccessiblememonly nofree nosync nounwind willreturn
declare void @llvm.experimental.noalias.scope.decl(metadata) #11

attributes #0 = { alwaysinline mustprogress nounwind uwtable "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+x87,-3dnow,-3dnowa,-aes,-avx,-avx2,-avx512bf16,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vp2intersect,-avx512vpopcntdq,-avxvnni,-f16c,-fma,-fma4,-gfni,-kl,-mmx,-pclmul,-sha,-sse,-sse2,-sse3,-sse4.1,-sse4.2,-sse4a,-ssse3,-vaes,-vpclmulqdq,-widekl,-xop" "tune-cpu"="generic" }
attributes #1 = { alwaysinline mustprogress nofree norecurse nosync nounwind uwtable willreturn "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+x87,-3dnow,-3dnowa,-aes,-avx,-avx2,-avx512bf16,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vp2intersect,-avx512vpopcntdq,-avxvnni,-f16c,-fma,-fma4,-gfni,-kl,-mmx,-pclmul,-sha,-sse,-sse2,-sse3,-sse4.1,-sse4.2,-sse4a,-ssse3,-vaes,-vpclmulqdq,-widekl,-xop" "tune-cpu"="generic" }
attributes #2 = { argmemonly nofree nosync nounwind willreturn }
attributes #3 = { nounwind }
attributes #4 = { mustprogress nounwind uwtable "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+x87,-3dnow,-3dnowa,-aes,-avx,-avx2,-avx512bf16,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vp2intersect,-avx512vpopcntdq,-avxvnni,-f16c,-fma,-fma4,-gfni,-kl,-mmx,-pclmul,-sha,-sse,-sse2,-sse3,-sse4.1,-sse4.2,-sse4a,-ssse3,-vaes,-vpclmulqdq,-widekl,-xop" "tune-cpu"="generic" }
attributes #5 = { "frame-pointer"="none" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+x87,-3dnow,-3dnowa,-aes,-avx,-avx2,-avx512bf16,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vp2intersect,-avx512vpopcntdq,-avxvnni,-f16c,-fma,-fma4,-gfni,-kl,-mmx,-pclmul,-sha,-sse,-sse2,-sse3,-sse4.1,-sse4.2,-sse4a,-ssse3,-vaes,-vpclmulqdq,-widekl,-xop" "tune-cpu"="generic" }
attributes #6 = { alwaysinline mustprogress nofree norecurse nosync nounwind readonly uwtable willreturn "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+x87,-3dnow,-3dnowa,-aes,-avx,-avx2,-avx512bf16,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vp2intersect,-avx512vpopcntdq,-avxvnni,-f16c,-fma,-fma4,-gfni,-kl,-mmx,-pclmul,-sha,-sse,-sse2,-sse3,-sse4.1,-sse4.2,-sse4a,-ssse3,-vaes,-vpclmulqdq,-widekl,-xop" "tune-cpu"="generic" }
attributes #7 = { argmemonly nofree nounwind willreturn writeonly }
attributes #8 = { mustprogress nofree norecurse nosync nounwind uwtable willreturn writeonly "frame-pointer"="none" "min-legal-vector-width"="0" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="x86-64" "target-features"="+cx8,+fxsr,+x87,-3dnow,-3dnowa,-aes,-avx,-avx2,-avx512bf16,-avx512bitalg,-avx512bw,-avx512cd,-avx512dq,-avx512er,-avx512f,-avx512ifma,-avx512pf,-avx512vbmi,-avx512vbmi2,-avx512vl,-avx512vnni,-avx512vp2intersect,-avx512vpopcntdq,-avxvnni,-f16c,-fma,-fma4,-gfni,-kl,-mmx,-pclmul,-sha,-sse,-sse2,-sse3,-sse4.1,-sse4.2,-sse4a,-ssse3,-vaes,-vpclmulqdq,-widekl,-xop" "tune-cpu"="generic" }
attributes #9 = { nofree nosync nounwind readnone speculatable willreturn }
attributes #10 = { alwaysinline }
attributes #11 = { inaccessiblememonly nofree nosync nounwind willreturn }
attributes #12 = { nounwind readnone }

!llvm.module.flags = !{!0, !1}
!llvm.ident = !{!2}

!0 = !{i32 1, !"wchar_size", i32 4}
!1 = !{i32 7, !"uwtable", i32 1}
!2 = !{!"clang version 13.0.1"}
!3 = !{!4, !4, i64 0}
!4 = !{!"long", !5, i64 0}
!5 = !{!"omnipotent char", !6, i64 0}
!6 = !{!"Simple C++ TBAA"}
!7 = !{!5, !5, i64 0}
!8 = !{i64 2152664474, i64 2152664510, i64 2152664534}
!9 = !{i8 0, i8 9}
!10 = !{!11}
!11 = distinct !{!11, !12, !"helperstub_1402454ad: %rax"}
!12 = distinct !{!12, !"helperstub_1402454ad"}
!13 = !{!14}
!14 = distinct !{!14, !12, !"helperstub_1402454ad: %rbx"}
!15 = !{!16}
!16 = distinct !{!16, !12, !"helperstub_1402454ad: %rcx"}
!17 = !{!18}
!18 = distinct !{!18, !12, !"helperstub_1402454ad: %rdx"}
!19 = !{!20}
!20 = distinct !{!20, !12, !"helperstub_1402454ad: %rsi"}
!21 = !{!22}
!22 = distinct !{!22, !12, !"helperstub_1402454ad: %rdi"}
!23 = !{!24}
!24 = distinct !{!24, !12, !"helperstub_1402454ad: %rbp"}
!25 = !{!26}
!26 = distinct !{!26, !12, !"helperstub_1402454ad: %r8"}
!27 = !{!28}
!28 = distinct !{!28, !12, !"helperstub_1402454ad: %r9"}
!29 = !{!30}
!30 = distinct !{!30, !12, !"helperstub_1402454ad: %r10"}
!31 = !{!32}
!32 = distinct !{!32, !12, !"helperstub_1402454ad: %r11"}
!33 = !{!34}
!34 = distinct !{!34, !12, !"helperstub_1402454ad: %r12"}
!35 = !{!36}
!36 = distinct !{!36, !12, !"helperstub_1402454ad: %r13"}
!37 = !{!38}
!38 = distinct !{!38, !12, !"helperstub_1402454ad: %r14"}
!39 = !{!40}
!40 = distinct !{!40, !12, !"helperstub_1402454ad: %r15"}
!41 = !{!42}
!42 = distinct !{!42, !12, !"helperstub_1402454ad: %flags"}
!43 = !{!44}
!44 = distinct !{!44, !12, !"helperstub_1402454ad: %vsp"}
!45 = !{!11, !14, !16, !18, !20, !22, !24, !26, !28, !30, !32, !34, !36, !38, !40, !42, !46, !47}
!46 = distinct !{!46, !12, !"helperstub_1402454ad: %vip"}
!47 = distinct !{!47, !12, !"helperstub_1402454ad: %vmregs"}
!48 = !{!11, !14, !16, !18, !20, !22, !24, !26, !28, !30, !32, !34, !36, !38, !40, !42, !46}
!49 = !{!11, !14, !16, !18, !20, !22, !24, !26, !28, !30, !32, !34, !36, !40, !42, !44, !46, !47}
!50 = !{!11, !14, !16, !18, !20, !22, !24, !26, !28, !30, !32, !34, !36, !38, !40, !44, !46, !47}
!51 = !{!11, !14, !16, !18, !20, !22, !24, !28, !30, !32, !34, !36, !38, !40, !42, !44, !46, !47}
!52 = !{!11, !14, !18, !20, !22, !24, !26, !28, !30, !32, !34, !36, !38, !40, !42, !44, !46, !47}
!53 = !{!11, !14, !16, !18, !20, !22, !24, !26, !28, !30, !32, !34, !38, !40, !42, !44, !46, !47}
!54 = !{!11, !14, !16, !18, !20, !24, !26, !28, !30, !32, !34, !36, !38, !40, !42, !44, !46, !47}
!55 = !{!11, !14, !16, !20, !22, !24, !26, !28, !30, !32, !34, !36, !38, !40, !42, !44, !46, !47}
!56 = !{!14, !16, !18, !20, !22, !24, !26, !28, !30, !32, !34, !36, !38, !40, !42, !44, !46, !47}
!57 = !{!11, !14, !16, !18, !20, !22, !26, !28, !30, !32, !34, !36, !38, !40, !42, !44, !46, !47}
!58 = !{!11, !14, !16, !18, !20, !22, !24, !26, !28, !30, !32, !34, !36, !38, !42, !44, !46, !47}
!59 = !{!11, !14, !16, !18, !20, !22, !24, !26, !28, !30, !32, !36, !38, !40, !42, !44, !46, !47}
!60 = !{!11, !14, !16, !18, !20, !22, !24, !26, !28, !32, !34, !36, !38, !40, !42, !44, !46, !47}
!61 = !{!11, !16, !18, !20, !22, !24, !26, !28, !30, !32, !34, !36, !38, !40, !42, !44, !46, !47}
!62 = !{!11, !14, !16, !18, !20, !22, !24, !26, !28, !30, !34, !36, !38, !40, !42, !44, !46, !47}
!63 = !{!11, !14, !16, !18, !22, !24, !26, !28, !30, !32, !34, !36, !38, !40, !42, !44, !46, !47}
!64 = !{!11, !14, !16, !18, !20, !22, !24, !26, !30, !32, !34, !36, !38, !40, !42, !44, !46, !47}

```