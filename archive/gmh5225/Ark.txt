Project Path: arc_gmh5225_Ark_iuvrmtml

Source Tree:

```txt
arc_gmh5225_Ark_iuvrmtml
├── CITATION.cff
├── CMakeLists.txt
├── CODE_OF_CONDUCT.md
├── LICENSE
├── README.md
├── SECURITY.md
├── SUPPORT.md
├── ark
│   ├── CMakeLists.txt
│   ├── ark.cc
│   ├── cpu_timer.cc
│   ├── cpu_timer.h
│   ├── dims.cc
│   ├── dims_test.cc
│   ├── env.cc
│   ├── env.h
│   ├── executor.cc
│   ├── file_io.cc
│   ├── file_io.h
│   ├── gpu
│   │   ├── gpu_buf.cc
│   │   ├── gpu_buf.h
│   │   ├── gpu_comm_sw.cc
│   │   ├── gpu_comm_sw.h
│   │   ├── gpu_common.h
│   │   ├── gpu_compile.cc
│   │   ├── gpu_compile.h
│   │   ├── gpu_kernel.cc
│   │   ├── gpu_kernel.h
│   │   ├── gpu_kernel_test.cc
│   │   ├── gpu_logging.h
│   │   ├── gpu_mem.cc
│   │   ├── gpu_mem.h
│   │   ├── gpu_mem_test.cc
│   │   ├── gpu_mgr.cc
│   │   ├── gpu_mgr.h
│   │   └── gpu_mgr_test.cc
│   ├── include
│   │   ├── ark.h
│   │   ├── ark_utils.h
│   │   └── kernels
│   │       ├── activation.h
│   │       ├── arch.h
│   │       ├── arithmetic.h
│   │       ├── ark_kernels.h
│   │       ├── broadcast.h
│   │       ├── comm.h
│   │       ├── comm_mm.h
│   │       ├── common.h
│   │       ├── device.h
│   │       ├── ewise.h
│   │       ├── gemm.h
│   │       ├── half.h
│   │       ├── im2col.h
│   │       ├── layernorm.h
│   │       ├── matmul.h
│   │       ├── platform.h
│   │       ├── reduce.h
│   │       ├── smem.h
│   │       ├── softmax.h
│   │       ├── static_math.h
│   │       ├── sync.h
│   │       ├── transpose.h
│   │       ├── unit_op.h
│   │       └── vec.h
│   ├── ipc
│   │   ├── ipc_coll.cc
│   │   ├── ipc_coll.h
│   │   ├── ipc_coll_test.cc
│   │   ├── ipc_hosts.cc
│   │   ├── ipc_hosts.h
│   │   ├── ipc_lock.cc
│   │   ├── ipc_lock.h
│   │   ├── ipc_mem.cc
│   │   ├── ipc_mem.h
│   │   ├── ipc_mem_test.cc
│   │   ├── ipc_shm.cc
│   │   ├── ipc_shm.h
│   │   ├── ipc_socket.cc
│   │   ├── ipc_socket.h
│   │   ├── ipc_socket_test.cc
│   │   ├── ipc_table.cc
│   │   └── ipc_table.h
│   ├── kahypar.h
│   ├── kahypar_test.cc
│   ├── logging.cc
│   ├── logging.h
│   ├── math.cc
│   ├── math.h
│   ├── model.cc
│   ├── model.h
│   ├── model_test.cc
│   ├── net
│   │   ├── net_ib.cc
│   │   ├── net_ib.h
│   │   └── net_ib_test.cc
│   ├── ops
│   │   ├── kernels
│   │   │   ├── simple_add.h
│   │   │   ├── simple_dot.h
│   │   │   ├── simple_matmul_nt.h
│   │   │   ├── simple_mul.h
│   │   │   ├── simple_reduce.h
│   │   │   └── simple_scale.h
│   │   ├── ops_add.cc
│   │   ├── ops_add_test.cc
│   │   ├── ops_all_gather.cc
│   │   ├── ops_all_reduce.cc
│   │   ├── ops_all_reduce_test.cc
│   │   ├── ops_common.cc
│   │   ├── ops_common.h
│   │   ├── ops_dot_test.cc
│   │   ├── ops_gelu.cc
│   │   ├── ops_gelu_test.cc
│   │   ├── ops_identity.cc
│   │   ├── ops_identity_test.cc
│   │   ├── ops_im2col.cc
│   │   ├── ops_im2col_test.cc
│   │   ├── ops_layernorm.cc
│   │   ├── ops_layernorm_test.cc
│   │   ├── ops_matmul.cc
│   │   ├── ops_matmul_test.cc
│   │   ├── ops_max_pool.cc
│   │   ├── ops_mul.cc
│   │   ├── ops_mul_test.cc
│   │   ├── ops_reduce.cc
│   │   ├── ops_reduce_test.cc
│   │   ├── ops_relu.cc
│   │   ├── ops_reshape.cc
│   │   ├── ops_reshape_test.cc
│   │   ├── ops_scale.cc
│   │   ├── ops_scale_test.cc
│   │   ├── ops_sendrecv.cc
│   │   ├── ops_sendrecv_mm.cc
│   │   ├── ops_sendrecv_mm_test.cc
│   │   ├── ops_sendrecv_test.cc
│   │   ├── ops_sharding.cc
│   │   ├── ops_softmax.cc
│   │   ├── ops_tensor.cc
│   │   ├── ops_tensor_test.cc
│   │   ├── ops_test_common.cc
│   │   ├── ops_test_common.h
│   │   ├── ops_transpose.cc
│   │   └── ops_transpose_test.cc
│   ├── random.cc
│   ├── sched
│   │   ├── sched
│   │   │   ├── sched_default.cc
│   │   │   ├── sched_kahypar.cc
│   │   │   └── sched_simple.cc
│   │   ├── sched.cc
│   │   ├── sched.h
│   │   ├── sched_branch.cc
│   │   ├── sched_branch.h
│   │   ├── sched_branch_test.cc
│   │   ├── sched_codegen.cc
│   │   ├── sched_codegen.h
│   │   ├── sched_op.cc
│   │   ├── sched_op.h
│   │   ├── sched_opgraph.cc
│   │   ├── sched_opgraph.h
│   │   ├── sched_opgraph_test.cc
│   │   ├── sched_opseq.cc
│   │   ├── sched_opseq.h
│   │   ├── sched_profiler.cc
│   │   ├── sched_profiler.h
│   │   ├── sched_stream.cc
│   │   ├── sched_stream.h
│   │   ├── sched_stream_test.cc
│   │   ├── sched_test.cc
│   │   ├── sched_tile.cc
│   │   └── sched_tile.h
│   ├── tensor.cc
│   ├── tensor.h
│   ├── threading.h
│   ├── unittest
│   │   ├── unittest_utils.cc
│   │   └── unittest_utils.h
│   └── utils.cc
├── cmake
│   ├── FindIBVerbs.cmake
│   ├── FindNUMA.cmake
│   └── Utils.cmake
├── docker
│   ├── base-cuda11.8.dockerfile
│   └── base-cuda12.1.dockerfile
├── docs
│   ├── doxygen
│   │   └── Doxyfile
│   ├── env.md
│   ├── imgs
│   │   ├── GPU-driven_System_Architecture.svg
│   │   └── logos.svg
│   ├── install.md
│   ├── quickstart.md
│   ├── sphinx
│   │   ├── Makefile
│   │   ├── requirements.txt
│   │   └── source
│   │       ├── api.rst
│   │       ├── conf.py
│   │       └── index.rst
│   └── tutorial
│       ├── module_tutorial.md
│       └── multi_gpu_tutorial.md
├── examples
│   ├── ffn
│   │   ├── Makefile
│   │   └── ffn.cc
│   ├── transformer
│   │   ├── megatron_ark.py
│   │   ├── megatron_test.py
│   │   ├── transformer_ark.py
│   │   ├── transformer_pytorch.py
│   │   ├── transformer_test.py
│   │   └── transformer_utils.py
│   └── tutorial
│       ├── model_tutorial.py
│       ├── module_tutorial.py
│       ├── multi_gpu_tutorial.py
│       └── quickstart_tutorial.py
├── pyproject.toml
├── python
│   ├── CMakeLists.txt
│   ├── ark
│   │   ├── __init__.py
│   │   ├── executor.py
│   │   ├── model.py
│   │   ├── module.py
│   │   ├── runtime.py
│   │   ├── serialize.py
│   │   └── tensor.py
│   ├── bindings.cpp
│   └── unittest
│       ├── test_allgather.py
│       ├── test_allgather_parallel.py
│       ├── test_allreduce.py
│       ├── test_api.py
│       ├── test_gelu.py
│       ├── test_layernorm.py
│       ├── test_matmul.py
│       ├── test_reduce.py
│       ├── test_sendrecv.py
│       └── test_softmax.py
├── requirements.txt
└── third_party
    ├── CMakeLists.txt
    ├── Makefile
    ├── cutlass
    ├── gpudma
    ├── json
    │   └── json.h
    └── patches
        ├── cutlass
        │   └── include
        │       └── cutlass
        │           ├── conv
        │           │   └── threadblock
        │           │       ├── depthwise_fprop_pipelined.h.patch
        │           │       ├── implicit_gemm_fprop_fusion_multistage.h.patch
        │           │       ├── implicit_gemm_multistage.h.patch
        │           │       ├── implicit_gemm_pipelined.h.patch
        │           │       └── implicit_gemm_wgrad_fusion_multistage.h.patch
        │           ├── epilogue
        │           │   └── threadblock
        │           │       ├── epilogue.h.patch
        │           │       ├── epilogue_planar_complex.h.patch
        │           │       ├── epilogue_with_broadcast.h.patch
        │           │       ├── epilogue_with_reduction.h.patch
        │           │       └── epilogue_with_visitor.h.patch
        │           └── gemm
        │               ├── kernel
        │               │   └── gemm.h.patch
        │               ├── threadblock
        │               │   ├── mma_blas3_multistage.h.patch
        │               │   ├── mma_layernorm_mainloop_fusion_multistage.h.patch
        │               │   ├── mma_multistage.h.patch
        │               │   ├── mma_pipelined.h.patch
        │               │   ├── mma_planar_complex_multistage.h.patch
        │               │   ├── mma_planar_complex_pipelined.h.patch
        │               │   ├── mma_singlestage.h.patch
        │               │   ├── mma_softmax_mainloop_fusion_multistage.h.patch
        │               │   ├── mma_sparse_multistage.h.patch
        │               │   └── mma_with_reduction_multistage.h.patch
        │               └── warp
        │                   └── mma_tensor_op_tile_iterator_sm70.h.patch
        └── gpudma
            └── module
                └── ioctlrw.c.patch

```

`CITATION.cff`:

```cff
cff-version: 1.2.0
title: "ARK: A GPU-driven system framework for scalable AI applications"
version: 0.1.0
message: >-
  If you use this project in your research, please cite it as below.

repository-code: 'https://github.com/microsoft/ark'
abstract: >-
  ARK is a deep learning framework especially designed for highly optimized
  performance over distributed GPUs. Specifically, ARK adopts a GPU-driven
  execution model, where the GPU autonomously schedule and execute both
  computation and communication without any CPU intervention.
  ARK provides a set of APIs for users to express their distributed deep
  learning applications. ARK then automatically schedules a GPU-driven
  execution plan for the application, which generates a GPU kernel code
  called loop kernel. The loop kernel is a GPU kernel that contains a loop
  that iteratively executes the entire application, including both
  computation and communication. ARK then executes the loop kernel on the
  distributed GPUs.
license: MIT
license-url: https://github.com/microsoft/ark/blob/main/LICENSE

preferred-citation:
  type: conference-paper
  title: "ARK: GPU-driven Code Execution for Distributed Deep Learning"
  authors:
  - given-names: Changho
    family-names: Hwang
    affiliation: Microsoft Research, KAIST
  - given-names: KyoungSoo
    family-names: Park
    affiliation: KAIST
  - given-names: Ran
    family-names: Shu
    affiliation: Microsoft Research
  - given-names: Xinyuan
    family-names: Qu
    affiliation: Microsoft Research
  - given-names: Peng
    family-names: Cheng
    affiliation: Microsoft Research
  - given-names: Yongqiang
    family-names: Xiong
    affiliation: Microsoft Research
  booktitle: 20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)
  publisher: USENIX Association
  address: Boston, MA
  month: April
  year: 2023
  url: https://www.usenix.org/conference/nsdi23/presentation/hwang

```

`CMakeLists.txt`:

```txt
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

set(ARK_MAJOR "0")
set(ARK_MINOR "1")
set(ARK_PATCH "0")

set(ARK_VERSION "${ARK_MAJOR}.${ARK_MINOR}.${ARK_PATCH}")
set(ARK_SOVERSION "${ARK_MAJOR}.${ARK_MINOR}")

option(USE_KAHYPAR "Use KaHyPar for scheduling" OFF)

cmake_minimum_required(VERSION 3.25)
project(ark LANGUAGES CXX)
set(CMAKE_CXX_STANDARD 14)
set(CMAKE_CXX_FLAGS "${CMAKE_CXX_FLAGS} -Wall -Wextra")
set(BUILD_DIR ${CMAKE_CURRENT_BINARY_DIR})

# Find ibverbs
include(${PROJECT_SOURCE_DIR}/cmake/FindIBVerbs.cmake)

# Find NUMA
include(${PROJECT_SOURCE_DIR}/cmake/FindNUMA.cmake)

# Find CUDAToolkit
find_package(CUDAToolkit REQUIRED)

# Third party libraries
add_subdirectory(third_party)

# ARK object
add_library(ark_obj OBJECT)
set_target_properties(ark_obj PROPERTIES
    LINKER_LANGUAGE CXX
    POSITION_INDEPENDENT_CODE 1
    VERSION ${ARK_VERSION}
    SOVERSION ${ARK_SOVERSION}
)
add_dependencies(ark_obj tp-cutlass-patch)

# Build
add_custom_target(build)
add_dependencies(build ark_obj tp-cutlass)
add_custom_command(TARGET build POST_BUILD
    COMMAND ${CMAKE_COMMAND} -E copy_directory ${CMAKE_SOURCE_DIR}/ark/include ${BUILD_DIR}/include
)

# ARK shared library
add_library(ark SHARED)
set_target_properties(ark PROPERTIES
    VERSION ${ARK_VERSION}
    SOVERSION ${ARK_SOVERSION}
)
target_link_libraries(ark PUBLIC ark_obj)
add_dependencies(ark build)

# ARK static library
add_library(ark_static STATIC)
set_target_properties(ark_static PROPERTIES
    VERSION ${ARK_VERSION}
    SOVERSION ${ARK_SOVERSION}
)
target_link_libraries(ark_static PUBLIC ark_obj)
add_dependencies(ark_static build)

# ARK unit tests
include(CTest)
add_custom_target(ut)

# Details
add_subdirectory(ark)

# Install libraries
install(TARGETS ark ark_static
    LIBRARY DESTINATION ark/lib
    ARCHIVE DESTINATION ark/lib
    FILE_SET install_headers DESTINATION ark/include
    FILE_SET install_cutlass_headers DESTINATION ark/include/kernels
)

# Install Python module
if(BUILD_PYTHON)
    add_subdirectory(python)
endif()

# Utils
include(${PROJECT_SOURCE_DIR}/cmake/Utils.cmake)

```

`CODE_OF_CONDUCT.md`:

```md
# Microsoft Open Source Code of Conduct

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).

Resources:

- [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/)
- [Microsoft Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/)
- Contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with questions or concerns

```

`LICENSE`:

```
    MIT License

    Copyright (c) Microsoft Corporation.

    Permission is hereby granted, free of charge, to any person obtaining a copy
    of this software and associated documentation files (the "Software"), to deal
    in the Software without restriction, including without limitation the rights
    to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
    copies of the Software, and to permit persons to whom the Software is
    furnished to do so, subject to the following conditions:

    The above copyright notice and this permission notice shall be included in all
    copies or substantial portions of the Software.

    THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
    IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
    FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
    AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
    LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
    OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
    SOFTWARE

```

`README.md`:

```md
# ARK

A GPU-driven system framework for scalable AI applications.

[![License](https://img.shields.io/github/license/microsoft/ark.svg)](LICENSE)
[![CodeQL](https://github.com/microsoft/ark/actions/workflows/codeql.yml/badge.svg)](https://github.com/microsoft/ark/actions/workflows/codeql.yml)

See [Quick Start](docs/quickstart.md) to quickly get started.

## Overview

ARK is a deep learning framework especially designed for highly optimized performance over distributed GPUs. Specifically, ARK adopts a GPU-driven execution model, where the GPU autonomously schedule and execute both computation and communication without any CPU intervention.

ARK provides a set of APIs for users to express their distributed deep learning applications. ARK then automatically schedules a GPU-driven execution plan for the application, which generates a GPU kernel code called *loop kernel*. The loop kernel is a GPU kernel that contains a loop that iteratively executes the entire application, including both computation and communication. ARK then executes the loop kernel on the distributed GPUs.

![GPU-driven System Architecture](./docs/imgs/GPU-driven_System_Architecture.svg)

## Status & Roadmap

ARK is under active development and a part of its features will be added in a future release. The following describes key features of each version.

### ARK v0.1 (Latest Release)

* The default tile-based operator scheduler
* A simple software communication stack
* Transformer inference examples

### ARK v0.2 (TBU, Sep. 2023)

* A simple operator scheduler for debugging
* Extended communication interfaces
* Support more operators
* More inference examples

### ARK v0.3 (TBU, Nov. 2023)

* Full support for the operator profiler
* High-performance collective communication
* Support more operators
* More inference & training examples

## Contributing

This project welcomes contributions and suggestions.  Most contributions require you to agree to a
Contributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us
the rights to use your contribution. For details, visit https://cla.opensource.microsoft.com.

When you submit a pull request, a CLA bot will automatically determine whether you need to provide
a CLA and decorate the PR appropriately (e.g., status check, comment). Simply follow the instructions
provided by the bot. You will only need to do this once across all repos using our CLA.

This project has adopted the [Microsoft Open Source Code of Conduct](https://opensource.microsoft.com/codeofconduct/).
For more information see the [Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or
contact [opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.

## Trademarks

This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft 
trademarks or logos is subject to and must follow 
[Microsoft's Trademark & Brand Guidelines](https://www.microsoft.com/en-us/legal/intellectualproperty/trademarks/usage/general).
Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship.
Any use of third-party trademarks or logos are subject to those third-party's policies.

## Citations

<img src="./docs/imgs/logos.svg" alt="KAIST and Microsoft Logos" style="width: 350px;"/>

ARK is a collaborative research initiative between KAIST and Microsoft Research.
If you use this project in your research, please cite our [NSDI'23 paper]:

```bibtex
@inproceedings{HwangPSQCX23,
  author    = {Changho Hwang and
               KyoungSoo Park and
               Ran Shu and
               Xinyuan Qu and
               Peng Cheng and
               Yongqiang Xiong},
  title     = {ARK: GPU-driven Code Execution for Distributed Deep Learning},
  booktitle = {20th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 23)},
  year      = {2023},
  publisher = {{USENIX} Association},
}
```

[NSDI'23 paper]: https://www.usenix.org/conference/nsdi23/presentation/hwang

```

`SECURITY.md`:

```md
<!-- BEGIN MICROSOFT SECURITY.MD V0.0.8 BLOCK -->

## Security

Microsoft takes the security of our software products and services seriously, which includes all source code repositories managed through our GitHub organizations, which include [Microsoft](https://github.com/microsoft), [Azure](https://github.com/Azure), [DotNet](https://github.com/dotnet), [AspNet](https://github.com/aspnet), [Xamarin](https://github.com/xamarin), and [our GitHub organizations](https://opensource.microsoft.com/).

If you believe you have found a security vulnerability in any Microsoft-owned repository that meets [Microsoft's definition of a security vulnerability](https://aka.ms/opensource/security/definition), please report it to us as described below.

## Reporting Security Issues

**Please do not report security vulnerabilities through public GitHub issues.**

Instead, please report them to the Microsoft Security Response Center (MSRC) at [https://msrc.microsoft.com/create-report](https://aka.ms/opensource/security/create-report).

If you prefer to submit without logging in, send email to [secure@microsoft.com](mailto:secure@microsoft.com).  If possible, encrypt your message with our PGP key; please download it from the [Microsoft Security Response Center PGP Key page](https://aka.ms/opensource/security/pgpkey).

You should receive a response within 24 hours. If for some reason you do not, please follow up via email to ensure we received your original message. Additional information can be found at [microsoft.com/msrc](https://aka.ms/opensource/security/msrc). 

Please include the requested information listed below (as much as you can provide) to help us better understand the nature and scope of the possible issue:

  * Type of issue (e.g. buffer overflow, SQL injection, cross-site scripting, etc.)
  * Full paths of source file(s) related to the manifestation of the issue
  * The location of the affected source code (tag/branch/commit or direct URL)
  * Any special configuration required to reproduce the issue
  * Step-by-step instructions to reproduce the issue
  * Proof-of-concept or exploit code (if possible)
  * Impact of the issue, including how an attacker might exploit the issue

This information will help us triage your report more quickly.

If you are reporting for a bug bounty, more complete reports can contribute to a higher bounty award. Please visit our [Microsoft Bug Bounty Program](https://aka.ms/opensource/security/bounty) page for more details about our active programs.

## Preferred Languages

We prefer all communications to be in English.

## Policy

Microsoft follows the principle of [Coordinated Vulnerability Disclosure](https://aka.ms/opensource/security/cvd).

<!-- END MICROSOFT SECURITY.MD BLOCK -->

```

`SUPPORT.md`:

```md
# Support

## How to file issues and get help  

This project uses GitHub Issues to track bugs and feature requests. Please search the existing 
issues before filing new issues to avoid duplicates.  For new issues, file your bug or 
feature request as a new Issue.

For help and questions about using this project, please file them as new Issues.

## Microsoft Support Policy  

Support for this project is limited to the resources listed above.

```

`ark/CMakeLists.txt`:

```txt
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

file(GLOB_RECURSE SOURCES CONFIGURE_DEPENDS *.cc)
file(GLOB_RECURSE UT_SOURCES CONFIGURE_DEPENDS *_test.cc)
file(GLOB_RECURSE UT_COMMON_SOURCES CONFIGURE_DEPENDS ${CMAKE_CURRENT_SOURCE_DIR}/unittest/*.cc)
list(REMOVE_ITEM SOURCES ${UT_SOURCES} ${UT_COMMON_SOURCES})
file(GLOB_RECURSE INTERFACE_HEADERS CONFIGURE_DEPENDS include/ark*.h)
file(GLOB_RECURSE KERNEL_HEADERS CONFIGURE_DEPENDS include/kernels/*.h)
file(GLOB_RECURSE CUTLASS_HEADERS CONFIGURE_DEPENDS ${PROJECT_SOURCE_DIR}/third_party/cutlass/include/*.h)
if (NOT USE_KAHYPAR)
    list(REMOVE_ITEM SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/sched/sched/sched_kahypar.cc)
    list(REMOVE_ITEM UT_SOURCES ${CMAKE_CURRENT_SOURCE_DIR}/kahypar_test.cc)
endif()

set(COMMON_LIBS CUDA::cuda_driver CUDA::nvml ARK::numa ARK::ibverbs pthread rt)

# ARK object
target_include_directories(ark_obj PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/include)
target_include_directories(ark_obj PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})
target_include_directories(ark_obj SYSTEM PRIVATE
    ${PROJECT_SOURCE_DIR}/third_party/cutlass/include
    ${PROJECT_SOURCE_DIR}/third_party/gpudma/module
    ${PROJECT_SOURCE_DIR}/third_party/json
    ${CUDAToolkit_INCLUDE_DIRS}
    ${IBVERBS_INCLUDE_DIRS}
    ${NUMA_INCLUDE_DIRS}
)
target_sources(ark_obj PRIVATE ${SOURCES})
target_link_libraries(ark_obj PRIVATE ${COMMON_LIBS})

# Add header files to library targets
target_sources(ark PUBLIC
    FILE_SET install_headers
    TYPE HEADERS
    BASE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}/include
    FILES ${INTERFACE_HEADERS} ${KERNEL_HEADERS}
)
target_sources(ark_static PUBLIC
    FILE_SET install_headers
    TYPE HEADERS
    BASE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}/include
    FILES ${INTERFACE_HEADERS} ${KERNEL_HEADERS}
)
target_sources(ark PUBLIC
    FILE_SET install_cutlass_headers
    TYPE HEADERS
    BASE_DIRS ${PROJECT_SOURCE_DIR}/third_party/cutlass/include
    FILES ${CUTLASS_HEADERS}
)
target_sources(ark_static PUBLIC
    FILE_SET install_cutlass_headers
    TYPE HEADERS
    BASE_DIRS ${PROJECT_SOURCE_DIR}/third_party/cutlass/include
    FILES ${CUTLASS_HEADERS}
)

# ARK unit tests
foreach(ut_source IN ITEMS ${UT_SOURCES})
    get_filename_component(exe_name ${ut_source} NAME)
    add_executable(${exe_name} ${ut_source} ${UT_COMMON_SOURCES})
    add_dependencies(${exe_name} build)
    set_target_properties(${exe_name} PROPERTIES EXCLUDE_FROM_ALL TRUE)
    target_link_libraries(${exe_name} PRIVATE ark_obj ${COMMON_LIBS})
    target_include_directories(${exe_name} PRIVATE ${CMAKE_CURRENT_SOURCE_DIR})
    target_include_directories(${exe_name} SYSTEM PRIVATE
        ${PROJECT_SOURCE_DIR}/third_party/json
        ${CUDAToolkit_INCLUDE_DIRS}
        ${IBVERBS_INCLUDE_DIRS}
        ${NUMA_INCLUDE_DIRS}
    )
    add_test(NAME ${exe_name}
        WORKING_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}
        COMMAND ${exe_name}
    )
    add_dependencies(ut ${exe_name})
endforeach()

```

`ark/ark.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <sstream>
#include <string>
#include <vector>

#include "env.h"
#include "file_io.h"
#include "include/ark.h"
#include "logging.h"

#define SHM_DIR "/dev/shm/"

namespace ark {

std::string version()
{
    std::stringstream ss;
    ss << ARK_MAJOR << "." << ARK_MINOR << "." << ARK_PATCH;
    return ss.str();
}

void init()
{
    LOG(DEBUG, "init ark");
    // Clean up the shared memory directory. This is useful when the previous
    // run crashed, as this forces to remove locks generated by previous runs.
    // This may crash other ARK processes running on the same machine, if there
    // are any.
    const std::string shm_dir = SHM_DIR;
    const size_t len = shm_dir.size();
    const std::string &prefix = get_env().shm_name_prefix;
    std::vector<std::string> paths = list_dir(shm_dir);
    for (auto &path : paths) {
        if (path.substr(len, prefix.size()) == prefix) {
            if (remove_file(path) != 0) {
                LOGERR("init failed: failed to remove ", path, " (errno ",
                       errno, ")");
            }
        }
    }
}

} // namespace ark

```

`ark/cpu_timer.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "cpu_timer.h"
#include "logging.h"
#include <iostream>
#include <time.h>

namespace ark {

// Measure current time in second.
double cpu_timer(void)
{
    struct timespec tspec;
    if (clock_gettime(CLOCK_MONOTONIC, &tspec) == -1) {
        return -1;
    }
    return (tspec.tv_nsec / 1.0e9) + tspec.tv_sec;
}

// Measure current time in nanosecond.
long cpu_ntimer(void)
{
    struct timespec tspec;
    if (clock_gettime(CLOCK_MONOTONIC, &tspec) == -1) {
        return -1;
    }
    return tspec.tv_nsec;
}

// Sleep in second.
int cpu_timer_sleep(double sec)
{
    struct timespec tspec;
    tspec.tv_sec = (time_t)sec;
    tspec.tv_nsec = (long)((sec - tspec.tv_sec) * 1.0e9);
    return nanosleep(&tspec, 0);
}

// Sleep in nanosecond.
int cpu_ntimer_sleep(long nsec)
{
    struct timespec tspec;
    tspec.tv_sec = 0;
    tspec.tv_nsec = nsec;
    return nanosleep(&tspec, 0);
}

} // namespace ark

```

`ark/cpu_timer.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_CPU_TIMER_H_
#define ARK_CPU_TIMER_H_

namespace ark {

// Measure current time in second.
double cpu_timer(void);
// Measure current time in nanosecond.
long cpu_ntimer(void);
// Sleep in second.
int cpu_timer_sleep(double sec);
// Sleep in nanosecond.
int cpu_ntimer_sleep(long nsec);

} // namespace ark

#endif // ARK_CPU_TIMER_H_

```

`ark/dims.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "include/ark.h"
#include "json.h"
#include "logging.h"
#include <vector>

namespace ark {

// Construct with given four dimensions.
Dims::Dims(DimType d0, DimType d1, DimType d2, DimType d3)
{
    this->data[0] = d0;
    this->data[1] = d1;
    this->data[2] = d2;
    this->data[3] = d3;
    if (this->is_invalid()) {
        LOGERR("invalid dims given: <", d0, ", ", d1, ", ", d2, ", ", d3, ">");
    }
}

// Copy another Dims object.
Dims::Dims(const Dims &dims_)
{
    if (dims_.is_invalid()) {
        LOGERR("invalid dims given");
    }
    for (int i = 0; i < DIMS_LEN; ++i) {
        this->data[i] = dims_.data[i];
    }
}

// Construct from a vector. If the vector is shorter than DIMS_LEN, put
// following NO_DIMs. Raise an error if the vector is longer than DIMS_LEN.
Dims::Dims(const std::vector<DimType> &vec)
{
    int ds = (int)vec.size();
    if (ds > DIMS_LEN) {
        LOGERR("only support dims with size <= ", DIMS_LEN, ". Given: ", *this);
    }
    int i = 0;
    for (; i < ds; ++i) {
        const DimType &v = vec[i];
        if (v < 0 && v != NO_DIM) {
            LOGERR("invalid dims given at index ", i, ": ", v);
        }
        this->data[i] = v;
    }
    for (; i < DIMS_LEN; ++i) {
        this->data[i] = NO_DIM;
    }
}

// Return the volume of dimensions. If the dimensions are invalid, return -1.
DimType Dims::size() const
{
    const DimType *v = this->data;
    if (v[0] == NO_DIM) {
        return -1;
    }
    DimType ret = v[0];
    for (int i = 1; i < DIMS_LEN; ++i) {
        if (v[i] == NO_DIM) {
            break;
        } else {
            ret *= v[i];
        }
    }
    return ret;
}

// Return the number of valid dimensions.
int Dims::ndims() const
{
    const DimType *v = this->data;
    int ret = 0;
    for (; ret < DIMS_LEN; ++ret) {
        if (v[ret] == NO_DIM) {
            break;
        }
    }
    return ret;
}

// Return a new Dims object with 4 valid dimensions by prepending 1s.
Dims Dims::dims4() const
{
    const DimType *v = this->data;
    int nd = this->ndims();
    if (nd > DIMS_LEN) {
        LOGERR("only support dims with size <= ", DIMS_LEN, ". Given: ", nd);
    }
    Dims ret;
    for (int i = 0; i < DIMS_LEN - nd; ++i) {
        ret.data[i] = 1;
    }
    for (int i = 0; i < nd; ++i) {
        ret.data[DIMS_LEN - nd + i] = v[i];
    }
    return ret;
}

// Return true if the dimensions are empty.
bool Dims::is_no_dim() const
{
    const DimType *v = this->data;
    for (int i = 0; i < DIMS_LEN; ++i) {
        if (v[i] != NO_DIM) {
            return false;
        }
    }
    return true;
}

// Return true if the dimensions are invalid.
bool Dims::is_invalid() const
{
    // NO_DIM should not appear before a valid dimension.
    bool invalid_seen = false;
    const DimType *v = this->data;
    for (int i = 0; i < DIMS_LEN; ++i) {
        if (invalid_seen) {
            if (v[i] != NO_DIM) {
                return true;
            }
        } else {
            if (v[i] == NO_DIM) {
                invalid_seen = true;
            } else if (v[i] < 0) {
                return true;
            }
        }
    }
    return false;
}

DimType Dims::erase(int idx)
{
    int nd = this->ndims();
    if (idx >= nd || -idx > nd) {
        LOGERR("invalid index given: ", idx, " for ", *this);
    }
    if (idx < 0) {
        idx += nd;
    }
    DimType ret = this->data[idx];
    for (int i = idx; i < nd - 1; ++i) {
        this->data[i] = this->data[i + 1];
    }
    this->data[nd - 1] = NO_DIM;
    return ret;
}

DimType &Dims::operator[](int idx)
{
    int nd = this->ndims();
    if (idx >= nd || -idx > nd) {
        LOGERR("invalid index given: ", idx, " for ", *this);
    }
    if (idx < 0) {
        idx += nd;
    }
    return this->data[idx];
}

const DimType &Dims::operator[](int idx) const
{
    int nd = this->ndims();
    if (idx >= nd || -idx > nd) {
        LOGERR("invalid index given: ", idx, " for ", *this);
    }
    if (idx < 0) {
        idx += nd;
    }
    return this->data[idx];
}

bool operator==(const Dims &a, const Dims &b)
{
    for (int i = 0; i < DIMS_LEN; ++i) {
        if (a.data[i] != b.data[i]) {
            return false;
        }
    }
    return true;
}

bool operator!=(const Dims &a, const Dims &b)
{
    return !(a == b);
}

void to_json(nlohmann::json &j, const Dims &dims)
{
    j.clear();
    for (int i = 0; i < dims.ndims(); ++i) {
        j.push_back(dims.data[i]);
    }
}

void from_json(const nlohmann::json &j, Dims &dims)
{
    dims = Dims{j.get<std::vector<DimType>>()};
}

std::ostream &operator<<(std::ostream &os, const Dims &dims)
{
    if (dims.is_invalid()) {
        LOGERR("invalid dims given");
    }
    os << '<';
    if (dims.data[0] != NO_DIM) {
        os << dims.data[0];
        for (int i = 1; i < DIMS_LEN; ++i) {
            if (dims.data[i] == NO_DIM) {
                break;
            }
            os << ", " << dims.data[i];
        }
    }
    os << '>';
    return os;
}

} // namespace ark

```

`ark/dims_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "sstream"

#include "include/ark.h"
#include "unittest/unittest_utils.h"

using namespace std;

ark::unittest::State test_dims_basic()
{
    ark::Dims d0{1, 5, 9};
    UNITTEST_TRUE(!d0.is_invalid());
    UNITTEST_TRUE(!d0.is_no_dim());
    UNITTEST_EQ(d0.size(), 45);
    UNITTEST_EQ(d0.ndims(), 3);

    stringstream ss;
    ss << d0;
    UNITTEST_EQ(ss.str(), "<1, 5, 9>");

    ark::Dims d1{1, 5, 9, 1};
    UNITTEST_NE(d0, d1);
    UNITTEST_EQ(d0.size(), d1.size());

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_dims_no_dim()
{
    ark::Dims d0{};
    UNITTEST_TRUE(!d0.is_invalid());
    UNITTEST_TRUE(d0.is_no_dim());
    UNITTEST_EQ(d0.size(), -1);
    UNITTEST_EQ(d0.ndims(), 0);

    stringstream ss;
    ss << d0;
    UNITTEST_EQ(ss.str(), "<>");

    ark::Dims d1;
    UNITTEST_EQ(d0, d1);

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_dims_zero()
{
    ark::Dims d0{0, 10, 0};
    UNITTEST_TRUE(!d0.is_invalid());
    UNITTEST_TRUE(!d0.is_no_dim());
    UNITTEST_EQ(d0.size(), 0);
    UNITTEST_EQ(d0.ndims(), 3);

    stringstream ss;
    ss << d0;
    UNITTEST_EQ(ss.str(), "<0, 10, 0>");

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_dims_from_dims()
{
    ark::Dims d0{1, 2, 3, 4};
    UNITTEST_TRUE(!d0.is_invalid());
    UNITTEST_TRUE(!d0.is_no_dim());

    ark::Dims d1{d0};
    UNITTEST_EQ(d0, d1);

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_dims_from_vector()
{
    vector<ark::DimType> v0{1, 2, 3, 4};
    ark::Dims d0{v0};
    UNITTEST_TRUE(!d0.is_invalid());
    UNITTEST_TRUE(!d0.is_no_dim());

    UNITTEST_EQ(d0.size(), 24);
    UNITTEST_EQ(d0.ndims(), 4);

    UNITTEST_EQ(d0[0], 1);
    UNITTEST_EQ(d0[1], 2);
    UNITTEST_EQ(d0[2], 3);
    UNITTEST_EQ(d0[3], 4);

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_dims_neg_index()
{
    ark::Dims d0{10, 20, 30, 40};

    UNITTEST_EQ(d0[-0], 10);
    UNITTEST_EQ(d0[-1], 40);
    UNITTEST_EQ(d0[-2], 30);
    UNITTEST_EQ(d0[-3], 20);
    UNITTEST_EQ(d0[-4], 10);

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_dims_erase()
{
    ark::Dims d0{10, 20, 30, 40};

    UNITTEST_EQ(d0[0], 10);
    UNITTEST_EQ(d0[1], 20);
    UNITTEST_EQ(d0[2], 30);
    UNITTEST_EQ(d0[3], 40);
    UNITTEST_EQ(d0.ndims(), 4);

    ark::DimType ret = d0.erase(1);
    UNITTEST_EQ(ret, 20);

    UNITTEST_EQ(d0[0], 10);
    UNITTEST_EQ(d0[1], 30);
    UNITTEST_EQ(d0[2], 40);
    UNITTEST_EQ(d0.ndims(), 3);
    UNITTEST_EQ(d0.size(), 12000);
    UNITTEST_TRUE(!d0.is_invalid());
    UNITTEST_TRUE(!d0.is_no_dim());

    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_dims_basic);
    UNITTEST(test_dims_no_dim);
    UNITTEST(test_dims_zero);
    UNITTEST(test_dims_from_dims);
    UNITTEST(test_dims_from_vector);
    UNITTEST(test_dims_neg_index);
    UNITTEST(test_dims_erase);
    return 0;
}

```

`ark/env.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <cassert>
#include <cstdlib>
#include <cstring>
#include <iostream>

#include "env.h"

using namespace std;

#define DEFAULT_ARK_ROOT "/usr/local/ark"
#define DEFAULT_ARK_TMP "/tmp/ark"
#define DEFAULT_ARK_HOSTFILE_NAME "hostfile"
#define DEFAULT_ARK_IPC_LISTEN_PORT_BASE 42000
#define DEFAULT_ARK_NUM_RANKS_PER_HOST 8
#define DEFAULT_ARK_SCHEDULER "Default"
#define DEFAULT_ARK_DISABLE_GRAPH_OPT true
#define DEFAULT_ARK_SHM_NAME_PREFIX "ark."

namespace ark {

Env::Env()
{
    // Get log level.
    this->log_level = getenv("ARK_LOG_LEVEL");
    // Check if ARK_ROOT is set.
    const char *root_ca = getenv("ARK_ROOT");
    if (root_ca == nullptr) {
        root_ca = DEFAULT_ARK_ROOT;
    }
    this->path_root_dir = root_ca;
    // Set temporal directory path.
    const char *tmp_ca = getenv("ARK_TMP");
    if (tmp_ca == nullptr) {
        this->path_tmp_dir = DEFAULT_ARK_TMP;
    } else {
        this->path_tmp_dir = tmp_ca;
    }
    // If `ARK_KEEP_TMP=1`, we do not remove temporal files in `ARK_TMP`.
    const char *keep_tmp_ca = getenv("ARK_KEEP_TMP");
    if (keep_tmp_ca != nullptr && strncmp(keep_tmp_ca, "1", 2) == 0) {
        this->keep_tmp = true;
    } else {
        this->keep_tmp = false;
    }
    // Get the PCIe name (domain:bus:slot.function) of the FPGA.
    const char *fpga_ca = getenv("ARK_FPGA_DBSF");
    if (fpga_ca == nullptr) {
        this->fpga_dbsf = "";
    } else {
        this->fpga_dbsf = fpga_ca;
    }
    // Get the hostfile path.
    const char *hostfile_ca = getenv("ARK_HOSTFILE");
    if (hostfile_ca == nullptr) {
        this->hostfile = this->path_root_dir + "/" + DEFAULT_ARK_HOSTFILE_NAME;
    } else {
        this->hostfile = hostfile_ca;
    }
    // Get the listen socket port.
    const char *ipc_ca = getenv("ARK_IPC_LISTEN_PORT_BASE");
    if (ipc_ca == nullptr) {
        this->ipc_listen_port_base = DEFAULT_ARK_IPC_LISTEN_PORT_BASE;
    } else {
        this->ipc_listen_port_base = atoi(ipc_ca);
    }
    // Get the number of ranks per host.
    const char *ranks_ca = getenv("ARK_NUM_RANKS_PER_HOST");
    if (ranks_ca == nullptr) {
        this->num_ranks_per_host = DEFAULT_ARK_NUM_RANKS_PER_HOST;
    } else {
        this->num_ranks_per_host = atoi(ranks_ca);
    }
    // If `ARK_DISABLE_IB=1`, we disable IB networking.
    const char *disable_ib_ca = getenv("ARK_DISABLE_IB");
    if ((disable_ib_ca != nullptr) && (strncmp(disable_ib_ca, "1", 2) == 0)) {
        this->disable_ib = true;
    } else {
        this->disable_ib = false;
    }
    // If `ARK_DISABLE_P2P_MEMCPY=1`, we disable P2P CUDA memcpy.
    const char *disable_p2p_memcpy_ca = getenv("ARK_DISABLE_P2P_MEMCPY");
    if ((disable_p2p_memcpy_ca != nullptr) &&
        (strncmp(disable_p2p_memcpy_ca, "1", 2) == 0)) {
        this->disable_p2p_memcpy = true;
    } else {
        this->disable_p2p_memcpy = false;
    }
    // Specify the scheduler implementation. Supports "Default" and "Simple".
    const char *scheduler_ca = getenv("ARK_SCHEDULER");
    if (scheduler_ca == nullptr) {
        this->scheduler = DEFAULT_ARK_SCHEDULER;
    } else {
        this->scheduler = scheduler_ca;
    }
    // If `ARK_DISABLE_GRAPH_OPT=1`, we disable graph optimization.
    const char *disable_graph_opt_ca = getenv("ARK_DISABLE_GRAPH_OPT");
    if (disable_graph_opt_ca == nullptr) {
        this->disable_graph_opt = DEFAULT_ARK_DISABLE_GRAPH_OPT;
    } else if (strncmp(disable_graph_opt_ca, "1", 2) == 0) {
        this->disable_graph_opt = true;
    } else {
        this->disable_graph_opt = false;
    }
    //
    const char *shm_name_prefix_ca = getenv("ARK_SHM_NAME_PREFIX");
    if (shm_name_prefix_ca == nullptr) {
        this->shm_name_prefix = DEFAULT_ARK_SHM_NAME_PREFIX;
    } else {
        this->shm_name_prefix = shm_name_prefix_ca;
    }
}

// Global Env.
Env *_ARK_ENV_GLOBAL = nullptr;

// Get the global Env.
const Env &get_env()
{
    if (_ARK_ENV_GLOBAL == nullptr) {
        _ARK_ENV_GLOBAL = new Env;
        assert(_ARK_ENV_GLOBAL != nullptr);
    }
    return *_ARK_ENV_GLOBAL;
}

} // namespace ark

```

`ark/env.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_ENV_H_
#define ARK_ENV_H_

#include <string>

namespace ark {

// Environment variables.
struct Env
{
    Env();
    // Log level.
    const char *log_level;
    // Root directory where ARK is installed.
    std::string path_root_dir;
    // Temporary directory.
    std::string path_tmp_dir;
    // If true, we do not remove temporal files in `path_tmp_dir`.
    bool keep_tmp;
    // Hostfile.
    std::string hostfile;
    // PCIe name (domain:bus:slot.function) of the FPGA.
    std::string fpga_dbsf;
    // Base value of listen socket ports.
    int ipc_listen_port_base;
    // Number of ranks per host.
    int num_ranks_per_host;
    // Disable IB.
    bool disable_ib;
    // Disable P2P CUDA memcpy.
    bool disable_p2p_memcpy;
    // The scheduler to use.
    std::string scheduler;
    // Disable the heuristic ARK graph optimization.
    bool disable_graph_opt;
    // Prefix of shared memory file names.
    std::string shm_name_prefix;
};

// Get the global Env.
const Env &get_env();

} // namespace ark

#endif // ARK_ENV_H_

```

`ark/executor.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "env.h"
#include "gpu/gpu_kernel.h"
#include "include/ark.h"
#include "include/ark_utils.h"

#include "logging.h"
#include "sched/sched.h"
#include <algorithm>
#include <string>

using namespace std;

namespace ark {

class Executor::Impl
{
  public:
    GpuMgrCtx *ctx;
    BaseScheduler *sched;
    GpuLoopKernel *glk = nullptr;
    GpuStream stream = nullptr;
};

// Constructor.
Executor::Executor(const int gpu_id_, int rank_, int world_size_, Model &model,
                   const string &name, int num_warps_per_sm_)
    : gpu_id{gpu_id_}, rank{rank_},
      world_size{world_size_}, impl{make_unique<Impl>()}
{
    //
    GpuMgr *mgr = get_gpu_mgr(gpu_id);
    const GpuInfo &ginfo = mgr->get_gpu_info();
    if (get_env().scheduler == "Simple") {
        this->impl->sched = new SimpleScheduler{model, gpu_id_, rank_,
                                                world_size_, num_warps_per_sm_};
    }
    if (get_env().scheduler == "Default") {
        this->impl->sched = new DefaultScheduler{
            model, gpu_id_, rank_, world_size_, num_warps_per_sm_};
    }
#ifdef USE_KAHYPAR
    if (get_env().scheduler == "Kahypar") {
        this->impl->sched = new KahyparScheduler{
            model, gpu_id_, rank_, world_size_, num_warps_per_sm_};
    }
#endif // USE_KAHYPAR

    this->impl->sched->schedule();
    this->impl->ctx = this->impl->sched->create_context(name);
    this->impl->stream = this->impl->ctx->create_stream();
    auto codes = this->impl->sched->gen_code();

    this->impl->glk = new GpuLoopKernel{name,
                                        codes,
                                        (unsigned int)ginfo.num_sm,
                                        (unsigned int)num_warps_per_sm_,
                                        (unsigned int)ginfo.smem_block_total,
                                        "",
                                        this->impl->ctx};
}

// Destructor.
Executor::~Executor()
{
    if (this->impl->glk != nullptr) {
        delete this->impl->glk;
    }
    if (this->impl->ctx != nullptr) {
        GpuMgr *mgr = get_gpu_mgr(this->gpu_id);
        mgr->destroy_context(this->impl->ctx);
        this->impl->ctx = nullptr;
    }
}

// Compile the model. This must be called before `launch()`.
void Executor::compile()
{
    GpuMgr *mgr = get_gpu_mgr(gpu_id);
    this->impl->glk->compile(mgr->get_gpu_info());
}

// Launch the model (not running yet). This must be called after `compile()`.
void Executor::launch()
{
    this->impl->glk->load();
    GpuState ret = this->impl->glk->launch(this->impl->stream, false);
    if (ret != 0) {
        LOGERR("failed to launch this executor.");
    }
}

// Run the model for `iter` iterations.
void Executor::run(int iter)
{
    this->impl->glk->run(iter);
}

// Wait for the previous run to finish.
void Executor::wait()
{
    this->impl->glk->wait();
}

// Stop the model and return the elapsed time in milliseconds.
// Once this is called, we need to call `launch()` again to run the model again.
float Executor::stop()
{
    this->impl->glk->stop();
    return this->impl->glk->get_elapsed_msec();
}

// Get the corresponding GPU buffer of the executor from the given model tensor.
GpuBuf *Executor::get_gpu_buf(Tensor *tns) const
{
    return this->impl->sched->get_gpu_buf(tns);
}

// Copy contiguous data from a host buffer to the given tensor's (possibly
// non-contiguous) data range on GPU.
void Executor::tensor_memcpy(Tensor *dst, const void *src, size_t bytes)
{
    GpuBuf *buf = this->get_gpu_buf(dst);
    if (buf == nullptr) {
        LOGERR("failed to get GPU buffer for tensor ", dst->id);
    }
    Tensor *tns = dst;
    if (bytes > (size_t)tns->shape_bytes()) {
        LOGERR("the given number of bytes (", bytes,
               ") is larger than the tensor size (", tns->shape_bytes(), ")");
    }
    int ndims = tns->ndims();
    char *ps = (char *)src;
    if (ndims == 1) {
        gpu_memcpy(buf->ref(tns->offset_bytes(0)), ps, bytes);
        return;
    }
    size_t done = 0;
    size_t rem = bytes;
    for (DimType i = 0; i < tns->shape[0]; ++i) {
        if (ndims == 2) {
            size_t cb = min(rem, (size_t)tns->shape[1] * tns->type_bytes());
            gpu_memcpy(buf->ref(tns->offset_bytes(i, 0)), &ps[done], cb);
            rem -= cb;
            done += cb;
            if (rem == 0) {
                break;
            }
            continue;
        }
        for (DimType j = 0; j < tns->shape[1]; ++j) {
            if (ndims == 3) {
                size_t cb = min(rem, (size_t)tns->shape[2] * tns->type_bytes());
                gpu_memcpy(buf->ref(tns->offset_bytes(i, j, 0)), &ps[done], cb);
                rem -= cb;
                done += cb;
                if (rem == 0) {
                    break;
                }
                continue;
            }
            for (DimType k = 0; k < tns->shape[2]; ++k) {
                size_t cb = min(rem, (size_t)tns->shape[3] * tns->type_bytes());
                gpu_memcpy(buf->ref(tns->offset_bytes(i, j, k, 0)), &ps[done],
                           cb);
                rem -= cb;
                done += cb;
                if (rem == 0) {
                    break;
                }
            }
        }
    }
    assert(rem == 0);
    assert(done == bytes);
}

// Copy (possibly non-contiguous) data from a tensor on GPU to a contiguous
// host buffer. The given number of bytes is copied, in order of appearance
// on the memory. This function assumes that `dst` is large enough to hold
// the data.
void Executor::tensor_memcpy(void *dst, Tensor *src, size_t bytes)
{
    GpuBuf *buf = this->get_gpu_buf(src);
    if (buf == nullptr) {
        LOGERR("failed to get GPU buffer for tensor ", src->id);
    }
    Tensor *tns = src;
    if (bytes == 0) {
        bytes = tns->shape_bytes();
    } else if (bytes > (size_t)tns->shape_bytes()) {
        LOGERR("the given number of bytes (", bytes,
               ") is larger than the tensor size (", tns->shape_bytes(), ")");
    }
    int ndims = tns->ndims();
    char *pd = (char *)dst;
    if (ndims == 1) {
        gpu_memcpy(pd, buf->ref(tns->offset_bytes(0)), bytes);
        return;
    }
    size_t done = 0;
    size_t rem = bytes;
    for (DimType i = 0; i < tns->shape[0]; ++i) {
        if (ndims == 2) {
            size_t cb = min(rem, (size_t)tns->shape[1] * tns->type_bytes());
            gpu_memcpy(&pd[done], buf->ref(tns->offset_bytes(i, 0)), cb);
            rem -= cb;
            done += cb;
            if (rem == 0) {
                break;
            }
            continue;
        }
        for (DimType j = 0; j < tns->shape[1]; ++j) {
            if (ndims == 3) {
                size_t cb = min(rem, (size_t)tns->shape[2] * tns->type_bytes());
                gpu_memcpy(&pd[done], buf->ref(tns->offset_bytes(i, j, 0)), cb);
                rem -= cb;
                done += cb;
                if (rem == 0) {
                    break;
                }
                continue;
            }
            for (DimType k = 0; k < tns->shape[2]; ++k) {
                size_t cb = min(rem, (size_t)tns->shape[3] * tns->type_bytes());
                gpu_memcpy(&pd[done], buf->ref(tns->offset_bytes(i, j, k, 0)),
                           cb);
                rem -= cb;
                done += cb;
                if (rem == 0) {
                    break;
                }
            }
        }
    }
    assert(rem == 0);
    assert(done == bytes);
}

// Set all bytes of `tns` into zero.
void Executor::tensor_clear(Tensor *tns)
{
    GpuBuf *buf = this->get_gpu_buf(tns);
    if (buf == nullptr) {
        LOGERR("failed to get GPU buffer for tensor ", tns->id);
    }
    int ndims = tns->ndims();
    size_t bytes = tns->shape_bytes();
    assert(bytes % 4 == 0);
    size_t num = bytes >> 2;
    if (ndims == 1) {
        gpu_memset(buf->ref(tns->offset_bytes(0)), 0, num);
        return;
    }
    size_t done = 0;
    size_t rem = num;
    for (DimType i = 0; i < tns->shape[0]; ++i) {
        if (ndims == 2) {
            bytes = (size_t)tns->shape[1] * tns->type_bytes();
            assert(bytes % 4 == 0);
            size_t cn = min(rem, bytes >> 2);
            gpu_memset(buf->ref(tns->offset_bytes(i, 0)), 0, cn);
            rem -= cn;
            done += cn;
            if (rem == 0) {
                break;
            }
            continue;
        }
        for (DimType j = 0; j < tns->shape[1]; ++j) {
            if (ndims == 3) {
                bytes = (size_t)tns->shape[2] * tns->type_bytes();
                assert(bytes % 4 == 0);
                size_t cn = min(rem, bytes >> 2);
                gpu_memset(buf->ref(tns->offset_bytes(i, j, 0)), 0, cn);
                rem -= cn;
                done += cn;
                if (rem == 0) {
                    break;
                }
                continue;
            }
            for (DimType k = 0; k < tns->shape[2]; ++k) {
                bytes = (size_t)tns->shape[3] * tns->type_bytes();
                assert(bytes % 4 == 0);
                size_t cn = min(rem, bytes >> 2);
                gpu_memset(buf->ref(tns->offset_bytes(i, j, k, 0)), 0, cn);
                rem -= cn;
                done += cn;
                if (rem == 0) {
                    break;
                }
            }
        }
    }
    assert(rem == 0);
    assert(done == num);
}

void Executor::print_tensor(Tensor *tns)
{
    half_t *p = (half_t *)malloc(tns->shape_bytes());
    this->tensor_memcpy(p, tns, tns->shape_bytes());
    for (DimType i = 0; i < tns->shape[0]; ++i) {
        for (DimType j = 0; j < tns->shape[1]; ++j) {
            for (DimType k = 0; k < tns->shape[2]; ++k) {
                for (DimType l = 0; l < tns->shape[3]; ++l) {
                    printf("%f ", (float)p[tns->offset(i, j, k, l)]);
                }
                printf("\n");
            }
            printf("\n");
        }
        printf("\n");
    }
}

} // namespace ark

```

`ark/file_io.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <cerrno>
#include <cstring>
#include <dirent.h>
#include <fcntl.h>
#include <fstream>
#include <sstream>
#include <sys/stat.h>
#include <sys/types.h>
#include <unistd.h>

#include "file_io.h"
#include "logging.h"

using namespace std;

static bool get_stat(const char *path, struct stat *st)
{
    if (stat(path, st) == -1) {
        switch (errno) {
        case EACCES:
            LOGERR("permission denied: ", path);
        case EFAULT:
            LOGERR("bad address: ", path);
        case ENOENT:
        case ENOTDIR:
            return false;
        default:
            LOGERR("stat() for file path ", path, " failed with errno ", errno);
        };
    }
    return true;
}

namespace ark {

bool is_exist(const string &path)
{
    struct stat st;
    return get_stat(path.c_str(), &st);
}

bool is_dir(const string &path)
{
    struct stat st;
    if (!get_stat(path.c_str(), &st)) {
        return false;
    }
    return S_ISDIR(st.st_mode);
}

bool is_file(const string &path)
{
    struct stat st;
    if (!get_stat(path.c_str(), &st)) {
        return false;
    }
    return S_ISREG(st.st_mode);
}

int create_dir(const string &path)
{
    if (mkdir(path.c_str(), S_IRWXU | S_IRWXG | S_IROTH | S_IXOTH) == -1) {
        return errno;
    }
    return 0;
}

// Helper function to remove all files in a directory given a file descriptor.
int clear_dirat_helper(int dir_fd, const char *name)
{
    int r = -1;
    int fd = openat(dir_fd, name, O_RDONLY | O_NOFOLLOW | O_CLOEXEC);

    if (fd >= 0) {
        struct stat statbuf;

        if (!fstat(fd, &statbuf)) {
            if (S_ISDIR(statbuf.st_mode)) {
                r = clear_dirat_helper(fd, ".");
                if (!r) {
                    r = unlinkat(dir_fd, name, AT_REMOVEDIR);
                }
            } else {
                r = unlinkat(dir_fd, name, 0);
            }
        }
        close(fd);
    }

    return r;
}

// Remove all files in a directory.
int clear_dir(const string &path)
{
    const char *path_c = path.c_str();
    DIR *d = opendir(path_c);
    int r = -1;

    if (d) {
        struct dirent *p;
        int dir_fd = dirfd(d);

        r = 0;
        while (!r && (p = readdir(d))) {
            int r2 = -1;

            // Skip the names "." and ".." as we don't want to recurse on
            // them.
            if (!strcmp(p->d_name, ".") || !strcmp(p->d_name, "..")) {
                continue;
            }
            r2 = clear_dirat_helper(dir_fd, p->d_name);
            r = r2;
        }
        closedir(d);
    }

    return r;
}

vector<string> list_dir(const string &path)
{
    string path_str;
    if (path[path.size() - 1] == '/') {
        path_str = path.substr(0, path.size() - 1);
    } else {
        path_str = path;
    }
    const char *path_c = path_str.c_str();
    DIR *d = opendir(path_c);
    size_t path_len = strlen(path_c);
    int r = -1;

    vector<string> ret;

    if (d) {
        struct dirent *p;

        r = 0;
        while (!r && (p = readdir(d))) {
            int r2 = -1;
            char *buf;
            size_t len;

            // Skip the names "." and ".." as we don't want to recurse on them.
            if (!strcmp(p->d_name, ".") || !strcmp(p->d_name, "..")) {
                continue;
            }
            len = path_len + strlen(p->d_name) + 2;
            buf = (char *)malloc(len);

            if (buf) {
                struct stat statbuf;

                snprintf(buf, len, "%s/%s", path_c, p->d_name);
                if (!stat(buf, &statbuf)) {
                    ret.emplace_back(buf);
                }
                free(buf);
            }
            r = r2;
        }
        closedir(d);
    }

    return ret;
}

string read_file(const string &path)
{
    ifstream file(path);
    stringstream ss;
    ss << file.rdbuf();
    return ss.str();
}

void write_file(const string &path, const string &data)
{
    ofstream file(path, ios::out | ios::trunc);
    file << data;
}

int remove_file(const string &path)
{
    LOG(DEBUG, "remove file: ", path);
    return remove(path.c_str());
}

string get_dir(const string &path)
{
    size_t len = path.size();
    while (len-- > 0) {
        if (path[len] == '/') {
            break;
        }
    }
    return path.substr(0, len);
}

} // namespace ark

```

`ark/file_io.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_FILE_IO_H_
#define ARK_FILE_IO_H_

#include <string>
#include <vector>

namespace ark {

bool is_exist(const std::string &path);

bool is_dir(const std::string &path);
bool is_file(const std::string &path);
int create_dir(const std::string &path);
// int clear_dir(const std::string &path);
std::vector<std::string> list_dir(const std::string &path);

std::string read_file(const std::string &path);
void write_file(const std::string &path, const std::string &data);
int remove_file(const std::string &path);
std::string get_dir(const std::string &path);

} // namespace ark

#endif // ARK_FILE_IO_H_

```

`ark/gpu/gpu_buf.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <cassert>

#include "gpu/gpu_buf.h"

using namespace std;

namespace ark {

GpuBuf::GpuBuf(const GpuMem *mem_, int id_, size_t offset_, size_t bytes_)
    : mem{mem_}, id{id_}, offset{offset_}, bytes{bytes_}
{
    assert(mem_ != nullptr);
}

GpuPtr GpuBuf::ref(size_t off) const
{
    return this->mem->ref(this->offset + off);
}

uint64_t GpuBuf::pref(size_t off) const
{
    return this->mem->pref(this->offset + off);
}

void *GpuBuf::href(size_t off) const
{
    return this->mem->href(this->offset + off);
}

} // namespace ark

```

`ark/gpu/gpu_buf.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_GPU_BUF_H_
#define ARK_GPU_BUF_H_

#include <memory>

#include "gpu/gpu_mem.h"

namespace ark {

//
class GpuBuf
{
  public:
    GpuBuf(const GpuMem *mem, int id, size_t offset, size_t bytes);

    GpuPtr ref(size_t off = 0) const;

    uint64_t pref(size_t off = 0) const;
    void *href(size_t off = 0) const;

    const size_t &get_offset() const
    {
        return offset;
    }
    void set_offset(size_t off)
    {
        offset = off;
    }

    const GpuMem *get_mem() const
    {
        return mem;
    }
    const int &get_id() const
    {
        return id;
    }
    const size_t &get_bytes() const
    {
        return bytes;
    }

  private:
    const GpuMem *mem;
    // ID of a local buffer or SID of a remote buffer.
    int id;
    size_t offset;
    size_t bytes;
};

} // namespace ark

#endif // ARK_GPU_BUF_H_

```

`ark/gpu/gpu_comm_sw.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <cassert>
#include <cerrno>

#include "cpu_timer.h"
#include "env.h"
#include "gpu/gpu_comm_sw.h"
#include "gpu/gpu_logging.h"
#include "gpu/gpu_mgr.h"
#include "ipc/ipc_coll.h"
#include "ipc/ipc_hosts.h"

using namespace std;

namespace ark {

//
GpuCommSw::GpuCommSw(const string &name_, const int gpu_id_, const int rank_,
                     const int world_size_, GpuMem *data_mem, GpuMem *sc_rc_mem)
    : name{name_}, gpu_id{gpu_id_}, rank{rank_},
      world_size{world_size_}, request{new Request}
{
    // Register `this->request` as a mapped & pinned address.
    CULOG(cuMemHostRegister((void *)this->request, sizeof(request),
                            CU_MEMHOSTREGISTER_DEVICEMAP));

    // Reserve entries for GPU communication stack information.
    // Power of 2 larger than `gpu_id_` and at least 8.
    int num_entries = 8;
    while (gpu_id_ >= num_entries) {
        num_entries *= 2;
    }
    this->data_mems.resize(num_entries, nullptr);
    this->sc_rc_mems.resize(num_entries, nullptr);
    this->infos.resize(num_entries, nullptr);

    // Create the local stack info.
    this->data_mems[gpu_id_] = data_mem;
    this->sc_rc_mems[gpu_id_] = sc_rc_mem;

    IpcMem *ie =
        new IpcMem{ARK_GPU_INFO_NAME + name_ + to_string(gpu_id_), true};
    this->infos_storage.emplace_back(ie);
    this->infos[gpu_id_] = ie;

    int port_base = get_env().ipc_listen_port_base;
    int host_id = rank_ / get_env().num_ranks_per_host;
    this->ipc_socket = new IpcSocket{get_host(host_id), port_base + gpu_id_};

    if (!get_env().disable_ib) {
        int num_ib_dev = get_net_ib_device_num();
        // TODO: be aware of GPU-NIC interconnect topology.
        int ib_dev_id = gpu_id_ % num_ib_dev;
        this->net_ib_mgr = get_net_ib_mgr(ib_dev_id);
        this->sid_mrs.resize(MAX_NUM_SID, nullptr);
    }
}

GpuCommSw::~GpuCommSw()
{
    this->stop_request_loop();
    if (this->request != nullptr) {
        cuMemHostUnregister((void *)this->request);
        delete this->request;
    }
    delete this->ipc_socket;
}

void GpuCommSw::reg_sendrecv(int sid, int remote_rank, size_t bytes,
                             bool is_recv)
{
    this->send_recv_infos.emplace_back(
        GpuSendRecvInfo{sid, remote_rank, bytes, is_recv});
}

//
void GpuCommSw::configure(vector<pair<int, size_t>> &export_sid_offs,
                          map<int, vector<GpuBuf *>> &import_gid_bufs)
{
    map<int, size_t> sid_max_bytes;
    if (this->is_using_ib()) {
        // Create QPs
        for (auto &srinfo : this->send_recv_infos) {
            auto it = this->qps.find(srinfo.remote_rank);
            if (it == this->qps.end()) {
                NetIbQp *qp = this->net_ib_mgr->create_qp();
                if (qp == nullptr) {
                    LOGERR("create_qp failed");
                }
                this->qps[srinfo.remote_rank] = qp;
            }
            sid_max_bytes[srinfo.sid] =
                max(sid_max_bytes[srinfo.sid], srinfo.bytes);
        }
    }

    //
    GpuMem *data_mem = this->get_data_mem(this->gpu_id);
    for (auto &p : export_sid_offs) {
        int sid = p.first;
        size_t off = p.second;
        this->addr_table[this->gpu_id][sid] = data_mem->ref(off);
    }

    if (this->is_using_ib()) {
        // Create MRs
        for (auto &p : export_sid_offs) {
            int sid = p.first;
            if (this->sid_mrs[sid] == nullptr) {
                NetIbMr *mr = this->net_ib_mgr->reg_mr(
                    (void *)this->addr_table[this->gpu_id][sid],
                    sid_max_bytes[sid]);
                this->sid_mrs[sid] = mr;
            }
        }
    }

    //
    GpuCommInfo gi;
    for (auto &p : export_sid_offs) {
        int sid = p.first;
        size_t off = p.second;
        gi.sid_offs[sid] = off;
    }
    gi.bytes = data_mem->get_bytes();
    if (this->is_using_ib()) {
        for (size_t sid = 0; sid < this->sid_mrs.size(); ++sid) {
            if (this->sid_mrs[sid] != nullptr) {
                gi.sid_mris[sid] = this->sid_mrs[sid]->get_info();
            }
        }
    }
    IpcSocket::State s;
    for (auto &p : this->qps) {
        int remote_rank = p.first;
        NetIbQp *qp = p.second;
        if (qp == nullptr) {
            LOGERR("unexpected error");
        }
        gi.qpi = qp->get_info();
        s = this->ipc_socket->add_item(
            "gpu_comm_info_" + std::to_string(remote_rank), &gi, sizeof(gi));
        if (s != IpcSocket::State::SUCCESS) {
            LOGERR("Failed to add gpu_comm_info to ipc_socket");
        }
    }

    //
    IpcMem *info = this->get_info(this->gpu_id);
    {
        //
        IpcLockGuard lg{info->get_lock()};
        GpuCommInfo *gi = (GpuCommInfo *)info->alloc(sizeof(GpuCommInfo));
        for (auto &p : export_sid_offs) {
            int sid = p.first;
            size_t off = p.second;
            gi->sid_offs[sid] = off;
        }
        gi->bytes = data_mem->get_bytes();
    }

    //
    for (auto &p : import_gid_bufs) {
        int gid = p.first;
        IpcMem *ie = this->get_info(gid);
        if (ie->get_bytes() == 0) {
            ie->alloc(sizeof(GpuCommInfo));
        }

        GpuMem *mem = this->get_data_mem(gid);
        GpuCommInfo *info = (GpuCommInfo *)ie->get_addr();
        assert(info != nullptr);

        // Create a GPU memory mapping if it has not done yet.
        if (mem->get_bytes() == 0) {
            while (info->bytes == 0) {
                sched_yield();
            }
            mem->alloc(info->bytes);
        }

        //
        IpcLockGuard lg{ie->get_lock()};
        for (GpuBuf *buf : p.second) {
            int sid = buf->get_id();
            size_t off = info->sid_offs[sid];
            this->addr_table[gid][sid] = mem->ref(off);
            buf->set_offset(off);
        }
    }

    //
    if (this->is_using_ib()) {
        int port_base = get_env().ipc_listen_port_base;
        int ret;
        for (auto &p : this->qps) {
            int remote_rank = p.first;
            // TODO: generalize converting rank to GPU ID.
            int nrph = get_env().num_ranks_per_host;
            int remote_gpu_id = remote_rank % nrph;
            int remote_host_id = remote_rank / nrph;
            NetIbQp *qp = p.second;
            GpuCommInfo gi_remote;

            LOG(DEBUG, "querying gpu_comm_info_", this->rank, " from rank ",
                remote_rank, " (", get_host(remote_host_id), ":",
                port_base + remote_gpu_id, ")");
            s = this->ipc_socket->query_item(
                get_host(remote_host_id), port_base + remote_gpu_id,
                "gpu_comm_info_" + std::to_string(this->rank), &gi_remote,
                sizeof(gi_remote), true);
            if (s != IpcSocket::State::SUCCESS) {
                LOGERR("Failed to query gpu_comm_info from ipc_socket");
            }

            ret = qp->rtr(&gi_remote.qpi);
            if (ret != 0) {
                LOG(ERROR, "NetIbQp::rtr failed");
            }
            LOG(DEBUG, "RANK ", this->rank, " QP ", qp->get_info().qpn,
                " <--> RANK ", remote_rank, " QP ", gi_remote.qpi.qpn);
            ret = qp->rts();
            if (ret != 0) {
                LOG(ERROR, "NetIbQp::rts failed");
            }

            auto &mri_vec = this->mris[remote_rank];
            mri_vec.resize(MAX_NUM_SID);
            for (int sid = 0; sid < (int)mri_vec.size(); ++sid) {
                mri_vec[sid] = gi_remote.sid_mris[sid];
            }
        }

        // Sync with remote QPs
        int dummy_data = 42;
        s = this->ipc_socket->add_item("comm_config_done", &dummy_data,
                                       sizeof(dummy_data));
        if (s != IpcSocket::State::SUCCESS) {
            LOGERR("Failed to add comm_config_done to ipc_socket");
        }
        for (auto &p : this->qps) {
            int remote_rank = p.first;
            // TODO: generalize converting rank to GPU ID.
            int nrph = get_env().num_ranks_per_host;
            int remote_gpu_id = remote_rank % nrph;
            int remote_host_id = remote_rank / nrph;
            int remote_data;
            s = this->ipc_socket->query_item(
                get_host(remote_host_id), port_base + remote_gpu_id,
                "comm_config_done", &remote_data, sizeof(remote_data), true);
            if (s != IpcSocket::State::SUCCESS) {
                LOGERR("Failed to query gpu_comm_info from ipc_socket");
            }
            if (remote_data != dummy_data) {
                LOGERR("Failed to sync comm_config_done");
            }
        }
    }
    LOG(DEBUG, "RANK ", this->rank, " config done");
}

//
void GpuCommSw::import_buf(const int gid, GpuBuf *buf)
{
    IpcMem *ie = this->get_info(gid);
    if (ie->get_bytes() == 0) {
        ie->alloc(sizeof(GpuCommInfo));
    }

    GpuMem *mem = this->get_data_mem(gid);
    GpuCommInfo *info = (GpuCommInfo *)ie->get_addr();
    assert(info != nullptr);

    // Create a GPU memory mapping if it has not done yet.
    if (mem->get_bytes() == 0) {
        if (info->bytes == 0) {
            LOGERR("unexpected error");
        }
        mem->alloc(info->bytes);
    }
    //
    {
        IpcLockGuard lg{ie->get_lock()};
        int sid = buf->get_id();
        size_t off = info->sid_offs[sid];
        this->addr_table[gid][sid] = mem->ref(off);
        buf->set_offset(off);
    }
}

//
void GpuCommSw::launch_request_loop()
{
    if (this->request_loop_thread == nullptr) {
        this->run_request_loop_thread = true;
        this->request_loop_thread = new thread([&, gid = this->gpu_id] {
            //
            GpuState ret = get_gpu_mgr(gid)->set_current();
            if (ret == CUDA_SUCCESS) {
                //
                this->request_loop();
            } else if (ret != CUDA_ERROR_DEINITIALIZED) {
                CULOG(ret);
            }
        });
        assert(this->request_loop_thread != nullptr);
    } else {
        assert(this->run_request_loop_thread);
    }
}

//
void GpuCommSw::request_loop()
{
    const size_t sc_offset = 0;
    const size_t rc_offset = MAX_NUM_SID * sizeof(int);
    const int sid_shift = 8;

    // Get the local SC/RC host addresses.
    volatile int *sc_href =
        (volatile int *)this->get_sc_rc_mem(this->gpu_id)->href(sc_offset);
    assert(sc_href != nullptr);
    volatile int *rc_href =
        (volatile int *)this->get_sc_rc_mem(this->gpu_id)->href(rc_offset);
    assert(rc_href != nullptr);

    for (int r = 0; r < (int)this->qps.size(); ++r) {
        NetIbQp *qp = this->qps[r];
        if (qp != nullptr) {
            int ret = qp->post_recv(((uint64_t)r << sid_shift) + 1);
            if (ret != 0) {
                LOGERR("post_recv() returns ", ret);
            }
        }
    }

    //
    const bool is_using_p2p_memcpy = !get_env().disable_p2p_memcpy;
    const bool is_using_ib = this->is_using_ib();
    if (!is_using_p2p_memcpy && !is_using_ib) {
        LOGERR("no method for transport");
    }
    bool is_idle = false;
    unsigned int busy_counter = 0;
    const unsigned int max_busy_counter = 3000000000;
    // Request pointer.
    volatile uint64_t *db_val = &(this->request->value);
    // Request processing loop.
    while (this->run_request_loop_thread) {
        int wcn = 0;
        if (is_using_ib) {
            wcn = this->net_ib_mgr->poll_cq();
        }
        if (wcn > 0) {
            for (int i = 0; i < wcn; ++i) {
                int status = this->net_ib_mgr->get_wc_status(i);
                if (status != 0) {
                    LOGERR("get_wc_status() returns ", status, ": ",
                           this->net_ib_mgr->get_wc_status_str(i));
                }
                uint64_t wr_id = this->net_ib_mgr->get_wc_wr_id(i);
                if (wr_id & 0x1) {
                    // recv complete
                    unsigned int sid_dst = this->net_ib_mgr->get_wc_imm_data(i);
                    rc_href[sid_dst] = 1;
                    NetIbQp *qp = this->qps[wr_id >> sid_shift];
                    if (qp == nullptr) {
                        LOGERR("Unexpected error");
                    }
                    int ret = qp->post_recv(wr_id);
                    if (ret != 0) {
                        LOGERR("post_recv() returns ", ret);
                    }
                    LOG(DEBUG, "RC DST: ", sid_dst);
                } else {
                    // send complete
                    unsigned int sid_src = wr_id >> sid_shift;
                    sc_href[sid_src] = 1;
                    LOG(DEBUG, "SC SRC: ", sid_src);
                }
            }
            is_idle = false;
        } else if (wcn < 0) {
            LOGERR("poll_cq() returns ", wcn);
        }
        uint64_t v = *db_val;
        if (v == (uint64_t)REQUEST_INVALID) {
            if (wcn == 0) {
                if (is_idle) {
                    if (cpu_ntimer_sleep(0) != 0) {
                        LOG(WARN, "cpu_ntimer_sleep() returns errno ", errno);
                    }
                } else if (++busy_counter > max_busy_counter) {
                    is_idle = true;
                    LOG(DEBUG, "Idle.");
                }
            }
            continue;
        }
        *db_val = (uint64_t)REQUEST_INVALID;
        Request &db = (Request &)v;
        LOG(DEBUG, "Request arrived.");
        //
        GpuPtr src = this->addr_table[this->gpu_id][db.fields.src];
        if (src == 0) {
            LOGERR("Invalid SRC SID ", db.fields.src, " in GPU ", this->gpu_id);
        }
        LOG(DEBUG, "Request SRC: RANK ", this->rank, ", sid ", db.fields.src,
            ", ", (void *)src);
        GpuPtr dst = 0;
        // TODO: generalize converting rank to GPU ID.
        int nrph = get_env().num_ranks_per_host;
        int gid_dst = db.fields.rank % nrph;
        if ((db.fields.rank / nrph) != (this->rank / nrph)) {
            // This GPU is not in this machine.
            gid_dst = -1;
            LOG(DEBUG, "Request DST: RANK ", db.fields.rank, ", sid ",
                db.fields.dst, ", remote");
        } else {
            dst = this->addr_table[gid_dst][db.fields.dst];
            if (dst == 0) {
                LOGERR("Invalid DST SID ", db.fields.dst, " in GPU ", gid_dst);
            }
            LOG(DEBUG, "Request DST: RANK ", db.fields.rank, ", sid ",
                db.fields.dst, ", ", (void *)dst);
        }
        LOG(DEBUG, "Request LEN: ", db.fields.len);

        // Transfer data.
        if (is_using_p2p_memcpy && (gid_dst != -1)) {
            CULOG(cuMemcpyDtoD(dst, src, db.fields.len));
            GpuMem *mem = this->get_sc_rc_mem(db.fields.rank);
            volatile int *rc_array = (volatile int *)mem->href(rc_offset);
            if (rc_array != nullptr) {
                rc_array[db.fields.dst] = 1;
            } else {
                GpuPtr rc_ref =
                    mem->ref(rc_offset + db.fields.dst * sizeof(int));
                CULOG(cuMemsetD32(rc_ref, 1, 1));
            }
            sc_href[db.fields.src] = 1;
        } else {
            NetIbQp *qp = this->qps[db.fields.rank];
            int ret = qp->stage_send(
                this->sid_mrs[db.fields.src],
                &this->mris[db.fields.rank][db.fields.dst], db.fields.len,
                (db.fields.src << sid_shift), db.fields.dst);
            if (ret != 1) {
                LOGERR("stage_send() returns ", ret);
            }
            ret = qp->post_send();
            if (ret != 0) {
                LOGERR("post_send() returns ", ret);
            }
        }
        LOG(DEBUG, "Request processed.");
        //
        is_idle = false;
        busy_counter = 0;
    }
}

//
void GpuCommSw::stop_request_loop()
{
    this->run_request_loop_thread = false;
    if (this->request_loop_thread != nullptr) {
        if (this->request_loop_thread->joinable()) {
            this->request_loop_thread->join();
        }
        delete this->request_loop_thread;
        this->request_loop_thread = nullptr;
    }
}

//
void GpuCommSw::set_request(const Request &db)
{
    if (this->request != nullptr) {
        *(this->request) = db;
    }
}

//
GpuMem *GpuCommSw::get_data_mem(const int gid)
{
    int sz = (int)this->data_mems.size();
    assert(sz == (int)this->sc_rc_mems.size());
    if (sz <= gid) {
        while (sz <= gid) {
            sz *= 2;
        }
        this->data_mems.resize(sz, nullptr);
    }
    GpuMem *dm = this->data_mems[gid];
    if (dm == nullptr) {
        dm = new GpuMem{ARK_GPU_DATA_NAME + this->name + to_string(gid), 0,
                        false};
        assert(dm != nullptr);
        this->data_mems[gid] = dm;
        this->remote_data_mems_storage.emplace_back(dm);
    }
    //
    while (this->addr_table.size() < this->data_mems.size()) {
        this->addr_table.emplace_back();
        this->addr_table.back().resize(256, 0);
    }
    return dm;
}

//
GpuMem *GpuCommSw::get_sc_rc_mem(const int gid)
{
    int sz = (int)this->sc_rc_mems.size();
    assert(sz == (int)this->sc_rc_mems.size());
    if (sz <= gid) {
        while (sz <= gid) {
            sz *= 2;
        }
        this->sc_rc_mems.resize(sz, nullptr);
    }
    GpuMem *sm = this->sc_rc_mems[gid];
    if (sm == nullptr) {
        sm = new GpuMem{ARK_GPU_SC_RC_NAME + this->name + to_string(gid),
                        2 * MAX_NUM_SID * sizeof(int), false};
        this->sc_rc_mems[gid] = sm;
        this->remote_sc_rc_mems_storage.emplace_back(sm);
    }
    return sm;
}

//
IpcMem *GpuCommSw::get_info(const int gid)
{
    int sz = (int)this->infos.size();
    assert(sz == (int)this->infos.size());
    if (sz <= gid) {
        while (sz <= gid) {
            sz *= 2;
        }
        this->infos.resize(sz, nullptr);
    }
    IpcMem *ie = this->infos[gid];
    if (ie == nullptr) {
        ie = new IpcMem{ARK_GPU_INFO_NAME + this->name + to_string(gid), false};
        assert(ie != nullptr);
        this->infos_storage.emplace_back(ie);
        this->infos[gid] = ie;
    }
    return ie;
}

//
GpuPtr GpuCommSw::get_request_ref() const
{
    GpuPtr ref;
    CULOG(cuMemHostGetDevicePointer(&ref, this->request, 0));
    return ref;
}

} // namespace ark

```

`ark/gpu/gpu_comm_sw.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_GPU_COMM_SW_H_
#define ARK_GPU_COMM_SW_H_

#include <list>
#include <map>
#include <memory>
#include <set>
#include <string>
#include <sys/mman.h>
#include <thread>
#include <vector>

#include "gpu/gpu_buf.h"
#include "gpu/gpu_common.h"
#include "ipc/ipc_socket.h"
#include "net/net_ib.h"

namespace ark {

//
struct GpuCommInfo
{
    NetIbMr::Info sid_mris[MAX_NUM_SID];
    uint64_t sid_offs[MAX_NUM_SID];
    NetIbQp::Info qpi;
    uint64_t bytes;
};

//
struct GpuSendRecvInfo
{
    int sid;
    int remote_rank;
    std::size_t bytes;
    bool is_recv;
};

//
class GpuCommSw
{
  public:
    GpuCommSw(const std::string &name, const int gpu_id_, const int rank_,
              const int world_size_, GpuMem *data_mem, GpuMem *sc_rc_mem);
    ~GpuCommSw();

    void reg_sendrecv(int sid, int remote_rank, std::size_t bytes,
                      bool is_recv);
    void configure(std::vector<std::pair<int, size_t>> &export_sid_offs,
                   std::map<int, std::vector<GpuBuf *>> &import_gid_bufs);
    void import_buf(const int gid, GpuBuf *buf);

    void request_loop();
    void launch_request_loop();
    void stop_request_loop();

    void set_request(const Request &db);

    GpuMem *get_data_mem(const int gid);
    GpuMem *get_sc_rc_mem(const int gid);
    IpcMem *get_info(const int gid);
    GpuPtr get_request_ref() const;
    bool is_using_ib() const
    {
        return this->net_ib_mgr != nullptr;
    }

  private:
    //
    const std::string name;
    //
    const int gpu_id;
    const int rank;
    const int world_size;
    //
    std::list<std::unique_ptr<GpuMem>> remote_data_mems_storage;
    std::list<std::unique_ptr<GpuMem>> remote_sc_rc_mems_storage;
    std::list<std::unique_ptr<IpcMem>> infos_storage;
    //
    std::vector<GpuMem *> data_mems;
    //
    std::vector<GpuMem *> sc_rc_mems;
    //
    std::vector<IpcMem *> infos;
    //
    std::vector<std::vector<GpuPtr>> addr_table;
    //
    Request *request = nullptr;
    std::thread *request_loop_thread = nullptr;
    volatile bool run_request_loop_thread = false;
    //
    IpcSocket *ipc_socket = nullptr;
    //
    NetIbMgr *net_ib_mgr = nullptr;
    std::vector<NetIbMr *> sid_mrs;
    std::map<int, NetIbQp *> qps;
    std::vector<GpuSendRecvInfo> send_recv_infos;
    std::map<int, std::vector<NetIbMr::Info>> mris;
};

} // namespace ark

#endif // ARK_GPU_COMM_SW_H_

```

`ark/gpu/gpu_common.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_GPU_COMMON_H_
#define ARK_GPU_COMMON_H_

#include <cstdint>
#include <cuda.h>

#define ARK_GPU_NAME_PREFIX "gpu."
#define ARK_GPU_DATA_NAME ARK_GPU_NAME_PREFIX "data."
#define ARK_GPU_SC_RC_NAME ARK_GPU_NAME_PREFIX "sc_rc."
#define ARK_GPU_INFO_NAME ARK_GPU_NAME_PREFIX "info."

namespace ark {

// Constants.
enum
{
    REQUEST_INVALID = -1,
    MAX_NUM_SID = 256
};

//
union alignas(8) Request {
    uint64_t value = REQUEST_INVALID;
    struct
    {
        uint64_t req : 2;  // Request type
        uint64_t dst : 8;  // Dst segment ID
        uint64_t src : 8;  // Src segment ID
        uint64_t rank : 7; // Rank
        uint64_t len : 34; // Length
        uint64_t rsv : 5;  // Unused (reserved)
    } fields;
};

//
typedef CUresult GpuState;

} // namespace ark

#endif // ARK_GPU_COMM_COMMON_H_

```

`ark/gpu/gpu_compile.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <algorithm>
#include <cassert>
#include <cstdio>
#include <cstdlib>
#include <cstring>
#include <fstream>
#include <functional>
#include <sys/types.h>
#include <sys/wait.h>
#include <unistd.h>

#include "env.h"
#include "gpu/gpu_compile.h"
#include "gpu/gpu_logging.h"
#include "include/ark.h"
#include "threading.h"

#define ARK_USE_NVRTC 0
#define ARK_DEBUG_KERNEL 0

#if (ARK_USE_NVRTC)
#include <nvrtc.h>
#endif // (ARK_USE_NVRTC)

using namespace std;

// Generate a random alpha-numeric string.
static const string rand_anum(size_t len)
{
    auto randchar = []() -> char {
        const char charset[] = "0123456789"
                               "ABCDEFGHIJKLMNOPQRSTUVWXYZ"
                               "abcdefghijklmnopqrstuvwxyz";
        const size_t max_index = sizeof(charset) - 1;
        return charset[rand() % max_index];
    };
    string str(len, 0);
    generate_n(str.begin(), len, randchar);
    return str;
}

namespace ark {

#if (ARK_USE_NVRTC)
const string nvrtc_compile(const string &ark_root, const string &arch,
                           const string &code, unsigned int max_reg_cnt)
{
    nvrtcProgram prog;
    NVRTCLOG(nvrtcCreateProgram(&prog, code.c_str(), nullptr, 0, 0, 0));
    string opt_arch = "-arch=compute_" + arch;
    string opt_arch_def = "--define-macro=ARK_TARGET_CUDA_ARCH=" + arch;
    string opt_reg = "-maxrregcount=" + to_string(max_reg_cnt);
    string opt_inc_0 = "-I" + ark_root + "/include";
    string opt_inc_1 = "-I" + ark_root + "/include/kernels";
    string opt_inc_2 = "-I" + ark_root + "/include/kernels/nvrtc";
    const char *opts[] = {
        opt_arch.c_str(),
        "-std=c++17",
        "-default-device",
#if (ARK_DEBUG_KERNEL)
        "--device-debug",
        "--generate-line-info",
#endif // (ARK_DEBUG_KERNEL)
        opt_reg.c_str(),
        opt_arch_def.c_str(),
        opt_inc_0.c_str(),
        opt_inc_1.c_str(),
        opt_inc_2.c_str(),
        "-I/usr/local/cuda/include",
    };
    // Print compile options for debugging.
    stringstream ss;
    for (size_t i = 0; i < sizeof(opts) / sizeof(opts[0]); ++i) {
        ss << opts[i] << " ";
    }
    LOG(DEBUG, ss.str());
    // Compile.
    nvrtcResult compileResult =
        nvrtcCompileProgram(prog, sizeof(opts) / sizeof(opts[0]), opts);
    // Obtain compilation log from the program.
    size_t log_size;
    NVRTCLOG(nvrtcGetProgramLogSize(prog, &log_size));
    if (log_size > 1) {
        char *log = new char[log_size];
        NVRTCLOG(nvrtcGetProgramLog(prog, log));
        // LOGERR(endl, log, endl);
        LOG(DEBUG, endl, log, endl);
        delete[] log;
    }
    NVRTCLOG(compileResult);
    // Obtain PTX from the program.
    size_t ptx_size;
    NVRTCLOG(nvrtcGetPTXSize(prog, &ptx_size));
    char *ptx = new char[ptx_size];
    NVRTCLOG(nvrtcGetPTX(prog, ptx));
    NVRTCLOG(nvrtcDestroyProgram(&prog));
    // Write the result PTX file.
    return string(ptx);
}

const string link(const vector<string> &ptxs)
{
    unsigned int buflen = 8192;
    char *infobuf = new char[buflen];
    char *errbuf = new char[buflen];
    assert(infobuf != nullptr);
    assert(errbuf != nullptr);
    int enable = 1;
    int num_opts = 5;
    CUjit_option *opts = new CUjit_option[num_opts];
    void **optvals = new void *[num_opts];
    assert(opts != nullptr);
    assert(optvals != nullptr);

    opts[0] = CU_JIT_INFO_LOG_BUFFER;
    optvals[0] = (void *)infobuf;

    opts[1] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES;
    optvals[1] = (void *)(long)buflen;

    opts[2] = CU_JIT_ERROR_LOG_BUFFER;
    optvals[2] = (void *)errbuf;

    opts[3] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;
    optvals[3] = (void *)(long)buflen;

    opts[4] = CU_JIT_GENERATE_DEBUG_INFO;
    optvals[4] = (void *)(long)enable;

    CUlinkState lstate;
    CULOG(cuLinkCreate(num_opts, opts, optvals, &lstate));
    for (const auto &ptx : ptxs) {
        CULOG(cuLinkAddData(lstate, CU_JIT_INPUT_PTX, (void *)ptx.c_str(),
                            ptx.size() + 1, 0, 0, 0, 0));
    }
    char **cubin = nullptr;
    size_t cubin_size;
    CUresult res = cuLinkComplete(lstate, (void **)cubin, &cubin_size);
    if (res != CUDA_SUCCESS) {
        LOG(DEBUG, errbuf);
        CULOG(res);
    }
    assert(cubin != nullptr);
    string ret{*cubin};
    CULOG(cuLinkDestroy(lstate));
    delete[] infobuf;
    delete[] errbuf;
    return ret;
}

#endif // (ARK_USE_NVRTC)

const string gpu_compile(const vector<string> &codes,
                         const GpuArchType &arch_type, unsigned int max_reg_cnt,
                         bool use_comm_sw)
{
    const string &ark_root = get_env().path_root_dir;
    string arch;
    if (arch_type == GPU_ARCH_CUDA_60) {
        arch = "60";
    } else if (arch_type == GPU_ARCH_CUDA_70) {
        arch = "70";
    } else if (arch_type == GPU_ARCH_CUDA_75) {
        arch = "75";
    } else if (arch_type == GPU_ARCH_CUDA_80) {
        arch = "80";
    } else {
        arch = "";
    }

#if (ARK_USE_NVRTC)
    vector<string> ptxs;
    for (auto &code : codes) {
        ptxs.emplace_back(nvrtc_compile(ark_root, arch, code, max_reg_cnt));
    }
    // return link(ark_root, ptxs);
    return ptxs[0];
#else
    // assert(false);
    // return "";
    vector<pair<string, string>> items;
    items.reserve(codes.size());
    srand();
    for (auto &code : codes) {
        string rand_str;
        for (;;) {
            rand_str = rand_anum(16);
            bool retry = false;
            for (auto &p : items) {
                if (p.second == rand_str) {
                    retry = true;
                    break;
                }
            }
            if (!retry) {
                break;
            }
        }
        // TODO: retry if the file name already exists.
        items.emplace_back(code, "/tmp/ark_" + rand_str);
    }
    assert(items.size() == 1);
    para_exec<pair<string, string>>(
        items, 20,
        [&arch, &ark_root, max_reg_cnt,
         use_comm_sw](pair<string, string> &item) {
            string cu_file_path = item.second + ".cu";
            // Write CUDA code file.
            {
                ofstream cu_file(cu_file_path, ios::out | ios::trunc);
                cu_file << item.first;
            }
            // Compile command using NVCC.
            stringstream exec_cmd;
            exec_cmd << "/usr/local/cuda/bin/nvcc -cubin ";
#if (ARK_DEBUG_KERNEL)
            exec_cmd << "-G ";
#endif // (ARK_DEBUG_KERNEL)
            if (max_reg_cnt > 0) {
                exec_cmd << "-maxrregcount " << max_reg_cnt << " ";
            }
            // clang-format off
            exec_cmd << "-ccbin g++ -std c++17 -lcuda "
                "--define-macro=ARK_TARGET_CUDA_ARCH=" << arch << " "
                "--define-macro=ARK_COMM_SW=" << (int)use_comm_sw << " "
                "-I" << ark_root << "/include "
                "-I" << ark_root << "/include/kernels "
                "-gencode arch=compute_" << arch
                << ",code=sm_" << arch << " "
                "-o " << item.second << ".cubin "
                << cu_file_path << " 2>&1";
            // clang-format on
            LOG(INFO, "Compiling ", cu_file_path);
            LOG(DEBUG, exec_cmd.str());
            // Run the command.
            array<char, 4096> buffer;
            stringstream exec_print;
            unique_ptr<FILE, decltype(&pclose)> pipe(
                popen(exec_cmd.str().c_str(), "r"), pclose);
            if (!pipe) {
                LOGERR("popen() failed");
            }
            while (fgets(buffer.data(), buffer.size(), pipe.get()) != nullptr) {
                exec_print << buffer.data();
            }
            string exec_print_str = exec_print.str();
            if (exec_print_str.size() > 0) {
                LOGERR(endl, exec_print_str, endl);
            }
        });
    string cu_file_path = items[0].second + ".cu";
    string cubin_file_path = items[0].second + ".cubin";
    ifstream cubin_file(cubin_file_path);
    stringstream ss;
    ss << cubin_file.rdbuf();
    // remove(cu_file_path.c_str());
    remove(cubin_file_path.c_str());
    return ss.str();
#endif // (ARK_USE_NVRTC)
}

} // namespace ark
```

`ark/gpu/gpu_compile.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_GPU_COMPILE_H_
#define ARK_GPU_COMPILE_H_

#include <string>
#include <vector>

#include "gpu/gpu_mgr.h"

namespace ark {

const std::string gpu_compile(const std::vector<std::string> &codes,
                              const GpuArchType &arch, unsigned int max_reg_cnt,
                              bool use_comm_sw);

} // namespace ark

#endif // ARK_GPU_COMPILE_H_

```

`ark/gpu/gpu_kernel.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <cassert>
#include <cstdlib>
#include <cstring>
#include <fstream>
#include <sstream>

#include "cpu_timer.h"
#include "env.h"
#include "gpu/gpu_compile.h"
#include "gpu/gpu_kernel.h"
#include "gpu/gpu_logging.h"

using namespace std;

// This should be a power of 2.
#define CLKS_CNT 1048576
//
#define MAX_LOOP_COUNTER 10000000

namespace ark {

//
GpuKernel::GpuKernel(const string &name_, const vector<string> &codes_,
                     const array<unsigned int, 3> &grid_dims,
                     const array<unsigned int, 3> &block_dims,
                     unsigned int smem_bytes_,
                     initializer_list<GpuBuf *> buf_args_,
                     initializer_list<size_t> buf_offs_,
                     initializer_list<pair<void *, size_t>> args,
                     const string &cubin_)
    : name{name_}, codes{codes_}, gd{grid_dims}, bd{block_dims},
      smem_bytes{smem_bytes_}, buf_args{buf_args_}, buf_offs{buf_offs_},
      ptr_args(buf_args.size(), 0), num_params{(int)(buf_args.size() +
                                                     args.size())},
      params{new void *[num_params]}, cubin{cubin_}
{
    if (this->name.size() == 0) {
        LOGERR("Invalid kernel name: ", this->name);
    }
    assert(this->params != nullptr);
    // Default offset zero.
    if (this->buf_offs.size() == 0) {
        this->buf_offs.resize(buf_args.size(), 0);
    }
    int idx = buf_args.size();
    for (auto &pa : args) {
        void *p = malloc(pa.second);
        assert(p != nullptr);
        if (pa.first != nullptr) {
            ::memcpy(p, pa.first, pa.second);
        }
        this->params[idx++] = p;
    }
}

//
GpuKernel::~GpuKernel()
{
    if (this->params != nullptr) {
        for (int i = buf_args.size(); i < this->num_params; ++i) {
            if (this->params[i] != nullptr) {
                free(this->params[i]);
            }
        }
        delete this->params;
        this->params = nullptr;
    }
}

//
void GpuKernel::compile(const GpuInfo &gpu_info, bool use_comm_sw)
{
    if (this->is_compiled()) {
        return;
    }
    unsigned int max_reg_cnt = gpu_info.max_registers_per_block /
                               (this->bd[0] * this->bd[1] * this->bd[2]);
    if (max_reg_cnt >= gpu_info.max_registers_per_thread) {
        max_reg_cnt = gpu_info.max_registers_per_thread - 1;
    }
    //
    if (this->cubin.empty()) {
        this->cubin =
            gpu_compile(this->codes, gpu_info.arch, max_reg_cnt, use_comm_sw);
    }
}

//
void GpuKernel::load()
{
    //
    unsigned int buflen = 8192;
    char *infobuf = new char[buflen];
    char *errbuf = new char[buflen];
    assert(infobuf != nullptr);
    assert(errbuf != nullptr);
    int enable = 1;
    int num_opts = 5;
    CUjit_option *opts = new CUjit_option[num_opts];
    void **optvals = new void *[num_opts];
    assert(opts != nullptr);
    assert(optvals != nullptr);

    opts[0] = CU_JIT_INFO_LOG_BUFFER;
    optvals[0] = (void *)infobuf;

    opts[1] = CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES;
    optvals[1] = (void *)(long)buflen;

    opts[2] = CU_JIT_ERROR_LOG_BUFFER;
    optvals[2] = (void *)errbuf;

    opts[3] = CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES;
    optvals[3] = (void *)(long)buflen;

    opts[4] = CU_JIT_GENERATE_DEBUG_INFO;
    optvals[4] = (void *)(long)enable;

    if (cuModuleLoadDataEx(&this->module, this->cubin.c_str(), num_opts, opts,
                           optvals) != CUDA_SUCCESS) {
        LOG(DEBUG, infobuf);
        LOGERR("cuModuleLoadDataEx() failed: ", errbuf);
    }
    delete[] infobuf;
    delete[] errbuf;
    CULOG(cuModuleGetFunction(&this->kernel, this->module, this->name.c_str()));
    //
    int static_smem_size_bytes;
    CULOG(cuFuncGetAttribute(&static_smem_size_bytes,
                             CU_FUNC_ATTRIBUTE_SHARED_SIZE_BYTES,
                             this->kernel));
    int dynamic_smem_size_bytes = smem_bytes - static_smem_size_bytes;
    CULOG(cuFuncSetAttribute(this->kernel,
                             CU_FUNC_ATTRIBUTE_MAX_DYNAMIC_SHARED_SIZE_BYTES,
                             dynamic_smem_size_bytes));
    // Now code string is not needed.
    // this->code.clear();
}

//
GpuState GpuKernel::launch(GpuStream stream)
{
    if (!this->is_compiled()) {
        LOGERR("Kernel is not compiled yet.");
    }
    auto it_ptr = this->ptr_args.begin();
    auto it_off = this->buf_offs.begin();
    for (GpuBuf *buf : this->buf_args) {
        *it_ptr++ = buf->ref(*it_off++);
    }
    void **params = this->params;
    for (size_t i = 0; i < this->ptr_args.size(); ++i) {
        *params++ = &this->ptr_args[i];
    }
    return cuLaunchKernel(this->kernel, this->gd[0], this->gd[1], this->gd[2],
                          this->bd[0], this->bd[1], this->bd[2],
                          this->smem_bytes, stream, this->params, 0);
}

int GpuKernel::get_function_attribute(CUfunction_attribute attr) const
{
    if (this->kernel == nullptr) {
        LOGERR("Kernel is not compiled yet.");
    }
    int ret;
    CULOG(cuFuncGetAttribute(&ret, attr, this->kernel));
    return ret;
}

////////////////////////////////////////////////////////////////////////////////

GpuLoopKernel::GpuLoopKernel(const string &name_,
                             const vector<string> &codes_body,
                             unsigned int num_sm, unsigned int num_warp,
                             unsigned int smem_bytes, const string &cubin_,
                             GpuMgrCtx *ctx_)
    : GpuKernel{name_,
                {},
                {num_sm, 1, 1},
                {num_warp * 32, 1, 1},
                (smem_bytes < 4) ? 4 : smem_bytes,
                {},
                {},
                {{0, sizeof(GpuPtr)}, {0, sizeof(GpuPtr)}},
                cubin_},
      ctx{ctx_}, timer_begin{ctx_->create_event(false, nullptr)},
      timer_end{ctx_->create_event(false, nullptr)}
{
    ctx_->set_current();
    this->flag = make_unique<GpuMem>("", sizeof(int), true);
    this->clocks =
        make_unique<GpuMem>("", CLKS_CNT * sizeof(long long int), true);
    this->flag_href = (volatile int *)this->flag->href(0);

    *(GpuPtr *)this->params[0] = this->flag->ref(0);
    std::memset(this->clocks->href(), 0, this->clocks->get_bytes());

    if (codes_body.size() > 0) {
        const string *ark_loop_body_code = nullptr;
        for (auto &code : codes_body) {
            if (code.find("ark_loop_body") == string::npos) {
                //
                // this->codes.emplace_back(body_prefix.str() + code);
            } else {
                ark_loop_body_code = &code;
            }
        }
        assert(ark_loop_body_code != nullptr);

        stringstream ss;
        // clang-format off
        ss <<
        "// THIS KERNEL IS MACHINE-GENERATED BY ARK.\n"
        "#define ARK_THREADS_PER_BLOCK " << num_warp * 32 << "\n"
        "#define ARK_KERNELS_SYNC_CLKS_CNT " << CLKS_CNT << "\n"
        "__device__ volatile unsigned long long int *" ARK_REQ_NAME ";\n"
        "__device__ volatile unsigned           int *_ARK_SC;\n"
        "__device__ volatile unsigned           int *_ARK_RC;\n"
        "__device__ long long int *_ARK_CLKS;\n"
        "__device__ int _ITER = 0;\n"
        "#include \"ark_kernels.h\"\n"
        "__device__ ark::sync::State " ARK_LSS_NAME ";\n"
        "__device__ char *" ARK_BUF_NAME ";\n"
        << *ark_loop_body_code <<
        "extern \"C\" __global__ __launch_bounds__(" << num_warp * 32 << ", 1)\n"
        "void " << name_ << "(volatile int *_it)\n"
        "{\n"
        "  for (;;) {\n"
        "    if (threadIdx.x == 0 && blockIdx.x == 0) {\n"
        "      int iter;\n"
        "      while ((iter = *_it) == 0) {}\n"
        "      _ITER = iter;\n"
        "    }\n"
        "    ark::sync_gpu<" << num_sm << ">(" ARK_LSS_NAME ");\n"
        "    if (_ITER < 0) {\n"
        "      return;\n"
        "    }\n"
        "    for (int _i = 0; _i < _ITER; ++_i) {\n"
        "      ark_loop_body(_i);\n"
        "      ark::sync_gpu<" << num_sm << ">(" ARK_LSS_NAME ");\n"
        "    }\n"
        "    if (threadIdx.x == 0 && blockIdx.x == 0) {\n"
        "      *_it = 0;\n"
        "    }\n"
        "  }\n"
        "}\n";
        // clang-format on
        this->codes.emplace_back(ss.str());
    }
}

void GpuLoopKernel::compile(const GpuInfo &gpu_info)
{
    this->ctx->set_current();
    if (this->is_compiled()) {
        return;
    }
    // Compile the code.
    GpuKernel::compile(gpu_info, this->ctx->is_comm_sw());
}

void GpuLoopKernel::load()
{
    this->ctx->set_current();
    GpuKernel::load();
    //
    if (!this->is_compiled()) {
        LOGERR("Need to compile first before initialization.");
    }
    if (this->stream != nullptr) {
        // Wait until previous works finish.
        this->wait();
    } else {
        // Initialize global variables in the loop kernel.
        GpuPtr buf_ptr_val = this->ctx->get_data_ref();
        GpuPtr lss_ptr_addr;
        GpuPtr buf_ptr_addr;
        CULOG(cuModuleGetGlobal(&lss_ptr_addr, 0, this->module, ARK_LSS_NAME));
        CULOG(cuModuleGetGlobal(&buf_ptr_addr, 0, this->module, ARK_BUF_NAME));
        CULOG(cuMemsetD32(lss_ptr_addr, 0, 4));
        CULOG(cuMemcpyHtoD(buf_ptr_addr, &buf_ptr_val, sizeof(GpuPtr)));
        //
        GpuPtr sc_ptr_val = this->ctx->get_sc_ref(0);
        GpuPtr rc_ptr_val = this->ctx->get_rc_ref(0);
        GpuPtr sc_ptr_addr;
        GpuPtr rc_ptr_addr;
        CULOG(cuModuleGetGlobal(&sc_ptr_addr, 0, this->module, ARK_SC_NAME));
        CULOG(cuModuleGetGlobal(&rc_ptr_addr, 0, this->module, ARK_RC_NAME));
        CULOG(cuMemcpyHtoD(sc_ptr_addr, &sc_ptr_val, sizeof(GpuPtr)));
        CULOG(cuMemcpyHtoD(rc_ptr_addr, &rc_ptr_val, sizeof(GpuPtr)));
        //
        GpuPtr db_ptr_val = this->ctx->get_request_ref();
        GpuPtr db_ptr_addr;
        CULOG(cuModuleGetGlobal(&db_ptr_addr, 0, this->module, ARK_REQ_NAME));
        CULOG(cuMemcpyHtoD(db_ptr_addr, &db_ptr_val, sizeof(GpuPtr)));
        //
        GpuPtr clks_ptr_val = this->clocks->ref();
        GpuPtr clks_ptr_addr;
        CULOG(
            cuModuleGetGlobal(&clks_ptr_addr, 0, this->module, ARK_CLKS_NAME));
        CULOG(cuMemcpyHtoD(clks_ptr_addr, &clks_ptr_val, sizeof(GpuPtr)));
        // set the data buffer pointers of remote gpus
        if (this->ctx->is_comm_sw()) {
            int nrph = get_env().num_ranks_per_host;
            int nodes_id = this->ctx->get_gpu_id() / nrph;
            // only set the GPU remote data buf pointers of the GPUs on the same
            // node
            for (int i = nodes_id * nrph;
                 i < (nodes_id + 1) * nrph && i < this->ctx->get_world_size();
                 i++) {
                GpuPtr data_buf_value = this->ctx->get_data_ref(i);
                if (data_buf_value == 0) {
                    continue;
                }
                GpuPtr data_buf_ptr;
                string data_buf_name = ARK_BUF_NAME + std::to_string(i);
                CUresult _e = cuModuleGetGlobal(&data_buf_ptr, 0, this->module,
                                                data_buf_name.c_str());
                // in some test code the symbol _ARK_BUF_0 is not defined
                if (_e == CUDA_ERROR_NOT_FOUND) {
                    LOG(DEBUG, "global variable ", data_buf_name, " not found");
                    continue;
                }
                // CULOG(_e);
                LOG(DEBUG, data_buf_name, " data_buf_ptr=", std::hex,
                    data_buf_ptr, " data_buf_value=", data_buf_value);
                CULOG(cuMemcpyHtoD(data_buf_ptr, &data_buf_value,
                                   sizeof(GpuPtr)));
            }
        }
    }
}

GpuState GpuLoopKernel::launch(CUstream stream, bool disable_timing)
{
    this->elapsed_msec = -1;
    if (!this->is_compiled()) {
        LOGERR("Need to compile first before initialization.");
    } else if (stream == nullptr) {
        LOGERR("Given an invalid stream.");
    } else if (this->stream != nullptr) {
        if (this->stream == stream) {
            LOG(WARN, "Ignore launching twice.");
            return CUDA_SUCCESS;
        } else {
            LOGERR("This loop kernel is already running.");
        }
    }
    if (!disable_timing) {
        CULOG(cuEventRecord(this->timer_begin, stream));
    }
    // Initialize loop flags.
    *(this->flag_href) = 0;
    GpuState res = GpuKernel::launch(stream);
    if (res == CUDA_SUCCESS) {
        this->stream = stream;
        if (!disable_timing) {
            CULOG(cuEventRecord(this->timer_end, stream));
            this->is_recording = true;
        }
    }
    return res;
}

void GpuLoopKernel::run(int iter)
{
    if (iter > 0) {
#if 0
        int idx = this->flip_flag ? 0 : 1;
        int rem = iter;
        while (rem--) {
            while (this->get_flag(idx) > 0) {
                cpu_ntimer_sleep(500);
            }
            this->set_flag(idx, 1);
            idx ^= 1;
        }
        if (iter & 1) {
            this->flip_flag = !(this->flip_flag);
        }
#else
        volatile int *href = this->flag_href;
        while (*href > 0) {
        }
        *href = iter;
#endif
    }
}

bool GpuLoopKernel::poll()
{
    return *(this->flag_href) <= 0;
}

void GpuLoopKernel::wait()
{
    volatile int *href = this->flag_href;
    int cnt = MAX_LOOP_COUNTER;
    while (*href > 0) {
        if (--cnt > 0) {
            continue;
        }
        // Check if the kernel encountered an error.
        CUresult res = cuStreamQuery(this->stream);
        if (res == CUDA_SUCCESS) {
            if (*href > 0) {
                LOG(WARN, "Stream is finished but the loop flag is still set.");
                break;
            } else {
                LOG(WARN, "wait() is delayed by a stream query. Regarding "
                          "timing measurements may be inaccurate.");
                break;
            }
        } else if (res == CUDA_ERROR_NOT_READY) {
            cnt = MAX_LOOP_COUNTER;
        } else {
            CULOG(res);
        }
    }
}

void GpuLoopKernel::stop()
{
    this->wait();
    *(this->flag_href) = -1;
    CULOG(cuStreamSynchronize(this->stream));
    if (is_recording) {
        CULOG(cuEventElapsedTime(&(this->elapsed_msec), this->timer_begin,
                                 this->timer_end));
        this->is_recording = false;
    }
    this->stream = nullptr;
}

} // namespace ark

```

`ark/gpu/gpu_kernel.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_GPU_KERNEL_H_
#define ARK_GPU_KERNEL_H_

#include <cuda.h>
#include <string>
#include <thread>
#include <vector>

#include "gpu/gpu_mgr.h"

#define ARK_BUF_NAME "_ARK_BUF"
#define ARK_SC_NAME "_ARK_SC"
#define ARK_RC_NAME "_ARK_RC"
#define ARK_LSS_NAME "_ARK_LOOP_SYNC_STATE"
#define ARK_REQ_NAME "_ARK_REQUEST"
#define ARK_CLKS_NAME "_ARK_CLKS"

namespace ark {

class GpuKernel
{
  public:
    GpuKernel(const std::string &name, const std::vector<std::string> &codes,
              const std::array<unsigned int, 3> &grid_dims,
              const std::array<unsigned int, 3> &block_dims,
              unsigned int smem_bytes, std::initializer_list<GpuBuf *> buf_args,
              std::initializer_list<size_t> buf_offs,
              std::initializer_list<std::pair<void *, size_t>> args,
              const std::string &cubin);
    ~GpuKernel();

    void compile(const GpuInfo &gpu_info, bool use_comm_sw = true);
    void load();
    GpuState launch(GpuStream stream);

    const std::string &get_name()
    {
        return name;
    }
    const std::vector<std::string> &get_codes()
    {
        return codes;
    }
    const std::string &get_cubin()
    {
        return cubin;
    }
    int get_function_attribute(CUfunction_attribute attr) const;
    bool is_compiled() const
    {
        return this->kernel != nullptr;
    }

  protected:
    const std::string name;
    std::vector<std::string> codes;
    std::array<unsigned int, 3> const gd;
    std::array<unsigned int, 3> const bd;
    unsigned int smem_bytes;
    // Input data buffers of this kernel.
    std::vector<GpuBuf *> buf_args;
    //
    std::vector<size_t> buf_offs;
    // Pointers to an entry of each buffer in `buf_args`.
    std::vector<GpuPtr> ptr_args;
    int num_params;
    void **params;
    std::string cubin;

    CUmodule module;
    CUfunction kernel = nullptr;
};

class GpuLoopKernel : public GpuKernel
{
  public:
    GpuLoopKernel(const std::string &name,
                  const std::vector<std::string> &codes_body,
                  unsigned int num_sm, unsigned int num_warp,
                  unsigned int smem_bytes, const std::string &cubin,
                  GpuMgrCtx *ctx);

    void compile(const GpuInfo &gpu_info);
    GpuState launch(CUstream stream, bool disable_timing = true);
    void load();
    void run(int iter = 1);
    bool poll();
    void wait();
    void stop();

    void set_buf(GpuPtr buf_ptr);
    const float &get_elapsed_msec() const
    {
        return elapsed_msec;
    }
    const long long int *get_clocks() const
    {
        return (const long long int *)clocks->href();
    }

  private:
    GpuMgrCtx *ctx;
    GpuEvent timer_begin;
    GpuEvent timer_end;

    std::unique_ptr<GpuMem> flag;
    std::unique_ptr<GpuMem> clocks;

    volatile int *flag_href;

    GpuStream stream = nullptr;
    bool is_recording = false;
    float elapsed_msec = -1;
};

} // namespace ark

#endif // ARK_GPU_KERNEL_H_

```

`ark/gpu/gpu_kernel_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_kernel.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "unittest/unittest_utils.h"

using namespace std;
using namespace ark;

//
const string test_kernel_loop_void =
    "__device__ void ark_loop_body(int _iter) {\n"
    "  // Do nothing. Print iteration counter.\n"
    "  if (threadIdx.x == 0 && blockIdx.x == 0)\n"
    "    if (_iter % 50 == 49) {\n"
    "      printf(\".\\n\");\n"
    "    } else {\n"
    "      printf(\".\");\n"
    "    }\n"
    "}\n";

//
unittest::State test_gpu_kernel_loop_void()
{
    int pid = ark::utils::proc_spawn([] {
        GpuMgr *mgr = get_gpu_mgr(0);
        GpuMgrCtx *ctx = mgr->create_context("test_loop_void", 0, 1);
        ctx->freeze();

        GpuLoopKernel glk{"test_kernel_loop_void",
                          {test_kernel_loop_void},
                          (unsigned int)mgr->get_gpu_info().num_sm,
                          1,
                          0,
                          "",
                          ctx};
        glk.compile(mgr->get_gpu_info());
        glk.load();

        GpuState ret = glk.launch(ctx->create_stream());
        UNITTEST_EQ(ret, 0);
        glk.run(100);
        glk.stop();

        return 0;
    });
    UNITTEST_NE(pid, -1);
    int ret = ark::utils::proc_wait(pid);
    UNITTEST_EQ(ret, 0);
    return unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_gpu_kernel_loop_void);
    return 0;
}

```

`ark/gpu/gpu_logging.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_GPU_LOGGING_H_
#define ARK_GPU_LOGGING_H_

#include <cuda.h>
#include <sstream>

#include "logging.h"

#define CULOG(cmd)                                                             \
    do {                                                                       \
        CUresult _e = cmd;                                                     \
        if (_e != CUDA_SUCCESS) {                                              \
            const char *_estr;                                                 \
            cuGetErrorString(_e, &_estr);                                      \
            LOGERR("CUDA error ", _e, " '", _estr, "'");                       \
        }                                                                      \
    } while (0)

#define NVMLLOG(cmd)                                                           \
    do {                                                                       \
        nvmlReturn_t _e = cmd;                                                 \
        if (_e != NVML_SUCCESS) {                                              \
            LOGERR("NVML error ", _e, " '", nvmlErrorString(_e), "'");         \
        }                                                                      \
    } while (0)

#define NVRTCLOG(cmd)                                                          \
    do {                                                                       \
        nvrtcResult _e = cmd;                                                  \
        if (_e != NVRTC_SUCCESS) {                                             \
            LOGERR("NVRTC error ", _e, " '", nvrtcGetErrorString(_e), "'");    \
        }                                                                      \
    } while (0)

#endif // ARK_GPU_LOGGING_H_
```

`ark/gpu/gpu_mem.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <cassert>
#include <fcntl.h>
#include <stdint.h>
#include <string.h>
#include <sys/mman.h>
#include <unistd.h>

#include "gpumemioctl.h"
#define GPUMEM_DRIVER_PATH "/dev/" GPUMEM_DRIVER_NAME

#include "gpu/gpu_logging.h"
#include "gpu/gpu_mem.h"

using namespace std;

namespace ark {

// Expose GPU memory space into CPU memory.
static int mem_expose(GpuMemExposalInfo *info, GpuPtr addr, uint64_t bytes)
{
    int flag = 1;
    CULOG(cuPointerSetAttribute(&flag, CU_POINTER_ATTRIBUTE_SYNC_MEMOPS, addr));
    // Convert virtual into physical address.
    int fd = open(GPUMEM_DRIVER_PATH, O_RDWR, 0);
    if (fd < 0) {
        return errno;
    }
    gpudma_lock_t lock;
    lock.handle = 0;
    lock.addr = addr;
    lock.size = bytes;
    if (ioctl(fd, IOCTL_GPUMEM_LOCK, &lock) < 0) {
        return errno;
    }
    uint64_t npage = bytes >> 16;
    assert(npage == lock.page_count);
    int state_bytes = sizeof(gpudma_state_t) + npage * sizeof(uint64_t);
    gpudma_state_t *state = (gpudma_state_t *)malloc(state_bytes);
    if (state == 0) {
        return errno;
    }
    memset(state, 0, state_bytes);
    state->handle = lock.handle;
    state->page_count = npage;
    if (ioctl(fd, IOCTL_GPUMEM_STATE, state) < 0) {
        return errno;
    }
    // Set the physical address.
    info->phys = state->pages[0];
    info->npage = npage;
    free(state);
    // Create mmap of all pages.
    info->mmap =
        mmap(0, bytes, PROT_READ | PROT_WRITE, MAP_SHARED, fd, info->phys);
    if (info->mmap == MAP_FAILED) {
        return errno;
    }
#ifdef DEBUG_ARK_GPU_MEM
    // Test mapping.
    int *tmp0 = (int *)info->mmap;
    *tmp0 = 77;
    int tmp1;
    CULOG(cuMemcpyDtoH(&tmp1, addr, 4));
    if (tmp1 != 77) {
        LOGERR("mmap test failed: GPU reads ", tmp1, ", expected 77");
    }
    CULOG(cuMemsetD32(addr, 55, 1));
    if (*tmp0 != 55) {
        LOGERR("mmap test failed: CPU reads ", *tmp0, ", expected 55");
    }
    // Reset the tested address.
    *tmp0 = 0;
#endif // DEBUG_ARK_GPU_MEM
    close(fd);
    return 0;
}

//
static void *map_pa_to_va(uint64_t pa, uint64_t bytes)
{
    int fd = open(GPUMEM_DRIVER_PATH, O_RDWR, 0);
    if (fd < 0) {
        LOGERR("open: ", strerror(errno), " (", errno, ")");
    }
    void *map = mmap(0, bytes, PROT_READ | PROT_WRITE, MAP_SHARED, fd, pa);
    if (map == MAP_FAILED) {
        LOGERR("mmap: ", strerror(errno), " (", errno, ")");
        close(fd);
    }
    close(fd);
    return map;
}

//
GpuMem::GpuMem(const string &name, size_t bytes, bool create, bool try_create)
{
    if (name.size() > 0) {
        this->shm = make_unique<IpcMem>(name, create, try_create);
    } else {
        this->shm.reset();
    }
    if (bytes > 0) {
        this->alloc(bytes);
    }
}

// Destructor.
GpuMem::~GpuMem()
{
    if (!this->shm.get()) {
        return;
    } else if (this->shm->is_create()) {
        if (this->addr != 0) {
            cuMemFree(this->raw_addr);
            this->addr = 0;
            this->raw_addr = 0;
        }
        if (this->exp_info.mmap != 0) {
            munmap(this->exp_info.mmap, this->exp_info.npage << 16);
            this->exp_info.mmap = 0;
            this->exp_info.npage = 0;
            this->exp_info.phys = 0;
        }
    } else {
        if (this->addr != 0) {
            cuIpcCloseMemHandle(this->addr);
        }
    }
}

//
void GpuMem::alloc(size_t bytes)
{
    // Align the bytes by 64KB.
    this->bytes = ((bytes + 65535) >> 16) << 16;
    if (this->bytes == 0) {
        LOGERR("Tried to allocate zero byte.");
    }
    if (this->shm.get() && !this->shm->is_create()) {
        GpuMemInfo *info = (GpuMemInfo *)this->shm->alloc(sizeof(GpuMemInfo));
        IpcLockGuard lg{this->shm->get_lock()};
        CUresult res = cuIpcOpenMemHandle(&this->addr, info->ipc_hdl,
                                          CU_IPC_MEM_LAZY_ENABLE_PEER_ACCESS);
        if (res == CUDA_ERROR_PEER_ACCESS_UNSUPPORTED) {
            // this->addr = 0;
            LOGERR("not implemented yet.");
        } else if (res != CUDA_SUCCESS) {
            // Unexpected error.
            CULOG(res);
        }
        this->exp_info.mmap = map_pa_to_va(info->phys_addr, this->bytes);
        if (this->exp_info.mmap == nullptr) {
            LOGERR("map_pa_to_va failed");
        }
        LOG(DEBUG, "Imported GpuMem addr ", hex, this->addr, " map ",
            this->exp_info.mmap, dec, " bytes ", this->bytes);
    } else {
        CULOG(cuMemAlloc(&this->raw_addr, this->bytes + 65536));
        this->addr = (CUdeviceptr)(((uint64_t)this->raw_addr + 65535) & ~65535);
        LOG(DEBUG, "request bytes ", this->bytes, " addr ", hex, this->addr);
        int state = mem_expose(&this->exp_info, this->addr, this->bytes);
        if (state != 0) {
            LOGERR("mem_expose() failed with errno ", state);
        }
        if (this->shm.get()) {
            IpcLockGuard lg{this->shm->get_lock()};
            GpuMemInfo *info =
                (GpuMemInfo *)this->shm->alloc(sizeof(GpuMemInfo));
            CULOG(cuIpcGetMemHandle(&info->ipc_hdl, this->addr));
            info->phys_addr = this->exp_info.phys;
        }
        LOG(DEBUG, "Created GpuMem addr ", hex, this->addr, " map ",
            this->exp_info.mmap, dec, " bytes ", this->bytes);
    }
}

// GPU-side virtual address.
GpuPtr GpuMem::ref(size_t offset) const
{
    if (this->addr == 0) {
        return 0;
    }
    return this->addr + offset;
}

// GPU-side physical address.
uint64_t GpuMem::pref(size_t offset) const
{
    if (this->exp_info.phys == 0) {
        return 0;
    }
    return this->exp_info.phys + offset;
}

// Host-side mapped address.
void *GpuMem::href(size_t offset) const
{
    if (this->exp_info.mmap == 0) {
        return nullptr;
    }
    return (void *)((char *)this->exp_info.mmap + offset);
}

} // namespace ark

```

`ark/gpu/gpu_mem.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_GPU_MEM_H_
#define ARK_GPU_MEM_H_

#include "ipc/ipc_mem.h"
#include <cuda.h>
#include <memory>

namespace ark {

typedef CUdeviceptr GpuPtr;

struct GpuMemInfo
{
    CUipcMemHandle ipc_hdl;
    uint64_t phys_addr;
    uint64_t bytes;
};

// Information on GPU memory exposal to CPU.
struct GpuMemExposalInfo
{
    // Physical address of GPU pointer.
    uint64_t phys = 0;
    // Number of mmapped 64KB pages.
    uint64_t npage = 0;
    // Base address of mmaped pages.
    void *mmap = 0;
};

class GpuMem
{
  public:
    GpuMem(const std::string &name, size_t bytes, bool create,
           bool try_create = false);
    ~GpuMem();

    // Allocate a GPU memory chunk.
    void alloc(size_t bytes);
    // GPU-side virtual address.
    GpuPtr ref(size_t offset = 0) const;
    // GPU-side physical address.
    uint64_t pref(size_t offset = 0) const;
    // Host-side mapped address.
    void *href(size_t offset = 0) const;

    // Return allocated number of bytes.
    const uint64_t &get_bytes() const
    {
        return bytes;
    }

  private:
    //
    std::unique_ptr<IpcMem> shm;
    //
    GpuPtr addr = 0;
    //
    GpuPtr raw_addr = 0;
    //
    uint64_t bytes = 0;
    //
    GpuMemExposalInfo exp_info;
};

} // namespace ark

#endif // ARK_GPU_MEM_H_

```

`ark/gpu/gpu_mem_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_logging.h"
#include "gpu/gpu_mem.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "unittest/unittest_utils.h"

using namespace ark;
using namespace std;

unittest::State test_gpu_mem_no_ipc()
{
    int pid = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{5};

        // Create a CUDA context of GPU 0.
        CULOG(cuInit(0));
        CUdevice dev0;
        CUcontext ctx0;
        CULOG(cuDeviceGet(&dev0, 0));
        CULOG(cuCtxCreate(&ctx0, 0, dev0));
        CULOG(cuCtxSetCurrent(ctx0));

        // Local memory in GPU 0.
        GpuMem mem0{"gpu_mem_0", 4096, true};

        // Create a CUDA context of GPU 1.
        CUdevice dev1;
        CUcontext ctx1;
        CULOG(cuDeviceGet(&dev1, 1));
        CULOG(cuCtxCreate(&ctx1, 0, dev1));
        CULOG(cuCtxSetCurrent(ctx1));

        // Remote memory in GPU 1.
        GpuMem mem1{"gpu_mem_1", 4096, true};

        // Set data on GPU 0.
        CULOG(cuCtxSetCurrent(ctx0));
        CULOG(cuMemsetD32(mem0.ref(), 7, 1024));
        // Check data on GPU 0.
        volatile int *href0 = (volatile int *)mem0.href();
        for (int i = 0; i < 1024; ++i) {
            UNITTEST_EQ(href0[i], 7);
        }

        // Set data on GPU 1.
        CULOG(cuCtxSetCurrent(ctx1));
        CULOG(cuMemsetD32(mem1.ref(), 9, 1024));
        // Check data on GPU 1.
        volatile int *href1 = (volatile int *)mem1.href();
        for (int i = 0; i < 1024; ++i) {
            UNITTEST_EQ(href1[i], 9);
        }

        return 0;
    });
    UNITTEST_NE(pid, -1);

    int ret = ark::utils::proc_wait(pid);
    UNITTEST_EQ(ret, 0);
    return unittest::SUCCESS;
}

unittest::State test_gpu_mem_ipc()
{
    int pid0 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{5};

        // Create a CUDA context of GPU 0.
        CULOG(cuInit(0));
        CUdevice dev;
        CUcontext ctx;
        CULOG(cuDeviceGet(&dev, 0));
        CULOG(cuCtxCreate(&ctx, 0, dev));
        CULOG(cuCtxSetCurrent(ctx));

        // Local memory in GPU 0.
        GpuMem mem0{"gpu_mem_0", 4096, true};
        // Remote memory in GPU 1.
        GpuMem mem1{"gpu_mem_1", 4096, false};

        // Wait until another process writes data on the local GPU 0.
        volatile int *href = (volatile int *)mem0.href();
        for (int i = 0; i < 1024; ++i) {
            while (href[i] != 7) {
            }
        }

        // Set data on the remote GPU 1.
        CULOG(cuMemsetD32(mem1.ref(), 9, 1024));

        return 0;
    });
    UNITTEST_NE(pid0, -1);

    int pid1 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{5};

        // Create a CUDA context of GPU 1.
        CULOG(cuInit(0));
        CUdevice dev;
        CUcontext ctx;
        CULOG(cuDeviceGet(&dev, 1));
        CULOG(cuCtxCreate(&ctx, 0, dev));
        CULOG(cuCtxSetCurrent(ctx));

        // Remote memory in GPU 0.
        GpuMem mem0{"gpu_mem_0", 4096, false};
        // Local memory in GPU 1.
        GpuMem mem1{"gpu_mem_1", 4096, true};

        // Set data on the remote GPU 0.
        CULOG(cuMemsetD32(mem0.ref(), 7, 1024));

        // Wait until another process writes data on the local GPU 1.
        volatile int *href = (volatile int *)mem1.href();
        for (int i = 0; i < 1024; ++i) {
            while (href[i] != 9) {
            }
        }

        return 0;
    });
    UNITTEST_NE(pid1, -1);

    int ret = ark::utils::proc_wait({pid0, pid1});
    UNITTEST_EQ(ret, 0);
    return unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_gpu_mem_no_ipc);
    UNITTEST(test_gpu_mem_ipc);
    return 0;
}

```

`ark/gpu/gpu_mgr.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <cassert>
#include <cstring>
#include <fcntl.h>
#include <fstream>
#include <iomanip>
#include <memory>
#include <sys/mman.h>
#include <unistd.h>

#include <nvml.h>

#include "env.h"
#include "gpu/gpu_logging.h"
#include "gpu/gpu_mgr.h"
#include "include/ark.h"
#include "math.h"

using namespace std;

namespace ark {

// Initialize APIs.
static void gpu_init()
{
    // Initialize CUDA driver APIs.
    CULOG(cuInit(0));
    // Initialize NVML APIs.
    NVMLLOG(nvmlInit());
}

// Return the number of GPUs in the system.
static int gpu_num()
{
    int n;
    CULOG(cuDeviceGetCount(&n));
    return n;
}

//
void GpuInfo::init(const int gpu_id)
{
    CUdevice dev;
    CULOG(cuDeviceGet(&dev, gpu_id));
    //
    size_t gmem_free;
    CULOG(cuMemGetInfo(&gmem_free, &(this->gmem_total)));
    //
    CULOG(cuDeviceGetAttribute(
        &(this->cc_major), CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, dev));
    CULOG(cuDeviceGetAttribute(
        &(this->cc_minor), CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, dev));
    CULOG(cuDeviceGetAttribute(&(this->num_sm),
                               CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT, dev));
    CULOG(cuDeviceGetAttribute(
        &(this->smem_total),
        CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR, dev));
    CULOG(cuDeviceGetAttribute(
        &(this->smem_block_total),
        CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN, dev));
    CULOG(cuDeviceGetAttribute(&(this->clk_rate),
                               CU_DEVICE_ATTRIBUTE_CLOCK_RATE, dev));

    this->arch_str = to_string(this->cc_major * 10 + this->cc_minor);
    if (this->arch_str == "60") {
        this->arch = GPU_ARCH_CUDA_60;
    } else if (this->arch_str == "70") {
        this->arch = GPU_ARCH_CUDA_70;
    } else if (this->arch_str == "75") {
        this->arch = GPU_ARCH_CUDA_75;
    } else if (this->arch_str == "80") {
        this->arch = GPU_ARCH_CUDA_80;
    } else {
        this->arch = GPU_ARCH_UNKNOWN;
    }
    // Get PCIe info.
    int pci_domain;
    int pci_bus;
    int pci_device;
    CULOG(cuDeviceGetAttribute(&pci_domain, CU_DEVICE_ATTRIBUTE_PCI_DOMAIN_ID,
                               dev));
    CULOG(cuDeviceGetAttribute(&pci_bus, CU_DEVICE_ATTRIBUTE_PCI_BUS_ID, dev));
    CULOG(cuDeviceGetAttribute(&pci_device, CU_DEVICE_ATTRIBUTE_PCI_DEVICE_ID,
                               dev));
    stringstream dbsf_s;
    dbsf_s << hex << setfill('0') << setw(4) << pci_domain << ":"
           << setfill('0') << setw(2) << pci_bus << ":" << setfill('0')
           << setw(2) << pci_device << ".0";
    this->dbsf = dbsf_s.str();
}

////////////////////////////////////////////////////////////////////////////////

GpuMgr::GpuMgr(const int gpu_id_) : gpu_id{gpu_id_}
{
    // Create a CUDA context.
    CUdevice dev;
    CULOG(cuDeviceGet(&dev, gpu_id_));
    CULOG(cuCtxCreate(&(this->cuda_ctx), CU_CTX_MAP_HOST, dev));

    gpu_info.init(gpu_id_);
}

//
GpuMgr::~GpuMgr()
{
}

//
GpuMgrCtx *GpuMgr::create_context(const std::string &name, int rank,
                                  int world_size)
{
    for (auto &ctx : this->mgr_ctxs) {
        if (ctx->get_name() == name) {
            LOGERR("GpuMgrCtx ", name, " already exists.");
        }
    }
    GpuMgrCtx *ctx = new GpuMgrCtx{this, rank, world_size, name};
    this->mgr_ctxs.emplace_back(ctx);
    return ctx;
}

//
void GpuMgr::destroy_context(GpuMgrCtx *ctx)
{
    auto it = this->mgr_ctxs.begin();
    for (; it != this->mgr_ctxs.end(); ++it) {
        if (it->get() == ctx) {
            this->mgr_ctxs.erase(it);
            break;
        }
    }
}

//
void GpuMgr::validate_total_bytes()
{
    size_t total_bytes = 0;
    for (auto &mgr_ctx : this->mgr_ctxs) {
        total_bytes += mgr_ctx->get_total_bytes();
    }
    if (total_bytes > this->gpu_info.gmem_total) {
        LOGERR("out of GPU memory. Requested ", total_bytes, " bytes");
    }
    LOG(DEBUG, "Requested ", total_bytes, " bytes");
}

//
GpuState GpuMgr::set_current()
{
    return cuCtxSetCurrent(this->cuda_ctx);
}

////////////////////////////////////////////////////////////////////////////////

//
GpuMgrCtx::GpuMgrCtx(GpuMgr *gpu_mgr_, int rank_, int world_size_,
                     const std::string &name_)
    : gpu_mgr{gpu_mgr_}, rank{rank_}, world_size{world_size_}, name{name_},
      data_mem{ARK_GPU_DATA_NAME + name_ + to_string(gpu_mgr_->gpu_id), 0,
               true},
      sc_rc_mem{ARK_GPU_SC_RC_NAME + name_ + to_string(gpu_mgr_->gpu_id),
                2 * MAX_NUM_SID * sizeof(int), true}
{
    // Initialize SCs to ones.
    int *href = (int *)this->sc_rc_mem.href();
    for (int i = 0; i < MAX_NUM_SID; ++i) {
        href[i] = 1;
    }
    // Initialize RCs to zeros.
    for (int i = MAX_NUM_SID; i < 2 * MAX_NUM_SID; ++i) {
        href[i] = 0;
    }
    // Use the CPU-side software communication stack.
    this->comm_sw = new GpuCommSw{name_,       gpu_mgr_->gpu_id, rank_,
                                  world_size_, &data_mem,        &sc_rc_mem};
    assert(this->comm_sw != nullptr);
}

//
GpuMgrCtx::~GpuMgrCtx()
{
    //
    if (this->comm_sw != nullptr) {
        delete this->comm_sw;
    }
    //
    for (GpuStream s : this->streams) {
        cuStreamDestroy(s);
    }
}

//
GpuStream GpuMgrCtx::create_stream()
{
    GpuStream s;
    this->gpu_mgr->set_current();
    CULOG(cuStreamCreate(&s, CU_STREAM_NON_BLOCKING));
    this->streams.emplace_back(s);
    return s;
}

//
GpuState GpuMgrCtx::sync_stream(const GpuStream &s)
{
    return cuStreamSynchronize(s);
}

//
void GpuMgrCtx::destroy_stream(const GpuStream &s)
{
    auto it = this->streams.begin();
    for (; it != this->streams.end(); ++it) {
        if (*it == s) {
            cuStreamDestroy(s);
            this->streams.erase(it);
            break;
        }
    }
}

//
GpuEvent GpuMgrCtx::create_event(bool disable_timing, CUipcEventHandle *handle)
{
    GpuEvent cuda_event;
    unsigned int flags = 0;
    if (disable_timing) {
        flags |= CU_EVENT_DISABLE_TIMING;
    }
    if (handle != nullptr) {
        flags |= CU_EVENT_INTERPROCESS;
    }
    CULOG(cuEventCreate(&cuda_event, flags));
    if (handle != nullptr) {
        CULOG(cuIpcGetEventHandle(handle, cuda_event));
    }
    return cuda_event;
}

//
GpuBuf *GpuMgrCtx::mem_alloc(size_t bytes, int align)
{
    if (bytes == 0) {
        return nullptr;
    }
    int al;
    if (bytes > 32768) {
        al = 65536;
    } else if (bytes > 64) {
        al = 128;
    } else {
        al = 128;
    }
    if (al < align) {
        al = align;
    }
    size_t sz = math::pad(bytes, (size_t)al);
    size_t off;
    int id = this->next_id;
    id_in_use.emplace(this->next_id++);
    //
    std::list<Chunk>::iterator it = this->chunks.begin();
    for (; it != this->chunks.end(); ++it) {
        size_t b = math::pad(it->b, al);
        if ((it->e - b) >= sz) {
            off = b;
            this->usage.emplace_back(off, off + sz);
            if (it->b != b) {
                this->chunks.emplace(it, it->b, b);
            }
            if ((it->e - off) > sz) {
                it->b = off + sz;
            } else {
                this->chunks.erase(it);
            }
            break;
        }
    }
    if (it == this->chunks.end()) {
        // No more segment available.
        // If the last byte is unused, enlarge the last segment.
        // Otherwise, create a new segment.
        if ((this->chunks.size() > 0) &&
            (this->chunks.back().e == this->total_bytes)) {
            Chunk &chunk = this->chunks.back();
            off = math::pad(chunk.b, al);
            if (off != chunk.b) {
                chunk.e = off;
            } else {
                this->chunks.pop_back();
            }
        } else {
            off = math::pad(total_bytes, al);
            if (off != total_bytes) {
                this->chunks.emplace_back(total_bytes, off);
            }
        }
        total_bytes = off + sz;
        this->usage.emplace_back(off, off + sz);
    }
    GpuBuf *buf = new GpuBuf{&this->data_mem, id, off, bytes};
    assert(buf != nullptr);
    this->bufs.emplace_back(buf);
    LOG(DEBUG, "GPU Buffer ", this->name, " ID ", id, " off 0x", hex, off, dec,
        " bytes ", bytes);
    return buf;
}

//
void GpuMgrCtx::mem_free(GpuBuf *buf)
{
    int id = buf->get_id();
    if ((size_t)id >= this->usage.size()) {
        LOGERR("GpuBuf ID ", id, " has never been allocated");
    }
    auto search = this->id_in_use.find(id);
    if (search == this->id_in_use.end()) {
        LOGERR("GpuBuf ID ", id, " is already freed");
    }
    this->id_in_use.erase(search);
    size_t b = this->usage[id].b;
    size_t e = this->usage[id].e;
    if (this->chunks.size() == 0) {
        this->chunks.emplace_back(b, e);
    } else {
        std::list<Chunk>::iterator it = this->chunks.begin();
        for (; it != this->chunks.end(); ++it) {
            if (it->e >= b) {
                if ((it->e == b) && (next(it)->b == e)) {
                    // Merge both sides.
                    it->e = next(it)->e;
                    this->chunks.erase(next(it));
                } else if (it->e == b) {
                    // Merge into left-side.
                    it->e = e;
                } else if (next(it)->b == e) {
                    // Merge into right-side.
                    next(it)->b = b;
                } else {
                    // No merge, just insert.
                    this->chunks.emplace(it, b, e);
                }
                break;
            }
        }
    }
}

//
void GpuMgrCtx::mem_export(GpuBuf *buf, size_t offset, int sid)
{
    // TODO: Check if `buf` is created by this context.
    this->export_sid_offs.emplace_back(sid, buf->get_offset() + offset);
    LOG(DEBUG, "Exported GPU Buffer sid ", sid, " offset ",
        buf->get_offset() + offset);
}

//
GpuBuf *GpuMgrCtx::mem_import(size_t bytes, int sid, int gid)
{
    if (this->comm_sw == nullptr) {
        LOGERR("mem_import() is supported only for the "
               "SW communication stack.");
    }
    GpuMem *dm = this->comm_sw->get_data_mem(gid);
    GpuBuf *buf = new GpuBuf{dm, sid, 0, bytes};
    assert(buf != nullptr);
    LOG(DEBUG, "Imported GPU Buffer from GPU ", gid, " sid ", sid, " bytes ",
        bytes);
    this->bufs.emplace_back(buf);
    this->import_gid_bufs[gid].emplace_back(buf);

    //
    if (this->data_mem.get_bytes() > 0) {
        // Configuration is already done,
        // so we can import the buffer immediately.
        this->comm_sw->import_buf(gid, buf);
    }
    return buf;
}

void GpuMgrCtx::reg_sendrecv(int sid, int remote_gpu_id, size_t bytes,
                             bool is_recv)
{
    this->comm_sw->reg_sendrecv(sid, remote_gpu_id, bytes, is_recv);
}

//
void GpuMgrCtx::freeze()
{
    //
    this->gpu_mgr->validate_total_bytes();

    //
    if (total_bytes > 0) {
        this->data_mem.alloc(total_bytes);
        // init the data mem
        CULOG(cuMemsetD32(this->data_mem.ref(), 0, total_bytes >> 2));
    }

    //
    if (this->comm_sw != nullptr) {
        this->comm_sw->configure(this->export_sid_offs, this->import_gid_bufs);
        this->comm_sw->launch_request_loop();
    }
}

// Write a request.
void GpuMgrCtx::send(int src, int dst, int rank, size_t bytes)
{
    // Wait for the send completion.
    volatile int *sc = this->get_sc_href(src);
    while (*sc == 0) {
    }
    *sc = 0;

    Request db;
    db.fields.req = 0;
    db.fields.dst = dst;
    db.fields.src = src;
    db.fields.rank = rank;
    db.fields.len = bytes;
    db.fields.rsv = 0;
    if (this->comm_sw != nullptr) {
        this->comm_sw->set_request(db);
    }
}

//
GpuState GpuMgrCtx::set_current()
{
    return this->gpu_mgr->set_current();
}

// Get the host memory address of an SC flag.
volatile int *GpuMgrCtx::get_sc_href(int sid) const
{
    return (volatile int *)this->sc_rc_mem.href(sid * sizeof(int));
}

// Get the host memory address of an RC flag.
volatile int *GpuMgrCtx::get_rc_href(int sid) const
{
    return (volatile int *)this->sc_rc_mem.href((MAX_NUM_SID + sid) *
                                                sizeof(int));
}

//
GpuPtr GpuMgrCtx::get_data_ref(int gid) const
{
    if (gid == -1)
        return this->data_mem.ref();

    if (this->comm_sw == nullptr) {
        LOGERR("get_data_ref() is supported only for the "
               "SW communication stack.");
    }
    return this->comm_sw->get_data_mem(gid)->ref();
}

// Get the GPU memory address of an SC flag.
GpuPtr GpuMgrCtx::get_sc_ref(int sid) const
{
    return this->sc_rc_mem.ref(sid * sizeof(int));
}

// Get the GPU memory address of an RC flag.
GpuPtr GpuMgrCtx::get_rc_ref(int sid) const
{
    return this->sc_rc_mem.ref((MAX_NUM_SID + sid) * sizeof(int));
}

//
GpuPtr GpuMgrCtx::get_request_ref() const
{
    if (this->comm_sw != nullptr) {
        return this->comm_sw->get_request_ref();
    } else {
        LOGERR("Unexpected error.");
    }
}

////////////////////////////////////////////////////////////////////////////////

// Global GpuMgr vector.
vector<unique_ptr<GpuMgr>> ARK_GPU_MGR_GLOBAL;

// Return a pointer to a global GpuMgr.
GpuMgr *get_gpu_mgr(const int gpu_id)
{
    if (gpu_id < 0) {
        LOGERR("invalid GPU ID ", gpu_id);
    }
    if (ARK_GPU_MGR_GLOBAL.size() == 0) {
        gpu_init();
        int ngpu = gpu_num();
        if (ngpu <= 0) {
            LOGERR("No CUDA-capable GPU is detected.");
        }
        ARK_GPU_MGR_GLOBAL.resize(ngpu);
    }
    if ((unsigned int)gpu_id >= ARK_GPU_MGR_GLOBAL.size()) {
        LOGERR("invalid GPU ID ", gpu_id);
    }
    GpuMgr *mgr = ARK_GPU_MGR_GLOBAL[gpu_id].get();
    if (mgr == nullptr) {
        mgr = new GpuMgr{gpu_id};
        assert(mgr != nullptr);
        ARK_GPU_MGR_GLOBAL[gpu_id].reset(mgr);
    }
    return mgr;
}

void gpu_memset(GpuPtr buf, int val, size_t num)
{
    CULOG(cuMemsetD32(buf, val, num));
}
void gpu_memcpy(GpuPtr dst, const void *src, size_t bytes)
{
    CULOG(cuMemcpyHtoD(dst, src, bytes));
}
void gpu_memcpy(void *dst, const GpuPtr src, size_t bytes)
{
    CULOG(cuMemcpyDtoH(dst, src, bytes));
}
void gpu_memset(GpuBuf *buf, int val, size_t num)
{
    const size_t &bytes = buf->get_bytes();
    assert(bytes >= 4);
    if ((bytes >> 2) < num) {
        LOGERR("memset requests too many elements. Expected <= ", bytes >> 2,
               ", given ", num);
    }
    GpuPtr pb = buf->ref();
    if (pb != 0) {
        assert((pb % 4) == 0);
        CULOG(cuMemsetD32(pb, val, num));
    } else {
        int *phb = (int *)buf->href();
        assert(phb != nullptr);
        for (size_t i = 0; i < num; ++i) {
            phb[i] = val;
        }
    }
}
void gpu_memcpy(GpuBuf *dst, const void *src, size_t bytes)
{
    CULOG(cuMemcpyHtoD(dst->ref(), src, bytes));
}
void gpu_memcpy(void *dst, const GpuBuf *src, size_t bytes)
{
    CULOG(cuMemcpyDtoH(dst, src->ref(), bytes));
}
void gpu_memcpy(GpuBuf *dst, const GpuBuf *src, size_t bytes)
{
    GpuPtr rd = dst->ref();
    GpuPtr rs = src->ref();
    if ((rd != 0) && (rs != 0)) {
        CULOG(cuMemcpyDtoD(dst->ref(), src->ref(), bytes));
    } else if (rd != 0) {
        CULOG(cuMemcpyHtoD(dst->ref(), src->href(), bytes));
    } else if (rs != 0) {
        CULOG(cuMemcpyDtoH(dst->href(), src->ref(), bytes));
    } else {
        // ::memcpy(dst->href(), src->href(), bytes);
        LOGERR("Unexpected case.");
    }
}

} // namespace ark

```

`ark/gpu/gpu_mgr.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_GPU_MGR_H_
#define ARK_GPU_MGR_H_

#include <cuda.h>
#include <list>
#include <map>
#include <set>
#include <string>
#include <thread>
#include <vector>

#include "gpu/gpu_buf.h"
#include "gpu/gpu_comm_sw.h"

namespace ark {

// Types of GPU architectures.
typedef enum
{
    GPU_ARCH_UNKNOWN = -1,
    GPU_ARCH_CUDA_60,
    GPU_ARCH_CUDA_70,
    GPU_ARCH_CUDA_75,
    GPU_ARCH_CUDA_80,
} GpuArchType;

// Details of a GPU device.
struct GpuInfo
{
    // Constructor.
    void init(const int gpu_id);

    int cc_major;
    int cc_minor;
    size_t gmem_total;
    int smem_total;
    int smem_block_total;
    int num_sm;
    int clk_rate;
    GpuArchType arch;
    std::string arch_str;
    // PCIe "Domain:Bus:Slot.Function"
    std::string dbsf;
    //
    const unsigned int threads_per_warp = 32;
    const unsigned int max_registers_per_thread = 256;
    const unsigned int max_registers_per_block = 65536;
    const unsigned int min_threads_per_block =
        max_registers_per_block / max_registers_per_thread;
    const unsigned int max_threads_per_block = 1024;
    const unsigned int smem_align = 128;
};

////////////////////////////////////////////////////////////////////////////////

//
typedef CUstream GpuStream;
typedef CUevent GpuEvent;

//
class GpuMgrCtx;

//
class GpuMgr
{
  public:
    GpuMgr(const int gpu_id);
    ~GpuMgr();

    GpuMgrCtx *create_context(const std::string &name, int rank,
                              int world_size);
    void destroy_context(GpuMgrCtx *ctx);

    void validate_total_bytes();

    GpuState set_current();

    const GpuInfo &get_gpu_info() const
    {
        return gpu_info;
    }

    //
    const int gpu_id;

  private:
    //
    GpuInfo gpu_info;
    // CUDA context of this GPU.
    CUcontext cuda_ctx;
    //
    std::list<std::unique_ptr<GpuMgrCtx>> mgr_ctxs;
};

//
class GpuMgrCtx
{
  public:
    GpuMgrCtx(GpuMgr *gpu_mgr, int rank_, int world_size_,
              const std::string &name);
    ~GpuMgrCtx();

    GpuStream create_stream();
    GpuState sync_stream(const GpuStream &s);
    void destroy_stream(const GpuStream &s);
    GpuEvent create_event(bool disable_timing,
                          CUipcEventHandle *handle = nullptr);

    //
    GpuBuf *mem_alloc(size_t bytes, int align = 1);
    void mem_free(GpuBuf *buf);
    void mem_export(GpuBuf *buf, size_t offset, int sid);
    GpuBuf *mem_import(size_t bytes, int sid, int gpu_id);
    void reg_sendrecv(int sid, int gpu_dst, std::size_t bytes, bool is_recv);
    void freeze();
    void send(int src, int dst, int rank, size_t bytes);
    GpuState set_current();
    const int &get_world_size() const
    {
        return world_size;
    }
    const int &get_gpu_id() const
    {
        return gpu_mgr->gpu_id;
    }
    const std::string &get_name() const
    {
        return name;
    }
    const size_t &get_total_bytes() const
    {
        return total_bytes;
    }
    // Get the host memory address of an SC flag.
    volatile int *get_sc_href(int sid) const;
    // Get the host memory address of an RC flag.
    volatile int *get_rc_href(int sid) const;
    // Get the GPU memory address of a GPU data buffer.
    GpuPtr get_data_ref(int gid = -1) const;
    // Get the GPU memory address of an SC flag.
    GpuPtr get_sc_ref(int sid) const;
    // Get the GPU memory address of an RC flag.
    GpuPtr get_rc_ref(int sid) const;
    //
    GpuPtr get_request_ref() const;

    //
    bool is_comm_sw() const
    {
        return (this->comm_sw != nullptr);
    }

  private:
    //
    struct Chunk
    {
        Chunk(size_t b_, size_t e_) : b{b_}, e{e_}
        {
        }
        size_t b;
        size_t e;
    };

    int next_id = 0;
    size_t total_bytes = 0;
    std::list<Chunk> chunks;
    std::vector<Chunk> usage;
    std::set<int> id_in_use;

    GpuMgr *gpu_mgr;
    const int rank;
    const int world_size;
    std::string name;
    GpuMem data_mem;
    GpuMem sc_rc_mem;

    //
    std::vector<GpuStream> streams;

    std::list<std::unique_ptr<GpuBuf>> bufs;

    std::vector<std::pair<int, size_t>> export_sid_offs;
    std::map<int, std::vector<GpuBuf *>> import_gid_bufs;

    GpuCommSw *comm_sw = nullptr;

    std::set<int> sids_in_use;
};

//
GpuMgr *get_gpu_mgr(const int gpu_id);

//
void gpu_memset(GpuPtr buf, int val, size_t num);
void gpu_memcpy(GpuPtr dst, const void *src, size_t bytes);
void gpu_memcpy(void *dst, const GpuPtr src, size_t bytes);
void gpu_memset(GpuBuf *buf, int val, size_t num);
void gpu_memcpy(GpuBuf *dst, const void *src, size_t bytes);
void gpu_memcpy(void *dst, const GpuBuf *src, size_t bytes);
void gpu_memcpy(GpuBuf *dst, const GpuBuf *src, size_t bytes);

} // namespace ark

#endif // ARK_GPU_MGR_H_

```

`ark/gpu/gpu_mgr_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_mgr.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "unittest/unittest_utils.h"

using namespace std;
using namespace ark;

// Test initializing and destroying GpuMgr and GpuMgrCtx.
unittest::State test_gpu_mgr_basic()
{
    int pid = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{10};
        GpuMgr *mgr_a = get_gpu_mgr(0);
        GpuMgr *mgr_b = get_gpu_mgr(0);

        UNITTEST_EQ(mgr_a, mgr_b);

        GpuMgr *mgr_c = get_gpu_mgr(1);

        UNITTEST_NE(mgr_a, mgr_c);

        GpuMgrCtx *ctx_a = mgr_a->create_context("test_a", 0, 1);
        UNITTEST_NE(ctx_a, (GpuMgrCtx *)nullptr);

        mgr_a->destroy_context(ctx_a);

        return 0;
    });

    int ret = ark::utils::proc_wait(pid);
    UNITTEST_EQ(ret, 0);
    return unittest::SUCCESS;
}

// Test accessing remote GPU's memory space.
unittest::State test_gpu_mgr_remote()
{
    int pid0 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{10};
        GpuMgr *mgr = get_gpu_mgr(0);
        GpuMgrCtx *ctx = mgr->create_context("test", 0, 2);

        GpuBuf *gpu0_eid3 = ctx->mem_alloc(sizeof(int));
        GpuBuf *gpu0_eid4 = ctx->mem_alloc(sizeof(int));
        ctx->mem_export(gpu0_eid3, 0, 3);
        ctx->mem_export(gpu0_eid4, 0, 4);

        GpuBuf *gpu1_eid5 = ctx->mem_import(sizeof(int), 5, 1);
        GpuBuf *gpu1_eid6 = ctx->mem_import(sizeof(int), 6, 1);

        ctx->freeze();

        volatile int *ptr = (volatile int *)gpu0_eid3->href();
        while (*ptr != 7890) {
        }

        gpu_memset(gpu1_eid5, 1234, 1);

        ptr = (volatile int *)gpu0_eid4->href();
        while (*ptr != 3456) {
        }

        gpu_memset(gpu1_eid6, 5678, 1);
        return 0;
    });
    UNITTEST_NE(pid0, -1);

    int pid1 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{10};
        GpuMgr *mgr = get_gpu_mgr(1);
        GpuMgrCtx *ctx = mgr->create_context("test", 1, 2);

        GpuBuf *gpu1_eid5 = ctx->mem_alloc(sizeof(int));
        GpuBuf *gpu1_eid6 = ctx->mem_alloc(sizeof(int));
        ctx->mem_export(gpu1_eid5, 0, 5);
        ctx->mem_export(gpu1_eid6, 0, 6);

        GpuBuf *gpu0_eid3 = ctx->mem_import(sizeof(int), 3, 0);
        GpuBuf *gpu0_eid4 = ctx->mem_import(sizeof(int), 4, 0);

        ctx->freeze();

        gpu_memset(gpu0_eid3, 7890, 1);

        volatile int *ptr = (volatile int *)gpu1_eid5->href();
        while (*ptr != 1234) {
        }

        gpu_memset(gpu0_eid4, 3456, 1);

        ptr = (volatile int *)gpu1_eid6->href();
        while (*ptr != 5678) {
        }

        return 0;
    });
    UNITTEST_NE(pid1, -1);

    int ret = ark::utils::proc_wait({pid0, pid1});
    UNITTEST_EQ(ret, 0);
    return unittest::SUCCESS;
}

// Test accessing remote GPU's memory space after the context is freezed.
unittest::State test_gpu_mgr_remote_lazy_import()
{
    int pid0 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{10};
        GpuMgr *mgr = get_gpu_mgr(0);
        GpuMgrCtx *ctx = mgr->create_context("test", 0, 2);

        GpuBuf *gpu0_eid3 = ctx->mem_alloc(sizeof(int));
        GpuBuf *gpu0_eid4 = ctx->mem_alloc(sizeof(int));
        ctx->mem_export(gpu0_eid3, 0, 3);
        ctx->mem_export(gpu0_eid4, 0, 4);

        ctx->freeze();

        GpuBuf *gpu1_eid5 = ctx->mem_import(sizeof(int), 5, 1);
        GpuBuf *gpu1_eid6 = ctx->mem_import(sizeof(int), 6, 1);

        volatile int *ptr = (volatile int *)gpu0_eid3->href();
        while (*ptr != 7890) {
        }

        gpu_memset(gpu1_eid5, 1234, 1);

        ptr = (volatile int *)gpu0_eid4->href();
        while (*ptr != 3456) {
        }

        gpu_memset(gpu1_eid6, 5678, 1);

        return 0;
    });
    UNITTEST_NE(pid0, -1);

    int pid1 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{10};
        GpuMgr *mgr = get_gpu_mgr(1);
        GpuMgrCtx *ctx = mgr->create_context("test", 1, 2);

        GpuBuf *gpu1_eid5 = ctx->mem_alloc(sizeof(int));
        GpuBuf *gpu1_eid6 = ctx->mem_alloc(sizeof(int));
        ctx->mem_export(gpu1_eid5, 0, 5);
        ctx->mem_export(gpu1_eid6, 0, 6);

        ctx->freeze();

        GpuBuf *gpu0_eid3 = ctx->mem_import(sizeof(int), 3, 0);
        GpuBuf *gpu0_eid4 = ctx->mem_import(sizeof(int), 4, 0);

        gpu_memset(gpu0_eid3, 7890, 1);

        volatile int *ptr = (volatile int *)gpu1_eid5->href();
        while (*ptr != 1234) {
        }

        gpu_memset(gpu0_eid4, 3456, 1);

        ptr = (volatile int *)gpu1_eid6->href();
        while (*ptr != 5678) {
        }

        return 0;
    });
    UNITTEST_NE(pid1, -1);

    int ret = ark::utils::proc_wait({pid0, pid1});
    UNITTEST_EQ(ret, 0);
    return unittest::SUCCESS;
}

// Test inter-GPU communication via sending a request.
unittest::State test_gpu_mgr_request()
{
    int pid0 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{10};
        GpuMgr *mgr = get_gpu_mgr(0);
        GpuMgrCtx *ctx = mgr->create_context("test", 0, 2);

        GpuBuf *gpu0_src = ctx->mem_alloc(1024 * sizeof(int));
        ctx->mem_export(gpu0_src, 0, 7);
        ctx->mem_import(1024 * sizeof(int), 5, 1);

        ctx->freeze();

        // Set source data.
        int *data = (int *)gpu0_src->href();
        for (int i = 0; i < 1024; ++i) {
            data[i] = i + 1;
        }

        // Send a request.
        ctx->send(7, 5, 1, 1024 * sizeof(int));

        // Wait for the send completion.
        volatile int *sc = ctx->get_sc_href(7);
        while (*sc == 0) {
        }

        return 0;
    });
    UNITTEST_NE(pid0, -1);

    int pid1 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{10};
        GpuMgr *mgr = get_gpu_mgr(1);
        GpuMgrCtx *ctx = mgr->create_context("test", 1, 2);

        GpuBuf *gpu1_dst = ctx->mem_alloc(1024 * sizeof(int));
        ctx->mem_export(gpu1_dst, 0, 5);
        ctx->mem_import(1024 * sizeof(int), 7, 0);

        ctx->freeze();

        // Wait for the receive completion.
        volatile int *rc = ctx->get_rc_href(5);
        while (*rc == 0) {
        }

        // Verify the received data.
        int *data = (int *)gpu1_dst->href();
        for (int i = 0; i < 1024; ++i) {
            UNITTEST_EQ(data[i], i + 1);
        }

        return 0;
    });
    UNITTEST_NE(pid1, -1);

    int ret = ark::utils::proc_wait({pid0, pid1});
    UNITTEST_EQ(ret, 0);
    return unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_gpu_mgr_basic);
    UNITTEST(test_gpu_mgr_remote);
    UNITTEST(test_gpu_mgr_remote_lazy_import);
    UNITTEST(test_gpu_mgr_request);
    return 0;
}

```

`ark/include/ark.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_H
#define ARK_H

#include <memory>
#include <string>
#include <vector>

#define ARK_MAJOR 0
#define ARK_MINOR 1
#define ARK_PATCH 0
#define ARK_VERSION (ARK_MAJOR * 10000 + ARK_MINOR * 100 + ARK_PATCH)

namespace ark {

/// Return a version string.
std::string version();

// set random seed
void srand(int seed = -1);

// get random number
int rand();

/// Initialize the ARK runtime.
///
/// This function should be called by the user before any other functions are
/// called. It is safe to call this function multiple times.
void init();

// Data type for dimension.
typedef long long int DimType;

// DIMS_LEN is the maximum number of dimensions of a tensor. If a tensor
// has less than DIMS_LEN dimensions, the remaining dimensions will be NO_DIM.
enum
{
    DIMS_LEN = 4,
    NO_DIM = -1
};

// Up-to-`DIMS_LEN`-dimensional vector.
struct Dims
{
    // Construct with given four dimensions.
    Dims(DimType d0 = NO_DIM, DimType d1 = NO_DIM, DimType d2 = NO_DIM,
         DimType d3 = NO_DIM);
    // Copy another Dims object.
    Dims(const Dims &dims_);
    // Construct from a vector. If the vector is shorter than DIMS_LEN, put
    // following NO_DIMs. Raise an error if the vector is longer than DIMS_LEN.
    Dims(const std::vector<DimType> &vec);

    // Return the volume of dimensions. If the dimensions are invalid, return
    // -1.
    DimType size() const;
    // Return the number of valid dimensions.
    int ndims() const;
    // Return a new Dims object with 4 valid dimensions by prepending 1s.
    Dims dims4() const;
    // Return true if the dimensions are empty.
    bool is_no_dim() const;
    // Return true if the dimensions are invalid.
    bool is_invalid() const;
    // Erase the dimension at the given index and return the erased dimension.
    DimType erase(int idx);

    DimType &operator[](int idx);

    const DimType &operator[](int idx) const;

    constexpr Dims &operator=(const Dims &) = default;

    friend bool operator==(const Dims &a, const Dims &b);
    friend bool operator!=(const Dims &a, const Dims &b);

    friend std::ostream &operator<<(std::ostream &os, const Dims &dims);

    DimType data[DIMS_LEN];
};

// TensorBuf refers to a data array that can be shared by multiple tensors.
struct TensorBuf
{
    TensorBuf(const DimType &bytes = 0, int id = -1);
    TensorBuf(const TensorBuf &) = default;

    DimType bytes;
    int id;
    bool immutable = false;
};

// Type of tensor data.
typedef enum
{
    FP16,
    FP32,
    INT32,
    BYTE,
} TensorType;

// Tensor is a view of a TensorBuf.
//
// Illustration of a single axis of a tensor:
//
// 0           off                                                        ldim
// |------------|-------------shape-------------|---------------------------|
//               <----------------------------->
//                  data range of this tensor
//
struct Tensor
{
    // Tensor constructor
    Tensor(const Dims &shape, TensorType type, TensorBuf *buf,
           const Dims &ldims, const Dims &offs, const Dims &pads, bool exported,
           int imported_rank, int id, const std::string &name);
    Tensor(const Tensor &) = default;

    void update_pads(const std::vector<DimType> &pads);
    // Offset to the element [i0][i1][i2][i3] of this tensor in the TensorBuf.
    DimType offset(DimType i0 = 0, DimType i1 = 0, DimType i2 = 0,
                   DimType i3 = 0) const;
    // Number of elements in the tensor excluding padding.
    DimType size() const;
    // Number of dimensions in the tensor.
    int ndims() const;
    // Shape of the tensor including padding.
    Dims padded_shape() const;
    // Number of bytes of each element in the tensor.
    unsigned int type_bytes() const;
    // Number of bytes of the tensor.
    DimType shape_bytes() const;
    // Should be the same as the number of bytes of the TensorBuf.
    DimType ldims_bytes() const;
    // Offset in bytes.
    DimType offset_bytes(DimType i0 = 0, DimType i1 = 0, DimType i2 = 0,
                         DimType i3 = 0) const;
    // TODO: deprecate this function.
    bool is_sequential() const;

    // TensorBuf that this tensor is associated with
    TensorBuf *buf;
    // Data type of each element in the tensor
    TensorType type;
    // Shape of the tensor
    Dims shape;
    // Leading dimensions of the underlying data array
    Dims ldims;
    // Offset of the tensor in the underlying data array
    Dims offs;
    // Unit dimensions of the underlying data array. ldims[x] should be always
    // divided by pads[x].
    Dims pads;
    // Whether this tensor is accessed by remote devices
    bool exported;
    // If imported_rank is non-negative, the tensor is imported from another GPU
    // and don't need to allocate a TensorBuf for it.
    int imported_rank;
    // Unique id of this tensor
    int id;
    // Name of this tensor
    const std::string name;
};

class Model
{
  public:
    // Constructors.
    Model(int rank_ = 0);
    Model(const Model &) = delete;
    Model &operator=(const Model &) = delete;

    ~Model();

    // construct a tensor with given shape and data type.
    Tensor *tensor(const Dims &shape, TensorType dtype,
                   TensorBuf *buf = nullptr, const Dims &ldims = {},
                   const Dims &offs = {}, const Dims &pads = {},
                   const std::vector<Tensor *> &deps = {},
                   bool exported = false, int imported_rank = -1,
                   const std::string &name = "tensor");

    Tensor *reshape(Tensor *input, const Dims &shape, bool allowzero = false,
                    Tensor *output = nullptr,
                    const std::string &name = "reshape");
    // Reshape `input` to `shape`. If one dimension of `shape` is -1, it will be
    // inferred from the `input`. If one dimension of `shape` is 0, by default
    // (`allowzero` is false), that dimension is unchanged from the
    // corresponding one of `input`. If `allowzero` is true, that dimension is
    // set to 0, which means that the reshaped tensor is an empty tensor, i.e.,
    // `input` should also be an empty tensor. If `allowzero` is true, `shape`
    // should not include both 0 and -1 at the same time. If `shape` is an empty
    // vector, `input` will be converted to a scalar.
    Tensor *reshape(Tensor *input, std::initializer_list<DimType> shape,
                    bool allowzero = false, Tensor *output = nullptr,
                    const std::string &name = "reshape");
    // Returns an identical tensor of `input` with execution dependencies
    // `deps`.
    Tensor *identity(Tensor *input, const std::vector<Tensor *> &deps = {},
                     const std::string &name = "identity");

    // Shard `input` along `axis` into `dim_per_shard`-dimensional shards.
    std::vector<Tensor *> sharding(Tensor *input, DimType axis,
                                   DimType dim_per_shard,
                                   const std::string &name = "sharding");
    // Performs reduction along the `axis` of the `input` tensor and stores the
    // result in `output`.
    // Currently, only reduction along the last dimension is supported.
    Tensor *reduce_sum(Tensor *input, int axis, Tensor *output = nullptr,
                       const std::string &name = "reduce_sum");
    Tensor *reduce_mean(Tensor *input, int axis, Tensor *output = nullptr,
                        const std::string &name = "reduce_mean");
    Tensor *reduce_max(Tensor *input, int axis, Tensor *output = nullptr,
                       const std::string &name = "reduce_max");
    // Applies layer normalization to the `input` tensor and returns the
    // normalized tensor as `output`.

    Tensor *layernorm(Tensor *input, Tensor *output = nullptr,
                      const std::string &name = "layernorm");
    // Applies softmax activation to the `input` tensor, with the softmax
    // operator
    // being performed on the last dimension of the input tensor.
    Tensor *softmax(Tensor *input, Tensor *output = nullptr,
                    const std::string &name = "softmax");
    // Transposes the `input` tensor according to the given `perm` permutation.
    // For example, transpose(input, {0, 1 ,3, 2}) will swap the last two
    // dimensions of the input tensor. Currently, only 4D tensors are supported.
    Tensor *transpose(Tensor *input, Dims perm, Tensor *output = nullptr,
                      const std::string &name = "transpose");
    // Performs matrix multiplication between the `input` tensor and another
    // `other` tensor, storing the result in `output`.
    // Optional parameters allow controlling the behavior of the multiplication,
    // such as transposing the input tensors and applying a ReLU activation.
    Tensor *matmul(Tensor *input, Tensor *other, Tensor *output = nullptr,
                   DimType splitk = 1, bool trans_input = false,
                   bool trans_other = false, bool is_relu = false,
                   const std::string &name = "matmul", int gran_lev = -1);
    // Implements the 'im2col' method for 2D convolution layers, which takes an
    // `input` tensor and reshapes it to a 2D matrix by extracting image patches
    // from the input tensor based on the provided parameters.
    Tensor *im2col(Tensor *input, int kernel_height, int kernel_width,
                   int stride_height, int stride_width, int pad_height,
                   int pad_width, int dilation_height, int dilation_width,
                   Tensor *output = nullptr,
                   const std::string &name = "im2col");
    // Implements a 2D convolution layer using the 'im2col' method.
    Tensor *conv2d(Tensor *input, DimType in_channels, DimType out_channels,
                   DimType kernel_size, DimType stride, DimType padding,
                   bool bias = false, Tensor *output = nullptr,
                   const std::string &name = "conv2d");
    // Applies max-pooling on the `input` tensor using `kernel_size` and
    // `stride`, reducing its spatial size. The output shape is calculated based
    // on the input tensor's shape and the stride value as follows: {is[0],
    // (is[1] + stride - 1) / stride, (is[2] + stride - 1) / stride, is[3]},
    // where 'is' represents the input tensor's shape.
    Tensor *max_pool(Tensor *input, DimType kernel_size, DimType stride,
                     Tensor *output = nullptr,
                     const std::string &name = "max_pool");
    // Multiplies the `input` tensor by a scalar `val`, element-wise.
    Tensor *scale(Tensor *input, float val, Tensor *output = nullptr,
                  const std::string &name = "scale");
    // ReLU activation
    Tensor *relu(Tensor *input, Tensor *output = nullptr,
                 const std::string &name = "relu");
    // Applies the Gaussian Error Linear Unit (GELU) activation function to the
    // `input` tensor, element-wise. GELU is a smooth approximation of the
    // rectifier function and is widely used in deep learning models.
    Tensor *gelu(Tensor *input, Tensor *output = nullptr,
                 const std::string &name = "gelu");
    // Performs an element-wise addition operator between the `input` tensor
    // and the `other` tensor
    Tensor *add(Tensor *input, Tensor *other, Tensor *output = nullptr,
                const std::string &name = "add");
    // Performs an element-wise multiplication operator between the `input`
    // tensor and the `other` tensor,
    Tensor *mul(Tensor *input, Tensor *other, Tensor *output = nullptr,
                const std::string &name = "mul");
    /// Sends a tensor to a destination GPU (@p dst_rank). Multiple tensors can
    /// be sent to the same GPU,so an identifier `id` is required to distinguish
    /// the tensor. Each 'send' operator must have a corresponding 'recv'
    /// operator that have the same id in another GPU's model.
    ///
    /// @param input
    /// @param id
    /// @param dst_rank Rank of the GPU to send to.
    /// @param bytes
    /// @param output
    /// @param name
    /// @return
    Tensor *send(Tensor *input, int id, int dst_rank, std::size_t bytes = 0,
                 Tensor *output = nullptr, const std::string &name = "send");
    // Blocks the execution until the corresponding 'send' operator with the
    // specified `id` is completed.
    Tensor *send_done(Tensor *input, int id, int dst_rank,
                      Tensor *output = nullptr,
                      const std::string &name = "send_done");
    // Receives a tensor from a source GPU (@p src_rank), identified by the `id`
    // parameter. Blocks the execution until the corresponding 'recv' operator
    // is completed.
    Tensor *recv(Tensor *input, int id, int src_rank, std::size_t bytes = 0,
                 Tensor *output = nullptr, const std::string &name = "recv");
    // Similar to the 'send_done' function, but implemented using CUDA in-stream
    // RDMA copy and Low Latency (LL) protocol.
    Tensor *send_mm(Tensor *input, int id, int gpu_dst, std::size_t bytes = 0,
                    Tensor *output = nullptr,
                    const std::string &name = "send_mm");
    // Similar to the 'recv' function, but implemented using CUDA in-stream RDMA
    // copy and Low Latency (LL) protocol.
    Tensor *recv_mm(Tensor *input, int id, int gpu_src, std::size_t bytes = 0,
                    Tensor *output = nullptr,
                    const std::string &name = "recv_mm");
    // Performs an all-reduce operator across all GPUs, aggregating the input
    // tensors. Takes the `input` tensor, the current GPU's `gpu_id`, and the
    // total number of GPUs `gpu_num`.
    Tensor *all_reduce(Tensor *input, int gpu_id, int gpu_num,
                       Tensor *output = nullptr,
                       const std::string &name = "all_reduce");
    // Performs an all-gather operator across all GPUs, aggregating the input
    // tensors. Takes the `input` tensor, the current GPU's `gpu_id`, and the
    // total number of GPUs `gpu_num`. Returns a vector of tensors, each
    // containing the aggregated data from all GPUs.
    std::vector<Tensor *> all_gather(Tensor *input, int gpu_id, int gpu_num,
                                     std::vector<Tensor *> output,
                                     const std::string &name);

    /// Verify if this model is valid.
    /// @return true if the model is valid, false otherwise.
    bool verify() const;

  protected:
    class Impl;
    friend class OpGraph;
    friend class SimpleScheduler;
    friend class DefaultScheduler;

  private:
    std::unique_ptr<Impl> impl;
};

class GpuBuf;

// Convenience class for executing a model.
class Executor
{
  public:
    // Constructor.
    Executor(const int gpu_id_, int rank_, int world_size_, Model &model,
             const std::string &name, int num_warps_per_sm_ = 16);
    ~Executor();
    // Compile the model. This must be called before `launch()`.
    void compile();
    // Launch the model (not running yet). This must be called after
    // `compile()`.
    void launch();
    // Run the model for `iter` iterations.
    void run(int iter);
    // Wait for the previous run to finish.
    void wait();
    // Stop the model and return the elapsed time in milliseconds.
    // Once this is called, we need to call `launch()` again to run the model
    // again.
    float stop();
    // Get the corresponding GPU buffer of the executor from the given model
    // tensor.
    GpuBuf *get_gpu_buf(Tensor *tns) const;
    // Copy contiguous data from a host buffer to the given tensor's (possibly
    // non-contiguous) data range on GPU.
    void tensor_memcpy(Tensor *tns, const void *src, size_t bytes);
    // Copy (possibly non-contiguous) data from a tensor on GPU to a contiguous
    // host buffer. The given number of bytes is copied, in order of appearance
    // on the memory. This function assumes that `dst` is large enough to hold
    // the data.
    void tensor_memcpy(void *dst, Tensor *src, size_t bytes);
    // Set all bytes of `tns` into zero.
    void tensor_clear(Tensor *tns);
    // Print the content of `tns` to stdout.
    void print_tensor(Tensor *tns);

  protected:
    class Impl;

  private:
    const int gpu_id;
    const int rank;
    const int world_size;
    std::unique_ptr<Impl> impl;
};

} // namespace ark

#endif // ARK_H

```

`ark/include/ark_utils.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_UTILS_H
#define ARK_UTILS_H

#include "ark.h"
#include <functional>
#include <memory>
#include <vector>

namespace ark {

// 16-bit floating point type.
struct alignas(2) half_t
{
    uint16_t storage;
    half_t() = default;
    // Constructor with float parameter
    half_t(float f);
    // Conversion operator from half to float
    operator float() const;
};

} // namespace ark

ark::half_t operator+(ark::half_t const &lhs, ark::half_t const &rhs);
ark::half_t operator-(ark::half_t const &lhs, ark::half_t const &rhs);
ark::half_t operator*(ark::half_t const &lhs, ark::half_t const &rhs);
ark::half_t &operator+=(ark::half_t &lhs, ark::half_t const &rhs);
ark::half_t &operator-=(ark::half_t &lhs, ark::half_t const &rhs);

ark::half_t abs(ark::half_t const &val);

// A set of utility functions
namespace ark {
namespace utils {

// Return a random value array.
template <typename T> std::unique_ptr<T[]> rand_array(size_t num, float max_val)
{
    int mid = RAND_MAX / 2;
    T *ret = new T[num];
    for (size_t i = 0; i < num; ++i) {
        ret[i] = T((ark::rand() - mid) / (float)mid * max_val);
    }
    return std::unique_ptr<T[]>(ret);
}

// Return a random half_t array.
std::unique_ptr<half_t[]> rand_halfs(size_t num, float max_val);
// Return a random float array.
std::unique_ptr<float[]> rand_floats(size_t num, float max_val);

// Return a half_t range array.
std::unique_ptr<half_t[]> range_halfs(size_t num, float begin = 1.0f,
                                      float diff = 1.0f);
// Return a float range array.
std::unique_ptr<float[]> range_floats(size_t num, float begin = 1.0f,
                                      float diff = 1.0f);

// Return an array where each element is 0.
template <typename T> std::unique_ptr<T[]> zeros(size_t num)
{
    T *ret = new T[num];
    for (size_t i = 0; i < num; ++i) {
        ret[i] = T(0);
    }
    return std::unique_ptr<T[]>(ret);
}

// Return an array where each element is 1.
template <typename T> std::unique_ptr<T[]> ones(size_t num)
{
    T *ret = new T[num];
    for (size_t i = 0; i < num; ++i) {
        ret[i] = T(1);
    }
    return std::unique_ptr<T[]>(ret);
}

// Return the error rate between two values.
float error_rate(half_t a, half_t b);
float error_rate(float a, float b);

// Return mean squared error and max error rate between two matrices.
std::pair<float, float> cmp_matrix(half_t *ground_truth, half_t *res,
                                   unsigned int m, unsigned int n,
                                   unsigned int bs = 1, unsigned int lm = 0,
                                   unsigned int ln = 0, bool print = false);
std::pair<float, float> cmp_matrix(float *ground_truth, float *res,
                                   unsigned int m, unsigned int n,
                                   unsigned int bs = 1, unsigned int lm = 0,
                                   unsigned int ln = 0, bool print = false);

// Print a matrix.
void print_matrix(half_t *val, unsigned int m, unsigned int n, unsigned int bs,
                  unsigned int lm, unsigned int ln);
void print_matrix(float *val, unsigned int m, unsigned int n, unsigned int bs,
                  unsigned int lm, unsigned int ln);

//
std::pair<float, float> tensor_compare(half_t *ground_truth, half_t *res,
                                       Dims shape, bool print);
std::pair<float, float> tensor_compare(float *ground_truth, float *res,
                                       Dims shape, bool print);

// Spawn a process that runs `func`. Returns PID of the spawned process.
int proc_spawn(const std::function<int()> &func);
// Wait for a spawned process with PID `pid`.
// Return -1 on any unexpected failures, otherwise return the exit status.
int proc_wait(int pid);
// Wait for multiple child processes.
// Return 0 on success, -1 on any unexpected failure, otherwise the first seen
// non-zero exit status.
int proc_wait(const std::vector<int> &pids);

} // namespace utils
} // namespace ark

#endif

```

`ark/include/kernels/activation.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_ACTIVATION_H_
#define ARK_KERNELS_ACTIVATION_H_

#include "broadcast.h"

namespace ark {

struct Relu
{
    static DEVICE __half2 compute(__half2 input)
    {
        return __hmax2(input, (__half2_raw){0, 0});
    }
};

struct Gelu
{
    static DEVICE __half2 compute(__half2 input)
    {
        __half2 half_pi =
            __float2half2_rn(0.7978845608f); // sqrt(2 / pi) = 0.7978845608
        __half2 coeff = __float2half2_rn(0.044715f);
        __half2 one = __float2half2_rn(1.0f);

        __half2 x_cubed = __hmul2(input, __hmul2(input, input));
        __half2 tanh_input = __hadd2(__hmul2(input, half_pi),
                                     __hmul2(x_cubed, __hmul2(coeff, half_pi)));

        // Convert __half2 to float2
        float2 input_float2 = __half22float2(tanh_input);

        // Compute tanh for each float in the float2 variable
        float2 output_float2 =
            make_float2(tanhf(input_float2.x), tanhf(input_float2.y));

        // Convert float2 back to __half2
        __half2 tanh_output = __float22half2_rn(output_float2);

        return __hmul2(__hmul2(input, __hadd2(one, tanh_output)),
                       __float2half2_rn(0.5f));
    }
};

template <typename _ActivationType, typename _InShape, typename _DataType,
          int _NelemPerThread>
struct Activation;

template <typename _ActivationType, typename _InShape>
struct Activation<_ActivationType, _InShape, half, 2>
{
    using DataType = half;
    static const int NelemPerThread = 2;

    static DEVICE void compute(half *output, const half *input)
    {
        __half2 *pout = (__half2 *)output;
        if (_InShape::W == 1) {
            *pout =
                _ActivationType::compute(__half2half2(*(const __half *)input));
        } else {
            __half2 *pin = (__half2 *)input;
            *pout = _ActivationType::compute(*pin);
        }
    }
};

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes>
DEVICE void relu(half *out, half *in, int uop_idx, int)
{
    Broadcast1<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
               SmemBytes, Activation<Relu, InShape, half, 2>>::run(out, in,
                                                                   uop_idx);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes>
DEVICE void gelu(half *out, half *in, int uop_idx, int)
{
    Broadcast1<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
               SmemBytes, Activation<Gelu, InShape, half, 2>>::run(out, in,
                                                                   uop_idx);
}

} // namespace ark

#endif // ARK_KERNELS_ACTIVATION_H_

```

`ark/include/kernels/arch.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_ARCH_H_
#define ARK_KERNELS_ARCH_H_

#include "device.h"
#include "static_math.h"

namespace ark {

struct Arch
{
#if (ARK_TARGET_CUDA_ARCH == 60)
    static const int ThreadsPerWarp = 32;
    static const int MaxRegistersPerBlock = 65536;
    static const int MaxSmemBytesPerBlock = 49152;
    static const int MaxRegistersPerThread = 256;
#elif (ARK_TARGET_CUDA_ARCH == 70)
    static const int ThreadsPerWarp = 32;
    static const int MaxRegistersPerBlock = 65536;
    static const int MaxSmemBytesPerBlock = 98304;
    static const int MaxRegistersPerThread = 256;
#elif (ARK_TARGET_CUDA_ARCH == 75)
    static const int ThreadsPerWarp = 32;
    static const int MaxRegistersPerBlock = 65536;
    static const int MaxSmemBytesPerBlock = 65536;
    static const int MaxRegistersPerThread = 256;
#elif (ARK_TARGET_CUDA_ARCH == 80)
    static const int ThreadsPerWarp = 32;
    static const int MaxRegistersPerBlock = 65536;
    static const int MaxSmemBytesPerBlock = 166912;
    static const int MaxRegistersPerThread = 256;
#endif

    static const int ArkMinThreadsPerBlock =
        MaxRegistersPerBlock / MaxRegistersPerThread;
};

DEVICE int warp_id()
{
    return threadIdx.x >> math::log2_up<Arch::ThreadsPerWarp>::value;
}

} // namespace ark

#endif // ARK_KERNELS_ARCH_H_

```

`ark/include/kernels/arithmetic.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_ARITHMETIC_H_
#define ARK_KERNELS_ARITHMETIC_H_

#include "common.h"

namespace ark {

struct Add
{
    static DEVICE float compute(float a, float b)
    {
        return a + b;
    }
    static DEVICE half compute(half a, half b)
    {
        return a + b;
    }
    static DEVICE __half compute(__half a, __half b)
    {
        return __hadd(a, b);
    }
    static DEVICE __half2 compute(__half2 a, __half2 b)
    {
        return __hadd2(a, b);
    }
};

struct Mul
{
    static DEVICE float compute(float a, float b)
    {
        return a * b;
    }
    static DEVICE half compute(half a, half b)
    {
        return a * b;
    }
    static DEVICE __half compute(__half a, __half b)
    {
        return __hmul(a, b);
    }
    static DEVICE __half2 compute(__half2 a, __half2 b)
    {
        return __hmul2(a, b);
    }
};

template <typename _ArithmeticType, typename _In0Shape, typename _In1Shape,
          typename _DataType, int _NelemPerThread>
struct Arithmetic
{
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *c, const DataType *a,
                               const DataType *b)
    {
        *c = *a + *b;
        if (_In0Shape::W == 1) {
#pragma unroll
            for (int i = 1; i < NelemPerThread; ++i) {
                c[i] = _ArithmeticType::compute(*a, b[i]);
            }
        } else if (_In1Shape::W == 1) {
#pragma unroll
            for (int i = 1; i < NelemPerThread; ++i) {
                c[i] = _ArithmeticType::compute(a[i], *b);
            }
        } else {
#pragma unroll
            for (int i = 1; i < NelemPerThread; ++i) {
                c[i] = _ArithmeticType::compute(a[i], b[i]);
            }
        }
    }
};

template <typename _ArithmeticType, typename _In0Shape, typename _In1Shape>
struct Arithmetic<_ArithmeticType, _In0Shape, _In1Shape, float, 2>
{
    using DataType = float;
    static const int NelemPerThread = 2;

    static DEVICE void compute(float *c, const float *a, const float *b)
    {
        float2 *pc = (float2 *)c;
        if (_In0Shape::W == 1) {
            float2 *pb = (float2 *)b;
            pc->x = _ArithmeticType::compute(*a, pb->x);
            pc->y = _ArithmeticType::compute(*a, pb->y);
        } else if (_In1Shape::W == 1) {
            float2 *pa = (float2 *)a;
            pc->x = _ArithmeticType::compute(pa->x, *b);
            pc->y = _ArithmeticType::compute(pa->y, *b);
        } else {
            float2 *pa = (float2 *)a;
            float2 *pb = (float2 *)b;
            pc->x = _ArithmeticType::compute(pa->x, pb->x);
            pc->y = _ArithmeticType::compute(pa->y, pb->y);
        }
    }
};

template <typename _ArithmeticType, typename _In0Shape, typename _In1Shape>
struct Arithmetic<_ArithmeticType, _In0Shape, _In1Shape, float, 4>
{
    using DataType = float;
    static const int NelemPerThread = 4;

    static DEVICE void compute(float *c, const float *a, const float *b)
    {
        if (_In0Shape::W == 1) {
            longlong2 reg_b;
            longlong2 reg_c;
            asm volatile("ld.global.v2.u64 {%0,%1}, [%2];"
                         : "=l"(reg_b.x), "=l"(reg_b.y)
                         : "l"(b)
                         : "memory");
            float4 *pb = (float4 *)&reg_b;
            float4 *pc = (float4 *)&reg_c;
            float v = *a;
            pc->w = _ArithmeticType::compute(v, pb->w);
            pc->x = _ArithmeticType::compute(v, pb->x);
            pc->y = _ArithmeticType::compute(v, pb->y);
            pc->z = _ArithmeticType::compute(v, pb->z);
            asm volatile("st.global.v2.u64 [%0], {%1,%2};"
                         :
                         : "l"(c), "l"(reg_c.x), "l"(reg_c.y)
                         : "memory");
        } else if (_In1Shape::W == 1) {
            longlong2 reg_a;
            longlong2 reg_c;
            asm volatile("ld.global.v2.u64 {%0,%1}, [%2];"
                         : "=l"(reg_a.x), "=l"(reg_a.y)
                         : "l"(a)
                         : "memory");
            float4 *pa = (float4 *)&reg_a;
            float4 *pc = (float4 *)&reg_c;
            float v = *b;
            pc->w = _ArithmeticType::compute(pa->w, v);
            pc->x = _ArithmeticType::compute(pa->x, v);
            pc->y = _ArithmeticType::compute(pa->y, v);
            pc->z = _ArithmeticType::compute(pa->z, v);
            asm volatile("st.global.v2.u64 [%0], {%1,%2};"
                         :
                         : "l"(c), "l"(reg_c.x), "l"(reg_c.y)
                         : "memory");
        } else {
            longlong2 reg_a;
            longlong2 reg_b;
            longlong2 reg_c;
            asm volatile("ld.global.v2.u64 {%0,%1}, [%2];"
                         : "=l"(reg_a.x), "=l"(reg_a.y)
                         : "l"(a)
                         : "memory");
            asm volatile("ld.global.v2.u64 {%0,%1}, [%2];"
                         : "=l"(reg_b.x), "=l"(reg_b.y)
                         : "l"(b)
                         : "memory");
            float4 *pa = (float4 *)&reg_a;
            float4 *pb = (float4 *)&reg_b;
            float4 *pc = (float4 *)&reg_c;
            pc->w = _ArithmeticType::compute(pa->w, pb->w);
            pc->x = _ArithmeticType::compute(pa->x, pb->x);
            pc->y = _ArithmeticType::compute(pa->y, pb->y);
            pc->z = _ArithmeticType::compute(pa->z, pb->z);
            asm volatile("st.global.v2.u64 [%0], {%1,%2};"
                         :
                         : "l"(c), "l"(reg_c.x), "l"(reg_c.y)
                         : "memory");
        }
    }
};

template <typename _ArithmeticType, typename _In0Shape, typename _In1Shape>
struct Arithmetic<_ArithmeticType, _In0Shape, _In1Shape, half, 2>
{
    using DataType = half;
    static const int NelemPerThread = 2;

    static DEVICE void compute(half *c, const half *a, const half *b)
    {
        __half2 *pc = (__half2 *)c;
        if (_In0Shape::W == 1) {
            __half2 *pb = (__half2 *)b;
            *pc =
                _ArithmeticType::compute(__half2half2(*(const __half *)a), *pb);
        } else if (_In1Shape::W == 1) {
            __half2 *pa = (__half2 *)a;
            *pc =
                _ArithmeticType::compute(*pa, __half2half2(*(const __half *)b));
        } else {
            __half2 *pa = (__half2 *)a;
            __half2 *pb = (__half2 *)b;
            *pc = _ArithmeticType::compute(*pa, *pb);
        }
    }
};

template <typename _ArithmeticType, typename _In0Shape, typename _In1Shape>
struct Arithmetic<_ArithmeticType, _In0Shape, _In1Shape, half, 4>
{
    using DataType = half;
    static const int NelemPerThread = 4;

    static DEVICE void compute(half *c, const half *a, const half *b)
    {
        if (_In0Shape::W == 1) {
            uint64_t reg_b = *(uint64_t *)b;
            uint64_t reg_c;
            __half2 *pb = (__half2 *)&reg_b;
            __half2 *pc = (__half2 *)&reg_c;
            __half2 v = __half2half2(*(const __half *)a);
            pc[0] = _ArithmeticType::compute(v, pb[0]);
            pc[1] = _ArithmeticType::compute(v, pb[1]);
            *(uint64_t *)c = reg_c;
        } else if (_In1Shape::W == 1) {
            uint64_t reg_a = *(uint64_t *)a;
            uint64_t reg_c;
            __half2 *pa = (__half2 *)&reg_a;
            __half2 *pc = (__half2 *)&reg_c;
            __half2 v = __half2half2(*(const __half *)b);
            pc[0] = _ArithmeticType::compute(pa[0], v);
            pc[1] = _ArithmeticType::compute(pa[1], v);
            *(uint64_t *)c = reg_c;
        } else {
            uint64_t reg_a = *(uint64_t *)a;
            uint64_t reg_b = *(uint64_t *)b;
            uint64_t reg_c;
            __half2 *pa = (__half2 *)&reg_a;
            __half2 *pb = (__half2 *)&reg_b;
            __half2 *pc = (__half2 *)&reg_c;
            pc[0] = _ArithmeticType::compute(pa[0], pb[0]);
            pc[1] = _ArithmeticType::compute(pa[1], pb[1]);
            *(uint64_t *)c = reg_c;
        }
    }
};

template <typename _ArithmeticType, typename _In0Shape, typename _In1Shape>
struct Arithmetic<_ArithmeticType, _In0Shape, _In1Shape, half, 8>
{
    using DataType = half;
    static const int NelemPerThread = 8;

    static DEVICE void compute(half *c, const half *a, const half *b)
    {
        if (_In0Shape::W == 1) {
            longlong2 reg_b;
            longlong2 reg_c;
            asm volatile("ld.global.v2.u64 {%0,%1}, [%2];"
                         : "=l"(reg_b.x), "=l"(reg_b.y)
                         : "l"(b)
                         : "memory");
            __half2 *pb = (__half2 *)&reg_b;
            __half2 *pc = (__half2 *)&reg_c;
            __half2 v = __half2half2(*(const __half *)a);
            pc[0] = _ArithmeticType::compute(v, pb[0]);
            pc[1] = _ArithmeticType::compute(v, pb[1]);
            pc[2] = _ArithmeticType::compute(v, pb[2]);
            pc[3] = _ArithmeticType::compute(v, pb[3]);
            asm volatile("st.global.v2.u64 [%0], {%1,%2};"
                         :
                         : "l"(c), "l"(reg_c.x), "l"(reg_c.y)
                         : "memory");
        } else if (_In1Shape::W == 1) {
            longlong2 reg_a;
            longlong2 reg_c;
            asm volatile("ld.global.v2.u64 {%0,%1}, [%2];"
                         : "=l"(reg_a.x), "=l"(reg_a.y)
                         : "l"(a)
                         : "memory");
            __half2 *pa = (__half2 *)&reg_a;
            __half2 *pc = (__half2 *)&reg_c;
            __half2 v = __half2half2(*(const __half *)b);
            pc[0] = _ArithmeticType::compute(pa[0], v);
            pc[1] = _ArithmeticType::compute(pa[1], v);
            pc[2] = _ArithmeticType::compute(pa[2], v);
            pc[3] = _ArithmeticType::compute(pa[3], v);
            asm volatile("st.global.v2.u64 [%0], {%1,%2};"
                         :
                         : "l"(c), "l"(reg_c.x), "l"(reg_c.y)
                         : "memory");
        } else {
            longlong2 reg_a;
            longlong2 reg_b;
            longlong2 reg_c;
            asm volatile("ld.global.v2.u64 {%0,%1}, [%2];"
                         : "=l"(reg_a.x), "=l"(reg_a.y)
                         : "l"(a)
                         : "memory");
            asm volatile("ld.global.v2.u64 {%0,%1}, [%2];"
                         : "=l"(reg_b.x), "=l"(reg_b.y)
                         : "l"(b)
                         : "memory");
            __half2 *pa = (__half2 *)&reg_a;
            __half2 *pb = (__half2 *)&reg_b;
            __half2 *pc = (__half2 *)&reg_c;
            pc[0] = _ArithmeticType::compute(pa[0], pb[0]);
            pc[1] = _ArithmeticType::compute(pa[1], pb[1]);
            pc[2] = _ArithmeticType::compute(pa[2], pb[2]);
            pc[3] = _ArithmeticType::compute(pa[3], pb[3]);
            asm volatile("st.global.v2.u64 [%0], {%1,%2};"
                         :
                         : "l"(c), "l"(reg_c.x), "l"(reg_c.y)
                         : "memory");
        }
    }
};

template <typename In0Dims, typename In0Shape, typename In1Dims,
          typename In1Shape, typename OutDims, typename OutShape,
          typename UnitOutDims, int NumThreads, int SmemBytes>
DEVICE void add(float *c, const float *a, const float *b, int uop_idx, int)
{
    constexpr int NelemPerThread = (UnitOutDims::W % 4 == 0)   ? 4
                                   : (UnitOutDims::W % 2 == 0) ? 2
                                                               : 1;
    Broadcast2<In0Dims, In0Shape, In1Dims, In1Shape, OutDims, OutShape,
               UnitOutDims, NumThreads, SmemBytes,
               Arithmetic<Add, In0Shape, In1Shape, float,
                          NelemPerThread>>::run(c, a, b, uop_idx);
}

template <typename In0Dims, typename In0Shape, typename In1Dims,
          typename In1Shape, typename OutDims, typename OutShape,
          typename UnitOutDims, int NumThreads, int SmemBytes>
DEVICE void add(half *c, const half *a, const half *b, int uop_idx, int)
{
    constexpr int NelemPerThread = (UnitOutDims::W % 8 == 0)   ? 8
                                   : (UnitOutDims::W % 4 == 0) ? 4
                                   : (UnitOutDims::W % 2 == 0) ? 2
                                                               : 1;
    Broadcast2<In0Dims, In0Shape, In1Dims, In1Shape, OutDims, OutShape,
               UnitOutDims, NumThreads, SmemBytes,
               Arithmetic<Add, In0Shape, In1Shape, half,
                          NelemPerThread>>::run(c, a, b, uop_idx);
}

template <typename In0Dims, typename In0Shape, typename In1Dims,
          typename In1Shape, typename OutDims, typename OutShape,
          typename UnitOutDims, int NumThreads, int SmemBytes>
DEVICE void mul(float *c, float *a, float *b, int uop_idx, int)
{
    constexpr int NelemPerThread = (UnitOutDims::W % 4 == 0)   ? 4
                                   : (UnitOutDims::W % 2 == 0) ? 2
                                                               : 1;
    Broadcast2<In0Dims, In0Shape, In1Dims, In1Shape, OutDims, OutShape,
               UnitOutDims, NumThreads, SmemBytes,
               Arithmetic<Mul, In0Shape, In1Shape, float,
                          NelemPerThread>>::run(c, a, b, uop_idx);
}

template <typename In0Dims, typename In0Shape, typename In1Dims,
          typename In1Shape, typename OutDims, typename OutShape,
          typename UnitOutDims, int NumThreads, int SmemBytes>
DEVICE void mul(half *c, half *a, half *b, int uop_idx, int)
{
    constexpr int NelemPerThread = (UnitOutDims::W % 8 == 0)   ? 8
                                   : (UnitOutDims::W % 4 == 0) ? 4
                                   : (UnitOutDims::W % 2 == 0) ? 2
                                                               : 1;
    Broadcast2<In0Dims, In0Shape, In1Dims, In1Shape, OutDims, OutShape,
               UnitOutDims, NumThreads, SmemBytes,
               Arithmetic<Mul, In0Shape, In1Shape, half,
                          NelemPerThread>>::run(c, a, b, uop_idx);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes>
DEVICE void scale(half *y, half *x, float val, int uop_idx, int)
{
    constexpr int NelemPerThread = (UnitOutDims::W % 8 == 0)   ? 8
                                   : (UnitOutDims::W % 4 == 0) ? 4
                                   : (UnitOutDims::W % 2 == 0) ? 2
                                                               : 1;
    half val_h(val);
    using ValDims = Vec<1, 1, 1, 1>;
    using ValShape = Vec<1, 1, 1, 1>;
    Broadcast2<
        InDims, InShape, ValDims, ValShape, OutDims, OutShape, UnitOutDims,
        NumThreads, SmemBytes,
        Arithmetic<Mul, InShape, ValShape, half, NelemPerThread>>::run(y, x,
                                                                       &val_h,
                                                                       uop_idx);
}

} // namespace ark

#endif // ARK_KERNELS_ARITHMETIC_H_

```

`ark/include/kernels/ark_kernels.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifdef __CUDACC__
#ifndef ARK_KERNELS_H_
#define ARK_KERNELS_H_

// clang-format off
#include "common.h"
// clang-format on

#include "activation.h"
#include "arithmetic.h"
#include "comm.h"
#include "comm_mm.h"
#include "im2col.h"
#include "layernorm.h"
#include "matmul.h"
#include "reduce.h"
#include "softmax.h"
#include "transpose.h"

#endif // ARK_KERNELS_H_
#endif // __CUDACC__

```

`ark/include/kernels/broadcast.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_BROADCAST_H_
#define ARK_KERNELS_BROADCAST_H_

#include "common.h"

namespace ark {

// Static checker if InShape can be broadcasted into OutShape.
template <typename InShape, typename OutShape> struct BroadcastShapeChecker1
{
    static_assert(InShape::N == 1 || OutShape::N == 1 ||
                      InShape::N == OutShape::N,
                  "Cannot broadcast dimension N of the input");
    static_assert(InShape::C == 1 || OutShape::C == 1 ||
                      InShape::C == OutShape::C,
                  "Cannot broadcast dimension C of the input");
    static_assert(InShape::H == 1 || OutShape::H == 1 ||
                      InShape::H == OutShape::H,
                  "Cannot broadcast dimension H of the input");
    static_assert(InShape::W == 1 || OutShape::W == 1 ||
                      InShape::W == OutShape::W,
                  "Cannot broadcast dimension W of the input");

    // Derived OutShape.
    using DerOutShape = OutShape;
};

// Static checker if In0Shape and In1Shape can be broadcasted into OutShape.
template <typename In0Shape, typename In1Shape, typename OutShape>
struct BroadcastShapeChecker2
{
    static_assert(In0Shape::N == 1 || In1Shape::N == 1 ||
                      In0Shape::N == In1Shape::N,
                  "Cannot broadcast dimension N of inputs");
    static_assert(In0Shape::C == 1 || In1Shape::C == 1 ||
                      In0Shape::C == In1Shape::C,
                  "Cannot broadcast dimension C of inputs");
    static_assert(In0Shape::H == 1 || In1Shape::H == 1 ||
                      In0Shape::H == In1Shape::H,
                  "Cannot broadcast dimension H of inputs");
    static_assert(In0Shape::W == 1 || In1Shape::W == 1 ||
                      In0Shape::W == In1Shape::W,
                  "Cannot broadcast dimension W of inputs");

    // Derived OutShape.
    using DerOutShape = Vec<math::max<In0Shape::N, In1Shape::N>::value,
                            math::max<In0Shape::C, In1Shape::C>::value,
                            math::max<In0Shape::H, In1Shape::H>::value,
                            math::max<In0Shape::W, In1Shape::W>::value>;
    static_assert(
        DerOutShape::N == OutShape::N,
        "Dimension N of output is not as expected from broadcast rules");
    static_assert(
        DerOutShape::C == OutShape::C,
        "Dimension C of output is not as expected from broadcast rules");
    static_assert(
        DerOutShape::H == OutShape::H,
        "Dimension H of output is not as expected from broadcast rules");
    static_assert(
        DerOutShape::W == OutShape::W,
        "Dimension W of output is not as expected from broadcast rules");
};

// Broadcast a unit operator. Follows NumPy-style broadcasting:
// https://numpy.org/doc/stable/user/basics.broadcasting.html
template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, typename CompType>
struct Broadcast1
{
    using UnitOp =
        UnitOp<OutDims, OutShape, UnitOutDims, NumThreads, SmemBytes>;
    using DataType = typename CompType::DataType;
    static const int NelemPerThread = CompType::NelemPerThread;

    static_assert(NelemPerThread > 0, "NelemPerThread must be positive");
    static_assert(UnitOutDims::W % NelemPerThread == 0,
                  "UnitOutDims::W must be divisible by NelemPerThread");

    /// Conduct computation on one input and broadcast the result to output.
    /// @param out Output data.
    /// @param in1 Input data.
    /// @param uop_idx Index of the unit operator.
    static DEVICE void run(DataType *out, const DataType *in, int uop_idx)
    {
        using InOutChk = BroadcastShapeChecker1<InShape, OutShape>;

        int un = UnitOp::uop_idx_n(uop_idx);
        int uc = UnitOp::uop_idx_c(uop_idx);
        int uh = UnitOp::uop_idx_h(uop_idx);
        int uw = UnitOp::uop_idx_w(uop_idx);

        for (int tid = UnitOp::thread_id();; tid += NumThreads) {
            int tid_w = (tid * NelemPerThread) % UnitOutDims::W;
            int tid_h =
                ((tid * NelemPerThread) / UnitOutDims::W) % UnitOutDims::H;
            int tid_c =
                ((tid * NelemPerThread) / UnitOutDims::HW) % UnitOutDims::C;
            int tid_n = (tid * NelemPerThread) / UnitOutDims::CHW;

            if (tid_n >= UnitOutDims::N) {
                break;
            }

            int idx_out = (tid_w + uw * UnitOutDims::W) +
                          (tid_h + uh * UnitOutDims::H) * OutDims::W +
                          (tid_c + uc * UnitOutDims::C) * OutDims::HW +
                          (tid_n + un * UnitOutDims::N) * OutDims::CHW;

            int idx_in;

            if constexpr (VecIsEq<InShape, OutShape>::value) {
                idx_in = idx_out;
            } else {
                idx_in =
                    ((InShape::W == 1) ? 0 : (tid_w + uw * UnitOutDims::W)) +
                    ((InShape::H == 1) ? 0 : (tid_h + uh * UnitOutDims::H)) *
                        InDims::W +
                    ((InShape::C == 1) ? 0 : (tid_c + uc * UnitOutDims::C)) *
                        InDims::HW +
                    ((InShape::N == 1) ? 0 : (tid_n + un * UnitOutDims::N)) *
                        InDims::CHW;
            }

            CompType::compute(&out[idx_out], &in[idx_in]);
        }
    }
};

// Broadcast a unit operator. Follows NumPy-style broadcasting:
// https://numpy.org/doc/stable/user/basics.broadcasting.html
template <typename In0Dims, typename In0Shape, typename In1Dims,
          typename In1Shape, typename OutDims, typename OutShape,
          typename UnitOutDims, int NumThreads, int SmemBytes,
          typename CompType>
struct Broadcast2
{
    using UnitOp =
        UnitOp<OutDims, OutShape, UnitOutDims, NumThreads, SmemBytes>;
    using DataType = typename CompType::DataType;
    static const int NelemPerThread = CompType::NelemPerThread;

    static_assert(NelemPerThread > 0, "NelemPerThread must be positive");
    static_assert(UnitOutDims::W % NelemPerThread == 0,
                  "UnitOutDims::W must be divisible by NelemPerThread");

    /// Conduct computation on two inputs and broadcast the result to output.
    /// @param out Output data.
    /// @param in0 Input data 0.
    /// @param in1 Input data 1.
    /// @param uop_idx Index of the unit operator.
    static DEVICE void run(DataType *out, const DataType *in0,
                           const DataType *in1, int uop_idx)
    {
        using InOutChk = BroadcastShapeChecker2<In0Shape, In1Shape, OutShape>;

        int un = UnitOp::uop_idx_n(uop_idx);
        int uc = UnitOp::uop_idx_c(uop_idx);
        int uh = UnitOp::uop_idx_h(uop_idx);
        int uw = UnitOp::uop_idx_w(uop_idx);

        for (int tid = UnitOp::thread_id();; tid += NumThreads) {
            int tid_w = (tid * NelemPerThread) % UnitOutDims::W;
            int tid_h =
                ((tid * NelemPerThread) / UnitOutDims::W) % UnitOutDims::H;
            int tid_c =
                ((tid * NelemPerThread) / UnitOutDims::HW) % UnitOutDims::C;
            int tid_n = (tid * NelemPerThread) / UnitOutDims::CHW;

            if (tid_n >= UnitOutDims::N) {
                break;
            }

            int idx_out = (tid_w + uw * UnitOutDims::W) +
                          (tid_h + uh * UnitOutDims::H) * OutDims::W +
                          (tid_c + uc * UnitOutDims::C) * OutDims::HW +
                          (tid_n + un * UnitOutDims::N) * OutDims::CHW;

            int idx_in0;
            int idx_in1;

            if constexpr (VecIsEq<In0Shape, OutShape>::value) {
                idx_in0 = idx_out;
            } else {
                idx_in0 =
                    ((In0Shape::W == 1) ? 0 : (tid_w + uw * UnitOutDims::W)) +
                    ((In0Shape::H == 1) ? 0 : (tid_h + uh * UnitOutDims::H)) *
                        In0Dims::W +
                    ((In0Shape::C == 1) ? 0 : (tid_c + uc * UnitOutDims::C)) *
                        In0Dims::HW +
                    ((In0Shape::N == 1) ? 0 : (tid_n + un * UnitOutDims::N)) *
                        In0Dims::CHW;
            }

            if constexpr (VecIsEq<In1Shape, OutShape>::value) {
                idx_in1 = idx_out;
            } else if constexpr (VecIsEq<In1Shape, In0Shape>::value) {
                idx_in1 = idx_in0;
            } else {
                idx_in1 =
                    ((In1Shape::W == 1) ? 0 : (tid_w + uw * UnitOutDims::W)) +
                    ((In1Shape::H == 1) ? 0 : (tid_h + uh * UnitOutDims::H)) *
                        In1Dims::W +
                    ((In1Shape::C == 1) ? 0 : (tid_c + uc * UnitOutDims::C)) *
                        In1Dims::HW +
                    ((In1Shape::N == 1) ? 0 : (tid_n + un * UnitOutDims::N)) *
                        In1Dims::CHW;
            }

            CompType::compute(&out[idx_out], &in0[idx_in0], &in1[idx_in1]);
        }
    }
};

} // namespace ark

#endif // ARK_KERNELS_BROADCAST_H_

```

`ark/include/kernels/comm.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_COMM_H_
#define ARK_KERNELS_COMM_H_

#include "common.h"

namespace ark {
namespace comm {

/// Request types.
struct ReqType
{
    static const unsigned int Send = 0;
    static const unsigned int Recv = 1;
};

/// Constructs a 64-bit Request to be sent to the proxy.
/// @tparam ReqType Request type.
template <unsigned int ReqType, unsigned int DstSid, unsigned int SrcSid,
          unsigned int DstRank, unsigned long long int Length>
struct Request
{
    static const unsigned long long int value =
        ((Length & 0x3ffffffff) << 25) + ((DstRank & 0x7f) << 18) +
        ((SrcSid & 0xff) << 10) + ((DstSid & 0xff) << 2) + (ReqType & 0x3);
};

#if (ARK_COMM_SW != 0)
__device__ int _ARK_COMM_SW_SEND_LOCK = 0;
#endif // (ARK_COMM_SW != 0)

// Send a Request to the proxy.
template <unsigned int Rank, unsigned int DstRank, unsigned int SrcSid,
          unsigned int DstSid, unsigned long long int Length>
DEVICE void send(int, int)
{
    using UnitOp = UnitOp<ark::Vec<>, ark::Vec<>, ark::Vec<>, 32, 0>;
    if (UnitOp::thread_id() != 0) {
        return;
    }
    volatile unsigned int *done = &(_ARK_SC[SrcSid]);
    while (!(*done)) {
    }
    *done = 0;
    constexpr unsigned long long int dbval =
        Request<ReqType::Send, DstSid, SrcSid, DstRank, Length>::value;
#if (ARK_COMM_SW != 0)
#if 1
    constexpr unsigned long long int invalid = (unsigned long long int)-1;
    while (atomicCAS(&_ARK_COMM_SW_SEND_LOCK, 0, 1) != 0) {
    }
    while (*_ARK_REQUEST != invalid) {
    }
    *_ARK_REQUEST = dbval;
    atomicExch(&_ARK_COMM_SW_SEND_LOCK, 0);
#else
    // This version is slower than the above one.
    constexpr unsigned long long int invalid = (unsigned long long int)-1;
    for (;;) {
        while (*_ARK_REQUEST != invalid) {
        }
        if (atomicCAS((unsigned long long int *)_ARK_REQUEST,
                      (unsigned long long int)invalid,
                      (unsigned long long int)dbval) == invalid) {
            break;
        }
    }
#endif // 1
#else
    *_ARK_REQUEST = dbval;
#endif // (ARK_COMM_SW != 0)
}

// Poll SC and reset.
template <unsigned int Rank, unsigned int DstRank, unsigned int SrcSid>
DEVICE void send_done(int, int)
{
    using UnitOp = UnitOp<ark::Vec<>, ark::Vec<>, ark::Vec<>, 32, 0>;
    if (UnitOp::thread_id() != 0) {
        return;
    }
    volatile unsigned int *done = &(_ARK_SC[SrcSid]);
    while (!(*done)) {
    }
    *done = 0;
}

//
template <unsigned int Rank, unsigned int SrcRank, unsigned int DstSid>
DEVICE void recv(int, int)
{
    using UnitOp = UnitOp<ark::Vec<>, ark::Vec<>, ark::Vec<>, 32, 0>;
    if (UnitOp::thread_id() != 0) {
        return;
    }
    volatile unsigned int *len = &(_ARK_RC[DstSid]);
    while (!(*len)) {
    }
    *len = 0;
}

} // namespace comm
} // namespace ark

#endif // ARK_KERNELS_COMM_H_

```

`ark/include/kernels/comm_mm.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_COMM_MM_H_
#define ARK_COMM_MM_H_

#include "common.h"

namespace ark {
namespace comm {

union alignas(16) DataPacketLL {
    // on 64bits machine, the access to the 64bits data is atomic, so we combine
    // the 32bits data and flag into one 64bits data to insure the GPU receive
    // complete data
    struct
    {
        uint32_t data1;
        uint32_t flag1;
        uint32_t data2;
        uint32_t flag2;
    };
    uint64_t v[2];
    int4 i4;
};

// the tensor's data is float16 data,so the matrix size is TDM * TDN *
// sizeof(float16). the data is stored in column major, and LDM is the
// major dimension. The TN is the thread number, TDM and TDN is the tile
// size.
template <int FLAG> DEVICE uint64_t readLL(DataPacketLL *recv_buf)
{
    uint32_t data1, flag1, data2, flag2;
    do {
        asm volatile("ld.volatile.global.v4.u32 {%0,%1,%2,%3}, [%4];"
                     : "=r"(data1), "=r"(flag1), "=r"(data2), "=r"(flag2)
                     : "l"(&recv_buf->i4));
    } while ((flag1 != FLAG) || (flag2 != FLAG));
    uint64_t val64 = data1 + (((uint64_t)data2) << 32);
    return val64;
}
template <int FLAG> DEVICE void storeLL(DataPacketLL *data_dst, uint64_t val)
{
    asm volatile(
        "st.volatile.global.v4.u32 [%0], {%1,%2,%3,%4};" ::"l"(&data_dst->i4),
        "r"((uint32_t)val), "r"(FLAG), "r"((uint32_t)(val >> 32)), "r"(FLAG));
}

template <int TN>
DEVICE void pre_send_mm_op(volatile int *send_ready_flag, int uop_idx)
{
    if (threadIdx.x % TN == 0) {
        while (send_ready_flag[uop_idx] != 0) {
        }
        send_ready_flag[uop_idx] = 1;
    }
    sync_warps<TN>();
}

template <int TN>
DEVICE void post_recv_mm_op(volatile int *send_ready_flag, int uop_idx)
{
    sync_warps<TN>();
    if (threadIdx.x % TN == 0) {
        // reset the send_ready_flag to 0
        send_ready_flag[uop_idx] = 0;
    }
    __syncwarp();
}

template <int LDM, int LDN, int TN, int SmemBytes, int TDM, int TDN,
          int FLAG = 1>
// send a tile of the tensor from data_src to recv_buff
DEVICE void sendLL(void *recv_buff, ark::half *data_src,
                   volatile int *send_ready_flag, int uop_idx, int)
{
    using UnitOp = UnitOp<Vec<1, 1, LDN, LDM>, Vec<1, 1, LDN, LDM>,
                          Vec<1, 1, TDN, TDM>, TN, SmemBytes>;

    DataPacketLL *recv_buff_ptr = reinterpret_cast<DataPacketLL *>(recv_buff);

    // elementwise copy, a thread copies 4 float16 data (64 bits)
    // in a loop, so the ElePerLoop is 4
    constexpr int ElePerLoop = 4;

    constexpr int MNumPerLoop = math::min<TDM, ElePerLoop * TN>::value;
    constexpr int NNumPerLoop = math::max<1, ElePerLoop * TN / TDM>::value;

    constexpr int IterMNum = math::div_up<TDM, MNumPerLoop>::value;
    constexpr int IterNNum = TDN / NNumPerLoop;

    int t0 = UnitOp::uop_idx_w(uop_idx);
    int t1 = UnitOp::uop_idx_h(uop_idx);
    int midx = TDM * t0 + math::mod<TDM>(ElePerLoop * UnitOp::thread_id());
    int nidx = TDN * t1 + math::div<TDM>(ElePerLoop * UnitOp::thread_id());
    pre_send_mm_op<TN>(send_ready_flag, uop_idx);
#pragma unroll
    for (int i = 0; i < IterNNum; ++i) {
#pragma unroll
        for (int j = 0; j < IterMNum; ++j) {
            int idx = midx + j * MNumPerLoop + (nidx + i * NNumPerLoop) * LDM;
            // in ARK the src and dst store the float16 data, so a
            // thread copied 4 float16 (64 bits) data in a loop
            storeLL<FLAG>(recv_buff_ptr + math::div<4>(idx),
                          *(uint64_t *)&((ark::half *)data_src)[idx]);
        }
    }
}

// recv a tile of the tensor from recv_buff to data_dst
template <int LDM, int LDN, int TN, int SmemBytes, int TDM, int TDN,
          int FLAG = 1>
DEVICE void recvLL(void *recv_buff, ark::half *data_dst,
                   volatile int *send_ready_flag, int uop_idx, int)
{
    using UnitOp = UnitOp<Vec<1, 1, LDN, LDM>, Vec<1, 1, LDN, LDM>,
                          Vec<1, 1, TDN, TDM>, TN, SmemBytes>;

    DataPacketLL *recv_buff_ptr = reinterpret_cast<DataPacketLL *>(recv_buff);

    // elementwise copy, a thread copies 4 float16 data (64 bits)
    // in a loop, so the ElePerLoop is 4
    constexpr int ElePerLoop = 4;

    constexpr int MNumPerLoop = math::min<TDM, ElePerLoop * TN>::value;
    constexpr int NNumPerLoop = math::max<1, ElePerLoop * TN / TDM>::value;

    constexpr int IterMNum = math::div_up<TDM, MNumPerLoop>::value;
    constexpr int IterNNum = TDN / NNumPerLoop;

    int t0 = UnitOp::uop_idx_w(uop_idx);
    int t1 = UnitOp::uop_idx_h(uop_idx);
    int midx = TDM * t0 + math::mod<TDM>(ElePerLoop * UnitOp::thread_id());
    int nidx = TDN * t1 + math::div<TDM>(ElePerLoop * UnitOp::thread_id());
#pragma unroll
    for (int i = 0; i < IterNNum; ++i) {
#pragma unroll
        for (int j = 0; j < IterMNum; ++j) {
            int idx = midx + j * MNumPerLoop + (nidx + i * NNumPerLoop) * LDM;
            // in ARK the src and dst store the float16 data, so a
            // thread copied 4 float16 (64 bits) data in a loop
            *(uint64_t *)&(((ark::half *)data_dst)[idx]) =
                readLL<FLAG>(recv_buff_ptr + math::div<4>(idx));
            (recv_buff_ptr + math::div<4>(idx))->i4 = make_int4(0, 0, 0, 0);
        }
    }
    post_recv_mm_op<TN>(send_ready_flag, uop_idx);
}

} // namespace comm
} // namespace ark

#endif // ARK_COMM_MM_H_

```

`ark/include/kernels/common.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_COMMON_H_
#define ARK_KERNELS_COMMON_H_

#include "arch.h"
#include "device.h"
#include "half.h"
#include "platform.h"
#include "smem.h"
#include "static_math.h"
#include "sync.h"
#include "unit_op.h"
#include "vec.h"

#endif // ARK_KERNELS_COMMON_H_

```

`ark/include/kernels/device.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_DEVICE_H_
#define ARK_KERNELS_DEVICE_H_

#if defined(__NVCC__) || (defined(__clang__) && defined(__CUDA__)) ||          \
    defined(__CUDACC_RTC__)
#define DEVICE __forceinline__ __device__
#else
#define DEVICE inline
#endif

#endif // ARK_KERNELS_DEVICE_H_

```

`ark/include/kernels/ewise.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_EWISE_H_
#define ARK_KERNELS_EWISE_H_

#include "common.h"

namespace ark {

/// Element-wise computation operator with a single input.
template <typename OutDims, typename OutShape, typename UnitOutDims,
          int NumThreads, int SmemBytes, typename CompType>
struct Ewise1
{
    using UnitOp =
        UnitOp<OutDims, OutShape, UnitOutDims, NumThreads, SmemBytes>;
    using DataType = typename CompType::DataType;
    static const int NelemPerThread = CompType::NelemPerThread;

    static_assert(NelemPerThread > 0, "NelemPerThread must be positive");
    static_assert(UnitOutDims::W % NelemPerThread == 0,
                  "UnitOutDims::W must be divisible by NelemPerThread");

    /// Conduct element-wise computation on input and write the result on
    /// output.
    /// @param out Output data.
    /// @param in Input data.
    /// @param uop_idx Index of the unit operator.
    static DEVICE void run(DataType *out, DataType *in, int uop_idx)
    {
        int un = UnitOp::uop_idx_n(uop_idx);
        int uc = UnitOp::uop_idx_c(uop_idx);
        int uh = UnitOp::uop_idx_h(uop_idx);
        int uw = UnitOp::uop_idx_w(uop_idx);

        for (int tid = UnitOp::thread_id();; tid += NumThreads) {
            int tid_w = (tid * NelemPerThread) % UnitOutDims::W;
            int tid_h =
                ((tid * NelemPerThread) / UnitOutDims::W) % UnitOutDims::H;
            int tid_c =
                ((tid * NelemPerThread) / UnitOutDims::HW) % UnitOutDims::C;
            int tid_n = (tid * NelemPerThread) / UnitOutDims::CHW;

            if (tid_n >= UnitOutDims::N) {
                break;
            }

            CompType::compute(out, in, tid_n + un * UnitOutDims::N,
                              tid_c + uc * UnitOutDims::C,
                              tid_h + uh * UnitOutDims::H,
                              tid_w + uw * UnitOutDims::W);
        }
    }
};

} // namespace ark

#endif // ARK_KERNELS_EWISE_H_

```

`ark/include/kernels/gemm.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_GEMM_H_
#define ARK_KERNELS_GEMM_H_

#include "cutlass/gemm/device/default_gemm_configuration.h"
#include "cutlass/gemm/gemm.h"
#include "cutlass/gemm/kernel/default_gemm.h"

#include "common.h"

namespace ark {

template <typename OperatorClass, typename ArchTag, typename ElementA,
          typename ElementB, typename ElementC, typename ElementAccumulator,
          typename Shape>
struct GemmConfiguration;

template <typename ArchTag, typename ElementA, typename ElementB,
          typename ElementC, typename ElementAccumulator>
struct GemmConfiguration<cutlass::arch::OpClassSimt, ArchTag, ElementA,
                         ElementB, ElementC, ElementAccumulator,
                         cutlass::gemm::GemmShape<128, 128, 8>>
{
    static int const kAlignmentA = 1;
    static int const kAlignmentB = 1;

    using WarpShape = cutlass::gemm::GemmShape<32, 64, 8>;
    using InstructionShape = cutlass::gemm::GemmShape<1, 1, 1>;
    static int const kStages = 2;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombination<
        ElementC, 1, ElementAccumulator, ElementAccumulator>;

    using Operator = cutlass::arch::OpMultiplyAdd;
};

template <typename ArchTag, typename ElementC>
struct GemmConfiguration<cutlass::arch::OpClassSimt, ArchTag, int8_t, int8_t,
                         ElementC, int32_t,
                         cutlass::gemm::GemmShape<128, 128, 32>>
{
    static int const kAlignmentA = 4;
    static int const kAlignmentB = 4;

    using WarpShape = cutlass::gemm::GemmShape<32, 64, 32>;
    using InstructionShape = cutlass::gemm::GemmShape<1, 1, 4>;
    static int const kStages = 2;

    using EpilogueOutputOp =
        cutlass::epilogue::thread::LinearCombinationClamp<ElementC, 1, int32_t,
                                                          float>;

    using Operator = cutlass::arch::OpMultiplyAdd;
};

template <typename ElementA, typename ElementB, typename ElementC,
          typename ElementAccumulator>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm70,
                         ElementA, ElementB, ElementC, ElementAccumulator,
                         cutlass::gemm::GemmShape<64, 64, 32>>
{
    static int const kAlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value;
    static int const kAlignmentB = 128 / cutlass::sizeof_bits<ElementA>::value;

    using WarpShape = cutlass::gemm::GemmShape<32, 32, 32>;
    using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombination<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value,
        ElementAccumulator, ElementAccumulator>;

    using Operator = typename cutlass::platform::conditional<
        (cutlass::platform::is_same<ElementA, int8_t>::value ||
         cutlass::platform::is_same<ElementA, cutlass::int4b_t>::value ||
         cutlass::platform::is_same<ElementA, uint8_t>::value ||
         cutlass::platform::is_same<ElementA, cutlass::uint4b_t>::value),
        cutlass::arch::OpMultiplyAddSaturate,
        cutlass::arch::OpMultiplyAdd>::type;
};

template <typename ElementA, typename ElementB, typename ElementC,
          typename ElementAccumulator>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm70,
                         ElementA, ElementB, ElementC, ElementAccumulator,
                         cutlass::gemm::GemmShape<128, 64, 32>>
{
    static int const kAlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value;
    static int const kAlignmentB = 128 / cutlass::sizeof_bits<ElementA>::value;

    using WarpShape = cutlass::gemm::GemmShape<64, 32, 32>;
    using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombination<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value,
        ElementAccumulator, ElementAccumulator>;

    using Operator = typename cutlass::platform::conditional<
        (cutlass::platform::is_same<ElementA, int8_t>::value ||
         cutlass::platform::is_same<ElementA, cutlass::int4b_t>::value ||
         cutlass::platform::is_same<ElementA, uint8_t>::value ||
         cutlass::platform::is_same<ElementA, cutlass::uint4b_t>::value),
        cutlass::arch::OpMultiplyAddSaturate,
        cutlass::arch::OpMultiplyAdd>::type;
};

template <typename ElementA, typename ElementB, typename ElementC,
          typename ElementAccumulator>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm70,
                         ElementA, ElementB, ElementC, ElementAccumulator,
                         cutlass::gemm::GemmShape<64, 128, 32>>
{
    static int const kAlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value;
    static int const kAlignmentB = 128 / cutlass::sizeof_bits<ElementA>::value;

    using WarpShape = cutlass::gemm::GemmShape<32, 64, 32>;
    using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombination<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value,
        ElementAccumulator, ElementAccumulator>;

    using Operator = typename cutlass::platform::conditional<
        (cutlass::platform::is_same<ElementA, int8_t>::value ||
         cutlass::platform::is_same<ElementA, cutlass::int4b_t>::value ||
         cutlass::platform::is_same<ElementA, uint8_t>::value ||
         cutlass::platform::is_same<ElementA, cutlass::uint4b_t>::value),
        cutlass::arch::OpMultiplyAddSaturate,
        cutlass::arch::OpMultiplyAdd>::type;
};

template <typename ElementA, typename ElementB, typename ElementC,
          typename ElementAccumulator>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm70,
                         ElementA, ElementB, ElementC, ElementAccumulator,
                         cutlass::gemm::GemmShape<128, 128, 32>>
{
    static int const kAlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value;
    static int const kAlignmentB = 128 / cutlass::sizeof_bits<ElementA>::value;

    using WarpShape = cutlass::gemm::GemmShape<64, 64, 32>;
    using InstructionShape = cutlass::gemm::GemmShape<8, 8, 4>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombination<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value,
        ElementAccumulator, ElementAccumulator>;

    using Operator = typename cutlass::platform::conditional<
        (cutlass::platform::is_same<ElementA, int8_t>::value ||
         cutlass::platform::is_same<ElementA, cutlass::int4b_t>::value ||
         cutlass::platform::is_same<ElementA, uint8_t>::value ||
         cutlass::platform::is_same<ElementA, cutlass::uint4b_t>::value),
        cutlass::arch::OpMultiplyAddSaturate,
        cutlass::arch::OpMultiplyAdd>::type;
};

template <typename ElementA, typename ElementB, typename ElementC,
          typename ElementAccumulator>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
                         ElementA, ElementB, ElementC, ElementAccumulator,
                         cutlass::gemm::GemmShape<64, 64, 64>>
{
    static int const kAlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value;
    static int const kAlignmentB = 128 / cutlass::sizeof_bits<ElementA>::value;

    using WarpShape = cutlass::gemm::GemmShape<32, 32, 64>;
    using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombination<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value,
        ElementAccumulator, ElementAccumulator>;

    using Operator = typename cutlass::platform::conditional<
        (cutlass::platform::is_same<ElementA, int8_t>::value ||
         cutlass::platform::is_same<ElementA, cutlass::int4b_t>::value ||
         cutlass::platform::is_same<ElementA, uint8_t>::value ||
         cutlass::platform::is_same<ElementA, cutlass::uint4b_t>::value),
        cutlass::arch::OpMultiplyAddSaturate,
        cutlass::arch::OpMultiplyAdd>::type;
};

template <typename ElementA, typename ElementB, typename ElementC,
          typename ElementAccumulator>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
                         ElementA, ElementB, ElementC, ElementAccumulator,
                         cutlass::gemm::GemmShape<128, 128, 64>>
{
    static int const kAlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value;
    static int const kAlignmentB = 128 / cutlass::sizeof_bits<ElementA>::value;

    using WarpShape = cutlass::gemm::GemmShape<64, 32, 64>;
    using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombination<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value,
        ElementAccumulator, ElementAccumulator>;

    using Operator = typename cutlass::platform::conditional<
        (cutlass::platform::is_same<ElementA, int8_t>::value ||
         cutlass::platform::is_same<ElementA, cutlass::int4b_t>::value ||
         cutlass::platform::is_same<ElementA, uint8_t>::value ||
         cutlass::platform::is_same<ElementA, cutlass::uint4b_t>::value),
        cutlass::arch::OpMultiplyAddSaturate,
        cutlass::arch::OpMultiplyAdd>::type;
};

template <typename ElementA, typename ElementB, typename ElementC,
          typename ElementAccumulator>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
                         ElementA, ElementB, ElementC, ElementAccumulator,
                         cutlass::gemm::GemmShape<128, 256, 64>>
{
    static int const kAlignmentA = 128 / cutlass::sizeof_bits<ElementA>::value;
    static int const kAlignmentB = 128 / cutlass::sizeof_bits<ElementA>::value;

    using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
    using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombination<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value,
        ElementAccumulator, ElementAccumulator>;

    using Operator = typename cutlass::platform::conditional<
        (cutlass::platform::is_same<ElementA, int8_t>::value ||
         cutlass::platform::is_same<ElementA, cutlass::int4b_t>::value ||
         cutlass::platform::is_same<ElementA, uint8_t>::value ||
         cutlass::platform::is_same<ElementA, cutlass::uint4b_t>::value),
        cutlass::arch::OpMultiplyAddSaturate,
        cutlass::arch::OpMultiplyAdd>::type;
};

template <typename ElementC, typename ElementAccumulator>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
                         double, double, ElementC, ElementAccumulator,
                         cutlass::gemm::GemmShape<128, 256, 64>>
{
    static int const kAlignmentA = 1;
    static int const kAlignmentB = 1;

    using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
    using InstructionShape = cutlass::gemm::GemmShape<16, 8, 16>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombination<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value,
        ElementAccumulator, ElementAccumulator>;

    using Operator = cutlass::arch::OpMultiplyAdd;
};

template <typename ElementC>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
                         int8_t, int8_t, ElementC, int32_t,
                         cutlass::gemm::GemmShape<128, 256, 64>>
{
    static int const kAlignmentA = 128 / cutlass::sizeof_bits<int8_t>::value;
    static int const kAlignmentB = 128 / cutlass::sizeof_bits<int8_t>::value;

    using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
    using InstructionShape = cutlass::gemm::GemmShape<16, 8, 32>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombinationClamp<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value, int32_t, float>;

    using Operator = cutlass::arch::OpMultiplyAddSaturate;
};

template <typename ElementC>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
                         int8_t, uint8_t, ElementC, int32_t,
                         cutlass::gemm::GemmShape<128, 256, 64>>
{
    static int const kAlignmentA = 128 / cutlass::sizeof_bits<int8_t>::value;
    static int const kAlignmentB = 128 / cutlass::sizeof_bits<uint8_t>::value;

    using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
    using InstructionShape = cutlass::gemm::GemmShape<16, 8, 32>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombinationClamp<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value, int32_t, float>;

    using Operator = cutlass::arch::OpMultiplyAddSaturate;
};

template <typename ElementC>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
                         uint8_t, int8_t, ElementC, int32_t,
                         cutlass::gemm::GemmShape<128, 256, 64>>
{
    static int const kAlignmentA = 128 / cutlass::sizeof_bits<uint8_t>::value;
    static int const kAlignmentB = 128 / cutlass::sizeof_bits<int8_t>::value;

    using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
    using InstructionShape = cutlass::gemm::GemmShape<16, 8, 32>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombinationClamp<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value, int32_t, float>;

    using Operator = cutlass::arch::OpMultiplyAddSaturate;
};

template <typename ElementC>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
                         uint8_t, uint8_t, ElementC, int32_t,
                         cutlass::gemm::GemmShape<128, 256, 64>>
{
    static int const kAlignmentA = 128 / cutlass::sizeof_bits<uint8_t>::value;
    static int const kAlignmentB = 128 / cutlass::sizeof_bits<uint8_t>::value;

    using WarpShape = cutlass::gemm::GemmShape<64, 64, 64>;
    using InstructionShape = cutlass::gemm::GemmShape<16, 8, 32>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombinationClamp<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value, int32_t, float>;

    using Operator = cutlass::arch::OpMultiplyAddSaturate;
};

template <typename ElementC>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
                         cutlass::int4b_t, cutlass::int4b_t, ElementC, int32_t,
                         cutlass::gemm::GemmShape<128, 256, 128>>
{
    static int const kAlignmentA =
        128 / cutlass::sizeof_bits<cutlass::int4b_t>::value;
    static int const kAlignmentB =
        128 / cutlass::sizeof_bits<cutlass::int4b_t>::value;

    using WarpShape = cutlass::gemm::GemmShape<64, 64, 128>;
    using InstructionShape = cutlass::gemm::GemmShape<16, 8, 64>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombinationClamp<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value, int32_t, float>;

    using Operator = cutlass::arch::OpMultiplyAddSaturate;
};

template <typename ElementC>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
                         cutlass::int4b_t, cutlass::uint4b_t, ElementC, int32_t,
                         cutlass::gemm::GemmShape<128, 256, 128>>
{
    static int const kAlignmentA =
        128 / cutlass::sizeof_bits<cutlass::int4b_t>::value;
    static int const kAlignmentB =
        128 / cutlass::sizeof_bits<cutlass::uint4b_t>::value;

    using WarpShape = cutlass::gemm::GemmShape<64, 64, 128>;
    using InstructionShape = cutlass::gemm::GemmShape<16, 8, 64>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombinationClamp<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value, int32_t, float>;

    using Operator = cutlass::arch::OpMultiplyAddSaturate;
};

template <typename ElementC>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
                         cutlass::uint4b_t, cutlass::int4b_t, ElementC, int32_t,
                         cutlass::gemm::GemmShape<128, 256, 128>>
{
    static int const kAlignmentA =
        128 / cutlass::sizeof_bits<cutlass::uint4b_t>::value;
    static int const kAlignmentB =
        128 / cutlass::sizeof_bits<cutlass::int4b_t>::value;

    using WarpShape = cutlass::gemm::GemmShape<64, 64, 128>;
    using InstructionShape = cutlass::gemm::GemmShape<16, 8, 64>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombinationClamp<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value, int32_t, float>;

    using Operator = cutlass::arch::OpMultiplyAddSaturate;
};

template <typename ElementC>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
                         cutlass::uint4b_t, cutlass::uint4b_t, ElementC,
                         int32_t, cutlass::gemm::GemmShape<128, 256, 128>>
{
    static int const kAlignmentA =
        128 / cutlass::sizeof_bits<cutlass::uint4b_t>::value;
    static int const kAlignmentB =
        128 / cutlass::sizeof_bits<cutlass::uint4b_t>::value;

    using WarpShape = cutlass::gemm::GemmShape<64, 64, 128>;
    using InstructionShape = cutlass::gemm::GemmShape<16, 8, 64>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombinationClamp<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value, int32_t, float>;

    using Operator = cutlass::arch::OpMultiplyAddSaturate;
};

template <typename ElementC>
struct GemmConfiguration<cutlass::arch::OpClassTensorOp, cutlass::arch::Sm80,
                         cutlass::uint1b_t, cutlass::uint1b_t, ElementC,
                         int32_t, cutlass::gemm::GemmShape<128, 256, 512>>
{
    static int const kAlignmentA =
        128 / cutlass::sizeof_bits<cutlass::uint1b_t>::value;
    static int const kAlignmentB =
        128 / cutlass::sizeof_bits<cutlass::uint1b_t>::value;

    using WarpShape = cutlass::gemm::GemmShape<64, 64, 512>;
    using InstructionShape = cutlass::gemm::GemmShape<16, 8, 256>;
    static int const kStages = 3;

    using EpilogueOutputOp = cutlass::epilogue::thread::LinearCombinationClamp<
        ElementC, 128 / cutlass::sizeof_bits<ElementC>::value, int32_t, float>;

    using Operator = cutlass::arch::OpMultiplyAdd;
};

/// Custom ThreadblockSwizzle for ARK.
template <typename UnitOp> struct GemmThreadblockSwizzle
{
    DEVICE GemmThreadblockSwizzle()
    {
    }

    DEVICE cutlass::gemm::GemmCoord get_tiled_shape() const
    {
        return cutlass::gemm::GemmCoord(UnitOp::UnitOpDims::H,
                                        UnitOp::UnitOpDims::W, 1);
    }

    DEVICE int get_log_tile(cutlass::gemm::GemmCoord) const
    {
        return 0;
    }

    DEVICE cutlass::gemm::GemmCoord get_tile_offset(int log_tile) const
    {
        // log_tile is actually uop_idx here.
        int uh = UnitOp::uop_idx_h(log_tile);
        int uw = UnitOp::uop_idx_w(log_tile);
        return cutlass::gemm::GemmCoord{uh, uw, 0};
    }
};

/// Half-precision GEMM. Row-major.
template <typename OutDims, typename NCA, typename NCB, typename Shape,
          typename ProblemSize, typename LeadingDims, bool IsColumnA,
          bool IsColumnB, bool IsRelu, int NumThreads, int SmemBytes>
DEVICE void gemm(ark::half *C, ark::half *A, ark::half *B, ark::half alpha,
                 ark::half beta, int uop_idx, int smem_per_warp)
{
    static_assert(NCA::D2 == 1 && NCA::D3 == 1,
                  "NCA should be two dimensional.");
    static_assert(NCB::D2 == 1 && NCB::D3 == 1,
                  "NCB should be two dimensional.");
    static_assert(Shape::D3 == 1, "Shape should be three dimensional.");
    static_assert(ProblemSize::D3 == 1,
                  "ProblemSize should be three dimensional.");

    // N dimension of C is max(N dimension of A, N dimension of B)
    constexpr int NC = (NCA::D0 > NCB::D0) ? NCA::D0 : NCB::D0;
    // C dimension of C is max(C dimension of A, C dimension of B)
    constexpr int CC = (NCA::D1 > NCB::D1) ? NCA::D1 : NCB::D1;

    using OutShape = Vec<NC, CC, ProblemSize::D0, ProblemSize::D1>;
    using UnitOutDims = Vec<1, 1, Shape::D0, Shape::D1>;
    using UnitOp =
        UnitOp<OutDims, OutShape, UnitOutDims, NumThreads, SmemBytes>;

    using LayoutA = typename cutlass::platform::conditional<
        IsColumnA, cutlass::layout::ColumnMajor,
        cutlass::layout::RowMajor>::type;
    using LayoutB = typename cutlass::platform::conditional<
        IsColumnB, cutlass::layout::ColumnMajor,
        cutlass::layout::RowMajor>::type;
    using LayoutC = cutlass::layout::RowMajor;

#if (ARK_TARGET_CUDA_ARCH == 60)
    using ArchTag = cutlass::arch::Sm60;
#elif (ARK_TARGET_CUDA_ARCH == 70)
    using ArchTag = cutlass::arch::Sm70;
#elif (ARK_TARGET_CUDA_ARCH == 80)
    using ArchTag = cutlass::arch::Sm80;
#else
    using ArchTag = cutlass::arch::Sm60;
#endif

    using ThreadblockSwizzle = ark::GemmThreadblockSwizzle<UnitOp>;

    using GemmShape = cutlass::gemm::GemmShape<Shape::D0, Shape::D1, Shape::D2>;
    using GemmConfig = typename ark::GemmConfiguration<
        cutlass::arch::OpClassTensorOp, ArchTag, cutlass::half_t,
        cutlass::half_t, cutlass::half_t, cutlass::half_t, GemmShape>;
    using GemmKernel = typename cutlass::gemm::kernel::DefaultGemm<
        cutlass::half_t, LayoutA, GemmConfig::kAlignmentA, cutlass::half_t,
        LayoutB, GemmConfig::kAlignmentB, cutlass::half_t, LayoutC,
        cutlass::half_t, cutlass::arch::OpClassTensorOp, ArchTag, GemmShape,
        typename GemmConfig::WarpShape, typename GemmConfig::InstructionShape,
        typename GemmConfig::EpilogueOutputOp, ThreadblockSwizzle,
        GemmConfig::kStages, false, typename GemmConfig::Operator>::GemmKernel;

    constexpr int SizeA = math::mul<ProblemSize::D0, ProblemSize::D2>::value;
    constexpr int SizeB = math::mul<ProblemSize::D1, ProblemSize::D2>::value;
    constexpr int SizeC = math::mul<ProblemSize::D0, ProblemSize::D1>::value;

    int un = UnitOp::uop_idx_n(uop_idx);
    int uc = UnitOp::uop_idx_c(uop_idx);

    // Broadcasting
    cutlass::half_t *pA, *pB;
    cutlass::half_t *pC = &C[un * math::mul<CC, SizeC>::value + uc * SizeC];
    if (NCA::D0 == 1 && NCA::D1 == 1) {
        pA = A;
    } else if (NCA::D0 == 1) {
        pA = &A[uc * SizeA];
    } else if (NCA::D1 == 1) {
        pA = &A[un * SizeA];
    } else {
        pA = &A[un * math::mul<CC, SizeA>::value + uc * SizeA];
    }
    if (NCB::D0 == 1 && NCB::D1 == 1) {
        pB = B;
    } else if (NCB::D0 == 1) {
        pB = &B[uc * SizeB];
    } else if (NCB::D1 == 1) {
        pB = &B[un * SizeB];
    } else {
        pB = &B[un * math::mul<CC, SizeB>::value + uc * SizeB];
    }

    LayoutA layout_a(LeadingDims::D0);
    LayoutB layout_b(LeadingDims::D3);
    LayoutC layout_c(LeadingDims::D1);
    cutlass::TensorRef<cutlass::half_t, LayoutA> ref_a(pA, layout_a);
    cutlass::TensorRef<cutlass::half_t, LayoutB> ref_b(pB, layout_b);
    cutlass::TensorRef<cutlass::half_t, LayoutC> ref_c(pC, layout_c);

    cutlass::gemm::GemmCoord problem_size(ProblemSize::D0, ProblemSize::D1,
                                          ProblemSize::D2);

    cutlass::gemm::GemmCoord threadblock_shape(Shape::D0, Shape::D1, Shape::D2);

    ThreadblockSwizzle swizzle;

    cutlass::gemm::GemmCoord tiled_shape(swizzle.get_tiled_shape());

    typename GemmKernel::Params params(problem_size, tiled_shape, ref_a, ref_b,
                                       ref_c, ref_c);

    // A hack for custom threadblock swizzle. swizzle_log_tile is useless
    // for ARK, instead we need uop_idx to determine the tile offset.
    // Since swizzle_log_tile is the input to get_tile_offset(), we can
    // use it to pass uop_idx.
    params.swizzle_log_tile = uop_idx;

    typename GemmKernel::SharedStorage *ps =
        UnitOp::template shared_memory<GemmKernel::SharedStorage>(
            smem_per_warp);

    GemmKernel gemm_kernel;

    gemm_kernel(params, *ps);
}

} // namespace ark

#endif // ARK_KERNELS_GEMM_H_

```

`ark/include/kernels/half.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_HALF_H_
#define ARK_KERNELS_HALF_H_

// clang-format off
#include "cutlass/numeric_types.h"
#include "cutlass/half.h"
// clang-format on

namespace ark {
using half = cutlass::half_t;
} // namespace ark

#endif // ARK_KERNELS_HALF_H_

```

`ark/include/kernels/im2col.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_IM2COL_H_
#define ARK_KERNELS_IM2COL_H_

#include "ewise.h"

namespace ark {

template <typename _InShape, typename _InDims, typename _OutDims,
          typename _UnitOutDims, typename _DataType, int _NelemPerThread,
          int KernelHeight, int KernelWidth, int StrideHeight, int StrideWidth,
          int PadHeight, int PadWidth, int DilationHeight, int DilationWidth>
struct Im2Col;

template <typename _InShape, typename _InDims, typename _OutDims,
          typename _UnitOutDims, int KernelHeight, int KernelWidth,
          int StrideHeight, int StrideWidth, int PadHeight, int PadWidth,
          int DilationHeight, int DilationWidth>
struct Im2Col<_InShape, _InDims, _OutDims, _UnitOutDims, half, 2, KernelHeight,
              KernelWidth, StrideHeight, StrideWidth, PadHeight, PadWidth,
              DilationHeight, DilationWidth>
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = half;
    static const int NelemPerThread = 2;

    static const int InN = InDims::HW;

    static const int Height = _InShape::H;
    static const int Width = _InShape::W;

    static const int PatchNumHeight =
        (Height - KernelHeight + 2 * PadHeight) / StrideHeight + 1;
    static const int PatchNumWidth =
        (Width - KernelWidth + 2 * PadWidth) / StrideWidth + 1;
    static const int PatchNum = math::mul<PatchNumHeight, PatchNumWidth>::value;
    static const int OutHeight = math::pad<PatchNum, _UnitOutDims::H>::value;

    static const int KHW = math::mul<KernelHeight, KernelWidth>::value;

    static const int MaxMIdx = PatchNum;
    static const int MaxNIdx = math::mul<_InShape::NC, KHW>::value;

    // Index of the input element is derived as follows:
    //   channel_idx = nidx / (KernelHeight*KernelWidth);
    //   per_channel_patch_idx = midx;
    //   per_channel_patch_pos_width
    //      = (per_channel_patch_idx % PatchNumWidth) * StrideWidth;
    //   per_channel_patch_pos_height
    //      = (per_channel_patch_idx / PatchNumWidth) * StrideHeight;
    //   per_patch_elem_idx = nidx % (KernelHeight*KernelWidth);
    //   per_patch_elem_pos_width = per_patch_elem_idx % KernelWidth;
    //   per_patch_elem_pos_height = per_patch_elem_idx / KernelWidth;
    //   elem_width =
    //      per_channel_patch_pos_width
    //      + per_patch_elem_pos_width - PadWidth;
    //   elem_height =
    //      per_channel_patch_pos_height
    //      + per_patch_elem_pos_height - PadHeight;
    //   elem_idx = elem_width + elem_height * Width + channel_idx * InN;
    //
    // with exception:
    //   if (elem_width < 0) or (elem_width > Width - 1) --> output is zero
    //   if (elem_height < 0) or (elem_height > Height - 1) --> output is zero
    //
    // This function reads a half value while avoiding 2-byte misaligned access.
    // Return the value as a float for efficiency.
    // CAUTION: This function assumes that `x` address is 4-byte aligned.
    static DEVICE float read_elem(half *x, int midx, int nidx)
    {
        int elem_width = math::mod<PatchNumWidth>(midx) * StrideWidth +
                         math::mod<KernelWidth>(nidx) - PadWidth;
        int elem_height = math::div<PatchNumWidth>(midx) * StrideHeight +
                          math::div<KernelWidth>(math::mod<KHW>(nidx)) -
                          PadHeight;

        if (elem_height < 0 || elem_height >= Height || elem_width < 0 ||
            elem_width >= Width) {
            return 0.0f;
        }

        int idx = elem_width + elem_height * Width + math::div<KHW>(nidx) * InN;

        float2 fx = __half22float2(((__half2 *)x)[idx >> 1]);
        return ((float *)&fx)[idx & 1];
    }

    static DEVICE void compute(half *out, half *in, int idx_n, int idx_c,
                               int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;

        int midx = idx_w;
        int nidx = idx_h + idx_c * OutDims::H + idx_n * OutDims::CH;

        float f1 = 0;
        float f2 = 0;
        if (nidx <= MaxNIdx) {
            if (midx <= MaxMIdx) {
                f1 = read_elem(in, midx, nidx);
            }
            if (midx + 1 <= MaxMIdx) {
                f2 = read_elem(in, midx + 1, nidx);
            }
        }
        __syncwarp();
        *(__half2 *)out = __floats2half2_rn(f1, f2);
    }
};

// Half-precision image to column operation.
// TODO: support dilation.
template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, int KernelHeight, int KernelWidth, int StrideHeight,
          int StrideWidth, int PadHeight, int PadWidth, int DilationHeight,
          int DilationWidth>
DEVICE void im2col(half *y, half *x, int uop_idx, int)
{
    Ewise1<OutDims, OutShape, UnitOutDims, NumThreads, SmemBytes,
           Im2Col<InShape, InDims, OutDims, UnitOutDims, half, 2, KernelHeight,
                  KernelWidth, StrideHeight, StrideWidth, PadHeight, PadWidth,
                  DilationHeight, DilationWidth>>::run(y, x, uop_idx);
    sync_warps<NumThreads>();
}

} // namespace ark

#endif // ARK_KERNELS_IM2COL_H_

```

`ark/include/kernels/layernorm.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_LAYERNORM_H_
#define ARK_KERNELS_LAYERNORM_H_

#include "reduce.h"

namespace ark {

// Static checkers if InShape can be reduced into OutShape.
template <typename InShape, typename OutShape> struct LayerNormShapeChecker
{
    static_assert(InShape::N == OutShape::N,
                  "Dimension N of input and output do not match");
    static_assert(InShape::C == OutShape::C,
                  "Dimension C of input and output do not match");
    static_assert(InShape::H == OutShape::H,
                  "Dimension H of input and output do not match");
    static_assert(OutShape::W == OutShape::W,
                  "Dimension W of input and output do not match");
};

// Perform layer normalization on input and write the result on output.
template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, typename DataType, int NelemPerThread>
struct LayerNorm
{
    using UnitOp =
        UnitOp<OutDims, OutShape, UnitOutDims, NumThreads, SmemBytes>;

    static_assert(NelemPerThread > 0, "NelemPerThread must be positive");
    static DEVICE void run(DataType *out, const DataType *in, int uop_idx,
                           int smem_per_warp)
    {
        using InOutChk = LayerNormShapeChecker<InShape, OutShape>;
        using ReduceTypeMean = ReduceTypeMean<DataType, NelemPerThread>;

        constexpr int NonReduceDimLength = UnitOutDims::NCH;
        // The reduction dimension of the final stage.
        // Assume this division is always exact.
        static_assert((NumThreads * NelemPerThread) % NonReduceDimLength == 0);
        // If we reshape the input into a 2D matrix (NCH x W), NumThreads
        // threads compute NCH rows, and each row's sum is computed by
        // ThreadsPerRow threads. If ThreadsPerRow is larger than warp size, we
        // need to use shared memory to reduce the result of each warp.
        constexpr int ThreadsPerRow =
            (NumThreads * NelemPerThread) / NonReduceDimLength;

        int tid = UnitOp::thread_id();
        int tid_w = (tid * NelemPerThread) % ThreadsPerRow;
        int tid_h = ((tid * NelemPerThread) / ThreadsPerRow) % UnitOutDims::H;
        int tid_c = ((tid * NelemPerThread) / ThreadsPerRow / UnitOutDims::H) %
                    UnitOutDims::C;
        int tid_n = (tid * NelemPerThread) / ThreadsPerRow / UnitOutDims::CH;

        int un = UnitOp::uop_idx_n(uop_idx);
        int uc = UnitOp::uop_idx_c(uop_idx);
        int uh = UnitOp::uop_idx_h(uop_idx);

        int idx_in_base = (tid_h + uh * UnitOutDims::H) * InDims::W +
                          (tid_c + uc * UnitOutDims::C) * InDims::HW +
                          (tid_n + un * UnitOutDims::N) * InDims::CHW;

        DataType reduced;
        ReduceTypeMean::singleIdentity(&reduced);
        for (int idx_in_w = tid_w; idx_in_w < InShape::W;
             idx_in_w += ThreadsPerRow) {
            int idx_in = idx_in_base + idx_in_w;
            ReduceTypeMean::singleReduce(&reduced, &reduced, &in[idx_in]);
        }
        UnitOp::sync_threads();
        // final reduction on shared memory using warp shuffle.
        reduced = warpsReduce<ReduceTypeMean, UnitOp, ThreadsPerRow>(
            reduced, tid, smem_per_warp);
        // get the average result.
        ReduceTypeMean::singlePostReduce(&reduced, &reduced, UnitOutDims::W);
        DataType variance;
        ReduceTypeMean::singleIdentity(&variance);
        // get the variance
        UnitOp::sync_threads();
        for (int idx_in_w = tid_w; idx_in_w < InShape::W;
             idx_in_w += ThreadsPerRow) {
            int idx_in = idx_in_base + idx_in_w;
            variance += (in[idx_in] - reduced) * (in[idx_in] - reduced);
        }
        UnitOp::sync_threads();
        variance = warpsReduce<ReduceTypeMean, UnitOp, ThreadsPerRow>(
            variance, tid, smem_per_warp);
        ReduceTypeMean::singlePostReduce(&variance, &variance, UnitOutDims::W);
        UnitOp::sync_threads();
        // the output is (input - mean) / sqrt(variance)
        for (int idx_in_w = tid_w; idx_in_w < InShape::W;
             idx_in_w += ThreadsPerRow) {
            int idx_in = idx_in_base + idx_in_w;
            out[idx_in] = (in[idx_in] - reduced) * rsqrtf(variance + 1e-5f);
        }
    }
};

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes>
DEVICE void layernorm(float *out, const float *in, int uop_idx,
                      int smem_per_warp)
{
    constexpr int NelemPerThread = 1;
    LayerNorm<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
              SmemBytes, float, NelemPerThread>::run(out, in, uop_idx,
                                                     smem_per_warp);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes>
DEVICE void layernorm(ark::half *out, const ark::half *in, int uop_idx,
                      int smem_per_warp)
{
    constexpr int NelemPerThread = 1;
    LayerNorm<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
              SmemBytes, ark::half, NelemPerThread>::run(out, in, uop_idx,
                                                         smem_per_warp);
}

} // namespace ark

#endif // ARK_KERNELS_LAYERNORM_H_

```

`ark/include/kernels/matmul.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_MATMUL_H_
#define ARK_KERNELS_MATMUL_H_

#include "gemm.h"

namespace ark {

// Matrix multiplication. Reuse GEMM kernels. Row-major.
template <typename OutDims, typename NCA, typename NCB, typename Shape,
          typename ProblemSize, typename LeadingDims, bool IsColumnA,
          bool IsColumnB, bool IsRelu, int NumThreads, int SmemBytes>
DEVICE void matmul(ark::half *C, ark::half *A, ark::half *B, int uop_idx,
                   int smem_per_warp)
{
    // 0x3c00 represents constant 1.0 in half-precision floating point format.
    gemm<OutDims, NCA, NCB, Shape, ProblemSize, LeadingDims, IsColumnA,
         IsColumnB, IsRelu, NumThreads, SmemBytes>(
        C, A, B, ark::half::bitcast(0x3c00), ark::half::bitcast(0x0), uop_idx,
        smem_per_warp);
}

// /* Fused matrix multiplication and scale kernel. */
// template <int M, int N, int K, bool TA, bool TB, int BcastType, bool IsRelu,
//           int NumThreads, int SmemBytes, int TDimM, int TDimN, int TDimK>
// DEVICE void matmul(
//    half *C, half *A, half *B, half scale, int tx, int ty, int tz)
// {
//     constexpr int BT = BcastType == 0 ? 0 : BcastType == 1 ? 2 : 1;
//     // 0x3c00 represents constant 1.0 in half-precision floating point
//     // format.
//     gemm<N, M, K, TB, TA, BT, IsRelu, NumThreads, SmemBytes, TDimN,
//     TDimM, TDimK>(
//         C, B, A, half{scale}, half::bitcast(0x0), ty, tx, tz);
// }

} // namespace ark

#endif // ARK_KERNELS_MATMUL_H_

```

`ark/include/kernels/platform.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_PLATFORM_H_
#define ARK_KERNELS_PLATFORM_H_

#include "cutlass/platform/platform.h"
#include "device.h"
#include "half.h"
#include <cfloat>
#include <limits>

namespace platform {

template <> struct numeric_limits<float>
{
    static DEVICE float lowest()
    {
        return -FLT_MAX;
    }
};

} // namespace platform

#endif // ARK_KERNELS_PLATFORM_H_

```

`ark/include/kernels/reduce.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_REDUCE_H_
#define ARK_KERNELS_REDUCE_H_

#include "ewise.h"
#include <type_traits>

namespace ark {

typedef enum
{
    N = 0,
    C = 1,
    H = 2,
    W = 3,
} AxisType;

// Shared memory for reduction.
template <typename DataType> struct ReduceSharedStorage
{
    DataType storage[32];
};

/* Reduce single-precision `val` within a single warp. */
template <typename ReduceType, int LanesNum,
          typename DataType = typename ReduceType::DataType>
DEVICE DataType warpReduce(DataType val)
{
    DataType res = val;
    DataType tmp;
    if (LanesNum >= 32) {
        tmp = __shfl_xor_sync(0xffffffff, res, 16, 32);
        ReduceType::singleReduce(&res, &res, &tmp);
        tmp = __shfl_xor_sync(0xffffffff, res, 8, 16);
        ReduceType::singleReduce(&res, &res, &tmp);
        tmp = __shfl_xor_sync(0xffffffff, res, 4, 8);
        ReduceType::singleReduce(&res, &res, &tmp);
        tmp = __shfl_xor_sync(0xffffffff, res, 2, 4);
        ReduceType::singleReduce(&res, &res, &tmp);
        tmp = __shfl_xor_sync(0xffffffff, res, 1, 2);
        ReduceType::singleReduce(&res, &res, &tmp);
    } else {
        if (LanesNum > 16) {
            tmp = __shfl_xor_sync(0xffffffff, res, 16, 32);
            ReduceType::singleReduce(&res, &res, &tmp);
        }
        if (LanesNum > 8) {
            tmp = __shfl_xor_sync(0xffffffff, res, 8, 16);
            ReduceType::singleReduce(&res, &res, &tmp);
        }
        if (LanesNum > 4) {
            tmp = __shfl_xor_sync(0xffffffff, res, 4, 8);
            ReduceType::singleReduce(&res, &res, &tmp);
        }
        if (LanesNum > 2) {
            tmp = __shfl_xor_sync(0xffffffff, res, 2, 4);
            ReduceType::singleReduce(&res, &res, &tmp);
        }
        if (LanesNum > 1) {
            tmp = __shfl_xor_sync(0xffffffff, res, 1, 2);
            ReduceType::singleReduce(&res, &res, &tmp);
        }
    }
    return res;
}

// Reduce single-precision `val` within multiple warps.
template <typename ReduceType, typename UnitOp, int LanesNum,
          typename DataType = typename ReduceType::DataType>
DEVICE DataType warpsReduce(DataType val, int tid, int smem_per_warp)
{
    val = warpReduce<ReduceType, LanesNum>(val);
    if (LanesNum > 32) {
        ReduceSharedStorage<DataType> *shared =
            UnitOp::template shared_memory<ReduceSharedStorage<DataType>>(
                smem_per_warp);
        int laneId = tid & 31;
        int warpId = tid >> 5;
        if (laneId == 0) {
            shared->storage[warpId] = val;
        }
        UnitOp::sync_threads();
        if (laneId < (LanesNum >> 5)) {
            val = shared->storage[laneId];
        } else {
            ReduceType::singleIdentity(&val);
        }
        val = warpReduce<ReduceType, 32>(val);
    }
    return val;
}

// Check if InShape can be reduced into OutShape and if UnitOutDims is valid.
template <typename InShape, typename OutShape, typename UnitOutDims, int Axis>
struct ReduceShapeChecker
{
    static_assert((InShape::N == OutShape::N) ||
                      (Axis == AxisType::N && OutShape::N == 1),
                  "Invalid dimension N");
    static_assert((InShape::C == OutShape::C) ||
                      (Axis == AxisType::C && OutShape::C == 1),
                  "Invalid dimension C");
    static_assert((InShape::H == OutShape::H) ||
                      (Axis == AxisType::H && OutShape::H == 1),
                  "Invalid dimension H");
    static_assert((InShape::W == OutShape::W) ||
                      (Axis == AxisType::W && OutShape::W == 1),
                  "Invalid dimension W");
    static_assert((UnitOutDims::N == 1) || (Axis != AxisType::N),
                  "Invalid UnitOutDims::N");
    static_assert((UnitOutDims::C == 1) || (Axis != AxisType::C),
                  "Invalid UnitOutDims::C");
    static_assert((UnitOutDims::H == 1) || (Axis != AxisType::H),
                  "Invalid UnitOutDims::H");
    static_assert((UnitOutDims::W == 1) || (Axis != AxisType::W),
                  "Invalid UnitOutDims::W");
};

template <typename _DataType, int _NelemPerThread> struct ReduceTypeSum
{
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void identity(DataType *v)
    {
#pragma unroll
        for (int elem = 0; elem < NelemPerThread; ++elem) {
            v[elem] = 0;
        }
    }
    static DEVICE void reduce(DataType *out, const DataType *in0,
                              const DataType *in1)
    {
#pragma unroll
        for (int elem = 0; elem < NelemPerThread; ++elem) {
            out[elem] = in0[elem] + in1[elem];
        }
    }
    static DEVICE void postReduce(DataType *out, const DataType *in,
                                  int nelem = 1)
    {
#pragma unroll
        for (int elem = 0; elem < NelemPerThread; ++elem) {
            out[elem] = in[elem];
        }
    }
    static DEVICE void singleIdentity(DataType *v)
    {
        *v = 0;
    }
    static DEVICE void singleReduce(DataType *out, const DataType *in0,
                                    const DataType *in1)
    {
        *out = *in0 + *in1;
    }
    static DEVICE void singlePostReduce(DataType *out, const DataType *in,
                                        int nelem = 1)
    {
        *out = *in;
    }
};

template <> struct ReduceTypeSum<half, 2>
{
    using DataType = half;
    static const int NelemPerThread = 2;

    static DEVICE void identity(half *v)
    {
        *reinterpret_cast<__half2 *>(v) = (__half2_raw){0, 0};
    }
    static DEVICE void reduce(half *out, const half *in0, const half *in1)
    {
        __half2 *out2 = reinterpret_cast<__half2 *>(out);
        const __half2 *in02 = reinterpret_cast<const __half2 *>(in0);
        const __half2 *in12 = reinterpret_cast<const __half2 *>(in1);
        *out2 = __hadd2(*in02, *in12);
    }
    static DEVICE void postReduce(half *out, const half *in, int nelem = 1)
    {
        __half2 *out2 = reinterpret_cast<__half2 *>(out);
        const __half2 *in2 = reinterpret_cast<const __half2 *>(in);
        *out2 = *in2;
    }
    static DEVICE void singleIdentity(half *v)
    {
        *v = 0;
    }
    static DEVICE void singleReduce(half *out, const half *in0, const half *in1)
    {
        *out = *in0 + *in1;
    }
    static DEVICE void singlePostReduce(half *out, const half *in,
                                        int nelem = 1)
    {
        *out = *in;
    }
};

template <typename _DataType, int _NelemPerThread> struct ReduceTypeMax
{
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void identity(DataType *v)
    {
#pragma unroll
        for (int elem = 0; elem < NelemPerThread; ++elem) {
            v[elem] = platform::numeric_limits<DataType>::lowest();
        }
    }
    static DEVICE void reduce(DataType *out, const DataType *in0,
                              const DataType *in1)
    {
#pragma unroll
        for (int elem = 0; elem < NelemPerThread; ++elem) {
            out[elem] = (in0[elem] > in1[elem]) ? in0[elem] : in1[elem];
        }
    }
    static DEVICE void postReduce(DataType *out, const DataType *in,
                                  int nelem = 1)
    {
#pragma unroll
        for (int elem = 0; elem < NelemPerThread; ++elem) {
            out[elem] = in[elem];
        }
    }
    static DEVICE void singleIdentity(DataType *v)
    {
        *v = platform::numeric_limits<DataType>::lowest();
    }

    static DEVICE void singleReduce(DataType *out, const DataType *in0,
                                    const DataType *in1)
    {
        *out = (*in0 > *in1) ? *in0 : *in1;
    }
    static DEVICE void singlePostReduce(DataType *out, const DataType *in,
                                        int nelem = 1)
    {
        *out = *in;
    }
};

template <> struct ReduceTypeMax<half, 2>
{
    using DataType = half;
    static const int NelemPerThread = 2;

    static DEVICE void identity(half *v)
    {
        *reinterpret_cast<__half2 *>(v) = (__half2_raw){0xfbff, 0xfbff};
    }
    static DEVICE void reduce(half *out, const half *in0, const half *in1)
    {
#if (__CUDA_ARCH__ >= 800)
        __half2 *out2 = reinterpret_cast<__half2 *>(out);
        const __half2 *in02 = reinterpret_cast<const __half2 *>(in0);
        const __half2 *in12 = reinterpret_cast<const __half2 *>(in1);
        *out2 = __hmax2(*in02, *in12);
#else
#pragma unroll
        for (int elem = 0; elem < NelemPerThread; ++elem) {
            out[elem] = (in0[elem] > in1[elem]) ? in0[elem] : in1[elem];
        }
#endif // (__CUDA_ARCH__ >= 800)
    }
    static DEVICE void postReduce(half *out, const half *in, int nelem = 1)
    {
        __half2 *out2 = reinterpret_cast<__half2 *>(out);
        const __half2 *in2 = reinterpret_cast<const __half2 *>(in);
        *out2 = *in2;
    }
    static DEVICE void singleIdentity(half *v)
    {
        *v = platform::numeric_limits<half>::lowest();
    }
    static DEVICE void singleReduce(half *out, const half *in0, const half *in1)
    {
        *out = (*in0 > *in1) ? *in0 : *in1;
    }
    static DEVICE void singlePostReduce(half *out, const half *in,
                                        int nelem = 1)
    {
        *out = *in;
    }
};

template <typename _DataType, int _NelemPerThread> struct ReduceTypeMean
{
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void identity(DataType *v)
    {
#pragma unroll
        for (int elem = 0; elem < NelemPerThread; ++elem) {
            v[elem] = 0;
        }
    }
    static DEVICE void reduce(DataType *out, const DataType *in0,
                              const DataType *in1)
    {
#pragma unroll
        for (int elem = 0; elem < NelemPerThread; ++elem) {
            out[elem] = in0[elem] + in1[elem];
        }
    }
    static DEVICE void postReduce(DataType *out, const DataType *in,
                                  int nelem = 1)
    {
#pragma unroll
        for (int elem = 0; elem < NelemPerThread; ++elem) {
            out[elem] = in[elem] / nelem;
        }
    }
    static DEVICE void singleIdentity(DataType *v)
    {
        *v = 0;
    }
    static DEVICE void singleReduce(DataType *out, const DataType *in0,
                                    const DataType *in1)
    {
        *out = *in0 + *in1;
    }
    static DEVICE void singlePostReduce(DataType *out, const DataType *in,
                                        int nelem = 1)
    {
        *out = *in / nelem;
    }
};

template <> struct ReduceTypeMean<half, 2>
{
    using DataType = half;
    static const int NelemPerThread = 2;

    static DEVICE void identity(half *v)
    {
        *reinterpret_cast<__half2 *>(v) = (__half2_raw){0, 0};
    }
    static DEVICE void reduce(half *out, const half *in0, const half *in1)
    {
        __half2 *out2 = reinterpret_cast<__half2 *>(out);
        const __half2 *in02 = reinterpret_cast<const __half2 *>(in0);
        const __half2 *in12 = reinterpret_cast<const __half2 *>(in1);
        *out2 = __hadd2(*in02, *in12);
    }
    static DEVICE void postReduce(half *out, const half *in, int nelem = 1)
    {
        __half2 *out2 = reinterpret_cast<__half2 *>(out);
        const __half2 *in2 = reinterpret_cast<const __half2 *>(in);
        *out2 = __h2div(*in2, __float2half2_rn((float)nelem));
    }
    static DEVICE void singleIdentity(half *v)
    {
        *v = 0;
    }
    static DEVICE void singleReduce(half *out, const half *in0, const half *in1)
    {
        *out = *in0 + *in1;
    }
    static DEVICE void singlePostReduce(half *out, const half *in,
                                        int nelem = 1)
    {
        *out = *in / nelem;
    }
};

template <typename InDims, typename InShape, typename OutDims,
          typename ReduceType, int Axis>
struct EwiseReduceCompType;

// Conduct reduction on N dimension of the input.
template <typename InDims, typename InShape, typename OutDims,
          typename ReduceType>
struct EwiseReduceCompType<InDims, InShape, OutDims, ReduceType, AxisType::N>
{
    using DataType = typename ReduceType::DataType;
    static const int NelemPerThread = ReduceType::NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        int idx_out = idx_c * OutDims::HW + idx_h * OutDims::W + idx_w;
        int idx_in = idx_c * InDims::HW + idx_h * InDims::W + idx_w;
        DataType reduced[NelemPerThread];

        ReduceType::identity(reduced);
#pragma unroll
        for (int i = 0; i < InShape::N; ++i) {
            ReduceType::reduce(reduced, reduced, &in[idx_in + i * InDims::CHW]);
        }
        ReduceType::postReduce(&out[idx_out], reduced, InShape::N);
    }
};

// Conduct reduction on C dimension of the input.
template <typename InDims, typename InShape, typename OutDims,
          typename ReduceType>
struct EwiseReduceCompType<InDims, InShape, OutDims, ReduceType, AxisType::C>
{
    using DataType = typename ReduceType::DataType;
    static const int NelemPerThread = ReduceType::NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        int idx_out = idx_n * OutDims::CHW + idx_h * OutDims::W + idx_w;
        int idx_in = idx_n * InDims::CHW + idx_h * InDims::W + idx_w;
        DataType reduced[NelemPerThread];

        ReduceType::identity(reduced);
#pragma unroll
        for (int i = 0; i < InShape::C; ++i) {
            ReduceType::reduce(reduced, reduced, &in[idx_in + i * InDims::HW]);
        }
        ReduceType::postReduce(&out[idx_out], reduced, InShape::C);
    }
};

// Conduct reduction on H dimension of the input.
template <typename InDims, typename InShape, typename OutDims,
          typename ReduceType>
struct EwiseReduceCompType<InDims, InShape, OutDims, ReduceType, AxisType::H>
{
    using DataType = typename ReduceType::DataType;
    static const int NelemPerThread = ReduceType::NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        int idx_out = idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_w;
        int idx_in = idx_n * InDims::CHW + idx_c * InDims::HW + idx_w;
        DataType reduced[NelemPerThread];

        ReduceType::identity(reduced);
#pragma unroll
        for (int i = 0; i < InShape::H; ++i) {
            ReduceType::reduce(reduced, reduced, &in[idx_in + i * InDims::W]);
        }
        ReduceType::postReduce(&out[idx_out], reduced, InShape::H);
    }
};

// Conduct reduction on W dimension of the input.
template <typename InDims, typename InShape, typename OutDims,
          typename ReduceType>
struct EwiseReduceCompType<InDims, InShape, OutDims, ReduceType, AxisType::W>
{
    using DataType = typename ReduceType::DataType;
    static const int NelemPerThread = ReduceType::NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        int idx_out =
            idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W;
        int idx_in =
            idx_n * InDims::CHW + idx_c * InDims::HW + idx_h * InDims::W;
        DataType reduced[NelemPerThread];

        ReduceType::identity(reduced);
#pragma unroll
        for (int i = 0; i < InShape::W; ++i) {
            ReduceType::reduce(reduced, reduced, &in[idx_in + i]);
        }

        DataType finalSum;
        ReduceType::singleIdentity(&finalSum);
#pragma unroll
        for (int i = 0; i < NelemPerThread; ++i) {
            ReduceType::singleReduce(&finalSum, &finalSum, &reduced[i]);
        }
        ReduceType::singlePostReduce(&out[idx_out], &finalSum, InShape::W);
    }
};

// Reduce one dimension of input into output.
template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, typename ReduceType, int Axis>
struct EwiseReduce
{
    using UnitOp =
        UnitOp<OutDims, OutShape, UnitOutDims, NumThreads, SmemBytes>;
    using DataType = typename ReduceType::DataType;

    static const int NelemPerThread = ReduceType::NelemPerThread;
    static_assert(NelemPerThread > 0, "NelemPerThread must be positive");
    static_assert(UnitOutDims::W % NelemPerThread == 0,
                  "UnitOutDims::W must be divisible by NelemPerThread");

    /// Conduct reduction of the input.
    /// @param out Output tensor.
    /// @param in Input tensor.
    /// @param uop_idx Index of the unit operator.
    static DEVICE void run(DataType *out, DataType *in, int uop_idx)
    {
        static_assert(Axis == AxisType::N || Axis == AxisType::C ||
                          Axis == AxisType::H || Axis == AxisType::W,
                      "Invalid reduction axis.");

        using ShapeChecker =
            ReduceShapeChecker<InShape, OutShape, UnitOutDims, Axis>;

        Ewise1<OutDims, OutShape, UnitOutDims, NumThreads, SmemBytes,
               EwiseReduceCompType<InDims, InShape, OutDims, ReduceType,
                                   Axis>>::run(out, in, uop_idx);
    }
};

// Warp-wise reduction. Only support reduction along the W dimension.
template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, typename ReduceType, int Axis>
struct WwiseReduce
{
    using UnitOp =
        UnitOp<OutDims, OutShape, UnitOutDims, NumThreads, SmemBytes>;
    using DataType = typename ReduceType::DataType;
    static const int NelemPerThread = ReduceType::NelemPerThread;

    static_assert(NelemPerThread > 0, "NelemPerThread must be positive");
    static_assert(UnitOutDims::W % NelemPerThread == 0,
                  "UnitOutDims::W must be divisible by NelemPerThread");
    static_assert(Axis == AxisType::W, "Only support reduction along W axis");

    // TODO(chhwang): support NelemPerThread > 1.
    static_assert(NelemPerThread == 1, "Unimplemented");

    /// Conduct reduction on W dimension of the input.
    /// @param out Output tensor.
    /// @param in Input tensor.
    /// @param uop_idx Index of the unit operator.
    static DEVICE void runW(DataType *out, DataType *in, int uop_idx,
                            int smem_per_warp)
    {
        using ShapeChecker =
            ReduceShapeChecker<InShape, OutShape, UnitOutDims, Axis>;

        constexpr int NonReduceDimLength =
            UnitOutDims::N * UnitOutDims::C * UnitOutDims::H;
        // The reduction dimension of the final stage.
        // Assume this division is always exact.
        static_assert((NumThreads * NelemPerThread) % NonReduceDimLength == 0);
        // If we reshape the input into a 2D matrix (NCH x W), NumThreads
        // threads compute NCH rows, and each row's sum is computed by
        // ThreadsPerRow threads. If ThreadsPerRow is larger than warp size, we
        // need to use shared memory to reduce the result of each warp.
        constexpr int ThreadsPerRow =
            (NumThreads * NelemPerThread) / NonReduceDimLength;
        int tid = UnitOp::thread_id();
        int tid_w = (tid * NelemPerThread) % ThreadsPerRow;
        int tid_h = ((tid * NelemPerThread) / ThreadsPerRow) % UnitOutDims::H;
        int tid_c = ((tid * NelemPerThread) / ThreadsPerRow / UnitOutDims::H) %
                    UnitOutDims::C;
        int tid_n = (tid * NelemPerThread) / ThreadsPerRow / UnitOutDims::CH;

        int un = UnitOp::uop_idx_n(uop_idx);
        int uc = UnitOp::uop_idx_c(uop_idx);
        int uh = UnitOp::uop_idx_h(uop_idx);

        int idx_out = (tid_h + uh * UnitOutDims::H) * OutDims::W +
                      (tid_c + uc * UnitOutDims::C) * OutDims::HW +
                      (tid_n + un * UnitOutDims::N) * OutDims::CHW;
        int idx_in_base = (tid_h + uh * UnitOutDims::H) * InDims::W +
                          (tid_c + uc * UnitOutDims::C) * InDims::HW +
                          (tid_n + un * UnitOutDims::N) * InDims::CHW;

        DataType reduced[NelemPerThread];

        ReduceType::identity(reduced);
#pragma unroll
        for (int idx_in_w = tid_w; idx_in_w < InShape::W;
             idx_in_w += ThreadsPerRow) {
            int idx_in = idx_in_base + idx_in_w;
            ReduceType::reduce(reduced, reduced, &in[idx_in]);
        }

        DataType finalSum;
        ReduceType::singleIdentity(&finalSum);
#pragma unroll
        for (int i = 0; i < NelemPerThread; ++i) {
            ReduceType::singleReduce(&finalSum, &finalSum, &reduced[i]);
        }

        UnitOp::sync_threads();

        // final reduction on shared memory using warp shuffle.
        finalSum = warpsReduce<ReduceType, UnitOp, ThreadsPerRow>(
            finalSum, tid, smem_per_warp);

        // write the result to output.
        if (tid % ThreadsPerRow == 0) {
            ReduceType::singlePostReduce(&out[idx_out], &finalSum, InShape::W);
        }
    }
};

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, int Axis>
DEVICE void reduce_e_sum(half *out, half *in, int uop_idx, int)
{
    EwiseReduce<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
                SmemBytes, ReduceTypeSum<half, 2>, Axis>::run(out, in, uop_idx);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, int Axis>
DEVICE void reduce_e_sum(float *out, float *in, int uop_idx)
{
    EwiseReduce<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
                SmemBytes, ReduceTypeSum<float, 1>, Axis>::run(out, in,
                                                               uop_idx);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, int Axis>
DEVICE void reduce_e_mean(half *out, half *in, int uop_idx)
{
    EwiseReduce<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
                SmemBytes, ReduceTypeMean<half, 2>, Axis>::run(out, in,
                                                               uop_idx);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, int Axis>
DEVICE void reduce_e_mean(float *out, float *in, int uop_idx)
{
    EwiseReduce<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
                SmemBytes, ReduceTypeMean<float, 1>, Axis>::run(out, in,
                                                                uop_idx);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, int Axis>
DEVICE void reduce_e_max(half *out, half *in, int uop_idx)
{
    EwiseReduce<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
                SmemBytes, ReduceTypeMax<half, 2>, Axis>::run(out, in, uop_idx);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, int Axis>
DEVICE void reduce_e_max(float *out, float *in, int uop_idx)
{
    EwiseReduce<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
                SmemBytes, ReduceTypeMax<float, 1>, Axis>::run(out, in,
                                                               uop_idx);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, int Axis>
DEVICE void reduce_w_sum(half *out, half *in, int uop_idx, int smem_per_warp)
{
    WwiseReduce<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
                SmemBytes, ReduceTypeSum<half, 1>, Axis>::runW(out, in, uop_idx,
                                                               smem_per_warp);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, int Axis>
DEVICE void reduce_w_sum(float *out, float *in, int uop_idx, int smem_per_warp)
{
    WwiseReduce<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
                SmemBytes, ReduceTypeSum<float, 1>, Axis>::runW(out, in,
                                                                uop_idx,
                                                                smem_per_warp);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, int Axis>
DEVICE void reduce_w_mean(half *out, half *in, int uop_idx, int smem_per_warp)
{
    WwiseReduce<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
                SmemBytes, ReduceTypeMean<half, 1>, Axis>::runW(out, in,
                                                                uop_idx,
                                                                smem_per_warp);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, int Axis>
DEVICE void reduce_w_mean(float *out, float *in, int uop_idx, int smem_per_warp)
{
    WwiseReduce<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
                SmemBytes, ReduceTypeMean<float, 1>, Axis>::runW(out, in,
                                                                 uop_idx,
                                                                 smem_per_warp);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, int Axis>
DEVICE void reduce_w_max(half *out, half *in, int uop_idx, int smem_per_warp)
{
    WwiseReduce<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
                SmemBytes, ReduceTypeMax<half, 1>, Axis>::runW(out, in, uop_idx,
                                                               smem_per_warp);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, int Axis>
DEVICE void reduce_w_max(float *out, float *in, int uop_idx, int smem_per_warp)
{
    WwiseReduce<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
                SmemBytes, ReduceTypeMax<float, 1>, Axis>::runW(out, in,
                                                                uop_idx,
                                                                smem_per_warp);
}

} // namespace ark

#endif // ARK_KERNELS_REDUCE_H_

```

`ark/include/kernels/smem.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_SMEM_H_
#define ARK_KERNELS_SMEM_H_

#include "arch.h"
#include "device.h"
#include "static_math.h"

extern __shared__ int _ARK_SMEM[];

namespace ark {

#if defined(ARK_THREADS_PER_BLOCK)
template <typename T, int NumThreads> struct SharedMemory
{
    static DEVICE int smem_base_offset(int smem_per_warp)
    {
        // The smallest warp ID in the uop.
        constexpr int NumWarps = NumThreads / Arch::ThreadsPerWarp;
        int least_warp_id = math::gm<NumWarps>(warp_id());
        return math::div<sizeof(int)>(least_warp_id * smem_per_warp);
    }

    static DEVICE T *get(int smem_per_warp)
    {
        return (T *)&_ARK_SMEM[smem_base_offset(smem_per_warp)];
    }
};
#endif // defined(ARK_THREADS_PER_BLOCK)

} // namespace ark

#endif // ARK_KERNELS_SMEM_H_

```

`ark/include/kernels/softmax.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_SOFTMAX_H_
#define ARK_KERNELS_SOFTMAX_H_

#include "reduce.h"

namespace ark {

// Static checkers if InShape can be reduced into OutShape.
template <typename InShape, typename OutShape> struct SoftmaxShapeChecker
{
    static_assert(InShape::N == OutShape::N,
                  "Dimension N of input and output do not match");
    static_assert(InShape::C == OutShape::C,
                  "Dimension C of input and output do not match");
    static_assert(InShape::H == OutShape::H,
                  "Dimension H of input and output do not match");
    static_assert(OutShape::W == OutShape::W,
                  "Dimension W of input and output do not match");
};

// Perform layer normalization on input and write the result on output.
template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes, typename DataType, int NelemPerThread>
struct Softmax
{
    using UnitOp =
        UnitOp<OutDims, OutShape, UnitOutDims, NumThreads, SmemBytes>;

    static_assert(NelemPerThread > 0, "NelemPerThread must be positive");

    // TODO(chhwang): support NelemPerThread > 1.
    static_assert(NelemPerThread == 1, "Unimplemented");

    static DEVICE void run(DataType *out, const DataType *in, int uop_idx,
                           int smem_per_warp)
    {
        using InOutChk = SoftmaxShapeChecker<InShape, OutShape>;
        using ReduceTypeMax = ReduceTypeMax<DataType, NelemPerThread>;
        using ReduceTypeSum = ReduceTypeSum<DataType, NelemPerThread>;

        constexpr int NonReduceDimLength = UnitOutDims::NCH;
        // The reduction dimension of the final stage.
        // Assume this division is always exact.
        static_assert((NumThreads * NelemPerThread) % NonReduceDimLength == 0);
        // If we reshape the input into a 2D matrix (NCH x W), NumThreads
        // threads compute NCH rows, and each row's sum is computed by
        // ThreadsPerRow threads. If ThreadsPerRow is larger than warp size, we
        // need to use shared memory to reduce the result of each warp.
        constexpr int ThreadsPerRow =
            (NumThreads * NelemPerThread) / NonReduceDimLength;

        int tid = UnitOp::thread_id();
        int tid_w = (tid * NelemPerThread) % ThreadsPerRow;
        int tid_h = ((tid * NelemPerThread) / ThreadsPerRow) % UnitOutDims::H;
        int tid_c = ((tid * NelemPerThread) / ThreadsPerRow / UnitOutDims::H) %
                    UnitOutDims::C;
        int tid_n = (tid * NelemPerThread) / ThreadsPerRow / UnitOutDims::CH;

        int un = UnitOp::uop_idx_n(uop_idx);
        int uc = UnitOp::uop_idx_c(uop_idx);
        int uh = UnitOp::uop_idx_h(uop_idx);

        int idx_in_base = (tid_h + uh * UnitOutDims::H) * InDims::W +
                          (tid_c + uc * UnitOutDims::C) * InDims::HW +
                          (tid_n + un * UnitOutDims::N) * InDims::CHW;

        // get the max input.
        DataType max_input;
        ReduceTypeMax::singleIdentity(&max_input);
        for (int idx_in_w = tid_w; idx_in_w < InShape::W;
             idx_in_w += ThreadsPerRow) {
            int idx_in = idx_in_base + idx_in_w;
            ReduceTypeMax::singleReduce(&max_input, &max_input, &in[idx_in]);
        }
        UnitOp::sync_threads();

        // final reduction on shared memory using warp shuffle.
        max_input = warpsReduce<ReduceTypeMax, UnitOp, ThreadsPerRow>(
            max_input, tid, smem_per_warp);
        // get the max input.
        ReduceTypeMax::singlePostReduce(&max_input, &max_input, UnitOutDims::W);

        // get the exp input sum, use float to avoid overflow.
        DataType exp_sum_input;
        ReduceTypeSum::singleIdentity(&exp_sum_input);
        UnitOp::sync_threads();
        for (int idx_in_w = tid_w; idx_in_w < InShape::W;
             idx_in_w += ThreadsPerRow) {
            int idx_in = idx_in_base + idx_in_w;
            exp_sum_input = exp_sum_input + expf(in[idx_in] - max_input);
        }
        UnitOp::sync_threads();
        exp_sum_input = warpsReduce<ReduceTypeSum, UnitOp, ThreadsPerRow>(
            exp_sum_input, tid, smem_per_warp);
        ReduceTypeSum::singlePostReduce(&exp_sum_input, &exp_sum_input);
        UnitOp::sync_threads();
        // the output is
        for (int idx_in_w = tid_w; idx_in_w < InShape::W;
             idx_in_w += ThreadsPerRow) {
            int idx_in = idx_in_base + idx_in_w;
            out[idx_in] = expf(in[idx_in] - max_input) / exp_sum_input;
        }
    }
};

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes>
DEVICE void softmax(float *out, float *in, int uop_idx, int smem_per_warp)
{
    constexpr int NelemPerThread = 1;
    Softmax<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
            SmemBytes, float, NelemPerThread>::run(out, in, uop_idx,
                                                   smem_per_warp);
}

template <typename InDims, typename InShape, typename OutDims,
          typename OutShape, typename UnitOutDims, int NumThreads,
          int SmemBytes>
DEVICE void softmax(ark::half *out, ark::half *in, int uop_idx,
                    int smem_per_warp)
{
    constexpr int NelemPerThread = 1;
    Softmax<InDims, InShape, OutDims, OutShape, UnitOutDims, NumThreads,
            SmemBytes, ark::half, NelemPerThread>::run(out, in, uop_idx,
                                                       smem_per_warp);
}

} // namespace ark

#endif // ARK_KERNELS_SOFTMAX_H_

```

`ark/include/kernels/static_math.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_STATIC_MATH_H_
#define ARK_KERNELS_STATIC_MATH_H_

#include "device.h"

namespace ark {
namespace math {

// Absolute value
template <int N> struct abs
{
    enum
    {
        value = (N > 0) ? N : -N
    };
};

// Larger value
template <int A, int B> struct max
{
    enum
    {
        value = (A > B) ? A : B
    };
};

// Smaller value
template <int A, int B> struct min
{
    enum
    {
        value = (A < B) ? A : B
    };
};

// Statically determine log2(N), rounded up
template <int N, int CurrentVal = N, int Count = 0> struct log2_up
{
    enum
    {
        value = log2_up<N, (CurrentVal >> 1), Count + 1>::value
    };
};
template <int N, int Count> struct log2_up<N, 1, Count>
{
    static_assert(N > 0, "invalid input domain.");
    enum
    {
        value = ((1 << Count) < N) ? Count + 1 : Count
    };
};

////////////////////////////////////////////////////////////////////////////////

// Safe multiplication for preventing overflow.
template <int A, int B> struct mul
{
    static const int Log2AbsA = log2_up<abs<A>::value>::value;
    static const int Log2AbsB = log2_up<abs<B>::value>::value;
    static_assert(Log2AbsA + Log2AbsB <= 31, "overflow detected");
    enum
    {
        value = A * B
    };
};

////////////////////////////////////////////////////////////////////////////////

// Integer division, rounded up
template <int A, int B> struct div_up
{
    enum
    {
        value = (A + B - 1) / B
    };
};

// Least multiple of B equal to or larger than A
template <int A, int B> struct lm
{
    enum
    {
        value = mul<div_up<A, B>::value, B>::value
    };
};

// Integer subtraction
template <int A, int B> struct sub
{
    enum
    {
        value = A - B
    };
};

// 1 if N is power of 2, otherwise 0
template <int N> struct is_pow2
{
    enum
    {
        value = N && (!(N & (N - 1)))
    };
};

//
template <int X, int Pad> struct pad
{
    enum
    {
        value = mul<div_up<X, Pad>::value, Pad>::value
    };
};

////////////////////////////////////////////////////////////////////////////////

// Helper of div.
template <int Divisor, bool IsPow2> struct Div
{
};

template <int Divisor> struct Div<Divisor, true>
{
    static DEVICE int compute(int x)
    {
        return x >> math::log2_up<Divisor>::value;
    }
};

template <int Divisor> struct Div<Divisor, false>
{
    static DEVICE int compute(int x)
    {
        return x / Divisor;
    }
};

// Fast division by pow2 divisor.
template <int Divisor> static DEVICE int div(int x)
{
    return Div<Divisor, math::is_pow2<Divisor>::value>::compute(x);
}

////////////////////////////////////////////////////////////////////////////////

// Helper of mod.
template <int Divisor, bool IsPow2> struct Mod
{
};

template <int Divisor> struct Mod<Divisor, true>
{
    static DEVICE int compute(int x)
    {
        return x & (Divisor - 1);
    }
};

template <int Divisor> struct Mod<Divisor, false>
{
    static DEVICE int compute(int x)
    {
        return x % Divisor;
    }
};

// Fast modulo by pow2 divisor.
template <int Divisor> static DEVICE int mod(int x)
{
    return Mod<Divisor, math::is_pow2<Divisor>::value>::compute(x);
}

////////////////////////////////////////////////////////////////////////////////

/// Greatest multiple of Divisor equal to or smaller than x
template <int Divisor> static DEVICE int gm(int x)
{
    return math::div<Divisor>(x) * Divisor;
}

} // namespace math
} // namespace ark

#endif // ARK_KERNELS_STATIC_MATH_H_

```

`ark/include/kernels/sync.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_SYNC_H_
#define ARK_KERNELS_SYNC_H_

#include "device.h"
#include "static_math.h"

namespace ark {

namespace sync {

struct State
{
    volatile int flag;
    int cnt;
    int is_add;
    int clks_cnt;
};

} // namespace sync

// Synchronize multiple thread blocks inside a kernel. Guarantee that all
// previous work of all threads in cooperating blocks is finished and
// visible to all threads in the device.
template <int BlockNum> DEVICE void sync_gpu(sync::State &state)
{
    constexpr int MaxOldCnt = BlockNum - 1;
#ifdef ARK_KERNELS_SYNC_CLKS_CNT
    static_assert(math::is_pow2<ARK_KERNELS_SYNC_CLKS_CNT>::value == 1, "");
    if (threadIdx.x == 0 && blockIdx.x == 0) {
        _ARK_CLKS[state.clks_cnt] = clock64();
        state.clks_cnt = (state.clks_cnt + 1) & (ARK_KERNELS_SYNC_CLKS_CNT - 1);
    }
#endif // ARK_KERNELS_SYNC_CLKS_CNT
    __syncthreads();
    if (BlockNum == 1) {
        return;
    }
    if (threadIdx.x == 0) {
        // Make sure that all threads in this block have done `__threadfence()`
        // before to flip `flag`.
        __threadfence();
        int is_add_ = state.is_add ^ 1;
        if (is_add_) {
            if (atomicAdd(&state.cnt, 1) == MaxOldCnt) {
                state.flag = 1;
            }
            while (!state.flag) {
            }
        } else {
            if (atomicSub(&state.cnt, 1) == 1) {
                state.flag = 0;
            }
            while (state.flag) {
            }
        }
        state.is_add = is_add_;
    }
    // We need sync here because only a single thread is checking whether
    // the flag is flipped.
    __syncthreads();
}

// Synchronize a group of warps.
// This function replaces `__syncthreads()` of legacy kernel implementations.
// It is needed because in normal practices to implement a kernel, each thread
// block typically processes a single unit task (e.g. one tile) so it is common
// to synchronize all co-working threads via `__syncthreads()`, however in our
// case, we often run multiple tasks or tiles in a single thread block, so we
// need a function which lets each tile to synchronize their own using threads
// only. Since `__syncthreads()` synchronize the entire threads in a thread
// block, we implement a finer-grained version of this via `barrier.sync` PTX
// instruction.
template <int ThreadsPerWarpGroup> DEVICE void sync_warps()
{
    static_assert(ThreadsPerWarpGroup == 32 || ThreadsPerWarpGroup == 64 ||
                      ThreadsPerWarpGroup == 128 ||
                      ThreadsPerWarpGroup == 256 ||
                      ThreadsPerWarpGroup == 512 || ThreadsPerWarpGroup == 1024,
                  "");
    // When ThreadsPerWarpGroup is 64, this function should not be called in
    // parallel with __syncthreads(). The following is the explanation why.
    // GPUs have 16 hardware barriers, numbered 0~15. This means that more than
    // sixteen `barrier.sync` instructions cannot run at the same time due to HW
    // limitation. This is reasonable because the maximum threads per block is
    // 1024 (32 warps), so we need at most 16 barriers, which happens in the
    // case when we synchronize warps in two-pairs. If we synchronize warps in
    // four-pairs, we need at most 8 barriers. The problem here is that
    // `__syncthreads()` always uses barrier 0, so if `__syncthreads()`
    // instruction is on flight, `sync_warps()` should not use barrier 0.
    // However, we cannot know whether `__syncthreads()` will be on flight or
    // not. So we have two options. Option 1: Let users take the risk and we
    // just use barrier 0, which enables to support 64 threads (sync in
    // two-pairs). In this case, users should make sure that their kernels never
    // issue `__syncthreads()` and `sync_warps()` at the same time, otherwise
    // the kernel may stop unexpectedly during runtime. Option 2: Do not use
    // barrier 0 to be more safe, instead we cannot support 64 threads. Here we
    // select the first option.
    if (ThreadsPerWarpGroup == 32) {
        __syncwarp();
    } else if (ThreadsPerWarpGroup == 64) {
        asm volatile("barrier.sync %0, 64;" ::"r"((threadIdx.x >> 6)));
    } else if (ThreadsPerWarpGroup == 128) {
        asm volatile("barrier.sync %0, 128;" ::"r"((threadIdx.x >> 7) + 8));
    } else if (ThreadsPerWarpGroup == 256) {
        asm volatile("barrier.sync %0, 256;" ::"r"((threadIdx.x >> 8) + 8));
    } else if (ThreadsPerWarpGroup == 512) {
        asm volatile("barrier.sync %0, 512;" ::"r"((threadIdx.x >> 9) + 8));
    } else if (ThreadsPerWarpGroup == 1024) {
        // If we sync 1024 threads, it means we sync all threads in a thread
        // block because the maximum number of threads per block is 1024 in
        // all NVIDIA devices. Therefore, we do not check `threadIdx.x` and
        // just use barrier 8.
        asm volatile("barrier.sync 8, 1024;");
    }
}

} // namespace ark

#endif // ARK_KERNELS_SYNC_H_

```

`ark/include/kernels/transpose.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_TRANSPOSE_H_
#define ARK_KERNELS_TRANSPOSE_H_

#include "ewise.h"

namespace ark {

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose0132
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_n * InDims::CHW + idx_c * InDims::HW + idx_w * InDims::W +
              idx_h;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::W];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose0213
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_n * InDims::CHW + idx_h * InDims::HW + idx_c * InDims::W +
              idx_w;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose0231
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_n * InDims::CHW + idx_h * InDims::HW + idx_w * InDims::W +
              idx_c;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::W];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose0312
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_n * InDims::CHW + idx_w * InDims::HW + idx_c * InDims::W +
              idx_h;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::HW];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose0321
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_n * InDims::CHW + idx_w * InDims::HW + idx_h * InDims::W +
              idx_c;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::HW];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose1023
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_c * InDims::CHW + idx_n * InDims::HW + idx_h * InDims::W +
              idx_w;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose1032
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_c * InDims::CHW + idx_n * InDims::HW + idx_w * InDims::W +
              idx_h;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::W];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose1203
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_c * InDims::CHW + idx_h * InDims::HW + idx_n * InDims::W +
              idx_w;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose1230
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_c * InDims::CHW + idx_h * InDims::HW + idx_w * InDims::W +
              idx_n;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::W];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose1302
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_c * InDims::CHW + idx_w * InDims::HW + idx_n * InDims::W +
              idx_h;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::HW];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose1320
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_c * InDims::CHW + idx_w * InDims::HW + idx_h * InDims::W +
              idx_n;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::HW];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose2013
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_h * InDims::CHW + idx_n * InDims::HW + idx_c * InDims::W +
              idx_w;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose2031
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_h * InDims::CHW + idx_n * InDims::HW + idx_w * InDims::W +
              idx_c;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::W];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose2103
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_h * InDims::CHW + idx_c * InDims::HW + idx_n * InDims::W +
              idx_w;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose2130
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_h * InDims::CHW + idx_c * InDims::HW + idx_w * InDims::W +
              idx_n;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::W];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose2301
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_h * InDims::CHW + idx_w * InDims::HW + idx_n * InDims::W +
              idx_c;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::HW];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose2310
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_h * InDims::CHW + idx_w * InDims::HW + idx_c * InDims::W +
              idx_n;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::HW];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose3012
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_w * InDims::CHW + idx_n * InDims::HW + idx_c * InDims::W +
              idx_h;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::CHW];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose3021
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_w * InDims::CHW + idx_n * InDims::HW + idx_h * InDims::W +
              idx_c;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::CHW];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose3102
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_w * InDims::CHW + idx_c * InDims::HW + idx_n * InDims::W +
              idx_h;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::CHW];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose3120
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_w * InDims::CHW + idx_c * InDims::HW + idx_h * InDims::W +
              idx_n;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::CHW];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose3201
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_w * InDims::CHW + idx_h * InDims::HW + idx_n * InDims::W +
              idx_c;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::CHW];
        }
    }
};

template <typename _InDims, typename _OutDims, typename _DataType,
          int _NelemPerThread>
struct Transpose3210
{
    using InDims = _InDims;
    using OutDims = _OutDims;
    using DataType = _DataType;
    static const int NelemPerThread = _NelemPerThread;

    static DEVICE void compute(DataType *out, DataType *in, int idx_n,
                               int idx_c, int idx_h, int idx_w)
    {
        out += idx_n * OutDims::CHW + idx_c * OutDims::HW + idx_h * OutDims::W +
               idx_w;
        //
        in += idx_w * InDims::CHW + idx_h * InDims::HW + idx_c * InDims::W +
              idx_n;
        *out = *in;
#pragma unroll
        for (int i = 1; i < NelemPerThread; ++i) {
            out[i] = in[i * InDims::CHW];
        }
    }
};

////////////////////////////////////////////////////////////////////////////////

template <typename InDims, typename OutDims, typename OutShape,
          typename UnitOutDims, int NumThreads, int SmemBytes,
          typename Transpose>
DEVICE void _transpose(float *out, float *in, int uop_idx)
{
    Ewise1<OutDims, OutShape, UnitOutDims, NumThreads, SmemBytes,
           Transpose>::run(out, in, uop_idx);
}

// TODO: we need to use NelemPerThread=2 for half in the future, if out is a
// __half pointer, this can cause a memory bug, because GPU DRAM access should
// be always 4-byte aligned
template <typename InDims, typename OutDims, typename OutShape,
          typename UnitOutDims, int NumThreads, int SmemBytes,
          typename Transpose>
DEVICE void _transpose(ark::half *out, ark::half *in, int uop_idx)
{
    Ewise1<OutDims, OutShape, UnitOutDims, NumThreads, SmemBytes,
           Transpose>::run(out, in, uop_idx);
}

#define _DEC_TRANSPOSE(tp_type)                                                \
    template <typename InDims, typename OutDims, typename OutShape,            \
              typename UnitOutDims, int NumThreads, int SmemBytes,             \
              typename DataType>                                               \
    DEVICE void transpose##tp_type(DataType *out, DataType *in, int uop_idx,   \
                                   int)                                        \
    {                                                                          \
        _transpose<InDims, OutDims, OutShape, UnitOutDims, NumThreads,         \
                   SmemBytes,                                                  \
                   Transpose##tp_type<InDims, OutDims, DataType, 1>>(out, in,  \
                                                                     uop_idx); \
    }

_DEC_TRANSPOSE(0132)
_DEC_TRANSPOSE(0213)
_DEC_TRANSPOSE(0231)
_DEC_TRANSPOSE(0312)
_DEC_TRANSPOSE(0321)
_DEC_TRANSPOSE(1023)
_DEC_TRANSPOSE(1032)
_DEC_TRANSPOSE(1203)
_DEC_TRANSPOSE(1230)
_DEC_TRANSPOSE(1302)
_DEC_TRANSPOSE(1320)
_DEC_TRANSPOSE(2013)
_DEC_TRANSPOSE(2031)
_DEC_TRANSPOSE(2103)
_DEC_TRANSPOSE(2130)
_DEC_TRANSPOSE(2301)
_DEC_TRANSPOSE(2310)
_DEC_TRANSPOSE(3012)
_DEC_TRANSPOSE(3021)
_DEC_TRANSPOSE(3102)
_DEC_TRANSPOSE(3120)
_DEC_TRANSPOSE(3201)
_DEC_TRANSPOSE(3210)

} // namespace ark

#endif // ARK_KERNELS_TRANSPOSE_H_

```

`ark/include/kernels/unit_op.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_UNIT_OP_H_
#define ARK_KERNELS_UNIT_OP_H_

#include "device.h"
#include "smem.h"
#include "static_math.h"
#include "sync.h"
#include "vec.h"

namespace ark {

/// Helper for defining unit operators. Follows the NCHW tensor layout.
///
/// @tparam _OutDims Shape of the actual output data layout. Each dimension
/// should be divided by the corresponding dimension in @ref _UnitOutDims.
/// @tparam _OutShape Shape of the output data that the entire operator is
/// operating on. This is usually the same as @ref _OutDims, but can be smaller
/// for operators that operate on a subset of the data.
/// @tparam _UnitOutDims Shape of the output data that a unit operator is
/// operating on. Shape of the output data that a unit operator is operating on.
/// Each dimension should be equal to or smaller than the corresponding
/// dimension in @ref _OutShape. Even if each dimension in @ref _OutShape is not
/// divided by the corresponding dimension in @ref _UnitOutDims, @ref UnitOp
/// provides no mechanism for informing unit operators whether they are
/// operating on the boundary of the data or not. Instead, it should be managed
/// by the underlying implementation.
/// @tparam _NumThreads Number of threads that a unit operator is using.
/// @tparam _SmemBytes Bytes of shared memory that a unit operator is using.
///
template <typename _OutDims, typename _OutShape, typename _UnitOutDims,
          int _NumThreads, int _SmemBytes>
struct UnitOp
{
    static_assert(_OutDims::N >= _OutShape::N,
                  "Dimension N is smaller than tensor shape");
    static_assert(_OutDims::C >= _OutShape::C,
                  "Dimension C is smaller than tensor shape");
    static_assert(_OutDims::H >= _OutShape::H,
                  "Dimension H is smaller than tensor shape");
    static_assert(_OutDims::W >= _OutShape::W,
                  "Dimension W is smaller than tensor shape");

    static_assert(_UnitOutDims::N > 0,
                  "Unit dimension is not positive in dimension N");
    static_assert(_UnitOutDims::C > 0,
                  "Unit dimension is not positive in dimension C");
    static_assert(_UnitOutDims::H > 0,
                  "Unit dimension is not positive in dimension H");
    static_assert(_UnitOutDims::W > 0,
                  "Unit dimension is not positive in dimension W");

    static_assert(_OutDims::N % _UnitOutDims::N == 0,
                  "Dimension N is not divisible by the unit dimension");
    static_assert(_OutDims::C % _UnitOutDims::C == 0,
                  "Dimension C is not divisible by the unit dimension");
    static_assert(_OutDims::H % _UnitOutDims::H == 0,
                  "Dimension H is not divisible by the unit dimension");
    static_assert(_OutDims::W % _UnitOutDims::W == 0,
                  "Dimension W is not divisible by the unit dimension");

    static_assert(_NumThreads > 0, "# of threads is not positive");
    static_assert(_SmemBytes >= 0, "Bytes of shared memory is negative");

    // Number of unit operators in each dimension.
    using UnitOpDims =
        Vec<_OutDims::N / _UnitOutDims::N, _OutDims::C / _UnitOutDims::C,
            _OutDims::H / _UnitOutDims::H, _OutDims::W / _UnitOutDims::W>;

    static const int NumThreads = _NumThreads;
    static const int SmemBytes = _SmemBytes;

    /// Do not use `threadIdx` and use this function instead.
    static DEVICE int thread_id()
    {
        return math::mod<NumThreads>(threadIdx.x);
    }

    /// Convert a unit operator ID to the corresponding index along the N
    /// dimension.
    /// @param uop_id Unit operator ID.
    static DEVICE int uop_idx_n(int uop_id)
    {
        return uop_id / UnitOpDims::CHW;
    }

    /// Convert a unit operator ID to the corresponding index along the C
    /// dimension.
    /// @param uop_id Unit operator ID.
    static DEVICE int uop_idx_c(int uop_id)
    {
        return (uop_id / UnitOpDims::HW) % UnitOpDims::C;
    }

    /// Convert a unit operator ID to the corresponding index along the H
    /// dimension.
    /// @param uop_id Unit operator ID.
    static DEVICE int uop_idx_h(int uop_id)
    {
        return (uop_id / UnitOpDims::W) % UnitOpDims::H;
    }

    /// Convert a unit operator ID to the corresponding index along the W
    /// dimension.
    /// @param uop_id Unit operator ID.
    static DEVICE int uop_idx_w(int uop_id)
    {
        return uop_id % UnitOpDims::W;
    }

    /// Return a shared memory pointer.
    /// @tparam T Type of the underlying data.
    /// @param smem_per_warp Bytes of shared memory per warp.
    template <typename T> static DEVICE T *shared_memory(int smem_per_warp)
    {
        static_assert(sizeof(T) <= SmemBytes,
                      "Shared memory is not large enough");
        return SharedMemory<T, NumThreads>::get(smem_per_warp);
    }

    /// Do not use `__syncthreads()` and use this function instead.
    static DEVICE void sync_threads()
    {
        sync_warps<NumThreads>();
    }
};

} // namespace ark

#endif // ARK_KERNELS_UNIT_OP_H_

```

`ark/include/kernels/vec.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KERNELS_VEC_H_
#define ARK_KERNELS_VEC_H_

#include "static_math.h"

namespace ark {

using DimType = int;

template <DimType _D0 = 1, DimType _D1 = 1, DimType _D2 = 1, DimType _D3 = 1>
struct Vec
{
    static_assert(_D0 >= 0, "");
    static_assert(_D1 >= 0, "");
    static_assert(_D2 >= 0, "");
    static_assert(_D3 >= 0, "");

    // 4D representation.
    static const DimType D0 = _D0;
    static const DimType D1 = _D1;
    static const DimType D2 = _D2;
    static const DimType D3 = _D3;
    static const DimType N = _D0;
    static const DimType C = _D1;
    static const DimType H = _D2;
    static const DimType W = _D3;

    // 3D representation.
    static const DimType X = _D0;
    static const DimType Y = _D1;
    static const DimType Z = _D2;

    // Multiplied values.
    static const DimType NCHW =
        math::mul<N, math::mul<C, math::mul<H, W>::value>::value>::value;
    static const DimType NCH = math::mul<N, math::mul<C, H>::value>::value;
    static const DimType NCW = math::mul<N, math::mul<C, W>::value>::value;
    static const DimType NHW = math::mul<N, math::mul<H, W>::value>::value;
    static const DimType CHW = math::mul<C, math::mul<H, W>::value>::value;
    static const DimType NC = math::mul<N, C>::value;
    static const DimType NH = math::mul<N, H>::value;
    static const DimType NW = math::mul<N, W>::value;
    static const DimType CH = math::mul<C, H>::value;
    static const DimType CW = math::mul<C, W>::value;
    static const DimType HW = math::mul<H, W>::value;
};

template <typename Vec1, typename Vec2> struct VecIsEq
{
    enum
    {
        value = (Vec1::D0 == Vec2::D0 && Vec1::D1 == Vec2::D1 &&
                 Vec1::D2 == Vec2::D2 && Vec1::D3 == Vec2::D3)
    };
};

} // namespace ark

#endif // ARK_KERNELS_VEC_H_

```

`ark/ipc/ipc_coll.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <cstring>

#include "cpu_timer.h"
#include "ipc/ipc_coll.h"
#include "logging.h"

namespace ark {

// Constructor.
IpcAllGather::IpcAllGather(const std::string &name_, int rank_, int size_,
                           const void *addr_, std::size_t bytes_)
    : rank{rank_}, size{size_}, bytes{bytes_}
{
    this->mem = new IpcMem{name_, false, true};
    char *ptr = (char *)this->mem->alloc(bytes_ * size_ + 8);
    if (addr_ != nullptr) {
        void *data = ptr + rank_ * bytes_;
        std::memcpy(data, addr_, bytes_);
    }
}

// Desctructor.
IpcAllGather::~IpcAllGather()
{
    delete this->mem;
}

void IpcAllGather::sync()
{
    char *ptr = (char *)this->mem->get_addr();
    volatile int *cnt = (volatile int *)(ptr + this->bytes * this->size);
    volatile int *flag = cnt + 1;
    int is_dec;
    {
        IpcLockGuard lg{this->mem->get_lock()};
        int old = *cnt;
        is_dec = *flag;
        if (is_dec) {
            *cnt = old - 1;
            if (old == 1) {
                *flag = 0;
            }
        } else {
            *cnt = old + 1;
            if (old == this->size - 1) {
                *flag = 1;
            }
        }
    }
    if (is_dec) {
        while (*flag == 1) {
            cpu_ntimer_sleep(1e6);
        }
    } else {
        while (*flag == 0) {
            cpu_ntimer_sleep(1e6);
        }
    }
}

void *IpcAllGather::get_data(int rank_) const
{
    return (char *)this->mem->get_addr() + rank_ * this->bytes;
}

} // namespace ark

```

`ark/ipc/ipc_coll.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_IPC_COLL_H_
#define ARK_IPC_COLL_H_

#include "ipc/ipc_mem.h"

namespace ark {

//
class IpcAllGather
{
  public:
    // Constructor.
    IpcAllGather(const std::string &name, int rank, int size, const void *addr,
                 std::size_t bytes);
    // Desctructor.
    ~IpcAllGather();
    //
    void sync();
    // Get data of the given rank.
    void *get_data(int rank_) const;

  private:
    IpcMem *mem;
    int rank;
    int size;
    // Per-rank bytes.
    std::size_t bytes;
};

} // namespace ark

#endif // ARK_IPC_COLL_H_

```

`ark/ipc/ipc_coll_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "include/ark.h"
#include "include/ark_utils.h"
#include "ipc/ipc_coll.h"
#include "unittest/unittest_utils.h"

using namespace ark;
using namespace std;

unittest::State test_ipc_coll_allgather()
{
    // Launch 100 workers.
    vector<int> pids;
    for (int i = 0; i < 100; ++i) {
        int pid = ark::utils::proc_spawn([i]() {
            unittest::Timeout timeout{5};
            int rank = i;
            int size = 100;
            int data = rank + 1;
            IpcAllGather iag{"ipc_coll_allgather_test", rank, size,
                             (const void *)&data, sizeof(data)};
            iag.sync();
            for (int i = 0; i < size; ++i) {
                int *ptr = (int *)iag.get_data(i);
                UNITTEST_EQ(*ptr, i + 1);
            }
            return 0;
        });
        UNITTEST_NE(pid, -1);
        pids.emplace_back(pid);
    }
    // Wait until all workers finish.
    int ret = ark::utils::proc_wait(pids);
    UNITTEST_EQ(ret, 0);
    return unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_ipc_coll_allgather);
    return 0;
}

```

`ark/ipc/ipc_hosts.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <arpa/inet.h>
#include <cstring>
#include <netdb.h>
#include <vector>

#include "env.h"
#include "ipc/ipc_hosts.h"
#include "logging.h"

namespace ark {

static std::vector<std::string> hosts;

const std::string &get_host(int idx)
{
    if (hosts.size() == 0) {
        const char *hostfile = get_env().hostfile.c_str();
        FILE *fp = fopen(hostfile, "r");
        if (fp == nullptr) {
            LOG(WARN, "cannot open hostfile: ", hostfile, ", assume localhost");
            hosts.push_back("127.0.0.1");
        } else {
            char buf[1024];
            buf[1023] = 0;
            int host_idx = 0;
            while (fgets(buf, sizeof(buf), fp) != nullptr) {
                if (buf[1023] != 0) {
                    LOGERR("hostfile line too long: ", buf);
                }
                // Erase the newline character
                int l = strlen(buf);
                buf[l - 1] = 0;
                // Hostname to IP
                struct hostent *ent = gethostbyname(buf);
                if (ent == nullptr) {
                    LOGERR("cannot resolve hostname: ", buf);
                }
                char *host = inet_ntoa(*(struct in_addr *)ent->h_addr);
                LOG(INFO, "HOST ", host_idx, ": ", host);
                hosts.emplace_back(host);
                host_idx++;
            }
        }
    }
    if ((idx < 0) || (idx >= (int)hosts.size())) {
        LOGERR("invalid host index: ", idx);
    }
    return hosts[idx];
}

} // namespace ark

```

`ark/ipc/ipc_hosts.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_IPC_HOSTS_H_
#define ARK_IPC_HOSTS_H_

#include <string>

namespace ark {

const std::string &get_host(int idx);

} // namespace ark

#endif // ARK_IPC_HOSTS_H_

```

`ark/ipc/ipc_lock.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <cassert>

#include "ipc_lock.h"
#include "logging.h"

namespace ark {

// Initialize the lock.
int ipc_lock_init(IpcLock *lock)
{
    assert(lock != nullptr);
    int ret;
    pthread_mutexattr_t attr;
    ret = pthread_mutexattr_init(&attr);
    if (ret != 0) {
        return ret;
    }
    ret = pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_RECURSIVE);
    if (ret != 0) {
        return ret;
    }
    ret = pthread_mutexattr_setpshared(&attr, PTHREAD_PROCESS_SHARED);
    if (ret != 0) {
        return ret;
    }
    ret = pthread_mutex_init(&(lock->mtx), &attr);
    if (ret != 0) {
        return ret;
    }
    // Acquire this lock before setting `is_init`.
    ret = pthread_mutex_lock(&(lock->mtx));
    if (ret != 0) {
        return ret;
    }
    lock->is_init = true;
    return 0;
}

// Destroy the lock.
int ipc_lock_destroy(IpcLock *lock)
{
    assert(lock != nullptr);
    // Regardless of whether the destruction successes or not,
    // do not reuse this lock.
    lock->is_init = false;
    return pthread_mutex_destroy(&(lock->mtx));
}

// Acquire the lock.
int ipc_lock_acquire(IpcLock *lock)
{
    assert(lock != nullptr);
    return pthread_mutex_lock(&(lock->mtx));
}

// Release the lock.
int ipc_lock_release(IpcLock *lock)
{
    assert(lock != nullptr);
    return pthread_mutex_unlock(&(lock->mtx));
}

////////////////////////////////////////////////////////////////////////////////

// Constructor.
IpcLockGuard::IpcLockGuard(IpcLock *lock_) : lock{lock_}
{
    assert(lock_ != nullptr);
    assert(lock->is_init == true);
    int r = ipc_lock_acquire(lock_);
    if (r != 0) {
        LOG(ERROR, "ipc_lock_acquire failed (errno ", r, ")");
    }
}

// Desctructor.
IpcLockGuard::~IpcLockGuard()
{
    ipc_lock_release(this->lock);
}

} // namespace ark

```

`ark/ipc/ipc_lock.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_IPC_LOCK_H_
#define ARK_IPC_LOCK_H_

#include <pthread.h>

namespace ark {

// IPC lock.
struct IpcLock
{
    // Mutex lock.
    pthread_mutex_t mtx;
    // True if this lock is initialized.
    bool is_init = false;
};

// Initialize and acquire the lock immediately.
int ipc_lock_init(IpcLock *lock);
// Destroy the lock.
int ipc_lock_destroy(IpcLock *lock);
// Acquire the lock.
int ipc_lock_acquire(IpcLock *lock);
// Release the lock.
int ipc_lock_release(IpcLock *lock);

// IPC lock guard.
class IpcLockGuard
{
  public:
    // Constructor.
    IpcLockGuard(IpcLock *lock);
    // Desctructor.
    ~IpcLockGuard();

  private:
    // Lock to guard.
    IpcLock *lock;
};

} // namespace ark

#endif // ARK_IPC_LOCK_H_

```

`ark/ipc/ipc_mem.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <cassert>
#include <cstring>
#include <sys/mman.h>
#include <sys/stat.h>
#include <sys/types.h>
#include <unistd.h>

#include "cpu_timer.h"
#include "env.h"
#include "ipc/ipc_mem.h"
#include "ipc/ipc_shm.h"
#include "logging.h"

using namespace std;

#define NAME_LOCK_POSTFIX ".lock"
#define NAME_DATA_POSTFIX ".data"

namespace ark {

// Constructor.
//
// When the producer is statically determined:
//   Producer:  create_=true,   try_create=any
//   Consumer:  create_=false,  try_create=false
//
// To elect a producer:
//   Everybody: create_=false,  try_create=true
//
// NOTE: To elect the producer, we need to make sure that the lock file name
// does not already exist, otherwise it will cause a deadlock.
//
IpcMem::IpcMem(const string &name_, bool create_, bool try_create)
    : name{name_}, create{create_}
{
    assert(name_.size() > 0);
    string lock_name_str =
        get_env().shm_name_prefix + name_ + NAME_LOCK_POSTFIX;
    const char *lock_name = lock_name_str.c_str();
    int fd;
    if (create_ || try_create) {
        // Try creating a lock file.
        fd = ipc_shm_create(lock_name);
        if (fd != -1) {
            // Succeed.
            int r = ftruncate(fd, sizeof(IpcLock));
            if (r != 0) {
                LOG(ERROR, "ftruncate failed (errno ", r, ")");
            }
            create_ = true;
        } else if (create_) {
            fd = ipc_shm_open(lock_name);
            assert(fd != -1);
            int r = ftruncate(fd, sizeof(IpcLock));
            if (r != 0) {
                LOG(ERROR, "ftruncate failed (errno ", r, ")");
            }
        }
    }
    if (!create_) {
        // Wait until we can open the lock file.
        fd = ipc_shm_open_blocking(lock_name);
        assert(fd != -1);
        struct stat s;
        for (;;) {
            int r = fstat(fd, &s);
            if (r != 0) {
                LOG(ERROR, "fstat failed (errno ", r, ")");
            }
            if (s.st_size > 0) {
                assert(s.st_size == sizeof(IpcLock));
                break;
            }
            // Wait until the creator finishes `ftruncate()`.
            cpu_ntimer_sleep(1000);
        }
    }
    // Get mmap of the lock.
    this->lock = (IpcLock *)mmap(0, sizeof(IpcLock), PROT_READ | PROT_WRITE,
                                 MAP_SHARED, fd, 0);
    assert(this->lock != MAP_FAILED);
    close(fd);
    if (create_) {
        // Initialize and acquire the lock.
        int r = ipc_lock_init(this->lock);
        if (r != 0) {
            LOG(ERROR, "ipc_lock_init failed (errno ", r, ")");
        }
        // Release the lock immediately.
        r = ipc_lock_release(this->lock);
        if (r != 0) {
            LOG(ERROR, "ipc_lock_release failed (errno ", r, ")");
        }
    } else {
        // Wait until the lock is initialized.
        // This must finish shortly, so we just wait polling.
        while (!this->lock->is_init) {
            cpu_ntimer_sleep(1000);
        }
    }
    this->create = create_;
}

// Destructor.
IpcMem::~IpcMem()
{
    if (this->lock != nullptr) {
        if (this->create) {
            string lock_name =
                get_env().shm_name_prefix + this->name + NAME_LOCK_POSTFIX;
            shm_unlink(lock_name.c_str());
        }
        munmap(this->lock, sizeof(IpcLock));
    }
    if (this->addr != nullptr) {
        if (this->create) {
            string data_name =
                get_env().shm_name_prefix + this->name + NAME_DATA_POSTFIX;
            shm_unlink(data_name.c_str());
        }
        munmap(this->addr, this->total_bytes);
    }
}

// Allocate/re-allocate the shared memory space of the data file.
// Return the current mmapped address if the given `bytes`
// is equal to or less than `total_bytes`.
void *IpcMem::alloc(size_t bytes)
{
    assert((bytes != 0) || !this->create);
    if ((bytes != 0) && (bytes <= this->total_bytes)) {
        // If `total_bytes` is zero, nullptr is returned.
        return this->addr;
    }
    // Open the data file.
    string data_name_str =
        get_env().shm_name_prefix + this->name + NAME_DATA_POSTFIX;
    const char *data_name = data_name_str.c_str();
    int fd;
    if ((this->total_bytes == 0) && this->create) {
        // Create an empty data file.
        fd = ipc_shm_create(data_name);
        if (fd == -1) {
            fd = ipc_shm_open(data_name);
            if (fd == -1) {
                LOGERR("ipc_shm_open: ", strerror(errno), " (", errno, ")");
            }
        }
    } else if (this->create) {
        // Open the existing data file.
        fd = ipc_shm_open(data_name);
        if (fd == -1) {
            LOGERR("ipc_shm_open: ", strerror(errno), " (", errno, ")");
        }
    } else {
        // Wait until the data file appears.
        fd = ipc_shm_open_blocking(data_name);
        if (fd == -1) {
            LOGERR("ipc_shm_open_blocking: ", strerror(errno), " (", errno,
                   ")");
        }
    }
    if (this->create) {
        assert(bytes > 0);
        // Truncate the file size.
        int r = ftruncate(fd, bytes);
        if (r != 0) {
            LOG(ERROR, "ftruncate failed (errno ", r, ")");
        }
    } else {
        // Wait until the file size becomes equal to or larger than `bytes`.
        // We assume that in most cases the file size is already as large as
        // `bytes` or is going to be so very soon.
        // NOTE: this method cannot prevent race conditions if the data file
        // size may decrease.
        struct stat s;
        for (;;) {
            int r = fstat(fd, &s);
            if (r != 0) {
                LOG(ERROR, "fstat failed (errno ", r, ")");
            }
            if ((bytes == 0) && (s.st_size > 0)) {
                bytes = s.st_size;
                break;
            } else if ((bytes > 0) && (size_t)s.st_size >= bytes) {
                break;
            }
            // Wait until the creator finishes `ftruncate()`.
            cpu_ntimer_sleep(1000);
        }
    }
    // Create a new mmap.
    void *old = this->addr;
    this->addr = mmap(0, bytes, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0);
    assert(this->addr != MAP_FAILED);
    // Remove the old mmap.
    if (old != nullptr) {
        munmap(old, this->total_bytes);
    }
    close(fd);
    this->total_bytes = bytes;
    return this->addr;
}

} // namespace ark

```

`ark/ipc/ipc_mem.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_IPC_ENTRY_H_
#define ARK_IPC_ENTRY_H_

#include <string>

#include "ipc/ipc_lock.h"

namespace ark {

// A single IpcMem consists of a data file and a lock file
// to ensure atomic read/write on the data file.
class IpcMem
{
  public:
    // Constructor.
    IpcMem(const std::string &name, bool create, bool try_create = false);
    // Destructor.
    ~IpcMem();

    // Allocate/re-allocate the shared memory space of the data file.
    // Return the current mmapped address if the given `bytes`
    // is less than `total_bytes`.
    void *alloc(std::size_t bytes);
    // Return the lock file mmap address.
    IpcLock *get_lock() const
    {
        return this->lock;
    }
    // Return the current mmap address.
    void *get_addr() const
    {
        return this->addr;
    }
    // Get the bytes of the mmapped memory space of the data file.
    std::size_t get_bytes() const
    {
        return total_bytes;
    }
    // Return true if object is the data creator.
    bool is_create() const
    {
        return create;
    }

  private:
    const std::string name;
    // If true, this object will create shared object files and
    // may change their sizes or destroy them.
    // If false, this object will busy wait for the creator if those
    // files do not exist or when their sizes are not as expected.
    bool create;
    // Pointer to the mmapped memory space of the lock file.
    IpcLock *lock = nullptr;
    // Pointer to the mmapped memory space of the data file.
    void *addr = nullptr;
    // Size of the mmapped memory space of the data file.
    std::size_t total_bytes = 0;
};

} // namespace ark

#endif // ARK_IPC_ENTRY_H_

```

`ark/ipc/ipc_mem_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "include/ark.h"
#include "include/ark_utils.h"
#include "ipc/ipc_mem.h"
#include "unittest/unittest_utils.h"

using namespace ark;
using namespace std;

unittest::State test_ipc_mem_lock_simple()
{
    function<int()> fc = [] {
        unittest::Timeout timeout{3};
        IpcMem im{"ipc_mem_lock_test", true};
        volatile int *ptr = (volatile int *)im.alloc(4);
        while (*ptr == 0) {
        }
        while (*ptr == 1) {
        }
        UNITTEST_EQ(*ptr, 2);
        return 0;
    };
    function<int()> fn = [] {
        unittest::Timeout timeout{3};
        IpcMem im{"ipc_mem_lock_test", false};
        volatile int *ptr = (volatile int *)im.alloc(4);
        IpcLockGuard lg{im.get_lock()};
        *ptr += 1;
        return 0;
    };

    for (int i = 0; i < 10; ++i) {
        int idx = i % 3;
        int pid0 = ark::utils::proc_spawn(idx == 0 ? fc : fn);
        UNITTEST_NE(pid0, -1);
        int pid1 = ark::utils::proc_spawn(idx == 1 ? fc : fn);
        UNITTEST_NE(pid1, -1);
        int pid2 = ark::utils::proc_spawn(idx == 2 ? fc : fn);
        UNITTEST_NE(pid2, -1);
        int ret = ark::utils::proc_wait({pid0, pid1, pid2});
        UNITTEST_EQ(ret, 0);
    }
    return unittest::SUCCESS;
}

unittest::State test_ipc_mem_lock_many()
{
    function<int()> worker = [] {
        unittest::Timeout timeout{3};
        // Elect the earliest starting worker as the creator.
        IpcMem im{"ipc_mem_lock_test_many", false, true};
        volatile int *ptr = (volatile int *)im.alloc(8);
        volatile int *data = &ptr[0];
        volatile int *counter = &ptr[1];
        // Each worker increases the shared data by 10000.
        for (int i = 0; i < 10000; ++i) {
            IpcLockGuard lg{im.get_lock()};
            *data += 1;
        }
        {
            // Count finished workers.
            IpcLockGuard lg{im.get_lock()};
            *counter += 1;
        }
        if (im.is_create()) {
            // Wait until all other workers finish.
            while (*counter != 100) {
            }
            // Validate the result.
            UNITTEST_EQ(*data, 1000000);
        }
        return 0;
    };

    // Launch 100 workers.
    vector<int> pids;
    for (int i = 0; i < 100; ++i) {
        int pid = ark::utils::proc_spawn(worker);
        UNITTEST_NE(pid, -1);
        pids.emplace_back(pid);
    }
    // Wait until all workers finish.
    int ret = ark::utils::proc_wait(pids);
    UNITTEST_EQ(ret, 0);
    return unittest::SUCCESS;
}

unittest::State test_ipc_mem_finishing()
{
    int pid0 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{3};
        IpcMem im{"ipc_mem_finishing", true};
        volatile int *ptr = (volatile int *)im.alloc(4);
        ptr[0] = 7;
        // Wait until another process reads the results.
        while (ptr[0] == 7) {
        }
        // Modify the data.
        ptr[0] = 77;
        // Just return without waiting for another process.
        return 0;
    });
    UNITTEST_NE(pid0, -1);

    int pid1 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{3};
        IpcMem im{"ipc_mem_finishing", false};
        volatile int *ptr = (volatile int *)im.alloc(4);
        while (ptr[0] != 7) {
        }
        // Notification.
        ptr[0] = -1;
        // Wait for a while until the `f0` process completes.
        cpu_timer_sleep(0.1);
        // Read the modified data.
        // This should work even though `f0` is already returned.
        while (ptr[0] != 77) {
        }
        return 0;
    });
    UNITTEST_NE(pid1, -1);

    int ret = ark::utils::proc_wait({pid0, pid1});
    UNITTEST_EQ(ret, 0);
    return unittest::SUCCESS;
}

unittest::State test_ipc_mem_realloc()
{
    int pid0 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{3};
        IpcMem im{"ipc_mem_realloc", true};
        volatile int *ptr = (volatile int *)im.alloc(4);
        ptr[0] = 7;
        ptr = (volatile int *)im.alloc(8);
        while (ptr[0] == 7) {
        }
        ptr[0] = 77;
        ptr[1] = 88;
        return 0;
    });
    UNITTEST_NE(pid0, -1);

    int pid1 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{3};
        IpcMem im{"ipc_mem_realloc", false};
        volatile int *ptr = (volatile int *)im.alloc(4);
        while (ptr[0] != 7) {
        }
        ptr = (volatile int *)im.alloc(8);
        ptr[0] = -1;
        cpu_timer_sleep(0.1);
        while (ptr[0] != 77) {
        }
        while (ptr[1] != 88) {
        }
        return 0;
    });
    UNITTEST_NE(pid1, -1);

    int ret = ark::utils::proc_wait({pid0, pid1});
    UNITTEST_EQ(ret, 0);
    return unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_ipc_mem_lock_simple);
    UNITTEST(test_ipc_mem_lock_many);
    UNITTEST(test_ipc_mem_finishing);
    UNITTEST(test_ipc_mem_realloc);
    return 0;
}

```

`ark/ipc/ipc_shm.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <cassert>
#include <cstring>
#include <errno.h>
#include <fcntl.h>
#include <poll.h>
#include <stdexcept>
#include <string>
#include <sys/inotify.h>
#include <sys/mman.h>
#include <sys/stat.h>
#include <unistd.h>

#include "ipc/ipc_shm.h"
#include "logging.h"

#define SHM_DIR "/dev/shm/"
#define SHM_MODE 0666

using namespace std;

namespace ark {

// Create a shm file if it does not exist, and truncate it to zero bytes.
// If the creation fails, return -1.
int ipc_shm_create(const char *name)
{
    return shm_open(name, O_RDWR | O_CREAT | O_EXCL, SHM_MODE);
}

// Try opening a shm file.
// Return its file descriptor on success, otherwise return -1.
int ipc_shm_open(const char *name)
{
    return shm_open(name, O_RDWR, SHM_MODE);
}

// Open a shm file and return its file descriptor.
// If opening fails due to non-existence, block until it can open.
// If opening fails due to any other reasons, return -1.
int ipc_shm_open_blocking(const char *name)
{
    // Monitor file creations in the shm directory.
    int ifd = inotify_init1(IN_NONBLOCK);
    if (ifd == -1) {
        LOGERR("inotify_init1: ", strerror(errno), " (", errno, ")");
    }
    if (inotify_add_watch(ifd, SHM_DIR, IN_CREATE) == -1) {
        close(ifd);
        LOGERR("inotify_add_watch: ", strerror(errno), " (", errno, ")");
    }
    // Check whether the file already exists.
    // NOTE: `inotify_add_watch()` should come before `shm_open()`
    // to avoid race conditions.
    int fd = shm_open(name, O_RDWR, SHM_MODE);
    if ((fd != -1) || (errno != ENOENT)) {
        close(ifd);
        return fd;
    }
    pollfd pfd;
    pfd.fd = ifd;
    pfd.events = POLLIN;
    char ibuf[1024] __attribute__((aligned(__alignof__(inotify_event))));
    for (;;) {
        int poll_num = poll(&pfd, 1, -1);
        if (poll_num == -1) {
            if (errno != EINTR) {
                close(ifd);
                LOGERR("poll: ", strerror(errno), " (", errno, ")");
            }
        } else if ((poll_num > 0) && (pfd.revents & POLLIN)) {
            // Read inotify events.
            for (;;) {
                ssize_t len = read(ifd, ibuf, sizeof(ibuf));
                if (len == -1) {
                    if (errno != EAGAIN) {
                        close(ifd);
                        LOGERR("read: ", strerror(errno), " (", errno, ")");
                    }
                    break;
                }
                char *p = ibuf;
                while (p < (ibuf + len)) {
                    const inotify_event *event = (const inotify_event *)p;
                    if (event->mask & IN_CREATE) {
                        if (strncmp(name, event->name, strlen(name)) == 0) {
                            close(ifd);
                            return shm_open(name, O_RDWR, SHM_MODE);
                        }
                    }
                    p += sizeof(inotify_event) + event->len;
                }
            }
        }
    }
    // Never reaches here.
    return -1;
}

// Destroy a shm file.
int ipc_shm_destroy(const char *name)
{
    return shm_unlink(name);
}

// Return zero if we can open the shm file.
int ipc_shm_exist(const char *name)
{
    int fd = shm_open(name, O_RDWR, SHM_MODE);
    if (fd != -1) {
        close(fd);
        return true;
    }
    return false;
}

} // namespace ark

```

`ark/ipc/ipc_shm.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_IPC_SHM_H_
#define ARK_IPC_SHM_H_

namespace ark {

// Create a shm file if it does not exist, and truncate it to zero bytes.
// If the creation fails, return -1.
int ipc_shm_create(const char *name);
// Try opening a shm file.
// Return its file descriptor on success, otherwise return -1.
int ipc_shm_open(const char *name);
// Open a shm file and return its file descriptor.
// If opening fails due to non-existence, block until it can open.
// If opening fails due to any other reasons, return -1.
int ipc_shm_open_blocking(const char *name);
// Destroy a shm file.
int ipc_shm_destroy(const char *name);
// Return zero if we can open the shm file.
int ipc_shm_exist(const char *name);

} // namespace ark

#endif // ARK_IPC_SHM_H_

```

`ark/ipc/ipc_socket.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <arpa/inet.h>
#include <cstring>
#include <fcntl.h>
#include <sys/epoll.h>
#include <sys/socket.h>
#include <unistd.h>

#include "ipc/ipc_socket.h"
#include "logging.h"

#define MAX_LISTEN_LEN 4096
#define MAX_ITEM_NAME_LEN 256

namespace ark {

IpcSocket::IpcSocket(const std::string &ip_, int port_, bool create_)
    : ip{ip_}, port{port_}, create{create_}, server{nullptr}
{
    struct sockaddr_in addr;
    addr.sin_family = AF_INET;
    addr.sin_port = htons(port_);
    addr.sin_addr.s_addr = inet_addr(ip_.c_str());
    this->sock_listen = socket(AF_INET, SOCK_STREAM, 0);
    int opt = 1;
    int ret;
    ret = setsockopt(this->sock_listen, SOL_SOCKET, SO_REUSEADDR, &opt,
                     sizeof(int));
    if (ret != 0) {
        LOGERR("setsockopt: ", strerror(errno), " (", errno, ")");
    }
    int flags = fcntl(this->sock_listen, F_GETFL, 0);
    if (flags == -1) {
        LOGERR("fcntl: ", strerror(errno), " (", errno, ")");
    }
    if (fcntl(this->sock_listen, F_SETFL, flags | O_NONBLOCK) == -1) {
        LOGERR("fcntl: ", strerror(errno), " (", errno, ")");
    }
    LOG(DEBUG, "listen ", ip_, ":", port_);
    ret = bind(this->sock_listen, (struct sockaddr *)&addr, sizeof(addr));
    if (ret != 0) {
        LOGERR("bind: ", strerror(errno), " (", errno, ")");
    }
    ret = listen(this->sock_listen, MAX_LISTEN_LEN);
    if (ret != 0) {
        LOGERR("listen: ", strerror(errno), " (", errno, ")");
    }
    this->run_server = true;
    this->server = new std::thread([&] {
        int epfd = epoll_create1(0);
        if (epfd == -1) {
            LOGERR("epoll_create1: ", strerror(errno), " (", errno, ")");
        }
        struct epoll_event ev;
        ev.events = EPOLLIN;
        ev.data.fd = this->sock_listen;
        if (epoll_ctl(epfd, EPOLL_CTL_ADD, this->sock_listen, &ev) == -1) {
            LOGERR("epoll_ctl: ", strerror(errno), " (", errno, ")");
        }
        struct epoll_event events[MAX_LISTEN_LEN];
        while (this->run_server) {
            int num = epoll_wait(epfd, events, MAX_LISTEN_LEN, 100);
            if (num == -1) {
                if (errno != EINTR) {
                    LOGERR("epoll_wait: ", strerror(errno), " (", errno, ")");
                }
            } else if (num > 0) {
                this->serve_item();
            }
        }
    });
}

IpcSocket::~IpcSocket()
{
    this->run_server = false;
    if (this->server) {
        this->server->join();
        delete this->server;
    }
    close(this->sock_listen);
    for (auto &item : this->items) {
        if (item.second.data == nullptr) {
            free(item.second.data);
        }
    }
}

IpcSocket::State IpcSocket::add_item(const std::string &name, void *data,
                                     int size)
{
    if (name.size() > MAX_ITEM_NAME_LEN) {
        LOGERR("name too long");
    }
    void *copy;
    if ((data == nullptr) || (size == 0)) {
        copy = nullptr;
    } else {
        copy = malloc(size);
        memcpy(copy, data, size);
    }
    struct Item item;
    item.data = copy;
    item.size = size;
    item.cnt = 0;
    this->items.emplace(name, item);
    return SUCCESS;
}

IpcSocket::State IpcSocket::remove_item(const std::string &name)
{
    auto it = this->items.find(name);
    if (it == this->items.end()) {
        return ITEM_NOT_FOUND;
    }
    if (it->second.data != nullptr) {
        free(it->second.data);
    }
    this->items.erase(it);
    return SUCCESS;
}

IpcSocket::State IpcSocket::query_item_internal(const std::string &ip, int port,
                                                const std::string &name,
                                                void *data, int size,
                                                bool block)
{
    int sock = socket(AF_INET, SOCK_STREAM, 0);
    struct sockaddr_in addr;
    addr.sin_family = AF_INET;
    addr.sin_port = htons(port);
    addr.sin_addr.s_addr = inet_addr(ip.c_str());
    int ret;
    for (;;) {
        ret = connect(sock, (struct sockaddr *)&addr, sizeof(addr));
        if (ret == 0) {
            break;
        } else if (block) {
            if ((errno != EINTR) && (errno != ECONNREFUSED)) {
                LOGERR("connect: ", strerror(errno), " (", errno, ")");
            }
            sched_yield();
        } else {
            close(sock);
            return CONNECT_FAILED;
        }
    }
    int flags = fcntl(sock, F_GETFL, 0);
    if (flags == -1) {
        LOGERR("fcntl: ", strerror(errno), " (", errno, ")");
    }
    if (fcntl(sock, F_SETFL, flags | O_NONBLOCK) == -1) {
        LOGERR("fcntl: ", strerror(errno), " (", errno, ")");
    }
    ret = this->send_all(sock, name.c_str(), name.size());
    if (ret != 0) {
        close(sock);
        return SEND_FAILED;
    }
    ret = this->recv_all(sock, data, size);
    if (ret < 0) {
        close(sock);
        return RECV_FAILED;
    } else if (ret == 0) {
        close(sock);
        return ITEM_NOT_FOUND;
    }
    close(sock);
    return SUCCESS;
}

IpcSocket::State IpcSocket::query_item(const std::string &ip, int port,
                                       const std::string &name, void *data,
                                       int size, bool block)
{
    State s;
    for (;;) {
        s = query_item_internal(ip, port, name, data, size, block);
        if (!block || (s != ITEM_NOT_FOUND)) {
            break;
        }
        sched_yield();
    }
    return s;
}

IpcSocket::State IpcSocket::serve_item()
{
    int sock = accept(this->sock_listen, NULL, NULL);
    if (sock < 0) {
        return ACCEPT_FAILED;
    }
    int flags = fcntl(sock, F_GETFL, 0);
    if (flags == -1) {
        LOGERR("fcntl: ", strerror(errno), " (", errno, ")");
    }
    if (fcntl(sock, F_SETFL, flags | O_NONBLOCK) == -1) {
        LOGERR("fcntl: ", strerror(errno), " (", errno, ")");
    }
    char name[MAX_ITEM_NAME_LEN + 1];
    int ret = this->recv_all(sock, name, MAX_ITEM_NAME_LEN);
    if (ret < 0) {
        close(sock);
        return RECV_FAILED;
    }
    name[ret] = '\0';
    auto it = this->items.find(name);
    if (it != this->items.end()) {
        ret = this->send_all(sock, it->second.data, it->second.size);
        if (ret != 0) {
            close(sock);
            return SEND_FAILED;
        }
        it->second.cnt++;
    }
    close(sock);
    return SUCCESS;
}

const IpcSocket::Item *IpcSocket::get_item(const std::string &name) const
{
    auto it = this->items.find(name);
    if (it != this->items.end()) {
        return &it->second;
    }
    return nullptr;
}

////////////////////////////////////////////////////////////////////////////////

int IpcSocket::send_all(int sock, const void *buf, int size)
{
    int sent = 0;
    const char *ptr = (const char *)buf;
    while (sent < size) {
        int ret = send(sock, ptr + sent, size - sent, 0);
        if (ret < 0) {
            if ((errno == EINTR) || (errno == EAGAIN)) {
                continue;
            }
            return ret;
        }
        sent += ret;
    }
    return 0;
}

int IpcSocket::recv_all(int sock, void *buf, int size)
{
    int cnt = 0;
    int received = 0;
    char *ptr = (char *)buf;
    while (received < size) {
        int ret = recv(sock, ptr + received, size - received, 0);
        if (ret < 0) {
            if ((errno == EINTR) || (errno == EAGAIN)) {
                if (++cnt > 10) {
                    // Avoid deadlocks
                    sched_yield();
                }
                continue;
            }
            return ret;
        } else if (ret < size - received) {
            received += ret;
            break;
        }
        received += ret;
    }
    return received;
}

} // namespace ark

```

`ark/ipc/ipc_socket.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_IPC_IPC_SOCKET_H_
#define ARK_IPC_IPC_SOCKET_H_

#include <map>
#include <string>
#include <thread>

namespace ark {

class IpcSocket
{
  public:
    typedef enum
    {
        SUCCESS = 0,
        ACCEPT_FAILED,
        CONNECT_FAILED,
        SEND_FAILED,
        RECV_FAILED,
        ITEM_NOT_FOUND,
    } State;

    struct Item
    {
        void *data;
        int size;
        int cnt;
    };

    IpcSocket(const std::string &ip_, int port_, bool create_ = true);
    ~IpcSocket();

    State add_item(const std::string &name, void *data, int size);
    State remove_item(const std::string &name);
    State query_item(const std::string &ip, int port, const std::string &name,
                     void *data, int size, bool block = false);
    State serve_item();

    const Item *get_item(const std::string &name) const;

  private:
    State query_item_internal(const std::string &ip, int port,
                              const std::string &name, void *data, int size,
                              bool block);

    int send_all(int sock, const void *buf, int size);
    int recv_all(int sock, void *buf, int size);

    const std::string ip;
    const int port;
    const bool create;

    int sock_listen;
    bool run_server;
    std::thread *server;

    std::map<std::string, struct Item> items;
};

} // namespace ark

#endif // ARK_IPC_IPC_SOCKET_H_

```

`ark/ipc/ipc_socket_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "env.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "ipc/ipc_hosts.h"
#include "ipc/ipc_socket.h"
#include "logging.h"
#include "unittest/unittest_utils.h"

using namespace ark;

struct TestIpcSocketItem
{
    int a;
    int b;
    int c;
};

unittest::State test_ipc_socket_simple()
{
    int pid0 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{5};
        int port_base = get_env().ipc_listen_port_base;
        IpcSocket is{get_host(0), port_base};

        struct TestIpcSocketItem item;
        item.a = 1;
        item.b = 2;
        item.c = 3;
        is.add_item("test_item_name", &item, sizeof(item));

        struct TestIpcSocketItem remote_item;
        IpcSocket::State s =
            is.query_item(get_host(0), port_base + 1, "test_item_name",
                          &remote_item, sizeof(remote_item), true);
        UNITTEST_TRUE(s == IpcSocket::SUCCESS);
        UNITTEST_EQ(remote_item.a, 4);
        UNITTEST_EQ(remote_item.b, 5);
        UNITTEST_EQ(remote_item.c, 6);

        const IpcSocket::Item *item_ptr = is.get_item("test_item_name");
        UNITTEST_TRUE(item_ptr != nullptr);
        while (item_ptr->cnt == 0) {
            sched_yield();
        }
        return 0;
    });
    UNITTEST_NE(pid0, -1);

    int pid1 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{5};
        int port_base = get_env().ipc_listen_port_base;
        IpcSocket is{get_host(0), port_base + 1};

        struct TestIpcSocketItem item;
        item.a = 4;
        item.b = 5;
        item.c = 6;
        is.add_item("test_item_name", &item, sizeof(item));

        struct TestIpcSocketItem remote_item;
        IpcSocket::State s =
            is.query_item(get_host(0), port_base, "test_item_name",
                          &remote_item, sizeof(remote_item), true);
        UNITTEST_TRUE(s == IpcSocket::SUCCESS);
        UNITTEST_EQ(remote_item.a, 1);
        UNITTEST_EQ(remote_item.b, 2);
        UNITTEST_EQ(remote_item.c, 3);

        const IpcSocket::Item *item_ptr = is.get_item("test_item_name");
        UNITTEST_TRUE(item_ptr != nullptr);
        while (item_ptr->cnt == 0) {
            sched_yield();
        }
        return 0;
    });
    UNITTEST_NE(pid1, -1);

    int ret = ark::utils::proc_wait({pid0, pid1});
    UNITTEST_EQ(ret, 0);
    return unittest::SUCCESS;
}

unittest::State test_ipc_socket_no_item()
{
    int pid0 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{10};
        int port_base = get_env().ipc_listen_port_base;
        IpcSocket is{get_host(0), port_base};

        struct TestIpcSocketItem remote_item;
        IpcSocket::State s =
            is.query_item(get_host(0), port_base + 1, "test_item_name",
                          &remote_item, sizeof(remote_item), true);
        UNITTEST_TRUE(s == IpcSocket::SUCCESS);
        UNITTEST_EQ(remote_item.a, 4);
        UNITTEST_EQ(remote_item.b, 5);
        UNITTEST_EQ(remote_item.c, 6);
        return 0;
    });
    UNITTEST_NE(pid0, -1);

    int pid1 = ark::utils::proc_spawn([] {
        unittest::Timeout timeout{10};
        int port_base = get_env().ipc_listen_port_base;
        IpcSocket is{get_host(0), port_base + 1};

        // Sleep for a while to make the remote experience NO_ITEM
        cpu_timer_sleep(2);

        struct TestIpcSocketItem item;
        item.a = 4;
        item.b = 5;
        item.c = 6;
        is.add_item("test_item_name", &item, sizeof(item));

        const IpcSocket::Item *item_ptr = is.get_item("test_item_name");
        UNITTEST_TRUE(item_ptr != nullptr);
        while (item_ptr->cnt == 0) {
            sched_yield();
        }
        return 0;
    });
    UNITTEST_NE(pid1, -1);

    int ret = ark::utils::proc_wait({pid0, pid1});
    UNITTEST_EQ(ret, 0);
    return unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_ipc_socket_simple);
    UNITTEST(test_ipc_socket_no_item);
    return 0;
}

```

`ark/ipc/ipc_table.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <algorithm>
#include <cstring>

#include "ipc/ipc_table.h"
#include "logging.h"

using namespace std;

namespace ark {

IpcTable::IpcTable(const string &name_, int rank_, size_t elem_bytes_)
    : name{name_}, rank{rank_}, elem_bytes{elem_bytes_}
{
    IpcMem *key = new IpcMem{name_ + ".key" + to_string(rank_), true};
    IpcMem *val = new IpcMem{name_ + ".val" + to_string(rank_), true};
    this->key_storage.emplace_back(key);
    this->val_storage.emplace_back(val);
    this->keys.resize(rank_ + 1, nullptr);
    this->vals.resize(rank_ + 1, nullptr);
    this->maps.resize(rank_ + 1);
    this->keys[rank_] = key;
    this->vals[rank_] = val;
}

void *IpcTable::add_entry(int rank, const string &key)
{
    if (key.find(' ') != string::npos) {
        LOGERR("the key should not contain any blank character.");
    }
    if (rank == this->rank) {
        auto &rank_map = this->maps[rank];
        if (rank_map.find(key) != rank_map.end()) {
            return rank_map[key];
        }
        this->key_stream << key << ' ';
        size_t offset = this->total_val_bytes;
        this->total_val_bytes += this->elem_bytes;
        void *ptr = this->vals[rank]->alloc(this->total_val_bytes);
        void *ret = (char *)ptr + offset;
        this->maps[rank][key] = ret;
        return ret;
    } else {
        bool do_init = false;
        if (rank <= (int)this->keys.size()) {
            this->keys.resize(rank + 1, nullptr);
            this->vals.resize(rank + 1, nullptr);
            this->maps.resize(rank + 1);
            do_init = true;
        } else if (this->keys[rank] == nullptr) {
            do_init = true;
        }
        if (do_init) {
            IpcMem *key =
                new IpcMem{this->name + ".key" + to_string(rank), false};
            IpcMem *val =
                new IpcMem{this->name + ".val" + to_string(rank), false};
            this->key_storage.emplace_back(key);
            this->key_storage.emplace_back(val);
            this->keys[rank] = key;
            this->vals[rank] = val;
        }
        auto &rank_map = this->maps[rank];
        if (rank_map.find(key) != rank_map.end()) {
            return rank_map[key];
        }
        rank_map[key] = nullptr;
        return nullptr;
    }
}

//
void IpcTable::freeze()
{
    const string key_str = this->key_stream.str();
    const int len = (int)key_str.size();
    const char *key_c_str = key_str.c_str();
    char *pk = (char *)this->keys[this->rank]->alloc(len + 1);
    memcpy(pk, key_c_str, len);
    pk[len] = '\0';

    for (int i = 0; i < (int)this->keys.size(); ++i) {
        if ((i == this->rank) || (this->keys[i] == nullptr)) {
            continue;
        }
        this->import_rank_data(i);
    }
    this->freezed = true;
}

void *IpcTable::get_entry(int rank, const string &key)
{
    if (rank == this->rank) {
        if (!this->freezed) {
            return this->add_entry(rank, key);
        }
    } else {
        void *ret = this->add_entry(rank, key);
        if ((ret != nullptr) || !this->freezed) {
            return ret;
        }
        this->import_rank_data(rank);
    }
    auto &rank_map = this->maps[rank];
    if (rank_map.find(key) == rank_map.end()) {
        LOGERR("the entry does not exist: rank=", rank, ", key=", key);
    }
    return rank_map[key];
}

void IpcTable::import_rank_data(int rank)
{
    char *keys_ptr = (char *)this->keys[rank]->alloc(0);
    string keys_str = string{keys_ptr};
    size_t num_keys = count(keys_str.begin(), keys_str.end(), ' ');
    char *vals_ptr =
        (char *)this->vals[rank]->alloc(num_keys * this->elem_bytes);

    auto &rank_map = this->maps[rank];
    string::size_type pos_s = 0;
    string::size_type pos_e;
    for (size_t j = 0; j < num_keys; ++j) {
        pos_e = keys_str.find(' ', pos_s);
        rank_map[keys_str.substr(pos_s, pos_e - pos_s)] =
            (void *)(vals_ptr + j * this->elem_bytes);
        pos_s = pos_e + 1;
    }
}

} // namespace ark

```

`ark/ipc/ipc_table.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_IPC_TABLE_H_
#define ARK_IPC_TABLE_H_

#include <list>
#include <map>
#include <memory>
#include <sstream>
#include <string>
#include <vector>

#include "ipc/ipc_mem.h"

namespace ark {

class IpcTable
{
  public:
    IpcTable(const std::string &name, int rank, size_t elem_bytes);
    void *add_entry(int rank, const std::string &key);
    void freeze();
    void *get_entry(int rank, const std::string &key);

  private:
    void import_rank_data(int rank);

    const std::string name;
    const int rank;
    const size_t elem_bytes;

    size_t total_val_bytes = 0;

    std::list<std::unique_ptr<IpcMem>> key_storage;
    std::list<std::unique_ptr<IpcMem>> val_storage;
    std::vector<IpcMem *> keys;
    std::vector<IpcMem *> vals;
    std::vector<std::map<std::string, void *>> maps;

    std::stringstream key_stream;
    bool freezed = false;
};

} // namespace ark

#endif // ARK_IPC_TABLE_H_

```

`ark/kahypar.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_KAHYPAR_H_
#define ARK_KAHYPAR_H_

#include <algorithm>
#include <cstdlib>
#include <functional>
#include <initializer_list>
#include <limits>
#include <map>
#include <memory>
#include <set>
#include <string>
#include <vector>

#include "env.h"
#include "logging.h"
#include "third_party/kahypar/include/libkahypar.h"

namespace ark {

template <typename ItemType> class KahyparGraph
{
  public:
    // Constructor.
    KahyparGraph()
    {
        // Get the context file path.
        ctx_file_path =
            get_env().path_root_dir + "/cut_kKaHyPar_dissertation.ini";
    };

    // Copy constructors.
    KahyparGraph(const KahyparGraph &) = delete;
    KahyparGraph &operator=(const KahyparGraph &) = delete;

    // Add nodes from an initializer list.
    void add_nodes(int weight, std::initializer_list<ItemType *> init)
    {
        for (ItemType *item : init) {
            add_node(weight, item);
        }
        resize_wm();
    }

    // Add nodes from a generator.
    void add_nodes(int weight, std::function<ItemType *()> gen)
    {
        ItemType *item;
        while ((item = gen()) != nullptr) {
            add_node(weight, item);
        }
        resize_wm();
    }

    // Add an edge which consists of nodes from an initializer list.
    void add_edge(int weight, std::initializer_list<ItemType *> init)
    {
        std::vector<size_t> nodes;
        for (ItemType *item : init) {
            nodes.push_back(item2node.at(*item));
        }
        if (nodes.size() > 0) {
            add_nodes_into_edge(weight, nodes);
        }
    }

    // Add an edge which consists of nodes from a generator.
    void add_edge(int weight, std::function<ItemType *()> gen)
    {
        std::vector<size_t> nodes;
        ItemType *item;
        while ((item = gen()) != nullptr) {
            nodes.push_back(item2node.at(*item));
        }
        if (nodes.size() > 0) {
            add_nodes_into_edge(weight, nodes);
        }
    }

    // Finalize the graph and partition it into `num_part` parts.
    std::vector<std::vector<std::pair<ItemType *, ItemType *>>> &partition(
        int num_part)
    {
        // Verify the graph.
        size_t num_nodes = nws.size();
        if (num_nodes != items.size() || num_nodes != wm.size()) {
            LOGERR("Unexpected error.");
            throw;
        }
        for (auto wv : wm) {
            if (num_nodes != wv.size()) {
                LOGERR("Unexpected error.");
                throw;
            }
        }
        if (ews.size() == 0) {
            // If there is no edges, add a virtual edge with weight 0.
            eis.push_back(edges.size());
            ews.push_back(0);
            for (size_t i = 0; i < num_nodes; ++i) {
                edges.push_back(i);
            }
        }
        // Finalize hypergraph.
        eis.push_back(edges.size());
        // Run partition.
        kahypar_context_t *ctx = kahypar_context_new();
        kahypar_configure_context_from_file(ctx, ctx_file_path.c_str());
        kahypar_hyperedge_weight_t objective = 0;
        std::vector<kahypar_partition_id_t> partition(num_nodes, -1);
        kahypar_partition(num_nodes, ews.size(), 0.03, num_part,
                          (kahypar_hypernode_weight_t *)&nws[0],
                          (kahypar_hyperedge_weight_t *)&ews[0],
                          (size_t *)&eis[0],
                          (kahypar_hyperedge_id_t *)&edges[0], &objective, ctx,
                          partition.data());
        // Group nodes in the same part.
        std::vector<std::vector<size_t>> nparts(num_part);
        std::set<size_t> dup_chk;
        for (size_t i = 0; i < partition.size(); ++i) {
            nparts[partition[i]].push_back(i);
            dup_chk.insert(i);
        }
        // Correctness check: duplicated nodes.
        if (partition.size() != dup_chk.size()) {
            LOGERR("Found duplicated nodes.");
            throw;
        }
        // Correctness check: node count.
        if (partition.size() != num_nodes) {
            LOGERR("Node count mismatch.");
            throw;
        }
        //
        parts.resize(nparts.size());
        std::vector<std::pair<size_t, size_t>> cands;
        for (size_t i = 0; i < nparts.size(); ++i) {
            auto &npart = nparts.at(i);
            auto &part = parts.at(i);
            // Sort `npart` in increasing order of items.
            std::sort(npart.begin(), npart.end(), [&](size_t a, size_t b) {
                return *items[a] < *items[b];
            });
            //
            while (npart.size() > 1) {
                // Get candidate nodes which have the maximum weight.
                cands.clear();
                int mw = INT_MIN;
                for (auto it0 = npart.begin(); it0 != npart.end(); ++it0) {
                    for (auto it1 = next(it0); it1 != npart.end(); ++it1) {
                        int mvw = wm[*it0][*it1];
                        if (mvw > mw) {
                            mw = mvw;
                            cands.clear();
                        }
                        if (mvw >= mw) {
                            cands.emplace_back(*it0, *it1);
                        }
                    }
                }
                // If there are multiple candidates, select the smallest one.
                auto &select = cands[0];
                // Put corresponding items of the selected nodes into `part`.
                part.emplace_back(items[select.first].get(),
                                  items[select.second].get());
                // Exclude selected nodes from `npart`.
                auto it = npart.begin();
                int cnt = 0;
                while (it != npart.end()) {
                    if (*it == select.first || *it == select.second) {
                        it = npart.erase(it);
                        ++cnt;
                        if (cnt == 2) {
                            break;
                        }
                    } else {
                        ++it;
                    }
                }
            }
            // Put the last single node into `part` without pair.
            if (npart.size() == 1) {
                part.emplace_back(items[npart[0]].get(), nullptr);
            }
        }
        return parts;
    }

    int get_num_nodes()
    {
        return nws.size();
    }

  private:
    // Add a new node for `item`.
    void add_node(int weight, ItemType *item)
    {
        item2node[*item] = nws.size();
        items.push_back(std::unique_ptr<ItemType>(item));
        nws.push_back(weight);
    }

    // Add `nodes` as a new edge.
    void add_nodes_into_edge(int weight, std::vector<size_t> &nodes)
    {
        eis.push_back(edges.size());
        ews.push_back(weight);
        edges.insert(edges.end(), nodes.begin(), nodes.end());
        // Update the weight matrix.
        for (auto it0 = nodes.begin(); it0 != nodes.end(); ++it0) {
            for (auto it1 = next(it0); it1 != nodes.end(); ++it1) {
                wm[*it0][*it1] += weight;
                wm[*it1][*it0] += weight;
            }
        }
    }

    // Resize the weight matrix when the number of nodes is changed.
    void resize_wm()
    {
        for (auto &wv : wm) {
            wv.resize(nws.size(), 0);
        }
        size_t nws_size = nws.size();
        size_t wm_size = wm.size();
        for (size_t i = 0; i < nws_size - wm_size; ++i) {
            wm.emplace_back(nws_size, 0);
        }
    }

    static const int INT_MIN = std::numeric_limits<int>::min();

    // KaHyPar context file path.
    std::string ctx_file_path;
    // KaHyPar hypergraph representation.
    std::vector<int> nws;
    std::vector<int> ews;
    std::vector<size_t> eis;
    std::vector<unsigned int> edges;
    std::vector<std::unique_ptr<ItemType>> items;
    std::map<ItemType, size_t> item2node;
    // Edge weight matrix.
    std::vector<std::vector<int>> wm;
    // Partition result.
    std::vector<std::vector<std::pair<ItemType *, ItemType *>>> parts;
};

} // namespace ark

#endif // ARK_KAHYPAR_H_
```

`ark/kahypar_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

// Example usage of KaHyPar interface in ARK.
// Environment variable `ARK_ROOT` should be set to run.
// `LD_LIBRARY_PATH` should include `$ARK_ROOT/lib` directory.

#include "include/ark.h"
#include "kahypar.h"
#include "unittest/unittest_utils.h"

#define ITERATION 3

using namespace std;

struct TestObj
{
    TestObj(int id_) : id(id_)
    {
    }
    int id;

    bool operator<(const TestObj &rhs) const
    {
        return id < rhs.id;
    }
};

function<TestObj *()> gen(initializer_list<int> seq)
{
    return [&] {
        const vector<int> seq_(seq);
        size_t cnt = 0;
        return [=]() mutable {
            return cnt < seq.size() ? new TestObj(seq_[cnt++]) : nullptr;
        };
    }();
}

ark::unittest::State test_simple()
{
    ark::KahyparGraph<TestObj> kg;
    kg.add_nodes(1, gen({0, 1, 2, 3, 4, 5, 6}));

    kg.add_edge(1, gen({0, 2}));
    kg.add_edge(1000, gen({0, 1, 3, 4}));
    kg.add_edge(1, gen({3, 4, 6}));
    kg.add_edge(1000, gen({2, 5, 6}));

    auto &parts = kg.partition(2);

    // Print results. Desired outputs:
    // res[0] = { 2 5 6 } and res[1] = { 3 4 0 1 }, or
    // res[0] = { 3 4 0 1 } and res[1] = { 2 5 6 }.
    vector<TestObj *> res[2];
    for (int idx = 0; idx < 2; ++idx) {
        for (auto &p : parts[idx]) {
            res[idx].push_back(p.first);
            if (p.second)
                res[idx].push_back(p.second);
        }
    }

    // Verify results.
    if (res[0][0]->id == 2) {
        UNITTEST_EQ(res[0][0]->id, 2);
        UNITTEST_EQ(res[0][1]->id, 5);
        UNITTEST_EQ(res[0][2]->id, 6);
        UNITTEST_EQ(res[1][0]->id, 3);
        UNITTEST_EQ(res[1][1]->id, 4);
        UNITTEST_EQ(res[1][2]->id, 0);
        UNITTEST_EQ(res[1][3]->id, 1);
    } else {
        UNITTEST_EQ(res[0][0]->id, 3);
        UNITTEST_EQ(res[0][1]->id, 4);
        UNITTEST_EQ(res[0][2]->id, 0);
        UNITTEST_EQ(res[0][3]->id, 1);
        UNITTEST_EQ(res[1][0]->id, 2);
        UNITTEST_EQ(res[1][1]->id, 5);
        UNITTEST_EQ(res[1][2]->id, 6);
    }

    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    for (int i = 0; i < ITERATION; ++i) {
        UNITTEST(test_simple);
    }
    return 0;
}

```

`ark/logging.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <cassert>
#include <cstring>
#include <iomanip>
#include <sstream>
#include <unistd.h>

#include "cpu_timer.h"
#include "env.h"
#include "logging.h"

using namespace std;

namespace ark {

Logging::Logging(const char *lv) : pid{getpid()}
{
    if (lv == nullptr) {
        this->level = INFO;
    } else if (strncmp(lv, "DEBUG", 6) == 0) {
        this->level = DEBUG;
    } else if (strncmp(lv, "WARN", 5) == 0) {
        this->level = WARN;
    } else if (strncmp(lv, "ERROR", 6) == 0) {
        this->level = ERROR;
    } else {
        this->level = INFO;
    }
}

const LogLevel &Logging::get_level() const
{
    return this->level;
}

void Logging::set_level(LogLevel lv)
{
    this->level = lv;
};

////////////////////////////////////////////////////////////////////////////////

unique_ptr<Logging> _ARK_LOGGING_GLOBAL = nullptr;

// Get the global Logging.
Logging &get_logging()
{
    if (_ARK_LOGGING_GLOBAL.get() == nullptr) {
        _ARK_LOGGING_GLOBAL.reset(new Logging{get_env().log_level});
        assert(_ARK_LOGGING_GLOBAL.get() != nullptr);
    }
    return *_ARK_LOGGING_GLOBAL;
}

void log_header(ostream &os, const LogLevel ll, const string &file,
                const int line)
{
    long usec = cpu_ntimer() / 1000;
    os << dec << setfill('0') << setw(6) << usec << " ARK " << setfill(' ')
       << setw(5) << getpid() << ' ';
    switch (ll) {
    case INFO:
        os << "INFO ";
        break;
    case DEBUG:
        os << "DEBUG ";
        break;
    case WARN:
        os << "WARN ";
        break;
    case ERROR:
        os << "ERROR ";
        break;
    }
    os << file << ':' << line << ' ';
}

ostream &log(ostream &os, const LogLevel ll, const string &file, const int line)
{
    log_header(os, ll, file, line);
    return os;
}

void set_log_level(LogLevel lv)
{
    get_logging().set_level(lv);
}

const LogLevel &get_log_level()
{
    return get_logging().get_level();
}

} // namespace ark

```

`ark/logging.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_LOGGING_H_
#define ARK_LOGGING_H_

#include <iostream>
#include <sstream>
#include <string>

namespace ark {

typedef enum
{
    DEBUG,
    INFO,
    WARN,
    ERROR
} LogLevel;

class Logging
{
  public:
    Logging(const char *lv);

    const LogLevel &get_level() const;
    void set_level(LogLevel lv);

  private:
    const pid_t pid;
    LogLevel level;
};

// Get the global Logging.
Logging &get_logging();

void log_header(std::ostream &os, const ark::LogLevel ll,
                const std::string &file, const int line);

std::ostream &log(std::ostream &os, const ark::LogLevel ll,
                  const std::string &file, const int line);

void set_log_level(LogLevel lv);
const LogLevel &get_log_level();

// Logging macros.
#define SSTREAM_1(_1) _ss << (_1)
#define SSTREAM_2(_1, _2) _ss << (_1) << (_2)
#define SSTREAM_3(_1, _2, _3) _ss << (_1) << (_2) << (_3)
#define SSTREAM_4(_1, _2, _3, _4) _ss << (_1) << (_2) << (_3) << (_4)
#define SSTREAM_5(_1, _2, _3, _4, _5)                                          \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5)
#define SSTREAM_6(_1, _2, _3, _4, _5, _6)                                      \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6)
#define SSTREAM_7(_1, _2, _3, _4, _5, _6, _7)                                  \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7)
#define SSTREAM_8(_1, _2, _3, _4, _5, _6, _7, _8)                              \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)
#define SSTREAM_9(_1, _2, _3, _4, _5, _6, _7, _8, _9)                          \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8) << (_9)
#define SSTREAM_10(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10)                    \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10)
#define SSTREAM_11(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11)               \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10) << (_11)
#define SSTREAM_12(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12)          \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10) << (_11) << (_12)
#define SSTREAM_13(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13)     \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10) << (_11) << (_12) << (_13)
#define SSTREAM_14(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13,     \
                   _14)                                                        \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10) << (_11) << (_12) << (_13) << (_14)
#define SSTREAM_15(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13,     \
                   _14, _15)                                                   \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10) << (_11) << (_12) << (_13) << (_14) << (_15)
#define SSTREAM_16(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13,     \
                   _14, _15, _16)                                              \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10) << (_11) << (_12) << (_13) << (_14) << (_15) << (_16)
#define SSTREAM_17(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13,     \
                   _14, _15, _16, _17)                                         \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10) << (_11) << (_12) << (_13) << (_14) << (_15) << (_16) \
        << (_17)
#define SSTREAM_18(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13,     \
                   _14, _15, _16, _17, _18)                                    \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10) << (_11) << (_12) << (_13) << (_14) << (_15) << (_16) \
        << (_17) << (_18)
#define SSTREAM_19(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13,     \
                   _14, _15, _16, _17, _18, _19)                               \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10) << (_11) << (_12) << (_13) << (_14) << (_15) << (_16) \
        << (_17) << (_18) << (_19)
#define SSTREAM_20(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13,     \
                   _14, _15, _16, _17, _18, _19, _20)                          \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10) << (_11) << (_12) << (_13) << (_14) << (_15) << (_16) \
        << (_17) << (_18) << (_19) << (_20)
#define SSTREAM_21(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13,     \
                   _14, _15, _16, _17, _18, _19, _20, _21)                     \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10) << (_11) << (_12) << (_13) << (_14) << (_15) << (_16) \
        << (_17) << (_18) << (_19) << (_20) << (_21)
#define SSTREAM_22(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13,     \
                   _14, _15, _16, _17, _18, _19, _20, _21, _22)                \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10) << (_11) << (_12) << (_13) << (_14) << (_15) << (_16) \
        << (_17) << (_18) << (_19) << (_20) << (_21) << (_22)
#define SSTREAM_23(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13,     \
                   _14, _15, _16, _17, _18, _19, _20, _21, _22, _23)           \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10) << (_11) << (_12) << (_13) << (_14) << (_15) << (_16) \
        << (_17) << (_18) << (_19) << (_20) << (_21) << (_22) << (_23)
#define SSTREAM_24(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13,     \
                   _14, _15, _16, _17, _18, _19, _20, _21, _22, _23, _24)      \
    _ss << (_1) << (_2) << (_3) << (_4) << (_5) << (_6) << (_7) << (_8)        \
        << (_9) << (_10) << (_11) << (_12) << (_13) << (_14) << (_15) << (_16) \
        << (_17) << (_18) << (_19) << (_20) << (_21) << (_22) << (_23)         \
        << (_24)
#define NARGS_(_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12, _13, _14,    \
               _15, _16, _17, _18, _19, _20, _21, _22, _23, _24, N, ...)       \
    N
#define NARGS(...)                                                             \
    NARGS_(__VA_ARGS__, 24, 23, 22, 21, 20, 19, 18, 17, 16, 15, 14, 13, 12,    \
           11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0)
#define CONCATENATE(x, y) x##y
#define SSTREAM_(N, ...) CONCATENATE(SSTREAM_, N)(__VA_ARGS__)
#define SSTREAM(...) SSTREAM_(NARGS(__VA_ARGS__), __VA_ARGS__)

// Logging.
#define LOG_(level, ...)                                                       \
    do {                                                                       \
        if (level < ark::get_logging().get_level()) {                          \
            break;                                                             \
        }                                                                      \
        std::stringstream _ss;                                                 \
        ark::log_header(_ss, level, __FILE__, __LINE__);                       \
        SSTREAM(__VA_ARGS__);                                                  \
        _ss << '\n';                                                           \
        std::clog << _ss.str();                                                \
        if (level == ark::ERROR) {                                             \
            throw std::runtime_error("ARK runtime error");                     \
        }                                                                      \
    } while (0)

//
#define LOG(level, ...) LOG_(level, __VA_ARGS__)

// Logging of an error message and exit.
#define LOGERR(...)                                                            \
    do {                                                                       \
        std::stringstream _ss;                                                 \
        ark::log_header(_ss, ark::ERROR, __FILE__, __LINE__);                  \
        SSTREAM(__VA_ARGS__);                                                  \
        _ss << '\n';                                                           \
        std::clog << _ss.str();                                                \
        throw std::runtime_error("ARK runtime error");                         \
    } while (0)

#define CHECK(cond)                                                            \
    do {                                                                       \
        if (!(cond)) {                                                         \
            LOGERR("failed condition: " #cond);                                \
        }                                                                      \
    } while (0)

} // namespace ark

#endif // ARK_LOGGING_H_

```

`ark/math.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "math.h"
#include "logging.h"

namespace ark {
namespace math {

// Calculate the ceiling of x / div.
size_t div_up(size_t x, size_t div)
{
    if (div == 0) {
        LOGERR("division by zero");
    }
    if (x == 0) {
        return 0;
    }
    return 1 + ((x - 1) / div);
}

// Calculate the minimum multiple of u that is greater than or equal to x.
size_t pad(size_t x, size_t u)
{
    return div_up(x, u) * u;
}

// Return true if x is a power of 2.
bool is_pow2(size_t x)
{
    if (x == 0) {
        return false;
    }
    return (x & (x - 1)) == 0;
}

// Return the log base 2 of x. x must be a power of 2.
unsigned int ilog2(unsigned int x)
{
    if (x == 0) {
        LOGERR("log of zero is undefined");
    }
    return (sizeof(unsigned int) * 8) - __builtin_clz(x) - 1;
}

// Greatest Common Divisor.
size_t gcd(size_t a, size_t b)
{
    if (a == 0) {
        return b;
    }
    if (b == 0) {
        return a;
    }
    while (b != 0) {
        size_t t = b;
        b = a % b;
        a = t;
    }
    return a;
}

// Least Common Multiple.
size_t lcm(size_t a, size_t b)
{
    return a / gcd(a, b) * b;
}

} // namespace math
} // namespace ark

```

`ark/math.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_MATH_H_
#define ARK_MATH_H_

#include <cstddef>

namespace ark {
namespace math {

size_t div_up(size_t x, size_t div);
size_t pad(size_t x, size_t u);
bool is_pow2(size_t x);
unsigned int ilog2(unsigned int x);
size_t gcd(size_t a, size_t b);
size_t lcm(size_t a, size_t b);

} // namespace math
} // namespace ark

#endif // ARK_MATH_H_

```

`ark/model.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <stack>

#include "logging.h"
#include "model.h"

using namespace std;

namespace ark {

// Create a new TensorBuf object with `bytes` bytes.
// A common usage is setting `bytes` to 0 during declaring a model and let the
// scheduler determine the value after the model is completely defined.
TensorBuf *Model::Impl::create_tensor_buf(const DimType bytes)
{
    this->tns_bufs_storage.emplace_back(
        make_unique<TensorBuf>(bytes, (int)this->tns_bufs_storage.size()));
    return this->tns_bufs_storage.back().get();
}

// Remove a TensorBuf object from the model.
void Model::Impl::destroy_tensor_buf(const TensorBuf *buf)
{
    for (auto &tns : this->tns_storage) {
        if (tns->buf == buf) {
            LOGERR("dangling tensor detected");
        }
    }
    bool is_found = false;
    auto it = this->tns_bufs_storage.begin();
    for (; it != this->tns_bufs_storage.end(); ++it) {
        if (it->get() == buf) {
            this->tns_bufs_storage.erase(it);
            is_found = true;
            break;
        }
    }
    if (!is_found) {
        LOGERR("the given TensorBuf is not found");
    }
}

std::vector<Tensor *> Model::Impl::add_op(
    const OpType type, const OpPrecType prec_type,
    const vector<Tensor *> &inputs, const vector<Tensor *> &outputs,
    const OpArgs &args, const string &name, const OpConfigMap *cfg_map,
    int gran_lev)
{
    Op op{type, prec_type, inputs, outputs, args, name, cfg_map, gran_lev};
    return this->add_op(op);
}

std::string Model::Impl::append_name_postfix(const std::string &name)
{
    string suffix_str;
    auto p = this->name_cnts.emplace(name, 1);
    if (!p.second) {
        int suffix_num = p.first->second;
        this->name_cnts[name] = suffix_num + 1;
        suffix_str = "_" + to_string(suffix_num);
    } else {
        suffix_str = "";
    }
    return name + suffix_str;
}

std::vector<Tensor *> Model::Impl::add_op(Op &op)
{
    op.name = append_name_postfix(op.name);
    this->ops_storage.emplace_back(make_unique<Op>(op));

    Op *op_ptr = this->ops_storage.back().get();
    for (auto &tns : op_ptr->inputs) {
        // If an input tensor is not generated by another op,
        // the buffer for this tensor may store external input data,
        // so we set the buffer to be immutable.
        if (this->get_producer(tns) == nullptr) {
            tns->buf->immutable = true;
        }
        this->tns_to_users[tns].insert(op_ptr);
    }
    if (op_ptr->type == OP_TENSOR) {
        // Only for TensorOps, output references directly become outputs.
        for (auto &tns : op_ptr->output_refs) {
            this->tns_to_producer[tns] = op_ptr;
            if (this->tns_to_users.find(tns) == this->tns_to_users.end()) {
                this->tns_to_users[tns] = {};
            }
        }
        op_ptr->outputs = op_ptr->output_refs;

        // Clear references.
        op_ptr->output_refs.clear();
        return op_ptr->outputs;
    }
    std::vector<Tensor *> output_tensors;
    for (auto &tns : op_ptr->output_refs) {
        Tensor *output_tensor;

        // If Op type is not OP_TENSOR and if we set the producer of the
        // given output reference to be the current Op, then the TensorOp that
        // has produced this output reference will be forgotten (because there
        // can be only one producer Op for each tensor). To avoid this,
        // we create an identical tensor and set the producer of the new tensor
        // to be the current Op.
        this->tns_storage.emplace_back(make_unique<Tensor>(
            tns->shape, tns->type, tns->buf, tns->ldims, tns->offs, tns->pads,
            tns->exported, tns->imported_rank, (int)this->tns_storage.size(),
            tns->name));
        output_tensor = this->tns_storage.back().get();

        this->tns_to_producer[output_tensor] = op_ptr;
        this->tns_to_users[output_tensor] = {};

        // The current Op becomes a user (not producer) of the given output
        // tensor.
        this->tns_to_users[tns].insert(op_ptr);

        output_tensors.push_back(output_tensor);
    }
    op_ptr->outputs = output_tensors;
    return output_tensors;
}

/// Delete an existing operator from the model.
/// @param op the existing op to be deleted.
void Model::Impl::delete_op(Op *op)
{
    // Remove the operator from the set of operators that have the given tensor
    // as one of their inputs.
    for (auto &tns : op->inputs) {
        auto search = this->tns_to_users.find(tns);
        if (search == this->tns_to_users.end()) {
            LOGERR("Not an existing tensor.");
        }
        search->second.erase(op);
    }
    // Remove the operator from the set of operators that have the given tensor
    // as its output.
    for (auto &tns : op->outputs) {
        auto search = this->tns_to_producer.find(tns);
        if (search == this->tns_to_producer.end()) {
            LOGERR("Not an existing tensor.");
        }
        this->tns_to_producer.erase(search);
    }
    // Remove the operator from the model.
    bool is_found = false;
    auto it = this->ops_storage.begin();
    for (; it != this->ops_storage.end(); ++it) {
        if (it->get() == op) {
            this->ops_storage.erase(it);
            is_found = true;
            break;
        }
    }
    if (!is_found) {
        LOGERR("the given Op is not found");
    }
    auto search = this->name_cnts.find(op->name);
    if (search != this->name_cnts.end()) {
        if (search->second == 1) {
            this->name_cnts.erase(search);
        }
        // If there are multiple operators with the same name, we do not
        // decrease the counter to avoid conflicts when creating new operators
        // with the same name.
    }
}

std::list<TensorBuf *> Model::Impl::get_tensor_bufs() const
{
    std::list<TensorBuf *> tns_buf_list;
    for (auto &tns_buf : this->tns_bufs_storage) {
        tns_buf_list.emplace_back(tns_buf.get());
    }
    return tns_buf_list;
};

std::list<Tensor *> Model::Impl::get_tensors() const
{
    std::list<Tensor *> tns_list;
    for (auto &tns : this->tns_storage) {
        tns_list.emplace_back(tns.get());
    }
    return tns_list;
};

std::list<Op *> Model::Impl::get_ops() const
{
    std::list<Op *> ops;
    for (auto &op : this->ops_storage) {
        ops.emplace_back(op.get());
    }
    return ops;
};

// Returns the latest-declared operator that has the given tensor as its output.
const Op *Model::Impl::get_producer(Tensor *tns) const
{
    auto search = this->tns_to_producer.find(tns);
    if (search == this->tns_to_producer.end()) {
        return nullptr;
    }
    return search->second;
}

// Returns the set of operators that have the given tensor as one of their
// inputs.
const std::set<Op *> &Model::Impl::get_users(Tensor *tns) const
{
    auto search = this->tns_to_users.find(tns);
    if (search == this->tns_to_users.end()) {
        LOGERR("Not an existing tensor.");
    }
    return search->second;
}

// Returns true if the given tensor is not an input of any operator.
bool Model::Impl::is_no_user(Tensor *tns) const
{
    auto search = this->tns_to_users.find(tns);
    if (search == this->tns_to_users.end()) {
        return true;
    }
    return false;
}

std::list<const Op *> Model::Impl::get_leaf_ops() const
{
    std::list<const Op *> leaf_ops;
    for (auto &op : this->ops_storage) {
        bool is_leaf = true;
        for (auto &tns : op->outputs) {
            if (!this->is_no_user(tns)) {
                is_leaf = false;
                break;
            }
        }
        if (is_leaf) {
            leaf_ops.emplace_back(op.get());
        }
    }
    return leaf_ops;
}

std::list<const Op *> Model::Impl::get_producer_ops(const Op *op) const
{
    // Input tensors and output reference tensors are all producer tensors.
    std::vector<Tensor *> producer_tensors = op->inputs;
    for (auto &tns : op->output_refs) {
        producer_tensors.push_back(tns);
    }

    std::list<const Op *> producer_ops;
    for (auto &tns : producer_tensors) {
        const Op *producer_op = this->get_producer(tns);
        if (producer_op != nullptr) {
            producer_ops.emplace_back(producer_op);
        }
    }
    return producer_ops;
}

/// Returns the set of Ops that are user of the given Op's output.
std::list<const Op *> Model::Impl::get_user_ops(const Op *op) const
{
    std::list<const Op *> user_ops;
    for (auto &tns : op->outputs) {
        const std::set<Op *> &user_op_set = this->get_users(tns);
        for (auto &user_op : user_op_set) {
            user_ops.emplace_back(user_op);
        }
    }
    return user_ops;
}

const Op *Model::Impl::get_cyclic_op() const
{
    std::list<const Op *> leaf_ops = this->get_leaf_ops();
    std::set<const Op *> visited_ops;
    std::stack<const Op *> op_stack;
    for (auto &op : leaf_ops) {
        op_stack.push(op);
    }
    while (!op_stack.empty()) {
        const Op *op = op_stack.top();
        op_stack.pop();
        if (visited_ops.find(op) != visited_ops.end()) {
            return op;
        }
        visited_ops.insert(op);
        std::list<const Op *> producer_ops = this->get_producer_ops(op);
        for (auto &producer_op : producer_ops) {
            op_stack.push(producer_op);
        }
    }
    return nullptr;
}

//
Model::Model(int rank_) : impl{make_unique<Model::Impl>()}
{
    this->impl->rank = rank_;
}

Model::~Model() = default;

bool Model::verify() const
{
    const Op *cyclic_op = this->impl->get_cyclic_op();
    if (cyclic_op != nullptr) {
        LOG(WARN, "Cyclic dependency detected around Op ",
            cyclic_op->name.c_str(), " and its inputs.");
        return false;
    }
    return true;
}

} // namespace ark

```

`ark/model.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_MODEL_H_
#define ARK_MODEL_H_

#include "include/ark.h"
#include "ops/ops_common.h"
#include <list>
#include <map>
#include <set>

namespace ark {

class Model::Impl
{
  public:
    Impl() = default;
    ~Impl() = default;

    /// Create a new @ref TensorBuf object with @p bytes bytes.
    ///
    /// A common usage is setting @p bytes to 0 during declaring a model and let
    /// the scheduler determine the value after the model is completely defined.
    ///
    /// @param bytes the number of bytes of the @ref TensorBuf
    /// @return the created @ref TensorBuf
    TensorBuf *create_tensor_buf(const DimType bytes = 0);

    /// Remove a @ref TensorBuf object from the model.
    void destroy_tensor_buf(const TensorBuf *buf);

    /// Add a new @ref Op to the model.
    /// @param type the type of the @ref Op.
    /// @param prec_type the precision type of the @ref Op.
    /// @param inputs the input tensors of the @ref Op, including execution
    /// dependencies.
    /// @param output_refs the output reference tensors of the @ref Op. Output
    /// tensors are created based on these references.
    /// @param args the arguments of the @ref Op.
    /// @param name the name of the @ref Op.
    /// @param cfg_map the configuration map of the @ref Op
    /// @param gran_lev the granularity level of the @ref Op. Larger values
    /// should indicate finer-grained Ops. If it is -1, the granularity level
    /// will be automatically determined by the scheduler.
    /// @return the output tensors of the @ref Op.
    std::vector<Tensor *> add_op(const OpType type, const OpPrecType prec_type,
                                 const std::vector<Tensor *> &inputs,
                                 const std::vector<Tensor *> &output_refs,
                                 const OpArgs &args, const std::string &name,
                                 const OpConfigMap *cfg_map, int gran_lev = -1);

    /// Add a new @ref Op to the model.
    /// @param type the type of the @ref Op.
    /// @param prec_type the precision type of the @ref Op.
    /// @param inputs the input tensors of the @ref Op, including execution
    /// dependencies.
    /// @param output_refs the output reference tensors of the @ref Op. Output
    /// tensors are created based on these references.
    /// @param args the arguments of the @ref Op.
    /// @param name the name of the @ref Op.
    /// @param cfg_map the configuration map of the @ref Op
    /// @param gran_lev the granularity level of the @ref Op. Larger values
    /// should indicate finer-grained Ops. If it is -1, the granularity level
    /// will be automatically determined by the scheduler.
    /// @return the output tensors of the @ref Op.
    std::vector<Tensor *> add_op(Op &op);

    /// Delete an existing @ref Op from the model.
    /// @param op the existing @ref Op to be deleted.
    void delete_op(Op *op);

    /// Get references to all @ref TensorBuf objects.
    /// @return a list of @ref TensorBuf pointers.
    std::list<TensorBuf *> get_tensor_bufs() const;

    /// Get references to all @ref Tensor objects.
    /// @return a list of @ref Tensor pointers.
    std::list<Tensor *> get_tensors() const;

    /// Get references to all @ref Op objects.
    /// @return a list of @ref Op pointers.
    std::list<Op *> get_ops() const;

    /// Get the producer @ref Op of @p tns.
    /// @param tns the @ref Tensor to query.
    const Op *get_producer(Tensor *tns) const;

    /// Get the user @ref Op of @p tns.
    /// @param tns the @ref Tensor to query.
    const std::set<Op *> &get_users(Tensor *tns) const;

    /// True if @p tns has no user.
    /// @param tns the @ref Tensor to query.
    bool is_no_user(Tensor *tns) const;

    /// Model graph analysis

    /// Get a list of all operators that have no user.
    /// @return a list of @ref Op pointers.
    std::list<const Op *> get_leaf_ops() const;

    /// Get a list of all operators that produce inputs or output references
    /// of @p op.
    /// @param op the @ref Op to query.
    /// @return a list of @ref Op pointers.
    std::list<const Op *> get_producer_ops(const Op *op) const;

    /// Get a list of all operators that consume any output tensors of @p op.
    /// @param op the @ref Op to query.
    /// @return a list of @ref Op pointers.
    std::list<const Op *> get_user_ops(const Op *op) const;

    /// Check if there is any cyclic dependency in the model. If so, return
    /// the first cyclic @ref Op.
    /// @return the first cyclic @ref Op if there is any, otherwise nullptr.
    const Op *get_cyclic_op() const;

  protected:
    /// Rank of this model.
    int rank;
    /// Number of assigned EIDs.
    int next_eid = 0;

    friend class Model;

  private:
    /// Append a postfix to a name to make it unique.
    /// @param name the name to append postfix.
    /// @return the name with postfix.
    std::string append_name_postfix(const std::string &name);

    /// Stores all tensor buffers.
    std::list<std::unique_ptr<TensorBuf>> tns_bufs_storage;
    /// Stores all tensors.
    std::list<std::unique_ptr<Tensor>> tns_storage;
    /// Stores all Ops.
    std::list<std::unique_ptr<Op>> ops_storage;
    /// Maps a tensor to its producer Op.
    std::map<Tensor *, Op *> tns_to_producer;
    /// Maps a tensor to its user Ops.
    std::map<Tensor *, std::set<Op *>> tns_to_users;
    /// Count the number of tensors requested the same name.
    std::map<std::string, int> name_cnts;
};

} // namespace ark

#endif // ARK_MODEL_H_

```

`ark/model_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "include/ark.h"
#include "unittest/unittest_utils.h"

#define ITERATION 1

using namespace std;

ark::unittest::State test_simple_mm()
{
    // Hidden dimension of the dense layer.
    unsigned int units = 1024;
    // Input dimension of the dense layer.
    unsigned int in_dim = 1024;
    // Extra dimension of the input. CHANNEL=1 for 2D inputs.
    unsigned int channel = 128;
    // Batch size of the input.
    unsigned int batch_size = 1;

    ark::Model m;
    ark::Tensor *input = m.tensor({batch_size, channel, in_dim}, ark::FP16);
    ark::Tensor *weight = m.tensor({in_dim, units}, ark::FP16);
    m.matmul(input, weight);

    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    for (int i = 0; i < ITERATION; ++i) {
        UNITTEST(test_simple_mm);
    }
    return 0;
}

```

`ark/net/net_ib.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <cassert>
#include <cstdlib>
#include <cstring>
#include <infiniband/verbs.h>
#include <malloc.h>
#include <map>
#include <unistd.h>

#include "file_io.h"
#include "logging.h"
#include "net/net_ib.h"

namespace ark {

// IB memory region
NetIbMr::NetIbMr(void *mr_, void *buffer_) : mr{mr_}, buffer{buffer_}
{
    this->info.addr = (uint64_t)this->get_buf();
    this->info.rkey = this->get_rkey();
}
NetIbMr::~NetIbMr()
{
    // IB resources should be freed by NetIbMgr.
}
void *NetIbMr::get_addr() const
{
    return ((struct ibv_mr *)this->mr)->addr;
}
uint32_t NetIbMr::get_lkey() const
{
    return ((struct ibv_mr *)this->mr)->lkey;
}
uint32_t NetIbMr::get_rkey() const
{
    return ((struct ibv_mr *)this->mr)->rkey;
}
size_t NetIbMr::get_length() const
{
    return ((struct ibv_mr *)this->mr)->length;
}

// IB queue pair
NetIbQp::NetIbQp(void *qp_, int port_)
    : qp{qp_}, wrs{new struct ibv_send_wr[ARK_NET_IB_MAX_SENDS]},
      sges{new struct ibv_sge[ARK_NET_IB_MAX_SENDS]}, wrn{0}
{
    assert(qp_ != nullptr);
    struct ibv_context *ctx = ((struct ibv_qp *)qp_)->context;
    struct ibv_port_attr port_attr;
    if (ibv_query_port(ctx, port_, &port_attr) != 0) {
        LOGERR("failed to query IB port: %d", port_);
    }
    this->info.lid = port_attr.lid;
    this->info.port = port_;
    this->info.link_layer = port_attr.link_layer;
    this->info.qpn = ((struct ibv_qp *)qp_)->qp_num;
    this->info.mtu = port_attr.active_mtu;
    this->info.is_grh = (port_attr.flags & IBV_QPF_GRH_REQUIRED);
    if (port_attr.link_layer != IBV_LINK_LAYER_INFINIBAND ||
        this->info.is_grh) {
        union ibv_gid gid;
        if (ibv_query_gid(ctx, port_, 0, &gid) != 0) {
            LOGERR("failed to query GID");
        }
        this->info.spn = gid.global.subnet_prefix;
        this->info.iid = gid.global.interface_id;
    }
    if (this->init() != 0) {
        LOGERR("failed to modify QP to INIT");
    }
    std::memset(this->wrs, 0,
                sizeof(struct ibv_send_wr) * ARK_NET_IB_MAX_SENDS);
    std::memset(this->sges, 0, sizeof(struct ibv_sge) * ARK_NET_IB_MAX_SENDS);
    LOG(DEBUG, "QP INIT: qpn=", this->info.qpn);
}

NetIbQp::~NetIbQp()
{
    // IB resources should be freed by NetIbMgr.
    delete reinterpret_cast<struct ibv_send_wr *>(this->wrs);
    delete reinterpret_cast<struct ibv_sge *>(this->sges);
}

int NetIbQp::init()
{
    struct ibv_qp_attr qp_attr;
    std::memset(&qp_attr, 0, sizeof(struct ibv_qp_attr));
    qp_attr.qp_state = IBV_QPS_INIT;
    qp_attr.pkey_index = 0;
    qp_attr.port_num = this->info.port;
    qp_attr.qp_access_flags = IBV_ACCESS_REMOTE_WRITE;
    if (ibv_modify_qp((struct ibv_qp *)this->qp, &qp_attr,
                      IBV_QP_STATE | IBV_QP_PKEY_INDEX | IBV_QP_PORT |
                          IBV_QP_ACCESS_FLAGS) != 0) {
        LOG(WARN, "ibv_modify_qp failed (", errno, ")");
        return -1;
    }
    return 0;
}

int NetIbQp::rtr(const NetIbQp::Info *info)
{
    struct ibv_qp_attr qp_attr;
    std::memset(&qp_attr, 0, sizeof(struct ibv_qp_attr));
    qp_attr.qp_state = IBV_QPS_RTR;
    qp_attr.path_mtu = static_cast<ibv_mtu>(info->mtu);
    qp_attr.dest_qp_num = info->qpn;
    qp_attr.rq_psn = 0;
    qp_attr.max_dest_rd_atomic = 1;
    qp_attr.min_rnr_timer = 0x12;
    if (info->link_layer == IBV_LINK_LAYER_ETHERNET || info->is_grh) {
        qp_attr.ah_attr.is_global = 1;
        qp_attr.ah_attr.grh.dgid.global.subnet_prefix = info->spn;
        qp_attr.ah_attr.grh.dgid.global.interface_id = info->iid;
        qp_attr.ah_attr.grh.flow_label = 0;
        qp_attr.ah_attr.grh.sgid_index = 0;
        qp_attr.ah_attr.grh.hop_limit = 255;
        qp_attr.ah_attr.grh.traffic_class = 0;
    } else {
        qp_attr.ah_attr.is_global = 0;
    }
    qp_attr.ah_attr.dlid = info->lid;
    qp_attr.ah_attr.sl = 0;
    qp_attr.ah_attr.src_path_bits = 0;
    qp_attr.ah_attr.port_num = info->port;
    LOG(DEBUG, "QP RTR: qpn=", this->info.qpn, " remote=", info->qpn);
    if (ibv_modify_qp((struct ibv_qp *)this->qp, &qp_attr,
                      IBV_QP_STATE | IBV_QP_AV | IBV_QP_PATH_MTU |
                          IBV_QP_DEST_QPN | IBV_QP_RQ_PSN |
                          IBV_QP_MAX_DEST_RD_ATOMIC | IBV_QP_MIN_RNR_TIMER) !=
        0) {
        LOG(WARN, "ibv_modify_qp failed (", errno, ")");
        return -1;
    }
    return 0;
}

int NetIbQp::rts()
{
    struct ibv_qp_attr qp_attr;
    std::memset(&qp_attr, 0, sizeof(struct ibv_qp_attr));
    qp_attr.qp_state = IBV_QPS_RTS;
    qp_attr.timeout = 18;
    qp_attr.retry_cnt = 7;
    qp_attr.rnr_retry = 7;
    qp_attr.sq_psn = 0;
    qp_attr.max_rd_atomic = 1;
    if (ibv_modify_qp((struct ibv_qp *)this->qp, &qp_attr,
                      IBV_QP_STATE | IBV_QP_TIMEOUT | IBV_QP_RETRY_CNT |
                          IBV_QP_RNR_RETRY | IBV_QP_SQ_PSN |
                          IBV_QP_MAX_QP_RD_ATOMIC) != 0) {
        LOG(WARN, "ibv_modify_qp failed (", errno, ")");
        return -1;
    }
    LOG(DEBUG, "QP RTS: qpn=", this->info.qpn);
    return 0;
}

int NetIbQp::stage_send(void *mr, const NetIbMr::Info *info, int size,
                        uint64_t wr_id, unsigned int imm_data, int offset)
{
    if (this->wrn >= ARK_NET_IB_MAX_SENDS) {
        LOG(WARN, "wrn=", this->wrn);
        return -1;
    }
    int wrn = this->wrn;
    struct ibv_send_wr *wr_ = &((struct ibv_send_wr *)this->wrs)[wrn];
    struct ibv_sge *sge_ = &((struct ibv_sge *)this->sges)[wrn];
    std::memset(wr_, 0, sizeof(struct ibv_send_wr));
    std::memset(sge_, 0, sizeof(struct ibv_sge));
    wr_->wr_id = wr_id;
    wr_->sg_list = sge_;
    wr_->num_sge = 1;
    wr_->opcode = IBV_WR_RDMA_WRITE_WITH_IMM;
    wr_->imm_data = imm_data;
    wr_->send_flags = IBV_SEND_SIGNALED;
    wr_->wr.rdma.remote_addr = info->addr;
    wr_->wr.rdma.rkey = info->rkey;
    wr_->next = nullptr;
    sge_->addr = (uint64_t)(((NetIbMr *)mr)->get_buf()) + (uint64_t)offset;
    sge_->length = size;
    sge_->lkey = ((NetIbMr *)mr)->get_lkey();
    if (wrn > 0) {
        ((struct ibv_send_wr *)this->wrs)[wrn - 1].next = wr_;
    }
    // LOG(DEBUG, "stage_send addr=", (void *)sge_->addr,
    //     " remote_addr=", (void *)wr_->wr.rdma.remote_addr);
    this->wrn++;
    return this->wrn;
}

int NetIbQp::post_send()
{
    if (this->wrn == 0) {
        return 0;
    }
    struct ibv_send_wr *bad_wr;
    int ret = ibv_post_send((struct ibv_qp *)this->qp,
                            (struct ibv_send_wr *)this->wrs, &bad_wr);
    if (ret != 0) {
        LOG(WARN, "ibv_post_send failed (", ret, ")");
        return -1;
    }
    std::memset(this->wrs, 0, sizeof(struct ibv_send_wr) * this->wrn);
    std::memset(this->sges, 0, sizeof(struct ibv_sge) * this->wrn);
    this->wrn = 0;
    return 0;
}

int NetIbQp::post_recv(uint64_t wr_id)
{
    struct ibv_recv_wr wr, *bad_wr;
    wr.wr_id = wr_id;
    wr.sg_list = nullptr;
    wr.num_sge = 0;
    wr.next = nullptr;
    if (ibv_post_recv((struct ibv_qp *)this->qp, &wr, &bad_wr) != 0) {
        LOG(WARN, "ibv_post_recv failed (", errno, ")");
        return -1;
    }
    return 0;
}

////////////////////////////////////////////////////////////////////////////////

// Holds resources of a single IB device.
NetIbMgr::NetIbMgr(int ib_dev_id, bool sep_sc_rc_)
    : wcs{new struct ibv_wc[ARK_NET_IB_CQ_POLL_NUM]}, sep_sc_rc{sep_sc_rc_}
{
    int num;
    struct ibv_device **devices = ibv_get_device_list(&num);
    if (ib_dev_id >= num) {
        LOGERR("ib_dev_id=", ib_dev_id, " num=", num);
    }
    this->device_name = ibv_get_device_name(devices[ib_dev_id]);
    struct ibv_context *ctx_ = ibv_open_device(devices[ib_dev_id]);
    std::string ibdev_path(devices[ib_dev_id]->ibdev_path);
    ibv_free_device_list(devices);
    if (ctx_ == nullptr) {
        LOGERR("failed to open IB device: ", this->device_name);
    }
    this->ctx = ctx_;
    // Get the NUMA node
    this->numa_node = -1;
    if (is_dir(ibdev_path)) {
        std::string numa_node_path = ibdev_path + "/device/numa_node";
        if (is_file(numa_node_path)) {
            std::string numa_node_str = read_file(numa_node_path);
            this->numa_node = std::stoi(numa_node_str);
        }
    }
    LOG(DEBUG, "opened IB device: ", this->device_name, " numa_node ",
        this->numa_node);
    // Check available ports
    struct ibv_device_attr devAttr;
    if (ibv_query_device(ctx_, &devAttr) != 0) {
        LOGERR("failed to query IB device: ", this->device_name);
    }
    for (int port = 1; port <= devAttr.phys_port_cnt; port++) {
        struct ibv_port_attr portAttr;
        if (ibv_query_port(ctx_, port, &portAttr) != 0) {
            LOG(WARN, "failed to query IB port: ", port);
            continue;
        }
        if (portAttr.state != IBV_PORT_ACTIVE) {
            continue;
        }
        if (portAttr.link_layer != IBV_LINK_LAYER_INFINIBAND &&
            portAttr.link_layer != IBV_LINK_LAYER_ETHERNET) {
            continue;
        }
        this->ports.emplace_back(port);
    }
    //
    if (sep_sc_rc_) {
        this->scq =
            ibv_create_cq(ctx_, ARK_NET_IB_CQ_SIZE, nullptr, nullptr, 0);
        this->rcq =
            ibv_create_cq(ctx_, ARK_NET_IB_CQ_SIZE, nullptr, nullptr, 0);
        this->cq = nullptr;
    } else {
        this->cq = ibv_create_cq(ctx_, ARK_NET_IB_CQ_SIZE, nullptr, nullptr, 0);
        this->scq = nullptr;
        this->rcq = nullptr;
    }
    this->pd = ibv_alloc_pd(ctx_);
}

NetIbMgr::~NetIbMgr()
{
    for (auto &mr : this->mrs) {
        ibv_dereg_mr((struct ibv_mr *)mr->get_mr());
    }
    for (auto &qp : this->qps) {
        ibv_destroy_qp((struct ibv_qp *)qp->get_qp());
    }
    ibv_dealloc_pd((struct ibv_pd *)this->pd);
    if (this->sep_sc_rc) {
        ibv_destroy_cq((struct ibv_cq *)this->scq);
        ibv_destroy_cq((struct ibv_cq *)this->rcq);
    } else {
        ibv_destroy_cq((struct ibv_cq *)this->cq);
    }
    ibv_close_device((struct ibv_context *)this->ctx);
    delete reinterpret_cast<struct ibv_wc *>(this->wcs);
}

NetIbQp *NetIbMgr::create_qp(int port)
{
    if (port < 0) {
        port = this->ports[0];
    } else {
        bool found = false;
        for (auto &p : this->ports) {
            if (p == port) {
                found = true;
                break;
            }
        }
        if (!found) {
            LOG(WARN, "invalid port: ", port);
            return nullptr;
        }
    }
    struct ibv_qp_init_attr qp_init_attr;
    std::memset(&qp_init_attr, 0, sizeof(struct ibv_qp_init_attr));
    qp_init_attr.sq_sig_all = 0;
    if (this->sep_sc_rc) {
        qp_init_attr.send_cq = (struct ibv_cq *)this->scq;
        qp_init_attr.recv_cq = (struct ibv_cq *)this->rcq;
    } else {
        qp_init_attr.send_cq = (struct ibv_cq *)this->cq;
        qp_init_attr.recv_cq = (struct ibv_cq *)this->cq;
    }
    qp_init_attr.qp_type = IBV_QPT_RC;
    qp_init_attr.cap.max_send_wr = 8192;
    qp_init_attr.cap.max_recv_wr = 8192;
    qp_init_attr.cap.max_send_sge = 1;
    qp_init_attr.cap.max_recv_sge = 1;
    qp_init_attr.cap.max_inline_data = 0;
    struct ibv_qp *qp = ibv_create_qp((struct ibv_pd *)this->pd, &qp_init_attr);
    if (qp == nullptr) {
        LOG(WARN, "ibv_create_qp failed (", errno, ")");
        return nullptr;
    }
    this->qps.emplace_back(new NetIbQp{qp, port});
    return this->qps.back().get();
}

// Register a memory region for remote write.
NetIbMr *NetIbMgr::reg_mr(void *buffer, size_t size)
{
    if (size == 0) {
        return nullptr;
    }
    static __thread uintptr_t pageSize = 0;
    if (pageSize == 0) {
        pageSize = sysconf(_SC_PAGESIZE);
    }
    uintptr_t addr = (uintptr_t)buffer & -pageSize;
    size_t pages = ((uintptr_t)buffer + size - addr + pageSize - 1) / pageSize;
    struct ibv_mr *mr =
        ibv_reg_mr((struct ibv_pd *)this->pd, (void *)addr, pages * pageSize,
                   IBV_ACCESS_LOCAL_WRITE | IBV_ACCESS_REMOTE_WRITE |
                       IBV_ACCESS_REMOTE_READ | IBV_ACCESS_RELAXED_ORDERING);
    if (mr == nullptr) {
        LOG(WARN, "ibv_reg_mr failed (errno ", errno, "). pd ", std::hex,
            this->pd, " addr ", (void *)addr, std::dec, " size ", size,
            " pages ", pages, " pageSize ", pageSize);
        return nullptr;
    }
    LOG(DEBUG, "MR addr ", (void *)addr, " buffer ", (void *)buffer, " size ",
        pages * pageSize);
    this->mrs.emplace_back(new NetIbMr{mr, buffer});
    return this->mrs.back().get();
}

int NetIbMgr::poll_cq()
{
    if (this->sep_sc_rc) {
        LOGERR("poll_cq not supported for separate send/recv CQs");
    }
    int ret = ibv_poll_cq((struct ibv_cq *)this->cq, ARK_NET_IB_CQ_POLL_NUM,
                          (struct ibv_wc *)this->wcs);
    if (ret < 0) {
        LOG(WARN, "ibv_poll_cq failed (", errno, ")");
        return -1;
    }
    this->wcn = ret;
    return ret;
}

int NetIbMgr::poll_scq()
{
    if (!this->sep_sc_rc) {
        LOGERR("poll_scq not supported for single send/recv CQ");
    }
    int ret = ibv_poll_cq((struct ibv_cq *)this->scq, ARK_NET_IB_CQ_POLL_NUM,
                          (struct ibv_wc *)this->wcs);
    if (ret < 0) {
        LOG(WARN, "ibv_poll_cq failed (", errno, ")");
        return -1;
    }
    this->wcn = ret;
    return ret;
}

int NetIbMgr::poll_rcq()
{
    if (!this->sep_sc_rc) {
        LOGERR("poll_rcq not supported for single send/recv CQ");
    }
    int ret = ibv_poll_cq((struct ibv_cq *)this->rcq, ARK_NET_IB_CQ_POLL_NUM,
                          (struct ibv_wc *)this->wcs);
    if (ret < 0) {
        LOG(WARN, "ibv_poll_cq failed (", errno, ")");
        return -1;
    }
    this->wcn = ret;
    return ret;
}

int NetIbMgr::get_wc_status(int i) const
{
    if (i < 0 || i >= this->wcn) {
        return -1;
    }
    struct ibv_wc *wc = (struct ibv_wc *)this->wcs + i;
    return wc->status;
}

const char *NetIbMgr::get_wc_status_str(int i) const
{
    if (i < 0 || i >= this->wcn) {
        return nullptr;
    }
    struct ibv_wc *wc = (struct ibv_wc *)this->wcs + i;
    return ibv_wc_status_str(wc->status);
}

uint64_t NetIbMgr::get_wc_wr_id(int i) const
{
    if (i < 0 || i >= this->wcn) {
        return -1;
    }
    struct ibv_wc *wc = (struct ibv_wc *)this->wcs + i;
    return wc->wr_id;
}

unsigned int NetIbMgr::get_wc_imm_data(int i) const
{
    if (i < 0 || i >= this->wcn) {
        return -1;
    }
    struct ibv_wc *wc = (struct ibv_wc *)this->wcs + i;
    return wc->imm_data;
}

////////////////////////////////////////////////////////////////////////////////

std::map<int, NetIbMgr *> ARK_NET_IB_MGR_GLOBAL;

// Get a NetIbMgr instance
NetIbMgr *get_net_ib_mgr(int ib_dev_id)
{
    auto it = ARK_NET_IB_MGR_GLOBAL.find(ib_dev_id);
    if (it == ARK_NET_IB_MGR_GLOBAL.end()) {
        ARK_NET_IB_MGR_GLOBAL[ib_dev_id] = new NetIbMgr{ib_dev_id};
        return ARK_NET_IB_MGR_GLOBAL[ib_dev_id];
    }
    return it->second;
}

// Get the number of IB devices
int get_net_ib_device_num()
{
    int num;
    struct ibv_device **devices = ibv_get_device_list(&num);
    ibv_free_device_list(devices);
    return num;
}

#define DIVUP(x, y) (((x) + (y)-1) / (y))
#define ROUNDUP(x, y) (DIVUP((x), (y)) * (y))

void *page_memalign(std::size_t size)
{
    std::size_t page_size = sysconf(_SC_PAGESIZE);
    void *p;
    int size_aligned = ROUNDUP(size, page_size);
    p = memalign(page_size, size_aligned);
    if (p == nullptr) {
        return nullptr;
    }
    memset(p, 0, size);
    return p;
}

} // namespace ark

```

`ark/net/net_ib.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_NET_IB_H_
#define ARK_NET_IB_H_

#include <list>
#include <memory>
#include <string>
#include <vector>

#define ARK_NET_IB_CQ_SIZE 1024
#define ARK_NET_IB_CQ_POLL_NUM 4
#define ARK_NET_IB_MAX_SENDS 64

namespace ark {

// IB memory region
class NetIbMr
{
  public:
    // To be shared with the remote peer
    struct Info
    {
        uint64_t addr;
        uint32_t rkey;
    };

    NetIbMr(void *mr_, void *buffer_);
    ~NetIbMr();

    void *get_addr() const;
    uint32_t get_lkey() const;
    uint32_t get_rkey() const;
    size_t get_length() const;
    void *get_mr() const
    {
        return mr;
    }
    void *get_buf() const
    {
        return buffer;
    };
    const struct Info &get_info() const
    {
        return info;
    };

  private:
    void *mr;
    void *buffer;
    struct Info info;
};

// IB queue pair
class NetIbQp
{
  public:
    // To be shared with the remote peer
    struct Info
    {
        uint16_t lid;
        uint8_t port;
        uint8_t link_layer;
        uint32_t qpn;
        uint64_t spn;
        uint64_t iid;
        int mtu;
        bool is_grh;
    };

    NetIbQp(void *qp_, int port_);
    ~NetIbQp();
    int init();
    int rtr(const NetIbQp::Info *info);
    int rts();
    int stage_send(void *mr, const NetIbMr::Info *info, int size,
                   uint64_t wr_id, unsigned int imm_data, int offset = 0);
    int post_send();
    int post_recv(uint64_t wr_id);

    //
    void *get_qp() const
    {
        return qp;
    }
    const struct Info &get_info() const
    {
        return info;
    }

  private:
    void *qp;
    struct Info info;
    void *wrs;
    void *sges;
    int wrn;
};

// Holds resources of a single IB device.
class NetIbMgr
{
  public:
    NetIbMgr(int ib_dev_id, bool sep_sc_rc_ = false);
    ~NetIbMgr();

    // Create a new queue pair.
    NetIbQp *create_qp(int port = -1);
    // Register a memory region.
    NetIbMr *reg_mr(void *buffer, size_t size);
    //
    int poll_cq();
    int poll_scq();
    int poll_rcq();

    int get_numa_node() const
    {
        return numa_node;
    };
    void *get_wcs() const
    {
        return wcs;
    };
    int get_wcn() const
    {
        return wcn;
    };
    int get_wc_status(int i) const;
    const char *get_wc_status_str(int i) const;
    uint64_t get_wc_wr_id(int i) const;
    unsigned int get_wc_imm_data(int i) const;

  private:
    std::string device_name;
    int numa_node;
    void *ctx;
    void *cq;
    void *scq;
    void *rcq;
    void *pd;
    void *wcs;
    const bool sep_sc_rc;
    int wcn;
    std::vector<int> ports;
    std::list<std::unique_ptr<NetIbQp>> qps;
    std::list<std::unique_ptr<NetIbMr>> mrs;
};

NetIbMgr *get_net_ib_mgr(int ib_dev_id);
int get_net_ib_device_num();
void *page_memalign(std::size_t size);

} // namespace ark

#endif // ARK_NET_IB_H_

```

`ark/net/net_ib_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_logging.h"
#include "gpu/gpu_mem.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "ipc/ipc_coll.h"
#include "net/net_ib.h"
#include "unittest/unittest_utils.h"
#include <cstring>
#include <numa.h>
#include <string>

#define BW_TEST_BYTES 1073741824UL

static void numa_bind(int node)
{
    nodemask_t mask;
    nodemask_zero(&mask);
    nodemask_set_compat(&mask, node);
    numa_bind_compat(&mask);
}

static double sr_loop(ark::NetIbMgr *mgr, ark::NetIbQp *qp, ark::NetIbMr *mr1,
                      ark::NetIbMr *mr2, ark::NetIbMr::Info *rmi1,
                      ark::NetIbMr::Info *rmi2, int num_iter, int bytes,
                      bool is_recv, bool bidir)
{
    int ret;
    int max_num_wc = bidir ? 2 : 1;
    double start = ark::cpu_timer();
    for (int iter = 0; iter < num_iter; ++iter) {
        if (is_recv) {
            ret = qp->post_recv(0);
            UNITTEST_EQ(ret, 0);
            if (bidir) {
                ret = qp->stage_send(mr2, rmi2, bytes, 0, 0);
                UNITTEST_EQ(ret, 1);
                ret = qp->post_send();
                UNITTEST_EQ(ret, 0);
            }
        } else {
            if (bidir) {
                ret = qp->post_recv(0);
                UNITTEST_EQ(ret, 0);
            }
            ret = qp->stage_send(mr1, rmi1, bytes, 0, 0);
            UNITTEST_EQ(ret, 1);
            ret = qp->post_send();
            UNITTEST_EQ(ret, 0);
        }

        int num_wc = 0;
        do {
            ret = mgr->poll_cq();
            for (int i = 0; i < ret; ++i) {
                UNITTEST_EQ(mgr->get_wc_status(i), 0);
            }
            num_wc += ret;
        } while (num_wc < max_num_wc);
        UNITTEST_EQ(num_wc, max_num_wc);
    }
    return ark::cpu_timer() - start;
}

int test_net_ib_cpu_internal(std::size_t bytes, bool is_recv)
{
    int ret;
    int rank = is_recv ? 0 : 1;
    int dev = (ark::get_net_ib_device_num() >= 2) ? rank : 0;
    bytes = (bytes + 3) & -4;

    ark::NetIbMgr mgr{dev};

    void *buf = ark::page_memalign(bytes);
    UNITTEST_NE(buf, (void *)nullptr);
    ark::NetIbMr *mr = mgr.reg_mr(buf, bytes);
    UNITTEST_NE(mr, (ark::NetIbMr *)nullptr);

    ark::NetIbQp *qp = mgr.create_qp();
    UNITTEST_NE(qp, (ark::NetIbQp *)nullptr);

    const ark::NetIbMr::Info &lmi = mr->get_info();
    const ark::NetIbQp::Info &lqi = qp->get_info();

    ark::IpcAllGather iag_mr{"rdma_test_mr", rank, 2, &lmi, sizeof(lmi)};
    ark::IpcAllGather iag_qp{"rdma_test_qp", rank, 2, &lqi, sizeof(lqi)};
    iag_mr.sync();
    iag_qp.sync();

    ark::NetIbMr::Info rmi;
    ark::NetIbQp::Info rqi;
    std::memcpy(&rmi, iag_mr.get_data(rank == 0 ? 1 : 0), sizeof(rmi));
    std::memcpy(&rqi, iag_qp.get_data(rank == 0 ? 1 : 0), sizeof(rqi));

    ret = qp->rtr(&rqi);
    UNITTEST_EQ(ret, 0);

    ret = qp->rts();
    UNITTEST_EQ(ret, 0);

    // Leverage just for sync.
    iag_qp.sync();

    for (int iter = 0; iter < 1000; ++iter) {
        if (is_recv) {
            ret = qp->post_recv(0);
            UNITTEST_EQ(ret, 0);

            do {
                ret = mgr.poll_cq();
            } while (ret == 0);

            UNITTEST_EQ(ret, 1);
            if (iter == 9999) {
                UNITTEST_EQ(*(int *)buf, 9999);
                UNITTEST_EQ(*((int *)buf + 1), 0);
            }
        } else {
            // Should be sent
            *(int *)buf = iter;
            // Should not be sent
            *((int *)buf + 1) = iter;

            ret = qp->stage_send(mr, &rmi, sizeof(int), 0, 0);
            UNITTEST_EQ(ret, 1);

            ret = qp->post_send();
            UNITTEST_EQ(ret, 0);

            do {
                ret = mgr.poll_cq();
            } while (ret == 0);
            UNITTEST_EQ(ret, 1);
            UNITTEST_EQ(mgr.get_wc_status(0), 0);
        }
    }
    return 0;
}

int test_net_ib_cpu_bw_internal(std::size_t bytes, bool is_recv)
{
    int ret;
    int num_iter = 10;
    int rank = is_recv ? 0 : 1;
    int dev = rank + 1;
    bytes = (bytes + 3) & -4;

    ark::NetIbMgr mgr{(ark::get_net_ib_device_num() >= 2) ? dev : 0};

    numa_bind(mgr.get_numa_node());

    void *buf = ark::page_memalign(bytes);
    UNITTEST_NE(buf, (void *)nullptr);
    void *buf2 = ark::page_memalign(bytes);
    UNITTEST_NE(buf2, (void *)nullptr);
    ark::NetIbMr *mr = mgr.reg_mr(buf, bytes);
    UNITTEST_NE(mr, (ark::NetIbMr *)nullptr);
    ark::NetIbMr *mr2 = mgr.reg_mr(buf2, bytes);
    UNITTEST_NE(mr2, (ark::NetIbMr *)nullptr);

    ark::NetIbQp *qp = mgr.create_qp();
    UNITTEST_NE(qp, (ark::NetIbQp *)nullptr);

    const ark::NetIbMr::Info &lmi = mr->get_info();
    const ark::NetIbMr::Info &lmi2 = mr->get_info();
    const ark::NetIbQp::Info &lqi = qp->get_info();

    ark::IpcAllGather iag_mr{"rdma_test_mr", rank, 2, &lmi, sizeof(lmi)};
    ark::IpcAllGather iag_mr2{"rdma_test_mr2", rank, 2, &lmi2, sizeof(lmi2)};
    ark::IpcAllGather iag_qp{"rdma_test_qp", rank, 2, &lqi, sizeof(lqi)};
    iag_mr.sync();
    iag_mr2.sync();
    iag_qp.sync();

    ark::NetIbMr::Info rmi;
    ark::NetIbMr::Info rmi2;
    ark::NetIbQp::Info rqi;
    std::memcpy(&rmi, iag_mr.get_data(rank == 0 ? 1 : 0), sizeof(rmi));
    std::memcpy(&rmi2, iag_mr2.get_data(rank == 0 ? 1 : 0), sizeof(rmi2));
    std::memcpy(&rqi, iag_qp.get_data(rank == 0 ? 1 : 0), sizeof(rqi));

    ret = qp->rtr(&rqi);
    UNITTEST_EQ(ret, 0);

    ret = qp->rts();
    UNITTEST_EQ(ret, 0);

    // Leverage just for sync.
    iag_qp.sync();

    double elapsed;
    elapsed = sr_loop(&mgr, qp, mr, mr2, &rmi, &rmi2, num_iter, bytes, is_recv,
                      false);
    LOG(ark::INFO, "Uni-dir: ", bytes * num_iter / elapsed / 1e9, " GB/s");
    elapsed =
        sr_loop(&mgr, qp, mr, mr2, &rmi, &rmi2, num_iter, bytes, is_recv, true);
    LOG(ark::INFO, "Bi-dir: ", bytes * num_iter / elapsed / 1e9, " GB/s");
    return 0;
}

int test_net_ib_gpu_internal(std::size_t bytes, bool is_recv)
{
    int ret;
    int rank = is_recv ? 0 : 1;
    int dev = (ark::get_net_ib_device_num() >= 2) ? rank : 0;
    bytes = (bytes + 3) & -4;

    // Create a CUDA context
    CULOG(cuInit(0));
    CUdevice cudev;
    CUcontext cuctx;
    CULOG(cuDeviceGet(&cudev, rank));
    CULOG(cuCtxCreate(&cuctx, 0, cudev));
    CULOG(cuCtxSetCurrent(cuctx));

    ark::NetIbMgr mgr{dev};

    ark::GpuMem mem{"gpu_mem_" + std::to_string(rank), bytes, true};
    void *buf = mem.href();
    ark::NetIbMr *mr = mgr.reg_mr((void *)mem.ref(), bytes);
    UNITTEST_NE(mr, (ark::NetIbMr *)nullptr);

    ark::NetIbQp *qp = mgr.create_qp();
    UNITTEST_NE(qp, (ark::NetIbQp *)nullptr);

    const ark::NetIbMr::Info &lmi = mr->get_info();
    const ark::NetIbQp::Info &lqi = qp->get_info();

    ark::IpcAllGather iag_mr{"rdma_test_mr", rank, 2, &lmi, sizeof(lmi)};
    ark::IpcAllGather iag_qp{"rdma_test_qp", rank, 2, &lqi, sizeof(lqi)};
    iag_mr.sync();
    iag_qp.sync();

    ark::NetIbMr::Info rmi;
    ark::NetIbQp::Info rqi;
    std::memcpy(&rmi, iag_mr.get_data(rank == 0 ? 1 : 0), sizeof(rmi));
    std::memcpy(&rqi, iag_qp.get_data(rank == 0 ? 1 : 0), sizeof(rqi));

    ret = qp->rtr(&rqi);
    UNITTEST_EQ(ret, 0);

    ret = qp->rts();
    UNITTEST_EQ(ret, 0);

    // Leverage just for sync.
    iag_qp.sync();

    for (int iter = 0; iter < 1000; ++iter) {
        if (is_recv) {
            ret = qp->post_recv(0);
            UNITTEST_EQ(ret, 0);

            do {
                ret = mgr.poll_cq();
            } while (ret == 0);

            UNITTEST_EQ(ret, 1);
            if (iter == 9999) {
                UNITTEST_EQ(*(volatile int *)buf, 9999);
                UNITTEST_EQ(*((volatile int *)buf + 1), 0);
            }
        } else {
            // Should be sent
            *(volatile int *)buf = iter;
            // Should not be sent
            *((volatile int *)buf + 1) = iter;

            ret = qp->stage_send(mr, &rmi, sizeof(int), 0, 0);
            UNITTEST_EQ(ret, 1);

            ret = qp->post_send();
            UNITTEST_EQ(ret, 0);

            do {
                ret = mgr.poll_cq();
            } while (ret == 0);
            UNITTEST_EQ(ret, 1);
            UNITTEST_EQ(mgr.get_wc_status(0), 0);
        }
    }
    return 0;
}

int test_net_ib_gpu_bw_internal(std::size_t bytes, bool is_recv)
{
    int ret;
    int num_iter = 10;
    int rank = is_recv ? 0 : 1;
    int dev = rank + 1;
    bytes = (bytes + 3) & -4;

    // Create a CUDA context
    CULOG(cuInit(0));
    CUdevice cudev;
    CUcontext cuctx;
    CULOG(cuDeviceGet(&cudev, dev));
    CULOG(cuCtxCreate(&cuctx, 0, cudev));
    CULOG(cuCtxSetCurrent(cuctx));

    ark::NetIbMgr mgr{(ark::get_net_ib_device_num() >= 2) ? dev : 0};

    numa_bind(mgr.get_numa_node());

    ark::GpuMem mem{"gpu_mem_" + std::to_string(rank), bytes, true};
    ark::GpuMem mem2{"gpu_mem2_" + std::to_string(rank), bytes, true};
    ark::NetIbMr *mr = mgr.reg_mr((void *)mem.ref(), bytes);
    ark::NetIbMr *mr2 = mgr.reg_mr((void *)mem2.ref(), bytes);
    UNITTEST_NE(mr, (ark::NetIbMr *)nullptr);
    UNITTEST_NE(mr2, (ark::NetIbMr *)nullptr);

    ark::NetIbQp *qp = mgr.create_qp();
    UNITTEST_NE(qp, (ark::NetIbQp *)nullptr);

    const ark::NetIbMr::Info &lmi = mr->get_info();
    const ark::NetIbMr::Info &lmi2 = mr2->get_info();
    const ark::NetIbQp::Info &lqi = qp->get_info();

    ark::IpcAllGather iag_mr{"rdma_test_mr", rank, 2, &lmi, sizeof(lmi)};
    ark::IpcAllGather iag_mr2{"rdma_test_mr2", rank, 2, &lmi2, sizeof(lmi2)};
    ark::IpcAllGather iag_qp{"rdma_test_qp", rank, 2, &lqi, sizeof(lqi)};
    iag_mr.sync();
    iag_mr2.sync();
    iag_qp.sync();

    ark::NetIbMr::Info rmi;
    ark::NetIbMr::Info rmi2;
    ark::NetIbQp::Info rqi;
    std::memcpy(&rmi, iag_mr.get_data(rank == 0 ? 1 : 0), sizeof(rmi));
    std::memcpy(&rmi2, iag_mr2.get_data(rank == 0 ? 1 : 0), sizeof(rmi2));
    std::memcpy(&rqi, iag_qp.get_data(rank == 0 ? 1 : 0), sizeof(rqi));

    ret = qp->rtr(&rqi);
    UNITTEST_EQ(ret, 0);

    ret = qp->rts();
    UNITTEST_EQ(ret, 0);

    // Leverage just for sync.
    iag_qp.sync();

    double elapsed;
    elapsed = sr_loop(&mgr, qp, mr, mr2, &rmi, &rmi2, num_iter, bytes, is_recv,
                      false);
    LOG(ark::INFO, "Uni-dir: ", bytes * num_iter / elapsed / 1e9, " GB/s");
    elapsed =
        sr_loop(&mgr, qp, mr, mr2, &rmi, &rmi2, num_iter, bytes, is_recv, true);
    LOG(ark::INFO, "Bi-dir: ", bytes * num_iter / elapsed / 1e9, " GB/s");
    return 0;
}

//
ark::unittest::State test_net_ib_cpu()
{
    for (int i = 0; i < 100; ++i) {
        int pid0 = ark::utils::proc_spawn([] {
            ark::unittest::Timeout timeout{30};
            std::size_t bytes = 1024 * 1024;
            return test_net_ib_cpu_internal(bytes, true);
        });
        UNITTEST_NE(pid0, -1);
        int pid1 = ark::utils::proc_spawn([] {
            ark::unittest::Timeout timeout{30};
            std::size_t bytes = 1024 * 1024;
            return test_net_ib_cpu_internal(bytes, false);
        });
        UNITTEST_NE(pid1, -1);

        int ret = ark::utils::proc_wait({pid0, pid1});
        UNITTEST_EQ(ret, 0);
    }
    return ark::unittest::SUCCESS;
}

//
ark::unittest::State test_net_ib_cpu_bw()
{
    int pid0 = ark::utils::proc_spawn([] {
        ark::unittest::Timeout timeout{30};
        std::size_t bytes = BW_TEST_BYTES;
        return test_net_ib_cpu_bw_internal(bytes, true);
    });
    UNITTEST_NE(pid0, -1);
    int pid1 = ark::utils::proc_spawn([] {
        ark::unittest::Timeout timeout{30};
        std::size_t bytes = BW_TEST_BYTES;
        return test_net_ib_cpu_bw_internal(bytes, false);
    });
    UNITTEST_NE(pid1, -1);

    int ret = ark::utils::proc_wait({pid0, pid1});
    UNITTEST_EQ(ret, 0);

    return ark::unittest::SUCCESS;
}

//
ark::unittest::State test_net_ib_gpu()
{
    for (int i = 0; i < 10; ++i) {
        int pid0 = ark::utils::proc_spawn([] {
            ark::unittest::Timeout timeout{30};
            std::size_t bytes = 1024 * 1024;
            return test_net_ib_gpu_internal(bytes, true);
        });
        UNITTEST_NE(pid0, -1);
        int pid1 = ark::utils::proc_spawn([] {
            ark::unittest::Timeout timeout{30};
            std::size_t bytes = 1024 * 1024;
            return test_net_ib_gpu_internal(bytes, false);
        });
        UNITTEST_NE(pid1, -1);

        int ret = ark::utils::proc_wait({pid0, pid1});
        UNITTEST_EQ(ret, 0);
    }
    return ark::unittest::SUCCESS;
}

//
ark::unittest::State test_net_ib_gpu_bw()
{
    int pid0 = ark::utils::proc_spawn([] {
        ark::unittest::Timeout timeout{30};
        std::size_t bytes = BW_TEST_BYTES;
        return test_net_ib_gpu_bw_internal(bytes, true);
    });
    UNITTEST_NE(pid0, -1);
    int pid1 = ark::utils::proc_spawn([] {
        ark::unittest::Timeout timeout{30};
        std::size_t bytes = BW_TEST_BYTES;
        return test_net_ib_gpu_bw_internal(bytes, false);
    });
    UNITTEST_NE(pid1, -1);

    int ret = ark::utils::proc_wait({pid0, pid1});
    UNITTEST_EQ(ret, 0);

    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_net_ib_cpu);
    UNITTEST(test_net_ib_gpu);
    UNITTEST(test_net_ib_cpu_bw);
    UNITTEST(test_net_ib_gpu_bw);
    return 0;
}

```

`ark/ops/kernels/simple_add.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "ark_kernels.h"
// CAUTION: len should be even.

template <typename T>
__device__ void simple_add(T *c, T *a, T *b, unsigned int bs, unsigned int len)
{
    for (unsigned int i = 0; i < bs; ++i) {
        for (unsigned int tid = threadIdx.x + blockIdx.x * blockDim.x;
             tid < len; tid += blockDim.x * gridDim.x) {
            c[i * len + tid] = a[i * len + tid] + b[tid];
        }
    }
}

extern "C" __global__ void simple_add_fp32(float *c, float *a, float *b,
                                           unsigned int bs, unsigned int len)
{
    simple_add<float>(c, a, b, bs, len);
}

extern "C" __global__ void simple_add_fp16(ark::half *c, ark::half *a,
                                           ark::half *b, unsigned int bs,
                                           unsigned int len)
{
    simple_add<__half2>((__half2 *)c, (__half2 *)a, (__half2 *)b, bs, len / 2);
}

```

`ark/ops/kernels/simple_dot.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "ark_kernels.h"
// CAUTION: len should be even.
extern "C" __global__ void simple_dot(float *c, ark::half *a, ark::half *b,
                                      unsigned int len)
{
    unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid == 0) {
        unsigned int num = len / 2;
        if (num == 0) {
            *c = 0;
            return;
        }
        // __half2 *pa = (__half2 *)a;
        // __half2 *pb = (__half2 *)b;
        // __half2 sum = __hmul2(*pa, *pb);
        // for (unsigned int i = 1; i < num; ++i) {
        //     sum = __hadd2(sum, __hmul2(pa[i], pb[i]));
        // }
        // *c = __half2float(__hadd(__low2half(sum), __high2half(sum)));
        __half2 *pa = (__half2 *)a;
        __half2 *pb = (__half2 *)b;
        __half sum = 0;
        for (unsigned int i = 0; i < num; ++i) {
            __half2 tmp = __hmul2(pa[i], pb[i]);
            // printf("%f\n", __half2float(sum));
            sum = __hadd(sum, __low2half(tmp));
            // printf("%f\n", __half2float(sum));
            sum = __hadd(sum, __high2half(tmp));
        }
        // printf("%f\n", __half2float(sum));
        *c = __half2float(sum);
    }
}

```

`ark/ops/kernels/simple_matmul_nt.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

// Simple mixed-precision matrix multiplication (C = A x B).
// Assume `A` is in column-major and `B` is in row-major.
// CAUTION: `m` and `n` should be even numbers!
#include "ark_kernels.h"
extern "C" __global__ void simple_matmul_nt(ark::half *C, ark::half *A,
                                            ark::half *B, unsigned int m,
                                            unsigned int n, unsigned int k,
                                            bool is_relu)
{
    unsigned int coldiv2 = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int rowdiv2 = blockIdx.y * blockDim.y + threadIdx.y;
    unsigned int mdiv2 = m >> 1;
    unsigned int ndiv2 = n >> 1;
    unsigned int col = coldiv2 << 1;
    unsigned int row = rowdiv2 << 1;
    if (col < n && row < m) {
        __half2 *pA = (__half2 *)A;
        __half2 *pB = (__half2 *)B;
        __half2 *pC = (__half2 *)C;
#if 0
        float2 s2_0;
        float2 s2_1;
        s2_0.x = 0;
        s2_0.y = 0;
        s2_1.x = 0;
        s2_1.y = 0;
        for (unsigned int i = 0; i < k; ++i) {
            // (2x1) x (1x2) = (2x2)
            float2 a2 = __half22float2(pA[i * mdiv2 + rowdiv2]);
            float2 b2 = __half22float2(pB[i * ndiv2 + coldiv2]);
            s2_0.x += a2.x * b2.x;
            s2_0.y += a2.y * b2.x;
            s2_1.x += a2.x * b2.y;
            s2_1.y += a2.y * b2.y;
        }
        pC[col * mdiv2 + rowdiv2] = __float22half2_rn(s2_0);
        pC[col * mdiv2 + mdiv2 + rowdiv2] = __float22half2_rn(s2_1);
#else
        __half2 s2_0 = __half2half2((__half)0x0);
        __half2 s2_1 = __half2half2((__half)0x0);
        for (unsigned int i = 0; i < k; ++i) {
            // __half2 lb = __low2half2(pB[i * ndiv2 + coldiv2]);
            // __half2 hb = __high2half2(pB[i * ndiv2 + coldiv2]);
            // s2_0 += pA[i * mdiv2 + rowdiv2] * lb;
            // s2_1 += pA[i * mdiv2 + rowdiv2] * hb;
            __half a2x = __low2half(pA[i * mdiv2 + rowdiv2]);
            __half a2y = __high2half(pA[i * mdiv2 + rowdiv2]);
            __half b2x = __low2half(pB[i * ndiv2 + coldiv2]);
            __half b2y = __high2half(pB[i * ndiv2 + coldiv2]);
            s2_0 = __hadd2(s2_0, __halves2half2(a2x * b2x, a2y * b2x));
            s2_1 = __hadd2(s2_1, __halves2half2(a2x * b2y, a2y * b2y));
        }
        if (is_relu) {
            pC[col * mdiv2 + rowdiv2] = s2_0;
            pC[col * mdiv2 + mdiv2 + rowdiv2] = s2_1;
        } else {
            float2 fs2_0 = __half22float2(s2_0);
            float2 fs2_1 = __half22float2(s2_1);
            pC[col * mdiv2 + rowdiv2] =
                __floats2half2_rn(fmaxf(fs2_0.x, 0.0f), fmaxf(fs2_0.y, 0.0f));
            pC[col * mdiv2 + mdiv2 + rowdiv2] =
                __floats2half2_rn(fmaxf(fs2_1.x, 0.0f), fmaxf(fs2_1.y, 0.0f));
        }
#endif
    }
}

```

`ark/ops/kernels/simple_mul.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "ark_kernels.h"
// CAUTION: len should be even.

template <typename T>
__device__ void simple_mul(T *c, T *a, T *b, unsigned int bs, unsigned int len)
{
    unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
    while (tid < len) {
        for (unsigned int i = 0; i < bs; ++i) {
            for (unsigned int j = 0; j < len; ++j) {
                c[i * len + j] = a[i * len + j] * b[j];
            }
        }
        tid += gridDim.x * blockDim.x;
    }
}

extern "C" __global__ void simple_mul_fp32(float *c, float *a, float *b,
                                           unsigned int bs, unsigned int len)
{
    simple_mul<float>(c, a, b, bs, len);
}

extern "C" __global__ void simple_mul_fp16(ark::half *c, ark::half *a,
                                           ark::half *b, unsigned int bs,
                                           unsigned int len)
{
    simple_mul<__half2>((__half2 *)c, (__half2 *)a, (__half2 *)b, bs, len / 2);
}

```

`ark/ops/kernels/simple_reduce.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "ark_kernels.h"
// CAUTION: n*m should be even.
extern "C" __global__ void simple_reduce(ark::half *y, ark::half *x,
                                         unsigned int m, unsigned int n,
                                         unsigned int k, bool is_relu)
{
    unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int num = n * m / 2;
    while (tid < num) {
        __half2 *py = (__half2 *)y;
        __half2 *px = (__half2 *)x;
        __half2 sum = px[tid];
        for (unsigned int i = 1; i < k; ++i) {
            sum = __hadd2(sum, px[tid + i * num]);
        }
        if (is_relu) {
            float2 fsum = __half22float2(sum);
            py[tid] =
                __floats2half2_rn(fmaxf(fsum.x, 0.0f), fmaxf(fsum.y, 0.0f));
        } else {
            py[tid] = sum;
        }
        tid += gridDim.x * blockDim.x;
    }
}

```

`ark/ops/kernels/simple_scale.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "ark_kernels.h"
// CAUTION: len should be even.
extern "C" __global__ void simple_scale(ark::half *y, ark::half *x, float val,
                                        unsigned int len)
{
    unsigned int tid = blockIdx.x * blockDim.x + threadIdx.x;
    unsigned int num = len / 2;
    __half2 val2 = __float2half2_rn(val);
    __half2 *px = (__half2 *)x;
    __half2 *py = (__half2 *)y;
    while (tid < num) {
        py[tid] = __hmul2(px[tid], val2);
        tid += gridDim.x * blockDim.x;
    }
}

```

`ark/ops/ops_add.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include "tensor.h"

using namespace std;

namespace ark {

extern const OpConfigMap ArithmeticConfigMap;

AddOp::AddOp(OpPrecType prec_type, Tensor *input, Tensor *other, Tensor *output,
             const string &name)
    : Op{OP_ADD, prec_type, {input, other},       {output},
         {},     name,      &ArithmeticConfigMap, -1,
         true}
{
}

std::string AddOp::function_name(const OpConfig &cfg) const
{
    Tensor *input = this->inputs[0];
    Tensor *other = this->inputs[1];
    Tensor *output = this->outputs[0];

    int ndims = output->shape.ndims();
    const OpTile &tile_out = cfg.output_tiles[0];
    CHECK(output->ldims[ndims - 1] % tile_out.y == 0);
    if (ndims > 1) {
        CHECK(output->ldims[ndims - 2] % tile_out.x == 0);
    } else {
        CHECK(tile_out.x == 1);
    }

    Dims unit_out_dims{1, 1, tile_out.x, tile_out.y};
    return Op::function_name("ark::add", {{
                                             input->ldims.dims4(),  // In0Dims
                                             input->shape.dims4(),  // In0Shape
                                             other->ldims.dims4(),  // In1Dims
                                             other->shape.dims4(),  // In1Shape
                                             output->ldims.dims4(), // OutDims
                                             output->shape.dims4(), // OutShape
                                             unit_out_dims,      // UnitOutDims
                                             cfg.num_warps * 32, // NumThreads
                                             cfg.smem_bytes,     // SmemBytes
                                         }});
}

Tensor *Model::add(Tensor *input, Tensor *other, Tensor *output,
                   const string &name)
{
    LOG(DEBUG, "add ", input->shape, " ", other->shape);
    CHECK(input != nullptr);
    CHECK(other != nullptr);
    OpPrecType pt;
    if (input->type == FP16) {
        pt = OP_PREC_FP16;
    } else if (input->type == FP32) {
        pt = OP_PREC_FP32;
    } else {
        LOGERR("unsupported input data type: ", type_str(input->type));
    }
    if (input->type != other->type) {
        LOGERR("input data types mismatch: ", type_str(input->type), ", ",
               type_str(other->type));
    }
    if (output != nullptr && input->type != output->type) {
        LOGERR("invalid output data type: ", type_str(output->type));
    }
    Dims output_shape = broadcast(input->shape, other->shape);
    if (output == nullptr) {
        output = this->tensor(output_shape, input->type);
    } else if (output->shape != output_shape) {
        LOGERR("invalid output shape: ", output->shape);
    } else if (output == input) {
        output = this->identity(output);
    }
    AddOp op{pt, input, other, output, name};
    return this->impl->add_op(op)[0];
}

const OpConfigMap ArithmeticConfigMap = {
    {{OP_ARCH_CUDA_70, OP_PREC_FP32},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {8, 0, {{128, 256}, {128, 256}}, {{128, 256}}, false, false},
         {8, 0, {{256, 128}, {256, 128}}, {{256, 128}}, false, false},
         {8, 0, {{128, 128}, {128, 128}}, {{128, 128}}, false, false},
         {4, 0, {{64, 64}, {64, 64}}, {{64, 64}}, false, false},
         {2, 0, {{32, 64}, {32, 64}}, {{32, 64}}, false, false},
         {1, 0, {{16, 64}, {16, 64}}, {{16, 64}}, false, false},
         {1, 0, {{8, 64}, {8, 64}}, {{8, 64}}, false, false},
         {1, 0, {{2, 128}, {2, 128}}, {{2, 128}}, false, false},
         {1, 0, {{4, 64}, {4, 64}}, {{4, 64}}, false, false},
         {1, 0, {{2, 64}, {2, 64}}, {{2, 64}}, false, false},
         {1, 0, {{1, 64}, {1, 64}}, {{1, 64}}, false, false},
         {1, 0, {{1, 32}, {1, 32}}, {{1, 32}}, false, false},
     }},
    {{OP_ARCH_CUDA_70, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {4, 0, {{64, 64}, {64, 64}}, {{64, 64}}, false, false},
         {2, 0, {{32, 64}, {32, 64}}, {{32, 64}}, false, false},
         {1, 0, {{16, 64}, {16, 64}}, {{16, 64}}, false, false},
         {1, 0, {{8, 64}, {8, 64}}, {{8, 64}}, false, false},
         {1, 0, {{2, 128}, {2, 128}}, {{2, 128}}, false, false},
         {1, 0, {{4, 64}, {4, 64}}, {{4, 64}}, false, false},
         {1, 0, {{2, 64}, {2, 64}}, {{2, 64}}, false, false},
         {1, 0, {{1, 64}, {1, 64}}, {{1, 64}}, false, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_FP32},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {8, 0, {{128, 256}, {128, 256}}, {{128, 256}}, false, false},
         {8, 0, {{256, 128}, {256, 128}}, {{256, 128}}, false, false},
         {8, 0, {{128, 128}, {128, 128}}, {{128, 128}}, false, false},
         {4, 0, {{64, 64}, {64, 64}}, {{64, 64}}, false, false},
         {2, 0, {{32, 64}, {32, 64}}, {{32, 64}}, false, false},
         {1, 0, {{16, 64}, {16, 64}}, {{16, 64}}, false, false},
         {1, 0, {{8, 64}, {8, 64}}, {{8, 64}}, false, false},
         {1, 0, {{2, 128}, {2, 128}}, {{2, 128}}, false, false},
         {1, 0, {{4, 64}, {4, 64}}, {{4, 64}}, false, false},
         {1, 0, {{2, 64}, {2, 64}}, {{2, 64}}, false, false},
         {1, 0, {{1, 128}, {1, 128}}, {{1, 128}}, false, false},
         {1, 0, {{1, 64}, {1, 64}}, {{1, 64}}, false, false},
         {1, 0, {{1, 32}, {1, 32}}, {{1, 32}}, false, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {8, 0, {{128, 256}, {128, 256}}, {{128, 256}}, false, false},
         {8, 0, {{256, 128}, {256, 128}}, {{256, 128}}, false, false},
         {8, 0, {{128, 128}, {128, 128}}, {{128, 128}}, false, false},
         {4, 0, {{64, 64}, {64, 64}}, {{64, 64}}, false, false},
         {2, 0, {{32, 64}, {32, 64}}, {{32, 64}}, false, false},
         {1, 0, {{16, 64}, {16, 64}}, {{16, 64}}, false, false},
         {1, 0, {{8, 64}, {8, 64}}, {{8, 64}}, false, false},
         {1, 0, {{2, 128}, {2, 128}}, {{2, 128}}, false, false},
         {1, 0, {{4, 64}, {4, 64}}, {{4, 64}}, false, false},
         {1, 0, {{2, 64}, {2, 64}}, {{2, 64}}, false, false},
         {1, 0, {{1, 256}, {1, 256}}, {{1, 256}}, false, false},
         {1, 0, {{1, 128}, {1, 128}}, {{1, 128}}, false, false},
         {1, 0, {{1, 64}, {1, 64}}, {{1, 64}}, false, false},
     }},
};

} // namespace ark

```

`ark/ops/ops_add_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "include/ark.h"
#include "ops_test_common.h"
#include "unittest/unittest_utils.h"

ark::unittest::State test_add_fp32()
{
    test_bcast_fp32("add", 2, 1024, 512);
    test_bcast_fp32("add", 1, 1, 64);
    test_bcast_fp32("add", 1, 128, 128);
    test_bcast_fp32("add", 1, 1024, 512);
    test_bcast_fp32("add", 1, 512, 1024);
    test_bcast_fp32("add", 2, 1, 64);
    test_bcast_fp32("add", 2, 128, 128);
    test_bcast_fp32("add", 4, 1024, 512);
    test_bcast_fp32("add", 4, 512, 1024);
    return ark::unittest::SUCCESS;
}

ark::unittest::State test_add_fp16()
{
    test_bcast_fp16("add", 1, 1, 2);
    test_bcast_fp16("add", 1, 1, 64);
    test_bcast_fp16("add", 1, 128, 128);
    test_bcast_fp16("add", 1, 1024, 512);
    test_bcast_fp16("add", 1, 512, 1024);
    test_bcast_fp16("add", 2, 1, 64);
    test_bcast_fp16("add", 2, 128, 128);
    test_bcast_fp16("add", 4, 1024, 512);
    test_bcast_fp16("add", 4, 512, 1024);
    return ark::unittest::SUCCESS;
}

ark::unittest::State test_add_overwrite()
{
    test_bcast_fp32("add", 2, 1024, 512, true);
    test_bcast_fp16("add", 2, 1024, 512, true);
    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_add_fp32);
    UNITTEST(test_add_fp16);
    UNITTEST(test_add_overwrite);
    return ark::unittest::SUCCESS;
}

```

`ark/ops/ops_all_gather.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "math.h"
#include "model.h"
#include "ops_common.h"
#include <cassert>

namespace ark {

std::vector<Tensor *> Model::all_gather(Tensor *input, int gpu_id, int gpu_num,
                                        std::vector<Tensor *> output,
                                        const std::string &)
{
    assert(input != nullptr);
    LOG(DEBUG, "all_gather ", input->shape, " ", gpu_id, " ", gpu_num);
    if (input->ndims() > 1) {
        LOG(INFO,
            "warning: if the send tensor if not contiguous, the all_gather "
            "may not work correctly");
    }
    LOG(DEBUG, "all gather output size: ", output.size());

    int base = this->impl->next_eid;
    std::vector<Tensor *> recv_dep_tensors;
    for (int gpu_dst = 0; gpu_dst < gpu_num; gpu_dst++) {
        if (gpu_dst == gpu_id)
            continue;
        Tensor *send_tensor =
            this->send(input, base + gpu_id * gpu_num + gpu_dst, gpu_dst);
        Tensor *send_done_tensor =
            this->send_done(input, base + gpu_id * gpu_num + gpu_dst, gpu_dst);
        recv_dep_tensors.push_back(send_tensor);
        recv_dep_tensors.push_back(send_done_tensor);
    }
    recv_dep_tensors.push_back(input);

    Tensor *recv_buf;
    for (int gpu_src = 0; gpu_src < gpu_num; gpu_src++) {
        recv_buf = nullptr;
        // if gpu_src == gpu_id, the tensor is local
        if (gpu_src == gpu_id) {
            output.push_back(input);
            continue;
        }
        if (output.size() > (size_t)gpu_src) {
            recv_buf = output[gpu_src];
        }
        if (recv_buf == nullptr) {
            recv_buf = this->tensor(input->shape, input->type);
            output.push_back(recv_buf);
        }
        Tensor *recv = this->recv(this->identity(recv_buf, recv_dep_tensors),
                                  base + gpu_src * gpu_num + gpu_id, gpu_src);
        // The output should depend on the recv tensor
        recv_buf = this->identity(recv_buf, {recv});
        output[gpu_src] = recv_buf;
    }

    this->impl->next_eid += gpu_num * gpu_num;
    return output;
}

} // namespace ark

```

`ark/ops/ops_all_reduce.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "math.h"
#include "model.h"
#include "ops_common.h"
#include <cassert>

namespace ark {

Tensor *Model::all_reduce(Tensor *input, int gpu_id, int gpu_num,
                          Tensor *output, const std::string &)
{
    assert(input != nullptr);
    if (output != nullptr) {
        LOGERR("all_reduce output is not supported");
    }
    LOG(DEBUG, "all_reduce ", input->shape, " ", gpu_id, " ", gpu_num);
    if (input->ndims() > 1) {
        LOGERR("supports only 1D input");
    }
    if (math::pad(input->shape[0], input->pads[0]) < (size_t)input->ldims[0]) {
        LOGERR("all_reduce of a split tensor is not supported");
    }

    int base = this->impl->next_eid;
    Tensor *prev_recv = nullptr;
    Tensor *cumulate = input;
    for (int i = 1; i < gpu_num; i++) {
        int gpu_dst = (gpu_id + i) % gpu_num;
        int gpu_src = (gpu_id + gpu_num - i) % gpu_num;
        Tensor *send_data;
        if (prev_recv != nullptr) {
            send_data = this->identity(input, {prev_recv});
        } else {
            send_data = input;
        }
        Tensor *send_tensor =
            this->send(send_data, base + gpu_id * gpu_num + gpu_dst, gpu_dst);
        Tensor *send_done_tensor =
            this->send_done(this->identity(input, {send_tensor}),
                            base + gpu_id * gpu_num + gpu_dst, gpu_dst);
        Tensor *recv_buf = this->tensor(input->shape, input->type);
        Tensor *recv = this->recv(this->identity(recv_buf, {send_done_tensor}),
                                  base + gpu_src * gpu_num + gpu_id, gpu_dst);
        prev_recv = recv;
        cumulate = this->add(cumulate, this->identity(recv_buf, {recv}));
    }
    this->impl->next_eid += gpu_num * gpu_num;
    return cumulate;
}

} // namespace ark

```

`ark/ops/ops_all_reduce_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_kernel.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "logging.h"
#include "unittest/unittest_utils.h"

using namespace std;
using namespace ark;
// used for print the all_reduce result and check the correctness
//  #define PRINT_MATRIX
void test_all_reduce_internal(size_t bytes, int num_gpus, int iter)
{
    // bytes/num_gpus is the number of bytes a GPU send in one iteration, the
    // bytes must be multiple of num_gpus, the tensor shape is {1, bytes /
    // sizeof(ark::half_t), 1, 1}.
    if (bytes % num_gpus != 0) {
        LOG(INFO, "bytes must be multiple of num_gpus");
        return;
    }
    // init input data and ground truth.
    ark::srand();
    vector<unique_ptr<ark::half_t[]>> input_data(num_gpus);
    for (int i = 0; i < num_gpus; i++) {
        input_data[i] =
            ark::utils::rand_halfs(bytes / sizeof(ark::half_t), 0.01);
    }

    // calculate ground truth of all_reduce.
    ark::half_t *gt = (ark::half_t *)malloc(bytes);
    UNITTEST_NE(gt, (void *)nullptr);
    // first convert the input data to float, then sum them up, finally convert
    // the result to ark::half_t.
    for (size_t i = 0; i < bytes / sizeof(ark::half_t); i++) {
        float sum = 0;
        for (int j = 0; j < num_gpus; j++) {
            sum += (float)input_data[j].get()[i];
        }
        gt[i] = ark::half_t(sum);
    }
#ifdef PRINT_MATRIX
    // print input data.
    for (int gpu_id = 0; gpu_id < num_gpus; gpu_id++) {
        cout << "input data of gpu_id: " << gpu_id << endl;
        for (size_t i = 0; i < bytes / sizeof(ark::half_t) && i < 10; i++) {
            cout << (float)input_data[gpu_id].get()[i] << " ";
        }
        cout << endl;
    }
    // print ground truth.
    cout << "ground truth: " << endl;
    for (size_t i = 0; i < bytes / sizeof(ark::half_t) && i < 10; i++) {
        cout << (float)gt[i] << " ";
    }
    cout << endl;
#endif
    for (int gpu_id = 0; gpu_id < num_gpus; ++gpu_id) {
        ark::unittest::spawn_process([gpu_id, num_gpus, &input_data, &gt, bytes,
                                      iter]() {
            // define model.
            Model model{gpu_id};
            Tensor *data = model.tensor(
                {
                    (ark::DimType)(bytes / sizeof(ark::half_t)),
                },
                FP16);
            Tensor *allreduce_result = model.all_reduce(data, gpu_id, num_gpus);
            Executor exe{gpu_id, gpu_id, num_gpus, model, "test_all_reduce"};
            exe.compile();

            // Get the auto-scheduled buffers.
            ark::GpuBuf *buf_tns = exe.get_gpu_buf(allreduce_result);
            UNITTEST_NE(buf_tns, (ark::GpuBuf *)nullptr);

            // Set data.
            ark::gpu_memcpy(buf_tns, input_data[gpu_id].get(), bytes);

            // launch kernel
            exe.launch();
            exe.run(iter);
            float elapsed_msec = exe.stop();

            // Copy results of the loop kernel routine into CPU memory.
            ark::half_t *res = (ark::half_t *)malloc(bytes);
            UNITTEST_NE(res, (void *)nullptr);
            ark::gpu_memcpy(res, buf_tns, bytes);

            // Compare results with the ground truth.
            auto p = ark::utils::cmp_matrix((ark::half_t *)gt,
                                            (ark::half_t *)res, 1, bytes / 2);
#ifdef PRINT_MATRIX
            // print result, to avoid too long output, only print the first 10
            // elements if(gpu_id == 0)
            {
                cout << "result on gpu_id: " << gpu_id << " ";
                for (size_t i = 0; i < bytes / sizeof(ark::half_t) && i < 10;
                     i++) {
                    cout << (float)res[i] << " ";
                }
                cout << endl;
            }
#endif
            free(res);
            LOG(ark::INFO, " all_reduce on gpu: ", gpu_id,
                " num_gpus: ", num_gpus, " total_bytes: ", bytes,
                " iter: ", iter, setprecision(4), " mse: ", p.first,
                " max_err: ", p.second * 100, "%",
                " elapsed_msec: ", elapsed_msec, "ms");
            return ark::unittest::SUCCESS;
        });
    }

    ark::unittest::wait_all_processes();
    free(gt);
}

ark::unittest::State test_all_reduce()
{
    test_all_reduce_internal(8, 2, 1);
    test_all_reduce_internal(16, 4, 1);
    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_all_reduce);
    return ark::unittest::SUCCESS;
}

```

`ark/ops/ops_common.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "ops_common.h"
#include "include/ark.h"
#include "json.h"
#include "logging.h"
#include <algorithm>
#include <ostream>

using namespace std;

namespace ark {

Dims broadcast(const Dims &dims1, const Dims &dims2)
{
    std::vector<DimType> output_dims_reversed;
    int ndims = std::max(dims1.ndims(), dims2.ndims());
    for (int i = 1; i < ndims + 1; ++i) {
        int d1 = (i - 1 < dims1.ndims()) ? dims1[-i] : 1;
        int d2 = (i - 1 < dims2.ndims()) ? dims2[-i] : 1;
        if (d1 == d2) {
            output_dims_reversed.push_back(d1);
        } else if (d1 == 1) {
            output_dims_reversed.push_back(d2);
        } else if (d2 == 1) {
            output_dims_reversed.push_back(d1);
        } else {
            LOGERR("input and other cannot be broadcasted: ", dims1, ", ",
                   dims2);
        }
    }
    std::reverse(output_dims_reversed.begin(), output_dims_reversed.end());
    return Dims{output_dims_reversed};
}

bool operator<(const OpConfigKey &ops1, const OpConfigKey &ops2)
{
    if (ops1.arch_type != ops2.arch_type) {
        return ops1.arch_type < ops2.arch_type;
    } else {
        return ops1.prec_type < ops2.prec_type;
    }
}

bool operator==(const OpConfigKey &ops1, const OpConfigKey &ops2)
{
    return ops1.arch_type == ops2.arch_type && ops1.prec_type == ops2.prec_type;
}

ostream &operator<<(ostream &os, const OpType &s)
{
    // clang-format off
    switch (s) {
    case OP_UNKNOWN:       os << "OP_UNKNOWN";       break;
    case OP_TENSOR:        os << "OP_TENSOR";        break;
    case OP_REFER:         os << "OP_REFER";         break;
    case OP_RESHAPE:       os << "OP_RESHAPE";       break;
    case OP_MERGE:         os << "OP_MERGE";         break;
    case OP_REDUCE_E_SUM:  os << "OP_REDUCE_E_SUM";  break;
    case OP_REDUCE_E_MEAN: os << "OP_REDUCE_E_MEAN"; break;
    case OP_REDUCE_E_MAX:  os << "OP_REDUCE_E_MAX";  break;
    case OP_REDUCE_W_SUM:  os << "OP_REDUCE_W_SUM";  break;
    case OP_REDUCE_W_MEAN: os << "OP_REDUCE_W_MEAN"; break;
    case OP_REDUCE_W_MAX:  os << "OP_REDUCE_W_MAX";  break;
    case OP_SCALE:         os << "OP_SCALE";         break;
    case OP_MATMUL:        os << "OP_MATMUL";        break;
    case OP_MAX_POOL:      os << "OP_MAX_POOL";      break;
    case OP_ADD:           os << "OP_ADD";           break;
    case OP_MUL:           os << "OP_MUL";           break;
    case OP_IM2COL:        os << "OP_IM2COL";        break;
    case OP_TRANSPOSE:     os << "OP_TRANSPOSE";     break;
    case OP_SEND:          os << "OP_SEND";          break;
    case OP_SEND_DONE:     os << "OP_SEND_DONE";     break;
    case OP_SEND_MM:       os << "OP_SEND_MM";       break;
    case OP_RECV:          os << "OP_RECV";          break;
    case OP_RECV_MM:       os << "OP_RECV_MM";       break;
    case OP_LAYERNORM:     os << "OP_LAYERNORM";     break;
    case OP_SOFTMAX:       os << "OP_SOFTMAX";       break;
    case OP_RELU:          os << "OP_RELU";          break;
    case OP_GELU:          os << "OP_GELU";          break;
    }
    // clang-format on
    return os;
}

OpArg::OpArg(int arg) : type{OP_ARG_INT}, val{new int{arg}}
{
    assert(this->val != nullptr);
}
OpArg::OpArg(DimType arg) : type{OP_ARG_INT64}, val{new DimType{arg}}
{
    assert(this->val != nullptr);
}
OpArg::OpArg(uint64_t arg) : type{OP_ARG_UINT64}, val{new uint64_t{arg}}
{
    assert(this->val != nullptr);
}
OpArg::OpArg(bool arg) : type{OP_ARG_BOOL}, val{new bool{arg}}
{
    assert(this->val != nullptr);
}
OpArg::OpArg(float arg) : type{OP_ARG_FLOAT}, val{new float{arg}}
{
    assert(this->val != nullptr);
}
OpArg::OpArg(const Dims &arg) : type{OP_ARG_DIMS}, val{new Dims{arg}}
{
    assert(this->val != nullptr);
}
OpArg::OpArg(Tensor *arg) : type{OP_ARG_TENSOR}, val{arg}
{
    assert(this->val != nullptr);
}
OpArg::OpArg(const OpArg &arg) : type{arg.type}
{
    if (this->type == OP_ARG_INT) {
        this->val = new int{*(int *)arg.val};
    } else if (this->type == OP_ARG_INT64) {
        this->val = new DimType{*(DimType *)arg.val};
    } else if (this->type == OP_ARG_UINT64) {
        this->val = new uint64_t{*(uint64_t *)arg.val};
    } else if (this->type == OP_ARG_BOOL) {
        this->val = new bool{*(bool *)arg.val};
    } else if (this->type == OP_ARG_FLOAT) {
        this->val = new float{*(float *)arg.val};
    } else if (this->type == OP_ARG_DIMS) {
        this->val = new Dims{*(Dims *)arg.val};
    } else if (this->type == OP_ARG_TENSOR) {
        this->val = arg.val;
    } else {
        LOGERR("invalid argument type ", this->type);
    }
}
OpArg::~OpArg()
{
    if (this->type == OP_ARG_INT) {
        delete static_cast<int *>(this->val);
    } else if (this->type == OP_ARG_INT64) {
        delete static_cast<DimType *>(this->val);
    } else if (this->type == OP_ARG_UINT64) {
        delete static_cast<uint64_t *>(this->val);
    } else if (this->type == OP_ARG_BOOL) {
        delete static_cast<bool *>(this->val);
    } else if (this->type == OP_ARG_FLOAT) {
        delete static_cast<float *>(this->val);
    } else if (this->type == OP_ARG_DIMS) {
        delete static_cast<Dims *>(this->val);
    } else if (this->type == OP_ARG_TENSOR) {
        // Do nothing
    }
}
void OpArg::get(int *arg) const
{
    if (this->type != OP_ARG_INT) {
        LOGERR("invalid argument type ", this->type);
    }
    *arg = *static_cast<int *>(this->val);
}

void OpArg::get(long long int *arg) const
{
    if (this->type != OP_ARG_INT64) {
        LOGERR("invalid argument type ", this->type);
    }
    *arg = *static_cast<long long int *>(this->val);
}

void OpArg::get(uint64_t *arg) const
{
    if (this->type != OP_ARG_UINT64) {
        LOGERR("invalid argument type ", this->type);
    }
    *arg = *static_cast<uint64_t *>(this->val);
}

void OpArg::get(bool *arg) const
{
    if (this->type != OP_ARG_BOOL) {
        LOGERR("invalid argument type ", this->type);
    }
    *arg = *static_cast<bool *>(this->val);
}

void OpArg::get(float *arg) const
{
    if (this->type != OP_ARG_FLOAT) {
        LOGERR("invalid argument type ", this->type);
    }
    *arg = *static_cast<float *>(this->val);
}

void OpArg::get(Dims *arg) const
{
    if (this->type != OP_ARG_DIMS) {
        LOGERR("invalid argument type ", this->type);
    }
    *arg = *static_cast<Dims *>(this->val);
}

void OpArg::get(Tensor **arg) const
{
    if (this->type != OP_ARG_TENSOR) {
        LOGERR("invalid argument type ", this->type);
    }
    *arg = static_cast<Tensor *>(this->val);
}

bool operator<(const OpArg &oa1, const OpArg &oa2)
{
    if (oa1.type != oa2.type) {
        return oa1.type < oa2.type;
    }
    assert(oa1.val != nullptr);
    assert(oa2.val != nullptr);
    switch (oa1.type) {
    case OP_ARG_INT:
        return *(int *)oa1.val < *(int *)oa2.val;
    case OP_ARG_INT64:
        return *(DimType *)oa1.val < *(DimType *)oa2.val;
    case OP_ARG_UINT64:
        return *(uint64_t *)oa1.val < *(uint64_t *)oa2.val;
    case OP_ARG_BOOL:
        return *(bool *)oa1.val < *(bool *)oa2.val;
    case OP_ARG_FLOAT:
        return *(float *)oa1.val < *(float *)oa2.val;
    case OP_ARG_DIMS:
        return *(Dims *)oa1.val < *(Dims *)oa2.val;
    case OP_ARG_TENSOR:
        return (uintptr_t)oa1.val < (uintptr_t)oa2.val;
    }
    assert(false);
    return false;
}
bool operator==(const OpArg &oa1, const OpArg &oa2)
{
    if (oa1.type != oa2.type) {
        return false;
    }
    assert(oa1.val != nullptr);
    assert(oa2.val != nullptr);
    switch (oa1.type) {
    case OP_ARG_INT:
        return *(int *)oa1.val == *(int *)oa2.val;
    case OP_ARG_INT64:
        return *(DimType *)oa1.val == *(DimType *)oa2.val;
    case OP_ARG_UINT64:
        return *(uint64_t *)oa1.val == *(uint64_t *)oa2.val;
    case OP_ARG_BOOL:
        return *(bool *)oa1.val == *(bool *)oa2.val;
    case OP_ARG_FLOAT:
        return *(float *)oa1.val == *(float *)oa2.val;
    case OP_ARG_DIMS:
        return *(Dims *)oa1.val == *(Dims *)oa2.val;
    case OP_ARG_TENSOR:
        return oa1.val == oa2.val;
    }
    assert(false);
    return false;
}

OpArgs::OpArgs(const std::vector<OpArg> &args) : args{args}
{
}

OpArgs &OpArgs::operator=(const OpArgs &opargs)
{
    if (this != &opargs) {
        this->args = opargs.args;
    }
    return *this;
}

void OpArgs::put(const OpArg &arg)
{
    this->args.emplace_back(arg);
}

void OpArgs::get(int *arg, size_t idx) const
{
    if (this->args.size() <= idx) {
        LOGERR("invalid argument index ", idx, " size ", this->args.size());
    }
    if (this->args[idx].type != OP_ARG_INT) {
        LOGERR("invalid argument type ", this->args[idx].type);
    }
    *arg = *static_cast<int *>(this->args[idx].val);
}

void OpArgs::get(long long int *arg, size_t idx) const
{
    if (this->args.size() <= idx) {
        LOGERR("invalid argument index ", idx, " size ", this->args.size());
    }
    if (this->args[idx].type != OP_ARG_INT64) {
        LOGERR("invalid argument type ", this->args[idx].type);
    }
    *arg = *static_cast<long long int *>(this->args[idx].val);
}

void OpArgs::get(uint64_t *arg, size_t idx) const
{
    if (this->args.size() <= idx) {
        LOGERR("invalid argument index ", idx, " size ", this->args.size());
    }
    if (this->args[idx].type != OP_ARG_UINT64) {
        LOGERR("invalid argument type ", this->args[idx].type);
    }
    *arg = *static_cast<uint64_t *>(this->args[idx].val);
}

void OpArgs::get(bool *arg, size_t idx) const
{
    if (this->args.size() <= idx) {
        LOGERR("invalid argument index ", idx, " size ", this->args.size());
    }
    if (this->args[idx].type != OP_ARG_BOOL) {
        LOGERR("invalid argument type ", this->args[idx].type);
    }
    *arg = *static_cast<bool *>(this->args[idx].val);
}

void OpArgs::get(float *arg, size_t idx) const
{
    if (this->args.size() <= idx) {
        LOGERR("invalid argument index ", idx, " size ", this->args.size());
    }
    if (this->args[idx].type != OP_ARG_FLOAT) {
        LOGERR("invalid argument type ", this->args[idx].type);
    }
    *arg = *static_cast<float *>(this->args[idx].val);
}

void OpArgs::get(Dims *arg, size_t idx) const
{
    if (this->args.size() <= idx) {
        LOGERR("invalid argument index ", idx, " size ", this->args.size());
    }
    if (this->args[idx].type != OP_ARG_DIMS) {
        LOGERR("invalid argument type ", this->args[idx].type);
    }
    *arg = *static_cast<Dims *>(this->args[idx].val);
}

void OpArgs::get(Tensor **arg, size_t idx) const
{
    if (this->args.size() <= idx) {
        LOGERR("invalid argument index ", idx, " size ", this->args.size());
    }
    if (this->args[idx].type != OP_ARG_TENSOR) {
        LOGERR("invalid argument type ", this->args[idx].type);
    }
    *arg = static_cast<Tensor *>(this->args[idx].val);
}

const std::vector<OpArg> &OpArgs::get_args() const
{
    return this->args;
}

bool operator<(const OpArgs &opargs1, const OpArgs &opargs2)
{
    for (size_t i = 0; i < opargs1.args.size(); ++i) {
        if (opargs1.args[i] == opargs2.args[i]) {
            continue;
        }
        return opargs1.args[i] < opargs2.args[i];
    }
    return false;
}

bool operator==(const OpArgs &opargs1, const OpArgs &opargs2)
{
    for (size_t i = 0; i < opargs1.args.size(); ++i) {
        if (opargs1.args[i] == opargs2.args[i]) {
            continue;
        }
        return false;
    }
    return true;
}

bool operator!=(const OpArgs &opargs1, const OpArgs &opargs2)
{
    return !(opargs1 == opargs2);
}

Op::Op(const OpType &type_, const OpPrecType &prec_type_,
       const vector<Tensor *> &inputs_, const vector<Tensor *> &output_refs_,
       const OpArgs &args_, const string &name_, const OpConfigMap *cfg_map_,
       int gran_lev_, bool force_inline_)
    : type{type_}, prec_type{prec_type_}, inputs{inputs_},
      output_refs{output_refs_}, args{args_}, name{name_}, cfg_map{cfg_map_},
      gran_lev{gran_lev_}, force_inline{force_inline_}
{
    for (auto &tns : inputs_) {
        if (tns == nullptr) {
            LOG(ERROR, "input tensor is null");
        }
    }
    for (auto &tns : output_refs_) {
        if (tns == nullptr) {
            LOG(ERROR, "output reference tensor is null");
        }
    }
}

std::string Op::function_name(const OpConfig &cfg) const
{
    switch (this->type) {
    case OP_REDUCE_E_SUM:
        return static_cast<const ReduceESumOp *>(this)->function_name(cfg);
    case OP_REDUCE_E_MEAN:
        return static_cast<const ReduceEMeanOp *>(this)->function_name(cfg);
    case OP_REDUCE_E_MAX:
        return static_cast<const ReduceEMaxOp *>(this)->function_name(cfg);
    case OP_REDUCE_W_SUM:
        return static_cast<const ReduceWSumOp *>(this)->function_name(cfg);
    case OP_REDUCE_W_MEAN:
        return static_cast<const ReduceWMeanOp *>(this)->function_name(cfg);
    case OP_REDUCE_W_MAX:
        return static_cast<const ReduceWMaxOp *>(this)->function_name(cfg);
    case OP_SCALE:
        return static_cast<const ScaleOp *>(this)->function_name(cfg);
    case OP_MATMUL:
        return static_cast<const MatmulOp *>(this)->function_name(cfg);
    case OP_MAX_POOL:
        return static_cast<const MaxPoolOp *>(this)->function_name(cfg);
    case OP_ADD:
        return static_cast<const AddOp *>(this)->function_name(cfg);
    case OP_MUL:
        return static_cast<const MulOp *>(this)->function_name(cfg);
    case OP_IM2COL:
        return static_cast<const Im2colOp *>(this)->function_name(cfg);
    case OP_TRANSPOSE:
        return static_cast<const TransposeOp *>(this)->function_name(cfg);
    case OP_SEND:
        return static_cast<const SendOp *>(this)->function_name(cfg);
    case OP_SEND_DONE:
        return static_cast<const SendDoneOp *>(this)->function_name(cfg);
    case OP_SEND_MM:
        return static_cast<const SendMMOp *>(this)->function_name(cfg);
    case OP_RECV:
        return static_cast<const RecvOp *>(this)->function_name(cfg);
    case OP_RECV_MM:
        return static_cast<const RecvMMOp *>(this)->function_name(cfg);
    case OP_LAYERNORM:
        return static_cast<const LayernormOp *>(this)->function_name(cfg);
    case OP_SOFTMAX:
        return static_cast<const SoftmaxOp *>(this)->function_name(cfg);
    case OP_RELU:
        return static_cast<const ReluOp *>(this)->function_name(cfg);
    case OP_GELU:
        return static_cast<const GeluOp *>(this)->function_name(cfg);
    default:
        return "";
    }
    // Never reach here.
    return "";
}

OpArgs Op::function_call_args(const OpConfig &cfg) const
{
    switch (this->type) {
    case OP_SCALE:
        return static_cast<const ScaleOp *>(this)->function_call_args(cfg);
    case OP_SEND:
        return static_cast<const SendOp *>(this)->function_call_args(cfg);
    case OP_SEND_DONE:
        return static_cast<const SendDoneOp *>(this)->function_call_args(cfg);
    case OP_RECV:
        return static_cast<const RecvOp *>(this)->function_call_args(cfg);
    case OP_SEND_MM:
        return static_cast<const SendMMOp *>(this)->function_call_args(cfg);
    case OP_RECV_MM:
        return static_cast<const RecvMMOp *>(this)->function_call_args(cfg);
    default:
        OpArgs opargs;
        std::vector<Tensor *> deps = this->outputs;
        deps.insert(deps.end(), this->inputs.begin(), this->inputs.end());
        for (Tensor *tns : deps) {
            opargs.put(tns);
        }
        return opargs;
    }
    // Never reach here.
    return {};
}

std::string Op::function_name(const std::string &kernel_name,
                              const OpArgs &template_args)
{
    std::stringstream ss;
    ss << kernel_name;
    size_t num_args = template_args.args.size();
    if (num_args == 0) {
        return ss.str();
    }
    ss << "<";
    for (size_t i = 0; i < num_args; ++i) {
        auto &arg = template_args.args[i];
        if (arg.type == OP_ARG_INT) {
            int val;
            template_args.get(&val, i);
            ss << val;
        } else if (arg.type == OP_ARG_INT64) {
            long long int val;
            template_args.get(&val, i);
            ss << val;
        } else if (arg.type == OP_ARG_UINT64) {
            uint64_t val;
            template_args.get(&val, i);
            ss << val;
        } else if (arg.type == OP_ARG_BOOL) {
            bool val;
            template_args.get(&val, i);
            ss << val;
        } else if (arg.type == OP_ARG_FLOAT) {
            LOGERR("float template args are not supported");
        } else if (arg.type == OP_ARG_DIMS) {
            Dims val;
            template_args.get(&val, i);
            ss << "ark::Vec" << val;
        }
        if (i < num_args - 1) {
            ss << ", ";
        }
    }
    ss << ">";
    return ss.str();
}

bool Op::is_virtual() const
{
    return this->cfg_map == nullptr;
}

bool Op::is_comm() const
{
    return this->type == OP_SEND || this->type == OP_SEND_DONE ||
           this->type == OP_RECV;
}

bool operator<(const Op &op1, const Op &op2)
{
    if (op1.type < op2.type) {
        return true;
    }
    if (op1.prec_type < op2.prec_type) {
        return true;
    }
    if (op1.args < op2.args) {
        return true;
    }
    return false;
}

bool operator==(const Op &op1, const Op &op2)
{
    if (op1.type != op2.type) {
        return false;
    }
    if (op1.prec_type != op2.prec_type) {
        return false;
    }
    if (op1.args != op2.args) {
        return false;
    }
    return true;
}

} // namespace ark

```

`ark/ops/ops_common.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_OPS_COMMON_H_
#define ARK_OPS_COMMON_H_

#include "include/ark.h"
#include <map>
#include <ostream>
#include <vector>

namespace ark {

/// Return the output shape of broadcasting between two shapes.
/// Follow NumPy rules.
/// https://numpy.org/doc/stable/user/basics.broadcasting.html
/// @param dims1 The first shape.
/// @param dims2 The second shape.
Dims broadcast(const Dims &dims1, const Dims &dims2);

/// Type of operator argument.
typedef enum
{
    OP_ARG_INT,
    OP_ARG_INT64,
    OP_ARG_UINT64,
    OP_ARG_BOOL,
    OP_ARG_FLOAT,
    OP_ARG_DIMS,
    OP_ARG_TENSOR,
} OpArgType;

/// Stores an arbitrary type of argument given to an operator.
struct OpArg
{
    OpArg(int arg);
    OpArg(long long int arg);
    OpArg(uint64_t arg);
    OpArg(bool arg);
    OpArg(float arg);
    OpArg(const Dims &arg);
    OpArg(Tensor *arg);
    OpArg(const OpArg &);
    ~OpArg();

    void get(int *arg) const;
    void get(long long int *arg) const;
    void get(uint64_t *arg) const;
    void get(bool *arg) const;
    void get(float *arg) const;
    void get(Dims *arg) const;
    void get(Tensor **arg) const;

    OpArgType type;
    void *val;

    friend bool operator<(const OpArg &oa1, const OpArg &oa2);
    friend bool operator==(const OpArg &oa1, const OpArg &oa2);
};

class Op;

/// Stores a list of @ref OpArg.
class OpArgs
{
  public:
    OpArgs(const std::vector<OpArg> &args = {});
    OpArgs(const OpArgs &) = default;
    ~OpArgs(){};

    OpArgs &operator=(const OpArgs &opargs);

    void put(const OpArg &arg);

    void get(int *arg, size_t idx) const;
    void get(long long int *arg, size_t idx) const;
    void get(uint64_t *arg, size_t idx) const;
    void get(bool *arg, size_t idx) const;
    void get(float *arg, size_t idx) const;
    void get(Dims *arg, size_t idx) const;
    void get(Tensor **arg, size_t idx) const;

    const std::vector<OpArg> &get_args() const;

  protected:
    std::vector<OpArg> args;

    friend class Op;
    friend bool operator<(const OpArgs &opargs1, const OpArgs &opargs2);
    friend bool operator==(const OpArgs &opargs1, const OpArgs &opargs2);
    friend bool operator!=(const OpArgs &opargs1, const OpArgs &opargs2);
};

/// Type of @ref Op.
typedef enum
{
    OP_UNKNOWN = 0,
    OP_TENSOR,
    OP_REFER,
    OP_RESHAPE,
    OP_MERGE,
    OP_REDUCE_E_SUM,
    OP_REDUCE_E_MEAN,
    OP_REDUCE_E_MAX,
    OP_REDUCE_W_SUM,
    OP_REDUCE_W_MEAN,
    OP_REDUCE_W_MAX,
    OP_LAYERNORM,
    OP_SOFTMAX,
    OP_SCALE,
    OP_RELU,
    OP_GELU,
    OP_MATMUL,
    OP_MAX_POOL,
    OP_ADD,
    OP_MUL,
    OP_IM2COL,
    OP_TRANSPOSE,
    OP_SEND,
    OP_SEND_DONE,
    OP_RECV,
    OP_SEND_MM,
    OP_RECV_MM,
} OpType;

/// Type of precision of @ref Op.
typedef enum
{
    OP_PREC_NONE,
    OP_PREC_FP16,
    OP_PREC_FP32,
} OpPrecType;

/// Type of hardware architecture support.
typedef enum
{
    OP_ARCH_CUDA_70,
    OP_ARCH_CUDA_80,
} OpArchType;

struct Tensor;

/// 2-dimensional op tile
struct OpTile
{
    DimType x;
    DimType y;
};

/// Configurations for execution of a @ref Op.
struct OpConfig
{
    int num_warps = 0;
    int smem_bytes = 0;
    std::vector<OpTile> input_tiles;
    std::vector<OpTile> output_tiles;
    bool sync_pre = false;
    bool sync_post = false;
};

/// Key to find a list of OpConfigs from OpConfigMap.
struct OpConfigKey
{
    OpArchType arch_type;
    OpPrecType prec_type;
};

bool operator<(const OpConfigKey &ops1, const OpConfigKey &ops2);

bool operator==(const OpConfigKey &ops1, const OpConfigKey &ops2);

/// Map from OpConfigKey to a list of OpConfigs.
using OpConfigMap = std::map<OpConfigKey, std::vector<OpConfig>>;

/// Operator.
class Op
{
  public:
    /// Construct an operator.
    Op() = default;

    /// Construct an operator.
    /// @param type the type of the @ref Op.
    /// @param prec_type the precision type of the @ref Op.
    /// @param inputs the input tensors of the @ref Op, including execution
    /// dependencies.
    /// @param output_refs the output reference tensors of the @ref Op. Output
    /// tensors are created based on these references.
    /// @param args the arguments of the @ref Op.
    /// @param name the name of the @ref Op.
    /// @param cfg_map the configuration map of the @ref Op
    /// @param gran_lev the granularity level of the @ref Op. Larger values
    /// should indicate finer-grained Ops. If it is -1, the granularity level
    /// will be automatically determined by the scheduler.
    /// @param force_inline whether to force inline the kernel of @ref Op.
    Op(const OpType &type, const OpPrecType &prec_type,
       const std::vector<Tensor *> &inputs,
       const std::vector<Tensor *> &output_refs, const OpArgs &args,
       const std::string &name, const OpConfigMap *cfg_map = nullptr,
       int gran_lev = -1, bool force_inline = false);

    /// Construct an operator.
    Op(const Op &) = default;

    /// Destruct the operator.
    ~Op(){};

    /// Return the kernel function name of the operator. Includes the template
    /// arguments of the kernel, if any.
    /// @param cfg the configuration of the operator.
    /// @return the kernel function name of the operator.
    std::string function_name(const OpConfig &) const;

    /// Return the kernel function's runtime arguments of the operator.
    /// @param cfg the configuration of the operator.
    /// @return the runtime arguments of the kernel function.
    OpArgs function_call_args(const OpConfig &) const;

    /// Returns true if the operator is virtual (i.e., performs no computation).
    bool is_virtual() const;

    /// Returns true if the operator is a communication operator.
    bool is_comm() const;

    /// Type of the operator.
    OpType type;
    /// Precision type of the operator.
    OpPrecType prec_type;
    /// The input tensors of the operator.
    std::vector<Tensor *> inputs;
    /// The output tensors of the operator.
    std::vector<Tensor *> outputs;
    /// The reference tensors of the output tensors.
    std::vector<Tensor *> output_refs;
    /// Additional arguments of the operator.
    OpArgs args;
    /// Name of the operator.
    std::string name;
    /// Map from OpConfigKey to a list of OpConfigs.
    const OpConfigMap *cfg_map;
    /// Granularity level of the operator.
    int gran_lev;
    /// Force inlining of the operator kernel.
    bool force_inline;

    friend bool operator<(const Op &op1, const Op &op2);
    friend bool operator==(const Op &op1, const Op &op2);

  protected:
    static std::string function_name(const std::string &kernel_name,
                                     const OpArgs &template_args);
};

std::ostream &operator<<(std::ostream &os, const OpType &s);

/// List all operator classes below.

class AddOp : public Op
{
  public:
    AddOp(OpPrecType prec_type, Tensor *input, Tensor *other, Tensor *output,
          const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
};

class GeluOp : public Op
{
  public:
    GeluOp(OpPrecType prec_type, Tensor *input, Tensor *output,
           const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
};

class Im2colOp : public Op
{
  public:
    Im2colOp(OpPrecType prec_type, Tensor *input, Tensor *output,
             int kernel_height, int kernel_width, int stride_height,
             int stride_width, int pad_height, int pad_width,
             int dilation_height, int dilation_width, const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
};

class LayernormOp : public Op
{
  public:
    LayernormOp(OpPrecType prec_type, Tensor *input, Tensor *output,
                const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
};

class MatmulOp : public Op
{
  public:
    MatmulOp(OpPrecType prec_type, Tensor *mat_a, Tensor *mat_b, Tensor *mat_y,
             Dims nca, Dims ncb, Dims problem_size, Dims leading_dims,
             bool is_column_a, bool is_column_b, bool is_relu,
             const std::string &name, int gran_lev);
    std::string function_name(const OpConfig &cfg) const;
};

class MaxPoolOp : public Op
{
  public:
    MaxPoolOp(OpPrecType prec_type, Tensor *input, Tensor *output,
              DimType kernel_size, DimType stride, const std::string &name);
};

class MulOp : public Op
{
  public:
    MulOp(OpPrecType prec_type, Tensor *input, Tensor *other, Tensor *output,
          const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
};

class ReduceOp : public Op
{
  public:
    ReduceOp(const OpType &type, const OpPrecType &prec_type,
             const std::vector<Tensor *> &inputs,
             const std::vector<Tensor *> &outputs, const OpArgs &args,
             const std::string &name, const OpConfigMap *cfg_map, int gran_lev);

  protected:
    std::string function_name(const OpConfig &cfg,
                              const std::string &type) const;
};

class ReduceWSumOp : public ReduceOp
{
  public:
    ReduceWSumOp(OpPrecType prec_type, Tensor *input, Tensor *output, int axis,
                 const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
};

class ReduceESumOp : public ReduceOp
{
  public:
    ReduceESumOp(OpPrecType prec_type, Tensor *input, Tensor *output, int axis,
                 const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
};

class ReduceWMaxOp : public ReduceOp
{
  public:
    ReduceWMaxOp(OpPrecType prec_type, Tensor *input, Tensor *output, int axis,
                 const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
};

class ReduceEMaxOp : public ReduceOp
{
  public:
    ReduceEMaxOp(OpPrecType prec_type, Tensor *input, Tensor *output, int axis,
                 const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
};

class ReduceWMeanOp : public ReduceOp
{
  public:
    ReduceWMeanOp(OpPrecType prec_type, Tensor *input, Tensor *output, int axis,
                  const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
};

class ReduceEMeanOp : public ReduceOp
{
  public:
    ReduceEMeanOp(OpPrecType prec_type, Tensor *input, Tensor *output, int axis,
                  const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
};

class ReluOp : public Op
{
  public:
    ReluOp(OpPrecType prec_type, Tensor *input, Tensor *output,
           const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
};

class ReshapeOp : public Op
{
  public:
    ReshapeOp(OpPrecType prec_type, Tensor *input, Tensor *output,
              const std::string &name);
};

class ScaleOp : public Op
{
  public:
    ScaleOp(OpPrecType prec_type, Tensor *input, Tensor *output, float val,
            const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
    OpArgs function_call_args(const OpConfig &) const;
};

class SendMMOp : public Op
{
  public:
    SendMMOp(OpPrecType prec_type, Tensor *input, Tensor *recvbuf,
             Tensor *send_ready_flag, Tensor *output, int id, int gpu_dst,
             size_t bytes, const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
    OpArgs function_call_args(const OpConfig &cfg) const;
};

class RecvMMOp : public Op
{
  public:
    RecvMMOp(OpPrecType prec_type, Tensor *input, Tensor *recvbuf,
             Tensor *send_ready_flag, Tensor *output, int id, int gpu_src,
             size_t bytes, const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
    OpArgs function_call_args(const OpConfig &cfg) const;
};

class SendOp : public Op
{
  public:
    SendOp(OpPrecType prec_type, Tensor *input, Tensor *output, int sid,
           int rank, int dst_rank, size_t bytes, const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
    OpArgs function_call_args(const OpConfig &cfg) const;
};

class SendDoneOp : public Op
{
  public:
    SendDoneOp(OpPrecType prec_type, Tensor *input, Tensor *output, int sid,
               int rank, int dst_rank, const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
    OpArgs function_call_args(const OpConfig &cfg) const;
};

class RecvOp : public Op
{
  public:
    RecvOp(OpPrecType prec_type, Tensor *input, Tensor *output, int sid,
           int rank, int src_rank, size_t bytes, const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
    OpArgs function_call_args(const OpConfig &cfg) const;
};

class SoftmaxOp : public Op
{
  public:
    SoftmaxOp(OpPrecType prec_type, Tensor *input, Tensor *output,
              const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
};

class TensorOp : public Op
{
  public:
    TensorOp(const std::vector<Tensor *> &deps, Tensor *output,
             const std::string &name);
};

class TransposeOp : public Op
{
  public:
    TransposeOp(OpPrecType prec_type, Tensor *input, Tensor *output,
                int tp_type, const std::string &name);
    std::string function_name(const OpConfig &cfg) const;
};

} // namespace ark

#endif // ARK_OPS_COMMON_H_

```

`ark/ops/ops_dot_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

// #include <fstream>

#include "gpu/gpu_kernel.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "logging.h"
#include "unittest/unittest_utils.h"

using namespace std;

//
void test_dot_internal(unsigned int len)
{
    ark::GpuMgr *mgr = ark::get_gpu_mgr(0);
    ark::GpuMgrCtx *ctx = mgr->create_context("test_simple_dot", 0, 1);

    ark::GpuBuf *buf_a = ctx->mem_alloc(len * sizeof(ark::half_t));
    ark::GpuBuf *buf_b = ctx->mem_alloc(len * sizeof(ark::half_t));
    ark::GpuBuf *buf_c = ctx->mem_alloc(4);

    ctx->freeze();

    ark::GpuKernel gk{"simple_dot",
                      {ark::unittest::get_kernel_code("simple_dot")},
                      {1, 1, 1},
                      {128, 1, 1},
                      0,
                      {buf_c, buf_a, buf_b},
                      {},
                      {
                          {&len, sizeof(len)},
                      },
                      ""};
    gk.compile(mgr->get_gpu_info());
    gk.load();

    ark::srand();
    float res = 0;
    ark::half_t gt = 0;

    for (int iter = 0; iter < 10; ++iter) {
        // Set data.
        auto data_a = ark::utils::rand_halfs(len, 1);
        auto data_b = ark::utils::rand_halfs(len, 1);

        ark::gpu_memcpy(buf_a, data_a.get(), len * sizeof(ark::half_t));
        ark::gpu_memcpy(buf_b, data_b.get(), len * sizeof(ark::half_t));

        // Run the GPU kernel.
        ark::GpuStream s = ctx->create_stream();
        int ret = gk.launch(s);
        UNITTEST_EQ(ret, 0);
        ret = ctx->sync_stream(s);
        UNITTEST_EQ(ret, 0);

        // Copy the result into CPU memory.
        ark::gpu_memcpy(&res, buf_c, 4);

        // Calculate the ground truth.
        gt = 0;
        for (unsigned int i = 0; i < len; ++i) {
            gt = gt + data_a.get()[i] * data_b.get()[i];
        }

        float err = ark::utils::error_rate((float)gt, res);

        LOG(ark::INFO, "dot:", len, setprecision(4), " res ", res, " gt ", gt,
            " err ", err * 100, "%");

        UNITTEST_TRUE(err < 0.01);
    }
    mgr->destroy_context(ctx);
}

ark::unittest::State test_dot()
{
    test_dot_internal(1024);
    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_dot);
    return ark::unittest::SUCCESS;
}

```

`ark/ops/ops_gelu.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include "tensor.h"
#include <cassert>

namespace ark {

extern const OpConfigMap ActivationConfigMap;

GeluOp::GeluOp(OpPrecType prec_type, Tensor *input, Tensor *output,
               const std::string &name)
    : Op{OP_GELU, prec_type, {input}, {output}, {}, name, &ActivationConfigMap,
         -1,      true}
{
}

std::string GeluOp::function_name(const OpConfig &cfg) const
{
    Tensor *input = this->inputs[0];
    Tensor *output = this->outputs[0];

    int ndims = output->shape.ndims();
    const OpTile &tile_out = cfg.output_tiles[0];
    CHECK(output->ldims[ndims - 1] % tile_out.y == 0);
    if (ndims > 1) {
        CHECK(output->ldims[ndims - 2] % tile_out.x == 0);
    } else {
        CHECK(tile_out.x == 1);
    }

    Dims unit_out_dims{1, 1, tile_out.x, tile_out.y};
    return Op::function_name("ark::gelu", {{
                                              input->ldims.dims4(),  // InDims
                                              input->shape.dims4(),  // InShape
                                              output->ldims.dims4(), // OutDims
                                              output->shape.dims4(), // OutShape
                                              unit_out_dims,      // UnitOutDims
                                              cfg.num_warps * 32, // NumThreads
                                              cfg.smem_bytes,     // SmemBytes
                                          }});
}

Tensor *Model::gelu(Tensor *input, Tensor *output, const std::string &name)
{
    assert(input != nullptr);
    OpPrecType pt;
    if (input->type == FP16) {
        pt = OP_PREC_FP16;
    } else if (input->type == FP32) {
        pt = OP_PREC_FP32;
    } else {
        LOGERR("unsupported input data type: ", type_str(input->type));
    }
    if (output != nullptr && input->type != output->type) {
        LOGERR("invalid output data type: ", type_str(output->type));
    }
    if (output == nullptr) {
        output = this->tensor(input->shape, input->type, input->buf);
    } else if (output->shape != input->shape) {
        LOGERR("invalid output shape: ", output->shape);
    }
    GeluOp op{pt, input, output, name};
    return this->impl->add_op(op)[0];
}

} // namespace ark

```

`ark/ops/ops_gelu_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_kernel.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "logging.h"
#include "unittest/unittest_utils.h"
#include <cmath>

using namespace std;

float gelu(float x)
{
    return 0.5 * x * (1 + tanh(sqrt(2 / M_PI) * (x + 0.044715 * pow(x, 3))));
}

//
void test_gelu_internal(unsigned int bs, unsigned int n, unsigned int m)
{
    unsigned int len = bs * m * n;
    // Set data.
    ark::srand();
    auto data_x = ark::utils::rand_halfs(len, 0.01);

    // Get ground truth
    void *gt = malloc(len * sizeof(ark::half_t));
    UNITTEST_NE(gt, (void *)nullptr);
    for (unsigned int i = 0; i < len; ++i) {
        ((ark::half_t *)gt)[i] = gelu(((ark::half_t *)data_x.get())[i]);
    }
    //
    ark::Model model;
    ark::Tensor *tns_x = model.tensor({bs, n, m}, ark::FP16);
    ark::Tensor *tns_y = model.gelu(tns_x);

    //
    ark::Executor exe{0, 0, 1, model, "test_gelu"};
    exe.compile();

    // Set data.
    exe.tensor_memcpy(tns_x, data_x.get(), len * sizeof(ark::half_t));

    exe.launch();
    exe.run(1);
    exe.stop();

    // Copy results of the loop kernel routine into CPU memory.
    void *res = malloc(len * sizeof(ark::half_t));
    UNITTEST_NE(res, (void *)nullptr);
    exe.tensor_memcpy(res, tns_y, len * sizeof(ark::half_t));

    // Compare results with the ground truth.
    auto p =
        ark::utils::cmp_matrix((ark::half_t *)gt, (ark::half_t *)res, m, n, bs);
    float max_err = p.second;
    LOG(ark::INFO, "gelu:", n, 'x', m, ",bs=", bs, setprecision(4), " mse ",
        p.first, " max_err ", max_err * 100, "%");

    free(res);
    free(gt);

    UNITTEST_EQ(max_err, 0.0);
}

ark::unittest::State test_gelu()
{
    test_gelu_internal(1, 1, 64);
    test_gelu_internal(1, 64, 64);
    test_gelu_internal(1, 128, 128);
    test_gelu_internal(1, 4096, 1024);
    test_gelu_internal(1, 1024, 4096);
    test_gelu_internal(2, 1, 64);
    test_gelu_internal(2, 128, 128);
    test_gelu_internal(8, 4096, 1024);
    test_gelu_internal(8, 1024, 4096);
    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_gelu);
    return ark::unittest::SUCCESS;
}

```

`ark/ops/ops_identity.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include <cassert>

namespace ark {

// Returns an identical tensor of `input` with execution dependencies `deps`.
Tensor *Model::identity(Tensor *input, const std::vector<Tensor *> &deps,
                        const std::string &name)
{
    assert(input != nullptr);
    LOG(DEBUG, "identity ", input->shape);
    std::set<Tensor *> dep_set;
    dep_set.emplace(input);
    for (auto &dep : deps) {
        dep_set.emplace(dep);
    }
    std::vector<Tensor *> dep_vec;
    for (auto &dep : dep_set) {
        dep_vec.emplace_back(dep);
    }
    return this->tensor(input->shape, input->type, input->buf, input->ldims,
                        input->offs, input->pads, dep_vec, input->exported,
                        input->imported_rank, name + "/identity");
}

} // namespace ark

```

`ark/ops/ops_identity_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "include/ark.h"
#include "include/ark_utils.h"
#include "unittest/unittest_utils.h"
using namespace std;

ark::unittest::State test_identity()
{
    ark::Model model;
    // float buf[2][3][4][5];
    ark::Tensor *tns0 = model.tensor({2, 3, 4, 5}, ark::FP32);
    ark::Tensor *tns1 = model.identity(tns0);

    // Create an executor
    ark::Executor exe{0, 0, 1, model, "test_tensor_layout"};
    exe.compile();

    int num_elem = 2 * 3 * 4 * 5;

    ark::GpuBuf *buf0 = exe.get_gpu_buf(tns0);
    ark::GpuBuf *buf1 = exe.get_gpu_buf(tns1);
    UNITTEST_NE(buf0, (ark::GpuBuf *)nullptr);
    UNITTEST_NE(buf1, (ark::GpuBuf *)nullptr);
    UNITTEST_EQ(buf0->get_bytes(), num_elem * sizeof(float));
    UNITTEST_EQ(buf1->get_bytes(), num_elem * sizeof(float));

    // Fill tensor data: {1.0, 2.0, 3.0, ..., 120.0}
    auto data = ark::utils::range_floats(num_elem);
    exe.tensor_memcpy(tns0, data.get(), num_elem * sizeof(float));

    // Check identity values
    float *ref_val = new float[num_elem];
    exe.tensor_memcpy(ref_val, tns1, num_elem * sizeof(float));
    for (int i = 0; i < num_elem; ++i) {
        UNITTEST_EQ(ref_val[i], (float)(i + 1));
    }

    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_identity);
    return ark::unittest::SUCCESS;
}

```

`ark/ops/ops_im2col.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include "tensor.h"
#include <cassert>

namespace ark {

extern const OpConfigMap Im2colConfigMap;

Im2colOp::Im2colOp(OpPrecType prec_type, Tensor *input, Tensor *output,
                   int kernel_height, int kernel_width, int stride_height,
                   int stride_width, int pad_height, int pad_width,
                   int dilation_height, int dilation_width,
                   const std::string &name)
    : Op{OP_IM2COL,
         prec_type,
         {input},
         {output},
         {{kernel_height, kernel_width, stride_height, stride_width, pad_height,
           pad_width, dilation_height, dilation_width}},
         name,
         &Im2colConfigMap,
         -1,
         true}
{
}

std::string Im2colOp::function_name(const OpConfig &cfg) const
{
    Tensor *input = this->inputs[0];
    Tensor *output = this->outputs[0];

    int ndims = output->shape.ndims();
    const OpTile &tile_out = cfg.output_tiles[0];
    CHECK(output->ldims[ndims - 1] % tile_out.y == 0);
    if (ndims > 1) {
        CHECK(output->ldims[ndims - 2] % tile_out.x == 0);
    } else {
        CHECK(tile_out.x == 1);
    }

    int kernel_height;
    int kernel_width;
    int stride_height;
    int stride_width;
    int pad_height;
    int pad_width;
    int dilation_height;
    int dilation_width;
    this->args.get(&kernel_height, 0);
    this->args.get(&kernel_width, 1);
    this->args.get(&stride_height, 2);
    this->args.get(&stride_width, 3);
    this->args.get(&pad_height, 4);
    this->args.get(&pad_width, 5);
    this->args.get(&dilation_height, 6);
    this->args.get(&dilation_width, 7);

    Dims unit_out_dims{1, 1, tile_out.x, tile_out.y};
    return Op::function_name("ark::im2col",
                             {{
                                 input->ldims.dims4(),  // InDims
                                 input->shape.dims4(),  // InShape
                                 output->ldims.dims4(), // OutDims
                                 output->shape.dims4(), // OutShape
                                 unit_out_dims,         // UnitOutDims
                                 cfg.num_warps * 32,    // NumThreads
                                 cfg.smem_bytes,        // SmemBytes
                                 kernel_height,         // KernelHeight
                                 kernel_width,          // KernelWidth
                                 stride_height,         // StrideHeight
                                 stride_width,          // StrideWidth
                                 pad_height,            // PadHeight
                                 pad_width,             // PadWidth
                                 dilation_height,       // DilationHeight
                                 dilation_width,        // DilationWidth
                             }});
}

Tensor *Model::im2col(Tensor *input, int kernel_height, int kernel_width,
                      int stride_height, int stride_width, int pad_height,
                      int pad_width, int dilation_height, int dilation_width,
                      Tensor *output, const std::string &name)
{
    assert(input != nullptr);
    DimType n, c, h, w;
    int input_ndims = input->ndims();
    if (input_ndims == 2) {
        n = 1;
        c = 1;
        h = input->shape[0];
        w = input->shape[1];
    } else if (input_ndims == 3) {
        n = 1;
        c = input->shape[0];
        h = input->shape[1];
        w = input->shape[2];
    } else if (input_ndims == 4) {
        n = input->shape[0];
        c = input->shape[1];
        h = input->shape[2];
        w = input->shape[3];
    } else {
        LOGERR("invalid # of input dimensions. Expected 2, 3, or 4, but given ",
               input_ndims);
    }
    OpPrecType pt;
    if (input->type == FP16) {
        pt = OP_PREC_FP16;
    } else if (input->type == FP32) {
        pt = OP_PREC_FP32;
    } else {
        LOGERR("unsupported input data type: ", type_str(input->type));
    }
    DimType out_h = (h + 2 * pad_height - kernel_height) / stride_height + 1;
    DimType out_w = (w + 2 * pad_width - kernel_width) / stride_width + 1;
    assert((out_h > 0) && (out_w > 0));
    DimType out_m = out_h * out_w;
    DimType inner_dim = c * kernel_height * kernel_width;
    Dims out_shape;
    if (input_ndims <= 3) {
        out_shape = {inner_dim, out_m};
    } else {
        out_shape = {n, inner_dim, out_m};
    }
    if (output == nullptr) {
        output = this->tensor(out_shape, input->type);
    } else {
        assert(output->shape == out_shape);
    }
    Im2colOp op{pt,           input,           output,         kernel_height,
                kernel_width, stride_height,   stride_width,   pad_height,
                pad_width,    dilation_height, dilation_width, name};
    return this->impl->add_op(op)[0];
}

const OpConfigMap Im2colConfigMap = {
    {{OP_ARCH_CUDA_70, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {8, 0, {{1, 1}}, {{128, 128}}, true, false},
         {4, 0, {{1, 1}}, {{64, 128}}, true, false},
         {4, 0, {{1, 1}}, {{128, 64}}, true, false},
         {4, 0, {{1, 1}}, {{64, 64}}, true, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {8, 0, {{1, 1}}, {{128, 128}}, true, false},
         {4, 0, {{1, 1}}, {{64, 128}}, true, false},
         {4, 0, {{1, 1}}, {{128, 64}}, true, false},
         {4, 0, {{1, 1}}, {{64, 64}}, true, false},
     }},
};

} // namespace ark

```

`ark/ops/ops_im2col_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_kernel.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "logging.h"
#include "unittest/unittest_utils.h"

using namespace std;

//
void test_im2col_internal(ark::DimType n, ark::DimType h, ark::DimType w,
                          ark::DimType c, ark::DimType kernel_height,
                          ark::DimType kernel_width, ark::DimType stride_height,
                          ark::DimType stride_width, ark::DimType pad_height,
                          ark::DimType pad_width, ark::DimType dilation_height,
                          ark::DimType dilation_width)
{
    //
    ark::Model model;
    ark::Tensor *tns_x = model.tensor({n, c, h, w}, ark::FP16);
    ark::Tensor *tns_y = model.im2col(
        tns_x, kernel_height, kernel_width, stride_height, stride_width,
        pad_height, pad_width, dilation_height, dilation_width);
    UNITTEST_EQ(tns_y->ndims(), 3);

    //
    ark::Executor exe{0, 0, 1, model, "test_im2col"};
    exe.compile();

    // Set data.
    ark::srand();
    auto data_x =
        ark::utils::range_halfs(tns_x->shape_bytes(), 0.00001, 0.00001);
    exe.tensor_memcpy(tns_x, data_x.get(), tns_x->shape_bytes());

    // ark::utils::print_matrix(data_x.get(), h * w, c, h * w, c);

    exe.launch();
    exe.run(1);
    exe.stop();

    // Copy results of the loop kernel routine into CPU memory.
    ark::half_t *res =
        (ark::half_t *)calloc(tns_y->shape.size(), sizeof(ark::half_t));
    UNITTEST_NE(res, (ark::half_t *)nullptr);
    exe.tensor_memcpy(res, tns_y, tns_y->shape_bytes());

    // Calculate CPU results
    ark::half_t *gt = (ark::half_t *)calloc(
        tns_y->shape_bytes() / sizeof(ark::half_t), sizeof(ark::half_t));
    UNITTEST_NE(gt, (ark::half_t *)nullptr);
    ark::DimType patch_num_height =
        (h - kernel_height + 2 * pad_height) / stride_height + 1;
    ark::DimType patch_num_width =
        (w - kernel_width + 2 * pad_width) / stride_width + 1;
    ark::DimType mdim = patch_num_height * patch_num_width;
    ark::DimType inner_dim = kernel_height * kernel_width * c;
    for (ark::DimType nidx = 0; nidx < inner_dim; ++nidx) {
        for (ark::DimType midx = 0; midx < patch_num_height * patch_num_width;
             ++midx) {
            ark::DimType channel_idx = nidx / (kernel_height * kernel_width);
            ark::DimType per_channel_patch_idx = midx;
            ark::DimType per_channel_patch_pos_width =
                (per_channel_patch_idx % patch_num_width) * stride_width;
            ark::DimType per_channel_patch_pos_height =
                (per_channel_patch_idx / patch_num_width) * stride_height;
            ark::DimType per_patch_elem_idx =
                nidx % (kernel_height * kernel_width);
            ark::DimType per_patch_elem_pos_width =
                per_patch_elem_idx % kernel_width;
            ark::DimType per_patch_elem_pos_height =
                per_patch_elem_idx / kernel_width;
            ark::DimType elem_width = per_channel_patch_pos_width +
                                      per_patch_elem_pos_width - pad_width;
            ark::DimType elem_height = per_channel_patch_pos_height +
                                       per_patch_elem_pos_height - pad_height;

            if (elem_height < 0 || elem_height >= h || elem_width < 0 ||
                elem_width >= w) {
                gt[midx + nidx * mdim] = ark::half_t(0);
            } else {
                ark::DimType elem_idx =
                    elem_width + elem_height * w + channel_idx * h * w;
                gt[midx + nidx * mdim] = data_x.get()[elem_idx];
            }
        }
    }

    // Compare results with the ground truth.
    auto p = ark::utils::cmp_matrix((ark::half_t *)gt, (ark::half_t *)res, mdim,
                                    inner_dim, n, mdim, inner_dim);
    float max_err = p.second;
    stringstream ss;
    ss << "im2col:n=" << n << ",c=" << c << ",h=" << h << ",w=" << w
       << ",kh=" << kernel_height << ",kw=" << kernel_width
       << ",sh=" << stride_height << ",sw=" << stride_width
       << ",ph=" << pad_height << ",pw=" << pad_width
       << ",dh=" << dilation_height << ",dw=" << dilation_width
       << setprecision(4) << " mse " << p.first << " max_err " << max_err * 100
       << "%";
    LOG(ark::INFO, ss.str());

    free(res);
    free(gt);

    UNITTEST_EQ(max_err, 0.0);
}

ark::unittest::State test_im2col()
{
    test_im2col_internal(1, 2, 2, 2, 2, 2, 1, 1, 0, 0, 1, 1);
    test_im2col_internal(1, 4, 4, 3, 2, 2, 1, 1, 0, 0, 1, 1);
    test_im2col_internal(1, 4, 4, 15, 2, 2, 1, 1, 0, 0, 1, 1);
    test_im2col_internal(1, 4, 4, 16, 2, 2, 1, 1, 0, 0, 1, 1);
    test_im2col_internal(1, 4, 4, 17, 2, 2, 1, 1, 0, 0, 1, 1);
    test_im2col_internal(1, 4, 4, 64, 2, 2, 1, 1, 0, 0, 1, 1);

    test_im2col_internal(1, 7, 7, 3, 2, 2, 1, 1, 0, 0, 1, 1);
    test_im2col_internal(1, 8, 8, 3, 2, 2, 1, 1, 0, 0, 1, 1);
    test_im2col_internal(1, 9, 9, 3, 2, 2, 1, 1, 0, 0, 1, 1);
    test_im2col_internal(1, 64, 64, 3, 2, 2, 1, 1, 0, 0, 1, 1);

    test_im2col_internal(1, 4, 4, 3, 3, 3, 1, 1, 0, 0, 1, 1);
    test_im2col_internal(1, 8, 8, 3, 3, 3, 1, 1, 0, 0, 1, 1);
    test_im2col_internal(1, 64, 64, 3, 7, 7, 1, 1, 0, 0, 1, 1);

    test_im2col_internal(1, 4, 4, 3, 2, 2, 1, 1, 1, 1, 1, 1);

    test_im2col_internal(1, 8, 8, 3, 3, 3, 1, 1, 0, 0, 1, 1);

    test_im2col_internal(1, 256, 256, 3, 3, 3, 1, 1, 0, 0, 1, 1);
    test_im2col_internal(1, 97, 97, 13, 5, 5, 1, 1, 0, 0, 1, 1);

    test_im2col_internal(1, 256, 256, 3, 3, 3, 1, 1, 1, 1, 1, 1);
    test_im2col_internal(1, 97, 97, 13, 5, 5, 1, 1, 2, 2, 1, 1);

    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_im2col);
    return ark::unittest::SUCCESS;
}

```

`ark/ops/ops_layernorm.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include "tensor.h"
#include <cassert>

namespace ark {

extern const OpConfigMap LayernormConfigMap;

LayernormOp::LayernormOp(OpPrecType prec_type, Tensor *input, Tensor *output,
                         const std::string &name)
    : Op{OP_LAYERNORM, prec_type,           {input}, {output}, {},
         name,         &LayernormConfigMap, -1}
{
}

std::string LayernormOp::function_name(const OpConfig &cfg) const
{
    Tensor *input = this->inputs[0];
    Tensor *output = this->outputs[0];

    int ndims = output->shape.ndims();
    const OpTile &tile_out = cfg.output_tiles[0];
    CHECK(output->ldims[ndims - 1] % tile_out.y == 0);
    if (ndims > 1) {
        CHECK(output->ldims[ndims - 2] % tile_out.x == 0);
    } else {
        CHECK(tile_out.x == 1);
    }

    Dims unit_out_dims{1, 1, tile_out.x, tile_out.y};
    return Op::function_name("ark::layernorm",
                             {{
                                 input->ldims.dims4(),  // InDims
                                 input->shape.dims4(),  // InShape
                                 output->ldims.dims4(), // OutDims
                                 output->shape.dims4(), // OutShape
                                 unit_out_dims,         // UnitOutDims
                                 cfg.num_warps * 32,    // NumThreads
                                 cfg.smem_bytes,        // SmemBytes
                             }});
}

Tensor *Model::layernorm(Tensor *input, Tensor *output, const std::string &name)
{
    assert(input != nullptr);
    LOG(DEBUG, "layernorm ", input->shape, " ", input->ldims, " ");
    OpPrecType pt;
    if (input->type == FP16) {
        pt = OP_PREC_FP16;
    } else if (input->type == FP32) {
        pt = OP_PREC_FP32;
    } else {
        LOGERR("unsupported input data type: ", type_str(input->type));
    }
    if (output != nullptr && input->type != output->type) {
        LOGERR("invalid output data type: ", type_str(output->type));
    }
    if (output == nullptr) {
        output = this->tensor(input->shape, input->type);
    } else if (output == input) {
        output = this->identity(output);
    }
    LayernormOp op{pt, input, output, name};
    return this->impl->add_op(op)[0];
}

const OpConfigMap LayernormConfigMap = {
    {{OP_ARCH_CUDA_70, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {1, 128, {{32, -1}}, {{32, -1}}, true, false},
         {1, 128, {{16, -1}}, {{16, -1}}, true, false},
         {1, 128, {{8, -1}}, {{8, -1}}, true, false},
         {1, 128, {{4, -1}}, {{4, -1}}, true, false},
         {1, 128, {{2, -1}}, {{2, -1}}, true, false},
         {1, 128, {{1, -1}}, {{1, -1}}, true, false},
         {4, 128, {{1, -1}}, {{1, -1}}, true, false},
         {8, 128, {{1, -1}}, {{1, -1}}, true, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {1, 128, {{32, -1}}, {{32, -1}}, true, false},
         {1, 128, {{16, -1}}, {{16, -1}}, true, false},
         {1, 128, {{8, -1}}, {{8, -1}}, true, false},
         {1, 128, {{4, -1}}, {{4, -1}}, true, false},
         {1, 128, {{2, -1}}, {{2, -1}}, true, false},
         {1, 128, {{1, -1}}, {{1, -1}}, true, false},
         {4, 128, {{1, -1}}, {{1, -1}}, true, false},
         {8, 128, {{1, -1}}, {{1, -1}}, true, false},
     }},
    {{OP_ARCH_CUDA_70, OP_PREC_FP32},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {1, 128, {{32, -1}}, {{32, -1}}, true, false},
         {1, 128, {{16, -1}}, {{16, -1}}, true, false},
         {1, 128, {{8, -1}}, {{8, -1}}, true, false},
         {1, 128, {{4, -1}}, {{4, -1}}, true, false},
         {1, 128, {{2, -1}}, {{2, -1}}, true, false},
         {1, 128, {{1, -1}}, {{1, -1}}, true, false},
         {4, 128, {{1, -1}}, {{1, -1}}, true, false},
         {8, 128, {{1, -1}}, {{1, -1}}, true, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_FP32},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {1, 128, {{32, -1}}, {{32, -1}}, true, false},
         {1, 128, {{16, -1}}, {{16, -1}}, true, false},
         {1, 128, {{8, -1}}, {{8, -1}}, true, false},
         {1, 128, {{4, -1}}, {{4, -1}}, true, false},
         {1, 128, {{2, -1}}, {{2, -1}}, true, false},
         {1, 128, {{1, -1}}, {{1, -1}}, true, false},
         {4, 128, {{1, -1}}, {{1, -1}}, true, false},
         {8, 128, {{1, -1}}, {{1, -1}}, true, false},
     }},
};

} // namespace ark

```

`ark/ops/ops_layernorm_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_kernel.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "logging.h"
#include "unittest/unittest_utils.h"

using namespace std;

//
void test_layernorm_internal(unsigned int n, unsigned int m, unsigned int k)
{
    size_t buf_x_sz = (size_t)m * (size_t)n * (size_t)k * sizeof(ark::half_t);
    size_t buf_y_sz = (size_t)m * (size_t)n * sizeof(ark::half_t);

    // Set data.
    ark::srand();
    auto data_a = ark::utils::rand_halfs(buf_x_sz / sizeof(ark::half_t), 0.01);

    // Copy the ground truth results into CPU memory.
    void *gt = malloc(buf_y_sz);
    UNITTEST_NE(gt, (void *)nullptr);

    //
    ark::Model model;
    ark::Tensor *tns_x = model.tensor({m, n, k}, ark::FP32);
    /* ark::Tensor *tns_y = */ model.layernorm(tns_x);

    //
    ark::Executor exe{0, 0, 1, model, "test_layernorm"};
    exe.compile();

    // Set data.
    exe.tensor_memcpy(tns_x, data_a.get(), buf_x_sz);

    exe.launch();
    exe.run(1);
    exe.stop();
}

ark::unittest::State test_layernorm()
{
    test_layernorm_internal(1, 64, 4);
    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_layernorm);
    return ark::unittest::SUCCESS;
}

```

`ark/ops/ops_matmul.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "math.h"
#include "model.h"
#include "tensor.h"

using namespace std;

namespace ark {

extern const OpConfigMap MatmulConfigMap;

MatmulOp::MatmulOp(OpPrecType prec_type, Tensor *mat_a, Tensor *mat_b,
                   Tensor *mat_y, Dims nca, Dims ncb, Dims problem_size,
                   Dims leading_dims, bool is_column_a, bool is_column_b,
                   bool is_relu, const string &name, int gran_lev)
    : Op{OP_MATMUL,
         prec_type,
         {mat_a, mat_b},
         {mat_y},
         {{nca, ncb, problem_size, leading_dims, is_column_a, is_column_b,
           is_relu}},
         name,
         &MatmulConfigMap,
         gran_lev}
{
}

std::string MatmulOp::function_name(const OpConfig &cfg) const
{
    Tensor *mat_a = this->inputs[0];
    Tensor *mat_b = this->inputs[1];
    Tensor *mat_y = this->outputs[0];

    int ndims_y = mat_y->shape.ndims();
    const OpTile &tile_out = cfg.output_tiles[0];
    CHECK(mat_y->ldims[ndims_y - 1] % tile_out.y == 0);
    if (ndims_y > 1) {
        CHECK(mat_y->ldims[ndims_y - 2] % tile_out.x == 0);
    } else {
        CHECK(tile_out.x == 1);
    }

    Dims nca;
    Dims ncb;
    Dims problem_size;
    Dims leading_dims;
    bool is_column_a;
    bool is_column_b;
    bool is_relu;
    this->args.get(&nca, 0);
    this->args.get(&ncb, 1);
    this->args.get(&problem_size, 2);
    this->args.get(&leading_dims, 3);
    this->args.get(&is_column_a, 4);
    this->args.get(&is_column_b, 5);
    this->args.get(&is_relu, 6);

    /// Re-calculate the exact leading dimensions. Assume this function is
    /// called after scheduling is done.

    const Dims &ldims_a = mat_a->ldims;
    const Dims &ldims_b = mat_b->ldims;
    const Dims &ldims_y = mat_y->ldims;
    int ndims_a = ldims_a.ndims();
    int ndims_b = ldims_b.ndims();
    leading_dims[0] = is_column_a ? ldims_a[ndims_a - 2] : ldims_a[ndims_a - 1];
    leading_dims[1] = ldims_y[ldims_y.ndims() - 1];
    leading_dims[2] = ldims_y[ldims_y.ndims() - 1];
    leading_dims[3] = is_column_b ? ldims_b[ndims_b - 2] : ldims_b[ndims_b - 1];

    // TODO: verify `leading_dims`

    const OpTile &tile_in0 = cfg.input_tiles[0];
    const OpTile &tile_in1 = cfg.input_tiles[1];
    CHECK(tile_in0.y == tile_in1.x);
    Dims shape{tile_out.x, tile_out.y, tile_in0.y};

    return Op::function_name("ark::matmul",
                             {{
                                 mat_y->ldims.dims4(), // OutDims
                                 nca,                  // NCA
                                 ncb,                  // NCB
                                 shape,                // Shape
                                 problem_size,         // ProblemSize
                                 leading_dims,         // LeadingDims
                                 is_column_a,          // IsColumnA
                                 is_column_b,          // IsColumnB
                                 is_relu,              // IsRelu
                                 cfg.num_warps * 32,   // NumThreads
                                 cfg.smem_bytes,       // SmemBytes
                             }});
}

Tensor *Model::matmul(Tensor *mat_a, Tensor *mat_b, Tensor *mat_y,
                      DimType split_k, bool trans_a, bool trans_b, bool is_relu,
                      const string &name, int gran_lev)
{
    CHECK(mat_a != nullptr);
    CHECK(mat_b != nullptr);
    CHECK(split_k >= 1);
    LOG(DEBUG, "matmul ", mat_a->shape, " ", mat_b->shape, " ", mat_a->ldims,
        " ", mat_b->ldims, " ", split_k);

    // Shape verification.
    const Dims &shp_a = mat_a->shape;
    const Dims &shp_b = mat_b->shape;
    int ndims_a = shp_a.ndims();
    int ndims_b = shp_b.ndims();

    if (ndims_a < 1) {
        LOGERR("mat_a has an empty shape: ", shp_a);
    }
    if (ndims_b < 1) {
        LOGERR("mat_b has an empty shape: ", shp_b);
    }

    // m: the number of rows of output matrix (row-major)
    // n: the number of columns of output matrix (row-major)
    // k: the inner dimension of matrix multiplication
    DimType m;
    DimType n;
    DimType k;
    DimType k2;

    m = (ndims_a == 1) ? 1 : shp_a[ndims_a - 2];
    k = shp_a[ndims_a - 1];
    if (trans_a) {
        DimType tmp = m;
        m = k;
        k = tmp;
    }
    n = (ndims_b == 1) ? 1 : shp_b[ndims_b - 1];
    k2 = (ndims_b == 1) ? shp_b[0] : shp_b[ndims_b - 2];
    if (trans_b) {
        DimType tmp = n;
        n = k2;
        k2 = tmp;
    }
    if (k != k2) {
        LOGERR("inner dimensions mismatch: ", k, " and ", k2);
    }

    OpPrecType pt;
    if (mat_a->type == FP16) {
        pt = OP_PREC_FP16;
    } else if (mat_a->type == FP32) {
        pt = OP_PREC_FP32;
    } else {
        LOGERR("unsupported input data type: ", type_str(mat_a->type));
    }
    if (mat_a->type != mat_b->type) {
        LOGERR("input data types mismatch: ", type_str(mat_a->type), ", ",
               type_str(mat_b->type));
    }
    if (mat_y != nullptr && mat_a->type != mat_b->type) {
        LOGERR("invalid output data type: ", type_str(mat_y->type));
    }

    // N and C dimensions of matrix A
    Dims nca{1, 1};
    if (ndims_a == 4) {
        nca[0] = shp_a[0];
        nca[1] = shp_a[1];
    } else if (ndims_a == 3) {
        nca[1] = shp_a[0];
    }

    // N and C dimensions of matrix B
    Dims ncb{1, 1};
    if (ndims_b == 4) {
        ncb[0] = shp_b[0];
        ncb[1] = shp_b[1];
    } else if (ndims_b == 3) {
        ncb[1] = shp_b[0];
    }

    // Verify broadcasting
    if (nca[0] != ncb[0] && nca[0] != 1 && ncb[0] != 1) {
        LOGERR("N dimension mismatch: ", nca[0], " and ", ncb[0]);
    }
    if (nca[1] != ncb[1] && nca[1] != 1 && ncb[1] != 1) {
        LOGERR("C dimension mismatch: ", nca[1], " and ", ncb[1]);
    }

    // N and C dimension of output matrix
    Dims ncc{max(nca[0], ncb[0]), max(nca[1], ncb[1])};

    Dims output_shape;
    if (max(ndims_a, ndims_b) == 4) {
        output_shape = Dims{ncc[0], ncc[1], m, n};
    } else if (max(ndims_a, ndims_b) == 3) {
        output_shape = Dims{ncc[1], m, n};
    } else {
        output_shape = Dims{m, n};
    }

    // Create an output Tensor.
    if (mat_y == nullptr) {
        mat_y = this->tensor(output_shape, mat_a->type);
    } else {
        if (mat_y->type != mat_a->type) {
            LOGERR("output data type mismatch: ", type_str(mat_y->type),
                   " and ", type_str(mat_a->type));
        }
        if (mat_y->shape != output_shape) {
            LOGERR("output shape mismatch: ", mat_y->shape, " and ",
                   output_shape);
        }
    }

    // TODO: change matmul interface to receive `spu` value instead of
    // `split_k`.
    DimType spu = math::pad(math::div_up(k, split_k), 32);
    split_k = math::div_up(k, spu);
    if (split_k == 1) {
        const Dims &ldims_a = mat_a->ldims;
        const Dims &ldims_b = mat_b->ldims;
        const Dims &ldims_y = mat_y->ldims;
        // NOTE: `leading_dims` here is just an expected value. We can
        // calculate the exact value after the OpConfig is given in
        // `MatmulOp::function_name()`.
        Dims leading_dims{
            trans_a ? ldims_a[ndims_a - 2] : ldims_a[ndims_a - 1],
            ldims_y[ldims_y.ndims() - 1], ldims_y[ldims_y.ndims() - 1],
            trans_b ? ldims_b[ndims_b - 2] : ldims_b[ndims_b - 1]};
        Dims problem_size{m, n, k};
        MatmulOp op{pt,      mat_a,        mat_b,        mat_y,   nca,
                    ncb,     problem_size, leading_dims, trans_a, trans_b,
                    is_relu, name,         gran_lev};
        return this->impl->add_op(op)[0];
    } else if (split_k > k) {
        LOGERR("Split-K given larger than the K dimension size.");
    }

    // Split the inner dimension.
    Tensor *output_buffer;
    vector<Tensor *> mat_y_shards;
    if (mat_y->shape.ndims() == 4) {
        output_buffer =
            this->tensor({ncc[0] * split_k, ncc[1], m, n}, mat_y->type);
        mat_y_shards =
            this->sharding(output_buffer, 0, ncc[0], name + "/sharding_mat_y");
    } else {
        output_buffer = this->tensor({ncc[1] * split_k, m, n}, mat_y->type);
        mat_y_shards =
            this->sharding(output_buffer, 0, ncc[1], name + "/sharding_mat_y");
    }
    for (size_t i = 0; i < mat_y_shards.size(); ++i) {
        Tensor *t = mat_y_shards[i];
        // If the output dimension is not matching, drop the leading 1s.
        if (t->shape.ndims() != output_shape.ndims()) {
            Dims new_shape = t->shape;
            while (new_shape.ndims() != output_shape.ndims()) {
                if (new_shape[0] != 1) {
                    LOGERR("invalid shard shape: ", t->shape);
                }
                new_shape.erase(0);
            }
            mat_y_shards[i] = this->reshape(t, new_shape);
        }
    }

    int axis_a;
    int axis_b;
    if (trans_a) {
        axis_a = (ndims_a == 1) ? (ndims_a - 1) : (ndims_a - 2);
    } else {
        axis_a = ndims_a - 1;
    }
    if (trans_b) {
        axis_b = ndims_b - 1;
    } else {
        axis_b = (ndims_b == 1) ? (ndims_b - 1) : (ndims_b - 2);
    }
    vector<Tensor *> mat_a_shards =
        this->sharding(mat_a, axis_a, spu, name + "/sharding_mat_a");
    vector<Tensor *> mat_b_shards =
        this->sharding(mat_b, axis_b, spu, name + "/sharding_mat_b");

    CHECK(mat_y_shards.size() == (size_t)split_k);
    CHECK(mat_a_shards.size() == (size_t)split_k);
    CHECK(mat_b_shards.size() == (size_t)split_k);

    std::vector<Tensor *> shard_outputs;
    for (DimType i = 0; i < split_k; ++i) {
        Tensor *shard_output = this->matmul(
            mat_a_shards[i], mat_b_shards[i], mat_y_shards[i], 1, trans_a,
            trans_b, false, name + "/matmul_shard_" + to_string(i), gran_lev);
        shard_outputs.push_back(shard_output);
    }
    // Reduce after all outputs are ready.
    Tensor *ref =
        this->identity(output_buffer, shard_outputs, name + "/identity");
    Tensor *reduced = this->reduce_sum(ref, 0, mat_y, name + "/reduce_sum");
    if (is_relu) {
        // TODO: overwrite
        reduced = this->relu(reduced, nullptr, name + "/relu");
    }
    return reduced;
}

const OpConfigMap MatmulConfigMap = {
    {{OP_ARCH_CUDA_70, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {8, 49152, {{128, 32}, {32, 128}}, {{128, 128}}, true, false},
         {4, 24576, {{64, 32}, {32, 128}}, {{64, 128}}, true, false},
         {4, 24576, {{128, 32}, {32, 64}}, {{128, 64}}, true, false},
         {4, 24576, {{64, 32}, {32, 64}}, {{64, 64}}, true, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {8, 166912, {{128, 64}, {64, 256}}, {{128, 256}}, true, false},
         // {8, 166912, {{256, 64}, {64, 128}}, {{256, 128}}, true, false},
         {8, 166912, {{128, 64}, {64, 128}}, {{128, 128}}, true, false},
         {4, 83456, {{64, 64}, {64, 64}}, {{64, 64}}, true, false},
     }},
};

} // namespace ark

```

`ark/ops/ops_matmul_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_kernel.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "logging.h"
#include "unittest/unittest_utils.h"
#include <cassert>

using namespace std;

// m,n,k: Problem size. CAUTION: `m` and `n` are assumed to be multiple of 16.
// bs_a: Batch size of left-side matrix.
// bs_b: Batch size of right-side matrix.
// is_relu: Use ReLU activation if true.
// iter: Number of iterations.
void test_matmul_internal(unsigned int m, unsigned int n, unsigned int k,
                          unsigned int bs_a, unsigned int bs_b, int split_k = 1,
                          bool is_relu = false, int gran_lev = -1,
                          unsigned int iter = 1)
{
    assert(bs_a == bs_b || bs_a == 1 || bs_b == 1);
    unsigned int bs_res = bs_a > bs_b ? bs_a : bs_b;

    ark::GpuMgr *mgr = ark::get_gpu_mgr(0);
    ark::GpuMgrCtx *ctx = mgr->create_context("test_simple_matmul_nt", 0, 1);

    size_t buf_a_sz =
        (size_t)bs_a * (size_t)m * (size_t)k * sizeof(ark::half_t);
    size_t buf_b_sz =
        (size_t)bs_b * (size_t)k * (size_t)n * sizeof(ark::half_t);
    size_t buf_res_sz =
        (size_t)bs_res * (size_t)m * (size_t)n * sizeof(ark::half_t);

    // Reserved GPU buffers for execution of a manually written kernel,
    // `simple_matmul_nt`.
    ark::GpuBuf *buf_a = ctx->mem_alloc(buf_a_sz);
    ark::GpuBuf *buf_b = ctx->mem_alloc(buf_b_sz);
    ark::GpuBuf *buf_gt = ctx->mem_alloc(buf_res_sz);
    // ark::GpuBuf *buf_res = ctx->mem_alloc(buf_res_sz);

    ctx->freeze();

    // Define `simple_matmul_nt` kernel to generate the ground truth.
    ark::GpuKernel gk{"simple_matmul_nt",
                      {ark::unittest::get_kernel_code("simple_matmul_nt")},
                      {n / 16, m / 16, 1},
                      {16, 16, 1},
                      0,
                      {buf_gt, buf_a, buf_b},
                      {},
                      {{&m, sizeof(m)},
                       {&n, sizeof(n)},
                       {&k, sizeof(k)},
                       {&is_relu, sizeof(is_relu)}},
                      ""};
    gk.compile(mgr->get_gpu_info());
    gk.load();

    // Generate random data for tests.
    ark::srand();
    auto data_a = ark::utils::rand_halfs(buf_a_sz / sizeof(ark::half_t), 0.001);
    auto data_b = ark::utils::rand_halfs(buf_b_sz / sizeof(ark::half_t), 0.001);
    ark::gpu_memcpy(buf_a, data_a.get(), buf_a_sz);
    ark::gpu_memcpy(buf_b, data_b.get(), buf_b_sz);

    // Run the GPU kernel.
    ark::GpuStream s = ctx->create_stream();
    int ret = gk.launch(s);
    UNITTEST_EQ(ret, 0);
    ret = ctx->sync_stream(s);
    UNITTEST_EQ(ret, 0);

    // Copy the ground truth results into CPU memory.
    void *gt = malloc(buf_res_sz);
    UNITTEST_NE(gt, (void *)nullptr);
    ark::gpu_memcpy(gt, buf_gt, buf_res_sz);

    // Declare an equivalent matmul using Model APIs.
    ark::Model model;
    ark::Tensor *tns_a = model.tensor({m, k}, ark::FP16);
    ark::Tensor *tns_b = model.tensor({k, n}, ark::FP16);
    ark::Tensor *tns_res = model.matmul(tns_a, tns_b, nullptr, split_k, false,
                                        false, is_relu, "matmul", gran_lev);

    mgr->destroy_context(ctx);

    //
    ark::Executor exe{0, 0, 1, model, "test_matmul_nt"};
    exe.compile();

    // Get the auto-scheduled buffers.
    ark::GpuBuf *buf_tns_a = exe.get_gpu_buf(tns_a);
    ark::GpuBuf *buf_tns_b = exe.get_gpu_buf(tns_b);
    ark::GpuBuf *buf_tns_res = exe.get_gpu_buf(tns_res);

    UNITTEST_NE(buf_tns_a, (ark::GpuBuf *)nullptr);
    UNITTEST_NE(buf_tns_b, (ark::GpuBuf *)nullptr);

    // Set data.
    ark::gpu_memcpy(buf_tns_a, data_a.get(), buf_a_sz);
    ark::gpu_memcpy(buf_tns_b, data_b.get(), buf_b_sz);

    exe.launch();
    exe.run(iter);
    float elapsed = exe.stop();

    // Copy results of the loop kernel routine into CPU memory.
    void *res = malloc(buf_res_sz);
    UNITTEST_NE(res, (void *)nullptr);
    ark::gpu_memcpy(res, buf_tns_res, buf_res_sz);

    // Calculate CPU results
    // float temp;
    // unsigned int h;
    // unsigned int w;
    // for (unsigned int i = 0; i < (size_t)bs_res * (size_t)m * (size_t)n; ++i)
    // {
    //     temp = 0;
    //     h = i % m;
    //     w = i / m;
    //     for (unsigned int j = 0; j < k; ++j) {
    //         temp += (float)(data_a.get()[j * m + h]) * (float)(data_b.get()[j
    //         * n + w]);
    //     }
    //     ((ark::half_t *)gt)[i] = ark::half_t(temp);
    // }

    // Compare results with the ground truth.
    auto p =
        ark::utils::cmp_matrix((ark::half_t *)gt, (ark::half_t *)res, m, n);
    float max_err = p.second;
    LOG(ark::INFO, "matmul:", m, 'x', n, 'x', k, "(split_k=", split_k,
        ",relu=", is_relu, ",gran_lev=", gran_lev, ") ", setprecision(4),
        " mse ", p.first, " max_err ", max_err * 100, "%", " elapsed ", elapsed,
        "ms iter ", iter);

    free(res);
    free(gt);

    UNITTEST_EQ(max_err, 0.0);
}

ark::unittest::State test_matmul_gran0()
{
    test_matmul_internal(/*m=*/64, /*n=*/64, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/0);
    test_matmul_internal(/*m=*/128, /*n=*/64, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/0);
    test_matmul_internal(/*m=*/64, /*n=*/128, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/0);
    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/0);

    test_matmul_internal(/*m=*/64, /*n=*/64, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/0);
    test_matmul_internal(/*m=*/128, /*n=*/64, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/0);
    test_matmul_internal(/*m=*/64, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/0);
    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/0);
    test_matmul_internal(/*m=*/256, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/0);

    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/256, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/1, /*is_relu=*/false,
                         /*gran_lev=*/0);

    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/1, /*is_relu=*/false,
                         /*gran_lev=*/0);
    return ark::unittest::SUCCESS;
}

ark::unittest::State test_matmul_gran1()
{
    test_matmul_internal(/*m=*/64, /*n=*/64, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/1);
    test_matmul_internal(/*m=*/128, /*n=*/64, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/1);
    test_matmul_internal(/*m=*/64, /*n=*/128, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/1);
    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/1);

    test_matmul_internal(/*m=*/64, /*n=*/64, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/1);
    test_matmul_internal(/*m=*/128, /*n=*/64, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/1);
    test_matmul_internal(/*m=*/64, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/1);
    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/1);
    test_matmul_internal(/*m=*/256, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/1);

    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/256, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/1, /*is_relu=*/false,
                         /*gran_lev=*/1);

    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/1, /*is_relu=*/false,
                         /*gran_lev=*/1);
    return ark::unittest::SUCCESS;
}

ark::unittest::State test_matmul_gran2()
{
    test_matmul_internal(/*m=*/64, /*n=*/64, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/64, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/2);
    test_matmul_internal(/*m=*/64, /*n=*/128, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/2);

    test_matmul_internal(/*m=*/64, /*n=*/64, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/64, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/2);
    test_matmul_internal(/*m=*/64, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/2);
    test_matmul_internal(/*m=*/256, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/2);

    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/256, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/1, /*is_relu=*/false,
                         /*gran_lev=*/2);

    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/1, /*is_relu=*/false,
                         /*gran_lev=*/2);
    return ark::unittest::SUCCESS;
}

ark::unittest::State test_matmul_gran3()
{
    test_matmul_internal(/*m=*/64, /*n=*/64, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/3);
    test_matmul_internal(/*m=*/128, /*n=*/64, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/3);
    test_matmul_internal(/*m=*/64, /*n=*/128, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/3);
    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/32, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/3);

    test_matmul_internal(/*m=*/64, /*n=*/64, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/3);
    test_matmul_internal(/*m=*/128, /*n=*/64, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/3);
    test_matmul_internal(/*m=*/64, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/3);
    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/3);
    test_matmul_internal(/*m=*/256, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/3);

    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/256, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/1, /*is_relu=*/false,
                         /*gran_lev=*/3);

    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/1, /*is_relu=*/false,
                         /*gran_lev=*/3);
    return ark::unittest::SUCCESS;
}

ark::unittest::State test_matmul_relu()
{
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/1, /*is_relu=*/true);
    return ark::unittest::SUCCESS;
}

ark::unittest::State test_matmul_split()
{
    test_matmul_internal(/*m=*/64, /*n=*/64, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/2, /*is_relu=*/false, /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/64, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/2, /*is_relu=*/false, /*gran_lev=*/2);
    test_matmul_internal(/*m=*/64, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/2, /*is_relu=*/false, /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/2, /*is_relu=*/false, /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/256, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/2, /*is_relu=*/false,
                         /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/2, /*is_relu=*/false,
                         /*gran_lev=*/2);

    test_matmul_internal(/*m=*/64, /*n=*/64, /*k=*/128, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/4, /*is_relu=*/false, /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/64, /*k=*/128, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/4, /*is_relu=*/false, /*gran_lev=*/2);
    test_matmul_internal(/*m=*/64, /*n=*/128, /*k=*/128, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/4, /*is_relu=*/false, /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/128, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/4, /*is_relu=*/false,
                         /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/256, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/4, /*is_relu=*/false,
                         /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/4, /*is_relu=*/false,
                         /*gran_lev=*/2);

    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/4, /*is_relu=*/false,
                         /*gran_lev=*/0);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/4, /*is_relu=*/false,
                         /*gran_lev=*/1);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/4, /*is_relu=*/false,
                         /*gran_lev=*/2);

    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/3, /*is_relu=*/false,
                         /*gran_lev=*/0);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/3, /*is_relu=*/false,
                         /*gran_lev=*/1);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/3, /*is_relu=*/false,
                         /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/3, /*is_relu=*/false,
                         /*gran_lev=*/2);

    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/5, /*is_relu=*/false,
                         /*gran_lev=*/0);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/5, /*is_relu=*/false,
                         /*gran_lev=*/1);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/5, /*is_relu=*/false,
                         /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/5, /*is_relu=*/false,
                         /*gran_lev=*/2);

    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/6, /*is_relu=*/false,
                         /*gran_lev=*/0);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/6, /*is_relu=*/false,
                         /*gran_lev=*/1);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/6, /*is_relu=*/false,
                         /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/6, /*is_relu=*/false,
                         /*gran_lev=*/2);

    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/7, /*is_relu=*/false,
                         /*gran_lev=*/0);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/7, /*is_relu=*/false,
                         /*gran_lev=*/1);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/7, /*is_relu=*/false,
                         /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/7, /*is_relu=*/false,
                         /*gran_lev=*/2);

    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/8, /*is_relu=*/false,
                         /*gran_lev=*/0);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/8, /*is_relu=*/false,
                         /*gran_lev=*/1);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/8, /*is_relu=*/false,
                         /*gran_lev=*/2);
    test_matmul_internal(/*m=*/128, /*n=*/4096, /*k=*/1024, /*bs_a=*/1,
                         /*bs_b=*/1, /*split_k=*/8, /*is_relu=*/false,
                         /*gran_lev=*/2);

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_matmul_perf()
{
    test_matmul_internal(/*m=*/64, /*n=*/64, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/-1,
                         /*iter=*/1000);
    test_matmul_internal(/*m=*/64, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/-1,
                         /*iter=*/1000);
    test_matmul_internal(/*m=*/128, /*n=*/64, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/-1,
                         /*iter=*/1000);
    test_matmul_internal(/*m=*/128, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/-1,
                         /*iter=*/1000);
    test_matmul_internal(/*m=*/256, /*n=*/128, /*k=*/64, /*bs_a=*/1, /*bs_b=*/1,
                         /*split_k=*/1, /*is_relu=*/false, /*gran_lev=*/-1,
                         /*iter=*/1000);
    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_matmul_gran0);
    UNITTEST(test_matmul_gran1);
    UNITTEST(test_matmul_gran2);
    // UNITTEST(test_matmul_gran3);
    UNITTEST(test_matmul_relu);
    UNITTEST(test_matmul_split);
    UNITTEST(test_matmul_perf);
    return ark::unittest::SUCCESS;
}

```

`ark/ops/ops_max_pool.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include "tensor.h"
#include <cassert>

namespace ark {

MaxPoolOp::MaxPoolOp(OpPrecType prec_type, Tensor *input, Tensor *output,
                     DimType kernel_size, DimType stride,
                     const std::string &name)
    : Op{OP_MAX_POOL, prec_type, {input}, {output}, {{kernel_size, stride}},
         name,        nullptr,   -1}
{
}

// TODO: implement
Tensor *Model::max_pool(Tensor *input, DimType kernel_size, DimType stride,
                        Tensor *output, const std::string &name)
{
    assert(input != nullptr);
    OpPrecType pt;
    if (input->type == FP16) {
        pt = OP_PREC_FP16;
    } else if (input->type == FP32) {
        pt = OP_PREC_FP32;
    } else {
        LOGERR("unsupported input data type: ", type_str(input->type));
    }
    if (output != nullptr && input->type != output->type) {
        LOGERR("invalid output data type: ", type_str(output->type));
    }
    const Dims &is = input->shape;
    Dims os{{is[0], (is[1] + stride - 1) / stride,
             (is[2] + stride - 1) / stride, is[3]}};
    if (output == nullptr) {
        output = this->tensor(os, input->type);
    }
    MaxPoolOp op{pt, input, output, kernel_size, stride, name};
    return this->impl->add_op(op)[0];
}

} // namespace ark

```

`ark/ops/ops_mul.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include "tensor.h"
#include <cassert>

namespace ark {

extern const OpConfigMap ArithmeticConfigMap;

MulOp::MulOp(OpPrecType prec_type, Tensor *input, Tensor *other, Tensor *output,
             const std::string &name)
    : Op{OP_MUL, prec_type, {input, other},       {output},
         {},     name,      &ArithmeticConfigMap, -1,
         true}
{
}

std::string MulOp::function_name(const OpConfig &cfg) const
{
    Tensor *input = this->inputs[0];
    Tensor *other = this->inputs[1];
    Tensor *output = this->outputs[0];

    int ndims = output->shape.ndims();
    const OpTile &tile_out = cfg.output_tiles[0];
    CHECK(output->ldims[ndims - 1] % tile_out.y == 0);
    if (ndims > 1) {
        CHECK(output->ldims[ndims - 2] % tile_out.x == 0);
    } else {
        CHECK(tile_out.x == 1);
    }

    Dims unit_out_dims{1, 1, tile_out.x, tile_out.y};
    return Op::function_name("ark::mul", {{
                                             input->ldims.dims4(),  // In0Dims
                                             input->shape.dims4(),  // In0Shape
                                             other->ldims.dims4(),  // In1Dims
                                             other->shape.dims4(),  // In1Shape
                                             output->ldims.dims4(), // OutDims
                                             output->shape.dims4(), // OutShape
                                             unit_out_dims,      // UnitOutDims
                                             cfg.num_warps * 32, // NumThreads
                                             cfg.smem_bytes,     // SmemBytes
                                         }});
}

Tensor *Model::mul(Tensor *input, Tensor *other, Tensor *output,
                   const std::string &name)
{
    LOG(DEBUG, "mul ", input->shape, " ", other->shape);
    assert(input != nullptr);
    assert(other != nullptr);
    OpPrecType pt;
    if (input->type == FP16) {
        pt = OP_PREC_FP16;
    } else if (input->type == FP32) {
        pt = OP_PREC_FP32;
    } else {
        LOGERR("unsupported input data type: ", type_str(input->type));
    }
    if (input->type != other->type) {
        LOGERR("input data types mismatch: ", type_str(input->type), ", ",
               type_str(other->type));
    }
    if (output != nullptr && input->type != output->type) {
        LOGERR("invalid output data type: ", type_str(output->type));
    }
    Dims output_shape = broadcast(input->shape, other->shape);
    if (output == nullptr) {
        output = this->tensor(output_shape, input->type);
    } else if (output->shape != output_shape) {
        LOGERR("invalid output shape: ", output->shape);
    } else if (output == input) {
        output = this->identity(output);
    }
    MulOp op{pt, input, other, output, name};
    return this->impl->add_op(op)[0];
}

} // namespace ark

```

`ark/ops/ops_mul_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "include/ark.h"
#include "ops_test_common.h"
#include "unittest/unittest_utils.h"

ark::unittest::State test_mul_fp32()
{
    test_bcast_fp32("mul", 1, 1, 2);
    test_bcast_fp32("mul", 1, 1, 64);
    test_bcast_fp32("mul", 1, 128, 128);
    test_bcast_fp32("mul", 1, 1024, 512);
    test_bcast_fp32("mul", 1, 512, 1024);
    test_bcast_fp32("mul", 2, 1, 64);
    test_bcast_fp32("mul", 2, 128, 128);
    test_bcast_fp32("mul", 4, 1024, 512);
    test_bcast_fp32("mul", 4, 512, 1024);
    return ark::unittest::SUCCESS;
}

ark::unittest::State test_mul_fp16()
{
    test_bcast_fp16("mul", 1, 1, 2);
    test_bcast_fp16("mul", 1, 1, 64);
    test_bcast_fp16("mul", 1, 128, 128);
    test_bcast_fp16("mul", 1, 1024, 512);
    test_bcast_fp16("mul", 1, 512, 1024);
    test_bcast_fp16("mul", 2, 1, 64);
    test_bcast_fp16("mul", 2, 128, 128);
    test_bcast_fp16("mul", 4, 1024, 512);
    test_bcast_fp16("mul", 4, 512, 1024);
    return ark::unittest::SUCCESS;
}

ark::unittest::State test_mul_overwrite()
{
    test_bcast_fp32("mul", 2, 1024, 512, true);
    test_bcast_fp16("mul", 2, 1024, 512, true);
    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_mul_fp32);
    UNITTEST(test_mul_fp16);
    UNITTEST(test_mul_overwrite);
    return ark::unittest::SUCCESS;
}

```

`ark/ops/ops_reduce.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include "tensor.h"
#include <cassert>

namespace ark {

ReduceOp::ReduceOp(const OpType &type, const OpPrecType &prec_type,
                   const std::vector<Tensor *> &inputs,
                   const std::vector<Tensor *> &outputs, const OpArgs &args,
                   const std::string &name, const OpConfigMap *cfg_map,
                   int gran_lev)
    : Op{type, prec_type, inputs, outputs, args, name, cfg_map, gran_lev, true}
{
}

///
/// @param cfg
/// @param type "[w|e]_[sum|max|mean]"
/// @return
std::string ReduceOp::function_name(const OpConfig &cfg,
                                    const std::string &type) const
{
    Tensor *input = this->inputs[0];
    Tensor *output = this->outputs[0];

    int ndims = output->shape.ndims();
    const OpTile &tile_out = cfg.output_tiles[0];
    CHECK(output->ldims[ndims - 1] % tile_out.y == 0);
    if (ndims > 1) {
        CHECK(output->ldims[ndims - 2] % tile_out.x == 0);
    } else {
        CHECK(tile_out.x == 1);
    }

    Dims shp_in = input->shape;
    int axis;
    this->args.get(&axis, 0);

    // Translate the axis value into 4D representation.
    axis += 4 - shp_in.ndims();

    if (type[0] == 'w') {
        // Warp-wise reduction is supported only for the last axis.
        CHECK(axis == 3);
    }

    Dims unit_out_dims{1, 1, tile_out.x, tile_out.y};
    return Op::function_name("ark::reduce_" + type,
                             {{
                                 input->ldims.dims4(),  // InDims
                                 input->shape.dims4(),  // InShape
                                 output->ldims.dims4(), // OutDims
                                 output->shape.dims4(), // OutShape
                                 unit_out_dims,         // UnitOutDims
                                 cfg.num_warps * 32,    // NumThreads
                                 cfg.smem_bytes,        // SmemBytes
                                 axis,                  // Axis
                             }});
}

extern const OpConfigMap ReduceWConfigMap;
extern const OpConfigMap ReduceEConfigMap;

ReduceWSumOp::ReduceWSumOp(OpPrecType prec_type, Tensor *input, Tensor *output,
                           int axis, const std::string &name)
    : ReduceOp{OP_REDUCE_W_SUM, prec_type, {input},           {output},
               {{axis}},        name,      &ReduceWConfigMap, -1}
{
}

std::string ReduceWSumOp::function_name(const OpConfig &cfg) const
{
    return ReduceOp::function_name(cfg, "w_sum");
}

ReduceESumOp::ReduceESumOp(OpPrecType prec_type, Tensor *input, Tensor *output,
                           int axis, const std::string &name)
    : ReduceOp{OP_REDUCE_E_SUM, prec_type, {input},           {output},
               {{axis}},        name,      &ReduceEConfigMap, -1}
{
}

std::string ReduceESumOp::function_name(const OpConfig &cfg) const
{
    return ReduceOp::function_name(cfg, "e_sum");
}

ReduceWMaxOp::ReduceWMaxOp(OpPrecType prec_type, Tensor *input, Tensor *output,
                           int axis, const std::string &name)
    : ReduceOp{OP_REDUCE_W_MAX, prec_type, {input},           {output},
               {{axis}},        name,      &ReduceWConfigMap, -1}
{
}

std::string ReduceWMaxOp::function_name(const OpConfig &cfg) const
{
    return ReduceOp::function_name(cfg, "w_max");
}

ReduceEMaxOp::ReduceEMaxOp(OpPrecType prec_type, Tensor *input, Tensor *output,
                           int axis, const std::string &name)
    : ReduceOp{OP_REDUCE_E_MAX, prec_type, {input},           {output},
               {{axis}},        name,      &ReduceEConfigMap, -1}
{
}

std::string ReduceEMaxOp::function_name(const OpConfig &cfg) const
{
    return ReduceOp::function_name(cfg, "e_max");
}

ReduceWMeanOp::ReduceWMeanOp(OpPrecType prec_type, Tensor *input,
                             Tensor *output, int axis, const std::string &name)
    : ReduceOp{OP_REDUCE_W_MEAN, prec_type, {input},           {output},
               {{axis}},         name,      &ReduceWConfigMap, -1}
{
}

std::string ReduceWMeanOp::function_name(const OpConfig &cfg) const
{
    return ReduceOp::function_name(cfg, "w_mean");
}

ReduceEMeanOp::ReduceEMeanOp(OpPrecType prec_type, Tensor *input,
                             Tensor *output, int axis, const std::string &name)
    : ReduceOp{OP_REDUCE_E_MEAN, prec_type, {input},           {output},
               {{axis}},         name,      &ReduceEConfigMap, -1}
{
}

std::string ReduceEMeanOp::function_name(const OpConfig &cfg) const
{
    return ReduceOp::function_name(cfg, "e_mean");
}

Tensor *Model::reduce_sum(Tensor *input, int axis, Tensor *output,
                          const std::string &name)
{
    assert(input != nullptr);
    LOG(DEBUG, "reduce_sum ", input->shape, " ", input->ldims, " ", axis);
    OpPrecType pt;
    if (input->type == FP16) {
        pt = OP_PREC_FP16;
    } else if (input->type == FP32) {
        pt = OP_PREC_FP32;
    } else {
        LOGERR("unsupported input data type: ", type_str(input->type));
    }
    if (output != nullptr && input->type != output->type) {
        LOGERR("invalid output data type: ", type_str(output->type));
    }
    if (output == nullptr) {
        Dims reduced_shape{input->shape};
        reduced_shape[axis] = 1;
        output = this->tensor(reduced_shape, input->type);
    } else if (output == input) {
        LOGERR("output tensor cannot be the same as input tensor for "
               "reduce_sum op");
    }
    Tensor *ret;
    if (axis == input->shape.ndims() - 1) {
        ReduceWSumOp op{pt, input, output, axis, name};
        ret = this->impl->add_op(op)[0];
    } else {
        ReduceESumOp op{pt, input, output, axis, name};
        ret = this->impl->add_op(op)[0];
    }
    return ret;
}

Tensor *Model::reduce_mean(Tensor *input, int axis, Tensor *output,
                           const std::string &name)
{
    assert(input != nullptr);
    LOG(DEBUG, "reduce_mean ", input->shape, " ", input->ldims, " ", axis);
    OpPrecType pt;
    if (input->type == FP16) {
        pt = OP_PREC_FP16;
    } else if (input->type == FP32) {
        pt = OP_PREC_FP32;
    } else {
        LOGERR("unsupported input data type: ", type_str(input->type));
    }
    if (output != nullptr && input->type != output->type) {
        LOGERR("invalid output data type: ", type_str(output->type));
    }
    if (output == nullptr) {
        Dims reduced_shape{input->shape};
        reduced_shape[axis] = 1;
        output = this->tensor(reduced_shape, input->type);
    } else if (output == input) {
        LOGERR("output tensor cannot be the same as input tensor for "
               "reduce_mean op");
    }
    Tensor *ret;
    if (axis == input->shape.ndims() - 1) {
        ReduceWMeanOp op{pt, input, output, axis, name};
        ret = this->impl->add_op(op)[0];
    } else {
        ReduceEMeanOp op{pt, input, output, axis, name};
        ret = this->impl->add_op(op)[0];
    }
    return ret;
}

Tensor *Model::reduce_max(Tensor *input, int axis, Tensor *output,
                          const std::string &name)
{
    assert(input != nullptr);
    LOG(DEBUG, "reduce_max ", input->shape, " ", input->ldims, " ", axis);
    OpPrecType pt;
    if (input->type == FP16) {
        pt = OP_PREC_FP16;
    } else if (input->type == FP32) {
        pt = OP_PREC_FP32;
    } else {
        LOGERR("unsupported input data type: ", type_str(input->type));
    }
    if (output != nullptr && input->type != output->type) {
        LOGERR("invalid output data type: ", type_str(output->type));
    }
    if (output == nullptr) {
        Dims reduced_shape{input->shape};
        reduced_shape[axis] = 1;
        output = this->tensor(reduced_shape, input->type);
    } else if (output == input) {
        LOGERR("output tensor cannot be the same as input tensor for "
               "reduce_max op");
    }
    Tensor *ret;
    if (axis == input->shape.ndims() - 1) {
        ReduceWMaxOp op{pt, input, output, axis, name};
        ret = this->impl->add_op(op)[0];
    } else {
        ReduceEMaxOp op{pt, input, output, axis, name};
        ret = this->impl->add_op(op)[0];
    }
    return ret;
}

const OpConfigMap ReduceEConfigMap = {
    {{OP_ARCH_CUDA_80, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {8, 0, {{128, 256}, {128, 256}}, {{128, 256}}, true, false},
         {8, 0, {{256, 128}, {256, 128}}, {{256, 128}}, true, false},
         {8, 0, {{128, 128}, {128, 128}}, {{128, 128}}, true, false},
         {4, 0, {{64, 64}, {64, 64}}, {{64, 64}}, true, false},
         {2, 0, {{32, 64}, {32, 64}}, {{32, 64}}, true, false},
         {1, 0, {{16, 64}, {16, 64}}, {{16, 64}}, true, false},
         {1, 0, {{8, 64}, {8, 64}}, {{8, 64}}, true, false},
         {1, 0, {{2, 128}, {2, 128}}, {{2, 128}}, true, false},
         {1, 0, {{4, 64}, {4, 64}}, {{4, 64}}, true, false},
         {1, 0, {{2, 64}, {2, 64}}, {{2, 64}}, true, false},
         {1, 0, {{1, 64}, {1, 64}}, {{1, 64}}, true, false},
         {1, 0, {{1, 32}, {1, 32}}, {{1, 32}}, true, false},
     }},
};

const OpConfigMap ReduceWConfigMap = {
    {{OP_ARCH_CUDA_70, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {1, 128, {{32, 1}}, {{32, 1}}, true, false},
         {1, 128, {{16, 1}}, {{16, 1}}, true, false},
         {1, 128, {{8, 1}}, {{8, 1}}, true, false},
         {1, 128, {{4, 1}}, {{4, 1}}, true, false},
         {1, 128, {{2, 1}}, {{2, 1}}, true, false},
         {1, 128, {{1, 1}}, {{1, 1}}, true, false},
         {4, 128, {{1, 1}}, {{1, 1}}, true, false},
         {8, 128, {{1, 1}}, {{1, 1}}, true, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {1, 128, {{32, 1}}, {{32, 1}}, true, false},
         {1, 128, {{16, 1}}, {{16, 1}}, true, false},
         {1, 128, {{8, 1}}, {{8, 1}}, true, false},
         {1, 128, {{4, 1}}, {{4, 1}}, true, false},
         {1, 128, {{2, 1}}, {{2, 1}}, true, false},
         {1, 128, {{1, 1}}, {{1, 1}}, true, false},
         {4, 128, {{1, 1}}, {{1, 1}}, true, false},
         {8, 128, {{1, 1}}, {{1, 1}}, true, false},
     }},
    {{OP_ARCH_CUDA_70, OP_PREC_FP32},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {1, 128, {{32, 1}}, {{32, 1}}, true, false},
         {1, 128, {{16, 1}}, {{16, 1}}, true, false},
         {1, 128, {{8, 1}}, {{8, 1}}, true, false},
         {1, 128, {{4, 1}}, {{4, 1}}, true, false},
         {1, 128, {{2, 1}}, {{2, 1}}, true, false},
         {1, 128, {{1, 1}}, {{1, 1}}, true, false},
         {4, 128, {{1, 1}}, {{1, 1}}, true, false},
         {8, 128, {{1, 1}}, {{1, 1}}, true, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_FP32},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {1, 128, {{32, 1}}, {{32, 1}}, true, false},
         {1, 128, {{16, 1}}, {{16, 1}}, true, false},
         {1, 128, {{8, 1}}, {{8, 1}}, true, false},
         {1, 128, {{4, 1}}, {{4, 1}}, true, false},
         {1, 128, {{2, 1}}, {{2, 1}}, true, false},
         {1, 128, {{1, 1}}, {{1, 1}}, true, false},
         {4, 128, {{1, 1}}, {{1, 1}}, true, false},
         {8, 128, {{1, 1}}, {{1, 1}}, true, false},
     }},
};

} // namespace ark

```

`ark/ops/ops_reduce_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_kernel.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "logging.h"
#include "unittest/unittest_utils.h"
#include <cassert>

using namespace std;

//
void test_reduce_internal(unsigned int n, unsigned int m, unsigned int k,
                          int axis)
{
    size_t buf_x_sz = (size_t)m * (size_t)n * (size_t)k * sizeof(ark::half_t);
    size_t buf_y_sz = (size_t)m * (size_t)n * sizeof(ark::half_t);

    // Set data.
    ark::srand();
    auto data_a = ark::utils::rand_halfs(buf_x_sz / sizeof(ark::half_t), 0.01);

    // Copy the ground truth results into CPU memory.
    void *gt = malloc(buf_y_sz);
    UNITTEST_NE(gt, (void *)nullptr);

    for (unsigned int i = 0; i < n; ++i) {
        for (unsigned int j = 0; j < m; ++j) {
            ark::half_t v = 0;
            for (unsigned int l = 0; l < k; ++l) {
                int idx;
                if (axis == 0) {
                    idx = i * m + j + l * m * n;
                } else if (axis == 1) {
                    idx = i * m * k + j + l * m;
                } else if (axis == 2) {
                    idx = i * m * k + j * k + l;
                } else {
                    assert(false);
                }
                ark::half_t x = data_a[idx];
                v += x;
            }
            ((ark::half_t *)gt)[i * m + j] = v;
        }
    }

    //
    ark::Model model;
    ark::Tensor *tns_x = nullptr;
    ark::Tensor *tns_y = nullptr;
    if (axis == 0) {
        tns_x = model.tensor({k, n, m}, ark::FP16);
        tns_y = model.tensor({1, n, m}, ark::FP16);
    } else if (axis == 1) {
        tns_x = model.tensor({n, k, m}, ark::FP16);
        tns_y = model.tensor({n, 1, m}, ark::FP16);
    } else if (axis == 2) {
        tns_x = model.tensor({n, m, k}, ark::FP16);
        tns_y = model.tensor({n, m, 1}, ark::FP16);
    } else {
        LOGERR("invalid axis");
    }

    model.reduce_sum(tns_x, axis, tns_y);

    //
    ark::Executor exe{0, 0, 1, model, "test_reduce"};
    exe.compile();

    // Set data.
    exe.tensor_memcpy(tns_x, data_a.get(), buf_x_sz);

    exe.launch();
    exe.run(1);
    exe.stop();

    // Copy results of the loop kernel routine into CPU memory.
    void *res = malloc(buf_y_sz);
    UNITTEST_NE(res, (void *)nullptr);
    exe.tensor_memcpy(res, tns_y, buf_y_sz);

    // Compare results with the ground truth.
    auto p =
        ark::utils::cmp_matrix((ark::half_t *)gt, (ark::half_t *)res, m, n);
    float max_err = p.second;
    LOG(ark::INFO, "reduce:", n, 'x', m, 'x', k, " axis ", axis, " ",
        setprecision(4), " mse ", p.first, " max_err ", max_err * 100, "%");

    free(res);
    free(gt);

    UNITTEST_EQ(max_err, 0.0);
}

ark::unittest::State test_reduce()
{
    // TODO: implement reduce for axis = 0 and axis = 1
    for (int axis = 2; axis < 3; axis++) {
        test_reduce_internal(1, 64, 2, axis);
        test_reduce_internal(1, 64, 8, axis);
        test_reduce_internal(1, 64, 9, axis);
        test_reduce_internal(2, 64, 4, axis);
        test_reduce_internal(8, 64, 4, axis);
        test_reduce_internal(64, 64, 4, axis);
        test_reduce_internal(1, 256, 256, axis);
        test_reduce_internal(1024, 384, 4, axis);
    }

    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_reduce);
    return ark::unittest::SUCCESS;
}

```

`ark/ops/ops_relu.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include "tensor.h"
#include <cassert>

namespace ark {

extern const OpConfigMap ActivationConfigMap;

ReluOp::ReluOp(OpPrecType prec_type, Tensor *input, Tensor *output,
               const std::string &name)
    : Op{OP_RELU, prec_type, {input}, {output}, {}, name, &ActivationConfigMap,
         -1,      true}
{
}

std::string ReluOp::function_name(const OpConfig &cfg) const
{
    Tensor *input = this->inputs[0];
    Tensor *output = this->outputs[0];

    int ndims = output->shape.ndims();
    const OpTile &tile_out = cfg.output_tiles[0];
    CHECK(output->ldims[ndims - 1] % tile_out.y == 0);
    if (ndims > 1) {
        CHECK(output->ldims[ndims - 2] % tile_out.x == 0);
    } else {
        CHECK(tile_out.x == 1);
    }

    Dims unit_out_dims{1, 1, tile_out.x, tile_out.y};
    return Op::function_name("ark::relu", {{
                                              input->ldims.dims4(),  // InDims
                                              input->shape.dims4(),  // InShape
                                              output->ldims.dims4(), // OutDims
                                              output->shape.dims4(), // OutShape
                                              unit_out_dims,      // UnitOutDims
                                              cfg.num_warps * 32, // NumThreads
                                              cfg.smem_bytes,     // SmemBytes
                                          }});
}

Tensor *Model::relu(Tensor *input, Tensor *output, const std::string &name)
{
    assert(input != nullptr);
    OpPrecType pt;
    if (input->type == FP16) {
        pt = OP_PREC_FP16;
    } else if (input->type == FP32) {
        pt = OP_PREC_FP32;
    } else {
        LOGERR("unsupported input data type: ", type_str(input->type));
    }
    if (output != nullptr && input->type != output->type) {
        LOGERR("invalid output data type: ", type_str(output->type));
    }
    if (output == nullptr) {
        output = this->tensor(input->shape, input->type, input->buf);
    } else if (output->shape != input->shape) {
        LOGERR("invalid output shape: ", output->shape);
    }
    ReluOp op{pt, input, output, name};
    return this->impl->add_op(op)[0];
}

const OpConfigMap ActivationConfigMap = {
    {{OP_ARCH_CUDA_70, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {4, 0, {{64, 64}}, {{64, 64}}, false, false},
         {2, 0, {{32, 64}}, {{32, 64}}, false, false},
         {1, 0, {{16, 64}}, {{16, 64}}, false, false},
         {1, 0, {{8, 64}}, {{8, 64}}, false, false},
         {1, 0, {{2, 128}}, {{2, 128}}, false, false},
         {1, 0, {{4, 64}}, {{4, 64}}, false, false},
         {1, 0, {{2, 64}}, {{2, 64}}, false, false},
         {1, 0, {{1, 64}}, {{1, 64}}, false, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {8, 0, {{128, 256}}, {{128, 256}}, false, false},
         {8, 0, {{256, 128}}, {{256, 128}}, false, false},
         {8, 0, {{128, 128}}, {{128, 128}}, false, false},
         {4, 0, {{64, 64}}, {{64, 64}}, false, false},
         {2, 0, {{32, 64}}, {{32, 64}}, false, false},
         {1, 0, {{16, 64}}, {{16, 64}}, false, false},
         {1, 0, {{8, 64}}, {{8, 64}}, false, false},
         {1, 0, {{2, 128}}, {{2, 128}}, false, false},
         {1, 0, {{4, 64}}, {{4, 64}}, false, false},
         {1, 0, {{2, 64}}, {{2, 64}}, false, false},
         {1, 0, {{1, 64}}, {{1, 64}}, false, false},
     }},
};

} // namespace ark

```

`ark/ops/ops_reshape.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include "tensor.h"
#include <cassert>

namespace ark {

ReshapeOp::ReshapeOp(OpPrecType prec_type, Tensor *input, Tensor *output,
                     const std::string &name)
    : Op{OP_RESHAPE, prec_type, {input}, {output}, {}, name, nullptr, -1, true}
{
}

// Reshape `input` to `shape`. This interface does not support -1 as a dimension
// of `shape`, because Dims does not allow -1 as a valid dimension.
static Tensor *_reshape(Model *model, Tensor *input, const Dims &shape,
                        bool allowzero, Tensor *output, const std::string &)
{
    if (input == nullptr) {
        LOGERR("input is null");
    }
    LOG(DEBUG, "reshape ", input->shape, " ", shape);
    // Infer the actual shape
    std::vector<DimType> inferred_shape;
    if (shape.ndims() == 0) {
        // Convert to a scalar
        inferred_shape.emplace_back(1);
        if (input->shape.size() != 1) {
            LOGERR("number of elements mismatch: reshape from ", input->shape,
                   " to ", shape);
        }
    } else {
        DimType total_size = 1;
        for (int i = 0; i < shape.ndims(); i++) {
            if (shape[i] == 0) {
                if (allowzero) {
                    inferred_shape.push_back(0);
                    total_size = 0;
                } else {
                    inferred_shape.push_back(input->shape[i]);
                    total_size *= input->shape[i];
                }
            } else {
                assert(shape[i] > 0);
                inferred_shape.push_back(shape[i]);
                total_size *= shape[i];
            }
        }
        if (input->shape.size() != total_size) {
            LOGERR("number of elements mismatch: reshape from ", input->shape,
                   " to ", shape);
        }
    }
    Dims new_shape{inferred_shape};

    if (output != nullptr) {
        // Verfiy given `output`
        if (input->type != output->type) {
            LOGERR("invalid output data type: ", type_str(output->type));
        }
        if (input->shape.size() != output->shape.size()) {
            LOGERR("shape sizes mismatch: input ", input->shape, ", output ",
                   output->shape);
        }
    }

    // TODO: check if this reshape requires any copy

    if (output == nullptr) {
        output = model->tensor(new_shape, input->type, input->buf, shape);
    }
    return output;
}

//
Tensor *Model::reshape(Tensor *input, const Dims &shape, bool allowzero,
                       Tensor *output, const std::string &name)
{
    output = _reshape(this, input, shape, allowzero, output, name);
    ReshapeOp op{OP_PREC_NONE, input, output, name};
    return this->impl->add_op(op)[0];
}

// Reshape `input` to `shape`. If one dimension of `shape` is -1, it will be
// inferred from the `input`. If one dimension of `shape` is 0, by default
// (`allowzero` is false), that dimension is unchanged from the corresponding
// one of `input`. If `allowzero` is true, that dimension is set to 0, which
// means that the reshaped tensor is an empty tensor, i.e., `input` should also
// be an empty tensor. If `allowzero` is true, `shape` should not include both
// 0 and -1 at the same time. If `shape` is an empty vector, `input` will be
// converted to a scalar.
Tensor *Model::reshape(Tensor *input,
                       const std::initializer_list<DimType> shape,
                       bool allowzero, Tensor *output, const std::string &name)
{
    if (input == nullptr) {
        LOGERR("input is null");
    }
    std::vector<DimType> shape_vec{shape};
    // Infer -1 dimension if exists
    int neg_idx = -1;
    bool zero_exists = false;
    std::vector<DimType> inferred_shape;
    DimType total_size = 1;
    for (size_t i = 0; i < shape_vec.size(); i++) {
        if (shape_vec[i] == -1) {
            if (neg_idx != -1) {
                LOGERR("multiple -1 in shape: ", Dims(shape_vec));
            }
            neg_idx = (int)i;
        } else if (shape_vec[i] < 0) {
            LOGERR("shape cannot include negative values except -1. "
                   "Given: ",
                   Dims(shape_vec));
        } else {
            if (shape_vec[i] == 0) {
                zero_exists = true;
            }
            total_size *= shape_vec[i];
        }
        inferred_shape.push_back(shape_vec[i]);
    }
    if (neg_idx != -1) {
        if (zero_exists) {
            LOGERR("shape cannot include both 0 and -1 at the same "
                   "time. Given: ",
                   Dims(shape_vec));
        }
        // Infer the -1 dimension
        if (total_size <= 0) {
            LOGERR("Unexpected error");
        }
        if (input->shape.size() % total_size != 0) {
            LOGERR("number of elements mismatch: reshape from ", input->shape,
                   " to ", Dims(shape_vec));
        }
        inferred_shape[neg_idx] = input->shape.size() / total_size;
    } else if (input->shape.size() != total_size) {
        LOGERR("number of elements mismatch: reshape from ", input->shape,
               " to ", Dims(shape_vec));
    }
    output =
        _reshape(this, input, Dims{inferred_shape}, allowzero, output, name);
    ReshapeOp op{OP_PREC_NONE, input, output, name};
    return this->impl->add_op(op)[0];
}

} // namespace ark

```

`ark/ops/ops_reshape_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "include/ark.h"
#include "include/ark_utils.h"
#include "unittest/unittest_utils.h"

using namespace std;

ark::unittest::State test_reshape()
{
    ark::Model model;
    // float buf[2][3][4][5];
    ark::Tensor *tns0 = model.tensor({2, 3, 4, 5}, ark::FP32);
    ark::Tensor *tns1 = model.reshape(tns0, {5, 4, 3, 2});

    // Create an executor
    ark::Executor exe{0, 0, 1, model, "test_tensor_layout"};
    exe.compile();

    int num_elem = 2 * 3 * 4 * 5;

    UNITTEST_EQ(tns1->shape, ark::Dims(5, 4, 3, 2));

    ark::GpuBuf *buf0 = exe.get_gpu_buf(tns0);
    ark::GpuBuf *buf1 = exe.get_gpu_buf(tns1);
    UNITTEST_NE(buf0, (ark::GpuBuf *)nullptr);
    UNITTEST_NE(buf1, (ark::GpuBuf *)nullptr);
    UNITTEST_EQ(buf0->get_bytes(), num_elem * sizeof(float));
    UNITTEST_EQ(buf1->get_bytes(), num_elem * sizeof(float));

    // Fill tensor data: {1.0, 2.0, 3.0, ..., 120.0}
    auto data = ark::utils::range_floats(num_elem);
    exe.tensor_memcpy(tns0, data.get(), num_elem * sizeof(float));

    // Check identity values
    float *ref_val = new float[num_elem];
    exe.tensor_memcpy(ref_val, tns1, num_elem * sizeof(float));
    for (int i = 0; i < num_elem; ++i) {
        UNITTEST_EQ(ref_val[i], (float)(i + 1));
    }

    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_reshape);
    return ark::unittest::SUCCESS;
}

```

`ark/ops/ops_scale.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include "tensor.h"
#include <cassert>

namespace ark {

extern const OpConfigMap ScaleConfigMap;

ScaleOp::ScaleOp(OpPrecType prec_type, Tensor *input, Tensor *output, float val,
                 const std::string &name)
    : Op{OP_SCALE, prec_type,       {input}, {output}, {{val}},
         name,     &ScaleConfigMap, -1,      true}
{
}

std::string ScaleOp::function_name(const OpConfig &cfg) const
{
    Tensor *input = this->inputs[0];
    Tensor *output = this->outputs[0];

    int ndims = output->shape.ndims();
    const OpTile &tile_out = cfg.output_tiles[0];
    CHECK(output->ldims[ndims - 1] % tile_out.y == 0);
    if (ndims > 1) {
        CHECK(output->ldims[ndims - 2] % tile_out.x == 0);
    } else {
        CHECK(tile_out.x == 1);
    }

    Dims unit_out_dims{1, 1, tile_out.x, tile_out.y};
    return Op::function_name("ark::scale",
                             {{
                                 input->ldims.dims4(),  // InDims
                                 input->shape.dims4(),  // InShape
                                 output->ldims.dims4(), // OutDims
                                 output->shape.dims4(), // OutShape
                                 unit_out_dims,         // UnitOutDims
                                 cfg.num_warps * 32,    // NumThreads
                                 cfg.smem_bytes,        // SmemBytes
                             }});
}

OpArgs ScaleOp::function_call_args(const OpConfig &) const
{
    OpArgs opargs;
    std::vector<Tensor *> deps = this->outputs;
    deps.insert(deps.end(), this->inputs.begin(), this->inputs.end());
    for (Tensor *tns : deps) {
        opargs.put(tns);
    }
    float val;
    this->args.get(&val, 0);
    opargs.put(val);
    return opargs;
}

// Multiply `input` by `val`.
Tensor *Model::scale(Tensor *input, float val, Tensor *output,
                     const std::string &name)
{
    assert(input != nullptr);
    OpPrecType pt;
    if (input->type == FP16) {
        pt = OP_PREC_FP16;
    } else if (input->type == FP32) {
        pt = OP_PREC_FP32;
    } else {
        LOGERR("unsupported input data type: ", type_str(input->type));
    }
    if (output != nullptr && input->type != output->type) {
        LOGERR("invalid output data type: ", type_str(output->type));
    }
    if (output == nullptr) {
        output = this->tensor(input->shape, input->type, input->buf);
    } else if (output->shape != input->shape) {
        LOGERR("invalid output shape: ", output->shape);
    }
    ScaleOp op{pt, input, output, val, name};
    return this->impl->add_op(op)[0];
}

const OpConfigMap ScaleConfigMap = {
    {{OP_ARCH_CUDA_70, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {4, 0, {{64, 64}}, {{64, 64}}, false, false},
         {2, 0, {{32, 64}}, {{32, 64}}, false, false},
         {1, 0, {{16, 64}}, {{16, 64}}, false, false},
         {1, 0, {{8, 64}}, {{8, 64}}, false, false},
         {1, 0, {{2, 128}}, {{2, 128}}, false, false},
         {1, 0, {{4, 64}}, {{4, 64}}, false, false},
         {1, 0, {{2, 64}}, {{2, 64}}, false, false},
         {1, 0, {{1, 64}}, {{1, 64}}, false, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {8, 0, {{128, 256}}, {{128, 256}}, false, false},
         {8, 0, {{256, 128}}, {{256, 128}}, false, false},
         {8, 0, {{128, 128}}, {{128, 128}}, false, false},
         {4, 0, {{64, 64}}, {{64, 64}}, false, false},
         {2, 0, {{32, 64}}, {{32, 64}}, false, false},
         {1, 0, {{16, 64}}, {{16, 64}}, false, false},
         {1, 0, {{8, 64}}, {{8, 64}}, false, false},
         {1, 0, {{2, 128}}, {{2, 128}}, false, false},
         {1, 0, {{4, 64}}, {{4, 64}}, false, false},
         {1, 0, {{2, 64}}, {{2, 64}}, false, false},
         {1, 0, {{1, 64}}, {{1, 64}}, false, false},
     }},
};

} // namespace ark

```

`ark/ops/ops_scale_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_kernel.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "logging.h"
#include "unittest/unittest_utils.h"

using namespace std;

//
void test_scale_internal(unsigned int bs, unsigned int n, unsigned int m,
                         float val = 0.7)
{
    ark::GpuMgr *mgr = ark::get_gpu_mgr(0);
    ark::GpuMgrCtx *ctx = mgr->create_context("test_simple_scale", 0, 1);

    unsigned int len = bs * m * n;
    ark::GpuBuf *buf_x = ctx->mem_alloc(len * sizeof(ark::half_t));
    ark::GpuBuf *buf_y = ctx->mem_alloc(len * sizeof(ark::half_t));

    ctx->freeze();

    ark::GpuKernel gk{"simple_scale",
                      {ark::unittest::get_kernel_code("simple_scale")},
                      {(unsigned int)mgr->get_gpu_info().num_sm, 1, 1},
                      {512, 1, 1},
                      0,
                      {buf_y, buf_x},
                      {},
                      {
                          {&val, sizeof(val)},
                          {&len, sizeof(len)},
                      },
                      ""};
    gk.compile(mgr->get_gpu_info());
    gk.load();

    // Set data.
    ark::srand();
    auto data_x = ark::utils::rand_halfs(len, 0.01);
    ark::gpu_memcpy(buf_x, data_x.get(), len * sizeof(ark::half_t));

    // Run the GPU kernel.
    ark::GpuStream s = ctx->create_stream();
    int ret = gk.launch(s);
    UNITTEST_EQ(ret, 0);
    ret = ctx->sync_stream(s);
    UNITTEST_EQ(ret, 0);

    // Copy the ground truth results into CPU memory.
    void *gt = malloc(len * sizeof(ark::half_t));
    UNITTEST_NE(gt, (void *)nullptr);
    ark::gpu_memcpy(gt, buf_y, len * sizeof(ark::half_t));

    mgr->destroy_context(ctx);

    //
    ark::Model model;
    ark::Tensor *tns_x = model.tensor({bs, n, m}, ark::FP16);
    ark::Tensor *tns_y = model.scale(tns_x, val);

    //
    ark::Executor exe{0, 0, 1, model, "test_scale"};
    exe.compile();

    // Set data.
    exe.tensor_memcpy(tns_x, data_x.get(), len * sizeof(ark::half_t));

    exe.launch();
    exe.run(1);
    exe.stop();

    // Copy results of the loop kernel routine into CPU memory.
    void *res = malloc(len * sizeof(ark::half_t));
    UNITTEST_NE(res, (void *)nullptr);
    exe.tensor_memcpy(res, tns_y, len * sizeof(ark::half_t));

    // Compare results with the ground truth.
    auto p =
        ark::utils::cmp_matrix((ark::half_t *)gt, (ark::half_t *)res, m, n, bs);
    float max_err = p.second;
    LOG(ark::INFO, "scale:", n, 'x', m, ",bs=", bs, setprecision(4), " mse ",
        p.first, " max_err ", max_err * 100, "%");

    free(res);
    free(gt);

    UNITTEST_EQ(max_err, 0.0);
}

ark::unittest::State test_scale()
{
    test_scale_internal(1, 1, 64);
    test_scale_internal(1, 128, 128);
    test_scale_internal(1, 4096, 1024);
    test_scale_internal(1, 1024, 4096);
    test_scale_internal(2, 1, 64);
    test_scale_internal(2, 128, 128);
    test_scale_internal(8, 4096, 1024);
    test_scale_internal(8, 1024, 4096);
    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_scale);
    return ark::unittest::SUCCESS;
}

```

`ark/ops/ops_sendrecv.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include <cassert>

namespace ark {

extern const OpConfigMap CommConfigMap;

SendOp::SendOp(OpPrecType prec_type, Tensor *input, Tensor *output, int sid,
               int rank, int dst_rank, size_t bytes, const std::string &name)
    : Op{OP_SEND,
         prec_type,
         {input},
         {output},
         {{sid, rank, dst_rank, bytes}},
         name,
         &CommConfigMap,
         -1,
         true}
{
}

std::string SendOp::function_name(const OpConfig &) const
{
    Tensor *input = this->inputs[0];
    CHECK(input->is_sequential());

    int sid;
    int rank;
    int dst_rank;
    size_t bytes;
    this->args.get(&sid, 0);
    this->args.get(&rank, 1);
    this->args.get(&dst_rank, 2);
    this->args.get(&bytes, 3);

    return Op::function_name("ark::comm::send", {{
                                                    rank,     // Rank
                                                    dst_rank, // DstRank
                                                    sid,      // SrcSid
                                                    sid,      // DstSid
                                                    bytes,    // Length
                                                }});
}

OpArgs SendOp::function_call_args(const OpConfig &) const
{
    return {};
}

SendDoneOp::SendDoneOp(OpPrecType prec_type, Tensor *input, Tensor *output,
                       int sid, int rank, int dst_rank, const std::string &name)
    : Op{OP_SEND_DONE,
         prec_type,
         {input},
         {output},
         {{sid, rank, dst_rank}},
         name,
         &CommConfigMap,
         -1,
         true}
{
}

std::string SendDoneOp::function_name(const OpConfig &) const
{
    int sid;
    int rank;
    int dst_rank;
    this->args.get(&sid, 0);
    this->args.get(&rank, 1);
    this->args.get(&dst_rank, 2);

    return Op::function_name("ark::comm::send_done", {{
                                                         rank,     // Rank
                                                         dst_rank, // DstRank
                                                         sid,      // SrcSid
                                                     }});
}

OpArgs SendDoneOp::function_call_args(const OpConfig &) const
{
    return {};
}

RecvOp::RecvOp(OpPrecType prec_type, Tensor *input, Tensor *output, int sid,
               int rank, int src_rank, size_t bytes, const std::string &name)
    : Op{OP_RECV,
         prec_type,
         {input},
         {output},
         {{sid, rank, src_rank, bytes}},
         name,
         &CommConfigMap,
         -1,
         true}
{
}

std::string RecvOp::function_name(const OpConfig &) const
{
    Tensor *input = this->inputs[0];
    CHECK(input->is_sequential());

    int sid;
    int rank;
    int src_rank;
    this->args.get(&sid, 0);
    this->args.get(&rank, 1);
    this->args.get(&src_rank, 2);

    return Op::function_name("ark::comm::recv", {{
                                                    rank,     // Rank
                                                    src_rank, // DstRank
                                                    sid,      // SrcSid
                                                }});
}

OpArgs RecvOp::function_call_args(const OpConfig &) const
{
    return {};
}

//
Tensor *Model::send(Tensor *input, int id, int dst_rank, size_t bytes,
                    Tensor *output, const std::string &name)
{
    size_t max_bytes = input->shape_bytes();
    if (max_bytes < bytes) {
        LOGERR("invalid bytes: ", bytes, ", max: ", max_bytes);
    }
    if (bytes == 0) {
        bytes = max_bytes;
    }
    LOG(DEBUG, "send ", input->shape, " ", id, " ", dst_rank, " ", bytes);
    input->exported = true;
    if (output == nullptr) {
        output = this->tensor({1, 1, 1, 1}, INT32);
    }
    SendOp op{OP_PREC_NONE,     input,    output, id,
              this->impl->rank, dst_rank, bytes,  name};
    return this->impl->add_op(op)[0];
}

//
Tensor *Model::send_done(Tensor *input, int id, int dst_rank, Tensor *output,
                         const std::string &name)
{
    LOG(DEBUG, "send_done ", input->shape, " ", id);
    if (output == nullptr) {
        output = this->tensor({1, 1, 1, 1}, INT32);
    }
    SendDoneOp op{OP_PREC_NONE,     input,    output, id,
                  this->impl->rank, dst_rank, name};
    return this->impl->add_op(op)[0];
}

//
Tensor *Model::recv(Tensor *input, int id, int src_rank, size_t bytes,
                    Tensor *output, const std::string &name)
{
    assert(input != nullptr);
    size_t max_bytes = input->shape_bytes();
    if (max_bytes < bytes) {
        LOGERR("invalid bytes: ", bytes, ", max: ", max_bytes);
    }
    if (bytes == 0) {
        bytes = max_bytes;
    }
    LOG(DEBUG, "recv ", input->shape, " ", id, " ", src_rank, " ", bytes);
    input->exported = true;
    if (output == nullptr) {
        output = this->tensor({1, 1, 1, 1}, INT32);
    }
    RecvOp op{OP_PREC_NONE,     input,    output, id,
              this->impl->rank, src_rank, bytes,  name};
    return this->impl->add_op(op)[0];
}

const OpConfigMap CommConfigMap = {
    {{OP_ARCH_CUDA_70, OP_PREC_NONE},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {1, 0, {{1, 1}}, {{1, 1}}, true, true},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_NONE},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {1, 0, {{1, 1}}, {{1, 1}}, true, true},
     }},
};

} // namespace ark

```

`ark/ops/ops_sendrecv_mm.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "env.h"
#include "logging.h"
#include "model.h"
#include "tensor.h"
#include <cassert>

namespace ark {

extern const OpConfigMap SendRecvMMConfigMap;

SendMMOp::SendMMOp(OpPrecType prec_type, Tensor *input, Tensor *recvbuf,
                   Tensor *send_ready_flag, Tensor *output, int id, int gpu_dst,
                   size_t bytes, const std::string &name)
    : Op{OP_SEND_MM,
         prec_type,
         {input, recvbuf, send_ready_flag},
         {output},
         {{id, gpu_dst, bytes}},
         name,
         &SendRecvMMConfigMap,
         -1}
{
}

std::string SendMMOp::function_name(const OpConfig &cfg) const
{
    Tensor *input = this->inputs[0];
    Dims shp_in = input->shape;
    Tensor *output = this->outputs[0];

    int ndims = output->shape.ndims();
    const OpTile &tile_out = cfg.output_tiles[0];
    CHECK(output->ldims[ndims - 1] % tile_out.y == 0);
    if (ndims > 1) {
        CHECK(output->ldims[ndims - 2] % tile_out.x == 0);
    } else {
        CHECK(tile_out.x == 1);
    }

    CHECK(ndims == 2);
    DimType m = shp_in[ndims - 1];
    DimType n = shp_in[ndims - 2];
    Dims pad_in = input->pads;
    const OpTile &tile_in = cfg.input_tiles[0];
    // Verify paddings
    CHECK((shp_in[ndims - 2] % tile_in.x == 0) ||
          (pad_in[ndims - 2] >= tile_in.x));
    CHECK((shp_in[ndims - 1] % tile_in.y == 0) ||
          (pad_in[ndims - 1] >= tile_in.y));

    return Op::function_name("ark::comm::sendLL",
                             {{
                                 m,                  // LDM
                                 n,                  // LDN
                                 cfg.num_warps * 32, // TN
                                 cfg.smem_bytes,     // SmemBytes
                                 tile_in.y,          // TDM
                                 tile_in.x,          // TDN
                                 1,                  // FLAG
                             }});
}

OpArgs SendMMOp::function_call_args(const OpConfig &) const
{
    OpArgs opargs;
    opargs.put(this->inputs[1]);
    opargs.put(this->inputs[0]);
    opargs.put(this->inputs[2]);
    return opargs;
}

RecvMMOp::RecvMMOp(OpPrecType prec_type, Tensor *input, Tensor *recvbuf,
                   Tensor *send_ready_flag, Tensor *output, int id, int gpu_src,
                   size_t bytes, const std::string &name)
    : Op{OP_RECV_MM,
         prec_type,
         {input, recvbuf, send_ready_flag},
         {output},
         {{id, gpu_src, bytes}},
         name,
         &SendRecvMMConfigMap,
         -1}
{
}

std::string RecvMMOp::function_name(const OpConfig &cfg) const
{
    Tensor *input = this->inputs[0];
    Dims shp_in = input->shape;
    Tensor *output = this->outputs[0];

    int ndims = output->shape.ndims();
    const OpTile &tile_out = cfg.output_tiles[0];
    CHECK(output->ldims[ndims - 1] % tile_out.y == 0);
    if (ndims > 1) {
        CHECK(output->ldims[ndims - 2] % tile_out.x == 0);
    } else {
        CHECK(tile_out.x == 1);
    }

    CHECK(ndims == 2);
    DimType m = shp_in[ndims - 1];
    DimType n = shp_in[ndims - 2];
    Dims pad_in = input->pads;
    const OpTile &tile_in = cfg.input_tiles[0];
    // Verify paddings
    CHECK((shp_in[ndims - 2] % tile_in.x == 0) ||
          (pad_in[ndims - 2] >= tile_in.x));
    CHECK((shp_in[ndims - 1] % tile_in.y == 0) ||
          (pad_in[ndims - 1] >= tile_in.y));

    return Op::function_name("ark::comm::recvLL",
                             {{
                                 m,                  // LDM
                                 n,                  // LDN
                                 cfg.num_warps * 32, // TN
                                 cfg.smem_bytes,     // SmemBytes
                                 tile_in.y,          // TDM
                                 tile_in.x,          // TDN
                                 1,                  // FLAG
                             }});
}

OpArgs RecvMMOp::function_call_args(const OpConfig &) const
{
    OpArgs opargs;
    opargs.put(this->inputs[1]);
    opargs.put(this->inputs[0]);
    opargs.put(this->inputs[2]);
    return opargs;
}

// TODO: set the max_tile_num according to the tile number of the op
const int max_tile_num = 2048;

// send data from src to dst of id
Tensor *Model::send_mm(Tensor *input, int id, int gpu_dst, size_t bytes,
                       Tensor *output, const std::string &name)
{
    assert(input != nullptr);
    size_t max_bytes = input->ldims_bytes();
    if (max_bytes < bytes) {
        LOGERR("invalid bytes: ", bytes, ", max: ", max_bytes);
    }
    if (bytes == 0) {
        bytes = max_bytes;
    }
    LOG(DEBUG, "send_mm", input->shape, " ", id, " ", gpu_dst, " ", bytes);
    if (output != nullptr && input->type != output->type) {
        LOGERR("invalid output data type: ", type_str(output->type));
    }
    if (output == nullptr) {
        output = this->tensor(input->shape, input->type, input->buf);
    } else if (output->shape != input->shape) {
        LOGERR("invalid output shape: ", output->shape);
    }
    Dims recvbuf_shape = input->shape;
    int ndims = recvbuf_shape.ndims();
    recvbuf_shape[ndims - 2] *= 2 * input->type_bytes();
    Tensor *recvbuf = this->tensor(recvbuf_shape, ark::BYTE);
    recvbuf->imported_rank = gpu_dst;
    Tensor *send_ready_flag = this->tensor(
        {
            max_tile_num,
        },
        INT32);
    send_ready_flag->exported = true;
    SendMMOp op{OP_PREC_NONE, input, recvbuf, send_ready_flag, output, id,
                gpu_dst,      bytes, name};
    return this->impl->add_op(op)[0];
}

//
Tensor *Model::recv_mm(Tensor *input, int id, int gpu_src, size_t bytes,
                       Tensor *output, const std::string &name)
{
    assert(input != nullptr);
    size_t max_bytes = input->ldims_bytes();
    if (max_bytes < bytes) {
        LOGERR("invalid bytes: ", bytes, ", max: ", max_bytes);
    }
    if (bytes == 0) {
        bytes = max_bytes;
    }
    LOG(DEBUG, "recv_mm", input->shape, " ", id, " ", gpu_src, " ", bytes);
    input->exported = true;

    if (output != nullptr && input->type != output->type) {
        LOGERR("invalid output data type: ", type_str(output->type));
    }
    if (output == nullptr) {
        output = this->tensor(input->shape, input->type, input->buf);
    } else if (output->shape != input->shape) {
        LOGERR("invalid output shape: ", output->shape);
    }
    // use a tensor as recvbuf to store the received data, the size of the
    // recvbuf is twice of the input
    Dims recvbuf_shape = input->shape;
    int ndims = recvbuf_shape.ndims();
    recvbuf_shape[ndims - 2] *= 2 * input->type_bytes();
    Tensor *recvbuf = this->tensor(recvbuf_shape, ark::BYTE);
    recvbuf->exported = true;
    Tensor *send_ready_flag = this->tensor(
        {
            max_tile_num,
        },
        INT32);
    send_ready_flag->imported_rank = gpu_src;
    RecvMMOp op{OP_PREC_NONE, input, recvbuf, send_ready_flag, output, id,
                gpu_src,      bytes, name};
    return this->impl->add_op(op)[0];
}

const OpConfigMap SendRecvMMConfigMap = {
    {{OP_ARCH_CUDA_70, OP_PREC_NONE},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {4, 0, {{64, 64}, {64, 64}, {1, 1}}, {{64, 64}}, false, false},
         {2, 0, {{32, 64}, {32, 64}, {1, 1}}, {{32, 64}}, false, false},
         {1, 0, {{16, 64}, {16, 64}, {1, 1}}, {{16, 64}}, false, false},
         {1, 0, {{8, 64}, {8, 64}, {1, 1}}, {{8, 64}}, false, false},
         {1, 0, {{2, 128}, {2, 128}, {1, 1}}, {{2, 128}}, false, false},
         {1, 0, {{4, 64}, {4, 64}, {1, 1}}, {{4, 64}}, false, false},
         {1, 0, {{2, 64}, {2, 64}, {1, 1}}, {{2, 64}}, false, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_NONE},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {4, 0, {{64, 64}, {64, 64}, {1, 1}}, {{64, 64}}, false, false},
         {2, 0, {{32, 64}, {32, 64}, {1, 1}}, {{32, 64}}, false, false},
         {1, 0, {{16, 64}, {16, 64}, {1, 1}}, {{16, 64}}, false, false},
         {1, 0, {{8, 64}, {8, 64}, {1, 1}}, {{8, 64}}, false, false},
         {1, 0, {{2, 128}, {2, 128}, {1, 1}}, {{2, 128}}, false, false},
         {1, 0, {{4, 64}, {4, 64}, {1, 1}}, {{4, 64}}, false, false},
         {1, 0, {{2, 64}, {2, 64}, {1, 1}}, {{2, 64}}, false, false},
     }},
};

} // namespace ark

```

`ark/ops/ops_sendrecv_mm_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "include/ark.h"
#include "include/ark_utils.h"
#include "logging.h"
#include "unittest/unittest_utils.h"

ark::unittest::State test_sendrecv_mm_copy_internal(ark::DimType mat_length)
{
    ark::srand();

    ark::DimType mat_size = mat_length * mat_length;
    auto send_data = ark::utils::rand_halfs(mat_size, 5.0f);

    ark::unittest::spawn_process([&]() {
        ark::Model m;
        ark::Tensor *data = m.tensor({mat_length, mat_length}, ark::FP16);
        m.send_mm(data, 0, 1, 0);

        ark::Executor exe{0, 0, 2, m, "test_sendrecv_mm_copy"};
        exe.compile();
        exe.tensor_memcpy(data, send_data.get(),
                          mat_size * sizeof(ark::half_t));
        exe.launch();
        exe.run(1);
        exe.stop();

        return ark::unittest::SUCCESS;
    });

    ark::unittest::spawn_process([&]() {
        ark::Model m;
        ark::Tensor *recvbuf = m.tensor({mat_length, mat_length}, ark::FP16);
        m.recv_mm(recvbuf, 0, 0, 0);

        ark::Executor exe{1, 1, 2, m, "test_sendrecv_mm_copy"};
        exe.compile();
        exe.launch();
        exe.run(1);
        exe.stop();

        auto recv_data = ark::utils::zeros<ark::half_t>(mat_size);
        exe.tensor_memcpy(recv_data.get(), recvbuf,
                          mat_size * sizeof(ark::half_t));

        for (int i = 0; i < mat_size; i++) {
            if (recv_data[i] != send_data[i]) {
                LOG(ark::INFO, "error at ", i,
                    ": recv_data=", float(recv_data[i]),
                    "send_data=", float(send_data[i]));
                return ark::unittest::FAILURE;
            }
        }
        return ark::unittest::SUCCESS;
    });
    ark::unittest::wait_all_processes();
    return ark::unittest::SUCCESS;
}

ark::unittest::State test_sendrecv_mm_copy_bidir_internal(
    ark::DimType mat_length)
{
    ark::srand();

    ark::DimType mat_size = mat_length * mat_length;
    auto send_data_0 = ark::utils::rand_halfs(mat_size, 5.0f);
    auto send_data_1 = ark::utils::rand_halfs(mat_size, 5.0f);

    ark::unittest::spawn_process([&]() {
        ark::Model m;

        ark::Tensor *data = m.tensor({mat_length, mat_length}, ark::FP16);
        m.send_mm(data, 0, 1, 0);

        ark::Tensor *recvbuf = m.tensor({mat_length, mat_length}, ark::FP16);
        m.recv_mm(recvbuf, 1, 1, 0);

        ark::Executor exe{0, 0, 2, m, "test_sendrecv_mm_copy"};
        exe.compile();
        exe.tensor_memcpy(data, send_data_0.get(),
                          mat_size * sizeof(ark::half_t));
        exe.launch();
        exe.run(1);
        exe.stop();

        auto recv_data = ark::utils::zeros<ark::half_t>(mat_size);
        exe.tensor_memcpy(recv_data.get(), recvbuf,
                          mat_size * sizeof(ark::half_t));

        for (int i = 0; i < mat_size; i++) {
            if (recv_data[i] != send_data_1[i]) {
                LOG(ark::INFO, "error at ", i,
                    ": recv_data=", float(recv_data[i]),
                    "send_data=", float(send_data_1[i]));
                return ark::unittest::FAILURE;
            }
        }
        return ark::unittest::SUCCESS;
    });

    ark::unittest::spawn_process([&]() {
        ark::Model m;
        ark::Tensor *data = m.tensor({mat_length, mat_length}, ark::FP16);
        m.send_mm(data, 1, 0, 0);

        ark::Tensor *recvbuf = m.tensor({mat_length, mat_length}, ark::FP16);
        m.recv_mm(recvbuf, 0, 0, 0);

        ark::Executor exe{1, 1, 2, m, "test_sendrecv_mm_copy"};
        exe.compile();
        exe.tensor_memcpy(data, send_data_1.get(),
                          mat_size * sizeof(ark::half_t));
        exe.launch();
        exe.run(1);
        exe.stop();

        auto recv_data = ark::utils::zeros<ark::half_t>(mat_size);
        exe.tensor_memcpy(recv_data.get(), recvbuf,
                          mat_size * sizeof(ark::half_t));

        for (int i = 0; i < mat_size; i++) {
            if (recv_data[i] != send_data_0[i]) {
                LOG(ark::INFO, "error at ", i,
                    ": recv_data=", float(recv_data[i]),
                    "send_data=", float(send_data_0[i]));
                return ark::unittest::FAILURE;
            }
        }
        return ark::unittest::SUCCESS;
    });
    ark::unittest::wait_all_processes();
    return ark::unittest::SUCCESS;
}

ark::unittest::State test_sendrecv_mm_4gpus()
{
    // the four gpus send recv data in a ring, gpu0->gpu1->gpu2->gpu3->gpu0
    const int gpu_num = 4;
    ark::DimType mat_length = 64;
    ark::DimType mat_size = mat_length * mat_length;

    std::unique_ptr<ark::half_t[]> send_data[gpu_num];
    for (int i = 0; i < gpu_num; ++i) {
        send_data[i] = ark::utils::rand_halfs(mat_size, 5.0f);
    }

    for (int gpu_id = 0; gpu_id < gpu_num; gpu_id++) {
        ark::unittest::spawn_process([&]() {
            ark::Model m;
            ark::Tensor *data = m.tensor({mat_length, mat_length}, ark::FP16);
            m.send_mm(data, (gpu_id + 1) % gpu_num, (gpu_id + 1) % gpu_num);

            ark::Tensor *recvbuf =
                m.tensor({mat_length, mat_length}, ark::FP16);
            m.recv_mm(recvbuf, gpu_id, (gpu_id - 1 + gpu_num) % gpu_num);

            ark::Executor exe{gpu_id, gpu_id, gpu_num, m,
                              "test_sendrecv_mm_copy"};
            exe.compile();
            exe.tensor_memcpy(data, send_data[gpu_id].get(),
                              mat_size * sizeof(ark::half_t));
            exe.launch();
            exe.run(1);
            exe.stop();

            auto recv_data = ark::utils::zeros<ark::half_t>(mat_size);
            exe.tensor_memcpy(recv_data.get(), recvbuf,
                              mat_size * sizeof(ark::half_t));

            auto &gt = send_data[(gpu_id - 1 + gpu_num) % gpu_num];
            for (int i = 0; i < mat_size; i++) {
                if (recv_data[i] != gt[i]) {
                    LOG(ark::INFO, "error at ", i,
                        ": recv_data=", float(recv_data[i]),
                        "send_data=", float(gt[i]));
                    return ark::unittest::FAILURE;
                }
            }
            return ark::unittest::SUCCESS;
        });
    }

    ark::unittest::wait_all_processes();
    return ark::unittest::SUCCESS;
}

ark::unittest::State test_sendrecv_mm_copy()
{
    test_sendrecv_mm_copy_internal(64);
    test_sendrecv_mm_copy_internal(128);
    test_sendrecv_mm_copy_internal(256);
    test_sendrecv_mm_copy_internal(512);
    test_sendrecv_mm_copy_internal(1024);
    test_sendrecv_mm_copy_internal(2048);

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_sendrecv_mm_copy_bidir()
{
    test_sendrecv_mm_copy_bidir_internal(64);
    test_sendrecv_mm_copy_bidir_internal(128);
    test_sendrecv_mm_copy_bidir_internal(256);
    test_sendrecv_mm_copy_bidir_internal(512);
    test_sendrecv_mm_copy_bidir_internal(1024);
    test_sendrecv_mm_copy_bidir_internal(2048);

    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_sendrecv_mm_copy);
    UNITTEST(test_sendrecv_mm_copy_bidir);
    UNITTEST(test_sendrecv_mm_4gpus);
    return 0;
}

```

`ark/ops/ops_sendrecv_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_kernel.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "logging.h"
#include "unittest/unittest_utils.h"

using namespace std;

void test_sendrecv_internal()
{
    for (int gpu_id = 0; gpu_id < 2; ++gpu_id) {
        ark::unittest::spawn_process([gpu_id]() {
            //
            ark::Model model{gpu_id};
            ark::Tensor *tns_x = model.tensor({1024}, ark::FP16);
            if (gpu_id == 0) {
                model.send(tns_x, 0, 1, 1024);
                model.send_done(tns_x, 0, 1);
            }
            if (gpu_id == 1) {
                model.recv(tns_x, 0, 0);
            }

            ark::Executor exe{gpu_id, gpu_id, 2, model, "test_sendrecv"};
            exe.compile();

            exe.launch();
            exe.run(1);
            exe.stop();
            return ark::unittest::SUCCESS;
        });
    }

    ark::unittest::wait_all_processes();
}

ark::unittest::State test_sendrecv()
{
    test_sendrecv_internal();
    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_sendrecv);
    return ark::unittest::SUCCESS;
}

```

`ark/ops/ops_sharding.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "math.h"
#include "model.h"
#include <cassert>

namespace ark {

// Shard `input` along `axis` into `dim_per_shard`-dimensional shards.
std::vector<Tensor *> Model::sharding(Tensor *input, DimType axis,
                                      DimType dim_per_shard,
                                      const std::string &name)
{
    assert(input != nullptr);
    LOG(DEBUG, "sharding ", input->shape, " ", axis, " ", dim_per_shard);
    if (axis >= DIMS_LEN) {
        LOGERR("invlaid axis value: ", axis);
    }
    if ((input->shape[axis] % dim_per_shard) != 0) {
        // If the total dimension is not divided by the per-shard size,
        // we need to check whether we can put a padding here.
        // If the padded dimension of the input tensor is smaller than
        // the leading dimension size, it means that the input tensor refers to
        // a part of a buffer -- in this case, we cannot put a padding because
        // the tensor has adjacent data.
        DimType pdim = math::pad(input->shape[axis], input->pads[axis]);
        if (pdim < input->ldims[axis]) {
            LOGERR("the dimension of axis ", axis, " (", input->shape[axis],
                   ") is not divided by the dimension per shard (",
                   dim_per_shard, ") and this tensor cannot be padded.");
        }
    }
    std::vector<Tensor *> shards;
    DimType num_shard = math::div_up(input->shape[axis], dim_per_shard);
    Dims shard_shape = input->shape;
    Dims shard_offs = input->offs;
    Dims shard_pads = input->pads;
    for (DimType i = 0; i < num_shard; ++i) {
        DimType dim;
        if (i == (num_shard - 1)) {
            dim = input->shape[axis] - (i * dim_per_shard);
            shard_pads[axis] = input->pads[axis];
        } else {
            dim = dim_per_shard;
            shard_pads[axis] = 1;
        }
        shard_shape[axis] = dim;
        Tensor *shard =
            this->identity(this->tensor(shard_shape, input->type, input->buf,
                                        input->ldims, shard_offs, shard_pads),
                           {input}, name + "/shard_" + std::to_string(i));
        shards.emplace_back(shard);
        shard_offs[axis] += dim;
    }
    return shards;
}

} // namespace ark

```

`ark/ops/ops_softmax.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include "tensor.h"
#include <cassert>

namespace ark {

extern const OpConfigMap SoftmaxConfigMap;

SoftmaxOp::SoftmaxOp(OpPrecType prec_type, Tensor *input, Tensor *output,
                     const std::string &name)
    : Op{OP_SOFTMAX, prec_type, {input},           {output},
         {},         name,      &SoftmaxConfigMap, -1}
{
}

std::string SoftmaxOp::function_name(const OpConfig &cfg) const
{
    Tensor *input = this->inputs[0];
    Tensor *output = this->outputs[0];

    Dims shp_out = output->shape;
    int ndims = shp_out.ndims();
    CHECK(ndims < 4);

    const OpTile &tile_out = cfg.output_tiles[0];
    Dims unit_out_dims{1, 1, tile_out.x, tile_out.y};

    return Op::function_name("ark::softmax",
                             {{
                                 input->ldims.dims4(),  // InDims
                                 input->shape.dims4(),  // InShape
                                 output->ldims.dims4(), // OutDims
                                 output->shape.dims4(), // OutShape
                                 unit_out_dims,         // UnitOutDims
                                 cfg.num_warps * 32,    // NumThreads
                                 cfg.smem_bytes,        // SmemBytes
                             }});
}

Tensor *Model::softmax(Tensor *input, Tensor *output, const std::string &name)
{
    assert(input != nullptr);
    LOG(DEBUG, "softmax ", input->shape, " ", input->ldims, " ");
    OpPrecType pt;
    if (input->type == FP16) {
        pt = OP_PREC_FP16;
    } else if (input->type == FP32) {
        pt = OP_PREC_FP32;
    } else {
        LOGERR("unsupported input data type: ", type_str(input->type));
    }
    if (output != nullptr && input->type != output->type) {
        LOGERR("invalid output data type: ", type_str(output->type));
    }
    if (output == nullptr) {
        output = this->tensor(input->shape, input->type);
    } else if (output == input) {
        output = this->identity(output);
    }
    SoftmaxOp op{pt, input, output, name};
    return this->impl->add_op(op)[0];
}

const OpConfigMap SoftmaxConfigMap = {
    {{OP_ARCH_CUDA_70, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {1, 128, {{32, -1}}, {{32, -1}}, true, false},
         {1, 128, {{16, -1}}, {{16, -1}}, true, false},
         {1, 128, {{8, -1}}, {{8, -1}}, true, false},
         {1, 128, {{4, -1}}, {{4, -1}}, true, false},
         {1, 128, {{2, -1}}, {{2, -1}}, true, false},
         {1, 128, {{1, -1}}, {{1, -1}}, true, false},
         {4, 128, {{1, -1}}, {{1, -1}}, true, false},
         {8, 128, {{1, -1}}, {{1, -1}}, true, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {1, 128, {{32, -1}}, {{32, -1}}, true, false},
         {1, 128, {{16, -1}}, {{16, -1}}, true, false},
         {1, 128, {{8, -1}}, {{8, -1}}, true, false},
         {1, 128, {{4, -1}}, {{4, -1}}, true, false},
         {1, 128, {{2, -1}}, {{2, -1}}, true, false},
         {1, 128, {{1, -1}}, {{1, -1}}, true, false},
         {4, 128, {{1, -1}}, {{1, -1}}, true, false},
         {8, 128, {{1, -1}}, {{1, -1}}, true, false},
     }},
    {{OP_ARCH_CUDA_70, OP_PREC_FP32},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {1, 128, {{32, -1}}, {{32, -1}}, true, false},
         {1, 128, {{16, -1}}, {{16, -1}}, true, false},
         {1, 128, {{8, -1}}, {{8, -1}}, true, false},
         {1, 128, {{4, -1}}, {{4, -1}}, true, false},
         {1, 128, {{2, -1}}, {{2, -1}}, true, false},
         {1, 128, {{1, -1}}, {{1, -1}}, true, false},
         {4, 128, {{1, -1}}, {{1, -1}}, true, false},
         {8, 128, {{1, -1}}, {{1, -1}}, true, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_FP32},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {1, 128, {{32, -1}}, {{32, -1}}, true, false},
         {1, 128, {{16, -1}}, {{16, -1}}, true, false},
         {1, 128, {{8, -1}}, {{8, -1}}, true, false},
         {1, 128, {{4, -1}}, {{4, -1}}, true, false},
         {1, 128, {{2, -1}}, {{2, -1}}, true, false},
         {1, 128, {{1, -1}}, {{1, -1}}, true, false},
         {4, 128, {{1, -1}}, {{1, -1}}, true, false},
         {8, 128, {{1, -1}}, {{1, -1}}, true, false},
     }},
};

} // namespace ark

```

`ark/ops/ops_tensor.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include "tensor.h"
#include <cassert>

namespace ark {

TensorOp::TensorOp(const std::vector<Tensor *> &deps, Tensor *output,
                   const std::string &name)
    : Op{OP_TENSOR, OP_PREC_NONE, deps, {output}, {}, name, nullptr, -1}
{
}

Tensor *Model::tensor(const Dims &shape, TensorType type, TensorBuf *buf,
                      const Dims &ldims, const Dims &offs, const Dims &pads,
                      const std::vector<Tensor *> &deps, bool exported,
                      int imported_rank, const std::string &name)
{
    LOG(DEBUG, "tensor ", name, " ", shape, " ", type, " ", ldims, " ", offs,
        " ", pads);
    if (buf == nullptr) {
        buf = this->impl->create_tensor_buf();
    }
    Tensor *ret =
        new Tensor{shape,    type,          buf,
                   ldims,    offs,          pads,
                   exported, imported_rank, (int)this->impl->tns_storage.size(),
                   name};
    assert(ret != nullptr);
    this->impl->tns_storage.emplace_back(ret);
    std::set<Tensor *> dep_set;
    for (auto &dep : deps) {
        dep_set.emplace(dep);
    }
    std::vector<Tensor *> dep_vec;
    for (auto &dep : dep_set) {
        dep_vec.emplace_back(dep);
    }
    TensorOp op{dep_vec, ret, name};
    return this->impl->add_op(op)[0];
}

} // namespace ark

```

`ark/ops/ops_tensor_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_buf.h"
#include "gpu/gpu_mgr.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "unittest/unittest_utils.h"
#include <cstring>

using namespace std;

ark::unittest::State test_tensor_memcpy()
{
    ark::Model model;
    ark::Dims shape{2, 3, 4, 5};
    ark::Dims ldims{9, 8, 7, 6};
    ark::Dims offs{4, 3, 2, 1};
    ark::Tensor *tns = model.tensor(shape, ark::FP32, nullptr, ldims, offs);

    // Create an executor
    ark::Executor exe{0, 0, 1, model, "test_tensor_memcpy"};
    exe.compile();

    ark::GpuBuf *buf = exe.get_gpu_buf(tns);
    UNITTEST_NE(buf, (ark::GpuBuf *)nullptr);
    UNITTEST_EQ(buf->get_bytes(), ldims.size() * sizeof(float));

    // Fill tensor data: {1.0, 2.0, 3.0, ..., 3024.0}
    auto data = ark::utils::range_floats(ldims.size());
    ark::gpu_memcpy(buf, data.get(), ldims.size() * sizeof(float));

    // Copy tensor data from GPU to CPU
    float *res = (float *)malloc(shape.size() * sizeof(float));
    UNITTEST_NE(res, (float *)nullptr);
    memset(res, 0, shape.size() * sizeof(float));
    exe.tensor_memcpy(res, tns, shape.size() * sizeof(float));

    // Validate
    int idx = 0;
    for (int i = 0; i < shape[0]; ++i) {
        for (int j = 0; j < shape[1]; ++j) {
            for (int k = 0; k < shape[2]; ++k) {
                for (int l = 0; l < shape[3]; ++l) {
                    float truth =
                        (i + offs[0]) * (ldims[1] * ldims[2] * ldims[3]) +
                        (j + offs[1]) * (ldims[2] * ldims[3]) +
                        (k + offs[2]) * (ldims[3]) + (l + offs[3]) + 1;
                    UNITTEST_EQ(res[idx], truth);
                    res[idx] *= 2;
                    ++idx;
                }
            }
        }
    }

    // Copy tensor data from CPU to GPU
    exe.tensor_memcpy(tns, res, shape.size() * sizeof(float));

    // Copy all data from GPU to CPU
    float *res2 = (float *)malloc(ldims.size() * sizeof(float));
    UNITTEST_NE(res2, (float *)nullptr);
    ark::gpu_memcpy(res2, buf, ldims.size() * sizeof(float));

    // Validate
    idx = 0;
    for (int i = 0; i < ldims[0]; ++i) {
        for (int j = 0; j < ldims[1]; ++j) {
            for (int k = 0; k < ldims[2]; ++k) {
                for (int l = 0; l < ldims[3]; ++l) {
                    float val = ldims[1] * ldims[2] * ldims[3] * i +
                                ldims[2] * ldims[3] * j + ldims[3] * k + l + 1;
                    if (i >= offs[0] && i < offs[0] + shape[0] &&
                        j >= offs[1] && j < offs[1] + shape[1] &&
                        k >= offs[2] && k < offs[2] + shape[2] &&
                        l >= offs[3] && l < offs[3] + shape[3]) {
                        val *= 2;
                    }
                    UNITTEST_EQ(res2[idx], val);
                    idx++;
                }
            }
        }
    }

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_tensor_layout()
{
    ark::Model model;
    // float buf[2][3][4][5];
    ark::Tensor *tns =
        model.tensor({2, 3, 4, 5}, ark::FP32, nullptr, {8, 7, 6, 5});
    // Refer to each value
    ark::Tensor *ref[2][3][4][5];
    for (int i = 0; i < 2; ++i) {
        for (int j = 0; j < 3; ++j) {
            for (int k = 0; k < 4; ++k) {
                for (int l = 0; l < 5; ++l) {
                    ref[i][j][k][l] =
                        model.tensor({1, 1, 1, 1}, ark::FP32, tns->buf,
                                     tns->ldims, {i, j, k, l}, {});
                }
            }
        }
    }

    // Create an executor
    ark::Executor exe{0, 0, 1, model, "test_tensor_layout"};
    exe.compile();

    ark::GpuBuf *buf = exe.get_gpu_buf(tns);
    UNITTEST_NE(buf, (ark::GpuBuf *)nullptr);
    UNITTEST_EQ(buf->get_bytes(), 8 * 7 * 6 * 5 * sizeof(float));

    // Fill tensor data: {1.0, 2.0, 3.0, ..., 120.0}
    auto data = ark::utils::range_floats(2 * 3 * 4 * 5);
    exe.tensor_memcpy(tns, data.get(), 2 * 3 * 4 * 5 * sizeof(float));

    // Check reference values
    float ref_val;
    float truth = 1.0f;
    for (int i = 0; i < 2; ++i) {
        for (int j = 0; j < 3; ++j) {
            for (int k = 0; k < 4; ++k) {
                for (int l = 0; l < 5; ++l) {
                    exe.tensor_memcpy(&ref_val, ref[i][j][k][l], sizeof(float));
                    UNITTEST_EQ(ref_val, truth);
                    truth += 1.0f;
                }
            }
        }
    }

    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_tensor_memcpy);
    UNITTEST(test_tensor_layout);
    return ark::unittest::SUCCESS;
}

```

`ark/ops/ops_test_common.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "ops_test_common.h"
#include "gpu/gpu_kernel.h"
#include "include/ark_utils.h"
#include "logging.h"
#include "unittest/unittest_utils.h"

using namespace std;

// op_name: "add", "mul"
template <typename T>
void test_bcast_internal(string op_name, ark::TensorType type, ark::DimType bs,
                         ark::DimType n, ark::DimType m, bool overwrite)
{
    string type_name;
    if (type == ark::FP32) {
        type_name = "fp32";
    } else if (type == ark::FP16) {
        type_name = "fp16";
    } else {
        UNITTEST_FEXIT("Unsupported tensor type:", type);
    }
    string kernel_name = "simple_" + op_name + "_" + type_name;

    ark::GpuMgr *mgr = ark::get_gpu_mgr(0);
    ark::GpuMgrCtx *ctx = mgr->create_context("test_simple_" + op_name, 0, 1);

    ark::DimType len = m * n;
    ark::GpuBuf *buf_a = ctx->mem_alloc(bs * len * sizeof(T));
    ark::GpuBuf *buf_b = ctx->mem_alloc(len * sizeof(T));
    ark::GpuBuf *buf_c = ctx->mem_alloc(bs * len * sizeof(T));

    ctx->freeze();

    ark::GpuKernel gk{kernel_name,
                      {ark::unittest::get_kernel_code("simple_" + op_name)},
                      {(unsigned int)mgr->get_gpu_info().num_sm, 1, 1},
                      {512, 1, 1},
                      0,
                      {buf_c, buf_a, buf_b},
                      {},
                      {
                          {&bs, sizeof(bs)},
                          {&len, sizeof(len)},
                      },
                      ""};
    gk.compile(mgr->get_gpu_info());
    gk.load();

    // Set data.
    ark::srand();
    auto data_a = ark::utils::rand_array<T>(bs * len, 0.01);
    auto data_b = ark::utils::rand_array<T>(len, 0.01);
    ark::gpu_memcpy(buf_a, data_a.get(), bs * len * sizeof(T));
    ark::gpu_memcpy(buf_b, data_b.get(), len * sizeof(T));

    // Run the GPU kernel.
    ark::GpuStream s = ctx->create_stream();
    int ret = gk.launch(s);
    UNITTEST_EQ(ret, 0);
    ret = ctx->sync_stream(s);
    UNITTEST_EQ(ret, 0);

    // Copy the ground truth results into CPU memory.
    T *gt = (T *)malloc(bs * len * sizeof(T));
    UNITTEST_NE(gt, (T *)nullptr);
    ark::gpu_memcpy(gt, buf_c, bs * len * sizeof(T));

    mgr->destroy_context(ctx);

    //
    ark::Model model;
    ark::Tensor *tns_a = model.tensor({bs, n, m}, type);
    ark::Tensor *tns_b = model.tensor({1, n, m}, type);
    ark::Tensor *tns_c = nullptr;
    if (op_name == "add") {
        if (overwrite) {
            tns_c = model.add(tns_a, tns_b, tns_a);
        } else {
            tns_c = model.add(tns_a, tns_b);
        }
    } else if (op_name == "mul") {
        if (overwrite) {
            tns_c = model.mul(tns_a, tns_b, tns_a);
        } else {
            tns_c = model.mul(tns_a, tns_b);
        }
    }
    UNITTEST_NE(tns_c, (ark::Tensor *)nullptr);

    //
    ark::Executor exe{0, 0, 1, model, "test_" + op_name + "_" + type_name};
    exe.compile();

    // Set data.
    exe.tensor_memcpy(tns_a, data_a.get(), bs * len * sizeof(T));
    exe.tensor_memcpy(tns_b, data_b.get(), len * sizeof(T));

    exe.launch();
    exe.run(1);
    exe.stop();

    // Copy results of the loop kernel routine into CPU memory.
    T *res = (T *)malloc(bs * len * sizeof(T));
    UNITTEST_NE(res, (T *)nullptr);
    exe.tensor_memcpy(res, tns_c, bs * len * sizeof(T));

    // Compare results with the ground truth.
    std::pair<float, float> p =
        ark::utils::tensor_compare(gt, res, tns_c->shape, true);
    float max_err = p.second;

    if (overwrite) {
        exe.tensor_memcpy(res, tns_a, bs * len * sizeof(T));
        p = ark::utils::tensor_compare(gt, res, tns_a->shape, true);
        max_err = std::max(max_err, p.second);
    }

    LOG(ark::INFO, op_name, ":", n, 'x', m, ",", type_name, ",bs=", bs,
        ",overwrite=", overwrite, setprecision(4), " mse ", p.first,
        " max_err ", max_err * 100, "%");

    free(res);
    free(gt);

    UNITTEST_EQ(max_err, 0.0);
}

void test_bcast_fp32(string op_name, ark::DimType bs, ark::DimType n,
                     ark::DimType m, bool overwrite)
{
    test_bcast_internal<float>(op_name, ark::FP32, bs, n, m, overwrite);
}

void test_bcast_fp16(string op_name, ark::DimType bs, ark::DimType n,
                     ark::DimType m, bool overwrite)
{
    test_bcast_internal<ark::half_t>(op_name, ark::FP16, bs, n, m, overwrite);
}

```

`ark/ops/ops_test_common.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_OPS_TEST_COMMON_H_
#define ARK_OPS_TEST_COMMON_H_

#include "include/ark.h"
#include <string>

void test_bcast_fp32(std::string op_name, ark::DimType bs, ark::DimType n,
                     ark::DimType m, bool overwrite = false);
void test_bcast_fp16(std::string op_name, ark::DimType bs, ark::DimType n,
                     ark::DimType m, bool overwrite = false);

#endif // ARK_OPS_TEST_COMMON_H_

```

`ark/ops/ops_transpose.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "logging.h"
#include "model.h"
#include "tensor.h"
#include <cassert>

namespace ark {

extern const OpConfigMap TransposeConfigMap;

TransposeOp::TransposeOp(OpPrecType prec_type, Tensor *input, Tensor *output,
                         int tp_type, const std::string &name)
    : Op{OP_TRANSPOSE, prec_type,           {input}, {output}, {{tp_type}},
         name,         &TransposeConfigMap, -1}
{
}

std::string TransposeOp::function_name(const OpConfig &cfg) const
{
    int tp_type;
    this->args.get(&tp_type, 0);

    std::string tp_type_str = std::to_string(tp_type);
    if (tp_type_str.size() == DIMS_LEN - 1) {
        tp_type_str = "0" + tp_type_str;
    }
    if (tp_type_str.size() != DIMS_LEN) {
        LOGERR("Unexpected error");
    }

    Tensor *input = this->inputs[0];
    Tensor *output = this->outputs[0];
    const OpTile &tile_out = cfg.output_tiles[0];
    Dims unit_out_dims{1, 1, tile_out.x, tile_out.y};

    return Op::function_name("ark::transpose" + tp_type_str,
                             {{
                                 input->ldims.dims4(),  // InDims
                                 output->ldims.dims4(), // OutDims
                                 output->shape.dims4(), // OutShape
                                 unit_out_dims,         // UnitOutDims
                                 cfg.num_warps * 32,    // NumThreads
                                 cfg.smem_bytes,        // SmemBytes
                             }});
}

Tensor *Model::transpose(Tensor *input, Dims perm, Tensor *output,
                         const std::string &name)
{
    OpPrecType pt;
    if (input->type == FP16) {
        pt = OP_PREC_FP16;
    } else if (input->type == FP32) {
        pt = OP_PREC_FP32;
    } else {
        LOGERR("unsupported input data type: ", type_str(input->type));
    }
    int input_ndims = input->ndims();
    Dims in_shape{1, 1, 1, 1};
    if (input_ndims < 2 || input_ndims > 4) {
        LOGERR("Invalid # of input dimensions. Expected 2, 3, or 4, but given ",
               input_ndims);
    }
    for (int i = 0; i < input_ndims; ++i) {
        in_shape[4 - input_ndims + i] = input->shape[i];
    }
    if (perm.ndims() != input_ndims) {
        LOGERR("Permutation should have the same number of dimensions as the "
               "one of input. Given input shape: ",
               input->shape, ", permutation: ", perm);
    }
    int count[DIMS_LEN];
    for (int i = 0; i < input_ndims; ++i) {
        count[i] = 0;
    }
    for (int i = 0; i < input_ndims; ++i) {
        if (perm[i] >= input_ndims) {
            LOGERR("Each value in permutation should be less than the number "
                   "of input dimensions. Given permutation: ",
                   perm);
        }
        if (count[perm[i]] > 0) {
            LOGERR("Each value in permutation should be unique. Given "
                   "permutation: ",
                   perm);
        }
        count[perm[i]]++;
    }
    int tp_type = perm[0] * 1000 + perm[1] * 100 + perm[2] * 10 + perm[3];
    Dims out_shape{in_shape[perm[0]], in_shape[perm[1]], in_shape[perm[2]],
                   in_shape[perm[3]]};
    if (output == nullptr) {
        output = this->tensor(out_shape, input->type);
    } else {
        assert(output->shape == out_shape);
    }
    TransposeOp op{pt, input, output, tp_type, name};
    return this->impl->add_op(op)[0];
}

const OpConfigMap TransposeConfigMap = {
    {{OP_ARCH_CUDA_70, OP_PREC_FP32},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {8, 0, {{1, 1}}, {{128, 128}}, true, false},
         {4, 0, {{1, 1}}, {{64, 128}}, true, false},
         {4, 0, {{1, 1}}, {{128, 64}}, true, false},
         {4, 0, {{1, 1}}, {{64, 64}}, true, false},
         {2, 0, {{1, 1}}, {{32, 32}}, true, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_FP32},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {8, 0, {{1, 1}}, {{128, 128}}, true, false},
         {4, 0, {{1, 1}}, {{64, 128}}, true, false},
         {4, 0, {{1, 1}}, {{128, 64}}, true, false},
         {4, 0, {{1, 1}}, {{64, 64}}, true, false},
         {2, 0, {{1, 1}}, {{32, 32}}, true, false},
     }},
    {{OP_ARCH_CUDA_70, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {8, 0, {{1, 1}}, {{128, 128}}, true, false},
         {4, 0, {{1, 1}}, {{64, 128}}, true, false},
         {4, 0, {{1, 1}}, {{128, 64}}, true, false},
         {4, 0, {{1, 1}}, {{64, 64}}, true, false},
         {2, 0, {{1, 1}}, {{32, 32}}, true, false},
         {1, 0, {{1, 1}}, {{16, 16}}, true, false},
         {1, 0, {{1, 1}}, {{8, 16}}, true, false},
     }},
    {{OP_ARCH_CUDA_80, OP_PREC_FP16},
     {
         // NumWarps, SmemBytes, InDepsTiles, OutDepsTiles, SyncPre, SyncPost
         {8, 0, {{1, 1}}, {{128, 128}}, true, false},
         {4, 0, {{1, 1}}, {{64, 128}}, true, false},
         {4, 0, {{1, 1}}, {{128, 64}}, true, false},
         {4, 0, {{1, 1}}, {{64, 64}}, true, false},
         {2, 0, {{1, 1}}, {{32, 32}}, true, false},
         {1, 0, {{1, 1}}, {{16, 16}}, true, false},
         {1, 0, {{1, 1}}, {{8, 16}}, true, false},
         {1, 0, {{1, 1}}, {{4, 8}}, true, false},
     }},
};

} // namespace ark

```

`ark/ops/ops_transpose_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_kernel.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "logging.h"
#include "unittest/unittest_utils.h"

template <typename T>
void test_transpose_internal(ark::TensorType type, ark::DimType n,
                             ark::DimType c, ark::DimType h, ark::DimType w,
                             ark::DimType pn, ark::DimType pc, ark::DimType ph,
                             ark::DimType pw)
{
    ark::Model model;
    ark::Tensor *tns_in = model.tensor({n, c, h, w}, type);
    ark::Tensor *tns_out = model.transpose(tns_in, {pn, pc, ph, pw});

    ark::Executor exe{/*gpu_id=*/0, /*rank=*/0, /*world_size=*/1, model,
                      "test_transpose"};
    exe.compile();

    // Set data.
    ark::srand();
    auto data_in = ark::utils::rand_array<T>(n * c * h * w, 0.01);
    T *in_ptr = data_in.get();

    exe.tensor_memcpy(tns_in, in_ptr, n * c * h * w * sizeof(T));

    exe.launch();
    exe.run(1);
    exe.stop();

    // Copy results of the loop kernel routine into CPU memory.
    T *res = (T *)malloc(n * c * h * w * sizeof(T));
    UNITTEST_NE(res, (T *)nullptr);
    exe.tensor_memcpy(res, tns_out, n * c * h * w * sizeof(T));

    // int on = tns_in->shape[pn];
    ark::DimType oc = tns_in->shape[pc];
    ark::DimType oh = tns_in->shape[ph];
    ark::DimType ow = tns_in->shape[pw];

    // Check results.
    for (ark::DimType i = 0; i < n; ++i) {
        for (ark::DimType j = 0; j < c; ++j) {
            for (ark::DimType k = 0; k < h; ++k) {
                for (ark::DimType l = 0; l < w; ++l) {
                    ark::Dims axis{i, j, k, l};
                    ark::Dims new_axis{axis[pn], axis[pc], axis[ph], axis[pw]};
                    ark::DimType in_idx = i * c * h * w + j * h * w + k * w + l;
                    ark::DimType res_idx = new_axis[0] * oc * oh * ow +
                                           new_axis[1] * oh * ow +
                                           new_axis[2] * ow + new_axis[3];
                    UNITTEST_TRUE(in_idx < n * c * h * w);
                    UNITTEST_TRUE(res_idx < n * c * h * w);
                    UNITTEST_EQ(in_ptr[in_idx], res[res_idx]);
                }
            }
        }
    }
}

ark::unittest::State test_transpose_fp32()
{
    test_transpose_internal<float>(ark::FP32, 3, 2048, 96, 128, 0, 2, 1, 3);
    test_transpose_internal<ark::half_t>(ark::FP16, 1, 1, 64, 32, 0, 1, 3, 2);

    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_transpose_fp32);
    return ark::unittest::SUCCESS;
}

```

`ark/random.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <ctime>
#include <random>
#include <sys/syscall.h>
#include <unistd.h>
#define gettid() syscall(SYS_gettid)

#include "include/ark.h"

namespace ark {

// Initialize the random number generator.
void srand(int seed)
{
    if (seed == -1) {
        ::srand(time(0) + getpid() + gettid());
    } else {
        ::srand(seed);
    }
}

// Generate a random integer.
int rand()
{
    return ::rand();
}

} // namespace ark

```

`ark/sched/sched.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "sched/sched.h"
#include "logging.h"
#include "math.h"

using namespace std;

namespace ark {

BaseScheduler::BaseScheduler(Model &model, int gpu_id, int rank_,
                             int world_size_, int num_warps_per_sm_)
    : model{&model}, gpu_mgr{get_gpu_mgr(gpu_id)}, rank{rank_},
      world_size{world_size_}, num_warps_per_sm{num_warps_per_sm_}
{
    const GpuInfo &gpu_info = this->gpu_mgr->get_gpu_info();
    int max_warps_per_sm =
        (int)(gpu_info.max_threads_per_block / gpu_info.threads_per_warp);
    this->num_warps_per_sm = std::min(num_warps_per_sm_, max_warps_per_sm);
    this->codegen = std::make_unique<CodeGenerator>(this->buf_trans, gpu_info,
                                                    num_warps_per_sm_);
}

// create context on gpu for the model
GpuMgrCtx *BaseScheduler::create_context(const std::string &name)
{
    GpuMgrCtx *ctx =
        this->gpu_mgr->create_context(name, this->rank, this->world_size);
    for (BufInfo &bi : this->buf_infos) {
        GpuBuf *buf;
        if (bi.gpu_id == this->gpu_mgr->gpu_id) {
            auto search = this->buf_trans.find(bi.tbuf);
            if (search != this->buf_trans.end()) {
                // Already allocated.
                buf = search->second;
                if (bi.sid != -1) {
                    ctx->mem_export(this->buf_trans[bi.tbuf], bi.offset,
                                    bi.sid);
                }
            } else if (bi.sid == -1) {
                buf = ctx->mem_alloc(bi.bytes, 1);
            } else {
                // Align for RDMA performance.
                buf = ctx->mem_alloc(bi.bytes, 65536);
                ctx->mem_export(buf, bi.offset, bi.sid);
            }
        } else {
            buf = ctx->mem_import(bi.bytes, bi.sid, bi.gpu_id);
        }
        this->buf_trans[bi.tbuf] = buf;
    }
    for (auto &srop : this->send_recv_ops) {
        int sid;
        int remote_rank;
        size_t bytes;
        srop->args.get(&sid, 0);
        srop->args.get(&remote_rank, 2);
        srop->args.get(&bytes, 3);

        LOG(DEBUG, "reg_sendrecv: sid=", sid, " remote=", remote_rank,
            " bytes=", bytes, " is_recv=", srop->type == OP_RECV);
        ctx->reg_sendrecv(sid, remote_rank, bytes, srop->type == OP_RECV);
    }
    ctx->freeze();
    this->ctx = ctx;
    return ctx;
}

const OpConfig *BaseScheduler::sched_op_config(const Op *op)
{
    if (op == nullptr || op->outputs.size() == 0) {
        LOG(ERROR, "unexpected error");
    }
    Tensor *output = op->outputs[0];
    if (output == nullptr || op->cfg_map == nullptr) {
        return nullptr;
    }
    const GpuInfo &gpu_info = this->gpu_mgr->get_gpu_info();
    OpArchType arch_type;
    if (gpu_info.arch == GPU_ARCH_CUDA_70) {
        arch_type = OP_ARCH_CUDA_70;
    } else if (gpu_info.arch == GPU_ARCH_CUDA_80) {
        arch_type = OP_ARCH_CUDA_80;
    } else {
        LOGERR("unsupported GPU architecture: ", gpu_info.arch);
    }
    auto search = op->cfg_map->find({arch_type, op->prec_type});
    if (search == op->cfg_map->end()) {
        return nullptr;
    } else if (op->gran_lev >= 0) {
        if (search->second.size() > (unsigned int)op->gran_lev) {
            return &search->second[op->gran_lev];
        }
        LOGERR("invalid granularity level: ", op->gran_lev);
    }
    std::vector<const OpConfig *> feasible_configs;
    for (auto &cfg : search->second) {
        if (cfg.num_warps <= this->num_warps_per_sm) {
            feasible_configs.push_back(&cfg);
        }
    }
    // Heuristic auto-selection of granularity level
    int gran_lev = 0;
    int ndims = output->shape.ndims();
    unsigned int min_wps =
        gpu_info.min_threads_per_block / gpu_info.threads_per_warp;
    for (auto &cfg : feasible_configs) {
        assert(cfg->output_tiles.size() > 0);
        const OpTile &ot = cfg->output_tiles[0];
        DimType ot_x = (ot.x == -1) ? output->ldims[ndims - 2] : ot.x;
        DimType ot_y = (ot.y == -1) ? output->ldims[ndims - 1] : ot.y;
        DimType num_tiles;
        DimType dim_0;
        DimType dim_1;
        if (ndims == 1) {
            if (ot_x != 1) {
                ++gran_lev;
                continue;
            }
            dim_0 = output->shape[0];
            dim_1 = 1;
            num_tiles = math::div_up(dim_0, ot_y);
        } else {
            num_tiles = 1;
            for (int i = 0; i < ndims - 2; ++i) {
                num_tiles *= output->shape[i];
            }
            dim_0 = output->shape[ndims - 1];
            dim_1 = output->shape[ndims - 2];
            num_tiles *= math::div_up(dim_0, ot_y);
            num_tiles *= math::div_up(dim_1, ot_x);
        }
        if (gran_lev == (int)feasible_configs.size() - 1) {
            // no more option, just use the finest-grained config
            break;
        }
        // magic condition
        if ((dim_0 * 2 > ot_y) && (dim_1 * 2 > ot_x) &&
            ((num_tiles * cfg->num_warps) >= (min_wps * gpu_info.num_sm / 2))) {
            break;
        }
        ++gran_lev;
    }
    if (gran_lev == (int)feasible_configs.size()) {
        stringstream configs_str;
        if (feasible_configs.size() > 0) {
            const OpTile &ot = feasible_configs[0]->output_tiles[0];
            DimType ot_x = (ot.x == -1) ? output->ldims[ndims - 2] : ot.x;
            DimType ot_y = (ot.y == -1) ? output->ldims[ndims - 1] : ot.y;
            configs_str << "{ " << ot_x << ", " << ot_y << " }";
        }
        for (int i = 1; i < (int)feasible_configs.size(); ++i) {
            const OpTile &ot = feasible_configs[i]->output_tiles[0];
            DimType ot_x = (ot.x == -1) ? output->ldims[ndims - 2] : ot.x;
            DimType ot_y = (ot.y == -1) ? output->ldims[ndims - 1] : ot.y;
            configs_str << ", { " << ot_x << ", " << ot_y << " }";
        }
        configs_str << ".";
        LOGERR("no valid tile configuration found. Output shape ",
               output->shape, ", available tiles: ", configs_str.str());
    }
    const OpConfig *cfg = feasible_configs[gran_lev];
    OpConfig *cfg_new = new OpConfig(*cfg);
    OpTile &op_tile = cfg_new->output_tiles[0];
    if (op_tile.x == -1) {
        op_tile.x = output->ldims[ndims - 2];
    }
    if (op_tile.y == -1) {
        op_tile.y = output->ldims[ndims - 1];
    }
    return cfg_new;
}

GpuBuf *BaseScheduler::get_gpu_buf(Tensor *tns) const
{
    if (tns == nullptr) {
        return nullptr;
    }
    if (tns->buf == nullptr) {
        return nullptr;
    }
    auto search = this->buf_trans.find(tns->buf);
    if (search == this->buf_trans.end()) {
        return nullptr;
    }
    return search->second;
}

} // namespace ark
```

`ark/sched/sched.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_SCHED_H_
#define ARK_SCHED_H_

#include "gpu/gpu_kernel.h"
#include "gpu/gpu_mgr.h"
#include "include/ark.h"
#include "sched/sched_codegen.h"
#include "sched/sched_opgraph.h"
#include "sched/sched_profiler.h"
#include "sched/sched_stream.h"

namespace ark {

struct BufInfo
{
    // all the information of a GPU data buffer
    BufInfo(int gpu_id_, size_t bytes_, TensorBuf *tbuf_, int sid_,
            size_t offset_)
        : gpu_id{gpu_id_}, bytes{bytes_}, tbuf{tbuf_}, sid{sid_}, offset{
                                                                      offset_}
    {
    }
    // gpu_id: the id of the GPU where the buffer is allocated. If the
    // gpu_id is the same as this rank's gpu_id, the buffer is allocated on
    // the GPU, otherwise it will be imported from another GPU.
    int gpu_id;
    size_t bytes;
    TensorBuf *tbuf;
    // sid: a unique id of the buffer, used to identify the buffer when
    // we need to export or import the buffer. If the TensorBuf is located
    // on this GPU and the sid is not -1, it will be exported.
    int sid;
    size_t offset;
};

class BaseScheduler
{
  public:
    BaseScheduler(Model &model, int gpu_id, int rank_, int world_size_,
                  int num_warps_per_sm_ = 16);

    // create context on gpu for the model
    GpuMgrCtx *create_context(const std::string &name);

    const OpConfig *sched_op_config(const Op *op);

    GpuBuf *get_gpu_buf(Tensor *tns) const;

    virtual void schedule() = 0;

    //
    virtual std::vector<std::string> gen_code() = 0;

  protected:
    Model *model;
    GpuMgr *gpu_mgr;
    int rank;
    int world_size;
    int num_warps_per_sm;
    std::unique_ptr<CodeGenerator> codegen;

    std::vector<std::unique_ptr<SchedOpSeq>> opseqs;

    // the information of the GPU buffers
    std::vector<BufInfo> buf_infos;

    // map from TensorBuf to gpu buffer, the TensorBuf is an abstract of the
    // data buffer in the model layer, and the GpuBuf is the real buffer in the
    // GPU address space
    std::map<TensorBuf *, GpuBuf *> buf_trans;

    std::vector<const Op *> send_recv_ops;

    GpuMgrCtx *ctx;
};

class SimpleScheduler : public BaseScheduler
{
  public:
    SimpleScheduler(Model &model, int gpu_id, int rank, int world_size,
                    int num_warps_per_sm = 16);

    void schedule();

    std::vector<std::string> gen_code();

  private:
    // This function is used to configure the TensorBuf. The TensorBuf is an
    // abstraction of the GPU memory, it correspond to a memory region on the
    // _ARK_BUF. This function will configure the allocation, import and export
    // of the TensorBuf and stores the TensorBuf information in buf_infos for
    // later use
    void configure_gpu_buf(const std::list<Tensor *> &model_tensors);
    void schedule_sched_opseq(SchedOpSeq &sop, int max_wps, int max_sm_num,
                              std::vector<Sched> &scheds);

    std::vector<SchedOp> sched_ops;
};

class DefaultScheduler : public BaseScheduler
{
  public:
    DefaultScheduler(Model &model, int gpu_id, int rank_, int world_size_,
                     int num_warps_per_sm = 16);

    std::vector<std::string> gen_code();
    void schedule();

  protected:
    void configure_gpu_buf(const std::list<Tensor *> &model_tensors);
    void schedule_depth(std::vector<SchedOpSeq *> &depth,
                        std::vector<Sched> &scheds);
    void schedule_depth_comm(std::vector<SchedOpSeq *> &depth,
                             std::vector<Sched> &scheds);
    void heuristic_optimize_model(Model &model, Model::Impl *model_impl,
                                  const GpuInfo &gpu_info, int num_sm);
    void heuristic_optimize_matmul(Model &model, Model::Impl *model_impl,
                                   Op &matmul_op, const GpuInfo &gpu_info,
                                   int num_sm);

  private:
    void recursive_schedule(std::list<OpNode *> &nodes,
                            std::set<OpNode *> &seen_nodes);

    std::unique_ptr<OpGraph> op_graph;
    std::vector<std::unique_ptr<SchedStream>> comp_stream;
    std::vector<std::unique_ptr<SchedStream>> comm_stream;
};

class KahyparScheduler : public DefaultScheduler
{
  public:
    KahyparScheduler(const int gpu_id, int rank_, int world_size_,
                     const Model &model, unsigned int num_warps_per_sm = 16);
    std::vector<std::string> gen_code();

  private:
    std::vector<Sched> simplify_sched(std::vector<Sched> &original_scheds);

    int kahypar_schedule_depth(std::vector<SchedOpSeq *> &depth,
                               std::vector<Sched> &scheds);
    SchedProfiler profiler;
};

} // namespace ark

#endif // ARK_SCHED_H_

```

`ark/sched/sched/sched_default.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "env.h"
#include "logging.h"
#include "math.h"
#include "model.h"
#include "sched/sched.h"

using namespace std;

namespace ark {

/// Calculate the number of tiles for a given op and a tile.
/// @param op input op
/// @param tile input tile
/// @return number of tiles
static int calc_num_tiles(const Op &op, const OpTile &tile)
{
    if (op.outputs.size() == 0) {
        // This op has no output.
        return 0;
    }
    assert(op.outputs[0] != nullptr);
    auto &s = op.outputs[0]->shape;
    int ndims = s.ndims();
    if (ndims == 0) {
        // The output has no element.
        return 0;
    }
    // tile.y corresponds to the last dimension.
    int num_tiles = math::div_up(s[ndims - 1], tile.y);
    // tile.x corresponds to the second last dimension.
    if (ndims > 1) {
        num_tiles *= math::div_up(s[ndims - 2], tile.x);
    } else if (tile.x != 1) {
        LOGERR("The tile is 2D, but the output is 1D.");
    }
    // The remaining dimensions are not tiled.
    int remain_dims = ndims - 2;
    while (remain_dims > 0) {
        num_tiles *= s[remain_dims - 1];
        remain_dims--;
    }
    return num_tiles;
}

/// Heuristic matmul optimization. Overwrite the input matmul op with an
/// optimized op.
/// @param model target model
/// @param matmul_op matmul op in the target model
/// @param gpu_info GPU info to optimize for
/// @param num_sm number of SMs to use for this op. This should be equal to or
/// less than the number of SMs on the GPU (`gpu_info.num_sm`).
void DefaultScheduler::heuristic_optimize_matmul(Model &model,
                                                 Model::Impl *model_impl,
                                                 Op &matmul_op,
                                                 const GpuInfo &gpu_info,
                                                 int num_sm)
{
    if (matmul_op.type != OP_MATMUL) {
        LOGERR("This is not a matmul op.");
    }
    if (matmul_op.gran_lev != -1) {
        // `gran_lev` is manually set. Do not optimize.
        return;
    }
    if (num_sm > gpu_info.num_sm) {
        LOGERR("The total number of SMs (%d) is less than the number of SMs "
               "requested (%d).",
               gpu_info.num_sm, num_sm);
    }
    const OpConfig *cfg = this->sched_op_config(&matmul_op);
    assert(cfg->output_tiles.size() == 1);
    int num_tiles = calc_num_tiles(matmul_op, cfg->output_tiles[0]);
    if (num_tiles == 0) {
        LOGERR("This matmul has no output tiles.");
    }

    // Heuristically select a split_k value. If split_k is larger than 1, split
    // the inner dimension of the matmul into split_k parts, where each part is
    // computed by a separate matmul op and the results are accumulated. Larger
    // split_k is preferred when the number of tiles is small and the inner
    // dimension is large.
    int split_k = 1;
    if (num_tiles < num_sm) {
        // If the number of tiles is less than the number of SMs, we can
        // potentially use more SMs to compute the matmul. We can split the
        // inner dimension into multiple parts and distribute them to different
        // SMs. We use a heuristic to determine the number of parts.

        // Calculate the maximum possible split_k according to the tile shape.
        const Dims &fst_input_shape = matmul_op.inputs[0]->shape;
        const OpTile &fst_input_tile = cfg->input_tiles[0];
        DimType inner_dim = fst_input_shape[fst_input_shape.ndims() - 1];
        DimType inner_dim_tile_len = fst_input_tile.y;
        size_t max_split_k = math::div_up(inner_dim, inner_dim_tile_len);

        // Calculate the max split_k to run two or less tiles per SM. Exceeding
        // this limit is heuristically bad for performance.
        size_t split_k_for_two_tiles_per_sm = num_sm * 2 / num_tiles;
        size_t tmp_split_k = min(max_split_k, split_k_for_two_tiles_per_sm);

        // Calculate the actual split_k if we can split the inner dimension
        // into tmp_split_k parts.
        size_t each_part_len =
            math::pad(math::div_up(inner_dim, tmp_split_k), inner_dim_tile_len);
        split_k = math::div_up(inner_dim, each_part_len);
        assert(split_k > 0);
    }
    if (split_k == 1) {
        // No optimization is needed.
        return;
    }
    LOG(DEBUG, "Optimize matmul %s with split_k=%d.", matmul_op.name, split_k);

    Tensor *input_a = matmul_op.inputs[0];
    Tensor *input_b = matmul_op.inputs[1];
    Tensor *output = matmul_op.outputs[0];
    bool is_column_a;
    bool is_column_b;
    bool is_relu;
    matmul_op.args.get(&is_column_a, 4);
    matmul_op.args.get(&is_column_b, 5);
    matmul_op.args.get(&is_relu, 6);
    std::string matmul_name = matmul_op.name;

    // Remove the original matmul op from the model.
    model_impl->delete_op(&matmul_op);

    // Create a new matmul op with the optimized split_k.
    model.matmul(input_a, input_b, output, split_k, is_column_a, is_column_b,
                 is_relu, matmul_name);
}

/// Heuristically optimize the model. Overwrite the model with an optimized
/// model.
/// @param model target model
/// @param gpu_info GPU info to optimize for
/// @param num_sm number of SMs to use for this op. This should be equal to or
/// less than the number of SMs on the GPU (`gpu_info.num_sm`).
void DefaultScheduler::heuristic_optimize_model(Model &model,
                                                Model::Impl *model_impl,
                                                const GpuInfo &gpu_info,
                                                int num_sm)
{
    if (get_env().disable_graph_opt) {
        LOG(INFO, "Graph optimization is disabled.");
        return;
    }
    // Make a copy of the ops because we will modify the model.
    std::vector<Op *> ops;
    for (auto &op : model_impl->get_ops()) {
        ops.push_back(op);
    }
    for (auto &op : ops) {
        if (op->type == OP_MATMUL) {
            heuristic_optimize_matmul(model, model_impl, *op, gpu_info, num_sm);
        }
    }
}

DefaultScheduler::DefaultScheduler(Model &model, int gpu_id, int rank_,
                                   int world_size_, int num_warps_per_sm_)
    : BaseScheduler(model, gpu_id, rank_, world_size_, num_warps_per_sm_)
{
    const GpuInfo &gpu_info = this->gpu_mgr->get_gpu_info();

    // Number of SMs to use for computation. The last SM is preserved for
    // communication only.
    int num_sm_calc = gpu_info.num_sm - 1;

    heuristic_optimize_model(model, model.impl.get(), gpu_info, num_sm_calc);

    this->op_graph = make_unique<OpGraph>(model);
}

void DefaultScheduler::schedule()
{
    LOG(DEBUG, "DefaultScheduler start scheduling");

    auto &nodes = this->op_graph->get_nodes();

    std::list<OpNode *> root_nodes;
    for (auto &node : nodes) {
        if (node->producers.empty()) {
            root_nodes.emplace_back(node.get());
        }
    }

    std::set<OpNode *> seen_nodes;
    recursive_schedule(root_nodes, seen_nodes);

    this->configure_gpu_buf(this->model->impl->get_tensors());

    if (this->comp_stream.size() != this->comm_stream.size()) {
        LOG(ERROR, "unexpected error");
    }
}

///
void DefaultScheduler::recursive_schedule(std::list<OpNode *> &nodes,
                                          std::set<OpNode *> &seen_nodes)
{
    if (nodes.empty()) {
        return;
    }
    const GpuInfo &gpu_info = this->gpu_mgr->get_gpu_info();

    std::list<OpNode *> next_nodes;
    std::vector<SchedItem> comp_items;
    std::vector<SchedItem> comm_items;
    bool sync_comm = false;
    bool sync_comp = false;
    for (auto &node : nodes) {
        if (node->ops.size() == 0) {
            LOG(ERROR, "unexpected error: empty OpNode");
        }
        Op *op = node->ops[0];
        const OpConfig *cfg = this->sched_op_config(op);
        int opseq_id = (int)this->opseqs.size();
        this->opseqs.emplace_back(make_unique<SchedOpSeq>(opseq_id, op, cfg));
        SchedOpSeq *opseq = this->opseqs.back().get();

        bool broke_node = false;
        for (size_t i = 1; i < node->ops.size(); i++) {
            // If there are multiple Ops, check if the Op configs allow merging.
            Op *next_op = node->ops[i];
            const OpConfig *next_cfg = this->sched_op_config(next_op);
            bool need_sync_between_ops = cfg->sync_post || next_cfg->sync_pre;
            bool comm_and_comp = (op->is_comm() && !next_op->is_comm()) ||
                                 (!op->is_comm() && next_op->is_comm());
            if (!need_sync_between_ops && !comm_and_comp) {
                if (opseq->append(next_op, next_cfg)) {
                    // Merge succeeded.
                    continue;
                }
            }
            // Cannot merge. Add remaining part of the OpNode to next_nodes.
            OpNode *next_node = this->op_graph->break_node(node, i);
            next_nodes.emplace_back(next_node);
            broke_node = true;
            break;
        }

        // Check if we need to sync between comp and comm.
        if (!sync_comm && opseq->is_comm()) {
            // Check if any producer is a computation Op.
            for (auto &producer : node->producers) {
                // As we do not merge computation Ops with communication Ops,
                // we only need to check the first Op.
                if (!producer->ops[0]->is_comm()) {
                    sync_comm = true;
                    break;
                }
            }
        } else if (!sync_comp && !opseq->is_comm()) {
            // Check if any producer is a communication Op.
            for (auto &producer : node->producers) {
                // As we do not merge computation Ops with communication Ops,
                // we only need to check the first Op.
                if (producer->ops[0]->is_comm()) {
                    sync_comp = true;
                    break;
                }
            }
        }

        auto p = seen_nodes.emplace(node);
        if (!p.second) {
            LOG(ERROR, "unexpected error: already seen node ", node->get_name(),
                " (", node->ops.size(), " ops)");
        }

        // Align shared memory size
        int smem_bytes = opseq->get_smem_bytes();
        int aligned_smem_bytes = math::pad(smem_bytes, gpu_info.smem_align);

        // Create a scheduling item.
        SchedItem item;
        item.opseq_id = opseq_id;
        item.num_uops = opseq->get_tdims_size();
        item.num_warps_per_uop = opseq->get_num_warps();
        item.smem_bytes_per_uop = aligned_smem_bytes;
        if (op->is_comm()) {
            comm_items.emplace_back(item);
        } else {
            comp_items.emplace_back(item);
        }

        if (!broke_node) {
            // If OpNode is completely merged, add its users to
            // next_nodes.
            for (auto &user_node : node->users) {
                // If any producer is unseen, skip the user.
                bool skip = false;
                for (auto &producer : user_node->producers) {
                    if (seen_nodes.find(producer) == seen_nodes.end()) {
                        skip = true;
                        break;
                    }
                }
                if (!skip) {
                    next_nodes.emplace_back(user_node);
                }
            }
        }
    }

    if (this->comp_stream.empty() || sync_comp || sync_comm) {
        // Create a new stream.
        this->comp_stream.emplace_back(make_unique<SchedStream>(
            0, gpu_info.num_sm - 1, this->num_warps_per_sm,
            gpu_info.smem_block_total));
    }
    if (this->comm_stream.empty() || sync_comp || sync_comm) {
        // Create a new stream.
        this->comm_stream.emplace_back(make_unique<SchedStream>(
            gpu_info.num_sm - 1, gpu_info.num_sm, this->num_warps_per_sm,
            gpu_info.smem_block_total));
    }

    // Schedule the Ops.
    this->comp_stream.back()->add_items(comp_items);
    this->comm_stream.back()->add_items(comm_items);

    LOG(DEBUG, "scheduled ", nodes.size(), " nodes");
    for (auto &item : comp_items) {
        LOG(DEBUG, "  comp: ", this->opseqs[item.opseq_id]->get_name());
    }
    for (auto &item : comm_items) {
        LOG(DEBUG, "  comm: ", this->opseqs[item.opseq_id]->get_name());
    }

    recursive_schedule(next_nodes, seen_nodes);
}

void DefaultScheduler::configure_gpu_buf(
    const std::list<Tensor *> &model_tensors)
{
    // A TensorBuf can be located on a local GPU or a remote GPU. If it is on
    // this rank's GPU, it should be allocated and might be exported to other
    // GPUs. If it is on a remote GPU (the gid is not equal to this rank), it
    // should be imported.
    // A TensorBuf can have multi tensors pointing to it. Different Tensor
    // represent a different sharding or view of the same TensorBuf.
    std::map<TensorBuf *, std::vector<Tensor *>> bufs;
    // export_tns_sids is a map of the TensorBuf that needed to be exported, and
    // the corresponding tensors and sids. A TensorBuf can have multiple tensors
    // pointing to it, and might be exported to multiple ranks as different
    // Tensor.
    std::map<TensorBuf *, std::vector<std::pair<Tensor *, int>>>
        export_tns_sids;

    for (auto &opseq : this->opseqs) {
        for (auto &sop : opseq->get_sched_ops()) {
            for (unsigned int i = 0; i < sop.get_op()->inputs.size(); ++i) {
                auto &tile = sop.get_cfg()->input_tiles[i];
                sop.get_op()->inputs[i]->update_pads({tile.x, tile.y});
            }
            for (unsigned int i = 0; i < sop.get_op()->outputs.size(); ++i) {
                auto &tile = sop.get_cfg()->output_tiles[i];
                sop.get_op()->outputs[i]->update_pads({tile.x, tile.y});
            }
        }
    }

    for (auto &opseq : this->opseqs) {
        for (auto &sop : opseq->get_sched_ops()) {
            const Op *op = sop.get_op();
            std::vector<Tensor *> tensors = op->inputs;
            tensors.insert(tensors.end(), op->outputs.begin(),
                           op->outputs.end());

            for (auto &tns : tensors) {
                // If the tensor is not imported, it should be allocated on this
                // GPU
                if (tns->imported_rank < 0) {
                    bufs[tns->buf].emplace_back(tns);
                }
            }

            const int send_ready_flag_sid_offset = 128;

            //
            if (op->type == OP_SEND) {
                //
                Tensor *in = op->inputs[0];
                int sid;
                int rank;
                int dst_rank;
                size_t bytes;
                op->args.get(&sid, 0);
                op->args.get(&rank, 1);
                op->args.get(&dst_rank, 2);
                op->args.get(&bytes, 3);
                size_t off = in->offset() * in->type_bytes();
                // TODO: generalize converting rank to GPU ID.
                int nrph = get_env().num_ranks_per_host;
                int dst_gpu_id = dst_rank % nrph;
                if ((dst_rank / nrph) == (this->rank / nrph)) {
                    // Same node.
                    this->buf_infos.emplace_back(dst_gpu_id, bytes, nullptr,
                                                 sid, off);
                }
                export_tns_sids[in->buf].emplace_back(in, sid);
                this->send_recv_ops.emplace_back(op);
            } else if (op->type == OP_RECV) {
                //
                Tensor *in = op->inputs[0];
                int sid;
                op->args.get(&sid, 0);
                export_tns_sids[in->buf].emplace_back(in, sid);
                this->send_recv_ops.emplace_back(op);
            } else if (op->type == OP_SEND_MM) {
                int sid;
                int dst_gid;
                sop.get_op()->args.get(&sid, 0);
                sop.get_op()->args.get(&dst_gid, 1);
                // import the recvbuf, the recvbuf should be allocated on the
                // receiver GPU
                Tensor *recvbuf = sop.get_op()->inputs[1];
                this->buf_infos.emplace_back(dst_gid, recvbuf->shape_bytes(),
                                             recvbuf->buf, sid, 0);

                // configure the send_ready_flag, the send_ready_flag needed to
                // be exported to the recv GPU, since the sid of the
                // send_ready_flag should not be the same as the recvBuf, so I
                // use the sid+128 as the sid of the send_ready_flag
                Tensor *send_ready_flag = sop.get_op()->inputs[2];
                export_tns_sids[send_ready_flag->buf].emplace_back(
                    send_ready_flag, sid + send_ready_flag_sid_offset);
            } else if (op->type == OP_RECV_MM) {
                int sid;
                int src_gid;
                sop.get_op()->args.get(&sid, 0);
                sop.get_op()->args.get(&src_gid, 1);
                // configure the recvbuf, the recvbuf needed to be export the to
                // the sender GPU, the sid is the same as the sid of the send_mm
                // op and the recv_mm op
                Tensor *recvbuf = sop.get_op()->inputs[1];
                export_tns_sids[recvbuf->buf].emplace_back(recvbuf, sid);

                // import the send_ready_flag, the send_ready_flag tensor should
                // be allocated on the sender GPU
                Tensor *send_ready_flag = sop.get_op()->inputs[2];
                this->buf_infos.emplace_back(
                    src_gid, send_ready_flag->shape_bytes(),
                    send_ready_flag->buf, sid + send_ready_flag_sid_offset, 0);
            }
        }
    }

    for (auto &tns : model_tensors) {
        auto search = bufs.find(tns->buf);
        if (search == bufs.end()) {
            bufs[tns->buf].emplace_back(tns);
        }
    }

    // Fix TensorBuf size.
    for (auto &el : bufs) {
        TensorBuf *buf = el.first;
        vector<Tensor *> &tensors = el.second;
        size_t max_bytes = 0;
        for (auto &tns : tensors) {
            size_t tns_bytes = tns->ldims_bytes();
            if (max_bytes < tns_bytes) {
                max_bytes = tns_bytes;
            }
            // TODO: more verficiations.
            auto &sh = tns->shape;
            auto &ld = tns->ldims;
            LOG(DEBUG, "Tensor buf ", tns->buf, " pads ", tns->pads,
                " padding ", sh, " -> ", ld, " exported ", tns->exported);
        }
        // Store the size.
        buf->bytes = max_bytes;
    }

    // Allocate all GPU buffers.
    for (auto &el : bufs) {
        TensorBuf *buf = el.first;
        int sid = -1;
        size_t off = 0;
        auto search = export_tns_sids.find(buf);
        if (search != export_tns_sids.end()) {
            for (auto &p : search->second) {
                Tensor *t = p.first;
                sid = p.second;
                off = t->offset() * t->type_bytes();
                this->buf_infos.emplace_back(this->gpu_mgr->gpu_id, buf->bytes,
                                             buf, sid, off);
            }
        } else {
            this->buf_infos.emplace_back(this->gpu_mgr->gpu_id, buf->bytes, buf,
                                         sid, off);
        }
    }
}

std::vector<std::string> DefaultScheduler::gen_code()
{
    std::stringstream code;

    std::set<int> imported_ranks;
    for (auto &tns : this->model->impl->get_tensors()) {
        if (tns->imported_rank >= 0) {
            imported_ranks.insert(tns->imported_rank);
        }
    }
    for (auto rank : imported_ranks) {
        this->codegen->def_remote_buf(code, rank);
    }

    this->codegen->def_sync_stream(code, 0);
    this->codegen->def_sync_stream(code, 1);

    std::map<std::string, int> uop_map;
    for (auto &opseq : this->opseqs) {
        for (auto &sop : opseq->get_sched_ops()) {
            int uop_id = (int)uop_map.size();
            std::string sop_func_str = sop.function_name();
            // Insert only if it does not exist
            auto p = uop_map.emplace(sop_func_str, uop_id);
            if (p.second) {
                // If this is a new function, define it.
                this->codegen->def_uop(code, sop, uop_id);
            }
        }
    }
    for (auto &opseq : this->opseqs) {
        this->codegen->opseq(code, "op" + std::to_string(opseq->get_id()),
                             *opseq, uop_map);
    }

    const GpuInfo &gpu_info = this->gpu_mgr->get_gpu_info();
    int num_sm_comp = gpu_info.num_sm - 1;
    int num_sm_comm = 1;

    code << "__device__ void ark_loop_body(int _iter) {\n";
    for (size_t i = 0; i < this->comp_stream.size(); ++i) {
        auto comp_streams = this->comp_stream[i]->get_streams();
        for (size_t j = 0; j < comp_streams.size(); ++j) {
            auto &stream = comp_streams[j];
            for (auto &branch : stream.branches) {
                this->codegen->branch(code, branch);
            }
            if (!stream.branches.empty() && j != comp_streams.size() - 1) {
                code << "  ";
                this->codegen->sync_stream(code, 0, 0, num_sm_comp);
            }
        }
        auto comm_streams = this->comm_stream[i]->get_streams();
        for (size_t j = 0; j < comm_streams.size(); ++j) {
            auto &stream = comm_streams[j];
            for (auto &branch : stream.branches) {
                this->codegen->branch(code, branch);
            }
            if (!stream.branches.empty() && j != comm_streams.size() - 1) {
                code << "  ";
                this->codegen->sync_stream(code, 1, num_sm_comp,
                                           num_sm_comp + num_sm_comm);
            }
        }
        if (i != this->comp_stream.size() - 1) {
            code << "  ";
            this->codegen->sync_gpu(code);
        }
    }
    code << "}\n";
    return {code.str()};
}

} // namespace ark

```

`ark/sched/sched/sched_kahypar.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "kahypar.h"
#include "logging.h"
#include "math.h"
#include "sched/sched.h"

using namespace std;

#define PRESERVE_WARP_FOR_COMM 1

namespace ark {

KahyparScheduler::KahyparScheduler(const int gpu_id, int rank_, int world_size_,
                                   const Model &model, unsigned int wps_)
    : DefaultScheduler(gpu_id, rank_, world_size_, model, wps_),
      profiler{get_gpu_mgr(gpu_id), wps_}
{
}

vector<Sched> KahyparScheduler::simplify_sched(vector<Sched> &original_scheds)
{
    vector<Sched> merged_scheds;
    SchedOpSeq *opseq = NULL;
    int sm_b = 0;
    int sm_e = 0;
    int th_b = 0;
    int th_e = 0;
    int alpha = 1;
    int beta = 0;
    int id = 0;
    // merge the scheds that are in the sequential order
    for (auto &sched : original_scheds) {
        if (opseq == sched.opseq && sm_e == sched.sm_b && th_e == sched.th_e &&
            th_b == sched.th_b && id + 1 == sched.beta) {
            // we can merge the sched into current scheds
            LOG(DEBUG, " merged op", opseq->get_id(), " sm_b ", sm_b, " sm_e",
                sm_e, " th_b ", th_b, " th_e ", th_e, " id ", beta);
            sm_e = sched.sm_e;
            id++;
        } else {
            // we cannot merge the sched into current scheds
            if (opseq != NULL) {
                LOG(DEBUG, "  op", opseq->get_id(), " sm_b ", sm_b, " sm_e ",
                    sm_e, " th_b ", th_b, " th_e ", th_e, " id ", beta);
                LOG(DEBUG, "  op", sched.opseq->get_id(), " sm_b ", sched.sm_b,
                    " sm_e ", sched.sm_e, " th_b ", sched.th_b, " th_e ",
                    sched.th_e, " id ", sched.beta);
            }
            if (opseq != NULL)
                merged_scheds.emplace_back(opseq, sm_b, sm_e, th_b, th_e, alpha,
                                           beta);
            opseq = sched.opseq;
            sm_b = sched.sm_b;
            sm_e = sched.sm_e;
            th_b = sched.th_b;
            th_e = sched.th_e;
            // alpha = sched.alpha;
            beta = sched.beta;
            id = beta;
        }
    }
}

function<SchedTile *()> gen_tile_nodes(const SchedOpSeq *opseq, int xz_min = -1,
                                       int xz_max = -1, int y_min = -1,
                                       int y_max = -1)
{
    int xz_b = (xz_min == -1) ? 0 : xz_min;
    int xz_e = (xz_max == -1) ? opseq->get_tdim_xz() : xz_max + 1;
    int y_b = (y_min == -1) ? 0 : y_min;
    int y_e = (y_max == -1) ? opseq->get_tdim_y() : y_max + 1;
    return [opseq, xz_b, xz_e, y_b, y_e] {
        const int ydim = opseq->get_tdim_y();
        int xzidx = xz_b;
        int yidx = y_b;
        int id = yidx + xzidx * ydim;
        return [=]() mutable {
            if (xzidx >= xz_e) {
                return (SchedTile *)nullptr;
            }
            SchedTile *ret = new SchedTile{opseq, id++};
            assert(ret != nullptr);
            if (++yidx == y_e) {
                ++xzidx;
                yidx = y_b;
                id = yidx + xzidx * ydim;
            }
            return ret;
        };
    }();
}

function<SchedTile *()> gen_tile_nodes(const SchedOpSeq *opseq0,
                                       const SchedOpSeq *opseq1)
{
    return [=] {
        SchedOpSeq *opseq = (SchedOpSeq *)opseq0;
        int max_id = opseq0->get_tdims_size();
        int id = 0;
        return [=]() mutable {
            SchedTile *ret = new SchedTile{opseq, id++};
            assert(ret != nullptr);
            if (id == max_id) {
                if (opseq == opseq1) {
                    return (SchedTile *)nullptr;
                }
                opseq = (SchedOpSeq *)opseq1;
                max_id = opseq1->get_tdims_size();
                id = 0;
            }
            return ret;
        };
    }();
}

int KahyparScheduler::kahypar_schedule_depth(vector<SchedOpSeq *> &depth,
                                             vector<Sched> &scheds)
{
    const GpuInfo &gpu_info = this->gpu_mgr->get_gpu_info();
#ifdef PRESERVE_WARP_FOR_COMM
    // Number of SMs to use for computation. The last SM is preserved for
    // communication only.
    int num_sm_calc = gpu_info.num_sm - 1;
#else
    int num_sm_calc = gpu_info.num_sm;
#endif
    KahyparGraph<SchedTile> kg;
    auto search = this->profiler.wps_prof_results.find(this->num_warps_per_sm);
    if (search == this->profiler.wps_prof_results.end()) {
        LOGERR("Unexpected error.");
    }
    auto &opseq_perf = search->second;
    for (auto &opseq : depth) {
        auto &perf = opseq_perf[opseq];
        // Add tile nodes.
        int vw = perf.s.elapsed * 1e3;
        kg.add_nodes(vw, gen_tile_nodes(opseq));
        // Check the minimum scored inter-tile edge.
        int min_score = -1;
        if (perf.xy.is_set()) {
            min_score = perf.xy.score;
        }
        if (perf.x.is_set() && min_score > perf.x.score) {
            min_score = perf.x.score;
        }
        if (perf.y.is_set() && min_score > perf.x.score) {
            min_score = perf.x.score;
        }
        if (min_score == -1) {
            // No inter-tile edges.
            continue;
        }
        // Add an edge connecting all vertices.
        int ew = perf.x.score - min_score;
        kg.add_edge(ew, gen_tile_nodes(opseq));
        if (perf.x.is_set() && perf.x.score > min_score * 1.03) {
            // Add edges connecting vertices with the same y-axis, i.e.
            // intra-x-axis correlation.
            ew = perf.x.score - min_score;
            for (int yidx = 0; yidx < opseq->get_tdim_y(); ++yidx) {
                // Weak overall correlations.
                kg.add_edge(ew * 0.05,
                            gen_tile_nodes(opseq, -1, -1, yidx, yidx));
                // Strong adjacent correlations.
                for (int xzidx = 1; xzidx < opseq->get_tdim_xz(); ++xzidx) {
                    kg.add_edge(ew, gen_tile_nodes(opseq, xzidx - 1, xzidx,
                                                   yidx, yidx));
                }
            }
        }
        if (perf.y.is_set() && (perf.y.score > min_score * 1.03)) {
            // Add edges connecting vertices with the same x-axis, i.e.
            // intra-y-axis correlation.
            ew = perf.y.score - min_score;
            for (int xzidx = 0; xzidx < opseq->get_tdim_xz(); ++xzidx) {
                // Weak overall correlations.
                kg.add_edge(ew * 0.05,
                            gen_tile_nodes(opseq, xzidx, xzidx, -1, -1));
                // Strong adjacent correlations.
                for (int yidx = 1; yidx < opseq->get_tdim_y(); ++yidx) {
                    kg.add_edge(ew, gen_tile_nodes(opseq, xzidx, xzidx,
                                                   yidx - 1, yidx));
                }
            }
        }
    }
    // Add inter-taskset edges.
    set<const SchedOpSeq *> seen;
    for (auto &opseq0 : depth) {
        auto &perf = opseq_perf[opseq0];
        seen.insert(opseq0);
        for (auto &el : perf.mixed) {
            auto &opseq1 = el.first;
            auto search = seen.find(opseq1);
            if (search != seen.end()) {
                continue;
            }
            int ew = el.second.score;
            kg.add_edge(ew, gen_tile_nodes(opseq0, opseq1));
            kg.add_edge(-ew, gen_tile_nodes(opseq0));
            kg.add_edge(-ew, gen_tile_nodes(opseq1));
        }
    }
    //
    if (kg.get_num_nodes() > num_sm_calc) {
        LOG(DEBUG, "Partitioning graph with ", kg.get_num_nodes(), " nodes");
        auto &parts = kg.partition(num_sm_calc);
        SchedTileDepth *tile_depth = new SchedTileDepth(num_sm_calc);
        for (int i = 0; i < num_sm_calc; ++i) {
            auto &sm = tile_depth->sms[i];
            auto &part = parts[i];
            if (part.size() > 0)
                sm.emplace_back();
            for (auto &p : part) {
                if (sm.back().get_num_warps() >= this->num_warps_per_sm)
                    sm.emplace_back();
                sm.back().tiles.emplace_back(move(*p.first));
                if (p.second == nullptr)
                    continue;
                if (sm.back().get_num_warps() >= this->num_warps_per_sm)
                    sm.emplace_back();
                sm.back().tiles.emplace_back(move(*p.second));
            }
        }
        std::vector<std::vector<SchedTileSet>> &sms = tile_depth->sms;
        // LOG(DEBUG, "Sorting tiles");
        // order the tiles in each SM by their id
        // sort(sms.begin(), sms.end(),
        //      [](const std::vector<SchedTileSet> &a,
        //         const std::vector<SchedTileSet> &b) {
        //          return a[0].tiles[0] < b[0].tiles[0];
        //      });
        // LOG(DEBUG, "Generating tiles");
        auto scheds_ = gen_sched(tile_depth, this->num_warps_per_sm);
        LOG(DEBUG, "Generated ", scheds_.size(), " scheds");
        for (auto sched : scheds_) {
            scheds.push_back(sched);
        }
        return 0;
    } else {
        LOG(DEBUG, "Can't generate kahypar scheds from graph with ",
            kg.get_num_nodes(), " nodes");
        return -1;
    }
}

vector<string> KahyparScheduler::schedule()
{
    LOG(DEBUG, "KahyparScheduler start scheduling");

    LOG(DEBUG, "profiling the ops ...");
    this->profiler.profile(this->op_graph, this->scg, this->ctx);
    vector<Sched> scheds;
    vector<GpuLoopKernel *> glks;
    for (auto &depth : this->op_graph->depth_nodes) {
        vector<Sched> ds;
        vector<SchedOpSeq *> calc_opseqs;
        vector<SchedOpSeq *> send_opseqs;
        vector<SchedOpSeq *> recv_opseqs;
        for (auto &ogn : depth) {
            if (ogn->opseq.is_send()) {
                send_opseqs.emplace_back(&(ogn->opseq));
            } else if (ogn->opseq.is_recv()) {
                recv_opseqs.emplace_back(&(ogn->opseq));
            } else {
                calc_opseqs.emplace_back(&(ogn->opseq));
            }
        }
        LOG(DEBUG, "schedule depth");
        this->schedule_depth_comm(send_opseqs, scheds);
        // The kahypar schedule algorithm only works for the calculation ops. If
        // the tile number is less than sm_num, we will use the original
        // schedule algorithm instead.
        if (this->kahypar_schedule_depth(calc_opseqs, scheds) != 0) {
            LOG(DEBUG, "schedule depth calc ops failed");
            this->schedule_depth(calc_opseqs, scheds);
        }
        this->schedule_depth_comm(recv_opseqs, scheds);
        // TODO: profile one depth
        // Global sync.
        scheds.emplace_back(nullptr, 0, 0, 0, 0, 0, 0);
    }
    return this->scg.codegen_codes_body(scheds);
}

} // namespace ark

```

`ark/sched/sched/sched_simple.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "env.h"
#include "logging.h"
#include "math.h"
#include "model.h"
#include "sched/sched.h"

using namespace std;

namespace ark {

SimpleScheduler::SimpleScheduler(Model &model, int gpu_id, int rank_,
                                 int world_size_, int num_warps_per_sm_)
    : BaseScheduler(model, gpu_id, rank_, world_size_, num_warps_per_sm_)
{
}

//
void SimpleScheduler::schedule()
{
    LOG(DEBUG, "SimpleScheduler start scheduling");

    int op_idx = 0;
    int opseq_idx = 0;
    std::vector<Op *> all_ops;
    for (auto &op : model->impl->get_ops()) {
        all_ops.push_back(op);
    }
    std::vector<Tensor *> finished_tensors;

    // get the input tensors of the model, and add them to the
    // finished_tensors vector
    for (auto &tns : model->impl->get_tensors()) {
        if (model->impl->get_producer(tns) == nullptr) {
            finished_tensors.push_back(tns);
        }
    }
    this->opseqs.emplace_back(std::make_unique<SchedOpSeq>(opseq_idx));
    opseq_idx++;

    while (!all_ops.empty()) {
        // find the next op to schedule
        Op *op = nullptr;
        for (size_t i = 0; i < all_ops.size(); i++) {
            bool input_ready = true;
            // check if all the input tensors of the op are ready
            for (Tensor *&tns : all_ops[i]->inputs) {
                if (std::find(finished_tensors.begin(), finished_tensors.end(),
                              tns) == finished_tensors.end()) {
                    input_ready = false;
                    break;
                }
            }
            if (input_ready) {
                op = all_ops[i];
                all_ops.erase(all_ops.begin() + i);
                // add the output tensors of the op to the
                // finished_tensors
                for (Tensor *tns : op->outputs) {
                    finished_tensors.push_back(tns);
                }
                break;
            }
        }
        if (op == nullptr) {
            LOG(INFO, "Cannot find next op to schedule");
            break;
        }
        // schedule model with sched_op_config and pad the tensors
        // by looking up the config table of the op in sched_op_config
        const OpConfig *cfg = this->sched_op_config(op);

        string sched_op_name("op_" + to_string(op_idx) + "_" + op->name);
        for (size_t i = 0; i < sched_op_name.size(); i++) {
            if (sched_op_name[i] == '/') {
                sched_op_name[i] = '_';
            }
        }
        LOG(DEBUG, "sched_op: ", sched_op_name);
        SchedOp sched_op(op, cfg, sched_op_name);
        // some virtual ops like ops_tensor are not scheduled, but we need to
        // allocated gpu_buf for them
        this->sched_ops.push_back(sched_op);
        if (cfg == nullptr) {
            continue;
        }
        // We create an opseq for each op for simplicity, we don't merge ops
        // into opseqs in SimpleScheduler
        this->opseqs.emplace_back(std::make_unique<SchedOpSeq>(op_idx));
        SchedOpSeq *opseq = this->opseqs.back().get();

        LOG(DEBUG, "get_sched_ops: ", opseq->get_sched_ops().size());
        opseq->append(op, cfg);
        op_idx++;
    }
    if (!all_ops.empty()) {
        LOGERR("Cannot schedule all ops");
    }

    this->configure_gpu_buf(model->impl->get_tensors());
}

void SimpleScheduler::schedule_sched_opseq(SchedOpSeq &seq, int max_wps,
                                           int max_sm_num,
                                           vector<Sched> &scheds)
{
    int seq_tile_num = seq.get_tdims_size();
    LOG(DEBUG, "opseq", seq.get_id(), " seq_tile_num: ", seq_tile_num);
    int warps_per_tile = seq.get_num_warps();

    int sched_tile_idx = 0;
    for (int seq_depth = 0; sched_tile_idx < seq_tile_num; seq_depth++) {
        int tile_num = seq_tile_num - sched_tile_idx;
        // calculate the number of tiles that can be scheduled in this
        // seq_depth
        int max_tiles_per_sm = max_wps / warps_per_tile;
        int max_tile_num = max_sm_num * max_wps / warps_per_tile;
        int sm_b;
        int sm_e;
        int warp_b;
        int warp_e;
        int tiles_per_sm;
        int sched_tile_num;
        int remained_tile_num = 0;
        bool have_remain = false;
        // if the seq_tile_num * warp_per_tile > max_sm_num *
        // max_wps, the seq can't be finished in one depth
        // we use all the sm resources to do the task
        if (tile_num > max_tile_num) {
            sched_tile_num = max_tile_num;
            tiles_per_sm = max_tiles_per_sm;
            sm_b = 0;
            sm_e = max_sm_num;
            warp_b = 0;
            warp_e = tiles_per_sm * warps_per_tile;
        } else {
            // if the tile_num is less than the number of the sm, we
            // only assign one tile to tile_num sm
            if (tile_num < max_sm_num) {
                sched_tile_num = tile_num;
                tiles_per_sm = 1;
                sm_b = 0;
                sm_e = tile_num;
                warp_b = 0;
                warp_e = tiles_per_sm * warps_per_tile;
            } else {
                // if the tile can be evenly distributed to all sm, we
                // assign each sm with the same number of tiles
                if (tile_num % max_sm_num == 0) {
                    sched_tile_num = tile_num;
                    tiles_per_sm = tile_num / max_sm_num;
                    sm_b = 0;
                    sm_e = max_sm_num;
                    warp_b = 0;
                    warp_e = tiles_per_sm * warps_per_tile;
                } else {
                    have_remain = true;
                }
            }
        }
        if (have_remain == false) {
            // Sched sched{};
            LOG(DEBUG, "sched: ", seq.get_id(), "sm_b: ", sm_b, " sm_e: ", sm_e,
                " warp_b: ", warp_b, " warp_e: ", warp_e,
                " tiles_per_sm: ", tiles_per_sm,
                " sched_tile_idx: ", sched_tile_idx);
            scheds.emplace_back(&seq, sm_b, sm_e, warp_b * 32, warp_e * 32,
                                tiles_per_sm, sched_tile_idx);
            sched_tile_idx += sched_tile_num;
        } else {
            // if the tile can't be evenly distributed to all
            // sm, we first use the first remained_tile_num sm
            // , each sm execute tiles_per_sm+1 tiles, and the
            // rest sm execute tiles_per_sm tiles
            // this is the first SchedTileTask
            tiles_per_sm = tile_num / max_sm_num + 1;
            remained_tile_num = tile_num % max_sm_num;
            sched_tile_num = remained_tile_num * tiles_per_sm;
            sm_b = 0;
            sm_e = remained_tile_num;
            warp_b = 0;
            warp_e = tiles_per_sm * warps_per_tile;
            scheds.emplace_back(&seq, sm_b, sm_e, warp_b * 32, warp_e * 32,
                                tiles_per_sm, sched_tile_idx);
            sched_tile_idx += sched_tile_num;

            // the second SchedTileTask
            tiles_per_sm = tile_num / max_sm_num;
            sched_tile_num = (max_sm_num - remained_tile_num) * tiles_per_sm;
            sm_b = remained_tile_num;
            sm_e = max_sm_num;
            warp_b = 0;
            warp_e = tiles_per_sm * warps_per_tile;
            scheds.emplace_back(&seq, sm_b, sm_e, warp_b * 32, warp_e * 32,
                                tiles_per_sm, sched_tile_idx);
            sched_tile_idx += sched_tile_num;
        }
        LOG(DEBUG, "sched_tile_idx: ", sched_tile_idx);
    }
    if (seq_tile_num != -1 && sched_tile_idx != seq_tile_num) {
        LOGERR("only ", sched_tile_idx, " tiles are scheduled, but ",
               seq_tile_num, " tiles are needed to be scheduled");
    }
}

vector<string> SimpleScheduler::gen_code()
{
    LOG(DEBUG, "SimpleScheduler start scheduling");
    int num_sm = this->gpu_mgr->get_gpu_info().num_sm;
    vector<Sched> scheds;
    for (auto &seq : this->opseqs) {
        this->schedule_sched_opseq(*seq, this->num_warps_per_sm, num_sm,
                                   scheds);
    }

    stringstream loop_body_code, sched_opseq_code, data_buf_code;
    for (int i = 0; i < this->world_size; i++) {
        data_buf_code << "__device__ char *" << ARK_BUF_NAME << i << ";\n";
    }
    loop_body_code << "__device__ void ark_loop_body(int _iter) {\n";
    // to avoid the same opseq code being generated multiple times
    set<int> opseq_ids;
    std::map<std::string, int> uop_map;
    for (Sched &sched : scheds) {
        bool virt_opseq = false;
        // for the baseline scheduler, one opseq is a depth and have a global
        // sync between each opseq
        SchedOpSeq *opseq = sched.opseq;
        if (opseq_ids.find(opseq->get_id()) != opseq_ids.end()) {
        } else {
            opseq_ids.insert(opseq->get_id());
            this->codegen->opseq(sched_opseq_code,
                                 "opseq_" + to_string(opseq->get_id()), *opseq,
                                 uop_map);
        }
        if (virt_opseq)
            continue;
        this->codegen->sched(loop_body_code, sched);
        loop_body_code << "  ";
        this->codegen->sync_gpu(loop_body_code);
    }
    loop_body_code << "}\n";
    vector<string> ret;
    ret.emplace_back(data_buf_code.str() + sched_opseq_code.str() +
                     loop_body_code.str());
    return ret;
}

void SimpleScheduler::configure_gpu_buf(const std::list<Tensor *> &)
{
    // A TensorBuf can be located on a local GPU or a remote GPU. If it is on
    // this rank's GPU, it should be allocated and might be exported to other
    // GPUs. If it is on a remote GPU (the gid is not equal to this rank), it
    // should be imported.
    // A TensorBuf can have multi tensors pointing to it. Different Tensor
    // represent a different sharding or view of the same TensorBuf.
    map<TensorBuf *, vector<Tensor *>> bufs;
    // export_tns_sids is a map of the TensorBuf that needed to be exported, and
    // the corresponding tensors and sids. A TensorBuf can have multiple tensors
    // pointing to it, and might be exported to multiple ranks as different
    // Tensor.
    map<TensorBuf *, vector<pair<Tensor *, int>>> export_tns_sids;
    // pad the tensors according to the tile size of the op
    for (auto &sop : this->sched_ops) {
        const int send_ready_flag_sid_offset = 128;

        LOG(DEBUG, "configure_gpu_buf: ", sop.get_op()->name);
        if (sop.get_op()->type == OP_SEND_MM) {
            int sid;
            int dst_gid;
            sop.get_op()->args.get(&sid, 0);
            sop.get_op()->args.get(&dst_gid, 1);
            // import the recvbuf, the recvbuf should be allocated on the
            // receiver GPU
            Tensor *recvbuf = sop.get_op()->inputs[1];
            this->buf_infos.emplace_back(dst_gid, recvbuf->shape_bytes(),
                                         recvbuf->buf, sid, 0);

            // configure the send_ready_flag, the send_ready_flag needed to be
            // exported to the recv GPU, since the sid of the send_ready_flag
            // should not be the same as the recvBuf, so I use the sid+128 as
            // the sid of the send_ready_flag
            Tensor *send_ready_flag = sop.get_op()->inputs[2];
            export_tns_sids[send_ready_flag->buf].emplace_back(
                send_ready_flag, sid + send_ready_flag_sid_offset);
        } else if (sop.get_op()->type == OP_RECV_MM) {
            int sid;
            int src_gid;
            sop.get_op()->args.get(&sid, 0);
            sop.get_op()->args.get(&src_gid, 1);
            // configure the recvbuf, the recvbuf needed to be export the to the
            // sender GPU, the sid is the same as the sid of the send_mm op and
            // the recv_mm op
            Tensor *recvbuf = sop.get_op()->inputs[1];
            export_tns_sids[recvbuf->buf].emplace_back(recvbuf, sid);

            // import the send_ready_flag, the send_ready_flag tensor should be
            // allocated on the sender GPU
            Tensor *send_ready_flag = sop.get_op()->inputs[2];
            this->buf_infos.emplace_back(
                src_gid, send_ready_flag->shape_bytes(), send_ready_flag->buf,
                sid + send_ready_flag_sid_offset, 0);
        }

        if (sop.get_op()->type == OP_SEND) {
            Tensor *in = sop.get_op()->inputs[0];
            int sid;
            int rank;
            int dst_rank;
            size_t bytes;
            sop.get_op()->args.get(&sid, 0);
            sop.get_op()->args.get(&rank, 1);
            sop.get_op()->args.get(&dst_rank, 2);
            sop.get_op()->args.get(&bytes, 3);
            // TODO: generalize converting rank to GPU ID.
            int nrph = get_env().num_ranks_per_host;
            int dst_gpu_id = dst_rank % nrph;
            if ((dst_rank / nrph) == (this->rank / nrph)) {
                // Same node.
                this->buf_infos.emplace_back(dst_gpu_id, bytes, nullptr, sid,
                                             0);
            }
            export_tns_sids[in->buf].emplace_back(in, sid);
            this->send_recv_ops.emplace_back(sop.get_op());
        } else if (sop.get_op()->type == OP_RECV) {
            Tensor *in = sop.get_op()->inputs[0];
            int sid;
            sop.get_op()->args.get(&sid, 0);
            export_tns_sids[in->buf].emplace_back(in, sid);
            this->send_recv_ops.emplace_back(sop.get_op());
        }
        for (auto &tns : sop.get_op()->inputs) {
            // if the tensor is not imported, it should be allocated on this GPU
            if (tns->imported_rank < 0)
                bufs[tns->buf].emplace_back(tns);
        }
        // TODO: print warning if the tensor is not used by any real computation
        for (auto &tns : sop.get_op()->outputs) {
            if (tns->imported_rank < 0)
                bufs[tns->buf].emplace_back(tns);
        }
    }
    // Fix TensorBuf size. The size of the TensorBuf is the max size of its
    // tensors.
    for (auto &el : bufs) {
        TensorBuf *buf = el.first;
        vector<Tensor *> &tensors = el.second;
        size_t max_bytes = 0;
        for (auto &tns : tensors) {
            size_t tns_bytes = tns->ldims_bytes();
            if (max_bytes < tns_bytes) {
                max_bytes = tns_bytes;
            }
            // TODO: more verficiations.
            auto &sh = tns->shape;
            auto &ld = tns->ldims;
            stringstream ss;
            LOG(DEBUG, "Tensor buf ", tns->buf, " pads ", tns->pads, " shape ",
                sh, " ldims ", ld, " offs ", tns->offs, " exported ",
                tns->exported, " max_bytes ", max_bytes);
        }
        // Store the size.
        buf->bytes = max_bytes;
    }
    LOG(DEBUG, "bufs.size(): ", bufs.size());
    // the tensor that needed to be allocated
    vector<TensorBuf *> to_alloc;
    for (auto &sop : this->sched_ops) {
        for (auto &tns : sop.get_op()->inputs) {
            size_t buf_num = bufs.erase(tns->buf);
            if (buf_num > 0) {
                assert(buf_num == 1);
                to_alloc.emplace_back(tns->buf);
            }
        }
        for (auto &tns : sop.get_op()->outputs) {
            size_t buf_num = bufs.erase(tns->buf);
            if (buf_num > 0) {
                assert(buf_num == 1);
                to_alloc.emplace_back(tns->buf);
            }
        }
    }
    LOG(DEBUG, "alloc: ", to_alloc.size(), " TensorBufs");
    // Allocate GPU buffers.
    for (auto &buf : to_alloc) {
        int sid = -1;
        size_t off = 0;
        auto search = export_tns_sids.find(buf);
        if (search != export_tns_sids.end()) {
            for (auto &p : search->second) {
                Tensor *t = p.first;
                sid = p.second;
                off = t->offset() * t->type_bytes();
                // the TensorBuf that needed to be allocated and exported
                this->buf_infos.emplace_back(this->gpu_mgr->gpu_id, buf->bytes,
                                             buf, sid, off);
            }
        } else {
            // the TensorBuf that needed to be allocated
            this->buf_infos.emplace_back(this->gpu_mgr->gpu_id, buf->bytes, buf,
                                         sid, off);
        }
    }
}

} // namespace ark

```

`ark/sched/sched_branch.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "sched_branch.h"
#include "logging.h"
#include <algorithm>
#include <map>
#include <set>
#include <unordered_set>
#include <vector>

#define DEBUG_BRANCH 0
#define BRANCH_DEBUG(...)                                                      \
    do {                                                                       \
        if (DEBUG_BRANCH) {                                                    \
            LOG(DEBUG, __VA_ARGS__);                                           \
        }                                                                      \
    } while (0);

namespace ark {

/// Contains information on which SMs and warps execute a given opseq.
///
/// Indicates:
/// ```
///   if (sm_id_begin <= sm_id < sm_id_end &&
///       warp_id_begin <= warp_id < warp_id_end) {
///     num_uops_per_sm = (warp_id_end - warp_id_begin) / num_warps_per_uop;
///     warp_idx = warp_id - warp_id_begin;
///     sm_idx = sm_id - sm_id_begin;
///     op = opseq_id;
///     uop = uop_id_diff * (warp_idx / num_warps_per_uop + num_uops_per_sm *
///                          sm_idx) + uop_id_begin;
///   }
/// ```
struct OpBranchInfo
{
    /// The opseq ID.
    int opseq_id;
    /// The SM ID range [sm_id_begin, sm_id_end).
    int sm_id_begin;
    /// The SM ID range [sm_id_begin, sm_id_end).
    int sm_id_end;
    /// The warp ID range [warp_id_begin, warp_id_end).
    int warp_id_begin;
    /// The warp ID range [warp_id_begin, warp_id_end).
    int warp_id_end;
    /// The uop ID that the first warp in the range executes.
    int uop_id_begin;
    /// The uop ID that the last warp in the range executes.
    int uop_id_last;
    /// The difference between the uop ID of the first warp and the uop ID of
    /// the `num_warps_per_uop`-th warp.
    int uop_id_diff;
    /// The number of warps per uop.
    int num_warps_per_uop;
    /// Bytes of shared memory allowed per warp.
    int smem_bytes_per_warp;

    int get_num_uops() const
    {
        int uops_per_sm = (warp_id_end - warp_id_begin) / num_warps_per_uop;
        int num_sms = sm_id_end - sm_id_begin;
        return uops_per_sm * num_sms;
    }
};

class SchedBranch::Impl
{
  private:
    struct UnitOp
    {
        int opseq_id;
        int uop_id;
        int sm_id;
        int warp_id_begin;
        int warp_id_end;
    };

    static bool cmp_uop(const UnitOp &a, const UnitOp &b);
    std::vector<OpBranchInfo> get_op_branch_info(
        const std::map<int, int> &sm_id_to_smem_per_warp);

    std::vector<UnitOp> uops;
    std::map<int, std::set<int>> opseq_to_uop_ids;

  public:
    Impl();
    ~Impl();

  protected:
    void add(int opseq_id, int uop_id, int sm_id, int warp_id_begin,
             int warp_id_end);
    void clear();
    std::vector<Branch> get_branches(
        const std::map<int, int> &sm_id_to_smem_per_warp);

    friend class SchedBranch;
};

SchedBranch::Impl::Impl()
{
}

SchedBranch::Impl::~Impl()
{
}

void SchedBranch::Impl::add(int opseq_id, int uop_id, int sm_id,
                            int warp_id_begin, int warp_id_end)
{
    if (uop_id < 0) {
        LOG(ERROR, "uop_id ", uop_id, " out of range [0, inf)");
    }
    if (sm_id < 0) {
        LOG(ERROR, "sm_id ", sm_id, " out of range [0, inf)");
    }
    if (warp_id_begin < 0) {
        LOG(ERROR, "warp_id_begin ", warp_id_begin, " out of range [0, inf)");
    }
    if (warp_id_end <= warp_id_begin) {
        LOG(ERROR, "warp_id_end ", warp_id_end, " <= warp_id_begin ",
            warp_id_begin);
    }
    auto p = this->opseq_to_uop_ids[opseq_id].insert(uop_id);
    if (!p.second) {
        LOG(ERROR, "opseq_id ", opseq_id, " uop_id ", uop_id,
            " already exists");
    }
    UnitOp uop{opseq_id, uop_id, sm_id, warp_id_begin, warp_id_end};
    this->uops.emplace_back(uop);
}

void SchedBranch::Impl::clear()
{
    this->uops.clear();
}

bool SchedBranch::Impl::cmp_uop(const UnitOp &a, const UnitOp &b)
{
    if (a.opseq_id != b.opseq_id) {
        return a.opseq_id < b.opseq_id;
    } else if (a.sm_id != b.sm_id) {
        return a.sm_id < b.sm_id;
    } else if (a.warp_id_begin != b.warp_id_begin) {
        return a.warp_id_begin < b.warp_id_begin;
    } else if (a.warp_id_end != b.warp_id_end) {
        return a.warp_id_end < b.warp_id_end;
    } else {
        return a.uop_id < b.uop_id;
    }
}

std::vector<OpBranchInfo> SchedBranch::Impl::get_op_branch_info(
    const std::map<int, int> &sm_id_to_smem_per_warp)
{
    std::vector<OpBranchInfo> infos;

    std::sort(this->uops.begin(), this->uops.end(), cmp_uop);

    std::unordered_set<size_t> merged_uop_indices;

    for (size_t i = 0; i < this->uops.size(); ++i) {
        if (merged_uop_indices.find(i) != merged_uop_indices.end()) {
            continue;
        }
        size_t num_merged_uops = merged_uop_indices.size();

        UnitOp current_uop = this->uops[i];
        OpBranchInfo info;
        info.opseq_id = current_uop.opseq_id;
        info.sm_id_begin = current_uop.sm_id;
        info.sm_id_end = current_uop.sm_id + 1;
        info.warp_id_begin = current_uop.warp_id_begin;
        info.warp_id_end = current_uop.warp_id_end;
        info.uop_id_begin = current_uop.uop_id;
        info.uop_id_last = current_uop.uop_id;
        info.uop_id_diff = 0;
        info.num_warps_per_uop =
            current_uop.warp_id_end - current_uop.warp_id_begin;

        auto search = sm_id_to_smem_per_warp.find(current_uop.sm_id);
        if (search == sm_id_to_smem_per_warp.end()) {
            info.smem_bytes_per_warp = 0;
        } else {
            info.smem_bytes_per_warp = search->second;
        }

        merged_uop_indices.emplace(i);
        BRANCH_DEBUG("merged uop id ", current_uop.uop_id, " sm_id ",
                     current_uop.sm_id, " warp_id_begin ",
                     current_uop.warp_id_begin, " warp_id_end ",
                     current_uop.warp_id_end);

        int current_warp_id_end = info.warp_id_end;

        for (size_t j = i + 1; j < this->uops.size(); j++) {
            if (merged_uop_indices.find(j) != merged_uop_indices.end()) {
                continue;
            }
            UnitOp next_uop = this->uops[j];
            if (next_uop.opseq_id != current_uop.opseq_id) {
                // Scheduling another opseq. There is no more uop to merge.
                // Break.
                break;
            }
            // Scheduling the same opseq.
            if (next_uop.warp_id_end - next_uop.warp_id_begin !=
                info.num_warps_per_uop) {
                // The same opseq should have the same number of warps per
                // uop.
                LOG(ERROR, "invalid num_warps_per_uop: ",
                    next_uop.warp_id_end - next_uop.warp_id_begin,
                    ", expected: ", info.num_warps_per_uop);
            }
            if (next_uop.sm_id == info.sm_id_end - 1) {
                // Scheduling the same opseq on the same SM as the previous
                // uop.
                if (next_uop.warp_id_begin >= info.warp_id_begin &&
                    next_uop.warp_id_begin < current_warp_id_end) {
                    // Scheduling another uop from the same opseq on the
                    // same SM and warp. This should be handled in another
                    // branch. Skip here.
                    continue;
                } else if (next_uop.warp_id_begin != current_warp_id_end) {
                    // Non-contiguous warps. Break.
                    break;
                }
                // Contiguous warps. Try merge.
            } else if (next_uop.sm_id == info.sm_id_end) {
                // Scheduling the same opseq on the next SM.
                if (next_uop.warp_id_begin != info.warp_id_begin) {
                    // Using different warp IDs from the next SM. Break.
                    break;
                }

                search = sm_id_to_smem_per_warp.find(next_uop.sm_id);
                if (search != sm_id_to_smem_per_warp.end()) {
                    if (info.smem_bytes_per_warp != search->second) {
                        // Different SMs have different shared memory bytes
                        // per warp. Break.
                        break;
                    }
                } else {
                    // The next SM is supposed to not use shared memory.
                    // We can merge it. Do nothing.
                }
                // Contiguous SMs and using the same warp IDs. Try merge.
            } else {
                // Non-contiguous SMs. Break.
                break;
            }

            // Try merge.

            if (info.uop_id_diff != 0 &&
                info.uop_id_diff != next_uop.uop_id - info.uop_id_last) {
                // Diff is different from the previous uop. Break.
                break;
            }
            if (info.sm_id_end - info.sm_id_begin > 1 &&
                (next_uop.warp_id_end > info.warp_id_end ||
                 next_uop.warp_id_begin < info.warp_id_begin)) {
                // This branch is scheduling multiple SMs and next_uop
                // uses warp IDs that are not used by previous SMs.
                // Break.
                break;
            }

            // Merge.
            if (info.uop_id_diff == 0) {
                // Diff is undetermined yet. Set it.
                info.uop_id_diff = next_uop.uop_id - info.uop_id_last;
            } else {
                // Diff is the same as the previous uop. Do nothing.
            }
            if (next_uop.sm_id == info.sm_id_end) {
                // Scheduling the same opseq on the next SM.
                info.sm_id_end = next_uop.sm_id + 1;
            }
            current_warp_id_end = next_uop.warp_id_end;
            info.uop_id_last = next_uop.uop_id;
            if (info.warp_id_end < current_warp_id_end) {
                info.warp_id_end = current_warp_id_end;
            }
            merged_uop_indices.emplace(j);
            BRANCH_DEBUG("merged uop id ", next_uop.uop_id, " sm_id ",
                         next_uop.sm_id, " warp_id_begin ",
                         next_uop.warp_id_begin, " warp_id_end ",
                         next_uop.warp_id_end);
        }

        if (current_warp_id_end < info.warp_id_end) {
            // The last scheduled SM uses less warps than the previous
            // scheduled SMs. Break the info into two.

            int num_uops_in_last_sm =
                (current_warp_id_end - info.warp_id_begin) /
                info.num_warps_per_uop;
            int first_uop_id_in_last_sm =
                info.uop_id_last - (num_uops_in_last_sm - 1) * info.uop_id_diff;

            OpBranchInfo new_info;
            new_info.opseq_id = info.opseq_id;
            new_info.sm_id_begin = info.sm_id_end - 1;
            new_info.sm_id_end = info.sm_id_end;
            new_info.warp_id_begin = info.warp_id_begin;
            new_info.warp_id_end = current_warp_id_end;
            new_info.uop_id_begin = first_uop_id_in_last_sm;
            new_info.uop_id_last = info.uop_id_last;
            new_info.uop_id_diff = info.uop_id_diff;
            new_info.num_warps_per_uop = info.num_warps_per_uop;

            search = sm_id_to_smem_per_warp.find(new_info.sm_id_begin);
            if (search == sm_id_to_smem_per_warp.end()) {
                new_info.smem_bytes_per_warp = 0;
            } else {
                new_info.smem_bytes_per_warp = search->second;
            }

            info.sm_id_end -= 1;
            info.uop_id_last = first_uop_id_in_last_sm - info.uop_id_diff;

            if (merged_uop_indices.size() - num_merged_uops !=
                (size_t)(info.get_num_uops() + new_info.get_num_uops())) {
                LOG(ERROR,
                    "unexpected error: numbers of newly merged uops mismatch (",
                    merged_uop_indices.size() - num_merged_uops, " vs ",
                    info.get_num_uops() + new_info.get_num_uops(), ")");
            }

            infos.emplace_back(info);
            infos.emplace_back(new_info);
        } else if (current_warp_id_end != info.warp_id_end) {
            LOG(ERROR, "unexpected error");
        } else {
            if (merged_uop_indices.size() - num_merged_uops !=
                (size_t)info.get_num_uops()) {
                LOG(ERROR,
                    "unexpected error: numbers of newly merged uops mismatch (",
                    merged_uop_indices.size() - num_merged_uops, " vs ",
                    info.get_num_uops(), ")");
            }
            infos.emplace_back(info);
        }
    }

    // Verify if there is a missing uop.
    for (auto &p : this->opseq_to_uop_ids) {
        int expected_id = 0;
        for (int uop_id : p.second) {
            if (uop_id != expected_id) {
                LOG(ERROR, "missing uop ", expected_id, " in opseq ", p.first);
            }
            expected_id += 1;
        }
    }

    // Verify if the number of uops matches.
    int num_uops = 0;
    for (const OpBranchInfo &info : infos) {
        num_uops += info.get_num_uops();
    }
    size_t compare_num_uops = 0;
    for (auto &p : this->opseq_to_uop_ids) {
        compare_num_uops += p.second.size();
    }
    if ((size_t)num_uops != compare_num_uops) {
        LOG(ERROR, "invalid number of uops: ", num_uops,
            ", expected: ", compare_num_uops,
            " merged_uop_indices.size(): ", merged_uop_indices.size());
    }

    return infos;
}

std::vector<Branch> SchedBranch::Impl::get_branches(
    const std::map<int, int> &sm_id_to_smem_per_warp)
{
    std::vector<OpBranchInfo> op_branch_info =
        this->get_op_branch_info(sm_id_to_smem_per_warp);
    std::vector<Branch> branches;
    for (const OpBranchInfo &info : op_branch_info) {
        if (!branches.empty() &&
            branches.back().sm_id_begin == info.sm_id_begin &&
            branches.back().sm_id_end == info.sm_id_end &&
            (branches.back().smem_bytes_per_warp == info.smem_bytes_per_warp ||
             branches.back().smem_bytes_per_warp * info.smem_bytes_per_warp ==
                 0)) {
            Branch &branch = branches.back();
            // Merge with the previous branch.
            BranchOp branch_op;
            branch_op.opseq_id = info.opseq_id;
            branch_op.uop_id_begin = info.uop_id_begin;
            branch_op.uop_id_diff = info.uop_id_diff;
            branch_op.num_warps_per_uop = info.num_warps_per_uop;
            branch_op.num_uops_per_sm =
                (info.warp_id_end - info.warp_id_begin) /
                info.num_warps_per_uop;
            if (branch.smem_bytes_per_warp != info.smem_bytes_per_warp) {
                branch.smem_bytes_per_warp = std::max(
                    branch.smem_bytes_per_warp, info.smem_bytes_per_warp);
            }
            WarpBranch &warp_branch = branch.warp_branches.back();
            if (warp_branch.warp_id_begin == info.warp_id_begin &&
                warp_branch.warp_id_end == info.warp_id_end) {
                // Merge with the previous warp branch.
                warp_branch.branch_ops.emplace_back(std::move(branch_op));
            } else if (warp_branch.warp_id_end <= info.warp_id_begin ||
                       info.warp_id_begin == 0) {
                // Add a new warp branch.
                WarpBranch warp_branch;
                warp_branch.warp_id_begin = info.warp_id_begin;
                warp_branch.warp_id_end = info.warp_id_end;
                warp_branch.branch_ops.emplace_back(std::move(branch_op));
                branch.warp_branches.emplace_back(std::move(warp_branch));
            } else {
                // This may be not possible.
                LOG(ERROR, "unexpected error");
            }
        } else {
            // Add a new branch.
            Branch branch;
            branch.sm_id_begin = info.sm_id_begin;
            branch.sm_id_end = info.sm_id_end;
            branch.smem_bytes_per_warp = info.smem_bytes_per_warp;
            WarpBranch warp_branch;
            warp_branch.warp_id_begin = info.warp_id_begin;
            warp_branch.warp_id_end = info.warp_id_end;
            BranchOp branch_op;
            branch_op.opseq_id = info.opseq_id;
            branch_op.uop_id_begin = info.uop_id_begin;
            branch_op.uop_id_diff = info.uop_id_diff;
            branch_op.num_warps_per_uop = info.num_warps_per_uop;
            branch_op.num_uops_per_sm =
                (info.warp_id_end - info.warp_id_begin) /
                info.num_warps_per_uop;
            warp_branch.branch_ops.emplace_back(std::move(branch_op));
            branch.warp_branches.emplace_back(std::move(warp_branch));
            branches.emplace_back(std::move(branch));
        }
    }
    return branches;
}

SchedBranch::SchedBranch()
{
    this->impl = std::make_unique<Impl>();
}

SchedBranch::~SchedBranch()
{
}

void SchedBranch::add(int opseq_id, int uop_id, int sm_id, int warp_id_begin,
                      int warp_id_end)
{
    this->impl->add(opseq_id, uop_id, sm_id, warp_id_begin, warp_id_end);
}

void SchedBranch::clear()
{
    this->impl->clear();
}

std::vector<Branch> SchedBranch::get_branches(
    const std::map<int, int> &sm_id_to_smem_per_warp)
{
    return this->impl->get_branches(sm_id_to_smem_per_warp);
}

} // namespace ark

```

`ark/sched/sched_branch.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_SCHED_BRANCH_H_
#define ARK_SCHED_BRANCH_H_

#include <map>
#include <memory>
#include <vector>

namespace ark {

/// Information of an operator that runs on a branch.
struct BranchOp
{
    /// The opseq ID.
    int opseq_id;
    /// The uop ID that runs on the first warp among the entire @ref Branch.
    int uop_id_begin;
    /// The difference between the uop ID of the first warp and the uop ID of
    /// the `num_warps_per_uop`-th warp.
    int uop_id_diff;
    /// The number of warps that run the same uop.
    int num_warps_per_uop;
    /// The number of uops that runs in a SM at the same time.
    int num_uops_per_sm;
};

/// A branch of execution that runs on a SM.
struct WarpBranch
{
    /// The warp ID range of this branch [warp_id_begin, warp_id_end).
    int warp_id_begin;
    /// The warp ID range of this branch [warp_id_begin, warp_id_end).
    int warp_id_end;
    /// The list of operators that run on this branch.
    std::vector<BranchOp> branch_ops;
};

/// A branch of execution that runs over multiple SMs.
struct Branch
{
    /// The SM ID range of this branch [sm_id_begin, sm_id_end).
    int sm_id_begin;
    /// The SM ID range of this branch [sm_id_begin, sm_id_end).
    int sm_id_end;
    /// Bytes of shared memory allowed per warp.
    int smem_bytes_per_warp;
    /// The list of warp branches that run on every SMs in the range.
    std::vector<WarpBranch> warp_branches;
};

/// A class that records the branches of execution.
class SchedBranch
{
  public:
    /// Construct a @ref SchedBranch.
    SchedBranch();

    /// Destruct a @ref SchedBranch.
    ~SchedBranch();

    /// Add an execution branch.
    /// @param opseq_id The opseq ID.
    /// @param uop_id The uop ID.
    /// @param sm_id The SM ID to run the uop.
    /// @param warp_id_begin The warp ID range [warp_id_begin, warp_id_end) to
    /// run the uop.
    /// @param warp_id_end The warp ID range [warp_id_begin, warp_id_end) to run
    /// the uop.
    void add(int opseq_id, int uop_id, int sm_id, int warp_id_begin,
             int warp_id_end);

    /// Clear all the branches.
    void clear();

    /// Get the branches.
    std::vector<Branch> get_branches(
        const std::map<int, int> &sm_id_to_smem_per_warp);

  private:
    class Impl;
    std::unique_ptr<Impl> impl;
};

} // namespace ark

#endif // ARK_SCHED_BRANCH_H_

```

`ark/sched/sched_branch_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "ark.h"
#include "logging.h"
#include "sched_branch.h"
#include "unittest/unittest_utils.h"

ark::unittest::State test_sched_branch_single_opseq()
{
    std::map<int, int> sm_id_to_smem_per_warp;

    {
        // Test:
        //   if (0 <= sm_id < 1 && 0 <= warp_id < 4) {
        //     op = 0; uop = 1 * warp_id + 0;
        //   }

        ark::SchedBranch sb;
        for (int uop_id = 0; uop_id < 4; ++uop_id) {
            int sm_id = (uop_id / 4) % 5;
            int warp_id = uop_id % 4;
            sb.add(/*opseq_id*/ 0, /*uop_id*/ uop_id, /*sm_id*/ sm_id,
                   /*warp_id_begin*/ warp_id, /*warp_id_end*/ warp_id + 1);
        }

        std::vector<ark::Branch> branches =
            sb.get_branches(sm_id_to_smem_per_warp);

        UNITTEST_EQ(branches.size(), 1UL);
        UNITTEST_EQ(branches[0].sm_id_begin, 0);
        UNITTEST_EQ(branches[0].sm_id_end, 1);
        UNITTEST_EQ(branches[0].warp_branches.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_end, 4);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].opseq_id, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[0].warp_branches[0].branch_ops[0].num_warps_per_uop, 1);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].num_uops_per_sm,
                    4);
    }

    {
        // Test:
        //   if (0 <= sm_id < 3 && 0 <= warp_id < 4) {
        //     op = 0; uop = 1 * (warp_id + 4 * sm_id) + 0;
        //   }

        ark::SchedBranch sb;
        for (int uop_id = 0; uop_id < 12; ++uop_id) {
            int sm_id = (uop_id / 4) % 5;
            int warp_id = uop_id % 4;
            sb.add(/*opseq_id*/ 0, /*uop_id*/ uop_id, /*sm_id*/ sm_id,
                   /*warp_id_begin*/ warp_id, /*warp_id_end*/ warp_id + 1);
        }

        std::vector<ark::Branch> branches =
            sb.get_branches(sm_id_to_smem_per_warp);

        UNITTEST_EQ(branches.size(), 1UL);
        UNITTEST_EQ(branches[0].sm_id_begin, 0);
        UNITTEST_EQ(branches[0].sm_id_end, 3);
        UNITTEST_EQ(branches[0].warp_branches.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_end, 4);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].opseq_id, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[0].warp_branches[0].branch_ops[0].num_warps_per_uop, 1);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].num_uops_per_sm,
                    4);
    }

    {
        // Test:
        //   if (0 <= sm_id < 5 && 0 <= warp_id < 4) {
        //     op = 0; uop = 1 * (warp_id + 4 * sm_id) + 0;
        //   }
        //   if (0 <= sm_id < 2 && 0 <= warp_id < 4) {
        //     op = 0; uop = 1 * (warp_id + 4 * sm_id) + 20;
        //   }

        ark::SchedBranch sb;
        for (int uop_id = 0; uop_id < 28; ++uop_id) {
            int sm_id = (uop_id / 4) % 5;
            int warp_id = uop_id % 4;
            sb.add(/*opseq_id*/ 0, /*uop_id*/ uop_id, /*sm_id*/ sm_id,
                   /*warp_id_begin*/ warp_id, /*warp_id_end*/ warp_id + 1);
        }

        std::vector<ark::Branch> branches =
            sb.get_branches(sm_id_to_smem_per_warp);

        UNITTEST_EQ(branches.size(), 2UL);

        UNITTEST_EQ(branches[0].sm_id_begin, 0);
        UNITTEST_EQ(branches[0].sm_id_end, 5);
        UNITTEST_EQ(branches[0].warp_branches.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_end, 4);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[0].warp_branches[0].branch_ops[0].num_warps_per_uop, 1);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].num_uops_per_sm,
                    4);

        UNITTEST_EQ(branches[1].sm_id_begin, 0);
        UNITTEST_EQ(branches[1].sm_id_end, 2);
        UNITTEST_EQ(branches[1].warp_branches.size(), 1UL);
        UNITTEST_EQ(branches[1].warp_branches[0].warp_id_begin, 0);
        UNITTEST_EQ(branches[1].warp_branches[0].warp_id_end, 4);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].uop_id_begin,
                    20);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[0].warp_branches[0].branch_ops[0].num_warps_per_uop, 1);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].num_uops_per_sm,
                    4);
    }

    {
        // Test:
        //   if (0 <= sm_id < 5 && 0 <= warp_id < 4) {
        //     op = 0; uop = 1 * (warp_id + 4 * sm_id) + 0;
        //   }
        //   if (0 <= sm_id < 2 && 0 <= warp_id < 4) {
        //     op = 0; uop = 1 * (warp_id + 4 * sm_id) + 20;
        //   }
        //   if (sm_id == 2 && 0 <= warp_id < 2) {
        //     op = 0; uop = 1 * (warp_id) + 28;
        //   }

        ark::SchedBranch sb;
        for (int uop_id = 0; uop_id < 30; ++uop_id) {
            int sm_id = (uop_id / 4) % 5;
            int warp_id = uop_id % 4;
            sb.add(/*opseq_id*/ 0, /*uop_id*/ uop_id, /*sm_id*/ sm_id,
                   /*warp_id_begin*/ warp_id, /*warp_id_end*/ warp_id + 1);
        }

        std::vector<ark::Branch> branches =
            sb.get_branches(sm_id_to_smem_per_warp);

        UNITTEST_EQ(branches.size(), 3UL);

        UNITTEST_EQ(branches[0].sm_id_begin, 0);
        UNITTEST_EQ(branches[0].sm_id_end, 5);
        UNITTEST_EQ(branches[0].warp_branches.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_end, 4);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[0].warp_branches[0].branch_ops[0].num_warps_per_uop, 1);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].num_uops_per_sm,
                    4);

        UNITTEST_EQ(branches[1].sm_id_begin, 0);
        UNITTEST_EQ(branches[1].sm_id_end, 2);
        UNITTEST_EQ(branches[1].warp_branches.size(), 1UL);
        UNITTEST_EQ(branches[1].warp_branches[0].warp_id_begin, 0);
        UNITTEST_EQ(branches[1].warp_branches[0].warp_id_end, 4);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].uop_id_begin,
                    20);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[0].warp_branches[0].branch_ops[0].num_warps_per_uop, 1);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].num_uops_per_sm,
                    4);

        UNITTEST_EQ(branches[2].sm_id_begin, 2);
        UNITTEST_EQ(branches[2].sm_id_end, 3);
        UNITTEST_EQ(branches[2].warp_branches.size(), 1UL);
        UNITTEST_EQ(branches[2].warp_branches[0].warp_id_begin, 0);
        UNITTEST_EQ(branches[2].warp_branches[0].warp_id_end, 2);
        UNITTEST_EQ(branches[2].warp_branches[0].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[2].warp_branches[0].branch_ops[0].uop_id_begin,
                    28);
        UNITTEST_EQ(branches[2].warp_branches[0].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[0].warp_branches[0].branch_ops[0].num_warps_per_uop, 1);
        UNITTEST_EQ(branches[2].warp_branches[0].branch_ops[0].num_uops_per_sm,
                    2);
    }

    {
        // Test:
        //   if (0 <= sm_id < 5 && 0 <= warp_id < 2) {
        //     op = 0; uop = 1 * sm_id + 0;
        //   }
        //   if (0 <= sm_id < 2 && 0 <= warp_id < 2) {
        //     op = 0; uop = 1 * sm_id + 5;
        //   }

        ark::SchedBranch sb;
        for (int uop_id = 0; uop_id < 7; ++uop_id) {
            int sm_id = uop_id % 5;
            sb.add(/*opseq_id*/ 0, /*uop_id*/ uop_id, /*sm_id*/ sm_id,
                   /*warp_id_begin*/ 0, /*warp_id_end*/ 2);
        }

        std::vector<ark::Branch> branches =
            sb.get_branches(sm_id_to_smem_per_warp);

        UNITTEST_EQ(branches.size(), 2UL);

        UNITTEST_EQ(branches[0].sm_id_begin, 0);
        UNITTEST_EQ(branches[0].sm_id_end, 5);
        UNITTEST_EQ(branches[0].warp_branches.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_end, 2);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[0].warp_branches[0].branch_ops[0].num_warps_per_uop, 2);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].num_uops_per_sm,
                    1);

        UNITTEST_EQ(branches[1].sm_id_begin, 0);
        UNITTEST_EQ(branches[1].sm_id_end, 2);
        UNITTEST_EQ(branches[1].warp_branches.size(), 1UL);
        UNITTEST_EQ(branches[1].warp_branches[0].warp_id_begin, 0);
        UNITTEST_EQ(branches[1].warp_branches[0].warp_id_end, 2);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].uop_id_begin, 5);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[0].warp_branches[0].branch_ops[0].num_warps_per_uop, 2);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].num_uops_per_sm,
                    1);
    }

    {
        // Test:
        //   if (2 <= sm_id < 3 && 2 <= warp_id < 4) {
        //     op = 0; uop = 1 * ((warp_id - 2) + 2 * (sm_id - 2)) + 3;
        //   }
        //   if (3 <= sm_id < 4 && 0 <= warp_id < 3) {
        //     op = 0; uop = 1 * (warp_id + 3 * (sm_id - 3)) + 5;
        //   }

        ark::SchedBranch sb;
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 0, /*sm_id*/ 2, /*warp_id_begin*/ 2,
               /*warp_id_end*/ 3);
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 1, /*sm_id*/ 2, /*warp_id_begin*/ 3,
               /*warp_id_end*/ 4);
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 2, /*sm_id*/ 3, /*warp_id_begin*/ 0,
               /*warp_id_end*/ 1);
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 3, /*sm_id*/ 3, /*warp_id_begin*/ 1,
               /*warp_id_end*/ 2);
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 4, /*sm_id*/ 3, /*warp_id_begin*/ 2,
               /*warp_id_end*/ 3);

        std::vector<ark::Branch> branches =
            sb.get_branches(sm_id_to_smem_per_warp);

        UNITTEST_EQ(branches.size(), 2UL);

        UNITTEST_EQ(branches[0].sm_id_begin, 2);
        UNITTEST_EQ(branches[0].sm_id_end, 3);
        UNITTEST_EQ(branches[0].warp_branches.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_begin, 2);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_end, 4);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].opseq_id, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[0].warp_branches[0].branch_ops[0].num_warps_per_uop, 1);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].num_uops_per_sm,
                    2);

        UNITTEST_EQ(branches[1].sm_id_begin, 3);
        UNITTEST_EQ(branches[1].sm_id_end, 4);
        UNITTEST_EQ(branches[1].warp_branches.size(), 1UL);
        UNITTEST_EQ(branches[1].warp_branches[0].warp_id_begin, 0);
        UNITTEST_EQ(branches[1].warp_branches[0].warp_id_end, 3);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].opseq_id, 0);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].uop_id_begin, 2);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[0].warp_branches[0].branch_ops[0].num_warps_per_uop, 1);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].num_uops_per_sm,
                    3);
    }

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_sched_branch_multi_opseq()
{
    std::map<int, int> sm_id_to_smem_per_warp;

    {
        ark::SchedBranch sb;
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 0, /*sm_id*/ 0, /*warp_id_begin*/ 0,
               /*warp_id_end*/ 1);
        sb.add(/*opseq_id*/ 1, /*uop_id*/ 0, /*sm_id*/ 0, /*warp_id_begin*/ 2,
               /*warp_id_end*/ 3);
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 1, /*sm_id*/ 0, /*warp_id_begin*/ 1,
               /*warp_id_end*/ 2);
        sb.add(/*opseq_id*/ 1, /*uop_id*/ 1, /*sm_id*/ 0, /*warp_id_begin*/ 3,
               /*warp_id_end*/ 4);

        sb.add(/*opseq_id*/ 0, /*uop_id*/ 2, /*sm_id*/ 1, /*warp_id_begin*/ 0,
               /*warp_id_end*/ 1);
        sb.add(/*opseq_id*/ 1, /*uop_id*/ 2, /*sm_id*/ 1, /*warp_id_begin*/ 2,
               /*warp_id_end*/ 3);
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 3, /*sm_id*/ 1, /*warp_id_begin*/ 1,
               /*warp_id_end*/ 2);
        sb.add(/*opseq_id*/ 1, /*uop_id*/ 3, /*sm_id*/ 1, /*warp_id_begin*/ 3,
               /*warp_id_end*/ 4);

        sb.add(/*opseq_id*/ 0, /*uop_id*/ 4, /*sm_id*/ 2, /*warp_id_begin*/ 0,
               /*warp_id_end*/ 1);
        sb.add(/*opseq_id*/ 1, /*uop_id*/ 4, /*sm_id*/ 2, /*warp_id_begin*/ 2,
               /*warp_id_end*/ 3);
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 5, /*sm_id*/ 2, /*warp_id_begin*/ 1,
               /*warp_id_end*/ 2);
        sb.add(/*opseq_id*/ 1, /*uop_id*/ 5, /*sm_id*/ 2, /*warp_id_begin*/ 3,
               /*warp_id_end*/ 4);

        sb.add(/*opseq_id*/ 0, /*uop_id*/ 6, /*sm_id*/ 3, /*warp_id_begin*/ 0,
               /*warp_id_end*/ 1);
        sb.add(/*opseq_id*/ 1, /*uop_id*/ 6, /*sm_id*/ 3, /*warp_id_begin*/ 2,
               /*warp_id_end*/ 3);
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 7, /*sm_id*/ 3, /*warp_id_begin*/ 1,
               /*warp_id_end*/ 2);
        sb.add(/*opseq_id*/ 1, /*uop_id*/ 7, /*sm_id*/ 3, /*warp_id_begin*/ 3,
               /*warp_id_end*/ 4);

        sb.add(/*opseq_id*/ 0, /*uop_id*/ 8, /*sm_id*/ 4, /*warp_id_begin*/ 0,
               /*warp_id_end*/ 1);
        sb.add(/*opseq_id*/ 1, /*uop_id*/ 8, /*sm_id*/ 4, /*warp_id_begin*/ 2,
               /*warp_id_end*/ 3);
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 9, /*sm_id*/ 4, /*warp_id_begin*/ 1,
               /*warp_id_end*/ 2);
        sb.add(/*opseq_id*/ 1, /*uop_id*/ 9, /*sm_id*/ 4, /*warp_id_begin*/ 3,
               /*warp_id_end*/ 4);

        std::vector<ark::Branch> branches =
            sb.get_branches(sm_id_to_smem_per_warp);

        UNITTEST_EQ(branches.size(), 1UL);

        UNITTEST_EQ(branches[0].sm_id_begin, 0);
        UNITTEST_EQ(branches[0].sm_id_end, 5);
        UNITTEST_EQ(branches[0].warp_branches.size(), 2UL);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_end, 2);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].opseq_id, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[0].warp_branches[0].branch_ops[0].num_warps_per_uop, 1);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].num_uops_per_sm,
                    2);
        UNITTEST_EQ(branches[0].warp_branches[1].warp_id_begin, 2);
        UNITTEST_EQ(branches[0].warp_branches[1].warp_id_end, 4);
        UNITTEST_EQ(branches[0].warp_branches[1].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[1].branch_ops[0].opseq_id, 1);
        UNITTEST_EQ(branches[0].warp_branches[1].branch_ops[0].uop_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[1].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[0].warp_branches[1].branch_ops[0].num_warps_per_uop, 1);
        UNITTEST_EQ(branches[0].warp_branches[1].branch_ops[0].num_uops_per_sm,
                    2);
    }

    {
        // Test:
        //   if (sm_id == 0 && warp_id == 0) {
        //     op = 0; uop = 0;
        //   }
        //   if (sm_id == 1 && 0 <= warp_id < 4) {
        //     op = 0; uop = warp_id + 1;
        //   }
        //
        // However, due to the greedy nature of the algorithm, the result will
        // be:
        //   if (0 <= sm_id < 2 && warp_id == 0) {
        //     op = 0; uop = sm_id;
        //   }
        //   if (sm_id == 1 && 1 <= warp_id < 4) {
        //     op = 0; uop = warp_id + 1;
        //   }

        ark::SchedBranch sb;
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 0, /*sm_id*/ 0, /*warp_id_begin*/ 0,
               /*warp_id_end*/ 1);
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 1, /*sm_id*/ 1, /*warp_id_begin*/ 0,
               /*warp_id_end*/ 1);
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 2, /*sm_id*/ 1, /*warp_id_begin*/ 1,
               /*warp_id_end*/ 2);
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 3, /*sm_id*/ 1, /*warp_id_begin*/ 2,
               /*warp_id_end*/ 3);
        sb.add(/*opseq_id*/ 0, /*uop_id*/ 4, /*sm_id*/ 1, /*warp_id_begin*/ 3,
               /*warp_id_end*/ 4);

        std::vector<ark::Branch> branches =
            sb.get_branches(sm_id_to_smem_per_warp);

        UNITTEST_EQ(branches.size(), 2UL);
        UNITTEST_EQ(branches[0].sm_id_begin, 0);
        UNITTEST_EQ(branches[0].sm_id_end, 2);
        UNITTEST_EQ(branches[0].warp_branches.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_begin, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].warp_id_end, 1);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].opseq_id, 0);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].uop_id_begin, 0);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[0].warp_branches[0].branch_ops[0].num_warps_per_uop, 1);
        UNITTEST_EQ(branches[0].warp_branches[0].branch_ops[0].num_uops_per_sm,
                    1);
        UNITTEST_EQ(branches[1].sm_id_begin, 1);
        UNITTEST_EQ(branches[1].sm_id_end, 2);
        UNITTEST_EQ(branches[1].warp_branches.size(), 1UL);
        UNITTEST_EQ(branches[1].warp_branches[0].warp_id_begin, 1);
        UNITTEST_EQ(branches[1].warp_branches[0].warp_id_end, 4);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops.size(), 1UL);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].opseq_id, 0);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].uop_id_begin, 2);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].uop_id_diff, 1);
        UNITTEST_EQ(
            branches[1].warp_branches[0].branch_ops[0].num_warps_per_uop, 1);
        UNITTEST_EQ(branches[1].warp_branches[0].branch_ops[0].num_uops_per_sm,
                    3);
    }

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_sched_branch_clear()
{
    std::map<int, int> sm_id_to_smem_per_warp;
    {
        ark::SchedBranch sb;
        for (int uop_id = 0; uop_id < 4; ++uop_id) {
            int sm_id = (uop_id / 4) % 5;
            int warp_id = uop_id % 4;
            sb.add(/*opseq_id*/ 0, /*uop_id*/ uop_id, /*sm_id*/ sm_id,
                   /*warp_id_begin*/ warp_id, /*warp_id_end*/ warp_id + 1);
        }

        std::vector<ark::Branch> branches =
            sb.get_branches(sm_id_to_smem_per_warp);

        UNITTEST_EQ(branches.size(), 1UL);

        branches.clear();

        UNITTEST_EQ(branches.size(), 0UL);
    }
    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_sched_branch_single_opseq);
    UNITTEST(test_sched_branch_multi_opseq);
    UNITTEST(test_sched_branch_clear);
    return 0;
}

```

`ark/sched/sched_codegen.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <cassert>
#include <fstream>
#include <initializer_list>
#include <ostream>
#include <unistd.h>

#include "env.h"
#include "logging.h"
#include "math.h"
#include "sched/sched_codegen.h"

using namespace std;

#define COM ", "
#define OP_PREFIX "op"
#define UNIT_OP_PREFIX "uop"

namespace ark {

CodeGenerator::CodeGenerator(const std::map<TensorBuf *, GpuBuf *> &buf_trans,
                             const GpuInfo &gpu_info_, int num_warps_per_sm_)
    : buf_trans{buf_trans}, gpu_info{gpu_info_}, sm_num{gpu_info_.num_sm},
      num_warps_per_sm{num_warps_per_sm_}, num_indent{0}
{
}

size_t CodeGenerator::get_tensor_offset(const Tensor *tensor) const
{
    size_t off = this->buf_trans.find(tensor->buf)->second->get_offset();
    assert(off % 8 == 0);
    return off + tensor->offset_bytes();
}

std::ostream &CodeGenerator::def_remote_buf(std::ostream &os,
                                            int remote_rank) const
{
    os << "__device__ char *" ARK_BUF_NAME << remote_rank << ";\n";
    return os;
}

std::ostream &CodeGenerator::sync_gpu(std::ostream &os) const
{
    os << "ark::sync_gpu<" << this->sm_num << ">(" ARK_LSS_NAME ");\n";
    return os;
}

std::ostream &CodeGenerator::def_sync_stream(std::ostream &os,
                                             int stream_id) const
{
    os << "__device__ ark::sync::State " ARK_LSS_NAME "_" << stream_id << ";\n";
    return os;
}

std::ostream &CodeGenerator::sync_stream(std::ostream &os, int stream_id,
                                         int sm_id_begin, int sm_id_end) const
{
    if (sm_id_begin >= sm_id_end) {
        LOG(ERROR, "invalid SM range");
    }
    if (sm_id_begin == 0) {
        os << "if (blockIdx.x < " << sm_id_end << ") {";
    } else if (sm_id_begin + 1 == sm_id_end) {
        os << "if (blockIdx.x == " << sm_id_begin << ") {";
    } else {
        os << "if (blockIdx.x >= " << sm_id_begin << " && blockIdx.x < "
           << sm_id_end << ") {";
    }
    os << " ark::sync_gpu<" << sm_id_end - sm_id_begin << ">(" ARK_LSS_NAME "_"
       << stream_id << "); }\n";
    return os;
}

ostream &CodeGenerator::tensor(ostream &os, const Tensor *tensor) const
{
    size_t off = this->get_tensor_offset(tensor);
    if (tensor->type == FP16) {
        os << "(ark::half *)";
    } else if (tensor->type == FP32) {
        os << "(float *)";
    } else if (tensor->type == INT32) {
        os << "(int *)";
    } else if (tensor->type == BYTE) {
        os << "(void *)";
    } else {
        LOGERR("unknown tensor type");
    }
    std::string buf_name = ARK_BUF_NAME;
    if (tensor->imported_rank >= 0) {
        buf_name += std::to_string(tensor->imported_rank);
    }
    os << "&" << buf_name << "[" << off << "]";
    return os;
}

std::ostream &CodeGenerator::def_oparg(std::ostream &os, const OpArg &arg,
                                       const std::string &name) const
{
    if (arg.type == OP_ARG_TENSOR) {
        Tensor *tns;
        arg.get(&tns);
        switch (tns->type) {
        case FP16:
            os << "ark::half *" << name;
            break;
        case FP32:
            os << "float *" << name;
            break;
        case INT32:
            os << "int *" << name;
            break;
        case BYTE:
            os << "void *" << name;
            break;
        default:
            LOGERR("Not implemented");
            break;
        }
    } else if (arg.type == OP_ARG_FLOAT) {
        os << "float " << name;
    } else if (arg.type == OP_ARG_INT) {
        os << "int " << name;
    } else if (arg.type == OP_ARG_BOOL) {
        os << "bool " << name;
    } else if (arg.type == OP_ARG_INT64) {
        os << "long long int " << name;
    } else if (arg.type == OP_ARG_UINT64) {
        os << "uint64_t " << name;
    } else {
        LOGERR("Not implemented");
    }
    return os;
}

std::ostream &CodeGenerator::oparg(std::ostream &os, const OpArg &arg) const
{
    if (arg.type == OP_ARG_TENSOR) {
        Tensor *tns;
        arg.get(&tns);
        this->tensor(os, tns);
    } else if (arg.type == OP_ARG_FLOAT) {
        float val;
        arg.get(&val);
        os << val;
    } else if (arg.type == OP_ARG_INT) {
        int val;
        arg.get(&val);
        os << val;
    } else if (arg.type == OP_ARG_BOOL) {
        bool val;
        arg.get(&val);
        os << val;
    } else if (arg.type == OP_ARG_INT64) {
        long long int val;
        arg.get(&val);
        os << val;
    } else if (arg.type == OP_ARG_UINT64) {
        uint64_t val;
        arg.get(&val);
        os << val;
    } else {
        LOGERR("Not implemented");
    }
    return os;
}

std::ostream &CodeGenerator::branch(std::ostream &os, const Branch &br,
                                    int prev_sm_id_end) const
{
    if (br.warp_branches.empty()) {
        return os;
    }
    if (prev_sm_id_end < 0) {
        prev_sm_id_end = this->sm_num;
    }
    if (br.sm_id_begin == 0) {
        if (br.sm_id_end == this->sm_num) {
            os << "\n  { // for all SMs";
        } else {
            os << "\n  if (blockIdx.x < " << br.sm_id_end << ") {";
        }
    } else if (br.sm_id_begin == prev_sm_id_end) {
        if (br.sm_id_end == this->sm_num) {
            os << " else {";
        } else {
            os << " else if (blockIdx.x < " << br.sm_id_end << ") {";
        }
    } else if (br.sm_id_begin < prev_sm_id_end) {
        if (br.sm_id_begin == br.sm_id_end) {
            os << "\n  if (blockIdx.x == " << br.sm_id_begin << ") {";
        } else {
            os << "\n  if (blockIdx.x >= " << br.sm_id_begin
               << " && blockIdx.x < " << br.sm_id_end << ") {";
        }
    } else {
        if (br.sm_id_begin == br.sm_id_end) {
            os << " else if (blockIdx.x == " << br.sm_id_begin << ") {";
        } else {
            os << " else if (blockIdx.x >= " << br.sm_id_begin
               << " && blockIdx.x < " << br.sm_id_end << ") {";
        }
    }

    int tpw = this->gpu_info.threads_per_warp;

    for (auto &warp_branch : br.warp_branches) {
        int thread_begin = warp_branch.warp_id_begin * tpw;
        int thread_end = warp_branch.warp_id_end * tpw;
        if (warp_branch.warp_id_begin == 0) {
            if (warp_branch.warp_id_end == this->num_warps_per_sm) {
                os << "\n    { // for all threads\n";
            } else {
                os << "\n    if (threadIdx.x < " << thread_end << ") {\n";
            }
        } else {
            os << "\n    if (threadIdx.x >= " << thread_begin
               << " && threadIdx.x < " << thread_end << ") {\n";
        }

        for (auto &branch_op : warp_branch.branch_ops) {
            os << "      " << OP_PREFIX << branch_op.opseq_id << '(';
            // num_uops = (warp_id_end - warp_id_begin) / num_warps_per_uop;
            // warp_idx = warp_id - warp_id_begin;
            // sm_idx = sm_id - sm_id_begin;
            // uop = uop_id_diff * (warp_idx / num_warps_per_uop +
            //                      num_uops * sm_idx) + uop_id_begin;
            int num_warps = warp_branch.warp_id_end - warp_branch.warp_id_begin;
            int num_uops = num_warps / branch_op.num_warps_per_uop;
            int num_threads_per_uop = branch_op.num_warps_per_uop * tpw;
            if (branch_op.uop_id_diff != 0) {
                std::stringstream thread_indexing;
                std::stringstream sm_indexing;
                if (thread_end - thread_begin > num_threads_per_uop) {
                    if (thread_begin > 0) {
                        thread_indexing << "((threadIdx.x - " << thread_begin
                                        << ")";
                    } else {
                        thread_indexing << "(threadIdx.x";
                    }
                    if (math::is_pow2(num_threads_per_uop)) {
                        thread_indexing << " >> "
                                        << math::ilog2(num_threads_per_uop)
                                        << ")";
                    } else {
                        thread_indexing << " / " << num_threads_per_uop << ")";
                    }
                }
                if (br.sm_id_end - br.sm_id_begin > 1) {
                    if (br.sm_id_begin > 0) {
                        sm_indexing << "((blockIdx.x - " << br.sm_id_begin
                                    << ")";
                    } else {
                        sm_indexing << "(blockIdx.x";
                    }
                    if (num_uops > 1) {
                        sm_indexing << " * " << num_uops;
                    }
                    sm_indexing << ")";
                }
                std::string indexing;
                if (thread_indexing.str().empty()) {
                    indexing = sm_indexing.str();
                } else if (sm_indexing.str().empty()) {
                    indexing = thread_indexing.str();
                } else {
                    indexing = "(" + sm_indexing.str() + " + " +
                               thread_indexing.str() + ")";
                }
                if (!indexing.empty()) {
                    if (branch_op.uop_id_diff != 1) {
                        os << branch_op.uop_id_diff << " * ";
                    }
                    os << indexing << " + ";
                }
            }
            os << branch_op.uop_id_begin << ", " << br.smem_bytes_per_warp
               << ");\n";
        }
        os << "    }\n";
    }
    os << "  }\n";
    return os;
}

ostream &CodeGenerator::def_uop(ostream &os, const SchedOp &sop,
                                int uop_id) const
{
    std::string uop_name = UNIT_OP_PREFIX + std::to_string(uop_id);
    std::string func_name = sop.function_name();
    assert(!func_name.empty());

    const Op *op = sop.get_op();
    if (op->force_inline) {
        os << "DEVICE ";
    } else {
        os << "__noinline__ __device__ ";
    }
    os << "void " << uop_name << "(";

    OpArgs call_args = op->function_call_args(*sop.get_cfg());
    int cnt_param = 0;
    for (const OpArg &arg : call_args.get_args()) {
        this->def_oparg(os, arg, "_" + std::to_string(cnt_param)) << ", ";
        ++cnt_param;
    }

    os << "int _uop_idx, int _smem_per_warp) {\n";
    os << "  " << func_name << "(";

    for (int i = 0; i < cnt_param; ++i) {
        os << '_' << i << ", ";
    }
    os << "_uop_idx, _smem_per_warp);\n}\n";
    return os;
}

std::ostream &CodeGenerator::uop(std::ostream &os, int uop_id) const
{
    os << UNIT_OP_PREFIX << uop_id;
    return os;
}

//
ostream &CodeGenerator::opseq(ostream &os, const string &name,
                              const SchedOpSeq &opseq,
                              map<string, int> &uop_map) const
{
    auto &sched_ops = opseq.get_sched_ops();
    unsigned int idx = sched_ops.size();
    auto it = sched_ops.rbegin();
    for (; it != sched_ops.rend(); ++it) {
        auto &sop = *it;
        if (sop.is_virtual()) {
            continue;
        }
        if (idx == sched_ops.size()) {
            os << "// tile dims: (" << opseq.get_tdims()[0] << COM
               << opseq.get_tdims()[1] << COM << opseq.get_tdims()[2] << ")\n"
               << "__noinline__ __device__ void " << name
               << "(int _uop_idx, int _smem_per_warp) {\n";
        }
        --idx;
        os << "  ";
        auto uop_map_it = uop_map.find(sop.function_name());
        if (uop_map_it != uop_map.end()) {
            this->uop(os, uop_map_it->second);
        } else {
            os << sop.function_name();
        }
        os << '(';

        OpArgs call_args = sop.get_op()->function_call_args(*sop.get_cfg());
        for (const OpArg &arg : call_args.get_args()) {
            this->oparg(os, arg) << ", ";
        }

        os << "_uop_idx, _smem_per_warp);\n";
    }
    if (idx != sched_ops.size()) {
        os << "}\n";
    }
    return os;
}

std::ostream &CodeGenerator::sched(std::ostream &os, Sched &sched) const
{

    os << "if(";
    if (sched.sm_b > 0)
        os << "blockIdx.x >= " << sched.sm_b << "&& ";
    os << "blockIdx.x < " << sched.sm_e << "){\n";
    os << "  if(";
    if (sched.th_b > 0)
        os << "threadIdx.x >= " << sched.th_b << " && ";
    os << "threadIdx.x<" << sched.th_e << "){\n";
    SchedOpSeq *opseq = sched.opseq;
    os << "    opseq_" << opseq->get_id() << "_tile_task"
       << "(" << sched.alpha << "*(blockIdx.x - " << sched.sm_b << ") + "
       << "threadIdx.x / " << opseq->get_num_warps() * 32 << " + " << sched.beta
       << ");\n";
    os << "    "
       << "ark::sync_warps<" << opseq->get_num_warps() * 32 << ">();\n";
    os << "  }\n"
       << " }\n";
    return os;
}

} // namespace ark

```

`ark/sched/sched_codegen.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_SCHED_CODEGEN_H_
#define ARK_SCHED_CODEGEN_H_

#include "gpu/gpu_kernel.h"
#include "sched/sched_op.h"
#include "sched/sched_opseq.h"
#include "sched_branch.h"
#include <map>

namespace ark {

class CodeGenerator
{
  public:
    CodeGenerator(const std::map<TensorBuf *, GpuBuf *> &buf_trans,
                  const GpuInfo &gpu_info_, int num_warps_per_sm_);

    std::ostream &def_remote_buf(std::ostream &os, int remote_rank) const;

    std::ostream &sync_gpu(std::ostream &os) const;

    std::ostream &def_sync_stream(std::ostream &os, int stream_id) const;
    std::ostream &sync_stream(std::ostream &os, int stream_id, int sm_id_begin,
                              int sm_id_end) const;

    std::ostream &tensor(std::ostream &os, const Tensor *tensor) const;

    std::ostream &def_oparg(std::ostream &os, const OpArg &arg,
                            const std::string &name) const;
    std::ostream &oparg(std::ostream &os, const OpArg &arg) const;

    std::ostream &branch(std::ostream &os, const Branch &branch,
                         int prev_sm_id_end = -1) const;

    std::ostream &def_uop(std::ostream &os, const SchedOp &sop,
                          int uop_id) const;

    std::ostream &uop(std::ostream &os, int uop_id) const;

    std::ostream &opseq(std::ostream &os, const std::string &name,
                        const SchedOpSeq &opseq,
                        std::map<std::string, int> &uop_map) const;

    std::ostream &sched(std::ostream &os, Sched &sched) const;

  protected:
    size_t get_tensor_offset(const Tensor *tensor) const;

    const std::map<TensorBuf *, GpuBuf *> &buf_trans;
    const GpuInfo &gpu_info;
    int sm_num;
    int num_warps_per_sm;
    int world_size;
    int num_indent;
};

} // namespace ark

#endif // ARK_SCHED_CODEGEN_H_

```

`ark/sched/sched_op.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "sched/sched_op.h"
#include "logging.h"
#include "math.h"

using namespace std;

namespace ark {

SchedOp::SchedOp(const Op *op_, const OpConfig *cfg_, const string name)
    : op{op_}, cfg{cfg_}, name{name}, tnums{}
{
    if (op_ == nullptr) {
        return;
    }
    if (cfg_ == nullptr) {
        LOG(DEBUG, "virtual op: ", op_->name);
        return;
    }
    LOG(DEBUG, "op: ", op_->name, ", cfg: num_warps ", cfg_->num_warps,
        " smem_bytes ", cfg_->smem_bytes, " #inputs ", cfg_->input_tiles.size(),
        " #outputs ", cfg_->output_tiles.size(), " sync_pre ", cfg_->sync_pre,
        " sync_post ", cfg_->sync_post);
    // pad the tensor of the SchedOp
    for (unsigned int i = 0; i < this->op->inputs.size(); ++i) {
        if (i >= this->cfg->input_tiles.size()) {
            LOG(DEBUG, "input tensor can not be all padded");
            break;
        }
        // Update pads based on the tile shape. The tiling is applied to the
        // last two dimensions of the tensor. If the tensor is 1D, the first
        // dimension of the tile shape should be 1.
        auto tile = this->cfg->input_tiles[i];
        if (tile.x < 0) {
            tile.x = 1;
        }
        if (tile.y < 0) {
            tile.y = 1;
        }
        int ndims = this->op->inputs[i]->ndims();
        vector<DimType> pads;
        if (ndims == 1) {
            if (tile.x != 1) {
                LOGERR("invalid tile shape for 1D tensor: {", tile.x, ", ",
                       tile.y, "}");
            }
            pads.emplace_back(tile.y);
        } else {
            for (int j = 0; j < ndims - 2; ++j) {
                pads.emplace_back(1);
            }
            pads.emplace_back(tile.x);
            pads.emplace_back(tile.y);
        }
        this->op->inputs[i]->update_pads(pads);
    }
    for (unsigned int i = 0; i < this->op->outputs.size(); ++i) {
        auto tile = this->cfg->output_tiles[i];
        if (tile.x < 0) {
            tile.x = 1;
        }
        if (tile.y < 0) {
            tile.y = 1;
        }
        int ndims = this->op->outputs[i]->ndims();
        vector<DimType> pads;
        if (ndims == 1) {
            if (tile.x != 1) {
                LOGERR("invalid tile shape for 1D tensor: {", tile.x, ", ",
                       tile.y, "}");
            }
            pads.emplace_back(tile.y);
        } else {
            for (int j = 0; j < ndims - 2; ++j) {
                pads.emplace_back(1);
            }
            pads.emplace_back(tile.x);
            pads.emplace_back(tile.y);
        }
        this->op->outputs[i]->update_pads(pads);
    }
    // claculate the tile size for the SchedOp
    if ((this->op->outputs.size() == 1) && (this->cfg != nullptr)) {
        const OpTile &tile = this->cfg->output_tiles[0];
        const Dims &s = this->op->outputs[0]->shape;
        int ndims = s.ndims();
        vector<DimType> vec;
        if (ndims == 1) {
            vec.emplace_back((DimType)math::div_up(s[0], tile.y));
        } else {
            int i = 0;
            for (; i < ndims - 2; ++i) {
                vec.emplace_back(s[i]);
            }
            vec.emplace_back((DimType)math::div_up(s[i], tile.x));
            vec.emplace_back((DimType)math::div_up(s[i + 1], tile.y));
        }
        this->tnums = Dims{vec};
        LOG(DEBUG, "SchedOp: ", name, " tile num: ", this->tnums,
            " tile size: {", tile.x, ", ", tile.y, "}");
    }
}

const string SchedOp::function_name() const
{
    if (this->cfg == nullptr) {
        return "";
    }
    return this->op->function_name(*this->cfg);
}

bool SchedOp::is_virtual() const
{
    return this->cfg == nullptr;
}

} // namespace ark

```

`ark/sched/sched_op.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_SCHED_OP_H_
#define ARK_SCHED_OP_H_

#include "gpu/gpu_mgr.h"
#include "include/ark.h"
#include "json.h"
#include "ops/ops_common.h"

namespace ark {

class SchedOp
{
  public:
    SchedOp(const Op *op_, const OpConfig *cfg_, const std::string name);
    const Op *get_op() const
    {
        return op;
    }
    int get_num_warps() const
    {
        return const_cast<OpConfig *>(cfg)->num_warps;
    }
    const Dims &get_tnums() const
    {
        return tnums;
    }
    const std::string &get_name() const
    {
        return name;
    }
    const OpConfig *get_cfg() const
    {
        return cfg;
    }
    const std::string function_name() const;
    bool is_virtual() const;

  private:
    const Op *op;
    const OpConfig *cfg;
    std::string name;
    // The number of tiles along each axis of the operator.
    Dims tnums;
};

} // namespace ark

#endif // ARK_SCHED_OP_H_

```

`ark/sched/sched_opgraph.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "sched/sched_opgraph.h"
#include "logging.h"
#include "model.h"
#include <algorithm>

using namespace std;

#define DEBUG_OPGRAPH 0
#define OPGRAPH_DEBUG(...)                                                     \
    do {                                                                       \
        if (DEBUG_OPGRAPH) {                                                   \
            LOG(DEBUG, __VA_ARGS__);                                           \
        }                                                                      \
    } while (0);

namespace ark {

void OpNode::remove_self()
{
    // Remove self from users and producers.
    for (auto &user : this->users) {
        user->producers.erase(this);
    }
    for (auto &producer : this->producers) {
        producer->users.erase(this);
    }
    // Connect users and producers.
    for (auto &user : this->users) {
        for (auto &producer : this->producers) {
            user->producers.insert(producer);
            producer->users.insert(user);
        }
    }
}

std::string OpNode::get_name() const
{
    std::stringstream name;
    for (auto &op : this->ops) {
        name << op->name << ";";
    }
    return name.str();
}

OpGraph::OpGraph(const Model &model)
{
    if (!model.verify()) {
        LOG(ERROR, "Model verification failed");
    }
    this->create_nodes(model);
}

OpGraph::OpGraph(OpGraph &graph)
{
    // Copy nodes_storage
    *this = graph;
}

OpGraph &OpGraph::operator=(const OpGraph &graph)
{
    // Copy nodes_storage
    this->nodes_storage.clear();
    for (auto &node : graph.nodes_storage) {
        this->nodes_storage.emplace_back(std::make_unique<OpNode>());
        this->nodes_storage.back()->ops = node->ops;
        this->nodes_storage.back()->users = node->users;
        this->nodes_storage.back()->producers = node->producers;
    }
    return *this;
}

/// Traverse the model graph and merge Ops that one of them is the only
/// user of the other and the other is the only producer of the first.
///
/// @param model The @ref Model.
///
void OpGraph::create_nodes(const Model &model)
{
    std::list<OpNode *> leaf_nodes;
    std::map<const Op *, OpNode *> op2node;
    // Initialize OpNode.
    OPGRAPH_DEBUG("initialize OpNode. ", model.impl->get_ops().size(), " ops");
    for (auto &op : model.impl->get_ops()) {
        this->nodes_storage.emplace_back(std::make_unique<OpNode>());
        this->nodes_storage.back()->ops.emplace_back(op);
        op2node[op] = this->nodes_storage.back().get();
        if (model.impl->get_user_ops(op).size() == 0) {
            leaf_nodes.emplace_back(this->nodes_storage.back().get());
        }
    }
    // Complete producers and users of OpNode.
    for (auto &node : this->nodes_storage) {
        // As nothing is merged yet, all OpNode should have only one Op.
        Op *op = node->ops[0];
        OPGRAPH_DEBUG("node ", op->name);
        for (auto &producer_op : model.impl->get_producer_ops(op)) {
            node->producers.insert(op2node[producer_op]);
            OPGRAPH_DEBUG("  producer ", producer_op->name);
        }
        for (auto &user_op : model.impl->get_user_ops(op)) {
            node->users.insert(op2node[user_op]);
            OPGRAPH_DEBUG("  user ", user_op->name);
        }
    }

    std::set<OpNode *> seen_nodes;

    // Remove virtual Ops.
    recursive_rm_virt(this->nodes_storage, seen_nodes, leaf_nodes);
    seen_nodes.clear();

    // Recreate leaf_nodes.
    leaf_nodes.clear();
    for (auto &node : this->nodes_storage) {
        if (node->users.empty()) {
            leaf_nodes.emplace_back(node.get());
        }
    }

    // Merge Ops.
    recursive_merge(this->nodes_storage, seen_nodes, leaf_nodes);
}

/// Helper of @ref create_nodes().
/// Traverse the model graph and remove virtual Ops that perform no computation.
///
/// @param nodes The list of @ref OpNode.
/// @param boundary_nodes The list of boundary @ref OpNode.
///
void OpGraph::recursive_rm_virt(std::list<std::unique_ptr<OpNode>> &nodes,
                                std::set<OpNode *> &seen_nodes,
                                const std::list<OpNode *> &boundary_nodes)
{
    if (boundary_nodes.size() == 0) {
        return;
    }
    OPGRAPH_DEBUG("remove virtual ops");
    std::list<OpNode *> new_boundary_nodes;
    for (auto &boundary_node : boundary_nodes) {
        if (boundary_node->ops.size() != 1) {
            LOG(ERROR, "unexpected error");
        }
        OPGRAPH_DEBUG("  boundary node");
        OPGRAPH_DEBUG("    op: ", boundary_node->get_name());
        for (auto &producer : boundary_node->producers) {
            // Exception: if any user of the producer (rather than the current
            // boundary_node) is unseen, we should not add the producer to the
            // next boundary.
            bool should_add = true;
            for (auto &user : producer->users) {
                if (user == boundary_node) {
                    continue;
                }
                if (seen_nodes.find(user) == seen_nodes.end()) {
                    should_add = false;
                    break;
                }
            }
            if (!should_add) {
                continue;
            }
            if (seen_nodes.find(producer) != seen_nodes.end()) {
                LOG(ERROR, "unexpected error: circular dependency detected");
            }
            OPGRAPH_DEBUG("      added ", producer->get_name(),
                          " to next boundary");
            new_boundary_nodes.emplace_back(producer);
        }
        if (boundary_node->ops[0]->is_virtual()) {
            OPGRAPH_DEBUG("    remove op: ", boundary_node->get_name());
            // Remove this node from the graph.
            boundary_node->remove_self();
            // Remove this node from the list of nodes.
            auto it = std::find_if(
                nodes.begin(), nodes.end(),
                [boundary_node](const std::unique_ptr<OpNode> &node) {
                    return node.get() == boundary_node;
                });
            if (it == nodes.end()) {
                LOG(ERROR, "unexpected error");
            }
            nodes.erase(it);
            OPGRAPH_DEBUG("      nodes.size() ", nodes.size());
        } else {
            seen_nodes.insert(boundary_node);
        }
    }
    recursive_rm_virt(nodes, seen_nodes, new_boundary_nodes);
}

/// Helper of @ref create_nodes().
/// Traverse the model graph and merge pairs of Ops that are the only user
/// and producer of each other.
///
/// @param nodes The list of @ref OpNode.
/// @param seen_nodes The set of @ref OpNode that have been seen.
/// @param boundary_nodes The list of boundary @ref OpNode.
///
void OpGraph::recursive_merge(std::list<std::unique_ptr<OpNode>> &nodes,
                              std::set<OpNode *> &seen_nodes,
                              const std::list<OpNode *> &boundary_nodes)
{
    if (boundary_nodes.size() == 0) {
        return;
    }
    OPGRAPH_DEBUG("merge ops");
    std::list<OpNode *> new_boundary_nodes;
    for (auto &boundary_node : boundary_nodes) {
        OPGRAPH_DEBUG("  boundary node");
        OPGRAPH_DEBUG("    op: ", boundary_node->get_name());
        if (boundary_node->producers.size() == 0) {
            // This node is a root.
            seen_nodes.insert(boundary_node);
            OPGRAPH_DEBUG("    root");
            continue;
        }
        // Add all producers of this node to the next boundary.
        for (auto &producer : boundary_node->producers) {
            // Exception: if any user of the producer (rather than the current
            // boundary_node) is unseen, we should not add the producer to the
            // next boundary.
            bool should_add = true;
            for (auto &user : producer->users) {
                if (user == boundary_node) {
                    continue;
                }
                if (seen_nodes.find(user) == seen_nodes.end()) {
                    should_add = false;
                    break;
                }
            }
            if (!should_add) {
                continue;
            }
            if (seen_nodes.find(producer) != seen_nodes.end()) {
                LOG(ERROR, "unexpected error: circular dependency detected");
            }
            new_boundary_nodes.emplace_back(producer);
        }
        if (boundary_node->producers.size() > 1) {
            // This node has multiple producers. It cannot be merged.
            seen_nodes.insert(boundary_node);
            OPGRAPH_DEBUG("    multiple producers");
            continue;
        }
        // This node has only one producer.
        OpNode *producer = *(boundary_node->producers.begin());
        if (producer->users.size() == 0) {
            LOG(ERROR, "unexpected error: graph is incomplete");
        }
        if (producer->users.size() > 1) {
            // The producer has multiple users. It cannot be merged.
            seen_nodes.insert(boundary_node);
            OPGRAPH_DEBUG("    multiple users");
            continue;
        }
        // The producer has only one user. Merge the two nodes.

        // Merge `boundary_node` into `producer`.
        OPGRAPH_DEBUG("  merge ops: ", producer->get_name(), " -> ",
                      boundary_node->get_name());
        auto &ops = boundary_node->ops;
        producer->ops.insert(producer->ops.end(), ops.begin(), ops.end());
        producer->users = boundary_node->users;
        for (auto &user : producer->users) {
            user->producers.erase(boundary_node);
            user->producers.insert(producer);
        }

        // Remove `boundary_node` from `nodes`.
        auto it =
            std::find_if(nodes.begin(), nodes.end(),
                         [boundary_node](const std::unique_ptr<OpNode> &node) {
                             return node.get() == boundary_node;
                         });
        if (it == nodes.end()) {
            LOG(ERROR, "unexpected error");
        }
        nodes.erase(it);

        // Since producer is already in the next boundary and boundary_node is
        // merged into producer, we don't need to add anything to
        // seen_nodes here.
    }
    recursive_merge(nodes, seen_nodes, new_boundary_nodes);
}

OpNode *OpGraph::break_node(OpNode *node, int op_idx)
{
    if (op_idx == 0) {
        return node;
    }
    if (op_idx < 0 || op_idx >= (int)node->ops.size()) {
        LOG(ERROR, "unexpected error: op_idx out of range");
    }
    this->nodes_storage.emplace_back(std::make_unique<OpNode>());
    OpNode *new_node = this->nodes_storage.back().get();
    new_node->ops.insert(new_node->ops.end(), node->ops.begin() + op_idx,
                         node->ops.end());
    new_node->users = node->users;
    new_node->producers.insert(node);
    for (auto &user : node->users) {
        user->producers.erase(node);
        user->producers.insert(new_node);
    }
    node->ops.erase(node->ops.begin() + op_idx, node->ops.end());
    node->users.clear();
    node->users.insert(new_node);
    return new_node;
}

} // namespace ark

```

`ark/sched/sched_opgraph.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef _ARK_SCHED_OPGRAPH_H_
#define _ARK_SCHED_OPGRAPH_H_

#include "ops/ops_common.h"
#include <list>
#include <memory>
#include <set>
#include <vector>

namespace ark {

class Model;

/// A node in the @ref OpGraph.
class OpNode
{
  public:
    /// Construct an empty @ref OpNode.
    OpNode(){};

    /// Destruct an @ref OpNode.
    ~OpNode(){};

    /// The list of @ref Op that this @ref OpNode contains. Sorted in the
    /// execution order.
    std::vector<Op *> ops;

    /// The list of @ref OpNode that depends on this @ref OpNode.
    std::set<OpNode *> users;

    /// The list of @ref OpNode that this @ref OpNode depends on.
    std::set<OpNode *> producers;

    /// Remove this @ref OpNode from the graph.
    void remove_self();

    /// Get the name of this @ref OpNode.
    std::string get_name() const;
};

/// A directed acyclic graph of operators.
///
/// The @ref OpGraph is a DAG of operators, where each @ref OpNode is a
/// node. The edges are the dependencies between @ref OpNode.
///
class OpGraph
{
  public:
    /// Construct an @ref OpGraph from a @ref Model.
    ///
    /// The @ref OpGraph is a DAG of operators, where each @ref OpNode is a
    /// node. The edges are the dependencies between @ref OpNode.
    ///
    /// @param model The @ref Model.
    ///
    OpGraph(const Model &model);

    /// Construct an @ref OpGraph from another @ref OpGraph.
    OpGraph(OpGraph &graph);

    /// Construct an empty @ref OpGraph.
    OpGraph(){};

    /// Destruct an @ref OpGraph.
    ~OpGraph(){};

    OpGraph &operator=(const OpGraph &);

    /// Get the @ref OpNode list.
    /// @return The @ref OpNode list.
    const std::list<std::unique_ptr<OpNode>> &get_nodes() const
    {
        return this->nodes_storage;
    }

    /// Break a @ref OpNode into two @ref OpNode.
    ///
    /// The original node will have the first @p op_idx ops, and the new node
    /// will have the rest.
    ///
    /// @param node The @ref OpNode to break.
    /// @param op_idx The index of the first op in the new @ref OpNode.
    /// @return The new @ref OpNode.
    OpNode *break_node(OpNode *node, int op_idx);

  private:
    std::list<std::unique_ptr<OpNode>> nodes_storage;

    void create_nodes(const Model &model);
    static void recursive_rm_virt(std::list<std::unique_ptr<OpNode>> &nodes,
                                  std::set<OpNode *> &seen_nodes,
                                  const std::list<OpNode *> &boundary_nodes);
    static void recursive_merge(std::list<std::unique_ptr<OpNode>> &nodes,
                                std::set<OpNode *> &seen_nodes,
                                const std::list<OpNode *> &boundary_nodes);
};

} // namespace ark

#endif // _ARK_SCHED_OPGRAPH_H_

```

`ark/sched/sched_opgraph_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "ark.h"
#include "logging.h"
#include "sched_opgraph.h"
#include "unittest/unittest_utils.h"

ark::unittest::State test_sched_opgraph()
{
    ark::Model model;

    // Basic Test.
    // Model graph:
    //
    //   TensorOp --> t0 --+--> AddOp --> t2
    //                     |
    //   TensorOp --> t1 --+
    //                     |
    //   TensorOp --> tx --+  (tx is the output reference, hidden from the code)
    //

    ark::Tensor *t0 = model.tensor({1}, ark::FP32);
    ark::Tensor *t1 = model.tensor({1}, ark::FP32);
    ark::Tensor *t2 = model.add(t0, t1);
    UNITTEST_TRUE(model.verify());

    // OpNode graph (parentheses indicate a OpNode):
    //
    //   (AddOp,)
    //

    ark::OpGraph graph(model);
    UNITTEST_EQ(graph.get_nodes().size(), 1UL);

    auto node = graph.get_nodes().front().get();
    UNITTEST_EQ(node->ops.size(), 1UL);
    UNITTEST_EQ(node->ops[0]->outputs[0], t2);
    UNITTEST_EQ(node->ops[0]->inputs[0], t0);
    UNITTEST_EQ(node->ops[0]->inputs[1], t1);
    UNITTEST_EQ(node->users.size(), 0UL);
    UNITTEST_EQ(node->producers.size(), 0UL);

    // Test a chain of Ops that share an input tensor.
    // Model graph:
    //
    // TensorOp --> t0 --+--> AddOp --> t2 ------+--> AddOp --> t3
    //                   |                       |
    // TensorOp --> t1 --+-----------------------+
    //                   |                       |
    // TensorOp --> tx --+     TensorOp --> ty --+
    //
    // (tx and ty are output references, hidden from the code)
    //

    ark::Tensor *t3 = model.add(t2, t1);
    UNITTEST_TRUE(model.verify());

    // OpNode graph (parentheses indicate a OpNode):
    //
    //   (AddOp,AddOp,)
    //

    graph = ark::OpGraph(model);
    UNITTEST_EQ(graph.get_nodes().size(), 1UL);

    node = graph.get_nodes().front().get();

    UNITTEST_EQ(node->ops[0]->outputs[0], t2);
    UNITTEST_EQ(node->ops[0]->inputs[0], t0);
    UNITTEST_EQ(node->ops[0]->inputs[1], t1);
    UNITTEST_EQ(node->ops[1]->outputs[0], t3);
    UNITTEST_EQ(node->ops[1]->inputs[0], t2);
    UNITTEST_EQ(node->ops[1]->inputs[1], t1);
    UNITTEST_EQ(node->users.size(), 0UL);
    UNITTEST_EQ(node->producers.size(), 0UL);

    // Test a chain of Ops without shared input tensors.
    // Model graph (omit leftmost part):
    //
    // ... ----+--> AddOp --> t3 ----+-> ReluOp --> t4
    // ...     |                     |
    // ... ----+   TensorOp --> tz --+
    // ...     |
    // ...   --+   (tz is the output reference, hidden from the code)
    //

    ark::Tensor *t4 = model.relu(t3);
    UNITTEST_TRUE(model.verify());

    // OpNode graph (parentheses indicate a OpNode):
    //
    //   (AddOp,AddOp,ReluOp,)
    //

    graph = ark::OpGraph(model);
    UNITTEST_EQ(graph.get_nodes().size(), 1UL);

    node = graph.get_nodes().front().get();

    UNITTEST_EQ(node->ops[0]->outputs[0], t2);
    UNITTEST_EQ(node->ops[0]->inputs[0], t0);
    UNITTEST_EQ(node->ops[0]->inputs[1], t1);
    UNITTEST_EQ(node->ops[1]->outputs[0], t3);
    UNITTEST_EQ(node->ops[1]->inputs[0], t2);
    UNITTEST_EQ(node->ops[1]->inputs[1], t1);
    UNITTEST_EQ(node->ops[2]->outputs[0], t4);
    UNITTEST_EQ(node->ops[2]->inputs[0], t3);
    UNITTEST_EQ(node->users.size(), 0UL);
    UNITTEST_EQ(node->producers.size(), 0UL);

    // Test a chain of Ops that use the output from the same previous Op.
    // Model graph (omit leftmost part):
    //
    // ...   +---- (this is t2) -------------------------+--> AddOp --> t5
    // ...   |                                           |
    // ... --+-+--> AddOp --> t3 ----+-> ReluOp --> t4 --+
    // ...     |                     |                   |
    // ... ----+   TensorOp --> tz --+                   |
    // ...     |                       TensorOp --> tw --+
    // ...   --+
    //
    // (tz and tw are output references, hidden from the code)
    //

    ark::Tensor *t5 = model.add(t2, t4);
    UNITTEST_TRUE(model.verify());

    // OpNode graph (parentheses indicate a OpNode):
    //
    //              +----------------------+
    //              |                      |
    //   (AddOp,) --+--> (AddOp,ReluOp,) --+--> (AddOp,)
    //

    graph = ark::OpGraph(model);
    UNITTEST_EQ(graph.get_nodes().size(), 3UL);

    auto nodes_iter = graph.get_nodes().begin();
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t2);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t3);
    UNITTEST_EQ(node->ops[1]->outputs[0], t4);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t5);

    // Test an Op that uses outputs from multiple previous Ops.
    // Model graph (omit leftmost part):
    //
    // ... ----- (this is t2) --+--> AddOp --> t5
    // ...                      |              |
    // ... -+-> ReluOp --> t4 --+              |
    // ...  |                   |              |
    // ... -+                   |              |
    // ...    TensorOp --> tw --+              |
    // ...                                     |
    //                                         |
    //   TensorOp --> t6 --+--> AddOp --> t8 --+--> AddOp --> t9
    //                     |
    //   TensorOp --> t7 --+
    //                     |
    //   TensorOp --> tu --+
    //
    // (tw and tu are output references, hidden from the code)
    //

    ark::Tensor *t6 = model.tensor({1}, ark::FP32);
    ark::Tensor *t7 = model.tensor({1}, ark::FP32);
    ark::Tensor *t8 = model.add(t6, t7);
    ark::Tensor *t9 = model.add(t5, t8);
    UNITTEST_TRUE(model.verify());

    // OpNode graph (parentheses indicate a OpNode):
    //
    //              +----------------------+
    //              |                      |
    //   (AddOp,) --+--> (AddOp,ReluOp,) --+--> (AddOp,) --+
    //                                                     |
    //                                          (AddOp,) --+--> (AddOp,)
    //

    graph = ark::OpGraph(model);
    UNITTEST_EQ(graph.get_nodes().size(), 5UL);

    nodes_iter = graph.get_nodes().begin();
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t2);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t3);
    UNITTEST_EQ(node->ops[1]->outputs[0], t4);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t5);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t8);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t9);

    // Test an Op that uses a single input tensor for multiple inputs.
    // Model graph (omit leftmost part):
    //
    // ... ----- (this is t2) --+--> AddOp --> t5
    // ...                      |              |
    // ... -+-> ReluOp --> t4 --+              |
    // ...  |                   |              |
    // ... -+                   |              |
    // ...    TensorOp --> tw --+              |
    // ...                                     |
    //                                         |
    //   TensorOp --> t6 --+--> AddOp --> t8 --+--> AddOp --> t9
    //                     |
    //   TensorOp --> t7 --+
    //                     |
    //   TensorOp --> tu --+
    //
    //   TensorOp --> t10 --+--> AddOp --> t11
    //                      |    ^  ^
    //                      |    |  |
    //                      +----+  |
    //                              |
    //   TensorOp --> tv -----------+
    //
    // (tw, tu, and tv are output references, hidden from the code)
    //

    ark::Tensor *t10 = model.tensor({1}, ark::FP32);
    ark::Tensor *t11 = model.add(t10, t10);
    UNITTEST_TRUE(model.verify());

    // OpNode graph (parentheses indicate a OpNode):
    //
    //              +----------------------+
    //              |                      |
    //   (AddOp,) --+--> (AddOp,ReluOp,) --+--> (AddOp,) --+
    //                                                     |
    //                                          (AddOp,) --+--> (AddOp,)
    //
    //                                                          (AddOp,)
    //

    graph = ark::OpGraph(model);
    UNITTEST_EQ(graph.get_nodes().size(), 6UL);

    nodes_iter = graph.get_nodes().begin();
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t2);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t3);
    UNITTEST_EQ(node->ops[1]->outputs[0], t4);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t5);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t8);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t9);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t11);

    // Test using previous Ops' outputs from multiple different Ops.
    // Model graph (omit leftmost part):
    //
    // ... ----- (this is t2) --+--> AddOp --> t5
    // ...                      |              |
    // ... -+-> ReluOp --> t4 --+              |
    // ...  |                   |              |
    // ... -+                   |              |
    // ...    TensorOp --> tw --+              |
    // ...                                     |
    //                                         |
    //   TensorOp --> t6 --+--> AddOp --> t8 --+--> AddOp --> t9
    //                     |                   |
    //   TensorOp --> t7 --+                   +--> AddOp --> t12
    //                     |
    //   TensorOp --> tu --+
    //
    //   TensorOp --> t10 --+--> AddOp --> t11
    //                      |    ^  ^
    //                      |    |  |
    //                      +----+  |
    //                              |
    //   TensorOp --> tv -----------+
    //
    // (tw, tu, and tv are output references, hidden from the code)
    //

    ark::Tensor *t12 = model.add(t5, t8);
    UNITTEST_TRUE(model.verify());

    // OpNode graph (parentheses indicate a OpNode):
    //
    //              +----------------------+
    //              |                      |
    //   (AddOp,) --+--> (AddOp,ReluOp,) --+--> (AddOp,) --+--> (AddOp,)
    //                                                     |
    //                                          (AddOp,) --+--> (AddOp,)
    //
    //                                                          (AddOp,)
    //

    graph = ark::OpGraph(model);
    UNITTEST_EQ(graph.get_nodes().size(), 7UL);

    nodes_iter = graph.get_nodes().begin();
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t2);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t3);
    UNITTEST_EQ(node->ops[1]->outputs[0], t4);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t5);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t8);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t9);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t11);
    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t12);

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_sched_opgraph_noop()
{
    ark::Model model;
    model.tensor({1}, ark::FP32);
    model.tensor({1}, ark::FP32);
    model.tensor({1}, ark::FP32);
    UNITTEST_TRUE(model.verify());

    ark::OpGraph graph(model);
    UNITTEST_EQ(graph.get_nodes().size(), 0UL);
    return ark::unittest::SUCCESS;
}

ark::unittest::State test_sched_opgraph_identity()
{
    // OpNode graph (parentheses indicate a OpNode):
    //
    //   (Relu,) --+
    //             |
    //   (Relu,) --+--> (Relu,)
    //

    ark::Model model;
    ark::Tensor *t0 = model.tensor({1}, ark::FP32);
    ark::Tensor *t1 = model.tensor({1}, ark::FP32);
    ark::Tensor *t2 = model.tensor({1}, ark::FP32);

    ark::Tensor *r0 = model.relu(t0);
    ark::Tensor *r1 = model.relu(t1);
    ark::Tensor *t3 = model.identity(t2, {r0, r1});

    ark::Tensor *t4 = model.relu(t3);
    UNITTEST_TRUE(model.verify());

    ark::OpGraph graph(model);
    UNITTEST_EQ(graph.get_nodes().size(), 3UL);

    auto nodes_iter = graph.get_nodes().begin();
    auto node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], r0);
    UNITTEST_EQ(node->producers.size(), 0UL);
    UNITTEST_EQ(node->users.size(), 1UL);

    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], r1);
    UNITTEST_EQ(node->producers.size(), 0UL);
    UNITTEST_EQ(node->users.size(), 1UL);

    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t4);
    UNITTEST_EQ(node->producers.size(), 2UL);
    UNITTEST_EQ(node->users.size(), 0UL);

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_sched_opgraph_sharding()
{
    // OpNode graph (parentheses indicate a OpNode):
    //
    //   (Relu,) --+
    //             |
    //   (Relu,) --+
    //             |
    //   (Relu,) --+--> (Relu,)
    //

    ark::Model model;
    ark::Tensor *t0 = model.tensor({3}, ark::FP32);

    std::vector<ark::Tensor *> vec = model.sharding(t0, 0, 1);
    UNITTEST_EQ(vec.size(), 3UL);

    ark::Tensor *t1 = vec[0];
    ark::Tensor *t2 = vec[1];
    ark::Tensor *t3 = vec[2];

    ark::Tensor *r0 = model.relu(t1);
    ark::Tensor *r1 = model.relu(t2);
    ark::Tensor *r2 = model.relu(t3);

    ark::Tensor *t4 = model.identity(t0, {r0, r1, r2});

    ark::Tensor *t5 = model.relu(t4);
    UNITTEST_TRUE(model.verify());

    ark::OpGraph graph(model);
    UNITTEST_EQ(graph.get_nodes().size(), 4UL);

    auto nodes_iter = graph.get_nodes().begin();
    auto node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], r0);
    UNITTEST_EQ(node->producers.size(), 0UL);
    UNITTEST_EQ(node->users.size(), 1UL);

    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], r1);
    UNITTEST_EQ(node->producers.size(), 0UL);
    UNITTEST_EQ(node->users.size(), 1UL);

    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], r2);
    UNITTEST_EQ(node->producers.size(), 0UL);
    UNITTEST_EQ(node->users.size(), 1UL);

    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], t5);
    UNITTEST_EQ(node->producers.size(), 3UL);
    UNITTEST_EQ(node->users.size(), 0UL);

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_sched_opgraph_split_matmul()
{
    // OpNode graph (parentheses indicate a OpNode):
    //
    //   (Matmul,) --+
    //               |
    //   (Matmul,) --+--> (Reduce,)
    //

    ark::Model model;
    ark::Tensor *t0 = model.tensor({64, 128}, ark::FP16);
    ark::Tensor *t1 = model.tensor({128, 64}, ark::FP16);
    ark::Tensor *m0 =
        model.matmul(t0, t1, nullptr, 2, false, false, false, "matmul", 3);
    UNITTEST_TRUE(model.verify());

    ark::OpGraph graph(model);
    UNITTEST_EQ(graph.get_nodes().size(), 3UL);

    auto nodes_iter = graph.get_nodes().begin();
    auto node = (nodes_iter++)->get();
    UNITTEST_EQ(node->producers.size(), 0UL);
    UNITTEST_EQ(node->users.size(), 1UL);

    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->producers.size(), 0UL);
    UNITTEST_EQ(node->users.size(), 1UL);

    node = (nodes_iter++)->get();
    UNITTEST_EQ(node->ops[0]->outputs[0], m0);
    UNITTEST_EQ(node->producers.size(), 2UL);
    UNITTEST_EQ(node->users.size(), 0UL);

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_sched_opgraph_cumulate()
{
    // OpNode graph (parentheses indicate a OpNode):
    //
    //       (Relu,) --+   (Relu,) --+
    //                 |             |
    //   (Relu,Add,) --+--> (Add,) --+--> (Add,)
    //

    ark::Model model;
    ark::Tensor *cumulate = model.tensor({1}, ark::FP32);

    for (int i = 0; i < 3; ++i) {
        ark::Tensor *t = model.tensor({1}, ark::FP32);
        ark::Tensor *r = model.relu(t);
        cumulate = model.add(cumulate, r);
    }

    UNITTEST_TRUE(model.verify());

    ark::OpGraph graph(model);
    UNITTEST_EQ(graph.get_nodes().size(), 5UL);

    auto last_node = graph.get_nodes().back().get();
    UNITTEST_EQ(last_node->ops[0]->outputs[0], cumulate);
    UNITTEST_EQ(last_node->producers.size(), 2UL);
    UNITTEST_EQ(last_node->users.size(), 0UL);

    return ark::unittest::SUCCESS;
}

ark::unittest::State test_sched_opgraph_all_reduce()
{
    // OpNode graph (parentheses indicate a OpNode):
    //
    //               +--> (S,SD,R,) --+--> (S,SD,R,) --+
    //               |                |                |
    //   (S,SD,R,) --+--> (Add,)      +--> (Add,)      +--> (Add,)
    //                      |               ^  |              ^
    //                      |               |  |              |
    //                      +---------------+  +--------------+

    ark::Model model;
    ark::Tensor *input = model.tensor({1}, ark::FP32);
    ark::Tensor *output = model.all_reduce(input, 0, 4);

    UNITTEST_TRUE(model.verify());

    ark::OpGraph graph(model);
    UNITTEST_EQ(graph.get_nodes().size(), 6UL);

    auto nodes_iter = graph.get_nodes().begin();
    auto node = (nodes_iter++)->get();
    UNITTEST_EQ(node->get_name(), "send;send_done;recv;");
    UNITTEST_EQ(node->producers.size(), 0UL);

    std::vector<ark::OpNode *> users;
    for (auto &user : node->users) {
        users.push_back(user);
    }
    UNITTEST_EQ(users[0]->get_name(), "add;");
    UNITTEST_EQ(users[0]->producers.size(), 1UL);
    UNITTEST_EQ(users[0]->users.size(), 1UL);
    UNITTEST_EQ((*(users[0]->users.begin()))->get_name(), "add_1;");

    UNITTEST_EQ(users[1]->get_name(), "send_1;send_done_1;recv_1;");
    UNITTEST_EQ(users[1]->producers.size(), 1UL);
    UNITTEST_EQ(users[1]->users.size(), 2UL);

    node = users[1];
    users.clear();
    for (auto &user : node->users) {
        users.push_back(user);
    }
    UNITTEST_EQ(users[0]->get_name(), "add_1;");
    UNITTEST_EQ(users[0]->producers.size(), 2UL);
    UNITTEST_EQ(users[0]->users.size(), 1UL);
    UNITTEST_EQ((*(users[0]->users.begin()))->get_name(), "add_2;");

    UNITTEST_EQ(users[1]->get_name(), "send_2;send_done_2;recv_2;");
    UNITTEST_EQ(users[1]->producers.size(), 1UL);
    UNITTEST_EQ(users[1]->users.size(), 1UL);
    UNITTEST_EQ((*(users[1]->users.begin()))->get_name(), "add_2;");
    UNITTEST_EQ((*(users[1]->users.begin()))->ops[0]->outputs[0], output);

    return ark::unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_sched_opgraph);
    UNITTEST(test_sched_opgraph_noop);
    UNITTEST(test_sched_opgraph_identity);
    UNITTEST(test_sched_opgraph_sharding);
    UNITTEST(test_sched_opgraph_split_matmul);
    UNITTEST(test_sched_opgraph_cumulate);
    UNITTEST(test_sched_opgraph_all_reduce);
    return 0;
}

```

`ark/sched/sched_opseq.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "sched/sched_opseq.h"
#include "env.h"
#include "logging.h"
#include "math.h"

using namespace std;

namespace ark {

SchedOpSeq::SchedOpSeq(int id_) : id{id_}
{
    LOG(DEBUG, "create SchedOpSeq", id);
}

SchedOpSeq::SchedOpSeq(int id_, const Op *op, const OpConfig *cfg) : id{id_}
{
    this->append(op, cfg);
}

bool SchedOpSeq::is_send() const
{
    for (auto &sop : this->seq) {
        if (sop.is_virtual()) {
            continue;
        }
        const OpType &ot = sop.get_op()->type;
        if (ot == OP_SEND) {
            continue;
        }
        return false;
    }
    return true;
}

bool SchedOpSeq::is_send_done() const
{
    for (auto &sop : this->seq) {
        if (sop.is_virtual()) {
            continue;
        }
        const OpType &ot = sop.get_op()->type;
        if (ot == OP_SEND_DONE) {
            continue;
        }
        return false;
    }
    return true;
}

bool SchedOpSeq::is_recv() const
{
    for (auto &sop : this->seq) {
        if (sop.is_virtual()) {
            continue;
        }
        const OpType &ot = sop.get_op()->type;
        if (ot == OP_RECV) {
            continue;
        }
        return false;
    }
    return true;
}

bool SchedOpSeq::is_comm() const
{
    return this->is_send() || this->is_send_done() || this->is_recv();
}

bool SchedOpSeq::append(const Op *op, const OpConfig *cfg)
{
    assert(op != nullptr);
    int dx = 0;
    int dy = 0;
    int dz = 0;
    int wn;
    int sb;
    if (cfg != nullptr) {
        wn = cfg->num_warps;
        sb = cfg->smem_bytes;
    } else {
        wn = 0;
        sb = 0;
    }
    if ((op->outputs.size() > 0) && (wn > 0)) {
        const Dims &s = op->outputs[0]->shape;
        const OpTile &tile = cfg->output_tiles[0];
        // TODO: temporal.
        int ndims = s.ndims();
        assert(ndims != 0);
        if (ndims == 1) {
            dx = math::div_up(s[0], tile.y);
            dy = 1;
            dz = 1;
        } else {
            dx = math::div_up(s[ndims - 1], tile.y);
            dy = math::div_up(s[ndims - 2], tile.x);
            dz = s.size() / s[ndims - 1] / s[ndims - 2];
        }
    }
    if (dz == 0) {
    } else if ((this->seq.size() == 0) || (this->tdims[0] == 0)) {
        this->num_warps = wn;
        this->smem_bytes = sb;
        this->tdims[0] = dz;
        this->tdims[1] = dy;
        this->tdims[2] = dx;
    } else {
        // Merge condition
        if (wn != this->num_warps || this->tdims[0] != dz ||
            this->tdims[1] != dy || this->tdims[2] != dx) {
            return false;
        }
        this->smem_bytes = max(this->smem_bytes, sb);
    }
    this->seq.emplace_back(op, cfg, "");
    return true;
}
bool operator<(const SchedOpSeq &ops1, const SchedOpSeq &ops2)
{
    auto &seq1 = ops1.get_sched_ops();
    auto &seq2 = ops2.get_sched_ops();
    for (size_t i = 0; i < seq1.size(); ++i) {
        if (seq2.size() <= i) {
            return true;
        } else if (seq1[i].get_cfg() != seq2[i].get_cfg()) {
            return seq1[i].get_cfg() < seq2[i].get_cfg();
        } else if (seq1[i].get_op() == nullptr) {
            return false;
        } else if (seq2[i].get_op() == nullptr) {
            return true;
        } else if (*seq1[i].get_op() == *seq2[i].get_op()) {
            continue;
        }
        return *seq1[i].get_op() < *seq2[i].get_op();
    }
    return false;
}
bool operator==(const SchedOpSeq &ops1, const SchedOpSeq &ops2)
{
    auto &seq1 = ops1.get_sched_ops();
    auto &seq2 = ops2.get_sched_ops();
    if (seq1.size() != seq2.size()) {
        return false;
    }
    for (size_t i = 0; i < seq1.size(); ++i) {
        if (seq1[i].get_cfg() != seq2[i].get_cfg()) {
            return false;
        } else if ((seq1[i].get_op() == nullptr) &&
                   (seq2[i].get_op() == nullptr)) {
            continue;
        } else if ((seq1[i].get_op() == nullptr) ||
                   (seq2[i].get_op() == nullptr)) {
            return false;
        } else if (*seq1[i].get_op() == *seq2[i].get_op()) {
            continue;
        }
        return false;
    }
    return true;
}

} // namespace ark

```

`ark/sched/sched_opseq.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_SCHED_OPSEQ_H_
#define ARK_SCHED_OPSEQ_H_

#include <cassert>
#include <map>
#include <sstream>
#include <string>
#include <tuple>

#include "gpu/gpu_mgr.h"
#include "sched/sched_op.h"

namespace ark {

class SchedOpSeq
{
  public:
    SchedOpSeq() : id{-1}
    {
    }
    SchedOpSeq(int id);
    SchedOpSeq(int id_, const Op *op, const OpConfig *cfg);
    bool append(const Op *op, const OpConfig *cfg);

    bool is_virtual() const
    {
        return num_warps == 0;
    }
    bool is_send() const;
    bool is_send_done() const;
    bool is_recv() const;
    bool is_comm() const;

    const std::string get_name() const
    {
        std::stringstream ss;
        for (auto &sop : seq) {
            ss << sop.get_op()->name << ";";
        }
        return ss.str();
    }

    const int &get_id() const
    {
        return id;
    }
    const std::vector<SchedOp> &get_sched_ops() const
    {
        return seq;
    }
    const Op *get_last_op() const
    {
        return seq.back().get_op();
    }
    int get_num_warps() const
    {
        return num_warps;
    }
    int get_smem_bytes() const
    {
        return smem_bytes;
    }
    const std::array<int, 3> &get_tdims() const
    {
        return tdims;
    }
    int get_tdims_size() const
    {
        return tdims[0] * tdims[1] * tdims[2];
    }
    int get_tdim_x() const
    {
        return tdims[2];
    }
    int get_tdim_y() const
    {
        return tdims[1];
    }
    int get_tdim_z() const
    {
        return tdims[0];
    }
    int get_tdim_xz() const
    {
        return tdims[0] * tdims[2];
    }

    friend bool operator<(const SchedOpSeq &ops1, const SchedOpSeq &ops2);
    friend bool operator==(const SchedOpSeq &ops1, const SchedOpSeq &ops2);

  private:
    const int id;
    std::vector<SchedOp> seq;
    int num_warps = 0;
    int smem_bytes = 0;
    std::array<int, 3> tdims = {{0, 0, 0}};
};

struct Sched
{
    // Indicates:
    // if (sm_b <= blockIdx.x < sm_e) and (th_b <= threadIdx.x < th_e);
    // do run opseq(alpha * (blockIdx.x - sm_b) + beta); fi.
    // opseq == nullptr implicitly indicates a global sync.
    Sched(SchedOpSeq *opseq_, int sm_b_, int sm_e_, int th_b_, int th_e_,
          int alpha_, int beta_)
        : opseq{opseq_}, sm_b{sm_b_}, sm_e{sm_e_}, th_b{th_b_}, th_e{th_e_},
          alpha{alpha_}, beta{beta_}
    {
    }
    SchedOpSeq *opseq;
    int sm_b;
    int sm_e;
    int th_b;
    int th_e;
    int alpha;
    int beta;
};

} // namespace ark

#endif // ARK_SCHED_OPSEQ_H_

```

`ark/sched/sched_profiler.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "json.h"
#include <algorithm>
#include <cassert>
#include <fstream>
#include <initializer_list>
#include <ostream>
#include <unistd.h>

#include "logging.h"
#include "math.h"
#include "sched/sched_opseq.h"
#include "sched/sched_profiler.h"
#include "sched/sched_tile.h"

using namespace std;

namespace ark {

float SchedProfiler::profile_routine(GpuLoopKernel *glk, GpuMgrCtx *ctx)
{
    const int probe_iter = 10;

    glk->load();
    GpuState ret = glk->launch(ctx->create_stream(), false);
    if (ret != CUDA_SUCCESS) {
        LOGERR("launch() failed with error code ", ret);
    }

    int test_iter = probe_iter * 1e2 / 1;
    if (test_iter == 0)
        test_iter = 1;
    // LOG(DEBUG, "test_iter", test_iter);
    glk->run(test_iter);
    glk->stop();
    // msec->usec. Drop values below usec.
    return (float)((glk->get_elapsed_msec() / test_iter * 1e3));
}

struct ProfInfo
{
    ProfInfo(const SchedTileSetType &type_, const SchedOpSeq *opseq0_,
             const SchedOpSeq *opseq1_ = nullptr)
        : type{type_}, opseq0{opseq0_}, opseq1{opseq1_}
    {
        if (type == SCHED_TILE_SET_MIXED) {
            assert(opseq1_ != nullptr);
        } else {
            assert(opseq1_ == nullptr);
        }
    }

    const string get_name() const
    {
        if (type == SCHED_TILE_SET_S) {
            return "prof_" + to_string(opseq0->get_id()) + "_s";
        } else if (type == SCHED_TILE_SET_X) {
            return "prof_" + to_string(opseq0->get_id()) + "_x";
        } else if (type == SCHED_TILE_SET_Y) {
            return "prof_" + to_string(opseq0->get_id()) + "_y";
        } else if (type == SCHED_TILE_SET_XY) {
            return "prof_" + to_string(opseq0->get_id()) + "_xy";
        } else if (type == SCHED_TILE_SET_MIXED) {
            return "prof_" + to_string(opseq0->get_id()) + "_" +
                   to_string(opseq1->get_id());
        }
    }

    const SchedTileSetType type;
    const SchedOpSeq *opseq0;
    const SchedOpSeq *opseq1;
};

bool operator<(const ProfInfo &info0, const ProfInfo &info1)
{
    if (!(*info0.opseq0 == *info1.opseq0)) {
        return *info0.opseq0 < *info1.opseq0;
    } else if (info0.opseq1 != nullptr && info1.opseq1 == nullptr) {
        return true;
    } else if (info0.opseq1 == nullptr && info1.opseq1 != nullptr) {
        return false;
    } else if (info0.opseq1 == nullptr && info1.opseq1 == nullptr) {
        return info0.type < info1.type;
    } else if (!(*info0.opseq1 == *info1.opseq1)) {
        return *info0.opseq1 < *info1.opseq1;
    }
    return info0.type < info1.type;
}

static string prof_name(const SchedTileSet &ts)
{
    const int &id0 = ts.tiles[0].opseq->get_id();
    if (ts.type == SCHED_TILE_SET_MIXED) {
        const int &id1 = ts.tiles[1].opseq->get_id();
        return "prof_" + to_string(id0) + "_" + to_string(id1);
    } else if (ts.type == SCHED_TILE_SET_S) {
        return "prof_" + to_string(id0) + "_s";
    } else if (ts.type == SCHED_TILE_SET_X) {
        return "prof_" + to_string(id0) + "_x";
    } else if (ts.type == SCHED_TILE_SET_Y) {
        return "prof_" + to_string(id0) + "_y";
    } else {
        assert(ts.type == SCHED_TILE_SET_XY);
        return "prof_" + to_string(id0) + "_xy";
    }
}

// convert SchedTileDepth to Sched
vector<Sched> gen_sched(SchedTileDepth *tile_depths, int num_warps_per_sm)
{
    vector<Sched> scheds;
    int sm_b = 0;
    int sm_e = 1;
    int th_b = 0;
    int th_e = 0;
    for (auto &sm : tile_depths->sms) {
        th_e = 0;
        th_b = 0;
        // execute all tiles in the sm sm_b
        vector<Sched> tmp_sched;
        for (auto &tileset : sm) {
            // execute all tiles in the tileset
            for (auto &tile : tileset.tiles) {
                int id = tile.id;
                SchedOpSeq *opseq = const_cast<ark::SchedOpSeq *>(tile.opseq);
                // DimType tnum = opseq->get_tdims_size();
                DimType wnum = opseq->get_num_warps();
                th_e = th_b + wnum * 32;
                if (th_e > num_warps_per_sm * 32) {
                    th_b = 0;
                    th_e = wnum * 32;
                }
                // LOG(DEBUG, "  op", opseq->get_id(), ": tnum ", tnum, " wnum",
                //     wnum, " sm_b ", sm_b, " sm_e ", sm_e, " th_b ", th_b,
                //     " th_e ", th_e, " id ", id);
                tmp_sched.emplace_back(opseq, sm_b, sm_e, th_b, th_e, 1, id);
                th_b = th_e;
            }
        }
        // order the tmp_sched by the th_b
        std::sort(tmp_sched.begin(), tmp_sched.end(),
                  [](const Sched &sched1, const Sched &sched2) {
                      return sched1.th_b < sched2.th_b;
                  });
        // push the tmp_sched to scheds
        for (auto &sched : tmp_sched) {
            // LOG(DEBUG, "  op", sched.opseq->get_id(), ": tnum ",
            //     sched.opseq->get_tdims_size(), " wnum",
            //     sched.opseq->get_num_warps(), " sm_b ", sched.sm_b, " sm_e ",
            //     sched.sm_e, " th_b ", sched.th_b, " th_e ", sched.th_e, " id
            //     ", sched.beta);
            scheds.push_back(sched);
        }
        sm_b++;
        sm_e++;
    }
    return scheds;
}

void SchedProfiler::profile(OpGraph *op_graph, CodeGenerator &codegen,
                            GpuMgrCtx *ctx)
{
#if 0
    using ProfCallback = function<void(float, int)>;
    const GpuInfo &gpu_info = this->gpu_mgr->get_gpu_info();
    // Get or create entry of profile results.
    auto &res = this->wps_prof_results[this->num_warps_per_sm];
    //
    map<ProfInfo, unsigned int> info2id;
    vector<vector<tuple<SchedTileDepth *, ProfCallback>>> to_prof;
    //
    size_t num_depth = op_graph->get_num_depth();
    for (size_t depth = 0; depth < num_depth; ++depth) {
        auto &depth_nodes = op_graph->get_depth(depth);
        for (auto &ogn : depth_nodes) {
            auto &opseq = ogn->opseq;
            if (opseq.is_send() || opseq.is_recv() || opseq.is_send_done() ||
                opseq.is_virtual())
                continue;
            SchedOpSeqPerf &perf = res[&opseq];
            // Skip if this already has profiled results.
            if (perf.s.is_set())
                continue;
            assert(opseq.get_num_warps() <= this->num_warps_per_sm);
            //
            {
                auto p =
                    info2id.emplace(ProfInfo{SCHED_TILE_SET_S, &opseq, nullptr},
                                    info2id.size());
                if (p.second)
                    to_prof.emplace_back();
                assert(to_prof.size() > p.first->second);
                SchedTileDepth *sd = new SchedTileDepth{1};
                sd->append_tiles(0, {{&opseq, 0}}, SCHED_TILE_SET_S);
                to_prof[p.first->second].emplace_back(
                    sd, [&](float e, int r) { perf.s.set(e, r); });
            }
            if (this->num_warps_per_sm < opseq.get_num_warps() * 2 ||
                gpu_info.smem_block_total < opseq.get_smem_bytes() * 2)
                // Cannot run two tiles concurrently in a SM.
                continue;
            //
            if (opseq.get_tdim_x() > 1) {
                auto p =
                    info2id.emplace(ProfInfo{SCHED_TILE_SET_X, &opseq, nullptr},
                                    info2id.size());
                if (p.second)
                    to_prof.emplace_back();
                assert(to_prof.size() > p.first->second);
                SchedTileDepth *sd = new SchedTileDepth{1};
                sd->append_tiles(0, {{&opseq, 0, 0, 0}, {&opseq, 1, 0, 0}},
                                 SCHED_TILE_SET_X);
                to_prof[p.first->second].emplace_back(sd, [&](float e, int r) {
                    perf.x.set(e, r, (perf.s.elapsed * 2 - e) * 1e3);
                });
            }
            if (opseq.get_tdim_y() > 1) {
                auto p =
                    info2id.emplace(ProfInfo{SCHED_TILE_SET_Y, &opseq, nullptr},
                                    info2id.size());
                if (p.second)
                    to_prof.emplace_back();
                assert(to_prof.size() > p.first->second);
                SchedTileDepth *sd = new SchedTileDepth{1};
                sd->append_tiles(0, {{&opseq, 0, 0, 0}, {&opseq, 0, 1, 0}},
                                 SCHED_TILE_SET_Y);
                to_prof[p.first->second].emplace_back(sd, [&](float e, int r) {
                    perf.y.set(e, r, (perf.s.elapsed * 2 - e) * 1e3);
                });
            }
            if (opseq.get_tdim_x() > 1 && opseq.get_tdim_y() > 1) {
                auto p = info2id.emplace(
                    ProfInfo{SCHED_TILE_SET_XY, &opseq, nullptr},
                    info2id.size());
                if (p.second)
                    to_prof.emplace_back();
                assert(to_prof.size() > p.first->second);
                SchedTileDepth *sd = new SchedTileDepth{1};
                sd->append_tiles(0, {{&opseq, 0, 0, 0}, {&opseq, 1, 1, 0}},
                                 SCHED_TILE_SET_XY);
                to_prof[p.first->second].emplace_back(sd, [&](float e, int r) {
                    perf.xy.set(e, r, (perf.s.elapsed * 2 - e) * 1e3);
                });
            }
        }
        // Inter-Op profiling
        for (auto it0 = depth_nodes.begin(); it0 != depth_nodes.end(); ++it0) {
            const SchedOpSeq &opseq0 = (*it0)->opseq;
            if (opseq0.is_send() || opseq0.is_recv() || opseq0.is_send_done() ||
                opseq0.is_virtual())
                continue;
            if (opseq0.get_num_warps() >= this->num_warps_per_sm)
                continue;
            SchedOpSeqPerf &perf0 = res[&opseq0];
            for (auto it1 = next(it0); it1 != depth_nodes.end(); ++it1) {
                const SchedOpSeq &opseq1 = (*it1)->opseq;
                if (perf0.mixed[&opseq1].is_set())
                    continue;
                if (opseq0.get_num_warps() + opseq1.get_num_warps() > this->num_warps_per_sm)
                    continue;
                //
                auto p = info2id.emplace(
                    ProfInfo{SCHED_TILE_SET_MIXED, &opseq0, &opseq1},
                    info2id.size());
                if (p.second)
                    to_prof.emplace_back();
                assert(to_prof.size() > p.first->second);
                SchedTileDepth *sd = new SchedTileDepth{1};
                sd->append_tiles(0, {{&opseq0, 0, 0, 0}, {&opseq1, 0, 0, 0}},
                                 SCHED_TILE_SET_MIXED);
                to_prof[p.first->second].emplace_back(sd, [&](float e, int r) {
                    SchedOpSeqPerf &perf1 = res[&opseq1];
                    assert(perf0.s.is_set());
                    assert(perf1.s.is_set());
                    float &e0 = perf0.s.elapsed;
                    float &e1 = perf1.s.elapsed;
                    float emin = e0 > e1 ? e1 : e0;
                    float emax = e0 > e1 ? e0 : e1;
                    int score = (e0 + e1 - e) / emin * emax * 1000;
                    perf0.mixed[&opseq1].set(e, r, score);
                    perf1.mixed[&opseq0].set(e, r, score);
                });
            }
        }
    }

    bool cache_exists = (access(this->wps_prof_cache_path.c_str(), F_OK) != -1);
    nlohmann::json cache_json;
    nlohmann::json wps_cache_json;
    if (cache_exists) {
        try {
            ifstream cache_file_stream(this->wps_prof_cache_path);
            cache_file_stream >> cache_json;
            wps_cache_json = cache_json.at(to_string(this->num_warps_per_sm));
        } catch (...) {
            wps_cache_json = {};
        }
    }

    for (auto &tp : to_prof) {
        assert(tp.size() > 0);
        auto &pf = tp[0];
        SchedTileDepth *tile_depth = get<0>(pf);
        string name = prof_name(tile_depth->sms[0][0]);
        auto search = wps_cache_json.find(name);
        if (search != wps_cache_json.end()) {
            float e = wps_cache_json[name][0].get<float>();
            int r = wps_cache_json[name][1].get<int>();
            // Call all callbacks.
            for (auto &p : tp)
                get<1>(p)(e, r);
            return;
        }
        assert(tile_depth != nullptr);
        assert(tile_depth->get_num_warps() <= this->num_warps_per_sm);
        // `gpu_loop_kernel()` is supposed to be thread-safe
        // as long as `glk` is excessed by only a single thread.
        auto scheds = gen_sched(tile_depth, this->num_warps_per_sm);
        auto codes = codegen.codegen_codes_body(scheds);

        GpuLoopKernel *glk =
            new GpuLoopKernel(name, codes, gpu_info.num_sm, this->num_warps_per_sm,
                              gpu_info.smem_block_total, "", ctx, 1);
        glk->compile(gpu_info);
        float e = this->profile_routine(glk, ctx);
        int r = glk->get_function_attribute(CU_FUNC_ATTRIBUTE_NUM_REGS);
        // Store profile results.
        // Call all callbacks & cache results.
        LOG(INFO, name, ' ', e, "us ", r, "regs");

        for (auto &p : tp) {
            get<1>(p)(e, r);
            wps_cache_json[prof_name(get<0>(p)->sms[0][0])] = {e, r};
        }
        // Performance summary.
    }
    // write the profiling results as json to the cache file
    ofstream cache_file_stream(this->wps_prof_cache_path);
    cache_json[to_string(this->num_warps_per_sm)] = wps_cache_json;
    cache_file_stream << cache_json;
#endif
}

} // namespace ark
```

`ark/sched/sched_profiler.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef _ARK_SCHED_PROFILER_H_
#define _ARK_SCHED_PROFILER_H_

#include "gpu/gpu_kernel.h"
#include "gpu/gpu_mgr.h"
#include "include/ark.h"
#include "sched/sched_codegen.h"
#include "sched/sched_op.h"
#include "sched/sched_opgraph.h"
#include "sched/sched_opseq.h"
#include "sched/sched_tile.h"
namespace ark {

struct SchedPerf
{
    SchedPerf()
    {
    }
    SchedPerf(std::tuple<float, int, int> perf)
        : elapsed{std::get<0>(perf)}, regs_num{std::get<1>(perf)},
          score{std::get<2>(perf)}
    {
        assert(this->elapsed > 0);
        assert(this->regs_num > 0);
    }

    void set(float e, int r, int s = -1)
    {
        assert(e > 0);
        assert(r > 0);
        elapsed = e;
        regs_num = r;
        score = s;
    }
    bool is_set() const
    {
        return elapsed > 0 && regs_num > 0;
    }
    //
    float elapsed = -1;
    int regs_num = -1;
    int score = -1;
};

struct SchedOpSeqPerf
{
    SchedPerf s;
    SchedPerf x;
    SchedPerf y;
    SchedPerf xy;
    std::map<const SchedOpSeq *, SchedPerf> mixed;
};

std::vector<Sched> gen_sched(SchedTileDepth *tile_depths, int num_warps_per_sm);

class SchedProfiler
{
  public:
    SchedProfiler(GpuMgr *gpu_mgr, int num_warps_per_sm_)
        : gpu_mgr{gpu_mgr}, num_warps_per_sm{num_warps_per_sm_}
    {
    }
    void profile(OpGraph *op_graph, CodeGenerator &codegen, GpuMgrCtx *ctx);
    float profile_routine(GpuLoopKernel *glk, GpuMgrCtx *ctx);

    GpuMgr *gpu_mgr;
    int num_warps_per_sm;
    const std::string wps_prof_cache_path = "wps_prof_cache.json";
    std::map<int, std::map<const SchedOpSeq *, SchedOpSeqPerf>>
        wps_prof_results;
};

}; // namespace ark

#endif

```

`ark/sched/sched_stream.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "sched_stream.h"
#include "logging.h"
#include "math.h"
#include "sched_branch.h"
#include <algorithm>
#include <list>
#include <map>
#include <vector>

namespace ark {

class SchedStream::Impl
{
  private:
    int sm_id_begin;
    int sm_id_end;
    int num_warps_per_sm;
    int smem_bytes_per_sm;

    struct BranchInfo
    {
        SchedBranch sched_branch;
        std::map<int, int> sm_id_to_smem_per_warp;
    };

    std::vector<std::unique_ptr<BranchInfo>> branch_infos;

  public:
    Impl(int sm_id_begin, int sm_id_end, int num_warps_per_sm,
         int smem_bytes_per_sm);
    ~Impl();

  protected:
    void add_items(const std::vector<SchedItem> &items);
    void sync();
    void clear();
    std::vector<Stream> get_streams();
    int get_num_sm() const;

    friend class SchedStream;
};

SchedStream::Impl::Impl(int sm_id_begin_, int sm_id_end_, int num_warps_per_sm_,
                        int smem_bytes_per_sm_)
    : sm_id_begin{sm_id_begin_}, sm_id_end{sm_id_end_},
      num_warps_per_sm{num_warps_per_sm_}, smem_bytes_per_sm{smem_bytes_per_sm_}
{
    this->branch_infos.emplace_back(std::make_unique<BranchInfo>());
}

SchedStream::Impl::~Impl()
{
}

void SchedStream::Impl::add_items(const std::vector<SchedItem> &items)
{
    for (auto &item : items) {
        if (item.num_warps_per_uop > this->num_warps_per_sm) {
            LOG(ERROR, "uop requires more warps (", item.num_warps_per_uop,
                ") than available on SM (", this->num_warps_per_sm, ")");
        }
        if (item.smem_bytes_per_uop > this->smem_bytes_per_sm) {
            LOG(ERROR, "uop requires more shared memory (",
                item.smem_bytes_per_uop, ") than available on SM (",
                this->smem_bytes_per_sm, ")");
        }
    }

    auto &branch_info = this->branch_infos.back();

    // Sort items in decreasing order of smem_bytes_per_uop / num_warps_per_uop.
    std::vector<SchedItem> sorted_items(items);
    std::sort(sorted_items.begin(), sorted_items.end(),
              [](const SchedItem &a, const SchedItem &b) {
                  int smem_per_warp_a =
                      math::div_up(a.smem_bytes_per_uop, a.num_warps_per_uop);
                  int smem_per_warp_b =
                      math::div_up(b.smem_bytes_per_uop, b.num_warps_per_uop);
                  if (smem_per_warp_a != smem_per_warp_b) {
                      return smem_per_warp_a > smem_per_warp_b;
                  } else {
                      return a.opseq_id < b.opseq_id;
                  }
              });

    int num_sms = this->sm_id_end - this->sm_id_begin;
    int warps_to_schedule = 0;
    for (auto &item : sorted_items) {
        warps_to_schedule += item.num_warps_per_uop * item.num_uops;
    }
    int target_warps_per_sm = warps_to_schedule / num_sms;
    if (target_warps_per_sm > this->num_warps_per_sm) {
        target_warps_per_sm = this->num_warps_per_sm;
    }

    // opseq_id -> SchedItem
    std::map<int, SchedItem> remaining_items;
    for (auto &item : sorted_items) {
        remaining_items[item.opseq_id] = item;
    }

    // Cache the last scheduled uop_idx for each opseq_id.
    // opseq_id -> uop_idx
    std::map<int, int> uop_idx_cache;

    while (!remaining_items.empty()) {
        std::vector<int> remaining_warps(num_sms, this->num_warps_per_sm);
        std::vector<int> done_items;
        bool no_progress = true;

        for (auto &p : remaining_items) {
            auto &item = p.second;
            int current_sm_idx = this->sm_id_begin;
            int current_warp_idx = 0;
            int uop_idx = uop_idx_cache[item.opseq_id];

            while (uop_idx < item.num_uops &&
                   current_sm_idx < this->sm_id_end) {
                int rem_warp =
                    remaining_warps[current_sm_idx - this->sm_id_begin];
                if (rem_warp < item.num_warps_per_uop) {
                    // No room on this SM for this uop, move to next SM
                    current_sm_idx++;
                    current_warp_idx = 0;
                    continue;
                }

                int target_rem_warp =
                    this->num_warps_per_sm -
                    std::max(item.num_warps_per_uop, target_warps_per_sm);
                if (rem_warp <= target_rem_warp) {
                    // This SM has too many warps scheduled, move to next SM
                    current_sm_idx++;
                    current_warp_idx = 0;
                    continue;
                }

                int smem_per_warp =
                    branch_info->sm_id_to_smem_per_warp[current_sm_idx];
                int item_smem_per_warp = math::div_up(item.smem_bytes_per_uop,
                                                      item.num_warps_per_uop);
                int max_smem_per_warp =
                    std::max(smem_per_warp, item_smem_per_warp);
                if (max_smem_per_warp > 0) {
                    int max_warps_per_sm =
                        std::min(this->num_warps_per_sm,
                                 this->smem_bytes_per_sm / max_smem_per_warp);
                    if (current_warp_idx >= max_warps_per_sm) {
                        // This SM has too much shared memory used, move to next
                        // SM
                        current_sm_idx++;
                        current_warp_idx = 0;
                        continue;
                    }
                }
                branch_info->sm_id_to_smem_per_warp[current_sm_idx] =
                    max_smem_per_warp;

                // Schedule this uop on this SM
                branch_info->sched_branch.add(
                    item.opseq_id, uop_idx, current_sm_idx, current_warp_idx,
                    current_warp_idx + item.num_warps_per_uop);
                if (current_warp_idx + item.num_warps_per_uop >
                    this->num_warps_per_sm) {
                    LOG(ERROR, "unexpected error");
                }
                remaining_warps[current_sm_idx - this->sm_id_begin] -=
                    item.num_warps_per_uop;
                current_warp_idx += item.num_warps_per_uop;
                uop_idx++;
                no_progress = false;
            }
            if (uop_idx == item.num_uops) {
                done_items.push_back(item.opseq_id);
            }
            uop_idx_cache[item.opseq_id] = uop_idx;
        }

        if (no_progress) {
            auto &item = remaining_items.begin()->second;
            LOG(ERROR,
                "Unable to schedule any items (given resources: "
                "num_warps_per_sm=",
                this->num_warps_per_sm,
                " smem_bytes_per_sm=", this->smem_bytes_per_sm,
                "). For example: opseq_id=", item.opseq_id,
                " num_uops=", item.num_uops,
                " num_warps_per_uop=", item.num_warps_per_uop,
                " smem_bytes_per_uop=", item.smem_bytes_per_uop);
        }

        // Remove items that are done
        for (int done_opseq_id : done_items) {
            remaining_items.erase(done_opseq_id);
        }
    }

    if (sorted_items.size() > 0) {
        this->sync();
    }
}

void SchedStream::Impl::sync()
{
    this->branch_infos.emplace_back(std::make_unique<BranchInfo>());
}

void SchedStream::Impl::clear()
{
    this->branch_infos.clear();
}

std::vector<Stream> SchedStream::Impl::get_streams()
{
    std::vector<Stream> streams;
    for (auto &branch_info : this->branch_infos) {
        Stream stream;
        stream.branches = branch_info->sched_branch.get_branches(
            branch_info->sm_id_to_smem_per_warp);
        streams.emplace_back(std::move(stream));
    }
    return streams;
}

int SchedStream::Impl::get_num_sm() const
{
    return this->sm_id_end - this->sm_id_begin;
}

SchedStream::SchedStream(int sm_id_begin, int sm_id_end, int num_warps_per_sm,
                         int smem_bytes_per_sm)
{
    this->impl = std::make_unique<Impl>(sm_id_begin, sm_id_end,
                                        num_warps_per_sm, smem_bytes_per_sm);
}

SchedStream::~SchedStream()
{
}

void SchedStream::add_items(const std::vector<SchedItem> &items)
{
    this->impl->add_items(items);
}

void SchedStream::sync()
{
    this->impl->sync();
}

void SchedStream::clear()
{
    this->impl->clear();
}

std::vector<Stream> SchedStream::get_streams()
{
    return this->impl->get_streams();
}

int SchedStream::get_num_sm() const
{
    return this->impl->get_num_sm();
}

} // namespace ark

```

`ark/sched/sched_stream.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_SCHED_STREAM_H_
#define ARK_SCHED_STREAM_H_

#include "sched_branch.h"
#include <map>
#include <memory>
#include <vector>

namespace ark {

struct SchedItem
{
    int opseq_id;
    int num_uops;
    int num_warps_per_uop;
    int smem_bytes_per_uop;
};

struct Stream
{
    /// Ordered list of branches in the stream
    std::vector<Branch> branches;
    /// sm_id -> assigned smem bytes per warp
    std::map<int, int> sm_id_to_smem_per_warp;
};

class SchedStream
{
  public:
    SchedStream(int sm_id_begin, int sm_id_end, int num_warps_per_sm,
                int smem_bytes_per_sm);
    ~SchedStream();

    void add_items(const std::vector<SchedItem> &items);
    void sync();
    void clear();
    std::vector<Stream> get_streams();

    int get_num_sm() const;

  private:
    class Impl;
    std::unique_ptr<Impl> impl;
};

} // namespace ark

#endif // ARK_SCHED_STREAM_H_

```

`ark/sched/sched_stream_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "ark.h"
#include "logging.h"
#include "unittest/unittest_utils.h"

int main()
{
    ark::init();
    return 0;
}

```

`ark/sched/sched_test.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "gpu/gpu_kernel.h"
#include "include/ark.h"
#include "include/ark_utils.h"
#include "logging.h"
#include "sched/sched.h"
#include "unittest/unittest_utils.h"

using namespace std;
using namespace ark;

ark::unittest::State test_sched_mm_add()
{
    unittest::spawn_process([&]() {
        DimType batch_size = 1;
        DimType dim_input = 2048;
        DimType dim_hidden = 12288;
        TensorType dtype = FP16;

        Model model;
        Tensor *input =
            model.tensor({batch_size, dim_input, dim_hidden}, dtype);
        Tensor *weight = model.tensor({dim_hidden, dim_hidden}, dtype);

        Tensor *mm = model.matmul(input, weight);
        /* Tensor *mm_add = */ model.add(mm, input);

        GpuMgr *mgr = get_gpu_mgr(0);
        const GpuInfo &ginfo = mgr->get_gpu_info();
        ark::DefaultScheduler sched{model, 0, 0, 1, 8};
        GpuMgrCtx *ctx = sched.create_context("test_sched_mm_add");
        sched.schedule();
        auto codes = sched.gen_code();

        GpuLoopKernel glk{"test_sched_mm_add",
                          codes,
                          (unsigned int)ginfo.num_sm,
                          8,
                          (unsigned int)ginfo.smem_block_total,
                          "",
                          ctx};
        glk.compile(ginfo);
        glk.load();
        GpuStream stream = ctx->create_stream();
        GpuState ret = glk.launch(stream, false);
        UNITTEST_EQ(ret, 0);
        int iter = 1000;
        glk.run(iter);
        glk.stop();

        LOG(INFO, "test_sched_mm_add: batch_size ", batch_size, " dim_input ",
            dim_input, " dim_hidden ", dim_hidden, " dtype ", dtype,
            " elapsed ", glk.get_elapsed_msec() / (float)iter, " ms/iter");

        return unittest::SUCCESS;
    });
    ark::unittest::wait_all_processes();
    return unittest::SUCCESS;
}

ark::unittest::State test_scheduler_simple_mm()
{
    // Hidden dimension of the dense layer.
    unsigned int units = 2048;
    // Input dimension of the dense layer.
    unsigned int in_dim = 2048;
    // Extra dimension of the input. CHANNEL=1 for 2D inputs.
    unsigned int channel = 2048;
    // Batch size of the input.
    unsigned int batch_size = 1;

    Model m;
    Tensor *input = m.tensor({batch_size, channel, in_dim}, FP16);
    Tensor *weight = m.tensor({in_dim, units}, FP16);
    m.matmul(input, weight);

    GpuMgr *mgr = get_gpu_mgr(0);
    const GpuInfo &ginfo = mgr->get_gpu_info();

    DefaultScheduler sched{m, 0, 0, 1, 8};
    GpuMgrCtx *ctx = sched.create_context("test_scheduler_simple_mm");
    sched.schedule();
    auto codes = sched.gen_code();

    GpuLoopKernel glk{"test_scheduler_simple_mm",
                      codes,
                      (unsigned int)ginfo.num_sm,
                      8,
                      (unsigned int)ginfo.smem_block_total,
                      "",
                      ctx};
    glk.compile(ginfo);
    glk.load();

    GpuStream stream = ctx->create_stream();
    for (int i = 0; i < 10; ++i) {
        GpuState ret = glk.launch(stream, false);
        UNITTEST_EQ(ret, 0);
        glk.run(100);
        glk.stop();
        LOG(INFO, glk.get_elapsed_msec());
    }

    return unittest::SUCCESS;
}

Tensor *MultiheadAttention(Model *model, Tensor *input, DimType embed_dim,
                           DimType num_heads, float dropout, TensorType dtype)
{
    // input: (batch_size, seq_len, embed_dim)
    // output: (batch_size, seq_len, embed_dim)
    Tensor *w_q_proj = model->tensor({embed_dim, embed_dim}, dtype);
    Tensor *w_k_proj = model->tensor({embed_dim, embed_dim}, dtype);
    Tensor *w_v_proj = model->tensor({embed_dim, embed_dim}, dtype);
    Tensor *w_out_proj = model->tensor({embed_dim, embed_dim}, dtype);

    Tensor *q_proj = model->matmul(input, w_q_proj);
    Tensor *k_proj = model->matmul(input, w_k_proj);
    Tensor *v_proj = model->matmul(input, w_v_proj);
    Tensor *q_proj_r_t =
        model->reshape(q_proj, {input->shape[0], input->shape[1], num_heads,
                                embed_dim / num_heads});
    Tensor *k_proj_r_t =
        model->reshape(k_proj, {input->shape[0], input->shape[1], num_heads,
                                embed_dim / num_heads});
    Tensor *v_proj_r_t =
        model->reshape(v_proj, {input->shape[0], input->shape[1], num_heads,
                                embed_dim / num_heads});
    // Tensor *q_proj_r_t = model->transpose(q_proj_r, {0, 2, 1, 3});
    // Tensor *k_proj_r_t = model->transpose(k_proj_r, {0, 2, 1, 3});
    // Tensor *v_proj_r_t = model->transpose(v_proj_r, {0, 2, 1, 3});
    q_proj_r_t =
        model->reshape(q_proj_r_t, {input->shape[0] * num_heads,
                                    input->shape[1], embed_dim / num_heads});
    k_proj_r_t =
        model->reshape(k_proj_r_t, {input->shape[0] * num_heads,
                                    input->shape[1], embed_dim / num_heads});
    v_proj_r_t =
        model->reshape(v_proj_r_t, {input->shape[0] * num_heads,
                                    input->shape[1], embed_dim / num_heads});

    // scaled dot product
    Tensor *attn_logits =
        model->matmul(q_proj_r_t, k_proj_r_t, nullptr, 1, false, true);
    Tensor *attn_logits_scaled =
        model->scale(attn_logits, 1.0 / sqrt(embed_dim / num_heads));

    // Tensor *attention = model->softmax(attn_logits_scaled, 2);
    Tensor *attention = attn_logits_scaled;
    Tensor *values = model->matmul(attention, v_proj_r_t);
    // values = model->reshape(values, {input->shape[0], num_heads,
    // input->shape[1], embed_dim / num_heads});

    // Tensor *values_t = model->transpose(values, {0, 2, 1, 3});
    Tensor *values_t_r =
        model->reshape(values, {input->shape[0], input->shape[1], embed_dim});
    Tensor *output = model->matmul(values_t_r, w_out_proj);

    if (dropout > 0.0) {
        // output = model->dropout(output, dropout);
    }
    return output;
}

Tensor *TransformerLayerForward(Model *model, Tensor *input, DimType embed_dim,
                                DimType num_heads, DimType dim_ff,
                                float dropout, TensorType dtype)
{
    Tensor *attn_out =
        MultiheadAttention(model, input, embed_dim, num_heads, dropout, dtype);
    Tensor *res = model->add(input, attn_out);
    // res = model->layernorm(res, res);

    Tensor *w_ff1 = model->tensor({embed_dim, dim_ff}, dtype);
    Tensor *w_ff2 = model->tensor({dim_ff, embed_dim}, dtype);

    Tensor *ff1 = model->matmul(res, w_ff1);
    Tensor *ff2 = model->matmul(ff1, w_ff2);
    Tensor *ret = model->add(res, ff2);
    // ret = model->layernorm(ret, ret);
    return ret;
}

Tensor *GPT3LayerForward(Model *model, Tensor *input, TensorType dtype)
{
    return TransformerLayerForward(model, input,
                                   /*embed_dim=*/12288,
                                   /*num_heads=*/96,
                                   /*dim_ff=*/49152,
                                   /*dropout=*/0.0, dtype);
}

ark::unittest::State test_sched_gpt3()
{
    Model model;
    DimType batch_size = 1;
    DimType seq_len = 2048;
    DimType embed_dim = 12288;
    TensorType dtype = FP16;
    Tensor *input = model.tensor({batch_size, seq_len, embed_dim}, dtype);
    GPT3LayerForward(&model, input, dtype);

    unittest::spawn_process([&]() {
        GpuMgr *mgr = get_gpu_mgr(0);
        const GpuInfo &ginfo = mgr->get_gpu_info();
        ark::DefaultScheduler sched{model, 0, 0, 1, 8};
        GpuMgrCtx *ctx = sched.create_context("test_sched_gpt3");
        sched.schedule();
        auto codes = sched.gen_code();

        GpuLoopKernel glk{"test_sched_gpt3",
                          codes,
                          (unsigned int)ginfo.num_sm,
                          8,
                          (unsigned int)ginfo.smem_block_total,
                          "",
                          ctx};
        glk.compile(ginfo);
        glk.load();
        GpuStream stream = ctx->create_stream();
        GpuState ret = glk.launch(stream, false);
        UNITTEST_EQ(ret, 0);
        int iter = 100;
        glk.run(iter);
        glk.stop();

        LOG(INFO, "test_sched_gpt3: batch_size ", batch_size, " seq_len ",
            seq_len, " embed_dim ", embed_dim, " dtype ", dtype, " elapsed ",
            glk.get_elapsed_msec() / (float)iter, " ms/iter");

        return unittest::SUCCESS;
    });
    ark::unittest::wait_all_processes();
    return unittest::SUCCESS;
}

ark::unittest::State test_sched_comp_baseline()
{
    // Hidden dimension of the dense layer.
    unsigned int units = 512;
    // Input dimension of the dense layer.
    unsigned int in_dim = 512;
    // Extra dimension of the input. CHANNEL=1 for 2D inputs.
    unsigned int channel = 512;
    // Batch size of the input.
    unsigned int batch_size = 1;
    int bytes = channel * in_dim * sizeof(ark::half_t);
    int input_tensor_num = 3;
    ark::srand();
    vector<unique_ptr<ark::half_t[]>> input_data(input_tensor_num);
    for (int i = 0; i < input_tensor_num; i++) {
        input_data[i] = ark::utils::rand_halfs(channel * in_dim, 0.01);
    }
    // the result of the new scheduler
    ark::half_t *output_data1 = (ark::half_t *)malloc(bytes);
    UNITTEST_NE(output_data1, (void *)nullptr);

    // the result of the old scheduler
    ark::half_t *output_data2 = (ark::half_t *)malloc(bytes);
    UNITTEST_NE(output_data2, (void *)nullptr);

    // test the baseline scheduler
    ark::unittest::spawn_process([&]() {
        Model m;
        Tensor *input[3];
        input[0] = m.tensor({batch_size, channel, in_dim}, FP16);
        input[1] = m.tensor({batch_size, in_dim, units}, FP16);
        input[2] = m.tensor({batch_size, units, units}, FP16);

        Tensor *middle_result = m.matmul(input[0], input[1]);

        Tensor *middle_result1 = m.add(middle_result, input[2]);
        Tensor *output = m.scale(middle_result1, 2.3);
        GpuMgr *mgr = get_gpu_mgr(0);
        const GpuInfo &ginfo = mgr->get_gpu_info();
        ark::SimpleScheduler sched{m, 0, 0, 1, 8};
        GpuMgrCtx *ctx = sched.create_context("test_scheduler_simple_mm");
        sched.schedule();
        auto codes = sched.gen_code();

        GpuLoopKernel glk{"test_scheduler_simple_mm",
                          codes,
                          (unsigned int)ginfo.num_sm,
                          8,
                          (unsigned int)ginfo.smem_block_total,
                          "",
                          ctx};
        glk.compile(ginfo);
        for (int i = 0; i < input_tensor_num; i++) {
            // Get the auto-scheduled buffers.
            ark::GpuBuf *input_tensor_buf = sched.get_gpu_buf(input[i]);
            UNITTEST_NE(input_tensor_buf, (ark::GpuBuf *)nullptr);

            // Set data.
            ark::gpu_memcpy(input_tensor_buf, input_data[i].get(), bytes);
        }
        // load the data into the input

        glk.load();

        GpuStream stream = ctx->create_stream();
        GpuState ret = glk.launch(stream, false);
        UNITTEST_EQ(ret, 0);
        glk.run(1);
        glk.stop();
        ark::GpuBuf *output_tensor_buf = sched.get_gpu_buf(output);
        ark::gpu_memcpy(output_data1, output_tensor_buf, bytes);
        for (int i = 0; i < 10; i++) {
            LOG(DEBUG, "output_data1: ", (float)output_data1[i]);
        }
        return unittest::SUCCESS;
    });
    ark::unittest::wait_all_processes();

    // test the old scheduler
    ark::unittest::spawn_process([&]() {
        Model m;
        Tensor *input[3];
        input[0] = m.tensor({batch_size, channel, in_dim}, FP16);
        input[1] = m.tensor({batch_size, in_dim, units}, FP16);
        input[2] = m.tensor({batch_size, units, units}, FP16);

        Tensor *middle_result = m.matmul(input[0], input[1]);

        Tensor *middle_result1 = m.add(middle_result, input[2]);
        Tensor *output = m.scale(middle_result1, 2.3);

        GpuMgr *mgr = get_gpu_mgr(0);
        const GpuInfo &ginfo = mgr->get_gpu_info();
        ark::DefaultScheduler sched{m, 0, 0, 1, 8};
        GpuMgrCtx *ctx = sched.create_context("test_scheduler_simple_mm");
        sched.schedule();
        auto codes = sched.gen_code();

        GpuLoopKernel glk{"test_scheduler_simple_mm",
                          codes,
                          (unsigned int)ginfo.num_sm,
                          8,
                          (unsigned int)ginfo.smem_block_total,
                          "",
                          ctx};
        glk.compile(ginfo);
        for (int i = 0; i < input_tensor_num; i++) {
            // Get the auto-scheduled buffers.
            ark::GpuBuf *input_tensor_buf = sched.get_gpu_buf(input[i]);
            UNITTEST_NE(input_tensor_buf, (ark::GpuBuf *)nullptr);

            // Set data.
            ark::gpu_memcpy(input_tensor_buf, input_data[i].get(), bytes);
        }
        glk.load();
        GpuStream stream = ctx->create_stream();
        GpuState ret = glk.launch(stream, false);
        UNITTEST_EQ(ret, 0);
        glk.run(1);
        glk.stop();
        ark::GpuBuf *output_tensor_buf = sched.get_gpu_buf(output);
        ark::gpu_memcpy(output_data2, output_tensor_buf, bytes);
        for (int i = 0; i < 10; i++) {
            LOG(DEBUG, "output_data2: ", (float)output_data2[i]);
        }
        return unittest::SUCCESS;
    });
    ark::unittest::wait_all_processes();
    // TODO: the output data are set on different processes,  we need to copy
    //  run the test on the same process
    auto p =
        ark::utils::cmp_matrix((ark::half_t *)output_data1,
                               (ark::half_t *)output_data2, channel, units);
    LOG(ark::INFO, " scheduler compare test: ", " total_bytes: ", bytes,
        " iter: ", 1, setprecision(4), " mse: ", p.first,
        " max_err: ", p.second * 100, "%");
    return unittest::SUCCESS;
}

int main()
{
    ark::init();
    UNITTEST(test_sched_mm_add);
    // UNITTEST(test_scheduler_simple_mm);
    UNITTEST(test_sched_gpt3);
    UNITTEST(test_sched_comp_baseline);
    return 0;
}

```

`ark/sched/sched_tile.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "json.h"
#include <algorithm>
#include <cassert>
#include <fstream>
#include <initializer_list>
#include <ostream>
#include <unistd.h>

#include "logging.h"
#include "math.h"
#include "sched/sched_tile.h"

using namespace std;

#define COM ", "
#define OP_PREFIX "op"

#define USE_KAHYPAR 0
#define EVAL_DEPTHS 1
#define COMPRESS_BRANCH 1
#define MATMUL_GRAPH_OPT 1
#define ALLOC_UNUSED_TENSORS 1

namespace ark {

SchedTile::SchedTile(const SchedOpSeq *opseq_, int x, int y, int z)
    : opseq{opseq_}
{
    auto &tdims = opseq_->get_tdims();
    this->id = x + (y * tdims[2]) + (z * tdims[2] * tdims[1]);
}
SchedTile::SchedTile(const SchedOpSeq *opseq_, int id_) : id{id_}, opseq{opseq_}
{
}

SchedTileSet::SchedTileSet(std::initializer_list<SchedTile> tiles_,
                           SchedTileSetType type_)
    : tiles{tiles_}, type{type_}
{
}

SchedTileDepth::SchedTileDepth(int num_sm)
{
    this->sms.resize(num_sm);
}
void SchedTileDepth::append_tiles(int sm_id, initializer_list<SchedTile> tiles,
                                  SchedTileSetType type)
{
    assert((size_t)sm_id < this->sms.size());
    this->sms[sm_id].emplace_back(tiles, type);
}

void SchedTileDepth::clear()
{
    for (auto &sm : this->sms) {
        sm.clear();
    }
}

} // namespace ark
```

`ark/sched/sched_tile.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef _ARK_SCHED_TILE_H_
#define _ARK_SCHED_TILE_H_
#include "sched/sched_opseq.h"
// #include "sched/sched/sched_profiler.h"
namespace ark {

struct SchedTile
{
    SchedTile(const SchedOpSeq *opseq, int x, int y, int z);
    SchedTile(const SchedOpSeq *opseq, int id);
    //
    bool operator<(const SchedTile &rhs) const
    {
        const int &o_id = opseq->get_id();
        const int &o_rhs_id = rhs.opseq->get_id();
        return o_id == o_rhs_id ? id < rhs.id : o_id < o_rhs_id;
    }
    //
    int id;
    const SchedOpSeq *opseq;
};

typedef enum
{
    SCHED_TILE_SET_S,
    SCHED_TILE_SET_X,
    SCHED_TILE_SET_Y,
    SCHED_TILE_SET_XY,
    SCHED_TILE_SET_MIXED,
} SchedTileSetType;

struct SchedTileSet
{
    SchedTileSet()
    {
    }
    SchedTileSet(std::initializer_list<SchedTile> tiles, SchedTileSetType type);

    int get_num_warps() const
    {
        int sum = 0;
        for (auto &t : tiles)
            sum += t.opseq->get_num_warps();
        return sum;
    }
    //
    std::vector<SchedTile> tiles;
    SchedTileSetType type;
    // SchedPerf perf;
};

struct SchedTileDepth
{
    SchedTileDepth(int num_sm);

    void append_tiles(int sm_id, std::initializer_list<SchedTile> tiles,
                      SchedTileSetType type);
    void clear();

    int get_num_sm() const
    {
        return (int)sms.size();
    }
    int get_num_warps() const
    {
        int max_num_warps = 0;
        for (auto &sm : sms) {
            for (auto &ts : sm) {
                int nw = ts.get_num_warps();
                if (nw > max_num_warps)
                    max_num_warps = nw;
            }
        }
        return max_num_warps;
    }
    bool is_full() const
    {
        for (auto &sm : sms) {
            if (sm.size() == 0)
                return false;
        }
        return true;
    }
    std::vector<std::vector<SchedTileSet>> sms;
};

} // namespace ark

#endif // _ARK_SCHED_TILE_H_

```

`ark/tensor.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "tensor.h"
#include "logging.h"
#include "math.h"
#include <string>

using namespace std;

namespace ark {

TensorBuf::TensorBuf(const DimType &bytes_, int id_) : bytes{bytes_}, id{id_}
{
}

// Tensor constructor
Tensor::Tensor(const Dims &shape_, TensorType type_, TensorBuf *buf_,
               const Dims &ldims_, const Dims &offs_, const Dims &pads_,
               bool exported_, int imported_rank_, int id_, const string &name_)
    : buf{buf_}, type{type_}, exported{exported_},
      imported_rank{imported_rank_}, id{id_}, name{name_}
{
    if (shape_.size() == 0) {
        LOGERR("Tensor shape should consist of positive numbers. Given: ",
               shape_);
    } else if (shape_.is_no_dim()) {
        // Assume a single-element constant
        this->shape = {1};
    } else {
        this->shape = shape_;
    }
    int ndims = this->shape.ndims();
    if (ldims_.is_no_dim()) {
        this->ldims = this->shape;
    } else {
        if (ndims != ldims_.ndims()) {
            LOGERR("Tensor shape and ldims should have the same number of "
                   "dimensions. Given: shape ",
                   this->shape, " ldims ", ldims_);
        }
        this->ldims = ldims_;
    }
    if (offs_.is_no_dim()) {
        vector<DimType> dims_vec;
        for (int i = 0; i < ndims; ++i) {
            dims_vec.push_back(0);
        }
        this->offs = Dims{dims_vec};
    } else {
        if (ndims != offs_.ndims()) {
            LOGERR("Tensor shape and offs should have the same number of "
                   "dimensions. Given: shape ",
                   this->shape, " offs ", offs_);
        }
        this->offs = offs_;
    }
    if (pads_.is_no_dim()) {
        vector<DimType> dims_vec;
        for (int i = 0; i < ndims; ++i) {
            dims_vec.push_back(1);
        }
        this->pads = Dims{dims_vec};
    } else {
        if (ndims != pads_.ndims()) {
            LOGERR("Tensor shape and pads should have the same number of "
                   "dimensions. Given: shape ",
                   this->shape, " pads ", pads_);
        }
        this->pads = pads_;
    }
    for (int i = 0; i < ndims; ++i) {
        if (this->ldims[i] % this->pads[i] != 0) {
            LOGERR("Tensor ldims should be a multiple of pads. ldims ",
                   this->ldims, " pads ", this->pads);
        }
    }
    for (int i = 0; i < ndims; ++i) {
        if (this->offs[i] + this->shape[i] > this->ldims[i]) {
            LOGERR("Tensor exceeds the memory boundary. offs ", this->offs,
                   " shape ", this->shape, " ldims ", this->ldims);
        }
    }
}

//
void Tensor::update_pads(const vector<DimType> &pads_)
{
    int ndims = this->ldims.ndims();
    vector<DimType> tmp;
    for (int i = 0; i < ndims - (int)pads_.size(); ++i) {
        tmp.emplace_back(1);
    }
    for (int i = 0; i < (int)pads_.size(); ++i) {
        tmp.emplace_back(pads_[i] == -1 ? 1 : pads_[i]);
    }
    Dims new_pads{tmp};
    for (int i = 0; i < ndims; ++i) {
        DimType new_udim = math::lcm(this->pads[i], new_pads[i]);
        this->pads[i] = new_udim;
        this->ldims[i] = math::pad(this->ldims[i], new_udim);
    }
}

// Offset to the element [i0][i1][i2][i3] of this tensor in the TensorBuf.
DimType Tensor::offset(DimType i0, DimType i1, DimType i2, DimType i3) const
{
    auto &l = this->ldims;
    auto &o = this->offs;
    int ndims = this->shape.ndims();
    if (ndims == 0) {
        return 0;
    } else if (ndims == 1) {
        return o[0] + i0;
    } else if (ndims == 2) {
        return ((o[0] + i0) * l[1]) + o[1] + i1;
    } else if (ndims == 3) {
        return ((o[0] + i0) * l[1] * l[2]) + ((o[1] + i1) * l[2]) + o[2] + i2;
    }
    return ((o[0] + i0) * l[1] * l[2] * l[3]) + ((o[1] + i1) * l[2] * l[3]) +
           ((o[2] + i2) * l[3]) + o[3] + i3;
}

// Number of elements in the tensor excluding padding.
DimType Tensor::size() const
{
    return this->shape.size();
}

// Number of dimensions in the tensor.
int Tensor::ndims() const
{
    return this->shape.ndims();
}

// Shape of the tensor including padding.
Dims Tensor::padded_shape() const
{
    std::vector<DimType> padded_shape;
    for (int i = 0; i < this->shape.ndims(); ++i) {
        padded_shape.push_back(math::pad(this->shape[i], this->pads[i]));
    }
    Dims ps{padded_shape};
    return ps;
}

// Number of bytes of each element in the tensor.
unsigned int Tensor::type_bytes() const
{
    if (this->type == FP16) {
        return 2;
    } else if (this->type == FP32) {
        return 4;
    } else if (this->type == INT32) {
        return 4;
    } else if (this->type == BYTE) {
        return 1;
    }
    return 0;
}

// Number of bytes of the tensor.
DimType Tensor::shape_bytes() const
{
    return this->shape.size() * this->type_bytes();
}

// Should be the same as the number of bytes of the TensorBuf.
DimType Tensor::ldims_bytes() const
{
    return this->ldims.size() * this->type_bytes();
}

// Offset in bytes.
DimType Tensor::offset_bytes(DimType i0, DimType i1, DimType i2,
                             DimType i3) const
{
    return this->offset(i0, i1, i2, i3) * this->type_bytes();
}

// TODO: deprecate this function.
bool Tensor::is_sequential() const
{
    // if a tensor's last (ndims-1) shape is the same as its ldims, the tensor
    // is sequential
    int ndims = this->shape.ndims();
    for (int i = 1; i < ndims; ++i) {
        if (this->shape[i] != this->ldims[i]) {
            return false;
        }
    }
    return true;
}

const string type_str(const TensorType &type)
{
    if (type == FP16)
        return "fp16";
    else if (type == FP32)
        return "fp32";
    else if (type == INT32)
        return "int32";
    else if (type == BYTE)
        return "byte";
    return "none";
}

std::ostream &operator<<(std::ostream &os, TensorType type)
{
    os << type_str(type);
    return os;
}

} // namespace ark

```

`ark/tensor.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_TENSOR_H_
#define ARK_TENSOR_H_

#include "include/ark.h"
#include <string>

namespace ark {

const std::string type_str(const TensorType &type);
std::ostream &operator<<(std::ostream &os, TensorType type);

} // namespace ark

#endif // ARK_TENSOR_H_

```

`ark/threading.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_THREADING_H_
#define ARK_THREADING_H_

#include <functional>
#include <mutex>
#include <thread>
#include <vector>

namespace ark {

template <typename ItemType>
void para_exec(std::vector<ItemType> &items, int max_num_threads,
               const std::function<void(ItemType &)> &func)
{
    size_t nthread = (size_t)max_num_threads;
    if (nthread > items.size()) {
        nthread = items.size();
    }
    std::vector<std::thread> threads;
    threads.reserve(nthread);
    std::mutex mtx;
    size_t idx = 0;
    for (size_t i = 0; i < nthread; ++i) {
        threads.emplace_back([&items, &mtx, &idx, &func] {
            size_t local_idx = -1;
            for (;;) {
                {
                    const std::lock_guard<std::mutex> lock(mtx);
                    local_idx = idx++;
                }
                if (local_idx >= items.size())
                    break;
                func(items[local_idx]);
            }
        });
    }
    for (auto &t : threads) {
        t.join();
    }
}

} // namespace ark

#endif // ARK_THREADING_H_

```

`ark/unittest/unittest_utils.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <signal.h>
#include <sys/types.h>
#include <sys/wait.h>
#include <unistd.h>
#include <vector>

#include "file_io.h"
#include "logging.h"
#include "unittest/unittest_utils.h"

using namespace std;

// Grep SIGALRM and exit.
static void sigalrm_timeout_handler(int)
{
    signal(SIGALRM, SIG_IGN);
    UNITTEST_FEXIT("timeout");
}

namespace ark {
namespace unittest {

// Temporal unittest states.
struct TempStates
{
    vector<int> pids;
    vector<thread *> threads;
};

TempStates GLOBAL_TEMP_STATES_;

// Set a timeout of the current process.
Timeout::Timeout(int timeout)
{
    signal(SIGALRM, sigalrm_timeout_handler);
    alarm(timeout);
}

// Remove the timeout.
Timeout::~Timeout()
{
    alarm(0);
    signal(SIGALRM, SIG_DFL);
}

// Spawn a thread that runs the given function.
thread *spawn_thread(function<State()> func)
{
    thread *t = new thread(func);
    GLOBAL_TEMP_STATES_.threads.emplace_back(t);
    return t;
}

// Wait for all threads to finish.
void wait_all_threads()
{
    for (thread *t : GLOBAL_TEMP_STATES_.threads) {
        if (t->joinable()) {
            t->join();
        }
        delete t;
    }
    GLOBAL_TEMP_STATES_.threads.clear();
}

// Spawn a process that runs the given function.
int spawn_process(function<State()> func)
{
    pid_t pid = fork();
    if (pid < 0) {
        UNITTEST_UEXIT("fork() failed");
    } else if (pid == 0) {
        State ret = func();
        std::exit(ret);
    }
    GLOBAL_TEMP_STATES_.pids.push_back(pid);
    return (int)pid;
}

// Wait for all processes to finish.
void wait_all_processes()
{
    size_t nproc = GLOBAL_TEMP_STATES_.pids.size();
    for (size_t i = 0; i < nproc; ++i) {
        pid_t pid;
        int status;
        do {
            pid = wait(&status);
            if (pid == -1) {
                UNITTEST_UEXIT("wait() failed");
            }
        } while (!WIFEXITED(status));
        status = WEXITSTATUS(status);
        if (status != State::SUCCESS) {
            UNITTEST_EXIT((State)status, "process " + to_string(pid));
        }
    }
    GLOBAL_TEMP_STATES_.pids.clear();
}

// Run the given test function.
State test(function<State()> test_func)
{
    return test_func();
}

//
string get_kernel_code(const string &name)
{
    return ark::read_file(ark::get_dir(string{__FILE__}) + "/../ops/kernels/" +
                          name + ".h");
}

} // namespace unittest
} // namespace ark

```

`ark/unittest/unittest_utils.h`:

```h
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#ifndef ARK_UNITTEST_UNITTEST_UTILS_H_
#define ARK_UNITTEST_UNITTEST_UTILS_H_

#include <cstdlib>
#include <functional>
#include <iomanip>
#include <string>
#include <thread>

#include "cpu_timer.h"
#include "gpu/gpu_buf.h"
#include "logging.h"

namespace ark {
namespace unittest {

typedef enum
{
    SUCCESS = 0,
    FAILURE,
    UNEXPECTED
} State;

void exit(State s, const std::string &errmsg);
void fexit(const std::string &errmsg = "");
void uexit(const std::string &errmsg = "");
void sexit(const std::string &errmsg = "");

//
class Timeout
{
  public:
    Timeout(int timeout);
    ~Timeout();
};

std::thread *spawn_thread(std::function<State()> func);
void wait_all_threads();

int spawn_process(std::function<State()> func);
void wait_all_processes();

State test(std::function<State()> test_func);
//
std::string get_kernel_code(const std::string &name);

} // namespace unittest
} // namespace ark

// Run the given test function.
#define UNITTEST(test_func)                                                    \
    do {                                                                       \
        LOG(ark::INFO, "unittest start: " #test_func);                         \
        double _s = ark::cpu_timer();                                          \
        ark::unittest::State _ret = ark::unittest::test(test_func);            \
        double _e = ark::cpu_timer() - _s;                                     \
        if (_ret != ark::unittest::SUCCESS) {                                  \
            UNITTEST_EXIT(_ret, "Unexpected exit");                            \
        }                                                                      \
        LOG(ark::INFO, "unittest succeed: " #test_func " (elapsed ",           \
            std::setprecision(4), _e, "s)");                                   \
    } while (0)

// Exit with proper error messages and return values.
#define UNITTEST_EXIT(state, ...)                                              \
    do {                                                                       \
        if ((state) == ark::unittest::FAILURE) {                               \
            LOG(ark::ERROR, "unittest failed: ", __VA_ARGS__);                 \
        } else if ((state) == ark::unittest::UNEXPECTED) {                     \
            LOG(ark::ERROR,                                                    \
                "Unexpected error during unittest: ", __VA_ARGS__);            \
        } else if ((state) == ark::unittest::SUCCESS) {                        \
            LOG(ark::INFO, "unittest succeed");                                \
        }                                                                      \
        std::exit(state);                                                      \
    } while (0)

// Fail the test.
#define UNITTEST_FEXIT(...) UNITTEST_EXIT(ark::unittest::FAILURE, __VA_ARGS__)
// Unexpected error during test.
#define UNITTEST_UEXIT(...)                                                    \
    UNITTEST_EXIT(ark::unittest::UNEXPECTED, __VA_ARGS__)
// Success.
#define UNITTEST_SEXIT() UNITTEST_EXIT(ark::unittest::SUCCESS, "")

// Check if the given condition is true.
#define UNITTEST_TRUE(cond)                                                    \
    do {                                                                       \
        if (cond) {                                                            \
            break;                                                             \
        }                                                                      \
        UNITTEST_FEXIT("condition `" #cond "` failed");                        \
    } while (0)
// Check if the given expressions are equal.
#define UNITTEST_EQ(exp0, exp1)                                                \
    do {                                                                       \
        auto _v0 = (exp0);                                                     \
        auto _v1 = (exp1);                                                     \
        if (_v0 == _v1) {                                                      \
            break;                                                             \
        }                                                                      \
        UNITTEST_FEXIT("`" #exp0 "` (value: ", _v0,                            \
                       ") != `" #exp1 "` (value: ", _v1, ")");                 \
    } while (0)
// Check if the given expressions are not equal.
#define UNITTEST_NE(exp0, exp1)                                                \
    do {                                                                       \
        auto _v0 = (exp0);                                                     \
        auto _v1 = (exp1);                                                     \
        if (_v0 != _v1) {                                                      \
            break;                                                             \
        }                                                                      \
        UNITTEST_FEXIT("`" #exp0 "` (value: ", _v0,                            \
                       ") == `" #exp1 "` (value: ", _v1, ")");                 \
    } while (0)

#endif // ARK_UNITTEST_UNITTEST_UTILS_H_

```

`ark/utils.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include <iomanip>
#include <iostream>
#include <sys/types.h>
#include <sys/wait.h>
#include <unistd.h>
#include <vector>

#include "include/ark.h"
#include "include/ark_utils.h"

// clang-format off
#include "vector_types.h"
#include "cutlass/half.h"
// clang-format on

using namespace std;

/// Convert cutlass::half_t to @ref ark::half_t
/// @param cuh cutlass::half_t
/// @return @ref ark::half_t
inline static const ark::half_t convert(const cutlass::half_t &cuh)
{
    ark::half_t ret;
    ret.storage = cuh.raw();
    return ret;
}

/// Numeric limits of @ref ark::half_t
template <> struct std::numeric_limits<ark::half_t>
{
    static ark::half_t max()
    {
        return convert(std::numeric_limits<cutlass::half_t>::max());
    }
    static ark::half_t min()
    {
        return convert(std::numeric_limits<cutlass::half_t>::min());
    }
    static ark::half_t epsilon()
    {
        return convert(std::numeric_limits<cutlass::half_t>::epsilon());
    }
};

ark::half_t operator+(ark::half_t const &lhs, ark::half_t const &rhs)
{
    return convert(cutlass::half_t::bitcast(lhs.storage) +
                   cutlass::half_t::bitcast(rhs.storage));
}

ark::half_t operator-(ark::half_t const &lhs, ark::half_t const &rhs)
{
    return convert(cutlass::half_t::bitcast(lhs.storage) -
                   cutlass::half_t::bitcast(rhs.storage));
}

ark::half_t operator*(ark::half_t const &lhs, ark::half_t const &rhs)
{
    return convert(cutlass::half_t::bitcast(lhs.storage) *
                   cutlass::half_t::bitcast(rhs.storage));
}

ark::half_t &operator+=(ark::half_t &lhs, ark::half_t const &rhs)
{
    cutlass::half_t v = cutlass::half_t::bitcast(lhs.storage) +
                        cutlass::half_t::bitcast(rhs.storage);
    lhs.storage = v.raw();
    return lhs;
}

ark::half_t &operator-=(ark::half_t &lhs, ark::half_t const &rhs)
{
    cutlass::half_t v = cutlass::half_t::bitcast(lhs.storage) -
                        cutlass::half_t::bitcast(rhs.storage);
    lhs.storage = v.raw();
    return lhs;
}

/// Return the absolute value of a @ref ark::half_t
/// @param val Input value
/// @return @ref Absolute value of `val`
ark::half_t abs(ark::half_t const &val)
{
    return convert(cutlass::abs(cutlass::half_t::bitcast(val.storage)));
}

namespace ark {

/// Construct a @ref half_t from a float
/// @param f Input value
half_t::half_t(float f)
{
    this->storage = cutlass::half_t(f).raw();
}

/// Convert a @ref half_t to a float
/// @return float
half_t::operator float() const
{
    return float(cutlass::half_t::bitcast(this->storage));
}

namespace utils {

/// Return a random @ref half_t array.
/// @param num Number of elements
/// @param max_val Maximum value
/// @return std::unique_ptr<half_t[]>
unique_ptr<half_t[]> rand_halfs(size_t num, float max_val)
{
    return rand_array<half_t>(num, max_val);
}

/// Return a random float array.
/// @param num Number of elements
/// @param max_val Maximum value
/// @return std::unique_ptr<float[]>
unique_ptr<float[]> rand_floats(size_t num, float max_val)
{
    return rand_array<float>(num, max_val);
}

/// Return an array of values starting from `begin` with difference `diff`.
/// @tparam T Type of the array
/// @param num Number of elements
/// @param begin First value
/// @param diff Difference between two values
/// @return std::unique_ptr<T[]>
template <typename T>
unique_ptr<T[]> range_array(size_t num, float begin, float diff)
{
    T *ret = new T[num];
    for (size_t i = 0; i < num; ++i) {
        ret[i] = T(begin);
        begin += diff;
    }
    return unique_ptr<T[]>(ret);
}

/// Return a @ref half_t range array.
/// @param num Number of elements
/// @param begin First value
/// @param diff Difference between two values
/// @return std::unique_ptr<half_t[]>
unique_ptr<half_t[]> range_halfs(size_t num, float begin, float diff)
{
    return range_array<half_t>(num, begin, diff);
}

/// Return a float range array.
/// @param num Number of elements
/// @param begin First value
/// @param diff Difference between two values
/// @return std::unique_ptr<float[]>
unique_ptr<float[]> range_floats(size_t num, float begin, float diff)
{
    return range_array<float>(num, begin, diff);
}

/// Calculate the error rate between two values.
/// @tparam T Type of the values
/// @param a First value
/// @param b Second value
/// @return The error rate
template <typename T> float error_rate(T a, T b)
{
    T diff = abs(a - b);
    if (diff < numeric_limits<T>::min()) {
        return 0;
    }
    diff -= numeric_limits<T>::epsilon();
    T half_eps = numeric_limits<T>::epsilon() * T(0.5);
    if (a > b) {
        a -= half_eps;
        b += half_eps;
    } else {
        a += half_eps;
        b -= half_eps;
    }
    return (float)diff / max(abs((float)a), abs((float)b));
}

/// Calculate the error rate between two @ref half_t values.
/// @param a First value
/// @param b Second value
/// @return The error rate
float error_rate(half_t a, half_t b)
{
    return error_rate<half_t>(a, b);
}

/// Calculate the error rate between two floats.
/// @param a First value
/// @param b Second value
/// @return The error rate
float error_rate(float a, float b)
{
    return error_rate<float>(a, b);
}

/// Return mean squared error and max error rate between two matrices.
template <typename T>
pair<float, float> cmp_matrix(T *ground_truth, T *res, unsigned int m,
                              unsigned int n, unsigned int bs, unsigned int lm,
                              unsigned int ln, bool print)
{
    // TODO: deprecate this function.

    if (lm == 0) {
        lm = m;
    }
    if (ln == 0) {
        ln = n;
    }
    size_t num = (size_t)lm * (size_t)ln;

    const float thres_err = 0.01;

    float l2_loss = 0;
    float max_err = 0;
    // int cnt_flip = 0;
    // float max_err_gv;
    // float max_err_rv;
    for (unsigned int bidx = 0; bidx < bs; ++bidx) {
        for (unsigned int nidx = 0; nidx < n; ++nidx) {
            for (unsigned int midx = 0; midx < m; ++midx) {
                unsigned int idx = midx + nidx * lm + bidx * lm * ln;
                T gv = ground_truth[idx];
                T rv = res[idx];
                float diff = (float)(gv - rv);
                l2_loss += diff * diff;
                float err = error_rate(gv, rv);
                // if ((err > thres_err) && (error_rate(gv, -rv) <
                // thres_err) &&
                // (((float)gv * (float)rv) < 0)) {
                //     cnt_flip++;
                //     cout << (float)gv << "," << (float)rv << endl;
                //     cout << hex << gv.storage << "," << rv.storage << dec <<
                //     endl;
                // }
                if (err > max_err) {
                    max_err = err;
                    // max_err_gv = (float)gv;
                    // max_err_rv = (float)rv;
                }
            }
        }
    }
    if (print) {
        unsigned int x = 0;
        unsigned int cc = 0;
        cout << setprecision(4);
        for (unsigned int bidx = 0; bidx < bs; ++bidx) {
            for (unsigned int nidx = 0; nidx < n; ++nidx) {
                for (unsigned int midx = 0; midx < m; ++midx) {
                    unsigned int idx = midx + nidx * lm + bidx * lm * ln;
                    T exp = ground_truth[idx];
                    T act = res[idx];
                    if (error_rate(exp, act) < thres_err) {
                        cout << (float)act << ',';
                    } else {
                        cout << "\033[0;31m" << (float)act << "\033[0m,"
                             << "\033[0;32m" << (float)exp << "\033[0m,";
                    }
                    if (++cc == m) {
                        cout << '[' << x << ']' << endl;
                        cc = 0;
                        x++;
                    }
                }
            }
        }
    }
    // cout << max_err_gv << endl;
    // cout << max_err_rv << endl;
    // cout << cnt_flip << endl;
    return {l2_loss / num, max_err};
}

//
pair<float, float> cmp_matrix(half_t *ground_truth, half_t *res, unsigned int m,
                              unsigned int n, unsigned int bs, unsigned int lm,
                              unsigned int ln, bool print)
{
    // TODO: deprecate this function.

    return cmp_matrix<half_t>(ground_truth, res, m, n, bs, lm, ln, print);
}

//
pair<float, float> cmp_matrix(float *ground_truth, float *res, unsigned int m,
                              unsigned int n, unsigned int bs, unsigned int lm,
                              unsigned int ln, bool print)
{
    // TODO: deprecate this function.

    return cmp_matrix<float>(ground_truth, res, m, n, bs, lm, ln, print);
}

//
template <typename T>
void print_matrix(T *val, unsigned int m, unsigned int n, unsigned int bs,
                  unsigned int lm, unsigned int ln)
{
    // TODO: deprecate this function.

    unsigned int x = 0;
    unsigned int cc = 0;
    cout << setprecision(4);
    for (unsigned int bidx = 0; bidx < bs; ++bidx) {
        for (unsigned int nidx = 0; nidx < n; ++nidx) {
            for (unsigned int midx = 0; midx < m; ++midx) {
                unsigned int idx = midx + nidx * lm + bidx * lm * ln;
                T v = val[idx];
                cout << (float)v << ',';
                if (++cc == m) {
                    cout << '[' << x << ']' << endl;
                    cc = 0;
                    x++;
                }
            }
        }
    }
}

void print_matrix(half_t *val, unsigned int m, unsigned int n, unsigned int bs,
                  unsigned int lm, unsigned int ln)
{
    // TODO: deprecate this function.

    print_matrix<half_t>(val, m, n, bs, lm, ln);
}

void print_matrix(float *val, unsigned int m, unsigned int n, unsigned int bs,
                  unsigned int lm, unsigned int ln)
{
    // TODO: deprecate this function.

    print_matrix<float>(val, m, n, bs, lm, ln);
}

/// Return mean squared error and max error rate between two tensors.
/// @tparam T data type of the tensors.
/// @param ground_truth ground truth data array.
/// @param res input data array to compare with the ground truth.
/// @param shape shape of the tensor.
/// @param print whether to print wrong values.
/// @return a pair of mean squared error and max error rate.
template <typename T>
std::pair<float, float> tensor_compare(T *ground_truth, T *res, Dims shape,
                                       bool print = false)
{
    DimType nelem = shape.size();
    int ndims = shape.ndims();
    float l2_loss = 0;
    float max_err = 0;
    for (DimType i = 0; i < nelem; ++i) {
        float diff = (float)(ground_truth[i] - res[i]);
        l2_loss += diff * diff;

        float err = error_rate(ground_truth[i], res[i]);
        if (err > 0.) {
            if (print) {
                Dims idx;
                for (int j = 0; j < ndims; ++j) {
                    DimType vol = 1;
                    for (int k = j + 1; k < ndims; ++k) {
                        vol *= shape[k];
                    }
                    idx[j] = (i / vol) % shape[j];
                }
                std::cout << idx << " expected " << ground_truth[i]
                          << ", actually " << res[i] << " (err: " << err << ")"
                          << std::endl;
            }
            if (err > max_err) {
                max_err = err;
            }
        }
    }
    return {l2_loss / nelem, max_err};
}

/// Return mean squared error and max error rate between two @ref half_t
/// tensors.
/// @param ground_truth ground truth data array.
/// @param res input data array to compare with the ground truth.
/// @param shape shape of the tensor.
/// @param print whether to print wrong values.
/// @return a pair of mean squared error and max error rate.
std::pair<float, float> tensor_compare(half_t *ground_truth, half_t *res,
                                       Dims shape, bool print = false)
{
    return tensor_compare<half_t>(ground_truth, res, shape, print);
}

/// Return mean squared error and max error rate between two float tensors.
/// @param ground_truth ground truth data array.
/// @param res input data array to compare with the ground truth.
/// @param shape shape of the tensor.
/// @param print whether to print wrong values.
/// @return a pair of mean squared error and max error rate.
std::pair<float, float> tensor_compare(float *ground_truth, float *res,
                                       Dims shape, bool print = false)
{
    return tensor_compare<float>(ground_truth, res, shape, print);
}

/// Spawn a process that runs `func`.
/// @param func function to run in the spawned process.
/// @return PID of the spawned process.
int proc_spawn(const function<int()> &func)
{
    pid_t pid = fork();
    if (pid < 0) {
        return -1;
    } else if (pid == 0) {
        int ret = func();
        std::exit(ret);
    }
    return (int)pid;
}

/// Wait for a spawned process with PID `pid`.
/// @param pid PID of the spawned process.
/// @return -1 on any unexpected failure, otherwise return the exit status.
int proc_wait(int pid)
{
    int status;
    if (waitpid(pid, &status, 0) == -1) {
        return -1;
    }
    if (WIFEXITED(status)) {
        return WEXITSTATUS(status);
    }
    return -1;
}

/// Wait for multiple child processes.
/// @param pids PIDs of the spawned processes.
/// @return 0 on success, -1 on any unexpected failure, otherwise the first seen
/// non-zero exit status.
int proc_wait(const vector<int> &pids)
{
    int ret = 0;
    for (auto &pid : pids) {
        int status;
        if (waitpid(pid, &status, 0) == -1) {
            return -1;
        }
        int r;
        if (WIFEXITED(status)) {
            r = WEXITSTATUS(status);
        } else if (WIFSIGNALED(status)) {
            r = -1;
        } else {
            r = -1;
        }
        if ((ret == 0) && (r != 0)) {
            ret = r;
        }
    }
    return ret;
}

} // namespace utils
} // namespace ark

```

`cmake/FindIBVerbs.cmake`:

```cmake
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

# Find the IB Verbs libraries
#
# The following variables are optionally searched for defaults
#  IBVERBS_ROOT_DIR: Base directory where all ibverbs components are found
#  IBVERBS_INCLUDE_DIR: Directory where ibverbs headers are found
#  IBVERBS_LIB_DIR: Directory where ibverbs libraries are found

# The following are set after configuration is done:
#  IBVERBS_FOUND
#  IBVERBS_INCLUDE_DIRS
#  IBVERBS_LIBRARIES

# An imported target ARK::ibverbs is created if the library is found.

find_path(IBVERBS_INCLUDE_DIRS
    NAMES infiniband/verbs.h
    HINTS
    ${IBVERBS_INCLUDE_DIR}
    ${IBVERBS_ROOT_DIR}
    ${IBVERBS_ROOT_DIR}/include
)

find_library(IBVERBS_LIBRARIES
    NAMES ibverbs
    HINTS
    ${IBVERBS_LIB_DIR}
    ${IBVERBS_ROOT_DIR}
    ${IBVERBS_ROOT_DIR}/lib
)

include(FindPackageHandleStandardArgs)
find_package_handle_standard_args(IBVerbs DEFAULT_MSG IBVERBS_INCLUDE_DIRS IBVERBS_LIBRARIES)
mark_as_advanced(IBVERBS_INCLUDE_DIR IBVERBS_LIBRARIES)

if(IBVERBS_FOUND)
    if(NOT TARGET ARK::ibverbs)
        add_library(ARK::ibverbs UNKNOWN IMPORTED)
    endif()
    set_target_properties(ARK::ibverbs PROPERTIES
        INTERFACE_INCLUDE_DIRECTORIES "${IBVERBS_INCLUDE_DIR}"
        IMPORTED_LINK_INTERFACE_LANGUAGES "C"
        IMPORTED_LOCATION "${IBVERBS_LIBRARIES}"
    )
endif()

```

`cmake/FindNUMA.cmake`:

```cmake
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

# Find the numa libraries
#
# The following variables are optionally searched for defaults
#  NUMA_ROOT_DIR: Base directory where all numa components are found
#  NUMA_INCLUDE_DIR: Directory where numa headers are found
#  NUMA_LIB_DIR: Directory where numa libraries are found

# The following are set after configuration is done:
#  NUMA_FOUND
#  NUMA_INCLUDE_DIRS
#  NUMA_LIBRARIES

# An imported target ARK::numa is created if the library is found.

find_path(NUMA_INCLUDE_DIRS
    NAMES numa.h
    HINTS
    ${NUMA_INCLUDE_DIR}
    ${NUMA_ROOT_DIR}
    ${NUMA_ROOT_DIR}/include
)

find_library(NUMA_LIBRARIES
    NAMES numa
    HINTS
    ${NUMA_LIB_DIR}
    ${NUMA_ROOT_DIR}
    ${NUMA_ROOT_DIR}/lib
)

include(FindPackageHandleStandardArgs)
find_package_handle_standard_args(NUMA DEFAULT_MSG NUMA_INCLUDE_DIRS NUMA_LIBRARIES)
mark_as_advanced(NUMA_INCLUDE_DIR NUMA_LIBRARIES)

if(NUMA_FOUND)
    if(NOT TARGET ARK::numa)
        add_library(ARK::numa UNKNOWN IMPORTED)
    endif()
    set_target_properties(ARK::numa PROPERTIES
        INTERFACE_INCLUDE_DIRECTORIES "${NUMA_INCLUDE_DIR}"
        IMPORTED_LINK_INTERFACE_LANGUAGES "C"
        IMPORTED_LOCATION "${NUMA_LIBRARIES}"
    )
endif()

```

`cmake/Utils.cmake`:

```cmake
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

# clang-format
find_program(CLANG_FORMAT clang-format)
if(CLANG_FORMAT)
    message(STATUS "Found clang-format: ${CLANG_FORMAT}")
    set(FIND_DIRS
        ${PROJECT_SOURCE_DIR}/ark
        ${PROJECT_SOURCE_DIR}/python
        ${PROJECT_SOURCE_DIR}/examples
    )
    add_custom_target(cpplint
        COMMAND ${CLANG_FORMAT} -style=file --dry-run `find ${FIND_DIRS} -type f -name *.h -o -name *.hpp -o -name *.c -o -name *.cc -o -name *.cpp -o -name *.cu`
    )
    add_custom_target(cpplint-autofix
        COMMAND ${CLANG_FORMAT} -style=file -i `find ${FIND_DIRS} -type f -name *.h -o -name *.hpp -o -name *.c -o -name *.cc -o -name *.cpp -o -name *.cu`
    )
else()
    message(STATUS "clang-format not found.")
endif()

# black
add_custom_target(pylint
    COMMAND python3.8 -m black --check --config ${PROJECT_SOURCE_DIR}/pyproject.toml ${PROJECT_SOURCE_DIR}
)
add_custom_target(pylint-autofix
    COMMAND python3.8 -m black --config ${PROJECT_SOURCE_DIR}/pyproject.toml ${PROJECT_SOURCE_DIR}
)

# Insert gpumem module
add_custom_target(gpumem
    COMMENT "Inserting gpumem module..."
    COMMAND insmod ${PROJECT_SOURCE_DIR}/third_party/gpudma/module/gpumem.ko
    COMMAND chmod 666 /dev/gpumem
)
add_dependencies(gpumem tp-gpudma)

```

`docker/base-cuda11.8.dockerfile`:

```dockerfile
FROM nvidia/cuda:11.8.0-devel-ubuntu20.04

LABEL maintainer="ARK"
LABEL org.opencontainers.image.source https://github.com/microsoft/ark

ENV DEBIAN_FRONTEND=noninteractive

RUN rm -rf /opt/nvidia

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        build-essential \
        ca-certificates \
        curl \
        git \
        libcap2 \
        libnuma-dev \
        openssh-client \
        openssh-server \
        python3-dev \
        python3-pip \
        python3-setuptools \
        python3-wheel \
        sudo \
        wget \
        && \
    apt-get autoremove && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* /tmp/*

# Install OFED
ENV OFED_VERSION=5.2-2.2.3.0
RUN cd /tmp && \
    wget -q https://content.mellanox.com/ofed/MLNX_OFED-${OFED_VERSION}/MLNX_OFED_LINUX-${OFED_VERSION}-ubuntu20.04-x86_64.tgz && \
    tar xzf MLNX_OFED_LINUX-${OFED_VERSION}-ubuntu20.04-x86_64.tgz && \
    MLNX_OFED_LINUX-${OFED_VERSION}-ubuntu20.04-x86_64/mlnxofedinstall --user-space-only --without-fw-update --force --all && \
    rm -rf /tmp/MLNX_OFED_LINUX-${OFED_VERSION}*

# Install OpenMPI
ENV OPENMPI_VERSION=4.1.5
RUN cd /tmp && \
    export ompi_v_parsed="$(echo ${OPENMPI_VERSION} | sed -E 's/^([0-9]+)\.([0-9]+)\..*/\1.\2/')" && \
    wget -q https://download.open-mpi.org/release/open-mpi/v${ompi_v_parsed}/openmpi-${OPENMPI_VERSION}.tar.gz && \
    tar xzf openmpi-${OPENMPI_VERSION}.tar.gz && \
    cd openmpi-${OPENMPI_VERSION} && \
    ./configure --prefix=/usr/local/mpi && \
    make -j && \
    make install && \
    cd .. && \
    rm -rf /tmp/openmpi-${OPENMPI_VERSION}*

ENV PATH="/usr/local/mpi/bin:${PATH}" \
    LD_LIBRARY_PATH="/usr/local/mpi/lib:/usr/local/cuda-11.8/lib64:${LD_LIBRARY_PATH}"

RUN echo PATH="${PATH}" > /etc/environment && \
    echo LD_LIBRARY_PATH="${LD_LIBRARY_PATH}" >> /etc/environment

```

`docker/base-cuda12.1.dockerfile`:

```dockerfile
FROM nvidia/cuda:12.1.1-devel-ubuntu20.04

LABEL maintainer="ARK"
LABEL org.opencontainers.image.source https://github.com/microsoft/ark

ENV DEBIAN_FRONTEND=noninteractive

RUN rm -rf /opt/nvidia

RUN apt-get update && \
    apt-get install -y --no-install-recommends \
        build-essential \
        ca-certificates \
        curl \
        git \
        libcap2 \
        libnuma-dev \
        openssh-client \
        openssh-server \
        python3-dev \
        python3-pip \
        python3-setuptools \
        python3-wheel \
        sudo \
        wget \
        && \
    apt-get autoremove && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* /tmp/*

# Install OFED
ENV OFED_VERSION=5.2-2.2.3.0
RUN cd /tmp && \
    wget -q https://content.mellanox.com/ofed/MLNX_OFED-${OFED_VERSION}/MLNX_OFED_LINUX-${OFED_VERSION}-ubuntu20.04-x86_64.tgz && \
    tar xzf MLNX_OFED_LINUX-${OFED_VERSION}-ubuntu20.04-x86_64.tgz && \
    MLNX_OFED_LINUX-${OFED_VERSION}-ubuntu20.04-x86_64/mlnxofedinstall --user-space-only --without-fw-update --force --all && \
    rm -rf /tmp/MLNX_OFED_LINUX-${OFED_VERSION}*

# Install OpenMPI
ENV OPENMPI_VERSION=4.1.5
RUN cd /tmp && \
    export ompi_v_parsed="$(echo ${OPENMPI_VERSION} | sed -E 's/^([0-9]+)\.([0-9]+)\..*/\1.\2/')" && \
    wget -q https://download.open-mpi.org/release/open-mpi/v${ompi_v_parsed}/openmpi-${OPENMPI_VERSION}.tar.gz && \
    tar xzf openmpi-${OPENMPI_VERSION}.tar.gz && \
    cd openmpi-${OPENMPI_VERSION} && \
    ./configure --prefix=/usr/local/mpi && \
    make -j && \
    make install && \
    cd .. && \
    rm -rf /tmp/openmpi-${OPENMPI_VERSION}*

ENV PATH="/usr/local/mpi/bin:${PATH}" \
    LD_LIBRARY_PATH="/usr/local/mpi/lib:/usr/local/cuda-12.1/compat:/usr/local/cuda-12.1/lib64:${LD_LIBRARY_PATH}"

RUN echo PATH="${PATH}" > /etc/environment && \
    echo LD_LIBRARY_PATH="${LD_LIBRARY_PATH}" >> /etc/environment

```

`docs/doxygen/Doxyfile`:

```
# Doxyfile 1.8.17

# This file describes the settings to be used by the documentation system
# doxygen (www.doxygen.org) for a project.
#
# All text after a double hash (##) is considered a comment and is placed in
# front of the TAG it is preceding.
#
# All text after a single hash (#) is considered a comment and will be ignored.
# The format is:
# TAG = value [value, ...]
# For lists, items can also be appended using:
# TAG += value [value, ...]
# Values that contain spaces should be placed between quotes (\" \").

#---------------------------------------------------------------------------
# Project related configuration options
#---------------------------------------------------------------------------

# This tag specifies the encoding used for all characters in the configuration
# file that follow. The default is UTF-8 which is also the encoding used for all
# text before the first occurrence of this tag. Doxygen uses libiconv (or the
# iconv built into libc) for the transcoding. See
# https://www.gnu.org/software/libiconv/ for the list of possible encodings.
# The default value is: UTF-8.

DOXYFILE_ENCODING      = UTF-8

# The PROJECT_NAME tag is a single word (or a sequence of words surrounded by
# double-quotes, unless you are using Doxywizard) that should identify the
# project for which the documentation is generated. This name is used in the
# title of most generated pages and in a few other places.
# The default value is: My Project.

PROJECT_NAME           = "ARK"

# The PROJECT_NUMBER tag can be used to enter a project or revision number. This
# could be handy for archiving the generated documentation or if some version
# control system is used.

PROJECT_NUMBER         =

# Using the PROJECT_BRIEF tag one can provide an optional one line description
# for a project that appears at the top of each page and should give viewer a
# quick idea about the purpose of the project. Keep the description short.

PROJECT_BRIEF          = "A GPU-driven system framework for scalable AI applications"

# With the PROJECT_LOGO tag one can specify a logo or an icon that is included
# in the documentation. The maximum height of the logo should not exceed 55
# pixels and the maximum width should not exceed 200 pixels. Doxygen will copy
# the logo to the output directory.

PROJECT_LOGO           =

# The OUTPUT_DIRECTORY tag is used to specify the (relative or absolute) path
# into which the generated documentation will be written. If a relative path is
# entered, it will be relative to the location where doxygen was started. If
# left blank the current directory will be used.

OUTPUT_DIRECTORY       = doxygen

# If the CREATE_SUBDIRS tag is set to YES then doxygen will create 4096 sub-
# directories (in 2 levels) under the output directory of each output format and
# will distribute the generated files over these directories. Enabling this
# option can be useful when feeding doxygen a huge amount of source files, where
# putting all generated files in the same directory would otherwise causes
# performance problems for the file system.
# The default value is: NO.

CREATE_SUBDIRS         = NO

# If the ALLOW_UNICODE_NAMES tag is set to YES, doxygen will allow non-ASCII
# characters to appear in the names of generated files. If set to NO, non-ASCII
# characters will be escaped, for example _xE3_x81_x84 will be used for Unicode
# U+3044.
# The default value is: NO.

ALLOW_UNICODE_NAMES    = NO

# The OUTPUT_LANGUAGE tag is used to specify the language in which all
# documentation generated by doxygen is written. Doxygen will use this
# information to generate all constant output in the proper language.
# Possible values are: Afrikaans, Arabic, Armenian, Brazilian, Catalan, Chinese,
# Chinese-Traditional, Croatian, Czech, Danish, Dutch, English (United States),
# Esperanto, Farsi (Persian), Finnish, French, German, Greek, Hungarian,
# Indonesian, Italian, Japanese, Japanese-en (Japanese with English messages),
# Korean, Korean-en (Korean with English messages), Latvian, Lithuanian,
# Macedonian, Norwegian, Persian (Farsi), Polish, Portuguese, Romanian, Russian,
# Serbian, Serbian-Cyrillic, Slovak, Slovene, Spanish, Swedish, Turkish,
# Ukrainian and Vietnamese.
# The default value is: English.

OUTPUT_LANGUAGE        = English

# The OUTPUT_TEXT_DIRECTION tag is used to specify the direction in which all
# documentation generated by doxygen is written. Doxygen will use this
# information to generate all generated output in the proper direction.
# Possible values are: None, LTR, RTL and Context.
# The default value is: None.

OUTPUT_TEXT_DIRECTION  = None

# If the BRIEF_MEMBER_DESC tag is set to YES, doxygen will include brief member
# descriptions after the members that are listed in the file and class
# documentation (similar to Javadoc). Set to NO to disable this.
# The default value is: YES.

BRIEF_MEMBER_DESC      = YES

# If the REPEAT_BRIEF tag is set to YES, doxygen will prepend the brief
# description of a member or function before the detailed description
#
# Note: If both HIDE_UNDOC_MEMBERS and BRIEF_MEMBER_DESC are set to NO, the
# brief descriptions will be completely suppressed.
# The default value is: YES.

REPEAT_BRIEF           = YES

# This tag implements a quasi-intelligent brief description abbreviator that is
# used to form the text in various listings. Each string in this list, if found
# as the leading text of the brief description, will be stripped from the text
# and the result, after processing the whole list, is used as the annotated
# text. Otherwise, the brief description is used as-is. If left blank, the
# following values are used ($name is automatically replaced with the name of
# the entity):The $name class, The $name widget, The $name file, is, provides,
# specifies, contains, represents, a, an and the.

ABBREVIATE_BRIEF       = "The $name class" \
                         "The $name widget" \
                         "The $name file" \
                         is \
                         provides \
                         specifies \
                         contains \
                         represents \
                         a \
                         an \
                         the

# If the ALWAYS_DETAILED_SEC and REPEAT_BRIEF tags are both set to YES then
# doxygen will generate a detailed section even if there is only a brief
# description.
# The default value is: NO.

ALWAYS_DETAILED_SEC    = NO

# If the INLINE_INHERITED_MEMB tag is set to YES, doxygen will show all
# inherited members of a class in the documentation of that class as if those
# members were ordinary class members. Constructors, destructors and assignment
# operators of the base classes will not be shown.
# The default value is: NO.

INLINE_INHERITED_MEMB  = NO

# If the FULL_PATH_NAMES tag is set to YES, doxygen will prepend the full path
# before files name in the file list and in the header files. If set to NO the
# shortest path that makes the file name unique will be used
# The default value is: YES.

FULL_PATH_NAMES        = NO

# The STRIP_FROM_PATH tag can be used to strip a user-defined part of the path.
# Stripping is only done if one of the specified strings matches the left-hand
# part of the path. The tag can be used to show relative paths in the file list.
# If left blank the directory from which doxygen is run is used as the path to
# strip.
#
# Note that you can specify absolute paths here, but also relative paths, which
# will be relative from the directory where doxygen is started.
# This tag requires that the tag FULL_PATH_NAMES is set to YES.

STRIP_FROM_PATH        =

# The STRIP_FROM_INC_PATH tag can be used to strip a user-defined part of the
# path mentioned in the documentation of a class, which tells the reader which
# header file to include in order to use a class. If left blank only the name of
# the header file containing the class definition is used. Otherwise one should
# specify the list of include paths that are normally passed to the compiler
# using the -I flag.

STRIP_FROM_INC_PATH    =

# If the SHORT_NAMES tag is set to YES, doxygen will generate much shorter (but
# less readable) file names. This can be useful is your file systems doesn't
# support long names like on DOS, Mac, or CD-ROM.
# The default value is: NO.

SHORT_NAMES            = NO

# If the JAVADOC_AUTOBRIEF tag is set to YES then doxygen will interpret the
# first line (until the first dot) of a Javadoc-style comment as the brief
# description. If set to NO, the Javadoc-style will behave just like regular Qt-
# style comments (thus requiring an explicit @brief command for a brief
# description.)
# The default value is: NO.

JAVADOC_AUTOBRIEF      = NO

# If the JAVADOC_BANNER tag is set to YES then doxygen will interpret a line
# such as
# /***************
# as being the beginning of a Javadoc-style comment "banner". If set to NO, the
# Javadoc-style will behave just like regular comments and it will not be
# interpreted by doxygen.
# The default value is: NO.

JAVADOC_BANNER         = NO

# If the QT_AUTOBRIEF tag is set to YES then doxygen will interpret the first
# line (until the first dot) of a Qt-style comment as the brief description. If
# set to NO, the Qt-style will behave just like regular Qt-style comments (thus
# requiring an explicit \brief command for a brief description.)
# The default value is: NO.

QT_AUTOBRIEF           = NO

# The MULTILINE_CPP_IS_BRIEF tag can be set to YES to make doxygen treat a
# multi-line C++ special comment block (i.e. a block of //! or /// comments) as
# a brief description. This used to be the default behavior. The new default is
# to treat a multi-line C++ comment block as a detailed description. Set this
# tag to YES if you prefer the old behavior instead.
#
# Note that setting this tag to YES also means that rational rose comments are
# not recognized any more.
# The default value is: NO.

MULTILINE_CPP_IS_BRIEF = NO

# If the INHERIT_DOCS tag is set to YES then an undocumented member inherits the
# documentation from any documented member that it re-implements.
# The default value is: YES.

INHERIT_DOCS           = YES

# If the SEPARATE_MEMBER_PAGES tag is set to YES then doxygen will produce a new
# page for each member. If set to NO, the documentation of a member will be part
# of the file/class/namespace that contains it.
# The default value is: NO.

SEPARATE_MEMBER_PAGES  = NO

# The TAB_SIZE tag can be used to set the number of spaces in a tab. Doxygen
# uses this value to replace tabs by spaces in code fragments.
# Minimum value: 1, maximum value: 16, default value: 4.

TAB_SIZE               = 4

# This tag can be used to specify a number of aliases that act as commands in
# the documentation. An alias has the form:
# name=value
# For example adding
# "sideeffect=@par Side Effects:\n"
# will allow you to put the command \sideeffect (or @sideeffect) in the
# documentation, which will result in a user-defined paragraph with heading
# "Side Effects:". You can put \n's in the value part of an alias to insert
# newlines (in the resulting output). You can put ^^ in the value part of an
# alias to insert a newline as if a physical newline was in the original file.
# When you need a literal { or } or , in the value part of an alias you have to
# escape them by means of a backslash (\), this can lead to conflicts with the
# commands \{ and \} for these it is advised to use the version @{ and @} or use
# a double escape (\\{ and \\})

ALIASES                =

# This tag can be used to specify a number of word-keyword mappings (TCL only).
# A mapping has the form "name=value". For example adding "class=itcl::class"
# will allow you to use the command class in the itcl::class meaning.

TCL_SUBST              =

# Set the OPTIMIZE_OUTPUT_FOR_C tag to YES if your project consists of C sources
# only. Doxygen will then generate output that is more tailored for C. For
# instance, some of the names that are used will be different. The list of all
# members will be omitted, etc.
# The default value is: NO.

OPTIMIZE_OUTPUT_FOR_C  = NO

# Set the OPTIMIZE_OUTPUT_JAVA tag to YES if your project consists of Java or
# Python sources only. Doxygen will then generate output that is more tailored
# for that language. For instance, namespaces will be presented as packages,
# qualified scopes will look different, etc.
# The default value is: NO.

OPTIMIZE_OUTPUT_JAVA   = NO

# Set the OPTIMIZE_FOR_FORTRAN tag to YES if your project consists of Fortran
# sources. Doxygen will then generate output that is tailored for Fortran.
# The default value is: NO.

OPTIMIZE_FOR_FORTRAN   = NO

# Set the OPTIMIZE_OUTPUT_VHDL tag to YES if your project consists of VHDL
# sources. Doxygen will then generate output that is tailored for VHDL.
# The default value is: NO.

OPTIMIZE_OUTPUT_VHDL   = NO

# Set the OPTIMIZE_OUTPUT_SLICE tag to YES if your project consists of Slice
# sources only. Doxygen will then generate output that is more tailored for that
# language. For instance, namespaces will be presented as modules, types will be
# separated into more groups, etc.
# The default value is: NO.

OPTIMIZE_OUTPUT_SLICE  = NO

# Doxygen selects the parser to use depending on the extension of the files it
# parses. With this tag you can assign which parser to use for a given
# extension. Doxygen has a built-in mapping, but you can override or extend it
# using this tag. The format is ext=language, where ext is a file extension, and
# language is one of the parsers supported by doxygen: IDL, Java, JavaScript,
# Csharp (C#), C, C++, D, PHP, md (Markdown), Objective-C, Python, Slice,
# Fortran (fixed format Fortran: FortranFixed, free formatted Fortran:
# FortranFree, unknown formatted Fortran: Fortran. In the later case the parser
# tries to guess whether the code is fixed or free formatted code, this is the
# default for Fortran type files), VHDL, tcl. For instance to make doxygen treat
# .inc files as Fortran files (default is PHP), and .f files as C (default is
# Fortran), use: inc=Fortran f=C.
#
# Note: For files without extension you can use no_extension as a placeholder.
#
# Note that for custom extensions you also need to set FILE_PATTERNS otherwise
# the files are not read by doxygen.

EXTENSION_MAPPING      = cu=C++

# If the MARKDOWN_SUPPORT tag is enabled then doxygen pre-processes all comments
# according to the Markdown format, which allows for more readable
# documentation. See https://daringfireball.net/projects/markdown/ for details.
# The output of markdown processing is further processed by doxygen, so you can
# mix doxygen, HTML, and XML commands with Markdown formatting. Disable only in
# case of backward compatibilities issues.
# The default value is: YES.

MARKDOWN_SUPPORT       = YES

# When the TOC_INCLUDE_HEADINGS tag is set to a non-zero value, all headings up
# to that level are automatically included in the table of contents, even if
# they do not have an id attribute.
# Note: This feature currently applies only to Markdown headings.
# Minimum value: 0, maximum value: 99, default value: 5.
# This tag requires that the tag MARKDOWN_SUPPORT is set to YES.

TOC_INCLUDE_HEADINGS   = 5

# When enabled doxygen tries to link words that correspond to documented
# classes, or namespaces to their corresponding documentation. Such a link can
# be prevented in individual cases by putting a % sign in front of the word or
# globally by setting AUTOLINK_SUPPORT to NO.
# The default value is: YES.

AUTOLINK_SUPPORT       = YES

# If you use STL classes (i.e. std::string, std::vector, etc.) but do not want
# to include (a tag file for) the STL sources as input, then you should set this
# tag to YES in order to let doxygen match functions declarations and
# definitions whose arguments contain STL classes (e.g. func(std::string);
# versus func(std::string) {}). This also make the inheritance and collaboration
# diagrams that involve STL classes more complete and accurate.
# The default value is: NO.

BUILTIN_STL_SUPPORT    = NO

# If you use Microsoft's C++/CLI language, you should set this option to YES to
# enable parsing support.
# The default value is: NO.

CPP_CLI_SUPPORT        = NO

# Set the SIP_SUPPORT tag to YES if your project consists of sip (see:
# https://www.riverbankcomputing.com/software/sip/intro) sources only. Doxygen
# will parse them like normal C++ but will assume all classes use public instead
# of private inheritance when no explicit protection keyword is present.
# The default value is: NO.

SIP_SUPPORT            = NO

# For Microsoft's IDL there are propget and propput attributes to indicate
# getter and setter methods for a property. Setting this option to YES will make
# doxygen to replace the get and set methods by a property in the documentation.
# This will only work if the methods are indeed getting or setting a simple
# type. If this is not the case, or you want to show the methods anyway, you
# should set this option to NO.
# The default value is: YES.

IDL_PROPERTY_SUPPORT   = YES

# If member grouping is used in the documentation and the DISTRIBUTE_GROUP_DOC
# tag is set to YES then doxygen will reuse the documentation of the first
# member in the group (if any) for the other members of the group. By default
# all members of a group must be documented explicitly.
# The default value is: NO.

DISTRIBUTE_GROUP_DOC   = NO

# If one adds a struct or class to a group and this option is enabled, then also
# any nested class or struct is added to the same group. By default this option
# is disabled and one has to add nested compounds explicitly via \ingroup.
# The default value is: NO.

GROUP_NESTED_COMPOUNDS = NO

# Set the SUBGROUPING tag to YES to allow class member groups of the same type
# (for instance a group of public functions) to be put as a subgroup of that
# type (e.g. under the Public Functions section). Set it to NO to prevent
# subgrouping. Alternatively, this can be done per class using the
# \nosubgrouping command.
# The default value is: YES.

SUBGROUPING            = YES

# When the INLINE_GROUPED_CLASSES tag is set to YES, classes, structs and unions
# are shown inside the group in which they are included (e.g. using \ingroup)
# instead of on a separate page (for HTML and Man pages) or section (for LaTeX
# and RTF).
#
# Note that this feature does not work in combination with
# SEPARATE_MEMBER_PAGES.
# The default value is: NO.

INLINE_GROUPED_CLASSES = NO

# When the INLINE_SIMPLE_STRUCTS tag is set to YES, structs, classes, and unions
# with only public data fields or simple typedef fields will be shown inline in
# the documentation of the scope in which they are defined (i.e. file,
# namespace, or group documentation), provided this scope is documented. If set
# to NO, structs, classes, and unions are shown on a separate page (for HTML and
# Man pages) or section (for LaTeX and RTF).
# The default value is: NO.

INLINE_SIMPLE_STRUCTS  = NO

# When TYPEDEF_HIDES_STRUCT tag is enabled, a typedef of a struct, union, or
# enum is documented as struct, union, or enum with the name of the typedef. So
# typedef struct TypeS {} TypeT, will appear in the documentation as a struct
# with name TypeT. When disabled the typedef will appear as a member of a file,
# namespace, or class. And the struct will be named TypeS. This can typically be
# useful for C code in case the coding convention dictates that all compound
# types are typedef'ed and only the typedef is referenced, never the tag name.
# The default value is: NO.

TYPEDEF_HIDES_STRUCT   = NO

# The size of the symbol lookup cache can be set using LOOKUP_CACHE_SIZE. This
# cache is used to resolve symbols given their name and scope. Since this can be
# an expensive process and often the same symbol appears multiple times in the
# code, doxygen keeps a cache of pre-resolved symbols. If the cache is too small
# doxygen will become slower. If the cache is too large, memory is wasted. The
# cache size is given by this formula: 2^(16+LOOKUP_CACHE_SIZE). The valid range
# is 0..9, the default is 0, corresponding to a cache size of 2^16=65536
# symbols. At the end of a run doxygen will report the cache usage and suggest
# the optimal cache size from a speed point of view.
# Minimum value: 0, maximum value: 9, default value: 0.

LOOKUP_CACHE_SIZE      = 0

#---------------------------------------------------------------------------
# Build related configuration options
#---------------------------------------------------------------------------

# If the EXTRACT_ALL tag is set to YES, doxygen will assume all entities in
# documentation are documented, even if no documentation was available. Private
# class members and static file members will be hidden unless the
# EXTRACT_PRIVATE respectively EXTRACT_STATIC tags are set to YES.
# Note: This will also disable the warnings about undocumented members that are
# normally produced when WARNINGS is set to YES.
# The default value is: NO.

EXTRACT_ALL            = YES

# If the EXTRACT_PRIVATE tag is set to YES, all private members of a class will
# be included in the documentation.
# The default value is: NO.

EXTRACT_PRIVATE        = NO

# If the EXTRACT_PRIV_VIRTUAL tag is set to YES, documented private virtual
# methods of a class will be included in the documentation.
# The default value is: NO.

EXTRACT_PRIV_VIRTUAL   = NO

# If the EXTRACT_PACKAGE tag is set to YES, all members with package or internal
# scope will be included in the documentation.
# The default value is: NO.

EXTRACT_PACKAGE        = NO

# If the EXTRACT_STATIC tag is set to YES, all static members of a file will be
# included in the documentation.
# The default value is: NO.

EXTRACT_STATIC         = NO

# If the EXTRACT_LOCAL_CLASSES tag is set to YES, classes (and structs) defined
# locally in source files will be included in the documentation. If set to NO,
# only classes defined in header files are included. Does not have any effect
# for Java sources.
# The default value is: YES.

EXTRACT_LOCAL_CLASSES  = YES

# This flag is only useful for Objective-C code. If set to YES, local methods,
# which are defined in the implementation section but not in the interface are
# included in the documentation. If set to NO, only methods in the interface are
# included.
# The default value is: NO.

EXTRACT_LOCAL_METHODS  = NO

# If this flag is set to YES, the members of anonymous namespaces will be
# extracted and appear in the documentation as a namespace called
# 'anonymous_namespace{file}', where file will be replaced with the base name of
# the file that contains the anonymous namespace. By default anonymous namespace
# are hidden.
# The default value is: NO.

EXTRACT_ANON_NSPACES   = NO

# If the HIDE_UNDOC_MEMBERS tag is set to YES, doxygen will hide all
# undocumented members inside documented classes or files. If set to NO these
# members will be included in the various overviews, but no documentation
# section is generated. This option has no effect if EXTRACT_ALL is enabled.
# The default value is: NO.

HIDE_UNDOC_MEMBERS     = NO

# If the HIDE_UNDOC_CLASSES tag is set to YES, doxygen will hide all
# undocumented classes that are normally visible in the class hierarchy. If set
# to NO, these classes will be included in the various overviews. This option
# has no effect if EXTRACT_ALL is enabled.
# The default value is: NO.

HIDE_UNDOC_CLASSES     = NO

# If the HIDE_FRIEND_COMPOUNDS tag is set to YES, doxygen will hide all friend
# declarations. If set to NO, these declarations will be included in the
# documentation.
# The default value is: NO.

HIDE_FRIEND_COMPOUNDS  = NO

# If the HIDE_IN_BODY_DOCS tag is set to YES, doxygen will hide any
# documentation blocks found inside the body of a function. If set to NO, these
# blocks will be appended to the function's detailed documentation block.
# The default value is: NO.

HIDE_IN_BODY_DOCS      = NO

# The INTERNAL_DOCS tag determines if documentation that is typed after a
# \internal command is included. If the tag is set to NO then the documentation
# will be excluded. Set it to YES to include the internal documentation.
# The default value is: NO.

INTERNAL_DOCS          = NO

# If the CASE_SENSE_NAMES tag is set to NO then doxygen will only generate file
# names in lower-case letters. If set to YES, upper-case letters are also
# allowed. This is useful if you have classes or files whose names only differ
# in case and if your file system supports case sensitive file names. Windows
# (including Cygwin) ands Mac users are advised to set this option to NO.
# The default value is: system dependent.

CASE_SENSE_NAMES       = YES

# If the HIDE_SCOPE_NAMES tag is set to NO then doxygen will show members with
# their full class and namespace scopes in the documentation. If set to YES, the
# scope will be hidden.
# The default value is: NO.

HIDE_SCOPE_NAMES       = NO

# If the HIDE_COMPOUND_REFERENCE tag is set to NO (default) then doxygen will
# append additional text to a page's title, such as Class Reference. If set to
# YES the compound reference will be hidden.
# The default value is: NO.

HIDE_COMPOUND_REFERENCE= NO

# If the SHOW_INCLUDE_FILES tag is set to YES then doxygen will put a list of
# the files that are included by a file in the documentation of that file.
# The default value is: YES.

SHOW_INCLUDE_FILES     = YES

# If the SHOW_GROUPED_MEMB_INC tag is set to YES then Doxygen will add for each
# grouped member an include statement to the documentation, telling the reader
# which file to include in order to use the member.
# The default value is: NO.

SHOW_GROUPED_MEMB_INC  = NO

# If the FORCE_LOCAL_INCLUDES tag is set to YES then doxygen will list include
# files with double quotes in the documentation rather than with sharp brackets.
# The default value is: NO.

FORCE_LOCAL_INCLUDES   = NO

# If the INLINE_INFO tag is set to YES then a tag [inline] is inserted in the
# documentation for inline members.
# The default value is: YES.

INLINE_INFO            = YES

# If the SORT_MEMBER_DOCS tag is set to YES then doxygen will sort the
# (detailed) documentation of file and class members alphabetically by member
# name. If set to NO, the members will appear in declaration order.
# The default value is: YES.

SORT_MEMBER_DOCS       = YES

# If the SORT_BRIEF_DOCS tag is set to YES then doxygen will sort the brief
# descriptions of file, namespace and class members alphabetically by member
# name. If set to NO, the members will appear in declaration order. Note that
# this will also influence the order of the classes in the class list.
# The default value is: NO.

SORT_BRIEF_DOCS        = NO

# If the SORT_MEMBERS_CTORS_1ST tag is set to YES then doxygen will sort the
# (brief and detailed) documentation of class members so that constructors and
# destructors are listed first. If set to NO the constructors will appear in the
# respective orders defined by SORT_BRIEF_DOCS and SORT_MEMBER_DOCS.
# Note: If SORT_BRIEF_DOCS is set to NO this option is ignored for sorting brief
# member documentation.
# Note: If SORT_MEMBER_DOCS is set to NO this option is ignored for sorting
# detailed member documentation.
# The default value is: NO.

SORT_MEMBERS_CTORS_1ST = NO

# If the SORT_GROUP_NAMES tag is set to YES then doxygen will sort the hierarchy
# of group names into alphabetical order. If set to NO the group names will
# appear in their defined order.
# The default value is: NO.

SORT_GROUP_NAMES       = NO

# If the SORT_BY_SCOPE_NAME tag is set to YES, the class list will be sorted by
# fully-qualified names, including namespaces. If set to NO, the class list will
# be sorted only by class name, not including the namespace part.
# Note: This option is not very useful if HIDE_SCOPE_NAMES is set to YES.
# Note: This option applies only to the class list, not to the alphabetical
# list.
# The default value is: NO.

SORT_BY_SCOPE_NAME     = NO

# If the STRICT_PROTO_MATCHING option is enabled and doxygen fails to do proper
# type resolution of all parameters of a function it will reject a match between
# the prototype and the implementation of a member function even if there is
# only one candidate or it is obvious which candidate to choose by doing a
# simple string match. By disabling STRICT_PROTO_MATCHING doxygen will still
# accept a match between prototype and implementation in such cases.
# The default value is: NO.

STRICT_PROTO_MATCHING  = NO

# The GENERATE_TODOLIST tag can be used to enable (YES) or disable (NO) the todo
# list. This list is created by putting \todo commands in the documentation.
# The default value is: YES.

GENERATE_TODOLIST      = YES

# The GENERATE_TESTLIST tag can be used to enable (YES) or disable (NO) the test
# list. This list is created by putting \test commands in the documentation.
# The default value is: YES.

GENERATE_TESTLIST      = YES

# The GENERATE_BUGLIST tag can be used to enable (YES) or disable (NO) the bug
# list. This list is created by putting \bug commands in the documentation.
# The default value is: YES.

GENERATE_BUGLIST       = YES

# The GENERATE_DEPRECATEDLIST tag can be used to enable (YES) or disable (NO)
# the deprecated list. This list is created by putting \deprecated commands in
# the documentation.
# The default value is: YES.

GENERATE_DEPRECATEDLIST= YES

# The ENABLED_SECTIONS tag can be used to enable conditional documentation
# sections, marked by \if <section_label> ... \endif and \cond <section_label>
# ... \endcond blocks.

ENABLED_SECTIONS       =

# The MAX_INITIALIZER_LINES tag determines the maximum number of lines that the
# initial value of a variable or macro / define can have for it to appear in the
# documentation. If the initializer consists of more lines than specified here
# it will be hidden. Use a value of 0 to hide initializers completely. The
# appearance of the value of individual variables and macros / defines can be
# controlled using \showinitializer or \hideinitializer command in the
# documentation regardless of this setting.
# Minimum value: 0, maximum value: 10000, default value: 30.

MAX_INITIALIZER_LINES  = 30

# Set the SHOW_USED_FILES tag to NO to disable the list of files generated at
# the bottom of the documentation of classes and structs. If set to YES, the
# list will mention the files that were used to generate the documentation.
# The default value is: YES.

SHOW_USED_FILES        = YES

# Set the SHOW_FILES tag to NO to disable the generation of the Files page. This
# will remove the Files entry from the Quick Index and from the Folder Tree View
# (if specified).
# The default value is: YES.

SHOW_FILES             = YES

# Set the SHOW_NAMESPACES tag to NO to disable the generation of the Namespaces
# page. This will remove the Namespaces entry from the Quick Index and from the
# Folder Tree View (if specified).
# The default value is: YES.

SHOW_NAMESPACES        = YES

# The FILE_VERSION_FILTER tag can be used to specify a program or script that
# doxygen should invoke to get the current version for each file (typically from
# the version control system). Doxygen will invoke the program by executing (via
# popen()) the command command input-file, where command is the value of the
# FILE_VERSION_FILTER tag, and input-file is the name of an input file provided
# by doxygen. Whatever the program writes to standard output is used as the file
# version. For an example see the documentation.

FILE_VERSION_FILTER    =

# The LAYOUT_FILE tag can be used to specify a layout file which will be parsed
# by doxygen. The layout file controls the global structure of the generated
# output files in an output format independent way. To create the layout file
# that represents doxygen's defaults, run doxygen with the -l option. You can
# optionally specify a file name after the option, if omitted DoxygenLayout.xml
# will be used as the name of the layout file.
#
# Note that if you run doxygen from a directory containing a file called
# DoxygenLayout.xml, doxygen will parse it automatically even if the LAYOUT_FILE
# tag is left empty.

LAYOUT_FILE            =

# The CITE_BIB_FILES tag can be used to specify one or more bib files containing
# the reference definitions. This must be a list of .bib files. The .bib
# extension is automatically appended if omitted. This requires the bibtex tool
# to be installed. See also https://en.wikipedia.org/wiki/BibTeX for more info.
# For LaTeX the style of the bibliography can be controlled using
# LATEX_BIB_STYLE. To use this feature you need bibtex and perl available in the
# search path. See also \cite for info how to create references.

CITE_BIB_FILES         =

#---------------------------------------------------------------------------
# Configuration options related to warning and progress messages
#---------------------------------------------------------------------------

# The QUIET tag can be used to turn on/off the messages that are generated to
# standard output by doxygen. If QUIET is set to YES this implies that the
# messages are off.
# The default value is: NO.

QUIET                  = NO

# The WARNINGS tag can be used to turn on/off the warning messages that are
# generated to standard error (stderr) by doxygen. If WARNINGS is set to YES
# this implies that the warnings are on.
#
# Tip: Turn warnings on while writing the documentation.
# The default value is: YES.

WARNINGS               = YES

# If the WARN_IF_UNDOCUMENTED tag is set to YES then doxygen will generate
# warnings for undocumented members. If EXTRACT_ALL is set to YES then this flag
# will automatically be disabled.
# The default value is: YES.

WARN_IF_UNDOCUMENTED   = YES

# If the WARN_IF_DOC_ERROR tag is set to YES, doxygen will generate warnings for
# potential errors in the documentation, such as not documenting some parameters
# in a documented function, or documenting parameters that don't exist or using
# markup commands wrongly.
# The default value is: YES.

WARN_IF_DOC_ERROR      = YES

# This WARN_NO_PARAMDOC option can be enabled to get warnings for functions that
# are documented, but have no documentation for their parameters or return
# value. If set to NO, doxygen will only warn about wrong or incomplete
# parameter documentation, but not about the absence of documentation. If
# EXTRACT_ALL is set to YES then this flag will automatically be disabled.
# The default value is: NO.

WARN_NO_PARAMDOC       = NO

# If the WARN_AS_ERROR tag is set to YES then doxygen will immediately stop when
# a warning is encountered.
# The default value is: NO.

WARN_AS_ERROR          = NO

# The WARN_FORMAT tag determines the format of the warning messages that doxygen
# can produce. The string should contain the $file, $line, and $text tags, which
# will be replaced by the file and line number from which the warning originated
# and the warning text. Optionally the format may contain $version, which will
# be replaced by the version of the file (if it could be obtained via
# FILE_VERSION_FILTER)
# The default value is: $file:$line: $text.

WARN_FORMAT            = "$file:$line: $text"

# The WARN_LOGFILE tag can be used to specify a file to which warning and error
# messages should be written. If left blank the output is written to standard
# error (stderr).

WARN_LOGFILE           =

#---------------------------------------------------------------------------
# Configuration options related to the input files
#---------------------------------------------------------------------------

# The INPUT tag is used to specify the files and/or directories that contain
# documented source files. You may enter file names like myfile.cpp or
# directories like /usr/src/myproject. Separate the files or directories with
# spaces. See also FILE_PATTERNS and EXTENSION_MAPPING
# Note: If this tag is empty the current directory is searched.

INPUT                  = ../../ark

# This tag can be used to specify the character encoding of the source files
# that doxygen parses. Internally doxygen uses the UTF-8 encoding. Doxygen uses
# libiconv (or the iconv built into libc) for the transcoding. See the libiconv
# documentation (see: https://www.gnu.org/software/libiconv/) for the list of
# possible encodings.
# The default value is: UTF-8.

INPUT_ENCODING         = UTF-8

# If the value of the INPUT tag contains directories, you can use the
# FILE_PATTERNS tag to specify one or more wildcard patterns (like *.cpp and
# *.h) to filter out the source-files in the directories.
#
# Note that for custom extensions or not directly supported extensions you also
# need to set EXTENSION_MAPPING for the extension otherwise the files are not
# read by doxygen.
#
# If left blank the following patterns are tested:*.c, *.cc, *.cxx, *.cpp,
# *.c++, *.java, *.ii, *.ixx, *.ipp, *.i++, *.inl, *.idl, *.ddl, *.odl, *.h,
# *.hh, *.hxx, *.hpp, *.h++, *.cs, *.d, *.php, *.php4, *.php5, *.phtml, *.inc,
# *.m, *.markdown, *.md, *.mm, *.dox (to be provided as doxygen C comment),
# *.doc (to be provided as doxygen C comment), *.txt (to be provided as doxygen
# C comment), *.py, *.pyw, *.f90, *.f95, *.f03, *.f08, *.f, *.for, *.tcl, *.vhd,
# *.vhdl, *.ucf, *.qsf and *.ice.

FILE_PATTERNS          = *.c \
                         *.cc \
                         *.cxx \
                         *.cpp \
                         *.cu \
                         *.c++ \
                         *.java \
                         *.ii \
                         *.ixx \
                         *.ipp \
                         *.i++ \
                         *.inl \
                         *.idl \
                         *.ddl \
                         *.odl \
                         *.h \
                         *.hh \
                         *.hxx \
                         *.hpp \
                         *.h++ \
                         *.cs \
                         *.d \
                         *.php \
                         *.php4 \
                         *.php5 \
                         *.phtml \
                         *.inc \
                         *.m \
                         *.markdown \
                         *.md \
                         *.mm \
                         *.dox \
                         *.doc \
                         *.txt \
                         *.py \
                         *.pyw \
                         *.f90 \
                         *.f95 \
                         *.f03 \
                         *.f08 \
                         *.f \
                         *.for \
                         *.tcl \
                         *.vhd \
                         *.vhdl \
                         *.ucf \
                         *.qsf \
                         *.ice

# The RECURSIVE tag can be used to specify whether or not subdirectories should
# be searched for input files as well.
# The default value is: NO.

RECURSIVE              = YES

# The EXCLUDE tag can be used to specify files and/or directories that should be
# excluded from the INPUT source files. This way you can easily exclude a
# subdirectory from a directory tree whose root is specified with the INPUT tag.
#
# Note that relative paths are relative to the directory from which doxygen is
# run.

EXCLUDE                =

# The EXCLUDE_SYMLINKS tag can be used to select whether or not files or
# directories that are symbolic links (a Unix file system feature) are excluded
# from the input.
# The default value is: NO.

EXCLUDE_SYMLINKS       = NO

# If the value of the INPUT tag contains directories, you can use the
# EXCLUDE_PATTERNS tag to specify one or more wildcard patterns to exclude
# certain files from those directories.
#
# Note that the wildcards are matched against the file with absolute path, so to
# exclude all test directories for example use the pattern */test/*

EXCLUDE_PATTERNS       =

# The EXCLUDE_SYMBOLS tag can be used to specify one or more symbol names
# (namespaces, classes, functions, etc.) that should be excluded from the
# output. The symbol name can be a fully qualified name, a word, or if the
# wildcard * is used, a substring. Examples: ANamespace, AClass,
# AClass::ANamespace, ANamespace::*Test
#
# Note that the wildcards are matched against the file with absolute path, so to
# exclude all test directories use the pattern */test/*

EXCLUDE_SYMBOLS        =

# The EXAMPLE_PATH tag can be used to specify one or more files or directories
# that contain example code fragments that are included (see the \include
# command).

EXAMPLE_PATH           =

# If the value of the EXAMPLE_PATH tag contains directories, you can use the
# EXAMPLE_PATTERNS tag to specify one or more wildcard pattern (like *.cpp and
# *.h) to filter out the source-files in the directories. If left blank all
# files are included.

EXAMPLE_PATTERNS       = *

# If the EXAMPLE_RECURSIVE tag is set to YES then subdirectories will be
# searched for input files to be used with the \include or \dontinclude commands
# irrespective of the value of the RECURSIVE tag.
# The default value is: NO.

EXAMPLE_RECURSIVE      = NO

# The IMAGE_PATH tag can be used to specify one or more files or directories
# that contain images that are to be included in the documentation (see the
# \image command).

IMAGE_PATH             =

# The INPUT_FILTER tag can be used to specify a program that doxygen should
# invoke to filter for each input file. Doxygen will invoke the filter program
# by executing (via popen()) the command:
#
# <filter> <input-file>
#
# where <filter> is the value of the INPUT_FILTER tag, and <input-file> is the
# name of an input file. Doxygen will then use the output that the filter
# program writes to standard output. If FILTER_PATTERNS is specified, this tag
# will be ignored.
#
# Note that the filter must not add or remove lines; it is applied before the
# code is scanned, but not when the output code is generated. If lines are added
# or removed, the anchors will not be placed correctly.
#
# Note that for custom extensions or not directly supported extensions you also
# need to set EXTENSION_MAPPING for the extension otherwise the files are not
# properly processed by doxygen.

INPUT_FILTER           =

# The FILTER_PATTERNS tag can be used to specify filters on a per file pattern
# basis. Doxygen will compare the file name with each pattern and apply the
# filter if there is a match. The filters are a list of the form: pattern=filter
# (like *.cpp=my_cpp_filter). See INPUT_FILTER for further information on how
# filters are used. If the FILTER_PATTERNS tag is empty or if none of the
# patterns match the file name, INPUT_FILTER is applied.
#
# Note that for custom extensions or not directly supported extensions you also
# need to set EXTENSION_MAPPING for the extension otherwise the files are not
# properly processed by doxygen.

FILTER_PATTERNS        =

# If the FILTER_SOURCE_FILES tag is set to YES, the input filter (if set using
# INPUT_FILTER) will also be used to filter the input files that are used for
# producing the source files to browse (i.e. when SOURCE_BROWSER is set to YES).
# The default value is: NO.

FILTER_SOURCE_FILES    = NO

# The FILTER_SOURCE_PATTERNS tag can be used to specify source filters per file
# pattern. A pattern will override the setting for FILTER_PATTERN (if any) and
# it is also possible to disable source filtering for a specific pattern using
# *.ext= (so without naming a filter).
# This tag requires that the tag FILTER_SOURCE_FILES is set to YES.

FILTER_SOURCE_PATTERNS =

# If the USE_MDFILE_AS_MAINPAGE tag refers to the name of a markdown file that
# is part of the input, its contents will be placed on the main page
# (index.html). This can be useful if you have a project on for instance GitHub
# and want to reuse the introduction page also for the doxygen output.

USE_MDFILE_AS_MAINPAGE =

#---------------------------------------------------------------------------
# Configuration options related to source browsing
#---------------------------------------------------------------------------

# If the SOURCE_BROWSER tag is set to YES then a list of source files will be
# generated. Documented entities will be cross-referenced with these sources.
#
# Note: To get rid of all source code in the generated output, make sure that
# also VERBATIM_HEADERS is set to NO.
# The default value is: NO.

SOURCE_BROWSER         = NO

# Setting the INLINE_SOURCES tag to YES will include the body of functions,
# classes and enums directly into the documentation.
# The default value is: NO.

INLINE_SOURCES         = NO

# Setting the STRIP_CODE_COMMENTS tag to YES will instruct doxygen to hide any
# special comment blocks from generated source code fragments. Normal C, C++ and
# Fortran comments will always remain visible.
# The default value is: YES.

STRIP_CODE_COMMENTS    = YES

# If the REFERENCED_BY_RELATION tag is set to YES then for each documented
# entity all documented functions referencing it will be listed.
# The default value is: NO.

REFERENCED_BY_RELATION = NO

# If the REFERENCES_RELATION tag is set to YES then for each documented function
# all documented entities called/used by that function will be listed.
# The default value is: NO.

REFERENCES_RELATION    = NO

# If the REFERENCES_LINK_SOURCE tag is set to YES and SOURCE_BROWSER tag is set
# to YES then the hyperlinks from functions in REFERENCES_RELATION and
# REFERENCED_BY_RELATION lists will link to the source code. Otherwise they will
# link to the documentation.
# The default value is: YES.

REFERENCES_LINK_SOURCE = YES

# If SOURCE_TOOLTIPS is enabled (the default) then hovering a hyperlink in the
# source code will show a tooltip with additional information such as prototype,
# brief description and links to the definition and documentation. Since this
# will make the HTML file larger and loading of large files a bit slower, you
# can opt to disable this feature.
# The default value is: YES.
# This tag requires that the tag SOURCE_BROWSER is set to YES.

SOURCE_TOOLTIPS        = YES

# If the USE_HTAGS tag is set to YES then the references to source code will
# point to the HTML generated by the htags(1) tool instead of doxygen built-in
# source browser. The htags tool is part of GNU's global source tagging system
# (see https://www.gnu.org/software/global/global.html). You will need version
# 4.8.6 or higher.
#
# To use it do the following:
# - Install the latest version of global
# - Enable SOURCE_BROWSER and USE_HTAGS in the configuration file
# - Make sure the INPUT points to the root of the source tree
# - Run doxygen as normal
#
# Doxygen will invoke htags (and that will in turn invoke gtags), so these
# tools must be available from the command line (i.e. in the search path).
#
# The result: instead of the source browser generated by doxygen, the links to
# source code will now point to the output of htags.
# The default value is: NO.
# This tag requires that the tag SOURCE_BROWSER is set to YES.

USE_HTAGS              = NO

# If the VERBATIM_HEADERS tag is set the YES then doxygen will generate a
# verbatim copy of the header file for each class for which an include is
# specified. Set to NO to disable this.
# See also: Section \class.
# The default value is: YES.

VERBATIM_HEADERS       = YES

# If the CLANG_ASSISTED_PARSING tag is set to YES then doxygen will use the
# clang parser (see: http://clang.llvm.org/) for more accurate parsing at the
# cost of reduced performance. This can be particularly helpful with template
# rich C++ code for which doxygen's built-in parser lacks the necessary type
# information.
# Note: The availability of this option depends on whether or not doxygen was
# generated with the -Duse_libclang=ON option for CMake.
# The default value is: NO.

CLANG_ASSISTED_PARSING = NO

# If clang assisted parsing is enabled you can provide the compiler with command
# line options that you would normally use when invoking the compiler. Note that
# the include paths will already be set by doxygen for the files and directories
# specified with INPUT and INCLUDE_PATH.
# This tag requires that the tag CLANG_ASSISTED_PARSING is set to YES.

CLANG_OPTIONS          =

# If clang assisted parsing is enabled you can provide the clang parser with the
# path to the compilation database (see:
# http://clang.llvm.org/docs/HowToSetupToolingForLLVM.html) used when the files
# were built. This is equivalent to specifying the "-p" option to a clang tool,
# such as clang-check. These options will then be passed to the parser.
# Note: The availability of this option depends on whether or not doxygen was
# generated with the -Duse_libclang=ON option for CMake.

CLANG_DATABASE_PATH    =

#---------------------------------------------------------------------------
# Configuration options related to the alphabetical class index
#---------------------------------------------------------------------------

# If the ALPHABETICAL_INDEX tag is set to YES, an alphabetical index of all
# compounds will be generated. Enable this if the project contains a lot of
# classes, structs, unions or interfaces.
# The default value is: YES.

ALPHABETICAL_INDEX     = YES

# The COLS_IN_ALPHA_INDEX tag can be used to specify the number of columns in
# which the alphabetical index list will be split.
# Minimum value: 1, maximum value: 20, default value: 5.
# This tag requires that the tag ALPHABETICAL_INDEX is set to YES.

COLS_IN_ALPHA_INDEX    = 5

# In case all classes in a project start with a common prefix, all classes will
# be put under the same header in the alphabetical index. The IGNORE_PREFIX tag
# can be used to specify a prefix (or a list of prefixes) that should be ignored
# while generating the index headers.
# This tag requires that the tag ALPHABETICAL_INDEX is set to YES.

IGNORE_PREFIX          =

#---------------------------------------------------------------------------
# Configuration options related to the HTML output
#---------------------------------------------------------------------------

# If the GENERATE_HTML tag is set to YES, doxygen will generate HTML output
# The default value is: YES.

GENERATE_HTML          = YES

# The HTML_OUTPUT tag is used to specify where the HTML docs will be put. If a
# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of
# it.
# The default directory is: html.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_OUTPUT            = html

# The HTML_FILE_EXTENSION tag can be used to specify the file extension for each
# generated HTML page (for example: .htm, .php, .asp).
# The default value is: .html.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_FILE_EXTENSION    = .html

# The HTML_HEADER tag can be used to specify a user-defined HTML header file for
# each generated HTML page. If the tag is left blank doxygen will generate a
# standard header.
#
# To get valid HTML the header file that includes any scripts and style sheets
# that doxygen needs, which is dependent on the configuration options used (e.g.
# the setting GENERATE_TREEVIEW). It is highly recommended to start with a
# default header using
# doxygen -w html new_header.html new_footer.html new_stylesheet.css
# YourConfigFile
# and then modify the file new_header.html. See also section "Doxygen usage"
# for information on how to generate the default header that doxygen normally
# uses.
# Note: The header is subject to change so you typically have to regenerate the
# default header when upgrading to a newer version of doxygen. For a description
# of the possible markers and block names see the documentation.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_HEADER            =

# The HTML_FOOTER tag can be used to specify a user-defined HTML footer for each
# generated HTML page. If the tag is left blank doxygen will generate a standard
# footer. See HTML_HEADER for more information on how to generate a default
# footer and what special commands can be used inside the footer. See also
# section "Doxygen usage" for information on how to generate the default footer
# that doxygen normally uses.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_FOOTER            =

# The HTML_STYLESHEET tag can be used to specify a user-defined cascading style
# sheet that is used by each HTML page. It can be used to fine-tune the look of
# the HTML output. If left blank doxygen will generate a default style sheet.
# See also section "Doxygen usage" for information on how to generate the style
# sheet that doxygen normally uses.
# Note: It is recommended to use HTML_EXTRA_STYLESHEET instead of this tag, as
# it is more robust and this tag (HTML_STYLESHEET) will in the future become
# obsolete.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_STYLESHEET        =

# The HTML_EXTRA_STYLESHEET tag can be used to specify additional user-defined
# cascading style sheets that are included after the standard style sheets
# created by doxygen. Using this option one can overrule certain style aspects.
# This is preferred over using HTML_STYLESHEET since it does not replace the
# standard style sheet and is therefore more robust against future updates.
# Doxygen will copy the style sheet files to the output directory.
# Note: The order of the extra style sheet files is of importance (e.g. the last
# style sheet in the list overrules the setting of the previous ones in the
# list). For an example see the documentation.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_EXTRA_STYLESHEET  =

# The HTML_EXTRA_FILES tag can be used to specify one or more extra images or
# other source files which should be copied to the HTML output directory. Note
# that these files will be copied to the base HTML output directory. Use the
# $relpath^ marker in the HTML_HEADER and/or HTML_FOOTER files to load these
# files. In the HTML_STYLESHEET file, use the file name only. Also note that the
# files will be copied as-is; there are no commands or markers available.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_EXTRA_FILES       =

# The HTML_COLORSTYLE_HUE tag controls the color of the HTML output. Doxygen
# will adjust the colors in the style sheet and background images according to
# this color. Hue is specified as an angle on a colorwheel, see
# https://en.wikipedia.org/wiki/Hue for more information. For instance the value
# 0 represents red, 60 is yellow, 120 is green, 180 is cyan, 240 is blue, 300
# purple, and 360 is red again.
# Minimum value: 0, maximum value: 359, default value: 220.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_COLORSTYLE_HUE    = 220

# The HTML_COLORSTYLE_SAT tag controls the purity (or saturation) of the colors
# in the HTML output. For a value of 0 the output will use grayscales only. A
# value of 255 will produce the most vivid colors.
# Minimum value: 0, maximum value: 255, default value: 100.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_COLORSTYLE_SAT    = 100

# The HTML_COLORSTYLE_GAMMA tag controls the gamma correction applied to the
# luminance component of the colors in the HTML output. Values below 100
# gradually make the output lighter, whereas values above 100 make the output
# darker. The value divided by 100 is the actual gamma applied, so 80 represents
# a gamma of 0.8, The value 220 represents a gamma of 2.2, and 100 does not
# change the gamma.
# Minimum value: 40, maximum value: 240, default value: 80.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_COLORSTYLE_GAMMA  = 80

# If the HTML_TIMESTAMP tag is set to YES then the footer of each generated HTML
# page will contain the date and time when the page was generated. Setting this
# to YES can help to show when doxygen was last run and thus if the
# documentation is up to date.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_TIMESTAMP         = NO

# If the HTML_DYNAMIC_MENUS tag is set to YES then the generated HTML
# documentation will contain a main index with vertical navigation menus that
# are dynamically created via JavaScript. If disabled, the navigation index will
# consists of multiple levels of tabs that are statically embedded in every HTML
# page. Disable this option to support browsers that do not have JavaScript,
# like the Qt help browser.
# The default value is: YES.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_DYNAMIC_MENUS     = YES

# If the HTML_DYNAMIC_SECTIONS tag is set to YES then the generated HTML
# documentation will contain sections that can be hidden and shown after the
# page has loaded.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_DYNAMIC_SECTIONS  = NO

# With HTML_INDEX_NUM_ENTRIES one can control the preferred number of entries
# shown in the various tree structured indices initially; the user can expand
# and collapse entries dynamically later on. Doxygen will expand the tree to
# such a level that at most the specified number of entries are visible (unless
# a fully collapsed tree already exceeds this amount). So setting the number of
# entries 1 will produce a full collapsed tree by default. 0 is a special value
# representing an infinite number of entries and will result in a full expanded
# tree by default.
# Minimum value: 0, maximum value: 9999, default value: 100.
# This tag requires that the tag GENERATE_HTML is set to YES.

HTML_INDEX_NUM_ENTRIES = 100

# If the GENERATE_DOCSET tag is set to YES, additional index files will be
# generated that can be used as input for Apple's Xcode 3 integrated development
# environment (see: https://developer.apple.com/xcode/), introduced with OSX
# 10.5 (Leopard). To create a documentation set, doxygen will generate a
# Makefile in the HTML output directory. Running make will produce the docset in
# that directory and running make install will install the docset in
# ~/Library/Developer/Shared/Documentation/DocSets so that Xcode will find it at
# startup. See https://developer.apple.com/library/archive/featuredarticles/Doxy
# genXcode/_index.html for more information.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

GENERATE_DOCSET        = NO

# This tag determines the name of the docset feed. A documentation feed provides
# an umbrella under which multiple documentation sets from a single provider
# (such as a company or product suite) can be grouped.
# The default value is: Doxygen generated docs.
# This tag requires that the tag GENERATE_DOCSET is set to YES.

DOCSET_FEEDNAME        = "Doxygen generated docs"

# This tag specifies a string that should uniquely identify the documentation
# set bundle. This should be a reverse domain-name style string, e.g.
# com.mycompany.MyDocSet. Doxygen will append .docset to the name.
# The default value is: org.doxygen.Project.
# This tag requires that the tag GENERATE_DOCSET is set to YES.

DOCSET_BUNDLE_ID       = org.doxygen.Project

# The DOCSET_PUBLISHER_ID tag specifies a string that should uniquely identify
# the documentation publisher. This should be a reverse domain-name style
# string, e.g. com.mycompany.MyDocSet.documentation.
# The default value is: org.doxygen.Publisher.
# This tag requires that the tag GENERATE_DOCSET is set to YES.

DOCSET_PUBLISHER_ID    = org.doxygen.Publisher

# The DOCSET_PUBLISHER_NAME tag identifies the documentation publisher.
# The default value is: Publisher.
# This tag requires that the tag GENERATE_DOCSET is set to YES.

DOCSET_PUBLISHER_NAME  = Publisher

# If the GENERATE_HTMLHELP tag is set to YES then doxygen generates three
# additional HTML index files: index.hhp, index.hhc, and index.hhk. The
# index.hhp is a project file that can be read by Microsoft's HTML Help Workshop
# (see: https://www.microsoft.com/en-us/download/details.aspx?id=21138) on
# Windows.
#
# The HTML Help Workshop contains a compiler that can convert all HTML output
# generated by doxygen into a single compiled HTML file (.chm). Compiled HTML
# files are now used as the Windows 98 help format, and will replace the old
# Windows help format (.hlp) on all Windows platforms in the future. Compressed
# HTML files also contain an index, a table of contents, and you can search for
# words in the documentation. The HTML workshop also contains a viewer for
# compressed HTML files.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

GENERATE_HTMLHELP      = NO

# The CHM_FILE tag can be used to specify the file name of the resulting .chm
# file. You can add a path in front of the file if the result should not be
# written to the html output directory.
# This tag requires that the tag GENERATE_HTMLHELP is set to YES.

CHM_FILE               =

# The HHC_LOCATION tag can be used to specify the location (absolute path
# including file name) of the HTML help compiler (hhc.exe). If non-empty,
# doxygen will try to run the HTML help compiler on the generated index.hhp.
# The file has to be specified with full path.
# This tag requires that the tag GENERATE_HTMLHELP is set to YES.

HHC_LOCATION           =

# The GENERATE_CHI flag controls if a separate .chi index file is generated
# (YES) or that it should be included in the master .chm file (NO).
# The default value is: NO.
# This tag requires that the tag GENERATE_HTMLHELP is set to YES.

GENERATE_CHI           = NO

# The CHM_INDEX_ENCODING is used to encode HtmlHelp index (hhk), content (hhc)
# and project file content.
# This tag requires that the tag GENERATE_HTMLHELP is set to YES.

CHM_INDEX_ENCODING     =

# The BINARY_TOC flag controls whether a binary table of contents is generated
# (YES) or a normal table of contents (NO) in the .chm file. Furthermore it
# enables the Previous and Next buttons.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTMLHELP is set to YES.

BINARY_TOC             = NO

# The TOC_EXPAND flag can be set to YES to add extra items for group members to
# the table of contents of the HTML help documentation and to the tree view.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTMLHELP is set to YES.

TOC_EXPAND             = NO

# If the GENERATE_QHP tag is set to YES and both QHP_NAMESPACE and
# QHP_VIRTUAL_FOLDER are set, an additional index file will be generated that
# can be used as input for Qt's qhelpgenerator to generate a Qt Compressed Help
# (.qch) of the generated HTML documentation.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

GENERATE_QHP           = NO

# If the QHG_LOCATION tag is specified, the QCH_FILE tag can be used to specify
# the file name of the resulting .qch file. The path specified is relative to
# the HTML output folder.
# This tag requires that the tag GENERATE_QHP is set to YES.

QCH_FILE               =

# The QHP_NAMESPACE tag specifies the namespace to use when generating Qt Help
# Project output. For more information please see Qt Help Project / Namespace
# (see: https://doc.qt.io/archives/qt-4.8/qthelpproject.html#namespace).
# The default value is: org.doxygen.Project.
# This tag requires that the tag GENERATE_QHP is set to YES.

QHP_NAMESPACE          = org.doxygen.Project

# The QHP_VIRTUAL_FOLDER tag specifies the namespace to use when generating Qt
# Help Project output. For more information please see Qt Help Project / Virtual
# Folders (see: https://doc.qt.io/archives/qt-4.8/qthelpproject.html#virtual-
# folders).
# The default value is: doc.
# This tag requires that the tag GENERATE_QHP is set to YES.

QHP_VIRTUAL_FOLDER     = doc

# If the QHP_CUST_FILTER_NAME tag is set, it specifies the name of a custom
# filter to add. For more information please see Qt Help Project / Custom
# Filters (see: https://doc.qt.io/archives/qt-4.8/qthelpproject.html#custom-
# filters).
# This tag requires that the tag GENERATE_QHP is set to YES.

QHP_CUST_FILTER_NAME   =

# The QHP_CUST_FILTER_ATTRS tag specifies the list of the attributes of the
# custom filter to add. For more information please see Qt Help Project / Custom
# Filters (see: https://doc.qt.io/archives/qt-4.8/qthelpproject.html#custom-
# filters).
# This tag requires that the tag GENERATE_QHP is set to YES.

QHP_CUST_FILTER_ATTRS  =

# The QHP_SECT_FILTER_ATTRS tag specifies the list of the attributes this
# project's filter section matches. Qt Help Project / Filter Attributes (see:
# https://doc.qt.io/archives/qt-4.8/qthelpproject.html#filter-attributes).
# This tag requires that the tag GENERATE_QHP is set to YES.

QHP_SECT_FILTER_ATTRS  =

# The QHG_LOCATION tag can be used to specify the location of Qt's
# qhelpgenerator. If non-empty doxygen will try to run qhelpgenerator on the
# generated .qhp file.
# This tag requires that the tag GENERATE_QHP is set to YES.

QHG_LOCATION           =

# If the GENERATE_ECLIPSEHELP tag is set to YES, additional index files will be
# generated, together with the HTML files, they form an Eclipse help plugin. To
# install this plugin and make it available under the help contents menu in
# Eclipse, the contents of the directory containing the HTML and XML files needs
# to be copied into the plugins directory of eclipse. The name of the directory
# within the plugins directory should be the same as the ECLIPSE_DOC_ID value.
# After copying Eclipse needs to be restarted before the help appears.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

GENERATE_ECLIPSEHELP   = NO

# A unique identifier for the Eclipse help plugin. When installing the plugin
# the directory name containing the HTML and XML files should also have this
# name. Each documentation set should have its own identifier.
# The default value is: org.doxygen.Project.
# This tag requires that the tag GENERATE_ECLIPSEHELP is set to YES.

ECLIPSE_DOC_ID         = org.doxygen.Project

# If you want full control over the layout of the generated HTML pages it might
# be necessary to disable the index and replace it with your own. The
# DISABLE_INDEX tag can be used to turn on/off the condensed index (tabs) at top
# of each HTML page. A value of NO enables the index and the value YES disables
# it. Since the tabs in the index contain the same information as the navigation
# tree, you can set this option to YES if you also set GENERATE_TREEVIEW to YES.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

DISABLE_INDEX          = NO

# The GENERATE_TREEVIEW tag is used to specify whether a tree-like index
# structure should be generated to display hierarchical information. If the tag
# value is set to YES, a side panel will be generated containing a tree-like
# index structure (just like the one that is generated for HTML Help). For this
# to work a browser that supports JavaScript, DHTML, CSS and frames is required
# (i.e. any modern browser). Windows users are probably better off using the
# HTML help feature. Via custom style sheets (see HTML_EXTRA_STYLESHEET) one can
# further fine-tune the look of the index. As an example, the default style
# sheet generated by doxygen has an example that shows how to put an image at
# the root of the tree instead of the PROJECT_NAME. Since the tree basically has
# the same information as the tab index, you could consider setting
# DISABLE_INDEX to YES when enabling this option.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

GENERATE_TREEVIEW      = NO

# The ENUM_VALUES_PER_LINE tag can be used to set the number of enum values that
# doxygen will group on one line in the generated HTML documentation.
#
# Note that a value of 0 will completely suppress the enum values from appearing
# in the overview section.
# Minimum value: 0, maximum value: 20, default value: 4.
# This tag requires that the tag GENERATE_HTML is set to YES.

ENUM_VALUES_PER_LINE   = 4

# If the treeview is enabled (see GENERATE_TREEVIEW) then this tag can be used
# to set the initial width (in pixels) of the frame in which the tree is shown.
# Minimum value: 0, maximum value: 1500, default value: 250.
# This tag requires that the tag GENERATE_HTML is set to YES.

TREEVIEW_WIDTH         = 250

# If the EXT_LINKS_IN_WINDOW option is set to YES, doxygen will open links to
# external symbols imported via tag files in a separate window.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

EXT_LINKS_IN_WINDOW    = NO

# Use this tag to change the font size of LaTeX formulas included as images in
# the HTML documentation. When you change the font size after a successful
# doxygen run you need to manually remove any form_*.png images from the HTML
# output directory to force them to be regenerated.
# Minimum value: 8, maximum value: 50, default value: 10.
# This tag requires that the tag GENERATE_HTML is set to YES.

FORMULA_FONTSIZE       = 10

# Use the FORMULA_TRANSPARENT tag to determine whether or not the images
# generated for formulas are transparent PNGs. Transparent PNGs are not
# supported properly for IE 6.0, but are supported on all modern browsers.
#
# Note that when changing this option you need to delete any form_*.png files in
# the HTML output directory before the changes have effect.
# The default value is: YES.
# This tag requires that the tag GENERATE_HTML is set to YES.

FORMULA_TRANSPARENT    = YES

# The FORMULA_MACROFILE can contain LaTeX \newcommand and \renewcommand commands
# to create new LaTeX commands to be used in formulas as building blocks. See
# the section "Including formulas" for details.

FORMULA_MACROFILE      =

# Enable the USE_MATHJAX option to render LaTeX formulas using MathJax (see
# https://www.mathjax.org) which uses client side JavaScript for the rendering
# instead of using pre-rendered bitmaps. Use this if you do not have LaTeX
# installed or if you want to formulas look prettier in the HTML output. When
# enabled you may also need to install MathJax separately and configure the path
# to it using the MATHJAX_RELPATH option.
# The default value is: NO.
# This tag requires that the tag GENERATE_HTML is set to YES.

USE_MATHJAX            = NO

# When MathJax is enabled you can set the default output format to be used for
# the MathJax output. See the MathJax site (see:
# http://docs.mathjax.org/en/latest/output.html) for more details.
# Possible values are: HTML-CSS (which is slower, but has the best
# compatibility), NativeMML (i.e. MathML) and SVG.
# The default value is: HTML-CSS.
# This tag requires that the tag USE_MATHJAX is set to YES.

MATHJAX_FORMAT         = HTML-CSS

# When MathJax is enabled you need to specify the location relative to the HTML
# output directory using the MATHJAX_RELPATH option. The destination directory
# should contain the MathJax.js script. For instance, if the mathjax directory
# is located at the same level as the HTML output directory, then
# MATHJAX_RELPATH should be ../mathjax. The default value points to the MathJax
# Content Delivery Network so you can quickly see the result without installing
# MathJax. However, it is strongly recommended to install a local copy of
# MathJax from https://www.mathjax.org before deployment.
# The default value is: https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/.
# This tag requires that the tag USE_MATHJAX is set to YES.

MATHJAX_RELPATH        = https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/

# The MATHJAX_EXTENSIONS tag can be used to specify one or more MathJax
# extension names that should be enabled during MathJax rendering. For example
# MATHJAX_EXTENSIONS = TeX/AMSmath TeX/AMSsymbols
# This tag requires that the tag USE_MATHJAX is set to YES.

MATHJAX_EXTENSIONS     =

# The MATHJAX_CODEFILE tag can be used to specify a file with javascript pieces
# of code that will be used on startup of the MathJax code. See the MathJax site
# (see: http://docs.mathjax.org/en/latest/output.html) for more details. For an
# example see the documentation.
# This tag requires that the tag USE_MATHJAX is set to YES.

MATHJAX_CODEFILE       =

# When the SEARCHENGINE tag is enabled doxygen will generate a search box for
# the HTML output. The underlying search engine uses javascript and DHTML and
# should work on any modern browser. Note that when using HTML help
# (GENERATE_HTMLHELP), Qt help (GENERATE_QHP), or docsets (GENERATE_DOCSET)
# there is already a search function so this one should typically be disabled.
# For large projects the javascript based search engine can be slow, then
# enabling SERVER_BASED_SEARCH may provide a better solution. It is possible to
# search using the keyboard; to jump to the search box use <access key> + S
# (what the <access key> is depends on the OS and browser, but it is typically
# <CTRL>, <ALT>/<option>, or both). Inside the search box use the <cursor down
# key> to jump into the search results window, the results can be navigated
# using the <cursor keys>. Press <Enter> to select an item or <escape> to cancel
# the search. The filter options can be selected when the cursor is inside the
# search box by pressing <Shift>+<cursor down>. Also here use the <cursor keys>
# to select a filter and <Enter> or <escape> to activate or cancel the filter
# option.
# The default value is: YES.
# This tag requires that the tag GENERATE_HTML is set to YES.

SEARCHENGINE           = YES

# When the SERVER_BASED_SEARCH tag is enabled the search engine will be
# implemented using a web server instead of a web client using JavaScript. There
# are two flavors of web server based searching depending on the EXTERNAL_SEARCH
# setting. When disabled, doxygen will generate a PHP script for searching and
# an index file used by the script. When EXTERNAL_SEARCH is enabled the indexing
# and searching needs to be provided by external tools. See the section
# "External Indexing and Searching" for details.
# The default value is: NO.
# This tag requires that the tag SEARCHENGINE is set to YES.

SERVER_BASED_SEARCH    = NO

# When EXTERNAL_SEARCH tag is enabled doxygen will no longer generate the PHP
# script for searching. Instead the search results are written to an XML file
# which needs to be processed by an external indexer. Doxygen will invoke an
# external search engine pointed to by the SEARCHENGINE_URL option to obtain the
# search results.
#
# Doxygen ships with an example indexer (doxyindexer) and search engine
# (doxysearch.cgi) which are based on the open source search engine library
# Xapian (see: https://xapian.org/).
#
# See the section "External Indexing and Searching" for details.
# The default value is: NO.
# This tag requires that the tag SEARCHENGINE is set to YES.

EXTERNAL_SEARCH        = NO

# The SEARCHENGINE_URL should point to a search engine hosted by a web server
# which will return the search results when EXTERNAL_SEARCH is enabled.
#
# Doxygen ships with an example indexer (doxyindexer) and search engine
# (doxysearch.cgi) which are based on the open source search engine library
# Xapian (see: https://xapian.org/). See the section "External Indexing and
# Searching" for details.
# This tag requires that the tag SEARCHENGINE is set to YES.

SEARCHENGINE_URL       =

# When SERVER_BASED_SEARCH and EXTERNAL_SEARCH are both enabled the unindexed
# search data is written to a file for indexing by an external tool. With the
# SEARCHDATA_FILE tag the name of this file can be specified.
# The default file is: searchdata.xml.
# This tag requires that the tag SEARCHENGINE is set to YES.

SEARCHDATA_FILE        = searchdata.xml

# When SERVER_BASED_SEARCH and EXTERNAL_SEARCH are both enabled the
# EXTERNAL_SEARCH_ID tag can be used as an identifier for the project. This is
# useful in combination with EXTRA_SEARCH_MAPPINGS to search through multiple
# projects and redirect the results back to the right project.
# This tag requires that the tag SEARCHENGINE is set to YES.

EXTERNAL_SEARCH_ID     =

# The EXTRA_SEARCH_MAPPINGS tag can be used to enable searching through doxygen
# projects other than the one defined by this configuration file, but that are
# all added to the same external search index. Each project needs to have a
# unique id set via EXTERNAL_SEARCH_ID. The search mapping then maps the id of
# to a relative location where the documentation can be found. The format is:
# EXTRA_SEARCH_MAPPINGS = tagname1=loc1 tagname2=loc2 ...
# This tag requires that the tag SEARCHENGINE is set to YES.

EXTRA_SEARCH_MAPPINGS  =

#---------------------------------------------------------------------------
# Configuration options related to the LaTeX output
#---------------------------------------------------------------------------

# If the GENERATE_LATEX tag is set to YES, doxygen will generate LaTeX output.
# The default value is: YES.

GENERATE_LATEX         = NO

# The LATEX_OUTPUT tag is used to specify where the LaTeX docs will be put. If a
# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of
# it.
# The default directory is: latex.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_OUTPUT           = latex

# The LATEX_CMD_NAME tag can be used to specify the LaTeX command name to be
# invoked.
#
# Note that when not enabling USE_PDFLATEX the default is latex when enabling
# USE_PDFLATEX the default is pdflatex and when in the later case latex is
# chosen this is overwritten by pdflatex. For specific output languages the
# default can have been set differently, this depends on the implementation of
# the output language.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_CMD_NAME         =

# The MAKEINDEX_CMD_NAME tag can be used to specify the command name to generate
# index for LaTeX.
# Note: This tag is used in the Makefile / make.bat.
# See also: LATEX_MAKEINDEX_CMD for the part in the generated output file
# (.tex).
# The default file is: makeindex.
# This tag requires that the tag GENERATE_LATEX is set to YES.

MAKEINDEX_CMD_NAME     = makeindex

# The LATEX_MAKEINDEX_CMD tag can be used to specify the command name to
# generate index for LaTeX. In case there is no backslash (\) as first character
# it will be automatically added in the LaTeX code.
# Note: This tag is used in the generated output file (.tex).
# See also: MAKEINDEX_CMD_NAME for the part in the Makefile / make.bat.
# The default value is: makeindex.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_MAKEINDEX_CMD    = makeindex

# If the COMPACT_LATEX tag is set to YES, doxygen generates more compact LaTeX
# documents. This may be useful for small projects and may help to save some
# trees in general.
# The default value is: NO.
# This tag requires that the tag GENERATE_LATEX is set to YES.

COMPACT_LATEX          = NO

# The PAPER_TYPE tag can be used to set the paper type that is used by the
# printer.
# Possible values are: a4 (210 x 297 mm), letter (8.5 x 11 inches), legal (8.5 x
# 14 inches) and executive (7.25 x 10.5 inches).
# The default value is: a4.
# This tag requires that the tag GENERATE_LATEX is set to YES.

PAPER_TYPE             = a4

# The EXTRA_PACKAGES tag can be used to specify one or more LaTeX package names
# that should be included in the LaTeX output. The package can be specified just
# by its name or with the correct syntax as to be used with the LaTeX
# \usepackage command. To get the times font for instance you can specify :
# EXTRA_PACKAGES=times or EXTRA_PACKAGES={times}
# To use the option intlimits with the amsmath package you can specify:
# EXTRA_PACKAGES=[intlimits]{amsmath}
# If left blank no extra packages will be included.
# This tag requires that the tag GENERATE_LATEX is set to YES.

EXTRA_PACKAGES         =

# The LATEX_HEADER tag can be used to specify a personal LaTeX header for the
# generated LaTeX document. The header should contain everything until the first
# chapter. If it is left blank doxygen will generate a standard header. See
# section "Doxygen usage" for information on how to let doxygen write the
# default header to a separate file.
#
# Note: Only use a user-defined header if you know what you are doing! The
# following commands have a special meaning inside the header: $title,
# $datetime, $date, $doxygenversion, $projectname, $projectnumber,
# $projectbrief, $projectlogo. Doxygen will replace $title with the empty
# string, for the replacement values of the other commands the user is referred
# to HTML_HEADER.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_HEADER           =

# The LATEX_FOOTER tag can be used to specify a personal LaTeX footer for the
# generated LaTeX document. The footer should contain everything after the last
# chapter. If it is left blank doxygen will generate a standard footer. See
# LATEX_HEADER for more information on how to generate a default footer and what
# special commands can be used inside the footer.
#
# Note: Only use a user-defined footer if you know what you are doing!
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_FOOTER           =

# The LATEX_EXTRA_STYLESHEET tag can be used to specify additional user-defined
# LaTeX style sheets that are included after the standard style sheets created
# by doxygen. Using this option one can overrule certain style aspects. Doxygen
# will copy the style sheet files to the output directory.
# Note: The order of the extra style sheet files is of importance (e.g. the last
# style sheet in the list overrules the setting of the previous ones in the
# list).
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_EXTRA_STYLESHEET =

# The LATEX_EXTRA_FILES tag can be used to specify one or more extra images or
# other source files which should be copied to the LATEX_OUTPUT output
# directory. Note that the files will be copied as-is; there are no commands or
# markers available.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_EXTRA_FILES      =

# If the PDF_HYPERLINKS tag is set to YES, the LaTeX that is generated is
# prepared for conversion to PDF (using ps2pdf or pdflatex). The PDF file will
# contain links (just like the HTML output) instead of page references. This
# makes the output suitable for online browsing using a PDF viewer.
# The default value is: YES.
# This tag requires that the tag GENERATE_LATEX is set to YES.

PDF_HYPERLINKS         = YES

# If the USE_PDFLATEX tag is set to YES, doxygen will use pdflatex to generate
# the PDF file directly from the LaTeX files. Set this option to YES, to get a
# higher quality PDF documentation.
# The default value is: YES.
# This tag requires that the tag GENERATE_LATEX is set to YES.

USE_PDFLATEX           = YES

# If the LATEX_BATCHMODE tag is set to YES, doxygen will add the \batchmode
# command to the generated LaTeX files. This will instruct LaTeX to keep running
# if errors occur, instead of asking the user for help. This option is also used
# when generating formulas in HTML.
# The default value is: NO.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_BATCHMODE        = NO

# If the LATEX_HIDE_INDICES tag is set to YES then doxygen will not include the
# index chapters (such as File Index, Compound Index, etc.) in the output.
# The default value is: NO.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_HIDE_INDICES     = NO

# If the LATEX_SOURCE_CODE tag is set to YES then doxygen will include source
# code with syntax highlighting in the LaTeX output.
#
# Note that which sources are shown also depends on other settings such as
# SOURCE_BROWSER.
# The default value is: NO.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_SOURCE_CODE      = NO

# The LATEX_BIB_STYLE tag can be used to specify the style to use for the
# bibliography, e.g. plainnat, or ieeetr. See
# https://en.wikipedia.org/wiki/BibTeX and \cite for more info.
# The default value is: plain.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_BIB_STYLE        = plain

# If the LATEX_TIMESTAMP tag is set to YES then the footer of each generated
# page will contain the date and time when the page was generated. Setting this
# to NO can help when comparing the output of multiple runs.
# The default value is: NO.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_TIMESTAMP        = NO

# The LATEX_EMOJI_DIRECTORY tag is used to specify the (relative or absolute)
# path from which the emoji images will be read. If a relative path is entered,
# it will be relative to the LATEX_OUTPUT directory. If left blank the
# LATEX_OUTPUT directory will be used.
# This tag requires that the tag GENERATE_LATEX is set to YES.

LATEX_EMOJI_DIRECTORY  =

#---------------------------------------------------------------------------
# Configuration options related to the RTF output
#---------------------------------------------------------------------------

# If the GENERATE_RTF tag is set to YES, doxygen will generate RTF output. The
# RTF output is optimized for Word 97 and may not look too pretty with other RTF
# readers/editors.
# The default value is: NO.

GENERATE_RTF           = NO

# The RTF_OUTPUT tag is used to specify where the RTF docs will be put. If a
# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of
# it.
# The default directory is: rtf.
# This tag requires that the tag GENERATE_RTF is set to YES.

RTF_OUTPUT             = rtf

# If the COMPACT_RTF tag is set to YES, doxygen generates more compact RTF
# documents. This may be useful for small projects and may help to save some
# trees in general.
# The default value is: NO.
# This tag requires that the tag GENERATE_RTF is set to YES.

COMPACT_RTF            = NO

# If the RTF_HYPERLINKS tag is set to YES, the RTF that is generated will
# contain hyperlink fields. The RTF file will contain links (just like the HTML
# output) instead of page references. This makes the output suitable for online
# browsing using Word or some other Word compatible readers that support those
# fields.
#
# Note: WordPad (write) and others do not support links.
# The default value is: NO.
# This tag requires that the tag GENERATE_RTF is set to YES.

RTF_HYPERLINKS         = NO

# Load stylesheet definitions from file. Syntax is similar to doxygen's
# configuration file, i.e. a series of assignments. You only have to provide
# replacements, missing definitions are set to their default value.
#
# See also section "Doxygen usage" for information on how to generate the
# default style sheet that doxygen normally uses.
# This tag requires that the tag GENERATE_RTF is set to YES.

RTF_STYLESHEET_FILE    =

# Set optional variables used in the generation of an RTF document. Syntax is
# similar to doxygen's configuration file. A template extensions file can be
# generated using doxygen -e rtf extensionFile.
# This tag requires that the tag GENERATE_RTF is set to YES.

RTF_EXTENSIONS_FILE    =

# If the RTF_SOURCE_CODE tag is set to YES then doxygen will include source code
# with syntax highlighting in the RTF output.
#
# Note that which sources are shown also depends on other settings such as
# SOURCE_BROWSER.
# The default value is: NO.
# This tag requires that the tag GENERATE_RTF is set to YES.

RTF_SOURCE_CODE        = NO

#---------------------------------------------------------------------------
# Configuration options related to the man page output
#---------------------------------------------------------------------------

# If the GENERATE_MAN tag is set to YES, doxygen will generate man pages for
# classes and files.
# The default value is: NO.

GENERATE_MAN           = NO

# The MAN_OUTPUT tag is used to specify where the man pages will be put. If a
# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of
# it. A directory man3 will be created inside the directory specified by
# MAN_OUTPUT.
# The default directory is: man.
# This tag requires that the tag GENERATE_MAN is set to YES.

MAN_OUTPUT             = man

# The MAN_EXTENSION tag determines the extension that is added to the generated
# man pages. In case the manual section does not start with a number, the number
# 3 is prepended. The dot (.) at the beginning of the MAN_EXTENSION tag is
# optional.
# The default value is: .3.
# This tag requires that the tag GENERATE_MAN is set to YES.

MAN_EXTENSION          = .3

# The MAN_SUBDIR tag determines the name of the directory created within
# MAN_OUTPUT in which the man pages are placed. If defaults to man followed by
# MAN_EXTENSION with the initial . removed.
# This tag requires that the tag GENERATE_MAN is set to YES.

MAN_SUBDIR             =

# If the MAN_LINKS tag is set to YES and doxygen generates man output, then it
# will generate one additional man file for each entity documented in the real
# man page(s). These additional files only source the real man page, but without
# them the man command would be unable to find the correct page.
# The default value is: NO.
# This tag requires that the tag GENERATE_MAN is set to YES.

MAN_LINKS              = NO

#---------------------------------------------------------------------------
# Configuration options related to the XML output
#---------------------------------------------------------------------------

# If the GENERATE_XML tag is set to YES, doxygen will generate an XML file that
# captures the structure of the code including all documentation.
# The default value is: NO.

GENERATE_XML           = NO

# The XML_OUTPUT tag is used to specify where the XML pages will be put. If a
# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of
# it.
# The default directory is: xml.
# This tag requires that the tag GENERATE_XML is set to YES.

XML_OUTPUT             = xml

# If the XML_PROGRAMLISTING tag is set to YES, doxygen will dump the program
# listings (including syntax highlighting and cross-referencing information) to
# the XML output. Note that enabling this will significantly increase the size
# of the XML output.
# The default value is: YES.
# This tag requires that the tag GENERATE_XML is set to YES.

XML_PROGRAMLISTING     = YES

# If the XML_NS_MEMB_FILE_SCOPE tag is set to YES, doxygen will include
# namespace members in file scope as well, matching the HTML output.
# The default value is: NO.
# This tag requires that the tag GENERATE_XML is set to YES.

XML_NS_MEMB_FILE_SCOPE = NO

#---------------------------------------------------------------------------
# Configuration options related to the DOCBOOK output
#---------------------------------------------------------------------------

# If the GENERATE_DOCBOOK tag is set to YES, doxygen will generate Docbook files
# that can be used to generate PDF.
# The default value is: NO.

GENERATE_DOCBOOK       = NO

# The DOCBOOK_OUTPUT tag is used to specify where the Docbook pages will be put.
# If a relative path is entered the value of OUTPUT_DIRECTORY will be put in
# front of it.
# The default directory is: docbook.
# This tag requires that the tag GENERATE_DOCBOOK is set to YES.

DOCBOOK_OUTPUT         = docbook

# If the DOCBOOK_PROGRAMLISTING tag is set to YES, doxygen will include the
# program listings (including syntax highlighting and cross-referencing
# information) to the DOCBOOK output. Note that enabling this will significantly
# increase the size of the DOCBOOK output.
# The default value is: NO.
# This tag requires that the tag GENERATE_DOCBOOK is set to YES.

DOCBOOK_PROGRAMLISTING = NO

#---------------------------------------------------------------------------
# Configuration options for the AutoGen Definitions output
#---------------------------------------------------------------------------

# If the GENERATE_AUTOGEN_DEF tag is set to YES, doxygen will generate an
# AutoGen Definitions (see http://autogen.sourceforge.net/) file that captures
# the structure of the code including all documentation. Note that this feature
# is still experimental and incomplete at the moment.
# The default value is: NO.

GENERATE_AUTOGEN_DEF   = NO

#---------------------------------------------------------------------------
# Configuration options related to the Perl module output
#---------------------------------------------------------------------------

# If the GENERATE_PERLMOD tag is set to YES, doxygen will generate a Perl module
# file that captures the structure of the code including all documentation.
#
# Note that this feature is still experimental and incomplete at the moment.
# The default value is: NO.

GENERATE_PERLMOD       = NO

# If the PERLMOD_LATEX tag is set to YES, doxygen will generate the necessary
# Makefile rules, Perl scripts and LaTeX code to be able to generate PDF and DVI
# output from the Perl module output.
# The default value is: NO.
# This tag requires that the tag GENERATE_PERLMOD is set to YES.

PERLMOD_LATEX          = NO

# If the PERLMOD_PRETTY tag is set to YES, the Perl module output will be nicely
# formatted so it can be parsed by a human reader. This is useful if you want to
# understand what is going on. On the other hand, if this tag is set to NO, the
# size of the Perl module output will be much smaller and Perl will parse it
# just the same.
# The default value is: YES.
# This tag requires that the tag GENERATE_PERLMOD is set to YES.

PERLMOD_PRETTY         = YES

# The names of the make variables in the generated doxyrules.make file are
# prefixed with the string contained in PERLMOD_MAKEVAR_PREFIX. This is useful
# so different doxyrules.make files included by the same Makefile don't
# overwrite each other's variables.
# This tag requires that the tag GENERATE_PERLMOD is set to YES.

PERLMOD_MAKEVAR_PREFIX =

#---------------------------------------------------------------------------
# Configuration options related to the preprocessor
#---------------------------------------------------------------------------

# If the ENABLE_PREPROCESSING tag is set to YES, doxygen will evaluate all
# C-preprocessor directives found in the sources and include files.
# The default value is: YES.

ENABLE_PREPROCESSING   = YES

# If the MACRO_EXPANSION tag is set to YES, doxygen will expand all macro names
# in the source code. If set to NO, only conditional compilation will be
# performed. Macro expansion can be done in a controlled way by setting
# EXPAND_ONLY_PREDEF to YES.
# The default value is: NO.
# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.

MACRO_EXPANSION        = NO

# If the EXPAND_ONLY_PREDEF and MACRO_EXPANSION tags are both set to YES then
# the macro expansion is limited to the macros specified with the PREDEFINED and
# EXPAND_AS_DEFINED tags.
# The default value is: NO.
# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.

EXPAND_ONLY_PREDEF     = NO

# If the SEARCH_INCLUDES tag is set to YES, the include files in the
# INCLUDE_PATH will be searched if a #include is found.
# The default value is: YES.
# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.

SEARCH_INCLUDES        = YES

# The INCLUDE_PATH tag can be used to specify one or more directories that
# contain include files that are not input files but should be processed by the
# preprocessor.
# This tag requires that the tag SEARCH_INCLUDES is set to YES.

INCLUDE_PATH           =

# You can use the INCLUDE_FILE_PATTERNS tag to specify one or more wildcard
# patterns (like *.h and *.hpp) to filter out the header-files in the
# directories. If left blank, the patterns specified with FILE_PATTERNS will be
# used.
# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.

INCLUDE_FILE_PATTERNS  =

# The PREDEFINED tag can be used to specify one or more macro names that are
# defined before the preprocessor is started (similar to the -D option of e.g.
# gcc). The argument of the tag is a list of macros of the form: name or
# name=definition (no spaces). If the definition and the "=" are omitted, "=1"
# is assumed. To prevent a macro definition from being undefined via #undef or
# recursively expanded use the := operator instead of the = operator.
# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.

PREDEFINED             = __CUDACC__

# If the MACRO_EXPANSION and EXPAND_ONLY_PREDEF tags are set to YES then this
# tag can be used to specify a list of macro names that should be expanded. The
# macro definition that is found in the sources will be used. Use the PREDEFINED
# tag if you want to use a different macro definition that overrules the
# definition found in the source code.
# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.

EXPAND_AS_DEFINED      =

# If the SKIP_FUNCTION_MACROS tag is set to YES then doxygen's preprocessor will
# remove all references to function-like macros that are alone on a line, have
# an all uppercase name, and do not end with a semicolon. Such function macros
# are typically used for boiler-plate code, and will confuse the parser if not
# removed.
# The default value is: YES.
# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.

SKIP_FUNCTION_MACROS   = YES

#---------------------------------------------------------------------------
# Configuration options related to external references
#---------------------------------------------------------------------------

# The TAGFILES tag can be used to specify one or more tag files. For each tag
# file the location of the external documentation should be added. The format of
# a tag file without this location is as follows:
# TAGFILES = file1 file2 ...
# Adding location for the tag files is done as follows:
# TAGFILES = file1=loc1 "file2 = loc2" ...
# where loc1 and loc2 can be relative or absolute paths or URLs. See the
# section "Linking to external documentation" for more information about the use
# of tag files.
# Note: Each tag file must have a unique name (where the name does NOT include
# the path). If a tag file is not located in the directory in which doxygen is
# run, you must also specify the path to the tagfile here.

TAGFILES               =

# When a file name is specified after GENERATE_TAGFILE, doxygen will create a
# tag file that is based on the input files it reads. See section "Linking to
# external documentation" for more information about the usage of tag files.

GENERATE_TAGFILE       =

# If the ALLEXTERNALS tag is set to YES, all external class will be listed in
# the class index. If set to NO, only the inherited external classes will be
# listed.
# The default value is: NO.

ALLEXTERNALS           = NO

# If the EXTERNAL_GROUPS tag is set to YES, all external groups will be listed
# in the modules index. If set to NO, only the current project's groups will be
# listed.
# The default value is: YES.

EXTERNAL_GROUPS        = YES

# If the EXTERNAL_PAGES tag is set to YES, all external pages will be listed in
# the related pages index. If set to NO, only the current project's pages will
# be listed.
# The default value is: YES.

EXTERNAL_PAGES         = YES

#---------------------------------------------------------------------------
# Configuration options related to the dot tool
#---------------------------------------------------------------------------

# If the CLASS_DIAGRAMS tag is set to YES, doxygen will generate a class diagram
# (in HTML and LaTeX) for classes with base or super classes. Setting the tag to
# NO turns the diagrams off. Note that this option also works with HAVE_DOT
# disabled, but it is recommended to install and use dot, since it yields more
# powerful graphs.
# The default value is: YES.

CLASS_DIAGRAMS         = YES

# You can include diagrams made with dia in doxygen documentation. Doxygen will
# then run dia to produce the diagram and insert it in the documentation. The
# DIA_PATH tag allows you to specify the directory where the dia binary resides.
# If left empty dia is assumed to be found in the default search path.

DIA_PATH               =

# If set to YES the inheritance and collaboration graphs will hide inheritance
# and usage relations if the target is undocumented or is not a class.
# The default value is: YES.

HIDE_UNDOC_RELATIONS   = YES

# If you set the HAVE_DOT tag to YES then doxygen will assume the dot tool is
# available from the path. This tool is part of Graphviz (see:
# http://www.graphviz.org/), a graph visualization toolkit from AT&T and Lucent
# Bell Labs. The other options in this section have no effect if this option is
# set to NO
# The default value is: YES.

HAVE_DOT               = YES

# The DOT_NUM_THREADS specifies the number of dot invocations doxygen is allowed
# to run in parallel. When set to 0 doxygen will base this on the number of
# processors available in the system. You can set it explicitly to a value
# larger than 0 to get control over the balance between CPU load and processing
# speed.
# Minimum value: 0, maximum value: 32, default value: 0.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_NUM_THREADS        = 0

# When you want a differently looking font in the dot files that doxygen
# generates you can specify the font name using DOT_FONTNAME. You need to make
# sure dot is able to find the font, which can be done by putting it in a
# standard location or by setting the DOTFONTPATH environment variable or by
# setting DOT_FONTPATH to the directory containing the font.
# The default value is: Helvetica.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_FONTNAME           = Helvetica

# The DOT_FONTSIZE tag can be used to set the size (in points) of the font of
# dot graphs.
# Minimum value: 4, maximum value: 24, default value: 10.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_FONTSIZE           = 10

# By default doxygen will tell dot to use the default font as specified with
# DOT_FONTNAME. If you specify a different font using DOT_FONTNAME you can set
# the path where dot can find it using this tag.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_FONTPATH           =

# If the CLASS_GRAPH tag is set to YES then doxygen will generate a graph for
# each documented class showing the direct and indirect inheritance relations.
# Setting this tag to YES will force the CLASS_DIAGRAMS tag to NO.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

CLASS_GRAPH            = YES

# If the COLLABORATION_GRAPH tag is set to YES then doxygen will generate a
# graph for each documented class showing the direct and indirect implementation
# dependencies (inheritance, containment, and class references variables) of the
# class with other documented classes.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

COLLABORATION_GRAPH    = YES

# If the GROUP_GRAPHS tag is set to YES then doxygen will generate a graph for
# groups, showing the direct groups dependencies.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

GROUP_GRAPHS           = YES

# If the UML_LOOK tag is set to YES, doxygen will generate inheritance and
# collaboration diagrams in a style similar to the OMG's Unified Modeling
# Language.
# The default value is: NO.
# This tag requires that the tag HAVE_DOT is set to YES.

UML_LOOK               = NO

# If the UML_LOOK tag is enabled, the fields and methods are shown inside the
# class node. If there are many fields or methods and many nodes the graph may
# become too big to be useful. The UML_LIMIT_NUM_FIELDS threshold limits the
# number of items for each type to make the size more manageable. Set this to 0
# for no limit. Note that the threshold may be exceeded by 50% before the limit
# is enforced. So when you set the threshold to 10, up to 15 fields may appear,
# but if the number exceeds 15, the total amount of fields shown is limited to
# 10.
# Minimum value: 0, maximum value: 100, default value: 10.
# This tag requires that the tag HAVE_DOT is set to YES.

UML_LIMIT_NUM_FIELDS   = 10

# If the TEMPLATE_RELATIONS tag is set to YES then the inheritance and
# collaboration graphs will show the relations between templates and their
# instances.
# The default value is: NO.
# This tag requires that the tag HAVE_DOT is set to YES.

TEMPLATE_RELATIONS     = NO

# If the INCLUDE_GRAPH, ENABLE_PREPROCESSING and SEARCH_INCLUDES tags are set to
# YES then doxygen will generate a graph for each documented file showing the
# direct and indirect include dependencies of the file with other documented
# files.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

INCLUDE_GRAPH          = YES

# If the INCLUDED_BY_GRAPH, ENABLE_PREPROCESSING and SEARCH_INCLUDES tags are
# set to YES then doxygen will generate a graph for each documented file showing
# the direct and indirect include dependencies of the file with other documented
# files.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

INCLUDED_BY_GRAPH      = YES

# If the CALL_GRAPH tag is set to YES then doxygen will generate a call
# dependency graph for every global function or class method.
#
# Note that enabling this option will significantly increase the time of a run.
# So in most cases it will be better to enable call graphs for selected
# functions only using the \callgraph command. Disabling a call graph can be
# accomplished by means of the command \hidecallgraph.
# The default value is: NO.
# This tag requires that the tag HAVE_DOT is set to YES.

CALL_GRAPH             = NO

# If the CALLER_GRAPH tag is set to YES then doxygen will generate a caller
# dependency graph for every global function or class method.
#
# Note that enabling this option will significantly increase the time of a run.
# So in most cases it will be better to enable caller graphs for selected
# functions only using the \callergraph command. Disabling a caller graph can be
# accomplished by means of the command \hidecallergraph.
# The default value is: NO.
# This tag requires that the tag HAVE_DOT is set to YES.

CALLER_GRAPH           = NO

# If the GRAPHICAL_HIERARCHY tag is set to YES then doxygen will graphical
# hierarchy of all classes instead of a textual one.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

GRAPHICAL_HIERARCHY    = YES

# If the DIRECTORY_GRAPH tag is set to YES then doxygen will show the
# dependencies a directory has on other directories in a graphical way. The
# dependency relations are determined by the #include relations between the
# files in the directories.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

DIRECTORY_GRAPH        = YES

# The DOT_IMAGE_FORMAT tag can be used to set the image format of the images
# generated by dot. For an explanation of the image formats see the section
# output formats in the documentation of the dot tool (Graphviz (see:
# http://www.graphviz.org/)).
# Note: If you choose svg you need to set HTML_FILE_EXTENSION to xhtml in order
# to make the SVG files visible in IE 9+ (other browsers do not have this
# requirement).
# Possible values are: png, png:cairo, png:cairo:cairo, png:cairo:gd, png:gd,
# png:gd:gd, jpg, jpg:cairo, jpg:cairo:gd, jpg:gd, jpg:gd:gd, gif, gif:cairo,
# gif:cairo:gd, gif:gd, gif:gd:gd, svg, png:gd, png:gd:gd, png:cairo,
# png:cairo:gd, png:cairo:cairo, png:cairo:gdiplus, png:gdiplus and
# png:gdiplus:gdiplus.
# The default value is: png.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_IMAGE_FORMAT       = png

# If DOT_IMAGE_FORMAT is set to svg, then this option can be set to YES to
# enable generation of interactive SVG images that allow zooming and panning.
#
# Note that this requires a modern browser other than Internet Explorer. Tested
# and working are Firefox, Chrome, Safari, and Opera.
# Note: For IE 9+ you need to set HTML_FILE_EXTENSION to xhtml in order to make
# the SVG files visible. Older versions of IE do not have SVG support.
# The default value is: NO.
# This tag requires that the tag HAVE_DOT is set to YES.

INTERACTIVE_SVG        = NO

# The DOT_PATH tag can be used to specify the path where the dot tool can be
# found. If left blank, it is assumed the dot tool can be found in the path.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_PATH               =

# The DOTFILE_DIRS tag can be used to specify one or more directories that
# contain dot files that are included in the documentation (see the \dotfile
# command).
# This tag requires that the tag HAVE_DOT is set to YES.

DOTFILE_DIRS           =

# The MSCFILE_DIRS tag can be used to specify one or more directories that
# contain msc files that are included in the documentation (see the \mscfile
# command).

MSCFILE_DIRS           =

# The DIAFILE_DIRS tag can be used to specify one or more directories that
# contain dia files that are included in the documentation (see the \diafile
# command).

DIAFILE_DIRS           =

# When using plantuml, the PLANTUML_JAR_PATH tag should be used to specify the
# path where java can find the plantuml.jar file. If left blank, it is assumed
# PlantUML is not used or called during a preprocessing step. Doxygen will
# generate a warning when it encounters a \startuml command in this case and
# will not generate output for the diagram.

PLANTUML_JAR_PATH      =

# When using plantuml, the PLANTUML_CFG_FILE tag can be used to specify a
# configuration file for plantuml.

PLANTUML_CFG_FILE      =

# When using plantuml, the specified paths are searched for files specified by
# the !include statement in a plantuml block.

PLANTUML_INCLUDE_PATH  =

# The DOT_GRAPH_MAX_NODES tag can be used to set the maximum number of nodes
# that will be shown in the graph. If the number of nodes in a graph becomes
# larger than this value, doxygen will truncate the graph, which is visualized
# by representing a node as a red box. Note that doxygen if the number of direct
# children of the root node in a graph is already larger than
# DOT_GRAPH_MAX_NODES then the graph will not be shown at all. Also note that
# the size of a graph can be further restricted by MAX_DOT_GRAPH_DEPTH.
# Minimum value: 0, maximum value: 10000, default value: 50.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_GRAPH_MAX_NODES    = 50

# The MAX_DOT_GRAPH_DEPTH tag can be used to set the maximum depth of the graphs
# generated by dot. A depth value of 3 means that only nodes reachable from the
# root by following a path via at most 3 edges will be shown. Nodes that lay
# further from the root node will be omitted. Note that setting this option to 1
# or 2 may greatly reduce the computation time needed for large code bases. Also
# note that the size of a graph can be further restricted by
# DOT_GRAPH_MAX_NODES. Using a depth of 0 means no depth restriction.
# Minimum value: 0, maximum value: 1000, default value: 0.
# This tag requires that the tag HAVE_DOT is set to YES.

MAX_DOT_GRAPH_DEPTH    = 0

# Set the DOT_TRANSPARENT tag to YES to generate images with a transparent
# background. This is disabled by default, because dot on Windows does not seem
# to support this out of the box.
#
# Warning: Depending on the platform used, enabling this option may lead to
# badly anti-aliased labels on the edges of a graph (i.e. they become hard to
# read).
# The default value is: NO.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_TRANSPARENT        = NO

# Set the DOT_MULTI_TARGETS tag to YES to allow dot to generate multiple output
# files in one run (i.e. multiple -o and -T options on the command line). This
# makes dot run faster, but since only newer versions of dot (>1.8.10) support
# this, this feature is disabled by default.
# The default value is: NO.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_MULTI_TARGETS      = NO

# If the GENERATE_LEGEND tag is set to YES doxygen will generate a legend page
# explaining the meaning of the various boxes and arrows in the dot generated
# graphs.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

GENERATE_LEGEND        = YES

# If the DOT_CLEANUP tag is set to YES, doxygen will remove the intermediate dot
# files that are used to generate the various graphs.
# The default value is: YES.
# This tag requires that the tag HAVE_DOT is set to YES.

DOT_CLEANUP            = YES
```

`docs/env.md`:

```md
# Environment variables:  

- `ARK_ROOT`: The installation directory of ARK. Defaults to `/usr/local/ark` when unset.  

```  
export ARK_ROOT=/usr/local/ark  
```  

- `ARK_SCHEDULER`: The scheduler used by ARK. The available schedulers are `Default` and `Simple`. Defaults to `Default` when unset. `Simple` is a simple scheduler that is used for debugging.

```  
export ARK_SCHEDULER=Default  
```  

- `ARK_LOG_LEVEL`: The log level of ARK. The available log levels are `DEBUG`, `INFO`, `WARN`, and `ERROR`. Defaults to `INFO` when unset.

```
export ARK_LOG_LEVEL=DEBUG
```

- `ARK_IPC_LISTEN_PORT_BASE`: The base port number for IPC communication. Defaults to `42000` when unset. If we start multiple ARK processes on the same machine, different processes should use different port numbers. For example, if the base port is set to `42000`, the first process will use `42000`, the second process will use `42001`. Note that if the port number is already in use, ARK will fail to start.

```
export ARK_IPC_LISTEN_PORT_BASE=42000
```

```

`docs/imgs/GPU-driven_System_Architecture.svg`:

```svg
<svg width="3953" height="867" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" overflow="hidden"><g transform="translate(-205 -613)"><rect x="222.5" y="639.5" width="3933" height="837" stroke="#FFFFFF" stroke-width="4.58333" stroke-miterlimit="8" fill="#FFFFFF"/><rect x="1781.5" y="761.5" width="2356" height="703" stroke="#404040" stroke-width="4.58333" stroke-miterlimit="8" fill="#FFFFFF"/><text fill="#4472C4" font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="700" font-size="83" transform="matrix(1 0 0 1 2670.52 838)">GPU Loop Kernel</text><rect x="1820.5" y="1328.5" width="2269" height="105" stroke="#404040" stroke-width="4.58333" stroke-miterlimit="8" fill="#FFFFFF"/><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="83" transform="matrix(1 0 0 1 2727.31 1408)">NVIDIA CUDA</text><rect x="1820.5" y="1040.5" width="1117" height="257" stroke="#404040" stroke-width="4.58333" stroke-miterlimit="8" fill="#FFFFFF"/><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="83" transform="matrix(1 0 0 1 2083.95 1152)">Computation OPs</text><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="73" transform="matrix(1 0 0 1 2179.33 1243)">(</text><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="73" transform="matrix(1 0 0 1 2201.67 1243)">GeMM</text><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="73" transform="matrix(1 0 0 1 2409.64 1243)">, etc.)</text><rect x="2963.5" y="1182.5" width="1126" height="115" stroke="#404040" stroke-width="4.58333" stroke-miterlimit="8" fill="#FFFFFF"/><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="83" transform="matrix(1 0 0 1 3012.94 1267)">GPU</text><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="83" transform="matrix(1 0 0 1 3160.18 1267)">-</text><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="83" transform="matrix(1 0 0 1 3185.39 1267)">controlled DMA Interface</text><rect x="3539.5" y="1041.5" width="550" height="114" stroke="#404040" stroke-width="4.58333" stroke-miterlimit="8" fill="#FFFFFF"/><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="83" transform="matrix(1 0 0 1 3576.62 1126)">Collective OPs</text><rect x="2962.5" y="1041.5" width="551" height="114" stroke="#404040" stroke-width="4.58333" stroke-miterlimit="8" fill="#FFFFFF"/><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="83" transform="matrix(1 0 0 1 2985.68 1126)">Send/</text><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="83" transform="matrix(1 0 0 1 3183.91 1126)">Recv</text><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="83" transform="matrix(1 0 0 1 3360.94 1126)">OPs</text><rect x="1820.5" y="906.5" width="2269" height="106" stroke="#404040" stroke-width="4.58333" stroke-miterlimit="8" fill="#FFFFFF"/><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="83" transform="matrix(1 0 0 1 2793.19 987)">DL Model</text><rect x="229" y="1042" width="567" height="89" fill="#FFFFFF"/><text fill="#4472C4" font-family="Arial,Arial_MSFontService,sans-serif" font-weight="700" font-size="73" transform="matrix(1 0 0 1 249.471 1113)">Runtime Executor</text><rect x="233" y="1133" width="1408" height="309" fill="#8FAADC"/><rect x="245.5" y="1319.5" width="657" height="105" stroke="#404040" stroke-width="4.58333" stroke-miterlimit="8" fill="#FFFFFF"/><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="73" transform="matrix(1 0 0 1 444.287 1397)">GPU HW</text><rect x="245.5" y="1145.5" width="1377" height="105" stroke="#404040" stroke-width="4.58333" stroke-miterlimit="8" fill="#FFFFFF"/><text fill="#4472C4" font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="700" font-size="73" transform="matrix(1 0 0 1 676.589 1222)">GPU Loop Kernel</text><rect x="245.5" y="912.5" width="1377" height="105" stroke="#404040" stroke-width="4.58333" stroke-miterlimit="8" fill="#FFFFFF"/><text fill="#4472C4" font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="700" font-size="73" transform="matrix(1 0 0 1 521.904 989)">Offline Operator Scheduler</text><rect x="245.5" y="769.5" width="1376" height="105" stroke="#404040" stroke-width="4.58333" stroke-miterlimit="8" fill="#FFFFFF"/><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="73" transform="matrix(1 0 0 1 655.353 846)">Operational Graph</text><rect x="965.5" y="1319.5" width="657" height="105" stroke="#404040" stroke-width="4.58333" stroke-miterlimit="8" fill="#FFFFFF"/><text font-family="Calibri,Calibri_MSFontService,sans-serif" font-weight="400" font-size="73" transform="matrix(1 0 0 1 1240.9 1396)">NIC</text><path d="M5.86473 17.1417 6.20893 60.7205-5.24905 60.811-5.59324 17.2322ZM-17.006 23.0517 0 0 17.368 22.7802ZM17.6217 54.9011 0.615682 77.9528-16.7523 55.1726Z" transform="matrix(-1 0 0 1 581.116 1243.5)"/><path d="M5.86473 17.1417 6.20893 60.7205-5.24905 60.811-5.59324 17.2322ZM-17.006 23.0517 0 0 17.368 22.7802ZM17.6217 54.9011 0.615682 77.9528-16.7523 55.1726Z" transform="matrix(-1 0 0 1 1294.12 1243.5)"/><path d="M9.16619-0.0939455 9.50003 32.4791-8.83234 32.667-9.16619 0.0939455ZM27.7385 23.125 0.615682 60.0716-27.2587 23.6887Z" transform="matrix(-1 0 0 1 931.616 863)"/><path d="M9.16658-0.0394904 9.66379 115.374-8.66937 115.453-9.16658 0.0394904ZM27.9575 106.129 0.615682 142.913-27.042 106.366Z" transform="matrix(-1 0 0 1 931.616 1008)"/><rect x="266" y="638" width="1327" height="100" fill="#FFFFFF"/><text font-family="Arial,Arial_MSFontService,sans-serif" font-weight="700" font-size="83" transform="matrix(1 0 0 1 289.896 718)">GPU</text><text font-family="Arial,Arial_MSFontService,sans-serif" font-weight="700" font-size="83" transform="matrix(1 0 0 1 468.646 718)">-</text><text font-family="Arial,Arial_MSFontService,sans-serif" font-weight="700" font-size="83" transform="matrix(1 0 0 1 496.146 718)">driven System Architecture</text><path d="M0 0 159.265 383.572" stroke="#404040" stroke-width="5.72917" stroke-miterlimit="8" stroke-dasharray="22.9167 17.1875" fill="#FFFFFF" fill-rule="evenodd" transform="matrix(1 0 0 -1 1621 1151.57)"/><path d="M1621 1250 1771.84 1456.77" stroke="#404040" stroke-width="5.72917" stroke-miterlimit="8" stroke-dasharray="22.9167 17.1875" fill="#FFFFFF" fill-rule="evenodd"/></g></svg>
```

`docs/imgs/logos.svg`:

```svg
<svg width="1937" height="271" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" xml:space="preserve" overflow="hidden"><defs><image width="751" height="215" xlink:href="data:image/png;base64,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" preserveAspectRatio="none" id="img0"></image><clipPath id="clip1"><rect x="0" y="0" width="2071275" height="592975"/></clipPath><clipPath id="clip2"><rect x="-2770.45" y="-0.454545" width="2798618" height="595745"/></clipPath><image width="2008" height="900" xlink:href="data:image/png;base64,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" preserveAspectRatio="none" id="img3"></image><clipPath id="clip4"><path d="M557154 561866 3350230 561866 3350230 1156349 557154 1156349Z" fill-rule="evenodd" clip-rule="evenodd"/></clipPath></defs><g transform="translate(-1317 -1776)"><rect x="1319.5" y="1778.5" width="1932" height="266" stroke="#FFFFFF" stroke-width="4.58333" stroke-miterlimit="8" fill="#FFFFFF"/><g transform="matrix(0.000360892 0 0 0.000360892 1343 1811)"><g clip-path="url(#clip1)" transform="matrix(1.00467 0 0 1 -0.0495605 -0.25597)"><use width="100%" height="100%" xlink:href="#img0" transform="scale(2758.02 2758.02)"></use></g></g><g clip-path="url(#clip2)" transform="matrix(0.000360892 0 0 0.000360892 2239 1798)"><g clip-path="url(#clip4)" transform="matrix(1 0 0 1.00212 -557154 -563059)"><use width="100%" height="100%" xlink:href="#img3" transform="scale(1925.65 1925.65)"></use></g></g></g></svg>
```

`docs/install.md`:

```md
# ARK Install Instructions

## Preliminaries

* Linux kernel >= 4.15.0

    - If you have a lower version, you can upgrade it via:
        ```
        apt-get update
        apt-get install -y linux-image-4.15.0-13-generic linux-header-4.15.0-13-generic
        ```

* CMake >= 3.25.0 and Python >= 3.7

* GPUs with CUDA compute capability >= 7.0 and CUDA version >= 11.1

* To run ARK in a Docker container, we need to mount `/dev` and `/lib/modules` into the container so that the container can use `gpumem` driver. Add the following options in the `docker run` command:
    ```
    -v /dev:/dev -v /lib/modules:/lib/modules
    ```

## Install `gpudma`

*NOTE: if you are using a Docker container, the following steps should be done on the host.*

1. Compile `gpudma`.

    ```
    cd third_party
    make gpudma
    ```
    - This may fail if you don't have a proper `gcc` version, which will be notified by an error message. In that case, [install an alternative version of `gcc`](https://github.com/chhwang/devel-note/wiki/Building-GCC-from-source).

2. Load `gpumem` driver.

    ```
    insmod third_party/gpudma/module/gpumem.ko
    chmod 666 /dev/gpumem
    ```

3. Check if the `gpumem` driver is running.

    ```
    lsmod | grep gpumem
    ```

## Install ARK Python

1. Go to the repo root directory and install Python dependencies.

    ```
    python3 -m pip install -r requirements.txt
    ```

2. Install ARK Python.

    ```
    python3 -m pip install .
    ```

3. (Optional) Run the tutorial code to verify the installation.

    ```
    cd examples/tutorial
    python3 tutorial.py
    ```

## (Optional) Install ARK C++ and Run Unit Tests

If you want to use only the core C++ interfaces, follow the instructions below.

1. Go to the repo root directory and configure CMake. Replace `CMAKE_INSTALL_PREFIX` with your desired installation directory.

    ```
    mkdir build
    cd build
    cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/usr/local ..
    ```

2. Build ARK.

    ```
    make -j build
    ```

3. (Optional) We offer CTest unit tests for ARK C++. To build the tests, run:

    ```
    make -j ut
    ```

    **NOTE:** currently unit tests require at least 4 GPUs in the system for communication tests.

    Lock GPU clock frequency for stable test results:

    ```
    nvidia-smi -pm 1
    for i in $(seq 0 $(( $(nvidia-smi -L | wc -l) - 1 ))); do
        nvidia-smi -ac $(nvidia-smi --query-gpu=clocks.max.memory,clocks.max.sm --format=csv,noheader,nounits -i $i | sed 's/\ //') -i $i
    done
    ```

    Run the tests:

    ```
    ARK_ROOT=$PWD ctest --output-on-failure
    ```

    **NOTE:** unit tests may take tens of minutes to finish.

4. Install ARK C++.

    ```
    sudo make install
    ```

```

`docs/quickstart.md`:

```md
# A Quick Guide to Using ARK with Python for DNN Applications

Welcome to this tutorial on using ARK to run a simple deep neural network (DNN) application in Python. We will walk you through a basic Python example to illustrate the process.

After completing the [installation](./install.md), you can run the tutorial example at [tutorial.py](../examples/tutorial/quickstart_tutorial.py) to see how ARK works.

```bash
python examples/tutorial/quickstart_tutorial.py
```

There are environment variables available to configure ARK. For more details about these variables, please refer to [Environment Variables](./env.md).

Before diving in, let's import the required modules and initialize ARK runtime:

```python
import ark
import numpy as np

# Initialize the ARK runtime
runtime = ark.Runtime()

```
First, we need to create the operational graph for our DNN model. In this example, we define a simple model with two input tensors. The output tensor is the sum of these input tensors.

```python
M, N = 64, 64
# Create an input tensor
input_tensor = ark.tensor([M, N])
# Create another tensor
other_tensor = ark.tensor([M, N])

# Add the two tensors
output_tensor = ark.add(input_tensor, input_tensor)
```

Next, we need to launch the ARK runtime and initialize the input and output tensors. You can copy a numpy array into a tensor on GPU using `tensor.from_numpy(ndarray)`. By calling `runtime.launch()`, the ARK runtime will be launched. It will freeze the model and allocate GPU memory. Then it will schedule the model, generate and compile the CUDA kernel for the model. Therefore, it is necessary to call `runtime.launch()` before copying the tensor between the host and device. It is not allowed to modify the model after launching the ARK runtime.


```python
# Launch the ARK runtime
runtime.launch()

# Initialize the input and other tensor with random values
input_tensor_host = np.random.rand(M, N).astype(np.float32)
input_tensor.from_numpy(input_tensor_host)
other_tensor_host = np.random.rand(M, N).astype(np.float32)
other_tensor.from_numpy(other_tensor_host)
```

Next, you can run the ARK runtime using runtime.run(). This will launch the CUDA kernel and wait for the kernel to finish.

```python
# Run the ARK program
runtime.run()
```

Lastly, copy the output tensor back to the host and verify the result.

```python
# Copy the output tensor from device memory to host memory, if dst is 
# None, a new numpy array of the same shape as the src tensor will be returned
output_tensor_host = output_tensor.to_numpy()
# Check if the output tensor is equal to the sum of the input and other tensor
np.testing.assert_allclose(
    output_tensor_host, input_tensor_host + other_tensor_host
)
```

Congratulations! You have successfully learned how to run a DNN model over ARK. Happy coding!

For more tutorials, please refer to [Tutorials](./tutorial/).

```

`docs/sphinx/Makefile`:

```
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

SPHINXOPTS    ?=
SPHINXBUILD   ?= sphinx-build
SOURCEDIR     = source
BUILDDIR      = build

# Put it first so that "make" without argument is like "make help".
help:
	@$(SPHINXBUILD) -M help "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)

.PHONY: help Makefile

# Catch-all target: route all unknown targets to Sphinx using the new
# "make mode" option.  $(O) is meant as a shortcut for $(SPHINXOPTS).
%: Makefile
	@$(SPHINXBUILD) -M $@ "$(SOURCEDIR)" "$(BUILDDIR)" $(SPHINXOPTS) $(O)

```

`docs/sphinx/requirements.txt`:

```txt
sphinx
sphinx-book-theme
sphinx-prompt
rinohtype
myst-parser

```

`docs/sphinx/source/api.rst`:

```rst
******************
API Document
******************

``ark``
=====================

.. automodule:: ark
    :members: init, srand, rand, NO_DIM, DIMS_LEN


``ark.Dims``
=====================

.. automodule:: ark.Dims
    :members: size, ndims, __getitem__, __setitem__, __repr__

``ark.Model``
=====================

.. automodule:: ark.Model
    :members: tensor, reshape, identity, sharding, reduce, layernorm, softmax,
            transpose, linear, im2col, conv2d, max_pool, scale, relu, gelu, add, mul, send,
            send_done, recv, send_mm, recv_mm, all_reduce

``ark.Executor``
=====================

.. automodule:: ark.Executor
    :members: compile, launch, run, wait, stop, get_tensor, tensor_memcpy_host_to_device,
            tensor_memcpy_device_to_host, tensor_clear


```

`docs/sphinx/source/conf.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

# Configuration file for the Sphinx documentation builder.
#
# This file only contains a selection of the most common options. For a full
# list see the documentation:
# https://www.sphinx-doc.org/en/master/usage/configuration.html

# -- Path setup --------------------------------------------------------------

# If extensions (or modules to document with autodoc) are in another directory,
# add these directories to sys.path here. If the directory is relative to the
# documentation root, use os.path.abspath to make it absolute, like shown here.
#
import ark

# -- Project information -----------------------------------------------------

project = "ARK"
copyright = "2023, ARK Team"
author = "ARK Team"
version = "0.1.0"
release = "0.1.0"

# -- General configuration ---------------------------------------------------

# Add any Sphinx extension module names here, as strings. They can be
# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom
# ones.
extensions = [
    "myst_parser",
    "rinoh.frontend.sphinx",
    "sphinx.ext.todo",
    "sphinx.ext.viewcode",
    "sphinx.ext.autodoc",
    "sphinx.ext.napoleon",
    "sphinx-prompt",
]

# Add any paths that contain templates here, relative to this directory.
templates_path = ["_templates"]

# List of patterns, relative to source directory, that match files and
# directories to ignore when looking for source files.
# This pattern also affects html_static_path and html_extra_path.
exclude_patterns = ["_build", "Thumbs.db", ".DS_Store"]


# -- Options for HTML output -------------------------------------------------

# The theme to use for HTML and HTML Help pages.  See the documentation for
# a list of builtin themes.
#
html_theme = "sphinx_book_theme"

# Add any paths that contain custom static files (such as style sheets) here,
# relative to this directory. They are copied after the builtin static files,
# so a file named "default.css" will overwrite the builtin "default.css".
html_static_path = ["_static"]

myst_enable_extensions = ["deflist"]

# -- Options for PDF generation with Rinohtype -------------------------------

latex_elements = {
    "papersize": "letterpaper",
    "pointsize": "10pt",
    "preamble": "",
    "figure_align": "htbp",
}

```

`docs/sphinx/source/index.rst`:

```rst
.. Copyright (c) Microsoft Corporation.
   Licensed under the MIT license.

Welcome to ARK's documentation!
=================================

This document explains the usage of the ARK system.

.. toctree::
   :caption: ARK Guides

   api


.. toctree::

   ARK GitHub Repo <https://github.com/microsoft/ark>

```

`docs/tutorial/module_tutorial.md`:

```md
# Module Tutorial
This tutorial provides an overview of how to use the module and its features. The module is similar to the pytorch module API and provides a set of functionalities such as model save and load.

## Usage
To use the module, you need to create a class that inherits from the `ark.Module` class. You can then define the `forward` and `backward` functions of the class. The parameters and submodules of the module is defined in the `__init__` function.

```python

# Define the parameters of the model
batch_size = 1
seq_len = 64
d_model = 512
d_ff = 2048

class SubModuleARK(ark.Module):
    def __init__(self):
        super(SubModuleARK, self).__init__()
        # Define the parameters of the submodule
        self.weight_2 = ark.Parameter(ark.tensor([d_ff, d_model], ark.FP16))

    def forward(self, inputs):
        # Perform the forward pass of the submodule
        middle_result1 = ark.matmul(inputs, self.weight_2)
        return middle_result1


class TestModelARK(ark.Module):
    def __init__(self):
        super(TestModelARK, self).__init__()
        # Define the parameters of the module
        self.weight_1 = ark.Parameter(ark.tensor([d_model, d_ff], ark.FP16))
        # Create a submodule of the module
        self.submodule = SubModuleARK()

    def forward(self, inputs):
        # Perform the forward pass of the model
        middle_result = ark.matmul(inputs, self.weight_1, is_relu=True)
        middle_result1 = self.submodule(middle_result)
        output = ark.add(middle_result1, inputs)
        output_layernorm = ark.layernorm(output)
        return output_layernorm
```

Here, we can create this model and then launch it.

```python
# Initialize the ARK runtime
runtime = ark.Runtime()
# Create an input tensor
input_tensor = ark.tensor([batch_size, seq_len, d_model], ark.FP16)

# Create an ARK module
ark_model = TestModelARK()

# Perform the forward pass
output_tensor = ark_model(input_tensor)

# Launch the ARK runtime
runtime.launch()
```

The initialization of the model can be done using a state_dict. Note that the parameters of this model in the state_dict must have the same name as the parameters defined in the module. Then, we can use `load_state_dict` to import the parameters of this model.

```python
# Initialize the input tensor
input_tensor_host = (
    (np.random.rand(batch_size, seq_len, d_model) - 0.5) * 0.1
).astype(np.float16)
input_tensor.from_numpy(input_tensor_host)
    
# Initialize the parameters of the ARK module using numpy state_dict
weight_1_host = ((np.random.rand(d_model, d_ff) - 0.5) * 0.1).astype(
    np.float16
)
weight_2_host = ((np.random.rand(d_ff, d_model) - 0.5) * 0.1).astype(
    np.float16
)
state_dict = {
    "weight_1": weight_1_host,
    "submodule.weight_2": weight_2_host,
}

# Load model parameters
ark_model.load_state_dict(state_dict)
```

If needed, we can save this state_dict using `save`. We provide a set of modules for saving and loading this model's parameters using Python's `pickle` library.

```python
ark.save(ark_model.state_dict(), "test_model.pt")
ark.load("test_model.pt")
```


Then we can run the model and get the output.

```python
# Run the ARK model
runtime.run()

# Copy the ARK module output tensor from device to host
output_tensor_host = output_tensor.to_numpy()
```

ARK's module is similar to PyTorch's module. Here we can use a similar pytorch module to compare their results.

```python
# Use pytorch to define the same model
class SubModulePytorch(nn.Module):
    def __init__(self):
        super(SubModulePytorch, self).__init__()
        self.weight_2 = nn.Parameter(torch.FloatTensor(d_ff, d_model))

    def forward(self, inputs):
        middle_result1 = torch.matmul(inputs, self.weight_2)
        return middle_result1


class TestModelPytorch(nn.Module):
    def __init__(self):
        super(TestModelPytorch, self).__init__()
        # Define the parameters of the module
        self.weight_1 = nn.Parameter(torch.FloatTensor(d_model, d_ff))
        # Create a submodule of the module
        self.submodule = SubModulePytorch()

    def forward(self, inputs):
        # Perform the forward pass of the model
        output = torch.matmul(inputs, self.weight_1)
        output = nn.ReLU()(output)
        output = self.submodule(output)
        output = nn.LayerNorm(d_model)(output + inputs)
        return output
```

We can then create this model and run it.

```python
 # For simplicity, we use float32 to compute the ground truth using pytorch
input_tensor_host_float32 = input_tensor_host.astype(np.float32)
torch_input = torch.from_numpy(input_tensor_host_float32)

torch_model = TestModelPytorch()
```

We can also convert ARK's state_dict into a PyTorch state_dict. This way, we can directly import the parameters of this model into the corresponding PyTorch model.

ARK state_dict's format is
```
{
    "weight_1": weight_1_numpy,
    "submodule.weight_2": weight_2_numpy,
}
```
`weight_1_numpy` and `weight_2_numpy` are `numpy.ndarray` type. PyTorch state_dict's format is
```
{
    "weight_1": weight_1_torch,
    "submodule.weight_2": weight_2_torch,
}
```
`weight_1_torch` and `weight_2_torch` are `torch.Tensor` type. We need to convert the `numpy.ndarray` type state_dict to `torch.Tensor` type state_dict using `ark.convert_state_dict`.


```python
    # Convert the numpy.ndarray type state_dict to torch.Tensor type state_dict using       
    #  ark.convert_state_dict
    torch_state_dict = ark.convert_state_dict(state_dict, "torch")
    # Load model parameters
    torch_model.load_state_dict(torch_state_dict)
```

Then we can run the model and compare the results.

```python
    # Run the pytorch model to compute the ground truth
    gt = torch_model(torch_input).detach().numpy().astype(np.float16)

    # Test if the result is correct
    max_error = np.max(np.abs(output_tensor_host - gt))
    avg_error = np.mean(np.abs(output_tensor_host - gt))

    # Use ark_model.state_dict() to get the state_dict of the ARK module
    # Note that the state_dict of the ARK module might be modified at the ARK kernel launch time
    ark_state_dict = ark_model.state_dict()

    # Test if the parameters are the same
    for k, v in state_dict.items():
        np.testing.assert_allclose(v, ark_state_dict[k])

    print("ARK module test")
    print(
        "batch_size:",
        batch_size,
        "seq_len:",
        seq_len,
        "d_model:",
        d_model,
        "d_ff:",
        d_ff,
    )
    print("max error: ", max_error, "avg error: ", avg_error)

```
```

`docs/tutorial/multi_gpu_tutorial.md`:

```md
# Multi-GPU Parallel Training

This tutorial will guide you through implementing parallel training across multiple GPUs using multi-process programming. Parallel training is beneficial for harnessing the full potential of multiple GPUs and accelerating the training process.

## Prerequisites

Before you begin, make sure you have completed the [installation](./install.md) process. Next, you can run the tutorial example at [multi_gpu_tutorial.py](../examples/tutorial/multi_gpu_tutorial.py) to learn how to use ARK to communicate between different GPUs.

## Ping-Pong Transfer Example

In this example, we will demonstrate communication between two GPUs (GPU 0 and GPU 1) using a simple ping-pong transfer. We will send a tensor from GPU 0 to GPU 1, and then send back the received tensor from GPU 1 to GPU 0.

To begin, we need to import the necessary modules. We will use the `multiprocessing` module to create multiple processes. Alternatively, we can use MPI to create multiple processes. We will also use the `numpy` module.

```python
import ark
import numpy as np
import multiprocessing

world_size = 2

tensor_len = 2048
tensor_size = tensor_len * 2
```

Since we have 2 GPUs, we need to run 2 processes. Each process will be assigned to a GPU. 

```python
num_processes = world_size  # number of processes
processes = []
np_inputs = np.random.rand(tensor_len).astype(np.float16)

# Create a process for each GPU  
for i in range(num_processes):
    process = multiprocessing.Process(
        target=sendrecv_test_ping_pong_function, args=(i, np_inputs)
    )
    process.start()
    processes.append(process)

# Use process.join() to wait for the completion of all processes
for process in processes:
    process.join()
```

The following is the main function for the two processes. We first use `ark.Runtime(rank, world_size)` to create the ARK runtime. The `rank` parameter is the rank of the process, and the `world_size` parameter is the number of processes. In ARK, we assume that one process corresponds to one GPU. 



```python
def sendrecv_test_ping_pong_function(rank, np_inputs):
    print("rank:", rank)
    # Initialize the ARK runtime
    runtime = ark.Runtime(rank, world_size)
```


The first Model on process 0 will send send_tensor from GPU 0 to GPU 1. Since multiple tensors can be sent to the same GPU, an identifier `id` is required to distinguish the tensor. Here, we set the first `id` to 0. Then, the first process will receive recv_tensor from GPU 1. The received `id` will be 1.

For more information about the `send` and `recv` operator, please refer to the [API documentation](../docs/api.md).

```python
    # Define the behavior for rank 0
    if rank == 0:
        send_tensor = ark.tensor(ark.Dims(tensor_len), ark.FP16)
        recv_tensor = ark.tensor(ark.Dims(tensor_len), ark.FP16)

        # send the tensor to rank 1
        send_id, dst_rank = 0, 1
        send_dep_tensor = ark.send(send_tensor, send_id, dst_rank, tensor_size)
        # A identity operation is used to add an execution dependency and
        # make sure execution order correct
        ark.send_done(
            ark.identity(send_tensor, [send_dep_tensor]), send_id, dst_rank
        )
        # recv the tensor from rank 1
        recv_id, recv_rank = 1, 1
        ark.recv(recv_tensor, recv_id, recv_rank)
```

The following is the model definition for GPU1. Here, GPU1 receives the tensor from GPU0 and sends it back to GPU0.

```python
    # Define the behavior for rank 1
    if rank == 1:
        # recv the tensor from rank 0
        recv_tensor = ark.tensor(ark.Dims(tensor_len), ark.FP16)
        recv_id, recv_rank = 0, 0
        recv_dep = ark.recv(recv_tensor, recv_id, recv_rank)

        # The send must be executed after the recv, identity is used to
        # add an execution dependency between the two operations
        send_tensor = ark.identity(recv_tensor, [recv_dep])

        # Send the received tensor back to rank 0
        send_id, dst_rank = 1, 0
        send_dep_tensor = ark.send(send_tensor, send_id, dst_rank, tensor_size)
        # A identity operation is used to add an execution dependency and
        # make sure execution order correct
        ark.send_done(
            ark.identity(send_tensor, [send_dep_tensor]), send_id, dst_rank
        )
```

Note that there is a line that describes the dependency between the send and recv operation:

```python
send_tensor = model.identity(recv_tensor, [recv_dep])
```

This is because the send operation must be executed after the recv operation. In the current scheduler, if this dependency is not specified, the send operation may be executed before the recv operation, causing an error. We will improve the scheduler in the future to automatically handle this situation.
    
Finally, we can use `runtime.launch()` to compile the kernel code and create contexts for each GPU. The connection between the two GPUs will be established automatically.

After we lauch the ARK model, we need to copy the send tensor to GPU0 to initialize the send tensor. Then we can run the ARK program.

```python
    # Launch the ARK runtime
    runtime.launch()

    # Copy send data to GPU0
    if rank == 0:
        send_tensor.from_numpy(np_inputs)

    # Run the ARK program
    runtime.run()
```

Finally, we can copy the recv_tensor to the host to check the result. The recv_tensor on both GPUs should be the same as the original send tensor.

```python
    # Copy data back to host and calculate errors
    host_output = recv_tensor.to_numpy()

    # Print output and error information
    print("host_output:", host_output)
    print("np_inputs:", np_inputs)
    max_error = np.max(np.abs(host_output - np_inputs))
    mean_error = np.mean(np.abs(host_output - np_inputs))
    print("max error:", max_error, "mean error:", mean_error)
    print("rank:", rank, "done")
```
```

`examples/ffn/Makefile`:

```
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

ARK_ROOT ?= /usr/local/ark
CUDIR    ?= /usr/local/cuda

CXX      := g++
CXXFLAGS := -std=c++17 -Wall -Wextra
INCLUDE  := -I$(ARK_ROOT)/include -I $(CUDIR)/include -I$(ARK_ROOT)/include/kernels
LDFLAGS  := -L$(CUDIR)/lib64/stubs -Wl,-rpath,$(CUDIR)/lib64
LDLIBS   := -lcuda -lnvidia-ml -lnvrtc -lpthread -lrt -libverbs -lnuma

all: build/ffn
	
build/ffn: build/ffn.o
	$(CXX) -o $@ $< -L$(ARK_ROOT)/lib -lark $(LDFLAGS) $(LDLIBS)

build/ffn.o: ffn.cc
	mkdir -p $(@D)
	$(CXX) -o $@ $(CXXFLAGS) $(INCLUDE) -c $<

clean:
	rm -r build/

```

`examples/ffn/ffn.cc`:

```cc
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "ark.h"
#include "ark_utils.h"
#include <cassert>
#include <fstream>
#include <iomanip>
#include <iostream>
#include <map>
#include <sys/types.h>
#include <sys/wait.h>
#include <unistd.h>
#include <vector>

using namespace std;
using namespace ark;

void print_tensor(Tensor *tensor, Executor *exe)
{
    if (tensor == nullptr) {
        return;
    }
    cout << "tensor: " << tensor->name << endl;
    size_t tensor_size = tensor->shape_bytes();
    half_t *data = (half_t *)malloc(tensor_size);
    exe->tensor_memcpy(data, tensor, tensor_size);
    for (int i = 0; i < tensor->size(); ++i) {
        cout << data[i] << " ";
    }
    cout << endl;
    delete[] data;
}

class FullyConnectedLayer
{
  public:
    FullyConnectedLayer(int dim_input, int dim_output, TensorType dtype,
                        Model &model)
        : model{model}
    {
        Tensor *weight = model.tensor({dim_input, dim_output}, dtype);
        Tensor *bias = model.tensor({1, dim_output}, dtype);
        params = {weight, bias};
    }

    Tensor *forward(Tensor *input)
    {
        this->input = input;
        Tensor *weight = params[0];
        Tensor *output1 = model.matmul(input, weight);
        Tensor *bias = params[1];
        Tensor *output2 = model.add(output1, bias);
        return output2;
    }

    Tensor *backward(Tensor *grad)
    {

        Tensor *weight = params[0];
        Tensor *bias = params[1];
        Tensor *grad_output2 = grad;
        Tensor *grad_bias = model.tensor(bias->shape, bias->type);
        grad_bias = model.scale(grad_output2, 1, grad_bias);
        Tensor *grad_output1 = grad_output2;
        Tensor *grad_input = model.tensor(input->shape, input->type);
        Tensor *grad_weight = model.tensor(weight->shape, weight->type);
        grad_input =
            model.matmul(grad_output1, weight, nullptr, 1, false, true);
        grad_weight =
            model.matmul(input, grad_output1, nullptr, 1, true, false);
        grads[weight] = grad_weight;
        grads[bias] = grad_bias;
        return grad_input;
    }

    void apply_grads()
    {
        for (auto &param : params) {
            Tensor *grad = grads[param];
            // the learning rate
            Tensor *grad_scale = model.scale(grad, -0.0001);
            Tensor *param_identity = model.identity(param);
            model.add(param, grad_scale, param_identity);
        }
    }

    void print_tensors(Executor *exe)
    {
        print_tensor(input, exe);
        // print the parameters.
        for (size_t i = 0; i < params.size(); ++i) {
            print_tensor(params[i], exe);
        }
    }

    Tensor *input;
    vector<Tensor *> params;
    map<Tensor *, Tensor *> grads;
    Model &model;
};

class FFN_Model
{
  public:
    //
    FFN_Model(int dim_model, TensorType dtype, Model &model, int layer_num,
              int num_gpus, int gpu_id)
        : model{model}, num_gpus{num_gpus}, gpu_id{gpu_id}
    {
        for (int i = 0; i < layer_num; ++i) {
            FullyConnectedLayer layer{dim_model, dim_model, dtype, model};
            layers.push_back(layer);
        }
    }

    Model &get_model()
    {
        return model;
    }

    //
    Tensor *forward(Tensor *input = nullptr)
    {
        for (size_t i = 0; i < layers.size(); ++i) {
            printf("forward layer: %d\n", i);
            input = layers[i].forward(input);
        }
        return input;
    }

    //
    void backward(Tensor *grad)
    {
        for (int i = layers.size() - 1; i >= 0; --i) {
            printf("backward layer: %d\n", i);
            grad = layers[i].backward(grad);
        }
        DimType grads_size = 0;
        vector<Tensor *> grads;

        for (auto &layer : layers) {
            for (auto &param : layer.params) {
                grads.push_back(layer.grads[param]);
                grads_size += layer.grads[param]->size();
            }
        }

        // All-reduce gradients
        if (num_gpus > 1) {
            Tensor *gradients = model.tensor({1, grads_size, 1, 1}, FP16);
            Tensor *idn = model.identity(gradients, {grads});

            model.all_reduce(idn, gpu_id, num_gpus);
        }
    }

    void print_tensors(Executor *exe)
    {
        for (size_t i = 0; i < layers.size(); ++i) {
            printf("layer: %d\n", i);
            layers[i].print_tensors(exe);
        }
    }

    Model &model;
    // model parameters.
    vector<FullyConnectedLayer> layers;
    Tensor *model_input;
    int num_gpus;
    int gpu_id;
};

class LossFn
{
  public:
    LossFn(Model &model) : model{model}
    {
    }

    Tensor *forward(Tensor *output, Tensor *ground_truth)
    {
        this->output = output;
        printf("loss forward");
        neg_ground_truth =
            model.tensor(ground_truth->shape, ground_truth->type);
        neg_ground_truth = model.scale(ground_truth, -1, neg_ground_truth);
        diff = model.tensor(output->shape, output->type);
        model.add(output, neg_ground_truth, diff);
        diff1 = model.tensor(diff->shape, diff->type);
        model.scale(diff, 1, diff1);
        loss_tensor = model.tensor(diff->shape, diff->type);
        model.mul(diff, diff1, loss_tensor);
        return loss_tensor;
    }

    Tensor *backward(Tensor *loss_tensor)
    {
        printf("loss backward");
        grad_diff = model.tensor(diff->shape, diff->type);
        model.mul(loss_tensor, diff, grad_diff);
        return grad_diff;
    }

    void print_tensors(Executor *exe)
    {
        printf("loss_fn.output: ");
        print_tensor(this->output, exe);
        printf("loss_fn.neg_ground_truth: ");
        print_tensor(this->neg_ground_truth, exe);
        printf("loss_fn.diff: ");
        print_tensor(this->diff, exe);
        printf("loss_fn.diff1: ");
        print_tensor(this->diff1, exe);
        printf("loss_fn.neg_ground_truth: ");
        print_tensor(this->neg_ground_truth, exe);
        printf("loss_fn.loss_tensor: ");
        print_tensor(this->loss_tensor, exe);
        printf("loss_fn.grad_diff: ");
        print_tensor(this->grad_diff, exe);
    }
    Tensor *output;
    Tensor *loss_tensor;
    Tensor *neg_ground_truth;
    Tensor *diff;
    Tensor *diff1;
    Tensor *grad_diff;
    Model &model;
};

class Trainer
{
  public:
    Trainer(Model &model, int dim_input, int batch_size, int gpu_id,
            int num_gpus)
        : model{model}, ffn_model{dim_input, FP16, model, 2, num_gpus, gpu_id},
          loss_fn{model},
          batch_size{batch_size}, num_gpus{num_gpus}, gpu_id{gpu_id}
    {
        input = model.tensor({batch_size, dim_input}, FP16);
        ground_truth = model.tensor({batch_size, dim_input}, FP16);
        output = ffn_model.forward(input);
        loss_tensor = loss_fn.forward(output, ground_truth);
        grad_loss = model.tensor(loss_tensor->shape, loss_tensor->type);
        grad_output = loss_fn.backward(grad_loss);
        ffn_model.backward(grad_output);
        apply_grad();

        exe = new Executor(gpu_id, gpu_id, (int)num_gpus, model,
                           "sampleFFN_Model");
        exe->compile();
    }

    void init_data()
    {
        // init the input and ground_truth.
        auto data_input =
            ark::utils::range_halfs(this->input->shape_bytes(), 1, 0);
        exe->tensor_memcpy(this->input, data_input.get(),
                           this->input->shape_bytes());
        auto data_ground_truth =
            ark::utils::range_halfs(this->ground_truth->shape_bytes(), 2, 0);
        exe->tensor_memcpy(this->ground_truth, data_ground_truth.get(),
                           this->ground_truth->shape_bytes());
        // init the grad_loss with 1.
        auto data_grad_loss =
            ark::utils::range_halfs(this->grad_loss->shape_bytes(), 1, 0);
        exe->tensor_memcpy(this->grad_loss, data_grad_loss.get(),
                           this->grad_loss->shape_bytes());
        // init all the parameters of the model with random values.
        for (auto &layer : ffn_model.layers) {
            for (auto &param : layer.params) {
                auto data = ark::utils::rand_halfs(param->shape_bytes(), 1);
                exe->tensor_memcpy(param, data.get(), param->shape_bytes());
            }
        }
    }

    void train(int iter, int print_interval = 1)
    {
        exe->launch();
        if (print_interval == 0) {
            // don't print the loss for debug.
            exe->run(iter);
        } else {
            // we only print the loss every print_interval iterations for debug.
            for (int i = 0; i < iter; ++i) {
                exe->run(1);
                exe->wait();
                if (i % print_interval == 0) {
                    float loss = get_loss();
                    cout << "iter: " << i << ", loss: " << loss << endl;
                }
            }
        }
        float elapsed_msec = exe->stop();
        cout << "Elapsed: " << elapsed_msec / iter << " ms/iter\n";
    }

    float get_loss()
    {
        size_t tensor_size = this->loss_tensor->shape_bytes();
        half_t *loss = (half_t *)malloc(tensor_size);
        exe->tensor_memcpy(loss, this->loss_tensor, tensor_size);
        float loss_sum = 0;
        for (int i = 0; i < this->loss_tensor->size(); ++i) {
            loss_sum += (float)loss[i];
        }
        delete[] loss;
        return loss_sum;
    }

    void apply_grad()
    {
        for (auto &layer : ffn_model.layers) {
            layer.apply_grads();
        }
    }

    void print_tensors(Executor *exe)
    {
        printf("loss_tensor: ");
        print_tensor(this->loss_tensor, exe);
        printf("input: ");
        print_tensor(this->input, exe);
        printf("output: ");
        print_tensor(this->output, exe);
        printf("ground_truth: ");
        print_tensor(this->ground_truth, exe);
        printf("ffn_model: ");
        this->ffn_model.print_tensors(exe);
        printf("loss_fn: ");
        this->loss_fn.print_tensors(exe);
    }

    Model &model;
    Tensor *loss_tensor, *input, *ground_truth, *output;
    Tensor *grad_output;
    Tensor *grad_loss;
    FFN_Model ffn_model;
    LossFn loss_fn;
    Executor *exe;
    int batch_size;
    int num_gpus;
    int gpu_id;
};

struct Args
{
    int batch_size;
    int dims;
    int num_gpus;
    int iterations;
    int print_interval;
    int seed;
    bool verbose;
};

Args parse_args(int argc, const char **argv)
{
    string prog = argv[0];
    vector<string> args(argv + 1, argv + argc);

    auto print_help = [&prog]() {
        cerr << "Usage: " << prog << " [options]\n"
             << "Options:\n"
             << "  -h, --help\t\t\tPrint this help message\n"
             << "  -b, --batch-size <int>\t\tBatch size\n"
             << "  -d, --dims <int>\t\tDimensions\n"
             << "  -g, --num-gpus <int>\t\tNumber of GPUs\n"
             << "  -i, --iter <int>\t\tNumber of iterations\n"
             << "  -p, --print-interval <int>\tPrint interval\n"
             << "  -s, --seed <int>\t\tRandom seed\n"
             << "  -v, --verbose\t\t\tVerbose output\n";
        exit(0);
    };

    Args ret;

    // Default arguments
    ret.batch_size = 1;
    ret.dims = 64;
    ret.num_gpus = 1;
    ret.iterations = 10;
    ret.print_interval = 1;
    ret.seed = -1;
    ret.verbose = false;

    for (auto it = args.begin(); it != args.end(); ++it) {
        if (*it == "-h" || *it == "--help") {
            print_help();
        } else if (*it == "-b" || *it == "--batch-size") {
            if (++it == args.end()) {
                cerr << "Error: missing argument for " << *(it - 1) << endl;
                exit(1);
            }
            ret.batch_size = stoi(*it);
        } else if (*it == "-d" || *it == "--dims") {
            if (++it == args.end()) {
                cerr << "Error: missing argument for " << *(it - 1) << endl;
                exit(1);
            }
            ret.dims = stoi(*it);
        } else if (*it == "-g" || *it == "--num-gpus") {
            if (++it == args.end()) {
                cerr << "Error: missing argument for " << *(it - 1) << endl;
                exit(1);
            }
            ret.num_gpus = stoi(*it);
        } else if (*it == "-i" || *it == "--iter") {
            if (++it == args.end()) {
                cerr << "Error: missing argument for " << *(it - 1) << endl;
                exit(1);
            }
            ret.iterations = stoi(*it);
        } else if (*it == "-p" || *it == "--print-interval") {
            if (++it == args.end()) {
                cerr << "Error: missing argument for " << *(it - 1) << endl;
                exit(1);
            }
            ret.print_interval = stoi(*it);
        } else if (*it == "-s" || *it == "--seed") {
            if (++it == args.end()) {
                cerr << "Error: missing argument for " << *(it - 1) << endl;
                exit(1);
            }
            ret.seed = stoi(*it);
        } else if (*it == "-v" || *it == "--verbose") {
            ret.verbose = true;
        } else {
            cerr << "Error: unknown option " << *it << endl;
            print_help();
        }
    }

    return ret;
}

int main(int argc, const char **argv)
{
    Args args = parse_args(argc, argv);

    cout << "--" << endl
         << "batch_size=" << args.batch_size << endl
         << "dims=" << args.dims << endl
         << "num_gpus=" << args.num_gpus << endl
         << "iterations=" << args.iterations << endl
         << "print_interval=" << args.print_interval << endl
         << "seed=" << args.seed << endl
         << "verbose=" << args.verbose << endl
         << "--" << endl;

    vector<int> pids;
    for (int gpu_id = 0; gpu_id < args.num_gpus; ++gpu_id) {
        pids.emplace_back(ark::utils::proc_spawn([&] {
            ark::srand(args.seed);

            Model model{gpu_id};
            Trainer trainer{model, args.dims, args.batch_size, gpu_id,
                            args.num_gpus};
            trainer.init_data();
            // train the model.
            trainer.train(args.iterations, args.print_interval);
            // trainer.print_tensors(trainer.exe);
            return 0;
        }));
    }
    int state = 0;
    for (auto pid : pids) {
        int ret = ark::utils::proc_wait(pid);
        if (ret != 0) {
            cerr << "E: Process " << pid << " returned " << ret << endl;
            state = 1;
        }
    }
    return state;
}

```

`examples/transformer/megatron_ark.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from transformer_utils import *
import transformer_ark


# PoswiseFeedForwardNet that uses tensor parallelism
class PoswiseFeedForwardNet:
    def __init__(self, model, rank):
        self.model = model
        self.rank = rank
        # The weight_1 and weight_2 are split into num_gpu parts, using tensor parallelism
        self.weight_1 = model.tensor(
            ark.Dims(d_model, d_ff // num_gpu), ark.TensorType.FP16
        )
        self.weight_2 = model.tensor(
            ark.Dims(d_ff // num_gpu, d_model), ark.TensorType.FP16
        )

    def forward(self, inputs):
        middle_result = self.model.matmul(inputs, self.weight_1, is_relu=True)
        middle_result1 = self.model.matmul(middle_result, self.weight_2)
        middle_result1 = self.model.reshape(
            middle_result1, ark.Dims(batch_size * seq_len * d_model)
        )
        middle_result_allreduced = self.model.all_reduce(
            middle_result1, self.rank, num_gpu
        )
        # all_reduce the middle_result to all the GPUs
        middle_result_allreduced = self.model.reshape(
            middle_result_allreduced, ark.Dims(batch_size, seq_len, d_model)
        )
        output = self.model.add(middle_result_allreduced, inputs)

        output_layernorm = self.model.layernorm(output)
        return output_layernorm

    def init_model(self, param, exe, prefix=""):
        weight_1 = param[prefix + "weight_1"]
        weight_1_shared = np.split(weight_1, num_gpu, axis=1)[self.rank]
        weight_1_shared_copy = weight_1_shared.copy()
        exe.tensor_memcpy_host_to_device(self.weight_1, weight_1_shared_copy)
        weight_2 = param[prefix + "weight_2"]
        weight_2_shared = np.split(weight_2, num_gpu, axis=0)[self.rank]
        weight_2_shared_copy = weight_2_shared.copy()
        exe.tensor_memcpy_host_to_device(self.weight_2, weight_2_shared_copy)


# MultiHeadAttention that uses tensor parallelism, different heads are splited on different GPUs
# The final fc layer is also splited on different GPUs
class MultiHeadAttention:
    def __init__(self, model, rank):
        self.model = model
        self.rank = rank
        self.W_Q = model.tensor(
            ark.Dims(d_model, d_k * n_heads_per_gpu), ark.TensorType.FP16
        )
        self.W_K = model.tensor(
            ark.Dims(d_model, d_k * n_heads_per_gpu), ark.TensorType.FP16
        )
        self.W_V = model.tensor(
            ark.Dims(d_model, d_v * n_heads_per_gpu), ark.TensorType.FP16
        )
        self.fc = model.tensor(
            ark.Dims(d_v * n_heads_per_gpu, d_model), ark.TensorType.FP16
        )
        self.scaled_dot_product_attention = (
            transformer_ark.ScaledDotProductAttention(model)
        )

    def forward(self, input_Q, input_K, input_V, attn_mask=None):
        # input_Q: [batch_size, len_q, d_model]
        # input_K: [batch_size, len_k, d_model]
        # input_V: [batch_size, len_v(=len_k), d_model]
        # attn_mask: [batch_size, seq_len, seq_len]
        # residual: [batch_size, len_q, d_model]
        batch_size = input_Q.shape[0]
        len_q = input_Q.shape[1]

        # Q: [batch_size, len_q, n_heads_per_gpu * d_k]
        Q = self.model.matmul(input_Q, self.W_Q)
        # Q: [batch_size, len_q, n_heads_per_gpu, d_k]
        Q = self.model.reshape(
            Q, ark.Dims(batch_size, len_q, n_heads_per_gpu, d_k)
        )
        # Q: [batch_size, n_heads_per_gpu, len_q, d_k]
        Q = self.model.transpose(Q, ark.Dims(0, 2, 1, 3))

        len_k = input_K.shape[1]
        # K: [batch_size, len_k, n_heads_per_gpu * d_k]
        K = self.model.matmul(input_K, self.W_K)
        # K: [batch_size, len_k, n_heads_per_gpu, d_k]
        K = self.model.reshape(
            K, ark.Dims(batch_size, len_k, n_heads_per_gpu, d_k)
        )
        # K: [batch_size, n_heads_per_gpu, len_k, d_k]
        K = self.model.transpose(K, ark.Dims(0, 2, 1, 3))

        len_v = input_V.shape[1]
        # V: [batch_size, len_v(=len_k), n_heads_per_gpu * d_v]
        V = self.model.matmul(input_V, self.W_V)
        # V: [batch_size, len_v(=len_k), n_heads_per_gpu, d_v]
        V = self.model.reshape(
            V, ark.Dims(batch_size, len_v, n_heads_per_gpu, d_v)
        )
        # V: [batch_size, n_heads_per_gpu, len_v(=len_k), d_v]
        V = self.model.transpose(V, ark.Dims(0, 2, 1, 3))

        context, attn = self.scaled_dot_product_attention.forward(
            Q, K, V, attn_mask
        )

        # context: [batch_size, n_heads_per_gpu, len_q, d_v]
        context1 = self.model.reshape(
            context, ark.Dims(batch_size, n_heads_per_gpu, len_q, d_v)
        )

        # context: [batch_size, len_q, n_heads_per_gpu, d_v]
        context2 = self.model.transpose(context1, ark.Dims(0, 2, 1, 3))

        context3 = self.model.reshape(
            context2, ark.Dims(batch_size, len_q, n_heads_per_gpu * d_v)
        )  # context: [batch_size, len_q, n_heads_per_gpu * d_v]

        # output: [batch_size, len_q, d_model]
        output = self.model.matmul(context3, self.fc)
        output_reshape = self.model.reshape(
            output, ark.Dims(batch_size * len_q * d_model)
        )
        output_allreduce = self.model.all_reduce(
            output_reshape, self.rank, num_gpu
        )
        output_allreduce_reshape = self.model.reshape(
            output_allreduce, ark.Dims(batch_size, len_q, d_model)
        )
        output_plus_residual = self.model.add(output_allreduce_reshape, input_Q)
        output_layernorm = self.model.layernorm(output_plus_residual)
        return output_layernorm, attn

    def init_model(self, param, exe, prefix=""):
        W_Q = param[prefix + "W_Q"]
        W_Q_shared = np.split(W_Q, num_gpu, axis=1)[self.rank]
        W_Q_shared_copy = W_Q_shared.copy()
        exe.tensor_memcpy_host_to_device(self.W_Q, W_Q_shared_copy)
        W_K = param[prefix + "W_K"]
        W_K_shared = np.split(W_K, num_gpu, axis=1)[self.rank]
        W_K_shared_copy = W_K_shared.copy()
        exe.tensor_memcpy_host_to_device(self.W_K, W_K_shared_copy)
        W_V = param[prefix + "W_V"]
        W_V_shared = np.split(W_V, num_gpu, axis=1)[self.rank]
        W_V_shared_copy = W_V_shared.copy()
        exe.tensor_memcpy_host_to_device(self.W_V, W_V_shared_copy)
        fc = param[prefix + "fc"]
        fc_shared = np.split(fc, num_gpu, axis=0)[self.rank]
        fc_shared_copy = fc_shared.copy()
        exe.tensor_memcpy_host_to_device(self.fc, fc_shared_copy)


class EncoderLayer:
    def __init__(self, model, rank):
        self.rank = rank
        self.model = model
        self.enc_self_attn = MultiHeadAttention(
            model, rank
        )  # Multi-Head Attention mechanism
        self.pos_ffn = PoswiseFeedForwardNet(model, rank)

    def forward(self, enc_inputs, enc_self_attn_mask=None):
        enc_outputs, attn = self.enc_self_attn.forward(
            enc_inputs,
            enc_inputs,
            enc_inputs,
            # enc_outputs: [batch_size, src_len, d_model],
            enc_self_attn_mask,
        )  # attn: [batch_size, n_heads, src_len, src_len]
        enc_outputs1 = self.pos_ffn.forward(
            enc_outputs
        )  # enc_outputs: [batch_size, src_len, d_model]
        return enc_outputs1, attn

    def init_model(self, param, exe, prefix=""):
        self.enc_self_attn.init_model(param, exe, prefix + "enc_self_attn.")
        self.pos_ffn.init_model(param, exe, prefix + "pos_ffn.")

```

`examples/transformer/megatron_test.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import transformer_pytorch
import megatron_ark
from transformer_utils import *
import multiprocessing
import transformer_ark


def test_PoswiseFeedForwardNet_process(rank, param):
    # Create a Model instance
    model = ark.Model(rank)

    input_tensor = model.tensor(
        ark.Dims(batch_size, seq_len, d_model), ark.TensorType.FP16
    )

    ark_model = megatron_ark.PoswiseFeedForwardNet(model, rank)

    output_tensor = ark_model.forward(input_tensor)

    exe = ark.Executor(rank, rank, num_gpu, model, "test_poswiseFeedForwardNet")
    exe.compile()

    exe.launch()

    input_tensor_host = param["input_tensor"]
    exe.tensor_memcpy_host_to_device(input_tensor, input_tensor_host)
    ark_model.init_model(param, exe)
    exe.run(1)
    exe.stop()

    output_tensor_host = np.zeros(
        (batch_size, seq_len, d_model), dtype=np.float16
    )

    exe.tensor_memcpy_device_to_host(output_tensor_host, output_tensor)

    input_tensor_host_float32 = input_tensor_host.astype(np.float32)

    torch_input = torch.from_numpy(input_tensor_host_float32)

    torch_model = transformer_pytorch.PoswiseFeedForwardNet()

    torch_model.init_model(param)

    gt = torch_model(torch_input).detach().numpy().astype(np.float16)

    # test if the result is correct
    max_error = np.max(np.abs(output_tensor_host - gt))
    avg_error = np.mean(np.abs(output_tensor_host - gt))
    # print("input_tensor_host", input_tensor_host)
    # print("output_tensor_host", output_tensor_host)
    # print("gt", gt)
    print("Megatron poswise feed forward net test")
    print(
        "batch_size:",
        batch_size,
        "seq_len:",
        seq_len,
        "d_model:",
        d_model,
        "d_ff:",
        d_ff,
    )
    print("max error: ", max_error, "avg error: ", avg_error)


def multi_process_test_main(func, np_inputs):
    ark.init()
    num_processes = num_gpu  # number of processes
    processes = []

    for i in range(num_processes):
        process = multiprocessing.Process(target=func, args=(i, np_inputs))
        process.start()
        processes.append(process)

    for process in processes:
        process.join()


def test_PoswiseFeedForwardNet():
    # set random seed
    np.random.seed(1234)
    input_tensor_host = (
        (np.random.rand(batch_size, seq_len, d_model) - 0.5) * 0.1
    ).astype(np.float16)
    weight_1_host = ((np.random.rand(d_model, d_ff) - 0.5) * 0.1).astype(
        np.float16
    )
    weight_2_host = ((np.random.rand(d_ff, d_model) - 0.5) * 0.1).astype(
        np.float16
    )
    param = {
        "input_tensor": input_tensor_host,
        "weight_1": weight_1_host,
        "weight_2": weight_2_host,
    }
    multi_process_test_main(test_PoswiseFeedForwardNet_process, param)


def test_MultiHeadAttention_process(rank, param):
    # Create a Model instance
    model = ark.Model(rank)

    Q = model.tensor(
        ark.Dims(batch_size, seq_len, d_model), ark.TensorType.FP16
    )
    K = model.tensor(
        ark.Dims(batch_size, seq_len, d_model), ark.TensorType.FP16
    )
    V = model.tensor(
        ark.Dims(batch_size, seq_len, d_model), ark.TensorType.FP16
    )

    ark_model = megatron_ark.MultiHeadAttention(model, rank)

    attn_mask = model.tensor(
        ark.Dims(batch_size * n_heads_per_gpu, seq_len, seq_len),
        ark.TensorType.FP16,
    )

    context, attn = ark_model.forward(Q, K, V, attn_mask)

    exe = ark.Executor(rank, rank, num_gpu, model, "test_multiHeadAttention")
    exe.compile()

    exe.launch()
    Q_host = param["Q"]
    K_host = param["K"]
    V_host = param["V"]
    exe.tensor_memcpy_host_to_device(Q, Q_host)
    exe.tensor_memcpy_host_to_device(K, K_host)
    exe.tensor_memcpy_host_to_device(V, V_host)
    transformer_ark.attn_pad_mask_init(attn_mask, exe, input_seq_len)
    ark_model.init_model(param, exe)
    exe.run(1)
    exe.stop()

    context_host = np.zeros(
        (batch_size, seq_len, n_heads * d_v), dtype=np.float16
    )
    attn_host = np.zeros(
        (batch_size, n_heads_per_gpu, seq_len, seq_len), dtype=np.float16
    )

    exe.tensor_memcpy_device_to_host(context_host, context)
    exe.tensor_memcpy_device_to_host(attn_host, attn)

    torch_Q = torch.from_numpy(Q_host.astype(np.float32))
    torch_K = torch.from_numpy(K_host.astype(np.float32))
    torch_V = torch.from_numpy(V_host.astype(np.float32))

    torch_model = transformer_pytorch.MultiHeadAttention()
    torch_model.init_model(param)
    input_seq = np.zeros((batch_size, seq_len), dtype=np.int32)
    for i in range(batch_size):
        for j in range(seq_len):
            if j < input_seq_len:
                input_seq[i][j] = 1
    input_seq_torch = torch.from_numpy(input_seq)
    attn_mask_torch = transformer_pytorch.get_attn_pad_mask(
        input_seq_torch, input_seq_torch
    )
    context_torch, attn_torch = torch_model(
        torch_Q, torch_K, torch_V, attn_mask_torch
    )

    gt_context = context_torch.detach().numpy().astype(np.float16)
    gt_attn = attn_torch.detach().numpy().astype(np.float16)

    context_max_error = np.max(np.abs(context_host - gt_context))
    context_avg_error = np.mean(np.abs(context_host - gt_context))
    relative_context_error = np.max(
        np.abs(context_host - gt_context)
    ) / np.mean(np.abs(gt_context))
    gt_attn_shard = np.split(gt_attn, num_gpu, axis=1)[rank]
    attn_max_error = np.max(np.abs(attn_host - gt_attn_shard))
    attn_avg_error = np.mean(np.abs(attn_host - gt_attn_shard))
    relative_attn_error = np.max(np.abs(attn_host - gt_attn_shard)) / np.mean(
        np.abs(gt_attn_shard)
    )
    print("rank:", rank)
    print("multi head attention test")
    print(
        "batch_size:",
        batch_size,
        "seq_len:",
        seq_len,
        "d_model:",
        d_model,
        "d_ff:",
        d_ff,
    )
    print(
        "max context error: ",
        context_max_error,
        "avg context error: ",
        context_avg_error,
        "relative context error: ",
        relative_context_error,
        "max attn error: ",
        attn_max_error,
        "avg attn error: ",
        attn_avg_error,
        "relative attn error: ",
        relative_attn_error,
    )
    # print("context_host", context_host)
    # print("gt_context", gt_context)
    # print("context_host - gt_context", context_host - gt_context)
    # print("attn_host", attn_host)
    # print("gt_attn", gt_attn)
    # print("attn_host - gt_attn", attn_host - gt_attn_shard)


def test_MultiHeadAttention():
    # set random seed
    np.random.seed(1234)
    Q_host = ((np.random.rand(batch_size, seq_len, d_model) - 0.5)).astype(
        np.float16
    )
    K_host = ((np.random.rand(batch_size, seq_len, d_model) - 0.5)).astype(
        np.float16
    )
    V_host = ((np.random.rand(batch_size, seq_len, d_model) - 0.5)).astype(
        np.float16
    )

    W_Q_host = ((np.random.rand(d_model, d_k * n_heads) - 0.5)).astype(
        np.float16
    )
    W_K_host = ((np.random.rand(d_model, d_k * n_heads) - 0.5)).astype(
        np.float16
    )
    W_V_host = ((np.random.rand(d_model, d_v * n_heads) - 0.5)).astype(
        np.float16
    )
    fc_host = ((np.random.rand(d_v * n_heads, d_model) - 0.5)).astype(
        np.float16
    )
    param = {
        "Q": Q_host,
        "K": K_host,
        "V": V_host,
        "W_Q": W_Q_host,
        "W_K": W_K_host,
        "W_V": W_V_host,
        "fc": fc_host,
    }
    multi_process_test_main(test_MultiHeadAttention_process, param)


def test_EncoderLayer_process(rank, param):
    # Create a Model instance
    model = ark.Model(rank)

    enc_inputs = model.tensor(
        ark.Dims(batch_size, seq_len, d_model), ark.TensorType.FP16
    )

    ark_model = megatron_ark.EncoderLayer(model, rank)

    attn_mask = model.tensor(
        ark.Dims(batch_size * n_heads_per_gpu, seq_len, seq_len),
        ark.TensorType.FP16,
    )

    context, attn = ark_model.forward(enc_inputs, attn_mask)
    # Test the mul method
    exe = ark.Executor(rank, rank, num_gpu, model, "test_encoderlayer")
    exe.compile()
    enc_inputs_host = param["enc_inputs"]
    ark_model.init_model(param, exe)

    exe.launch()
    exe.tensor_memcpy_host_to_device(enc_inputs, enc_inputs_host)
    transformer_ark.attn_pad_mask_init(attn_mask, exe, input_seq_len)
    exe.run(1)
    exe.stop()

    context_host = np.zeros(
        (batch_size, seq_len, n_heads * d_v), dtype=np.float16
    )
    attn_host = np.zeros(
        (batch_size, n_heads_per_gpu, seq_len, seq_len), dtype=np.float16
    )

    exe.tensor_memcpy_device_to_host(context_host, context)
    exe.tensor_memcpy_device_to_host(attn_host, attn)

    torch_enc_inputs = torch.from_numpy(enc_inputs_host.astype(np.float32))

    torch_model = transformer_pytorch.EncoderLayer()
    torch_model.init_model(param)
    input_seq = np.zeros((batch_size, seq_len), dtype=np.int32)
    for i in range(batch_size):
        for j in range(seq_len):
            if j < input_seq_len:
                input_seq[i][j] = 1
    input_seq_torch = torch.from_numpy(input_seq)
    attn_mask_torch = transformer_pytorch.get_attn_pad_mask(
        input_seq_torch, input_seq_torch
    )
    context_torch, attn_torch = torch_model(torch_enc_inputs, attn_mask_torch)

    gt_context = context_torch.detach().numpy().astype(np.float16)
    # gt_context = gt_context.reshape(batch_size*n_heads* seq_len* d_v)
    gt_attn = attn_torch.detach().numpy().astype(np.float16)
    gt_attn_shard = np.split(gt_attn, num_gpu, axis=1)[rank]
    context_max_error = np.max(np.abs(context_host - gt_context))
    context_avg_error = np.mean(np.abs(context_host - gt_context))
    attn_max_error = np.max(np.abs(attn_host - gt_attn_shard))
    attn_avg_error = np.mean(np.abs(attn_host - gt_attn_shard))
    print("EncoderLayer test")
    print(
        "batch_size:",
        batch_size,
        "seq_len:",
        seq_len,
        "d_model:",
        d_model,
        "d_ff:",
        d_ff,
    )
    print(
        "max context error: ",
        context_max_error,
        "avg context error: ",
        context_avg_error,
        "max attn error: ",
        attn_max_error,
        "avg attn error: ",
        attn_avg_error,
    )
    # print(context_host)
    # print(gt_context)


def test_EncoderLayer():
    enc_inputs_host = (
        (np.random.rand(batch_size, seq_len, d_model) - 0.5)
    ).astype(np.float16)

    W_Q_host = ((np.random.rand(d_model, d_k * n_heads) - 0.5)).astype(
        np.float16
    )
    W_K_host = ((np.random.rand(d_model, d_k * n_heads) - 0.5)).astype(
        np.float16
    )
    W_V_host = ((np.random.rand(d_model, d_v * n_heads) - 0.5)).astype(
        np.float16
    )
    fc_host = ((np.random.rand(d_v * n_heads, d_model) - 0.5)).astype(
        np.float16
    )
    pos_ffn_weight_1_host = (
        (np.random.rand(d_model, d_ff) - 0.5) * 0.1
    ).astype(np.float16)
    pos_ffn_weight_2_host = (
        (np.random.rand(d_ff, d_model) - 0.5) * 0.1
    ).astype(np.float16)

    param = {
        "enc_inputs": enc_inputs_host,
        "enc_self_attn.W_Q": W_Q_host,
        "enc_self_attn.W_K": W_K_host,
        "enc_self_attn.W_V": W_V_host,
        "enc_self_attn.fc": fc_host,
        "pos_ffn.weight_1": pos_ffn_weight_1_host,
        "pos_ffn.weight_2": pos_ffn_weight_2_host,
    }
    multi_process_test_main(test_EncoderLayer_process, param)


if __name__ == "__main__":
    test_PoswiseFeedForwardNet()
    test_MultiHeadAttention()
    test_EncoderLayer()

```

`examples/transformer/transformer_ark.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from transformer_utils import *


def attn_pad_mask_init(attention_mask, exe, seq_k_len):
    # attention_mask: [batch_size * n_heads, len_q, len_k]
    attention_mask_shape = attention_mask.shape
    attention_mask_host = np.zeros(
        (
            attention_mask_shape[0],
            attention_mask_shape[1],
            attention_mask_shape[2],
        ),
        dtype=np.float16,
    )
    for i in range(attention_mask_shape[0]):
        for j in range(attention_mask_shape[1]):
            for k in range(attention_mask_shape[2]):
                if k >= seq_k_len:
                    attention_mask_host[i][j][k] = -1000

    exe.tensor_memcpy_host_to_device(attention_mask, attention_mask_host)


class PoswiseFeedForwardNet:
    def __init__(self, model):
        self.model = model
        self.weight_1 = model.tensor(
            ark.Dims(d_model, d_ff), ark.TensorType.FP16
        )
        self.weight_2 = model.tensor(
            ark.Dims(d_ff, d_model), ark.TensorType.FP16
        )

    def forward(self, inputs):
        middle_result = self.model.matmul(inputs, self.weight_1, is_relu=True)
        middle_result1 = self.model.matmul(middle_result, self.weight_2)
        output = self.model.add(middle_result1, inputs)
        output_layernorm = self.model.layernorm(output)
        return output_layernorm

    def init_model(self, param, exe, prefix=""):
        exe.tensor_memcpy_host_to_device(
            self.weight_1, param[prefix + "weight_1"]
        )
        exe.tensor_memcpy_host_to_device(
            self.weight_2, param[prefix + "weight_2"]
        )


class ScaledDotProductAttention:
    def __init__(self, model):
        self.model = model

    def forward(self, Q, K, V, attn_mask=None):
        # Q: [batch_size, n_heads, len_q, d_k]
        # K: [batch_size, n_heads, len_k, d_k]
        # V: [batch_size, n_heads, len_v(=len_k), d_v]

        K_transpose = self.model.transpose(K, ark.Dims(0, 1, 3, 2))
        # reshape K_transpose to [batch_size * n_heads, d_k, len_k]
        K_transpose_shape = K_transpose.shape
        K_transpose_reshape = self.model.reshape(
            K_transpose,
            ark.Dims(
                K_transpose_shape[0] * K_transpose_shape[1],
                K_transpose_shape[2],
                K_transpose_shape[3],
            ),
        )
        # reshape Q to [batch_size * n_heads, len_q, d_k]
        Q_shape = Q.shape
        Q_reshape = self.model.reshape(
            Q, ark.Dims(Q_shape[0] * Q_shape[1], Q_shape[2], Q_shape[3])
        )
        # scores: [batch_size * n_heads, len_q, len_k]
        scores = self.model.matmul(Q_reshape, K_transpose_reshape)
        scores_scale = self.model.scale(scores, 1 / np.sqrt(d_k))
        if attn_mask is not None:
            scores_scale = self.model.add(scores_scale, attn_mask)
        attn = self.model.softmax(scores_scale)

        # reshape V to [batch_size * n_heads, len_v, d_v]
        V_shape = V.shape
        V_reshape = self.model.reshape(
            V, ark.Dims(V_shape[0] * V_shape[1], V_shape[2], V_shape[3])
        )

        # context: [batch_size * n_heads, len_q, d_v]
        context = self.model.matmul(attn, V_reshape)
        return context, attn


class MultiHeadAttention:
    def __init__(self, model):
        self.model = model
        self.W_Q = model.tensor(
            ark.Dims(d_model, d_k * n_heads), ark.TensorType.FP16
        )
        self.W_K = model.tensor(
            ark.Dims(d_model, d_k * n_heads), ark.TensorType.FP16
        )
        self.W_V = model.tensor(
            ark.Dims(d_model, d_v * n_heads), ark.TensorType.FP16
        )
        self.fc = model.tensor(
            ark.Dims(d_v * n_heads, d_model), ark.TensorType.FP16
        )
        self.scaled_dot_product_attention = ScaledDotProductAttention(model)

    def forward(self, input_Q, input_K, input_V, attn_mask=None):
        # input_Q: [batch_size, len_q, d_model]
        # input_K: [batch_size, len_k, d_model]
        # input_V: [batch_size, len_v(=len_k), d_model]
        # attn_mask: [batch_size, seq_len, seq_len]
        # residual: [batch_size, len_q, d_model]
        batch_size = input_Q.shape[0]
        len_q = input_Q.shape[1]
        # Q: [batch_size, len_q, n_heads * d_k]
        Q = self.model.matmul(input_Q, self.W_Q)
        # Q: [batch_size, len_q, n_heads, d_k]
        Q = self.model.reshape(Q, ark.Dims(batch_size, len_q, n_heads, d_k))
        # Q: [batch_size, n_heads, len_q, d_k]
        Q = self.model.transpose(Q, ark.Dims(0, 2, 1, 3))

        len_k = input_K.shape[1]
        # K: [batch_size, len_k, n_heads * d_k]
        K = self.model.matmul(input_K, self.W_K)
        # K: [batch_size, len_k, n_heads, d_k]
        K = self.model.reshape(K, ark.Dims(batch_size, len_k, n_heads, d_k))
        # K: [batch_size, n_heads, len_k, d_k]
        K = self.model.transpose(K, ark.Dims(0, 2, 1, 3))

        len_v = input_V.shape[1]
        # V: [batch_size, len_v(=len_k), n_heads * d_v]
        V = self.model.matmul(input_V, self.W_V)
        # V: [batch_size, len_v(=len_k), n_heads, d_v]
        V = self.model.reshape(V, ark.Dims(batch_size, len_v, n_heads, d_v))
        # V: [batch_size, n_heads, len_v(=len_k), d_v]
        V = self.model.transpose(V, ark.Dims(0, 2, 1, 3))

        context, attn = self.scaled_dot_product_attention.forward(
            Q, K, V, attn_mask
        )

        # context: [batch_size, n_heads, len_q, d_v]
        context1 = self.model.reshape(
            context, ark.Dims(batch_size, n_heads, len_q, d_v)
        )

        # context: [batch_size, len_q, n_heads, d_v]
        context2 = self.model.transpose(context1, ark.Dims(0, 2, 1, 3))

        context3 = self.model.reshape(
            context2, ark.Dims(batch_size, len_q, n_heads * d_v)
        )  # context: [batch_size, len_q, n_heads * d_v]

        # output: [batch_size, len_q, d_model]
        output = self.model.matmul(context3, self.fc)
        output_plus_residual = self.model.add(output, input_Q)
        output_layernorm = self.model.layernorm(output_plus_residual)
        return output_layernorm, attn

    def init_model(self, param, exe, prefix=""):
        exe.tensor_memcpy_host_to_device(self.W_Q, param[prefix + "W_Q"])
        exe.tensor_memcpy_host_to_device(self.W_K, param[prefix + "W_K"])
        exe.tensor_memcpy_host_to_device(self.W_V, param[prefix + "W_V"])
        exe.tensor_memcpy_host_to_device(self.fc, param[prefix + "fc"])


class EncoderLayer:
    def __init__(self, model):
        self.model = model
        self.enc_self_attn = MultiHeadAttention(
            model
        )  # Multi-Head Attention mechanism
        self.pos_ffn = PoswiseFeedForwardNet(model)

    def forward(self, enc_inputs, enc_self_attn_mask=None):
        enc_outputs, attn = self.enc_self_attn.forward(
            enc_inputs,
            enc_inputs,
            enc_inputs,
            # enc_outputs: [batch_size, src_len, d_model],
            enc_self_attn_mask,
        )  # attn: [batch_size, n_heads, src_len, src_len]
        enc_outputs1 = self.pos_ffn.forward(
            enc_outputs
        )  # enc_outputs: [batch_size, src_len, d_model]
        return enc_outputs1, attn

    def init_model(self, param, exe, prefix=""):
        self.enc_self_attn.init_model(param, exe, prefix + "enc_self_attn.")
        self.pos_ffn.init_model(param, exe, prefix + "pos_ffn.")


class Encoder:
    def __init__(self, model):
        self.layers = []
        for i in range(n_layers):
            self.layers.append(EncoderLayer(model))

    def forward(self, enc_inputs, enc_self_attn_mask=None):
        enc_self_attns = []
        for layer in self.layers:
            enc_outputs, enc_self_attn = layer.forward(
                enc_inputs, enc_self_attn_mask
            )
            enc_self_attns.append(enc_self_attn)
        return enc_outputs, enc_self_attns

    def init_model(self, param, exe, prefix=""):
        for i, layer in enumerate(self.layers):
            layer.init_model(param, exe, prefix + "layers." + str(i) + ".")

```

`examples/transformer/transformer_pytorch.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from transformer_utils import *


class PositionalEncoding(nn.Module):
    def __init__(self, d_model, dropout=0.1, max_len=5000):
        super(PositionalEncoding, self).__init__()
        self.dropout = nn.Dropout(p=dropout)
        pos_table = np.array(
            [
                [pos / np.power(10000, 2 * i / d_model) for i in range(d_model)]
                if pos != 0
                else np.zeros(d_model)
                for pos in range(max_len)
            ]
        )
        pos_table[1:, 0::2] = np.sin(
            pos_table[1:, 0::2]
        )  # word embedding dimmentions is even
        pos_table[1:, 1::2] = np.cos(
            pos_table[1:, 1::2]
        )  # word embedding dimmentions is odd
        self.pos_table = torch.FloatTensor(
            pos_table
        ).cuda()  # enc_inputs: [seq_len, d_model]

    def forward(self, enc_inputs):  # enc_inputs: [batch_size, seq_len, d_model]
        enc_inputs += self.pos_table[: enc_inputs.size(1), :]
        return self.dropout(enc_inputs.cuda())


def get_attn_pad_mask(
    seq_q, seq_k
):  # seq_q: [batch_size, seq_len] ,seq_k: [batch_size, seq_len]
    batch_size, len_q = seq_q.size()
    batch_size, len_k = seq_k.size()
    pad_attn_mask = seq_k.data.eq(0).unsqueeze(
        1
    )  # mark seq_k that contain P (=0) with 1 ,[batch_size, 1, len_k]
    return pad_attn_mask.expand(
        batch_size, len_q, len_k
    )  # expand into multiple dimensions


def get_attn_subsequence_mask(seq):  # seq: [batch_size, tgt_len]
    attn_shape = [seq.size(0), seq.size(1), seq.size(1)]
    subsequence_mask = np.triu(
        np.ones(attn_shape), k=1
    )  # generate an upper triangular matrix, [batch_size, tgt_len, tgt_len]
    subsequence_mask = torch.from_numpy(
        subsequence_mask
    ).byte()  # [batch_size, tgt_len, tgt_len]
    return subsequence_mask


class PoswiseFeedForwardNet(nn.Module):
    def __init__(self):
        super(PoswiseFeedForwardNet, self).__init__()
        self.weight_1 = nn.Parameter(torch.FloatTensor(d_model, d_ff))
        self.weight_2 = nn.Parameter(torch.FloatTensor(d_ff, d_model))

    # inputs: [batch_size, seq_len, d_model]
    def forward(self, inputs):
        output = torch.matmul(
            inputs, self.weight_1
        )  # [batch_size, seq_len, d_ff]
        output = nn.ReLU()(output)
        output = torch.matmul(
            output, self.weight_2
        )  # [batch_size, seq_len, d_model]
        output = nn.LayerNorm(d_model)(
            output + inputs
        )  # [batch_size, seq_len, d_model]
        return output

    def init_model(self, param, prefix=""):
        self.weight_1.data.copy_(torch.from_numpy(param[prefix + "weight_1"]))
        self.weight_2.data.copy_(torch.from_numpy(param[prefix + "weight_2"]))


class ScaledDotProductAttention(nn.Module):
    def __init__(self):
        super(ScaledDotProductAttention, self).__init__()

    def forward(
        self, Q, K, V, attn_mask=None
    ):  # Q: [batch_size, n_heads, len_q, d_k]
        # K: [batch_size, n_heads, len_k, d_k]
        # V: [batch_size, n_heads, len_v(=len_k), d_v]
        # attn_mask: [batch_size, n_heads, seq_len, seq_len]
        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k)
        # scores : [batch_size, n_heads, len_q, len_k]
        if attn_mask is not None:
            scores.masked_fill_(attn_mask, -1e9)
        attn = nn.Softmax(dim=-1)(scores)
        context = torch.matmul(attn, V)  # [batch_size, n_heads, len_q, d_v]
        return context, attn


class MultiHeadAttention(nn.Module):
    def __init__(self):
        super(MultiHeadAttention, self).__init__()
        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)
        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)
        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)
        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)

    # input_Q: [batch_size, len_q, d_model]
    def forward(self, input_Q, input_K, input_V, attn_mask=None):
        # input_K: [batch_size, len_k, d_model]
        # input_V: [batch_size, len_v(=len_k), d_model]
        # attn_mask: [batch_size, seq_len, seq_len]
        residual, batch_size = input_Q, input_Q.size(0)
        # Q: [batch_size, n_heads, len_q, d_k]
        Q = self.W_Q(input_Q).view(batch_size, -1, n_heads, d_k).transpose(1, 2)
        # K: [batch_size, n_heads, len_k, d_k]
        K = self.W_K(input_K).view(batch_size, -1, n_heads, d_k).transpose(1, 2)
        # V: [batch_size, n_heads, len_v(=len_k), d_v]
        V = self.W_V(input_V).view(batch_size, -1, n_heads, d_v).transpose(1, 2)
        if attn_mask is not None:
            attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)
        # attn_mask : [batch_size, n_heads, seq_len, seq_len]
        # context: [batch_size, n_heads, len_q, d_v]
        context, attn = ScaledDotProductAttention()(Q, K, V, attn_mask)
        # attn: [batch_size, n_heads, len_q, len_k]
        context = context.transpose(1, 2).reshape(batch_size, -1, n_heads * d_v)

        # context: [batch_size, len_q, n_heads * d_v]
        # [batch_size, len_q, d_model]
        output = self.fc(context)
        layer_norm = nn.LayerNorm(d_model)
        # return output + residual, attn
        return nn.LayerNorm(d_model)(output + residual), attn

    def init_model(self, param, prefix=""):
        self.W_Q.weight.data.copy_(
            torch.from_numpy(param[prefix + "W_Q"].transpose(1, 0))
        )
        self.W_K.weight.data.copy_(
            torch.from_numpy(param[prefix + "W_K"].transpose(1, 0))
        )
        self.W_V.weight.data.copy_(
            torch.from_numpy(param[prefix + "W_V"].transpose(1, 0))
        )
        self.fc.weight.data.copy_(
            torch.from_numpy(param[prefix + "fc"].transpose(1, 0))
        )


class EncoderLayer(nn.Module):
    def __init__(self):
        super(EncoderLayer, self).__init__()
        self.enc_self_attn = (
            MultiHeadAttention()
        )  # Multi-Head Attention mechanism
        self.pos_ffn = PoswiseFeedForwardNet()  # FeedForward neural networks

    def forward(
        self, enc_inputs, enc_self_attn_mask
    ):  # enc_inputs: [batch_size, src_len, d_model]
        # enc_self_attn_mask: [batch_size, src_len, src_len]
        enc_outputs, attn = self.enc_self_attn(
            enc_inputs,
            enc_inputs,
            enc_inputs,
            # enc_outputs: [batch_size, src_len, d_model],
            enc_self_attn_mask,
        )  # attn: [batch_size, n_heads, src_len, src_len]
        enc_outputs = self.pos_ffn(
            enc_outputs
        )  # enc_outputs: [batch_size, src_len, d_model]
        return enc_outputs, attn

    def init_model(self, param, prefix=""):
        self.enc_self_attn.init_model(param, prefix + "enc_self_attn.")
        self.pos_ffn.init_model(param, prefix + "pos_ffn.")


# TODO: test the Encoder
class Encoder(nn.Module):
    def __init__(self):
        super(Encoder, self).__init__()
        # self.EmbeddingLyaer = EmbeddingLyaer()
        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])

    def forward(
        self, enc_outputs, enc_self_attn_mask
    ):  # enc_inputs: [batch_size, src_len]
        enc_self_attns = []
        for layer in self.layers:
            enc_outputs, enc_self_attn = layer(
                enc_outputs, enc_self_attn_mask
            )  # enc_outputs :   [batch_size, src_len, d_model],
            # enc_self_attn : [batch_size, n_heads, src_len, src_len]
            enc_self_attns.append(enc_self_attn)
        return enc_outputs, enc_self_attns

    def init_model(self, param, prefix=""):
        for i, layer in enumerate(self.layers):
            layer.init_model(param, prefix + "layers." + str(i) + ".")

```

`examples/transformer/transformer_test.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import transformer_pytorch
import transformer_ark
from transformer_utils import *


def test_PoswiseFeedForwardNet():
    ark.init()

    # Create a Model instance
    model = ark.Model()

    input_tensor = model.tensor(
        ark.Dims(batch_size, seq_len, d_model), ark.TensorType.FP16
    )

    ark_model = transformer_ark.PoswiseFeedForwardNet(model)
    output_tensor = ark_model.forward(input_tensor)
    # Test the mul method
    exe = ark.Executor(0, 0, 1, model, "test_PoswiseFeedForwardNet")
    exe.compile()
    input_tensor_host = (
        (np.random.rand(batch_size, seq_len, d_model) - 0.5) * 0.1
    ).astype(np.float16)

    exe.launch()
    exe.tensor_memcpy_host_to_device(input_tensor, input_tensor_host)

    weight_1_host = ((np.random.rand(d_model, d_ff) - 0.5) * 0.1).astype(
        np.float16
    )
    weight_2_host = ((np.random.rand(d_ff, d_model) - 0.5) * 0.1).astype(
        np.float16
    )

    param = {"weight_1": weight_1_host, "weight_2": weight_2_host}

    ark_model.init_model(param, exe)

    exe.run(1)
    exe.stop()

    output_tensor_host = np.zeros(
        (batch_size, seq_len, d_model), dtype=np.float16
    )

    exe.tensor_memcpy_device_to_host(output_tensor_host, output_tensor)

    input_tensor_host_float32 = input_tensor_host.astype(np.float32)

    torch_input = torch.from_numpy(input_tensor_host_float32)

    torch_model = transformer_pytorch.PoswiseFeedForwardNet()

    torch_model.init_model(param)

    gt = torch_model(torch_input).detach().numpy().astype(np.float16)

    # test if the result is correct
    max_error = np.max(np.abs(output_tensor_host - gt))
    avg_error = np.mean(np.abs(output_tensor_host - gt))
    # print(input_tensor_host)
    # print(output_tensor_host)
    # print(gt)
    print("poswise feed forward net test")
    print(
        "batch_size:",
        batch_size,
        "seq_len:",
        seq_len,
        "d_model:",
        d_model,
        "d_ff:",
        d_ff,
    )
    print("max error: ", max_error, "avg error: ", avg_error)


def test_ScaledDotProductAttention():
    ark.init()

    # Create a Model instance
    model = ark.Model()

    Q = model.tensor(
        ark.Dims(batch_size, n_heads, seq_len, d_k), ark.TensorType.FP16
    )
    K = model.tensor(
        ark.Dims(batch_size, n_heads, seq_len, d_k), ark.TensorType.FP16
    )
    V = model.tensor(
        ark.Dims(batch_size, n_heads, seq_len, d_v), ark.TensorType.FP16
    )

    ark_model = transformer_ark.ScaledDotProductAttention(model)

    attn_mask = model.tensor(
        ark.Dims(batch_size * n_heads, seq_len, seq_len), ark.TensorType.FP16
    )

    context, attn = ark_model.forward(Q, K, V, attn_mask)

    # Test the mul method
    exe = ark.Executor(0, 0, 1, model, "test_ScaledDotProductAttention")
    exe.compile()
    Q_host = ((np.random.rand(batch_size, n_heads, seq_len, d_k) - 0.5)).astype(
        np.float16
    )
    K_host = ((np.random.rand(batch_size, n_heads, seq_len, d_k) - 0.5)).astype(
        np.float16
    )
    V_host = ((np.random.rand(batch_size, n_heads, seq_len, d_v) - 0.5)).astype(
        np.float16
    )

    exe.launch()
    exe.tensor_memcpy_host_to_device(Q, Q_host)
    exe.tensor_memcpy_host_to_device(K, K_host)
    exe.tensor_memcpy_host_to_device(V, V_host)
    transformer_ark.attn_pad_mask_init(attn_mask, exe, input_seq_len)

    exe.run(1)
    exe.stop()

    context_host = np.zeros(
        (batch_size, n_heads, seq_len, d_v), dtype=np.float16
    )
    attn_host = np.zeros(
        (batch_size, n_heads, seq_len, seq_len), dtype=np.float16
    )

    exe.tensor_memcpy_device_to_host(context_host, context)
    exe.tensor_memcpy_device_to_host(attn_host, attn)

    input_seq = np.zeros((batch_size, seq_len), dtype=np.int32)
    for i in range(batch_size):
        for j in range(seq_len):
            if j < input_seq_len:
                input_seq[i][j] = 1
    input_seq_torch = torch.from_numpy(input_seq)
    attn_mask_torch = transformer_pytorch.get_attn_pad_mask(
        input_seq_torch, input_seq_torch
    )
    attn_mask_torch = attn_mask_torch.unsqueeze(1).repeat(1, n_heads, 1, 1)
    torch_Q = torch.from_numpy(Q_host.astype(np.float32))
    torch_K = torch.from_numpy(K_host.astype(np.float32))
    torch_V = torch.from_numpy(V_host.astype(np.float32))

    torch_model = transformer_pytorch.ScaledDotProductAttention()

    context_torch, attn_torch = torch_model(
        torch_Q, torch_K, torch_V, attn_mask_torch
    )

    gt_context = context_torch.detach().numpy().astype(np.float16)
    gt_attn = attn_torch.detach().numpy().astype(np.float16)

    context_max_error = np.max(np.abs(context_host - gt_context))
    context_avg_error = np.mean(np.abs(context_host - gt_context))
    attn_max_error = np.max(np.abs(attn_host - gt_attn))
    attn_avg_error = np.mean(np.abs(attn_host - gt_attn))
    print("scaled dot product attention test")
    print(
        "batch_size:",
        batch_size,
        "seq_len:",
        seq_len,
        "d_model:",
        d_model,
        "d_ff:",
        d_ff,
    )
    print(
        "max context error: ",
        context_max_error,
        "avg context error: ",
        context_avg_error,
        "max attn error: ",
        attn_max_error,
        "avg attn error: ",
        attn_avg_error,
    )


def test_MultiHeadAttention():
    ark.init()

    # Create a Model instance
    model = ark.Model()

    Q = model.tensor(
        ark.Dims(batch_size, seq_len, d_model), ark.TensorType.FP16
    )
    K = model.tensor(
        ark.Dims(batch_size, seq_len, d_model), ark.TensorType.FP16
    )
    V = model.tensor(
        ark.Dims(batch_size, seq_len, d_model), ark.TensorType.FP16
    )

    ark_model = transformer_ark.MultiHeadAttention(model)

    attn_mask = model.tensor(
        ark.Dims(batch_size * n_heads, seq_len, seq_len), ark.TensorType.FP16
    )

    context, attn = ark_model.forward(Q, K, V, attn_mask)
    # Test the mul method
    exe = ark.Executor(0, 0, 1, model, "test_MultiHeadAttention")
    exe.compile()
    Q_host = ((np.random.rand(batch_size, seq_len, d_model) - 0.5)).astype(
        np.float16
    )
    K_host = ((np.random.rand(batch_size, seq_len, d_model) - 0.5)).astype(
        np.float16
    )
    V_host = ((np.random.rand(batch_size, seq_len, d_model) - 0.5)).astype(
        np.float16
    )

    W_Q_host = ((np.random.rand(d_model, d_k * n_heads) - 0.5)).astype(
        np.float16
    )
    W_K_host = ((np.random.rand(d_model, d_k * n_heads) - 0.5)).astype(
        np.float16
    )
    W_V_host = ((np.random.rand(d_model, d_v * n_heads) - 0.5)).astype(
        np.float16
    )
    fc_host = ((np.random.rand(d_v * n_heads, d_model) - 0.5)).astype(
        np.float16
    )

    param = {"W_Q": W_Q_host, "W_K": W_K_host, "W_V": W_V_host, "fc": fc_host}
    ark_model.init_model(param, exe)

    exe.launch()
    exe.tensor_memcpy_host_to_device(Q, Q_host)
    exe.tensor_memcpy_host_to_device(K, K_host)
    exe.tensor_memcpy_host_to_device(V, V_host)
    transformer_ark.attn_pad_mask_init(attn_mask, exe, input_seq_len)
    exe.run(1)
    exe.stop()

    context_host = np.zeros(
        (batch_size, seq_len, n_heads * d_v), dtype=np.float16
    )
    attn_host = np.zeros(
        (batch_size, n_heads, seq_len, seq_len), dtype=np.float16
    )

    exe.tensor_memcpy_device_to_host(context_host, context)
    exe.tensor_memcpy_device_to_host(attn_host, attn)

    torch_Q = torch.from_numpy(Q_host.astype(np.float32))
    torch_K = torch.from_numpy(K_host.astype(np.float32))
    torch_V = torch.from_numpy(V_host.astype(np.float32))

    torch_model = transformer_pytorch.MultiHeadAttention()
    torch_model.init_model(param)
    input_seq = np.zeros((batch_size, seq_len), dtype=np.int32)
    for i in range(batch_size):
        for j in range(seq_len):
            if j < input_seq_len:
                input_seq[i][j] = 1
    input_seq_torch = torch.from_numpy(input_seq)
    attn_mask_torch = transformer_pytorch.get_attn_pad_mask(
        input_seq_torch, input_seq_torch
    )
    context_torch, attn_torch = torch_model(
        torch_Q, torch_K, torch_V, attn_mask_torch
    )

    gt_context = context_torch.detach().numpy().astype(np.float16)
    # gt_context = gt_context.reshape(batch_size*n_heads* seq_len* d_v)
    gt_attn = attn_torch.detach().numpy().astype(np.float16)

    context_max_error = np.max(np.abs(context_host - gt_context))
    context_avg_error = np.mean(np.abs(context_host - gt_context))
    attn_max_error = np.max(np.abs(attn_host - gt_attn))
    attn_avg_error = np.mean(np.abs(attn_host - gt_attn))
    print("multi head attention test")
    print(
        "batch_size:",
        batch_size,
        "seq_len:",
        seq_len,
        "d_model:",
        d_model,
        "d_ff:",
        d_ff,
    )
    print(
        "max context error: ",
        context_max_error,
        "avg context error: ",
        context_avg_error,
        "max attn error: ",
        attn_max_error,
        "avg attn error: ",
        attn_avg_error,
    )
    # print(context_host)
    # print(gt_context)


def test_EncoderLayer():
    ark.init()

    # Create a Model instance
    model = ark.Model()

    enc_inputs = model.tensor(
        ark.Dims(batch_size, seq_len, d_model), ark.TensorType.FP16
    )

    ark_model = transformer_ark.EncoderLayer(model)

    attn_mask = model.tensor(
        ark.Dims(batch_size * n_heads, seq_len, seq_len), ark.TensorType.FP16
    )

    context, attn = ark_model.forward(enc_inputs, attn_mask)
    # Test the mul method
    exe = ark.Executor(0, 0, 1, model, "test_EncoderLayer")
    exe.compile()
    enc_inputs_host = (
        (np.random.rand(batch_size, seq_len, d_model) - 0.5)
    ).astype(np.float16)

    W_Q_host = ((np.random.rand(d_model, d_k * n_heads) - 0.5)).astype(
        np.float16
    )
    W_K_host = ((np.random.rand(d_model, d_k * n_heads) - 0.5)).astype(
        np.float16
    )
    W_V_host = ((np.random.rand(d_model, d_v * n_heads) - 0.5)).astype(
        np.float16
    )
    fc_host = ((np.random.rand(d_v * n_heads, d_model) - 0.5)).astype(
        np.float16
    )
    pos_ffn_weight_1_host = (
        (np.random.rand(d_model, d_ff) - 0.5) * 0.1
    ).astype(np.float16)
    pos_ffn_weight_2_host = (
        (np.random.rand(d_ff, d_model) - 0.5) * 0.1
    ).astype(np.float16)

    param = {
        "enc_self_attn.W_Q": W_Q_host,
        "enc_self_attn.W_K": W_K_host,
        "enc_self_attn.W_V": W_V_host,
        "enc_self_attn.fc": fc_host,
        "pos_ffn.weight_1": pos_ffn_weight_1_host,
        "pos_ffn.weight_2": pos_ffn_weight_2_host,
    }
    ark_model.init_model(param, exe)

    exe.launch()
    exe.tensor_memcpy_host_to_device(enc_inputs, enc_inputs_host)
    transformer_ark.attn_pad_mask_init(attn_mask, exe, input_seq_len)
    exe.run(1)
    exe.stop()

    context_host = np.zeros(
        (batch_size, seq_len, n_heads * d_v), dtype=np.float16
    )
    attn_host = np.zeros(
        (batch_size, n_heads, seq_len, seq_len), dtype=np.float16
    )

    exe.tensor_memcpy_device_to_host(context_host, context)
    exe.tensor_memcpy_device_to_host(attn_host, attn)

    torch_enc_inputs = torch.from_numpy(enc_inputs_host.astype(np.float32))

    torch_model = transformer_pytorch.EncoderLayer()
    torch_model.init_model(param)
    input_seq = np.zeros((batch_size, seq_len), dtype=np.int32)
    for i in range(batch_size):
        for j in range(seq_len):
            if j < input_seq_len:
                input_seq[i][j] = 1
    input_seq_torch = torch.from_numpy(input_seq)
    attn_mask_torch = transformer_pytorch.get_attn_pad_mask(
        input_seq_torch, input_seq_torch
    )
    context_torch, attn_torch = torch_model(torch_enc_inputs, attn_mask_torch)

    gt_context = context_torch.detach().numpy().astype(np.float16)
    # gt_context = gt_context.reshape(batch_size*n_heads* seq_len* d_v)
    gt_attn = attn_torch.detach().numpy().astype(np.float16)

    context_max_error = np.max(np.abs(context_host - gt_context))
    context_avg_error = np.mean(np.abs(context_host - gt_context))
    attn_max_error = np.max(np.abs(attn_host - gt_attn))
    attn_avg_error = np.mean(np.abs(attn_host - gt_attn))
    print("EncoderLayer test")
    print(
        "batch_size:",
        batch_size,
        "seq_len:",
        seq_len,
        "d_model:",
        d_model,
        "d_ff:",
        d_ff,
    )
    print(
        "max context error: ",
        context_max_error,
        "avg context error: ",
        context_avg_error,
        "max attn error: ",
        attn_max_error,
        "avg attn error: ",
        attn_avg_error,
    )
    # print(context_host)
    # print(gt_context)


def test_Encoder():
    ark.init()

    # Create a Model instance
    model = ark.Model()

    enc_inputs = model.tensor(
        ark.Dims(batch_size, seq_len, d_model), ark.TensorType.FP16
    )

    ark_model = transformer_ark.Encoder(model)

    attn_mask = model.tensor(
        ark.Dims(batch_size * n_heads, seq_len, seq_len), ark.TensorType.FP16
    )

    context, attns = ark_model.forward(enc_inputs, attn_mask)
    # Test the mul method
    exe = ark.Executor(0, 0, 1, model, "test_Encoder")
    exe.compile()
    enc_inputs_host = (
        (np.random.rand(batch_size, seq_len, d_model) - 0.5)
    ).astype(np.float16)

    param = {}

    for i in range(n_layers):
        W_Q_host = ((np.random.rand(d_model, d_k * n_heads) - 0.5)).astype(
            np.float16
        )
        W_K_host = ((np.random.rand(d_model, d_k * n_heads) - 0.5)).astype(
            np.float16
        )
        W_V_host = ((np.random.rand(d_model, d_v * n_heads) - 0.5)).astype(
            np.float16
        )
        fc_host = ((np.random.rand(d_v * n_heads, d_model) - 0.5)).astype(
            np.float16
        )
        pos_ffn_weight_1_host = (
            (np.random.rand(d_model, d_ff) - 0.5) * 0.1
        ).astype(np.float16)
        pos_ffn_weight_2_host = (
            (np.random.rand(d_ff, d_model) - 0.5) * 0.1
        ).astype(np.float16)

        prefix = "layers." + str(i) + "."
        param[prefix + "enc_self_attn.W_Q"] = W_Q_host
        param[prefix + "enc_self_attn.W_K"] = W_K_host
        param[prefix + "enc_self_attn.W_V"] = W_V_host
        param[prefix + "enc_self_attn.fc"] = fc_host
        param[prefix + "pos_ffn.weight_1"] = pos_ffn_weight_1_host
        param[prefix + "pos_ffn.weight_2"] = pos_ffn_weight_2_host

    ark_model.init_model(param, exe)

    exe.launch()
    exe.tensor_memcpy_host_to_device(enc_inputs, enc_inputs_host)
    transformer_ark.attn_pad_mask_init(attn_mask, exe, input_seq_len)
    exe.run(1)
    exe.stop()

    context_host = np.zeros(
        (batch_size, seq_len, n_heads * d_v), dtype=np.float16
    )

    exe.tensor_memcpy_device_to_host(context_host, context)
    attns_host = []
    for i in range(n_layers):
        attn_host = np.zeros(
            (batch_size, n_heads, seq_len, seq_len), dtype=np.float16
        )
        exe.tensor_memcpy_device_to_host(attn_host, attns[i])
        attns_host.append(attn_host)

    torch_enc_inputs = torch.from_numpy(enc_inputs_host.astype(np.float32))

    torch_model = transformer_pytorch.Encoder()
    torch_model.init_model(param)
    input_seq = np.zeros((batch_size, seq_len), dtype=np.int32)
    for i in range(batch_size):
        for j in range(seq_len):
            if j < input_seq_len:
                input_seq[i][j] = 1
    input_seq_torch = torch.from_numpy(input_seq)
    attn_mask_torch = transformer_pytorch.get_attn_pad_mask(
        input_seq_torch, input_seq_torch
    )
    context_torch, attns_torch = torch_model(torch_enc_inputs, attn_mask_torch)

    gt_context = context_torch.detach().numpy().astype(np.float16)
    # gt_context = gt_context.reshape(batch_size*n_heads* seq_len* d_v)
    # gt_attn = attn_torch.detach().numpy().astype(np.float16)

    context_max_error = np.max(np.abs(context_host - gt_context))
    context_avg_error = np.mean(np.abs(context_host - gt_context))
    attns_max_error = []
    attns_avg_error = []
    for i in range(n_layers):
        attn_host = attns_host[i]
        attn_torch = attns_torch[i]
        gt_attn = attn_torch.detach().numpy().astype(np.float16)
        attn_max_error = np.max(np.abs(attn_host - gt_attn))
        attn_avg_error = np.mean(np.abs(attn_host - gt_attn))
        attns_max_error.append(attn_max_error)
        attns_avg_error.append(attn_avg_error)
    # attn_max_error = np.max(np.abs(attn_host - gt_attn))
    # attn_avg_error = np.mean(np.abs(attn_host - gt_attn))
    print("Encoder test")
    print(
        "batch_size:",
        batch_size,
        "seq_len:",
        seq_len,
        "d_model:",
        d_model,
        "d_ff:",
        d_ff,
    )
    print(
        "max context error: ",
        context_max_error,
        "avg context error: ",
        context_avg_error,
        "max attn error: ",
        attns_max_error,
        "avg attn error: ",
        attns_avg_error,
    )
    # print(context_host)
    # print(gt_context)


if __name__ == "__main__":
    test_PoswiseFeedForwardNet()
    test_ScaledDotProductAttention()
    test_MultiHeadAttention()
    test_EncoderLayer()
    # TODO: test_Encoder() and test_Decoder()
    # test_Encoder()

```

`examples/transformer/transformer_utils.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import torch
import numpy as np
import torch.nn as nn

import ark

d_model = 512  # Dimension of word embeddings
d_ff = 2048  # Dimension of the hidden layer in the feed-forward network
d_k = d_v = 64  # Dimensions of K(=Q) and V in the attention mechanism
n_layers = 2  # Number of encoder and decoder layers
n_heads = 8  # Number of heads in Multi-Head Attention set to 8

batch_size = 1
seq_len = 64
src_vocab_size = 128

# The number of input tokens is 10
# Used for constructing the masks
input_seq_len = 10

# Megatron-LM on 2 GPU
num_gpu = 2
n_heads_per_gpu = n_heads // num_gpu

```

`examples/tutorial/model_tutorial.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import ark
import numpy as np


def model_tutorial():
    # Create a Model instance
    model = ark.Model()

    # Create two tensors
    input = model.tensor(ark.Dims(32), ark.TensorType.FP16)
    other = model.tensor(ark.Dims(32), ark.TensorType.FP16)

    # Add input and other to get output tensor
    output = model.add(input, other)

    # Create the executor instance, the scheduler will be created and
    # start scheduling the model when the executor is created
    exe = ark.Executor(0, 0, 1, model, "tutorial_model")

    # Compile the generated code from the code generator
    exe.compile()

    # Initialize the input tensors
    input_np = np.random.rand(1, 32).astype(np.float16)
    other_np = np.random.rand(1, 32).astype(np.float16)

    exe.tensor_memcpy_host_to_device(input, input_np)
    exe.tensor_memcpy_host_to_device(other, other_np)

    print("input: ", input_np)
    print("other: ", other_np)

    # Launch the kernel and run for 1 iteration
    exe.launch()
    exe.run(1)

    # Wait for the kernel to finish
    exe.stop()

    # Copy the output tensor back to host
    output_np = np.zeros((1, 32), dtype=np.float16)
    exe.tensor_memcpy_device_to_host(output_np, output)

    print("output: ", output_np)

    # test if the result is correct
    assert np.allclose(output_np, input_np + other_np)

    max_error = np.max(np.abs(output_np - (input_np + other_np)))
    mean_error = np.mean(np.abs(output_np - (input_np + other_np)))

    print("max error: ", max_error, "mean error: ", mean_error)
    print("test_add passed")


if __name__ == "__main__":
    model_tutorial()

```

`examples/tutorial/module_tutorial.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import torch
import numpy as np
import torch.nn as nn
import ark

# Define the parameters of the model
batch_size = 1
seq_len = 64
d_model = 512
d_ff = 2048


class SubModuleARK(ark.Module):
    def __init__(self):
        super(SubModuleARK, self).__init__()
        # Define the parameters of the submodule
        self.weight_2 = ark.Parameter(ark.tensor([d_ff, d_model], ark.FP16))

    def forward(self, inputs):
        # Perform the forward pass of the submodule
        middle_result1 = ark.matmul(inputs, self.weight_2)
        return middle_result1


class TestModelARK(ark.Module):
    def __init__(self):
        super(TestModelARK, self).__init__()
        # Define the parameters of the module
        self.weight_1 = ark.Parameter(ark.tensor([d_model, d_ff], ark.FP16))
        # Create a submodule of the module
        self.submodule = SubModuleARK()

    def forward(self, inputs):
        # Perform the forward pass of the model
        middle_result = ark.matmul(inputs, self.weight_1, is_relu=True)
        middle_result1 = self.submodule(middle_result)
        output = ark.add(middle_result1, inputs)
        output_layernorm = ark.layernorm(output)
        return output_layernorm


# Use pytorch to define the same model
class SubModulePytorch(nn.Module):
    def __init__(self):
        super(SubModulePytorch, self).__init__()
        self.weight_2 = nn.Parameter(torch.FloatTensor(d_ff, d_model))

    def forward(self, inputs):
        middle_result1 = torch.matmul(inputs, self.weight_2)
        return middle_result1


class TestModelPytorch(nn.Module):
    def __init__(self):
        super(TestModelPytorch, self).__init__()
        # Define the parameters of the module
        self.weight_1 = nn.Parameter(torch.FloatTensor(d_model, d_ff))
        # Create a submodule of the module
        self.submodule = SubModulePytorch()

    def forward(self, inputs):
        # Perform the forward pass of the model
        output = torch.matmul(inputs, self.weight_1)
        output = nn.ReLU()(output)
        output = self.submodule(output)
        output = nn.LayerNorm(d_model)(output + inputs)
        return output


# An example of using the ARK module
def module_test():
    # Initialize the ARK runtime
    runtime = ark.Runtime()
    # Create an input tensor
    input_tensor = ark.tensor([batch_size, seq_len, d_model], ark.FP16)

    # Create an ARK module
    ark_model = TestModelARK()

    # Perform the forward pass
    output_tensor = ark_model(input_tensor)

    # Launch the ARK runtime
    runtime.launch()

    # Initialize the input tensor
    input_tensor_host = (
        (np.random.rand(batch_size, seq_len, d_model) - 0.5) * 0.1
    ).astype(np.float16)
    input_tensor.from_numpy(input_tensor_host)

    # Initialize the parameters of the ARK module using numpy state_dict
    weight_1_host = ((np.random.rand(d_model, d_ff) - 0.5) * 0.1).astype(
        np.float16
    )
    weight_2_host = ((np.random.rand(d_ff, d_model) - 0.5) * 0.1).astype(
        np.float16
    )
    state_dict = {
        "weight_1": weight_1_host,
        "submodule.weight_2": weight_2_host,
    }

    # Load model parameters
    ark_model.load_state_dict(state_dict)

    # Run the ARK model
    runtime.run()

    # Copy the ARK module output tensor from device to host
    output_tensor_host = output_tensor.to_numpy()

    # For simplicity, we use float32 to compute the ground truth using pytorch
    input_tensor_host_float32 = input_tensor_host.astype(np.float32)
    torch_input = torch.from_numpy(input_tensor_host_float32)

    torch_model = TestModelPytorch()

    # Convert the numpy.ndarray type state_dict to torch.Tensor type state_dict using ark.convert_state_dict
    torch_state_dict = ark.convert_state_dict(state_dict, "torch")
    # Load model parameters
    torch_model.load_state_dict(torch_state_dict)

    # Run the pytorch model to compute the ground truth
    gt = torch_model(torch_input).detach().numpy().astype(np.float16)

    # Test if the result is correct
    max_error = np.max(np.abs(output_tensor_host - gt))
    avg_error = np.mean(np.abs(output_tensor_host - gt))

    # Use ark_model.state_dict() to get the state_dict of the ARK module
    # Note that the state_dict of the ARK module might be modified at the ARK kernel launch time
    ark_state_dict = ark_model.state_dict()

    # Test if the parameters are the same
    for k, v in state_dict.items():
        np.testing.assert_allclose(v, ark_state_dict[k])

    print("ARK module test")
    print(
        "batch_size:",
        batch_size,
        "seq_len:",
        seq_len,
        "d_model:",
        d_model,
        "d_ff:",
        d_ff,
    )
    print("max error: ", max_error, "avg error: ", avg_error)


if __name__ == "__main__":
    module_test()

```

`examples/tutorial/multi_gpu_tutorial.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import ark
import numpy as np
import multiprocessing

world_size = 2

tensor_len = 2048
tensor_size = tensor_len * 2


def sendrecv_test_ping_pong_function(rank, np_inputs):
    print("rank:", rank)
    # Initialize the ARK runtime
    runtime = ark.Runtime(rank, world_size)

    # Define the behavior for rank 0
    if rank == 0:
        send_tensor = ark.tensor(ark.Dims(tensor_len), ark.FP16)
        recv_tensor = ark.tensor(ark.Dims(tensor_len), ark.FP16)

        # send the tensor to rank 1
        send_id, dst_rank = 0, 1
        send_dep_tensor = ark.send(send_tensor, send_id, dst_rank, tensor_size)
        # A identity operation is used to add an execution dependency and
        # make sure execution order correct
        ark.send_done(
            ark.identity(send_tensor, [send_dep_tensor]), send_id, dst_rank
        )
        # recv the tensor from rank 1
        recv_id, recv_rank = 1, 1
        ark.recv(recv_tensor, recv_id, recv_rank)

    # Define the behavior for rank 1
    if rank == 1:
        # recv the tensor from rank 0
        recv_tensor = ark.tensor(ark.Dims(tensor_len), ark.FP16)
        recv_id, recv_rank = 0, 0
        recv_dep = ark.recv(recv_tensor, recv_id, recv_rank)

        # The send must be executed after the recv, identity is used to
        # add an execution dependency between the two operations
        send_tensor = ark.identity(recv_tensor, [recv_dep])

        # Send the received tensor back to rank 0
        send_id, dst_rank = 1, 0
        send_dep_tensor = ark.send(send_tensor, send_id, dst_rank, tensor_size)
        # A identity operation is used to add an execution dependency and
        # make sure execution order correct
        ark.send_done(
            ark.identity(send_tensor, [send_dep_tensor]), send_id, dst_rank
        )

    # Launch the ARK runtime
    runtime.launch()

    # Copy send data to GPU0
    if rank == 0:
        send_tensor.from_numpy(np_inputs)

    # Run the ARK program
    runtime.run()

    # Copy data back to host and calculate errors
    host_output = recv_tensor.to_numpy()

    # Print output and error information
    print("host_output:", host_output)
    print("np_inputs:", np_inputs)
    max_error = np.max(np.abs(host_output - np_inputs))
    mean_error = np.mean(np.abs(host_output - np_inputs))
    print("max error:", max_error, "mean error:", mean_error)
    print("rank:", rank, "done")


def sendrecv_test_ping_pong():
    num_processes = world_size  # number of processes
    processes = []
    np_inputs = np.random.rand(tensor_len).astype(np.float16)

    # Create a process for each GPU
    for i in range(num_processes):
        process = multiprocessing.Process(
            target=sendrecv_test_ping_pong_function, args=(i, np_inputs)
        )
        process.start()
        processes.append(process)

    # Join the processes after completion
    for process in processes:
        process.join()


if __name__ == "__main__":
    sendrecv_test_ping_pong()

```

`examples/tutorial/quickstart_tutorial.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import numpy as np
import ark


def quickstart_tutorial():
    # Initialize the ARK runtime
    runtime = ark.Runtime()

    M, N = 64, 64
    # Create an input tensor
    input_tensor = ark.tensor([M, N])
    # Create another tensor
    other_tensor = ark.tensor([M, N])

    # Add the two tensors
    output_tensor = ark.add(input_tensor, other_tensor)

    # Launch the ARK runtime
    runtime.launch()

    # Initialize the input and other tensor with random values
    input_tensor_host = np.random.rand(M, N).astype(np.float32)
    input_tensor.from_numpy(input_tensor_host)
    other_tensor_host = np.random.rand(M, N).astype(np.float32)
    other_tensor.from_numpy(other_tensor_host)

    # Run the ARK program
    runtime.run()

    # Copy the output tensor from device memory to host memory, if dst is
    # None, a new numpy array of the same shape as the src tensor will be returned
    output_tensor_host = output_tensor.to_numpy()
    # Check if the output tensor is equal to the sum of the input and other tensor
    np.testing.assert_allclose(
        output_tensor_host, input_tensor_host + other_tensor_host
    )


if __name__ == "__main__":
    quickstart_tutorial()

```

`pyproject.toml`:

```toml
[build-system]
requires = ["scikit-build-core"]
build-backend = "scikit_build_core.build"

[project]
name = "ark"
version = "0.1.0"

[tool.scikit-build]
cmake.minimum-version = "3.25"
cmake.args = []
cmake.verbose = false
cmake.build-type = "Release"
wheel.packages = ["python/ark"]
build-dir = "build/{wheel_tag}"

[tool.scikit-build.cmake.define]
BUILD_PYTHON = "ON"

[tool.black]
line-length = 80
target-version = ['py38']
include = '\.pyi?$'
exclude = '/(\.eggs|\.git|\.hg|\.mypy_cache|\.nox|\.tox|\.venv|_build|buck-out|build|dist|third_party|docs)/'

```

`python/CMakeLists.txt`:

```txt
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

find_package(Python3 COMPONENTS Interpreter Development.Module REQUIRED)
include(FetchContent)
FetchContent_Declare(
    pybind11
    GIT_REPOSITORY https://github.com/pybind/pybind11.git
    GIT_TAG        v2.6.2
)
FetchContent_MakeAvailable(pybind11)

pybind11_add_module(ark_py ${CMAKE_CURRENT_SOURCE_DIR}/bindings.cpp)
set_target_properties(ark_py PROPERTIES OUTPUT_NAME _ark_core)
target_link_libraries(ark_py PRIVATE ark_static)

if (SKBUILD)
    install(TARGETS ark_py LIBRARY DESTINATION ark)
endif()

```

`python/ark/__init__.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import os

ark_root = os.environ.get("ARK_ROOT", None)
if ark_root is None:
    os.environ["ARK_ROOT"] = os.path.abspath(os.path.dirname(__file__))

from . import _ark_core

__version__ = _ark_core.version()

from ._ark_core import (
    init,
    srand,
    rand,
    Dims,
    TensorBuf,
    TensorType,
)

from .runtime import Runtime

from .tensor import Tensor, Parameter, FP16, FP32, INT32, BYTE

from .module import Module
from .executor import Executor
from .serialize import (
    save,
    load,
    convert_state_dict,
)

from .model import (
    Model,
    tensor,
    reshape,
    identity,
    sharding,
    reduce_sum,
    reduce_mean,
    reduce_max,
    layernorm,
    softmax,
    transpose,
    matmul,
    im2col,
    max_pool,
    scale,
    relu,
    gelu,
    add,
    mul,
    send,
    send_done,
    recv,
    send_mm,
    recv_mm,
    all_gather,
    all_reduce,
)


__all__ = [
    "init",
    "srand",
    "rand",
    "Dims",
    "TensorBuf",
    "TensorType",
    "Tensor",
    "Parameter",
    "FP16",
    "FP32",
    "INT32",
    "BYTE",
    "Runtime",
    "Module",
    "Executor",
    "save",
    "load",
    "convert_state_dict",
    "Optimizer",
    "Trainer",
    "Model",
    "tensor",
    "reshape",
    "identity",
    "sharding",
    "reduce_sum",
    "reduce_mean",
    "reduce_max",
    "layernorm",
    "softmax",
    "transpose",
    "matmul",
    "im2col",
    "max_pool",
    "scale",
    "relu",
    "gelu",
    "add",
    "mul",
    "send",
    "send_done",
    "recv",
    "send_mm",
    "recv_mm",
    "all_gather",
    "all_reduce",
]

```

`python/ark/executor.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from ._ark_core import _Executor, _Tensor
import numpy as np
import logging


class Executor(_Executor):
    """
    Convenience class for executing a model.
    """

    global_executor = None

    def __init__(
        self,
        gpu_id: int,
        rank: int,
        world_size: int,
        model,
        name: str,
        num_warps_per_sm: int = 16,
    ):
        super().__init__(
            gpu_id, rank, world_size, model, name, num_warps_per_sm
        )

    def tensor_memcpy_host_to_device(self, dst: _Tensor, src: np.ndarray):
        """
        Copy contiguous data from a host buffer to the given tensor's
        (possibly non-contiguous) data range on GPU.
        """
        if not isinstance(src, np.ndarray):
            logging.error("src is not a numpy array")
            raise TypeError("src is not a numpy array")
        # Check if src is contiguous is memory
        if not src.flags["C_CONTIGUOUS"]:
            logging.debug(
                "Warning: src is not contiguous in memory, copy to a contiguous array"
            )
            src = np.ascontiguousarray(src)
        super().tensor_memcpy_host_to_device(dst, src)

    def tensor_memcpy_device_to_host(self, dst: np.ndarray, src: _Tensor):
        """
        Copy (possibly non-contiguous) data from a tensor on GPU to a
        contiguous host buffer. The given number of bytes is copied, in
        order of appearance on the memory. This function assumes that
        `dst` is large enough to hold the data.
        """
        if not isinstance(dst, np.ndarray):
            logging.error("dst is not a numpy array")
            raise TypeError("dst is not a numpy array")
        if not dst.flags["C_CONTIGUOUS"]:
            logging.error("dst is not contiguous in memory")
            raise ValueError("dst is not contiguous in memory")
        super().tensor_memcpy_device_to_host(dst, src)

    @staticmethod
    def get_global_executor():
        """
        Get the global executor
        """
        if Executor.global_executor is None:
            logging.error("Executor is not initialized")
            raise RuntimeError("Executor is not initialized")
        return Executor.global_executor

```

`python/ark/model.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from ._ark_core import _Model, TensorBuf, TensorType, Dims
from .tensor import Tensor
import logging
from typing import List


def tensor(
    shape,
    dtype: TensorType = TensorType.FP32,
    buf: TensorBuf = None,
    ldims: Dims = Dims(),
    offs: Dims = Dims(),
    pads: Dims = Dims(),
    deps: list = [],
    exported: bool = False,
    imported: bool = False,
    name: str = "tensor",
) -> Tensor:
    """
    Construct a tensor with given shape and data type.
    Usage:
    tensor = ark.tensor([1, 2, 3, 4], dtype=TensorType.FP32)
    tensor = ark.tensor(ark.Dims(1, 2), dtype=TensorType.FP16)
    """
    # if shape is a list of integers, convert it to a Dims object
    if isinstance(shape, list):
        # only support tensors with up to 4 dimensions
        if len(shape) > 4:
            logging.error("Only support tensors with up to 4 dimensions")
            raise ValueError("Only support tensors with up to 4 dimensions")
        shape = Dims(*shape)
    _tensor = Model.get_global_model().tensor(
        shape, dtype, buf, ldims, offs, pads, deps, exported, imported, name
    )
    return Tensor(_tensor)


def reshape(
    input: Tensor,
    shape: list,
    allowzero: bool = False,
    output: Tensor = None,
    name: str = "reshape",
) -> Tensor:
    """
    Reshape `input` to `shape`. If one dimension of `shape` is -1, it will
    be inferred from the `input`. If one dimension of `shape` is 0,
    by default (`allowzero` is false), that dimension is unchanged from
    the corresponding one of `input`. If `allowzero` is true, that dimension
    is set to 0, which means that the reshaped tensor is an empty tensor,
    i.e., `input` should also be an empty tensor. If `allowzero` is true,
    `shape` should not include both 0 and -1 at the same time. If `shape`
    is an empty vector, `input` will be converted to a scalar.
    Usage:
    # tensors shape is [128, 64]
    tensor = ark.reshape(tensor, [2, 64, 64])
    """
    if isinstance(shape, list):
        # only support tensors with up to 4 dimensions
        if len(shape) > 4:
            logging.error("Only support tensors with up to 4 dimensions")
            raise ValueError("Only support tensors with up to 4 dimensions")
        shape = Dims(*shape)
    if output is not None:
        output = output._tensor
    input = input._tensor
    _tensor = Model.get_global_model().reshape(
        input, shape, allowzero, output, name
    )
    return Tensor(_tensor)


def identity(
    input: Tensor,
    deps: List[Tensor] = [],
    output: Tensor = None,
    name: str = "identity",
) -> Tensor:
    """
    Returns an identical tensor of `input` with execution dependencies `deps`.
    Usage:
    tensor_identity = ark.identity(tensor, deps=[tensor1, tensor2])
    """
    dep_tensor = []
    for dep in deps:
        if not isinstance(dep, Tensor):
            logging.error("All dependencies should be tensors")
            raise TypeError("All dependencies should be tensors")
        dep_tensor.append(dep._tensor)
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().identity(
        input._tensor, dep_tensor, output, name
    )
    return Tensor(_tensor)


def sharding(
    input: Tensor,
    axis: int,
    dim_per_shard: int,
    name: str = "sharding",
) -> List[Tensor]:
    """
    Shard `input` along `axis` into `dim_per_shard`-dimensional shards.
    Usage:
    # tensors shape is [64, 128]
    tensor_sharding = ark.sharding(tensor, axis=1, dim_per_shard=64)
    # tensor_sharding is a list of 2 tensors, each of which has shape [64, 64]
    # The first tensor's buffer is the same as the first 64 columns of tensor
    # The second tensor's buffer is the same as the last 64 columns of tensor
    """
    _tensor_list = Model.get_global_model().sharding(
        input._tensor, axis, dim_per_shard, name
    )
    tensor_list = []
    for _tensor in _tensor_list:
        tensor_list.append(Tensor(_tensor))
    return tensor_list


def reduce_sum(
    input: Tensor,
    axis: int,
    output: Tensor = None,
    name: str = "reduce_sum",
) -> Tensor:
    """
    Performs reduction along the `axis` of the `input` tensor and
    stores the result in `output`.
    Usage:
    # tensors shape is [64, 128]
    tensor_reduce_sum = ark.reduce_sum(tensor, axis=1)
    # tensor_reduce_sum is a tensor with shape [64, 1]
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().reduce_sum(
        input._tensor, axis, output, name
    )
    return Tensor(_tensor)


def reduce_mean(
    input: Tensor,
    axis: int,
    output: Tensor = None,
    name: str = "reduce_mean",
) -> Tensor:
    """
    Performs reduction along the `axis` of the `input` tensor and
    stores the result in `output`.
    Usage:
    tensor_reduce_mean = ark.reduce_mean(tensor, axis=1)
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().reduce_mean(
        input._tensor, axis, output, name
    )
    return Tensor(_tensor)


def reduce_max(
    input: Tensor,
    axis: int,
    output: Tensor = None,
    name: str = "reduce_max",
) -> Tensor:
    """
    Performs reduction along the `axis` of the `input` tensor and
    stores the result in `output`.
    Usage:
    tensor_reduce_max = ark.reduce_max(tensor, axis=1)
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().reduce_max(
        input._tensor, axis, output, name
    )
    return Tensor(_tensor)


def layernorm(
    input: Tensor,
    output: Tensor = None,
    name: str = "layernorm",
) -> Tensor:
    """
    Applies layer normalization to the `input` tensor and returns
    the normalized tensor as `output`.
    Usage:
    tensor_layernorm = ark.layernorm(tensor)
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().layernorm(input._tensor, output, name)
    return Tensor(_tensor)


def softmax(
    input: Tensor,
    output: Tensor = None,
    name: str = "softmax",
) -> Tensor:
    """
    Applies softmax  to the `input` tensor on the last dimension.
    Usage:
    tensor_softmax = ark.softmax(tensor)
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().softmax(input._tensor, output, name)
    return Tensor(_tensor)


def transpose(
    input: Tensor,
    perm: list,
    output: Tensor = None,
    name: str = "transpose",
) -> Tensor:
    """
    Transposes the `input` tensor according to the given `perm` permutation.
    For example, transpose(input, [0, 1 ,3, 2]) will swap the last two
    dimensions of the input tensor. Currently, only 4D tensors are supported.
    Usage:
    # tensors shape is [1, 64, 128, 32]
    tensor_transpose = ark.transpose(tensor, perm=[0, 1, 3, 2])
    # tensor_transpose is a tensor with shape [1, 64, 32, 128]
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().transpose(
        input._tensor, perm, output, name
    )
    return Tensor(_tensor)


def matmul(
    input: Tensor,
    other: Tensor,
    output: Tensor = None,
    splitk: int = 1,
    transpose_a: bool = False,
    transpose_b: bool = False,
    is_relu: bool = False,
    name: str = "matmul",
    gran_lev: int = -1,
) -> Tensor:
    """
    Performs matrix multiplication between the `input` tensor and
    `other` tensor, storing the result in `output`. Optional
    parameters allow controlling the behavior of the multiplication,
    such as transposing the input tensors and applying a ReLU
    activation.
    Usage:
    tensor_matmul = ark.matmul(tensor1, tensor2)
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().matmul(
        input._tensor,
        other._tensor,
        output,
        splitk,
        transpose_a,
        transpose_b,
        is_relu,
        name,
        gran_lev,
    )
    return Tensor(_tensor)


def im2col(
    input: Tensor,
    kernel_height: int,
    kernel_width: int,
    stride_height: int,
    stride_width: int,
    pad_height: int,
    pad_width: int,
    dilation_height: int,
    dilation_width: int,
    output: Tensor = None,
    name: str = "im2col",
) -> Tensor:
    """
    Implements the 'im2col' method for 2D convolution layers, which
    takes an `input` tensor and reshapes it to a 2D matrix by
    extracting image patches from the input tensor based on the
    provided parameters.
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().im2col(
        input._tensor,
        kernel_height,
        kernel_width,
        stride_height,
        stride_width,
        pad_height,
        pad_width,
        dilation_height,
        dilation_width,
        output,
        name,
    )
    return Tensor(_tensor)


def conv2d(
    input: Tensor,
    in_channels: int,
    out_channels: int,
    kernel_size: list,
    stride: list,
    padding: list,
    bias: bool = False,
    output: Tensor = None,
    name: str = "conv2d",
) -> Tensor:
    """
    Implements a 2D convolution layer using the 'im2col' method.
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().conv2d(
        input._tensor,
        in_channels,
        out_channels,
        kernel_size,
        stride,
        padding,
        bias,
        output,
        name,
    )
    return Tensor(_tensor)


def max_pool(
    input: Tensor,
    kernel_size: int,
    stride: int,
    output: Tensor = None,
    name: str = "max_pool",
) -> Tensor:
    """
    Applies max-pooling on the `input` tensor using `kernel_size`
    and `stride`, reducing its spatial size. The output shape is
    calculated based on the input tensor's shape and the stride
    value as follows: {is[0], (is[1] + stride - 1) / stride, (is[2]
    + stride - 1) / stride, is[3]}, where 'is' represents the input
    tensor's shape.
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().max_pool(
        input._tensor,
        kernel_size,
        stride,
        output,
        name,
    )
    return Tensor(_tensor)


def scale(
    input: Tensor,
    val: float,
    output: Tensor = None,
    name: str = "scale",
) -> Tensor:
    """
    Multiplies the `input` tensor by a scalar `val`, element-wise.
    Usage:
    tensor_scale = ark.scale(tensor, 1.6)
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().scale(
        input._tensor,
        val,
        output,
        name,
    )
    return Tensor(_tensor)


def relu(
    input: Tensor,
    output: Tensor = None,
    name: str = "relu",
) -> Tensor:
    """
    Applies the ReLU activation function to the `input` tensor,
    element-wise.
    Usage:
    tensor_relu = ark.relu(tensor)
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().relu(input._tensor, output, name)
    return Tensor(_tensor)


def gelu(
    input: Tensor,
    output: Tensor = None,
    name: str = "gelu",
) -> Tensor:
    """
    Applies the Gaussian Error Linear Unit (GELU) activation
    function to the `input` tensor, element-wise. GELU is a smooth
    approximation of the rectifier function and is widely used in
    deep learning models.
    Usage:
    tensor_gelu = ark.gelu(tensor)
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().gelu(input._tensor, output, name)
    return Tensor(_tensor)


def add(
    input: Tensor,
    other: Tensor,
    output: Tensor = None,
    name: str = "add",
) -> Tensor:
    """
    Performs an element-wise addition operator between the `input`
    tensor and the `other` tensor.
    Usage:
    tensor_add = ark.add(tensor1, tensor2)
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().add(
        input._tensor, other._tensor, output, name
    )
    return Tensor(_tensor)


def mul(
    input: Tensor,
    other: Tensor,
    output: Tensor = None,
    name: str = "mul",
) -> Tensor:
    """
    Performs an element-wise multiplication operator between the
    `input` tensor and the `other` tensor.
    Usage:
    tensor_mul = ark.mul(tensor1, tensor2)
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().mul(
        input._tensor, other._tensor, output, name
    )
    return Tensor(_tensor)


def send(
    input: Tensor,
    id: int,
    dst_rank: int,
    bytes: int = 0,
    output: Tensor = None,
    name: str = "send",
) -> Tensor:
    """
    Sends a tensor to a destination GPU (`dst_rank`). Multiple
    tensors can be sent to the same GPU, so an identifier `id` is
    required to distinguish the tensor. Each 'send' operator must
    have a corresponding 'recv' operator that have the same id in
    another GPU's model.
    Usage:
    # on GPU0:
    ark.send(tensor_send, 1, 1)
    ark.send_done(tensor_send, 1, 1)
    # on GPU1:
    ark.recv(tensor, 1, 0)
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().send(
        input._tensor,
        id,
        dst_rank,
        bytes,
        output,
        name,
    )
    return Tensor(_tensor)


def send_done(
    input: Tensor,
    id: int,
    dst_rank: int,
    output: Tensor = None,
    name: str = "send_done",
) -> Tensor:
    """
    Blocks the execution until the corresponding 'send' operator
    with the specified `id` is completed.
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().send_done(
        input._tensor,
        id,
        dst_rank,
        output,
        name,
    )
    return Tensor(_tensor)


def recv(
    input: Tensor,
    id: int,
    src_rank: int,
    bytes: int = 0,
    output: Tensor = None,
    name: str = "recv",
) -> Tensor:
    """
    Receives a tensor from a source GPU (`src_rank`), identified by
    the `id` parameter. Blocks the execution until the corresponding
    'recv' operator is completed.
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().recv(
        input._tensor,
        id,
        src_rank,
        bytes,
        output,
        name,
    )
    return Tensor(_tensor)


def send_mm(
    input: Tensor,
    id: int,
    gpu_dst: int,
    bytes: int = 0,
    output: Tensor = None,
    name: str = "send_mm",
) -> Tensor:
    """
    Similar to the 'send_done' function, but implemented using CUDA
    in-stream RDMA copy and Low Latency (LL) protocol.
    Usage:
    # on GPU0:
    ark.send_mm(tensor_send, 1, 1)
    # on GPU1:
    ark.recv_mm(tensor, 1, 0)
    """
    if output is not None:
        output = output._tensor

    _tensor = Model.get_global_model().send_mm(
        input._tensor,
        id,
        gpu_dst,
        bytes,
        output,
        name,
    )
    return Tensor(_tensor)


def recv_mm(
    input: Tensor,
    id: int,
    gpu_src: int,
    bytes: int = 0,
    output: Tensor = None,
    name: str = "recv_mm",
) -> Tensor:
    """
    Similar to the 'recv' function, but implemented using CUDA
    in-stream RDMA copy and Low Latency (LL) protocol.
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().recv_mm(
        input._tensor,
        id,
        gpu_src,
        bytes,
        output,
        name,
    )
    return Tensor(_tensor)


def all_gather(
    input: Tensor,
    gpu_id: int,
    gpu_num: int,
    output: List[Tensor] = [],
    name: str = "all_gather",
) -> List[Tensor]:
    """
    Performs an all-gather operator across all GPUs.
    Usage:
    # all-gather
    ark.init(rank, world_size)
    input_tensor = ark.tensor([tensor_len], ark.TensorType.FP16)
    # The all_gather operation will create the recv tensor shards and return
    them as a list. The allgather_result[rank] is the same as input_tensor
    allgather_result = ark.all_gather(input_tensor, rank, world_size)

    # in-place all-gather
    ark.init(rank, world_size)
    output_tensor = ark.tensor(
        [tensor_len * world_size], ark.TensorType.FP16
    )
    # Shard the output tensor into world_size shards
    output_shard = ark.sharding(output_tensor, 0, tensor_len)
    # The input tensor is the rank'th shard of the output tensor
    input_tensor = output_shard[rank]
    allgather_result = ark.all_gather(
        input_tensor, rank, world_size, output_shard
    )
    """
    for output_shard in output:
        if output_shard is not None:
            output_shard = output_shard._tensor
    tensor_shards = Model.get_global_model().all_gather(
        input._tensor,
        gpu_id,
        gpu_num,
        output,
        name,
    )
    return [Tensor(_tensor) for _tensor in tensor_shards]


def all_reduce(
    input: Tensor,
    gpu_id: int,
    gpu_num: int,
    output: Tensor = None,
    name: str = "all_reduce",
) -> Tensor:
    """
    Performs an all-reduce operator across all GPUs, aggregating the
    input tensors. Takes the `input` tensor, the current GPU's
    `gpu_id`, and the total number of GPUs `gpu_num`.
    Usage:
    ark.init(rank, world_size)
    input_tensor = ark.tensor([tensor_len], ark.TensorType.FP16)
    allreduce_result = ark.all_reduce(input_tensor, rank, world_size)
    """
    if output is not None:
        output = output._tensor
    _tensor = Model.get_global_model().all_reduce(
        input._tensor,
        gpu_id,
        gpu_num,
        output,
        name,
    )
    return Tensor(_tensor)


class Model(_Model):
    """
    The Model class will record the all operators and tensors defined
    by the user.
    """

    # A global model object
    global_model = None

    @staticmethod
    def get_global_model():
        if Model.global_model is None:
            logging.error("Model is not initialized")
            raise RuntimeError("Model is not initialized")
        return Model.global_model

```

`python/ark/module.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from .tensor import Tensor
import logging
import numpy as np
from typing import Dict, Callable, Any


class Module:
    """
    Base class for all neural network modules.
    """

    # The submodules of the module.
    sub_modules: Dict[str, "Module"]
    # The parameters of the module.
    parameters: Dict[str, Tensor]

    def __init__(self):
        self.sub_modules = dict()
        self.parameters = dict()

    # Adds a child module to the current module.
    def register_module(self, name: str, module: "Module") -> None:
        if not isinstance(module, Module):
            logging.error("module must be a Module")
            raise TypeError("module must be a Module")
        self.sub_modules[name] = module

    # Adds a parameter to the module.
    def register_parameter(self, name: str, param: Tensor) -> None:
        if not isinstance(param, Tensor):
            logging.error("param must be a Tensor")
            raise TypeError("param must be a Tensor")
        self.parameters[name] = param

    def __setattr__(self, __name: str, __value: Any) -> None:
        """
        When setting an attribute, if the attribute is a Module, add it to
        the sub_modules. If the attribute is a Tensor and this Tensor is a
        parameter, add it to the parameters.
        """
        if isinstance(__value, Module):
            self.register_module(__name, __value)
        elif isinstance(__value, Tensor):
            if __value.is_parameter:
                self.register_parameter(__name, __value)
        super().__setattr__(__name, __value)

    def load_state_dict(self, state_dict, prefix=""):
        """
        Loads a model from a state_dict and copy the parameters to the device GPU.
        Must be called after the executor is launched.
        """
        logging.info("Loading model from state_dict")
        for name, module in self.sub_modules.items():
            if module is not None:
                module.load_state_dict(state_dict, prefix=prefix + name + ".")
        for name, param in self.parameters.items():
            param.from_numpy(state_dict[prefix + name])

    def state_dict(self, prefix=""):
        """
        Copies the parameters from the device GPU to the host and saves the model to a state_dict.
        Must be called after the executor is launched.
        """
        state_dict = {}
        for name, module in self.sub_modules.items():
            if module is not None:
                state_dict.update(module.state_dict(prefix=prefix + name + "."))
        for name, param in self.parameters.items():
            param_np = param.to_numpy()
            state_dict[prefix + name] = param_np
        return state_dict

    forward: Callable[..., Any] = NotImplemented
    backward: Callable[..., Any] = NotImplemented

    def __call__(self, *args: Any, **kwds: Any) -> Any:
        return self.forward(*args, **kwds)

```

`python/ark/runtime.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from .model import Model
from .executor import Executor
import logging
from enum import Enum

# Use a global variable to track the state of the ARK runtime


class RuntimeState(Enum):
    init = 0
    launch = 1
    run = 2
    stop = 3
    destroy = 4


class Runtime:
    global_runtime = None

    def __init__(self, rank=0, world_size=1):
        self.rank = rank
        self.world_size = world_size
        self.ark_runtime_state = RuntimeState.init
        Model.global_model = Model(rank)

    def __del__(self):
        """
        Destroy the ARK runtime and release all the resources.
        """
        if (
            self.ark_runtime_state == RuntimeState.run
            or self.ark_runtime_state == RuntimeState.launch
        ):
            self.stop()
        self.ark_runtime_state = RuntimeState.destroy
        Executor.global_executor = None
        Model.global_model = None

    @staticmethod
    def get_global_runtime():
        """
        Get the global ARK runtime.
        """
        if Runtime.global_runtime is None:
            logging.error("ARK runtime is not initialized")
            raise RuntimeError("ARK runtime is not initialized")
        return Runtime.global_runtime

    def launch(self):
        """
        Create an executor and schedule the ARK model. The scheduler will generate
        the CUDA kernels. The GPU context and the connection between GPUs will be
        initialized. The executor will compile the cuda kernels and launch the ARK runtime.
        """
        if (
            self.ark_runtime_state != RuntimeState.init
            and self.ark_runtime_state != RuntimeState.stop
        ):
            logging.warn(
                "ARK runtime is not initialized or already launched, skip launching"
            )
            return
        # If the RuntimeState is init, we need to create a new executor and
        # compile the kernels
        if self.ark_runtime_state == RuntimeState.init:
            Executor.global_executor = Executor(
                self.rank,
                self.rank,
                self.world_size,
                Model.get_global_model(),
                "Executor",
            )
            Executor.get_global_executor().compile()
        Executor.get_global_executor().launch()
        self.ark_runtime_state = RuntimeState.launch

    def run(self, iter=1, async_run=False):
        """
        Run the ARK program for iter iterations and wait for the kernel to finish.
        """
        if self.ark_runtime_state != RuntimeState.launch:
            logging.error("ARK runtime is not launched")
            raise RuntimeError("ARK runtime is not launched")
        self.ark_runtime_state = RuntimeState.run
        Executor.get_global_executor().run(iter)
        if not async_run:
            self.wait()

    def wait(self):
        """
        Wait for the kernel to finish.
        """
        if self.ark_runtime_state != RuntimeState.run:
            logging.warn("ARK runtime is not running, skip waiting")
            return
        Executor.get_global_executor().wait()
        self.ark_runtime_state = RuntimeState.launch

    def stop(self):
        """
        Stop the model and return the elapsed time in milliseconds.
        Once this is called, we need to call `launch()` again to run the model again.
        """
        if (
            self.ark_runtime_state != RuntimeState.run
            and self.ark_runtime_state != RuntimeState.launch
        ):
            logging.warn(
                "ARK runtime is not running or launched, skip stopping"
            )
            return
        Executor.get_global_executor().stop()
        self.ark_runtime_state = RuntimeState.stop

```

`python/ark/serialize.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import torch
import pickle
import logging


def save(state_dict, state_dict_file_path: str):
    """
    Save the state_dict of a module to a file
    """
    if not isinstance(state_dict, dict):
        logging.warn(
            "Warning: Invalid state_dict saved to", state_dict_file_path
        )
    with open(state_dict_file_path, "wb") as f:
        pickle.dump(state_dict, f)


def load(state_dict_file_path: str):
    """
    Load the state_dict of a module from a file
    """
    with open(state_dict_file_path, "rb") as f:
        state_dict = pickle.load(f)
        if not isinstance(state_dict, dict):
            logging.warn("Warning: Invalid state_dict file")
        return state_dict


def convert_state_dict(state_dict: dict, type="numpy"):
    """
    Convert the state_dict of a module to np.ndarray or torch.Tensor type
    """
    new_state_dict = {}
    for key in state_dict:
        if type == "torch":
            new_state_dict[key] = torch.from_numpy(state_dict[key])
        elif type == "numpy":
            new_state_dict[key] = state_dict[key].numpy()
        else:
            logging.error(
                "Invalid type: " + type + " valid types are torch and numpy"
            )
            raise TypeError(
                "Invalid type: " + type + " valid types are torch and numpy"
            )
    return new_state_dict

```

`python/ark/tensor.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from ._ark_core import _Tensor, TensorType
from . import executor
import numpy as np
import logging


class Tensor:
    def __init__(self, _tensor: _Tensor):
        """
        Initializes a new instance of the Tensor class.
        Args:
            _tensor (_ark_core._Tensor): The underlying _Tensor object.
        """
        self._tensor = _tensor
        self.shape = _tensor.shape
        self.is_parameter = False

    def offset(self, i0=0, i1=0, i2=0, i3=0):
        """
        Returns the offset of the tensor at the specified indices.
        """
        return self._tensor.offset(i0, i1, i2, i3)

    def size(self):
        """
        Returns the number of elements in the tensor excluding padding.
        """
        return self._tensor.size()

    def ndims(self):
        """
        Returns the number of dimensions in the tensor.
        """
        return self._tensor.ndims()

    def padded_shape(self):
        """
        Returns the shape of the tensor including padding.
        """
        return self._tensor.padded_shape()

    def type_bytes(self):
        """
        Returns the number of bytes of each element in the tensor.
        """
        return self._tensor.type_bytes()

    def shape_bytes(self):
        """
        Returns the number of bytes of the tensor.
        """
        return self._tensor.shape_bytes()

    def ldims_bytes(self):
        """
        Returns the number of bytes of the TensorBuf.
        """
        return self._tensor.ldims_bytes()

    def offset_bytes(self, i0=0, i1=0, i2=0, i3=0):
        """
        Returns the offset of the tensor at the specified indices in bytes.
        """
        return self._tensor.offset_bytes(i0, i1, i2, i3)

    def tensor_type(self):
        """
        Returns the type of the tensor.
        """
        return self._tensor.type

    def to_numpy(self, ndarray: np.ndarray = None):
        """
        Copy a tensor from device to host. If dst is None, a new numpy array will be created.
        """
        # Create a new numpy array if dst is None
        if ndarray is None:
            np_type = None
            if self.tensor_type() == TensorType.FP32:
                np_type = np.float32
            elif self.tensor_type() == TensorType.FP16:
                np_type = np.float16
            else:
                logging.error("Unsupported tensor type")
                raise TypeError("Unsupported tensor type")
            ndarray = np.empty(self.shape, dtype=np_type)
        executor.Executor.get_global_executor().tensor_memcpy_device_to_host(
            ndarray, self._tensor
        )
        return ndarray

    def from_numpy(self, ndarray: np.ndarray):
        """
        Copies the tensor from a host numpy array to the device.
        """
        executor.Executor.get_global_executor().tensor_memcpy_host_to_device(
            self._tensor, ndarray
        )
        return self


def Parameter(
    tensor: Tensor,
) -> Tensor:
    """
    Set the tensor as a parameter.

    Args:
        tensor (Tensor): The tensor to set as a parameter.

    Returns:
        Tensor: The input tensor marked as a parameter.
    """
    tensor.is_parameter = True
    return tensor


FP16 = TensorType.FP16
FP32 = TensorType.FP32
INT32 = TensorType.INT32
BYTE = TensorType.BYTE

```

`python/bindings.cpp`:

```cpp
// Copyright (c) Microsoft Corporation.
// Licensed under the MIT license.

#include "ark.h"
#include <iostream>
#include <pybind11/operators.h>
#include <pybind11/pybind11.h>
#include <pybind11/stl.h>
#include <sstream>

namespace py = pybind11;

void tensor_memcpy_host_to_device(ark::Executor *executor, ark::Tensor *tns,
                                  py::buffer host_buffer)
{
    py::buffer_info info = host_buffer.request();
    size_t bytes = tns->shape_bytes();
    void *host_buffer_ptr = info.ptr;
    executor->tensor_memcpy(tns, (const void *)host_buffer_ptr, bytes);
}

void tensor_memcpy_device_to_host(ark::Executor *executor,
                                  py::buffer host_buffer, ark::Tensor *tns)
{
    py::buffer_info info = host_buffer.request();
    size_t bytes = tns->shape_bytes();
    void *host_buffer_ptr = info.ptr;
    executor->tensor_memcpy((void *)host_buffer_ptr, tns, bytes);
}

PYBIND11_MODULE(_ark_core, m)
{
    m.doc() = "ARK python module interface";

    m.def("version", &ark::version, "Return a version string.");

    m.def("init", &ark::init,
          "Init an ark program. Call this function to clean up the shared "
          "memory directory. This is useful when the previous run crashed, as "
          "this forces to remove locks generated by previous runs. This may "
          "crash other ARK processes running on the same machine, if there are "
          "any.");

    m.def("srand", &ark::srand, py::arg("seed") = -1,
          "Sets the seed for the random number generator");
    m.def("rand", &ark::rand, "Generates a random integer");
    m.attr("DIMS_LEN") = py::int_(static_cast<int>(ark::DIMS_LEN));
    m.attr("NO_DIM") = py::int_(static_cast<int>(ark::NO_DIM));

    py::class_<ark::Dims>(m, "Dims", "Up-to-`DIMS_LEN`-dimensional vector.")
        .def(py::init([](ark::DimType d0, ark::DimType d1, ark::DimType d2,
                         ark::DimType d3) {
                 return std::make_unique<ark::Dims>(d0, d1, d2, d3);
             }),
             py::arg_v("d0", static_cast<int>(ark::NO_DIM),
                       "default value: NO_DIM"),
             py::arg_v("d1", static_cast<int>(ark::NO_DIM),
                       "default value: NO_DIM"),
             py::arg_v("d2", static_cast<int>(ark::NO_DIM),
                       "default value: NO_DIM"),
             py::arg_v("d3", static_cast<int>(ark::NO_DIM),
                       "default value: NO_DIM"))
        .def(py::init<const ark::Dims &>(), "Copy another Dims object.")
        .def(py::init<const std::vector<ark::DimType> &>(),
             "Construct from a vector. If the vector is shorter than DIMS_LEN, "
             "put following NO_DIMs. Raise an error if the vector is longer "
             "than DIMS_LEN.")
        .def("size", &ark::Dims::size,
             "Return the volume of dimensions. If the dimensions are invalid, "
             "return -1")
        .def("ndims", &ark::Dims::ndims,
             "Return the number of valid dimensions.")
        .def("__getitem__",
             [](const ark::Dims &d, ark::DimType idx) { return d[idx]; })
        .def("__setitem__", [](ark::Dims &d, ark::DimType idx,
                               ark::DimType value) { d[idx] = value; })
        .def("__repr__", [](const ark::Dims &d) {
            std::ostringstream os;
            os << d;
            return os.str();
        });

    py::enum_<ark::TensorType>(
        m, "TensorType", "Type of tensor data. FP16, FP32, INT32, or BYTE")
        .value("FP16", ark::TensorType::FP16)
        .value("FP32", ark::TensorType::FP32)
        .value("INT32", ark::TensorType::INT32)
        .value("BYTE", ark::TensorType::BYTE)
        .export_values();

    py::class_<ark::TensorBuf>(m, "TensorBuf",
                               "TensorBuf refers to a data array that can be "
                               "shared by multiple tensors.")
        .def(py::init<const ark::DimType &, int>(), py::arg("bytes") = 0,
             py::arg("id") = -1)
        .def_readwrite("bytes", &ark::TensorBuf::bytes)
        .def_readwrite("id", &ark::TensorBuf::id)
        .def_readwrite("immutable", &ark::TensorBuf::immutable);

    py::class_<ark::Tensor>(m, "_Tensor")
        .def(py::init<const ark::Dims &, ark::TensorType, ark::TensorBuf *,
                      const ark::Dims &, const ark::Dims &, const ark::Dims &,
                      bool, int, int, const std::string &>(),
             py::arg("shape"), py::arg("type"), py::arg("buf"),
             py::arg("ldims"), py::arg("offs"), py::arg("pads"),
             py::arg("exported"), py::arg("imported_rank"), py::arg("id"),
             py::arg("name"))
        .def_property_readonly("shape",
                               [](const ark::Tensor &t) {
                                   py::list shape_list;
                                   for (int i = 0; i < t.ndims(); ++i) {
                                       shape_list.append((int)t.shape[i]);
                                   }
                                   return shape_list;
                               })
        .def_property_readonly("type",
                               [](const ark::Tensor &t) { return t.type; })
        .def("offset", &ark::Tensor::offset, py::arg("i0") = 0,
             py::arg("i1") = 0, py::arg("i2") = 0, py::arg("i3") = 0)
        .def("size", &ark::Tensor::size,
             "Number of elements in the tensor excluding padding.")
        .def("ndims", &ark::Tensor::ndims,
             "Number of dimensions in the tensor.")
        .def("padded_shape", &ark::Tensor::padded_shape,
             "Shape of the tensor including padding.")
        .def("type_bytes", &ark::Tensor::type_bytes,
             "Number of bytes of each element in the tensor.")
        .def("shape_bytes", &ark::Tensor::shape_bytes,
             "Number of bytes of the tensor.")
        .def("ldims_bytes", &ark::Tensor::ldims_bytes,
             "Should be the same as the number of bytes of the TensorBuf.")
        .def("offset_bytes", &ark::Tensor::offset_bytes, py::arg("i0") = 0,
             py::arg("i1") = 0, py::arg("i2") = 0, py::arg("i3") = 0);

    py::class_<ark::Model>(m, "_Model")
        .def(py::init<int>(), py::arg("rank") = 0)
        .def("tensor", &ark::Model::tensor,
             "construct a tensor with given shape and data type.",
             py::return_value_policy::reference_internal, py::arg("shape"),
             py::arg("dtype"), py::arg("buf") = nullptr,
             py::arg("ldims") = ark::Dims(), py::arg("offs") = ark::Dims(),
             py::arg("pads") = ark::Dims(),
             py::arg("deps") = std::vector<ark::Tensor *>(),
             py::arg("exported") = false, py::arg("imported_rank") = -1,
             py::arg("name") = "tensor")
        .def("reshape",
             (ark::Tensor * (ark::Model::*)(ark::Tensor *, const ark::Dims &,
                                            bool, ark::Tensor *,
                                            const std::string &)) &
                 ark::Model::reshape,
             "Reshape `input` to `shape`. If one dimension of `shape` is -1, "
             "it will be inferred from the `input`. If one dimension of "
             "`shape` is 0, by default (`allowzero` is false), that dimension "
             "is unchanged from the corresponding one of `input`. If "
             "`allowzero` is true, that dimension is set to 0, which means "
             "that the reshaped tensor is an empty tensor, i.e., `input` "
             "should also be an empty tensor. If `allowzero` is true, `shape` "
             "should not include both 0 and -1 at the same time. If `shape` is "
             "an empty vector, `input` will be converted to a scalar.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("shape"), py::arg("allowzero") = false,
             py::arg("output") = nullptr, py::arg("name") = "reshape")
        .def("reshape",
             (ark::Tensor * (ark::Model::*)(ark::Tensor *,
                                            std::initializer_list<ark::DimType>,
                                            bool, ark::Tensor *,
                                            const std::string &)) &
                 ark::Model::reshape,
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("shape"), py::arg("allowzero") = false,
             py::arg("output") = nullptr, py::arg("name") = "reshape")
        .def("identity", &ark::Model::identity,
             "Returns an identical tensor of `input` with execution "
             "dependencies `deps`.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("deps") = std::vector<ark::Tensor *>(),
             py::arg("name") = "identity")
        .def("sharding", &ark::Model::sharding,
             "Shard `input` along `axis` into `dim_per_shard`-dimensional "
             "shards.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("axis"), py::arg("dim_per_shard"),
             py::arg("name") = "sharding")
        .def("reduce_sum", &ark::Model::reduce_sum,
             "Performs reduction along the `axis` of the `input` tensor and "
             "stores the result in `output`.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("axis"), py::arg("output") = nullptr,
             py::arg("name") = "reduce_sum")
        .def("reduce_mean", &ark::Model::reduce_mean,
             "Performs reduction along the `axis` of the `input` tensor and "
             "stores the result in `output`.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("axis"), py::arg("output") = nullptr,
             py::arg("name") = "reduce_mean")
        .def("reduce_max", &ark::Model::reduce_max,
             "Performs reduction along the `axis` of the `input` tensor and "
             "stores the result in `output`.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("axis"), py::arg("output") = nullptr,
             py::arg("name") = "reduce_max")
        .def("layernorm", &ark::Model::layernorm,
             "Applies layer normalization to the `input` tensor and returns "
             "the normalized tensor as `output`.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("output") = nullptr, py::arg("name") = "layernorm")
        .def("softmax", &ark::Model::softmax,
             "Applies softmax activation to the `input` tensor, with the "
             "softmax operator being performed on the last dimension of the "
             "input tensor.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("output") = nullptr, py::arg("name") = "softmax")
        .def("transpose", &ark::Model::transpose,
             "Transposes the `input` tensor according to the given `perm` "
             "permutation. For example, transpose(input, {0, 1 ,3, 2}) will "
             "swap the last two dimensions of the input tensor. Currently, "
             "only 4D tensors are supported.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("perm"), py::arg("output") = nullptr,
             py::arg("name") = "transpose")
        .def("matmul", &ark::Model::matmul,
             "Performs matrix multiplication between the `input` tensor and "
             "`other` tensor, storing the result in `output`. Optional "
             "parameters allow controlling the behavior of the multiplication, "
             "such as transposing the input tensors and applying a ReLU "
             "activation.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("other"), py::arg("output") = nullptr,
             py::arg("splitk") = 1, py::arg("trans_input") = false,
             py::arg("trans_other") = false, py::arg("is_relu") = false,
             py::arg("name") = "matmul", py::arg("gran_lev") = -1)
        .def("im2col", &ark::Model::im2col,
             "Implements the 'im2col' method for 2D convolution layers, which "
             "takes an `input` tensor and reshapes it to a 2D matrix by "
             "extracting image patches from the input tensor based on the "
             "provided parameters.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("kernel_height"), py::arg("kernel_width"),
             py::arg("stride_height"), py::arg("stride_width"),
             py::arg("pad_height"), py::arg("pad_width"),
             py::arg("dilation_height"), py::arg("dilation_width"),
             py::arg("output") = nullptr, py::arg("name") = "im2col")
        .def("max_pool", &ark::Model::max_pool,
             "Applies max-pooling on the `input` tensor using `kernel_size` "
             "and `stride`, reducing its spatial size. The output shape is "
             "calculated based on the input tensor's shape and the stride "
             "value as follows: {is[0], (is[1] + stride - 1) / stride, (is[2] "
             "+ stride - 1) / stride, is[3]}, where 'is' represents the input "
             "tensor's shape.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("kernel_size"), py::arg("stride"),
             py::arg("output") = nullptr, py::arg("name") = "max_pool")
        .def("scale", &ark::Model::scale,
             "Multiplies the `input` tensor by a scalar `val`, element-wise.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("val"), py::arg("output") = nullptr,
             py::arg("name") = "scale")
        .def("relu", &ark::Model::relu, "ReLU activation",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("output") = nullptr, py::arg("name") = "relu")
        .def("gelu", &ark::Model::gelu,
             "Applies the Gaussian Error Linear Unit (GELU) activation "
             "function to the `input` tensor, element-wise. GELU is a smooth "
             "approximation of the rectifier function and is widely used in "
             "deep learning models.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("output") = nullptr, py::arg("name") = "gelu")
        .def("add", &ark::Model::add,
             "Performs an element-wise addition operator between the `input` "
             "tensor and the `other` tensor",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("other"), py::arg("output") = nullptr,
             py::arg("name") = "add")
        .def("mul", &ark::Model::mul,
             "Performs an element-wise multiplication operator between the "
             "`input` tensor and the `other` tensor,",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("other"), py::arg("output") = nullptr,
             py::arg("name") = "mul")
        .def("send", &ark::Model::send,
             "Sends a tensor to a destination GPU (`dst_rank`). Multiple "
             "tensors can be sent to the same GPU,so an identifier `id` is "
             "required to distinguish the tensor. Each 'send' operator must "
             "have a corresponding 'recv' operator that have the same id in "
             "another GPU's model.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("id"), py::arg("dst_rank"), py::arg("bytes") = 0,
             py::arg("output") = nullptr, py::arg("name") = "send")
        .def("send_done", &ark::Model::send_done,
             "Blocks the execution until the corresponding 'send' operator "
             "with the specified `id` is completed.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("id"), py::arg("dst_rank"), py::arg("output") = nullptr,
             py::arg("name") = "send_done")
        .def("recv", &ark::Model::recv,
             "Receives a tensor from a source GPU (`src_rank`), identified by "
             "the `id` parameter. Blocks the execution until the corresponding "
             "'recv' operator is completed.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("id"), py::arg("src_rank"), py::arg("bytes") = 0,
             py::arg("output") = nullptr, py::arg("name") = "recv")
        .def("send_mm", &ark::Model::send_mm,
             "Similar to the 'send_done' function, but implemented using CUDA "
             "in-stream RDMA copy and Low Latency (LL) protocol.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("id"), py::arg("gpu_dst"), py::arg("bytes") = 0,
             py::arg("output") = nullptr, py::arg("name") = "send_mm")
        .def("recv_mm", &ark::Model::recv_mm,
             "Similar to the 'recv' function, but implemented using CUDA "
             "in-stream RDMA copy and Low Latency (LL) protocol.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("id"), py::arg("gpu_src"), py::arg("bytes") = 0,
             py::arg("output") = nullptr, py::arg("name") = "recv_mm")
        .def("all_gather", &ark::Model::all_gather,
             "Performs an all-gather operator across all GPUs",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("gpu_id"), py::arg("gpu_num"),
             py::arg("output") = std::vector<ark::Tensor *>(),
             py::arg("name") = "all_gather")
        .def("all_reduce", &ark::Model::all_reduce,
             "Performs an all-reduce operator across all GPUs, aggregating "
             "the input tensors. Takes the `input` tensor, the current GPU's "
             "`gpu_id`, and the total number of GPUs `gpu_num`.",
             py::return_value_policy::reference_internal, py::arg("input"),
             py::arg("gpu_id"), py::arg("gpu_num"), py::arg("output") = nullptr,
             py::arg("name") = "all_reduce");

    py::class_<ark::Executor>(m, "_Executor",
                              "Convenience class for executing a model.")
        .def(py::init<const int, int, int, ark::Model &, const std::string &,
                      int>(),
             py::arg("gpu_id"), py::arg("rank"), py::arg("world_size"),
             py::arg("model"), py::arg("name"),
             py::arg("num_warps_per_sm") = 16)
        .def("compile", &ark::Executor::compile,
             "Compile the model. This must be called before `launch()`.")
        .def("launch", &ark::Executor::launch,
             "Launch the model (not running yet). This must be called after "
             "`compile()`.")
        .def("run", &ark::Executor::run, py::arg("iter"),
             "Run the model for `iter` iterations.")
        .def("wait", &ark::Executor::wait,
             "Wait for the previous run to finish.")
        .def("stop", &ark::Executor::stop,
             "Stop the model and return the elapsed time in milliseconds. Once "
             "this is called, we need to call `launch()` again to run the "
             "model again.")
        .def("tensor_memcpy_host_to_device", &tensor_memcpy_host_to_device,
             "Copy contiguous data from a host buffer to the given tensor's "
             "(possibly non-contiguous) data range on GPU.",
             py::arg("tns"), py::arg("src"))
        .def("tensor_memcpy_device_to_host", &tensor_memcpy_device_to_host,
             "Copy (possibly non-contiguous) data from a tensor on GPU to a "
             "contiguous host buffer. The given number of bytes is copied, in "
             "order of appearance on the memory. This function assumes that "
             "`dst` is large enough to hold the data.",
             py::arg("dst"), py::arg("tns"))
        .def("tensor_clear", &ark::Executor::tensor_clear,
             "Set all bytes of `tns` into zero.", py::arg("tns"));
}

```

`python/unittest/test_allgather.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import ark
import numpy as np
import multiprocessing
import unittest


def all_gather_test_not_inplace(
    rank, np_inputs, world_size, tensor_len, iter=1
):
    # Create a Model instance
    model = ark.Model(rank)

    input_tensor = model.tensor(ark.Dims(tensor_len), ark.TensorType.FP16)
    # The all_gather operation will create the recv tensor shards and return them as a list. The allgather_result[rank] is the same as input_tensor
    allgather_result = model.all_gather(input_tensor, rank, world_size)

    exe = ark.Executor(
        rank, rank, world_size, model, "all_gather_test_not_inplace"
    )
    exe.compile()

    exe.launch()

    exe.tensor_memcpy_host_to_device(input_tensor, np_inputs[rank])
    exe.run(iter)
    elapsed = exe.stop()
    max_abs_error = 0
    for tensor_shard in range(world_size):
        # if tensor_shard == rank, then this is a local shard. The allgather_result[tensor_shard] is the same as input_tensor
        host_output = np.zeros(tensor_len, dtype=np.float16)
        exe.tensor_memcpy_device_to_host(
            host_output, allgather_result[tensor_shard]
        )
        gt = np_inputs[tensor_shard]

        max_abs_error = max(max_abs_error, np.max(np.abs(host_output - gt)))
        numeric_epsilon_half = np.finfo(np.float16).eps
        np.testing.assert_allclose(host_output, gt, rtol=numeric_epsilon_half)
    print(
        "allgather not-inplace test",
        "world_size:",
        world_size,
        "rank:",
        rank,
        "tensor_len:",
        "{:6d}".format(tensor_len),
        "max_abs_error:",
        "{:.5f}".format(max_abs_error),
        "elapsed",
        "{:.5f}".format(elapsed),
        " ms ",
        " iter ",
        iter,
        "elapsed_per_iter",
        "{:.5f}".format(elapsed / iter),
        " ms ",
    )


def all_gather_test_inplace(rank, np_inputs, world_size, tensor_len, iter=1):
    # Create a Model instance
    model = ark.Model(rank)

    # input_tensor = model.tensor(ark.Dims(tensor_len), ark.TensorType.FP16)

    output_tensor = model.tensor(
        ark.Dims(tensor_len * world_size), ark.TensorType.FP16
    )
    # Shard the output tensor into world_size shards
    output_shard = model.sharding(output_tensor, 0, tensor_len)
    # The input tensor is the rank'th shard of the output tensor
    input_tensor = output_shard[rank]
    allgather_result = model.all_gather(
        input_tensor, rank, world_size, output_shard
    )

    exe = ark.Executor(rank, rank, world_size, model, "all_gather_test_inplace")
    exe.compile()

    exe.launch()
    exe.tensor_memcpy_host_to_device(input_tensor, np_inputs[rank])
    exe.run(iter)
    elapsed = exe.stop()
    host_output = np.zeros(tensor_len * world_size, dtype=np.float16)

    exe.tensor_memcpy_device_to_host(host_output, output_tensor)

    gt = np.concatenate(np_inputs, axis=0)

    max_abs_error = np.max(np.abs(host_output - gt))
    mean_abs_error = np.mean(np.abs(host_output - gt))
    print(
        "allgather-inplace test",
        "world_size:",
        world_size,
        "rank:",
        rank,
        "tensor_len:",
        "{:6d}".format(tensor_len),
        "max_abs_error:",
        "{:.5f}".format(max_abs_error),
        "mean_abs_error:",
        "{:.5f}".format(mean_abs_error),
        "elapsed",
        "{:.5f}".format(elapsed),
        " ms ",
        " iter ",
        iter,
        "elapsed_per_iter",
        "{:.5f}".format(elapsed / iter),
        " ms ",
    )


def all_gather_test_main(
    world_size, tensor_len, allgather_test_func=all_gather_test_not_inplace
):
    ark.init()
    num_processes = world_size  # number of processes
    processes = []
    np_inputs = []
    for i in range(num_processes):
        np_inputs.append(np.random.rand(tensor_len).astype(np.float16))
    for i in range(num_processes):
        process = multiprocessing.Process(
            target=allgather_test_func,
            args=(i, np_inputs, world_size, tensor_len),
        )
        process.start()
        processes.append(process)

    for process in processes:
        process.join()


class TestAllgather(unittest.TestCase):
    def test_allgather_not_inplace(self):
        all_gather_test_main(2, 2048, all_gather_test_not_inplace)
        all_gather_test_main(4, 2048, all_gather_test_not_inplace)

    def test_allgather_inplace(self):
        all_gather_test_main(2, 2048, all_gather_test_inplace)
        all_gather_test_main(4, 2048, all_gather_test_inplace)
        all_gather_test_main(6, 2048, all_gather_test_inplace)
        all_gather_test_main(8, 2048, all_gather_test_inplace)


if __name__ == "__main__":
    unittest.main()

```

`python/unittest/test_allgather_parallel.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import ark
import numpy as np
import multiprocessing
import unittest

m, n, k = 64, 256, 8


# Use all-gather operation to perform tensor parallel matmul
def all_gather_tensor_parallel(rank, np_inputs, world_size, iter=1):
    print("rank:", rank)

    # The number of columns per GPU
    N_pergpu = n // world_size
    # Create a Model instance
    model = ark.Model(rank)

    # The input and other tensor of the matmul, note that the other is only a shard of the whole other tensor, we split the other tensor to perform tensor parallel matmul
    input_tensor = model.tensor(ark.Dims(m, k), ark.TensorType.FP16)
    other_tensor_shard = model.tensor(
        ark.Dims(k, N_pergpu), ark.TensorType.FP16
    )
    # The whole output tensor of the matmul
    whole_output_trans = model.tensor(ark.Dims(n, m), ark.TensorType.FP16)

    # shard the other at the first dim, the rank'th shard is the output of the matmul, and we will perform an inplace all-gather operation to get the whole output tensor
    whole_output_shard = model.sharding(whole_output_trans, 0, N_pergpu)

    output_tensor_shard_trans = whole_output_shard[rank]

    # output_tensor_shard = matmul(input, other) => output_tensor_shard.transpose = matmul(other.transpose, input.transpose)
    model.matmul(
        other_tensor_shard,
        input_tensor,
        output_tensor_shard_trans,
        1,
        trans_input=True,
        trans_other=True,
    )
    # In-place all-gather operation to get the whole output tensor
    allgather_result = model.all_gather(
        output_tensor_shard_trans, rank, world_size, whole_output_shard
    )

    # Create an executor instance
    exe = ark.Executor(rank, rank, world_size, model, "test_all_gather")
    exe.compile()
    exe.launch()

    # Copy input and other tensors to device
    exe.tensor_memcpy_host_to_device(input_tensor, np_inputs["input"])
    np_other = np_inputs["other"]
    np_other_shard = np.split(np_other, world_size, axis=1)[rank]
    np_other_shard_copy = np_other_shard.copy()
    exe.tensor_memcpy_host_to_device(other_tensor_shard, np_other_shard_copy)

    # Run the executor
    exe.run(iter)
    elapsed = exe.stop()

    # Copy output tensor to host
    output_host_trans = np.zeros((n, m), dtype=np.float16)
    exe.tensor_memcpy_device_to_host(output_host_trans, whole_output_trans)
    output_host = output_host_trans.transpose()

    # Calculate ground truth
    gt = np.matmul(np_inputs["input"], np_other)

    max_abs_error = np.max(np.abs(output_host - gt))
    mean_abs_error = np.mean(np.abs(output_host - gt))
    print(
        "allgather_parallel_test",
        "world_size:",
        world_size,
        "rank:",
        rank,
        "m",
        m,
        "n",
        n,
        "k",
        k,
        "max_abs_error:",
        "{:.5f}".format(max_abs_error),
        "mean_abs_error:",
        "{:.5f}".format(mean_abs_error),
        "elapsed",
        "{:.5f}".format(elapsed),
        " ms ",
        " iter ",
        iter,
        "elapsed_per_iter",
        "{:.5f}".format(elapsed / iter),
        " ms ",
    )


def all_gather_test_main(
    world_size, allgather_test_func=all_gather_tensor_parallel
):
    ark.init()
    num_processes = world_size  # number of processes
    processes = []
    np_inputs = {
        "input": np.random.randn(m, k).astype(np.float16),
        "other": np.random.randn(k, n).astype(np.float16),
    }

    for i in range(num_processes):
        process = multiprocessing.Process(
            target=allgather_test_func,
            args=(i, np_inputs, world_size),
        )
        process.start()
        processes.append(process)

    for process in processes:
        process.join()


class TestAllreduce(unittest.TestCase):
    def test_all_gather(self):
        all_gather_test_main(2, all_gather_tensor_parallel)
        all_gather_test_main(4, all_gather_tensor_parallel)


if __name__ == "__main__":
    unittest.main()

```

`python/unittest/test_allreduce.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import ark
import numpy as np
import multiprocessing
import unittest


def all_reduce_test(rank, np_inputs, world_size, tensor_len, iter=1):
    tensor_size = tensor_len * 2
    # Create a Model instance
    model = ark.Model(rank)

    input_tensor = model.tensor(ark.Dims(tensor_len), ark.TensorType.FP16)

    allreduce_result = model.all_reduce(input_tensor, rank, world_size)

    exe = ark.Executor(rank, rank, world_size, model, "all_reduce_test")
    exe.compile()

    exe.launch()
    exe.tensor_memcpy_host_to_device(input_tensor, np_inputs[rank])
    exe.run(iter)
    elapsed = exe.stop()

    host_output = np.zeros(tensor_len, dtype=np.float16)
    exe.tensor_memcpy_device_to_host(host_output, allreduce_result)
    gt = np.zeros(tensor_len, dtype=np.float16)
    for np_input in np_inputs:
        gt += np_input

    max_abs_error = np.max(np.abs(host_output - gt))
    mean_abs_error = np.mean(np.abs(host_output - gt))
    # The numeric error of half precision of the machine
    numeric_epsilon_half = np.finfo(np.float16).eps
    np.testing.assert_allclose(
        host_output, gt, atol=2 * world_size * numeric_epsilon_half
    )
    print(
        "allreduce test:",
        "world_size",
        world_size,
        "rank",
        rank,
        "tensor_len",
        "{:6d}".format(tensor_len),
        "max_abs_error",
        "{:.5f}".format(max_abs_error),
        "mean_abs_error",
        "{:.5f}".format(mean_abs_error),
        "elapsed",
        "{:.5f}".format(elapsed),
        "ms",
        "iter",
        iter,
        "elapsed_per_iter",
        "{:.5f}".format(elapsed / iter),
        " ms ",
    )


def test_allreduce_internal(world_size, tensor_len):
    ark.init()
    num_processes = world_size  # number of processes
    processes = []
    np_inputs = []
    for i in range(num_processes):
        np_inputs.append(np.random.rand(tensor_len).astype(np.float16))
    for i in range(num_processes):
        process = multiprocessing.Process(
            target=all_reduce_test,
            args=(i, np_inputs, world_size, tensor_len),
        )
        process.start()
        processes.append(process)

    for process in processes:
        process.join()


class TestAllreduce(unittest.TestCase):
    def test_allreduce(self):
        test_allreduce_internal(2, 2048)
        test_allreduce_internal(4, 2048)
        test_allreduce_internal(8, 2048)


if __name__ == "__main__":
    unittest.main()

```

`python/unittest/test_api.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import torch
import numpy as np
import torch.nn as nn
import ark
import unittest

d_model = 512
d_ff = 2048

batch_size = 1
seq_len = 64

import ark


class TestModelARK(ark.Module):
    def __init__(self):
        super(TestModelARK, self).__init__()
        self.weight_1 = ark.Parameter(
            ark.tensor(ark.Dims(d_model, d_ff), ark.TensorType.FP16)
        )
        self.weight_2 = ark.Parameter(
            ark.tensor(ark.Dims(d_ff, d_model), ark.TensorType.FP16)
        )

    def forward(self, inputs):
        middle_result = ark.matmul(inputs, self.weight_1, is_relu=True)
        middle_result1 = ark.matmul(middle_result, self.weight_2)
        output = ark.add(middle_result1, inputs)
        output_layernorm = ark.layernorm(output)
        return output_layernorm


class TestModel(nn.Module):
    def __init__(self):
        super(TestModel, self).__init__()
        self.weight_1 = nn.Parameter(torch.FloatTensor(d_model, d_ff))
        self.weight_2 = nn.Parameter(torch.FloatTensor(d_ff, d_model))

    # inputs: [batch_size, seq_len, d_model]
    def forward(self, inputs):
        output = torch.matmul(
            inputs, self.weight_1
        )  # [batch_size, seq_len, d_ff]
        output = nn.ReLU()(output)
        output = torch.matmul(
            output, self.weight_2
        )  # [batch_size, seq_len, d_model]
        output = nn.LayerNorm(d_model)(
            output + inputs
        )  # [batch_size, seq_len, d_model]
        return output


def test_TestModel():
    runtime = ark.Runtime()

    input_tensor = ark.tensor(
        ark.Dims(batch_size, seq_len, d_model), ark.TensorType.FP16
    )
    ark_model = TestModelARK()
    output_tensor = ark_model(input_tensor)
    # Test the mul method

    runtime.launch()

    input_tensor_host = (
        (np.random.rand(batch_size, seq_len, d_model) - 0.5) * 0.1
    ).astype(np.float16)

    input_tensor.from_numpy(input_tensor_host)

    weight_1_host = ((np.random.rand(d_model, d_ff) - 0.5) * 0.1).astype(
        np.float16
    )
    weight_2_host = ((np.random.rand(d_ff, d_model) - 0.5) * 0.1).astype(
        np.float16
    )
    state_dict = {"weight_1": weight_1_host, "weight_2": weight_2_host}

    ark_model.load_state_dict(state_dict)
    runtime.run()

    output_tensor_host = output_tensor.to_numpy()

    input_tensor_host_float32 = input_tensor_host.astype(np.float32)

    torch_input = torch.from_numpy(input_tensor_host_float32)

    torch_model = TestModel()

    torch_model.load_state_dict(ark.convert_state_dict(state_dict, "torch"))

    gt = torch_model(torch_input).detach().numpy().astype(np.float16)

    # test if the result is correct
    max_error = np.max(np.abs(output_tensor_host - gt))
    avg_error = np.mean(np.abs(output_tensor_host - gt))
    ark_state_dict = ark_model.state_dict()
    for k, v in state_dict.items():
        np.testing.assert_allclose(v, ark_state_dict[k])
    # print(input_tensor_host)
    # print(output_tensor_host)
    # print(gt)
    print("ARK module test")
    print(
        "batch_size:",
        batch_size,
        "seq_len:",
        seq_len,
        "d_model:",
        d_model,
        "d_ff:",
        d_ff,
    )
    print("max error: ", max_error, "avg error: ", avg_error)


class TestAPI(unittest.TestCase):
    def test_api(self):
        test_TestModel()


if __name__ == "__main__":
    unittest.main()

```

`python/unittest/test_gelu.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import ark

import torch
import torch.nn.functional as F
import numpy as np
import unittest


def test_gelu_internal(batch_size, m, n, iter=1):
    ark.init()
    # Create a Model instance
    model = ark.Model()

    input_tensor = model.tensor(ark.Dims(batch_size, m, n), ark.TensorType.FP16)

    output_tensor = model.gelu(input_tensor)
    # Test the mul method
    exe = ark.Executor(0, 0, 1, model, "ops_gelu_test")
    exe.compile()
    input_tensor_host = np.random.rand(batch_size, m, n).astype(np.float16)

    exe.launch()
    exe.tensor_memcpy_host_to_device(input_tensor, input_tensor_host)

    exe.run(iter)

    elapsed = exe.stop()

    output_tensor_host = np.zeros((batch_size, m, n), dtype=np.float16)

    exe.tensor_memcpy_device_to_host(output_tensor_host, output_tensor)

    input_tensor_host_float32 = input_tensor_host.astype(np.float32)

    torch_input = torch.from_numpy(input_tensor_host_float32)

    gt = F.gelu(torch_input).detach().numpy().astype(np.float16)

    # test if the result is correct
    max_abs_error = np.max(np.abs(output_tensor_host - gt))
    mean_abs_error = np.mean(np.abs(output_tensor_host - gt))

    numeric_epsilon_half = np.finfo(np.float16).eps

    np.testing.assert_allclose(
        output_tensor_host, gt, atol=numeric_epsilon_half
    )

    print(
        "gelu test:",
        "batch_size",
        "{:6d}".format(batch_size),
        "m",
        "{:6d}".format(m),
        "n",
        "{:6d}".format(n),
        "max_abs_error",
        "{:.5f}".format(max_abs_error),
        "mean_abs_error",
        "{:.5f}".format(mean_abs_error),
        "elapsed",
        "{:.5f}".format(elapsed),
        "ms",
        "iter",
        iter,
        "elapsed per iter",
        "{:.5f}".format(elapsed / iter),
        " ms ",
    )


class TestGelu(unittest.TestCase):
    def test_gelu(self):
        test_gelu_internal(1, 1, 64)
        test_gelu_internal(1, 128, 128)
        test_gelu_internal(1, 4096, 1024)
        test_gelu_internal(1, 1024, 4096)
        test_gelu_internal(2, 1, 64)
        test_gelu_internal(2, 128, 128)
        test_gelu_internal(8, 4096, 1024)
        test_gelu_internal(8, 1024, 4096)


if __name__ == "__main__":
    unittest.main()

```

`python/unittest/test_layernorm.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import ark

import torch
import torch.nn.functional as F
import numpy as np
import unittest


def test_layernorm_internal(batch_size, m, n, data_type="float", iter=1):
    ark.init()

    # Create a Model instance
    model = ark.Model()
    if data_type == "float":
        ark_data_type = ark.TensorType.FP32
        numpy_data_type = np.float32
    elif data_type == "half":
        ark_data_type = ark.TensorType.FP16
        numpy_data_type = np.float16
    input_tensor = model.tensor(ark.Dims(batch_size, m, n), ark_data_type)

    output_tensor = model.layernorm(input_tensor)
    # Test the mul method
    exe = ark.Executor(0, 0, 1, model, "ops_layernorm_test")
    exe.compile()
    input_tensor_host = np.random.rand(batch_size, m, n).astype(numpy_data_type)

    exe.launch()
    exe.tensor_memcpy_host_to_device(input_tensor, input_tensor_host)

    exe.run(iter)

    elapsed = exe.stop()

    output_tensor_host = np.zeros((batch_size, m, n), dtype=numpy_data_type)

    exe.tensor_memcpy_device_to_host(output_tensor_host, output_tensor)

    input_tensor_host_float32 = input_tensor_host.astype(np.float32)

    torch_input = torch.from_numpy(input_tensor_host_float32)

    # get the ground truth
    gt = torch.nn.LayerNorm([n], eps=1e-5, elementwise_affine=False)(
        torch_input
    ).numpy()
    # test if the result is correct
    max_abs_error = np.max(np.abs(output_tensor_host - gt))
    mean_abs_error = np.mean(np.abs(output_tensor_host - gt))
    numeric_epsilon_half = np.finfo(np.float16).eps
    # layernorm half precision error is too large now
    # np.testing.assert_allclose(
    #     output_tensor_host, gt, atol=10 * numeric_epsilon_half
    # )

    print(
        "layernorm test",
        "batch_size:",
        "{:6d}".format(batch_size),
        "m:",
        "{:6d}".format(m),
        "n:",
        "{:6d}".format(n),
        "data_type:",
        data_type,
        "max_abs_error:",
        "{:.5f}".format(max_abs_error),
        "mean_abs_error:",
        "{:.5f}".format(mean_abs_error),
        "elapsed",
        "{:.5f}".format(elapsed),
        " ms ",
        " iter ",
        iter,
        "elapsed_per_iter",
        "{:.5f}".format(elapsed / iter),
        " ms ",
    )


class TestLayernorm(unittest.TestCase):
    def test_layernorm(self):
        test_layernorm_internal(1, 32, 4, "half")
        test_layernorm_internal(1, 32, 512, "half")
        test_layernorm_internal(1, 64, 4, "half")
        test_layernorm_internal(1, 128, 128, "half")
        test_layernorm_internal(1, 256, 256, "half")
        test_layernorm_internal(1, 512, 512, "half")

        test_layernorm_internal(1, 8, 4)
        test_layernorm_internal(1, 128, 128)
        test_layernorm_internal(1, 256, 256)
        test_layernorm_internal(1, 512, 512)
        test_layernorm_internal(1, 1024, 1024)
        test_layernorm_internal(1, 4096, 1024)
        test_layernorm_internal(1, 1024, 4096)
        test_layernorm_internal(2, 64, 64)
        test_layernorm_internal(2, 128, 128)
        test_layernorm_internal(8, 4096, 1024)
        test_layernorm_internal(8, 1024, 4096)


if __name__ == "__main__":
    unittest.main()

```

`python/unittest/test_matmul.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import ark

import torch
import torch.nn.functional as F
import numpy as np
import unittest


def test_matmul_internal(
    m,
    n,
    k,
    bs_a,
    bs_b,
    split_k,
    is_relu=False,
    gran_lev=-1,
    iter=1,
    data_type="float",
):
    ark.init()

    # Create a Model instance
    model = ark.Model()
    if data_type == "float":
        ark_data_type = ark.TensorType.FP32
        numpy_data_type = np.float32
    elif data_type == "half":
        ark_data_type = ark.TensorType.FP16
        numpy_data_type = np.float16
    input_tensor = model.tensor(ark.Dims(bs_a, m, k), ark_data_type)
    other_tensor = model.tensor(ark.Dims(bs_b, k, n), ark_data_type)

    output_tensor = model.matmul(
        input_tensor,
        other_tensor,
        None,
        split_k,
        False,
        False,
        is_relu,
        "matmul",
        gran_lev,
    )
    # Test the mul method
    exe = ark.Executor(0, 0, 1, model, "ops_matmul_test")
    exe.compile()
    exe.launch()
    input_tensor_host = np.random.rand(bs_a, m, k).astype(numpy_data_type)
    other_tensor_host = np.random.rand(bs_b, k, n).astype(numpy_data_type)
    exe.tensor_memcpy_host_to_device(input_tensor, input_tensor_host)
    exe.tensor_memcpy_host_to_device(other_tensor, other_tensor_host)
    exe.run(1)

    elapsed = exe.stop()

    output_tensor_host = np.zeros((bs_a, m, n), dtype=numpy_data_type)

    exe.tensor_memcpy_device_to_host(output_tensor_host, output_tensor)

    gt = np.matmul(input_tensor_host, other_tensor_host)
    if is_relu:
        gt = np.maximum(gt, 0)
    # test if the result is correct
    max_abs_error = np.max(np.abs(output_tensor_host - gt))
    mean_abs_error = np.mean(np.abs(output_tensor_host - gt))
    numeric_epsilon_half = np.finfo(np.float16).eps
    atol = 2 * numeric_epsilon_half * k
    np.testing.assert_allclose(output_tensor_host, gt, atol=atol)

    print(
        "matmul test:",
        "bs_a",
        "{:6d}".format(bs_a),
        "bs_b",
        "{:6d}".format(bs_b),
        "m",
        "{:6d}".format(m),
        "x",
        "{:6d}".format(n),
        "x",
        "{:6d}".format(k),
        "(split_k=",
        split_k,
        ", relu=",
        is_relu,
        ", gran_lev=",
        gran_lev,
        ") ",
        " max_abs_error ",
        "{:.5f}".format(max_abs_error),
        " elapsed ",
        " mse ",
        "{:.5f}".format(mean_abs_error),
        "{:.5f}".format(elapsed),
        " ms ",
        " iter ",
        iter,
        "elapsed_per_iter",
        "{:.5f}".format(elapsed / iter),
        " ms ",
    )
    return True


# Test the correctness of matmul at small scale
def test_matmul_small_sizes(split_k, is_relu, gran_lev, iter=1):
    test_matmul_internal(
        64, 64, 32, 1, 1, split_k, is_relu, gran_lev, iter, "half"
    )
    test_matmul_internal(
        128, 64, 32, 1, 1, split_k, is_relu, gran_lev, iter, "half"
    )
    test_matmul_internal(
        64, 128, 32, 1, 1, split_k, is_relu, gran_lev, iter, "half"
    )
    test_matmul_internal(
        128, 128, 32, 1, 1, split_k, is_relu, gran_lev, iter, "half"
    )

    test_matmul_internal(
        64, 64, 64, 1, 1, split_k, is_relu, gran_lev, iter, "half"
    )
    test_matmul_internal(
        128, 64, 64, 1, 1, split_k, is_relu, gran_lev, iter, "half"
    )
    test_matmul_internal(
        64, 128, 64, 1, 1, split_k, is_relu, gran_lev, iter, "half"
    )
    test_matmul_internal(
        128, 128, 64, 1, 1, split_k, is_relu, gran_lev, iter, "half"
    )
    test_matmul_internal(
        256, 128, 64, 1, 1, split_k, is_relu, gran_lev, iter, "half"
    )

    test_matmul_internal(
        128, 128, 256, 1, 1, split_k, is_relu, gran_lev, iter, "half"
    )


class TestMatmul(unittest.TestCase):
    def test_matmul_gran(self):
        for gran_lev in range(-1, 3):
            print("test_matmul_gran gran_lev=", gran_lev)
            test_matmul_small_sizes(1, False, gran_lev)

    def test_matmul_relu(self):
        print("test_matmul_relu")
        test_matmul_small_sizes(1, True, -1)

    def test_matmul_split(self):
        print("test_matmul_split")
        for split_k in range(2, 4):
            test_matmul_small_sizes(split_k, False, -1)
        for split_k in range(3, 8):
            for gran_lev in range(-1, 3):
                test_matmul_internal(
                    128, 4096, 1024, 1, 1, split_k, False, gran_lev, 1, "half"
                )

    def test_matmul_perf(self):
        test_matmul_small_sizes(1, False, -1, 1000)


if __name__ == "__main__":
    unittest.main()

```

`python/unittest/test_reduce.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import ark

import torch
import torch.nn.functional as F
import numpy as np
import unittest


def test_reduce_internal(batch_size, m, n, data_type="float", iter=1):
    ark.init()

    # Create a Model instance
    model = ark.Model()
    if data_type == "float":
        ark_data_type = ark.TensorType.FP32
        numpy_data_type = np.float32
    elif data_type == "half":
        ark_data_type = ark.TensorType.FP16
        numpy_data_type = np.float16
    input_tensor = model.tensor(ark.Dims(batch_size, m, n), ark_data_type)

    output_tensor = model.reduce_sum(input_tensor, 2)
    # Test the mul method
    exe = ark.Executor(0, 0, 1, model, "ops_reduce_test")
    exe.compile()
    input_tensor_host = np.random.rand(batch_size, m, n).astype(numpy_data_type)

    exe.launch()
    exe.tensor_memcpy_host_to_device(input_tensor, input_tensor_host)

    exe.run(iter)

    elapsed = exe.stop()

    output_tensor_host = np.zeros((batch_size, m, 1), dtype=numpy_data_type)

    exe.tensor_memcpy_device_to_host(output_tensor_host, output_tensor)

    input_tensor_host_float32 = input_tensor_host.astype(np.float32)

    torch_input = torch.from_numpy(input_tensor_host_float32)

    gt = torch.sum(torch_input, dim=2, keepdim=True).numpy()

    # test if the result is correct
    max_abs_error = np.max(np.abs(output_tensor_host - gt))
    mean_abs_error = np.mean(np.abs(output_tensor_host - gt))
    # The numeric error of half precision of the machine
    numeric_epsilon_half = np.finfo(np.float16).eps
    # reduce add n numbers, so we assume the atol to be n * numeric_epsilon_half
    # rtol should be atol/max(abs(gt) + epsilon). The epsilon is to avoid
    # divide by zero, here we set it to be numeric_epsilon_half

    atol = numeric_epsilon_half * n
    np.testing.assert_allclose(output_tensor_host, gt, atol=atol)

    print(
        "reduce test",
        "batch_size:",
        "{:6d}".format(batch_size),
        "m:",
        "{:6d}".format(m),
        "n:",
        "{:6d}".format(n),
        "data_type:",
        data_type,
        "max_abs_error:",
        "{:.5f}".format(max_abs_error),
        "mean_abs_error:",
        "{:.5f}".format(mean_abs_error),
        "elapsed",
        "{:.5f}".format(elapsed),
        " ms ",
        " iter ",
        iter,
        "elapsed_per_iter",
        "{:.5f}".format(elapsed / iter),
        " ms ",
    )


class TestReduce(unittest.TestCase):
    def test_reduce(self):
        test_reduce_internal(1, 64, 4, "half")
        test_reduce_internal(1, 128, 128, "half")
        test_reduce_internal(1, 256, 256, "half")
        test_reduce_internal(1, 512, 512, "half")

        test_reduce_internal(1, 64, 4)
        test_reduce_internal(1, 128, 128)
        test_reduce_internal(1, 256, 256)
        test_reduce_internal(1, 512, 512)
        test_reduce_internal(1, 1024, 1024)
        test_reduce_internal(1, 4096, 1024)
        test_reduce_internal(1, 1024, 4096)
        test_reduce_internal(2, 64, 64)
        test_reduce_internal(2, 128, 128)
        test_reduce_internal(8, 4096, 1024)
        test_reduce_internal(8, 1024, 4096)


if __name__ == "__main__":
    unittest.main()

```

`python/unittest/test_sendrecv.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import ark
import numpy as np
import multiprocessing
import unittest

world_size = 2

tensor_len = 2048
tensor_size = tensor_len * 2


def sendrecv_test_one_dir_function(rank, np_inputs, iter=1):
    # Create a Model instance
    model = ark.Model(rank)

    input_tensor = model.tensor(ark.Dims(tensor_len), ark.TensorType.FP16)
    if rank == 0:
        model.send(input_tensor, 0, 1, tensor_size)
        model.send_done(input_tensor, 0, 1)
    if rank == 1:
        model.recv(input_tensor, 0, 0)
    # model.all_reduce(input_tensor, rank, world_size)

    exe = ark.Executor(rank, rank, world_size, model, "sendrecv_test_one_dir")
    exe.compile()

    exe.launch()
    if rank == 0:
        exe.tensor_memcpy_host_to_device(input_tensor, np_inputs)
    exe.run(iter)
    elapsed = exe.stop()
    if rank == 1:
        host_output = np.zeros(tensor_len, dtype=np.float16)
        exe.tensor_memcpy_device_to_host(host_output, input_tensor)

        max_abs_error = np.max(np.abs(host_output - np_inputs))
        mean_abs_error = np.mean(np.abs(host_output - np_inputs))
        # The numeric error of half precision of the machine
        numeric_epsilon_half = np.finfo(np.float16).eps
        atol = numeric_epsilon_half
        np.testing.assert_allclose(host_output, np_inputs, atol=atol)
        print(
            "sendrecv_test_one_dir:",
            "rank",
            rank,
            "tensor_len",
            "{:6d}".format(tensor_len),
            "max_abs_error:",
            "{:.5f}".format(max_abs_error),
            "mean_abs_error:",
            "{:.5f}".format(mean_abs_error),
            "elapsed",
            "{:.5f}".format(elapsed),
            " ms ",
            " iter ",
            iter,
            "elapsed_per_iter",
            "{:.5f}".format(elapsed / iter),
            " ms ",
        )


def sendrecv_test_one_dir():
    ark.init()
    num_processes = world_size  # number of processes
    processes = []
    np_inputs = np.random.rand(tensor_len).astype(np.float16)
    for i in range(num_processes):
        process = multiprocessing.Process(
            target=sendrecv_test_one_dir_function, args=(i, np_inputs)
        )
        process.start()
        processes.append(process)

    for process in processes:
        process.join()


def sendrecv_test_bi_dir_function(rank, np_inputs, iter=1):
    other_rank = 1 - rank
    # Create a Model instance
    model = ark.Model(rank)

    send_tensor = model.tensor(ark.Dims(tensor_len), ark.TensorType.FP16)
    recv_tensor = model.tensor(ark.Dims(tensor_len), ark.TensorType.FP16)
    model.send(send_tensor, rank, other_rank, tensor_size)
    model.send_done(send_tensor, rank, other_rank)
    model.recv(recv_tensor, other_rank, other_rank)

    exe = ark.Executor(rank, rank, world_size, model, "sendrecv_test_bi_dir")
    exe.compile()

    exe.launch()
    exe.tensor_memcpy_host_to_device(send_tensor, np_inputs[rank])

    exe.run(iter)
    elapsed = exe.stop()

    host_output = np.zeros(tensor_len, dtype=np.float16)
    exe.tensor_memcpy_device_to_host(host_output, recv_tensor)

    gt = np_inputs[other_rank]
    max_abs_error = np.max(np.abs(host_output - gt))
    mean_abs_error = np.mean(np.abs(host_output - gt))
    # The numeric error of half precision of the machine
    numeric_epsilon_half = np.finfo(np.float16).eps
    np.testing.assert_allclose(host_output, gt, atol=numeric_epsilon_half)
    print(
        "sendrecv_test_bi_dir:",
        "rank",
        rank,
        "tensor_len",
        "{:6d}".format(tensor_len),
        "max_abs_error:",
        "{:.5f}".format(max_abs_error),
        "mean_abs_error:",
        "{:.5f}".format(mean_abs_error),
        "elapsed",
        "{:.5f}".format(elapsed),
        " ms ",
        " iter ",
        iter,
    )


def sendrecv_test_bi_dir():
    ark.init()
    num_processes = world_size  # number of processes
    processes = []
    np_inputs = []
    np_inputs.append(np.random.rand(tensor_len).astype(np.float16))
    np_inputs.append(np.random.rand(tensor_len).astype(np.float16))
    for i in range(num_processes):
        process = multiprocessing.Process(
            target=sendrecv_test_bi_dir_function, args=(i, np_inputs)
        )
        process.start()
        processes.append(process)

    for process in processes:
        process.join()


class SendRecvTest(unittest.TestCase):
    def test_sendrecv_one_dir(self):
        sendrecv_test_one_dir()

    def test_sendrecv_bi_dir(self):
        sendrecv_test_bi_dir()


if __name__ == "__main__":
    unittest.main()

```

`python/unittest/test_softmax.py`:

```py
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

import ark

import torch
import torch.nn.functional as F
import numpy as np
import unittest


def test_softmax_internal(batch_size, m, n, data_type="float", iter=1):
    ark.init()

    # Create a Model instance
    model = ark.Model()
    if data_type == "float":
        ark_data_type = ark.TensorType.FP32
        numpy_data_type = np.float32
    elif data_type == "half":
        ark_data_type = ark.TensorType.FP16
        numpy_data_type = np.float16
    input_tensor = model.tensor(ark.Dims(batch_size, m, n), ark_data_type)

    output_tensor = model.softmax(input_tensor)
    # Test the mul method
    exe = ark.Executor(0, 0, 1, model, "ops_softmax_test")
    exe.compile()
    input_tensor_host = np.random.rand(batch_size, m, n).astype(numpy_data_type)

    exe.launch()
    exe.tensor_memcpy_host_to_device(input_tensor, input_tensor_host)

    exe.run(iter)

    elapsed = exe.stop()

    output_tensor_host = np.zeros((batch_size, m, n), dtype=numpy_data_type)

    exe.tensor_memcpy_device_to_host(output_tensor_host, output_tensor)

    input_tensor_host_float32 = input_tensor_host.astype(np.float32)

    torch_input = torch.from_numpy(input_tensor_host_float32)

    # get the ground truth
    gt = F.softmax(torch_input, dim=-1).numpy()
    # test if the result is correct
    max_abs_error = np.max(np.abs(output_tensor_host - gt))
    mean_abs_error = np.mean(np.abs(output_tensor_host - gt))
    # The numeric error of half precision of the machine
    numeric_epsilon_half = np.finfo(np.float16).eps
    np.testing.assert_allclose(
        output_tensor_host, gt, atol=numeric_epsilon_half
    )

    print(
        "softmax test",
        "batch_size:",
        "{:6d}".format(batch_size),
        "m:",
        "{:6d}".format(m),
        "n:",
        "{:6d}".format(n),
        "data_type:",
        data_type,
        "max_abs_error:",
        "{:.5f}".format(max_abs_error),
        "mean_abs_error:",
        "{:.5f}".format(mean_abs_error),
        "elapsed",
        "{:.5f}".format(elapsed),
        " ms ",
        " iter ",
        iter,
        "elapsed_per_iter",
        "{:.5f}".format(elapsed / iter),
        " ms ",
    )


class TestSoftmax(unittest.TestCase):
    def test_softmax(self):
        test_softmax_internal(1, 32, 4, "half")
        test_softmax_internal(1, 32, 512, "half")
        test_softmax_internal(1, 64, 4, "half")
        test_softmax_internal(1, 128, 128, "half")
        test_softmax_internal(1, 256, 256, "half")
        test_softmax_internal(1, 512, 512, "half")

        test_softmax_internal(1, 8, 4)
        test_softmax_internal(1, 128, 128)
        test_softmax_internal(1, 256, 256)
        test_softmax_internal(1, 512, 512)
        test_softmax_internal(1, 1024, 1024)
        test_softmax_internal(1, 4096, 1024)
        test_softmax_internal(1, 1024, 4096)
        test_softmax_internal(2, 64, 64)
        test_softmax_internal(2, 128, 128)
        test_softmax_internal(8, 4096, 1024)
        test_softmax_internal(8, 1024, 4096)


if __name__ == "__main__":
    unittest.main()

```

`requirements.txt`:

```txt
scikit-build-core
pyproject_metadata
numpy
pickle

```

`third_party/CMakeLists.txt`:

```txt
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

set(CUTLASS_INCLUDE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}/cutlass/include)
set(GPUDMA_INCLUDE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}/gpudma/module)
set(JSON_INCLUDE_DIRS ${CMAKE_CURRENT_SOURCE_DIR}/json)

# Update submodules
add_custom_target(tp-update
    COMMAND cd ${CMAKE_CURRENT_SOURCE_DIR} && git submodule update --init --recursive
)

# Patch CUTLASS
add_custom_target(tp-cutlass-patch
    COMMAND cd ${CMAKE_CURRENT_SOURCE_DIR} && make cutlass
)
add_dependencies(tp-cutlass-patch tp-update)

# Copy CUTLASS header files to the build directory
add_custom_target(tp-cutlass
    COMMAND ${CMAKE_COMMAND} -E copy_directory ${CMAKE_CURRENT_SOURCE_DIR}/cutlass/include ${BUILD_DIR}/include/kernels
)

# Build GPUDMA
add_custom_target(tp-gpudma
    COMMAND cd ${CMAKE_CURRENT_SOURCE_DIR} && make gpudma
)
add_dependencies(tp-gpudma tp-update)

# Clean up
add_custom_target(tp-clean
    COMMAND cd ${CMAKE_CURRENT_SOURCE_DIR} && make clean
)

```

`third_party/Makefile`:

```
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

MKDIR := $(patsubst %/,%,$(dir $(abspath $(lastword $(MAKEFILE_LIST)))))

GPUDMA_DIR := $(MKDIR)/gpudma
GPUDMA_NVIDIA_SRC ?= $(shell ls -d1 /usr/src/nvidia-*.*.* | tail -n 1)
GPUDMA_PATCHES       := $(shell find $(MKDIR)/patches/gpudma -name '*.patch')
GPUDMA_PATCH_TARGETS := $(patsubst %,patch-%,$(GPUDMA_PATCHES))
GPUDMA_TARGET := $(GPUDMA_DIR)/module/gpumem.ko

CUTLASS_DIR := $(MKDIR)/cutlass
CUTLASS_PATCHES       := $(shell find $(MKDIR)/patches/cutlass -name '*.patch')
CUTLASS_PATCH_TARGETS := $(patsubst %,patch-%,$(CUTLASS_PATCHES))

TMPDIR := $(MKDIR)/.tmp


.PHONY: cutlass clean

gpudma: $(GPUDMA_TARGET)

$(GPUDMA_TARGET): $(GPUDMA_PATCHES)
	mkdir -p $(TMPDIR)/nvidia
	cp -r $(shell ls -d1 /usr/src/nvidia-*.*.* | tail -n 1) $(TMPDIR)/nvidia/kernel
	$(MAKE) -j -C $(TMPDIR)/nvidia/kernel
	$(MAKE) -j $(GPUDMA_PATCH_TARGETS)
	cd $(GPUDMA_DIR)/module && GPUDMA_DIR=$(TMPDIR) $(MAKE) -j
	rm -r $(TMPDIR)

cutlass:
	@$(MAKE) -j $(CUTLASS_PATCH_TARGETS)

patch-$(MKDIR)/patches/%.patch: $(MKDIR)/% $(MKDIR)/patches/%.patch
	@patch -sN $< -i $(word 2,$^) -r /dev/null | true

clean:
	$(MAKE) -C $(GPUDMA_DIR)/module -f $(GPUDMA_DIR)/module/Makefile clean
	rm -rf $(TMPDIR)
	cd $(GPUDMA_DIR) && git reset --hard
	cd $(CUTLASS_DIR) && git reset --hard

```

`third_party/json/json.h`:

```h
/*
    __ _____ _____ _____
 __|  |   __|     |   | |  JSON for Modern C++
|  |  |__   |  |  | | | |  version 3.7.3
|_____|_____|_____|_|___|  https://github.com/nlohmann/json

Licensed under the MIT License <http://opensource.org/licenses/MIT>.
SPDX-License-Identifier: MIT
Copyright (c) 2013-2019 Niels Lohmann <http://nlohmann.me>.

Permission is hereby  granted, free of charge, to any  person obtaining a copy
of this software and associated  documentation files (the "Software"), to deal
in the Software  without restriction, including without  limitation the rights
to  use, copy,  modify, merge,  publish, distribute,  sublicense, and/or  sell
copies  of  the Software,  and  to  permit persons  to  whom  the Software  is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE  IS PROVIDED "AS  IS", WITHOUT WARRANTY  OF ANY KIND,  EXPRESS OR
IMPLIED,  INCLUDING BUT  NOT  LIMITED TO  THE  WARRANTIES OF  MERCHANTABILITY,
FITNESS FOR  A PARTICULAR PURPOSE AND  NONINFRINGEMENT. IN NO EVENT  SHALL THE
AUTHORS  OR COPYRIGHT  HOLDERS  BE  LIABLE FOR  ANY  CLAIM,  DAMAGES OR  OTHER
LIABILITY, WHETHER IN AN ACTION OF  CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE  OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
*/

#ifndef INCLUDE_NLOHMANN_JSON_HPP_
#define INCLUDE_NLOHMANN_JSON_HPP_

#define NLOHMANN_JSON_VERSION_MAJOR 3
#define NLOHMANN_JSON_VERSION_MINOR 7
#define NLOHMANN_JSON_VERSION_PATCH 3

#include <algorithm> // all_of, find, for_each
#include <cassert> // assert
#include <ciso646> // and, not, or
#include <cstddef> // nullptr_t, ptrdiff_t, size_t
#include <functional> // hash, less
#include <initializer_list> // initializer_list
#include <iosfwd> // istream, ostream
#include <iterator> // random_access_iterator_tag
#include <memory> // unique_ptr
#include <numeric> // accumulate
#include <string> // string, stoi, to_string
#include <utility> // declval, forward, move, pair, swap
#include <vector> // vector

// #include <nlohmann/adl_serializer.hpp>


#include <utility>

// #include <nlohmann/detail/conversions/from_json.hpp>


#include <algorithm> // transform
#include <array> // array
#include <ciso646> // and, not
#include <forward_list> // forward_list
#include <iterator> // inserter, front_inserter, end
#include <map> // map
#include <string> // string
#include <tuple> // tuple, make_tuple
#include <type_traits> // is_arithmetic, is_same, is_enum, underlying_type, is_convertible
#include <unordered_map> // unordered_map
#include <utility> // pair, declval
#include <valarray> // valarray

// #include <nlohmann/detail/exceptions.hpp>


#include <exception> // exception
#include <stdexcept> // runtime_error
#include <string> // to_string

// #include <nlohmann/detail/input/position_t.hpp>


#include <cstddef> // size_t

namespace nlohmann
{
namespace detail
{
/// struct to capture the start position of the current token
struct position_t
{
    /// the total number of characters read
    std::size_t chars_read_total = 0;
    /// the number of characters read in the current line
    std::size_t chars_read_current_line = 0;
    /// the number of lines read
    std::size_t lines_read = 0;

    /// conversion to size_t to preserve SAX interface
    constexpr operator size_t() const
    {
        return chars_read_total;
    }
};

} // namespace detail
} // namespace nlohmann

// #include <nlohmann/detail/macro_scope.hpp>


#include <utility> // pair
// #include <nlohmann/thirdparty/hedley/hedley.hpp>
/* Hedley - https://nemequ.github.io/hedley
 * Created by Evan Nemerson <evan@nemerson.com>
 *
 * To the extent possible under law, the author(s) have dedicated all
 * copyright and related and neighboring rights to this software to
 * the public domain worldwide. This software is distributed without
 * any warranty.
 *
 * For details, see <http://creativecommons.org/publicdomain/zero/1.0/>.
 * SPDX-License-Identifier: CC0-1.0
 */

#if !defined(JSON_HEDLEY_VERSION) || (JSON_HEDLEY_VERSION < 11)
#if defined(JSON_HEDLEY_VERSION)
    #undef JSON_HEDLEY_VERSION
#endif
#define JSON_HEDLEY_VERSION 11

#if defined(JSON_HEDLEY_STRINGIFY_EX)
    #undef JSON_HEDLEY_STRINGIFY_EX
#endif
#define JSON_HEDLEY_STRINGIFY_EX(x) #x

#if defined(JSON_HEDLEY_STRINGIFY)
    #undef JSON_HEDLEY_STRINGIFY
#endif
#define JSON_HEDLEY_STRINGIFY(x) JSON_HEDLEY_STRINGIFY_EX(x)

#if defined(JSON_HEDLEY_CONCAT_EX)
    #undef JSON_HEDLEY_CONCAT_EX
#endif
#define JSON_HEDLEY_CONCAT_EX(a,b) a##b

#if defined(JSON_HEDLEY_CONCAT)
    #undef JSON_HEDLEY_CONCAT
#endif
#define JSON_HEDLEY_CONCAT(a,b) JSON_HEDLEY_CONCAT_EX(a,b)

#if defined(JSON_HEDLEY_VERSION_ENCODE)
    #undef JSON_HEDLEY_VERSION_ENCODE
#endif
#define JSON_HEDLEY_VERSION_ENCODE(major,minor,revision) (((major) * 1000000) + ((minor) * 1000) + (revision))

#if defined(JSON_HEDLEY_VERSION_DECODE_MAJOR)
    #undef JSON_HEDLEY_VERSION_DECODE_MAJOR
#endif
#define JSON_HEDLEY_VERSION_DECODE_MAJOR(version) ((version) / 1000000)

#if defined(JSON_HEDLEY_VERSION_DECODE_MINOR)
    #undef JSON_HEDLEY_VERSION_DECODE_MINOR
#endif
#define JSON_HEDLEY_VERSION_DECODE_MINOR(version) (((version) % 1000000) / 1000)

#if defined(JSON_HEDLEY_VERSION_DECODE_REVISION)
    #undef JSON_HEDLEY_VERSION_DECODE_REVISION
#endif
#define JSON_HEDLEY_VERSION_DECODE_REVISION(version) ((version) % 1000)

#if defined(JSON_HEDLEY_GNUC_VERSION)
    #undef JSON_HEDLEY_GNUC_VERSION
#endif
#if defined(__GNUC__) && defined(__GNUC_PATCHLEVEL__)
    #define JSON_HEDLEY_GNUC_VERSION JSON_HEDLEY_VERSION_ENCODE(__GNUC__, __GNUC_MINOR__, __GNUC_PATCHLEVEL__)
#elif defined(__GNUC__)
    #define JSON_HEDLEY_GNUC_VERSION JSON_HEDLEY_VERSION_ENCODE(__GNUC__, __GNUC_MINOR__, 0)
#endif

#if defined(JSON_HEDLEY_GNUC_VERSION_CHECK)
    #undef JSON_HEDLEY_GNUC_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_GNUC_VERSION)
    #define JSON_HEDLEY_GNUC_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_GNUC_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_GNUC_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_MSVC_VERSION)
    #undef JSON_HEDLEY_MSVC_VERSION
#endif
#if defined(_MSC_FULL_VER) && (_MSC_FULL_VER >= 140000000)
    #define JSON_HEDLEY_MSVC_VERSION JSON_HEDLEY_VERSION_ENCODE(_MSC_FULL_VER / 10000000, (_MSC_FULL_VER % 10000000) / 100000, (_MSC_FULL_VER % 100000) / 100)
#elif defined(_MSC_FULL_VER)
    #define JSON_HEDLEY_MSVC_VERSION JSON_HEDLEY_VERSION_ENCODE(_MSC_FULL_VER / 1000000, (_MSC_FULL_VER % 1000000) / 10000, (_MSC_FULL_VER % 10000) / 10)
#elif defined(_MSC_VER)
    #define JSON_HEDLEY_MSVC_VERSION JSON_HEDLEY_VERSION_ENCODE(_MSC_VER / 100, _MSC_VER % 100, 0)
#endif

#if defined(JSON_HEDLEY_MSVC_VERSION_CHECK)
    #undef JSON_HEDLEY_MSVC_VERSION_CHECK
#endif
#if !defined(_MSC_VER)
    #define JSON_HEDLEY_MSVC_VERSION_CHECK(major,minor,patch) (0)
#elif defined(_MSC_VER) && (_MSC_VER >= 1400)
    #define JSON_HEDLEY_MSVC_VERSION_CHECK(major,minor,patch) (_MSC_FULL_VER >= ((major * 10000000) + (minor * 100000) + (patch)))
#elif defined(_MSC_VER) && (_MSC_VER >= 1200)
    #define JSON_HEDLEY_MSVC_VERSION_CHECK(major,minor,patch) (_MSC_FULL_VER >= ((major * 1000000) + (minor * 10000) + (patch)))
#else
    #define JSON_HEDLEY_MSVC_VERSION_CHECK(major,minor,patch) (_MSC_VER >= ((major * 100) + (minor)))
#endif

#if defined(JSON_HEDLEY_INTEL_VERSION)
    #undef JSON_HEDLEY_INTEL_VERSION
#endif
#if defined(__INTEL_COMPILER) && defined(__INTEL_COMPILER_UPDATE)
    #define JSON_HEDLEY_INTEL_VERSION JSON_HEDLEY_VERSION_ENCODE(__INTEL_COMPILER / 100, __INTEL_COMPILER % 100, __INTEL_COMPILER_UPDATE)
#elif defined(__INTEL_COMPILER)
    #define JSON_HEDLEY_INTEL_VERSION JSON_HEDLEY_VERSION_ENCODE(__INTEL_COMPILER / 100, __INTEL_COMPILER % 100, 0)
#endif

#if defined(JSON_HEDLEY_INTEL_VERSION_CHECK)
    #undef JSON_HEDLEY_INTEL_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_INTEL_VERSION)
    #define JSON_HEDLEY_INTEL_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_INTEL_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_INTEL_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_PGI_VERSION)
    #undef JSON_HEDLEY_PGI_VERSION
#endif
#if defined(__PGI) && defined(__PGIC__) && defined(__PGIC_MINOR__) && defined(__PGIC_PATCHLEVEL__)
    #define JSON_HEDLEY_PGI_VERSION JSON_HEDLEY_VERSION_ENCODE(__PGIC__, __PGIC_MINOR__, __PGIC_PATCHLEVEL__)
#endif

#if defined(JSON_HEDLEY_PGI_VERSION_CHECK)
    #undef JSON_HEDLEY_PGI_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_PGI_VERSION)
    #define JSON_HEDLEY_PGI_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_PGI_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_PGI_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_SUNPRO_VERSION)
    #undef JSON_HEDLEY_SUNPRO_VERSION
#endif
#if defined(__SUNPRO_C) && (__SUNPRO_C > 0x1000)
    #define JSON_HEDLEY_SUNPRO_VERSION JSON_HEDLEY_VERSION_ENCODE((((__SUNPRO_C >> 16) & 0xf) * 10) + ((__SUNPRO_C >> 12) & 0xf), (((__SUNPRO_C >> 8) & 0xf) * 10) + ((__SUNPRO_C >> 4) & 0xf), (__SUNPRO_C & 0xf) * 10)
#elif defined(__SUNPRO_C)
    #define JSON_HEDLEY_SUNPRO_VERSION JSON_HEDLEY_VERSION_ENCODE((__SUNPRO_C >> 8) & 0xf, (__SUNPRO_C >> 4) & 0xf, (__SUNPRO_C) & 0xf)
#elif defined(__SUNPRO_CC) && (__SUNPRO_CC > 0x1000)
    #define JSON_HEDLEY_SUNPRO_VERSION JSON_HEDLEY_VERSION_ENCODE((((__SUNPRO_CC >> 16) & 0xf) * 10) + ((__SUNPRO_CC >> 12) & 0xf), (((__SUNPRO_CC >> 8) & 0xf) * 10) + ((__SUNPRO_CC >> 4) & 0xf), (__SUNPRO_CC & 0xf) * 10)
#elif defined(__SUNPRO_CC)
    #define JSON_HEDLEY_SUNPRO_VERSION JSON_HEDLEY_VERSION_ENCODE((__SUNPRO_CC >> 8) & 0xf, (__SUNPRO_CC >> 4) & 0xf, (__SUNPRO_CC) & 0xf)
#endif

#if defined(JSON_HEDLEY_SUNPRO_VERSION_CHECK)
    #undef JSON_HEDLEY_SUNPRO_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_SUNPRO_VERSION)
    #define JSON_HEDLEY_SUNPRO_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_SUNPRO_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_SUNPRO_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_EMSCRIPTEN_VERSION)
    #undef JSON_HEDLEY_EMSCRIPTEN_VERSION
#endif
#if defined(__EMSCRIPTEN__)
    #define JSON_HEDLEY_EMSCRIPTEN_VERSION JSON_HEDLEY_VERSION_ENCODE(__EMSCRIPTEN_major__, __EMSCRIPTEN_minor__, __EMSCRIPTEN_tiny__)
#endif

#if defined(JSON_HEDLEY_EMSCRIPTEN_VERSION_CHECK)
    #undef JSON_HEDLEY_EMSCRIPTEN_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_EMSCRIPTEN_VERSION)
    #define JSON_HEDLEY_EMSCRIPTEN_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_EMSCRIPTEN_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_EMSCRIPTEN_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_ARM_VERSION)
    #undef JSON_HEDLEY_ARM_VERSION
#endif
#if defined(__CC_ARM) && defined(__ARMCOMPILER_VERSION)
    #define JSON_HEDLEY_ARM_VERSION JSON_HEDLEY_VERSION_ENCODE(__ARMCOMPILER_VERSION / 1000000, (__ARMCOMPILER_VERSION % 1000000) / 10000, (__ARMCOMPILER_VERSION % 10000) / 100)
#elif defined(__CC_ARM) && defined(__ARMCC_VERSION)
    #define JSON_HEDLEY_ARM_VERSION JSON_HEDLEY_VERSION_ENCODE(__ARMCC_VERSION / 1000000, (__ARMCC_VERSION % 1000000) / 10000, (__ARMCC_VERSION % 10000) / 100)
#endif

#if defined(JSON_HEDLEY_ARM_VERSION_CHECK)
    #undef JSON_HEDLEY_ARM_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_ARM_VERSION)
    #define JSON_HEDLEY_ARM_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_ARM_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_ARM_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_IBM_VERSION)
    #undef JSON_HEDLEY_IBM_VERSION
#endif
#if defined(__ibmxl__)
    #define JSON_HEDLEY_IBM_VERSION JSON_HEDLEY_VERSION_ENCODE(__ibmxl_version__, __ibmxl_release__, __ibmxl_modification__)
#elif defined(__xlC__) && defined(__xlC_ver__)
    #define JSON_HEDLEY_IBM_VERSION JSON_HEDLEY_VERSION_ENCODE(__xlC__ >> 8, __xlC__ & 0xff, (__xlC_ver__ >> 8) & 0xff)
#elif defined(__xlC__)
    #define JSON_HEDLEY_IBM_VERSION JSON_HEDLEY_VERSION_ENCODE(__xlC__ >> 8, __xlC__ & 0xff, 0)
#endif

#if defined(JSON_HEDLEY_IBM_VERSION_CHECK)
    #undef JSON_HEDLEY_IBM_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_IBM_VERSION)
    #define JSON_HEDLEY_IBM_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_IBM_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_IBM_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_TI_VERSION)
    #undef JSON_HEDLEY_TI_VERSION
#endif
#if defined(__TI_COMPILER_VERSION__)
    #define JSON_HEDLEY_TI_VERSION JSON_HEDLEY_VERSION_ENCODE(__TI_COMPILER_VERSION__ / 1000000, (__TI_COMPILER_VERSION__ % 1000000) / 1000, (__TI_COMPILER_VERSION__ % 1000))
#endif

#if defined(JSON_HEDLEY_TI_VERSION_CHECK)
    #undef JSON_HEDLEY_TI_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_TI_VERSION)
    #define JSON_HEDLEY_TI_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_TI_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_TI_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_CRAY_VERSION)
    #undef JSON_HEDLEY_CRAY_VERSION
#endif
#if defined(_CRAYC)
    #if defined(_RELEASE_PATCHLEVEL)
        #define JSON_HEDLEY_CRAY_VERSION JSON_HEDLEY_VERSION_ENCODE(_RELEASE_MAJOR, _RELEASE_MINOR, _RELEASE_PATCHLEVEL)
    #else
        #define JSON_HEDLEY_CRAY_VERSION JSON_HEDLEY_VERSION_ENCODE(_RELEASE_MAJOR, _RELEASE_MINOR, 0)
    #endif
#endif

#if defined(JSON_HEDLEY_CRAY_VERSION_CHECK)
    #undef JSON_HEDLEY_CRAY_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_CRAY_VERSION)
    #define JSON_HEDLEY_CRAY_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_CRAY_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_CRAY_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_IAR_VERSION)
    #undef JSON_HEDLEY_IAR_VERSION
#endif
#if defined(__IAR_SYSTEMS_ICC__)
    #if __VER__ > 1000
        #define JSON_HEDLEY_IAR_VERSION JSON_HEDLEY_VERSION_ENCODE((__VER__ / 1000000), ((__VER__ / 1000) % 1000), (__VER__ % 1000))
    #else
        #define JSON_HEDLEY_IAR_VERSION JSON_HEDLEY_VERSION_ENCODE(VER / 100, __VER__ % 100, 0)
    #endif
#endif

#if defined(JSON_HEDLEY_IAR_VERSION_CHECK)
    #undef JSON_HEDLEY_IAR_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_IAR_VERSION)
    #define JSON_HEDLEY_IAR_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_IAR_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_IAR_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_TINYC_VERSION)
    #undef JSON_HEDLEY_TINYC_VERSION
#endif
#if defined(__TINYC__)
    #define JSON_HEDLEY_TINYC_VERSION JSON_HEDLEY_VERSION_ENCODE(__TINYC__ / 1000, (__TINYC__ / 100) % 10, __TINYC__ % 100)
#endif

#if defined(JSON_HEDLEY_TINYC_VERSION_CHECK)
    #undef JSON_HEDLEY_TINYC_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_TINYC_VERSION)
    #define JSON_HEDLEY_TINYC_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_TINYC_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_TINYC_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_DMC_VERSION)
    #undef JSON_HEDLEY_DMC_VERSION
#endif
#if defined(__DMC__)
    #define JSON_HEDLEY_DMC_VERSION JSON_HEDLEY_VERSION_ENCODE(__DMC__ >> 8, (__DMC__ >> 4) & 0xf, __DMC__ & 0xf)
#endif

#if defined(JSON_HEDLEY_DMC_VERSION_CHECK)
    #undef JSON_HEDLEY_DMC_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_DMC_VERSION)
    #define JSON_HEDLEY_DMC_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_DMC_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_DMC_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_COMPCERT_VERSION)
    #undef JSON_HEDLEY_COMPCERT_VERSION
#endif
#if defined(__COMPCERT_VERSION__)
    #define JSON_HEDLEY_COMPCERT_VERSION JSON_HEDLEY_VERSION_ENCODE(__COMPCERT_VERSION__ / 10000, (__COMPCERT_VERSION__ / 100) % 100, __COMPCERT_VERSION__ % 100)
#endif

#if defined(JSON_HEDLEY_COMPCERT_VERSION_CHECK)
    #undef JSON_HEDLEY_COMPCERT_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_COMPCERT_VERSION)
    #define JSON_HEDLEY_COMPCERT_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_COMPCERT_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_COMPCERT_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_PELLES_VERSION)
    #undef JSON_HEDLEY_PELLES_VERSION
#endif
#if defined(__POCC__)
    #define JSON_HEDLEY_PELLES_VERSION JSON_HEDLEY_VERSION_ENCODE(__POCC__ / 100, __POCC__ % 100, 0)
#endif

#if defined(JSON_HEDLEY_PELLES_VERSION_CHECK)
    #undef JSON_HEDLEY_PELLES_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_PELLES_VERSION)
    #define JSON_HEDLEY_PELLES_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_PELLES_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_PELLES_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_GCC_VERSION)
    #undef JSON_HEDLEY_GCC_VERSION
#endif
#if \
    defined(JSON_HEDLEY_GNUC_VERSION) && \
    !defined(__clang__) && \
    !defined(JSON_HEDLEY_INTEL_VERSION) && \
    !defined(JSON_HEDLEY_PGI_VERSION) && \
    !defined(JSON_HEDLEY_ARM_VERSION) && \
    !defined(JSON_HEDLEY_TI_VERSION) && \
    !defined(__COMPCERT__)
    #define JSON_HEDLEY_GCC_VERSION JSON_HEDLEY_GNUC_VERSION
#endif

#if defined(JSON_HEDLEY_GCC_VERSION_CHECK)
    #undef JSON_HEDLEY_GCC_VERSION_CHECK
#endif
#if defined(JSON_HEDLEY_GCC_VERSION)
    #define JSON_HEDLEY_GCC_VERSION_CHECK(major,minor,patch) (JSON_HEDLEY_GCC_VERSION >= JSON_HEDLEY_VERSION_ENCODE(major, minor, patch))
#else
    #define JSON_HEDLEY_GCC_VERSION_CHECK(major,minor,patch) (0)
#endif

#if defined(JSON_HEDLEY_HAS_ATTRIBUTE)
    #undef JSON_HEDLEY_HAS_ATTRIBUTE
#endif
#if defined(__has_attribute)
    #define JSON_HEDLEY_HAS_ATTRIBUTE(attribute) __has_attribute(attribute)
#else
    #define JSON_HEDLEY_HAS_ATTRIBUTE(attribute) (0)
#endif

#if defined(JSON_HEDLEY_GNUC_HAS_ATTRIBUTE)
    #undef JSON_HEDLEY_GNUC_HAS_ATTRIBUTE
#endif
#if defined(__has_attribute)
    #define JSON_HEDLEY_GNUC_HAS_ATTRIBUTE(attribute,major,minor,patch) __has_attribute(attribute)
#else
    #define JSON_HEDLEY_GNUC_HAS_ATTRIBUTE(attribute,major,minor,patch) JSON_HEDLEY_GNUC_VERSION_CHECK(major,minor,patch)
#endif

#if defined(JSON_HEDLEY_GCC_HAS_ATTRIBUTE)
    #undef JSON_HEDLEY_GCC_HAS_ATTRIBUTE
#endif
#if defined(__has_attribute)
    #define JSON_HEDLEY_GCC_HAS_ATTRIBUTE(attribute,major,minor,patch) __has_attribute(attribute)
#else
    #define JSON_HEDLEY_GCC_HAS_ATTRIBUTE(attribute,major,minor,patch) JSON_HEDLEY_GCC_VERSION_CHECK(major,minor,patch)
#endif

#if defined(JSON_HEDLEY_HAS_CPP_ATTRIBUTE)
    #undef JSON_HEDLEY_HAS_CPP_ATTRIBUTE
#endif
#if \
    defined(__has_cpp_attribute) && \
    defined(__cplusplus) && \
    (!defined(JSON_HEDLEY_SUNPRO_VERSION) || JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,15,0))
    #define JSON_HEDLEY_HAS_CPP_ATTRIBUTE(attribute) __has_cpp_attribute(attribute)
#else
    #define JSON_HEDLEY_HAS_CPP_ATTRIBUTE(attribute) (0)
#endif

#if defined(JSON_HEDLEY_HAS_CPP_ATTRIBUTE_NS)
    #undef JSON_HEDLEY_HAS_CPP_ATTRIBUTE_NS
#endif
#if !defined(__cplusplus) || !defined(__has_cpp_attribute)
    #define JSON_HEDLEY_HAS_CPP_ATTRIBUTE_NS(ns,attribute) (0)
#elif \
    !defined(JSON_HEDLEY_PGI_VERSION) && \
    (!defined(JSON_HEDLEY_SUNPRO_VERSION) || JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,15,0)) && \
    (!defined(JSON_HEDLEY_MSVC_VERSION) || JSON_HEDLEY_MSVC_VERSION_CHECK(19,20,0))
    #define JSON_HEDLEY_HAS_CPP_ATTRIBUTE_NS(ns,attribute) JSON_HEDLEY_HAS_CPP_ATTRIBUTE(ns::attribute)
#else
    #define JSON_HEDLEY_HAS_CPP_ATTRIBUTE_NS(ns,attribute) (0)
#endif

#if defined(JSON_HEDLEY_GNUC_HAS_CPP_ATTRIBUTE)
    #undef JSON_HEDLEY_GNUC_HAS_CPP_ATTRIBUTE
#endif
#if defined(__has_cpp_attribute) && defined(__cplusplus)
    #define JSON_HEDLEY_GNUC_HAS_CPP_ATTRIBUTE(attribute,major,minor,patch) __has_cpp_attribute(attribute)
#else
    #define JSON_HEDLEY_GNUC_HAS_CPP_ATTRIBUTE(attribute,major,minor,patch) JSON_HEDLEY_GNUC_VERSION_CHECK(major,minor,patch)
#endif

#if defined(JSON_HEDLEY_GCC_HAS_CPP_ATTRIBUTE)
    #undef JSON_HEDLEY_GCC_HAS_CPP_ATTRIBUTE
#endif
#if defined(__has_cpp_attribute) && defined(__cplusplus)
    #define JSON_HEDLEY_GCC_HAS_CPP_ATTRIBUTE(attribute,major,minor,patch) __has_cpp_attribute(attribute)
#else
    #define JSON_HEDLEY_GCC_HAS_CPP_ATTRIBUTE(attribute,major,minor,patch) JSON_HEDLEY_GCC_VERSION_CHECK(major,minor,patch)
#endif

#if defined(JSON_HEDLEY_HAS_BUILTIN)
    #undef JSON_HEDLEY_HAS_BUILTIN
#endif
#if defined(__has_builtin)
    #define JSON_HEDLEY_HAS_BUILTIN(builtin) __has_builtin(builtin)
#else
    #define JSON_HEDLEY_HAS_BUILTIN(builtin) (0)
#endif

#if defined(JSON_HEDLEY_GNUC_HAS_BUILTIN)
    #undef JSON_HEDLEY_GNUC_HAS_BUILTIN
#endif
#if defined(__has_builtin)
    #define JSON_HEDLEY_GNUC_HAS_BUILTIN(builtin,major,minor,patch) __has_builtin(builtin)
#else
    #define JSON_HEDLEY_GNUC_HAS_BUILTIN(builtin,major,minor,patch) JSON_HEDLEY_GNUC_VERSION_CHECK(major,minor,patch)
#endif

#if defined(JSON_HEDLEY_GCC_HAS_BUILTIN)
    #undef JSON_HEDLEY_GCC_HAS_BUILTIN
#endif
#if defined(__has_builtin)
    #define JSON_HEDLEY_GCC_HAS_BUILTIN(builtin,major,minor,patch) __has_builtin(builtin)
#else
    #define JSON_HEDLEY_GCC_HAS_BUILTIN(builtin,major,minor,patch) JSON_HEDLEY_GCC_VERSION_CHECK(major,minor,patch)
#endif

#if defined(JSON_HEDLEY_HAS_FEATURE)
    #undef JSON_HEDLEY_HAS_FEATURE
#endif
#if defined(__has_feature)
    #define JSON_HEDLEY_HAS_FEATURE(feature) __has_feature(feature)
#else
    #define JSON_HEDLEY_HAS_FEATURE(feature) (0)
#endif

#if defined(JSON_HEDLEY_GNUC_HAS_FEATURE)
    #undef JSON_HEDLEY_GNUC_HAS_FEATURE
#endif
#if defined(__has_feature)
    #define JSON_HEDLEY_GNUC_HAS_FEATURE(feature,major,minor,patch) __has_feature(feature)
#else
    #define JSON_HEDLEY_GNUC_HAS_FEATURE(feature,major,minor,patch) JSON_HEDLEY_GNUC_VERSION_CHECK(major,minor,patch)
#endif

#if defined(JSON_HEDLEY_GCC_HAS_FEATURE)
    #undef JSON_HEDLEY_GCC_HAS_FEATURE
#endif
#if defined(__has_feature)
    #define JSON_HEDLEY_GCC_HAS_FEATURE(feature,major,minor,patch) __has_feature(feature)
#else
    #define JSON_HEDLEY_GCC_HAS_FEATURE(feature,major,minor,patch) JSON_HEDLEY_GCC_VERSION_CHECK(major,minor,patch)
#endif

#if defined(JSON_HEDLEY_HAS_EXTENSION)
    #undef JSON_HEDLEY_HAS_EXTENSION
#endif
#if defined(__has_extension)
    #define JSON_HEDLEY_HAS_EXTENSION(extension) __has_extension(extension)
#else
    #define JSON_HEDLEY_HAS_EXTENSION(extension) (0)
#endif

#if defined(JSON_HEDLEY_GNUC_HAS_EXTENSION)
    #undef JSON_HEDLEY_GNUC_HAS_EXTENSION
#endif
#if defined(__has_extension)
    #define JSON_HEDLEY_GNUC_HAS_EXTENSION(extension,major,minor,patch) __has_extension(extension)
#else
    #define JSON_HEDLEY_GNUC_HAS_EXTENSION(extension,major,minor,patch) JSON_HEDLEY_GNUC_VERSION_CHECK(major,minor,patch)
#endif

#if defined(JSON_HEDLEY_GCC_HAS_EXTENSION)
    #undef JSON_HEDLEY_GCC_HAS_EXTENSION
#endif
#if defined(__has_extension)
    #define JSON_HEDLEY_GCC_HAS_EXTENSION(extension,major,minor,patch) __has_extension(extension)
#else
    #define JSON_HEDLEY_GCC_HAS_EXTENSION(extension,major,minor,patch) JSON_HEDLEY_GCC_VERSION_CHECK(major,minor,patch)
#endif

#if defined(JSON_HEDLEY_HAS_DECLSPEC_ATTRIBUTE)
    #undef JSON_HEDLEY_HAS_DECLSPEC_ATTRIBUTE
#endif
#if defined(__has_declspec_attribute)
    #define JSON_HEDLEY_HAS_DECLSPEC_ATTRIBUTE(attribute) __has_declspec_attribute(attribute)
#else
    #define JSON_HEDLEY_HAS_DECLSPEC_ATTRIBUTE(attribute) (0)
#endif

#if defined(JSON_HEDLEY_GNUC_HAS_DECLSPEC_ATTRIBUTE)
    #undef JSON_HEDLEY_GNUC_HAS_DECLSPEC_ATTRIBUTE
#endif
#if defined(__has_declspec_attribute)
    #define JSON_HEDLEY_GNUC_HAS_DECLSPEC_ATTRIBUTE(attribute,major,minor,patch) __has_declspec_attribute(attribute)
#else
    #define JSON_HEDLEY_GNUC_HAS_DECLSPEC_ATTRIBUTE(attribute,major,minor,patch) JSON_HEDLEY_GNUC_VERSION_CHECK(major,minor,patch)
#endif

#if defined(JSON_HEDLEY_GCC_HAS_DECLSPEC_ATTRIBUTE)
    #undef JSON_HEDLEY_GCC_HAS_DECLSPEC_ATTRIBUTE
#endif
#if defined(__has_declspec_attribute)
    #define JSON_HEDLEY_GCC_HAS_DECLSPEC_ATTRIBUTE(attribute,major,minor,patch) __has_declspec_attribute(attribute)
#else
    #define JSON_HEDLEY_GCC_HAS_DECLSPEC_ATTRIBUTE(attribute,major,minor,patch) JSON_HEDLEY_GCC_VERSION_CHECK(major,minor,patch)
#endif

#if defined(JSON_HEDLEY_HAS_WARNING)
    #undef JSON_HEDLEY_HAS_WARNING
#endif
#if defined(__has_warning)
    #define JSON_HEDLEY_HAS_WARNING(warning) __has_warning(warning)
#else
    #define JSON_HEDLEY_HAS_WARNING(warning) (0)
#endif

#if defined(JSON_HEDLEY_GNUC_HAS_WARNING)
    #undef JSON_HEDLEY_GNUC_HAS_WARNING
#endif
#if defined(__has_warning)
    #define JSON_HEDLEY_GNUC_HAS_WARNING(warning,major,minor,patch) __has_warning(warning)
#else
    #define JSON_HEDLEY_GNUC_HAS_WARNING(warning,major,minor,patch) JSON_HEDLEY_GNUC_VERSION_CHECK(major,minor,patch)
#endif

#if defined(JSON_HEDLEY_GCC_HAS_WARNING)
    #undef JSON_HEDLEY_GCC_HAS_WARNING
#endif
#if defined(__has_warning)
    #define JSON_HEDLEY_GCC_HAS_WARNING(warning,major,minor,patch) __has_warning(warning)
#else
    #define JSON_HEDLEY_GCC_HAS_WARNING(warning,major,minor,patch) JSON_HEDLEY_GCC_VERSION_CHECK(major,minor,patch)
#endif

/* JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_ is for
   HEDLEY INTERNAL USE ONLY.  API subject to change without notice. */
#if defined(JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_)
    #undef JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_
#endif
#if defined(__cplusplus) && JSON_HEDLEY_HAS_WARNING("-Wc++98-compat")
#  define JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_(xpr) \
    JSON_HEDLEY_DIAGNOSTIC_PUSH \
    _Pragma("clang diagnostic ignored \"-Wc++98-compat\"") \
    xpr \
    JSON_HEDLEY_DIAGNOSTIC_POP
#else
#  define JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_(x) x
#endif

#if \
    (defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L)) || \
    defined(__clang__) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(3,0,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_IAR_VERSION_CHECK(8,0,0) || \
    JSON_HEDLEY_PGI_VERSION_CHECK(18,4,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0) || \
    JSON_HEDLEY_TI_VERSION_CHECK(6,0,0) || \
    JSON_HEDLEY_CRAY_VERSION_CHECK(5,0,0) || \
    JSON_HEDLEY_TINYC_VERSION_CHECK(0,9,17) || \
    JSON_HEDLEY_SUNPRO_VERSION_CHECK(8,0,0) || \
    (JSON_HEDLEY_IBM_VERSION_CHECK(10,1,0) && defined(__C99_PRAGMA_OPERATOR))
    #define JSON_HEDLEY_PRAGMA(value) _Pragma(#value)
#elif JSON_HEDLEY_MSVC_VERSION_CHECK(15,0,0)
    #define JSON_HEDLEY_PRAGMA(value) __pragma(value)
#else
    #define JSON_HEDLEY_PRAGMA(value)
#endif

#if defined(JSON_HEDLEY_DIAGNOSTIC_PUSH)
    #undef JSON_HEDLEY_DIAGNOSTIC_PUSH
#endif
#if defined(JSON_HEDLEY_DIAGNOSTIC_POP)
    #undef JSON_HEDLEY_DIAGNOSTIC_POP
#endif
#if defined(__clang__)
    #define JSON_HEDLEY_DIAGNOSTIC_PUSH _Pragma("clang diagnostic push")
    #define JSON_HEDLEY_DIAGNOSTIC_POP _Pragma("clang diagnostic pop")
#elif JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_PUSH _Pragma("warning(push)")
    #define JSON_HEDLEY_DIAGNOSTIC_POP _Pragma("warning(pop)")
#elif JSON_HEDLEY_GCC_VERSION_CHECK(4,6,0)
    #define JSON_HEDLEY_DIAGNOSTIC_PUSH _Pragma("GCC diagnostic push")
    #define JSON_HEDLEY_DIAGNOSTIC_POP _Pragma("GCC diagnostic pop")
#elif JSON_HEDLEY_MSVC_VERSION_CHECK(15,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_PUSH __pragma(warning(push))
    #define JSON_HEDLEY_DIAGNOSTIC_POP __pragma(warning(pop))
#elif JSON_HEDLEY_ARM_VERSION_CHECK(5,6,0)
    #define JSON_HEDLEY_DIAGNOSTIC_PUSH _Pragma("push")
    #define JSON_HEDLEY_DIAGNOSTIC_POP _Pragma("pop")
#elif JSON_HEDLEY_TI_VERSION_CHECK(8,1,0)
    #define JSON_HEDLEY_DIAGNOSTIC_PUSH _Pragma("diag_push")
    #define JSON_HEDLEY_DIAGNOSTIC_POP _Pragma("diag_pop")
#elif JSON_HEDLEY_PELLES_VERSION_CHECK(2,90,0)
    #define JSON_HEDLEY_DIAGNOSTIC_PUSH _Pragma("warning(push)")
    #define JSON_HEDLEY_DIAGNOSTIC_POP _Pragma("warning(pop)")
#else
    #define JSON_HEDLEY_DIAGNOSTIC_PUSH
    #define JSON_HEDLEY_DIAGNOSTIC_POP
#endif

#if defined(JSON_HEDLEY_DIAGNOSTIC_DISABLE_DEPRECATED)
    #undef JSON_HEDLEY_DIAGNOSTIC_DISABLE_DEPRECATED
#endif
#if JSON_HEDLEY_HAS_WARNING("-Wdeprecated-declarations")
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_DEPRECATED _Pragma("clang diagnostic ignored \"-Wdeprecated-declarations\"")
#elif JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_DEPRECATED _Pragma("warning(disable:1478 1786)")
#elif JSON_HEDLEY_PGI_VERSION_CHECK(17,10,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_DEPRECATED _Pragma("diag_suppress 1215,1444")
#elif JSON_HEDLEY_GCC_VERSION_CHECK(4,3,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_DEPRECATED _Pragma("GCC diagnostic ignored \"-Wdeprecated-declarations\"")
#elif JSON_HEDLEY_MSVC_VERSION_CHECK(15,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_DEPRECATED __pragma(warning(disable:4996))
#elif JSON_HEDLEY_TI_VERSION_CHECK(8,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_DEPRECATED _Pragma("diag_suppress 1291,1718")
#elif JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,13,0) && !defined(__cplusplus)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_DEPRECATED _Pragma("error_messages(off,E_DEPRECATED_ATT,E_DEPRECATED_ATT_MESS)")
#elif JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,13,0) && defined(__cplusplus)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_DEPRECATED _Pragma("error_messages(off,symdeprecated,symdeprecated2)")
#elif JSON_HEDLEY_IAR_VERSION_CHECK(8,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_DEPRECATED _Pragma("diag_suppress=Pe1444,Pe1215")
#elif JSON_HEDLEY_PELLES_VERSION_CHECK(2,90,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_DEPRECATED _Pragma("warn(disable:2241)")
#else
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_DEPRECATED
#endif

#if defined(JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_PRAGMAS)
    #undef JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_PRAGMAS
#endif
#if JSON_HEDLEY_HAS_WARNING("-Wunknown-pragmas")
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_PRAGMAS _Pragma("clang diagnostic ignored \"-Wunknown-pragmas\"")
#elif JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_PRAGMAS _Pragma("warning(disable:161)")
#elif JSON_HEDLEY_PGI_VERSION_CHECK(17,10,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_PRAGMAS _Pragma("diag_suppress 1675")
#elif JSON_HEDLEY_GCC_VERSION_CHECK(4,3,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_PRAGMAS _Pragma("GCC diagnostic ignored \"-Wunknown-pragmas\"")
#elif JSON_HEDLEY_MSVC_VERSION_CHECK(15,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_PRAGMAS __pragma(warning(disable:4068))
#elif JSON_HEDLEY_TI_VERSION_CHECK(8,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_PRAGMAS _Pragma("diag_suppress 163")
#elif JSON_HEDLEY_IAR_VERSION_CHECK(8,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_PRAGMAS _Pragma("diag_suppress=Pe161")
#else
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_PRAGMAS
#endif

#if defined(JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_CPP_ATTRIBUTES)
    #undef JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_CPP_ATTRIBUTES
#endif
#if JSON_HEDLEY_HAS_WARNING("-Wunknown-attributes")
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_CPP_ATTRIBUTES _Pragma("clang diagnostic ignored \"-Wunknown-attributes\"")
#elif JSON_HEDLEY_GCC_VERSION_CHECK(4,6,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_CPP_ATTRIBUTES _Pragma("GCC diagnostic ignored \"-Wdeprecated-declarations\"")
#elif JSON_HEDLEY_INTEL_VERSION_CHECK(17,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_CPP_ATTRIBUTES _Pragma("warning(disable:1292)")
#elif JSON_HEDLEY_MSVC_VERSION_CHECK(19,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_CPP_ATTRIBUTES __pragma(warning(disable:5030))
#elif JSON_HEDLEY_PGI_VERSION_CHECK(17,10,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_CPP_ATTRIBUTES _Pragma("diag_suppress 1097")
#elif JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,14,0) && defined(__cplusplus)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_CPP_ATTRIBUTES _Pragma("error_messages(off,attrskipunsup)")
#elif JSON_HEDLEY_TI_VERSION_CHECK(8,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_CPP_ATTRIBUTES _Pragma("diag_suppress 1173")
#else
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_CPP_ATTRIBUTES
#endif

#if defined(JSON_HEDLEY_DIAGNOSTIC_DISABLE_CAST_QUAL)
    #undef JSON_HEDLEY_DIAGNOSTIC_DISABLE_CAST_QUAL
#endif
#if JSON_HEDLEY_HAS_WARNING("-Wcast-qual")
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_CAST_QUAL _Pragma("clang diagnostic ignored \"-Wcast-qual\"")
#elif JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_CAST_QUAL _Pragma("warning(disable:2203 2331)")
#elif JSON_HEDLEY_GCC_VERSION_CHECK(3,0,0)
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_CAST_QUAL _Pragma("GCC diagnostic ignored \"-Wcast-qual\"")
#else
    #define JSON_HEDLEY_DIAGNOSTIC_DISABLE_CAST_QUAL
#endif

#if defined(JSON_HEDLEY_DEPRECATED)
    #undef JSON_HEDLEY_DEPRECATED
#endif
#if defined(JSON_HEDLEY_DEPRECATED_FOR)
    #undef JSON_HEDLEY_DEPRECATED_FOR
#endif
#if defined(__cplusplus) && (__cplusplus >= 201402L)
    #define JSON_HEDLEY_DEPRECATED(since) JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_([[deprecated("Since " #since)]])
    #define JSON_HEDLEY_DEPRECATED_FOR(since, replacement) JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_([[deprecated("Since " #since "; use " #replacement)]])
#elif \
    JSON_HEDLEY_HAS_EXTENSION(attribute_deprecated_with_message) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(4,5,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(5,6,0) || \
    JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,13,0) || \
    JSON_HEDLEY_PGI_VERSION_CHECK(17,10,0) || \
    JSON_HEDLEY_TI_VERSION_CHECK(8,3,0)
    #define JSON_HEDLEY_DEPRECATED(since) __attribute__((__deprecated__("Since " #since)))
    #define JSON_HEDLEY_DEPRECATED_FOR(since, replacement) __attribute__((__deprecated__("Since " #since "; use " #replacement)))
#elif \
    JSON_HEDLEY_HAS_ATTRIBUTE(deprecated) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(3,1,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0) || \
    JSON_HEDLEY_TI_VERSION_CHECK(8,0,0) || \
    (JSON_HEDLEY_TI_VERSION_CHECK(7,3,0) && defined(__TI_GNU_ATTRIBUTE_SUPPORT__))
    #define JSON_HEDLEY_DEPRECATED(since) __attribute__((__deprecated__))
    #define JSON_HEDLEY_DEPRECATED_FOR(since, replacement) __attribute__((__deprecated__))
#elif JSON_HEDLEY_MSVC_VERSION_CHECK(14,0,0)
    #define JSON_HEDLEY_DEPRECATED(since) __declspec(deprecated("Since " # since))
    #define JSON_HEDLEY_DEPRECATED_FOR(since, replacement) __declspec(deprecated("Since " #since "; use " #replacement))
#elif \
    JSON_HEDLEY_MSVC_VERSION_CHECK(13,10,0) || \
    JSON_HEDLEY_PELLES_VERSION_CHECK(6,50,0)
    #define JSON_HEDLEY_DEPRECATED(since) __declspec(deprecated)
    #define JSON_HEDLEY_DEPRECATED_FOR(since, replacement) __declspec(deprecated)
#elif JSON_HEDLEY_IAR_VERSION_CHECK(8,0,0)
    #define JSON_HEDLEY_DEPRECATED(since) _Pragma("deprecated")
    #define JSON_HEDLEY_DEPRECATED_FOR(since, replacement) _Pragma("deprecated")
#else
    #define JSON_HEDLEY_DEPRECATED(since)
    #define JSON_HEDLEY_DEPRECATED_FOR(since, replacement)
#endif

#if defined(JSON_HEDLEY_UNAVAILABLE)
    #undef JSON_HEDLEY_UNAVAILABLE
#endif
#if \
    JSON_HEDLEY_HAS_ATTRIBUTE(warning) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(4,3,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0)
    #define JSON_HEDLEY_UNAVAILABLE(available_since) __attribute__((__warning__("Not available until " #available_since)))
#else
    #define JSON_HEDLEY_UNAVAILABLE(available_since)
#endif

#if defined(JSON_HEDLEY_WARN_UNUSED_RESULT)
    #undef JSON_HEDLEY_WARN_UNUSED_RESULT
#endif
#if defined(__cplusplus) && (__cplusplus >= 201703L)
    #define JSON_HEDLEY_WARN_UNUSED_RESULT JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_([[nodiscard]])
#elif \
    JSON_HEDLEY_HAS_ATTRIBUTE(warn_unused_result) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(3,4,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_TI_VERSION_CHECK(8,0,0) || \
    (JSON_HEDLEY_TI_VERSION_CHECK(7,3,0) && defined(__TI_GNU_ATTRIBUTE_SUPPORT__)) || \
    (JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,15,0) && defined(__cplusplus)) || \
    JSON_HEDLEY_PGI_VERSION_CHECK(17,10,0)
    #define JSON_HEDLEY_WARN_UNUSED_RESULT __attribute__((__warn_unused_result__))
#elif defined(_Check_return_) /* SAL */
    #define JSON_HEDLEY_WARN_UNUSED_RESULT _Check_return_
#else
    #define JSON_HEDLEY_WARN_UNUSED_RESULT
#endif

#if defined(JSON_HEDLEY_SENTINEL)
    #undef JSON_HEDLEY_SENTINEL
#endif
#if \
    JSON_HEDLEY_HAS_ATTRIBUTE(sentinel) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(4,0,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(5,4,0)
    #define JSON_HEDLEY_SENTINEL(position) __attribute__((__sentinel__(position)))
#else
    #define JSON_HEDLEY_SENTINEL(position)
#endif

#if defined(JSON_HEDLEY_NO_RETURN)
    #undef JSON_HEDLEY_NO_RETURN
#endif
#if JSON_HEDLEY_IAR_VERSION_CHECK(8,0,0)
    #define JSON_HEDLEY_NO_RETURN __noreturn
#elif JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0)
    #define JSON_HEDLEY_NO_RETURN __attribute__((__noreturn__))
#elif defined(__STDC_VERSION__) && __STDC_VERSION__ >= 201112L
    #define JSON_HEDLEY_NO_RETURN _Noreturn
#elif defined(__cplusplus) && (__cplusplus >= 201103L)
    #define JSON_HEDLEY_NO_RETURN JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_([[noreturn]])
#elif \
    JSON_HEDLEY_HAS_ATTRIBUTE(noreturn) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(3,2,0) || \
    JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,11,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0) || \
    JSON_HEDLEY_IBM_VERSION_CHECK(10,1,0) || \
    JSON_HEDLEY_TI_VERSION_CHECK(18,0,0) || \
    (JSON_HEDLEY_TI_VERSION_CHECK(17,3,0) && defined(__TI_GNU_ATTRIBUTE_SUPPORT__))
    #define JSON_HEDLEY_NO_RETURN __attribute__((__noreturn__))
#elif JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,10,0)
    #define JSON_HEDLEY_NO_RETURN _Pragma("does_not_return")
#elif JSON_HEDLEY_MSVC_VERSION_CHECK(13,10,0)
    #define JSON_HEDLEY_NO_RETURN __declspec(noreturn)
#elif JSON_HEDLEY_TI_VERSION_CHECK(6,0,0) && defined(__cplusplus)
    #define JSON_HEDLEY_NO_RETURN _Pragma("FUNC_NEVER_RETURNS;")
#elif JSON_HEDLEY_COMPCERT_VERSION_CHECK(3,2,0)
    #define JSON_HEDLEY_NO_RETURN __attribute((noreturn))
#elif JSON_HEDLEY_PELLES_VERSION_CHECK(9,0,0)
    #define JSON_HEDLEY_NO_RETURN __declspec(noreturn)
#else
    #define JSON_HEDLEY_NO_RETURN
#endif

#if defined(JSON_HEDLEY_NO_ESCAPE)
    #undef JSON_HEDLEY_NO_ESCAPE
#endif
#if JSON_HEDLEY_HAS_ATTRIBUTE(noescape)
    #define JSON_HEDLEY_NO_ESCAPE __attribute__((__noescape__))
#else
    #define JSON_HEDLEY_NO_ESCAPE
#endif

#if defined(JSON_HEDLEY_UNREACHABLE)
    #undef JSON_HEDLEY_UNREACHABLE
#endif
#if defined(JSON_HEDLEY_UNREACHABLE_RETURN)
    #undef JSON_HEDLEY_UNREACHABLE_RETURN
#endif
#if \
    (JSON_HEDLEY_HAS_BUILTIN(__builtin_unreachable) && (!defined(JSON_HEDLEY_ARM_VERSION))) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(4,5,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_IBM_VERSION_CHECK(13,1,5)
    #define JSON_HEDLEY_UNREACHABLE() __builtin_unreachable()
#elif JSON_HEDLEY_MSVC_VERSION_CHECK(13,10,0)
    #define JSON_HEDLEY_UNREACHABLE() __assume(0)
#elif JSON_HEDLEY_TI_VERSION_CHECK(6,0,0)
    #if defined(__cplusplus)
        #define JSON_HEDLEY_UNREACHABLE() std::_nassert(0)
    #else
        #define JSON_HEDLEY_UNREACHABLE() _nassert(0)
    #endif
    #define JSON_HEDLEY_UNREACHABLE_RETURN(value) return value
#elif defined(EXIT_FAILURE)
    #define JSON_HEDLEY_UNREACHABLE() abort()
#else
    #define JSON_HEDLEY_UNREACHABLE()
    #define JSON_HEDLEY_UNREACHABLE_RETURN(value) return value
#endif
#if !defined(JSON_HEDLEY_UNREACHABLE_RETURN)
    #define JSON_HEDLEY_UNREACHABLE_RETURN(value) JSON_HEDLEY_UNREACHABLE()
#endif

#if defined(JSON_HEDLEY_ASSUME)
    #undef JSON_HEDLEY_ASSUME
#endif
#if \
    JSON_HEDLEY_MSVC_VERSION_CHECK(13,10,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0)
    #define JSON_HEDLEY_ASSUME(expr) __assume(expr)
#elif JSON_HEDLEY_HAS_BUILTIN(__builtin_assume)
    #define JSON_HEDLEY_ASSUME(expr) __builtin_assume(expr)
#elif JSON_HEDLEY_TI_VERSION_CHECK(6,0,0)
    #if defined(__cplusplus)
        #define JSON_HEDLEY_ASSUME(expr) std::_nassert(expr)
    #else
        #define JSON_HEDLEY_ASSUME(expr) _nassert(expr)
    #endif
#elif \
    (JSON_HEDLEY_HAS_BUILTIN(__builtin_unreachable) && !defined(JSON_HEDLEY_ARM_VERSION)) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(4,5,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_IBM_VERSION_CHECK(13,1,5)
    #define JSON_HEDLEY_ASSUME(expr) ((void) ((expr) ? 1 : (__builtin_unreachable(), 1)))
#else
    #define JSON_HEDLEY_ASSUME(expr) ((void) (expr))
#endif

JSON_HEDLEY_DIAGNOSTIC_PUSH
#if JSON_HEDLEY_HAS_WARNING("-Wpedantic")
    #pragma clang diagnostic ignored "-Wpedantic"
#endif
#if JSON_HEDLEY_HAS_WARNING("-Wc++98-compat-pedantic") && defined(__cplusplus)
    #pragma clang diagnostic ignored "-Wc++98-compat-pedantic"
#endif
#if JSON_HEDLEY_GCC_HAS_WARNING("-Wvariadic-macros",4,0,0)
    #if defined(__clang__)
        #pragma clang diagnostic ignored "-Wvariadic-macros"
    #elif defined(JSON_HEDLEY_GCC_VERSION)
        #pragma GCC diagnostic ignored "-Wvariadic-macros"
    #endif
#endif
#if defined(JSON_HEDLEY_NON_NULL)
    #undef JSON_HEDLEY_NON_NULL
#endif
#if \
    JSON_HEDLEY_HAS_ATTRIBUTE(nonnull) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(3,3,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0)
    #define JSON_HEDLEY_NON_NULL(...) __attribute__((__nonnull__(__VA_ARGS__)))
#else
    #define JSON_HEDLEY_NON_NULL(...)
#endif
JSON_HEDLEY_DIAGNOSTIC_POP

#if defined(JSON_HEDLEY_PRINTF_FORMAT)
    #undef JSON_HEDLEY_PRINTF_FORMAT
#endif
#if defined(__MINGW32__) && JSON_HEDLEY_GCC_HAS_ATTRIBUTE(format,4,4,0) && !defined(__USE_MINGW_ANSI_STDIO)
    #define JSON_HEDLEY_PRINTF_FORMAT(string_idx,first_to_check) __attribute__((__format__(ms_printf, string_idx, first_to_check)))
#elif defined(__MINGW32__) && JSON_HEDLEY_GCC_HAS_ATTRIBUTE(format,4,4,0) && defined(__USE_MINGW_ANSI_STDIO)
    #define JSON_HEDLEY_PRINTF_FORMAT(string_idx,first_to_check) __attribute__((__format__(gnu_printf, string_idx, first_to_check)))
#elif \
    JSON_HEDLEY_HAS_ATTRIBUTE(format) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(3,1,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(5,6,0) || \
    JSON_HEDLEY_IBM_VERSION_CHECK(10,1,0) || \
    JSON_HEDLEY_TI_VERSION_CHECK(8,0,0) || \
    (JSON_HEDLEY_TI_VERSION_CHECK(7,3,0) && defined(__TI_GNU_ATTRIBUTE_SUPPORT__))
    #define JSON_HEDLEY_PRINTF_FORMAT(string_idx,first_to_check) __attribute__((__format__(__printf__, string_idx, first_to_check)))
#elif JSON_HEDLEY_PELLES_VERSION_CHECK(6,0,0)
    #define JSON_HEDLEY_PRINTF_FORMAT(string_idx,first_to_check) __declspec(vaformat(printf,string_idx,first_to_check))
#else
    #define JSON_HEDLEY_PRINTF_FORMAT(string_idx,first_to_check)
#endif

#if defined(JSON_HEDLEY_CONSTEXPR)
    #undef JSON_HEDLEY_CONSTEXPR
#endif
#if defined(__cplusplus)
    #if __cplusplus >= 201103L
        #define JSON_HEDLEY_CONSTEXPR JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_(constexpr)
    #endif
#endif
#if !defined(JSON_HEDLEY_CONSTEXPR)
    #define JSON_HEDLEY_CONSTEXPR
#endif

#if defined(JSON_HEDLEY_PREDICT)
    #undef JSON_HEDLEY_PREDICT
#endif
#if defined(JSON_HEDLEY_LIKELY)
    #undef JSON_HEDLEY_LIKELY
#endif
#if defined(JSON_HEDLEY_UNLIKELY)
    #undef JSON_HEDLEY_UNLIKELY
#endif
#if defined(JSON_HEDLEY_UNPREDICTABLE)
    #undef JSON_HEDLEY_UNPREDICTABLE
#endif
#if JSON_HEDLEY_HAS_BUILTIN(__builtin_unpredictable)
    #define JSON_HEDLEY_UNPREDICTABLE(expr) __builtin_unpredictable(!!(expr))
#endif
#if \
  JSON_HEDLEY_HAS_BUILTIN(__builtin_expect_with_probability) || \
  JSON_HEDLEY_GCC_VERSION_CHECK(9,0,0)
#  define JSON_HEDLEY_PREDICT(expr, value, probability) __builtin_expect_with_probability(expr, value, probability)
#  define JSON_HEDLEY_PREDICT_TRUE(expr, probability) __builtin_expect_with_probability(!!(expr), 1, probability)
#  define JSON_HEDLEY_PREDICT_FALSE(expr, probability) __builtin_expect_with_probability(!!(expr), 0, probability)
#  define JSON_HEDLEY_LIKELY(expr) __builtin_expect(!!(expr), 1)
#  define JSON_HEDLEY_UNLIKELY(expr) __builtin_expect(!!(expr), 0)
#if !defined(JSON_HEDLEY_BUILTIN_UNPREDICTABLE)
    #define JSON_HEDLEY_BUILTIN_UNPREDICTABLE(expr) __builtin_expect_with_probability(!!(expr), 1, 0.5)
#endif
#elif \
  JSON_HEDLEY_HAS_BUILTIN(__builtin_expect) || \
  JSON_HEDLEY_GCC_VERSION_CHECK(3,0,0) || \
  JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
  (JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,15,0) && defined(__cplusplus)) || \
  JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0) || \
  JSON_HEDLEY_IBM_VERSION_CHECK(10,1,0) || \
  JSON_HEDLEY_TI_VERSION_CHECK(6,1,0) || \
  JSON_HEDLEY_TINYC_VERSION_CHECK(0,9,27)
#  define JSON_HEDLEY_PREDICT(expr, expected, probability) \
    (((probability) >= 0.9) ? __builtin_expect(!!(expr), (expected)) : (((void) (expected)), !!(expr)))
#  define JSON_HEDLEY_PREDICT_TRUE(expr, probability) \
    (__extension__ ({ \
        JSON_HEDLEY_CONSTEXPR double hedley_probability_ = (probability); \
        ((hedley_probability_ >= 0.9) ? __builtin_expect(!!(expr), 1) : ((hedley_probability_ <= 0.1) ? __builtin_expect(!!(expr), 0) : !!(expr))); \
    }))
#  define JSON_HEDLEY_PREDICT_FALSE(expr, probability) \
    (__extension__ ({ \
        JSON_HEDLEY_CONSTEXPR double hedley_probability_ = (probability); \
        ((hedley_probability_ >= 0.9) ? __builtin_expect(!!(expr), 0) : ((hedley_probability_ <= 0.1) ? __builtin_expect(!!(expr), 1) : !!(expr))); \
    }))
#  define JSON_HEDLEY_LIKELY(expr)   __builtin_expect(!!(expr), 1)
#  define JSON_HEDLEY_UNLIKELY(expr) __builtin_expect(!!(expr), 0)
#else
#  define JSON_HEDLEY_PREDICT(expr, expected, probability) (((void) (expected)), !!(expr))
#  define JSON_HEDLEY_PREDICT_TRUE(expr, probability) (!!(expr))
#  define JSON_HEDLEY_PREDICT_FALSE(expr, probability) (!!(expr))
#  define JSON_HEDLEY_LIKELY(expr) (!!(expr))
#  define JSON_HEDLEY_UNLIKELY(expr) (!!(expr))
#endif
#if !defined(JSON_HEDLEY_UNPREDICTABLE)
    #define JSON_HEDLEY_UNPREDICTABLE(expr) JSON_HEDLEY_PREDICT(expr, 1, 0.5)
#endif

#if defined(JSON_HEDLEY_MALLOC)
    #undef JSON_HEDLEY_MALLOC
#endif
#if \
    JSON_HEDLEY_HAS_ATTRIBUTE(malloc) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(3,1,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,11,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0) || \
    JSON_HEDLEY_IBM_VERSION_CHECK(12,1,0) || \
    JSON_HEDLEY_TI_VERSION_CHECK(8,0,0) || \
    (JSON_HEDLEY_TI_VERSION_CHECK(7,3,0) && defined(__TI_GNU_ATTRIBUTE_SUPPORT__))
    #define JSON_HEDLEY_MALLOC __attribute__((__malloc__))
#elif JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,10,0)
    #define JSON_HEDLEY_MALLOC _Pragma("returns_new_memory")
#elif JSON_HEDLEY_MSVC_VERSION_CHECK(14, 0, 0)
    #define JSON_HEDLEY_MALLOC __declspec(restrict)
#else
    #define JSON_HEDLEY_MALLOC
#endif

#if defined(JSON_HEDLEY_PURE)
    #undef JSON_HEDLEY_PURE
#endif
#if \
    JSON_HEDLEY_HAS_ATTRIBUTE(pure) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(2,96,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,11,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0) || \
    JSON_HEDLEY_IBM_VERSION_CHECK(10,1,0) || \
    JSON_HEDLEY_TI_VERSION_CHECK(8,0,0) || \
    (JSON_HEDLEY_TI_VERSION_CHECK(7,3,0) && defined(__TI_GNU_ATTRIBUTE_SUPPORT__)) || \
    JSON_HEDLEY_PGI_VERSION_CHECK(17,10,0)
    #define JSON_HEDLEY_PURE __attribute__((__pure__))
#elif JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,10,0)
    #define JSON_HEDLEY_PURE _Pragma("does_not_write_global_data")
#elif JSON_HEDLEY_TI_VERSION_CHECK(6,0,0) && defined(__cplusplus)
    #define JSON_HEDLEY_PURE _Pragma("FUNC_IS_PURE;")
#else
    #define JSON_HEDLEY_PURE
#endif

#if defined(JSON_HEDLEY_CONST)
    #undef JSON_HEDLEY_CONST
#endif
#if \
    JSON_HEDLEY_HAS_ATTRIBUTE(const) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(2,5,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,11,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0) || \
    JSON_HEDLEY_IBM_VERSION_CHECK(10,1,0) || \
    JSON_HEDLEY_TI_VERSION_CHECK(8,0,0) || \
    (JSON_HEDLEY_TI_VERSION_CHECK(7,3,0) && defined(__TI_GNU_ATTRIBUTE_SUPPORT__)) || \
    JSON_HEDLEY_PGI_VERSION_CHECK(17,10,0)
    #define JSON_HEDLEY_CONST __attribute__((__const__))
#elif \
    JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,10,0)
    #define JSON_HEDLEY_CONST _Pragma("no_side_effect")
#else
    #define JSON_HEDLEY_CONST JSON_HEDLEY_PURE
#endif

#if defined(JSON_HEDLEY_RESTRICT)
    #undef JSON_HEDLEY_RESTRICT
#endif
#if defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) && !defined(__cplusplus)
    #define JSON_HEDLEY_RESTRICT restrict
#elif \
    JSON_HEDLEY_GCC_VERSION_CHECK(3,1,0) || \
    JSON_HEDLEY_MSVC_VERSION_CHECK(14,0,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0) || \
    JSON_HEDLEY_IBM_VERSION_CHECK(10,1,0) || \
    JSON_HEDLEY_PGI_VERSION_CHECK(17,10,0) || \
    JSON_HEDLEY_TI_VERSION_CHECK(8,0,0) || \
    (JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,14,0) && defined(__cplusplus)) || \
    JSON_HEDLEY_IAR_VERSION_CHECK(8,0,0) || \
    defined(__clang__)
    #define JSON_HEDLEY_RESTRICT __restrict
#elif JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,3,0) && !defined(__cplusplus)
    #define JSON_HEDLEY_RESTRICT _Restrict
#else
    #define JSON_HEDLEY_RESTRICT
#endif

#if defined(JSON_HEDLEY_INLINE)
    #undef JSON_HEDLEY_INLINE
#endif
#if \
    (defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L)) || \
    (defined(__cplusplus) && (__cplusplus >= 199711L))
    #define JSON_HEDLEY_INLINE inline
#elif \
    defined(JSON_HEDLEY_GCC_VERSION) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(6,2,0)
    #define JSON_HEDLEY_INLINE __inline__
#elif \
    JSON_HEDLEY_MSVC_VERSION_CHECK(12,0,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0) || \
    JSON_HEDLEY_TI_VERSION_CHECK(8,0,0)
    #define JSON_HEDLEY_INLINE __inline
#else
    #define JSON_HEDLEY_INLINE
#endif

#if defined(JSON_HEDLEY_ALWAYS_INLINE)
    #undef JSON_HEDLEY_ALWAYS_INLINE
#endif
#if \
    JSON_HEDLEY_HAS_ATTRIBUTE(always_inline) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(4,0,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,11,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0) || \
    JSON_HEDLEY_IBM_VERSION_CHECK(10,1,0) || \
    JSON_HEDLEY_TI_VERSION_CHECK(8,0,0) || \
    (JSON_HEDLEY_TI_VERSION_CHECK(7,3,0) && defined(__TI_GNU_ATTRIBUTE_SUPPORT__))
    #define JSON_HEDLEY_ALWAYS_INLINE __attribute__((__always_inline__)) JSON_HEDLEY_INLINE
#elif JSON_HEDLEY_MSVC_VERSION_CHECK(12,0,0)
    #define JSON_HEDLEY_ALWAYS_INLINE __forceinline
#elif JSON_HEDLEY_TI_VERSION_CHECK(7,0,0) && defined(__cplusplus)
    #define JSON_HEDLEY_ALWAYS_INLINE _Pragma("FUNC_ALWAYS_INLINE;")
#elif JSON_HEDLEY_IAR_VERSION_CHECK(8,0,0)
    #define JSON_HEDLEY_ALWAYS_INLINE _Pragma("inline=forced")
#else
    #define JSON_HEDLEY_ALWAYS_INLINE JSON_HEDLEY_INLINE
#endif

#if defined(JSON_HEDLEY_NEVER_INLINE)
    #undef JSON_HEDLEY_NEVER_INLINE
#endif
#if \
    JSON_HEDLEY_HAS_ATTRIBUTE(noinline) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(4,0,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,11,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0) || \
    JSON_HEDLEY_IBM_VERSION_CHECK(10,1,0) || \
    JSON_HEDLEY_TI_VERSION_CHECK(8,0,0) || \
    (JSON_HEDLEY_TI_VERSION_CHECK(7,3,0) && defined(__TI_GNU_ATTRIBUTE_SUPPORT__))
    #define JSON_HEDLEY_NEVER_INLINE __attribute__((__noinline__))
#elif JSON_HEDLEY_MSVC_VERSION_CHECK(13,10,0)
    #define JSON_HEDLEY_NEVER_INLINE __declspec(noinline)
#elif JSON_HEDLEY_PGI_VERSION_CHECK(10,2,0)
    #define JSON_HEDLEY_NEVER_INLINE _Pragma("noinline")
#elif JSON_HEDLEY_TI_VERSION_CHECK(6,0,0) && defined(__cplusplus)
    #define JSON_HEDLEY_NEVER_INLINE _Pragma("FUNC_CANNOT_INLINE;")
#elif JSON_HEDLEY_IAR_VERSION_CHECK(8,0,0)
    #define JSON_HEDLEY_NEVER_INLINE _Pragma("inline=never")
#elif JSON_HEDLEY_COMPCERT_VERSION_CHECK(3,2,0)
    #define JSON_HEDLEY_NEVER_INLINE __attribute((noinline))
#elif JSON_HEDLEY_PELLES_VERSION_CHECK(9,0,0)
    #define JSON_HEDLEY_NEVER_INLINE __declspec(noinline)
#else
    #define JSON_HEDLEY_NEVER_INLINE
#endif

#if defined(JSON_HEDLEY_PRIVATE)
    #undef JSON_HEDLEY_PRIVATE
#endif
#if defined(JSON_HEDLEY_PUBLIC)
    #undef JSON_HEDLEY_PUBLIC
#endif
#if defined(JSON_HEDLEY_IMPORT)
    #undef JSON_HEDLEY_IMPORT
#endif
#if defined(_WIN32) || defined(__CYGWIN__)
    #define JSON_HEDLEY_PRIVATE
    #define JSON_HEDLEY_PUBLIC   __declspec(dllexport)
    #define JSON_HEDLEY_IMPORT   __declspec(dllimport)
#else
    #if \
        JSON_HEDLEY_HAS_ATTRIBUTE(visibility) || \
        JSON_HEDLEY_GCC_VERSION_CHECK(3,3,0) || \
        JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,11,0) || \
        JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
        JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0) || \
        JSON_HEDLEY_IBM_VERSION_CHECK(13,1,0) || \
        JSON_HEDLEY_TI_VERSION_CHECK(8,0,0) || \
        (JSON_HEDLEY_TI_VERSION_CHECK(7,3,0) && defined(__TI_EABI__) && defined(__TI_GNU_ATTRIBUTE_SUPPORT__))
        #define JSON_HEDLEY_PRIVATE __attribute__((__visibility__("hidden")))
        #define JSON_HEDLEY_PUBLIC  __attribute__((__visibility__("default")))
    #else
        #define JSON_HEDLEY_PRIVATE
        #define JSON_HEDLEY_PUBLIC
    #endif
    #define JSON_HEDLEY_IMPORT    extern
#endif

#if defined(JSON_HEDLEY_NO_THROW)
    #undef JSON_HEDLEY_NO_THROW
#endif
#if \
    JSON_HEDLEY_HAS_ATTRIBUTE(nothrow) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(3,3,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0)
    #define JSON_HEDLEY_NO_THROW __attribute__((__nothrow__))
#elif \
    JSON_HEDLEY_MSVC_VERSION_CHECK(13,1,0) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0)
    #define JSON_HEDLEY_NO_THROW __declspec(nothrow)
#else
    #define JSON_HEDLEY_NO_THROW
#endif

#if defined(JSON_HEDLEY_FALL_THROUGH)
    #undef JSON_HEDLEY_FALL_THROUGH
#endif
#if JSON_HEDLEY_GNUC_HAS_ATTRIBUTE(fallthrough,7,0,0) && !defined(JSON_HEDLEY_PGI_VERSION)
    #define JSON_HEDLEY_FALL_THROUGH __attribute__((__fallthrough__))
#elif JSON_HEDLEY_HAS_CPP_ATTRIBUTE_NS(clang,fallthrough)
    #define JSON_HEDLEY_FALL_THROUGH JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_([[clang::fallthrough]])
#elif JSON_HEDLEY_HAS_CPP_ATTRIBUTE(fallthrough)
    #define JSON_HEDLEY_FALL_THROUGH JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_([[fallthrough]])
#elif defined(__fallthrough) /* SAL */
    #define JSON_HEDLEY_FALL_THROUGH __fallthrough
#else
    #define JSON_HEDLEY_FALL_THROUGH
#endif

#if defined(JSON_HEDLEY_RETURNS_NON_NULL)
    #undef JSON_HEDLEY_RETURNS_NON_NULL
#endif
#if \
    JSON_HEDLEY_HAS_ATTRIBUTE(returns_nonnull) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(4,9,0)
    #define JSON_HEDLEY_RETURNS_NON_NULL __attribute__((__returns_nonnull__))
#elif defined(_Ret_notnull_) /* SAL */
    #define JSON_HEDLEY_RETURNS_NON_NULL _Ret_notnull_
#else
    #define JSON_HEDLEY_RETURNS_NON_NULL
#endif

#if defined(JSON_HEDLEY_ARRAY_PARAM)
    #undef JSON_HEDLEY_ARRAY_PARAM
#endif
#if \
    defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) && \
    !defined(__STDC_NO_VLA__) && \
    !defined(__cplusplus) && \
    !defined(JSON_HEDLEY_PGI_VERSION) && \
    !defined(JSON_HEDLEY_TINYC_VERSION)
    #define JSON_HEDLEY_ARRAY_PARAM(name) (name)
#else
    #define JSON_HEDLEY_ARRAY_PARAM(name)
#endif

#if defined(JSON_HEDLEY_IS_CONSTANT)
    #undef JSON_HEDLEY_IS_CONSTANT
#endif
#if defined(JSON_HEDLEY_REQUIRE_CONSTEXPR)
    #undef JSON_HEDLEY_REQUIRE_CONSTEXPR
#endif
/* JSON_HEDLEY_IS_CONSTEXPR_ is for
   HEDLEY INTERNAL USE ONLY.  API subject to change without notice. */
#if defined(JSON_HEDLEY_IS_CONSTEXPR_)
    #undef JSON_HEDLEY_IS_CONSTEXPR_
#endif
#if \
    JSON_HEDLEY_HAS_BUILTIN(__builtin_constant_p) || \
    JSON_HEDLEY_GCC_VERSION_CHECK(3,4,0) || \
    JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
    JSON_HEDLEY_TINYC_VERSION_CHECK(0,9,19) || \
    JSON_HEDLEY_ARM_VERSION_CHECK(4,1,0) || \
    JSON_HEDLEY_IBM_VERSION_CHECK(13,1,0) || \
    JSON_HEDLEY_TI_VERSION_CHECK(6,1,0) || \
    (JSON_HEDLEY_SUNPRO_VERSION_CHECK(5,10,0) && !defined(__cplusplus)) || \
    JSON_HEDLEY_CRAY_VERSION_CHECK(8,1,0)
    #define JSON_HEDLEY_IS_CONSTANT(expr) __builtin_constant_p(expr)
#endif
#if !defined(__cplusplus)
#  if \
       JSON_HEDLEY_HAS_BUILTIN(__builtin_types_compatible_p) || \
       JSON_HEDLEY_GCC_VERSION_CHECK(3,4,0) || \
       JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
       JSON_HEDLEY_IBM_VERSION_CHECK(13,1,0) || \
       JSON_HEDLEY_CRAY_VERSION_CHECK(8,1,0) || \
       JSON_HEDLEY_ARM_VERSION_CHECK(5,4,0) || \
       JSON_HEDLEY_TINYC_VERSION_CHECK(0,9,24)
#if defined(__INTPTR_TYPE__)
    #define JSON_HEDLEY_IS_CONSTEXPR_(expr) __builtin_types_compatible_p(__typeof__((1 ? (void*) ((__INTPTR_TYPE__) ((expr) * 0)) : (int*) 0)), int*)
#else
    #include <stdint.h>
    #define JSON_HEDLEY_IS_CONSTEXPR_(expr) __builtin_types_compatible_p(__typeof__((1 ? (void*) ((intptr_t) ((expr) * 0)) : (int*) 0)), int*)
#endif
#  elif \
       (defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 201112L) && !defined(JSON_HEDLEY_SUNPRO_VERSION) && !defined(JSON_HEDLEY_PGI_VERSION)) || \
       JSON_HEDLEY_HAS_EXTENSION(c_generic_selections) || \
       JSON_HEDLEY_GCC_VERSION_CHECK(4,9,0) || \
       JSON_HEDLEY_INTEL_VERSION_CHECK(17,0,0) || \
       JSON_HEDLEY_IBM_VERSION_CHECK(12,1,0) || \
       JSON_HEDLEY_ARM_VERSION_CHECK(5,3,0)
#if defined(__INTPTR_TYPE__)
    #define JSON_HEDLEY_IS_CONSTEXPR_(expr) _Generic((1 ? (void*) ((__INTPTR_TYPE__) ((expr) * 0)) : (int*) 0), int*: 1, void*: 0)
#else
    #include <stdint.h>
    #define JSON_HEDLEY_IS_CONSTEXPR_(expr) _Generic((1 ? (void*) ((intptr_t) * 0) : (int*) 0), int*: 1, void*: 0)
#endif
#  elif \
       defined(JSON_HEDLEY_GCC_VERSION) || \
       defined(JSON_HEDLEY_INTEL_VERSION) || \
       defined(JSON_HEDLEY_TINYC_VERSION) || \
       defined(JSON_HEDLEY_TI_VERSION) || \
       defined(__clang__)
#    define JSON_HEDLEY_IS_CONSTEXPR_(expr) ( \
        sizeof(void) != \
        sizeof(*( \
                  1 ? \
                  ((void*) ((expr) * 0L) ) : \
((struct { char v[sizeof(void) * 2]; } *) 1) \
                ) \
              ) \
                                            )
#  endif
#endif
#if defined(JSON_HEDLEY_IS_CONSTEXPR_)
    #if !defined(JSON_HEDLEY_IS_CONSTANT)
        #define JSON_HEDLEY_IS_CONSTANT(expr) JSON_HEDLEY_IS_CONSTEXPR_(expr)
    #endif
    #define JSON_HEDLEY_REQUIRE_CONSTEXPR(expr) (JSON_HEDLEY_IS_CONSTEXPR_(expr) ? (expr) : (-1))
#else
    #if !defined(JSON_HEDLEY_IS_CONSTANT)
        #define JSON_HEDLEY_IS_CONSTANT(expr) (0)
    #endif
    #define JSON_HEDLEY_REQUIRE_CONSTEXPR(expr) (expr)
#endif

#if defined(JSON_HEDLEY_BEGIN_C_DECLS)
    #undef JSON_HEDLEY_BEGIN_C_DECLS
#endif
#if defined(JSON_HEDLEY_END_C_DECLS)
    #undef JSON_HEDLEY_END_C_DECLS
#endif
#if defined(JSON_HEDLEY_C_DECL)
    #undef JSON_HEDLEY_C_DECL
#endif
#if defined(__cplusplus)
    #define JSON_HEDLEY_BEGIN_C_DECLS extern "C" {
    #define JSON_HEDLEY_END_C_DECLS }
    #define JSON_HEDLEY_C_DECL extern "C"
#else
    #define JSON_HEDLEY_BEGIN_C_DECLS
    #define JSON_HEDLEY_END_C_DECLS
    #define JSON_HEDLEY_C_DECL
#endif

#if defined(JSON_HEDLEY_STATIC_ASSERT)
    #undef JSON_HEDLEY_STATIC_ASSERT
#endif
#if \
  !defined(__cplusplus) && ( \
      (defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 201112L)) || \
      JSON_HEDLEY_HAS_FEATURE(c_static_assert) || \
      JSON_HEDLEY_GCC_VERSION_CHECK(6,0,0) || \
      JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0) || \
      defined(_Static_assert) \
    )
#  define JSON_HEDLEY_STATIC_ASSERT(expr, message) _Static_assert(expr, message)
#elif \
  (defined(__cplusplus) && (__cplusplus >= 201103L)) || \
  JSON_HEDLEY_MSVC_VERSION_CHECK(16,0,0) || \
  (defined(__cplusplus) && JSON_HEDLEY_TI_VERSION_CHECK(8,3,0))
#  define JSON_HEDLEY_STATIC_ASSERT(expr, message) JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_(static_assert(expr, message))
#else
#  define JSON_HEDLEY_STATIC_ASSERT(expr, message)
#endif

#if defined(JSON_HEDLEY_CONST_CAST)
    #undef JSON_HEDLEY_CONST_CAST
#endif
#if defined(__cplusplus)
#  define JSON_HEDLEY_CONST_CAST(T, expr) (const_cast<T>(expr))
#elif \
  JSON_HEDLEY_HAS_WARNING("-Wcast-qual") || \
  JSON_HEDLEY_GCC_VERSION_CHECK(4,6,0) || \
  JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0)
#  define JSON_HEDLEY_CONST_CAST(T, expr) (__extension__ ({ \
        JSON_HEDLEY_DIAGNOSTIC_PUSH \
        JSON_HEDLEY_DIAGNOSTIC_DISABLE_CAST_QUAL \
        ((T) (expr)); \
        JSON_HEDLEY_DIAGNOSTIC_POP \
    }))
#else
#  define JSON_HEDLEY_CONST_CAST(T, expr) ((T) (expr))
#endif

#if defined(JSON_HEDLEY_REINTERPRET_CAST)
    #undef JSON_HEDLEY_REINTERPRET_CAST
#endif
#if defined(__cplusplus)
    #define JSON_HEDLEY_REINTERPRET_CAST(T, expr) (reinterpret_cast<T>(expr))
#else
    #define JSON_HEDLEY_REINTERPRET_CAST(T, expr) (*((T*) &(expr)))
#endif

#if defined(JSON_HEDLEY_STATIC_CAST)
    #undef JSON_HEDLEY_STATIC_CAST
#endif
#if defined(__cplusplus)
    #define JSON_HEDLEY_STATIC_CAST(T, expr) (static_cast<T>(expr))
#else
    #define JSON_HEDLEY_STATIC_CAST(T, expr) ((T) (expr))
#endif

#if defined(JSON_HEDLEY_CPP_CAST)
    #undef JSON_HEDLEY_CPP_CAST
#endif
#if defined(__cplusplus)
    #define JSON_HEDLEY_CPP_CAST(T, expr) static_cast<T>(expr)
#else
    #define JSON_HEDLEY_CPP_CAST(T, expr) (expr)
#endif

#if defined(JSON_HEDLEY_NULL)
    #undef JSON_HEDLEY_NULL
#endif
#if defined(__cplusplus)
    #if __cplusplus >= 201103L
        #define JSON_HEDLEY_NULL JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_(nullptr)
    #elif defined(NULL)
        #define JSON_HEDLEY_NULL NULL
    #else
        #define JSON_HEDLEY_NULL JSON_HEDLEY_STATIC_CAST(void*, 0)
    #endif
#elif defined(NULL)
    #define JSON_HEDLEY_NULL NULL
#else
    #define JSON_HEDLEY_NULL ((void*) 0)
#endif

#if defined(JSON_HEDLEY_MESSAGE)
    #undef JSON_HEDLEY_MESSAGE
#endif
#if JSON_HEDLEY_HAS_WARNING("-Wunknown-pragmas")
#  define JSON_HEDLEY_MESSAGE(msg) \
    JSON_HEDLEY_DIAGNOSTIC_PUSH \
    JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_PRAGMAS \
    JSON_HEDLEY_PRAGMA(message msg) \
    JSON_HEDLEY_DIAGNOSTIC_POP
#elif \
  JSON_HEDLEY_GCC_VERSION_CHECK(4,4,0) || \
  JSON_HEDLEY_INTEL_VERSION_CHECK(13,0,0)
#  define JSON_HEDLEY_MESSAGE(msg) JSON_HEDLEY_PRAGMA(message msg)
#elif JSON_HEDLEY_CRAY_VERSION_CHECK(5,0,0)
#  define JSON_HEDLEY_MESSAGE(msg) JSON_HEDLEY_PRAGMA(_CRI message msg)
#elif JSON_HEDLEY_IAR_VERSION_CHECK(8,0,0)
#  define JSON_HEDLEY_MESSAGE(msg) JSON_HEDLEY_PRAGMA(message(msg))
#elif JSON_HEDLEY_PELLES_VERSION_CHECK(2,0,0)
#  define JSON_HEDLEY_MESSAGE(msg) JSON_HEDLEY_PRAGMA(message(msg))
#else
#  define JSON_HEDLEY_MESSAGE(msg)
#endif

#if defined(JSON_HEDLEY_WARNING)
    #undef JSON_HEDLEY_WARNING
#endif
#if JSON_HEDLEY_HAS_WARNING("-Wunknown-pragmas")
#  define JSON_HEDLEY_WARNING(msg) \
    JSON_HEDLEY_DIAGNOSTIC_PUSH \
    JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_PRAGMAS \
    JSON_HEDLEY_PRAGMA(clang warning msg) \
    JSON_HEDLEY_DIAGNOSTIC_POP
#elif \
  JSON_HEDLEY_GCC_VERSION_CHECK(4,8,0) || \
  JSON_HEDLEY_PGI_VERSION_CHECK(18,4,0)
#  define JSON_HEDLEY_WARNING(msg) JSON_HEDLEY_PRAGMA(GCC warning msg)
#elif JSON_HEDLEY_MSVC_VERSION_CHECK(15,0,0)
#  define JSON_HEDLEY_WARNING(msg) JSON_HEDLEY_PRAGMA(message(msg))
#else
#  define JSON_HEDLEY_WARNING(msg) JSON_HEDLEY_MESSAGE(msg)
#endif

#if defined(JSON_HEDLEY_REQUIRE)
    #undef JSON_HEDLEY_REQUIRE
#endif
#if defined(JSON_HEDLEY_REQUIRE_MSG)
    #undef JSON_HEDLEY_REQUIRE_MSG
#endif
#if JSON_HEDLEY_HAS_ATTRIBUTE(diagnose_if)
#  if JSON_HEDLEY_HAS_WARNING("-Wgcc-compat")
#    define JSON_HEDLEY_REQUIRE(expr) \
    JSON_HEDLEY_DIAGNOSTIC_PUSH \
    _Pragma("clang diagnostic ignored \"-Wgcc-compat\"") \
    __attribute__((diagnose_if(!(expr), #expr, "error"))) \
    JSON_HEDLEY_DIAGNOSTIC_POP
#    define JSON_HEDLEY_REQUIRE_MSG(expr,msg) \
    JSON_HEDLEY_DIAGNOSTIC_PUSH \
    _Pragma("clang diagnostic ignored \"-Wgcc-compat\"") \
    __attribute__((diagnose_if(!(expr), msg, "error"))) \
    JSON_HEDLEY_DIAGNOSTIC_POP
#  else
#    define JSON_HEDLEY_REQUIRE(expr) __attribute__((diagnose_if(!(expr), #expr, "error")))
#    define JSON_HEDLEY_REQUIRE_MSG(expr,msg) __attribute__((diagnose_if(!(expr), msg, "error")))
#  endif
#else
#  define JSON_HEDLEY_REQUIRE(expr)
#  define JSON_HEDLEY_REQUIRE_MSG(expr,msg)
#endif

#if defined(JSON_HEDLEY_FLAGS)
    #undef JSON_HEDLEY_FLAGS
#endif
#if JSON_HEDLEY_HAS_ATTRIBUTE(flag_enum)
    #define JSON_HEDLEY_FLAGS __attribute__((__flag_enum__))
#endif

#if defined(JSON_HEDLEY_FLAGS_CAST)
    #undef JSON_HEDLEY_FLAGS_CAST
#endif
#if JSON_HEDLEY_INTEL_VERSION_CHECK(19,0,0)
#  define JSON_HEDLEY_FLAGS_CAST(T, expr) (__extension__ ({ \
        JSON_HEDLEY_DIAGNOSTIC_PUSH \
        _Pragma("warning(disable:188)") \
        ((T) (expr)); \
        JSON_HEDLEY_DIAGNOSTIC_POP \
    }))
#else
#  define JSON_HEDLEY_FLAGS_CAST(T, expr) JSON_HEDLEY_STATIC_CAST(T, expr)
#endif

#if defined(JSON_HEDLEY_EMPTY_BASES)
    #undef JSON_HEDLEY_EMPTY_BASES
#endif
#if JSON_HEDLEY_MSVC_VERSION_CHECK(19,0,23918) && !JSON_HEDLEY_MSVC_VERSION_CHECK(20,0,0)
    #define JSON_HEDLEY_EMPTY_BASES __declspec(empty_bases)
#else
    #define JSON_HEDLEY_EMPTY_BASES
#endif

/* Remaining macros are deprecated. */

#if defined(JSON_HEDLEY_GCC_NOT_CLANG_VERSION_CHECK)
    #undef JSON_HEDLEY_GCC_NOT_CLANG_VERSION_CHECK
#endif
#if defined(__clang__)
    #define JSON_HEDLEY_GCC_NOT_CLANG_VERSION_CHECK(major,minor,patch) (0)
#else
    #define JSON_HEDLEY_GCC_NOT_CLANG_VERSION_CHECK(major,minor,patch) JSON_HEDLEY_GCC_VERSION_CHECK(major,minor,patch)
#endif

#if defined(JSON_HEDLEY_CLANG_HAS_ATTRIBUTE)
    #undef JSON_HEDLEY_CLANG_HAS_ATTRIBUTE
#endif
#define JSON_HEDLEY_CLANG_HAS_ATTRIBUTE(attribute) JSON_HEDLEY_HAS_ATTRIBUTE(attribute)

#if defined(JSON_HEDLEY_CLANG_HAS_CPP_ATTRIBUTE)
    #undef JSON_HEDLEY_CLANG_HAS_CPP_ATTRIBUTE
#endif
#define JSON_HEDLEY_CLANG_HAS_CPP_ATTRIBUTE(attribute) JSON_HEDLEY_HAS_CPP_ATTRIBUTE(attribute)

#if defined(JSON_HEDLEY_CLANG_HAS_BUILTIN)
    #undef JSON_HEDLEY_CLANG_HAS_BUILTIN
#endif
#define JSON_HEDLEY_CLANG_HAS_BUILTIN(builtin) JSON_HEDLEY_HAS_BUILTIN(builtin)

#if defined(JSON_HEDLEY_CLANG_HAS_FEATURE)
    #undef JSON_HEDLEY_CLANG_HAS_FEATURE
#endif
#define JSON_HEDLEY_CLANG_HAS_FEATURE(feature) JSON_HEDLEY_HAS_FEATURE(feature)

#if defined(JSON_HEDLEY_CLANG_HAS_EXTENSION)
    #undef JSON_HEDLEY_CLANG_HAS_EXTENSION
#endif
#define JSON_HEDLEY_CLANG_HAS_EXTENSION(extension) JSON_HEDLEY_HAS_EXTENSION(extension)

#if defined(JSON_HEDLEY_CLANG_HAS_DECLSPEC_DECLSPEC_ATTRIBUTE)
    #undef JSON_HEDLEY_CLANG_HAS_DECLSPEC_DECLSPEC_ATTRIBUTE
#endif
#define JSON_HEDLEY_CLANG_HAS_DECLSPEC_ATTRIBUTE(attribute) JSON_HEDLEY_HAS_DECLSPEC_ATTRIBUTE(attribute)

#if defined(JSON_HEDLEY_CLANG_HAS_WARNING)
    #undef JSON_HEDLEY_CLANG_HAS_WARNING
#endif
#define JSON_HEDLEY_CLANG_HAS_WARNING(warning) JSON_HEDLEY_HAS_WARNING(warning)

#endif /* !defined(JSON_HEDLEY_VERSION) || (JSON_HEDLEY_VERSION < X) */


// This file contains all internal macro definitions
// You MUST include macro_unscope.hpp at the end of json.hpp to undef all of them

// exclude unsupported compilers
#if !defined(JSON_SKIP_UNSUPPORTED_COMPILER_CHECK)
    #if defined(__clang__)
        #if (__clang_major__ * 10000 + __clang_minor__ * 100 + __clang_patchlevel__) < 30400
            #error "unsupported Clang version - see https://github.com/nlohmann/json#supported-compilers"
        #endif
    #elif defined(__GNUC__) && !(defined(__ICC) || defined(__INTEL_COMPILER))
        #if (__GNUC__ * 10000 + __GNUC_MINOR__ * 100 + __GNUC_PATCHLEVEL__) < 40800
            #error "unsupported GCC version - see https://github.com/nlohmann/json#supported-compilers"
        #endif
    #endif
#endif

// C++ language standard detection
#if (defined(__cplusplus) && __cplusplus >= 201703L) || (defined(_HAS_CXX17) && _HAS_CXX17 == 1) // fix for issue #464
    #define JSON_HAS_CPP_17
    #define JSON_HAS_CPP_14
#elif (defined(__cplusplus) && __cplusplus >= 201402L) || (defined(_HAS_CXX14) && _HAS_CXX14 == 1)
    #define JSON_HAS_CPP_14
#endif

// disable float-equal warnings on GCC/clang
#if defined(__clang__) || defined(__GNUC__) || defined(__GNUG__)
    #pragma GCC diagnostic push
    #pragma GCC diagnostic ignored "-Wfloat-equal"
#endif

// disable documentation warnings on clang
#if defined(__clang__)
    #pragma GCC diagnostic push
    #pragma GCC diagnostic ignored "-Wdocumentation"
#endif

// allow to disable exceptions
#if (defined(__cpp_exceptions) || defined(__EXCEPTIONS) || defined(_CPPUNWIND)) && !defined(JSON_NOEXCEPTION)
    #define JSON_THROW(exception) throw exception
    #define JSON_TRY try
    #define JSON_CATCH(exception) catch(exception)
    #define JSON_INTERNAL_CATCH(exception) catch(exception)
#else
    #include <cstdlib>
    #define JSON_THROW(exception) std::abort()
    #define JSON_TRY if(true)
    #define JSON_CATCH(exception) if(false)
    #define JSON_INTERNAL_CATCH(exception) if(false)
#endif

// override exception macros
#if defined(JSON_THROW_USER)
    #undef JSON_THROW
    #define JSON_THROW JSON_THROW_USER
#endif
#if defined(JSON_TRY_USER)
    #undef JSON_TRY
    #define JSON_TRY JSON_TRY_USER
#endif
#if defined(JSON_CATCH_USER)
    #undef JSON_CATCH
    #define JSON_CATCH JSON_CATCH_USER
    #undef JSON_INTERNAL_CATCH
    #define JSON_INTERNAL_CATCH JSON_CATCH_USER
#endif
#if defined(JSON_INTERNAL_CATCH_USER)
    #undef JSON_INTERNAL_CATCH
    #define JSON_INTERNAL_CATCH JSON_INTERNAL_CATCH_USER
#endif

/*!
@brief macro to briefly define a mapping between an enum and JSON
@def NLOHMANN_JSON_SERIALIZE_ENUM
@since version 3.4.0
*/
#define NLOHMANN_JSON_SERIALIZE_ENUM(ENUM_TYPE, ...)                                            \
    template<typename BasicJsonType>                                                            \
    inline void to_json(BasicJsonType& j, const ENUM_TYPE& e)                                   \
    {                                                                                           \
        static_assert(std::is_enum<ENUM_TYPE>::value, #ENUM_TYPE " must be an enum!");          \
        static const std::pair<ENUM_TYPE, BasicJsonType> m[] = __VA_ARGS__;                     \
        auto it = std::find_if(std::begin(m), std::end(m),                                      \
                               [e](const std::pair<ENUM_TYPE, BasicJsonType>& ej_pair) -> bool  \
        {                                                                                       \
            return ej_pair.first == e;                                                          \
        });                                                                                     \
        j = ((it != std::end(m)) ? it : std::begin(m))->second;                                 \
    }                                                                                           \
    template<typename BasicJsonType>                                                            \
    inline void from_json(const BasicJsonType& j, ENUM_TYPE& e)                                 \
    {                                                                                           \
        static_assert(std::is_enum<ENUM_TYPE>::value, #ENUM_TYPE " must be an enum!");          \
        static const std::pair<ENUM_TYPE, BasicJsonType> m[] = __VA_ARGS__;                     \
        auto it = std::find_if(std::begin(m), std::end(m),                                      \
                               [&j](const std::pair<ENUM_TYPE, BasicJsonType>& ej_pair) -> bool \
        {                                                                                       \
            return ej_pair.second == j;                                                         \
        });                                                                                     \
        e = ((it != std::end(m)) ? it : std::begin(m))->first;                                  \
    }

// Ugly macros to avoid uglier copy-paste when specializing basic_json. They
// may be removed in the future once the class is split.

#define NLOHMANN_BASIC_JSON_TPL_DECLARATION                                \
    template<template<typename, typename, typename...> class ObjectType,   \
             template<typename, typename...> class ArrayType,              \
             class StringType, class BooleanType, class NumberIntegerType, \
             class NumberUnsignedType, class NumberFloatType,              \
             template<typename> class AllocatorType,                       \
             template<typename, typename = void> class JSONSerializer>

#define NLOHMANN_BASIC_JSON_TPL                                            \
    basic_json<ObjectType, ArrayType, StringType, BooleanType,             \
    NumberIntegerType, NumberUnsignedType, NumberFloatType,                \
    AllocatorType, JSONSerializer>


namespace nlohmann
{
namespace detail
{
////////////////
// exceptions //
////////////////

/*!
@brief general exception of the @ref basic_json class

This class is an extension of `std::exception` objects with a member @a id for
exception ids. It is used as the base class for all exceptions thrown by the
@ref basic_json class. This class can hence be used as "wildcard" to catch
exceptions.

Subclasses:
- @ref parse_error for exceptions indicating a parse error
- @ref invalid_iterator for exceptions indicating errors with iterators
- @ref type_error for exceptions indicating executing a member function with
                  a wrong type
- @ref out_of_range for exceptions indicating access out of the defined range
- @ref other_error for exceptions indicating other library errors

@internal
@note To have nothrow-copy-constructible exceptions, we internally use
      `std::runtime_error` which can cope with arbitrary-length error messages.
      Intermediate strings are built with static functions and then passed to
      the actual constructor.
@endinternal

@liveexample{The following code shows how arbitrary library exceptions can be
caught.,exception}

@since version 3.0.0
*/
class exception : public std::exception
{
  public:
    /// returns the explanatory string
    JSON_HEDLEY_RETURNS_NON_NULL
    const char* what() const noexcept override
    {
        return m.what();
    }

    /// the id of the exception
    const int id;

  protected:
    JSON_HEDLEY_NON_NULL(3)
    exception(int id_, const char* what_arg) : id(id_), m(what_arg) {}

    static std::string name(const std::string& ename, int id_)
    {
        return "[json.exception." + ename + "." + std::to_string(id_) + "] ";
    }

  private:
    /// an exception object as storage for error messages
    std::runtime_error m;
};

/*!
@brief exception indicating a parse error

This exception is thrown by the library when a parse error occurs. Parse errors
can occur during the deserialization of JSON text, CBOR, MessagePack, as well
as when using JSON Patch.

Member @a byte holds the byte index of the last read character in the input
file.

Exceptions have ids 1xx.

name / id                      | example message | description
------------------------------ | --------------- | -------------------------
json.exception.parse_error.101 | parse error at 2: unexpected end of input; expected string literal | This error indicates a syntax error while deserializing a JSON text. The error message describes that an unexpected token (character) was encountered, and the member @a byte indicates the error position.
json.exception.parse_error.102 | parse error at 14: missing or wrong low surrogate | JSON uses the `\uxxxx` format to describe Unicode characters. Code points above above 0xFFFF are split into two `\uxxxx` entries ("surrogate pairs"). This error indicates that the surrogate pair is incomplete or contains an invalid code point.
json.exception.parse_error.103 | parse error: code points above 0x10FFFF are invalid | Unicode supports code points up to 0x10FFFF. Code points above 0x10FFFF are invalid.
json.exception.parse_error.104 | parse error: JSON patch must be an array of objects | [RFC 6902](https://tools.ietf.org/html/rfc6902) requires a JSON Patch document to be a JSON document that represents an array of objects.
json.exception.parse_error.105 | parse error: operation must have string member 'op' | An operation of a JSON Patch document must contain exactly one "op" member, whose value indicates the operation to perform. Its value must be one of "add", "remove", "replace", "move", "copy", or "test"; other values are errors.
json.exception.parse_error.106 | parse error: array index '01' must not begin with '0' | An array index in a JSON Pointer ([RFC 6901](https://tools.ietf.org/html/rfc6901)) may be `0` or any number without a leading `0`.
json.exception.parse_error.107 | parse error: JSON pointer must be empty or begin with '/' - was: 'foo' | A JSON Pointer must be a Unicode string containing a sequence of zero or more reference tokens, each prefixed by a `/` character.
json.exception.parse_error.108 | parse error: escape character '~' must be followed with '0' or '1' | In a JSON Pointer, only `~0` and `~1` are valid escape sequences.
json.exception.parse_error.109 | parse error: array index 'one' is not a number | A JSON Pointer array index must be a number.
json.exception.parse_error.110 | parse error at 1: cannot read 2 bytes from vector | When parsing CBOR or MessagePack, the byte vector ends before the complete value has been read.
json.exception.parse_error.112 | parse error at 1: error reading CBOR; last byte: 0xF8 | Not all types of CBOR or MessagePack are supported. This exception occurs if an unsupported byte was read.
json.exception.parse_error.113 | parse error at 2: expected a CBOR string; last byte: 0x98 | While parsing a map key, a value that is not a string has been read.
json.exception.parse_error.114 | parse error: Unsupported BSON record type 0x0F | The parsing of the corresponding BSON record type is not implemented (yet).

@note For an input with n bytes, 1 is the index of the first character and n+1
      is the index of the terminating null byte or the end of file. This also
      holds true when reading a byte vector (CBOR or MessagePack).

@liveexample{The following code shows how a `parse_error` exception can be
caught.,parse_error}

@sa - @ref exception for the base class of the library exceptions
@sa - @ref invalid_iterator for exceptions indicating errors with iterators
@sa - @ref type_error for exceptions indicating executing a member function with
                    a wrong type
@sa - @ref out_of_range for exceptions indicating access out of the defined range
@sa - @ref other_error for exceptions indicating other library errors

@since version 3.0.0
*/
class parse_error : public exception
{
  public:
    /*!
    @brief create a parse error exception
    @param[in] id_       the id of the exception
    @param[in] pos       the position where the error occurred (or with
                         chars_read_total=0 if the position cannot be
                         determined)
    @param[in] what_arg  the explanatory string
    @return parse_error object
    */
    static parse_error create(int id_, const position_t& pos, const std::string& what_arg)
    {
        std::string w = exception::name("parse_error", id_) + "parse error" +
                        position_string(pos) + ": " + what_arg;
        return parse_error(id_, pos.chars_read_total, w.c_str());
    }

    static parse_error create(int id_, std::size_t byte_, const std::string& what_arg)
    {
        std::string w = exception::name("parse_error", id_) + "parse error" +
                        (byte_ != 0 ? (" at byte " + std::to_string(byte_)) : "") +
                        ": " + what_arg;
        return parse_error(id_, byte_, w.c_str());
    }

    /*!
    @brief byte index of the parse error

    The byte index of the last read character in the input file.

    @note For an input with n bytes, 1 is the index of the first character and
          n+1 is the index of the terminating null byte or the end of file.
          This also holds true when reading a byte vector (CBOR or MessagePack).
    */
    const std::size_t byte;

  private:
    parse_error(int id_, std::size_t byte_, const char* what_arg)
        : exception(id_, what_arg), byte(byte_) {}

    static std::string position_string(const position_t& pos)
    {
        return " at line " + std::to_string(pos.lines_read + 1) +
               ", column " + std::to_string(pos.chars_read_current_line);
    }
};

/*!
@brief exception indicating errors with iterators

This exception is thrown if iterators passed to a library function do not match
the expected semantics.

Exceptions have ids 2xx.

name / id                           | example message | description
----------------------------------- | --------------- | -------------------------
json.exception.invalid_iterator.201 | iterators are not compatible | The iterators passed to constructor @ref basic_json(InputIT first, InputIT last) are not compatible, meaning they do not belong to the same container. Therefore, the range (@a first, @a last) is invalid.
json.exception.invalid_iterator.202 | iterator does not fit current value | In an erase or insert function, the passed iterator @a pos does not belong to the JSON value for which the function was called. It hence does not define a valid position for the deletion/insertion.
json.exception.invalid_iterator.203 | iterators do not fit current value | Either iterator passed to function @ref erase(IteratorType first, IteratorType last) does not belong to the JSON value from which values shall be erased. It hence does not define a valid range to delete values from.
json.exception.invalid_iterator.204 | iterators out of range | When an iterator range for a primitive type (number, boolean, or string) is passed to a constructor or an erase function, this range has to be exactly (@ref begin(), @ref end()), because this is the only way the single stored value is expressed. All other ranges are invalid.
json.exception.invalid_iterator.205 | iterator out of range | When an iterator for a primitive type (number, boolean, or string) is passed to an erase function, the iterator has to be the @ref begin() iterator, because it is the only way to address the stored value. All other iterators are invalid.
json.exception.invalid_iterator.206 | cannot construct with iterators from null | The iterators passed to constructor @ref basic_json(InputIT first, InputIT last) belong to a JSON null value and hence to not define a valid range.
json.exception.invalid_iterator.207 | cannot use key() for non-object iterators | The key() member function can only be used on iterators belonging to a JSON object, because other types do not have a concept of a key.
json.exception.invalid_iterator.208 | cannot use operator[] for object iterators | The operator[] to specify a concrete offset cannot be used on iterators belonging to a JSON object, because JSON objects are unordered.
json.exception.invalid_iterator.209 | cannot use offsets with object iterators | The offset operators (+, -, +=, -=) cannot be used on iterators belonging to a JSON object, because JSON objects are unordered.
json.exception.invalid_iterator.210 | iterators do not fit | The iterator range passed to the insert function are not compatible, meaning they do not belong to the same container. Therefore, the range (@a first, @a last) is invalid.
json.exception.invalid_iterator.211 | passed iterators may not belong to container | The iterator range passed to the insert function must not be a subrange of the container to insert to.
json.exception.invalid_iterator.212 | cannot compare iterators of different containers | When two iterators are compared, they must belong to the same container.
json.exception.invalid_iterator.213 | cannot compare order of object iterators | The order of object iterators cannot be compared, because JSON objects are unordered.
json.exception.invalid_iterator.214 | cannot get value | Cannot get value for iterator: Either the iterator belongs to a null value or it is an iterator to a primitive type (number, boolean, or string), but the iterator is different to @ref begin().

@liveexample{The following code shows how an `invalid_iterator` exception can be
caught.,invalid_iterator}

@sa - @ref exception for the base class of the library exceptions
@sa - @ref parse_error for exceptions indicating a parse error
@sa - @ref type_error for exceptions indicating executing a member function with
                    a wrong type
@sa - @ref out_of_range for exceptions indicating access out of the defined range
@sa - @ref other_error for exceptions indicating other library errors

@since version 3.0.0
*/
class invalid_iterator : public exception
{
  public:
    static invalid_iterator create(int id_, const std::string& what_arg)
    {
        std::string w = exception::name("invalid_iterator", id_) + what_arg;
        return invalid_iterator(id_, w.c_str());
    }

  private:
    JSON_HEDLEY_NON_NULL(3)
    invalid_iterator(int id_, const char* what_arg)
        : exception(id_, what_arg) {}
};

/*!
@brief exception indicating executing a member function with a wrong type

This exception is thrown in case of a type error; that is, a library function is
executed on a JSON value whose type does not match the expected semantics.

Exceptions have ids 3xx.

name / id                     | example message | description
----------------------------- | --------------- | -------------------------
json.exception.type_error.301 | cannot create object from initializer list | To create an object from an initializer list, the initializer list must consist only of a list of pairs whose first element is a string. When this constraint is violated, an array is created instead.
json.exception.type_error.302 | type must be object, but is array | During implicit or explicit value conversion, the JSON type must be compatible to the target type. For instance, a JSON string can only be converted into string types, but not into numbers or boolean types.
json.exception.type_error.303 | incompatible ReferenceType for get_ref, actual type is object | To retrieve a reference to a value stored in a @ref basic_json object with @ref get_ref, the type of the reference must match the value type. For instance, for a JSON array, the @a ReferenceType must be @ref array_t &.
json.exception.type_error.304 | cannot use at() with string | The @ref at() member functions can only be executed for certain JSON types.
json.exception.type_error.305 | cannot use operator[] with string | The @ref operator[] member functions can only be executed for certain JSON types.
json.exception.type_error.306 | cannot use value() with string | The @ref value() member functions can only be executed for certain JSON types.
json.exception.type_error.307 | cannot use erase() with string | The @ref erase() member functions can only be executed for certain JSON types.
json.exception.type_error.308 | cannot use push_back() with string | The @ref push_back() and @ref operator+= member functions can only be executed for certain JSON types.
json.exception.type_error.309 | cannot use insert() with | The @ref insert() member functions can only be executed for certain JSON types.
json.exception.type_error.310 | cannot use swap() with number | The @ref swap() member functions can only be executed for certain JSON types.
json.exception.type_error.311 | cannot use emplace_back() with string | The @ref emplace_back() member function can only be executed for certain JSON types.
json.exception.type_error.312 | cannot use update() with string | The @ref update() member functions can only be executed for certain JSON types.
json.exception.type_error.313 | invalid value to unflatten | The @ref unflatten function converts an object whose keys are JSON Pointers back into an arbitrary nested JSON value. The JSON Pointers must not overlap, because then the resulting value would not be well defined.
json.exception.type_error.314 | only objects can be unflattened | The @ref unflatten function only works for an object whose keys are JSON Pointers.
json.exception.type_error.315 | values in object must be primitive | The @ref unflatten function only works for an object whose keys are JSON Pointers and whose values are primitive.
json.exception.type_error.316 | invalid UTF-8 byte at index 10: 0x7E | The @ref dump function only works with UTF-8 encoded strings; that is, if you assign a `std::string` to a JSON value, make sure it is UTF-8 encoded. |
json.exception.type_error.317 | JSON value cannot be serialized to requested format | The dynamic type of the object cannot be represented in the requested serialization format (e.g. a raw `true` or `null` JSON object cannot be serialized to BSON) |

@liveexample{The following code shows how a `type_error` exception can be
caught.,type_error}

@sa - @ref exception for the base class of the library exceptions
@sa - @ref parse_error for exceptions indicating a parse error
@sa - @ref invalid_iterator for exceptions indicating errors with iterators
@sa - @ref out_of_range for exceptions indicating access out of the defined range
@sa - @ref other_error for exceptions indicating other library errors

@since version 3.0.0
*/
class type_error : public exception
{
  public:
    static type_error create(int id_, const std::string& what_arg)
    {
        std::string w = exception::name("type_error", id_) + what_arg;
        return type_error(id_, w.c_str());
    }

  private:
    JSON_HEDLEY_NON_NULL(3)
    type_error(int id_, const char* what_arg) : exception(id_, what_arg) {}
};

/*!
@brief exception indicating access out of the defined range

This exception is thrown in case a library function is called on an input
parameter that exceeds the expected range, for instance in case of array
indices or nonexisting object keys.

Exceptions have ids 4xx.

name / id                       | example message | description
------------------------------- | --------------- | -------------------------
json.exception.out_of_range.401 | array index 3 is out of range | The provided array index @a i is larger than @a size-1.
json.exception.out_of_range.402 | array index '-' (3) is out of range | The special array index `-` in a JSON Pointer never describes a valid element of the array, but the index past the end. That is, it can only be used to add elements at this position, but not to read it.
json.exception.out_of_range.403 | key 'foo' not found | The provided key was not found in the JSON object.
json.exception.out_of_range.404 | unresolved reference token 'foo' | A reference token in a JSON Pointer could not be resolved.
json.exception.out_of_range.405 | JSON pointer has no parent | The JSON Patch operations 'remove' and 'add' can not be applied to the root element of the JSON value.
json.exception.out_of_range.406 | number overflow parsing '10E1000' | A parsed number could not be stored as without changing it to NaN or INF.
json.exception.out_of_range.407 | number overflow serializing '9223372036854775808' | UBJSON and BSON only support integer numbers up to 9223372036854775807. |
json.exception.out_of_range.408 | excessive array size: 8658170730974374167 | The size (following `#`) of an UBJSON array or object exceeds the maximal capacity. |
json.exception.out_of_range.409 | BSON key cannot contain code point U+0000 (at byte 2) | Key identifiers to be serialized to BSON cannot contain code point U+0000, since the key is stored as zero-terminated c-string |

@liveexample{The following code shows how an `out_of_range` exception can be
caught.,out_of_range}

@sa - @ref exception for the base class of the library exceptions
@sa - @ref parse_error for exceptions indicating a parse error
@sa - @ref invalid_iterator for exceptions indicating errors with iterators
@sa - @ref type_error for exceptions indicating executing a member function with
                    a wrong type
@sa - @ref other_error for exceptions indicating other library errors

@since version 3.0.0
*/
class out_of_range : public exception
{
  public:
    static out_of_range create(int id_, const std::string& what_arg)
    {
        std::string w = exception::name("out_of_range", id_) + what_arg;
        return out_of_range(id_, w.c_str());
    }

  private:
    JSON_HEDLEY_NON_NULL(3)
    out_of_range(int id_, const char* what_arg) : exception(id_, what_arg) {}
};

/*!
@brief exception indicating other library errors

This exception is thrown in case of errors that cannot be classified with the
other exception types.

Exceptions have ids 5xx.

name / id                      | example message | description
------------------------------ | --------------- | -------------------------
json.exception.other_error.501 | unsuccessful: {"op":"test","path":"/baz", "value":"bar"} | A JSON Patch operation 'test' failed. The unsuccessful operation is also printed.

@sa - @ref exception for the base class of the library exceptions
@sa - @ref parse_error for exceptions indicating a parse error
@sa - @ref invalid_iterator for exceptions indicating errors with iterators
@sa - @ref type_error for exceptions indicating executing a member function with
                    a wrong type
@sa - @ref out_of_range for exceptions indicating access out of the defined range

@liveexample{The following code shows how an `other_error` exception can be
caught.,other_error}

@since version 3.0.0
*/
class other_error : public exception
{
  public:
    static other_error create(int id_, const std::string& what_arg)
    {
        std::string w = exception::name("other_error", id_) + what_arg;
        return other_error(id_, w.c_str());
    }

  private:
    JSON_HEDLEY_NON_NULL(3)
    other_error(int id_, const char* what_arg) : exception(id_, what_arg) {}
};
}  // namespace detail
}  // namespace nlohmann

// #include <nlohmann/detail/macro_scope.hpp>

// #include <nlohmann/detail/meta/cpp_future.hpp>


#include <ciso646> // not
#include <cstddef> // size_t
#include <type_traits> // conditional, enable_if, false_type, integral_constant, is_constructible, is_integral, is_same, remove_cv, remove_reference, true_type

namespace nlohmann
{
namespace detail
{
// alias templates to reduce boilerplate
template<bool B, typename T = void>
using enable_if_t = typename std::enable_if<B, T>::type;

template<typename T>
using uncvref_t = typename std::remove_cv<typename std::remove_reference<T>::type>::type;

// implementation of C++14 index_sequence and affiliates
// source: https://stackoverflow.com/a/32223343
template<std::size_t... Ints>
struct index_sequence
{
    using type = index_sequence;
    using value_type = std::size_t;
    static constexpr std::size_t size() noexcept
    {
        return sizeof...(Ints);
    }
};

template<class Sequence1, class Sequence2>
struct merge_and_renumber;

template<std::size_t... I1, std::size_t... I2>
struct merge_and_renumber<index_sequence<I1...>, index_sequence<I2...>>
        : index_sequence < I1..., (sizeof...(I1) + I2)... > {};

template<std::size_t N>
struct make_index_sequence
    : merge_and_renumber < typename make_index_sequence < N / 2 >::type,
      typename make_index_sequence < N - N / 2 >::type > {};

template<> struct make_index_sequence<0> : index_sequence<> {};
template<> struct make_index_sequence<1> : index_sequence<0> {};

template<typename... Ts>
using index_sequence_for = make_index_sequence<sizeof...(Ts)>;

// dispatch utility (taken from ranges-v3)
template<unsigned N> struct priority_tag : priority_tag < N - 1 > {};
template<> struct priority_tag<0> {};

// taken from ranges-v3
template<typename T>
struct static_const
{
    static constexpr T value{};
};

template<typename T>
constexpr T static_const<T>::value;
}  // namespace detail
}  // namespace nlohmann

// #include <nlohmann/detail/meta/type_traits.hpp>


#include <ciso646> // not
#include <limits> // numeric_limits
#include <type_traits> // false_type, is_constructible, is_integral, is_same, true_type
#include <utility> // declval

// #include <nlohmann/detail/iterators/iterator_traits.hpp>


#include <iterator> // random_access_iterator_tag

// #include <nlohmann/detail/meta/void_t.hpp>


namespace nlohmann
{
namespace detail
{
template <typename ...Ts> struct make_void
{
    using type = void;
};
template <typename ...Ts> using void_t = typename make_void<Ts...>::type;
} // namespace detail
}  // namespace nlohmann

// #include <nlohmann/detail/meta/cpp_future.hpp>


namespace nlohmann
{
namespace detail
{
template <typename It, typename = void>
struct iterator_types {};

template <typename It>
struct iterator_types <
    It,
    void_t<typename It::difference_type, typename It::value_type, typename It::pointer,
    typename It::reference, typename It::iterator_category >>
{
    using difference_type = typename It::difference_type;
    using value_type = typename It::value_type;
    using pointer = typename It::pointer;
    using reference = typename It::reference;
    using iterator_category = typename It::iterator_category;
};

// This is required as some compilers implement std::iterator_traits in a way that
// doesn't work with SFINAE. See https://github.com/nlohmann/json/issues/1341.
template <typename T, typename = void>
struct iterator_traits
{
};

template <typename T>
struct iterator_traits < T, enable_if_t < !std::is_pointer<T>::value >>
            : iterator_types<T>
{
};

template <typename T>
struct iterator_traits<T*, enable_if_t<std::is_object<T>::value>>
{
    using iterator_category = std::random_access_iterator_tag;
    using value_type = T;
    using difference_type = ptrdiff_t;
    using pointer = T*;
    using reference = T&;
};
} // namespace detail
} // namespace nlohmann

// #include <nlohmann/detail/macro_scope.hpp>

// #include <nlohmann/detail/meta/cpp_future.hpp>

// #include <nlohmann/detail/meta/detected.hpp>


#include <type_traits>

// #include <nlohmann/detail/meta/void_t.hpp>


// http://en.cppreference.com/w/cpp/experimental/is_detected
namespace nlohmann
{
namespace detail
{
struct nonesuch
{
    nonesuch() = delete;
    ~nonesuch() = delete;
    nonesuch(nonesuch const&) = delete;
    nonesuch(nonesuch const&&) = delete;
    void operator=(nonesuch const&) = delete;
    void operator=(nonesuch&&) = delete;
};

template <class Default,
          class AlwaysVoid,
          template <class...> class Op,
          class... Args>
struct detector
{
    using value_t = std::false_type;
    using type = Default;
};

template <class Default, template <class...> class Op, class... Args>
struct detector<Default, void_t<Op<Args...>>, Op, Args...>
{
    using value_t = std::true_type;
    using type = Op<Args...>;
};

template <template <class...> class Op, class... Args>
using is_detected = typename detector<nonesuch, void, Op, Args...>::value_t;

template <template <class...> class Op, class... Args>
using detected_t = typename detector<nonesuch, void, Op, Args...>::type;

template <class Default, template <class...> class Op, class... Args>
using detected_or = detector<Default, void, Op, Args...>;

template <class Default, template <class...> class Op, class... Args>
using detected_or_t = typename detected_or<Default, Op, Args...>::type;

template <class Expected, template <class...> class Op, class... Args>
using is_detected_exact = std::is_same<Expected, detected_t<Op, Args...>>;

template <class To, template <class...> class Op, class... Args>
using is_detected_convertible =
    std::is_convertible<detected_t<Op, Args...>, To>;
}  // namespace detail
}  // namespace nlohmann

// #include <nlohmann/json_fwd.hpp>
#ifndef INCLUDE_NLOHMANN_JSON_FWD_HPP_
#define INCLUDE_NLOHMANN_JSON_FWD_HPP_

#include <cstdint> // int64_t, uint64_t
#include <map> // map
#include <memory> // allocator
#include <string> // string
#include <vector> // vector

/*!
@brief namespace for Niels Lohmann
@see https://github.com/nlohmann
@since version 1.0.0
*/
namespace nlohmann
{
/*!
@brief default JSONSerializer template argument

This serializer ignores the template arguments and uses ADL
([argument-dependent lookup](https://en.cppreference.com/w/cpp/language/adl))
for serialization.
*/
template<typename T = void, typename SFINAE = void>
struct adl_serializer;

template<template<typename U, typename V, typename... Args> class ObjectType =
         std::map,
         template<typename U, typename... Args> class ArrayType = std::vector,
         class StringType = std::string, class BooleanType = bool,
         class NumberIntegerType = std::int64_t,
         class NumberUnsignedType = std::uint64_t,
         class NumberFloatType = double,
         template<typename U> class AllocatorType = std::allocator,
         template<typename T, typename SFINAE = void> class JSONSerializer =
         adl_serializer>
class basic_json;

/*!
@brief JSON Pointer

A JSON pointer defines a string syntax for identifying a specific value
within a JSON document. It can be used with functions `at` and
`operator[]`. Furthermore, JSON pointers are the base for JSON patches.

@sa [RFC 6901](https://tools.ietf.org/html/rfc6901)

@since version 2.0.0
*/
template<typename BasicJsonType>
class json_pointer;

/*!
@brief default JSON class

This type is the default specialization of the @ref basic_json class which
uses the standard template types.

@since version 1.0.0
*/
using json = basic_json<>;
}  // namespace nlohmann

#endif  // INCLUDE_NLOHMANN_JSON_FWD_HPP_


namespace nlohmann
{
/*!
@brief detail namespace with internal helper functions

This namespace collects functions that should not be exposed,
implementations of some @ref basic_json methods, and meta-programming helpers.

@since version 2.1.0
*/
namespace detail
{
/////////////
// helpers //
/////////////

// Note to maintainers:
//
// Every trait in this file expects a non CV-qualified type.
// The only exceptions are in the 'aliases for detected' section
// (i.e. those of the form: decltype(T::member_function(std::declval<T>())))
//
// In this case, T has to be properly CV-qualified to constraint the function arguments
// (e.g. to_json(BasicJsonType&, const T&))

template<typename> struct is_basic_json : std::false_type {};

NLOHMANN_BASIC_JSON_TPL_DECLARATION
struct is_basic_json<NLOHMANN_BASIC_JSON_TPL> : std::true_type {};

//////////////////////////
// aliases for detected //
//////////////////////////

template <typename T>
using mapped_type_t = typename T::mapped_type;

template <typename T>
using key_type_t = typename T::key_type;

template <typename T>
using value_type_t = typename T::value_type;

template <typename T>
using difference_type_t = typename T::difference_type;

template <typename T>
using pointer_t = typename T::pointer;

template <typename T>
using reference_t = typename T::reference;

template <typename T>
using iterator_category_t = typename T::iterator_category;

template <typename T>
using iterator_t = typename T::iterator;

template <typename T, typename... Args>
using to_json_function = decltype(T::to_json(std::declval<Args>()...));

template <typename T, typename... Args>
using from_json_function = decltype(T::from_json(std::declval<Args>()...));

template <typename T, typename U>
using get_template_function = decltype(std::declval<T>().template get<U>());

// trait checking if JSONSerializer<T>::from_json(json const&, udt&) exists
template <typename BasicJsonType, typename T, typename = void>
struct has_from_json : std::false_type {};

template <typename BasicJsonType, typename T>
struct has_from_json<BasicJsonType, T,
           enable_if_t<not is_basic_json<T>::value>>
{
    using serializer = typename BasicJsonType::template json_serializer<T, void>;

    static constexpr bool value =
        is_detected_exact<void, from_json_function, serializer,
        const BasicJsonType&, T&>::value;
};

// This trait checks if JSONSerializer<T>::from_json(json const&) exists
// this overload is used for non-default-constructible user-defined-types
template <typename BasicJsonType, typename T, typename = void>
struct has_non_default_from_json : std::false_type {};

template<typename BasicJsonType, typename T>
struct has_non_default_from_json<BasicJsonType, T, enable_if_t<not is_basic_json<T>::value>>
{
    using serializer = typename BasicJsonType::template json_serializer<T, void>;

    static constexpr bool value =
        is_detected_exact<T, from_json_function, serializer,
        const BasicJsonType&>::value;
};

// This trait checks if BasicJsonType::json_serializer<T>::to_json exists
// Do not evaluate the trait when T is a basic_json type, to avoid template instantiation infinite recursion.
template <typename BasicJsonType, typename T, typename = void>
struct has_to_json : std::false_type {};

template <typename BasicJsonType, typename T>
struct has_to_json<BasicJsonType, T, enable_if_t<not is_basic_json<T>::value>>
{
    using serializer = typename BasicJsonType::template json_serializer<T, void>;

    static constexpr bool value =
        is_detected_exact<void, to_json_function, serializer, BasicJsonType&,
        T>::value;
};


///////////////////
// is_ functions //
///////////////////

template <typename T, typename = void>
struct is_iterator_traits : std::false_type {};

template <typename T>
struct is_iterator_traits<iterator_traits<T>>
{
  private:
    using traits = iterator_traits<T>;

  public:
    static constexpr auto value =
        is_detected<value_type_t, traits>::value &&
        is_detected<difference_type_t, traits>::value &&
        is_detected<pointer_t, traits>::value &&
        is_detected<iterator_category_t, traits>::value &&
        is_detected<reference_t, traits>::value;
};

// source: https://stackoverflow.com/a/37193089/4116453

template <typename T, typename = void>
struct is_complete_type : std::false_type {};

template <typename T>
struct is_complete_type<T, decltype(void(sizeof(T)))> : std::true_type {};

template <typename BasicJsonType, typename CompatibleObjectType,
          typename = void>
struct is_compatible_object_type_impl : std::false_type {};

template <typename BasicJsonType, typename CompatibleObjectType>
struct is_compatible_object_type_impl <
    BasicJsonType, CompatibleObjectType,
    enable_if_t<is_detected<mapped_type_t, CompatibleObjectType>::value and
    is_detected<key_type_t, CompatibleObjectType>::value >>
{

    using object_t = typename BasicJsonType::object_t;

    // macOS's is_constructible does not play well with nonesuch...
    static constexpr bool value =
        std::is_constructible<typename object_t::key_type,
        typename CompatibleObjectType::key_type>::value and
        std::is_constructible<typename object_t::mapped_type,
        typename CompatibleObjectType::mapped_type>::value;
};

template <typename BasicJsonType, typename CompatibleObjectType>
struct is_compatible_object_type
    : is_compatible_object_type_impl<BasicJsonType, CompatibleObjectType> {};

template <typename BasicJsonType, typename ConstructibleObjectType,
          typename = void>
struct is_constructible_object_type_impl : std::false_type {};

template <typename BasicJsonType, typename ConstructibleObjectType>
struct is_constructible_object_type_impl <
    BasicJsonType, ConstructibleObjectType,
    enable_if_t<is_detected<mapped_type_t, ConstructibleObjectType>::value and
    is_detected<key_type_t, ConstructibleObjectType>::value >>
{
    using object_t = typename BasicJsonType::object_t;

    static constexpr bool value =
        (std::is_default_constructible<ConstructibleObjectType>::value and
         (std::is_move_assignable<ConstructibleObjectType>::value or
          std::is_copy_assignable<ConstructibleObjectType>::value) and
         (std::is_constructible<typename ConstructibleObjectType::key_type,
          typename object_t::key_type>::value and
          std::is_same <
          typename object_t::mapped_type,
          typename ConstructibleObjectType::mapped_type >::value)) or
        (has_from_json<BasicJsonType,
         typename ConstructibleObjectType::mapped_type>::value or
         has_non_default_from_json <
         BasicJsonType,
         typename ConstructibleObjectType::mapped_type >::value);
};

template <typename BasicJsonType, typename ConstructibleObjectType>
struct is_constructible_object_type
    : is_constructible_object_type_impl<BasicJsonType,
      ConstructibleObjectType> {};

template <typename BasicJsonType, typename CompatibleStringType,
          typename = void>
struct is_compatible_string_type_impl : std::false_type {};

template <typename BasicJsonType, typename CompatibleStringType>
struct is_compatible_string_type_impl <
    BasicJsonType, CompatibleStringType,
    enable_if_t<is_detected_exact<typename BasicJsonType::string_t::value_type,
    value_type_t, CompatibleStringType>::value >>
{
    static constexpr auto value =
        std::is_constructible<typename BasicJsonType::string_t, CompatibleStringType>::value;
};

template <typename BasicJsonType, typename ConstructibleStringType>
struct is_compatible_string_type
    : is_compatible_string_type_impl<BasicJsonType, ConstructibleStringType> {};

template <typename BasicJsonType, typename ConstructibleStringType,
          typename = void>
struct is_constructible_string_type_impl : std::false_type {};

template <typename BasicJsonType, typename ConstructibleStringType>
struct is_constructible_string_type_impl <
    BasicJsonType, ConstructibleStringType,
    enable_if_t<is_detected_exact<typename BasicJsonType::string_t::value_type,
    value_type_t, ConstructibleStringType>::value >>
{
    static constexpr auto value =
        std::is_constructible<ConstructibleStringType,
        typename BasicJsonType::string_t>::value;
};

template <typename BasicJsonType, typename ConstructibleStringType>
struct is_constructible_string_type
    : is_constructible_string_type_impl<BasicJsonType, ConstructibleStringType> {};

template <typename BasicJsonType, typename CompatibleArrayType, typename = void>
struct is_compatible_array_type_impl : std::false_type {};

template <typename BasicJsonType, typename CompatibleArrayType>
struct is_compatible_array_type_impl <
    BasicJsonType, CompatibleArrayType,
    enable_if_t<is_detected<value_type_t, CompatibleArrayType>::value and
    is_detected<iterator_t, CompatibleArrayType>::value and
// This is needed because json_reverse_iterator has a ::iterator type...
// Therefore it is detected as a CompatibleArrayType.
// The real fix would be to have an Iterable concept.
    not is_iterator_traits<
    iterator_traits<CompatibleArrayType>>::value >>
{
    static constexpr bool value =
        std::is_constructible<BasicJsonType,
        typename CompatibleArrayType::value_type>::value;
};

template <typename BasicJsonType, typename CompatibleArrayType>
struct is_compatible_array_type
    : is_compatible_array_type_impl<BasicJsonType, CompatibleArrayType> {};

template <typename BasicJsonType, typename ConstructibleArrayType, typename = void>
struct is_constructible_array_type_impl : std::false_type {};

template <typename BasicJsonType, typename ConstructibleArrayType>
struct is_constructible_array_type_impl <
    BasicJsonType, ConstructibleArrayType,
    enable_if_t<std::is_same<ConstructibleArrayType,
    typename BasicJsonType::value_type>::value >>
            : std::true_type {};

template <typename BasicJsonType, typename ConstructibleArrayType>
struct is_constructible_array_type_impl <
    BasicJsonType, ConstructibleArrayType,
    enable_if_t<not std::is_same<ConstructibleArrayType,
    typename BasicJsonType::value_type>::value and
    std::is_default_constructible<ConstructibleArrayType>::value and
(std::is_move_assignable<ConstructibleArrayType>::value or
 std::is_copy_assignable<ConstructibleArrayType>::value) and
is_detected<value_type_t, ConstructibleArrayType>::value and
is_detected<iterator_t, ConstructibleArrayType>::value and
is_complete_type<
detected_t<value_type_t, ConstructibleArrayType>>::value >>
{
    static constexpr bool value =
        // This is needed because json_reverse_iterator has a ::iterator type,
        // furthermore, std::back_insert_iterator (and other iterators) have a
        // base class `iterator`... Therefore it is detected as a
        // ConstructibleArrayType. The real fix would be to have an Iterable
        // concept.
        not is_iterator_traits<iterator_traits<ConstructibleArrayType>>::value and

        (std::is_same<typename ConstructibleArrayType::value_type,
         typename BasicJsonType::array_t::value_type>::value or
         has_from_json<BasicJsonType,
         typename ConstructibleArrayType::value_type>::value or
         has_non_default_from_json <
         BasicJsonType, typename ConstructibleArrayType::value_type >::value);
};

template <typename BasicJsonType, typename ConstructibleArrayType>
struct is_constructible_array_type
    : is_constructible_array_type_impl<BasicJsonType, ConstructibleArrayType> {};

template <typename RealIntegerType, typename CompatibleNumberIntegerType,
          typename = void>
struct is_compatible_integer_type_impl : std::false_type {};

template <typename RealIntegerType, typename CompatibleNumberIntegerType>
struct is_compatible_integer_type_impl <
    RealIntegerType, CompatibleNumberIntegerType,
    enable_if_t<std::is_integral<RealIntegerType>::value and
    std::is_integral<CompatibleNumberIntegerType>::value and
    not std::is_same<bool, CompatibleNumberIntegerType>::value >>
{
    // is there an assert somewhere on overflows?
    using RealLimits = std::numeric_limits<RealIntegerType>;
    using CompatibleLimits = std::numeric_limits<CompatibleNumberIntegerType>;

    static constexpr auto value =
        std::is_constructible<RealIntegerType,
        CompatibleNumberIntegerType>::value and
        CompatibleLimits::is_integer and
        RealLimits::is_signed == CompatibleLimits::is_signed;
};

template <typename RealIntegerType, typename CompatibleNumberIntegerType>
struct is_compatible_integer_type
    : is_compatible_integer_type_impl<RealIntegerType,
      CompatibleNumberIntegerType> {};

template <typename BasicJsonType, typename CompatibleType, typename = void>
struct is_compatible_type_impl: std::false_type {};

template <typename BasicJsonType, typename CompatibleType>
struct is_compatible_type_impl <
    BasicJsonType, CompatibleType,
    enable_if_t<is_complete_type<CompatibleType>::value >>
{
    static constexpr bool value =
        has_to_json<BasicJsonType, CompatibleType>::value;
};

template <typename BasicJsonType, typename CompatibleType>
struct is_compatible_type
    : is_compatible_type_impl<BasicJsonType, CompatibleType> {};

// https://en.cppreference.com/w/cpp/types/conjunction
template<class...> struct conjunction : std::true_type { };
template<class B1> struct conjunction<B1> : B1 { };
template<class B1, class... Bn>
struct conjunction<B1, Bn...>
: std::conditional<bool(B1::value), conjunction<Bn...>, B1>::type {};

template <typename T1, typename T2>
struct is_constructible_tuple : std::false_type {};

template <typename T1, typename... Args>
struct is_constructible_tuple<T1, std::tuple<Args...>> : conjunction<std::is_constructible<T1, Args>...> {};
}  // namespace detail
}  // namespace nlohmann

// #include <nlohmann/detail/value_t.hpp>


#include <array> // array
#include <ciso646> // and
#include <cstddef> // size_t
#include <cstdint> // uint8_t
#include <string> // string

namespace nlohmann
{
namespace detail
{
///////////////////////////
// JSON type enumeration //
///////////////////////////

/*!
@brief the JSON type enumeration

This enumeration collects the different JSON types. It is internally used to
distinguish the stored values, and the functions @ref basic_json::is_null(),
@ref basic_json::is_object(), @ref basic_json::is_array(),
@ref basic_json::is_string(), @ref basic_json::is_boolean(),
@ref basic_json::is_number() (with @ref basic_json::is_number_integer(),
@ref basic_json::is_number_unsigned(), and @ref basic_json::is_number_float()),
@ref basic_json::is_discarded(), @ref basic_json::is_primitive(), and
@ref basic_json::is_structured() rely on it.

@note There are three enumeration entries (number_integer, number_unsigned, and
number_float), because the library distinguishes these three types for numbers:
@ref basic_json::number_unsigned_t is used for unsigned integers,
@ref basic_json::number_integer_t is used for signed integers, and
@ref basic_json::number_float_t is used for floating-point numbers or to
approximate integers which do not fit in the limits of their respective type.

@sa @ref basic_json::basic_json(const value_t value_type) -- create a JSON
value with the default value for a given type

@since version 1.0.0
*/
enum class value_t : std::uint8_t
{
    null,             ///< null value
    object,           ///< object (unordered set of name/value pairs)
    array,            ///< array (ordered collection of values)
    string,           ///< string value
    boolean,          ///< boolean value
    number_integer,   ///< number value (signed integer)
    number_unsigned,  ///< number value (unsigned integer)
    number_float,     ///< number value (floating-point)
    discarded         ///< discarded by the the parser callback function
};

/*!
@brief comparison operator for JSON types

Returns an ordering that is similar to Python:
- order: null < boolean < number < object < array < string
- furthermore, each type is not smaller than itself
- discarded values are not comparable

@since version 1.0.0
*/
inline bool operator<(const value_t lhs, const value_t rhs) noexcept
{
    static constexpr std::array<std::uint8_t, 8> order = {{
            0 /* null */, 3 /* object */, 4 /* array */, 5 /* string */,
            1 /* boolean */, 2 /* integer */, 2 /* unsigned */, 2 /* float */
        }
    };

    const auto l_index = static_cast<std::size_t>(lhs);
    const auto r_index = static_cast<std::size_t>(rhs);
    return l_index < order.size() and r_index < order.size() and order[l_index] < order[r_index];
}
}  // namespace detail
}  // namespace nlohmann


namespace nlohmann
{
namespace detail
{
template<typename BasicJsonType>
void from_json(const BasicJsonType& j, typename std::nullptr_t& n)
{
    if (JSON_HEDLEY_UNLIKELY(not j.is_null()))
    {
        JSON_THROW(type_error::create(302, "type must be null, but is " + std::string(j.type_name())));
    }
    n = nullptr;
}

// overloads for basic_json template parameters
template<typename BasicJsonType, typename ArithmeticType,
         enable_if_t<std::is_arithmetic<ArithmeticType>::value and
                     not std::is_same<ArithmeticType, typename BasicJsonType::boolean_t>::value,
                     int> = 0>
void get_arithmetic_value(const BasicJsonType& j, ArithmeticType& val)
{
    switch (static_cast<value_t>(j))
    {
        case value_t::number_unsigned:
        {
            val = static_cast<ArithmeticType>(*j.template get_ptr<const typename BasicJsonType::number_unsigned_t*>());
            break;
        }
        case value_t::number_integer:
        {
            val = static_cast<ArithmeticType>(*j.template get_ptr<const typename BasicJsonType::number_integer_t*>());
            break;
        }
        case value_t::number_float:
        {
            val = static_cast<ArithmeticType>(*j.template get_ptr<const typename BasicJsonType::number_float_t*>());
            break;
        }

        default:
            JSON_THROW(type_error::create(302, "type must be number, but is " + std::string(j.type_name())));
    }
}

template<typename BasicJsonType>
void from_json(const BasicJsonType& j, typename BasicJsonType::boolean_t& b)
{
    if (JSON_HEDLEY_UNLIKELY(not j.is_boolean()))
    {
        JSON_THROW(type_error::create(302, "type must be boolean, but is " + std::string(j.type_name())));
    }
    b = *j.template get_ptr<const typename BasicJsonType::boolean_t*>();
}

template<typename BasicJsonType>
void from_json(const BasicJsonType& j, typename BasicJsonType::string_t& s)
{
    if (JSON_HEDLEY_UNLIKELY(not j.is_string()))
    {
        JSON_THROW(type_error::create(302, "type must be string, but is " + std::string(j.type_name())));
    }
    s = *j.template get_ptr<const typename BasicJsonType::string_t*>();
}

template <
    typename BasicJsonType, typename ConstructibleStringType,
    enable_if_t <
        is_constructible_string_type<BasicJsonType, ConstructibleStringType>::value and
        not std::is_same<typename BasicJsonType::string_t,
                         ConstructibleStringType>::value,
        int > = 0 >
void from_json(const BasicJsonType& j, ConstructibleStringType& s)
{
    if (JSON_HEDLEY_UNLIKELY(not j.is_string()))
    {
        JSON_THROW(type_error::create(302, "type must be string, but is " + std::string(j.type_name())));
    }

    s = *j.template get_ptr<const typename BasicJsonType::string_t*>();
}

template<typename BasicJsonType>
void from_json(const BasicJsonType& j, typename BasicJsonType::number_float_t& val)
{
    get_arithmetic_value(j, val);
}

template<typename BasicJsonType>
void from_json(const BasicJsonType& j, typename BasicJsonType::number_unsigned_t& val)
{
    get_arithmetic_value(j, val);
}

template<typename BasicJsonType>
void from_json(const BasicJsonType& j, typename BasicJsonType::number_integer_t& val)
{
    get_arithmetic_value(j, val);
}

template<typename BasicJsonType, typename EnumType,
         enable_if_t<std::is_enum<EnumType>::value, int> = 0>
void from_json(const BasicJsonType& j, EnumType& e)
{
    typename std::underlying_type<EnumType>::type val;
    get_arithmetic_value(j, val);
    e = static_cast<EnumType>(val);
}

// forward_list doesn't have an insert method
template<typename BasicJsonType, typename T, typename Allocator,
         enable_if_t<std::is_convertible<BasicJsonType, T>::value, int> = 0>
void from_json(const BasicJsonType& j, std::forward_list<T, Allocator>& l)
{
    if (JSON_HEDLEY_UNLIKELY(not j.is_array()))
    {
        JSON_THROW(type_error::create(302, "type must be array, but is " + std::string(j.type_name())));
    }
    l.clear();
    std::transform(j.rbegin(), j.rend(),
                   std::front_inserter(l), [](const BasicJsonType & i)
    {
        return i.template get<T>();
    });
}

// valarray doesn't have an insert method
template<typename BasicJsonType, typename T,
         enable_if_t<std::is_convertible<BasicJsonType, T>::value, int> = 0>
void from_json(const BasicJsonType& j, std::valarray<T>& l)
{
    if (JSON_HEDLEY_UNLIKELY(not j.is_array()))
    {
        JSON_THROW(type_error::create(302, "type must be array, but is " + std::string(j.type_name())));
    }
    l.resize(j.size());
    std::copy(j.begin(), j.end(), std::begin(l));
}

template <typename BasicJsonType, typename T, std::size_t N>
auto from_json(const BasicJsonType& j, T (&arr)[N])
-> decltype(j.template get<T>(), void())
{
    for (std::size_t i = 0; i < N; ++i)
    {
        arr[i] = j.at(i).template get<T>();
    }
}

template<typename BasicJsonType>
void from_json_array_impl(const BasicJsonType& j, typename BasicJsonType::array_t& arr, priority_tag<3> /*unused*/)
{
    arr = *j.template get_ptr<const typename BasicJsonType::array_t*>();
}

template <typename BasicJsonType, typename T, std::size_t N>
auto from_json_array_impl(const BasicJsonType& j, std::array<T, N>& arr,
                          priority_tag<2> /*unused*/)
-> decltype(j.template get<T>(), void())
{
    for (std::size_t i = 0; i < N; ++i)
    {
        arr[i] = j.at(i).template get<T>();
    }
}

template<typename BasicJsonType, typename ConstructibleArrayType>
auto from_json_array_impl(const BasicJsonType& j, ConstructibleArrayType& arr, priority_tag<1> /*unused*/)
-> decltype(
    arr.reserve(std::declval<typename ConstructibleArrayType::size_type>()),
    j.template get<typename ConstructibleArrayType::value_type>(),
    void())
{
    using std::end;

    ConstructibleArrayType ret;
    ret.reserve(j.size());
    std::transform(j.begin(), j.end(),
                   std::inserter(ret, end(ret)), [](const BasicJsonType & i)
    {
        // get<BasicJsonType>() returns *this, this won't call a from_json
        // method when value_type is BasicJsonType
        return i.template get<typename ConstructibleArrayType::value_type>();
    });
    arr = std::move(ret);
}

template <typename BasicJsonType, typename ConstructibleArrayType>
void from_json_array_impl(const BasicJsonType& j, ConstructibleArrayType& arr,
                          priority_tag<0> /*unused*/)
{
    using std::end;

    ConstructibleArrayType ret;
    std::transform(
        j.begin(), j.end(), std::inserter(ret, end(ret)),
        [](const BasicJsonType & i)
    {
        // get<BasicJsonType>() returns *this, this won't call a from_json
        // method when value_type is BasicJsonType
        return i.template get<typename ConstructibleArrayType::value_type>();
    });
    arr = std::move(ret);
}

template <typename BasicJsonType, typename ConstructibleArrayType,
          enable_if_t <
              is_constructible_array_type<BasicJsonType, ConstructibleArrayType>::value and
              not is_constructible_object_type<BasicJsonType, ConstructibleArrayType>::value and
              not is_constructible_string_type<BasicJsonType, ConstructibleArrayType>::value and
              not is_basic_json<ConstructibleArrayType>::value,
              int > = 0 >

auto from_json(const BasicJsonType& j, ConstructibleArrayType& arr)
-> decltype(from_json_array_impl(j, arr, priority_tag<3> {}),
j.template get<typename ConstructibleArrayType::value_type>(),
void())
{
    if (JSON_HEDLEY_UNLIKELY(not j.is_array()))
    {
        JSON_THROW(type_error::create(302, "type must be array, but is " +
                                      std::string(j.type_name())));
    }

    from_json_array_impl(j, arr, priority_tag<3> {});
}

template<typename BasicJsonType, typename ConstructibleObjectType,
         enable_if_t<is_constructible_object_type<BasicJsonType, ConstructibleObjectType>::value, int> = 0>
void from_json(const BasicJsonType& j, ConstructibleObjectType& obj)
{
    if (JSON_HEDLEY_UNLIKELY(not j.is_object()))
    {
        JSON_THROW(type_error::create(302, "type must be object, but is " + std::string(j.type_name())));
    }

    ConstructibleObjectType ret;
    auto inner_object = j.template get_ptr<const typename BasicJsonType::object_t*>();
    using value_type = typename ConstructibleObjectType::value_type;
    std::transform(
        inner_object->begin(), inner_object->end(),
        std::inserter(ret, ret.begin()),
        [](typename BasicJsonType::object_t::value_type const & p)
    {
        return value_type(p.first, p.second.template get<typename ConstructibleObjectType::mapped_type>());
    });
    obj = std::move(ret);
}

// overload for arithmetic types, not chosen for basic_json template arguments
// (BooleanType, etc..); note: Is it really necessary to provide explicit
// overloads for boolean_t etc. in case of a custom BooleanType which is not
// an arithmetic type?
template<typename BasicJsonType, typename ArithmeticType,
         enable_if_t <
             std::is_arithmetic<ArithmeticType>::value and
             not std::is_same<ArithmeticType, typename BasicJsonType::number_unsigned_t>::value and
             not std::is_same<ArithmeticType, typename BasicJsonType::number_integer_t>::value and
             not std::is_same<ArithmeticType, typename BasicJsonType::number_float_t>::value and
             not std::is_same<ArithmeticType, typename BasicJsonType::boolean_t>::value,
             int> = 0>
void from_json(const BasicJsonType& j, ArithmeticType& val)
{
    switch (static_cast<value_t>(j))
    {
        case value_t::number_unsigned:
        {
            val = static_cast<ArithmeticType>(*j.template get_ptr<const typename BasicJsonType::number_unsigned_t*>());
            break;
        }
        case value_t::number_integer:
        {
            val = static_cast<ArithmeticType>(*j.template get_ptr<const typename BasicJsonType::number_integer_t*>());
            break;
        }
        case value_t::number_float:
        {
            val = static_cast<ArithmeticType>(*j.template get_ptr<const typename BasicJsonType::number_float_t*>());
            break;
        }
        case value_t::boolean:
        {
            val = static_cast<ArithmeticType>(*j.template get_ptr<const typename BasicJsonType::boolean_t*>());
            break;
        }

        default:
            JSON_THROW(type_error::create(302, "type must be number, but is " + std::string(j.type_name())));
    }
}

template<typename BasicJsonType, typename A1, typename A2>
void from_json(const BasicJsonType& j, std::pair<A1, A2>& p)
{
    p = {j.at(0).template get<A1>(), j.at(1).template get<A2>()};
}

template<typename BasicJsonType, typename Tuple, std::size_t... Idx>
void from_json_tuple_impl(const BasicJsonType& j, Tuple& t, index_sequence<Idx...> /*unused*/)
{
    t = std::make_tuple(j.at(Idx).template get<typename std::tuple_element<Idx, Tuple>::type>()...);
}

template<typename BasicJsonType, typename... Args>
void from_json(const BasicJsonType& j, std::tuple<Args...>& t)
{
    from_json_tuple_impl(j, t, index_sequence_for<Args...> {});
}

template <typename BasicJsonType, typename Key, typename Value, typename Compare, typename Allocator,
          typename = enable_if_t<not std::is_constructible<
                                     typename BasicJsonType::string_t, Key>::value>>
void from_json(const BasicJsonType& j, std::map<Key, Value, Compare, Allocator>& m)
{
    if (JSON_HEDLEY_UNLIKELY(not j.is_array()))
    {
        JSON_THROW(type_error::create(302, "type must be array, but is " + std::string(j.type_name())));
    }
    m.clear();
    for (const auto& p : j)
    {
        if (JSON_HEDLEY_UNLIKELY(not p.is_array()))
        {
            JSON_THROW(type_error::create(302, "type must be array, but is " + std::string(p.type_name())));
        }
        m.emplace(p.at(0).template get<Key>(), p.at(1).template get<Value>());
    }
}

template <typename BasicJsonType, typename Key, typename Value, typename Hash, typename KeyEqual, typename Allocator,
          typename = enable_if_t<not std::is_constructible<
                                     typename BasicJsonType::string_t, Key>::value>>
void from_json(const BasicJsonType& j, std::unordered_map<Key, Value, Hash, KeyEqual, Allocator>& m)
{
    if (JSON_HEDLEY_UNLIKELY(not j.is_array()))
    {
        JSON_THROW(type_error::create(302, "type must be array, but is " + std::string(j.type_name())));
    }
    m.clear();
    for (const auto& p : j)
    {
        if (JSON_HEDLEY_UNLIKELY(not p.is_array()))
        {
            JSON_THROW(type_error::create(302, "type must be array, but is " + std::string(p.type_name())));
        }
        m.emplace(p.at(0).template get<Key>(), p.at(1).template get<Value>());
    }
}

struct from_json_fn
{
    template<typename BasicJsonType, typename T>
    auto operator()(const BasicJsonType& j, T& val) const
    noexcept(noexcept(from_json(j, val)))
    -> decltype(from_json(j, val), void())
    {
        return from_json(j, val);
    }
};
}  // namespace detail

/// namespace to hold default `from_json` function
/// to see why this is required:
/// http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2015/n4381.html
namespace
{
constexpr const auto& from_json = detail::static_const<detail::from_json_fn>::value;
} // namespace
} // namespace nlohmann

// #include <nlohmann/detail/conversions/to_json.hpp>


#include <algorithm> // copy
#include <ciso646> // or, and, not
#include <iterator> // begin, end
#include <string> // string
#include <tuple> // tuple, get
#include <type_traits> // is_same, is_constructible, is_floating_point, is_enum, underlying_type
#include <utility> // move, forward, declval, pair
#include <valarray> // valarray
#include <vector> // vector

// #include <nlohmann/detail/iterators/iteration_proxy.hpp>


#include <cstddef> // size_t
#include <iterator> // input_iterator_tag
#include <string> // string, to_string
#include <tuple> // tuple_size, get, tuple_element

// #include <nlohmann/detail/meta/type_traits.hpp>

// #include <nlohmann/detail/value_t.hpp>


namespace nlohmann
{
namespace detail
{
template<typename string_type>
void int_to_string( string_type& target, std::size_t value )
{
    target = std::to_string(value);
}
template <typename IteratorType> class iteration_proxy_value
{
  public:
    using difference_type = std::ptrdiff_t;
    using value_type = iteration_proxy_value;
    using pointer = value_type * ;
    using reference = value_type & ;
    using iterator_category = std::input_iterator_tag;
    using string_type = typename std::remove_cv< typename std::remove_reference<decltype( std::declval<IteratorType>().key() ) >::type >::type;

  private:
    /// the iterator
    IteratorType anchor;
    /// an index for arrays (used to create key names)
    std::size_t array_index = 0;
    /// last stringified array index
    mutable std::size_t array_index_last = 0;
    /// a string representation of the array index
    mutable string_type array_index_str = "0";
    /// an empty string (to return a reference for primitive values)
    const string_type empty_str = "";

  public:
    explicit iteration_proxy_value(IteratorType it) noexcept : anchor(it) {}

    /// dereference operator (needed for range-based for)
    iteration_proxy_value& operator*()
    {
        return *this;
    }

    /// increment operator (needed for range-based for)
    iteration_proxy_value& operator++()
    {
        ++anchor;
        ++array_index;

        return *this;
    }

    /// equality operator (needed for InputIterator)
    bool operator==(const iteration_proxy_value& o) const
    {
        return anchor == o.anchor;
    }

    /// inequality operator (needed for range-based for)
    bool operator!=(const iteration_proxy_value& o) const
    {
        return anchor != o.anchor;
    }

    /// return key of the iterator
    const string_type& key() const
    {
        assert(anchor.m_object != nullptr);

        switch (anchor.m_object->type())
        {
            // use integer array index as key
            case value_t::array:
            {
                if (array_index != array_index_last)
                {
                    int_to_string( array_index_str, array_index );
                    array_index_last = array_index;
                }
                return array_index_str;
            }

            // use key from the object
            case value_t::object:
                return anchor.key();

            // use an empty key for all primitive types
            default:
                return empty_str;
        }
    }

    /// return value of the iterator
    typename IteratorType::reference value() const
    {
        return anchor.value();
    }
};

/// proxy class for the items() function
template<typename IteratorType> class iteration_proxy
{
  private:
    /// the container to iterate
    typename IteratorType::reference container;

  public:
    /// construct iteration proxy from a container
    explicit iteration_proxy(typename IteratorType::reference cont) noexcept
        : container(cont) {}

    /// return iterator begin (needed for range-based for)
    iteration_proxy_value<IteratorType> begin() noexcept
    {
        return iteration_proxy_value<IteratorType>(container.begin());
    }

    /// return iterator end (needed for range-based for)
    iteration_proxy_value<IteratorType> end() noexcept
    {
        return iteration_proxy_value<IteratorType>(container.end());
    }
};
// Structured Bindings Support
// For further reference see https://blog.tartanllama.xyz/structured-bindings/
// And see https://github.com/nlohmann/json/pull/1391
template <std::size_t N, typename IteratorType, enable_if_t<N == 0, int> = 0>
auto get(const nlohmann::detail::iteration_proxy_value<IteratorType>& i) -> decltype(i.key())
{
    return i.key();
}
// Structured Bindings Support
// For further reference see https://blog.tartanllama.xyz/structured-bindings/
// And see https://github.com/nlohmann/json/pull/1391
template <std::size_t N, typename IteratorType, enable_if_t<N == 1, int> = 0>
auto get(const nlohmann::detail::iteration_proxy_value<IteratorType>& i) -> decltype(i.value())
{
    return i.value();
}
}  // namespace detail
}  // namespace nlohmann

// The Addition to the STD Namespace is required to add
// Structured Bindings Support to the iteration_proxy_value class
// For further reference see https://blog.tartanllama.xyz/structured-bindings/
// And see https://github.com/nlohmann/json/pull/1391
namespace std
{
#if defined(__clang__)
    // Fix: https://github.com/nlohmann/json/issues/1401
    #pragma clang diagnostic push
    #pragma clang diagnostic ignored "-Wmismatched-tags"
#endif
template <typename IteratorType>
class tuple_size<::nlohmann::detail::iteration_proxy_value<IteratorType>>
            : public std::integral_constant<std::size_t, 2> {};

template <std::size_t N, typename IteratorType>
class tuple_element<N, ::nlohmann::detail::iteration_proxy_value<IteratorType >>
{
  public:
    using type = decltype(
                     get<N>(std::declval <
                            ::nlohmann::detail::iteration_proxy_value<IteratorType >> ()));
};
#if defined(__clang__)
    #pragma clang diagnostic pop
#endif
} // namespace std

// #include <nlohmann/detail/meta/cpp_future.hpp>

// #include <nlohmann/detail/meta/type_traits.hpp>

// #include <nlohmann/detail/value_t.hpp>


namespace nlohmann
{
namespace detail
{
//////////////////
// constructors //
//////////////////

template<value_t> struct external_constructor;

template<>
struct external_constructor<value_t::boolean>
{
    template<typename BasicJsonType>
    static void construct(BasicJsonType& j, typename BasicJsonType::boolean_t b) noexcept
    {
        j.m_type = value_t::boolean;
        j.m_value = b;
        j.assert_invariant();
    }
};

template<>
struct external_constructor<value_t::string>
{
    template<typename BasicJsonType>
    static void construct(BasicJsonType& j, const typename BasicJsonType::string_t& s)
    {
        j.m_type = value_t::string;
        j.m_value = s;
        j.assert_invariant();
    }

    template<typename BasicJsonType>
    static void construct(BasicJsonType& j, typename BasicJsonType::string_t&& s)
    {
        j.m_type = value_t::string;
        j.m_value = std::move(s);
        j.assert_invariant();
    }

    template<typename BasicJsonType, typename CompatibleStringType,
             enable_if_t<not std::is_same<CompatibleStringType, typename BasicJsonType::string_t>::value,
                         int> = 0>
    static void construct(BasicJsonType& j, const CompatibleStringType& str)
    {
        j.m_type = value_t::string;
        j.m_value.string = j.template create<typename BasicJsonType::string_t>(str);
        j.assert_invariant();
    }
};

template<>
struct external_constructor<value_t::number_float>
{
    template<typename BasicJsonType>
    static void construct(BasicJsonType& j, typename BasicJsonType::number_float_t val) noexcept
    {
        j.m_type = value_t::number_float;
        j.m_value = val;
        j.assert_invariant();
    }
};

template<>
struct external_constructor<value_t::number_unsigned>
{
    template<typename BasicJsonType>
    static void construct(BasicJsonType& j, typename BasicJsonType::number_unsigned_t val) noexcept
    {
        j.m_type = value_t::number_unsigned;
        j.m_value = val;
        j.assert_invariant();
    }
};

template<>
struct external_constructor<value_t::number_integer>
{
    template<typename BasicJsonType>
    static void construct(BasicJsonType& j, typename BasicJsonType::number_integer_t val) noexcept
    {
        j.m_type = value_t::number_integer;
        j.m_value = val;
        j.assert_invariant();
    }
};

template<>
struct external_constructor<value_t::array>
{
    template<typename BasicJsonType>
    static void construct(BasicJsonType& j, const typename BasicJsonType::array_t& arr)
    {
        j.m_type = value_t::array;
        j.m_value = arr;
        j.assert_invariant();
    }

    template<typename BasicJsonType>
    static void construct(BasicJsonType& j, typename BasicJsonType::array_t&& arr)
    {
        j.m_type = value_t::array;
        j.m_value = std::move(arr);
        j.assert_invariant();
    }

    template<typename BasicJsonType, typename CompatibleArrayType,
             enable_if_t<not std::is_same<CompatibleArrayType, typename BasicJsonType::array_t>::value,
                         int> = 0>
    static void construct(BasicJsonType& j, const CompatibleArrayType& arr)
    {
        using std::begin;
        using std::end;
        j.m_type = value_t::array;
        j.m_value.array = j.template create<typename BasicJsonType::array_t>(begin(arr), end(arr));
        j.assert_invariant();
    }

    template<typename BasicJsonType>
    static void construct(BasicJsonType& j, const std::vector<bool>& arr)
    {
        j.m_type = value_t::array;
        j.m_value = value_t::array;
        j.m_value.array->reserve(arr.size());
        for (const bool x : arr)
        {
            j.m_value.array->push_back(x);
        }
        j.assert_invariant();
    }

    template<typename BasicJsonType, typename T,
             enable_if_t<std::is_convertible<T, BasicJsonType>::value, int> = 0>
    static void construct(BasicJsonType& j, const std::valarray<T>& arr)
    {
        j.m_type = value_t::array;
        j.m_value = value_t::array;
        j.m_value.array->resize(arr.size());
        if (arr.size() > 0)
        {
            std::copy(std::begin(arr), std::end(arr), j.m_value.array->begin());
        }
        j.assert_invariant();
    }
};

template<>
struct external_constructor<value_t::object>
{
    template<typename BasicJsonType>
    static void construct(BasicJsonType& j, const typename BasicJsonType::object_t& obj)
    {
        j.m_type = value_t::object;
        j.m_value = obj;
        j.assert_invariant();
    }

    template<typename BasicJsonType>
    static void construct(BasicJsonType& j, typename BasicJsonType::object_t&& obj)
    {
        j.m_type = value_t::object;
        j.m_value = std::move(obj);
        j.assert_invariant();
    }

    template<typename BasicJsonType, typename CompatibleObjectType,
             enable_if_t<not std::is_same<CompatibleObjectType, typename BasicJsonType::object_t>::value, int> = 0>
    static void construct(BasicJsonType& j, const CompatibleObjectType& obj)
    {
        using std::begin;
        using std::end;

        j.m_type = value_t::object;
        j.m_value.object = j.template create<typename BasicJsonType::object_t>(begin(obj), end(obj));
        j.assert_invariant();
    }
};

/////////////
// to_json //
/////////////

template<typename BasicJsonType, typename T,
         enable_if_t<std::is_same<T, typename BasicJsonType::boolean_t>::value, int> = 0>
void to_json(BasicJsonType& j, T b) noexcept
{
    external_constructor<value_t::boolean>::construct(j, b);
}

template<typename BasicJsonType, typename CompatibleString,
         enable_if_t<std::is_constructible<typename BasicJsonType::string_t, CompatibleString>::value, int> = 0>
void to_json(BasicJsonType& j, const CompatibleString& s)
{
    external_constructor<value_t::string>::construct(j, s);
}

template<typename BasicJsonType>
void to_json(BasicJsonType& j, typename BasicJsonType::string_t&& s)
{
    external_constructor<value_t::string>::construct(j, std::move(s));
}

template<typename BasicJsonType, typename FloatType,
         enable_if_t<std::is_floating_point<FloatType>::value, int> = 0>
void to_json(BasicJsonType& j, FloatType val) noexcept
{
    external_constructor<value_t::number_float>::construct(j, static_cast<typename BasicJsonType::number_float_t>(val));
}

template<typename BasicJsonType, typename CompatibleNumberUnsignedType,
         enable_if_t<is_compatible_integer_type<typename BasicJsonType::number_unsigned_t, CompatibleNumberUnsignedType>::value, int> = 0>
void to_json(BasicJsonType& j, CompatibleNumberUnsignedType val) noexcept
{
    external_constructor<value_t::number_unsigned>::construct(j, static_cast<typename BasicJsonType::number_unsigned_t>(val));
}

template<typename BasicJsonType, typename CompatibleNumberIntegerType,
         enable_if_t<is_compatible_integer_type<typename BasicJsonType::number_integer_t, CompatibleNumberIntegerType>::value, int> = 0>
void to_json(BasicJsonType& j, CompatibleNumberIntegerType val) noexcept
{
    external_constructor<value_t::number_integer>::construct(j, static_cast<typename BasicJsonType::number_integer_t>(val));
}

template<typename BasicJsonType, typename EnumType,
         enable_if_t<std::is_enum<EnumType>::value, int> = 0>
void to_json(BasicJsonType& j, EnumType e) noexcept
{
    using underlying_type = typename std::underlying_type<EnumType>::type;
    external_constructor<value_t::number_integer>::construct(j, static_cast<underlying_type>(e));
}

template<typename BasicJsonType>
void to_json(BasicJsonType& j, const std::vector<bool>& e)
{
    external_constructor<value_t::array>::construct(j, e);
}

template <typename BasicJsonType, typename CompatibleArrayType,
          enable_if_t<is_compatible_array_type<BasicJsonType,
                      CompatibleArrayType>::value and
                      not is_compatible_object_type<
                          BasicJsonType, CompatibleArrayType>::value and
                      not is_compatible_string_type<BasicJsonType, CompatibleArrayType>::value and
                      not is_basic_json<CompatibleArrayType>::value,
                      int> = 0>
void to_json(BasicJsonType& j, const CompatibleArrayType& arr)
{
    external_constructor<value_t::array>::construct(j, arr);
}

template<typename BasicJsonType, typename T,
         enable_if_t<std::is_convertible<T, BasicJsonType>::value, int> = 0>
void to_json(BasicJsonType& j, const std::valarray<T>& arr)
{
    external_constructor<value_t::array>::construct(j, std::move(arr));
}

template<typename BasicJsonType>
void to_json(BasicJsonType& j, typename BasicJsonType::array_t&& arr)
{
    external_constructor<value_t::array>::construct(j, std::move(arr));
}

template<typename BasicJsonType, typename CompatibleObjectType,
         enable_if_t<is_compatible_object_type<BasicJsonType, CompatibleObjectType>::value and not is_basic_json<CompatibleObjectType>::value, int> = 0>
void to_json(BasicJsonType& j, const CompatibleObjectType& obj)
{
    external_constructor<value_t::object>::construct(j, obj);
}

template<typename BasicJsonType>
void to_json(BasicJsonType& j, typename BasicJsonType::object_t&& obj)
{
    external_constructor<value_t::object>::construct(j, std::move(obj));
}

template <
    typename BasicJsonType, typename T, std::size_t N,
    enable_if_t<not std::is_constructible<typename BasicJsonType::string_t,
                const T(&)[N]>::value,
                int> = 0 >
void to_json(BasicJsonType& j, const T(&arr)[N])
{
    external_constructor<value_t::array>::construct(j, arr);
}

template < typename BasicJsonType, typename T1, typename T2, enable_if_t < std::is_constructible<BasicJsonType, T1>::value&& std::is_constructible<BasicJsonType, T2>::value, int > = 0 >
void to_json(BasicJsonType& j, const std::pair<T1, T2>& p)
{
    j = { p.first, p.second };
}

// for https://github.com/nlohmann/json/pull/1134
template < typename BasicJsonType, typename T,
           enable_if_t<std::is_same<T, iteration_proxy_value<typename BasicJsonType::iterator>>::value, int> = 0>
void to_json(BasicJsonType& j, const T& b)
{
    j = { {b.key(), b.value()} };
}

template<typename BasicJsonType, typename Tuple, std::size_t... Idx>
void to_json_tuple_impl(BasicJsonType& j, const Tuple& t, index_sequence<Idx...> /*unused*/)
{
    j = { std::get<Idx>(t)... };
}

template<typename BasicJsonType, typename T, enable_if_t<is_constructible_tuple<BasicJsonType, T>::value, int > = 0>
void to_json(BasicJsonType& j, const T& t)
{
    to_json_tuple_impl(j, t, make_index_sequence<std::tuple_size<T>::value> {});
}

struct to_json_fn
{
    template<typename BasicJsonType, typename T>
    auto operator()(BasicJsonType& j, T&& val) const noexcept(noexcept(to_json(j, std::forward<T>(val))))
    -> decltype(to_json(j, std::forward<T>(val)), void())
    {
        return to_json(j, std::forward<T>(val));
    }
};
}  // namespace detail

/// namespace to hold default `to_json` function
namespace
{
constexpr const auto& to_json = detail::static_const<detail::to_json_fn>::value;
} // namespace
} // namespace nlohmann


namespace nlohmann
{

template<typename, typename>
struct adl_serializer
{
    /*!
    @brief convert a JSON value to any value type

    This function is usually called by the `get()` function of the
    @ref basic_json class (either explicit or via conversion operators).

    @param[in] j        JSON value to read from
    @param[in,out] val  value to write to
    */
    template<typename BasicJsonType, typename ValueType>
    static auto from_json(BasicJsonType&& j, ValueType& val) noexcept(
        noexcept(::nlohmann::from_json(std::forward<BasicJsonType>(j), val)))
    -> decltype(::nlohmann::from_json(std::forward<BasicJsonType>(j), val), void())
    {
        ::nlohmann::from_json(std::forward<BasicJsonType>(j), val);
    }

    /*!
    @brief convert any value type to a JSON value

    This function is usually called by the constructors of the @ref basic_json
    class.

    @param[in,out] j  JSON value to write to
    @param[in] val    value to read from
    */
    template <typename BasicJsonType, typename ValueType>
    static auto to_json(BasicJsonType& j, ValueType&& val) noexcept(
        noexcept(::nlohmann::to_json(j, std::forward<ValueType>(val))))
    -> decltype(::nlohmann::to_json(j, std::forward<ValueType>(val)), void())
    {
        ::nlohmann::to_json(j, std::forward<ValueType>(val));
    }
};

}  // namespace nlohmann

// #include <nlohmann/detail/conversions/from_json.hpp>

// #include <nlohmann/detail/conversions/to_json.hpp>

// #include <nlohmann/detail/exceptions.hpp>

// #include <nlohmann/detail/input/binary_reader.hpp>


#include <algorithm> // generate_n
#include <array> // array
#include <cassert> // assert
#include <cmath> // ldexp
#include <cstddef> // size_t
#include <cstdint> // uint8_t, uint16_t, uint32_t, uint64_t
#include <cstdio> // snprintf
#include <cstring> // memcpy
#include <iterator> // back_inserter
#include <limits> // numeric_limits
#include <string> // char_traits, string
#include <utility> // make_pair, move

// #include <nlohmann/detail/exceptions.hpp>

// #include <nlohmann/detail/input/input_adapters.hpp>


#include <array> // array
#include <cassert> // assert
#include <cstddef> // size_t
#include <cstdio> //FILE *
#include <cstring> // strlen
#include <istream> // istream
#include <iterator> // begin, end, iterator_traits, random_access_iterator_tag, distance, next
#include <memory> // shared_ptr, make_shared, addressof
#include <numeric> // accumulate
#include <string> // string, char_traits
#include <type_traits> // enable_if, is_base_of, is_pointer, is_integral, remove_pointer
#include <utility> // pair, declval

// #include <nlohmann/detail/iterators/iterator_traits.hpp>

// #include <nlohmann/detail/macro_scope.hpp>


namespace nlohmann
{
namespace detail
{
/// the supported input formats
enum class input_format_t { json, cbor, msgpack, ubjson, bson };

////////////////////
// input adapters //
////////////////////

/*!
@brief abstract input adapter interface

Produces a stream of std::char_traits<char>::int_type characters from a
std::istream, a buffer, or some other input type. Accepts the return of
exactly one non-EOF character for future input. The int_type characters
returned consist of all valid char values as positive values (typically
unsigned char), plus an EOF value outside that range, specified by the value
of the function std::char_traits<char>::eof(). This value is typically -1, but
could be any arbitrary value which is not a valid char value.
*/
struct input_adapter_protocol
{
    /// get a character [0,255] or std::char_traits<char>::eof().
    virtual std::char_traits<char>::int_type get_character() = 0;
    virtual ~input_adapter_protocol() = default;
};

/// a type to simplify interfaces
using input_adapter_t = std::shared_ptr<input_adapter_protocol>;

/*!
Input adapter for stdio file access. This adapter read only 1 byte and do not use any
 buffer. This adapter is a very low level adapter.
*/
class file_input_adapter : public input_adapter_protocol
{
  public:
    JSON_HEDLEY_NON_NULL(2)
    explicit file_input_adapter(std::FILE* f)  noexcept
        : m_file(f)
    {}

    // make class move-only
    file_input_adapter(const file_input_adapter&) = delete;
    file_input_adapter(file_input_adapter&&) = default;
    file_input_adapter& operator=(const file_input_adapter&) = delete;
    file_input_adapter& operator=(file_input_adapter&&) = default;
    ~file_input_adapter() override = default;

    std::char_traits<char>::int_type get_character() noexcept override
    {
        return std::fgetc(m_file);
    }

  private:
    /// the file pointer to read from
    std::FILE* m_file;
};


/*!
Input adapter for a (caching) istream. Ignores a UFT Byte Order Mark at
beginning of input. Does not support changing the underlying std::streambuf
in mid-input. Maintains underlying std::istream and std::streambuf to support
subsequent use of standard std::istream operations to process any input
characters following those used in parsing the JSON input.  Clears the
std::istream flags; any input errors (e.g., EOF) will be detected by the first
subsequent call for input from the std::istream.
*/
class input_stream_adapter : public input_adapter_protocol
{
  public:
    ~input_stream_adapter() override
    {
        // clear stream flags; we use underlying streambuf I/O, do not
        // maintain ifstream flags, except eof
        is.clear(is.rdstate() & std::ios::eofbit);
    }

    explicit input_stream_adapter(std::istream& i)
        : is(i), sb(*i.rdbuf())
    {}

    // delete because of pointer members
    input_stream_adapter(const input_stream_adapter&) = delete;
    input_stream_adapter& operator=(input_stream_adapter&) = delete;
    input_stream_adapter(input_stream_adapter&&) = delete;
    input_stream_adapter& operator=(input_stream_adapter&&) = delete;

    // std::istream/std::streambuf use std::char_traits<char>::to_int_type, to
    // ensure that std::char_traits<char>::eof() and the character 0xFF do not
    // end up as the same value, eg. 0xFFFFFFFF.
    std::char_traits<char>::int_type get_character() override
    {
        auto res = sb.sbumpc();
        // set eof manually, as we don't use the istream interface.
        if (res == EOF)
        {
            is.clear(is.rdstate() | std::ios::eofbit);
        }
        return res;
    }

  private:
    /// the associated input stream
    std::istream& is;
    std::streambuf& sb;
};

/// input adapter for buffer input
class input_buffer_adapter : public input_adapter_protocol
{
  public:
    input_buffer_adapter(const char* b, const std::size_t l) noexcept
        : cursor(b), limit(b == nullptr ? nullptr : (b + l))
    {}

    // delete because of pointer members
    input_buffer_adapter(const input_buffer_adapter&) = delete;
    input_buffer_adapter& operator=(input_buffer_adapter&) = delete;
    input_buffer_adapter(input_buffer_adapter&&) = delete;
    input_buffer_adapter& operator=(input_buffer_adapter&&) = delete;
    ~input_buffer_adapter() override = default;

    std::char_traits<char>::int_type get_character() noexcept override
    {
        if (JSON_HEDLEY_LIKELY(cursor < limit))
        {
            assert(cursor != nullptr and limit != nullptr);
            return std::char_traits<char>::to_int_type(*(cursor++));
        }

        return std::char_traits<char>::eof();
    }

  private:
    /// pointer to the current character
    const char* cursor;
    /// pointer past the last character
    const char* const limit;
};

template<typename WideStringType, size_t T>
struct wide_string_input_helper
{
    // UTF-32
    static void fill_buffer(const WideStringType& str,
                            size_t& current_wchar,
                            std::array<std::char_traits<char>::int_type, 4>& utf8_bytes,
                            size_t& utf8_bytes_index,
                            size_t& utf8_bytes_filled)
    {
        utf8_bytes_index = 0;

        if (current_wchar == str.size())
        {
            utf8_bytes[0] = std::char_traits<char>::eof();
            utf8_bytes_filled = 1;
        }
        else
        {
            // get the current character
            const auto wc = static_cast<unsigned int>(str[current_wchar++]);

            // UTF-32 to UTF-8 encoding
            if (wc < 0x80)
            {
                utf8_bytes[0] = static_cast<std::char_traits<char>::int_type>(wc);
                utf8_bytes_filled = 1;
            }
            else if (wc <= 0x7FF)
            {
                utf8_bytes[0] = static_cast<std::char_traits<char>::int_type>(0xC0u | ((wc >> 6u) & 0x1Fu));
                utf8_bytes[1] = static_cast<std::char_traits<char>::int_type>(0x80u | (wc & 0x3Fu));
                utf8_bytes_filled = 2;
            }
            else if (wc <= 0xFFFF)
            {
                utf8_bytes[0] = static_cast<std::char_traits<char>::int_type>(0xE0u | ((wc >> 12u) & 0x0Fu));
                utf8_bytes[1] = static_cast<std::char_traits<char>::int_type>(0x80u | ((wc >> 6u) & 0x3Fu));
                utf8_bytes[2] = static_cast<std::char_traits<char>::int_type>(0x80u | (wc & 0x3Fu));
                utf8_bytes_filled = 3;
            }
            else if (wc <= 0x10FFFF)
            {
                utf8_bytes[0] = static_cast<std::char_traits<char>::int_type>(0xF0u | ((wc >> 18u) & 0x07u));
                utf8_bytes[1] = static_cast<std::char_traits<char>::int_type>(0x80u | ((wc >> 12u) & 0x3Fu));
                utf8_bytes[2] = static_cast<std::char_traits<char>::int_type>(0x80u | ((wc >> 6u) & 0x3Fu));
                utf8_bytes[3] = static_cast<std::char_traits<char>::int_type>(0x80u | (wc & 0x3Fu));
                utf8_bytes_filled = 4;
            }
            else
            {
                // unknown character
                utf8_bytes[0] = static_cast<std::char_traits<char>::int_type>(wc);
                utf8_bytes_filled = 1;
            }
        }
    }
};

template<typename WideStringType>
struct wide_string_input_helper<WideStringType, 2>
{
    // UTF-16
    static void fill_buffer(const WideStringType& str,
                            size_t& current_wchar,
                            std::array<std::char_traits<char>::int_type, 4>& utf8_bytes,
                            size_t& utf8_bytes_index,
                            size_t& utf8_bytes_filled)
    {
        utf8_bytes_index = 0;

        if (current_wchar == str.size())
        {
            utf8_bytes[0] = std::char_traits<char>::eof();
            utf8_bytes_filled = 1;
        }
        else
        {
            // get the current character
            const auto wc = static_cast<unsigned int>(str[current_wchar++]);

            // UTF-16 to UTF-8 encoding
            if (wc < 0x80)
            {
                utf8_bytes[0] = static_cast<std::char_traits<char>::int_type>(wc);
                utf8_bytes_filled = 1;
            }
            else if (wc <= 0x7FF)
            {
                utf8_bytes[0] = static_cast<std::char_traits<char>::int_type>(0xC0u | ((wc >> 6u)));
                utf8_bytes[1] = static_cast<std::char_traits<char>::int_type>(0x80u | (wc & 0x3Fu));
                utf8_bytes_filled = 2;
            }
            else if (0xD800 > wc or wc >= 0xE000)
            {
                utf8_bytes[0] = static_cast<std::char_traits<char>::int_type>(0xE0u | ((wc >> 12u)));
                utf8_bytes[1] = static_cast<std::char_traits<char>::int_type>(0x80u | ((wc >> 6u) & 0x3Fu));
                utf8_bytes[2] = static_cast<std::char_traits<char>::int_type>(0x80u | (wc & 0x3Fu));
                utf8_bytes_filled = 3;
            }
            else
            {
                if (current_wchar < str.size())
                {
                    const auto wc2 = static_cast<unsigned int>(str[current_wchar++]);
                    const auto charcode = 0x10000u + (((wc & 0x3FFu) << 10u) | (wc2 & 0x3FFu));
                    utf8_bytes[0] = static_cast<std::char_traits<char>::int_type>(0xF0u | (charcode >> 18u));
                    utf8_bytes[1] = static_cast<std::char_traits<char>::int_type>(0x80u | ((charcode >> 12u) & 0x3Fu));
                    utf8_bytes[2] = static_cast<std::char_traits<char>::int_type>(0x80u | ((charcode >> 6u) & 0x3Fu));
                    utf8_bytes[3] = static_cast<std::char_traits<char>::int_type>(0x80u | (charcode & 0x3Fu));
                    utf8_bytes_filled = 4;
                }
                else
                {
                    // unknown character
                    ++current_wchar;
                    utf8_bytes[0] = static_cast<std::char_traits<char>::int_type>(wc);
                    utf8_bytes_filled = 1;
                }
            }
        }
    }
};

template<typename WideStringType>
class wide_string_input_adapter : public input_adapter_protocol
{
  public:
    explicit wide_string_input_adapter(const WideStringType& w) noexcept
        : str(w)
    {}

    std::char_traits<char>::int_type get_character() noexcept override
    {
        // check if buffer needs to be filled
        if (utf8_bytes_index == utf8_bytes_filled)
        {
            fill_buffer<sizeof(typename WideStringType::value_type)>();

            assert(utf8_bytes_filled > 0);
            assert(utf8_bytes_index == 0);
        }

        // use buffer
        assert(utf8_bytes_filled > 0);
        assert(utf8_bytes_index < utf8_bytes_filled);
        return utf8_bytes[utf8_bytes_index++];
    }

  private:
    template<size_t T>
    void fill_buffer()
    {
        wide_string_input_helper<WideStringType, T>::fill_buffer(str, current_wchar, utf8_bytes, utf8_bytes_index, utf8_bytes_filled);
    }

    /// the wstring to process
    const WideStringType& str;

    /// index of the current wchar in str
    std::size_t current_wchar = 0;

    /// a buffer for UTF-8 bytes
    std::array<std::char_traits<char>::int_type, 4> utf8_bytes = {{0, 0, 0, 0}};

    /// index to the utf8_codes array for the next valid byte
    std::size_t utf8_bytes_index = 0;
    /// number of valid bytes in the utf8_codes array
    std::size_t utf8_bytes_filled = 0;
};

class input_adapter
{
  public:
    // native support
    JSON_HEDLEY_NON_NULL(2)
    input_adapter(std::FILE* file)
        : ia(std::make_shared<file_input_adapter>(file)) {}
    /// input adapter for input stream
    input_adapter(std::istream& i)
        : ia(std::make_shared<input_stream_adapter>(i)) {}

    /// input adapter for input stream
    input_adapter(std::istream&& i)
        : ia(std::make_shared<input_stream_adapter>(i)) {}

    input_adapter(const std::wstring& ws)
        : ia(std::make_shared<wide_string_input_adapter<std::wstring>>(ws)) {}

    input_adapter(const std::u16string& ws)
        : ia(std::make_shared<wide_string_input_adapter<std::u16string>>(ws)) {}

    input_adapter(const std::u32string& ws)
        : ia(std::make_shared<wide_string_input_adapter<std::u32string>>(ws)) {}

    /// input adapter for buffer
    template<typename CharT,
             typename std::enable_if<
                 std::is_pointer<CharT>::value and
                 std::is_integral<typename std::remove_pointer<CharT>::type>::value and
                 sizeof(typename std::remove_pointer<CharT>::type) == 1,
                 int>::type = 0>
    input_adapter(CharT b, std::size_t l)
        : ia(std::make_shared<input_buffer_adapter>(reinterpret_cast<const char*>(b), l)) {}

    // derived support

    /// input adapter for string literal
    template<typename CharT,
             typename std::enable_if<
                 std::is_pointer<CharT>::value and
                 std::is_integral<typename std::remove_pointer<CharT>::type>::value and
                 sizeof(typename std::remove_pointer<CharT>::type) == 1,
                 int>::type = 0>
    input_adapter(CharT b)
        : input_adapter(reinterpret_cast<const char*>(b),
                        std::strlen(reinterpret_cast<const char*>(b))) {}

    /// input adapter for iterator range with contiguous storage
    template<class IteratorType,
             typename std::enable_if<
                 std::is_same<typename iterator_traits<IteratorType>::iterator_category, std::random_access_iterator_tag>::value,
                 int>::type = 0>
    input_adapter(IteratorType first, IteratorType last)
    {
#ifndef NDEBUG
        // assertion to check that the iterator range is indeed contiguous,
        // see http://stackoverflow.com/a/35008842/266378 for more discussion
        const auto is_contiguous = std::accumulate(
                                       first, last, std::pair<bool, int>(true, 0),
                                       [&first](std::pair<bool, int> res, decltype(*first) val)
        {
            res.first &= (val == *(std::next(std::addressof(*first), res.second++)));
            return res;
        }).first;
        assert(is_contiguous);
#endif

        // assertion to check that each element is 1 byte long
        static_assert(
            sizeof(typename iterator_traits<IteratorType>::value_type) == 1,
            "each element in the iterator range must have the size of 1 byte");

        const auto len = static_cast<size_t>(std::distance(first, last));
        if (JSON_HEDLEY_LIKELY(len > 0))
        {
            // there is at least one element: use the address of first
            ia = std::make_shared<input_buffer_adapter>(reinterpret_cast<const char*>(&(*first)), len);
        }
        else
        {
            // the address of first cannot be used: use nullptr
            ia = std::make_shared<input_buffer_adapter>(nullptr, len);
        }
    }

    /// input adapter for array
    template<class T, std::size_t N>
    input_adapter(T (&array)[N])
        : input_adapter(std::begin(array), std::end(array)) {}

    /// input adapter for contiguous container
    template<class ContiguousContainer, typename
             std::enable_if<not std::is_pointer<ContiguousContainer>::value and
                            std::is_base_of<std::random_access_iterator_tag, typename iterator_traits<decltype(std::begin(std::declval<ContiguousContainer const>()))>::iterator_category>::value,
                            int>::type = 0>
    input_adapter(const ContiguousContainer& c)
        : input_adapter(std::begin(c), std::end(c)) {}

    operator input_adapter_t()
    {
        return ia;
    }

  private:
    /// the actual adapter
    input_adapter_t ia = nullptr;
};
}  // namespace detail
}  // namespace nlohmann

// #include <nlohmann/detail/input/json_sax.hpp>


#include <cassert> // assert
#include <cstddef>
#include <string> // string
#include <utility> // move
#include <vector> // vector

// #include <nlohmann/detail/exceptions.hpp>

// #include <nlohmann/detail/macro_scope.hpp>


namespace nlohmann
{

/*!
@brief SAX interface

This class describes the SAX interface used by @ref nlohmann::json::sax_parse.
Each function is called in different situations while the input is parsed. The
boolean return value informs the parser whether to continue processing the
input.
*/
template<typename BasicJsonType>
struct json_sax
{
    /// type for (signed) integers
    using number_integer_t = typename BasicJsonType::number_integer_t;
    /// type for unsigned integers
    using number_unsigned_t = typename BasicJsonType::number_unsigned_t;
    /// type for floating-point numbers
    using number_float_t = typename BasicJsonType::number_float_t;
    /// type for strings
    using string_t = typename BasicJsonType::string_t;

    /*!
    @brief a null value was read
    @return whether parsing should proceed
    */
    virtual bool null() = 0;

    /*!
    @brief a boolean value was read
    @param[in] val  boolean value
    @return whether parsing should proceed
    */
    virtual bool boolean(bool val) = 0;

    /*!
    @brief an integer number was read
    @param[in] val  integer value
    @return whether parsing should proceed
    */
    virtual bool number_integer(number_integer_t val) = 0;

    /*!
    @brief an unsigned integer number was read
    @param[in] val  unsigned integer value
    @return whether parsing should proceed
    */
    virtual bool number_unsigned(number_unsigned_t val) = 0;

    /*!
    @brief an floating-point number was read
    @param[in] val  floating-point value
    @param[in] s    raw token value
    @return whether parsing should proceed
    */
    virtual bool number_float(number_float_t val, const string_t& s) = 0;

    /*!
    @brief a string was read
    @param[in] val  string value
    @return whether parsing should proceed
    @note It is safe to move the passed string.
    */
    virtual bool string(string_t& val) = 0;

    /*!
    @brief the beginning of an object was read
    @param[in] elements  number of object elements or -1 if unknown
    @return whether parsing should proceed
    @note binary formats may report the number of elements
    */
    virtual bool start_object(std::size_t elements) = 0;

    /*!
    @brief an object key was read
    @param[in] val  object key
    @return whether parsing should proceed
    @note It is safe to move the passed string.
    */
    virtual bool key(string_t& val) = 0;

    /*!
    @brief the end of an object was read
    @return whether parsing should proceed
    */
    virtual bool end_object() = 0;

    /*!
    @brief the beginning of an array was read
    @param[in] elements  number of array elements or -1 if unknown
    @return whether parsing should proceed
    @note binary formats may report the number of elements
    */
    virtual bool start_array(std::size_t elements) = 0;

    /*!
    @brief the end of an array was read
    @return whether parsing should proceed
    */
    virtual bool end_array() = 0;

    /*!
    @brief a parse error occurred
    @param[in] position    the position in the input where the error occurs
    @param[in] last_token  the last read token
    @param[in] ex          an exception object describing the error
    @return whether parsing should proceed (must return false)
    */
    virtual bool parse_error(std::size_t position,
                             const std::string& last_token,
                             const detail::exception& ex) = 0;

    virtual ~json_sax() = default;
};


namespace detail
{
/*!
@brief SAX implementation to create a JSON value from SAX events

This class implements the @ref json_sax interface and processes the SAX events
to create a JSON value which makes it basically a DOM parser. The structure or
hierarchy of the JSON value is managed by the stack `ref_stack` which contains
a pointer to the respective array or object for each recursion depth.

After successful parsing, the value that is passed by reference to the
constructor contains the parsed value.

@tparam BasicJsonType  the JSON type
*/
template<typename BasicJsonType>
class json_sax_dom_parser
{
  public:
    using number_integer_t = typename BasicJsonType::number_integer_t;
    using number_unsigned_t = typename BasicJsonType::number_unsigned_t;
    using number_float_t = typename BasicJsonType::number_float_t;
    using string_t = typename BasicJsonType::string_t;

    /*!
    @param[in, out] r  reference to a JSON value that is manipulated while
                       parsing
    @param[in] allow_exceptions_  whether parse errors yield exceptions
    */
    explicit json_sax_dom_parser(BasicJsonType& r, const bool allow_exceptions_ = true)
        : root(r), allow_exceptions(allow_exceptions_)
    {}

    // make class move-only
    json_sax_dom_parser(const json_sax_dom_parser&) = delete;
    json_sax_dom_parser(json_sax_dom_parser&&) = default;
    json_sax_dom_parser& operator=(const json_sax_dom_parser&) = delete;
    json_sax_dom_parser& operator=(json_sax_dom_parser&&) = default;
    ~json_sax_dom_parser() = default;

    bool null()
    {
        handle_value(nullptr);
        return true;
    }

    bool boolean(bool val)
    {
        handle_value(val);
        return true;
    }

    bool number_integer(number_integer_t val)
    {
        handle_value(val);
        return true;
    }

    bool number_unsigned(number_unsigned_t val)
    {
        handle_value(val);
        return true;
    }

    bool number_float(number_float_t val, const string_t& /*unused*/)
    {
        handle_value(val);
        return true;
    }

    bool string(string_t& val)
    {
        handle_value(val);
        return true;
    }

    bool start_object(std::size_t len)
    {
        ref_stack.push_back(handle_value(BasicJsonType::value_t::object));

        if (JSON_HEDLEY_UNLIKELY(len != std::size_t(-1) and len > ref_stack.back()->max_size()))
        {
            JSON_THROW(out_of_range::create(408,
                                            "excessive object size: " + std::to_string(len)));
        }

        return true;
    }

    bool key(string_t& val)
    {
        // add null at given key and store the reference for later
        object_element = &(ref_stack.back()->m_value.object->operator[](val));
        return true;
    }

    bool end_object()
    {
        ref_stack.pop_back();
        return true;
    }

    bool start_array(std::size_t len)
    {
        ref_stack.push_back(handle_value(BasicJsonType::value_t::array));

        if (JSON_HEDLEY_UNLIKELY(len != std::size_t(-1) and len > ref_stack.back()->max_size()))
        {
            JSON_THROW(out_of_range::create(408,
                                            "excessive array size: " + std::to_string(len)));
        }

        return true;
    }

    bool end_array()
    {
        ref_stack.pop_back();
        return true;
    }

    bool parse_error(std::size_t /*unused*/, const std::string& /*unused*/,
                     const detail::exception& ex)
    {
        errored = true;
        if (allow_exceptions)
        {
            // determine the proper exception type from the id
            switch ((ex.id / 100) % 100)
            {
                case 1:
                    JSON_THROW(*static_cast<const detail::parse_error*>(&ex));
                case 4:
                    JSON_THROW(*static_cast<const detail::out_of_range*>(&ex));
                // LCOV_EXCL_START
                case 2:
                    JSON_THROW(*static_cast<const detail::invalid_iterator*>(&ex));
                case 3:
                    JSON_THROW(*static_cast<const detail::type_error*>(&ex));
                case 5:
                    JSON_THROW(*static_cast<const detail::other_error*>(&ex));
                default:
                    assert(false);
                    // LCOV_EXCL_STOP
            }
        }
        return false;
    }

    constexpr bool is_errored() const
    {
        return errored;
    }

  private:
    /*!
    @invariant If the ref stack is empty, then the passed value will be the new
               root.
    @invariant If the ref stack contains a value, then it is an array or an
               object to which we can add elements
    */
    template<typename Value>
    JSON_HEDLEY_RETURNS_NON_NULL
    BasicJsonType* handle_value(Value&& v)
    {
        if (ref_stack.empty())
        {
            root = BasicJsonType(std::forward<Value>(v));
            return &root;
        }

        assert(ref_stack.back()->is_array() or ref_stack.back()->is_object());

        if (ref_stack.back()->is_array())
        {
            ref_stack.back()->m_value.array->emplace_back(std::forward<Value>(v));
            return &(ref_stack.back()->m_value.array->back());
        }

        assert(ref_stack.back()->is_object());
        assert(object_element);
        *object_element = BasicJsonType(std::forward<Value>(v));
        return object_element;
    }

    /// the parsed JSON value
    BasicJsonType& root;
    /// stack to model hierarchy of values
    std::vector<BasicJsonType*> ref_stack {};
    /// helper to hold the reference for the next object element
    BasicJsonType* object_element = nullptr;
    /// whether a syntax error occurred
    bool errored = false;
    /// whether to throw exceptions in case of errors
    const bool allow_exceptions = true;
};

template<typename BasicJsonType>
class json_sax_dom_callback_parser
{
  public:
    using number_integer_t = typename BasicJsonType::number_integer_t;
    using number_unsigned_t = typename BasicJsonType::number_unsigned_t;
    using number_float_t = typename BasicJsonType::number_float_t;
    using string_t = typename BasicJsonType::string_t;
    using parser_callback_t = typename BasicJsonType::parser_callback_t;
    using parse_event_t = typename BasicJsonType::parse_event_t;

    json_sax_dom_callback_parser(BasicJsonType& r,
                                 const parser_callback_t cb,
                                 const bool allow_exceptions_ = true)
        : root(r), callback(cb), allow_exceptions(allow_exceptions_)
    {
        keep_stack.push_back(true);
    }

    // make class move-only
    json_sax_dom_callback_parser(const json_sax_dom_callback_parser&) = delete;
    json_sax_dom_callback_parser(json_sax_dom_callback_parser&&) = default;
    json_sax_dom_callback_parser& operator=(const json_sax_dom_callback_parser&) = delete;
    json_sax_dom_callback_parser& operator=(json_sax_dom_callback_parser&&) = default;
    ~json_sax_dom_callback_parser() = default;

    bool null()
    {
        handle_value(nullptr);
        return true;
    }

    bool boolean(bool val)
    {
        handle_value(val);
        return true;
    }

    bool number_integer(number_integer_t val)
    {
        handle_value(val);
        return true;
    }

    bool number_unsigned(number_unsigned_t val)
    {
        handle_value(val);
        return true;
    }

    bool number_float(number_float_t val, const string_t& /*unused*/)
    {
        handle_value(val);
        return true;
    }

    bool string(string_t& val)
    {
        handle_value(val);
        return true;
    }

    bool start_object(std::size_t len)
    {
        // check callback for object start
        const bool keep = callback(static_cast<int>(ref_stack.size()), parse_event_t::object_start, discarded);
        keep_stack.push_back(keep);

        auto val = handle_value(BasicJsonType::value_t::object, true);
        ref_stack.push_back(val.second);

        // check object limit
        if (ref_stack.back() and JSON_HEDLEY_UNLIKELY(len != std::size_t(-1) and len > ref_stack.back()->max_size()))
        {
            JSON_THROW(out_of_range::create(408, "excessive object size: " + std::to_string(len)));
        }

        return true;
    }

    bool key(string_t& val)
    {
        BasicJsonType k = BasicJsonType(val);

        // check callback for key
        const bool keep = callback(static_cast<int>(ref_stack.size()), parse_event_t::key, k);
        key_keep_stack.push_back(keep);

        // add discarded value at given key and store the reference for later
        if (keep and ref_stack.back())
        {
            object_element = &(ref_stack.back()->m_value.object->operator[](val) = discarded);
        }

        return true;
    }

    bool end_object()
    {
        if (ref_stack.back() and not callback(static_cast<int>(ref_stack.size()) - 1, parse_event_t::object_end, *ref_stack.back()))
        {
            // discard object
            *ref_stack.back() = discarded;
        }

        assert(not ref_stack.empty());
        assert(not keep_stack.empty());
        ref_stack.pop_back();
        keep_stack.pop_back();

        if (not ref_stack.empty() and ref_stack.back() and ref_stack.back()->is_object())
        {
            // remove discarded value
            for (auto it = ref_stack.back()->begin(); it != ref_stack.back()->end(); ++it)
            {
                if (it->is_discarded())
                {
                    ref_stack.back()->erase(it);
                    break;
                }
            }
        }

        return true;
    }

    bool start_array(std::size_t len)
    {
        const bool keep = callback(static_cast<int>(ref_stack.size()), parse_event_t::array_start, discarded);
        keep_stack.push_back(keep);

        auto val = handle_value(BasicJsonType::value_t::array, true);
        ref_stack.push_back(val.second);

        // check array limit
        if (ref_stack.back() and JSON_HEDLEY_UNLIKELY(len != std::size_t(-1) and len > ref_stack.back()->max_size()))
        {
            JSON_THROW(out_of_range::create(408, "excessive array size: " + std::to_string(len)));
        }

        return true;
    }

    bool end_array()
    {
        bool keep = true;

        if (ref_stack.back())
        {
            keep = callback(static_cast<int>(ref_stack.size()) - 1, parse_event_t::array_end, *ref_stack.back());
            if (not keep)
            {
                // discard array
                *ref_stack.back() = discarded;
            }
        }

        assert(not ref_stack.empty());
        assert(not keep_stack.empty());
        ref_stack.pop_back();
        keep_stack.pop_back();

        // remove discarded value
        if (not keep and not ref_stack.empty() and ref_stack.back()->is_array())
        {
            ref_stack.back()->m_value.array->pop_back();
        }

        return true;
    }

    bool parse_error(std::size_t /*unused*/, const std::string& /*unused*/,
                     const detail::exception& ex)
    {
        errored = true;
        if (allow_exceptions)
        {
            // determine the proper exception type from the id
            switch ((ex.id / 100) % 100)
            {
                case 1:
                    JSON_THROW(*static_cast<const detail::parse_error*>(&ex));
                case 4:
                    JSON_THROW(*static_cast<const detail::out_of_range*>(&ex));
                // LCOV_EXCL_START
                case 2:
                    JSON_THROW(*static_cast<const detail::invalid_iterator*>(&ex));
                case 3:
                    JSON_THROW(*static_cast<const detail::type_error*>(&ex));
                case 5:
                    JSON_THROW(*static_cast<const detail::other_error*>(&ex));
                default:
                    assert(false);
                    // LCOV_EXCL_STOP
            }
        }
        return false;
    }

    constexpr bool is_errored() const
    {
        return errored;
    }

  private:
    /*!
    @param[in] v  value to add to the JSON value we build during parsing
    @param[in] skip_callback  whether we should skip calling the callback
               function; this is required after start_array() and
               start_object() SAX events, because otherwise we would call the
               callback function with an empty array or object, respectively.

    @invariant If the ref stack is empty, then the passed value will be the new
               root.
    @invariant If the ref stack contains a value, then it is an array or an
               object to which we can add elements

    @return pair of boolean (whether value should be kept) and pointer (to the
            passed value in the ref_stack hierarchy; nullptr if not kept)
    */
    template<typename Value>
    std::pair<bool, BasicJsonType*> handle_value(Value&& v, const bool skip_callback = false)
    {
        assert(not keep_stack.empty());

        // do not handle this value if we know it would be added to a discarded
        // container
        if (not keep_stack.back())
        {
            return {false, nullptr};
        }

        // create value
        auto value = BasicJsonType(std::forward<Value>(v));

        // check callback
        const bool keep = skip_callback or callback(static_cast<int>(ref_stack.size()), parse_event_t::value, value);

        // do not handle this value if we just learnt it shall be discarded
        if (not keep)
        {
            return {false, nullptr};
        }

        if (ref_stack.empty())
        {
            root = std::move(value);
            return {true, &root};
        }

        // skip this value if we already decided to skip the parent
        // (https://github.com/nlohmann/json/issues/971#issuecomment-413678360)
        if (not ref_stack.back())
        {
            return {false, nullptr};
        }

        // we now only expect arrays and objects
        assert(ref_stack.back()->is_array() or ref_stack.back()->is_object());

        // array
        if (ref_stack.back()->is_array())
        {
            ref_stack.back()->m_value.array->push_back(std::move(value));
            return {true, &(ref_stack.back()->m_value.array->back())};
        }

        // object
        assert(ref_stack.back()->is_object());
        // check if we should store an element for the current key
        assert(not key_keep_stack.empty());
        const bool store_element = key_keep_stack.back();
        key_keep_stack.pop_back();

        if (not store_element)
        {
            return {false, nullptr};
        }

        assert(object_element);
        *object_element = std::move(value);
        return {true, object_element};
    }

    /// the parsed JSON value
    BasicJsonType& root;
    /// stack to model hierarchy of values
    std::vector<BasicJsonType*> ref_stack {};
    /// stack to manage which values to keep
    std::vector<bool> keep_stack {};
    /// stack to manage which object keys to keep
    std::vector<bool> key_keep_stack {};
    /// helper to hold the reference for the next object element
    BasicJsonType* object_element = nullptr;
    /// whether a syntax error occurred
    bool errored = false;
    /// callback function
    const parser_callback_t callback = nullptr;
    /// whether to throw exceptions in case of errors
    const bool allow_exceptions = true;
    /// a discarded value for the callback
    BasicJsonType discarded = BasicJsonType::value_t::discarded;
};

template<typename BasicJsonType>
class json_sax_acceptor
{
  public:
    using number_integer_t = typename BasicJsonType::number_integer_t;
    using number_unsigned_t = typename BasicJsonType::number_unsigned_t;
    using number_float_t = typename BasicJsonType::number_float_t;
    using string_t = typename BasicJsonType::string_t;

    bool null()
    {
        return true;
    }

    bool boolean(bool /*unused*/)
    {
        return true;
    }

    bool number_integer(number_integer_t /*unused*/)
    {
        return true;
    }

    bool number_unsigned(number_unsigned_t /*unused*/)
    {
        return true;
    }

    bool number_float(number_float_t /*unused*/, const string_t& /*unused*/)
    {
        return true;
    }

    bool string(string_t& /*unused*/)
    {
        return true;
    }

    bool start_object(std::size_t  /*unused*/ = std::size_t(-1))
    {
        return true;
    }

    bool key(string_t& /*unused*/)
    {
        return true;
    }

    bool end_object()
    {
        return true;
    }

    bool start_array(std::size_t  /*unused*/ = std::size_t(-1))
    {
        return true;
    }

    bool end_array()
    {
        return true;
    }

    bool parse_error(std::size_t /*unused*/, const std::string& /*unused*/, const detail::exception& /*unused*/)
    {
        return false;
    }
};
}  // namespace detail

}  // namespace nlohmann

// #include <nlohmann/detail/macro_scope.hpp>

// #include <nlohmann/detail/meta/is_sax.hpp>


#include <cstdint> // size_t
#include <utility> // declval
#include <string> // string

// #include <nlohmann/detail/meta/detected.hpp>

// #include <nlohmann/detail/meta/type_traits.hpp>


namespace nlohmann
{
namespace detail
{
template <typename T>
using null_function_t = decltype(std::declval<T&>().null());

template <typename T>
using boolean_function_t =
    decltype(std::declval<T&>().boolean(std::declval<bool>()));

template <typename T, typename Integer>
using number_integer_function_t =
    decltype(std::declval<T&>().number_integer(std::declval<Integer>()));

template <typename T, typename Unsigned>
using number_unsigned_function_t =
    decltype(std::declval<T&>().number_unsigned(std::declval<Unsigned>()));

template <typename T, typename Float, typename String>
using number_float_function_t = decltype(std::declval<T&>().number_float(
                                    std::declval<Float>(), std::declval<const String&>()));

template <typename T, typename String>
using string_function_t =
    decltype(std::declval<T&>().string(std::declval<String&>()));

template <typename T>
using start_object_function_t =
    decltype(std::declval<T&>().start_object(std::declval<std::size_t>()));

template <typename T, typename String>
using key_function_t =
    decltype(std::declval<T&>().key(std::declval<String&>()));

template <typename T>
using end_object_function_t = decltype(std::declval<T&>().end_object());

template <typename T>
using start_array_function_t =
    decltype(std::declval<T&>().start_array(std::declval<std::size_t>()));

template <typename T>
using end_array_function_t = decltype(std::declval<T&>().end_array());

template <typename T, typename Exception>
using parse_error_function_t = decltype(std::declval<T&>().parse_error(
        std::declval<std::size_t>(), std::declval<const std::string&>(),
        std::declval<const Exception&>()));

template <typename SAX, typename BasicJsonType>
struct is_sax
{
  private:
    static_assert(is_basic_json<BasicJsonType>::value,
                  "BasicJsonType must be of type basic_json<...>");

    using number_integer_t = typename BasicJsonType::number_integer_t;
    using number_unsigned_t = typename BasicJsonType::number_unsigned_t;
    using number_float_t = typename BasicJsonType::number_float_t;
    using string_t = typename BasicJsonType::string_t;
    using exception_t = typename BasicJsonType::exception;

  public:
    static constexpr bool value =
        is_detected_exact<bool, null_function_t, SAX>::value &&
        is_detected_exact<bool, boolean_function_t, SAX>::value &&
        is_detected_exact<bool, number_integer_function_t, SAX,
        number_integer_t>::value &&
        is_detected_exact<bool, number_unsigned_function_t, SAX,
        number_unsigned_t>::value &&
        is_detected_exact<bool, number_float_function_t, SAX, number_float_t,
        string_t>::value &&
        is_detected_exact<bool, string_function_t, SAX, string_t>::value &&
        is_detected_exact<bool, start_object_function_t, SAX>::value &&
        is_detected_exact<bool, key_function_t, SAX, string_t>::value &&
        is_detected_exact<bool, end_object_function_t, SAX>::value &&
        is_detected_exact<bool, start_array_function_t, SAX>::value &&
        is_detected_exact<bool, end_array_function_t, SAX>::value &&
        is_detected_exact<bool, parse_error_function_t, SAX, exception_t>::value;
};

template <typename SAX, typename BasicJsonType>
struct is_sax_static_asserts
{
  private:
    static_assert(is_basic_json<BasicJsonType>::value,
                  "BasicJsonType must be of type basic_json<...>");

    using number_integer_t = typename BasicJsonType::number_integer_t;
    using number_unsigned_t = typename BasicJsonType::number_unsigned_t;
    using number_float_t = typename BasicJsonType::number_float_t;
    using string_t = typename BasicJsonType::string_t;
    using exception_t = typename BasicJsonType::exception;

  public:
    static_assert(is_detected_exact<bool, null_function_t, SAX>::value,
                  "Missing/invalid function: bool null()");
    static_assert(is_detected_exact<bool, boolean_function_t, SAX>::value,
                  "Missing/invalid function: bool boolean(bool)");
    static_assert(is_detected_exact<bool, boolean_function_t, SAX>::value,
                  "Missing/invalid function: bool boolean(bool)");
    static_assert(
        is_detected_exact<bool, number_integer_function_t, SAX,
        number_integer_t>::value,
        "Missing/invalid function: bool number_integer(number_integer_t)");
    static_assert(
        is_detected_exact<bool, number_unsigned_function_t, SAX,
        number_unsigned_t>::value,
        "Missing/invalid function: bool number_unsigned(number_unsigned_t)");
    static_assert(is_detected_exact<bool, number_float_function_t, SAX,
                  number_float_t, string_t>::value,
                  "Missing/invalid function: bool number_float(number_float_t, const string_t&)");
    static_assert(
        is_detected_exact<bool, string_function_t, SAX, string_t>::value,
        "Missing/invalid function: bool string(string_t&)");
    static_assert(is_detected_exact<bool, start_object_function_t, SAX>::value,
                  "Missing/invalid function: bool start_object(std::size_t)");
    static_assert(is_detected_exact<bool, key_function_t, SAX, string_t>::value,
                  "Missing/invalid function: bool key(string_t&)");
    static_assert(is_detected_exact<bool, end_object_function_t, SAX>::value,
                  "Missing/invalid function: bool end_object()");
    static_assert(is_detected_exact<bool, start_array_function_t, SAX>::value,
                  "Missing/invalid function: bool start_array(std::size_t)");
    static_assert(is_detected_exact<bool, end_array_function_t, SAX>::value,
                  "Missing/invalid function: bool end_array()");
    static_assert(
        is_detected_exact<bool, parse_error_function_t, SAX, exception_t>::value,
        "Missing/invalid function: bool parse_error(std::size_t, const "
        "std::string&, const exception&)");
};
}  // namespace detail
}  // namespace nlohmann

// #include <nlohmann/detail/value_t.hpp>


namespace nlohmann
{
namespace detail
{
///////////////////
// binary reader //
///////////////////

/*!
@brief deserialization of CBOR, MessagePack, and UBJSON values
*/
template<typename BasicJsonType, typename SAX = json_sax_dom_parser<BasicJsonType>>
class binary_reader
{
    using number_integer_t = typename BasicJsonType::number_integer_t;
    using number_unsigned_t = typename BasicJsonType::number_unsigned_t;
    using number_float_t = typename BasicJsonType::number_float_t;
    using string_t = typename BasicJsonType::string_t;
    using json_sax_t = SAX;

  public:
    /*!
    @brief create a binary reader

    @param[in] adapter  input adapter to read from
    */
    explicit binary_reader(input_adapter_t adapter) : ia(std::move(adapter))
    {
        (void)detail::is_sax_static_asserts<SAX, BasicJsonType> {};
        assert(ia);
    }

    // make class move-only
    binary_reader(const binary_reader&) = delete;
    binary_reader(binary_reader&&) = default;
    binary_reader& operator=(const binary_reader&) = delete;
    binary_reader& operator=(binary_reader&&) = default;
    ~binary_reader() = default;

    /*!
    @param[in] format  the binary format to parse
    @param[in] sax_    a SAX event processor
    @param[in] strict  whether to expect the input to be consumed completed

    @return
    */
    JSON_HEDLEY_NON_NULL(3)
    bool sax_parse(const input_format_t format,
                   json_sax_t* sax_,
                   const bool strict = true)
    {
        sax = sax_;
        bool result = false;

        switch (format)
        {
            case input_format_t::bson:
                result = parse_bson_internal();
                break;

            case input_format_t::cbor:
                result = parse_cbor_internal();
                break;

            case input_format_t::msgpack:
                result = parse_msgpack_internal();
                break;

            case input_format_t::ubjson:
                result = parse_ubjson_internal();
                break;

            default:            // LCOV_EXCL_LINE
                assert(false);  // LCOV_EXCL_LINE
        }

        // strict mode: next byte must be EOF
        if (result and strict)
        {
            if (format == input_format_t::ubjson)
            {
                get_ignore_noop();
            }
            else
            {
                get();
            }

            if (JSON_HEDLEY_UNLIKELY(current != std::char_traits<char>::eof()))
            {
                return sax->parse_error(chars_read, get_token_string(),
                                        parse_error::create(110, chars_read, exception_message(format, "expected end of input; last byte: 0x" + get_token_string(), "value")));
            }
        }

        return result;
    }

    /*!
    @brief determine system byte order

    @return true if and only if system's byte order is little endian

    @note from http://stackoverflow.com/a/1001328/266378
    */
    static constexpr bool little_endianess(int num = 1) noexcept
    {
        return *reinterpret_cast<char*>(&num) == 1;
    }

  private:
    //////////
    // BSON //
    //////////

    /*!
    @brief Reads in a BSON-object and passes it to the SAX-parser.
    @return whether a valid BSON-value was passed to the SAX parser
    */
    bool parse_bson_internal()
    {
        std::int32_t document_size;
        get_number<std::int32_t, true>(input_format_t::bson, document_size);

        if (JSON_HEDLEY_UNLIKELY(not sax->start_object(std::size_t(-1))))
        {
            return false;
        }

        if (JSON_HEDLEY_UNLIKELY(not parse_bson_element_list(/*is_array*/false)))
        {
            return false;
        }

        return sax->end_object();
    }

    /*!
    @brief Parses a C-style string from the BSON input.
    @param[in, out] result  A reference to the string variable where the read
                            string is to be stored.
    @return `true` if the \x00-byte indicating the end of the string was
             encountered before the EOF; false` indicates an unexpected EOF.
    */
    bool get_bson_cstr(string_t& result)
    {
        auto out = std::back_inserter(result);
        while (true)
        {
            get();
            if (JSON_HEDLEY_UNLIKELY(not unexpect_eof(input_format_t::bson, "cstring")))
            {
                return false;
            }
            if (current == 0x00)
            {
                return true;
            }
            *out++ = static_cast<char>(current);
        }

        return true;
    }

    /*!
    @brief Parses a zero-terminated string of length @a len from the BSON
           input.
    @param[in] len  The length (including the zero-byte at the end) of the
                    string to be read.
    @param[in, out] result  A reference to the string variable where the read
                            string is to be stored.
    @tparam NumberType The type of the length @a len
    @pre len >= 1
    @return `true` if the string was successfully parsed
    */
    template<typename NumberType>
    bool get_bson_string(const NumberType len, string_t& result)
    {
        if (JSON_HEDLEY_UNLIKELY(len < 1))
        {
            auto last_token = get_token_string();
            return sax->parse_error(chars_read, last_token, parse_error::create(112, chars_read, exception_message(input_format_t::bson, "string length must be at least 1, is " + std::to_string(len), "string")));
        }

        return get_string(input_format_t::bson, len - static_cast<NumberType>(1), result) and get() != std::char_traits<char>::eof();
    }

    /*!
    @brief Read a BSON document element of the given @a element_type.
    @param[in] element_type The BSON element type, c.f. http://bsonspec.org/spec.html
    @param[in] element_type_parse_position The position in the input stream,
               where the `element_type` was read.
    @warning Not all BSON element types are supported yet. An unsupported
             @a element_type will give rise to a parse_error.114:
             Unsupported BSON record type 0x...
    @return whether a valid BSON-object/array was passed to the SAX parser
    */
    bool parse_bson_element_internal(const int element_type,
                                     const std::size_t element_type_parse_position)
    {
        switch (element_type)
        {
            case 0x01: // double
            {
                double number;
                return get_number<double, true>(input_format_t::bson, number) and sax->number_float(static_cast<number_float_t>(number), "");
            }

            case 0x02: // string
            {
                std::int32_t len;
                string_t value;
                return get_number<std::int32_t, true>(input_format_t::bson, len) and get_bson_string(len, value) and sax->string(value);
            }

            case 0x03: // object
            {
                return parse_bson_internal();
            }

            case 0x04: // array
            {
                return parse_bson_array();
            }

            case 0x08: // boolean
            {
                return sax->boolean(get() != 0);
            }

            case 0x0A: // null
            {
                return sax->null();
            }

            case 0x10: // int32
            {
                std::int32_t value;
                return get_number<std::int32_t, true>(input_format_t::bson, value) and sax->number_integer(value);
            }

            case 0x12: // int64
            {
                std::int64_t value;
                return get_number<std::int64_t, true>(input_format_t::bson, value) and sax->number_integer(value);
            }

            default: // anything else not supported (yet)
            {
                std::array<char, 3> cr{{}};
                (std::snprintf)(cr.data(), cr.size(), "%.2hhX", static_cast<unsigned char>(element_type));
                return sax->parse_error(element_type_parse_position, std::string(cr.data()), parse_error::create(114, element_type_parse_position, "Unsupported BSON record type 0x" + std::string(cr.data())));
            }
        }
    }

    /*!
    @brief Read a BSON element list (as specified in the BSON-spec)

    The same binary layout is used for objects and arrays, hence it must be
    indicated with the argument @a is_array which one is expected
    (true --> array, false --> object).

    @param[in] is_array Determines if the element list being read is to be
                        treated as an object (@a is_array == false), or as an
                        array (@a is_array == true).
    @return whether a valid BSON-object/array was passed to the SAX parser
    */
    bool parse_bson_element_list(const bool is_array)
    {
        string_t key;
        while (int element_type = get())
        {
            if (JSON_HEDLEY_UNLIKELY(not unexpect_eof(input_format_t::bson, "element list")))
            {
                return false;
            }

            const std::size_t element_type_parse_position = chars_read;
            if (JSON_HEDLEY_UNLIKELY(not get_bson_cstr(key)))
            {
                return false;
            }

            if (not is_array and not sax->key(key))
            {
                return false;
            }

            if (JSON_HEDLEY_UNLIKELY(not parse_bson_element_internal(element_type, element_type_parse_position)))
            {
                return false;
            }

            // get_bson_cstr only appends
            key.clear();
        }

        return true;
    }

    /*!
    @brief Reads an array from the BSON input and passes it to the SAX-parser.
    @return whether a valid BSON-array was passed to the SAX parser
    */
    bool parse_bson_array()
    {
        std::int32_t document_size;
        get_number<std::int32_t, true>(input_format_t::bson, document_size);

        if (JSON_HEDLEY_UNLIKELY(not sax->start_array(std::size_t(-1))))
        {
            return false;
        }

        if (JSON_HEDLEY_UNLIKELY(not parse_bson_element_list(/*is_array*/true)))
        {
            return false;
        }

        return sax->end_array();
    }

    //////////
    // CBOR //
    //////////

    /*!
    @param[in] get_char  whether a new character should be retrieved from the
                         input (true, default) or whether the last read
                         character should be considered instead

    @return whether a valid CBOR value was passed to the SAX parser
    */
    bool parse_cbor_internal(const bool get_char = true)
    {
        switch (get_char ? get() : current)
        {
            // EOF
            case std::char_traits<char>::eof():
                return unexpect_eof(input_format_t::cbor, "value");

            // Integer 0x00..0x17 (0..23)
            case 0x00:
            case 0x01:
            case 0x02:
            case 0x03:
            case 0x04:
            case 0x05:
            case 0x06:
            case 0x07:
            case 0x08:
            case 0x09:
            case 0x0A:
            case 0x0B:
            case 0x0C:
            case 0x0D:
            case 0x0E:
            case 0x0F:
            case 0x10:
            case 0x11:
            case 0x12:
            case 0x13:
            case 0x14:
            case 0x15:
            case 0x16:
            case 0x17:
                return sax->number_unsigned(static_cast<number_unsigned_t>(current));

            case 0x18: // Unsigned integer (one-byte uint8_t follows)
            {
                std::uint8_t number;
                return get_number(input_format_t::cbor, number) and sax->number_unsigned(number);
            }

            case 0x19: // Unsigned integer (two-byte uint16_t follows)
            {
                std::uint16_t number;
                return get_number(input_format_t::cbor, number) and sax->number_unsigned(number);
            }

            case 0x1A: // Unsigned integer (four-byte uint32_t follows)
            {
                std::uint32_t number;
                return get_number(input_format_t::cbor, number) and sax->number_unsigned(number);
            }

            case 0x1B: // Unsigned integer (eight-byte uint64_t follows)
            {
                std::uint64_t number;
                return get_number(input_format_t::cbor, number) and sax->number_unsigned(number);
            }

            // Negative integer -1-0x00..-1-0x17 (-1..-24)
            case 0x20:
            case 0x21:
            case 0x22:
            case 0x23:
            case 0x24:
            case 0x25:
            case 0x26:
            case 0x27:
            case 0x28:
            case 0x29:
            case 0x2A:
            case 0x2B:
            case 0x2C:
            case 0x2D:
            case 0x2E:
            case 0x2F:
            case 0x30:
            case 0x31:
            case 0x32:
            case 0x33:
            case 0x34:
            case 0x35:
            case 0x36:
            case 0x37:
                return sax->number_integer(static_cast<std::int8_t>(0x20 - 1 - current));

            case 0x38: // Negative integer (one-byte uint8_t follows)
            {
                std::uint8_t number;
                return get_number(input_format_t::cbor, number) and sax->number_integer(static_cast<number_integer_t>(-1) - number);
            }

            case 0x39: // Negative integer -1-n (two-byte uint16_t follows)
            {
                std::uint16_t number;
                return get_number(input_format_t::cbor, number) and sax->number_integer(static_cast<number_integer_t>(-1) - number);
            }

            case 0x3A: // Negative integer -1-n (four-byte uint32_t follows)
            {
                std::uint32_t number;
                return get_number(input_format_t::cbor, number) and sax->number_integer(static_cast<number_integer_t>(-1) - number);
            }

            case 0x3B: // Negative integer -1-n (eight-byte uint64_t follows)
            {
                std::uint64_t number;
                return get_number(input_format_t::cbor, number) and sax->number_integer(static_cast<number_integer_t>(-1)
                        - static_cast<number_integer_t>(number));
            }

            // UTF-8 string (0x00..0x17 bytes follow)
            case 0x60:
            case 0x61:
            case 0x62:
            case 0x63:
            case 0x64:
            case 0x65:
            case 0x66:
            case 0x67:
            case 0x68:
            case 0x69:
            case 0x6A:
            case 0x6B:
            case 0x6C:
            case 0x6D:
            case 0x6E:
            case 0x6F:
            case 0x70:
            case 0x71:
            case 0x72:
            case 0x73:
            case 0x74:
            case 0x75:
            case 0x76:
            case 0x77:
            case 0x78: // UTF-8 string (one-byte uint8_t for n follows)
            case 0x79: // UTF-8 string (two-byte uint16_t for n follow)
            case 0x7A: // UTF-8 string (four-byte uint32_t for n follow)
            case 0x7B: // UTF-8 string (eight-byte uint64_t for n follow)
            case 0x7F: // UTF-8 string (indefinite length)
            {
                string_t s;
                return get_cbor_string(s) and sax->string(s);
            }

            // array (0x00..0x17 data items follow)
            case 0x80:
            case 0x81:
            case 0x82:
            case 0x83:
            case 0x84:
            case 0x85:
            case 0x86:
            case 0x87:
            case 0x88:
            case 0x89:
            case 0x8A:
            case 0x8B:
            case 0x8C:
            case 0x8D:
            case 0x8E:
            case 0x8F:
            case 0x90:
            case 0x91:
            case 0x92:
            case 0x93:
            case 0x94:
            case 0x95:
            case 0x96:
            case 0x97:
                return get_cbor_array(static_cast<std::size_t>(static_cast<unsigned int>(current) & 0x1Fu));

            case 0x98: // array (one-byte uint8_t for n follows)
            {
                std::uint8_t len;
                return get_number(input_format_t::cbor, len) and get_cbor_array(static_cast<std::size_t>(len));
            }

            case 0x99: // array (two-byte uint16_t for n follow)
            {
                std::uint16_t len;
                return get_number(input_format_t::cbor, len) and get_cbor_array(static_cast<std::size_t>(len));
            }

            case 0x9A: // array (four-byte uint32_t for n follow)
            {
                std::uint32_t len;
                return get_number(input_format_t::cbor, len) and get_cbor_array(static_cast<std::size_t>(len));
            }

            case 0x9B: // array (eight-byte uint64_t for n follow)
            {
                std::uint64_t len;
                return get_number(input_format_t::cbor, len) and get_cbor_array(static_cast<std::size_t>(len));
            }

            case 0x9F: // array (indefinite length)
                return get_cbor_array(std::size_t(-1));

            // map (0x00..0x17 pairs of data items follow)
            case 0xA0:
            case 0xA1:
            case 0xA2:
            case 0xA3:
            case 0xA4:
            case 0xA5:
            case 0xA6:
            case 0xA7:
            case 0xA8:
            case 0xA9:
            case 0xAA:
            case 0xAB:
            case 0xAC:
            case 0xAD:
            case 0xAE:
            case 0xAF:
            case 0xB0:
            case 0xB1:
            case 0xB2:
            case 0xB3:
            case 0xB4:
            case 0xB5:
            case 0xB6:
            case 0xB7:
                return get_cbor_object(static_cast<std::size_t>(static_cast<unsigned int>(current) & 0x1Fu));

            case 0xB8: // map (one-byte uint8_t for n follows)
            {
                std::uint8_t len;
                return get_number(input_format_t::cbor, len) and get_cbor_object(static_cast<std::size_t>(len));
            }

            case 0xB9: // map (two-byte uint16_t for n follow)
            {
                std::uint16_t len;
                return get_number(input_format_t::cbor, len) and get_cbor_object(static_cast<std::size_t>(len));
            }

            case 0xBA: // map (four-byte uint32_t for n follow)
            {
                std::uint32_t len;
                return get_number(input_format_t::cbor, len) and get_cbor_object(static_cast<std::size_t>(len));
            }

            case 0xBB: // map (eight-byte uint64_t for n follow)
            {
                std::uint64_t len;
                return get_number(input_format_t::cbor, len) and get_cbor_object(static_cast<std::size_t>(len));
            }

            case 0xBF: // map (indefinite length)
                return get_cbor_object(std::size_t(-1));

            case 0xF4: // false
                return sax->boolean(false);

            case 0xF5: // true
                return sax->boolean(true);

            case 0xF6: // null
                return sax->null();

            case 0xF9: // Half-Precision Float (two-byte IEEE 754)
            {
                const int byte1_raw = get();
                if (JSON_HEDLEY_UNLIKELY(not unexpect_eof(input_format_t::cbor, "number")))
                {
                    return false;
                }
                const int byte2_raw = get();
                if (JSON_HEDLEY_UNLIKELY(not unexpect_eof(input_format_t::cbor, "number")))
                {
                    return false;
                }

                const auto byte1 = static_cast<unsigned char>(byte1_raw);
                const auto byte2 = static_cast<unsigned char>(byte2_raw);

                // code from RFC 7049, Appendix D, Figure 3:
                // As half-precision floating-point numbers were only added
                // to IEEE 754 in 2008, today's programming platforms often
                // still only have limited support for them. It is very
                // easy to include at least decoding support for them even
                // without such support. An example of a small decoder for
                // half-precision floating-point numbers in the C language
                // is shown in Fig. 3.
                const auto half = static_cast<unsigned int>((byte1 << 8u) + byte2);
                const double val = [&half]
                {
                    const int exp = (half >> 10u) & 0x1Fu;
                    const unsigned int mant = half & 0x3FFu;
                    assert(0 <= exp and exp <= 32);
                    assert(mant <= 1024);
                    switch (exp)
                    {
                        case 0:
                            return std::ldexp(mant, -24);
                        case 31:
                            return (mant == 0)
                            ? std::numeric_limits<double>::infinity()
                            : std::numeric_limits<double>::quiet_NaN();
                        default:
                            return std::ldexp(mant + 1024, exp - 25);
                    }
                }();
                return sax->number_float((half & 0x8000u) != 0
                                         ? static_cast<number_float_t>(-val)
                                         : static_cast<number_float_t>(val), "");
            }

            case 0xFA: // Single-Precision Float (four-byte IEEE 754)
            {
                float number;
                return get_number(input_format_t::cbor, number) and sax->number_float(static_cast<number_float_t>(number), "");
            }

            case 0xFB: // Double-Precision Float (eight-byte IEEE 754)
            {
                double number;
                return get_number(input_format_t::cbor, number) and sax->number_float(static_cast<number_float_t>(number), "");
            }

            default: // anything else (0xFF is handled inside the other types)
            {
                auto last_token = get_token_string();
                return sax->parse_error(chars_read, last_token, parse_error::create(112, chars_read, exception_message(input_format_t::cbor, "invalid byte: 0x" + last_token, "value")));
            }
        }
    }

    /*!
    @brief reads a CBOR string

    This function first reads starting bytes to determine the expected
    string length and then copies this number of bytes into a string.
    Additionally, CBOR's strings with indefinite lengths are supported.

    @param[out] result  created string

    @return whether string creation completed
    */
    bool get_cbor_string(string_t& result)
    {
        if (JSON_HEDLEY_UNLIKELY(not unexpect_eof(input_format_t::cbor, "string")))
        {
            return false;
        }

        switch (current)
        {
            // UTF-8 string (0x00..0x17 bytes follow)
            case 0x60:
            case 0x61:
            case 0x62:
            case 0x63:
            case 0x64:
            case 0x65:
            case 0x66:
            case 0x67:
            case 0x68:
            case 0x69:
            case 0x6A:
            case 0x6B:
            case 0x6C:
            case 0x6D:
            case 0x6E:
            case 0x6F:
            case 0x70:
            case 0x71:
            case 0x72:
            case 0x73:
            case 0x74:
            case 0x75:
            case 0x76:
            case 0x77:
            {
                return get_string(input_format_t::cbor, static_cast<unsigned int>(current) & 0x1Fu, result);
            }

            case 0x78: // UTF-8 string (one-byte uint8_t for n follows)
            {
                std::uint8_t len;
                return get_number(input_format_t::cbor, len) and get_string(input_format_t::cbor, len, result);
            }

            case 0x79: // UTF-8 string (two-byte uint16_t for n follow)
            {
                std::uint16_t len;
                return get_number(input_format_t::cbor, len) and get_string(input_format_t::cbor, len, result);
            }

            case 0x7A: // UTF-8 string (four-byte uint32_t for n follow)
            {
                std::uint32_t len;
                return get_number(input_format_t::cbor, len) and get_string(input_format_t::cbor, len, result);
            }

            case 0x7B: // UTF-8 string (eight-byte uint64_t for n follow)
            {
                std::uint64_t len;
                return get_number(input_format_t::cbor, len) and get_string(input_format_t::cbor, len, result);
            }

            case 0x7F: // UTF-8 string (indefinite length)
            {
                while (get() != 0xFF)
                {
                    string_t chunk;
                    if (not get_cbor_string(chunk))
                    {
                        return false;
                    }
                    result.append(chunk);
                }
                return true;
            }

            default:
            {
                auto last_token = get_token_string();
                return sax->parse_error(chars_read, last_token, parse_error::create(113, chars_read, exception_message(input_format_t::cbor, "expected length specification (0x60-0x7B) or indefinite string type (0x7F); last byte: 0x" + last_token, "string")));
            }
        }
    }

    /*!
    @param[in] len  the length of the array or std::size_t(-1) for an
                    array of indefinite size
    @return whether array creation completed
    */
    bool get_cbor_array(const std::size_t len)
    {
        if (JSON_HEDLEY_UNLIKELY(not sax->start_array(len)))
        {
            return false;
        }

        if (len != std::size_t(-1))
        {
            for (std::size_t i = 0; i < len; ++i)
            {
                if (JSON_HEDLEY_UNLIKELY(not parse_cbor_internal()))
                {
                    return false;
                }
            }
        }
        else
        {
            while (get() != 0xFF)
            {
                if (JSON_HEDLEY_UNLIKELY(not parse_cbor_internal(false)))
                {
                    return false;
                }
            }
        }

        return sax->end_array();
    }

    /*!
    @param[in] len  the length of the object or std::size_t(-1) for an
                    object of indefinite size
    @return whether object creation completed
    */
    bool get_cbor_object(const std::size_t len)
    {
        if (JSON_HEDLEY_UNLIKELY(not sax->start_object(len)))
        {
            return false;
        }

        string_t key;
        if (len != std::size_t(-1))
        {
            for (std::size_t i = 0; i < len; ++i)
            {
                get();
                if (JSON_HEDLEY_UNLIKELY(not get_cbor_string(key) or not sax->key(key)))
                {
                    return false;
                }

                if (JSON_HEDLEY_UNLIKELY(not parse_cbor_internal()))
                {
                    return false;
                }
                key.clear();
            }
        }
        else
        {
            while (get() != 0xFF)
            {
                if (JSON_HEDLEY_UNLIKELY(not get_cbor_string(key) or not sax->key(key)))
                {
                    return false;
                }

                if (JSON_HEDLEY_UNLIKELY(not parse_cbor_internal()))
                {
                    return false;
                }
                key.clear();
            }
        }

        return sax->end_object();
    }

    /////////////
    // MsgPack //
    /////////////

    /*!
    @return whether a valid MessagePack value was passed to the SAX parser
    */
    bool parse_msgpack_internal()
    {
        switch (get())
        {
            // EOF
            case std::char_traits<char>::eof():
                return unexpect_eof(input_format_t::msgpack, "value");

            // positive fixint
            case 0x00:
            case 0x01:
            case 0x02:
            case 0x03:
            case 0x04:
            case 0x05:
            case 0x06:
            case 0x07:
            case 0x08:
            case 0x09:
            case 0x0A:
            case 0x0B:
            case 0x0C:
            case 0x0D:
            case 0x0E:
            case 0x0F:
            case 0x10:
            case 0x11:
            case 0x12:
            case 0x13:
            case 0x14:
            case 0x15:
            case 0x16:
            case 0x17:
            case 0x18:
            case 0x19:
            case 0x1A:
            case 0x1B:
            case 0x1C:
            case 0x1D:
            case 0x1E:
            case 0x1F:
            case 0x20:
            case 0x21:
            case 0x22:
            case 0x23:
            case 0x24:
            case 0x25:
            case 0x26:
            case 0x27:
            case 0x28:
            case 0x29:
            case 0x2A:
            case 0x2B:
            case 0x2C:
            case 0x2D:
            case 0x2E:
            case 0x2F:
            case 0x30:
            case 0x31:
            case 0x32:
            case 0x33:
            case 0x34:
            case 0x35:
            case 0x36:
            case 0x37:
            case 0x38:
            case 0x39:
            case 0x3A:
            case 0x3B:
            case 0x3C:
            case 0x3D:
            case 0x3E:
            case 0x3F:
            case 0x40:
            case 0x41:
            case 0x42:
            case 0x43:
            case 0x44:
            case 0x45:
            case 0x46:
            case 0x47:
            case 0x48:
            case 0x49:
            case 0x4A:
            case 0x4B:
            case 0x4C:
            case 0x4D:
            case 0x4E:
            case 0x4F:
            case 0x50:
            case 0x51:
            case 0x52:
            case 0x53:
            case 0x54:
            case 0x55:
            case 0x56:
            case 0x57:
            case 0x58:
            case 0x59:
            case 0x5A:
            case 0x5B:
            case 0x5C:
            case 0x5D:
            case 0x5E:
            case 0x5F:
            case 0x60:
            case 0x61:
            case 0x62:
            case 0x63:
            case 0x64:
            case 0x65:
            case 0x66:
            case 0x67:
            case 0x68:
            case 0x69:
            case 0x6A:
            case 0x6B:
            case 0x6C:
            case 0x6D:
            case 0x6E:
            case 0x6F:
            case 0x70:
            case 0x71:
            case 0x72:
            case 0x73:
            case 0x74:
            case 0x75:
            case 0x76:
            case 0x77:
            case 0x78:
            case 0x79:
            case 0x7A:
            case 0x7B:
            case 0x7C:
            case 0x7D:
            case 0x7E:
            case 0x7F:
                return sax->number_unsigned(static_cast<number_unsigned_t>(current));

            // fixmap
            case 0x80:
            case 0x81:
            case 0x82:
            case 0x83:
            case 0x84:
            case 0x85:
            case 0x86:
            case 0x87:
            case 0x88:
            case 0x89:
            case 0x8A:
            case 0x8B:
            case 0x8C:
            case 0x8D:
            case 0x8E:
            case 0x8F:
                return get_msgpack_object(static_cast<std::size_t>(static_cast<unsigned int>(current) & 0x0Fu));

            // fixarray
            case 0x90:
            case 0x91:
            case 0x92:
            case 0x93:
            case 0x94:
            case 0x95:
            case 0x96:
            case 0x97:
            case 0x98:
            case 0x99:
            case 0x9A:
            case 0x9B:
            case 0x9C:
            case 0x9D:
            case 0x9E:
            case 0x9F:
                return get_msgpack_array(static_cast<std::size_t>(static_cast<unsigned int>(current) & 0x0Fu));

            // fixstr
            case 0xA0:
            case 0xA1:
            case 0xA2:
            case 0xA3:
            case 0xA4:
            case 0xA5:
            case 0xA6:
            case 0xA7:
            case 0xA8:
            case 0xA9:
            case 0xAA:
            case 0xAB:
            case 0xAC:
            case 0xAD:
            case 0xAE:
            case 0xAF:
            case 0xB0:
            case 0xB1:
            case 0xB2:
            case 0xB3:
            case 0xB4:
            case 0xB5:
            case 0xB6:
            case 0xB7:
            case 0xB8:
            case 0xB9:
            case 0xBA:
            case 0xBB:
            case 0xBC:
            case 0xBD:
            case 0xBE:
            case 0xBF:
            case 0xD9: // str 8
            case 0xDA: // str 16
            case 0xDB: // str 32
            {
                string_t s;
                return get_msgpack_string(s) and sax->string(s);
            }

            case 0xC0: // nil
                return sax->null();

            case 0xC2: // false
                return sax->boolean(false);

            case 0xC3: // true
                return sax->boolean(true);

            case 0xCA: // float 32
            {
                float number;
                return get_number(input_format_t::msgpack, number) and sax->number_float(static_cast<number_float_t>(number), "");
            }

            case 0xCB: // float 64
            {
                double number;
                return get_number(input_format_t::msgpack, number) and sax->number_float(static_cast<number_float_t>(number), "");
            }

            case 0xCC: // uint 8
            {
                std::uint8_t number;
                return get_number(input_format_t::msgpack, number) and sax->number_unsigned(number);
            }

            case 0xCD: // uint 16
            {
                std::uint16_t number;
                return get_number(input_format_t::msgpack, number) and sax->number_unsigned(number);
            }

            case 0xCE: // uint 32
            {
                std::uint32_t number;
                return get_number(input_format_t::msgpack, number) and sax->number_unsigned(number);
            }

            case 0xCF: // uint 64
            {
                std::uint64_t number;
                return get_number(input_format_t::msgpack, number) and sax->number_unsigned(number);
            }

            case 0xD0: // int 8
            {
                std::int8_t number;
                return get_number(input_format_t::msgpack, number) and sax->number_integer(number);
            }

            case 0xD1: // int 16
            {
                std::int16_t number;
                return get_number(input_format_t::msgpack, number) and sax->number_integer(number);
            }

            case 0xD2: // int 32
            {
                std::int32_t number;
                return get_number(input_format_t::msgpack, number) and sax->number_integer(number);
            }

            case 0xD3: // int 64
            {
                std::int64_t number;
                return get_number(input_format_t::msgpack, number) and sax->number_integer(number);
            }

            case 0xDC: // array 16
            {
                std::uint16_t len;
                return get_number(input_format_t::msgpack, len) and get_msgpack_array(static_cast<std::size_t>(len));
            }

            case 0xDD: // array 32
            {
                std::uint32_t len;
                return get_number(input_format_t::msgpack, len) and get_msgpack_array(static_cast<std::size_t>(len));
            }

            case 0xDE: // map 16
            {
                std::uint16_t len;
                return get_number(input_format_t::msgpack, len) and get_msgpack_object(static_cast<std::size_t>(len));
            }

            case 0xDF: // map 32
            {
                std::uint32_t len;
                return get_number(input_format_t::msgpack, len) and get_msgpack_object(static_cast<std::size_t>(len));
            }

            // negative fixint
            case 0xE0:
            case 0xE1:
            case 0xE2:
            case 0xE3:
            case 0xE4:
            case 0xE5:
            case 0xE6:
            case 0xE7:
            case 0xE8:
            case 0xE9:
            case 0xEA:
            case 0xEB:
            case 0xEC:
            case 0xED:
            case 0xEE:
            case 0xEF:
            case 0xF0:
            case 0xF1:
            case 0xF2:
            case 0xF3:
            case 0xF4:
            case 0xF5:
            case 0xF6:
            case 0xF7:
            case 0xF8:
            case 0xF9:
            case 0xFA:
            case 0xFB:
            case 0xFC:
            case 0xFD:
            case 0xFE:
            case 0xFF:
                return sax->number_integer(static_cast<std::int8_t>(current));

            default: // anything else
            {
                auto last_token = get_token_string();
                return sax->parse_error(chars_read, last_token, parse_error::create(112, chars_read, exception_message(input_format_t::msgpack, "invalid byte: 0x" + last_token, "value")));
            }
        }
    }

    /*!
    @brief reads a MessagePack string

    This function first reads starting bytes to determine the expected
    string length and then copies this number of bytes into a string.

    @param[out] result  created string

    @return whether string creation completed
    */
    bool get_msgpack_string(string_t& result)
    {
        if (JSON_HEDLEY_UNLIKELY(not unexpect_eof(input_format_t::msgpack, "string")))
        {
            return false;
        }

        switch (current)
        {
            // fixstr
            case 0xA0:
            case 0xA1:
            case 0xA2:
            case 0xA3:
            case 0xA4:
            case 0xA5:
            case 0xA6:
            case 0xA7:
            case 0xA8:
            case 0xA9:
            case 0xAA:
            case 0xAB:
            case 0xAC:
            case 0xAD:
            case 0xAE:
            case 0xAF:
            case 0xB0:
            case 0xB1:
            case 0xB2:
            case 0xB3:
            case 0xB4:
            case 0xB5:
            case 0xB6:
            case 0xB7:
            case 0xB8:
            case 0xB9:
            case 0xBA:
            case 0xBB:
            case 0xBC:
            case 0xBD:
            case 0xBE:
            case 0xBF:
            {
                return get_string(input_format_t::msgpack, static_cast<unsigned int>(current) & 0x1Fu, result);
            }

            case 0xD9: // str 8
            {
                std::uint8_t len;
                return get_number(input_format_t::msgpack, len) and get_string(input_format_t::msgpack, len, result);
            }

            case 0xDA: // str 16
            {
                std::uint16_t len;
                return get_number(input_format_t::msgpack, len) and get_string(input_format_t::msgpack, len, result);
            }

            case 0xDB: // str 32
            {
                std::uint32_t len;
                return get_number(input_format_t::msgpack, len) and get_string(input_format_t::msgpack, len, result);
            }

            default:
            {
                auto last_token = get_token_string();
                return sax->parse_error(chars_read, last_token, parse_error::create(113, chars_read, exception_message(input_format_t::msgpack, "expected length specification (0xA0-0xBF, 0xD9-0xDB); last byte: 0x" + last_token, "string")));
            }
        }
    }

    /*!
    @param[in] len  the length of the array
    @return whether array creation completed
    */
    bool get_msgpack_array(const std::size_t len)
    {
        if (JSON_HEDLEY_UNLIKELY(not sax->start_array(len)))
        {
            return false;
        }

        for (std::size_t i = 0; i < len; ++i)
        {
            if (JSON_HEDLEY_UNLIKELY(not parse_msgpack_internal()))
            {
                return false;
            }
        }

        return sax->end_array();
    }

    /*!
    @param[in] len  the length of the object
    @return whether object creation completed
    */
    bool get_msgpack_object(const std::size_t len)
    {
        if (JSON_HEDLEY_UNLIKELY(not sax->start_object(len)))
        {
            return false;
        }

        string_t key;
        for (std::size_t i = 0; i < len; ++i)
        {
            get();
            if (JSON_HEDLEY_UNLIKELY(not get_msgpack_string(key) or not sax->key(key)))
            {
                return false;
            }

            if (JSON_HEDLEY_UNLIKELY(not parse_msgpack_internal()))
            {
                return false;
            }
            key.clear();
        }

        return sax->end_object();
    }

    ////////////
    // UBJSON //
    ////////////

    /*!
    @param[in] get_char  whether a new character should be retrieved from the
                         input (true, default) or whether the last read
                         character should be considered instead

    @return whether a valid UBJSON value was passed to the SAX parser
    */
    bool parse_ubjson_internal(const bool get_char = true)
    {
        return get_ubjson_value(get_char ? get_ignore_noop() : current);
    }

    /*!
    @brief reads a UBJSON string

    This function is either called after reading the 'S' byte explicitly
    indicating a string, or in case of an object key where the 'S' byte can be
    left out.

    @param[out] result   created string
    @param[in] get_char  whether a new character should be retrieved from the
                         input (true, default) or whether the last read
                         character should be considered instead

    @return whether string creation completed
    */
    bool get_ubjson_string(string_t& result, const bool get_char = true)
    {
        if (get_char)
        {
            get();  // TODO(niels): may we ignore N here?
        }

        if (JSON_HEDLEY_UNLIKELY(not unexpect_eof(input_format_t::ubjson, "value")))
        {
            return false;
        }

        switch (current)
        {
            case 'U':
            {
                std::uint8_t len;
                return get_number(input_format_t::ubjson, len) and get_string(input_format_t::ubjson, len, result);
            }

            case 'i':
            {
                std::int8_t len;
                return get_number(input_format_t::ubjson, len) and get_string(input_format_t::ubjson, len, result);
            }

            case 'I':
            {
                std::int16_t len;
                return get_number(input_format_t::ubjson, len) and get_string(input_format_t::ubjson, len, result);
            }

            case 'l':
            {
                std::int32_t len;
                return get_number(input_format_t::ubjson, len) and get_string(input_format_t::ubjson, len, result);
            }

            case 'L':
            {
                std::int64_t len;
                return get_number(input_format_t::ubjson, len) and get_string(input_format_t::ubjson, len, result);
            }

            default:
                auto last_token = get_token_string();
                return sax->parse_error(chars_read, last_token, parse_error::create(113, chars_read, exception_message(input_format_t::ubjson, "expected length type specification (U, i, I, l, L); last byte: 0x" + last_token, "string")));
        }
    }

    /*!
    @param[out] result  determined size
    @return whether size determination completed
    */
    bool get_ubjson_size_value(std::size_t& result)
    {
        switch (get_ignore_noop())
        {
            case 'U':
            {
                std::uint8_t number;
                if (JSON_HEDLEY_UNLIKELY(not get_number(input_format_t::ubjson, number)))
                {
                    return false;
                }
                result = static_cast<std::size_t>(number);
                return true;
            }

            case 'i':
            {
                std::int8_t number;
                if (JSON_HEDLEY_UNLIKELY(not get_number(input_format_t::ubjson, number)))
                {
                    return false;
                }
                result = static_cast<std::size_t>(number);
                return true;
            }

            case 'I':
            {
                std::int16_t number;
                if (JSON_HEDLEY_UNLIKELY(not get_number(input_format_t::ubjson, number)))
                {
                    return false;
                }
                result = static_cast<std::size_t>(number);
                return true;
            }

            case 'l':
            {
                std::int32_t number;
                if (JSON_HEDLEY_UNLIKELY(not get_number(input_format_t::ubjson, number)))
                {
                    return false;
                }
                result = static_cast<std::size_t>(number);
                return true;
            }

            case 'L':
            {
                std::int64_t number;
                if (JSON_HEDLEY_UNLIKELY(not get_number(input_format_t::ubjson, number)))
                {
                    return false;
                }
                result = static_cast<std::size_t>(number);
                return true;
            }

            default:
            {
                auto last_token = get_token_string();
                return sax->parse_error(chars_read, last_token, parse_error::create(113, chars_read, exception_message(input_format_t::ubjson, "expected length type specification (U, i, I, l, L) after '#'; last byte: 0x" + last_token, "size")));
            }
        }
    }

    /*!
    @brief determine the type and size for a container

    In the optimized UBJSON format, a type and a size can be provided to allow
    for a more compact representation.

    @param[out] result  pair of the size and the type

    @return whether pair creation completed
    */
    bool get_ubjson_size_type(std::pair<std::size_t, int>& result)
    {
        result.first = string_t::npos; // size
        result.second = 0; // type

        get_ignore_noop();

        if (current == '$')
        {
            result.second = get();  // must not ignore 'N', because 'N' maybe the type
            if (JSON_HEDLEY_UNLIKELY(not unexpect_eof(input_format_t::ubjson, "type")))
            {
                return false;
            }

            get_ignore_noop();
            if (JSON_HEDLEY_UNLIKELY(current != '#'))
            {
                if (JSON_HEDLEY_UNLIKELY(not unexpect_eof(input_format_t::ubjson, "value")))
                {
                    return false;
                }
                auto last_token = get_token_string();
                return sax->parse_error(chars_read, last_token, parse_error::create(112, chars_read, exception_message(input_format_t::ubjson, "expected '#' after type information; last byte: 0x" + last_token, "size")));
            }

            return get_ubjson_size_value(result.first);
        }

        if (current == '#')
        {
            return get_ubjson_size_value(result.first);
        }

        return true;
    }

    /*!
    @param prefix  the previously read or set type prefix
    @return whether value creation completed
    */
    bool get_ubjson_value(const int prefix)
    {
        switch (prefix)
        {
            case std::char_traits<char>::eof():  // EOF
                return unexpect_eof(input_format_t::ubjson, "value");

            case 'T':  // true
                return sax->boolean(true);
            case 'F':  // false
                return sax->boolean(false);

            case 'Z':  // null
                return sax->null();

            case 'U':
            {
                std::uint8_t number;
                return get_number(input_format_t::ubjson, number) and sax->number_unsigned(number);
            }

            case 'i':
            {
                std::int8_t number;
                return get_number(input_format_t::ubjson, number) and sax->number_integer(number);
            }

            case 'I':
            {
                std::int16_t number;
                return get_number(input_format_t::ubjson, number) and sax->number_integer(number);
            }

            case 'l':
            {
                std::int32_t number;
                return get_number(input_format_t::ubjson, number) and sax->number_integer(number);
            }

            case 'L':
            {
                std::int64_t number;
                return get_number(input_format_t::ubjson, number) and sax->number_integer(number);
            }

            case 'd':
            {
                float number;
                return get_number(input_format_t::ubjson, number) and sax->number_float(static_cast<number_float_t>(number), "");
            }

            case 'D':
            {
                double number;
                return get_number(input_format_t::ubjson, number) and sax->number_float(static_cast<number_float_t>(number), "");
            }

            case 'C':  // char
            {
                get();
                if (JSON_HEDLEY_UNLIKELY(not unexpect_eof(input_format_t::ubjson, "char")))
                {
                    return false;
                }
                if (JSON_HEDLEY_UNLIKELY(current > 127))
                {
                    auto last_token = get_token_string();
                    return sax->parse_error(chars_read, last_token, parse_error::create(113, chars_read, exception_message(input_format_t::ubjson, "byte after 'C' must be in range 0x00..0x7F; last byte: 0x" + last_token, "char")));
                }
                string_t s(1, static_cast<char>(current));
                return sax->string(s);
            }

            case 'S':  // string
            {
                string_t s;
                return get_ubjson_string(s) and sax->string(s);
            }

            case '[':  // array
                return get_ubjson_array();

            case '{':  // object
                return get_ubjson_object();

            default: // anything else
            {
                auto last_token = get_token_string();
                return sax->parse_error(chars_read, last_token, parse_error::create(112, chars_read, exception_message(input_format_t::ubjson, "invalid byte: 0x" + last_token, "value")));
            }
        }
    }

    /*!
    @return whether array creation completed
    */
    bool get_ubjson_array()
    {
        std::pair<std::size_t, int> size_and_type;
        if (JSON_HEDLEY_UNLIKELY(not get_ubjson_size_type(size_and_type)))
        {
            return false;
        }

        if (size_and_type.first != string_t::npos)
        {
            if (JSON_HEDLEY_UNLIKELY(not sax->start_array(size_and_type.first)))
            {
                return false;
            }

            if (size_and_type.second != 0)
            {
                if (size_and_type.second != 'N')
                {
                    for (std::size_t i = 0; i < size_and_type.first; ++i)
                    {
                        if (JSON_HEDLEY_UNLIKELY(not get_ubjson_value(size_and_type.second)))
                        {
                            return false;
                        }
                    }
                }
            }
            else
            {
                for (std::size_t i = 0; i < size_and_type.first; ++i)
                {
                    if (JSON_HEDLEY_UNLIKELY(not parse_ubjson_internal()))
                    {
                        return false;
                    }
                }
            }
        }
        else
        {
            if (JSON_HEDLEY_UNLIKELY(not sax->start_array(std::size_t(-1))))
            {
                return false;
            }

            while (current != ']')
            {
                if (JSON_HEDLEY_UNLIKELY(not parse_ubjson_internal(false)))
                {
                    return false;
                }
                get_ignore_noop();
            }
        }

        return sax->end_array();
    }

    /*!
    @return whether object creation completed
    */
    bool get_ubjson_object()
    {
        std::pair<std::size_t, int> size_and_type;
        if (JSON_HEDLEY_UNLIKELY(not get_ubjson_size_type(size_and_type)))
        {
            return false;
        }

        string_t key;
        if (size_and_type.first != string_t::npos)
        {
            if (JSON_HEDLEY_UNLIKELY(not sax->start_object(size_and_type.first)))
            {
                return false;
            }

            if (size_and_type.second != 0)
            {
                for (std::size_t i = 0; i < size_and_type.first; ++i)
                {
                    if (JSON_HEDLEY_UNLIKELY(not get_ubjson_string(key) or not sax->key(key)))
                    {
                        return false;
                    }
                    if (JSON_HEDLEY_UNLIKELY(not get_ubjson_value(size_and_type.second)))
                    {
                        return false;
                    }
                    key.clear();
                }
            }
            else
            {
                for (std::size_t i = 0; i < size_and_type.first; ++i)
                {
                    if (JSON_HEDLEY_UNLIKELY(not get_ubjson_string(key) or not sax->key(key)))
                    {
                        return false;
                    }
                    if (JSON_HEDLEY_UNLIKELY(not parse_ubjson_internal()))
                    {
                        return false;
                    }
                    key.clear();
                }
            }
        }
        else
        {
            if (JSON_HEDLEY_UNLIKELY(not sax->start_object(std::size_t(-1))))
            {
                return false;
            }

            while (current != '}')
            {
                if (JSON_HEDLEY_UNLIKELY(not get_ubjson_string(key, false) or not sax->key(key)))
                {
                    return false;
                }
                if (JSON_HEDLEY_UNLIKELY(not parse_ubjson_internal()))
                {
                    return false;
                }
                get_ignore_noop();
                key.clear();
            }
        }

        return sax->end_object();
    }

    ///////////////////////
    // Utility functions //
    ///////////////////////

    /*!
    @brief get next character from the input

    This function provides the interface to the used input adapter. It does
    not throw in case the input reached EOF, but returns a -'ve valued
    `std::char_traits<char>::eof()` in that case.

    @return character read from the input
    */
    int get()
    {
        ++chars_read;
        return current = ia->get_character();
    }

    /*!
    @return character read from the input after ignoring all 'N' entries
    */
    int get_ignore_noop()
    {
        do
        {
            get();
        }
        while (current == 'N');

        return current;
    }

    /*
    @brief read a number from the input

    @tparam NumberType the type of the number
    @param[in] format   the current format (for diagnostics)
    @param[out] result  number of type @a NumberType

    @return whether conversion completed

    @note This function needs to respect the system's endianess, because
          bytes in CBOR, MessagePack, and UBJSON are stored in network order
          (big endian) and therefore need reordering on little endian systems.
    */
    template<typename NumberType, bool InputIsLittleEndian = false>
    bool get_number(const input_format_t format, NumberType& result)
    {
        // step 1: read input into array with system's byte order
        std::array<std::uint8_t, sizeof(NumberType)> vec;
        for (std::size_t i = 0; i < sizeof(NumberType); ++i)
        {
            get();
            if (JSON_HEDLEY_UNLIKELY(not unexpect_eof(format, "number")))
            {
                return false;
            }

            // reverse byte order prior to conversion if necessary
            if (is_little_endian != InputIsLittleEndian)
            {
                vec[sizeof(NumberType) - i - 1] = static_cast<std::uint8_t>(current);
            }
            else
            {
                vec[i] = static_cast<std::uint8_t>(current); // LCOV_EXCL_LINE
            }
        }

        // step 2: convert array into number of type T and return
        std::memcpy(&result, vec.data(), sizeof(NumberType));
        return true;
    }

    /*!
    @brief create a string by reading characters from the input

    @tparam NumberType the type of the number
    @param[in] format the current format (for diagnostics)
    @param[in] len number of characters to read
    @param[out] result string created by reading @a len bytes

    @return whether string creation completed

    @note We can not reserve @a len bytes for the result, because @a len
          may be too large. Usually, @ref unexpect_eof() detects the end of
          the input before we run out of string memory.
    */
    template<typename NumberType>
    bool get_string(const input_format_t format,
                    const NumberType len,
                    string_t& result)
    {
        bool success = true;
        std::generate_n(std::back_inserter(result), len, [this, &success, &format]()
        {
            get();
            if (JSON_HEDLEY_UNLIKELY(not unexpect_eof(format, "string")))
            {
                success = false;
            }
            return static_cast<char>(current);
        });
        return success;
    }

    /*!
    @param[in] format   the current format (for diagnostics)
    @param[in] context  further context information (for diagnostics)
    @return whether the last read character is not EOF
    */
    JSON_HEDLEY_NON_NULL(3)
    bool unexpect_eof(const input_format_t format, const char* context) const
    {
        if (JSON_HEDLEY_UNLIKELY(current == std::char_traits<char>::eof()))
        {
            return sax->parse_error(chars_read, "<end of file>",
                                    parse_error::create(110, chars_read, exception_message(format, "unexpected end of input", context)));
        }
        return true;
    }

    /*!
    @return a string representation of the last read byte
    */
    std::string get_token_string() const
    {
        std::array<char, 3> cr{{}};
        (std::snprintf)(cr.data(), cr.size(), "%.2hhX", static_cast<unsigned char>(current));
        return std::string{cr.data()};
    }

    /*!
    @param[in] format   the current format
    @param[in] detail   a detailed error message
    @param[in] context  further context information
    @return a message string to use in the parse_error exceptions
    */
    std::string exception_message(const input_format_t format,
                                  const std::string& detail,
                                  const std::string& context) const
    {
        std::string error_msg = "syntax error while parsing ";

        switch (format)
        {
            case input_format_t::cbor:
                error_msg += "CBOR";
                break;

            case input_format_t::msgpack:
                error_msg += "MessagePack";
                break;

            case input_format_t::ubjson:
                error_msg += "UBJSON";
                break;

            case input_format_t::bson:
                error_msg += "BSON";
                break;

            default:            // LCOV_EXCL_LINE
                assert(false);  // LCOV_EXCL_LINE
        }

        return error_msg + " " + context + ": " + detail;
    }

  private:
    /// input adapter
    input_adapter_t ia = nullptr;

    /// the current character
    int current = std::char_traits<char>::eof();

    /// the number of characters read
    std::size_t chars_read = 0;

    /// whether we can assume little endianess
    const bool is_little_endian = little_endianess();

    /// the SAX parser
    json_sax_t* sax = nullptr;
};
}  // namespace detail
}  // namespace nlohmann

// #include <nlohmann/detail/input/input_adapters.hpp>

// #include <nlohmann/detail/input/lexer.hpp>


#include <array> // array
#include <clocale> // localeconv
#include <cstddef> // size_t
#include <cstdio> // snprintf
#include <cstdlib> // strtof, strtod, strtold, strtoll, strtoull
#include <initializer_list> // initializer_list
#include <string> // char_traits, string
#include <utility> // move
#include <vector> // vector

// #include <nlohmann/detail/input/input_adapters.hpp>

// #include <nlohmann/detail/input/position_t.hpp>

// #include <nlohmann/detail/macro_scope.hpp>


namespace nlohmann
{
namespace detail
{
///////////
// lexer //
///////////

/*!
@brief lexical analysis

This class organizes the lexical analysis during JSON deserialization.
*/
template<typename BasicJsonType>
class lexer
{
    using number_integer_t = typename BasicJsonType::number_integer_t;
    using number_unsigned_t = typename BasicJsonType::number_unsigned_t;
    using number_float_t = typename BasicJsonType::number_float_t;
    using string_t = typename BasicJsonType::string_t;

  public:
    /// token types for the parser
    enum class token_type
    {
        uninitialized,    ///< indicating the scanner is uninitialized
        literal_true,     ///< the `true` literal
        literal_false,    ///< the `false` literal
        literal_null,     ///< the `null` literal
        value_string,     ///< a string -- use get_string() for actual value
        value_unsigned,   ///< an unsigned integer -- use get_number_unsigned() for actual value
        value_integer,    ///< a signed integer -- use get_number_integer() for actual value
        value_float,      ///< an floating point number -- use get_number_float() for actual value
        begin_array,      ///< the character for array begin `[`
        begin_object,     ///< the character for object begin `{`
        end_array,        ///< the character for array end `]`
        end_object,       ///< the character for object end `}`
        name_separator,   ///< the name separator `:`
        value_separator,  ///< the value separator `,`
        parse_error,      ///< indicating a parse error
        end_of_input,     ///< indicating the end of the input buffer
        literal_or_value  ///< a literal or the begin of a value (only for diagnostics)
    };

    /// return name of values of type token_type (only used for errors)
    JSON_HEDLEY_RETURNS_NON_NULL
    JSON_HEDLEY_CONST
    static const char* token_type_name(const token_type t) noexcept
    {
        switch (t)
        {
            case token_type::uninitialized:
                return "<uninitialized>";
            case token_type::literal_true:
                return "true literal";
            case token_type::literal_false:
                return "false literal";
            case token_type::literal_null:
                return "null literal";
            case token_type::value_string:
                return "string literal";
            case lexer::token_type::value_unsigned:
            case lexer::token_type::value_integer:
            case lexer::token_type::value_float:
                return "number literal";
            case token_type::begin_array:
                return "'['";
            case token_type::begin_object:
                return "'{'";
            case token_type::end_array:
                return "']'";
            case token_type::end_object:
                return "'}'";
            case token_type::name_separator:
                return "':'";
            case token_type::value_separator:
                return "','";
            case token_type::parse_error:
                return "<parse error>";
            case token_type::end_of_input:
                return "end of input";
            case token_type::literal_or_value:
                return "'[', '{', or a literal";
            // LCOV_EXCL_START
            default: // catch non-enum values
                return "unknown token";
                // LCOV_EXCL_STOP
        }
    }

    explicit lexer(detail::input_adapter_t&& adapter)
        : ia(std::move(adapter)), decimal_point_char(get_decimal_point()) {}

    // delete because of pointer members
    lexer(const lexer&) = delete;
    lexer(lexer&&) = delete;
    lexer& operator=(lexer&) = delete;
    lexer& operator=(lexer&&) = delete;
    ~lexer() = default;

  private:
    /////////////////////
    // locales
    /////////////////////

    /// return the locale-dependent decimal point
    JSON_HEDLEY_PURE
    static char get_decimal_point() noexcept
    {
        const auto loc = localeconv();
        assert(loc != nullptr);
        return (loc->decimal_point == nullptr) ? '.' : *(loc->decimal_point);
    }

    /////////////////////
    // scan functions
    /////////////////////

    /*!
    @brief get codepoint from 4 hex characters following `\u`

    For input "\u c1 c2 c3 c4" the codepoint is:
      (c1 * 0x1000) + (c2 * 0x0100) + (c3 * 0x0010) + c4
    = (c1 << 12) + (c2 << 8) + (c3 << 4) + (c4 << 0)

    Furthermore, the possible characters '0'..'9', 'A'..'F', and 'a'..'f'
    must be converted to the integers 0x0..0x9, 0xA..0xF, 0xA..0xF, resp. The
    conversion is done by subtracting the offset (0x30, 0x37, and 0x57)
    between the ASCII value of the character and the desired integer value.

    @return codepoint (0x0000..0xFFFF) or -1 in case of an error (e.g. EOF or
            non-hex character)
    */
    int get_codepoint()
    {
        // this function only makes sense after reading `\u`
        assert(current == 'u');
        int codepoint = 0;

        const auto factors = { 12u, 8u, 4u, 0u };
        for (const auto factor : factors)
        {
            get();

            if (current >= '0' and current <= '9')
            {
                codepoint += static_cast<int>((static_cast<unsigned int>(current) - 0x30u) << factor);
            }
            else if (current >= 'A' and current <= 'F')
            {
                codepoint += static_cast<int>((static_cast<unsigned int>(current) - 0x37u) << factor);
            }
            else if (current >= 'a' and current <= 'f')
            {
                codepoint += static_cast<int>((static_cast<unsigned int>(current) - 0x57u) << factor);
            }
            else
            {
                return -1;
            }
        }

        assert(0x0000 <= codepoint and codepoint <= 0xFFFF);
        return codepoint;
    }

    /*!
    @brief check if the next byte(s) are inside a given range

    Adds the current byte and, for each passed range, reads a new byte and
    checks if it is inside the range. If a violation was detected, set up an
    error message and return false. Otherwise, return true.

    @param[in] ranges  list of integers; interpreted as list of pairs of
                       inclusive lower and upper bound, respectively

    @pre The passed list @a ranges must have 2, 4, or 6 elements; that is,
         1, 2, or 3 pairs. This precondition is enforced by an assertion.

    @return true if and only if no range violation was detected
    */
    bool next_byte_in_range(std::initializer_list<int> ranges)
    {
        assert(ranges.size() == 2 or ranges.size() == 4 or ranges.size() == 6);
        add(current);

        for (auto range = ranges.begin(); range != ranges.end(); ++range)
        {
            get();
            if (JSON_HEDLEY_LIKELY(*range <= current and current <= *(++range)))
            {
                add(current);
            }
            else
            {
                error_message = "invalid string: ill-formed UTF-8 byte";
                return false;
            }
        }

        return true;
    }

    /*!
    @brief scan a string literal

    This function scans a string according to Sect. 7 of RFC 7159. While
    scanning, bytes are escaped and copied into buffer token_buffer. Then the
    function returns successfully, token_buffer is *not* null-terminated (as it
    may contain \0 bytes), and token_buffer.size() is the number of bytes in the
    string.

    @return token_type::value_string if string could be successfully scanned,
            token_type::parse_error otherwise

    @note In case of errors, variable error_message contains a textual
          description.
    */
    token_type scan_string()
    {
        // reset token_buffer (ignore opening quote)
        reset();

        // we entered the function by reading an open quote
        assert(current == '\"');

        while (true)
        {
            // get next character
            switch (get())
            {
                // end of file while parsing string
                case std::char_traits<char>::eof():
                {
                    error_message = "invalid string: missing closing quote";
                    return token_type::parse_error;
                }

                // closing quote
                case '\"':
                {
                    return token_type::value_string;
                }

                // escapes
                case '\\':
                {
                    switch (get())
                    {
                        // quotation mark
                        case '\"':
                            add('\"');
                            break;
                        // reverse solidus
                        case '\\':
                            add('\\');
                            break;
                        // solidus
                        case '/':
                            add('/');
                            break;
                        // backspace
                        case 'b':
                            add('\b');
                            break;
                        // form feed
                        case 'f':
                            add('\f');
                            break;
                        // line feed
                        case 'n':
                            add('\n');
                            break;
                        // carriage return
                        case 'r':
                            add('\r');
                            break;
                        // tab
                        case 't':
                            add('\t');
                            break;

                        // unicode escapes
                        case 'u':
                        {
                            const int codepoint1 = get_codepoint();
                            int codepoint = codepoint1; // start with codepoint1

                            if (JSON_HEDLEY_UNLIKELY(codepoint1 == -1))
                            {
                                error_message = "invalid string: '\\u' must be followed by 4 hex digits";
                                return token_type::parse_error;
                            }

                            // check if code point is a high surrogate
                            if (0xD800 <= codepoint1 and codepoint1 <= 0xDBFF)
                            {
                                // expect next \uxxxx entry
                                if (JSON_HEDLEY_LIKELY(get() == '\\' and get() == 'u'))
                                {
                                    const int codepoint2 = get_codepoint();

                                    if (JSON_HEDLEY_UNLIKELY(codepoint2 == -1))
                                    {
                                        error_message = "invalid string: '\\u' must be followed by 4 hex digits";
                                        return token_type::parse_error;
                                    }

                                    // check if codepoint2 is a low surrogate
                                    if (JSON_HEDLEY_LIKELY(0xDC00 <= codepoint2 and codepoint2 <= 0xDFFF))
                                    {
                                        // overwrite codepoint
                                        codepoint = static_cast<int>(
                                                        // high surrogate occupies the most significant 22 bits
                                                        (static_cast<unsigned int>(codepoint1) << 10u)
                                                        // low surrogate occupies the least significant 15 bits
                                                        + static_cast<unsigned int>(codepoint2)
                                                        // there is still the 0xD800, 0xDC00 and 0x10000 noise
                                                        // in the result so we have to subtract with:
                                                        // (0xD800 << 10) + DC00 - 0x10000 = 0x35FDC00
                                                        - 0x35FDC00u);
                                    }
                                    else
                                    {
                                        error_message = "invalid string: surrogate U+DC00..U+DFFF must be followed by U+DC00..U+DFFF";
                                        return token_type::parse_error;
                                    }
                                }
                                else
                                {
                                    error_message = "invalid string: surrogate U+DC00..U+DFFF must be followed by U+DC00..U+DFFF";
                                    return token_type::parse_error;
                                }
                            }
                            else
                            {
                                if (JSON_HEDLEY_UNLIKELY(0xDC00 <= codepoint1 and codepoint1 <= 0xDFFF))
                                {
                                    error_message = "invalid string: surrogate U+DC00..U+DFFF must follow U+D800..U+DBFF";
                                    return token_type::parse_error;
                                }
                            }

                            // result of the above calculation yields a proper codepoint
                            assert(0x00 <= codepoint and codepoint <= 0x10FFFF);

                            // translate codepoint into bytes
                            if (codepoint < 0x80)
                            {
                                // 1-byte characters: 0xxxxxxx (ASCII)
                                add(codepoint);
                            }
                            else if (codepoint <= 0x7FF)
                            {
                                // 2-byte characters: 110xxxxx 10xxxxxx
                                add(static_cast<int>(0xC0u | (static_cast<unsigned int>(codepoint) >> 6u)));
                                add(static_cast<int>(0x80u | (static_cast<unsigned int>(codepoint) & 0x3Fu)));
                            }
                            else if (codepoint <= 0xFFFF)
                            {
                                // 3-byte characters: 1110xxxx 10xxxxxx 10xxxxxx
                                add(static_cast<int>(0xE0u | (static_cast<unsigned int>(codepoint) >> 12u)));
                                add(static_cast<int>(0x80u | ((static_cast<unsigned int>(codepoint) >> 6u) & 0x3Fu)));
                                add(static_cast<int>(0x80u | (static_cast<unsigned int>(codepoint) & 0x3Fu)));
                            }
                            else
                            {
                                // 4-byte characters: 11110xxx 10xxxxxx 10xxxxxx 10xxxxxx
                                add(static_cast<int>(0xF0u | (static_cast<unsigned int>(codepoint) >> 18u)));
                                add(static_cast<int>(0x80u | ((static_cast<unsigned int>(codepoint) >> 12u) & 0x3Fu)));
                                add(static_cast<int>(0x80u | ((static_cast<unsigned int>(codepoint) >> 6u) & 0x3Fu)));
                                add(static_cast<int>(0x80u | (static_cast<unsigned int>(codepoint) & 0x3Fu)));
                            }

                            break;
                        }

                        // other characters after escape
                        default:
                            error_message = "invalid string: forbidden character after backslash";
                            return token_type::parse_error;
                    }

                    break;
                }

                // invalid control characters
                case 0x00:
                {
                    error_message = "invalid string: control character U+0000 (NUL) must be escaped to \\u0000";
                    return token_type::parse_error;
                }

                case 0x01:
                {
                    error_message = "invalid string: control character U+0001 (SOH) must be escaped to \\u0001";
                    return token_type::parse_error;
                }

                case 0x02:
                {
                    error_message = "invalid string: control character U+0002 (STX) must be escaped to \\u0002";
                    return token_type::parse_error;
                }

                case 0x03:
                {
                    error_message = "invalid string: control character U+0003 (ETX) must be escaped to \\u0003";
                    return token_type::parse_error;
                }

                case 0x04:
                {
                    error_message = "invalid string: control character U+0004 (EOT) must be escaped to \\u0004";
                    return token_type::parse_error;
                }

                case 0x05:
                {
                    error_message = "invalid string: control character U+0005 (ENQ) must be escaped to \\u0005";
                    return token_type::parse_error;
                }

                case 0x06:
                {
                    error_message = "invalid string: control character U+0006 (ACK) must be escaped to \\u0006";
                    return token_type::parse_error;
                }

                case 0x07:
                {
                    error_message = "invalid string: control character U+0007 (BEL) must be escaped to \\u0007";
                    return token_type::parse_error;
                }

                case 0x08:
                {
                    error_message = "invalid string: control character U+0008 (BS) must be escaped to \\u0008 or \\b";
                    return token_type::parse_error;
                }

                case 0x09:
                {
                    error_message = "invalid string: control character U+0009 (HT) must be escaped to \\u0009 or \\t";
                    return token_type::parse_error;
                }

                case 0x0A:
                {
                    error_message = "invalid string: control character U+000A (LF) must be escaped to \\u000A or \\n";
                    return token_type::parse_error;
                }

                case 0x0B:
                {
                    error_message = "invalid string: control character U+000B (VT) must be escaped to \\u000B";
                    return token_type::parse_error;
                }

                case 0x0C:
                {
                    error_message = "invalid string: control character U+000C (FF) must be escaped to \\u000C or \\f";
                    return token_type::parse_error;
                }

                case 0x0D:
                {
                    error_message = "invalid string: control character U+000D (CR) must be escaped to \\u000D or \\r";
                    return token_type::parse_error;
                }

                case 0x0E:
                {
                    error_message = "invalid string: control character U+000E (SO) must be escaped to \\u000E";
                    return token_type::parse_error;
                }

                case 0x0F:
                {
                    error_message = "invalid string: control character U+000F (SI) must be escaped to \\u000F";
                    return token_type::parse_error;
                }

                case 0x10:
                {
                    error_message = "invalid string: control character U+0010 (DLE) must be escaped to \\u0010";
                    return token_type::parse_error;
                }

                case 0x11:
                {
                    error_message = "invalid string: control character U+0011 (DC1) must be escaped to \\u0011";
                    return token_type::parse_error;
                }

                case 0x12:
                {
                    error_message = "invalid string: control character U+0012 (DC2) must be escaped to \\u0012";
                    return token_type::parse_error;
                }

                case 0x13:
                {
                    error_message = "invalid string: control character U+0013 (DC3) must be escaped to \\u0013";
                    return token_type::parse_error;
                }

                case 0x14:
                {
                    error_message = "invalid string: control character U+0014 (DC4) must be escaped to \\u0014";
                    return token_type::parse_error;
                }

                case 0x15:
                {
                    error_message = "invalid string: control character U+0015 (NAK) must be escaped to \\u0015";
                    return token_type::parse_error;
                }

                case 0x16:
                {
                    error_message = "invalid string: control character U+0016 (SYN) must be escaped to \\u0016";
                    return token_type::parse_error;
                }

                case 0x17:
                {
                    error_message = "invalid string: control character U+0017 (ETB) must be escaped to \\u0017";
                    return token_type::parse_error;
                }

                case 0x18:
                {
                    error_message = "invalid string: control character U+0018 (CAN) must be escaped to \\u0018";
                    return token_type::parse_error;
                }

                case 0x19:
                {
                    error_message = "invalid string: control character U+0019 (EM) must be escaped to \\u0019";
                    return token_type::parse_error;
                }

                case 0x1A:
                {
                    error_message = "invalid string: control character U+001A (SUB) must be escaped to \\u001A";
                    return token_type::parse_error;
                }

                case 0x1B:
                {
                    error_message = "invalid string: control character U+001B (ESC) must be escaped to \\u001B";
                    return token_type::parse_error;
                }

                case 0x1C:
                {
                    error_message = "invalid string: control character U+001C (FS) must be escaped to \\u001C";
                    return token_type::parse_error;
                }

                case 0x1D:
                {
                    error_message = "invalid string: control character U+001D (GS) must be escaped to \\u001D";
                    return token_type::parse_error;
                }

                case 0x1E:
                {
                    error_message = "invalid string: control character U+001E (RS) must be escaped to \\u001E";
                    return token_type::parse_error;
                }

                case 0x1F:
                {
                    error_message = "invalid string: control character U+001F (US) must be escaped to \\u001F";
                    return token_type::parse_error;
                }

                // U+0020..U+007F (except U+0022 (quote) and U+005C (backspace))
                case 0x20:
                case 0x21:
                case 0x23:
                case 0x24:
                case 0x25:
                case 0x26:
                case 0x27:
                case 0x28:
                case 0x29:
                case 0x2A:
                case 0x2B:
                case 0x2C:
                case 0x2D:
                case 0x2E:
                case 0x2F:
                case 0x30:
                case 0x31:
                case 0x32:
                case 0x33:
                case 0x34:
                case 0x35:
                case 0x36:
                case 0x37:
                case 0x38:
                case 0x39:
                case 0x3A:
                case 0x3B:
                case 0x3C:
                case 0x3D:
                case 0x3E:
                case 0x3F:
                case 0x40:
                case 0x41:
                case 0x42:
                case 0x43:
                case 0x44:
                case 0x45:
                case 0x46:
                case 0x47:
                case 0x48:
                case 0x49:
                case 0x4A:
                case 0x4B:
                case 0x4C:
                case 0x4D:
                case 0x4E:
                case 0x4F:
                case 0x50:
                case 0x51:
                case 0x52:
                case 0x53:
                case 0x54:
                case 0x55:
                case 0x56:
                case 0x57:
                case 0x58:
                case 0x59:
                case 0x5A:
                case 0x5B:
                case 0x5D:
                case 0x5E:
                case 0x5F:
                case 0x60:
                case 0x61:
                case 0x62:
                case 0x63:
                case 0x64:
                case 0x65:
                case 0x66:
                case 0x67:
                case 0x68:
                case 0x69:
                case 0x6A:
                case 0x6B:
                case 0x6C:
                case 0x6D:
                case 0x6E:
                case 0x6F:
                case 0x70:
                case 0x71:
                case 0x72:
                case 0x73:
                case 0x74:
                case 0x75:
                case 0x76:
                case 0x77:
                case 0x78:
                case 0x79:
                case 0x7A:
                case 0x7B:
                case 0x7C:
                case 0x7D:
                case 0x7E:
                case 0x7F:
                {
                    add(current);
                    break;
                }

                // U+0080..U+07FF: bytes C2..DF 80..BF
                case 0xC2:
                case 0xC3:
                case 0xC4:
                case 0xC5:
                case 0xC6:
                case 0xC7:
                case 0xC8:
                case 0xC9:
                case 0xCA:
                case 0xCB:
                case 0xCC:
                case 0xCD:
                case 0xCE:
                case 0xCF:
                case 0xD0:
                case 0xD1:
                case 0xD2:
                case 0xD3:
                case 0xD4:
                case 0xD5:
                case 0xD6:
                case 0xD7:
                case 0xD8:
                case 0xD9:
                case 0xDA:
                case 0xDB:
                case 0xDC:
                case 0xDD:
                case 0xDE:
                case 0xDF:
                {
                    if (JSON_HEDLEY_UNLIKELY(not next_byte_in_range({0x80, 0xBF})))
                    {
                        return token_type::parse_error;
                    }
                    break;
                }

                // U+0800..U+0FFF: bytes E0 A0..BF 80..BF
                case 0xE0:
                {
                    if (JSON_HEDLEY_UNLIKELY(not (next_byte_in_range({0xA0, 0xBF, 0x80, 0xBF}))))
                    {
                        return token_type::parse_error;
                    }
                    break;
                }

                // U+1000..U+CFFF: bytes E1..EC 80..BF 80..BF
                // U+E000..U+FFFF: bytes EE..EF 80..BF 80..BF
                case 0xE1:
                case 0xE2:
                case 0xE3:
                case 0xE4:
                case 0xE5:
                case 0xE6:
                case 0xE7:
                case 0xE8:
                case 0xE9:
                case 0xEA:
                case 0xEB:
                case 0xEC:
                case 0xEE:
                case 0xEF:
                {
                    if (JSON_HEDLEY_UNLIKELY(not (next_byte_in_range({0x80, 0xBF, 0x80, 0xBF}))))
                    {
                        return token_type::parse_error;
                    }
                    break;
                }

                // U+D000..U+D7FF: bytes ED 80..9F 80..BF
                case 0xED:
                {
                    if (JSON_HEDLEY_UNLIKELY(not (next_byte_in_range({0x80, 0x9F, 0x80, 0xBF}))))
                    {
                        return token_type::parse_error;
                    }
                    break;
                }

                // U+10000..U+3FFFF F0 90..BF 80..BF 80..BF
                case 0xF0:
                {
                    if (JSON_HEDLEY_UNLIKELY(not (next_byte_in_range({0x90, 0xBF, 0x80, 0xBF, 0x80, 0xBF}))))
                    {
                        return token_type::parse_error;
                    }
                    break;
                }

                // U+40000..U+FFFFF F1..F3 80..BF 80..BF 80..BF
                case 0xF1:
                case 0xF2:
                case 0xF3:
                {
                    if (JSON_HEDLEY_UNLIKELY(not (next_byte_in_range({0x80, 0xBF, 0x80, 0xBF, 0x80, 0xBF}))))
                    {
                        return token_type::parse_error;
                    }
                    break;
                }

                // U+100000..U+10FFFF F4 80..8F 80..BF 80..BF
                case 0xF4:
                {
                    if (JSON_HEDLEY_UNLIKELY(not (next_byte_in_range({0x80, 0x8F, 0x80, 0xBF, 0x80, 0xBF}))))
                    {
                        return token_type::parse_error;
                    }
                    break;
                }

                // remaining bytes (80..C1 and F5..FF) are ill-formed
                default:
                {
                    error_message = "invalid string: ill-formed UTF-8 byte";
                    return token_type::parse_error;
                }
            }
        }
    }

    JSON_HEDLEY_NON_NULL(2)
    static void strtof(float& f, const char* str, char** endptr) noexcept
    {
        f = std::strtof(str, endptr);
    }

    JSON_HEDLEY_NON_NULL(2)
    static void strtof(double& f, const char* str, char** endptr) noexcept
    {
        f = std::strtod(str, endptr);
    }

    JSON_HEDLEY_NON_NULL(2)
    static void strtof(long double& f, const char* str, char** endptr) noexcept
    {
        f = std::strtold(str, endptr);
    }

    /*!
    @brief scan a number literal

    This function scans a string according to Sect. 6 of RFC 7159.

    The function is realized with a deterministic finite state machine derived
    from the grammar described in RFC 7159. Starting in state "init", the
    input is read and used to determined the next state. Only state "done"
    accepts the number. State "error" is a trap state to model errors. In the
    table below, "anything" means any character but the ones listed before.

    state    | 0        | 1-9      | e E      | +       | -       | .        | anything
    ---------|----------|----------|----------|---------|---------|----------|-----------
    init     | zero     | any1     | [error]  | [error] | minus   | [error]  | [error]
    minus    | zero     | any1     | [error]  | [error] | [error] | [error]  | [error]
    zero     | done     | done     | exponent | done    | done    | decimal1 | done
    any1     | any1     | any1     | exponent | done    | done    | decimal1 | done
    decimal1 | decimal2 | [error]  | [error]  | [error] | [error] | [error]  | [error]
    decimal2 | decimal2 | decimal2 | exponent | done    | done    | done     | done
    exponent | any2     | any2     | [error]  | sign    | sign    | [error]  | [error]
    sign     | any2     | any2     | [error]  | [error] | [error] | [error]  | [error]
    any2     | any2     | any2     | done     | done    | done    | done     | done

    The state machine is realized with one label per state (prefixed with
    "scan_number_") and `goto` statements between them. The state machine
    contains cycles, but any cycle can be left when EOF is read. Therefore,
    the function is guaranteed to terminate.

    During scanning, the read bytes are stored in token_buffer. This string is
    then converted to a signed integer, an unsigned integer, or a
    floating-point number.

    @return token_type::value_unsigned, token_type::value_integer, or
            token_type::value_float if number could be successfully scanned,
            token_type::parse_error otherwise

    @note The scanner is independent of the current locale. Internally, the
          locale's decimal point is used instead of `.` to work with the
          locale-dependent converters.
    */
    token_type scan_number()  // lgtm [cpp/use-of-goto]
    {
        // reset token_buffer to store the number's bytes
        reset();

        // the type of the parsed number; initially set to unsigned; will be
        // changed if minus sign, decimal point or exponent is read
        token_type number_type = token_type::value_unsigned;

        // state (init): we just found out we need to scan a number
        switch (current)
        {
            case '-':
            {
                add(current);
                goto scan_number_minus;
            }

            case '0':
            {
                add(current);
                goto scan_number_zero;
            }

            case '1':
            case '2':
            case '3':
            case '4':
            case '5':
            case '6':
            case '7':
            case '8':
            case '9':
            {
                add(current);
                goto scan_number_any1;
            }

            // all other characters are rejected outside scan_number()
            default:            // LCOV_EXCL_LINE
                assert(false);  // LCOV_EXCL_LINE
        }

scan_number_minus:
        // state: we just parsed a leading minus sign
        number_type = token_type::value_integer;
        switch (get())
        {
            case '0':
            {
                add(current);
                goto scan_number_zero;
            }

            case '1':
            case '2':
            case '3':
            case '4':
            case '5':
            case '6':
            case '7':
            case '8':
            case '9':
            {
                add(current);
                goto scan_number_any1;
            }

            default:
            {
                error_message = "invalid number; expected digit after '-'";
                return token_type::parse_error;
            }
        }

scan_number_zero:
        // state: we just parse a zero (maybe with a leading minus sign)
        switch (get())
        {
            case '.':
            {
                add(decimal_point_char);
                goto scan_number_decimal1;
            }

            case 'e':
            case 'E':
            {
                add(current);
                goto scan_number_exponent;
            }

            default:
                goto scan_number_done;
        }

scan_number_any1:
        // state: we just parsed a number 0-9 (maybe with a leading minus sign)
        switch (get())
        {
            case '0':
            case '1':
            case '2':
            case '3':
            case '4':
            case '5':
            case '6':
            case '7':
            case '8':
            case '9':
            {
                add(current);
                goto scan_number_any1;
            }

            case '.':
            {
                add(decimal_point_char);
                goto scan_number_decimal1;
            }

            case 'e':
            case 'E':
            {
                add(current);
                goto scan_number_exponent;
            }

            default:
                goto scan_number_done;
        }

scan_number_decimal1:
        // state: we just parsed a decimal point
        number_type = token_type::value_float;
        switch (get())
        {
            case '0':
            case '1':
            case '2':
            case '3':
            case '4':
            case '5':
            case '6':
            case '7':
            case '8':
            case '9':
            {
                add(current);
                goto scan_number_decimal2;
            }

            default:
            {
                error_message = "invalid number; expected digit after '.'";
                return token_type::parse_error;
            }
        }

scan_number_decimal2:
        // we just parsed at least one number after a decimal point
        switch (get())
        {
            case '0':
            case '1':
            case '2':
            case '3':
            case '4':
            case '5':
            case '6':
            case '7':
            case '8':
            case '9':
            {
                add(current);
                goto scan_number_decimal2;
            }

            case 'e':
            case 'E':
            {
                add(current);
                goto scan_number_exponent;
            }

            default:
                goto scan_number_done;
        }

scan_number_exponent:
        // we just parsed an exponent
        number_type = token_type::value_float;
        switch (get())
        {
            case '+':
            case '-':
            {
                add(current);
                goto scan_number_sign;
            }

            case '0':
            case '1':
            case '2':
            case '3':
            case '4':
            case '5':
            case '6':
            case '7':
            case '8':
            case '9':
            {
                add(current);
                goto scan_number_any2;
            }

            default:
            {
                error_message =
                    "invalid number; expected '+', '-', or digit after exponent";
                return token_type::parse_error;
            }
        }

scan_number_sign:
        // we just parsed an exponent sign
        switch (get())
        {
            case '0':
            case '1':
            case '2':
            case '3':
            case '4':
            case '5':
            case '6':
            case '7':
            case '8':
            case '9':
            {
                add(current);
                goto scan_number_any2;
            }

            default:
            {
                error_message = "invalid number; expected digit after exponent sign";
                return token_type::parse_error;
            }
        }

scan_number_any2:
        // we just parsed a number after the exponent or exponent sign
        switch (get())
        {
            case '0':
            case '1':
            case '2':
            case '3':
            case '4':
            case '5':
            case '6':
            case '7':
            case '8':
            case '9':
            {
                add(current);
                goto scan_number_any2;
            }

            default:
                goto scan_number_done;
        }

scan_number_done:
        // unget the character after the number (we only read it to know that
        // we are done scanning a number)
        unget();

        char* endptr = nullptr;
        errno = 0;

        // try to parse integers first and fall back to floats
        if (number_type == token_type::value_unsigned)
        {
            const auto x = std::strtoull(token_buffer.data(), &endptr, 10);

            // we checked the number format before
            assert(endptr == token_buffer.data() + token_buffer.size());

            if (errno == 0)
            {
                value_unsigned = static_cast<number_unsigned_t>(x);
                if (value_unsigned == x)
                {
                    return token_type::value_unsigned;
                }
            }
        }
        else if (number_type == token_type::value_integer)
        {
            const auto x = std::strtoll(token_buffer.data(), &endptr, 10);

            // we checked the number format before
            assert(endptr == token_buffer.data() + token_buffer.size());

            if (errno == 0)
            {
                value_integer = static_cast<number_integer_t>(x);
                if (value_integer == x)
                {
                    return token_type::value_integer;
                }
            }
        }

        // this code is reached if we parse a floating-point number or if an
        // integer conversion above failed
        strtof(value_float, token_buffer.data(), &endptr);

        // we checked the number format before
        assert(endptr == token_buffer.data() + token_buffer.size());

        return token_type::value_float;
    }

    /*!
    @param[in] literal_text  the literal text to expect
    @param[in] length        the length of the passed literal text
    @param[in] return_type   the token type to return on success
    */
    JSON_HEDLEY_NON_NULL(2)
    token_type scan_literal(const char* literal_text, const std::size_t length,
                            token_type return_type)
    {
        assert(current == literal_text[0]);
        for (std::size_t i = 1; i < length; ++i)
        {
            if (JSON_HEDLEY_UNLIKELY(get() != literal_text[i]))
            {
                error_message = "invalid literal";
                return token_type::parse_error;
            }
        }
        return return_type;
    }

    /////////////////////
    // input management
    /////////////////////

    /// reset token_buffer; current character is beginning of token
    void reset() noexcept
    {
        token_buffer.clear();
        token_string.clear();
        token_string.push_back(std::char_traits<char>::to_char_type(current));
    }

    /*
    @brief get next character from the input

    This function provides the interface to the used input adapter. It does
    not throw in case the input reached EOF, but returns a
    `std::char_traits<char>::eof()` in that case.  Stores the scanned characters
    for use in error messages.

    @return character read from the input
    */
    std::char_traits<char>::int_type get()
    {
        ++position.chars_read_total;
        ++position.chars_read_current_line;

        if (next_unget)
        {
            // just reset the next_unget variable and work with current
            next_unget = false;
        }
        else
        {
            current = ia->get_character();
        }

        if (JSON_HEDLEY_LIKELY(current != std::char_traits<char>::eof()))
        {
            token_string.push_back(std::char_traits<char>::to_char_type(current));
        }

        if (current == '\n')
        {
            ++position.lines_read;
            position.chars_read_current_line = 0;
        }

        return current;
    }

    /*!
    @brief unget current character (read it again on next get)

    We implement unget by setting variable next_unget to true. The input is not
    changed - we just simulate ungetting by modifying chars_read_total,
    chars_read_current_line, and token_string. The next call to get() will
    behave as if the unget character is read again.
    */
    void unget()
    {
        next_unget = true;

        --position.chars_read_total;

        // in case we "unget" a newline, we have to also decrement the lines_read
        if (position.chars_read_current_line == 0)
        {
            if (position.lines_read > 0)
            {
                --position.lines_read;
            }
        }
        else
        {
            --position.chars_read_current_line;
        }

        if (JSON_HEDLEY_LIKELY(current != std::char_traits<char>::eof()))
        {
            assert(not token_string.empty());
            token_string.pop_back();
        }
    }

    /// add a character to token_buffer
    void add(int c)
    {
        token_buffer.push_back(std::char_traits<char>::to_char_type(c));
    }

  public:
    /////////////////////
    // value getters
    /////////////////////

    /// return integer value
    constexpr number_integer_t get_number_integer() const noexcept
    {
        return value_integer;
    }

    /// return unsigned integer value
    constexpr number_unsigned_t get_number_unsigned() const noexcept
    {
        return value_unsigned;
    }

    /// return floating-point value
    constexpr number_float_t get_number_float() const noexcept
    {
        return value_float;
    }

    /// return current string value (implicitly resets the token; useful only once)
    string_t& get_string()
    {
        return token_buffer;
    }

    /////////////////////
    // diagnostics
    /////////////////////

    /// return position of last read token
    constexpr position_t get_position() const noexcept
    {
        return position;
    }

    /// return the last read token (for errors only).  Will never contain EOF
    /// (an arbitrary value that is not a valid char value, often -1), because
    /// 255 may legitimately occur.  May contain NUL, which should be escaped.
    std::string get_token_string() const
    {
        // escape control characters
        std::string result;
        for (const auto c : token_string)
        {
            if ('\x00' <= c and c <= '\x1F')
            {
                // escape control characters
                std::array<char, 9> cs{{}};
                (std::snprintf)(cs.data(), cs.size(), "<U+%.4X>", static_cast<unsigned char>(c));
                result += cs.data();
            }
            else
            {
                // add character as is
                result.push_back(c);
            }
        }

        return result;
    }

    /// return syntax error message
    JSON_HEDLEY_RETURNS_NON_NULL
    constexpr const char* get_error_message() const noexcept
    {
        return error_message;
    }

    /////////////////////
    // actual scanner
    /////////////////////

    /*!
    @brief skip the UTF-8 byte order mark
    @return true iff there is no BOM or the correct BOM has been skipped
    */
    bool skip_bom()
    {
        if (get() == 0xEF)
        {
            // check if we completely parse the BOM
            return get() == 0xBB and get() == 0xBF;
        }

        // the first character is not the beginning of the BOM; unget it to
        // process is later
        unget();
        return true;
    }

    token_type scan()
    {
        // initially, skip the BOM
        if (position.chars_read_total == 0 and not skip_bom())
        {
            error_message = "invalid BOM; must be 0xEF 0xBB 0xBF if given";
            return token_type::parse_error;
        }

        // read next character and ignore whitespace
        do
        {
            get();
        }
        while (current == ' ' or current == '\t' or current == '\n' or current == '\r');

        switch (current)
        {
            // structural characters
            case '[':
                return token_type::begin_array;
            case ']':
                return token_type::end_array;
            case '{':
                return token_type::begin_object;
            case '}':
                return token_type::end_object;
            case ':':
                return token_type::name_separator;
            case ',':
                return token_type::value_separator;

            // literals
            case 't':
                return scan_literal("true", 4, token_type::literal_true);
            case 'f':
                return scan_literal("false", 5, token_type::literal_false);
            case 'n':
                return scan_literal("null", 4, token_type::literal_null);

            // string
            case '\"':
                return scan_string();

            // number
            case '-':
            case '0':
            case '1':
            case '2':
            case '3':
            case '4':
            case '5':
            case '6':
            case '7':
            case '8':
            case '9':
                return scan_number();

            // end of input (the null byte is needed when parsing from
            // string literals)
            case '\0':
            case std::char_traits<char>::eof():
                return token_type::end_of_input;

            // error
            default:
                error_message = "invalid literal";
                return token_type::parse_error;
        }
    }

  private:
    /// input adapter
    detail::input_adapter_t ia = nullptr;

    /// the current character
    std::char_traits<char>::int_type current = std::char_traits<char>::eof();

    /// whether the next get() call should just return current
    bool next_unget = false;

    /// the start position of the current token
    position_t position {};

    /// raw input token string (for error messages)
    std::vector<char> token_string {};

    /// buffer for variable-length tokens (numbers, strings)
    string_t token_buffer {};

    /// a description of occurred lexer errors
    const char* error_message = "";

    // number values
    number_integer_t value_integer = 0;
    number_unsigned_t value_unsigned = 0;
    number_float_t value_float = 0;

    /// the decimal point
    const char decimal_point_char = '.';
};
}  // namespace detail
}  // namespace nlohmann

// #include <nlohmann/detail/input/parser.hpp>


#include <cassert> // assert
#include <cmath> // isfinite
#include <cstdint> // uint8_t
#include <functional> // function
#include <string> // string
#include <utility> // move
#include <vector> // vector

// #include <nlohmann/detail/exceptions.hpp>

// #include <nlohmann/detail/input/input_adapters.hpp>

// #include <nlohmann/detail/input/json_sax.hpp>

// #include <nlohmann/detail/input/lexer.hpp>

// #include <nlohmann/detail/macro_scope.hpp>

// #include <nlohmann/detail/meta/is_sax.hpp>

// #include <nlohmann/detail/value_t.hpp>


namespace nlohmann
{
namespace detail
{
////////////
// parser //
////////////

/*!
@brief syntax analysis

This class implements a recursive decent parser.
*/
template<typename BasicJsonType>
class parser
{
    using number_integer_t = typename BasicJsonType::number_integer_t;
    using number_unsigned_t = typename BasicJsonType::number_unsigned_t;
    using number_float_t = typename BasicJsonType::number_float_t;
    using string_t = typename BasicJsonType::string_t;
    using lexer_t = lexer<BasicJsonType>;
    using token_type = typename lexer_t::token_type;

  public:
    enum class parse_event_t : uint8_t
    {
        /// the parser read `{` and started to process a JSON object
        object_start,
        /// the parser read `}` and finished processing a JSON object
        object_end,
        /// the parser read `[` and started to process a JSON array
        array_start,
        /// the parser read `]` and finished processing a JSON array
        array_end,
        /// the parser read a key of a value in an object
        key,
        /// the parser finished reading a JSON value
        value
    };

    using parser_callback_t =
        std::function<bool(int depth, parse_event_t event, BasicJsonType& parsed)>;

    /// a parser reading from an input adapter
    explicit parser(detail::input_adapter_t&& adapter,
                    const parser_callback_t cb = nullptr,
                    const bool allow_exceptions_ = true)
        : callback(cb), m_lexer(std::move(adapter)), allow_exceptions(allow_exceptions_)
    {
        // read first token
        get_token();
    }

    /*!
    @brief public parser interface

    @param[in] strict      whether to expect the last token to be EOF
    @param[in,out] result  parsed JSON value

    @throw parse_error.101 in case of an unexpected token
    @throw parse_error.102 if to_unicode fails or surrogate error
    @throw parse_error.103 if to_unicode fails
    */
    void parse(const bool strict, BasicJsonType& result)
    {
        if (callback)
        {
            json_sax_dom_callback_parser<BasicJsonType> sdp(result, callback, allow_exceptions);
            sax_parse_internal(&sdp);
            result.assert_invariant();

            // in strict mode, input must be completely read
            if (strict and (get_token() != token_type::end_of_input))
            {
                sdp.parse_error(m_lexer.get_position(),
                                m_lexer.get_token_string(),
                                parse_error::create(101, m_lexer.get_position(),
                                                    exception_message(token_type::end_of_input, "value")));
            }

            // in case of an error, return discarded value
            if (sdp.is_errored())
            {
                result = value_t::discarded;
                return;
            }

            // set top-level value to null if it was discarded by the callback
            // function
            if (result.is_discarded())
            {
                result = nullptr;
            }
        }
        else
        {
            json_sax_dom_parser<BasicJsonType> sdp(result, allow_exceptions);
            sax_parse_internal(&sdp);
            result.assert_invariant();

            // in strict mode, input must be completely read
            if (strict and (get_token() != token_type::end_of_input))
            {
                sdp.parse_error(m_lexer.get_position(),
                                m_lexer.get_token_string(),
                                parse_error::create(101, m_lexer.get_position(),
                                                    exception_message(token_type::end_of_input, "value")));
            }

            // in case of an error, return discarded value
            if (sdp.is_errored())
            {
                result = value_t::discarded;
                return;
            }
        }
    }

    /*!
    @brief public accept interface

    @param[in] strict  whether to expect the last token to be EOF
    @return whether the input is a proper JSON text
    */
    bool accept(const bool strict = true)
    {
        json_sax_acceptor<BasicJsonType> sax_acceptor;
        return sax_parse(&sax_acceptor, strict);
    }

    template <typename SAX>
    JSON_HEDLEY_NON_NULL(2)
    bool sax_parse(SAX* sax, const bool strict = true)
    {
        (void)detail::is_sax_static_asserts<SAX, BasicJsonType> {};
        const bool result = sax_parse_internal(sax);

        // strict mode: next byte must be EOF
        if (result and strict and (get_token() != token_type::end_of_input))
        {
            return sax->parse_error(m_lexer.get_position(),
                                    m_lexer.get_token_string(),
                                    parse_error::create(101, m_lexer.get_position(),
                                            exception_message(token_type::end_of_input, "value")));
        }

        return result;
    }

  private:
    template <typename SAX>
    JSON_HEDLEY_NON_NULL(2)
    bool sax_parse_internal(SAX* sax)
    {
        // stack to remember the hierarchy of structured values we are parsing
        // true = array; false = object
        std::vector<bool> states;
        // value to avoid a goto (see comment where set to true)
        bool skip_to_state_evaluation = false;

        while (true)
        {
            if (not skip_to_state_evaluation)
            {
                // invariant: get_token() was called before each iteration
                switch (last_token)
                {
                    case token_type::begin_object:
                    {
                        if (JSON_HEDLEY_UNLIKELY(not sax->start_object(std::size_t(-1))))
                        {
                            return false;
                        }

                        // closing } -> we are done
                        if (get_token() == token_type::end_object)
                        {
                            if (JSON_HEDLEY_UNLIKELY(not sax->end_object()))
                            {
                                return false;
                            }
                            break;
                        }

                        // parse key
                        if (JSON_HEDLEY_UNLIKELY(last_token != token_type::value_string))
                        {
                            return sax->parse_error(m_lexer.get_position(),
                                                    m_lexer.get_token_string(),
                                                    parse_error::create(101, m_lexer.get_position(),
                                                            exception_message(token_type::value_string, "object key")));
                        }
                        if (JSON_HEDLEY_UNLIKELY(not sax->key(m_lexer.get_string())))
                        {
                            return false;
                        }

                        // parse separator (:)
                        if (JSON_HEDLEY_UNLIKELY(get_token() != token_type::name_separator))
                        {
                            return sax->parse_error(m_lexer.get_position(),
                                                    m_lexer.get_token_string(),
                                                    parse_error::create(101, m_lexer.get_position(),
                                                            exception_message(token_type::name_separator, "object separator")));
                        }

                        // remember we are now inside an object
                        states.push_back(false);

                        // parse values
                        get_token();
                        continue;
                    }

                    case token_type::begin_array:
                    {
                        if (JSON_HEDLEY_UNLIKELY(not sax->start_array(std::size_t(-1))))
                        {
                            return false;
                        }

                        // closing ] -> we are done
                        if (get_token() == token_type::end_array)
                        {
                            if (JSON_HEDLEY_UNLIKELY(not sax->end_array()))
                            {
                                return false;
                            }
                            break;
                        }

                        // remember we are now inside an array
                        states.push_back(true);

                        // parse values (no need to call get_token)
                        continue;
                    }

                    case token_type::value_float:
                    {
                        const auto res = m_lexer.get_number_float();

                        if (JSON_HEDLEY_UNLIKELY(not std::isfinite(res)))
                        {
                            return sax->parse_error(m_lexer.get_position(),
                                                    m_lexer.get_token_string(),
                                                    out_of_range::create(406, "number overflow parsing '" + m_lexer.get_token_string() + "'"));
                        }

                        if (JSON_HEDLEY_UNLIKELY(not sax->number_float(res, m_lexer.get_string())))
                        {
                            return false;
                        }

                        break;
                    }

                    case token_type::literal_false:
                    {
                        if (JSON_HEDLEY_UNLIKELY(not sax->boolean(false)))
                        {
                            return false;
                        }
                        break;
                    }

                    case token_type::literal_null:
                    {
                        if (JSON_HEDLEY_UNLIKELY(not sax->null()))
                        {
                            return false;
                        }
                        break;
                    }

                    case token_type::literal_true:
                    {
                        if (JSON_HEDLEY_UNLIKELY(not sax->boolean(true)))
                        {
                            return false;
                        }
                        break;
                    }

                    case token_type::value_integer:
                    {
                        if (JSON_HEDLEY_UNLIKELY(not sax->number_integer(m_lexer.get_number_integer())))
                        {
                            return false;
                        }
                        break;
                    }

                    case token_type::value_string:
                    {
                        if (JSON_HEDLEY_UNLIKELY(not sax->string(m_lexer.get_string())))
                        {
                            return false;
                        }
                        break;
                    }

                    case token_type::value_unsigned:
                    {
                        if (JSON_HEDLEY_UNLIKELY(not sax->number_unsigned(m_lexer.get_number_unsigned())))
                        {
                            return false;
                        }
                        break;
                    }

                    case token_type::parse_error:
                    {
                        // using "uninitialized" to avoid "expected" message
                        return sax->parse_error(m_lexer.get_position(),
                                                m_lexer.get_token_string(),
                                                parse_error::create(101, m_lexer.get_position(),
                                                        exception_message(token_type::uninitialized, "value")));
                    }

                    default: // the last token was unexpected
                    {
                        return sax->parse_error(m_lexer.get_position(),
                                                m_lexer.get_token_string(),
                                                parse_error::create(101, m_lexer.get_position(),
                                                        exception_message(token_type::literal_or_value, "value")));
                    }
                }
            }
            else
            {
                skip_to_state_evaluation = false;
            }

            // we reached this line after we successfully parsed a value
            if (states.empty())
            {
                // empty stack: we reached the end of the hierarchy: done
                return true;
            }

            if (states.back())  // array
            {
                // comma -> next value
                if (get_token() == token_type::value_separator)
                {
                    // parse a new value
                    get_token();
                    continue;
                }

                // closing ]
                if (JSON_HEDLEY_LIKELY(last_token == token_type::end_array))
                {
                    if (JSON_HEDLEY_UNLIKELY(not sax->end_array()))
                    {
                        return false;
                    }

                    // We are done with this array. Before we can parse a
                    // new value, we need to evaluate the new state first.
                    // By setting skip_to_state_evaluation to false, we
                    // are effectively jumping to the beginning of this if.
                    assert(not states.empty());
                    states.pop_back();
                    skip_to_state_evaluation = true;
                    continue;
                }

                return sax->parse_error(m_lexer.get_position(),
                                        m_lexer.get_token_string(),
                                        parse_error::create(101, m_lexer.get_position(),
                                                exception_message(token_type::end_array, "array")));
            }
            else  // object
            {
                // comma -> next value
                if (get_token() == token_type::value_separator)
                {
                    // parse key
                    if (JSON_HEDLEY_UNLIKELY(get_token() != token_type::value_string))
                    {
                        return sax->parse_error(m_lexer.get_position(),
                                                m_lexer.get_token_string(),
                                                parse_error::create(101, m_lexer.get_position(),
                                                        exception_message(token_type::value_string, "object key")));
                    }

                    if (JSON_HEDLEY_UNLIKELY(not sax->key(m_lexer.get_string())))
                    {
                        return false;
                    }

                    // parse separator (:)
                    if (JSON_HEDLEY_UNLIKELY(get_token() != token_type::name_separator))
                    {
                        return sax->parse_error(m_lexer.get_position(),
                                                m_lexer.get_token_string(),
                                                parse_error::create(101, m_lexer.get_position(),
                                                        exception_message(token_type::name_separator, "object separator")));
                    }

                    // parse values
                    get_token();
                    continue;
                }

                // closing }
                if (JSON_HEDLEY_LIKELY(last_token == token_type::end_object))
                {
                    if (JSON_HEDLEY_UNLIKELY(not sax->end_object()))
                    {
                        return false;
                    }

                    // We are done with this object. Before we can parse a
                    // new value, we need to evaluate the new state first.
                    // By setting skip_to_state_evaluation to false, we
                    // are effectively jumping to the beginning of this if.
                    assert(not states.empty());
                    states.pop_back();
                    skip_to_state_evaluation = true;
                    continue;
                }

                return sax->parse_error(m_lexer.get_position(),
                                        m_lexer.get_token_string(),
                                        parse_error::create(101, m_lexer.get_position(),
                                                exception_message(token_type::end_object, "object")));
            }
        }
    }

    /// get next token from lexer
    token_type get_token()
    {
        return last_token = m_lexer.scan();
    }

    std::string exception_message(const token_type expected, const std::string& context)
    {
        std::string error_msg = "syntax error ";

        if (not context.empty())
        {
            error_msg += "while parsing " + context + " ";
        }

        error_msg += "- ";

        if (last_token == token_type::parse_error)
        {
            error_msg += std::string(m_lexer.get_error_message()) + "; last read: '" +
                         m_lexer.get_token_string() + "'";
        }
        else
        {
            error_msg += "unexpected " + std::string(lexer_t::token_type_name(last_token));
        }

        if (expected != token_type::uninitialized)
        {
            error_msg += "; expected " + std::string(lexer_t::token_type_name(expected));
        }

        return error_msg;
    }

  private:
    /// callback function
    const parser_callback_t callback = nullptr;
    /// the type of the last read token
    token_type last_token = token_type::uninitialized;
    /// the lexer
    lexer_t m_lexer;
    /// whether to throw exceptions in case of errors
    const bool allow_exceptions = true;
};
}  // namespace detail
}  // namespace nlohmann

// #include <nlohmann/detail/iterators/internal_iterator.hpp>


// #include <nlohmann/detail/iterators/primitive_iterator.hpp>


#include <cstddef> // ptrdiff_t
#include <limits>  // numeric_limits

namespace nlohmann
{
namespace detail
{
/*
@brief an iterator for primitive JSON types

This class models an iterator for primitive JSON types (boolean, number,
string). It's only purpose is to allow the iterator/const_iterator classes
to "iterate" over primitive values. Internally, the iterator is modeled by
a `difference_type` variable. Value begin_value (`0`) models the begin,
end_value (`1`) models past the end.
*/
class primitive_iterator_t
{
  private:
    using difference_type = std::ptrdiff_t;
    static constexpr difference_type begin_value = 0;
    static constexpr difference_type end_value = begin_value + 1;

    /// iterator as signed integer type
    difference_type m_it = (std::numeric_limits<std::ptrdiff_t>::min)();

  public:
    constexpr difference_type get_value() const noexcept
    {
        return m_it;
    }

    /// set iterator to a defined beginning
    void set_begin() noexcept
    {
        m_it = begin_value;
    }

    /// set iterator to a defined past the end
    void set_end() noexcept
    {
        m_it = end_value;
    }

    /// return whether the iterator can be dereferenced
    constexpr bool is_begin() const noexcept
    {
        return m_it == begin_value;
    }

    /// return whether the iterator is at end
    constexpr bool is_end() const noexcept
    {
        return m_it == end_value;
    }

    friend constexpr bool operator==(primitive_iterator_t lhs, primitive_iterator_t rhs) noexcept
    {
        return lhs.m_it == rhs.m_it;
    }

    friend constexpr bool operator<(primitive_iterator_t lhs, primitive_iterator_t rhs) noexcept
    {
        return lhs.m_it < rhs.m_it;
    }

    primitive_iterator_t operator+(difference_type n) noexcept
    {
        auto result = *this;
        result += n;
        return result;
    }

    friend constexpr difference_type operator-(primitive_iterator_t lhs, primitive_iterator_t rhs) noexcept
    {
        return lhs.m_it - rhs.m_it;
    }

    primitive_iterator_t& operator++() noexcept
    {
        ++m_it;
        return *this;
    }

    primitive_iterator_t const operator++(int) noexcept
    {
        auto result = *this;
        ++m_it;
        return result;
    }

    primitive_iterator_t& operator--() noexcept
    {
        --m_it;
        return *this;
    }

    primitive_iterator_t const operator--(int) noexcept
    {
        auto result = *this;
        --m_it;
        return result;
    }

    primitive_iterator_t& operator+=(difference_type n) noexcept
    {
        m_it += n;
        return *this;
    }

    primitive_iterator_t& operator-=(difference_type n) noexcept
    {
        m_it -= n;
        return *this;
    }
};
}  // namespace detail
}  // namespace nlohmann


namespace nlohmann
{
namespace detail
{
/*!
@brief an iterator value

@note This structure could easily be a union, but MSVC currently does not allow
unions members with complex constructors, see https://github.com/nlohmann/json/pull/105.
*/
template<typename BasicJsonType> struct internal_iterator
{
    /// iterator for JSON objects
    typename BasicJsonType::object_t::iterator object_iterator {};
    /// iterator for JSON arrays
    typename BasicJsonType::array_t::iterator array_iterator {};
    /// generic iterator for all other types
    primitive_iterator_t primitive_iterator {};
};
}  // namespace detail
}  // namespace nlohmann

// #include <nlohmann/detail/iterators/iter_impl.hpp>


#include <ciso646> // not
#include <iterator> // iterator, random_access_iterator_tag, bidirectional_iterator_tag, advance, next
#include <type_traits> // conditional, is_const, remove_const

// #include <nlohmann/detail/exceptions.hpp>

// #include <nlohmann/detail/iterators/internal_iterator.hpp>

// #include <nlohmann/detail/iterators/primitive_iterator.hpp>

// #include <nlohmann/detail/macro_scope.hpp>

// #include <nlohmann/detail/meta/cpp_future.hpp>

// #include <nlohmann/detail/meta/type_traits.hpp>

// #include <nlohmann/detail/value_t.hpp>


namespace nlohmann
{
namespace detail
{
// forward declare, to be able to friend it later on
template<typename IteratorType> class iteration_proxy;
template<typename IteratorType> class iteration_proxy_value;

/*!
@brief a template for a bidirectional iterator for the @ref basic_json class
This class implements a both iterators (iterator and const_iterator) for the
@ref basic_json class.
@note An iterator is called *initialized* when a pointer to a JSON value has
      been set (e.g., by a constructor or a copy assignment). If the iterator is
      default-constructed, it is *uninitialized* and most methods are undefined.
      **The library uses assertions to detect calls on uninitialized iterators.**
@requirement The class satisfies the following concept requirements:
-
[BidirectionalIterator](https://en.cppreference.com/w/cpp/named_req/BidirectionalIterator):
  The iterator that can be moved can be moved in both directions (i.e.
  incremented and decremented).
@since version 1.0.0, simplified in version 2.0.9, change to bidirectional
       iterators in version 3.0.0 (see https://github.com/nlohmann/json/issues/593)
*/
template<typename BasicJsonType>
class iter_impl
{
    /// allow basic_json to access private members
    friend iter_impl<typename std::conditional<std::is_const<BasicJsonType>::value, typename std::remove_const<BasicJsonType>::type, const BasicJsonType>::type>;
    friend BasicJsonType;
    friend iteration_proxy<iter_impl>;
    friend iteration_proxy_value<iter_impl>;

    using object_t = typename BasicJsonType::object_t;
    using array_t = typename BasicJsonType::array_t;
    // make sure BasicJsonType is basic_json or const basic_json
    static_assert(is_basic_json<typename std::remove_const<BasicJsonType>::type>::value,
                  "iter_impl only accepts (const) basic_json");

  public:

    /// The std::iterator class template (used as a base class to provide typedefs) is deprecated in C++17.
    /// The C++ Standard has never required user-defined iterators to derive from std::iterator.
    /// A user-defined iterator should provide publicly accessible typedefs named
    /// iterator_category, value_type, difference_type, pointer, and reference.
    /// Note that value_type is required to be non-const, even for constant iterators.
    using iterator_category = std::bidirectional_iterator_tag;

    /// the type of the values when the iterator is dereferenced
    using value_type = typename BasicJsonType::value_type;
    /// a type to represent differences between iterators
    using difference_type = typename BasicJsonType::difference_type;
    /// defines a pointer to the type iterated over (value_type)
    using pointer = typename std::conditional<std::is_const<BasicJsonType>::value,
          typename BasicJsonType::const_pointer,
          typename BasicJsonType::pointer>::type;
    /// defines a reference to the type iterated over (value_type)
    using reference =
        typename std::conditional<std::is_const<BasicJsonType>::value,
        typename BasicJsonType::const_reference,
        typename BasicJsonType::reference>::type;

    /// default constructor
    iter_impl() = default;

    /*!
    @brief constructor for a given JSON instance
    @param[in] object  pointer to a JSON object for this iterator
    @pre object != nullptr
    @post The iterator is initialized; i.e. `m_object != nullptr`.
    */
    explicit iter_impl(pointer object) noexcept : m_object(object)
    {
        assert(m_object != nullptr);

        switch (m_object->m_type)
        {
            case value_t::object:
            {
                m_it.object_iterator = typename object_t::iterator();
                break;
            }

            case value_t::array:
            {
                m_it.array_iterator = typename array_t::iterator();
                break;
            }

            default:
            {
                m_it.primitive_iterator = primitive_iterator_t();
                break;
            }
        }
    }

    /*!
    @note The conventional copy constructor and copy assignment are implicitly
          defined. Combined with the following converting constructor and
          assignment, they support: (1) copy from iterator to iterator, (2)
          copy from const iterator to const iterator, and (3) conversion from
          iterator to const iterator. However conversion from const iterator
          to iterator is not defined.
    */

    /*!
    @brief const copy constructor
    @param[in] other const iterator to copy from
    @note This copy constructor had to be defined explicitly to circumvent a bug
          occurring on msvc v19.0 compiler (VS 2015) debug build. For more
          information refer to: https://github.com/nlohmann/json/issues/1608
    */
    iter_impl(const iter_impl<const BasicJsonType>& other) noexcept
        : m_object(other.m_object), m_it(other.m_it)
    {}

    /*!
    @brief converting assignment
    @param[in] other const iterator to copy from
    @return const/non-const iterator
    @note It is not checked whether @a other is initialized.
    */
    iter_impl& operator=(const iter_impl<const BasicJsonType>& other) noexcept
    {
        m_object = other.m_object;
        m_it = other.m_it;
        return *this;
    }

    /*!
    @brief converting constructor
    @param[in] other  non-const iterator to copy from
    @note It is not checked whether @a other is initialized.
    */
    iter_impl(const iter_impl<typename std::remove_const<BasicJsonType>::type>& other) noexcept
        : m_object(other.m_object), m_it(other.m_it)
    {}

    /*!
    @brief converting assignment
    @param[in] other  non-const iterator to copy from
    @return const/non-const iterator
    @note It is not checked whether @a other is initialized.
    */
    iter_impl& operator=(const iter_impl<typename std::remove_const<BasicJsonType>::type>& other) noexcept
    {
        m_object = other.m_object;
        m_it = other.m_it;
        return *this;
    }

  private:
    /*!
    @brief set the iterator to the first value
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    void set_begin() noexcept
    {
        assert(m_object != nullptr);

        switch (m_object->m_type)
        {
            case value_t::object:
            {
                m_it.object_iterator = m_object->m_value.object->begin();
                break;
            }

            case value_t::array:
            {
                m_it.array_iterator = m_object->m_value.array->begin();
                break;
            }

            case value_t::null:
            {
                // set to end so begin()==end() is true: null is empty
                m_it.primitive_iterator.set_end();
                break;
            }

            default:
            {
                m_it.primitive_iterator.set_begin();
                break;
            }
        }
    }

    /*!
    @brief set the iterator past the last value
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    void set_end() noexcept
    {
        assert(m_object != nullptr);

        switch (m_object->m_type)
        {
            case value_t::object:
            {
                m_it.object_iterator = m_object->m_value.object->end();
                break;
            }

            case value_t::array:
            {
                m_it.array_iterator = m_object->m_value.array->end();
                break;
            }

            default:
            {
                m_it.primitive_iterator.set_end();
                break;
            }
        }
    }

  public:
    /*!
    @brief return a reference to the value pointed to by the iterator
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    reference operator*() const
    {
        assert(m_object != nullptr);

        switch (m_object->m_type)
        {
            case value_t::object:
            {
                assert(m_it.object_iterator != m_object->m_value.object->end());
                return m_it.object_iterator->second;
            }

            case value_t::array:
            {
                assert(m_it.array_iterator != m_object->m_value.array->end());
                return *m_it.array_iterator;
            }

            case value_t::null:
                JSON_THROW(invalid_iterator::create(214, "cannot get value"));

            default:
            {
                if (JSON_HEDLEY_LIKELY(m_it.primitive_iterator.is_begin()))
                {
                    return *m_object;
                }

                JSON_THROW(invalid_iterator::create(214, "cannot get value"));
            }
        }
    }

    /*!
    @brief dereference the iterator
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    pointer operator->() const
    {
        assert(m_object != nullptr);

        switch (m_object->m_type)
        {
            case value_t::object:
            {
                assert(m_it.object_iterator != m_object->m_value.object->end());
                return &(m_it.object_iterator->second);
            }

            case value_t::array:
            {
                assert(m_it.array_iterator != m_object->m_value.array->end());
                return &*m_it.array_iterator;
            }

            default:
            {
                if (JSON_HEDLEY_LIKELY(m_it.primitive_iterator.is_begin()))
                {
                    return m_object;
                }

                JSON_THROW(invalid_iterator::create(214, "cannot get value"));
            }
        }
    }

    /*!
    @brief post-increment (it++)
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    iter_impl const operator++(int)
    {
        auto result = *this;
        ++(*this);
        return result;
    }

    /*!
    @brief pre-increment (++it)
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    iter_impl& operator++()
    {
        assert(m_object != nullptr);

        switch (m_object->m_type)
        {
            case value_t::object:
            {
                std::advance(m_it.object_iterator, 1);
                break;
            }

            case value_t::array:
            {
                std::advance(m_it.array_iterator, 1);
                break;
            }

            default:
            {
                ++m_it.primitive_iterator;
                break;
            }
        }

        return *this;
    }

    /*!
    @brief post-decrement (it--)
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    iter_impl const operator--(int)
    {
        auto result = *this;
        --(*this);
        return result;
    }

    /*!
    @brief pre-decrement (--it)
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    iter_impl& operator--()
    {
        assert(m_object != nullptr);

        switch (m_object->m_type)
        {
            case value_t::object:
            {
                std::advance(m_it.object_iterator, -1);
                break;
            }

            case value_t::array:
            {
                std::advance(m_it.array_iterator, -1);
                break;
            }

            default:
            {
                --m_it.primitive_iterator;
                break;
            }
        }

        return *this;
    }

    /*!
    @brief  comparison: equal
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    bool operator==(const iter_impl& other) const
    {
        // if objects are not the same, the comparison is undefined
        if (JSON_HEDLEY_UNLIKELY(m_object != other.m_object))
        {
            JSON_THROW(invalid_iterator::create(212, "cannot compare iterators of different containers"));
        }

        assert(m_object != nullptr);

        switch (m_object->m_type)
        {
            case value_t::object:
                return (m_it.object_iterator == other.m_it.object_iterator);

            case value_t::array:
                return (m_it.array_iterator == other.m_it.array_iterator);

            default:
                return (m_it.primitive_iterator == other.m_it.primitive_iterator);
        }
    }

    /*!
    @brief  comparison: not equal
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    bool operator!=(const iter_impl& other) const
    {
        return not operator==(other);
    }

    /*!
    @brief  comparison: smaller
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    bool operator<(const iter_impl& other) const
    {
        // if objects are not the same, the comparison is undefined
        if (JSON_HEDLEY_UNLIKELY(m_object != other.m_object))
        {
            JSON_THROW(invalid_iterator::create(212, "cannot compare iterators of different containers"));
        }

        assert(m_object != nullptr);

        switch (m_object->m_type)
        {
            case value_t::object:
                JSON_THROW(invalid_iterator::create(213, "cannot compare order of object iterators"));

            case value_t::array:
                return (m_it.array_iterator < other.m_it.array_iterator);

            default:
                return (m_it.primitive_iterator < other.m_it.primitive_iterator);
        }
    }

    /*!
    @brief  comparison: less than or equal
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    bool operator<=(const iter_impl& other) const
    {
        return not other.operator < (*this);
    }

    /*!
    @brief  comparison: greater than
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    bool operator>(const iter_impl& other) const
    {
        return not operator<=(other);
    }

    /*!
    @brief  comparison: greater than or equal
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    bool operator>=(const iter_impl& other) const
    {
        return not operator<(other);
    }

    /*!
    @brief  add to iterator
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    iter_impl& operator+=(difference_type i)
    {
        assert(m_object != nullptr);

        switch (m_object->m_type)
        {
            case value_t::object:
                JSON_THROW(invalid_iterator::create(209, "cannot use offsets with object iterators"));

            case value_t::array:
            {
                std::advance(m_it.array_iterator, i);
                break;
            }

            default:
            {
                m_it.primitive_iterator += i;
                break;
            }
        }

        return *this;
    }

    /*!
    @brief  subtract from iterator
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    iter_impl& operator-=(difference_type i)
    {
        return operator+=(-i);
    }

    /*!
    @brief  add to iterator
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    iter_impl operator+(difference_type i) const
    {
        auto result = *this;
        result += i;
        return result;
    }

    /*!
    @brief  addition of distance and iterator
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    friend iter_impl operator+(difference_type i, const iter_impl& it)
    {
        auto result = it;
        result += i;
        return result;
    }

    /*!
    @brief  subtract from iterator
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    iter_impl operator-(difference_type i) const
    {
        auto result = *this;
        result -= i;
        return result;
    }

    /*!
    @brief  return difference
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    difference_type operator-(const iter_impl& other) const
    {
        assert(m_object != nullptr);

        switch (m_object->m_type)
        {
            case value_t::object:
                JSON_THROW(invalid_iterator::create(209, "cannot use offsets with object iterators"));

            case value_t::array:
                return m_it.array_iterator - other.m_it.array_iterator;

            default:
                return m_it.primitive_iterator - other.m_it.primitive_iterator;
        }
    }

    /*!
    @brief  access to successor
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    reference operator[](difference_type n) const
    {
        assert(m_object != nullptr);

        switch (m_object->m_type)
        {
            case value_t::object:
                JSON_THROW(invalid_iterator::create(208, "cannot use operator[] for object iterators"));

            case value_t::array:
                return *std::next(m_it.array_iterator, n);

            case value_t::null:
                JSON_THROW(invalid_iterator::create(214, "cannot get value"));

            default:
            {
                if (JSON_HEDLEY_LIKELY(m_it.primitive_iterator.get_value() == -n))
                {
                    return *m_object;
                }

                JSON_THROW(invalid_iterator::create(214, "cannot get value"));
            }
        }
    }

    /*!
    @brief  return the key of an object iterator
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    const typename object_t::key_type& key() const
    {
        assert(m_object != nullptr);

        if (JSON_HEDLEY_LIKELY(m_object->is_object()))
        {
            return m_it.object_iterator->first;
        }

        JSON_THROW(invalid_iterator::create(207, "cannot use key() for non-object iterators"));
    }

    /*!
    @brief  return the value of an iterator
    @pre The iterator is initialized; i.e. `m_object != nullptr`.
    */
    reference value() const
    {
        return operator*();
    }

  private:
    /// associated JSON instance
    pointer m_object = nullptr;
    /// the actual iterator of the associated instance
    internal_iterator<typename std::remove_const<BasicJsonType>::type> m_it {};
};
} // namespace detail
} // namespace nlohmann

// #include <nlohmann/detail/iterators/iteration_proxy.hpp>

// #include <nlohmann/detail/iterators/json_reverse_iterator.hpp>


#include <cstddef> // ptrdiff_t
#include <iterator> // reverse_iterator
#include <utility> // declval

namespace nlohmann
{
namespace detail
{
//////////////////////
// reverse_iterator //
//////////////////////

/*!
@brief a template for a reverse iterator class

@tparam Base the base iterator type to reverse. Valid types are @ref
iterator (to create @ref reverse_iterator) and @ref const_iterator (to
create @ref const_reverse_iterator).

@requirement The class satisfies the following concept requirements:
-
[BidirectionalIterator](https://en.cppreference.com/w/cpp/named_req/BidirectionalIterator):
  The iterator that can be moved can be moved in both directions (i.e.
  incremented and decremented).
- [OutputIterator](https://en.cppreference.com/w/cpp/named_req/OutputIterator):
  It is possible to write to the pointed-to element (only if @a Base is
  @ref iterator).

@since version 1.0.0
*/
template<typename Base>
class json_reverse_iterator : public std::reverse_iterator<Base>
{
  public:
    using difference_type = std::ptrdiff_t;
    /// shortcut to the reverse iterator adapter
    using base_iterator = std::reverse_iterator<Base>;
    /// the reference type for the pointed-to element
    using reference = typename Base::reference;

    /// create reverse iterator from iterator
    explicit json_reverse_iterator(const typename base_iterator::iterator_type& it) noexcept
        : base_iterator(it) {}

    /// create reverse iterator from base class
    explicit json_reverse_iterator(const base_iterator& it) noexcept : base_iterator(it) {}

    /// post-increment (it++)
    json_reverse_iterator const operator++(int)
    {
        return static_cast<json_reverse_iterator>(base_iterator::operator++(1));
    }

    /// pre-increment (++it)
    json_reverse_iterator& operator++()
    {
        return static_cast<json_reverse_iterator&>(base_iterator::operator++());
    }

    /// post-decrement (it--)
    json_reverse_iterator const operator--(int)
    {
        return static_cast<json_reverse_iterator>(base_iterator::operator--(1));
    }

    /// pre-decrement (--it)
    json_reverse_iterator& operator--()
    {
        return static_cast<json_reverse_iterator&>(base_iterator::operator--());
    }

    /// add to iterator
    json_reverse_iterator& operator+=(difference_type i)
    {
        return static_cast<json_reverse_iterator&>(base_iterator::operator+=(i));
    }

    /// add to iterator
    json_reverse_iterator operator+(difference_type i) const
    {
        return static_cast<json_reverse_iterator>(base_iterator::operator+(i));
    }

    /// subtract from iterator
    json_reverse_iterator operator-(difference_type i) const
    {
        return static_cast<json_reverse_iterator>(base_iterator::operator-(i));
    }

    /// return difference
    difference_type operator-(const json_reverse_iterator& other) const
    {
        return base_iterator(*this) - base_iterator(other);
    }

    /// access to successor
    reference operator[](difference_type n) const
    {
        return *(this->operator+(n));
    }

    /// return the key of an object iterator
    auto key() const -> decltype(std::declval<Base>().key())
    {
        auto it = --this->base();
        return it.key();
    }

    /// return the value of an iterator
    reference value() const
    {
        auto it = --this->base();
        return it.operator * ();
    }
};
}  // namespace detail
}  // namespace nlohmann

// #include <nlohmann/detail/iterators/primitive_iterator.hpp>

// #include <nlohmann/detail/json_pointer.hpp>


#include <algorithm> // all_of
#include <cassert> // assert
#include <cctype> // isdigit
#include <numeric> // accumulate
#include <string> // string
#include <utility> // move
#include <vector> // vector

// #include <nlohmann/detail/exceptions.hpp>

// #include <nlohmann/detail/macro_scope.hpp>

// #include <nlohmann/detail/value_t.hpp>


namespace nlohmann
{
template<typename BasicJsonType>
class json_pointer
{
    // allow basic_json to access private members
    NLOHMANN_BASIC_JSON_TPL_DECLARATION
    friend class basic_json;

  public:
    /*!
    @brief create JSON pointer

    Create a JSON pointer according to the syntax described in
    [Section 3 of RFC6901](https://tools.ietf.org/html/rfc6901#section-3).

    @param[in] s  string representing the JSON pointer; if omitted, the empty
                  string is assumed which references the whole JSON value

    @throw parse_error.107 if the given JSON pointer @a s is nonempty and does
                           not begin with a slash (`/`); see example below

    @throw parse_error.108 if a tilde (`~`) in the given JSON pointer @a s is
    not followed by `0` (representing `~`) or `1` (representing `/`); see
    example below

    @liveexample{The example shows the construction several valid JSON pointers
    as well as the exceptional behavior.,json_pointer}

    @since version 2.0.0
    */
    explicit json_pointer(const std::string& s = "")
        : reference_tokens(split(s))
    {}

    /*!
    @brief return a string representation of the JSON pointer

    @invariant For each JSON pointer `ptr`, it holds:
    @code {.cpp}
    ptr == json_pointer(ptr.to_string());
    @endcode

    @return a string representation of the JSON pointer

    @liveexample{The example shows the result of `to_string`.,json_pointer__to_string}

    @since version 2.0.0
    */
    std::string to_string() const
    {
        return std::accumulate(reference_tokens.begin(), reference_tokens.end(),
                               std::string{},
                               [](const std::string & a, const std::string & b)
        {
            return a + "/" + escape(b);
        });
    }

    /// @copydoc to_string()
    operator std::string() const
    {
        return to_string();
    }

    /*!
    @brief append another JSON pointer at the end of this JSON pointer

    @param[in] ptr  JSON pointer to append
    @return JSON pointer with @a ptr appended

    @liveexample{The example shows the usage of `operator/=`.,json_pointer__operator_add}

    @complexity Linear in the length of @a ptr.

    @sa @ref operator/=(std::string) to append a reference token
    @sa @ref operator/=(std::size_t) to append an array index
    @sa @ref operator/(const json_pointer&, const json_pointer&) for a binary operator

    @since version 3.6.0
    */
    json_pointer& operator/=(const json_pointer& ptr)
    {
        reference_tokens.insert(reference_tokens.end(),
                                ptr.reference_tokens.begin(),
                                ptr.reference_tokens.end());
        return *this;
    }

    /*!
    @brief append an unescaped reference token at the end of this JSON pointer

    @param[in] token  reference token to append
    @return JSON pointer with @a token appended without escaping @a token

    @liveexample{The example shows the usage of `operator/=`.,json_pointer__operator_add}

    @complexity Amortized constant.

    @sa @ref operator/=(const json_pointer&) to append a JSON pointer
    @sa @ref operator/=(std::size_t) to append an array index
    @sa @ref operator/(const json_pointer&, std::size_t) for a binary operator

    @since version 3.6.0
    */
    json_pointer& operator/=(std::string token)
    {
        push_back(std::move(token));
        return *this;
    }

    /*!
    @brief append an array index at the end of this JSON pointer

    @param[in] array_index  array index to append
    @return JSON pointer with @a array_index appended

    @liveexample{The example shows the usage of `operator/=`.,json_pointer__operator_add}

    @complexity Amortized constant.

    @sa @ref operator/=(const json_pointer&) to append a JSON pointer
    @sa @ref operator/=(std::string) to append a reference token
    @sa @ref operator/(const json_pointer&, std::string) for a binary operator

    @since version 3.6.0
    */
    json_pointer& operator/=(std::size_t array_index)
    {
        return *this /= std::to_string(array_index);
    }

    /*!
    @brief create a new JSON pointer by appending the right JSON pointer at the end of the left JSON pointer

    @param[in] lhs  JSON pointer
    @param[in] rhs  JSON pointer
    @return a new JSON pointer with @a rhs appended to @a lhs

    @liveexample{The example shows the usage of `operator/`.,json_pointer__operator_add_binary}

    @complexity Linear in the length of @a lhs and @a rhs.

    @sa @ref operator/=(const json_pointer&) to append a JSON pointer

    @since version 3.6.0
    */
    friend json_pointer operator/(const json_pointer& lhs,
                                  const json_pointer& rhs)
    {
        return json_pointer(lhs) /= rhs;
    }

    /*!
    @brief create a new JSON pointer by appending the unescaped token at the end of the JSON pointer

    @param[in] ptr  JSON pointer
    @param[in] token  reference token
    @return a new JSON pointer with unescaped @a token appended to @a ptr

    @liveexample{The example shows the usage of `operator/`.,json_pointer__operator_add_binary}

    @complexity Linear in the length of @a ptr.

    @sa @ref operator/=(std::string) to append a reference token

    @since version 3.6.0
    */
    friend json_pointer operator/(const json_pointer& ptr, std::string token)
    {
        return json_pointer(ptr) /= std::move(token);
    }

    /*!
    @brief create a new JSON pointer by appending the array-index-token at the end of the JSON pointer

    @param[in] ptr  JSON pointer
    @param[in] array_index  array index
    @return a new JSON pointer with @a array_index appended to @a ptr

    @liveexample{The example shows the usage of `operator/`.,json_pointer__operator_add_binary}

    @complexity Linear in the length of @a ptr.

    @sa @ref operator/=(std::size_t) to append an array index

    @since version 3.6.0
    */
    friend json_pointer operator/(const json_pointer& ptr, std::size_t array_index)
    {
        return json_pointer(ptr) /= array_index;
    }

    /*!
    @brief returns the parent of this JSON pointer

    @return parent of this JSON pointer; in case this JSON pointer is the root,
            the root itself is returned

    @complexity Linear in the length of the JSON pointer.

    @liveexample{The example shows the result of `parent_pointer` for different
    JSON Pointers.,json_pointer__parent_pointer}

    @since version 3.6.0
    */
    json_pointer parent_pointer() const
    {
        if (empty())
        {
            return *this;
        }

        json_pointer res = *this;
        res.pop_back();
        return res;
    }

    /*!
    @brief remove last reference token

    @pre not `empty()`

    @liveexample{The example shows the usage of `pop_back`.,json_pointer__pop_back}

    @complexity Constant.

    @throw out_of_range.405 if JSON pointer has no parent

    @since version 3.6.0
    */
    void pop_back()
    {
        if (JSON_HEDLEY_UNLIKELY(empty()))
        {
            JSON_THROW(detail::out_of_range::create(405, "JSON pointer has no parent"));
        }

        reference_tokens.pop_back();
    }

    /*!
    @brief return last reference token

    @pre not `empty()`
    @return last reference token

    @liveexample{The example shows the usage of `back`.,json_pointer__back}

    @complexity Constant.

    @throw out_of_range.405 if JSON pointer has no parent

    @since version 3.6.0
    */
    const std::string& back() const
    {
        if (JSON_HEDLEY_UNLIKELY(empty()))
        {
            JSON_THROW(detail::out_of_range::create(405, "JSON pointer has no parent"));
        }

        return reference_tokens.back();
    }

    /*!
    @brief append an unescaped token at the end of the reference pointer

    @param[in] token  token to add

    @complexity Amortized constant.

    @liveexample{The example shows the result of `push_back` for different
    JSON Pointers.,json_pointer__push_back}

    @since version 3.6.0
    */
    void push_back(const std::string& token)
    {
        reference_tokens.push_back(token);
    }

    /// @copydoc push_back(const std::string&)
    void push_back(std::string&& token)
    {
        reference_tokens.push_back(std::move(token));
    }

    /*!
    @brief return whether pointer points to the root document

    @return true iff the JSON pointer points to the root document

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this function never throws exceptions.

    @liveexample{The example shows the result of `empty` for different JSON
    Pointers.,json_pointer__empty}

    @since version 3.6.0
    */
    bool empty() const noexcept
    {
        return reference_tokens.empty();
    }

  private:
    /*!
    @param[in] s  reference token to be converted into an array index

    @return integer representation of @a s

    @throw out_of_range.404 if string @a s could not be converted to an integer
    */
    static int array_index(const std::string& s)
    {
        std::size_t processed_chars = 0;
        const int res = std::stoi(s, &processed_chars);

        // check if the string was completely read
        if (JSON_HEDLEY_UNLIKELY(processed_chars != s.size()))
        {
            JSON_THROW(detail::out_of_range::create(404, "unresolved reference token '" + s + "'"));
        }

        return res;
    }

    json_pointer top() const
    {
        if (JSON_HEDLEY_UNLIKELY(empty()))
        {
            JSON_THROW(detail::out_of_range::create(405, "JSON pointer has no parent"));
        }

        json_pointer result = *this;
        result.reference_tokens = {reference_tokens[0]};
        return result;
    }

    /*!
    @brief create and return a reference to the pointed to value

    @complexity Linear in the number of reference tokens.

    @throw parse_error.109 if array index is not a number
    @throw type_error.313 if value cannot be unflattened
    */
    BasicJsonType& get_and_create(BasicJsonType& j) const
    {
        using size_type = typename BasicJsonType::size_type;
        auto result = &j;

        // in case no reference tokens exist, return a reference to the JSON value
        // j which will be overwritten by a primitive value
        for (const auto& reference_token : reference_tokens)
        {
            switch (result->type())
            {
                case detail::value_t::null:
                {
                    if (reference_token == "0")
                    {
                        // start a new array if reference token is 0
                        result = &result->operator[](0);
                    }
                    else
                    {
                        // start a new object otherwise
                        result = &result->operator[](reference_token);
                    }
                    break;
                }

                case detail::value_t::object:
                {
                    // create an entry in the object
                    result = &result->operator[](reference_token);
                    break;
                }

                case detail::value_t::array:
                {
                    // create an entry in the array
                    JSON_TRY
                    {
                        result = &result->operator[](static_cast<size_type>(array_index(reference_token)));
                    }
                    JSON_CATCH(std::invalid_argument&)
                    {
                        JSON_THROW(detail::parse_error::create(109, 0, "array index '" + reference_token + "' is not a number"));
                    }
                    break;
                }

                /*
                The following code is only reached if there exists a reference
                token _and_ the current value is primitive. In this case, we have
                an error situation, because primitive values may only occur as
                single value; that is, with an empty list of reference tokens.
                */
                default:
                    JSON_THROW(detail::type_error::create(313, "invalid value to unflatten"));
            }
        }

        return *result;
    }

    /*!
    @brief return a reference to the pointed to value

    @note This version does not throw if a value is not present, but tries to
          create nested values instead. For instance, calling this function
          with pointer `"/this/that"` on a null value is equivalent to calling
          `operator[]("this").operator[]("that")` on that value, effectively
          changing the null value to an object.

    @param[in] ptr  a JSON value

    @return reference to the JSON value pointed to by the JSON pointer

    @complexity Linear in the length of the JSON pointer.

    @throw parse_error.106   if an array index begins with '0'
    @throw parse_error.109   if an array index was not a number
    @throw out_of_range.404  if the JSON pointer can not be resolved
    */
    BasicJsonType& get_unchecked(BasicJsonType* ptr) const
    {
        using size_type = typename BasicJsonType::size_type;
        for (const auto& reference_token : reference_tokens)
        {
            // convert null values to arrays or objects before continuing
            if (ptr->is_null())
            {
                // check if reference token is a number
                const bool nums =
                    std::all_of(reference_token.begin(), reference_token.end(),
                                [](const unsigned char x)
                {
                    return std::isdigit(x);
                });

                // change value to array for numbers or "-" or to object otherwise
                *ptr = (nums or reference_token == "-")
                       ? detail::value_t::array
                       : detail::value_t::object;
            }

            switch (ptr->type())
            {
                case detail::value_t::object:
                {
                    // use unchecked object access
                    ptr = &ptr->operator[](reference_token);
                    break;
                }

                case detail::value_t::array:
                {
                    // error condition (cf. RFC 6901, Sect. 4)
                    if (JSON_HEDLEY_UNLIKELY(reference_token.size() > 1 and reference_token[0] == '0'))
                    {
                        JSON_THROW(detail::parse_error::create(106, 0,
                                                               "array index '" + reference_token +
                                                               "' must not begin with '0'"));
                    }

                    if (reference_token == "-")
                    {
                        // explicitly treat "-" as index beyond the end
                        ptr = &ptr->operator[](ptr->m_value.array->size());
                    }
                    else
                    {
                        // convert array index to number; unchecked access
                        JSON_TRY
                        {
                            ptr = &ptr->operator[](
                                static_cast<size_type>(array_index(reference_token)));
                        }
                        JSON_CATCH(std::invalid_argument&)
                        {
                            JSON_THROW(detail::parse_error::create(109, 0, "array index '" + reference_token + "' is not a number"));
                        }
                    }
                    break;
                }

                default:
                    JSON_THROW(detail::out_of_range::create(404, "unresolved reference token '" + reference_token + "'"));
            }
        }

        return *ptr;
    }

    /*!
    @throw parse_error.106   if an array index begins with '0'
    @throw parse_error.109   if an array index was not a number
    @throw out_of_range.402  if the array index '-' is used
    @throw out_of_range.404  if the JSON pointer can not be resolved
    */
    BasicJsonType& get_checked(BasicJsonType* ptr) const
    {
        using size_type = typename BasicJsonType::size_type;
        for (const auto& reference_token : reference_tokens)
        {
            switch (ptr->type())
            {
                case detail::value_t::object:
                {
                    // note: at performs range check
                    ptr = &ptr->at(reference_token);
                    break;
                }

                case detail::value_t::array:
                {
                    if (JSON_HEDLEY_UNLIKELY(reference_token == "-"))
                    {
                        // "-" always fails the range check
                        JSON_THROW(detail::out_of_range::create(402,
                                                                "array index '-' (" + std::to_string(ptr->m_value.array->size()) +
                                                                ") is out of range"));
                    }

                    // error condition (cf. RFC 6901, Sect. 4)
                    if (JSON_HEDLEY_UNLIKELY(reference_token.size() > 1 and reference_token[0] == '0'))
                    {
                        JSON_THROW(detail::parse_error::create(106, 0,
                                                               "array index '" + reference_token +
                                                               "' must not begin with '0'"));
                    }

                    // note: at performs range check
                    JSON_TRY
                    {
                        ptr = &ptr->at(static_cast<size_type>(array_index(reference_token)));
                    }
                    JSON_CATCH(std::invalid_argument&)
                    {
                        JSON_THROW(detail::parse_error::create(109, 0, "array index '" + reference_token + "' is not a number"));
                    }
                    break;
                }

                default:
                    JSON_THROW(detail::out_of_range::create(404, "unresolved reference token '" + reference_token + "'"));
            }
        }

        return *ptr;
    }

    /*!
    @brief return a const reference to the pointed to value

    @param[in] ptr  a JSON value

    @return const reference to the JSON value pointed to by the JSON
    pointer

    @throw parse_error.106   if an array index begins with '0'
    @throw parse_error.109   if an array index was not a number
    @throw out_of_range.402  if the array index '-' is used
    @throw out_of_range.404  if the JSON pointer can not be resolved
    */
    const BasicJsonType& get_unchecked(const BasicJsonType* ptr) const
    {
        using size_type = typename BasicJsonType::size_type;
        for (const auto& reference_token : reference_tokens)
        {
            switch (ptr->type())
            {
                case detail::value_t::object:
                {
                    // use unchecked object access
                    ptr = &ptr->operator[](reference_token);
                    break;
                }

                case detail::value_t::array:
                {
                    if (JSON_HEDLEY_UNLIKELY(reference_token == "-"))
                    {
                        // "-" cannot be used for const access
                        JSON_THROW(detail::out_of_range::create(402,
                                                                "array index '-' (" + std::to_string(ptr->m_value.array->size()) +
                                                                ") is out of range"));
                    }

                    // error condition (cf. RFC 6901, Sect. 4)
                    if (JSON_HEDLEY_UNLIKELY(reference_token.size() > 1 and reference_token[0] == '0'))
                    {
                        JSON_THROW(detail::parse_error::create(106, 0,
                                                               "array index '" + reference_token +
                                                               "' must not begin with '0'"));
                    }

                    // use unchecked array access
                    JSON_TRY
                    {
                        ptr = &ptr->operator[](
                            static_cast<size_type>(array_index(reference_token)));
                    }
                    JSON_CATCH(std::invalid_argument&)
                    {
                        JSON_THROW(detail::parse_error::create(109, 0, "array index '" + reference_token + "' is not a number"));
                    }
                    break;
                }

                default:
                    JSON_THROW(detail::out_of_range::create(404, "unresolved reference token '" + reference_token + "'"));
            }
        }

        return *ptr;
    }

    /*!
    @throw parse_error.106   if an array index begins with '0'
    @throw parse_error.109   if an array index was not a number
    @throw out_of_range.402  if the array index '-' is used
    @throw out_of_range.404  if the JSON pointer can not be resolved
    */
    const BasicJsonType& get_checked(const BasicJsonType* ptr) const
    {
        using size_type = typename BasicJsonType::size_type;
        for (const auto& reference_token : reference_tokens)
        {
            switch (ptr->type())
            {
                case detail::value_t::object:
                {
                    // note: at performs range check
                    ptr = &ptr->at(reference_token);
                    break;
                }

                case detail::value_t::array:
                {
                    if (JSON_HEDLEY_UNLIKELY(reference_token == "-"))
                    {
                        // "-" always fails the range check
                        JSON_THROW(detail::out_of_range::create(402,
                                                                "array index '-' (" + std::to_string(ptr->m_value.array->size()) +
                                                                ") is out of range"));
                    }

                    // error condition (cf. RFC 6901, Sect. 4)
                    if (JSON_HEDLEY_UNLIKELY(reference_token.size() > 1 and reference_token[0] == '0'))
                    {
                        JSON_THROW(detail::parse_error::create(106, 0,
                                                               "array index '" + reference_token +
                                                               "' must not begin with '0'"));
                    }

                    // note: at performs range check
                    JSON_TRY
                    {
                        ptr = &ptr->at(static_cast<size_type>(array_index(reference_token)));
                    }
                    JSON_CATCH(std::invalid_argument&)
                    {
                        JSON_THROW(detail::parse_error::create(109, 0, "array index '" + reference_token + "' is not a number"));
                    }
                    break;
                }

                default:
                    JSON_THROW(detail::out_of_range::create(404, "unresolved reference token '" + reference_token + "'"));
            }
        }

        return *ptr;
    }

    /*!
    @throw parse_error.106   if an array index begins with '0'
    @throw parse_error.109   if an array index was not a number
    */
    bool contains(const BasicJsonType* ptr) const
    {
        using size_type = typename BasicJsonType::size_type;
        for (const auto& reference_token : reference_tokens)
        {
            switch (ptr->type())
            {
                case detail::value_t::object:
                {
                    if (not ptr->contains(reference_token))
                    {
                        // we did not find the key in the object
                        return false;
                    }

                    ptr = &ptr->operator[](reference_token);
                    break;
                }

                case detail::value_t::array:
                {
                    if (JSON_HEDLEY_UNLIKELY(reference_token == "-"))
                    {
                        // "-" always fails the range check
                        return false;
                    }

                    // error condition (cf. RFC 6901, Sect. 4)
                    if (JSON_HEDLEY_UNLIKELY(reference_token.size() > 1 and reference_token[0] == '0'))
                    {
                        JSON_THROW(detail::parse_error::create(106, 0,
                                                               "array index '" + reference_token +
                                                               "' must not begin with '0'"));
                    }

                    JSON_TRY
                    {
                        const auto idx = static_cast<size_type>(array_index(reference_token));
                        if (idx >= ptr->size())
                        {
                            // index out of range
                            return false;
                        }

                        ptr = &ptr->operator[](idx);
                        break;
                    }
                    JSON_CATCH(std::invalid_argument&)
                    {
                        JSON_THROW(detail::parse_error::create(109, 0, "array index '" + reference_token + "' is not a number"));
                    }
                    break;
                }

                default:
                {
                    // we do not expect primitive values if there is still a
                    // reference token to process
                    return false;
                }
            }
        }

        // no reference token left means we found a primitive value
        return true;
    }

    /*!
    @brief split the string input to reference tokens

    @note This function is only called by the json_pointer constructor.
          All exceptions below are documented there.

    @throw parse_error.107  if the pointer is not empty or begins with '/'
    @throw parse_error.108  if character '~' is not followed by '0' or '1'
    */
    static std::vector<std::string> split(const std::string& reference_string)
    {
        std::vector<std::string> result;

        // special case: empty reference string -> no reference tokens
        if (reference_string.empty())
        {
            return result;
        }

        // check if nonempty reference string begins with slash
        if (JSON_HEDLEY_UNLIKELY(reference_string[0] != '/'))
        {
            JSON_THROW(detail::parse_error::create(107, 1,
                                                   "JSON pointer must be empty or begin with '/' - was: '" +
                                                   reference_string + "'"));
        }

        // extract the reference tokens:
        // - slash: position of the last read slash (or end of string)
        // - start: position after the previous slash
        for (
            // search for the first slash after the first character
            std::size_t slash = reference_string.find_first_of('/', 1),
            // set the beginning of the first reference token
            start = 1;
            // we can stop if start == 0 (if slash == std::string::npos)
            start != 0;
            // set the beginning of the next reference token
            // (will eventually be 0 if slash == std::string::npos)
            start = (slash == std::string::npos) ? 0 : slash + 1,
            // find next slash
            slash = reference_string.find_first_of('/', start))
        {
            // use the text between the beginning of the reference token
            // (start) and the last slash (slash).
            auto reference_token = reference_string.substr(start, slash - start);

            // check reference tokens are properly escaped
            for (std::size_t pos = reference_token.find_first_of('~');
                    pos != std::string::npos;
                    pos = reference_token.find_first_of('~', pos + 1))
            {
                assert(reference_token[pos] == '~');

                // ~ must be followed by 0 or 1
                if (JSON_HEDLEY_UNLIKELY(pos == reference_token.size() - 1 or
                                         (reference_token[pos + 1] != '0' and
                                          reference_token[pos + 1] != '1')))
                {
                    JSON_THROW(detail::parse_error::create(108, 0, "escape character '~' must be followed with '0' or '1'"));
                }
            }

            // finally, store the reference token
            unescape(reference_token);
            result.push_back(reference_token);
        }

        return result;
    }

    /*!
    @brief replace all occurrences of a substring by another string

    @param[in,out] s  the string to manipulate; changed so that all
                   occurrences of @a f are replaced with @a t
    @param[in]     f  the substring to replace with @a t
    @param[in]     t  the string to replace @a f

    @pre The search string @a f must not be empty. **This precondition is
    enforced with an assertion.**

    @since version 2.0.0
    */
    static void replace_substring(std::string& s, const std::string& f,
                                  const std::string& t)
    {
        assert(not f.empty());
        for (auto pos = s.find(f);                // find first occurrence of f
                pos != std::string::npos;         // make sure f was found
                s.replace(pos, f.size(), t),      // replace with t, and
                pos = s.find(f, pos + t.size()))  // find next occurrence of f
        {}
    }

    /// escape "~" to "~0" and "/" to "~1"
    static std::string escape(std::string s)
    {
        replace_substring(s, "~", "~0");
        replace_substring(s, "/", "~1");
        return s;
    }

    /// unescape "~1" to tilde and "~0" to slash (order is important!)
    static void unescape(std::string& s)
    {
        replace_substring(s, "~1", "/");
        replace_substring(s, "~0", "~");
    }

    /*!
    @param[in] reference_string  the reference string to the current value
    @param[in] value             the value to consider
    @param[in,out] result        the result object to insert values to

    @note Empty objects or arrays are flattened to `null`.
    */
    static void flatten(const std::string& reference_string,
                        const BasicJsonType& value,
                        BasicJsonType& result)
    {
        switch (value.type())
        {
            case detail::value_t::array:
            {
                if (value.m_value.array->empty())
                {
                    // flatten empty array as null
                    result[reference_string] = nullptr;
                }
                else
                {
                    // iterate array and use index as reference string
                    for (std::size_t i = 0; i < value.m_value.array->size(); ++i)
                    {
                        flatten(reference_string + "/" + std::to_string(i),
                                value.m_value.array->operator[](i), result);
                    }
                }
                break;
            }

            case detail::value_t::object:
            {
                if (value.m_value.object->empty())
                {
                    // flatten empty object as null
                    result[reference_string] = nullptr;
                }
                else
                {
                    // iterate object and use keys as reference string
                    for (const auto& element : *value.m_value.object)
                    {
                        flatten(reference_string + "/" + escape(element.first), element.second, result);
                    }
                }
                break;
            }

            default:
            {
                // add primitive value with its reference string
                result[reference_string] = value;
                break;
            }
        }
    }

    /*!
    @param[in] value  flattened JSON

    @return unflattened JSON

    @throw parse_error.109 if array index is not a number
    @throw type_error.314  if value is not an object
    @throw type_error.315  if object values are not primitive
    @throw type_error.313  if value cannot be unflattened
    */
    static BasicJsonType
    unflatten(const BasicJsonType& value)
    {
        if (JSON_HEDLEY_UNLIKELY(not value.is_object()))
        {
            JSON_THROW(detail::type_error::create(314, "only objects can be unflattened"));
        }

        BasicJsonType result;

        // iterate the JSON object values
        for (const auto& element : *value.m_value.object)
        {
            if (JSON_HEDLEY_UNLIKELY(not element.second.is_primitive()))
            {
                JSON_THROW(detail::type_error::create(315, "values in object must be primitive"));
            }

            // assign value to reference pointed to by JSON pointer; Note that if
            // the JSON pointer is "" (i.e., points to the whole value), function
            // get_and_create returns a reference to result itself. An assignment
            // will then create a primitive value.
            json_pointer(element.first).get_and_create(result) = element.second;
        }

        return result;
    }

    /*!
    @brief compares two JSON pointers for equality

    @param[in] lhs  JSON pointer to compare
    @param[in] rhs  JSON pointer to compare
    @return whether @a lhs is equal to @a rhs

    @complexity Linear in the length of the JSON pointer

    @exceptionsafety No-throw guarantee: this function never throws exceptions.
    */
    friend bool operator==(json_pointer const& lhs,
                           json_pointer const& rhs) noexcept
    {
        return lhs.reference_tokens == rhs.reference_tokens;
    }

    /*!
    @brief compares two JSON pointers for inequality

    @param[in] lhs  JSON pointer to compare
    @param[in] rhs  JSON pointer to compare
    @return whether @a lhs is not equal @a rhs

    @complexity Linear in the length of the JSON pointer

    @exceptionsafety No-throw guarantee: this function never throws exceptions.
    */
    friend bool operator!=(json_pointer const& lhs,
                           json_pointer const& rhs) noexcept
    {
        return not (lhs == rhs);
    }

    /// the reference tokens
    std::vector<std::string> reference_tokens;
};
}  // namespace nlohmann

// #include <nlohmann/detail/json_ref.hpp>


#include <initializer_list>
#include <utility>

// #include <nlohmann/detail/meta/type_traits.hpp>


namespace nlohmann
{
namespace detail
{
template<typename BasicJsonType>
class json_ref
{
  public:
    using value_type = BasicJsonType;

    json_ref(value_type&& value)
        : owned_value(std::move(value)), value_ref(&owned_value), is_rvalue(true)
    {}

    json_ref(const value_type& value)
        : value_ref(const_cast<value_type*>(&value)), is_rvalue(false)
    {}

    json_ref(std::initializer_list<json_ref> init)
        : owned_value(init), value_ref(&owned_value), is_rvalue(true)
    {}

    template <
        class... Args,
        enable_if_t<std::is_constructible<value_type, Args...>::value, int> = 0 >
    json_ref(Args && ... args)
        : owned_value(std::forward<Args>(args)...), value_ref(&owned_value),
          is_rvalue(true) {}

    // class should be movable only
    json_ref(json_ref&&) = default;
    json_ref(const json_ref&) = delete;
    json_ref& operator=(const json_ref&) = delete;
    json_ref& operator=(json_ref&&) = delete;
    ~json_ref() = default;

    value_type moved_or_copied() const
    {
        if (is_rvalue)
        {
            return std::move(*value_ref);
        }
        return *value_ref;
    }

    value_type const& operator*() const
    {
        return *static_cast<value_type const*>(value_ref);
    }

    value_type const* operator->() const
    {
        return static_cast<value_type const*>(value_ref);
    }

  private:
    mutable value_type owned_value = nullptr;
    value_type* value_ref = nullptr;
    const bool is_rvalue;
};
}  // namespace detail
}  // namespace nlohmann

// #include <nlohmann/detail/macro_scope.hpp>

// #include <nlohmann/detail/meta/cpp_future.hpp>

// #include <nlohmann/detail/meta/type_traits.hpp>

// #include <nlohmann/detail/output/binary_writer.hpp>


#include <algorithm> // reverse
#include <array> // array
#include <cstdint> // uint8_t, uint16_t, uint32_t, uint64_t
#include <cstring> // memcpy
#include <limits> // numeric_limits
#include <string> // string

// #include <nlohmann/detail/input/binary_reader.hpp>

// #include <nlohmann/detail/macro_scope.hpp>

// #include <nlohmann/detail/output/output_adapters.hpp>


#include <algorithm> // copy
#include <cstddef> // size_t
#include <ios> // streamsize
#include <iterator> // back_inserter
#include <memory> // shared_ptr, make_shared
#include <ostream> // basic_ostream
#include <string> // basic_string
#include <vector> // vector
// #include <nlohmann/detail/macro_scope.hpp>


namespace nlohmann
{
namespace detail
{
/// abstract output adapter interface
template<typename CharType> struct output_adapter_protocol
{
    virtual void write_character(CharType c) = 0;
    virtual void write_characters(const CharType* s, std::size_t length) = 0;
    virtual ~output_adapter_protocol() = default;
};

/// a type to simplify interfaces
template<typename CharType>
using output_adapter_t = std::shared_ptr<output_adapter_protocol<CharType>>;

/// output adapter for byte vectors
template<typename CharType>
class output_vector_adapter : public output_adapter_protocol<CharType>
{
  public:
    explicit output_vector_adapter(std::vector<CharType>& vec) noexcept
        : v(vec)
    {}

    void write_character(CharType c) override
    {
        v.push_back(c);
    }

    JSON_HEDLEY_NON_NULL(2)
    void write_characters(const CharType* s, std::size_t length) override
    {
        std::copy(s, s + length, std::back_inserter(v));
    }

  private:
    std::vector<CharType>& v;
};

/// output adapter for output streams
template<typename CharType>
class output_stream_adapter : public output_adapter_protocol<CharType>
{
  public:
    explicit output_stream_adapter(std::basic_ostream<CharType>& s) noexcept
        : stream(s)
    {}

    void write_character(CharType c) override
    {
        stream.put(c);
    }

    JSON_HEDLEY_NON_NULL(2)
    void write_characters(const CharType* s, std::size_t length) override
    {
        stream.write(s, static_cast<std::streamsize>(length));
    }

  private:
    std::basic_ostream<CharType>& stream;
};

/// output adapter for basic_string
template<typename CharType, typename StringType = std::basic_string<CharType>>
class output_string_adapter : public output_adapter_protocol<CharType>
{
  public:
    explicit output_string_adapter(StringType& s) noexcept
        : str(s)
    {}

    void write_character(CharType c) override
    {
        str.push_back(c);
    }

    JSON_HEDLEY_NON_NULL(2)
    void write_characters(const CharType* s, std::size_t length) override
    {
        str.append(s, length);
    }

  private:
    StringType& str;
};

template<typename CharType, typename StringType = std::basic_string<CharType>>
class output_adapter
{
  public:
    output_adapter(std::vector<CharType>& vec)
        : oa(std::make_shared<output_vector_adapter<CharType>>(vec)) {}

    output_adapter(std::basic_ostream<CharType>& s)
        : oa(std::make_shared<output_stream_adapter<CharType>>(s)) {}

    output_adapter(StringType& s)
        : oa(std::make_shared<output_string_adapter<CharType, StringType>>(s)) {}

    operator output_adapter_t<CharType>()
    {
        return oa;
    }

  private:
    output_adapter_t<CharType> oa = nullptr;
};
}  // namespace detail
}  // namespace nlohmann


namespace nlohmann
{
namespace detail
{
///////////////////
// binary writer //
///////////////////

/*!
@brief serialization to CBOR and MessagePack values
*/
template<typename BasicJsonType, typename CharType>
class binary_writer
{
    using string_t = typename BasicJsonType::string_t;

  public:
    /*!
    @brief create a binary writer

    @param[in] adapter  output adapter to write to
    */
    explicit binary_writer(output_adapter_t<CharType> adapter) : oa(adapter)
    {
        assert(oa);
    }

    /*!
    @param[in] j  JSON value to serialize
    @pre       j.type() == value_t::object
    */
    void write_bson(const BasicJsonType& j)
    {
        switch (j.type())
        {
            case value_t::object:
            {
                write_bson_object(*j.m_value.object);
                break;
            }

            default:
            {
                JSON_THROW(type_error::create(317, "to serialize to BSON, top-level type must be object, but is " + std::string(j.type_name())));
            }
        }
    }

    /*!
    @param[in] j  JSON value to serialize
    */
    void write_cbor(const BasicJsonType& j)
    {
        switch (j.type())
        {
            case value_t::null:
            {
                oa->write_character(to_char_type(0xF6));
                break;
            }

            case value_t::boolean:
            {
                oa->write_character(j.m_value.boolean
                                    ? to_char_type(0xF5)
                                    : to_char_type(0xF4));
                break;
            }

            case value_t::number_integer:
            {
                if (j.m_value.number_integer >= 0)
                {
                    // CBOR does not differentiate between positive signed
                    // integers and unsigned integers. Therefore, we used the
                    // code from the value_t::number_unsigned case here.
                    if (j.m_value.number_integer <= 0x17)
                    {
                        write_number(static_cast<std::uint8_t>(j.m_value.number_integer));
                    }
                    else if (j.m_value.number_integer <= (std::numeric_limits<std::uint8_t>::max)())
                    {
                        oa->write_character(to_char_type(0x18));
                        write_number(static_cast<std::uint8_t>(j.m_value.number_integer));
                    }
                    else if (j.m_value.number_integer <= (std::numeric_limits<std::uint16_t>::max)())
                    {
                        oa->write_character(to_char_type(0x19));
                        write_number(static_cast<std::uint16_t>(j.m_value.number_integer));
                    }
                    else if (j.m_value.number_integer <= (std::numeric_limits<std::uint32_t>::max)())
                    {
                        oa->write_character(to_char_type(0x1A));
                        write_number(static_cast<std::uint32_t>(j.m_value.number_integer));
                    }
                    else
                    {
                        oa->write_character(to_char_type(0x1B));
                        write_number(static_cast<std::uint64_t>(j.m_value.number_integer));
                    }
                }
                else
                {
                    // The conversions below encode the sign in the first
                    // byte, and the value is converted to a positive number.
                    const auto positive_number = -1 - j.m_value.number_integer;
                    if (j.m_value.number_integer >= -24)
                    {
                        write_number(static_cast<std::uint8_t>(0x20 + positive_number));
                    }
                    else if (positive_number <= (std::numeric_limits<std::uint8_t>::max)())
                    {
                        oa->write_character(to_char_type(0x38));
                        write_number(static_cast<std::uint8_t>(positive_number));
                    }
                    else if (positive_number <= (std::numeric_limits<std::uint16_t>::max)())
                    {
                        oa->write_character(to_char_type(0x39));
                        write_number(static_cast<std::uint16_t>(positive_number));
                    }
                    else if (positive_number <= (std::numeric_limits<std::uint32_t>::max)())
                    {
                        oa->write_character(to_char_type(0x3A));
                        write_number(static_cast<std::uint32_t>(positive_number));
                    }
                    else
                    {
                        oa->write_character(to_char_type(0x3B));
                        write_number(static_cast<std::uint64_t>(positive_number));
                    }
                }
                break;
            }

            case value_t::number_unsigned:
            {
                if (j.m_value.number_unsigned <= 0x17)
                {
                    write_number(static_cast<std::uint8_t>(j.m_value.number_unsigned));
                }
                else if (j.m_value.number_unsigned <= (std::numeric_limits<std::uint8_t>::max)())
                {
                    oa->write_character(to_char_type(0x18));
                    write_number(static_cast<std::uint8_t>(j.m_value.number_unsigned));
                }
                else if (j.m_value.number_unsigned <= (std::numeric_limits<std::uint16_t>::max)())
                {
                    oa->write_character(to_char_type(0x19));
                    write_number(static_cast<std::uint16_t>(j.m_value.number_unsigned));
                }
                else if (j.m_value.number_unsigned <= (std::numeric_limits<std::uint32_t>::max)())
                {
                    oa->write_character(to_char_type(0x1A));
                    write_number(static_cast<std::uint32_t>(j.m_value.number_unsigned));
                }
                else
                {
                    oa->write_character(to_char_type(0x1B));
                    write_number(static_cast<std::uint64_t>(j.m_value.number_unsigned));
                }
                break;
            }

            case value_t::number_float:
            {
                oa->write_character(get_cbor_float_prefix(j.m_value.number_float));
                write_number(j.m_value.number_float);
                break;
            }

            case value_t::string:
            {
                // step 1: write control byte and the string length
                const auto N = j.m_value.string->size();
                if (N <= 0x17)
                {
                    write_number(static_cast<std::uint8_t>(0x60 + N));
                }
                else if (N <= (std::numeric_limits<std::uint8_t>::max)())
                {
                    oa->write_character(to_char_type(0x78));
                    write_number(static_cast<std::uint8_t>(N));
                }
                else if (N <= (std::numeric_limits<std::uint16_t>::max)())
                {
                    oa->write_character(to_char_type(0x79));
                    write_number(static_cast<std::uint16_t>(N));
                }
                else if (N <= (std::numeric_limits<std::uint32_t>::max)())
                {
                    oa->write_character(to_char_type(0x7A));
                    write_number(static_cast<std::uint32_t>(N));
                }
                // LCOV_EXCL_START
                else if (N <= (std::numeric_limits<std::uint64_t>::max)())
                {
                    oa->write_character(to_char_type(0x7B));
                    write_number(static_cast<std::uint64_t>(N));
                }
                // LCOV_EXCL_STOP

                // step 2: write the string
                oa->write_characters(
                    reinterpret_cast<const CharType*>(j.m_value.string->c_str()),
                    j.m_value.string->size());
                break;
            }

            case value_t::array:
            {
                // step 1: write control byte and the array size
                const auto N = j.m_value.array->size();
                if (N <= 0x17)
                {
                    write_number(static_cast<std::uint8_t>(0x80 + N));
                }
                else if (N <= (std::numeric_limits<std::uint8_t>::max)())
                {
                    oa->write_character(to_char_type(0x98));
                    write_number(static_cast<std::uint8_t>(N));
                }
                else if (N <= (std::numeric_limits<std::uint16_t>::max)())
                {
                    oa->write_character(to_char_type(0x99));
                    write_number(static_cast<std::uint16_t>(N));
                }
                else if (N <= (std::numeric_limits<std::uint32_t>::max)())
                {
                    oa->write_character(to_char_type(0x9A));
                    write_number(static_cast<std::uint32_t>(N));
                }
                // LCOV_EXCL_START
                else if (N <= (std::numeric_limits<std::uint64_t>::max)())
                {
                    oa->write_character(to_char_type(0x9B));
                    write_number(static_cast<std::uint64_t>(N));
                }
                // LCOV_EXCL_STOP

                // step 2: write each element
                for (const auto& el : *j.m_value.array)
                {
                    write_cbor(el);
                }
                break;
            }

            case value_t::object:
            {
                // step 1: write control byte and the object size
                const auto N = j.m_value.object->size();
                if (N <= 0x17)
                {
                    write_number(static_cast<std::uint8_t>(0xA0 + N));
                }
                else if (N <= (std::numeric_limits<std::uint8_t>::max)())
                {
                    oa->write_character(to_char_type(0xB8));
                    write_number(static_cast<std::uint8_t>(N));
                }
                else if (N <= (std::numeric_limits<std::uint16_t>::max)())
                {
                    oa->write_character(to_char_type(0xB9));
                    write_number(static_cast<std::uint16_t>(N));
                }
                else if (N <= (std::numeric_limits<std::uint32_t>::max)())
                {
                    oa->write_character(to_char_type(0xBA));
                    write_number(static_cast<std::uint32_t>(N));
                }
                // LCOV_EXCL_START
                else if (N <= (std::numeric_limits<std::uint64_t>::max)())
                {
                    oa->write_character(to_char_type(0xBB));
                    write_number(static_cast<std::uint64_t>(N));
                }
                // LCOV_EXCL_STOP

                // step 2: write each element
                for (const auto& el : *j.m_value.object)
                {
                    write_cbor(el.first);
                    write_cbor(el.second);
                }
                break;
            }

            default:
                break;
        }
    }

    /*!
    @param[in] j  JSON value to serialize
    */
    void write_msgpack(const BasicJsonType& j)
    {
        switch (j.type())
        {
            case value_t::null: // nil
            {
                oa->write_character(to_char_type(0xC0));
                break;
            }

            case value_t::boolean: // true and false
            {
                oa->write_character(j.m_value.boolean
                                    ? to_char_type(0xC3)
                                    : to_char_type(0xC2));
                break;
            }

            case value_t::number_integer:
            {
                if (j.m_value.number_integer >= 0)
                {
                    // MessagePack does not differentiate between positive
                    // signed integers and unsigned integers. Therefore, we used
                    // the code from the value_t::number_unsigned case here.
                    if (j.m_value.number_unsigned < 128)
                    {
                        // positive fixnum
                        write_number(static_cast<std::uint8_t>(j.m_value.number_integer));
                    }
                    else if (j.m_value.number_unsigned <= (std::numeric_limits<std::uint8_t>::max)())
                    {
                        // uint 8
                        oa->write_character(to_char_type(0xCC));
                        write_number(static_cast<std::uint8_t>(j.m_value.number_integer));
                    }
                    else if (j.m_value.number_unsigned <= (std::numeric_limits<std::uint16_t>::max)())
                    {
                        // uint 16
                        oa->write_character(to_char_type(0xCD));
                        write_number(static_cast<std::uint16_t>(j.m_value.number_integer));
                    }
                    else if (j.m_value.number_unsigned <= (std::numeric_limits<std::uint32_t>::max)())
                    {
                        // uint 32
                        oa->write_character(to_char_type(0xCE));
                        write_number(static_cast<std::uint32_t>(j.m_value.number_integer));
                    }
                    else if (j.m_value.number_unsigned <= (std::numeric_limits<std::uint64_t>::max)())
                    {
                        // uint 64
                        oa->write_character(to_char_type(0xCF));
                        write_number(static_cast<std::uint64_t>(j.m_value.number_integer));
                    }
                }
                else
                {
                    if (j.m_value.number_integer >= -32)
                    {
                        // negative fixnum
                        write_number(static_cast<std::int8_t>(j.m_value.number_integer));
                    }
                    else if (j.m_value.number_integer >= (std::numeric_limits<std::int8_t>::min)() and
                             j.m_value.number_integer <= (std::numeric_limits<std::int8_t>::max)())
                    {
                        // int 8
                        oa->write_character(to_char_type(0xD0));
                        write_number(static_cast<std::int8_t>(j.m_value.number_integer));
                    }
                    else if (j.m_value.number_integer >= (std::numeric_limits<std::int16_t>::min)() and
                             j.m_value.number_integer <= (std::numeric_limits<std::int16_t>::max)())
                    {
                        // int 16
                        oa->write_character(to_char_type(0xD1));
                        write_number(static_cast<std::int16_t>(j.m_value.number_integer));
                    }
                    else if (j.m_value.number_integer >= (std::numeric_limits<std::int32_t>::min)() and
                             j.m_value.number_integer <= (std::numeric_limits<std::int32_t>::max)())
                    {
                        // int 32
                        oa->write_character(to_char_type(0xD2));
                        write_number(static_cast<std::int32_t>(j.m_value.number_integer));
                    }
                    else if (j.m_value.number_integer >= (std::numeric_limits<std::int64_t>::min)() and
                             j.m_value.number_integer <= (std::numeric_limits<std::int64_t>::max)())
                    {
                        // int 64
                        oa->write_character(to_char_type(0xD3));
                        write_number(static_cast<std::int64_t>(j.m_value.number_integer));
                    }
                }
                break;
            }

            case value_t::number_unsigned:
            {
                if (j.m_value.number_unsigned < 128)
                {
                    // positive fixnum
                    write_number(static_cast<std::uint8_t>(j.m_value.number_integer));
                }
                else if (j.m_value.number_unsigned <= (std::numeric_limits<std::uint8_t>::max)())
                {
                    // uint 8
                    oa->write_character(to_char_type(0xCC));
                    write_number(static_cast<std::uint8_t>(j.m_value.number_integer));
                }
                else if (j.m_value.number_unsigned <= (std::numeric_limits<std::uint16_t>::max)())
                {
                    // uint 16
                    oa->write_character(to_char_type(0xCD));
                    write_number(static_cast<std::uint16_t>(j.m_value.number_integer));
                }
                else if (j.m_value.number_unsigned <= (std::numeric_limits<std::uint32_t>::max)())
                {
                    // uint 32
                    oa->write_character(to_char_type(0xCE));
                    write_number(static_cast<std::uint32_t>(j.m_value.number_integer));
                }
                else if (j.m_value.number_unsigned <= (std::numeric_limits<std::uint64_t>::max)())
                {
                    // uint 64
                    oa->write_character(to_char_type(0xCF));
                    write_number(static_cast<std::uint64_t>(j.m_value.number_integer));
                }
                break;
            }

            case value_t::number_float:
            {
                oa->write_character(get_msgpack_float_prefix(j.m_value.number_float));
                write_number(j.m_value.number_float);
                break;
            }

            case value_t::string:
            {
                // step 1: write control byte and the string length
                const auto N = j.m_value.string->size();
                if (N <= 31)
                {
                    // fixstr
                    write_number(static_cast<std::uint8_t>(0xA0 | N));
                }
                else if (N <= (std::numeric_limits<std::uint8_t>::max)())
                {
                    // str 8
                    oa->write_character(to_char_type(0xD9));
                    write_number(static_cast<std::uint8_t>(N));
                }
                else if (N <= (std::numeric_limits<std::uint16_t>::max)())
                {
                    // str 16
                    oa->write_character(to_char_type(0xDA));
                    write_number(static_cast<std::uint16_t>(N));
                }
                else if (N <= (std::numeric_limits<std::uint32_t>::max)())
                {
                    // str 32
                    oa->write_character(to_char_type(0xDB));
                    write_number(static_cast<std::uint32_t>(N));
                }

                // step 2: write the string
                oa->write_characters(
                    reinterpret_cast<const CharType*>(j.m_value.string->c_str()),
                    j.m_value.string->size());
                break;
            }

            case value_t::array:
            {
                // step 1: write control byte and the array size
                const auto N = j.m_value.array->size();
                if (N <= 15)
                {
                    // fixarray
                    write_number(static_cast<std::uint8_t>(0x90 | N));
                }
                else if (N <= (std::numeric_limits<std::uint16_t>::max)())
                {
                    // array 16
                    oa->write_character(to_char_type(0xDC));
                    write_number(static_cast<std::uint16_t>(N));
                }
                else if (N <= (std::numeric_limits<std::uint32_t>::max)())
                {
                    // array 32
                    oa->write_character(to_char_type(0xDD));
                    write_number(static_cast<std::uint32_t>(N));
                }

                // step 2: write each element
                for (const auto& el : *j.m_value.array)
                {
                    write_msgpack(el);
                }
                break;
            }

            case value_t::object:
            {
                // step 1: write control byte and the object size
                const auto N = j.m_value.object->size();
                if (N <= 15)
                {
                    // fixmap
                    write_number(static_cast<std::uint8_t>(0x80 | (N & 0xF)));
                }
                else if (N <= (std::numeric_limits<std::uint16_t>::max)())
                {
                    // map 16
                    oa->write_character(to_char_type(0xDE));
                    write_number(static_cast<std::uint16_t>(N));
                }
                else if (N <= (std::numeric_limits<std::uint32_t>::max)())
                {
                    // map 32
                    oa->write_character(to_char_type(0xDF));
                    write_number(static_cast<std::uint32_t>(N));
                }

                // step 2: write each element
                for (const auto& el : *j.m_value.object)
                {
                    write_msgpack(el.first);
                    write_msgpack(el.second);
                }
                break;
            }

            default:
                break;
        }
    }

    /*!
    @param[in] j  JSON value to serialize
    @param[in] use_count   whether to use '#' prefixes (optimized format)
    @param[in] use_type    whether to use '$' prefixes (optimized format)
    @param[in] add_prefix  whether prefixes need to be used for this value
    */
    void write_ubjson(const BasicJsonType& j, const bool use_count,
                      const bool use_type, const bool add_prefix = true)
    {
        switch (j.type())
        {
            case value_t::null:
            {
                if (add_prefix)
                {
                    oa->write_character(to_char_type('Z'));
                }
                break;
            }

            case value_t::boolean:
            {
                if (add_prefix)
                {
                    oa->write_character(j.m_value.boolean
                                        ? to_char_type('T')
                                        : to_char_type('F'));
                }
                break;
            }

            case value_t::number_integer:
            {
                write_number_with_ubjson_prefix(j.m_value.number_integer, add_prefix);
                break;
            }

            case value_t::number_unsigned:
            {
                write_number_with_ubjson_prefix(j.m_value.number_unsigned, add_prefix);
                break;
            }

            case value_t::number_float:
            {
                write_number_with_ubjson_prefix(j.m_value.number_float, add_prefix);
                break;
            }

            case value_t::string:
            {
                if (add_prefix)
                {
                    oa->write_character(to_char_type('S'));
                }
                write_number_with_ubjson_prefix(j.m_value.string->size(), true);
                oa->write_characters(
                    reinterpret_cast<const CharType*>(j.m_value.string->c_str()),
                    j.m_value.string->size());
                break;
            }

            case value_t::array:
            {
                if (add_prefix)
                {
                    oa->write_character(to_char_type('['));
                }

                bool prefix_required = true;
                if (use_type and not j.m_value.array->empty())
                {
                    assert(use_count);
                    const CharType first_prefix = ubjson_prefix(j.front());
                    const bool same_prefix = std::all_of(j.begin() + 1, j.end(),
                                                         [this, first_prefix](const BasicJsonType & v)
                    {
                        return ubjson_prefix(v) == first_prefix;
                    });

                    if (same_prefix)
                    {
                        prefix_required = false;
                        oa->write_character(to_char_type('$'));
                        oa->write_character(first_prefix);
                    }
                }

                if (use_count)
                {
                    oa->write_character(to_char_type('#'));
                    write_number_with_ubjson_prefix(j.m_value.array->size(), true);
                }

                for (const auto& el : *j.m_value.array)
                {
                    write_ubjson(el, use_count, use_type, prefix_required);
                }

                if (not use_count)
                {
                    oa->write_character(to_char_type(']'));
                }

                break;
            }

            case value_t::object:
            {
                if (add_prefix)
                {
                    oa->write_character(to_char_type('{'));
                }

                bool prefix_required = true;
                if (use_type and not j.m_value.object->empty())
                {
                    assert(use_count);
                    const CharType first_prefix = ubjson_prefix(j.front());
                    const bool same_prefix = std::all_of(j.begin(), j.end(),
                                                         [this, first_prefix](const BasicJsonType & v)
                    {
                        return ubjson_prefix(v) == first_prefix;
                    });

                    if (same_prefix)
                    {
                        prefix_required = false;
                        oa->write_character(to_char_type('$'));
                        oa->write_character(first_prefix);
                    }
                }

                if (use_count)
                {
                    oa->write_character(to_char_type('#'));
                    write_number_with_ubjson_prefix(j.m_value.object->size(), true);
                }

                for (const auto& el : *j.m_value.object)
                {
                    write_number_with_ubjson_prefix(el.first.size(), true);
                    oa->write_characters(
                        reinterpret_cast<const CharType*>(el.first.c_str()),
                        el.first.size());
                    write_ubjson(el.second, use_count, use_type, prefix_required);
                }

                if (not use_count)
                {
                    oa->write_character(to_char_type('}'));
                }

                break;
            }

            default:
                break;
        }
    }

  private:
    //////////
    // BSON //
    //////////

    /*!
    @return The size of a BSON document entry header, including the id marker
            and the entry name size (and its null-terminator).
    */
    static std::size_t calc_bson_entry_header_size(const string_t& name)
    {
        const auto it = name.find(static_cast<typename string_t::value_type>(0));
        if (JSON_HEDLEY_UNLIKELY(it != BasicJsonType::string_t::npos))
        {
            JSON_THROW(out_of_range::create(409,
                                            "BSON key cannot contain code point U+0000 (at byte " + std::to_string(it) + ")"));
        }

        return /*id*/ 1ul + name.size() + /*zero-terminator*/1u;
    }

    /*!
    @brief Writes the given @a element_type and @a name to the output adapter
    */
    void write_bson_entry_header(const string_t& name,
                                 const std::uint8_t element_type)
    {
        oa->write_character(to_char_type(element_type)); // boolean
        oa->write_characters(
            reinterpret_cast<const CharType*>(name.c_str()),
            name.size() + 1u);
    }

    /*!
    @brief Writes a BSON element with key @a name and boolean value @a value
    */
    void write_bson_boolean(const string_t& name,
                            const bool value)
    {
        write_bson_entry_header(name, 0x08);
        oa->write_character(value ? to_char_type(0x01) : to_char_type(0x00));
    }

    /*!
    @brief Writes a BSON element with key @a name and double value @a value
    */
    void write_bson_double(const string_t& name,
                           const double value)
    {
        write_bson_entry_header(name, 0x01);
        write_number<double, true>(value);
    }

    /*!
    @return The size of the BSON-encoded string in @a value
    */
    static std::size_t calc_bson_string_size(const string_t& value)
    {
        return sizeof(std::int32_t) + value.size() + 1ul;
    }

    /*!
    @brief Writes a BSON element with key @a name and string value @a value
    */
    void write_bson_string(const string_t& name,
                           const string_t& value)
    {
        write_bson_entry_header(name, 0x02);

        write_number<std::int32_t, true>(static_cast<std::int32_t>(value.size() + 1ul));
        oa->write_characters(
            reinterpret_cast<const CharType*>(value.c_str()),
            value.size() + 1);
    }

    /*!
    @brief Writes a BSON element with key @a name and null value
    */
    void write_bson_null(const string_t& name)
    {
        write_bson_entry_header(name, 0x0A);
    }

    /*!
    @return The size of the BSON-encoded integer @a value
    */
    static std::size_t calc_bson_integer_size(const std::int64_t value)
    {
        return (std::numeric_limits<std::int32_t>::min)() <= value and value <= (std::numeric_limits<std::int32_t>::max)()
               ? sizeof(std::int32_t)
               : sizeof(std::int64_t);
    }

    /*!
    @brief Writes a BSON element with key @a name and integer @a value
    */
    void write_bson_integer(const string_t& name,
                            const std::int64_t value)
    {
        if ((std::numeric_limits<std::int32_t>::min)() <= value and value <= (std::numeric_limits<std::int32_t>::max)())
        {
            write_bson_entry_header(name, 0x10); // int32
            write_number<std::int32_t, true>(static_cast<std::int32_t>(value));
        }
        else
        {
            write_bson_entry_header(name, 0x12); // int64
            write_number<std::int64_t, true>(static_cast<std::int64_t>(value));
        }
    }

    /*!
    @return The size of the BSON-encoded unsigned integer in @a j
    */
    static constexpr std::size_t calc_bson_unsigned_size(const std::uint64_t value) noexcept
    {
        return (value <= static_cast<std::uint64_t>((std::numeric_limits<std::int32_t>::max)()))
               ? sizeof(std::int32_t)
               : sizeof(std::int64_t);
    }

    /*!
    @brief Writes a BSON element with key @a name and unsigned @a value
    */
    void write_bson_unsigned(const string_t& name,
                             const std::uint64_t value)
    {
        if (value <= static_cast<std::uint64_t>((std::numeric_limits<std::int32_t>::max)()))
        {
            write_bson_entry_header(name, 0x10 /* int32 */);
            write_number<std::int32_t, true>(static_cast<std::int32_t>(value));
        }
        else if (value <= static_cast<std::uint64_t>((std::numeric_limits<std::int64_t>::max)()))
        {
            write_bson_entry_header(name, 0x12 /* int64 */);
            write_number<std::int64_t, true>(static_cast<std::int64_t>(value));
        }
        else
        {
            JSON_THROW(out_of_range::create(407, "integer number " + std::to_string(value) + " cannot be represented by BSON as it does not fit int64"));
        }
    }

    /*!
    @brief Writes a BSON element with key @a name and object @a value
    */
    void write_bson_object_entry(const string_t& name,
                                 const typename BasicJsonType::object_t& value)
    {
        write_bson_entry_header(name, 0x03); // object
        write_bson_object(value);
    }

    /*!
    @return The size of the BSON-encoded array @a value
    */
    static std::size_t calc_bson_array_size(const typename BasicJsonType::array_t& value)
    {
        std::size_t array_index = 0ul;

        const std::size_t embedded_document_size = std::accumulate(std::begin(value), std::end(value), std::size_t(0), [&array_index](std::size_t result, const typename BasicJsonType::array_t::value_type & el)
        {
            return result + calc_bson_element_size(std::to_string(array_index++), el);
        });

        return sizeof(std::int32_t) + embedded_document_size + 1ul;
    }

    /*!
    @brief Writes a BSON element with key @a name and array @a value
    */
    void write_bson_array(const string_t& name,
                          const typename BasicJsonType::array_t& value)
    {
        write_bson_entry_header(name, 0x04); // array
        write_number<std::int32_t, true>(static_cast<std::int32_t>(calc_bson_array_size(value)));

        std::size_t array_index = 0ul;

        for (const auto& el : value)
        {
            write_bson_element(std::to_string(array_index++), el);
        }

        oa->write_character(to_char_type(0x00));
    }

    /*!
    @brief Calculates the size necessary to serialize the JSON value @a j with its @a name
    @return The calculated size for the BSON document entry for @a j with the given @a name.
    */
    static std::size_t calc_bson_element_size(const string_t& name,
            const BasicJsonType& j)
    {
        const auto header_size = calc_bson_entry_header_size(name);
        switch (j.type())
        {
            case value_t::object:
                return header_size + calc_bson_object_size(*j.m_value.object);

            case value_t::array:
                return header_size + calc_bson_array_size(*j.m_value.array);

            case value_t::boolean:
                return header_size + 1ul;

            case value_t::number_float:
                return header_size + 8ul;

            case value_t::number_integer:
                return header_size + calc_bson_integer_size(j.m_value.number_integer);

            case value_t::number_unsigned:
                return header_size + calc_bson_unsigned_size(j.m_value.number_unsigned);

            case value_t::string:
                return header_size + calc_bson_string_size(*j.m_value.string);

            case value_t::null:
                return header_size + 0ul;

            // LCOV_EXCL_START
            default:
                assert(false);
                return 0ul;
                // LCOV_EXCL_STOP
        }
    }

    /*!
    @brief Serializes the JSON value @a j to BSON and associates it with the
           key @a name.
    @param name The name to associate with the JSON entity @a j within the
                current BSON document
    @return The size of the BSON entry
    */
    void write_bson_element(const string_t& name,
                            const BasicJsonType& j)
    {
        switch (j.type())
        {
            case value_t::object:
                return write_bson_object_entry(name, *j.m_value.object);

            case value_t::array:
                return write_bson_array(name, *j.m_value.array);

            case value_t::boolean:
                return write_bson_boolean(name, j.m_value.boolean);

            case value_t::number_float:
                return write_bson_double(name, j.m_value.number_float);

            case value_t::number_integer:
                return write_bson_integer(name, j.m_value.number_integer);

            case value_t::number_unsigned:
                return write_bson_unsigned(name, j.m_value.number_unsigned);

            case value_t::string:
                return write_bson_string(name, *j.m_value.string);

            case value_t::null:
                return write_bson_null(name);

            // LCOV_EXCL_START
            default:
                assert(false);
                return;
                // LCOV_EXCL_STOP
        }
    }

    /*!
    @brief Calculates the size of the BSON serialization of the given
           JSON-object @a j.
    @param[in] j  JSON value to serialize
    @pre       j.type() == value_t::object
    */
    static std::size_t calc_bson_object_size(const typename BasicJsonType::object_t& value)
    {
        std::size_t document_size = std::accumulate(value.begin(), value.end(), std::size_t(0),
                                    [](size_t result, const typename BasicJsonType::object_t::value_type & el)
        {
            return result += calc_bson_element_size(el.first, el.second);
        });

        return sizeof(std::int32_t) + document_size + 1ul;
    }

    /*!
    @param[in] j  JSON value to serialize
    @pre       j.type() == value_t::object
    */
    void write_bson_object(const typename BasicJsonType::object_t& value)
    {
        write_number<std::int32_t, true>(static_cast<std::int32_t>(calc_bson_object_size(value)));

        for (const auto& el : value)
        {
            write_bson_element(el.first, el.second);
        }

        oa->write_character(to_char_type(0x00));
    }

    //////////
    // CBOR //
    //////////

    static constexpr CharType get_cbor_float_prefix(float /*unused*/)
    {
        return to_char_type(0xFA);  // Single-Precision Float
    }

    static constexpr CharType get_cbor_float_prefix(double /*unused*/)
    {
        return to_char_type(0xFB);  // Double-Precision Float
    }

    /////////////
    // MsgPack //
    /////////////

    static constexpr CharType get_msgpack_float_prefix(float /*unused*/)
    {
        return to_char_type(0xCA);  // float 32
    }

    static constexpr CharType get_msgpack_float_prefix(double /*unused*/)
    {
        return to_char_type(0xCB);  // float 64
    }

    ////////////
    // UBJSON //
    ////////////

    // UBJSON: write number (floating point)
    template<typename NumberType, typename std::enable_if<
                 std::is_floating_point<NumberType>::value, int>::type = 0>
    void write_number_with_ubjson_prefix(const NumberType n,
                                         const bool add_prefix)
    {
        if (add_prefix)
        {
            oa->write_character(get_ubjson_float_prefix(n));
        }
        write_number(n);
    }

    // UBJSON: write number (unsigned integer)
    template<typename NumberType, typename std::enable_if<
                 std::is_unsigned<NumberType>::value, int>::type = 0>
    void write_number_with_ubjson_prefix(const NumberType n,
                                         const bool add_prefix)
    {
        if (n <= static_cast<std::uint64_t>((std::numeric_limits<std::int8_t>::max)()))
        {
            if (add_prefix)
            {
                oa->write_character(to_char_type('i'));  // int8
            }
            write_number(static_cast<std::uint8_t>(n));
        }
        else if (n <= (std::numeric_limits<std::uint8_t>::max)())
        {
            if (add_prefix)
            {
                oa->write_character(to_char_type('U'));  // uint8
            }
            write_number(static_cast<std::uint8_t>(n));
        }
        else if (n <= static_cast<std::uint64_t>((std::numeric_limits<std::int16_t>::max)()))
        {
            if (add_prefix)
            {
                oa->write_character(to_char_type('I'));  // int16
            }
            write_number(static_cast<std::int16_t>(n));
        }
        else if (n <= static_cast<std::uint64_t>((std::numeric_limits<std::int32_t>::max)()))
        {
            if (add_prefix)
            {
                oa->write_character(to_char_type('l'));  // int32
            }
            write_number(static_cast<std::int32_t>(n));
        }
        else if (n <= static_cast<std::uint64_t>((std::numeric_limits<std::int64_t>::max)()))
        {
            if (add_prefix)
            {
                oa->write_character(to_char_type('L'));  // int64
            }
            write_number(static_cast<std::int64_t>(n));
        }
        else
        {
            JSON_THROW(out_of_range::create(407, "integer number " + std::to_string(n) + " cannot be represented by UBJSON as it does not fit int64"));
        }
    }

    // UBJSON: write number (signed integer)
    template<typename NumberType, typename std::enable_if<
                 std::is_signed<NumberType>::value and
                 not std::is_floating_point<NumberType>::value, int>::type = 0>
    void write_number_with_ubjson_prefix(const NumberType n,
                                         const bool add_prefix)
    {
        if ((std::numeric_limits<std::int8_t>::min)() <= n and n <= (std::numeric_limits<std::int8_t>::max)())
        {
            if (add_prefix)
            {
                oa->write_character(to_char_type('i'));  // int8
            }
            write_number(static_cast<std::int8_t>(n));
        }
        else if (static_cast<std::int64_t>((std::numeric_limits<std::uint8_t>::min)()) <= n and n <= static_cast<std::int64_t>((std::numeric_limits<std::uint8_t>::max)()))
        {
            if (add_prefix)
            {
                oa->write_character(to_char_type('U'));  // uint8
            }
            write_number(static_cast<std::uint8_t>(n));
        }
        else if ((std::numeric_limits<std::int16_t>::min)() <= n and n <= (std::numeric_limits<std::int16_t>::max)())
        {
            if (add_prefix)
            {
                oa->write_character(to_char_type('I'));  // int16
            }
            write_number(static_cast<std::int16_t>(n));
        }
        else if ((std::numeric_limits<std::int32_t>::min)() <= n and n <= (std::numeric_limits<std::int32_t>::max)())
        {
            if (add_prefix)
            {
                oa->write_character(to_char_type('l'));  // int32
            }
            write_number(static_cast<std::int32_t>(n));
        }
        else if ((std::numeric_limits<std::int64_t>::min)() <= n and n <= (std::numeric_limits<std::int64_t>::max)())
        {
            if (add_prefix)
            {
                oa->write_character(to_char_type('L'));  // int64
            }
            write_number(static_cast<std::int64_t>(n));
        }
        // LCOV_EXCL_START
        else
        {
            JSON_THROW(out_of_range::create(407, "integer number " + std::to_string(n) + " cannot be represented by UBJSON as it does not fit int64"));
        }
        // LCOV_EXCL_STOP
    }

    /*!
    @brief determine the type prefix of container values

    @note This function does not need to be 100% accurate when it comes to
          integer limits. In case a number exceeds the limits of int64_t,
          this will be detected by a later call to function
          write_number_with_ubjson_prefix. Therefore, we return 'L' for any
          value that does not fit the previous limits.
    */
    CharType ubjson_prefix(const BasicJsonType& j) const noexcept
    {
        switch (j.type())
        {
            case value_t::null:
                return 'Z';

            case value_t::boolean:
                return j.m_value.boolean ? 'T' : 'F';

            case value_t::number_integer:
            {
                if ((std::numeric_limits<std::int8_t>::min)() <= j.m_value.number_integer and j.m_value.number_integer <= (std::numeric_limits<std::int8_t>::max)())
                {
                    return 'i';
                }
                if ((std::numeric_limits<std::uint8_t>::min)() <= j.m_value.number_integer and j.m_value.number_integer <= (std::numeric_limits<std::uint8_t>::max)())
                {
                    return 'U';
                }
                if ((std::numeric_limits<std::int16_t>::min)() <= j.m_value.number_integer and j.m_value.number_integer <= (std::numeric_limits<std::int16_t>::max)())
                {
                    return 'I';
                }
                if ((std::numeric_limits<std::int32_t>::min)() <= j.m_value.number_integer and j.m_value.number_integer <= (std::numeric_limits<std::int32_t>::max)())
                {
                    return 'l';
                }
                // no check and assume int64_t (see note above)
                return 'L';
            }

            case value_t::number_unsigned:
            {
                if (j.m_value.number_unsigned <= static_cast<std::uint64_t>((std::numeric_limits<std::int8_t>::max)()))
                {
                    return 'i';
                }
                if (j.m_value.number_unsigned <= static_cast<std::uint64_t>((std::numeric_limits<std::uint8_t>::max)()))
                {
                    return 'U';
                }
                if (j.m_value.number_unsigned <= static_cast<std::uint64_t>((std::numeric_limits<std::int16_t>::max)()))
                {
                    return 'I';
                }
                if (j.m_value.number_unsigned <= static_cast<std::uint64_t>((std::numeric_limits<std::int32_t>::max)()))
                {
                    return 'l';
                }
                // no check and assume int64_t (see note above)
                return 'L';
            }

            case value_t::number_float:
                return get_ubjson_float_prefix(j.m_value.number_float);

            case value_t::string:
                return 'S';

            case value_t::array:
                return '[';

            case value_t::object:
                return '{';

            default:  // discarded values
                return 'N';
        }
    }

    static constexpr CharType get_ubjson_float_prefix(float /*unused*/)
    {
        return 'd';  // float 32
    }

    static constexpr CharType get_ubjson_float_prefix(double /*unused*/)
    {
        return 'D';  // float 64
    }

    ///////////////////////
    // Utility functions //
    ///////////////////////

    /*
    @brief write a number to output input
    @param[in] n number of type @a NumberType
    @tparam NumberType the type of the number
    @tparam OutputIsLittleEndian Set to true if output data is
                                 required to be little endian

    @note This function needs to respect the system's endianess, because bytes
          in CBOR, MessagePack, and UBJSON are stored in network order (big
          endian) and therefore need reordering on little endian systems.
    */
    template<typename NumberType, bool OutputIsLittleEndian = false>
    void write_number(const NumberType n)
    {
        // step 1: write number to array of length NumberType
        std::array<CharType, sizeof(NumberType)> vec;
        std::memcpy(vec.data(), &n, sizeof(NumberType));

        // step 2: write array to output (with possible reordering)
        if (is_little_endian != OutputIsLittleEndian)
        {
            // reverse byte order prior to conversion if necessary
            std::reverse(vec.begin(), vec.end());
        }

        oa->write_characters(vec.data(), sizeof(NumberType));
    }

  public:
    // The following to_char_type functions are implement the conversion
    // between uint8_t and CharType. In case CharType is not unsigned,
    // such a conversion is required to allow values greater than 128.
    // See <https://github.com/nlohmann/json/issues/1286> for a discussion.
    template < typename C = CharType,
               enable_if_t < std::is_signed<C>::value and std::is_signed<char>::value > * = nullptr >
    static constexpr CharType to_char_type(std::uint8_t x) noexcept
    {
        return *reinterpret_cast<char*>(&x);
    }

    template < typename C = CharType,
               enable_if_t < std::is_signed<C>::value and std::is_unsigned<char>::value > * = nullptr >
    static CharType to_char_type(std::uint8_t x) noexcept
    {
        static_assert(sizeof(std::uint8_t) == sizeof(CharType), "size of CharType must be equal to std::uint8_t");
        static_assert(std::is_pod<CharType>::value, "CharType must be POD");
        CharType result;
        std::memcpy(&result, &x, sizeof(x));
        return result;
    }

    template<typename C = CharType,
             enable_if_t<std::is_unsigned<C>::value>* = nullptr>
    static constexpr CharType to_char_type(std::uint8_t x) noexcept
    {
        return x;
    }

    template < typename InputCharType, typename C = CharType,
               enable_if_t <
                   std::is_signed<C>::value and
                   std::is_signed<char>::value and
                   std::is_same<char, typename std::remove_cv<InputCharType>::type>::value
                   > * = nullptr >
    static constexpr CharType to_char_type(InputCharType x) noexcept
    {
        return x;
    }

  private:
    /// whether we can assume little endianess
    const bool is_little_endian = binary_reader<BasicJsonType>::little_endianess();

    /// the output
    output_adapter_t<CharType> oa = nullptr;
};
}  // namespace detail
}  // namespace nlohmann

// #include <nlohmann/detail/output/output_adapters.hpp>

// #include <nlohmann/detail/output/serializer.hpp>


#include <algorithm> // reverse, remove, fill, find, none_of
#include <array> // array
#include <cassert> // assert
#include <ciso646> // and, or
#include <clocale> // localeconv, lconv
#include <cmath> // labs, isfinite, isnan, signbit
#include <cstddef> // size_t, ptrdiff_t
#include <cstdint> // uint8_t
#include <cstdio> // snprintf
#include <limits> // numeric_limits
#include <string> // string
#include <type_traits> // is_same
#include <utility> // move

// #include <nlohmann/detail/conversions/to_chars.hpp>


#include <array> // array
#include <cassert> // assert
#include <ciso646> // or, and, not
#include <cmath>   // signbit, isfinite
#include <cstdint> // intN_t, uintN_t
#include <cstring> // memcpy, memmove
#include <limits> // numeric_limits
#include <type_traits> // conditional
// #include <nlohmann/detail/macro_scope.hpp>


namespace nlohmann
{
namespace detail
{

/*!
@brief implements the Grisu2 algorithm for binary to decimal floating-point
conversion.

This implementation is a slightly modified version of the reference
implementation which may be obtained from
http://florian.loitsch.com/publications (bench.tar.gz).

The code is distributed under the MIT license, Copyright (c) 2009 Florian Loitsch.

For a detailed description of the algorithm see:

[1] Loitsch, "Printing Floating-Point Numbers Quickly and Accurately with
    Integers", Proceedings of the ACM SIGPLAN 2010 Conference on Programming
    Language Design and Implementation, PLDI 2010
[2] Burger, Dybvig, "Printing Floating-Point Numbers Quickly and Accurately",
    Proceedings of the ACM SIGPLAN 1996 Conference on Programming Language
    Design and Implementation, PLDI 1996
*/
namespace dtoa_impl
{

template <typename Target, typename Source>
Target reinterpret_bits(const Source source)
{
    static_assert(sizeof(Target) == sizeof(Source), "size mismatch");

    Target target;
    std::memcpy(&target, &source, sizeof(Source));
    return target;
}

struct diyfp // f * 2^e
{
    static constexpr int kPrecision = 64; // = q

    std::uint64_t f = 0;
    int e = 0;

    constexpr diyfp(std::uint64_t f_, int e_) noexcept : f(f_), e(e_) {}

    /*!
    @brief returns x - y
    @pre x.e == y.e and x.f >= y.f
    */
    static diyfp sub(const diyfp& x, const diyfp& y) noexcept
    {
        assert(x.e == y.e);
        assert(x.f >= y.f);

        return {x.f - y.f, x.e};
    }

    /*!
    @brief returns x * y
    @note The result is rounded. (Only the upper q bits are returned.)
    */
    static diyfp mul(const diyfp& x, const diyfp& y) noexcept
    {
        static_assert(kPrecision == 64, "internal error");

        // Computes:
        //  f = round((x.f * y.f) / 2^q)
        //  e = x.e + y.e + q

        // Emulate the 64-bit * 64-bit multiplication:
        //
        // p = u * v
        //   = (u_lo + 2^32 u_hi) (v_lo + 2^32 v_hi)
        //   = (u_lo v_lo         ) + 2^32 ((u_lo v_hi         ) + (u_hi v_lo         )) + 2^64 (u_hi v_hi         )
        //   = (p0                ) + 2^32 ((p1                ) + (p2                )) + 2^64 (p3                )
        //   = (p0_lo + 2^32 p0_hi) + 2^32 ((p1_lo + 2^32 p1_hi) + (p2_lo + 2^32 p2_hi)) + 2^64 (p3                )
        //   = (p0_lo             ) + 2^32 (p0_hi + p1_lo + p2_lo                      ) + 2^64 (p1_hi + p2_hi + p3)
        //   = (p0_lo             ) + 2^32 (Q                                          ) + 2^64 (H                 )
        //   = (p0_lo             ) + 2^32 (Q_lo + 2^32 Q_hi                           ) + 2^64 (H                 )
        //
        // (Since Q might be larger than 2^32 - 1)
        //
        //   = (p0_lo + 2^32 Q_lo) + 2^64 (Q_hi + H)
        //
        // (Q_hi + H does not overflow a 64-bit int)
        //
        //   = p_lo + 2^64 p_hi

        const std::uint64_t u_lo = x.f & 0xFFFFFFFFu;
        const std::uint64_t u_hi = x.f >> 32u;
        const std::uint64_t v_lo = y.f & 0xFFFFFFFFu;
        const std::uint64_t v_hi = y.f >> 32u;

        const std::uint64_t p0 = u_lo * v_lo;
        const std::uint64_t p1 = u_lo * v_hi;
        const std::uint64_t p2 = u_hi * v_lo;
        const std::uint64_t p3 = u_hi * v_hi;

        const std::uint64_t p0_hi = p0 >> 32u;
        const std::uint64_t p1_lo = p1 & 0xFFFFFFFFu;
        const std::uint64_t p1_hi = p1 >> 32u;
        const std::uint64_t p2_lo = p2 & 0xFFFFFFFFu;
        const std::uint64_t p2_hi = p2 >> 32u;

        std::uint64_t Q = p0_hi + p1_lo + p2_lo;

        // The full product might now be computed as
        //
        // p_hi = p3 + p2_hi + p1_hi + (Q >> 32)
        // p_lo = p0_lo + (Q << 32)
        //
        // But in this particular case here, the full p_lo is not required.
        // Effectively we only need to add the highest bit in p_lo to p_hi (and
        // Q_hi + 1 does not overflow).

        Q += std::uint64_t{1} << (64u - 32u - 1u); // round, ties up

        const std::uint64_t h = p3 + p2_hi + p1_hi + (Q >> 32u);

        return {h, x.e + y.e + 64};
    }

    /*!
    @brief normalize x such that the significand is >= 2^(q-1)
    @pre x.f != 0
    */
    static diyfp normalize(diyfp x) noexcept
    {
        assert(x.f != 0);

        while ((x.f >> 63u) == 0)
        {
            x.f <<= 1u;
            x.e--;
        }

        return x;
    }

    /*!
    @brief normalize x such that the result has the exponent E
    @pre e >= x.e and the upper e - x.e bits of x.f must be zero.
    */
    static diyfp normalize_to(const diyfp& x, const int target_exponent) noexcept
    {
        const int delta = x.e - target_exponent;

        assert(delta >= 0);
        assert(((x.f << delta) >> delta) == x.f);

        return {x.f << delta, target_exponent};
    }
};

struct boundaries
{
    diyfp w;
    diyfp minus;
    diyfp plus;
};

/*!
Compute the (normalized) diyfp representing the input number 'value' and its
boundaries.

@pre value must be finite and positive
*/
template <typename FloatType>
boundaries compute_boundaries(FloatType value)
{
    assert(std::isfinite(value));
    assert(value > 0);

    // Convert the IEEE representation into a diyfp.
    //
    // If v is denormal:
    //      value = 0.F * 2^(1 - bias) = (          F) * 2^(1 - bias - (p-1))
    // If v is normalized:
    //      value = 1.F * 2^(E - bias) = (2^(p-1) + F) * 2^(E - bias - (p-1))

    static_assert(std::numeric_limits<FloatType>::is_iec559,
                  "internal error: dtoa_short requires an IEEE-754 floating-point implementation");

    constexpr int      kPrecision = std::numeric_limits<FloatType>::digits; // = p (includes the hidden bit)
    constexpr int      kBias      = std::numeric_limits<FloatType>::max_exponent - 1 + (kPrecision - 1);
    constexpr int      kMinExp    = 1 - kBias;
    constexpr std::uint64_t kHiddenBit = std::uint64_t{1} << (kPrecision - 1); // = 2^(p-1)

    using bits_type = typename std::conditional<kPrecision == 24, std::uint32_t, std::uint64_t >::type;

    const std::uint64_t bits = reinterpret_bits<bits_type>(value);
    const std::uint64_t E = bits >> (kPrecision - 1);
    const std::uint64_t F = bits & (kHiddenBit - 1);

    const bool is_denormal = E == 0;
    const diyfp v = is_denormal
                    ? diyfp(F, kMinExp)
                    : diyfp(F + kHiddenBit, static_cast<int>(E) - kBias);

    // Compute the boundaries m- and m+ of the floating-point value
    // v = f * 2^e.
    //
    // Determine v- and v+, the floating-point predecessor and successor if v,
    // respectively.
    //
    //      v- = v - 2^e        if f != 2^(p-1) or e == e_min                (A)
    //         = v - 2^(e-1)    if f == 2^(p-1) and e > e_min                (B)
    //
    //      v+ = v + 2^e
    //
    // Let m- = (v- + v) / 2 and m+ = (v + v+) / 2. All real numbers _strictly_
    // between m- and m+ round to v, regardless of how the input rounding
    // algorithm breaks ties.
    //
    //      ---+-------------+-------------+-------------+-------------+---  (A)
    //         v-            m-            v             m+            v+
    //
    //      -----------------+------+------+-------------+-------------+---  (B)
    //                       v-     m-     v             m+            v+

    const bool lower_boundary_is_closer = F == 0 and E > 1;
    const diyfp m_plus = diyfp(2 * v.f + 1, v.e - 1);
    const diyfp m_minus = lower_boundary_is_closer
                          ? diyfp(4 * v.f - 1, v.e - 2)  // (B)
                          : diyfp(2 * v.f - 1, v.e - 1); // (A)

    // Determine the normalized w+ = m+.
    const diyfp w_plus = diyfp::normalize(m_plus);

    // Determine w- = m- such that e_(w-) = e_(w+).
    const diyfp w_minus = diyfp::normalize_to(m_minus, w_plus.e);

    return {diyfp::normalize(v), w_minus, w_plus};
}

// Given normalized diyfp w, Grisu needs to find a (normalized) cached
// power-of-ten c, such that the exponent of the product c * w = f * 2^e lies
// within a certain range [alpha, gamma] (Definition 3.2 from [1])
//
//      alpha <= e = e_c + e_w + q <= gamma
//
// or
//
//      f_c * f_w * 2^alpha <= f_c 2^(e_c) * f_w 2^(e_w) * 2^q
//                          <= f_c * f_w * 2^gamma
//
// Since c and w are normalized, i.e. 2^(q-1) <= f < 2^q, this implies
//
//      2^(q-1) * 2^(q-1) * 2^alpha <= c * w * 2^q < 2^q * 2^q * 2^gamma
//
// or
//
//      2^(q - 2 + alpha) <= c * w < 2^(q + gamma)
//
// The choice of (alpha,gamma) determines the size of the table and the form of
// the digit generation procedure. Using (alpha,gamma)=(-60,-32) works out well
// in practice:
//
// The idea is to cut the number c * w = f * 2^e into two parts, which can be
// processed independently: An integral part p1, and a fractional part p2:
//
//      f * 2^e = ( (f div 2^-e) * 2^-e + (f mod 2^-e) ) * 2^e
//              = (f div 2^-e) + (f mod 2^-e) * 2^e
//              = p1 + p2 * 2^e
//
// The conversion of p1 into decimal form requires a series of divisions and
// modulos by (a power of) 10. These operations are faster for 32-bit than for
// 64-bit integers, so p1 should ideally fit into a 32-bit integer. This can be
// achieved by choosing
//
//      -e >= 32   or   e <= -32 := gamma
//
// In order to convert the fractional part
//
//      p2 * 2^e = p2 / 2^-e = d[-1] / 10^1 + d[-2] / 10^2 + ...
//
// into decimal form, the fraction is repeatedly multiplied by 10 and the digits
// d[-i] are extracted in order:
//
//      (10 * p2) div 2^-e = d[-1]
//      (10 * p2) mod 2^-e = d[-2] / 10^1 + ...
//
// The multiplication by 10 must not overflow. It is sufficient to choose
//
//      10 * p2 < 16 * p2 = 2^4 * p2 <= 2^64.
//
// Since p2 = f mod 2^-e < 2^-e,
//
//      -e <= 60   or   e >= -60 := alpha

constexpr int kAlpha = -60;
constexpr int kGamma = -32;

struct cached_power // c = f * 2^e ~= 10^k
{
    std::uint64_t f;
    int e;
    int k;
};

/*!
For a normalized diyfp w = f * 2^e, this function returns a (normalized) cached
power-of-ten c = f_c * 2^e_c, such that the exponent of the product w * c
satisfies (Definition 3.2 from [1])

     alpha <= e_c + e + q <= gamma.
*/
inline cached_power get_cached_power_for_binary_exponent(int e)
{
    // Now
    //
    //      alpha <= e_c + e + q <= gamma                                    (1)
    //      ==> f_c * 2^alpha <= c * 2^e * 2^q
    //
    // and since the c's are normalized, 2^(q-1) <= f_c,
    //
    //      ==> 2^(q - 1 + alpha) <= c * 2^(e + q)
    //      ==> 2^(alpha - e - 1) <= c
    //
    // If c were an exact power of ten, i.e. c = 10^k, one may determine k as
    //
    //      k = ceil( log_10( 2^(alpha - e - 1) ) )
    //        = ceil( (alpha - e - 1) * log_10(2) )
    //
    // From the paper:
    // "In theory the result of the procedure could be wrong since c is rounded,
    //  and the computation itself is approximated [...]. In practice, however,
    //  this simple function is sufficient."
    //
    // For IEEE double precision floating-point numbers converted into
    // normalized diyfp's w = f * 2^e, with q = 64,
    //
    //      e >= -1022      (min IEEE exponent)
    //           -52        (p - 1)
    //           -52        (p - 1, possibly normalize denormal IEEE numbers)
    //           -11        (normalize the diyfp)
    //         = -1137
    //
    // and
    //
    //      e <= +1023      (max IEEE exponent)
    //           -52        (p - 1)
    //           -11        (normalize the diyfp)
    //         = 960
    //
    // This binary exponent range [-1137,960] results in a decimal exponent
    // range [-307,324]. One does not need to store a cached power for each
    // k in this range. For each such k it suffices to find a cached power
    // such that the exponent of the product lies in [alpha,gamma].
    // This implies that the difference of the decimal exponents of adjacent
    // table entries must be less than or equal to
    //
    //      floor( (gamma - alpha) * log_10(2) ) = 8.
    //
    // (A smaller distance gamma-alpha would require a larger table.)

    // NB:
    // Actually this function returns c, such that -60 <= e_c + e + 64 <= -34.

    constexpr int kCachedPowersMinDecExp = -300;
    constexpr int kCachedPowersDecStep = 8;

    static constexpr std::array<cached_power, 79> kCachedPowers =
    {
        {
            { 0xAB70FE17C79AC6CA, -1060, -300 },
            { 0xFF77B1FCBEBCDC4F, -1034, -292 },
            { 0xBE5691EF416BD60C, -1007, -284 },
            { 0x8DD01FAD907FFC3C,  -980, -276 },
            { 0xD3515C2831559A83,  -954, -268 },
            { 0x9D71AC8FADA6C9B5,  -927, -260 },
            { 0xEA9C227723EE8BCB,  -901, -252 },
            { 0xAECC49914078536D,  -874, -244 },
            { 0x823C12795DB6CE57,  -847, -236 },
            { 0xC21094364DFB5637,  -821, -228 },
            { 0x9096EA6F3848984F,  -794, -220 },
            { 0xD77485CB25823AC7,  -768, -212 },
            { 0xA086CFCD97BF97F4,  -741, -204 },
            { 0xEF340A98172AACE5,  -715, -196 },
            { 0xB23867FB2A35B28E,  -688, -188 },
            { 0x84C8D4DFD2C63F3B,  -661, -180 },
            { 0xC5DD44271AD3CDBA,  -635, -172 },
            { 0x936B9FCEBB25C996,  -608, -164 },
            { 0xDBAC6C247D62A584,  -582, -156 },
            { 0xA3AB66580D5FDAF6,  -555, -148 },
            { 0xF3E2F893DEC3F126,  -529, -140 },
            { 0xB5B5ADA8AAFF80B8,  -502, -132 },
            { 0x87625F056C7C4A8B,  -475, -124 },
            { 0xC9BCFF6034C13053,  -449, -116 },
            { 0x964E858C91BA2655,  -422, -108 },
            { 0xDFF9772470297EBD,  -396, -100 },
            { 0xA6DFBD9FB8E5B88F,  -369,  -92 },
            { 0xF8A95FCF88747D94,  -343,  -84 },
            { 0xB94470938FA89BCF,  -316,  -76 },
            { 0x8A08F0F8BF0F156B,  -289,  -68 },
            { 0xCDB02555653131B6,  -263,  -60 },
            { 0x993FE2C6D07B7FAC,  -236,  -52 },
            { 0xE45C10C42A2B3B06,  -210,  -44 },
            { 0xAA242499697392D3,  -183,  -36 },
            { 0xFD87B5F28300CA0E,  -157,  -28 },
            { 0xBCE5086492111AEB,  -130,  -20 },
            { 0x8CBCCC096F5088CC,  -103,  -12 },
            { 0xD1B71758E219652C,   -77,   -4 },
            { 0x9C40000000000000,   -50,    4 },
            { 0xE8D4A51000000000,   -24,   12 },
            { 0xAD78EBC5AC620000,     3,   20 },
            { 0x813F3978F8940984,    30,   28 },
            { 0xC097CE7BC90715B3,    56,   36 },
            { 0x8F7E32CE7BEA5C70,    83,   44 },
            { 0xD5D238A4ABE98068,   109,   52 },
            { 0x9F4F2726179A2245,   136,   60 },
            { 0xED63A231D4C4FB27,   162,   68 },
            { 0xB0DE65388CC8ADA8,   189,   76 },
            { 0x83C7088E1AAB65DB,   216,   84 },
            { 0xC45D1DF942711D9A,   242,   92 },
            { 0x924D692CA61BE758,   269,  100 },
            { 0xDA01EE641A708DEA,   295,  108 },
            { 0xA26DA3999AEF774A,   322,  116 },
            { 0xF209787BB47D6B85,   348,  124 },
            { 0xB454E4A179DD1877,   375,  132 },
            { 0x865B86925B9BC5C2,   402,  140 },
            { 0xC83553C5C8965D3D,   428,  148 },
            { 0x952AB45CFA97A0B3,   455,  156 },
            { 0xDE469FBD99A05FE3,   481,  164 },
            { 0xA59BC234DB398C25,   508,  172 },
            { 0xF6C69A72A3989F5C,   534,  180 },
            { 0xB7DCBF5354E9BECE,   561,  188 },
            { 0x88FCF317F22241E2,   588,  196 },
            { 0xCC20CE9BD35C78A5,   614,  204 },
            { 0x98165AF37B2153DF,   641,  212 },
            { 0xE2A0B5DC971F303A,   667,  220 },
            { 0xA8D9D1535CE3B396,   694,  228 },
            { 0xFB9B7CD9A4A7443C,   720,  236 },
            { 0xBB764C4CA7A44410,   747,  244 },
            { 0x8BAB8EEFB6409C1A,   774,  252 },
            { 0xD01FEF10A657842C,   800,  260 },
            { 0x9B10A4E5E9913129,   827,  268 },
            { 0xE7109BFBA19C0C9D,   853,  276 },
            { 0xAC2820D9623BF429,   880,  284 },
            { 0x80444B5E7AA7CF85,   907,  292 },
            { 0xBF21E44003ACDD2D,   933,  300 },
            { 0x8E679C2F5E44FF8F,   960,  308 },
            { 0xD433179D9C8CB841,   986,  316 },
            { 0x9E19DB92B4E31BA9,  1013,  324 },
        }
    };

    // This computation gives exactly the same results for k as
    //      k = ceil((kAlpha - e - 1) * 0.30102999566398114)
    // for |e| <= 1500, but doesn't require floating-point operations.
    // NB: log_10(2) ~= 78913 / 2^18
    assert(e >= -1500);
    assert(e <=  1500);
    const int f = kAlpha - e - 1;
    const int k = (f * 78913) / (1 << 18) + static_cast<int>(f > 0);

    const int index = (-kCachedPowersMinDecExp + k + (kCachedPowersDecStep - 1)) / kCachedPowersDecStep;
    assert(index >= 0);
    assert(static_cast<std::size_t>(index) < kCachedPowers.size());

    const cached_power cached = kCachedPowers[static_cast<std::size_t>(index)];
    assert(kAlpha <= cached.e + e + 64);
    assert(kGamma >= cached.e + e + 64);

    return cached;
}

/*!
For n != 0, returns k, such that pow10 := 10^(k-1) <= n < 10^k.
For n == 0, returns 1 and sets pow10 := 1.
*/
inline int find_largest_pow10(const std::uint32_t n, std::uint32_t& pow10)
{
    // LCOV_EXCL_START
    if (n >= 1000000000)
    {
        pow10 = 1000000000;
        return 10;
    }
    // LCOV_EXCL_STOP
    else if (n >= 100000000)
    {
        pow10 = 100000000;
        return  9;
    }
    else if (n >= 10000000)
    {
        pow10 = 10000000;
        return  8;
    }
    else if (n >= 1000000)
    {
        pow10 = 1000000;
        return  7;
    }
    else if (n >= 100000)
    {
        pow10 = 100000;
        return  6;
    }
    else if (n >= 10000)
    {
        pow10 = 10000;
        return  5;
    }
    else if (n >= 1000)
    {
        pow10 = 1000;
        return  4;
    }
    else if (n >= 100)
    {
        pow10 = 100;
        return  3;
    }
    else if (n >= 10)
    {
        pow10 = 10;
        return  2;
    }
    else
    {
        pow10 = 1;
        return 1;
    }
}

inline void grisu2_round(char* buf, int len, std::uint64_t dist, std::uint64_t delta,
                         std::uint64_t rest, std::uint64_t ten_k)
{
    assert(len >= 1);
    assert(dist <= delta);
    assert(rest <= delta);
    assert(ten_k > 0);

    //               <--------------------------- delta ---->
    //                                  <---- dist --------->
    // --------------[------------------+-------------------]--------------
    //               M-                 w                   M+
    //
    //                                  ten_k
    //                                <------>
    //                                       <---- rest ---->
    // --------------[------------------+----+--------------]--------------
    //                                  w    V
    //                                       = buf * 10^k
    //
    // ten_k represents a unit-in-the-last-place in the decimal representation
    // stored in buf.
    // Decrement buf by ten_k while this takes buf closer to w.

    // The tests are written in this order to avoid overflow in unsigned
    // integer arithmetic.

    while (rest < dist
            and delta - rest >= ten_k
            and (rest + ten_k < dist or dist - rest > rest + ten_k - dist))
    {
        assert(buf[len - 1] != '0');
        buf[len - 1]--;
        rest += ten_k;
    }
}

/*!
Generates V = buffer * 10^decimal_exponent, such that M- <= V <= M+.
M- and M+ must be normalized and share the same exponent -60 <= e <= -32.
*/
inline void grisu2_digit_gen(char* buffer, int& length, int& decimal_exponent,
                             diyfp M_minus, diyfp w, diyfp M_plus)
{
    static_assert(kAlpha >= -60, "internal error");
    static_assert(kGamma <= -32, "internal error");

    // Generates the digits (and the exponent) of a decimal floating-point
    // number V = buffer * 10^decimal_exponent in the range [M-, M+]. The diyfp's
    // w, M- and M+ share the same exponent e, which satisfies alpha <= e <= gamma.
    //
    //               <--------------------------- delta ---->
    //                                  <---- dist --------->
    // --------------[------------------+-------------------]--------------
    //               M-                 w                   M+
    //
    // Grisu2 generates the digits of M+ from left to right and stops as soon as
    // V is in [M-,M+].

    assert(M_plus.e >= kAlpha);
    assert(M_plus.e <= kGamma);

    std::uint64_t delta = diyfp::sub(M_plus, M_minus).f; // (significand of (M+ - M-), implicit exponent is e)
    std::uint64_t dist  = diyfp::sub(M_plus, w      ).f; // (significand of (M+ - w ), implicit exponent is e)

    // Split M+ = f * 2^e into two parts p1 and p2 (note: e < 0):
    //
    //      M+ = f * 2^e
    //         = ((f div 2^-e) * 2^-e + (f mod 2^-e)) * 2^e
    //         = ((p1        ) * 2^-e + (p2        )) * 2^e
    //         = p1 + p2 * 2^e

    const diyfp one(std::uint64_t{1} << -M_plus.e, M_plus.e);

    auto p1 = static_cast<std::uint32_t>(M_plus.f >> -one.e); // p1 = f div 2^-e (Since -e >= 32, p1 fits into a 32-bit int.)
    std::uint64_t p2 = M_plus.f & (one.f - 1);                    // p2 = f mod 2^-e

    // 1)
    //
    // Generate the digits of the integral part p1 = d[n-1]...d[1]d[0]

    assert(p1 > 0);

    std::uint32_t pow10;
    const int k = find_largest_pow10(p1, pow10);

    //      10^(k-1) <= p1 < 10^k, pow10 = 10^(k-1)
    //
    //      p1 = (p1 div 10^(k-1)) * 10^(k-1) + (p1 mod 10^(k-1))
    //         = (d[k-1]         ) * 10^(k-1) + (p1 mod 10^(k-1))
    //
    //      M+ = p1                                             + p2 * 2^e
    //         = d[k-1] * 10^(k-1) + (p1 mod 10^(k-1))          + p2 * 2^e
    //         = d[k-1] * 10^(k-1) + ((p1 mod 10^(k-1)) * 2^-e + p2) * 2^e
    //         = d[k-1] * 10^(k-1) + (                         rest) * 2^e
    //
    // Now generate the digits d[n] of p1 from left to right (n = k-1,...,0)
    //
    //      p1 = d[k-1]...d[n] * 10^n + d[n-1]...d[0]
    //
    // but stop as soon as
    //
    //      rest * 2^e = (d[n-1]...d[0] * 2^-e + p2) * 2^e <= delta * 2^e

    int n = k;
    while (n > 0)
    {
        // Invariants:
        //      M+ = buffer * 10^n + (p1 + p2 * 2^e)    (buffer = 0 for n = k)
        //      pow10 = 10^(n-1) <= p1 < 10^n
        //
        const std::uint32_t d = p1 / pow10;  // d = p1 div 10^(n-1)
        const std::uint32_t r = p1 % pow10;  // r = p1 mod 10^(n-1)
        //
        //      M+ = buffer * 10^n + (d * 10^(n-1) + r) + p2 * 2^e
        //         = (buffer * 10 + d) * 10^(n-1) + (r + p2 * 2^e)
        //
        assert(d <= 9);
        buffer[length++] = static_cast<char>('0' + d); // buffer := buffer * 10 + d
        //
        //      M+ = buffer * 10^(n-1) + (r + p2 * 2^e)
        //
        p1 = r;
        n--;
        //
        //      M+ = buffer * 10^n + (p1 + p2 * 2^e)
        //      pow10 = 10^n
        //

        // Now check if enough digits have been generated.
        // Compute
        //
        //      p1 + p2 * 2^e = (p1 * 2^-e + p2) * 2^e = rest * 2^e
        //
        // Note:
        // Since rest and delta share the same exponent e, it suffices to
        // compare the significands.
        const std::uint64_t rest = (std::uint64_t{p1} << -one.e) + p2;
        if (rest <= delta)
        {
            // V = buffer * 10^n, with M- <= V <= M+.

            decimal_exponent += n;

            // We may now just stop. But instead look if the buffer could be
            // decremented to bring V closer to w.
            //
            // pow10 = 10^n is now 1 ulp in the decimal representation V.
            // The rounding procedure works with diyfp's with an implicit
            // exponent of e.
            //
            //      10^n = (10^n * 2^-e) * 2^e = ulp * 2^e
            //
            const std::uint64_t ten_n = std::uint64_t{pow10} << -one.e;
            grisu2_round(buffer, length, dist, delta, rest, ten_n);

            return;
        }

        pow10 /= 10;
        //
        //      pow10 = 10^(n-1) <= p1 < 10^n
        // Invariants restored.
    }

    // 2)
    //
    // The digits of the integral part have been generated:
    //
    //      M+ = d[k-1]...d[1]d[0] + p2 * 2^e
    //         = buffer            + p2 * 2^e
    //
    // Now generate the digits of the fractional part p2 * 2^e.
    //
    // Note:
    // No decimal point is generated: the exponent is adjusted instead.
    //
    // p2 actually represents the fraction
    //
    //      p2 * 2^e
    //          = p2 / 2^-e
    //          = d[-1] / 10^1 + d[-2] / 10^2 + ...
    //
    // Now generate the digits d[-m] of p1 from left to right (m = 1,2,...)
    //
    //      p2 * 2^e = d[-1]d[-2]...d[-m] * 10^-m
    //                      + 10^-m * (d[-m-1] / 10^1 + d[-m-2] / 10^2 + ...)
    //
    // using
    //
    //      10^m * p2 = ((10^m * p2) div 2^-e) * 2^-e + ((10^m * p2) mod 2^-e)
    //                = (                   d) * 2^-e + (                   r)
    //
    // or
    //      10^m * p2 * 2^e = d + r * 2^e
    //
    // i.e.
    //
    //      M+ = buffer + p2 * 2^e
    //         = buffer + 10^-m * (d + r * 2^e)
    //         = (buffer * 10^m + d) * 10^-m + 10^-m * r * 2^e
    //
    // and stop as soon as 10^-m * r * 2^e <= delta * 2^e

    assert(p2 > delta);

    int m = 0;
    for (;;)
    {
        // Invariant:
        //      M+ = buffer * 10^-m + 10^-m * (d[-m-1] / 10 + d[-m-2] / 10^2 + ...) * 2^e
        //         = buffer * 10^-m + 10^-m * (p2                                 ) * 2^e
        //         = buffer * 10^-m + 10^-m * (1/10 * (10 * p2)                   ) * 2^e
        //         = buffer * 10^-m + 10^-m * (1/10 * ((10*p2 div 2^-e) * 2^-e + (10*p2 mod 2^-e)) * 2^e
        //
        assert(p2 <= (std::numeric_limits<std::uint64_t>::max)() / 10);
        p2 *= 10;
        const std::uint64_t d = p2 >> -one.e;     // d = (10 * p2) div 2^-e
        const std::uint64_t r = p2 & (one.f - 1); // r = (10 * p2) mod 2^-e
        //
        //      M+ = buffer * 10^-m + 10^-m * (1/10 * (d * 2^-e + r) * 2^e
        //         = buffer * 10^-m + 10^-m * (1/10 * (d + r * 2^e))
        //         = (buffer * 10 + d) * 10^(-m-1) + 10^(-m-1) * r * 2^e
        //
        assert(d <= 9);
        buffer[length++] = static_cast<char>('0' + d); // buffer := buffer * 10 + d
        //
        //      M+ = buffer * 10^(-m-1) + 10^(-m-1) * r * 2^e
        //
        p2 = r;
        m++;
        //
        //      M+ = buffer * 10^-m + 10^-m * p2 * 2^e
        // Invariant restored.

        // Check if enough digits have been generated.
        //
        //      10^-m * p2 * 2^e <= delta * 2^e
        //              p2 * 2^e <= 10^m * delta * 2^e
        //                    p2 <= 10^m * delta
        delta *= 10;
        dist  *= 10;
        if (p2 <= delta)
        {
            break;
        }
    }

    // V = buffer * 10^-m, with M- <= V <= M+.

    decimal_exponent -= m;

    // 1 ulp in the decimal representation is now 10^-m.
    // Since delta and dist are now scaled by 10^m, we need to do the
    // same with ulp in order to keep the units in sync.
    //
    //      10^m * 10^-m = 1 = 2^-e * 2^e = ten_m * 2^e
    //
    const std::uint64_t ten_m = one.f;
    grisu2_round(buffer, length, dist, delta, p2, ten_m);

    // By construction this algorithm generates the shortest possible decimal
    // number (Loitsch, Theorem 6.2) which rounds back to w.
    // For an input number of precision p, at least
    //
    //      N = 1 + ceil(p * log_10(2))
    //
    // decimal digits are sufficient to identify all binary floating-point
    // numbers (Matula, "In-and-Out conversions").
    // This implies that the algorithm does not produce more than N decimal
    // digits.
    //
    //      N = 17 for p = 53 (IEEE double precision)
    //      N = 9  for p = 24 (IEEE single precision)
}

/*!
v = buf * 10^decimal_exponent
len is the length of the buffer (number of decimal digits)
The buffer must be large enough, i.e. >= max_digits10.
*/
JSON_HEDLEY_NON_NULL(1)
inline void grisu2(char* buf, int& len, int& decimal_exponent,
                   diyfp m_minus, diyfp v, diyfp m_plus)
{
    assert(m_plus.e == m_minus.e);
    assert(m_plus.e == v.e);

    //  --------(-----------------------+-----------------------)--------    (A)
    //          m-                      v                       m+
    //
    //  --------------------(-----------+-----------------------)--------    (B)
    //                      m-          v                       m+
    //
    // First scale v (and m- and m+) such that the exponent is in the range
    // [alpha, gamma].

    const cached_power cached = get_cached_power_for_binary_exponent(m_plus.e);

    const diyfp c_minus_k(cached.f, cached.e); // = c ~= 10^-k

    // The exponent of the products is = v.e + c_minus_k.e + q and is in the range [alpha,gamma]
    const diyfp w       = diyfp::mul(v,       c_minus_k);
    const diyfp w_minus = diyfp::mul(m_minus, c_minus_k);
    const diyfp w_plus  = diyfp::mul(m_plus,  c_minus_k);

    //  ----(---+---)---------------(---+---)---------------(---+---)----
    //          w-                      w                       w+
    //          = c*m-                  = c*v                   = c*m+
    //
    // diyfp::mul rounds its result and c_minus_k is approximated too. w, w- and
    // w+ are now off by a small amount.
    // In fact:
    //
    //      w - v * 10^k < 1 ulp
    //
    // To account for this inaccuracy, add resp. subtract 1 ulp.
    //
    //  --------+---[---------------(---+---)---------------]---+--------
    //          w-  M-                  w                   M+  w+
    //
    // Now any number in [M-, M+] (bounds included) will round to w when input,
    // regardless of how the input rounding algorithm breaks ties.
    //
    // And digit_gen generates the shortest possible such number in [M-, M+].
    // Note that this does not mean that Grisu2 always generates the shortest
    // possible number in the interval (m-, m+).
    const diyfp M_minus(w_minus.f + 1, w_minus.e);
    const diyfp M_plus (w_plus.f  - 1, w_plus.e );

    decimal_exponent = -cached.k; // = -(-k) = k

    grisu2_digit_gen(buf, len, decimal_exponent, M_minus, w, M_plus);
}

/*!
v = buf * 10^decimal_exponent
len is the length of the buffer (number of decimal digits)
The buffer must be large enough, i.e. >= max_digits10.
*/
template <typename FloatType>
JSON_HEDLEY_NON_NULL(1)
void grisu2(char* buf, int& len, int& decimal_exponent, FloatType value)
{
    static_assert(diyfp::kPrecision >= std::numeric_limits<FloatType>::digits + 3,
                  "internal error: not enough precision");

    assert(std::isfinite(value));
    assert(value > 0);

    // If the neighbors (and boundaries) of 'value' are always computed for double-precision
    // numbers, all float's can be recovered using strtod (and strtof). However, the resulting
    // decimal representations are not exactly "short".
    //
    // The documentation for 'std::to_chars' (https://en.cppreference.com/w/cpp/utility/to_chars)
    // says "value is converted to a string as if by std::sprintf in the default ("C") locale"
    // and since sprintf promotes float's to double's, I think this is exactly what 'std::to_chars'
    // does.
    // On the other hand, the documentation for 'std::to_chars' requires that "parsing the
    // representation using the corresponding std::from_chars function recovers value exactly". That
    // indicates that single precision floating-point numbers should be recovered using
    // 'std::strtof'.
    //
    // NB: If the neighbors are computed for single-precision numbers, there is a single float
    //     (7.0385307e-26f) which can't be recovered using strtod. The resulting double precision
    //     value is off by 1 ulp.
#if 0
    const boundaries w = compute_boundaries(static_cast<double>(value));
#else
    const boundaries w = compute_boundaries(value);
#endif

    grisu2(buf, len, decimal_exponent, w.minus, w.w, w.plus);
}

/*!
@brief appends a decimal representation of e to buf
@return a pointer to the element following the exponent.
@pre -1000 < e < 1000
*/
JSON_HEDLEY_NON_NULL(1)
JSON_HEDLEY_RETURNS_NON_NULL
inline char* append_exponent(char* buf, int e)
{
    assert(e > -1000);
    assert(e <  1000);

    if (e < 0)
    {
        e = -e;
        *buf++ = '-';
    }
    else
    {
        *buf++ = '+';
    }

    auto k = static_cast<std::uint32_t>(e);
    if (k < 10)
    {
        // Always print at least two digits in the exponent.
        // This is for compatibility with printf("%g").
        *buf++ = '0';
        *buf++ = static_cast<char>('0' + k);
    }
    else if (k < 100)
    {
        *buf++ = static_cast<char>('0' + k / 10);
        k %= 10;
        *buf++ = static_cast<char>('0' + k);
    }
    else
    {
        *buf++ = static_cast<char>('0' + k / 100);
        k %= 100;
        *buf++ = static_cast<char>('0' + k / 10);
        k %= 10;
        *buf++ = static_cast<char>('0' + k);
    }

    return buf;
}

/*!
@brief prettify v = buf * 10^decimal_exponent

If v is in the range [10^min_exp, 10^max_exp) it will be printed in fixed-point
notation. Otherwise it will be printed in exponential notation.

@pre min_exp < 0
@pre max_exp > 0
*/
JSON_HEDLEY_NON_NULL(1)
JSON_HEDLEY_RETURNS_NON_NULL
inline char* format_buffer(char* buf, int len, int decimal_exponent,
                           int min_exp, int max_exp)
{
    assert(min_exp < 0);
    assert(max_exp > 0);

    const int k = len;
    const int n = len + decimal_exponent;

    // v = buf * 10^(n-k)
    // k is the length of the buffer (number of decimal digits)
    // n is the position of the decimal point relative to the start of the buffer.

    if (k <= n and n <= max_exp)
    {
        // digits[000]
        // len <= max_exp + 2

        std::memset(buf + k, '0', static_cast<size_t>(n - k));
        // Make it look like a floating-point number (#362, #378)
        buf[n + 0] = '.';
        buf[n + 1] = '0';
        return buf + (n + 2);
    }

    if (0 < n and n <= max_exp)
    {
        // dig.its
        // len <= max_digits10 + 1

        assert(k > n);

        std::memmove(buf + (n + 1), buf + n, static_cast<size_t>(k - n));
        buf[n] = '.';
        return buf + (k + 1);
    }

    if (min_exp < n and n <= 0)
    {
        // 0.[000]digits
        // len <= 2 + (-min_exp - 1) + max_digits10

        std::memmove(buf + (2 + -n), buf, static_cast<size_t>(k));
        buf[0] = '0';
        buf[1] = '.';
        std::memset(buf + 2, '0', static_cast<size_t>(-n));
        return buf + (2 + (-n) + k);
    }

    if (k == 1)
    {
        // dE+123
        // len <= 1 + 5

        buf += 1;
    }
    else
    {
        // d.igitsE+123
        // len <= max_digits10 + 1 + 5

        std::memmove(buf + 2, buf + 1, static_cast<size_t>(k - 1));
        buf[1] = '.';
        buf += 1 + k;
    }

    *buf++ = 'e';
    return append_exponent(buf, n - 1);
}

} // namespace dtoa_impl

/*!
@brief generates a decimal representation of the floating-point number value in [first, last).

The format of the resulting decimal representation is similar to printf's %g
format. Returns an iterator pointing past-the-end of the decimal representation.

@note The input number must be finite, i.e. NaN's and Inf's are not supported.
@note The buffer must be large enough.
@note The result is NOT null-terminated.
*/
template <typename FloatType>
JSON_HEDLEY_NON_NULL(1, 2)
JSON_HEDLEY_RETURNS_NON_NULL
char* to_chars(char* first, const char* last, FloatType value)
{
    static_cast<void>(last); // maybe unused - fix warning
    assert(std::isfinite(value));

    // Use signbit(value) instead of (value < 0) since signbit works for -0.
    if (std::signbit(value))
    {
        value = -value;
        *first++ = '-';
    }

    if (value == 0) // +-0
    {
        *first++ = '0';
        // Make it look like a floating-point number (#362, #378)
        *first++ = '.';
        *first++ = '0';
        return first;
    }

    assert(last - first >= std::numeric_limits<FloatType>::max_digits10);

    // Compute v = buffer * 10^decimal_exponent.
    // The decimal digits are stored in the buffer, which needs to be interpreted
    // as an unsigned decimal integer.
    // len is the length of the buffer, i.e. the number of decimal digits.
    int len = 0;
    int decimal_exponent = 0;
    dtoa_impl::grisu2(first, len, decimal_exponent, value);

    assert(len <= std::numeric_limits<FloatType>::max_digits10);

    // Format the buffer like printf("%.*g", prec, value)
    constexpr int kMinExp = -4;
    // Use digits10 here to increase compatibility with version 2.
    constexpr int kMaxExp = std::numeric_limits<FloatType>::digits10;

    assert(last - first >= kMaxExp + 2);
    assert(last - first >= 2 + (-kMinExp - 1) + std::numeric_limits<FloatType>::max_digits10);
    assert(last - first >= std::numeric_limits<FloatType>::max_digits10 + 6);

    return dtoa_impl::format_buffer(first, len, decimal_exponent, kMinExp, kMaxExp);
}

} // namespace detail
} // namespace nlohmann

// #include <nlohmann/detail/exceptions.hpp>

// #include <nlohmann/detail/macro_scope.hpp>

// #include <nlohmann/detail/meta/cpp_future.hpp>

// #include <nlohmann/detail/output/binary_writer.hpp>

// #include <nlohmann/detail/output/output_adapters.hpp>

// #include <nlohmann/detail/value_t.hpp>


namespace nlohmann
{
namespace detail
{
///////////////////
// serialization //
///////////////////

/// how to treat decoding errors
enum class error_handler_t
{
    strict,  ///< throw a type_error exception in case of invalid UTF-8
    replace, ///< replace invalid UTF-8 sequences with U+FFFD
    ignore   ///< ignore invalid UTF-8 sequences
};

template<typename BasicJsonType>
class serializer
{
    using string_t = typename BasicJsonType::string_t;
    using number_float_t = typename BasicJsonType::number_float_t;
    using number_integer_t = typename BasicJsonType::number_integer_t;
    using number_unsigned_t = typename BasicJsonType::number_unsigned_t;
    static constexpr std::uint8_t UTF8_ACCEPT = 0;
    static constexpr std::uint8_t UTF8_REJECT = 1;

  public:
    /*!
    @param[in] s  output stream to serialize to
    @param[in] ichar  indentation character to use
    @param[in] error_handler_  how to react on decoding errors
    */
    serializer(output_adapter_t<char> s, const char ichar,
               error_handler_t error_handler_ = error_handler_t::strict)
        : o(std::move(s))
        , loc(std::localeconv())
        , thousands_sep(loc->thousands_sep == nullptr ? '\0' : * (loc->thousands_sep))
        , decimal_point(loc->decimal_point == nullptr ? '\0' : * (loc->decimal_point))
        , indent_char(ichar)
        , indent_string(512, indent_char)
        , error_handler(error_handler_)
    {}

    // delete because of pointer members
    serializer(const serializer&) = delete;
    serializer& operator=(const serializer&) = delete;
    serializer(serializer&&) = delete;
    serializer& operator=(serializer&&) = delete;
    ~serializer() = default;

    /*!
    @brief internal implementation of the serialization function

    This function is called by the public member function dump and organizes
    the serialization internally. The indentation level is propagated as
    additional parameter. In case of arrays and objects, the function is
    called recursively.

    - strings and object keys are escaped using `escape_string()`
    - integer numbers are converted implicitly via `operator<<`
    - floating-point numbers are converted to a string using `"%g"` format

    @param[in] val             value to serialize
    @param[in] pretty_print    whether the output shall be pretty-printed
    @param[in] indent_step     the indent level
    @param[in] current_indent  the current indent level (only used internally)
    */
    void dump(const BasicJsonType& val, const bool pretty_print,
              const bool ensure_ascii,
              const unsigned int indent_step,
              const unsigned int current_indent = 0)
    {
        switch (val.m_type)
        {
            case value_t::object:
            {
                if (val.m_value.object->empty())
                {
                    o->write_characters("{}", 2);
                    return;
                }

                if (pretty_print)
                {
                    o->write_characters("{\n", 2);

                    // variable to hold indentation for recursive calls
                    const auto new_indent = current_indent + indent_step;
                    if (JSON_HEDLEY_UNLIKELY(indent_string.size() < new_indent))
                    {
                        indent_string.resize(indent_string.size() * 2, ' ');
                    }

                    // first n-1 elements
                    auto i = val.m_value.object->cbegin();
                    for (std::size_t cnt = 0; cnt < val.m_value.object->size() - 1; ++cnt, ++i)
                    {
                        o->write_characters(indent_string.c_str(), new_indent);
                        o->write_character('\"');
                        dump_escaped(i->first, ensure_ascii);
                        o->write_characters("\": ", 3);
                        dump(i->second, true, ensure_ascii, indent_step, new_indent);
                        o->write_characters(",\n", 2);
                    }

                    // last element
                    assert(i != val.m_value.object->cend());
                    assert(std::next(i) == val.m_value.object->cend());
                    o->write_characters(indent_string.c_str(), new_indent);
                    o->write_character('\"');
                    dump_escaped(i->first, ensure_ascii);
                    o->write_characters("\": ", 3);
                    dump(i->second, true, ensure_ascii, indent_step, new_indent);

                    o->write_character('\n');
                    o->write_characters(indent_string.c_str(), current_indent);
                    o->write_character('}');
                }
                else
                {
                    o->write_character('{');

                    // first n-1 elements
                    auto i = val.m_value.object->cbegin();
                    for (std::size_t cnt = 0; cnt < val.m_value.object->size() - 1; ++cnt, ++i)
                    {
                        o->write_character('\"');
                        dump_escaped(i->first, ensure_ascii);
                        o->write_characters("\":", 2);
                        dump(i->second, false, ensure_ascii, indent_step, current_indent);
                        o->write_character(',');
                    }

                    // last element
                    assert(i != val.m_value.object->cend());
                    assert(std::next(i) == val.m_value.object->cend());
                    o->write_character('\"');
                    dump_escaped(i->first, ensure_ascii);
                    o->write_characters("\":", 2);
                    dump(i->second, false, ensure_ascii, indent_step, current_indent);

                    o->write_character('}');
                }

                return;
            }

            case value_t::array:
            {
                if (val.m_value.array->empty())
                {
                    o->write_characters("[]", 2);
                    return;
                }

                if (pretty_print)
                {
                    o->write_characters("[\n", 2);

                    // variable to hold indentation for recursive calls
                    const auto new_indent = current_indent + indent_step;
                    if (JSON_HEDLEY_UNLIKELY(indent_string.size() < new_indent))
                    {
                        indent_string.resize(indent_string.size() * 2, ' ');
                    }

                    // first n-1 elements
                    for (auto i = val.m_value.array->cbegin();
                            i != val.m_value.array->cend() - 1; ++i)
                    {
                        o->write_characters(indent_string.c_str(), new_indent);
                        dump(*i, true, ensure_ascii, indent_step, new_indent);
                        o->write_characters(",\n", 2);
                    }

                    // last element
                    assert(not val.m_value.array->empty());
                    o->write_characters(indent_string.c_str(), new_indent);
                    dump(val.m_value.array->back(), true, ensure_ascii, indent_step, new_indent);

                    o->write_character('\n');
                    o->write_characters(indent_string.c_str(), current_indent);
                    o->write_character(']');
                }
                else
                {
                    o->write_character('[');

                    // first n-1 elements
                    for (auto i = val.m_value.array->cbegin();
                            i != val.m_value.array->cend() - 1; ++i)
                    {
                        dump(*i, false, ensure_ascii, indent_step, current_indent);
                        o->write_character(',');
                    }

                    // last element
                    assert(not val.m_value.array->empty());
                    dump(val.m_value.array->back(), false, ensure_ascii, indent_step, current_indent);

                    o->write_character(']');
                }

                return;
            }

            case value_t::string:
            {
                o->write_character('\"');
                dump_escaped(*val.m_value.string, ensure_ascii);
                o->write_character('\"');
                return;
            }

            case value_t::boolean:
            {
                if (val.m_value.boolean)
                {
                    o->write_characters("true", 4);
                }
                else
                {
                    o->write_characters("false", 5);
                }
                return;
            }

            case value_t::number_integer:
            {
                dump_integer(val.m_value.number_integer);
                return;
            }

            case value_t::number_unsigned:
            {
                dump_integer(val.m_value.number_unsigned);
                return;
            }

            case value_t::number_float:
            {
                dump_float(val.m_value.number_float);
                return;
            }

            case value_t::discarded:
            {
                o->write_characters("<discarded>", 11);
                return;
            }

            case value_t::null:
            {
                o->write_characters("null", 4);
                return;
            }

            default:            // LCOV_EXCL_LINE
                assert(false);  // LCOV_EXCL_LINE
        }
    }

  private:
    /*!
    @brief dump escaped string

    Escape a string by replacing certain special characters by a sequence of an
    escape character (backslash) and another character and other control
    characters by a sequence of "\u" followed by a four-digit hex
    representation. The escaped string is written to output stream @a o.

    @param[in] s  the string to escape
    @param[in] ensure_ascii  whether to escape non-ASCII characters with
                             \uXXXX sequences

    @complexity Linear in the length of string @a s.
    */
    void dump_escaped(const string_t& s, const bool ensure_ascii)
    {
        std::uint32_t codepoint;
        std::uint8_t state = UTF8_ACCEPT;
        std::size_t bytes = 0;  // number of bytes written to string_buffer

        // number of bytes written at the point of the last valid byte
        std::size_t bytes_after_last_accept = 0;
        std::size_t undumped_chars = 0;

        for (std::size_t i = 0; i < s.size(); ++i)
        {
            const auto byte = static_cast<uint8_t>(s[i]);

            switch (decode(state, codepoint, byte))
            {
                case UTF8_ACCEPT:  // decode found a new code point
                {
                    switch (codepoint)
                    {
                        case 0x08: // backspace
                        {
                            string_buffer[bytes++] = '\\';
                            string_buffer[bytes++] = 'b';
                            break;
                        }

                        case 0x09: // horizontal tab
                        {
                            string_buffer[bytes++] = '\\';
                            string_buffer[bytes++] = 't';
                            break;
                        }

                        case 0x0A: // newline
                        {
                            string_buffer[bytes++] = '\\';
                            string_buffer[bytes++] = 'n';
                            break;
                        }

                        case 0x0C: // formfeed
                        {
                            string_buffer[bytes++] = '\\';
                            string_buffer[bytes++] = 'f';
                            break;
                        }

                        case 0x0D: // carriage return
                        {
                            string_buffer[bytes++] = '\\';
                            string_buffer[bytes++] = 'r';
                            break;
                        }

                        case 0x22: // quotation mark
                        {
                            string_buffer[bytes++] = '\\';
                            string_buffer[bytes++] = '\"';
                            break;
                        }

                        case 0x5C: // reverse solidus
                        {
                            string_buffer[bytes++] = '\\';
                            string_buffer[bytes++] = '\\';
                            break;
                        }

                        default:
                        {
                            // escape control characters (0x00..0x1F) or, if
                            // ensure_ascii parameter is used, non-ASCII characters
                            if ((codepoint <= 0x1F) or (ensure_ascii and (codepoint >= 0x7F)))
                            {
                                if (codepoint <= 0xFFFF)
                                {
                                    (std::snprintf)(string_buffer.data() + bytes, 7, "\\u%04x",
                                                    static_cast<std::uint16_t>(codepoint));
                                    bytes += 6;
                                }
                                else
                                {
                                    (std::snprintf)(string_buffer.data() + bytes, 13, "\\u%04x\\u%04x",
                                                    static_cast<std::uint16_t>(0xD7C0u + (codepoint >> 10u)),
                                                    static_cast<std::uint16_t>(0xDC00u + (codepoint & 0x3FFu)));
                                    bytes += 12;
                                }
                            }
                            else
                            {
                                // copy byte to buffer (all previous bytes
                                // been copied have in default case above)
                                string_buffer[bytes++] = s[i];
                            }
                            break;
                        }
                    }

                    // write buffer and reset index; there must be 13 bytes
                    // left, as this is the maximal number of bytes to be
                    // written ("\uxxxx\uxxxx\0") for one code point
                    if (string_buffer.size() - bytes < 13)
                    {
                        o->write_characters(string_buffer.data(), bytes);
                        bytes = 0;
                    }

                    // remember the byte position of this accept
                    bytes_after_last_accept = bytes;
                    undumped_chars = 0;
                    break;
                }

                case UTF8_REJECT:  // decode found invalid UTF-8 byte
                {
                    switch (error_handler)
                    {
                        case error_handler_t::strict:
                        {
                            std::string sn(3, '\0');
                            (std::snprintf)(&sn[0], sn.size(), "%.2X", byte);
                            JSON_THROW(type_error::create(316, "invalid UTF-8 byte at index " + std::to_string(i) + ": 0x" + sn));
                        }

                        case error_handler_t::ignore:
                        case error_handler_t::replace:
                        {
                            // in case we saw this character the first time, we
                            // would like to read it again, because the byte
                            // may be OK for itself, but just not OK for the
                            // previous sequence
                            if (undumped_chars > 0)
                            {
                                --i;
                            }

                            // reset length buffer to the last accepted index;
                            // thus removing/ignoring the invalid characters
                            bytes = bytes_after_last_accept;

                            if (error_handler == error_handler_t::replace)
                            {
                                // add a replacement character
                                if (ensure_ascii)
                                {
                                    string_buffer[bytes++] = '\\';
                                    string_buffer[bytes++] = 'u';
                                    string_buffer[bytes++] = 'f';
                                    string_buffer[bytes++] = 'f';
                                    string_buffer[bytes++] = 'f';
                                    string_buffer[bytes++] = 'd';
                                }
                                else
                                {
                                    string_buffer[bytes++] = detail::binary_writer<BasicJsonType, char>::to_char_type('\xEF');
                                    string_buffer[bytes++] = detail::binary_writer<BasicJsonType, char>::to_char_type('\xBF');
                                    string_buffer[bytes++] = detail::binary_writer<BasicJsonType, char>::to_char_type('\xBD');
                                }

                                // write buffer and reset index; there must be 13 bytes
                                // left, as this is the maximal number of bytes to be
                                // written ("\uxxxx\uxxxx\0") for one code point
                                if (string_buffer.size() - bytes < 13)
                                {
                                    o->write_characters(string_buffer.data(), bytes);
                                    bytes = 0;
                                }

                                bytes_after_last_accept = bytes;
                            }

                            undumped_chars = 0;

                            // continue processing the string
                            state = UTF8_ACCEPT;
                            break;
                        }

                        default:            // LCOV_EXCL_LINE
                            assert(false);  // LCOV_EXCL_LINE
                    }
                    break;
                }

                default:  // decode found yet incomplete multi-byte code point
                {
                    if (not ensure_ascii)
                    {
                        // code point will not be escaped - copy byte to buffer
                        string_buffer[bytes++] = s[i];
                    }
                    ++undumped_chars;
                    break;
                }
            }
        }

        // we finished processing the string
        if (JSON_HEDLEY_LIKELY(state == UTF8_ACCEPT))
        {
            // write buffer
            if (bytes > 0)
            {
                o->write_characters(string_buffer.data(), bytes);
            }
        }
        else
        {
            // we finish reading, but do not accept: string was incomplete
            switch (error_handler)
            {
                case error_handler_t::strict:
                {
                    std::string sn(3, '\0');
                    (std::snprintf)(&sn[0], sn.size(), "%.2X", static_cast<std::uint8_t>(s.back()));
                    JSON_THROW(type_error::create(316, "incomplete UTF-8 string; last byte: 0x" + sn));
                }

                case error_handler_t::ignore:
                {
                    // write all accepted bytes
                    o->write_characters(string_buffer.data(), bytes_after_last_accept);
                    break;
                }

                case error_handler_t::replace:
                {
                    // write all accepted bytes
                    o->write_characters(string_buffer.data(), bytes_after_last_accept);
                    // add a replacement character
                    if (ensure_ascii)
                    {
                        o->write_characters("\\ufffd", 6);
                    }
                    else
                    {
                        o->write_characters("\xEF\xBF\xBD", 3);
                    }
                    break;
                }

                default:            // LCOV_EXCL_LINE
                    assert(false);  // LCOV_EXCL_LINE
            }
        }
    }

    /*!
    @brief count digits

    Count the number of decimal (base 10) digits for an input unsigned integer.

    @param[in] x  unsigned integer number to count its digits
    @return    number of decimal digits
    */
    inline unsigned int count_digits(number_unsigned_t x) noexcept
    {
        unsigned int n_digits = 1;
        for (;;)
        {
            if (x < 10)
            {
                return n_digits;
            }
            if (x < 100)
            {
                return n_digits + 1;
            }
            if (x < 1000)
            {
                return n_digits + 2;
            }
            if (x < 10000)
            {
                return n_digits + 3;
            }
            x = x / 10000u;
            n_digits += 4;
        }
    }

    /*!
    @brief dump an integer

    Dump a given integer to output stream @a o. Works internally with
    @a number_buffer.

    @param[in] x  integer number (signed or unsigned) to dump
    @tparam NumberType either @a number_integer_t or @a number_unsigned_t
    */
    template<typename NumberType, detail::enable_if_t<
                 std::is_same<NumberType, number_unsigned_t>::value or
                 std::is_same<NumberType, number_integer_t>::value,
                 int> = 0>
    void dump_integer(NumberType x)
    {
        static constexpr std::array<std::array<char, 2>, 100> digits_to_99
        {
            {
                {{'0', '0'}}, {{'0', '1'}}, {{'0', '2'}}, {{'0', '3'}}, {{'0', '4'}}, {{'0', '5'}}, {{'0', '6'}}, {{'0', '7'}}, {{'0', '8'}}, {{'0', '9'}},
                {{'1', '0'}}, {{'1', '1'}}, {{'1', '2'}}, {{'1', '3'}}, {{'1', '4'}}, {{'1', '5'}}, {{'1', '6'}}, {{'1', '7'}}, {{'1', '8'}}, {{'1', '9'}},
                {{'2', '0'}}, {{'2', '1'}}, {{'2', '2'}}, {{'2', '3'}}, {{'2', '4'}}, {{'2', '5'}}, {{'2', '6'}}, {{'2', '7'}}, {{'2', '8'}}, {{'2', '9'}},
                {{'3', '0'}}, {{'3', '1'}}, {{'3', '2'}}, {{'3', '3'}}, {{'3', '4'}}, {{'3', '5'}}, {{'3', '6'}}, {{'3', '7'}}, {{'3', '8'}}, {{'3', '9'}},
                {{'4', '0'}}, {{'4', '1'}}, {{'4', '2'}}, {{'4', '3'}}, {{'4', '4'}}, {{'4', '5'}}, {{'4', '6'}}, {{'4', '7'}}, {{'4', '8'}}, {{'4', '9'}},
                {{'5', '0'}}, {{'5', '1'}}, {{'5', '2'}}, {{'5', '3'}}, {{'5', '4'}}, {{'5', '5'}}, {{'5', '6'}}, {{'5', '7'}}, {{'5', '8'}}, {{'5', '9'}},
                {{'6', '0'}}, {{'6', '1'}}, {{'6', '2'}}, {{'6', '3'}}, {{'6', '4'}}, {{'6', '5'}}, {{'6', '6'}}, {{'6', '7'}}, {{'6', '8'}}, {{'6', '9'}},
                {{'7', '0'}}, {{'7', '1'}}, {{'7', '2'}}, {{'7', '3'}}, {{'7', '4'}}, {{'7', '5'}}, {{'7', '6'}}, {{'7', '7'}}, {{'7', '8'}}, {{'7', '9'}},
                {{'8', '0'}}, {{'8', '1'}}, {{'8', '2'}}, {{'8', '3'}}, {{'8', '4'}}, {{'8', '5'}}, {{'8', '6'}}, {{'8', '7'}}, {{'8', '8'}}, {{'8', '9'}},
                {{'9', '0'}}, {{'9', '1'}}, {{'9', '2'}}, {{'9', '3'}}, {{'9', '4'}}, {{'9', '5'}}, {{'9', '6'}}, {{'9', '7'}}, {{'9', '8'}}, {{'9', '9'}},
            }
        };

        // special case for "0"
        if (x == 0)
        {
            o->write_character('0');
            return;
        }

        // use a pointer to fill the buffer
        auto buffer_ptr = number_buffer.begin();

        const bool is_negative = std::is_same<NumberType, number_integer_t>::value and not(x >= 0); // see issue #755
        number_unsigned_t abs_value;

        unsigned int n_chars;

        if (is_negative)
        {
            *buffer_ptr = '-';
            abs_value = remove_sign(x);

            // account one more byte for the minus sign
            n_chars = 1 + count_digits(abs_value);
        }
        else
        {
            abs_value = static_cast<number_unsigned_t>(x);
            n_chars = count_digits(abs_value);
        }

        // spare 1 byte for '\0'
        assert(n_chars < number_buffer.size() - 1);

        // jump to the end to generate the string from backward
        // so we later avoid reversing the result
        buffer_ptr += n_chars;

        // Fast int2ascii implementation inspired by "Fastware" talk by Andrei Alexandrescu
        // See: https://www.youtube.com/watch?v=o4-CwDo2zpg
        while (abs_value >= 100)
        {
            const auto digits_index = static_cast<unsigned>((abs_value % 100));
            abs_value /= 100;
            *(--buffer_ptr) = digits_to_99[digits_index][1];
            *(--buffer_ptr) = digits_to_99[digits_index][0];
        }

        if (abs_value >= 10)
        {
            const auto digits_index = static_cast<unsigned>(abs_value);
            *(--buffer_ptr) = digits_to_99[digits_index][1];
            *(--buffer_ptr) = digits_to_99[digits_index][0];
        }
        else
        {
            *(--buffer_ptr) = static_cast<char>('0' + abs_value);
        }

        o->write_characters(number_buffer.data(), n_chars);
    }

    /*!
    @brief dump a floating-point number

    Dump a given floating-point number to output stream @a o. Works internally
    with @a number_buffer.

    @param[in] x  floating-point number to dump
    */
    void dump_float(number_float_t x)
    {
        // NaN / inf
        if (not std::isfinite(x))
        {
            o->write_characters("null", 4);
            return;
        }

        // If number_float_t is an IEEE-754 single or double precision number,
        // use the Grisu2 algorithm to produce short numbers which are
        // guaranteed to round-trip, using strtof and strtod, resp.
        //
        // NB: The test below works if <long double> == <double>.
        static constexpr bool is_ieee_single_or_double
            = (std::numeric_limits<number_float_t>::is_iec559 and std::numeric_limits<number_float_t>::digits == 24 and std::numeric_limits<number_float_t>::max_exponent == 128) or
              (std::numeric_limits<number_float_t>::is_iec559 and std::numeric_limits<number_float_t>::digits == 53 and std::numeric_limits<number_float_t>::max_exponent == 1024);

        dump_float(x, std::integral_constant<bool, is_ieee_single_or_double>());
    }

    void dump_float(number_float_t x, std::true_type /*is_ieee_single_or_double*/)
    {
        char* begin = number_buffer.data();
        char* end = ::nlohmann::detail::to_chars(begin, begin + number_buffer.size(), x);

        o->write_characters(begin, static_cast<size_t>(end - begin));
    }

    void dump_float(number_float_t x, std::false_type /*is_ieee_single_or_double*/)
    {
        // get number of digits for a float -> text -> float round-trip
        static constexpr auto d = std::numeric_limits<number_float_t>::max_digits10;

        // the actual conversion
        std::ptrdiff_t len = (std::snprintf)(number_buffer.data(), number_buffer.size(), "%.*g", d, x);

        // negative value indicates an error
        assert(len > 0);
        // check if buffer was large enough
        assert(static_cast<std::size_t>(len) < number_buffer.size());

        // erase thousands separator
        if (thousands_sep != '\0')
        {
            const auto end = std::remove(number_buffer.begin(),
                                         number_buffer.begin() + len, thousands_sep);
            std::fill(end, number_buffer.end(), '\0');
            assert((end - number_buffer.begin()) <= len);
            len = (end - number_buffer.begin());
        }

        // convert decimal point to '.'
        if (decimal_point != '\0' and decimal_point != '.')
        {
            const auto dec_pos = std::find(number_buffer.begin(), number_buffer.end(), decimal_point);
            if (dec_pos != number_buffer.end())
            {
                *dec_pos = '.';
            }
        }

        o->write_characters(number_buffer.data(), static_cast<std::size_t>(len));

        // determine if need to append ".0"
        const bool value_is_int_like =
            std::none_of(number_buffer.begin(), number_buffer.begin() + len + 1,
                         [](char c)
        {
            return c == '.' or c == 'e';
        });

        if (value_is_int_like)
        {
            o->write_characters(".0", 2);
        }
    }

    /*!
    @brief check whether a string is UTF-8 encoded

    The function checks each byte of a string whether it is UTF-8 encoded. The
    result of the check is stored in the @a state parameter. The function must
    be called initially with state 0 (accept). State 1 means the string must
    be rejected, because the current byte is not allowed. If the string is
    completely processed, but the state is non-zero, the string ended
    prematurely; that is, the last byte indicated more bytes should have
    followed.

    @param[in,out] state  the state of the decoding
    @param[in,out] codep  codepoint (valid only if resulting state is UTF8_ACCEPT)
    @param[in] byte       next byte to decode
    @return               new state

    @note The function has been edited: a std::array is used.

    @copyright Copyright (c) 2008-2009 Bjoern Hoehrmann <bjoern@hoehrmann.de>
    @sa http://bjoern.hoehrmann.de/utf-8/decoder/dfa/
    */
    static std::uint8_t decode(std::uint8_t& state, std::uint32_t& codep, const std::uint8_t byte) noexcept
    {
        static const std::array<std::uint8_t, 400> utf8d =
        {
            {
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, // 00..1F
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, // 20..3F
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, // 40..5F
                0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, // 60..7F
                1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, // 80..9F
                7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, // A0..BF
                8, 8, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, // C0..DF
                0xA, 0x3, 0x3, 0x3, 0x3, 0x3, 0x3, 0x3, 0x3, 0x3, 0x3, 0x3, 0x3, 0x4, 0x3, 0x3, // E0..EF
                0xB, 0x6, 0x6, 0x6, 0x5, 0x8, 0x8, 0x8, 0x8, 0x8, 0x8, 0x8, 0x8, 0x8, 0x8, 0x8, // F0..FF
                0x0, 0x1, 0x2, 0x3, 0x5, 0x8, 0x7, 0x1, 0x1, 0x1, 0x4, 0x6, 0x1, 0x1, 0x1, 0x1, // s0..s0
                1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, // s1..s2
                1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, // s3..s4
                1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, // s5..s6
                1, 3, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1 // s7..s8
            }
        };

        const std::uint8_t type = utf8d[byte];

        codep = (state != UTF8_ACCEPT)
                ? (byte & 0x3fu) | (codep << 6u)
                : (0xFFu >> type) & (byte);

        state = utf8d[256u + state * 16u + type];
        return state;
    }

    /*
     * Overload to make the compiler happy while it is instantiating
     * dump_integer for number_unsigned_t.
     * Must never be called.
     */
    number_unsigned_t remove_sign(number_unsigned_t x)
    {
        assert(false); // LCOV_EXCL_LINE
        return x; // LCOV_EXCL_LINE
    }

    /*
     * Helper function for dump_integer
     *
     * This function takes a negative signed integer and returns its absolute
     * value as unsigned integer. The plus/minus shuffling is necessary as we can
     * not directly remove the sign of an arbitrary signed integer as the
     * absolute values of INT_MIN and INT_MAX are usually not the same. See
     * #1708 for details.
     */
    inline number_unsigned_t remove_sign(number_integer_t x) noexcept
    {
        assert(x < 0 and x < (std::numeric_limits<number_integer_t>::max)());
        return static_cast<number_unsigned_t>(-(x + 1)) + 1;
    }

  private:
    /// the output of the serializer
    output_adapter_t<char> o = nullptr;

    /// a (hopefully) large enough character buffer
    std::array<char, 64> number_buffer{{}};

    /// the locale
    const std::lconv* loc = nullptr;
    /// the locale's thousand separator character
    const char thousands_sep = '\0';
    /// the locale's decimal point character
    const char decimal_point = '\0';

    /// string buffer
    std::array<char, 512> string_buffer{{}};

    /// the indentation character
    const char indent_char;
    /// the indentation string
    string_t indent_string;

    /// error_handler how to react on decoding errors
    const error_handler_t error_handler;
};
}  // namespace detail
}  // namespace nlohmann

// #include <nlohmann/detail/value_t.hpp>

// #include <nlohmann/json_fwd.hpp>


/*!
@brief namespace for Niels Lohmann
@see https://github.com/nlohmann
@since version 1.0.0
*/
namespace nlohmann
{

/*!
@brief a class to store JSON values

@tparam ObjectType type for JSON objects (`std::map` by default; will be used
in @ref object_t)
@tparam ArrayType type for JSON arrays (`std::vector` by default; will be used
in @ref array_t)
@tparam StringType type for JSON strings and object keys (`std::string` by
default; will be used in @ref string_t)
@tparam BooleanType type for JSON booleans (`bool` by default; will be used
in @ref boolean_t)
@tparam NumberIntegerType type for JSON integer numbers (`int64_t` by
default; will be used in @ref number_integer_t)
@tparam NumberUnsignedType type for JSON unsigned integer numbers (@c
`uint64_t` by default; will be used in @ref number_unsigned_t)
@tparam NumberFloatType type for JSON floating-point numbers (`double` by
default; will be used in @ref number_float_t)
@tparam AllocatorType type of the allocator to use (`std::allocator` by
default)
@tparam JSONSerializer the serializer to resolve internal calls to `to_json()`
and `from_json()` (@ref adl_serializer by default)

@requirement The class satisfies the following concept requirements:
- Basic
 - [DefaultConstructible](https://en.cppreference.com/w/cpp/named_req/DefaultConstructible):
   JSON values can be default constructed. The result will be a JSON null
   value.
 - [MoveConstructible](https://en.cppreference.com/w/cpp/named_req/MoveConstructible):
   A JSON value can be constructed from an rvalue argument.
 - [CopyConstructible](https://en.cppreference.com/w/cpp/named_req/CopyConstructible):
   A JSON value can be copy-constructed from an lvalue expression.
 - [MoveAssignable](https://en.cppreference.com/w/cpp/named_req/MoveAssignable):
   A JSON value van be assigned from an rvalue argument.
 - [CopyAssignable](https://en.cppreference.com/w/cpp/named_req/CopyAssignable):
   A JSON value can be copy-assigned from an lvalue expression.
 - [Destructible](https://en.cppreference.com/w/cpp/named_req/Destructible):
   JSON values can be destructed.
- Layout
 - [StandardLayoutType](https://en.cppreference.com/w/cpp/named_req/StandardLayoutType):
   JSON values have
   [standard layout](https://en.cppreference.com/w/cpp/language/data_members#Standard_layout):
   All non-static data members are private and standard layout types, the
   class has no virtual functions or (virtual) base classes.
- Library-wide
 - [EqualityComparable](https://en.cppreference.com/w/cpp/named_req/EqualityComparable):
   JSON values can be compared with `==`, see @ref
   operator==(const_reference,const_reference).
 - [LessThanComparable](https://en.cppreference.com/w/cpp/named_req/LessThanComparable):
   JSON values can be compared with `<`, see @ref
   operator<(const_reference,const_reference).
 - [Swappable](https://en.cppreference.com/w/cpp/named_req/Swappable):
   Any JSON lvalue or rvalue of can be swapped with any lvalue or rvalue of
   other compatible types, using unqualified function call @ref swap().
 - [NullablePointer](https://en.cppreference.com/w/cpp/named_req/NullablePointer):
   JSON values can be compared against `std::nullptr_t` objects which are used
   to model the `null` value.
- Container
 - [Container](https://en.cppreference.com/w/cpp/named_req/Container):
   JSON values can be used like STL containers and provide iterator access.
 - [ReversibleContainer](https://en.cppreference.com/w/cpp/named_req/ReversibleContainer);
   JSON values can be used like STL containers and provide reverse iterator
   access.

@invariant The member variables @a m_value and @a m_type have the following
relationship:
- If `m_type == value_t::object`, then `m_value.object != nullptr`.
- If `m_type == value_t::array`, then `m_value.array != nullptr`.
- If `m_type == value_t::string`, then `m_value.string != nullptr`.
The invariants are checked by member function assert_invariant().

@internal
@note ObjectType trick from http://stackoverflow.com/a/9860911
@endinternal

@see [RFC 7159: The JavaScript Object Notation (JSON) Data Interchange
Format](http://rfc7159.net/rfc7159)

@since version 1.0.0

@nosubgrouping
*/
NLOHMANN_BASIC_JSON_TPL_DECLARATION
class basic_json
{
  private:
    template<detail::value_t> friend struct detail::external_constructor;
    friend ::nlohmann::json_pointer<basic_json>;
    friend ::nlohmann::detail::parser<basic_json>;
    friend ::nlohmann::detail::serializer<basic_json>;
    template<typename BasicJsonType>
    friend class ::nlohmann::detail::iter_impl;
    template<typename BasicJsonType, typename CharType>
    friend class ::nlohmann::detail::binary_writer;
    template<typename BasicJsonType, typename SAX>
    friend class ::nlohmann::detail::binary_reader;
    template<typename BasicJsonType>
    friend class ::nlohmann::detail::json_sax_dom_parser;
    template<typename BasicJsonType>
    friend class ::nlohmann::detail::json_sax_dom_callback_parser;

    /// workaround type for MSVC
    using basic_json_t = NLOHMANN_BASIC_JSON_TPL;

    // convenience aliases for types residing in namespace detail;
    using lexer = ::nlohmann::detail::lexer<basic_json>;
    using parser = ::nlohmann::detail::parser<basic_json>;

    using primitive_iterator_t = ::nlohmann::detail::primitive_iterator_t;
    template<typename BasicJsonType>
    using internal_iterator = ::nlohmann::detail::internal_iterator<BasicJsonType>;
    template<typename BasicJsonType>
    using iter_impl = ::nlohmann::detail::iter_impl<BasicJsonType>;
    template<typename Iterator>
    using iteration_proxy = ::nlohmann::detail::iteration_proxy<Iterator>;
    template<typename Base> using json_reverse_iterator = ::nlohmann::detail::json_reverse_iterator<Base>;

    template<typename CharType>
    using output_adapter_t = ::nlohmann::detail::output_adapter_t<CharType>;

    using binary_reader = ::nlohmann::detail::binary_reader<basic_json>;
    template<typename CharType> using binary_writer = ::nlohmann::detail::binary_writer<basic_json, CharType>;

    using serializer = ::nlohmann::detail::serializer<basic_json>;

  public:
    using value_t = detail::value_t;
    /// JSON Pointer, see @ref nlohmann::json_pointer
    using json_pointer = ::nlohmann::json_pointer<basic_json>;
    template<typename T, typename SFINAE>
    using json_serializer = JSONSerializer<T, SFINAE>;
    /// how to treat decoding errors
    using error_handler_t = detail::error_handler_t;
    /// helper type for initializer lists of basic_json values
    using initializer_list_t = std::initializer_list<detail::json_ref<basic_json>>;

    using input_format_t = detail::input_format_t;
    /// SAX interface type, see @ref nlohmann::json_sax
    using json_sax_t = json_sax<basic_json>;

    ////////////////
    // exceptions //
    ////////////////

    /// @name exceptions
    /// Classes to implement user-defined exceptions.
    /// @{

    /// @copydoc detail::exception
    using exception = detail::exception;
    /// @copydoc detail::parse_error
    using parse_error = detail::parse_error;
    /// @copydoc detail::invalid_iterator
    using invalid_iterator = detail::invalid_iterator;
    /// @copydoc detail::type_error
    using type_error = detail::type_error;
    /// @copydoc detail::out_of_range
    using out_of_range = detail::out_of_range;
    /// @copydoc detail::other_error
    using other_error = detail::other_error;

    /// @}


    /////////////////////
    // container types //
    /////////////////////

    /// @name container types
    /// The canonic container types to use @ref basic_json like any other STL
    /// container.
    /// @{

    /// the type of elements in a basic_json container
    using value_type = basic_json;

    /// the type of an element reference
    using reference = value_type&;
    /// the type of an element const reference
    using const_reference = const value_type&;

    /// a type to represent differences between iterators
    using difference_type = std::ptrdiff_t;
    /// a type to represent container sizes
    using size_type = std::size_t;

    /// the allocator type
    using allocator_type = AllocatorType<basic_json>;

    /// the type of an element pointer
    using pointer = typename std::allocator_traits<allocator_type>::pointer;
    /// the type of an element const pointer
    using const_pointer = typename std::allocator_traits<allocator_type>::const_pointer;

    /// an iterator for a basic_json container
    using iterator = iter_impl<basic_json>;
    /// a const iterator for a basic_json container
    using const_iterator = iter_impl<const basic_json>;
    /// a reverse iterator for a basic_json container
    using reverse_iterator = json_reverse_iterator<typename basic_json::iterator>;
    /// a const reverse iterator for a basic_json container
    using const_reverse_iterator = json_reverse_iterator<typename basic_json::const_iterator>;

    /// @}


    /*!
    @brief returns the allocator associated with the container
    */
    static allocator_type get_allocator()
    {
        return allocator_type();
    }

    /*!
    @brief returns version information on the library

    This function returns a JSON object with information about the library,
    including the version number and information on the platform and compiler.

    @return JSON object holding version information
    key         | description
    ----------- | ---------------
    `compiler`  | Information on the used compiler. It is an object with the following keys: `c++` (the used C++ standard), `family` (the compiler family; possible values are `clang`, `icc`, `gcc`, `ilecpp`, `msvc`, `pgcpp`, `sunpro`, and `unknown`), and `version` (the compiler version).
    `copyright` | The copyright line for the library as string.
    `name`      | The name of the library as string.
    `platform`  | The used platform as string. Possible values are `win32`, `linux`, `apple`, `unix`, and `unknown`.
    `url`       | The URL of the project as string.
    `version`   | The version of the library. It is an object with the following keys: `major`, `minor`, and `patch` as defined by [Semantic Versioning](http://semver.org), and `string` (the version string).

    @liveexample{The following code shows an example output of the `meta()`
    function.,meta}

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes to any JSON value.

    @complexity Constant.

    @since 2.1.0
    */
    JSON_HEDLEY_WARN_UNUSED_RESULT
    static basic_json meta()
    {
        basic_json result;

        result["copyright"] = "(C) 2013-2017 Niels Lohmann";
        result["name"] = "JSON for Modern C++";
        result["url"] = "https://github.com/nlohmann/json";
        result["version"]["string"] =
            std::to_string(NLOHMANN_JSON_VERSION_MAJOR) + "." +
            std::to_string(NLOHMANN_JSON_VERSION_MINOR) + "." +
            std::to_string(NLOHMANN_JSON_VERSION_PATCH);
        result["version"]["major"] = NLOHMANN_JSON_VERSION_MAJOR;
        result["version"]["minor"] = NLOHMANN_JSON_VERSION_MINOR;
        result["version"]["patch"] = NLOHMANN_JSON_VERSION_PATCH;

#ifdef _WIN32
        result["platform"] = "win32";
#elif defined __linux__
        result["platform"] = "linux";
#elif defined __APPLE__
        result["platform"] = "apple";
#elif defined __unix__
        result["platform"] = "unix";
#else
        result["platform"] = "unknown";
#endif

#if defined(__ICC) || defined(__INTEL_COMPILER)
        result["compiler"] = {{"family", "icc"}, {"version", __INTEL_COMPILER}};
#elif defined(__clang__)
        result["compiler"] = {{"family", "clang"}, {"version", __clang_version__}};
#elif defined(__GNUC__) || defined(__GNUG__)
        result["compiler"] = {{"family", "gcc"}, {"version", std::to_string(__GNUC__) + "." + std::to_string(__GNUC_MINOR__) + "." + std::to_string(__GNUC_PATCHLEVEL__)}};
#elif defined(__HP_cc) || defined(__HP_aCC)
        result["compiler"] = "hp"
#elif defined(__IBMCPP__)
        result["compiler"] = {{"family", "ilecpp"}, {"version", __IBMCPP__}};
#elif defined(_MSC_VER)
        result["compiler"] = {{"family", "msvc"}, {"version", _MSC_VER}};
#elif defined(__PGI)
        result["compiler"] = {{"family", "pgcpp"}, {"version", __PGI}};
#elif defined(__SUNPRO_CC)
        result["compiler"] = {{"family", "sunpro"}, {"version", __SUNPRO_CC}};
#else
        result["compiler"] = {{"family", "unknown"}, {"version", "unknown"}};
#endif

#ifdef __cplusplus
        result["compiler"]["c++"] = std::to_string(__cplusplus);
#else
        result["compiler"]["c++"] = "unknown";
#endif
        return result;
    }


    ///////////////////////////
    // JSON value data types //
    ///////////////////////////

    /// @name JSON value data types
    /// The data types to store a JSON value. These types are derived from
    /// the template arguments passed to class @ref basic_json.
    /// @{

#if defined(JSON_HAS_CPP_14)
    // Use transparent comparator if possible, combined with perfect forwarding
    // on find() and count() calls prevents unnecessary string construction.
    using object_comparator_t = std::less<>;
#else
    using object_comparator_t = std::less<StringType>;
#endif

    /*!
    @brief a type for an object

    [RFC 7159](http://rfc7159.net/rfc7159) describes JSON objects as follows:
    > An object is an unordered collection of zero or more name/value pairs,
    > where a name is a string and a value is a string, number, boolean, null,
    > object, or array.

    To store objects in C++, a type is defined by the template parameters
    described below.

    @tparam ObjectType  the container to store objects (e.g., `std::map` or
    `std::unordered_map`)
    @tparam StringType the type of the keys or names (e.g., `std::string`).
    The comparison function `std::less<StringType>` is used to order elements
    inside the container.
    @tparam AllocatorType the allocator to use for objects (e.g.,
    `std::allocator`)

    #### Default type

    With the default values for @a ObjectType (`std::map`), @a StringType
    (`std::string`), and @a AllocatorType (`std::allocator`), the default
    value for @a object_t is:

    @code {.cpp}
    std::map<
      std::string, // key_type
      basic_json, // value_type
      std::less<std::string>, // key_compare
      std::allocator<std::pair<const std::string, basic_json>> // allocator_type
    >
    @endcode

    #### Behavior

    The choice of @a object_t influences the behavior of the JSON class. With
    the default type, objects have the following behavior:

    - When all names are unique, objects will be interoperable in the sense
      that all software implementations receiving that object will agree on
      the name-value mappings.
    - When the names within an object are not unique, it is unspecified which
      one of the values for a given key will be chosen. For instance,
      `{"key": 2, "key": 1}` could be equal to either `{"key": 1}` or
      `{"key": 2}`.
    - Internally, name/value pairs are stored in lexicographical order of the
      names. Objects will also be serialized (see @ref dump) in this order.
      For instance, `{"b": 1, "a": 2}` and `{"a": 2, "b": 1}` will be stored
      and serialized as `{"a": 2, "b": 1}`.
    - When comparing objects, the order of the name/value pairs is irrelevant.
      This makes objects interoperable in the sense that they will not be
      affected by these differences. For instance, `{"b": 1, "a": 2}` and
      `{"a": 2, "b": 1}` will be treated as equal.

    #### Limits

    [RFC 7159](http://rfc7159.net/rfc7159) specifies:
    > An implementation may set limits on the maximum depth of nesting.

    In this class, the object's limit of nesting is not explicitly constrained.
    However, a maximum depth of nesting may be introduced by the compiler or
    runtime environment. A theoretical limit can be queried by calling the
    @ref max_size function of a JSON object.

    #### Storage

    Objects are stored as pointers in a @ref basic_json type. That is, for any
    access to object values, a pointer of type `object_t*` must be
    dereferenced.

    @sa @ref array_t -- type for an array value

    @since version 1.0.0

    @note The order name/value pairs are added to the object is *not*
    preserved by the library. Therefore, iterating an object may return
    name/value pairs in a different order than they were originally stored. In
    fact, keys will be traversed in alphabetical order as `std::map` with
    `std::less` is used by default. Please note this behavior conforms to [RFC
    7159](http://rfc7159.net/rfc7159), because any order implements the
    specified "unordered" nature of JSON objects.
    */
    using object_t = ObjectType<StringType,
          basic_json,
          object_comparator_t,
          AllocatorType<std::pair<const StringType,
          basic_json>>>;

    /*!
    @brief a type for an array

    [RFC 7159](http://rfc7159.net/rfc7159) describes JSON arrays as follows:
    > An array is an ordered sequence of zero or more values.

    To store objects in C++, a type is defined by the template parameters
    explained below.

    @tparam ArrayType  container type to store arrays (e.g., `std::vector` or
    `std::list`)
    @tparam AllocatorType allocator to use for arrays (e.g., `std::allocator`)

    #### Default type

    With the default values for @a ArrayType (`std::vector`) and @a
    AllocatorType (`std::allocator`), the default value for @a array_t is:

    @code {.cpp}
    std::vector<
      basic_json, // value_type
      std::allocator<basic_json> // allocator_type
    >
    @endcode

    #### Limits

    [RFC 7159](http://rfc7159.net/rfc7159) specifies:
    > An implementation may set limits on the maximum depth of nesting.

    In this class, the array's limit of nesting is not explicitly constrained.
    However, a maximum depth of nesting may be introduced by the compiler or
    runtime environment. A theoretical limit can be queried by calling the
    @ref max_size function of a JSON array.

    #### Storage

    Arrays are stored as pointers in a @ref basic_json type. That is, for any
    access to array values, a pointer of type `array_t*` must be dereferenced.

    @sa @ref object_t -- type for an object value

    @since version 1.0.0
    */
    using array_t = ArrayType<basic_json, AllocatorType<basic_json>>;

    /*!
    @brief a type for a string

    [RFC 7159](http://rfc7159.net/rfc7159) describes JSON strings as follows:
    > A string is a sequence of zero or more Unicode characters.

    To store objects in C++, a type is defined by the template parameter
    described below. Unicode values are split by the JSON class into
    byte-sized characters during deserialization.

    @tparam StringType  the container to store strings (e.g., `std::string`).
    Note this container is used for keys/names in objects, see @ref object_t.

    #### Default type

    With the default values for @a StringType (`std::string`), the default
    value for @a string_t is:

    @code {.cpp}
    std::string
    @endcode

    #### Encoding

    Strings are stored in UTF-8 encoding. Therefore, functions like
    `std::string::size()` or `std::string::length()` return the number of
    bytes in the string rather than the number of characters or glyphs.

    #### String comparison

    [RFC 7159](http://rfc7159.net/rfc7159) states:
    > Software implementations are typically required to test names of object
    > members for equality. Implementations that transform the textual
    > representation into sequences of Unicode code units and then perform the
    > comparison numerically, code unit by code unit, are interoperable in the
    > sense that implementations will agree in all cases on equality or
    > inequality of two strings. For example, implementations that compare
    > strings with escaped characters unconverted may incorrectly find that
    > `"a\\b"` and `"a\u005Cb"` are not equal.

    This implementation is interoperable as it does compare strings code unit
    by code unit.

    #### Storage

    String values are stored as pointers in a @ref basic_json type. That is,
    for any access to string values, a pointer of type `string_t*` must be
    dereferenced.

    @since version 1.0.0
    */
    using string_t = StringType;

    /*!
    @brief a type for a boolean

    [RFC 7159](http://rfc7159.net/rfc7159) implicitly describes a boolean as a
    type which differentiates the two literals `true` and `false`.

    To store objects in C++, a type is defined by the template parameter @a
    BooleanType which chooses the type to use.

    #### Default type

    With the default values for @a BooleanType (`bool`), the default value for
    @a boolean_t is:

    @code {.cpp}
    bool
    @endcode

    #### Storage

    Boolean values are stored directly inside a @ref basic_json type.

    @since version 1.0.0
    */
    using boolean_t = BooleanType;

    /*!
    @brief a type for a number (integer)

    [RFC 7159](http://rfc7159.net/rfc7159) describes numbers as follows:
    > The representation of numbers is similar to that used in most
    > programming languages. A number is represented in base 10 using decimal
    > digits. It contains an integer component that may be prefixed with an
    > optional minus sign, which may be followed by a fraction part and/or an
    > exponent part. Leading zeros are not allowed. (...) Numeric values that
    > cannot be represented in the grammar below (such as Infinity and NaN)
    > are not permitted.

    This description includes both integer and floating-point numbers.
    However, C++ allows more precise storage if it is known whether the number
    is a signed integer, an unsigned integer or a floating-point number.
    Therefore, three different types, @ref number_integer_t, @ref
    number_unsigned_t and @ref number_float_t are used.

    To store integer numbers in C++, a type is defined by the template
    parameter @a NumberIntegerType which chooses the type to use.

    #### Default type

    With the default values for @a NumberIntegerType (`int64_t`), the default
    value for @a number_integer_t is:

    @code {.cpp}
    int64_t
    @endcode

    #### Default behavior

    - The restrictions about leading zeros is not enforced in C++. Instead,
      leading zeros in integer literals lead to an interpretation as octal
      number. Internally, the value will be stored as decimal number. For
      instance, the C++ integer literal `010` will be serialized to `8`.
      During deserialization, leading zeros yield an error.
    - Not-a-number (NaN) values will be serialized to `null`.

    #### Limits

    [RFC 7159](http://rfc7159.net/rfc7159) specifies:
    > An implementation may set limits on the range and precision of numbers.

    When the default type is used, the maximal integer number that can be
    stored is `9223372036854775807` (INT64_MAX) and the minimal integer number
    that can be stored is `-9223372036854775808` (INT64_MIN). Integer numbers
    that are out of range will yield over/underflow when used in a
    constructor. During deserialization, too large or small integer numbers
    will be automatically be stored as @ref number_unsigned_t or @ref
    number_float_t.

    [RFC 7159](http://rfc7159.net/rfc7159) further states:
    > Note that when such software is used, numbers that are integers and are
    > in the range \f$[-2^{53}+1, 2^{53}-1]\f$ are interoperable in the sense
    > that implementations will agree exactly on their numeric values.

    As this range is a subrange of the exactly supported range [INT64_MIN,
    INT64_MAX], this class's integer type is interoperable.

    #### Storage

    Integer number values are stored directly inside a @ref basic_json type.

    @sa @ref number_float_t -- type for number values (floating-point)

    @sa @ref number_unsigned_t -- type for number values (unsigned integer)

    @since version 1.0.0
    */
    using number_integer_t = NumberIntegerType;

    /*!
    @brief a type for a number (unsigned)

    [RFC 7159](http://rfc7159.net/rfc7159) describes numbers as follows:
    > The representation of numbers is similar to that used in most
    > programming languages. A number is represented in base 10 using decimal
    > digits. It contains an integer component that may be prefixed with an
    > optional minus sign, which may be followed by a fraction part and/or an
    > exponent part. Leading zeros are not allowed. (...) Numeric values that
    > cannot be represented in the grammar below (such as Infinity and NaN)
    > are not permitted.

    This description includes both integer and floating-point numbers.
    However, C++ allows more precise storage if it is known whether the number
    is a signed integer, an unsigned integer or a floating-point number.
    Therefore, three different types, @ref number_integer_t, @ref
    number_unsigned_t and @ref number_float_t are used.

    To store unsigned integer numbers in C++, a type is defined by the
    template parameter @a NumberUnsignedType which chooses the type to use.

    #### Default type

    With the default values for @a NumberUnsignedType (`uint64_t`), the
    default value for @a number_unsigned_t is:

    @code {.cpp}
    uint64_t
    @endcode

    #### Default behavior

    - The restrictions about leading zeros is not enforced in C++. Instead,
      leading zeros in integer literals lead to an interpretation as octal
      number. Internally, the value will be stored as decimal number. For
      instance, the C++ integer literal `010` will be serialized to `8`.
      During deserialization, leading zeros yield an error.
    - Not-a-number (NaN) values will be serialized to `null`.

    #### Limits

    [RFC 7159](http://rfc7159.net/rfc7159) specifies:
    > An implementation may set limits on the range and precision of numbers.

    When the default type is used, the maximal integer number that can be
    stored is `18446744073709551615` (UINT64_MAX) and the minimal integer
    number that can be stored is `0`. Integer numbers that are out of range
    will yield over/underflow when used in a constructor. During
    deserialization, too large or small integer numbers will be automatically
    be stored as @ref number_integer_t or @ref number_float_t.

    [RFC 7159](http://rfc7159.net/rfc7159) further states:
    > Note that when such software is used, numbers that are integers and are
    > in the range \f$[-2^{53}+1, 2^{53}-1]\f$ are interoperable in the sense
    > that implementations will agree exactly on their numeric values.

    As this range is a subrange (when considered in conjunction with the
    number_integer_t type) of the exactly supported range [0, UINT64_MAX],
    this class's integer type is interoperable.

    #### Storage

    Integer number values are stored directly inside a @ref basic_json type.

    @sa @ref number_float_t -- type for number values (floating-point)
    @sa @ref number_integer_t -- type for number values (integer)

    @since version 2.0.0
    */
    using number_unsigned_t = NumberUnsignedType;

    /*!
    @brief a type for a number (floating-point)

    [RFC 7159](http://rfc7159.net/rfc7159) describes numbers as follows:
    > The representation of numbers is similar to that used in most
    > programming languages. A number is represented in base 10 using decimal
    > digits. It contains an integer component that may be prefixed with an
    > optional minus sign, which may be followed by a fraction part and/or an
    > exponent part. Leading zeros are not allowed. (...) Numeric values that
    > cannot be represented in the grammar below (such as Infinity and NaN)
    > are not permitted.

    This description includes both integer and floating-point numbers.
    However, C++ allows more precise storage if it is known whether the number
    is a signed integer, an unsigned integer or a floating-point number.
    Therefore, three different types, @ref number_integer_t, @ref
    number_unsigned_t and @ref number_float_t are used.

    To store floating-point numbers in C++, a type is defined by the template
    parameter @a NumberFloatType which chooses the type to use.

    #### Default type

    With the default values for @a NumberFloatType (`double`), the default
    value for @a number_float_t is:

    @code {.cpp}
    double
    @endcode

    #### Default behavior

    - The restrictions about leading zeros is not enforced in C++. Instead,
      leading zeros in floating-point literals will be ignored. Internally,
      the value will be stored as decimal number. For instance, the C++
      floating-point literal `01.2` will be serialized to `1.2`. During
      deserialization, leading zeros yield an error.
    - Not-a-number (NaN) values will be serialized to `null`.

    #### Limits

    [RFC 7159](http://rfc7159.net/rfc7159) states:
    > This specification allows implementations to set limits on the range and
    > precision of numbers accepted. Since software that implements IEEE
    > 754-2008 binary64 (double precision) numbers is generally available and
    > widely used, good interoperability can be achieved by implementations
    > that expect no more precision or range than these provide, in the sense
    > that implementations will approximate JSON numbers within the expected
    > precision.

    This implementation does exactly follow this approach, as it uses double
    precision floating-point numbers. Note values smaller than
    `-1.79769313486232e+308` and values greater than `1.79769313486232e+308`
    will be stored as NaN internally and be serialized to `null`.

    #### Storage

    Floating-point number values are stored directly inside a @ref basic_json
    type.

    @sa @ref number_integer_t -- type for number values (integer)

    @sa @ref number_unsigned_t -- type for number values (unsigned integer)

    @since version 1.0.0
    */
    using number_float_t = NumberFloatType;

    /// @}

  private:

    /// helper for exception-safe object creation
    template<typename T, typename... Args>
    JSON_HEDLEY_RETURNS_NON_NULL
    static T* create(Args&& ... args)
    {
        AllocatorType<T> alloc;
        using AllocatorTraits = std::allocator_traits<AllocatorType<T>>;

        auto deleter = [&](T * object)
        {
            AllocatorTraits::deallocate(alloc, object, 1);
        };
        std::unique_ptr<T, decltype(deleter)> object(AllocatorTraits::allocate(alloc, 1), deleter);
        AllocatorTraits::construct(alloc, object.get(), std::forward<Args>(args)...);
        assert(object != nullptr);
        return object.release();
    }

    ////////////////////////
    // JSON value storage //
    ////////////////////////

    /*!
    @brief a JSON value

    The actual storage for a JSON value of the @ref basic_json class. This
    union combines the different storage types for the JSON value types
    defined in @ref value_t.

    JSON type | value_t type    | used type
    --------- | --------------- | ------------------------
    object    | object          | pointer to @ref object_t
    array     | array           | pointer to @ref array_t
    string    | string          | pointer to @ref string_t
    boolean   | boolean         | @ref boolean_t
    number    | number_integer  | @ref number_integer_t
    number    | number_unsigned | @ref number_unsigned_t
    number    | number_float    | @ref number_float_t
    null      | null            | *no value is stored*

    @note Variable-length types (objects, arrays, and strings) are stored as
    pointers. The size of the union should not exceed 64 bits if the default
    value types are used.

    @since version 1.0.0
    */
    union json_value
    {
        /// object (stored with pointer to save storage)
        object_t* object;
        /// array (stored with pointer to save storage)
        array_t* array;
        /// string (stored with pointer to save storage)
        string_t* string;
        /// boolean
        boolean_t boolean;
        /// number (integer)
        number_integer_t number_integer;
        /// number (unsigned integer)
        number_unsigned_t number_unsigned;
        /// number (floating-point)
        number_float_t number_float;

        /// default constructor (for null values)
        json_value() = default;
        /// constructor for booleans
        json_value(boolean_t v) noexcept : boolean(v) {}
        /// constructor for numbers (integer)
        json_value(number_integer_t v) noexcept : number_integer(v) {}
        /// constructor for numbers (unsigned)
        json_value(number_unsigned_t v) noexcept : number_unsigned(v) {}
        /// constructor for numbers (floating-point)
        json_value(number_float_t v) noexcept : number_float(v) {}
        /// constructor for empty values of a given type
        json_value(value_t t)
        {
            switch (t)
            {
                case value_t::object:
                {
                    object = create<object_t>();
                    break;
                }

                case value_t::array:
                {
                    array = create<array_t>();
                    break;
                }

                case value_t::string:
                {
                    string = create<string_t>("");
                    break;
                }

                case value_t::boolean:
                {
                    boolean = boolean_t(false);
                    break;
                }

                case value_t::number_integer:
                {
                    number_integer = number_integer_t(0);
                    break;
                }

                case value_t::number_unsigned:
                {
                    number_unsigned = number_unsigned_t(0);
                    break;
                }

                case value_t::number_float:
                {
                    number_float = number_float_t(0.0);
                    break;
                }

                case value_t::null:
                {
                    object = nullptr;  // silence warning, see #821
                    break;
                }

                default:
                {
                    object = nullptr;  // silence warning, see #821
                    if (JSON_HEDLEY_UNLIKELY(t == value_t::null))
                    {
                        JSON_THROW(other_error::create(500, "961c151d2e87f2686a955a9be24d316f1362bf21 3.7.3")); // LCOV_EXCL_LINE
                    }
                    break;
                }
            }
        }

        /// constructor for strings
        json_value(const string_t& value)
        {
            string = create<string_t>(value);
        }

        /// constructor for rvalue strings
        json_value(string_t&& value)
        {
            string = create<string_t>(std::move(value));
        }

        /// constructor for objects
        json_value(const object_t& value)
        {
            object = create<object_t>(value);
        }

        /// constructor for rvalue objects
        json_value(object_t&& value)
        {
            object = create<object_t>(std::move(value));
        }

        /// constructor for arrays
        json_value(const array_t& value)
        {
            array = create<array_t>(value);
        }

        /// constructor for rvalue arrays
        json_value(array_t&& value)
        {
            array = create<array_t>(std::move(value));
        }

        void destroy(value_t t) noexcept
        {
            // flatten the current json_value to a heap-allocated stack
            std::vector<basic_json> stack;

            // move the top-level items to stack
            if (t == value_t::array)
            {
                stack.reserve(array->size());
                std::move(array->begin(), array->end(), std::back_inserter(stack));
            }
            else if (t == value_t::object)
            {
                stack.reserve(object->size());
                for (auto&& it : *object)
                {
                    stack.push_back(std::move(it.second));
                }
            }

            while (not stack.empty())
            {
                // move the last item to local variable to be processed
                basic_json current_item(std::move(stack.back()));
                stack.pop_back();

                // if current_item is array/object, move
                // its children to the stack to be processed later
                if (current_item.is_array())
                {
                    std::move(current_item.m_value.array->begin(), current_item.m_value.array->end(),
                              std::back_inserter(stack));

                    current_item.m_value.array->clear();
                }
                else if (current_item.is_object())
                {
                    for (auto&& it : *current_item.m_value.object)
                    {
                        stack.push_back(std::move(it.second));
                    }

                    current_item.m_value.object->clear();
                }

                // it's now safe that current_item get destructed
                // since it doesn't have any children
            }

            switch (t)
            {
                case value_t::object:
                {
                    AllocatorType<object_t> alloc;
                    std::allocator_traits<decltype(alloc)>::destroy(alloc, object);
                    std::allocator_traits<decltype(alloc)>::deallocate(alloc, object, 1);
                    break;
                }

                case value_t::array:
                {
                    AllocatorType<array_t> alloc;
                    std::allocator_traits<decltype(alloc)>::destroy(alloc, array);
                    std::allocator_traits<decltype(alloc)>::deallocate(alloc, array, 1);
                    break;
                }

                case value_t::string:
                {
                    AllocatorType<string_t> alloc;
                    std::allocator_traits<decltype(alloc)>::destroy(alloc, string);
                    std::allocator_traits<decltype(alloc)>::deallocate(alloc, string, 1);
                    break;
                }

                default:
                {
                    break;
                }
            }
        }
    };

    /*!
    @brief checks the class invariants

    This function asserts the class invariants. It needs to be called at the
    end of every constructor to make sure that created objects respect the
    invariant. Furthermore, it has to be called each time the type of a JSON
    value is changed, because the invariant expresses a relationship between
    @a m_type and @a m_value.
    */
    void assert_invariant() const noexcept
    {
        assert(m_type != value_t::object or m_value.object != nullptr);
        assert(m_type != value_t::array or m_value.array != nullptr);
        assert(m_type != value_t::string or m_value.string != nullptr);
    }

  public:
    //////////////////////////
    // JSON parser callback //
    //////////////////////////

    /*!
    @brief parser event types

    The parser callback distinguishes the following events:
    - `object_start`: the parser read `{` and started to process a JSON object
    - `key`: the parser read a key of a value in an object
    - `object_end`: the parser read `}` and finished processing a JSON object
    - `array_start`: the parser read `[` and started to process a JSON array
    - `array_end`: the parser read `]` and finished processing a JSON array
    - `value`: the parser finished reading a JSON value

    @image html callback_events.png "Example when certain parse events are triggered"

    @sa @ref parser_callback_t for more information and examples
    */
    using parse_event_t = typename parser::parse_event_t;

    /*!
    @brief per-element parser callback type

    With a parser callback function, the result of parsing a JSON text can be
    influenced. When passed to @ref parse, it is called on certain events
    (passed as @ref parse_event_t via parameter @a event) with a set recursion
    depth @a depth and context JSON value @a parsed. The return value of the
    callback function is a boolean indicating whether the element that emitted
    the callback shall be kept or not.

    We distinguish six scenarios (determined by the event type) in which the
    callback function can be called. The following table describes the values
    of the parameters @a depth, @a event, and @a parsed.

    parameter @a event | description | parameter @a depth | parameter @a parsed
    ------------------ | ----------- | ------------------ | -------------------
    parse_event_t::object_start | the parser read `{` and started to process a JSON object | depth of the parent of the JSON object | a JSON value with type discarded
    parse_event_t::key | the parser read a key of a value in an object | depth of the currently parsed JSON object | a JSON string containing the key
    parse_event_t::object_end | the parser read `}` and finished processing a JSON object | depth of the parent of the JSON object | the parsed JSON object
    parse_event_t::array_start | the parser read `[` and started to process a JSON array | depth of the parent of the JSON array | a JSON value with type discarded
    parse_event_t::array_end | the parser read `]` and finished processing a JSON array | depth of the parent of the JSON array | the parsed JSON array
    parse_event_t::value | the parser finished reading a JSON value | depth of the value | the parsed JSON value

    @image html callback_events.png "Example when certain parse events are triggered"

    Discarding a value (i.e., returning `false`) has different effects
    depending on the context in which function was called:

    - Discarded values in structured types are skipped. That is, the parser
      will behave as if the discarded value was never read.
    - In case a value outside a structured type is skipped, it is replaced
      with `null`. This case happens if the top-level element is skipped.

    @param[in] depth  the depth of the recursion during parsing

    @param[in] event  an event of type parse_event_t indicating the context in
    the callback function has been called

    @param[in,out] parsed  the current intermediate parse result; note that
    writing to this value has no effect for parse_event_t::key events

    @return Whether the JSON value which called the function during parsing
    should be kept (`true`) or not (`false`). In the latter case, it is either
    skipped completely or replaced by an empty discarded object.

    @sa @ref parse for examples

    @since version 1.0.0
    */
    using parser_callback_t = typename parser::parser_callback_t;

    //////////////////
    // constructors //
    //////////////////

    /// @name constructors and destructors
    /// Constructors of class @ref basic_json, copy/move constructor, copy
    /// assignment, static functions creating objects, and the destructor.
    /// @{

    /*!
    @brief create an empty value with a given type

    Create an empty JSON value with a given type. The value will be default
    initialized with an empty value which depends on the type:

    Value type  | initial value
    ----------- | -------------
    null        | `null`
    boolean     | `false`
    string      | `""`
    number      | `0`
    object      | `{}`
    array       | `[]`

    @param[in] v  the type of the value to create

    @complexity Constant.

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes to any JSON value.

    @liveexample{The following code shows the constructor for different @ref
    value_t values,basic_json__value_t}

    @sa @ref clear() -- restores the postcondition of this constructor

    @since version 1.0.0
    */
    basic_json(const value_t v)
        : m_type(v), m_value(v)
    {
        assert_invariant();
    }

    /*!
    @brief create a null object

    Create a `null` JSON value. It either takes a null pointer as parameter
    (explicitly creating `null`) or no parameter (implicitly creating `null`).
    The passed null pointer itself is not read -- it is only used to choose
    the right constructor.

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this constructor never throws
    exceptions.

    @liveexample{The following code shows the constructor with and without a
    null pointer parameter.,basic_json__nullptr_t}

    @since version 1.0.0
    */
    basic_json(std::nullptr_t = nullptr) noexcept
        : basic_json(value_t::null)
    {
        assert_invariant();
    }

    /*!
    @brief create a JSON value

    This is a "catch all" constructor for all compatible JSON types; that is,
    types for which a `to_json()` method exists. The constructor forwards the
    parameter @a val to that method (to `json_serializer<U>::to_json` method
    with `U = uncvref_t<CompatibleType>`, to be exact).

    Template type @a CompatibleType includes, but is not limited to, the
    following types:
    - **arrays**: @ref array_t and all kinds of compatible containers such as
      `std::vector`, `std::deque`, `std::list`, `std::forward_list`,
      `std::array`, `std::valarray`, `std::set`, `std::unordered_set`,
      `std::multiset`, and `std::unordered_multiset` with a `value_type` from
      which a @ref basic_json value can be constructed.
    - **objects**: @ref object_t and all kinds of compatible associative
      containers such as `std::map`, `std::unordered_map`, `std::multimap`,
      and `std::unordered_multimap` with a `key_type` compatible to
      @ref string_t and a `value_type` from which a @ref basic_json value can
      be constructed.
    - **strings**: @ref string_t, string literals, and all compatible string
      containers can be used.
    - **numbers**: @ref number_integer_t, @ref number_unsigned_t,
      @ref number_float_t, and all convertible number types such as `int`,
      `size_t`, `int64_t`, `float` or `double` can be used.
    - **boolean**: @ref boolean_t / `bool` can be used.

    See the examples below.

    @tparam CompatibleType a type such that:
    - @a CompatibleType is not derived from `std::istream`,
    - @a CompatibleType is not @ref basic_json (to avoid hijacking copy/move
         constructors),
    - @a CompatibleType is not a different @ref basic_json type (i.e. with different template arguments)
    - @a CompatibleType is not a @ref basic_json nested type (e.g.,
         @ref json_pointer, @ref iterator, etc ...)
    - @ref @ref json_serializer<U> has a
         `to_json(basic_json_t&, CompatibleType&&)` method

    @tparam U = `uncvref_t<CompatibleType>`

    @param[in] val the value to be forwarded to the respective constructor

    @complexity Usually linear in the size of the passed @a val, also
                depending on the implementation of the called `to_json()`
                method.

    @exceptionsafety Depends on the called constructor. For types directly
    supported by the library (i.e., all types for which no `to_json()` function
    was provided), strong guarantee holds: if an exception is thrown, there are
    no changes to any JSON value.

    @liveexample{The following code shows the constructor with several
    compatible types.,basic_json__CompatibleType}

    @since version 2.1.0
    */
    template <typename CompatibleType,
              typename U = detail::uncvref_t<CompatibleType>,
              detail::enable_if_t<
                  not detail::is_basic_json<U>::value and detail::is_compatible_type<basic_json_t, U>::value, int> = 0>
    basic_json(CompatibleType && val) noexcept(noexcept(
                JSONSerializer<U>::to_json(std::declval<basic_json_t&>(),
                                           std::forward<CompatibleType>(val))))
    {
        JSONSerializer<U>::to_json(*this, std::forward<CompatibleType>(val));
        assert_invariant();
    }

    /*!
    @brief create a JSON value from an existing one

    This is a constructor for existing @ref basic_json types.
    It does not hijack copy/move constructors, since the parameter has different
    template arguments than the current ones.

    The constructor tries to convert the internal @ref m_value of the parameter.

    @tparam BasicJsonType a type such that:
    - @a BasicJsonType is a @ref basic_json type.
    - @a BasicJsonType has different template arguments than @ref basic_json_t.

    @param[in] val the @ref basic_json value to be converted.

    @complexity Usually linear in the size of the passed @a val, also
                depending on the implementation of the called `to_json()`
                method.

    @exceptionsafety Depends on the called constructor. For types directly
    supported by the library (i.e., all types for which no `to_json()` function
    was provided), strong guarantee holds: if an exception is thrown, there are
    no changes to any JSON value.

    @since version 3.2.0
    */
    template <typename BasicJsonType,
              detail::enable_if_t<
                  detail::is_basic_json<BasicJsonType>::value and not std::is_same<basic_json, BasicJsonType>::value, int> = 0>
    basic_json(const BasicJsonType& val)
    {
        using other_boolean_t = typename BasicJsonType::boolean_t;
        using other_number_float_t = typename BasicJsonType::number_float_t;
        using other_number_integer_t = typename BasicJsonType::number_integer_t;
        using other_number_unsigned_t = typename BasicJsonType::number_unsigned_t;
        using other_string_t = typename BasicJsonType::string_t;
        using other_object_t = typename BasicJsonType::object_t;
        using other_array_t = typename BasicJsonType::array_t;

        switch (val.type())
        {
            case value_t::boolean:
                JSONSerializer<other_boolean_t>::to_json(*this, val.template get<other_boolean_t>());
                break;
            case value_t::number_float:
                JSONSerializer<other_number_float_t>::to_json(*this, val.template get<other_number_float_t>());
                break;
            case value_t::number_integer:
                JSONSerializer<other_number_integer_t>::to_json(*this, val.template get<other_number_integer_t>());
                break;
            case value_t::number_unsigned:
                JSONSerializer<other_number_unsigned_t>::to_json(*this, val.template get<other_number_unsigned_t>());
                break;
            case value_t::string:
                JSONSerializer<other_string_t>::to_json(*this, val.template get_ref<const other_string_t&>());
                break;
            case value_t::object:
                JSONSerializer<other_object_t>::to_json(*this, val.template get_ref<const other_object_t&>());
                break;
            case value_t::array:
                JSONSerializer<other_array_t>::to_json(*this, val.template get_ref<const other_array_t&>());
                break;
            case value_t::null:
                *this = nullptr;
                break;
            case value_t::discarded:
                m_type = value_t::discarded;
                break;
            default:            // LCOV_EXCL_LINE
                assert(false);  // LCOV_EXCL_LINE
        }
        assert_invariant();
    }

    /*!
    @brief create a container (array or object) from an initializer list

    Creates a JSON value of type array or object from the passed initializer
    list @a init. In case @a type_deduction is `true` (default), the type of
    the JSON value to be created is deducted from the initializer list @a init
    according to the following rules:

    1. If the list is empty, an empty JSON object value `{}` is created.
    2. If the list consists of pairs whose first element is a string, a JSON
       object value is created where the first elements of the pairs are
       treated as keys and the second elements are as values.
    3. In all other cases, an array is created.

    The rules aim to create the best fit between a C++ initializer list and
    JSON values. The rationale is as follows:

    1. The empty initializer list is written as `{}` which is exactly an empty
       JSON object.
    2. C++ has no way of describing mapped types other than to list a list of
       pairs. As JSON requires that keys must be of type string, rule 2 is the
       weakest constraint one can pose on initializer lists to interpret them
       as an object.
    3. In all other cases, the initializer list could not be interpreted as
       JSON object type, so interpreting it as JSON array type is safe.

    With the rules described above, the following JSON values cannot be
    expressed by an initializer list:

    - the empty array (`[]`): use @ref array(initializer_list_t)
      with an empty initializer list in this case
    - arrays whose elements satisfy rule 2: use @ref
      array(initializer_list_t) with the same initializer list
      in this case

    @note When used without parentheses around an empty initializer list, @ref
    basic_json() is called instead of this function, yielding the JSON null
    value.

    @param[in] init  initializer list with JSON values

    @param[in] type_deduction internal parameter; when set to `true`, the type
    of the JSON value is deducted from the initializer list @a init; when set
    to `false`, the type provided via @a manual_type is forced. This mode is
    used by the functions @ref array(initializer_list_t) and
    @ref object(initializer_list_t).

    @param[in] manual_type internal parameter; when @a type_deduction is set
    to `false`, the created JSON value will use the provided type (only @ref
    value_t::array and @ref value_t::object are valid); when @a type_deduction
    is set to `true`, this parameter has no effect

    @throw type_error.301 if @a type_deduction is `false`, @a manual_type is
    `value_t::object`, but @a init contains an element which is not a pair
    whose first element is a string. In this case, the constructor could not
    create an object. If @a type_deduction would have be `true`, an array
    would have been created. See @ref object(initializer_list_t)
    for an example.

    @complexity Linear in the size of the initializer list @a init.

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes to any JSON value.

    @liveexample{The example below shows how JSON values are created from
    initializer lists.,basic_json__list_init_t}

    @sa @ref array(initializer_list_t) -- create a JSON array
    value from an initializer list
    @sa @ref object(initializer_list_t) -- create a JSON object
    value from an initializer list

    @since version 1.0.0
    */
    basic_json(initializer_list_t init,
               bool type_deduction = true,
               value_t manual_type = value_t::array)
    {
        // check if each element is an array with two elements whose first
        // element is a string
        bool is_an_object = std::all_of(init.begin(), init.end(),
                                        [](const detail::json_ref<basic_json>& element_ref)
        {
            return element_ref->is_array() and element_ref->size() == 2 and (*element_ref)[0].is_string();
        });

        // adjust type if type deduction is not wanted
        if (not type_deduction)
        {
            // if array is wanted, do not create an object though possible
            if (manual_type == value_t::array)
            {
                is_an_object = false;
            }

            // if object is wanted but impossible, throw an exception
            if (JSON_HEDLEY_UNLIKELY(manual_type == value_t::object and not is_an_object))
            {
                JSON_THROW(type_error::create(301, "cannot create object from initializer list"));
            }
        }

        if (is_an_object)
        {
            // the initializer list is a list of pairs -> create object
            m_type = value_t::object;
            m_value = value_t::object;

            std::for_each(init.begin(), init.end(), [this](const detail::json_ref<basic_json>& element_ref)
            {
                auto element = element_ref.moved_or_copied();
                m_value.object->emplace(
                    std::move(*((*element.m_value.array)[0].m_value.string)),
                    std::move((*element.m_value.array)[1]));
            });
        }
        else
        {
            // the initializer list describes an array -> create array
            m_type = value_t::array;
            m_value.array = create<array_t>(init.begin(), init.end());
        }

        assert_invariant();
    }

    /*!
    @brief explicitly create an array from an initializer list

    Creates a JSON array value from a given initializer list. That is, given a
    list of values `a, b, c`, creates the JSON value `[a, b, c]`. If the
    initializer list is empty, the empty array `[]` is created.

    @note This function is only needed to express two edge cases that cannot
    be realized with the initializer list constructor (@ref
    basic_json(initializer_list_t, bool, value_t)). These cases
    are:
    1. creating an array whose elements are all pairs whose first element is a
    string -- in this case, the initializer list constructor would create an
    object, taking the first elements as keys
    2. creating an empty array -- passing the empty initializer list to the
    initializer list constructor yields an empty object

    @param[in] init  initializer list with JSON values to create an array from
    (optional)

    @return JSON array value

    @complexity Linear in the size of @a init.

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes to any JSON value.

    @liveexample{The following code shows an example for the `array`
    function.,array}

    @sa @ref basic_json(initializer_list_t, bool, value_t) --
    create a JSON value from an initializer list
    @sa @ref object(initializer_list_t) -- create a JSON object
    value from an initializer list

    @since version 1.0.0
    */
    JSON_HEDLEY_WARN_UNUSED_RESULT
    static basic_json array(initializer_list_t init = {})
    {
        return basic_json(init, false, value_t::array);
    }

    /*!
    @brief explicitly create an object from an initializer list

    Creates a JSON object value from a given initializer list. The initializer
    lists elements must be pairs, and their first elements must be strings. If
    the initializer list is empty, the empty object `{}` is created.

    @note This function is only added for symmetry reasons. In contrast to the
    related function @ref array(initializer_list_t), there are
    no cases which can only be expressed by this function. That is, any
    initializer list @a init can also be passed to the initializer list
    constructor @ref basic_json(initializer_list_t, bool, value_t).

    @param[in] init  initializer list to create an object from (optional)

    @return JSON object value

    @throw type_error.301 if @a init is not a list of pairs whose first
    elements are strings. In this case, no object can be created. When such a
    value is passed to @ref basic_json(initializer_list_t, bool, value_t),
    an array would have been created from the passed initializer list @a init.
    See example below.

    @complexity Linear in the size of @a init.

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes to any JSON value.

    @liveexample{The following code shows an example for the `object`
    function.,object}

    @sa @ref basic_json(initializer_list_t, bool, value_t) --
    create a JSON value from an initializer list
    @sa @ref array(initializer_list_t) -- create a JSON array
    value from an initializer list

    @since version 1.0.0
    */
    JSON_HEDLEY_WARN_UNUSED_RESULT
    static basic_json object(initializer_list_t init = {})
    {
        return basic_json(init, false, value_t::object);
    }

    /*!
    @brief construct an array with count copies of given value

    Constructs a JSON array value by creating @a cnt copies of a passed value.
    In case @a cnt is `0`, an empty array is created.

    @param[in] cnt  the number of JSON copies of @a val to create
    @param[in] val  the JSON value to copy

    @post `std::distance(begin(),end()) == cnt` holds.

    @complexity Linear in @a cnt.

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes to any JSON value.

    @liveexample{The following code shows examples for the @ref
    basic_json(size_type\, const basic_json&)
    constructor.,basic_json__size_type_basic_json}

    @since version 1.0.0
    */
    basic_json(size_type cnt, const basic_json& val)
        : m_type(value_t::array)
    {
        m_value.array = create<array_t>(cnt, val);
        assert_invariant();
    }

    /*!
    @brief construct a JSON container given an iterator range

    Constructs the JSON value with the contents of the range `[first, last)`.
    The semantics depends on the different types a JSON value can have:
    - In case of a null type, invalid_iterator.206 is thrown.
    - In case of other primitive types (number, boolean, or string), @a first
      must be `begin()` and @a last must be `end()`. In this case, the value is
      copied. Otherwise, invalid_iterator.204 is thrown.
    - In case of structured types (array, object), the constructor behaves as
      similar versions for `std::vector` or `std::map`; that is, a JSON array
      or object is constructed from the values in the range.

    @tparam InputIT an input iterator type (@ref iterator or @ref
    const_iterator)

    @param[in] first begin of the range to copy from (included)
    @param[in] last end of the range to copy from (excluded)

    @pre Iterators @a first and @a last must be initialized. **This
         precondition is enforced with an assertion (see warning).** If
         assertions are switched off, a violation of this precondition yields
         undefined behavior.

    @pre Range `[first, last)` is valid. Usually, this precondition cannot be
         checked efficiently. Only certain edge cases are detected; see the
         description of the exceptions below. A violation of this precondition
         yields undefined behavior.

    @warning A precondition is enforced with a runtime assertion that will
             result in calling `std::abort` if this precondition is not met.
             Assertions can be disabled by defining `NDEBUG` at compile time.
             See https://en.cppreference.com/w/cpp/error/assert for more
             information.

    @throw invalid_iterator.201 if iterators @a first and @a last are not
    compatible (i.e., do not belong to the same JSON value). In this case,
    the range `[first, last)` is undefined.
    @throw invalid_iterator.204 if iterators @a first and @a last belong to a
    primitive type (number, boolean, or string), but @a first does not point
    to the first element any more. In this case, the range `[first, last)` is
    undefined. See example code below.
    @throw invalid_iterator.206 if iterators @a first and @a last belong to a
    null value. In this case, the range `[first, last)` is undefined.

    @complexity Linear in distance between @a first and @a last.

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes to any JSON value.

    @liveexample{The example below shows several ways to create JSON values by
    specifying a subrange with iterators.,basic_json__InputIt_InputIt}

    @since version 1.0.0
    */
    template<class InputIT, typename std::enable_if<
                 std::is_same<InputIT, typename basic_json_t::iterator>::value or
                 std::is_same<InputIT, typename basic_json_t::const_iterator>::value, int>::type = 0>
    basic_json(InputIT first, InputIT last)
    {
        assert(first.m_object != nullptr);
        assert(last.m_object != nullptr);

        // make sure iterator fits the current value
        if (JSON_HEDLEY_UNLIKELY(first.m_object != last.m_object))
        {
            JSON_THROW(invalid_iterator::create(201, "iterators are not compatible"));
        }

        // copy type from first iterator
        m_type = first.m_object->m_type;

        // check if iterator range is complete for primitive values
        switch (m_type)
        {
            case value_t::boolean:
            case value_t::number_float:
            case value_t::number_integer:
            case value_t::number_unsigned:
            case value_t::string:
            {
                if (JSON_HEDLEY_UNLIKELY(not first.m_it.primitive_iterator.is_begin()
                                         or not last.m_it.primitive_iterator.is_end()))
                {
                    JSON_THROW(invalid_iterator::create(204, "iterators out of range"));
                }
                break;
            }

            default:
                break;
        }

        switch (m_type)
        {
            case value_t::number_integer:
            {
                m_value.number_integer = first.m_object->m_value.number_integer;
                break;
            }

            case value_t::number_unsigned:
            {
                m_value.number_unsigned = first.m_object->m_value.number_unsigned;
                break;
            }

            case value_t::number_float:
            {
                m_value.number_float = first.m_object->m_value.number_float;
                break;
            }

            case value_t::boolean:
            {
                m_value.boolean = first.m_object->m_value.boolean;
                break;
            }

            case value_t::string:
            {
                m_value = *first.m_object->m_value.string;
                break;
            }

            case value_t::object:
            {
                m_value.object = create<object_t>(first.m_it.object_iterator,
                                                  last.m_it.object_iterator);
                break;
            }

            case value_t::array:
            {
                m_value.array = create<array_t>(first.m_it.array_iterator,
                                                last.m_it.array_iterator);
                break;
            }

            default:
                JSON_THROW(invalid_iterator::create(206, "cannot construct with iterators from " +
                                                    std::string(first.m_object->type_name())));
        }

        assert_invariant();
    }


    ///////////////////////////////////////
    // other constructors and destructor //
    ///////////////////////////////////////

    /// @private
    basic_json(const detail::json_ref<basic_json>& ref)
        : basic_json(ref.moved_or_copied())
    {}

    /*!
    @brief copy constructor

    Creates a copy of a given JSON value.

    @param[in] other  the JSON value to copy

    @post `*this == other`

    @complexity Linear in the size of @a other.

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes to any JSON value.

    @requirement This function helps `basic_json` satisfying the
    [Container](https://en.cppreference.com/w/cpp/named_req/Container)
    requirements:
    - The complexity is linear.
    - As postcondition, it holds: `other == basic_json(other)`.

    @liveexample{The following code shows an example for the copy
    constructor.,basic_json__basic_json}

    @since version 1.0.0
    */
    basic_json(const basic_json& other)
        : m_type(other.m_type)
    {
        // check of passed value is valid
        other.assert_invariant();

        switch (m_type)
        {
            case value_t::object:
            {
                m_value = *other.m_value.object;
                break;
            }

            case value_t::array:
            {
                m_value = *other.m_value.array;
                break;
            }

            case value_t::string:
            {
                m_value = *other.m_value.string;
                break;
            }

            case value_t::boolean:
            {
                m_value = other.m_value.boolean;
                break;
            }

            case value_t::number_integer:
            {
                m_value = other.m_value.number_integer;
                break;
            }

            case value_t::number_unsigned:
            {
                m_value = other.m_value.number_unsigned;
                break;
            }

            case value_t::number_float:
            {
                m_value = other.m_value.number_float;
                break;
            }

            default:
                break;
        }

        assert_invariant();
    }

    /*!
    @brief move constructor

    Move constructor. Constructs a JSON value with the contents of the given
    value @a other using move semantics. It "steals" the resources from @a
    other and leaves it as JSON null value.

    @param[in,out] other  value to move to this object

    @post `*this` has the same value as @a other before the call.
    @post @a other is a JSON null value.

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this constructor never throws
    exceptions.

    @requirement This function helps `basic_json` satisfying the
    [MoveConstructible](https://en.cppreference.com/w/cpp/named_req/MoveConstructible)
    requirements.

    @liveexample{The code below shows the move constructor explicitly called
    via std::move.,basic_json__moveconstructor}

    @since version 1.0.0
    */
    basic_json(basic_json&& other) noexcept
        : m_type(std::move(other.m_type)),
          m_value(std::move(other.m_value))
    {
        // check that passed value is valid
        other.assert_invariant();

        // invalidate payload
        other.m_type = value_t::null;
        other.m_value = {};

        assert_invariant();
    }

    /*!
    @brief copy assignment

    Copy assignment operator. Copies a JSON value via the "copy and swap"
    strategy: It is expressed in terms of the copy constructor, destructor,
    and the `swap()` member function.

    @param[in] other  value to copy from

    @complexity Linear.

    @requirement This function helps `basic_json` satisfying the
    [Container](https://en.cppreference.com/w/cpp/named_req/Container)
    requirements:
    - The complexity is linear.

    @liveexample{The code below shows and example for the copy assignment. It
    creates a copy of value `a` which is then swapped with `b`. Finally\, the
    copy of `a` (which is the null value after the swap) is
    destroyed.,basic_json__copyassignment}

    @since version 1.0.0
    */
    basic_json& operator=(basic_json other) noexcept (
        std::is_nothrow_move_constructible<value_t>::value and
        std::is_nothrow_move_assignable<value_t>::value and
        std::is_nothrow_move_constructible<json_value>::value and
        std::is_nothrow_move_assignable<json_value>::value
    )
    {
        // check that passed value is valid
        other.assert_invariant();

        using std::swap;
        swap(m_type, other.m_type);
        swap(m_value, other.m_value);

        assert_invariant();
        return *this;
    }

    /*!
    @brief destructor

    Destroys the JSON value and frees all allocated memory.

    @complexity Linear.

    @requirement This function helps `basic_json` satisfying the
    [Container](https://en.cppreference.com/w/cpp/named_req/Container)
    requirements:
    - The complexity is linear.
    - All stored elements are destroyed and all memory is freed.

    @since version 1.0.0
    */
    ~basic_json() noexcept
    {
        assert_invariant();
        m_value.destroy(m_type);
    }

    /// @}

  public:
    ///////////////////////
    // object inspection //
    ///////////////////////

    /// @name object inspection
    /// Functions to inspect the type of a JSON value.
    /// @{

    /*!
    @brief serialization

    Serialization function for JSON values. The function tries to mimic
    Python's `json.dumps()` function, and currently supports its @a indent
    and @a ensure_ascii parameters.

    @param[in] indent If indent is nonnegative, then array elements and object
    members will be pretty-printed with that indent level. An indent level of
    `0` will only insert newlines. `-1` (the default) selects the most compact
    representation.
    @param[in] indent_char The character to use for indentation if @a indent is
    greater than `0`. The default is ` ` (space).
    @param[in] ensure_ascii If @a ensure_ascii is true, all non-ASCII characters
    in the output are escaped with `\uXXXX` sequences, and the result consists
    of ASCII characters only.
    @param[in] error_handler  how to react on decoding errors; there are three
    possible values: `strict` (throws and exception in case a decoding error
    occurs; default), `replace` (replace invalid UTF-8 sequences with U+FFFD),
    and `ignore` (ignore invalid UTF-8 sequences during serialization).

    @return string containing the serialization of the JSON value

    @throw type_error.316 if a string stored inside the JSON value is not
                          UTF-8 encoded

    @complexity Linear.

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes in the JSON value.

    @liveexample{The following example shows the effect of different @a indent\,
    @a indent_char\, and @a ensure_ascii parameters to the result of the
    serialization.,dump}

    @see https://docs.python.org/2/library/json.html#json.dump

    @since version 1.0.0; indentation character @a indent_char, option
           @a ensure_ascii and exceptions added in version 3.0.0; error
           handlers added in version 3.4.0.
    */
    string_t dump(const int indent = -1,
                  const char indent_char = ' ',
                  const bool ensure_ascii = false,
                  const error_handler_t error_handler = error_handler_t::strict) const
    {
        string_t result;
        serializer s(detail::output_adapter<char, string_t>(result), indent_char, error_handler);

        if (indent >= 0)
        {
            s.dump(*this, true, ensure_ascii, static_cast<unsigned int>(indent));
        }
        else
        {
            s.dump(*this, false, ensure_ascii, 0);
        }

        return result;
    }

    /*!
    @brief return the type of the JSON value (explicit)

    Return the type of the JSON value as a value from the @ref value_t
    enumeration.

    @return the type of the JSON value
            Value type                | return value
            ------------------------- | -------------------------
            null                      | value_t::null
            boolean                   | value_t::boolean
            string                    | value_t::string
            number (integer)          | value_t::number_integer
            number (unsigned integer) | value_t::number_unsigned
            number (floating-point)   | value_t::number_float
            object                    | value_t::object
            array                     | value_t::array
            discarded                 | value_t::discarded

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this member function never throws
    exceptions.

    @liveexample{The following code exemplifies `type()` for all JSON
    types.,type}

    @sa @ref operator value_t() -- return the type of the JSON value (implicit)
    @sa @ref type_name() -- return the type as string

    @since version 1.0.0
    */
    constexpr value_t type() const noexcept
    {
        return m_type;
    }

    /*!
    @brief return whether type is primitive

    This function returns true if and only if the JSON type is primitive
    (string, number, boolean, or null).

    @return `true` if type is primitive (string, number, boolean, or null),
    `false` otherwise.

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this member function never throws
    exceptions.

    @liveexample{The following code exemplifies `is_primitive()` for all JSON
    types.,is_primitive}

    @sa @ref is_structured() -- returns whether JSON value is structured
    @sa @ref is_null() -- returns whether JSON value is `null`
    @sa @ref is_string() -- returns whether JSON value is a string
    @sa @ref is_boolean() -- returns whether JSON value is a boolean
    @sa @ref is_number() -- returns whether JSON value is a number

    @since version 1.0.0
    */
    constexpr bool is_primitive() const noexcept
    {
        return is_null() or is_string() or is_boolean() or is_number();
    }

    /*!
    @brief return whether type is structured

    This function returns true if and only if the JSON type is structured
    (array or object).

    @return `true` if type is structured (array or object), `false` otherwise.

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this member function never throws
    exceptions.

    @liveexample{The following code exemplifies `is_structured()` for all JSON
    types.,is_structured}

    @sa @ref is_primitive() -- returns whether value is primitive
    @sa @ref is_array() -- returns whether value is an array
    @sa @ref is_object() -- returns whether value is an object

    @since version 1.0.0
    */
    constexpr bool is_structured() const noexcept
    {
        return is_array() or is_object();
    }

    /*!
    @brief return whether value is null

    This function returns true if and only if the JSON value is null.

    @return `true` if type is null, `false` otherwise.

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this member function never throws
    exceptions.

    @liveexample{The following code exemplifies `is_null()` for all JSON
    types.,is_null}

    @since version 1.0.0
    */
    constexpr bool is_null() const noexcept
    {
        return m_type == value_t::null;
    }

    /*!
    @brief return whether value is a boolean

    This function returns true if and only if the JSON value is a boolean.

    @return `true` if type is boolean, `false` otherwise.

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this member function never throws
    exceptions.

    @liveexample{The following code exemplifies `is_boolean()` for all JSON
    types.,is_boolean}

    @since version 1.0.0
    */
    constexpr bool is_boolean() const noexcept
    {
        return m_type == value_t::boolean;
    }

    /*!
    @brief return whether value is a number

    This function returns true if and only if the JSON value is a number. This
    includes both integer (signed and unsigned) and floating-point values.

    @return `true` if type is number (regardless whether integer, unsigned
    integer or floating-type), `false` otherwise.

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this member function never throws
    exceptions.

    @liveexample{The following code exemplifies `is_number()` for all JSON
    types.,is_number}

    @sa @ref is_number_integer() -- check if value is an integer or unsigned
    integer number
    @sa @ref is_number_unsigned() -- check if value is an unsigned integer
    number
    @sa @ref is_number_float() -- check if value is a floating-point number

    @since version 1.0.0
    */
    constexpr bool is_number() const noexcept
    {
        return is_number_integer() or is_number_float();
    }

    /*!
    @brief return whether value is an integer number

    This function returns true if and only if the JSON value is a signed or
    unsigned integer number. This excludes floating-point values.

    @return `true` if type is an integer or unsigned integer number, `false`
    otherwise.

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this member function never throws
    exceptions.

    @liveexample{The following code exemplifies `is_number_integer()` for all
    JSON types.,is_number_integer}

    @sa @ref is_number() -- check if value is a number
    @sa @ref is_number_unsigned() -- check if value is an unsigned integer
    number
    @sa @ref is_number_float() -- check if value is a floating-point number

    @since version 1.0.0
    */
    constexpr bool is_number_integer() const noexcept
    {
        return m_type == value_t::number_integer or m_type == value_t::number_unsigned;
    }

    /*!
    @brief return whether value is an unsigned integer number

    This function returns true if and only if the JSON value is an unsigned
    integer number. This excludes floating-point and signed integer values.

    @return `true` if type is an unsigned integer number, `false` otherwise.

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this member function never throws
    exceptions.

    @liveexample{The following code exemplifies `is_number_unsigned()` for all
    JSON types.,is_number_unsigned}

    @sa @ref is_number() -- check if value is a number
    @sa @ref is_number_integer() -- check if value is an integer or unsigned
    integer number
    @sa @ref is_number_float() -- check if value is a floating-point number

    @since version 2.0.0
    */
    constexpr bool is_number_unsigned() const noexcept
    {
        return m_type == value_t::number_unsigned;
    }

    /*!
    @brief return whether value is a floating-point number

    This function returns true if and only if the JSON value is a
    floating-point number. This excludes signed and unsigned integer values.

    @return `true` if type is a floating-point number, `false` otherwise.

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this member function never throws
    exceptions.

    @liveexample{The following code exemplifies `is_number_float()` for all
    JSON types.,is_number_float}

    @sa @ref is_number() -- check if value is number
    @sa @ref is_number_integer() -- check if value is an integer number
    @sa @ref is_number_unsigned() -- check if value is an unsigned integer
    number

    @since version 1.0.0
    */
    constexpr bool is_number_float() const noexcept
    {
        return m_type == value_t::number_float;
    }

    /*!
    @brief return whether value is an object

    This function returns true if and only if the JSON value is an object.

    @return `true` if type is object, `false` otherwise.

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this member function never throws
    exceptions.

    @liveexample{The following code exemplifies `is_object()` for all JSON
    types.,is_object}

    @since version 1.0.0
    */
    constexpr bool is_object() const noexcept
    {
        return m_type == value_t::object;
    }

    /*!
    @brief return whether value is an array

    This function returns true if and only if the JSON value is an array.

    @return `true` if type is array, `false` otherwise.

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this member function never throws
    exceptions.

    @liveexample{The following code exemplifies `is_array()` for all JSON
    types.,is_array}

    @since version 1.0.0
    */
    constexpr bool is_array() const noexcept
    {
        return m_type == value_t::array;
    }

    /*!
    @brief return whether value is a string

    This function returns true if and only if the JSON value is a string.

    @return `true` if type is string, `false` otherwise.

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this member function never throws
    exceptions.

    @liveexample{The following code exemplifies `is_string()` for all JSON
    types.,is_string}

    @since version 1.0.0
    */
    constexpr bool is_string() const noexcept
    {
        return m_type == value_t::string;
    }

    /*!
    @brief return whether value is discarded

    This function returns true if and only if the JSON value was discarded
    during parsing with a callback function (see @ref parser_callback_t).

    @note This function will always be `false` for JSON values after parsing.
    That is, discarded values can only occur during parsing, but will be
    removed when inside a structured value or replaced by null in other cases.

    @return `true` if type is discarded, `false` otherwise.

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this member function never throws
    exceptions.

    @liveexample{The following code exemplifies `is_discarded()` for all JSON
    types.,is_discarded}

    @since version 1.0.0
    */
    constexpr bool is_discarded() const noexcept
    {
        return m_type == value_t::discarded;
    }

    /*!
    @brief return the type of the JSON value (implicit)

    Implicitly return the type of the JSON value as a value from the @ref
    value_t enumeration.

    @return the type of the JSON value

    @complexity Constant.

    @exceptionsafety No-throw guarantee: this member function never throws
    exceptions.

    @liveexample{The following code exemplifies the @ref value_t operator for
    all JSON types.,operator__value_t}

    @sa @ref type() -- return the type of the JSON value (explicit)
    @sa @ref type_name() -- return the type as string

    @since version 1.0.0
    */
    constexpr operator value_t() const noexcept
    {
        return m_type;
    }

    /// @}

  private:
    //////////////////
    // value access //
    //////////////////

    /// get a boolean (explicit)
    boolean_t get_impl(boolean_t* /*unused*/) const
    {
        if (JSON_HEDLEY_LIKELY(is_boolean()))
        {
            return m_value.boolean;
        }

        JSON_THROW(type_error::create(302, "type must be boolean, but is " + std::string(type_name())));
    }

    /// get a pointer to the value (object)
    object_t* get_impl_ptr(object_t* /*unused*/) noexcept
    {
        return is_object() ? m_value.object : nullptr;
    }

    /// get a pointer to the value (object)
    constexpr const object_t* get_impl_ptr(const object_t* /*unused*/) const noexcept
    {
        return is_object() ? m_value.object : nullptr;
    }

    /// get a pointer to the value (array)
    array_t* get_impl_ptr(array_t* /*unused*/) noexcept
    {
        return is_array() ? m_value.array : nullptr;
    }

    /// get a pointer to the value (array)
    constexpr const array_t* get_impl_ptr(const array_t* /*unused*/) const noexcept
    {
        return is_array() ? m_value.array : nullptr;
    }

    /// get a pointer to the value (string)
    string_t* get_impl_ptr(string_t* /*unused*/) noexcept
    {
        return is_string() ? m_value.string : nullptr;
    }

    /// get a pointer to the value (string)
    constexpr const string_t* get_impl_ptr(const string_t* /*unused*/) const noexcept
    {
        return is_string() ? m_value.string : nullptr;
    }

    /// get a pointer to the value (boolean)
    boolean_t* get_impl_ptr(boolean_t* /*unused*/) noexcept
    {
        return is_boolean() ? &m_value.boolean : nullptr;
    }

    /// get a pointer to the value (boolean)
    constexpr const boolean_t* get_impl_ptr(const boolean_t* /*unused*/) const noexcept
    {
        return is_boolean() ? &m_value.boolean : nullptr;
    }

    /// get a pointer to the value (integer number)
    number_integer_t* get_impl_ptr(number_integer_t* /*unused*/) noexcept
    {
        return is_number_integer() ? &m_value.number_integer : nullptr;
    }

    /// get a pointer to the value (integer number)
    constexpr const number_integer_t* get_impl_ptr(const number_integer_t* /*unused*/) const noexcept
    {
        return is_number_integer() ? &m_value.number_integer : nullptr;
    }

    /// get a pointer to the value (unsigned number)
    number_unsigned_t* get_impl_ptr(number_unsigned_t* /*unused*/) noexcept
    {
        return is_number_unsigned() ? &m_value.number_unsigned : nullptr;
    }

    /// get a pointer to the value (unsigned number)
    constexpr const number_unsigned_t* get_impl_ptr(const number_unsigned_t* /*unused*/) const noexcept
    {
        return is_number_unsigned() ? &m_value.number_unsigned : nullptr;
    }

    /// get a pointer to the value (floating-point number)
    number_float_t* get_impl_ptr(number_float_t* /*unused*/) noexcept
    {
        return is_number_float() ? &m_value.number_float : nullptr;
    }

    /// get a pointer to the value (floating-point number)
    constexpr const number_float_t* get_impl_ptr(const number_float_t* /*unused*/) const noexcept
    {
        return is_number_float() ? &m_value.number_float : nullptr;
    }

    /*!
    @brief helper function to implement get_ref()

    This function helps to implement get_ref() without code duplication for
    const and non-const overloads

    @tparam ThisType will be deduced as `basic_json` or `const basic_json`

    @throw type_error.303 if ReferenceType does not match underlying value
    type of the current JSON
    */
    template<typename ReferenceType, typename ThisType>
    static ReferenceType get_ref_impl(ThisType& obj)
    {
        // delegate the call to get_ptr<>()
        auto ptr = obj.template get_ptr<typename std::add_pointer<ReferenceType>::type>();

        if (JSON_HEDLEY_LIKELY(ptr != nullptr))
        {
            return *ptr;
        }

        JSON_THROW(type_error::create(303, "incompatible ReferenceType for get_ref, actual type is " + std::string(obj.type_name())));
    }

  public:
    /// @name value access
    /// Direct access to the stored value of a JSON value.
    /// @{

    /*!
    @brief get special-case overload

    This overloads avoids a lot of template boilerplate, it can be seen as the
    identity method

    @tparam BasicJsonType == @ref basic_json

    @return a copy of *this

    @complexity Constant.

    @since version 2.1.0
    */
    template<typename BasicJsonType, detail::enable_if_t<
                 std::is_same<typename std::remove_const<BasicJsonType>::type, basic_json_t>::value,
                 int> = 0>
    basic_json get() const
    {
        return *this;
    }

    /*!
    @brief get special-case overload

    This overloads converts the current @ref basic_json in a different
    @ref basic_json type

    @tparam BasicJsonType == @ref basic_json

    @return a copy of *this, converted into @tparam BasicJsonType

    @complexity Depending on the implementation of the called `from_json()`
                method.

    @since version 3.2.0
    */
    template<typename BasicJsonType, detail::enable_if_t<
                 not std::is_same<BasicJsonType, basic_json>::value and
                 detail::is_basic_json<BasicJsonType>::value, int> = 0>
    BasicJsonType get() const
    {
        return *this;
    }

    /*!
    @brief get a value (explicit)

    Explicit type conversion between the JSON value and a compatible value
    which is [CopyConstructible](https://en.cppreference.com/w/cpp/named_req/CopyConstructible)
    and [DefaultConstructible](https://en.cppreference.com/w/cpp/named_req/DefaultConstructible).
    The value is converted by calling the @ref json_serializer<ValueType>
    `from_json()` method.

    The function is equivalent to executing
    @code {.cpp}
    ValueType ret;
    JSONSerializer<ValueType>::from_json(*this, ret);
    return ret;
    @endcode

    This overloads is chosen if:
    - @a ValueType is not @ref basic_json,
    - @ref json_serializer<ValueType> has a `from_json()` method of the form
      `void from_json(const basic_json&, ValueType&)`, and
    - @ref json_serializer<ValueType> does not have a `from_json()` method of
      the form `ValueType from_json(const basic_json&)`

    @tparam ValueTypeCV the provided value type
    @tparam ValueType the returned value type

    @return copy of the JSON value, converted to @a ValueType

    @throw what @ref json_serializer<ValueType> `from_json()` method throws

    @liveexample{The example below shows several conversions from JSON values
    to other types. There a few things to note: (1) Floating-point numbers can
    be converted to integers\, (2) A JSON array can be converted to a standard
    `std::vector<short>`\, (3) A JSON object can be converted to C++
    associative containers such as `std::unordered_map<std::string\,
    json>`.,get__ValueType_const}

    @since version 2.1.0
    */
    template<typename ValueTypeCV, typename ValueType = detail::uncvref_t<ValueTypeCV>,
             detail::enable_if_t <
                 not detail::is_basic_json<ValueType>::value and
                 detail::has_from_json<basic_json_t, ValueType>::value and
                 not detail::has_non_default_from_json<basic_json_t, ValueType>::value,
                 int> = 0>
    ValueType get() const noexcept(noexcept(
                                       JSONSerializer<ValueType>::from_json(std::declval<const basic_json_t&>(), std::declval<ValueType&>())))
    {
        // we cannot static_assert on ValueTypeCV being non-const, because
        // there is support for get<const basic_json_t>(), which is why we
        // still need the uncvref
        static_assert(not std::is_reference<ValueTypeCV>::value,
                      "get() cannot be used with reference types, you might want to use get_ref()");
        static_assert(std::is_default_constructible<ValueType>::value,
                      "types must be DefaultConstructible when used with get()");

        ValueType ret;
        JSONSerializer<ValueType>::from_json(*this, ret);
        return ret;
    }

    /*!
    @brief get a value (explicit); special case

    Explicit type conversion between the JSON value and a compatible value
    which is **not** [CopyConstructible](https://en.cppreference.com/w/cpp/named_req/CopyConstructible)
    and **not** [DefaultConstructible](https://en.cppreference.com/w/cpp/named_req/DefaultConstructible).
    The value is converted by calling the @ref json_serializer<ValueType>
    `from_json()` method.

    The function is equivalent to executing
    @code {.cpp}
    return JSONSerializer<ValueTypeCV>::from_json(*this);
    @endcode

    This overloads is chosen if:
    - @a ValueType is not @ref basic_json and
    - @ref json_serializer<ValueType> has a `from_json()` method of the form
      `ValueType from_json(const basic_json&)`

    @note If @ref json_serializer<ValueType> has both overloads of
    `from_json()`, this one is chosen.

    @tparam ValueTypeCV the provided value type
    @tparam ValueType the returned value type

    @return copy of the JSON value, converted to @a ValueType

    @throw what @ref json_serializer<ValueType> `from_json()` method throws

    @since version 2.1.0
    */
    template<typename ValueTypeCV, typename ValueType = detail::uncvref_t<ValueTypeCV>,
             detail::enable_if_t<not std::is_same<basic_json_t, ValueType>::value and
                                 detail::has_non_default_from_json<basic_json_t, ValueType>::value,
                                 int> = 0>
    ValueType get() const noexcept(noexcept(
                                       JSONSerializer<ValueType>::from_json(std::declval<const basic_json_t&>())))
    {
        static_assert(not std::is_reference<ValueTypeCV>::value,
                      "get() cannot be used with reference types, you might want to use get_ref()");
        return JSONSerializer<ValueType>::from_json(*this);
    }

    /*!
    @brief get a value (explicit)

    Explicit type conversion between the JSON value and a compatible value.
    The value is filled into the input parameter by calling the @ref json_serializer<ValueType>
    `from_json()` method.

    The function is equivalent to executing
    @code {.cpp}
    ValueType v;
    JSONSerializer<ValueType>::from_json(*this, v);
    @endcode

    This overloads is chosen if:
    - @a ValueType is not @ref basic_json,
    - @ref json_serializer<ValueType> has a `from_json()` method of the form
      `void from_json(const basic_json&, ValueType&)`, and

    @tparam ValueType the input parameter type.

    @return the input parameter, allowing chaining calls.

    @throw what @ref json_serializer<ValueType> `from_json()` method throws

    @liveexample{The example below shows several conversions from JSON values
    to other types. There a few things to note: (1) Floating-point numbers can
    be converted to integers\, (2) A JSON array can be converted to a standard
    `std::vector<short>`\, (3) A JSON object can be converted to C++
    associative containers such as `std::unordered_map<std::string\,
    json>`.,get_to}

    @since version 3.3.0
    */
    template<typename ValueType,
             detail::enable_if_t <
                 not detail::is_basic_json<ValueType>::value and
                 detail::has_from_json<basic_json_t, ValueType>::value,
                 int> = 0>
    ValueType & get_to(ValueType& v) const noexcept(noexcept(
                JSONSerializer<ValueType>::from_json(std::declval<const basic_json_t&>(), v)))
    {
        JSONSerializer<ValueType>::from_json(*this, v);
        return v;
    }

    template <
        typename T, std::size_t N,
        typename Array = T (&)[N],
        detail::enable_if_t <
            detail::has_from_json<basic_json_t, Array>::value, int > = 0 >
    Array get_to(T (&v)[N]) const
    noexcept(noexcept(JSONSerializer<Array>::from_json(
                          std::declval<const basic_json_t&>(), v)))
    {
        JSONSerializer<Array>::from_json(*this, v);
        return v;
    }


    /*!
    @brief get a pointer value (implicit)

    Implicit pointer access to the internally stored JSON value. No copies are
    made.

    @warning Writing data to the pointee of the result yields an undefined
    state.

    @tparam PointerType pointer type; must be a pointer to @ref array_t, @ref
    object_t, @ref string_t, @ref boolean_t, @ref number_integer_t,
    @ref number_unsigned_t, or @ref number_float_t. Enforced by a static
    assertion.

    @return pointer to the internally stored JSON value if the requested
    pointer type @a PointerType fits to the JSON value; `nullptr` otherwise

    @complexity Constant.

    @liveexample{The example below shows how pointers to internal values of a
    JSON value can be requested. Note that no type conversions are made and a
    `nullptr` is returned if the value and the requested pointer type does not
    match.,get_ptr}

    @since version 1.0.0
    */
    template<typename PointerType, typename std::enable_if<
                 std::is_pointer<PointerType>::value, int>::type = 0>
    auto get_ptr() noexcept -> decltype(std::declval<basic_json_t&>().get_impl_ptr(std::declval<PointerType>()))
    {
        // delegate the call to get_impl_ptr<>()
        return get_impl_ptr(static_cast<PointerType>(nullptr));
    }

    /*!
    @brief get a pointer value (implicit)
    @copydoc get_ptr()
    */
    template<typename PointerType, typename std::enable_if<
                 std::is_pointer<PointerType>::value and
                 std::is_const<typename std::remove_pointer<PointerType>::type>::value, int>::type = 0>
    constexpr auto get_ptr() const noexcept -> decltype(std::declval<const basic_json_t&>().get_impl_ptr(std::declval<PointerType>()))
    {
        // delegate the call to get_impl_ptr<>() const
        return get_impl_ptr(static_cast<PointerType>(nullptr));
    }

    /*!
    @brief get a pointer value (explicit)

    Explicit pointer access to the internally stored JSON value. No copies are
    made.

    @warning The pointer becomes invalid if the underlying JSON object
    changes.

    @tparam PointerType pointer type; must be a pointer to @ref array_t, @ref
    object_t, @ref string_t, @ref boolean_t, @ref number_integer_t,
    @ref number_unsigned_t, or @ref number_float_t.

    @return pointer to the internally stored JSON value if the requested
    pointer type @a PointerType fits to the JSON value; `nullptr` otherwise

    @complexity Constant.

    @liveexample{The example below shows how pointers to internal values of a
    JSON value can be requested. Note that no type conversions are made and a
    `nullptr` is returned if the value and the requested pointer type does not
    match.,get__PointerType}

    @sa @ref get_ptr() for explicit pointer-member access

    @since version 1.0.0
    */
    template<typename PointerType, typename std::enable_if<
                 std::is_pointer<PointerType>::value, int>::type = 0>
    auto get() noexcept -> decltype(std::declval<basic_json_t&>().template get_ptr<PointerType>())
    {
        // delegate the call to get_ptr
        return get_ptr<PointerType>();
    }

    /*!
    @brief get a pointer value (explicit)
    @copydoc get()
    */
    template<typename PointerType, typename std::enable_if<
                 std::is_pointer<PointerType>::value, int>::type = 0>
    constexpr auto get() const noexcept -> decltype(std::declval<const basic_json_t&>().template get_ptr<PointerType>())
    {
        // delegate the call to get_ptr
        return get_ptr<PointerType>();
    }

    /*!
    @brief get a reference value (implicit)

    Implicit reference access to the internally stored JSON value. No copies
    are made.

    @warning Writing data to the referee of the result yields an undefined
    state.

    @tparam ReferenceType reference type; must be a reference to @ref array_t,
    @ref object_t, @ref string_t, @ref boolean_t, @ref number_integer_t, or
    @ref number_float_t. Enforced by static assertion.

    @return reference to the internally stored JSON value if the requested
    reference type @a ReferenceType fits to the JSON value; throws
    type_error.303 otherwise

    @throw type_error.303 in case passed type @a ReferenceType is incompatible
    with the stored JSON value; see example below

    @complexity Constant.

    @liveexample{The example shows several calls to `get_ref()`.,get_ref}

    @since version 1.1.0
    */
    template<typename ReferenceType, typename std::enable_if<
                 std::is_reference<ReferenceType>::value, int>::type = 0>
    ReferenceType get_ref()
    {
        // delegate call to get_ref_impl
        return get_ref_impl<ReferenceType>(*this);
    }

    /*!
    @brief get a reference value (implicit)
    @copydoc get_ref()
    */
    template<typename ReferenceType, typename std::enable_if<
                 std::is_reference<ReferenceType>::value and
                 std::is_const<typename std::remove_reference<ReferenceType>::type>::value, int>::type = 0>
    ReferenceType get_ref() const
    {
        // delegate call to get_ref_impl
        return get_ref_impl<ReferenceType>(*this);
    }

    /*!
    @brief get a value (implicit)

    Implicit type conversion between the JSON value and a compatible value.
    The call is realized by calling @ref get() const.

    @tparam ValueType non-pointer type compatible to the JSON value, for
    instance `int` for JSON integer numbers, `bool` for JSON booleans, or
    `std::vector` types for JSON arrays. The character type of @ref string_t
    as well as an initializer list of this type is excluded to avoid
    ambiguities as these types implicitly convert to `std::string`.

    @return copy of the JSON value, converted to type @a ValueType

    @throw type_error.302 in case passed type @a ValueType is incompatible
    to the JSON value type (e.g., the JSON value is of type boolean, but a
    string is requested); see example below

    @complexity Linear in the size of the JSON value.

    @liveexample{The example below shows several conversions from JSON values
    to other types. There a few things to note: (1) Floating-point numbers can
    be converted to integers\, (2) A JSON array can be converted to a standard
    `std::vector<short>`\, (3) A JSON object can be converted to C++
    associative containers such as `std::unordered_map<std::string\,
    json>`.,operator__ValueType}

    @since version 1.0.0
    */
    template < typename ValueType, typename std::enable_if <
                   not std::is_pointer<ValueType>::value and
                   not std::is_same<ValueType, detail::json_ref<basic_json>>::value and
                   not std::is_same<ValueType, typename string_t::value_type>::value and
                   not detail::is_basic_json<ValueType>::value

#ifndef _MSC_VER  // fix for issue #167 operator<< ambiguity under VS2015
                   and not std::is_same<ValueType, std::initializer_list<typename string_t::value_type>>::value
#if defined(JSON_HAS_CPP_17) && (defined(__GNUC__) || (defined(_MSC_VER) and _MSC_VER <= 1914))
                   and not std::is_same<ValueType, typename std::string_view>::value
#endif
#endif
                   and detail::is_detected<detail::get_template_function, const basic_json_t&, ValueType>::value
                   , int >::type = 0 >
    operator ValueType() const
    {
        // delegate the call to get<>() const
        return get<ValueType>();
    }

    /// @}


    ////////////////////
    // element access //
    ////////////////////

    /// @name element access
    /// Access to the JSON value.
    /// @{

    /*!
    @brief access specified array element with bounds checking

    Returns a reference to the element at specified location @a idx, with
    bounds checking.

    @param[in] idx  index of the element to access

    @return reference to the element at index @a idx

    @throw type_error.304 if the JSON value is not an array; in this case,
    calling `at` with an index makes no sense. See example below.
    @throw out_of_range.401 if the index @a idx is out of range of the array;
    that is, `idx >= size()`. See example below.

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes in the JSON value.

    @complexity Constant.

    @since version 1.0.0

    @liveexample{The example below shows how array elements can be read and
    written using `at()`. It also demonstrates the different exceptions that
    can be thrown.,at__size_type}
    */
    reference at(size_type idx)
    {
        // at only works for arrays
        if (JSON_HEDLEY_LIKELY(is_array()))
        {
            JSON_TRY
            {
                return m_value.array->at(idx);
            }
            JSON_CATCH (std::out_of_range&)
            {
                // create better exception explanation
                JSON_THROW(out_of_range::create(401, "array index " + std::to_string(idx) + " is out of range"));
            }
        }
        else
        {
            JSON_THROW(type_error::create(304, "cannot use at() with " + std::string(type_name())));
        }
    }

    /*!
    @brief access specified array element with bounds checking

    Returns a const reference to the element at specified location @a idx,
    with bounds checking.

    @param[in] idx  index of the element to access

    @return const reference to the element at index @a idx

    @throw type_error.304 if the JSON value is not an array; in this case,
    calling `at` with an index makes no sense. See example below.
    @throw out_of_range.401 if the index @a idx is out of range of the array;
    that is, `idx >= size()`. See example below.

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes in the JSON value.

    @complexity Constant.

    @since version 1.0.0

    @liveexample{The example below shows how array elements can be read using
    `at()`. It also demonstrates the different exceptions that can be thrown.,
    at__size_type_const}
    */
    const_reference at(size_type idx) const
    {
        // at only works for arrays
        if (JSON_HEDLEY_LIKELY(is_array()))
        {
            JSON_TRY
            {
                return m_value.array->at(idx);
            }
            JSON_CATCH (std::out_of_range&)
            {
                // create better exception explanation
                JSON_THROW(out_of_range::create(401, "array index " + std::to_string(idx) + " is out of range"));
            }
        }
        else
        {
            JSON_THROW(type_error::create(304, "cannot use at() with " + std::string(type_name())));
        }
    }

    /*!
    @brief access specified object element with bounds checking

    Returns a reference to the element at with specified key @a key, with
    bounds checking.

    @param[in] key  key of the element to access

    @return reference to the element at key @a key

    @throw type_error.304 if the JSON value is not an object; in this case,
    calling `at` with a key makes no sense. See example below.
    @throw out_of_range.403 if the key @a key is is not stored in the object;
    that is, `find(key) == end()`. See example below.

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes in the JSON value.

    @complexity Logarithmic in the size of the container.

    @sa @ref operator[](const typename object_t::key_type&) for unchecked
    access by reference
    @sa @ref value() for access by value with a default value

    @since version 1.0.0

    @liveexample{The example below shows how object elements can be read and
    written using `at()`. It also demonstrates the different exceptions that
    can be thrown.,at__object_t_key_type}
    */
    reference at(const typename object_t::key_type& key)
    {
        // at only works for objects
        if (JSON_HEDLEY_LIKELY(is_object()))
        {
            JSON_TRY
            {
                return m_value.object->at(key);
            }
            JSON_CATCH (std::out_of_range&)
            {
                // create better exception explanation
                JSON_THROW(out_of_range::create(403, "key '" + key + "' not found"));
            }
        }
        else
        {
            JSON_THROW(type_error::create(304, "cannot use at() with " + std::string(type_name())));
        }
    }

    /*!
    @brief access specified object element with bounds checking

    Returns a const reference to the element at with specified key @a key,
    with bounds checking.

    @param[in] key  key of the element to access

    @return const reference to the element at key @a key

    @throw type_error.304 if the JSON value is not an object; in this case,
    calling `at` with a key makes no sense. See example below.
    @throw out_of_range.403 if the key @a key is is not stored in the object;
    that is, `find(key) == end()`. See example below.

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes in the JSON value.

    @complexity Logarithmic in the size of the container.

    @sa @ref operator[](const typename object_t::key_type&) for unchecked
    access by reference
    @sa @ref value() for access by value with a default value

    @since version 1.0.0

    @liveexample{The example below shows how object elements can be read using
    `at()`. It also demonstrates the different exceptions that can be thrown.,
    at__object_t_key_type_const}
    */
    const_reference at(const typename object_t::key_type& key) const
    {
        // at only works for objects
        if (JSON_HEDLEY_LIKELY(is_object()))
        {
            JSON_TRY
            {
                return m_value.object->at(key);
            }
            JSON_CATCH (std::out_of_range&)
            {
                // create better exception explanation
                JSON_THROW(out_of_range::create(403, "key '" + key + "' not found"));
            }
        }
        else
        {
            JSON_THROW(type_error::create(304, "cannot use at() with " + std::string(type_name())));
        }
    }

    /*!
    @brief access specified array element

    Returns a reference to the element at specified location @a idx.

    @note If @a idx is beyond the range of the array (i.e., `idx >= size()`),
    then the array is silently filled up with `null` values to make `idx` a
    valid reference to the last stored element.

    @param[in] idx  index of the element to access

    @return reference to the element at index @a idx

    @throw type_error.305 if the JSON value is not an array or null; in that
    cases, using the [] operator with an index makes no sense.

    @complexity Constant if @a idx is in the range of the array. Otherwise
    linear in `idx - size()`.

    @liveexample{The example below shows how array elements can be read and
    written using `[]` operator. Note the addition of `null`
    values.,operatorarray__size_type}

    @since version 1.0.0
    */
    reference operator[](size_type idx)
    {
        // implicitly convert null value to an empty array
        if (is_null())
        {
            m_type = value_t::array;
            m_value.array = create<array_t>();
            assert_invariant();
        }

        // operator[] only works for arrays
        if (JSON_HEDLEY_LIKELY(is_array()))
        {
            // fill up array with null values if given idx is outside range
            if (idx >= m_value.array->size())
            {
                m_value.array->insert(m_value.array->end(),
                                      idx - m_value.array->size() + 1,
                                      basic_json());
            }

            return m_value.array->operator[](idx);
        }

        JSON_THROW(type_error::create(305, "cannot use operator[] with a numeric argument with " + std::string(type_name())));
    }

    /*!
    @brief access specified array element

    Returns a const reference to the element at specified location @a idx.

    @param[in] idx  index of the element to access

    @return const reference to the element at index @a idx

    @throw type_error.305 if the JSON value is not an array; in that case,
    using the [] operator with an index makes no sense.

    @complexity Constant.

    @liveexample{The example below shows how array elements can be read using
    the `[]` operator.,operatorarray__size_type_const}

    @since version 1.0.0
    */
    const_reference operator[](size_type idx) const
    {
        // const operator[] only works for arrays
        if (JSON_HEDLEY_LIKELY(is_array()))
        {
            return m_value.array->operator[](idx);
        }

        JSON_THROW(type_error::create(305, "cannot use operator[] with a numeric argument with " + std::string(type_name())));
    }

    /*!
    @brief access specified object element

    Returns a reference to the element at with specified key @a key.

    @note If @a key is not found in the object, then it is silently added to
    the object and filled with a `null` value to make `key` a valid reference.
    In case the value was `null` before, it is converted to an object.

    @param[in] key  key of the element to access

    @return reference to the element at key @a key

    @throw type_error.305 if the JSON value is not an object or null; in that
    cases, using the [] operator with a key makes no sense.

    @complexity Logarithmic in the size of the container.

    @liveexample{The example below shows how object elements can be read and
    written using the `[]` operator.,operatorarray__key_type}

    @sa @ref at(const typename object_t::key_type&) for access by reference
    with range checking
    @sa @ref value() for access by value with a default value

    @since version 1.0.0
    */
    reference operator[](const typename object_t::key_type& key)
    {
        // implicitly convert null value to an empty object
        if (is_null())
        {
            m_type = value_t::object;
            m_value.object = create<object_t>();
            assert_invariant();
        }

        // operator[] only works for objects
        if (JSON_HEDLEY_LIKELY(is_object()))
        {
            return m_value.object->operator[](key);
        }

        JSON_THROW(type_error::create(305, "cannot use operator[] with a string argument with " + std::string(type_name())));
    }

    /*!
    @brief read-only access specified object element

    Returns a const reference to the element at with specified key @a key. No
    bounds checking is performed.

    @warning If the element with key @a key does not exist, the behavior is
    undefined.

    @param[in] key  key of the element to access

    @return const reference to the element at key @a key

    @pre The element with key @a key must exist. **This precondition is
         enforced with an assertion.**

    @throw type_error.305 if the JSON value is not an object; in that case,
    using the [] operator with a key makes no sense.

    @complexity Logarithmic in the size of the container.

    @liveexample{The example below shows how object elements can be read using
    the `[]` operator.,operatorarray__key_type_const}

    @sa @ref at(const typename object_t::key_type&) for access by reference
    with range checking
    @sa @ref value() for access by value with a default value

    @since version 1.0.0
    */
    const_reference operator[](const typename object_t::key_type& key) const
    {
        // const operator[] only works for objects
        if (JSON_HEDLEY_LIKELY(is_object()))
        {
            assert(m_value.object->find(key) != m_value.object->end());
            return m_value.object->find(key)->second;
        }

        JSON_THROW(type_error::create(305, "cannot use operator[] with a string argument with " + std::string(type_name())));
    }

    /*!
    @brief access specified object element

    Returns a reference to the element at with specified key @a key.

    @note If @a key is not found in the object, then it is silently added to
    the object and filled with a `null` value to make `key` a valid reference.
    In case the value was `null` before, it is converted to an object.

    @param[in] key  key of the element to access

    @return reference to the element at key @a key

    @throw type_error.305 if the JSON value is not an object or null; in that
    cases, using the [] operator with a key makes no sense.

    @complexity Logarithmic in the size of the container.

    @liveexample{The example below shows how object elements can be read and
    written using the `[]` operator.,operatorarray__key_type}

    @sa @ref at(const typename object_t::key_type&) for access by reference
    with range checking
    @sa @ref value() for access by value with a default value

    @since version 1.1.0
    */
    template<typename T>
    JSON_HEDLEY_NON_NULL(2)
    reference operator[](T* key)
    {
        // implicitly convert null to object
        if (is_null())
        {
            m_type = value_t::object;
            m_value = value_t::object;
            assert_invariant();
        }

        // at only works for objects
        if (JSON_HEDLEY_LIKELY(is_object()))
        {
            return m_value.object->operator[](key);
        }

        JSON_THROW(type_error::create(305, "cannot use operator[] with a string argument with " + std::string(type_name())));
    }

    /*!
    @brief read-only access specified object element

    Returns a const reference to the element at with specified key @a key. No
    bounds checking is performed.

    @warning If the element with key @a key does not exist, the behavior is
    undefined.

    @param[in] key  key of the element to access

    @return const reference to the element at key @a key

    @pre The element with key @a key must exist. **This precondition is
         enforced with an assertion.**

    @throw type_error.305 if the JSON value is not an object; in that case,
    using the [] operator with a key makes no sense.

    @complexity Logarithmic in the size of the container.

    @liveexample{The example below shows how object elements can be read using
    the `[]` operator.,operatorarray__key_type_const}

    @sa @ref at(const typename object_t::key_type&) for access by reference
    with range checking
    @sa @ref value() for access by value with a default value

    @since version 1.1.0
    */
    template<typename T>
    JSON_HEDLEY_NON_NULL(2)
    const_reference operator[](T* key) const
    {
        // at only works for objects
        if (JSON_HEDLEY_LIKELY(is_object()))
        {
            assert(m_value.object->find(key) != m_value.object->end());
            return m_value.object->find(key)->second;
        }

        JSON_THROW(type_error::create(305, "cannot use operator[] with a string argument with " + std::string(type_name())));
    }

    /*!
    @brief access specified object element with default value

    Returns either a copy of an object's element at the specified key @a key
    or a given default value if no element with key @a key exists.

    The function is basically equivalent to executing
    @code {.cpp}
    try {
        return at(key);
    } catch(out_of_range) {
        return default_value;
    }
    @endcode

    @note Unlike @ref at(const typename object_t::key_type&), this function
    does not throw if the given key @a key was not found.

    @note Unlike @ref operator[](const typename object_t::key_type& key), this
    function does not implicitly add an element to the position defined by @a
    key. This function is furthermore also applicable to const objects.

    @param[in] key  key of the element to access
    @param[in] default_value  the value to return if @a key is not found

    @tparam ValueType type compatible to JSON values, for instance `int` for
    JSON integer numbers, `bool` for JSON booleans, or `std::vector` types for
    JSON arrays. Note the type of the expected value at @a key and the default
    value @a default_value must be compatible.

    @return copy of the element at key @a key or @a default_value if @a key
    is not found

    @throw type_error.302 if @a default_value does not match the type of the
    value at @a key
    @throw type_error.306 if the JSON value is not an object; in that case,
    using `value()` with a key makes no sense.

    @complexity Logarithmic in the size of the container.

    @liveexample{The example below shows how object elements can be queried
    with a default value.,basic_json__value}

    @sa @ref at(const typename object_t::key_type&) for access by reference
    with range checking
    @sa @ref operator[](const typename object_t::key_type&) for unchecked
    access by reference

    @since version 1.0.0
    */
    template<class ValueType, typename std::enable_if<
                 std::is_convertible<basic_json_t, ValueType>::value, int>::type = 0>
    ValueType value(const typename object_t::key_type& key, const ValueType& default_value) const
    {
        // at only works for objects
        if (JSON_HEDLEY_LIKELY(is_object()))
        {
            // if key is found, return value and given default value otherwise
            const auto it = find(key);
            if (it != end())
            {
                return *it;
            }

            return default_value;
        }

        JSON_THROW(type_error::create(306, "cannot use value() with " + std::string(type_name())));
    }

    /*!
    @brief overload for a default value of type const char*
    @copydoc basic_json::value(const typename object_t::key_type&, const ValueType&) const
    */
    string_t value(const typename object_t::key_type& key, const char* default_value) const
    {
        return value(key, string_t(default_value));
    }

    /*!
    @brief access specified object element via JSON Pointer with default value

    Returns either a copy of an object's element at the specified key @a key
    or a given default value if no element with key @a key exists.

    The function is basically equivalent to executing
    @code {.cpp}
    try {
        return at(ptr);
    } catch(out_of_range) {
        return default_value;
    }
    @endcode

    @note Unlike @ref at(const json_pointer&), this function does not throw
    if the given key @a key was not found.

    @param[in] ptr  a JSON pointer to the element to access
    @param[in] default_value  the value to return if @a ptr found no value

    @tparam ValueType type compatible to JSON values, for instance `int` for
    JSON integer numbers, `bool` for JSON booleans, or `std::vector` types for
    JSON arrays. Note the type of the expected value at @a key and the default
    value @a default_value must be compatible.

    @return copy of the element at key @a key or @a default_value if @a key
    is not found

    @throw type_error.302 if @a default_value does not match the type of the
    value at @a ptr
    @throw type_error.306 if the JSON value is not an object; in that case,
    using `value()` with a key makes no sense.

    @complexity Logarithmic in the size of the container.

    @liveexample{The example below shows how object elements can be queried
    with a default value.,basic_json__value_ptr}

    @sa @ref operator[](const json_pointer&) for unchecked access by reference

    @since version 2.0.2
    */
    template<class ValueType, typename std::enable_if<
                 std::is_convertible<basic_json_t, ValueType>::value, int>::type = 0>
    ValueType value(const json_pointer& ptr, const ValueType& default_value) const
    {
        // at only works for objects
        if (JSON_HEDLEY_LIKELY(is_object()))
        {
            // if pointer resolves a value, return it or use default value
            JSON_TRY
            {
                return ptr.get_checked(this);
            }
            JSON_INTERNAL_CATCH (out_of_range&)
            {
                return default_value;
            }
        }

        JSON_THROW(type_error::create(306, "cannot use value() with " + std::string(type_name())));
    }

    /*!
    @brief overload for a default value of type const char*
    @copydoc basic_json::value(const json_pointer&, ValueType) const
    */
    JSON_HEDLEY_NON_NULL(3)
    string_t value(const json_pointer& ptr, const char* default_value) const
    {
        return value(ptr, string_t(default_value));
    }

    /*!
    @brief access the first element

    Returns a reference to the first element in the container. For a JSON
    container `c`, the expression `c.front()` is equivalent to `*c.begin()`.

    @return In case of a structured type (array or object), a reference to the
    first element is returned. In case of number, string, or boolean values, a
    reference to the value is returned.

    @complexity Constant.

    @pre The JSON value must not be `null` (would throw `std::out_of_range`)
    or an empty array or object (undefined behavior, **guarded by
    assertions**).
    @post The JSON value remains unchanged.

    @throw invalid_iterator.214 when called on `null` value

    @liveexample{The following code shows an example for `front()`.,front}

    @sa @ref back() -- access the last element

    @since version 1.0.0
    */
    reference front()
    {
        return *begin();
    }

    /*!
    @copydoc basic_json::front()
    */
    const_reference front() const
    {
        return *cbegin();
    }

    /*!
    @brief access the last element

    Returns a reference to the last element in the container. For a JSON
    container `c`, the expression `c.back()` is equivalent to
    @code {.cpp}
    auto tmp = c.end();
    --tmp;
    return *tmp;
    @endcode

    @return In case of a structured type (array or object), a reference to the
    last element is returned. In case of number, string, or boolean values, a
    reference to the value is returned.

    @complexity Constant.

    @pre The JSON value must not be `null` (would throw `std::out_of_range`)
    or an empty array or object (undefined behavior, **guarded by
    assertions**).
    @post The JSON value remains unchanged.

    @throw invalid_iterator.214 when called on a `null` value. See example
    below.

    @liveexample{The following code shows an example for `back()`.,back}

    @sa @ref front() -- access the first element

    @since version 1.0.0
    */
    reference back()
    {
        auto tmp = end();
        --tmp;
        return *tmp;
    }

    /*!
    @copydoc basic_json::back()
    */
    const_reference back() const
    {
        auto tmp = cend();
        --tmp;
        return *tmp;
    }

    /*!
    @brief remove element given an iterator

    Removes the element specified by iterator @a pos. The iterator @a pos must
    be valid and dereferenceable. Thus the `end()` iterator (which is valid,
    but is not dereferenceable) cannot be used as a value for @a pos.

    If called on a primitive type other than `null`, the resulting JSON value
    will be `null`.

    @param[in] pos iterator to the element to remove
    @return Iterator following the last removed element. If the iterator @a
    pos refers to the last element, the `end()` iterator is returned.

    @tparam IteratorType an @ref iterator or @ref const_iterator

    @post Invalidates iterators and references at or after the point of the
    erase, including the `end()` iterator.

    @throw type_error.307 if called on a `null` value; example: `"cannot use
    erase() with null"`
    @throw invalid_iterator.202 if called on an iterator which does not belong
    to the current JSON value; example: `"iterator does not fit current
    value"`
    @throw invalid_iterator.205 if called on a primitive type with invalid
    iterator (i.e., any iterator which is not `begin()`); example: `"iterator
    out of range"`

    @complexity The complexity depends on the type:
    - objects: amortized constant
    - arrays: linear in distance between @a pos and the end of the container
    - strings: linear in the length of the string
    - other types: constant

    @liveexample{The example shows the result of `erase()` for different JSON
    types.,erase__IteratorType}

    @sa @ref erase(IteratorType, IteratorType) -- removes the elements in
    the given range
    @sa @ref erase(const typename object_t::key_type&) -- removes the element
    from an object at the given key
    @sa @ref erase(const size_type) -- removes the element from an array at
    the given index

    @since version 1.0.0
    */
    template<class IteratorType, typename std::enable_if<
                 std::is_same<IteratorType, typename basic_json_t::iterator>::value or
                 std::is_same<IteratorType, typename basic_json_t::const_iterator>::value, int>::type
             = 0>
    IteratorType erase(IteratorType pos)
    {
        // make sure iterator fits the current value
        if (JSON_HEDLEY_UNLIKELY(this != pos.m_object))
        {
            JSON_THROW(invalid_iterator::create(202, "iterator does not fit current value"));
        }

        IteratorType result = end();

        switch (m_type)
        {
            case value_t::boolean:
            case value_t::number_float:
            case value_t::number_integer:
            case value_t::number_unsigned:
            case value_t::string:
            {
                if (JSON_HEDLEY_UNLIKELY(not pos.m_it.primitive_iterator.is_begin()))
                {
                    JSON_THROW(invalid_iterator::create(205, "iterator out of range"));
                }

                if (is_string())
                {
                    AllocatorType<string_t> alloc;
                    std::allocator_traits<decltype(alloc)>::destroy(alloc, m_value.string);
                    std::allocator_traits<decltype(alloc)>::deallocate(alloc, m_value.string, 1);
                    m_value.string = nullptr;
                }

                m_type = value_t::null;
                assert_invariant();
                break;
            }

            case value_t::object:
            {
                result.m_it.object_iterator = m_value.object->erase(pos.m_it.object_iterator);
                break;
            }

            case value_t::array:
            {
                result.m_it.array_iterator = m_value.array->erase(pos.m_it.array_iterator);
                break;
            }

            default:
                JSON_THROW(type_error::create(307, "cannot use erase() with " + std::string(type_name())));
        }

        return result;
    }

    /*!
    @brief remove elements given an iterator range

    Removes the element specified by the range `[first; last)`. The iterator
    @a first does not need to be dereferenceable if `first == last`: erasing
    an empty range is a no-op.

    If called on a primitive type other than `null`, the resulting JSON value
    will be `null`.

    @param[in] first iterator to the beginning of the range to remove
    @param[in] last iterator past the end of the range to remove
    @return Iterator following the last removed element. If the iterator @a
    second refers to the last element, the `end()` iterator is returned.

    @tparam IteratorType an @ref iterator or @ref const_iterator

    @post Invalidates iterators and references at or after the point of the
    erase, including the `end()` iterator.

    @throw type_error.307 if called on a `null` value; example: `"cannot use
    erase() with null"`
    @throw invalid_iterator.203 if called on iterators which does not belong
    to the current JSON value; example: `"iterators do not fit current value"`
    @throw invalid_iterator.204 if called on a primitive type with invalid
    iterators (i.e., if `first != begin()` and `last != end()`); example:
    `"iterators out of range"`

    @complexity The complexity depends on the type:
    - objects: `log(size()) + std::distance(first, last)`
    - arrays: linear in the distance between @a first and @a last, plus linear
      in the distance between @a last and end of the container
    - strings: linear in the length of the string
    - other types: constant

    @liveexample{The example shows the result of `erase()` for different JSON
    types.,erase__IteratorType_IteratorType}

    @sa @ref erase(IteratorType) -- removes the element at a given position
    @sa @ref erase(const typename object_t::key_type&) -- removes the element
    from an object at the given key
    @sa @ref erase(const size_type) -- removes the element from an array at
    the given index

    @since version 1.0.0
    */
    template<class IteratorType, typename std::enable_if<
                 std::is_same<IteratorType, typename basic_json_t::iterator>::value or
                 std::is_same<IteratorType, typename basic_json_t::const_iterator>::value, int>::type
             = 0>
    IteratorType erase(IteratorType first, IteratorType last)
    {
        // make sure iterator fits the current value
        if (JSON_HEDLEY_UNLIKELY(this != first.m_object or this != last.m_object))
        {
            JSON_THROW(invalid_iterator::create(203, "iterators do not fit current value"));
        }

        IteratorType result = end();

        switch (m_type)
        {
            case value_t::boolean:
            case value_t::number_float:
            case value_t::number_integer:
            case value_t::number_unsigned:
            case value_t::string:
            {
                if (JSON_HEDLEY_LIKELY(not first.m_it.primitive_iterator.is_begin()
                                       or not last.m_it.primitive_iterator.is_end()))
                {
                    JSON_THROW(invalid_iterator::create(204, "iterators out of range"));
                }

                if (is_string())
                {
                    AllocatorType<string_t> alloc;
                    std::allocator_traits<decltype(alloc)>::destroy(alloc, m_value.string);
                    std::allocator_traits<decltype(alloc)>::deallocate(alloc, m_value.string, 1);
                    m_value.string = nullptr;
                }

                m_type = value_t::null;
                assert_invariant();
                break;
            }

            case value_t::object:
            {
                result.m_it.object_iterator = m_value.object->erase(first.m_it.object_iterator,
                                              last.m_it.object_iterator);
                break;
            }

            case value_t::array:
            {
                result.m_it.array_iterator = m_value.array->erase(first.m_it.array_iterator,
                                             last.m_it.array_iterator);
                break;
            }

            default:
                JSON_THROW(type_error::create(307, "cannot use erase() with " + std::string(type_name())));
        }

        return result;
    }

    /*!
    @brief remove element from a JSON object given a key

    Removes elements from a JSON object with the key value @a key.

    @param[in] key value of the elements to remove

    @return Number of elements removed. If @a ObjectType is the default
    `std::map` type, the return value will always be `0` (@a key was not
    found) or `1` (@a key was found).

    @post References and iterators to the erased elements are invalidated.
    Other references and iterators are not affected.

    @throw type_error.307 when called on a type other than JSON object;
    example: `"cannot use erase() with null"`

    @complexity `log(size()) + count(key)`

    @liveexample{The example shows the effect of `erase()`.,erase__key_type}

    @sa @ref erase(IteratorType) -- removes the element at a given position
    @sa @ref erase(IteratorType, IteratorType) -- removes the elements in
    the given range
    @sa @ref erase(const size_type) -- removes the element from an array at
    the given index

    @since version 1.0.0
    */
    size_type erase(const typename object_t::key_type& key)
    {
        // this erase only works for objects
        if (JSON_HEDLEY_LIKELY(is_object()))
        {
            return m_value.object->erase(key);
        }

        JSON_THROW(type_error::create(307, "cannot use erase() with " + std::string(type_name())));
    }

    /*!
    @brief remove element from a JSON array given an index

    Removes element from a JSON array at the index @a idx.

    @param[in] idx index of the element to remove

    @throw type_error.307 when called on a type other than JSON object;
    example: `"cannot use erase() with null"`
    @throw out_of_range.401 when `idx >= size()`; example: `"array index 17
    is out of range"`

    @complexity Linear in distance between @a idx and the end of the container.

    @liveexample{The example shows the effect of `erase()`.,erase__size_type}

    @sa @ref erase(IteratorType) -- removes the element at a given position
    @sa @ref erase(IteratorType, IteratorType) -- removes the elements in
    the given range
    @sa @ref erase(const typename object_t::key_type&) -- removes the element
    from an object at the given key

    @since version 1.0.0
    */
    void erase(const size_type idx)
    {
        // this erase only works for arrays
        if (JSON_HEDLEY_LIKELY(is_array()))
        {
            if (JSON_HEDLEY_UNLIKELY(idx >= size()))
            {
                JSON_THROW(out_of_range::create(401, "array index " + std::to_string(idx) + " is out of range"));
            }

            m_value.array->erase(m_value.array->begin() + static_cast<difference_type>(idx));
        }
        else
        {
            JSON_THROW(type_error::create(307, "cannot use erase() with " + std::string(type_name())));
        }
    }

    /// @}


    ////////////
    // lookup //
    ////////////

    /// @name lookup
    /// @{

    /*!
    @brief find an element in a JSON object

    Finds an element in a JSON object with key equivalent to @a key. If the
    element is not found or the JSON value is not an object, end() is
    returned.

    @note This method always returns @ref end() when executed on a JSON type
          that is not an object.

    @param[in] key key value of the element to search for.

    @return Iterator to an element with key equivalent to @a key. If no such
    element is found or the JSON value is not an object, past-the-end (see
    @ref end()) iterator is returned.

    @complexity Logarithmic in the size of the JSON object.

    @liveexample{The example shows how `find()` is used.,find__key_type}

    @sa @ref contains(KeyT&&) const -- checks whether a key exists

    @since version 1.0.0
    */
    template<typename KeyT>
    iterator find(KeyT&& key)
    {
        auto result = end();

        if (is_object())
        {
            result.m_it.object_iterator = m_value.object->find(std::forward<KeyT>(key));
        }

        return result;
    }

    /*!
    @brief find an element in a JSON object
    @copydoc find(KeyT&&)
    */
    template<typename KeyT>
    const_iterator find(KeyT&& key) const
    {
        auto result = cend();

        if (is_object())
        {
            result.m_it.object_iterator = m_value.object->find(std::forward<KeyT>(key));
        }

        return result;
    }

    /*!
    @brief returns the number of occurrences of a key in a JSON object

    Returns the number of elements with key @a key. If ObjectType is the
    default `std::map` type, the return value will always be `0` (@a key was
    not found) or `1` (@a key was found).

    @note This method always returns `0` when executed on a JSON type that is
          not an object.

    @param[in] key key value of the element to count

    @return Number of elements with key @a key. If the JSON value is not an
    object, the return value will be `0`.

    @complexity Logarithmic in the size of the JSON object.

    @liveexample{The example shows how `count()` is used.,count}

    @since version 1.0.0
    */
    template<typename KeyT>
    size_type count(KeyT&& key) const
    {
        // return 0 for all nonobject types
        return is_object() ? m_value.object->count(std::forward<KeyT>(key)) : 0;
    }

    /*!
    @brief check the existence of an element in a JSON object

    Check whether an element exists in a JSON object with key equivalent to
    @a key. If the element is not found or the JSON value is not an object,
    false is returned.

    @note This method always returns false when executed on a JSON type
          that is not an object.

    @param[in] key key value to check its existence.

    @return true if an element with specified @a key exists. If no such
    element with such key is found or the JSON value is not an object,
    false is returned.

    @complexity Logarithmic in the size of the JSON object.

    @liveexample{The following code shows an example for `contains()`.,contains}

    @sa @ref find(KeyT&&) -- returns an iterator to an object element
    @sa @ref contains(const json_pointer&) const -- checks the existence for a JSON pointer

    @since version 3.6.0
    */
    template<typename KeyT, typename std::enable_if<
                 not std::is_same<typename std::decay<KeyT>::type, json_pointer>::value, int>::type = 0>
    bool contains(KeyT && key) const
    {
        return is_object() and m_value.object->find(std::forward<KeyT>(key)) != m_value.object->end();
    }

    /*!
    @brief check the existence of an element in a JSON object given a JSON pointer

    Check whether the given JSON pointer @a ptr can be resolved in the current
    JSON value.

    @note This method can be executed on any JSON value type.

    @param[in] ptr JSON pointer to check its existence.

    @return true if the JSON pointer can be resolved to a stored value, false
    otherwise.

    @post If `j.contains(ptr)` returns true, it is safe to call `j[ptr]`.

    @throw parse_error.106   if an array index begins with '0'
    @throw parse_error.109   if an array index was not a number

    @complexity Logarithmic in the size of the JSON object.

    @liveexample{The following code shows an example for `contains()`.,contains_json_pointer}

    @sa @ref contains(KeyT &&) const -- checks the existence of a key

    @since version 3.7.0
    */
    bool contains(const json_pointer& ptr) const
    {
        return ptr.contains(this);
    }

    /// @}


    ///////////////
    // iterators //
    ///////////////

    /// @name iterators
    /// @{

    /*!
    @brief returns an iterator to the first element

    Returns an iterator to the first element.

    @image html range-begin-end.svg "Illustration from cppreference.com"

    @return iterator to the first element

    @complexity Constant.

    @requirement This function helps `basic_json` satisfying the
    [Container](https://en.cppreference.com/w/cpp/named_req/Container)
    requirements:
    - The complexity is constant.

    @liveexample{The following code shows an example for `begin()`.,begin}

    @sa @ref cbegin() -- returns a const iterator to the beginning
    @sa @ref end() -- returns an iterator to the end
    @sa @ref cend() -- returns a const iterator to the end

    @since version 1.0.0
    */
    iterator begin() noexcept
    {
        iterator result(this);
        result.set_begin();
        return result;
    }

    /*!
    @copydoc basic_json::cbegin()
    */
    const_iterator begin() const noexcept
    {
        return cbegin();
    }

    /*!
    @brief returns a const iterator to the first element

    Returns a const iterator to the first element.

    @image html range-begin-end.svg "Illustration from cppreference.com"

    @return const iterator to the first element

    @complexity Constant.

    @requirement This function helps `basic_json` satisfying the
    [Container](https://en.cppreference.com/w/cpp/named_req/Container)
    requirements:
    - The complexity is constant.
    - Has the semantics of `const_cast<const basic_json&>(*this).begin()`.

    @liveexample{The following code shows an example for `cbegin()`.,cbegin}

    @sa @ref begin() -- returns an iterator to the beginning
    @sa @ref end() -- returns an iterator to the end
    @sa @ref cend() -- returns a const iterator to the end

    @since version 1.0.0
    */
    const_iterator cbegin() const noexcept
    {
        const_iterator result(this);
        result.set_begin();
        return result;
    }

    /*!
    @brief returns an iterator to one past the last element

    Returns an iterator to one past the last element.

    @image html range-begin-end.svg "Illustration from cppreference.com"

    @return iterator one past the last element

    @complexity Constant.

    @requirement This function helps `basic_json` satisfying the
    [Container](https://en.cppreference.com/w/cpp/named_req/Container)
    requirements:
    - The complexity is constant.

    @liveexample{The following code shows an example for `end()`.,end}

    @sa @ref cend() -- returns a const iterator to the end
    @sa @ref begin() -- returns an iterator to the beginning
    @sa @ref cbegin() -- returns a const iterator to the beginning

    @since version 1.0.0
    */
    iterator end() noexcept
    {
        iterator result(this);
        result.set_end();
        return result;
    }

    /*!
    @copydoc basic_json::cend()
    */
    const_iterator end() const noexcept
    {
        return cend();
    }

    /*!
    @brief returns a const iterator to one past the last element

    Returns a const iterator to one past the last element.

    @image html range-begin-end.svg "Illustration from cppreference.com"

    @return const iterator one past the last element

    @complexity Constant.

    @requirement This function helps `basic_json` satisfying the
    [Container](https://en.cppreference.com/w/cpp/named_req/Container)
    requirements:
    - The complexity is constant.
    - Has the semantics of `const_cast<const basic_json&>(*this).end()`.

    @liveexample{The following code shows an example for `cend()`.,cend}

    @sa @ref end() -- returns an iterator to the end
    @sa @ref begin() -- returns an iterator to the beginning
    @sa @ref cbegin() -- returns a const iterator to the beginning

    @since version 1.0.0
    */
    const_iterator cend() const noexcept
    {
        const_iterator result(this);
        result.set_end();
        return result;
    }

    /*!
    @brief returns an iterator to the reverse-beginning

    Returns an iterator to the reverse-beginning; that is, the last element.

    @image html range-rbegin-rend.svg "Illustration from cppreference.com"

    @complexity Constant.

    @requirement This function helps `basic_json` satisfying the
    [ReversibleContainer](https://en.cppreference.com/w/cpp/named_req/ReversibleContainer)
    requirements:
    - The complexity is constant.
    - Has the semantics of `reverse_iterator(end())`.

    @liveexample{The following code shows an example for `rbegin()`.,rbegin}

    @sa @ref crbegin() -- returns a const reverse iterator to the beginning
    @sa @ref rend() -- returns a reverse iterator to the end
    @sa @ref crend() -- returns a const reverse iterator to the end

    @since version 1.0.0
    */
    reverse_iterator rbegin() noexcept
    {
        return reverse_iterator(end());
    }

    /*!
    @copydoc basic_json::crbegin()
    */
    const_reverse_iterator rbegin() const noexcept
    {
        return crbegin();
    }

    /*!
    @brief returns an iterator to the reverse-end

    Returns an iterator to the reverse-end; that is, one before the first
    element.

    @image html range-rbegin-rend.svg "Illustration from cppreference.com"

    @complexity Constant.

    @requirement This function helps `basic_json` satisfying the
    [ReversibleContainer](https://en.cppreference.com/w/cpp/named_req/ReversibleContainer)
    requirements:
    - The complexity is constant.
    - Has the semantics of `reverse_iterator(begin())`.

    @liveexample{The following code shows an example for `rend()`.,rend}

    @sa @ref crend() -- returns a const reverse iterator to the end
    @sa @ref rbegin() -- returns a reverse iterator to the beginning
    @sa @ref crbegin() -- returns a const reverse iterator to the beginning

    @since version 1.0.0
    */
    reverse_iterator rend() noexcept
    {
        return reverse_iterator(begin());
    }

    /*!
    @copydoc basic_json::crend()
    */
    const_reverse_iterator rend() const noexcept
    {
        return crend();
    }

    /*!
    @brief returns a const reverse iterator to the last element

    Returns a const iterator to the reverse-beginning; that is, the last
    element.

    @image html range-rbegin-rend.svg "Illustration from cppreference.com"

    @complexity Constant.

    @requirement This function helps `basic_json` satisfying the
    [ReversibleContainer](https://en.cppreference.com/w/cpp/named_req/ReversibleContainer)
    requirements:
    - The complexity is constant.
    - Has the semantics of `const_cast<const basic_json&>(*this).rbegin()`.

    @liveexample{The following code shows an example for `crbegin()`.,crbegin}

    @sa @ref rbegin() -- returns a reverse iterator to the beginning
    @sa @ref rend() -- returns a reverse iterator to the end
    @sa @ref crend() -- returns a const reverse iterator to the end

    @since version 1.0.0
    */
    const_reverse_iterator crbegin() const noexcept
    {
        return const_reverse_iterator(cend());
    }

    /*!
    @brief returns a const reverse iterator to one before the first

    Returns a const reverse iterator to the reverse-end; that is, one before
    the first element.

    @image html range-rbegin-rend.svg "Illustration from cppreference.com"

    @complexity Constant.

    @requirement This function helps `basic_json` satisfying the
    [ReversibleContainer](https://en.cppreference.com/w/cpp/named_req/ReversibleContainer)
    requirements:
    - The complexity is constant.
    - Has the semantics of `const_cast<const basic_json&>(*this).rend()`.

    @liveexample{The following code shows an example for `crend()`.,crend}

    @sa @ref rend() -- returns a reverse iterator to the end
    @sa @ref rbegin() -- returns a reverse iterator to the beginning
    @sa @ref crbegin() -- returns a const reverse iterator to the beginning

    @since version 1.0.0
    */
    const_reverse_iterator crend() const noexcept
    {
        return const_reverse_iterator(cbegin());
    }

  public:
    /*!
    @brief wrapper to access iterator member functions in range-based for

    This function allows to access @ref iterator::key() and @ref
    iterator::value() during range-based for loops. In these loops, a
    reference to the JSON values is returned, so there is no access to the
    underlying iterator.

    For loop without iterator_wrapper:

    @code{cpp}
    for (auto it = j_object.begin(); it != j_object.end(); ++it)
    {
        std::cout << "key: " << it.key() << ", value:" << it.value() << '\n';
    }
    @endcode

    Range-based for loop without iterator proxy:

    @code{cpp}
    for (auto it : j_object)
    {
        // "it" is of type json::reference and has no key() member
        std::cout << "value: " << it << '\n';
    }
    @endcode

    Range-based for loop with iterator proxy:

    @code{cpp}
    for (auto it : json::iterator_wrapper(j_object))
    {
        std::cout << "key: " << it.key() << ", value:" << it.value() << '\n';
    }
    @endcode

    @note When iterating over an array, `key()` will return the index of the
          element as string (see example).

    @param[in] ref  reference to a JSON value
    @return iteration proxy object wrapping @a ref with an interface to use in
            range-based for loops

    @liveexample{The following code shows how the wrapper is used,iterator_wrapper}

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes in the JSON value.

    @complexity Constant.

    @note The name of this function is not yet final and may change in the
    future.

    @deprecated This stream operator is deprecated and will be removed in
                future 4.0.0 of the library. Please use @ref items() instead;
                that is, replace `json::iterator_wrapper(j)` with `j.items()`.
    */
    JSON_HEDLEY_DEPRECATED(3.1.0)
    static iteration_proxy<iterator> iterator_wrapper(reference ref) noexcept
    {
        return ref.items();
    }

    /*!
    @copydoc iterator_wrapper(reference)
    */
    JSON_HEDLEY_DEPRECATED(3.1.0)
    static iteration_proxy<const_iterator> iterator_wrapper(const_reference ref) noexcept
    {
        return ref.items();
    }

    /*!
    @brief helper to access iterator member functions in range-based for

    This function allows to access @ref iterator::key() and @ref
    iterator::value() during range-based for loops. In these loops, a
    reference to the JSON values is returned, so there is no access to the
    underlying iterator.

    For loop without `items()` function:

    @code{cpp}
    for (auto it = j_object.begin(); it != j_object.end(); ++it)
    {
        std::cout << "key: " << it.key() << ", value:" << it.value() << '\n';
    }
    @endcode

    Range-based for loop without `items()` function:

    @code{cpp}
    for (auto it : j_object)
    {
        // "it" is of type json::reference and has no key() member
        std::cout << "value: " << it << '\n';
    }
    @endcode

    Range-based for loop with `items()` function:

    @code{cpp}
    for (auto& el : j_object.items())
    {
        std::cout << "key: " << el.key() << ", value:" << el.value() << '\n';
    }
    @endcode

    The `items()` function also allows to use
    [structured bindings](https://en.cppreference.com/w/cpp/language/structured_binding)
    (C++17):

    @code{cpp}
    for (auto& [key, val] : j_object.items())
    {
        std::cout << "key: " << key << ", value:" << val << '\n';
    }
    @endcode

    @note When iterating over an array, `key()` will return the index of the
          element as string (see example). For primitive types (e.g., numbers),
          `key()` returns an empty string.

    @return iteration proxy object wrapping @a ref with an interface to use in
            range-based for loops

    @liveexample{The following code shows how the function is used.,items}

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes in the JSON value.

    @complexity Constant.

    @since version 3.1.0, structured bindings support since 3.5.0.
    */
    iteration_proxy<iterator> items() noexcept
    {
        return iteration_proxy<iterator>(*this);
    }

    /*!
    @copydoc items()
    */
    iteration_proxy<const_iterator> items() const noexcept
    {
        return iteration_proxy<const_iterator>(*this);
    }

    /// @}


    //////////////
    // capacity //
    //////////////

    /// @name capacity
    /// @{

    /*!
    @brief checks whether the container is empty.

    Checks if a JSON value has no elements (i.e. whether its @ref size is `0`).

    @return The return value depends on the different types and is
            defined as follows:
            Value type  | return value
            ----------- | -------------
            null        | `true`
            boolean     | `false`
            string      | `false`
            number      | `false`
            object      | result of function `object_t::empty()`
            array       | result of function `array_t::empty()`

    @liveexample{The following code uses `empty()` to check if a JSON
    object contains any elements.,empty}

    @complexity Constant, as long as @ref array_t and @ref object_t satisfy
    the Container concept; that is, their `empty()` functions have constant
    complexity.

    @iterators No changes.

    @exceptionsafety No-throw guarantee: this function never throws exceptions.

    @note This function does not return whether a string stored as JSON value
    is empty - it returns whether the JSON container itself is empty which is
    false in the case of a string.

    @requirement This function helps `basic_json` satisfying the
    [Container](https://en.cppreference.com/w/cpp/named_req/Container)
    requirements:
    - The complexity is constant.
    - Has the semantics of `begin() == end()`.

    @sa @ref size() -- returns the number of elements

    @since version 1.0.0
    */
    bool empty() const noexcept
    {
        switch (m_type)
        {
            case value_t::null:
            {
                // null values are empty
                return true;
            }

            case value_t::array:
            {
                // delegate call to array_t::empty()
                return m_value.array->empty();
            }

            case value_t::object:
            {
                // delegate call to object_t::empty()
                return m_value.object->empty();
            }

            default:
            {
                // all other types are nonempty
                return false;
            }
        }
    }

    /*!
    @brief returns the number of elements

    Returns the number of elements in a JSON value.

    @return The return value depends on the different types and is
            defined as follows:
            Value type  | return value
            ----------- | -------------
            null        | `0`
            boolean     | `1`
            string      | `1`
            number      | `1`
            object      | result of function object_t::size()
            array       | result of function array_t::size()

    @liveexample{The following code calls `size()` on the different value
    types.,size}

    @complexity Constant, as long as @ref array_t and @ref object_t satisfy
    the Container concept; that is, their size() functions have constant
    complexity.

    @iterators No changes.

    @exceptionsafety No-throw guarantee: this function never throws exceptions.

    @note This function does not return the length of a string stored as JSON
    value - it returns the number of elements in the JSON value which is 1 in
    the case of a string.

    @requirement This function helps `basic_json` satisfying the
    [Container](https://en.cppreference.com/w/cpp/named_req/Container)
    requirements:
    - The complexity is constant.
    - Has the semantics of `std::distance(begin(), end())`.

    @sa @ref empty() -- checks whether the container is empty
    @sa @ref max_size() -- returns the maximal number of elements

    @since version 1.0.0
    */
    size_type size() const noexcept
    {
        switch (m_type)
        {
            case value_t::null:
            {
                // null values are empty
                return 0;
            }

            case value_t::array:
            {
                // delegate call to array_t::size()
                return m_value.array->size();
            }

            case value_t::object:
            {
                // delegate call to object_t::size()
                return m_value.object->size();
            }

            default:
            {
                // all other types have size 1
                return 1;
            }
        }
    }

    /*!
    @brief returns the maximum possible number of elements

    Returns the maximum number of elements a JSON value is able to hold due to
    system or library implementation limitations, i.e. `std::distance(begin(),
    end())` for the JSON value.

    @return The return value depends on the different types and is
            defined as follows:
            Value type  | return value
            ----------- | -------------
            null        | `0` (same as `size()`)
            boolean     | `1` (same as `size()`)
            string      | `1` (same as `size()`)
            number      | `1` (same as `size()`)
            object      | result of function `object_t::max_size()`
            array       | result of function `array_t::max_size()`

    @liveexample{The following code calls `max_size()` on the different value
    types. Note the output is implementation specific.,max_size}

    @complexity Constant, as long as @ref array_t and @ref object_t satisfy
    the Container concept; that is, their `max_size()` functions have constant
    complexity.

    @iterators No changes.

    @exceptionsafety No-throw guarantee: this function never throws exceptions.

    @requirement This function helps `basic_json` satisfying the
    [Container](https://en.cppreference.com/w/cpp/named_req/Container)
    requirements:
    - The complexity is constant.
    - Has the semantics of returning `b.size()` where `b` is the largest
      possible JSON value.

    @sa @ref size() -- returns the number of elements

    @since version 1.0.0
    */
    size_type max_size() const noexcept
    {
        switch (m_type)
        {
            case value_t::array:
            {
                // delegate call to array_t::max_size()
                return m_value.array->max_size();
            }

            case value_t::object:
            {
                // delegate call to object_t::max_size()
                return m_value.object->max_size();
            }

            default:
            {
                // all other types have max_size() == size()
                return size();
            }
        }
    }

    /// @}


    ///////////////
    // modifiers //
    ///////////////

    /// @name modifiers
    /// @{

    /*!
    @brief clears the contents

    Clears the content of a JSON value and resets it to the default value as
    if @ref basic_json(value_t) would have been called with the current value
    type from @ref type():

    Value type  | initial value
    ----------- | -------------
    null        | `null`
    boolean     | `false`
    string      | `""`
    number      | `0`
    object      | `{}`
    array       | `[]`

    @post Has the same effect as calling
    @code {.cpp}
    *this = basic_json(type());
    @endcode

    @liveexample{The example below shows the effect of `clear()` to different
    JSON types.,clear}

    @complexity Linear in the size of the JSON value.

    @iterators All iterators, pointers and references related to this container
               are invalidated.

    @exceptionsafety No-throw guarantee: this function never throws exceptions.

    @sa @ref basic_json(value_t) -- constructor that creates an object with the
        same value than calling `clear()`

    @since version 1.0.0
    */
    void clear() noexcept
    {
        switch (m_type)
        {
            case value_t::number_integer:
            {
                m_value.number_integer = 0;
                break;
            }

            case value_t::number_unsigned:
            {
                m_value.number_unsigned = 0;
                break;
            }

            case value_t::number_float:
            {
                m_value.number_float = 0.0;
                break;
            }

            case value_t::boolean:
            {
                m_value.boolean = false;
                break;
            }

            case value_t::string:
            {
                m_value.string->clear();
                break;
            }

            case value_t::array:
            {
                m_value.array->clear();
                break;
            }

            case value_t::object:
            {
                m_value.object->clear();
                break;
            }

            default:
                break;
        }
    }

    /*!
    @brief add an object to an array

    Appends the given element @a val to the end of the JSON value. If the
    function is called on a JSON null value, an empty array is created before
    appending @a val.

    @param[in] val the value to add to the JSON array

    @throw type_error.308 when called on a type other than JSON array or
    null; example: `"cannot use push_back() with number"`

    @complexity Amortized constant.

    @liveexample{The example shows how `push_back()` and `+=` can be used to
    add elements to a JSON array. Note how the `null` value was silently
    converted to a JSON array.,push_back}

    @since version 1.0.0
    */
    void push_back(basic_json&& val)
    {
        // push_back only works for null objects or arrays
        if (JSON_HEDLEY_UNLIKELY(not(is_null() or is_array())))
        {
            JSON_THROW(type_error::create(308, "cannot use push_back() with " + std::string(type_name())));
        }

        // transform null object into an array
        if (is_null())
        {
            m_type = value_t::array;
            m_value = value_t::array;
            assert_invariant();
        }

        // add element to array (move semantics)
        m_value.array->push_back(std::move(val));
        // invalidate object: mark it null so we do not call the destructor
        // cppcheck-suppress accessMoved
        val.m_type = value_t::null;
    }

    /*!
    @brief add an object to an array
    @copydoc push_back(basic_json&&)
    */
    reference operator+=(basic_json&& val)
    {
        push_back(std::move(val));
        return *this;
    }

    /*!
    @brief add an object to an array
    @copydoc push_back(basic_json&&)
    */
    void push_back(const basic_json& val)
    {
        // push_back only works for null objects or arrays
        if (JSON_HEDLEY_UNLIKELY(not(is_null() or is_array())))
        {
            JSON_THROW(type_error::create(308, "cannot use push_back() with " + std::string(type_name())));
        }

        // transform null object into an array
        if (is_null())
        {
            m_type = value_t::array;
            m_value = value_t::array;
            assert_invariant();
        }

        // add element to array
        m_value.array->push_back(val);
    }

    /*!
    @brief add an object to an array
    @copydoc push_back(basic_json&&)
    */
    reference operator+=(const basic_json& val)
    {
        push_back(val);
        return *this;
    }

    /*!
    @brief add an object to an object

    Inserts the given element @a val to the JSON object. If the function is
    called on a JSON null value, an empty object is created before inserting
    @a val.

    @param[in] val the value to add to the JSON object

    @throw type_error.308 when called on a type other than JSON object or
    null; example: `"cannot use push_back() with number"`

    @complexity Logarithmic in the size of the container, O(log(`size()`)).

    @liveexample{The example shows how `push_back()` and `+=` can be used to
    add elements to a JSON object. Note how the `null` value was silently
    converted to a JSON object.,push_back__object_t__value}

    @since version 1.0.0
    */
    void push_back(const typename object_t::value_type& val)
    {
        // push_back only works for null objects or objects
        if (JSON_HEDLEY_UNLIKELY(not(is_null() or is_object())))
        {
            JSON_THROW(type_error::create(308, "cannot use push_back() with " + std::string(type_name())));
        }

        // transform null object into an object
        if (is_null())
        {
            m_type = value_t::object;
            m_value = value_t::object;
            assert_invariant();
        }

        // add element to array
        m_value.object->insert(val);
    }

    /*!
    @brief add an object to an object
    @copydoc push_back(const typename object_t::value_type&)
    */
    reference operator+=(const typename object_t::value_type& val)
    {
        push_back(val);
        return *this;
    }

    /*!
    @brief add an object to an object

    This function allows to use `push_back` with an initializer list. In case

    1. the current value is an object,
    2. the initializer list @a init contains only two elements, and
    3. the first element of @a init is a string,

    @a init is converted into an object element and added using
    @ref push_back(const typename object_t::value_type&). Otherwise, @a init
    is converted to a JSON value and added using @ref push_back(basic_json&&).

    @param[in] init  an initializer list

    @complexity Linear in the size of the initializer list @a init.

    @note This function is required to resolve an ambiguous overload error,
          because pairs like `{"key", "value"}` can be both interpreted as
          `object_t::value_type` or `std::initializer_list<basic_json>`, see
          https://github.com/nlohmann/json/issues/235 for more information.

    @liveexample{The example shows how initializer lists are treated as
    objects when possible.,push_back__initializer_list}
    */
    void push_back(initializer_list_t init)
    {
        if (is_object() and init.size() == 2 and (*init.begin())->is_string())
        {
            basic_json&& key = init.begin()->moved_or_copied();
            push_back(typename object_t::value_type(
                          std::move(key.get_ref<string_t&>()), (init.begin() + 1)->moved_or_copied()));
        }
        else
        {
            push_back(basic_json(init));
        }
    }

    /*!
    @brief add an object to an object
    @copydoc push_back(initializer_list_t)
    */
    reference operator+=(initializer_list_t init)
    {
        push_back(init);
        return *this;
    }

    /*!
    @brief add an object to an array

    Creates a JSON value from the passed parameters @a args to the end of the
    JSON value. If the function is called on a JSON null value, an empty array
    is created before appending the value created from @a args.

    @param[in] args arguments to forward to a constructor of @ref basic_json
    @tparam Args compatible types to create a @ref basic_json object

    @return reference to the inserted element

    @throw type_error.311 when called on a type other than JSON array or
    null; example: `"cannot use emplace_back() with number"`

    @complexity Amortized constant.

    @liveexample{The example shows how `push_back()` can be used to add
    elements to a JSON array. Note how the `null` value was silently converted
    to a JSON array.,emplace_back}

    @since version 2.0.8, returns reference since 3.7.0
    */
    template<class... Args>
    reference emplace_back(Args&& ... args)
    {
        // emplace_back only works for null objects or arrays
        if (JSON_HEDLEY_UNLIKELY(not(is_null() or is_array())))
        {
            JSON_THROW(type_error::create(311, "cannot use emplace_back() with " + std::string(type_name())));
        }

        // transform null object into an array
        if (is_null())
        {
            m_type = value_t::array;
            m_value = value_t::array;
            assert_invariant();
        }

        // add element to array (perfect forwarding)
#ifdef JSON_HAS_CPP_17
        return m_value.array->emplace_back(std::forward<Args>(args)...);
#else
        m_value.array->emplace_back(std::forward<Args>(args)...);
        return m_value.array->back();
#endif
    }

    /*!
    @brief add an object to an object if key does not exist

    Inserts a new element into a JSON object constructed in-place with the
    given @a args if there is no element with the key in the container. If the
    function is called on a JSON null value, an empty object is created before
    appending the value created from @a args.

    @param[in] args arguments to forward to a constructor of @ref basic_json
    @tparam Args compatible types to create a @ref basic_json object

    @return a pair consisting of an iterator to the inserted element, or the
            already-existing element if no insertion happened, and a bool
            denoting whether the insertion took place.

    @throw type_error.311 when called on a type other than JSON object or
    null; example: `"cannot use emplace() with number"`

    @complexity Logarithmic in the size of the container, O(log(`size()`)).

    @liveexample{The example shows how `emplace()` can be used to add elements
    to a JSON object. Note how the `null` value was silently converted to a
    JSON object. Further note how no value is added if there was already one
    value stored with the same key.,emplace}

    @since version 2.0.8
    */
    template<class... Args>
    std::pair<iterator, bool> emplace(Args&& ... args)
    {
        // emplace only works for null objects or arrays
        if (JSON_HEDLEY_UNLIKELY(not(is_null() or is_object())))
        {
            JSON_THROW(type_error::create(311, "cannot use emplace() with " + std::string(type_name())));
        }

        // transform null object into an object
        if (is_null())
        {
            m_type = value_t::object;
            m_value = value_t::object;
            assert_invariant();
        }

        // add element to array (perfect forwarding)
        auto res = m_value.object->emplace(std::forward<Args>(args)...);
        // create result iterator and set iterator to the result of emplace
        auto it = begin();
        it.m_it.object_iterator = res.first;

        // return pair of iterator and boolean
        return {it, res.second};
    }

    /// Helper for insertion of an iterator
    /// @note: This uses std::distance to support GCC 4.8,
    ///        see https://github.com/nlohmann/json/pull/1257
    template<typename... Args>
    iterator insert_iterator(const_iterator pos, Args&& ... args)
    {
        iterator result(this);
        assert(m_value.array != nullptr);

        auto insert_pos = std::distance(m_value.array->begin(), pos.m_it.array_iterator);
        m_value.array->insert(pos.m_it.array_iterator, std::forward<Args>(args)...);
        result.m_it.array_iterator = m_value.array->begin() + insert_pos;

        // This could have been written as:
        // result.m_it.array_iterator = m_value.array->insert(pos.m_it.array_iterator, cnt, val);
        // but the return value of insert is missing in GCC 4.8, so it is written this way instead.

        return result;
    }

    /*!
    @brief inserts element

    Inserts element @a val before iterator @a pos.

    @param[in] pos iterator before which the content will be inserted; may be
    the end() iterator
    @param[in] val element to insert
    @return iterator pointing to the inserted @a val.

    @throw type_error.309 if called on JSON values other than arrays;
    example: `"cannot use insert() with string"`
    @throw invalid_iterator.202 if @a pos is not an iterator of *this;
    example: `"iterator does not fit current value"`

    @complexity Constant plus linear in the distance between @a pos and end of
    the container.

    @liveexample{The example shows how `insert()` is used.,insert}

    @since version 1.0.0
    */
    iterator insert(const_iterator pos, const basic_json& val)
    {
        // insert only works for arrays
        if (JSON_HEDLEY_LIKELY(is_array()))
        {
            // check if iterator pos fits to this JSON value
            if (JSON_HEDLEY_UNLIKELY(pos.m_object != this))
            {
                JSON_THROW(invalid_iterator::create(202, "iterator does not fit current value"));
            }

            // insert to array and return iterator
            return insert_iterator(pos, val);
        }

        JSON_THROW(type_error::create(309, "cannot use insert() with " + std::string(type_name())));
    }

    /*!
    @brief inserts element
    @copydoc insert(const_iterator, const basic_json&)
    */
    iterator insert(const_iterator pos, basic_json&& val)
    {
        return insert(pos, val);
    }

    /*!
    @brief inserts elements

    Inserts @a cnt copies of @a val before iterator @a pos.

    @param[in] pos iterator before which the content will be inserted; may be
    the end() iterator
    @param[in] cnt number of copies of @a val to insert
    @param[in] val element to insert
    @return iterator pointing to the first element inserted, or @a pos if
    `cnt==0`

    @throw type_error.309 if called on JSON values other than arrays; example:
    `"cannot use insert() with string"`
    @throw invalid_iterator.202 if @a pos is not an iterator of *this;
    example: `"iterator does not fit current value"`

    @complexity Linear in @a cnt plus linear in the distance between @a pos
    and end of the container.

    @liveexample{The example shows how `insert()` is used.,insert__count}

    @since version 1.0.0
    */
    iterator insert(const_iterator pos, size_type cnt, const basic_json& val)
    {
        // insert only works for arrays
        if (JSON_HEDLEY_LIKELY(is_array()))
        {
            // check if iterator pos fits to this JSON value
            if (JSON_HEDLEY_UNLIKELY(pos.m_object != this))
            {
                JSON_THROW(invalid_iterator::create(202, "iterator does not fit current value"));
            }

            // insert to array and return iterator
            return insert_iterator(pos, cnt, val);
        }

        JSON_THROW(type_error::create(309, "cannot use insert() with " + std::string(type_name())));
    }

    /*!
    @brief inserts elements

    Inserts elements from range `[first, last)` before iterator @a pos.

    @param[in] pos iterator before which the content will be inserted; may be
    the end() iterator
    @param[in] first begin of the range of elements to insert
    @param[in] last end of the range of elements to insert

    @throw type_error.309 if called on JSON values other than arrays; example:
    `"cannot use insert() with string"`
    @throw invalid_iterator.202 if @a pos is not an iterator of *this;
    example: `"iterator does not fit current value"`
    @throw invalid_iterator.210 if @a first and @a last do not belong to the
    same JSON value; example: `"iterators do not fit"`
    @throw invalid_iterator.211 if @a first or @a last are iterators into
    container for which insert is called; example: `"passed iterators may not
    belong to container"`

    @return iterator pointing to the first element inserted, or @a pos if
    `first==last`

    @complexity Linear in `std::distance(first, last)` plus linear in the
    distance between @a pos and end of the container.

    @liveexample{The example shows how `insert()` is used.,insert__range}

    @since version 1.0.0
    */
    iterator insert(const_iterator pos, const_iterator first, const_iterator last)
    {
        // insert only works for arrays
        if (JSON_HEDLEY_UNLIKELY(not is_array()))
        {
            JSON_THROW(type_error::create(309, "cannot use insert() with " + std::string(type_name())));
        }

        // check if iterator pos fits to this JSON value
        if (JSON_HEDLEY_UNLIKELY(pos.m_object != this))
        {
            JSON_THROW(invalid_iterator::create(202, "iterator does not fit current value"));
        }

        // check if range iterators belong to the same JSON object
        if (JSON_HEDLEY_UNLIKELY(first.m_object != last.m_object))
        {
            JSON_THROW(invalid_iterator::create(210, "iterators do not fit"));
        }

        if (JSON_HEDLEY_UNLIKELY(first.m_object == this))
        {
            JSON_THROW(invalid_iterator::create(211, "passed iterators may not belong to container"));
        }

        // insert to array and return iterator
        return insert_iterator(pos, first.m_it.array_iterator, last.m_it.array_iterator);
    }

    /*!
    @brief inserts elements

    Inserts elements from initializer list @a ilist before iterator @a pos.

    @param[in] pos iterator before which the content will be inserted; may be
    the end() iterator
    @param[in] ilist initializer list to insert the values from

    @throw type_error.309 if called on JSON values other than arrays; example:
    `"cannot use insert() with string"`
    @throw invalid_iterator.202 if @a pos is not an iterator of *this;
    example: `"iterator does not fit current value"`

    @return iterator pointing to the first element inserted, or @a pos if
    `ilist` is empty

    @complexity Linear in `ilist.size()` plus linear in the distance between
    @a pos and end of the container.

    @liveexample{The example shows how `insert()` is used.,insert__ilist}

    @since version 1.0.0
    */
    iterator insert(const_iterator pos, initializer_list_t ilist)
    {
        // insert only works for arrays
        if (JSON_HEDLEY_UNLIKELY(not is_array()))
        {
            JSON_THROW(type_error::create(309, "cannot use insert() with " + std::string(type_name())));
        }

        // check if iterator pos fits to this JSON value
        if (JSON_HEDLEY_UNLIKELY(pos.m_object != this))
        {
            JSON_THROW(invalid_iterator::create(202, "iterator does not fit current value"));
        }

        // insert to array and return iterator
        return insert_iterator(pos, ilist.begin(), ilist.end());
    }

    /*!
    @brief inserts elements

    Inserts elements from range `[first, last)`.

    @param[in] first begin of the range of elements to insert
    @param[in] last end of the range of elements to insert

    @throw type_error.309 if called on JSON values other than objects; example:
    `"cannot use insert() with string"`
    @throw invalid_iterator.202 if iterator @a first or @a last does does not
    point to an object; example: `"iterators first and last must point to
    objects"`
    @throw invalid_iterator.210 if @a first and @a last do not belong to the
    same JSON value; example: `"iterators do not fit"`

    @complexity Logarithmic: `O(N*log(size() + N))`, where `N` is the number
    of elements to insert.

    @liveexample{The example shows how `insert()` is used.,insert__range_object}

    @since version 3.0.0
    */
    void insert(const_iterator first, const_iterator last)
    {
        // insert only works for objects
        if (JSON_HEDLEY_UNLIKELY(not is_object()))
        {
            JSON_THROW(type_error::create(309, "cannot use insert() with " + std::string(type_name())));
        }

        // check if range iterators belong to the same JSON object
        if (JSON_HEDLEY_UNLIKELY(first.m_object != last.m_object))
        {
            JSON_THROW(invalid_iterator::create(210, "iterators do not fit"));
        }

        // passed iterators must belong to objects
        if (JSON_HEDLEY_UNLIKELY(not first.m_object->is_object()))
        {
            JSON_THROW(invalid_iterator::create(202, "iterators first and last must point to objects"));
        }

        m_value.object->insert(first.m_it.object_iterator, last.m_it.object_iterator);
    }

    /*!
    @brief updates a JSON object from another object, overwriting existing keys

    Inserts all values from JSON object @a j and overwrites existing keys.

    @param[in] j  JSON object to read values from

    @throw type_error.312 if called on JSON values other than objects; example:
    `"cannot use update() with string"`

    @complexity O(N*log(size() + N)), where N is the number of elements to
                insert.

    @liveexample{The example shows how `update()` is used.,update}

    @sa https://docs.python.org/3.6/library/stdtypes.html#dict.update

    @since version 3.0.0
    */
    void update(const_reference j)
    {
        // implicitly convert null value to an empty object
        if (is_null())
        {
            m_type = value_t::object;
            m_value.object = create<object_t>();
            assert_invariant();
        }

        if (JSON_HEDLEY_UNLIKELY(not is_object()))
        {
            JSON_THROW(type_error::create(312, "cannot use update() with " + std::string(type_name())));
        }
        if (JSON_HEDLEY_UNLIKELY(not j.is_object()))
        {
            JSON_THROW(type_error::create(312, "cannot use update() with " + std::string(j.type_name())));
        }

        for (auto it = j.cbegin(); it != j.cend(); ++it)
        {
            m_value.object->operator[](it.key()) = it.value();
        }
    }

    /*!
    @brief updates a JSON object from another object, overwriting existing keys

    Inserts all values from from range `[first, last)` and overwrites existing
    keys.

    @param[in] first begin of the range of elements to insert
    @param[in] last end of the range of elements to insert

    @throw type_error.312 if called on JSON values other than objects; example:
    `"cannot use update() with string"`
    @throw invalid_iterator.202 if iterator @a first or @a last does does not
    point to an object; example: `"iterators first and last must point to
    objects"`
    @throw invalid_iterator.210 if @a first and @a last do not belong to the
    same JSON value; example: `"iterators do not fit"`

    @complexity O(N*log(size() + N)), where N is the number of elements to
                insert.

    @liveexample{The example shows how `update()` is used__range.,update}

    @sa https://docs.python.org/3.6/library/stdtypes.html#dict.update

    @since version 3.0.0
    */
    void update(const_iterator first, const_iterator last)
    {
        // implicitly convert null value to an empty object
        if (is_null())
        {
            m_type = value_t::object;
            m_value.object = create<object_t>();
            assert_invariant();
        }

        if (JSON_HEDLEY_UNLIKELY(not is_object()))
        {
            JSON_THROW(type_error::create(312, "cannot use update() with " + std::string(type_name())));
        }

        // check if range iterators belong to the same JSON object
        if (JSON_HEDLEY_UNLIKELY(first.m_object != last.m_object))
        {
            JSON_THROW(invalid_iterator::create(210, "iterators do not fit"));
        }

        // passed iterators must belong to objects
        if (JSON_HEDLEY_UNLIKELY(not first.m_object->is_object()
                                 or not last.m_object->is_object()))
        {
            JSON_THROW(invalid_iterator::create(202, "iterators first and last must point to objects"));
        }

        for (auto it = first; it != last; ++it)
        {
            m_value.object->operator[](it.key()) = it.value();
        }
    }

    /*!
    @brief exchanges the values

    Exchanges the contents of the JSON value with those of @a other. Does not
    invoke any move, copy, or swap operations on individual elements. All
    iterators and references remain valid. The past-the-end iterator is
    invalidated.

    @param[in,out] other JSON value to exchange the contents with

    @complexity Constant.

    @liveexample{The example below shows how JSON values can be swapped with
    `swap()`.,swap__reference}

    @since version 1.0.0
    */
    void swap(reference other) noexcept (
        std::is_nothrow_move_constructible<value_t>::value and
        std::is_nothrow_move_assignable<value_t>::value and
        std::is_nothrow_move_constructible<json_value>::value and
        std::is_nothrow_move_assignable<json_value>::value
    )
    {
        std::swap(m_type, other.m_type);
        std::swap(m_value, other.m_value);
        assert_invariant();
    }

    /*!
    @brief exchanges the values

    Exchanges the contents of a JSON array with those of @a other. Does not
    invoke any move, copy, or swap operations on individual elements. All
    iterators and references remain valid. The past-the-end iterator is
    invalidated.

    @param[in,out] other array to exchange the contents with

    @throw type_error.310 when JSON value is not an array; example: `"cannot
    use swap() with string"`

    @complexity Constant.

    @liveexample{The example below shows how arrays can be swapped with
    `swap()`.,swap__array_t}

    @since version 1.0.0
    */
    void swap(array_t& other)
    {
        // swap only works for arrays
        if (JSON_HEDLEY_LIKELY(is_array()))
        {
            std::swap(*(m_value.array), other);
        }
        else
        {
            JSON_THROW(type_error::create(310, "cannot use swap() with " + std::string(type_name())));
        }
    }

    /*!
    @brief exchanges the values

    Exchanges the contents of a JSON object with those of @a other. Does not
    invoke any move, copy, or swap operations on individual elements. All
    iterators and references remain valid. The past-the-end iterator is
    invalidated.

    @param[in,out] other object to exchange the contents with

    @throw type_error.310 when JSON value is not an object; example:
    `"cannot use swap() with string"`

    @complexity Constant.

    @liveexample{The example below shows how objects can be swapped with
    `swap()`.,swap__object_t}

    @since version 1.0.0
    */
    void swap(object_t& other)
    {
        // swap only works for objects
        if (JSON_HEDLEY_LIKELY(is_object()))
        {
            std::swap(*(m_value.object), other);
        }
        else
        {
            JSON_THROW(type_error::create(310, "cannot use swap() with " + std::string(type_name())));
        }
    }

    /*!
    @brief exchanges the values

    Exchanges the contents of a JSON string with those of @a other. Does not
    invoke any move, copy, or swap operations on individual elements. All
    iterators and references remain valid. The past-the-end iterator is
    invalidated.

    @param[in,out] other string to exchange the contents with

    @throw type_error.310 when JSON value is not a string; example: `"cannot
    use swap() with boolean"`

    @complexity Constant.

    @liveexample{The example below shows how strings can be swapped with
    `swap()`.,swap__string_t}

    @since version 1.0.0
    */
    void swap(string_t& other)
    {
        // swap only works for strings
        if (JSON_HEDLEY_LIKELY(is_string()))
        {
            std::swap(*(m_value.string), other);
        }
        else
        {
            JSON_THROW(type_error::create(310, "cannot use swap() with " + std::string(type_name())));
        }
    }

    /// @}

  public:
    //////////////////////////////////////////
    // lexicographical comparison operators //
    //////////////////////////////////////////

    /// @name lexicographical comparison operators
    /// @{

    /*!
    @brief comparison: equal

    Compares two JSON values for equality according to the following rules:
    - Two JSON values are equal if (1) they are from the same type and (2)
      their stored values are the same according to their respective
      `operator==`.
    - Integer and floating-point numbers are automatically converted before
      comparison. Note than two NaN values are always treated as unequal.
    - Two JSON null values are equal.

    @note Floating-point inside JSON values numbers are compared with
    `json::number_float_t::operator==` which is `double::operator==` by
    default. To compare floating-point while respecting an epsilon, an alternative
    [comparison function](https://github.com/mariokonrad/marnav/blob/master/src/marnav/math/floatingpoint.hpp#L34-#L39)
    could be used, for instance
    @code {.cpp}
    template<typename T, typename = typename std::enable_if<std::is_floating_point<T>::value, T>::type>
    inline bool is_same(T a, T b, T epsilon = std::numeric_limits<T>::epsilon()) noexcept
    {
        return std::abs(a - b) <= epsilon;
    }
    @endcode

    @note NaN values never compare equal to themselves or to other NaN values.

    @param[in] lhs  first JSON value to consider
    @param[in] rhs  second JSON value to consider
    @return whether the values @a lhs and @a rhs are equal

    @exceptionsafety No-throw guarantee: this function never throws exceptions.

    @complexity Linear.

    @liveexample{The example demonstrates comparing several JSON
    types.,operator__equal}

    @since version 1.0.0
    */
    friend bool operator==(const_reference lhs, const_reference rhs) noexcept
    {
        const auto lhs_type = lhs.type();
        const auto rhs_type = rhs.type();

        if (lhs_type == rhs_type)
        {
            switch (lhs_type)
            {
                case value_t::array:
                    return *lhs.m_value.array == *rhs.m_value.array;

                case value_t::object:
                    return *lhs.m_value.object == *rhs.m_value.object;

                case value_t::null:
                    return true;

                case value_t::string:
                    return *lhs.m_value.string == *rhs.m_value.string;

                case value_t::boolean:
                    return lhs.m_value.boolean == rhs.m_value.boolean;

                case value_t::number_integer:
                    return lhs.m_value.number_integer == rhs.m_value.number_integer;

                case value_t::number_unsigned:
                    return lhs.m_value.number_unsigned == rhs.m_value.number_unsigned;

                case value_t::number_float:
                    return lhs.m_value.number_float == rhs.m_value.number_float;

                default:
                    return false;
            }
        }
        else if (lhs_type == value_t::number_integer and rhs_type == value_t::number_float)
        {
            return static_cast<number_float_t>(lhs.m_value.number_integer) == rhs.m_value.number_float;
        }
        else if (lhs_type == value_t::number_float and rhs_type == value_t::number_integer)
        {
            return lhs.m_value.number_float == static_cast<number_float_t>(rhs.m_value.number_integer);
        }
        else if (lhs_type == value_t::number_unsigned and rhs_type == value_t::number_float)
        {
            return static_cast<number_float_t>(lhs.m_value.number_unsigned) == rhs.m_value.number_float;
        }
        else if (lhs_type == value_t::number_float and rhs_type == value_t::number_unsigned)
        {
            return lhs.m_value.number_float == static_cast<number_float_t>(rhs.m_value.number_unsigned);
        }
        else if (lhs_type == value_t::number_unsigned and rhs_type == value_t::number_integer)
        {
            return static_cast<number_integer_t>(lhs.m_value.number_unsigned) == rhs.m_value.number_integer;
        }
        else if (lhs_type == value_t::number_integer and rhs_type == value_t::number_unsigned)
        {
            return lhs.m_value.number_integer == static_cast<number_integer_t>(rhs.m_value.number_unsigned);
        }

        return false;
    }

    /*!
    @brief comparison: equal
    @copydoc operator==(const_reference, const_reference)
    */
    template<typename ScalarType, typename std::enable_if<
                 std::is_scalar<ScalarType>::value, int>::type = 0>
    friend bool operator==(const_reference lhs, const ScalarType rhs) noexcept
    {
        return lhs == basic_json(rhs);
    }

    /*!
    @brief comparison: equal
    @copydoc operator==(const_reference, const_reference)
    */
    template<typename ScalarType, typename std::enable_if<
                 std::is_scalar<ScalarType>::value, int>::type = 0>
    friend bool operator==(const ScalarType lhs, const_reference rhs) noexcept
    {
        return basic_json(lhs) == rhs;
    }

    /*!
    @brief comparison: not equal

    Compares two JSON values for inequality by calculating `not (lhs == rhs)`.

    @param[in] lhs  first JSON value to consider
    @param[in] rhs  second JSON value to consider
    @return whether the values @a lhs and @a rhs are not equal

    @complexity Linear.

    @exceptionsafety No-throw guarantee: this function never throws exceptions.

    @liveexample{The example demonstrates comparing several JSON
    types.,operator__notequal}

    @since version 1.0.0
    */
    friend bool operator!=(const_reference lhs, const_reference rhs) noexcept
    {
        return not (lhs == rhs);
    }

    /*!
    @brief comparison: not equal
    @copydoc operator!=(const_reference, const_reference)
    */
    template<typename ScalarType, typename std::enable_if<
                 std::is_scalar<ScalarType>::value, int>::type = 0>
    friend bool operator!=(const_reference lhs, const ScalarType rhs) noexcept
    {
        return lhs != basic_json(rhs);
    }

    /*!
    @brief comparison: not equal
    @copydoc operator!=(const_reference, const_reference)
    */
    template<typename ScalarType, typename std::enable_if<
                 std::is_scalar<ScalarType>::value, int>::type = 0>
    friend bool operator!=(const ScalarType lhs, const_reference rhs) noexcept
    {
        return basic_json(lhs) != rhs;
    }

    /*!
    @brief comparison: less than

    Compares whether one JSON value @a lhs is less than another JSON value @a
    rhs according to the following rules:
    - If @a lhs and @a rhs have the same type, the values are compared using
      the default `<` operator.
    - Integer and floating-point numbers are automatically converted before
      comparison
    - In case @a lhs and @a rhs have different types, the values are ignored
      and the order of the types is considered, see
      @ref operator<(const value_t, const value_t).

    @param[in] lhs  first JSON value to consider
    @param[in] rhs  second JSON value to consider
    @return whether @a lhs is less than @a rhs

    @complexity Linear.

    @exceptionsafety No-throw guarantee: this function never throws exceptions.

    @liveexample{The example demonstrates comparing several JSON
    types.,operator__less}

    @since version 1.0.0
    */
    friend bool operator<(const_reference lhs, const_reference rhs) noexcept
    {
        const auto lhs_type = lhs.type();
        const auto rhs_type = rhs.type();

        if (lhs_type == rhs_type)
        {
            switch (lhs_type)
            {
                case value_t::array:
                    // note parentheses are necessary, see
                    // https://github.com/nlohmann/json/issues/1530
                    return (*lhs.m_value.array) < (*rhs.m_value.array);

                case value_t::object:
                    return (*lhs.m_value.object) < (*rhs.m_value.object);

                case value_t::null:
                    return false;

                case value_t::string:
                    return (*lhs.m_value.string) < (*rhs.m_value.string);

                case value_t::boolean:
                    return (lhs.m_value.boolean) < (rhs.m_value.boolean);

                case value_t::number_integer:
                    return (lhs.m_value.number_integer) < (rhs.m_value.number_integer);

                case value_t::number_unsigned:
                    return (lhs.m_value.number_unsigned) < (rhs.m_value.number_unsigned);

                case value_t::number_float:
                    return (lhs.m_value.number_float) < (rhs.m_value.number_float);

                default:
                    return false;
            }
        }
        else if (lhs_type == value_t::number_integer and rhs_type == value_t::number_float)
        {
            return static_cast<number_float_t>(lhs.m_value.number_integer) < rhs.m_value.number_float;
        }
        else if (lhs_type == value_t::number_float and rhs_type == value_t::number_integer)
        {
            return lhs.m_value.number_float < static_cast<number_float_t>(rhs.m_value.number_integer);
        }
        else if (lhs_type == value_t::number_unsigned and rhs_type == value_t::number_float)
        {
            return static_cast<number_float_t>(lhs.m_value.number_unsigned) < rhs.m_value.number_float;
        }
        else if (lhs_type == value_t::number_float and rhs_type == value_t::number_unsigned)
        {
            return lhs.m_value.number_float < static_cast<number_float_t>(rhs.m_value.number_unsigned);
        }
        else if (lhs_type == value_t::number_integer and rhs_type == value_t::number_unsigned)
        {
            return lhs.m_value.number_integer < static_cast<number_integer_t>(rhs.m_value.number_unsigned);
        }
        else if (lhs_type == value_t::number_unsigned and rhs_type == value_t::number_integer)
        {
            return static_cast<number_integer_t>(lhs.m_value.number_unsigned) < rhs.m_value.number_integer;
        }

        // We only reach this line if we cannot compare values. In that case,
        // we compare types. Note we have to call the operator explicitly,
        // because MSVC has problems otherwise.
        return operator<(lhs_type, rhs_type);
    }

    /*!
    @brief comparison: less than
    @copydoc operator<(const_reference, const_reference)
    */
    template<typename ScalarType, typename std::enable_if<
                 std::is_scalar<ScalarType>::value, int>::type = 0>
    friend bool operator<(const_reference lhs, const ScalarType rhs) noexcept
    {
        return lhs < basic_json(rhs);
    }

    /*!
    @brief comparison: less than
    @copydoc operator<(const_reference, const_reference)
    */
    template<typename ScalarType, typename std::enable_if<
                 std::is_scalar<ScalarType>::value, int>::type = 0>
    friend bool operator<(const ScalarType lhs, const_reference rhs) noexcept
    {
        return basic_json(lhs) < rhs;
    }

    /*!
    @brief comparison: less than or equal

    Compares whether one JSON value @a lhs is less than or equal to another
    JSON value by calculating `not (rhs < lhs)`.

    @param[in] lhs  first JSON value to consider
    @param[in] rhs  second JSON value to consider
    @return whether @a lhs is less than or equal to @a rhs

    @complexity Linear.

    @exceptionsafety No-throw guarantee: this function never throws exceptions.

    @liveexample{The example demonstrates comparing several JSON
    types.,operator__greater}

    @since version 1.0.0
    */
    friend bool operator<=(const_reference lhs, const_reference rhs) noexcept
    {
        return not (rhs < lhs);
    }

    /*!
    @brief comparison: less than or equal
    @copydoc operator<=(const_reference, const_reference)
    */
    template<typename ScalarType, typename std::enable_if<
                 std::is_scalar<ScalarType>::value, int>::type = 0>
    friend bool operator<=(const_reference lhs, const ScalarType rhs) noexcept
    {
        return lhs <= basic_json(rhs);
    }

    /*!
    @brief comparison: less than or equal
    @copydoc operator<=(const_reference, const_reference)
    */
    template<typename ScalarType, typename std::enable_if<
                 std::is_scalar<ScalarType>::value, int>::type = 0>
    friend bool operator<=(const ScalarType lhs, const_reference rhs) noexcept
    {
        return basic_json(lhs) <= rhs;
    }

    /*!
    @brief comparison: greater than

    Compares whether one JSON value @a lhs is greater than another
    JSON value by calculating `not (lhs <= rhs)`.

    @param[in] lhs  first JSON value to consider
    @param[in] rhs  second JSON value to consider
    @return whether @a lhs is greater than to @a rhs

    @complexity Linear.

    @exceptionsafety No-throw guarantee: this function never throws exceptions.

    @liveexample{The example demonstrates comparing several JSON
    types.,operator__lessequal}

    @since version 1.0.0
    */
    friend bool operator>(const_reference lhs, const_reference rhs) noexcept
    {
        return not (lhs <= rhs);
    }

    /*!
    @brief comparison: greater than
    @copydoc operator>(const_reference, const_reference)
    */
    template<typename ScalarType, typename std::enable_if<
                 std::is_scalar<ScalarType>::value, int>::type = 0>
    friend bool operator>(const_reference lhs, const ScalarType rhs) noexcept
    {
        return lhs > basic_json(rhs);
    }

    /*!
    @brief comparison: greater than
    @copydoc operator>(const_reference, const_reference)
    */
    template<typename ScalarType, typename std::enable_if<
                 std::is_scalar<ScalarType>::value, int>::type = 0>
    friend bool operator>(const ScalarType lhs, const_reference rhs) noexcept
    {
        return basic_json(lhs) > rhs;
    }

    /*!
    @brief comparison: greater than or equal

    Compares whether one JSON value @a lhs is greater than or equal to another
    JSON value by calculating `not (lhs < rhs)`.

    @param[in] lhs  first JSON value to consider
    @param[in] rhs  second JSON value to consider
    @return whether @a lhs is greater than or equal to @a rhs

    @complexity Linear.

    @exceptionsafety No-throw guarantee: this function never throws exceptions.

    @liveexample{The example demonstrates comparing several JSON
    types.,operator__greaterequal}

    @since version 1.0.0
    */
    friend bool operator>=(const_reference lhs, const_reference rhs) noexcept
    {
        return not (lhs < rhs);
    }

    /*!
    @brief comparison: greater than or equal
    @copydoc operator>=(const_reference, const_reference)
    */
    template<typename ScalarType, typename std::enable_if<
                 std::is_scalar<ScalarType>::value, int>::type = 0>
    friend bool operator>=(const_reference lhs, const ScalarType rhs) noexcept
    {
        return lhs >= basic_json(rhs);
    }

    /*!
    @brief comparison: greater than or equal
    @copydoc operator>=(const_reference, const_reference)
    */
    template<typename ScalarType, typename std::enable_if<
                 std::is_scalar<ScalarType>::value, int>::type = 0>
    friend bool operator>=(const ScalarType lhs, const_reference rhs) noexcept
    {
        return basic_json(lhs) >= rhs;
    }

    /// @}

    ///////////////////
    // serialization //
    ///////////////////

    /// @name serialization
    /// @{

    /*!
    @brief serialize to stream

    Serialize the given JSON value @a j to the output stream @a o. The JSON
    value will be serialized using the @ref dump member function.

    - The indentation of the output can be controlled with the member variable
      `width` of the output stream @a o. For instance, using the manipulator
      `std::setw(4)` on @a o sets the indentation level to `4` and the
      serialization result is the same as calling `dump(4)`.

    - The indentation character can be controlled with the member variable
      `fill` of the output stream @a o. For instance, the manipulator
      `std::setfill('\\t')` sets indentation to use a tab character rather than
      the default space character.

    @param[in,out] o  stream to serialize to
    @param[in] j  JSON value to serialize

    @return the stream @a o

    @throw type_error.316 if a string stored inside the JSON value is not
                          UTF-8 encoded

    @complexity Linear.

    @liveexample{The example below shows the serialization with different
    parameters to `width` to adjust the indentation level.,operator_serialize}

    @since version 1.0.0; indentation character added in version 3.0.0
    */
    friend std::ostream& operator<<(std::ostream& o, const basic_json& j)
    {
        // read width member and use it as indentation parameter if nonzero
        const bool pretty_print = o.width() > 0;
        const auto indentation = pretty_print ? o.width() : 0;

        // reset width to 0 for subsequent calls to this stream
        o.width(0);

        // do the actual serialization
        serializer s(detail::output_adapter<char>(o), o.fill());
        s.dump(j, pretty_print, false, static_cast<unsigned int>(indentation));
        return o;
    }

    /*!
    @brief serialize to stream
    @deprecated This stream operator is deprecated and will be removed in
                future 4.0.0 of the library. Please use
                @ref operator<<(std::ostream&, const basic_json&)
                instead; that is, replace calls like `j >> o;` with `o << j;`.
    @since version 1.0.0; deprecated since version 3.0.0
    */
    JSON_HEDLEY_DEPRECATED(3.0.0)
    friend std::ostream& operator>>(const basic_json& j, std::ostream& o)
    {
        return o << j;
    }

    /// @}


    /////////////////////
    // deserialization //
    /////////////////////

    /// @name deserialization
    /// @{

    /*!
    @brief deserialize from a compatible input

    This function reads from a compatible input. Examples are:
    - an array of 1-byte values
    - strings with character/literal type with size of 1 byte
    - input streams
    - container with contiguous storage of 1-byte values. Compatible container
      types include `std::vector`, `std::string`, `std::array`,
      `std::valarray`, and `std::initializer_list`. Furthermore, C-style
      arrays can be used with `std::begin()`/`std::end()`. User-defined
      containers can be used as long as they implement random-access iterators
      and a contiguous storage.

    @pre Each element of the container has a size of 1 byte. Violating this
    precondition yields undefined behavior. **This precondition is enforced
    with a static assertion.**

    @pre The container storage is contiguous. Violating this precondition
    yields undefined behavior. **This precondition is enforced with an
    assertion.**

    @warning There is no way to enforce all preconditions at compile-time. If
             the function is called with a noncompliant container and with
             assertions switched off, the behavior is undefined and will most
             likely yield segmentation violation.

    @param[in] i  input to read from
    @param[in] cb  a parser callback function of type @ref parser_callback_t
    which is used to control the deserialization by filtering unwanted values
    (optional)
    @param[in] allow_exceptions  whether to throw exceptions in case of a
    parse error (optional, true by default)

    @return deserialized JSON value; in case of a parse error and
            @a allow_exceptions set to `false`, the return value will be
            value_t::discarded.

    @throw parse_error.101 if a parse error occurs; example: `""unexpected end
    of input; expected string literal""`
    @throw parse_error.102 if to_unicode fails or surrogate error
    @throw parse_error.103 if to_unicode fails

    @complexity Linear in the length of the input. The parser is a predictive
    LL(1) parser. The complexity can be higher if the parser callback function
    @a cb has a super-linear complexity.

    @note A UTF-8 byte order mark is silently ignored.

    @liveexample{The example below demonstrates the `parse()` function reading
    from an array.,parse__array__parser_callback_t}

    @liveexample{The example below demonstrates the `parse()` function with
    and without callback function.,parse__string__parser_callback_t}

    @liveexample{The example below demonstrates the `parse()` function with
    and without callback function.,parse__istream__parser_callback_t}

    @liveexample{The example below demonstrates the `parse()` function reading
    from a contiguous container.,parse__contiguouscontainer__parser_callback_t}

    @since version 2.0.3 (contiguous containers)
    */
    JSON_HEDLEY_WARN_UNUSED_RESULT
    static basic_json parse(detail::input_adapter&& i,
                            const parser_callback_t cb = nullptr,
                            const bool allow_exceptions = true)
    {
        basic_json result;
        parser(i, cb, allow_exceptions).parse(true, result);
        return result;
    }

    static bool accept(detail::input_adapter&& i)
    {
        return parser(i).accept(true);
    }

    /*!
    @brief generate SAX events

    The SAX event lister must follow the interface of @ref json_sax.

    This function reads from a compatible input. Examples are:
    - an array of 1-byte values
    - strings with character/literal type with size of 1 byte
    - input streams
    - container with contiguous storage of 1-byte values. Compatible container
      types include `std::vector`, `std::string`, `std::array`,
      `std::valarray`, and `std::initializer_list`. Furthermore, C-style
      arrays can be used with `std::begin()`/`std::end()`. User-defined
      containers can be used as long as they implement random-access iterators
      and a contiguous storage.

    @pre Each element of the container has a size of 1 byte. Violating this
    precondition yields undefined behavior. **This precondition is enforced
    with a static assertion.**

    @pre The container storage is contiguous. Violating this precondition
    yields undefined behavior. **This precondition is enforced with an
    assertion.**

    @warning There is no way to enforce all preconditions at compile-time. If
             the function is called with a noncompliant container and with
             assertions switched off, the behavior is undefined and will most
             likely yield segmentation violation.

    @param[in] i  input to read from
    @param[in,out] sax  SAX event listener
    @param[in] format  the format to parse (JSON, CBOR, MessagePack, or UBJSON)
    @param[in] strict  whether the input has to be consumed completely

    @return return value of the last processed SAX event

    @throw parse_error.101 if a parse error occurs; example: `""unexpected end
    of input; expected string literal""`
    @throw parse_error.102 if to_unicode fails or surrogate error
    @throw parse_error.103 if to_unicode fails

    @complexity Linear in the length of the input. The parser is a predictive
    LL(1) parser. The complexity can be higher if the SAX consumer @a sax has
    a super-linear complexity.

    @note A UTF-8 byte order mark is silently ignored.

    @liveexample{The example below demonstrates the `sax_parse()` function
    reading from string and processing the events with a user-defined SAX
    event consumer.,sax_parse}

    @since version 3.2.0
    */
    template <typename SAX>
    JSON_HEDLEY_NON_NULL(2)
    static bool sax_parse(detail::input_adapter&& i, SAX* sax,
                          input_format_t format = input_format_t::json,
                          const bool strict = true)
    {
        assert(sax);
        return format == input_format_t::json
               ? parser(std::move(i)).sax_parse(sax, strict)
               : detail::binary_reader<basic_json, SAX>(std::move(i)).sax_parse(format, sax, strict);
    }

    /*!
    @brief deserialize from an iterator range with contiguous storage

    This function reads from an iterator range of a container with contiguous
    storage of 1-byte values. Compatible container types include
    `std::vector`, `std::string`, `std::array`, `std::valarray`, and
    `std::initializer_list`. Furthermore, C-style arrays can be used with
    `std::begin()`/`std::end()`. User-defined containers can be used as long
    as they implement random-access iterators and a contiguous storage.

    @pre The iterator range is contiguous. Violating this precondition yields
    undefined behavior. **This precondition is enforced with an assertion.**
    @pre Each element in the range has a size of 1 byte. Violating this
    precondition yields undefined behavior. **This precondition is enforced
    with a static assertion.**

    @warning There is no way to enforce all preconditions at compile-time. If
             the function is called with noncompliant iterators and with
             assertions switched off, the behavior is undefined and will most
             likely yield segmentation violation.

    @tparam IteratorType iterator of container with contiguous storage
    @param[in] first  begin of the range to parse (included)
    @param[in] last  end of the range to parse (excluded)
    @param[in] cb  a parser callback function of type @ref parser_callback_t
    which is used to control the deserialization by filtering unwanted values
    (optional)
    @param[in] allow_exceptions  whether to throw exceptions in case of a
    parse error (optional, true by default)

    @return deserialized JSON value; in case of a parse error and
            @a allow_exceptions set to `false`, the return value will be
            value_t::discarded.

    @throw parse_error.101 in case of an unexpected token
    @throw parse_error.102 if to_unicode fails or surrogate error
    @throw parse_error.103 if to_unicode fails

    @complexity Linear in the length of the input. The parser is a predictive
    LL(1) parser. The complexity can be higher if the parser callback function
    @a cb has a super-linear complexity.

    @note A UTF-8 byte order mark is silently ignored.

    @liveexample{The example below demonstrates the `parse()` function reading
    from an iterator range.,parse__iteratortype__parser_callback_t}

    @since version 2.0.3
    */
    template<class IteratorType, typename std::enable_if<
                 std::is_base_of<
                     std::random_access_iterator_tag,
                     typename std::iterator_traits<IteratorType>::iterator_category>::value, int>::type = 0>
    static basic_json parse(IteratorType first, IteratorType last,
                            const parser_callback_t cb = nullptr,
                            const bool allow_exceptions = true)
    {
        basic_json result;
        parser(detail::input_adapter(first, last), cb, allow_exceptions).parse(true, result);
        return result;
    }

    template<class IteratorType, typename std::enable_if<
                 std::is_base_of<
                     std::random_access_iterator_tag,
                     typename std::iterator_traits<IteratorType>::iterator_category>::value, int>::type = 0>
    static bool accept(IteratorType first, IteratorType last)
    {
        return parser(detail::input_adapter(first, last)).accept(true);
    }

    template<class IteratorType, class SAX, typename std::enable_if<
                 std::is_base_of<
                     std::random_access_iterator_tag,
                     typename std::iterator_traits<IteratorType>::iterator_category>::value, int>::type = 0>
    JSON_HEDLEY_NON_NULL(3)
    static bool sax_parse(IteratorType first, IteratorType last, SAX* sax)
    {
        return parser(detail::input_adapter(first, last)).sax_parse(sax);
    }

    /*!
    @brief deserialize from stream
    @deprecated This stream operator is deprecated and will be removed in
                version 4.0.0 of the library. Please use
                @ref operator>>(std::istream&, basic_json&)
                instead; that is, replace calls like `j << i;` with `i >> j;`.
    @since version 1.0.0; deprecated since version 3.0.0
    */
    JSON_HEDLEY_DEPRECATED(3.0.0)
    friend std::istream& operator<<(basic_json& j, std::istream& i)
    {
        return operator>>(i, j);
    }

    /*!
    @brief deserialize from stream

    Deserializes an input stream to a JSON value.

    @param[in,out] i  input stream to read a serialized JSON value from
    @param[in,out] j  JSON value to write the deserialized input to

    @throw parse_error.101 in case of an unexpected token
    @throw parse_error.102 if to_unicode fails or surrogate error
    @throw parse_error.103 if to_unicode fails

    @complexity Linear in the length of the input. The parser is a predictive
    LL(1) parser.

    @note A UTF-8 byte order mark is silently ignored.

    @liveexample{The example below shows how a JSON value is constructed by
    reading a serialization from a stream.,operator_deserialize}

    @sa parse(std::istream&, const parser_callback_t) for a variant with a
    parser callback function to filter values while parsing

    @since version 1.0.0
    */
    friend std::istream& operator>>(std::istream& i, basic_json& j)
    {
        parser(detail::input_adapter(i)).parse(false, j);
        return i;
    }

    /// @}

    ///////////////////////////
    // convenience functions //
    ///////////////////////////

    /*!
    @brief return the type as string

    Returns the type name as string to be used in error messages - usually to
    indicate that a function was called on a wrong JSON type.

    @return a string representation of a the @a m_type member:
            Value type  | return value
            ----------- | -------------
            null        | `"null"`
            boolean     | `"boolean"`
            string      | `"string"`
            number      | `"number"` (for all number types)
            object      | `"object"`
            array       | `"array"`
            discarded   | `"discarded"`

    @exceptionsafety No-throw guarantee: this function never throws exceptions.

    @complexity Constant.

    @liveexample{The following code exemplifies `type_name()` for all JSON
    types.,type_name}

    @sa @ref type() -- return the type of the JSON value
    @sa @ref operator value_t() -- return the type of the JSON value (implicit)

    @since version 1.0.0, public since 2.1.0, `const char*` and `noexcept`
    since 3.0.0
    */
    JSON_HEDLEY_RETURNS_NON_NULL
    const char* type_name() const noexcept
    {
        {
            switch (m_type)
            {
                case value_t::null:
                    return "null";
                case value_t::object:
                    return "object";
                case value_t::array:
                    return "array";
                case value_t::string:
                    return "string";
                case value_t::boolean:
                    return "boolean";
                case value_t::discarded:
                    return "discarded";
                default:
                    return "number";
            }
        }
    }


  private:
    //////////////////////
    // member variables //
    //////////////////////

    /// the type of the current element
    value_t m_type = value_t::null;

    /// the value of the current element
    json_value m_value = {};

    //////////////////////////////////////////
    // binary serialization/deserialization //
    //////////////////////////////////////////

    /// @name binary serialization/deserialization support
    /// @{

  public:
    /*!
    @brief create a CBOR serialization of a given JSON value

    Serializes a given JSON value @a j to a byte vector using the CBOR (Concise
    Binary Object Representation) serialization format. CBOR is a binary
    serialization format which aims to be more compact than JSON itself, yet
    more efficient to parse.

    The library uses the following mapping from JSON values types to
    CBOR types according to the CBOR specification (RFC 7049):

    JSON value type | value/range                                | CBOR type                          | first byte
    --------------- | ------------------------------------------ | ---------------------------------- | ---------------
    null            | `null`                                     | Null                               | 0xF6
    boolean         | `true`                                     | True                               | 0xF5
    boolean         | `false`                                    | False                              | 0xF4
    number_integer  | -9223372036854775808..-2147483649          | Negative integer (8 bytes follow)  | 0x3B
    number_integer  | -2147483648..-32769                        | Negative integer (4 bytes follow)  | 0x3A
    number_integer  | -32768..-129                               | Negative integer (2 bytes follow)  | 0x39
    number_integer  | -128..-25                                  | Negative integer (1 byte follow)   | 0x38
    number_integer  | -24..-1                                    | Negative integer                   | 0x20..0x37
    number_integer  | 0..23                                      | Integer                            | 0x00..0x17
    number_integer  | 24..255                                    | Unsigned integer (1 byte follow)   | 0x18
    number_integer  | 256..65535                                 | Unsigned integer (2 bytes follow)  | 0x19
    number_integer  | 65536..4294967295                          | Unsigned integer (4 bytes follow)  | 0x1A
    number_integer  | 4294967296..18446744073709551615           | Unsigned integer (8 bytes follow)  | 0x1B
    number_unsigned | 0..23                                      | Integer                            | 0x00..0x17
    number_unsigned | 24..255                                    | Unsigned integer (1 byte follow)   | 0x18
    number_unsigned | 256..65535                                 | Unsigned integer (2 bytes follow)  | 0x19
    number_unsigned | 65536..4294967295                          | Unsigned integer (4 bytes follow)  | 0x1A
    number_unsigned | 4294967296..18446744073709551615           | Unsigned integer (8 bytes follow)  | 0x1B
    number_float    | *any value*                                | Double-Precision Float             | 0xFB
    string          | *length*: 0..23                            | UTF-8 string                       | 0x60..0x77
    string          | *length*: 23..255                          | UTF-8 string (1 byte follow)       | 0x78
    string          | *length*: 256..65535                       | UTF-8 string (2 bytes follow)      | 0x79
    string          | *length*: 65536..4294967295                | UTF-8 string (4 bytes follow)      | 0x7A
    string          | *length*: 4294967296..18446744073709551615 | UTF-8 string (8 bytes follow)      | 0x7B
    array           | *size*: 0..23                              | array                              | 0x80..0x97
    array           | *size*: 23..255                            | array (1 byte follow)              | 0x98
    array           | *size*: 256..65535                         | array (2 bytes follow)             | 0x99
    array           | *size*: 65536..4294967295                  | array (4 bytes follow)             | 0x9A
    array           | *size*: 4294967296..18446744073709551615   | array (8 bytes follow)             | 0x9B
    object          | *size*: 0..23                              | map                                | 0xA0..0xB7
    object          | *size*: 23..255                            | map (1 byte follow)                | 0xB8
    object          | *size*: 256..65535                         | map (2 bytes follow)               | 0xB9
    object          | *size*: 65536..4294967295                  | map (4 bytes follow)               | 0xBA
    object          | *size*: 4294967296..18446744073709551615   | map (8 bytes follow)               | 0xBB

    @note The mapping is **complete** in the sense that any JSON value type
          can be converted to a CBOR value.

    @note If NaN or Infinity are stored inside a JSON number, they are
          serialized properly. This behavior differs from the @ref dump()
          function which serializes NaN or Infinity to `null`.

    @note The following CBOR types are not used in the conversion:
          - byte strings (0x40..0x5F)
          - UTF-8 strings terminated by "break" (0x7F)
          - arrays terminated by "break" (0x9F)
          - maps terminated by "break" (0xBF)
          - date/time (0xC0..0xC1)
          - bignum (0xC2..0xC3)
          - decimal fraction (0xC4)
          - bigfloat (0xC5)
          - tagged items (0xC6..0xD4, 0xD8..0xDB)
          - expected conversions (0xD5..0xD7)
          - simple values (0xE0..0xF3, 0xF8)
          - undefined (0xF7)
          - half and single-precision floats (0xF9-0xFA)
          - break (0xFF)

    @param[in] j  JSON value to serialize
    @return MessagePack serialization as byte vector

    @complexity Linear in the size of the JSON value @a j.

    @liveexample{The example shows the serialization of a JSON value to a byte
    vector in CBOR format.,to_cbor}

    @sa http://cbor.io
    @sa @ref from_cbor(detail::input_adapter&&, const bool, const bool) for the
        analogous deserialization
    @sa @ref to_msgpack(const basic_json&) for the related MessagePack format
    @sa @ref to_ubjson(const basic_json&, const bool, const bool) for the
             related UBJSON format

    @since version 2.0.9
    */
    static std::vector<uint8_t> to_cbor(const basic_json& j)
    {
        std::vector<uint8_t> result;
        to_cbor(j, result);
        return result;
    }

    static void to_cbor(const basic_json& j, detail::output_adapter<uint8_t> o)
    {
        binary_writer<uint8_t>(o).write_cbor(j);
    }

    static void to_cbor(const basic_json& j, detail::output_adapter<char> o)
    {
        binary_writer<char>(o).write_cbor(j);
    }

    /*!
    @brief create a MessagePack serialization of a given JSON value

    Serializes a given JSON value @a j to a byte vector using the MessagePack
    serialization format. MessagePack is a binary serialization format which
    aims to be more compact than JSON itself, yet more efficient to parse.

    The library uses the following mapping from JSON values types to
    MessagePack types according to the MessagePack specification:

    JSON value type | value/range                       | MessagePack type | first byte
    --------------- | --------------------------------- | ---------------- | ----------
    null            | `null`                            | nil              | 0xC0
    boolean         | `true`                            | true             | 0xC3
    boolean         | `false`                           | false            | 0xC2
    number_integer  | -9223372036854775808..-2147483649 | int64            | 0xD3
    number_integer  | -2147483648..-32769               | int32            | 0xD2
    number_integer  | -32768..-129                      | int16            | 0xD1
    number_integer  | -128..-33                         | int8             | 0xD0
    number_integer  | -32..-1                           | negative fixint  | 0xE0..0xFF
    number_integer  | 0..127                            | positive fixint  | 0x00..0x7F
    number_integer  | 128..255                          | uint 8           | 0xCC
    number_integer  | 256..65535                        | uint 16          | 0xCD
    number_integer  | 65536..4294967295                 | uint 32          | 0xCE
    number_integer  | 4294967296..18446744073709551615  | uint 64          | 0xCF
    number_unsigned | 0..127                            | positive fixint  | 0x00..0x7F
    number_unsigned | 128..255                          | uint 8           | 0xCC
    number_unsigned | 256..65535                        | uint 16          | 0xCD
    number_unsigned | 65536..4294967295                 | uint 32          | 0xCE
    number_unsigned | 4294967296..18446744073709551615  | uint 64          | 0xCF
    number_float    | *any value*                       | float 64         | 0xCB
    string          | *length*: 0..31                   | fixstr           | 0xA0..0xBF
    string          | *length*: 32..255                 | str 8            | 0xD9
    string          | *length*: 256..65535              | str 16           | 0xDA
    string          | *length*: 65536..4294967295       | str 32           | 0xDB
    array           | *size*: 0..15                     | fixarray         | 0x90..0x9F
    array           | *size*: 16..65535                 | array 16         | 0xDC
    array           | *size*: 65536..4294967295         | array 32         | 0xDD
    object          | *size*: 0..15                     | fix map          | 0x80..0x8F
    object          | *size*: 16..65535                 | map 16           | 0xDE
    object          | *size*: 65536..4294967295         | map 32           | 0xDF

    @note The mapping is **complete** in the sense that any JSON value type
          can be converted to a MessagePack value.

    @note The following values can **not** be converted to a MessagePack value:
          - strings with more than 4294967295 bytes
          - arrays with more than 4294967295 elements
          - objects with more than 4294967295 elements

    @note The following MessagePack types are not used in the conversion:
          - bin 8 - bin 32 (0xC4..0xC6)
          - ext 8 - ext 32 (0xC7..0xC9)
          - float 32 (0xCA)
          - fixext 1 - fixext 16 (0xD4..0xD8)

    @note Any MessagePack output created @ref to_msgpack can be successfully
          parsed by @ref from_msgpack.

    @note If NaN or Infinity are stored inside a JSON number, they are
          serialized properly. This behavior differs from the @ref dump()
          function which serializes NaN or Infinity to `null`.

    @param[in] j  JSON value to serialize
    @return MessagePack serialization as byte vector

    @complexity Linear in the size of the JSON value @a j.

    @liveexample{The example shows the serialization of a JSON value to a byte
    vector in MessagePack format.,to_msgpack}

    @sa http://msgpack.org
    @sa @ref from_msgpack for the analogous deserialization
    @sa @ref to_cbor(const basic_json& for the related CBOR format
    @sa @ref to_ubjson(const basic_json&, const bool, const bool) for the
             related UBJSON format

    @since version 2.0.9
    */
    static std::vector<uint8_t> to_msgpack(const basic_json& j)
    {
        std::vector<uint8_t> result;
        to_msgpack(j, result);
        return result;
    }

    static void to_msgpack(const basic_json& j, detail::output_adapter<uint8_t> o)
    {
        binary_writer<uint8_t>(o).write_msgpack(j);
    }

    static void to_msgpack(const basic_json& j, detail::output_adapter<char> o)
    {
        binary_writer<char>(o).write_msgpack(j);
    }

    /*!
    @brief create a UBJSON serialization of a given JSON value

    Serializes a given JSON value @a j to a byte vector using the UBJSON
    (Universal Binary JSON) serialization format. UBJSON aims to be more compact
    than JSON itself, yet more efficient to parse.

    The library uses the following mapping from JSON values types to
    UBJSON types according to the UBJSON specification:

    JSON value type | value/range                       | UBJSON type | marker
    --------------- | --------------------------------- | ----------- | ------
    null            | `null`                            | null        | `Z`
    boolean         | `true`                            | true        | `T`
    boolean         | `false`                           | false       | `F`
    number_integer  | -9223372036854775808..-2147483649 | int64       | `L`
    number_integer  | -2147483648..-32769               | int32       | `l`
    number_integer  | -32768..-129                      | int16       | `I`
    number_integer  | -128..127                         | int8        | `i`
    number_integer  | 128..255                          | uint8       | `U`
    number_integer  | 256..32767                        | int16       | `I`
    number_integer  | 32768..2147483647                 | int32       | `l`
    number_integer  | 2147483648..9223372036854775807   | int64       | `L`
    number_unsigned | 0..127                            | int8        | `i`
    number_unsigned | 128..255                          | uint8       | `U`
    number_unsigned | 256..32767                        | int16       | `I`
    number_unsigned | 32768..2147483647                 | int32       | `l`
    number_unsigned | 2147483648..9223372036854775807   | int64       | `L`
    number_float    | *any value*                       | float64     | `D`
    string          | *with shortest length indicator*  | string      | `S`
    array           | *see notes on optimized format*   | array       | `[`
    object          | *see notes on optimized format*   | map         | `{`

    @note The mapping is **complete** in the sense that any JSON value type
          can be converted to a UBJSON value.

    @note The following values can **not** be converted to a UBJSON value:
          - strings with more than 9223372036854775807 bytes (theoretical)
          - unsigned integer numbers above 9223372036854775807

    @note The following markers are not used in the conversion:
          - `Z`: no-op values are not created.
          - `C`: single-byte strings are serialized with `S` markers.

    @note Any UBJSON output created @ref to_ubjson can be successfully parsed
          by @ref from_ubjson.

    @note If NaN or Infinity are stored inside a JSON number, they are
          serialized properly. This behavior differs from the @ref dump()
          function which serializes NaN or Infinity to `null`.

    @note The optimized formats for containers are supported: Parameter
          @a use_size adds size information to the beginning of a container and
          removes the closing marker. Parameter @a use_type further checks
          whether all elements of a container have the same type and adds the
          type marker to the beginning of the container. The @a use_type
          parameter must only be used together with @a use_size = true. Note
          that @a use_size = true alone may result in larger representations -
          the benefit of this parameter is that the receiving side is
          immediately informed on the number of elements of the container.

    @param[in] j  JSON value to serialize
    @param[in] use_size  whether to add size annotations to container types
    @param[in] use_type  whether to add type annotations to container types
                         (must be combined with @a use_size = true)
    @return UBJSON serialization as byte vector

    @complexity Linear in the size of the JSON value @a j.

    @liveexample{The example shows the serialization of a JSON value to a byte
    vector in UBJSON format.,to_ubjson}

    @sa http://ubjson.org
    @sa @ref from_ubjson(detail::input_adapter&&, const bool, const bool) for the
        analogous deserialization
    @sa @ref to_cbor(const basic_json& for the related CBOR format
    @sa @ref to_msgpack(const basic_json&) for the related MessagePack format

    @since version 3.1.0
    */
    static std::vector<uint8_t> to_ubjson(const basic_json& j,
                                          const bool use_size = false,
                                          const bool use_type = false)
    {
        std::vector<uint8_t> result;
        to_ubjson(j, result, use_size, use_type);
        return result;
    }

    static void to_ubjson(const basic_json& j, detail::output_adapter<uint8_t> o,
                          const bool use_size = false, const bool use_type = false)
    {
        binary_writer<uint8_t>(o).write_ubjson(j, use_size, use_type);
    }

    static void to_ubjson(const basic_json& j, detail::output_adapter<char> o,
                          const bool use_size = false, const bool use_type = false)
    {
        binary_writer<char>(o).write_ubjson(j, use_size, use_type);
    }


    /*!
    @brief Serializes the given JSON object `j` to BSON and returns a vector
           containing the corresponding BSON-representation.

    BSON (Binary JSON) is a binary format in which zero or more ordered key/value pairs are
    stored as a single entity (a so-called document).

    The library uses the following mapping from JSON values types to BSON types:

    JSON value type | value/range                       | BSON type   | marker
    --------------- | --------------------------------- | ----------- | ------
    null            | `null`                            | null        | 0x0A
    boolean         | `true`, `false`                   | boolean     | 0x08
    number_integer  | -9223372036854775808..-2147483649 | int64       | 0x12
    number_integer  | -2147483648..2147483647           | int32       | 0x10
    number_integer  | 2147483648..9223372036854775807   | int64       | 0x12
    number_unsigned | 0..2147483647                     | int32       | 0x10
    number_unsigned | 2147483648..9223372036854775807   | int64       | 0x12
    number_unsigned | 9223372036854775808..18446744073709551615| --   | --
    number_float    | *any value*                       | double      | 0x01
    string          | *any value*                       | string      | 0x02
    array           | *any value*                       | document    | 0x04
    object          | *any value*                       | document    | 0x03

    @warning The mapping is **incomplete**, since only JSON-objects (and things
    contained therein) can be serialized to BSON.
    Also, integers larger than 9223372036854775807 cannot be serialized to BSON,
    and the keys may not contain U+0000, since they are serialized a
    zero-terminated c-strings.

    @throw out_of_range.407  if `j.is_number_unsigned() && j.get<std::uint64_t>() > 9223372036854775807`
    @throw out_of_range.409  if a key in `j` contains a NULL (U+0000)
    @throw type_error.317    if `!j.is_object()`

    @pre The input `j` is required to be an object: `j.is_object() == true`.

    @note Any BSON output created via @ref to_bson can be successfully parsed
          by @ref from_bson.

    @param[in] j  JSON value to serialize
    @return BSON serialization as byte vector

    @complexity Linear in the size of the JSON value @a j.

    @liveexample{The example shows the serialization of a JSON value to a byte
    vector in BSON format.,to_bson}

    @sa http://bsonspec.org/spec.html
    @sa @ref from_bson(detail::input_adapter&&, const bool strict) for the
        analogous deserialization
    @sa @ref to_ubjson(const basic_json&, const bool, const bool) for the
             related UBJSON format
    @sa @ref to_cbor(const basic_json&) for the related CBOR format
    @sa @ref to_msgpack(const basic_json&) for the related MessagePack format
    */
    static std::vector<uint8_t> to_bson(const basic_json& j)
    {
        std::vector<uint8_t> result;
        to_bson(j, result);
        return result;
    }

    /*!
    @brief Serializes the given JSON object `j` to BSON and forwards the
           corresponding BSON-representation to the given output_adapter `o`.
    @param j The JSON object to convert to BSON.
    @param o The output adapter that receives the binary BSON representation.
    @pre The input `j` shall be an object: `j.is_object() == true`
    @sa @ref to_bson(const basic_json&)
    */
    static void to_bson(const basic_json& j, detail::output_adapter<uint8_t> o)
    {
        binary_writer<uint8_t>(o).write_bson(j);
    }

    /*!
    @copydoc to_bson(const basic_json&, detail::output_adapter<uint8_t>)
    */
    static void to_bson(const basic_json& j, detail::output_adapter<char> o)
    {
        binary_writer<char>(o).write_bson(j);
    }


    /*!
    @brief create a JSON value from an input in CBOR format

    Deserializes a given input @a i to a JSON value using the CBOR (Concise
    Binary Object Representation) serialization format.

    The library maps CBOR types to JSON value types as follows:

    CBOR type              | JSON value type | first byte
    ---------------------- | --------------- | ----------
    Integer                | number_unsigned | 0x00..0x17
    Unsigned integer       | number_unsigned | 0x18
    Unsigned integer       | number_unsigned | 0x19
    Unsigned integer       | number_unsigned | 0x1A
    Unsigned integer       | number_unsigned | 0x1B
    Negative integer       | number_integer  | 0x20..0x37
    Negative integer       | number_integer  | 0x38
    Negative integer       | number_integer  | 0x39
    Negative integer       | number_integer  | 0x3A
    Negative integer       | number_integer  | 0x3B
    Negative integer       | number_integer  | 0x40..0x57
    UTF-8 string           | string          | 0x60..0x77
    UTF-8 string           | string          | 0x78
    UTF-8 string           | string          | 0x79
    UTF-8 string           | string          | 0x7A
    UTF-8 string           | string          | 0x7B
    UTF-8 string           | string          | 0x7F
    array                  | array           | 0x80..0x97
    array                  | array           | 0x98
    array                  | array           | 0x99
    array                  | array           | 0x9A
    array                  | array           | 0x9B
    array                  | array           | 0x9F
    map                    | object          | 0xA0..0xB7
    map                    | object          | 0xB8
    map                    | object          | 0xB9
    map                    | object          | 0xBA
    map                    | object          | 0xBB
    map                    | object          | 0xBF
    False                  | `false`         | 0xF4
    True                   | `true`          | 0xF5
    Null                   | `null`          | 0xF6
    Half-Precision Float   | number_float    | 0xF9
    Single-Precision Float | number_float    | 0xFA
    Double-Precision Float | number_float    | 0xFB

    @warning The mapping is **incomplete** in the sense that not all CBOR
             types can be converted to a JSON value. The following CBOR types
             are not supported and will yield parse errors (parse_error.112):
             - byte strings (0x40..0x5F)
             - date/time (0xC0..0xC1)
             - bignum (0xC2..0xC3)
             - decimal fraction (0xC4)
             - bigfloat (0xC5)
             - tagged items (0xC6..0xD4, 0xD8..0xDB)
             - expected conversions (0xD5..0xD7)
             - simple values (0xE0..0xF3, 0xF8)
             - undefined (0xF7)

    @warning CBOR allows map keys of any type, whereas JSON only allows
             strings as keys in object values. Therefore, CBOR maps with keys
             other than UTF-8 strings are rejected (parse_error.113).

    @note Any CBOR output created @ref to_cbor can be successfully parsed by
          @ref from_cbor.

    @param[in] i  an input in CBOR format convertible to an input adapter
    @param[in] strict  whether to expect the input to be consumed until EOF
                       (true by default)
    @param[in] allow_exceptions  whether to throw exceptions in case of a
    parse error (optional, true by default)

    @return deserialized JSON value; in case of a parse error and
            @a allow_exceptions set to `false`, the return value will be
            value_t::discarded.

    @throw parse_error.110 if the given input ends prematurely or the end of
    file was not reached when @a strict was set to true
    @throw parse_error.112 if unsupported features from CBOR were
    used in the given input @a v or if the input is not valid CBOR
    @throw parse_error.113 if a string was expected as map key, but not found

    @complexity Linear in the size of the input @a i.

    @liveexample{The example shows the deserialization of a byte vector in CBOR
    format to a JSON value.,from_cbor}

    @sa http://cbor.io
    @sa @ref to_cbor(const basic_json&) for the analogous serialization
    @sa @ref from_msgpack(detail::input_adapter&&, const bool, const bool) for the
        related MessagePack format
    @sa @ref from_ubjson(detail::input_adapter&&, const bool, const bool) for the
        related UBJSON format

    @since version 2.0.9; parameter @a start_index since 2.1.1; changed to
           consume input adapters, removed start_index parameter, and added
           @a strict parameter since 3.0.0; added @a allow_exceptions parameter
           since 3.2.0
    */
    JSON_HEDLEY_WARN_UNUSED_RESULT
    static basic_json from_cbor(detail::input_adapter&& i,
                                const bool strict = true,
                                const bool allow_exceptions = true)
    {
        basic_json result;
        detail::json_sax_dom_parser<basic_json> sdp(result, allow_exceptions);
        const bool res = binary_reader(detail::input_adapter(i)).sax_parse(input_format_t::cbor, &sdp, strict);
        return res ? result : basic_json(value_t::discarded);
    }

    /*!
    @copydoc from_cbor(detail::input_adapter&&, const bool, const bool)
    */
    template<typename A1, typename A2,
             detail::enable_if_t<std::is_constructible<detail::input_adapter, A1, A2>::value, int> = 0>
    JSON_HEDLEY_WARN_UNUSED_RESULT
    static basic_json from_cbor(A1 && a1, A2 && a2,
                                const bool strict = true,
                                const bool allow_exceptions = true)
    {
        basic_json result;
        detail::json_sax_dom_parser<basic_json> sdp(result, allow_exceptions);
        const bool res = binary_reader(detail::input_adapter(std::forward<A1>(a1), std::forward<A2>(a2))).sax_parse(input_format_t::cbor, &sdp, strict);
        return res ? result : basic_json(value_t::discarded);
    }

    /*!
    @brief create a JSON value from an input in MessagePack format

    Deserializes a given input @a i to a JSON value using the MessagePack
    serialization format.

    The library maps MessagePack types to JSON value types as follows:

    MessagePack type | JSON value type | first byte
    ---------------- | --------------- | ----------
    positive fixint  | number_unsigned | 0x00..0x7F
    fixmap           | object          | 0x80..0x8F
    fixarray         | array           | 0x90..0x9F
    fixstr           | string          | 0xA0..0xBF
    nil              | `null`          | 0xC0
    false            | `false`         | 0xC2
    true             | `true`          | 0xC3
    float 32         | number_float    | 0xCA
    float 64         | number_float    | 0xCB
    uint 8           | number_unsigned | 0xCC
    uint 16          | number_unsigned | 0xCD
    uint 32          | number_unsigned | 0xCE
    uint 64          | number_unsigned | 0xCF
    int 8            | number_integer  | 0xD0
    int 16           | number_integer  | 0xD1
    int 32           | number_integer  | 0xD2
    int 64           | number_integer  | 0xD3
    str 8            | string          | 0xD9
    str 16           | string          | 0xDA
    str 32           | string          | 0xDB
    array 16         | array           | 0xDC
    array 32         | array           | 0xDD
    map 16           | object          | 0xDE
    map 32           | object          | 0xDF
    negative fixint  | number_integer  | 0xE0-0xFF

    @warning The mapping is **incomplete** in the sense that not all
             MessagePack types can be converted to a JSON value. The following
             MessagePack types are not supported and will yield parse errors:
              - bin 8 - bin 32 (0xC4..0xC6)
              - ext 8 - ext 32 (0xC7..0xC9)
              - fixext 1 - fixext 16 (0xD4..0xD8)

    @note Any MessagePack output created @ref to_msgpack can be successfully
          parsed by @ref from_msgpack.

    @param[in] i  an input in MessagePack format convertible to an input
                  adapter
    @param[in] strict  whether to expect the input to be consumed until EOF
                       (true by default)
    @param[in] allow_exceptions  whether to throw exceptions in case of a
    parse error (optional, true by default)

    @return deserialized JSON value; in case of a parse error and
            @a allow_exceptions set to `false`, the return value will be
            value_t::discarded.

    @throw parse_error.110 if the given input ends prematurely or the end of
    file was not reached when @a strict was set to true
    @throw parse_error.112 if unsupported features from MessagePack were
    used in the given input @a i or if the input is not valid MessagePack
    @throw parse_error.113 if a string was expected as map key, but not found

    @complexity Linear in the size of the input @a i.

    @liveexample{The example shows the deserialization of a byte vector in
    MessagePack format to a JSON value.,from_msgpack}

    @sa http://msgpack.org
    @sa @ref to_msgpack(const basic_json&) for the analogous serialization
    @sa @ref from_cbor(detail::input_adapter&&, const bool, const bool) for the
        related CBOR format
    @sa @ref from_ubjson(detail::input_adapter&&, const bool, const bool) for
        the related UBJSON format
    @sa @ref from_bson(detail::input_adapter&&, const bool, const bool) for
        the related BSON format

    @since version 2.0.9; parameter @a start_index since 2.1.1; changed to
           consume input adapters, removed start_index parameter, and added
           @a strict parameter since 3.0.0; added @a allow_exceptions parameter
           since 3.2.0
    */
    JSON_HEDLEY_WARN_UNUSED_RESULT
    static basic_json from_msgpack(detail::input_adapter&& i,
                                   const bool strict = true,
                                   const bool allow_exceptions = true)
    {
        basic_json result;
        detail::json_sax_dom_parser<basic_json> sdp(result, allow_exceptions);
        const bool res = binary_reader(detail::input_adapter(i)).sax_parse(input_format_t::msgpack, &sdp, strict);
        return res ? result : basic_json(value_t::discarded);
    }

    /*!
    @copydoc from_msgpack(detail::input_adapter&&, const bool, const bool)
    */
    template<typename A1, typename A2,
             detail::enable_if_t<std::is_constructible<detail::input_adapter, A1, A2>::value, int> = 0>
    JSON_HEDLEY_WARN_UNUSED_RESULT
    static basic_json from_msgpack(A1 && a1, A2 && a2,
                                   const bool strict = true,
                                   const bool allow_exceptions = true)
    {
        basic_json result;
        detail::json_sax_dom_parser<basic_json> sdp(result, allow_exceptions);
        const bool res = binary_reader(detail::input_adapter(std::forward<A1>(a1), std::forward<A2>(a2))).sax_parse(input_format_t::msgpack, &sdp, strict);
        return res ? result : basic_json(value_t::discarded);
    }

    /*!
    @brief create a JSON value from an input in UBJSON format

    Deserializes a given input @a i to a JSON value using the UBJSON (Universal
    Binary JSON) serialization format.

    The library maps UBJSON types to JSON value types as follows:

    UBJSON type | JSON value type                         | marker
    ----------- | --------------------------------------- | ------
    no-op       | *no value, next value is read*          | `N`
    null        | `null`                                  | `Z`
    false       | `false`                                 | `F`
    true        | `true`                                  | `T`
    float32     | number_float                            | `d`
    float64     | number_float                            | `D`
    uint8       | number_unsigned                         | `U`
    int8        | number_integer                          | `i`
    int16       | number_integer                          | `I`
    int32       | number_integer                          | `l`
    int64       | number_integer                          | `L`
    string      | string                                  | `S`
    char        | string                                  | `C`
    array       | array (optimized values are supported)  | `[`
    object      | object (optimized values are supported) | `{`

    @note The mapping is **complete** in the sense that any UBJSON value can
          be converted to a JSON value.

    @param[in] i  an input in UBJSON format convertible to an input adapter
    @param[in] strict  whether to expect the input to be consumed until EOF
                       (true by default)
    @param[in] allow_exceptions  whether to throw exceptions in case of a
    parse error (optional, true by default)

    @return deserialized JSON value; in case of a parse error and
            @a allow_exceptions set to `false`, the return value will be
            value_t::discarded.

    @throw parse_error.110 if the given input ends prematurely or the end of
    file was not reached when @a strict was set to true
    @throw parse_error.112 if a parse error occurs
    @throw parse_error.113 if a string could not be parsed successfully

    @complexity Linear in the size of the input @a i.

    @liveexample{The example shows the deserialization of a byte vector in
    UBJSON format to a JSON value.,from_ubjson}

    @sa http://ubjson.org
    @sa @ref to_ubjson(const basic_json&, const bool, const bool) for the
             analogous serialization
    @sa @ref from_cbor(detail::input_adapter&&, const bool, const bool) for the
        related CBOR format
    @sa @ref from_msgpack(detail::input_adapter&&, const bool, const bool) for
        the related MessagePack format
    @sa @ref from_bson(detail::input_adapter&&, const bool, const bool) for
        the related BSON format

    @since version 3.1.0; added @a allow_exceptions parameter since 3.2.0
    */
    JSON_HEDLEY_WARN_UNUSED_RESULT
    static basic_json from_ubjson(detail::input_adapter&& i,
                                  const bool strict = true,
                                  const bool allow_exceptions = true)
    {
        basic_json result;
        detail::json_sax_dom_parser<basic_json> sdp(result, allow_exceptions);
        const bool res = binary_reader(detail::input_adapter(i)).sax_parse(input_format_t::ubjson, &sdp, strict);
        return res ? result : basic_json(value_t::discarded);
    }

    /*!
    @copydoc from_ubjson(detail::input_adapter&&, const bool, const bool)
    */
    template<typename A1, typename A2,
             detail::enable_if_t<std::is_constructible<detail::input_adapter, A1, A2>::value, int> = 0>
    JSON_HEDLEY_WARN_UNUSED_RESULT
    static basic_json from_ubjson(A1 && a1, A2 && a2,
                                  const bool strict = true,
                                  const bool allow_exceptions = true)
    {
        basic_json result;
        detail::json_sax_dom_parser<basic_json> sdp(result, allow_exceptions);
        const bool res = binary_reader(detail::input_adapter(std::forward<A1>(a1), std::forward<A2>(a2))).sax_parse(input_format_t::ubjson, &sdp, strict);
        return res ? result : basic_json(value_t::discarded);
    }

    /*!
    @brief Create a JSON value from an input in BSON format

    Deserializes a given input @a i to a JSON value using the BSON (Binary JSON)
    serialization format.

    The library maps BSON record types to JSON value types as follows:

    BSON type       | BSON marker byte | JSON value type
    --------------- | ---------------- | ---------------------------
    double          | 0x01             | number_float
    string          | 0x02             | string
    document        | 0x03             | object
    array           | 0x04             | array
    binary          | 0x05             | still unsupported
    undefined       | 0x06             | still unsupported
    ObjectId        | 0x07             | still unsupported
    boolean         | 0x08             | boolean
    UTC Date-Time   | 0x09             | still unsupported
    null            | 0x0A             | null
    Regular Expr.   | 0x0B             | still unsupported
    DB Pointer      | 0x0C             | still unsupported
    JavaScript Code | 0x0D             | still unsupported
    Symbol          | 0x0E             | still unsupported
    JavaScript Code | 0x0F             | still unsupported
    int32           | 0x10             | number_integer
    Timestamp       | 0x11             | still unsupported
    128-bit decimal float | 0x13       | still unsupported
    Max Key         | 0x7F             | still unsupported
    Min Key         | 0xFF             | still unsupported

    @warning The mapping is **incomplete**. The unsupported mappings
             are indicated in the table above.

    @param[in] i  an input in BSON format convertible to an input adapter
    @param[in] strict  whether to expect the input to be consumed until EOF
                       (true by default)
    @param[in] allow_exceptions  whether to throw exceptions in case of a
    parse error (optional, true by default)

    @return deserialized JSON value; in case of a parse error and
            @a allow_exceptions set to `false`, the return value will be
            value_t::discarded.

    @throw parse_error.114 if an unsupported BSON record type is encountered

    @complexity Linear in the size of the input @a i.

    @liveexample{The example shows the deserialization of a byte vector in
    BSON format to a JSON value.,from_bson}

    @sa http://bsonspec.org/spec.html
    @sa @ref to_bson(const basic_json&) for the analogous serialization
    @sa @ref from_cbor(detail::input_adapter&&, const bool, const bool) for the
        related CBOR format
    @sa @ref from_msgpack(detail::input_adapter&&, const bool, const bool) for
        the related MessagePack format
    @sa @ref from_ubjson(detail::input_adapter&&, const bool, const bool) for the
        related UBJSON format
    */
    JSON_HEDLEY_WARN_UNUSED_RESULT
    static basic_json from_bson(detail::input_adapter&& i,
                                const bool strict = true,
                                const bool allow_exceptions = true)
    {
        basic_json result;
        detail::json_sax_dom_parser<basic_json> sdp(result, allow_exceptions);
        const bool res = binary_reader(detail::input_adapter(i)).sax_parse(input_format_t::bson, &sdp, strict);
        return res ? result : basic_json(value_t::discarded);
    }

    /*!
    @copydoc from_bson(detail::input_adapter&&, const bool, const bool)
    */
    template<typename A1, typename A2,
             detail::enable_if_t<std::is_constructible<detail::input_adapter, A1, A2>::value, int> = 0>
    JSON_HEDLEY_WARN_UNUSED_RESULT
    static basic_json from_bson(A1 && a1, A2 && a2,
                                const bool strict = true,
                                const bool allow_exceptions = true)
    {
        basic_json result;
        detail::json_sax_dom_parser<basic_json> sdp(result, allow_exceptions);
        const bool res = binary_reader(detail::input_adapter(std::forward<A1>(a1), std::forward<A2>(a2))).sax_parse(input_format_t::bson, &sdp, strict);
        return res ? result : basic_json(value_t::discarded);
    }



    /// @}

    //////////////////////////
    // JSON Pointer support //
    //////////////////////////

    /// @name JSON Pointer functions
    /// @{

    /*!
    @brief access specified element via JSON Pointer

    Uses a JSON pointer to retrieve a reference to the respective JSON value.
    No bound checking is performed. Similar to @ref operator[](const typename
    object_t::key_type&), `null` values are created in arrays and objects if
    necessary.

    In particular:
    - If the JSON pointer points to an object key that does not exist, it
      is created an filled with a `null` value before a reference to it
      is returned.
    - If the JSON pointer points to an array index that does not exist, it
      is created an filled with a `null` value before a reference to it
      is returned. All indices between the current maximum and the given
      index are also filled with `null`.
    - The special value `-` is treated as a synonym for the index past the
      end.

    @param[in] ptr  a JSON pointer

    @return reference to the element pointed to by @a ptr

    @complexity Constant.

    @throw parse_error.106   if an array index begins with '0'
    @throw parse_error.109   if an array index was not a number
    @throw out_of_range.404  if the JSON pointer can not be resolved

    @liveexample{The behavior is shown in the example.,operatorjson_pointer}

    @since version 2.0.0
    */
    reference operator[](const json_pointer& ptr)
    {
        return ptr.get_unchecked(this);
    }

    /*!
    @brief access specified element via JSON Pointer

    Uses a JSON pointer to retrieve a reference to the respective JSON value.
    No bound checking is performed. The function does not change the JSON
    value; no `null` values are created. In particular, the the special value
    `-` yields an exception.

    @param[in] ptr  JSON pointer to the desired element

    @return const reference to the element pointed to by @a ptr

    @complexity Constant.

    @throw parse_error.106   if an array index begins with '0'
    @throw parse_error.109   if an array index was not a number
    @throw out_of_range.402  if the array index '-' is used
    @throw out_of_range.404  if the JSON pointer can not be resolved

    @liveexample{The behavior is shown in the example.,operatorjson_pointer_const}

    @since version 2.0.0
    */
    const_reference operator[](const json_pointer& ptr) const
    {
        return ptr.get_unchecked(this);
    }

    /*!
    @brief access specified element via JSON Pointer

    Returns a reference to the element at with specified JSON pointer @a ptr,
    with bounds checking.

    @param[in] ptr  JSON pointer to the desired element

    @return reference to the element pointed to by @a ptr

    @throw parse_error.106 if an array index in the passed JSON pointer @a ptr
    begins with '0'. See example below.

    @throw parse_error.109 if an array index in the passed JSON pointer @a ptr
    is not a number. See example below.

    @throw out_of_range.401 if an array index in the passed JSON pointer @a ptr
    is out of range. See example below.

    @throw out_of_range.402 if the array index '-' is used in the passed JSON
    pointer @a ptr. As `at` provides checked access (and no elements are
    implicitly inserted), the index '-' is always invalid. See example below.

    @throw out_of_range.403 if the JSON pointer describes a key of an object
    which cannot be found. See example below.

    @throw out_of_range.404 if the JSON pointer @a ptr can not be resolved.
    See example below.

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes in the JSON value.

    @complexity Constant.

    @since version 2.0.0

    @liveexample{The behavior is shown in the example.,at_json_pointer}
    */
    reference at(const json_pointer& ptr)
    {
        return ptr.get_checked(this);
    }

    /*!
    @brief access specified element via JSON Pointer

    Returns a const reference to the element at with specified JSON pointer @a
    ptr, with bounds checking.

    @param[in] ptr  JSON pointer to the desired element

    @return reference to the element pointed to by @a ptr

    @throw parse_error.106 if an array index in the passed JSON pointer @a ptr
    begins with '0'. See example below.

    @throw parse_error.109 if an array index in the passed JSON pointer @a ptr
    is not a number. See example below.

    @throw out_of_range.401 if an array index in the passed JSON pointer @a ptr
    is out of range. See example below.

    @throw out_of_range.402 if the array index '-' is used in the passed JSON
    pointer @a ptr. As `at` provides checked access (and no elements are
    implicitly inserted), the index '-' is always invalid. See example below.

    @throw out_of_range.403 if the JSON pointer describes a key of an object
    which cannot be found. See example below.

    @throw out_of_range.404 if the JSON pointer @a ptr can not be resolved.
    See example below.

    @exceptionsafety Strong guarantee: if an exception is thrown, there are no
    changes in the JSON value.

    @complexity Constant.

    @since version 2.0.0

    @liveexample{The behavior is shown in the example.,at_json_pointer_const}
    */
    const_reference at(const json_pointer& ptr) const
    {
        return ptr.get_checked(this);
    }

    /*!
    @brief return flattened JSON value

    The function creates a JSON object whose keys are JSON pointers (see [RFC
    6901](https://tools.ietf.org/html/rfc6901)) and whose values are all
    primitive. The original JSON value can be restored using the @ref
    unflatten() function.

    @return an object that maps JSON pointers to primitive values

    @note Empty objects and arrays are flattened to `null` and will not be
          reconstructed correctly by the @ref unflatten() function.

    @complexity Linear in the size the JSON value.

    @liveexample{The following code shows how a JSON object is flattened to an
    object whose keys consist of JSON pointers.,flatten}

    @sa @ref unflatten() for the reverse function

    @since version 2.0.0
    */
    basic_json flatten() const
    {
        basic_json result(value_t::object);
        json_pointer::flatten("", *this, result);
        return result;
    }

    /*!
    @brief unflatten a previously flattened JSON value

    The function restores the arbitrary nesting of a JSON value that has been
    flattened before using the @ref flatten() function. The JSON value must
    meet certain constraints:
    1. The value must be an object.
    2. The keys must be JSON pointers (see
       [RFC 6901](https://tools.ietf.org/html/rfc6901))
    3. The mapped values must be primitive JSON types.

    @return the original JSON from a flattened version

    @note Empty objects and arrays are flattened by @ref flatten() to `null`
          values and can not unflattened to their original type. Apart from
          this example, for a JSON value `j`, the following is always true:
          `j == j.flatten().unflatten()`.

    @complexity Linear in the size the JSON value.

    @throw type_error.314  if value is not an object
    @throw type_error.315  if object values are not primitive

    @liveexample{The following code shows how a flattened JSON object is
    unflattened into the original nested JSON object.,unflatten}

    @sa @ref flatten() for the reverse function

    @since version 2.0.0
    */
    basic_json unflatten() const
    {
        return json_pointer::unflatten(*this);
    }

    /// @}

    //////////////////////////
    // JSON Patch functions //
    //////////////////////////

    /// @name JSON Patch functions
    /// @{

    /*!
    @brief applies a JSON patch

    [JSON Patch](http://jsonpatch.com) defines a JSON document structure for
    expressing a sequence of operations to apply to a JSON) document. With
    this function, a JSON Patch is applied to the current JSON value by
    executing all operations from the patch.

    @param[in] json_patch  JSON patch document
    @return patched document

    @note The application of a patch is atomic: Either all operations succeed
          and the patched document is returned or an exception is thrown. In
          any case, the original value is not changed: the patch is applied
          to a copy of the value.

    @throw parse_error.104 if the JSON patch does not consist of an array of
    objects

    @throw parse_error.105 if the JSON patch is malformed (e.g., mandatory
    attributes are missing); example: `"operation add must have member path"`

    @throw out_of_range.401 if an array index is out of range.

    @throw out_of_range.403 if a JSON pointer inside the patch could not be
    resolved successfully in the current JSON value; example: `"key baz not
    found"`

    @throw out_of_range.405 if JSON pointer has no parent ("add", "remove",
    "move")

    @throw other_error.501 if "test" operation was unsuccessful

    @complexity Linear in the size of the JSON value and the length of the
    JSON patch. As usually only a fraction of the JSON value is affected by
    the patch, the complexity can usually be neglected.

    @liveexample{The following code shows how a JSON patch is applied to a
    value.,patch}

    @sa @ref diff -- create a JSON patch by comparing two JSON values

    @sa [RFC 6902 (JSON Patch)](https://tools.ietf.org/html/rfc6902)
    @sa [RFC 6901 (JSON Pointer)](https://tools.ietf.org/html/rfc6901)

    @since version 2.0.0
    */
    basic_json patch(const basic_json& json_patch) const
    {
        // make a working copy to apply the patch to
        basic_json result = *this;

        // the valid JSON Patch operations
        enum class patch_operations {add, remove, replace, move, copy, test, invalid};

        const auto get_op = [](const std::string & op)
        {
            if (op == "add")
            {
                return patch_operations::add;
            }
            if (op == "remove")
            {
                return patch_operations::remove;
            }
            if (op == "replace")
            {
                return patch_operations::replace;
            }
            if (op == "move")
            {
                return patch_operations::move;
            }
            if (op == "copy")
            {
                return patch_operations::copy;
            }
            if (op == "test")
            {
                return patch_operations::test;
            }

            return patch_operations::invalid;
        };

        // wrapper for "add" operation; add value at ptr
        const auto operation_add = [&result](json_pointer & ptr, basic_json val)
        {
            // adding to the root of the target document means replacing it
            if (ptr.empty())
            {
                result = val;
                return;
            }

            // make sure the top element of the pointer exists
            json_pointer top_pointer = ptr.top();
            if (top_pointer != ptr)
            {
                result.at(top_pointer);
            }

            // get reference to parent of JSON pointer ptr
            const auto last_path = ptr.back();
            ptr.pop_back();
            basic_json& parent = result[ptr];

            switch (parent.m_type)
            {
                case value_t::null:
                case value_t::object:
                {
                    // use operator[] to add value
                    parent[last_path] = val;
                    break;
                }

                case value_t::array:
                {
                    if (last_path == "-")
                    {
                        // special case: append to back
                        parent.push_back(val);
                    }
                    else
                    {
                        const auto idx = json_pointer::array_index(last_path);
                        if (JSON_HEDLEY_UNLIKELY(static_cast<size_type>(idx) > parent.size()))
                        {
                            // avoid undefined behavior
                            JSON_THROW(out_of_range::create(401, "array index " + std::to_string(idx) + " is out of range"));
                        }

                        // default case: insert add offset
                        parent.insert(parent.begin() + static_cast<difference_type>(idx), val);
                    }
                    break;
                }

                // if there exists a parent it cannot be primitive
                default:            // LCOV_EXCL_LINE
                    assert(false);  // LCOV_EXCL_LINE
            }
        };

        // wrapper for "remove" operation; remove value at ptr
        const auto operation_remove = [&result](json_pointer & ptr)
        {
            // get reference to parent of JSON pointer ptr
            const auto last_path = ptr.back();
            ptr.pop_back();
            basic_json& parent = result.at(ptr);

            // remove child
            if (parent.is_object())
            {
                // perform range check
                auto it = parent.find(last_path);
                if (JSON_HEDLEY_LIKELY(it != parent.end()))
                {
                    parent.erase(it);
                }
                else
                {
                    JSON_THROW(out_of_range::create(403, "key '" + last_path + "' not found"));
                }
            }
            else if (parent.is_array())
            {
                // note erase performs range check
                parent.erase(static_cast<size_type>(json_pointer::array_index(last_path)));
            }
        };

        // type check: top level value must be an array
        if (JSON_HEDLEY_UNLIKELY(not json_patch.is_array()))
        {
            JSON_THROW(parse_error::create(104, 0, "JSON patch must be an array of objects"));
        }

        // iterate and apply the operations
        for (const auto& val : json_patch)
        {
            // wrapper to get a value for an operation
            const auto get_value = [&val](const std::string & op,
                                          const std::string & member,
                                          bool string_type) -> basic_json &
            {
                // find value
                auto it = val.m_value.object->find(member);

                // context-sensitive error message
                const auto error_msg = (op == "op") ? "operation" : "operation '" + op + "'";

                // check if desired value is present
                if (JSON_HEDLEY_UNLIKELY(it == val.m_value.object->end()))
                {
                    JSON_THROW(parse_error::create(105, 0, error_msg + " must have member '" + member + "'"));
                }

                // check if result is of type string
                if (JSON_HEDLEY_UNLIKELY(string_type and not it->second.is_string()))
                {
                    JSON_THROW(parse_error::create(105, 0, error_msg + " must have string member '" + member + "'"));
                }

                // no error: return value
                return it->second;
            };

            // type check: every element of the array must be an object
            if (JSON_HEDLEY_UNLIKELY(not val.is_object()))
            {
                JSON_THROW(parse_error::create(104, 0, "JSON patch must be an array of objects"));
            }

            // collect mandatory members
            const std::string op = get_value("op", "op", true);
            const std::string path = get_value(op, "path", true);
            json_pointer ptr(path);

            switch (get_op(op))
            {
                case patch_operations::add:
                {
                    operation_add(ptr, get_value("add", "value", false));
                    break;
                }

                case patch_operations::remove:
                {
                    operation_remove(ptr);
                    break;
                }

                case patch_operations::replace:
                {
                    // the "path" location must exist - use at()
                    result.at(ptr) = get_value("replace", "value", false);
                    break;
                }

                case patch_operations::move:
                {
                    const std::string from_path = get_value("move", "from", true);
                    json_pointer from_ptr(from_path);

                    // the "from" location must exist - use at()
                    basic_json v = result.at(from_ptr);

                    // The move operation is functionally identical to a
                    // "remove" operation on the "from" location, followed
                    // immediately by an "add" operation at the target
                    // location with the value that was just removed.
                    operation_remove(from_ptr);
                    operation_add(ptr, v);
                    break;
                }

                case patch_operations::copy:
                {
                    const std::string from_path = get_value("copy", "from", true);
                    const json_pointer from_ptr(from_path);

                    // the "from" location must exist - use at()
                    basic_json v = result.at(from_ptr);

                    // The copy is functionally identical to an "add"
                    // operation at the target location using the value
                    // specified in the "from" member.
                    operation_add(ptr, v);
                    break;
                }

                case patch_operations::test:
                {
                    bool success = false;
                    JSON_TRY
                    {
                        // check if "value" matches the one at "path"
                        // the "path" location must exist - use at()
                        success = (result.at(ptr) == get_value("test", "value", false));
                    }
                    JSON_INTERNAL_CATCH (out_of_range&)
                    {
                        // ignore out of range errors: success remains false
                    }

                    // throw an exception if test fails
                    if (JSON_HEDLEY_UNLIKELY(not success))
                    {
                        JSON_THROW(other_error::create(501, "unsuccessful: " + val.dump()));
                    }

                    break;
                }

                default:
                {
                    // op must be "add", "remove", "replace", "move", "copy", or
                    // "test"
                    JSON_THROW(parse_error::create(105, 0, "operation value '" + op + "' is invalid"));
                }
            }
        }

        return result;
    }

    /*!
    @brief creates a diff as a JSON patch

    Creates a [JSON Patch](http://jsonpatch.com) so that value @a source can
    be changed into the value @a target by calling @ref patch function.

    @invariant For two JSON values @a source and @a target, the following code
    yields always `true`:
    @code {.cpp}
    source.patch(diff(source, target)) == target;
    @endcode

    @note Currently, only `remove`, `add`, and `replace` operations are
          generated.

    @param[in] source  JSON value to compare from
    @param[in] target  JSON value to compare against
    @param[in] path    helper value to create JSON pointers

    @return a JSON patch to convert the @a source to @a target

    @complexity Linear in the lengths of @a source and @a target.

    @liveexample{The following code shows how a JSON patch is created as a
    diff for two JSON values.,diff}

    @sa @ref patch -- apply a JSON patch
    @sa @ref merge_patch -- apply a JSON Merge Patch

    @sa [RFC 6902 (JSON Patch)](https://tools.ietf.org/html/rfc6902)

    @since version 2.0.0
    */
    JSON_HEDLEY_WARN_UNUSED_RESULT
    static basic_json diff(const basic_json& source, const basic_json& target,
                           const std::string& path = "")
    {
        // the patch
        basic_json result(value_t::array);

        // if the values are the same, return empty patch
        if (source == target)
        {
            return result;
        }

        if (source.type() != target.type())
        {
            // different types: replace value
            result.push_back(
            {
                {"op", "replace"}, {"path", path}, {"value", target}
            });
            return result;
        }

        switch (source.type())
        {
            case value_t::array:
            {
                // first pass: traverse common elements
                std::size_t i = 0;
                while (i < source.size() and i < target.size())
                {
                    // recursive call to compare array values at index i
                    auto temp_diff = diff(source[i], target[i], path + "/" + std::to_string(i));
                    result.insert(result.end(), temp_diff.begin(), temp_diff.end());
                    ++i;
                }

                // i now reached the end of at least one array
                // in a second pass, traverse the remaining elements

                // remove my remaining elements
                const auto end_index = static_cast<difference_type>(result.size());
                while (i < source.size())
                {
                    // add operations in reverse order to avoid invalid
                    // indices
                    result.insert(result.begin() + end_index, object(
                    {
                        {"op", "remove"},
                        {"path", path + "/" + std::to_string(i)}
                    }));
                    ++i;
                }

                // add other remaining elements
                while (i < target.size())
                {
                    result.push_back(
                    {
                        {"op", "add"},
                        {"path", path + "/" + std::to_string(i)},
                        {"value", target[i]}
                    });
                    ++i;
                }

                break;
            }

            case value_t::object:
            {
                // first pass: traverse this object's elements
                for (auto it = source.cbegin(); it != source.cend(); ++it)
                {
                    // escape the key name to be used in a JSON patch
                    const auto key = json_pointer::escape(it.key());

                    if (target.find(it.key()) != target.end())
                    {
                        // recursive call to compare object values at key it
                        auto temp_diff = diff(it.value(), target[it.key()], path + "/" + key);
                        result.insert(result.end(), temp_diff.begin(), temp_diff.end());
                    }
                    else
                    {
                        // found a key that is not in o -> remove it
                        result.push_back(object(
                        {
                            {"op", "remove"}, {"path", path + "/" + key}
                        }));
                    }
                }

                // second pass: traverse other object's elements
                for (auto it = target.cbegin(); it != target.cend(); ++it)
                {
                    if (source.find(it.key()) == source.end())
                    {
                        // found a key that is not in this -> add it
                        const auto key = json_pointer::escape(it.key());
                        result.push_back(
                        {
                            {"op", "add"}, {"path", path + "/" + key},
                            {"value", it.value()}
                        });
                    }
                }

                break;
            }

            default:
            {
                // both primitive type: replace value
                result.push_back(
                {
                    {"op", "replace"}, {"path", path}, {"value", target}
                });
                break;
            }
        }

        return result;
    }

    /// @}

    ////////////////////////////////
    // JSON Merge Patch functions //
    ////////////////////////////////

    /// @name JSON Merge Patch functions
    /// @{

    /*!
    @brief applies a JSON Merge Patch

    The merge patch format is primarily intended for use with the HTTP PATCH
    method as a means of describing a set of modifications to a target
    resource's content. This function applies a merge patch to the current
    JSON value.

    The function implements the following algorithm from Section 2 of
    [RFC 7396 (JSON Merge Patch)](https://tools.ietf.org/html/rfc7396):

    ```
    define MergePatch(Target, Patch):
      if Patch is an Object:
        if Target is not an Object:
          Target = {} // Ignore the contents and set it to an empty Object
        for each Name/Value pair in Patch:
          if Value is null:
            if Name exists in Target:
              remove the Name/Value pair from Target
          else:
            Target[Name] = MergePatch(Target[Name], Value)
        return Target
      else:
        return Patch
    ```

    Thereby, `Target` is the current object; that is, the patch is applied to
    the current value.

    @param[in] apply_patch  the patch to apply

    @complexity Linear in the lengths of @a patch.

    @liveexample{The following code shows how a JSON Merge Patch is applied to
    a JSON document.,merge_patch}

    @sa @ref patch -- apply a JSON patch
    @sa [RFC 7396 (JSON Merge Patch)](https://tools.ietf.org/html/rfc7396)

    @since version 3.0.0
    */
    void merge_patch(const basic_json& apply_patch)
    {
        if (apply_patch.is_object())
        {
            if (not is_object())
            {
                *this = object();
            }
            for (auto it = apply_patch.begin(); it != apply_patch.end(); ++it)
            {
                if (it.value().is_null())
                {
                    erase(it.key());
                }
                else
                {
                    operator[](it.key()).merge_patch(it.value());
                }
            }
        }
        else
        {
            *this = apply_patch;
        }
    }

    /// @}
};

/*!
@brief user-defined to_string function for JSON values

This function implements a user-defined to_string  for JSON objects.

@param[in] j  a JSON object
@return a std::string object
*/

NLOHMANN_BASIC_JSON_TPL_DECLARATION
std::string to_string(const NLOHMANN_BASIC_JSON_TPL& j)
{
    return j.dump();
}
} // namespace nlohmann

///////////////////////
// nonmember support //
///////////////////////

// specialization of std::swap, and std::hash
namespace std
{

/// hash value for JSON objects
template<>
struct hash<nlohmann::json>
{
    /*!
    @brief return a hash value for a JSON object

    @since version 1.0.0
    */
    std::size_t operator()(const nlohmann::json& j) const
    {
        // a naive hashing via the string representation
        const auto& h = hash<nlohmann::json::string_t>();
        return h(j.dump());
    }
};

/// specialization for std::less<value_t>
/// @note: do not remove the space after '<',
///        see https://github.com/nlohmann/json/pull/679
template<>
struct less<::nlohmann::detail::value_t>
{
    /*!
    @brief compare two value_t enum values
    @since version 3.0.0
    */
    bool operator()(nlohmann::detail::value_t lhs,
                    nlohmann::detail::value_t rhs) const noexcept
    {
        return nlohmann::detail::operator<(lhs, rhs);
    }
};

/*!
@brief exchanges the values of two JSON objects

@since version 1.0.0
*/
template<>
inline void swap<nlohmann::json>(nlohmann::json& j1, nlohmann::json& j2) noexcept(
    is_nothrow_move_constructible<nlohmann::json>::value and
    is_nothrow_move_assignable<nlohmann::json>::value
)
{
    j1.swap(j2);
}

} // namespace std

/*!
@brief user-defined string literal for JSON values

This operator implements a user-defined string literal for JSON objects. It
can be used by adding `"_json"` to a string literal and returns a JSON object
if no parse error occurred.

@param[in] s  a string representation of a JSON object
@param[in] n  the length of string @a s
@return a JSON object

@since version 1.0.0
*/
JSON_HEDLEY_NON_NULL(1)
inline nlohmann::json operator "" _json(const char* s, std::size_t n)
{
    return nlohmann::json::parse(s, s + n);
}

/*!
@brief user-defined string literal for JSON pointer

This operator implements a user-defined string literal for JSON Pointers. It
can be used by adding `"_json_pointer"` to a string literal and returns a JSON pointer
object if no parse error occurred.

@param[in] s  a string representation of a JSON Pointer
@param[in] n  the length of string @a s
@return a JSON pointer object

@since version 2.0.0
*/
JSON_HEDLEY_NON_NULL(1)
inline nlohmann::json::json_pointer operator "" _json_pointer(const char* s, std::size_t n)
{
    return nlohmann::json::json_pointer(std::string(s, n));
}

// #include <nlohmann/detail/macro_unscope.hpp>


// restore GCC/clang diagnostic settings
#if defined(__clang__) || defined(__GNUC__) || defined(__GNUG__)
    #pragma GCC diagnostic pop
#endif
#if defined(__clang__)
    #pragma GCC diagnostic pop
#endif

// clean up
#undef JSON_INTERNAL_CATCH
#undef JSON_CATCH
#undef JSON_THROW
#undef JSON_TRY
#undef JSON_HAS_CPP_14
#undef JSON_HAS_CPP_17
#undef NLOHMANN_BASIC_JSON_TPL_DECLARATION
#undef NLOHMANN_BASIC_JSON_TPL

// #include <nlohmann/thirdparty/hedley/hedley_undef.hpp>
#undef JSON_HEDLEY_ALWAYS_INLINE
#undef JSON_HEDLEY_ARM_VERSION
#undef JSON_HEDLEY_ARM_VERSION_CHECK
#undef JSON_HEDLEY_ARRAY_PARAM
#undef JSON_HEDLEY_ASSUME
#undef JSON_HEDLEY_BEGIN_C_DECLS
#undef JSON_HEDLEY_C_DECL
#undef JSON_HEDLEY_CLANG_HAS_ATTRIBUTE
#undef JSON_HEDLEY_CLANG_HAS_BUILTIN
#undef JSON_HEDLEY_CLANG_HAS_CPP_ATTRIBUTE
#undef JSON_HEDLEY_CLANG_HAS_DECLSPEC_DECLSPEC_ATTRIBUTE
#undef JSON_HEDLEY_CLANG_HAS_EXTENSION
#undef JSON_HEDLEY_CLANG_HAS_FEATURE
#undef JSON_HEDLEY_CLANG_HAS_WARNING
#undef JSON_HEDLEY_COMPCERT_VERSION
#undef JSON_HEDLEY_COMPCERT_VERSION_CHECK
#undef JSON_HEDLEY_CONCAT
#undef JSON_HEDLEY_CONCAT_EX
#undef JSON_HEDLEY_CONST
#undef JSON_HEDLEY_CONST_CAST
#undef JSON_HEDLEY_CONSTEXPR
#undef JSON_HEDLEY_CPP_CAST
#undef JSON_HEDLEY_CRAY_VERSION
#undef JSON_HEDLEY_CRAY_VERSION_CHECK
#undef JSON_HEDLEY_DEPRECATED
#undef JSON_HEDLEY_DEPRECATED_FOR
#undef JSON_HEDLEY_DIAGNOSTIC_DISABLE_CAST_QUAL
#undef JSON_HEDLEY_DIAGNOSTIC_DISABLE_CPP98_COMPAT_WRAP_
#undef JSON_HEDLEY_DIAGNOSTIC_DISABLE_DEPRECATED
#undef JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_CPP_ATTRIBUTES
#undef JSON_HEDLEY_DIAGNOSTIC_DISABLE_UNKNOWN_PRAGMAS
#undef JSON_HEDLEY_DIAGNOSTIC_POP
#undef JSON_HEDLEY_DIAGNOSTIC_PUSH
#undef JSON_HEDLEY_DMC_VERSION
#undef JSON_HEDLEY_DMC_VERSION_CHECK
#undef JSON_HEDLEY_EMPTY_BASES
#undef JSON_HEDLEY_EMSCRIPTEN_VERSION
#undef JSON_HEDLEY_EMSCRIPTEN_VERSION_CHECK
#undef JSON_HEDLEY_END_C_DECLS
#undef JSON_HEDLEY_FALL_THROUGH
#undef JSON_HEDLEY_FLAGS
#undef JSON_HEDLEY_FLAGS_CAST
#undef JSON_HEDLEY_GCC_HAS_ATTRIBUTE
#undef JSON_HEDLEY_GCC_HAS_BUILTIN
#undef JSON_HEDLEY_GCC_HAS_CPP_ATTRIBUTE
#undef JSON_HEDLEY_GCC_HAS_DECLSPEC_ATTRIBUTE
#undef JSON_HEDLEY_GCC_HAS_EXTENSION
#undef JSON_HEDLEY_GCC_HAS_FEATURE
#undef JSON_HEDLEY_GCC_HAS_WARNING
#undef JSON_HEDLEY_GCC_NOT_CLANG_VERSION_CHECK
#undef JSON_HEDLEY_GCC_VERSION
#undef JSON_HEDLEY_GCC_VERSION_CHECK
#undef JSON_HEDLEY_GNUC_HAS_ATTRIBUTE
#undef JSON_HEDLEY_GNUC_HAS_BUILTIN
#undef JSON_HEDLEY_GNUC_HAS_CPP_ATTRIBUTE
#undef JSON_HEDLEY_GNUC_HAS_DECLSPEC_ATTRIBUTE
#undef JSON_HEDLEY_GNUC_HAS_EXTENSION
#undef JSON_HEDLEY_GNUC_HAS_FEATURE
#undef JSON_HEDLEY_GNUC_HAS_WARNING
#undef JSON_HEDLEY_GNUC_VERSION
#undef JSON_HEDLEY_GNUC_VERSION_CHECK
#undef JSON_HEDLEY_HAS_ATTRIBUTE
#undef JSON_HEDLEY_HAS_BUILTIN
#undef JSON_HEDLEY_HAS_CPP_ATTRIBUTE
#undef JSON_HEDLEY_HAS_CPP_ATTRIBUTE_NS
#undef JSON_HEDLEY_HAS_DECLSPEC_ATTRIBUTE
#undef JSON_HEDLEY_HAS_EXTENSION
#undef JSON_HEDLEY_HAS_FEATURE
#undef JSON_HEDLEY_HAS_WARNING
#undef JSON_HEDLEY_IAR_VERSION
#undef JSON_HEDLEY_IAR_VERSION_CHECK
#undef JSON_HEDLEY_IBM_VERSION
#undef JSON_HEDLEY_IBM_VERSION_CHECK
#undef JSON_HEDLEY_IMPORT
#undef JSON_HEDLEY_INLINE
#undef JSON_HEDLEY_INTEL_VERSION
#undef JSON_HEDLEY_INTEL_VERSION_CHECK
#undef JSON_HEDLEY_IS_CONSTANT
#undef JSON_HEDLEY_IS_CONSTEXPR_
#undef JSON_HEDLEY_LIKELY
#undef JSON_HEDLEY_MALLOC
#undef JSON_HEDLEY_MESSAGE
#undef JSON_HEDLEY_MSVC_VERSION
#undef JSON_HEDLEY_MSVC_VERSION_CHECK
#undef JSON_HEDLEY_NEVER_INLINE
#undef JSON_HEDLEY_NO_ESCAPE
#undef JSON_HEDLEY_NON_NULL
#undef JSON_HEDLEY_NO_RETURN
#undef JSON_HEDLEY_NO_THROW
#undef JSON_HEDLEY_NULL
#undef JSON_HEDLEY_PELLES_VERSION
#undef JSON_HEDLEY_PELLES_VERSION_CHECK
#undef JSON_HEDLEY_PGI_VERSION
#undef JSON_HEDLEY_PGI_VERSION_CHECK
#undef JSON_HEDLEY_PREDICT
#undef JSON_HEDLEY_PRINTF_FORMAT
#undef JSON_HEDLEY_PRIVATE
#undef JSON_HEDLEY_PUBLIC
#undef JSON_HEDLEY_PURE
#undef JSON_HEDLEY_REINTERPRET_CAST
#undef JSON_HEDLEY_REQUIRE
#undef JSON_HEDLEY_REQUIRE_CONSTEXPR
#undef JSON_HEDLEY_REQUIRE_MSG
#undef JSON_HEDLEY_RESTRICT
#undef JSON_HEDLEY_RETURNS_NON_NULL
#undef JSON_HEDLEY_SENTINEL
#undef JSON_HEDLEY_STATIC_ASSERT
#undef JSON_HEDLEY_STATIC_CAST
#undef JSON_HEDLEY_STRINGIFY
#undef JSON_HEDLEY_STRINGIFY_EX
#undef JSON_HEDLEY_SUNPRO_VERSION
#undef JSON_HEDLEY_SUNPRO_VERSION_CHECK
#undef JSON_HEDLEY_TINYC_VERSION
#undef JSON_HEDLEY_TINYC_VERSION_CHECK
#undef JSON_HEDLEY_TI_VERSION
#undef JSON_HEDLEY_TI_VERSION_CHECK
#undef JSON_HEDLEY_UNAVAILABLE
#undef JSON_HEDLEY_UNLIKELY
#undef JSON_HEDLEY_UNPREDICTABLE
#undef JSON_HEDLEY_UNREACHABLE
#undef JSON_HEDLEY_UNREACHABLE_RETURN
#undef JSON_HEDLEY_VERSION
#undef JSON_HEDLEY_VERSION_DECODE_MAJOR
#undef JSON_HEDLEY_VERSION_DECODE_MINOR
#undef JSON_HEDLEY_VERSION_DECODE_REVISION
#undef JSON_HEDLEY_VERSION_ENCODE
#undef JSON_HEDLEY_WARNING
#undef JSON_HEDLEY_WARN_UNUSED_RESULT



#endif  // INCLUDE_NLOHMANN_JSON_HPP_

```

`third_party/patches/cutlass/include/cutlass/conv/threadblock/depthwise_fprop_pipelined.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- depthwise_fprop_pipelined.h
+++ depthwise_fprop_pipelined.h
@@ -45,6 +45,8 @@
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/threadblock/mma_base.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -218,7 +220,7 @@ public:
     ++this->smem_iterator_A_;
     ++this->smem_iterator_B_;
 
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     // Pair of fragments used to overlap shared memory loads and math instructions
     WarpFragmentA warp_frag_A[2];
@@ -272,7 +274,7 @@ public:
 
           this->smem_iterator_B_.store(transform_B(tb_frag_B));
 
-          __syncthreads();
+          ark::sync_warps<Base::WarpCount::kCount * 32>();
           
           if(rs_plane_idx == gemm_k_iterations_per_channel - 1){
             // Move to next set of filter groups.

```

`third_party/patches/cutlass/include/cutlass/conv/threadblock/implicit_gemm_fprop_fusion_multistage.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- implicit_gemm_fprop_fusion_multistage.h
+++ implicit_gemm_fprop_fusion_multistage.h
@@ -67,6 +67,8 @@
 #include "cutlass/gemm/warp/scale_bias_tile_iterator.h"
 #include "cutlass/conv/warp/scale_bias_relu_transform.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -616,7 +618,7 @@ public:
 
     // Waits until kStages-2 stages have committed. 
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
@@ -740,7 +742,7 @@ public:
 
           // Waits until kStages-2 stages of cp.async have committed
           arch::cp_async_wait<Base::kStages - 2>();
-          __syncthreads();
+          ark::sync_warps<Base::WarpCount::kCount * 32>();
 
           // Move to the next stage
           iterator_A.advance();
@@ -788,7 +790,7 @@ public:
     // Insert fence and wait for all outstanding cp.async operations to commit.
     cutlass::arch::cp_async_fence();
     cutlass::arch::cp_async_wait<0>();
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
   }
 };

```

`third_party/patches/cutlass/include/cutlass/conv/threadblock/implicit_gemm_multistage.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- implicit_gemm_multistage.h
+++ implicit_gemm_multistage.h
@@ -44,6 +44,8 @@
 #include "cutlass/arch/cache_operation.h"
 #include "cutlass/gemm/threadblock/mma_base.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -351,7 +353,7 @@ public:
 
     // Waits until kStages-2 stages have committed. 
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
@@ -480,7 +482,7 @@ public:
 
           // Waits until kStages-2 stages of cp.async have committed
           arch::cp_async_wait<Base::kStages - 2>();
-          __syncthreads();
+          ark::sync_warps<Base::WarpCount::kCount * 32>();
 
           // Move to the next stage
           iterator_A.advance();
@@ -528,7 +530,7 @@ public:
     // Insert fence and wait for all outstanding cp.async operations to commit.
     cutlass::arch::cp_async_fence();
     cutlass::arch::cp_async_wait<0>();
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
   }
 };

```

`third_party/patches/cutlass/include/cutlass/conv/threadblock/implicit_gemm_pipelined.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- implicit_gemm_pipelined.h
+++ implicit_gemm_pipelined.h
@@ -45,6 +45,8 @@
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/threadblock/mma_base.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -218,7 +220,7 @@ public:
     ++this->smem_iterator_A_;
     ++this->smem_iterator_B_;
 
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     // Pair of fragments used to overlap shared memory loads and math instructions
     WarpFragmentA warp_frag_A[2];
@@ -264,7 +266,7 @@ public:
 
           this->smem_iterator_B_.store(transform_B(tb_frag_B));
 
-          __syncthreads();
+          ark::sync_warps<Base::WarpCount::kCount * 32>();
           
           ++this->smem_iterator_A_;
           ++this->smem_iterator_B_;

```

`third_party/patches/cutlass/include/cutlass/conv/threadblock/implicit_gemm_wgrad_fusion_multistage.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- implicit_gemm_wgrad_fusion_multistage.h
+++ implicit_gemm_wgrad_fusion_multistage.h
@@ -73,6 +73,8 @@
 #include "cutlass/gemm/warp/scale_bias_tile_iterator.h"
 #include "cutlass/conv/warp/scale_bias_relu_transform.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -551,7 +553,7 @@ public:
 
     // Waits until kStages-2 stages have committed. 
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
@@ -674,7 +676,7 @@ public:
 
           // Waits until kStages-2 stages of cp.async have committed
           arch::cp_async_wait<Base::kStages - 2>();
-          __syncthreads();
+          ark::sync_warps<Base::WarpCount::kCount * 32>();
 
           // Move to the next stage
           iterator_A.advance();
@@ -715,7 +717,7 @@ public:
     // Insert fence and wait for all outstanding cp.async operations to commit.
     cutlass::arch::cp_async_fence();
     cutlass::arch::cp_async_wait<0>();
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
   }
 };

```

`third_party/patches/cutlass/include/cutlass/epilogue/threadblock/epilogue.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- epilogue.h
+++ epilogue.h
@@ -62,6 +62,8 @@
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
 #include "cutlass/numeric_types.h"
 
+#include "sync.h"
+
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -273,7 +275,7 @@ private:
       // Convert and store fragment
       //
       
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
 
       acc2smem_source_not_needed<
@@ -282,7 +284,7 @@ private:
                                                                         accum_fragment_iterator,
                                                                         this->warp_tile_iterator_);
 
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       //
       // Load fragments from shared memory
@@ -399,13 +401,13 @@ private:
       //
       // Convert and store fragment
       //
-      
-      __syncthreads();
+
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       acc2smem_source_needed<cutlass::make_index_sequence<OutputTileIterator::kIterations>>::push(
           iter, accum_fragment_iterator, this->warp_tile_iterator_);
 
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       //
       // Load fragments from shared memory

```

`third_party/patches/cutlass/include/cutlass/epilogue/threadblock/epilogue_planar_complex.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- epilogue_planar_complex.h
+++ epilogue_planar_complex.h
@@ -56,6 +56,8 @@
 #include "cutlass/epilogue/threadblock/epilogue_base.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
 
+#include "sync.h"
+
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -285,7 +287,7 @@ public:
       // Convert and store fragment
       //
       
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       typename AccumulatorFragmentIterator::Fragment accum_fragment_real;
       typename AccumulatorFragmentIterator::Fragment accum_fragment_imag;
@@ -299,7 +301,7 @@ public:
       this->warp_tile_iterator_.store(accum_fragment_real);
       this->warp_tile_iterator_.store_with_pointer_offset(accum_fragment_imag, SharedStorage::kImaginaryStride);
 
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       //
       // Load fragments from shared memory

```

`third_party/patches/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_broadcast.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- epilogue_with_broadcast.h
+++ epilogue_with_broadcast.h
@@ -68,6 +68,8 @@
 
 #include "cutlass/numeric_types.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -536,7 +538,7 @@ private:
       //
       
 
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       acc2smem_source_not_needed<
           cutlass::make_index_sequence<OutputTileIterator::kIterations /
@@ -544,7 +546,7 @@ private:
                                                                         accum_fragment_iterator,
                                                                         this->warp_tile_iterator_);
 
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       //
       // Load fragments from shared memory
@@ -677,12 +679,12 @@ private:
       // Convert and store fragment
       //
       
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       acc2smem_source_needed<cutlass::make_index_sequence<OutputTileIterator::kIterations>>::push(
           iter, accum_fragment_iterator, this->warp_tile_iterator_);
 
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       //
       // Load fragments from shared memory

```

`third_party/patches/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_reduction.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- epilogue_with_reduction.h
+++ epilogue_with_reduction.h
@@ -64,6 +64,8 @@
 #include "cutlass/epilogue/threadblock/epilogue_base.h"
 #include "cutlass/epilogue/threadblock/predicated_tile_iterator.h"
 
+#include "sync.h"
+
 ////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -349,7 +351,7 @@ private:
     //
 
     // Guard against uses of the existing SMEM tile
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
     
     using AccessType = AlignedArray<ElementAccumulator, ThreadMap::kElementsPerAccess>;
 
@@ -378,7 +380,7 @@ private:
       aligned_reduction_ptr[col_idx] = frag_ptr[column];
     }
 
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     //
     // Now, threads are assigned several columns of the output. They fetch over all rows from
@@ -477,12 +479,12 @@ private:
       tensor_iterator.load(tensor_fragment);
       ++tensor_iterator;
       
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       acc2smem<cutlass::make_index_sequence<OutputTileIterator::kIterations>>::push(
           iter, accum_fragment_iterator, this->warp_tile_iterator_);
 
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       //
       // Load fragments from shared memory
@@ -585,12 +587,12 @@ private:
       // Convert and store fragment
       //
       
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       acc2smem<cutlass::make_index_sequence<OutputTileIterator::kIterations>>::push(
           iter, accum_fragment_iterator, this->warp_tile_iterator_);
 
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       //
       // Load fragments from shared memory

```

`third_party/patches/cutlass/include/cutlass/epilogue/threadblock/epilogue_with_visitor.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- epilogue_with_visitor.h
+++ epilogue_with_visitor.h
@@ -42,6 +42,8 @@
 #include "cutlass/semaphore.h"
 #include "cutlass/epilogue/threadblock/epilogue_base.h"
 
+#include "sync.h"
+
 ////////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -275,12 +277,12 @@ public:
       // Convert and store fragment
       //
 
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       acc2smem_source_needed<cutlass::make_index_sequence<Visitor::kIterations>>::push(
           iter_idx, accum_fragment_iterator, this->warp_tile_iterator_);
 
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       //
       // Load fragments from shared memory

```

`third_party/patches/cutlass/include/cutlass/gemm/kernel/gemm.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- gemm.h
+++ gemm.h
@@ -67,6 +67,7 @@ struct Gemm {
   /// Warp count (concept: GemmShape)
   using WarpCount = typename Mma::WarpCount;
   static int const kThreadCount = 32 * WarpCount::kCount;
+  static int const ThreadMask = kThreadCount - 1;
 
   /// Parameters structure
   struct Params {
@@ -235,7 +236,7 @@ struct Gemm {
     int gemm_k_iterations = (problem_size_k - tb_offset_A.column() + Mma::Shape::kK - 1) / Mma::Shape::kK;
 
     // Compute position within threadblock
-    int thread_idx = threadIdx.x;
+    int thread_idx = threadIdx.x & ThreadMask;
 
     // Construct iterators to A and B operands
     typename Mma::IteratorA iterator_A(
@@ -256,8 +257,8 @@ struct Gemm {
 
     // Broadcast the warp_id computed by lane 0 to ensure dependent code
     // is compiled as warp-uniform.
-    int warp_idx = __shfl_sync(0xffffffff, threadIdx.x / 32, 0);
-    int lane_idx = threadIdx.x % 32;
+    int warp_idx = __shfl_sync(0xffffffff, (threadIdx.x & ThreadMask) / 32, 0);
+    int lane_idx = (threadIdx.x & ThreadMask) % 32;
 
     //
     // Main loop

```

`third_party/patches/cutlass/include/cutlass/gemm/threadblock/mma_blas3_multistage.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- mma_blas3_multistage.h
+++ mma_blas3_multistage.h
@@ -46,6 +46,8 @@
 
 #include "cutlass/gemm/threadblock/mma_base.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -499,7 +501,7 @@ public:
 
     // Waits until kStages-2 stages have committed.
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
@@ -628,7 +630,7 @@ public:
 
           // Waits until kStages-2 stages have committed.
           arch::cp_async_wait<Base::kStages - 2>();
-          __syncthreads();
+          ark::sync_warps<Base::WarpCount::kCount * 32>();
 
           // Move to the next stage
           iterator_A.add_tile_offset({0, 1});
@@ -687,7 +689,7 @@ public:
       // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
       cutlass::arch::cp_async_fence();
       cutlass::arch::cp_async_wait<0>();
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
     }
 
   }

```

`third_party/patches/cutlass/include/cutlass/gemm/threadblock/mma_layernorm_mainloop_fusion_multistage.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- mma_layernorm_mainloop_fusion_multistage.h
+++ mma_layernorm_mainloop_fusion_multistage.h
@@ -51,6 +51,8 @@
 #include "cutlass/gemm/threadblock/mma_base.h"
 #include "cutlass/gemm/warp/layernorm_scale_bias_transform.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -663,7 +665,7 @@ public:
 
     // Waits until kStages-2 stages have committed.
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
@@ -785,7 +787,7 @@ public:
 
           // Waits until kStages-2 stages have committed.
           arch::cp_async_wait<Base::kStages - 2>();
-          __syncthreads();
+          ark::sync_warps<Base::WarpCount::kCount * 32>();
 
           // Move to the next stage
           iterator_A.add_tile_offset({0, 1});
@@ -850,7 +852,7 @@ public:
       // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
       cutlass::arch::cp_async_fence();
       cutlass::arch::cp_async_wait<0>();
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
     }
 
   }

```

`third_party/patches/cutlass/include/cutlass/gemm/threadblock/mma_multistage.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- mma_multistage.h
+++ mma_multistage.h
@@ -44,6 +44,8 @@
 
 #include "cutlass/gemm/threadblock/mma_base.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -427,7 +429,7 @@ public:
 
     // Waits until stages up to the previous (kStages-2)th stage have committed.
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
@@ -556,7 +558,7 @@ public:
 
           // Waits until stages up to the previous (kStages-2)th stage have committed.
           arch::cp_async_wait<Base::kStages - 2>();
-          __syncthreads();
+          ark::sync_warps<Base::WarpCount::kCount * 32>();
 
           // Move to the next stage
           iterator_A.add_tile_offset({0, 1});
@@ -615,7 +617,7 @@ public:
       // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
       cutlass::arch::cp_async_fence();
       cutlass::arch::cp_async_wait<0>();
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
     }
 
   }

```

`third_party/patches/cutlass/include/cutlass/gemm/threadblock/mma_pipelined.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- mma_pipelined.h
+++ mma_pipelined.h
@@ -45,6 +45,8 @@
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/threadblock/mma_base.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -217,7 +219,7 @@ public:
     ++this->smem_iterator_A_;
     ++this->smem_iterator_B_;
 
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     // Pair of fragments used to overlap shared memory loads and math instructions
     WarpFragmentA warp_frag_A[2];
@@ -267,7 +269,7 @@ public:
 
           this->smem_iterator_B_.store(transform_B(tb_frag_B));
 
-          __syncthreads();
+          ark::sync_warps<Base::WarpCount::kCount * 32>();
           
           ++this->smem_iterator_A_;
           ++this->smem_iterator_B_;

```

`third_party/patches/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_multistage.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- mma_planar_complex_multistage.h
+++ mma_planar_complex_multistage.h
@@ -47,6 +47,8 @@
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/threadblock/mma_planar_complex_base.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -482,7 +484,7 @@ public:
 
     // Blocks until all but kStages-2 cp.async stages have committed.
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
@@ -575,7 +577,7 @@ public:
 
           // Blocks until all but kStages-2 cp.async stages have committed.
           arch::cp_async_wait<Base::kStages - 2>();
-          __syncthreads();
+          ark::sync_warps<Base::WarpCount::kCount * 32>();
 
           // Move to the next stage
           iterator_A_real.add_tile_offset({0, 1});

```

`third_party/patches/cutlass/include/cutlass/gemm/threadblock/mma_planar_complex_pipelined.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- mma_planar_complex_pipelined.h
+++ mma_planar_complex_pipelined.h
@@ -44,6 +44,8 @@
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/threadblock/mma_planar_complex_base.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -287,7 +289,7 @@ public:
     ++this->smem_iterator_A_;
     ++this->smem_iterator_B_;
 
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     // Pair of fragments used to overlap shared memory loads and math instructions
     WarpFragmentA warp_frag_real_A[2];
@@ -349,7 +351,7 @@ public:
           this->smem_iterator_B_.store(tb_frag_B_real);
           this->smem_iterator_B_.store_with_pointer_offset(tb_frag_B_imag, Base::SharedStorage::kImaginaryStrideB);
 
-          __syncthreads();
+          ark::sync_warps<Base::WarpCount::kCount * 32>();
           
           ++this->smem_iterator_B_;
           ++this->smem_iterator_A_;

```

`third_party/patches/cutlass/include/cutlass/gemm/threadblock/mma_singlestage.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- a/include/cutlass/gemm/threadblock/mma_singlestage.h
+++ b/include/cutlass/gemm/threadblock/mma_singlestage.h
@@ -44,7 +44,7 @@
 #include "cutlass/gemm/gemm.h"
 #include "cutlass/gemm/threadblock/mma_base.h"
 
-
+#include "sync.h"
 
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
@@ -214,7 +214,7 @@ public:
       this->smem_iterator_A_.store(tb_frag_A);
       this->smem_iterator_B_.store(tb_frag_B);
 
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       //
       // Loop over GEMM K dimension
@@ -242,7 +242,7 @@ public:
       this->warp_tile_iterator_A_.add_tile_offset({0, -Policy::kPartitionsK * Base::kWarpGemmIterations});
       this->warp_tile_iterator_B_.add_tile_offset({-Policy::kPartitionsK * Base::kWarpGemmIterations, 0});
 
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
 
       iterator_A.load(tb_frag_A);
       iterator_B.load(tb_frag_B);

```

`third_party/patches/cutlass/include/cutlass/gemm/threadblock/mma_softmax_mainloop_fusion_multistage.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- mma_softmax_mainloop_fusion_multistage.h
+++ mma_softmax_mainloop_fusion_multistage.h
@@ -49,6 +49,8 @@
 #include "cutlass/gemm/threadblock/mma_base.h"
 #include "cutlass/gemm/warp/softmax_scale_bias_transform.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -559,7 +561,7 @@ public:
 
     // Waits until kStages-2 stages have committed.
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
@@ -675,7 +677,7 @@ public:
 
           // Waits until kStages-2 stages have committed.
           arch::cp_async_wait<Base::kStages - 2>();
-          __syncthreads();
+          ark::sync_warps<Base::WarpCount::kCount * 32>();
 
           // Move to the next stage
           iterator_A.add_tile_offset({0, 1});
@@ -736,7 +738,7 @@ public:
       // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
       cutlass::arch::cp_async_fence();
       cutlass::arch::cp_async_wait<0>();
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
     }
 
   }

```

`third_party/patches/cutlass/include/cutlass/gemm/threadblock/mma_sparse_multistage.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- mma_sparse_multistage.h
+++ mma_sparse_multistage.h
@@ -44,6 +44,8 @@
 
 #include "cutlass/gemm/threadblock/mma_sparse_base.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -476,7 +478,7 @@ public:
 
     // DEPBAR+SYNC
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
@@ -596,7 +598,7 @@ public:
 
           // Waits until kStages-2 stages have committed. 
           arch::cp_async_wait<Base::kStages - 2>();
-          __syncthreads();
+          ark::sync_warps<Base::WarpCount::kCount * 32>();
 
           // Move to the next stage
           iterator_A.add_tile_offset({0, 1});

```

`third_party/patches/cutlass/include/cutlass/gemm/threadblock/mma_with_reduction_multistage.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- mma_with_reduction_multistage.h
+++ mma_with_reduction_multistage.h
@@ -44,6 +44,8 @@
 
 #include "cutlass/gemm/threadblock/mma_base.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -383,7 +385,7 @@ public:
 
     // Waits until kStages-2 stages have committed.
     cutlass::arch::cp_async_wait<Base::kStages - 2>();
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
 
     // Pair of fragments used to overlap shared memory loads and math
     // instructions
@@ -480,7 +482,7 @@ public:
 
           // Waits until kStages-2 stages have committed.
           arch::cp_async_wait<Base::kStages - 2>();
-          __syncthreads();
+          ark::sync_warps<Base::WarpCount::kCount * 32>();
 
           // Move to the next stage
           iterator_A.add_tile_offset({0, 1});
@@ -532,7 +534,7 @@ public:
       // commit and drain all pending and predicated LDGSTS pnz from the GEMM mainloop
       cutlass::arch::cp_async_fence();
       cutlass::arch::cp_async_wait<0>();
-      __syncthreads();
+      ark::sync_warps<Base::WarpCount::kCount * 32>();
     }
 
   }

```

`third_party/patches/cutlass/include/cutlass/gemm/warp/mma_tensor_op_tile_iterator_sm70.h.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- mma_tensor_op_tile_iterator_sm70.h
+++ mma_tensor_op_tile_iterator_sm70.h
@@ -49,6 +49,8 @@
 
 #include "cutlass/platform/platform.h"
 
+#include "sync.h"
+
 /////////////////////////////////////////////////////////////////////////////////////////////////
 
 namespace cutlass {
@@ -2430,7 +2432,7 @@ public:
     }
 
     #if defined(__CUDA_ARCH__)
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
     #endif
 
     ref_.add_coord_offset(origin_);
@@ -2781,7 +2783,7 @@ public:
     }
 
     #if defined(__CUDA_ARCH__)
-    __syncthreads();
+    ark::sync_warps<Base::WarpCount::kCount * 32>();
     #endif
 
     ref_.add_coord_offset(origin_);

```

`third_party/patches/gpudma/module/ioctlrw.c.patch`:

```patch
# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

--- ioctlrw.c
+++ ioctlrw.c
@@ -34,7 +34,7 @@ void free_nvp_callback(void *data)
     if(entry) {
         res = nvidia_p2p_free_page_table(entry->page_table);
         if(res == 0) {
-            printk(KERN_ERR"%s(): nvidia_p2p_free_page_table() - OK!\n", __FUNCTION__);
+            //printk(KERN_ERR"%s(): nvidia_p2p_free_page_table() - OK!\n", __FUNCTION__);
             //entry->virt_start = 0ULL;
             //entry->page_table = 0;
         } else {
@@ -86,7 +86,7 @@ int ioctl_mem_lock(struct gpumem *drv, unsigned long arg)
     param.page_count = entry->page_table->entries;
     param.handle = entry;
 
-    printk(KERN_ERR"%s(): param.handle: %p\n", __FUNCTION__, param.handle);
+    //printk(KERN_ERR"%s(): param.handle: %p\n", __FUNCTION__, param.handle);
 
     if(copy_to_user((void *)arg, &param, sizeof(struct gpudma_lock_t))) {
         printk(KERN_ERR"%s(): Error in copy_from_user()\n", __FUNCTION__);
@@ -96,7 +96,7 @@ int ioctl_mem_lock(struct gpumem *drv, unsigned long arg)
 
     list_add_tail(&entry->list, &drv->table_list);
 
-    printk(KERN_ERR"%s(): Add new entry. handle: %p\n", __FUNCTION__, entry->handle);
+    //printk(KERN_ERR"%s(): Add new entry. handle: %p\n", __FUNCTION__, entry->handle);
 
     return 0;
 
@@ -129,8 +129,8 @@ int ioctl_mem_unlock(struct gpumem *drv, unsigned long arg)
         if(entry) {
             if(entry->handle == param.handle) {
 
-                printk(KERN_ERR"%s(): param.handle = %p\n", __FUNCTION__, param.handle);
-                printk(KERN_ERR"%s(): entry.handle = %p\n", __FUNCTION__, entry->handle);
+                //printk(KERN_ERR"%s(): param.handle = %p\n", __FUNCTION__, param.handle);
+                //printk(KERN_ERR"%s(): entry.handle = %p\n", __FUNCTION__, entry->handle);
 
                 if(entry->virt_start && entry->page_table) {
                     error = nvidia_p2p_put_pages(0, 0, entry->virt_start, entry->page_table);
@@ -140,14 +140,14 @@ int ioctl_mem_unlock(struct gpumem *drv, unsigned long arg)
                     }
                     //entry->virt_start = 0ULL;
                     //entry->page_table = 0;
-                    printk(KERN_ERR"%s(): nvidia_p2p_put_pages() - Ok!\n", __FUNCTION__);
+                    //printk(KERN_ERR"%s(): nvidia_p2p_put_pages() - Ok!\n", __FUNCTION__);
                 }
 
                 list_del(pos);
                 kfree(entry);
                 break;
             } else {
-                printk(KERN_ERR"%s(): Skip entry: %p\n", __FUNCTION__, entry->handle);
+                //printk(KERN_ERR"%s(): Skip entry: %p\n", __FUNCTION__, entry->handle);
             }
         }
     }
@@ -180,8 +180,8 @@ int ioctl_mem_state(struct gpumem *drv, unsigned long arg)
         if(entry) {
             if(entry->handle == header.handle) {
 
-                printk(KERN_ERR"%s(): param.handle = %p\n", __FUNCTION__, header.handle);
-                printk(KERN_ERR"%s(): entry.handle = %p\n", __FUNCTION__, entry->handle);
+                //printk(KERN_ERR"%s(): param.handle = %p\n", __FUNCTION__, header.handle);
+                //printk(KERN_ERR"%s(): entry.handle = %p\n", __FUNCTION__, entry->handle);
 
                 if(!entry->page_table) {
                     printk(KERN_ERR"%s(): Error - memory not pinned!\n", __FUNCTION__);
@@ -205,10 +205,10 @@ int ioctl_mem_state(struct gpumem *drv, unsigned long arg)
                     if(nvp) {
                         param->pages[i] = nvp->physical_address;
                         param->page_count++;
-                        printk(KERN_ERR"%s(): %02d - 0x%llx\n", __FUNCTION__, i, param->pages[i]);
+                        //printk(KERN_ERR"%s(): %02d - 0x%llx\n", __FUNCTION__, i, param->pages[i]);
                     }
                 }
-                printk(KERN_ERR"%s(): page_count = %ld\n", __FUNCTION__, (long int)param->page_count);
+                //printk(KERN_ERR"%s(): page_count = %ld\n", __FUNCTION__, (long int)param->page_count);
                 param->handle = header.handle;
                 if(copy_to_user((void *)arg, param, size)) {
                     printk(KERN_DEBUG"%s(): Error in copy_to_user()\n", __FUNCTION__);
@@ -217,7 +217,7 @@ int ioctl_mem_state(struct gpumem *drv, unsigned long arg)
 
                 kfree(param);
             } else {
-                printk(KERN_ERR"%s(): Skip entry: %p\n", __FUNCTION__, entry->handle);
+                //printk(KERN_ERR"%s(): Skip entry: %p\n", __FUNCTION__, entry->handle);
             }
         }
     }

```