Project Path: arc_gmh5225_ghostbusters_11a6k0uk

Source Tree:

```txt
arc_gmh5225_ghostbusters_11a6k0uk
├── README.md
├── main.cpp
├── notebook.ipynb
└── requirements.txt

```

`README.md`:

```md

### Senior Design: Anit-Cheat Detection system
---

#### "Measure Learning":
- Client Side Application
- Server Side Streaming
- UDP style streaming capabilites
- Camara Processing Capabilities
- Multithreading
- MIT / Free Licencing

### Behaviors:
- Bluetooch Monitoring
- Video Streaming
- Mouse Analysis
- Feature Analysis
- Privacy Concerns (FIPPA)

#### Frameworks Ideas:
- Flask
- React
- .NET MAUI

### Analysis/Training Methods:
Scope: Monitoring features to begin with, and AI enhancements to further the focus of the models and feature. Increase accurcy and integrityare of the utmost concern.

### Market Research:
- SSV
- [OpenCV](https://docs.opencv.org/3.4/index.html)
- FaceNet
- [Deepface](https://research.facebook.com/publications/deepface-closing-the-gap-to-human-level-performance-in-face-verification/)
- [PyGaze](http://www.pygaze.org/about/)
    - this will allow for naive tracking, and will work on a computer that does not have a eyetracker
- [OGAMA](http://www.ogama.net/)
- [Tobii](http://www.tobii.com/)
- [Gaze Recorder](https://gazerecorder.com/)
- [Arsfutura](https://arsfutura.com/magazine)
- [FFMPEG](https://ffmpeg.org/)
- [EthoVision XT](https://www.noldus.com/ethovision-xt)
- [Pupil](https://pupil-labs.com/products/core/)
- [RasterVision](https://docs.rastervision.io/en/stable/usage/basics.html)

#### Systematic Possiblities:
[1. CNN-based image recognition for topology optimization](https://www.sciencedirect.com/science/article/abs/pii/S0950705120302379)
[2. Support Vector Machine (SVM) for Anomaly Detection](https://towardsdatascience.com/support-vector-machine-svm-for-anomaly-detection-73a8d676c331)

#### Medium Articles:
[1. A FaceNet-Style Approach to Facial Recognition on the Google Coral Development board](https://towardsdatascience.com/a-facenet-style-approach-to-facial-recognition-dc0944efe8d1)

#### Datasets:
[1. One-dimensional CNN for human behavior classification](https://towardsdatascience.com/one-dimensional-cnn-for-human-behavior-classification-fb4371d03633)
[2. Machine learning for cognitive behavioral analysis: datasets, methods, paradigms, and research directions](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10390406/)


```

`main.cpp`:

```cpp
#include <iostream>
#include <tensorflow/cc/client/client_session.h>
#include <tensorflow/cc/ops/standard_ops.h>
#include <tensorflow/core/framework/tensor.h>

using namespace tensorflow;
using namespace tensorflow::ops;

class NN
{
public:
    NeuralNetwork(int input_size, int output_size, int hidden_size):
        input_size(input_size), output_size(output_size), hidden_size(hidden_size) {
            // Define the input the output placeholders
            x_ = Placeholder<float>(Placeholder<float>::Shape({-1, input_size}));
            y_ = Placeholder<float>(Placeholder<float>::Shape({-1, output_size}));

            // Define the weights and biases
            w1_ = Variable(Shape({input_size, hidden_size}), DT_FLOAT);
            b1_ = Variable(Shape({hidden_size}, DT_FLOAT));
            w2_ = Variable(Shape({hidden_size, output_size}), DT_FLOAT);
            b2_ = Variable(Shape({output_size}, DT_FLOAT));

            // Define the graph
            h1_ = Tanh(MatMul(x_, w1_) + b1_);
            y_hat_ = Tanh(MatMul(h1_, w2_) + b2_);

            // Define the output of the neural network
            auto hidden = Tanh(MatMul(x_, w1_) + b1_);
            output_ = Softmax(MatMul(hidden, w2_) + b2_);

            // Define the loss function
            loss_ = ReduceMean(Square(output_ - y_), {0,1});

            // Define the optimizer
            optimizer_ = GradientDescentOptimizer(0.5f);
            train_op_ = optimizer_.Minimize(loss_);

            // Initialize the session and variables
            session_ = new ClientSession();
            TF_CHECK_OK(session_->Run({
                                        Assign(w1_, RandomNormal(Shape({input_size, hidden_size}))),
                                        Assign(b1_, RandomNormal(Shape({hidden_size})),
                                        Assign(w2_, RandomNormal(Shape({hidden_size, output_size})),
                                        Assign(b2_, RandomNormal(Shape({output_size}))))
                                        )}
                                    ));
   
        }

    ~NeuralNetwork() {
        delete session_;
    }

    void Train(const Tensor& x_data, const Tensor& y_data) {
        // Predict the output for the input data
        Tensor output_data;
        TF_CHECK_OK(session_->Run({{x_, x_data}}, {output__}, &output_data));
        return output_data;
    }

private:
    int input_size;
    int output_size;
    int hidden_size;
    Placeholder<float> x_;
    Placeholder<float> y_;
    Variable w1_;
    Variable b1_;
    Variable w2_;
    Variable b2_;
    SoftmaxOutput output_;
    ReduceMean loss_;
    GradientDescentOptimizer optimizer_;
    Operation train_op_;
    ClientSession* session_;

};





int main()
{
    cout << "Hello World!" << endl;
    
    // Define the input and output sizes
    const int input_size = 784;
    const int output_size = 10;
    const int hidden_size = 100;

    // Create the neural network
    NeuralNetwork nn(input_size, output_size, hidden_size);

    // Train the neural network
    for(int i=0; i<1000; i++) {
        // Generate random input and output data
        Tensor x_data(DT_FLOAT, TensorShape({100, input_size}));
        Tensor y_data(DT_FLOAT, TensorShape({100, output_size}));
        auto x_data_map = x_data.tensor<float, 2>();
        auto y_data_map = y_data.tensor<float, 2>();
        for (int j = 0; j < 100; j++){
            for (int k = 0; k < input_size; k++){
                x_data_map(j, k) = rand() % 100;
            }
            for (int k = 0; k < output_size; k++){
                y_data_map(j, k) = rand() % 100;
            } 
        }
        // Train the network on the input and output data
        nn.Train(x_data, y_data);
    }

    // Predict the output for some input data
    Tensor x_test(DT_FLOAT, TensorShape({1, input_size}));
    auto x_test_map = x_test.tensor<float, 2>();
    for (int i = 0; i < input_size; i++){
        x_test_map(0, i) = rand() % 100;
    }

    Tensor y_test = nn.Predict(x_test);
    std::cout << "Output: " << Y_test.tensor<float, 2>()(0, 0) << std::endl;

    return 0;
}


```

`notebook.ipynb`:

```ipynb
Jupyter Notebook Summary:
Total cells: 2 (0 code, 2 markdown, 0 raw)

(No code cells found)

```

`requirements.txt`:

```txt
# notebooks
imutils
opencv-python

```